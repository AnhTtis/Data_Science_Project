\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\DeclareMathOperator{\lap}{Lap} 
\DeclareMathOperator{\Clip}{Clip} 
\DeclareMathOperator{\topk}{top} 
\DeclareMathOperator{\blurs}{blurs} 
\DeclareMathOperator{\blur}{blur} 
\DeclareMathOperator{\lus}{lus} 
\DeclareMathOperator{\randk}{rand} 
\DeclareMathOperator{\spar}{spar}
% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
% \let\oldthebibliography=\thebibliography
% \let\oldendthebibliography=\endthebibliography
% \renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{51}%
% }{\oldendthebibliography}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{4552} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
% \title{Make Landscape Flatter in Differentially Private Federated Learning for Author Response}  % **** Enter the paper title here

% \maketitle
\thispagestyle{empty}
\appendix

Thanks for the reviewersâ€™ positive comments, and then we address the weaknesses and minor comments below.\\ 
% \vspace{-0.25cm}
%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
% \section{Response to Reviewer Q427}
\textbf{1. Response to Reviewer Q427}
% \vspace{-0.25cm}

\noindent
\textbf{Q1:} About the writing. 
\textbf{A1:} We will give the main variable description on line 127 in the next version.

\noindent
\textbf{Q2:} About the novelty. 
\textbf{A2:} This paper aims to solve serious performance degradation in federated learning (FL) with differential privacy (DP) protection, and we are the first to research possible reasons from the model flatness perspective. 
In addition, we give a detailed theoretical analysis: sensitivity, privacy, and convergence analysis guarantees, which is non-trivial. Finally, we conduct extensive experiments, and visualize loss landscape and surface contour.
% the SAM optimizer is introduced to make the loss landscape flatter and achieve better weight perturbation robustness, thereby improving performance. In addition, we give a detailed theoretical analysis: sensitivity, privacy, and convergence analysis guarantees, experimentally verify effectiveness, and visualize loss landscape and contour to further echo with the research motivation.

\noindent
\textbf{Q3}: Comparison of
theoretical bounds. 
\textbf{A3:} 
% We specify the bounds of the SOTA baseline methods below.
% in the paper. To deeply analyze the role of SAM in privacy protection, we provide the sensitivity analysis. \\
% \emph{1. Sensitivity analysis.} All baseline methods (e.g., [8, 21]) use SGD optimizer and the other techniques (such as local update regularization and sparsification) they used do not affect the sensitivity analysis results. Thus, the bounds of baseline methods are the bound of local update with SGD in \textbf{Section} 5.1, and the bound of local update with SAM (DP-FedSAM) is obviously tighter in \textbf{Remark} 1.
% \emph{2. Privacy analysis.} Eq. (14) gives the calculation formulation of the privacy budget $\epsilon$ in each communication round, from which $\epsilon$ in each round can be obtained, while $\epsilon$ of the baseline methods are obtained according to the calculation given in the corresponding papers. Thus, the contribution is reflected in the experimental results, e.g., in Table 2 of \textbf{Section} 6.3. It is clear that our algorithm achieves a better trade-off between privacy and performance.
% \noindent
As local learning rate satisfies $\eta=\mathcal{O}({1}/{L\sqrt{KT}})$ , the bound in [8] (DP-FedAvg-blur and DP-FedAvg-blurs) is $\small \mathcal{O}\left( \frac{1}{\sqrt{KT}} + \frac{6K\sigma_{g}^2 + \sigma_l^2}{T} + \frac{B^2\sum_{t=1}^T(\overline{\alpha}^t + \tilde{\alpha}^t)}{T}+\frac{L^2 \sqrt{T}\sigma^2C^2d}{m^2\sqrt{K}}\right)$ and that in [21] ([21] proposes Fed-SMP-$\topk_k$ and Fed-SMP-$\randk_k$, [12] is the first paper to introduce SAM) is $\small \mathcal{O}\left( \frac{1}{\sqrt{KT}} + \frac{3\sigma_{g}^2 + 2\sigma_l^2}{\sqrt{KT}} +\frac{4L^2 \sqrt{T}\sigma^2C^2d}{m^2\sqrt{K}}\right)$. Thus, it is clear that the bound in [8] is tighter than that in [21]. And our bound $\small \mathcal{O}\left( \frac{1}{\sqrt{KT}} + \frac{
   L^2\sigma_l^2}{KT^2} + \frac{\sum_{t=1}^T(\overline{\alpha}^t \sigma_g^2 + \tilde{\alpha}^t L^2)}{T^2}+\frac{L^2 \sqrt{T}\sigma^2C^2d}{m^2\sqrt{K}}\right)$ is tighter than that in [8]. Specifically, we reduce the impacts of the local and global variance $\sigma_l^2$, $\sigma_g^2$. And the negative impacts of $\overline{\alpha}^t$ and $\tilde{\alpha}^t$ are also  mitigated upon convergence.
   
%   due to local SAM optimizer being adopted. It means that we effectively alleviate performance degradation caused by clipping operation in DP and achieve better performance under symmetric noise.\\
\noindent
\textbf{Q4:} About experimental evaluations. 
\textbf{A4:} 
% Experimental comparisons to prior work (\cite{cheng2022differentially, hu2022federated, McMahan2018learning}) are presented in this paper. 
Although the baseline methods seem to be over-fitting in Table 1, especially on more complex data (e.g., CIFAR-100 in Table 4 of \textbf{Appendix}). The main reasons are the random noise introduced by DP, and the over-fitting and inconsistency of the local models. And it is not solved by early stopping. In FL, due to the distribution and amount of data in each client, the local models are extremely easy to over-fit, but the global model after aggregation may be under-fitted. Thus test accuracy cannot reach a good value if early stopping. It also can be verified in training curves (Figure 2). 
\\
%-------------------------------------------------------------------------
% \vspace{-0.25cm}
% \section{Response to Reviewer MMpf}
\textbf{2. Response to Reviewer MMpf}
% \vspace{-0.25cm}

We greatly appreciate your support of our work.

\noindent
\textbf{Q:} About experimental evaluations: trade-off.
\textbf{A:} In Table 2 of \textbf{Section} 6.3, the accuracy comparison of all algorithms under different privacy budgets $\epsilon$ are given to balance these two. Note that $\epsilon$ is not a hyper-parameter, but can be obtained by the privacy design in each round such as Eq. (14).
% , while $\epsilon$ of the baseline methods are obtained according to the calculation given in the corresponding papers [8, 21].
Thus, similar to the existing works [8, 21], there is a better trade-off between privacy and performance when achieving better accuracy under the same $\epsilon$ or a smaller $\epsilon$ under approximately the same accuracy.\\
% \begin{table}[ht]
% \centering
% \caption{Performance comparison under different privacy budgets.}
% \vspace{-0.2cm}
% \label{privacy}
% \small 
% \renewcommand{\arraystretch}{0.6}
% \resizebox{\linewidth}{!}{
% \begin{tabular}{ccccc} 
% \toprule
% \multirow{2}{*}{Algorithm} & \multicolumn{4}{c}{Averaged test accuracy (\%) under different privacy budgets $\epsilon$}                                                                                                            \\ 
% \cmidrule{2-5}
%                           &  $\epsilon$ = 4                      & $\epsilon$ = 6                                  &$\epsilon$ =  8                                  & $\epsilon$ = 10                                  \\ 
% \midrule
% DP-FedAvg                  & 38.23 $\pm$
%   0.15 & 43.87 $\pm$ 0.62 & 46.74 $\pm$ 0.03 & 49.06 $\pm$ 0.49  \\
% Fed-SMP-$\randk_k$              & 33.78 $\pm$ 0.92    & 42.21 $\pm$ 0.21 & 48.20 $\pm$ 0.05 & 50.62 $\pm$ 0.14  \\
% Fed-SMP-$\topk_k$               & 38.99 $\pm$0.50     & 46.24 $\pm$ 0.80 & 49.78 $\pm$ 0.78 & 52.51 $\pm$ 0.83  \\
% DP-FedAvg-$\blur $            & 38.23 $\pm$ 0.70    & 43.93 $\pm$ 0.48 & 46.74 $\pm$ 0.92 & 49.06 $\pm$ 0.13  \\
% DP-FedAvg-$\blurs $             & 39.39 $\pm$0.43     & 46.64 $\pm$ 0.36  & 50.18 $\pm$ 0.27 & 52.91 $\pm$ 0.57  \\
% DP-FedSAM                  & \textbf{39.89$\pm$ 0.17}   & 47.92 $\pm$ 0.23 & 51.30 $\pm$ 0.95 & 53.18 $\pm$ 0.40  \\
% DP-FedSAM-$\topk_k$              & 38.96 $\pm$ 0.61    & \textbf{49.17 $\pm$ 0.15} & \textbf{53.64 $\pm$ 0.12} & \textbf{56.36 $\pm$ 0.36}  \\
% \bottomrule
% \end{tabular}
% }
% \vspace{-0.45cm}
% \end{table}
% \vspace{-0.25cm}
% \section{Response to Reviewer aUa8}
% \vspace{-0.25cm}
% \subsection{Response length}
\textbf{3. Response to Reviewer aUa8}

Please reconsider the contributions of our work.

\noindent
\textbf{Q1:} About really differentially private (DP).
\textbf{A1:} Existing work [21] has verified SGD and top k sparsification satisfying the Renyi DP. SAM optimizer only adds perturbation on the basis of SGD and affects on the model during training. And both SAM and top k sparsification are performed before the DP process, thereby satisfying the Renyi DP.
% \textbf{Q: General comment.} Note that clipping and adding Gaussian noise to gradients is a famous technique to achieve the differential privacy (DP) protection, in which the effectiveness has been proven in reference [1]. Inspired by this idea, we clipped the local update of each client and added Gaussian noise to it to achieve client-level DP. In addition, 

% \emph{\small [1] M. Nasr, S. Songi, A. Thakurta, N. Papernot and N. Carlin. Adversary instantiation: Lower bounds for differentially private machine learning. In Proc. IEEE Symposium on Security and Privacy (SP), 2021, pp. 866-882.}
% \emph{\small [1] Nasr et al., "Adversary instantiation: Lower bounds for differentially private machine learning" IEEE SP 2021.}
\noindent
\textbf{Q2:} About some claims.
% \textbf{A2:} 
%  Before answering this problem, we must retrospect the definition of client-level DP in FL.
\textbf{A2.1: In line 325-327.} Similar evidence has been drawn in \footnote{ Abadi et al., "Deep learning with differential privacy" CCS 2016.}. In addition, we aim to hide the participation information of clients, i.e., whether someone client participates in FL training or not. Thus, if we sample clients randomly, the adversary will be difficult to infer the participation information.
\textbf{A2.2: In formula 9.} Similar explanations have been very common in existing works [8, 21]. The variance of noise added is proportional to $C^2/m^2$ due to the DP sensitivity in \textbf{Lemma} 1. Because of the aggregation of $m$ local updates, the variance of aggregated noise will be reduced $1/m$ times. Therefore, we only need to add noise with variance $\sigma^2 C^2 I_d/m$. 
\textbf{A2.3: In line 375-376.} The generalization ability can be measured by the loss landscape and differential value between training and testing accuracy in Figure 4 (a) and Table 3, respectively. The robustness to DP noise can be measured by the loss surface contour in Figure 4 (b).
\textbf{A2.4: In Remark 1.} The experimental results in Table 2 show that our methods can achieve better accuracy than baseline methods with the same DP budget $\epsilon$ or smaller $\epsilon$ under approximately the same accuracy. 
% \textbf{Remark} 1 is obtained from \textbf{Theorem} 1, and reveals that the proposed DP-FedSAM can achieve a smaller DP sensitivity than DP-FedAvg, which means a smaller noise variance added on models as well as a better model performance with a given DP requirement. 
\textbf{A2.5: In Remark 3.} We verify that Remark 3 is correct by conducting some experiments on DP-FedSAM on EMNIST. And it is impossible to increase the training rounds and compensate for the utility loss. Specifically, under the same total client size ($M=500$) and privacy ($\epsilon=5.82$), when the sampling rate $q$ is 0.1, 0.08, and 0.05, the corresponding round $T$ is 100, 150, and 400, and test accuracy is 83.49\%, 69.84\%, and 56.23\%, respectively.
% Note that more training rounds are not allowed because the privacy budget also increases and has a limited value. For instance, 
% We agree with your opinion that increasing the training rounds can compensate for the utility loss in many cases. However, in terms of DP, more training rounds are not allowed because the privacy budget increases with communication rounds and has a limited value.

\noindent
\textbf{Q3:} About missing details in the experiment.
\textbf{A3.1: top k sparsification.} Sparsification is a very common method when considering privacy protection to introduce a large amount of random noise in FL [8, 19, 21]. It retains only the larger weight part of each layer of the local model with a sparsity ratio of $k/d$ (d is the weight scale), and the rest are sparse. The advantage is that the amount of random noise can be reduced (no noise needs to be added to the sparse weight position), so the performance can be improved, which has been thoroughly verified in [8, 19, 21].
\textbf{A3.2: non-private models.} The vanilla FedAvg performance is the upper limit (roughly 10-20\% higher on SVHN dataset) of the algorithm performance after adding DP noise, which has been verified in [21]. Thus, this paper only provides the performance on private models.
\textbf{A3.3: the cost of doing SAM plus the top k sparsification.} SAM needs to perform two gradient calculations and sparsification may lead to some performance degradation because the model is compressed and some information may be lost. 
% Thus, there is a balance between performance and the size of added DP noise. In general, it is better when sparsity ratio is set to 0.5.
\textbf{A3.4: In figure 5.} We will correct to total clients in the next version. 

%%%%%%%%% REFERENCES
% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{ref}
% }

\end{document}
