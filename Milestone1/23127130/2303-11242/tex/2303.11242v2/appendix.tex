\newpage
\onecolumn 

\vspace{0.5in}
\begin{center}
 \rule{6.875in}{0.7pt}\\ % 4.0
 {\Large\bf Supplementary Material for\\ `` Make Landscape Flatter in Differentially Private Federated Learning ''}
 \rule{6.875in}{0.7pt}
\end{center}
\appendix


\section{More Implementation Detail}\label{imp_detail}
\subsection{Dataset}
EMNIST \cite{cohen2017emnist} is a 62-class image classification dataset. In this paper, we use 20\% of the dataset, which includes 88,800 training samples and 14,800 validation examples. 
Both CIFAR-10 and CIFAR-100 \cite{krizhevsky2009learning} have 60,000 images. In addition, these images are divided into 50,000 training samples and 10,000 validation examples. CIFAR-100 has finer labeling, with 100 unique labels, in comparison to CIFAR-10, having 10 unique labels. Furthermore, we
divide these datasets to each client based on Dirichlet allocation over 500 clients by default. 


\subsection{Configuration}
For the EMNIST dataset, we set the mini-batch size to 32 and train with a simple CNN model, which includes two convolutional layers with 5Ã—5 kernels, max pooling, followed by a 512-unit dense layer. For CIFAR-10 and CIFAR-100 datasets, we set the mini-batch size to 50 and train with ResNet-18 \cite{he2016deep} architecture. 
For each algorithm and each dataset, the learning rate is set via grid search on the set $\{10^{-0.5}, 10^{-1}, 10^{-1.5}, 10^{-2}\}$. The weight perturbation ratio $\rho$ is set via grid search on the set $\{0.01, 0.1, 0.3, 0.5, 0.7, 1.0\}$. For all methods using the sparsification technique, the sparsity ratio is set to $p=0.4$.

\section{Additional Experiment on CIFAR-100}
% \subsection{The more results on EMNIST}
% Performance under different privacy budgets $\epsilon$ under non-IID setting with Dirichlet 0.3 and IID setting.

% \syf{Two table and some comments}
% \subsection{The more results on CIFAR-10}
% Performance under different privacy budgets $\epsilon$ under non-IID setting with Dirichlet 0.3 and IID setting.

% \syf{Two table and some comments}
\begin{figure*}[ht]
\centering
\includegraphics[width=1\textwidth]{figures/cifar100.pdf}
% \vspace{-0.35cm}
\caption{\small The averaged testing accuracy on \textit{CIFAR-100} dataset under symmetric noise for all compared methods.}
% \vspace{-0.35cm}
\label{fig:cifar100}
\end{figure*}


\begin{table}[ht]
% \vspace{-0.2cm}
\centering
\caption{Averaged training  and testing accuracy (\%) on \textit{CIFAR-100} in both IID and Non-IID settings under symmetric noise for all compared methods. Note that the performance of the CIFAR-100 dataset is relatively poor across all algorithms due to the more severe impact of DP in complex tasks.}
\small
% \scriptsize
\renewcommand{\arraystretch}{0.5}
\label{table:cifar100_all_baselines}
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{ccccccc} 
\toprule
\multirow{2}{*}{Algorithm} & \multicolumn{2}{c}{Dirichlet~0.3} & \multicolumn{2}{c}{Dirichlet~0.6} & \multicolumn{2}{c}{IID}  \\ 
\cmidrule{2-7}
                           & Train      & Validation           & Train      & Validation           & Train      & Validation            \\ 
\midrule
DP-FedAvg                  & 91.14$\pm$0.16 & 16.10$\pm$0.71           & 92.33$\pm$0.08 & 15.92$\pm$0.39           & 94.01$\pm$0.10 & 17.47$\pm$0.47            \\
Fed-SMP-$\randk_k$              & 90.70$\pm$0.01 & 17.25$\pm$0.16           & 92.28$\pm$0.32 & 17.50$\pm$0.19           & 94.31$\pm$0.02 & 17.68$\pm$0.44            \\
Fed-SMP-$\topk_k$                & 92.58$\pm$0.24 & 18.58$\pm$0.25           & 93.51$\pm$0.11 & 18.07$\pm$0.09           & 95.06$\pm$0.05 & 19.09$\pm$0.56            \\
DP-FedAvg-$\blur$             & 91.27$\pm$0.01 & 17.03$\pm$0.09           & 92.33$\pm$0.03 & 17.92$\pm$0.01           & 94.01$\pm$0.04 & 18.47$\pm$0.02            \\
DP-FedAvg-$\blurs$            & 92.98$\pm$0.24 & 18.98$\pm$0.25           & 94.01$\pm$0.11 & 18.27$\pm$0.19           & 95.46$\pm$0.05 & 19.59$\pm$0.06            \\
DP-FedSAM                  & 82.19$\pm$0.01 & 18.88$\pm$0.31           & 85.47$\pm$0.13 & 19.09$\pm$0.15           & 87.12$\pm$0.37 & 20.64$\pm$0.48            \\
DP-FedSAM-$\topk_k$              & 84.49$\pm$0.24 & \textbf{20.85$\pm$0.63}  & 88.23$\pm$0.23 & \textbf{21.24$\pm$0.69}  & 89.86$\pm$0.21 & \textbf{22.30$\pm$0.05}   \\
\bottomrule
\end{tabular}}
\end{table}



\subsection{Performance with Compared Baselines}

In Table \ref{table:cifar100_all_baselines} and Figure \ref{fig:cifar100}, we evaluate DP-FedSAM and DP-FedSAM-$\topk_k$ on CIFAR-100 dataset in both settings compared with all baselines from DP-FedAvg to DP-FedAvg-$\blurs$. From all these results, it is clearly seen that our proposed algorithms outperform other baselines under symmetric noise both on accuracy and generalization perspectives. It means that we significantly improve the performance and generate a better trade-off between performance and privacy in DPFL. 
For instance, in the IID setting, the averaged testing accuracy is $20.64\%$ in DP-FedSAM, where the accuracy gain is $3.17\%$ compared with DP-FedAvg.
And the average testing accuracy is $22.30\%$ in DP-FedSAM-$\topk_k$, 
where the accuracy gain is $3.21\%$ compared with Fed-SMP-$\topk_k$.
That means our algorithms significantly mitigate the performance degradation issue caused by DP.

% \ls{double check the report acc numbers. Report the acc gains}


\subsection{Impact of Non-IID levels}
Under different participation cases as shown in Table \ref{table:cifar100_all_baselines}, we further prove the robust generalization of the proposed algorithms. Heterogeneous data distribution of local clients is set to various participation levels from IID, Dirichlet 0.6, and Dirichlet 0.3, which makes the training of the global model more difficult. 
For instance, compared with DP-FedAvg on CIFAR-100, the test accuracy gain in DP-FedSAM is $\{2.78\%, 3.17 \%, 3.17\%\}$. Meanwhile, the test accuracy gain in DP-FedSAM-$\topk_k$ is $\{2.27\%, 3.17\%, 3.21\%\}$ compared with Fed-SMP-$\topk_k$.
%2.78 3.17 3.17
%2.27 3.17 3.21
%4.75 5.53 4.83
These observations confirm that our algorithms are more robust than baselines in various degrees of heterogeneous data.
%  \ls{double check the report acc numbers. Report the acc gains}
 

% \clearpage

\section{More details on Discussion for DP with SAM in FL}\label{exper_DP_appendix}
\begin{figure}
\centering
\begin{subfigure}{0.57\linewidth}
\centering
\includegraphics[width=1\textwidth]{figures/sam_landscape.pdf}
\caption{\small Loss landscape}
\end{subfigure}
% \quad
\hfill
\begin{subfigure}{0.41\linewidth}
\centering
\includegraphics[width=1\textwidth]{figures/contour_c_comparsion.pdf}
\caption{\small Loss surface contour}
\end{subfigure}
% \begin{subfigure}{0.45\linewidth}
% \centering
% \includegraphics[width=1\textwidth]{figures/norm1.pdf}
% \caption{\small Norm distribution and average norm of local updates.
% }
% \end{subfigure}
\vspace{-0.2cm}
\caption{\small Loss landscape and surface contour of DP-FedSAM. Compared with DP-FedAvg in the left of Figure \ref{landscape_fedavg_dpfedavg} (a) and (b) with the same setting, DP-FedSAM has a flatter landscape with both better generalization ability (flat minima, see Figure \ref{sam_land} (a)) and weight perturbation robustness (see Figure \ref{sam_land} (b)).
}
\vspace{-0.2cm}
\label{sam_land}
\end{figure}
% \noindent
% \textbf{The norm of local update.}
% To validate the theoretical results for mitigating the adverse impacts of the norm of local updates, we conduct experiments on DP-FedSAM and DP-FedAvg with clipping threshold $C=0.2$ as shown in Figure \ref{fig:norm}. We show the norm $\Delta_i^t$ distribution and average norm $\overline{\Delta}^t$ of local updates before clipping during the communication rounds. In contrast to DP-FedAvg, most of the norm is distributed at the smaller value in our scheme in Figure \ref{fig:norm} (a), which means that the clipping operation drops less information. Meanwhile, the on-average norm $\overline{\Delta}^t$ is smaller than DP-FedAvg as shown in Figure \ref{fig:norm} (b). These observations are also consistent with our theoretical results in section \ref{th}.



\noindent
\textbf{Loss landscape and contour.}
To visualize the sharpness of the flat minima and observe robustness to DP noise obtained by DP-FedSAM, we show the loss landscape and surface contour following by the plotting method \cite{li2018visualizing} in Figure \ref{sam_land}. It is clear that DP-FedSAM has flatter minima and better robustness to DP noise than DP-FedAvg in the left of Figure \ref{landscape_fedavg_dpfedavg} (a) and (b), respectively. It indicates that our proposed algorithm achieves better generalization and makes the training process more adaptive to the DPFL setting.

\section{Discussion for DP Guarantee in DP-FedSAM with Sparsification}\label{DP-sam-topk}
Sparsification is a very common method when considering privacy protection to introduce a large amount of random noise in FL \cite{hu2022federated, cheng2022differentially, Hu2021federated}. It retains only the larger weight part of each layer of the local model with a sparsity ratio of $k/d$ ($d$ is the weight scale), and the rest are sparse. The advantage is that the amount of random noise can be reduced (no noise needs to be added to the sparse weight position), so the performance can be improved, which has been thoroughly verified in \cite{hu2022federated, cheng2022differentially, Hu2021federated}. In our methods, SAM needs to perform two gradient calculations and sparsification may lead to some performance degradation because the model is compressed and some information may be lost. 

Existing work \cite{hu2022federated} has verified SGD and top-k sparsification satisfying the Renyi DP. SAM optimizer only adds perturbation on the basis of SGD and affects the model during training. And both SAM and top k sparsification are performed before the DP process, thereby satisfying the Renyi DP.
\section{Main Proof} \label{appendix_th}
\subsection{Preliminary Lemmas}
\begin{lemma}\label{e_delta}
(Lemma B.1, \cite{Qu2022Generalized}) Under Assumptions~\ref{a1}-\ref{a2}, the updates for any learning rate satisfying $\eta \leq \frac{1}{4KL}$ have the drift due to $\delta_{i,k} - \delta$:
\begin{equation}
    \frac{1}{M}\sum_{i}\mathbb{E} [\|\delta_{i,k} - \delta \|^2 ] \leq 2K^2 L^2 \eta^2 \rho^2 . \nonumber
\end{equation}
Where 
\begin{equation}
		\delta = \rho \frac{\nabla f(\mathbf{w}^t)}{\|\nabla f(\mathbf{w}^t)\|}, ~~~ \delta_{i,k} = \rho \frac{\nabla F_i (\mathbf{w}^{t,k} ,\xi_i )}{\|\nabla F_i (\mathbf{w}^{t,k}, \xi_i )\|}. \nonumber
\end{equation}
\end{lemma}
\begin{lemma}\label{e_w}
(lemma B.2, \cite{Qu2022Generalized}) Under above assumptions, the updates for any learning rate satisfying $\eta_l \leq \frac{1}{10KL}$ have the drift due to $\mathbf{w}^{t,k}(i)  - \mathbf{w}^t$:
\begin{equation}
\begin{split}
    \frac{1}{M}\sum_{i}\mathbb{E} [\|\mathbf{w}^{t,k}(i) - \mathbf{w}^t \|^2 ] 
    & \leq 5K\eta^2 \Big(2L^2 \rho^2 \sigma_l^2
    + 6K(3\sigma_g^2 + 6L^2 \rho^2 )  + 6K\|\nabla f(\mathbf{w}^t)\|^2 \Big) + 24K^3 \eta^4 L^4 \rho^2 . \nonumber
\end{split}
\end{equation}
% Proof. Under our assumption \ref{a2}, the local variance has the relationship of $ \sigma_l^{FedSAM}  \leq \sigma_l^{ours} $, where $\sigma_l^{FedSAM}$ is the left term of Lemma \ref{sigma_convert} in \cite{Qu2022Generalized}. Then we scale the inequality and generate the above proof result.
\end{lemma}
% \begin{lemma}\label{sigma_convert}
% Under Assumption \ref{a4}, we have
% \begin{equation}
% 	\mathbb{E}_{\xi_i}\left\|\frac{\nabla F_i (\mathbf{w},\xi_i )}{\|\nabla F_i (\mathbf{w},\xi_i )\|} - \frac{\nabla F_i (\mathbf{w})}{\|\nabla F_i (\mathbf{w})\|}\right\|^2 \leq \sigma_l^2.\nonumber
% \end{equation}
% \end{lemma}
% \begin{proof}
% Under Assumption \ref{a2} and \ref{a4}, we have
% \begin{equation}
% \begin{split}
%     & \mathbb{E}_{\xi_i}\|\frac{\nabla F_i (\mathbf{w},\xi_i )}{\|\nabla F_i (\mathbf{w},\xi_i )\|} - \frac{\nabla F_i (\mathbf{w})}{\|\nabla F_i (\mathbf{w})\|}\|^2 
% 	\leq \mathbb{E}_{\xi_i}\Big\|\|\nabla F_i (\mathbf{w},\xi_i ) \|\frac{\nabla F_i (\mathbf{w},\xi_i )}{\|\nabla F_i (\mathbf{w},\xi_i )\|}  - \|\nabla F_i (\mathbf{w})\| \frac{\nabla F_i (\mathbf{w})}{\|\nabla F_i (\mathbf{w})\|} \Big\|^2
% 	= \sigma_l^2.\nonumber
% \end{split}
% \end{equation}
% \end{proof}

\begin{lemma}\label{y_k-x_k}
The two model parameters conducted by two adjacent datasets which differ only one sample from client $i$ in the communication round $t$, 
\begin{equation}
    \sum_{k=0}^{K-1}\|\mathbf{y}^{t,k}(i)- \mathbf{x}^{t,k}(i) \|_2^2
    \leq 2K \max \|\Delta_i^t(\mathbf{y})-\Delta_i^t(\mathbf{x})\|_2^2 \nonumber.
\end{equation}
\end{lemma}
\begin{proof}
Recall the local update from client $i$ is $\sum_{k=0}^{K-1}\mathbf{w}^{t,k}(i) = \sum_{k=0}^{K-1}\mathbf{w}^{t,k-1}(i) + \Delta_i^t $, (the initial value is assumed as $ \mathbf{w}^{t, -1}= \mathbf{w}^{t, 0} =\mathbf{w}^{t}$). Then,
\begin{equation}
    \begin{split}
      & \sum_{k=0}^{K-1}\|\mathbf{y}^{t,k}(i)- \mathbf{x}^{t,k}(i) \|_2^2
    \leq 2 \sum_{k=0}^{K-1}\|\mathbf{y}^{t,k-1}(i)- \mathbf{x}^{t,k-1}(i) \|_2^2\\
    &+ 2 \|\Delta_i^t(\mathbf{y})-\Delta_i^t(\mathbf{x})\|_2^2. \nonumber
    \end{split}
\end{equation}
The recursion from $\tau=0$ to $k$ yields
\begin{equation}
    \sum_{k=0}^{K-1}\|\mathbf{y}^{t,k}(i)- \mathbf{x}^{t,k}(i) \|_2^2
    \overset{a)}{\leq} 2K \max \|\Delta_i^t(\mathbf{y})-\Delta_i^t(\mathbf{x})\|_2^2 \nonumber.
\end{equation}
Where a) uses the initial value $\mathbf{w}^t(i)=\mathbf{x}^{t, 0}(i) =\mathbf{y}^{t, 0}(i)$ and $0<k \leq K$.
\end{proof}

\begin{lemma}\label{Delta_average}
Under assumption \ref{a1} and \ref{a3},
the average of local update after the clipping operation from selected clients is 
\begin{equation}
    \mathbb{E}\|\frac{1}{m}\sum_{i\in \mathcal{W}^t}\tilde{\Delta}_i^t\|^2 \leq 3K\eta^2(L^2\rho^2+B^2) \nonumber
\end{equation}
\end{lemma}
\begin{proof}
\begin{equation}
\begin{split}
    \mathbb{E}\|\frac{1}{m}\sum_{i\in \mathcal{W}^t}\tilde{\Delta}_i^t\|^2 
    & \leq \mathbb{E} \|\frac{1}{m}\sum_{i\in \mathcal{W}^t}\sum_{i=0}^{K-1}\eta \tilde{\mathbf{g}}^{t,k}(i) \cdot \alpha_i^t\|^2  \leq \frac{\eta^2}{m} \sum_{i\in \mathcal{W}^t}\sum_{i=0}^{K-1}\mathbb{E} \| \nabla F_i(\mathbf{w}^{t,k}(i) +\delta; \xi_i) - \nabla F_i(\mathbf{w}^{t,K}(i); \xi_i)\\
    & + \nabla F_i(\mathbf{w}^{t,k}(i); \xi_i) - \nabla F_i(\mathbf{w}^{t}(i)) + \nabla F_i(\mathbf{w}^{t}(i))\|^2\\
    & \overset{a)}{\leq} 3K\eta^2(L^2\rho^2+B^2) ,\nonumber
\end{split}
\end{equation}
where a) uses assumption \ref{a1} and \ref{a3} and 
\begin{equation}
    \alpha_i^t := \min \Big (1, \frac{C}{\eta \|\sum_{k=0}^{K-1}\tilde{\mathbf{g}}^{t,k}(i)\|} \Big).\nonumber
\end{equation}
\end{proof}

% \subsection{Proof of the norm of local update}\label{pr:norm}
% \emph{Proof.} Recall that the local update before clipping and adding noise on client $i$ is $\Delta_i^t = \mathbf{w}^{t,K}(i)-\mathbf{w}^{t,0}(i)$. Then,
% \begin{equation}
%     \begin{split}
%     & \mathbb{E}\|\Delta_i^t\|_2^2  = \eta^2\mathbb{E} \|\sum_{k=0}^{K-1}\tilde{\mathbf{g}}^{t,k}(i)\|_2^2 \\
%     & = \eta^2 \mathbb{E} \sum_{k=0}^{K-1} \| \nabla F_i(\mathbf{w}^{t,k}(i)+\delta;\xi_i)-\nabla F_i(\mathbf{w}^{t}) \\
%     & +\nabla F_i(\mathbf{w}^{t}) - \nabla f(\mathbf{w}^{t}) + \nabla f(\mathbf{w}^{t})\|_2^2\\
%     & \overset{a)}\leq  3\eta^2K(L^2\rho^2+\beta^2+B^2),
% \end{split}
% \end{equation}

% where (a) is from Assumptions \ref{a1}-\ref{a3}. After that, we also present the local update norm with SGD in FL for comparison.\\
% \emph{Proof of local update norm with SGD in FL.}
% \begin{equation}
%     \begin{split}
%     & \mathbb{E}\|\Delta_{i, SGD}^t\|_2^2  = \eta^2\|\sum_{k=0}^{K-1}\mathbf{g}^{t,k}(i)\|_2^2 \\
%     & = \eta^2 \sum_{k=0}^{K-1} \| \nabla F_i(\mathbf{w}^{t,k}(i);\xi_i)-\nabla F_i(\mathbf{w}^{t}) \\
%     & +\nabla F_i(\mathbf{w}^{t}) - \nabla f(\mathbf{w}^{t}) + \nabla f(\mathbf{w}^{t})\|_2^2\\
%     & \overset{a)}\leq  3\eta^2K(\sigma_l^2+\beta^2+B^2).
% \end{split}
% \end{equation}
\subsection{Proof of Sensitivity Analysis}
\begin{proof}[Proof of Theorem \ref{th:sensitivity}]
% \ls{use this format.}
% Recall that the local iteration update on client $i$ is $\mathbf{w}^{t,}$\\
Recall that the local update before clipping and adding noise on client $i$ is $\Delta_i^t = \mathbf{w}^{t,K}(i)-\mathbf{w}^{t,0}(i)$. Then,
\begin{equation}
 \begin{split}
    \mathbb{E} \mathcal{S}_{\Delta_i^t}^2 & = \max  \mathbb{E} \| \Delta_i^t(\mathbf{x}) - \Delta_i^t(\mathbf{y})\|_2^2\\
    & = \mathbb{E} \| \mathbf{x}^{t,K}(i) - \mathbf{x}^{t,0}(i) - (\mathbf{y}^{t,K}(i) - \mathbf{y}^{t,0}(i))\|_2^2\\
    & =  \eta^2\mathbb{E} \sum_{k=0}^{K-1}\|\nabla F_i(\mathbf{x}^{t,k}(i)+\delta_x; \xi_i) -\nabla F_i(\mathbf{y}^{t,k}(i)+\delta_y; \xi_i^{'})\|_2^2 \\
    & =  \eta^2L^2 \mathbb{E} \sum_{k=0}^{K-1}\|\mathbf{y}^{t,k}(i)- \mathbf{x}^{t,k}(i) + (\delta_y-\delta_x)\|_2^2  \\
    & \overset{a)}{\leq} 2\eta^2L^2 K \max \|\Delta_i^t(\mathbf{y})-\Delta_i^t(\mathbf{x})\|_2^2 
    + 2\eta^2L^2\rho^2 \mathbb{E} \sum_{k=0}^{K-1} 
    \Big \|\frac{\nabla
    F_i(\mathbf{y}^{t,k}(i)+\delta_y; \xi_i^{'})}{\|\nabla
    F_i(\mathbf{y}^{t,k}(i)+\delta_y; \xi_i^{'})\|_2}- \frac{\nabla
    F_i(\mathbf{y}^{t,k}(i); \xi_i^{'})}{\|\nabla
    F_i(\mathbf{y}^{t,k}(i); \xi_i^{'})\|_2}\\
    & + ( \frac{\nabla
    F_i(\mathbf{x}^{t,k}(i); \xi_i)}{\|\nabla
    F_i(\mathbf{x}^{t}(i,k); \xi_i)\|_2} - \frac{\nabla
    F_i(\mathbf{x}^{t,k}(i)+\delta_x; \xi_i)}{\|\nabla
    F_i(\mathbf{x}^{t,k}(i)+\delta_x; \xi_i)\|_2})
     + \frac{\nabla
    F_i(\mathbf{y}^{t,k}(i); \xi_i^{'}))}{\|\nabla
    F_i(\mathbf{y}^{t,k}(i);\xi_i^{'}))\|_2} - \frac{\nabla
    F_i(\mathbf{x}^{t,k}(i);\xi_i)}{\|\nabla
    F_i(\mathbf{x}^{t,k}(i); \xi_i)\|_2}\Big \|_2^2\\
    & \leq 2\eta^2L^2K \max \|\Delta_i^t(\mathbf{y})-\Delta_i^t(\mathbf{x})\|_2^2 +
     6\eta^2\rho^2L^2 \mathbb{E} \sum_{k=0}^{K-1}\Big(4  + \frac{1}{\rho^2}\Big\|\rho \frac{\nabla
    F_i(\mathbf{y}^{t,k}(i); \xi_i^{'}))}{\|\nabla
    F_i(\mathbf{y}^{t,k}(i);\xi_i^{'}))\|_2} - \rho \frac{\nabla
    f(\mathbf{y}^{t})}{\|\nabla
    f(\mathbf{y}^{t})\|_2} \\
    &+ (  \rho \frac{\nabla
    f(\mathbf{x}^{t})}{\|\nabla
    f(\mathbf{x}^{t})\|_2} - \rho \frac{\nabla
    F_i(\mathbf{x}^{t,k}(i);\xi_i)}{\|\nabla
    F_i(\mathbf{x}^{t,k}(i); \xi_i)\|_2}) 
    + \rho \frac{\nabla
    f(\mathbf{y}^{t})}{\|\nabla
    f(\mathbf{y}^{t})\|_2} - \rho \frac{\nabla
    f(\mathbf{x}^{t})}{\|\nabla
    f(\mathbf{x}^{t})\|_2}\Big\|_2^2 \Big )\\
    & \overset{b)}{\leq}
    2\eta^2L^2 K \mathcal{S}_{\Delta_i^t}^2 + 6\eta^2\rho^2KL^2(4  + 12K^2L^2\eta^2+ 6) \\
    & \leq \frac{6\eta^2\rho^2KL^2(12K^2L^2\eta^2+ 10)}{1-2\eta^2L^2 K}
\end{split}   
\end{equation}
where a) and b)  uses lemma \ref{y_k-x_k} and \ref{e_delta}, respectively. 

When the local adaptive learning rate satisfies $\eta=\mathcal{O}({1}/{L\sqrt{KT}})$ and the perturbation amplitude $\rho$
proportional to the learning rate, e.g., $\rho = \mathcal{O}(\frac{1}{\sqrt{T}})$, we have
\begin{align}
\small
    \mathbb{E}\mathcal{S}^2_{\Delta_i^t} \leq
    \mathcal{O}\left(\frac{1}{T^2}\right). 
\end{align}
\end{proof}

For comparison, we also present the expected squared sensitivity of local update with SGD in DPFL as follows. It is clearly seen that the upper bound in $  \mathbb{E}\mathcal{S}^2_{\Delta_i^t, SAM}$ is tighter than that in $\mathbb{E}\mathcal{S}^2_{\Delta_i^t, SGD}$.
% From the perspective of privacy protection, it means DP-FedSAM has a better privacy guarantee than DP-FedAvg.
% \ls{Not SAM. It is DP-SAM better than DP-SGD. Should we mention that the DP-SAM is of independent interest.}.
% Another perspective from local iteration, which means both better model consistency among clients and training stability.

% For comparison, we also present the expected squared sensitivity of local update with SGD in DPFL, that is $ \mathbb{E}\mathcal{S}^2_{\Delta_i^t, SGD} \leq \frac{6\eta^2\sigma_l^2K}{1-3\eta^2KL^2}$.
% % \begin{equation} 
% % \small
% %     \mathbb{E}\mathcal{S}^2_{\Delta_i^t, SGD} \leq \frac{6\eta^2\sigma_l^2K}{1-3\eta^2KL^2}.
% % \end{equation}
% Thus $\mathbb{E}\mathcal{S}^2_{\Delta_i^t, SGD} \leq \mathcal{O}(\frac{\sigma_l^2}{KL^2T})$ when $\eta=\mathcal{O}({1}/{L\sqrt{KT}})$.

\begin{proof}[Proof of sensitivity with SGD in FL.]
% \ls{use this format.}
\begin{equation}
    \begin{split}
         \mathbb{E}  \mathcal{S}_{\Delta_i^t, SGD}^2 & = \max  \mathbb{E}  \| \Delta_i^t(\mathbf{x}) - \Delta_i^t(\mathbf{y})\|_2^2
     = \eta^2 \mathbb{E} \sum_{i=0}^{K-1}\|\nabla F_i( \mathbf{x}^{t,k}(i); \xi_i) - \nabla F_i( \mathbf{y}^{t,k}(i);\xi_i^{'})\|_2^2\\
    & = \eta^2 \mathbb{E} \sum_{i=0}^{K-1}\|\nabla F_i( \mathbf{x}^{t,k}(i); \xi_i) -\nabla F_i( \mathbf{x}^{t}(i)) + \nabla F_i( \mathbf{x}^{t}(i)) 
    - \nabla F_i( \mathbf{y}^{t}(i))  +\nabla F_i( \mathbf{y}^{t}(i)) -
    \nabla F_i( \mathbf{y}^{t,k}(i);\xi_i^{'})\|_2^2\\
    & \overset{a)}{\leq}
    3\eta^2 \mathbb{E} \sum_{i=0}^{K-1}(2\sigma_l^2+L^2\|y^{t,k}(i)-x^{t,k}(i)\|_2^2)\\
    & \overset{b)}{\leq} 6\eta^2K\sigma_l^2+3\eta^2L^2K  \max  \mathbb{E} \| \Delta_i^t(\mathbf{x}) - \Delta_i^t(\mathbf{y})\|_2^2\\
    & \leq \frac{6\eta^2\sigma_l^2K}{1-3\eta^2KL^2}.
    \end{split}
\end{equation}
Where a) and b) uses assumptions \ref{a1}-\ref{a2} and lemma \ref{y_k-x_k}, respectively. Thus $\mathbb{E}\mathcal{S}^2_{\Delta_i^t, SGD} \leq \mathcal{O}(\frac{\sigma_l^2}{KL^2T})$ when $\eta=\mathcal{O}({1}/{L\sqrt{KT}})$.
\end{proof}

\subsection{Proof of Convergence Analysis}
\begin{proof}[Proof of Theorem \ref{th:conver}]
We define the following notations for convenience:
\begin{equation}
    \begin{split}
        & \tilde{\Delta}_i^t = -\eta\sum_{k=0}^{K-1}\tilde{\mathbf{g}}^{t,k}(i) \cdot \alpha_i^t;\\
        & \overline{\Delta_i^t} = -\eta\sum_{k=0}^{K-1}\tilde{\mathbf{g}}^{t,k}(i) \cdot \overline{\alpha}^t, \nonumber
    \end{split}
\end{equation}
where 
\begin{equation}
    \begin{split}
        & \alpha_i^t := \min \Big (1, \frac{C}{\eta \|\sum_{k=0}^{K-1}\tilde{\mathbf{g}}^{t,k}(i)\|} \Big), \\
        & \overline{\alpha}^t := \frac{1}{M}\sum_{i=1}^{M} \alpha_i^t,\\
        & \tilde{\alpha}^t :=\frac{1}{M}\sum_{i=1}^{M} |\alpha_i^t - \overline{\alpha}^t|.
    \end{split} \nonumber
\end{equation}
The Lipschitz continuity of $\nabla f$:
\begin{equation}
    \begin{split}
        & \mathbb{E} f(\mathbf{w}^{t+1}) \\
        & \leq \mathbb{E} f(\mathbf{w}^t) + \mathbb{E} \Big \langle\nabla f(\mathbf{w}^{t}), \mathbf{w}^{t+1}-\mathbf{w}^t \Big \rangle
        + \mathbb{E} \frac{L}{2}\|\mathbf{w}^{t+1}-\mathbf{w}^t\|^2\\
        & = \mathbb{E} f(\mathbf{w}^t) + \mathbb{E}\Big  \langle \nabla f(\mathbf{w}^{t}), \frac{1}{m}\sum_{i\in \mathcal{W}^t}\tilde{\Delta}_i^t + z_i^t \Big \rangle
         + \frac{L}{2}\mathbb{E}\Big  \|\frac{1}{m}\sum_{i\in \mathcal{W}^t}\tilde{\Delta}_i^t + z_i^t \Big \|^2\\
        & = \mathbb{E} f(\mathbf{w}^t) + 
         \underbrace{\Big \langle \nabla f(\mathbf{w}^{t}), \mathbb{E} \frac{1}{m}\sum_{i\in \mathcal{W}^t}\tilde{\Delta}_i^t \Big \rangle}_{\text{I}}
        + 
        \frac{L}{2} \mathbb{E} \underbrace{\Big \langle \|\frac{1}{m}\sum_{i\in \mathcal{W}^t}\tilde{\Delta}_i^t\|^2\Big \rangle}_{\text{II}} + \frac{L\sigma^2C^2d}{2m^2} ,
    \end{split}
\end{equation}
where $d$ represents dimension of $\mathbf{w}_i^{t,k}$ and the mean of noise $z_i^t$ is zero. Then, we analyze I and II, respectively.\\
For I, we have
\begin{equation}
    \begin{split}
        & \Big \langle\nabla f(\mathbf{w}^{t}), \mathbb{E} \frac{1}{m}\sum_{i\in \mathcal{W}^t}\tilde{\Delta}_i^t \Big  \rangle = \Big \langle\nabla f(\mathbf{w}^{t}), \mathbb{E}\frac{1}{M}\sum_{i=1}^M\tilde{\Delta}_i^t-\overline{\Delta}_i^t  \Big \rangle
        + \Big \langle\nabla f(\mathbf{w}^{t}), \mathbb{E}\frac{1}{M}\sum_{i=1}^M \overline{\Delta}_i^t  \Big \rangle.
    \end{split}
\end{equation}
Then we bound the two terms in the above equality, respectively. For the first term, we have
\begin{equation}
    \begin{split}
        &\mathbb{E} \Big \langle\nabla f(\mathbf{w}^{t}), \mathbb{E}\frac{1}{M}\sum_{i=1}^M\tilde{\Delta}_i^t-\overline{\Delta}_i^t  \Big \rangle\\
        & \leq\mathbb{E} \Big \langle\nabla f(\mathbf{w}^{t}), \mathbb{E}\frac{1}{M}\sum_{i=1}^M \sum_{k=0}^{K-1}\eta |\alpha_i^t - \overline{\alpha}^t|\tilde{\mathbf{g}}^{t,k}(i)\Big \rangle\\
        & \leq \frac{\eta K}{M}\sum_{i=1}^{M}\mathbb{E}|\alpha_i^t - \overline{\alpha}^t| \Big \langle \nabla F_i(\mathbf{w}^{t}),\tilde{\mathbf{g}}^{t,k}(i) \Big \rangle\\
        & \overset{a)}{\leq} \frac{\eta K}{M}\sum_{i=1}^{M}\mathbb{E}|\alpha_i^t - \overline{\alpha}^t| \Big(-\frac{1}{2}(\|\nabla F_i(\mathbf{w}^{t,k})\|^2+\|F_i(\mathbf{w}^{t,k}+\delta; \xi_i)\|^2)
        + \frac{1}{2} \|\nabla F_i(\mathbf{w}^{t,k}+\delta; \xi_i) - \nabla F_i(\mathbf{w}^{t,k}; \xi_i)\|^2\Big)\\
        & \overset{b)}{\leq}\eta \tilde{\alpha}^t K(\frac{1}{2}L^2\rho^2-B^2),
    \end{split}
\end{equation}
where $\tilde{\alpha}^t =\frac{1}{M}\sum_{i=1}^{M} |\alpha_i^t - \overline{\alpha}^t|$, a) uses $\langle a,b \rangle = -\frac{1}{2}\|a\|^2-\frac{1}{2}\|b\|^2 + \frac{1}{2}\|a - b\|^2$ and b) bases on assumption \ref{a1},\ref{a3}.\\
For the second term, we have
\begin{equation}
    \begin{split}
         & \Big \langle\nabla f(\mathbf{w}^{t}), \mathbb{E}\frac{1}{M}\sum_{i=1}^M \overline{\Delta}_i^t  \Big \rangle\\
         & \overset{a)}{\leq} \frac{- \overline{\alpha}^t\eta K}{2}\|\nabla f(\mathbf{w}^{t})\|^2 - \frac{\overline{\alpha}^t}{2K} \mathbb{E}\Big \|\frac{1}{\overline{\alpha}^t M}\sum_{i=1}^M \overline{\Delta}_i^t \Big \|^2
          + \frac{ \overline{\alpha}^t}{2}
         \underbrace{\mathbb{E}\Big \|\sqrt{K}\nabla f(\mathbf{w}^{t})- \frac{1}{ \overline{\alpha}^tM\sqrt{K}}\sum_{i=1}^M \overline{\Delta}_i^t \Big \|^2}_{\text{III}},
    \end{split}
\end{equation}
where a) uses $\langle a,b \rangle = -\frac{1}{2}\|a\|^2-\frac{1}{2}\|b\|^2 + \frac{1}{2}\|a - b\|^2$ and $0< \eta <1 $. Next, we bound III as follows:
\begin{equation}
    \begin{split}
         \text{III} &= K\mathbb{E}\Big \| \nabla f(\mathbf{w}^{t}) + \frac{1}{MK}\sum_{i=1}^M\sum_{k=0}^{K-1} \nabla \eta F_i(\mathbf{w}^{t,k}+\delta; \xi_i) \Big\|^2\\
        & \leq \frac{1}{M}\sum_{i=1}^M\sum_{k=0}^{K-1} \mathbb{E}\Big \| \eta (F_i(\mathbf{w}^{t,k}+\delta; \xi_i) - \nabla F_i(\mathbf{w}^{t,k}; \xi_i)) 
        + \eta (\nabla F_i(\mathbf{w}^{t,k}; \xi_i) - \nabla F_i(\mathbf{w}^{t})) + (1+\eta) \nabla F_i(\mathbf{w}^{t})\Big \| ^2\\
        & \overset{a)}{\leq} 3K\eta^2L^2 \Big( \rho^2 + \mathbb{E} \|\mathbf{w}^{t,k} - \mathbf{w}^{t}\|^2 + 2B^2 \Big)\\
        & \overset{b)}{\leq} 3K\eta^2L^2 \Big[ \rho^2 +  5K\eta^2 \Big(2L^2 \rho^2 \sigma_l^2+ 6K(3\sigma_g^2 + 6L^2 \rho^2 ) 
        + 6K\|\nabla f(\mathbf{w}^t)\|^2 \Big) + 24K^3 \eta^4 L^4 \rho^2 + B^2\Big],
    \end{split}
\end{equation}
where $0< \eta <1$, a) and b uses assumption \ref{a1}, \ref{a3} and lemma \ref{e_w}, respectively.\\
For II, we uses lemma \ref{Delta_average}. Then, combining Eq. 12-16, we have
\begin{equation}
    \begin{split}
         \mathbb{E} f(\mathbf{w}^{t+1}) 
        & \leq \mathbb{E} f(\mathbf{w}^t) + \eta \tilde{\alpha}_t K(\frac{1}{2}L^2\rho^2-B^2)
        - \frac{\overline{\alpha}^t\eta K}{2}\|\nabla f(\mathbf{w}^{t})\|^2 -
         \frac{\eta \overline{\alpha}^t}{2K} \mathbb{E}\Big \|\frac{1}{\eta\overline{\alpha}^t M}\sum_{i=1}^M \overline{\Delta}_i^t \Big \|^2 \\
        & + \frac{3\overline{\alpha}^t\eta^2L^2K}{2}\Big[ \rho^2 +  5K\eta^2 \Big(2L^2 \rho^2 \sigma_l^2+ 6K(3\sigma_g^2 + 6L^2 \rho^2 )  + 6K\|\nabla f(\mathbf{w}^t)\|^2 \Big)\\
        &  + 24K^3 \eta^4 L^4 \rho^2 + B^2\Big ]
         + \frac{3\eta^2KL(L^2\rho^2+B^2)}{2} + \frac{L\sigma^2C^2d}{2m^2}.
    \end{split}
\end{equation}
When $\eta \leq \frac{1}{3\sqrt{KL}}$, the inequality is 
\begin{equation}
    \begin{split}
         \mathbb{E} f(\mathbf{w}^{t+1}) 
         & \leq \mathbb{E} f(\mathbf{w}^t) - \frac{ \overline{\alpha}^t \eta K}{2}\mathbb{E} \|\nabla f(\mathbf{w}^t)\|^2 + \frac{\tilde{\alpha}^t \eta KL^2\rho^2}{2}  + \frac{3\overline{\alpha}^t\eta^2 KL^2\rho^2}{2}
        -\tilde{\alpha}^t \eta KB^2 \\
        & + \frac{15\overline{\alpha}^t K\eta^4L^2}{2} \Big(2L^2 \rho^2 \sigma_l^2 + 6K(3\sigma_g^2 + 6L^2 \rho^2 )  + 6K\|\nabla f(\mathbf{w}^t)\|^2 \Big) + 36\eta^6K^4L^6\rho^2 \\
        & + \frac{3\eta^2KL(L^2 \rho^2+B^2)}{2} + \frac{L\sigma^2C^2d}{2m^2}.
    \end{split}
\end{equation}
Sum over $t$ from $1$ to $T$, we have
\begin{equation}
    \begin{split}
         \frac{1}{T}\sum_{t=1}^T \mathbb{E} \Big[\overline{\alpha}^t\|f(\mathbf{w}^t)\|^2\Big] & \leq \frac{2L(f({\bf w}^{1})-f^{*})}{\sqrt{KT}} + \frac{1}{T}\sum_{t=1}^T \tilde{\alpha}^t  L^2\rho^2  - 2 \tilde{\alpha}^t  B^2 + 30\eta^2 L^2\frac{1}{T}\sum_{t=1}^T \overline{\alpha}^t \Big(2L^2 \rho^2 \sigma_l^2 + 6K(3\sigma_g^2 + 6L^2 \rho^2 ) \Big)\\
        & + 72\eta^4K^3L^6\rho^2+3\eta L(L^2 \rho^2+B^2)  + \frac{L\sigma^2C^2d}{\eta m^2K}
    \end{split}
\end{equation}
Assume the local adaptive learning rate satisfies $\eta=\mathcal{O}({1}/{L\sqrt{KT}})$, both $\frac{1}{T}\sum_{t=1}^T \tilde{\alpha}^t $ and $\frac{1}{T}\sum_{t=1}^T \overline{\alpha}^t $ are two important parameters for measuring the impact of clipping. Meanwhile, both $\frac{1}{T}\sum_{t=1}^T \tilde{\alpha}^t $ and $\frac{1}{T}\sum_{t=1}^T \overline{\alpha}^t $ are also bounded by $1$. Then, our result is
\begin{equation}
% \small
    \begin{split}
            & \frac{1}{T} \sum_{t=1}^T
    \mathbb{E}\left[\overline{\alpha}^{t}\left\|\nabla f\left(\mathbf{w}^{t}\right)\right\|^{2}\right]   \leq  
    \underbrace{\mathcal{O}\left(\frac{2L(f({\bf w}^{1})-f^{*})}{\sqrt{KT}} + \frac{\sigma_{l}^2 L^2\rho}{KT}\right)}_{\text{From FedSAM}}   +
    \underbrace{
     \underbrace{\mathcal{O}\left(\sum_{t=1}^T( \frac{\overline{\alpha}^t  \sigma_{g}^2 }{T^2} + \frac{\tilde{\alpha}^t  L^2\rho^2 }{T} ) \right)}_{\text{Clipping}}
    + \underbrace{ \mathcal{O}\left(\frac{L^2 \sqrt{T}\sigma^2C^2d}{m^2\sqrt{K}} \right)}_{\text{Adding noise}}
    }_{\text{From operations for DP}} . 
    \end{split}
\end{equation}
Assume the perturbation amplitude $\rho$
proportional to the learning rate, e.g., $\rho = \mathcal{O}(\frac{1}{\sqrt{T}})$, we have
\begin{equation}
    \begin{split}
            & \frac{1}{T} \sum_{t=1}^T
    \mathbb{E}\left[\overline{\alpha}^{t}\left\|\nabla f\left(\mathbf{w}^{t}\right)\right\|^{2}\right]   \leq  
    \underbrace{\mathcal{O}\left(\frac{2L(f({\bf w}^{1})-f^{*})}{\sqrt{KT}} + \frac{ L^2\sigma_{l}^2}{KT^2}\right)}_{\text{From FedSAM}}   +
    \underbrace{
     \underbrace{\mathcal{O}\left(\sum_{t=1}^T( \frac{\overline{\alpha}^t  \sigma_{g}^2 }{T^2} + \frac{\tilde{\alpha}^t  L^2 }{T^2} ) \right)}_{\text{Clipping}}
    + \underbrace{ \mathcal{O}\left(\frac{L^2 \sqrt{T}\sigma^2C^2d}{m^2\sqrt{K}} \right)}_{\text{Adding noise}}
    }_{\text{From operations for DP}} . 
    \end{split}
\end{equation}
\end{proof}



