\section{Conclusion}
In this paper, we focus on severe performance degradation issue caused by dropped model information and exacerbated model inconsistency. And we are the first to alleviate this issue from the optimizer perspective and propose a novel and effective framework DP-FedSAM with a flatter loss landscape. Meanwhile, we present the analysis in detail of how SAM mitigates the adverse impacts of DP and achieve a 
tighter bound on convergence. 
Moreover, it is the first analysis to combine the impacts of the on-average norm of local updates and local update consistency among clients on training, and simultaneously provide the experimental observations.
Finally, empirical results also verify the superiority of our approach on several real-world data.

\vspace{0.1cm}

\noindent
\textbf{Acknowledgement} 
This work is supported by STI 2030â€”Major Projects (No. 2021ZD0201405), Shenzhen Philosophy and Social Science Foundation (Grant No. SZ2021B005).
