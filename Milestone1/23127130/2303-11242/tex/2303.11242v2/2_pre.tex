\section{Preliminary}

In this section, we first give the problem setup of FL, and then introduce the several terminologies in DP. 

\subsection{Federated Learning}
Consider a general FL system consisting of $M$ clients, in which each client owns its local dataset.
Let $\mathcal D^{\text{train}}_i$, $\mathcal D^{\text{val}}_i$ and $\mathcal D^{\text{test}}_i$ denote the training dataset, validation dataset and testing dataset, held by client $i$, respectively, where $i\in \mathcal{U} = \{1, 2,\ldots, M\}$.
Formally, the FL task is expressed as:
\begin{equation}
\small
\mathbf{w}^{\star} = \mathop{\arg\min}_{\mathbf{w}}\sum_{i\in \mathcal{U}}p_{i}f_i(\mathbf{w}, \mathcal D^{\text{train}}_{i}),
\end{equation}
where $p_{i} = \vert \mathcal D^{\text{train}}_i\vert/\vert \mathcal D^{\text{train}}\vert\geq 0$ with $\sum_{i\in \mathcal{U}}{p_{i}}=1$, and $f_i(\cdot)$ is the local loss function with $f_i(\mathbf{w}) = F_i(\mathbf{w}; \xi_i)$, $\xi_i$ is a batch sample data in client $i$. $\vert \mathcal D^{\text{train}}_{i}\vert $ is the size of training dataset $\mathcal D^{\text{train}}_i$ and $\vert \mathcal D^{\text{train}}\vert = \sum_{i\in \mathcal{U}}{\vert \mathcal D_{i}^{\text{train}}\vert}$ is the total size of training datasets, respectively.
For the $i$-th client, a local model is learned on its private training data $\mathcal D^{\text{train}}$ via:
\begin{equation}
\small
\mathbf{w}_{i}=\mathbf{w}_{i}^{t}-\eta \nabla f_i(\mathbf{w}_{i}, \mathcal D^{\text{train}}_{i}).
\end{equation}
Generally, the local loss function $f_i(\cdot)$ has the same expression across each client.
Then, the $M$ associated clients collaboratively learn a global model $\mathbf{w}$ over the heterogeneous training data $\mathcal D^{\text{train}}_{i}$, $\forall i \in \mathcal{U}$. 


\subsection{Differential Privacy}
Differential Privacy (DP) \cite{Dwork2014the} is a rigorous privacy notion for measuring privacy risk. In this paper, we consider a relaxed version: Rényi DP (RDP)~\cite{mironov2017renyi}.
% \begin{definition}
% ($(\epsilon, \delta)$-DP, \cite{Dwork2014the}). Given privacy parameters $\epsilon > 0$ and $0 \le \delta < 1$, a randomized mechanism $\mathcal{M}$ satisfies $(\epsilon, \delta)$-DP if for any pair of adjacent datasets $\mathcal D$, $\mathcal D^{\prime}$ , and any subset of outputs $O \subseteq range(\mathcal{M})$:
% \begin{equation}
% \operatorname{Pr}[\mathcal{M}(\mathcal D) \in O] \leq e^{\epsilon} \operatorname{Pr}\left[\mathcal{M}\left(\mathcal D^{\prime}\right) \in O\right]+\delta,
% \end{equation}
% where adjacent datasets are constructed by adding or removing any record. In addition,  $(\epsilon, \delta)$-DP is called $\epsilon$-DP, or pure DP when $\delta = 0$.
% \end{definition}
\begin{definition}
(Rényi DP, \cite{mironov2017renyi}). Given a real number $\alpha \in (1, \infty )$ and privacy parameter $\rho \ge 0$, a randomized mechanism $\mathcal{M}$ satisfies $(\alpha, \rho)$-RDP if for any two neighboring datasets $\mathcal D$, $\mathcal D'$ that differ in a single record, the Rényi $\alpha$-divergence between $\mathcal{M}(\mathcal D)$ and $\mathcal{M}(\mathcal D^{\prime})$ satisfies:
\begin{equation}
\small
\!\!\!D_{\alpha}\left[\mathcal{M}(\mathcal D) \| \mathcal{M}\left(\mathcal D^{\prime}\right)\right]\!:=\!\frac{1}{\alpha\!-\!1} \log \mathbb{E}\left[\left(\frac{\mathcal{M}(\mathcal D)}{\mathcal{M}\left(\mathcal D^{\prime}\right)}\right)^{\alpha}\!\right] \!\leq \!\rho,
\end{equation}
where the expectation is taken over the output of $\mathcal{M}(\mathcal D^{\prime})$.
\end{definition}
Rényi DP is a useful analytical tool to measure the privacy and accurately represent guarantees on the tails of the privacy loss, which is strictly stronger than $(\epsilon, \delta)$-DP for $\delta > 0 $. Thus, we provide the privacy analysis based on this tool for each user's privacy loss.
% There are wide range of mechanisms to quantify data privacy such as Laplace Mechanism and Gaussian Mechanism. The privacy loss random variable after composing multiple Gaussian Mechanisms also follows a Gaussian distribution, and this nice featuremakes composition analysis in the training of deep models more friendly. As a result, given the same
% privacy budget, we can achieve tighter privacy guarantee by composing multiple Gaussian Mechanisms in deep learning

\begin{definition}
\text{\rm ($l_2$ Sensitivity).}\label{sensitivity}
\cite[Definition 2]{cheng2022differentially}
Let $\mathcal{F}$ be a function, the $L_{2}$-sensitivity of $\mathcal{F}$ is defined as $\mathcal{S}=\max _{D \simeq D^{\prime} } \| \mathcal{F}\left(D\right)-$ $\mathcal{F}\left(D^{\prime}\right) \|_{2}$, where the maximization is taken over all pairs of adjacent datasets.
\end{definition}

The sensitivity of a function $\mathcal{F}$ captures the magnitude which a single individual’s data can change the function $\mathcal{F}$ in the worst case.
Therefore, it plays a crucial role in determining the magnitude of noise required to ensure DP.

\begin{definition}(Client-level DP) \cite[Definition 1]{mcmahan2017learning}. 
 A randomized algorithm $\mathcal{M}$ is $(\epsilon, \delta)$-DP if for any two adjacent datasets $U$, $U^{\prime}$ constructed by adding or removing all records of any client, and every possible subset of outputs $O$ satisfy the following inequality:
\begin{equation}
\operatorname{Pr}[\mathcal{M}(U) \in O] \leq e^{\epsilon} \operatorname{Pr}\left[\mathcal{M}\left(U^{\prime}\right) \in O\right]+\delta.
\end{equation}
\end{definition}

In client-level DP, we aim to ensure participation information for any clients. Therefore, we need to make local updates similar whether one client participates or not.

% In the client-level DP, we aim to ensure the participation information of any client. Thus, we need to make the local update similar whatever any client participates in or not.

% \ls{at the end of each definition, several discussion are needed. Why do we introduce these definitions.}