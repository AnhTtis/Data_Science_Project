% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}
% \usepackage[accsupp]{axessibility} 
%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
% \usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{appendix}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{xcolor,colortbl}
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.
% \usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage[linesnumbered,ruled]{algorithm2e}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


% \theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
% \theoremstyle{remark}
\newtheorem{remark}{Remark}

\DeclareMathOperator{\lap}{Lap} 
\DeclareMathOperator{\Clip}{Clip} 
\DeclareMathOperator{\topk}{top} 
\DeclareMathOperator{\blurs}{blurs} 
\DeclareMathOperator{\blur}{blur} 
\DeclareMathOperator{\lus}{lus} 
\DeclareMathOperator{\randk}{rand} 
\DeclareMathOperator{\spar}{spar}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{4552} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\newcommand{\ls}[1]{{\color{red}{\bf\sf [LS: #1]}}}
\newcommand{\syf}[1]{{\color{blue}{\bf\sf [SYF: #1]}}}

\begin{document}


%%%%%%%%% TITLE - PLEASE UPDATE
\title{Make Landscape Flatter in Differentially Private Federated Learning}

\author{
Yifan Shi\textsuperscript{\rm 1} 
\quad
Yingqi Liu\textsuperscript{\rm 2}
\quad
Kang Wei\textsuperscript{\rm 2}
\quad
Li Shen\textsuperscript{\rm 3,}\thanks{Corresponding authors: Li Shen and Xueqian Wang}
\quad
Xueqian Wang\textsuperscript{\rm 1,*}
\quad
Dacheng Tao\textsuperscript{\rm 3}
\\
\textsuperscript{\rm 1}Tsinghua University, Shenzhen, China; \textsuperscript{\rm 3}JD Explore Academy, Beijing, China\\
\textsuperscript{\rm 2}Nanjing University of Science and Technology, Nanjing, China\\
%\textsuperscript{\rm 3}JD Explore Academy, Beijing, China\\
{\tt\small shiyf21@mails.tsinghua.edu.cn;
lyq@njust.edu.cn;
kang.wei@njust.edu.cn; 
}\\
{\tt\small mathshenli@gmail.com; wang.xq@sz.tsinghua.edu.cn; dacheng.tao@gmail.com
}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
To defend the inference attacks and mitigate the sensitive information leakages in Federated Learning (FL), client-level Differentially Private FL (DPFL) is the de-facto standard for privacy protection by clipping local updates and adding random noise. However, existing DPFL methods tend to make a sharper loss landscape and have poorer weight perturbation robustness, resulting in severe performance degradation. To alleviate these issues, we propose a novel DPFL algorithm named DP-FedSAM, which leverages gradient perturbation to mitigate the negative impact of DP. Specifically, DP-FedSAM integrates Sharpness Aware Minimization (SAM) optimizer to generate local flatness models with better stability and weight perturbation robustness, which results in the small norm of local updates and robustness to DP noise, thereby improving the performance. From the theoretical perspective, we analyze in detail how DP-FedSAM mitigates the performance degradation induced by DP. Meanwhile, we give rigorous privacy guarantees with RÃ©nyi DP and present the sensitivity analysis of local updates. At last, we empirically confirm that our algorithm achieves state-of-the-art (SOTA) performance compared with existing SOTA baselines in DPFL. Code is available at \url{https://github.com/YMJS-Irfan/DP-FedSAM}
\end{abstract}

%%%%%%%%% BODY TEXT
\input{1_intro}
\input{3_related_work}
\input{2_pre}
\input{4_method}
\input{7_analysis}
\input{5_exper}
\input{6_conclusion}

% \clearpage
{
\small
\bibliographystyle{ieee_fullname}
\bibliography{ref}
}

\clearpage
\input{appendix}

\end{document}
