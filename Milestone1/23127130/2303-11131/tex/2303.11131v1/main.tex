% Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------

\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{adjustbox}
\usepackage{color}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}
\usepackage{hyperref}
% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

\def\arrvline{\hfil\kern\arraycolsep\vline\kern-\arraycolsep\hfilneg}
\definecolor{weining}{HTML}{246FFF}
\definecolor{maryam}{HTML}{12b637}
\definecolor{todo}{HTML}{E74C3C}

% Macro
\newcommand{\hubert}{HuBERT}

\newcommand{\wn}[1]{{\textcolor{weining}{[weining: #1]}}}
% \newcommand{\wn}[1]{}
\newcommand{\maryam}[1]{{\textcolor{maryam}{[maryam: #1]}}}
% \newcommand{\maryam}[1]{}
\newcommand{\todo}[1]{{\textcolor{todo}{#1}}}

% Title.
% ------
% \title{Cocktail HuBERT: Generalizing Self-Supervised Pre-training to\\Mixture Speech with Masked Pseudo Source Separation}
\title{Cocktail HuBERT: Generalized Self-Supervised Pre-training \\for Mixture and Single-Source Speech}
%
% Single address.
% ---------------
\name{Maryam Fazel-Zarandi and Wei-Ning Hsu}
\address{
    Meta AI - FAIR\\
    \texttt{\{maryamfazel,wnhsu\}@meta.com}
}

%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
\begin{document}
\ninept
%
\maketitle
%
\begin{abstract}
Self-supervised learning leverages unlabeled data effectively, improving label efficiency and generalization to domains without labeled data. While recent work has studied generalization to more acoustic/linguistic domains, languages, and modalities, these investigations are limited to single-source speech with one primary speaker in the recording. This paper presents Cocktail HuBERT, a self-supervised learning framework that generalizes to mixture speech using a masked pseudo source separation objective. This objective encourages the model to identify the number of sources, separate and understand the context, and infer the content of masked regions represented as discovered units. Cocktail HuBERT outperforms state-of-the-art results with $69\%$ lower WER on multi-speaker ASR, $31\%$ lower DER on diarization, and is competitive on single- and multi-speaker tasks from SUPERB. %Codes and mores are available at \url{https://github.com/facebookresearch/fairseq/tree/main/examples/hubert}.

% ssl good
% ssl not covered
% we propose 
% effective on what
\end{abstract}
%
\begin{keywords}
Self-supervised pre-training, diarization, multi-speaker ASR, source separation, cocktail party, mixture speech
\end{keywords}
%
\section{Introduction}
Self-supervised learning (SSL) has greatly advanced speech processing over the past few years~\cite{chung2019unsupervised, schneider2019wav2vec, Hsu2021HuBERTSS, baevski2022data2vec, chen2022wavlm}. Supervised fine-tuning from a pre-trained model enjoys better label efficiency, achieving performance on par with supervised models using hundredths fewer labeled data~\cite{Baevski2020}. The representations learned with pre-trained models are more universal: in contrast to those from supervised learning~\cite{chen2021speechnet, hsu2019transfer, jia2018transfer}, self-supervised representations benefit a wider range of tasks~\cite{Yang2021, Tsai2022}. Self-supervised representations also enable many novel applications, such as unsupervised speech recognition and synthesis~\cite{baevski2021unsupervised, liu2022simple, ni2022unsupervised}, disentangled speech codec~\cite{polyak2021speech}, text-free spoken language models and prompting~\cite{lakhotia2021generative, chang2022exploration}. 

A key advantage of self-supervised pre-training is that it uses unlabeled data instead of labeled data, such that a model can be pre-trained on data covering more domains~\cite{hsu2021robust, kawakami2020learning, conneau2020unsupervised, babu2021xls}. Consequently, the fine-tuned model is more robust to domain shift, suffering milder degradation when evaluated on domains unseen during fine-tuning~\cite{hsu2021robust}. Generalizing this idea, recent work also extends self-supervised pre-training to multi-modal speech~\cite{shi2022learning,hsu2022single} and demonstrates a multi-modal speech recognition system can be built with only labeled unimodal data. 
However, up until now, speech pre-training has been designed for single-source speech, which contains one primary speaker in each sample whereas other sources are assumed noise~\cite{chen2022wavlm}, leaving mixture speech unattended.

% Mixture speech, where multiple speakers may speak at the same time, occurs frequently in conversational scenarios (e.g., meetings~\cite{ami, chime6}, phone calls~\cite{swb}) and in-the-wild scenarios (e.g., busy street and restaurants~\cite{chime-4, easycom}). 
Mixture speech, where multiple speakers may speak at the same time, occurs frequently in conversational scenarios.
% (e.g., meetings, phone calls) and in-the-wild scenarios (e.g., busy street and restaurants). 
These scenarios impose greater challenges to applications common to single-source speech (e.g., recognizing the ``target'' speech from a mixture), and also generate applications specific to mixture speech, including speech diarization, source separation, multi-speaker speech recognition (transcribe everything) and more. Pre-trained models designed for single-source speech are likely to be sub-optimal for these tasks.

In an effort to broaden the applicability of pre-trained models to wider speech varieties, this paper presents Cocktail HuBERT (C-HuBERT), a self-supervised framework that pre-trains on both single-source and mixture speech with a unified objective. The pre-training objective can be summed as \textit{masked pseudo source separation}, which predicts automatically discovered units of randomly masked spans for each source in the mixture given unmasked speech context. The speech mixture contains one or more sources, created by artificially mixing single source samples. To excel at this task, the model is required to perform three tasks jointly: source separation, acoustic modeling, and language modeling.
% We pre-train Cocktail HuBERT on LibriSpeech~\cite{LibriSpeech} and Libri-light~\cite{LibriLight} for \textsc{base} and \textsc{large} architectures, respectively. 
Evaluation on multi-speaker automatic speech recognition (MS-ASR) and speech diarization (SD) verifies that the proposed objective is particularly effective for downstream tasks concerning mixture speech, outperforming state-of-the-art results by large margins (7.8\% vs 24.9\% WER on Libri2Mix MS-ASR, and 3.86\% vs 5.62\% DER on Libri(2+3)Mix SD). 
%The pre-trained models are also evaluated 
Evaluation on SUPERB~\cite{Yang2021} also shows strong performance on additional mixture tasks and slight-to-none degradation on single-source tasks.


\begin{figure*}[t]       
    \begin{subfigure}[t]{0.24\linewidth}
        \includegraphics[trim={220 50 330 40},clip,width=\linewidth]{figures/cocktail_hubert.pdf}   
        \caption{Cocktail HuBERT}
        \label{fig:model}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.74\linewidth}
        \includegraphics[trim={0 230 0 0},clip,width=\linewidth]{figures/chubert_mix.pdf}   
        \caption{Mixture simulation.}
        \label{fig:mix}
    \end{subfigure}
    \vspace{-5pt}
    \caption{(a) C-HuBERT ($K=2$) predicts hidden units of the masked frames for each source in the input audio mix generated by k-means clustering. (b) Mixture simulation with $K=3$, $n=1$, $(r_l, r_e, o) = (0.75, 2.0, 640)$. Step i: sample the number of extra sources $n$ and then sample additional sources $z_{extra}^{1:n}$. Step ii: chunk, scale, and shift according to sampled $(r_l, r_e, o)$. $e(y)$ denotes the energy of $y$. Step iii: mix audio and pad target units with [SIL] for silent frames (last frame of $z_{mix}^1$ and first two frames of $z_{mix}^2$) and silent streams ($z_{mix}^3$).} 
    \vspace{-12pt}
\end{figure*}

\section{Background}
This paper is built upon Hidden Unit BERT (\hubert{})~\cite{Hsu2021HuBERTSS}, one of the state-of-the-art speech pre-training frameworks. %We now provide a brief introduction for self-completeness.
The pre-training objective of \hubert{} is masked cluster prediction. Similar to BERT, it masks part of the speech input, and predicts given the context (unmasked part) some label derived from the masked input. While the label used in BERT is the input token itself, \hubert{} proposes to obtain discrete labels via clustering audio features and refine the label iteratively. 
Concretely, let $y$ be waveform, $x_t = f_t(y)$ be \hubert{} local feature extractor $f$ (CNN) output at time $t$, $c^l_t = g^l_t(x)$ be contextualized feature extractor $g$ ($L$-layer Transformer) output at time $t$ layer $l$, and $z_t$ be the target unit at time $t$. \hubert{} ($f$ and $g$) is pre-trained by predicting $z_t$ from $g^L_t(\text{MASK}(x))$ for time steps $t$ that are masked, where $\text{MASK}(\cdot)$ is an operation that randomly samples spans of 10 frames and replaces the features in those spans with a learned masked embedding $\text{[MSK]}$ following wav2vec2.0~\cite{Baevski2020}.
% 
In the first iteration, $z_t$ are obtained by clustering MFCC features. In the subsequent iterations, the latest iteration \hubert{} representations $c^l_t$ are used for clustering, which produces higher quality cluster assignments than those from raw or earlier iteration \hubert{} features.

Intuitively, \hubert{} pre-training solves acoustic and language modeling task jointly, where the model needs to understand the content from observed regions (acoustic model) and then infer the label for the masked frames (language model).



\section{Cocktail HuBERT}
\label{cocktail-hubert}
The ``cocktail party problem'' describes the setup where sounds from different sources are mixed prior to being perceived, such that estimation of individual sources given the mixture is required to perform downstream tasks. Human brains have impressive ability to focus on a particular stimulus leveraging structural properties of single sources. Researchers have also attempted to reproduce this capability and develop applications like source separation~\cite{shi2021discretization} and multi-speaker speech recognition~\cite{Chang2022}. 

In this paper, we present Cocktail HuBERT, a self-supervised learning objective that trains the model to tackle the cocktail party problem, as depicted in Figure~\ref{fig:model}. Conceptually, the model takes as input a masked mixture which contains one or more speech sources with spans of frames randomly masked, similar to \hubert{}. The model processes the masked mixture and predicts the discrete labels for each source for only the masked frames. The discrete labels can be viewed as automatically discovered frame-level phonetic transcripts. Thus, the pre-training task is analogous to pseudo source separation that predicts a proxy frame representation for each source.
% , and is only carried out for masked frames. We detail the training objective and mixture simulation in the following sections.
% Thus, performing this tasks requires the model to implicitly separate the mixture and understand the content, followed by inferring the frame-level labels of masked regions based on the context. 
% Figure~\ref{fig:cocktail-hubert} illustrates the training process. The training objective and the mixing algorithm are described formally in the following sections.


\subsection{Model and Training Objective}
Cocktail HuBERT adopts the same local and contextualized feature encoder ($f$ and $g$) as HuBERT, but instead of having only one projection head to predict the posterior over units for a single source, Cocktail HuBERT has $K$ project heads to predict units for at most $K$ sources. Let $y_{mix}$ be a mixture containing up to $K$ sources and $z^{i}_{mix}$ for $i \in [K]$ be the target unit sequence for source $i$ (some corresponds to silent sources). The model outputs $K$ streams of predictions, where $p^j_t(\cdot \mid g(\text{MASK}(f(y_{mix})))$ denotes the $j$-th stream prediction at time step $t$. The loss of the $j$-th stream prediction with respect to the $i$-th source unit sequence $z^i_{mix}$ is:

\vspace{-5pt}
\begin{equation}
    L_m^{j,i} = \sum_{t \in M} \log p^j(z^i_{mix,t} \mid g(\text{MASK}(f(y_{mix}))),
\end{equation}
where $M$ denotes the masked time steps. Since the order of the model predictions $j$ do not necessarily align with the order of the sources $i$, permutation invariant training (PIT) loss~\cite{Yu2017} is deployed which finds the alignment with the minimum loss. Let $\cal P$ be all permutations of $[K]$, the masked pseudo source separation objective is
\vspace{-5pt}
\begin{equation}
    \frac{1}{K} \arg \min_{\pi \in {\cal P}} \sum^{K}_{j = 1} L_m^{j, \pi(j)}.
\end{equation}
\vspace{-10pt}

% The pre-training objective of Cocktail HuBERT is masked pseudo source separation. To this end, we extend the \hubert{} model to predict pseudo-units for $n$ number of speakers present in an input mixture speech simultaneously using permutation invariant training (PIT) \cite{Yu2017}. Let $M$ denote the set of indices to be masked and $z_{it}$ be the target unit for speaker $i$ at time $t$. 
% \begin{equation}
%     L_{c} = \frac{1}{n} \arg \min_{\pi \in {\cal P}} \sum^{n}_{i = 1} L_m(g_i; X, M, Z_{\pi(i)})
% \end{equation}
% \noindent
% where, ${\cal P}$ is set of all permutations on $\{1, ..., n\}$ and $L_m$ is the cross-entropy loss computed over masked timesteps defined as: 
% \begin{equation}
%     L_m(g; X, M, Z_{i}) = \sum_{t \in M} \log p_{g}(z_{it}|MASK(x), t).  
% \end{equation}
% \noindent
% Our pre-trained models follow the \hubert~architecture, with a convolutional waveform encoder and a BERT encoder \cite{Devlin2018}, but have $n$ projection and code embedding layers, one for each speaker. 

%We follow a similar approach to \cite{Chen2021WavLM} for noisy/overlapped speech simulation, with some key differences: 1) We allow more than two utterances to be mixed together. The maximum number of speakers is an input parameter, and when the provided number is larger than two, we uniformly sample the number of utterances to mix from $\{2, ...,$ \textit{max-num-speakers}$\}$. The mixing scale for each utterance are computed relative to the primary utterance. 2) For all utterances except the primary utterance, we set the start position of the mixing region to be zero. 3) We allow for utterances not to overlap completely by sampling an offset indicating the min/max location to mix relative to primary utterance's start. Note that the offset can be negative, in which case we pad $|$\textit{offset}$|$ zeros to the beginning of the primary utterance and then add the other utterance from the beginning. Similarly, we zero pad the end of the utterance that ends first to the end of the other utterance. The pseudo labels of the padded frames are assigned a special blank token. The details of our overlapped speech simulation are shown in Algorithm 1. \maryam{do we need to say more?}

%\noindent
%\input{tables/algorithm}
\subsection{Mixture Simulation}
\label{data-mixing}
% To train c-hubert with max number of sources set to $K$, we generate a mixture input as follows:
% sample the number of sources $n <= K$ with probability 1-p_{mix} for n=1 and p_{mix} / (K-1) otherwise.
% for each source y^k where 1<k<=n, we sample length ratio $r_l$, energy ratio $r_e$, and offset $o$.

For training our models, overlapped speech for a maximum of $K$ speakers is simulated as follows (See Fig~\ref{fig:mix}).
A batch of $B$ utterances where $B \ge K$ is sampled from the dataset. 
For each utterance $y$ and its units $z$, the number of additional sources $n \in \{0, \cdots K-1\}$ is sampled with $P(n=0) = 1 - p_{mix}$ and $ P(n=k) = p_{mix} / (K-1)$ for $\forall k \ne 0$.
An additional source is either an utterance from the batch, or a non-speech noise sampled from a noise dataset. The probability of selecting noise is $p_{noise}$.
For each additional source $y_{extra}^{k}$, a tuple of length ratio, energy ratio, and offset $(r_l, r_e, o)$ are sampled from some uniform distributions, which are used to chunk, scale, and shift $y_{extra}^{k}$ with respect to $y$. Let $\hat{y}_{extra}^{k}$ be the resulting source and $\hat{z}_{extra}^k$ be the units chunked correspondingly if $y_{extra}^{k}$ is not noise. The resulting mixture $y_{mix} = y + \sum_{k=1}^{n} \hat{y}_{extra}^{k}$. 
Note that each source is right-padded to the maximum length among $y$ and $y_{extra}^{k}\; \forall k$ with silence. A special token [SIL] is used for frames corresponding to padded silence (including the offset at the beginning). The first $n+1$ unit sequences correspond to the [SIL]-padded $z$ and $\hat{z}_{extra}^k$ for non-noise $k$. The remaining $K - (n + 1)$ sequences and those corresponding to noise samples are set to [SIL] sequences. 

% Given a batch of speech utterances $U$ with corresponding pseudo label set $Z$, and a special pseudo label $SIL$ to denote padded frames:
% 1) Sample $m$ utterances to mix from the batch with mixing probability $p$.
% 2)  For each utterance $u^{(1)}\in U^m$, uniformly sample the number of utterances $s$ to mix with from $\{2, ..., K\}$. 
% 3)  For $s$ number of times, sample a secondary utterance from the batch or a noise from a set of DNS (Deep Noise Suppression) noises \cite{Reddy2021}, with mixing noise probability $p_n$. Let this set be $X^s$.
% 4)  For each utterance $u^{(i)} \in X^s$, sample length $l$ from $min({\cal U}(L_{min}, L_{max}) * L_{u^{(1)}}, L_{u^{(i)}})$, where $L$ refers to the length of an utterance, and $L_{min}$ and $L_{max}$ define the min/max length ratio between $u^{(1)}$ and $u^{(i)}$. Chunk $u^{(i)}$ to length $l$ and update the corresponding pseudo labels.
% 5)  Sample a target energy ratio from ${\cal U}(E_{min}, E_{max})$ and adjust the volume of $u^{(i)}$ accordingly.
% 6)  Sample a start position from $\{O_{min}, ..., O_{max}\}$, where $O_{min}$ and $O_{max}$ denote the min/max location to mix relative to $u^{(1)}$ start. Note that the offset can be negative, in which case we pad $|$\textit{offset}$|$ zeros to the beginning of the primary utterance and then add the other utterance from the beginning. Similarly, we zero pad the end of the utterance that ends first to the end of the other utterance. Pseudo labels for padded frames are assigned $SIL$.
% 7)  Add the chunked $u^{(i)}$ with adjusted volume to $u^{(1)}$ from the starting position. 
% 8)  If an input has fewer than $n$ speakers, consider a stream of $SIL$ pseudo labels for the missing speakers.


\section{Related Work}
Cocktail HuBERT is most related to HuBERT~\cite{Hsu2021HuBERTSS} and WavLM \cite{chen2022wavlm}, which are self-supervised speech pre-training frameworks based on masked cluster prediction. Similar to Cocktail HuBERT, WavLM also stochastically mixes single source speech with noise and/or other single source speech. However, WavLM is effectively HuBERT with data augmentation, which pre-trains the model using the same objective as HuBERT where only the units of the ``primary'' speech are predicted: the added noise and speech are both treated as noise and should be ignored by the model. Consequently, for the model to differentiate primary speech from interfering speech, WavLM deploys a more restrictive mixing strategy, where the interfering audio is at most half as long as the primary speech. % optionally reiterate c-hubert does not have any constraints

On the other hand, Cocktail HuBERT and \cite{shi2021discretization} also share similar training objectives. Inspired by the capability of converting units back to speech~\cite{polyak2021speech} and the connection between single/multi-speaker speech recognition to speech enhancement/separation, \cite{shi2021discretization} proposes an alternative approach to speech enhancement and source separation by predicting units instead of spectral masks or waveforms. Comparing Cocktail HuBERT with \cite{shi2021discretization}, the former are designed for pre-training, which masks partial input and predicts units only for masked frames, requiring the model to perform language modeling. It is also evaluated on a wide range of downstream tasks including both mixture and single-source speech processing. In contrast, the latter is designed for particular downstream tasks (enhancement and separation) that predict units for all frames without masking the input. It is unclear how the resulting model performs on other tasks.



% skip these as they are not pre-training 
% source separation and multi-speaker asr
% speech diarization


\section{Experimental Setup}
\label{tasks}
%\subsection{Data}

For unsupervised pre-training, we use 960 hours of LibriSpeech audio \cite{LibriSpeech} for the \textsc{Base} model and 60k hours of Libri-light audio \cite{LibriLight} for the \textsc{Large} model. 
We extract features from the $9$-th transformer layer of the HuBERT \textsc{Base} model for K-means with $500$ clusters and use those labels to train the Cocktail HuBERT models.
This ensures that we have high quality labels. %, and in a sense resembles a curriculum where we first train the model for single speaker and then continue training on mixed data.
We apply data mixing to $20\%$, $60\%$, and $100\%$ of the data, where the mixing noise probability is set to $10\%$.
The Cocktail \textsc{Base} and \textsc{Large} models are trained on $32$ and $128$ GPUs, respectively, for $400$k and $800$k steps. The batch sizes are at most $87.5$ and $56.25$ seconds of audio per GPU.
Adam \cite{Kingma2014} optimizer is used with $\beta = (0.9,0.98)$, and learning rate ramps up linearly from $0$ to peak learning for the first $32$k training steps, and then decays back to zero. The peak learning rates are 5e-4/1e-4 for Cocktail \textsc{Base}/\textsc{Large} models. 
$K/p_{mix}$ are set to 5/1.0 for \textsc{Base} and 3/1.0 for \textsc{Large} unless otherwise specified.
% The default $K/p_{mix}$ are 5/1.0 for \textsc{Base} and 3/1.0 for \textsc{Large}.
% \wn{base: 5spk, 100\% mixing; large: 3spk 100\% mixing}
%\subsection{Evaluation Tasks}

We evaluate our pre-trained models specifically on multi-speaker speech recognition and diarization tasks, as they involve multiple speakers. 
We use the LibriMix data \cite{cosentino2020librimix} which contains multi-speaker overlapped speech simulated by mixing utterances from the LibriSpeech corpus. We focus on the two- and three-speaker scenarios, %, which include 58h/11h/11h and 40h/11h/11h of train/dev/test, respectively. 
and mixes with the ``max mode'' where the shortest utterance is padded to the longest one.
For MS-ASR, we fine-tune our models on multi-speaker labeled data using the connectionist temporal classification (CTC) \cite{Graves2006} loss for each (output stream, label stream) pair and compute the PIT-CTC loss, to fine-tune the whole model except for the local feature extractor. The projection layers are removed and replaced with a randomly initialized softmax layer for each stream. The CTC target vocabulary includes 26 English characters, a space token, an apostrophe, and a special CTC blank symbol.
% We fine-tune each model on 8 GPUs on the \texttt{train-100-mix-clean} subset of Libri2Mix and Libri3Mix for 2-speaker and 3-speaker scenarios, respectively.
We fine-tune each model on 8 GPUs on the \texttt{train-100-mix-clean} subset of Libri2Mix for the 2-speaker scenario. 
The batch sizes per GPU are at most 200/80 seconds of audio for \textsc{Base}/\textsc{large} models. We sweep over peak learning rate ([1e-5, 1e-4]) for each model size using the PIT word error rate (WER) on the \texttt{dev\_mix\_clean} subset as criterion for model selection. All other hyperparameters are based on \cite{Hsu2021HuBERTSS}, except that we set \textit{freeze-step} to zero. We use beam search decoding with a 4-gram language model and a beam size of $500$.

For SD, we use a similar setup to SUPERB \cite{Yang2021}, where we freeze the pre-trained model and weight-sum the representations from different $g$ layers with learnable weights as the input to the diarization model. The diarization model uses a single layer 512-unit LSTM and is trained with the PIT loss.
We train each model on 1 GPU on the \texttt{train-100-mix-both} subset of Libri2Mix and Libri3Mix, and a mixture of both datasets. We use a batch size of 8, train for 30k steps, and sweep over learning rate ([1e-2, 1e-4]) for each model using accuracy on the \texttt{dev\_mix\_both} subset of each dataset as criterion for model selection. For the evaluation metric, we use the diarization error rate (DER) \cite{Fiscus2006}. 

%\wn{we may want to exclude speaker related results. privacy now does not permit publishing models that can do speaker verification or identification}
We also compare our models to other strong pre-trained models on a subset of SUPERB tasks~\cite{Yang2021, Tsai2022}, including
% which in addition to SD include: Speaker Identification (SID), Automatic Speaker Verification (ASV), 
Phoneme Recognition (PR), Automatic Speech Recognition (ASR), Keyword Spotting (KS), Query by Example Spoken Term Detection (QbE), Intent Classification (IC), Slot Filling (SF), Emotion Recognition (ER), Speech Enhancement (SE), and Speech Separation (SS) following the protocol. Overall score is computed following \cite{chen2022wavlm}. 
% We use the same downstream models as the SUPERB implementations, feed the weighted-sum of the hidden states of the pre-trained models as input, and freeze the pre-trained models when fine-tuning.


\section{Results}

%\subsection{Evaluation Tasks}

%\subsection{Multi-Speaker ASR}

%Table \ref{ms-asr-table} shows results of fine-tuning C-HuBERT on single-speaker and multi-speaker ASR tasks. For single-speaker ASR, we train and evaluate on LibriSpeech 10hr corpus. For MS-ASR, we train on Libri2Mix, and evaluate on both single-speaker input from LibriSpeech and two-speaker input from Libri2Mix. 
\input{tables/ms-asr}
\input{tables/ms-asr-single}

\subsection{Multi-speaker and Single-speaker ASR}

We first evaluate Cocktail HuBERT models on multi-speaker ASR and compare them to three state-of-the-art supervised baselines: (1) the end-to-end ASR model trained with PIT-CTC \cite{Chang2019}, (2) the end-to-end ASR model trained with the extended Graph-based temporal classification \cite{Chang2022} loss (GTC-e), and (3) the Conditional-Conformer-CTC model \cite{ConditionalConformer} that generates CTC predictions conditioning on past CTC predictions for other streams. To understand how pre-training objectives affect the multi-speaker ASR performance, we also fine-tune HuBERT \textsc{Base} and \textsc{Large}, which have the identical model architecture as C-HuBERT models, as our self-supervised baselines.

\input{tables/superb}

Results are reported in Table~\ref{ms-asr-table}. First, we observe that both Cocktail HuBERT \textsc{Base} and \textsc{Large} significantly outperform the baselines, with \textsc{Large} reducing the WER by 69\% relative (24.9\% $\rightarrow$ 7.8\%). More importantly, there is a considerable gap between the HuBERT and Cocktail HuBERT performance (37.2\% vs 13.7\% for \textsc{Base}, and 35.2\% vs 7.8\% for \textsc{Large}), validating that the proposed masked pseudo source separation objective brings significant gain to the multi-speaker downstream task.

We further investigate the performance of our MS-ASR models on single speaker input by comparing with models fine-tuned for single-speaker ASR (Table \ref{ms-asr-single}). Overall, we see degradation in performance across all settings when using MS-ASR models to transcribe single-speaker utterances. However, compared to HuBERT, C-HuBERT models are more robust to variation in the number of speakers: the WER of \textsc{Large} increases by 1.2\% and 4.1\% on test-clean and test-other when fine-tuned on Libri2Mix instead of LS-10h, lower than the 5.1\%/9.1\% WER increases for HuBERT.
% , showing that the same fine-tuned model could be used for both multi-speaker and single-speaker speech recognition.


\input{tables/sd}

\subsection{Speech Diarization}

Table \ref{sd-librimix} shows results on speech diarization for two-, three-, and a mix of two- and three-speaker datasets. When compared against HuBERT and WavLM \textsc{Base} and \textsc{Large} models, the best DERs across the three settings are attained using C-HuBERT models. In fact, the C-HuBERT \textsc{Base} model outperforms the WavLM \textsc{Large} model on Libri2Mix, Libri3Mix, and Libri(2+3)Mix by $14\%$, $23\%$, and $30\%$ relative, respectively. These are impressive gains since the \textsc{Base} model is considerably smaller than the WavLM \textsc{Large} model and was pre-trained on fewer hours of data. Performance on all test sets are further improved when scaling to the \textsc{Large} model.
% Overall, the C-HuBERT \textsc{Large} model significantly outperforms the other models, with \todo{30\%} relative improvements over WavLM \textsc{large} on Libri(2+3)Mix. 

\subsection{SUPERB}
We compare C-HuBERT with several state-of-the-art SSL models on the SUPERB tasks (Table~\ref{superb}). C-HuBERT shows strong performance on speech enhancement and source separation, which are closer to the pre-training task of C-HuBERT. It lags behind other models on single-speaker tasks such as PR and ASR . However, the performance can be improved and the gap can be reduced by simply scaling C-HuBERT (on PR, 12\% PER reduction (1 - 5.41 / 6.14) between HuBERT and C-HuBERT for BASE and 7\% gap for LARGE).
% Table \ref{superb} reports our results on SUPERB tasks mentioned in $\S$\ref{tasks}. We observe that C-HuBERT \textsc{large} model outperforms the current state-of-the-art WavLM \textsc{large} on the separation task, and C-HuBERT \textsc{base} model outperforms the other \textsc{base} models on enhancement and separation tasks, showing the benefit of our approach for the mixture speech scenarios. \maryam{what else do we want to say here?}


\input{tables/sd_ablation}

\subsection{Ablation Studies}
% \wn{can we also add ablation results showing single-speaker ASR and multi-speaker ASR here?} \maryam{not sure I understand how they relat2e to SD?}
We study the effect of mixing probability $p_{mix}$ and max number of speakers $K$ on speech diarization (SD), multi-speaker and single-speaker ASR (MS-ASR and ASR) with the C-HuBERT \textsc{Base} model (Table \ref{ablation}). On speech diarization, more aggressive mixing (larger $K$ and higher $p_{mix}$) leads to better results. The trend reverses on single-speaker speech recognition in general. Nevertheless, we observe that C-HuBERT outperforms HuBERT on single-speaker ASR for some configurations (e.g., $K=5$ and $p=0.2$ yields 3.9\%/9.3\% compared to 4.1\%/9.4\% from HuBERT in Table~\ref{ms-asr-table}).

We report results on both single- and multi-speaker test sets when fine-tuning C-HuBERT on multi-speaker data (columns below MS-ASR). Overall, $p_{mix}=0.2$ leads to the worst multi-speaker test results (23.5\% to 26.3\%), which are still better than those from HuBERT (35.8\%). The best multi-speaker test result is obtained with $K=5$ and $p_{mix}=0.6$. The results on single-speaker test sets (d-c, d-o) are interesting --- with $K=2$, single-speaker and multi-speaker WERs are negatively correlated, while with $K=5$ they are positively correlated. We believe the observation arises from the interaction of two factors: how mismatched pre-training and fine-tuning are, and how mismatched pre-training and testing are.

\section{Conclusion}
% summarize contribution
This paper presents Cocktail HuBERT, a large-scale pre-trained model with the objective of masked pseudo source separation. C-HuBERT extends the HuBERT framework to multiple speakers, enabling the models to outperform the state-of-the-art models on MS-ASR and SD tasks, while achieving competitive performance on other SUPERB single-speaker and multi-speaker tasks.

% limitation and future work
% we still rely on single-source speech to create training data
% we do envision that real-mixture data can be used for the subsequent iteration, which we leave for future work

% \section{Acknowledgments}
% \wn{todo}

% To start a new column (but not a new page) and help balance the last-page
% column length use \vfill\pagebreak.
% -------------------------------------------------------------------------
%\vfill
%\pagebreak


%In LaTeX, to start a new column (but not a new page) and help balance the
%last-page column lengths, you can use the command ``$\backslash$pagebreak'' as
%demonstrated on this page (see the LaTeX source below).

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{refs}

\end{document}
