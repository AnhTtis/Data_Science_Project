@ARTICLE{Fortunato2010,
  author  = {Fortunato, S.},
  title   = {Community detection in graphs},
  journal = {Phys. Rep.-Rev. Sec. Phys. Lett.}, 
  volume  = {486},
  year    = {2010},
  pages   = {75-174}
}

@ARTICLE{NewmanGirvan2004,
  author  = {Newman, M. E. J. and Girvan, M.},
  title   = {Finding and evaluating community structure in networks},
  journal = {Phys. Rev. E.}, 
  volume  = {69},
  year    = {2004},
  pages   = {026113}
}

@ARTICLE{Vehlowetal2013,
  author  = {Vehlow, C. and Reinhardt, T. and Weiskopf, D.},
  title   = {Visualizing Fuzzy Overlapping Communities in Networks},
  journal = {IEEE Trans. Vis. Comput. Graph.}, 
  volume  = {19},
  year    = {2013},
  pages   = {2486-2495}
}

@ARTICLE{Raghavanetal2007,
  author  = {Raghavan, U. and Albert, R. and Kumara, S.},
  title   = {Near linear time algorithm to detect community structures in large-scale networks},
  journal = {Phys. Rev E.}, 
  volume  = {76},
  year    = {2007},
  pages   = {036106}
}

@ARTICLE{SubeljBajec2011a,
  author  = {\v{S}ubelj, L. and Bajec, M.},
  title   = {Robust network community detection using balanced propagation},
  journal = {Eur. Phys. J. B.}, 
  volume  = {81},
  year    = {2011},
  pages   = {353-362}
}

@ARTICLE{Louetal2013,
  author  = {Lou, H. and Li, S. and Zhao, Y.},
  title   = {Detecting community structure using label propagation with weighted coherent neighborhood propinquity},
  journal = {Physica A.}, 
  volume  = {392},
  year    = {2013},
  pages   = {3095-3105}
}

@ARTICLE{Clausetetal2004,
  author  = {Clauset, A. and Newman, M. E. J. and Moore, C.},
  title   = {Finding community structure in very large networks},
  journal = {Phys. Rev. E.}, 
  volume  = {70},
  year    = {2004},
  pages   = {066111}
}

@ARTICLE{Blondeletal2008,
  author  = {Blondel, V. D. and Guillaume, J. L. and Lambiotte, R. and Lefebvre, E.},
  title   = {Fast unfolding of communities in large networks},
  journal = {J. Stat. Mech.-Theory Exp.}, 
  volume  = {2008},
  year    = {2008},
  pages   = {P10008}
}

@ARTICLE{SobolevskyCampari2014,
  author  = {Sobolevsky, S. and Campari, R.},
  title   = {General optimization technique for high-quality community detection in complex networks},
  journal = {Phys. Rev. E.}, 
  volume  = {90},
  year    = {2014},
  pages   = {012811}
}

@ARTICLE{FortunatoBarthelemy2007,
  author  = {Fortunato, S. and Barthelemy, M.},
  title   = {Resolution limit in community detection},
  journal = {Proc. Natl. Acad. Sci. U. S. A.}, 
  volume  = {104},
  year    = {2007},
  pages   = {36-41}
}

@ARTICLE{SubeljBajec2011b,
  author  = {\v{S}ubelj, L. and Bajec, M.},
  title   = {Unfolding communities in large complex networks: Combining defensive and offensive label propagation for core extraction},
  journal = {Phys. Rev. E.}, 
  volume  = {83},
  year    = {2011},
  pages   = {036103}
}

@ARTICLE{WangLi2013,
  author  = {Wang, X. and Li, J.},
  title   = {Detecting communities by the core-vertex and intimate degree in complex networks},
  journal = {Physica A.}, 
  volume  = {392},
  year    = {2013},
  pages   = {2555-2563}
}

@ARTICLE{Lietal2013,
  author  = {Li, J. and Wang, X. and Eustace, J.},
  title   = {Detecting overlapping communities by seed community in weighted complex networks},
  journal = {Physica A.}, 
  volume  = {392},
  year    = {2013},
  pages   = {6125-6134}
}

@ARTICLE{Fabioetal2013,
  author  = {Fabio, D. R. and Fabio, D. and Carlo, P.},
  title   = {Profiling core-periphery network structure by random walkers},
  journal = {Sci. Rep.}, 
  volume  = {3},
  year    = {2013},
  pages   = {1467}
}

@ARTICLE{Chenetal2013,
  author  = {Chen, Q. and Wu, T. T. and Fang, M.},
  title   = {Detecting local community structure in complex networks based on local degree central nodes},
  journal = {Physica A.}, 
  volume  = {392},
  year    = {2013},
  pages   = {529-537}
}

@ARTICLE{Zhangetal2007,
  author  = {Zhang, S. and Wang, R. and Zhang, X.},
  title   = {Identification of overlapping community structure in complex networks using fuzzy c-means clustering},
  journal = {Physica A.}, 
  volume  = {374},
  year    = {2007},
  pages   = {483-490}
}

@ARTICLE{Nepuszetal2008,
  author  = {Nepusz, T. and Petr\'oczi, A. and N\'egyessy, L. and Bazs\'o, F.},
  title   = {Fuzzy communities and the concept of bridgeness in complex networks},
  journal = {Phys. Rev. E.}, 
  volume  = {77},
  year    = {2008},
  pages   = {016107}
}

@ARTICLE{FabricioLiang2013,
  author  = {Fabricio, B. and Liang, Z.},
  title   = {Fuzzy community structure detection by particle competition and cooperation},
  journal = {Soft Comput.}, 
  volume  = {17},
  year    = {2013},
  pages   = {659-673}
}

@ARTICLE{Sunetal2011,
  author  = {Sun, P. and Gao, L. and Han, S.},
  title   = {Identification of overlapping and non-overlapping community structure by fuzzy clustering in complex networks},
  journal = {Inf. Sci.}, 
  volume  = {181},
  year    = {2011},
  pages   = {1060-1071}
}

@ARTICLE{Wangetal2013,
  author  = {Wang, W. and Liu, D. and Liu, X. and Pan, L.},
  title   = {Fuzzy overlapping community detection based on local random walk and multidimensional scaling},
  journal = {Physica A.}, 
  volume  = {392},
  year    = {2013},
  pages   = {6578-6586}
}

@ARTICLE{Psorakisetal2011,
  author  = {Psorakis, I. and Roberts, S. and Ebden, M. and Sheldon, B.},
  title   = {Overlapping community detection using Bayesian non-negative matrix factorization},
  journal = {Phys. Rev. E.}, 
  volume  = {83},
  year    = {2011},
  pages   = {066114}
}

@CONFERENCE{ZhangYeung2012,
  author  = {Zhang, Y. and Yeung, D.},
  title   = {Overlapping Community Detection via Bounded Nonnegative Matrix Tri-Factorization},
  booktitle = {In Proc. ACM SIGKDD Conf.}, 
  year    = {2012},
  pages   = {606-614}
}

@ARTICLE{Liu2010,
  author  = {Liu, J.},
  title   = {Fuzzy modularity and fuzzy community structure in networks},
  journal = {Eur. Phys. J. B.}, 
  volume  = {77},
  year    = {2010},
  pages   = {547-557}
}

@ARTICLE{Havensetal2013,
  author  = {Havens, T. C. and Bezdek, J. C. and Leckie, C., Ramamohanarao, K. and Palaniswami, M.},
  title   = {A Soft Modularity Function For Detecting Fuzzy Communities in Social Networks},
  journal = {IEEE Trans. Fuzzy Syst.}, 
  volume  = {21},
  year    = {2013},
  pages   = {1170-1175}
}

@misc{Newman2013,
  author = {Newman, M. E. J.},
  title  = {Network data},
  howpublished = "\url{http://www-personal.umich.edu/~mejn/netdata/}",
  year = {2013}
}

@ARTICLE{SubeljBajec2012,
  author  = {\v{S}ubelj, L. and Bajec, M.},
  title   = {Ubiquitousness of link-density and link-pattern communities in real-world networks},
  journal = {Eur. Phys. J. B.}, 
  volume  = {85},
  year    = {2012},
  pages   = {1-11}
}

@ARTICLE{Lancichinettietal2008,
  author  = {Lancichinetti, A. and Fortunato, S. and Radicchi, F.},
  title   = {Benchmark graphs for testing community detection algorithms},
  journal = {Phys. Rev. E.}, 
  volume  = {78},
  year    = {2008},
  pages   = {046110}
}

@ARTICLE{Liuetal2014,
  author  = {Liu, W. and Pellegrini, M. and Wang, X.},
  title   = {Detecting Communities Based on Network Topology},
  journal = {Sci. Rep.}, 
  volume  = {4},
  year    = {2014},
  pages   = {5739}
}

@ARTICLE{Danonetal2005,
  author  = {Danon, L. and Diaz-Guilera, A. and Duch, J. and Arenas, A.},
  title   = {Comparing community structure identification},
  journal = {J. Stat. Mech.-Theory Exp.}, 
  volume  = {},
  year    = {2005},
  pages   = {P09008}
}

@ARTICLE{Gregory2011,
  author  = {Gregory, S.},
  title   = {Fuzzy overlapping communities in networks},
  journal = {J. Stat. Mech.-Theory Exp.}, 
  volume  = {},
  year    = {2011},
  pages   = {P02017}
}

@ARTICLE{LancichinettiFortunato2009,
  author  = {Lancichinetti, A. and Fortunato, S.},
  title   = {Benchmarks for testing community detection algorithms on directed and weighted graphs with overlapping communities},
  journal = {Phys. Rev. E.}, 
  volume  = {80},
  year    = {2009},
  pages   = {016118}
}

@CONFERENCE{HullermeierRifqi2009,
  author  = {Hullermeier, E. and Rifqi, M.},
  title   = {A Fuzzy Variant of the Rand Index for Comparing Clustering Structures},
  booktitle = {in Proc. IFSA/EUSFLAT Conf.}, 
  year    = {2009},
  pages   = {1294-1298}
}

% ===============
@incollection{goodfellow14,
title = {Generative Adversarial Nets},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {2672--2680},
year = {2014},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf}
}

@article{Reed2016,
abstract = {Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.},
archivePrefix = {arXiv},
arxivId = {1605.05396},
author = {Reed, Scott and Akata, Zeynep and Yan, Xinchen and Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak},
eprint = {1605.05396},
file = {:C\:/Users/share/Downloads/Documents/reed16.pdf:pdf},
isbn = {9781510829008},
journal = {33rd International Conference on Machine Learning, ICML 2016},
mendeley-groups = {Text to Image},
pages = {1681--1690},
title = {{Generative adversarial text to image synthesis}},
volume = {3},
year = {2016}
}

@article{Mansimov2016,
abstract = {Motivated by the recent progress in generative models, we introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. After training on Microsoft COCO, we compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.},
archivePrefix = {arXiv},
arxivId = {1511.02793},
author = {Mansimov, Elman and Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
eprint = {1511.02793},
file = {:C\:/Users/share/Downloads/Documents/1511.02793.pdf:pdf},
journal = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
mendeley-groups = {Text to Image},
pages = {1--12},
title = {{Generating images from captions with attention}},
year = {2016}
}

@article{Gregor2015,
abstract = {This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.},
archivePrefix = {arXiv},
arxivId = {1502.04623},
author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
eprint = {1502.04623},
file = {:C\:/Users/share/Downloads/Documents/1502.04623.pdf:pdf},
isbn = {9781510810587},
journal = {32nd International Conference on Machine Learning, ICML 2015},
mendeley-groups = {Text to Image},
pages = {1462--1471},
title = {{DRAW: A recurrent neural network for image generation}},
volume = {2},
year = {2015}
}

@article{Tsue2020,
abstract = {We explore novel approaches to the task of image generation from their respective captions, building on state-of-the-art GAN architectures. Particularly, we baseline our models with the Attention-based GANs that learn attention mappings from words to image features. To better capture the features of the descriptions, we then built a novel cyclic design that learns an inverse function to maps the image back to original caption. Additionally, we incorporated recently developed BERT pretrained word embeddings as our initial text featurizer and observe a noticeable improvement in qualitative and quantitative performance compared to the Attention GAN baseline. 1},
archivePrefix = {arXiv},
arxivId = {2003.12137},
author = {Tsue, Trevor and Li, Jason and Sen, Samir},
eprint = {2003.12137},
file = {:C\:/Users/share/Downloads/Documents/2003.12137.pdf:pdf},
issn = {23318422},
journal = {arXiv},
mendeley-groups = {Text to Image},
title = {{Cycle Text-to-Image GAN with BERT}},
year = {2020}
}

@article{Ramesh2021,
abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
archivePrefix = {arXiv},
arxivId = {2102.12092},
author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
eprint = {2102.12092},
file = {:C\:/Users/share/Downloads/Documents/2102.12092v2.pdf:pdf},
mendeley-groups = {Text to Image},
title = {{Zero-Shot Text-to-Image Generation}},
url = {http://arxiv.org/abs/2102.12092},
volume = {200},
year = {2021}
}

@article{Zhang2019,
abstract = {Although Generative Adversarial Networks (GANs) have shown remarkable success in various tasks, they still face challenges in generating high quality images. In this paper, we propose Stacked Generative Adversarial Networks (StackGANs) aimed at generating high-resolution photo-realistic images. First, we propose a two-stage generative adversarial network architecture, StackGAN-v1, for text-to-image synthesis. The Stage-I GAN sketches the primitive shape and colors of a scene based on a given text description, yielding low-resolution images. The Stage-II GAN takes Stage-I results and the text description as inputs, and generates high-resolution images with photo-realistic details. Second, an advanced multi-stage generative adversarial network architecture, StackGAN-v2, is proposed for both conditional and unconditional generative tasks. Our StackGAN-v2 consists of multiple generators and multiple discriminators arranged in a tree-like structure; images at multiple scales corresponding to the same scene are generated from different branches of the tree. StackGAN-v2 shows more stable training behavior than StackGAN-v1 by jointly approximating multiple distributions. Extensive experiments demonstrate that the proposed stacked generative adversarial networks significantly outperform other state-of-the-art methods in generating photo-realistic images.},
archivePrefix = {arXiv},
arxivId = {1710.10916},
author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris N.},
doi = {10.1109/TPAMI.2018.2856256},
eprint = {1710.10916},
file = {:C\:/Users/share/Downloads/Documents/1710.10916v3.pdf:pdf},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Generative models,generative adversarial networks (GANs),multi-distribution approximation,multi-stage GANs,photo-realistic image generation,text-to-image synthesis},
mendeley-groups = {Text to Image},
number = {8},
pages = {1947--1962},
pmid = {30010548},
title = {{StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks}},
volume = {41},
year = {2019}
}

@techreport{WahCUB_200_2011,
	Title = {{The Caltech-UCSD Birds-200-2011 Dataset}},
	Author = {Wah, C. and Branson, S. and Welinder, P. and Perona, P. and Belongie, S.},
	Year = {2011},
	Institution = {California Institute of Technology},
	Number = {CNS-TR-2011-001}
}

@InProceedings{Nilsback08,
  author       = "Maria-Elena Nilsback and Andrew Zisserman",
  title        = "Automated Flower Classification over a Large Number of Classes",
  booktitle    = "Indian Conference on Computer Vision, Graphics and Image Processing",
  month        = "Dec",
  year         = "2008",
}

@article{Qiao22019,
abstract = {Text-to-image generation, i.e. generating an image given a text description, is a very challenging task due to the significant semantic gap between the two domains. Humans, however, tackle this problem intelligently. We learn from diverse objects to form a solid prior about semantics, textures, colors, shapes, and layouts. Given a text description, we immediately imagine an overall visual impression using this prior and, based on this, we draw a picture by progressively adding more and more details. In this paper, and inspired by this process, we propose a novel text-to-image method called LeicaGAN to combine the above three phases in a unified framework. First, we formulate the multiple priors learning phase as a textual-visual co-embedding (TVE) comprising a text-image encoder for learning semantic, texture, and color priors and a text-mask encoder for learning shape and layout priors. Then, we formulate the imagination phase as multiple priors aggregation (MPA) by combining these complementary priors and adding noise for diversity. Lastly, we formulate the creation phase by using a cascaded attentive generator (CAG) to progressively draw a picture from coarse to fine. We leverage adversarial learning for LeicaGAN to enforce semantic consistency and visual realism. Thorough experiments on two public benchmark datasets demonstrate LeicaGAN's superiority over the baseline method. Code has been made available at https://github.com/qiaott/LeicaGAN.},
author = {Qiao, Tingting and Zhang, Jing and Xu, Duanqing and Tao, Dacheng},
file = {:C\:/Users/share/Downloads/Documents/NeurIPS-2019-learn-imagine-and-create-text-to-image-generation-from-prior-knowledge-Paper.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {Text to Image},
number = {NeurIPS},
pages = {1--11},
title = {{Learn, imagine and create: Text-to-image generation from prior knowledge}},
volume = {32},
year = {2019}
}

@article{Xu2018,
abstract = {In this paper, we propose an Attentional Generative Adversarial Network (AttnGAN) that allows attention-driven, multi-stage refinement for fine-grained text-to-image generation. With a novel attentional generative network, the AttnGAN can synthesize fine-grained details at different sub-regions of the image by paying attentions to the relevant words in the natural language description. In addition, a deep attentional multimodal similarity model is proposed to compute a fine-grained image-text matching loss for training the generator. The proposed AttnGAN significantly outperforms the previous state of the art, boosting the best reported inception score by 14.14% on the CUB dataset and 170.25% on the more challenging COCO dataset. A detailed analysis is also performed by visualizing the attention layers of the AttnGAN. It for the first time shows that the layered attentional GAN is able to automatically select the condition at the word level for generating different parts of the image.},
archivePrefix = {arXiv},
arxivId = {1711.10485},
author = {Xu, Tao and Zhang, Pengchuan and Huang, Qiuyuan and Zhang, Han and Gan, Zhe and Huang, Xiaolei and He, Xiaodong},
doi = {10.1109/CVPR.2018.00143},
eprint = {1711.10485},
file = {:C\:/Users/share/Downloads/Documents/Xu_AttnGAN_Fine-Grained_Text_CVPR_2018_paper.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {Text to Image},
pages = {1316--1324},
title = {{AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks}},
year = {2018}
}

@article{Devlin2019,
abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, Jacob and Chang, Ming Wei and Lee, Kenton and Toutanova, Kristina},
eprint = {1810.04805},
file = {:C\:/Users/share/Downloads/Documents/1810.04805_2.pdf:pdf},
isbn = {9781950737130},
journal = {NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
mendeley-groups = {Text to Image},
number = {Mlm},
pages = {4171--4186},
title = {{BERT: Pre-training of deep bidirectional transformers for language understanding}},
volume = {1},
year = {2019}
}

@article{Brown2020,
abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
archivePrefix = {arXiv},
arxivId = {2005.14165},
author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
eprint = {2005.14165},
file = {:C\:/Users/share/Downloads/Documents/2005.14165_2.pdf:pdf},
mendeley-groups = {Text to Image},
title = {{Language Models are Few-Shot Learners}},
url = {http://arxiv.org/abs/2005.14165},
year = {2020}
}

@article{Qiao2019,
abstract = {Generating an image from a given text description has two goals: Visual realism and semantic consistency. Although significant progress has been made in generating high-quality and visually realistic images using generative adversarial networks, guaranteeing semantic consistency between the text description and visual content remains very challenging. In this paper, we address this problem by proposing a novel global-local attentive and semantic-preserving text-to-image-to-text framework called MirrorGAN. MirrorGAN exploits the idea of learning text-to-image generation by redescription and consists of three modules: A semantic text embedding module (STEM), a global-local collaborative attentive module for cascaded image generation (GLAM), and a semantic text regeneration and alignment module (STREAM). STEM generates word-and sentence-level embeddings. GLAM has a cascaded architecture for generating target images from coarse to fine scales, leveraging both local word attention and global sentence attention to progressively enhance the diversity and semantic consistency of the generated images. STREAM seeks to regenerate the text description from the generated image, which semantically aligns with the given text description. Thorough experiments on two public benchmark datasets demonstrate the superiority of MirrorGAN over other representative state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {1903.05854},
author = {Qiao, Tingting and Zhang, Jing and Xu, Duanqing and Tao, Dacheng},
doi = {10.1109/CVPR.2019.00160},
eprint = {1903.05854},
file = {:C\:/Users/share/Downloads/Documents/1903.05854.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Deep Learning,Image and Video Synthesis,Vision + Language},
mendeley-groups = {Text to Image},
pages = {1505--1514},
title = {{Mirrorgan: Learning text-to-image generation by redescription}},
volume = {2019-June},
year = {2019}
}

@article{Pennington2014,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. The model pro- duces a vector space with meaningful sub- structure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on simi- larity tasks and named entity recognition.},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D.},
doi = {10.1080/02688697.2017.1354122},
file = {:C\:/Users/share/Downloads/Documents/glove.pdf:pdf},
issn = {1360046X},
journal = {In Empirical Methods in Nat- ural Language Processing (EMNLP)},
mendeley-groups = {Text to Image},
number = {6},
pages = {1532-- 1543},
pmid = {28722516},
title = {{GloVe: Global Vectors for Word Representation}},
volume = {31},
year = {2014}
}

@article{journals/corr/Ioffe17,
  added-at = {2019-03-06T18:12:42.000+0100},
  author = {Ioffe, Sergey},
  biburl = {https://www.bibsonomy.org/bibtex/27adbcb618b58179852ca0f1d38c248ef/geistgesicht},
  description = {for batch sizes as low as e.g. 4},
  ee = {http://arxiv.org/abs/1702.03275},
  interhash = {b57bd8824bd52c9bf8661477ec02f55f},
  intrahash = {7adbcb618b58179852ca0f1d38c248ef},
  journal = {CoRR},
  keywords = {batch-norm conv-nets dnn},
  timestamp = {2019-03-06T18:12:42.000+0100},
  title = {Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1702.html#Ioffe17},
  volume = {abs/1702.03275},
  year = 2017
}

@article{Liu2021,
abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (86.4 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The code and models will be made publicly available at$\sim$\url{https://github.com/microsoft/Swin-Transformer}.},
archivePrefix = {arXiv},
arxivId = {2103.14030},
author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
eprint = {2103.14030},
file = {:C\:/Users/share/Downloads/Documents/2103.14030.pdf:pdf},
mendeley-groups = {Text to Image},
title = {{Swin Transformer: Hierarchical Vision Transformer using Shifted Windows}},
url = {http://arxiv.org/abs/2103.14030},
year = {2021}
}

@article{Dosovitskiy2020,
abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
archivePrefix = {arXiv},
arxivId = {2010.11929},
author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
eprint = {2010.11929},
file = {:C\:/Users/share/Downloads/Documents/2010.11929.pdf:pdf},
mendeley-groups = {Text to Image},
pages = {1--21},
title = {{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}},
url = {http://arxiv.org/abs/2010.11929},
year = {2020}
}

@article{Tan2019,
abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.},
archivePrefix = {arXiv},
arxivId = {1905.11946},
author = {Tan, Mingxing and Le, Quoc V.},
eprint = {1905.11946},
file = {:C\:/Users/share/Downloads/Documents/1905.11946_2.pdf:pdf},
isbn = {9781510886988},
journal = {36th International Conference on Machine Learning, ICML 2019},
mendeley-groups = {Text to Image},
pages = {10691--10700},
title = {{EfficientNet: Rethinking model scaling for convolutional neural networks}},
volume = {2019-June},
year = {2019}
}

@article{Arifianto2020,
abstract = {In the concept of data hiding, image is often used as a cover to hide sensitive data inside it. This approach is considered a good addition in securing information to cryptography which only hides the information and not the presence of the message itself. The combination of Deep Learning with Steganography and Cryptography is rarely done. By utilizing Deep Neural Networks to encrypt and hide the messages, it will be increasingly difficult to decrypt and track.In this study, we developed an encryption mechanism to not only conceal messages, but transforming them into images. The image containing the hidden messages can later be decrypted and converted back into the original message. We use Generative Adversarial Network to develop the encryption and decryption models. Text data is converted into a word vector using word2vec model which then used as input for the encryption model to produce the word images. We use the MNIST dataset to train models which are able to produce images that encrypt 1000 word variations. Based on our experiments, we were able to produce robust encrypted images with 98% accuracy of reversible words. We also show that our model is resistant to various minor image attacks such as scaling, noise addition, and image rotation.},
author = {Arifianto, Anditya and Maulana, Malik Anhar and Mahadi, Made Raharja Surya and Jamaluddin, Triwidyastuti and Subhi, Rajabandanu and Rendragraha, Adriansyah Dwi and Satya, Muhammad Ferianda},
doi = {10.1109/ICoICT49345.2020.9166184},
file = {:C\:/Users/share/Downloads/Documents/Paper_FG_CV_2019.pdf:pdf},
isbn = {9781728161426},
journal = {2020 8th International Conference on Information and Communication Technology, ICoICT 2020},
keywords = {encryption,generative adversarial network,steganography},
mendeley-groups = {Text to Image},
title = {{EDGAN: Disguising Text as Image using Generative Adversarial Network}},
year = {2020}
}

@article{Salimans2016,
abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
archivePrefix = {arXiv},
arxivId = {1606.03498},
author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
eprint = {1606.03498},
file = {:C\:/Users/share/Downloads/Documents/1606.03498.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {Text to Image},
pages = {2234--2242},
title = {{Improved techniques for training GANs}},
year = {2016}
}

@article{Szegedy2015,
abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {:C\:/Users/share/Downloads/Documents/1409.4842.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {Text to Image},
pages = {1--9},
title = {{Going deeper with convolutions}},
volume = {07-12-June-2015},
year = {2015}
}

@article{Koto2021,
abstract = {Although the Indonesian language is spoken by almost 200 million people and the 10th most spoken language in the world, it is under-represented in NLP research. Previous work on Indonesian has been hampered by a lack of annotated datasets, a sparsity of language resources, and a lack of resource standardization. In this work, we release the IndoLEM dataset comprising seven tasks for the Indonesian language, spanning morpho-syntax, semantics, and discourse. We additionally release IndoBERT, a new pre-trained language model for Indonesian, and evaluate it over IndoLEM, in addition to benchmarking it against existing resources. Our experiments show that IndoBERT achieves state-of-the-art performance over most of the tasks in IndoLEM.},
archivePrefix = {arXiv},
arxivId = {2011.00677},
author = {Koto, Fajri and Rahimi, Afshin and Lau, Jey Han and Baldwin, Timothy},
doi = {10.18653/v1/2020.coling-main.66},
eprint = {2011.00677},
file = {:C\:/Users/share/Downloads/Documents/2020.coling-main.66.pdf:pdf},
mendeley-groups = {Text to Image},
pages = {757--770},
title = {{IndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained Language Model for Indonesian NLP}},
year = {2021}
}

@article{Hu2021,
abstract = {A text to image generation (T2I) model aims to generate photo-realistic images which are semantically consistent with the text descriptions. Built upon the recent advances in generative adversarial networks (GANs), existing T2I models have made great progress. However, a close inspection of their generated images reveals two major limitations: (1) The condition batch normalization methods are applied on the whole image feature maps equally, ignoring the local semantics; (2) The text encoder is fixed during training, which should be trained with the image generator jointly to learn better text representations for image generation. To address these limitations, we propose a novel framework Semantic-Spatial Aware GAN, which is trained in an end-to-end fashion so that the text encoder can exploit better text information. Concretely, we introduce a novel Semantic-Spatial Aware Convolution Network, which (1) learns semantic-adaptive transformation conditioned on text to effectively fuse text features and image features, and (2) learns a mask map in a weakly-supervised way that depends on the current text-image fusion process in order to guide the transformation spatially. Experiments on the challenging COCO and CUB bird datasets demonstrate the advantage of our method over the recent state-of-the-art approaches, regarding both visual fidelity and alignment with input text description. Code is available at https://github.com/wtliao/text2image.},
archivePrefix = {arXiv},
arxivId = {2104.00567},
author = {Hu, Kai and Liao, Wentong and Yang, Michael Ying and Rosenhahn, Bodo},
eprint = {2104.00567},
file = {:C\:/Users/share/Downloads/Documents/2104.00567.pdf:pdf},
mendeley-groups = {Text to Image},
number = {1},
title = {{Text to Image Generation with Semantic-Spatial Aware GAN}},
url = {http://arxiv.org/abs/2104.00567},
year = {2021}
}

@article{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
eprint = {1706.03762},
file = {:C\:/Users/share/Downloads/Documents/1706.03762_2.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {Text to Image},
number = {Nips},
pages = {5999--6009},
title = {{Attention is all you need}},
volume = {2017-December},
year = {2017}
}

@article{Liuf2021,
abstract = {Training Generative Adversarial Networks (GAN) on high-fidelity images usually requires large-scale GPU-clusters and a vast number of training images. In this paper, we study the few-shot image synthesis task for GAN with minimum computing cost. We propose a light-weight GAN structure that gains superior quality on 1024*1024 resolution. Notably, the model converges from scratch with just a few hours of training on a single RTX-2080 GPU, and has a consistent performance, even with less than 100 training samples. Two technique designs constitute our work, a skip-layer channel-wise excitation module and a self-supervised discriminator trained as a feature-encoder. With thirteen datasets covering a wide variety of image domains (The datasets and code are available at: https://github.com/odegeasslbc/FastGAN-pytorch), we show our model's superior performance compared to the state-of-the-art StyleGAN2, when data and computing budget are limited.},
archivePrefix = {arXiv},
arxivId = {2101.04775},
author = {Liu, Bingchen and Zhu, Yizhe and Song, Kunpeng and Elgammal, Ahmed},
eprint = {2101.04775},
file = {:C\:/Users/share/Downloads/Documents/2101.04775.pdf:pdf},
mendeley-groups = {Text to Image},
pages = {1--22},
title = {{Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis}},
url = {http://arxiv.org/abs/2101.04775},
year = {2021}
}

@article{Reimers2020,
abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ($\sim$65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
archivePrefix = {arXiv},
arxivId = {1908.10084},
author = {Reimers, Nils and Gurevych, Iryna},
doi = {10.18653/v1/d19-1410},
eprint = {1908.10084},
file = {:C\:/Users/share/Downloads/Documents/1908.10084.pdf:pdf},
isbn = {9781950737901},
journal = {EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference},
mendeley-groups = {Text to Image},
pages = {3982--3992},
title = {{Sentence-BERT: Sentence embeddings using siamese BERT-networks}},
year = {2020}
}

@article{Gentile1999,
abstract = {We describe a unifying method for proving relative loss bounds for online linear threshold classification algorithms, such as the Perceptron and the Winnow algorithms. For classification problems the discrete loss is used, i.e., the total number of prediction mistakes. We introduce a continuous loss function, called the "linear hinge loss", that can be employed to derive the updates of the algorithms. We first prove bounds w.r.t. the linear hinge loss and then convert them to the discrete loss. We introduce a notion of "average margin" of a set of examples. We show how relative loss bounds based on the linear hinge loss can be converted to relative loss bounds i.t.o. the discrete loss using the average margin.},
author = {Gentile, Claudio and Warmuth, Manfred K.},
file = {:C\:/Users/share/Downloads/Documents/NIPS-1998-linear-hinge-loss-and-average-margin-Paper.pdf:pdf},
isbn = {0262112450},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {Text to Image},
pages = {225--231},
title = {{Linear hinge loss and average margin}},
year = {1999}
}

@article{Zhang2018,
abstract = {While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called 'perceptual losses'? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.},
archivePrefix = {arXiv},
arxivId = {1801.03924},
author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
doi = {10.1109/CVPR.2018.00068},
eprint = {1801.03924},
file = {:C\:/Users/share/Downloads/Documents/1801.03924.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {Text to Image},
number = {1},
pages = {586--595},
title = {{The Unreasonable Effectiveness of Deep Features as a Perceptual Metric}},
year = {2018}
}

@article{Heusel2017,
abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the 'Fr{\'{e}}chet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
archivePrefix = {arXiv},
arxivId = {1706.08500},
author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
eprint = {1706.08500},
file = {:C\:/Users/share/Downloads/Documents/1706.08500.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {6627--6638},
title = {{GANs trained by a two time-scale update rule converge to a local Nash equilibrium}},
volume = {2017-December},
year = {2017}
}

@article{Zhu2017,
abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X  Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y  X and introduce a cycle consistency loss to push F(G(X))  X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
archivePrefix = {arXiv},
arxivId = {1703.10593},
author = {Zhu, Jun Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
doi = {10.1109/ICCV.2017.244},
eprint = {1703.10593},
file = {:C\:/Users/share/Downloads/Documents/1703.10593.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
mendeley-groups = {Text to Image},
pages = {2242--2251},
title = {{Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks}},
volume = {2017-Octob},
year = {2017}
}

@article{Karras2020,
abstract = {The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.},
archivePrefix = {arXiv},
arxivId = {1912.04958},
author = {Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
doi = {10.1109/CVPR42600.2020.00813},
eprint = {1912.04958},
file = {:C\:/Users/share/Downloads/Documents/1912.04958v2.pdf:pdf},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {Text to Image},
pages = {8107--8116},
title = {{Analyzing and improving the image quality of stylegan}},
year = {2020}
}

@misc{cahya,
  author = {Cahya Wirawan},
  title = {{Indonesian BERT base model (uncased)}},
  howpublished = "\url{https://huggingface.co/cahya/bert-base-indonesian-522M}",
  year = {2020}, 
  note = "[Online; accessed 30-August-2021]"
}

@article{He2016,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
file = {:C\:/Users/share/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2016 - Deep residual learning for image recognition.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {Image Captioning},
pages = {770--778},
title = {{Deep residual learning for image recognition}},
volume = {2016-Decem},
year = {2016}
}

@misc{lim2017geometric,
      title={Geometric GAN}, 
      author={Jae Hyun Lim and Jong Chul Ye},
      year={2017},
      eprint={1705.02894},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{lin2014microsoft,
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in
object recognition by placing the question of object recognition in the context
of the broader question of scene understanding. This is achieved by gathering
images of complex everyday scenes containing common objects in their natural
context. Objects are labeled using per-instance segmentations to aid in precise
object localization. Our dataset contains photos of 91 objects types that would
be easily recognizable by a 4 year old. With a total of 2.5 million labeled
instances in 328k images, the creation of our dataset drew upon extensive crowd
worker involvement via novel user interfaces for category detection, instance
spotting and instance segmentation. We present a detailed statistical analysis
of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide
baseline performance analysis for bounding box and segmentation detection
results using a Deformable Parts Model.},
  added-at = {2020-06-07T20:25:18.000+0200},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollr, Piotr},
  biburl = {https://www.bibsonomy.org/bibtex/2f4ab9f41677ee189a8cbc5a92cc0dc74/jan.hofmann1},
  description = {Microsoft COCO: Common Objects in Context},
  interhash = {a3a26c6fe173264a6b812e3b7b4119bd},
  intrahash = {f4ab9f41677ee189a8cbc5a92cc0dc74},
  keywords = {thema:pyramid_scene_parsing},
  note = {cite arxiv:1405.0312Comment: 1) updated annotation pipeline description and figures; 2) added new  section describing datasets splits; 3) updated author list},
  timestamp = {2020-06-07T20:25:18.000+0200},
  title = {Microsoft COCO: Common Objects in Context},
  url = {http://arxiv.org/abs/1405.0312},
  year = 2014
}


