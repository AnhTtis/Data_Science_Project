\documentclass[a4paper,11pt]{ijamas}

% \usepackage[authoryear,longnamesfirst]{natbib}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{makecell}
\usepackage{hyperref}

\renewcommand{\baselinestretch}{1.21}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Indonesian Text-to-Image Synthesis with Sentence-BERT and FastGAN}

\author{\textbf{Made Raharja Surya Mahadi$^1$ and Nugraha Priya Utama$^1$}}

\date{$^1$School of Engineering and Informatics Institute Technology Bandung\\
23520022@std.stei.itb.ac.id\\
utama@informatika.org\\[0.3cm]}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
\noindent \emph{Currently, text-to-image synthesis uses text encoder and image generator architecture. Research on this topic is challenging. This is because of the domain gap between natural language and vision. Nowadays, most research on this topic only focuses on producing a photo-realistic image, but the other domain, in this case, is the language, which is less concentrated. A lot of the current research uses English as the input text. Besides, there are many languages around the world. Bahasa Indonesia, as the official language of Indonesia, is quite popular. This language has been taught in Philipines, Australia, and Japan. Translating or recreating a new dataset into another language with good quality will cost a lot. Research on this domain is necessary because we need to examine how the image generator performs in other languages besides generating photo-realistic images. To achieve this, we translate the CUB dataset into Bahasa using google translate and manually by humans. We use Sentence BERT as the text encoder and FastGAN as the image generator. FastGAN uses lots of skip excitation modules and auto-encoder to generate an image with resolution $512 \times 512 \times 3$, which is twice as bigger as the current state-of-the-art model \cite{Zhang2019}. We also get $4.76 \pm 0.43$ and $46.401$ on Inception Score and Fréchet inception distance, respectively, and comparable with the current English text-to-image generation models. The mean opinion score also gives as $3.22$ out of 5, which means the generated image is acceptable by humans.} Link to source code:  \href{https://github.com/share424/Indonesian-Text-to-Image-synthesis-with-Sentence-BERT-and-FastGAN}{https://github.com/share424/Indonesian-Text-to-Image-synthesis-with-Sentence-BERT-and-FastGAN}

\medskip

\noindent\textbf{Keywords:} Generative Adversarial Networks, Text-to-Image Synthesis, Bahasa Indonesia


\medskip

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Text-to-Image generation is challenging task because there is a domain gap between natural language and vision. Nowadays, researchers use text encoder and image generator architecture to produce photo-realistic images \cite{Reed2016,Zhang2019,Qiao2019,Tsue2020,Hu2021,Xu2018}. They use CNN-LSTM as the text encoder and Generative Adversarial Networks proposed by \citeasnoun{goodfellow14} as the image generator.

\bigskip

Lots of research in this area focus on how to generate a photo-realistic image, but on the other hand, research on the language area is rarely done. There is a thousand language across the world. One of the popular languages in South-East Asia is Bahasa Indonesia. Bahasa Indonesia is the official language of Indonesia and has been taught in the Philippines, Australia, and Japan. This language is also used in other countries such as Canada, Vietnamese, and Ukraine.

\bigskip

The main problem why research in this area is rarely done is because translating or creating a new dataset with good quality for those languages needs much cost. To examine how well text-to-image generation models perform in other languages, we translate the dataset using Google Translate. To increase the translation result, we also manually translate some of them.

\bigskip

To maintain the resolution of the generated image, we use FastGAN \cite{Liuf2021} as the image generator, which is can generate high-resolution image and comparable with StyleGAN2 \cite{Karras2020}. FastGAN proposed a skip excitation module, which is a skip connection layer. This architecture can generate a high-resolution image in just minimal iteration. The discriminator used auto-encoder-like architecture to distinguish real and fake images. To get better results, we also use Sentence-BERT \cite{Reimers2020} as the text encoder. Sentence-BERT can produce text embedding with lower cosine similarity on similar semantic sentence and vice versa.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Works}

Nowadays, text-to-image generation uses a text encoder and image generator architecture. \citeasnoun{Mansimov2016} first proposed a method that uses deep learning, where LSTM as the text encoder and DRAW as the image generator. This method will generate an image by patches based on the attended features on the text. Next, \citeasnoun{Reed2016} uses Generative Adversarial Networks \cite{goodfellow14} as the image generator, and CNN-LSTM as the text encoder. The text encoder generates text embedding and concatenates it with random vector $Z\sim\mathcal{N}(0, 1)$. This latent vector is also used as input in the discriminator, making the discriminator distinguish wrong images.

\bigskip

In order to generate high-resolution image, \citeasnoun{Zhang2019} uses a stack of GAN to generate an image from low features to higher features. They proposed Conditioning Augmentation networks to generate smooth latent vectors and small random perturbations to increase the generated images variations. They also proposed a JCU discriminator to perform the unconditional and conditional tasks. The unconditional GAN discriminator can only distinguish between real and fake images, and the conditional GAN discriminator can distinguish real, fakes, and wrong images. They use three stacks of GAN: the first GAN generates $64 \times 64$ images, the second GAN generates $128 \times 128$ images, and the last GAN generates $256 \times 256$. This method becomes the first and state-of-the-art photo-realistic text to image generation.

\bigskip
% fine-grained details at different sub-regions by paying attention to the relevant words
\citeasnoun{Xu2018} proposed Attention GAN to improve the generated image using word-level attention. They proposed a new Deep Attentional Multimodal Similarity Model (DAMSM) to perform the attention. While \citeasnoun{Qiao2019} use different approaches by improving the learning text-to-image generation idea using their proposed new module, Semantic Text Module (STEM), Global-Local collaborative Attentive Module in Cascaded Image Generators (GLAM), and Semantic Text Regeneration and Alignment Module (STREAM). STEM perform text embedding using global sentence features and word-level features. GLAM is a multi-stage Generator to generate a realistic image. STREAM is an image captioning module that semantically aligns with the given text descriptions.

\bigskip

In the same year, \citeasnoun{Qiao22019} also proposed a new method to perform text-to-image generation, which is inspired by how humans draw a picture from a given description. To mimic this process, they propose LeicaGAN, which consists of three-phase, firstly is multiple priors learning via Textual-Visual Co-Embedding (TVE), next is Imagination via Multiple Priors Aggregations (MPA), and the last one is Creation via Cascaded Attentive Generator (CAG). TVE is a module that generates text embedding with the same common semantic space as an image. MPA is used to imagine what image will be generated. The text embedding and text mask from the TVE module are used to convey visual information about the semantics, textures, colors, shapes, and layouts. CAG module is the actual generator module to produce the image.

\bigskip

To improve the attention results from \citeasnoun{Xu2018}, \citeasnoun{Tsue2020} use BERT \cite{Devlin2019} as the text encoder and CycleGAN \cite{Zhu2017} as the Generator. They proposed a new cyclic design to learn and map the generated image back to text descriptions. This method increases the inception score significantly.

\bigskip

Currently, to generate high-resolution images, researchers use a multi-stage generator. This makes the model size is significantly huge. \citeasnoun{Liu2021} proposed new lightweight GAN architecture that can be trained on a small amount of image and minimum computing cost to generate a high-resolution image on $1024 \times 1024$. They use skip-layer channel-wise excitation module and an auto-encoder-like discriminator to generate a high-resolution image that can be compared to the state-of-the-art StyleGAN2 \cite{Karras2020}.

\bigskip

Many research focuses on generating high-resolution images and semantically correct with the given text description. But research on the text encoder and other languages is necessary. CNN-LSTM has become popular text embedding since \citeasnoun{Reed2016} use that method. Then \citeasnoun{Xu2018} improve the text encoder with word-level attention to improve the generated image details. And then \citeasnoun{Tsue2020} use BERT as text encoder and word embedding. One of the BERT variants that is suitable for sentence embedding is Sentence BERT \cite{Reimers2020}. This variant uses a siamese architecture network to produce text embedding that can then be compared with cosine similarity to find sentence with similar meanings.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Dataset}
In this research, we only use CUB dataset \cite{WahCUB_200_2011} and translated into Bahasa Indonesia using google translate and manually by humans. This dataset contains 200 birds species and almost 12k images. Each image has $10$ captions in English. We split the dataset into $8.855$ as training data and $2.933$ as validation data. This makes the total captions for training data is $88.550$ and $29.330$ for the validation data. Because we use google translate as the translation tool, we cannot expect the translation results grammarly correct. So we were trying to fix this as much as possible manually.

\section{Method}
\subsection{Text Encoder}
\label{section_text_encoder}
To generate an image from a given text description, we need to extract the feature from the given text and use that feature as input to the image generator. The easiest way to achieve this is to use text embedding. Current state-of-the-art language modeling is BERT \cite{Devlin2019}, and their variant for generating text embedding is Sentence BERT \cite{Reimers2020}. This architecture uses a siamese network to perform the training process.

% \begin{figure}
% 	\centering
% 		\includegraphics[scale=0.6]{figs/encoder.png}
% 	\caption{Sentence BERT Architecture. The input sentence in English is "A red bird"}
% 	\label{fig_sbert}
% \end{figure}
\begin{figure*}
	\centering
		\includegraphics[scale=0.6]{figs/fast_gan_generator.drawio.png}
	\caption{Our Generator Architecture. The blue box and arrow represent the same up-sampling structure, the Yellow box represents the features map on the spatial size, and the red box represents the skip-layer excitation module.}
	\label{fig_generator}
\end{figure*}

\bigskip

Usually, BERT produces a sequence of word features. The easiest way to generate a single fixed-vector as the text embedding is to feed the output into the pooling layer. As seen in the figure \ref{fig_generator}, the output from the BERT is forwarded into the pooling layer to generate a fixed vector. We can feed the feature into fully connected layers to generate different vector sizes.

\bigskip

To train this module, first, we need to pre-train the BERT. Currently, there are state-of-the-art pre-trained BERT models in Bahasa Indonesia trained by \cite{cahya} using 522MB of Indonesian Wikipedia. This model is uncased and trained under Masked Language Modeling (MLM) objective. The generated word features vector shape is $786$. We increase the output size to $1024$ using a fully connected layer.

\bigskip

Next, fine-tune the model using siamese network architecture. The loss function plays an important role here because it can determine how well the model performs on a specific task. We use cosine similarity loss with $[0.8, 1)$ on positive pair and $[0.4, 0.6]$ on negative pair. Sentence with same image class is positive pair and vice versa. Because every sentence on the dataset talks about birds, we did not use the non-zero label on the negative pair.

\subsection{Image Generator}
To perform image generation, we need a model that can generate images from given features vector from the text encoder. To achieve this, we can use Generative Adversarial Networks (GAN) \cite{goodfellow14}. GAN can generate high-resolution images from a single latent vector. To build GAN architecture, we need a generator and discriminator. Generator and discriminator need input from the text encoder to distinguish real, fake, and wrong images.

\bigskip

In order to generate high-resolution images, we use FastGAN \cite{Liuf2021} as the image generator. We can train this model with minimal effort to generate high-resolution images. Data augmentation plays an important role in this architecture.

\subsubsection{Generator}
\citeasnoun{Liuf2021} proposed a novel skip-layer excitation module with reformulate the skip-connection idea from ResNet \cite{He2016}. They use channel-wise multiplication between the activations to reduce the computational cost. Since the channel-wise multiplication does not require equal spatial dimension, we can perform a skip-connection layer between longer range resolutions, making a long shortcut to the gradient flow.

\bigskip

Skip excitation module can be defined as:
\begin{equation}
    y = F(x_{low}, \{W_{i}\}) \cdot x_{high}
\end{equation}
Where x is the input feature, and y is the output feature-maps, the function $F$ performs the operation on the lower feature of $x$, and $W_{i}$ is the weights to be learned.

% \begin{figure*}
% 	\centering
% 		\includegraphics[scale=0.6]{figs/fast_gan_generator.drawio.png}
% 	\caption{Our Generator Architecture. Blue box and arrow represent the same up-sampling structure, Yellow box represent the features map on the spatial size, and red box represent the skip-layer excitation module}
% 	\label{fig_generator}
% \end{figure*}

\bigskip

Figure \ref{fig_generator} illustrate our Generator architecture with the output image is $512 \times 512 \times 3$. In order to generate diversity image from a single text description, We use the conditioning variable $\hat{c}$ from CA-net \cite{Zhang2019} and concatenate it with the random vector $z \thicksim \mathcal{N}(0, 1)$. To obtain $\hat{c}$, we can use equation \ref{formula_ca_net}

\begin{equation}
\label{formula_ca_net}
    \hat{c} = \mu + \sigma \odot \omega
\end{equation}

Feed the text embedding $\varphi_{t}$ into a fully connected layer to obtain $\mu$ and $\sigma$ (first-half element is $\mu$ and the rest is $\sigma$). The $\odot$ symbol is element-wise multiplication. The output dimension of the $\hat{c}$ is $128$, and the $z$ dimension is $100$, which makes our latent vector dimension is $228$. We find that normal distribution performs better than the uniform distribution for generating $z$ vector.

\subsubsection{Discriminator}
In order to perform strong regularization of the discriminator, we can treat the discriminator as auto-encoder-like architecture. There are differences with typical auto-encoder architecture, where the discriminator only decodes the image for real images on small resolution. The discriminator also performs a random crop with $\frac{1}{8}$ of its height and width on both input real images and the generated image from the decoder. The decoder only consists of $4\times$ nearest up-sampling layer, $3\times3$ convolution layer, Batch Normalization, and GLU activation function. This technique makes the discriminator learn how to reproduce the input image. 

\bigskip

\begin{figure*}
	\centering
		\includegraphics[scale=0.5]{figs/discriminator.png}
	\caption{Our Discriminator Architecture. Blue box and arrow represent the same down-sampling structure, Yellow box represent the features map on the spatial size, and red box represent the decoder module}
	\label{fig_discriminator}
\end{figure*}

In order to perform conditional GAN, we use the $\mu(\varphi_{t})$ from the CA-net and concatenate it with the extracted feature from the discriminator, where $\varphi_{t}$ is the text embedding. To calculate the total loss of this architecture, we divide it into two conditions. The first is when the input is the real images. We use perceptual loss \cite{Zhang2018} to the auto-encoder-like architecture and hinge adversarial loss \cite{lim2017geometric} to the discriminator output and sum the total. And the last one is when the input is the fake or wrong image, we only use the hinge adversarial loss from the generator output.

\bigskip

We can define the loss function for the generator and discriminator as:
% \begin{equation}
% \label{eq_percept}
%     \mathcal{L}_{percept} = \mathbb{E}_{f\thicksim D_{encode}(x), x\thicksim I_{real}}[\|\mathcal{G}(f) - \mathcal{T}(x) \|]
% \end{equation}
\begin{equation}
\label{eq_loss}
\begin{split}
    \mathcal{L}_{percept} &= \mathbb{E}_{f\thicksim D_{encode}(x), x\thicksim I_{real}}[\|\mathcal{G}(f) - \mathcal{T}(x) \|] \\
    \mathcal{L}_{D} &= -\mathbb{E}_{x\thicksim I_{real}}[min(0, -1 + D(x, \mu))] \\
    &\quad - \mathbb{E}_{x'\thicksim I_{wrong}}[min(0, -1 -D(x', \mu))] \\
    &\quad - \mathbb{E}_{\hat{x}\thicksim G(z, \hat{c})}[min(0, -1 -D(\hat{x}, \mu))] \\
    &\quad + \mathcal{L}_{percept} \\
    \mathcal{L}_{G} &= -\mathbb{E}_{z\thicksim\mathcal{N}}[D(G(z, \hat{c}), \mu)]
\end{split}
\end{equation}
% \begin{equation}
% \label{eq_loss_g}
%     \mathcal{L}_{G} = -\mathbb{E}_{z\thicksim\mathcal{N}}[D(G(z, \hat{c}), \mu)]
% \end{equation}

where $\mathcal{L}_{percept}$ is the perceptual loss from \cite{Zhang2018}, $\mu$ is the variational text embedding from equation \ref{formula_ca_net}.

\section{Experiment}
\subsection{Training Details}
In this section will explain the training details for both text encoder and image generator in this research.

\subsubsection{Text Encoder}
In order to train Sentence BERT, we need pre-trained BERT models. In this research, we use \cite{cahya} pre-trained BERT on Bahasa Indonesia language. We need appropriate labels for both positive and negative pairs to get better results. In the section \ref{section_text_encoder}, we explain that we use $[0.8, 1)$ as the positive pair label and $[0.4, 0.6]$ as the negative pair labels. To train Sentence BERT, we pass both sentences in the pair to the same networks, compare them with cosine similarity, and perform mean-squared error to calculate the loss value. We train our models for 10 epochs and save the best model only.

\subsubsection{Image Generator}
We perform both unconditional and conditional tasks to compare the results. We train our models within $50.000$ iterations and batch size 10. First, we encode all text descriptions into their embedding vector $\varphi_{t}$.

\bigskip

\textbf{Unconditional}. In this part, we only feed the generator with random vector $z \thicksim \mathcal{N}(0, 1)$ and conditioning text embedding $\hat{c}$. Because every image has 10 captions, we select one randomly and feed it into CA-net. The real and fake images are then augmented with random color brightness, saturation, contrast, and translation. Next, we calculate the loss value to update the discriminator. For the real images, we perform both perceptual and hinge loss from equation \ref{eq_loss} (without the wrong image part).

\bigskip

\textbf{Conditional}. We perform the same things with the generator on the unconditional task for this task. We feed real, wrong, and fake images with the conditioning augmentation vector $\mu$ from the CA-net in the discriminator. We calculate the loss using the equation \ref{eq_loss}.

\subsection{Evaluation}
To evaluate generative models is quite challenging because calculating the suitability of the resulting image with the given text is challenging to do. So we perform both quantitative and qualitative evaluation metrics.

\bigskip

\textbf{Inception Score}. This method calculates the distribution of the generated images. It makes us can calculate how the objectness of the generated image. Inception score is often used as an evaluation metric on generative models.

\bigskip

\textbf{Fréchet inception distance}. On the other hand, Fréchet inception distance \cite{Heusel2017} has a better approach. This method calculates the distance between training and fake images data distribution. However, this method still cannot evaluate how appropriate the generated image is with the given text description.

\bigskip

\textbf{Mean Opinion Score}. In order to perform qualitative evaluation, we evaluate our models by conducting a survey. The respondent will be given 10 pair of fake images and their text description. Every fake image consists of 4 generated images. They will select one of them and then do scoring between 0 and 5. While 0 means the generated image is not appropriate with the given text description and the image is not clear, and 5 means the generated image is appropriate with the given text description and the generated image is clear.


\section{Results}
We train both text encoder and image generator separately and then combine them on evaluation. To investigate our models, we also compare them with current state-of-the-art text-to-image synthesis models to examine how the text encoder performs in a different language, especially Bahasa Indonesia.

\bigskip

\begin{figure}
	\centering
	\begin{tikzpicture}
    	\begin{axis}[
        xlabel=$Epoch$,
        ylabel=$Cosine Pearson$,
        xmin=0, xmax=9,
        ymin=0.76, ymax=0.83,
        xtick={0, 1, 2, 3, 4, 5, 6, 7, 8, 9},
        ytick={0.76,0.77,...,0.83}
                ]
        \addplot[smooth,mark=*,blue] plot coordinates {
            (0,0.7700286279133701)
            (1,0.8071784439315165)
            (2,0.819403997499715)
            (3,0.8239669705573398)
            (4,0.8240654953735065)
            (5,0.823491266402737)
            (6,0.8209177960202888)
            (7,0.8187754799891874)
            (8,0.82)
            (9,0.819403997499715)
        };
        \end{axis}
	\end{tikzpicture}
% 		\includegraphics[scale=0.5]{figs/encoder_training_result.png}
	\caption{Text encoder training results}
	\label{fig_encoder_training_result}
\end{figure}


As shown in figure \ref{fig_encoder_training_result}, our text encoder reaches the best score at epoch 3 within 0.82 on cosine pearson. This means our text encoder can produce text embedding with higher cosine similarity on the similar sentence and vice versa.

\begin{center}
    \begin{table}[t]
    \caption{Evaluation results both on Inception Score and Fréchet inception distance compared with other English text-to-image generation models on CUB datasets}
    \begin{center}
    \begin{tabular}{|l|c|c|}
    \hline
    \textbf{Method} & \textbf{Inception Score} & \textbf{FID} \\
    \hline
    AttGAN \cite{Xu2018} & $4.36 \pm 0.03$ & 23.98 \\
    StackGAN \cite{Zhang2019} & $4.04 \pm 0.05$ & \textbf{15.30} \\
    MirrorGAN \cite{Qiao2019} & $4.56 \pm 0.05$ & - \\
    CycleGAN+BERT \cite{Tsue2020} & \textbf{5.92} & - \\
    SSA-GAN \cite{Hu2021} & $5.17 \pm 0.08$ & 15.61 \\
    \hline
    Our Unconditional & $4.75 \pm 0.19$ & 99.12 \\
    Our Conditional & $4.76 \pm 0.43$ & $46.401$ \\
    \hline
    \end{tabular}
    
    \label{quantitative_evaluation_results}
    \end{center}
    \end{table}
\end{center}

% \begin{table}[cols=3,pos=h]
% \label{quantitative_evaluation_results}
% \caption{Evaluation results both on Inception Score and Fréchet inception distance compared with other English text-to-image generation models on CUB datasets}\label{tbl1}
% \begin{tabular*}{\tblwidth}{@{} LCC @{} }
% \toprule
% Method & IS & FID\\
% \midrule
% AttGAN \citep{Xu2018} & $4.36 \pm 0.03$ & 23.98 \\
% StackGAN \citep{Zhang2019} & $4.04 \pm 0.05$ & \textbf{15.30} \\
% MirrorGAN \citep{Qiao2019} & $4.56 \pm 0.05$ & - \\
% CycleGAN+BERT \citep{Tsue2020} & \textbf{5.92} & - \\
% SSA-GAN \citep{Hu2021} & $5.17 \pm 0.08$ & 15.61 \\
% \hline
% Our Unconditional & $4.75 \pm 0.19$ & 99.12 \\
% Our Conditional & $4.76 \pm 0.43$ & $46.401$ \\
% \bottomrule
% \end{tabular*}
% \end{table}

The fréchet inception distance and inception score of our model compared with other English text-to-image synthesis are shown in Table \ref{quantitative_evaluation_results}. Our model easily beats StackGAN, AttentionGAN, and MirrorGAN on the inception score, which means our model can generate high-quality objects. However, our FID is higher than other models, which means our models, compared to the training data, have different data distribution and makes our generated images different from our datasets.

\bigskip

The Inception Score for our unconditional GAN is slightly lower than our conditional GAN but has a much higher FID. This means our unconditional and conditional GAN has the same objectness but different image data distribution against the datasets.

\begin{center}
    \begin{table}[t]
    \caption{Examples of images generated by our Unconditional and Conditional GAN. The input sentences in English is "A red bird perched on a tree branch", "This bird has yellow wing and black beak", "A bird with blue wing and red tail", "A blackbird perched on a tree", and "A yellow bird on the water"}
    \begin{center}
    \begin{tabular}{lcc}
    \hline
    \textbf{Sentence} & \textbf{Unconditional} & \textbf{Conditional} \\
    \hline
   \makecell[bl]{\scriptsize seekor burung \\\scriptsize merah yang \\\scriptsize hinggap di \\\scriptsize cabang pohon} & \includegraphics[width=0.4\textwidth]{figs/unconditional/output_0.jpg} & \includegraphics[width=0.4\textwidth]{figs/conditional/output_0.jpg} \\
 \hline
 \makecell[bl]{\scriptsize burung ini \\\scriptsize memiliki sayap \\\scriptsize berwarna kuning \\\scriptsize dengan paruh \\\scriptsize berwara hitam} & \includegraphics[width=0.4\textwidth]{figs/unconditional/output_1.jpg} & \includegraphics[width=0.4\textwidth]{figs/conditional/output_1.jpg} \\
 \hline
 \makecell[bl]{\scriptsize seekor burung \\\scriptsize dengan sayap \\\scriptsize berwarna biru \\\scriptsize dan ekor \\\scriptsize berwarna merah} & \includegraphics[width=0.4\textwidth]{figs/unconditional/output_7.jpg} & \includegraphics[width=0.4\textwidth]{figs/conditional/output_7.jpg} \\
 \hline
 \makecell[bl]{\scriptsize seekor burung \\\scriptsize hitam sedang \\\scriptsize bertengger di \\\scriptsize atas pohon} & \includegraphics[width=0.4\textwidth]{figs/unconditional/output_4.jpg} & \includegraphics[width=0.4\textwidth]{figs/conditional/output_4.jpg} \\
 \hline
 \makecell[bl]{\scriptsize seekor burung \\\scriptsize kuning di \\\scriptsize atas air} & \includegraphics[width=0.4\textwidth]{figs/unconditional/test_uncond_8.jpg} & \includegraphics[width=0.4\textwidth]{figs/conditional/test_cond_8.jpg} \\
    \hline
    \end{tabular}
    \label{example_generated_images}
    \end{center}
    \end{table}
\end{center}

% \begin{table*}[cols=3,pos=h]
% \label{example_generated_images}
% \caption{Examples of images generated by our Unconditional and Conditional GAN. The input sentences in English is "A red bird perched on a tree branch", "This bird has yellow wing and black beak", "A bird with blue wing and red tail", "A blackbird perched on a tree", and "A yellow bird on the water"}\label{tbl1}
% \begin{tabular*}{\tblwidth}{@{} CCC @{} }
% \toprule
% Sentence & Unconditional & Conditional\\
% \midrule
%  \makecell[br]{seekor burung \\merah yang \\hinggap di \\cabang pohon} & \includegraphics[width=0.42\textwidth]{figs/unconditional/output_0.jpg} & \includegraphics[width=0.42\textwidth]{figs/conditional/output_0.jpg} \\
%  \hline
%  \makecell[br]{burung ini \\memiliki sayap \\berwarna kuning \\dengan paruh \\berwara hitam} & \includegraphics[width=0.42\textwidth]{figs/unconditional/output_1.jpg} & \includegraphics[width=0.42\textwidth]{figs/conditional/output_1.jpg} \\
%  \hline
%  \makecell[br]{seekor burung \\dengan sayap \\berwarna biru \\dan ekor \\berwarna merah} & \includegraphics[width=0.42\textwidth]{figs/unconditional/output_7.jpg} & \includegraphics[width=0.42\textwidth]{figs/conditional/output_7.jpg} \\
%  \hline
%  \makecell[br]{seekor burung \\hitam sedang \\bertengger di \\atas pohon} & \includegraphics[width=0.42\textwidth]{figs/unconditional/output_4.jpg} & \includegraphics[width=0.42\textwidth]{figs/conditional/output_4.jpg} \\
%  \hline
%  \makecell[br]{seekor burung \\kuning di \\atas air} & \includegraphics[width=0.42\textwidth]{figs/unconditional/test_uncond_8.jpg} & \includegraphics[width=0.42\textwidth]{figs/conditional/test_cond_8.jpg} \\
% \bottomrule
% \end{tabular*}
% \end{table*}

As shown in Table \ref{example_generated_images}, we find out that our unconditional GAN is on collapse modes. This is a common problem on generative adversarial networks because the generator always tries to produce a similar image to fool the discriminator. This can happen when the discriminator cannot distinguish between fake and real images from different inputs. On the other hand, the conditional GAN can fix this problem using the conditioning augmentation from CA-net as input for the discriminator.

\bigskip

The conditional GAN also produces some novel images. The generator tries to generate a red bird with blue wings on the third output, which does not exist in the datasets. However, there is a red bird with black wings, so the generator is trying to change the wing color to blue. That makes the wing dark blue. Furthermore, there is no yellow bird on the water in the dataset, but the generator is trying to output that in the last sentence. So the generator is trying to change the existing yellow bird background with something like water.

\begin{center}
    \begin{table}[htbp]
    \caption{The mean opinion score for both our unconditional and conditional GAN}
    \begin{center}
    \begin{tabular}{|l|l|}
    \hline
    \textbf{Method} & \textbf{Mean Opinion Score} \\
    \hline
    Our Unconditional & 1.41 \\
    Our Conditional & \textbf{3.22} \\
    \hline
    \end{tabular}
    
    \label{qualitative_evaluation_results}
    \end{center}
    \end{table}
\end{center}

% \begin{table}[cols=2,pos=h]
% \label{qualitative_evaluation_results}
% \caption{The mean opinion score for both our unconditional and conditional GAN}\label{tbl1}
% \begin{tabular*}{\tblwidth}{@{} LC @{} }
% \toprule
% Method & Mean Opinion Score\\
% \midrule
% Our Unconditional & 1.41 \\
% Our Conditional & \textbf{3.22} \\
% \bottomrule
% \end{tabular*}
% \end{table}

In order to perform a qualitative evaluation, we conduct a survey and average the results. As shown in Table \ref{qualitative_evaluation_results}, our conditional GAN easily beats our unconditional GAN. The mean opinion score of the conditional GAN is 3.22, which means our models are still acceptable by humans.

\section{Conclusion}
This research investigates the text-to-image synthesis performance in different languages, especially Bahasa Indonesia. To break through the gap between natural language and vision, we use the current state-of-the-art sentence embedding, Sentence BERT as the text encoder. In order to generate photo-realistic images within minimal training effort, we use FastGAN as the image generator. We implement the Conditioning Augmentation network to make the generated images more diverse and use its output as input for generator and discriminator. This makes our conditional GAN perform superior to generating novel images. Our proposed architecture can generate high-resolution images using Bahasa Indonesia as the input language.

\bigskip

For future works, we suggest properly translating the datasets to produce a high-quality language model. We find out that text embeddings play an essential role in generating the image details. Implementing weighted sum on the discriminator loss also can produce high-quality images. AttentionGAN \cite{Xu2018} become another alternative to produce high-quality details. Using another challenging dataset such as COCO datasets \cite{lin2014microsoft} or even ImageNet datasets can evaluate the model performance on different image distributions. It would be helpful to train the model longer to increase the inception score and FID. Trying other GAN variants also can produce different image resolutions, such as StyleGAN2 \cite{Karras2020}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section*{Acknowledgment}

% This work was supported by the Funding Agency xpto.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{dcu}
\bibliography{IJAMAS_example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
