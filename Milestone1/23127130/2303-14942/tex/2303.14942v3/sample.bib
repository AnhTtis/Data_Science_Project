

@article{Kohler2001NonparametricRE,
  title={Nonparametric regression estimation using penalized least squares},
  author={Michael Kohler and Adam Krzyżak},
  journal={IEEE Trans. Inf. Theory},
  year={2001},
  volume={47},
  pages={3054-3059}
}

@article{Cucker2001OnTM,
  title={On the mathematical foundations of learning},
  author={Felipe Cucker and Stephen Smale},
  journal={Bulletin of the American Mathematical Society},
  year={2001},
  volume={39},
  pages={1-49}
}

@article{Caponnetto2007OptimalRF,
  title={Optimal Rates for the Regularized Least-Squares Algorithm},
  author={Andrea Caponnetto and Ernesto de Vito},
  journal={Foundations of Computational Mathematics},
  year={2007},
  volume={7},
  pages={331-368}
}

@inproceedings{Steinwart2009OptimalRF,
  title={Optimal Rates for Regularized Least Squares Regression},
  author={Ingo Steinwart and Don R. Hush and Clint Scovel},
  booktitle={Annual Conference Computational Learning Theory},
  year={2009}
}

@inproceedings{Steinwart2008SupportVM,
  title={Support Vector Machines},
  author={Ingo Steinwart and Andreas Christmann},
  booktitle={Information Science and Statistics},
  year={2008}
}

@article{fischer2020_SobolevNorm,
  title = {Sobolev Norm Learning Rates for Regularized Least-Squares Algorithms},
  author = {Fischer, Simon-Raphael and Steinwart, Ingo},
  year = {2020},
  journal = {Journal of Machine Learning Research},
  volume = {21},
  pages = {205:1-205:38},
  arxivid = {1702.07254},
  keywords = {KRR}
}

@article{rastogi2017_OptimalRates,
  title = {Optimal Rates for the Regularized Learning Algorithms under General Source Condition},
  author = {Rastogi, Abhishake and Sampath, Sivananthan},
  year = {2017},
  journal = {Frontiers in Applied Mathematics and Statistics},
  volume = {3},
  arxivid = {1611.01900}
}

@article{rosasco2005_SpectralMethods,
  title = {Spectral Methods for Regularization in Learning Theory},
  author = {Rosasco, Lorenzo and De Vito, Ernesto and Verri, Alessandro},
  year = {2005},
  journal = {DISI, Universita degli Studi di Genova, Italy, Technical Report DISI-TR-05-18}
}

@article{bauer2007_RegularizationAlgorithms,
  title = {On Regularization Algorithms in Learning Theory},
  author = {Bauer, F. and Pereverzyev, S. and Rosasco, L.},
  year = {2007},
  journal = {Journal of complexity},
  volume = {23},
  number = {1},
  pages = {52--72},
  publisher = {{Elsevier}},
  keywords = {saturation,Spectral algorithm}
}

@article{gerfo2008_SpectralAlgorithms,
  title = {Spectral Algorithms for Supervised Learning},
  author = {Gerfo, L. Lo and Rosasco, Lorenzo and Odone, Francesca and Vito, E. De and Verri, Alessandro},
  year = {2008},
  journal = {Neural Computation},
  volume = {20},
  number = {7},
  pages = {1873--1897},
  publisher = {{MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info \ldots}},
  keywords = {Spectral algorithm}
}

@article{mendelson2010_RegularizationKernel,
  title = {Regularization in Kernel Learning},
  author = {Mendelson, Shahar and Neeman, Joseph},
  year = {2010},
  month = feb,
  journal = {The Annals of Statistics},
  volume = {38},
  number = {1},
  pages = {526--565},
  publisher = {{Institute of Mathematical Statistics}},
  keywords = {Empirical processes,least-squares,Model selection,regression,Regulation,‎reproducing kernel Hilbert ‎space}
}



@techreport{caponnetto2006optimal,
  title={Optimal rates for regularization operators in learning theory},
  author={Caponnetto, Andrea},
  year={2006},
  institution={MASSACHUSETTS INST OF TECH CAMBRIDGE COMPUTER SCIENCE AND ARTIFICIAL~…}
}

@article{blanchard2018_OptimalRates,
  title = {Optimal Rates for Regularization of Statistical Inverse Learning Problems},
  author = {Blanchard, G. and M{\"u}cke, Nicole},
  year = {2018},
  journal = {Foundations of Computational Mathematics},
  volume = {18},
  pages = {971--1013},
  arxivid = {1604.04054}
}

@article{dicker2017_KernelRidge,
  title = {Kernel Ridge vs. Principal Component Regression: {{Minimax}} Bounds and the Qualification of Regularization Operators},
  author = {Dicker, L. and Foster, Dean Phillips and Hsu, Daniel J.},
  year = {2017},
  journal = {Electronic Journal of Statistics},
  volume = {11},
  pages = {1022--1047},
  keywords = {KRR,saturation}
}

@article{lin2018_OptimalRates,
  title = {Optimal Rates for Spectral Algorithms with Least-Squares Regression over {{Hilbert}} Spaces},
  author = {Lin, Junhong and Rudi, Alessandro and Rosasco, L. and Cevher, V.},
  year = {2018},
  journal = {Applied and Computational Harmonic Analysis},
  volume = {48},
  pages = {868--890},
  arxivid = {1801.06720},
  keywords = {Spectral algorithm}
}

@article{lin2020_OptimalConvergence,
  title = {Optimal Convergence for Distributed Learning with Stochastic Gradient Methods and Spectral Algorithms.},
  author = {Lin, Junhong and Cevher, Volkan},
  year = {2020},
  journal = {Journal of Machine Learning Research},
  volume = {21},
  pages = {147--1}
}

@inproceedings{steinwart2009_OptimalRates,
  title = {Optimal Rates for Regularized Least Squares Regression},
  booktitle = {{{COLT}}},
  author = {Steinwart, Ingo and Hush, D. and Scovel, C.},
  year = {2009},
  pages = {79--93},
  keywords = {Empirical processes}
}


@article{PillaudVivien2018StatisticalOO,
  title={Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes},
  author={Pillaud-Vivien, Loucas and Rudi, Alessandro and Bach, Francis},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}



@article{Celisse2020AnalyzingTD,
  title={Analyzing the discrepancy principle for kernelized spectral filter learning algorithms},
  author={Alain Celisse and Martin Wahl},
  journal={J. Mach. Learn. Res.},
  year={2020},
  volume={22},
  pages={76:1-76:59}
}

@article{Liu2022StatisticalOO,
  title={Statistical Optimality of Divide and Conquer Kernel-based Functional Linear Regression},
  author={Jiading Liu and Lei Shi},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.10968}
}

@article{steinwart2012_MercerTheorem,
  title = {Mercer's Theorem on General Domains: {{On}} the Interaction between Measures, Kernels, and {{RKHSs}}},
  shorttitle = {Mercer's Theorem on General Domains},
  author = {Steinwart, Ingo and Scovel, C.},
  year = {2012},
  journal = {Constructive Approximation},
  volume = {35},
  number = {3},
  pages = {363--417},
  publisher = {{Springer}},
  keywords = {Kernel}
}

@inproceedings{Cui2021GeneralizationER,
  title={Generalization Error Rates in Kernel Regression: The Crossover from the Noiseless to Noisy Regime},
  author={Hugo Cui and Bruno Loureiro and Florent Krzakala and Lenka Zdeborov'a},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{Bordelon2020SpectrumDL,
  title={Spectrum Dependent Learning Curves in Kernel Regression and Wide Neural Networks},
  author={Blake Bordelon and Abdulkadir Canatar and Cengiz Pehlevan},
  booktitle={ICML},
  year={2020}
}

@article{Spigler2020AsymptoticLC,
  title={Asymptotic learning curves of kernel methods: empirical data versus teacher–student paradigm},
  author={Stefano Spigler and Mario Geiger and Matthieu Wyart},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  year={2020},
  volume={2020}
}


@article{jun2019kernel,
  title={Kernel truncated randomized ridge regression: Optimal rates and low noise acceleration},
  author={Jun, Kwang-Sung and Cutkosky, Ashok and Orabona, Francesco},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@book{adams2003_SobolevSpaces,
  title = {Sobolev Spaces},
  author = {Adams, Robert A and Fournier, John JF},
  year = {2003},
  publisher = {{Elsevier}}
}

@book{sawano2018theory,
    title = {Theory of Besov spaces},
    author = {Sawano, Yoshihiro},
    volume = {56},
    year = {2018},
    publisher = {Springer}
}

@book{adams1975sobolev,
  title={Sobolev Spaces. Adams},
  author={Adams, R.A.},
  series={Pure and applied mathematics},
  url={https://books.google.co.uk/books?id=JxzpSAAACAAJ},
  year={1975},
  publisher={Academic Press}
}

@inbook{wendland_2004, place={Cambridge}, series={Cambridge Monographs on Applied and Computational Mathematics}, title={Native spaces}, DOI={10.1017/CBO9780511617539.011}, booktitle={Scattered Data Approximation}, publisher={Cambridge University Press}, author={Wendland, Holger}, year={2004}, pages={133–171}, collection={Cambridge Monographs on Applied and Computational Mathematics}}

@article{Smale2007LearningTE,
  title={Learning Theory Estimates via Integral Operators and Their Approximations},
  author={Stephen Smale and Ding-Xuan Zhou},
  journal={Constructive Approximation},
  year={2007},
  volume={26},
  pages={153-172}
}

@book{tsybakov2009_IntroductionNonparametric,
  title = {Introduction to Nonparametric Estimation},
  author = {Tsybakov, Alexandre B.},
  year = {2009},
  series = {Springer Series in Statistics},
  edition = {1st},
  publisher = {{Springer}},
  address = {{New York ; London}},
  langid = {english},
  lccn = {QA278.8 .T79 2009},
  keywords = {Estimation theory,Nonparametric statistics},
  annotation = {OCLC: ocn300399286}
}

@article{Lian2021DistributedLF,
  title={Distributed learning for sketched kernel regression},
  author={Heng Lian and Jiamin Liu and Zengyan Fan},
  journal={Neural networks : the official journal of the International Neural Network Society},
  year={2021},
  volume={143},
  pages={
          368-376
        }
}

@article{Guo2017LearningTO,
  title={Learning theory of distributed spectral algorithms},
  author={Zheng-Chu Guo and Shaobo Lin and Ding-Xuan Zhou},
  journal={Inverse Problems},
  year={2017},
  volume={33}
}

@inproceedings{Talwai2022OptimalLR,
  title={Optimal Learning Rates for Regularized Least-Squares with a Fourier Capacity Condition},
  author={Prem M. Talwai and David Simchi-Levi},
  year={2022}
}


@article{Li2022OptimalRF,
  title={Optimal rates for regularized conditional mean embedding learning},
  author={Li, Zhu and Meunier, Dimitri and Mollenhauer, Mattes and Gretton, Arthur},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={4433--4445},
  year={2022}
}

@book{wainwright2019_HighdimensionalStatistics,
  title = {High-Dimensional Statistics: {{A}} Non-Asymptotic Viewpoint},
  author = {Wainwright, Martin J.},
  year = {2019},
  series = {Cambridge {{Series}} in {{Statistical}} and {{Probabilistic Mathematics}}},
  publisher = {{Cambridge University Press}}
}

@article{ThomasAgnan1996ComputingAF,
  title={Computing a family of reproducing kernels for statistical applications},
  author={Christine Thomas-Agnan},
  journal={Numerical Algorithms},
  year={1996},
  volume={13},
  pages={21-32}
}

@article{Jin2022MinimaxOK,
  title={Minimax Optimal Kernel Operator Learning via Multilevel Training},
  author={Jikai Jin and Yiping Lu and Jos{\'e} H. Blanchet and Lexing Ying},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.14430}
}

@inproceedings{li2023_SaturationEffect,
  title = {On the Saturation Effect of Kernel Ridge Regression},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Li, Yicheng and Zhang, Haobo and Lin, Qian},
  year = {2023},
  month = feb,
  abstract = {The saturation effect refers to the phenomenon that the kernel ridge regression (KRR) fails to achieve the information theoretical lower bound when the smoothness of the underground truth function exceeds certain level. The saturation effect has been widely observed in practices and a saturation lower bound of KRR has been conjectured for decades. In this paper, we provide a proof of this long-standing conjecture.},
  langid = {english}
}


@book{tartar2007introduction,
  title={An introduction to Sobolev spaces and interpolation spaces},
  author={Tartar, Luc},
  volume={3},
  year={2007},
  publisher={Springer Science \& Business Media}
}

@article{dieuleveut2016nonparametric,
  title={NONPARAMETRIC STOCHASTIC APPROXIMATION WITH LARGE STEP-SIZES1},
  author={Dieuleveut, Aymeric and Bach, Francis},
  journal={THE ANNALS},
  volume={44},
  number={4},
  pages={1363--1399},
  year={2016}
}


@article{Mcke2018ParallelizingSR,
  title={Parallelizing Spectrally Regularized Kernel Algorithms},
  author={Nicole M{\"u}cke and Gilles Blanchard},
  journal={J. Mach. Learn. Res.},
  year={2018},
  volume={19},
  pages={30:1-30:29}
}

@article{Zhang2013DivideAC,
  title={Divide and conquer kernel ridge regression: a distributed algorithm with minimax optimal rates},
  author={Yuchen Zhang and John C. Duchi and Martin J. Wainwright},
  journal={J. Mach. Learn. Res.},
  year={2013},
  volume={16},
  pages={3299-3340}
}


@article{Lin2016DistributedLW,
  title={Distributed learning with regularized least squares},
  author={Lin, Shao-Bo and Guo, Xin and Zhou, Ding-Xuan},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={92},
  pages={1--31},
  year={2017}
}

@article{smola2000_RegularizationDotproduct,
  title = {Regularization with Dot-Product Kernels},
  author = {Smola, Alex and Ov{\'a}ri, Zolt{\'a}n and Williamson, Robert C.},
  year = {2000},
  journal = {Advances in neural information processing systems},
  volume = {13}
}

@book{dai2013_ApproximationTheory,
  title = {Approximation Theory and Harmonic Analysis on Spheres and Balls},
  author = {Dai, Feng and Xu, Yuan},
  year = {2013},
  series = {Springer {{Monographs}} in {{Mathematics}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4614-6660-4},
  isbn = {978-1-4614-6659-8 978-1-4614-6660-4},
  langid = {english}
}

@inproceedings{cho2009_KernelMethods,
  title = {Kernel Methods for Deep Learning},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Cho, Youngmin and Saul, Lawrence},
  editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. and Williams, C. and Culotta, A.},
  year = {2009},
  volume = {22},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{jacot2018_NeuralTangent,
  title = {Neural Tangent Kernel: {{Convergence}} and Generalization in Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}}
}

@article{bach2017_BreakingCurse,
  title = {Breaking the Curse of Dimensionality with Convex Neural Networks},
  author = {Bach, Francis},
  year = {2017},
  journal = {The Journal of Machine Learning Research},
  volume = {18},
  number = {1},
  pages = {629--681},
  publisher = {{JMLR. org}}
}

@article{Yao2007OnES,
  title={On Early Stopping in Gradient Descent Learning},
  author={Y. Yao and Lorenzo Rosasco and Andrea Caponnetto},
  journal={Constructive Approximation},
  year={2007},
  volume={26},
  pages={289-315}
}

@misc{beaglehole2022_KernelRidgeless,
  title = {Kernel Ridgeless Regression Is Inconsistent in Low Dimensions},
  author = {Beaglehole, Daniel and Belkin, Mikhail and Pandit, Parthe},
  year = {2022},
  month = jun,
  number = {arXiv:2205.13525},
  eprint = {2205.13525},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.13525},
  abstract = {We show that kernel interpolation for a large class of shift-invariant kernels is inconsistent in fixed dimension, even with bandwidth adaptive to the training set.},
  archiveprefix = {arXiv}
}

@article{JMLR:v23:21-0570,
  author  = {Wenjia Wang and Bing-Yi Jing},
  title   = {Gaussian process regression: Optimality, robustness, and relationship with kernel ridge regression},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {193},
  pages   = {1--67},
  url     = {http://jmlr.org/papers/v23/21-0570.html}
}

@book{edmunds_triebel_1996, place = {Cambridge}, series = {Cambridge Tracts in Mathematics}, title = {Function Spaces, Entropy Numbers, Differential Operators}, DOI = {10.1017/CBO9780511662201}, publisher = {Cambridge University Press}, author = {Edmunds, D. E. and Triebel, H.}, year = {1996}, collection = {Cambridge Tracts in Mathematics} }

@article{lin2017optimal,
  title={Optimal rates for multi-pass stochastic gradient methods},
  author={Lin, Junhong and Rosasco, Lorenzo},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={3375--3421},
  year={2017},
  publisher={JMLR. org}
}

@article{lin2020convergences,
  title={Convergences of regularized algorithms and stochastic gradient methods with random projections},
  author={Lin, Junhong and Cevher, Volkan},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={20},
  pages={1--44},
  year={2020}
}
