\subsection{Assumptions}\label{section assumption}

This subsection lists the standard assumptions that frequently appear in related literature.
\begin{assumption}[Eigenvalue decay rate (EDR)]\label{ass EDR}
 Suppose that the eigenvalue decay rate (EDR) of $\mathcal{H}$ is $\beta > 1$, i.e, there are positive constants $c$ and $C$ such that 
 \begin{displaymath}
   c i^{- \beta} \le \lambda_{i} \le C i^{-\beta}, \quad  \forall i \in N.
 \end{displaymath}
\end{assumption}
Note that the eigenvalues $\lambda_{i}$ and EDR are only determined by the marginal distribution $\mu$ and the RKHS $\mathcal{H}$. The polynomial eigenvalue decay rate assumption is standard in related literature and is also referred to as the capacity condition or effective dimension
condition \citep[etc.]{caponnetto2006optimal,Caponnetto2007OptimalRF}.
\newline

We say that $\mathcal{H}$ has the embedding property of order $\alpha\in [\frac{1}{\beta},1]$, if there is a constant $0 < A < \infty$ such that
\begin{equation}\label{embedding property 1.12}
    \left\|[\mathcal{H}]^\alpha \hookrightarrow L^{\infty}(\mathcal{X},\mu)\right\| \leq A,
\end{equation}
where $\|\cdot\| $ denotes the operator norm of the embedding.

In fact, for any $\alpha > 0$, we can define $M_{\alpha} $ as the smallest constant $A > 0$ such that 
\begin{equation}
    \label{eq:EMB_Eigenvalues}
  \sum_{i \in N} \lambda_i^\alpha e_i^2(x) \leq A^2, \quad \mu \text {-a.e. } x \in \mathcal{X},
\end{equation}
if there is no such constant, set $ M_{\alpha} = \infty$. Then \citet[Theorem 9]{fischer2020_SobolevNorm} shows that for $ \alpha > 0$,
\begin{displaymath}
  \left\|[\mathcal{H}]^\alpha \hookrightarrow L^{\infty}(\mathcal{X},\mu)\right\|=M_{\alpha}.
\end{displaymath}
    
Note that since $ \sup_{x \in \mathcal{X}} k(x,x) \le \kappa^{2} $, $M_{\alpha} \le \kappa < \infty$ is always true for $\alpha \ge 1$. In addition, \citet[Lemma 10]{fischer2020_SobolevNorm} also shows that $\alpha$ can not be less than $\frac{1}{\beta}$. By the inclusion relation of interpolation spaces, it is clear that if $\mathcal{H}$ has the embedding property of order $\alpha$, then it has the embedding property of order $\alpha^{\prime}$ for any $\alpha^{\prime} \geq \alpha$. Thus, we may introduce the following assumption: 

\begin{assumption}[Embedding index]\label{assumption embedding}
Suppose that there exists $\alpha_{0} > 0$, such that
\begin{displaymath}
  \alpha_{0} = \inf\left\{ \alpha \in [\frac{1}{\beta},1] :  \left\|[\mathcal{H}]^\alpha \hookrightarrow L^{\infty}(\mathcal{X},\mu)\right\| < \infty  \right\},
\end{displaymath}
and we refer to $\alpha_{0}$ as the \textit{embedding index} of an RKHS $\mathcal{H}$.
\end{assumption}

 Note that $ \mathcal{H}$ has the embedding property of order $\alpha$ for any $\alpha > \alpha_{0}$. This directly implies that all the functions in $[\mathcal{H}]^\alpha$ are $\mu \text {-a.e.}$ bounded, $\alpha > \alpha_{0}$. However, the embedding property may not hold for $\alpha = \alpha_{0}$.
 
\begin{assumption}[Source condition]\label{ass source condition}
  For $s > 0 $, there is a constant $R > 0 $ such that $f_{\rho}^{*} \in [\mathcal{H}]^{s}$ and
  \begin{displaymath}
    \| f_{\rho}^{*} \|_{[\mathcal{H}]^{s}} \le R.
  \end{displaymath}
\end{assumption}
Functions in $[\mathcal{H}]^{s}$ with smaller $s$ are less smooth, which will be harder for an algorithm to estimate.
\begin{assumption}[Moment of error]\label{ass mom of error}
  The noise $ \epsilon \coloneqq y - f_{\rho}^{*}(x)$ satisfies that there are constants $ \sigma, L > 0$ such that for any $ m \ge 2$,
  \begin{displaymath}
      \mathbb{E}\left(|\epsilon|^m \mid x\right) \leq \frac{1}{2} m ! \sigma^2 L^{m-2}, \quad \mu \text {-a.e. } x \in \mathcal{X}.
  \end{displaymath}
\end{assumption}
This is a standard assumption to control the noise such that the tail probability decays fast \citep{lin2020_OptimalConvergence,fischer2020_SobolevNorm}. It is satisfied for, for instance, the Gaussian noise with bounded variance or sub-Gaussian noise. Some literature (e.g., \citealt{steinwart2009_OptimalRates,PillaudVivien2018StatisticalOO,Jun2019KernelTR}, etc) also uses a stronger assumption $y \in [-L_{0},L_{0}]$ which implies both Assumption \ref{ass mom of error} and the boundedness of $f_{\rho}^{*}$.


\subsection{Convergence results}
Now we are ready to state our main results. Though this paper focuses the misspecified case, i.e., $ 0 < s < 1$, we state the theorems including those $ s \ge 1$ for completeness.
\setcounter{theorem}{0}
\begin{theorem}[Upper bound]\label{main theorem}
  Suppose that Assumption \ref{ass EDR},\ref{assumption embedding}, \ref{ass source condition} and \ref{ass mom of error} hold for $ 0 < s \le 2 \tau$ and $\frac{1}{\beta} \le \alpha_{0} < 1$. Let $\hat{f}_{\nu}$ be the estimator defined by \eqref{SA estimator}. Then for $0 \le \gamma \le 1$ with $\gamma \le s$:
  \begin{itemize}[leftmargin = 18pt]
      \item In the case of $s + \frac{1}{\beta} > \alpha_{0} $, by choosing $\nu \asymp n^{\frac{\beta }{s \beta + 1}}$, for any fixed $\delta \in (0,1)$, when $n$ is sufficiently large, with probability at least $1 - \delta$, we have
      \begin{equation}
          \left\|\hat{f}_{\nu}-f_{\rho}^*\right\|_{[\mathcal{H}]^{\gamma}}^2 \leq\left(\ln \frac{6}{\delta}\right)^2 C n^{-\frac{(s-\gamma) \beta}{s \beta+1}},
      \end{equation}
      where $C$ is a constant independent of $n$ and $\delta$.
      
      \item In the case of $s + \frac{1}{\beta} \le \alpha_{0} $, for any $ \alpha > \alpha_{0}$, by choosing $\nu \asymp (\frac{n}{\ln^{r}(n)})^{\frac{1}{\alpha}}$ for some $r > 1$, for any fixed $\delta \in (0,1)$, when $n$ is sufficiently large, with probability at least $1 - \delta$, we have
      \begin{equation}
          \left\|\hat{f}_\nu-f_{\rho}^*\right\|_{[\mathcal{H}]^{\gamma}}^2 \leq\left(\ln \frac{6}{\delta}\right)^2 C \left(\frac{n}{\ln ^r(n)}\right)^{-\frac{s-\gamma}{\alpha}},
      \end{equation}
      where $C$ is a constant independent of $n$ and $\delta$.
  \end{itemize}
\end{theorem}
Compared with the state-of-the-art results (\citealt{fischer2020_SobolevNorm,PillaudVivien2018StatisticalOO}), Theorem \ref{main theorem} removes the boundedness assumption $ \| f_{\rho}^{*} \|_{L^{\infty}} \le B_{\infty} < \infty$ and prove the same upper bound for general spectral algorithms. This improvement is nontrivial for $ s \le \alpha_{0}$, since $ [\mathcal{H}]^{s} \cap L^{\infty}(\mathcal{X,\mu}) \subsetneqq [\mathcal{H}]^{s}$ when $ s \le \alpha_{0}$. As we will see in Section \ref{section proofs}, the proof of Theorem \ref{main theorem} removes the boundedness assumption by analyzing the $L^{q}$-embedding property of $[\mathcal{H}]^{s} $. With the $L^{q}$-integrability of the functions in $ [\mathcal{H}]^{s} $, although the true function $f_{\rho}^{*}$ may not fall into $ L^{\infty}(\mathcal{X,\mu})$, the tail probability can be controlled appropriately. We present the convergence results for $ [\mathcal{H}]^{\gamma}$-norm generalization error, where the $L^{2}$-norm \eqref{def of gen} is a special case when $ \gamma = 0$. 

Now we are going to state the minimax lower bound, which is often referred to as the information-theoretic lower bound (see, e.g., \citealt{rastogi2017_OptimalRates}).
\begin{theorem}[Lower bound]\label{prop information lower bound}
    Let $\mu$ be a probability distribution on $\mathcal{X}$ such that Assumption \ref{ass EDR} is satisfied. Let $\mathcal{P}$ consist of all the distributions on $\mathcal{X}\times \mathcal{Y}$ satisfying \ref{ass source condition}, \ref{ass mom of error} for $ s > 0$ and with marginal distribution $\mu$. Then for $0 \le \gamma \le 1$ with $\gamma \le s$, there exists a constant $C$, for all learning methods, for any fixed $\delta \in (0,1)$, when $n$ is sufficiently large, there is a distribution $\rho \in \mathcal{P}$ such that, with probability at least $1 - \delta$, we have 
   \begin{equation}
       \left\|\hat{f}-f_\rho^*\right\|_{[\mathcal{H}]^{\gamma}}^2 \ge C \delta n^{-\frac{(s-\gamma)\beta}{s \beta +1}}.
   \end{equation}
\end{theorem}

\begin{remark}[Optimality]
A direct result from Theorem \ref{main theorem} and Theorem \ref{prop information lower bound} is that for $s \in \left(\alpha_0 - \frac{1}{\beta}, 2 \tau \right]$, the upper bound matches the minimax lower bound. Therefore, we prove the minimax optimality of spectral algorithms for $ s \in \left( \alpha_0 - \frac{1}{\beta}, 2 \tau \right]$ with respect to $ [\mathcal{H}]^{\gamma}$-norm ( $ 0 \le \gamma \le s$) generalization error.
\end{remark}
