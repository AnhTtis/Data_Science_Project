In this section, we aim to verify through experiments that when $ \alpha_{0}-\frac{1}{\beta} < s < \alpha_{0}$, for those functions $f_{\rho}^{*} $ in $ [\mathcal{H}]^{s}$ but not in $L^{\infty}$, the spectral algorithms can still achieve the optimal convergence rate. We show the $L^{2}$-norm convergence results for two kinds of RKHSs and the three kinds of spectral algorithms mentioned in Section \ref{section spectral algorithms}.

Suppose that $\mathcal{X} = [0,1]$ and the marginal distribution $\mu$ is the uniform distribution on $[0,1]$. The first considered RKHS is $\mathcal{H} = H^{1}(\mathcal{X})$, the Sobolev space with smoothness 1. Section \ref{section examples sobolev} shows that the EDR is $\beta = 2$ and embedding index is $\alpha_{0} = \frac{1}{\beta}$. We construct a function in $[\mathcal{H}]^{s} \backslash L^{\infty}$ by 
\begin{equation}\label{series of sobolev}
    f^{*}(x) =  \sum\limits_{k=1}^{\infty} \frac{1}{k^{s+0.5}} \left( \sin\left( 2 k \pi x\right) + \cos\left( 2 k \pi x\right) \right),
\end{equation}
for some $0 < s < \frac{1}{\beta} = 0.5$. We will show in Appendix C that the series in \eqref{series of sobolev} converges on $(0,1)$. In addition, since $ \sin 2 k \pi + \cos 2 k \pi \equiv 1$, we also have $f^{*} \notin L^{\infty}(\mathcal{X})$. The explicit formula of the kernel associated to $H^{1}(\mathcal{X})$ is given by \citet[Corollary 2]{ThomasAgnan1996ComputingAF}, i.e., $ k(x,y) = \frac{1}{\sinh{1}} \cosh{(1- \max(x,y)) \cosh{(1- \min(x,y))}}$. 

For the second kind of RKHS, it is well known that the following RKHS 
\begin{align}
    \mathcal{H} = \mathcal{H}_{\text{min}}(\mathcal{X}):=\Big\{f:[0,1] \rightarrow \mathbb{R} \mid f &\text { is A.C., } f(0)=0, \int_0^1\left(f^{\prime}(x)\right)^2 \mathrm{~d} x<\infty\Big\}. \notag
\end{align}
is associated with the kernel $k(x,y) = \min(x,y)$ \citep{wainwright2019_HighdimensionalStatistics}. Further, its eigenvalues and eigenfunctions can be written as
\begin{displaymath}
    \lambda_n=\left(\frac{2 n-1}{2} \pi\right)^{-2}, \quad n=1,2, \cdots
\end{displaymath}
and
\begin{displaymath}
    e_n(x)=\sqrt{2} \sin \left(\frac{2 n-1}{2} \pi x\right), \quad n=1,2, \cdots
\end{displaymath}
It is easy to see that the EDR is $\beta = 2$ and the eigenfunctions are uniformly bounded. Section \ref{section examples ui} shows that the embedding index is $\alpha_{0} = \frac{1}{\beta}$. We construct a function in $[\mathcal{H}]^{s} \backslash L^{\infty}$ by
\begin{equation}\label{series of min}
    f^{*}(x) = \sum\limits_{k = 1}^{\infty} \frac{1}{k^{s+0.5}} e_{2k-1}(x),
\end{equation}
for some $0< s < \frac{1}{\beta} = 0.5$. We will show in Appendix C that the series in \eqref{series of min} converges on $(0,1)$. Since $e_{2k-1}(1) \equiv 1$, we also have $f^{*} \notin L^{\infty}(\mathcal{X})$. 

We consider the following data generation procedure:
\begin{displaymath}
    y = f^{*}(x) + \epsilon, 
\end{displaymath}
where $f^{*}$ is numerically approximated by the first 3000 terms in \eqref{series of sobolev} or \eqref{series of min} with $s=0.4$, $x \sim \mathcal{U}[0,1]$ and $\epsilon \sim \mathcal{N}(0,1)$. Three kinds of spectral algorithms (kernel ridge regression, gradient flow and spectral cut-off) are used to construct estimators $\hat{f}$ for each RKHS, where we choose the regularization parameter as $\nu = c n^{\frac{\beta}{s \beta + 1}} = c n^{\frac{10}{9}}$ for a fixed $c$. The sample size $n$ is chosen from 1000 to 5000, with intervals of 100. We numerically compute the generalization error $ \|\hat{f} - f^{*}\|_{L^{2}}$ by Simpson's formula with $N \gg n$ testing points. For each $n$, we repeat the experiments 50 times and present the average generalization error as well as the region within one standard deviation. To visualize the convergence rate $r$, we perform logarithmic least-squares $\log \text{err} = r \log n + b$ to fit the generalization error with respect to the sample size and display the value of $r$.



\begin{figure}[ht]
\vskip 0.05in
\centering
\subfigure[]{\includegraphics[width=0.3\columnwidth]{SA_Min_KRR.pdf}}
\subfigure[]{\includegraphics[width=0.3\columnwidth]{SA_Min_GF.pdf}}
\subfigure[]{\includegraphics[width=0.3\columnwidth]{SA_Min_CUT.pdf}}

\subfigure[]{\includegraphics[width=0.3\columnwidth]{SA_Sob_KRR.pdf}}
\subfigure[]{\includegraphics[width=0.3\columnwidth]{SA_Sob_GF.pdf}}
\subfigure[]{\includegraphics[width=0.3\columnwidth]{SA_Sob_CUT.pdf}}

\caption{ Error decay curves of two kinds of RKHSs and three kinds of spectral algorithms with the best choice of $c$. Both axes are are scaled logarithmically. The curves show the average generalization errors over 50 trials; the regions within one standard deviation are shown in green. The dashed black lines are computed using logarithmic least-squares and the slopes represent the convergence rates $r$. Figures in the first row correspond to the Sobolev RKHS $\mathcal{H} = H^{1}(\mathcal{X})$ and the second correspond to the $\mathcal{H} = \mathcal{H}_{\text{min}}(\mathcal{X})$. } 
\label{figure bestc}
\vskip 0.05in
\end{figure}

We try different values of $c$, Figure \ref{figure bestc} presents the convergence curves under the best choice of $c$. For each setting, it can be concluded that the convergence rates of the $L^{2}$-norm generalization errors of spectral algorithms are indeed approximately equal to $ n^{-\frac{s \beta}{s \beta + 1}} = n^{-\frac{4}{9}}$, without the boundedness assumption of the true function $f^{*}$. We refer to Appendix C for more details on the experiments.