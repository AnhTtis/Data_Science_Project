In this section, we compare this paper's convergence rates and minimax optimality with the results in previous literature. Ignoring the log-term and the constants, Theorem \ref{main theorem} gives the upper bound of the convergence rates of spectral algorithms (with high probability)
\begin{equation}\label{discussion upper bound}
    \left\|\hat{f}_\nu-f_{\rho}^*\right\|_{[\mathcal{H}]^{\gamma}}^2 \le \begin{cases} n^{-\frac{(s-\gamma) \beta} {s \beta + 1}},\quad & \alpha_{0} - \frac{1}{\beta} < s \le 2 \tau , \\ n^{-\frac{s-\gamma } {\alpha_{0} + \epsilon}},\quad & 0 < s  \le \alpha_{0} - \frac{1}{\beta},~~ \forall \epsilon > 0. \end{cases}
\end{equation}
This $ [\mathcal{H}]^{\gamma} $-norm upper bound depends on $ \tau, \beta, s $ and $ \alpha_{0}$, among which $ \beta, \alpha_{0} $ characterize the information of the RKHS; $ s $ characterizes the relative `smoothness' of the true function; and $\tau$ characterizes the spectral algorithm. To our knowledge, this is the most general setting among related literature and will give the most refined analysis. In the well-specified case ($1 \le s \le 2\tau$ or $ f_{\rho}^{*} \in \mathcal{H}$), we recover the well-known minimax optimal rates from a lot of literature \citep[etc.]{Caponnetto2007OptimalRF,caponnetto2006optimal,dicker2017_KernelRidge, blanchard2018_OptimalRates,lin2018_OptimalRates,fischer2020_SobolevNorm} (for either general spectral algorithms or a specific kind). 


The improvement in the misspecified case ($ 0 < s < 1$ or $ f_{\rho}^{*} \notin \mathcal{H}$) of this paper is partly due to the advantage of considering the embedding index $ \alpha_{0}$ of the RKHS. The best upper bound without considering the embedding index is (see, e.g., \citealt{dieuleveut2016nonparametric,lin2018_OptimalRates,lin2020_OptimalConvergence})
\begin{equation}\label{discussion upper bound no emb}
    \left\|\hat{f}_\nu-f_{\rho}^*\right\|_{[\mathcal{H}]^{\gamma}}^2 \le \begin{cases} n^{-\frac{(s-\gamma) \beta} {s \beta + 1}},\quad & 1 - \frac{1}{\beta} < s \le 2 \tau , \\ n^{-(s-\gamma) },\quad & 0 < s  \le 1 - \frac{1}{\beta}. \end{cases}
\end{equation}
This rate coincides with our upper bound \eqref{discussion upper bound} if the embedding index $\alpha_{0} = 1$. For those RKHSs with $\alpha_{0} < 1 $, \eqref{discussion upper bound} gives refined upper bound for all $ 0 < s \le 2 \tau$. As shown in Section \ref{section examples}, this is the case for many kinds of RKHSs. This is also why we assume $\alpha_{0} \in (0,1)$ throughout our paper.

Compared with the line of work which considers the embedding index \citep[etc.]{Steinwart2008SupportVM, PillaudVivien2018StatisticalOO,fischer2020_SobolevNorm}, this paper removes the boundedness assumption, i.e., $ \| f_{\rho}^{*} \|_{L^{\infty}(\mathcal{X},\mu)} \le B_{\infty} < \infty$. The upper bound in these works is the same as \eqref{discussion upper bound}. But due to the boundedness assumption, \citet{fischer2020_SobolevNorm} reveals that the minimax lower rate associated to the
smaller function space $[\mathcal{H}]^{s} \cap L^{\infty}(\mathcal{X,\mu})$ is larger than
\begin{displaymath}
  n^{-\frac{\max{(s,\alpha)}\beta}{\max{(s,\alpha)}\beta + 1}}, \quad \forall \alpha > \alpha_{0}.
\end{displaymath}
Therefore, they only prove the minimax optimality in the regime
\begin{displaymath}
  \alpha_{0} < s \le 2 \tau.
\end{displaymath}
Combining Theorem \ref{main theorem} and Theorem \ref{prop information lower bound}, this paper extends the minimax optimality of the spectral algorithms to the regime
\begin{displaymath}
  \alpha_{0} - \frac{1}{\beta} < s \le 2 \tau.
\end{displaymath}
This improvement is mainly due to the $ L^{q}$-embedding property of the interpolation space $[\mathcal{H}]^{s}$ proved in Theorem \ref{integrability of Hs} and a truncation method in the proof. Note that only the $ L^{\infty}$-embedding property has been considered before this paper. This new regime of minimax optimality means a lot. Since we have proved that the embedding index $\alpha_{0}$ equals $\frac{1}{\beta}$ for many kinds of RKHSs, the optimality in the misspecified case is well understood for these RKHSs. 

We believe that the new technical tools in this paper can be used for more related topics. For instance, some literature considers the general source condition, i.e.,
\begin{displaymath}
  f_{\rho}^{*} = \phi(L_{k}) g_{0}, \quad \text{with} ~~ \|g_{0}\|_{L^{2}} \le R,
\end{displaymath}
where $ \phi:\left[0, \kappa^2\right] \rightarrow \mathbb{R}^{+} $ is a non-decreasing index function such that $\phi(0) = 0$ and $\phi(\kappa^{2}) < \infty $ \citep{bauer2007_RegularizationAlgorithms,rastogi2017_OptimalRates,lin2018_OptimalRates,Talwai2022OptimalLR}. The source condition in Assumption \ref{ass source condition} corresponds to a special choice of $ \phi(x) = x^{\frac{s}{2}}$, which is often referred to as the HÃ¶lder source condition. Another interesting topic is the distributed version of spectral algorithms \citep{Zhang2013DivideAC,Lin2016DistributedLW,Guo2017LearningTO,Mcke2018ParallelizingSR,lin2020_OptimalConvergence}. It aims to reduce the computation complexity of the original spectral algorithms while maintaining the estimation efficiency. It would be interesting to apply this paper's tools and try refining the convergence rates or optimality in these scenarios.

In addition, we also notice a line of work which studies the learning curves of kernel ridge regression \citep{Spigler2020AsymptoticLC,Bordelon2020SpectrumDL,Cui2021GeneralizationER} and crossovers between different noise magnitudes. At present, their results all rely on a Gaussian design assumption (or some variation), which is a very strong assumption. We believe that studying the misspecified case in our paper is a crucial step to remove the Gaussian design assumption and draw complete conclusions about the learning curves of kernel ridge regression (or further, general spectral algorithms).

The eigenvalue decay rate (also known as the capacity condition or effective dimension condition) and source condition are mentioned in almost all related literature studying the convergence behaviors of kernel methods but are denoted as various kinds of notations. At the end of this section, we list a dictionary of nations in related literature. Recall that in this paper we denote the eigenvalue decay rate as $\beta$ and denote the source condition as $s$. Table \ref{table1} summarizes the notations used in some of the references.

\begin{table}[ht]
\centering
\begin{tabular}{ccc} 
\toprule
$\beta$  &   $s$ & Reference \\
\midrule
 $1/p$       &  $\beta$   & \citet{steinwart2009_OptimalRates,fischer2020_SobolevNorm,Li2022OptimalRF}  \\
 \midrule
 $1/\gamma$  &  $2\zeta$ & \citet{lin2018_OptimalRates,lin2020_OptimalConvergence}  \\
 \midrule
 $b$         &  $c$      & \citet{caponnetto2006optimal,Caponnetto2007OptimalRF}  \\
 \midrule
 $-$         &  $2 r$    & \citet{bauer2007_RegularizationAlgorithms,Smale2007LearningTE,gerfo2008_SpectralAlgorithms}  \\
 \midrule
 $2 \nu$     &  $\zeta+1$ & \citet{dicker2017_KernelRidge}  \\
 \midrule
 $b$        &  $2r+1$ & \citet{rastogi2017_OptimalRates,blanchard2018_OptimalRates}  \\
 \midrule
 $1/b$  &  $2\beta$ & \citet{Jun2019KernelTR} \\
 \midrule
  
 \multirow{2}{*}{$\alpha$} & \multirow{2}{*}{$2r$} & \citet{dieuleveut2016nonparametric,PillaudVivien2018StatisticalOO} \\
  & & \citet{Celisse2020AnalyzingTD} \\
\bottomrule
\end{tabular}
\caption{A dictionary of notations in related literature.}
\label{table1}
\end{table}

%  Rosassco; divide and conque; fourier capacity;

% $\alpha$  &  $2r$ & \citet{dieuleveut2016nonparametric,PillaudVivien2018StatisticalOO,Celisse2020AnalyzingTD} \\