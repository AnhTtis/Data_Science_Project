Suppose that the samples $\{ (x_{i}, y_{i}) \}_{i=1}^{n}$ are i.i.d. sampled from an unknown distribution $\rho$ on $\mathcal{X} \times \mathcal{Y}$, where $\mathcal{X} \subseteq \mathbb{R}^{d}$ and $\mathcal{Y} \subseteq \mathbb{R}$. One of the goals of non-parametric least-squares regression is to find a function $\hat{f}$ based on the $n$ samples such that the risk
\begin{equation}\label{def of risk}
  \mathcal{E}(\hat{f}) = \mathbb{E}_{(x,y) \sim \rho} \left[ \left( \hat{f}(x) - y \right)^{2} \right]
\end{equation}
is relatively small. It is well known that the conditional mean function given by $f_{\rho}^*(x) \coloneqq \mathbb{E}_{\rho}[~y \;|\; x~] = \int_{\mathcal{Y}} y \mathrm{d} \rho(y|x)$ minimizes the risk $\mathcal{E}(f) $. 
Therefore, 
%a central theoretical problem is to 
we may focus on establishing the convergence rate (either in expectation or in probability) for the excess risk ($L^{2}$-norm generalization error)
\begin{equation}\label{def of gen}
    \mathbb{E}_{x \sim \mu} \left[ \left( \hat{f}(x) - f_{\rho}^{*}(x) \right)^{2} \right],
\end{equation}
where $ \mu $ is the marginal distribution of $\rho$ on $\mathcal{X}$. 

In the non-parametric regression settings, researchers often assume that $f_{\rho}^*(x)$ falls into a class of functions with a certain structure and develop non-parametric methods to obtain the estimator $\hat{f}$. One of the most popular non-parametric regression methods, the kernel method, aims to estimate $f_{\rho}^*$ using candidate functions from a reproducing kernel Hilbert space (RKHS) $\mathcal{H}$, a separable Hilbert space associated with a kernel function $k$ defined on $\mathcal{X}$, e.g., \citet{Kohler2001NonparametricRE, Cucker2001OnTM, Steinwart2008SupportVM}. This paper focuses on a class of kernel methods called the \textit{spectral algorithms} to construct the estimator of $ f_{\rho}^{*}$.

Since the minimax optimality of spectral algorithms has been proved for the attainable case $\left( f_{\rho}^{*} \in \mathcal{H} \right)$ \citep[etc.]{caponnetto2006optimal,Caponnetto2007OptimalRF}, a large body of literature has studied the convergence rate of the generalization error of misspecified spectral algorithms ($ f_{\rho}^{*} \notin \mathcal{H}$) and whether the rate is optimal in the minimax sense. It turns out that the qualification of the algorithm ($\tau >0$), the eigenvalue decay rate ($\beta > 1$), the source condition ($s>0$) and the embedding index ($\alpha_{0} < 1$) of the RKHS jointly determine the convergence behaviors of the spectral algorithms (see Section \ref{section assumption} for definitions). If we only assume that $f_{\rho}^{*}$ belongs to an interpolation space $[\mathcal{H}]^{s}$ of the RKHS $\mathcal{H}$ for some $s > 0$, the well known information-theoretic lower bound shows that the minimax lower bound (with respect to the $L^{2}$-norm generalization error) is $n^{-\frac{s \beta}{s \beta + 1}} $. The state-of-the-art result shows that when $ \alpha_{0} < s \le 2 \tau$, the upper bound of the convergence rate (with respect to the $L^{2}$-norm generalization error) is $ n^{-\frac{s \beta}{s \beta + 1}}$ and hence is optimal (\citealt{fischer2020_SobolevNorm} for kernel ridge regression and \citealt{PillaudVivien2018StatisticalOO} for gradient methods). However, when $ f_{\rho}^{*} \in [\mathcal{H}]^{s}$ for some $0 < s \le \alpha_{0}$, all the existing works need an additional boundedness assumption of $f_{\rho}^{*}$ to prove the same upper bound $ n^{-\frac{s \beta}{s \beta + 1}}$. The boundedness assumption will result in a smaller function space, i.e., $ [\mathcal{H}]^{s} \cap L^{\infty}(\mathcal{X,\mu}) \subsetneqq [\mathcal{H}]^{s}$ when $ s \le \alpha_{0}$. \citet{fischer2020_SobolevNorm} further reveals that the minimax rate associated with the smaller function space is larger than  $ n^{-\frac{\alpha \beta}{ \alpha \beta + 1}}$ for any $\alpha > \alpha_{0}$. This minimax lower bound is smaller than the upper bound of the convergence rate and hence they can not prove the minimax optimality of spectral algorithms when $s \le \alpha_{0}$. 

It has been an outstanding problem for years whether the spectral algorithms are minimax optimal for all $s\in (0,1)$, either with respect to the $L^{2}$-norm or the $[\mathcal{H}]^{\gamma}$-norm introduced later~\citep{PillaudVivien2018StatisticalOO, fischer2020_SobolevNorm, Liu2022StatisticalOO}.
% It has been an outstanding problem for years that when $s \le \alpha_{0}$, whether the boundedness assumption of $f_{\rho}^{*}$ in the proof of upper bound can be removed or weakened \cite{PillaudVivien2018StatisticalOO, fischer2020_SobolevNorm, Liu2022StatisticalOO}. 
To this end, this paper has three contributions.
\begin{itemize}[leftmargin = 18pt]
    \item Using the tools from real interpolation theory, we analyze the $L^{q}$-embedding property of $[\mathcal{H}]^{s}$, an interpolation space of the RKHS. Specifically, assume that $\mathcal{H} $ has embedding index $\alpha_{0}$. When $s \le \alpha_{0}, $ Theorem \ref{integrability of Hs} proves that $[\mathcal{H}]^{s}$ is continuously embedded into $ L^{q}(\mathcal{X},\mu)$, for $ q = \frac{2 \alpha}{\alpha - s}, \forall \alpha > \alpha_{0}$. 
    
    \item Based on the $ L^{q}$-embedding property of $[\mathcal{H}]^{s}$, the refined proof in this paper removes the boundedness assumption in previous literature and obtains the same upper bound of the convergence rate as the state-of-the-art upper bound. As a result, we prove the minimax optimality of spectral algorithms for $\alpha_0 - \frac{1}{\beta} < s \le 2 \tau$, which can only be proved for $\alpha_0 < s \le 2 \tau$ before. We also recover the upper bound in previous literature when $0 < s \le \alpha_0 - \frac{1}{\beta} $ (if exists) though the optimality does not hold. Note that in this paper, we present the results in terms of $[\mathcal{H}]^{\gamma}$-norm generalization error, where the $L^{2}$-norm \eqref{def of gen} is a special case when $ \gamma = 0$.
    
    \item We give several examples of RKHSs whose embedding index satisfies $ \alpha_{0} = \frac{1}{\beta}$. Besides RKHS with uniformly bounded eigenfunctions and the Sobolev RKHS \citep{fischer2020_SobolevNorm}, we first show that RKHS with shift-invariant kernels and RKHS with dot-product kernels on the sphere satisfy that $ \alpha_{0} = \frac{1}{\beta}$. Therefore, for these RKHSs, this paper proves the optimality of spectral algorithms for all $ 0 < s \le 2 \tau$.
\end{itemize}

% This paper concludes that, the boundedness assumption can be replaced with a $L^{q}$-integrability assumption, where $q$ depends on $s, \beta$ and $\alpha_{0}$. We prove the same upper bound as state-of-the-art results under this weaker assumption. Furthermore, we prove that the $L^q$-integrability assumption is naturally satisfied when $s > \alpha_0 - \frac{1}{\beta}$, i.e., $[\mathcal{H}]^{s} \hookrightarrow L^{q} $. The improvement is two-fold. On the one hand, for $\alpha_0 - \frac{1}{\beta} < s \le 2 \tau $, we obtain the state-of-the-art upper bound of the convergence rate for all the functions in $ [\mathcal{H}]^{s}$, which can only be proved for $ [\mathcal{H}]^{s} \cap L^{\infty}(\mathcal{X,\mu})$ before. On the other hand,  it can be shown that spectral algorithms are minimax optimal for all $\alpha_0 - \frac{1}{\beta} < s \le 2 \tau$, which can only be proved for $\alpha_0 < s \le 2 \tau$ before. As an example, we can apply the refined results to RKHS with embedding index $\alpha_{0} = \frac{1}{\beta}$ and show the optimality of spectral algorithms for all $ 0 < s \le 2 \tau$. Note that, in this paper, we discuss the more general $[\mathcal{H}]^{\gamma}$-norm learning rates with $ \gamma = 0$ as the commonly used $L^2$-norm learning rates.


\subsection{Related work}
% \begin{enumerate}
%     \item Spectral algorithms history. well specified case.
%     \item Misspecified case, embedding or not, boundedness assumption.
%     \item general source condition, distributed spectral algorithms, stochastic gradient descent with multiple passes.
% \end{enumerate}

General spectral algorithms in the setting of kernel methods were first proposed and studied by \citet{ rosasco2005_SpectralMethods,caponnetto2006optimal, bauer2007_RegularizationAlgorithms, gerfo2008_SpectralAlgorithms}. A large class of regularization methods are introduced collectively as spectral algorithms and are characterized through the corresponding filter functions. The qualification $\tau$ of a spectral algorithm and a prior assumption on $f_{\rho}^{*}$ characterizing the relative smoothness (source condition $s$) are also introduced for the problem setting. In this setting, \citet{bauer2007_RegularizationAlgorithms} proves the upper bound of the convergence rate with respect to the $L^{2}$-norm generalization error. \citet{caponnetto2006optimal} proves the `capacity-dependent' upper bound, i.e., considering the eigenvalue decay rate $\beta$ of the RKHS, which has been adopted by most of the later researchers. Note that these works focus on the well specified case ($f_{\rho}^{*} \in \mathcal{H}$) or assume that $\mathcal{H}$ is dense in $L^{2}(\mathcal{X},\mu)$. There are also other related works studying the well specified case, e.g., \citet{blanchard2018_OptimalRates, dicker2017_KernelRidge, rastogi2017_OptimalRates} for general spectral algorithms, \citet{Caponnetto2007OptimalRF, Smale2007LearningTE} for kernel ridge regression and \citet{Yao2007OnES} for gradient methods.

Since the convergence rates and the minimax optimality of spectral algorithms in the well specified case are clear, a large amount of literature studied the misspecified spectral algorithms. Among these work, \cite{steinwart2009_OptimalRates,dicker2017_KernelRidge,PillaudVivien2018StatisticalOO,fischer2020_SobolevNorm, Celisse2020AnalyzingTD,Li2022OptimalRF,  Talwai2022OptimalLR} consider the $L^{\infty}$-embedding property, while \citet{dieuleveut2016nonparametric,lin2018_OptimalRates,lin2020_OptimalConvergence,JMLR:v23:21-0570} do not. Note that considering the $L^{\infty}$-embedding property is equivalent to introducing the embedding index $\alpha_{0}$ in this paper. It has been shown that this will lead to faster convergence rates for certain embedding indexes (see Section \ref{section discussion} for detailed comparison). In addition, as mentioned in \cite{fischer2020_SobolevNorm}, the convergence rates with respect to the $L^{2}$-norm can be easily extended to the more general $ [\mathcal{H}]^{\gamma} $-norm if one uses the integral operator technique. Up to now, we have introduced five indexes $ \tau, s, \beta, \alpha_{0}$ and $ \gamma$ that we know as a priori to study the convergence rates of the spectral algorithms. To our knowledge, the state-of-the-art results on the convergence rates and the minimax optimality are \citet{fischer2020_SobolevNorm} for kernel ridge regression and \citet{PillaudVivien2018StatisticalOO} for gradient methods. 

But the spectral algorithms in the misspecified case have not been totally solved. When $f_{\rho}^{*}$ falls into a less-smooth interpolation space which does not imply the boundedness of functions therein, all existing works (either considering embedding index or not) require an additional boundedness assumption, i.e., $ \| f_{\rho}^{*} \|_{L^{\infty}(\mathcal{X},\mu)} \le B_{\infty} < \infty$ to prove the desired upper bound. As discussed in the introduction, this will lead to the suboptimality in the $s \le \alpha_{0}$ regime. As far as we know, the $L^{q}$-embedding property of $[\mathcal{H}]^{s}$ has not been discussed in related literature. This paper shows that it turns out to be a crucial property to remove the boundedness assumption and extend the minimax optimality to a broader regime. 

The outline of the rest of the paper is as follows. In Section \ref{section pre}, we introduce basic concepts of RKHS, integral operators and spectral algorithms. In Section \ref{section main}, we present our main results of the convergence rates and the minimax optimality. As examples, we further show that the embedding index satisfies $\alpha_{0} = \frac{1}{\beta}$ for some commonly used RKHSs in Section \ref{section examples}. Note that this is the ideal case where the minimax optimality can be proved for all $ 0 < s \le 2 \tau$. We verify our results through experiments in Section \ref{section experiments} and make a comparison with previous literature in Section \ref{section discussion}. All the proofs can be found in Section \ref{section proofs}.