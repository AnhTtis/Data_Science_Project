\subsection{Basic concepts}
Let a compact set $\mathcal{X} \subseteq \mathbb{R}^{d}$ be the input space and $ \mathcal{Y} \subseteq \mathbb{R}$ be the output space. Let $ \rho $ be an unknown probability distribution on $\mathcal{X} \times \mathcal{Y}$ satisfying $ \int_{\mathcal{X} \times \mathcal{Y}} y^{2} \mathrm{d}\rho(x,y) <\infty$ and denote the corresponding marginal distribution on $ \mathcal{X} $ as $\mu$. We use $L^{p}(\mathcal{X},\mu)$ (in short $L^{p}$) to represent the $L^{p}$-spaces. Denote 
\begin{displaymath}
  f_{\rho}^*(x) \coloneqq \mathbb{E}_{\rho}[~y \;|\; x~] = \int_{\mathcal{Y}} y ~ \mathrm{d} \rho(y|x)
\end{displaymath}
as the conditional mean. Throughout the paper, we denote $\mathcal{H}$ as a separable RKHS on $\mathcal{X}$ with respect to a continuous kernel function $k$ and satisfying
\begin{displaymath}
  \sup\limits_{x \in \mathcal{X}} k(x,x) \le \kappa^{2}.
\end{displaymath}

Denote the natural embedding inclusion operator as $S_{k}: \mathcal{H} \to L^{2}(\mathcal{X},\mu)$. Moreover, the adjoint operator $S_{k}^{*}: L^{2}(\mathcal{X},\mu) \to \mathcal{H}  $ is an integral operator, i.e., for $f \in L^{2}(\mathcal{X},\mu)$ and $x \in \mathcal{X}$, we have 
\begin{displaymath}
\left(S_{k}^{*} f\right)(x)=\int_\mathcal{X} k\left(x, x^{\prime}\right) f\left(x^{\prime}\right) \mathrm{d} \mu\left(x^{\prime}\right).
\end{displaymath}
Then $S_{k}$ and $S_{k}^{*}$ are Hilbert-Schmidt operators (thus compact) and the HS norms (denoted as $\left\| \cdot \right\|_{2}$) satisfy that
    \begin{displaymath}
        \left\| S_{k}^{*} \right\|_{2} = \left\| S_{k} \right\|_{2} = \|k\|_{L^{2}(\mathcal{X},\mu)}:=\left(\int_\mathcal{X} k(x, x) \mathrm{d} \mu(x)\right)^{1 / 2} \le \kappa.
    \end{displaymath}
Next, we can define two integral operators: 
\begin{equation}
    L_{k}:=S_{k} S_{k}^{*}: L^{2}(\mathcal{X},\mu) \rightarrow L^{2}(\mathcal{X},\mu), \quad  \quad T:=S_{k}^{*} S_{k}: \mathcal{H} \rightarrow \mathcal{H}.
\end{equation}
$L_{k}$ and $T$ are self-adjoint, positive-definite and trace class (thus Hilbert-Schmidt and compact) and the trace norms (denoted as $\left\| \cdot \right\|_{1}$) satisfy that
    \begin{displaymath}
      \left\|L_{k}\right\|_{1}=\left\|T\right\|_{1}=\left\|S_{k}\right\|_{2}^2=\left\|S_{k}^{*}\right\|_{2}^2.
    \end{displaymath}
The spectral theorem for self-adjoint compact operators yields that there is an at most countable index set $N$, a non-increasing summable sequence $\{ \lambda_{i} \}_{i \in N} \subseteq (0,\infty)$ and a family $\{ e_{i} \}_{i \in N} \subseteq \mathcal{H} $, such that $\{ e_{i} \}_{i \in N}$ is an orthonormal basis (ONB) of $\overline{\operatorname{ran} S_{k}} \subseteq L^{2}(\mathcal{X},\mu)$ and $\{ \lambda_{i}^{1/2} e_{i} \}_{i \in N}$ is an ONB of $\mathcal{H}$. Further, the integral operators can be written as 
    \begin{equation}\label{spectral representation}
       L_{k}=\sum_{i \in N} \lambda_i\left\langle\cdot,e_{i} \right\rangle_{L^{2}}e_{i} \quad \text { and } \quad T=\sum_{i \in N} \lambda_i\left\langle\cdot, \lambda_i^{1/2} e_i\right\rangle_{\mathcal{H}} \lambda_i^{1/2} e_i.
    \end{equation}
    We refer to $\{ e_{i} \}_{i \in N}$ and $\{ \lambda_{i} \}_{i \in N}$ as the eigenfunctions and eigenvalues. The celebrated Mercer's theorem (see, e.g., \citealt[Theorem 4.49]{Steinwart2008SupportVM}) shows that 
    \begin{displaymath}\label{Mercer decomposition}
      k\left(x, x^{\prime}\right)=\sum_{i \in N} \lambda_i e_i(x) e_i\left(x^{\prime}\right), \quad x, x^{\prime} \in \mathcal{X},
    \end{displaymath}
    where the convergence is absolute and uniform.
      

We also need to introduce the interpolation spaces (power spaces) of RKHS. For any $ s \ge 0$, the fractional power integral operator $L_{k}^{s}: L^{2}(\mathcal{X},\mu) \to L^{2}(\mathcal{X},\mu)$ is defined as 
\begin{displaymath}
  L_{k}^{s}(f)=\sum_{i \in N} \lambda_i^{s} \left\langle f, e_i\right\rangle_{L^2} e_i.
\end{displaymath}
Then the interpolation space (power space) $[\mathcal{H}]^s $ is defined as
\begin{equation}\label{def interpolation space}
  [\mathcal{H}]^s := \operatorname{Ran} L_{k}^{s/2} = \left\{\sum_{i \in N} a_i \lambda_i^{s / 2}e_{i}: \left(a_i\right)_{i \in N} \in \ell_2(N)\right\} \subseteq L^{2}(\mathcal{X},\mu),
\end{equation}
equipped with the inner product
\begin{equation}\label{def of interpolation norm}
    \langle f, g\rangle_{[\mathcal{H}]^s}=\left\langle L_{k}^{-\frac{s}{2}} f, L_{k}^{-\frac{s}{2}} g\right\rangle_{L^2} .
\end{equation}
It is easy to show that $[\mathcal{H}]^s $ is also a separable Hilbert space with orthogonal basis $ \{ \lambda_{i}^{s/2} e_{i}\}_{i \in N}$. Specially, we have $[\mathcal{H}]^0 \subseteq L^{2}(\mathcal{X},\mu) $ and $[\mathcal{H}]^1 = \mathcal{H}$. For $0 < s_{1} < s_{2}$, the embeddings $ [\mathcal{H}]^{s_{2}} \hookrightarrow[\mathcal{H}]^{s_{1}} \hookrightarrow[\mathcal{H}]^0 $ exist and are compact \citep{fischer2020_SobolevNorm}. For the functions in $[\mathcal{H}]^{s}$ with larger $s$, we say they have higher regularity (smoothness) with respect to the RKHS.

% (In fact, from the discussion above (7) in \cite{fischer2020_SobolevNorm}, we know that $ [\mathcal{H}]^{1} = ( \operatorname{Ker} S_{k})^{\bot} $ of $\mathcal{H}$. Further, since we assume the kernel is continuous so that $\{ \lambda_{i}^{1/2} e_{i} \}_{i \in N}$ is an ONB of $\mathcal{H}$. From (17), (19) of \cite{steinwart2012_MercerTheorem} we know that $[\mathcal{H}]^{1} = \mathcal{H} $.)

It is worth pointing out the relation between the definition \eqref{def interpolation space} and the interpolation space defined through the real method (real interpolation). For details of interpolation of Banach spaces through the real method, we refer to \citet[Chapter 4.2.2]{sawano2018theory}. Specifically, \citet[Theorem 4.6]{steinwart2012_MercerTheorem} reveals that for $0 < s < 1$,
\begin{equation}\label{inter relation}
  [\mathcal{H}]^{s} \cong \left(L^{2}(\mathcal{X},\mu), [\mathcal{H}]^{1} \right)_{s,2} .
\end{equation}
As an example, the Sobolev space $H^{m}(\mathcal{X})$ is an RKHS if $m > \frac{d}{2}$ and its interpolation space is still a Sobolev space given by $ [H^{m}(\mathcal{X})]^s \cong H^{m s}(\mathcal{X}), \forall s>0 $. See Section \ref{section examples sobolev} for detailed discussions. 


\subsection{Spectral algorithms}\label{section spectral algorithms}

Suppose that we observed the given samples $Z = \{ (x_{i}, y_{i}) \}_{i=1}^{n}$ and denote $X=\left( x_{1},\cdots,x_{n} \right)$. Define the sampling operator $ K_{x}: \mathbb{R} \rightarrow \mathcal{H}, ~ y \mapsto y k(x,\cdot) $ and its adjoint operator $K_{x}^{*}: \mathcal{H} \rightarrow \mathbb{R},~ f \mapsto f(x)$. Then we can define $T_{x} = K_{x} K_{x}^{*}$. Further, we define the sample covariance operator $T_{X}: \mathcal{H} \to \mathcal{H}$ as
\begin{equation}\label{def of TX}
    T_X:=\frac{1}{n} \sum_{i=1}^n K_{x_i} K_{x_i}^*.
\end{equation}
Then we know that $ \| T_{X} \| \le \left\| T_{X} \right\|_{1} \le \kappa^{2}$, where $\| \cdot \| $ denotes the operator norm and $\| \cdot \|_{1} $ denotes the trace norm. Further, define the sample basis function
\begin{displaymath}
  g_Z:=\frac{1}{n} \sum_{i=1}^n K_{x_i} y_i \in \mathcal{H}.
\end{displaymath}
Based on the $n$ samples, the kernel method aims to choose a function $ \hat{f} \in \mathcal{H}$ such that the risk given by \eqref{def of risk} is small. A direct estimator is $ \hat{f} \in \mathcal{H}$ that minimizing the empirical risk
\begin{displaymath}
  \hat{\mathcal{E}}(f) = \frac{1}{n}\sum\limits_{i=1}^{n} \left( f(x_i) - y_{i}  \right)^{2},
\end{displaymath}
which leads to an equation
\begin{displaymath}
  T_{X} \hat{f} = g_{Z}.
\end{displaymath}
However, on the one hand, minimizing the empirical risk may lead to overfitting. On the other hand, the inverse of the sample covariance operator $T_X$ does not exist in general. The spectral algorithms \citep[etc.]{rosasco2005_SpectralMethods,caponnetto2006optimal,bauer2007_RegularizationAlgorithms,  gerfo2008_SpectralAlgorithms} handle these issues by introducing the regularization and generate estimators through the filter functions. Now, we first define the filter function. 
\begin{definition}[Filter function]
  Let $\left\{\varphi_\nu:\left[0, \kappa^2\right] \rightarrow \mathbb{R}^{+} \mid \nu \in \Gamma \subseteq \mathbb{R}^{+}\right\}$ be a class of functions and $ \psi_\nu(z)=1-z \varphi_\nu(z) $. If $\varphi_{\nu} ~\text{and}~ \psi_{\nu}$ satisfy:
  \begin{itemize}
      \item $\forall \alpha \in [0,1],$ we have
      \begin{equation}\label{prop1}
          \sup _{z \in\left[0, \kappa^2\right]} z^\alpha \varphi_\nu(z) \leq E \nu^{1-\alpha}, \quad \forall \nu \in \Gamma;
      \end{equation}
      \item $\exists \tau \ge 1 $ s.t. $\forall \alpha \in [0,\tau] $, we have
      \begin{equation}\label{prop2}
          \sup _{z \in\left[0, \kappa^2\right]}\left|\psi_\nu(z)\right| z^\alpha \leq F_\tau \nu^{-\alpha}, \quad \forall \nu \in \Gamma ,
      \end{equation}
  \end{itemize}
  where $ E, F_{\tau}$ are absolute constants, then we call $ \varphi_{\nu} $ a filter function. We refer to $\nu$ as the regularization parameter and $\tau$ as the qualification. 
\end{definition}

Given a filter function $ \varphi_{\nu} $, we can define the corresponding spectral algorithm.
\begin{definition}[spectral algorithm]
   Let $\varphi_{\nu}$ be a filter function index with $ \nu >0$. Given the samples  $Z$, the spectral algorithm produces an estimator of $f_{\rho}^{*}$  given by  
   \begin{align}\label{SA estimator}
      \hat{f}_{\nu} = \varphi_{\nu}(T_{X}) g_{Z}.
   \end{align}
\end{definition}

Here we list three kinds of spectral algorithms that are commonly used. 
\begin{example}[Kernel ridge regression] Let the filter function $\varphi_{\nu} $ be defined as
\begin{displaymath}
    \varphi_{\nu}^{\mathrm{krr}}(z) = \frac{\nu}{\nu z + 1}.  
\end{displaymath}
Then the corresponding spectral algorithm is kernel ridge regression (Tikhonov regularization). The qualification $\tau = 1$ and $ E = F_{\tau} = 1$.
\end{example}

\begin{example}[Gradient flow] Let the filter function $\varphi_{\nu} $ be defined as
\begin{displaymath}
    \varphi_{\nu}^{\mathrm{gf}}(z) = \frac{1 - e^{-\nu z}}{z}.
\end{displaymath}
Then the corresponding spectral algorithm is gradient flow. The qualification $\tau$ could be any positive number, $ E = 1$ and $ F_{\tau} = \left( \tau / e \right)^{\tau}$.
\end{example}

\begin{example}[Spectral cut-off] Let the filter function $\varphi_{\nu} $ be defined as
\begin{displaymath}
    \varphi_\nu^{\mathrm{cut}}(z) = \begin{cases}z^{-1}, & z^{-1} \leq \nu, \\ 0, & z^{-1}>\nu. \end{cases}
\end{displaymath}
Then the corresponding spectral algorithm is Spectral cut-off (truncated singular value decomposition). The qualification $\tau$ could be any positive number and $ E = F_{\tau} = 1$.
\end{example}
For other examples of spectral algorithms (e.g., iterated Tikhonov, gradient methods, Landweber iteration, etc.), we refer to \cite{gerfo2008_SpectralAlgorithms}.