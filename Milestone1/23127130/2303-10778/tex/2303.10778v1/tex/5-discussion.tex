\section{Conclusion and Future Works}

We present a novel formulation for a differentiable DTW layer that can be embedded into a deep network, based on continuous time warping and implicit deep layers. Unlike existing approaches, DecDTW yields the \textit{optimal} alignment path as a differentiable function of layer inputs. We demonstrate the effectiveness of DecDTW in utilising path information in challenging, real-world alignment tasks compared to existing methods. One promising direction for future work is in exploring more compact, alternative parameterisations of the warp function to improve scalability, inspired by~\cite{zhou12gtw}. Another direction would be to integrate more sophisticated DTW variants such as jumpDTW~\citep{jumpdtw}, which allows for repeats and discontinuities in the warp, into the GDTW (and DecDTW) formulation. Finally, we presented methodology for allowing regularisation and constraints to be learnable parameters in a deep network but did not explore this in our experiments. Future work will also explore this capability in more detail.
