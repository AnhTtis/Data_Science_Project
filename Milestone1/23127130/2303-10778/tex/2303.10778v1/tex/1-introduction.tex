
% \begin{figure}[b!]
% \centering
% \noindent\includegraphics[width=0.99\textwidth]{figs/declarative_dtw_system.pdf}
% \caption{Learning with path information. \textbf{Left:} Using Soft-DTW, one can define a loss between the soft, (i.e., $\gamma>0$) \textit{expected} alignment path against a penalty matrix $\bOmega$. During inference, DTW (i.e., $\gamma=0$) is used to produce a predicted alignment. \textbf{Right:} Our DecDTW outputs the \textit{optimal} warping path $\bphi$ using GDTW at both train \textit{and inference} time, removing the disconnect present in Soft-DTW. DecDTW also allows the regularisation and constraint values to be learnable parameters.}\label{fig:systemfigure}
% \end{figure}
\begin{figure}[b!]
% \begin{wrapfigure}{l}{0.70\textwidth}
\centering
\noindent\includegraphics[width=0.92\textwidth]{figs/gdtw_example.pdf}
\caption{Classic DTW (a) is a discrete optimisation problem which finds the minimum cost warping path through a pairwise cost matrix (b). DecDTW uses a continuous time variant of classic DTW (GDTW) (c) and finds an optimal time warp \textit{function} between two continuous time signals (d). }\label{fig:gdtwvsdtw}
% \end{wrapfigure}
\end{figure}

\section{Introduction}

The dynamic time warping (DTW) algorithm computes a discrepancy measure between two temporal sequences, which is invariant to shifting and non-linear scaling in time. Because of this desirable invariance, DTW is ubiquitous in fields that analyze temporal sequences such as speech recognition, motion capture, time series classification and bioinformatics~\citep{kovar03motion, zhu03humming, sakoe78dp, tscbakeoff, petitjean14dtw, NEEDLEMAN1970443}.  The original formulation of DTW computes the minimum cost matching between elements of the two sequences, called an alignment (or warping) path, subject to temporal constraints imposed on the matches. For two sequences of length $m$ and $n$, this can be computed by first constructing an $m$-by-$n$ pairwise cost matrix between sequence elements and subsequently solving a dynamic program (DP) using Bellman's recursion in $O(mn)$ time. Figure \ref{fig:gdtwvsdtw} illustrates the mechanics of the DTW algorithm.

There has been interest in recent years around embedding DTW within deep learning models~\citep{cuturi17a, cai2019dtwnet, lohit19ttn, Chang_2019_CVPR, chang2021dpdtw}, with applications spread across a variety of learning tasks utilising audio and video data~\citep{garreau14temporal, dvornik2021drop, Haresh_2021_CVPR, Chang_2019_CVPR, chang2021dpdtw}, especially where an \textit{explicit} alignment step is desired. There are several distinct approaches in the literature for differentiable DTW layers: \cite{cuturi17a} (Soft-DTW) and~\cite{Chang_2019_CVPR} use a differentiable relaxation of $\min$ in the DP recursion,~\cite{cai2019dtwnet} and~\cite{chang2021dpdtw} observe that differentiation is possible after fixing the warping path, and others~\citep{lohit19ttn, weber19diffeo, grabocka18warp, kazlauskaite19a, abid19auto} regress warping paths directly from sequence elements without explicitly aligning. Note, methods based on DTW differ from methods such as CTC for speech recognition~\citep{pmlr-v32-graves14}, where word-level transcription is more important than frame-level alignment, and an explicit alignment step is not required. 

We propose a novel approach to differentiable DTW, which we name DecDTW, based on deep implicit layers. By adapting a continuous time formulation of the DTW problem proposed by~\cite{deriso2019general}, called GDTW, as an inequality constrained non-linear program (NLP), we can use the deep declarative networks (DDN) framework~\citep{Gould:PAMI2021} to define the forward and backward passes in our DTW layer. The forward pass involves solving for the optimal (continuous time) warping path; we achieve this using a custom dynamic programming approach similar to the original DTW algorithm. The backward pass uses the identities in~\cite{Gould:PAMI2021} to derive gradients using implicit differentiation through the solution computed in the forward pass. 

We will show that DecDTW has benefits compared to existing approaches based on Soft-DTW~\citep{cuturi17a, leguen19dilate, pmlr-v130-blondel21a}; the most important of which is that DecDTW is more effective and efficient at utilising \textit{alignment path information} in an end-to-end learning setting. This is particularly useful for applications where the objective of learning is to improve the accuracy of the alignment itself, and furthermore, ground truth alignments between time series pairs are provided. We show that using DecDTW yields both considerable performance gains and efficiency gains (during training) over Soft-DTW~\citep{leguen19dilate} in challenging real-world alignment tasks. An overview of our proposed DecDTW layer is illustrated in Figure~\ref{fig:systemfigure}.


\textbf{Our Contributions}\hspace{0.3cm} First, we propose a novel, inequality constrained NLP formulation of the DTW problem, building on the approach in~\cite{deriso2019general}. Second, we use this NLP formulation to specify our novel DecDTW layer, where gradients in the backward pass are computed implicitly as in the DDN framework~\citep{Gould:PAMI2021}. Third, we show how the alignment path produced by DecDTW can be used to minimise discrepancies to ground truth alignments. Last, we use our method to attain state-of-the-art performance on challenging real-world alignment tasks. 
