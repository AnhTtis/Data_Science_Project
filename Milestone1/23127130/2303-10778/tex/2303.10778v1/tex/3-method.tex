\section{Continuous Time Formulation for DTW (GDTW)}\label{sec:gdtwvariational}

In this section, we introduce a continuous time version of the DTW problem which is heavily inspired by GDTW~\citep{deriso2019general}. The GDTW problem is used to derive the NLP formulation for our DecDTW layer. We will describe how the results in this section can be used to derive a corresponding NLP in Section~\ref{sec:dtwnlp}. 

\subsection{Preliminaries}

% \paragraph{Signal} 

\textbf{Signal}\hspace{0.3cm} A time-varying signal $\bx:[0,1] \rightarrow \bbR^d$ is a vector-valued function of time. Signals are assumed to be differentiable (at least piecewise smooth) and can be constructed from a time series comprised of observation times $\bt\in[0,1]^N$ where $0 = t_1 < t_2 < \dots < t_N \leq 1$ and associated observations or \textit{features} given by $\bX = \{\bx_1, \dots, \bx_N\} \in \bbR^{N\times d}$, by interpolating between observations (e.g.,\ linear, cubic splines). Without loss of generality, we rescale time for all signals to $[0, 1]$.

% \paragraph{Time warp function} 
\textbf{Time warp function}\hspace{0.3cm} A time warp function $\phi:[0, 1] \rightarrow [0, 1]$ defines correspondences from times in one signal to another, and is similarly assumed to be piecewise smooth. Warp functions typically come with constraints; a common constraint requires that warp functions are non-decreasing, i.e.,~$\phi'(t)\geq 0$ for all $t$. We interpret $\bx \circ \phi$ to be the \textit{time warped} version of $\bx$.

% \paragraph{Dynamic time warping} 
\textbf{Dynamic time warping}\hspace{0.3cm} The GDTW problem can be formulated as follows. Given two signals $\bx$ and $\by$, find a warp function $\phi$ such that $\by \circ \phi \approx \bx$ in some sense over the time series features. In other words, we wish to bring signals $\bx$ and $\by$ together by warping time in $\by$. Figure~\ref{fig:gdtwvsdtw} illustrates how GDTW differs from classic DTW for an example sequence pair. We found in our experiments that GDTW outperformed DTW generally for real-world alignment tasks. We attribute this to the formerâ€™s ability to align \textit{in between} observations, thus enabling a more accurate alignment. 

\subsection{Optimisation problem formulation for GDTW}

GDTW can be specified as a constrained optimisation problem where the decision variable is the time warp function $\phi$ and input parameters are signals $\bx$ and $\by$. Concretely, the problem is given by
\begin{equation}\label{eq:optvariational}
\begin{array}{rl}
    \text{minimise} & f(\bx, \by, \lambda, \phi) \triangleq \cL(\bx, \by, \phi) + \lambda \cR(\phi)
    \\
    \text{subject to} & \underbrace{s^\text{min}(t) \leq \phi'(t) \leq s^\text{max}(t)}_\textbf{local constraints}, \quad \underbrace{b(t)^\text{min} \leq \phi(t) \leq b(t)^\text{max}}_\textbf{global constraints} \quad \forall t.
    \end{array}
\end{equation}
The objective function $f$ can be decomposed into the \textit{signal loss} $\cL$ relating to feature error and a warp regularisation term $\cR$, where $\lambda\geq 0$ is a hyperparameter controlling the strength of regularisation. Since $\phi$, $\bx$ and $\by$ are functions, our objective function $f$ is a functional, revealing the variational nature of the optimisation problem. We will see in Section~\ref{sec:dtwnlp} that restricting $\phi$ to the space of piecewise linear functions will allow us to easily solve Equation~\ref{eq:optvariational} while still allowing for expressive warps.

\begin{figure}[t!]
% \begin{wrapfigure}{r}{0.65\textwidth}
\centering
\noindent\includegraphics[width=0.77\textwidth]{figs/gdtw_constraints.pdf}
\caption{Regularisation (b) and constraints (c) can be applied to prevent pathological warps, e.g.,~$\lambda = 0$ in (b). Global and local constraints bound warp function values and derivatives, respectively. }\label{fig:reg_constr_dtw}
% \end{wrapfigure}
\end{figure}

Furthermore, a number of potentially time-varying constraints are usually imposed on $\phi$. We partition these constraints into \textit{local} and \textit{global} constraints as in~\cite{morel18}. Local constraints bound the time derivatives of the warp function and can be used to enforce the typical nondecreasing warp assumption. Global constraints bound the warp function values directly, with prominent examples being the R-K~\citep{ratamamaRKbound} and commonly used Sakoe-Chiba~\citep{sakoe78dp} bands. Furthermore, global constraints are used to enforce boundary constraints, i.e.,~$\phi(t)\in[0, 1]$ for all $t$ and endpoint constraints, i.e.,~$\phi(0)=0, \phi(1)=1$. Not including endpoint constraints allows for \textit{subsequence} GDTW, where a subsequence $\by$ is aligned to only a portion of $\bx$. Figure~\ref{fig:reg_constr_dtw} illustrates how constraints can be used to change the resultant warps returned by GDTW.

% \paragraph{Signal Loss} 
\textbf{Signal Loss}\hspace{0.3cm} The signal loss functional $\cL$ in Equation~\ref{eq:optvariational} is defined as
\begin{equation}\label{eq:signallossvariational}
    \cL(\bx, \by, \phi) = \int_0^1 L\big(\bx(t) - \by(\phi(t))\big) \text{d}t,
\end{equation}
where $L:\bbR^d\rightarrow \bbR$ is a twice-differentiable (a.e.) penalty function defined over features. In our experiments we use $L(\cdot) \triangleq\|\cdot\|_2^2$, however other penalty functions (e.g.,\ the 1-norm, Huber loss) can be used. The signal loss~(Equation \ref{eq:signallossvariational}) measures the separation of features between $\bx$ and time-warped $\by$ and is analogous to the classic DTW discrepancy given by the cost along the optimal warping path. 

% \paragraph{Warp Regularisation} 

\textbf{Warp Regularisation}\hspace{0.3cm} The warp regularisation functional $\cR$ in Equation~\ref{eq:optvariational} is defined as
\begin{equation}\label{eq:regvariational}
    \cR(\phi) = \int_0^1 R\big(\phi'(t) - 1\big) \text{d}t,
\end{equation}
where $R: \bbR\rightarrow \bbR$ is a penalty function on deviations from the identity warp, i.e.,~$\phi'(t) = 1$. We use a quadratic penalty $R(u) = u^2$ in this work, consistent with~\cite{deriso2019general}. This regularisation term penalises warp functions with large jumps and a sufficiently high $\lambda$ brings $\phi$ close to the identity. Regularisation is crucial for preventing noisy and/or pathological warps~\citep{ZHANG201791, wanggraphicaltw} from being produced from GDTW (and DTW, more generally), and can greatly affect alignment accuracy. We can select $\lambda$ optimally using cross-validation~\citep{deriso2019general}.

\section{Simplified NLP Formulation for GDTW}\label{sec:dtwnlp}

While Section~\ref{sec:gdtwvariational} provides a high-level overview of the GDTW problem, we did not specify a concrete parameterisation for $\phi$, which dictates how the optimisation problem in Equation~\ref{sec:gdtwvariational} will be solved. In this section, we provide simplifying assumptions on $\phi$, which allows us to solve Equation~\ref{sec:gdtwvariational}. We follow the approach in~\citet{deriso2019general} to reduce the infinite dimensional variational problem in Equation~\ref{eq:optvariational} to a finite dimensional NLP. This is achieved by first assuming that $\phi$ is piecewise linear, allowing it to be fully defined by its value at $m$ knot points $0 = t_1 < t_2 < \dots < t_{m} = 1$. Knot points can be uniformly spaced or just be the observation times used to parameterise $\by$. Formally, piecewise linearity allows us to represent $\phi$ as $\bphi = (\phi_1, \dots, \phi_{m})\in\bbR^m$ where $\phi_i = \phi(t_i)$. The other crucial assumption involved in reducing Equation~\ref{eq:optvariational} to an NLP is replacing the continuous integrals in the signal loss and warp regularisation in Section~\ref{sec:gdtwvariational} with numerical approximations given by the trapezoidal rule. These assumptions yield an alternative optimisation problem given by
\begin{equation}\label{eq:optnlp}
\begin{array}{rl}
    \text{minimise} & \hat{f}(\bx, \by, \lambda, \bphi) \triangleq \hat{\cL}(\bx, \by, \bphi) + \lambda \hat{\cR}(\bphi)\\
    \text{subject to} & s^\text{min}_i \leq \frac{\phi_{i+1}-\phi_i}{\Delta t_i} \leq s^\text{max}_i, \quad b_i^\text{min} \leq \phi_i \leq b_i^\text{max} \quad \forall i,
\end{array}
\end{equation}
where $\Delta t_i = t_{i+1} - t_i$. The new signal loss $\hat{\cL}$ is given by
\begin{equation}\label{eq:signallossnonvariational}
    \hat{\cL}(\bx, \by, \bphi) = \sum_{i=1}^{m-1} \frac{L_{i+1} + L_i}{2} \Delta t_i , \quad L_i \coloneqq L(\bx(t_i) - \by(\phi_i)),
\end{equation}
which follows from applying the definition of $\bphi$ and the trapezoidal approximation to Equation~\ref{eq:signallossvariational}. Note, the non-convexity of objective $\hat{f}$ (even when assuming convex $L$ and $R$) is caused by the $\bx(\phi_i)$ terms for arbitrary signal $\bx$. The new regularisation term $\hat{\cR}$ is given by
\begin{equation}\label{eq:regnonvariationl}
    \hat{\cR}(\bphi) = \sum_{i=1}^{m-1} R\left(\frac{\phi_{i+1}-\phi_i}{\Delta t_i} - 1\right) \Delta t_i ,
\end{equation}
noting that since $\phi'$ is assumed to be piecewise constant, we can simply use Riemann integration to evaluate the continuous integral in $\cR$ exactly. The simplified problem in Equation~\ref{eq:optnlp} is actually a continuous non-linear program with decision variable $\bphi\in\bbR^m$ and linear constraints. However, the objective function is non-convex~\citep{deriso2019general} and we will now describe the method used to find good solutions to Equation~\ref{eq:optnlp} using approximate dynamic programming.


\section{Declarative DTW Forward Pass}\label{sec:forwardpass}

Our DecDTW layer encodes an implicit function $\bphi^\star = \text{DecDTW}(\bx, \by, \lambda, \mathbf{s^\text{min}}, \mathbf{s^\text{max}}, \mathbf{b^\text{min}}, \mathbf{b^\text{max}})$, which yields the optimal warp given input signals $\bx, \by$, regularisation $\lambda$ and constraints $\mathbf{s^\text{min}}=\{s_i^\text{min}\}_{i=1}^m$, $\mathbf{s^\text{max}}=\{s_i^\text{max}\}_{i=1}^m$, $\mathbf{b^\text{min}}=\{b_i^\text{min}\}_{i=1}^m$, $\mathbf{b^\text{max}}=\{b_i^\text{max}\}_{i=1}^m$. The warp $\bphi^\star$ can be used in a downstream loss $J(\bphi^\star)$, e.g.,~the error between predicted warp $\bphi^\star$ and a ground truth warp $\bphi^\text{gt}$. The GDTW discrepancy is recovered by setting $J = \hat{f}$. Compared to Soft-DTW\citep{cuturi17a}, which produces soft alignments, DecDTW outputs the \textit{optimal}, continuous time warp. The DecDTW forward pass solves the GDTW problem given by Equation~\ref{eq:optnlp} given the input parameters. We solve Equation~\ref{eq:optnlp} using a dynamic programming (DP) approach as proposed in~\cite{deriso2019general} instead of calling a general-purpose NLP solver; this is to minimise computation time. 

The DP approach uses the fact that $\bphi$ lies on a compact subset of $\bbR^m$ defined by the global bounds, allowing for efficient discretisation. Finding the globally optimal solution to the discretised version of Equation~\ref{eq:optnlp} can be done using DP. The resultant solution $\bphi^\star$ is an approximation to the solution of the continuous NLP, with the approximation error dependent on the resolution of discretisation. We can reduce the error given a fixed resolution $M$ using multiple iterations of refinement. In each iteration, a tighter discretisation is generated around the previous solution and the new discrete problem solved. A detailed description of the solver can be found in~\cite{deriso2019general} and the Appendix.

Note, for the purpose of computing gradients in the backward pass, detailed in Section~\ref{sec:backwardpass}, it is important that the approximation $\bphi^\star$ is suitably close to the (true) solution of Equation~\ref{eq:optnlp}. Otherwise, the gradients computed in the backward pass may diverge from the true gradient, causing training to be unstable. The accuracy of $\bphi^\star$ is highly dependent on the resolution of discretisation and number of refinement iterations. We discuss how to set DP solver parameters in the Appendix.


\section{Declarative DTW Backward Pass}\label{sec:backwardpass}

We now derive the analytical solution of the gradients of $\bphi^\star$ recovered in the forward pass w.r.t.~inputs $\bz = \{\bx, \by, \lambda, \mathbf{s^\text{min}}, \mathbf{s^\text{max}}, \mathbf{b^\text{min}}, \mathbf{b^\text{max}}\}\in\bbR^n$ using Proposition 4.6 in~\citet{Gould:PAMI2021} (note that gradients w.r.t.~signals $\bx,\by$ are w.r.t.~the underlying observations $\bX,\bY$). Unlike existing approaches for differentiable DTW, DecDTW allows the regularisation weight $\lambda$ and constraints to be learnable parameters in a deep network. Let $\tilde{h} = [h_1, \dots, h_p]$, where each $h_i: \bbR^n \times \bbR^m \rightarrow \bbR$ corresponds to an \textit{active} constraint from the full set of inequality constraints described in Equation~\ref{eq:optnlp}, rearranged in standard form, i.e.,~$h_i(\cdot)\leq 0$. For instance, an active local constraint relating to time $t_j$ can be expressed as $h_i(\bz, \bphi) = \phi_{j+1} - \phi_j -\Delta t_j s_j^\text{max} \leq 0$. Assuming non-singular $H$ (note, this always holds for norm-squared $L$ and $R$) and $\text{rank}(D_{\bphi} \tilde{h}(\bz, \bphi)) = p$, the backward pass gradient is given by
\begin{equation}\label{eq:ddnbackward}
    D\bphi(\bz) = H^{-1}A^\top (A H^{-1}A^\top)^{-1} (AH^{-1}B-C) - H^{-1}B,
\end{equation}
where $D\phi(\bz) \in \bbR^{m\times n}$ is the Jacobian of estimated warp $\bphi$ w.r.t.~inputs $\bz$ and
\begin{equation}
    \begin{array}{rl}
    A = D_{\bphi} \tilde{h}(\bz, \bphi) \in \bbR^{p\times m} & B = D_{\bz\bphi}^2 \hat{f}(\bz, \bphi)\in\bbR^{m\times n} \\
    C = D_{\bz} \tilde{h}(\bz, \bphi) \in \bbR^{p\times n} & H = D_{\bphi\bphi}^2\hat{f} (\bz, \bphi) \in \bbR^{m\times m}.
    \end{array}
\end{equation}
Observe that the objective $\hat{f}$ as defined in Equation~\ref{eq:optnlp} only depends on a subset of $\bz$ corresponding to $\bx,\by,\lambda$ and similarly, constraints $\tilde{h}$ only depend on $\mathbf{s^\text{min}}, \mathbf{s^\text{max}}, \mathbf{b^\text{min}}, \mathbf{b^\text{max}}$. While Proposition 4.6 in~\cite{Gould:PAMI2021} has additional terms in $B$ and $H$ involving Lagrange multipliers, we note that since $\tilde{h}$ is at most first order w.r.t.~$\{\bz, \bphi\}$, these terms evaluate to be zero and can be ignored. We discuss how to evaluate Equation~\ref{eq:ddnbackward} using vector-Jacobian products efficiently in the Appendix.