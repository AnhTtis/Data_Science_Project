\section{Appendix}

We provide additional implementation details for DecDTW, as well as additional qualitative examples for both experiments in this section.

\subsection{Dynamic Programming solver for GDTW}\label{ssec:appsolver}

The mechanics of the solver are described as follows. For each $i$ we can discretise $\phi_i$ into $M$ values $\{\phi_{i,j}\}_{j=1}^M$ uniformly between global bounds $b_i^\text{min}, b_i^\text{max}$. This forms a graph where each of the $mM$ values correspond to nodes and furthermore, temporally adjacent nodes, i.e.,~$\phi_{i-1,j},\phi_{i,k}$ for all $j,k$ are connected with edges, for a total of $(m-1)\times M^2$ edges. Node costs correspond to signal loss values in Equation~\ref{eq:signallossnonvariational} and edge costs correspond to warp regularisation values in Equation~\ref{eq:regnonvariationl}. Edges which violate the local constraints $s_i^\text{min}, s_i^\text{max}$ are given a cost of $\infty$. The global minima of this new discrete optimisation problem corresponds to the minimum cost path through the graph and is solved using dynamic programming in $O(mM^2)$ time complexity. Iterative refinement of the discretisation and solution is performed as described in~\cite{deriso2019general}. We use three iterations of discretisation with discretisation factor $\eta = 0.125$ in all of our experiments. Furthermore, we set $M=\max(50, n)$, where $n$ is the length of time series $\bX$. Figure~\ref{fig:appsolver} illustrates the mechanics of the DP solver for a subsequence alignment problem and shows that the solution after refinement is comparable to calling an SLSQP solver~\citep{2020SciPy-NMeth} directly.

In general, one can set $\eta$ and $M$ using hyperparameter optimisation over a validation dataset. Assume we have a validation dataset where each observation is comprised of a time series pair. For any given $\eta$ and $M$ and number of refinement iterations $r$, we can use GDTW to compute the optimal alignment $\bphi^\star$ using the DP solver. Furthermore, we can use $\bphi^\star$ to initialise an NLP solver such as in~\cite{2020SciPy-NMeth} to find a refined solution $\bphi^\star_{NLP}$, which will be closer to the true solution of Equation~\ref{eq:optnlp}. The goal of the hyperparameter optimisation is to find the least computationally expensive solver configuration (measured by computation time) subject to constraints on approximation error (measured w.r.t.~objective function $\hat{f}$ or warp $\bphi$).

\begin{figure}[ht]
\noindent\includegraphics[width=\textwidth]{figs/gdtw_solver.pdf}
\caption{Illustration of the DP solver mechanics on a subsequence alignment problem, where $\phi$ maps times in the shorter sequence $\by$ to longer sequence $\bx$. A uniform discretisation is generated between global constraints in the first iteration. Subsequent discretisations are made between tighter bounds around the previous solution. The objective function value decreases at each iteration until convergence after 3 iterations. The objective value $\hat{f}$ from the SLSQP solver~\citep{2020SciPy-NMeth} is 0.02325, close to the DP solver at the final iteration and furthermore, we can see qualitatively that the solved warps are indistinguishable between the DP and SLSQP approach.}\label{fig:appsolver}
\end{figure}


\subsection{Efficient Computation of Backward Pass}

We do not construct the full Jacobian $D\phi(\bz)$ from Equation~\ref{eq:ddnbackward} explicitly, instead directly computing the vector-Jacobian products required for automatic differentiation libraries. Let $J(\bphi)$ be a downstream loss function defined over the estimated alignment (e.g.,~MSE to a ground truth alignment $\bphi^\text{gt}$). Our goal for the end-to-end model is to compute $DJ(\bz) = v^\top D\bphi(\bz)$, where $v^\top = J(\bphi)\in\bbR^{1\times m}$. To do this, we evaluate $v^\top D\bphi(\bz)$ from left to right, pre-computing and caching $H^{-1}A^\top$ beforehand. In addition, note that $H$ is tridiagonal from the definition of $\hat{f}$, where off-diagonal elements correspond only to the warp regularisation component $\hat{\cR}$. This allows us to solve $v^\top H^{-1}$ in $O(m)$ time. Finally, observe that $B$ and $C$ are in a sense complementary to each other. Specifically, columns of $B$ relating to $\mathbf{s^\text{min}}, \mathbf{s^\text{max}}, \mathbf{b^\text{min}}, \mathbf{b^\text{max}}$ are zero since the objective function $\hat{f}$ only depends on $\bx, \by, \lambda$. Conversely, columns of $C$ relating to $\bx, \by, \lambda$ are zero since the constraint functions only depend on $\bphi$ and $\mathbf{s^\text{min}}, \mathbf{s^\text{max}}, \mathbf{b^\text{min}}, \mathbf{b^\text{max}}$. This allows efficient evaluation of Equation~\ref{eq:ddnbackward} w.r.t~the two blocks $\{\bx, \by, \lambda\}$ and $\{\mathbf{s^\text{min}}, \mathbf{s^\text{max}}, \mathbf{b^\text{min}}, \mathbf{b^\text{max}}\}$ separately by setting $C$ (resp. $B$) to zero. Our full, efficient backward pass implementation as decribed is provided in our code at \href{https://github.com/mingu6/declarativedtw.git}{https://github.com/mingu6/declarativedtw.git}.

\subsection{Train Time Scalability Analysis}

We present results for train time scalability in this section using the same experimental setup as the test time scalability analyisis presented in Section~\ref{fig:scaletest}. Unlike at test time, we aren't able to trade-off warp estimation accuracy for computation time during training. This is because the backward pass computation given by Equation~\ref{eq:ddnbackward}, assumes the estimated warp $\phi^\star$ satisfies the first-order optimality conditions for the NLP defined in Equation~\ref{eq:optnlp}. To ensure this assumption holds, we need to ensure our solver discretises $\phi$ finely enough such that $\phi^\star$ is sufficiently close to an optimal point (up to a suitable numerical tolerance). In practice, this can be achieved with multiple rounds of iterative refinement (we use three in our experiments, as described in Section~\ref{fig:appsolver}) and a sufficiently large discretistion level $M$ (again, we set $M=n$ in this analysis). 

\begin{wrapfigure}{r}{0.40\textwidth}
\centering
\vspace{-0.7cm}
\noindent\includegraphics[width=0.35\textwidth]{figs/scalability_train.pdf}
\caption{Train time scalability}\label{fig:scaletrain}
\vspace{-0.7cm}
\end{wrapfigure}
Figure~\ref{fig:scaletrain} illustrates the results of our train time scalability analysis. Note, the computation times presented include the \textit{total} time taken for a training iteration, including the forward and backward pass. We observe that a training iteration of DecDTW is 4-20x (resp. 15-50x) slower than Soft-DTW (resp. DTWNet), due to the iterative warp refinement required for fine-grained warp estimation in the DecDTW forward pass. However, we observe that DILATE is in fact slower for training compared to DecDTW; this is attributed to the relevant efficiency of our backward pass being explicitly computed rather than computed through unrolling the DP recursion. 


\subsection{Experimental Setup for Audio-to-Score Alignment}

\paragraph{Dataset Description} Ground truth alignments were generated between the 193 piano performances to their associated scores using the methodology proposed in~\cite{thickstun2020rethinking}. We used the authors' default hyperparameters for ground truth generation. These 193 performances are divided into train (94), validation (49) and test (50) splits. Furthermore, scores are not shared across splits. Audio is sampled at 22.1kHz with a hop size of 1024 for generating a uniformly spaced time series of base features for each performance and synthesised score. Furthermore, each performance is split into uniform, non-overlapping slices of length 256, which corresponds to $\thicksim$12 seconds of audio per slice. There were a total of 995, 565 and 541 slices, across the train, validation and test splits, respectively. The hyperparameters used to generate constant-Q transform (48-dim) and chromagram (12-dim) features are identical to the ones used in~\cite{thickstun2020rethinking}. For the mel spectrogram features, we used 128 mel bands, yielding a 128-dim feature. We provide the full dataset generation procedure in our code. 

\paragraph{Learning Task} For learned methods (3-6), we apply a feature extractor comprised of a single layer bi-directional GRU~\cite{cho14gru} (with random initialisation) on top of base audio features. This feature extractor operates as a sequence-to-sequence model. Specifically, the GRU layer takes as input a length $N$ sequence of base audio features of shape $\bbR^{N\times d}$, and outputs learned features of shape $\bbR^{N\times d_1}$, where the output dimension of the GRU layer is set as $d_1=128$ in our experiments. The learned features are then L2-normalised before being used to compute both a downstream alignment and training loss for each learned comparison method. Alignments in Table~\ref{tab:audiosummary} are generated using DTW for Base (D), Soft-DTW, Along GT, DILATE and using GDTW for Base (G) and DecDTW. 

\paragraph{Comparison Methods} The training losses for (3-6) are described as follows: The Soft-DTW~\citep{cuturi17a} method (3) computes the Soft-DTW discrepancy between two feature sets and utilises no path information. Along GT (4) computes the discrepancy between features along the path given by ground truth alignments. DILATE~\citep{leguen19dilate} (5) is a hybrid loss which consists of a \textit{shape loss} (equivalent to the Soft-DTW discrepancy) and a \textit{temporal loss}, which penalises deviations from the predicted and ground truth paths. The relative weighting between both losses is given by a hyperparameter $0 \leq \alpha \leq 1$. Ground truth path information in DILATE is given by a penalty matrix $\bOmega$. Elements within each row (corresponding to a single point in time in the score) of $\bOmega$ (where ground truth alignments are defined) is given a penalty term based on the absolute alignment error. DecDTW (6) is able to train on the $\timerr$ loss directly. Full implementation for DecDTW and all comparison methods for this experiment are provided in our code.

\paragraph{Training Hyperparameters} We use the Adam~\citep{kingma:adam} optimiser with a learning rate of 0.0001 and a batch size of 5 for all methods. We trained for 300 epochs for DILATE and 20 epochs for all remaining methods and selected the model which yielded minimum $\timerr$ over the validation set. For DecDTW and Base (G), we set GDTW the regularisation hyperparameter $\lambda=0.1,0.7,0.9$ for CQT, chroma and melspec features, respectively. These regularisation weight values were selected using a grid search over the average $\timerr$ across the validation set for the base features. For the Soft-DTW~\citep{cuturi17a} comparison method we used $\gamma=1$, noting that results do not meaningfully change for different $\gamma$. For DILATE, $\alpha=0$ and $\gamma=0.1$ yielded the best results. We found that a low learning rate and high number of epochs were required for stable training under DILATE, in contrast to DecDTW.

\subsection{Experimental Setup for Visual Place Recognition}

\paragraph{Construction of Paired Sequences Dataset} The single-image dataset used to train the baseline VPR retrieval network is used to bootstrap the paired sequences dataset used for sequence-based fine-tuning. Within each train/val/test split, GPS data is used to produce sequence pairs. We designated a fixed set of reference traverses (6 for training, 1 each for validation and test), with each traverse having an associated full data collection run with a given date/time identifier (e.g.,~2015-08-17-13-30-19). Each reference traverse is split into geographically overlapping but distinct reference sequences of length 25, with images spaced 5 meters apart within each reference sequence. Adjacent reference sequences within the same traverse have 20 meters overlap between them for training and 12 meters overlap for validation and test splits. For each reference sequence, five query sequences are sampled. Query sequences are randomly selected from the pool of available remaining (i.e.~non-reference) traverses within each split. The starting location of each query sequence is chosen randomly such that the query sequence is geographically contained by the reference sequences. 

Our resultant dataset, comprised of $\thicksim$22k, $\thicksim$4k, $\thicksim$1.3k sequence pairs for training, validation and testing, respectively, full covers the geographic region and a large set of conditions encompassed by the RobotCar dataset. A large variety of sequence pairs spanning different locations and condition pairs between query and reference enables features learned by sequence fine-tuning to generalise between training and the unseen (both in geographic location and date) test set. We also ensure that the large number of and uniformly spaced test sequences allow for a comprehensive and challenging evaluation. While we can arbitrarily increase the size of this dataset, we find the variety present in our bootstrapped dataset is enough to yield significant gains for sequence fine-tuning. Full dataset lists for the paired sequence datasets are provided in the supplementary material.

\paragraph{Comparison Methods} Similar to the audio experiments, all learned sequence fine-tuning methods apply a loss on top of sequences of extracted image embeddings. The OTAM~\citep{Cao_2020_CVPR} loss is the Soft-DTW discrepancy loss with a minor modification to allow for subsequence alignment. Along GT computes the discrepancy between features along the path given by ground truth alignments. For DILATE~\citep{leguen19dilate}, similar to the audio experiments, we needed to parameterise the penalty matrix $\bOmega$. The value for element $i,j$ is given by the (squared) GPS error (in meters) between the geotages of query image $i$ and reference image $j$. Finally, DecDTW is able to use the \textit{task loss}, i.e.~the \textit{maximum} (or mean) GPS error along the \textit{optimal} alignment to improve localisation performance. DILATE is not able to learn directly on the task loss due to the path being encoded as a \textit{soft} alignment path matrix.

\paragraph{Hyperparameters for Single Image VPR} We train single image NetVLAD representations with VGG-16 backbone pretrained on ImageNet. NetVLAD layer uses 64 clusters, leading to a 32,768 dimensional time series embedding. Images are resized to 240 $\times$ 180 (width $\times$ height), consistent with~\cite{thoma20soft}. Triplet loss margin is set to 0.1 and 10 negatives per anchor-positive pair are used. The model is trained using the Adam optimizer with a learning rate of 1e-5 and a batch size of 4. Our training setup is based on the recently released visual geolocalization benchmark~\citep{berton2022deep} and its associated codebase\footnote{https://github.com/gmberton/deep-visual-geo-localization-benchmark}. We provide training and dataset setup code in the supplementary materials. 
 
\paragraph{Hyperparameters for Sequence-based VPR} For the sequence fine-tuning experiments, we use a learning rate of 0.0001, batch size of 8 for a maximum of 10 epochs across all methods. In contrast to the audio experiments, query and reference images are both fed through a shared feature extractor comprised of a VGG-16 backbone, followed by a NetVLAD aggregation layer. We checkpoint models during training based on the average maximum GPS error over the full validation set five times per epoch. We selected a fixed regularisation weight of $\lambda=0.1$ for DecDTW using the validation set and embeddings from the single-image model. For OTAM~\cite{Cao_2020_CVPR}, we selected $\gamma=1$, similar to the audio experiments, noting results were insensitive to this parameter. For DILATE~\cite{leguen19dilate}, we used $\gamma=0.1$ and similar to the audio experiments, we set $\alpha=0$ and $\gamma=1$.

\subsection{Additional Results for Audio-to-Score Alignment Experiments}

In this section, we investigate if features learned using each comparison method presented in Section~\ref{ssec:audioexper} generalises across both DTW and GDTW alignments at test time. In Table~\ref{tab:audiosummaryappdtw}, we present results for each comparison method using only DTW alignments at test time. Notably, the main difference to Table~\ref{tab:audiosummary} is the DecDTW column; the features output by the GRU(s) trained using $\timerr$ loss are placed into a DTW alignment layer (as opposed to GDTW) during testing. 

We can see in Table~\ref{tab:audiosummaryappdtw} that features learned with DecDTW produce poor alignment accuracy using DTW for both CQT and chroma features when compared to both base features (no learning) and DILATE. In addition, test alignment performance using DTW is significantly poorer compared to using GDTW (see last column in Table~\ref{tab:audiosummaryappgdtw}).

\begin{table}[h]
\centering
\caption{Results for the audio-to-score alignment experiments (DTW alignment)}
\label{tab:audiosummaryappdtw}
\begin{tabular}{l|ccccc} 
\toprule
\multicolumn{6}{c}{Reported metrics are TimeErr / TimeDev (ms)}                                                \\ 
\hline
Feature Type & Base (D)             & Soft-DTW  & Along GT & DILATE                    & DecDTW                     \\ 
\hline
CQT          & 35/ 90 & 49 / 130 & 49 / 130 & \textbf{29} / \textbf{45}          & 100 / 210                  \\
Chroma       & 50 / 115         & 59 / 142 & 59 / 142 & \textbf{28} /\textbf{41} & 45 / 117                   \\
Melspec      & 122 / 235        & 55 / 153 & 52 / 145 & \textbf{26} / \textbf{40}                  & 31 / 76 \\
\hline
\end{tabular}
\end{table}

We also present results in Table~\ref{tab:audiosummaryappgdtw}, which evaluate features learned under each comparison method using only GDTW alignments at test time. Features learned with methods based on DTW (i.e.~Soft-DTW, Along GT, DILATE) perform worse than base features for GDTW alignment. This holds for especially true for DILATE, which yielded accuracy gains when evaluating with DTW alignments compared to base features but significantly degraded accuracy for GDTW alignment. DecDTW as expected, yielded gains compared to base features when evaluated using GDTW alignments. 

\begin{table}[h]
\centering
\caption{Results for the audio-to-score alignment experiments (GDTW alignment)}
\label{tab:audiosummaryappgdtw}
\begin{tabular}{l|ccccc} 
\toprule
\multicolumn{6}{c}{Reported metrics are TimeErr / TimeDev (ms)}                         \\ 
\hline
Feature Type & Base (G)    & Soft-DTW   & Along GT  & DILATE    & DecDTW                     \\ 
\hline
CQT          & 19 / 30 & 22 / 33   & 22 / 33   & 43 / 69   & \textbf{17} / \textbf{27}  \\
Chroma       & 24 / 39 & 34 / 51   & 34 / 51   & 70 / 112 & \textbf{19} / \textbf{31}  \\
Melspec      & 56 / 81 & 155 / 219 & 147 / 211 & 43 / 68 & \textbf{16 }/ \textbf{27}  \\
\hline
\end{tabular}
\end{table}

Overall, we conclude that features trained with an underlying alignment method (either DTW or GDTW) tends to specialise to the particular alignment method used in training. Note that the continuous time formulation of GDTW, which allows for interpolation of alignments, and the availability of regularisation $\lambda$, cause GDTW alignments to outperform DTW overall at test time across most combinations of methods and features.

\subsection{Additional Results for Visual Place Recognition Experiments}

In this section, we provide additional results to the ones presented in Table~\ref{tab:vprsummary} in Table~\ref{tab:vprappmax}. First, we add results for the \textit{independent retrieval} task. The independent retrieval task involves finding the most visually similar reference image for each query using nearest neighbours, independently, using image embeddings (no DTW optimisation).  Interestingly, fine-tuning a network trained using contrastive learning on single images (this loss is designed to maximise single image retrieval performance) using a sequence-based loss tends to overall \text{improve} independent retrieval performance at finer error thresholds ($\leq 5$m). However, performance is reduced at the looser tolerances ($> 5$m). This holds uniformly and significantly for DecDTW and DILATE across all test conditions. We hypothesise that the embeddings of networks fine-tuned on the alignment task are trained to better differentiate between more nuanced visual details between nearby reference images.

\begin{table}[h]
\centering
\caption{VPR experiment results (maximum error): Test Accuracy @2/3/5/10m}
\label{tab:vprappmax}
\begin{tabular}{lccc} 
\toprule
Method          & Overcast                                       & Sunny                                          & Snow                                                      \\ 
\midrule
Base (DTW)      & 0.7/12.3/44.3/73.8                             & 0.5/8.7/40.2/62.2                              & 1.6/11.1/44.3/62.3                                        \\
Base (GDTW)     & 10.2/31.7/65.6/\textbf{\textbf{90.3}}          & 9.0/33.2/75.1/\textbf{\textbf{99.3}}           & 6.2/25.7/73.2/\textbf{\textbf{100.0}}                     \\
OTAM/Along GT   & 0.7/12.3/44.3/73.8                             & 0.5/8.7/40.2/62.2                              & 1.6/11.1/44.3/62.3                                        \\
DILATE          & 0.7/14.5/54.5/80.0                             & 1.5/11.9/40.9/69.7                             & 1.3/12.2/44.6/68.7                                        \\
DecDTW          & \textbf{25.4}/\textbf{45.0}/\textbf{68.3}/90.1 & \textbf{22.8}/\textbf{50.4}/\textbf{84.3}/98.3 & \textbf{13.5}/\textbf{44.8}/\textbf{88.0}/\textbf{100.0}  \\ 
\hline
Base (Indep.)   & 0.0/11.6/47.9/77.0                             & 0.0/7.5/40.4/81.1                              & 1.6/10.0/50.1/87.1                                        \\
DILATE (Indep.) & 0.2/11.1/58.4/84.7                             & 0.2/17.2/61.7/93.0                             & 1.6/13.7/66.3/96.2                                        \\
DecDTW (Indep.) & 0.2/14.5/50.1/65.6                             & 0.2/9.9/35.8/56.7                              & 0.9/11.1/57.7/85.8                                        \\
\bottomrule
\end{tabular}
\end{table}

We additionally present results for the \textit{mean} pose error across the sequence (as opposed to the maximum in Table~\ref{tab:vprappmax}) in Table~\ref{tab:vprappmean}. This measure of positioning error is commonly referred to as the absolute trajectory error (ATE). The conclusions are equivalent to the maximum error case; fine-tuning on the GPS error directly using our DecDTW layer yields considerable performance improvements at sub 5m tolerances over the comparison methods.

\begin{table}[h]
\centering
\caption{Additional VPR experiment results (mean error): Test Accuracy @2/3/5/10m}
\label{tab:vprappmean}
\begin{tabular}{lccc} 
\toprule
Method          & Overcast                                       & Sunny                                          & Snow                                                       \\ 
\midrule
Base (DTW)      & 16.9/48.9/70.0/94.7                            & 16.2/42.9/62.0/91.8                            & 16.2/42.9/62.0/91.8                                        \\
Base (GDTW)     & 44.8/70.2/90.1/\textbf{98.1}                   & 47.0/85.2/\textbf{99.5}/\textbf{99.8}          & 43.5/79.2/\textbf{100.0}/\textbf{100.0}                    \\
OTAM/Along GT   & 16.9/48.9/70.0/94.7                            & 46.2/42.9/62.0/91.8                            & 16.2/42.9/62.0/91.8                                        \\
DILATE          & 22.5/57.9/77.0/96.1                            & 19.1/43.1/69.5/93.2                            & 15.1/40.4/67.0/96.5                                        \\
DecDTW          & \textbf{52.1}/\textbf{80.2}/\textbf{91.8}/97.8 & \textbf{65.9}/\textbf{90.8}/98.8/\textbf{99.8} & \textbf{56.5}/\textbf{93.4}/\textbf{100.0}/\textbf{100.0}  \\ 
\hline
Base (Indep.)   & 18.6/55.7/80.1/91.5                            & 16.9/50.8/85.4/96.8                            & 17.0/58.5/90.9/98.4                                        \\
DILATE (Indep.) & 26.9/69.7/90.1/93.7                            & 32.9/69.0/94.2/\textbf{99.8}                            & 22.8/68.5/97.3/99.1                                        \\
DecDTW (Indep.) & 22.5/55.9/72.4/85.7                            & 26.2/59.8/72.6/85.7                            & 19.3/61.4/86.0/92.7                                        \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Additional Qualitative Examples for Audio-to-Score Alignment Experiments}

In Figure~\ref{fig:appaudio} we present additional visualisations of example alignments for the audio to score alignment task. All features types (CQT, chroma, melspec) and before/after learning are presented in the figures. Example alignments are randomly sampled from the test set. The visualisations are a way proposed by~\cite{thickstun2020rethinking} to visually compare two alignments (e.g.,~an estimated and ground truth alignment) by plotting \textit{performance aligned scores}, where a performance aligned score is generated by mapping the score through a warping function. 

We generate a performance aligned score using both the ground truth alignment and the estimated alignments using audio features. Red identifies notes indicated by audio feature alignments, but not by the ground truth, and yellow identifies notes indicated by the ground truth, but not by the audio features. Note, even if a predicted alignment is close to the ground truth (in a $\timerr$ sense), mistakes made by the performer may yield significant looking errors (orange and red) in the visualisations.

\vspace{1cm}
\begin{figure}[ht!]
\noindent\includegraphics[width=\textwidth]{figs/example_appendix_0.pdf}
\noindent\includegraphics[width=\textwidth]{figs/example_appendix_1.pdf}
\label{fig:appaudio0}
\end{figure}
% \vspace{2cm}

\begin{figure}[ht!]
\noindent\includegraphics[width=\textwidth]{figs/example_appendix_2.pdf}
\noindent\includegraphics[width=\textwidth]{figs/example_appendix_3.pdf}
\caption{Example of four randomly selected test set alignments for the audio-to-score alignment experiments. All recordings are performances of segments from Bach's \textit{The Well Tempered Clavier}. Each column represents a particular feature type and each row represents a particular training loss.}
\label{fig:appaudio}
\end{figure}


\subsection{Additional Qualitative Examples for Visual Place Recognition Experiments}

We provide additional qualitative examples for the VPR experiments in Figure~\ref{fig:appvpr}. These figures illustrate the effect of DecDTW fine-tuning on resultant positioning accuracy for a set of test sequence pairs. In addition, Figure~\ref{fig:appvpr} provides example test images to illustrate the amount of appearance change observed between reference and query images.

\begin{figure}[ht!]
\noindent\includegraphics[width=\textwidth]{figs/ddn_dtw_vpr_app_0.pdf}
\noindent\includegraphics[width=\textwidth]{figs/ddn_dtw_vpr_app_1.pdf}
\noindent\includegraphics[width=\textwidth]{figs/ddn_dtw_vpr_app_2.pdf}
\noindent\includegraphics[width=\textwidth]{figs/ddn_dtw_vpr_app_3.pdf}
\caption{Additional example localisation output for the VPR experiments selected from the test set. Example query images from the sequence are provided in the top row and example database images are provided in the bottom row. Query poses are offset from the database for visualisation purposes only. The substantial intra-sequence velocity change in the last (bottom) example, combined with warp regularisation causes DecDTW training to perform worse than Base (G) and DILATE.}
\label{fig:appvpr}
\end{figure}