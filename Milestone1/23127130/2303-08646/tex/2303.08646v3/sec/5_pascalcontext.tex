\section{Experiments on Pascal Context Dataset}
%\raggedbottom
The Pascal Context~\cite{cPascalContext} dataset contains 4,998 training images and 5,105 testing images.
%
Following the common practice, we use its 59 semantic classes to conduct the ablation studies and experiments.
%
Unless specified, we train the models on the training set for 30K iterations for the ResNet backbone and 40K for Swin-Large and ConvNeXt-Large.

%%%%%%%% Problem identification %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\input{tables/ablation_stuides_dilation_vs_fpn}

\subsection{Problem verification}
\label{sec:problem}

\noindent\textbf{SemanticFPN is generally worse than dilation.}
We conduct experiments in Tab.~\ref{tab:ablation_dilation_vs_fpn} to demonstrate the issue we identify (observation 1 in Fig.~\ref{fig:hfgd:concept}).
% 
Many famous methods for context encoding (``Extra Encoding'') produce less accurate predictions when combined with an upsampler net (i.e. SemanticFPN) than directly using a dilated backbone, even if the upsampler produces higher-resolution feature maps.
%
Refer to our appendix, for detailed settings.

\noindent\textbf{Negative influence from the upsampler on the backbone.}
We find the joint fine-tuning of the upsampler and the backbone results in deteriorated backbone features.
%
To demonstrate it, we modify SemanticFPN by introducing an auxiliary FCN to predict the mask from the high-level features produced by the backbone (see Issues in Fig.~\ref{fig:hfgd:concept}).
The predictions from the auxiliary FCN branch become worse than the original FCN (44.35\% vs 45.87\% mIOU).
Another variant stops the auxiliary FCN's gradient from propagating back to the backbone, which produces even worse results (40.04\%).

\noindent\textbf{Advantage of pre-training.}
We conduct experiments to show the benefit of using the pre-trained backbone in Tab.~\ref{tab:ablation_imagenet_simple}.
Due to limited training data, there still exists a substantial gap after 6 times training iterations.
Thus, protecting the backbone to ensure its generalization is necessary and critical.
More experiments about this are presented in the supplementary.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Ablation studies on HFGD}

\noindent\textbf{Ablation studies on HFGM.}
%
In Tab.~\ref{tab:ablation_hfgm}, we evaluate the effectiveness of our proposed HFGM based on SemanticFPN (\ie no ``Extra Encoding'').
%
Although using only HFG (``+guidance'') or only axial attention (``+AA'') is helpful, using the full HFGM brings the most gain (1.80\% mIOU).
probably because AA can effectively broadcast the guidance information of HFG to all spatial locations (also see Sec.~\ref{sec:method:hfgm} for more discussion).




\noindent\textbf{Importance of stopping gradients.}
We use several stop-grad operations to protect the backbone weights, especially the early low-level weights, and only allow gradients from the backbone branch to update their weights gradually (see. Fig.~\ref{fig:URD:Arch}).
We tested removing all stop-grad operations and obtained significantly decreased accuracy (48.94\% vs 48.50\%).
%
% We also analyze what happens if we no longer protect the backbone branch (i.e. letting gradients propagate from the upsampler branch).
% We found that doing so resulted in a drop in mIOU down to 48.50\%.


\noindent\textbf{Ablation studies on CAE.}
%
% We conduct ablation studies on CAE in Tab.~\ref{tab:ablation_cae} and Tab.~\ref{tab:ablation_hfgm_cae}.
% 
In Tab.~\ref{tab:ablation_cae}, we compare with CAR under different settings on the Pascal Context dataset 
since CAR~\cite{cCAR} performs best in Tab.~\ref{tab:ablation_dilation_vs_fpn} and CAE is based on CAR~\cite{cCAR}.
Results show that CAE is more compatible than CAR when using SFPN and ``SFPN + HFGM''.
%
Combined with results in Tab.~\ref{tab:ablation_dilation_vs_fpn}, our CAE design outperforms all the other alternatives (``Extra Encodings'') with and without SemanticFPN and approaches the accuracy of the best dilation-based model in Tab.~\ref{tab:ablation_dilation_vs_fpn}.


Similar to experiments in Tab.~\ref{tab:ablation_hfgm},
we also analyze the effects of CAE with different HFGM settings.
%
HFGM now provides better guidance to the upsampler with the help of CAE, leading to further improved final results (50.28\% vs 48.94\% and 49.22\% vs 47.67\% in Tab.~\ref{tab:ablation_hfgm}).



%Our CAE design surpasses the other alternatives (``Extra Encodings'') by a substantial margin, with and without SemanticFPN~\cite{cPanopticFPN}.
%Note that CAE is the best for methods using an upsampler while SA (CAR) is the best for dilation methods, showing that previous context encodings are unsuitable for OS=32 feature maps, and our modification on SA (CAR) effectively improves it.


\input{tables/ablation_imagenet_simple}
\input{tables/ablation_studies_hfgm}
\input{tables/ablation_studies_cae}
\input{tables/ablation_stuides_hfgm_cae}

\noindent\textbf{Ablation studies on U-SFPN.}
%
In Tab.~\ref{tab:ablation_upsampling}, we conduct ablation studies on U-SFPN to verify the effectiveness of our modification on SFPN while fixing CAE and HFGM.
Replacing SFPN with U-SFPN improves the mIOU of ResNet-50 (CAE + HFGM) by 0.48\%, reaching 50.76\%.
The improvement is even larger (1.14\%) when using Swin-Large as the backbone 
%(Tab.~\ref{tab:ablation_upsampling})
.

\input{tables/ablation_studies_upsampling}



\noindent\textbf{Module-level ablation studies on HFGD.}
%
In Tab.~\ref{tab:ablation_hfgd},
% Finally, 
%we conduct ablation studies on each module of HFGD using previously found best configurations in Tab.~\ref{tab:ablation_cae}-\ref{tab:ablation_upsampling}.
%Using all modules together leads to significant accuracy improvement, indicating the effectiveness of the overall architecture.
%
We performed ablation studies on each module of HFGD using the best configurations found in Tab.~\ref{tab:ablation_cae}-\ref{tab:ablation_upsampling}. Using all modules together significantly improved accuracy, indicating the overall architecture's effectiveness.


\input{tables/ablation_studies_final}
\input{tables/ablation_hfg_other_upsampler}

\subsection{Computational cost of HFGD}
%
The computational cost of our HFGD and two other state-of-the-art methods are listed in Tab.~\ref{tab:HFGD:flops}.
% 
HFGD uses much lower GFLOPs than a similar dilation model (Self-Attention + CAR~\cite{cCAR}) but achieves better mIOU (50.76\% vs 50.50\%~\cite{cCAR}).
% 
Compared with SemanticFPN, HFGD ($OS=4$) achieves 3.62\% % mIOU gain with an affordable extra computation cost (71.63 GFLOPs vs 45.65 GFLOPs).

\input{tables/computational_cost}

\subsection{Comparison with the state-of-the-art methods}

To compare with the state-of-the-art, we adopt ConvNeXt-L as the backbone for our HFGD.
% 
We set the training iterations to 40K while
all the other training settings are the same as stated in Sec.~\ref{sec:HFGD:training_settings}.
% except that we set the training iterations to 40K.
% 
As shown in Tab.~\ref{tab:urd:SOTA-PascalContext}, our HFGD achieved 63.8\% mIOU with single-scale without flipping and 64.9\% mIOU with multi-scales with flipping, outperforming previous state-of-the-art by 1\% mIOU in ECCV-2022.
 %
HFGD is now the new state-of-the-art method on Pascal Context for the methods that only use the ImageNet pre-trained backbone without extra techniques~\cite{cAugReg,cFocalLoss,cVNet}. 


\begin{figure*}[th]
\centering
\includegraphics[width=1.0\linewidth]{images/vis_main.pdf}
\caption{
Visual comparisons 
% (Upper-left:Pascal Context, upper-right:COCOStuff, bottom:Cityscapes) 
between SemanticFPN ($OS=4$), HFGD ($OS=4$), and HFGD ($OS=2$).
Zoom in to see better.
The results are obtained using single-scale without flipping.
%
More visualizations are presented in the supplementary.
}
\label{fig:CFGD:main-R50-Vis}
\end{figure*}

\begin{figure}[th]
\centering
\includegraphics[width=1.0\linewidth]{images/umap_vis.pdf}
\caption{
% Comparing the baselines (FCN and Semantic) with our proposed solution (HFGM or entire HFGD) using UMAP visualization on the Pascal Context dataset.
UMAP~\cite{cUMAP} Visualization on Pascal Context. 
HFGD features are most separable from the inter-class perspective and most compact from the intra-class perspective, resulting in the best accuracy (Tab.~\ref{tab:ablation_hfgd}).
}
\label{fig:umap_vis}
\end{figure}

\subsection{Apply high-level guide to existing upsamplers }

Though we recommend using U-SFPN + CAE to fit our proposed HFGM for efficiency, many existing methods attempt to improve the accuracy of upsamplers by improving intermediate-specific operations. 
For example, FaPN~\cite{cFaPN} uses SENet~\cite{cSENet} and deformable convolutions to try to align low-level features with high-level features.
%
UperNet~\cite{cUper} uses PPM~\cite{cPSPNet} to improve the high-level features of the backbone network.
%

From another perspective, HFGM directly optimizes the final upsampling quality rather than the intermediate process.
Thus, it should be able to improve the accuracy
of these upsamplers further, using high-level features as teachers to
constrain their upsampling results.

Using the ablation experimental setup in the main paper, Tab~\ref{tab:ablation_hfg_other_upsamplers} verifies our HFGM can effectively boost mIOU for different upsamplers. 
%
We believe that HFGM has generalizability to other similar upsamplers.
%