\begin{abstract}
\noindent Existing pyramid-based upsamplers (\eg SemanticFPN), although efficient, usually produce less accurate results compared to dilation-based models when using the same backbone. 
%
This is partially caused by the \emph{contaminated} high-level features since they are fused and fine-tuned with noisy low-level features on limited data.
% 
To address this issue, we propose to use powerful pre-trained \textit{\textbf{h}}igh-level \textit{\textbf{f}}eatures as \textit{\textbf{g}}uidance (HFG) 
so that the upsampler can produce robust results.
%learn fine-grained high-level features better.
%
% when learning to upsample the fine-grained low-level features.
% 
Specifically, \emph{only} the high-level features from the backbone are used to train the class tokens, which are then reused by the upsampler for classification, guiding the upsampler features to more discriminative backbone features.
% 
One crucial design of the HFG is to protect the high-level features from being contaminated by using proper stop-gradient operations so that the backbone does not update according to the noisy gradient from the upsampler.
% 
To push the upper limit of HFG, we introduce a \textit{\textbf{c}}ontext \textit{\textbf{a}}ugmentation \textit{\textbf{e}}ncoder (CAE) that can efficiently and effectively operate on the low-resolution high-level feature, resulting in improved representation and thus better guidance.
%
We named our complete solution as the High-Level Features Guided Decoder (HFGD).
% 
We evaluate the proposed HFGD on three benchmarks: Pascal Context, COCOStuff164k, and Cityscapes.
%
HFGD achieves state-of-the-art results 
% on all datasets 
among methods that do not use extra training data,
demonstrating its effectiveness and generalization ability. 
%The complete code will be released.
\end{abstract}