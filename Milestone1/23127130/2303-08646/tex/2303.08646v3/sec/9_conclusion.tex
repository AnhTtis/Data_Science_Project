\section{Conclusion }
In this paper, we propose to use the high-level features as the teacher to guide the training of the upsampler branch (student), resulting in an effective and efficient decoder framework.
%
Specifically, the core of our method is using high-level features as guidance (HFG) and proper stop-gradient operations for the upsampler learning, which effectively addresses the observed issues in Fig.~\ref{fig:hfgd:concept}.
%
In addition, we explore a context-augmented encoder (CAE) to effectively enhance the OS=32 high-level features
and propose a modified version of SemanticFPN to better fit our HFGM.
%
% 
With thorough experiments, HFGD achieves largely improved performance over previous upsampling-based state-of-the-art methods that does not use extra training data, on Pascal Context, COCOStuff, and Cityscapes.
%
HFGD also achieves slightly better results than dilation-based CNNs but using much less computation cost (\eg Self-Attention + CAR~\cite{cCAR}).
%.
%
We refer the readers to our appendix for more ablation studies, experiments, implementation details, and visualizations.

\clearpage