\section{Proposed Method}
\label{sec:method}

The key to our method is using \textit{\textbf{h}}igh-level \textit{\textbf{f}}eature as \textit{\textbf{g}}uidance (HFG) to effectively guide the upsampler training without introducing side-effect to the well-trained backbone branch (Sec.~\ref{sec:method:hfgm}).
%
Since the high-level features play a crucial role, we design a context-augmented encoder (CAE) that operates on OS=32 to strike a good balance between efficiency and effectiveness (Sec.~\ref{sec:method:cae}).
%
Lastly, we make some modifications based on SemanticFPN (SFPN)~\cite{cPanopticFPN} (Sec.~\ref{sec:method:upsampling}) so that it better fits our HFGM and CAE.
% }

\subsection{High-level feature guided module (HFGM)}
\label{sec:method:hfgm}

\noindent\textbf{Motivation.} 
In our high-level feature guidance module,
we treat the high-level feature branch as the teacher (\ie not updated by the gradient from the student) and the low-level feature branch as the student.

The last stage high-level features (OS=32) extracted from a classification backbone, are already ``good'' representations for semantic segmentation since it is essentially a per-pixel classification task.
% 
However, the high-level features are too coarse ($OS=32$) for accurate segmentation.
% 
In contrast, features from the earlier stages are in higher resolution and may contain some useful details ignored by the aforementioned high-level features.
% more detailed information, and thus suitable for compensating the lost spatial details.
% 
However, a substantial gap exists between low-level backbone features and the final segmentation feature.
Thus, a calibration, guided by the high-level features, on the low-level features is required to suit the final segmentation task better.

\noindent\textbf{Guiding the upsampler.} 
Guidance is realized by reusing the class tokens and a stop-gradient operation.
Specifically, the backbone and upsampler branches share the same set of class tokens to calculate class probabilities and the cross-entropy loss.
The class tokens are trained to co-evolve with only the high-level features (\ie the guiding branch). 
They are not updated by the gradients from the upsampler branch via the stop-gradient operations (\ie the guided branch).
%
As a result,  the upsampler branch pixels are forced to approach their corresponding high-level pixel representations from the backbone branch to improve generalization. 

\noindent\textbf{Protecting pre-trained backbone.} 
It is important to protect the backbone from being contaminated by the training of the upsampler, as evidenced by observation 2 in Fig.~\ref{fig:hfgd:concept}.
%
We realize this goal with proper stop-gradient operations between the upsampler and backbone branches, resulting in a teacher-student~\cite{cBYOT} like distillation architecture (Fig.~\ref{fig:URD:Arch}).
% 

Specifically, the stop-gradient operations on $OS=n$ paths (between the upsampler branch and the backbone branch) stop updates from randomly initialized upsampler branch so that they can protect well-trained backbone features from being negatively affected.
%


Another small but beneficial operation is the inclusion of an axial attention~\cite{cAxialAttention,cAxialDeepLab,cCAA} layer.
%
This axial attention broadcasts the gradient of one pixel to all spatial locations, resulting in further improvement (see Tab.~\ref{tab:ablation_hfgm}).
%
In consideration of computation, we choose axial attention~\cite{cAxialAttention,cAxialDeepLab,cCAA} instead of full self-attention for spatial context augmentation since the resolution of the path is high (\eg $OS=2$ or $4$).
% 
Note that axial attention acts as a helper for the guidance, possibly because it effectively broadcasts the guidance signal back to all spatial locations.
Without guidance, its effect is limited (Tab.~\ref{tab:ablation_hfgm})

\subsection{Context-augmented encoder (CAE)}
\label{sec:method:cae}


To further push the upper limit, we propose to enhance the high-level features that serve as the guidance/teacher.
Considering computation overhead, we choose to operate on the low-resolution OS=32 feature maps to use any powerful modules (\eg ASPP~\cite{cDeepLab,cDeepLabV3,cDeepLabV3Plus}, PPM~\cite{cPSPNet}, OCR~\cite{cOCR}, and full self-attention~\cite{cNonLocal,cDualAttention,cAttentionIsAllYourNeed,cCFNet}) freely.

In our experiments, we propose a CAR-based~\cite{cCAR} configuration that achieves most accuracy gain (see Tab.~\ref{tab:ablation_cae}).
%
Specifically, we use a wider linear layer (\ie $1\times1$ conv with 2048 channels) to process backbone features.
Then, we use a linear layer to reduce the features to 512-D, followed by a full self-attention layer regularized by class-aware regularizations (CAR\cite{cCAR}) and a trailing convolution layer (256-d).
% 
Using a wider linear layer (\ie wide linear in Fig.~\ref{fig:URD:Arch}) performs much better than the commonly used $3\times3$ conv layer, possibly because the receptive field size of a pixel in OS=32 feature maps is already large enough.
Increasing its channel number also helps since it compensates for the lost capacity after changing to smaller kernels.

\subsection{Ultra SemanticFPN (U-SFPN)}
\label{sec:method:upsampling}
%
We slightly modify SemanticFPN (SFPN) to fit our HFGM and CAE better and enable it to work on even higher-resolution (OS=2) feature maps.
%
Firstly, we change the leading $1\times1$ conv to $3\times3$ conv layers since it collects more spatial information to reduce potential ``noise`` in the low-level features.
% 
Secondly, we increase the channel numbers from 128 to 256 (applied to SemanticFPN in experiments for fairness) since we find it beneficial to obtain more accurate and detailed results.
% 
Last but not least,
we can keep upsampling to an even higher resolution (\ie $OS=2$) and achieve better accuracy with the help of HFG\footnote{We only test $OS=2$ results on Cityscapes since the ground truth annotations on other datasets (\eg Pascal Context) are not as accurate.}.
%
To achieve this goal, we include one more ``conv-upsample'' block for every branch to get $OS=2$ feature maps.
%
Note that we did not use $OS=2$ low-level features from the backbone.
