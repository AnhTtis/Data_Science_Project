\section{Related Works}
\label{sec:HFGD:related_work}

\noindent\textbf{Dilated convolutions.} 
Dilated CNNs~\cite{cDeepLabV3,cDeepLabV3Plus,cMSCA} usually use dilated convolution layers starting from $OS=8$ feature maps, resulting in the constant resolution thereafter.
%
Dilation models are easy and fast to train since only a few randomly initialized weights are appended to the well-trained high-level backbone features.
However, the computation cost for the final layers increases quadratically with the length of the feature map.
%
Another issue is that dilation is only applicable for CNN backbones (\eg VGG~\cite{cVGG}, ResNet~\cite{cResnet}, and ConvNeXt~\cite{cConvNeXT}) but not for Transformer-based backbones (\eg ViT~\cite{cViT} and Swin~\cite{cSwin}).
 
\noindent\textbf{Pyramid-based upsampler.}
Pyramid-based upsampler is another popular strategy, such as UNet~\cite{cUNet}, SemanticFPN~\cite{cFPN,cPanopticSeg}, Uper~\cite{cUper} and FaPN~\cite{cFaPN}.
%
They often progressively merge features from different stages of the backbone network (\ie a feature pyramid) into the upsampled high-level feature maps from $OS=32$, resulting in much less computation cost than using a dilated backbone.

However, their accuracy is usually inferior to dilation, which is implicitly mentioned in many works. 
For example, DeepLab V3+ still uses the dilated backbone in the final; UperNet has a slightly worse mIOU than PSPnet, even with more advanced training strategies such as Synced Batch Norm. 
One reason is that they introduce more randomly initialized weights, especially those weights used to process early-stage low-level features.
%
Its drawbacks are two-fold.
%
Firstly, the new learnable weights are trained only with limited semantic segmentation data.
%
Secondly, directly updating the early-stage backbone features through a deep-supervision/shortcut fashion with such small amount of data will probably be harmful for generalization (Fig.~\ref{fig:hfgd:concept}).

\begin{figure*}[th]
\centering
\includegraphics[width=1.0\linewidth]{images/fig3.pdf}
\caption{
\textbf{High-level Feature Guided Decoder (HFGD).}
%
The core of HFGD is the high-level feature guided module that guides the upsampler learning and protects pre-trained backbone features by properly stopping gradients.
%
Then a context augmentation encoder (CAE) is included after the backbone to push the upper limit of HFG efficiently and effectively.
Lastly, a slightly modified ultra SemanticFPN (U-SFPN) is proposed to fit our HFGM/CAE better and supports operating on even higher resolution (OS=2).
%
Refer to Sec.~\ref{sec:method} for details.
}
\label{fig:URD:Arch}
\end{figure*}

\noindent\textbf{Context encoding modules.} 
There exist many powerful modules focusing on context encoding, such as pyramid pooling module (PPM)~\cite{cPSPNet}, 
atrous spatial pyramid pooling (ASPP)~\cite{cDeepLab,cDeepLabV3,cDeepLabV3Plus}, 
Self-Attention~\cite{cNonLocal,cDualAttention,cAttentionIsAllYourNeed}, etc.
%
They all effectively enlarge the receptive field size and aggregate more context information to obtain more powerful pixel representations.
However, their computational costs are often huge and increase quadratically with the length of the feature map.
To strike a good balance between efficiency and effectiveness, we insert the extra encoding module after the OS=32 feature maps.
%
Under this setting, we found them less effective or harmful, possibly because they were originally proposed for processing OS=8 feature maps.
% 
Finally, we make some modifications on a CAR-based~\cite{cCAR} so that it suits better to the HFG.
