\clearpage
%\setcounter{page}{1}
%\maketitlesupplementary

\appendix

{
   \newpage
       \twocolumn[
        \centering
        \Large
        %\textbf{\thetitle}\\
        \vspace{0.5em}\textbf{Appendix}\\
        \vspace{2.5em}
       ] %< twocolumn
   }

%Due to the page limit in the main paper, extra details, ablation studies, and visualizations are presented in this supplementary material.
%The code will be made publicly available on GitHub.

\section{Extra details}

We show the detailed setting of the Ultra SemanticFPN (U-SFPN) here.
% As mentioned in the main paper, we simply changed SemanticFPN~\cite{cPanopticFPN} to better cooperate with high-level feature guidance. 
U-SFPN is based on SemanticFPN~\cite{cPanopticFPN} with some modifications to better cooperate with high-level feature guidance. 
% We show its architecture in Figure 1.
Note that the right side of SemanitcFPN was originally 128-D. 
For fairness, all our experiments are set to 256-d (mentioned in the main paper).

\begin{figure}[th]
\centering
\includegraphics[width=1.0\linewidth]{images/usfpn.pdf}
\caption{
Architecture of U-SFPN.
%
Ultra SemanticFPN (U-SFPN) is modified from SemanitcFPN (SFPN)~\cite{cPanopticFPN} to better cooperate with high-level feature guidance. 
%
We highlight the modified layers/operations with a green background.
}
\label{fig:CFGD:supp:vis:USFPN}
\end{figure}

\section{Extra experiments}

\subsection{Experiments on ADE20k}

In addition to the datasets mentioned in the main text, we also compared our HFGD on ADE20k. 
%
ADE20k dataset comprises of 150 classes and is divided into 20210/2000 for training and validation purposes.
%
Following other works~\cite{cKMaXDeepLab,cDDP},
we use $641\times641$ crop size, and other settings are the same as the main paper. 
%
The results are shown in Tab.~\ref{tab:HFGD:supp:SOTA-ADE20K}.
%
HFGD significantly outperforms the other methods.

\begin{table}[th]
\centering
\small
\resizebox{\linewidth}{!}
{
\begin{tabular}{l|c|c|c|c}
\toprule%[1pt]
Methods & Backbone  & Avenue &\multicolumn{2}{c}{mIOU(\%)}\\
& & & SS & MF \\
\midrule
\midrule
SETR~\cite{cSETR} & ViT-L & CVPR'21 & - & 50.0 \\
Segmenter~\cite{cSegmenter} & ViT-L & ICCV'21 & 51.7 & 53.6 \\
SegFormer~\cite{cSegFormer} & MiT-B5 & NIPS'21 & 51.0 & 51.8 \\
Uper~\cite{cUper,cConvNeXT} & ConvNeXt-L & CVPR'22 & - & 53.7 \\
SegNeXt-L~\cite{cSegNeXt} & MSCAN-L & NIPS'22 & 51.0 & 52.1 \\
TSG~\cite{cTSG} & Swin-L & CVPR'23 & - & 54.2 \\
DDP~\cite{cDDP}(step1) & Swin-L & ICCV'23 & 53.1 & 54.4   \\
DDP~\cite{cDDP}(step3) & Swin-L & ICCV'23 & 53.2 & 54.4  \\
\midrule
HFGD & ConvNeXt-L & - & \textbf{54.3} & \textbf{55.4}    \\ 
\bottomrule%[1pt]
\end{tabular}
}
\caption{
% \hy{
Comparisons to state-of-the-art methods on ADE20k dataset.
\textit{SS:} Single-scale performance w/o flipping.
\textit{MF:} Multi-scale performance w/ flipping.
``-'' in column \textit{SS} indicates that this result was not reported in the original paper.
}
\label{tab:HFGD:supp:SOTA-ADE20K}
\end{table}

\subsection{Experiments on COCOStuff10k}

We also test our HFGD on COCOStuff10k, which is a subset of COCOStuff164k. 
The subset is split into 9000/1000 for training and testing.
%
Our training settings are identical to COCOStuff164k from the main paper.
The results are shown in Tab.~\ref{tab:HFGD:supp:SOTA-COCOSTUFF10K}. HFGD significantly outperforms the other methods.

\begin{table}[th]
\centering
\small
\resizebox{\linewidth}{!}
{
\begin{tabular}{l|c|c|c|c}
\toprule%[1pt]
Methods & Backbone  & Avenue &\multicolumn{2}{c}{mIOU(\%)}\\
& & & SS & MF \\
\midrule
\midrule
OCR~\cite{cOCR} & HRNet-W48~\cite{cHRNet} & ECCV'20 & - & 45.2 \\
CAA~\cite{cEfficientNet} & EfficientNet-B7~\cite{cEfficientNet} & AAAI'22 & - & 45.4\\
RankSeg~\cite{cRankSeg} & ViT-L & ECCV'22 & - & 47.9 \\
CAA + CAR~\cite{cCAR} & ConvNeXt-L & ECCV'22 & 49.0 & 50.0 \\
SegNeXt-L~\cite{cSegNeXt} & MSCAN-L & NIPS'22 & 46.4 & 47.2 \\
\midrule
HFGD & ConvNeXt-L & - & \textbf{49.5} & \textbf{50.3}    \\ 
\bottomrule%[1pt]
\end{tabular}
}
\caption{
% \hy{
Comparisons to state-of-the-art methods on COCOStuff10k dataset.
\textit{SS:} Single-scale performance w/o flipping.
\textit{MF:} Multi-scale performance w/ flipping.
``-'' in column \textit{SS} indicates that this result was not reported in the original paper.
}
\label{tab:HFGD:supp:SOTA-COCOSTUFF10K}
\end{table}


\subsection{Advantage of pre-training on COCOStuff10k}

% In the main paper, we have shown the huge advantages of pre-training in Pascal Context (Tab.~xxx\TODO{}). 
In addition to Pascal Context (Tab.~\ref{tab:ablation_imagenet_simple} in the main paper),
We also test its advantage on COCOStuff10k, which contains 9000 training images (only 4998 in the Pascal Context) and approaches the 10K COCO training images mentioned in~\cite{cRethinkingImageNet}.
%
% Because COCOStuff10k contains more training images than Pascal Context (9000 vs 4998) and approaches the 10K COCO training images mentioned in~\cite{cRethinkingImageNet}, we also validate the advantage of pre-training in COCOStuff10k.
%
We find that despite the increase in training samples (Tab.~\ref{tab:supp:ablation_imagenet_simple_coco}), the huge advantage of ImageNet pre-training remains undiminished. 

In fact, we are not the first to be aware of this issue. 
%
As early as 2019, NAS-based AutoDeepLab~\cite{cAutoDeepLab}, although using the COCO dataset (which can be seen as COCOStuff164k) for pre-training, due to the lack of ImageNet pre-training, it still has slightly worse mIOU compared to other methods.
%
 % We guess the advantages of ImageNet pre-training cannot be smoothed out unless the number of samples is half of the ImageNet pre-training samples.

\begin{table}[th]
\centering
\small
\resizebox{\linewidth}{!}
{\def\arraystretch{1} \tabcolsep=0.6em 
\begin{tabular}{l|c|c|c} 
\toprule
Training Iterations & 30K & 90K & 180K  \\
\midrule
\midrule
ResNet-50 (ImageNet) + FCN & 32.92 & - & - \\
ResNet-50 (scratch) + FCN  & 16.70 & 22.62 & 24.62 \\
\bottomrule
\end{tabular}
}
\caption{Simple experiments to present the importance and advantage of ImageNet pre-train.
%
Trained on COCOStuff10k dataset.
%
Results are in mIOU(\%).
}
\label{tab:supp:ablation_imagenet_simple_coco}
\end{table}

\begin{figure*}[ht]
\centering
\includegraphics[width=1.0\linewidth]{images/fig_classtoken.pdf}
\caption{
Visualization of inter-class dependencies among all the class tokens for SemanticFPN and FCN/SemanticFPN (HFG).
%
Zoom in to see better (\eg class names).
%
More high-score (\eg closer to the red) in the diagonal indicates better class discriminability.
%
SemanticFPN relies more on inter-class representation than FCN/SemanticHFG for classification.
%
See section~\ref{sec:supp:vis-class-token} for details.
}
\label{fig:CFGD:supp:vis:class_token}
\end{figure*}

\section{Extra visualizations}

\subsection{Visualization on class token}
\label{sec:supp:vis-class-token}

In this section, we visualize the inter-class dependencies between class tokens of ``SemanticFPN'' and ``FCN/SemanitcFPN (HFG)'' (When using HFG, SemanticFPN uses the same token as FCN).
%
Our visualization approach is inspired by the CAR~\cite{cCAR}, where the major difference is that CAR visualizes the class centers extracted from the feature map, while we visualize the class token for final classification.
%
According to Fig.~\ref{fig:CFGD:supp:vis:class_token}, SemanticFPN's class tokens have slightly lower diagonal confidence scores than FCN (around 20\%), which means SemanticFPN relies more on other classes' information during classification.
. 
% Thus, during classification, it relies more on other class representations.
%
By using the proposed HFG, the class token's discriminability in SemanticFPN is no longer weakened. 
%
This results in high-resolution and robust class representation at the same time. (Please refer to the main paper's experiments section for further details).





\subsection{Visual results on Pascal Context dataset}

For Pascal Context, we compare visual results
% the Pascal Context visualizations 
of SemanticFPN, Self-Attention (CAR~\cite{cCAR})~(Dilation $OS=8$), and our HFGD in Fig.~\ref{fig:HFGD:supp:vis:PascalContext}. 
%
We can see that SemnaitcFPN has better details than SA while
% , and conversely, 
SA has better class discrimination than SemanitcFPN. 
%
Our HFGD performs the best in both perspectives.


\begin{figure*}[t]
\centering
\includegraphics[width=0.8\linewidth]{images/vis_pascalcontext_r50_vs.pdf}
\caption{
Visual comparisons between SemanticFPN ($OS=4$), Self-Attention(CAR)~($OS=8$), and HFGD ($OS=4$) on the Pascal Context dataset.
Zoom in to see better.
The results are obtained using single-scale without flipping.
}
\label{fig:HFGD:supp:vis:PascalContext}
\end{figure*}


\subsection{Visual results on COCOStuff164k dataset}

We then compare COCOStuff164k results of SemanticFPN and our HFGD in Fig.~\ref{fig:HFGD:supp:vis:COCOStuff}. 
%
We observed that our HFGD provides better class discrimination and finer details than SemanticFPN for scenes, both indoor and outdoor, at close-up and long-range distances.

\begin{figure*}[th]
\centering
\includegraphics[width=0.6\linewidth]{images/vis_cocostuff_convnext-l_vs.pdf}
\caption{
Visual comparisons between SemanticFPN ($OS=4$) and HFGD ($OS=4$) on the COCOStuff164k dataset.
Zoom in to see better.
The results are obtained using single-scale without flipping setting.
}
\label{fig:HFGD:supp:vis:COCOStuff}
\end{figure*}


\subsection{Visual results on Cityscapes dataset}

Finally, we compare the visualizations of SemanticFPN and our HFGD on the Cityscapes dataset in Fig.~\ref{fig:HFGD:supp:vis:Cityscapes}.
%
Our proposed HFGD outperforms SemanticFPN, particularly for small objects that are far away.
% far ahead, 
such as street signs and signal lights. 
This is critical to safe autonomous driving perception.

\begin{figure*}[th]
\centering
\includegraphics[width=\linewidth]{images/vis_cityscapes_r50_vs.pdf}
\caption{
Visual comparisons between SemanticFPN ($OS=4$), HFGD ($OS=4$), and HFGD ($OS=2$) on the Cityscapes dataset.
Zoom in to see better.
The results are obtained using single-scale without flipping.
}
\label{fig:HFGD:supp:vis:Cityscapes}
\end{figure*}

