%File: formatting-instructions-latex-2024.tex
%release 2024.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai24}  % DO NOT CHANGE THIS
%\usepackage[submission]{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2024.1)
}

\usepackage[x11names]{xcolor}
\usepackage{booktabs}
\usepackage{paralist}
\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{multirow}
\usepackage{pifont}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

%\nocopyright

\definecolor{glaucous}{rgb}{0.38, 0.51, 0.71}
\definecolor{stop_grad_red}{rgb}{0.75, 0, 0}

\newcommand{\dk}[1]{\textcolor{magenta}{#1}}
\newcommand{\dknote}[1]{\textcolor{orange}{[KD:\@ #1]}}
\newcommand{\hy}[1]{\textcolor{blue}{#1}}
\newcommand{\hynote}[1]{\textcolor{red}{[HY:\@ #1]}}
\newcommand{\todo}[1]{\textbf{\textcolor{red}{[TODO:\@ #1]}}}
\newcommand{\TODO}[1]{\textbf{\textcolor{red}{[TODO:\@ #1]}}}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{54}}%

\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{\emph{HFGD}: High-level Feature Guided Decoder for Semantic Segmentation}

\author{Ye Huang\textsuperscript{\rm 1}\quad
        Di Kang \textsuperscript{\rm 2}\quad
        Shenghua Gao \textsuperscript{\rm 3}\quad
        Wen Li \textsuperscript{\rm 1}\quad
        Lixin Duan\textsuperscript{\rm 1}\thanks{Corresponding author}
}
\affiliations{
    \textsuperscript{\rm 1} University of Electronic Science and Technology of China\\
    \textsuperscript{\rm 2} Tencent AI Lab\\
    \textsuperscript{\rm 3} ShanghaiTech University\\
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1,\rm 2},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi




% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}




%\maketitle
\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\begin{center}
\centering
\includegraphics[width=\textwidth]{images/fig1.pdf}
\captionof{figure}{
% \hy{Concept of this work, includes our observations, issue found (marking as red), our solution with some of the results.
% Zoom in to see better.
% All the experiments are fully under control variable.}
\textbf{Problem identification and our solution.}
We notice that 1) using an upsampler net (i.e. SemanticFPN) is worse than using a dilated backbone; 2) the joint fine-tuning of the upsampler and the backbone usually results in worse backbone features (see Sec.~\ref{sec:problem}).
And we propose to use powerful pretrained \textit{\textbf{h}}igh-level \textit{\textbf{f}}eatures as \textit{\textbf{g}}uidance when training the upsampler.
}
\label{fig:hfgd:concept}
\end{center}}]

% 1. High-level Feature-Guided Module (HFGM)
%     - Guidance while upsampling - higher resolution and more discriminative
%     - Stop gradients - protected backbone & high-level features (from being contaminated by randomly initialized upsampler weights)
% 2. Context Augmentation Encoder (CAE)
%     - pushing the upper limit of the HFGM
%     - operates on low-resolution high-level feature, efficiently and effectively

\begin{abstract}
% We found existing pyramid-based upsamplers, such as SemanticFPN, perform worse than dilation because they have randomly initialized weights and cannot handle the low-level features well after training on limited data, which leads to poor class representation and also negative effects on the high-level features and backbone.
% \dk{
Existing pyramid-based upsamplers (e.g. SemanticFPN), although efficient, usually produce less accurate results compared to dilation-based models when using the same backbone. 
% }
This is partially caused by the \emph{contaminated} high-level features since they are fused and fine-tuned with noisy low-level features on limited data.
% 
% 
To address this issue, we propose to use powerful pretrained \textit{\textbf{h}}igh-level \textit{\textbf{f}}eatures as \textit{\textbf{g}}uidance (HFG) 
% for the upsampler learning.
when learning to upsample the fine-grained low-level features.
% 
Specifically, the class tokens are trained along with only the high-level features from the backbone.
These class tokens are reused by the upsampler for classification, guiding the upsampler features to more discriminative backbone features.
One key design of the HFG is to protect the high-level features from being contaminated with proper stop-gradient operations so that the backbone does not update according to the gradient from the upsampler.
% 
To push the upper limit of HFG, we introduce an \textit{\textbf{c}}ontext \textit{\textbf{a}}ugmentation \textit{\textbf{e}}ncoder (CAE) that can efficiently and effectively operates on low-resolution high-level feature, resulting in improved representation and thus better guidance.
We evaluate the proposed method on three benchmarks: Pascal Context, COCOStuff164k, and Cityscapes. 
Our method achieves state-of-the-art results 
% on all datasets 
among methods that do not use extra training data,
demonstrating its effectiveness and generalization ability. 
The complete code will be released.
\end{abstract}

\section{Introduction}
\label{sec:hfgd:intro}

Semantic Segmentation, which is a fundamental task in computer vision, requires densely predicted per-pixel class labels for a given image.
% for every pixel of the input image.
Most methods consist of an \emph{encoder} (i.e. backbone) to extract coarse (i.e. low-resolution) high-level abstract features and a \emph{decoder} to compensate lost spatial details from high-resolution low-level features.



%
%Rapid progresses~\shortcite{cFCN,cUNet,cPSPNet,cDeepLab,cDeepLabV3,cDenseASPP,cUper,cENCNet,cDeepLabV3Plus,cCFNet,cDualAttention,cANNN,cEMANet,cCCNet,cKSAC,cSegFormer,cCAA,cCAR} have been witnessed since the existence of fully convolutional networks (FCNs)~\cite{cFCN}, which makes dense prediction tasks much more efficient than before. 

% Due to lack of densely labeled images (e.g. 4.9K in Pascal Context~\cite{cPascalContext}, 118K in COCOStuff~\cite{cCocoStuff}),
% training samples 
% in semantic segmentation
% \footnote{One of the largest semantic segmentation data is COCOStuff~\cite{cCocoStuff}, which contains 118K training images. There are only 4.9K training images in Pascal Context~\cite{cPascalContext} and 2.9K training images in Cityscapes~\cite{cCityScapes}.)},
% 

Existing segmentation methods usually utilize backbones pretrained on large-scale image classification dataset
to extract expressive and more generalizable high-level semantic features
due to lack of densely labeled images\footnote{The scales of common semantic segmentation datasets vary from 2.9K training images in Cityscapes to 118K in COCOStuff while ImageNet-1K/21K contains 1.2M/14M images.}.
%
Popular classification backbones (e.g. ResNet~\shortcite{cResnet}, Swin~\shortcite{cSwin} and ConvNeXt~\shortcite{cConvNeXT}) usually includes multiple downsampling layers and finally produce small feature maps in an output stride of 32 (OS=32).
%
However, the OS=32 feature maps are too coarse to obtain accurate and detailed (e.g. object boundaries, thin/small objects) per-pixel classifications.
%
So there exist many works focusing on investigating stronger and more efficient upsampler in recent years.
% has been one major direction in recent years.

In one line of works, dilation-based methods~\shortcite{cDeepLab,cPSPNet,cDeepLabV3Plus} replace the downsampling operations in the backbone with dilated convolutions to prevent resolution reduction.
Dilation-based methods usually convergence faster and generalize better since dilation can effectively utilize the pretrained weights and does not introduce random new weights.
%
The limitations of using dilation are also obvious: 1) it only applicable to CNN-based backbones; 2) it brings too much computational cost.

% However, dilation has two major limitations: 1) It only can be applied on CNN based backbone. 2) It brings too much computational cost because the downsampling operations in the backbone are replaced.


In another line of works~\shortcite{cUper,cFaPN}, an upsampler branch is introduced (e.g. SemanticFPN~\shortcite{cPanopticFPN}) to incorporate fine details from low-level features into upsampled high-level features.
%
Although computationally more efficient, they often less effective than dilation-based methods since new random weights are required and they can only be trained with limited segmentation data.
% \hy{
What's worse, the originally good high-level features may be contaminated during the fine-tuning
(Fig.~\ref{fig:hfgd:concept} and Sec.~\ref{sec:problem}).
% (see ``observations'' in Fig.~\ref{fig:hfgd:concept} and Sec.~\ref{sec:problem} for the details)
% }

% On the other hand, the pyramid-based decoder (e.g. SemanticFPN) provides a computationally efficient solution to recover the high-resolution feature map without incurring too much computational cost. 
% However, it is usually less effective than dilation approaches because it introduces many new layers or operations for handling the migration between the low-level and upsampled high-level features.
%
% Those new operations and layers have randomly initialized weights, so they are not generalized enough (through limited training samples) to handle low-level features from the backbone. 
% Since low-level features are not yet fully encoded, they pollute the robust representation from the high-level feature when adding the details.
%
% 
% 
% 
% Moreover, they can also harm the backbone through back-propagation, particularly in the early stages of the backbone. 
% Those early stages layers were originally only responsible for low-level feature encoding (e.g. edge detection) during pretraining. 
% Now, they also need to be impacted by the high-level encoding with limited training samples, thus further reducing the effectiveness of the whole model.
% %
% Some works enhance migration operations between the low-level feature and upsampled high-level feature, such as FaPN, however, they are still shares the same issues with the regular pyramid-based decoder (e.g. SemanticFPN), limited its effectiveness.

After identifying this root cause, our key insight is to \emph{protect} the good (high-level) backbone features and use its \textit{\textbf{h}}igh-level \textit{\textbf{f}}eatures as \textit{\textbf{g}}uidance (HFG) during the training of the upsampler branch.
% Specifically, we propose a simple yet effective method named High-level Feature Guided Module (HFGM) - 
Specifically, we first isolate/protect the backbone from the upsampler via proper stopping gradient operations.
Secondly, the training of the upsampler is guided/regularized by the good high-level backbone features since we force the upsampler features to mimic the backbone features.
From the perspective of distillation, the backbone branch is the teacher and the upsampler branch is the student except that the upsampler (student) uses information from the backbone (teacher) since they are parts of one single network.

To further push the upper limit of HFGD, we propose to introduce an \textit{\textbf{c}}entext \textit{\textbf{a}}ugmented \textit{\textbf{e}}ncoder (CAE) to enhance the high-level backbone feature.
% 
Experimentally, we find our CAE design can effectively operates on OS=32 feature maps, striking a good balance between efficiency and effectiveness.
We also make some modifications to SemanticFPN to better suit HFGM and (optionally) produce more detailed prediction in OS=2.
% }
% 

In summary, our contributions include:
\begin{itemize}
    \item We firstly identify the issues that limit the accuracy of existing pyramid-based upsamplers (e.g. SemanticFPN).
    %
    \item  %To address the issue, 
    We propose to use \textit{\textbf{h}}igh-level \textit{\textbf{f}}eatures as \textit{\textbf{g}}uidance (HFG) for the upsampler learning and protect the backbone features from being contaminated as previous methods.
    %
    \item To further push the upper limit, we propose \textit{\textbf{c}}entext \textit{\textbf{a}}ugmented \textit{\textbf{e}}ncoder (CAE) to enhance the high-level guidance features and make some modifications on SemanticFPN to fit the proposed HFG better .
    %
    \item 
    % 
    Our full HFG decoder 
    %
    achieves state-of-the-art accuracy on Pascal Context, COCOStuff-164k, and Cityscapes test set among methods that do not use extra training data.
\end{itemize}




\section{Related Works}
\label{sec:HFGD:related_work}

\noindent\textbf{Dilated convolutions.} 
Dilated CNNs~\shortcite{cDeepLabv1,cMSCA} usually use dilated convolutions layers starting from $OS=8$ feature maps, resulting in the constant resolution thereafter.
%
Dilation models are easy and fast to train since only a few randomly initialized weights are appended to the well-trained high-level backbone features.
However, the computation cost for the final layers increases quadratically with the length of the feature map.
%
Another issue is that dilation is only applicable for CNN backbones (e.g. VGG~\shortcite{cVGG},ResNet~\shortcite{cResnet}, MobileNet~\shortcite{cMobileNetV2} and ConvNeXt~\shortcite{cConvNeXT}) but not for Transformer-based backbones (e.g. ViT~\shortcite{cViT} and Swin~\shortcite{cSwin}).
 
\noindent\textbf{Pyramid-based upsampler.}
Pyramid-based upsampler is another popular strategy, such as UNet~\shortcite{cUNet}, SemanticFPN~\shortcite{cFPN,cPanopticSeg}, Uper~\shortcite{cUper} and FaPN~\shortcite{cFaPN}.
%
They often progressively merge features from different stages of the backbone network (i.e. a feature pyramid) into the upsampled high-level feature maps from $OS=32$, resulting in much less computation cost than using a dilated backbone.

However, the accuracy of them is usually inferior to dilation, which are implicitly mentioned in many works. 
For example, DeepLab V3+ still uses dilated backbone in the final; UperNet has a slightly worse mIOU than PSPnet even with more advanced training strategies such as Synced Batch Norm. 
One reason is that they introduce more randomly initialized weights, especially those weights used to process early-stage low-level features.
%
Its drawbacks are two-fold.
%
Firstly, the new learnable weights are trained only with limited semantic segmentation data.
%
Secondly, directly updating the early-stage backbone features through a deep-supervision/shortcut fashion with such small amount of data will probably be harmful for generalization (Fig.~\ref{fig:hfgd:concept}).

%FaPN proposed feature selection module (FSM) combines SENet \dknote{grammar} with an additional skip connection, which reduces the channel to help the net can automatically "find" and "keep" important channels of low-level features.
%However, the average pooling in SENet essentially makes the class that has more pixels (regions) in the feature map has a larger importance, thus conflict with the survival of small details. \dknote{Are these details relevant?}
%
%The additional skip connection may brings back those small details, but it also brings back the pixels that has already be enhanced in the SENet.

%More importantly, FSM cannot guarantee the encoded low-level features are robust enough without harmful the good high-level feature \dknote{grammar}.
%
%It also contains the `backbone harmful issue` mentioned previously.


\begin{figure*}[th]
\centering
\includegraphics[width=1.0\linewidth]{images/fig3.pdf}
\caption{
\textbf{High-level Feature Guided Decoder (HFGD).}
%
The core of HFGD is the high-level feature guided module that guides the upsampler learning and protects pretrained backbone features.
%
A context augmentation encoder (CAE) is included after the backbone to push the upper limit of HFG, efficiently and effectively.
Lastly, a slightly modified ultra SemanticFPN (U-SFPN) is proposed to better fit our HFGM/CAE and supports operating on even higher-resolution (OS=2).
%
Refer to Sec.~\ref{sec:method} for details.
}
\label{fig:URD:Arch}
\end{figure*}

\noindent\textbf{Context encoding modules.} 
There exist many powerful modules focusing on context encoding, such as pyramid pooling module (PPM)~\shortcite{cPSPNet}, 
atrous spatial pyramid pooling (ASPP)~\shortcite{cDeepLab,cDeepLabV3,cDeepLabV3Plus}, 
Self-Attention~\shortcite{cNonLocal,cDualAttention,cAttentionIsAllYourNeed}, etc.
%
All of them effectively enlarge the receptive field size and aggregate more context information to obtain more powerful pixel representations.
However, the computation cost of them are often huge and increases quadratically with respect to the feature map length.
To strike a good balance between efficiency and effectiveness, we choose to insert the extra encoding module after the OS=32 feature maps.
%
Under this setting, we experimentally found them less effective or harmful, possibly due to they are originally proposed for processing OS=8 feature maps.
% 
Finally, we make some modifications on a CAR-based~\shortcite{cCAR} so that it suits better to the HFG.



\section{Proposed Method}
\label{sec:method}

The key to our method is using \textit{\textbf{h}}igh-level \textit{\textbf{f}}eature as \textit{\textbf{g}}uidance (HFG) to effectively guide the upsampler training without introducing side-effect to the well trained backbone branch (Sec.~\ref{sec:method:hfgm}).
%
Since the high-level features play a crucial role, we design an context-augmented encoder (CAE) that operates on OS=32 to strike a good balance between efficiency and effectiveness (Sec.~\ref{sec:method:cae}).
%
Lastly, we make some modifications based on SemanticFPN (SFPN)~\shortcite{cPanopticFPN} (Sec.~\ref{sec:method:upsampling}) so that it better fits our HFGM and CAE.
% }

%To achieve a good balance between accuracy and efficiency, our proposed High-level Feature Guided Decoder (HFGD) adopts a computationally more efficient upsampling strategy but can better utilize the good high-level coarse features to guide the training process of the upsampling head (i.e. HFGM in Sec.~\ref{sec:method:hfgm}), 
% 
%Since the high-level coarse features play a crucial role, HFGD first improve the high-level coarse feature maps using a novel context-augmented encoder (CAE, in Sec.~\ref{sec:method:cae}).
% 
%Lastly, we make some modifications based on SemanticFPN~\cite{cPanopticFPN} and propose Ultra SemanticFPN (U-SFPN, in Sec.~\ref{sec:method:upsampling}).
%generates an accurate low-resolution feature map without using a dilated backbone to save computation. 


\subsection{High-level feature guided module (HFGM)}
\label{sec:method:hfgm}

\noindent\textbf{Motivation.} 
In our high-level feature guidance module,
we treat the high-level feature branch as the teacher (i.e. not updated by the gradient from the student) and the low-level feature branch the student.
% , resulting in our high-level feature guidance module.


The last stage high-level features (OS=32) extracted from a classification backbone are already ``good'' representations for semantic segmentation since it is essentially a per-pixel classification task.
% 
However, the high-level features are too coarse ($OS=32$) for accurate segmentation.
% 
In contrast, features from the earlier stages are in higher resolution and may contain some useful details ignored by the aforementioned high-level features.
% more detailed information, and thus suitable for compensating the lost spatial details.
% 
However, there exists a substantial gap between low-level backbone features and the final segmentation feature.
Thus, an calibration, guided by the high-level features, on the low-level features is required to better suit the final segmentation task.
% requires a calibration to the high-level feature
% 
% \dk{
% Different from previous FPN-like methods, we treat the high-level feature branch as the teacher (i.e. not updated by the gradient from the student \dknote{check}) and the low-level feature branch student, resulting in our high-level feature guidance module.
% }
% Different from previous FPN-like methods that treat the high-/low-level \dk{without obvious priority} during the fusing stage in the upsampling head
% % \hy{, or FaPN that use deformable convolution and SENet to seek the ease this issue.}
% % \hy{
% % In this work, we only care about the quality of the feature representation in the final upsampled feature instead of the detailed operation inside the upsampler.
% % }
% \hy{
% Thus, for the feature representation, we propose to only ``trust'' the high-level features}
% and use them as guidance for the upsampling head.

\noindent\textbf{Guiding the upsampler.} 
Guidance is realized by reusing the class tokens and a stop-gradient operation.
Specifically, both the backbone branch and the upsampler branch share the same set of class tokens to calculate class probabilities and the cross-entropy loss.
But the class tokens are trained to co-evolve with only the high-level features (i.e. the guiding branch). 
They are not updated by the gradients from the upsampler branch via the stop-gradient operations (i.e. the guided branch).
% Then, the upsampled feature from the upsampler branch can be guided by the `good` class token during training. 
% Because there is no gradient for the class token in the upsampler side,
% the weights in the upsampler must be adapt according to the class kernel to minimize the CE loss.
%
As a result, pixel representations from the upsampler are forced to approaching their corresponding high-level pixel representations from the backbone branch, resulting in better generalization.
%
% \hy{
% In this way, the class token are free from noisy gradient (especially in the early training epochs) from its left path and guide the learning of the upsampler branch.
% }

\noindent\textbf{Protecting pretrained backbone.} 
It is important to protect the backbone from being contaminated by the training of the upsampler as evidenced by observation 2 in Fig.~\ref{fig:hfgd:concept}.
%
We realize this goal with proper stop-gradient operations between the upsampler and backbone branches, resulting in a teacher-student~\shortcite{cBYOT} like distillation architecture (Fig.~\ref{fig:URD:Arch}).
% 
% The stop-gradient operation inside HFGM ensures the class kernels are updated only according to the right path (i.e., high-level coarse features on $OS=32$).
%
Specifically, the stop-gradient operations on $OS=n$ paths (between the upsampler branch and the backbone branch) stop updates from randomly initialized upsampler branch so that they can protect well-trained backbone features from being negatively affected.
%
% In this disentangled configuration, 
% the class tokens are trained to co-evolve with only the high-level features, \dk{resulting in better generalization.}
% (more %discriminative
% \hy{
% generalizable).
% }



Another small but very helpful operation is the inclusion of an axial attention~\shortcite{cAxialAttention,cAxialDeepLab,cCAA} layer.
%
This axial attention broadcasts the gradient of one pixel to all spatial locations, 
%
resulting in further improvement 
(see Tab.~\ref{tab:ablation_hfgm}).
%
In consideration of computation, we choose axial attention~\shortcite{cAxialAttention,cAxialDeepLab,cCAA} instead of full self-attention for spatial context augmentation since the resolution of the path is high (e.g. $OS=2$ or $4$).
% 
Note that axial-attention acts as a helper for the guidance possibly because it effectively broadcast the guidance signal back to all spatial locations.
Without guidance, its effect is limited (Tab.~\ref{tab:ablation_hfgm})



\subsection{Context-augmented encoder (CAE)}
\label{sec:method:cae}


To further push the upper limit, we propose to enhance the high-level features that serving as the guidance/teacher.
In consideration of computation overhead, we choose to operate on the low-resolution OS=32 feature maps so that we can freely use any powerful modules 
(e.g. ASPP~\shortcite{cDeepLab,cDeepLabV3,cDeepLabV3Plus}, PPM~\shortcite{cPSPNet}, OCR~\shortcite{cOCR}, and full self-attention~\shortcite{cNonLocal,cDualAttention,cAttentionIsAllYourNeed,cCFNet}).

In our experiments, we propose a CAR-based~\cite{cCAR} configuration and achieves most accuracy gain (see Tab.~\ref{tab:ablation_cae}).
%
Specifically, we use a wider linear layer (i.e. 1x1 conv with 2048 channels) to process backbone features.
Then we use a linear layer to reduce the features to 512-D, followed by a full self-attention layer regularized by class-aware regularizations (CAR\shortcite{cCAR}) and a trailing convolution layer (256-d).
% 
Using a 2048-D 1x1 conv layer (i.e. wide linear in Fig.~\ref{fig:URD:Arch}) performs much better than commonly used 3x3 conv layer possibly because the receptive field size of a pixel in OS=32 feature maps is already large enough.
Increasing its channel number also helps since it compensates for the lost capacity after changing to smaller kernels.

\subsection{Ultra SemanticFPN (U-SFPN)}
\label{sec:method:upsampling}
% \hy{
% To better fit HFGM and CAE for further accuracy improvement, 
We slightly modify SemanticFPN (SFPN) to better fit our HFGM and CAE and enable it to work on even higher-resolution (OS=2) feature maps.
% , named Ultra SemanticFPN (U-SFPN).
% }
% 
% \dknote{updated their orders}
Firstly, we change the leading $1\times1$ conv to $3\times3$ conv layers since it collects more spatial information to reduce potential ``noise`` in the low-level features.
% 
Secondly, we increase the channel numbers from 128 to 256 (applied to SemanticFPN in experiments for fairness) since we find it is beneficial for obtaining more accurate and detailed results.
% 
% \hy{Firstly, based on the guiding of HFGM}, 
Last but not least,
we can keep upsampling to an even higher resolution (i.e. $OS=2$) and achieve better accuracy with the help of HFG\footnote{We only test $OS=2$ results on Cityscapes since the ground truth annotations on other datasets (e.g. Pascal Context) are not as accurate.}.
%
To achieve this goal, we include one more ``conv-upsample'' block for every branch to get $OS=2$ feature maps.
%
Note that we did not use $OS=2$ low-level features from the backbone.
%\dk{See Fig.xx in our supplementary.}
% since some recent backbones (e.g. Swin~\shortcite{cSwin}, ConvNeXt~\shortcite{cConvNeXT}) do not have an $OS=2$ feature map (directly downsampled to $OS=4$).
% 







% \section{Implementation Details}
% % \label{sec:implementation}
% \label{sec:HFGD:training_settings}

\subsection{Training details}
\label{sec:HFGD:training_settings}

For fair comparison, we use the same training settings (listed below) for all the ablation studies and the experiments to compare with the other state-of-the-art methods unless specified.
%
For the ablation studies, we followed CCNet~\shortcite{cCCNet} (e.g. SGD optimizer, learning rate), which provides a simple and ``clean'' setting to study the effectiveness of a method.
% , we applied training settings as follows:
\begin{table}[h]
\centering
\small
\resizebox{\linewidth}{!}{%
\begin{tabular}{l|c|c} 
    \toprule
    Settings    & Ablation studies &  SOTA experiments \\
    \midrule
    \midrule
    Batch size & 16 & 16  \\
    Optimizer & SGD & AdamW \\
    Learning rate decay & \textit{poly}  & \textit{poly} \\
    Initial Learning rate & 0.01 & 0.00004 \\
    Weight decay & 0.0001  & 0.05 \\
    Photo Metric Distortion & - & \checkmark \\
    Sync Batch Norm & \checkmark & \checkmark \\
    \bottomrule
\end{tabular}
}
\label{tab:urd:ablation_training_settings}
\end{table}

% In addition, all the experiments in ablation studies, including baselines, are conducted or reproduced under deterministic technology with the same training settings, which brings complete fair comparisons to demonstrate the effectiveness of our HFGD.

\section{Experiments on Pascal Context Dataset}
The Pascal Context~\shortcite{cPascalContext} dataset contains 4,998 training images and 5,105 testing images.
%
Following the common practice, we use its 59 semantic classes to conduct the ablation studies and experiments.
%
Unless specified, we train the models on the training set for 30K iterations for ResNet backbone and 40K for Swin-Large and ConvNeXt-Large.

%%%%%%%% Problem identification %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\input{tables/ablation_studies_cae}

\subsection{Problem identification}
\label{sec:problem}

\noindent\textbf{SemanticFPN is generally worse than dilation.}
We conduct experiments in Tab.~\ref{tab:ablation_cae} to demonstrate the issue we identify (observation 1 in Fig.~\ref{fig:hfgd:concept}).
% 
Many famous methods for context encoding (``Extra Encoding'') produces less accurate predictions when combined with an upsampler net (i.e. SemanticFPN) than directly using a dilated backbone, even if the upsampler produces higher-resolution feature maps.
%
Refer to our supp. for detailed settings.

\noindent\textbf{Negative influence from the upsampler on the backbone.}
We find the joint fine-tuning of the upsampler and the backbone results in deteriorated backbone features.
%
To demonstrate it, we modify SemanticFPN by introducing an auxiliary FCN to predict the mask from the high-level features produced by the backbone (see Issues in Fig.~\ref{fig:hfgd:concept}).
The predictions from the auxiliary FCN branch become worse than the original FCN (44.35\% vs 45.87\% mIOU).
Another variant stops the auxiliary FCN's gradient from propagating back to the backbone, which produces even worse results (40.04\%).

\noindent\textbf{Advantage of pre-training.}
% We conducted a small experiment to present the substantial performance gap between pre-trained backbones and trained from scratch.
% As presented in Tab.~\ref{tab:ablation_imagenet_simple}, there is still a huge performance gap even using 6 times training iterations.
We conduct experiments to show the benefit of using pretrained backbone in Tab.~\ref{tab:ablation_imagenet_simple}.
Due to limited training data, there still exists a substantial gap after 6 times training iterations.
Thus, protecting the backbone to ensure its generalization is necessary and critical.
More experiments about this are presented in the supplementary.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Ablation stuides on HFGD}

\noindent\textbf{Ablation studies on HFGM}
%
In Tab.~\ref{tab:ablation_hfgm} (first half), we evaluate the effectiveness of our proposed HFGM based on SemanticFPN (i.e. no ``Extra Encoding'').
%
Although using only HFG (i.e. w/o AA) or only AA are both helpful, using the full HFGM brings the most gain (1.80\% mIOU).

Introducing an axial attention can efficiently broadcast the effect of HFG to all spatial locations (also see Sec.~\ref{sec:method:hfgm} for more discussion).

We also analyze what happens if we no longer protect the backbone branch (letting gradients propagate from the upsampler branch).
We found that doing so resulted in a drop in mIOU down to 48.50\%.


\noindent\textbf{Ablation studies on CAE.}
%
We conduct ablation studies on CAE in Tab.~\ref{tab:ablation_cae} (bottom).
% since we use the high-level coarse features to guide the training of the upsampling head.
Our CAE design surpasses the other alternatives (``Extra Encodings'') by a substantial margin, with and without SemanticFPN~\shortcite{cPanopticFPN}.
Note that CAE is the best for methods using an upsampler while SA (CAR) is the best for dilation methods, showing that previous context encodings are not very suitable for OS=32 feature maps and our modification on SA (CAR) effectively improves it.


\input{tables/ablation_imagenet_simple}
\input{tables/ablation_studies_hfgm}

\noindent\textbf{Ablation studies on U-SFPN.}
%
In Tab.~\ref{tab:ablation_upsampling}, we 
% After we evaluated the effectiveness of our proposed CAE and HFGM, we continued to 
conduct ablation studies on U-SFPN to verify the effectiveness of our modification on SFPN while fixing CAE and HFGM.
% Table 3 shows that 
Replacing SFPN with U-SFPN improves the mIOU of ResNet-50 (CAE + HFGM) by 0.48\%, reaching 50.76\%.
The improvement is even larger (1.14\%) when using Swin-Large as the backbone 
%(Tab.~\ref{tab:ablation_upsampling})
.

\input{tables/ablation_studies_upsampling}



\noindent\textbf{Module-level ablation studies on HFGD.}
%
In Tab.~\ref{tab:ablation_hfgd},
% Finally, 
we conduct ablation studies on each module of HFGD using previously found best configurations in Tab.~\ref{tab:ablation_cae}-\ref{tab:ablation_upsampling}.
%
Using all modules together leads to significant accuracy improvement, indicating the effectiveness of the overall architecture.


\input{tables/ablation_studies_final}

\subsection{Computational cost of HFGD}
%
The computational cost of our HFGD and two other state-of-the-art methods are listed in Tab.~\ref{tab:HFGD:flops}.
% 
HFGD uses much lower GFLOPs than a similar dilation model (Self-Attention + CAR~\cite{cCAR}) but achieves better mIOU (50.76\% vs 50.50\%~\cite{cCAR}).
% 
When compared with SemanticFPN, HFGD ($OS=4$) achieves 3.62\% mIOU gain with an affordable extra computation cost (71.63 GFLOPs vs 45.65 GFLOPs).

\input{tables/computational_cost}

\subsection{Comparison with the state-of-the-art methods}

To compare with the state-of-the-art, we adopt ConvNeXt-L as the backbone for our HFGD.
% 
We set the training iterations to 40K while
all the other training settings are the same as stated in Sec.~\ref{sec:HFGD:training_settings}.
% except that we set the training iterations to 40K.
% 
As shown in Tab.~\ref{tab:urd:SOTA-PascalContext}, our HFGD achieved 63.8\% mIOU with single-scale without flipping and 64.9\% mIOU with multi-scales with flipping, outperforming previous state-of-the-art by 1\% mIOU in ECCV-2022.
 %
HFGD is now the new state-of-art method on Pascal Context for the methods that only use the ImageNet pre-trained backbone without extra techniques~\cite{cAugReg}. 


%\noindent\textbf{Visualizations on Pascal Context.}
%We present visual comparisons among Self-Attention + CAR (dilation, $OS=8$), SemanticFPN, and our HFGD.
% 
%As shown in Fig.~\ref{fig:CFGD:PascalContext-R50-Vis}, HFGD achieves better accuracy than SemanticFPN and contains more details than dilated model (e.g. the last row, the legs of the person).

\begin{figure*}[th]
\centering
\includegraphics[width=1.0\linewidth]{images/vis_main.pdf}
\caption{
Visual comparisons 
% (Upper-left:Pascal Context, upper-right:COCOStuff, bottom:Cityscapes) 
between SemanticFPN ($OS=4$), HFGD ($OS=4$), and HFGD ($OS=2$).
Zoom in to see better.
The results are obtained using single-scale without flipping.
%
More visualizations are presented in the supplementary.
}
\label{fig:CFGD:main-R50-Vis}
\end{figure*}



\section{Experiments on COCOStuff164k Dataset}

\iffalse
\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\linewidth]{images/vis_pascalcontext_r50_vs.pdf}
\caption{
Visual comparisons on Pascal Context between SemanticFPN, Self-Attention + CAR (denoted as ``Dilated OS=8''), and our HFGD.
All models used ResNet-50 as the backbone.
The results are obtained using single-scale without flipping.
}
\label{fig:CFGD:PascalContext-R50-Vis}
\end{figure}
\fi



COCOStuff-164k~\shortcite{cCocoStuff}, which becomes popular in recent years, poses a great challenge for semantic segmentation models due to its high diversity (118k training images and 5000 testing images) and complexity (171 classes). 
% 
We adopt ConvNeXt-Large as our backbone network and follow the training settings described in Sec.~\ref{sec:HFGD:training_settings} and we train our model for 40K iterations.
In Tab.~\ref{tab:HFGD:SOTA-COCOStuff164k}, we compare our proposed HFGD with other state-of-the-art methods.
HFGD outperforms the previous state-of-the-art~\shortcite{cSegNeXt} by a large margin (49.4\% vs 47.3\% mIOU).


\input{tables/pascalcontext_sota_tab}
\input{tables/cocostuff164k_sota_tab}


\section{Experiments on Cityscapes Dataset}

Cityscapes~\shortcite{cCityScapes} is a semantic segmentation dataset that consists of high-resolution images of road scenes with accurate annotations.
It has 19 labeled classes and contains 2975/500/1525 training/validation/test images.

\subsection{Ablation studies on feature map resolution}
\label{sec:exps:ablation_os2}
\input{tables/ablation_studies_ct_os2}
%
We mainly conduct ablation experiments on Cityscapes to verify the superiority of using ultra high resolution feature maps (i.e. $OS=2$) since its GT annotations are the most accurate.
% 
We use ResNet-50 as the backbone and train SemanticFPN, HFGD ($OS=4$), and HFGD ($OS=2$) for 30K iterations following the training settings in Sec.~\ref{sec:HFGD:training_settings}. 
%
As shown in Tab.~\ref{tab:ablation_ct_os2}, the $OS=2$ has further improved the accuracy over $OS=4$ by 0.7\% mIOU (79.11 vs 79.81).

\subsection{Comparison with the state-of-the-art methods}
To compare our method against the state-of-the-art, we use ConvNeXt-Large and follow the training settings described in Sec.~\ref{sec:HFGD:training_settings}.
%
Note that we only compare with methods that are trained only on the Cityscapes fine annotations, similar to many works~\cite{cSegFormer,cKMaXDeepLab}.

We set the crop size to 513$\times$1025~\cite{cDeepLabV3,cPanopticDeepLab} and train our HFGD model for 60K iterations for the $OS=4$ version and 80K iterations for the $OS=2$ version.
%
As shown in Tab.~\ref{tab:HFGD:SOTA-Cityscapes}, the proposed HFGD outperforms the previous state-of-the-art proposed in recent years on Cityscapes validation set.

A similar comparison is conducted on Cityscapes test set, where we set batch size = 32 following kMaXDeepLab~\shortcite{cKMaXDeepLab}
while the other settings remaining unchanged.
% , and other settings remained the same as we described previously.
% 
% \hy{
Results on the test set can fairly demonstrate the effectiveness of the proposed method since no ground-truth are provided.
% }
%
Note that we did not use hard sample mining.
%
As shown in Tab.~\ref{tab:HFGD:test-Cityscapes}, our HFGD sets new state-of-the-art on Cityscapes test set (when only using fine set for training).
%

\input{tables/cityscapes_sota_tab}
\input{tables/cityscapes_test_tab}

\section{Visualizations}
% \hy{
We present visual comparisons between SemanticFPN and our HFGD on Pascal Context, CocoStuff and Cityscapes in Fig~\ref{fig:CFGD:main-R50-Vis}.
%
HFGD is clearly more capable of segmenting small/thin objects. 
For example, HFGD even performs better than manual annotation when segmenting very small birds in the sky.
When using OS=2, HFGD can segment the remote traffic light in Cityscapes, which is partially missing in OS=4.
% and might be dangerous for automatic vehicles.
%
% Due to the page limit, 
More visualizations are presented in the supplementary.
% }

%\subsection{Visualizations on Cityscapes}

%We present visual comparisons between SemanticFPN ($OS=4$) and our HFGD ($OS=4$ and $OS=2$) in Fig.~\ref{fig:CFGD:Cityscapes-R50-Vis}. 
%
%HFGD ($OS=2$) is clearly more capable of segmenting the small/thin objects. For example, the traffic light in Fig.~\ref{fig:CFGD:Cityscapes-R50-Vis} (last row) is partially missing in OS=4, which might be dangerous for automatic vehicles.

% \input{sections/limitations}



\section{Conclusion }
In this paper, we propose to use the high-level features as the teacher to guide the training of the upsampler branch (student), resulting in an effective and efficient decoder framework.
%
Specifically, the core of our method is using high-level features as guidance (HFG) and proper stop-gradient operations for the upsampler learning, which effectively addresses the observed issues in Fig.~\ref{fig:hfgd:concept}.
% as the core solution to address observed issues in Fig.1, 
In addition, we explore a context-augmented encoder (CAE) to effectively enhance the OS=32 high-level features
and propose a modified version of SemanticFPN to better fit our HFGM.
% and a minor modification of SemanticFPN, U-SFPN to fit HFGM and CAE better.
% 
With thorough experiments, HFGD achieves largely improved performance over previous upsampling-based state-of-the-art methods that does not use extra training data, on Pascal Context, COCOStuff, and Cityscapes.
% without using extra training data, 
HFGD also achieves slightly better results than dilation-based CNNs but using much less computation cost (e.g. Self-Attention + CAR~\cite{cCAR}).
% than dilated CNNs (e.g. Self-Attention + CAR~\cite{cCAR}).
%
We refer the readers to our supplementary material for more ablation studies, experiments, implementation details, and visualizations.

\clearpage
\clearpage

\bibliography{aaai24}

\end{document}
