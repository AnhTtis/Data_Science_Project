

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\linewidth]{images/vis_cityscapes_intro_vs.pdf}
\caption{
\textbf{\hy{
Thanks to guidance, the upsampler can produce a more detailed feature map (OS=2) without compromising on class representation. }
ResNet-50 is used as the backbone. }
% The results are obtained using single-scale without flipping.
% Examples of the results predicted from the Cityscapes dataset show the advantage of $OS=2$.
% All models use ResNet-50 as the backbone and predicted within the single-scale without flipping.
}
\label{fig:CFGD:Cityscapes-R50-IntroVis}
\end{figure*}

%Semantic Segmentation is a fundamental task in computer vision, requires densely predicted class labels for every pixel of the input image.
%
%Rapid progresses~\shortcite{cFCN,cUNet,cPSPNet,cDeepLab,cDeepLabV3,cDenseASPP,cUper,cENCNet,cDeepLabV3Plus,cCFNet,cDualAttention,cANNN,cEMANet,cCCNet,cKSAC,cSegFormer,cCAA,cCAR} have been witnessed since the existence of fully convolutional networks (FCNs)~\cite{cFCN}, which makes dense prediction tasks much more efficient than before. 

Due to the lack of training samples in semantic segmentation
%
\footnote{One of the largest semantic segmentation data is COCOStuff~\cite{cCocoStuff} that contains 118K training images. There are only 4.9K training images in Pascal Context~\cite{cPascalContext} and 2.9K training images in Cityscapes~\cite{cCityScapes}.)}
,
existing approaches usually relied on the image-classification backbone to encode (feature extraction) the RGB pixels from the input image to a high-level feature map that contains rich semantic information.
%
Commonly used image-classification backbones, such as ResNet, are data-driven and pretrained on large-scale image classification datasets (e.g. 1.2M image-level labeled training images in ImageNet-1K) to obtain good generalization from the training sample diversity. 
%
However, because image-classification only needs to output a single, image-level classification result, those backbones usually perform downsampling multiple times and produce a final feature map in an output stride of 32 (OS=32).
%
For the semantic segmentation, which is a per-pixel classification task, an OS=32 feature map is too coarse for accurate and detailed (e.g. object boundaries, thin/small objects).

Dilation is a direct solution that replaces some downsampling operations (e.g. strides) inside the backbone with the dilated convolution. 
%
Dilation does not import any new layers or operations with randomly initialized weights, so it can fully enjoy the benefit of pre-trained weights, resulting in fast convergence and robust pixel representation (good accuracy).
However, dilation has two major limitations: 1) It only can be applied on CNN based backbone. 2) It brings too much computational cost because the downsampling operations in the backbone are replaced.

On the other hand, the pyramid-based decoder (e.g. SemanticFPN) provides a computationally efficient solution to recover the high-resolution feature map without incurring too much computational cost. However, it is usually less effective than dilation approaches because it introduces many new layers or operations for handling the migration between the low-level and upsampled high-level features.
%
Those new operations and layers have randomly initialized weights, so they are not generalized enough (through limited training samples) to handle low-level features from the backbone. Since low-level features are not yet fully encoded, they pollute the robust representation from the high-level feature when adding the details.
%
Moreover, they can also harm the backbone through back-propagation, particularly in the early stages of the backbone. Those early stages layers were originally only responsible for low-level feature encoding (e.g. edge detection) during pretraining. Now, they also need to be impacted by the high-level encoding with limited training samples, thus further reducing the effectiveness of the whole model.
%
Some works enhance migration operations between the low-level feature and upsampled high-level feature, such as FaPN, however, they are still shares the same issues with the regular pyramid-based decoder (e.g. SemanticFPN), limited its effectiveness.

To address issues mentioned above, in this work, we proposed High-level Feature Guided Module (HFGM), a simple yet effective method that firstly decouples encoder branch from the upsampler to avoid negative impact mentioned above. 
After that, the HFGM directly controls the final quality of upsampled feature map, uses class token trained by the high-level feature of encoder branch to ``guide`` (regularize) the upsampler branch, and forces the upsampled feature map to have similar discrimination as high level feature.

Moreover, because an as possible accurate high-level feature not only brings a robustness class token for guidance but also gives the upsampler a good input for a good upsampled output, we decided to add a context augmentation module inside the encoder branch.

However, after conducting many experiments, we found that most existing context augmentation methods for high-level feature enhancement, such as self-attention and OCR, perform well in the dilation model (OS=8) but usually lack effectiveness in OS=32. 
%
Thus, we also explored an OS=32-friendly context augmentation module named `Context Augmentation Encoder` (CAE) in this work. 
%
CAE collaborated very well with HFGM.

Furthermore, we also modified SemanticFPN slightly based on the regularization from the HFGM, so that it can adapt better to HFGM and optionally produce more detailed results in OS=2 mode with high quality (thanks to guidance).

In summary, our contributions in this work include:
\begin{itemize}
    \item We recognize the issues of existing pyramid-based upsampler (e.g. SemanticFPN) that are harmful to the encoder branch (e.g. backbone) and result in limited representation correctness of upsampled feature.
    \item  To address the issue, we proposed HFGM, which decouples the backbone and encoder branch from the upsampler branch. This prevents them from being impacted by the upsampler branch during training and regularizes the upsampled feature to have similar discrimination as the high-level feature.
    \item To further enhance upsampler effectiveness, we explored CAE that can effectively enhance the OS=32 high-level feature for the upsampler (and guidance).
    \item We also slightly mortified SeamnticFPN to fit HFGM better. Our full solution High-Level Feature Guided Decoder achieves state-of-the-art accuracy on Pascal Context, COCOStuff-164k, and Cityscapes test set without using extra training samples or extra techniques.
\end{itemize}














