\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{images/urd_intro_v3.pdf}
\caption{
Comparison between our method (d) and existing methods (a/b/c).
Our method proposes to take full use of the better high-level pretrained features (i.e. CAE) to guide (i.e. HFGM) the upsampling process (U-SFPN), achieving better accuracy while using much less computation.
% 
\textcolor{glaucous}{Blue arrows} in the figure represent ``good'' high-level features that are more ready for final classification while
\textcolor{orange}{orange arrows} represent low-level features that exist a substantial gap to the task and thus possibly being ``noisy''. 
}
\label{fig:HFGD:Intro}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\linewidth]{images/vis_cityscapes_intro_vs.pdf}
\caption{
Demonstration of the impact of producing higher-resolution feature maps on Cityscapes. 
All the results are predicted using single-scale without flipping protocol.
ResNet-50 is used as the backbone. 
% The results are obtained using single-scale without flipping.
% Examples of the results predicted from the Cityscapes dataset show the advantage of $OS=2$.
% All models use ResNet-50 as the backbone and predicted within the single-scale without flipping.
}
\label{fig:CFGD:Cityscapes-R50-IntroVis}
\end{figure*}

Semantic Segmentation is a fundamental task in computer vision, requires densely predicted class labels for every pixel of the input image.
%
Rapid progresses~\cite{cFCN,cUNet,cPSPNet,cDeepLab,cDeepLabV3,cDenseASPP,cUper,cENCNet,cDeepLabV3Plus,cCFNet,cDualAttention,cANNN,cEMANet,cCCNet,cKSAC,cSegFormer,cCAA,cCAR} have been witnessed since the existence of fully convolutional networks (FCNs)~\cite{cFCN}, which makes dense prediction tasks much more efficient than before. 
%
Most methods consist of an \emph{encoder} used to extract coarse (i.e. low-resolution) high-level abstract features and a \emph{decoder} used to compensate lost spatial details from high-resolution low-level features. %\dknote{check}

Most commonly, the backbone encoder is pretrained on large-scale image classification dataset (e.g. 1.2M labeled training images in ImageNet-1k) since semantic segmentation is essentially a per-pixel classification task.
% 
%
However, the classification backbones usually produce feature maps in an output stride of 32 ($OS=32$), which is too coarse for accurate and detailed (e.g. object boundaries, thin/small objects).
%
So investigating stronger and more efficient upsampler has been one major direction % for semantic segmentation 
in recent years.


Existing approaches can be roughly classified into two categories, including dilation-based methods that stop further reducing feature map resolution after several stages (e.g. $OS=8$) and pyramid-based methods that use learned upsampler to gradually fuse multi-resolution feature maps from $OS=32$ to $OS=4$. 

Dilated CNNs~\cite{cDeepLabV3,cNonLocal,cANNN,cCCNet,cOCNet,cKSAC,cCAA} better utilize the ``good'' pretrained backbone features and usually result in faster convergence and better accuracy, but at the cost of huge computation overhead.
% 
pyramid-based methods are more efficient but not as effective as their dilated counterparts~\cite{cPanopticFPN,cFPN,cUNet} even with $OS=4$ feature maps.
%
We conclude that, during the training of the new upsampling head, the ``good'' high-level backbones features are negatively affected by the ``noisy'' low-level features (since there exists a big gap between the low-level features and the final classification features) and the randomly initialized weights in the head.

Thus, we propose to make better use of the ``good'' high-level backbone features by 
1) protecting them from the updating of the upsampling header and 
2) further using them to guide the learning of the upsampling head.
%
Specifically, the \emph{protection} is realized by stopping gradients from back-propagating through the upsampling header to the backbone and the \emph{guidance} is realized by reusing the class kernels (i.e. the final classification layer) learned along with the coarse high-level features in the upsampling head (See. Fig.~\ref{fig:HFGD:Intro}d).
% 
Since the coarse high-level features play a crucial role in our framework, We further explored different extra encoding modules to improve them, resulting in our context-augmented encoder (CAE).
%
Note that we can use strong encoding modules (such as ASPP, full self-attention) without introducing too much computational overhead since the resolution of the high-level feature maps is low.
% 
Lastly, we propose an ultra SemanticFPN (U-SFPN) based on the widely-used SemanticFPN (SFPN) to obtain better representation as our upsampling head.
%
Due to the efficiency and effectiveness of our framework, we can upsample the feature maps to an unseen size of $OS=2$ and still achieve accuracy gain see Fig.~\ref{fig:CFGD:Cityscapes-R50-IntroVis}, Fig.~\ref{fig:CFGD:Cityscapes-R50-Vis} and Tab.~\ref{tab:ablation_ct_os2}).

\noindent In summary, our contributions in this work include:

\begin{itemize}
    \item Our proposed HFGD is a novel guided upsampling paradigm for FPN like upsampler and can effectively and efficiently use good high-level coarse features to guide the learning of the upsampling head.
    %
    \item We propose a powerful context-augmented encoder based on CAR~\cite{cCAR} for the coarse ($OS=32$) guiding features, resulting in better accuracy with small computation overhead.
    \item We propose U-SFPN %based on SemanticFPN 
    to obtain better and higher-resolution feature maps of $OS=2$.
    \item HFGD achieves state-of-the-art accuracy (e.g. 64.9\% mIOU on Pascal Context) while being efficient among methods with backbones that only use ImageNet data for pretraining and without use more technique~\cite{cAugReg}.
\end{itemize}


