\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}

\usepackage{booktabs}
\usepackage{paralist}

%\usepackage{emoji}
\usepackage[normalem]{ulem}
\usepackage[x11names]{xcolor}

%%%%%%%!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
\iccvfinalcopy % *** Uncomment this line for the final submission
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Include other packages here, before hyperref.
\usepackage{multirow}

\usepackage{pifont}  % http://ctan.org/pkg/pifont


% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\ificcvfinal\usepackage[breaklinks=true,bookmarks=false]
\ificcvfinal\usepackage[breaklinks=true,bookmarks=false,colorlinks,pagebackref=True]{hyperref}\else\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}\fi

\def\iccvPaperID{1358} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\definecolor{glaucous}{rgb}{0.38, 0.51, 0.71}
\definecolor{stop_grad_red}{rgb}{0.75, 0, 0}

\newcommand{\dk}[1]{\textcolor{magenta}{#1}}
\newcommand{\dknote}[1]{\textcolor{orange}{[KD:\@ #1]}}
\newcommand{\hy}[1]{\textcolor{blue}{#1}}
\newcommand{\hynote}[1]{\textcolor{red}{[HY:\@ #1]}}
\newcommand{\todo}[1]{\textbf{\textcolor{red}{[TODO:\@ #1]}}}
\newcommand{\TODO}[1]{\textbf{\textcolor{red}{[TODO:\@ #1]}}}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{54}}%

%\setlength{\textfloatsep}{12pt}
%\setlength{\dbltextfloatsep}{12pt}  % two-column mode
%\setlength{\floatsep}{12pt}
%\setlength{\dblfloatsep}{12pt}  % two-column mode
%\setlength{\intextsep}{12pt}
\setlength{\abovecaptionskip}{2pt}
\setlength{\belowcaptionskip}{2pt}

\begin{document}

%%%%%%%%% TITLE
\title{\emph{HFGD}: High-level Feature Guided Decoder for Semantic Segmentation}

\author{Ye Huang\textsuperscript{\rm 1}\quad
        Di Kang \textsuperscript{\rm 2}\quad
        Shenghua Gao \textsuperscript{\rm 3}\quad
        Wen Li \textsuperscript{\rm 1}\quad
        Lixin Duan\textsuperscript{\rm 1}\thanks{Corresponding author} \\
\textsuperscript{\rm 1} University of Electronic Science and Technology of China\\
\textsuperscript{\rm 2} Tencent AI LAB\\
\textsuperscript{\rm 3} ShanghaiTech University\\
{\tt\small edward.ye.huang@qq.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}
%\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

\maketitle
%%%%%%%%% ABSTRACT
\begin{abstract}
\input{abstract.tex}
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:hfgd:intro}

\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{images/urd_intro_v3.pdf}
\caption{
Comparison between our method (d) and existing methods (a/b/c).
Our method proposes to take full use of the better high-level pretrained features (i.e. CAE) to guide (i.e. HFGM) the upsampling process (U-SFPN), achieving better accuracy while using much less computation.
% 
\textcolor{glaucous}{Blue arrows} in the figure represent ``good'' high-level features that are more ready for final classification while
\textcolor{orange}{orange arrows} represent low-level features that exist a substantial gap to the task and thus possibly being ``noisy''. 
}
\label{fig:HFGD:Intro}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\linewidth]{images/vis_cityscapes_intro_vs.pdf}
\caption{
Demonstration of the impact of producing higher-resolution feature maps on Cityscapes. 
All the results are predicted using single-scale without flipping protocol.
ResNet-50 is used as the backbone. 
% The results are obtained using single-scale without flipping.
% Examples of the results predicted from the Cityscapes dataset show the advantage of $OS=2$.
% All models use ResNet-50 as the backbone and predicted within the single-scale without flipping.
}
\label{fig:CFGD:Cityscapes-R50-IntroVis}
\end{figure*}

Semantic Segmentation is a fundamental task in computer vision, requires densely predicted class labels for every pixel of the input image.
%
Rapid progresses~\cite{cFCN,cUNet,cPSPNet,cDeepLab,cDeepLabV3,cDenseASPP,cUper,cENCNet,cDeepLabV3Plus,cCFNet,cDualAttention,cANNN,cEMANet,cCCNet,cKSAC,cSegFormer,cCAA,cCAR} have been witnessed since the existence of fully convolutional networks (FCNs)~\cite{cFCN}, which makes dense prediction tasks much more efficient than before. 
%
Most methods consist of an \emph{encoder} used to extract coarse (i.e. low-resolution) high-level abstract features and a \emph{decoder} used to compensate lost spatial details from high-resolution low-level features. %\dknote{check}

Most commonly, the backbone encoder is pretrained on large-scale image classification dataset (e.g. 1.2M labeled training images in ImageNet-1k) since semantic segmentation is essentially a per-pixel classification task.
% 
%
However, the classification backbones usually produce feature maps in an output stride of 32 ($OS=32$), which is too coarse for accurate and detailed (e.g. object boundaries, thin/small objects).
%
So investigating stronger and more efficient upsampler has been one major direction % for semantic segmentation 
in recent years.


Existing approaches can be roughly classified into two categories, including dilation-based methods that stop further reducing feature map resolution after several stages (e.g. $OS=8$) and pyramid-based methods that use learned upsampler to gradually fuse multi-resolution feature maps from $OS=32$ to $OS=4$. 
% 
Dilated CNNs~\cite{cDeepLabV3,cNonLocal,cANNN,cCCNet,cOCNet,cKSAC,cCAA} better utilize the ``good'' pretrained backbone features and usually result in faster convergence and better accuracy, but at the cost of huge computation overhead.
% 
pyramid-based methods are more efficient but not as effective as their dilated counterparts~\cite{cPanopticFPN,cFPN,cUNet} even with $OS=4$ feature maps.
% 
We conclude that, during the training of the new upsampling head, the ``good'' high-level backbones features are negatively affected by the ``noisy'' low-level features (since there exists a big gap between the low-level features and the final classification features) and the randomly initialized weights in the head.


Thus, we propose to make better use of the ``good'' high-level backbone features by 
1) protecting them from the updating of the upsampling header and 
2) further using them to guide the learning of the upsampling head.
%
Specifically, the \emph{protection} is realized by stopping gradients from back-propagating through the upsampling header to the backbone and the \emph{guidance} is realized by reusing the class kernels (i.e. the final classification layer) learned along with the coarse high-level features in the upsampling head (See. Fig.~\ref{fig:HFGD:Intro}d).
% 
Since the coarse high-level features play a crucial role in our framework, We further explored different extra encoding modules to improve them, resulting in our context-augmented encoder (CAE).
%
Note that we can use strong encoding modules (such as ASPP, full self-attention) without introducing too much computational overhead since the resolution of the high-level feature maps is low.
% 
Lastly, we propose an ultra SemanticFPN (U-SFPN) based on the widely-used SemanticFPN (SFPN) to obtain better representation as our upsampling head.
%
Due to the efficiency and effectiveness of our framework, we can upsample the feature maps to an unseen size of $OS=2$ and still achieve accuracy gain see Fig.~\ref{fig:CFGD:Cityscapes-R50-IntroVis}, Fig.~\ref{fig:CFGD:Cityscapes-R50-Vis} and Tab.~\ref{tab:ablation_ct_os2}).

\noindent In summary, our contributions in this work include:

\begin{compactenum}
\item Our proposed HFGD is a novel guided upsampling paradigm for FPN like upsampler and can effectively and efficiently use good high-level coarse features to guide the learning of the upsampling head.
%
\item We propose a powerful context-augmented encoder based on CAR~\cite{cCAR} for the coarse ($OS=32$) guiding features, resulting in better accuracy with small computation overhead.
\item We propose U-SFPN %based on SemanticFPN 
to obtain better and higher-resolution feature maps of $OS=2$.
\item HFGD achieves state-of-the-art accuracy (e.g. 64.9\% mIOU on Pascal Context) while being efficient among methods with backbones that only use ImageNet data for pretraining and without use more technique~\cite{cAugReg}.
\end{compactenum}




\section{Related works}
\label{sec:HFGD:related_work}
\noindent\textbf{ImageNet pretrained backbones.}
% \dk{
Due to limited training data\footnote{So far the largest semantic segmentation data is COCOStuff~\cite{cCocoStuff} that contains 118K training images. There are only 4.9K training images in Pascal Context~\cite{cPascalContext} and 2.9K training images in Cityscapes~\cite{cCityScapes}.)}, semantic segmentation methods usually use backbone networks pretrained on large-scale image classification datasets (e.g. 1.2M images in ImageNet-1K, 14M in ImageNet-21K).
%
If not using pretrained backbones (i.e. trained from scratch), 
there still exists a substantial performance gap even with much more training epochs.
For example, we can only get 37\% mIOU (vs 47\% using pretrained weights) if we train a classic FCN with ResNet-50 backbone from scratch after 4 times number of training iterations on Pascal Context.
%
Using a backbone pretrained on image classification task is particularly helpful for semantic segmentation because these two tasks are closely related and the scale of the classification dataset is at least one order of magnitude larger than that of the semantic segmentation dataset.
%
However, most of the popular backbones finally produce feature maps with an output stride (OS) size of 32, including VGG~\cite{cVGG}, ResNet~\cite{cResnet}, EfficientNet~\cite{cEfficientNet}, Swin~\cite{cSwin}, etc.
% 
Thus, dilated convolutions or an upsampling head is required to recover the lost spatial information for the $OS=32$ feature maps.

\vspace{1mm}
\noindent\textbf{Dilated convolutions.} 
Dilated CNNs~\cite{cDeepLabv1,cMSCA} usually use dilated convolutions layers starting from $OS=8$ feature maps, resulting in the constant resolution thereafter (see Fig.~\ref{fig:HFGD:Intro}a).
%
Since the resolution is not very coarse, only some extra feature encoding layers are introduced to obtain context-augmented features (but still operated in $OS=8$) before the final logit regression.
% 
Dilation models are easy and fast to train since only a few randomly initialized weights are appended to the well-trained high-level backbone features.
However, the computation cost for the final layers increases quadratically with the feature map width.
%
Another issue is that dilation is only applicable for CNN backbones~\cite{cVGG,cResnet,cXception,cMobileNetV2,cResnest,cHRNet,cConvNeXT} but not for Transformer-based backbones~\cite{cViT,cSwin}.

\vspace{1mm}
\noindent\textbf{Pyramid-based upsampling head.}
Another popular strategy adopts upsampling head, such as UNet~\cite{cUNet}, FPN~\cite{cFPN}, SemanticFPN~\cite{cPanopticSeg}, Uper~\cite{cUper} and JPU~\cite{cFastFCN} (see Fig.~\ref{fig:HFGD:Intro}b\&c).
%
They upsample the $OS=32$ feature map to the $OS=4$ feature map without using dilated convolutions layers, resulting in much less computation cost\footnote{Lightweight decoders intentionally avoid convolution layers on features maps that are in high-resolution and contains many channels simultaneously.}.
%
To better restore the lost spatial details in the $OS=32$ high-level feature maps, 
upsampling heads often progressively merge features from different stages of the backbone network (i.e. a feature pyramid) into the upsampled high-level feature maps.

However, the accuracy of these upsampling methods are usually not as good as dilated CNNs since they introduce more randomly initialized weights, especially those weights used to process early-stage low-level features mainly due to two reasons.
%
Firstly, the new learnable weights are trained only with semantic segmentation data that are at least an order of magnitude smaller than ImageNet.
%
Secondly, updating early-stage backbone features (low-level features) with such small amount of data could easily lead to negative effects (e.g. bad generalization).

\noindent\textbf{Extra context encoding modules.} 
Many powerful feature enhancement techniques have been proposed, 
including pyramid pooling module (PPM)~\cite{cPSPNet}, 
atrous spatial pyramid pooling (ASPP)~\cite{cDeepLab}, 
Self-Attention~\cite{cNonLocal,cDualAttention}, etc.
Most of these modules fuse pixel features from different locations from the same layer (i.e. context information).

For dilated CNNs (Fig.~\ref{fig:HFGD:Intro}a), they are applied between the backbone and the output logits.
% 
For upsampling-based models (Fig.~\ref{fig:HFGD:Intro}b\&c), these extra encoding modules could be applied either before (i.e.~on low-resolution feature maps) or after the upsampler (i.e.~on high-resolution feature maps).
Their accompanied extra computation cost could be huge (e.g. full self-attention) when applied on the high-resolution feature maps.
%
%
In our method, we insert these extra encoding modules before the upsampler (see CAE in Fig.~\ref{fig:HFGD:Intro}d) because we use the high-level coarse features as guidance of the upsampler.
More accuracy gain could be obtained if the high-level coarse features are better.
In fact, we can freely use as powerful as possible extra encoding module for our CAE since it operates on low-resolution feature maps.

\section{Proposed Method}

\begin{figure*}[th]
\centering
\includegraphics[width=1.0\linewidth]{images/urd_arch_v2.pdf}
\caption{
Architecture of the proposed high-level feature guided decoder (HFGD).
%
HFGD consists of three components: a context augmentation encoder (CAE) that generates accurate pixel representations, an ultra SemanticFPN (U-SFPN) that produces high-resolution feature maps, and a high-level feature guided module (HFGM) that guides the upsampling process in U-SFPN.
% the high-resolution feature map generation of U-SFPN. 
%
With carefully designed stop gradient operations (i.e. the red crosses~\textcolor{stop_grad_red}{\xmark}), the the class kernels will co-evolve with the CAE branch during training and guide the learning of the U-SFPN branch.
See Sec.~\ref{sec:method:hfgm} for details.
}
\label{fig:URD:Arch}
\end{figure*}


To achieve a good balance between accuracy and efficiency, our proposed High-level Feature Guided Decoder (HFGD) adopts a computationally more efficient upsampling strategy but can better utilize the good high-level coarse features to guide the training process of the upsampling head (i.e. HFGM in Sec.~\ref{sec:method:hfgm}), 
% 
Since the high-level coarse features play a crucial role, HFGD first improve the high-level coarse feature maps using a novel context-augmented encoder (CAE, in Sec.~\ref{sec:method:cae}).
% 
Lastly, we make some modifications based on SemanticFPN~\cite{cPanopticFPN} and propose Ultra SemanticFPN (U-SFPN, in Sec.~\ref{sec:method:upsampling}).
generates an accurate low-resolution feature map without using a dilated backbone to save computation. 


\subsection{High-level feature guided module (HFGM)}
\label{sec:method:hfgm}

\noindent\textbf{Motivation.} 
The last stage high-level features extracted from a backbone are already ``good'' representations for semantic segmentation since segmentation is essentially a per-pixel classification task.
% 
However, the high-level features are too coarse ($OS=32$) for accurate segmentation.

In contrast, the features from the earlier stages are in higher resolution and contains more detailed information, and thus suitable for compensating the lost spatial details.
% 
However, there exists a substantial gap between low-level backbone features and the final segmentation feature.
% 
Different from previous FPN-like methods that equally treat the high-/low-level during the fusing stage in the upsampling head, we propose to only ``trust'' the high-level features %more 
and use them as guidance for the upsampling head.

\noindent\textbf{Implementation.} 
We realize the aforementioned idea with properly stopped gradients (see~Fig.~\ref{fig:URD:Arch}).
%
The stop gradient operation inside HFGM ensures the class kernels are updated only according to the right path (i.e., high-level coarse features on $OS=32$).
%
The stop gradient operations on $OS=n$ paths (between U-SFPN and CAE~/~backbone) protect well-trained backbone features from being negatively affected by the noisy gradients from the randomly initialized upsampling head.


With these stop gradient operations, the class kernels are trained to cooperate with the ``good'' high-level CAE features and transform a 256-D pixel representation to class probabilities.
The class kernels guide the training of the upsampler because the weights in the upsampler adapt according to the class kernels (note the stop gradient operation inside HFGM) but not vice versa.
%
As a result, pixel representations from the upsampler are forced to approaching their corresponding pixel representations from CAE.
%
In this way, the class kernels are free from noisy gradient (especially in the early training epochs) from its left path and guide the learning of the upsampling head (e.g. U-SFPN).

Another small but very helpful operation is the inclusion of an axial attention~\cite{cAxialAttention,cAxialDeepLab,cCAA} layer.
%
This axial attention broadcasts the gradient of one pixel to all spatial locations, 
%
resulting in further improvement 
(see Tab.~\ref{tab:ablation_hfgm}).
%
In consideration of computation, we choose axial attention~\cite{cAxialAttention} instead of full self-attention for spatial context augmentation since the resolution of the path is high (e.g. $OS=2 or 4$).
% 
The computation difference between axial attention and full self-attention is huge for high-resolution input~\cite{cAxialAttention,cAxialDeepLab,cCAA}.

\subsection{Context-augmented encoder (CAE)}
\label{sec:method:cae}

We propose a context-augmented encoder to achieve further improvement since the coarse high-level features are used as guidance and play a crucial role in our HFGD.
% 
Note that the resolution of this path is low (i.e. $OS=32$), we can freely use any powerful modules (e.g. ASPP~\cite{cDeepLabV3,cDeepLabV3Plus}, , PPM~\cite{cPSPNet}, OCR~\cite{cOCR} and full self-attention~\cite{cNonLocal,cAttentionIsAllYourNeed,cCFNet}) without introducing too much computation.

In our experiments, we find using CAR~\cite{cCAR} based CAE brings most accuracy gain (see Tab.~\ref{tab:ablation_cae}).
Specifically, our CAE contains two leading convolution layers to 
% \hy{first transform the backbone features to 2048D features, then transform to the} 
transform the backbone features to 512-D,
followed by a full self-attention layer~\cite{cNonLocal} regularized by class-aware regularizations (CAR)~\cite{cCAR} and a trailing convolution layer.
%
Note that using $1\times1$ conv layers is much better than $3\times3$ conv layers (50.76\% vs 50.42\%) in CAE possibly due to that the receptive field size of every pixel is large enough.
%
Using two consecutive convolutions layers is better, possibly due to increased capacity, so that the training on segmentation data can focus more on updating these new weights without modifying the backbone weights too much.

\subsection{Ultra SemanticFPN (U-SFPN)}
\label{sec:method:upsampling}

For the upsampling head, we propose Ultra SemanticFPN (U-SFPN) based on SemanticFPN (SFPN)~\cite{cPanopticFPN} with the following three modifications.
%
Since our upsampling training is more effective, we can keep upsampling to an even higher resolution (i.e. $OS=2$) with better accuracy (79.81\% vs 79.11\% mIOU using ResNet-50 backbone on Cityscapes)\footnote{We only test $OS=2$ results on Cityscapes since the ground truth annotations on other datasets (e.g. Pascal Context) are not as accurate.}.


To achieve this goal, we include one more ``conv-upsample'' block for every branch to get $OS=2$ feature maps.
%
Note that we did not use $OS=2$ low-level features from the backbone since some recent backbones (e.g. Swin~\cite{cSwin}, ConvNeXT~\cite{cConvNeXT}) do not have an $OS=2$ feature map (directly downsampled to $OS=4$).
% 
Secondly, we change the leading $1\times1$ conv to $3\times3$ conv layers since it can collect more spatial information to correct potential ``noise`` in the low-level features.
% 
Thirdly, we increase the channel numbers from 128 to 256 since we find it is beneficial for obtaining more accurate and detailed results.






% \section{Implementation Details}
% % \label{sec:implementation}
% \label{sec:HFGD:training_settings}

\section{Training details}
\label{sec:HFGD:training_settings}

For fair comparison, we use the same training settings (listed below) for all the ablation studies and the experiments to compare with the other state-of-the-art methods unless specified.
% , we applied training settings as follows:
\begin{table}[h]
\centering
\small
\resizebox{\linewidth}{!}{%
\begin{tabular}{l|c|c} 
    \toprule
    Settings    & Ablation studies&  SOTA experiments \\
    \midrule
    \midrule
    Batch size & 16 & 16  \\
    Optimizer & SGD & AdamW\\
    Learning rate decay & \textit{poly}  & \textit{poly} \\
    Initial Learning rate & 0.01 & 0.00004 \\
    Weight decay & 0.0001  & 0.05\\
    Photo Metric Distortion & - & \checkmark \\
    Batch Norm Decay & 0.9997 & 0.9997 \\
    Sync Batch Norm & \checkmark & \checkmark \\
    Layer Norm epsilon & 1e-6 & 1e-6 \\
    Mixed precision & \checkmark & \checkmark \\
    \bottomrule
\end{tabular}
}
\label{tab:urd:ablation_training_settings}
\end{table}

% In addition, all the experiments in ablation studies, including baselines, are conducted or reproduced under deterministic technology with the same training settings, which brings complete fair comparisons to demonstrate the effectiveness of our HFGD.

\section{Experiments on Pascal Context Dataset}
The Pascal Context dataset contains 4,998 training images and 5,105 testing images.
%
Following the common practice, we use its 59 semantic classes to conduct the ablation studies and experiments.
%
Unless specified, we train the models on the training set for 30K iterations for ResNet backbone and 40K for Swin-Large and ConvNeXt-Large.



\subsection{Ablation studies on CAE}

We first conduct ablation studies on CAE in Tab.~\ref{tab:ablation_cae} since we use the high-level coarse features to guide the training of the upsampling head.
Our CAE design surpasses the other alternatives by a substantial margin, with and without SemanticFPN~\cite{cPanopticFPN}.

\input{tables/ablation_studies_cae}






\subsection{Ablation studies on HFGM}

In Tab.~\ref{tab:ablation_hfgm}, we evaluate the effectiveness of our proposed HFGM.
The upsampling head is fixed to SemanticFPN and the CAE part is fixed to our design.
%
HFGM without axial attention improves mIOU by 0.46\% while HFGM with axial attention improves mIOU by 1.52\%.
%
% As we mentioned earlier, 
% Possible reason has been discussed in Sec.~\ref{},
Axial attention can efficiently broadcast the effect of HFGM to all spatial locations (also see Sec.~\ref{sec:method:hfgm} for more discussion). 
% Therefore, using Axial-Attention before HFGM improves mIOU by 1.52\% over the baseline that is not using HFGM.

\input{tables/ablation_studies_hfgm}


\subsection{Ablation studies on U-SFPN}

In Tab.~\ref{tab:ablation_upsampling}, we 
% After we evaluated the effectiveness of our proposed CAE and HFGM, we continued to 
conduct ablation studies on U-SFPN to verify the effectiveness of our improved version of SFPN with fixed CAE and HFGM.
% Table 3 shows that 
Replacing SFPN with U-SFPN improves the mIOU of ResNet-50 (CAE + HFGM) by 0.48\%, reaching 50.76\%.
The improvement is even larger (1.14\%) when using Swin-Large as the backbone.

\input{tables/ablation_studies_upsampling}



\subsection{Ablation studies on HFGD}

In Tab.~\ref{tab:ablation_hfgd},
% Finally, 
we conduct ablation studies on each module of our HFGD framework using  previously found best configurations in Tab.~\ref{tab:ablation_cae}-\ref{tab:ablation_upsampling}.
%
Using all modules together leads to significant accuracy improvement, indicating the effectiveness of the overall architecture.
% of our proposed HFGD.

\input{tables/ablation_studies_final}

\subsection{Computational cost of HFGD}



The computational cost of our HFGD and two other state-of-the-art methods are listed in Tab.~\ref{tab:HFGD:flops}.
% 
HFGD uses much lower GFLOPs than a similar dilation model (Self-Attention + CAR~\cite{cCAR}) but achieves better mIOU (50.76\% vs 50.50\%~\cite{cCAR}).
% 
When compared with SemanticFPN, HFGD ($OS=4$) achieves 3.62\% mIOU gain with an affordable extra computation cost (71.63 GFLOPs vs 45.65 GFLOPs).

\input{tables/computational_cost}

\subsection{Comparison with the state-of-the-art methods}

To compare with the state-of-the-art, we adopt ConvNeXt-L as the backbone for our HFGD.
% 
We set the training iterations to 40K while
all the other training settings are the same as stated in Sec.~\ref{sec:HFGD:training_settings}.
% except that we set the training iterations to 40K.
% 
As shown in Tab.~\ref{tab:urd:SOTA-PascalContext}, our HFGD achieved 63.8\% mIOU with single-scale without flipping and 64.9\% mIOU with multi-scales with flipping, outperforming previous state-of-the-art by 1\% mIOU in ECCV-2022.
 %
HFGD is now the new state-of-art method on Pascal Context for the methods that only use the ImageNet pre-trained backbone without extra techniques~\cite{cAugReg}. 


\subsection{Visualizations on Pascal Context}

We present visual comparisons among Self-Attention + CAR (dilation, $OS=8$), SemanticFPN, and our HFGD.
% 
As shown in Fig.~\ref{fig:CFGD:PascalContext-R50-Vis}, HFGD achieves better accuracy than SemanticFPN and contains more details than dilated model (e.g. the last row, the legs of the person).



\section{Experiments on COCOStuff164k Dataset}

\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\linewidth]{images/vis_pascalcontext_r50_vs.pdf}
\caption{
Visual comparisons on Pascal Context between SemanticFPN, Self-Attention + CAR (denoted as ``Dilated OS=8''), and our HFGD.
All models used ResNet-50 as the backbone.
The results are obtained using single-scale without flipping.
}
\label{fig:CFGD:PascalContext-R50-Vis}
\end{figure}

\input{tables/pascalcontext_sota_tab}
\input{tables/cocostuff164k_sota_tab}

COCOStuff-164k, which becomes popular in recent years, poses a great challenge for semantic segmentation models due to its high diversity (118k training images and 5000 testing images) and complexity (171 classes). 
% 
We adopt ConvNeXt-Large as our backbone network and follow the training settings described in Sec.~\ref{sec:HFGD:training_settings} and we train our model for 40K iterations.
In Tab.~\ref{tab:HFGD:SOTA-COCOStuff164k}, we compare our proposed HFGD with other state-of-the-art methods.
HFGD outperforms the previous state-of-the-art by a large margin (49.4\% vs 47.3\% mIOU).


\begin{figure*}[t]
\centering
\includegraphics[width=1.0\linewidth]{images/vis_cityscapes_r50_vs.pdf}
\caption{
Visual comparisons on Cityscapes between SemanticFPN ($OS=4$), HFGD ($OS=4$), and HFGD ($OS=2$).
All models used ResNet-50 as the backbone.
The results are obtained using single-scale without flipping.
}
\label{fig:CFGD:Cityscapes-R50-Vis}
\end{figure*}

\section{Experiments on Cityscapes Dataset}

Cityscapes is a semantic segmentation dataset that consists of high-resolution images of road scenes with accurate annotations.
It has 19 labeled classes and contains 2975/500/1525 training/validation/test images.

\subsection{Ablation studies on feature map resolution}
\label{sec:exps:ablation_os2}
\input{tables/ablation_studies_ct_os2}
%
We mainly conduct ablation experiments on Cityscapes to verify the superiority of using ultra high resolution feature maps (i.e. $OS=2$) since its GT annotations are the most accurate.
% 
We use ResNet-50 as the backbone and train SemanticFPN, HFGD ($OS=4$), and HFGD ($OS=2$) for 30K iterations following the training settings in Sec.~\ref{sec:HFGD:training_settings}. 
%
As shown in Tab.~\ref{tab:ablation_ct_os2}, the $OS=2$ has further improved the accuracy over $OS=4$ by 0.7\% mIOU (79.11 vs 79.81).

\subsection{Comparison with the state-of-the-art methods}
To compare our method against the state-of-the-art, we use ConvNeXt-Large and follow the training settings described in Sec.~\ref{sec:HFGD:training_settings}.
%
We set the crop size to 513$\times$1025 and train our HFGD model for 60K iterations for the $OS=4$ version and 80K iterations for the $OS=2$ version.
%
As shown in Tab.~\ref{tab:HFGD:SOTA-Cityscapes}, the proposed HFGD outperforms the previous state-of-the-art proposed in ICML'22 and ECCV'22.

\input{tables/cityscapes_sota_tab}

\subsection{Visualizations on Cityscapes}

We present visual comparisons between SemanticFPN ($OS=4$) and our HFGD ($OS=4$ and $OS=2$) in Fig.~\ref{fig:CFGD:Cityscapes-R50-Vis}. 
%
HFGD ($OS=2$) is clearly more capable of 
% has a better ability to 
segmenting the small/thin objects.
For example, the traffic light in Fig.~\ref{fig:CFGD:Cityscapes-R50-Vis} (last row) is partially missing in OS=4, which might be dangerous for automatic vehicles.
% Missing a traffic light in autopilot mode is dangerous.

% \input{sections/limitations}


\section{Conclusion }
In this paper, we propose to use the high-level coarse feature to guide the training of the upsampling head (up to $OS=2$), resulting in an effective and efficient decoder framework.
%
Specifically, HFGD consists of three important components, including HFGM, a context-augmented encoder (CAE), and an upsampling head (U-SFPN), among which CAE and the upsampling head can be upgraded with the lasted advancements.
% 
With thorough experiments, HFGD achieves largely improved performance over previous upsampling-based state-of-the-art methods (i.e., SemanticFPN~\cite{cPanopticFPN}) while using much less computation cost than dilated CNNs (e.g. Self-Attention + CAR~\cite{cCAR}).

\section{Limitation}

Theoretically, we could upsample one more time inside U-SFPN, obtaining an $OS=1$ feature map. However, this increases the computation from 153.62 ($OS=2$) to 593.77 ($OS=1$) GFLOPs, which is not affordable. So even more lightweight decoder is required for $OS=1$.

\clearpage
\clearpage

\appendix
\section{Appendix}
\input{appendix/appendix_core}

\clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
