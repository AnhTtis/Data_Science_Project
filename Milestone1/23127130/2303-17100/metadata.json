{
    "arxiv_id": "2303.17100",
    "paper_title": "Dependent Task Offloading in Edge Computing Using GNN and Deep Reinforcement Learning",
    "authors": [
        "Zequn Cao",
        "Xiaoheng Deng"
    ],
    "submission_date": "2023-03-30",
    "revised_dates": [
        "2023-03-31"
    ],
    "latest_version": 1,
    "categories": [
        "cs.DC"
    ],
    "abstract": "Task offloading is a widely used technology in Mobile Edge Computing (MEC), which declines the completion time of user task with the help of resourceful edge servers. Existing works mainly focus on the case that the computation density of a user task is homogenous so that it can be offloaded in full or by percentage. However, various user tasks in real life consist of several inner dependent subtasks, each of which is a minimum execution unit logically. Motivated by this gap, we aim to solve the Dependent Task Offloading (DTO) problem under multi-user multi-edge scenario in this paper. We firstly use Directed Acyclic Graph (DAG) to represent dependent task where nodes indicate subtasks and directed edges indicate dependencies among subtasks. Then we propose a scheme based on Graph Attention Network (GAT) and Deep Reinforcement Learning (DRL) to minimize the makespan of user tasks. To utilize GAT efficiently, we put the training of it on resourceful cloud in unsupervised style due to the numerous data and computation resource requirements. In addition, we design a multi-discrete Action space for DRL algorithm to enhance the applicability of our proposed scheme. Experiments are conducted on broadly distributed synthetic data. The results demonstrate that our proposed approach can be adapted to both simple and complex MEC environments and outperforms other methods.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.17100v1"
    ],
    "publication_venue": null
}