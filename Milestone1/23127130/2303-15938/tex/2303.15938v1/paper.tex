\documentclass[runningheads]{llncs}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage[final]{pdfpages}
\usepackage{todonotes}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{placeins}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{boldline}
\usepackage{cellspace}
\setlength\cellspacetoplimit{4pt}
\setlength\cellspacebottomlimit{4pt}
\usepackage{color}
\renewcommand\UrlFont{\color{blue}\rmfamily}
\newcommand{\blue}[1]{{\color{blue}#1}}
\def\versionLong{1}
\begin{document}
\title{fRegGAN with K-space Loss Regularization for Medical Image Translation}
\if\versionLong0
\author{Anonymous}
\institute{Famous Institute, Famous State, Famous Country}
\else
\author{Ivo M. Baltruschat\inst{1} \and
Felix Kreis\inst{1} \and
Alexander Hoelscher\inst{1} \and
Melanie Dohmen\inst{1} \and
Matthias Lenga\inst{1}
}
\authorrunning{I. M. Baltruschat et al.}
\institute{Bayer AG, MÃ¼llerstr. 178, 13353 Berlin, Germany\\
\email{ivo.baltruschat@bayer.com}}
\fi
\maketitle              
\begin{abstract}
    Generative adversarial networks (GANs) have shown remarkable success in generating realistic images and are increasingly used in medical imaging for image-to-image translation tasks. However, GANs tend to suffer from a frequency bias towards low frequencies, which can lead to the removal of important structures in the generated images. To address this issue, we propose a novel frequency-aware image-to-image translation framework based on the supervised RegGAN approach, which we call fRegGAN. The framework employs a K-space loss to regularize the frequency content of the generated images and incorporates well-known properties of MRI K-space geometry to guide the network training process. By combine our method with the RegGAN approach, we can mitigate the effect of training with misaligned data and frequency bias at the same time. We evaluate our method on the public BraTS dataset and outperform the baseline methods in terms of both quantitative and qualitative metrics when synthesizing T2-weighted from T1-weighted MR images. Detailed ablation studies are provided to understand the effect of each modification on the final performance. The proposed method is a step towards improving the performance of image-to-image translation and synthesis in the medical domain and shows promise for other applications in the field of image processing and generation.
\end{abstract}
\section{Introduction} 
Generative adversarial networks (GANs)\cite{goodfellowGenerativeAdversarialNetworks2014} have gained a lot of attention in the last years due to their ability to generate realistic images. In the medical imaging field, they are often used for image-to-image translation problems, such as mapping magnetic resonance imaging (MRI) to computed tomography images\cite{masperoDoseEvaluationFast2018,yangUnsupervisedMRtoCTSynthesis2020}, T1-weighted to T2-weighted MRI\cite{liuUnifiedConditionalDisentanglement2021,shenMultiDomainImageCompletion2021}, or low- to high dose contrast enhanced MRIs\cite{hauboldContrastAgentDose2023,pasumarthiGenericDeepLearning2021}. Such approaches could potentially reduce healthcare costs and patient burden while maintaining or even improving the diagnostic value of a modality. While GANs have shown promising results in these tasks, they tend to suffer from a frequency bias towards low frequencies\cite{schwarzFrequencyBiasGenerative2021a}. This is especially problematic for medical image-to-image translation tasks, where preserving the high frequency content (i.e., edges) of the images is crucial. Local errors in the generated images can lead to the removal of important structures, such as lesions, which can have a significant impact on downstream tasks. To address this problem, we propose a novel frequency-aware image-to-image translation framework, which is based on the supervised RegGAN framework\cite{kongBreakingDilemmaMedical2021}. We employ a K-space loss, which is able to regularize the frequency content of the generated images (see Figure~\ref{fig1}). 
\begin{figure}
    
    
    
    \includegraphics[clip, trim=0cm 3.5cm 3.4cm 0.0cm, width=1.00\textwidth]{02_figs/Fig_overview.pdf}
    
    \caption{Overview of different models: RegGAN, fRegGAN, and fReg$^2$CycleGAN.}\label{fig1}
\end{figure}
Obtaining well-aligned data is often a challenging task in medical image-to-image translation. Common image misalignments, like poor registration of source and target domain samples, can lead to significant degradation in performance of supervised approaches like Pix2Pix\cite{isolaImagetoImageTranslationConditional2018}, which rely heavily on pixel-wise reconstruction losses. While unsupervised training methods like CycleGAN\cite{zhuUnpairedImagetoImageTranslation2020} can alleviate this issue, they generally tend to have lower performance compared to supervised methods. Huang et al\cite{kongBreakingDilemmaMedical2021} proposed the supervised RegGAN medical image-to-image translation approach. In the RegGAN framework, the misaligned target images are considered as noisy labels and the generator is trained with an additional registration network estimating a displacement vector-field to adaptively fit the misaligned noise distribution.
We are not the first to investigate the use of the frequency domain for improving the performance of GANs, but our work is the first to apply the frequency domain to the medical image-to-image translation task and to combine it with the RegGAN. 
In the field of image processing and generation, several recent studies \cite{yangFDAFourierDomain2020,jiangFocalFrequencyLoss2021,yangFreGANExploitingFrequency2022} have explored the use of the frequency domain for improving the performance of various models. 
The closest to our idea is in \cite{caiFrequencyDomainImage2021}, where a frequency domain image translation framework was proposed for image-to-image translation. The framework exploits frequency information by adding multiple losses based on the frequency transformed images to the optimization problem of the generator. They showed for natural images that the proposed method can improve the performance of image-to-image translation and synthesis. An open question is, if the proposed method can be combined with cycle consistency and applied to medical images. 
We summarize our contributions as follows:
\begin{enumerate}
    \item We further investigate the RegGAN approach by Huang et al\cite{kongBreakingDilemmaMedical2021} by generally applicable modifications to the architecture and training procedure. We extend their CycleGAN approach by adding a second registration network, which we call Reg$^2$CycleGAN. 
    \item We incorporate constraints in the frequency domain (K-space) to regularize and guide the network training process. This regularization technique is motivated by MRI acquisition and reconstruction methods which leverage the distribution of image feature information in K-space to reduce noise and/or accelerate image acquisition.
    \item Our proposed method is evaluated on the publicly available BraTS dataset. The results show that our method outperforms the baseline (i.e., RegGAN and CycleGAN) methods in terms of both quantitative and qualitative metrics. We also provide detailed ablation studies to understand the effect of each modification on the final performance. Our proposed method is a step towards improving the performance of image-to-image translation and synthesis in the medical domain with the goal to bring GAN based image synthetization closer to clinical application.
\end{enumerate}
\section{Methods}
\label{methods}
This section begins with a brief overview on how vanilla GANs\cite{goodfellowGenerativeAdversarialNetworks2014} are formulated and then summarize later improvements like the LSGAN\cite{maoLeastSquaresGenerative2017} and RegGAN\cite{caiFrequencyDomainImage2021}. Afterwards, we describe our new Reg$^2$CycleGAN and its extension to the RegGAN as well as the K-space Loss regularization.
GANs are generative models composed of two networks, a generator $G$ and a discriminator $D$. The generator learns a mapping $G: X \rightarrow Y$  from source domain $X$ to target domain $Y$ such that $\widehat{y} = G(x)$ is close to $y$ w.r.t. a specific metric. The discriminator $D$ is a binary classifier that is trained to distinguish between real $y$ and generated $\widehat{y}$. Here, $x$ and $y$ denote sets of paired images $\{(x_i, y_i)\}_{i=1}^N$ where $x_i$ is an image in the source domain $X$ and $y_i$ is the corresponding image in the target domain $Y$. Following the notation from \cite{kongBreakingDilemmaMedical2021}, the vanilla GAN optimization problem is:
\begin{equation}
    \label{eq:vanilla_gan}
    \min_{G} \max_{D} \mathcal{L}_{GAN}(D,G) = \mathbb{E}_{y} \log D(y)  + \mathbb{E}_{x}\log \left( 1 - D(G(x)) \right)
\end{equation}
Since optimizing $G$ and $D$ simultaneously is not possible \cite{goodfellowGenerativeAdversarialNetworks2014}, the optimization problem is solved by alternating between $D$ and $G$. The vanilla GAN training procedure has stability issues, which motivated the proposition of several modifications. The most relevant for our experiments is the Least Squares GAN (LSGAN)\cite{maoLeastSquaresGenerative2017} which replaces the binary cross entropy loss with the mean squared error loss, resulting in the optimization task:
\begin{equation}
    \begin{aligned}
         \min_{D} \mathcal{L}_{D,LSGAN}(D,G) &= \mathbb{E}_{y} \left( D(y) - 1 \right)^2  + \mathbb{E}_{x} \left( D(G(x)) \right)^2  \\
         \min_{G} \mathcal{L}_{G,LSGAN}(D,G) &= \mathbb{E}_{x} \left( D(G(x)) - 1 \right)^2 
    \end{aligned}
\end{equation}
In \cite{kongBreakingDilemmaMedical2021} the robustness of Pix2Pix\cite{isolaImagetoImageTranslationConditional2018}, which uses an L1-loss for supervision, was analyzed in the presence of misaligned source and target domain image data. Based on their experiments on the publicly available BraTS benchmark it is concluded that the L1-loss reconstruction term only works with well aligned images, which is often not the case in medical imaging. Therefore, the RegGAN method proposes to replace the L1-loss with the following correction loss:
\begin{equation}
    \min_{G,R} \mathcal{L}_{corr}(G,R) = \mathbb{E}_{x, y} \left| y - G(x) \circ R(G(x), y) \right| 
\end{equation}
Here $R(G(x), y)$ denotes a deformation vector field (DVF) which is estimated based on $G(x), y$ using VoxelMorph\cite{balakrishnanVoxelMorphLearningFramework2019} and $\circ$ is the resample operator. 
Adding the smoothness constraint $\mathcal{L}_{smooth}(R) = \mathbb{E}_{x, y} \left| \nabla R(G(x), y) \right|^2$ for the DVF, the final RegGAN optimization problem is then formulated as follows:
\begin{equation}
    \label{eq:reggan}
    \begin{aligned}
    \min_{D} \mathcal{L}_{D,reg}(D,G) &= \mathcal{L}_{D,LSGAN}(D,G) = \mathbb{E}_{y} \left( D(y) - 1 \right)^2  + \mathbb{E}_{x} \left( D(G(x)) \right)^2  \\
    \min_{G,R} \mathcal{L}_{G,reg}(D,G,R) &= \mathcal{L}_{G,LSGAN}(D,G) + \lambda_{1} \mathcal{L}_{corr}(G,R) + \lambda_{2} \mathcal{L}_{smooth}(R).
    \end{aligned}
\end{equation}
For unsupervised image-to-image translation, \cite{zhuUnpairedImagetoImageTranslation2020} proposed to use two generators $F, G$, and two discriminators $D_{X}, D_{Y}$, and to add a cycle consistency loss $\mathcal{L}_{cyc}(G,F) = \mathbb{E}_{x}  \left| F(G(x)) - y \right|   + \mathbb{E}_{y}  \left| G(F(y)) - x \right|  $. In additional experiments, they showed that an identity loss $\mathcal{L}_{id}(G, F) = \mathbb{E}_{y}  \left| G(y) - y \right|   + \mathbb{E}_{x}  \left| F(x) - x \right|  $ preserves content information. All four models are trained jointly and the optimization problem for the generators is formulated as follows:
\begin{equation}
    \label{eq:cyclegan}
    \begin{split}
    \min_{G,F} \mathcal{L}_{G,CycGAN}(D_{X}, D_{Y}, G, F) = \mathcal{L}_{G,LSGAN}(D_{Y},G) + \\ \mathcal{L}_{G,LSGAN}(D_{X},F)
    + \lambda_{3} \mathcal{L}_{cyc}(G,F) + \lambda_{4} \mathcal{L}_{idy}(G,F)
\end{split}
\end{equation}
The authors of \cite{kongBreakingDilemmaMedical2021} also proposed to combine cycle consistency with their registration loss, which turns the unsupervised training to become  supervised. This has the beneficial effect that the number of solutions for the generators is reduced and that the overall quality may improve.  
\begin{equation}
    \label{eq:regcyclegan}
    \begin{split}
    \min_{G, F, R} \mathcal{L}_{G,RegCycGAN}(D_{X}, D_{Y}, G, F, R) = 
    \mathcal{L}_{reg}(D_{Y},G, R) + \\ \mathcal{L}_{G,LSGAN}(D_{X},F)
    + \lambda_{3} \mathcal{L}_{cyc}(G,F) + \lambda_{4} \mathcal{L}_{id}(G,F)
\end{split}
\end{equation} 
\subsection{Reg$^2$CycleGAN}
\label{sec:reg2cyclegan}
In Eq.~\ref{eq:regcyclegan}, the registration and hence supervision is only used for generator $G$. We propose to extend the registration loss to generator $F$ as well. This is done by adding a second registration model $R_{X}$ and changing the optimization problem to: 
\begin{equation}
    \label{eq:regregcyclegan}
    \begin{split}
    \min_{G, F, R_{X}, R_{Y}} \mathcal{L}_{G, Reg^2CycGan}(D_{X}, D_{Y}, G, F, R_{X}, R_{Y}) = 
    \mathcal{L}_{reg}(D_{Y}, G, R_{Y}) + \\ \mathcal{L}_{reg}(D_{X}, F, R_{X})
    + \lambda_{3} \mathcal{L}_{cyc}(G,F) + \lambda_{4} \mathcal{L}_{idy}(G,F)
\end{split}
\end{equation} 
The second registration loss should improve performance of generator $F$, because $F$ suffers from the same data misalignment problem as generator $G$.The improvement of $F$ should also have an positiv impact on $G$ because cycle consistency links the optimization of both generators.
\subsection{Frequency regularization}
The proposed frequency regularization is motivated by MRI, where the raw signal is a frequency domain signal that is organized in K-space. An inverse Fourier transform of the K-space yields the final image. 
The K-space information comprises complex valued base coefficients for a wave-form decomposition of the reconstructed image, where coefficients near the K-space center account for low frequency components which capture large proportions of the image contrast and texture information. Moving to the outer boundary of K-space, the related wave forms attribute to high frequency features of the image. In MRI pulse sequence development, this knowledge about information content in different regions of K-space is crucial and lead to a preference for acquiring central K-space regions when acquisition time is limited. Another important property of the wave form base functions, that further motivates the proposed regularization, is their non-locality, i.e. locally changing image voxel values globally impact the K-space coefficients and local changes to K-space coefficients globally impact voxel values.
We define the frequency loss $\mathcal{L}_{freq}$ as mean L1 distance between the magnitude of the discrete Fourier transform $\mathcal{F}$ of the generated image $G(x)$ and the magnitude of the discrete Fourier transform of the target image $y$. In addition, we use a binary mask $M$, where the circular region around the origin with radius $r$ is set to 1, its inverse $\overline{M}$, and a weighting factor $w_{freq} \in \{f_\text{low}=1,f_\text{hi}=0, f_\text{all}=0.5\}$ to separately control the impact of low and high frequency coefficients. The frequency loss is defined as follows:
\begin{equation}
    \label{eq:frequency_loss}
    \begin{split}
        \min_{G}\mathcal{L}_{freq}(G) =  w_{freq} \, \mathbb{E}_{x}\left\| \left| \mathcal{F}\left\{ G(x) \right\} \right| \odot M  - \left| \mathcal{F}\left\{ y \right\} \right| \odot M \right\|_{1} \\
    + (1-w_{freq}) \, \mathbb{E}_{x} \left\| \left| \mathcal{F}\left\{ G(x) \right\} \right| \odot \overline{M} - \left| \mathcal{F}\left\{ y \right\} \right|\odot \overline{M} \right\|_{1} 
    \end{split}
\end{equation}
where $\odot$ is the Hadamard product.
Similar to the supervised L1-loss, the frequency loss can not be used if the data pair $(x,y)$ is not well aligned. Therefore, we propose to replace the generated image $G(x)$ with its registered version $G(x) \circ R(G(x),y) = x'$ in this case. In our experiments, we add $\lambda_{5} \mathcal{L}_{freq}(G)$ and $\lambda_{5} (\mathcal{L}_{freq}(G) + \mathcal{L}_{freq}(F))$ to the total loss of the GAN and CycleGAN training, respectively.
\section{Experiments and Results}
We perform two sets of experiments. First, we set a baseline and solely investigate our changes to the Reg$^2$CycleGAN. Secondly, we explore the combination of the RegGAN and Reg$^2$CycleGAN with our K-space loss (see Eq.~\ref{eq:frequency_loss}). All our experiments are trained on the public BraTS 2021 dataset\cite{baidRSNAASNRMICCAIBraTS20212021,menzeMultimodalBrainTumor2015,bakasAdvancingCancerGenome2017}. BraTS has 1251 MRI scans of patients with brain tumors. The dataset contains 4 different modalities but we only use the T1w and T2w modalities to train our models. All MRI scans are available as NIfTI files and were acquired with different clinical protocols and various scanners from multiple data contributing institutions. In addition, all volumes are pre-processed, i.e., co-registered to the same anatomical template, interpolated to the same resolution ($1\text{ mm}^3$) and skull-stripped. Hence, the dataset is well aligned and each volume has a size of $240 \times 240 \times 155$ voxels.
To train and evaluate our models, we randomly split the dataset on patient level into a training, validation, and testing set with 1000, 51, and 200 volumes, respectively. We further pre-process the dataset by normalizing each volume to the range $[0, 1]$ based on the $0.5$ and $99.5$ percentile of the volume. Afterwards we slice the 3d volumes in axial direction into 2d images and removed all slices without information. In total this results in 139221, 6891, and 27956 2d slices for training, validation, and testing, respectively.
Based on the survey papers\cite{liu3DBrainHeart2022,xieCrossModalityNeuroimageSynthesis2022,aliRoleGenerativeAdversarial2022}, we selected to most used metrics for evaluating our models: peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and multi-scale structural similarity index (MS-SSIM). 
The SSIM is a measure of the similarity between two images. It is based on the mean and variance of the local luminance and contrast of the images. The MS-SSIM is a multi-scale version of the SSIM. It is defined as the geometric mean of the SSIM at different scales. The SSIM and MS-SSIM are in the range $[0, 1]$ and a higher value indicates a better synthetization. Both metrics are implemented using the TorchMetrics~0.10.3 library\cite{detlefsenTorchMetricsMeasuringReproducibility2022}. 
\subsubsection{Implementation:}
\label{sec:implementation}
We used a fixed training setup for all experiments to ensure a fair comparison. All models are trained with online data augmentation. Here, we use random rotation between $-10$ and $10$ degrees, random translation between $-26$ and $26$ pixels for x and y direction, and random scaling between $0.9$ and $1.1$. In a second step, the input images and the target images are artificially misaligned by applying the same random transformations as the online data augmentation, but only to the target images. Similar to \cite{kongBreakingDilemmaMedical2021}, we use the term noise for this misalignment in the following.
As optimizer, we use Adam\cite{kingmaAdamMethodStochastic2017} with a learning rate of $0.0001$, $\beta_1$ of $0.5$, $\beta_2$ of $0.999$ and no weight decay. We train all models for $1 \times 10^6$ iterations with a batch size of $4$. For the total loss, we used $\lambda_{1}=20$, $\lambda_2=10$, $\lambda_3=10$, $\lambda_4=1$. Empirically, we found for our K-space loss that $r=21$, $\lambda_5=1$ for $f_{hi}$ and $\lambda_5=0.1$ for $f_{low}$ and $f_{all}$ work well.
To implement our models, we use PyTorch~1.12.1\cite{paszkePyTorchImperativeStyle2019} and the PyTorch Lightning~1.8.1\cite{Falcon_PyTorch_Lightning_2019} libraries. All experiments are preformed on a single NVIDIA Tesla A100 GPU with 40 GB of memory. Our trained models and code are publicly available at 
\if\versionLong0
\url{https://github.com/to-be-added-on-acceptance}. 
\else
\url{https://github.com/Bayer-Group/to-be-added}.
\fi
\subsubsection{RegGAN and variations}
Table~\ref{tab1} shows in the right block the results of our first set of experiments. Our baseline (i.e., without artificial misalignment) results show for all three metrics PSNR, SSIM, and MS-SSIM that the supervised GAN is performing better than the unsupervised CycleGAN (i.e., 26.0 vs 24.1, 0.91 vs 0.89, and 0.94 vs 0.91). This is expected since the CycleGAN is trained without any supervision. However, adding noise (i.e., artificial misalignment) to the training we can observe that the situation is vice-versa (i.e., 21.9 vs 23.8, 0.83 vs 0.88, and 0.84 vs 0.90). Both observation are in line with other results in the literature\cite{kongBreakingDilemmaMedical2021}. As described in Section~\ref{methods}, adding a registration loss to the CycleGan (i.e., RegCycleGAN) makes it also supervised. Now, the supervised RegCycleGAN is performing better than the supervised GAN (i.e., 27.4 vs 26.0, 0.92 vs 0.91, and 0.95 vs 0.94). The cycle consistency combined with supervised learning is a strong regularization term that helps to learn the mapping between the input and the target image. Adding a second registration loss (i.e., Reg$^2$CycleGAN) to the RegCycleGAN improves the results of the generator $F$ but did not help to further improve our main generator $G$. The second registration loss has only an indirect connection to the fist generator $G$ by the cycle consistency, which is not enough to improve the results. 
\begin{table}\centering
    \caption{Quantitative results for training the GAN and CycleGAN w/ and w/o the following features: registration, K-space loss, and noise (i.e., artificial misalignment, see Section~\ref{sec:implementation}). For example, we refer to a model trained with registration and noise as ''w/ n, r``. For our models trained as CycleGAN, we show results for both generators $G$ and $F$. Bold text emphasises the best overall result for each metric in each block. }\label{tab1}
    \begin{tabular}{clc|c|c || cc|c|c|c}\toprule
                            \multicolumn{3}{c}{ } & \multicolumn{2}{c}{\bf{CycleGAN}} & &\multicolumn{2}{c}{\bf{GAN}} & \multicolumn{2}{c}{\bf{CycleGAN}} \\
                            &  & \bf{GAN}   & \bf{$G$} & \bf{$F$} & & w/ n &  w/ n, r & w/ n &  w/ n, r$^2$ \\ \hlineB{1.25} \rowcolor[gray]{.95}
                            & w/o n, r  & 26.0  & 24.1      & 23.9  & baseline & 21.9 & 26.8 & 23.8 & 27.3 \\
                            & w/ n,     & 21.9  & 23.8      & 22.9  & f$_{hi}$ & 22.7 & 26.9 & 23.6 & 27.4\\\rowcolor[gray]{.95} 
                            & w/ n, r       & 26.8  & \bf{27.4} & 23.2  & f$_{low}$ & 24.4 & \bf{28.8} & 25.2 & 28.7\\
    \multirow{-4}{*}{PSNR}  & w/ n, r$^2$   & -     & 27.3      & 27.3  & f$_{all}$ & 24.6 & 28.4 & 25.2 & 28.7\\ \hlineB{1.25} \rowcolor[gray]{.95}
                            & w/o n  & 0.91  & 0.89      & 0.89  & baseline & 0.83 & 0.91 & 0.88 & 0.92 \\
                            & w/ n     & 0.83  & 0.88      & 0.87  & f$_{hi}$ & 0.86 & \bf{0.93} & 0.88 & \bf{0.93} \\\rowcolor[gray]{.95} 
                            & w/ n, r       & 0.91  & \bf{0.92} & 0.89  & f$_{low}$ & 0.88 & 0.92 & 0.89 & 0.92 \\
    \multirow{-4}{*}{SSIM}  & w/ n, r$^2$   & -     & \bf{0.92} & 0.90  & f$_{all}$ & 0.88 & 0.91 & 0.89 & 0.92 \\ \hlineB{1.25} \rowcolor[gray]{.95}
                            & w/o n  & 0.94  & 0.91      & 0.93  & baseline & 0.84 & 0.94 & 0.90 & 0.95 \\
                            & w/ n     & 0.84  & 0.90      & 0.92  & f$_{hi}$ & 0.87 & 0.94 & 0.90 & 0.95 \\\rowcolor[gray]{.95} 
                            & w/ n, r       & 0.94  & \bf{0.95} & 0.92  & f$_{low}$ & 0.91 & \bf{0.97} & 0.93 & 0.96 \\
    \multirow{-4}{*}{MS-SSIM} & w/ n, r$^2$ & -     & \bf{0.95} & 0.96  & f$_{all}$ & 0.92 & 0.96 & 0.93 & 0.96\\ \bottomrule
    \end{tabular}
\end{table}
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
\subsubsection{K-space loss}
We defined three different settings of the K-space loss for our experiments. First, we only select the low frequencies with $w_{freq} = f_\text{low} = 1$. Secondly, the high frequencies are selected by $w_{freq} = f_\text{hi} = 0$. Finally, we also test the effect if all frequencies are used $w_{freq} = f_\text{all}= 0.5$. All three settings are tested for both GAN and CycleGAN models with and without registration loss. The results are shown in
Table~\ref{tab1}. For all four training modes (i.e., GAN with noise, GAN with noise and reg, CycleGAN with noise, and CycleGAN with noise and reg), we observe that adding a frequency loss improves the quantitative metrics. The only exception is the CycleGAN model with $f_\text{hi}$. Here, the PSNR is slightly worse than the baseline (i.e., 23.6 vs 23.8). However, the SSIM and MS-SSIM are the same as the baseline with 0.88 and 0.90, respectively. The best results are achieved by the GAN model with $f_\text{low}$ and the registration loss. The PSNR is 28.8, the SSIM is 0.92, and the MS-SSIM is 0.97. The results show that the frequency loss is useful to improve the quantitative metrics. In Figure~\ref{fig3}, we show qualitative examples of the generated images. Here, we see that the models trained with $f_\text{low}$ and registration can better synthesize the highlighted region.
\begin{figure}
    
    \includegraphics[clip, trim=0cm 0cm 0cm 0cm, width=1.00\textwidth]{02_figs/Fig3.pdf}
    \caption{Qualitative results for training our models with K-space loss. The first column shows the T1w input image and the T2w ground truth. Column by column, we show the prediction and the residual to ground truth for each model. Row by row, we show different training setups for each of these models.} \label{fig3}
\end{figure}
\section{Conclusion} 
We have proposed a novel image-to-image translation approach in the medical imaging field called fRegGAN. Our method extends the RegGAN approach by incorporating a Fourier-space based loss to regularize and guide the network training process. Our extensive experiments on the BraTS 2021 dataset show that our proposed method outperforms the baseline methods in terms of both quantitative metrics and qualitative visual inspection. 
However, in our evaluation we found that comparison based on quantitative metrics with reference images has some limitations. Skull stripping (to anonymize patients) resulted in some artificial artifacts between input and output volumes (i.e., T1w and T2w). Our results provide evidence that incorporating Fourier-space information can improve the performance and visual quality of image-to-image translation in the medical domain.
 
\bibliographystyle{splncs04}
\bibliography{references}
\end{document}
