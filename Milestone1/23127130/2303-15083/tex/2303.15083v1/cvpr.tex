% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)
\pdfoutput=1
\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{epsfig}
\usepackage{float}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[table]{xcolor}
\usepackage{bm}
\usepackage{marvosym}
\usepackage[accsupp]{axessibility}
\usepackage{multicol}
\newlength\savewidth
\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
                            \global\arrayrulewidth 1.2pt}%
                   \hline
                   \noalign{\global\arrayrulewidth\savewidth}}
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{1911} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{UniDistill: A Universal Cross-Modality Knowledge Distillation Framework\\ for 3D Object Detection in Bird's-Eye View}

\author{Shengchao Zhou\textsuperscript{\rm 1}\thanks{Equal Contribution}\quad Weizhou Liu\textsuperscript{\rm 1}\footnotemark[1]\quad Chen Hu\textsuperscript{\rm 1}\thanks{Corresponding Author}\quad Shuchang Zhou\textsuperscript{\rm 1}\quad Chao Ma\textsuperscript{\rm 2}\vspace{5pt}\\
\textsuperscript{\rm 1} MEGVII Technology\\\textsuperscript{\rm 2} MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University\\
{\tt\small \{zhoushengchao,liuweizhou,huchen,zsc\}@megvii.com},~~~{\tt\small chaoma@sjtu.edu.cn}\\\small\url{https://github.com/megvii-research/CVPR2023-UniDistill}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
In the field of 3D object detection for autonomous driving, the sensor portfolio including multi-modality and single-modality is diverse and complex. Since the multi-modal methods have system complexity while the accuracy of single-modal ones is relatively low, how to make a tradeoff between them is difficult. In this work, we propose a universal cross-modality knowledge distillation framework (UniDistill) to improve the performance of single-modality detectors. Specifically, during training, UniDistill projects the features of both the teacher and the student detector into Bird's-Eye-View (BEV), which is a friendly representation for different modalities. Then, three distillation losses are calculated to sparsely align the foreground features, helping the student learn from the teacher without introducing additional cost during inference. Taking advantage of the similar detection paradigm of different detectors in BEV, UniDistill easily supports LiDAR-to-camera, camera-to-LiDAR, fusion-to-LiDAR and fusion-to-camera distillation paths. Furthermore, the three distillation losses can filter the effect of misaligned background information and balance between objects of different sizes, improving the distillation effectiveness. Extensive experiments on nuScenes demonstrate that UniDistill effectively improves the mAP and NDS of student detectors by 2.0\%$\sim$3.2\%.
\end{abstract}
\input{chapter/introduction}
	\input{chapter/relatedwork}
	\input{chapter/methodology}
	\input{chapter/experiments}
	\input{chapter/conclusion}
        \clearpage
	{\small
		\bibliographystyle{ieee_fullname}    
		\bibliography{cvpr}
	}
        \clearpage
        \input{chapter/appendix}
	\clearpage

\end{document}
