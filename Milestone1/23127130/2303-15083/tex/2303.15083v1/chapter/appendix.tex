\twocolumn[
\begin{@twocolumnfalse}
\begin{center}
\Large{\textbf{Supplemental Material}}
\end{center}
\end{@twocolumnfalse}
]
This supplementary material contains additional details of the main manuscript. Section \ref{sec:1} presents additional details of the models and training strategies. Section \ref{sec:2} complements more experiments not included in the main manuscript. Section \ref{sec:3} shows more visualization results to prove the effectiveness of UniDistill.
\section{Details of Models and Training}
\label{sec:1}
To prove the effectiveness of UniDistill, we introduce BEVDet, CenterPoint and BEVFusion as the camera based, LiDAR based and LiDAR-camera based detectors. For BEVDet, the features of input images are firstly extracted by the backbone of ResNet-50 and then projected to BEV through LSS. We set the projected features to be the low-level BEV features $\boldmath{F}_{\text{cam}}^{\text{low}}$. With respect to CenterPoint, the input LiDAR points are distributed to regular voxels and the features of each voxel are extracted by 3D convolution. Then, the features of voxels in the same column are concatenated and we set the result as $\boldmath{F}_{\text{ldr}}^{\text{low}}$. BEVFusion builds on the above detectors  by concatenating $\boldmath{F}_{\text{cam}}^{\text{low}}$ with $\boldmath{F}_{\text{ldr}}^{\text{low}}$ and then processing it with a fully convolutional network~(FCN). The output features are set to be $\boldmath{F}_{\text{fuse}}^{\text{low}}$. The following steps are the same for different detectors, where a FCN follows as an encoder to produce $\boldmath{F}^{\text{high}}$ and then a detection head of CenterPoint generates classification and regression heatmaps. These heatmaps are used to form $\boldmath{F}^{\text{resp}}$.

For all detectors, during training, the detection head will calculate a classification loss $\mathcal{L}_{\text{Cls}}$ and a regression loss $\mathcal{L}_{\text{Reg}}$ that are combined to form the detection loss $\mathcal{L}_{\text{Det}}$. In Section 4.2 of the main manuscript, to help the detectors perform better, we use auto-scaling to balance the scales between $\mathcal{L}_{\text{Cls}}$ and $\mathcal{L}_{\text{Reg}}$ but turn it off in Section 4.3 for efficiency.

The training of detectors is finished on 20 GeForce RTX 2080Ti GPUs. These GPUs are distributed on 5 machines, where each machine has 4 GPUs, so that we adopt distributed training. Because of the limited memory, each GPU is distributed with 1 training sample.
\section{Complementary Experiments}
\label{sec:2}
In this section, experiments not included in the main manuscript are complemented. In Section \ref{sec:2.1}, UniDistill is compared with BEVDepth and MonoDistill to show its advantages. In Section \ref{sec:2.2.1}, the performance of UniDistill on Waymo is evaluated to show its generalization to different datasets. In Section \ref{sec:2.2}, we replace the detection head with a TransFusion based one and the backbone of BEVDet to Swin Transformer to show the generalization to different architectures. In Section \ref{sec:2.3}, more ablation studies about the adaptive layers and feature distillation are supplemented. In Section \ref{sec:2.4}, the training time and memory usage of UniDistill are listed.
\subsection{Comparison with BEVDepth and MonoDistill}
\label{sec:2.1}
To transfer the depth knowledge of LiDAR points to the camera based detector, which is BEVDet in our experiments, UniDistill introduces knowledge distillation for help. BEVDepth provides another approach for knowledge transfer by supervising the depth prediction of LSS in BEVDet with ground truth generated by projecting LiDAR points to the perspective view. Therefore, we compare UniDistill with BEVDepth to show the advantages of knowledge distillation. We build BEVDepth based on BEVDet by combining the detection loss $\mathcal{L}_{\text{Det}}$ with another depth prediction loss $\mathcal{L}_{\text{Depth}}$ and train it with the full training dataset. The performance of BEVDepth on the testing dataset is in Table \ref{supp_tab:1}. From the results, UniDistill helps BEVDet obtain better performance than BEVDepth, showing the advantages of knowledge distillation.
\begin{table}[t]\small
\centering
\caption{Comparison between UniDistill and BEVDepth on testing dataset to show the advantages of knowledge distillation. “L” and “C” represent LiDAR and camera.}
\resizebox{0.9\width}{!}{
\begin{tabular}{c|c|cclc}
\shline
Method     & \begin{tabular}[c]{@{}c@{}}Teacher\\ Modality\end{tabular} & mAP~$\uparrow$  & mASE~$\downarrow$ & mAOE~$\downarrow$ & NDS~$\uparrow$  \\ \shline
Baseline   & -                                                          & 26.4 & 26.6 & 55.8 & 36.1 \\
BEVDepth   & -                                                          & 28.4 & 26.3 & 55.3 & 37.7 \\ \shline
\rowcolor{black!5}UniDistill & L                                                          & \textbf{28.9} & \textbf{25.9} & \textbf{51.4} & \textbf{38.4} \\
\rowcolor{black!10}UniDistill & L+C                                                        & \textbf{29.6} & \textbf{25.7} & \textbf{49.2} & \textbf{39.3} \\ \shline
\end{tabular}
}
\label{supp_tab:1}
\end{table}
\begin{table}[t]\small
\centering
%\setlength{\abovecaptionskip}{0.5pt}
%\setlength{\belowcaptionskip}{-3pt}
\caption{Comparison between UniDistill and MonoDistill on nuScenes test dataset. “L” and “C” represent LiDAR and camera.}
\resizebox{0.89\width}{!}{
\begin{tabular}{c|cc|ccc}
\shline
Method      & Modality           & \begin{tabular}[c]{@{}c@{}}Teacher\\ Modality\end{tabular} & mAP~$\uparrow$           & mASE~$\downarrow$          & NDS~$\uparrow$           \\ \shline
MonoDistill & \multirow{2}{*}{C} & \multirow{2}{*}{L}                                         & 23.2          & 28.7          & 34.3          \\ \cline{1-1} \cline{4-6} 
UniDistill  &                    &                                                            & \textbf{28.9} & \textbf{25.9} & \textbf{38.4} \\ \shline
\end{tabular}
}
\label{supp_tab:1.1}
\end{table}

\begin{table}[t]\small
\centering
%\setlength{\abovecaptionskip}{0.5pt}
%\setlength{\belowcaptionskip}{-2pt}
\caption{Analysis to show the generalization of UniDistill to Waymo dataset. “L” and “C” represent LiDAR and camera.}
\resizebox{0.89\width}{!}{
\begin{tabular}{c|cc|ccc}
\shline
Method                      & Modality & \begin{tabular}[c]{@{}c@{}}Teacher\\ Modality\end{tabular} & mAPL~$\uparrow$           & mAPH~$\uparrow$          & \multicolumn{1}{l}{mAP~$\uparrow$} \\ \shline
\multirow{3}{*}{UniDistill} & L+C      & -                                                          &71.0               & 71.4              &75.3                          \\
                            & C        & -                                                          &22.3          & 33.0          & 34.5                     \\ \cline{2-6} 
                            & C        & L+C                                                        & \textbf{24.5} & \textbf{36.2} & \textbf{37.7}            \\ \shline
\end{tabular}
}
\label{supp_tab:2.1}
\end{table}

MonoDistill is another knowledge distillation framework that transfers the knowledge from a LiDAR-based teacher to a camera-based student. It directly unifies the architecture of the teacher and student by training the teacher with LiDAR points projected to the perspective view. Therefore, we further compare UniDistill with MonoDistill and the results are listed in Table \ref{supp_tab:1.1}, showing the better performance of UniDistill for the modality combination (C, L).

\subsection{Generalization to Waymo}
\label{sec:2.2.1}
In the main manuscript, all experiments are conducted on the nuScenes dataset. To show the generalization of UniDistill to different datasets, in this section, we further evaluate its performance on Waymo dataset. Specifically, UniDistill is first trained on the Waymo-mini dataset for 18 epochs and then tested on the whole validation set. The results in distillation path (2) are listed in Table \ref{supp_tab:2.1}, showing the effectiveness and generalization of UniDistill on Waymo.

\subsection{Generalization to More Architectures}
\label{sec:2.2}
In the main manuscript, we set the detection head of all detectors to be the same as that of CenterPoint. Therefore, we substitute it with a TransFusion based one and re-evaluate UniDistill to show the generalization of UniDistill to other  detection heads. The evaluation is conducted in distillation path (1) on the validation dataset and the modified detectors are trained on 1/2 training dataset for efficiency. Since the response distillation in UniDistill is not applicable to the TransFusion head, we only leverage the feature distillation and relation distillation for knowledge transfer. The results in Table \ref{supp_tab:2} reveal that UniDistill also improves the performance of the student detector, showing its generalization to different detection heads.

In addition, we substitute the ResNet-50 in BEVDet with Swin Transformer to show the generalization of UniDistill to different backbones. For efficiency, the modified detectors are trained on 1/2 training dataset and then evaluated  in distillation path (2) on the validation dataset. The results in Table \ref{supp_tab:2.2} show that UniDistill improves the performance of student detector and generalizes to different backbones.
\begin{table}[t]\small
\centering
\caption{Performance analysis to show the generalization of UniDistill to TransFusion. “L” and “C” represent LiDAR and camera.}
\resizebox{0.88\width}{!}{
\begin{tabular}{c|cc|ccc}
\shline
Method                       & Modality & \begin{tabular}[c]{@{}c@{}}Teacher\\ Modality\end{tabular} & mAP~$\uparrow$  & mASE~$\downarrow$ & NDS~$\uparrow$  \\ \shline
\multirow{2}{*}{TransFusion} & L+C      & -                                                          & 63.4 & 25.2 & 67.6 \\
                             & L        & -                                                          & 58.5 & 27.2 & 63.4 \\ \shline
UniDistill                   & L        & L+C                                                        & \textbf{60.9} & \textbf{25.9} & \textbf{65.9} \\ \shline
\end{tabular}
}
\label{supp_tab:2}
\end{table}
\begin{table}[t]\small
\centering
%\setlength{\abovecaptionskip}{0.5pt}
%\setlength{\belowcaptionskip}{-3pt}
\caption{Analysis to show the generalization of UniDistill to Swin Transformer. “L” and “C” represent LiDAR and camera.}
\resizebox{0.92\width}{!}{
\begin{tabular}{c|cc|ccc}
\shline
Method                      & Modality & \begin{tabular}[c]{@{}c@{}}Teacher\\ Modality\end{tabular} & mAP~$\uparrow$           & mASE~$\downarrow$          & NDS~$\uparrow$           \\ \shline
\multirow{3}{*}{UniDistill} & L+C      & -                                                          & 63.3          & 24.7          & 69.0            \\
                            & C        & -                                                          & 27.8          & 27.6          & 36.0          \\ \cline{2-6} 
                            & C        & L+C                                                        & \textbf{32.5} & \textbf{25.7} & \textbf{39.7} \\ \shline
\end{tabular}
}
\label{supp_tab:2.2}
\end{table}

\subsection{Additional Ablation Studies}
\label{sec:2.3}
In Section 4.3.4 of the main manuscript, we conduct experiments to show that when evaluating in distillation path (3), the adaptive layers can avoid the performance degradation of the student after knowledge distillation. Some experiments in distillation path (4) are further designed to show that when the teacher detector performs better than the student, adopting the adaptive layers will decrease the effectiveness of UniDistill. The results are listed in Table \ref{supp_tab:3} and reveal that with the adaptive layers, the performance of the student slightly decreases. Therefore, when the teacher performs better than the student, there is no need to introduce the adaptive layers.

We also compare the detection loss $\mathcal{L}_{\text{Det}}$ with/without the adaptive layers and the baseline is the student without UniDistill. The results in Figure \ref{supp_img:1} show that with the adaptive layers, although the detection loss is lower than the baseline, it is always higher than that without the adaptive layers. We think the problem results from that the adaptive layers make it too free for the student to choose what to learn from the teacher. However, since the teacher detector is strong enough to instruct the student, directly aligning the features of the student with teacher can help the student learn better.

In addition, since most of the ablation studies are conducted in path (4), we complement the ablation studies in path (1) to improve the reliability.  As in Section 4.3.1 of the main manuscript,  we compare the original feature distillation with two modified ones that align the low-level BEV features (1) completely or (2) inside a Gaussian-like mask. The results are listed in Table \ref{supp_tab:3.1} and we can get the same conclusion that feature distillation performs better when selecting 9 crucial points for alignment.

\begin{figure}[t]
\centering
%\setlength{\belowcaptionskip}{-10pt}
\includegraphics[width=\linewidth]{image/bevdistill_conv_2-eps-converted-to.pdf}
\caption{Illustration to show that the adaptive layers increase the detection loss when the teacher performs better than the student.}
\label{supp_img:1}
\end{figure} 

\begin{table}[t]\small
%\setlength{\belowcaptionskip}{-20pt}
\centering
\caption{Ablation study in path (4) to show that the adaptive layers decrease the effectiveness of feature distillation and relation distillation when the teacher detector performs better than student.}
\resizebox{0.81\width}{!}{
\begin{tabular}{c|ccc|ccc}
\shline
\multirow{2}{*}{Method} & \multicolumn{3}{c|}{$\mathcal{L}_{\text{Fea}}$} & \multicolumn{3}{c}{$\mathcal{L}_{\text{Rel}}$} \\ \cline{2-7} 
                        & mAP~$\uparrow$    & mAVE~$\downarrow$  & NDS~$\uparrow$   & mAP~$\uparrow$   & mAVE~$\downarrow$  & NDS~$\uparrow$   \\ \shline
Baseline                & 20.3   & 95.2  & 33.1  & 20.3  & 95.2  & 33.1  \\
With Adapt                & 20.7   & 91.4  & 33.9  & 21.2  & 89.6  & 34.2  \\ \shline
W/O Adapt              & \textbf{21.1}   & \textbf{88.5}  & \textbf{34.3}  & \textbf{21.7}  & \textbf{84.5}  & \textbf{35.0}  \\ \shline
\end{tabular}
}
\label{supp_tab:3}
\end{table}

\subsection{Training Time and Memory Usage}
\label{sec:2.4}
In this section, the training time and memory usage of detectors with/without UniDistill are listed. The detectors are trained on 1 GeForce RTX 2080Ti GPU and the training batch size is 1. For the training time, we list the average time to calculate the training loss. With respect to memory usage, we report the max allocated memory during training. The results are illustrated in Table \ref{supp_tab:4} and show that UniDistill will increase the training time and memory usage a lot. Therefore, we plan to introduce the block-wise distillation and other techniques to accelerate the training of UniDistill and decrease its memory usage.

\section{More Visualization Results}
\label{sec:3}
\begin{table}[t]\small
\centering
%\setlength{\abovecaptionskip}{0.5pt}
%\setlength{\belowcaptionskip}{-3pt}
\caption{Ablation study in path (1) to show that feature distillation performs better when selecting crucial points for alignment.}
\resizebox{0.97\width}{!}{
\begin{tabular}{c|ccccc|c}
\shline
\multirow{2}{*}{Method} & \multicolumn{5}{c|}{AP~$\uparrow$}            & \multirow{2}{*}{NDS~$\uparrow$} \\ \cline{2-6}
                        & car  & truck & ped  & motor & mean &                      \\ \shline   
Baseline                    & 82.8 & 52.0  & 76.4 & 54.2  & 53.5 & 63.9                 \\            
Complete                    & 82.4 & 52.1  & 77.4 & 56.8  & 54.3 & 64.2                 \\
Gaussian                & \textbf{84.7} & \textbf{54.3}  & 76.1 & 53.4  & 54.7 & 64.8                 \\ \shline
Crucial                 & 82.9 & 50.5  & \textbf{82.4} & \textbf{61.7}  & \textbf{56.1} & \textbf{65.5}                 \\ \shline
\end{tabular}
}
\label{supp_tab:3.1}
\end{table}
\begin{table}[t]\small
\centering
\caption{Training time and memory usage of the detectors. “L” and “C” represent LiDAR and camera respectively.}
\resizebox{0.9\width}{!}{
\begin{tabular}{cc|cc}
\shline
Modality & \begin{tabular}[c]{@{}c@{}}Teacher\\ Modality\end{tabular} & Training Time~(s) & Memory Usage~(GB) \\ \shline
L+C      & -                                                          & 0.27                                                    & 5.96                                                   \\ \shline
C        & -                                                          & 0.13                                                    & 4.60                                                   \\
C        & L                                                          & 0.33~(+153\%)                                                    & 5.07~(+0.47)                                                   \\
C        & L+C                                                        & 0.40~(+207\%)                                                    & 6.44~(+1.84)                                                   \\ \shline
L        & -                                                          & 0.22                                                    & 3.21                                                   \\
L        & C                                                          & 0.46~(+109\%)                                                    & 4.51~(+1.30)                                                   \\
L        & L+C                                                        & 0.53~(+140\%)                                                    & 5.63~(+2.42)                                                   \\ \shline
\end{tabular}
}
\label{supp_tab:4}
\end{table}
\begin{figure*}[t]
\centering
%\setlength{\belowcaptionskip}{-10pt}
\includegraphics[width=0.8\linewidth,height=9.5cm]{image/cam_distill-eps-converted-to.pdf}
\caption{Visualization of the response features. The boxes in red are the ground truth bounding boxes. The teacher and student detectors are LiDAR-camera based and camera based respectively. The first and second rows represent the results of two scenes. With UniDistill, the background areas are suppressed and the object boundaries are more clear.}
\label{supp_img:2}
\end{figure*} 
\begin{figure*}[t]
\centering
%\setlength{\belowcaptionskip}{-10pt}
\includegraphics[width=0.8\linewidth,height=9.5cm]{image/lidar_distill-eps-converted-to.pdf}
\caption{Visualization of the response features. The boxes in red are the ground truth bounding boxes. The teacher and student detectors are LiDAR-camera based and LiDAR based respectively. The first and second rows show the results of two scenes. With UniDistill, the background areas are suppressed and the object boundaries are more clear.}
\label{supp_img:3}
\end{figure*} 
In this section, we provide more visualization results to show the effectiveness of UniDistill. For the response features of one teacher detector and one student with/without UniDistill, we calculate the mean along the channel dimension and visualize them. The results of the LiDAR-camera based teacher and the camera based student are illustrated in Figure \ref{supp_img:2} and that of the LiDAR-camera based teacher and the LiDAR based student are in Figure \ref{supp_img:3}. From the results, it is revealed that with UniDistill, the background areas are suppressed and the boundaries between objects are more clear. Therefore, there will be fewer false positive predictions and the detection performance is improved.

