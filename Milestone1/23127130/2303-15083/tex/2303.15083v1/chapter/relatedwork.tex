\begin{figure*}[ht]
\centering
%\setlength{\belowcaptionskip}{-20pt}
\includegraphics[width=0.8\linewidth,height=7.7cm]{image/bevdistill-eps-converted-to.pdf}
\caption{The overview of our proposed UniDistill. The branches on the top and bottom are the teacher detector and student detector respectively. For a teacher and a student, three distillation losses to align specific foreground features are calculated after extracting the low-level features and transforming them to BEV, which are feature distillation, relation distillation and response distillation. }
\label{img:2}
\end{figure*} 
\section{Related Work}
\label{sec:relatedwork}
\subsection{3D Object Detection}
Recent mainstream 3D object detectors can be generally divided into two categories: (1) Single-modality detectors based on LiDAR~\cite{pan20213d,liu2020tanet,yin2021center,mao2021pyramid,shi2020pv} or camera~\cite{huang2021bevdet,li2022bevdepth,wang2021fcos3d,wang2022detr3d,park2021pseudo,wang2019pseudo} (2) Multi-modality detectors~\cite{qi2018frustum,liu2022bevfusion,yoo20203d,huang2020epnet} with both types of data as input. One category of LiDAR based detectors are grid-based~\cite{yan2018second,lang2019pointpillars,zhou2018voxelnet,mao2021voxel,yin2021center}, where the unstructured LiDAR points are first distributed to regular grids. As the seminal work, VoxelNet~\cite{zhou2018voxelnet} voxelizes the point clouds, performs 3D convolution, reshapes the features into BEV features and then proposes bounding boxes. PointPillars~\cite{lang2019pointpillars} substitutes the voxels with pillars to encode point clouds, avoiding time-consuming 3D convolution operations. CenterPoint~\cite{yin2021center} proposes an anchor-free detection head and obtains better performance. 

Most camera based detectors perform detection in perspective view like 2D detection. In~\cite{wang2021fcos3d}, the authors proposed FCOS3D, which is an extension of FCOS~\cite{tian2019fcos} to the field of 3D object detection. Similarly in~\cite{wang2022detr3d}, following DETR~\cite{carion2020end}, the authors proposed DETR3D to perform detection in an attention pattern. Recently, some works~\cite{huang2021bevdet,li2022bevdepth} are proposed to detect objects in BEV by applying Lift-Splat-Shoot~\cite{philion2020lift} to transform image features from perspective view to BEV and achieve satisfactory improvement.

LiDAR-camera based detectors outperform the above single-modality counterparts by fusing the complementary knowledge of LiDAR points and images. With 2D detection results, F-PointNet~\cite{qi2018frustum} obtains candidate object areas, gathers the points inside and then performs LiDAR based detection. AVOD~\cite{ku2018joint} and MV3D~\cite{chen2017multi} perform modality fusion with object proposals via ROI pooling. UVTR~\cite{li2022uvtr} unifies multi-modality representations in the voxel space for transformer-based 3D object detection. A recent state-of-the-art detector BEVFusion~\cite{liu2022bevfusion} proposes to transform both the image and the LiDAR features to BEV for modality fusion and result prediction.

Our work is inspired by that the recent detectors adopt a similar detection paradigm in BEV. After transforming the features to BEV, a similar procedure follows, where a BEV encoder further encodes high-level features and a detection head produces prediction results. The similarity of the detection paradigm makes cross-modality knowledge distillation in a universal framework possible.
\subsection{Knowledge Distillation}
Knowledge distillation is initially proposed in \cite{hinton2015distilling} for model compression and the main idea is transferring the learned knowledge from a teacher network to a student. Different works have different interpretations of the knowledge, which include the soft targets of the output layer~\cite{hinton2015distilling} and the intermediate feature map~\cite{romero2014fitnets}. Because of the effectiveness of knowledge distillation, it has been widely investigated in a variety of computer vision tasks, such as 2D object detection~\cite{dai2021general,guo2021distilling,zhang2020improve} and semantic segmentation~\cite{hou2020inter,liu2019structured}.

Recently, it is introduced into 3D object detection for knowledge transfer to single-modality detectors. In~\cite{chong2021monodistill}, the authors proposed to transfer the depth knowledge of LiDAR points to a camera based student detector by training another camera based teacher with LiDAR projected to perspective view. In~\cite{zheng2022boosting,ju2022paint}, a PointPainting~\cite{vora2020pointpainting} teacher is leveraged to instruct a CenterPoint student to produce similar features and responses. 

Although these methods are effective for knowledge transfer, the modalities of the teacher and the student are restricted. However, the diverse and complex application of different detectors will restrict their application. Instead, our proposed UniDistill projects the features of detectors to BEV and supports LiDAR-to-camera, camera-to-LiDAR, fusion-to-LiDAR and fusion-to-camera distillation paths.

We note that a recent work BEVDistill also performs cross-modality distillation in BEV. However, it also imposes restrictions on the modalities of the teacher and the student, resulting in a limited application. Moreover, their main distillation losses are designed for transformer based detectors. Differently, our UniDistill aims to improve the performance of CNN based detectors.