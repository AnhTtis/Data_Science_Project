\begin{figure}[t]
\centering
%\setlength{\belowcaptionskip}{-20pt}
\includegraphics[width=0.8\linewidth]{image/bevdistill_intro-eps-converted-to.pdf}
\caption{Illustration of our proposed UniDistill. The characters in green and blue represent the data process of camera and LiDAR respectively. (a) and (b) show the procedure of two previous knowledge distillation methods, where the modalities of the teacher and the student are restricted. By contrast, our proposed UniDistill in (c) supports four distillation paths.}
\label{img:1}
\end{figure} 
%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

3D object detection plays a critical role in autonomous driving and robotic navigation. Generally, the popular 3D detectors can be categorized into (1) single-modality detectors that are based on LiDAR~\cite{shi2019pointrcnn,yang20203dssd,yin2021center,li20173d,shi2020points} or camera~\cite{huang2021bevdet,li2022bevdepth,brazil2019m3d,luo2021m3dssd} and (2) multi-modality detectors~\cite{qi2018frustum,liu2022bevfusion,vora2020pointpainting,wang2021pointaugmenting} that are based on both modalities. By fusing the complementary knowledge of two modalities, multi-modality detectors outperform their single-modality counterparts. Nevertheless, simultaneously processing the data of two modalities unavoidably introduces extra network designs and computational overhead. Worse still, the breakdown of any modality directly fails the detection, hindering the application of these detectors.

As a solution, some recent works introduced knowledge distillation to transfer complementary knowledge of other modalities to a single-modality detector. In~\cite{chong2021monodistill,ju2022paint,zheng2022boosting}, as illustrated in Figure \ref{img:1}(a) and \ref{img:1}(b), for a single-modality student detector, the authors first performed data transformation of different modalities to train a structurally identical teacher. The teacher was then leveraged to transfer knowledge by instructing the student to produce similar features and prediction results. In this way, the single-modality student obtains multi-modality knowledge and improves performance, without additional cost during inference.

Despite their effectiveness to transfer cross-modality knowledge, the application of existing methods is limited since the modalities of both the teacher and the student are restricted. In~\cite{chong2021monodistill}, the modalities of the teacher and student are fixed to be LiDAR and camera while in~\cite{zheng2022boosting,ju2022paint}, they are determined to be LiDAR-camera and LiDAR. However, the sensor portfolio in the field of 3D object detection results in a diverse and complex application of different detectors. With restricted modalities of both the teacher and student, these methods are difficult to be applied in more situations, \eg, the method in~\cite{chong2021monodistill} is not suitable to transfer knowledge from a camera based teacher to a LiDAR based student.

To solve the above problems, we propose a universal cross-modality knowledge distillation framework (UniDistill) that helps single-modality detectors improve performance. Our motivation is based on the observation that the detectors of different modalities adopt a similar detection paradigm in bird's-eye view (BEV), where after transforming the low-level features to BEV, a BEV encoder follows to further encode high-level features and a detection head produces response features to perform final prediction. 

UniDistill takes advantage of the similarity to construct the universal knowledge distillation framework. As in Figure \ref{img:1}(c), during training, UniDistill projects the features of both the teacher and the student detector into the unified BEV domain. Then for each ground truth bounding box, three distillation losses are calculated to transfer knowledge: (1) A feature distillation loss that transfers the semantic knowledge by aligning the low-level features of 9 crucial points. (2) A relation distillation loss that transfers the structural knowledge by aligning the relationship between the high-level features of 9 crucial points. (3) A response distillation loss that closes the prediction gap by aligning the response features in a Gaussian-like mask. Since the aligned features are commonly produced by different detectors, UniDistill easily supports LiDAR-to-camera, camera-to-LiDAR, fusion-to-LiDAR and fusion-to-camera distillation paths. Furthermore, the three losses sparsely align the foreground features to filter the effect of misaligned background information and balance between objects of different scales, improving the distillation effectiveness.

In summary, our contributions are three-fold:
\begin{itemize}
    \item We propose a universal cross-modality knowledge distillation framework (UniDistill) in the friendly BEV domain for single-modality 3D object detectors. With the transferred knowledge of different modalities, the performance of single-modality detectors is improved without additional cost during inference.
    \item Benefiting from the similar detection paradigm in BEV, UniDistill supports LiDAR-to-camera, camera-to-LiDAR, fusion-to-LiDAR and fusion-to-camera distillation paths. Moreover, three distillation losses are designed to sparsely align foreground features, filtering the effect of background information misalignment and balance between objects of different sizes.
    \item Extensive experiments on nuScenes demonstrate that UniDistill can effectively improve the mAP and NDS of student detectors by 2.0\%$\sim$3.2\%.
\end{itemize}