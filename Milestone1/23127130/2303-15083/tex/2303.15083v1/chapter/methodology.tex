\section{Methodology}

In this section, we describe our proposed UniDistill in detail. In Section \ref{sec:3.1}, we briefly introduce the similar detection paradigm of different detectors in BEV, where some low-level BEV features are obtained via view-transform and further encoded to be high-level features and response features.  In Section \ref{sec:3.2}, we depict the proposed UniDistill framework for cross-modality knowledge distillation. For a teacher and a student detector, after obtaining their features in BEV, three distillation losses aligning foreground features are calculated to perform knowledge transfer.

\subsection{Preliminary}
\label{sec:3.1}
The recent detectors adopt a similar detection paradigm in BEV, where the major difference is the approach to get the low-level BEV features. For the camera based detectors proposed in~\cite{huang2021bevdet,li2022bevdepth}, after pixel-wise depth estimation, the image features are projected from the perspective view into BEV via the view-transform operation proposed in~\cite{philion2020lift} to form low-level BEV features $\boldmath{F}_{\text{cam}}^{\text{low}}$. 

With respect to LiDAR based detectors presented in \cite{yin2021center,zhou2018voxelnet,lang2019pointpillars}, the unstructured point clouds are first distributed to regular voxels or pillars. The features of voxels or pillars are then extracted and reshaped into low-level BEV features $\boldmath{F}_{\text{ldr}}^{\text{low}}$ by concatenating the voxel features in the same column. 

In \cite{liu2022bevfusion}, the authors proposed a method to construct a LiDAR-camera based detector simply by fusing the low-level features $\boldmath{F}_{\text{ldr}}^{\text{low}}$ and $\boldmath{F}_{\text{cam}}^{\text{low}}$ of a LiDAR and camera based detector. It first concatenates $\boldmath{F}_{\text{ldr}}^{\text{low}}$ and $\boldmath{F}_{\text{cam}}^{\text{low}}$ and then processes the result with a fully convolutional network to produce the fused features $\boldmath{F}_{\text{fuse}}^{\text{low}}$.

The following steps are the same for different detectors. For a detector of any modality, a BEV encoder (BEVEnc) first takes its low-level BEV features $\boldmath{F}_{\text{mod}}^{\text{low}}$ as input to further encode the high-level features $\boldmath{F}_{\text{mod}}^{\text{high}}$:
\begin{equation}
\boldmath{F}_{\text{mod}}^{\text{high}} = \text{BEVEnc}(\boldmath{F}_{\text{mod}}^{\text{low}}),
\end{equation}
where mod is the detector modality and is in $\{\text{ldr,cam,fuse}\}$. Then a detection head (DetHead) produces the classification and regression heatmaps $\boldmath{F}_{\text{mod}}^{\text{cls}}$ and $\boldmath{F}_{\text{mod}}^{\text{reg}}$, based on which the final predictions are generated:
\begin{equation}
\boldmath{F}_{\text{mod}}^{\text{cls}}, \boldmath{F}_{\text{mod}}^{\text{reg}} = \text{DetHead}(\boldmath{F}_{\text{mod}}^{\text{high}}).
\end{equation}

Therefore, regardless of the modality, these detectors will consistently produce $\boldmath{F}^{\text{low}}$, $\boldmath{F}^{\text{high}}$, $\boldmath{F}^{\text{cls}}$ and $\boldmath{F}^{\text{reg}}$ during the procedure of detection.

\subsection{UniDistill}
\label{sec:3.2}
Taking advantage of the above similar detection paradigm in BEV, we propose a universal cross-modality knowledge distillation framework (UniDistill), which easily supports LiDAR-to-camera, camera-to-LiDAR, fusion-to-LiDAR and fusion-to-camera distillation paths. The overview of UniDistill is illustrated in Figure \ref{img:2}. During training, after transforming the low-level features of the teacher and the student of different modalities to BEV, three distillation losses are then calculated. The losses are finally combined with the original detection loss to train the student. In this way, the student can mimic the teacher to learn cross-modal knowledge and thus gain better detection results, without introducing additional cost during inference.

Denoting the modalities of the teacher and the student as MT and MS respectively, the three distillation losses can be interpreted as follows: (1) The first loss “feature distillation” transfers the semantic knowledge in the low-level BEV features $\boldmath{F}_{\text{MT}}^{\text{low}}$ to $\boldmath{F}_{\text{MS}}^{\text{low}}$ by point-wisely aligning the features of 9 crucial points of each ground truth bounding box. (2) The second loss “relation distillation” transfers the structural knowledge in the high-level BEV features $\boldmath{F}_{\text{MT}}^{\text{high}}$ to $\boldmath{F}_{\text{MS}}^{\text{high}}$ by group-wisely aligning the relationship between 9 crucial points of each ground truth bounding box. (3) The third loss “response distillation” closes the prediction gap by aligning the heatmaps $(\boldmath{F}_{\text{MS}}^{\text{cls}},\boldmath{F}_{\text{MS}}^{\text{reg}})$ with $(\boldmath{F}_{\text{MT}}^{\text{cls}},\boldmath{F}_{\text{MT}}^{\text{reg}})$ in a Gaussian-like mask for each ground truth bounding box.
\subsubsection{Feature Distillation}
\quad Since the low-level BEV features provide semantic knowledge for further process, we propose feature distillation to align $\boldmath{F}_{\text{MS}}^{\text{low}}$ with $\boldmath{F}_{\text{MT}}^{\text{low}}$. One intuitive method is completely aligning $\boldmath{F}_{\text{MS}}^{\text{low}}$ with $\boldmath{F}_{\text{MT}}^{\text{low}}$, however, the background information misalignment between different modalities will decrease the  effectiveness. Worse still, because different objects occupy areas of different sizes, it will focus more on aligning the features of large objects than small objects. 

To mitigate the above effects, in feature distillation, we only align the features of foreground objects and equally select 9 crucial points for each of them for alignment. Specifically, in BEV, the bounding box of one foreground object can be regarded as a 2D rotated box and described by the coordinates of its corners $\{(x_i,y_i)\}_{i=1}^4$. Based on the four corners, the coordinates of midpoints of 4 edges and the center of the box can be calculated. We collect these 9 points together as $\{(x_i,y_i)\}_{i=1}^9$ and regard them as crucial points for the bounding box. For each point $\bm{p}_i$=$(x_i,y_i)$, we calculate the difference between its features on $\boldmath{F}_{\text{MT}}^{\text{low}}$ and $\boldmath{F}_{\text{MS}}^{\text{low}}$ to form the feature distillation loss $\mathcal{L}_{\text{Fea}}$:
\begin{equation}
\mathcal{L}_{\text{Fea}} = (\sum_{i=1}^9|\bm{F}_{\text{MT}}^{\text{low}}(x_i, y_i)-\bm{F}_{\text{MS}}^{\text{low}}(x_i, y_i)|)/9.
\end{equation}

In addition, we note that when the performance of the teacher detector is worse than the student, \eg, the modalities of the teacher and the student are camera and LiDAR, using the vanilla feature distillation can even degrade the final performance. Inspired by the feature adaptation operation in ~\cite{romero2014fitnets,chen2017learning}, in this situation, we also introduce an adaptive layer $\text{Adapt}_{1}$, which is a one-layer convolutional network, after $\boldmath{F}_{\text{MS}}^{\text{low}}$ to produce new features $\hat{\bm{F}}_{\text{MS}}^{\text{low}}$ and calculate feature distillation instead with $\hat{\bm{F}}_{\text{MS}}^{\text{low}}$ and $\boldmath{F}_{\text{MT}}^{\text{low}}$. During inference, the adaptive layer is removed and the original low-level feature $\boldmath{F}_{\text{MS}}^{\text{low}}$ is further processed to generate predictions. Therefore, there is no modification to the structure of the student detector.
\subsubsection{Relation Distillation}
\quad The high-level features can provide knowledge about the structure of the scene, which is important for the final prediction. To transfer the structural knowledge from the teacher to the student, we propose relation distillation to align the relationship between the different parts of an object in $\bm{F}_{\text{MS}}^{\text{high}}$ and $\bm{F}_{\text{MT}}^{\text{high}}$. Specifically, for one ground truth bounding box, we also consider its 9 crucial points $\{(x_i,y_i)\}_{i=1}^9$ the same as those in feature distillation. We first gather their features on $\bm{F}_{\text{MS}}^{\text{high}}$ and calculate the relationship between them to form a relation matrix $\text{RelMat}^{\text{MS}}$ with the size of 9$\times$9:
\begin{equation}
\text{RelMat}^{\text{MS}}_{i,j} = \Phi(\bm{F}_{\text{MS}}^{\text{high}}(x_{i},y_{i}),\bm{F}_{\text{MS}}^{\text{high}}(x_j,y_{j})), 
\end{equation}
where 1$\leqslant$$i,j$$\leqslant$9 and $\Phi$ represents the cosine similarity function. With the same operation, another relation matrix $\text{RelMat}^{\text{MT}}$ can be calculated based on $\bm{F}_{\text{MT}}^{\text{high}}$ and then the relation distillation loss $\mathcal{L}_{\text{Rel}}$ is calculated to completely align $\text{RelMat}^{\text{MS}}$ with $\text{RelMat}^{\text{MT}}$:
\begin{equation}
\mathcal{L}_{\text{Rel}} = (\sum_{1\leqslant i,j \leqslant 9}|\text{RelMat}^{\text{MT}}_{i,j}-\text{RelMat}^{\text{MS}}_{i,j}|)/81.
\end{equation}

Furthermore, the same as that in feature distillation, when the teacher performs worse than student, we introduce another one-layer convolutional network as an adaptive layer $\text{Adapt}_2$ after $\bm{F}_{\text{MS}}^{\text{high}}$ to produce new features $\hat{\bm{F}}_{\text{MS}}^{\text{high}}$ and calculate relation distillation with $\hat{\bm{F}}_{\text{MS}}^{\text{high}}$ and $\bm{F}_{\text{MT}}^{\text{high}}$.

\subsubsection{Response Distillation}
\quad To make the final prediction of the student similar to the teacher, we further propose response distillation. Based on the high-level BEV features, the detection head will produce a classification heatmap $\bm{F}^{\text{cls}}\in R^{H\times W\times C}$ and a regression heatmap $\bm{F}^{\text{reg}}\in R^{H\times W\times T}$, where $H$ and $W$ are the height and width of the heatmap, $C$ is the number of classes to be predicted and $T$ is the number of regression targets. We further gather the max value of each position in $\bm{F}^{\text{cls}}$ to form a new heatmap $\bm{F}_{\text{max}}^{\text{cls}}$:
\begin{equation}
\bm{F}_{\text{max}}^{\text{cls}}(i,j)=\max_{1\leqslant k\leqslant C}\bm{F}^{\text{cls}}(i,j,k),
\end{equation}
where 1$\leqslant$$i$$\leqslant$$H$ and 1$\leqslant$$j$$\leqslant$$W$. Then $\bm{F}_{\text{max}}^{\text{cls}}$ is concatenated with $\bm{F}^{\text{reg}}$ to be the response features $\bm{F}^{\text{resp}}$. In this way, we obtain the response features of the teacher and the student, which are $\bm{F}_{\text{MT}}^{\text{resp}}$ and $\bm{F}_{\text{MS}}^{\text{resp}}$ respectively,  and calculate response distillation loss to align $\bm{F}_{\text{MS}}^{\text{resp}}$ with $\bm{F}_{\text{MT}}^{\text{resp}}$. To mitigate the effect of background information misalignment, we only align the part of response features near the foreground objects. Instead of selecting 9 crucial points, we find that the quality of the response values in $\bm{F}_{\text{MT}}^{\text{resp}}$ near the center of a ground truth bounding box is good enough to guide $\bm{F}_{\text{MS}}^{\text{resp}}$. Therefore, for one ground truth bounding box, we generate a Gaussian-like mask with the same method in \cite{chong2021monodistill}, gather the response values inside the mask and calculate response distillation loss $\mathcal{L}_{\text{Resp}}$ to align them:
\begin{equation}
\mathcal{L}_{\text{Resp}} = \sum_{1\leqslant i\leqslant \text{H},1\leqslant j\leqslant \text{W}}|\bm{F}_{\text{MT}}^{\text{resp}}(i,j)-\bm{F}_{\text{MS}}^{\text{resp}}(i,j)|\times \text{Mask}(i,j),
\end{equation}
where $\text{Mask}$ is the calculated Gaussian-like mask and only the area near the ground truth bounding box is non-zero.
\begin{table*}[ht]\small
\centering
\caption{Performance analysis of UniDistill in four distillation paths on the nuScenes test dataset. “*” indicates re-implementation on our new student detector. “L” and “C” represent the LiDAR and camera.}
\resizebox{0.92\width}{!}{
\begin{tabular}{c|cc|ccccccc}
\shline
Method      & Modality & \begin{tabular}[c]{@{}c@{}}Teacher\\ Modality\end{tabular} & mAP~$\uparrow$ & NDS~$\uparrow$                           & mATE~$\downarrow$                                & mASE~$\downarrow$                                & mAOE~$\downarrow$                                & mAVE~$\downarrow$                                  & mAAE~$\downarrow$                                \\ \shline
CVCNet\cite{chen2020every}      & L        & -                                                          & 55.8                & 64.2                & 30.0                & 24.8                & 43.1                & 26.9                  & 11.9                \\
Guided 3DOD\cite{fazlali2022versatile} & L        & -                                                          & 60.9                & 67.3                & 28.8                & 24.5                & 40.0                & 25.3                  & 12.8                \\
AFDetV2\cite{hu2022afdetv2}    & L        & -                                                          & 62.4                & 68.5                & 25.7                & 23.4                & 34.1                & 29.9                  & 13.7                \\
S2M2-SSD*\cite{zheng2022boosting}    & L        & L+C                                                        & 63.6                & 69.6                & 25.4                & 23.9                & 34.3                & 25.7                  & 12.8                \\ \shline
UniDistill  & L+C      & -                                                          & 65.4                & 70.6                & 25.1                & 23.8                & 32.5                & 25.6                 & 12.8                \\ \shline
UniDistill  & L        & -                                                          & 61.4                & 67.8                & 26.8                & 25.1                & 33.6                & 27.8                  & 14.8                \\
\rowcolor{black!5}UniDistill  & L        & C                                                          & \textbf{63.4(+2.0)} & \textbf{69.8(+2.0)} & \textbf{24.9(-1.9)} & \textbf{23.7(-1.4)} & \textbf{32.1(-1.5)} & \textbf{24.7(-3.1)}   & \textbf{13.1(-1.7)} \\
\rowcolor{black!10}UniDistill  & L        & L+C                                                        & \textbf{63.9(+2.5)} & \textbf{70.1(+2.3)} & \textbf{25.0(-1.8)} & \textbf{23.8(-1.3)} & \textbf{32.8(-0.8)} & \textbf{24.5(-3.3)}   & \textbf{12.7(-2.1)} \\ \shline
UniDistill  & C        & -                                                          & 26.4                & 36.1                & 69.7                & 26.6                & 55.8                & 117.3                 & 17.8                \\
\rowcolor{black!5}UniDistill  & C        & L                                                          & \textbf{28.9(+2.5)} & \textbf{38.4(+2.3)} & \textbf{65.9(-3.8)} & \textbf{25.9(-0.7)} & \textbf{51.4(-4.4)} & \textbf{106.4(-10.9)} & \textbf{17.0(-0.8)} \\
\rowcolor{black!10}UniDistill  & C        & L+C                                                        & \textbf{29.6(+3.2)} & \textbf{39.3(+3.2)} & \textbf{63.7(-6.0)} & \textbf{25.7(-0.9)} & \textbf{49.2(-6.6)} & \textbf{108.4(-8.9)}  & \textbf{16.7(-1.1)} \\ \shline
\end{tabular}
}
\label{tab:1}
\end{table*}

\subsubsection{Total Objectives}
\quad After being calculated for each ground truth bounding box, every distillation loss is then averaged over all bounding boxes. Finally, we combine the detection loss $\mathcal{L}_{\text{Det}}$ of the student with the distillation losses as the total loss $\mathcal{L}_{\text{Total}}$:
\begin{equation}
\mathcal{L}_{\text{Total}} = \mathcal{L}_{\text{Det}}+\lambda_1\cdot\mathcal{L}_{\text{Fea}}+\lambda_2\cdot\mathcal{L}_{\text{Rel}}+\lambda_3\cdot\mathcal{L}_{\text{Resp}},
\end{equation}
where $\lambda_1$, $\lambda_2$ and $\lambda_3$ are hyperparameters used to balance the scale of different losses. The student is then optimized by $\mathcal{L}_{\text{Total}}$, achieving better performance.
