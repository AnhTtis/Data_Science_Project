@book{akhiezer2013theory,
  title={Theory of Linear Operators in Hilbert Space},
  author={Akhiezer, N.I. and Glazman, I.M.},
  isbn={9780486318653},
  series={Dover Books on Mathematics},
  year={2013},
  publisher={Dover Publications}
}

@article{Altschuler2021KernelAO,
  title={Kernel approximation on algebraic varieties},
  author={Jason Altschuler and Pablo A. Parrilo},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.02755}
}




@article{ARAVKIN2012mse,
title = "On the MSE Properties of Empirical Bayes Methods for Sparse Estimation",
journal = "IFAC Proceedings Volumes",
volume = "45",
number = "16",
pages = "965 - 970",
year = "2012",
note = "16th IFAC Symposium on System Identification",
issn = "1474-6670",
author = "A. Aravkin and J.V. Burke and A. Chiuso and G. Pillonetto",
abstract = "Popular convex approaches for sparse estimation such as Lasso and Multiple Kernel Learning (MKL) can be derived in a Bayesian setting, starting from a particular stochastic model. In problems where groups of variables have to be estimated, we show that the same probabilistic model, under a suitable marginalization, leads to a different non-convex estimator where hyperparameters are optimized. Theoretical arguments, independent of the correctness of the priors entering the sparse model, are included to clarify the advantages of our non-convex technique in comparison with MKL and the group version of Lasso under assumption of orthogonal regressors."
}


@misc{arcari2020meta,
      title={Meta Learning MPC using Finite-Dimensional Gaussian Process Approximations}, 
      author={Elena Arcari and Andrea Carron and Melanie N. Zeilinger},
      year={2020},
      eprint={2008.05984},
      archivePrefix={arXiv},
      primaryClass={eess.SY}
}

@INPROCEEDINGS{arcari2021,
  author={Arcari, Elena and Scampicchio, Anna and Carron, Andrea and Zeilinger, Melanie N.},
  booktitle={2021 60th IEEE Conference on Decision and Control (CDC)}, 
  title={Bayesian multi-task learning using finite-dimensional models: A comparative study}, 
  year={2021},
  volume={},
  number={},
  pages={2218-2225},
  doi={10.1109/CDC45484.2021.9683483}}



@article{andersenchen2020fullsemi,
author = {Andersen, Martin S. and Chen, Tianshi},
title = {Smoothing Splines and Rank Structured Matrices: Revisiting the Spline Kernel},
journal = {SIAM Journal on Matrix Analysis and Applications},
volume = {41},
number = {2},
pages = {389-412},
year = {2020},
doi = {10.1137/19M1267349}
}

@article{aronszajn50reproducing,
  author = {Aronszajn, N.},
  description = {CiteULike: Theory of reproducing kernels},
  interhash = {5f0e5e40a1512aa0b21f287a39b81b31},
  intrahash = {024c71f807cbf95a8fb6b934c01f4919},
  journal = {Transactions of the American Mathematical Society},
  keywords = {kernel},
  number = 3,
  pages = {337--404},
  timestamp = {2010-10-07T14:13:58.000+0200},
  title = {Theory of Reproducing Kernels},
  volume = 68,
  year = 1950
}

@article{bach2012,
author = {Bach, Francis},
year = {2012},
month = {August},
pages = {},
title = {Sharp analysis of low-rank kernel matrix approximations},
volume = {30},
journal = {Journal of Machine Learning Research}
}

@article{Bakker:2003,
 author = {B. Bakker and T. Heskes},
 title = {Task clustering and gating for {B}ayesian multitask learning},
 journal = {J. Mach. Learn. Res.},
 volume = {4},
 year = {2003},
 pages = {83--99}
}

@article{Baxter:1997,
 author = {J. Baxter},
 title = {A {B}ayesian/{I}nformation theoretic model of learning to learn via multiple task sampling},
 journal = {Machine Learning},
 volume = {28},
 year = {1997},
 pages = {7--39}
}

@article{bentkus2004,
author = {Vidmantas Bentkus},
title = {{On Hoeffding’s inequalities}},
volume = {32},
journal = {The Annals of Probability},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {1650 -- 1673},
keywords = {bounded differences and random variables, bounds for tail probabilities, Hoeffding’s inequalities, Inequalities‎, martingale, Probabilities of large deviations},
year = {2004}
}






@article{benzi_golub_liesen_2005, title={Numerical solution of saddle point problems}, volume={14}, DOI={10.1017/S0962492904000212}, journal={Acta Numerica}, publisher={Cambridge University Press}, author={Benzi, Michele and Golub, Gene H. and Liesen, Jörg}, year={2005}, pages={1?137}}

@book{berberian1961introduction,
  title={Introduction to Hilbert Space},
  author={Berberian, S.K.},
  year={1961},
  publisher={Oxford University Press}
}





@book{billingsley,
Title = {Probability and Measure},
  Author = {Patrick Billingsley},
  Publisher = {John Wiley and Sons},
  Year= {2012},
  Edition = {Anniversary}
}


@INPROCEEDINGS{bobade2017,
  author={Bobade, Parag and Majumdar, Suprotim and Pereira, Savio and Kurdila, Andrew J. and Ferris, John B.},
  booktitle={2017 American Control Conference (ACC)}, 
  title={Adaptive estimation in reproducing kernel {H}ilbert spaces}, 
  year={2017},
  volume={},
  number={},
  pages={5678-5683},
  doi={10.23919/ACC.2017.7963839}}

@Inbook{Boucheron2004,
author="Boucheron, St{\'e}phane
and Lugosi, G{\'a}bor
and Bousquet, Olivier",
editor="Bousquet, Olivier
and von Luxburg, Ulrike
and R{\"a}tsch, Gunnar",
title="Concentration Inequalities",
bookTitle="Advanced Lectures on Machine Learning: ML Summer Schools 2003, Canberra, Australia, February 2 - 14, 2003, T{\"u}bingen, Germany, August 4 - 16, 2003, Revised Lectures",
year="2004",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="208--240",
abstract="Concentration inequalities deal with deviations of functions of independent random variables from their expectation. In the last decade new tools have been introduced making it possible to establish simple and powerful inequalities. These inequalities are at the heart of the mathematical analysis of various problems in machine learning and made it possible to derive new efficient algorithms. This text attempts to summarize some of the basic tools.",
isbn="978-3-540-28650-9"
}







@misc{boustati2020nonlinear,
      title={Non-linear Multitask Learning with Deep Gaussian Processes}, 
      author={Ayman Boustati and Theodoros Damoulas and Richard S. Savage},
      year={2020},
      eprint={1905.12407},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{Caruana:1997,
 author = {R. Caruana},
 title = {Multitask Learning},
 journal = {Mach. Learn.},
 volume = {28},
 number = {1},
 year = {1997},
 pages = {41--75},
 numpages = {35}
}

@article{Chen2014SystemIV,
  title={System Identification Via Sparse Multiple Kernel-Based Regularization Using Sequential Convex Optimization Techniques},
  author={Tianshi Chen and Martin S. Andersen and L. Ljung and A. Chiuso and G. Pillonetto},
  journal={IEEE Transactions on Automatic Control},
  year={2014},
  volume={59},
  pages={2933-2945}
}

@article{TLImpAlg2013,
    author={T. Chen and L. Ljung},
    title={Implementation of algorithms for tuning parameters in regularized least squares problems in system identification},
    journal={Automatica},
      volume={49},
  number={7},
  pages={2213--2220},
  year={2013},
}



@misc{chen2020efficient,
      title={Efficient Projection-Free Algorithms for Saddle Point Problems}, 
      author={Cheng Chen and Luo Luo and Weinan Zhang and Yong Yu},
      year={2020},
      eprint={2010.11737},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article {Champion22445,
	author = {Champion, Kathleen and Lusch, Bethany and Kutz, J. Nathan and Brunton, Steven L.},
	title = {Data-driven discovery of coordinates and governing equations},
	volume = {116},
	number = {45},
	pages = {22445--22451},
	year = {2019},
	doi = {10.1073/pnas.1906995116},
	publisher = {National Academy of Sciences},
	abstract = {Governing equations are essential to the study of physical systems, providing models that can generalize to predict previously unseen behaviors. There are many systems of interest across disciplines where large quantities of data have been collected, but the underlying governing equations remain unknown. This work introduces an approach to discover governing models from data. The proposed method addresses a key limitation of prior approaches by simultaneously discovering coordinates that admit a parsimonious dynamical model. Developing parsimonious and interpretable governing models has the potential to transform our understanding of complex systems, including in neuroscience, biology, and climate science.The discovery of governing equations from scientific data has the potential to transform data-rich fields that lack well-characterized quantitative descriptions. Advances in sparse regression are currently enabling the tractable identification of both the structure and parameters of a nonlinear dynamical system from data. The resulting models have the fewest terms necessary to describe the dynamics, balancing model complexity with descriptive ability, and thus promoting interpretability and generalizability. This provides an algorithmic approach to Occam{\textquoteright}s razor for model discovery. However, this approach fundamentally relies on an effective coordinate system in which the dynamics have a simple representation. In this work, we design a custom deep autoencoder network to discover a coordinate transformation into a reduced space where the dynamics may be sparsely represented. Thus, we simultaneously learn the governing equations and the associated coordinate system. We demonstrate this approach on several example high-dimensional systems with low-dimensional behavior. The resulting modeling framework combines the strengths of deep neural networks for flexible representation and sparse identification of nonlinear dynamics (SINDy) for parsimonious models. This method places the discovery of coordinates and models on an equal footing.},
	issn = {0027-8424},
	journal = {Proceedings of the National Academy of Sciences}
}

@ARTICLE{Cucker02,
    author = {Felipe Cucker and Steve Smale},
    title = {On the mathematical foundations of learning},
    journal = {Bulletin of the American Mathematical Society},
    year = {2002},
    volume = {39},
    pages = {1--49}
}


@book{cuckerzhou2007, place={Cambridge}, series={Cambridge Monographs on Applied and Computational Mathematics}, title={Learning Theory: An Approximation Theory Viewpoint}, DOI={10.1017/CBO9780511618796}, publisher={Cambridge University Press}, author={Cucker, Felipe and Zhou, Ding Xuan}, year={2007}, collection={Cambridge Monographs on Applied and Computational Mathematics}}

@inbook{dedieu_2001, place={Cambridge}, series={London Mathematical Society Lecture Note Series}, title={Newton's method and some complexity aspects of the zero-finding problem}, DOI={10.1017/CBO9781107360198.004}, booktitle={Foundations of Computational Mathematics}, publisher={Cambridge University Press}, author={Dedieu, J.-P.}, editor={Devore, Ronald and Iserles, Arieh and Süli, EndreEditors}, year={2001}, pages={45?68}, collection={London Mathematical Society Lecture Note Series}}



@inproceedings{Engl1996,
  title={Regularization of Inverse Problems},
  author={H. Engl and M. Hanke and A. Neubauer},
  year={1996}
}


@inproceedings{Evgeniou2004RegularizedML,
  title={Regularized multi--task learning},
  author={Theodoros Evgeniou and M. Pontil},
  booktitle={KDD '04},
  year={2004}
}

@article{Evgeniou:2005,
 author = {T. Evgeniou and C.A. Micchelli and M. Pontil},
 title = {Learning Multiple Tasks with Kernel Methods},
 journal = {J. Mach. Learn. Res.},
 volume = {6},
 month = {December},
 year = {2005},
 pages = {615--637}
}

@article{Evgeniou2000RegularizationNA,
  title={Regularization Networks and Support Vector Machines},
  author={Theodoros Evgeniou and Massimiliano Pontil and Tomaso A. Poggio},
  journal={Advances in Computational Mathematics},
  year={2000},
  volume={13},
  pages={1-50}
}


@misc{finn2017modelagnostic,
      title={Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks}, 
      author={Chelsea Finn and Pieter Abbeel and Sergey Levine},
      year={2017},
      eprint={1703.03400},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Fourure2017,
title = "Multi-task, multi-domain learning: application to semantic segmentation and pose regression",
journal = "Neurocomputing",
volume = "251",
pages = "68 - 80",
year = "2017",
author = "D. Fourure and R. Emonet and E. Fromont and D. Muselet and N. Neverova and A. Tremeau and C. Wolf"
}


@InProceedings{ghadikolaei19a,
  title = 	 {Learning and Data Selection in Big Datasets},
  author =       {Ghadikolaei, Hossein Shokri and Ghauch, Hadi and Fischione, Carlo and Skoglund, Mikael},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2191--2200},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {June},
  publisher =    {PMLR},
  abstract = 	 {Finding a dataset of minimal cardinality to characterize the optimal parameters of a model is of paramount importance in machine learning and distributed optimization over a network. This paper investigates the compressibility of large datasets. More specifically, we propose a framework that jointly learns the input-output mapping as well as the most representative samples of the dataset (sufficient dataset). Our analytical results show that the cardinality of the sufficient dataset increases sub-linearly with respect to the original dataset size. Numerical evaluations of real datasets reveal a large compressibility, up to 95%, without a noticeable drop in the learnability performance, measured by the generalization error.}
}





@book{gilks1995markov,
  added-at = {2014-03-16T11:12:07.000+0100},
  author = {Gilks, W.R. and Richardson, S. and Spiegelhalter, D.},
  isbn = {9780412055515},
  keywords = {MCMC reference},
  lccn = {98033429},
  publisher = {Taylor \& Francis},
  series = {Chapman \& Hall/CRC Interdisciplinary Statistics},
  timestamp = {2014-03-16T11:12:07.000+0100},
  title = {Markov Chain Monte Carlo in Practice},
  year = 1995
}



@article{Grossi_2017,
	doi = {10.1371/journal.pone.0169663},
	year = 2017,
	month = {January},
	publisher = {Public Library of Science ({PLoS})},
	volume = {12},
	number = {1},
	pages = {e0169663},
	author = {Giuliano Grossi and Raffaella Lanzarotti and Jianyi Lin},
	editor = {Dewen Hu},
	title = {Orthogonal Procrustes Analysis for Dictionary Learning in Sparse Linear Representation},
	journal = {{PLOS} {ONE}}
}

@article{Guo2013ConcentrationEF,
  title={Concentration estimates for learning with unbounded sampling},
  author={Zheng-Chu Guo and Ding-Xuan Zhou},
  journal={Advances in Computational Mathematics},
  year={2013},
  volume={38},
  pages={207-223}
}




@inproceedings{Hastie2018BestSF,
  title={Best Subset, Forward Stepwise, or Lasso? Analysis and Recommendations Based on Extensive Comparisons},
  author={T. Hastie and R. Tibshirani},
  year={2018}
}

@article{Hewing2020,
author = {Hewing, Lukas and Wabersich, Kim P. and Menner, Marcel and Zeilinger, Melanie N.},
title = {Learning-Based Model Predictive Control: Toward Safe Learning in Control},
journal = {Annual Review of Control, Robotics, and Autonomous Systems},
volume = {3},
number = {1},
pages = {269-296},
year = {2020},
    abstract = { Recent successes in the field of machine learning, as well as the availability of increased sensing and computational capabilities in modern control systems, have led to a growing interest in learning and data-driven control techniques. Model predictive control (MPC), as the prime methodology for constrained control, offers a significant opportunity to exploit the abundance of data in a reliable manner, particularly while taking safety constraints into account. This review aims at summarizing and categorizing previous research on learning-based MPC, i.e., the integration or combination of MPC with learning methods, for which we consider three main categories. Most of the research addresses learning for automatic improvement of the prediction model from recorded data. There is, however, also an increasing interest in techniques to infer the parameterization of the MPC controller, i.e., the cost and constraints, that lead to the best closed-loop performance. Finally, we discuss concepts that leverage MPC to augment learning-based controllers with constraint satisfaction properties. }
}






@article{Kimeldorf1971,
  title={Some results on Tchebycheffian spline functions},
  author={G. Kimeldorf and G. Wahba},
  journal={Journal of Mathematical Analysis and Applications},
  year={1971},
  volume={33},
  pages={82-95}
}

@inproceedings{ADAM,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015}
}

@article{Krause2011,
author = {Krause, Andreas and Guestrin, Carlos},
title = {Submodularity and Its Applications in Optimized Information Gathering},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
abstract = {Where should we place sensors to efficiently monitor natural drinking water resources for contamination? Which blogs should we read to learn about the biggest stories on the Web? These problems share a fundamental challenge: How can we obtain the most useful information about the state of the world, at minimum cost?Such information gathering, or active learning, problems are typically NP-hard, and were commonly addressed using heuristics without theoretical guarantees about the solution quality. In this article, we describe algorithms which efficiently find provably near-optimal solutions to large, complex information gathering problems. Our algorithms exploit submodularity, an intuitive notion of diminishing returns common to many sensing problems: the more sensors we have already deployed, the less we learn by placing another sensor. In addition to identifying the most informative sensing locations, our algorithms can handle more challenging settings, where sensors need to be able to reliably communicate over lossy links, where mobile robots are used for collecting data, or where solutions need to be robust against adversaries and sensor failures.We also present results applying our algorithms to several real-world sensing tasks, including environmental monitoring using robotic sensors, activity recognition using a built sensing chair, a sensor placement challenge, and deciding which blogs to read on the Web.},
journal = {ACM Transactions on Intelligent Systems and Technology},
month = {July},
articleno = {32},
numpages = {20},
keywords = {sensor networks, submodular functions, Information gathering, environmental monitoring, information overload, blogs, computational sustainability, active learning}
}



@inproceedings{Kulis2006,
author = {Kulis, Brian and Sustik, M\'{a}ty\'{a}s and Dhillon, Inderjit},
title = {Learning Low-Rank Kernel Matrices},
year = {2006},
isbn = {1595933832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Kernel learning plays an important role in many machine learning tasks. However, algorithms for learning a kernel matrix often scale poorly, with running times that are cubic in the number of data points. In this paper, we propose efficient algorithms for learning low-rank kernel matrices; our algorithms scale linearly in the number of data points and quadratically in the rank of the kernel. We introduce and employ Bregman matrix divergences for rank-deficient matrices---these divergences are natural for our problem since they preserve the rank as well as positive semi-definiteness of the kernel matrix. Special cases of our framework yield faster algorithms for various existing kernel learning problems. Experimental results demonstrate the effectiveness of our algorithms in learning both low-rank and full-rank kernels.},
booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
pages = {505–512},
numpages = {8},
location = {Pittsburgh, Pennsylvania, USA},
series = {ICML '06}
}



@article{lazaro2010sparse,
  title={Sparse spectrum {G}aussian process regression},
  author={L{\'a}zaro-Gredilla, Miguel and Qui{\~n}onero-Candela, Joaquin and Rasmussen, Carl Edward and Figueiras-Vidal, An{\'\i}bal R},
  journal={The Journal of Machine Learning Research},
  volume={11},
  pages={1865--1881},
  year={2010},
  publisher={JMLR. org}
}


@article{Ling2019,
title = "Improving person re-identification by multi-task learning",
journal = "Neurocomputing",
volume = "347",
pages = "109 - 118",
year = "2019",
author = "H. Ling and Z. Wang and P. Li and Y. Shi and J. Chen and F. Zou"
}


@ARTICLE{Liu2019,
author={Z. {Liu} and B. {Huang} and Y. {Cui} and Y. {Xu} and B. {Zhang} and L. {Zhu} and Y. {Wang} and L. {Jin} and D. {Wu}},
journal={IEEE Access},
title={Multi-Task Deep Learning With Dynamic Programming for Embryo Early Development Stage Classification From Time-Lapse Videos},
year={2019},
volume={7},
number={},
pages={122153-122163}
}









@article{Ljung2020,
author = {Lennart Ljung and Tianshi Chen and Biqiang Mu},
title = {A shift in paradigm for system identification},
journal = {International Journal of Control},
volume = {93},
number = {2},
pages = {173-180},
year  = {2020},
publisher = {Taylor & Francis},
doi = {10.1080/00207179.2019.1578407}
}




@inproceedings{lu2019blockaltnonconvminmax,
title = "Block Alternating Optimization for Non-convex Min-max Problems: Algorithms and Applications in Signal Processing and Communications",
abstract = "The min-max problem, also known as the saddle point problem, can be used to formulate a wide range of applications in signal processing and wireless communications. However, existing optimization theory and methods, which mostly deal with problems with certain convex-concave structure, are not applicable for the aforementioned applications, which oftentimes involve non-convexity. In this work, we consider a general block-wise one-sided non-convex min-max problem, in which the minimization problem consists of multiple blocks and is non-convex, while the maximization problem is (strongly) concave. We propose two simple algorithms, which alternatingly perform one gradient descent-type step for each minimization block and one gradient ascent-type step for the maximization problem. For the first time, we show that such simple alternating min-max algorithms converge to first-order stationary solutions. We conduct numerical tests on a robust learning problem, and a wireless communication problem in the presence of jammers, to validate the efficiency of the proposed algorithms.",
author = "Songtao Lu and Ioannis Tsaknakis and Mingyi Hong",
year = "2019",
month = "May",
doi = "10.1109/ICASSP.2019.8683795",
language = "English (US)",
series = "ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",
publisher = "Institute of Electrical and Electronics Engineers Inc.",
pages = "4754--4758",
booktitle = "2019 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2019 - Proceedings",
note = "44th IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2019 ; Conference date: 12-05-2019 Through 17-05-2019",
}


@Inbook{MacKay1992,
author="MacKay, David J. C.",
editor="Smith, C. Ray
and Erickson, Gary J.
and Neudorfer, Paul O.",
title="Bayesian Interpolation",
bookTitle="Maximum Entropy and Bayesian Methods: Seattle, 1991",
year="1992",
publisher="Springer Netherlands",
address="Dordrecht",
pages="39--66",
abstract="Although Bayesian analysis has been in use since Laplace, the Bayesian method of model-comparison has only recently been developed in depth. In this paper, the Bayesian approach to regularisation and model-comparison is demonstrated by studying the inference problem of interpolating noisy data. The concepts and methods described are quite general and can be applied to many other data modelling problems. Regularising constants are set by examining their posterior probability distribution. Alternative regularisers (priors) and alternative basis sets are objectively compared by evaluating the evidence for them. `Occam's razor' is automatically embodied by this process. The way in which Bayes infers the values of regularising constants and noise levels has an elegant interpretation in terms of the effective number of parameters determined by the data set. This framework is due to Gull and Skilling.",
isbn="978-94-017-2219-3",
doi="10.1007/978-94-017-2219-3_3"
}


@article{mackay1992b,
    author = {MacKay, David J. C.},
    title = "{Information-Based Objective Functions for Active Data Selection}",
    journal = {Neural Computation},
    volume = {4},
    number = {4},
    pages = {590-604},
    year = {1992},
    month = {July},
    abstract = "{Learning can be made more efficient if we can actively select particularly salient data points. Within a Bayesian learning framework, objective functions are discussed that measure the expected informativeness of candidate measurements. Three alternative specifications of what we want to gain information about lead to three different criteria for data selection. All these criteria depend on the assumption that the hypothesis space is correct, which may prove to be their main weakness.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1992.4.4.590}
}



@inproceedings{Maclaurin2014FireflyMC,
  title={Firefly {M}onte {C}arlo: {E}xact {MCMC} with Subsets of Data},
  author={Dougal Maclaurin and Ryan P. Adams},
  booktitle={UAI},
  year={2014}
}





@MISC{Magni98bayesianfunction,
    author = {Paolo Magni and Riccardo Bellazzi and Giuseppe De Nicolao},
    title = {Bayesian function learning using MCMC methods},
    year = {1998}
}


@article{Maurer2016,
 author = {A. Maurer and M. Pontil and B. {Romera-Paredes}},
 title = {The Benefit of Multitask Representation Learning},
 journal = {J. Mach. Learn. Res.},
 issue_date = {January 2016},
 volume = {17},
 number = {1},
 year = {2016},
 pages = {2853--2884}
 }


@inproceedings{mcintire2016sparsegp,
author = {McIntire, Mitchell and Ratner, Daniel and Ermon, Stefano},
title = {Sparse Gaussian Processes for Bayesian Optimization},
year = {2016},
isbn = {9780996643115},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Bayesian optimization schemes often rely on Gaussian processes (GP). GP models are very flexible, but are known to scale poorly with the number of training points. While several efficient sparse GP models are known, they have limitations when applied in optimization settings.We propose a novel Bayesian optimization framework that uses sparse online Gaussian processes. We introduce a new updating scheme for the online GP that accounts for our preference during optimization for regions with better performance. We apply this method to optimize the performance of a free-electron laser, and demonstrate empirically that the weighted updating scheme leads to substantial improvements to performance in optimization.},
booktitle = {Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence},
pages = {517?526},
numpages = {10},
location = {Jersey City, New Jersey, USA},
series = {UAI'16}
}

@article{Micchelli:2005,
 author = {C.A. Micchelli and M. Pontil},
 title = {On Learning Vector-Valued Functions},
 journal = {Neural Comput.},
 issue_date = {January 2005},
 volume = {17},
 number = {1},
 year = {2005},
 pages = {177--204}
}



@article{Naik1992MetaneuralNT,
  title={Meta-neural networks that learn by learning},
  author={Devang K. Naik and R. Mammone},
  journal={[Proceedings 1992] IJCNN International Joint Conference on Neural Networks},
  year={1992},
  volume={1},
  pages={437-442 vol.1}
}

@article{Niyogi99,
    author = {Partha Niyogi and Federico Girosi},
    title = {Generalization bounds for function approximation from Scattered Noisy Data},
    journal = {Advances in Computational Mathematics},
    volume = {10},
    number = {1},
    year = {1999}
}

@article{Pillonetto2010,
title = {A new kernel-based approach for linear system identification},
journal = {Automatica},
volume = {46},
number = {1},
pages = {81-93},
year = {2010},
issn = {0005-1098},
author = {Gianluigi Pillonetto and Giuseppe {De Nicolao}},
keywords = {Linear system identification, Kernel-based methods, Bayesian estimation, Regularization, Gaussian processes, Robust identification, Stochastic embedding},
abstract = {This paper describes a new kernel-based approach for linear system identification of stable systems. We model the impulse response as the realization of a Gaussian process whose statistics, differently from previously adopted priors, include information not only on smoothness but also on BIBO-stability. The associated autocovariance defines what we call a stable spline kernel. The corresponding minimum variance estimate belongs to a reproducing kernel Hilbert space which is spectrally characterized. Compared to parametric identification techniques, the impulse response of the system is searched for within an infinite-dimensional space, dense in the space of continuous functions. Overparametrization is avoided by tuning few hyperparameters via marginal likelihood maximization. The proposed approach may prove particularly useful in the context of robust identification in order to obtain reduced order models by exploiting a two-step procedure that projects the nonparametric estimate onto the space of nominal models. The continuous-time derivation immediately extends to the discrete-time case. On several continuous- and discrete-time benchmarks taken from the literature the proposed approach compares very favorably with the existing parametric and nonparametric techniques.}
}


@article{PillPAMI,
    author="G. Pillonetto and F. Dinuzzo and G. {De Nicolao}",
    title="{B}ayesian on-line multi-task learning of {G}aussian processes",
    journal="IEEE Trans. on Pattern Analysis and Machine Intelligence",
    year="2010",
    pages="193-205",
    volume="32",
    number="2"
}

@article{Pillonetto2014,
title = {Kernel methods in system identification, machine learning and function estimation: A survey},
journal = {Automatica},
volume = {50},
number = {3},
pages = {657-682},
year = {2014},
issn = {0005-1098},
author = {Gianluigi Pillonetto and Francesco Dinuzzo and Tianshi Chen and Giuseppe {De Nicolao} and Lennart Ljung},
keywords = {Linear system identification, Prediction error methods, Model complexity selection, Bias-variance trade-off, Kernel-based regularization, Inverse problems, Reproducing kernel Hilbert spaces, Gaussian processes},
abstract = {Most of the currently used techniques for linear system identification are based on classical estimation paradigms coming from mathematical statistics. In particular, maximum likelihood and prediction error methods represent the mainstream approaches to identification of linear dynamic systems, with a long history of theoretical and algorithmic contributions. Parallel to this, in the machine learning community alternative techniques have been developed. Until recently, there has been little contact between these two worlds. The first aim of this survey is to make accessible to the control community the key mathematical tools and concepts as well as the computational aspects underpinning these learning techniques. In particular, we focus on kernel-based regularization and its connections with reproducing kernel Hilbert spaces and Bayesian estimation of Gaussian processes. The second aim is to demonstrate that learning techniques tailored to the specific features of dynamic systems may outperform conventional parametric approaches for identification of stable linear systems.}
}


@ARTICLE{Qui2005,
    author = {J. {Qui\~{n}onero-Candela} and C.E. Rasmussen},
    title = {A unifying view of sparse approximate {G}aussian process regression},
    journal = {J. of Machine Learning Research},
    year = {2005},
    volume = {6},
    pages = {1939--1959}
}

@book{Rasmussen,
    Author = {C.E. Rasmussen and C.K.I. Williams},
    Publisher = {The MIT Press},
    Title = {{G}aussian Processes for Machine Learning},
    Year = {2006}}

@BOOK{Rudin,
  author =       {Rudin,~W.},
  title =        {Real and Complex Analysis},
  publisher =    {McGraw-Hill },
  year =         {1987},
  address =      {Singapore},
}


@misc{salimbeni2017doubly,
      title={Doubly Stochastic Variational Inference for Deep Gaussian Processes}, 
      author={Hugh Salimbeni and Marc Deisenroth},
      year={2017},
      eprint={1705.08933},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{sutherland2015,
author = {Sutherland, Danica J. and Schneider, Jeff},
title = {On the Error of Random {F}ourier Features},
year = {2015},
isbn = {9780996643108},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Kernel methods give powerful, flexible, and theoretically grounded approaches to solving many problems in machine learning. The standard approach, however, requires pairwise evaluations of a kernel function, which can lead to scalability issues for very large datasets. Rahimi and Recht (2007) suggested a popular approach to handling this problem, known as random Fourier features. The quality of this approximation, however, is not well understood. We improve the uniform error bound of that paper, as well as giving novel understandings of the embedding's variance, approximation error, and use in some machine learning methods. We also point out that surprisingly, of the two main variants of those features, the more widely used is strictly higher-variance for the Gaussian kernel and has worse bounds.},
booktitle = {Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence},
pages = {862–871},
numpages = {10},
location = {Amsterdam, Netherlands},
series = {UAI'15}
}



@book{Scholkopf2001,
author = {Sch\"olkopf, Bernhard and Smola, Alexander J.},
title = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},
year = {2001},
isbn = {0262194759},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {From the Publisher:In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs -kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics. Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.}
}

@inproceedings{Settles2009ActiveLL,
  title={Active Learning Literature Survey},
  author={Burr Settles},
  year={2009}
}



@article{Smundsson2018MetaRL,
  title={Meta Reinforcement Learning with Latent Variable Gaussian Processes},
  author={Steind{\'o}r S{\ae}mundsson and Katja Hofmann and M. Deisenroth},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.07551}
}

@book{shalev-shwartz_ben-david_2014, place={Cambridge}, title={Understanding Machine Learning: From Theory to Algorithms}, DOI={10.1017/CBO9781107298019}, publisher={Cambridge University Press}, author={Shalev-Shwartz, Shai and Ben-David, Shai}, year={2014}}

@misc{schmidhuber87,
author = {J. Schmidhuber},
title =  {{Evolutionary principles in self-referential learning, or
	  on learning how to learn: the meta-meta-... hook. Diploma thesis, 
	  Inst. f. Inf., Tech. Univ. Munich}},
	      note = {http://www.idsia.ch/\~{ }juergen/diploma.html},
year   = {1987}
}

@InProceedings{scholkopfGen2001,
author="Sch{\"o}lkopf, Bernhard
and Herbrich, Ralf
and Smola, Alex J.",
editor="Helmbold, David
and Williamson, Bob",
title="A Generalized Representer Theorem",
booktitle="Computational Learning Theory",
year="2001",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="416--426",
abstract="Wahba's classical representer theorem states that the solutions of certain risk minimization problems involving an empirical risk term and a quadratic regularizer can be written as expansions in terms of the training examples. We generalize the theorem to a larger class of regularizers and empirical risk terms, and give a self-contained proof utilizing the feature space associated with a kernel. The result shows that a wide range of problems have optimal solutions that live in the finite dimensional span of the training examples mapped into feature space, thus enabling us to carry out kernel algorithms independent of the (potentially infinite) dimensionality of the feature space.",
isbn="978-3-540-44581-4"
}






@article{smaleshannonI,
author = {Smale, Steve and Zhou, Ding-Xuan},
year = {2004},
month = {July},
pages = {279-306},
title = {Shannon sampling and function reconstruction from point values},
volume = {41},
journal = {Bulletin of The American Mathematical Society},
doi = {10.1090/S0273-0979-04-01025-0}
}

@article{smaleshannonII,
title = {Shannon sampling {II}: Connections to learning theory},
journal = {Applied and Computational Harmonic Analysis},
volume = {19},
number = {3},
pages = {285-302},
year = {2005},
issn = {1063-5203},
author = {Steve Smale and Ding-Xuan Zhou},
keywords = {Shannon sampling, Function reconstruction, Learning theory, Reproducing kernel Hilbert space, Frames},
abstract = {We continue our study [S. Smale, D.X. Zhou, Shannon sampling and function reconstruction from point values, Bull. Amer. Math. Soc. 41 (2004) 279?305] of Shannon sampling and function reconstruction. In this paper, the error analysis is improved. Then we show how our approach can be applied to learning theory: a functional analysis framework is presented; dimension independent probability estimates are given not only for the error in the L2 spaces, but also for the error in the reproducing kernel Hilbert space where the learning algorithm is performed. Covering number arguments are replaced by estimates of integral operators.}
}




@article{Smale2007LearningTE,
  title={Learning Theory Estimates via Integral Operators and Their Approximations},
  author={S. Smale and Ding-Xuan Zhou},
  journal={Constructive Approximation},
  year={2007},
  volume={26},
  pages={153-172}
}


@article{snelson2005sparse,
  title={Sparse {G}aussian processes using pseudo-inputs},
  author={Snelson, Edward and Ghahramani, Zoubin},
  journal={Advances in neural information processing systems},
  volume={18},
  pages={1257--1264},
  year={2005}
}


@ARTICLE{srinivas2012inforegret,
  author={Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M. and Seeger, Matthias W.},
  journal={IEEE Transactions on Information Theory}, 
  title={Information-Theoretic Regret Bounds for Gaussian Process Optimization in the Bandit Setting}, 
  year={2012},
  volume={58},
  number={5},
  pages={3250-3265},
  doi={10.1109/TIT.2011.2182033}}

@inproceedings{sriperumbudur2015,
author = {Sriperumbudur, Bharath K. and Szab\'{o}, Zolt\'{a}n},
title = {Optimal Rates for Random {F}ourier Features},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Kernel methods represent one of the most powerful tools in machine learning to tackle problems expressed in terms of function values and derivatives due to their capability to represent and model complex relations. While these methods show good versatility, they are computationally intensive and have poor scalability to large data as they require operations on Gram matrices. In order to mitigate this serious computational limitation, recently randomized constructions have been proposed in the literature, which allow the application of fast linear algorithms. Random Fourier features (RFF) are among the most popular and widely applied constructions: they provide an easily computable, low-dimensional feature representation for shift-invariant kernels. Despite the popularity of RFFs, very little is understood theoretically about their approximation quality. In this paper, we provide a detailed finite-sample theoretical analysis about the approximation quality of RFFs by (i) establishing optimal (in terms of the RFF dimension, and growing set size) performance guarantees in uniform norm, and (ii) presenting guarantees in Lr (1 ≤ r &lt; ∞) norms. We also propose an RFF approximation to derivatives of a kernel with a theoretical study on its approximation quality.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1144–1152},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}



@book{steinwartSVM, 
author = {Steinwart, Ingo and Christmann, Andreas}, 
title = {Support Vector Machines}, 
year = {2008}, isbn = {0387772413}, publisher = {Springer Publishing Company, Incorporated}, edition = {1st}, abstract = {This book explains the principles that make support vector machines (SVMs) a successful modelling and prediction tool for a variety of applications. The authors present the basic ideas of SVMs together with the latest developments and current research questions in a unified style. They identify three reasons for the success of SVMs: their ability to learn well with only a very small number of free parameters, their robustness against several types of model violations and outliers, and their computational efficiency compared to several other methods. Since their appearance in the early nineties, support vector machines and related kernel-based methods have been successfully applied in diverse fields of application such as bioinformatics, fraud detection, construction of insurance tariffs, direct marketing, and data and text mining. As a consequence, SVMs now play an important role in statistical machine learning and are used not only by statisticians, mathematicians, and computer scientists, but also by engineers and data analysts. The book provides a unique in-depth treatment of both fundamental and recent material on SVMs that so far has been scattered in the literature. The book can thus serve as both a basis for graduate courses and an introduction for statisticians, mathematicians, and computer scientists. It further provides a valuable reference for researchers working in the field. The book covers all important topics concerning support vector machines such as: loss functions and their role in the learning process; reproducing kernel Hilbert spaces and their properties; a thorough statistical analysis that uses both traditional uniform bounds and more advanced localized techniques based on Rademacher averages and Talagrand's inequality; a detailed treatment of classification and regression; a detailed robustness analysis; and a description of some of the most recent implementation techniques. To make the book self-contained, an extensive appendix is added which provides the reader with the necessary background from statistics, probability theory, functional analysis, convex analysis, and topology.} }

@book{suykens2002,
  added-at = {2008-03-11T14:52:34.000+0100},
  address = {Singapore},
  author = {Suykens, J. A. K. and Gestel, T. Van and Brabanter, J. De and Moor, B. De and Vandewalle, J.},
  publisher = {World Scientific},
  timestamp = {2008-03-11T14:52:37.000+0100},
  title = {Least Squares Support Vector Machines},
  year = 2002
}


@ARTICLE{Stoica2004,
  author={Stoica, P. and Selen, Y.},
  journal={IEEE Signal Processing Magazine}, 
  title={Model-order selection: a review of information criterion rules}, 
  year={2004},
  volume={21},
  number={4},
  pages={36-47},
  doi={10.1109/MSP.2004.1311138}}

@article{Sun2009ApplicationOI,
  title={Application of integral operator for regularized least-square regression},
  author={Hongwei Sun and Qiang Wu},
  journal={Mathematical and Computer Modelling},
  year={2009},
  volume={49},
  pages={276-285}
}

@inproceedings{Teh2005SemiparametricLF,
  title={Semiparametric latent factor models},
  author={Y. Teh and Matthias W. Seeger and Michael I. Jordan},
  booktitle={AISTATS},
  year={2005}
}

@book{theodoridis2015ml, author = {Theodoridis, Sergios}, title = {Machine Learning: A Bayesian and Optimization Perspective}, year = {2015}, isbn = {0128015225}, publisher = {Academic Press, Inc.}, address = {USA}, edition = {1st}, abstract = {This tutorial text gives a unifying perspective on machine learning by covering bothprobabilistic and deterministic approaches -which are based on optimization techniques together with the Bayesian inference approach, whose essence liesin the use of a hierarchy of probabilistic models. The book presents the major machine learning methods as they have been developed in different disciplines, such as statistics, statistical and adaptive signal processing and computer science. Focusing on the physical reasoning behind the mathematics, all the various methods and techniques are explained in depth, supported by examples and problems, giving an invaluable resource to the student and researcher for understanding and applying machine learning concepts. The book builds carefully from the basic classical methods to the most recent trends, with chapters written to be as self-contained as possible, making the text suitable for different courses: pattern recognition, statistical/adaptive signal processing, statistical/Bayesian learning, as well as short courses on sparse modeling, deep learning, and probabilistic graphical models. All major classical techniques: Mean/Least-Squares regression and filtering, Kalman filtering, stochastic approximation and online learning, Bayesian classification, decision trees, logistic regression and boosting methods. The latest trends: Sparsity, convex analysis and optimization, online distributed algorithms, learning in RKH spaces, Bayesian inference, graphical and hidden Markov models, particle filtering, deep learning, dictionary learning and latent variables modeling. Case studies - protein folding prediction, optical character recognition, text authorship identification, fMRI data analysis, change point detection, hyperspectral image unmixing, target localization, channel equalization and echo cancellation, show how the theory can be applied. MATLAB code for all the main algorithms are available on an accompanying website, enabling the reader to experiment with the code.} }


@book{Thrun97,
    author={S. Thrun and L. Pratt},
    title={Learning to learn},
    publisher={Kluwer},
    year={1997}
}


@article{tierney1994explpost,
 ISSN = {00905364},
 abstract = {Several Markov chain methods are available for sampling from a posterior distribution. Two important examples are the Gibbs sampler and the Metropolis algorithm. In addition, several strategies are available for constructing hybrid algorithms. This paper outlines some of the basic methods and strategies and discusses some related theoretical and practical issues. On the theoretical side, results from the theory of general state space Markov chains can be used to obtain convergence rates, laws of large numbers and central limit theorems for estimates obtained from Markov chain methods. These theoretical results can be used to guide the construction of more efficient algorithms. For the practical use of Markov chain methods, standard simulation methodology provides several variance reduction techniques and also give guidance on the choice of sample size and allocation.},
 author = {Luke Tierney},
 journal = {The Annals of Statistics},
 number = {4},
 pages = {1701--1728},
 publisher = {Institute of Mathematical Statistics},
 title = {Markov Chains for Exploring Posterior Distributions},
 volume = {22},
 year = {1994}
}

@book{Tikhonov,
    author={A.N. Tikhonov and V.Y. Arsenin},
    title={Solutions of Ill-Posed Problems},
    publisher={Washington, D.C.: Winston/Wiley},
    year={1977}
}

@article{tong2020,
    author = {Tong, Hongzhi and Gao, Jiajing},
    title = "{Analysis of Regression Algorithms with Unbounded Sampling}",
    journal = {Neural Computation},
    volume = {32},
    number = {10},
    pages = {1980-1997},
    year = {2020},
    month = {October},
    abstract = "{In this letter, we study a class of the regularized regression algorithms when the sampling process is unbounded. By choosing different loss functions, the learning algorithms can include a wide range of commonly used algorithms for regression. Unlike the prior work on theoretical analysis of unbounded sampling, no constraint on the output variables is specified in our setting. By an elegant error analysis, we prove consistency and finite sample bounds on the excess risk of the proposed algorithms under regular conditions.}",
    issn = {0899-7667},
    doi = {10.1162/neco_a_01313}
}



@article{vanhatalo2006thesis,
author = {Vanhatalo, Jarno and Vehtari, Aki},
year = {2007},
month = {January},
pages = {73-89},
title = {Sparse Log Gaussian Processes via MCMC for Spatial Epidemiology.},
volume = {1},
journal = {Journal of Machine Learning Research - Proceedings Track}
}

@book{Vapnik98,
    Address = {New York, NY, USA},
    Author = {Vapnik,~V.},
    Publisher = {Wiley},
    Title = {Statistical Learning Theory},
    Year = {1998}}
    
@incollection{LUXBURG2011651,
title = {Statistical Learning Theory: Models, Concepts, and Results},
editor = {Dov M. Gabbay and Stephan Hartmann and John Woods},
series = {Handbook of the History of Logic},
publisher = {North-Holland},
volume = {10},
pages = {651-706},
year = {2011},
booktitle = {Inductive Logic},
issn = {1874-5857},
doi = {https://doi.org/10.1016/B978-0-444-52936-7.50016-1},
author = {Ulrike von Luxburg and Bernhard Schölkopf},
abstract = {Publisher Summary
Statistical learning theory is regarded as one of the most beautifully developed branches of artificial intelligence. It provides the theoretical basis for many of today's machine learning algorithms. The theory helps to explore what permits to draw valid conclusions from empirical data. This chapter provides an overview of the key ideas and insights of statistical learning theory. The statistical learning theory begins with a class of hypotheses and uses empirical data to select one hypothesis from the class. If the data generating mechanism is benign, then it is observed that the difference between the training error and test error of a hypothesis from the class is small. The statistical learning theory generally avoids metaphysical statements about aspects of the true underlying dependency, and thus is precise by referring to the difference between training and test error. The chapter also describes some other variants of machine learning.}
}



    
    @inproceedings{ipopttutorial,
title = "Getting Started with IPOPT in 90 minutes: Short tutorial",
abstract = "Ipopt is an open-source software package for large-scale nonlinearoptimization. This tutorial gives a short introduction that shouldallow the reader to install and test the package on a UNIX-like system,and to run simple examples in a short period of time.",
author = "Andreas Waechter",
year = "2009",
language = "English (US)",
booktitle = "Combinatorial Scientific Computing",
}
    
    @article{Wang2020GeneralizingFA,
  title={Generalizing from a Few Examples},
  author={Yaqing Wang and Quanming Yao and James T. Kwok and L. Ni},
  journal={ACM Computing Surveys (CSUR)},
  year={2020},
  volume={53},
  pages={1 - 34}
}


@article{Wang2011,  
title     = "Optimal learning rates for least squares regularized regression with unbounded sampling",  abstract  = "A standard assumption in theoretical study of learning algorithms for regression is uniform boundedness of output sample values. This excludes the common case with Gaussian noise. In this paper we investigate the learning algorithm for regression generated by the least squares regularization scheme in reproducing kernel Hilbert spaces without the assumption of uniform boundedness for sampling. By imposing some incremental conditions on moments of the output variable, we derive learning rates in terms of regularity of the regression function and capacity of the hypothesis space. The novelty of our analysis is a new covering number argument for bounding the sample error. {\textcopyright} 2010 Elsevier Inc. All rights reserved.",  keywords  = "Covering number, Learning theory, Least squares regression, Regularization in reproducing kernel Hilbert spaces",  author    = "Cheng Wang and Ding-Xuan Zhou",  year      = "2011",  month     = february,   
language  = "English",  volume    = "27",  pages     = "55--67",  journal   = "Journal of Complexity",  issn      = "0885-064X",  publisher = "Academic Press",  number    = "1", }




@article{Wahba:90,
  added-at = {2008-03-11T14:52:34.000+0100},
  author = {Wahba, G.},
  journal = {Regional Conference Series in Applied Mathematics},
  keywords = {juergen},
  priority = {2},
  publisher = {{SIAM Press, Philadelphia}},
  timestamp = {2008-03-11T14:53:52.000+0100},
  title = {Spline Models for Observational Data},
  volume = 59,
  year = 1990
}

@inbook{wahba2019representer,
author = {Wahba, Grace and Wang, Yuedong},
publisher = {American Cancer Society},
isbn = {9781118445112},
title = {Representer Theorem},
booktitle = {Wiley StatsRef: Statistics Reference Online},
chapter = {},
pages = {1-11},
doi = {https://doi.org/10.1002/9781118445112.stat08200},
year = {2019},
keywords = {classification, functional data, nonparametric regression, penalized least squares, penalized likelihood, regularization, reproducing kernel Hilbert space, smoothing spline ANOVA, support vector machines},
abstract = {Abstract The representer theorem plays an outsized role in a large class of learning problems. It provides a means to reduce infinite dimensional optimization problems to tractable finite dimensional ones. This article reviews the representer theorem for various learning problems under the reproducing kernel Hilbert spaces framework. We present solutions to the penalized least squares and penalized likelihood for nonparametric regression and support vector machines for classification as a solution to the penalized hinge loss. We discuss extensions of the representer theorem for regression with functional data.}
}




@inproceedings{Williams2000,
 author = {Williams, Christopher and Seeger, Matthias},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 pages = {},
 publisher = {MIT Press},
 title = {Using the {N}ystr\"{o}m Method to Speed Up Kernel Machines},
 volume = {13},
 year = {2000}
}




@ARTICLE{Xu2018,
author={Y. {Xu} and X. {Li} and D. {Chen} and H. {Li}},
journal={IEEE Transactions on Neural Networks and Learning Systems},
title={Learning Rates of Regularized Regression With Multiple Gaussian Kernels for Multi-Task Learning},
year={2018},
volume={29},
number={11},
pages={5408-5418}
}     
    
    
@article{Zhang2019,
title = "Multi-task and multi-view training for end-to-end relation extraction",
journal = "Neurocomputing",
volume = "364",
pages = "245 - 253",
year = "2019",
author = "J. Zhang and Y. Zhang and D.J. Ji and M. Liu"
}  
    
    
@ARTICLE{Zhou2016,
author={Q. {Zhou} and Q. {Zhao}},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Flexible Clustered Multi-Task Learning by Learning Representative Tasks},
year={2016},
volume={38},
number={2},
pages={266-278}
}
    
 
    


@book{Scholkopf2004Bio,
    author = {Schölkopf, Bernhard and Tsuda, Koji and Vert, Jean-Philippe},
    title = "{Kernel Methods in Computational Biology}",
    publisher = {The MIT Press},
    year = {2004},
    month = {July},
    abstract = "{A detailed overview of current research in kernel methods and their application to computational biology.Modern machine learning techniques are proving to be extremely valuable for the analysis of data in computational biology problems. One branch of machine learning, kernel methods, lends itself particularly well to the difficult aspects of biological data, which include high dimensionality (as in microarray measurements), representation as discrete and structured data (as in DNA or amino acid sequences), and the need to combine heterogeneous sources of information. This book provides a detailed overview of current research in kernel methods and their applications to computational biology. Following three introductory chapters—an introduction to molecular and computational biology, a short review of kernel methods that focuses on intuitive concepts rather than technical details, and a detailed survey of recent applications of kernel methods in computational biology—the book is divided into three sections that reflect three general trends in current research. The first part presents different ideas for the design of kernel functions specifically adapted to various biological data; the second part covers different approaches to learning from heterogeneous data; and the third part offers examples of successful applications of support vector machine methods.}"
}



@article{Lampert2009computervision,
author = {Lampert, Christoph H.},
title = {Kernel Methods in Computer Vision},
year = {2009},
issue_date = {March 2009},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {4},
number = {3},
abstract = {Over the last years, <it>kernel methods</it> have established themselves as powerful tools for computer vision researchers as well as for practitioners. In this tutorial, we give an introduction to kernel methods in computer vision from a geometric perspective, introducing not only the ubiquitous support vector machines, but also less known techniques for regression, dimensionality reduction, outlier detection, and clustering. Additionally, we give an outlook on very recent, non-classical techniques for the prediction of structure data, for the estimation of statistical dependency, and for learning the kernel function itself. All methods are illustrated with examples of successful application from the recent computer vision research literature.},
journal = {Foundations and Trends in Computer Graphics and Vision},
month = {March},
pages = {193–285},
numpages = {93}
}


    
    @article{Caponnetto2007OptimalRF,
  title={Optimal Rates for the Regularized Least-Squares Algorithm},
  author={Andrea Caponnetto and Ernesto de Vito},
  journal={Foundations of Computational Mathematics},
  year={2007},
  volume={7},
  pages={331-368}
}


@article{WANG201155,
title = {Optimal learning rates for least squares regularized regression with unbounded sampling},
journal = {Journal of Complexity},
volume = {27},
number = {1},
pages = {55-67},
year = {2011},
issn = {0885-064X},
author = {Cheng Wang and Ding-Xuan Zhou},
keywords = {Learning theory, Least squares regression, Regularization in reproducing kernel Hilbert spaces, Covering number},
abstract = {A standard assumption in theoretical study of learning algorithms for regression is uniform boundedness of output sample values. This excludes the common case with Gaussian noise. In this paper we investigate the learning algorithm for regression generated by the least squares regularization scheme in reproducing kernel Hilbert spaces without the assumption of uniform boundedness for sampling. By imposing some incremental conditions on moments of the output variable, we derive learning rates in terms of regularity of the regression function and capacity of the hypothesis space. The novelty of our analysis is a new covering number argument for bounding the sample error.}
}
    
   
@article{mendelson2010,
author = {Shahar Mendelson and Joseph Neeman},
title = {{Regularization in kernel learning}},
volume = {38},
journal = {The Annals of Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {526 -- 565},
keywords = {least-squares, Model selection, regression, Regulation, reproducing kernel Hilbert space},
year = {2010}
}
   
   
@article{zhou2003,
author = {Zhou, Ding-Xuan},
year = {2003},
month = {August},
pages = {1743 - 1752},
title = {Capacity of reproducing kernel spaces in learning theory},
volume = {49},
journal = {IEEE Transactions on Information Theory},
doi = {10.1109/TIT.2003.813564}
}

@article{Davies2007NonparametricRC,
  title={Nonparametric Regression, Confidence Regions and Regularization},
  author={P. Laurie Davies and Arne Kovac and Monika Meise},
  journal={Annals of Statistics},
  year={2007},
  volume={37},
  pages={2597-2625}
}


@article{CO16,
author = {Chris J. Ostafew and Angela P. Schoellig and Timothy D. Barfoot},
title ={Robust Constrained Learning-based {NMPC} enabling reliable mobile robot path tracking},
journal = {The International Journal of Robotics Research},
volume = {35},
number = {13},
pages = {1547-1563},
year = {2016},
doi = {10.1177/0278364916645661}
}



@INPROCEEDINGS{JK04,
  author={Kocijan, J. and Murray-Smith, R. and Rasmussen, C.E. and Girard, A.},
  booktitle={Proceedings of the 2004 American Control Conference}, 
  title={Gaussian process model based predictive control},
  year={2004},
  volume={3},
  number={},
  pages={2214-2219},
  doi={10.23919/ACC.2004.1383790}}



@INPROCEEDINGS{TK18,
  author={Koller, Torsten and Berkenkamp, Felix and Turchetta, Matteo and Krause, Andreas},
  booktitle={2018 IEEE Conference on Decision and Control (CDC)}, 
  title={Learning-Based Model Predictive Control for Safe Exploration}, 
  year={2018},
  volume={},
  number={},
  pages={6059-6066},
  doi={10.1109/CDC.2018.8619572}}

   

 @book{maritz2018empirical,
  title={Empirical Bayes Methods with Applications},
  author={Maritz, J.S.},
  isbn={9781351080118},
  year={2018},
  publisher={CRC Press}
}


@inproceedings{rudi2017,
 author = {Rudi, Alessandro and Rosasco, Lorenzo},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generalization Properties of Learning with Random Features},
 volume = {30},
 year = {2017}
}



@article{cortes2010impactkernelapprox,
title = "On the impact of kernel approximation on learning accuracy",
abstract = "Kernel approximation is commonly used to scale kernel-based algorithms to applications containing as many as several million instances. This paper analyzes the effect of such approximations in the kernel matrix on the hypothesis generated by several widely used learning algorithms. We give stability bounds based on the norm of the kernel approximation for these algorithms, including SVMs, KRR, and graph Laplacian-based regularization algorithms. These bounds help determine the degree of approximation that can be tolerated in the estimation of the kernel matrix. Our analysis is general and applies to arbitrary approximations of the kernel matrix. However, we also give a specific analysis of the Nystr{\"o}m low-rank approximation in this context and report the results of experiments evaluating the quality of the Nystr{\"o}m low-rank kernel approximation when used with ridge regression. 9.",
author = "Corinna Cortes and Mehryar Mohri and Ameet Talwalkar",
year = "2010",
language = "English (US)",
volume = "9",
pages = "113--120",
journal = "Journal of Machine Learning Research",
issn = "1532-4435",
publisher = "Microtome Publishing",
note = "13th International Conference on Artificial Intelligence and Statistics, AISTATS 2010 ; Conference date: 13-05-2010 Through 15-05-2010",
}


@article{PENG2021107321,
title = {Reinforcement learning with Gaussian processes for condition-based maintenance},
journal = {Computers & Industrial Engineering},
volume = {158},
pages = {107321},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107321},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221002254},
author = {Shenglin Peng and Qianmei (May) Feng},
keywords = {Condition-based maintenance, Reinforcement learning, Gaussian process regression, Markov decision process, Gaussian processes for reinforcement learning, Function approximation},
abstract = {Condition-based maintenance strategies are effective in enhancing reliability and safety for complex engineering systems that exhibit degradation phenomena with uncertainty. Such sequential decision-making problems are often modeled as Markov decision processes (MDPs) when the underlying process has a Markov property. Recently, reinforcement learning (RL) becomes increasingly efficient to address MDP problems with large state spaces. In this paper, we model the condition-based maintenance problem as a discrete-time continuous-state MDP without discretizing the deterioration condition of the system. The Gaussian process regression is used as function approximation to model the state transition and the value functions of states in reinforcement learning. A RL algorithm is then developed to minimize the long-run average cost (instead of the commonly-used discounted reward) with iterations on the state-action value function and the state value function, respectively. We verify the capability of the proposed algorithm by simulation experiments and demonstrate its advantages in a case study on a battery maintenance decision-making problem. The proposed algorithm outperforms the discrete MDP approach by achieving lower long-run average costs.}
}


@inproceedings{malte2003gprl,
 author = {Kuss, Malte and Rasmussen, Carl},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 pages = {},
 publisher = {MIT Press},
 title = {Gaussian Processes in Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2003/file/7993e11204b215b27694b6f139e34ce8-Paper.pdf},
 volume = {16},
 year = {2003}
}




@book{davis1975interpolation,
  title={Interpolation and Approximation},
  author={Davis, P.J.},
  isbn={9780486624952},
  lccn={75002568},
  series={Dover Books on Mathematics},
  year={1975},
  publisher={Dover Publications}
}


@inproceedings{Tsybakov2009IntroductionTN,
  title={Introduction to Nonparametric Estimation},
  author={Alexandre B. Tsybakov},
  booktitle={Springer series in statistics},
  year={2009}
}

@article{Akaike1974ANL,
  title={A new look at the statistical model identification},
  author={Hirotugu Akaike},
  journal={IEEE Transactions on Automatic Control},
  year={1974},
  volume={19},
  pages={716-723}
}


@article{mallows1973,
 ISSN = {00401706},
 abstract = {We discuss the interpretation of CP-plots and show how they can be calibrated in several ways. We comment on the practice of using the display as a basis for formal selection of a subset-regression model, and extend the range of application of the device to encompass arbitrary linear estimates of the regression coefficients, for example Ridge estimates.},
 author = {C. L. Mallows},
 journal = {Technometrics},
 number = {4},
 pages = {661--675},
 publisher = {[Taylor & Francis, Ltd., American Statistical Association, American Society for Quality]},
 title = {Some Comments on $C_p$},
 volume = {15},
 year = {1973}
}

@article{shibata1981,
 ISSN = {00063444},
 abstract = {An asymptotically optimal selection of regression variables is proposed. The key assumption is that the number of control variables is infinite or increases with the sample size. It is also shown that Mallows's Cp, Akaike's FPE and AIC methods are all asymptotically equivalent to this method.},
 author = {Ritei Shibata},
 journal = {Biometrika},
 number = {1},
 pages = {45--54},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {An Optimal Selection of Regression Variables},
 volume = {68},
 year = {1981}
}


@article{Demmler1975OscillationMW,
  title={Oscillation matrices with spline smoothing},
  author={A L Demmler and Christian H. Reinsch},
  journal={Numerische Mathematik},
  year={1975},
  volume={24},
  pages={375-382}
}

@book{eubank1999nonparametric,
  title={Nonparametric Regression and Spline Smoothing},
  author={Eubank, R.L.},
  isbn={9781482273144},
  series={Statistics:  A Series of Textbooks and Monographs},
  year={1999},
  publisher={CRC Press}
}

@book{bary2014treatise,
  title={A Treatise on Trigonometric Series: Volume 1},
  author={Bary, N.K.},
  number={v. 1},
  isbn={9781483224190},
  year={2014},
  publisher={Elsevier Science}
}

@book{zygmund2002trigonometric,
  title={Trigonometric Series},
  author={Zygmund, A. and Fefferman, R.},
  number={v. 1},
  isbn={9780521890533},
  lccn={2002067363},
  series={Cambridge Mathematical Library},
  year={2002},
  publisher={Cambridge University Press}
}


@book{wasserman2006all,
  title={All of Nonparametric Statistics},
  author={Wasserman, L.},
  isbn={9780387306230},
  lccn={2005925603},
  series={Springer Texts in Statistics},
  year={2006},
  publisher={Springer New York}
}

@article{hoerl1970,
 ISSN = {00401706},
 abstract = {In multiple regression it is shown that parameter estimates based on minimum residual sum of squares have a high probability of being unsatisfactory, if not incorrect, if the prediction vectors are not orthogonal. Proposed is an estimation procedure based on adding small positive quantities to the diagonal of X′X. Introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality. It is then shown how to augment X′X to obtain biased estimates with smaller mean square error.},
 author = {Arthur E. Hoerl and Robert W. Kennard},
 journal = {Technometrics},
 number = {1},
 pages = {55--67},
 publisher = {[Taylor & Francis, Ltd., American Statistical Association, American Society for Quality]},
 title = {Ridge Regression: Biased Estimation for Nonorthogonal Problems},
 volume = {12},
 year = {1970}
}

@book{lunardi2009,
  title={Interpolation Theory},
  author={Alessandra Lunardi},
  series={Publications of the {S}cuola {N}ormale di {P}isa},
  year={2009},
  publisher={Edizioni della Normale Pisa}
}

@ARTICLE{Wu06learningrates,
    author = {Qiang Wu and Yiming Ying and Ding-Xuan Zhou},
    title = {Learning rates of least-square regularized regression},
    journal = {Foundations of Computational Mathematics},
    year = {2006},
    pages = {171--192}
}


@article{cucker2008bestchoices,
author = {Felipe Cucker and Steven Smale},
year = {2008},
month = {March},
pages = {413-428},
title = {Best Choices for Regularization Parameters in Learning Theory: On the Bias—Variance Problem},
volume = {2},
journal = {Foundations of Computational Mathematics},
doi = {10.1007/s102080010030}
}

@article{Lin2017distributed,
  author  = {Shao-Bo Lin and Xin Guo and Ding-Xuan Zhou},
  title   = {Distributed Learning with Regularized Least Squares},
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  volume  = {18},
  number  = {92},
  pages   = {1-31}
}


@article{zhang2005effdim,
    author = {Zhang, Tong},
    title = "{Learning Bounds for Kernel Regression Using Effective Data Dimensionality}",
    journal = {Neural Computation},
    volume = {17},
    number = {9},
    pages = {2077-2098},
    year = {2005},
    month = {September},
    abstract = "{Kernel methods can embed finite-dimensional data into infinite-dimensional feature spaces. In spite of the large underlying feature dimensionality, kernel methods can achieve good generalization ability. This observation is often wrongly interpreted, and it has been used to argue that kernel learning can magically avoid the “curse-of-dimensionality” phenomenon encountered in statistical estimation problems. This letter shows that although using kernel representation, one can embed data into an infinite-dimensional feature space; the effective dimensionality of this embedding, which determines the learning complexity of the underlying kernel machine, is usually small. In particular, we introduce an algebraic definition of a scale-sensitive effective dimension associated with a kernel representation. Based on this quantity, we derive upper bounds on the generalization performance of some kernel regression methods. Moreover, we show that the resulting convergent rates are optimal under various circumstances.}",
    issn = {0899-7667},
    doi = {10.1162/0899766054323008}
}



@article{MU2018381,
title = {On asymptotic properties of hyperparameter estimators for kernel-based regularization methods},
journal = {Automatica},
volume = {94},
pages = {381-395},
year = {2018},
issn = {0005-1098},
author = {Biqiang Mu and Tianshi Chen and Lennart Ljung},
keywords = {Kernel-based regularization, Empirical Bayes, Stein’s unbiased risk estimator, Asymptotic analysis},
abstract = {The kernel-based regularization method has two core issues: kernel design and hyperparameter estimation. In this paper, we focus on the second issue and study the properties of several hyperparameter estimators including the empirical Bayes (EB) estimator, two Stein’s unbiased risk estimators (SURE) (one related to impulse response reconstruction and the other related to output prediction) and their corresponding Oracle counterparts, with an emphasis on the asymptotic properties of these hyperparameter estimators. To this goal, we first derive and then rewrite the first order optimality conditions of these hyperparameter estimators, leading to several insights on these hyperparameter estimators. Then we show that as the number of data goes to infinity, the two SUREs converge to the best hyperparameter minimizing the corresponding mean square error, respectively, while the more widely used EB estimator converges to another best hyperparameter minimizing the expectation of the EB estimation criterion. This indicates that the two SUREs are asymptotically optimal in the corresponding MSE senses but the EB estimator is not. Surprisingly, the convergence rate of two SUREs is slower than that of the EB estimator, and moreover, unlike the two SUREs, the EB estimator is independent of the convergence rate of ΦTΦ∕N to its limit, where Φ is the regression matrix and N is the number of data. A Monte Carlo simulation is provided to demonstrate the theoretical results.}
}



@article{PILLONETTO2015106,
title = {Tuning complexity in regularized kernel-based regression and linear system identification: The robustness of the marginal likelihood estimator},
journal = {Automatica},
volume = {58},
pages = {106-117},
year = {2015},
issn = {0005-1098},
author = {Gianluigi Pillonetto and Alessandro Chiuso},
keywords = {Linear system identification, Bias–variance trade off, Kernel-based regularization, Stein’s unbiased risk estimation, Excess degrees of freedom},
abstract = {Kernel-based regularization approaches have been successfully applied in the last years for regression purposes. Recently, these machine learning techniques have been also introduced in linear system identification, by interpreting impulse response estimation as a function learning problem. The adopted estimator solves a regularized least squares problem which admits also a Bayesian interpretation where the impulse response is modeled as a zero-mean Gaussian vector. A possible choice for the covariance is the so called stable spline kernel. It includes information on smoothness and exponential stability, containing just two unknown parameters which can be tuned via marginal likelihood (ML) optimization. Experimental evidence has shown that this new approach may outperform traditional system identification approaches, such as PEM and subspace techniques. The aim of this work is to provide new insights on the stable spline estimator equipped with ML estimation of hyperparameters. To this purpose, we study the mean squared error properties of the ML procedure for hyperparameter estimation; in doing so we shall not assume the correctness of the Bayesian priors. Then, we derive the notion of excess degrees of freedom. This notion measures the additional complexity to be assigned to an estimator which is also required to determine hyperparameters from data. The conclusion of our investigation is that much of criticisms reported in the literature to robustness of ML is not well founded. On the contrary, in many situations ML can well balance data fit and excess degrees of freedom. Hence, it turns out an important tool for tuning model complexity in linear system identification also when undermodeling affects the kernel-based description of the impulse response.}
}


@article{Geman1982NonparametricML,
  title={Nonparametric Maximum Likelihood Estimation by the Method of Sieves},
  author={Stuart Geman and Chii-Ruey Hwang},
  journal={Annals of Statistics},
  year={1982},
  volume={10},
  pages={401-414}
}


@misc{arcari22robot,
  doi = {10.48550/ARXIV.2211.10270},
  
  url = {https://arxiv.org/abs/2211.10270},
  
  author = {Arcari, Elena and Minniti, Maria Vittoria and Scampicchio, Anna and Carron, Andrea and Farshidian, Farbod and Hutter, Marco and Zeilinger, Melanie N.},
  
  keywords = {Robotics (cs.RO), Systems and Control (eess.SY), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {Bayesian Multi-Task Learning MPC for Robotic Mobile Manipulation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}




@ARTICLE{liu2020scalablegp,
  author={Liu, Haitao and Ong, Yew-Soon and Shen, Xiaobo and Cai, Jianfei},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={When Gaussian Process Meets Big Data: A Review of Scalable GPs}, 
  year={2020},
  volume={31},
  number={11},
  pages={4405-4423},
  doi={10.1109/TNNLS.2019.2957109}}



@InProceedings{pmlr-v70-pan17a,
  title = 	 {Prediction under Uncertainty in Sparse Spectrum {G}aussian Processes with Applications to Filtering and Control},
  author =       {Yunpeng Pan and Xinyan Yan and Evangelos A. Theodorou and Byron Boots},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2760--2768},
  year = 	 {2017},
  volume = 	 {70},
  series = 	 {PMLR},
  month = 	 {August}
}

@article{CHEN20121525,
title = {On the estimation of transfer functions, regularizations and Gaussian processes—Revisited},
journal = {Automatica},
volume = {48},
number = {8},
pages = {1525-1535},
year = {2012},
issn = {0005-1098},
author = {Tianshi Chen and Henrik Ohlsson and Lennart Ljung},
keywords = {System identification, Transfer function estimation, Regularization, Bayesian inference, Gaussian process, Mean square error, Bias–variance trade-off},
abstract = {Intrigued by some recent results on impulse response estimation by kernel and nonparametric techniques, we revisit the old problem of transfer function estimation from input–output measurements. We formulate a classical regularization approach, focused on finite impulse response (FIR) models, and find that regularization is necessary to cope with the high variance problem. This basic, regularized least squares approach is then a focal point for interpreting other techniques, like Bayesian inference and Gaussian process regression. The main issue is how to determine a suitable regularization matrix (Bayesian prior or kernel). Several regularization matrices are provided and numerically evaluated on a data bank of test systems and data sets. Our findings based on the data bank are as follows. The classical regularization approach with carefully chosen regularization matrices shows slightly better accuracy and clearly better robustness in estimating the impulse response than the standard approach–the prediction error method/maximum likelihood (PEM/ML) approach. If the goal is to estimate a model of given order as well as possible, a low order model is often better estimated by the PEM/ML approach, and a higher order model is often better estimated by model reduction on a high order regularized FIR model estimated with careful regularization. Moreover, an optimal regularization matrix that minimizes the mean square error matrix is derived and studied. The importance of this result lies in that it gives the theoretical upper bound on the accuracy that can be achieved for this classical regularization approach.}
}


@ARTICLE{scalableGP,
  author={Liu, Haitao and Ong, Yew-Soon and Shen, Xiaobo and Cai, Jianfei},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={When Gaussian Process Meets Big Data: A Review of Scalable GPs}, 
  year={2020},
  volume={31},
  number={11},
  pages={4405-4423},
  doi={10.1109/TNNLS.2019.2957109}}



  @misc{trigarxiv2023,
      title={Error Analysis of regularized trigonometric linear regression with unbounded sampling: the statistical learning viewpoint}, 
      author={Anna Scampicchio and Elena Arcari and Melanie N. Zeilinger},
      year={2023},
      publisher={arxiv},
  url = {https://arxiv.org/abs/2211.10270}
}
