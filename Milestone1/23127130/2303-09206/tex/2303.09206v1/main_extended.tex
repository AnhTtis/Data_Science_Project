\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document



% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
%\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsmath,amssymb,amsfonts,mathrsfs}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor} 
\usepackage{xr-hyper,hyperref}
\let\proof\relax
\let\endproof\relax
\usepackage{wasysym}
\usepackage{graphicx,float}
\usepackage{capt-of}
\usepackage{textcomp,comment}  
\let\labelindent\relax %to fix enumitem 
\usepackage{stfloats,empheq,epstopdf,enumitem}
\usepackage{tikz}
\usetikzlibrary{arrows,shapes,positioning,automata,backgrounds,petri}
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{pgfplots}
\usepackage{caption}
\captionsetup[figure]{font=small,skip=5pt}
\captionsetup[table]{font=small,skip=5pt}


\newcommand{\cY}{\ensuremath{\mathscr{Y}}}
\newcommand{\cX}{\ensuremath{\mathscr{X}}}
\newcommand{\cZ}{\ensuremath{\mathscr{Z}}}
\newcommand{\cL}{\ensuremath{\mathscr{L}^2_{\rho_{\cX}}}}
\newcommand{\cH}{\ensuremath{\mathscr{H}}}
\newcommand{\cT}{\ensuremath{\mathscr{T}}}
\newcommand{\cK}{\ensuremath{\mathscr{K}}}
\newcommand{\Ss}{\ensuremath{\mathcal{S}_{\mathscr{X}}}}
\newcommand{\io}{\ensuremath{L_{\cK}}}
\newcommand{\lmin}{\ensuremath{\breve{\lambda}}}
\newcommand{\Var}{\ensuremath{\text{var}}}

\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}

\allowdisplaybreaks
\graphicspath{{Figures/}}





\title{\LARGE \bf
Error analysis of regularized trigonometric linear regression\\ with unbounded sampling: a statistical learning viewpoint
}



\author{Anna Scampicchio, Elena Arcari, Melanie N. Zeilinger% <-this % stops a space
\thanks{The Authors are affiliated with the Institute of Dynamic Systems and Control, ETH Z\"urich.
        {\tt\small $\lbrace$ascampicc,earcari,mzeilinger$\rbrace$@ethz.ch}}%
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
The effectiveness of non-parametric, kernel-based methods for function estimation comes at the price of high computational complexity, which hinders their applicability in adaptive, model-based control. Motivated by approximation techniques based on sparse spectrum Gaussian processes, 
we focus on models given by regularized trigonometric linear regression.  
This paper provides an analysis of the performance of such an estimation set-up within the statistical learning framework. 
In particular, we derive a novel bound for the sample error in finite-dimensional spaces, accounting for noise with potentially unbounded support. Next, we study the approximation error and discuss the bias-variance trade-off as a function of the regularization parameter by combining the two bounds. 
\end{abstract}

\section{INTRODUCTION}
Non-parametric approaches for regularized function estimation are a key tool in machine learning, and have been successfully applied to, e.g., system identification \cite{Pillonetto2014} and learning-based control~\cite{Hewing2020},~\cite{TK18}. Nevertheless, their applicability in real-time scenarios is hindered by their high computational complexity, which scales cubically with the number of data. 
The strategies proposed
to enable fast adaptation of kernel-based methods can be grouped into two main categories: input location selection, and low-rank approximations of the kernel \cite{scalableGP}. 
In this second class of approaches, a vast success was achieved by sparse spectrum Gaussian processes \cite{lazaro2010sparse,rudi2017}, 
where operations on the (shift-invariant) kernel yield a parametric approximation by means of linear combinations of Fourier features.\\ 
In this paper, we draw inspiration from the latter method and perform regularized regression within a finite-dimensional hypothesis space $\cH$ defined by a span of $E$ predefined trigonometric functions. 
Such a set-up relaxes the assumption of having shift-invariant kernels, and results more robust against potential basis function mis-specification compared to non-regularized, projection-based approaches \cite{hoerl1970}; for a review of parametric methods based on Fourier features, 
we refer to \cite[Chapter 1.7]{Tsybakov2009IntroductionTN}.  
Our goal is to assess the performance of the proposed estimator as a key step towards deriving 
reliable guarantees for data-driven, model-based control schemes that leverage such a model (see, e.g., \cite{pmlr-v70-pan17a,arcari2021,arcari22robot}).\\ 
We frame 
this analysis in the statistical learning set-up \cite{Cucker02,cuckerzhou2007}. %There, 
The function to be estimated (i.e., the \textit{regression function} $f_{\rho}$) is defined as the minimizer of the expected risk over a (partially) unknown probability distribution, jointly defined over the input-output spaces, and from which i.i.d.~samples are drawn. Consequently, this formulation can also handle fully nonlinear measurement models. 
Furthermore, 
$f_{\rho}$ is generally assumed to belong to the space of square-integrable functions $\mathscr{L}^2$, and the hypothesis space is typically taken as an infinite-dimensional Reproducing Kernel Hilbert Space (RKHS), which is 
related to $\mathscr{L}^2$ by interpolation spaces arguments (\cite[Theorem 2]{Smale2007LearningTE}, \cite{lunardi2009}). Differently from classic non-parametric set-ups, the regression function is not assumed to belong to the hypothesis space. Thus, two objects can be therein defined: the actual data-based estimate $f_z$ and its data-free limit $f_{\cH}$. The goal of error analysis consists in quantifying the approximation error, or \textit{bias}, $\|f_{\rho} - f_{\cH}\|_{\cL}$, and the sample error, or \textit{variance}, $\|f_{\cH} - f_z\|_{\cL}$. As regards the latter, results abound in the statistical learning literature. Most of them deal with probability measures on the outputs that have bounded support, and thus obtain bounds leveraging concentration inequalities 
such as Hoeffding's or Bennett's \cite{Boucheron2004}, \cite[Chapter 3.1]{cuckerzhou2007}. Works in this direction are, e.g., \cite{Wu06learningrates,cuckerzhou2007,cucker2008bestchoices,mendelson2010,WANG201155}. Contributions considering unbounded sampling include \cite{Caponnetto2007OptimalRF,Wang2011,Guo2013ConcentrationEF}. The bounds therein derived leverage
the so-called moment hypothesis, which relaxes the boundedness assumption, and hold also for (sub-)Gaussian noises. Such results rely on the computation of covering numbers quantifying the capacity of the hypothesis space \cite{zhou2003}, and showcase optimal rates of convergence; nevertheless, they tend to be of limited practical relevance in the non-asymptotic case due to the large values of the multiplicative coefficients, which are often furthermore
difficult to compute.\\
In this work, we perform error analysis for finite-dimensional hypothesis spaces given by trigonometric functions. 
Our first contribution is a sample error bound, which 
is less conservative than the ones available in the literature even if it accounts also for noises with unbounded supports. 
Our second contribution consists in studying the bias-variance trade-off of the regularized trigonometric regression set-up. To this end, we obtain two bounds on the approximation error, combine them with the sample complexity result and analyze the conditions ensuring the existence of a unique value of the regularization parameter $\gamma$ returning the optimal trade-off. The differences of the two approaches for estimating the regularization parameter are investigated in a Monte Carlo study, which shows that one of the two criteria returns a value of $\gamma$ that captures the oracle behavior (i.e., minimizing the overall error),
thus leading to fast estimation schemes that do not need preliminary hyper-parameter selection. 

\section{PROBLEM SET-UP}\label{sec:problem}
Let the metric space of inputs $\cX$ be a compact subset of $\mathbb{R}$: without loss of generality, we take $\cX = [-X/2,\, X/2]$ for some $X \in \mathbb{R}_+$ (the scalar case is presented just for ease of visualization: the multi-dimensional is a straightforward extension). The output space is assumed to be $\cY = \mathbb{R}$. There is a probability measure $\rho$ defined over $\cZ = \cX \times \cY$ that decomposes into $\rho_{\cX}(x)$ and $\rho(y|x)$ according to Fubini's Theorem. In the considered 
set-up, the probability measure defined on $\cX$ is the standard uniform: denoting with $\mu$ the Lebesgue measure, we have that $\rho_{\cX}(A) = \mu(A \cap \cX)/\mu(\cX) = \mu(A \cap \cX)/X$ for any set $A$ in the $\sigma-$algebra of interest. In this way, $\rho_{\cX}$ is a Borel non-degenerate, $\sigma-$finite measure. As regards $\rho(y|\cdot)$, we assume it is unknown and defined over $\mathbb{R}$.\\
Having $N$ independent 
samples drawn from $\rho$ collected in the data-set $\mathcal{D} = \{(x_t,\,y_t)\}_{t=1}^N$, the goal is to estimate the regression function 
\begin{equation}
    f_{\rho}(x) = \int_{\cY}yd\rho(y|x). \label{eq:regressionfunction}
\end{equation}
We make use of the following Assumption.
\begin{assumption}
The regression function $f_{\rho}$ belongs to the space of square-integrable functions on $\cX$, denoted by $\cL$, and is such that $\|f_{\rho}\|_{\cL} = \sqrt{\int_{\cX} f^2(x)d\rho_{\cX}(x)} = \sqrt{\int_{\cX}f^2(x)d\mu(x)/X} \leq B_f$. Moreover, we also have that $\sigma^2_{\rho} = \int_{\cX} \sigma^2_{\rho}(x)d\rho_{\cX}(x) = \int_{\cZ} (y-f_{\rho}(x))^2d\rho \leq B_{\sigma}^2$. \hfill$\square$
\label{assume:bounds}
\end{assumption}
In other words, we assume to have access to bounds on the energy of the unknown function to be estimated, and on the variance of the additive noises.\\
The space $\cL$ is a separable Hilbert space whose complete orthonormal basis by means of trigonometric functions \cite{akhiezer2013theory} is given by 
\begin{align}
& \Bigg\lbrace \sqrt{2}\sin\Big(\frac{2\pi q}{X}x\Big), \sqrt{2}\cos\Big(\frac{2\pi q}{X}x\Big) \Bigg\rbrace_{q\in \mathbb{N}} \\&= \lbrace \bar{\varphi}_q^s(x),\, \bar{\varphi}_q^c(x) \rbrace_{q\in\mathbb{N}} \quad \text{with } x \in \cX.  
\label{eq:basis}
\end{align}
Accordingly, any function $f \in \cL$ can be expressed as $f(\cdot) = \sum_{q\in\mathbb{N}}\alpha^s_q\bar{\varphi}^s_q(\cdot) + \alpha^c_q\bar{\varphi}^c_q(\cdot)$, which will be also compactly written as $f(\cdot) = \sum_{q\in\mathbb{N}}\alpha_q \bar{\varphi}_q(\cdot)$, with $\sum_{q\in \mathbb{N}}\alpha_q^2 < \infty$. Within this representation, we denote the target function as $f_{\rho}(\cdot) = \sum_{q\in\mathbb{N}}\bar{\alpha}^s_q\bar{\varphi}^s_q(\cdot) + \bar{\alpha}^c_q\bar{\varphi}^c_q(\cdot) = \sum_{q\in\mathbb{N}}\bar{\alpha}_q \bar{\varphi}_q(\cdot)$.\\
Function estimation in $\cL$ cannot be performed, because pointwise evaluation is not well defined. Therefore, we perform such a task within a hypothesis space having the structure of a RKHS. Specifically, we consider the RKHS obtained from a subset of functions in \eqref{eq:basis} with cardinality~$E$, where $E$ is chosen according to our computational capacity. Denote by $Q$ the set of selected frequencies, i.e., $Q = \{q_j\}_{j=1}^{E/2} \subset \mathbb{N}$, and consider the following functions extracted from \eqref{eq:basis} using $Q$ defined, for $j=1,...,E/2$, as
\begin{equation}
%\text{For } j=1,...,\frac{E}{2},\quad
\varphi_i(x) = 
    \begin{cases}
    \bar{\varphi}^s_{q_j}(x), \: i=j\\
    \bar{\varphi}^c_{q_j}(x), \: i = j+\frac{E}{2}.
    \end{cases}
    \label{eq:basisRKHS}
\end{equation}
Then, the RKHS of interest is the one induced by the following kernel:
\begin{align}
  \cK(x_a,x_b) = \phi^{\top}(x_a)\Sigma_{\alpha}\phi (x_b),\label{eq:ourkernel}
\end{align}
where $\Sigma_{\alpha} = \text{diag}(\lambda_1,...,\lambda_E)$ is a positive definite matrix, and the vector $\phi(\cdot) \in \mathbb{R}^E$ is such that $\phi^{\top}(x) =  [\varphi_1(x) \: \dots \: \varphi_E(x)].$ Clearly, \eqref{eq:ourkernel} is a Mercer kernel (\cite[(6), p.346]{aronszajn50reproducing}; it satisfies Mercer's condition $\int_{\cX}\int_{\cX}\cK(x,x^{\prime})^2d\rho_{\cX}(x)d\rho_{\cX}(x^{\prime}) = \sum_{i=1}^E \lambda_i^2$, and it is non-stationary if and only if $\lambda_i \neq \lambda_{i+E/2}$ for all $i=1,â€¦,E/2$. Furthermore, using the argument in \cite[Chapter 4.3]{steinwartSVM}), it holds that 
\begin{align}
C_{\cK} &= \sup_{x_a,x_b \in \cX} \sqrt{\cK(x_a,x_b)} \notag \\ &\leq \sqrt{\sum_{i=1}^{E/2}\max\{\lambda_i,
, \lambda_{i+E/2}\}} < +\infty.
    \label{eq:boundCk}
\end{align}
Being a Mercer kernel, we have from Moore-Aronszajn Theorem \cite{aronszajn50reproducing} that $\cK$ is in one-to-one correspondence with the Hilbert space of functions $(\cH, \langle \cdot,\, \cdot \rangle_{\cH})$, which is 
\begin{equation}
\cH = \{f \in \cL: f(\cdot) = \phi^{\top}(\cdot)\alpha,\; \alpha \in \mathbb{R}^E\}
\label{eq:RKHS}
\end{equation}
with inner product given, for $f^{(\natural)}(\cdot) = \phi^{\top}(\cdot)\alpha^{(\natural)}$ and  $\natural= a ,\, b$: 
\begin{equation}
    \langle f^{(a)},\, f^{(b)} \rangle_{\cH} = \langle \Sigma_{\alpha}^{-1/2}\alpha^{(a)}, \Sigma_{\alpha}^{-1/2}\alpha^{(b)}\rangle_{2}. \label{eq:innerprodH}
\end{equation}
Within the hypothesis space, we can compute the estimate from the data-set $\mathcal{D}$ as follows. Consider the sampling operator $\Ss: \cH \rightarrow \mathbb{R}^N$ such that $\Ss(f) = [f(x_1) \ \dots \ f(x_N)]^{\top}$, together with its adjoint $\Ss^{\top}: \mathbb{R}^N \rightarrow \cH$ yielding $\Ss^{\top}c = \sum_{t=1}^N c_t\cK(x_t,\cdot)$. Thus, considering $Y = [y_1, ..., y_N]^{\top}$ and regularization parameter $\gamma>0$, we have
\begin{align}
    f_z &= \arg\min_{f \in \cH} \frac{1}{N}\sum_{t=1}^N (y_t - f(x_t))^2 + \gamma \|f\|_{\cH}^2 \label{eq:fz}\\
    &= \Big(\frac{1}{N}\Ss^{\top}\Ss + \gamma I \Big)^{-1}\frac{1}{N}\Ss^{\top}Y \label{eq:solSampOp}.
\end{align}
The aim of error analysis is to quantify the discrepancy between $f_z$ and $f_{\rho}$. To this end, we additionally consider the data-free limit of \eqref{eq:fz} as 
\begin{align}
    f_{\cH} &=  \arg\min_{f \in \cH} \int_{\cX}(f(x)-f_{\rho}(x))^2d\rho_{\cX}(x) + \gamma \|f\|_{\cH}^2 \label{eq:probdatafree}\\
    &= (\io + \gamma I)^{-1}\io f_{\rho}, \label{eq:soldatafree}
\end{align}
where $\io(f)(\bar{x})=\int_{\cX} \cK(\bar{x},x)f(x)d\rho_{\cX}(x)$ is an integral operator which, thanks to the properties of $\cK$, is (a) is self-adjoint and strictly positive, (b) continuous and compact, (c) satisties the Spectral Theorem \cite[Theorem 4.3]{cuckerzhou2007} with eigenpairs $\{(\varphi_i(\cdot), \lambda_i)\}_{i=1}^E$. Thanks to these properties, given an arbitrary $\cL$ function $f(x) = \sum_{q\in\mathbb{N}}\alpha_q\bar{\varphi}_q(x)$, using linearity and orthonormality of the basis, we have
\begin{equation}
L_{\cK}(f)(\bar{x}) = \sum_{i=1}^E \lambda_i \alpha_{i}^{\pi} \varphi_i(\bar{x}),
\label{eq:intopker}
\end{equation}
where we define the $i-$th component of the vector $\alpha^{\pi}$ for $i=1,...,E$, along the lines of \eqref{eq:basisRKHS}, as follows:
\begin{equation}
    \text{For }j=1,...,\frac{E}{2},\quad \alpha_i^{\pi} = 
    \begin{cases}
    \alpha_{q_j}^s, \qquad i=j\\
    \alpha_{q_j}^c, \qquad i=j+E/2.
    \end{cases}
    \label{eq:apiRKHS}
\end{equation}
Moreover, thanks to property (a), we can also define the $r$-th power of the integral operator\footnote{Note that the case $r=-1/2$ plays a crucial role in connecting the norms in $\cL$ and $\cH$ for functions in the hypothesis space. Indeed, considering $f(\cdot)= \sum_{i=1}^E \alpha_i\varphi_i(\cdot)$, one has by definition of $\cH$ that $\|f\|_{\cH}^2 = \|\Sigma_{\alpha}^{-1/2}\alpha\|^2_2 = \sum_{i=1}^E \alpha_i^2/\lambda_i$. On the other hand, we have that $\io^{-1/2}(f)(\cdot) = \sum_{i=1}^E \alpha_i/\sqrt{\lambda_i}\varphi_i(\cdot)$, and its $\cL$-norm is equal, by Parseval's Theorem, to $\sum_{i=1}^E \alpha_i^2/\lambda_i$. Therefore, we obtain that $\|f\|_{\cH}^2 = \|\io^{-1/2}f\|_{\cL}^2$.} as \cite{Cucker02}:
\begin{equation}
    L_{\cK}^r(f)(\bar{x}) = \sum_{i=1}^E \lambda_i^r \alpha_{i}^{\pi} \varphi_i(\bar{x}).
    \label{eq:powerIntOp}
\end{equation}




In the following, we study the sample error $\|f_z - f_{\cH}\|_{\cL}$ introduced by the finiteness of the data-set $\mathcal{D}$, and the approximation error $\|f_{\cH} - f_{\rho}\|_{\cL}$ determined by the choice of the hypothesis space. The two bound the overall error as $\|f_z - f_{\rho}\|_{\cL} \leq \|f_z - f_{\cH}\|_{\cL} + \|f_{\cH} - f_{\rho}\|_{\cL}$, which is to be minimized as a function of the regularization parameter~$\gamma$.

\section{SAMPLE ERROR}\label{sec:sampleerror}
In this Section we provide the novel result concerning the error between $f_z$ and $f_{\cH}$ introduced in \eqref{eq:solSampOp} and \eqref{eq:soldatafree}. Its proof can be found in Appendix \ref{sec:proofUniformBound}. \begin{theorem}
Let Assumption \ref{assume:bounds} hold. Consider $C_{\cK}$ as introduced in \eqref{eq:boundCk}, and define $\lmin = \min_{i=1,...,E} \lambda_i$. Then, with confidence at least $1-\delta$, it holds that
\begin{equation}
    \|f_z - f_{\cH}\|_{\cL} \leq \frac{C^3_{\cK}}{\gamma}\sqrt{\frac{B_f^2 + B_{\sigma}^2}{\lmin N\delta}}. \label{eq:uniformbound}
\end{equation}
\hfill$\square$
\label{prop:uniformbound}
\end{theorem}
\begin{remark}
We did not study bounds for $\mathbb{E}_{\cZ}[\rho_N(\|f_z - f_{\cH}\|_{\cL}]$, because they typically return conservative values. A result for unbounded sampling is given, e.g., in \cite[Proposition 20]{Lin2017distributed}. Note also that our probabilistic guarantees fall in the category of ``honest" bounds rather than ``exact" bounds, following the definitions given in \cite{Davies2007NonparametricRC}: this means that, for a user-chosen confidence level $\delta$, the result holds with confidence "at least $1-\delta$" and not with "exact probability $1-\delta$".
\end{remark}

\section{APPROXIMATION ERROR}\label{sec:approximationerror}
We now study the error due the choice of the hypothesis space $\cH$, i.e., the $\cL$-distance between the solution $f_{\cH}$ introduced in \eqref{eq:soldatafree} and the regression function $f_{\rho}$ defined in~\eqref{eq:regressionfunction}. We first provide an expression for $f_{\cH}$: letting the regression function be expressed through the basis functions of $\cL$ as $f_{\rho}(\cdot) = \sum_{q \in \mathbb{N}}\bar{\alpha}_q\bar{\varphi}_q(\cdot)$, and recalling the definition of the RKHS basis functions $\varphi_i(\cdot)$ in \eqref{eq:basisRKHS} and of the coefficients $\alpha_i^{\pi}$ in \eqref{eq:apiRKHS}, we have
\begin{equation}
    f_{\cH}(\cdot) = \sum_{i=1}^E \frac{\lambda_i}{\lambda_i + \gamma}\bar{\alpha}_i^{\pi}\varphi_i(\cdot). 
    \label{eq:lemmafH}
\end{equation}
Thanks to this result, we derive two bounds on the approximation error depending on different norms of the vector $\bar{\alpha}^{\pi}$ defined in \eqref{eq:apiRKHS}. The discussion of their performance is deferred to Section \ref{subsec:bv}. We present the result in the following Proposition, which is proven in Appendix \ref{sec:proofApproxErrs}. 
\begin{proposition}
In the trigonometric linear regression framework presented in Section \ref{sec:problem}, the approximation error $\|f_{\cH} - f_{\rho}\|_{\cL}$ admits the following upper bounds:
\begin{align}
    &\text{(a)}  \qquad \frac{\gamma}{\lmin + \gamma}\|\bar{\alpha}^{\pi}\|_2 + \sqrt{\sum_{q \in \mathbb{N}\setminus Q}\bar{\alpha}^2_q}  \label{eq:approxerror1}\\
    &\text{(b)}  \qquad  \|\bar{\alpha}^{\pi}\|_{\infty}\gamma\sum_{i=1}^E\frac{1}{\lambda_i} + \sqrt{\sum_{q \in \mathbb{N}\setminus Q}\bar{\alpha}^2_q}.\label{eq:approxerror2}
\end{align}
\hfill$\square$
\label{prop:approxerrs}
\end{proposition}

\section{BIAS-VARIANCE TRADE-OFF}\label{sec:biasvar}
In this section we combine the bounds on the sample and approximation errors derived in Theorem \ref{prop:uniformbound} and Proposition \ref{prop:approxerrs}, respectively, and study the estimated overall error~$\|f_z - f_{\rho}\|_{\cL}$ as a function of the regularization parameter~$\gamma$. We perform our analysis after the RKHS $\cH$ has been completely specified, i.e., after having fixed $Q$ and~$\{\lambda_i\}_{i=1}^E$. \\
The main result is presented in the following Proposition proven in Appendix \ref{sec:proofTradeOff}.
\begin{proposition} 
\begin{enumerate}[label=(\alph*)]
\item[]
\item Consider the approximation error bound as in \eqref{eq:approxerror1}. Then, if the number of data $N$ and the confidence parameter $\delta$ are such that
\begin{equation}
    \sqrt{N\delta} > \frac{C_{\cK}^3}{\lmin^{3/2}}\sqrt{\frac{B_f^2 + B_{\sigma}^2}{\sum_{i=1}^E (\bar\alpha_i^{\pi})^2}},
    \label{eq:bvcond}
\end{equation}
there exists a unique $\gamma=\hat{\gamma}_{(a)}$ minimizing the estimated error $\|f_z - f_{\rho}\|_{\cL}$.\\
\item Take now the approximation error bound as in \eqref{eq:approxerror2}. Then, there always exist a unique $\gamma = \hat{\gamma}_{(b)}$ minimizing the estimated error $\|f_z - f_{\rho}\|_{\cL}$. \hfill$\square$
\end{enumerate}

\label{prop:tradeoff}
\end{proposition}
The closed-form expressions for $\hat{\gamma}_{(a)}$ and $\hat{\gamma}_{(b)}$ are provided in the proof.

\section{DISCUSSION}\label{sec:discussion}
We first study the performance of the sample error bound provided in Section \ref{sec:sampleerror} by comparing it with other bounds given in \cite{Smale2007LearningTE} and \cite{Lin2017distributed}. Next, we discuss the result of Proposition \ref{prop:tradeoff}, especially showcasing the capability of $\gamma^{(b)}$ to capture the behaviour of the oracle $\gamma$ minimizing the overall error. 

\subsection{Comparison with sample error bound in \cite[Theorem 5]{Smale2007LearningTE}}\label{sec:discSZ}
In the numerical set-up we assume that a uniformly distributed random noise with a Signal-to-Noise Ratio (SNR) of 150 affects the measurements of the regression function $f_{\rho}(x) = \sum_{q\in\mathbb{N}}\bar{\varphi}_q(x)\bar{\alpha}_q$ with $x \in [-1250,1250]$. Such a function is assumed to be characterized by 20 sine/cosine couples $\{\bar{\varphi}_q\}$, where $q$ is randomly drawn without repetitions from the set $\{1,...,30\}$. The hypothesis space $\cH$ is characterized by a subset of $E/2 = 10$ sine/cosine couples randomly selected among those that define the regression function.\\
We perform a Monte Carlo study of 500 runs, where at each step we draw a new set of basis functions defining the regression function and the hypothesis space.  Coefficients~$\bar\alpha_q$ of the regression function are drawn from a Gaussian distribution $\mathcal{N}(0,\lambda)$, where $\lambda$ is sampled from a uniform distribution on $(0,5)$, and also enters the definition of the hypothesis space as in \eqref{eq:innerprodH} as $\lambda_i = \lambda$ for all $i=1,...,E$. At each run, the number of data-points $N$ is randomly sampled from the set $\{100,101,...,1000\}$. We consider a confidence level of $\delta=0.1$. Then, we evaluate the sample error bounds corresponding to the minimum value of $\gamma$ satisfying the bound in \cite[Theorem 5]{Smale2007LearningTE}, and evaluate their relative difference with respect to the true sample error attained with such a $\gamma$. The results are displayed in Figure \ref{fig:SZandLGZboxplotBounds}. 
Both bounds decay as $1/\sqrt{N}$, but~\eqref{eq:uniformbound} evidences a more favorable behaviour in terms of the confidence level, at least for values of $\delta$ smaller than the solution of $1/\sqrt{\delta} = \log(4/\delta)$ in $(0,1]$, that is $\approx 0.0539$. Conservatism in the bound in \cite[Theorem 5]{Smale2007LearningTE} is mostly due to the linear dependence on the output values bound, $M$.  
The explicit condition on $M$ ensuring bound \eqref{eq:uniformbound} to be more conservative is the following:
\begin{equation}
    M \leq \frac{C_{\cK}^2}{12}\sqrt{\frac{B_f^2 + B_{\sigma}^2}{\lmin \gamma}}\frac{1}{\sqrt{\delta}\log(4/\delta)}. \label{eq:boundM}
\end{equation}
Such a value tends to be very small: e.g., in the Monte Carlo test, the bound \eqref{eq:boundM} returned a mean value of $3.50 \pm 2.02$, while the true value $M$ emerging from the (quite favorable) SNR attained a mean value of $39.02 \pm 14.66$.

\subsection{Comparison with sample error bound in \cite[Proposition 20]{Lin2017distributed}}\label{sec:discLGZ}
We consider the same numerical set-up as the previous section, and we translate the bound of \cite[Proposition 20]{Lin2017distributed} into a statement of the same type as Theorem \ref{prop:uniformbound} by using Markov's inequality. To further adapt to the context given in Section \ref{sec:problem}, we set $p=2$ and~$\mathcal{N}(\gamma) = \sum_{i=1}^E \lambda_i/(\lambda_i + \gamma)$. The bound of \cite[Proposition 20]{Lin2017distributed} shows a slower behaviour in the number of data $N$ with respect to \eqref{eq:uniformbound}; moreover, it depends on the approximation error, which is generally not known. We performed the Monte Carlo study by setting~$\|f_{\cH} - f_{\rho}\|_{\cL}$ to its true value, and the results are very conservative, as displayed in Figure \ref{fig:SZandLGZboxplotBounds}.
\begin{figure}[h]
    \centering
    \input{SZandLGZcomparison.tex}
    %\includegraphics[scale=0.25]{SZcomparison.eps}
    \caption{Behaviour of the sample error bounds in the Monte Carlo trials in Sections \ref{sec:discSZ} and \ref{sec:discLGZ}. The adopted score is the difference between bound and true sample error, normalized by the true sample error. For Theorem \ref{prop:uniformbound}, such an error attains a mean value of $21.44 \pm 4.093$, while for the bound in \cite{Smale2007LearningTE} it is $440.03 \pm 99.33$, and $3.40\times 10^6\pm 3.32 \times 10^6$ for the one in \cite{Lin2017distributed}. We display the values in logarithmic scale to facilitate visualization.  
   }
    \label{fig:SZandLGZboxplotBounds}
\end{figure}

\subsection{On the choice of $\gamma$ in view of the bias-variance trade-off} \label{subsec:bv}
We now perform a Monte Carlo study to discuss the results given in Proposition \ref{prop:tradeoff}. Consider $\cX = [-5\times 10^5,\,5\times 10^5]$ as input domain. The regression function is characterized by 30 sine/cosine pairs $\{\bar{\varphi}\}_{q=1}^{30}$, where each $q$ is randomly selected without repetitions from the set $\{1,...,100\}$, and each component of the vector of coefficients $\bar{\alpha}$ is drawn from a Gaussian random variable with zero mean and variance $\lambda=1$. The latter hyper-parameter also enters the definition of the RKHS $\cH$. The set of frequencies $Q$ is selected as a random subset with cardinality 10 from the set of those characterising the regression function. Fixing an SNR equal to 50, we draw 50 random regression functions and select the basis functions for the hypothesis space. The number of input/output pairs for each run is $N=2500$, and we consider a confidence parameter $\delta=0.5$. For each run, we compute $\gamma^{(a)}$ and $\gamma^{(b)}$ as in Proposition~\ref{prop:tradeoff}, compute the sample- and approximation error bounds as in Theorem~\ref{prop:uniformbound} and Proposition~\ref{prop:approxerrs}, and compare their values to the true errors yielded by $\gamma^{(a)}$ and $\gamma^{(b)}$. 
We observe that the bounds computed with $\gamma^{(a)}$ are closer to the true values. We display the values in Table \ref{tab:bv}.

\begin{table}[!h]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
        $\gamma_{(a)}$ & True value & Bound\\
         \hline
         $\|f_z - f_{\cH}\|_{\cL}$ &$0.036 \pm 0.007$ &$0.419 \pm 0.033$  \\
         \hline
         $\|f_{\cH} - f_{\rho}\|_{\cL}$ \tiny{(a)} &$9.313 \pm 0.077$ &$10.05 \pm 0.992$ \\
         \hline
    %\end{tabular} \\
    %\centering
    %\hspace{0.28em}\begin{tabular}{|c|c|c|}
    \hline
         $\gamma_{(b)}$ & True value & Bound\\
         \hline
         $\|f_z - f_{\cH}\|_{\cL}$  & $0.387 \pm 0.076$ & $14.04 \pm 1.942$ \\
         \hline
         $\|f_{\cH} - f_{\rho}\|_{\cL}$ \tiny{(b)} &$7.351 \pm 0.575$ & $20.41 \pm 2.193$\\
         \hline
    \end{tabular}
    \vspace{0.5em}
    \caption{Overall values (mean $\pm$ standard deviation) of sample and approximation error bounds compared to the true errors.}
    \label{tab:bv}
\end{table} 
The test above described was performed fixing the regularization parameter, and focused on the single errors. If we instead consider the overall error $\|f_z - f_{\rho}\|_{\cL}$, and compare values of $\gamma^{(a)}$ and $\gamma^{(b)}$ with the oracle value $\gamma^*$ (obtained via grid search) minimizing it, we observe that $\gamma^{(b)}$ is the one that performs best. The poor performance of $\gamma^{(a)}$ is due to the fact that the condition in~\eqref{eq:bvcond} needs a large number of data to be satisfied, and this leads to an overestimation of the regularization parameter. In this specific test, $\gamma^*$ was located at the minimum value of the grid, i.e. $\gamma^*=0.1$; the mean values for $\gamma^{(a)}$ and $\gamma^{(b)}$ were $7.7703 \pm 0.2115$ and $0.2308 \pm 0.0230$, respectively. 

\section{CONCLUSIONS}
In this paper, we analysed the estimation errors occurring in regularized trigonometric regression within the statistical learning set-up. To the best of the Authors' knowledge, such a study was missing in the literature, that mostly focused on non-parametric methods or non-regularized trigonometric regression. We derived a novel bound on the sample error that does not require the support of the output distribution to be finite; numerical tests showed it to be less conservative than classical bounds, at least in the non-asymptotic regime. Next, we computed two bounds for the approximation error, and combined them with the sample error bound to retrieve a selection criterion for the regularization parameter~$\gamma$, optimizing the trade-off between estimated bias and variance. In particular, we showed that one of the two criteria yields a value of the regularization parameter that is close to the oracle, and thus can in principle be used to speed up hyper-parameter selection. We stress that such an analysis can be extended to any other orthogonal basis of $\cL$. Moreover, we foresee that the generality of such a set-up can have an impact on an abstract treatment of bias learning, which is a planned extension of the present work. Forthcoming research will also focus on asymptotic behavior in terms of number of data $N$ and of the basis functions $E$.

\bibliographystyle{IEEEtran}
\bibliography{Main}
 \newpage
\onecolumn
\appendix
We provide in Appendix %Section 
\ref{sec:introstatlear} an introduction to error analysis in the statistical learning framework. Appendices %Sections 
\ref{sec:proofUniformBound}, \ref{sec:proofApproxErrs} and \ref{sec:proofTradeOff} report the proofs for the theoretical results stated in Sections \ref{sec:sampleerror}, \ref{sec:approximationerror} and \ref{sec:biasvar}, respectively. For ease of referencing, the statements of the Theorems used as benchmarks in Section \ref{sec:discussion} are given in Appendix %Section 
\ref{sec:statementsdisc}. Appendix %Section 
\ref{sec:discGZ} presents an additional comparison with the sample error bound with unbounded noise support reported in \cite{Wang2011}. In Appendix %Section 
\ref{sec:addendaBV}, we show additional plots related to the experiment of Section \ref{subsec:bv}. A discussion on the benefits of regularization is given in Appendix %Section 
\ref{subsec:regularization}.


\subsection{Statistical learning framework}\label{sec:introstatlear}

Let the metric space of inputs $\cX$ be compact, and let $\cY=\mathbb{R}$ be the space of outputs. There is a probability measure $\rho$ defined over $\cZ = \cX \times \cY$ that decomposes as $\rho(y|x)$ and $\rho_{\cX}(x)$ according to Fubini's Theorem: 
given an integrable function $\psi: \cZ \rightarrow \mathbb{R}$, it holds  
\begin{equation}
   \int_{\cZ} \psi(z)d\rho(z) = \int_{\cX}\Big(\int_{\cY}\psi(x,y)d\rho(y|x)\Big)d\rho_{\cX}(x). \notag
    \label{eq:fubini}
\end{equation}   
Assume to collect $N$ independent samples drawn from $\rho$ in the data-set $\mathcal{D} = \{(x_t,\,y_t)\}_{t=1}^N$. The aim of statistical learning is that of estimating the \textit{regression function} of $\rho$ defined as
\begin{equation}
    f_{\rho}(x) = \int_{\cY}yd\rho(y|x). \notag \label{eq:regressionfunction1}
\end{equation}
Such a function is the minimizer of the expected risk %, or mean squared error,
$\mathcal{I}[f] = \mathbb{E}_{\rho}[(y-f(x))^2] = \int_{\cZ} (y-f_{\rho}(x) + f_{\rho}(x) - f(x))^2d\rho = \mathcal{I}[f_{\rho}] + \int_{\cZ} ( f_{\rho}(x) - f(x))^2d\rho$ \cite{Niyogi99} and can be viewed as the first statistical moment of $\rho(y|x)$. Its variance is defined as
\begin{equation}
\sigma^2_{\rho}(x) = \int_{\cY}(y-f_{\rho}(x))^2d\rho(y|x), \notag
\label{eq:varRhox1}
\end{equation}
whose integral over the $\cX$ domain is %defined as
\begin{equation}
\sigma^2_{\rho} = \int_{\cX} \sigma^2_{\rho}(x)d\rho_{\cX}(x) = \int_{\cZ} (y-f_{\rho}(x))^2d\rho = \mathcal{I}[f_{\rho}]. \notag
\label{eq:varRho1}
\end{equation}
Note that $\sigma^2_{\rho}$ represents the unavoidable cost in the minimization of $\mathcal{I}(f)$ and it measures how well conditioned $\rho$ is: in other words, it is analogous to the notion of condition number in numerical linear algebra \cite{Cucker02}.\\

The regression function is assumed to belong to a certain \textit{target space} $\cT$, but is not computable in practice because $\rho$ is not known. Therefore, the estimate $f_z$ is searched within a \textit{hypothesis space} $\cH$ that is amenable to perform computations. To this aim, $\cH$ is chosen as a Reproducing Kernel Hilbert Space (RKHS) with inner product $\langle \cdot, \, \cdot \rangle_{\cH}$, and the regression function is estimated by solving the following Tikhonov regularization problem:
\begin{equation}
    f_z = \arg\min_{f \in \cH} \frac{1}{N}\sum_{t=1}^N (y_t - f(x_t))^2 + \gamma \|f\|_{\cH}^2. \notag
    \label{eq:fz1}
\end{equation}
The key feature in RKHSs is that function evaluation at any point in the domain $\cX$ is well defined by a functional that is linear and continuous. From Moore-Aronszajn Theorem \cite{aronszajn50reproducing}, it results that the RKHS is in one-to-one correspondence with a positive semi-definite kernel operator (see, e.g., \cite[Definition 2.8]{cuckerzhou2007})
\begin{equation}
\cK:\cX \times \cX \rightarrow \mathbb{R} \notag
\label{eq:kernel1}
\end{equation}
such that the \textit{reproducing property} holds, i.e., $f(x)=\langle \cK(x,\cdot), f\rangle_{\cH}.$ These facts, together with Riesz-Frechet theorem (see, e.g., \cite{berberian1961introduction}[Chapter V, Theorem 1] %or \cite{Rudin}[Theorem 6.19]
) lead to the so-called Representer Theorem \cite{wahba2019representer}, stating that the solution $f_z$ is a linear combination of $\{\mathscr{K}(x_{t},\cdot)\}_{t=1}^{N}$, i.e., of the kernel sections centered at the given input locations in $\mathscr{X}$. 
The same result can be expressed via the \textit{sampling operator} $\Ss$ \cite{smaleshannonII,Smale2007LearningTE}:
\begin{lemma}
Let $\Ss: \cH \rightarrow \mathbb{R}^N$ be an operator such that $\Ss(f) = [f(x_1) \ \dots \ f(x_N)]^{\top}$, and consider its adjoint $\Ss^{\top}: \mathbb{R}^N \rightarrow \cH$ yielding $\Ss^{\top}c = \sum_{t=1}^N c_t\cK(x_t,\cdot)$. Introducing $Y = [y_1 \ \dots \ \textbf{}y_N]^{\top}$, we have
\begin{equation}
f_{z} =  \Big(\frac{1}{N}\Ss^{\top}\Ss + \gamma I \Big)^{-1}\frac{1}{N}\Ss^{\top}Y. \notag
\label{eq:solSampOp1}
\end{equation}
\label{lemma:fz}
\end{lemma}
\textit{Proof.}
Let us begin with the derivation of the expression for $\Ss^{\top}$. By definition of adjoint operator, one must have that $\langle \Ss f, c \rangle_{2} = \langle f, \Ss^{\top}c \rangle_{\cH}$.  Now,  $\langle \Ss f, c \rangle_{2} = \sum_{t=1}^N c_tf(x_t)$, which by the reproducing property is equal to $\sum_{t=1}^N c_t \langle f, \cK(x_t,\cdot)\rangle_{\cH} = \langle f, \sum_{t=1}^N c_t\cK(x_t,\cdot)\rangle_{\cH}$.  By inspection of the definition of adjoint operator, it follows that $\Ss^{\top}c =\sum_{t=1}^N c_t\cK(x_t,\cdot)$. \\

Let us now retrieve the expression for $f_z$. 
Write the objective as 
\begin{align*}
    &\frac{1}{N}\Big(\langle \Ss f, \Ss f \rangle_{2} + \|Y\|_{2}^2 - 2\langle y, \Ss f \rangle_{2}\Big) + \gamma \langle f,f\rangle_{\cH}\\
    =&\Big\langle \Big(\frac{1}{N}\Ss^{\top}\Ss + \gamma I\Big)f,f\Big\rangle_{\cH} - \frac{2}{N}\langle \Ss^{\top}y, f\rangle_{\cH} + \frac{1}{N}\|Y\|_{2}^2.
\end{align*}
Solution follows by taking the functional derivative of the last expression. \hfill$\blacksquare$


The primary question of interest in error analysis is about quantifying how well $f_z$ approximates $f_{\rho}$. To this aim, we introduce the data-free limit of $f_z$ as 
\begin{equation}
    f_{\cH} = \arg\min_{f \in \cH} \int_{\cX}(f(x)-f_{\rho}(x))^2d\rho_{\cX}(x) + \gamma \|f\|_{\cH}^2.\notag
    \label{eq:probdatafree1}
\end{equation}
Its solution is given by means of the \textit{integral operator} 
\begin{equation}\label{eq:intOp}
\io(f)(\bar{x}) = \int_{\cX} \cK(\bar{x},x)f(x)d\rho_{\cX}(x), \notag
\end{equation}
and its expression is (\cite{Cucker02}, II.2, Theorem 3; and III.6, Proposition 7) \begin{equation}
f_{\cH} = (\io + \gamma I)^{-1}\io f_{\rho}. \notag
\label{eq:soldatafree1}
\end{equation}
Having defined $f_{\rho}$, $f_z$ and $f_{\cH}$, consider the metric $\|f_z - f_{\rho}\|_{\natural}$, where $\natural$ indicates the type of norm of interest. The overall error thus decomposes as
\begin{equation}
    \|f_z - f_{\rho}\|_{\natural} \leq \|f_z - f_{\cH}\|_{\natural} + \|f_{\cH} - f_{\rho}\|_{\natural}. \notag
    \label{eq:overallerror}
\end{equation}
The first addendum is named \textit{sample error}, and indicates the error within the RKHS $\cH$ due to the fact that we are operating with a finite amount of data. The second is called \textit{approximation error} and arises from the choice of the hypothesis space. When considering the error in the space of square-integrable functions, the two terms are also called \textit{variance} and \textit{bias}, respectively. The choice on the size of $\cH$ has an opposite effect on them: the larger the hypothesis space is, the smaller the distance from $f_{\cH}$ to $f_{\rho}$ can be; on the other hand, the more complex the model is, the more data are required to fit it. 

\begin{remark}
The statistical learning viewpoint was originally juxtaposed to the so-called \textit{sampling theory} approach for function estimation \cite{smaleshannonI,Niyogi99}. The first can be viewed as a more flexible framework to perform error analysis, and comprises the latter as a special case. Indeed, one could envisage $f_{\rho}$ as the ``true" function to be estimated, assuming data are generated as $y_t = f_{\rho}(x_t) + e_t$ and having noises with zero mean and variance $\sigma_{\rho}^2(x_t)$. For further comments, please refer to \cite[Section 7]{smaleshannonII}. 
\end{remark}

\subsection{Proof of Theorem \ref{prop:uniformbound}}\label{sec:proofUniformBound}
Defining $\xi_t : \cZ \rightarrow \cH$ such that $\xi_t(\cdot) = (y_t - f_{\cH}(x_t))\cK(x_t,\,\cdot)$, it holds that $\mathbb{E}_{\cZ}[\xi_t](\cdot) = L_{\cK}(f_{\rho} - f_{\cH})(\cdot) = \gamma f_{\cH}(\cdot)$. From this, and recalling the definition of sampling operator, it follows that $f_z(x) - f_{\cH}(x)$ is equal to \cite{Smale2007LearningTE}
\begin{equation}
   % f_z(x) - f_{\cH}(x) = 
   \Big(\frac{1}{N}\Ss^{\top}\Ss + \gamma I \Big)^{-1}\Bigg[\frac{1}{N}\sum_{t=1}^N \xi_t(x) - \mathbb{E}_{\cZ}[\xi](x)\Bigg].\notag
\end{equation}
We can now study the $\cL-$ norm of the expression above. Since $\cX$ is compact and the measure $\rho_{\cX}$ on it defined is a probability measure, $\|f\|_{\cL} \leq \|f\|_{\infty}$ for any function $f \in \cL$: therefore, $\|f_z - f_{\cH} \|_{\cL}$ is upper bounded by
\begin{equation}
  %\|f_z - f_{\cH} \|_{\cL} \leq
  \Big\|\Big(\frac{1}{N}\Ss^{\top}\Ss + \gamma I \Big)^{-1}\Big\|_{\infty}\Big\|\frac{1}{N}\sum_{t=1}^n\xi_t - \mathbb{E}_{\cZ}[\xi]\Big\|_{\infty}.  \notag
\end{equation}
Since the operator norm can be bounded by $\frac{C_{\cK}}{\gamma\sqrt{\lmin}}$ (the proof is reported at the end of this subsection), we can now study an upper bound for $\rho_{N}(\|f_z - f_{\cH}\|_{\cL} > \epsilon)$ which, for an arbitrary $\epsilon >0$, is
\begin{equation}
    \rho_{N}\Bigg(\Big\|\frac{1}{N}\sum_{t=1}^n\xi_t - \mathbb{E}_{\cZ}[\xi]\Big\|_{\infty}   > \frac{\epsilon\gamma \sqrt{\lmin}}{C_{\cK}} \Bigg).
    \label{eq:boundsrhoN}
\end{equation}
At an arbitrary input location $x \in \cX$ and a given $\bar{\epsilon} \in (0,1)$, Chebychev's inequality yields
\begin{equation}
    \rho_N\Big(\Big|\frac{1}{N}\sum_{t=1}^N \xi_t(x) - \mathbb{E}_{\cZ}[\xi](x)\Big|  > \bar{\epsilon}\Big) \leq
    \frac{\Var(\xi)(x)}{N\bar{\epsilon}^2}, \notag
\end{equation}
noting that $\{\xi_t\}_{t=1}^N$ are independent and identically distributed. Using this result, we can further bound \eqref{eq:boundsrhoN} as
\begin{equation}
    \rho_{N}(\|f_z - f_{\cH}\|_{\cL} > \epsilon) \leq \frac{C_{\cK}^2}{\gamma^2 \lmin}\frac{\|\Var(\xi)\|_{\infty}}{N\epsilon^2}.
    \label{eq:chebychevour}
\end{equation}
The variance term can be bounded as 
\begin{align*}
    \sup_{\bar{x}\in \cX} \Var(\xi)(\bar{x}) &\leq  \sup_{\bar{x}\in \cX} \int_{\cZ}\cK(\bar{x},x)^2(y - f_{\cH}(x))^2d\rho \leq C_{\cK}^4\int_{\cZ}(y - f_{\cH}(x))^2d\rho \leq B_f^2 + B_{\sigma}^2, \notag
\end{align*}
where the last inequality follows from the fact that $\int_{\cZ}(f(x)-y)^2d\rho - \int_{\cZ}(f_{\rho}(x)-y)^2d\rho = \|f - f_{\rho}\|^2_{\cL}$ for any $f:\cX \rightarrow \cY$ \cite{Smale2007LearningTE}, and that $\|f_{\cH} - f_{\rho}\|^2_{\cL} + \gamma\|f_{\cH}\|^2_{\cH} = \mathcal{J}(f_{\cH}) \leq \mathcal{J}(0)   = \|f_{\rho}\|^2_{\cL} \leq B_f^2$.
Coming back to \eqref{eq:chebychevour}, we have that 
\begin{equation}
    \rho_{N}\Big(\|f_z - f_{\cH}\|_{\cL} > \epsilon\Big) \leq \frac{C_{\cK}^6}{\gamma^2 \lmin}\frac{(B_f^2 + B_{\sigma}^2)}{N\epsilon^2} = \delta.
    \label{eq:lastep}
\end{equation}
The proof is concluded by retrieving the expression for $\epsilon$ from $\delta$ in the equality \eqref{eq:lastep}. \hfill$\blacksquare$
\paragraph*{Proof for operator norm bound} By definition, we look for a constant $\mathfrak{C}_{\infty}$ is such that, for any $u \in \cH$,  $\|(\Ss^{\top}\Ss/N + \gamma I)^{-1}u\|_{\infty} \leq \mathfrak{C}_{\infty}\|u\|_{\infty}$. By the reproducing property, $\Big\|\Big(\frac{1}{N}\Ss^{\top}\Ss + \gamma I \Big)^{-1} u\Big\|_{\infty} = \sup_{\bar{x} \in \cX} \Big| \Big\langle \Big(\frac{1}{N}\Ss^{\top}\Ss + \gamma I \Big)^{-1} u(\cdot), \cK(\bar{x},\cdot)  \Big\rangle_{\cH} \Big|$, which is further upper bounded by $C_{\cK}\Big\|\Big(\frac{1}{N}\Ss^{\top}\Ss + \gamma I \Big)^{-1}\Big\|_{\cH}\|u\|_{\cH}$ by Cauchy-Schwartz inequality and \eqref{eq:boundCk}. Now, by the bound on the operator norm in $\cH$ provided in \cite[Equation 3.5]{Smale2007LearningTE}, we have $\Big\|\Big(\frac{1}{N}\Ss^{\top}\Ss + \gamma I \Big)^{-1} u\Big\|_{\infty} \leq \frac{C_{\cK}}{\gamma}\|\io^{-1/2}\|_{\cL}\|u\|_{\cL} \leq \frac{C_{\cK}}{\gamma}\|\io^{-1/2}\|_{\cL}\|u\|_{\infty}$. The proof is concluded by deriving the operator norm for $\|\io^{-1/2}\|_{\cL}$, which is $\|\io^{-1/2}\|_{\cL} \leq 1/\sqrt{\lmin}$ because, for an arbitrary $f \in \cL$, $\|\io^{-1/2} f\|_{\cL} = \sqrt{\sum_{i=1}^E \frac{\alpha_i^2}{\lambda_i}} \leq \sqrt{\frac{1}{\lmin}\sum_{i=1}^E \alpha_i^2} \leq \frac{1}{\sqrt{\lmin}}\|f\|_{\cL}$. 

\subsection{Proof of Proposition \ref{prop:approxerrs}}\label{sec:proofApproxErrs}
Expressing the regression function as $f_{\rho} = \sum_{q\in\mathbb{N}}\bar{\alpha}_q\bar{\varphi}_q$ and $f_{\cH}$ as in \eqref{eq:lemmafH}, we apply the triangle inequality and Parseval's Theorem on $\|f_{\cH} - f_{\rho}\|_{\cL}$ and obtain
\begin{align}
    \|f_{\cH} - f_{\rho}\|_{\cL} &= \Big\|\sum_{i=1}^E \frac{\lambda_i}{\lambda_i + \gamma}\bar{\alpha}^{\pi}_{i}\varphi_i - \sum_{q \in \mathbb{N}}\bar{\alpha}_q\bar{\varphi}_q \Big\|_{\cL} 
    \leq \sqrt{\sum_{i=1}^E \Bigg(\frac{\gamma}{\lambda_i + \gamma}\Bigg)^2(\bar{\alpha}_{i}^{\pi})^2} + \sqrt{\sum_{q \in \mathbb{N}\setminus Q}\bar{\alpha}_{q}^2}. \notag
\end{align}
Let us now focus on the first term on the right-hand side. The first bound \eqref{eq:approxerror1} is obtained by considering $(\lambda_i + \gamma)^{-1} \leq (\lmin + \gamma)^{-1}$. As for the second, we take $\bar{\alpha}^{\pi}_{i} \leq \|\bar{\alpha}^{\pi}\|_{\infty}$, bound the square root of the sum as the sum of the square roots, and take $(\lambda_i + \gamma)^{-1} \leq (\lambda_i)^{-1}$.

\subsection{Proof of Proposition \ref{prop:tradeoff}}\label{sec:proofTradeOff}
(a) Consider the sample and approximation errors as obtained in \eqref{eq:uniformbound} and \eqref{eq:approxerror1}, respectively. 
Introducing the following notation:
\begin{equation}
A = C_{\cK}^3\sqrt{\frac{B_f^2 + B_{\sigma}^2}{N\delta\lmin}}, \quad b = \lmin, \quad
        B = \sqrt{\sum_{i=1}^E (\bar{\alpha}^{\pi}_{i})^2}, \quad C = \sqrt{\sum_{q \in \mathbb{N}\setminus Q}\bar\alpha_q^2},
\label{eq:Abbv}
\end{equation}
we have that the overall error can be bounded as follows:
\begin{equation}
    \|f_z - f_{\rho}\|_{\cL} \leq \frac{A}{\gamma} + \frac{B\gamma}{b + \gamma} + C = F(\gamma).
    \label{eq:symbBound}
\end{equation}
The function $F(\gamma)$ is always positive for $\gamma > 0$.
We aim at finding the condition for which there exists a unique, finite value of $\gamma$ minimizing $F(\gamma)$. To this end, let us study the first derivative:
\begin{equation}
   \frac{dF}{d\gamma} = 0 \longrightarrow \gamma^2(Bb-A) - 2Ab\gamma - Ab^2 = 0. \label{eq:firstDerBV}
\end{equation}
By applying Descartes' rule, we obtain that the condition ensuring a unique root on the positive real axis is $Bb-A > 0$, which is \eqref{eq:bvcond}. Such a condition implies the existence of a unique flexus on $\gamma>0$: this follows from the fact that 
\begin{equation}
\lim_{\gamma \rightarrow +\infty} F(\gamma) = B+C,\notag
\end{equation}
but the claim can be also verified by applying Descartes' rule on $\frac{d^2F(\gamma)}{d^2\gamma}$. \\Finally, the optimal $\gamma$ is obtained by solving \eqref{eq:firstDerBV} and has the following expression:
\begin{equation}
    \hat{\gamma}_{(a)} = \frac{b(A + \sqrt{ABb})}{Bb-A}.\notag
\end{equation}\\
(b) We proceed along the lines of the preceding argument, but considering the approximation error bound as in \eqref{eq:approxerror2}. Considering the following coefficients:
\begin{equation}
    A \text{ as in \eqref{eq:Abbv},} \qquad D = \sum_{i=1}^E \frac{\|\bar{\alpha}^{\pi}\|_{\infty}}{\lambda_i},
\end{equation}
the claim follows by proving that the function $F(\gamma) = \frac{A}{\gamma} + D\gamma$ has a unique minimum for $\gamma >0$. This is shown by studying the first and second derivatives, and using the fact that both $A$ and $D$ are positive. \\
The resulting optimal $\gamma$ always exists and takes the following value:
\begin{equation}
    \hat{\gamma}_{(b)} = \sqrt{\frac{A}{D}}.\notag
\end{equation}

\subsection{Statement of benchmark Theorems in Section \ref{sec:discussion}}\label{sec:statementsdisc}
For ease of referencing, we report the statements of \cite[Theorem 5]{Smale2007LearningTE} and \cite[Proposition 20]{Lin2017distributed} used in Sections \ref{sec:discSZ} and \ref{sec:discLGZ}, respectively.
\begin{theorem}[\cite{Smale2007LearningTE}, Theorem 5]
Let $\rho$ satisfy $|y| \leq M$ almost surely. Then for any $0 < \delta < 1$ %there holds
\begin{equation}
  \|f_z - f_{\cH}\|_{\cL} \leq \frac{12 C_{\cK}M \log(4/\delta)}{\sqrt{N\gamma}}
    \label{eq:SZbound}
\end{equation}
provided that
\begin{equation}
\gamma \geq \frac{8C_{\cK}^2\log(4/\delta)}{\sqrt{N}}.
    \label{eq:SZgamma}
\end{equation}
\hfill$\square$
\end{theorem}


\begin{proposition}[\cite{Lin2017distributed}, Proposition 20] 
Assume $\mathbb{E}[y^2] < \infty$ and that $\sigma_{\rho}^2 \in \mathscr{L}^p_{\cX}$ for some $1 \leq p \leq \infty$. Moreover, define the effective dimension $\mathcal{N}(\gamma) = \textup{Tr}((L_{\cK} + \gamma I)^{-1}L_{\cK})$~\cite{zhang2005effdim}. Then, 
\begin{align}\label{eq:LGZexpval}
    \mathbb{E}_{\cZ} [\|f_z - f_{\cH}\|_{\cL}] \leq& (2 + 56 C_{\cK}^4 + 57C_{\cK}^2) \Big(1 + \frac{1}{(N\gamma)^2} + \frac{\mathcal{N}(\gamma)}{N\gamma}\Big) \notag \\
    &\times \Bigg\lbrace C_{\cK}^{\frac{1}{p}}\sqrt{\|\sigma_{\rho}\|_p}\Big(\frac{\mathcal{N}(\gamma)}{N}\Big)^{\frac{1}{2}(1-\frac{1}{p})}\Big(\frac{1}{N\gamma}\Big)^{\frac{1}{2p}} + \notag  + C_{\cK}\frac{\|f_{\cH} - f_{\rho}\|_{\cL}}{\sqrt{N\gamma}} \Bigg\rbrace.
\end{align}
\hfill$\square$
\end{proposition}



\subsection{Comparison with sample error bound in \cite{Wang2011}}\label{sec:discGZ}
We now perform another Monte Carlo study in another set-up where noises have unbounded support. We compare bound~\eqref{eq:uniformbound} with the following:
\begin{theorem}[\cite{Wang2011}, Theorem 1]
Assume there exist constants $\tilde{M}>0$ and $C>0$ such that the so-called moment hypothesis holds:
\begin{equation}
    \int_{\cY}|y|^{\ell}d\rho(y|x) \leq C\ell!\tilde{M}^{\ell} \qquad \forall \ell \in \mathbb{N},\, x\in\cX. \label{eq:momenthypothesis}
\end{equation}
Furthermore, assume that there exists some $0<\beta\leq 1$ and a constant $C_{\beta} > 0$ such that
\begin{equation}
    \|f_{\cH} - f_{\rho}\|_{\cL}^2 + \gamma\|f_{\cH}\|_{\cH}^2 \leq C_{\beta}\gamma^{\beta}. \label{eq:Cbeta}
\end{equation}
Then, if the kernel $\cK$ is infinitely differentiable on $\cX\times \cX$, then for any $0<\varepsilon<1$ and $0<\delta<1$, with confidence $1-\delta$ we have, by taking $\gamma = N^{\varepsilon - 1}$,
\begin{equation}
    \|f_z - f_{\cH}\|_{\cL}^2 \leq \tilde{C}_{\varepsilon}N^{\varepsilon-1}\log(4/\delta)^{\frac{4}{\varepsilon} + 2}.\label{eq:GZbound}
\end{equation}
\hfill$\square$
\label{theorem:GZ}
\end{theorem}

Before presenting the details of the numerical experiment, we derive the expressions for $C$, $\tilde{M}$, $C_{\beta}$ and $\tilde{C}_{\varepsilon}$. Theorem \ref{theorem:GZ} is a corollary of the following result:
\begin{theorem}[\cite{Wang2011}, Theorem 2]
Assume the moment hypothesis \eqref{eq:momenthypothesis} with constants $C$ and $\tilde{M}$ holds; moreover, let condition \eqref{eq:Cbeta} with $0<\beta \leq 1$ and constant $C_{\beta}$ be valid. Define $\mathscr{N}(\mathcal{B}_1,\eta)$ the minimum number of disks with radius $\eta$ that cover the balls $\mathcal{B}_1 = \{f \in \cH \: : \: \|f\|_{\cH} \leq 1 \}$, and assume that $\cH$ has polynomial complexity exponent $s>0$, i.e., 
\begin{equation}
    \log\mathscr{N}(\mathcal{B}_1,\eta) \leq C_0 \Big(\frac{1}{\eta}\Big)^s.\label{eq:polycomplexity}
\end{equation}
If $0 < \varepsilon < \frac{\beta}{1+s}$, then by taking $\gamma = N^{\frac{\varepsilon}{\beta} - \frac{1}{s+1}}$, for any $0<\delta<1$, with confidence $1-\delta$ we have
\begin{equation}
    \|f_z - f_{\rho}\|_{\cL}^2 \leq \tilde{C}_{\varepsilon}N^{\varepsilon - \frac{\beta}{s+1}}\Big(\log\Big(\frac{4}{\delta}\Big)\Big)^{\frac{\beta(1+\beta)}{(s+1)\varepsilon}+2}.
\end{equation}
The constant $\tilde{C}_{\varepsilon}$ can be computed as follows:
\begin{equation}
    \tilde{C}_{\varepsilon} = \frac{C_5}{\varepsilon^2}C_2^{\frac{4\beta}{\varepsilon(s+1)}}\Big(1 + \log\Big(1 + \frac{2}{\varepsilon(s+1)}\Big) \Big)^{\frac{\beta(1+\beta)}{\varepsilon(s+1)}+2}, \notag
\end{equation}
where we have
\begin{equation}
    \begin{cases}
    C_5 &= 38C_{\beta} + 2(C_1 + 32^2(C+1)^2)C_4^2(2/(s+1))^2 + 480(C_{\cK}+1)^2C_{\beta}\\
    C_4 &= \tilde{M}(2C_{\cK}(C + (1 + 2\sqrt{2C}) +1)) + C_3\\
    C_3 &= \sqrt{38C_{\beta}} + (C_{\cK} + 1)\sqrt{480 C_{\beta}} + \tilde{M}\\
    C_2 &= \sqrt{2[C_1 + 32^2(C+1)^2]}\\
    C_1 &= 6C_{\cK} + 6C + 8(1 + \sqrt{2C})/\tilde{M} + 520 (C_{\cK} + C + 2(C+1))^2(C_0+1).
    \end{cases}
\end{equation}
\hfill $\square$
\label{theorem:completeGZ}
\end{theorem}

The choice in \cite{Wang2011} to obtain the statement of Theorem \ref{theorem:GZ} from the result above presented is to assume that $f_{\rho}$ belongs to the hypothesis space $\cH$, and setting $\beta=1$ and $s = \frac{\varepsilon}{1 - \varepsilon}$ with $0 < \varepsilon < 1/2$ (and then rescaling $2\varepsilon$ to $\varepsilon$). Our task is now to obtain explicit values for the constants $C$, $\tilde{M}$, $C_{\beta}$ and $C_0$. The bound for $C_{\cK}$ is given in \eqref{eq:boundCk}.

\paragraph*{Constants for moment hypothesis \eqref{eq:momenthypothesis} ($C$ and $\tilde{M}$)}
Following Example 1 in \cite{Wang2011}, we obtain that the moment hypothesis \eqref{eq:momenthypothesis} is satisfied for $C=4$ and $\tilde{M} = \max\{\sqrt{B_0},B_{\infty}\}$, where $B_0 = B_{\sigma}^2$ (see Assumption \ref{assume:bounds}) and $B_{\infty} \geq \|f_{\rho}\|_{\infty}$. Note that $B_{\infty} < \infty$ because $f_{\rho}$ belongs to $\cH$.

\paragraph*{Constant entering bound \eqref{eq:Cbeta} ($C_{\beta}$)}
%One needs to operate 
We evaluate the cost \eqref{eq:probdatafree} at $f_{\cH} = (I + \gamma\io^{-1})^{-1}f_{\rho}$. We then obtain
\begin{align*}
    \|f - f_{\rho}\|_{\cL}^2 + \gamma\|\io^{-1/2}f\|_{\cL}^2 &= \|[(I + \gamma\io^{-1})^{-1} - I]f_{\rho}\|_{\cL}^2 + \gamma\|\io^{-1/2}(I + \gamma\io^{-1})^{-1}f_{\rho}\|_{\cL}^2\\
    &=\sum_{i=1}^E \Big[\Big(\frac{\lambda_i}{\lambda_i + \gamma} - 1\Big)^2 + \gamma\Big(\frac{\sqrt{\lambda_i}}{\lambda_i + \gamma} \Big)^2 \Big](\bar{\alpha}_i^{\pi})^2\\
    &= \gamma\sum_{i=1}^E \frac{1}{\lambda_i + \gamma}(\bar{\alpha}_i^{\pi})^2 = \gamma\sum_{i=1}^E \Big( \frac{\lambda_i^{\beta}}{\lambda_i + \gamma}\Big)\lambda_i^{-\beta}(\bar{\alpha}_i^{\pi})^2
\end{align*}
for any $\beta$ in $(0,1]$. We can bound such an expression as follows (see Theorem 3, Chapter II.2 in \cite{Cucker02} for the whole derivation):
\begin{equation}
   \gamma\sum_{i=1}^E \Big( \frac{\lambda_i^{\beta}}{\lambda_i + \gamma}\Big)\lambda_i^{-\beta}(\alpha_i^{\pi})^2 \leq \gamma \Big(\sup_{\tau} \frac{\tau^{\beta}}{\tau + \gamma} \Big)\|\io^{-\beta/2}f_{\rho}\|_{\cL}^2 \leq \gamma^{\beta}\|\io^{-\beta/2}f_{\rho}\|_{\cL}^2.\notag
\end{equation}
Therefore, the constant $C_{\beta}$ is obtained by bounding $\|\io^{-\beta/2}f_{\rho}\|_{\cL}^2$. In the case $\beta=1$, which is of our interest, a solution could be $C_{\beta=1} \leq B_f^2/\lmin$.

\paragraph*{Constant for polynomial complexity \eqref{eq:polycomplexity} ($C_0$)}
Since $\cK$ in \eqref{eq:ourkernel} is infinitely differentiable, condition \eqref{eq:polycomplexity} is known to hold for any $s > 0$; however, since we are considering $s = \frac{\varepsilon}{1 - \varepsilon}$ with $0 < \varepsilon < 1/2$, we are restricting our attention to $0<s<1$.\\ 
Since $\cH$ is finite-dimensional with dimension $E$, Theorem 5.3 \cite{cuckerzhou2007} gives that $\mathscr{N}(\mathcal{B}_1,\eta) \leq (1 + 2/\eta)^E$. Taking logarithms on both sides, we obtain
\begin{equation*}
    \log\mathscr{N}(\mathcal{B}_1,\eta) \leq E\log\Big(1 + \frac{2}{\eta}\Big) \leq 2EG(s)\Big(\frac{1}{\eta}\Big)^s.
\end{equation*}
We now need to find $G(s)$. Its expression is summarized in the following result.
\begin{lemma}
Consider $x>0$. In order to have $\log(1+x) < G(s)x^s$ for $0<s<1$, it has to hold that $$G(s) > \Big(\frac{1-s}{s}\Big)^{1-s}.$$
\end{lemma}
\textit{Proof. } 
Consider the function $g(x) = G(s)x^s - \log(1+x)$. We want it to be always positive for $x>0$. Since $g(0) = 0$, this amounts to imposing that $\frac{dg(x)}{dx}$ is always positive. %\anna{Thus, we have} 
So, studying the first derivative, we have
\begin{equation}
\begin{aligned}
    &\frac{dg(x)}{dx} = \frac{G(s)s(x+1)-x^{1-s}}{x^{1-s}(1+x)} > 0 \longrightarrow \frac{x}{(1+x)^{\frac{1}{1-s}}} < \Big(G(s) s\Big)^{\frac{1}{1-s}}.
    \end{aligned}
    \notag
\end{equation}
The claim follows by maximising the term on the left-hand side.
\hfill $\blacksquare$

We are now ready to perform the numerical test. We randomize both on the regression function and on the number of data-points. The first is drawn in the same way as in Section~\ref{sec:discSZ}, while data-set cardinalities $N$ %\elena{dimensions?} 
are drawn from the set $\{300, 315,...,6990\}$. As in Section~\ref{sec:discSZ}, the SNR is set to 150, but noises are now distributed as Gaussian. The hyperparameters $\{\lambda_i\}_{i=1}^{20}$ 
ruling both the sampling of $\bar{\alpha}_q$ and the hypothesis space $\cH$ are all set to 10. The numerical values of the sample error bounds are computed with $\gamma$ set as in the statement of Theorem \ref{theorem:GZ}. For each Monte Carlo run, we evaluate the bounds with $\varepsilon$ in the grid $\{0.05,0.1,...,0.95\}.$ To compare the numerical values of the bounds, the score we consider is the difference between bound and true sample error, divided by the true sample error. The results are displayed in Figure \ref{fig:WZ}. From~\eqref{eq:GZbound} it is clear that the convergence rate in the number of data $N$ is better compared with that presented in~\eqref{eq:uniformbound}; however, the numerical values returned by the choice of $\gamma$ are extremely conservative.
\begin{figure}[h!]
    \begin{minipage}{0.5\textwidth}
    \centering
    \input{compWZ2.tex}
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
    \centering
    \input{compWZ1.tex}
    \end{minipage}
    \caption{Statistics of the difference between bound and true sample error, normalized by true sample error, in logarithmic scale. Top panel: behaviour of the mean value over the Monte Carlo iterations as a function of $\varepsilon$. Bottom panel: boxplots over the Monte Carlo runs for the case $\varepsilon=0.95$. The values (in logarithmic scale) returned by Theorem \ref{prop:uniformbound} are $6.238 \pm 4.694$, while the ones obtained using Theorem \ref{theorem:GZ} are $21.961 \pm 21.421$.}
    \label{fig:WZ}
\end{figure}

\subsection{Additional results for the tests in Section \ref{subsec:bv}}\label{sec:addendaBV}
We recall that the numerical values chosen for the Monte Carlo test were the following:
\begin{itemize}
    \item $\cX = [-5\times 10^4,\,5\times 10^4]$;
    \item $\delta = 0.5$;
    \item Regression function $f_{\rho} = \sum_{q}\bar{\alpha}_q\bar{\varphi}_q$ characterized by 30 sine/cosine pairs $\{\bar{\varphi}\}_{q=1}^{30}$, where each $q$ is randomly selected without repetitions from the set $\{1,...,100\}$;
    \item all components of $\bar{\alpha} \in \mathbb{R}^{30}$ are independent samples from a Gaussian distribution with zero mean and variance $\lambda=1$;
    \item in the hypothesis space, $\lambda_i = \lambda$ for all $i=1,...,E$;
    \item $Q$ is selected as a random subset with cardinality 10 from the set of frequencies characterising the regression function;
    \item SNR = 50;
    \item at each iteration, we draw a random regression function and select the basis functions of $\cH$;
    \item $N = 2500.$
\end{itemize}

We further display the results reported in Table \ref{tab:bv} in Figure \ref{fig:BVs}. In each boxplot, we consider the difference between the bound and the true error, and we normalize it by the latter. 



\begin{figure}[H]
   \begin{minipage}{0.5\textwidth}
   \centering
   \input{BV_sample.tex}
   \end{minipage}
   \begin{minipage}{0.5\textwidth}
   \centering
   \input{BV_approx.tex}
   \end{minipage}
    \caption{Boxplots of the normalized, relative differences between bounds and true values. Left panel: results for the sample error; right panel: results for the approximation errors. These complement the results presented in Table \ref{tab:bv}, Section \ref{subsec:bv}.}
    \label{fig:BVs}
\end{figure}



\subsection{The benefits of regularization: the case of additive noise model} \label{subsec:regularization}
We now discuss the performance of the estimation scheme proposed in \eqref{eq:fz} with respect to the one that would have been obtained without regularization. We carry out the analysis assuming an additive noise model, and regarding the regression function as the "true" function to be estimated from data. In this setting, the measurements model for each $t=1,...,N$ is $y_t = \phi^{\top}(x_t)\bar{\alpha}^{\pi} + r(x_t) + e_t$, where the first term is given by the projection of the regression function on the subspace spanned by the basis functions $\{\varphi_i(\cdot) \}_i\}_{i=1}^E$ defined in \eqref{eq:basisRKHS} and entering $\phi^{\top}(\cdot)$; the second collects the contribution of frequencies that have not been included in the subspace and is assumed to be bounded; the last one is the additive noise, which is assumed to be i.i.d.~with zero mean and known variance $\sigma^2$. The overall estimation problem to be solved reads as follows:
\begin{equation}
    \hat{\alpha} = \arg\min_{\alpha \in \mathbb{R}^E} \|Y - \Phi \alpha\|^2 + N\sigma^2 \alpha^{\top}P^{-1}\alpha, \label{eq:alphahat}
    \end{equation}
where $\Phi \in \mathbb{R}^{N\times E}$ stacks all $\{\phi^{\top}(x_t)\}_{t=1}^N$, $Y$ is defined as the one entering \eqref{eq:solSampOp}, and $P^{-1}$ is a regularization matrix. The solution \eqref{eq:alphahat} reads as 
\begin{equation}
    \hat{\alpha} = %P\Phi^{\top}(\Phi P \Phi^{\top} + N\sigma^2 I)^{-1}Y = 
    (\Phi^{\top}\Phi + N\sigma^2 P^{-1})^{-1}\Phi^{\top}Y. \label{eq:hatalpha}
\end{equation}
Our goal is to discuss the performance of such an estimator when considering $P^{-1} = \frac{\gamma \Sigma_{\alpha}^{-1}}{\sigma^2}$, which directly relates to \eqref{eq:fz}, as a function of $\gamma$. To do so, we consider $\mathcal{M}(\gamma)=\mathbb{E}_e[\|\hat{\alpha} - \bar{\alpha}^{\pi}\|^2]$, where $\mathbb{E}_e[\cdot]$ denotes expectation with respect to the noise distribution, as a performance score. Carrying out the computations with the particular choice of the regularizer above introduced, and defining $r = [r(x_1), ..., r(x_N)]^{\top}$ and $\tilde{\Sigma}_{\alpha} = \Sigma_{\alpha}/N$, it turns out that
\begin{align}
    \mathcal{M}(\gamma&) = \text{Tr}\Bigg((\gamma\tilde{\Sigma}_{\alpha}^{-1} + \Phi^{\top}\Phi)^{-1}\Big[ \sigma^2\Phi^{\top}\Phi + \gamma^2\tilde{\Sigma}_{\alpha}^{-1}\bar{\alpha}^{\pi}(\bar{\alpha}^{\pi})^{\top}\tilde{\Sigma}_{\alpha}^{-1} + \Phi^{\top}rr^{\top}\Phi + 2\gamma\tilde{\Sigma}_{\alpha}^{-1}\bar{\alpha}^{\pi}r^{\top}\Phi\Big](\gamma\tilde{\Sigma}_{\alpha}^{-1} + \Phi^{\top}\Phi)^{-1} \Bigg).
    \label{eq:MSEalpha}
\end{align}
Note that $\mathcal{M}(0) = \text{Tr}((\Phi^{\top}\Phi)^{-1}[\Phi^{\top}rr^{\top}\Phi + \sigma^2\Phi^{\top}\Phi](\Phi^{\top}\Phi)^{-1})$. 
Denoting with $R$ the expression in square brackets in \eqref{eq:MSEalpha}, we obtain
\begin{align}
    \frac{d\mathcal{M}(\gamma)}{d\gamma}=2\text{Tr}\Bigg((\gamma\tilde{\Sigma}_{\alpha}^{-1} + \Phi^{\top}\Phi)^{-1}\Big[\gamma\tilde{\Sigma}_{\alpha}^{-1}\bar{\alpha}^{\pi}(\bar{\alpha}^{\pi})^{\top}\tilde{\Sigma}_{\alpha}^{-1} + \tilde{\Sigma}_{\alpha}^{-1}\bar{\alpha}^{\pi}r^{\top}\Phi  - \tilde{\Sigma}_{\alpha}^{-1}(\gamma\bar{\Sigma}_{\alpha}^{-1} + \Phi^{\top}\Phi)^{-1}R \Big] (\gamma\tilde{\Sigma}_{\alpha}^{-1} + \Phi^{\top}\Phi)^{-1}\Bigg).
    \notag
\end{align}
Studying its limit as $\gamma \rightarrow 0^+$, one has
\begin{align}
2\text{Tr}\Bigg((\Phi^{\top}\Phi)^{-1}\tilde{\Sigma}_{\alpha}^{-1}\Big[\bar{\alpha}^{\pi}r^{\top}\Phi - (\Phi^{\top}\Phi)^{-1}(\Phi^{\top}rr^{\top}\Phi + \sigma^2\Phi^{\top}\Phi) \Big] (\Phi^{\top}\Phi)^{-1}\Bigg).\label{eq:dMat0}
\end{align}
The only case that is easy to study occurs when there is no residual term (i.e., the regression function belongs to the hypothesis space, so that $r(x) = 0$ for all $x$). In that scenario, 
the expression above is clearly negative, and this proves the fact that $\mathcal{M}(\gamma) < \mathcal{M}(0)$ at least in some small neighbourhood of the origin: see also Proposition 2 in \cite{MU2018381}.\\ 
To numerically test the impact of regularization, we perform a Monte Carlo study of 500 trials. We consider an input domain $\cX = [-25,25]$, and a regression function characterized by 50 basis functions randomly selected among the first 80 (ordered with increasing $q$ in \eqref{eq:basis}), linearly combined by a vector drawn from a Gaussian distribution with zero mean, i.i.d.~components and variance $\lambda=10$. The latter hyper-parameter also enters the definition of $\cH$; the set of frequencies $Q$ defining it is a sample of random dimension $E/2$ in $\{5,6,...,50\}$, selected among the ones defining $f_{\rho}$. Thus, in this testing situation the approximation error ruled by the residual $r(\cdot)$ is different from 0. We consider an SNR of 100 yielded by a Gaussian, zero-mean, i.i.d.~noise. On each run, the number of data $N$ is randomly selected in the interval $\{5,...,E/2\}$. For the regularization parameter, we both use $\gamma^{(b)}$ as in Proposition \ref{prop:tradeoff}(b) and a value $\hat{\gamma}$ estimated via marginal likelihood optimization performed with a Gibbs sampling scheme leveraging the Bayesian interpretation of the problem in \eqref{eq:alphahat} (\cite[Chapter 1]{gilks1995markov}; see also \cite{PILLONETTO2015106} for a thorough discussion on the robustness of the marginal likelihood hyper-parameter estimation. More details are reported below). We then study the values of the overall error $\bar{\mathcal{M}}(\gamma) = \|f_{\rho} - f_z\|_{\cL}$, where $f_z(\cdot) = \phi^{\top}\hat{\alpha}$ with $\hat\alpha$ as in \eqref{eq:hatalpha}, attained by $\gamma^{(b)}$, $\gamma=0$ and $\hat{\gamma}$, compared to the oracle value corresponding to $\gamma=\gamma^*$ computed by grid search. The results are summarized in Figure \ref{fig:regularization}. We note that the two regularized estimators yield comparable results, meaning that $\gamma^{(b)}$ is a good estimator of $\hat{\gamma}$ (and is faster to be computed); and that both always outperform the case with no regularization involved. Specifically, we obtain that the relative discrepancy between, e.g., the case with $\gamma=0$ and $\hat{\gamma}$ has a median value of $24.99\%$, with minimum and maximum values equal to $0.64\%$ and $66.97\%$, respectively.\\

\begin{figure}[h]
    \centering
    \input{analysis_reg.tex}
    \caption{Relative discrepancy $100\%(\bar{\mathcal{M}}(\hat{\gamma}) - \bar{\mathcal{M}}(\gamma^*))/\bar{\mathcal{M}}(\gamma^*)$ for the regularized and non-regularized cases. } 
    \label{fig:regularization}
\end{figure}




We conclude by providing the details for the selection rule for hyper-parameter $\gamma$ used as baseline in the Monte Carlo test.  Let us consider the estimation problem in \eqref{eq:alphahat}, i.e.,
\begin{align}
    \hat{\alpha} = \arg\min_{\alpha \in \mathbb{R}^E} \|Y - \Phi \alpha\|^2 + N\sigma^2 \alpha^{\top}P^{-1}\alpha  
     =(\Phi^{\top}\Phi + N\sigma^2 P^{-1})^{-1}\Phi^{\top}Y,  \label{eq:alphahatAPP} 
    \end{align}
where $\Phi \in \mathbb{R}^{N\times E}$ is the matrix that stacks all $\{\phi^{\top}(x_t)\}_{t=1}^N$ (i.e., the row vectors containing the basis functions $\{\varphi_i(\cdot)\}_{i=1}^E$ as defined in \eqref{eq:basisRKHS}), $P^{-1}$ is a regularization matrix, and $Y$ is the output measurements vector $[y_1,\,...,y_N]^{\top}$.\\ The objective in \eqref{eq:alphahatAPP} admits a stochastic interpretation. Consider a measurements model $Y = \Phi\alpha + E$, where $E$ is an $N$-dimensional Gaussian vector with zero mean and known covariance $\sigma^2 I_N$, and $\alpha \in \mathbb{R}^E$ is unknown. Taking the Bayesian viewpoint, $\alpha$ is modelled as a random vector; specifically, assume it is Gaussian, with zero mean and covariance $P/N$. In this set-up, the objective in \eqref{eq:alphahatAPP} is (apart from constants not depending on $\alpha$) the negative logarithm of the posterior probability $\alpha|Y$: that is, $\hat{\alpha}$ is computed as the Maximum a Posteriori (MAP) estimate, which corresponds to the minimum variance linear estimate in the Gaussian case we are considering. In fact, the solution to \eqref{eq:alphahatAPP} is indeed the expression of the posterior mean. \\
Within this framework, assume that (some of the) hyper-parameters entering $P^{-1}$, or $\sigma^2$, are unknown, and collect them in a vector
$\eta$. A possible strategy consists in estimating them from data, leveraging the so-called empirical Bayes approach~\cite{maritz2018empirical}: in particular, the estimate for $\eta$ is computed by maximising the evidence $Y|\eta$ (or, more conveniently, minimizing its negative logarithm), which is equivalent to the joint distribution $Y, \alpha|\eta$ where the dependence from $\alpha$ is integrated out. In the Gaussian case, this reads as
\begin{equation}
    \hat{\eta} = \arg\min_{\eta}\: Y^{\top}\Sigma_Y(\eta)^{-1}Y + \log\det\Sigma_Y(\eta),\label{eq:marglikh}
\end{equation}
where $\Sigma_Y = \Phi P\Phi^{\top}/N + \sigma^2I_N$. This problem is non-convex, and deterministic optimization routines might return unreliable results due to their sensitivity to initial conditions. A way to overcome this issue consists in resorting to Markov Chain Monte Carlo (MCMC). Specifically, the idea is to run a (single-component) Metropolis-Hastings algorithm to construct a Markov chain whose invariant distribution is (proportional to) the marginal likelihood of interest: this is a mechanism to draw samples from such a distribution, and solve \eqref{eq:marglikh} in sample-based form. For an introduction to MCMC we refer to \cite{gilks1995markov}.\\

Let us now relate \eqref{eq:alphahatAPP} to the original function estimation problem stated in \eqref{eq:fz} to provide an expression for $P^{-1}$, and detail the MCMC-based procedure for marginal likelihood optimization. The function is estimated as
\begin{equation}
    f_z = \arg\min_{f \in \cH} \frac{1}{N}\sum_{t=1}^N (y_t - f(x_t))^2 + \gamma \|f\|_{\cH}^2. \notag
\end{equation}
According to the definitions \eqref{eq:RKHS} and \eqref{eq:ourkernel} specifying the RKHS $\cH$, one can write $f_z(\cdot) = \phi^{\top}(\cdot)\hat{\alpha}$. In this way, estimating $f_z(\cdot)$ translates into solving \eqref{eq:alphahatAPP} for the particular choice $P^{-1} = \frac{\gamma\Sigma_{\alpha}^{-1}}{\sigma^2}$, where $\Sigma_{\alpha}$ is given by the choice of the hypothesis space (see, e.g., \eqref{eq:ourkernel}), and $\gamma$ is the positive scalar to be tuned.\\
Let us now detail the MCMC procedure for the scenario above specified. First, denoting with $p(\cdot)$ the probability density function of interest, we note that 
\begin{equation}
    p(\alpha,\gamma|Y) = p(\alpha|\gamma,Y)p(\gamma|Y) \propto p(\alpha | \gamma, Y)p(Y|\gamma), \notag
\end{equation}
which tells us that drawing samples from $\alpha,\gamma|Y$ is a suitable way to explore the marginal likelihood. Therefore, we set up a Gibbs sampler (a particular case of single-component Metropolis-Hastings algorithm) whose invariant distribution has density $p(\alpha,\gamma|Y) = \pi(\alpha,\gamma)$. In this particular scenario, samples from the full conditionals are easy to be computed: in fact, denoting with $\tilde{\Sigma}_{\alpha} = \Sigma_{\alpha}/N$ and with $\Gamma(\mathfrak{a},\mathfrak{b})$ a Gamma distribution with mean $\mathfrak{a}/\mathfrak{b}$, using conjugate distributions properties we obtain
\begin{align}
    &\pi(\alpha|\gamma) \longleftarrow \alpha|\gamma,Y \sim \mathcal{N}\Big( (\tilde{\Sigma}_{\alpha}^{-1} + \Phi^{\top}\Phi/\gamma)^{-1}\Phi^{\top}Y/\gamma,\: (\tilde{\Sigma}_{\alpha}^{-1} + \Phi^{\top}\Phi/\gamma)^{-1} \Big)\\
    &\pi(\gamma|\alpha) \longleftarrow \gamma|\alpha,Y \sim \Gamma\Big(\frac{N}{2},\: \frac{\|Y - \Phi\alpha\|^2}{2}\Big).
\end{align}
Finally, the last $N_g < N_G$ samples obtained from the Gibbs sampler can be used to compute $\hat{\gamma}$ maximising the marginal likelihood. 




\end{document}
