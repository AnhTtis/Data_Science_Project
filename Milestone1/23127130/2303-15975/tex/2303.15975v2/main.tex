\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}

\makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{wrapfig}
\usepackage{glossaries}
\usepackage[table]{xcolor}
\usepackage{colortbl}
\usepackage{bbding}
\usepackage{tikz}
\usepackage{comment}
\usepackage{color}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{nicefrac}
\usepackage{bbm}
\usepackage{bm}
\usepackage{tabularx,verbatim}
\usepackage{multirow}
\usepackage[small]{caption}

\definecolor{remark}{rgb}{1,.5,0} 
\definecolor{citecolor}{rgb}{0,0.443,0.737} 
\definecolor{linkcolor}{rgb}{0.956,0.298,0.235} 
% \definecolor{gray}{gray}{0.95}
\definecolor{cyan}{rgb}{0.831,0.901,0.945}

\usepackage[accsupp]{axessibility}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

% ======================================================================================================
% My macros
% ======================================================================================================
\input{utils/math_commands.tex}
\input{utils/my_commands.tex}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\colorlet{dark-blue}{blue!70!black}
\hypersetup{
    colorlinks=true,%
    citecolor=dark-blue,%
    filecolor=dark-blue,%
    linkcolor=red,%
    urlcolor=magenta
}

\usepackage[most]{tcolorbox}
\usepackage{caption} 
\captionsetup[table]{skip=0pt}
\captionsetup[figure]{skip=0pt}

\iccvfinalcopy

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\definecolor{mypink}{RGB}{219, 48, 122}

% \setcounter{page}{1}
% \ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Large-scale Pre-trained Models are Surprisingly Strong \\ in Incremental Novel Class Discovery}
\author{Mingxuan Liu$^{\textcolor{mypink}{1}}$, Subhankar Roy$^{\textcolor{mypink}{2}}$, Zhun Zhong$^{\textcolor{mypink}{1}*}$~, Nicu Sebe$^{\textcolor{mypink}{1}}$, Elisa Ricci$^{\textcolor{mypink}{1,3}}$ \\
 \normalsize{$^{\textcolor{mypink}{1}}$University of Trento~~ $^{\textcolor{mypink}{2}}$LTCI, Télécom Paris, Institut polytechnique de Paris~~$^{\textcolor{mypink}{3}}$Fondazione Bruno Kessler}\\
}

\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
% \vspace{-3em}
\maketitle
\vspace{-2em}
\begin{center}
  \centering
   \includegraphics[width=0.93\textwidth]{fig/setting_h.pdf}
   % \vspace{-.2in}
   \captionof{figure}{Overview of different learning paradigms for discovering novel (or \textit{new}) categories from \textit{unlabelled} data. (a) \textbf{DC} involves learning new concepts without any prior knowledge and supervision. (b) \textbf{\ncd} jointly exploits the prior learned from labelled data, and the unlabelled data itself, to discover new concepts. (c) \textbf{\cincd} is similar to \ncd, except it can not access labelled data during the discovery phase. (d) Unlike the existing settings, our proposed \textbf{\newsetting} derives prior from general-purpose large-scale pre-trained models to continuously learn new concepts, without accessing past data. Inference on test data is carried out in a task-\textit{agnostic} manner.}
   % \vspace{.05in}
   \label{fig:setting_comparison}
\end{center}
}]

% \ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
\vspace{-1em}
Discovering novel concepts from unlabelled data and in a continuous manner is an important desideratum of lifelong learners. In the literature such problems have been partially addressed under very restricted settings, where either access to labelled data is provided for discovering novel concepts (e.g., \ncd) or learning occurs for a limited number of incremental steps (e.g., \cincd). In this work we challenge the status quo and propose a more challenging and practical learning paradigm called \newsetting, where learning occurs continuously and unsupervisedly, while exploiting the rich priors from large-scale pre-trained models. To this end, we propose simple baselines that are not only resilient under longer learning scenarios, but are surprisingly strong when compared with sophisticated state-of-the-art methods. We conduct extensive empirical evaluation on a multitude of benchmarks and show the effectiveness of our proposed baselines, which significantly raise the bar\blfootnote{Code will be available at: \href{https://github.com/OatmealLiu/MSc-iNCD}{MSc-iNCD}. \newline \hspace*{0.2in}*Corresponding author: Zhun Zhong}.
\end{abstract}

%%%%%%%%% MAIN PAPER

\input{src/introduction.tex}

\input{src/related.tex}

\input{src/method.tex}

\input{src/experiment.tex}

\input{src/conclusion.tex}

\input{src/appendix}

\clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{reference}
}

\end{document}
