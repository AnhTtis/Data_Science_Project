\renewcommand\thesection{\Alph{section}}
\renewcommand\thefigure{\Alph{figure}}
\renewcommand\thetable{\Alph{table}}
\setcounter{section}{0}
\setcounter{figure}{0}
\setcounter{table}{0}

\section*{Appendix}

\begin{table*}[!t]
\begin{center}
    \begin{tabular}{l|rrrr|rrrrrrrrrr}
    \toprule
    
    Splits & \multicolumn{4}{c|}{Two-step} & \multicolumn{10}{c}{Five-step} \\
    Task & \multicolumn{2}{c}{$\task\tsone$} & \multicolumn{2}{c|}{$\task\tstwo$}   & \multicolumn{2}{c}{$\task\tsone$} & \multicolumn{2}{c}{$\task\tstwo$} & \multicolumn{2}{c}{$\task\tsthree$} & \multicolumn{2}{c}{$\task\tsfour$} & \multicolumn{2}{c}{$\task\tsfive$} \\
    
    Datasets & $\numclassesst$ & $\numdatast$ & $\numclassesst$ & $\numdatast$ & $\numclassesst$ & $\numdatast$ & $\numclassesst$ & $\numdatast$ & $\numclassesst$ & $\numdatast$ & $\numclassesst$ & $\numdatast$ & $\numclassesst$ & $\numdatast$  \\
    \hline
    
    C10 & 5 & 25.0k & 5 & 25.0k & 2 & 10.0k & 2 & 10.0k & 2 & 10.0k & 2 & 10.0k & 2 & 10.0k \\
    C100 & 50 & 25.0k & 50 & 25.0k & 20 & 10.0k & 20 & 10.0k & 20 & 10.0k & 20 & 10.0k & 20 & 10.0k \\
    T200 & 100 & 50.0k & 100 & 50.0k & 40 & 20.0k & 40 & 20.0k & 40 & 20.0k & 40 & 20.0k & 40 & 20.0k \\
    B200 & 100 & 2.4k & 100 & 2.4k & 40 & 0.9k & 40 & 0.9k & 40 & 0.9k & 40 & 0.9k & 40 & 0.9k \\
    H683 & 342 & 14.5k & 341 & 16.3k & 137 & 6.3k & 137 & 5.4k & 137 & 6.1k & 137 & 6.8k & 135 & 6.3k \\
    \bottomrule
    \end{tabular} 
    \captionof{table}{Two-step and five-step dataset splits for the \newsetting experiments. The number of novel classes $\classes\tst$ and the number of unlabelled images $\numdatast$ in $\data\tst$ for each task $\task\tst$ are reported.}
    \vspace{.05in}
    \label{tab:data_details}
\end{center}
\end{table*}

The supplementary material is organized as follows: in Sec.~\ref{sec:expt_setup_app} we provide details about the datasets and the splits. In Sec.~\ref{sec:app-impl-det} we elaborate the implementation details of our baselines and the re-implemented methods under the \newsetting setting. Finally in Sec.~\ref{sec:app-exp-results} we report additional experimental results.

%%%%%%%%% MAIN SUPPLEMENTARY
\section{Datasets and Splits}
\label{sec:expt_setup_app}


We conduct experiments on five datasets, which are: CIFAR-10 (C10), CIFAR-100 (C100), TinyImageNet-200 (T200), CUB-200 (B200) and Herbarium-683 (H683). The Tab.~\ref{tab:data_details} presents the detailed splits for the two adopted task sequences (two-step and five-step) on the five data sets~\cite{krizhevsky2009learning, le2015tiny, wah2011caltech, Tan2019TheHC}. For a task sequence of $T=2$, the total classes and their corresponding instances in the dataset are equally divided into two splits (\eg, for C100, 100 classes / 2 tasks = 50 novel classes per task). Similarly, for a task sequence of $T=5$, the same method is used to divide the classes and their corresponding instances into five splits (\eg, for C100, 100 classes / 5 tasks = 20 novel classes per task).

The experimental results on C10, C100, and T200 provide an indication of the performance of the studied \newsetting methods in common image recognition tasks, while the results on B200 and H683 show their performance in fine-grained image recognition tasks. Moreover, the evaluation on H683 offers insights into the performance of the studied methods in long-tailed task sequences and also when the downstream dataset is quite different from the internet-scale images.

\section{Implementation Details.}
\label{sec:app-impl-det}
In this section, we present the implementation details of image pre-processing and data augmentation, our baselines, and the adapted methods for \newsetting. In Sec.~\ref{sec:supp_dataaug}, the employed image pre-processing technique and data augmentation for the experiments are elaborated. Sec.~\ref{sec:supp_ours} offers an in-depth account of the training and inference processes for our \ours and \ourspp using Pytorch-like pseudo-code. Subsequently, Sec.~\ref{sec:supp_bounds} explores the development of the reference methods for the \newsetting setting. Ultimately, the adaption specifics and hyperparameters for the compared methods originating from \incd and \il fields are described in Sec.~\ref{sec:supp_incd} and Sec.~\ref{sec:supp_il}, correspondingly.

In order to maintain equitable evaluation, all methods examined in this work use the same \vitbsixteen~\cite{Dosovitskiy2020AnII} backbone, as employed by our \ours and \ourspp.

\subsection{Image Pre-processing and Data Augmentation}
\label{sec:supp_dataaug}
In order to utilize the publicly accessible pre-trained DINO-\vitbsixteen, it is necessary to adjust the input images to a fixed resolution of 224 $\times$ 224. In accordance with~\cite{Vaze2022GeneralizedCD}, the input images are initially upsampled to a resolution of 224 $\times$ 224 / 0.875 employing trilinear interpolation, followed by a center-crop of the upsampled images to achieve a 224 $\times$ 224 resolution for all the experiments. Subsequent to the aforementioned pre-processing procedure, SimCLR-like~\cite{Chen2020ASF} stochastic augmentations are predominantly employed throughout the experiments for all the methods.

\subsection{Simple yet Strong Baselines for \newsetting}
\label{sec:supp_ours}
In the present section, a thorough exposition of the training and inference procedures for both \ours and \ourspp is provided, accompanied by Pytorch-like pseudo-code, to effectively demonstrate the simplicity of our methods.

\begin{algorithm}[t]
\caption{Pseudo-code of our \ours training for the discovery task $\task\tst$ in a PyTorch-like style.}
\label{alg:ours_train}

\definecolor{codeblue}{rgb}{0.25,0.5,0.8}
\lstset{
  backgroundcolor=\color{white},
  basicstyle=\fontsize{7.2pt}{7.2pt}\ttfamily\selectfont,
  columns=fullflexible,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue},
  keywordstyle=\fontsize{7.2pt}{7.2pt},
%  frame=tb,
}
\begin{lstlisting}[language=python]
# g: frozen ViT-B/16 encoder network initialized by DINO weights, output 768-dimensional embedding
# h_t: task-specific linear classifier with randomly initialized weights
# temp: temperature
# C_t: number of novel classes present in task t

for x in train_loader:    # load a minibatch x with N samples
    x1 = aug(x)     # randomly augmented view 1
    x2 = aug(x)     # randomly augmented view 2

    # normalize weights
    with torch.no_grad():
        # temporarily store the weight vectors: C_tx768
        w_temp = h_t.linear_layer.weight.data.clone()
        w_temp = normalize(w_temp, dim=1, p=2)
        h_t.linear_layer.weight.copy_(w_temp)

    # extract feature embeddings
    z1 = g.forward(x1)      # Nx768
    z2 = g.forward(x2)      # Nx768

    # output logits
    l1 = h_t.forward(z1)    # NxC_t
    l2 = h_t.forward(z2)    # NxC_t

    # generate pseudo labels
    y1 = sinkhorn(l1)       # NxC_t
    y2 = sinkhorn(l2)       # NxC_t

    # swap prediction problem of the two views
    # cross-entropy loss, Eq.1
    loss1 = CrossEntropyLoss(l1/temp, y2)
    loss2 = CrossEntropyLoss(l2/temp, y1)
    loss = loss1 + loss2

    # SGD update: task-specific classifier
    loss.backward()
    update(h_t.params)
\end{lstlisting}
\end{algorithm}

\noindent
\textbf{Discovery Training.} Algo.~\ref{alg:ours_train} presents the pseudo-code for the shared discovery training loop employed by our \ours and \ourspp. For each unlabelled sample $\vx$, we generate two views of $\vx$ by applying the stochastic transformation delineated in Sec.~\ref{sec:supp_dataaug}. Prior to forwarding the input to the model, we perform cosine normalization on the task-specific classifier $h\tst$ through L2 normalization of the weight matrix $\theta_{h\tst}$ (feature-level L2 normalization is performed in $h\tst$). Subsequently, the two views are sequentially input into the frozen feature extractor $g$ and classifier $h\tst$ to produce the output logits. To optimize the objective defined in Eq.~\ref{eqn:ce} for the \textit{swapped} prediction problem, the \sinkhorn~\cite{Caron2020UnsupervisedLO} algorithm is employed to generate the pseudo-labels for the two views as soft-targets that are sawpped. The temperature parameter is set at 0.1. A total of 200 epochs of training are conducted for the discovery of both \ours and \ourspp.

\noindent
\textbf{Task-agnostic Inference.} After the discovery step for task $\task\tst$, our \ours can execute task-agnostic inference by simply concatenating the newly learned task-specific classifier $h\tst$ with the previous unified classifier $h\tstotminus$ to form a new unified classifier $h\tstot$, as described in Algo.~\ref{alg:ours_inference}.

\begin{algorithm}[t]
\caption{Pseudo-code of our \ours inference for the task sequence $\taskb=\{\task\tsone, \task\tstwo, \cdots, \task\tsend\}$ in a PyTorch-like style.}
\label{alg:ours_inference}
% \algcomment{\fontsize{7.2pt}{0em}\selectfont \texttt{bmm}: batch matrix multiplication; \texttt{mm}: matrix multiplication; \texttt{cat}: concatenation.
% %\vspace{-1.em}
% }
\definecolor{codeblue}{rgb}{0.25,0.5,0.8}
\lstset{
  backgroundcolor=\color{white},
  basicstyle=\fontsize{7.2pt}{7.2pt}\ttfamily\selectfont,
  columns=fullflexible,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue},
  keywordstyle=\fontsize{7.2pt}{7.2pt},
%  frame=tb,
}
\begin{lstlisting}[language=python]
# g: frozen ViT-B/16 encoder network initialized by DINO weights, output 768-dimensional embedding
# h_prev: unifeid classifier saved for the previous tasks (t=1, ..., t=t-1)
# h_t: newly learned task-specific classifier for task t
# h_tot: unified classifier for all the tasks seen so far (t=1, ..., t=t)
# C_tot: total number of novel classes discovered until task t.

# concatenate classifiers
h_tot = cat([h_prev, h_t], dim=0)     # C_totx768

# perform task-agnostic inference
for x in test_loader:    # load a minibatch x with N samples
    # extract feature embeddings
    z = g.forward(x)        # Nx768

    # output logits
    l = h_tot.forward(z)    # NxC_tot

    # take the cluster-id with maximum logit value as prediction
    prediction = max(l, dim=1)   # N
\end{lstlisting}
\end{algorithm}

\noindent
\textbf{\compfr Training.} As expounded in the primary manuscript, our \ourspp incorporates \textit{\compfrlong} (\compfr) training to jointly optimize the concatenated classifier $h\tstot$ further. Algo.~\ref{alg:ourspp_train} describes the \compfr training loop specifics. For each unlabelled sample $\vx$, the same stochastic transformation (see Sec.~\ref{sec:supp_dataaug}) is employed to generate two views of $\vx$. The \textit{cosine normalization} operation is applied to the unified classifier before forward propagation to maintain the weight vectors on the same scale. Subsequently, generative pseudo feature replay is utilized to replay an equal number of past feature embeddings from the preserved pseudo per-class prototype Gaussian distributions $\Mu$ as the current mini-batch size. The loss, as defined in Eq.~\ref{eqn:loss_past} for past novel classes, is calculated using the output logits of the replayed embeddings from $h\tstot$. To also preserve the discriminative capability for current novel classes, knowledge is transferred from $h\tst$ to $h\tstot$ using the pseudo-labels generated by $h\tst$ for the two views. The loss, as defined in Eq.~\ref{eqn:loss_now} for the current novel classes, is then computed using the output logits of the two views' embeddings from $h\tstot$ and the pseudo-labels. The ultimate \textit{past-current} objective (refer to Eq.~\ref{eqn:loss_all}) for \ourspp training is optimized by aggregating the two individual losses to update the parameters $\theta_{h\tstot}$ of the unified classifier $h\tstot$.

\begin{algorithm}[t]
\caption{Pseudo-code of the \compfr training in \ourspp during task $\task\tsend$ in a PyTorch-like style.}
\label{alg:ourspp_train}
% \algcomment{\fontsize{7.2pt}{0em}\selectfont \texttt{bmm}: batch matrix multiplication; \texttt{mm}: matrix multiplication; \texttt{cat}: concatenation.
% %\vspace{-1.em}
% }
\definecolor{codeblue}{rgb}{0.25,0.5,0.8}
\lstset{
  backgroundcolor=\color{white},
  basicstyle=\fontsize{7.2pt}{7.2pt}\ttfamily\selectfont,
  columns=fullflexible,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue},
  keywordstyle=\fontsize{7.2pt}{7.2pt},
%  frame=tb,
}
\begin{lstlisting}[language=python]
# g: frozen ViT-B/16 encoder network initialized by DINO weights, output 768-dimensional embedding
# h_prev: unifeid classifier saved for the previous tasks (t=1, ..., t=t-1)
# h_t: newly learned task-specific classifier for task t
# h_tot: unified classifier for all the tasks seen so far (t=1, ..., t=t)
# C_t: number of novel classes present in task t
# C_tot: total number of novel classes discovered until task t.
# M: saved per-class pseudo prototypes from previous tasks (t=1, ..., t=t-1)
# temp: temperature

# concatenate classifiers
h_tot = cat([h_prev, h_t], dim=0)     # C_totx768

# load a minibatch x with N samples
for x in train_loader:
    x1 = aug(x)     # randomly augmented view 1
    x2 = aug(x)     # randomly augmented view 2

    # normalize weights
    with torch.no_grad():
        # temporarily store the weight vectors: C_totx768
        w_temp = h_tot.linear_layer.weight.data.clone()
        w_temp = normalize(w_temp, dim=1, p=2)
        h_tot.linear_layer.weight.copy_(w_temp)

        
    # generatively replay saved prototypes fro past classes
    z_past, y_past = replay(M)  # Nx768, Nx1

    # output logits for past embeddings from unified classifier
    l_past = h_tot.forward(z_past)  # NxC_tot
    
    # cross-entropy loss for past classes, Eq.4
    loss_past = CrossEntropyLoss(l_past/temp, y_past)

    # extract feature embeddings
    z1 = g.forward(x1)      # Nx768
    z2 = g.forward(x2)      # Nx768

    # output logits
    l1 = h_T.forward(z1)    # NxC_t
    l2 = h_T.forward(z2)    # NxC_t

    # generate pseudo labels by using the task-specific classifier prediction
    y1 = max(l1, dim=1) + C_tot - C_t   # Nx1
    y2 = max(l2, dim=1) + C_tot - C_t   # Nx1

    # concatenate feature embeddings
    z_current = cat([z1, z2], dim=0)    # 2Nx768
    
    # concatenate pseudo labels
    y_current = cat([y1, y2], dim=0)    # 2Nx1

    # output logits for current embeddings from unified classifier
    l_now = h_tot.forward(z_current)    # 2NxC_tot
    
    # cross-entropy loss for current classes, Eq.5
    loss_current = CrossEntropyLoss(l_now/temp, y_current)
    
    # swap prediction problem of the two views
    # cross-entropy loss, Eq.6
    loss = l_past + loss_current

    # SGD update: task-specific classifier
    loss.backward()
    update(h_tot.params)
\end{lstlisting}
\end{algorithm}

\subsection{Building Reference Methods}
\label{sec:supp_bounds}
Since there is no prior work has investigated \newsetting setting, we build reference methods for the comparison in this work.

\noindent
\textbf{K-means~\cite{Arthur2007kmeansTA}.} We utilize the K-means algorithm to create a 'pseudo' \textit{lower-bound} reference. Specifically, we extract the 768-dimensional deep features $\vz \in \R^{768}$ of the given images using DINO-\vitbsixteen~\cite{Caron2021EmergingPI} as the feature extractor. Then, we perform K-means clustering on the $\vz$ extracted from the joint training datasets $\bigcup_{t=1}^{T} \data\tst$ to form $\bigcup_{t=1}^{T} \classes\tst$ semantic clusters at the end of a given task sequence. The maximum number of iterations is set to 300 for all the experiments. However, these \textit{lower-bound} results are only for reference as the K-means algorithm uses access to previous training data to form the clusters for task-agnostic evaluation and cannot accurately represent the minimum performance of \newsetting.

\noindent
\textbf{Joint (frozen) and Joint (unfrozen).} In accordance with the supervised \cil practice~\cite{wang2023comprehensive}, we construct two \textit{upper-bound} reference methods. The first method, denoted as $\upperb$, performs joint training on the unified model $f\tstoend = h\tstoend \circ g$ of \ours after task-specific discovery training, utilizing all the training data $\bigcup_{t=1}^{T} \data\tst$ up to the current step. The second method, denoted as $\upperbpp$, further unfreezes the last transformer block during both the discovery and joint training of $\upperb$. Notably, $\upperbpp$ does not unfreeze the last block at the beginning of the training since we observe in experiments that saturating the classifier $h\tst$ first and then fine-tuning the last block of $g$ yields better performance.

\subsection{Adapting \incd Methods to \newsetting}
\label{sec:supp_incd}

Since \newsetting setting does not allow the use of any labelled data, we need to adapt the two compared \incd solutions, ResTune\footnote {\url{https://github.com/liuyudut/ResTune}}~\cite{liu2022residual} and FRoST\footnote {\url{https://github.com/OatmealLiu/class-iNCD}}~\cite{Roy2022ClassincrementalNC}, to work without the supervised pre-training on the labelled data. To accomplish this, we initialize the feature extractors $g(\cdot)$ of ResTune and FRoST with the same self-supervised large-scale pre-trained weights $\theta_{g}$ (DINO~\cite{Caron2021EmergingPI}), instead of using supervised pre-training on labelled data. This enables ResTune and FRoST to perform continuous novel class discovery under the \newsetting setting with their own components to discover novel categoires and prevent forgetting.

\noindent
\textbf{ResTune} is an \incd solution that combines architecture-based and regularization-based \il techniques to prevent forgetting. ResTune grows a new block at each incremental step to learn new knowledge with a clustering objective~\cite{Xie2015UnsupervisedDE}, while adjusting the shared basic feature extractor for the new data under the regularization of a knowledge distillation objective~\cite{li2017learning}. The adapted ResTune in this work uses the first eleven transformer blocks of \vitbsixteen as the shared basic feature extractor, with only the last (11th) block unfrozen, while creating a new unfrozen transformer block branch initialized by DINO-weights to learn the residual feature at each step. The weight $\beta$ for the knowledge distillation objective is set to 1 for all the experiments, as in the original work.

\noindent
FRoST is a \cincd solution that combines regularization-based and rehearsal-based \il techniques to prevent forgetting, and it is based on ranking statistics~\cite{han2020automatically}. In this work, we strictly follow the configuration used in the original work~\cite{Roy2022ClassincrementalNC} for the hyperparameters. However, since there are no labels available in the \newsetting setting, we adapt the supervised feature replay of FRoST to the unsupervised pseudo feature replay by using the same approach in \ourspp.

\subsection{Adapting \il Methods to \newsetting}
\label{sec:supp_il}

To evaluate the effectiveness of our proposed methods in preventing forgetting, we also compare their performance with that of traditional \il techniques. For this purpose, we adapt two regularization-based methods (EwC~\cite{kirkpatrick2017overcoming} and LwF~\cite{li2017learning}) and one rehearsal-based method (DER~\cite{buzzega2020dark}) to \newsetting by using the publicly available \il framework codebase\footnote{\url{https://github.com/aimagelab/mammoth}}~\cite{buzzega2020dark, boschini2022class} in our experiments. However, unlike the \incd methods, these \il methods are originally designed for supervised settings and are not capable of discovering novel categories from unlabelled data. Therefore, we apply the same discovery strategy as our \ours to all the adapted \il methods. Specifically, we initialize the feature extractor $g$ with DINO~\cite{Caron2021EmergingPI} pre-trained weights and optimize the clustering objective defined in Eq.~\ref{eqn:ce} using the \sinkhorn cross-view pseudo-labelling algorithm~\cite{Caron2020UnsupervisedLO} to discover the novel classes contained in the given unlabelled data set $\data\tst$. Different from our \ours, we unfreeze the last transformer block of $g$ to adapt the model to the data present at each step in all the experiments. To prevent forgetting, we maintain the \il components in the original methods.

\noindent
\textbf{EwC} is a weight regularization \il method, which penalizes the model parameters selectively based on their importance for the past tasks using the calculated Fisher information matrix~\cite{kirkpatrick2017overcoming}. In the experiments, we set the hyperparameter $\lambda$ to 8000 to control the relative importance of past tasks compared to the new one, and the Fisher matrix fusion parameter $\alpha$ to 0.5.

\noindent
\textbf{LwF} is a function regularization \il solution that uses a knowledge distillation~\cite{Gou2020KnowledgeDA} objective function to prevent forgetting by constraining the current model output to not deviate too much from the old model~\cite{li2017learning}. In our experiments, we save the old model $f\tstotminus = h\tstotminus \circ g\tstminus$ to compute the LwF loss at each step $t$. The LwF loss weight $\lambda$, which determines the balance between the old and new tasks, is set to 1.0 for all experiments.

\noindent
\textbf{DER} is a rehearsal-based \il solution that involves storing a fixed-size buffer of old training samples with past model responses as proxies of old tasks to prevent forgetting~\cite{buzzega2020dark}. For our experiments, the adapted DER maintains a buffer of 500 old samples for each step, with the \textit{not-forgetting} loss weight $\alpha$ set to 0.5.


\section{Detailed Experimental Results}
\label{sec:app-exp-results}
In this section, we provide comprehensive per-step comparative results of our \ours and \ourspp, juxtaposed with the adapted state-of-the-art methods on two task splits (two-step and five-step) of CIFAR-10 (C10)~\cite{krizhevsky2009learning}, CIFAR-100 (C100)~\cite{krizhevsky2009learning}, TinyImageNet-200 (T200)~\cite{le2015tiny}, CUB-200 (B200)~\cite{wah2011caltech} and Herbarium-683 (H683)~\cite{Tan2019TheHC} under \newsetting setting in Fig.~\ref{fig:supp_sota_c10}, Fig.~\ref{fig:supp_sota_c100}, Fig.~\ref{fig:supp_sota_t200}, Fig.~\ref{fig:supp_sota_b200}, Fig.~\ref{fig:supp_sota_h683}, respectively. We report both the overall accuracy and maximum forgetting for each step, employing a task-agnostic evaluation.

As depicted in the reported figures, the overall accuracy exhibits a decline as the task sequence progresses, whereas the maximum forgetting for the novel classes discovered during the first step experiences an increase, attributable to the \textit{\forget} issue~\cite{wang2023comprehensive}. In the context of longer task sequences (five-step split, as observed in the top half of the figures), the \textit{forgetting} issue is exacerbated due to more frequent model updates.

During the first discovery task, the majority of adapted methods that unfreeze the final transformer block attain higher accuracy in most cases due to their adaptation to the current data. However, commencing from the second step, our \ours and \ourspp consistently surpass all compared methods across all datasets and splits in terms of overall accuracy. Although FRoST~\cite{Roy2022ClassincrementalNC} exhibits a better ability to mitigate forgetting for novel classes discovered in the first step in certain cases, our baselines demonstrate a more balanced performance between the past and current novel classes. The consistent experimental results from the five compared datasets and two task splitting strategies reiterate the preeminence of our \ours and \ourspp for the \newsetting task. A straightforward combination of existing \il components and \ncd solutions proves insufficient for the \newsetting task. While the two very recent \incd works (ResTune~\cite{liu2022residual} and FRoST~\cite{Roy2022ClassincrementalNC}) were designed for such unsupervised incremental scenarios, they fail to achieve satisfactory performance when the restrictive assumption of possessing a rich labelled base classes is relaxed. Conversely, our proposed baseline methods operate without the need for labelled base classes; nevertheless, utilizing rich labelled data to pre-supervise the large-scale pre-trained backbone can also be employed in our \ours and \ourspp to enhance single-step \ncd performance if such labelled data is accessible.

Lastly, upon comparing the top half (a and b) with the bottom half (c and d) of all the presented figures, it becomes evident that the accuracy/forgetting disparities between \ours and \ourspp widen as the task sequence lengthens. This observation underscores the significance and efficacy of the \compfr training employed by \ourspp in enhancing the class-discrimination capability across tasks.

\begin{figure}[!tbh]
\begin{center}
\includegraphics[width=0.9\linewidth]{fig_supp/sota_c10.pdf}
\end{center}
\caption{Comparison of our baseline methods with the adapted state-of-the-art methods (EwC, LwF, DER, ResTune, FRoST) on \textbf{C10} under the \newsetting setting. \textbf{Top (a, b)}: five-step split. \textbf{Bottom (c, d)}: two-step split. The overall accuracy and maximum forgetting are reported.}
\label{fig:supp_sota_c10}
\end{figure}

\begin{figure}[!tbh]
\begin{center}
\includegraphics[width=0.9\linewidth]{fig_supp/sota_c100.pdf}
\end{center}
\caption{Comparison of our baseline methods with the adapted state-of-the-art methods (EwC, LwF, DER, ResTune, FRoST) on \textbf{C100} under the \newsetting setting. \textbf{Top (a, b)}: five-step split. \textbf{Bottom (c, d)}: two-step split. The overall accuracy and maximum forgetting are reported.}
\label{fig:supp_sota_c100}
\end{figure}

\begin{figure}[!tbh]
\begin{center}
\includegraphics[width=0.9\linewidth]{fig_supp/sota_t200.pdf}
\end{center}
\caption{Comparison of our baseline methods with the adapted state-of-the-art methods (EwC, LwF, DER, ResTune, FRoST) on \textbf{T200} under the \newsetting setting. \textbf{Top (a, b)}: five-step split. \textbf{Bottom (c, d)}: two-step split. The overall accuracy and maximum forgetting are reported.}
\label{fig:supp_sota_t200}
\end{figure}

\begin{figure}[!tbh]
\begin{center}
\includegraphics[width=0.9\linewidth]{fig_supp/sota_b200.pdf}
\end{center}
\caption{Comparison of our baseline methods with the adapted state-of-the-art methods (EwC, LwF, DER, ResTune, FRoST) on \textbf{B200} under the \newsetting setting. \textbf{Top (a, b)}: five-step split. \textbf{Bottom (c, d)}: two-step split. The overall accuracy and maximum forgetting are reported.}
\label{fig:supp_sota_b200}
\end{figure}

\begin{figure}[!tbh]
\begin{center}
\includegraphics[width=0.9\linewidth]{fig_supp/sota_h683.pdf}
\end{center}
\caption{Comparison of our baseline methods with the adapted state-of-the-art methods (EwC, LwF, DER, ResTune, FRoST) on \textbf{H683} under the \newsetting setting. \textbf{Top (a, b)}: five-step split. \textbf{Bottom (c, d)}: two-step split. The overall accuracy and maximum forgetting are reported.}
\label{fig:supp_sota_h683}
\end{figure}