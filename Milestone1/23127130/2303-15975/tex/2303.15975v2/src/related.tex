\section{Related Work}
\label{sec:related_work}

\noindent
\textbf{\ncdtitle (\ncd)} was formalized by \cite{Han2019LearningTD} with the aim of diminishing the innate ambiguity in deep clustering~\cite{Chang2017DeepAI,Dizaji2017DeepCV,Xie2015UnsupervisedDE,Yang2016TowardsKS,Yang2016JointUL} and enhancing the clustering ability of novel classes in an unlabelled dataset, by leveraging the prior knowledge derived from a labelled dataset~\cite{Hsu2017LearningTC,Hsu2019MulticlassCW, Han2019LearningTD}. Many of the recent \ncd works utilize a joint training scheme that assumes access to both labelled and unlabelled data concurrently to exploit strong learning signal from the base classes~\cite{han2020automatically, Zhong2020OpenMixRK, fini2021unified, Jia2021JointRL, zhong2021neighborhood, Zhao2021NovelVC, Vaze2022GeneralizedCD, Fei2022XConLW, Yang2022DivideAC, wen2022simple}. Keeping in mind the data regulatory practices, the \ncd community has been paying more attention to the problem of \incdtitle (\incd)~\cite{liu2022residual} where the access to the labelled (or base) dataset is absent during the discovery stage. Unlike \incd, Roy \etal~\cite{Roy2022ClassincrementalNC} and Joseph \etal~\cite{Joseph2022NovelCD} investigate a more realistic yet challenging setting known as \cincdtitle (\cincd), where task-id information is not available during inference.

However, no methods to date have explored the \newsettingtitle (\newsetting), where the goal is to continuously discover novel classes in a sequence of unlabelled data sets over multiple steps, rather than the few steps in the \cincd literature (2 steps in \cite{Joseph2022NovelCD} and 1 step in \cite{Joseph2022NovelCD}). In this work, we study the problem of \newsetting (see Fig.~\ref{fig:setting_comparison}d). We empirically show that prior \cincd works are not competent under the \newsetting scenario. Staying inline with the original motivation of the \ncd paradigm, we propose a new direction to tackle the \newsetting problem by leveraging the prior knowledge learned from \textit{large-scale unlabelled} (\eg, DINO~\cite{Caron2021EmergingPI}) data, as opposed to relying on a large amount of highly related and expensive \textit{labelled} data.
% or \textit{noisy web-scale} (\eg, CLIP~\cite{Radford2021LearningTV}

\noindent
\textbf{\iltitle (\il)} aims to train a model on a sequence of tasks with access to data only from the current task, while the model's performance is assessed across all tasks it has encountered to date. The \il methods~\cite{kirkpatrick2017overcoming,Rebuffi2016iCaRLIC,li2017learning,buzzega2020dark} are devised with a dual objective of mitigating \textit{\forget}~\cite{French1999Catastrophic} of the model's knowledge on the previous tasks, while concurrently enabling it to learn new ones in a flexible manner. Our proposed \newsetting can be seen as a special case of \il, where the new classes arrive in a sequence of tasks without labels. As shown by Roy \etal,~\cite{Roy2022ClassincrementalNC} due to the difference in the learning objectives during the supervised pre-training and unsupervised new class training, learning continuously is more challenging than the supervised \il setting. Our proposed baselines attempt to mitigate this issue with \compcnlong of the classifier weights and frozen features. In addition, we wish to emphasize that the primary focus of our study is on incrementally discovering and grouping of novel classes rather than incremental representation learning. This distinguishes our setting from \uiltitle (\uil)~\cite{Madaan2021RepresentationalCF, Fini2021SelfSupervisedMA}.