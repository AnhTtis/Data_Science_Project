\section{Conclusion}
In this paper, we investigate the practical yet challenging task of \newsettingtitle (\newsetting). We reveal that the use of large-scale pre-trained models can achieve better or comparable performance to models trained with labelled data in \ncd. Building upon this observation, we propose to forego the need for expensive labelled data by leveraging large-scale pre-trained models for \newsetting. To this end, we introduce two strong baselines that incorporate \compcnlong and \compfrlong with frozen feature extractor freezing. Notably, our proposed baselines demonstrate significant improvements over the state-of-the-art methods across five datasets. We hope our work can provide a new, promising avenue towards effective \newsetting.