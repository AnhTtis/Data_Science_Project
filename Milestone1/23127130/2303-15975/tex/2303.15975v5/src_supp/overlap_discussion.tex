\section{Is it fair to use Self-supervised Pre-trained Models (PTMs) for \cincdlong?}
\label{sec:app-disc-overlap}
We believe the usage of self-supervised pre-trained models (PTMs) is \textbf{justified} due to several reasons: \textbf{i}) the PTM (\textbf{DINO}) was pre-trained \textit{without} labels, thus complying with the one of the assumptions made in NCD; \textbf{ii}) our proposal to use PTM can be viewed analogous to the Generalized Category Discovery (\textbf{GCD})~\cite{Vaze2022GeneralizedCD}, where the unlabelled samples can come from both previously \textit{seen} and \textit{unseen} classes; and \textbf{iii}) in real-world scenarios of clustering with a large pool of data, one would normally start from a \textit{generic} pre-trained model, \textit{without} having any knowledge about the pre-training classes. Thus, the core idea -- of \textbf{leveraging prior knowledge} to better cluster unlabelled data -- remains unchanged. Guided with these motivations, we start from a self-supervised PTM and show with extensive experiments that a simple Baseline is more adept at \cincd than many SOTA methods proposed in related areas. In addition, to fairly exam our methods, we validate the proposed baselines on \textbf{diverse} and \textbf{balanced} datasets. Two out of the five datasets, CUB-200 and Herb-19, \textbf{do not} significantly overlap with ImageNet. In detail, only 2 classes in CUB-200 \textit{exactly} overlap with ImageNet. Herb-19 is disjoint in its \textbf{entirety}, which is evident from the lowest performance among all the datasets.