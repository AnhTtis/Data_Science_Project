\section{Comparison with Unsupervised Incremental Learning Method}
\label{sec:app-disc-uil}
We wish to emphasize that the primary focus of our study is on incrementally discovering and grouping of novel classes rather than incremental representation learning. This distinguishes our setting from \uiltitle (\uil) task~\cite{Madaan2021RepresentationalCF,Fini2021SelfSupervisedMA}. {\uil has the following \textit{drawbacks} w.r.t our proposed \textbf{\cincd}: \textbf{i}) UIL deals with learning \textit{only} the backbone, whereas in \cincd one can learn both the backbone and the classifier; \textbf{ii}) UIL methods either need \textit{labelled} data to train a classifier, or require access to \textit{past training data} for k-NN classification, both of which are \textbf{\textit{not}} needed by \cincd methods.} Thus, we believe \cincd is \textbf{more general} and subsumes the UIL methods.

Even so, we adapted and compared with a SOTA UIL method, \textbf{CaSSLe}\footnote{\url{https://github.com/DonkeyShot21/cassle}}~\cite{Fini2021SelfSupervisedMA} (CVPR'22), on the two task splits of the five benchmarks with the same DINO-initialized \vitbsixteen as feature extractor $g$. At each step discovery step, CaSSLe first trains $g$ on $\data\tst$ to learn the representation with its self-supervised loss (BYOL~\cite{grill2020bootstrap}) and distillation loss. Being a \uil method, CaSSLe requires labelled samples to learn a classifier, which are not available in the \cincd setting. To learn the classifier \textit{unsupervisedly} for discovery, we equip CaSSLe with our self-labelling loss (Eq. 1) for \ncd and our CosNorm to make it task-agnostic for a fair comparison. The results is reported in Tab.~\ref{tab:expt_cassle}. We observe that our simpler baselines consistently outperform CaSSLe both in accuracy ($\accuracy$) and forgetting ($\forgetting$) metrics. The worse performance by CaSSLe suggests that although it adopts distillation mechanisms to map the current representations at each step back to the previous steps, learning a classifier along with the backbone unsupervisedly is quite intricate, which we try to resolve with our proposed Baselines.

\begin{table}[!t]
    % \tablestyle{1.0pt}{0.9}
    \small
    % \vspace{-0.7cm}
\caption{Comparison with the adapted state-of-the-art \uil method on two task splits of C10, C100, T200, B200, and H683 under \cincd setting. Overall accuracy and maximum forgetting are reported. All methods use DINO-\vitbsixteen as feature encoder.}
    \label{tab:expt_cassle}
    % \vspace{-0.3cm}
    \begin{center}
        \begin{tabular}{cl|cc|cc|cc|cc|cc}
            \toprule
            
            & Datasets 
            & \multicolumn{2}{c|}{C10} 
            & \multicolumn{2}{c|}{C100} 
            & \multicolumn{2}{c|}{T200}
            & \multicolumn{2}{c|}{B200}
            & \multicolumn{2}{c}{H683} \\

            & Methods 
            & $\forgetting\downarrow$ 
            & $\accuracy\uparrow$ 
            & $\forgetting\downarrow$ 
            & $\accuracy\uparrow$ 
            & $\forgetting\downarrow$ 
            & $\accuracy\uparrow$ 
            & $\forgetting\downarrow$ 
            & $\accuracy\uparrow$ 
            & $\forgetting\downarrow$ 
            & $\accuracy\uparrow$ \\
            \hline
    
            \multirow{3}{*}{\rotatebox[origin=c]{90}{\textbf{2-step}}}
            &CaSSLe~\cite{Fini2021SelfSupervisedMA} & 9.1 & 87.3 & 10.3 & 53.7 & 6.9 & 36.5 & 4.8 & 26.8 & 10.9 & 25.3 \\
            
            \cline{2-12}
            
            &\ours & 8.5 & \textbf{89.2} & 6.7 & \textbf{60.3} & 4.0 & \textbf{54.6} & 4.1 & \textbf{28.7} & 7.9 & \textbf{25.7} \\
            &\ourspp & 4.5 & \textbf{90.9} & 6.6 & \textbf{61.4} & \textbf{0.2} & \textbf{55.1} & 4.2 & \textbf{36.9} & \textbf{6.0} & \textbf{27.5}\\
            
            \toprule
            \multirow{3}{*}{\rotatebox[origin=c]{90}{\textbf{5-step}}}
            &CaSSLe~\cite{Fini2021SelfSupervisedMA} & 11.3 & 78.5 & 25.3 & 61.7 & 14.1 & 42.3 & 14.6 & 22.3 & 13.8 & 24.1\\
            
            \cline{2-12}
            
            &\ours & 8.2 & \textbf{85.4} & 15.6 & \textbf{63.7} & 9.2 & \textbf{53.3} & 13.7 & \textbf{28.9} & 3.1 & \textbf{25.2} \\
            &\ourspp & 7.6 & \textbf{91.7} & \textbf{12.3} & \textbf{67.7} & \textbf{1.6} & \textbf{56.5} & \textbf{0.6} & \textbf{41.1} & \textbf{2.7} & \textbf{26.1} \\
            
            \bottomrule
        \end{tabular}
    \end{center}
    % \vspace{-1.0cm}
\end{table}