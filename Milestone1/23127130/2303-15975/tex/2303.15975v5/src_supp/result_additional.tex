\section{Detailed Experimental Results}
\label{sec:app-exp-results}

\begin{figure}[!b]
\vspace{-4mm}
\begin{center}
    \includegraphics[width=0.95\linewidth]{fig/analysis_feat_replay.pdf}
\end{center}
% \vspace{-1mm}
\caption{Average output logits of our (a) \ours and (b) \ourspp for instances sampled from the 47th-class of C100. Results are evaluated on the four-step split at the end of the discovery task sequence.}
\label{fig:expt_featreplay}
\vspace{-6mm}
\end{figure}

\noindent \textbf{Qualititative Analysis of \compfrtitle (\compfr).} To better understand the benefit of \compfr, we present a qualitative analysis in Fig.~\ref{fig:expt_featreplay}. We show the average logit values obtained from the unified classifier $h\tstoend$, with and without using \compfr, for the test images of the $\texttt{47th}$ category in the four-step split C100. The plot reveals that in the absence of \compfr (see Fig.~\ref{fig:expt_featreplay}a), the logit corresponding to the incorrect cluster ($\texttt{89th}$, red-circled, discovered at the fourth task) exhibits a higher value compared to the correct cluster ($\texttt{32th}$, blue-circled, discovered at the second task), which corresponds to the $\texttt{47th}$ category. However, by incorporating \compfr (see Fig.~\ref{fig:expt_featreplay}b), the logits associated with the incorrect clusters become less active, as well as, the logit value for the correct cluster exhibits an increase, leading to better performance. This further demonstrates the effectiveness of \compfr in discriminating classes among all tasks.


\noindent \textbf{Per-step Comparison with the State-of-the-art Methods.} In this section, we provide comprehensive per-step comparative results of our \ours and \ourspp, juxtaposed with the adapted state-of-the-art methods on two task splits (two-step and five-step) of CIFAR-10 (C10)~\cite{krizhevsky2009learning}, CIFAR-100 (C100)~\cite{krizhevsky2009learning}, TinyImageNet-200 (T200)~\cite{le2015tiny}, CUB-200 (B200)~\cite{wah2011caltech} and Herbarium-683 (H683)~\cite{Tan2019TheHC} under \cincd setting in Fig.~\ref{fig:supp_sota_c10}, Fig.~\ref{fig:supp_sota_c100}, Fig.~\ref{fig:supp_sota_t200}, Fig.~\ref{fig:supp_sota_b200}, Fig.~\ref{fig:supp_sota_h683}, respectively. We report both the overall accuracy and maximum forgetting for each step, employing a task-agnostic evaluation.

As depicted in the reported figures, the overall accuracy exhibits a decline as the task sequence progresses, whereas the maximum forgetting for the novel classes discovered during the first step experiences an increase, attributable to the \textit{\forget} issue~\cite{wang2023comprehensive}. In the context of longer task sequences (five-step split, as observed in the top half of the figures), the \textit{forgetting} issue is exacerbated due to more frequent model updates.

During the first discovery task, the majority of adapted methods that unfreeze the final transformer block attain higher accuracy in most cases due to their adaptation to the current data. However, commencing from the second step, our \ours and \ourspp consistently surpass all compared methods across all datasets and splits in terms of overall accuracy. Although FRoST~\cite{Roy2022ClassincrementalNC} exhibits a better ability to mitigate forgetting for novel classes discovered in the first step in certain cases, our baselines demonstrate a more balanced performance between the past and current novel classes. The consistent experimental results from the five compared datasets and two task splitting strategies reiterate the preeminence of our \ours and \ourspp for the \cincd task. A straightforward combination of existing \il components and \ncd solutions proves insufficient for the \cincd task. While the two very recent \incd works (ResTune~\cite{liu2022residual} and FRoST~\cite{Roy2022ClassincrementalNC}) were designed for such unsupervised incremental scenarios, they fail to achieve satisfactory performance when the restrictive assumption of possessing a rich labelled base classes is relaxed. Conversely, our proposed baseline methods operate without the need for labelled base classes; nevertheless, utilizing rich labelled data to pre-supervise the self-supervised PTM can also be employed in our \ours and \ourspp to enhance single-step \ncd performance if such labelled data is accessible.

Lastly, upon comparing the top half (a and b) with the bottom half (c and d) of all the presented figures, it becomes evident that the accuracy/forgetting disparities between \ours and \ourspp widen as the task sequence lengthens. This observation underscores the significance and efficacy of the \compfr training employed by \ourspp in enhancing the class-discrimination capability across tasks.

\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.9\linewidth]{fig_supp/sota_c10.pdf}
\end{center}
\vspace{-1mm}
\caption{Comparison of our baseline methods with the adapted state-of-the-art methods (EwC, LwF, DER, ResTune, FRoST, CaSSLe) on \textbf{C10} under the \cincd setting. \textbf{Top (a, b)}: five-step split. \textbf{Bottom (c, d)}: two-step split. The overall accuracy and maximum forgetting are reported.}
\label{fig:supp_sota_c10}
\vspace{-5mm}
\end{figure}

\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.9\linewidth]{fig_supp/sota_c100.pdf}
\end{center}
\vspace{-1mm}
\caption{Comparison of our baseline methods with the adapted state-of-the-art methods (EwC, LwF, DER, ResTune, FRoST, CaSSLe) on \textbf{C100} under the \cincd setting. \textbf{Top (a, b)}: five-step split. \textbf{Bottom (c, d)}: two-step split. The overall accuracy and maximum forgetting are reported.}
\label{fig:supp_sota_c100}
\vspace{-5mm}
\end{figure}

\begin{figure}[!tbh]
\begin{center}
\includegraphics[width=0.9\linewidth]{fig_supp/sota_t200.pdf}
\end{center}
\vspace{-1mm}
\caption{Comparison of our baseline methods with the adapted state-of-the-art methods (EwC, LwF, DER, ResTune, FRoST, CaSSLe) on \textbf{T200} under the \cincd setting. \textbf{Top (a, b)}: five-step split. \textbf{Bottom (c, d)}: two-step split. The overall accuracy and maximum forgetting are reported.}
\label{fig:supp_sota_t200}
\vspace{-4mm}
\end{figure}

\begin{figure}[!tbh]
\begin{center}
\includegraphics[width=0.9\linewidth]{fig_supp/sota_b200.pdf}
\end{center}
\vspace{-1mm}
\caption{Comparison of our baseline methods with the adapted state-of-the-art methods (EwC, LwF, DER, ResTune, FRoST, CaSSLe) on \textbf{B200} under the \cincd setting. \textbf{Top (a, b)}: five-step split. \textbf{Bottom (c, d)}: two-step split. The overall accuracy and maximum forgetting are reported.}
\label{fig:supp_sota_b200}
\vspace{-4mm}
\end{figure}

\begin{figure}[!tbh]
\begin{center}
\includegraphics[width=0.9\linewidth]{fig_supp/sota_h683.pdf}
\end{center}
\vspace{-1mm}
\caption{Comparison of our baseline methods with the adapted state-of-the-art methods (EwC, LwF, DER, ResTune, FRoST, CaSSLe) on \textbf{H683} under the \cincd setting. \textbf{Top (a, b)}: five-step split. \textbf{Bottom (c, d)}: two-step split. The overall accuracy and maximum forgetting are reported.}
\label{fig:supp_sota_h683}
\vspace{-4mm}
\end{figure}

\clearpage
