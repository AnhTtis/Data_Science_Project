\vspace{-3mm}
\section{Conclusion}
In this work we address the practical yet challenging task of Class-incremental Novel Class Discovery (\cincd). First, we highlight that the use of self-supervised pre-trained models (PTMs) can achieve better or comparable performance to models trained with labelled data in \ncd. Building upon this observation, we propose to forego the need for expensive labelled data by leveraging PTMs for \cincd. Second, we introduce two simple yet strong baselines that comprise of frozen PTM, \compcnlong and \compfrlong. Notably, our proposed baselines demonstrate significant improvements over the state-of-the-art methods across five datasets. We hope our work can provide a new, promising avenue towards effective \cincd.