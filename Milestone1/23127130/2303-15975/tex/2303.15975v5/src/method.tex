\section{Method}
\label{sec:method}
\noindent\textbf{Problem Formulation.} As illustrated in Fig.~\ref{fig:setting_comparison}c, a \cincd model is trained continuously over $T$ sequential \ncd tasks, each of which, $\mathcal{T}\tst$, presents an unlabelled data set $\data\tst = \{\vx\tst_n\}^{N\tst}_{n=1}$ with $N\tst$ instances containing $C\tst$ novel classes that correspond to a label set $\Y\tst$. As in prior works~\cite{troisemaine2023novel}, we assume that novel classes in $\data\tsi$ and $\data\tsj$ are disjoint, \ie, $\Y\tsi \cap \Y\tsj = \emptyset$. Following the \ncd literature, we assume the number of novel classes $C\tst$ at each step is known as \textit{a priori}.
\begin{wrapfigure}{R}{0.55\textwidth}
    \vspace{-0.7cm}
    \centering
    \includegraphics[width=\linewidth]{fig/framework_vert.pdf}
    \lesspace
    \caption{
    Overview framework of the proposed methods \ours and \ourspp for class-iNCD task.
    }
    \label{fig:method_framework}
    % \vspace{-0.7cm}
    \lesspace
\end{wrapfigure}
During each discovery step $t$, we only have access to $\data\tst$. The aim of \cincd is to discover semantically meaningful categories in $\data\tst$ and accurately group the instances into the discovered clusters, without compromising its performance on the instances from $\data\tsone$ to $\data\tstminus$. In other words, a \cincd model comprises a unified mapping function $f \colon \X \to \bigcup_{t=1}^{T} \Y\tst$ that can group any test image $\vx$ into the categories $\bigcup_{t=1}^{T} \Y\tst$ discovered from the unlabelled task sequence $\taskb=\{\task\tsone, \task\tstwo, \cdots, \task\tsend\}$ without the help of task-id (\ie, task agnostic inference).

\subsection{Overall Framework}
\label{sec:method_overall_framework}
In this work our goal is to address \cincd by leveraging the priors learned by a self-supervised pre-trained model (PTM). To this end we propose a strong baseline called \ours that internally uses the PTM. As illustrated in Fig.~\ref{fig:method_framework}, the proposed \ours is marked by two steps -- (\textbf{i}) an initial \textbf{discovery step} (see pink box), where task-specific classifier is learned to discover the novel classes contained in $\data\tsone$ for the first task $\task\tsone$ with a clustering objective ($\mathcal{L}_\mathrm{baseline}$). Pseudo per-class prototypes are computed and stored; and (\textbf{ii}) it is followed by an \textbf{incremental discovery step} (see blue box), where \ours conducts the same discovery training, after which \textbf{task-agnostic inference} (see green box) is performed by simply concatenating the two learned task-specific classifiers. \ourspp further fine-tunes the concatenated classifier with $\mathcal{L}_\mathrm{past}$ and $\mathcal{L}_\mathrm{current}$ using the stored class prototypes to strength class-discrimination among tasks.
In the following sections, we first present a comprehensive overview of \ours. Additionally, we introduce an advanced variant of \ours, named \ourspp, which incorporates feature replay to further mitigate the issue of forgetting.

\noindent
\textbf{Discovery Step.}
In the introductory discovery task $\task\tsone$ (see Fig.~\ref{fig:method_framework}), we learn a mapping function $f\tsone \colon \X\tsone \to \Y\tsone$ in a self-supervised manner (\ie, using the Sinkhorn-Knopp cross-view pseudo-labelling~\cite{Caron2020UnsupervisedLO}) to discover the $C\tsone$ categories contained in the given unlabelled data set $\data\tsone$. The mapping function $f\tsone = h\tsone \circ g$ is modeled by a \textit{frozen} feature extractor $g(\cdot)$ and a \textit{Cosine Normalized} linear layer $h\tsone(\cdot)$ as task-specific classifier. The $g(\cdot)$ is initialized by the PTM weights $\theta_{g}$~\cite{Caron2021EmergingPI}, while $h\tsone(\cdot)$ is randomly initialized. In other words, only the classifier $h\tsone(\cdot)$ weights are learned during this step.

 
\noindent
\textbf{Incremental Discovery Step.}
After the first discovery step, $\data\tsone$ is discarded, and access to only $\data\tstwo$ is given in the first \textit{incremental} discovery step $\task\tstwo$ (see Fig.~\ref{fig:method_framework}). Same as the first step, we train a task-specific mapping function modeled by $f\tstwo = h\tstwo \circ g$. The $h\tstwo$ is newly initialized for the $C\tstwo$ novel classes of $\data\tstwo$, while the \textit{frozen} $g$ is shared across tasks. Thanks to the \textit{frozen} feature extractor and \textit{\compcntitle} (\compcn), \ours easily forms a unified model $f\tstotwo = h\tstotwo \circ g$ by sharing the feature extractor $g$, and concatenating the two task-specific heads $h\tstotwo(\cdot) = h\tsone(\cdot) \oplus h\tstwo(\cdot)$ for task-agnostic inference.

\noindent
\textbf{Task-agnostic Inference.} After training for $T$ steps, the inference on the test samples, belonging to any class presented in $\taskb$, is carried out with the final unified model $f\tstoend = h\tstoend \circ g$ in a task-agnostic manner (see Fig.~\ref{fig:method_framework}).

\subsection{Why Use Self-supervised Pre-trained Models?}
\begin{wraptable}{r}{0.49\textwidth}
    % \tablestyle{1.0pt}{0.1}
    \tiny
    \centering
    \vspace{-0.8cm}
    \caption{Analysis of \ncd accuracy using the same backbone (\vitbsixteen) with different pre-training settings.}
    \label{tab:ncd_pretraining}
    \begin{tabular}{lcccc}
        \toprule
        \multirow{2}{*}{Pre-training} & CIFAR-10 & CIFAR-100 & CUB-200 & \multirow{2}{*}{Avg. ($\Delta$)}\\
        & (5-5) & (50-50) & (100-100)&\\
        % \cline{2-5}
        \hline
        Supervised & 82.1 & 32.4 & 12.8 & 42.4\\
        PTM-DINO & \textbf{95.0} & 65.6 & 36.1 & 65.6 (\textcolor{darkgreen}{+23.2\%})\\
        PTM-DINO + Supervised & 94.5 & \textbf{67.2} & \textbf{42.5} & \textbf{68.1} (\textcolor{darkgreen}{+25.7\%})\\
     \bottomrule
    \end{tabular}
    % \vspace{-0.8cm}
    \lesspace
\end{wraptable}

Before delving into the specifics of our method, we first validate the benefits of leveraging self-supervised PTMs for \ncd, where supervised pre-training is the standard practice. Specifically, we conduct experiments with our \ours method under traditional \ncd setting and splits~\cite{fini2021unified} on three benchmarks (CIFAR-10, CIFAR-100 and CUB-200), comparing three pre-training strategies: (i) supervised pre-training on the labelled set starting from a randomly initialized model (as in Fig.~\ref{fig:setting_comparison}a), (ii) self-supervised PTM initialization (\eg, DINO~\cite{Caron2021EmergingPI}, a \textit{self-supervised} model) (as in Fig.~\ref{fig:setting_comparison}c), and (iii) supervised fine-tuning starting from PTM initialization. After this step the novel classes are discovered in the unlabelled set. In Tab.~\ref{tab:ncd_pretraining} we can see that the PTM-DINO, a model trained without any supervision, performs significantly better in discovering novel classes compared to the supervised counterpart (by +23.2\%), which is trained on the highly related base classes. This demonstrates that the original motivation of using a highly related labelled set to aid NCD~\cite{Han2019LearningTD} is clearly suboptimal when compared with self-supervised pre-training on a rather larger dataset. Additionally, fine-tuning PTM-DINO on the labelled samples only gives limited accuracy gain, with the PTM-DINO performing reasonably at-par (-2.5\%). Guided by these observations, we propose using strong PTMs (\eg, DINO~\cite{Caron2021EmergingPI}) with Vision Transformers (ViT)~\cite{Dosovitskiy2020AnII} as a new starting point for \ncd and \cincd, thereby eliminating the dependence on the labelled data.

\subsection{Strong Baselines for \cincd}
\label{sec:method_discovery}
In this section we detail the proposed methods, \ours and \ourspp, for solving the \cincd task. Both the baselines use PTMs, as backbone, that are general purpose and publicly available. Additionally, the \ourspp uses latent feature replay. The baselines have been designed to preserve stability on the \textit{past} novel classes, while being flexible enough to discover the novel classes in the \textit{current} task.


% \vspace{-.05in}
\begin{tcolorbox}[colback=gray!30,halign=center,valign=center,height=0.2in]
\textbf{Baseline}
\end{tcolorbox}
% \vspace{-.05in}

% \subsubsection{Baseline}
\noindent
\textbf{Self-supervised Training for Discovery.} Starting from a frozen feature extractor $g$, initialized with the weights from DINO~\cite{Caron2021EmergingPI}, we optimize a \textit{self-supervised} clustering objective to directly discover the novel categories at each step. In details, first we randomly initialize a learnable linear layer $h\tst$ as the task-specific classifier for the $C\tst$ novel classes contained in the unlabelled set $\data\tst$. To learn the task-specific network $f\tst$ for discovery, \ours employs the Sinkhorn-Knopp cross-view pseudo-labeling algorithm~\cite{Caron2020UnsupervisedLO}. We optimize a \textit{swapped} prediction problem, where the `code' $\vy_1$ of one view is predicted from the representation of another view $\vz_2$, derived from the same image $\vx$ through different image transformations, and vice-versa:

\begin{equation}
% \small
    \mathcal{L}_{\ours}= \ell(\vz_2, \vy_1) + \ell(\vz_1, \vy_2)
\label{eqn:ce}
\end{equation}
where $\ell(\cdot, \cdot)$ is the standard cross-entropy loss. We obtain the codes (or \textit{soft-targets}) $\vy_1$ and $\vy_2$ by using the \sinkhorn algorithm. 
%Refer to the supplement for details. 
Note that, we freeze the entire feature extractor $g$ during optimizing $\mathcal{L}_\ours$ as a straightforward way to prevent catastrophic forgetting.

\noindent
\textbf{Multi-step Class-Incremental Discovery.} Our ultimate goal is to learn a unified mapping function $f\tstoend \colon \X \to \bigcup_{t=1}^{T} \Y\tst$. If all the training data are available, an ideal clustering objective for $f\tstoend$ can be achieved by minimizing an adequate loss $\mathcal{L}\tstoend$ at the end of the task sequence:
\begin{equation}
% \small
% \vspace{-2mm}
\label{eqn:ideal}
    \mathcal{L}\tstoend =
    \E_{\task\tst\sim\taskb}
    % \bigl[
    %\E_{(\vx, \hat{\vy})\sim\task\tst} \ell(\hat{\vy}, f\tstoend(\vx)),
    % \bigr] 
    \mathcal{L}\tst
% \vspace{-2mm}
\end{equation}
%
However, due to the data unavailability of past tasks in \cincd, we can only pursue an approximation of this ideal joint objective defined by Eq.~\ref{eqn:ideal}. In this work, unlike most of the CIL solutions~\cite{wang2023comprehensive}, we pursue a better approximation from a new perspective: balancing the individual clustering objectives in each task to a unified importance. To be more specific, the proposed \ours adopts \textit{frozen feature extractor} with \textit{cosine normalized classifier} to unify the clustering objectives across tasks.
%, while \ourspp further leverages \textit{\compfrlong} training to achieve a better balance for all tasks. We introduce each of the proposed component below.
% The \textit{past-current} trade-off in this approximation incurs the notorious \textit{\forget} problem in \il tasks~\cite{wang2023comprehensive}.

\noindent\textit{\textbf{Frozen Feature Extractor.}} In \ours we freeze the entire PTM $g$ by introducing $\Vert \theta_{g}\tst - \theta_{g}\tstminus \Vert^{2} = 0$, $ t \in \{1, \ldots, T\}$ as a constraint. This enables us to leverage the power of the generalist PTM $g$ for all tasks \textit{equally}, without  introducing bias towards any particular task, \ie, avoiding the \textit{model drift} issue in CIL literature~\cite{wu2022class}.

\noindent\textit{\textbf{Cosine Normalization.}} The frozen feature extractor not only preserves the powerful prior knowledge from the pre-training data, but also maintains the cooperative mechanism between $g$ and each individual classifier $h\tst$. With the stable cooperative mechanism, the test data can be directly routed to the corresponding task-specific function network $f=h\tst \circ g$, if the task-id $t$ is available. However, the task-id is not available in \cincd. To achieve simple task-agnostic inference, we propose to apply \textit{\compcntitle} (\compcn)~\cite{Luo2017CosineNU,Hou2019LearningAU} on each individual linear classifier $h\tst$. This enables the learned classifiers to output scores of the same scale, avoiding imbalance between the past and current novel classes. 

Formally, given an input vector $\vx$, the L2 normalization operation can be defined as $\widetilde{\vx} = L2Norm(\vx) = \nicefrac{\vx}{\Vert \vx \Vert} = \nicefrac{\vx}{\sqrt{\vx\vx^{T} + \epsilon}}$, where $\epsilon$ is a small value to avoid division by zero and is set to 1e$^{-12}$ in this work.
%$\Vert \vx \Vert=\sqrt{\vx\vx^{T} + \epsilon}$ is L2-norm calculation ($\epsilon$ is a small value to avoid division by zero, which is set to 1e-12 in all experiments of this work). 
At every discovery step, $L2Norm(\cdot)$ is continuously applied to both the input feature embedding $\vz$ and each weight vector $\theta_{h}^{i}$ of the task-specific linear classifier $h\tst$. $\theta_{h}^{i} \in \R^{k}$ is the $i$-th column of the classifier weight matrix $\theta_{h}$, corresponding to one semantic cluster. Consequently, the $i$-th output logit from the classifier is then computed as:
\begin{equation}
% \small
\label{eqn:normlogits}
    \vl^{i} = \widetilde{\theta}_{h}^{i T} \widetilde{\vz} = \frac{\theta_{h}^{i T} \vz}{\Vert \theta_{h}^{i} \Vert \Vert \vz \Vert} = cos(\theta_{h}^{i})
\end{equation}
where $\Vert \theta_{h\tst}^{i} \Vert = \Vert \vz \Vert = 1$ and $cos(\theta_{h\tst}^{i})$ is the cosine similarity between the feature vector $\vz$ and the $i$-th weight vector $\theta_{h\tst}^{i}$. We thus use the term \textit{\compcn} for this operation.
%Therefore, the output calculation of $h\tst$ can be interpreted as the cosine similarity comparison between the input feature embedding and its weight vectors, hence the term \textit{\compcn} for this operation. 
The magnitude of the output logits $\vl$ is thereby unified to the same scale $[-1,1]$ for all classifiers from different steps.

\noindent
\textbf{Task-agnostic Inference.} Having the balanced classifier weights, we can then build a unified classification head $h\tstoend$ by simply concatenating the task-specific heads learned at each step $h\tstoend = h\tsone \oplus h\tstwo \oplus \ldots \oplus h\tsend$. By means of the frozen feature extractor and feature normalization, all the feature embedding $\widetilde{\vz}$ are mapped to the uniform feature space under the same scale. Incorporating with the normalized classifier weights in $h\tstoend$, task-agnostic inference can be fairly achieved using $f\tstoend=h\tstoend \circ g$ for all the discovered classes so far.

% \subsubsection{Baseline++}
% \vspace{-.05in}
\begin{tcolorbox}[colback=gray!30,halign=center,valign=center,height=0.2in]
\textbf{Baseline++}
\end{tcolorbox}
% \vspace{-.05in}

To take full advantage of the stable feature extractor, we propose \ourspp that additionally uses the learned model $f\tstminus = h\tstminus \circ g$ to compute the pseudo per-class feature prototypes $\protomean_{\hat{c}\tstminus}$ and variances $\protovar_{\hat{c}\tstminus}$ as \textit{proxies} for the novel classes discovered from the previous task $\mathcal{T}\tstminus$. For the subsequent tasks, features drawn from the Gaussian distribution, constructed with the stored $\protomean_{\hat{c}\tstminus}$ and $\protovar_{\hat{c}\tstminus}$, are replayed to reduce forgetting in the classifiers. We call this simplified replay mechanism as \textit{\compfrtitle} (\compfr) (see Fig.~\ref{fig:method_framework}), which we describe next.

\noindent
\textbf{Knowledge Transfer with Robust Feature Replay (\compfr).} 
At each previous discovery step $t \in \{1, \ldots, T-1\}$, \ourspp computes and stores a set $\Mu\tst = \{\mathcal{N}(\protomean_{\hat{c}_{j}\tst}, \protovar_{\hat{c}_{j}\tst})\}_{j=1}^{\classes\tst}$ that contains pseudo per-class feature prototype distributions derived from the unlabelled set $\data\tst$. Here, $\protomean_{\hat{c}_{j}\tst}$ and $\protovar_{\hat{c}_{j}\tst}$ are the calculated mean and variance of the feature embedding predicted by the task-specific model $f\tst$ as pseudo class $\hat{c}_{j}\tst$. Since the feature prototype set $\Mu\tst$ can represent and simulate the novel classes discovered at each previous step, \ourspp can further train the concatenated model $f\tstoend=h\tstoend \circ g$ by replaying the per-class features sampled from the saved Gaussian distributions in $\{\Mu\tsone, \ldots, \Mu\tstminuscap\}$ with the objective defined as:
\begin{equation}
% \small
\label{eqn:loss_past}
    % \begin{split}
    \mathcal{L}_\mathrm{past}=- \E_{\Mu\tst \sim \Mu\tstoendminusone}\E_{(\vz^{\hat{c}\tst}, \hat{\vy}^{\hat{c}\tst}) \sim \mathcal{N}(\protomean_{c\tst}, \protovar_{c\tst})}
    % &
    \sum_{j=1}^{\classes\tst}\hat{\vy}^{\hat{c}_{j}\tst} \log \sigma(\frac{h\tstoend(\vz^{\hat{c}_{j}\tst})}{\tau})
    % \end{split}
\end{equation}

where, $\sigma (\cdot)$ is a softmax function and $\tau$ is the temperature. By optimizing the objective defined in Eq.~\ref{eqn:loss_past}, \ourspp can better approximate the ideal objective defined in Eq.~\ref{eqn:ideal} by simulating the past data distribution. Furthermore, to maintain the clustering performance for the current novel classes in $\data\tsend$, we also transfer the knowledge from the current task-specific head $h\tsend$ to $h\tstoend$. In details, using the pseudo-labels $\hat{\vy}_{i}\tsend$ computed by the learned $f\tsend$, we can build a pseudo-labelled data set $\data_{PL}\tsend = \{\vx_{i}\tsend, \hat{\vy}_{i}\tsend\}_{i=1}^{N\tsend}$. The task-specific knowledge stored in the pseudo-labels can be then transferred to the unified classifier by optimizing the following objective:
\begin{equation}
% \small
\label{eqn:loss_now}
% \begin{split}
\mathcal{L}_\mathrm{current}=-\E_{(\vx\tsend, \hat{\vy}\tsend) \sim \data_{PL}\tsend}
    \sum^{C\tsend}_{j=1} \hat{y}_{j}^{c\tsend} \log \sigma(\frac{h\tstoend(g(\vx^{c\tsend}))}{\tau}).
% \end{split}
\end{equation}
The final \textit{past-current} objective for \compfr training at step $T$ of \ourspp is formulated as:
\begin{equation}
% \small
\label{eqn:loss_all}
    \mathcal{L}_\mathrm{\ourspp} = \mathcal{L}_\mathrm{past} + \mathcal{L}_\mathrm{current}
\end{equation}