\section{Introduction}
\label{sec:introduction}
Clustering unlabelled samples in a dataset is a long standing problem in computer vision, where the goal is to group samples into their respective semantic categories. Given, there could be multiple valid criteria (\eg, shape, size or color) that could be used to cluster data, Deep Clustering (DC)~\cite{Xie2015UnsupervisedDE} can at times lead to clusters without desired semantics. A more efficient alternative was proposed in the work of Novel Class Discovery (NCD)~\cite{Han2019LearningTD}, where the goal is to discover and learn new semantic categories in an unlabelled dataset by transferring prior knowledge from labelled samples of related yet disjoint classes (see Fig.~\ref{fig:setting_comparison}a). In other words, NCD can be viewed as unsupervised clustering guided by known classes. Due to its practical usefulness, the field of NCD has seen a tremendous growth, with application areas ranging from object detection~\cite{fomenko2022learning} to 3D point clouds~\cite{riz2023novel}.

\begin{wrapfigure}{R}{0.49\textwidth}
\centering
% \lesspace
\vspace{-0.9cm}
    \includegraphics[width=\linewidth]{fig/comp_ncd_pretrain.pdf}
    \vspace{-0.8cm}
    \caption{Comparison of traditional \textbf{Supervised} pre-training (Sup.) with self-supervised \textbf{pre-trained model} (PTM) initialization on the Novel Class Discovery.}
    \label{fig:ncd_pretrain_comp}
% \lesspace
\vspace{-0.7cm}
\end{wrapfigure}

A commonality in most of the \ncd methods~\cite{han2020automatically,fini2021unified} is that they rely on a reasonably large labelled dataset to learn good categorical and domain priors about the dataset. Thus, the success of these methods rely entirely on the availability of large labelled datasets, which might not always be guaranteed or can be expensive to collect. In this work we challenge the \textit{de facto} supervised pre-training step on a large labelled dataset for \ncd and show that supervised pre-training can be easily replaced by leveraging self-supervised pre-trained models (PTM), such as DINO~\cite{Caron2021EmergingPI}. PTMs being readily available off-the-shelf, it reduces the burden of pre-training on labelled data. As a part of a preliminary study, we compare supervised pre-training with PTMs and analyse their impact on the novel classes performance. As shown in Fig.~\ref{fig:ncd_pretrain_comp}, the PTMs achieve significantly better or at-par performance in comparison with the only supervised counterparts on all the datasets. Furthermore, when the PTMs are fine-tuned with supervised training on the labelled data, the performance is only marginally better. Note that the work in GCD~\cite{Vaze2022GeneralizedCD} used DINO as PTM, except it is used as initialization for the supervised training. Contrarily, we propose to entirely get rid of the supervised step.

Another striking drawback of the vast majority of \ncd methods, especially in ~\cite{han2020automatically,fini2021unified}, is that they assume access to the labelled dataset while discovering the novel (or \textit{new}) classes. Due to storage and privacy reasons the access to the labelled dataset can be revoked, which makes \ncd a very challenging problem. To address this, some very recent \cincdtitle (\cincd) methods~\cite{Roy2022ClassincrementalNC,Joseph2022NovelCD} have attempted to address \ncd from the lens of continual learning, by not accessing the labelled dataset when learning new classes (see Fig.~\ref{fig:setting_comparison}b). Albeit more practical than \ncd, the \cincd methods are still susceptible to catastrophic forgetting~\cite{French1999Catastrophic}, thereby impairing knowledge transfer from the labelled set to the unlabelled sets.

\begin{figure}[!t]
\begin{center}
\includegraphics[width=0.98\linewidth]{fig/c100_5steps.pdf}
\end{center}
% \lesspace
\vspace{-0.7cm}
\caption{Comparison of our proposed baselines with the incremental learning (EwC, LwF, DER), unsupervised incremental learning (CaSSLe), and \incd (ResTune, FRoST) methods on CIFAR-100. In each step 20 novel classes are learned. We report the Overall Accuracy and Maximum Forgetting.
}
\label{fig:sota_radar}
% \lesspace
\vspace{-0.7cm}
\end{figure}

In this work we aim to create a simple yet strong baseline for \cincd that can continually learn to cluster unlabelled data arriving in sessions, without losing its ability to cluster previously seen data. To this end, we propose \ours (see Fig.~\ref{fig:setting_comparison}c) that uses the DINO pre-trained ViT backbone, as a \textit{frozen} feature extractor, with a learnable linear \textit{cosine normalized} classifier~\cite{Hou2019LearningAU} on top. Every time an unlabelled set arrives, we simply train the task-specific classifier in a self-supervised manner, while keeping the backbone frozen. For testing we concatenate all the task-specific classifiers, yielding \textit{task-agnostic} inference. The simplicity of our approach lies in the decoupled training on task-specific data, while preserving performance across tasks. We characterize our \ours as \textit{frustratingly simple} as it \textit{neither} requires labelled data, \textit{nor} any specialized losses for preventing forgetting. Additionally, we propose \ourspp that stores discovered the novel class prototypes from the previous tasks to further reduce forgetting.

To verify the effectiveness of our proposed baselines, we compare with several state-of-the-art \cincd methods~\cite{liu2022residual,Roy2022ClassincrementalNC}, class-incremental learning methods (CIL)~\cite{kirkpatrick2017overcoming,li2017learning,buzzega2020dark} and unsupervised incremental learning (UIL)~\cite{Fini2021SelfSupervisedMA} methods adapted to the \cincd setting. In Fig.~\ref{fig:sota_radar} we plot the Overall Accuracy ($\mathcal{A}$) and Maximum Forgetting ($\mathcal{F}$) on CIFAR-100 for all the methods under consideration, where higher $\mathcal{A}$ and lower $\mathcal{F}$ is desired from an ideal method. Despite the simplicity, both the \ours and \ourspp surprisingly achieve the highest accuracy and least forgetting among all the competitors. Thus, our result sets a precedent to future \cincd methods and urge them to meticulously compare with our baselines, that are as simple as having a frozen backbone and a linear classifier.

In a nutshell, our \textbf{contributions} are three-fold: (\textbf{i}) We bring a paradigm shift in \ncd by proposing to use self-supervised pre-trained models as a new starting point, which can substitute the large annotated datasets. (\textbf{ii}) We, for the first time, highlight the paramount importance of having strong baselines in \cincd, by showcasing that simple baselines if properly implemented can outperform many state-of-the-art methods. To that end, we introduce two baselines (\ours and \ourspp) that are simple yet strong. (\textbf{iii}) We run extensive experiments on multiple benchmarks and for longer incremental settings.

To foster future research, we release a modular and easily expandable \href{https://github.com/OatmealLiu/MSc-iNCD}{PyTorch repository} for the \cincd task, that will allow practioners to replicate the results of this work, as well as build on top of our strong baselines.