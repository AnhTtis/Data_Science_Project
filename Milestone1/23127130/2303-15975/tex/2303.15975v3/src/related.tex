\section{Related Work}
\label{sec:related_work}

\noindent
% \textbf{\ncdtitle (\ncd)}
\textbf{\ncdtitle (\ncd)} was formalized by \cite{Han2019LearningTD} with the aim of alleviating the innate ambiguity in deep clustering~\cite{Chang2017DeepAI,Dizaji2017DeepCV,Xie2015UnsupervisedDE,Yang2016TowardsKS,Yang2016JointUL} and enhancing the clustering ability of novel classes in an unlabelled dataset, by leveraging the prior knowledge derived from related labelled samples~\cite{Hsu2017LearningTC,Hsu2019MulticlassCW,Han2019LearningTD}. Many of the recent \ncd works utilize a joint training scheme that assumes access to both labelled and unlabelled data concurrently to exploit strong learning signal from the labelled classes~\cite{han2020automatically,Zhong2020OpenMixRK,fini2021unified,Jia2021JointRL,zhong2021neighborhood,Zhao2021NovelVC,Vaze2022GeneralizedCD,Fei2022XConLW,Yang2022DivideAC}. 

Keeping in mind the data regulatory practices, the \ncd community has been paying more attention to the problem of \incdtitle (\incd)~\cite{liu2022residual} where the access to the labelled (or base) dataset is absent during the discovery stage. Unlike \incd, FRoST~\cite{Roy2022ClassincrementalNC} and NCDwF~\cite{Joseph2022NovelCD} investigate a more realistic yet challenging setting known as \cincdtitle (\cincd), where task-id information is not available during inference. However, all the \cincd methods so far have investigated learning in short incremental scenarios (2 steps in \cite{Roy2022ClassincrementalNC} and 1 step in \cite{Joseph2022NovelCD}). Differently, we explore a more realistic setting of longer incremental setting (up to 5 steps) and show that many existing \cincd methods deteriorate in such settings.

Importantly, staying aligned with the original motivation of the \ncd and GCD paradigm -- \textit{discovering new classes by leveraging prior knowledge} -- we propose a new direction to tackle the \cincd problem, \ie, by solely leveraging the prior knowledge learned from self-supervised PTMs (\eg, DINO~\cite{Caron2021EmergingPI}), as opposed to relying on a large amount of expensive highly related \textit{labelled} data.
% or \textit{noisy web-scale} (\eg, CLIP~\cite{Radford2021LearningTV}

\noindent
\textbf{Class-incremental Learning (CIL)}~\cite{masana2022class} aims to train a model on a sequence of tasks with access to labelled data only from the current task, while the model's performance is assessed across all tasks it has encountered to date. Notably, the \il methods~\cite{kirkpatrick2017overcoming,Rebuffi2016iCaRLIC,li2017learning,buzzega2020dark} are devised with a dual objective of mitigating \textit{\forget}~\cite{French1999Catastrophic} of the model's knowledge on the previous tasks, while concurrently enabling it to learn new ones in a flexible manner. To overcome the need of labelled data, unsupervised incremental learning (UIL)~\cite{Fini2021SelfSupervisedMA,madaan2022representational,lin2022continual} have recently been proposed that aim to learn generalized feature representation via self-supervision to reduce forgetting. Different from UIL, that solely aims to learn a feature encoder, the \cincd methods additionally learn a classifier on top of the encoder to classify the unalabelled samples.

Moreover, as shown in the \cincd method FRoST~\cite{Roy2022ClassincrementalNC}, due to the differences in the learning objectives of \cincd during the supervised pre-training and unsupervised novel class discovery stages, learning continuously is more challenging than the supervised CIL setting. Our proposed baselines attempt to mitigate this issue with \compcnlong of the classifier weights, frozen backbone and feature replay using prototypes, thus greatly simplifying \cincd.
