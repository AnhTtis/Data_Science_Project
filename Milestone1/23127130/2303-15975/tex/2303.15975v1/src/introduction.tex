\section{Introduction}
\label{sec:introduction}
In this work we study the problem of Novel Class Discovery (\ncd)~\cite{Han2019LearningTD} where the goal is to train neural networks to discover (or group) novel visual concepts present in an \textit{unlabelled} dataset into semantically meaningful clusters, while leveraging prior knowledge learned from supervised pre-training on a \textit{labelled} dataset containing disjoint classes (see Fig~\ref{fig:setting_comparison}b). Note that \ncd is different from fully unsupervised clustering as there can be several criteria to cluster a dataset unsupervisedly (see Fig.~\ref{fig:setting_comparison}a). Ever since the pioneering work by Han \etal,~\cite{Han2019LearningTD} the field of \ncd has seen a tremendous growth (see survey in ~\cite{troisemaine2023novel}), showing the relevance of such a problem in the real-world applications. 

A commonality in all the \ncd methods is that they rely on a reasonably large labelled dataset to learn good categorical and domain priors about the dataset. Thus, the success of these methods rely entirely on the availability of large labelled datasets, which might not always be guaranteed. Another striking drawback of the vast majority of \ncd methods, especially~\cite{han2020automatically,fini2021unified}, is that they assume access to the labelled dataset while discovering the novel (or \textit{new}) classes. Due to storage and privacy reasons the access to the labelled dataset can be revoked, which makes \ncd a very challenging problem. To address this, some very recent \cincdtitle (\cincd) methods~\cite{Roy2022ClassincrementalNC,Joseph2022NovelCD} have attempted to address \ncd from the lens of continual learning, by not accessing the labelled dataset when learning new classes (see Fig.~\ref{fig:setting_comparison}c). Albeit more practical than \ncd, the \cincd methods are still susceptible to catastrophic forgetting~\cite{French1999Catastrophic} for long sequences of learning tasks~\cite{Roy2022ClassincrementalNC}, thereby impairing knowledge transfer from the labelled dataset to the unlabelled sets.

In this work we challenge the \textit{de facto} supervised pre-training step on large labelled datasets for \ncd and show that it can be easily outperformed by large-scale pre-trained models (\eg, DINO~\cite{Caron2021EmergingPI}). As a part of a preliminary study, we compare supervised pre-training with large-scale pre-trained models and analyse their impact on the novel classes performance. As shown in Fig.~\ref{fig:ncd_pretrain_comp}, without \textit{bells and whistles} the large-scale pre-trained models attain significantly better or at-par performance in comparison to the only supervised counterparts on all the datasets. Moreover, when large-scale pre-trained models are paired with supervised training on labelled data, the performance is only marginally better. Guided by this remarkable observation, we seek for strong baselines that can better exploit pre-trained models and dispose off the need of expensive labelled data.


\begin{figure}[!t]
\begin{center}
   \includegraphics[width=0.7\columnwidth]{fig/comp_ncd_pretrain.pdf}
\end{center}
\vspace{-4.8mm}
\caption{Comparison of the impact of traditional \textbf{Supervised} pre-training (Sup.) with our proposed \textbf{Large-scale Self-Supervised} pre-training (L.s. Pre.) on the novel class discovery performance.}
\label{fig:ncd_pretrain_comp}
\vspace{-.1in}
\end{figure}

In this work, our focus is \textit{not} developing novel components for \ncd or \cincd, but to make minimal adaptations to the existing methods to establish strong baselines. In details, we propose \ours that uses the DINO pre-trained ViT backbone, as a \textit{frozen} feature extractor, with a learnable linear \textit{cosine normalized} classifier~\cite{Hou2019LearningAU} on top to classify the new classes. We train \ours in a self-supervised manner by employing the Sinkhorn-Knopp cross-view pseudo-labelling~\cite{Caron2020UnsupervisedLO}. Extending \ours to more than one task simply involves concatenating the task-specific classifiers during inference. Furthermore, inspired by~\cite{Roy2022ClassincrementalNC} we propose \ourspp that additionally stores pseudo prototypes for each discovered novel class from the previous steps, which are then replayed by sampling from a Gaussian centered around those prototypes.

\begin{figure}[!t]
\begin{center}
\includegraphics[width=0.9\linewidth]{fig/line_sota_comp.pdf}
\end{center}
\vspace{-.17in}
\caption{Comparison of our baseline methods with the incremental learning (EwC, DER) and \cincd (ResTune, FRoST) methods under the \newsetting setting. 20 novel classes are learned at each step. We report the Overall Accuracy and Maximum Forgetting.}
\label{fig:sota_radar}
\vspace{-.15in}
\end{figure}

Given, a learning agent in most real world applications will be exposed to long sequences of tasks, we propose slight modifications to the \cincd setting, where the model must discover novel classes in \textit{multiple} increments, by neither accessing the labelled data nor the past unlabelled data. We call this challenging yet pragmatic setting as \newsettingtitle (\newsetting) (see Fig.~\ref{fig:setting_comparison}d). We pit our proposed baselines against several state-of-the-art \cincd methods~\cite{liu2022residual,Roy2022ClassincrementalNC} and  class-incremental learning methods~\cite{kirkpatrick2017overcoming,buzzega2020dark} adapted to the \newsetting setting. In Fig.~\ref{fig:sota_radar} we plot the Overall Accuracy ($\mathcal{A}$) and Maximum Forgetting ($\mathcal{F}$) on CIFAR-100 for all the methods under consideration, where higher $\mathcal{A}$ and lower $\mathcal{F}$ is desired from an ideal method. Despite the simplicity, both the \ours and \ourspp surprisingly achieve the highest accuracy and least forgetting among all the competitors. Thus, our result sets a precedent to future \newsetting methods, while urging them to meticulously compare with baselines, that are as simple as having a frozen backbone and a linear classifier (see Sec.~\ref{sec:method} for training details).

In a nutshell, our \textbf{contributions} are three-fold: (\textbf{i}) We bring a paradigm shift in \ncd by proposing to use large-scale pre-trained models as a new starting point, which can substitute the large annotated datasets; and (\textbf{ii}) We propose a realistic setting called \newsettingtitle (\newsetting) that reflects real world learning scenarios; and (\textbf{iii}) We for the first time highlight the paramount importance of having strong baselines in \ncd, by showcasing that simple baselines if properly trained can outperform many state-of-the-art \ncd methods. To that end, we introduce two simple baselines (\ours and \ourspp) that are surprisingly strong yet simple.

We run thorough experimental evaluation on five visual datasets under numerous incremental scenarios. To foster future research, we plan to release a modular PyTorch repository for the \newsetting task, allowing for easy replication and adoption of not only our baselines, but also typical \ncd and \incd methods.