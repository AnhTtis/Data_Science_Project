\section{Experiments}
\label{sec:experiments}
\subsection{Experimental Settings}
\label{sec:expt_setup}
\noindent
\textbf{Datasets and Splits.} We conduct experiments on three generic image recognition datasets and two fine-grained recognition datasets: CIFAR-10 (C10)~\cite{krizhevsky2009learning}, CIFAR-100 (C100)~\cite{krizhevsky2009learning}, TinyImageNet-200 (T200)~\cite{le2015tiny}, CUB-200 (B200)~\cite{wah2011caltech} and Herbarium-683 (H683)~\cite{Tan2019TheHC}. For each dataset, we adopt two strategies (two-step and five-step) to generate the task sequences, where the total classes and corresponding instances of training data are divided averagely for each step. The test data are used for evaluation. Detailed data splits are provided in the supplementary material.

\noindent
\textbf{Evaluation Protocol.} We evaluate all the methods in \newsetting using the \textbf{task-agnostic} evaluation protocol~\cite{Roy2022ClassincrementalNC}. Specifically,  we do not know the task ID of the test sample during inference, and the network must route the sample to the correct segment of the unified classifier.

\noindent
\textbf{Evaluation Metrics.}
We report two metrics: maximum forgetting $\forgetting$ and overall discovery accuracy (or clustering accuracy~\cite{Roy2022ClassincrementalNC}) $\accuracy$ for all discovered classes by the end of the task sequence. $\forgetting$ measures the difference in clustering accuracy between the task-specific model $f\tsone$ and the unified model $f\tstoend$ (at the last step) for samples belonging to novel classes discovered at the first step. $\accuracy$ is the clustering accuracy from the unified model $f\tstoend$ on instances from all the novel classes discovered by the end of the task sequence.

\subsection{Implementation Details}
\label{sec:expt_implementation}

\noindent
\textbf{\ours and \ourspp.}
By default, \vitbsixteen~\cite{Dosovitskiy2020AnII} is used as the backbone $g$ with DINO~\cite{Caron2021EmergingPI} initialization for all data sets. The 768-dimensional output vector $\vz \in \R^{768}$, from the $[CLS]$ token followed is used as the deep features extracted from a given image. Following the backbone, one \textit{\cosnormed} linear layer (without bias) is randomly initialized as the task-specific classifier $h\tst$ with $\classes\tst$ output neurons. The backbone $g$ is frozen during training. Soft pseudo-labels self-supervised are generated using the \sinkhorn~\cite{Cuturi2013SinkhornDL, Caron2020UnsupervisedLO} algorithm with default hyper-parameters (number of iterations = 3 and $\epsilon=0.05$).

\noindent
\textbf{Training.}
For each individual task, we train the model for 200 epochs on the given unlabelled data set $\data\tst$ with the same data augmentation strategy~\cite{Chen2020ASF} in all the experiments. After the discovery stage, \ourspp further conducts \compfr training on the unified model $f\tstot$ for 200 epochs. A cosine annealing learning rate scheduler with a base rate of 0.1 is used. The model is trained on mini-batches of size 256 using SGD optimizer with a momentum of 0.9 and weight decay $10^{-4}$. The temperature $\tau$ is set to 0.1.

\subsection{Analysis and Ablation Study}
\label{sec:expt_self_analysis}

\begin{table}[!t]
\small
\setlength{\tabcolsep}{3.4pt}
\begin{center}
    \begin{tabular}{cl|cc|cc|cc}
        \toprule
        
        & Datasets & \multicolumn{2}{c|}{C10} & \multicolumn{2}{c|}{C100} & \multicolumn{2}{c}{T200} \\
        
        & Methods & $\forgetting\downarrow$ & $\accuracy\uparrow$ & $\forgetting\downarrow$ & $\accuracy\uparrow$ & $\forgetting\downarrow$ & $\accuracy\uparrow$ \\
        \hline
        \multirow{5}{*}{\rotatebox[origin=c]{90}{Two-step}}&Kmeans~\cite{Jain2008DataC5} & 93.9 & 87.3 & 68.2 & 56.7 & 62.0 & 47.1 \\
        &Joint (frozen) & 4.9 & 92.1 & 5.3 & 61.8 & 3.3 & 51.1 \\
        &Joint (unfrozen) & \textbf{0.8} & \textbf{92.4} & \textbf{2.5} & \textbf{65.2} & 2.3 & \textbf{56.5} \\
        
        \cline{2-8}
        
        &\ours & 8.5 & {89.2} & 6.7 & {60.3} & 4.0 & {54.6} \\
        &\ourspp & 4.5 & {90.9} & 6.6 & {61.4} & \textbf{0.2} & {55.1} \\
        
        \toprule

        \multirow{5}{*}{\rotatebox[origin=c]{90}{Five-step}}&Kmeans~\cite{Jain2008DataC5} & 99.1 & 82.1 & 76.3 & 54.3 & 66.0 & 52.9 \\
        &Joint (frozen) & 5.1 & 93.8 & 10.5 & 68.6 & 1.8 & 57.8 \\
        &Joint (unfrozen) & \textbf{1.5} & \textbf{97.5} & \textbf{5.9} & \textbf{74.9} & 3.0 & \textbf{60.7} \\ 
        \cline{2-8}
        &\ours & 8.2 & {85.4} & 15.6 & {63.7} & 9.2 & {53.3} \\
        &\ourspp & 7.6 & {91.7} & 12.3 & {67.7} & \textbf{1.6} & {56.5} \\
        
        \bottomrule
    \end{tabular}
\end{center}
\vspace{-3mm}
\caption{Comparison of our proposed baselines with reference methods (Kmeans, Joint (frozen), Joint (unfrozen)) on two task splits of C10, C100 and T200.}
\label{tab:expt_upperlower}
\end{table}

\noindent
\textbf{Comparison with Reference Methods.}
We first establish reference methods using K-means~\cite{Arthur2007kmeansTA} and joint training scheme ($\upperb$, based on \ours but access to the previous training data is given)~\cite{li2017learning}, respectively. To further enhance the upper reference performance, we unfreeze and fine-tune the last transformer block during training on joint data sets, which is referred as to $\upperbpp$ method.

We evaluate the proposed methods against the established reference methods on three object recognition data sets in Tab.~\ref{tab:expt_upperlower}. The joint training methods slightly outperform the proposed baselines on all data sets and splits, as they can jointly optimize the ideal objective defined in Eq.~\ref{eqn:ideal} using the given access to all training data. $\upperbpp$ with the unlocked last transformer block further lifts the upper bound of \newsetting by optimizing the feature extractor without worrying about corrupting the cooperation mechanism between $g$ and $h\tstoend$ and the task-recency bias issue. The performance gain from joint training is more noticeable under the five-step split setting due to increased forgetting severity. Nonetheless, we observe that the proposed baselines perform nearly as well as the joint training methods, indicating limited benefits from access to all unlabelled data  in this unsupervised setting compared to the supervised \cil setting~\cite{wang2023comprehensive}. This demonstrates the effectiveness of our baselines.

\begin{table}[!t]
\small
\setlength{\tabcolsep}{2.8pt}
\begin{center}
    \begin{tabular}{cccccc|cc|cc}
        \toprule
        
        &&\multicolumn{2}{l}{Datasets} & \multicolumn{2}{c|}{C10} & \multicolumn{2}{c|}{C100} & \multicolumn{2}{c}{T200} \\
        
        &&\compcn & \compfr  & $\forgetting\downarrow$ & $\accuracy\uparrow$ & $\forgetting\downarrow$ & $\accuracy\uparrow$ & $\forgetting\downarrow$ & $\accuracy\uparrow$ \\
        \hline
        \multirow{4}{*}{\rotatebox[origin=c]{90}{Two-step}}&(a)&\Checkmark & \Checkmark & \textbf{4.5} & \textbf{90.9} & \textbf{6.6} & \textbf{61.4} & \textbf{0.2} & \textbf{55.1} \\
        &(b)&\Checkmark & \XSolidBrush & 8.5 & 89.2 & 6.7 & 60.3 & 4.0 & 54.6 \\
        &(c)&\XSolidBrush & \Checkmark & 8.2 & 80.2 & 5.1 & 54.1 & 3.3 & 38.9\\
        &(d)&\XSolidBrush & \XSolidBrush & 16.1 & 74.3 & 7.3 & 50.1 & 4.3 & 33.2 \\
        
        \hline

        \hline
        \multirow{4}{*}{\rotatebox[origin=c]{90}{Five-step}}&(a)&\Checkmark & \Checkmark & \textbf{7.6} & \textbf{91.7} & \textbf{12.3} & \textbf{67.7} & \textbf{1.6} & \textbf{56.5} \\
        &(b)&\Checkmark & \XSolidBrush & 8.2 & 85.4 & 15.6 & 63.7 & 9.2 & 53.3 \\
        &(c)&\XSolidBrush & \Checkmark &6.3 & 90.7 & 14.3 & 58.2 & 0.7 &49.7 \\
        &(d)&\XSolidBrush & \XSolidBrush & 10.9 & 80.3 & 16.6 & 49.1 & 8.1 & 41.9 \\
        \bottomrule
    \end{tabular}
\end{center}

\vspace{-3mm}
\caption{Self-ablation analysis of the proposed components (\compcn and \compfr) on two task splits of C10, C100 and T200.}
\label{tab:expt_selfablation}
\vspace{-5mm}
\end{table}

\begin{figure}[!t]
\begin{center}
    \includegraphics[width=0.95\linewidth]{fig/analysis_feat_replay.pdf}
\end{center}
\vspace{-3mm}
\caption{Average output logits of our (a) \ours and (b) \ourspp for instances sampled from the 47th-class. Results are evaluated on the four-step split of C100 at the end of the discovery task sequence.}
\label{fig:expt_featreplay}
\vspace{-3mm}
\end{figure}

\noindent
\textbf{Ablation on Proposed Components.} We further present an ablation study on individual core components of our methods, namely \compcn and \compfr. Results are shown in Tab.~\ref{tab:expt_selfablation}. It is noticeable from the results that \compcn plays a substantial role in enhancing the overall accuracy of our proposed baselines (refer to \ours: \textit{(b) v.s. (d)} and \ourspp: \textit{(a) v.s. (c)}). This is attributed to its unification capability to effectively address the issue of that the weight vectors with significant magnitudes in $f\tstoend=h\tstoend \circ g$ always dominating the prediction. On the other hand, \compfr can improve the overall accuracy and mitigate the forgetting at the end of each task sequence (refer to \textit{(a) v.s. (b)} and \textit{(c) v.s. (d)}). Of particular note is that the performance gain attained by using \compfr is more significant when dealing with longer task sequences (refer to the \textit{upper half v.s. lower half} in Tab.~\ref{tab:expt_selfablation}). The underlying reason for this is that the stored pseudo per-class feature prototype enables our methods to better approximate the ideal objective defined in Eq.~\ref{eqn:ideal} using the \textit{past-current} objective defined in Eq.~\ref{eqn:loss_all} by simulating the past discovered classes distribution while transferring the knowledge from the currently learned task-specific classifier $h\tst$. \ourspp (a) equipped with both \compcn and \compfr achieves the best overall accuracy and the least forgetting in all splits. Notably, the proposed \ours (b) can achieve good performance with only a slight performance drop compared to \ourspp.

To better understand the benefit of \compfr, we present a qualitative analysis in Fig.~\ref{fig:expt_featreplay}. We show the average logit values obtained from the unified classifier $h\tstoend$, with and without using \compfr, for the test images of the $\texttt{47th}$ category in the four-step split C100. The plot reveals that in the absence of \compfr (see Fig.~\ref{fig:expt_featreplay}a), the logit corresponding to the incorrect cluster ($\texttt{89th}$, red-circled, discovered at the fourth task) exhibits a higher value compared to the correct cluster ($\texttt{32th}$, blue-circled, discovered at the second task), which corresponds to the $\texttt{47th}$ category. However, by incorporating \compfr (see Fig.~\ref{fig:expt_featreplay}b), the logits associated with the incorrect clusters become less active, as well as, the logit value for the correct cluster exhibits an increase, leading to better performance. This further demonstrates the effectiveness of \compfr in discriminating classes among all tasks.

\begin{table}[!t]
\small
\begin{center}
    \begin{tabular}{lcccccc}
        \toprule
         &\multicolumn{6}{c}{\textbf{\ours}}\\
        Datasets & \multicolumn{2}{c}{C10} & \multicolumn{2}{c}{C100} & \multicolumn{2}{c}{T200}\\
        
        Backbones& $\forgetting \downarrow$ & $\accuracy \uparrow$ & $\forgetting \downarrow$ & $\accuracy \uparrow$ & $\forgetting \downarrow$ & $\accuracy \uparrow$ \\
        \hline
        ResNet50-DINO & 37.5 & 45.8 & 16.4 & 38.5 & 10.1 & 24.7 \\
        ViT-B/16-DINO & 8.2 & 85.4 & \textbf{15.6} & \textbf{63.7} & \textbf{9.2} & \textbf{53.3}\\
        ViT-B/16-CLIP & \textbf{5.3}& \textbf{87.5} & 17.1 & 62.4 & 15.7 & 42.5 \\  
        
        \hline        
        &\multicolumn{6}{c}{\textbf{\ourspp}}\\
        ResNet50-DINO & 36.3 & 46.9 & 14.9 & 40.1 & 6.9 & 27.9 \\
        ViT-B/16-DINO & \textbf{7.6} & \textbf{91.7} & \textbf{12.3} & \textbf{67.7} & \textbf{1.6} & \textbf{56.5} \\
        ViT-B/16-CLIP & 8.9 & 89.8 & 15.0 & 62.9 & 6.4 & 47.1 \\  
        
        \bottomrule
    \end{tabular} 
\end{center}
\vspace{-3mm}
\caption{Ablation analysis of backbones and self-supervised learning strategies on five-step splits of C10, C100 and T200.}
\label{tab:expt_backboneablation}
\vspace{-2mm}
\end{table}

\noindent
\textbf{Evaluation on Large-scale Pre-trained Backbones.} In Tab.~\ref{tab:expt_backboneablation} we present a comparison of using different types of pre-trained backbones (ResNet50~\cite{He2015DeepRL} and \vitbsixteen~\cite{Dosovitskiy2020AnII}) and training strategies (CLIP~\cite{Radford2021LearningTV} and DINO~\cite{Caron2021EmergingPI}). When utilizing \vitbsixteen as the backbone, CLIP based pre-training attains comparable results to the DINO-ViT ones that achieve slightly better performance.
Moreover, with using the DINO, transformer architecture largely outperforms ResNet50, owing to its superior discrimination ability~\cite{Naseer2021IntriguingPO}.
\begin{table*}[!t]
\small
\setlength{\tabcolsep}{2.8pt}
\begin{center}
    \begin{tabular}{l|cc|cc|cc|cc|cc|cc|cc|cc|cc|cc}
        \toprule
       \multirow{2}{*}{Datasets}& \multicolumn{10}{c|}{Two-step} & \multicolumn{10}{c}{Five-step}\\       
       & \multicolumn{2}{c|}{\rotatebox[origin=c]{0}{C10}}& \multicolumn{2}{c|}{\rotatebox[origin=c]{0}{C100}}& \multicolumn{2}{c|}{\rotatebox[origin=c]{0}{T200}}& \multicolumn{2}{c|}{\rotatebox[origin=c]{0}{B200}}& \multicolumn{2}{c|}{\rotatebox[origin=c]{0}{H683}}&\multicolumn{2}{c|}{\rotatebox[origin=c]{0}{C10}}& \multicolumn{2}{c|}{\rotatebox[origin=c]{0}{C100}}& \multicolumn{2}{c|}{\rotatebox[origin=c]{0}{T200}}& \multicolumn{2}{c|}{\rotatebox[origin=c]{0}{B200}}& \multicolumn{2}{c}{\rotatebox[origin=c]{0}{H683}}\\
               
        Methods & $\forgetting\downarrow$ & $\accuracy\uparrow$ & $\forgetting\downarrow$ & $\accuracy\uparrow$ & $\forgetting\downarrow$ & $\accuracy\uparrow$ & $\forgetting\downarrow$ & $\accuracy\uparrow$ & $\forgetting\downarrow$ & $\accuracy\uparrow$ & $\forgetting \downarrow$ & $\accuracy \uparrow$ & $\forgetting \downarrow$ & $\accuracy \uparrow$ & $\forgetting \downarrow$ & $\accuracy \uparrow$ & $\forgetting \downarrow$ & $\accuracy \uparrow$ & $\forgetting \downarrow$ & $\accuracy \uparrow$ \\
        
        \hline
        EwC~\cite{kirkpatrick2017overcoming} & 32.4 & 79.0 & 42.5 & 43.9 & 27.2 & 33.3 & 18.1 & 25.5 & 13.8 & 25.1  & 21.1 & 81.1 & 60.1 & 30.6 & 48.0 & 23.2 & 21.2 & 19.1 & 15.7 & 22.4 \\
        LwF~\cite{li2017learning} & 30.4 & 34.4 & 44.1 & 42.4 & 40.0 & 27.2 & 20.2 & 23.9 & 16.3 & 24.9 & 20.1 & 25.8 & 60.9 & 16.1 & 53.7 & 15.6 & 21.7 & 15.7 & 16.5 & 23.4 \\
        DER~\cite{buzzega2020dark} & 49.0 & 69.9 & 29.8 & 30.3 & 39.0 & 28.9 & 5.0 & 20.4 & 14.0 & 24.7 & 30.1 & 76.2 & 62.6 & 36.2 & 52.1 & 21.7 & 16.2 & 16.3 & 18.0 & 22.3 \\
        ResTune~\cite{liu2022residual} & 97.6 & 47.2 & 32.7 & 17.1 & 32.3 & 17.2 & 12.0 & 13.0 & 27.4 & 17.1 & 95.5 & 49.2 & 83.3 & 19.4 & 60.4 & 12.2 & 24.2 & 12.4 & 28.2 & 11.2 \\
        FRoST~\cite{Roy2022ClassincrementalNC} & \textbf{2.5} & 46.6 & \textbf{4.7} & 34.2 & 4.3 & 26.1 & \textbf{3.9} & 17.6 & 16.2 & 18.4 & \textbf{0.9} & 69.2 & 14.2 & 43.6 & 14.4 & 31.0 & 19.4 & 18.5 & 13.5 & 23.4 \\
        \hline
        \textbf{\ours} & 8.5 & \textbf{89.2} & 6.7 & \textbf{60.3} & 4.0 & \textbf{54.6} & 4.1 & \textbf{28.7} & 7.9 & \textbf{25.7} & 8.2 & \textbf{85.4} & 15.6 & \textbf{63.7} & 9.2 & \textbf{53.3} & 13.7 & \textbf{28.9} & 3.1 & \textbf{25.2} \\
        \textbf{\ourspp} & 4.5 & \textbf{90.9} & 6.6 & \textbf{61.4} & \textbf{0.2} & \textbf{55.1} & 4.2 & \textbf{36.9} & \textbf{6.0} & \textbf{27.5}  & 7.6 & \textbf{91.7} & \textbf{12.3} & \textbf{67.7} & \textbf{1.6} & \textbf{56.5} & \textbf{0.6} & \textbf{41.1} & \textbf{2.7} & \textbf{26.1} \\
        \bottomrule
    \end{tabular}
\end{center}
\caption{Comparison with the adapted state-of-the-art methods on two task splits of C10, C100, T200, B200, and H683 under \newsetting setting. Overall accuracy and maximum forgetting are reported. All methods use DINO-\vitbsixteen as feature encoder.}
\label{tab:expt_sota}
\vspace{-5mm}
\end{table*}

\subsection{Comparison with the State-of-the-art Methods}
\label{sec:expt_sota_comparison}
There are no current solutions that can be directly applied to \newsetting task. To provide a comprehensive comparison, we extend methods from highly related fields for the state-of-the-art comparison. We first adapt ResTune~\cite{liu2022residual} and FRoST~\cite{Roy2022ClassincrementalNC}, to the \newsetting setting from the most related \incd field. In addition, we extend three representative \cil methods, EwC~\cite{kirkpatrick2017overcoming}, LwF~\cite{li2017learning}, and DER~\cite{buzzega2020dark}, to this self-supervised setting, in which two are regularization-based methods and one is rehearsal-based method. To ensure a fair comparison, all adapted methods use the model pre-trained by DINO-\vitbsixteen. For the extended \cil methods, we employ the same self-training strategy as our \ours method with their original components to prevent forgetting. All adapted methods unfreeze only the last transformer block of the feature extractor, except ResTune that unfreezes the last two blocks. This is because it is unnecessary to unlock all blocks of the large-scale pre-trained model, as previously observed in ~\cite{Wu2019LargeSI} and ~\cite{Boschini2022TransferWF}. Additional implementation details of the adapted methods are provided in the supplementary material.

Tab.~\ref{tab:expt_sota} presents a comparison between our proposed \ours and \ourspp and adapted methods. As can be observed, ResTune fails to perform well under \newsetting due to its need for task-id information, although it is designed for \incd. FRoST exhibits strong ability to prevent forgetting on all data sets and sequences due to its design that separates \textit{not-forgetting} regularization objective into feature extractor and classifier levels. All the adapted \il methods can incrementally discover novel classes using the prior knowledge from the large-scale pre-trained model. The adapted \il methods achieve better overall accuracy than those adapted from \cincd on most of the two-step split sequences, striking a better balance between the past and new classes. However, on five-step split sequences, the advantage of \il-based methods over \cincd-based methods is not evident anymore, because \il-based methods tend to forget tasks at the initial steps more when dealing with long sequences, as widely studied in \il literature. EwC achieves better discovery accuracy by applying its forgetting prevention component directly to the model parameters using Fisher information matrix. LwF~\cite{li2017learning} exhibits slow-fast learning interference when regularizing both the $g$ and $h\tstoend$ using one objective function. The poor performance of DER is attributed to the instability of the saved self-supervised optimization trajectories during discovery, which results in a wrong consistency with its past training trajectories. Our proposed \ours and \ourspp consistently outperform the adapted methods in all tested sequences. While FRoST gives lower forgetting in some two-step split cases, our \ourspp, by improving the capacity for class-discrimination across all tasks, achieves lower forgetting in most five-step split cases.

\noindent
\textbf{Generalizability Analysis.}
Our proposed methods offer an easy and versatile framework that can serve as a hub linking several related works. In a nutshell, our approach enables the incorporation of related methods that were originally not designed for \newsetting (\eg, \il, \ncd, or \owssltitle methods), converting them into effective multi-step class-incremental novel class discoverers. In Fig.~\ref{fig:expt_wn_analysis}, we equip two such methods, \ranks~\cite{han2020automatically} and \ocra~\cite{Cao2021OpenWorldSL}, with our proposed learning strategies (using large-scale pre-trained model, backbone frozen and \compcn). Results are evaluated on the five-step split CIFAR-100 data set, where we highlight the significance of the \compcn component in producing task-agnostic joint classifier. Our findings reveal that, by removing \compcn, the converted methods suffer from significant forgetting due to non-uniformly scaled weight vectors, resulting in a decrease in overall discovery accuracy. This echoes the importance of \compcn in aligning the magnitude of the classifiers learned at each step to the same scale in \newsetting scenarios. Instead, with using \compcn, unsupervised large-scale pre-trained models can be effectively leveraged to develop strong methods for the problem of \newsetting. 

\begin{figure}[!t]
\small
\begin{center}
   \includegraphics[width=0.99\linewidth]{fig/analysis_weight_norm.pdf}
\end{center}
\vspace{-3mm}
\caption{Generalizability analysis. We convert \ranks and \ocra to \newsetting models by injecting the proposed learning strategies (using large-scale pre-trained model, backbone frozen and CosNorm). Results are reported on the five-step split of C100 with DINO-ViT-B/16.}
\label{fig:expt_wn_analysis}
\vspace{-3mm}
\end{figure}
  

