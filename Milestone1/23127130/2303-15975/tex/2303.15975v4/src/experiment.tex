\section{Experiments}
\label{sec:experiments}
\subsection{Experimental Settings}
\label{sec:expt_setup}
\noindent
\textbf{Datasets and Splits.} We conduct experiments on three generic image recognition datasets and two fine-grained recognition datasets: CIFAR-10 (C10)~\cite{krizhevsky2009learning}, CIFAR-100 (C100)~\cite{krizhevsky2009learning}, TinyImageNet-200 (T200)~\cite{le2015tiny}, CUB-200 (B200)~\cite{wah2011caltech} and Herbarium-683 (H683)~\cite{Tan2019TheHC}. Although the PTM (DINO) used in our baselines and the methods we compared was pre-trained without labels,there's a potential for category overlap between the pre-training dataset (ImageNet~\cite{Deng2009ImageNetAL}) and C10, C100, and T200. To ensure a equitable evaluation, we include B200 and H683 datasets. Notably, B200 shares only two categories with DINO's pre-training dataset (ImageNet), whereas H683 has no overlap whatsoever. For each dataset, we adopt two strategies (two-step and five-step) to generate the task sequences, where the total classes and corresponding instances of training data are divided averagely for each step. The test data are used for evaluation. Detailed data splits are provided in the supplementary material.

\noindent
\textbf{Evaluation Protocol.} We evaluate all the methods in \cincd using the \textbf{task-agnostic} evaluation protocol~\cite{Roy2022ClassincrementalNC}. Specifically,  we do not know the task ID of the test sample during inference, and the network must route the sample to the correct segment of the unified classifier.

\noindent
\textbf{Evaluation Metrics.}
We report two metrics: maximum forgetting $\forgetting$ and overall discovery accuracy (or clustering accuracy~\cite{Roy2022ClassincrementalNC}) $\accuracy$ for all discovered classes by the end of the task sequence. $\forgetting$ measures the difference in clustering accuracy between the task-specific model $f\tsone$ and the unified model $f\tstoend$ (at the last step) for samples belonging to novel classes discovered at the first step. $\accuracy$ is the clustering accuracy from the unified model $f\tstoend$ on instances from all the novel classes discovered by the end of the sequence.

\subsection{Implementation Details}
\label{sec:expt_implementation}

\noindent
\textbf{\ours and \ourspp.}
By default, \vitbsixteen~\cite{Dosovitskiy2020AnII} is used as the backbone $g$ with DINO~\cite{Caron2021EmergingPI} initialization for all data sets. The 768-dimensional output vector $\vz \in \R^{768}$, from the $[CLS]$ token is used as the deep features extracted from a given image. $g$ is frozen during training. Following the backbone, one \textit{\cosnormed} linear layer (without bias) is randomly initialized as the task-specific classifier $h\tst$ with $\classes\tst$ output neurons. Soft pseudo-labels self-supervised are generated using the \sinkhorn~\cite{Cuturi2013SinkhornDL,Caron2020UnsupervisedLO} algorithm with default hyper-parameters (number of iterations = 3 and $\epsilon=0.05$).

\noindent
\textbf{Training.}
At each step, we train the model for 200 epochs on the given unlabelled data set $\data\tst$ with the same data augmentation strategy~\cite{Chen2020ASF} in all the experiments. After the discovery stage, \ourspp further conducts \compfr training on the unified model $f\tstot$ for 200 epochs. A cosine annealing learning rate scheduler with a base rate of 0.1 is used. The model is trained on mini-batches of size 256 using SGD optimizer with a momentum of 0.9 and weight decay $10^{-4}$. The temperature $\tau$ is set to 0.1.

\subsection{Analysis and Ablation Study}
\label{sec:expt_self_analysis}
%%%%%%%%%%%%%%%% here we compare our methods with the constructed lower and upper bounrds.
\noindent
\textbf{Comparison with Reference Methods.} We first establish reference methods using K-means~\cite{Arthur2007kmeansTA} and joint training scheme ($\upperb$, based on \ours but access to the previous training data is given)~\cite{li2017learning}, respectively. 
\begin{wraptable}{r}{0.52\textwidth}
    \tablestyle{1.0pt}{0.9}
    \small
    \vspace{-0.7cm}
    \caption{Comparison of our proposed baselines with reference methods on two task splits of C10, C100 and T200.}
    \label{tab:expt_upperlower}
    \lesspace
    \begin{center}
        \begin{tabular}{cl|cc|cc|cc}
            \toprule
            
            & Datasets & \multicolumn{2}{c|}{C10} & \multicolumn{2}{c|}{C100} & \multicolumn{2}{c}{T200} \\
            
            & Methods & $\forgetting\downarrow$ & $\accuracy\uparrow$ & $\forgetting\downarrow$ & $\accuracy\uparrow$ & $\forgetting\downarrow$ & $\accuracy\uparrow$ \\
            \hline
    
            \multirow{5}{*}{\rotatebox[origin=c]{90}{Two-step}}&Kmeans~\cite{Jain2008DataC5} & 93.9 & 87.3 & 68.2 & 56.7 & 62.0 & 47.1 \\
            &Joint (frozen) & 4.9 & 92.1 & 5.3 & 61.8 & 3.3 & 51.1 \\
            &Joint (unfrozen) & \textbf{0.8} & \textbf{92.4} & \textbf{2.5} & \textbf{65.2} & 2.3 & \textbf{56.5} \\
            
            \cline{2-8}
            
            &\ours & 8.5 & {89.2} & 6.7 & {60.3} & 4.0 & {54.6} \\
            &\ourspp & 4.5 & {90.9} & 6.6 & {61.4} & \textbf{0.2} & {55.1} \\
            
            \toprule
            \multirow{5}{*}{\rotatebox[origin=c]{90}{Five-step}}&Kmeans~\cite{Jain2008DataC5} & 99.1 & 82.1 & 76.3 & 54.3 & 66.0 & 52.9 \\
            &Joint (frozen) & 5.1 & 93.8 & 10.5 & 68.6 & 1.8 & 57.8 \\
            &Joint (unfrozen) & \textbf{1.5} & \textbf{97.5} & \textbf{5.9} & \textbf{74.9} & 3.0 & \textbf{60.7} \\ 
            \cline{2-8}
            &\ours & 8.2 & {85.4} & 15.6 & {63.7} & 9.2 & {53.3} \\
            &\ourspp & 7.6 & {91.7} & 12.3 & {67.7} & \textbf{1.6} & {56.5} \\
            
            \bottomrule
        \end{tabular}
    \end{center}
    \lesspace
% \end{table}
\end{wraptable}
To further enhance the upper reference performance, we unfreeze the last transformer block during training on joint data sets, which is referred as to $\upperbpp$ method. As shown in Tab.~\ref{tab:expt_upperlower}, the joint training methods slightly outperform our baselines on all data sets and splits, as they can jointly optimize the ideal objective defined in Eq.~\ref{eqn:ideal} using the given access to all training data. Nonetheless, our baselines perform nearly as well as the joint training methods, indicating limited benefits from access to all unlabelled data under \cincd and the effectiveness of our baselines.

\begin{table}[!t]
% \begin{wraptable}{r}{0.52\textwidth}
    % \tablestyle{1.0pt}{0.7}
    % \setlength{\tabcolsep}{3.2pt}
    % \vspace{-0.7cm}
    \caption{Self-ablation analysis of the proposed components on two task splits of C10, C100 and T200.}
    \label{tab:expt_selfablation}
    \vspace{-0.3cm}
    \begin{center}
        \begin{tabular}{cccccc|cc|cc}
            \toprule
            
            &&\multicolumn{2}{l}{Datasets} & \multicolumn{2}{c|}{C10} & \multicolumn{2}{c|}{C100} & \multicolumn{2}{c}{T200} \\
            
            &&\compcn & \compfr  & $\forgetting\downarrow$ & $\accuracy\uparrow$ & $\forgetting\downarrow$ & $\accuracy\uparrow$ & $\forgetting\downarrow$ & $\accuracy\uparrow$ \\
            \hline
            \multirow{4}{*}{\rotatebox[origin=c]{90}{Two-step}}&(a)&\Checkmark & \Checkmark & \textbf{4.5} & \textbf{90.9} & {6.6} & \textbf{61.4} & \textbf{0.2} & \textbf{55.1} \\
            &(b)&\Checkmark & \XSolidBrush & 8.5 & 89.2 & 6.7 & 60.3 & 4.0 & 54.6 \\
            &(c)&\XSolidBrush & \Checkmark & 8.2 & 80.2 & \textbf{5.1} & 54.1 & 3.3 & 38.9\\
            &(d)&\XSolidBrush & \XSolidBrush & 16.1 & 74.3 & 7.3 & 50.1 & 4.3 & 33.2 \\
            
            \hline
    
            \hline
            \multirow{4}{*}{\rotatebox[origin=c]{90}{Five-step}}&(a)&\Checkmark & \Checkmark & {7.6} & \textbf{91.7} & \textbf{12.3} & \textbf{67.7} & {1.6} & \textbf{56.5} \\
            &(b)&\Checkmark & \XSolidBrush & 8.2 & 85.4 & 15.6 & 63.7 & 9.2 & 53.3 \\
            &(c)&\XSolidBrush & \Checkmark &\textbf{6.3} & 90.7 & 14.3 & 58.2 & \textbf{0.7} &49.7 \\
            &(d)&\XSolidBrush & \XSolidBrush & 10.9 & 80.3 & 16.6 & 49.1 & 8.1 & 41.9 \\
            \bottomrule
        \end{tabular}
    \end{center}
    \vspace{-0.9cm}
\end{table}

\noindent
\textbf{Ablation on Proposed Components.} We further present an ablation study on the individual core components of our baseliens, namely \compcn and \compfr. Results are shown in Tab.~\ref{tab:expt_selfablation}. It is noticeable from the results that \compcn plays a substantial role in enhancing the overall accuracy of our proposed baselines (refer to \ours: \textit{(b) v.s. (d)} and \ourspp: \textit{(a) v.s. (c)}). This is attributed to its unification capability to effectively address the issue of that the weight vectors with significant magnitudes in $f\tstoend=h\tstoend \circ g$ always dominating the prediction. On the other hand, \compfr can improve the overall accuracy and mitigate the forgetting at the end of each task sequence (refer to \textit{(a) v.s. (b)} and \textit{(c) v.s. (d)}). Of particular note is that the performance gain attained by using \compfr is more significant when dealing with longer task sequences (refer to the \textit{upper half v.s. lower half} in Tab.~\ref{tab:expt_selfablation}).
\ourspp (a) equipped with both \compcn and \compfr achieves the best overall accuracy and the least forgetting.


%%%%%%%%%%%%%%%% here we analyze the effectiveness of different large-scale pre-trained models.
% \begin{table}[!t]
\begin{wraptable}{r}{0.48\textwidth}
    \tablestyle{1.pt}{0.9}
    % \setlength{\tabcolsep}{3.2pt}
    \vspace{-0.45cm}
    \small
    \lesspace
    \caption{Ablation of architectures and pre-training strategies of PTMs on five-step splits of C10, C100 and T200.}
    \label{tab:expt_backboneablation}
    \vspace{-0.5cm}
    \begin{center}
        \begin{tabular}{lcccccc}
            \toprule
            % Five-step Split ($\searrow$) &\multicolumn{6}{c}{\ours}\\
            &\multicolumn{6}{c}{\textbf{\ours}}\\
            Datasets & \multicolumn{2}{c}{C10} & \multicolumn{2}{c}{C100} & \multicolumn{2}{c}{T200}\\
    
            % & \multicolumn{2}{c}{$\task\tstofive$} & \multicolumn{2}{c}{$\task\tstofive$} & \multicolumn{2}{c}{$\task\tstofive$} \\
            
            Backbones& $\forgetting \downarrow$ & $\accuracy \uparrow$ & $\forgetting \downarrow$ & $\accuracy \uparrow$ & $\forgetting \downarrow$ & $\accuracy \uparrow$ \\
            \hline
            ResNet50-DINO & 37.5 & 45.8 & 16.4 & 38.5 & 10.1 & 24.7 \\
            ViT-B/16-DINO & 8.2 & 85.4 & \textbf{15.6} & \textbf{63.7} & \textbf{9.2} & \textbf{53.3}\\
            ViT-B/16-CLIP & \textbf{5.3}& \textbf{87.5} & 17.1 & 62.4 & 15.7 & 42.5 \\  
            
            \bottomrule
        \end{tabular} 
    \end{center}
    \vspace{-0.5cm}
\end{wraptable}
% \end{table}

\noindent
\textbf{Analysis of Pre-Trained Models (PTM)}. In Tab.\ref{tab:expt_backboneablation}, we present a comparison between different PTMs such as ResNet50~\cite{He2015DeepRL} and \vitbsixteen~\cite{Dosovitskiy2020AnII}, along with various pre-training strategies (CLIP~\cite{Radford2021LearningTV} and DINO~\cite{Caron2021EmergingPI}). Transformer architecture achieves superior performance owing to its discrimination ability~\cite{Naseer2021IntriguingPO}. CLIP pre-training achieves similar outcomes to DINO, demonstrating the effectiveness of strong PTM with a different pre-training strategy on web data.



\subsection{Comparison with the State-of-the-art Methods}
\label{sec:expt_sota_comparison}
For a comprehensive comparison, we adapt methods from closely related fields for state-of-the-art comparison. We adjust ResTune~\cite{liu2022residual} and FRoST~\cite{Roy2022ClassincrementalNC} to the multi-step \cincd setting from the closely related \incd field. We adapt three representative CIL methods: EwC~\cite{kirkpatrick2017overcoming}, LwF~\cite{li2017learning}, and DER~\cite{buzzega2020dark} to this self-supervised setting. Similarly, we adapt the \uil method, CaSSLe~\cite{Fini2021SelfSupervisedMA}, for incremental discovery. All adapted methods employ \vitbsixteen with the same DINO-initialization as a feature extractor. For the adapted CIL and \uil methods, the same self-training strategy is used as in our \ours method to prevent forgetting. All adapted methods unfreeze only the last transformer block of the feature extractor~\cite{Wu2019LargeSI,Boschini2022TransferWF}, except ResTune that unfreezes the last two blocks for model growing. More implementation details can be found in the supplementary material.


\begin{table}[!t]
    % \tablestyle{1.0pt}{0.9}
    \small
    % \vspace{-0.7cm}
    \caption{Comparison with the adapted state-of-the-art methods on two task splits of C10, C100, T200, B200, and H683 under \cincd setting with the same DINO-\vitbsixteen backbone. Overall accuracy and maximum forgetting are reported.}
    \label{tab:expt_sota}
    \vspace{-0.3cm}
    \begin{center}
        \begin{tabular}{cl|cc|cc|cc|cc|cc}
            \toprule
            
            & Datasets 
            & \multicolumn{2}{c|}{C10} 
            & \multicolumn{2}{c|}{C100} 
            & \multicolumn{2}{c|}{T200}
            & \multicolumn{2}{c|}{B200}
            & \multicolumn{2}{c}{H683} \\

            & Methods 
            & $\forgetting\downarrow$ 
            & $\accuracy\uparrow$ 
            & $\forgetting\downarrow$ 
            & $\accuracy\uparrow$ 
            & $\forgetting\downarrow$ 
            & $\accuracy\uparrow$ 
            & $\forgetting\downarrow$ 
            & $\accuracy\uparrow$ 
            & $\forgetting\downarrow$ 
            & $\accuracy\uparrow$ \\
            \hline
    
            \multirow{8}{*}{\rotatebox[origin=c]{90}{\textbf{Two-step}}}
            &EwC~\cite{kirkpatrick2017overcoming} & 32.4 & 79.0 & 42.5 & 43.9 & 27.2 & 33.3 & 18.1 & 25.5 & 13.8 & 25.1 \\
            &LwF~\cite{li2017learning} & 30.4 & 34.4 & 44.1 & 42.4 & 40.0 & 27.2 & 20.2 & 23.9 & 16.3 & 24.9\\
            &DER~\cite{buzzega2020dark} & 49.0 & 69.9 & 29.8 & 30.3 & 39.0 & 28.9 & 5.0 & 20.4 & 14.0 & 24.7\\
            &ResTune~\cite{liu2022residual} & 97.6 & 47.2 & 32.7 & 17.1 & 32.3 & 17.2 & 12.0 & 13.0 & 27.4 & 17.1\\
            &FRoST~\cite{Roy2022ClassincrementalNC} & \textbf{2.5} & 46.6 & \textbf{4.7} & 34.2 & 4.3 & 26.1 & \textbf{3.9} & 17.6 & 16.2 & 18.4 \\
            &CaSSLe~\cite{Fini2021SelfSupervisedMA} & 9.1 & 87.3 & 10.3 & 53.7 & 6.9 & 36.5 & 4.8 & 26.8 & 10.9 & 25.3 \\
            
            \cline{2-12}
            
            &\ours & 8.5 & \textbf{89.2} & 6.7 & \textbf{60.3} & 4.0 & \textbf{54.6} & 4.1 & \textbf{28.7} & 7.9 & \textbf{25.7} \\
            &\ourspp & 4.5 & \textbf{90.9} & 6.6 & \textbf{61.4} & \textbf{0.2} & \textbf{55.1} & 4.2 & \textbf{36.9} & \textbf{6.0} & \textbf{27.5}\\
            
            \toprule
            \multirow{8}{*}{\rotatebox[origin=c]{90}{\textbf{Five-step}}}
            &EwC~\cite{kirkpatrick2017overcoming} & 21.1 & 81.1 & 60.1 & 30.6 & 48.0 & 23.2 & 21.2 & 19.1 & 15.7 & 22.4\\
            &LwF~\cite{li2017learning} & 20.1 & 25.8 & 60.9 & 16.1 & 53.7 & 15.6 & 21.7 & 15.7 & 16.5 & 23.4\\
            &DER~\cite{buzzega2020dark} & 30.1 & 76.2 & 62.6 & 36.2 & 52.1 & 21.7 & 16.2 & 16.3 & 18.0 & 22.3\\
            &ResTune~\cite{liu2022residual} & 95.5 & 49.2 & 83.3 & 19.4 & 60.4 & 12.2 & 24.2 & 12.4 & 28.2 & 11.2\\
            &FRoST~\cite{Roy2022ClassincrementalNC} & \textbf{0.9} & 69.2 & 14.2 & 43.6 & 14.4 & 31.0 & 19.4 & 18.5 & 13.5 & 23.4 \\
            &CaSSLe~\cite{Fini2021SelfSupervisedMA} & 11.3 & 78.5 & 25.3 & 61.7 & 14.1 & 42.3 & 14.6 & 22.3 & 13.8 & 24.1\\
            
            \cline{2-12}
            
            &\ours & 8.2 & \textbf{85.4} & 15.6 & \textbf{63.7} & 9.2 & \textbf{53.3} & 13.7 & \textbf{28.9} & 3.1 & \textbf{25.2} \\
            &\ourspp & 7.6 & \textbf{91.7} & \textbf{12.3} & \textbf{67.7} & \textbf{1.6} & \textbf{56.5} & \textbf{0.6} & \textbf{41.1} & \textbf{2.7} & \textbf{26.1} \\
            
            \bottomrule
        \end{tabular}
    \end{center}
    \vspace{-1.0cm}
\end{table}

Tab.~\ref{tab:expt_sota} compares our proposed \ours and \ourspp with the adapted methods. ResTune underperforms in the \cincd setting due to its reliance on task-id information. FRoST exhibits strong ability to prevent forgetting on all data sets and sequences by segregating the \textit{not-forgetting} objective between the feature extractor and classifier. The adapted CIL methods capably discover new classes leveraging PTM knowledge. For two-step split sequences, these methods generally outperform \cincd adaptations by maintaining a balance between old and new classes. However, on five-step split sequences, the advantage of CIL-based methods over \cincd-based methods is not evident anymore, because CIL-based methods tend to forget tasks at the initial steps more when dealing with long sequences, as widely studied in CIL literature. EwC achieves better discovery accuracy by applying its forgetting prevention component directly to the model parameters using Fisher information matrix, while LwF~\cite{li2017learning} faces slow-fast learning interference issues. DER's performance suffers due to unstable self-supervised trajectories. CaSSLe is notably proficient in incremental discovery, attributed to its effective distillation mechanisms. Without \textit{bells} and \textit{whistles}, our \ours and \ourspp models consistently outperform adapted methods across datasets and sequences. While FRoST gives lower forgetting in some two-step split cases, our \ourspp, by improving the capacity for class-discrimination across all tasks, achieves lower forgetting in most five-step split cases.

\noindent\textbf{Generalizability Analysis.} Our proposed approach offers a versatile framework to convert related methods into effective \cincd solutions.
\begin{wrapfigure}{R}{0.49\textwidth}
    \vspace{-0.7cm}
    \centering
    \includegraphics[width=\linewidth]{fig/analysis_weight_norm.pdf}
    \vspace{-0.7cm}
    \caption{Generalizability analysis. Results are reported on the five-step split of C100 with DINO-ViT-B/16.}
    \label{fig:expt_wn_analysis}
    \vspace{-0.7cm}
\end{wrapfigure}
In Fig.~\ref{fig:expt_wn_analysis}, we equip two such methods, \ranks~\cite{han2020automatically} and \ocra~\cite{Cao2021OpenWorldSL}, with our proposed 
components (frozen PTM and \compcn). The results emphasize the pivotal role of \compcn in forming a task-agnostic classifier. Our findings reveal that, by removing \compcn, the converted methods suffer from significant forgetting due to non-uniformly scaled weight vectors, resulting in a decrease in overall discovery accuracy. This echoes the importance of \compcn in aligning the magnitude of the classifiers learned at each step to the same scale in \cincd scenarios.  Instead, with using \compcn, PTMs can be effectively leveraged to develop strong methods for the problem of \cincd.