% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
\usepackage{hyperref}
\usepackage{color}
\renewcommand\UrlFont{\color{blue}\rmfamily}
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\color{blue}\rmfamily}  % Miu: DO NOT CHANGE THIS
%

%%%%%%%%% MY PACKAGES
\usepackage[switch]{lineno}
\usepackage{newfloat}
\usepackage{listings}
\usepackage[accsupp]{axessibility}  % % The "axessiblity" package can be found at: https://ctan.org/pkg/axessibility?lang=en. Improves PDF readability for those with disabilities.
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
% Include other packages here, before hyperref.
\usepackage{wrapfig} % 
\usepackage{glossaries}
\usepackage[table]{xcolor}
\usepackage{colortbl}
\usepackage{bbding}
\usepackage{tikz}
\usepackage{comment}
\usepackage{color}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{nicefrac}
% \usepackage{bbm} % MIU: FORBIDEN BY AAAI
\usepackage{bm}
\usepackage{tabularx,verbatim}
\usepackage{multirow}
% \usepackage[small]{caption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
% \usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS

\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
% \usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
% \usepackage[authoryear]{natbib}
% \usepackage[authoryear,numbers,sort&compress]{natbib}



%------- yellow warnings
\usepackage[most]{tcolorbox}

% \captionsetup[table]{skip=0pt}
% \captionsetup[figure]{skip=0pt}

%%%%%%%%% MY COMMANDS
\input{utils/math_commands.tex}
% \input{utils/my_acronym.tex}
\input{utils/my_commands.tex}


\definecolor{darkgreen}{RGB}{119,185,0}
\definecolor{cyan}{rgb}{0.831,0.901,0.945}
\definecolor{remark}{rgb}{1,.5,0} 
\definecolor{citecolor}{rgb}{0,0.443,0.737} 
\definecolor{linkcolor}{rgb}{0.956,0.298,0.235} 

\begin{document}

\title{Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery}

%%%%%%%%% Authors
\titlerunning{Strong Baselines for Incremental Novel Class Discovery}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{
Mingxuan Liu\inst{1} \quad
Subhankar Roy\inst{3} \quad
Zhun Zhong\inst{4}\thanks{Corresponding author: Zhun Zhong.}\quad
Nicu Sebe\inst{1} \quad
Elisa Ricci\inst{1,2}
}
%
\authorrunning{
% Mingxuan Liu, Subhankar Roy, Zhun Zhong, Nicu Sebe, Elisa Ricci
Liu. et al.
}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used. $^{3}$ Hefei University of Technology, Hefei, China \quad

%
\institute{
University of Trento, Trento, Italy\\
\email{mingxuan.liu@unitn.it}\\ \and
Fondazione Bruno Kessler, Trento, Italy \and
University of Aberdeen, Aberdeen, UK \and
University of Nottingham, Nottingham, UK
% Princeton University, Princeton NJ 08544, USA \and
% Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
% \email{lncs@springer.com}\\
% \url{http://www.springer.com/gp/computer-science/lncs} \and
% ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
% \email{\{abc,lncs\}@uni-heidelberg.de}
}
%
% \maketitle              % typeset the header of the contribution

% \titlerunning{{Large-scale Pre-trained Models are Surprisingly Strong in class-iNCD}}% Part of RIGHT running header

%%%%%%%%% TEASER
% \twocolumn[{%/
% \vspace{-3em}
\maketitle
\begin{center}
  \centering
   \includegraphics[width=0.99\textwidth]{fig/teaser.pdf}
   \lesspace
   \captionof{figure}{Overview of different learning paradigms for discovering novel (or \textit{new}) categories from \textit{unlabelled} data. (a) \textbf{\ncd} learns and discovers novel classes in an unalabelled dataset by exploiting the priors learned from related labelled data. (b) \textbf{\cincd} is similar to \ncd, except it discovers novel classes arriving in sessions without any access to labelled data during the discovery phase. (c) Our proposed simple Baseline for \cincd that leverages a self-supervised pre-trained model (PTM) instead of expensive labelled data. Inference on test data is carried out in a task-\textit{agnostic} manner.}
   \lesspace
   \label{fig:setting_comparison}
\end{center}
% }]

% \linenumbers

%%%%%%%%% ABSTRACT
\begin{abstract}
   Discovering novel concepts in unlabelled datasets and in a continuous manner is an important desideratum of lifelong learners. In the literature such problems have been partially addressed under very restricted settings, where novel classes are learned by jointly accessing a related labelled set (e.g., \ncd) or by leveraging only a supervisedly pre-trained model (e.g., \cincd). In this work we challenge the status quo in \cincd and propose a learning paradigm where class discovery occurs continuously and truly unsupervisedly, without needing any related labelled set. In detail, we propose to exploit the richer priors from strong self-supervised pre-trained models (PTM). To this end, we propose simple baselines, composed of a frozen PTM backbone and a learnable linear classifier, that are not only simple to implement but also resilient under longer learning scenarios. We conduct extensive empirical evaluation on a multitude of benchmarks and show the effectiveness of our proposed baselines when compared with sophisticated state-of-the-art methods. The code is \href{https://github.com/OatmealLiu/MSc-iNCD}{open source}.
\keywords{Novel Class Discovery \and Class-Incremental Learning}
\end{abstract}

%%%%%%%%% MAIN PAPER
\input{src/introduction.tex}

\input{src/related.tex}

\input{src/method.tex}

\input{src/experiment.tex}

\input{src/conclusion.tex}


\subsubsection{Acknowledgements}
E.R. is supported by MUR PNRR project FAIR - Future AI Research (PE00000013), funded by NextGenerationEU and EU projects SPRING (No. 871245) and ELIAS (No. 01120237). M.L. is supported by the PRIN project LEGO-AI (Prot. 2020TA3K9N). This work was carried out in the Vision and Learning joint laboratory of FBK and UNITN.

%%%%%%%%% APENDIX



%%%%%%%%% REFERENCE
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%

%%%%%%%%%%% STD REF
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}

%%%%%%%%%%% ICPR 2024 REF
% \begin{thebibliography}{8}
\input{src/references}
% \end{thebibliography}

\input{main_supp}

\end{document}
