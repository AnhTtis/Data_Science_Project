{
    "arxiv_id": "2303.15975",
    "paper_title": "Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery",
    "authors": [
        "Mingxuan Liu",
        "Subhankar Roy",
        "Zhun Zhong",
        "Nicu Sebe",
        "Elisa Ricci"
    ],
    "submission_date": "2023-03-28",
    "revised_dates": [
        "2024-08-22"
    ],
    "latest_version": 4,
    "categories": [
        "cs.CV",
        "cs.LG"
    ],
    "abstract": "Discovering novel concepts in unlabelled datasets and in a continuous manner is an important desideratum of lifelong learners. In the literature such problems have been partially addressed under very restricted settings, where novel classes are learned by jointly accessing a related labelled set (e.g., NCD) or by leveraging only a supervisedly pre-trained model (e.g., class-iNCD). In this work we challenge the status quo in class-iNCD and propose a learning paradigm where class discovery occurs continuously and truly unsupervisedly, without needing any related labelled set. In detail, we propose to exploit the richer priors from strong self-supervised pre-trained models (PTM). To this end, we propose simple baselines, composed of a frozen PTM backbone and a learnable linear classifier, that are not only simple to implement but also resilient under longer learning scenarios. We conduct extensive empirical evaluation on a multitude of benchmarks and show the effectiveness of our proposed baselines when compared with sophisticated state-of-the-art methods. The code is open source.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.15975v1",
        "http://arxiv.org/pdf/2303.15975v2",
        "http://arxiv.org/pdf/2303.15975v3",
        "http://arxiv.org/pdf/2303.15975v4"
    ],
    "publication_venue": "Accepted as a conference paper to ICPR 2024"
}