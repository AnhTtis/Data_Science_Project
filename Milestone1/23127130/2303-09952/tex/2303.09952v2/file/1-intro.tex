\section{Introduction}
\IEEEPARstart{T}{he} method Neural Radiance Fields (NeRF)~\cite{mildenhall2020nerf} is proposed for photorealistic novel view synthesis. Given many views of the scene, it creates implicit multi-view geometry and learns for view synthesis. However, it has poor generalizations to new scenes and requires retraining or fine-tuning on each scene. 
 
 Recent work~\cite{Yu_2021_CVPR,Trevithick_2021_ICCV} has explored the ways of using a single image to train NeRF. They introduce a convolutional feature encoder to learn the image representation which gives it some limited generalization abilities to unseen scenes.  But, without fine-tuning, these methods produce many floats and artifacts in rendering novel views. 
 
  Multi-Plane Images (MPI) representation that learns multiple RGB images from a single image is also used in \cite{Wu_2021_ICCV,Tucker_2020_CVPR,wu2022remote} for  novel view synthesis. However, MPI heavily relies on the qualities of the planar images and needs plenty of image planes to avoid blurs. There is no strong 3D geometry constraint and it fails in many complex scenes.
  
  MINE~\cite{Li_2021_ICCV2} introduces the volume rendering of NeRF into the MPI. It runs faster and produces better depth rendering quality compared with single-view NeRFs~\cite{Yu_2021_CVPR,Trevithick_2021_ICCV}. However, the rendering quality heavily relies on the number of image planes. It needs high-resolution 4D volumes to store the 4-channel  (RGB and volume density) image planes that cost a large amount of GPU memory in both training and prediction.

Besides, some methods focus on shape or object information for novel view rendering.  AutoRF~\cite{N2022AutoRF} learns 3D object radiance fields from single-view observations.  CodeNeRF~\cite{Jang_2021_ICCV} learns separated embedding to disentangle shape and texture. Sharf~\cite{rematasICML21} uses shape information as guidance for learning NeRF from a single image.
 

There are also some other strategies introduced for single-view NeRF rendering. For example, 
PVSeRF~\cite{DBLP:journals/corr/abs-2202-04879} proposes a joint pixel-, voxel- and surface-aligned NeRF. 
Pix2NeRF~\cite{2022Pix2NeRF} introduces pi-GAN~\cite{pigan} to NeRF for learning novel view synthesis using a single image as input. 

 

 
 % \input{image/intro_compare}
 
 In this paper, we propose a joint rendering mechanism that takes the MPI strategy for coarse sampling proposals and the MLP\&volume-based rendering~\cite{mildenhall2020nerf} for fine sampling and rendering. Then, both the coarse point samples and the fine samples are combined according to their geometry distribution to realize a more accurate joint rendering. More importantly, we introduce a depth teacher net that serves as the guidance for the joint rendering. The monocular depth teacher predicts dense pseudo depth maps that assist the consistent 3D geometry learning between the MPI, the fine volume, and the joint rendering. It also boosts the multi-view geometry consistency between the source view and the target novel views that 
helps handle the occlusions, reduce the blurs and floats, and render accurate depths. 
 
In the experiments,  we verify the effectiveness of our method on three challenging real-scene datasets (RealEstate10K~\cite{zhou2018stereo}, NYU~\cite{silberman2012indoor} and  NeRF-LLFF~\cite{mildenhall2020nerf}) for novel view synthesis or depth estimation. Given a single image as input, our method is shown able to produce higher qualities in both the RGB image rendering and depth map prediction. It far outperforms state-of-the-art methods~\cite{Li_2021_ICCV2,Yu_2021_CVPR} with improvements of 5$\sim$20\% in PSNR and SSIM for the RGB rendering and reduces 20$\sim$50\% of the errors for the depth prediction.