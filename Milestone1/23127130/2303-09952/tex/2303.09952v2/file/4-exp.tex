\section{Experimental Results}

\subsection{Datasets}
We use NeRF-LLFF~\cite{mildenhall2020nerf} and Realestate10k~\cite{zhou2018stereo}  datasets for training and validation. Besides, we also use NYUv2 dataset~\cite{silberman2012indoor}  as the test set to evaluate the depth rendering. 
\paragraph{NeRF-LLFF}
NeRF-LLFF~\cite{mildenhall2020nerf} consists of 8 scenes. Following \cite{Li_2021_ICCV2}, we select images in each scene as the test set which consists of 35 images, and the rest 270 images are used as the training set.
In experiments on NeRF-LLFF, %$D$ is fixed to 32, $N_f$ is 16, 
we set $\lambda'_{d}=10$, $\lambda_{grad}=5$.
The resolution is set to 512$\times$384. We fine-tune our depth teacher for 1,000 iterations, train coarse planar rendering for 20,000 iterations and another 10,000 iterations for fine rendering. We use a batch size of 4. 
The learning rate decays every 8000 steps.


\paragraph{RealEstate10K}
RealEstate10K~\cite{zhou2018stereo} is a large dataset that consists of more than 70,000 video sequences. Limited by the available computing resource, we randomly choose 1,000 sequences from the pre-split training set to train our model and test it on 600 randomly selected sequences from the test set. %Each video sequence is  
For training and testing, we sample the source and target view pairs at a 10-frame interval from the video sequences, which gives us 32,000 training pairs (of source and target views) and 2,400 test pairs. %For training in RealEstate10k, we set 
In experiments on Realestate10k dataset, %$D$ and $N_f$ is 32 and 16,  
we set $\lambda'_{d}=1$, $\lambda_{grad}=20$.
The input resolution is set to 384$\times$256. We fine-tune our $\mathcal{G}$ 2,000 iterations, train the coarse planar rendering for 10,000 steps with a batch size of 24 and another 20,000 iterations for the fine decoder with a batch size of 8. 
The learning rate decays at 1000 steps and 8000 steps.




\input{image/llff_dpt_compare}

\subsection{Ablation Study}

As shown in Table \ref{tab:baseline_compare_detail}, we verify the effectiveness of different rendering components and study the effects of different settings on the LLFF dataset. We find that both our planar rendering and the volume rendering perform better than the baseline PixelNeRF. The joint rendering achieves a better PSNR and SSIM compared with each individual rendering way.
% add supp volume rendering
It should be noted that additional importance sampling can boost the planar rendering method for preserving more fine image/depth details. Since the number of planes sampled is limited, blur and artifacts are unavoidable. 
As shown in Figure \ref{fig: llff_is}, compared with the pure planar rendering~\cite{Li_2021_ICCV2},
the joint rendering predicts more details and fewer blurs in both the RGB and depth rendering. 
% Figure \ref{fig: llff_is} shows how extra fine points solve this problem and lead to a better rendering.
And the full setting with the depth teacher supervision achieves the best PSNR, SSIM and LPIPS.
Figure \ref{fig:llff_depth} also compares our method with the pure planar NeRF (MINE) and the pure volume rendering PixelNeRF. The joint rendering without our depth teacher achieves better depths in object edges and occlusions. By introducing the depth teacher supervision $\mathcal{L}'_d$, 
 the rendering quality of the  depth maps is significantly improved. 
\input{image/supplementary/llff_is}



Although teacher net (monocular depth estimation) is able to predict high-quality depth maps on the input image, it can't render depth maps for novel view. The  student net renders both the RGB and the depth maps for the novel views which are very important for applications like video editing, augmented reality etc. 
\paragraph{Coarse/Fine Point Sampling}
\input{table/supplementary/llff_NcNf_compare}
As shown in Table \ref{tab:llff_ncnf_compare}, we report the performance when using different coarse and fine samples. We find that using more  point samples (either coarse planar sampling $N_c$ or the fine volume samples $N_f$) will improve the rendering quality. When using 32 coarse planar samples and 32 fine volume samples, our method performs best. But, using many more planar samples (e.g. $N_c=64$) will not produce better results. %This might because  too many image planes are hard to learn. 

% It should be noted that additional importance sampling can boost the planar rendering method for preserving more fine image/depth details. Since the number of planes sampled is limited, blur and artifacts are unavoidable. Figure \ref{fig: llff_is} shows how extra fine points solve this problem and lead to a better rendering.


\input{table/supplementary/llff_backbone_compare}

\paragraph{Different Backbones}
We test two different feature encoder backbones, the default transformer backbone ViT and the ResNet50 (used in MINE). When using the same ResNet50 backbone, our method achieves 19.3 in PSNR which is 6\% improvement compared with MINE under the same setting. 
ViT slightly improves the PSNR on LLFF dataset when compared with ResNet50, and there is no improvement on RealEstate10K dataset.  The improvements are mainly from our joint rendering and depth teacher guidance strategies.






\input{image/llff_pixelnerf}

\input{image/supplementary/llff_compare}
\subsection{View Synthesis on NeRF-LLFF}


In table \ref{tab:llff_compare}, we compare our method with  PixelNeRF~\cite{Yu_2021_CVPR} and MINE ~\cite{Li_2021_ICCV2}  on the LLFF dataset.  We also compare to the pre-trained MPI~\cite{Tucker_2020_CVPR} on LLFF using the provided checkpoint. % 
Our method achieves the best performance in all the evaluation metrics of PSNR, SSIM and LPIPS. It outperforms the MINE  by 20\% in SSIM, 7\% in PSNR and 18\% in LPIPS.
\input{table/llff_compare}
As shown in Figure \ref{fig:llff_rgb_supp}, our method is able to render novel views with higher quality (more fine details and fewer blurs).   Moreover, the depth rendering results of our DT-NeRF is also far better than MINE (Figure \ref{fig:llff_depth}).


Our method also far outperforms the PixelNeRF~\cite{Yu_2021_CVPR}.
As shown in Figure \ref{fig:llff pixelnerf},  we compare our DT-NeRF with PixelNeRF. We only use our volume rendering which just takes 16 samples in rendering the image view. As a comparison, PixelNeRF needs 96 samples.  With just $1/6$ point samples,  our volume rendering can synthesize more realistic images than PixelNeRF. This is because our volume rendering is supervised by the depth teacher and learns better 3D geometry. Moreover, the point sampling is guided by the coarse planar sampling results which assist the more
precise sampling around the object surfaces and avoids sampling a large number of useless points. 
\input{table/llff_compare2}


 
\input{image/supplementary/realestate_compare}
\input{table/nyu_depth}
\subsection{View Synthesis on RealEstate10K}\label{sec:realestate}
We also evaluate our method on the RealEstate10K dataset. It is compared with the state-of-the-art MINE. Our method achieves the best PSNR, SSIM and LPIPS. It outperforms the MPI~\cite{Tucker_2020_CVPR} by 4$\sim$10\% in these three evaluation metrics, and also surpass the MINE~\cite{Li_2021_ICCV2} with a better RGB rendering quality. More importantly, compared with MPI and MINE, our DT-NeRF produces far better depth predictions with sharper depth edges and more accurate estimations in occluded regions in Figure \ref{fig_realestate_vis}.


\input{table/realestate_compare}


\subsection{Depth Estimation on NYU-V2}
\input{image/supplementary/nyu_depth}
We evaluate depth estimation on NYU-Depth V2 dataset~\cite{silberman2012indoor}. We perform our method on the labeled subset which consists of 1449 densely labeled pairs of RGB and depth images taken from a variety of indoor scenes. To solve the scale ambiguity problem of predicted depth, following \cite{niklaus20193d,Li_2021_ICCV2}, we scale and bias the predicted depth to minimize the $L2$ depth error with respect to ground truth. 

As shown in Table \ref{table_depth_estimation}, our model performs far better than MINE and MPI in all the evaluation metrics. Even with our model trained on LLFF which consists of only 8 scenes, it still has better depth results than MINE trained on the larger RealEstate10k dataset (1000 scenes and 32,000 training pairs). 
This is because our DT-NeRF training is guided by the dense pseudo depth maps which help it learn consistent 3D geometry across the source views and the target views in both the planar rendering and the volume rendering. Some results are shown in Figure \ref{fig_depth_vis_nyu}.


 \input{table/sinnerf_compare}
 \input{image/supplementary/nyu_stu_tea}
\input{table/stu_tea_depth}
\subsection{Analysis on Student and Teacher Net}
Table \ref{tab:stu_tea} shows $\mathcal{L}'_d$ will improve student net's prediction quality of depth map. And the student net even has stronger generalization performance than fine-tuned teacher net on the depth prediction. 
In Figure \ref{fig stu tea}, we show comparisons of the predicted depth maps between the depth teacher and the student net. We use the models trained on RealEstate10K(small) to inference depth maps on the NYUv2 dataset. Student net performs a little better in four  of the six evaluation metrics.

Although teacher net (monocular depth estimation) is able to predict high-quality depth maps on the input image, it can't render depth maps for novel view. The  student net renders both the RGB and the depth maps for the novel views which are very important for applications like video editing, augmented reality etc.
 
\subsection{Generalization}
Our DT-NeRF is shown able to generalize to new scenes without fine-tuning or retraining on each of them. This is because we design a depth teacher net to supervise the
joint student rendering mechanism and boost the learning of consistent 3D geometry. In this section, we  pretrain our DT-NeRF on the RealEstate10K datasets and then compare it with the fine-tuned (on the target NeRF-LLFF scenes) SinNeRF and PixelNeRF on the unseen NeRF-LLFF scenes. 
\paragraph{Comparison with Fine-tuned SinNeRF}
Our DT-NeRF shares a similarity with SinNeRF~\cite{sinnerf}. Both models use a pre-trained teacher net to supervise the training process, while DT-NeRF uses a dense monocular depth teacher, SinNeRF uses a semantic teacher and a geometry teacher. 
SinNeRF has no generalization abilities to unseen data/images. It needs to be retrained or fine-tuned on each scene. We train SinNeRF for each scene (512$\times$384) and use the ground truth depth \cite{sinnerf} to supervise. %The training on each scenes takes 24 hours.   
Our method has excellent generalization abilities to new scenes. So there is no need to fine-tune it on the test scenes. 
We directly use our model trained on a small part of RealEstate10K dataset (1000 scenes) and perform novel view synthesis on ``room'' scene in LLFF dataset. After training on the ``room'' scene, SinNeRF achieves 18.10 in PSNR. Our unadapted method achieves a similar 18.09 in PSNR. 




\paragraph{Comparison with Fine-tuned PixelNeRF}
We also compare our DT-NeRF (trained on unrelated RealEstate10K dataset) with the PixelNeRF~\cite{Yu_2021_CVPR} that is trained/fine-tuned on the LLFF dataset (including the test room scenes). Even without seeing the LLFF scenes during the training,  our method can still produce a similar performance (18.09 in PNSR) that is close to the test-data-fine-tuned PixelNeRF (18.23 in PSNR). After fine-tuning, our DT-NeRF achieves a 14\% improvement in PSNR which is far better than the fine-tuned PixelNeRF and SinNeRF.
