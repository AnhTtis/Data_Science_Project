\section{Related Work}

Neural radiance fields (NeRF) ~\cite{mildenhall2020nerf} have been proposed for photorealistic novel view rendering. 
However, the original NeRF requires many
different views of one scene for training and cannot be used for single-view-based novel view synthesis. 
\paragraph{Single-view NeRF}
Recently, different single-view-based NeRF technologies have been proposed for novel view rendering. 
PixelNeRF~\cite{Yu_2021_CVPR} introduces NeRF for novel view rendering using one or a few images as input. It uses a CNN encoder to learn the image representation before the volume rendering. The CNN encoder gives some limited generalization abilities to unseen images. GRF~\cite{Trevithick_2021_ICCV} learns local features for each pixel and projects the features for novel view rendering.

MINE~\cite{Li_2021_ICCV2} introduces the neural radiance fields to the Multi-Plane Images (MPI) synthesis. It learns to predict a 4-channel image (RGB and volume density) at arbitrary depth values for contiguous novel-view depth and RGB synthesis.
Similarly, AdaMPI~\cite{han2022single} proposes to learn adaptive multi-plane images for single-view-based novel view synthesis in the wild.

Besides, some methods focus on shape or object information for novel view rendering.  AutoRF~\cite{N2022AutoRF} learns 3D object radiance fields from single-view observations.  CodeNeRF~\cite{Jang_2021_ICCV} learns separated embedding to disentangle shape and texture. Sharf~\cite{rematasICML21} uses shape information as guidance for learning NeRF from a single image.
 

There are also some other strategies introduced for single-view NeRF rendering. For example, 
PVSeRF~\cite{DBLP:journals/corr/abs-2202-04879} proposes a joint pixel-, voxel- and surface-aligned NeRF. 
Pix2NeRF~\cite{2022Pix2NeRF} introduces pi-GAN~\cite{pigan} to NeRF for learning novel view synthesis using a single image as input. 




Our method is a little similar to SinNeRF~\cite{sinnerf}. It constructs geometry pseudo labels and semantic pseudo labels to guide the NeRF training.
However, SinNeRF has poor generalization abilities to other unseen scenes. It requires retraining or fine-tuning on each scene to achieve high-quality rendering. 
Different from SinNeRF, our method utilizes a monocular depth teacher network that can generate dense depth maps for both the source view and the target novel views. It does not need semantic information to further regulate the training. Moreover, it has great generalization abilities to new scenes. Namely, it can be easily applied to other unseen images that are not included in the training set.

\paragraph{Novel View Synthesis from Single Image}

 Multi-Plane Images (MPI) representation learned from a single image are used in \cite{Wu_2021_ICCV,Tucker_2020_CVPR,wu2022remote} to represent the 3D scenes and synthesize novel views.
 Unsupervised manners are proposed for novel view synthesis in  \cite{ramirez2021unsupervised,liu2020auto3d}.
Different transformer designs are introduced in \cite{9756742,N2022AutoRF,Rombach_2021_ICCV} for single-view-based novel view synthesis. 3D information, such as depth ~\cite{Shih3DP20,watson2020learning}, mesh ~\cite{Hu_2021_ICCV1} or point cloud ~\cite {Wiles_2020_CVPR} information are also effectively employed for novel view synthesis in  \cite{Shih3DP20,watson2020learning,Hu_2021_ICCV1}. 
 Besides, Liu et al.\cite{Liu_2021_ICCV} propose  long-range novel views generation.  PixelSynth~\cite{Rockwell_2021_ICCV} fuses 3D reasoning with autoregressive modeling for scene synthesis. Srinivasan et al.~\cite{Srinivasan_2017_ICCV} learn to synthesize an RGBD light field from a single image.
 


