
\section{Preliminary} \label{sec:preliminary}
\paragraph{Volume Rendering}
NeRF~\cite{mildenhall2020nerf} represents a scene as a continuous radiance field.
It takes the 3D position $\rm x_i\in \mathbb{R}^3$ and the viewing direction as input and outputs the corresponding color $\mathbf{c}_i$ with its differential density $\sigma_i$. 
NeRFs use volume rendering to render image pixels. For each 3D point $x_i$ in the space, its color can be rendered through the camera ray $\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}$ with $N$ stratified sampled  bins between the near and far bounds of the distance.
\begin{small}
\begin{equation}
    \hat{\mathbf I}(\textrm{r}) = \sum_{i=1}^NT_i(1-\exp({-\sigma_i\delta_i})) \mathbf c_i,
    \\ \quad T_i = \exp\left( -\sum_{j=1}^{i-1}\sigma_j\delta_j \right). \label{eq:volume_rendering}
% \end{small}
\end{equation}
\end{small}
Where $\mathbf{o}$ is the origin of the ray, $T_i$ is the accumulated transmittance along the ray, $\mathbf c_i$ and $\sigma_i$ are the corresponding color and density at the sampled point $t_i$. $\delta_j = t_{j+1}-t_j$ refers to the distance between the adjacent point samples. 

Similarly, the depth map can be rendered as:
\begin{equation} \label{eq_render_depth}
    \hat{\mathcal{D}} = \sum_{i=1}^{N}T_i\big(1-\exp(-\sigma_{i}\delta_{i})\big) z_i.
    % \vspace{-3mm}
\end{equation} 



\paragraph{Planar Neural Radiance Field}
Planar Neural Radiance Field, introduced by \cite{Li_2021_ICCV2}, is a perspective geometry for representing the camera frustum. For any pixel located on image coordinate $(x, y)$ , it represents the 3D location with candidate depth $z$ as $(x, y, z)$. Then, it learns $D$ multi-plane images (in a shape of $depth (D) \times height (H)\times width (W)\times 4$) with ($\mathbf{c}_i, \sigma_i$) at each 3D location $(x,y, z)$ to represent the image.

For novel view rendering, the intersection locations between the camera ray and each plane are sampled to achieve  $\mathbf{c}_i$ and $\sigma_i$ for image rendering using Eq. \eqref{eq:volume_rendering}.  The number of the sampled point $t_i$ equals the number of planes.

The planar rendering is faster than volume rendering and has better generalization abilities to unseen images. However, it's costly in terms of memory consumption since it requires storing a 4D volume for plane sampling. Also, the rendering quality heavily relies on the number of image planes and is thus not suitable for fine rendering.  


\section{Method} \label{sec:method}

\begin{figure*}[t]
% \vspace{-15mm}
\centering
\begin{overpic}[width=1.0\textwidth]{image/pipeline.pdf}
\put(694,365){\footnotesize $\mathcal{L}_p, \mathcal{L}'_d$}%
\put(779,365){\footnotesize $\mathcal{L}_{\mathrm{L}1}, \mathcal{L}_{\mathrm{ssim}}$}%
\end{overpic}
% \vspace{-3mm}
   \caption{Our model contains two parts: the teacher net and the student net. Student net is responsible for novel view synthesis task. It takes a single RGB image as input and outputs a coarse planar radiance field which is later refined by the extra points predicted by the fine decoder. We then combine coarse sampling and fine sampling to jointly render the high-quality novel views and depth maps. The teacher net aims at supervising the student net on depth estimation and boosting geometry consistency.
   }
   \label{fig:pipline}
 \vspace{-2mm}
\end{figure*}
In this section, we describe our DT-NeRF for single-view-based novel view synthesis.
As illustrated in Figure \ref{fig:pipline}, 
our method consists of two major parts: 1) the depth teacher net, % (Sec. \ref{sec:teachernet}) 
that is based on the monocular depth estimation and predicts pseudo dense depth to supervise the student net rendering.
2) the student rendering net. % (Sec. \ref{sec:studentnet}).
It consists of a coarse planar rendering decoder %(Sec. \ref{sec:coarseplannar}) 
and a fine volume rendering decoder. % (Sec. \ref{sec:finevolume}). 
We use the depth teacher to supervise the joint rendering and boost the geometry consistency between the two renderings.  


\subsection{Feature Encoder}
In order to generalize to new scenes without retraining or fine-tuning, we introduce a feature extractor to encode the input image. This allows the network to be trained across multiple scenes to learn a scene prior to generalizations to unseen data.
The whole output image feature tensor is then used as the input of the coarse planar decoder to learn 4-channel (RGB-$\sigma$) MPI, while the sampled point features are fed to the fine volume decoder for fine volume rendering.

We use a vision transformer~\cite{Ranftl_2021_ICCV} as our feature encoder. The vision transformer contains a ResNet50 backbone and 12 transformer blocks. %Following , %we extract the features from the forward process of vision transformer as the output of the encoder. 
In order to reduce the size of the model, the transformer encoder can be shared by the teacher net and the student net. 


\subsection{Depth Teacher}


Previous methods~\cite{Li_2021_ICCV2,Tucker_2020_CVPR} are basically supervised by color information (and some point clouds), their depth estimation is not always reliable, resulting in blurred boundaries or unnatural holes.
To generate a more reasonable depth estimation and boost the 3D geometry consistency between the novel views and the input source image, we add a depth teacher net $\mathcal{G}$ to generate a dense pseudo depth map from a single input RGB image. The dense depth maps of the source view and target view generated from $\mathcal{G}$ are used as pseudo labels to supervise the student net for both RGB and depth rendering. 





Our teacher net is adapted from \cite{Ranftl_2021_ICCV} which consists of a transformer encoder and a convolutional depth prediction decoder. The dense depth map generated by the teacher net builds strong alignment between RGB labels and depth pseudo labels. It contains more 3D prior knowledge than the previous method~\cite{Li_2021_ICCV2,Tucker_2020_CVPR}, and boosts the student net to learn more consistent geometries. 




\subsection{Student Joint Rendering} \label{sec:studentnet}

The teacher net outputs pseudo dense depth maps to supervise our student joint rendering networks. The student net consists of two components: 1) the coarse planar NeRF that learns coarse planar sampling and rendering as guidance to resample 2) the fine volume rendering. Finally both the samples are employed for joint rendering with Eq. \eqref{eq:volume_rendering} (illustrated in Figure \ref{fig:arch}). This is different from the coarse and fine sampling of the original NeRF~\cite{mildenhall2020nerf} where only fine samples are used for final RGB and depth rendering.


\begin{figure}[t]
 \vspace{-5mm}
\begin{center}
    \includegraphics[width=0.99\linewidth]{image/importance_sampling.pdf}
\end{center}
 \vspace{-4.5mm}
   \caption{Joint sampling and rendering consists of the coarse planar rendering (top) and the fine volume rendering (bottom). For rays emitted from the target view camera, the model can roughly estimate the probability density function of the weight $T_i$ from $N_c$ multi-plane images. Then, additional $N_f$  point samples are selected by the importance sampling. The corresponding point features are extracted from the  output of the feature encoder, and then input to the fine decoder together with positions and directions to obtain the RGB-$\sigma$. Finally, $N_c+ N_f$ points are combined for a joint rendering.}
   \label{fig:arch}
 %\vspace{-2mm}
\end{figure}

\paragraph{Coarse Planar Neural Radiance Field} \label{sec:coarsplannar}
Our planar neural radiance field network consists of an encoder and a decoder. The feature encoder takes a source view image as input and outputs a feature tensor. The feature tensor is input to the coarse planar decoder and predicts a 4-channel image ($\mathbf{c}_i,\sigma_i$) at each candidate depth $z_i$ (total $N_c$ candidate depth values). %in the shape of 

Similar to \cite{Li_2021_ICCV2}, for each pixel $(x_t, y_t)$ in the target novel views, we warp it to the source multi-plane images according to the camera rays. 

We define the homography transformation $\mathcal{W}(\cdot)$ between the pixel coordinates of the target novel view $(x_t,y_t)$ and the multi-plane images of the source view $(x_i, y_i, z_i)$. %:
For simplicity, we use $(x_i,y_i, z_i)=\mathcal{W}_{i}(x_t,y_t)$ to denote the mapping to a plane with depth of $z_i$ at the source camera (More details can be found in \cite{Li_2021_ICCV2} or the supplementary). 
$N_c$ coarse samples $(\mathbf{c}_i, \sigma_i)$ are selected at the warped location $(x_i, y_i, z_i)$ in $N_c$ image planes.  We then use Eq. \eqref{eq:volume_rendering} to render the coarse images.



\paragraph{Fine Volume Rendering}\label{sec:finevolume}

The planar rendering runs faster and has a better generalization to unseen data. However, it costs a large amount of memory to store multi-plane images and render high-resolution novel views. It's necessary to use a  small $N_c$  to reduce the memory costs. % and propose to implement a different fine volume rendering ways to further render more image details. 
However, insufficient sampling from the image planes (a small $N_c$) usually leads to blurry novel views. Also, the coarse planar rendering doesn't take the ray direction into account and can't model complex view-dependent effects (e.g. lighting variations).  %method which means $c$ and $\sigma$ are just functions of coordinates. 


We build a fine volume MLP decoder to boost the rendering and select more important RGB-$\sigma$ values of interested points in the space. %We improve the rendering by sampling new important points from the camera rays. 
The new importance sampling is guided by the coarse planar sampling results. Since the volume rendering of Eq.~\eqref{eq:volume_rendering} can be interpreted as a weighted sum of all sampled colors $\mathbf{c}_i$, we compute the weights of colors $T_{i}$ after the rendering with the coarse planar neural field for each ray. We can then obtain a probability density function (PDF) estimation of the weight along the ray by normalizing these weights  $\hat{T}_{i}=T_{i} / \sum_{j} T_{j}$. Then, we sample a new set of (a total of $N_f$) positions $\mathbf{x}_i = (x_i, y_i ,z_i)$ from this distribution using inverse transform sampling~\cite{mildenhall2020nerf}. 
%Fine decoder, 

According to the sampled 3D positions, we extract features from multi-plane images. Moreover, we project the 3D spatial coordinate $\mathbf{x}$ to the source image plane using the projection matrix $\mathbf{P}\sim [\mathbf{R}, \mathbf{T}, \mathbf{K}]$ ($(x_s, y_s) = \mathbf{P}\mathbf{x}$, where $\mathbf{R},\mathbf{T}$ are the camera rotation and translation from the target view to the source view and $\mathbf{K}$ is the camera intrinsic parameters).




Given an input feature tensor (output by the feature extractor) $\mathbf{F}_{src}$,  we use the projected pixel coordinates $(x_s, y_s)$ to extract the new feature samples $\mathbf{f}$. 
Then, the sampled feature vectors $\mathbf{f}_i$ together with the position and view direction are fed to the fine decoder network (implemented by MLP layers) and output the color $\mathbf{c}$ and density $\sigma$. This is similar to the original NeRF~\cite{mildenhall2020nerf}.  



\paragraph{Joint Sampling and Rendering}
%Both 
We combine the $N_c$ coarse samples and the $N_f$ fine samples by placing them correctly along the camera rays (as illustrated in Figure \ref{fig:arch}). The joint rendering takes the two types of $\mathbf{c}$ and $\sigma$ as inputs to the rendering function of Eq. \eqref{eq:volume_rendering} and outputs the better RGB and depth rendering results.


Since the volume rendering and the coarse planar rendering are not in the same space.  To correctly place the coarse and fine point samples along the camera rays (as illustrated in Figure \ref{fig:arch}).  It is important to preserve a consistent 3D geometry between the coarse MPI and the fine volume. We propose to use the dense pseudo depth maps predicted by our depth teacher to supervise the joint rendering and make them consistent with each other. 






\section{Implementation}

In order to accelerate the convergence in training, we divide our training into two stages. We first fine-tune the teacher net, then, train our student net with the depth teacher fixed.


\subsection{Depth Teacher Pre-training}
Since the depth map predicted by the teacher net is not completely accurate and there may be problems with scale, we pre-train/fine-tune the teacher net using the available sparse point cloud before training student net. Following \cite{monodepth2}, the pre-training is supervised by the projection color errors, the gradient consistency and the sparse 3d points (achieved by SfM algorithm~\cite{colmap}).
\begin{align}
     \mathcal{L}^\mathcal{G} = \mathcal{L}_{proj} + \mathcal{L}_{smooth} + \mathcal{L}_p \label{eq:fine-tune loss},
\end{align}
where the reprojection error $\mathcal{L}_{proj}$ and the edge-aware smoothness loss $\mathcal{L}_{smooth}$ are the same as those in \cite{monodepth2}, and $\mathcal{L}_p$ denotes the point cloud loss (Eq. \eqref{eq:point_loss}). 



\subsection{Learning Joint Rendering}


Our student net mainly has three parts: 1) the ``coarse'' view rendered by planar neural field, 2) the ``fine'' view rendered by fine volume rendering with only importance sampling points, and 3) the ``joint'' rendering results  that combined both the coarse samples and the fine samples. Each part will output the corresponding RGB image $\hat{\mathrm{I}}$ and depth map estimation $\hat{\mathcal{D}}$ respectively. The joint rendering is supervised by 
\begin{equation}
    \mathcal{L} = \mathcal{L}_c +  0.4\mathcal{L}_f + \mathcal{L}_j.
\end{equation}


Where $\{c,f,j\}$ represents the ``coarse'', ``fine'' and ``joint'' rendering. Each part has the same loss that consists of $L1$ loss $\mathcal{L}_{\mathrm{L}1}$,  $SSIM$ loss~\cite{wang2004image} $\mathcal{L}_{\mathrm{ssim}}$, sparse point cloud loss $\mathcal{L}_{p}$ (if available) and pseudo depth loss $\mathcal{L}'_{d}$:
\begin{equation}
\mathcal{L}_{c/f/j}= \mathcal{L}_{\mathrm{L}1}+\lambda_{\mathrm{ssim}} \mathcal{L}_{\mathrm{ssim}}+\lambda_{p} \mathcal{L}_{p}
+ \lambda'_{d} \mathcal{L}'_{d}.
\end{equation}




\begin{equation}
\mathcal{L}_{\mathrm{L}1}=\left|\hat{\mathrm{I}}-\mathrm{I}\right|,  \mathcal{L}_{\mathrm{ssim}}=1-\operatorname{SSIM}\left(\hat{\mathrm{I}}, \mathrm{I}\right)
\end{equation}
The RGB $L1$ loss and $SSIM$ loss make the target novel view $\hat{\mathrm{I}}$ generated by our model match the ground truth image $\mathrm{I}$. 


%\paragraph{Sparse point cloud loss}
If there are some sparse point clouds $\mathbf{P}$ available (usually generated by SfM method~\cite{colmap}), the  sparse point cloud loss can be used as


\begin{small}
\begin{equation}
\mathcal{L}_{p} =\frac{1}{\left|\mathbf{P}\right|} \sum_{(x, y, z) \in \mathbf{P}}\left(\ln \frac{\hat{\mathcal{D}}(x, y)}{s}-\ln \frac{1}{z}\right),
\label{eq:point_loss}
\end{equation}
\end{small}
where, $\hat{\mathcal{D}}$ represents the rendered disparity  (inverse depth) map and $s$ represents the scale factor relative to the point clouds set $\mathbf{P}$. The point cloud loss is applied for both the source view  and the target novel view rendering.

%\paragraph{Scale Calibration}
Since the scale $s$ between the disparity map generated by depth teacher network, student net and the point cloud  are usually different, it is necessary to scale them to a uniform scale before supervising training. When point cloud $\mathbf{P}$ is available, following \cite{Li_2021_ICCV2}, we unify the point cloud scale by
\begin{small}
\begin{equation}
s=\exp \left[\frac{1}{\left|\mathbf{P}\right|} 
\sum_{(x, y, z) \in \mathbf{P}}\left(\ln \frac{1}{z}-\ln \hat{\mathcal{D}}(x, y) \right)\right].
\end{equation}
\end{small}


For the pseudo depth loss $L'_d$, the $L1$ depth loss and the gradient  loss are used as

\begin{align}
\begin{array}{rrl}
    \mathcal{L}'_d &=& |\hat{\mathcal{D}}-\mathcal{D}^* |
    + \lambda_{grad}( |\partial_{x}(\hat{\mathcal{D}})-\partial_{x}(\mathcal{D}^*)| \\&+& |\partial_{y}(\hat{\mathcal{D}})-\partial_{y}(\mathcal{D}^*)|).
    \end{array}
    \label{eq:loss_dpt_l1}
\end{align}


$\hat{\mathcal{D}}$ and $\mathcal{D}^*$ are the scaled disparity  (inverse depth) maps predicted from the student net and the teacher net respectively. $\partial_{x}$ and $\partial_{y}$ are gradients of the disparity maps. We apply these two losses in both the source and the target view to boost the geometry consistency between the student rendering net and the depth teacher net. 





In practice, for stable training and faster convergence, we train our student net by two steps. We first train the coarse planar neural radiance field, then fix the coarse planar rendering, and set loss to $0.4\mathcal{L}_f+\mathcal{L}_j$ to train fine decoder.



\subsection{Inpainting Refinement}
Since it's difficult to render image borders of the novel views when it is out of the field of view of the source image, we implement a light-weighted inpainting module to refine both the RGB and the depth results at the occlusions and image borders.  We leverage the predicted depth map of the source view to compute the occlusion mask of the target view by depth warping. We then follow \cite{suvorov2021resolution} to learn the inpainting, but change to use a four-channel (RGB-D) representation for both the input and the output. Inspired by \cite{han2022single}, we use the warp-back strategy to augment the data for training the inpainting module.

\subsection{Training Details}

For our experiments, $N_c$ is fixed to 32, $N_f$ is 16, $\lambda_{\mathrm{ssim}}$, $\lambda_{L1}$ and $\lambda_{p}$  are set to 1. We use the
Adam Optimizer~\cite{kingma2014adam} with an initial learning rate of 0.001 for both the planar radiance decoder and fine decoder, 1e-5 for transformer encoder, and 1e-4 for depth teacher $\mathcal{G}$'s decoder. Our fine decoder is a light-weighted MLP module which has 5 hidden layers with 64 channels. Fine decoder processes a 150-dimension feature and output 4-channel $(\textbf{c}, \sigma)$ prediction. Before feeding positions into the fine decoder, we apply a positional encoding~\cite{mildenhall2020nerf} which maps 3D coordinates into a 63-dimension feature space.
  











