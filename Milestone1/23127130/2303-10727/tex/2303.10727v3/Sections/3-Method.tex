\section{The proposed ERSAM framework}

\begin{figure}[b]
\vspace{-1em}
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/overview_v3.pdf}
    \vspace{-2em}
    \caption{Overview of our proposed ERSAM framework.}
    \label{fig:overview}
\end{figure}

\textbf{Overview.} 
As shown in Fig.~\ref{fig:overview}, our ERSAM accepts training data and corresponding labels of limited size, hardware-aware network operators, and a costly teacher model as inputs, and then automatically generates a dedicated DNN to enable continuous, real-time, on-device DNN based SAM. 
ERSAM integrates two enablers: \underline{(i)} a hardware-aware search space dedicated to SAM based on the cost profiling observations of state-of-the-art DNN based SAM models on mobile phones, and \underline{(ii)} an efficient knowledge distillation scheme to be embedded in the search and training process by only making queries when the student model is uncertain under the given input data. Specifically, during each iteration, we sample one sub-network from the search space, and then pass the training data to the sampled sub-network. If the measured uncertainty based on the sampled sub-network's predictions is higher than a pre-defined threshold, the query to the teacher model will be enabled, and the corresponding back-propagation will be performed to incorporate an extra distillation loss~\cite{hinton2015distilling}. Otherwise, the training will be the same as the standard training process without knowledge distillation. Note that ERSAM is built on top of a weight-sharing NAS ~\cite{cai2019once,yu2020bignas}, which trains all the sub-networks in a weight-sharing supernet and then locates the optimal one under different cost constraints without re-training; thus, the search cost is shared by training the supernet.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{Figures/op_profile.pdf}
    \vspace{-1.5em}
   \caption{Latency of different operators in wav2vec~\cite{schneider2019wav2vec} profiled on a Pixel 3~\cite{pixel3} phone.}
    \label{fig:wav2vec_profile}
    \vspace{-1em}
\end{figure}

\subsection{Enabler 1: Hardware-aware search space design}
\label{sec:space_design}

As pointed out in prior works, even for DNNs that have the same width, depth, or input resolution, the real hardware-cost (e.g., latency and/or energy) can be quite different on a target device~\cite{li2021hw,fu2022depthshrinker}. Thus, a hardware-aware search space is critical for achieving our target continuous, real-time, and on-device DNN-based SAM. Specifically, we first analyze the latency of each operator in wav2vec~\cite{schneider2019wav2vec}, a state-of-the-art speech recognition model. As shown in Fig.~\ref{fig:wav2vec_profile}, the bottleneck operator, marked with a red circle in Fig.~\ref{fig:wav2vec_profile}, has a cost that is $>$ 35\% of the whole model's latency on a Pixel 3 phone. We find that the above cost-dominant operator features both a large kernel size (i.e., 8) and input sequence length (i.e., $0.2 \times$ of the original input sequence length), while maintaining the same channel size as other operators (i.e., 512). As such, we design a hardware-aware search space for SAM that is abstracted from the model architecture of wav2vec~\cite{schneider2019wav2vec}, while avoiding cases where a large channel size, kernel size, and input sequence length simultaneously exist within the same operator. The developed search space in our ERSAM framework, inspired by our cost profiling observations on mobile phones, is summarized in Tab.~\ref{tab:search_space}.

\begin{table}[t]
\caption{Macro-architecture of our designed hardware-aware hardware search space for SAM.}
\vspace{-0.5em}
\centering
  \resizebox{0.95\linewidth}{!}
  {
    \begin{tabular}{c||cccc}
    \toprule
    Operator Type & \#Channels & \#Repeats & Kernel Size & Stride \\
    \midrule
    1D Conv. & 16 - 32 & \{1\} & 10 & 5 \\
    1D Conv. & 32 - 64  & \{1\}  & 8 & 4 \\
    1D Conv. & 64 - 128  & \{1, 2, 3\} & 4 & 2 \\
    1D Conv. & 128 - 256  & \{1, 2, 3\} & 1 & 1 \\
    1D Conv. & 128 - 256  & \{1, 2, 3\} & \{1, 2, 3\} & 1 \\
    1D Conv. & 128 - 256  & \{1, 2, 3\} & \{4, 5, 6\} & 1 \\
    1D Conv. & 128 - 256  & \{1, 2, 3\} & \{7, 8, 9\} & 1 \\
    1D Conv. & 128 - 256 & \{1, 2, 3\} & \{10, 11, 12\} & 1 \\
    \bottomrule
    \end{tabular}
    }
  \label{tab:search_space}
  \vspace{-1em}
\end{table}


\subsection{Enabler 2: Efficient knowledge distillation scheme}
\label{sec:efficient_kd}
The proposed efficient knowledge distillation scheme is based on our observation that the improved accuracy of larger models over smaller models for SAM is a result of the case where the smaller models are wrong and uncertain but the larger models are correct and certain. Specifically, we compare the correct and wrong predictions of a larger model called wav2vec~\cite{schneider2019wav2vec} and a smaller counterpart, a uniformly scaled wav2vec with only 6 (0.3$\times$) layers and 128 (0.25$\times$) channels, on the LibriSpeech-SAM dataset described in Sec.~\ref{sec:exp_settings}.  


\begin{wrapfigure}{r}{0.54\linewidth}
    \centering 
    \vspace{-1.5em}
  \includegraphics[width=1.0\linewidth]{Figures/big_vs_small.pdf}
  \vspace{-2em}
  \caption{The percentage of cases when varying the smaller model's uncertainties (AC: both correct, LC: only the larger model correct, SC: only the smaller model correct, and AW: both wrong).}
  \vspace{-1.5em}
\label{fig:big_vs_small}
\end{wrapfigure}

As shown in Fig.~\ref{fig:big_vs_small}, we can observe that the more uncertain the smaller model is, the more improvement can be gained by switching to the larger model from the smaller one, as evidenced by the sharp increase in the red curve corresponding to the case where only the large model is correct. Such an observation is also consistent with recent findings in natural language processing tasks~\cite{narayan2022predicting}, i.e., the larger models' advantages over smaller ones appear when the latter are not uncertain about the input data.
Leveraging the above observation, in the forward process of our proposed efficient knowledge distillation scheme, we first obtain the student model's classification probability distribution, and then compare it with the prior classification probability distribution to measure the uncertainty of the current input data, which is defined as follows:  

\vspace{-2.5em}
\begin{align}
    \begin{split} \label{eq:conv1}
    S_{sample} = - \log \left(\frac{1}{n}\sum^{n-1}_{i=0} (Y_i - \hat{Y_i})^2\right),
    \end{split} \\
    \begin{split} \label{eq:conv2}
    S_{batch} = \frac{1}{s}\sum^{s-1}_{j=0} S_{sample,j},
     \end{split}
\end{align}
\vspace{-1em}

\noindent where $S_{sample}$ and $S_{batch}$ denote the uncertainty score of one sample and a batch of samples, $s$ is the batch size, $Y_i$ is the output probability of the model for class $i$, $\hat{Y_i}$ is the prior probability of the labels for class $i$, and $n$ is the number of classes. To simplify the measurement, all our experiments are based on the assumption that different classes have the same prior probability, i.e., $\hat{Y_i} = \hat{Y_j}$ for any $i$ and $j$. As shown in Fig.~\ref{fig:overview}, if $S_{batch}$ is larger than the pre-defined threshold, then a query to the teacher model will be made; otherwise, this iteration will proceed with the standard training without knowledge distillation. The proposed efficient knowledge distillation scheme can achieve the same or even better accuracy compared with that of training with vanilla knowledge distillation~\cite{hinton2015distilling} yet at a similar training speed as the standard training, as verified in Sec.~\ref{sec:abl_efficient_kd}.