\section{Related works}
\subsection{SAM based on counting concurrent speakers} 
Speech audio can provide important information about social ambiance, and can be applied to finding social hot spots~\cite{makhervaks2020combining}, event contexts~\cite{wang2014local}\cite{khan2015sensepresence}, and children's language environments~\cite{cristia2021thorough,ramirez2014look}. Specifically, \cite{wang2014local} measures social ambiance by inferring the occupancy and human chatter levels in local business scenarios; \cite{khan2015sensepresence} characterizes social ambiance from overlapping conversational data in a crowded environment. More recently, the number of concurrent speakers around an individual has been verified
to be associated with mental health symptoms~\cite{chen2021privacy}. The number of simultaneous speakers is leveraged as a proxy for the overall social activity in the associated environment based on the assumption that concurrent speakers provide fine-grained information of social ambiance~\cite{chen2021privacy}.   
 
Meanwhile, several recent studies~\cite{stoter2018countnet,peng2020competing} have shown that DNNs can achieve state-of-the-art accuracy in predicting the number of concurrent speakers as compared to non-DNN solutions. However, they either \underline{(i)} can only estimate up to five speakers, which cannot cover all social scenarios in daily life,  or \underline{(ii)} require more than 30 hours of training data, which can be challenging to acquire and label for SAM under clinical scenarios. In contrast, our proposed ERSAM can automatically deliver DNNs that perform energy-efficient
and real-time
SAM, based on a small-scale
training dataset which favors preserving privacy and timely feedback to fulfill the requirements of SAM under clinical scenarios. 

\subsection{Hardware-aware neural architecture search}
\label{sec:related_worke_hw_nas}
HW-NAS has been proposed with the aim of automating the search for efficient DNN structures under target hardware efficiency constraints (e.g., energy or latency on target devices) \cite{zhang2020dna}. Besides the early works based on reinforcement learning~\cite{tan2019mnasnet, howard2019searching}, ~\cite{wu2019fbnet,cai2018proxylessnas} develop differentiable HW-NAS following~\cite{liu2018darts} to significantly improve search efficiency. More recently,~\cite{cai2019once,yu2020bignas} propose to jointly train all sub-networks within the search space in a weight-sharing supernet and then locate the optimal architectures under different cost constraints without re-training or fine-tuning, thus reducing the cost of the whole search and training pipeline as compared to previous HW-NAS solutions. However, all these HW-NAS works are dedicated to computer vision or natural language processing applications and cannot be directly applied to SAM, due to the differences in input modalities, commonly used operators and dataset size. Note that although there exist prior works that apply NAS to speech recognition~\cite{chen2020darts,mo2020neural,kim2020evolved}, none of them target developing continuous, real-time, and on-device DNN-based SAM that meets all the requirements mentioned in Sec.~\ref{sec:intro}.

\subsection{Efficient knowledge distillation}
Although knowledge distillation~\cite{hinton2015distilling} is commonly used to boost the achievable accuracy vs. inference efficiency trade-offs of compact DNNs, it requires more training time due to the extra overhead of querying larger teacher models~\cite{gou2021knowledge,bommasani2021opportunities}.            

While there exist prior works that focus on efficient knowledge distillation, they either \underline{(i)} only focus on the data efficiency~\cite{wang2020neural,meng2019conditional} instead of the efficiency of the training latency or energy, or \underline{(ii)} are only designed for or verified on computer vision tasks with specially designed image operators~\cite{shen2021fast}. Specifically, ~\cite{meng2019conditional} leverages the insight that teacher models are not always perfect and bypasses the wrong predictions of teacher models to improve the accuracy of student models. Different from all the prior works mentioned above, our proposed efficient knowledge distillation scheme \underline{(i)} presents a different insight, \textcolor{black}{which is} the improved accuracy of the larger teacher model over the small student model is caused by the case when the smaller models are wrong and uncertain while the larger models are correct and certain and \underline{(ii)} targets compressing training/search cost.