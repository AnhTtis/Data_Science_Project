\section{Experiments results}
\subsection{Experiment settings}


\label{sec:exp_settings}
\textbf{Dataset.} To the best of our knowledge, there does not exist any realistic dataset labeled with the number of simultaneous speakers. Thus, we have synthesized a dataset with speech mixtures from LibriSpeech corpus\cite{panayotov2015librispeech}. To create a $k$-speaker mixture, where $k\in\{0, 1, 2, 3, 4, \geq5\}$, we first generate a 15-30 minutes recording by concatenating all the sentences from each speaker, and then randomly select an audio segment from each recording. Finally, segments from $k$ speakers are trimmed to 5 seconds and overlapped with each other to generate a speech mixture which is labeled as speaker count of $k$. Additionally, we include non-speech samples from the TUT Acoustic Scenes dataset \cite{Mesaros2018_IWAENC} in our training data when $k$ = 0 for scenarios where no speech is detected. Following the procedures above, 8 hours of speech mixtures are generated for training and validation. Meanwhile, the test subset with 2 hours of data is synthesized from a different set of speakers from LibriSpeech-test subset. The synthesized dataset is denoted as LibriSpeech-SAM and is used for all the experiments in this work. Specifically, to be fair with each possible speaker count, the dataset features balanced class distribution for both train and test subsets, i.e.,  $\sim1,000$ samples for each class in the training set and $\sim300$ samples in the testing set.

\textbf{Baselines and evaluation metrics.} We consider the following two models as our baselines: \underline{(i)} (uniformly scaled) wav2vec~\cite{schneider2019wav2vec}, one of the most common models for speech recognition. Because the original wav2vec~\cite{schneider2019wav2vec} exceeds the latency and energy budgets for being real-time on mobile devices, we scale it down by reducing its number of channels and number of layers uniformly to fulfill the requirements in Sec.~\ref{sec:intro}; \underline{(ii)} Countnet~\cite{stoter2018countnet}, the previous state-of-the-art for counting the number of speakers. For a fair comparison, we retrain it with the corresponding official implementation on our LibriSpeech-SAM dataset, because its accuracy reported in~\cite{stoter2018countnet} is trained on a different dataset. Moreover, we leverage the following evaluation metrics in all our experiments: \underline{(i)} the \textbf{error rate} on the test set of LibriSpeech-SAM; \underline{(ii)} the \textbf{latency} of an audio segment of 5 seconds on a Pixel 3 Phone; \underline{(iii)} the \textbf{energy consumption} on a Pixel 3 Phone when running all day (i.e., 12 h active time). 

\textbf{Experiment platforms.} All the training and search processes are performed on a workstation with a commonly-used NVIDIA 2080 Ti GPU to match the settings of the training on the local edge (e.g., on personal desktops or laptops). The latency and energy consumption of DNN's inference are measured on a Pixel 3 Phone by running the model using the official tflite benchmark binary~\cite{pixel3_latency_measurement} and being monitored by the Qualcomm Snapdragon Profiler~\cite{snapdragon_profiler}.


\begin{table}[b]
\vspace{-1.3em}
\caption{Compare the DNN searched by our proposed ERSAM with the previous state-of-the-art.}
\centering
\vspace{-0.5em}
  \resizebox{\linewidth}{!}
  {
    \begin{tabular}{c||ccc}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \textbf{Error Rate on} & \textbf{Latency on} & \textbf{Energy consumption}  \\
     & \textbf{LibriSpeech-SAM (\%)} &  \textbf{Pixel 3 (ms)} &  \textbf{on Pixel 3 (mW$\cdot$h)} \\
     \midrule
     Countnet~\cite{stoter2018countnet} & 16.7 & 492  & 422 \\
     (Uniformly Scaled) wav2vec~\cite{schneider2019wav2vec} & 17.4 & 50 & 40 \\
     \textbf{ERSAM} & \textbf{14.3} & \textbf{47} & \textbf{38}  \\
    \bottomrule
    \end{tabular}
    }
  \label{tab:compare_with_sota}
\end{table}

\subsection{Compare with state-of-the-art}
We summarize the comparison of our proposed ERSAM with the previous state-of-the-art in both Tab.~\ref{tab:compare_with_sota} and Fig.~\ref{fig:performance_overview}. We can observe that ours can achieve the best error rate vs. hardware cost (e.g., latency or energy consumption) trade-offs, verifying our ERSAM's ability to deliver continuous, real-time, and on-device DNN-based SAM in an automatic manner without manually searching.

\subsection{Ablation study on the proposed efficient knowledge distillation scheme}

To better understand the efficacy and efficiency of our proposed efficient knowledge distillation scheme introduced in Sec.~\ref{sec:efficient_kd}, we compare it with the vanilla knowledge distillation~\cite{hinton2015distilling} and the standard training without any knowledge distillation in terms of the error rate of the finally delivered DNN and the corresponding search and training time. 
As summarized in Tab.~\ref{tab:abs_efficient_distillation}, we can observe that \underline{(i)} the vanilla knowledge distillation can achieve a lower error rate (e.g., $\downarrow$ 2.4\%) but 
with $2.95 \times$ of the search and training cost, as compared to standard training (i.e., ERSAM w/o KD); \underline{(ii)} our proposed efficient knowledge distillation achieves the lowest error rate among all competitors while maintaining a similar (e.g., $1.05 \times$) search and training cost with standard training. Such observations imply that our proposed efficient knowledge distillation scheme can be used as a plug-and-play module in HW-NAS, thanks to its advantage of achieving a lower error rate than vanilla knowledge distillation without requiring extra search and training costs.

\label{sec:abl_efficient_kd}
\begin{table}[h]
\vspace{-0.5em}
\caption{Comparison among ERSAM w/o knowledge distillation (KD), w/ vanilla KD~\cite{hinton2015distilling}, and w/ our proposed efficient KD.}
\centering
\vspace{-0.5em}
  \resizebox{0.9\linewidth}{!}
  {
    \begin{tabular}{c||cc}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \textbf{Error Rate on } & \textbf{Search Cost}  \\
     & \textbf{LibriSpeech-SAM (\%)} & \textbf{(GPU hours)}  \\
     \midrule
     ERSAM w/o KD & 16.9   & 2.1 \\
     ERSAM w/ vanilla KD~\cite{hinton2015distilling} & 14.5 ($\downarrow$ 2.4)   & 6.2 (2.95 $\times$) \\
     \textbf{ERSAM w/ our proposed KD} &  \textbf{ 14.3 ($\downarrow$ 2.6) }  & \textbf{2.2 (1.05 $\times$)} \\
    \bottomrule
    \end{tabular}
    }
  \label{tab:abs_efficient_distillation}
  \vspace{-1.5em}
\end{table}


\subsection{Generalize to real-world scenarios}

To simulate speech audio collected from realistic scenarios, we \underline{(i)} apply MUSAN noises \cite{snyder2015musan} (12-18 dB) to each recording, \underline{(ii)} include domain shifts caused by recording devices using LibriAdapt~\cite{mathur2020libri}, and \underline{(iii)} construct a few noisy SAM datasets following Sec.~\ref{sec:exp_settings}. As summarized in Tab.~\ref{tab:generalization}, we can observe that the proposed ERSAM can still achieve a low error rate under challenging settings and beat the uniformly scaled wav2vec~\cite{baevski2020wav2vec} in terms of achieved error rate vs. hardware cost trade-offs (i.e., a $\downarrow$ 2.7\% $\sim$ 5.0\% lower error rate under similar latency and energy consumption).


\begin{table}[h]
\vspace{-0.5em}
\caption{Generalize the DNN searched by our proposed ERSAM and uniformly scaled wav2vec~\cite{baevski2020wav2vec} to the dataset with noises and device diversity for simulating the scenarios of real-world application.}
\centering
\vspace{-0.5em}
  \resizebox{\linewidth}{!}
  {
    \begin{tabular}{c||cc}
    \toprule
    Error Rate (\%) & (scaled) wav2vec~\cite{schneider2019wav2vec} & \textbf{ERSAM} \\
     \midrule
     LibriSpeech-SAM & 17.4 & \textbf{14.3} ($\downarrow$ 3.1) \\
     LibriSpeech-SAM, Noisy & 20.7 & \textbf{17.1} ($\downarrow$ 3.6) \\
     LibriAdapt-SAM on Pseye & 30.4 & \textbf{27.5} ($\downarrow$ 2.9) \\
     LibriAdapt-SAM on Respeaker & 28.6 & \textbf{23.6} ($\downarrow$ 5.0) \\
     LibriAdapt-SAM on Shure & 28.7 & \textbf{25.9} ($\downarrow$ 2.8) \\
    \bottomrule
    \end{tabular}
    }
  \label{tab:generalization}
  \vspace{-1.5em}
\end{table}