\section{Technique Overview}\label{sec:tech_overview}
This paper presents a proof demonstrating that a two-layer neural network employing the exponential activation function which can achieve a desired small loss value after sufficient iterations, given a large enough number of neurons $m$, an appropriate learning rate $\eta$, and the initialization method specified in Definition~\ref{def:duplicate_weights}.

By bounding the difference of the weights over the training and choosing a proper learning rate $\eta$, we bound the loss by induction. We will introduce how we bound the loss under the assumption for the small perturbation on the weights. And then we will introduce how we bound the  weights and gradients respectively.

\paragraph{Bounding the loss by induction}

To establish this result, we begin by bounding the summation of the differences between the weights at the current step and their initial values, assuming that $w$ is in a small range such that $\Delta w_r(t)\leq R\in (0,0.01)$ where $t$ denote the step here. 

To establish an upper bound on the prediction loss $\|y-F(t+1)\|_2^2$, we first assume that the weight $w$ is within a small range, namely $\Delta w_r(t)\leq R\in (0,0.01)$. We decompose the loss into four parts: 
\begin{itemize}
    \item The loss at the previous step  $\|y-F(t)\|_2^2$
    \item $C_1:=-2m \eta (F(t)-y)^\top H(t) (F(t)-y)$
    \item $C_2:=2 (F(t)-y)^\top H_{\asy} (t) (F(t)-y)$
    \item $C_3:=\| F(t+1) - F(t) \|_2^2$
\end{itemize}

Using terms that involve the parameters $m$, $\eta$, $n$, and $B$ multiplied by $\|y-F(t)\|_2^2$, we can compute the upper bounds $C_1$, $C_2$, and $C_3$ respectively. Then, by induction and choosing a proper value for $m$, we can bound $\|y-F(t)\|_2^2$ where the loss is $\|y-F(0)\|_2^2= \|y\|_2^2$ at initialization under the given assumption.

\paragraph{Bounding the weights by induction}
Finally, to complete the proof, we establish an upper bound for $\Delta w_r(t)=\|w_r(t)-w_r(0)\|_2$. To do this, we transform $\Delta w_r(t)$ into a form that is a multiple of $\exp(B+R)\sqrt{n}$ and the current loss $\|y-F(t)\|_2$. 

\paragraph{Bounding the gradients by induction}
 Based on the result above, we will continue our work on bounding the changing of the weights over the training. By appropriately choosing the learning rate $\eta$, we can ensure that $\Delta w_r(t)$ is small enough, namely $0.01$. We have the following definition 
\begin{align*}
    H(w)_{i,j} :=  \frac{1}{m} \langle x_i,x_j\rangle \sum_{r\in [m]} \exp(\langle w_r,x_i\rangle)\cdot \exp( \langle w_r,x_j\rangle)
\end{align*}
where $r\in [m]$ as the index the neurons, and $x_i$ denotes the data where $i\in [n]$ and $j\in [n]$.

Next, we will constrain the changes of $H$ under the assumption that $w$ is located within a small ball. In addition, we must bound the discrepancy between discrete and continuous functions. Drawing upon the conclusions reached regarding perturbations in weight $w$, we can guarantee the convergence of the over-parametrized neural network with an exponential activation function. 



