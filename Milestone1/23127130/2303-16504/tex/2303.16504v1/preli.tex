\section{Preliminary}\label{sec:preli}


In Section~\ref{sec:preli:notations}, we provide several basic notations and definitions. In Section~\ref{sec:preli:data_points}, we present our assumptions for data points. 
In Section~\ref{sec:preli:weights}, we outline our assumptions regarding weight initialization. Section~\ref{sec:preli:algera} provides some basic algebraic concepts used throughout the paper. Finally, Section~\ref{sec:preli:probtools} discusses various probability tools used in our work.

 

\subsection{Notations}\label{sec:preli:notations}


In our notation, $[n]$ represents the set $\{1,2,\cdots, n\}$. The $\exp$ activation function is denoted by $\phi(x) = \exp(x)$.

The $\ell_2$ norm of a vector $y \in \R^n$ is denoted by $\| y \|_2:= (\sum_{i=1}^n y_i^2)^{1/2}$ and represents the element-wise square root of the sum of squares of each entry in the vector.

The spectral norm of a matrix $B$ is denoted by $\| B \|$. We also define the Frobenius norm $\| B \|_F = ( \sum_{i} \sum_{j} B_{i,j}^2 )^{1/2}$ and the $\ell_1$ norm $\| B \|_1 = \sum_{i} \sum_{j} |B_{i,j}|$ of matrix $B$.

For any symmetric matrix $B\in \R^{k\times k}$, we define its eigenvalue decomposition as $U\Lambda U^\top$, where $\Lambda$ is a diagonal matrix. Let $\lambda_1,\cdots ,\lambda_k$ denote the entries on diagonal of $\lambda \in \R^{k\times k}$. We say $\lambda_i$ is the $i$-th eigenvalue. Usually, we write it as $\lambda_i(B)$ where $i\in [k]$.

We define 
\begin{align*}
    \lambda_{\min} (B) := \min_{i\in [k]}\{\lambda_1,\cdots,\lambda_k\}.
\end{align*}

We use ${\cal N}(\mu, \Sigma)$ to denote a $d$-dimensional Gaussian distribution with mean $\mu \in \R^d$ and covariance matrix $\Sigma \in \R^{d \times d}$.

\subsection{Data points}\label{sec:preli:data_points}

\begin{definition}\label{def:data}
We assume the data points satisfy 
\begin{itemize}
    \item $\| x_i \|_2 \leq 1$, for all $i \in [n]$
    \item $|y_i | \leq 1$, for all $i \in [n]$
\end{itemize}
\end{definition}

\subsection{Initialization Weights}\label{sec:preli:weights}
The following weights initialization are very standard in the literature, e.g., see \cite{dzps19,sy19,bpsw21,syz21}.
\begin{definition}\label{def:standard_weights}
We choose weights as follows
\begin{itemize}
    \item $\forall r\in [m]$, $a_r$ is randomly and uniformly sampled from the set $\{-1, +1\}$.
    \item We sample $w_r$ from ${\cal N}(0, \sigma^2 I)$ for each $r \in [m]$
\end{itemize}
\end{definition}

To improve the initialization bound $\| y - F(0) \|_2^2$, we will use an idea from \cite{mosw22}. 
The definition~\ref{def:standard_weights} and Definition~\ref{def:duplicate_weights} are the same up to constant factor. So for convenient of analysis, in most places we use Definition~\ref{def:standard_weights}. We only use Definition~\ref{def:duplicate_weights} for bounding the initialization $\| y - u (0) \|_2^2 = 0$.

\begin{definition}\label{def:duplicate_weights}
For each $r\in [m/2]$, we choose weights as follows
\begin{itemize}
    \item  We  sample $a_{2r-1}$ from $\{-1,+1\}$ uniformly at random. 
    \item We sample $w_{2r-1}$ from Gaussian distribution ${\cal N}(0, \sigma^2 I)$ .
    \item We choose $a_{2r} = -a_{2r-1}$.
    \item We choose $w_{2r-1} = w_{2r}$.
\end{itemize}
\end{definition}

\subsection{Basic Algebra}\label{sec:preli:algera}

\begin{fact}[Taylor series]\label{fac:exact_taylor}
We have
\begin{itemize}
    \item $\exp(x) = \sum_{i=0}^{\infty} \frac{1}{i!} x^i$
    \item $\cosh(x) = \sum_{i=0}^{\infty} \frac{1}{(2i)!} x^{2i} $
    \item $\sinh(x) = \sum_{i=0}^{\infty} \frac{1}{(2i+1)!} x^{2i+1} $
\end{itemize}

\end{fact}

\begin{fact}[Cauchy Schwarz]\label{fac:cauchy_schwarz}
For any two vectors $x,y \in \R^n$, we have 
\begin{align*}
\langle x, y \rangle \leq \|x \|_2 \cdot \| y \|_2.
\end{align*}
\end{fact}

\begin{fact}\label{fac:norm}
We have
\begin{itemize}
    \item $\| B  \| \leq \| B \|_F$  
    \item  $\forall B\in \R^{n \times n}$, $\| B \|_F \leq n \| B \|_{\infty} $
    \item  $\forall x \in  R^n$,  $x^\top B x \leq \| x \|_2^2 \cdot \| B \|$
    \item $\lambda_{\min}(A) \geq \lambda_{\min}(B) - \| A - B \| $  
\end{itemize}

\end{fact}

\begin{fact}\label{fac:taylor}
We have
\begin{itemize}
    \item For any $ |x| \leq 0.1$, then we have $| \exp(x) - 1 | \leq 2x$.
    \item For any $|x| \leq 0.1$, then we have $|\cosh(x) - 1| \leq x^2$
    \item For any $|x| \leq 0.1$, then we have $\exp(x) = 1+x + \Theta(1) x^2$.
    \item For any $|x| \leq 0.1$, then we have $(1-x)^{1/2} \leq 1-0.5 x$
    \item For any $x \in (0,0.1)$, we have $\sum_{i=0}^{\infty} x^i \leq \frac{1}{1-x}$
\end{itemize}
\end{fact}

\subsection{Probability Tools}\label{sec:preli:probtools}

We state the standard Bernstein inequality,
\begin{lemma}[Bernstein inequality \cite{b24}]\label{lem:bernstein}
If the following condition holds
\begin{itemize}
    \item $Z_1, \cdots, Z_n$ be independent zero-mean random variables
    \item $|Z_i| \leq M$ almost surely $\forall i\in [n]$
    \item Let $Z=\sum_{i=1}^n Z_i$.
    \item $\Var[Z] = \sum_{j=1}^n \E[Z_j^2]$.
\end{itemize}
 Then, for all positive $t$,
\begin{align*}
\Pr \left[ Z > t \right] \leq \exp \left( - \frac{ t^2/2 }{ \var[Z]  + M t /3 } \right).
\end{align*}
\end{lemma}

We state the standard Hoeffding inequality,
\begin{lemma}[Hoeffding inequality \cite{h63}]\label{lem:hoeffding}
If the following conditions hold
\begin{itemize}
    \item Let $Z_1, \cdots, Z_n$ denote $n$ independent variables
    \item $Z_i \in [\alpha_i,\beta_i]$, for all $i \in [n]$
    \item  Let $Z= \sum_{i=1}^n Z_i$.
\end{itemize}
 Then we have
\begin{align*}
\Pr[ | Z - \E[Z] | \geq t ] \leq 2\exp \left( - \frac{2t^2}{ \sum_{i\in [n]} (\beta_i - \alpha_i)^2 } \right).
\end{align*}
\end{lemma}



We state a standard tool from literature (see Lemma 1 on page 1325 of \cite{lm00}),
\begin{lemma}[Laurent and Massart \cite{lm00}]\label{lem:lm}
Suppose $X$ follows a chi-squared distribution with $k$ degrees of freedom denoted by $\mathcal{X}_k^2$. The mean of each variable is zero, and the variance is $\sigma^2$.
    Then,
    \begin{align*}
        \Pr[X - k\sigma^2 \geq (2\sqrt{kt} + 2t) \sigma^2]
        \leq & ~ \exp{(-t)}\\
        \Pr[k\sigma^2 - X \geq 2\sqrt{kt}\sigma^2]
        \leq & ~ \exp{(-t)}
    \end{align*}
    Further if $k \geq \Omega(\epsilon^{-2} t)$ and $t \geq \Omega(\log(1/\delta))$, then we have
    \begin{align*}
    \Pr[ | X - k \sigma^2 | \leq \epsilon k \sigma^2 ] \leq \delta.
    \end{align*}
\end{lemma}



\section{Problem Formulation}\label{sec:problem_formulation}
In previous formulation in \cite{dzps19,sy19}, they have normalization factor $\frac{1}{\sqrt{m}}$ and only work for ReLU activation function. In \cite{mosw22}, they don't have normalization factor and only work for ReLU activation function.

The statement refers to a specific type of neural network that has two layers. The hidden layer of the network consists of $m$ neurons, and the activation function  is the exponential function.
\begin{align*}
F (W,x,a) := \sum_{r=1}^m a_r \phi ( w_r^\top x ) ,
\end{align*}
To simplify optimization, we only focus on optimizing $W$ and not both $a$ and $W$ simultaneously, where $x \in \R^d$ represents the input, $w_1, \cdots, w_m \in \R^d$ are weight vectors in the first layer, and $a_1, \cdots, a_m \in \R$ are weights in the second layer.
 

$\forall r\in [m]$, given the function $\phi(x)=\exp(x)$,
we have
\begin{align}\label{eq:relu_derivative}
\frac{\partial F (W,x,a)}{\partial w_r}= a_r x\exp(\langle w_r,x\rangle).
\end{align}

\begin{definition}[$F(t)$, dynamic prediction]\label{def:u}
For any timestamp $t$, we define 
\begin{align*}
F_i(t) := \sum_{r=1}^m a_r \exp( \langle w_r(t) , x_i \rangle )
\end{align*}
\end{definition}


\begin{definition}[Loss function over time]
The objective function $L$ is defined as follows:
\begin{align*}
L (W(t) ) := \frac{1}{2} \sum_{i\in [n]} ( F_i(t)- y_i  )^2 .
\end{align*}
\end{definition}




Thus, we define
\begin{definition}[$\Delta w_r(t)$]\label{def:Delta_w_r_at_time_t}
We define $\Delta w_r(t) \in \R^d$, $\forall r \in [m]$ in the following:
\begin{align*}
\Delta w_r(t) : = \sum_{i=1}^n ( F_i(t) - y_i ) a_r x_i \exp( \langle w_r(t) , x_i \rangle ).
\end{align*}
\end{definition}

\begin{definition}[gradient descent update equation]\label{def:update}
The typical approach for optimizing the weight matrix $W$ involves applying the gradient descent algorithm in the following:
\begin{align*}
W(t+1) = W(t) - \eta \Delta W(t) .
\end{align*}
where $\Delta W(t) \in \R^{d \times m}$ and $\Delta w_r(t) \in \R^{d}$ is the $r$-th column of $\Delta W(t) \in \R^{d \times m}$.
\end{definition}




