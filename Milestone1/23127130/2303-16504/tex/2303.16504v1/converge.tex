








 



\section{Convergence}\label{sec:convergence}
In Section~\ref{sec:convergence}, when the neural network is excessively over-parametrized, we observe a linear decrease in the training error, leading to its ultimate convergence to 0.
In Section~\ref{sec:converge:mainresult}, we present our paper's main result. Section~\ref{sec:converge:induction_weights} outlines the induction lemma for weights, while Section~\ref{sec:converge:induction_loss} introduces the induction lemma for loss. Finally, in Section~\ref{sec:coverge:induction_gradient}, we provide the induction lemma for gradient.


\subsection{Main Result}\label{sec:converge:mainresult}

\begin{theorem}[Main result, formal version of Theorem~\ref{thm:informal}]\label{thm:formal}
If the following conditions hold
\begin{itemize}
    \item Let  $\lambda=\lambda_{\min}(H^{\cts})>0$
    \item  $m = \Omega( \lambda^{-2} n^2 \exp(4 B) \log^2(n/\delta) )$
    \item Let $w_r$ and $a_r$ be defined as Definition~\ref{def:duplicate_weights}.
    \item Let $\eta=0.01 \lambda / ( m n^2 \exp(4 B) )$
    \item Let $T = \Omega( (m \eta \lambda)^{-1} \log(n/\epsilon)  ) = \Omega( \lambda^{-2} n^2 \exp(4B) \cdot \log(n/\epsilon) )$
\end{itemize}
Then, we have running algorithm with $T$ iterations
\begin{align*}
    \| F(T) - y \|_2^2 \leq \epsilon
\end{align*}
\end{theorem}
\begin{proof}

We choose $\sigma = 1$.

We have proved $\| F(0) - y \|_2^2 \leq n$.

Using the choice of $T$, then it directly follows from alternatively applying Lemma~\ref{lem:induction_part_2_loss} and Lemma~\ref{lem:induction_part_1_weights}.

Since $\exp(\Theta(B)) = n^{o(1)}$, then in Theorem~\ref{thm:informal}, we can simplify the $n^2 \exp(\Theta(B)) = n^{2+o(1)}$.
\end{proof}


\subsection{Induction Part 1. For Weights}\label{sec:converge:induction_weights}

\begin{lemma}[Induction Part 1 for weights]\label{lem:induction_part_1_weights}
If the following condition hold
\begin{itemize}
    \item General Condition 1. Let $\lambda = \lambda_{\min} (H^{\cts}) > 0$
    \item General Condition 2. $\eta=0.01\lambda /(mn^2 \exp{(4 B)})$
    \item General Condition 3. Let $D$ be defined as Definition~\ref{def:D}
    \item General Condition 4. Let $w_r$ and $a_r$ be defined as Definition~\ref{def:duplicate_weights}.
    \item General Condition 5. $D < R$
    
    \item {\bf Weights Condition.} $\| w_r(i) - w_r(0)\|_2 \leq  R$ for all $i \in [t]$
    \item {\bf Loss Condition.}  $\| F(i) - y \|_2^2 \leq \| F(0) - y \|_2^2\cdot (1-m\eta \lambda/2)^i$,  $\forall i \in [t]$
    \item {\bf Gradient Condition.} $\eta \| \Delta w_r(i) \|_2 \leq 0.01$ for all $r \in [m]$, for all $i
    \in [t]$
\end{itemize}

For $t+1$ and $\forall r\in [m]$, it holds that:
\begin{align*}
\| w_r(t+1) - w_r(0) \|_2 \leq D.
\end{align*}
\end{lemma}
 

\begin{proof}


We have
\begin{align} \label{eq:upper_bound_etasum}
& ~ \eta \sum_{i=0}^\infty (1-n\lambda/2)^{i/2}\notag\\
= & ~ \eta \sum_{i=0}^\infty (1-\eta \lambda/4)^i \notag \\
\leq & ~ \eta \frac{1}{ \eta \lambda / 4 } \notag \\
\leq & ~ 8/\lambda
\end{align}
where the 1st step is due to Fact~\ref{fac:taylor}, the 2nd step is due to Fact~\ref{fac:taylor}, the last step is due to simple algebra.

Our approach involves utilizing the gradient's norm as a means of constraining the distance as follows:
\begin{align*}
& ~ \|w_r(0)-w_r(t+1)\|_2\\
\le & ~\eta \sum_{i=0}^{t} \| \Delta w_r(i) \|_2 \\
\le & ~ \eta \sum_{i=0}^{t} \exp(B+R) \cdot \sqrt{n}  \cdot \|F(i)-y\|_2 \\
\le & ~ \eta \sum_{i=0}^{t} (1-{\eta \lambda}/{2})^{i/2} \cdot \exp(B +  R) \cdot \sqrt{n} \cdot \|F(0)-y\|_2 \\
\le & ~ 8 \sqrt{n} \cdot \lambda^{-1} \cdot \exp(B + R)  \|F(0)-y\|_2 \\
= & ~ D
\end{align*}
where the 1st is from $w_r(s+1)-w_r(s)=\eta \cdot \Delta w_r(s)$, the 2nd step is due to Lemma~\ref{lem:bound_Delta_w_at_time_s} for $m t$ times, the 3rd step is due to Condition 3 in Lemma statement, the forth step is due to simple algebra, and the forth step is due to Eq.~\eqref{eq:upper_bound_etasum}, the last step is due to Condition 2 in Lemma statement.
\end{proof}




\subsection{Induction Part 2. For Loss}\label{sec:converge:induction_loss}
Now, we present our next induction lemma.
\begin{lemma}[Induction Part 2. For Loss]\label{lem:induction_part_2_loss}
Let $t$ be a fixed integer. 

If the following conditions hold
\begin{itemize}
   \item General Condition 1. Let $\lambda = \lambda_{\min} (H^{\cts}) > 0$
    \item General Condition 2. $\eta=0.01\lambda /(mn^2 \exp{(4 B)})$
    \item General Condition 3. Let $D$ be defined as Definition~\ref{def:D}
    \item General Condition 4. Let $w_r$ and $a_r$ be defined as Definition~\ref{def:duplicate_weights}.
    \item General Condition 5. $D < R$
    \item {\bf Weight Condition.} $\| w_r(t) - w_r(0) \|_2 \leq D < R$, $\forall r \in [m]$
    \item {\bf Loss Condition.} $\| F(i) - y \|_2^2 \leq (1-m\eta \lambda/2)^i \cdot \| F(0) - y \|_2^2$, for all $i \in [t]$
    \item {\bf Gradient Condition.} $\eta \| \Delta w_r(i) \|_2 \leq 0.01$  $\forall r \in [m]$, $\forall i\in [t]$
\end{itemize}
Then we have
\begin{align*}
\| F (t+1) - y \|_2^2 \leq ( 1 - m \eta \lambda / 2 )^{t+1} \cdot \| F (0) - y \|_2^2.
\end{align*}
\end{lemma}
\begin{proof}
 



 
Recall the update rule (Definition~\ref{def:update}),
\begin{align*}
w_{r}(t+1) = w_r(t) - \eta \cdot \Delta w_{r}(t)
\end{align*}

$\forall i \in [n]$, it follows that

\begin{align*}
& ~ F_i(t+1) - F_i(t) \\
= & ~   \sum_{r\in [m]} a_r \cdot ( \exp( \langle w_r(t+1),x_i \rangle) - \exp(\langle w_r(t),x_i \rangle) )  \\
= & ~  \sum_{r\in [m]} a_r \cdot \exp(\langle w_r(t),x_i\rangle) \cdot ( \exp(- \eta \langle \Delta w_r(t),x_i\rangle) - 1 ) \\
= & ~  \sum_{r\in [m]} a_r \cdot \exp(w_r(t)^\top x_i) \cdot (-\eta \langle \Delta w_r(t), x_i \rangle + \Theta(1) \eta^2 \langle \Delta w_r(t), x_i \rangle^2 ) \\
= & ~ v_{1,i} + v_{2,i}
\end{align*}
where the third step follows from $|\eta \Delta w_r(t)^\top x_i| \leq 0.01$ and Fact~\ref{fac:taylor},  the last step is from
\begin{align*}
v_{1,i}:= & ~ \sum_{r=1}^m a_r \cdot \exp(\langle w_r(t),x_i\rangle) \cdot (-\eta \langle \Delta w_r(t), x_i \rangle ) \\
v_{2,i}:= & ~ \sum_{r=1}^m a_r \cdot \exp(\langle w_r(t),x_i\rangle) \cdot \Theta(1) \cdot \eta^2 \cdot  \langle \Delta w_r(t), x_i \rangle^2
\end{align*}
Here $v_{1,i}$ is linear in $\eta$ and $v_{2,i}$ is quadratic in $\eta$. Thus, $v_{1,i}$ is the first order term, and $v_{2,i}$ is the second order term.
 


Recall the definition of $H$ over timestamp $t$ (see Definition~\ref{def:H_s})
\begin{align*}
H(t)_{i,j} = & ~ \frac{1}{m} \sum_{r\in [m]} x_i^\top x_j \exp( \langle w_r(t),x_i\rangle) \cdot \exp( \langle w_r(t),x_j\rangle) ,  
\end{align*}
Further, we define $C_1, C_2, C_3$
\begin{align*}
C_1 = & ~ -2 \eta (F(t)- y )^\top H(t) ( F(t)-y) , \\ 
C_2 = & ~ - 2 (F(t)- y  )^\top v_2 , \\
C_3 = & ~ \| F (t+1) - F(t) \|_2^2 . 
\end{align*}
Then we can rewrite
\begin{align*}
\| y -F(t+1) \|_2^2 = \| y - F(t) \|_2^2 + C_1 + C_2 + C_3
\end{align*}



We have
\begin{align*}
  \|F(t)-y\|_2^2 
 \leq  \|F(t-1)-y\|_2^2 \cdot (1-m\eta \lambda/2) 
\end{align*}
 where the first step follows is due to Lemma~\ref{lem:loss_one_step_shrinking}. 

 
 
Thus, we complete the proof.
\end{proof}
 

\subsection{Induction Part 3. For Gradient }\label{sec:coverge:induction_gradient}


\begin{lemma}[Induction Part 3. For Loss]\label{lem:induction_part_3_gradient}
Let $t$ be a fixed integer. 

If the following conditions hold
\begin{itemize}
   \item General Condition 1. Let $\lambda = \lambda_{\min} (H^{\cts}) > 0$
    \item General Condition 2. $\eta=0.01\lambda /(mn^2 \exp{(4 B)})$
    \item General Condition 3. Let $D$ be defined as Definition~\ref{def:D}
    \item General Condition 4. Let $w_r$ and $a_r$ be defined as Definition~\ref{def:duplicate_weights}.
    \item General Condition 5. $D < R$
    \item {\bf Weight Condition.} $\| w_r(t) - w_r(0) \|_2 \leq D < R$, $\forall r \in [m]$
    \item {\bf Loss Condition.} $\|  F(i) - y \|_2^2 \leq \| F(0) - y \|_2^2 \cdot (1-m\eta \lambda/2)^i $, $\forall i \in [t]$
    \item {\bf Gradient Condition.} $\eta \| \Delta w_r(i) \|_2 \leq 0.01$  $\forall r \in [m]$, $\forall i \in [t]$
\end{itemize}
Then we have
\begin{align*}
\eta \| \Delta w_r(t+1) \|_2 \leq 0.01, \forall r \in [m]
\end{align*}
\end{lemma}
\begin{proof}

We have
\begin{align*}
\eta \| \Delta w_r(t+1) \|_2 
= & ~ \eta \left\| \sum_{i=1}^n  a_r x_i \cdot (y_i - F_i(t+1))   \cdot \exp(  \langle w_r(t+1), x_i\rangle ) \right\|_2 \notag\\
\leq & ~ \eta \exp (B + R) \cdot    \sum_{i=1}^n | y_i - F_i(t+1) | \notag\\
\leq & ~ \eta \exp (B + R)  \cdot  \sqrt{n} \cdot \| y - F(s) \|_2  \\
\leq & ~ \eta \exp (B + R)  \cdot  \sqrt{n} \cdot \| y - F(0) \|_2  \\
\leq & ~ \eta \exp (B + R)  \cdot n \\
\leq & ~ 0.01
\end{align*}
where the 1st step follows from Definition~\ref{def:Delta_w_r_at_time_t}, the 2nd step is due to Lemma~\ref{lem:bound_on_exp_w_and_perturb_w}, the 3rd step is due to Cauchy-Schwartz inequality, the 4th step follows is due to {\bf Loss Condition}, the 5th step follows from $\| y-F(0) \|_2 = \sqrt{n}$, the sixth step is due to the choice of $\eta$.

\end{proof}