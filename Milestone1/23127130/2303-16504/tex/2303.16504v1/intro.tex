\section{Introduction}



Neural networks have proven to be effective in a range of different applications, such as image recognition \cite{ksh17, hzrs16} and speech recognition \cite{gmh13}. Overparametrization, the use of more parameters than necessary, is believed to be crucial to the success of deep learning \cite{als19a,als19b,dzps19,sy19,os20,syz21,bpsw21,mosw22,gqsw22}. Surprisingly, even when the data is improperly labeled and the target function is non-smooth and non-convex, over-parameterized neural networks trained with first-order methods can fit all training data, due to the modern architecture with ReLU activations. Furthermore, over-parameterized networks can improve generalization in practice, which contradicts traditional VC-dimension theory.

Large language models (LLMs) have proven to be more effective in processing natural language compared to smaller models and traditional algorithms. Examples of these models include Transformer \cite{vsp+17}, BERT \cite{dclt18}, GPT-3 \cite{bmr+20}, PaLM \cite{cnd+22}, and OPT \cite{zrg+22}.

The attention matrix is the key technical foundation of LLMs, as highlighted in previous research \cite{vsp+17, rns18, dclt18, rwc+19, bmr+20}. This square matrix has rows and columns that correspond to words or "tokens" in natural language, with entries representing the correlations between them. It is used to determine the importance of each token in a sequence when generating an output. In the attention mechanism, each input token is assigned a weight or score based on its relevance to the current output. These scores are calculated using a similarity function that compares the input and output states.




The exponential activation function \cite{dts15,sa19} is a commonly used activation function in neural networks. It maps the input to the output using the exponential function.
One of the main advantages of the exponential activation function is that it can produce positive outputs for any input, which can be useful in certain types of neural networks, such as those used for regression tasks. Additionally, the exponential activation function is continuously differentiable, which is important for backpropagation during training.

Another application of the exponential activation function is in the generation of natural language \cite{vsp+17,dclt18,bmr+20,cnd+22}. Specifically, it has been used in language models such as GPT-3 \cite{bmr+20} to generate text that closely mimics human writing. The exponential function can help to weight the importance of different words in a given context, leading to more accurate and coherent language generation. 

\cite{o23} present GPT-4, a multimodal model capable of producing text outputs from both image and text inputs. While GPT-4 falls short of human-level performance in some real-world scenarios, it achieves human-like results on various professional and academic benchmarks, including scoring in the top $10\%$ on a simulated bar exam. Based on the Transformer architecture, GPT-4 is pre-trained to predict the next token in a document. Post-training alignment improves its accuracy in factual content and adherence to desired behavior. \cite{o23} also developed infrastructure and optimization methods that scale predictably, allowing for accurate predictions of GPT-4's performance using models trained on just $1/1000$ th of its computational capacity.

 


In this work, we consider a natural question,
\begin{center}
{\it Is that possible to prove an over-parameterization bound for exponential activation in neural network learning?}
\end{center}
In this work, we provide a positive answer for this question. 
Assuming a set of data points $\{(x_i, y_i)\}_{i=1}^n \subset \mathbb{R}^d \times \mathbb{R}$, we use $\lambda$ as the minimum eigenvalue of the neural tangent kernel with respect to the exponential activation function, $n$ as the number of points and $m$ as the number of neurons. Moreover, we use $F(t)$ to represent our two-layer neural network with the exponential activation function at the $t$-th step.

Based on our analysis of the perturbation of the weights, we can derive a bound on the prediction loss $\|y-F(t)\|_2^2$. By choosing $m$ to be large enough, specifically $\Omega( \lambda^{-2} \log(n/\delta) n^{2+o(1)} )$, and setting the learning rate $\eta= \Theta( \lambda / ( m n^{2+o(1)} ) )$, we can ensure the convergence of the neural network with exponential activation. 
 


 

\subsection{Our Results}


Our primary result is presented below.
\begin{theorem}[Main result, formal version of Theorem~\ref{thm:formal}]\label{thm:informal}
Let $\delta \in (0,0.1)$ denote the failure probability. Let $\epsilon \in (0,0.1)$ denote the accuracy.
If the following conditions hold
\begin{itemize}
    \item Let  $\lambda > 0$ denote the minimum eigenvalue of neural tangent  kernel with respect to exponential activation.
    \item Let $m = \Omega( \lambda^{-2}  \log(n/\delta)n^{2+o(1)} )$ represent the number of neurons.
    \item Let $w_r$ be random Gaussian weights and $a_r$ be the random $\{-1,+1\}$ weights.
    \item Let $\eta= \Theta( \lambda / ( m n^{2+o(1)}  ) )$ denote the learning rate of gradient descent algorithm
    \item Let $T =\Omega( \lambda^{-2} n^{2+o(1)}  \cdot \log(n/\epsilon) )$ denote the number of iterations of gradient descent algorithm 
\end{itemize}
Then, we have after running algorithm with $T$ iterations. And with probability at least $1-\delta$, we obtain a $w(T)$ such that
\begin{align*}
    \| F(T) - y \|_2^2 \leq \epsilon.
\end{align*}
\end{theorem}

In order to demonstrate the convergence, we begin by selecting a sufficiently large value of $m$ to regulate changes in weights $w$ and gradients over the training. We then assume that $w$ is contained within a small ball, allowing us to complete the proof of convergence.



\section{Related Work}

\subsection{Training over-parameterized neural network}
\paragraph{Convergence}

\cite{dzps19} demonstrate that for a shallow neural network with ReLU activation consisting of $m$ hidden nodes and trained on $n$ data points, as long as $m$ is sufficiently large and no two inputs are parallel, randomly initialized gradient descent will converge to an optimal global solution at a linear rate of convergence for the quadratic loss function. This is due to the fact that over-parametrization and random initialization work together to ensure that every weight vector remains close to its initial value throughout all iterations.

\cite{als19a} have proven that under two assumptions, simple algorithms such as stochastic gradient descent (SGD) can discover Global Minima on the training objective of Deep Neural Networks (DNNs) in Polynomial Time. The two assumptions are that the inputs are non-degenerate, and the network is over-parameterized, meaning the number of hidden neurons is sufficiently large and is polynomial in $L$, the number of DNN layers, and in $n$, the number of training samples. \cite{als19b} study recurrent neural networks (RNNs) used in natural language processing in \cite{als19b}. They demonstrate that with enough neurons, SGD can minimize the regression loss at a linear rate, showing RNNs can memorize data. \cite{als19b} also develop a perturbation theory for analyzing first-order approximation of multi-layer networks. 


\paragraph{Over-parametrization bound, bound on $m$}
In deep learning theory, \cite{sy19} enhance the size of over-parametrization beyond three previous notable results \cite{ll18}, \cite{dzps18}, and \cite{adh+19}.



In neural network training, it is common to initialize all weights as independent Gaussian vectors. However, \cite{mosw22} observed that initializing the weights as independent pairs, where each pair consists of two identical Gaussian vectors, can improve the convergence analysis significantly.


\paragraph{Using data structure to speedup cost per iteration}

 


Preprocessing plays a critical role in the training of over-parameterized neural networks. \cite{syz21} demonstrates that the cost per iteration of training can be reduced by pre-processing the initial weights of the neural network or pre-processing the input data points. Specifically, pre-processing the initial weights can result in a cost of $\wt{O}(m^{1-\Theta(1/d)}nd)$ per iteration, while pre-processing the input data points can further reduce the cost to $\wt{O}(m^{4/5}nd)$ per iteration. \cite{als+22} also propose a new preprocessing method that employs a tree data structure to detect neuron firing during each iteration and  achieves $o(nmd)$ time per iteration and requires $O(nmd)$ time in preprocessing. By using $m^2$ cost only in the initialization phase, \cite{szz21,z22} reach the cost of  $m^{2-\Omega(1)}$ per iteration.

 

Some works focused on the faster second-order optimization algorithms. Although second-order algorithms have a remarkable convergence rate, their high computational cost per iteration renders them impractical. Recent work by \cite{zmg19, cgh+19} which focused on the second-order algorithms has mitigated this computational overhead, resulting in an $O(mn^2)$-time second-order algorithm for training two-layer over-parameterized neural networks. \cite{bpsw21} further accelerates the algorithm of \cite{cgh+19} to achieve an $\wt{O}(mn)$-time backpropagation algorithm for training mildly over-parameterized ReLU networks.

By utilizing data structures, certain methods can decrease the cost per iteration, resulting in faster performance. \cite{gqsw22} analyze the convergence guarantee of adversarial training on a two-layer neural network with shifted ReLU activation, finding that only $o(m)$ neurons are activated per input data per iteration which reached the training cost time cost of $o(mnd)$ per iteration.  \cite{hswz22} introduce a novel training approach for a standard neural network with $m = \poly(n)$ parameters and a batch of $n$ input data points in $\R^d$. By treating neural networks as a collection of binary search trees and making selective modifications to a subset of nodes at each iteration, with $\alpha \in (0.01,1)$ fixed, their method achieves a time complexity of $m^{1-\alpha}nd+n^3$ in the overparametrized regime.






 



\subsection{Attention Theory}

\paragraph{Fast computation and optimization}

In the field of optimazation, \cite{zkv+20} focused the role of adaptive methods in attention models and by \cite{szks21} focused on analyzing the dynamics of a single-head attention head to approximate the learning of a Seq2Seq architecture.
According to \cite{grs+23}, the attention mechanism is a versatile tool that can be used to execute complex, general-purpose programs even in shallow transformer models. \cite{zkv+20} for the role of adaptive methods in attention models and by \cite{szks21} for analyzing the dynamics of a single-head attention head to approximate the learning of a Seq2Seq architecture.

The computation of attention is a crucial aspect of training large language models. Given three matrices $Q, K, V \in [-B, B]^{n\times d}$ as input, the objective is to construct the matrix $\textsc{Att}(Q, K, V ) := \diag( A {\bf 1}_n)^{-1}AV \in \R^{n\times d}$, where $A =\exp(QK^{\top }/d)$ is the ‘attention matrix’. In \cite{as23}, the authors investigate whether faster algorithms are possible by implicitly utilizing the matrix $A$. They present two results that show a sharp transition occurs at $B=\Theta(\sqrt{\log n})$.
A recent study conducted by Zandieh, Han, Daliri, and Karbasi \cite{zhdk23} introduced the first algorithm with provable guarantees for attention approximation. Their algorithm employs techniques from locality sensitive hashing (LSH) \cite{ckns20}. \cite{zhdk23,as23} are mainly focusing on the static version of attention computation problem. \cite{bsz23} define and study the dynamic version of attention computation problem. \cite{bsz23} provides both algorithmic result and hardness result. \cite{rkr+23} introduce the Structured State Space (S4) sequence model, which utilizes a novel parametrization for the SSM. The study demonstrates that the S4 model can be computed with significantly greater efficiency than previous approaches, while still retaining their theoretical advantages. \cite{lsz23} study the regularized exponential regression problem, and provides an algorithm that runs in input sparsity time.



 



\paragraph{Expressivity for transformer}

 

 Expressivity has been studied by \cite{jadc20,egkz22} for self-attention blocks and by \cite{dgv+18,wcm22,hbsn20} for Transformers. 
Research has demonstrated that fine-tuning language models on a set of datasets expressed as instructions can enhance model performance and improve generalization to unfamiliar tasks. \cite{chl+22} investigate instruction fine-tuning with a specific emphasis on expanding the number of tasks, increasing the size of the model, and  fine-tuning on chain-of-thought data. \cite{egkz22} offers a thorough theoretical examination of the inductive biases associated with self-attention modules. The goal is to establish, with rigor, the types of functions and long-range dependencies that self-attention blocks are inclined to represent. 

 

A recent study conducted by \cite{lag+22} delved into this matter by exploring learning automata, which are discrete dynamic systems that are well-suited for recurrent modeling and expressing algorithmic tasks.





The study conducted by \cite{onr+22} commences by introducing a simple weight construction that establishes the likeness between data transformations generated by  a single linear self-attention layer and  gradient-descent (GD) implemented on a regression loss. \cite{llr23} offer a detailed mechanistic explanation of how transformers learn "semantic structure," defined as the ability to capture the co-occurrence patterns of words.








 


\paragraph{In-context learning}
Many works focused on the in-context learning in recent years. 
While beneficial, the quadratic complexity of self-attention on the
input sequence length has limited its application to longer sequences, a topic being actively studied in the community. To
address this limitation, \cite{xzc+21} propose Nystromformer, a model
that exhibits favorable scalability as a function of sequence
length.
During testing, in-context learning takes place as the language model (LM) deduces a common underlying concept among the examples given in a prompt. In a study by \cite{xrlm21}, it was demonstrated that this phenomenon can occur even when there is a difference in the distribution of prompts and the pretraining data. The study was conducted in a scenario where the pretraining data had a combination of hidden Markov models (HMMs).

\cite{asa+22} explore the possibility that transformer-based in-context learners implicitly execute standard learning algorithms by encoding smaller models within their activations and updating these implicit models as new examples are introduced in the context.




In order to gain a better understanding of in-context learning, \cite{gtlv22} examine the well-defined problem of training a model to in-context learn a function class (such as linear functions). Their study investigates whether a model can be trained to in-context learn "most" functions from this class when given data derived from some functions in the class. 
 

The key finding of \cite{egkz22} demonstrates that Transformer networks with bounded-norm have the ability to "create sparse variables." Specifically, a single self-attention head can represent a sparse function of the input sequence, and the sample complexity scales logarithmically with the context length. \cite{mss22} demonstrate that saturated transformers surpass the known limitations of hard-attention transformers. \cite{mss22} subsequently establish that saturated transformers, which utilize floating-point values, can be replicated through constant-depth threshold circuits, which restricts the class of formal languages they can identify. 


 \cite{kgw+23} introduce a watermarking framework designed specifically for proprietary language models. This watermark can be embedded with minimal impact on text quality, and can be detected using an efficient open-source algorithm that does not require access to the language model API or parameters.
 


\paragraph{Other applications and theories of transformer}


Some analysis focused on the Transformer computing on hardware. One of the key principles missing in attention algorithms is their lack of IO awareness, i.e., not accounting for the reads and writes between different levels of GPU memory. In order to address the aforementioned issue, the authors of the paper referenced as \cite{dfe+22} have introduced a new attention algorithm called FlashAttention. This algorithm is designed to be precise, taking into account input-output (IO) operations, while also leveraging tiling to minimize the number of times data needs to be transferred between on-chip SRAM and the GPU's high bandwidth memory (HBM).



\cite{beg+22} examines the ability of neural networks to learn a $k$-sparse parity of n bits, a well-known discrete search problem that is statistically simple but computationally difficult. Through empirical investigations, the authors observe that various neural networks are capable of successfully learning sparse parities, and they note discontinuous phase transitions in the training curves.

\cite{vkb23} propose modifications to generative model learning algorithms that provide strong bounds on the probability of sampling protected content. These modifications are made in an efficient and black box manner.





\paragraph{Roadmap.}
 

Our techniques are outlined briefly in Section~\ref{sec:tech_overview}, while Section~\ref{sec:preli} covers our preliminary tools and notations. In Section~\ref{sec:problem_formulation}, we introduce the problem of interest and define a two-layer neural network with exponential activation functions.
In Section~\ref{sec:intialization_and_perturbation}, we demonstrate that when the width $m$ is sufficiently large, the continuous and discrete versions of the input data's Gram matrix are spectrally close to each other. Section~\ref{sec:convergence} establishes that an over-parameterized neural network achieves linear convergence of the training error to $0$. In Section~\ref{sec:induction_for_weight}, we simplify the problem by defining $D_{\cts},H(s)$ and providing a gradient bound through induction. Then, in Section~\ref{sec:induction_for_loss}, we establish a similar induction-based bound for the loss $\|y-F(t+1)\|_2^2$ at any time.


 