\section{Induction Part 1: For Weight}\label{sec:induction_for_weight}
In Section~\ref{sec:induction_for_weight}, we present the weight bound, which helps us complete the proof. Section~\ref{sec:induction:definition} introduces various definitions used throughout the paper, while Section~\ref{sec:induction:bound_gradient} proposes the bounding gradient lemma and its corresponding proof.


\subsection{Definition of \texorpdfstring{$D$}{}}\label{sec:induction:definition}

To simplify the notation, we present the definition as follows.
\begin{definition}\label{def:D}
We define $D_{\cts}$ 
\begin{align*}
D := 8 \cdot \lambda^{-1} \cdot \exp( B + R ) \cdot \frac{ \sqrt{n} }{ m } \cdot \| y - F(0) \|_2 .
\end{align*}
\end{definition}

We define the kernel with respect to timestample $s$.
\begin{definition}\label{def:H_s}
Let $H(s) \in \R^{n \times n}$ be a matrix defined for any $s$ in the interval $[0,t]$.
\begin{align*}
H(s)_{i,j} := \frac{1}{m} \sum_{r\in [m]} x_i^\top x_j \cdot \exp{(\langle w_r(s),x_i\rangle)} \cdot \exp{(\langle w_r(s),x_j\rangle)}.
\end{align*} 
\end{definition}
\begin{definition} \label{def:H_asy}
For any matrix $P \in [-1,1]^{m \times n}$, we define asymmetric matrix $H_{\asy}$

\begin{align*}
H_{\asy}(s)_{i,j} := \frac{1}{m} \sum_{r\in [m]} x_i^\top x_j \cdot p_{i,r} \cdot \exp{(\langle w_r(s),x_i\rangle)} \cdot \exp{(\langle w_r(s),x_j\rangle)}.
\end{align*} 

\end{definition}
\begin{claim} \label{cla:upper_bound_HP}
We have
\begin{align*}
\| H_{\asy} (s) \|_{\infty} \leq \exp( 2 (B + R) ).
\end{align*}
holds with probability $1-\delta$.
\end{claim}
\begin{proof}

\begin{align*}
     \|H(P)\|_\infty
=  \max_{i\in [n], j\in [n]}\{ \frac{1}{m} \sum_{r\in [m]} x_i^\top x_j \cdot p_{i,r} \cdot \exp{(w_r(s)^\top x_i)} \cdot \exp{(w_r(s)^\top x_j)} \} 
\end{align*}
where the first step is from Definition~\ref{def:H_asy}.

It is sufficient to make a bound for each $i \in [n]$ and $j \in [n]$.

We have
\begin{align*}
  & ~ \frac{1}{m} \sum_{r\in [m]} x_i^\top x_j \cdot p_{i,r} \cdot \exp{(\langle w_r(s), x_i\rangle)} \cdot \exp{(\langle w_r(s), x_j\rangle)} \} \\
 \leq ~ &  \frac{1}{m} \sum_{r\in [m]}  \exp{(\langle w_r(s),x_i\rangle)} \cdot \exp{(\langle w_r(s), x_j\rangle)}\\
\leq ~ &  \frac{1}{m} \sum_{r\in [m]}  \exp{(2 (R+B))}\\
= ~ & \exp(2(B+R))
\end{align*}
 the 1st step is from $\| x_i \|_2 \leq 1$ and $|p_{i,r}| \leq 1$, the second step is due to Lemma~\ref{lem:bound_on_exp_w_and_perturb_w}.
\end{proof}

 

\subsection{Bounding the gradient at any time}\label{sec:induction:bound_gradient}
In this section, we bound the gradient at any time.
\begin{lemma}\label{lem:bound_Delta_w_at_time_s}
It the following condition hold,
\begin{itemize}
    \item $\| w_r(s) - w_r(0) \|_2 \leq R$
\end{itemize}
For any timestamp at time $s$, we have
\begin{align*}
\| \Delta w_r(s) \|_2 
\leq \exp(B+R )  \sqrt{n}  \| y - F(s) \|_2
\end{align*}
\end{lemma}
\begin{proof}

We have
\begin{align*}
\| \Delta w_r(s) \|_2 
= & ~  \left\| \sum_{i=1}^n (y_i - F_i)   a_r x_i \cdot \exp(  w_r(s)^\top x_i ) \right\|_2 \notag\\
\leq & ~ \exp (B + R) \cdot    \sum_{i=1}^n | y_i - F_i(s) | \notag\\
\leq & ~ \exp (B + R)  \cdot  \sqrt{n} \cdot \| y - F(s) \|_2 
\end{align*}
where the first step follows from Definition~\ref{def:Delta_w_r_at_time_t}, the second step follows from Lemma~\ref{lem:bound_on_exp_w_and_perturb_w}, the third step follows from Cauchy-Schwartz inequality.
\end{proof}

\begin{lemma}\label{lem:bound_Delta_w_times_eta}
It the following condition hold,
\begin{itemize}
    \item $\eta = 0.01 \lambda /(m n^2 \exp(4B))$
    \item $\| w_r(s) - w_r(0) \|_2 \leq R$
\end{itemize}
For any timestamp at time $s$, we have
\begin{align*}
\eta \| \Delta w_r(s) \|_2 
\leq 0.01
\end{align*}
\end{lemma}
\begin{proof}
This trivially follows from choice of $\eta$.
\end{proof}