\begin{figure}[t]
  \centering
  \centerline{\includegraphics[width=\columnwidth]{fig/teaser.png}}
  \caption{This illustration displays the capacity of our proposed method to transfer the cinematic motions from the in-the-wild famous film clip \textit{Jaws} (bottom) to another context, replicating and adapting the dolly-zoom effect (a combined translation plus field of view change in opposite directions, causing distinctive motions on background and foreground contents).}
  \label{fig:teaser}
  \vspace{-0.3cm}
\end{figure}

Almost all film directors and visual artists follow the paradigm of \textit{watch-and-learn} by drawing inspiration from others visual works. Imitating masterpieces (also known as visual homage) in their visual composition, camera motion or character action is a popular way to pay tribute to and honor the source work which influenced them. This subtly draws a faint, yet distinctive clue through the development of the whole film history. Examples are commonplace: across the dizzy effect of dolly-zoom in \textit{Vertigo} to the scary scene in \textit{Jaws} (see Fig.~\ref{fig:teaser}); or from the old school Bruce Lee's kung-fu films to blockbusters such as \textit{Kill Bill} and \textit{Matrix} series. The cinematic knowledge encompassed in reference sequences is therefore carried out and inherited through these visual homages, and have even been adapted to more modern visual media such as digital animation or video games. Audiences obviously acknowledge the strong references between the epic Western \textit{The good, the bad and the ugly} of 1966 and the 2010 \textit{Red Dead Redemption}. 

The creation of such \emph{visual homages} in real or virtual environments yet remains a challenging endeavor that requires more than just replicating a camera angle, motion or visual compositions. Such a \emph{cinematic transfer} task is successful only if it can achieve a similar visual perception (\textit{look-and-feel}) between the reference and homage clip, for example in terms of visual composition (\ie how visual elements are framed on screen), perceived motion (\ie how elements and scene move through the whole sequence), but also, camera focus, image depth, actions occurring or scene lighting.

Inspired by the aforementioned \textit{watch-and-learn} paradigm, we propose to address the cinematic transfer problem by focusing on motion characteristics, \ie solving a \emph{cinematic motion transfer} problem.
 
While different techniques are available to extract camera poses and trajectories from reference videos (\eg sparse or dense localization and mapping techniques) and potentially transfer them, the naive replication of such trajectories to new 3D environments (mesh-based or implicit NeRF-based) generally fail to reproduce the perceived motion due to scaling issues, different screen compositions, lack of visual anchors, or scene dissimilarities. Furthermore, these extractiontechniques are very sensitive to widespread camera effects such as shallow depths of field or motion blur. 

In this paper, we propose to address this cinematic motion transfer problem following a different path. Rather than extracting visual features from the reference clip (as geometric properties or encoded in a latent representation) and designing a technique to then recompute a new video clip using these visual features, we rely on the differentiable nature of NeRF representations. We propose the design of a fully differentiable pipeline which takes as input a reference clip, an existing NeRF representation of a scene, and optimizes a sequence of camera parameters (pose and focal length) in both space and time inside the NeRF so as to minimize differences between the motion features of the references views and the clip created from the optimized parameters. By exploiting an end-to-end differentiable pipeline, our process directly backpropagates the changes to spatial and temporal cinematic parameters. The key to successful cinematic motion transfer is then found in the design of relevant motion features and means to improve guidance in the optimization. Our work relies on the combination of an optical flow estimator, to ensure the transfer of camera directions of motions, and a character pose estimator to ensure the anchoring of motions around a target. A dedicated guidance map is then created to draw the attention of the framework on key aspects of the extracted features. 

\noindent The contributions of our work are:

\noindent \textbf{The first feature-driven cinematic motion transfer technique} that can reapply motion characteristics of an in-the-wild reference clip to a NeRF representation.

\noindent \textbf{The design of an end-to-end differentiable pipeline} to directly optimize spatial and temporal cinematic parameters from a reference clip, by exploiting differentiability of neural rendering and proxy networks.

\noindent \textbf{The proposal of robust cinematic losses combined with guidance maps} that ensure the effective transfer of both on-screen motions and character framing to keep the cinematic visual similarity of the reference and generated clip.
