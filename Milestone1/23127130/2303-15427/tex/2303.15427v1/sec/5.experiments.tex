\noindent \textbf{Implementation details.}
Our implementation is based on `torch-npg'~\cite{torch-ngp} and Pytorch~\cite{pytorch}, with Adam optimizer~\cite{kingma2014adam} and `GradNorm' algorithm~\cite{chen2018gradnorm} to adjust loss weights during the optimization. We used InstantNGP~\cite{muller2022instant} and D-NeRF~\cite{pumarola2021d} for neural rendering, RAFT~\cite{teed2020raft} for the optical flow estimation and LitePose~\cite{wang2022lite} to infer the joint heatmaps. Cameras are optimized two by two, and linked with a sliding window.
For more details see Suppl. material. 

\noindent \textbf{Datasets.} 
We test on multiple heterogeneous datasets to show the effectiveness and generality of our proposed method: including Blender-made dynamic dataset from D-NeRF~\cite{pumarola2021d}, multi-view light-field-like video dataset from~\cite{li2022neural,zhang2021stnerf}, mobile phone recorded video from nerfstudio~\cite{nerfstudio}, and our Unity synthesized animation renderings.

\subsection{Qualitative results}

\begin{figure*}
  \includegraphics[width=\textwidth]{fig/gallery}
  \caption{Examples of movies to NeRF transfers (\textit{frames}), and below the evolution along clips of: time (\textit{red}), focal length (\textit{blue}) and x-translation (\textit{brown}), y-translation (\textit{purple}), z-translation (\textit{green}) and yaw (\textit{grey}). 
  (a) a dolly-zoom in \textit{Jaws}; (b) an arc in \textit{House of Cards}; (c) handheld pull-out in \textit{Inception}; and (d) a push-in in \textit{The Matrix Reloaded}. x is along the width, y along the height, and z along depth.}
  \label{fig:gallery}
\end{figure*}

\noindent \textbf{Movies to NeRF.} Fig.~\ref{fig:gallery} shows examples of cinematic transfers from a classic shots to synthesized clips. Reference keyframes are sampled for optimizing the proposed cinematic parameters, interpolation is then applied on calculated parameters for high fps of final rendering.

\noindent In Fig.~\ref{fig:gallery}: (a) highlights the ability of our pipeline to search for the optimal focal length in addition to the camera pose: making it possible to reproduce the well-known \textit{Vertigo} effect, i.e. pushing forward and zooming out. With the parameters evolution (below \textit{frames}), we show that the focal length (\textit{blue}) and the z-translation (\textit{green}) tend to compensate each other, realizing the visual effect of ``retreating background and forwarding foreground"; (b) shows that JAWS can also reproduce complex arc motion, composed of yaw (z-rotation, \textit{grey}) and y-translation (\textit{purple}) at the same time; (c) demonstrates the aspect to mimic subtle camera motions, such as handheld movements. Evolution of x- and z-translation (\textit{brown} and \textit{green}) present the shaky effect with irregular progressions; (d) illustrates that our pipeline can align the temporal axis to fit synthesized content with the reference. The timing parameter (\textit{red}) increases with the z-translation (\textit{green}), s.t. both characters approach to each other while the camera is forwarding, to present similar framing to the reference. In addition to the shown examples, more demos are available in the Suppl. material.

% ##################################################################################

\begin{figure}
  \includegraphics[width=0.47\textwidth]{fig/unity.png}
  \caption{we show that the neural rendered results share high \textit{look-and-feel} quality to the original reference (see (a)), and the exported trajectory (b) can be easily manipulated by artists.}
  \label{fig:unity}
\end{figure}
\noindent \textbf{Movies to 3D engine.} We propose a possible application of our method on animation workflow: using JAWS to generate referenced camera trajectories to guide the animation shooting in graphics engine \eg Unity3D. It consist of: (i) training a NeRF by multi-view images rendered by Unity3D; (ii) generate desired cinematic transfer trajectory from JAWS according to a reference clip; (iii) re-apply the generated trajectory into Unity3D for rendering high quality animation. The advantage of combining two systems is to bridge the indifferentiability in graphics engine and lower animation quality in dynamic NeRF. See Fig.~\ref{fig:unity} for the rendered results presenting high visual similarity with the original reference, and the exported trajectory can be easily used by artists as a reasonable starting point of their workflow.


\subsection{Ablation study}

\begin{figure}
 \begin{center}
  \includegraphics[width=0.47\textwidth]{fig/ablation.png}
  \caption{Scenes tested during the ablation study.}
  \label{fig:ablation_loss}
 \end{center}
 \vspace{-1cm}
\end{figure}

\input{tab/loss_ablation}

\noindent \textbf{Loss ablation.} In order to demonstrate the effectiveness of each component and design of our system, we mainly test three aspects of our proposed method: 

\noindent (i) a video copying task is first undertaken, consists in transferring a NeRF rendered reference video (20 keyframes) which the trajectory is known, under the same and different scenes (see Fig.~\ref{fig:ablation_loss}), with same/close and different camera initialization positions. The objective is to demonstrate the robustness and characteristics of our two complementary cinematic losses across different scenes and influenced by different level of perturbations (\ie. initial position).

\noindent (ii) we carry out an experiment for retrieving correct timing and focal length information from images rendered with known parameters respectively. This experiment is to prove the ability of recovering these parameters by inverted optimization method on dynamic NeRF networks.

\noindent (iii) we investigate the influence of the guidance map by collecting the motion performance and memory usage of different sampling number (\ie number of pixels with gradient) to demonstrate that the guidance can help the convergence and mitigate the memory usage simultaneously.

Tab.~\ref{tab:loss_ablations} reports the ablation of losses for different setups: (i) same/different target scenes to the reference to emphasize the cross-domain transfer ability; (ii) same/different initial positions to highlight the robustness. Three metrics are computed: Absolute Trajectory Error (RMSE-ATE) reflects the quality of retrieved motion on $SE(3)$ similar to many tracking works~\cite{iMap,zhu2022nice}. To depict the on-screen composition similarity, we measure Pixel Error (PE) (for the same scene) and average Joint Error (JE) in pixel computed by Litepose (with a more accurate model).

We study several losses combinations: iNeRF~\cite{yen2021inerf}, \ie pixel loss with keypoint-driven sampling (w/o guidance map); pixel loss~\footnote{\textit{pixel loss} uses the guidance map to exploit gradients only for selected pixels but the loss is computed with \textit{all} pixels, whereas \textit{iNeRF} computes loss on selected ones}, pose and flow loss followed by combination of flow and pose losses (\textit{flow+pose} in Tab.~\ref{tab:loss_ablations}).

According to Tab.~\ref{tab:loss_ablations}, we can observe:
(i) iNeRF and pixel losses behave similarly: they both show good performance and robustness against perturbation for the same scene on motion (ATE) and composition qualities (PE, JE). However pixel-based methods fail frequently (\ie camera moving to non-defined area of the NeRF and yielding numerical error) under different scenes. For the barely succeeded experiments, the performances are low due to the misled pixel information by mismatched appearance from different scenes. (ii) Comparing to pixel-based methods, the others (\textit{pose}, \textit{flow}, \textit{flow+pose}) show invariance against appearance changing. Nevertheless, they all act differently: (a) flow loss tends to drift heavily if the initialization position is far to the correct one (JE and PE). The phenomenon is due to the fact that the flow focus on the inter-frame information and extracts no hint on the compositing; (b) in contrast, pose loss completely ignores the inter-frame motion and causes lower performance on ATE yet relatively better results in PE and JE on the same scene, suggesting possible ambiguities on similar human pose and different camera parameters. Heatmap pose feature also shows robustness against initial perturbation, with a small difference of ATE between the close and the different initializations; (c) by combining the two complementary losses: pose and flow, we achieve overall better performances than pose and flow separately, especially under different scene condition. Yet under the same scene, the performance is lower than pixel level tracking (iNeRF and pixel), reflecting on PE and ATE this is due to less sharpen convergence cone (see Fig.~\ref{fig:loss distribution}) limited resolution on extracted heatmap, and possible ambiguities.

% ##################################################################################

\begin{figure}
  \includegraphics[width=0.47\textwidth]{fig/f-t-ablation.png}
  \caption{Visualization of (i) temporal ablation, and (ii) focal length ablation. We show the initial and final frames of the six-frames clips used for ablation. The right column shows the error distribution on the ablated parameter (time and focal length) using our method (red) and a constant parameter setup (green).}
  \label{fig:f-t-ablation}
\end{figure}

\noindent \textbf{Parameter ablation.} We also undertake ablation for parameters: time $m$ and focal length $\phi$. The experiment is done by applying our method on 6-frames clips, where only time or focal length varies. Each experiment is run 16 times with a random initial value for the studied parameters ($m$ or $\phi$). For each studied parameter, we show in Fig.~\ref{fig:f-t-ablation} the distribution of the error between the prediction and the ground-truth (\textit{red}). We compare our results with the controlled case where the studied parameter remains constant, \ie equals to the random initial value, all along the 6-frames clips (\textit{blue}).

\noindent Fig.~\ref{fig:f-t-ablation} (i) shows the result for the time ablation, where only the arms of the mutant waving and Fig.~\ref{fig:f-t-ablation} (ii) shows the result for the focal length ablation, camera pose is fixed but the focal length varies. For both examples, our prediction error (\textit{red}) is smaller than the controlled one (\textit{blue}). Some large error values for our method are due to extreme initial points, making the convergence harder. Since we are optimizing \textit{all} cinematic parameters, some effects may have ambiguities with other parameters than the studied one. 

% ##################################################################################

\input{tab/guidance_ablation}
\noindent \textbf{Guidance ablation.} Tab.~\ref{tab:guidance_ablations} shows the influence of the guidance map on the performance and memory usage. The experiment is done in similar methodology to the Tab.~\ref{tab:loss_ablations} under the same scene with different initial position (see more in Suppl. Material). Low ATEs are reported when the sampling number is insufficient. An experimental optimum is around $4000$ as we used for our method. We notice higher std and memory usage when the sampling number keeps increasing. The performance re-improves when including the gradient of \textit{all} pixel in the image (66752). The low-yield behavior of higher sampling numbers could be due to the confused the gradient direction by non-informative pixels.