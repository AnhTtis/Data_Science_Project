\noindent \textbf{Neural rendering.} Our work strongly relies on NeRF representations of 3D scenes~\cite{mildenhall2020nerf} which describe a scene through a Multiple Layer Perceptron (MLP) network by inputting different 3D locations and view directions, and inferring color and volume density. Volumetric rendering techniques then enable the reconstruction of an image from any viewpoint by retrieving the color and density of each pixel and integrating on the line direction of its pixel-correspondent rays in the MLP. A NeRF model $\mathcal{N}_\mathbf{\Theta}$ can therefore be viewed as a mapping between a camera pose $\mathbf{T}$ and a reconstructed image $\mathbf{\hat{I}}$, where $\hat{\mathbf{I}}=\mathcal{N}(\mathbf{T})$ by abusing the term and regrouping all the rays into image pixels through a camera projective model: $\mathcal{N}_{\mathbf{\Theta}}: SE(3) \rightarrow \mathbb{R}^{H \times W \times 3} ; \mathbf{T}  \mapsto  \mathcal{N}(\mathbf{T})$.

In the training, parameters $\mathbf{\Theta}$ are updated by minimizing an on-screen image loss (eg. photometric loss) on each ray, which compares pixel-level information of the reconstructed image $\mathcal{N}(\mathbf{T})$ and its associated reference view $\mathbf{I}^*$: 
\begin{equation}
    \hat{\mathbf{\Theta}} = \arg \min_{\mathbf{\Theta}} \mathcal{L}(\mathbf{\Theta} \mid \mathcal{N}(\mathbf{T}), \mathbf{I}^*)
\label{eq: trainNeRF}
\end{equation}
Training a NeRF network can therefore be seen as optimizing MLP parameters  $\mathbf{\Theta}$ from a set of known camera poses and corresponding images. 

\noindent \textbf{Camera pose inference.}
The NeRF training process can also be applied in an inverse manner (see iNeRF~\cite{yen2021inerf}), to estimate a camera pose $\hat{\mathbf{T}} \in SE(3)$ in an already trained NeRF model, against a reference view $\mathbf{I}^{*}$. 
To ensure the estimated pose still lies in the $SE(3)$ manifold, the optimization process is performed in the canonical exponential coordinate system~\cite{ma20043dvision} based on an initial camera pose $\mathbf{T}(0)$:
\begin{equation}
\begin{array}{c}
\hat{\mathbf{T}}(\theta) = e^{[{\xi}]\theta} \ \mathbf{T}(0)\quad
\text{where} \quad e^{[{\xi}]\theta} = \begin{bmatrix}
e^{[{\omega}]\theta} & K(\theta, \omega, v) \\
0 & 1 
\end{bmatrix} \\ \\
\text{with}   \quad
 K(\theta, \omega, v) = \left\{ \begin{array}{cl}
         v\theta & \text{if} \quad \omega = 0 \\
        \frac{(I - e^{[{\omega}]\theta})[{\omega}] v+\omega\omega^Tv\theta}{\lVert \omega \rVert} & \text{otherwise}
    \end{array} \right.
\end{array}
\end{equation}

\noindent where $[{\xi}]$ is a twist matrix, $(\omega, v)$ are twist coordinates, and $\theta$ is a magnitude. The optimization problem is then changed to seek for the optimal camera parameters $(\hat{\theta}_k, \hat{\omega}_k, \hat{v}_k)$. 