\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{fig/main}
  \caption{Overview of JAWS pipeline. Given the cinematic parameters (camera motion, focal length and timing parameters) to optimize $(\hat{\mathcal{T}}, \hat{\Phi}, \hat{\mathcal{M}})$, we first synthesize views through a fixed  NeRF, then compute joint heatmaps and optical flows from the rendered (\textit{purple}) and reference (\textit{green}) views, via pose and flow estimators respectively. A guidance map (\textit{black}) is calculated and helps the sampling of pixel with gradient s.t. cinematic parameters can be updated through a backpropagation tracing back to all proxy and NeRF networks.}
  \label{fig:main}
  \vspace{-0.3cm}
\end{figure*}

\subsection{Cinematic transfer}

Inspired by the photorealistic recording capability of neural rendering methods (\eg NeRF) and the related camera pose estimation methods such as iNeRF, in this paper we explore the possibility of proposing a NeRF-based \textit{Cinematic Motion Transfer} from in-the-wild references, following the \textit{watch-and-learn} paradigm explained in Sec.~\ref{sec:introduction}. Such a problem consists in extracting cinematic information from a reference clip and reapplying it in another scene s.t. the rendered visual content shares high cinematic similarity and effects. We propose our method, JAWS, displayed in Fig.~\ref{fig:main}, to address the \textit{Cinematic Transfer} goal in a reference agnostic NeRF environment. The main idea is to optimize multiple cameras' cinematic parameters through the design of robust cinematic losses in a differentiable framework. To improve the performance of this optimization process, we extended our work with multiple techniques. 

\label{subsubsec: formulation of the problem}
\noindent \textbf{Formulation of the problem.} Similar to the formulation of Eq.~\ref{eq: trainNeRF}, we describe our problem as an inverted optimization. However, contrasting with iNeRF~\cite{yen2021inerf} which only optimizes a single camera pose and photometric loss, we target multiple cameras' cinematic parameters using two different, yet complementary, losses to ensure the transfer of key cinematic characteristics to synthesized clip. These robust cinematic losses are: (i) an \textit{on-screen} loss $\mathcal{L}_{s}$, which guides the framing and position of significant elements; and (ii) an \textit{inter-frame} loss $\mathcal{L}_{i}$, guiding the correct overall motion along the sequence.

We therefore re-formulate the problem as an inverted optimization of 
$N$ cameras $\mathcal{T}=\{\mathbf{T_i}\}_N$ w.r.t. robust cinematic losses $\mathcal{L} = \mathcal{L}_{s}+\mathcal{L}_{i}$ between a set of synthesized views $\mathcal{N}(\mathcal{T})$ and reference film clip images $\mathcal{I}_{r}$:

\begin{equation}
\begin{array}{c}
    \hat{\mathcal{T}} = \arg \underset{\mathcal{T}}\min \: \mathcal{L}(\mathcal{N}(\mathcal{T}) \mid \mathcal{I}_{r}, \mathbf{\Theta})
\end{array}
\label{eq:target_formular}
\end{equation}

The definition of cinematic camera parameters will be discussed in Sec.~\ref{subsubsection: Cinematic parameters}, and details of our proposed cinematic losses will be given in Sec.~\ref{subsection: Cinematic_loss}. We also propose specific enhancement techniques to further improve the efficiency of our system in Sec.~\ref{framework_enhancements}.

\label{subsubsection: Cinematic parameters}
\noindent \textbf{Cinematic parameters.} Film shooting cameras are usually more complex than relying on a $SE(3)$ camera model: variable focal length is often employed to realize many creative film effects, such as \textit{whip zoom} (rapid zooming to close up on actors facial area to express surprise and draw sudden focus) or \textit{dolly-zoom} (translating the camera in compensated direction to suggest strong emotional impact). 

In addition to camera parameters, another factor we need to take into consideration is that film shooting always comprises the dynamic nature of actors and scene props, which are rarely discussed in most NeRF-based pose estimation works~\cite{yen2021inerf,avraham2022nerfels,zhu2022nice}. They only adopt static environmental hypothesis.
It therefore appears mandatory to include extra timing parameter $m$ representing the temporal condition in dynamic scenes, and focal length $\phi$ into our cinematic parameters with 6DoF camera pose, in order to address properly the cinematic motion transfer problem. Fortunately, thanks to the recent progress in extending static NeRF to the dynamic scene with the help of temporal deformation representation~\cite{pumarola2021d,xian2021space,tretschk2021non}, dynamic NeRF methods can render unseen views at a specific instant (\ie timing parameter $m$) for animated scenes or objects. One of our main intuitions behind trying to handle dynamic scenes is to rely on another MLP to encode warp information of rays such that the displacement of pixels can be retrieved from a canonical space (\ie classic static NeRF methods). With this differentiable temporal MLP, we can follow the same optimization idea to further manipulate the timing of characters dynamic motion or scenes changing in the neural rendering system.

We therefore include intrinsic parameters, especially the focal length for each camera $\Phi = \{\phi_i\}_N$, and the timing moments, \ie temporal parameters $\mathcal{M} = \{m_i\}_N$ for dynamic scenes. We then transform the Eq.~\ref{eq:target_formular} into the following by relying our cinematic camera parameters:
\begin{equation}
\begin{array}{c}
    \hat{\mathcal{T}}, \hat{\Phi}, \hat{\mathcal{M}} = \arg \underset{{\mathcal{T}, \Phi, \mathcal{M}}}\min \: \mathcal{L}(\mathcal{N}(\mathcal{T}, \Phi, \mathcal{M}) \mid \mathcal{I}_r, \mathbf{\Theta})
\end{array}
\label{eq:target_formular2}
\end{equation}

\subsection{Cinematic losses}
\label{subsection: Cinematic_loss}

Two challenges hinder the understanding and transferring of cinematic motion from reference clips to a different scene: (i) the difference of content in terms of the relative scale, mismatched appearance and even dissimilar dynamic motion of characters; (ii) the necessity to describe and compare the motions of the cameras between each other s.t. generated sequence shares high visual dynamic similarity.

As described in Sec.~\ref{subsubsec: formulation of the problem}, we propose a combination of two different, yet complementary, optimization criteria, which we term as \textit{on-screen} and \textit{inter-frame} cinematic losses $\mathcal{L} = \mathcal{L}_{s}+\mathcal{L}_{i}$.
Intuitively, they cover \textit{framing} and \textit{camera motion} aspects respectively. 

\noindent \textbf{On-screen loss.} Our on-screen loss collects and transfers the on-screen information to ensure framing consistency. To this end, we decide to anchor the framing to the one of most significant entities on screen, human characters, like many previous works~\cite{Huang_2019_CVPR,bonatti2020autonomous,jiang20sig}. Exploiting character on-screen poses benefits from: (i) its sparse and robust information in comparison to pixel-wise criterion, and (ii) being descriptive enough to match corresponding key-regions between two views from different scenes. 
By contrast with similar tasks~\cite{jiang20sig, Huang_2019_CVPR}, which only extract the final human pose keypoints, in our work we exploit the detection confidence heatmaps predicted by a deep neural network. On the contrary to singular keypoints resulting from non-differentiable operations, those heatmaps are differentiable and contain richer information.

To compute a differentiable distance between two heatmaps, we use Wasserstein distance~\cite{kantorovich1960mathematical} (w-distance for short). 
The choice of w-distance aims to highlight the on-screen spatial relation (similar to on-image Euclidean joints distance) rather than MSE-like measures emphasizing on intensity difference (see more in~\cite{arjovsky2017wasserstein}). By minimizing the w-distance between two character pose heatmaps, we are able to transfer the framing information from a target reference to a differentiable rendering space. 

Lets consider a confidence heatmap $\mathbf{H} \in (\mathbb{R^+})^{H \times W \times J}$ with $J$ being joint number on human skeleton (see Fig.~\ref{fig:main}) generated from a pose estimation network, where each channel represents a detection probability of a given joint in the image domain. To calculate the character pose loss $\mathcal{L}_{\text{pose}}$, we compute w-distance $d_w$ between confidence maps of a reference $\mathbf{H}^*$ and a synthetic view $\hat{\mathbf{H}}$ on each channel.
\begin{equation}
\begin{array}{c}
    \mathcal{L}_{\text{pose}} = \sum\limits_{i=1}^{J}d_w(\mathbf{H}^*_i, \hat{\mathbf{H}_i}) + ||S(\mathbf{H}^*)-S(\hat{\mathbf{H}})||\\[5pt]
    \text{where} \quad S(\mathbf{H}) = \sum\limits_{i=1}^J\sum\limits_{j=1}^J d_w(\mathbf{H}_i, \mathbf{H}_j)
\end{array}
\end{equation}
The regularization $S$ is defined as an inter-joint matrix measured in w-distance of each joint $\mathbf{H_j}$ to all the others from the \textit{same} heatmap $\mathbf{H}$. This term assures the inter-joint shape similarity between heatmaps while optimizing the framing.

\begin{figure}
\begin{center}
  \includegraphics[width=0.47\textwidth]{fig/loss_distribution.png}
  \caption{Normalized photometric loss \emph{vs.} our pose loss under different camera position perturbations in the horizontal and vertical directions (x and y axis in col (b,c)) in the NeRF scene. First row shows loss convergence when applying the reference image to the same scene; and the second row shows results when testing the same reference under a different scene.
  }
  \label{fig:loss distribution}
 \end{center}
 \vspace{-0.7cm}
\end{figure}

Fig.~\ref{fig:loss distribution} illustrates the loss distribution when perturbing the camera pose around a reference position on $x$ and $y$ translational directions ($\Delta X, \Delta Y$), within same or different scene against the reference. We compare our proposed character pose loss to the photometric loss, which is commonly used in inverted NeRF methods~\cite{yen2021inerf,zhu2022nice}. Results demonstrate that the convergence cone of our loss preserves invariant while measuring the reference to a different content and provides correct human-centered framing information.

\noindent \textbf{Inter-frame loss.} On the other side, we present another criterion that depicts the relative camera motion performance along the sequence dimension of synthesized views.

In order to incorporate camera motion, we propose to extract the optical flow between two consecutive frames for estimating the similarity between reference and synthesized views. It takes advantages of (i) describing the motion within frames: enabling to infer indirectly the camera trajectory; and (ii) being agnostic towards frame content: allowing to compare views from different scenes.
 
In this work, we choose a differentiable deep neural network generated optical flow (see Fig.~\ref{fig:main}) for the purpose of back-propagating to the designed cinematic parameters through the neural rendering framework. To calculate the flow loss $\mathcal{L}_{flow}$, we compute the endpoint distance~\cite{baker2011database} between the reference $\mathbf{O}^*$ and synthetic $\hat{\mathbf{O}}$ sequences of flows:

\begin{equation}
    \mathcal{L}_{\text{flow}} =  \lVert \mathbf{O}^* - \hat{\mathbf{O}}\rVert_2
\end{equation}

\noindent \textbf{Optimization loss.} The final loss for the optimization problem $\mathcal{L}_{\text{total}}$ is a linear combination of the pose and flow loss, corresponding to the \textit{on-screen} and \textit{inter-frame} loss termed in Eq.~\ref{eq:target_formular2} respectively: $\mathcal{L}_{\text{total}} = \alpha \mathcal{L}_{\text{pose}} + \beta \mathcal{L}_{\text{flow}}$.

We adjust $\alpha$ and $\beta$ during the optimization to achieve robust and dynamic emphasis on the two aspects separately.

% #########################################

\subsection{Framework enhancements}
\label{framework_enhancements}
\noindent \textbf{Sampling strategy.}
Using pre-trained proxy networks to achieve high-level information is popular when solving complex computer vision tasks. The differentiable nature of neural networks allows the loss to be backpropagated to the upperstream networks such that high-level constraints can influence the whole system. 
Similarly, we design our on-screen and inter-frame losses, s.t. they are proxied by human pose~\cite{wang2022lite} and optical flow~\cite{teed2020raft} estimation networks. 

However, two main factors hinder the direct connection of proxy networks to the NeRF-based methods: (i) backfire happens on the memory usage and computational complexity since the images from NeRF methods are composed in ray-pixel elements, each pixel links to all MLP parameters when back-propagating, yielding amplified memory according to pixel number;
(ii) similar to avoiding textureless regions in computer vision tasks, our proposed loss is not uniformly distributed across the whole image field (especially the character pose loss). The divergence can be triggered by non-informative image regions.

Many NeRF-based camera estimation papers~\cite{yen2021inerf,zhu2022nice} rely on sampling skills to accelerate the computation, lighten the memory and focus on relevant regions.
Unfortunately, similar experiences can not be directly transplanted to our problem, since proxy networks expect \textit{entire} input image rather than sparsely sampled pixels.

We therefore come up with our guidance map skill to address this problem: instead of keeping only sampled pixels, we detached the gradient w.r.t. a probabilistic guidance map to achieve: (i) lighter memory usage and fast computation; (ii) attention-like mechanism to help convergence focus on informative regions. More specifically, objective parameters are updated by referring gradients backpropagated from only a Gaussian sampled small portion of pixels according to the intensity from guidance map during the optimization, whereas \textit{all} pixels contribute during the inference of proxy networks. In contrast to iNeRF~\cite{yen2021inerf} using keypoint-driven regions on color image, our guidance map $\textbf{G}$ (Fig.~\ref{fig:guidance}) is computed as united differential map between the generated human pose heatmaps and optical flows in order to highlight the key-regions contributing to the gradient:
\begin{equation}
\begin{array}{c}
    \mathbf{G} = f_{n}(|\mathbf{O}^*-\hat{\mathbf{O}}|) + f_{n}(|\mathbf{H}^*-\hat{\mathbf{H}}|)
\end{array}
\end{equation}
where $f_{n}$ is min-max normalization, and $|.|$ the absolute value.

\begin{figure}
 \begin{center}
  \includegraphics[width=0.47\textwidth]{fig/guidance.png}
  \caption{An example of computation of guidance maps (\textit{bottom-right}) for two cameras respectively, via uniting the difference of heatmaps (\textit{left}) and optical flows (\textit{top-right}). High intensities on the guidance maps represent the sampled pixels \textit{with} gradient.}
  \label{fig:guidance}
 \end{center}
\vspace{-0.5cm}
\end{figure}