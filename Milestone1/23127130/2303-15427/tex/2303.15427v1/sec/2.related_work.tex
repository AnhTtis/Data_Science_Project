\noindent \textbf{Neural scene representation.}
Representing a scene efficiently and properly according to different applications, has long remained an open question in the fields of computer vision and graphics. Fueled by the development of deep learning in the past decade, typical scene representation methods such as mesh \cite{zhang2020path,nimier2020radiative}, voxel \cite{lombardi2019neural,jiang2020sdfdiff}, point cloud \cite{li2020end} or light field \cite{shi2020learning,mildenhall2019llff} showed great progress in rendering photorealistic contents. More recently, the NeRF method~\cite{mildenhall2020nerf} has drawn a huge attention from both industry and academia fields. NeRF proposes to learn attributes of light rays from multiple images, thereby encoding scene information in an implicit way using a deep neural network. Subsequent works~\cite{pumarola2021d,xian2021space,tretschk2021non,li2021neural} extended the NeRF-based methods to consider dynamic scenarios, where both temporal and spatial information are encoded by the network, enabling to synthesize images from any viewpoint and at any time.

In addition to the extension to dynamic scenes, many efforts have been made to improve network performance by either accelerating its training \& rendering processes~\cite{garbin2021fastnerf,reiser2021kilonerf,yu2021plenoctrees,muller2022instant}, deterring aliasing~\cite{barron2021mip}, extending bounds~\cite{barron2022mip}, reducing required samples~\cite{chen2021mvsnerf,roessle2022dense}. All contribute to make NeRF-based methods a powerful scene representation approach for cinematic applications. It is noteworthy that NeRF scene representations can also serve as a differential environment to inversely optimize camera poses, as with iNeRF~\cite{yen2021inerf}, where authors retrieve the reference images $SE(3)$ pose in a given NeRF scene description. Others~\cite{zhu2022nice, iMap} propose to extend the iNeRF model by addressing the localization and mapping problems.
By then adding a functional network as a proxy, the whole workflow is capable of tackling more complex computer vision tasks~\cite{mildenhall2019llff,guo2018learning,Gao_2021_CVPR}. NeRF networks can likewise work as a functional scene descriptor~\cite{Mazur:etal:ARXIV2022,li2022nerf}, followed by a predefined proxy network, to produce desired intermediate features for the final task.

\noindent \textbf{Camera control and virtual cinematography.}
\emph{Camera control} is a well established task in robotics that consists in exploiting sensor-acquired information to guide camera motion through an environment under some desired constraints, with an optimization framework. This topic covers a wide range of problems,\eg visual servoing~\cite{marchand2002controlling}, viewpoint computation~\cite{Triggs1995AutomaticCP}, or target tracking~\cite{papanikolopoulos1993visual}. 

These seminal contributions have been largely extended to account for different visual quality metrics and constraints. Problems have also been transposed from real-environments (robotics) to virtual
% -environments 
ones (computer graphics) by addressing virtual camera control problems~\cite{christie2008camera}. 

While most contributions encoded motion properties (speed, jerk, optical flow) as constraints on the degrees of freedom of the camera (or of the camera trajectory path~\cite{huang2016trip}), an increasing number of techniques have been exploiting real data to constrain/guide the camera motion. In~\cite{kurz2010camera}, authors propose to perform camera motion style transfer by first extracting a camera trajectory from a given film clip using structure-from-motion techniques, performing a multi-frequential analysis of the motion, and regenerating the motion from its frequential parameters in a new 3D environment. This however only \emph{replayed} the motion without adapting it the the target scene contents.

Later, through advances in deep learning techniques, contributions have started to train camera motion strategies from datasets of camera motions. Drone cinematography is a good example. 
Techniques such as Imitation Learning (IL) were used, combining the idea of Reinforcement Learning but training a model to learn from expert examples. In~\cite{loquercio2021learning}, the authors designed an IL system with simulated sensor noise to train drones to fly across in-the-wild agnostic scenarios. 

Huang \etal~\cite{Huang_2019_CVPR} exploit optical flow and human poses to guide drone cinematography controls via an IL framework. Targeting similar drone cinematic objectives,~\cite{bonatti2020autonomous} designed a DQN to control four directional actions for achieving tasks such as: obstacles avoidance, target tracking and shooting style application. Recently, Jiang \etal~\cite{jiang20sig} trained a Toric-based feature extractor from human poses in synthetic and real film data for achieving cinematic style control via a pre-trained latent space when the 3D animation is known. Following the similar topic~\cite{jiang21siga} tackled the keyframing problem which allows users to constrain the generated trajectory by a dedicated LSTM framework.

While these approaches do achieve some form of transfer of camera and motion characteristics, these are either dedicated to specific sub-fields (drone cinematography~\cite{bonatti2020autonomous,Huang_2019_CVPR}), or focus on image composition \cite{jiang20sig,jiang21siga} without considering the image motion characteristics.