\section{System Overview}

% \begin{itemize}
% 	\item Key algorithms and modeling decisions in your system; resources used beyond the provided training data; challenging aspects of the task and how your system addresses them. This may require multiple pages and several subsections, and should allow the reader to mostly reimplement your system’s algorithms.
	
% 	\item Use equations and pseudocode if they help convey your original design decisions, as well as explaining them in English. If you are using a widely popular model/algorithm like logistic regression, an LSTM, or stochastic gradient descent, a citation will suffice—you do not need to spell out all the mathematical details.
	
% 	\item Give an example if possible to describe concretely the stages of your algorithm.
	
% 	\item If you have multiple systems/configurations, delineate them clearly.
	
% 	\item This is likely to be the longest section of your paper.
	
% \end{itemize}

\subsection{Rhetorical Role Classification}

Identifying rhetorical roles (RR) in legal documents is a challenging task, which requires machine learning models to accurately classify sentences into predefined RR categories. One of the primary challenges is the variability and complexity of natural language, including variations in sentence structure, word choice, and context. This requires training machine learning models on diverse and large datasets that capture the range of languages used in the real world. 
Another significant challenge is capturing the long-term dependencies between sentences in a sequence, which can be difficult to achieve but is essential to determine the overall meaning and context of the text. 

To solve this problem, we first observe that this task is basically a sequential sentence classification problem. Meanwhile, existing work proposes to solve this problem using encoder-decoder models, where the encoder embeds the sentence-level semantics and considers the context dependency information, and the decoder classifies individual sentences by the contextualized surrounding sentence information. For example, Jin et al.~\cite{jin2018hierarchical} classify medical abstract sentences with a hierarchical sequential labeling network, which is composed of a word encoder, a sentence encoder, and a document encoder. Specifically, the word-level encoder is a bidirectional LSTM that encodes each word in a sentence into a vector12. The sentence-level encoder is another bidirectional LSTM that encodes each sentence vector in a hidden state. The document-level encoder is either an LSTM or a CRF layer that models the dependencies between sentences and outputs a label for each sentence. 

\begin{figure}
\centering
\includegraphics[width=0.48\textwidth]{Figure/RR-arch.pdf}
\caption{Model Architecture of Legal-BEST-HSLN}
\label{fig:Legal-BEST-HSLN}
\vspace{-0.1in}
\end{figure}

We follow the paradigm and propose the Legal-BERT-HSLN model for legal rhetorical role classification. In our design, we applied the HSLN structure from Brack et al.~\cite{brack2022cross} and changed the backbone model to Legal-BERT~\cite{chalkidis2020legal}. The model architecture of Legal-BEST-HSLN is shown in figure~\autoref{fig:Legal-BEST-HSLN}. 
The model first takes as input the sequences of legal word tokens (\{$t_{i,1}, t_{i,2}, ..., t_{i,m}$\}) of sentence $s_{i}$ and Legal-BERT generates the corresponding token embeddings. Next, the token embeddings are further enriched with local context information within the sentence $s_{i}$ by Bi-LSTM and the attention pooling layer as the augmented token embeddings (\{$e_{i,1}, e_{i,2}, ..., e_{i,m}$\}), which is aggregated to generate the embedding of the sentence $e_{s_{i}}$. Since one of the most important features in this task is the inter-sentence dependency, we further enrich the sentence embedding with contextual information from surrounding sentences. The output layer transforms contextualized sentence embeddings into rhetorical role labels via a linear transformation and CRF. We also introduce dropout layers after each layer for regularization.


\subsection{Legal Named Entity Recognition}

In SemEval task 6~\cite{legaleval-2023}, the second subtask is the recognition of legal entities named entities. 
Specifically, the legal documents provide nonexhaustive metadata with 14 legal entities, including petitioner, respondent, court, statute, provision, precedents, etc. 
Identifying these legal entities can be both error prone and labor intensive~\cite{mohit2014named}. 

Although the task of legal NER may seem straightforward, it is in fact a challenging undertaking due to several reasons. 
First, the NER task itself is an unsolved problem~\cite{li2020survey}. 
Moreover, legal documents can be very noisy~\cite{kalamkar2022corpus,kalamkar2022named}. 
Legal texts contain morphological forms (e.g., synonyms, abbreviations, and even misspellings), which means that different legal cases can use different words and phrases to express the same meaning. 
To process the natural language texts, existing approaches use vocabulary-based embeddings~\cite{wang2020static}. 
However, legal documents involve many out-of-vocabulary words~\cite{jin2022symlm}, which further makes the task complex. 

Meanwhile, we have several insights to solve the problems. First, natural language preprocessing, e.g., tokenization, POS tagging, and sentence parsing, can mitigate the noise of legal documents. 
Moreover, the identified POS tags and sentence structure can help determine the legal entities by the nature of human languages. 
Second, compared to static entity representations that assign fixed embeddings to words and entities in the knowledge base, contextualized word representations can generate adaptive semantic embeddings of words and entities, which can be tuned by domain-specific context~\cite{ethayarajh2019contextual}. 
Finally, we observe that state-of-the-art entity-aware representations~\cite{yamada-etal-2020-luke,ri-etal-2022-mluke} can take advantage of both the advantages of contextualized word embeddings and create entity representations based on the rich entity-centric semantics encoded in the corresponding entity embeddings.
Therefore, we combine these insights and propose to identify legal entities with embeddings of legal-sensitive entities.

\begin{figure}
\centering
\includegraphics[width=0.48\textwidth]{Figure/legal-ner.pdf}
\caption{Entity-aware Contextualized Representation}
\label{fig:legal-luke}
\vspace{-0.1in}
\end{figure}

For legal NER, we introduce a legal entity recognition model: Legal-LUKE, based on the bidirectional transformer encoder of LUKE~\cite{yamada-etal-2020-luke}. The LUKE model was pre-trained by predicting both words and entities masked by the \texttt{[MASK]} tokens. As shown in~\autoref{fig:legal-luke}, Legal-LUKE takes as input the sequence of preprocessed words ($w_1, w_2, .., w_N$) and entities ($e_1, e_2, ..., e_M$). The encoder generates the legal-contextualized representations for both words and entities, i.e., ($H_{w_1}, H_{w_2}, .., H_{w_N}$) and ($H_{e_1}, H_{e_2}, .., H_{e_M}$). To compute the word and entity embeddings, three embeddings are added together, which include token embeddings, type embeddings, and position embeddings. Position embeddings are used to associate entity tokens with their corresponding word tokens, where the position of an entity token is determined by the positions of its corresponding word tokens. The entity position embeddings are then added up over these positions.