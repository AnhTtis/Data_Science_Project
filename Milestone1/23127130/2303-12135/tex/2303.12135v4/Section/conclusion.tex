\section{Conclusion}
In this paper, we propose to understand legal documents with context-sensitive language models. For  the sub-task of rhetorical role classification, we design the Legal-BERT-HSLN model, which learns the hierarchical context information to solve the sequential sentence classification problem. For legal named entity recognition, we implemented the Legal-LUKE model which is both contextualized and entity-aware. Our evaluation results reveal the outperformance of our models compared to baselines and we are the top-5 teams for the  rhetorical role classification task on the leaderboard.