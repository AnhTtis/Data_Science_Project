\section{Introduction}

The growing amount of legal cases and documents requires more and more human efforts to process them.~\cite{kalamkar2022corpus,malik2021semantic}
In some countries, such as India, legal cases have accumulated in an incredible number. For example, India has more than 47 million cases pending in the courts~\cite{kalamkar2022corpus}. This has created a need for automated methods to help judges efficiently understand and process relevant and reliable information. In addition, these methods can also help students, legal scholars, and court officials who deal with legal documents daily. One way to assist them is to automatically understand and highlight the key information and context of long legal documents. 
% In 

However, understanding legal documents by machines is not an easy task. 
First, legal documents are often full of technical terminologies, which can span different legal divisions~\cite{kalamkar2022corpus}. 
Furthermore, legal documents can be specific to special cases, such as health~\cite{young2009legal}, IT technology~\cite{lu2017convolutional}, and cyber security~\cite{shackelford2016unpacking,jin2018multi,lu2017game}, which will involve domain-specific words.
In addition, legal documents can be extremely long~\cite{kalamkar2022corpus}, which makes dependency-based techniques and models~\cite{luo2016text}, such as RNN models, fail to extract the context information due to gradient vanishes.
Finally, typos and unstructured documents introduce noises~\cite{kalamkar2022corpus}, which makes automated natural language processing challenging.

Despite these challenges, predicting rhetorical roles and recognizing named entities in legal documents are very useful for automating the processing and understanding of legal documents. Rhetorical role prediction segments texts and structures noisy legal documents into topically and semantically coherent units~\cite{ghosh2019identification}. Named entity recognition helps identify key legal entities in long documents~\cite{nadeau2007survey}, which can not only help judges process cases in a more efficient way, but also benefit the next automation steps. These two tasks can serve as key steps in these methods~\cite{legaleval-2023}.

In this paper, we propose to solve the rhetorical role prediction and named entity recognition problems in the legal document domain with contextualized large-language models. 
We first systematically build models with well-known design choices based on popular pre-trained models (e.g., BERT and XLM-roBERTa)~\cite{qiu2020pre}, then systematically evaluate the performance of different models and identify the key limitations, and eventually propose our legal contextualized models as our final solutions.
For rhetorical role prediction, we model this task as the sequential sentence classification problem and build the Legal-BERT-HSLN model, which considers the comprehensive context semantics in both intra- and inter-sentence levels.
For named entity recognition, we propose to build a legal-LUKE model that is both sensitive to context and entity-aware.
Our evaluation results show that our proposed models are more accurate than baselines, e.g., Legal-LUKE is 15.0\% better than our baseline BERT-CRF in F1 score (more details in \S\ref{sec:ner-results}). Furthermore, we also achieved the top performance on the rhetorical role prediction task leaderboard, i.e., ranked No.5  out of 27 teams and achieved the 0.8343 micro F1 score (see \S\ref{sec:rr-results}).

We briefly summarize
our primary contributions as follows.
\begin{itemize}[noitemsep]
\item We formalize the rhetorical role prediction task as a sequential sentence classification problem and build the Legal-BERT-HSLN framework to model comprehensive sentence semantics. 
\item We construct the legal-LUKE model with contextualized legal-document and entity-aware representations.
\item Our evaluations demonstrate the better performance of our proposed model compared to baselines and achieved promising results on the task leaderboard.
\end{itemize}