\section{Evaluations}

We conduct extensive experiments using the rhetorical role classification and legal named entity recognition tasks. 
We have implemented the Legal-BEST-HSLN model and the Legal-LUKE model based on Pytorch~\cite{paszke2019pytorch}. 

\subsection{Experimental Setup}

The input word sequence is created by inserting the tokens of \texttt{[CLS]} and \texttt{[SEP]} into the original word sequence as the first and last tokens, respectively, unless stated otherwise.
For the input entity sequence of Legal-LUKE, the legal entities from the training set are used to fine-tune the model.
Although we cannot access the test set before the task organizers open the evaluation system, we train and tune our models and baselines based on the training and validation set.

\subsubsection{Baselines for Rhetorical Role Classification}

For subtask A, we set up our experiments by selecting BERT-base and 3 BERT variants with minor modifications as our baselines:
% Subsequently, we conducted 3 BERT variants with minor modifications:
\begin{itemize}[noitemsep]
    \item \textbf{BERT-Base}. For this model, we directly use the \texttt{[CLS]} token embedding as the sentence embedding as the encoder of the classifier.
    \item \textbf{BERT-Mean}. Instead of using the hidden state of the token \texttt{[CLS]} for the classification output from BERT, we tried to use the mean value of 128 lengths of sequences.
    \item \textbf{BERT-Regularization}. In this method, we made a preprocessing work to the training set by regularizing symbols. These procedures include: (a) lowercase, (b) remove @name, (c) isolate and remove punctuation except ‘?’, (d) remove special characters such as columns and semi-columns, (e) remove stop words except ‘not’ and ‘can’, and (f) remove trailing whitespace.
    \item \textbf{BERT-Augmentation}. Each sentence training set is randomly swapped and the entire training size is doubled.
\end{itemize}

\subsubsection{Baselines for Legal Named Entity Recognition}

For subtask B, we select the following 3 models as our baselines:

\begin{itemize}[noitemsep]
    \item \textbf{BERT-CRF}. For this model, we use the BERT model as the encoder and transform the encodings into NER labels with the CRF prediction head.
    \item \textbf{BERT-Span}. BERT-Span~\cite{joshi2020spanbert} is the variation of BERT that uses BERT to train span boundary representations, which are more suitable for entity boundary detection.
    \item \textbf{XLM-roBERTa-CRF}. XLM-roBERTa-CRF  is a combination of XLM-RoBERTa~\cite{conneau2019unsupervised} and CRF. XLM-RoBERTa is a multilingual version of RoBERTa, a transformer model pre-trained on a large corpus in a self-supervised fashion. We used XLM-RoBERTa-large as the encoder model.
    \item \textbf{mLUKE}. mLUKE~\cite{ri2021mluke} is a multilingual extension of LUKE, a pre-trained language model that incorporates entity information from Wikipedia.
\end{itemize}

% \paragraph{Model and Hyperparameters}

\subsubsection{Test Environment} The evaluations are performed on a desktop server, with the Intel Xeon E5-1650 CPU, Ubuntu 18.04 OS, 64 GB memory, 4 TB storage, and 4 NVIDIA GeForce GTX 1080 Ti graphics cards. While training the Legal-LUKE model, we encountered out-of-memory issues, and we switched to another server, which has the Intel Xeon W-2245 CPU, Ubuntu 20.04 OS, 128 GB memory, 2 TB storage, and an NVIDIA RTX A5000 graphics card.

% \subsection{}

\subsection{Results of Rhetorical Role Classification}
\label{sec:rr-results}

The goal of rhetorical role classification is to provide a categorization label for each sentence at the unstructured document level. The number of classes is 13. The training set has 247 documents with a total sentence number of ~30k, validation set has 30 documents with a total sentence number of ~3k. In this section, all evaluations are reported based on the validation set.

\begin{table}[]
\centering
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{lrr}
\toprule
\textbf{Model}               & \textbf{Micro F1 Score} & \textbf{\textbf{Best Epoch}} \\
\midrule
BERT-Base           & 0.631                              & 5                              \\
BERT-Mean           & 0.641                              & 4                              \\
BERT-Regularization & 0.597                              & 4                              \\
BERT-Augmentation   & 0.645                              & 4                             \\

\textbf{Legal-BERT-HSLN}     & \textbf{0.828}                             & \textbf{16}
\\
\bottomrule
\end{tabular}
}
\caption{Rhetorical Role Classification Performance}
\label{tab:rr-summary}
\end{table}

\autoref{tab:rr-summary} shows the summary of micro F1 scores for Legal-BERT-HSLN and the baselines on the validation set, where Legal-BERT-HSLN significantly outperforms all baselines and it takes more epochs to converge, where we believe Legal-BERT-HSLN has the better capacity to understand legal documents. It is worth mentioning that the regularization process decreased the F1 score and suggests that the rhetorical role is sensitive to detailed stop words and external sentence markers.

\begin{figure}
% \vspace{-0.5in}
\centering
\includegraphics[width=0.48\textwidth]{Figure/confusion_matrix.pdf}
% \vspace{-0.7in}
\caption{Confusion Matrix of BERT Baseline Predictions}
\label{fig:baseline-confusion}
% \vspace{-0.1in}
\end{figure}

While our initial design choice was not Legal-BERT-HSLN, we originally trained the baseline models. 
Specifically, our first design choice was to treat all sentences as individual elements where they have no correlation to each other. We set the baseline model with the BERT network backbone with a simple multilayer perceptron. The resulting confusion matrix is shown in \autoref{fig:baseline-confusion} with the micro F1 0.631. 
% From the matrix, we can see that the major contribution of low micro F1 is from PREMABLE class. 
Subsequently, we conducted 3 BERT variants with minor modifications and obtained the performance shown in \autoref{tab:rr-summary}. From the results, we observed the significant performance gap between all the BERT baselines and the state-of-the-art solution~\cite{kalamkar2022corpus}.


\begin{figure}
% \vspace{-0.5in}
\centering
\includegraphics[width=0.4\textwidth]{Figure/sent-len.pdf}
% \vspace{-0.7in}
\caption{(A) Sentence Length and (B) Class Distributions in Rhetorical Role Dataset}
\label{fig:RR-dataset}
\vspace{-0.1in}
\end{figure}

To fully understand the problem, we put an effort into examining the distribution of the legal dataset~\cite{legaleval-2023} in term of sentence length and classes.
In~\autoref{fig:RR-dataset} (A), the sentence lengths overall satisfied Poisson’s distribution but there is a significant noise at <20 levels. These short sentences may influence the accuracy of classification due to a lack of meaningful information. For example, sentences 17928, 17929, and 17930 are "\textit{[326C-E}", "\textit{] 2.}", and "\textit{There may be circumstances where expenditure, even if incurred for obtaining advantage of enduring benefit would not amount to acquisition of asset}", respectively, whose ground-truth labels are all PREMABLE. This might be due to the discontinuous annotation from turning the page or a keyboard input mistake. Therefore, model prediction from the sentence level is likely not an optimal paradigm to proceed. In~\autoref{fig:RR-dataset} (B), we found that there is a significant bias of the labeling in the training set; therefore, we believe that more experiments, such as validating the label bias, can be tested for future research.

\begin{figure}
% \vspace{-0.3in}
\centering
\includegraphics[width=0.48\textwidth]{Figure/learn-rate.pdf}
% \vspace{-0.4in}
\caption{Legal-BERT-HSLN Performance Affected by (A) Maximum Sentence Length and Batch Size and (B) Learning Rate}
\label{fig:hyper-param}
% \vspace{-0.1in}
\end{figure}

Except for the design choices, we also evaluate the effectiveness of our hyper-parameters. 
\autoref{fig:hyper-param} shows the micro F1 score is influenced by the batch size, maximum sentence length, and learning rate, where the best combination we have tested so far is batch size 16, maximum sentence length 32, learning rate $1\times10^{-4}$.
All weighted-F1 scores in \autoref{fig:hyper-param} are calculated from \texttt{sklearn.metrics.f1\_score} function with the \texttt{weighted} option. 
\autoref{fig:Legal-BERT-HSLN-confusion} shows the confusion matrix using this combination of hyperparameters, and it is found that the best Micro F1 score we achieve in the validation set is 0.828.
Although the major error contribution is no longer in the PREMABLE label, the accuracy of the PRE\_NOT\_RELIED label is 0 which is the same as reported by the state-of-the-art solution~\cite{kalamkar2022corpus}. For future experiments, we suggest studying this outlier effect.
\begin{figure}
% \vspace{-0.3in}
\centering
\includegraphics[width=0.48\textwidth]{Figure/confusion_matrix_2.pdf}
% \vspace{-0.4in}
\caption{Confusion Matrix with Legal-BERT-HSLN}
\label{fig:Legal-BERT-HSLN-confusion}
% \vspace{-0.1in}
\end{figure}

\paragraph{Performance on Test Set} After the task organizer released the test, we test Legal-BEST-HSLN and submitted the predictions. Surprisingly, we obtained the micro F1 score of 0.8343 and ranked No. 5 out of 27 teams. Note that the test performance is better than all performance on the validation set. Therefore, we believe that there is a shift in the distribution of the training, validation, and test sets.

\subsection{Results of Legal Named Entity Recognition}
\label{sec:ner-results}

\begin{table}[]
\centering
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{lrr}
\toprule
\textbf{Model}               & \textbf{F1 Score} & \textbf{\textbf{Best Epoch}} \\
\midrule
BERT-CRF           & 0.694                              & 12                              \\
BERT-Span           & 0.712                              & 14                              \\ % 0.79476 precision, 0.75207 recall, 0.94074 accuracy
XLM-roBERTa-CRF & 0.773                       & 21                              \\
mLUKE   & 0.787                              & 12                             \\
\textbf{Legal-LUKE}     & \textbf{0.796}      & \textbf{18}
\\
\bottomrule
\end{tabular}
}
\caption{Overall Performance on Validation Set of Legal Named Entity
Recognition}
\label{tab:ner-summary}
\end{table}

\autoref{tab:ner-summary} presents the overall performance of Legal-LUKE and the baseline models for legal named entity recognition on the validation set.
We observe that Legal-LUKE outperforms all baselines with an up to 14.3\% better micro F1 score. 

\begin{figure}
% \vspace{-0.3in}
\centering
\includegraphics[width=0.4\textwidth]{Figure/ner-baseline.pdf}
\vspace{-0.2in}
\caption{The Best Performance of the Baseline Model XLM-roBERTa-CRF across Training Epochs}
\label{fig:ner-baseline}
% \vspace{-0.1in}
\end{figure}

Moreover, among the baselines, the BERT-Span model is better than the BERT-CRF model where we used the same encoder model but different prediction heads, which demonstrates the benefits of using the span boundary representations for the named entity recognition task. Moreover, the XLM-roBERTa encoder model shows better legal text encoding capacity because the XLM-roBERTa-CRF model and BERT-CRF model share the same decoder module, but XLM-roBERTa-CRF outperforms BERT-CRF with a 11.4\% better F1 score. Note that we gave the same parameters for the baseline models. For example, we consider context information by a natural language preprocessor with a maximum length of 100. To balance train efficiency and performance, we set the batch size per GPU as 16 with a learning rate scheduler whose initial rate is $5.0\times 10^{-6}$ and the AdamW optimizer. Although we set the maximum number of epochs as 10, we observed that all models achieved the best performance at early epochs. For example, \autoref{fig:ner-baseline} shows the best performance of the XLM-roBERTa-CRF baseline model on the validation set in different epochs, which shows the convergence effect as the training epoch increases.

\begin{figure}
% \vspace{-0.3in}
\centering
\includegraphics[width=0.4\textwidth]{Figure/ner-legal-ner.pdf}
\vspace{-0.2in}
\caption{The Best Performance of Legal-LUKE across Training Epochs}
\label{fig:ner-legal-luke}
% \vspace{-0.1in}
\end{figure}

For our Legal-LUKE model, we also use a learning rate scheduler that has a warm-up ratio of 0.06, the same initial rate $5.0\times 10^{-6}$ as baselines, and the AdamW optimizer. While the pre-trained LUKE model is too large for our GPU server, we tune the different batch sizes to trade off between training speed and performance, and eventually we set the batch size as 8. Similar to the baseline models, we also observe the performance convergence (as shown in~\autoref{fig:ner-legal-luke}) though we set the maximum epoch number at 100.

\paragraph{Performance on Test Set} After the task organizer released the test, we evaluated Legal-LUKE on the test set and submitted the predictions. We obtained an F1 score of 0.667 and ranked No. 13 out of 17 teams. Our test F1 score is far lower than the validation F1 score. However, according to the dataset~\cite{kalamkar2022named}, the validation set and test set should have similar distributions as they are from the same group of legal cases. Our hypothesis is that the event organizer adopted a different preprocessor from ours which produces different token indices and label shifts. For example, we observed that the legal documents contain many empty lines and we opted to remove such lines in our processing, which can result in the different token indices in our predictions. We plan to further investigate the root cause upon the release of the test set.