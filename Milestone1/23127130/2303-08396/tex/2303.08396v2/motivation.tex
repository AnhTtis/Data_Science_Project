\section{Code, Memory BW, and Latency Challenges in Cloud Datacenters}
\label{sec:motivation}





Existing benchmarks that are generally used to design processors do not accurately represent the workloads at hyperscalars. As highlighted by several previous works~\cite{ayers2018memory,ayers2019asmdb,kanev2015profiling, sriraman2019softsku, sriraman2020accelerometer},  cloud workloads exhibit fundamentally different IPC, cache miss rates, and other metrics compared to standard benchmarks. These factors make the SPEC and other commercially available benchmarks a bad proxy for studying the performance of server processors in a datacenter.



\begin{figure}
    \centering
        \includegraphics[width=\linewidth]{gen-over-gen.pdf}
    \vspace{-0.6cm}
    \caption{Gen-over-gen IPC change for the 9 workloads. \tracing{Missing Gen1 data corresponds to services that do not run on Gen1.} Years correspond to processor release year. %
    }
    \label{fig:gen-over-gen-ipc}
\end{figure}


Besides standard benchmarks' little resemblance to cloud workloads, we motivate the need to better understand cloud workloads by comparing the IPC change across server generations at \company. We chart the IPC change over the years across nine representative cloud workloads and four server generations running the same software versions. \reffig{fig:gen-over-gen-ipc} shows that the IPC improvement is minimal or even negative across generations, necessitating the need for a deeper understanding of cloud workloads.


To understand the reasons for this inhibited performance growth of server processors running cloud workloads, we will look into each of the three primary reasons for memory system stalls \ignore{from \reffig{fig:emon-trend}}and understand how they affect cloud workloads' performance.


\subsection{Increasing Code Footprint of Datacenter Applications}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{binsize_trend.png}
    \vspace{-0.5cm}
    \caption{Binary Size trend for \web and \insta. \insta binary size increased by 5.6$\times$ over the last four years.}
    \label{fig:codesize-trend}
\end{figure}

Cloud workloads are often frontend bound due to their large code footprint, resulting in high instruction cache miss rates~\cite{ayers2018memory}. The code footprint can be hundreds of MiBs and has been increasing over the years. \reffig{fig:codesize-trend} shows the increase in the binary size for \web and \insta, the two major web services at \company over the years. \insta shows an exponential increase in binary size, a 5.6$\times$ increase over four years. 

In contrast, the I-cache sizes have stayed relatively constant (32KB I\$ per core) across several server generations and have only recently started increasing on some server architectures. \marginpar{for the generations, they haven't; Hao modified}
Likewise, instruction TLB lookups are optimized for the latency-sensitive instruction fetch pipeline, making it harder for them to scale with cloud workloads' rapidly growing code footprints.
Previous work on cloud workload has observed a similar code-footprint trend. Kanev et al.~\cite{kanev2015profiling} show that the code footprint at Google is growing at a rate of 27\% per year. This imbalance between the processor's micro-architectural resources and the growing code footprint have resulted in frontend to be the leading source of bottleneck for many workloads across several server generations and resulted in stagnant IPC over time. 





\subsection{Memory Bandwidth Scaling Challenge}\label{sec:mem-bw-motivation}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{mem-bw-fleet.pdf}
    \caption{
    Memory bandwidth utilization (\%) CDF across three generations (1-min avg); BW bound and sensitive regions based on DDR memory bandwidth vs. latency characteristics.
    }
    \label{fig:fleet-mem-bw}
\end{figure}

Memory bandwidth is another major concern for cloud workloads, resulting from poor DRAM bandwidth scaling and increasing CPU core count. To understand cloud workloads' memory bandwidth utilization trend, we show the fleet-wide memory bandwidth consumption increase over the three generations of servers in \reffig{fig:fleet-mem-bw}. With the most recent generation (Gen 5), the 1-minute average memory bandwidth utilization shows that the majority of the fleet is either bandwidth sensitive or bandwidth bound. 

While mirco-benchmarks can generally drive memory bandwidth utilization to >80\%, we observe that production workloads rarely exceed 60\% memory bandwidth utilization, as any further increase results in an exponential increase in memory latency~\cite{radulovic2015another}. Thus, we classify workloads with higher than 60\% bandwidth utilization as memory bandwidth bound (shaded red in \reffig{fig:fleet-mem-bw}). Likewise, workloads with average memory bandwidth utilization between 40\%-60\% can have high transient memory bandwidth utilization and are thus classified as memory BW sensitive. While the fleet-wide data is 1-minute average, transient peaks can cause these workloads to become significantly bandwidth bound during shorter time intervals, causing tail latency spikes and limited overall system CPU utilization. 



Besides growing memory bandwidth utilization across the fleet, increasing disparity between DIMM's bandwidth and capacity~\cite{maruf2022tpp} makes alleviating memory bandwidth problems of cloud workloads harder as hyperscalars need to provision a large number of memory channels and DIMMs per server for sufficient memory bandwidth. 

The trend of increasing memory bandwidth utilization and the poor bandwidth scaling of DDR-based memories call for more efficient memory provisioning strategies in cloud datacenters to reduce server costs and power consumption~\cite{dayarathna2015data}.





\subsection{High Memory Latency Leading to CPU Underutilization}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{cpu-vs-latency.pdf}
    \caption{
    Relative memory latency vs. max CPU utilization for microservices; red markers indicate latency SLO constraints limiting CPU utilization; marker size represents service size; latency relative to peak load.}
    \label{fig:latency-vs-cpu-utilization}
        \vspace{-0.15cm}
\end{figure}


Finally, many cloud workloads are memory latency sensitive as they must meet service level objectives (SLOs) for request latencies~\cite{ding2019characterizing}. These workloads run at the maximum CPU utilization possible without violating SLO guarantees, often wasting CPU resources. \hl{Memory latency limits CPU utilization as an increase in memory latency lowers the system's IPC. Lower IPC for workloads results in cloud workloads spending more cycles doing ``on-cpu'' work, and thus results in longer query response times.}

To study this trend across the fleet, \reffig{fig:latency-vs-cpu-utilization} shows \tracing{a scatter plot with} the max CPU utilization vs. memory latency of workload relative to latency at peak load. Peak load is the max throughput a workload can
sustain without violating any SLOs. \hl{We make three important observations from the fleet-wide data: (a) Memory latency increases significantly with increasing CPU utilization. That is, a large portion of the fleet is operating at a relatively high memory latency (>0.5). (b) Workloads marked in red have 20-50\% stranded CPU cores because of the latency SLO constraints.} These workloads must leave CPU cores idle to avoid violating SLO guarantees\ignore{ resulting from high memory latency}, resulting in wasted resources and inefficiency at the datacenter scale. (c) And differences in memory access patterns (e.g., bursty memory load) result in workloads experiencing varying memory latencies at similar CPU utilization levels. %



Hardware prefetching is one of the key techniques used in modern server processors to hide memory latency. L2 hardware prefetchers, however, have a significant memory bandwidth overhead. 
Our analysis in \refsec{sec:mem-lat} reveals that workloads show a substantial increase in total memory bandwidth consumption with L2 prefetchers enabled while providing limited performance gains, signaling that the prefetchers have low efficiency. 


Next, we'll introduce \memprof, a profiling tool to characterize cloud workloads and explore performance improvement opportunities in cloud server processors.



