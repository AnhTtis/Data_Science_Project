\section{Related Works}
\label{sec:related}

Some previous works have analyzed data center workloads code and data behavior
to understand their characteristics. \ignore{These works have looked at the
  breakdown of stalls and made observations related to the growth of the code
  footprint.} To the best of our knowledge, none of the prior works have looked
at code sharing across threads and processes, memory bandwidth distribution of
production workloads, and the impact of hardware prefetchers on memory bandwidth
and performance.

Ayers et al.~\cite{ayers2018memory} analyzed Google's web search workload, observing large code footprints and high MPKI for caches. They proposed a large, low-latency L4 cache. In contrast, we suggest sharing L2 I-Cache and L2 I-TLB among four-core clusters, improving performance with minimal area and power increase.
Kundu et al.~\cite{kundu2003case} also studied
a shared L1 cache design, but using an OLTP benchmark. \chpar{Similarly, Alves
  et al.~\cite{alves2009investigation} studied shared L2 cache (data+code) using
  the NAS benchmark. HPC benchmarks have significantly different performance
  characteristics, so they observe significant performance regression from cache
  sharing. Tam et al.~\cite{tam2007managing} use software-based L2
  cache partitioning to avoid contention between cores which resembles Intel's CAT~\cite{intel-cat}}.


Kanev et al.~\cite{kanev2015profiling} investigated ``datacenter tax'' overhead at Google, identifying I-cache as a significant bottleneck due to growing code footprints in cloud workloads. They observed low memory bandwidth consumption resulting from overprovisioned machines, contrasting our findings that memory bandwidth is increasingly concerning. Nevertheless, \memprof insights can help future hardware prefetcher research, offering possibilities for overall performance improvements.

SoftSKU~\cite{sriraman2019softsku} examines Meta's cloud workloads, understanding OS and hardware behavior using "Soft SKUs" by adjusting core/uncore frequencies, LLC prioritization, and active core counts. They observe similar IPC, cache, and TLB MPKIs. Accelerometer~\cite{sriraman2020accelerometer} studies Meta's cloud workloads, finding most cycles spent on non-core application tasks, such as compression and serialization.

Some earlier
works~\cite{trancoso1997memory,barroso1998memory} have also looked at \uarch{}
improvements for cloud workloads. A few workloads have analyzed the commercial
benchmarks using similar techniques, e.g., PIN Tool~\cite{jaleel2007memory} and
top-down analysis~\cite{yasin2014top}.

\hl{For memory traces, we ensure that they accurately represent production
  behavior which we validate using \uarch{} statistics. Ranganathan et
  al.}~\cite{google-traces} \hl{ collected traces from production using dynamoRIO~\cite{bruening2012transparent}. Our
  approach is complementary to theirs.} 
\tracing{Payer et al.~\cite{payer2013lightweight} leverage 64-bit register sizes while executing 32-bit code to trace memory accesses, implementing a lightweight memory tracing solution. However, 32-bit code has become increasingly uncommon in modern computing. Daptrace~\cite{lee2020lightweight} employs page table access bit sampling to detect hot objects but does not record individual memory accesses. 
HMTT~\cite{bao2008hmtt} uses a dedicated hardware board for DIMM monitoring without application changes. However, HMTT requires specialized hardware and server modifications, often impractical in data centers. In contrast, MemProf avoids hardware changes, ensures minimal intrusiveness, and achieves low performance overhead.}


Some recent works have looked into the memory prefetcher behavior of SPEC and
cloud workloads. Litz et al.~\cite{litz2022crisp} present critical slice
prefetching (CRISP) to prefetch hard-to-predict loads\ignore{ using a new
  instruction prefix that increases the instruction's priority in the
  instruction scheduler}. \chpar{Jamilan et al.~\cite{jamilan2022apt} and
  I-SPY~\cite{khan2020spy} rely on Intel's LBR and dynamic execution information
  to optimize data and instruction prefetching, respectively.}  \ignore{CRISP
  uses a hardware-software technique to identify and tag instructions.} Ayers et
al.~\cite{ayers2020classifying} study SPEC and Google workloads and present an
automated way of classifying memory access patterns for software-based
prefetching. \chpar{Hashemi et al.~\cite{hashemi2018learning} use machine
  learning to study memory traces. \memprof enables low-overhead, accurate traces
  complementing their techniques. These works are orthogonal to ours and can be
  used together to optimize different aspects of cloud workloads.}


