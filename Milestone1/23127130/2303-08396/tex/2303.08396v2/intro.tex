\section{Introduction}
\label{sec:intro}

In recent years, the market share of the cloud as a portion of the total server market has constantly been increasing. Cloud deployments now account for >50\% of the total server processor market and are expected to grow even further~\cite{datacenter0, datacenter1}. However, CPU benchmarks available today \ignore{to optimize these processors}do not accurately represent microarchitectural and memory system behavior of real cloud workloads, leaving the processors largely unoptimized for their unique characteristics.




Cloud workloads show different behavior than the CPU benchmarks available today across several metrics. For one, the fleet-wide\footnote{Server fleet refers to an ensemble of servers across datacenters.} IPC (instructions per cycle) for major hyperscalers is significantly lower than that of the benchmark suites~\cite{sriraman2019softsku,kanev2015profiling}. With millions of servers running these workloads, even small improvements in IPC from understanding the behavior of these workloads can result in significant cost savings and efficiency improvements across the fleet. 


To improve the performance of cloud workloads, in this paper, we study and characterize nine microservices serving live production traffic. These microservices represent a diverse range of workloads at a hyperscalarâ€™s datacenters and run on a significant portion of the fleet.


\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{mem-bound-slots-total.pdf}
    \vspace{-0.15cm}
     \caption{Breakdown of where the CPU pipeline slots spend their time across nine production workloads using top-down analysis~\cite{yasin2014top}.}
    \label{fig:mem-bound}
\end{figure}

We find that the memory system is the primary reason for CPU pipeline stalls across most of these cloud workloads, as shown in \reffig{fig:mem-bound} using the top-down analysis~\cite{yasin2014top}.
These memory system-related stalls can have three sources, (1) stall on an instruction fetch (\textit{code-fetch bound}), (2) stall on the memory system with a high request pending queue occupancy (\textit{memory bandwidth bound}), or (3) stall on the memory system with a low request pending queue occupancy (\textit{memory latency bound}).

To further dive into these three sources of stalls in cloud workloads, we created a new tool, \memprof, that enables detailed profiling of workloads' micro-architectural and memory system behavior. We use observations from \memprof to propose and evaluate new micro-architectural changes and system architecture for cloud server processors.


The first source of stalls, code fetch-related stalls are a significant problem in cloud workloads~\cite{ayers2018memory, sriraman2019softsku}. Using \memprof, we observe that workloads at \company run similar code and share page table mappings across cores. Building on this observation, we propose sharing the L2 Cache and L2 \itlb among a small cluster of  cores (e.g., four) that are physically close. Shared \textmu-arch structures for instructions help reduce code-fetch stalls by enabling the cores running similar code to pool their caches for larger apparent cache size, even though they have the same per-core cache size. 














Memory bandwidth is another significant contributor to memory system stalls. Over the years, DDR memory capacity and bandwidth have been scaling at significantly different rates ~\cite{maruf2022tpp}. This increasing memory capacity and bandwidth disparity, along with growing core counts in modern server processors has resulted in limited per-core memory bandwidth, requiring processors to populate more memory channels, thus resulting in higher server TCO (total cost of ownership).  We use \memprof to study the memory bandwidth distributions of production workloads and find that only a small percentage of pages contribute to most of the memory bandwidth utilization across the workloads. We use this observation to propose and evaluate new \textit{bandwidth-tiered} memory systems as opposed to latency-tiered systems common today. In this new system, a small, high-bandwidth memory tier serves the bandwidth needs of the workload. In contrast, a low-bandwidth, large-capacity tier provides additional memory capacity, lowering servers' TCO. We find that by splitting the memory capacity in 30:70\% between the high-BW and low-BW tiers, we can achieve 1.46$\times$ better throughput and 13\% better throughput/cost than the baseline DDR-only configuration. 




Finally, we look into ways to improve future memory latency research for cloud workloads. 
Using \memprof, we observe that although L2 hardware prefetchers improve memory latency by prefetching cachelines, they often have very low coverage in real-world cloud workloads, resulting in minor IPC improvements. 
Further, we observe that L2 hardware prefetcher's inefficiencies result in high memory bandwidth consumption, exacerbating the memory bandwidth problem.
A major hurdle in optimizing these hardware prefetchers is access to the cloud workloads to study their memory behavior. To enable broader access to cloud workloads' behavior, we \tracing{built an efficient memory tracing tool capable of tracing live production workloads and} collect several memory access traces of live production workloads. \tracing{Our tracing tool has significantly less runtime overhead compared to state-of-the-art, DynamoRIO.} \tracing{We} plan to make the traces available in the future to help  hardware prefetcher research.





This paper is the first to study how cloud workloads interact with the memory system and to use these observations to propose shared \uarch{} structures and memory bandwidth tiering. While previous works from Google~\cite{ayers2018memory, kanev2015profiling} and Meta~\cite{sriraman2019softsku, sriraman2020accelerometer} have investigated the \uarch{} behavior of hyperscale workloads, they have not performed detailed studies of the code behavior, memory bandwidth distribution, or hardware prefetcher efficiency. In summary, this paper makes the following contributions:
\begin{itemize}[leftmargin=15pt, rightmargin=0cm,itemsep=-1pt]
    \item \textbf{Detailed study and characterization of cloud workloads.} We perform a detailed study of code, memory bandwidth, and memory latency trends at \company. 
    \item \textbf{New profiling methodology and tool.} We present \memprof, a profiling tool for cloud workloads that helps understand their interaction with the memory system.
    \item \textbf{Code sharing across cores.} We show that CPU cores run very similar code across all the workloads, enabling performance improvements from shared \uarch{} resources.
    \item \textbf{Memory bandwidth distribution.} We study the memory bandwidth distribution, find that very few pages contribute to a large amount of bandwidth and propose new tiered memory solutions for cloud workloads.
    \item \textbf{Hardware prefetcher efficiency.} L2 hardware prefetchers incur significant memory bandwidth overhead with small performance improvements. To address this, we will make production workload memory traces available to the community for future hardware prefetcher research.
\end{itemize}

\noindent{}In the next section, we look into the three reasons for memory system stalls: code fetch, memory bandwidth, and memory latency. We follow with the description of \memprof{} and use it to make observations and proposals in each area.

\ignore{\noindent{}In the next section, we look into the memory system stalls to understand the contribution of code fetch, memory bandwidth, and memory latency. Next, we present \memprof in \refsec{sec:memprof}, followed by the observations and proposals for improving code fetch-related stalls (\refsec{sec:code-fetch}), memory bandwidth-related stalls (\refsec{sec:mem-bw}), and memory latency-related stalls (\refsec{sec:mem-lat}). Finally, we discuss related works in \refsec{sec:related} and the conclusion in \refsec{sec:conclusion}. }