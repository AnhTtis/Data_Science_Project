\section{Related Works}
\label{sec:related}

Some previous works have analyzed data center workloads code and data behavior to understand their characteristics. \ignore{These works have looked at the breakdown of stalls and made observations related to the growth of the code footprint.} To the best of our knowledge, none of the prior works have looked at  code sharing across threads and processes, memory bandwidth distribution of production workloads, and the impact of hardware prefetchers on memory bandwidth and performance.

Ayers et al.~\cite{ayers2018memory} studied the web search workload at Google and performed a detailed \uarch study to understand where they spent the majority of the cycles. They observe that the search workload has a large code footprint, resulting in high MPKI for the caches and propose using a large, low access-latency L4 cache. %
In contrast, we propose sharing L2 I-Cache and L2 I-TLB among clusters of four cores, thus increasing performance without increasing the area and power cost of the caches. 

Kanev et al.~\cite{kanev2015profiling} perform a large-scale study at Google to understand the overhead of ``datacenter tax''. They make a similar observation that the I-cache is a significant bottleneck for the workloads resulting from the increasing code footprint of cloud workloads. They, however, observe that memory bandwidth consumption is fairly low across the samples because their machines have over-provisioned memory bandwidth. This is in contrast to our observations, where memory bandwidth is a growing source of concern. \ignore{Low memory bandwidth utilization observed by Kanev et al. also results in the workloads being more sensitive to memory latency.} Despite the differences in observations, observations from \memprof are helpful in improving coverage of hardware prefetchers, resulting in possibilities for performance improvement across the board. 

SoftSKU~\cite{sriraman2019softsku} characterizes cloud workloads at Meta to understand their OS and hardware behavior using ``Soft SKUs''. SoftSKUs work by tuning knobs that include core/uncore frequencies, code-data prioritization for LLC, active core counts, and others. They make similar observations about the cloud workload's IPC, cache, and TLB MPKIs. Accelerometer~\cite{sriraman2020accelerometer} studies the cloud workloads at Meta and observes that most cycles are spent performing tasks that are not core application logic (e.g., compression, serialization, etc). %

 \hl{For production traces, we ensure that they accurately represent production behavior which we validate using \uarch{} statistics. Ranganathan et al.}~\cite{google-traces} \hl{also collected traces from production. Our approach is complementary to theirs.}
 Apart from these recent works, some earlier works~\cite{trancoso1997memory,barroso1998memory} have also looked at \uarch{} improvements for cloud workloads. A few workloads have analyzed the commercial benchmarks using similar techniques, e.g., PIN Tool~\cite{jaleel2007memory} and top-down analysis~\cite{yasin2014top}.

Some recent works have looked into the memory prefetcher behavior of SPEC and cloud workloads. Litz et al. \cite{litz2022crisp} present critical slice prefetching (CRISP) to prefetch hard-to-predict loads using a new instruction prefix that increases the instruction's priority in the instruction scheduler. \ignore{CRISP uses a hardware-software technique to identify and tag instructions.} Ayers et al.~\cite{ayers2020classifying} study SPEC and Google workloads and present an automated way of classifying memory access patterns for software-based prefetching.

We proposed a shared L2 Cache and L2 I-TLB using our observations from code sharing of production workloads. Kundu et al.~\cite{kundu2003case} also studied a shared L1 cache design, but using an OLTP benchmark. %

