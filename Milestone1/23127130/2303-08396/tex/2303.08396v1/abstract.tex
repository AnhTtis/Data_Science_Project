\begin{abstract}

Hyperscalars run services across a large fleet of servers, serving billions of users worldwide.
These services, however, behave differently than commonly available benchmark suites\ignore{\ignore{ often used by CPU vendors}--this disparity between commercial benchmarks and production workloads \ignore{for server processors}results}, resulting in server architectures that are not optimized for cloud workloads. With datacenters becoming a primary server processor market, optimizing server processors for cloud workloads by better understanding their \ignore{architectural and micro-architectural} behavior has become crucial. To address this, in this paper, we present \memprof, a memory profiler that profiles the three major reasons for stalls in cloud workloads: code-fetch, memory bandwidth, and memory latency. We use \memprof to understand the behavior of cloud workloads \chtxt{at \company} and propose and evaluate micro-architectural and memory system design improvements that help cloud workloads' performance. 


\memprof's code analysis shows that cloud workloads \chtxt{at \company} execute the same code across CPU cores. Using this, we propose shared micro-architectural structures--a shared L2 \itlb and a shared L2 cache\ignore{ to make the apparent code cache size larger without significantly increasing its area cost}. Next, to help with memory bandwidth stalls, using \ignore{ we measure} workloads' memory bandwidth distribution, we find that only a few pages contribute to most of the system bandwidth. We use this finding to evaluate a new high-bandwidth, small-capacity memory tier and show that it performs  1.46$\times$ better than the current baseline configuration.
Finally, we look into ways to improve  memory latency for cloud workloads. Profiling using \memprof reveals that L2 hardware prefetchers, a common solution to reduce memory latency, \ignore{are meant to help hide memory latency, they} have very low coverage and consume a significant amount of memory bandwidth. To help improve \chtxt{future} hardware prefetcher performance, we built a memory tracing tool to collect and validate production memory access traces. 
\end{abstract}
