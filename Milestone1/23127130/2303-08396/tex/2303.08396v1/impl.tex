
\section{\memprof}\label{sec:memprof}
So far, we have looked at the sources and implications of memory system stalls. To understand how to mitigate these overheads, we will present \memprof, a code and memory profiler, to profile and understand the memory subsystem behavior of cloud workloads.  \memprof is an automated tool that measures several hardware counters, samples events using Intel PEBS~\cite{intel-pebs}, and generates reports regarding code sharing, memory bandwidth distribution, and memory latency behavior. 


\subsection{Design}


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{memprof-overview.pdf}
    \caption{\memprof overview. (a) \memprofcode, (b) \memprofbw, (c) \memproflat profile the code, memory BW, and memory latency behavior, respectively.}
    \label{fig:memprof-overview}
    \vspace{-0.2cm}
\end{figure*}

\reffig{fig:memprof-overview} shows the overview of \memprof. \memprof has three components that measure the three aspects of memory system-related stalls. (1) \textit{\memprofcode} measures the code behavior of a workload to understand code sharing across cores and code bandwidth distribution. (2) \textit{\memprofbw} generates the memory bandwidth distribution to help understand how hot memory pages in a system are. And, (3) \textit{\memproflat} generates data to understand the effectiveness of common techniques in reducing memory latency. %



\subsection{Methodology} 

Next, we will look at the methodology used by \memprof and understand its working.


\paragraph{\memprofcode} For profiling the code behavior of workloads, \memprof{} samples per-core L1 I-TLB miss events (\texttt{FRONTEND\_RETIRED.L1I\_MISS}) using Intel precise event-based sampling (PEBS). Larger granularity sampling using I-TLB miss events enables \memprof to cover wider address space regions. Further, I-TLB-based miss sampling  and dense binary layout of hot code using link-time optimization~\cite{bolt-anonymous} enable low frequency, low-overhead code footprint estimation. 

With the I-TLB miss event samples, \memprof computes the hit distribution across the address space of the workload and uses it to generate the code heatmap, code bandwidth distribution, and compute the correlation between the miss events of two cores (\reffig{fig:memprof-overview}a). 
 For all these calculations, \memprof uses the virtual address of the samples.

\memprofcode's results help understand the code footprint and code sharing across cores in cloud workloads.

\paragraph{\memprofbw} To understand the memory bandwidth requirements of a workload, \memprof samples the precise LLC demand load miss event (\texttt{MEM\_LOAD\_RETIRED.L3\_MISS}) and generates the memory bandwidth distribution profile for a workload (\reffig{fig:memprof-overview}b). 
Since Intel processors report virtual addresses for miss samples, \memprof translates the virtual address (VA) to physical address (PA) using the workload's page table and uses the resulting physical address for computing the memory bandwidth distribution. 
\memprof supports sampling over a configurable interval to study memory bandwidth trends over time and translates the samples' address at the end of the sampling interval. In cases where \ignore{the application creates or deletes mappings frequently, }the page table entries for an address range might not exist at the end of a measurement interval\ignore{. To solve this}, \memprof keeps around the last copy of the page table and uses it to translate any addresses missing in the current copy.

\paragraph{\memproflat} 
To measure how effective hardware prefetchers are in reducing memory latency, \memprof performs two sets of measurements. To compute their accuracy and coverage, \memprof measures the cache and prefetcher-related hardware counters and uses the following expressions. (CL = cachelines, pref. = prefetched)
\vspace{-0.2cm}\newcommand{\mathtok}[1]{\textit{#1}}
\begin{align*}
    &\mathtok{Accuracy} = 1-\frac{(\mathtok{Unused pref. CLs evicted})}{(\mathtok{Total pref. CLs})}\\
    &\mathtok{Coverage} ={}\\
    & \frac{(\mathtok{Total pref. CLs}) - (\mathtok{Unused pref. CLs evicted})}{(\mathtok{Total CLs brought in}) - (\mathtok{Unused pref. CLs evicted})}
\end{align*}

To measure the memory bandwidth overhead of enabling L2 hardware prefetchers, \memprof measures the total system bandwidth consumption and IPC with L2 prefetchers enabled and disabled across all cores. Since these counters are only available for L2 caches, \memprof only generates accuracy and coverage data for L2 prefetchers. 

Collecting accuracy, coverage, and memory bandwidth overhead of L2 hardware prefetchers gives us insights into the behavior of the prefetchers with different workloads.

\subsection{Production Servers and Workloads}


\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{cpl_fb_vs_bb_transparent.pdf}
    \caption{Distribution of workloads across the fleet based on \% frontend and backend stalls. Marker size is scaled by the workload's server count.}
    \label{fig:fb-vs-bb}
\end{figure}


\begin{table} %
\fontsize{8}{10}
\selectfont
  \caption{Gen 4 system configuration.}
  \label{tab:gen4-config}
  \centering
  \rowcolors{2}{gray!25}{white}
  \begin{tabular}{|l|l|}
    \rowcolor{gray!25}
    \hline
    \rowcolor[HTML]{d8d8ff}
    \textbf{Parameter} & \textbf{Value}\\
    \hline
    Cores/Threads & 26/52 \\\hline
    L1 I-/D-Cache               & 32~KiB/core each, private \\\hline
    L2 Cache                     & 1~MiB/core, unified, private                  \\\hline
                &  2~MiB/4~MiB pages fully assoc., 8 entries/thread;  \\
                \rowcolor{gray!25}
    \doublerown{I-L1 TLB} & 4~KiB pages 8-way, 128 entries/core\\\hline
    \rowcolor{white}
    LLC                          & 1.375~MiB/core                       \\\hline
  \end{tabular}
\end{table}


We run \chtxt{containerized} production workloads on bare metal machines
\chtxt{(that is, no hypervisor or virtual machines)} to characterize
microservices in \refsec{sec:memprof}, and for studies in
\refsec{sec:code-fetch} and \ref{sec:mem-bw}. These workloads represents a
large part of the datacenter fleet and exhibit a wide range of
\textmu-architectural behaviors. We run each of these workloads near peak load
operating point. Peak load is determined as the max throughput the workload can
sustain without violating any SLOs.

Further, all experiments were run on the fourth generation of servers, with the
hardware configuration listed in \reftab{tab:gen4-config}.


We use nine representative workloads \chtxt{running live production traffic} to
study the frontend and backend behavior of the fleet.  \reffig{fig:fb-vs-bb}
shows the percentage of CPU pipeline slots across the microservices that are
frontend and backend bound. \hl{Across the fleet, we see that workloads show
  extremely diverse \uarch{} behavior, ranging from highly frontend bound to
  highly backend bound.}

The nine labeled microservices in the scatter plot account for a significant portion of the fleet  and exhibit diverse \uarch{} behavior. We study these nine workloads in our work to perform a representative characterization of cloud workloads \chtxt{at \company}.%



\ignore{\paragraph{\web and \insta.} \web and \insta are the two web-serving workloads. \web uses a hip-hop virtual machine (HHVM)~\cite{adams2014hiphop} to serve web requests. While \insta is a Django-based web server that uses an optimized version of CPython~\cite{cpython} to interpret python bytecode. \insta{}'s optimized CPython implementation has its garbage collector disabled to improve requests' tail latency.

\paragraph{Ads1, Ads2, and Ads3.} \adfinder{}, \adranker{}, and \adretriever{} are the three ad microservices that maintain user-specific and ad-specific data. Together these services find, index, and retrieve ads to show them to the user.

\paragraph{\memcache and \tao.} 
\memcache and \tao are two in-memory caching microservices. \memcache implements a look-aside, while \tao implements a look-through cache. \memcache is bottlenecked by packet processing. To achieve the maximum query rate for \memcache, hosts use 2/3 of the cores for running the workload. The rest of the 1/3 cores are used for network processing and handling NIC IRQs.


\paragraph{Feed}
Feed workload aggregates responses from other microservices into ``stories'', and then characterizes them using various models and feature extractors. These stories are then sent for scoring and ranking to display to the user.

\paragraph{Reader} The reader microservice is part of the AI training pipeline, where it reads the data from the storage cluster and preprocesses it for the AI training cluster~\cite{dppreader-anonymous}.}


These microservices are \web, \insta, \adfinder, \adranker, \adretriever, \memcache, \tao, \feed{}, and \dppreader{}. To achieve the maximum query rate for \memcache, hosts use 2/3 of the cores for running the workload. The rest of the 1/3 cores are used for network processing and handling NIC IRQs. Most microservices spawn multiple threads, where only one thread is assigned to a logical core. In contrast, \insta spawns multiple processes and assigns up to one process per logical core.



\section{Code Behavior of Cloud Workloads}\label{sec:code-fetch}



\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{i-bound-slots.pdf}
    \caption{\% of cycles bound by code fetch (I-cache and I-TLB).}
    \label{fig:i-bound}
\end{figure}


\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{code-bw.pdf}
    \vspace{-0.3cm}
    \caption{Code bandwidth distribution for the nine production workloads using precise L1 \itlb miss events. \chtxt{X-axis represents code pages sorted by their hotness and the Y-axis shows the relative memory bandwidth contribution of the } \chtxt{`X' MiB of hottest pages.}}
    \label{fig:code-bw-dist}
    \vspace{-0.2cm}
\end{figure*}

As we showed in \refsec{sec:motivation}, stalls in the CPU frontend pipeline due to instruction cache and TLB misses are a significant fraction of the memory system  stalls across the nine workloads.%
To mitigate the code-fetch overhead, in this section, we look at the code access behavior of cloud workloads at the full system level and propose \uarch{} optimizations. 

To understand the impact of a large code footprint, \reffig{fig:i-bound} shows the percentage of all CPU cycles where the CPU core was stalled waiting for an \itlb miss or an I-cache miss. \web, \insta, and \tao spend over 20\% of their cycles stalling on instruction fetch.
This code fetch overhead is a direct result of the rapid footprint growth of cloud workload binaries due to their high development velocities, as shown in  \reffig{fig:codesize-trend} for \web and \insta workloads. %
Although, we use huge pages and code layout optimization techniques~\cite{chen2016autofdo,bolt-anonymous} to reduce \itlb and I-cache misses, we still see fairly high frontend stalls. This is partly because application developers do not control huge page usage and code layout for many dynamically linked libraries; and because the code footprint is too large and still growing. 

To help mitigate the code-fetch overhead, using \memprof, we make the key observation that cloud applications run very similar code on different CPU cores, in terms of both the virtual and physical address spaces, and for both multi-threaded and multi-process applications. 



\subsection{Working Set Size for Code}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{code-mpki.pdf}
    \vspace{-0.6cm}
    \caption{L1, L2, and L3 cache code misses per kilo-instruction.}
    \label{fig:code-mpki}
\end{figure}


Next, we will look at the code bandwidth and footprint of the nine workloads to understand why they are significantly code-fetch bound.

Using \memprofcode's L1 \itlb miss event measurements; we generate the code bandwidth of the workloads, as shown in \reffig{fig:code-bw-dist}. We find that all workloads have very large code footprints, with one notable example being the \web workload, which has a code footprint of 125 MiB. 

We further investigate the effects of the large code footprint by measuring the L1-L3 cache code miss rate. \reffig{fig:code-mpki} shows the code MPKI for the three levels of caches. \marginpar{I feel like this is an important observation.}For the nine workloads, we see that the large footprint of cloud workloads results in a high code miss rate across all three levels, with \web and \insta even fetching a significant amount of code lines from  memory.

\hl{Using \reffig{fig:code-bw-dist} and \reffig{fig:code-mpki}, we make three
  important observations: (a) despite the recent server processors trends
  resulting in larger last-level cache (LLC), large (albeit infrequently
  accessed) code footprint of some workloads (\reffig{fig:code-bw-dist}) results
  in diminishing returns in LLC code MPKI improvements. (b) Second, extremely
  large L1 code MPKI values suggest that active code footprint greatly exceeds
  the L1 code sizes. (c) Finally, workloads like \tao{} can show abnormally high
  L1 cache MPKI and code-fetch bound cycles despite a relatively smaller code
  footprint. Investigating further, we found that \tao{} suffers from a high
  branch misprediction rate resulting in it being significantly more code-fetch
  bound.}

\newcommand{\tx}{$\times$}
It is interesting to note that, worklaods show a \raisebox{0.5ex}{\texttildelow}10\tx{}  reduction in code MPKI from L2 to L3, which is much higher than the L1-to-L2 reduction. While considering the capacity, L2 is often 32\tx{} of L1 while per-core L3 is only about 1.35\tx{} of L2. This suggests that code benefit from shared L3 and effectively uses much more capacity than its per-core quota would suggest. %



\subsection{Code-Sharing Behavior for Cloud Workloads}

\begin{figure*}
  \centering \subfloat[L1 \itlb miss event distribution for the address space of
  \web. \chtxt{Each row represents a 2 MiB slice of the address space. Contiguous} \chtxt{
  rows represents a contiguous part of the address space.}]{ \centering
    \includegraphics[width=0.44\linewidth]{web-itlb-miss-heatmap.png}
        \label{fig:itlb-heatmap}
     }
     \hspace{0.8cm}
     \centering
     \subfloat[Hot functions across cores for \web stack sampling.]{
        \centering
        \includegraphics[width=0.42\linewidth]{top-100-funcs.pdf}

        \label{fig:top-100-funcs}
     }

    \caption{Code sharing across cores using \memprof.}
    \label{fig:fleet-ipc}
\end{figure*}



So far, we have looked into the code-fetch overhead of cloud workloads and the resulting performance loss; next, we will present our observation that cloud workloads running across cores share code cachelines, allowing shared cache structures to improve the workload's performance. 

To understand if the cores are accessing identical address locations, using \memprof, we sampled the L1 \itlb miss events for different cores running the same workload. 
\reffig{fig:itlb-heatmap} shows the \itlb miss heatmap for two cores running the \web workload for the first 1.28 GiB of the address space. Across the address space, both cores show very similar code access patterns, suggesting that the cores are accessing common code pages. We test our hypothesis by computing the Pearson correlation coefficient for the two cores; the \web workload shows a correlation value of 0.9997. A Pearson correlation value close to 1 suggests that the L1 \itlb miss events for both cores are highly correlated, and thus, the cores have very similar code access patterns.


{
\renewcommand{\arraystretch}{1.07}

\begin{table}[]
    \fontsize{8}{10}
    \selectfont
    \centering
    \rowcolors{2}{gray!25}{white}
\caption{Pearson correlation between Core0 and Core1 for L1 \itlb miss events.}
    
    \begin{tabular}{|c|ccc|c|}
        \hline
        \rowcolor[HTML]{d8d8ff}
                                   &                             & &                & \\
        \rowcolor[HTML]{d8d8ff}
        \doublerown{\textbf{Workload}} & \doublerown{\textbf{Correr.}\\\textbf{value}}   &  & \doublerown{\textbf{Workload}} & \doublerown{\textbf{Correr.}\\\textbf{value}} \\ \hline%
        \web               & 0.9997                      & & \insta         & 0.9881 \\
        \adfinder          & 0.9977                      & & \adranker      & 0.9921 \\
        \adretriever       & 0.9833                      & & \feed          & 0.9977 \\
            &                             & &      &  \\
        \rowcolor{white}
        \doublerown{\memcache b/w\\workload cores}      & \doublerown{0.9947}    & & \doublerown{\memcache b/w\\NIC cores}  & \doublerown{0.9168} \\
        \rowcolor{gray!25}
            &                             & & & \\
        \doublerown{\memcache b/w\\workload \& NIC}      & \doublerown{0.0010}    & & \doublerown{\tao}& \doublerown{0.9978} \\
        \dppreader & 0.9887 &&&\\
        \hline
    \end{tabular} 
    \label{tab:correlation-coefficient}
    \vspace{-0.15cm}
\end{table}
}


\reftab{tab:correlation-coefficient} shows a similar trend for the other workloads. All nine workloads show a high correlation between cores for L1 \itlb miss events. 

For multithreaded workloads, accessing the same pages implies that the CPU cores access common mappings across threads. For the \insta microservice that spawns multiple processes, we use the page table to verify that code page mappings have the same translation across processes.
For \insta{}, we observe common code mappings across processes as all child processes spawn from the same parent. 






To confirm that cores are executing the same code, we collect stack samples across all cores and check if they execute the same functions. 
\reffig{fig:top-100-funcs} shows the average, p90, and p10 weight of the top 100 functions across all the cores for the \web workload. The L1 I-TLB miss correlation and cores executing identical functions across them confirm that the CPU cores execute identical code, leading to our first key observation.
\obs{obs:shared-code-private-data}{Cores run similar code across them.}



Using this observation about cloud workloads, we will propose two new \uarch{} improvements and evaluate their effectiveness \hl{using the \web{} workload. We chose the \web{} workload as it is significantly frontend bound (\reffig{fig:fb-vs-bb}) and runs across a large portion of the fleet.}

\subsection{Reducing I-Cache Misses Using a Shared L2 I-Cache}
\label{sec:shared-l2-cache}
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{shared-l2-arch.pdf}
    \caption{Shared cache design with unified L2 cache shared among four cores. (a) Private 1 MiB/core L2. (b) Shared, 1 MiB/core shared L2 (total 4 MiB) cache. }
    \label{fig:shared-l2-arch}
\end{figure}

Since the CPU cores share code; with a shared L2 cache, the cores will see an increased apparent cache size for shared cachelines. To take advantage of this, we propose using a shared L2 architecture (\reffig{fig:shared-l2-arch}) where a cluster of four cores shares a unified L2 cache. To keep the hardware cost similar, we propose the per-core L2 cache size stay similar. 
Thus, for a processor resembling the Gen4 platform (\reftab{tab:gen4-config}) should share 4 MiB of L2 cache among four cores (1 MiB/core).

\subsubsection{Evaluation Methodology}
To evaluate the performance improvements from a shared L2 cache, we use Intel's
L2 CAT~\cite{intel-cat} and CDP~\cite{intel-cdp} to measure and project how
\web's performance changes with increasing code cache size.

\chpar{While using simulations can allow us to closely model the shared cache
  design, simulations suffer from several limitations. For example, full-system
  or system-call emulations have significant performance overhead which
  changes application's runtime behavior. While trace-based simulations can have
  lower overhead, they cannot accurately capture the timing characteristics,
  inter-thread dependencies, or RPCs, making them impractical for cloud
  workloads.}


\subsubsection{Results}

\begin{table} %
    \fontsize{8}{10}
    \selectfont
  \caption{Evaluated system configuration for cache scaling.}
  \label{tab:evalsysconfig}
  \centering
  \rowcolors{2}{gray!25}{white}
  \begin{tabular}{|l|l|}
    \rowcolor{gray!25}
    \hline
    \rowcolor[HTML]{d8d8ff}
    \textbf{Parameter} & \textbf{Value}\\
    \hline
    I-L1 Cache                     & 32~KiB/core, private \\\hline
    L2 Cache                     & 2~MiB/core, unified, private                  \\\hline
    LLC                          & 1.875~MiB/core                       \\\hline
  \end{tabular}
\end{table}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{code-scaling.pdf}
    \caption{IPC scaling and projection along with L2 code MPKI change for \web with increasing L2 code cache allocation. %
    }
    \label{fig:web-ipc-projection}
\end{figure}

\reffig{fig:web-ipc-projection} shows the measured and projected IPC along with the L2 code MPKI for the \web workload serving live traffic. We use a machine with the cache configuration listed in \reftab{tab:evalsysconfig} and scale the L2 code partition from 128 KiB to 1.5 MiB while keeping the L2 data partition constant at 0.5 MiB (50\% of Gen4 L2\$, based on our study of code-data split in L2 cache). \ignore{To limit the effects of a larger LLC, we limit the LLC size to prevent interference from other cores.} To project performance beyond 1.5 MiB code partition, we extrapolate the experimental data using linear projection in logspace. We chose this projection to model the cache behavior where the performance improves linearly with an exponential increase in cache size~\cite{alameldeen2006ipc}. 

The \web workload shows a 9.1\% performance improvement when the cache size increases from 0.5 MiB (50\% of private 1 MiB L2 cache) to 2 MiB (50\% of shared 4 MiB L2 cache), that is, 4$\times{}$ increase in apparent code cache size. %

\hl{Thus, based on code sharing across cores and the performance increase from a larger code cache for \web{}, one of the most frontend bound workloads, we observe that cloud workloads benefit from a larger shared L2 cache.}%

\subsection{Reducing \itlb Misses with a Shared L2 \itlb}
\label{sec:shared-itlb}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{shared-itlb-arch.pdf}
    \caption{Shared L2 \itlb design with unified L2 cache shared among 4 cores. (a) Private 1 MiB/core unified L2. (b) Shared, 1 MiB/core Unified L2 (total 4 MiB) cache. }
    \label{fig:shared-itlb-arch}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{tlb-entry-shared.pdf}
    \caption{TLB entry design for (a) private TLB (b) shared TLB.}
    \label{fig:shared-itlb-entry}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{shared-tlb-flowchart.pdf}
    \caption{Page table lookup and update for a shared L2 \itlb{}.}
    \label{fig:shared-tlb-flowchart}
\end{figure}

Similar to a shared L2 cache, we propose a shared L2 \itlb to exploit common page mappings for code across cores for cloud workloads. \reffig{fig:shared-itlb-arch} shows a shared L2 \itlb architecture where a cluster of four cores shares the L2 \itlb, but each core still has a private L2 \dtlb. 
\marginpar{Shared libs example?}A shared L2 \itlb allows the cores running identical code to share the TLB entries, making a larger number of TLB entries available to the workload.

Supporting shared \itlb for multithreaded programs requires no additional changes to the TLB entries. Each TLB entry includes an ASID (\reffig{fig:shared-itlb-entry}a) to track which process the entry belongs to. Since each CPU core is part of the same address space in multithreaded programs, the shared unmodified TLB entries can support multiple cores.

For multiple processes across cores, we propose the shared \itlb entries to include multiple ASIDs (\reffig{fig:shared-itlb-entry}b). For our design, we propose 4 processes to share a single L2 \itlb; thus, each TLB entry only needs 4 ASIDs, one for each core.

\reffig{fig:shared-tlb-flowchart} shows the mechanism for a TLB lookup and to install a new entry if it does not already exist. When a translation request comes into the TLB, if the virtual address (VFN) and the entry's application-specific ID (ASID) match, the corresponding physical address (PFN) is returned. If the entry for the VFN does not exist, or the entry's ASIDs do not match the request, the TLB performs a page table walk and adds a new entry or updates the existing page table entry.




\subsubsection{Evaluation Methodology}\label{sec:itlb-sharing-validation}


\chpar{Next, we will validate whether cloud workloads benefit from larger \itlb
  sizes by changing the L1 \itlb entry count to 2$\times$. We are unable to do
  the similar scaling study as \refsec{sec:shared-l2-cache}\ignore{ here for
    \itlb{}}, because the cache partitioning feature is not available for
  TLBs. Further, as previously stated, \uarch{} simulators cannot capture the
  behavior of production environment making detailed architecture simulation
  impractical.}

\subsubsection{Results}
  \reffig{fig:smt-tlb-mpki}
shows the change in L1 \itlb MPKI when the number of entries is doubled, showing
that workloads benefit from an increased \itlb size. To increase the count of
\itlb entries, we measure per-thread L1 \itlb MPKI with simultaneous
multithreading (SMT) on (1$\times$ entries) and off (2$\times$ entries).  

\marginpar{Need to rephrase this.}\hl{Thus, shared code mappings and an increase in performance with more L1 \itlb{} entries show that cloud workloads benefit from a larger, shared L2 \itlb{}. While a larger \itlb{} results in a lower MPKI, shared TLB can still suffer from trashing over smaller time intervals if the threads sharing the TLB are in different application phases.} %



\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{itlb-mpki.pdf}
    \caption{I-TLB MPKI with 1$\times$ and 2$\times$ L1 \itlb entries for four cloud workloads.}
    \label{fig:smt-tlb-mpki}
    \vspace{-0.3cm}
\end{figure}







\section{Understanding and Improving Memory Bandwidth}\label{sec:mem-bw}
The challenges of memory bandwidth scaling and limited per-DIMM bandwidth have resulted in cloud workloads being more memory bandwidth-bound (\refsec{sec:mem-bw-motivation}). Current server processors only support a single memory tier; thus,  hyperscalars need to populate more DRAM channels to meet the memory bandwidth needs. This results in higher power consumption and wasted memory capacity at the datacenter scale.


Recent memory industry trends have resulted in new memory technologies like HBM and HB-DIMMs that can be directly connected to the CPUs to deliver high bandwidth. These technologies present a potential to explore memory bandwidth tiering as a solution to the bandwidth and capacity scaling challenges. 

In this section, we will study the memory bandwidth characteristics of cloud workloads and look at the potential of high bandwidth memory technologies in improving cloud workloads' performance and server TCO at \chtxt{\company}. %

\subsection{Memory Bandwidth Distribution}
\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{mem-bw.pdf}
    \vspace{-0.3cm}\caption{Memory bandwidth distribution of the workloads over three different measurement intervals.}%
    \label{fig:bw-dist-bar}
\end{figure*}





Using \memprof, we measured the memory bandwidth distribution over 30, 60, and 120 seconds. \reffig{fig:bw-dist-bar} shows the amount of memory contributing to different bandwidth percentiles. Across the nine workloads, we observe that over 30 seconds, the active memory footprint is less than 25\% of the total memory capacity, while the 90\%-tile bandwidth is contributed by less than 10\% of the memory capacity. Moreover, a similar memory bandwidth profile for different measurement intervals suggests that the memory bandwidth distribution remains relatively constant over time, supporting the possibility of memory bandwidth tiering.  %

\memprof, using its LLC demand load miss sampling, enables measuring the relative hit rate of a memory page\ignore{ to compute the memory bandwidth distribution of a workload}. This is unlike current active page tracking techniques, e.g., Kernel Idle Page Tracking (IPT)~\cite{ipt}, which only track the active pages with no visibility into how hot an active page is. Since the system does not know the ``hotness'' of a page, these existing techniques cannot measure the memory bandwidth distribution of a workload.




Using \memprof's memory bandwidth distribution experiments, we make our second key observation of cloud workloads' memory behavior:
\obs{obs:mem-bw-dist}{Only a few pages contribute to most of the memory bandwidth over varying time intervals.}
Using these observations, we will study opportunities for improving system performance without adding significant cost and power overheads.

\subsection{Memory Bandwidth Tiering}
Memory bandwidth tiering enables servers with memory tiers that efficiently serve memory traffic to the high- and low-bandwidth memory pages. To explore opportunities for memory bandwidth tiering, we will look at memory technologies that support considerably higher memory bandwidth than conventional DDR-DRAM, and study their power requirements and cost overheads. 

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{mem-bw-arch-single.pdf}
    \caption{Mem BW tiering architectures for cloud workloads. 
    }
    \label{fig:mem-bw-arch}
\end{figure}

\subsubsection{Opportunities}
Emerging memory technologies like High Bandwidth DIMMs (HB-DIMMs)~\cite{mccall2021high} and High bandwidth memory (HBM)~\cite{standard2013high} enable memory tiers with higher than DDR bandwidth. A higher bandwidth tier in the memory hierarchy enables workloads to meet their bandwidth needs from a small capacity memory while using DDR or CXL-based low-bandwidth, large-capacity tier to meet their memory capacity needs. HB-DIMMs provide up to 2$\times$ the bandwidth of conventional DDR5 memory but are expected to consume up to 2$\times$ the power and cost. On-package HBM~\cite{spr-hbm} shows a similar trend (higher bandwidth at the expense of higher power and cost~\cite{li2018performance}). 

\reffig{fig:mem-bw-arch} shows the possible memory bandwidth tiering architecture for cloud workloads based on our observations of the nine representative workloads. While emerging memories support significantly higher memory bandwidth, this comes at increased power consumption and higher cost, making a single high bandwidth tier impractical.

\subsubsection{Evaluation}
To evaluate the possibility of memory bandwidth tiering in datacenters, we will expand memory bandwidth using an HB-DIMM tier, while using a CXL-based memory tier for memory capacity (\reffig{fig:mem-bw-arch}). Please note that while we evaluate a conservative scenario with a higher memory latency far tier (CXL-based), possible memory bandwidth tiers can differ only in memory bandwidth while having similar memory latencies. For example, high-bandwidth tier using HB-DIMMs, while a low-bandwidth tier using DDR memories.




\begin{table}[] 
    \fontsize{7}{10}
    \selectfont
    \centering
    \caption{Capacity and theoretical peak memory bandwidth for the three memory bandwidth tiering configurations.}
    \rowcolors{2}{gray!25}{white}
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        \rowcolor[HTML]{d8d8ff}
        \textbf{Config.} & \textbf{Near Memory}& \textbf{Far Memory}\\\hline
        \textbf{Baseline} & 100\%, 100 GB/s & - \\
        {\textbf{Ideal}} & 100\%, 200~GB/s \textit{(HB-DIMM like)} & {-} \\
        {\textbf{Tiered}} & 37.5\%, 200~GB/s \textit{(HB-DIMM like)} & 62.5\%, 100 GB/s \textit{(CXL-like)}\\
        \hline
    \end{tabular}
    \label{tab:mem-bw-configs}
\end{table}



{
\def\arraystretch{0.8}%
\setlength{\tabcolsep}{0.2em} %

\begin{table}[] 
    \fontsize{7}{10}
    \selectfont
    \centering
    \caption{Measured relative throughput, memory BW, and relative throughput/cost for the three configurations (\reftab{tab:mem-bw-configs}).}
    \rowcolors{2}{gray!25}{white}
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
        \hline
        \rowcolor[HTML]{d8d8ff}
         &  \textbf{} & \multicolumn{2}{c|}{\textbf{Measured}} &   \multicolumn{3}{c|}{} & \textbf{Relative}\\
        \rowcolor[HTML]{d8d8ff}
         &  \textbf{Relative} & \multicolumn{2}{c|}{\textbf{BW (GiB/s)}} & \multicolumn{3}{c|}{\doublerown{\textbf{Cost (relative)}}} & \textbf{Tput.} \\\cline{3-7}
        \rowcolor[HTML]{d8d8ff}
        \triplerown{\textbf{Config}} &  \textbf{Tput.} & \textbf{\hspace{0.3cm}Near\hspace{0.3cm}} & \textbf{Far} & \textbf{\hspace{0.1cm}Near\hspace{0.1cm}} & \textbf{Far} & \textbf{Total} & {\textbf{/cost}} \\\hline
        \textbf{Baseline}  & 1 & 67.81 & - & 1.0 & - & 1.0 & 1\\
        \textbf{Ideal}  & 1.55 & 108.41 & - & 2.0 & - & 2.0 & 0.73 \\
         &  &  &  & 0.75 & 0.625  &  & \\
         \rowcolor{gray!25}
        \doublerown{\textbf{Tiered}}& \doublerown{1.47} & \doublerown{84.60} & \doublerown{19.22} & \textit{(2$\times$37.5\%)} & \textit{(1$\times$62.5\%)} & \doublerown{1.375} & \doublerown{1.13}\\
        \hline
    \end{tabular}
    \label{tab:mem-bw-results}
\end{table}
}


We use the three configurations listed in \reftab{tab:mem-bw-configs} for our evaluation. The baseline configuration resembles current servers deployed in the datacenters at a hyperscalar. In contrast, the Ideal configuration shows the maximum possible performance improvement using a high-bandwidth-only tier. Finally, the Tiered configuration allocates about 1/3 of the capacity using the high-bandwidth tier that resembles HB-DIMMs, while the rest of the capacity is allocated on far memory that resembles CXL memory. The tiered configuration uses the page placement technique described by Maruf et al.~\cite{maruf2022tpp}.

We use a dual-socket server that uses DDR-5 DIMMs with configurable channel counts to allocate the memory bandwidth. Only one socket has its CPU cores enabled, with the memory connected to this socket referred to as \textit{Near Memory}. The memory connected to the socket with all its CPU cores disabled is referred to as \textit{Far Memory}. HB-DIMMs are expected to have DDR-5 like latencies, so we model HB-DIMMs using DDR-5 based near memory. Likewise, CXL-based memories are expected to have latency and bandwidth characteristics similar to NUMA links between the sockets~\cite{maruf2022tpp} (Intel UPI), \textit{Far Memory} represents a CXL-based memory tier. 

\subsubsection{Results}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{req-rate-time-series.pdf}
    \vspace{-0.6cm}
    \caption{Relative throughput of the \dppreader workload for the three memory configurations (\reftab{tab:mem-bw-configs}).}
    \label{fig:req-rate-time-series}
\end{figure}

\reffig{fig:req-rate-time-series} shows the relative throughput of the \dppreader{} microservice serving live production traffic for the three configurations. At the steady state ($t > 100~\mathrm{s}$), the Tiered memory configuration achieves 1.46$\times$ better throughput than the Baseline and within 6.32\% of the Ideal configuration. Out of the three configurations, the Tiered configuration takes the longest to warm up from the page migration overhead between near and far memories, which the Baseline and Ideal configurations do not have. \hl{Due to the limited and geo-restricted availability of the evaluation server machine, we only evaluate the \dppreader{} microservice, which is the most backend-bound workload out of the nine representative workloads (\reffig{fig:fb-vs-bb}).}

\reftab{tab:mem-bw-results} lists the measured memory bandwidth and relative throughput/cost for the three configurations. As HB-DIMMs are not commercially available, for our TCO analysis, we use an early assessment of HB-DIMM's cost as 2$\times$ that of DDR-5 DIMMs. The tiered memory configuration provides a 13\% improvement in performance/cost over the Baseline, and a 54.8\% improvement over the Ideal configuration, making it the most cost-efficient memory hierarchy, confirming our memory bandwidth observations using \memprof. 




\section{\chtxt{Understanding} Memory Latency}\label{sec:mem-lat}




\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{ipc-mem-bw-l2-pf-tight.pdf}
    \caption{IPC and mem BW change with L2 prefetchers enabled.} %
    \label{fig:ipc-mem-bw-l2-pf}
    \vspace{-0.2cm}
\end{figure}

Memory latency is another significant concern for cloud workloads~\cite{ayers2018memory}, with server processors often relying on hardware prefetchers to alleviate the performance penalty. \AB{Should we also mention that SW prefetching is not a standard practice in our environment for most workloads and generally across cloud workloads because of difficulty in implementing them effectively with large code footprints.} \ignore{\reffig{fig:emon-trend} showed that a large percentage of cycles had at least one outstanding instruction waiting for the memory to respond.} With cloud microservices operating under strict request latency SLO constraints, workloads have to limit CPU utilization to meet the SLO guarantees, resulting in wasted resources, \hl{as discussed in \refsec{sec:mem-bw-motivation}}.

A common solution to hide memory latency, the hardware prefetchers, work by prefetching soon-to-be-used cachelines, thus reducing the overall memory latency. While prefetchers hide some memory latency, they also significantly increase  memory bandwidth consumption. \reffig{fig:ipc-mem-bw-l2-pf} shows the increase in IPC and the corresponding increase in memory bandwidth with L2 prefetchers turned on. Several workloads show a significant increase in memory bandwidth consumption (e.g., for \memcache, it increases by 31\%), signalling prefetcher inefficiencies. \hl{While we present data for L2 prefetchers, from our experiments, we observe that L1 prefetchers have much higher efficiencies; thus, we omit them from our study.}

\hl{Although modern hardware prefetchers monitor bandwidth utilization and throttle to reduce inefficiencies~{\cite{heirman2018near}}, we observe that workloads like \adfinder{} and \dppreader{} still suffer from a large memory bandwidth overhead.}

Next, we will look into reasons for prefetcher inefficiencies and opportunities to mitigate them using production memory access traces.
\subsection{Hardware Prefetcher's Accuracy and Coverage}
\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{hw_pf_accuracy_coverage-tight.pdf}
    \caption{L2 hardware prefetcher accuracy and coverage.}
    \label{fig:hw-pf-accuracy-coverage}
    \vspace{-0.2cm}
\end{figure}

\reffig{fig:hw-pf-accuracy-coverage} shows the accuracy and coverage of L2 hardware prefetchers for the nine workloads. Most workloads show very low coverage (<50\%), but are relatively accurate (>75\%). This tells us that the L2 hardware prefetcher cannot predict a large fraction of access patterns of the workloads.

Using \reffig{fig:hw-pf-accuracy-coverage}, we make several important observations: (a) while some workloads show negligible improvement in IPC, we keep L2 prefetchers enabled as the servers have enough memory bandwidth headroom for the listed workloads. In the past, with limited memory bandwidth, turning L2 prefetchers off has resulted in performance improvements. (b) Workloads with predictable memory access patterns (e.g., CPU-based inference) like \adfinder{} show a significant IPC improvement, suggesting efficient hardware prefetcher are important in improving workload performance. %


While there have been proposals to improve hardware prefetchers~\cite{litz2022crisp,ayers2020classifying}, research on hardware prefetchers for cloud workloads to improve their performance is limited by access to these workloads. 


\subsection{\chtxt{Memory Tracing for Future Architecture Research}}
In this section, we will look into the challenges of tracing memory accesses in a production environment; next, we will present a Pin Tool that can collect memory access traces of production workloads with low overhead. Finally, we will measure these traces' accuracy in replicating cloud workloads' production behavior.

\subsection{Tracing Production Workloads}
Tracing memory accesses of production workloads has several challenges that can affect the accuracy and usefulness of these traces to emulate the memory accesses offline. %



    \textit{Memory tracing overhead.} Common memory tracing tools use dynamic instrumentation to trace accesses and add significant execution overhead to the application.
    \ignore{This overhead often slows down the application significantly (e.g., 5-10$\times$ for Intel's Memory Latency Checker~\cite{intel-mlc} in our experiments).}  The effect of workload slowdown is different across the different production workloads we tried to trace, with common causes being: (a) the kernel scheduler schedules some other thread on the core, (b) requests timeout, resulting in retries or failures, and (c) the load balancer reducing machine load.
    
    \textit{Dynamic application phases.} Tracing memory access at small granularity poses another challenge with workloads that exhibit diverse and short-lived application phases. For example, parsing a request, communicating with other microservices, or querying a database. Collecting a single small trace (millions of accesses) would not be enough to get representative behavior of the workload.

\subsubsection{Tracing Methodology}
To overcome these challenges, we built a PIN Tool that can attach itself to a process, record memory access for a configurable duration (in the order of microseconds), and detaches to let the workload continue. We collect several such traces from multiple hosts and stitch them together to create a representative trace of the workload. \hl{This allows low-overhead tracing where services do not violate their request-level latency guarantees.}

\begin{table}[]
    \fontsize{8}{10}
    \selectfont
    \centering
    \rowcolors{2}{gray!25}{white}
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \rowcolor[HTML]{d8d8ff}
        \hline
         & \multicolumn{3}{c|}{\textbf{L1D Hit Ratio}}& \multicolumn{3}{c|}{\textbf{R:W Ratio}}\\\cline{2-7}
        \rowcolor[HTML]{d8d8ff}
        \doublerown{\textbf{}} & \textbf{Prod.} & \textbf{Trace}  & \textbf{Error}  & {\textbf{Prod.}} & \textbf{Trace} & \textbf{Error}\\\hline
        \memcache & 0.93 & 0.88  & 5.38\% & 1.84 & 1.92 & -4.34\%\\\hline
        \feed & 0.95 & 0.93  & 2.11\% & 2.14 & 2.20 & -2.8\%\\\hline
        \web & 0.94 & 0.90  & 4.25\% & 1.72 & 1.67 & 2.3\%\\\hline
        
    \end{tabular}
    \caption{Measured (Prod.) and simulated (Trace) results. \vspace{-0.4cm}}
    \label{tab:hit-rate}
\end{table}

\subsubsection{Trace Validation}
To verify the accuracy of the traces, we used a simple cache simulator with the same cache architecture as the production servers and verified the L1D hit rate. 
\reftab{tab:hit-rate} shows the measured and simulated hit rate for the L1-D cache and the R:W ratios. All workloads show a small error, signifying that the traces are accurate for studying the memory behavior of the workloads. 


