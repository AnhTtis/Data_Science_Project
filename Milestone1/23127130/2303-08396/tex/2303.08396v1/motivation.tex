\section{Code, Memory BW, and Latency Challenges in Cloud Datacenters}
\label{sec:motivation}





Existing benchmarks that are generally used to design processors do not accurately represent the workloads at hyperscalars. As highlighted by several previous works~\cite{ayers2018memory,ayers2019asmdb,kanev2015profiling, sriraman2019softsku, sriraman2020accelerometer}, because of the fundamentally different behavior of cloud workloads, they show significantly different IPC, cache miss rate, and other metrics than standard benchmarks. These factors make the SPEC and other commercially available benchmarks a bad proxy for studying the performance of server processors in a datacenter.



\begin{figure}
    \centering
        \includegraphics[width=\linewidth]{gen-over-gen.pdf}
    \vspace{-0.6cm}
    \caption{Generation over generation IPC change for the nine workloads. Some services do not run on the Gen1 servers and thus miss their IPC data.}
    \label{fig:gen-over-gen-ipc}
\end{figure}

To understand how the IPC has changed over the years at \chtxt{\company}, we chart nine representative cloud workloads' IPC over four server generations. \reffig{fig:gen-over-gen-ipc} shows that cloud workloads have very small or even negative IPC improvement between server generations, motivating the need to understand cloud workloads better to optimize server processors for hyperscalars.


To understand the reasons for this inhibited performance growth of server processors running cloud workloads, we will look into each of the three primary reasons for memory system stalls \ignore{from \reffig{fig:emon-trend}}and understand how they affect cloud workloads' performance.


\subsection{Increasing Code Footprint of Datacenter Applications}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{binsize_trend.png}
    \vspace{-0.6cm}
    \caption{Binary Size trend for \web and \insta. \insta binary size increased by 5.6$\times$ over the last four years.}
    \label{fig:codesize-trend}
    \vspace{-0.1cm}
\end{figure}

Cloud workloads are often frontend bound due to their large code footprint, resulting in high instruction cache miss rates~\cite{ayers2018memory}. The code footprint can be several 100s of MiBs and has been increasing over the years. \reffig{fig:codesize-trend} shows the increase in the binary size for \web and \insta, the two major web services at \chtxt{\company} over the years. \insta shows an exponential increase in binary size, a 5.6$\times$ increase over four years. 

In contrast, the I-cache sizes have stayed relatively constant (32KB I\$ per core) across several server generations and have only recently started increasing on some server architectures. \marginpar{for the generations, they haven't; Hao modified}\hl{Likewise, instruction TLB lookups are optimized for the latency-sensitive instruction fetch pipeline, which makes it harder for them to scale with the rapidly growing code footprints of cloud workloads.} Previous work on cloud workload has observed a similar code-footprint trend. Kanev et al.~\cite{kanev2015profiling} show that the code footprint at Google is growing at a rate of 27\% per year. This imbalance between the processor's micro-architectural resources and the growing code footprint have resulted in frontend to be the leading source of bottleneck for many workloads across several server generations and resulted in stagnant IPC over time. These trends necessitate new processor micro-architecture optimization.





\subsection{Memory Bandwidth Scaling Challenge}\label{sec:mem-bw-motivation}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{mem-bw-fleet.pdf}
    \vspace{-0.6cm}
    \caption{Fleet memory bandwidth utilization (\%) across three generations (1-minute average) as a CDF. BW bound and BW sensitive regions are based on the memory bandwidth vs latency characteristics of DDR memories.%
    }
    \label{fig:fleet-mem-bw}
    \vspace{-0.15cm}
\end{figure}

Memory bandwidth is another major concern for cloud workloads, resulting from poor DRAM bandwidth scaling and increasing CPU core count. To understand cloud workloads' memory bandwidth utilization trend, we show the fleet-wide memory bandwidth consumption increase over the three generations of servers in \reffig{fig:fleet-mem-bw}. With the most recent generation (Gen 5), the 1-minute average memory bandwidth utilization shows that majority of the fleet is either bandwidth sensitive or bandwidth bound. While mirco-benchmarks can generally push memory bandwidth utilization to >80\%, we observe that production workloads are rarely able to push memory bandwidth utilization beyond 60\% utilization as any further increase results in exponential increase in memory latency~\cite{radulovic2015another}. Thus, we classify workloads with higher than 60\% bandwidth utilization as memory bandwidth bound (shaded red in \reffig{fig:fleet-mem-bw}). Likewise, workloads with average memory bandwidth utilization between 40\%-60\% can have high transient memory bandwidth utilization and are thus classified as memory BW sensitive. While the fleet-wide data is 1-minute average, transient peaks can make these workloads significantly bandwidth bound during shorter time intervals causing tail latency spikes and limiting overall system CPU utilization. 



Besides growing memory bandwidth utilization across the fleet, increasing disparity between DIMM's bandwidth and capacity~\cite{maruf2022tpp} makes alleviating memory bandwidth problems of cloud workloads harder as hyperscalars need to provision a large number of memory channels and DIMMs per server for sufficient memory bandwidth. 

This trend towards growing memory bandwidth utilization and poor bandwidth scaling of DDR-based memories necessitates a better way to provision memory in cloud datacenters and curb the cost and power consumption of the servers~\cite{dayarathna2015data}.




\subsection{High Memory Latency Leading to CPU Underutilization}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{cpu-vs-latency.pdf}
    \caption{Relative memory latency vs. max CPU utilization for microservices across the fleet.  Services, where latency SLO constraints bind CPU utilization are shown in red. Marker size is scaled by service's relative size.}
    \label{fig:latency-vs-cpu-utilization}
        \vspace{-0.15cm}
\end{figure}


Finally, many cloud workloads are memory latency sensitive as they must meet requests'  latency service level objectives (SLOs)~\cite{ding2019characterizing}. These workloads run at the maximum CPU utilization possible without violating SLO guarantees, often wasting CPU resources. \hl{Memory latency limits CPU utilization as increase in memory latency lowers system's IPC. Lower IPC for workloads results in cloud workloads spending more cycles doing ``on-cpu'' work, and thus results in longer query response times.}

To study this trend across the fleet, \reffig{fig:latency-vs-cpu-utilization} shows the max CPU utilization vs. relative memory latency of workloads using a scatter plot. \hl{We make three important observations from the fleet-wide data: (a) Memory latency increases significantly with increasing CPU utilization. That is, a large portion of the fleet is operating at a relatively high memory latency (>0.5). (b) Workloads marked in red have 20-50\% stranded CPU cores because of the latency SLO constraints.} These workloads need to leave CPU cores idle to avoid violating SLO guarantees\ignore{ resulting from high memory latency}, resulting in wasted resources and inefficiency at datacenter scale. (c) And, differences in memory access patterns (e.g., bursty memory load) results in workload to have different memory latency at similar CPU utilizations. %



Hardware prefetching is one of the key techniques used in modern server processors to hide memory latency. L2 hardware prefetchers, however, have a significant memory bandwidth overhead. 
From our analysis in \refsec{sec:mem-lat}, we find that workloads show a substantial increase in total memory bandwidth consumption with L2 prefetchers enabled while providing limited performance gains, signaling that the prefetchers have low efficiency. 


Next, we will present our profiling tool, \memprof, to characterize cloud workloads and use it to explore opportunities for performance improvements in cloud server processors.


