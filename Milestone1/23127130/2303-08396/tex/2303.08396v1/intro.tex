
\section{Introduction}
\label{sec:intro}




Over the last few years, the market share of the cloud as a portion of the total server market has constantly been increasing. Cloud deployments now account for >50\% of the total server processor market and are expected to grow even further~\cite{datacenter0, datacenter1}. However, CPU benchmarks available today \ignore{to optimize these processors}do not accurately represent cloud workloads' microarchitectural and memory system behavior, leaving the processors largely unoptimized for their unique characteristics.




Cloud workloads show different behavior than the CPU benchmarks available today across several metrics. For one, the fleet-wide IPC for major hyperscalers is significantly lower than the benchmark suites~\cite{sriraman2019softsku,kanev2015profiling}. With millions of servers running these workloads, even small improvements in IPC from understanding the behavior of these workloads can result in significant cost savings and efficiency improvements across the fleet. 


To improve the performance of cloud workloads, in this paper, we study and characterize nine microservices serving live production traffic. These microservices represent a diverse range of workloads at a hyperscalarâ€™s datacenters and run on a large portion of the fleet.


\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{mem-bound-slots-total.pdf}
    \vspace{-0.15cm}
     \caption{Breakdown of where the CPU pipeline slots spend their time across nine production workloads using top-down analysis~\cite{yasin2014top}.}
    \label{fig:mem-bound}
    \vspace{-0.2cm}
\end{figure}

\reffig{fig:mem-bound} presents the top-down analysis~\cite{yasin2014top} of the nine workloads \chtxt{at \company} to show that the memory system is the primary reason for CPU pipeline stalls that leads to low instructions per cycle (IPC) of cloud workloads.
Memory system-related stalls can have three sources, (1) stall on an instruction fetch (\textit{code-fetch bound}), (2) stall on the memory system with a high request pending queue occupancy (\textit{memory bandwidth bound}), or (3) stall on the memory system with a low request pending queue occupancy (\textit{memory latency bound}).

To dive further into these three sources of stalls in cloud workloads, we created a new tool, \memprof, that enables detailed profiling of workloads' micro-architectural and memory system behavior. We use observations from \memprof to propose and evaluate new micro-architectural changes and system architecture for cloud server processors.


Code fetch-related stalls are a significant problem in cloud workloads~\cite{ayers2018memory, sriraman2019softsku}. Using \memprof, we observe \chtxt{workloads at \company} run similar code and share page table mappings across cores. Building on this observation, we propose sharing the L2 Cache and L2 \itlb among a small cluster of CPU cores (e.g., four) that are physically close. Shared micro-architectural structures for instructions help reduce code-fetch stalls by enabling the cores running similar code to pool their caches for a larger apparent cache size even though they have the same per-core cache size. 














Memory bandwidth is another significant contributor to memory system stalls. Over the years, the DDR memory capacity and bandwidth have been scaling at very different rates ~\cite{maruf2022tpp}. Increasing memory bandwidth and capacity disparity, along with growing core counts in modern server processors has resulted in limited per-core memory bandwidth, requiring processors to populate more memory channels, thus resulting in higher server TCO.  We use \memprof to study the memory bandwidth distributions of production workloads and find that only a small percentage of pages contribute to most of the memory bandwidth utilization across the workloads. We use this observation to propose and evaluate new tiered memory systems where a small, high-bandwidth memory tier serves the bandwidth needs of the workload. In contrast, a low-bandwidth, large-capacity tier provides additional memory capacity, lowering the total cost of ownership (TCO) of the servers. We find that by splitting the memory capacity in 30:70\% between the high-BW and low-BW tiers, we can achieve 1.46$\times$ better throughput and 13\% better throughput/cost than the baseline DDR-only configuration. 




Finally, we look into ways to improve memory latency for cloud workloads. 
Using \memprof, we observe that although L2 hardware prefetchers improve memory latency by prefetching cachelines, they often have very low coverage in real-world cloud workloads, resulting in minor IPC improvements. 
Further, we observe that L2 hardware prefetcher's inefficiencies result in high memory bandwidth consumption, exacerbating the memory bandwidth problem.
A major hurdle in optimizing these hardware prefetchers is access to the cloud workloads to study their memory behavior. To enable broader access to cloud workloads' behavior, we collect memory access traces from production workloads to help future hardware prefetcher research.





This paper is the first to study how cloud workloads interact with the memory system and to use these observations to propose shared \uarch{} structures and memory bandwidth tiering. While previous works from Google~\cite{ayers2018memory, kanev2015profiling} and Meta~\cite{sriraman2019softsku, sriraman2020accelerometer} have looked into the \uarch{} behavior of the hyperscale workloads, they have not performed detailed studies of the code behavior, memory bandwidth distribution, or hardware prefetcher efficiency. In summary, in this paper, we make the following contributions:
\begin{itemize}
    \item \textbf{Detailed study and characterization of cloud workloads.} We perform a detailed study of the code, memory bandwidth, and memory latency trends \chtxt{at \company}. 
    \item \textbf{New profiling methodology and tool.} We present \memprof, a profiling tool for cloud workloads, to understand their interaction with the memory system.
    \item \textbf{Code sharing across cores.} We show that CPU cores run very similar code across all the workloads, enabling performance improvements from shared \uarch{} resources.
    \item \textbf{Memory bandwidth distribution.} We study the memory bandwidth distribution, find that very few pages contribute to a large amount of bandwidth and propose new tiered memory solutions for cloud workloads.
    \item \textbf{Hardware prefetcher efficiency.} We observe that L2 hardware prefetchers have significant memory bandwidth overhead and small performance improvements. To mitigate this, we collect memory traces from production workloads to help future hardware prefetcher research.
\end{itemize}

\noindent{}In the next section, we look into the three reasons for memory system stalls: code fetch, memory bandwidth, and memory latency. We follow with description of \memprof{} and use it to make observations and proposals in each area.

\ignore{\noindent{}In the next section, we look into the memory system stalls to understand the contribution of code fetch, memory bandwidth, and memory latency. Next, we present \memprof in \refsec{sec:memprof}, followed by the observations and proposals for improving code fetch-related stalls (\refsec{sec:code-fetch}), memory bandwidth-related stalls (\refsec{sec:mem-bw}), and memory latency-related stalls (\refsec{sec:mem-lat}). Finally, we discuss related works in \refsec{sec:related} and the conclusion in \refsec{sec:conclusion}. }