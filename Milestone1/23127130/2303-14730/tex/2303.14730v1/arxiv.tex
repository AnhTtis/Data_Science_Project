\documentclass[10pt,twocolumn,letterpaper]{article}


\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{multirow}


\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}



\def\cvprPaperID{*****} 
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Semantic Neural Decoding via Cross-Modal Generation}

\author{Xuelin Qian\thanks{Equal contribution}, ~Yikai Wang$^{*}$, ~Yanwei Fu, ~Xiangyang Xue, ~Jianfeng Feng\\
Fudan University\\
}
\maketitle

\renewcommand{\thefootnote}%
{\fnsymbol{footnote}}
\footnotetext[2]{Preliminary report, work in progress.}


%%%%%%%%% ABSTRACT
\begin{abstract}
Semantic neural decoding aims to elucidate the cognitive processes of the human brain by reconstructing observed images from brain recordings. 
Although recent works have utilized deep generative models to generate images conditioned on fMRI signals, achieving high-quality generation with consistent semantics has proven to be a formidable challenge. 
To address this issue, we propose an end-to-end framework, SemanSig, which directly encodes fMRI signals and extracts semantic information. 
SemanSig leverages a deep generative model to decode the semantic information into high-quality images. 
To enhance the effectiveness of our framework, we use the ImageNet class prototype space as the internal representation space of fMRI signals, thereby reducing signal redundancy and learning difficulty. 
Consequently, this forms a semantic-rich and visually-friendly internal representation for generative models to decode. 
Notably, SemanSig does not require pre-training on a large fMRI dataset, and performs remarkably well when trained from scratch, even when the fMRI signal is limited. Our experimental results validate the effectiveness of SemanSig in achieving high-quality image generation with consistent semantics.
\end{abstract}




\section{Introduction} \label{sec:intro}

The human brain responds to images perceived by the eyes constantly~\cite{teng2019visual}, and this response can be indirectly measured through functional Magnetic Resonance Imaging (fMRI). 
Identifying and categorizing different brain activity patterns according to visual stimuli is crucial for a comprehensive understanding of the human brain. 
In pursuit of this goal, researchers aim to invert the brain's response process, recovering observed images from fMRI signals~\cite{parthasarathy2017neural,horikawa2017generic}. 
Given the complex and intricate nature of images, recovering the pixel-level details is both challenging and unnecessary. 
As such, researchers primarily focus on decoding the semantic information, thus giving rise to the task of semantic neural decoding~\cite{horikawa2017generic,shen2019end, beliy2019voxels, gaziv2022self}.


\begin{figure}
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{fig/intro.pdf}}
\caption{
(a) The conventional conditional generator typically follows a pipeline that requires a considerable number of training samples or a well-pre-trained initialization to bridge the two modalities of fMRI and images.
(b) Our proposed method is illustrated in this figure, which involves narrowing down the learning space via (1) introducing class prototypes to assist in capturing semantic representations from fMRI signals, (2) mapping conditions to discrete codes, rather than the continuous and high-dimensional image space.}
\label{fig:intro}
\end{center}
\vskip -0.2in
\end{figure}

However, due to the fact that fMRI signals are an indirect measure of brain activity, the information they contain may not always correlate with the semantics of the observed image, and can also be redundant~\cite{chang2019bold5000}. 
Moreover, the fMRI signal is known to vary significantly across different scanner setups, experimental conditions, and even between individuals~\cite{chen2022seeing}. 
Consequently, decoding the fMRI signal is a challenging task that presents the following difficulties:

\textbf{(1)} \emph{Redundancy}: The semantic information may be sparsely distributed in the signal, and neighbors in the signal are highly correlated, which suggests that the fMRI signal exhibits redundancy.

\textbf{(2)} \emph{Instability}: The fMRI signal suffers from the domain shift problem, whereby the signals of one individual on one scanner for one image may not be useful for decoding fMRI signals for another individual, scanner, or image.
Furthermore, in practical scenarios where only a few image-signal pairs are available, the use of popular deep learning methods that rely on large training sets is challenging for semantic neural decoding.

To address the quality and quantity issues of fMRI signals, we propose two key principles for effective semantic neural decoding:
reduce signal redundancy, and
reduce learning difficulty.

To reduce signal redundancy, a dense and informative embedding is necessary to represent the original fMRI signal. 
Several approaches have been proposed to achieve this, including the use of auto-encoders to compress the full information~\cite{beliy2019voxels, chen2022seeing}, and the design of additional models to select only the most useful information~\cite{mozafari2020reconstructing, ozcelik2022reconstruction}.

Our proposed approach involves learning a class prototype for the fMRI signal that captures the most salient category information. 
To achieve this, we employ multiple transformer encoder layers to extract informative features from the fMRI signals, which are then projected into the class prototype space of ImageNet classes.
The use of transformer layers is particularly advantageous for capturing long-range dependencies, due to their expansive receptive field~\cite{Vaswani2017Attention}. 
This capability enables our approach to better extract sparsely distributed semantic information from the fMRI signals, which is critical for accurate and effective decoding.

Furthermore, the utilization of the target ImageNet class prototype space imposes constraints on the variability of the fMRI signal representations. 
To be specific, this particular representation space compels the encoder to extract semantic information while discarding surplus information. 
The latter, however, is generally favored in models trained for fMRI signal reconstruction, as per the study conducted by~\cite{chen2022seeing}. 
Additionally, the imposition of this representation space reduces the complexity of the learning process by restricting the space to a specific region, rather than the entire Euclidean space. 
Therefore, the learning difficulty is alleviated while preserving the representation capacity, all thanks to the ingeniously designed ImageNet class prototype space.

Building upon these insights, our paper presents the SemanSig framework for semantic neural decoding. 
This framework comprises an encoder-decoder transformer architecture that is \emph{trained from scratch}. 
Specifically, the encoder maps the fMRI signals onto the semantic-rich prototype space of ImageNet classes. Subsequently, the decoder generates outputs within the vector-quantized discrete space, conditioned on the embedded fMRI signals in an auto-regressive manner. 
We demonstrate that SemanSig is both efficient and effective in semantic neural decoding, as evidenced by its performance on the BOLD5000 dataset.

\iffalse
\textbf{Contribution}. Our contributions can be summarized as:
\begin{itemize}[leftmargin=*,itemsep=0pt,topsep=0pt,parsep=0pt]
\item We develop an encoder-decoder architecture that is trained from scratch to form semantic neural decoding.
\item To mitigate the issues of signal redundancy and learning difficulty, we propose an innovative approach that involves embedding the fMRI signal into a semantic-rich prototype space derived from ImageNet classes.
\item We demonstrate the effectiveness of our proposed SemanSig framework on the BOLD5000 dataset, showcasing comparable performance when compared to larger models that have been pre-trained on large datasets.
\end{itemize}
\fi


\section{Related Work} \label{sec:related-work}
\noindent \textbf{Semantic Neural Decoding} aims to reconstruct the observed image from the fMRI signal, with the target of enhancing our understanding of human brain activity. 
Traditional approaches to this problem~\cite{horikawa2017generic, shen2019deep} involve leveraging both the fMRI signal and hierarchical image features to perform decoding. 
Some recent works~\cite{mozafari2020reconstructing, ozcelik2022reconstruction} have utilized regression models to extract useful information from the fMRI signal. 
More recently, MinD-Vis~\cite{chen2022seeing} has adopted an auto-encoder to compress the fMRI signal while maintaining as much information as possible.
However, in contrast to these approaches, we contend that, due to the redundancy and instability of fMRI signals, it is preferable to perform information selection rather than information preservation when compressing the fMRI representations. 
Thus, in this paper, we propose a novel approach that involves projecting the fMRI signal onto the ImageNet class prototype space, with the aim of promoting the preservation of semantic information.

\noindent\textbf{Cross-Modal Generation} tackles the challenging task of generating images with multi-modal controls, such as text, speech, or fMRI signals. 
Popular methods include text-to-image~\cite{nichol2022glide,Chang2022maskgit}, image-to-image~\cite{isola2017image,cao2021image}, and speech-to-image~\cite{li2020direct}, among others. 
Semantic neural decoding can be viewed as a cross-modal generation task conditioned on the fMRI signal, where the goal is to accurately follow the signal to generate images while maintaining fidelity.

A typical cross-modal generation pipeline involves training generative adversarial networks~\cite{goodfellow2020generative} to generate images by deceiving discriminators. 
With the emergence of transformers~\cite{Vaswani2017Attention}, some approaches~\cite{Chang2022maskgit} first translate images into sequences of tokens and then perform masked-language modeling tasks similar to those in natural language processing~\cite{devlin2018bert} to iteratively predict tokens and generate images. 
Recently, diffusion models~\cite{sohl2015deep} have also been proposed, which construct a random process between real images and random noise and perform generation via iteratively removing the noise~\cite{nichol2022glide}. 
In this paper, we propose an auto-regressive generation pipeline on the compressed image token spaces that strikes a balance between fidelity and efficiency for cross-modal image generation.

\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=16cm,height=8cm]{fig/framework.pdf}}
\caption{
Illustration of SemanSig. 
The neural encoder employs multiple transformer layers to derive the fMRI signals and map them onto the semantically-enriched prototype space of ImageNet categories. 
This representation of fMRI signals then guides the image generation process via the semantic decoder. 
The generation process is performed discretely in the latent visual code space autoregressively. Subsequently, the sequence of reconstructed image tokens is translated into the RGB image space to form the results of semantic neural decoding.
}
\label{fig:framework}
\end{center}
% \vskip -0.2in
\end{figure*}

\noindent\textbf{Vector-quantized Image Modeling} draws inspiration from the remarkable success of pre-trained language models in natural language processing in predicting the next token~\cite{devlin2018bert}. 
To apply this capability to computer vision, VIM endeavors to discover a latent discrete token space that can holistically retain the pixel-level details of images~\cite{van2017neural, Esser2021taming}. 
This latent space enables image generation akin to next-token prediction in language models. 
VIM commonly represents this latent discrete token space as a codebook of visual vocabulary~\cite{van2017neural, Esser2021taming}. 
The codebook is trained in an auto-encoder fashion to guarantee that the image can be reconstructed seamlessly from the learned codebook. 
Due to this framework's ability to compress the original image into a concise token sequence, we employ VIM in this paper to facilitate generation on the latent token space for improved efficiency.


\section{Methodology} \label{sec:method}

\subsection{Problem Definition and Overview}
The objective of this paper is to reconstruct the image $\mathbf{I} \in \mathbb{R}^{H\times W \times 3}$ that corresponds to the fMRI signal $\mathbf{f}$ obtained from brain activity. 
The reconstructed images $\hat{\mathbf{I}}$ should ideally match the original image. 
However, due to the individual-specificity of biological mechanisms, the brain activity stimulated by the same image can be quite varied. 
To address this challenge, we follow the approach proposed in~\cite{mozafari2020reconstructing,ozcelik2022reconstruction} and require the generated images $\hat{\mathbf{I}}$ to be \textit{semantically consistent} with $\mathbf{I}$, thereby formulating the task of semantic neural decoding.

Generating natural images conditioned on fMRI signals is a non-trivial task due to the significant domain gap between the two modalities. To overcome this, we propose an auto-regressive generator equipped with a semantic-guided transformer encoder, inspired by the remarkable performance of transformer-like architectures in visual understanding and generation tasks~\cite{Chang2022maskgit, Dosovitskiy2021an}. 
Our framework, illustrated in Figure~\ref{fig:framework}, effectively bridges the gap between the two modalities, enabling the synthesis of natural and plausible images that are faithful to the fMRI signals.

In the following sections, we present a neural encoding module that extracts semantic-rich and compact representations for fMRI signals. Subsequently, we elaborate on the structure of the decoder via a cross-modal generator.


\subsection{Semantic-guided Neural Encoding}
\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{fig/fmri.pdf}}
\caption{
This figure, adapted from~\cite{haxby2001distributed}, illustrates the variability of fMRI signals across different images.
When individuals observe different images, the corresponding fMRI signals exhibit distinct patterns of activation. 
Conversely, for the same image, different runs show similar fMRI responses across the same individual. }
\label{fig:fmri}
\end{center}
\vskip -0.2in
\end{figure}

The fMRI indirectly records neural activity in the visual cortex of the brain by measuring the blood-oxygen-level-dependent (BOLD) signals, which is usually represented as data of 3D voxels, covering five visual regions of EarlyVis, LOC, OPA, PPA, and RSC \cite{Horikawa2015Generic}. 
As visualized in Figure~\ref{fig:fmri}, when observing different images, fMRI signals will have different patterns.
For the same individual in different running experiments, the fMRI signals show similar behavior.
The biological principles indicate that neighboring voxels in the brain's visual cortex often show a similar intensity of stimulus-response \cite{Ugurbil2013Pushing}, leading to high redundancy of fMRI signals. 

To explore the potential responding patterns according to the visual stimuli and build the reverse mapping for semantic encoding, it is thus necessary to focus on the relationships among different regions and simultaneously perceive the activity changes from long-range voxels. Inspired by the advantage of long-range receptive fields from transformer layers, we design a semantic-guided neural encoding module, which consists of a long-range neural encoder with a self-attention mechanism and a projection head with semantic prototypes.


\noindent \textbf{Long-range Neural Encoder.} Following \cite{chen2022seeing}, we serialize voxels within each region and concatenate them together. It produces a vector with thousands of elements $x \in \mathbb{R}^{k \times 1}$, where the total number of voxels $k$ varies depending on different individuals. Considering that adjacent voxels have redundancy, we then aggregate signals within a small range, to not only extract low-level features but also reduce the redundancy and computation, as
\begin{equation}
\mathbf{X} = \text{Conv}\left(x; s\right) \in \mathbb{R}^{k' \times c}, 
\end{equation}

\noindent where ``Conv'' denotes a 1D convolution layer whose kernel size and stride are exactly the same as the range of aggregation $s$; $k'$ and $c$ are the length and dimension of the output vector, respectively. By treating vector elements as tokens, several transformer layers~\cite{Vaswani2017Attention} are attached to progressively explore correlations among tokens, extracting high-level semantic features. More concretely, we first project features into three latent spaces of $\textit{key}$, $\textit{query}$ and $\textit{value}$, each of which is instantiated by a linear function, as
\begin{equation}
\mathbf{Q} = \mathbf{X} \mathbf{W}_{1},~~\mathbf{K} = \mathbf{X}\mathbf{W}_{2},~~\mathbf{V} = \mathbf{X}\mathbf{W}_{3}.
\end{equation}
Subsequently, we achieve long-range correlations by computing self-attention weights based on the dot-product similarity between the $\textit{key}$ and $\textit{query}$, as,
\begin{equation}
\alpha = \frac{ \exp \left( \mathbf{Q}\mathbf{K}^{\mathrm{T}}\right)}{\sum_{j=1}^{k'}\exp \left( \mathbf{Q}\mathbf{K}^{\mathrm{T}}\right)_{j}  } \in \mathbb{R}^{K' \times K'},
\label{eq:attention}
\end{equation}
 
\noindent where $\mathbf{W}_{1}$, $\mathbf{W}_{2}$ and $\mathbf{W}_{3}$ are parameters of three linear projections, $j$ is the index of columns. The attention weights can 
effectively reflect the short-range redundancy and the long-range relations. Therefore, each token embedding can be calibrated and refined from a broader perspective based on the weighted sum of the \textit{value} according to the attention weights. Furthermore, the output is processed by a feed-forward network $\text{FFN}\left(\cdot\right)$ which consists of two fully-connected layers to integrate features from multiply attention heads \cite{Vaswani2017Attention}, as, 
\begin{equation}
\mathbf{X} \leftarrow \text{FFN}\left( \alpha \mathbf{V}  \right) + \mathbf{X} \in \mathbb{R}^{k' \times c}.
\label{eq:value}
\end{equation}

\noindent \textbf{Semantic Projection Head.} 
Obviously, the fMRI signal is an indirect activity signal and is represented as simple voxels, making it difficult to learn semantic meanings from underlying patterns. In addition, the scarcity of paired fMRI-image training samples limits the effective learning of our neural encoder with transformer layers. 

To mitigate this challenge, we introduce class prototypes from ImageNet, which naturally builds a well-defined latent space, to help capture semantic-rich representations. By transforming the learning of a multi-class decision space into a similarity projection of the semantic space, such a straightforward way could easily enforce the encoder to represent semantic information. 

More specifically, an off-the-shelf classification model pre-trained on the ImageNet dataset is leveraged to first assign pseudo labels to fMRI signals according to the predicted probabilities of the corresponding images. Then, we export its weights of the last linear function, $\mathbf{W}_{\mathrm{fc}}$, as class prototypes. 
Given the first token extracted from Eq.~\eqref{eq:value}, denoted as the [CLS] token embedding $\mathbf{X}_{[\mathrm{cls}]}$, we, therefore, design a projection head with one fully-connected layer to project the embedding into the semantic latent space, so as to pull closer to one prototype sharing the same category label, and push away from the others, as
\begin{equation}
\mathrm{score}_t = \frac{ \exp \left( \mathcal{P}\left(\mathbf{X}_{[\mathrm{cls}]}\right)\mathbf{W}_{\mathrm{fc},t}^{\mathrm{T}}\right)}{\sum_{j}\exp \left( \mathcal{P}\left(\mathbf{X}_{[\mathrm{cls}]}\right)\mathbf{W}_{\mathrm{fc},j}^{\mathrm{T}}\right)  }.
\end{equation}
\noindent where $\mathcal{P}\left(\cdot\right)$ denotes the projection head. Assuming the pseudo label of the fMRI signal is $t$, $\mathrm{score}_{t}$ is thus expected to be close to 1.




\subsection{Autoregressive Cross-modal Decoding\label{sec:decoding}}
Intuitively, a na\"ive approach for cross-modal decoding is building a conditional generator, where it learns to synthesize natural images conditioned on fMRI embeddings obtained by our proposed neural encoding module. However, we argue that one of the major challenges for cross-model decoding is the huge domain gap between the two modalities. The fMRI measures the BOLD signals to reveal brain activities in an indirect and aggregate way, which are further represented as 1D vectors. Nevertheless, RGB images are composed of color pixels, which express semantic information by forming different structures, color distributions, foreground objects, and so on. It inevitably 
becomes computationally intractable to learn a mapping (generator) from simple numerical signals to continuous, high-dimensional, and semantically abstract image space.

Inspired by recent approaches that overcome similar challenges for image synthesis \cite{Chang2022maskgit, Esser2021taming, Oord2017neural}, we leverage compact discrete representation to represent images with discretized codes.
Thereby, the task of our cross-modal decoding turns from learning diverse high-dimensional semantic features to the distribution of code combinations, conditioned on the given fMRI signal.


\noindent \textbf{Discrete Representation.} Given an image $\textbf{I} \in \mathbb{R}^{H\times W\times3}$, we first utilize  $\mathbf{E}\left(\cdot\right)$ to extract latent features $\textbf{F}$ from input images. In order to represent images with discrete codes, we introduce a learnable codebook $\mathbf{Q} \in \mathbb{R}^{n\times c}$ whose entry is expected to describe a particular type of local-part visual semantics. Here, $n$ indicates the number of learned codes. 

Formally, the vector quantization can be performed by replacing the features in $\textbf{F}$ with the closest entry in the codebook \cite{Esser2021taming},
\begin{equation}
   \mathbf{Z} = \mathbf{Q}\left(\textbf{F}\right) := \arg\min_{\mathbf{e}_{i}\in \mathbf{Q}} || \textbf{F}_{\left(j\right)} - \mathbf{e}_{i} ||
\label{eq:quantize}
\end{equation}
\noindent where $\mathbf{e}_{i}$ and $\textbf{F}_{\left(j\right)}$ means the $i$-th and $j$-th element in the codebook $\mathbf{Q}$ and latent feature $\textbf{F}$, respectively. To supervise the learning of the codebook, a  $\mathbf{D}\left(\cdot\right)$ with the symmetric structure is further designed to reconstruct the input images from the discrete codes $\mathbf{Z}$, serving as an image reconstruction problem. 

Once trained, we are naturally equipped with a powerful codebook of visual semantic representations. By choosing and combining different codes, we can generate diverse natural images. Consequently, any input image $\textbf{I}$ can be represented as discretized codes $\mathbf{Z} \in \mathbb{R}^{l \times c}$ or a more compact 1D vector $\mathbf{z} \in \mathbb{R}^{l \times 1}$, where each element indicates the index of the code in the codebook.

\noindent \textbf{Autoregressive Decoding.}
Through the aforementioned process, we can successfully encode neural signals of fMRI by long-range receptive fields and use [CLS] token embedding $\mathbf{X}_{[\mathrm{cls}]}$ to carry the mined potential semantic features. Also, the corresponding image causing visual stimuli is represented as a compact discrete vector $\mathbf{z}$. 

Now, we can easily perform cross-modal decoding to autoregressively learn the combination order given the semantic conditions. More concretely, we start by appending a learnable [SOS] token $\mathbf{X}_{[\mathrm{sos}]}$ to the extracted embedding of $\mathbf{X}_{[\mathrm{cls}]}$, serving as the simplest condition way of concatenation. Following \cite{Esser2021taming}, we adopt a decoder-only transformer structure with several stacked multi-head self-attention layers as a cross-model generator. During training, the ground-truth discrete vector $\mathbf{z}$ is additionally attached to $\mathbf{X}_{[\mathrm{sos}]}$. The generator takes them as inputs and learns to progressively predict the likelihood of the $j$-th code index according to the relations among the previous ground-truth indices $\mathbf{z}_{<j}$, which can be formulated as,
\begin{equation}
p\left(\mathbf{z}\right) = \prod_{i=1}^{l}p\left(\mathbf{z}_{i}~|~\mathbf{X}_{[\mathrm{cls}]},~\mathbf{X}_{[\mathrm{sos}]},~\mathbf{z}_{<i} \right)
\label{eq:cond}
\end{equation}
\noindent During inference, $\mathbf{X}_{[\mathrm{sos}]}$ is responsible for predicting the first index $\hat{\mathbf{z}}_{1}$, so that we can auto-repressively decode the next index of code conditioned on the previously predicted indices and fMRI signals. Once all elements in $\mathbf{z}$ are sampled to form $\hat{\mathbf{z}}$, we can feed $\hat{\mathbf{z}}$ into  $\mathbf{D}\left(\cdot\right)$ to translate the discrete token sequence into the RGB images.

\subsection{Training SemanSig}
The training of SemanSig can be divided into two phases.
In the first preparation phase, we learn the discrete codebook we introduced in Sec.~\ref{sec:decoding}.
More specifically, we leverage the pre-trained ImageNet model and fine-tune it on the relevant dataset of interest. Once this discrete codebook has been obtained, we can proceed with the end-to-end training of the encoder-decoder architecture, thereby enabling the generation of the discrete image token sequence that incorporates the learned codebook details\footnote{We refer readers to \cite{Esser2021taming} for details of the codebook learning.}.


In the second phase, we train the encoder-decoder architecture from scratch.
Specifically, the neural encoder is trained by minimizing the divergence between the encoded fMRI signal and the prototype of the corresponding pseudo-labeled ImageNet class, while maximizing the divergence with respect to the other prototypes through cross-entropy loss, as:
\begin{equation}
\mathcal{L}_{\mathrm{encoder}} = \mathrm{CE}(\text{score}_{t}).
\end{equation}
For the semantic decoder, we follow standard auto-regressive tasks to minimize negative log-likelihood applied to the discrete image token, as
\begin{equation}
\mathcal{L}_{\mathrm{decoder}}=-\mathbb{E}_{\mathbf{z}_i} \log p\left(\mathbf{z}_{i}~|~\mathbf{X}_{[\mathrm{cls}]},~\mathbf{X}_{[\mathrm{sos}]},~\mathbf{z}_{<i} \right).
\end{equation}
Combined together, the training loss is defined as:
\begin{equation}
\mathcal{L}\coloneqq\mathcal{L}_{\mathrm{encoder}}+\mathcal{L}_{\mathrm{decoder}}.
\end{equation}
With this direct objective, 
SemanSig can be effectively trained, and can yield high-quality image generation results that are conditioned on fMRI signals while maintaining semantic consistency. Additionally, SemanSig trained using this objective does not necessitate the use of extra-large scale datasets for pre-training, thereby demonstrating its potential for learning from limited training data.

\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1\textwidth]{fig/vis.pdf}}
\caption{Decoding Performance Comparisons on BOLD5000 dataset. 
SemanSig performs high-fidelity generation with semantic consistency while MinD-Vis fails in some cases.
For example, in the first row of the left part, MinD-Vis ignores the human while our SemanSig well-reconstruct humans.
In the second row of the right part, MinD-Vis falsely generate the indoor scene while our SemanSig correctly generates the outdoor scene.
}
\label{fig:vis}
\end{center}
% \vskip -0.2in
\end{figure*}


\section{Empirical Analysis} \label{sec:experiment}

\noindent \textbf{Dataset.} We validate our SemanSig on 
Brain, Object, Landscape Dataset (BOLD5000) \cite{chang2019bold5000}. We choose $4,803$ images presented on a single trial for training, and the remaining $113$ images for testing. 

\noindent \textbf{Implementation.}
The architectures for learning the codebook are the same as \cite{Chang2022maskgit}, where five residual blocks are stacked to first downsample the feature size four times, and then a symmetrical structure with upsample layers serves as~$\textbf{D}$. 
For the second stage, we train the neural encoder and the cross-model generator with $12$ transformer layers in an end-to-end manner.

\subsection{Effectiveness}
\noindent \textbf{Our method can reconstruct images that are semantic-consistently with observed images.}
The generated images have the same semantic information as the ground-truth including the humans, objects, animals, architecture, and landscapes, while MinD-Vis may fail in some cases.
For example in the first row of the left part, SemanSig enjoys a semantic-consistent generation that involves a human on the water, while MinD-Vis only reconstructs the water.
In the second row of the right part, SemanSig correctly reconstructs the outdoor scene while MinD-Vis falsely generate the indoor scene.
Such results show that our generated images can not only retain the correct objects but also have more detailed semantics. 
Further, although we use a much smaller generation model, the fidelity of images is also achieved in many cases. 


\begin{table}[ht] 
\centering
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{lcc}
\toprule
 & Params & Training set size \\
 \midrule
 MinD-Vis & 303M + 400M & 136,000 + 4803  \\
SemanSig & 76.7M + 54.5M & 4803  \\
\bottomrule
\end{tabular}}
\caption{Complexity comparison.\label{tab:complexity}}
% \vspace{-0.1in}
\end{table}

\begin{figure*}[h]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1\textwidth]{fig/vis2.pdf}}
\caption{Visualizations of synthesized images conditioned on randomly sampled noise as fake fMRI signals. Models are trained on subject 1. Note that the distribution of ground-truth fMRI signals from BOLD5000 dataset is estimated as $\left(\mu, \sigma^{2}\right)\approx \left(-5.2e\text{-}10, 1.0 \right)$. 
When the divergence between fake fMRI signals and real fMRI distribution enlarges, our SemanSig fast fails but MinD-Vis still generate high-fidelity images.
This may be questionable if we cannot ensure that the generated images are conditioned on the fMRI signal rather than some random noise.
}
\label{fig:ood}
\end{center}
% \vskip -0.2in
\end{figure*}

\subsection{Reliability}
\noindent \textbf{Does the semantic neural decoding indeed decode fMRI signal?}
As generation models are fast-developed in recent years and achieve promising performance even in the unconditional generation that directly generates images from random noise, it is doubtful whether the high-fidelity generation results are indeed the decoding of fMRI signals.

Specifically, we estimate the distribution statistics of subject 1 in the BOLD5000 dataset.
Then we generate random Gaussian noise as fake fMRI signals as the input of semantic neural decoding models.
Results are shown in Figure~\ref{fig:ood}.
Surprisingly, when the fake fMRI signal is far from the original fMRI distribution, the generation result of MinD-Vis is still reasonable and of high quality.
In contrast, SemanSig returns unrealistic images as the divergence between fake fMRI signals and ground-truth distribution enlarges.
These interesting results indicates that:

\textbf{(1)}
The high-fidelity generation of the pre-trained large-scale generation model may influence the trustworthiness of semantic neural decoding capacity.
As a large-scale pre-trained generation model can perform realistic generation results even with random noise, it is questionable whether the decoded image is the reaction of the input fMRI signal or some random noise.

\textbf{(2)}
In contrast, models that do not rely on knowledge of extra datasets are more conservative and fail with a high possibility when the input fMRI signal is far from the training distributions.
This suggests us to rethink the usage of large-scale pre-training for semantic neural decoding tasks.


\section{Conclusions} 
\label{sec:conclusions}
In this paper, we tackle the problem of semantic neural decoding via the encoder-decoder architecture trained from scratch.
We adopt the ImageNet class prototype space as the internal representation space of fMRI signals to better extract the semantic information that is sparsely distributed in the signal.
With this internal space, the redundant and unstable fMRI signal can be represented in a semantic-rich and visual-friendly manner.
Then the deep generative model can reconstruct the observed image progressively with high-fidelity and semantic consistency.
All these modules consist of our framework, dubbed as SemanSig.
Experiments on BOLD5000 validate the effectiveness of SemanSig.


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
