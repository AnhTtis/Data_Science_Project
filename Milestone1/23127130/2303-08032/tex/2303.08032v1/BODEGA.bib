@inproceedings{Przybya2020,
abstract = {In this study we aim to explore automatic methods that can detect online documents of low credibility, especially fake news, based on the style they are written in. We show that general-purpose text classifiers, despite seemingly good performance when evaluated simplistically, in fact overfit to sources of documents in training data. In order to achieve a truly style-based prediction, we gather a corpus of 103,219 documents from 223 online sources labelled by media experts, devise realistic evaluation scenarios and design two new classifiers: a neural network and a model based on stylometric features. The evaluation shows that the proposed classifiers maintain high accuracy in case of documents on previously unseen topics (e.g. new events) and from previously unseen sources (e.g. emerging news websites). An analysis of the stylometric model indicates it indeed focuses on sensational and affective vocabulary, known to be typical for fake news.},
address = {New York, USA},
author = {Przyby{\l}a, Piotr},
booktitle = {Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20)},
doi = {10.1609/aaai.v34i01.5386},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Przyby{\l}a - 2020 - Capturing the Style of Fake News.pdf:pdf},
issn = {2159-5399},
number = {01},
pages = {490--497},
publisher = {AAAI Press},
title = {{Capturing the Style of Fake News}},
url = {https://aaai.org/ojs/index.php/AAAI/article/view/5386},
volume = {34},
year = {2020}
}
@inproceedings{kiesel-etal-2019-semeval,
abstract = {Hyperpartisan news is news that takes an extreme left-wing or right-wing standpoint. If one is able to reliably compute this meta information, news articles may be automatically tagged, this way encouraging or discouraging readers to consume the text. It is an open question how successfully hyperpartisan news detection can be automated, and the goal of this SemEval task was to shed light on the state of the art. We developed new resources for this purpose, including a manually labeled dataset with 1,273 articles, and a second dataset with 754,000 articles, labeled via distant supervision. The interest of the research community in our task exceeded all our expectations: The datasets were downloaded about 1,000 times, 322 teams registered, of which 184 configured a virtual machine on our shared task cloud service TIRA, of which in turn 42 teams submitted a valid run. The best team achieved an accuracy of 0.822 on a balanced sample (yes : no hyperpartisan) drawn from the manually tagged corpus; an ensemble of the submitted systems increased the accuracy by 0.048.},
address = {Minneapolis, Minnesota, USA},
author = {Kiesel, Johannes and Mestre, Maria and Shukla, Rishabh and Vincent, Emmanuel and Adineh, Payam and Corney, David and Stein, Benno and Potthast, Martin},
booktitle = {Proceedings of the 13th International Workshop on Semantic Evaluation},
doi = {10.18653/v1/S19-2145},
pages = {829--839},
publisher = {Association for Computational Linguistics},
title = {{SemEval-2019 Task 4: Hyperpartisan News Detection}},
url = {https://aclanthology.org/S19-2145},
year = {2019}
}
@article{VanderLinden2022,
abstract = {The spread of misinformation poses a considerable threat to public health and the successful management of a global pandemic. For example, studies find that exposure to misinformation can undermine vaccination uptake and compliance with public-health guidelines. As research on the science of misinformation is rapidly emerging, this conceptual Review summarizes what we know along three key dimensions of the infodemic: susceptibility, spread, and immunization. Extant research is evaluated on the questions of why (some) people are (more) susceptible to misinformation, how misinformation spreads in online social networks, and which interventions can help to boost psychological immunity to misinformation. Implications for managing the infodemic are discussed. This Review provides an overview of the psychology of misinformation, from susceptibility to spread and interventions to help boost psychological immunity.},
author = {van der Linden, Sander},
doi = {10.1038/s41591-022-01713-6},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/van der Linden - 2022 - Misinformation susceptibility, spread, and interventions to immunize the public.pdf:pdf},
issn = {1546-170X},
journal = {Nature Medicine 2022 28:3},
keywords = {Communication,Psychology},
number = {3},
pages = {460--467},
pmid = {35273402},
publisher = {Nature Publishing Group},
title = {{Misinformation: susceptibility, spread, and interventions to immunize the public}},
url = {https://www.nature.com/articles/s41591-022-01713-6},
volume = {28},
year = {2022}
}
@article{Ciampaglia2018,
abstract = {The deluge of online and offline misinformation is overloading the exchange of ideas upon which democracies depend. Fake news, conspiracy theories, and deceptive social bots proliferate, facilitating the manipulation of public opinion. Countering misinformation while protecting freedom of speech will require collaboration across industry, journalism, and academia. The Workshop on Digital Misinformation — held in May 2017 in conjunction with the International Conference on Web and Social Media in Montr{\'{e}}al, Qu{\'{e}}bec, Canada — was intended to foster these efforts. The meeting brought together more than 100 stakeholders from academia, media, and tech companies to discuss the research challenges implicit in building a trustworthy Web. Below we outline the main findings from the discussion.},
annote = {Relacja z workshopu 'Digital Misinformation' na ICWSM 2017},
author = {Ciampaglia, Giovanni Luca and Mantzarlis, Alexios and Maus, Gregory and Menczer, Filippo},
doi = {10.1609/aimag.v39i1.2783},
issn = {0738-4602},
journal = {AI Magazine},
number = {1},
pages = {65},
title = {{Research Challenges of Digital Misinformation: Toward a Trustworthy Web}},
url = {https://144.208.67.177/ojs/index.php/aimagazine/article/view/2783},
volume = {39},
year = {2018}
}
@phdthesis{MacCartney2009,
author = {MacCartney, Bill},
school = {Stanford University},
title = {{Natural Language Inference}},
type = {Ph. D. thesis},
year = {2009}
}
@inproceedings{ribeiro-etal-2018-semantically,
abstract = {Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs) {--} semantic-preserving perturbations that induce changes in the model{'}s predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) {--} simple, universal replacement rules that induce adversaries on many instances. We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question-answering, and sentiment analysis. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models using data augmentation significantly reduces bugs, while maintaining accuracy.},
address = {Melbourne, Australia},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
doi = {10.18653/v1/P18-1079},
pages = {856--865},
publisher = {Association for Computational Linguistics},
title = {{Semantically Equivalent Adversarial Rules for Debugging NLP models}},
url = {https://aclanthology.org/P18-1079},
year = {2018}
}
@article{Gorwa2020,
abstract = {As government pressure on major technology companies builds, both firms and legislators are searching for technical solutions to difficult platform governance puzzles such as hate speech and misinf...},
author = {Gorwa, Robert and Binns, Reuben and Katzenbach, Christian},
doi = {10.1177/2053951719897945},
issn = {20539517},
journal = {Big Data \& Society},
keywords = {Platform governance,algorithms,artificial intelligence,content moderation,copyright,toxic speech},
number = {1},
publisher = {SAGE Publications},
title = {{Algorithmic content moderation: Technical and political challenges in the automation of platform governance}},
url = {https://journals.sagepub.com/doi/full/10.1177/2053951719897945},
volume = {7},
year = {2020}
}
@inproceedings{Martino2020a,
abstract = {We present the results and the main findings of SemEval-2020 Task 11 on Detection of Propaganda Techniques in News Articles. The task featured two subtasks. Subtask SI is about Span Identification: given a plain-text document, spot the specific text fragments containing propaganda. Subtask TC is about Technique Classification: given a specific text fragment, in the context of a full document, determine the propaganda technique it uses, choosing from an inventory of 14 possible propaganda techniques. The task attracted a large number of participants: 250 teams signed up to participate and 44 made a submission on the test set. In this paper, we present the task, analyze the results, and discuss the system submissions and the methods they used. For both subtasks, the best systems used pre-trained Transformers and ensembles.},
archivePrefix = {arXiv},
arxivId = {2009.02696},
author = {{da San Martino}, Giovanni and Barr{\'{o}}n-Cede{\~{n}}o, Alberto and Wachsmuth, Henning and Petrov, Rostislav and Nakov, Preslav},
booktitle = {Proceedings of the Fourteenth Workshop on Semantic Evaluation (SemEval-2020)},
eprint = {2009.02696},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/da San Martino et al. - 2020 - SemEval-2020 Task 11 Detection of Propaganda Techniques in News Articles.pdf:pdf},
pages = {1377--1414},
title = {{SemEval-2020 Task 11: Detection of Propaganda Techniques in News Articles}},
url = {http://propaganda.qcri.org/annotations/definitions.html http://arxiv.org/abs/2009.02696},
year = {2020}
}
@article{Church2022,
abstract = {Many papers are chasing state-of-the-art (SOTA) numbers, and more will do so in the future. SOTA-chasing comes with many costs. SOTA-chasing squeezes out more promising opportunities such as coopetition and interdisciplinary collaboration. In addition, there is a risk that too much SOTA-chasing could lead to claims of superhuman performance, unrealistic expectations, and the next AI winter. Two root causes for SOTA-chasing will be discussed: (1) lack of leadership and (2) iffy reviewing processes. SOTA-chasing may be similar to the replication crisis in the scientific literature. The replication crisis is yet another example, like evaluation, of over-confidence in accepted practices and the scientific method, even when such practices lead to absurd consequences.},
annote = {Rozwa{\.{z}}ania na temat SOTA-chasing: prac koncentruj{\c{a}}cych si{\c{e}} na pobiciu najlepszego wyniku dla znanego benchmarku, a nie zawieraj{\c{a}}cych pr{\'{o}}b analizy lub zrozumienia badanych zjawisk.

Om{\'{o}}wione koszty zjawiska:
- koncentrowanie si{\c{e}} na rywalizacji, a nie kooperacji, szczeg{\'{o}}lnie w shared taskach,
- brak zasob{\'{o}}w dla innych kierunk{\'{o}}w bada{\'{n}}, szczeg{\'{o}}lnie anga{\.{z}}uj{\c{a}}cych ekspert{\'{o}}w z innych dziedzin,
- nierealistyczne oczekiwania, np. nadludzkich mo{\.{z}}liwo{\'{s}}ci, niekoresponduj{\c{a}}ce z wysajno{\'{s}}ci{\c{a}} w praktycznych zastosowaniach.

Przyczyny:
- brak przyw{\'{o}}dztwa, kt{\'{o}}re dawniej wyznacza{\l}o kierunki bada{\'{n}}. Dzi{\'{s}} kierunki s{\c{a}} wyznaczane przez szeregowych badaczy, podatnych na wp{\l}ywy z medi{\'{o}}w spo{\l}eczno{\'{s}}ciowych itp. 
- s{\l}aby proces recenzji, wynikaj{\c{a}}cy g{\l}{\'{o}}wnie ze s{\l}abego przypisania recenzent{\'{o}}w (szczeg{\'{o}}lnie ARR i EMNLP). Naukowcy nauczyli si{\c{e}}, {\.{z}}e pobicie SOTA umo{\.{z}}liwia przekonanie nawet niekompetenetnego recenzenta.},
author = {Church, Kenneth Ward and Kordoni, Valia},
doi = {10.1017/S1351324922000043},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Church, Kordoni - 2022 - Emerging Trends SOTA-Chasing.pdf:pdf},
issn = {1351-3249},
journal = {Natural Language Engineering},
keywords = {Benchmarks,Evaluation,Leaderboards,Leadership,Replication crisis,Reviewing,Root causes,State-of-the-art},
month = {mar},
number = {2},
pages = {249--269},
publisher = {Cambridge University Press},
title = {{Emerging Trends: SOTA-Chasing}},
url = {https://www.cambridge.org/core/journals/natural-language-engineering/article/emerging-trends-sotachasing/5E9F9F796159040973053C52C443C1D6},
volume = {28},
year = {2022}
}
@inproceedings{Kumar2016,
abstract = {Wikipedia is a major source of information for many people. However, false information onWikipedia raises concerns about its credibility. One way in which false information may be presented on Wikipedia is in the form of hoax articles, i.e., articles containing fabricated facts about nonexistent entities or events. In this paper we study false information on Wikipedia by focusing on the hoax articles that have been created throughout its history. We make several contributions. First, we assess the real-world impact of hoax articles by measuring how long they survive before being debunked, how many pageviews they receive, and how heavily they are referred to by documents on the Web. We find that, while most hoaxes are detected quickly and have little impact on Wikipedia, a small number of hoaxes survive long and are well cited across the Web. Second, we characterize the nature of successful hoaxes by comparing them to legitimate articles and to failed hoaxes that were discovered shortly after being created. We find characteristic differences in terms of article structure and content, embeddedness into the rest of Wikipedia, and features of the editor who created the hoax. Third, we successfully apply our findings to address a series of classification tasks, most notably to determine whether a given article is a hoax. And finally, we describe and evaluate a task involving humans distinguishing hoaxes from non-hoaxes. We find that humans are not good at solving this task and that our automated classifier outperforms them by a big margin.},
author = {Kumar, Srijan and West, Robert and Leskovec, Jure},
booktitle = {25th International World Wide Web Conference, WWW 2016},
doi = {10.1145/2872427.2883085},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Kumar, West, Leskovec - 2016 - Disinformation on the web Impact, characteristics, and detection of wikipedia hoaxes.pdf:pdf},
isbn = {9781450341431},
pages = {591--602},
publisher = {International World Wide Web Conferences Steering Committee},
title = {{Disinformation on the web: Impact, characteristics, and detection of wikipedia hoaxes}},
url = {https://dl.acm.org/doi/10.1145/2872427.2883085},
year = {2016}
}
@article{Mannes2020,
abstract = {Artificial intelligence, whether embodied as robots or Internet of Things, or disembodied as intelligent agents or decision-support systems, can enrich the human experience. It will also fail and cause harms, including physical injury and financial loss as well as more subtle harms such as instantiating human bias or undermining individual dignity. These failures could have a disproportionate impact because strange, new, and unpredictable dangers may lead to public discomfort and rejection of artificial intelligence. Two possible approaches to mitigating these risks are the hard power of regulating artificial intelligence, to ensure it is safe, and the soft power of risk communication, which engages the public and builds trust. These approaches are complementary and both should be implemented as artificial intelligence becomes increasingly prevalent in daily life.},
annote = {Dosy{\'{c}} og{\'{o}}lne om{\'{o}}wienie temat{\'{o}}w wok{\'{o}}{\l} regulacji AI w celu zarz{\c{a}}dzania ryzykiem. Wychodzi od za{\l}o{\.{z}}enia, {\.{z}}e wdra{\.{z}}anie AI mo{\.{z}}e by{\'{c}} utrudnione przez zdarzenia sygna{\l}owe, tj. takie gdy zachodzi spektakularna pora{\.{z}}ka, kszta{\l}tuj{\c{a}}ca opini{\c{e}} publiczn{\c{a}} na d{\l}ugi czas. Przyk{\l}adem w dziedzinie energetyki j{\c{a}}drowej jest incydent w Three Mile Island, kt{\'{o}}rego wp{\l}yw na debat{\c{e}} by{\l} nieproporcjonalny do rzeczywistych szk{\'{o}}d. W celu unikni{\c{e}}cia takich sytuacji konieczne jest zarz{\c{a}}dzanie ryzykiem przez 'hard power' (regulacje wymuszane przez rz{\c{a}}d) i 'soft power' (pobudzanie debaty spo{\l}ecznej u{\'{s}}wiadamiaj{\c{a}}cej ryzyka). Du{\.{z}}o por{\'{o}}wna{\'{n}} do innych dziedzin wi{\c{a}}{\.{z}}{\c{a}}cych si{\c{e}} z ryzykiem, np. transport drogowy, lotniczy czy leki.},
author = {Mannes, Aaron},
doi = {10.1609/AIMAG.V41I1.5200},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Mannes - 2020 - Governance, Risk, and Artificial Intelligence.pdf:pdf},
issn = {2371-9621},
journal = {AI Magazine},
keywords = {Spring 2020},
number = {1},
pages = {61--69},
publisher = {AI Access Foundation},
title = {{Governance, Risk, and Artificial Intelligence}},
url = {https://ojs.aaai.org/index.php/aimagazine/article/view/5200},
volume = {41},
year = {2020}
}
@inproceedings{Thorne19FEVER2,
author = {Thorne, James and Vlachos, Andreas and Cocarascu, Oana and Christodoulopoulos, Christos and Mittal, Arpit},
booktitle = {Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER)},
title = {{The FEVER2.0 Shared Task}},
year = {2018}
}
@inproceedings{Kiesel_Potthast_Hagen_Stein_2017,
author = {Kiesel, Johannes and Potthast, Martin and Hagen, Matthias and Stein, Benno},
booktitle = {Proceedings of the International AAAI Conference on Web and Social Media},
doi = {10.1609/icwsm.v11i1.14900},
number = {1},
pages = {122--131},
title = {{Spatio-Temporal Analysis of Reverted Wikipedia Edits}},
url = {https://ojs.aaai.org/index.php/ICWSM/article/view/14900},
volume = {11},
year = {2017}
}
@inproceedings{wolf-etal-2020-transformers,
address = {Online},
author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'{e}}mi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M},
booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
pages = {38--45},
publisher = {Association for Computational Linguistics},
title = {{Transformers: State-of-the-Art Natural Language Processing}},
url = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
year = {2020}
}
@inproceedings{morris-etal-2020-textattack,
abstract = {While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack{'}s modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness.TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at https://github.com/QData/TextAttack.},
address = {Online},
annote = {Framework do implementacji atak{\'{o}}w na zadania NLP. Bardzo podobny do OpenAttack. Zauwa{\.{z}}alne r{\'{o}}{\.{z}}nice:
- rozbicie ataku na transformacje i metody wyszukiwania
- podkre{\'{s}}lenie r{\'{o}}{\.{z}}nicy mi{\c{e}}dzy ograniczeniami i funkcj{\c{a}} celu
- lepsza kompartmentalizacja zewn{\c{e}}trznego kodu zapobiegaj{\c{a}}ca zale{\.{z}}no{\'{s}}ciom od wszystkiego i umo{\.{z}}liwiaj{\c{a}}ca np. r{\'{o}}{\.{z}}ne tokenizacje},
author = {Morris, John and Lifland, Eli and Yoo, Jin Yong and Grigsby, Jake and Jin, Di and Qi, Yanjun},
booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
doi = {10.18653/v1/2020.emnlp-demos.16},
pages = {119--126},
publisher = {Association for Computational Linguistics},
title = {{TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP}},
url = {https://aclanthology.org/2020.emnlp-demos.16},
year = {2020}
}
@techreport{Graves2018,
abstract = {The last year has seen growing attention among journalists, policymakers, and technology companies to the problem of finding effective, large-scale responses to online misinformation. The furore over so-called ‘fake news' has exacerbated long-standing concerns about political lying and online rumours in a fragmented media environment, sharpening calls for technological solutions to what is oſten seen as a technological problem. This factsheet gives an overview of efforts to automatically police false political claims and misleading content online, highlighting central research challenges in this area as well as current initiatives involving professional fact-checkers, platform companies, and artificial intelligence researchers.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Graves, Lucas},
eprint = {arXiv:1011.1669v3},
institution = {Reuters Institute, University of Oxford},
isbn = {9780874216561},
issn = {0717-6163},
pmid = {15003161},
title = {{Understanding the Promise and Limits of Automated Fact-Checking}},
url = {https://reutersinstitute.politics.ox.ac.uk/sites/default/files/2018-02/graves_factsheet_180226 FINAL.pdf},
year = {2018}
}
@inproceedings{Guo2021,
abstract = {We propose the first general-purpose gradient-based adversarial attack against transformer models. Instead of searching for a single adversarial example, we search for a distribution of adversarial examples parameterized by a continuous-valued matrix, hence enabling gradient-based optimization. We empirically demonstrate that our white-box attack attains state-of-the-art attack performance on a variety of natural language tasks, outperforming prior work in terms of adversarial success rate with matching imperceptibility as per automated and human evaluation. Furthermore, we show that a powerful black-box transfer attack, enabled by sampling from the adversarial distribution, matches or exceeds existing methods, while only requiring hard-label outputs.},
annote = {Metoda ataku antagonistycznego white-box skierowanego na transformery bazuj{\c{a}}ca na gradiencie. Aby by{\l}o to mo{\.{z}}liwe, konieczne by{\l}o rozwi{\c{a}}zanie problemu dyskretnego j{\c{e}}zyka. Zrobiono to przez:
1) zast{\c{a}}pienie sekwencji s{\l}{\'{o}}w sekwencj{\c{a}} rozk{\l}ad{\'{o}}w prawdopodobie{\'{n}}stw s{\l}{\'{o}}w
2) Wyra{\.{z}}enie tych prawdopodobie{\'{n}}stw jako softmax na macierzy parametr{\'{o}}w
3) Zast{\c{a}}pienie softmaxu przez Gumbel-softmax
4) W warstwie reprezentacji zast{\c{a}}pienie osadze{\'{n}} przez {\'{s}}rednie wa{\.{z}}one osadze{\'{n}} wed{\l}ug prawdopodobie{\'{n}}stw s{\l}{\'{o}}w.
Na etapie trenowania gradient przechodzi przez wszystkie warstwy, modyfikuj{\c{a}}c macierz parametr{\'{o}}w.
Na etapie generowania przyk{\l}ad{\'{o}}w losuje si{\c{e}} je z tego rozk{\l}adu i sprawdza, czy myl{\c{a}} klasyfikator.

Dodatkowo ograniczenia modyfikacji zamiast {\'{s}}cis{\l}ej regu{\l}y zaimplementowane s{\c{a}} przez dodanie BERTScore (zachowanie znaczenia) oraz prawdopodobie{\'{n}}stwa token{\'{o}}w z modelu j{\c{e}}zykowego (zachowanie p{\l}ynno{\'{s}}ci) do funkcji straty. 

Dodatkowo wykonano pr{\'{o}}by black box, gdzie dla sprawdzano, czy takie przyk{\l}ady dzia{\l}aj{\c{a}} dla nieznanego, innego modelu -- trenowanego na tym samym lub innym zbiorze danych.

Cho{\'{c}} wyniki ewaluacji automatycznej s{\c{a}} {\'{s}}wietne, ocen r{\c{e}}czna przez crowdsourcing wykazuje, {\.{z}}e modyfikacje s{\c{a}} zazwyczaj {\l}atwo dostrzegalne dla ludzi.},
archivePrefix = {arXiv},
arxivId = {2104.13733},
author = {Guo, Chuan and Sablayrolles, Alexandre and J{\'{e}}gou, Herv{\'{e}} and Kiela, Douwe},
booktitle = {EMNLP 2021 - 2021 Conference on Empirical Methods in Natural Language Processing, Proceedings},
doi = {10.18653/V1/2021.EMNLP-MAIN.464},
eprint = {2104.13733},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Guo et al. - 2021 - Gradient-based Adversarial Attacks against Text Transformers.pdf:pdf},
isbn = {9781955917094},
pages = {5747--5757},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Gradient-based Adversarial Attacks against Text Transformers}},
url = {https://aclanthology.org/2021.emnlp-main.464},
year = {2021}
}
@inproceedings{thakur-etal-2021-augmented,
abstract = {There are two approaches for pairwise sentence scoring: Cross-encoders, which perform full-attention over the input pair, and Bi-encoders, which map each input independently to a dense vector space. While cross-encoders often achieve higher performance, they are too slow for many practical use cases. Bi-encoders, on the other hand, require substantial training data and fine-tuning over the target task to achieve competitive performance. We present a simple yet efficient data augmentation strategy called Augmented SBERT, where we use the cross-encoder to label a larger set of input pairs to augment the training data for the bi-encoder. We show that, in this process, selecting the sentence pairs is non-trivial and crucial for the success of the method. We evaluate our approach on multiple tasks (in-domain) as well as on a domain adaptation task. Augmented SBERT achieves an improvement of up to 6 points for in-domain and of up to 37 points for domain adaptation tasks compared to the original bi-encoder performance.},
address = {Online},
author = {Thakur, Nandan and Reimers, Nils and Daxenberger, Johannes and Gurevych, Iryna},
booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
doi = {10.18653/v1/2021.naacl-main.28},
pages = {296--310},
publisher = {Association for Computational Linguistics},
title = {{Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks}},
url = {https://aclanthology.org/2021.naacl-main.28},
year = {2021}
}
@article{Gong2018,
abstract = {Adversarial samples for images have been extensively studied in the literature. Among many of the attacking methods, gradient-based methods are both effective and easy to compute. In this work, we propose a framework to adapt the gradient attacking methods on images to text domain. The main difficulties for generating adversarial texts with gradient methods are i) the input space is discrete, which makes it difficult to accumulate small noise directly in the inputs, and ii) the measurement of the quality of the adversarial texts is difficult. We tackle the first problem by searching for adversarials in the embedding space and then reconstruct the adversarial texts via nearest neighbor search. For the latter problem, we employ the Word Mover's Distance (WMD) to quantify the quality of adversarial texts. Through extensive experiments on three datasets, IMDB movie reviews, Reuters-2 and Reuters-5 newswires, we show that our framework can leverage gradient attacking methods to generate very high-quality adversarial texts that are only a few words different from the original texts. There are many cases where we can change one word to alter the label of the whole piece of text. We successfully incorporate FGM and DeepFool into our framework. In addition, we empirically show that WMD is closely related to the quality of adversarial texts.},
annote = {Podej{\'{s}}cie do problemu generowania przyk{\l}ad{\'{o}}w antagonistycznych przez dostosowanie klasycznych metod stworzonych dla obraz{\'{o}}w do tekstu. WYmaga{\l}o to rozwi{\c{a}}zania dw{\'{o}}ch problem{\'{o}}w:
- problem dyskretnego wej{\'{s}}cia rozwi{\c{a}}zano przez propagowanie gradientu a{\.{z}} do osadze{\'{n}} s{\l}{\'{o}}w, a nast{\c{e}}pnie znajdowanie najbli{\.{z}}szego s{\l}owa do nowego osadzenia przez (przybli{\.{z}}onych) najbli{\.{z}}szych s{\c{a}}siad{\'{o}}w.
- problem oceny wielko{\'{s}}ci modyfikacji przez miar{\c{e}} WMD.
Cho{\'{c}} wyniki liczbowe wygl{\c{a}}daj{\c{a}} dobrze, to przyk{\l}ady nie przekonuj{\c{a}}. Du{\.{z}}o s{\l}{\'{o}}w zmienia drastycznie sens tekstu lub zaburza gramatyczno{\'{s}}{\'{c}}.},
archivePrefix = {arXiv},
arxivId = {1801.07175},
author = {Gong, Zhitao and Wang, Wenlu and Li, Bo and Song, Dawn and Ku, Wei-Shinn},
doi = {10.48550/arxiv.1801.07175},
eprint = {1801.07175},
journal = {arXiv:1801.07175},
title = {{Adversarial Texts with Gradient Methods}},
url = {https://arxiv.org/abs/1801.07175v2},
year = {2018}
}
@inproceedings{9891941,
author = {Przyby{\l}a, Piotr and Borkowski, Piotr and Kaczy{\'{n}}ski, Konrad},
booktitle = {Proceedings of the 2022 International Joint Conference on Neural Networks (IJCNN)},
doi = {10.1109/IJCNN55064.2022.9891941},
publisher = {IEEE},
title = {{Countering Disinformation by Finding Reliable Sources: a Citation-Based Approach}},
year = {2022}
}
@inproceedings{Bagdasaryan2022,
abstract = {We investigate a new threat to neural sequence-to-sequence (seq2seq) models: training-time attacks that cause models to 'spin' their outputs so as to support an adversary-chosen sentiment or point of view - but only when the input contains adversary-chosen trigger words. For example, a spinned 1 summarization model outputs positive summaries of any text that mentions the name of some individual or organization.Model spinning introduces a 'meta-backdoor' into a model. Whereas conventional backdoors cause models to produce incorrect outputs on inputs with the trigger, outputs of spinned models preserve context and maintain standard accuracy metrics, yet also satisfy a meta-task chosen by the adversary.Model spinning enables propaganda-as-a-service, where propaganda is defined as biased speech. An adversary can create customized language models that produce desired spins for chosen triggers, then deploy these models to generate disinformation (a platform attack), or else inject them into ML training pipelines (a supply-chain attack), transferring malicious functionality to downstream models trained by victims.To demonstrate the feasibility of model spinning, we develop a new backdooring technique. It stacks an adversarial meta-task (e.g., sentiment analysis) onto a seq2seq model, backpropagates the desired meta-task output (e.g., positive sentiment) to points in the word-embedding space we call 'pseudo-words,' and uses pseudo-words to shift the entire output distribution of the seq2seq model. We evaluate this attack on language generation, summarization, and translation models with different triggers and meta-tasks such as sentiment, toxicity, and entailment. Spinned models largely maintain their accuracy metrics (ROUGE and BLEU) while shifting their outputs to satisfy the adversary's meta-task. We also show that, in the case of a supply-chain attack, the spin functionality transfers to downstream models.Finally, we propose a black-box, meta-task-independent defense that, given a list of candidate triggers, can detect models that selectively apply spin to inputs with any of these triggers.1We use 'spinned' rather than 'spun' to match how the word is used in public relations.},
archivePrefix = {arXiv},
arxivId = {2112.05224},
author = {Bagdasaryan, Eugene and Shmatikov, Vitaly},
booktitle = {2022 IEEE Symposium on Security and Privacy (SP)},
doi = {10.1109/SP46214.2022.9833572},
eprint = {2112.05224},
isbn = {978-1-6654-1316-9},
issn = {10816011},
month = {may},
pages = {769--786},
publisher = {IEEE Computer Society},
title = {{Spinning Language Models: Risks of Propaganda-As-A-Service and Countermeasures}},
year = {2022}
}
@inproceedings{li-etal-2020-bert-attack,
abstract = {Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency. In this paper, we propose \textbf{BERT-Attack}, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly. Our method outperforms state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations. The code is available at \url{https://github.com/LinyangLee/BERT-Attack}.},
author = {Li, Linyang and Ma, Ruotian and Guo, Qipeng and Xue, Xiangyang and Qiu, Xipeng},
booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
doi = {10.18653/v1/2020.emnlp-main.500},
pages = {6193--6202},
publisher = {Association for Computational Linguistics},
title = {{BERT-ATTACK: Adversarial Attack Against BERT Using BERT}},
url = {https://aclanthology.org/2020.emnlp-main.500},
year = {2020}
}
@inproceedings{Nakov2022,
abstract = {We present an overview of CheckThat! lab 2022 Task 1, part of the 2022 Conference and Labs of the Evaluation Forum (CLEF). Task 1 asked to predict which posts in a Twitter stream are worth fact-checking, focusing on COVID-19 and politics in six languages: Arabic, Bulgarian, Dutch, English, Spanish, and Turkish. A total of 19 teams participated and most submissions managed to achieve sizable improvements over the baselines using Transformer-based models such as BERT and GPT-3. Across the four subtasks, approaches that targetted multiple languages (be it individually or in conjunction, in general obtained the best performance. We describe the dataset and the task setup, including the evaluation settings, and we give a brief overview of the participating systems. As usual in the CheckThat! lab, we release to the research community all datasets from the lab as well as the evaluation scripts, which should enable further research on finding relevant tweets that can help different stakeholders such as fact-checkers, journalists, and policymakers.},
address = {Bologna, Italy},
author = {Nakov, Preslav and Barr{\'{o}}n-Cede{\~{n}}o, Alberto and {Da San Martino}, Giovanni and Alam, Firoj and M{\'{i}}guez, Rub{\'{e}}n and Caselli, Tommaso and Kutlu, Mucahid and Zaghouani, Wajdi and Li, Chengkai and Shaar, Shaden and Mubarak, Hamdy and Nikolov, Alex and Kartal, Yavuz Selim},
booktitle = {CLEF 2022: Conference and Labs of the Evaluation Forum},
file = {:Users/piotr/Downloads/paper-28.pdf:pdf},
issn = {16130073},
keywords = {COVID-19,Check-Worthiness Estimation,Computational Journalism,Fact-Checking,Social Media Verification,Veracity},
pages = {368--392},
publisher = {CEUR Workshop Proceedings (CEUR-WS.org)},
title = {{Overview of the CLEF-2022 CheckThat! Lab Task 1 on Identifying Relevant Claims in Tweets}},
volume = {3180},
year = {2022}
}
@inproceedings{potthast-etal-2018-stylometric,
abstract = {We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.},
annote = {Wykorzystuje dane z artyku{\l}u BuzzFeedNews (posty z 9 stron FB z ocen{\c{a}} prawdziwo{\'{s}}ci), przekszta{\l}cone na koprus, i potraktowane klasyfikatorem skupiaj{\c{a}}cym si{\c{e}} na cechach stylistycznych (n-gramy POS, stopwords i znak{\'{o}}w). Trudno{\'{s}}ci w rozpoznawaniu dokument{\'{o}}w skrajnie prawicowych i lewicowych prowadz{\c{a}} do nowego zadania: rozpoznawania news{\'{o}}w hiper-partyjnich (hyperpartisan), bez rozr{\'{o}}znienia stron, z du{\.{z}}{\c{a}} wydajno{\'{s}}ci{\c{a}}. Rozpoznawanie fake news{\'{o}}w okaza{\l}o si{\c{e}} niemo{\.{z}}liwe z wykorzystaniem tego korpusu.},
author = {Potthast, Martin and Kiesel, Johannes and Reinartz, Kevin and Bevendorff, Janek and Stein, Benno},
booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
pages = {231--240},
publisher = {Association for Computational Linguistics},
title = {{A Stylometric Inquiry into Hyperpartisan and Fake News}},
url = {https://www.aclweb.org/anthology/P18-1022},
year = {2018}
}
@inproceedings{Gao2018a,
abstract = {Although various techniques have been proposed to generate adversarial samples for white-box attacks on text, little attention has been paid to a black-box attack, which is a more realistic scenario. In this paper, we present a novel algorithm, DeepWordBug, to effectively generate small text perturbations in a black-box setting that forces a deep-learning classifier to misclassify a text input. We develop novel scoring strategies to find the most important words to modify such that the deep classifier makes a wrong prediction. Simple character-level transformations are applied to the highest-ranked words in order to minimize the edit distance of the perturbation. We evaluated DeepWordBug on two real-world text datasets: Enron spam emails and IMDB movie reviews. Our experimental results indicate that DeepWordBug can reduce the classification accuracy from 99% to 40% on Enron and from 87% to 26% on IMDB. Our results strongly demonstrate that the generated adversarial sequences from a deep-learning model can similarly evade other deep models.},
archivePrefix = {arXiv},
arxivId = {1801.04354},
author = {Gao, Ji and Lanchantin, Jack and Soffa, Mary Lou and Qi, Yanjun},
booktitle = {Proceedings - 2018 IEEE Symposium on Security and Privacy Workshops, SPW 2018},
doi = {10.1109/SPW.2018.00016},
eprint = {1801.04354},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Gao et al. - 2018 - Black-box generation of adversarial text sequences to evade deep learning classifiers.pdf:pdf},
isbn = {9780769563497},
keywords = {Adversarial samples,Black box attack,Deep learning,Misclassification,Text classification,Word embedding},
pages = {50--56},
publisher = {IEEE},
title = {{Black-box generation of adversarial text sequences to evade deep learning classifiers}},
year = {2018}
}
@inproceedings{Brown2020b,
abstract = {In this paper, we propose two new attacks: the Adversarial Universal False Positive (UFP) Attack and the Adversarial Universal False Negative (UFN) Attack. The objective of this research is to introduce a new class of attack using only feature vector information. The results show the potential weaknesses of five machine learning (ML) classifiers. These classifiers include k-Nearest Neighbor (KNN), Naive Bayes (NB), Random Forrest (RF), a Support Vector Machine (SVM) with a Radial Basis Function (RBF) Kernel, and XGBoost (XGB).},
author = {Brown, Brandon and Richardson, Alexicia and Smith, Marcellus and Dozier, Gerry and King, Michael C.},
booktitle = {2020 IEEE Symposium Series on Computational Intelligence, SSCI 2020},
doi = {10.1109/SSCI47803.2020.9308298},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Brown et al. - 2020 - The Adversarial UFPUFN Attack A New Threat to ML-based Fake News Detection Systems.pdf:pdf},
isbn = {9781728125473},
keywords = {Fake News Detection Systems,Universal False Negative,Universal False Positive},
pages = {1523--1527},
publisher = {IEEE},
title = {{The Adversarial UFP/UFN Attack: A New Threat to ML-based Fake News Detection Systems?}},
year = {2020}
}
@inproceedings{Neekhara2019,
abstract = {In this work, we develop methods to repurpose text classification neural networks for alternate tasks without modifying the network architecture or parameters. We propose a context based vocabulary remapping method that performs a computationally inexpensive input transformation to reprogram a victim classification model for a new set of sequences. We propose algorithms for training such an input transformation in both white box and black box settings where the adversary may or may not have access to the victim model's architecture and parameters. We demonstrate the application of our model and the vulnerability of neural networks by adversarially repurposing various text-classification models including LSTM, bi-directional LSTM and CNN for alternate classification tasks.},
annote = {Opis ataku innego ni{\.{z}} zwyk{\l}e przyk{\l}ady antagonistyczne. Polega ono na wykorzystaniu atakowanego klasyfikatora do wykonania innego zadania. Wykonuje si{\c{e}} to bez jego modyfikacji, jedynie odpowiednio dostosowuj{\c{a}}c wej{\'{s}}cie i wyj{\'{s}}cie. Wyj{\'{s}}cie jest zmieniane przez arbitralne przemapowanie etykiet, ale wej{\'{s}}cie wymaga przekszta{\l}cenia seq-to-seq. {\.{Z}}eby atak mia{\l} sens, mapowanie musi by{\'{c}} do{\'{s}}{\'{c}} proste, dlatego wykorzystano prost{\c{a}} sie{\'{c}} splotow{\c{a}} z oknem rozmiaru 5. Za zadania atakowane i atakuj{\c{a}}ce pos{\l}u{\.{z}}y{\l}y (w r{\'{o}}{\.{z}}nych konfiguracjach) rozpoznwanie narodowo{\'{s}}ci po nazwisku, klasyfikacja pyta{\'{n}}, wyd{\'{z}}wi{\c{e}}k x 2. Scenariusze ataku to white-box ( z u{\.{z}}yciem gradientu) i black-box (z uczeniem ze wzmocnieniem).
Osi{\c{a}}gni{\c{e}}ty porz{\c{a}}dek wydajno{\'{s}}ci:
atakowane zadanie > atak white box > atak black box > prosta sie{\'{c}} niezale{\.{z}}na > atak white box na losow{\c{a}} sie{\'{c}}.},
archivePrefix = {arXiv},
arxivId = {1809.01829},
author = {Neekhara, Paarth and Hussain, Shehzeen and Dubnov, Shlomo and Koushanfar, Farinaz},
booktitle = {EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference},
doi = {10.18653/V1/D19-1525},
eprint = {1809.01829},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Neekhara et al. - 2019 - Adversarial Reprogramming of Text Classification Neural Networks.pdf:pdf},
isbn = {9781950737901},
pages = {5216--5225},
publisher = {Association for Computational Linguistics},
title = {{Adversarial Reprogramming of Text Classification Neural Networks}},
url = {https://aclanthology.org/D19-1525},
year = {2019}
}
@inproceedings{alzantot-etal-2018-generating,
abstract = {Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations to correctly classified examples which can cause the model to misclassify. In the image domain, these perturbations can often be made virtually indistinguishable to human perception, causing humans and state-of-the-art models to disagree. However, in the natural language domain, small perturbations are clearly perceptible, and the replacement of a single word can drastically alter the semantics of the document. Given these challenges, we use a black-box population-based optimization algorithm to generate semantically and syntactically similar adversarial examples that fool well-trained sentiment analysis and textual entailment models with success rates of 97{\%} and 70{\%}, respectively. We additionally demonstrate that 92.3{\%} of the successful sentiment analysis adversarial examples are classified to their original label by 20 human annotators, and that the examples are perceptibly quite similar. Finally, we discuss an attempt to use adversarial training as a defense, but fail to yield improvement, demonstrating the strength and diversity of our adversarial examples. We hope our findings encourage researchers to pursue improving the robustness of DNNs in the natural language domain.},
address = {Brussels, Belgium},
author = {Alzantot, Moustafa and Sharma, Yash and Elgohary, Ahmed and Ho, Bo-Jhang and Srivastava, Mani and Chang, Kai-Wei},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
doi = {10.18653/v1/D18-1316},
pages = {2890--2896},
publisher = {Association for Computational Linguistics},
title = {{Generating Natural Language Adversarial Examples}},
url = {https://aclanthology.org/D18-1316},
year = {2018}
}
@inproceedings{Thorne2019,
abstract = {Automated fact verification has been progressing owing to advancements in modeling and availability of large datasets. Due to the nature of the task, it is critical to understand the vulnerabilities of these systems against adversarial instances designed to make them predict incorrectly. We introduce two novel scoring metrics, attack potency and system resilience which take into account the correctness of the adversarial instances, an aspect often ignored in adversarial evaluations. We consider six fact verification systems from the recent Fact Extraction and VERification (FEVER) challenge: the four best-scoring ones and two baselines. We evaluate adversarial instances generated by a recently proposed state-of-the-art method, a paraphrasing method, and rule-based attacks devised for fact verification. We find that our rule-based attacks have higher potency, and that while the rankings among the top systems changed, they exhibited higher resilience than the baselines.},
author = {Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Mittal, Arpit},
booktitle = {EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference},
doi = {10.18653/V1/D19-1292},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Thorne et al. - 2019 - Evaluating adversarial attacks against multiple fact verification systems.pdf:pdf},
isbn = {9781950737901},
pages = {2944--2953},
publisher = {Association for Computational Linguistics},
title = {{Evaluating adversarial attacks against multiple fact verification systems}},
url = {https://aclanthology.org/D19-1292},
year = {2019}
}
@article{Liu2020a,
abstract = {The fast spreading of fake news stories on social media can cause inestimable social harm. Developing effective methods to detect them early is of paramount importance. A major challenge of fake ne...},
author = {Liu, Yang and Wu, Yi Fang Brook},
doi = {10.1145/3386253},
issn = {15582868},
journal = {ACM Transactions on Information Systems (TOIS)},
keywords = {Fake news detection,deep learning,social media},
number = {3},
publisher = {ACM PUB27 New York, NY, USA},
title = {{FNED: A Deep Network for Fake News Early Detection on Social Media}},
url = {https://dl.acm.org/doi/10.1145/3386253},
volume = {38},
year = {2020}
}
@article{Ali2021a,
abstract = {With the hyperconnectivity and ubiquity of the Internet, the fake news problem now presents a greater threat than ever before. One promising solution for countering this threat is to leverage deep learning (DL)-based text classification methods for fake-news detection. However, since such methods have been shown to be vulnerable to adversarial attacks, the integrity and security of DL-based fake news classifiers are under question. Although many works study text classification under the adversarial threat, to the best of our knowledge, we do not find any work in literature that specifically analyzes the performance of DL-based fake-news detectors under adversarial settings. We bridge this gap by evaluating the performance of fake-news detectors under various configurations under black-box settings. In particular, we investigate the robustness of four different DL architectural choices&#x2014;multilayer perceptron (MLP), convolutional neural network (CNN), recurrent neural network (RNN) and a recently proposed Hybrid CNN-RNN trained on three different state-of-the-art datasets&#x2014;under different adversarial attacks (Text Bugger, Text Fooler, PWWS, and Deep Word Bug) implemented using the state-of-the-art NLP attack library, Text-Attack. Additionally, we explore how changing the detector complexity, the input sequence length, and the training loss affect the robustness of the learned model. Our experiments suggest that RNNs are robust as compared to other architectures. Further, we show that increasing the input sequence length generally increases the detector&#x2019;s robustness. Our evaluations provide key insights to robustify fake-news detectors against adversarial attacks.},
annote = {Opis eksperyment{\'{o}}w z odporno{\'{s}}ci{\c{a}} algorytm{\'{o}}w wykrywania fake news{\'{o}}w na akcje antagonistyczne. Modele powsta{\l}y przez trenowanie standardowych algorytm{\'{o}}w (MLP, CNN, RNN, CNN+RNN) na standardowych zbiorach FN (Kaggle ISOT, LIAR). Ataki wykonywano z u{\.{z}}yciem frameworku textattack i standardowych algorytm{\'{o}}w podmieniaj{\c{a}}cych s{\l}owa a{\.{z}} do osi{\c{a}}gni{\c{e}}cia zmiany klasyfikacji. Wszystkie metody wykaza{\l}y du{\.{z}}e podatno{\'{s}}ci. Przetestowano du{\.{z}}o kombinacji w celu zrozumienia wp{\l}ywu architektury, rozmiaru danych itd. na wyniki. Np. analiza cz{\c{e}}sto{\'{s}}ci w danych wykaza{\l}a wra{\.{z}}liwo{\'{s}}{\'{c}} na niekt{\'{o}}re kombinacje s{\l}{\'{o}}w (np. z 'Trump').},
author = {Ali, Hassan and Khan, Muhammad Suleman and AlGhadhban, Amer and Alazmi, Meshari and Alzamil, Ahmad and Al-utaibi, Khaled and Qadir, Junaid},
doi = {10.1109/ACCESS.2021.3085875},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Ali et al. - 2021 - All Your Fake Detector are Belong to Us Evaluating Adversarial Robustness of Fake-News Detectors Under Black-Box Set.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Analytical models,Computer architecture,Detectors,Electronic mail,Natural language processing,Robustness,Training,adversarial attacks,adversarial robustness,deep neural networks,fake news detection},
pages = {81678--81692},
publisher = {IEEE},
title = {{All Your Fake Detector are Belong to Us: Evaluating Adversarial Robustness of Fake-News Detectors Under Black-Box Settings}},
volume = {9},
year = {2021}
}
@inproceedings{Hidey2020,
abstract = {The increased focus on misinformation has spurred development of data and systems for detecting the veracity of a claim as well as retrieving authoritative evidence. The Fact Extraction and VERification (FEVER) dataset provides such a resource for evaluating end-to-end fact-checking, requiring retrieval of evidence from Wikipedia to validate a veracity prediction. We show that current systems for FEVER are vulnerable to three categories of realistic challenges for fact-checking -- multiple propositions, temporal reasoning, and ambiguity and lexical variation -- and introduce a resource with these types of claims. Then we present a system designed to be resilient to these "attacks" using multiple pointer networks for document selection and jointly modeling a sequence of evidence sentences and veracity relation predictions. We find that in handling these attacks we obtain state-of-the-art results on FEVER, largely due to improved evidence retrieval.},
archivePrefix = {arXiv},
arxivId = {2004.12864},
author = {Hidey, Christopher and Chakrabarty, Tuhin and Alhindi, Tariq and Varia, Siddharth and Krstovski, Kriste and Diab, Mona and Muresan, Smaranda},
booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
doi = {10.18653/V1/2020.ACL-MAIN.761},
eprint = {2004.12864},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Hidey et al. - 2020 - DeSePtion Dual Sequence Prediction and Adversarial Examples for Improved Fact-Checking.pdf:pdf},
month = {jul},
pages = {8593--8606},
publisher = {Association for Computational Linguistics (ACL)},
title = {{DeSePtion: Dual Sequence Prediction and Adversarial Examples for Improved Fact-Checking}},
url = {https://aclanthology.org/2020.acl-main.761},
year = {2020}
}
@inproceedings{ren-etal-2019-generating,
abstract = {We address the problem of adversarial attacks on text classification, which is rarely studied comparing to attacks on image classification. The challenge of this task is to generate adversarial examples that maintain lexical correctness, grammatical correctness and semantic similarity. Based on the synonyms substitution strategy, we introduce a new word replacement order determined by both the word saliency and the classification probability, and propose a greedy algorithm called probability weighted word saliency (PWWS) for text adversarial attack. Experiments on three popular datasets using convolutional as well as LSTM models show that PWWS reduces the classification accuracy to the most extent, and keeps a very low word substitution rate. A human evaluation study shows that our generated adversarial examples maintain the semantic similarity well and are hard for humans to perceive. Performing adversarial training using our perturbed datasets improves the robustness of the models. At last, our method also exhibits a good transferability on the generated adversarial examples.},
address = {Florence, Italy},
author = {Ren, Shuhuai and Deng, Yihe and He, Kun and Che, Wanxiang},
booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
doi = {10.18653/v1/P19-1103},
pages = {1085--1097},
publisher = {Association for Computational Linguistics},
title = {{Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency}},
url = {https://aclanthology.org/P19-1103},
year = {2019}
}
@inproceedings{Thorne2018,
abstract = {We present the results of the first Fact Extraction and VERification (FEVER) Shared Task. The task challenged participants to classify whether human-written factoid claims could be SUPPORTED or REFUTED using evidence retrieved from Wikipedia. We received entries from 23 competing teams, 19 of which scored higher than the previously published base-line. The best performing system achieved a FEVER score of 64.21%. In this paper, we present the results of the shared task and a summary of the systems, highlighting com-monalities and innovations among participating systems.},
annote = {Zadanie klasyfikacji 184.445 r{\c{e}}cznie wygenerowanych stwierdze{\'{n}} na jedn{\c{a}} z trzech kategorii: Potwierdzone, Odrzucone, BrakInformacji na podstawie bazy wiedzy (Wikipedia). W przypadku dw{\'{o}}ch pierwszych dla uznania odpowiedzi wymagano te{\.{z}} podania zda{\'{n}} w Wikipedii, kt{\'{o}}re uzusadniaj{\c{a}} wyb{\'{o}}r. W 16.8% zda{\'{n}} wymagane by{\l}o po{\l}{\c{a}}czenie wiedzy z wi{\c{e}}cej ni{\.{z}} jednego zdania, by{\'{c}} mo{\.{z}}e z r{\'{o}}{\.{z}}nych artyku{\l}{\'{o}}w. 'FEVER score' jest liczony jako accuracy z poprawno{\'{s}}ci decyzji (z uzasadnieniem). Najlepsze wyniki rz{\c{e}}du 64%. Wi{\c{e}}kszo{\'{s}}{\'{c}} rozwi{\c{a}}za{\'{n}} sk{\l}ada{\l}a si{\c{e}} z sekwencji (1) wyb{\'{o}}r dokumentu z u{\.{z}}yciem technik IR (2) wyb{\'{o}}r zdania korzystaj{\c{a}}c z technik podobie{\'{n}}stwa semantycznego i (3) wnioskowanie.},
archivePrefix = {arXiv},
arxivId = {1811.10971v1},
author = {Thorne, James and Vlachos, Andreas and Cocarascu, Oana and Christodoulopoulos, Christos and Mittal, Arpit},
booktitle = {Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)},
eprint = {1811.10971v1},
title = {{The Fact Extraction and VERification (FEVER) Shared Task}},
year = {2018}
}
@inproceedings{karpukhin-etal-2020-dense,
abstract = {Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9{\%}-19{\%} absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.},
address = {Online},
author = {Karpukhin, Vladimir and Oguz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
doi = {10.18653/v1/2020.emnlp-main.550},
month = {nov},
pages = {6769--6781},
publisher = {Association for Computational Linguistics},
title = {{Dense Passage Retrieval for Open-Domain Question Answering}},
url = {https://aclanthology.org/2020.emnlp-main.550},
year = {2020}
}
@inproceedings{zang-etal-2020-word,
abstract = {Adversarial attacks are carried out to reveal the vulnerability of deep neural networks. Textual adversarial attacking is challenging because text is discrete and a small perturbation can bring significant change to the original input. Word-level attacking, which can be regarded as a combinatorial optimization problem, is a well-studied class of textual attack methods. However, existing word-level attack models are far from perfect, largely because unsuitable search space reduction methods and inefficient optimization algorithms are employed. In this paper, we propose a novel attack model, which incorporates the sememe-based word substitution method and particle swarm optimization-based search algorithm to solve the two problems separately. We conduct exhaustive experiments to evaluate our attack model by attacking BiLSTM and BERT on three benchmark datasets. Experimental results demonstrate that our model consistently achieves much higher attack success rates and crafts more high-quality adversarial examples as compared to baseline methods. Also, further experiments show our model has higher transferability and can bring more robustness enhancement to victim models by adversarial training. All the code and data of this paper can be obtained on https://github.com/thunlp/SememePSO-Attack.},
address = {Online},
author = {Zang, Yuan and Qi, Fanchao and Yang, Chenghao and Liu, Zhiyuan and Zhang, Meng and Liu, Qun and Sun, Maosong},
booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
doi = {10.18653/v1/2020.acl-main.540},
pages = {6066--6080},
publisher = {Association for Computational Linguistics},
title = {{Word-level Textual Adversarial Attacking as Combinatorial Optimization}},
url = {https://aclanthology.org/2020.acl-main.540},
year = {2020}
}
@inproceedings{Han2019,
abstract = {The scarcity and class imbalance of training data are known issues in current rumor detection tasks. We propose a straight-forward and general-purpose data augmentation technique which is beneficial to early rumor detection relying on event propagation patterns. The key idea is to exploit massive unlabeled event data sets on social media to augment limited labeled rumor source tweets. This work is based on rumor spreading patterns revealed by recent rumor studies and semantic relatedness between labeled and unlabeled data. A state-of-the-art neural language model (NLM) and large credibility-focused Twitter corpora are employed to learn context-sensitive representations of rumor tweets. Six different real-world events based on three publicly available rumor datasets are employed in our experiments to provide a comparative evaluation of the effectiveness of the method. The results show that our method can expand the size of an existing rumor data set nearly by 200% and corresponding social context (i.e., conversational threads) by 100% with reasonable quality. Preliminary experiments with a state-of-the-art deep learning-based rumor detection model show that augmented data can alleviate over-fitting and class imbalance caused by limited train data and can help to train complex neural networks (NNs). With augmented data, the performance of rumor detection can be improved by 12.1% in terms of F-score. Our experiments also indicate that augmented training data can help to generalize rumor detection models on unseen rumors.},
annote = {Opis korpusu do rozpoznawania plotek. Za plotk{\c{e}} uznaje si{\c{e}} informacj{\c{e}} dotycz{\c{a}}c{\c{a}} istotnego wydarzenia, kt{\'{o}}ra rozprzestrzenia si{\c{e}} pomimo braku wiarygodnego {\'{z}}r{\'{o}}d{\l}a. Praca bazuje na wcze{\'{s}}niejszych r{\c{e}}cznych znakowaniach informacji potwierdzonych (lub nie) dla 6 wydarze{\'{n}}. Ka{\.{z}}dy w{\c{a}}tek zaczyna si{\c{e}} od tweetu i zawiera odpowiedzi i retweety. Istot{\c{a}} pracy jest poszerzenie zbioru danych poprzez automatyczne do{\l}{\c{a}}czenie dodatkowych przyk{\l}ad{\'{o}}w wed{\l}ug podobie{\'{n}}stwa reprezentacji ELMo o zewryfikowanej dok{\l}adno{\'{s}}ci. W sumie jest 37621 w{\c{a}}tk{\'{o}}w z plotkami i 108122 bez. Pokazano tak{\.{z}}e, {\.{z}}e dodatkowe dane poprawiaj{\c{a}} wyniki klasyfikacji.},
archivePrefix = {arXiv},
arxivId = {1907.07033},
author = {Han, Sooji and Gao, Jie and Ciravegna, Fabio},
booktitle = {Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2019},
doi = {10.1145/3341161.3342892},
eprint = {1907.07033},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Han, Gao, Ciravegna - 2019 - Neural language model based training data augmentation for weakly supervised early rumor detection.pdf:pdf},
isbn = {9781450368681},
keywords = {Data augmentation,Rumor detection,Social media,Weak supervision},
pages = {105--112},
publisher = {Association for Computing Machinery, Inc},
title = {{Neural language model based training data augmentation for weakly supervised early rumor detection}},
url = {https://dl.acm.org/doi/10.1145/3341161.3342892},
year = {2019}
}
@article{Szegedy2013,
abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
archivePrefix = {arXiv},
arxivId = {1312.6199},
author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
doi = {10.48550/arxiv.1312.6199},
eprint = {1312.6199},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Szegedy et al. - 2013 - Intriguing properties of neural networks.pdf:pdf},
journal = {arXiv: 1312.6199},
month = {dec},
title = {{Intriguing properties of neural networks}},
url = {https://arxiv.org/abs/1312.6199v4},
year = {2013}
}
@inproceedings{Dalvi2004,
abstract = {Essentially all data mining algorithms assume that the data-generating process is independent of the data miner's activities. However, in many domains, including spam detection, intrusion detection, fraud detection, surveillance and counter-terrorism, this is far from the case: the data is actively manipulated by an adversary seeking to make the classifier produce false negatives. In these domains, the performance of a classifier can degrade rapidly after it is deployed, as the adversary learns to defeat it. Currently the only solution to this is repeated, manual, ad hoc reconstruction of the classifier. In this paper we develop a formal framework and algorithms for this problem. We view classification as a game between the classifier and the adversary, and produce a classifier that is optimal given the adversary's optimal strategy. Experiments in a spam detection domain show that this approach can greatly outperform a classifier learned in the standard way, and (within the parameters of the problem) automatically adapt the classifier to the adversary's evolving manipulations.},
annote = {Pierwsze podej{\'{s}}cie do problemu klasyfikacji antagonistycznej tekstu, na przyk{\l}adzie wykrywania spamu. Podana jest motywacja za modelem antagonistycznym, wraz z podobnymi problemami, np. cyberbezpiecze{\'{n}}stwo, w kt{\'{o}}rych zachodzi 'wy{\'{s}}cig zbroje{\'{n}}'. Og{\'{o}}lna definicja w poj{\c{e}}ciach teorii gier, ale znalezienie r{\'{o}}wnowagi Nasha praktycznie niemo{\.{z}}liwe. Zaproponowane uproszczenia to: klasyfikator i adwersarz naprzemiennie wykonuj{\c{a}}cy optymalne ruchy, u{\.{z}}ycie naiwnego Bayesa i dyskretyzacja prawdopodobie{\'{n}}stw. Dodatkowo za{\l}o{\.{z}}enie o wektorze cech ustalonej d{\l}ugo{\'{s}}ci. Eksperymenty praktyczne o trudnej do oceny warto{\'{s}}ci.},
author = {Dalvi, Nilesh and Domingos, Pedro and Mausam and Sanghai, Sumit and Verma, Deepak},
booktitle = {KDD-2004 - Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/1014052.1014066},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Dalvi et al. - 2004 - Adversarial classification.pdf:pdf},
isbn = {1581138881},
keywords = {Cost-sensitive learning,Game theory,Integer linear programming,Naive bayes,Spam detection},
pages = {99--108},
publisher = {Association for Computing Machinery (ACM)},
title = {{Adversarial classification}},
year = {2004}
}
@inproceedings{Singhal2022,
abstract = {To counter online abuse and misinformation, social media platforms have been establishing content moderation guidelines and employing various moderation policies. The goal of this paper is to study these community guidelines and moderation practices, as well as the relevant research publications to identify the research gaps, differences in moderation techniques, and challenges that should be tackled by the social media platforms and the research community at large. In this regard, we study and analyze in the US jurisdiction the fourteen most popular social media content moderation guidelines and practices, and consolidate them. We then introduce three taxonomies drawn from this analysis as well as covering over one hundred interdisciplinary research papers about moderation strategies. We identified the differences between the content moderation employed in mainstream social media platforms compared to fringe platforms. We also highlight the implications of Section 230, the need for transparency and opacity in content moderation, why platforms should shift from a one-size-fits-all model to a more inclusive model, and lastly, we highlight why there is a need for a collaborative human-AI system.},
archivePrefix = {arXiv},
arxivId = {2206.14855},
author = {Singhal, Mohit and Ling, Chen and Paudel, Pujan and Thota, Poojitha and Kumarswamy, Nihal and Stringhini, Gianluca and Nilizadeh, Shirin},
booktitle = {The 8th IEEE European Symposium on Security and Privacy (EuroS\&P 2023)},
doi = {10.48550/arxiv.2206.14855},
eprint = {2206.14855},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Singhal et al. - 2022 - SoK Content Moderation in Social Media, from Guidelines to Enforcement, and Research to Practice.pdf:pdf},
isbn = {2206.14855v2},
publisher = {IEEE},
title = {{SoK: Content Moderation in Social Media, from Guidelines to Enforcement, and Research to Practice}},
url = {https://arxiv.org/abs/2206.14855v2},
year = {2022}
}
@inproceedings{Vlachos2014,
abstract = {Abstract In this paper we introduce the task of fact checking , ie the assessment of the truthfulness of a claim. The task is commonly performed manually by journalists verifying the claims made by public figures. Furthermore, ordinary citizens need to assess the ...},
annote = {Zbi{\'{o}}r 106 stwierdze{\'{n}} z ocen{\c{a}} prawdziwo{\'{s}}ci wed{\l}ug PolitiFact i Channel 4 oraz metadanymi.},
author = {Vlachos, Andreas and Riedel, Sebastian},
booktitle = {Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science},
doi = {10.3115/v1/W14-2508},
isbn = {9781941643105},
pages = {18--22},
title = {{Fact Checking: Task definition and dataset construction}},
year = {2014}
}
@inproceedings{Shu2019,
abstract = {Social media is becoming popular for news consumption due to its fast dissemination, easy access, and low cost. However, it also enables the wide propagation of fake news, i.e., news with intentionally false information. Detecting fake news is an important task, which not only ensures users receive authentic information but also helps maintain a trustworthy news ecosystem. The majority of existing detection algorithms focus on finding clues from news contents, which are generally not effective because fake news is often intentionally written to mislead users by mimicking true news. Therefore, we need to explore auxiliary information to improve detection. The social context during news dissemination process on social media forms the inherent tri-relationship, the relationship among publishers, news pieces, and users, which has potential to improve fake news detection. For example, partisan-biased publishers are more likely to publish fake news, and low-credible users are more likely to share fake news. In this paper, we study the novel problem of exploiting social context for fake news detection. We propose a tri-relationship embedding framework TriFN, which models publisher-news relations and user-news interactions simultaneously for fake news classification. We conduct experiments on two real-world datasets, which demonstrate that the proposed approach significantly outperforms other baseline methods for fake news detection.},
address = {New York, NY, USA},
author = {Shu, Kai and Wang, Suhang and Liu, Huan},
booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
doi = {10.1145/3289600},
isbn = {9781450359405},
publisher = {ACM},
title = {{Beyond News Contents: The Role of Social Context for Fake News Detection}},
url = {https://doi.org/10.1145/3289600.3290994},
volume = {9},
year = {2019}
}
@inproceedings{DBLP:conf/aaai/JinJZS20,
annote = {Uzyty zbior fake news},
author = {Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter},
booktitle = {The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020,},
pages = {8018--8025},
publisher = {AAAI Press},
title = {{Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment}},
url = {https://ojs.aaai.org/index.php/AAAI/article/view/6311},
year = {2020}
}
@inproceedings{Zhou2019b,
abstract = {News plays a significant role in shaping people's beliefs and opinions. Fake news has always been a problem, which wasn't exposed to the mass public until the past election cycle for the 45th President of the United States. While quite a few detection methods have been proposed to combat fake news since 2015, they focus mainly on linguistic aspects of an article without any fact checking. In this paper, we argue that these models have the potential to misclassify fact-tampering fake news as well as under-written real news. Through experiments on Fakebox, a state-of-the-art fake news detector, we show that fact tampering attacks can be effective. To address these weaknesses, we argue that fact checking should be adopted in conjunction with linguistic characteristics analysis, so as to truly separate fake news from real news. A crowdsourced knowledge graph is proposed as a straw man solution to collecting timely facts about news events.},
annote = {Bardzo elementarna pracka na temat wra{\.{z}}liwo{\'{s}}ci klasyfikator{\'{o}}w fake news{\'{o}}w na ataki. Pokazuje, {\.{z}}e model zbudowany na bazie zwyk{\l}ego klasyfikatora tekstu nie zmienia istotnie swojego wyj{\'{s}}cia przy zmianach wej{\'{s}}cia zmieniaj{\c{a}}cych tre{\'{s}}{\'{c}}, np. podmiana daty lub zamiana podmiotu z przedmiotem w zdaniu. Cho{\'{c}} teza jest oczywi{\'{s}}cie prawdziwa, to uzasadnienie eksperymentalne nie imponuje -- klasyfikator mia{\l} s{\l}ab{\c{a}} dok{\l}adno{\'{s}}{\'{c}} nawet przed zmianami, kt{\'{o}}re wykonywano r{\c{e}}cznie w nieznanej ilo{\'{s}}ci. Wnioskiem jest apel o wykrywanie fake news{\'{o}}w jako fact checking, np. przez crowdsourcing.},
archivePrefix = {arXiv},
arxivId = {1901.09657v1},
author = {Zhou, Zhixuan and Guan, Huankang and Bhat, Meghana Moorthy and Hsu, Justin},
booktitle = {ICAART 2019 - Proceedings of the 11th International Conference on Agents and Artificial Intelligence},
doi = {10.5220/0007566307940800},
eprint = {1901.09657v1},
keywords = {Attack,Fact Checking,Fake News Detection,NLP,Outsourced Knowledge Graph},
pages = {794--800},
publisher = {SciTePress},
title = {{Fake News Detection via NLP is Vulnerable to Adversarial Attacks}},
url = {http://arxiv.org/abs/1901.09657 http://dx.doi.org/10.5220/0007566307940800},
volume = {2},
year = {2019}
}
@article{Al-Sarem2019,
abstract = {With the rapid increase in the popularity of social networks, the propagation of rumors is also increasing. Rumors can spread among thousands of users immediately without verification and can cause serious damages. Recently, several research studies have been investigated to control online rumors automatically by mining rich text available on the open network with deep learning techniques. In this paper, we conducted a systematic literature review for rumor detection using deep neural network approaches. A total of 108 studies were retrieved using manual research from five databases (IEEE Explore, Springer Link, Science Direct, ACM Digital Library, and Google Scholar). The considered studies are then examined in our systematic review to answer the seven research questions that we have formulated to deeply understand the overall trends in the use of deep learning methods for rumor detection. Apart from this, our systematic review also presents the challenges and issues that are faced by the researchers in this area and suggests promising future research directions. Our review will be beneficial for researchers in this domain as it will facilitate researchers' comparison with the existing works due to the availability of a complete description of the used performance matrices, dataset characteristics, and the deep learning model used per each work. Our review will also assist researchers in finding the available annotated datasets that can be used as benchmarks for comparing their new proposed approaches with the existing state-of-the-art works.},
author = {Al-Sarem, Mohammed and Boulila, Wadii and Al-Harby, Muna and Qadir, Junaid and Alsaeedi, Abdullah},
doi = {10.1109/ACCESS.2019.2947855},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Al-Sarem et al. - 2019 - Deep learning-based rumor detection on microblogging platforms A systematic review.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Deep learning,Rumor detection,Systematic review,Twitter analysis},
pages = {152788--152812},
publisher = {IEEE},
title = {{Deep learning-based rumor detection on microblogging platforms: A systematic review}},
volume = {7},
year = {2019}
}
@article{1965,
author = {Levenshtein, Vladimir Iosifovich},
journal = {Soviet Physics Doklady},
pages = {707--710},
publisher = {Russian Academy of Sciences},
title = {{Binary codes capable of correcting deletions, insertions, and reversals}},
volume = {10},
year = {1966}
}
@article{Hochreiter1997a,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
issn = {08997667},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}
@inproceedings{Iyyer2018,
abstract = {We propose syntactically controlled paraphrase networks (SCPNs) and use them to generate adversarial examples. Given a sentence and a target syntactic form (e.g., a constituency parse), SCPNs are trained to produce a paraphrase of the sentence with the desired syntax. We show it is possible to create training data for this task by first doing backtranslation at a very large scale, and then using a parser to label the syntactic transformations that naturally occur during this process. Such data allows us to train a neural encoderdecoder model with extra inputs to specify the target syntax. A combination of automated and human evaluations show that SCPNs generate paraphrases that follow their target specifications without decreasing paraphrase quality when compared to baseline (uncontrolled) paraphrase systems. Furthermore, they are more capable of generating syntactically adversarial examples that both (1) "fool" pretrained models and (2) improve the robustness of these models to syntactic variation when used to augment their training data.},
author = {Iyyer, Mohit and Wieting, John and Gimpel, Kevin and Zettlemoyer, Luke},
booktitle = {NAACL HLT 2018 - 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
doi = {10.18653/V1/N18-1170},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Iyyer et al. - 2018 - Adversarial Example Generation with Syntactically Controlled Paraphrase Networks.pdf:pdf},
pages = {1875--1885},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Adversarial Example Generation with Syntactically Controlled Paraphrase Networks}},
url = {https://aclanthology.org/N18-1170},
volume = {1},
year = {2018}
}
@book{Smith1989a,
abstract = {This important new book assembles the work of leading figures in contemporary propaganda scholarship. Analyzing propaganda from a multidisciplinary focus, the book presents several contemporary theoretical perspectives, explores key issues in propaganda analysis, and defines two major research traditions while providing examples of their applications. The contributors examine many of the most complicated issues in the field: the nature of suggestion, the relation of propaganda to ideology, and the interaction of pluralism and truth. Various chapters, written by scholars of communication, rhetoric, journalism, mass communication, government, history, and political science, consider both historical and contemporary issues and events in relation to propaganda. Propaganda: A Pluralistic Perspective marks the renewed development of scholarship in this fascinating field and extends the depth and range of propaganda analysis. The book begins with a focus on theoretical and definitional concerns, including a history of American propaganda analysis and traces four social responses to the subject. Further chapters develop different theoretical positions from diverse perspectives. The book concludes with a focus on key issues in propaganda research, including a study of First Amendment issues in the recent legal controversy over the classification of three Canadian films as political propaganda. Students and scholars of communication, rhetoric, journalism, history, political science, sociology, and many other disciplines will find Propaganda: A Pluralistic Perspective a provocative book full of stimulating ideas.},
author = {Smith, Ted J.},
publisher = {Praeger},
title = {{Propaganda: A Pluralistic Perspective}},
year = {1989}
}
@inproceedings{eger-etal-2019-text,
abstract = {Visual modifications to text are often used to obfuscate offensive comments in social media (e.g., {``}!d10t{''}) or as a writing style ({``}1337{''} in {``}leet speak{''}), among other scenarios. We consider this as a new type of adversarial attack in NLP, a setting to which humans are very robust, as our experiments with both simple and more difficult visual perturbations demonstrate. We investigate the impact of visual adversarial attacks on current NLP systems on character-, word-, and sentence-level tasks, showing that both neural and non-neural models are, in contrast to humans, extremely sensitive to such attacks, suffering performance decreases of up to 82{\%}. We then explore three shielding methods{---}visual character embeddings, adversarial training, and rule-based recovery{---}which substantially improve the robustness of the models. However, the shielding methods still fall behind performances achieved in non-attack scenarios, which demonstrates the difficulty of dealing with visual attacks.},
address = {Minneapolis, Minnesota},
author = {Eger, Steffen and Şahin, G{\"{o}}zde G{\"{u}}l and R{\"{u}}ckl{\'{e}}, Andreas and Lee, Ji-Ung and Schulz, Claudia and Mesgar, Mohsen and Swarnkar, Krishnkant and Simpson, Edwin and Gurevych, Iryna},
booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
doi = {10.18653/v1/N19-1165},
month = {jun},
pages = {1634--1647},
publisher = {Association for Computational Linguistics},
title = {{Text Processing Like Humans Do: Visually Attacking and Shielding NLP Systems}},
url = {https://aclanthology.org/N19-1165},
year = {2019}
}
@article{Lee2023,
abstract = {The success of Transformer architecture has seen increased interest in machine translation (MT). The translation quality of neural network-based MT transcends that of translations derived using statistical methods. This growth in MT research has entailed the development of accurate automatic evaluation metrics that allow us to track the performance of MT. However, automatically evaluating and comparing MT systems is a challenging task. Several studies have shown that traditional metrics (e.g., BLEU, TER) show poor performance in capturing semantic similarity between MT outputs and human reference translations. To date, to improve performance, various evaluation metrics have been proposed using the Transformer architecture. However, a systematic and comprehensive literature review on these metrics is still missing. Therefore, it is necessary to survey the existing automatic evaluation metrics of MT to enable both established and new researchers to quickly understand the trend of MT evaluation over the past few years. In this survey, we present the trend of automatic evaluation metrics. To better understand the developments in the field, we provide the taxonomy of the automatic evaluation metrics. Then, we explain the key contributions and shortcomings of the metrics. In addition, we select the representative metrics from the taxonomy, and conduct experiments to analyze related problems. Finally, we discuss the limitation of the current automatic metric studies through the experimentation and our suggestions for further research to improve the automatic evaluation metrics.},
author = {Lee, Seungjun and Lee, Jungseob and Moon, Hyeonseok and Park, Chanjun and Seo, Jaehyung and Eo, Sugyeong and Koo, Seonmin and Lim, Heuiseok},
doi = {10.3390/MATH11041006},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - 2023 - A Survey on Evaluation Metrics for Machine Translation.pdf:pdf},
issn = {2227-7390},
journal = {Mathematics},
keywords = {Transformer,automatic evaluation metric,deep learning,machine translation},
number = {4},
pages = {1006},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{A Survey on Evaluation Metrics for Machine Translation}},
url = {https://www.mdpi.com/2227-7390/11/4/1006/htm https://www.mdpi.com/2227-7390/11/4/1006},
volume = {11},
year = {2023}
}
@inproceedings{Jaime2022,
abstract = {With the proliferation of online misinformation, fake news detection has gained importance in the artificial intelligence community. In this paper, we propose an adversarial benchmark that tests the ability of fake news detectors to reason about real-world facts. We formulate adversarial attacks that target three aspects of "understanding": compositional semantics, lexical relations, and sensitivity to modifiers. We test our benchmark using BERT classifiers fine-tuned on the LIAR arXiv:arch-ive/1705648 and Kaggle Fake-News datasets, and show that both models fail to respond to changes in compositional and lexical meaning. Our results strengthen the need for such models to be used in conjunction with other fact checking methods.},
annote = {Prosty benchmark odporno{\'{s}}ci algorytm{\'{o}}w wykrywaj{\c{a}}cych fake news na modyfikacje wej{\'{s}}cia, potencjalnie stanowi{\c{a}}ce atak. Fake news s{\c{a}} rozumiane jako nieprawdziwe wiadomo{\'{s}}ci, dlatego od testowanych modeli oczekuje si{\c{e}}, {\.{z}}e dadz{\c{a}} ten sam wynik, o ile znaczenie wej{\'{s}}cia jest to samo.
Testuj{\c{a}} trzy ataki:
- Podmiana zdania na przecz{\c{a}}ce i vice versa (oczekiwana zmiana etykiety -- w{\c{a}}tpliwe)
- Podmiana osoby b{\c{e}}d{\c{a}}cej podmiotem w zdaniu (oczekiwana zmiana na fake -- zweryfikowane r{\c{e}}cznie)
- Obni{\.{z}}enie emocjonalno{\'{s}}ci stwierdzenia (oczekiwana zmiana -- brak).
Klasyfikator to BERT dostrajany na znanych zbiorach.

Wyniki pokazuj{\c{a}}, {\.{z}}e zazwyczaj klasyfikatory nie zmieniaj{\c{a}} decyzji przy tak ma{\l}ych zmianach tre{\'{s}}ci -- cho{\'{c}} powinny (przy przeczeniach i osobach) lub nie (przy emocjonalno{\'{s}}ci).

Komentarz: oczekiwanie {\.{z}}e dostrojony BERT bez {\'{z}}r{\'{o}}d{\l}a iwedzy b{\c{e}}dzie wykonywa{\l} fact-checking jest do{\'{s}}{\'{c}} naiwne.

Bardzo skromna analiza.},
archivePrefix = {arXiv},
arxivId = {2201.00912},
author = {Jaime, Lorenzo and Flores, Yu and Hao, Yiding},
booktitle = {The AAAI-22 Workshop on Adversarial Machine Learning and Beyond},
doi = {10.48550/arxiv.2201.00912},
eprint = {2201.00912},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Jaime, Flores, Hao - 2022 - An Adversarial Benchmark for Fake News Detection Models.pdf:pdf},
title = {{An Adversarial Benchmark for Fake News Detection Models}},
url = {https://arxiv.org/abs/2201.00912v1},
year = {2022}
}
@inproceedings{Loshchilov,
abstract = {L 2 regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is not the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L 2 regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by decoupling the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
address = {New Orleans, LA, USA},
archivePrefix = {arXiv},
arxivId = {1711.05101v3},
author = {Loshchilov, Ilya and Hutter, Frank},
booktitle = {7th International Conference on Learning Representations, ICLR 2019},
eprint = {1711.05101v3},
title = {{Decoupled Weight Decay Regularization}},
url = {https://openreview.net/forum?id=Bkg6RiCqY7},
year = {2019}
}
@inproceedings{DBLP:conf/iclr/ZhangKWWA20,
address = {Addis Ababa, Ethiopia},
annote = {Opis BERTScore, miary podobie{\'{n}}stwa zda{\'{n}} s{\l}u{\.{z}}{\c{a}}cej do por{\'{o}}wnywania predykcji ze wzorcem w ewaluacji zada{\'{n}} generowania tekstu, np. MT, tworzenie podpisa{\'{o}}w itp.

Warto{\'{s}}{\'{c}} jest obliczana w nast{\c{e}}puj{\c{a}}cy spos{\'{o}}b:
1. Przedstawienie obu zda{\'{n}} w przestrzeni osadze{\'{n}} kontekstowych z modelu pretrenowanego typu BERT
2. Obliczenie macierzy podobie{\'{n}}stwa przez iloczyn skalarny reprezentacji
3. Obliczenie precyzji i pokrycia przez iloczyn skalarny najlepszego partnera dla ka{\.{z}}dego s{\l}owa z obu zda{\'{n}}
4. Obliczenie F-score z precyzji i pokrycia
5. Liniowa kalibracja otrzymanej warto{\'{s}}ci w celu uzyskania liczby z (0,1) [na podstawie warto{\'{s}}ci pobranych z du{\.{z}}ego koprusu]

Miara BERTScore wykazuje wi{\c{e}}ksz{\c{a}} korelacj{\c{e}} z os{\c{a}}dem ludzi ni{\.{z}} prostsze miary typu BLEU etc, cho{\'{c}} nie dla ka{\.{z}}dej mo{\.{z}}liwej kombinacji.},
author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
booktitle = {8th International Conference on Learning Representations, ICLR 2020},
title = {{BERTScore: Evaluating Text Generation with BERT}},
url = {https://openreview.net/forum?id=SkeHuCVFDr},
year = {2020}
}
@inproceedings{Horne2017,
abstract = {The problem of fake news has gained a lot of attention as it is claimed to have had a significant impact on 2016 US Presidential Elections. Fake news is not a new problem and its spread in social networks is well-studied. Often an underlying assumption in fake news discussion is that it is written to look like real news, fooling the reader who does not check for reliability of the sources or the arguments in its content. Through a unique study of three data sets and features that capture the style and the language of articles, we show that this assumption is not true. Fake news in most cases is more similar to satire than to real news, leading us to conclude that persuasion in fake news is achieved through heuristics rather than the strength of arguments. We show overall title structure and the use of proper nouns in titles are very significant in differentiating fake from real. This leads us to conclude that fake news is targeted for audiences who are not likely to read beyond titles and is aimed at creating mental associations between entities and claims.},
annote = {Ssed datasets with 35 fake news items from a BuzzFeed News article and 75 gathered by themselves manually and make interesting observations on the stylistic cues affecting credibility, but the prediction performance may not be reliable due to small data size.},
archivePrefix = {arXiv},
arxivId = {1703.09398},
author = {Horne, Benjamin D. and Adali, Sibel},
booktitle = {Proceedings of the 2nd International Workshop on News and Public Opinion at ICWSM},
eprint = {1703.09398},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Horne, Adali - 2017 - This Just In Fake News Packs a Lot in Title, Uses Simpler, Repetitive Content in Text Body, More Similar to Satire.pdf:pdf},
publisher = {Association for the Advancement of Artificial Intelligence},
title = {{This Just In: Fake News Packs a Lot in Title, Uses Simpler, Repetitive Content in Text Body, More Similar to Satire than Real News}},
url = {http://arxiv.org/abs/1703.09398},
year = {2017}
}
@inproceedings{Kingma2015a,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
address = {San Diego, USA},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
eprint = {1412.6980},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Ba - 2015 - Adam A method for stochastic optimization.pdf:pdf},
publisher = {ICLR},
title = {{Adam: A method for stochastic optimization}},
url = {https://arxiv.org/abs/1412.6980v9},
year = {2015}
}
@techreport{Meyer2019,
author = {Meyer, T and Marsden, C},
institution = {European Parliament},
title = {{Regulating disinformation with artificial intelligence: Effects of disinformation initiatives on freedom of expression and media pluralism}},
url = {https://data.europa.eu/doi/10.2861/003689},
year = {2019}
}
@article{Alsmadi2022,
author = {Alsmadi, Izzat and Ahmad, Kashif and Nazzal, Mahmoud and Alam, Firoj and Al-Fuqaha, Ala and Khreishah, Abdallah and Algosaibi, Abdulelah},
doi = {10.1109/TCSS.2022.3218743},
issn = {2329924X},
journal = {IEEE Transactions on Computational Social Systems},
publisher = {IEEE},
title = {{Adversarial NLP for Social Network Applications: Attacks, Defenses, and Research Directions}},
year = {2022}
}
@inproceedings{DBLP:conf/iclr/HeLGC21,
author = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
booktitle = {9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021},
publisher = {OpenReview.net},
title = {{Deberta: decoding-Enhanced Bert with Disentangled Attention}},
url = {https://openreview.net/forum?id=XPZIaotutsD},
year = {2021}
}
@inproceedings{garg-ramakrishnan-2020-bae,
abstract = {Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model. Recent works in NLP use rule-based synonym replacement strategies to generate adversarial examples. These strategies can lead to out-of-context and unnaturally complex token replacements, which are easily identifiable by humans. We present BAE, a black box attack for generating adversarial examples using contextual perturbations from a BERT masked language model. BAE replaces and inserts tokens in the original text by masking a portion of the text and leveraging the BERT-MLM to generate alternatives for the masked tokens. Through automatic and human evaluations, we show that BAE performs a stronger attack, in addition to generating adversarial examples with improved grammaticality and semantic coherence as compared to prior work.},
address = {Online},
author = {Garg, Siddhant and Ramakrishnan, Goutham},
booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
doi = {10.18653/v1/2020.emnlp-main.498},
pages = {6174--6181},
publisher = {Association for Computational Linguistics},
title = {{BAE: BERT-based Adversarial Examples for Text Classification}},
url = {https://aclanthology.org/2020.emnlp-main.498},
year = {2020}
}
@inproceedings{Srivastava2023,
abstract = {AI services are known to have unstable behavior when subjected to changes in data, models or users. Such behaviors, whether triggered by omission or commission, lead to trust issues when AI works with humans. The current approach of assessing AI services in a black box setting, where the consumer does not have access to the AI's source code or training data, is limited. The consumer has to rely on the AI developer's documentation and trust that the system has been built as stated. Further, if the AI consumer reuses the service to build other services which they sell to their customers, the consumer is at the risk of the service providers (both data and model providers). Our approach, in this context, is inspired by the success of nutritional labeling in food industry to promote health and seeks to assess and rate AI services for trust from the perspective of an independent stakeholder. The ratings become a means to communicate the behavior of AI systems so that the consumer is informed about the risks and can make an informed decision. In this paper, we will first describe recent progress in developing rating methods for text-based machine translator AI services that have been found promising with user studies. Then, we will outline challenges and vision for a principled, multi-modal, causality-based rating methodologies and its implication for decision-support in real-world scenarios like health and food recommendation.},
archivePrefix = {arXiv},
arxivId = {2302.09079},
author = {Srivastava, Biplav and Lakkaraju, Kausik and Bernagozzi, Mariana and Valtorta, Marco},
booktitle = {Spring Symposium on AI Trustworthiness Assessment},
doi = {10.48550/arxiv.2302.09079},
eprint = {2302.09079},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Srivastava et al. - 2023 - Advances in Automatically Rating the Trustworthiness of Text Processing Services.pdf:pdf},
title = {{Advances in Automatically Rating the Trustworthiness of Text Processing Services}},
url = {https://arxiv.org/abs/2302.09079v1},
year = {2023}
}
@techreport{Tucker2018,
annote = {Przegl{\c{a}}d literatury na temat problem{\'{o}}w na styku medi{\'{o}}w spo{\l}eczno{\'{s}}ciowych, polaryzacji ideologicznej i dezinformacji. Og{\'{o}}lny wyd{\'{z}}wi{\c{e}}k jest taki, {\.{z}}e ze wzgl{\c{e}}du na trudny problem, bada{\'{n}} jest niewiele, prowadzonych w r{\'{o}}{\.{z}}nych kontekstach, z r{\'{o}}{\.{z}}nymi wynikami, cz{\c{e}}sto nie potwierdzaj{\c{a}}cymi potocznych pogl{\c{a}}d{\'{o}}w na kwestie.
Np.
- polaryzacja polityczna nie jest tak ekstremalna jak si{\c{e}} uwa{\.{z}}a, a jej wzrost by{\l} najwy{\.{z}}szy w{\'{s}}r{\'{o}}d grup ma{\l}o korzystaj{\c{a}}cych z internetu,
- szara strefa pomi{\c{e}}dzy fa{\l}szywymi a prawdziwymi informacjami zawiera 'clickbait', teorie spiskowe, 'hyperpartisan news', plagiaty
- ekspozycja u{\.{z}}ytkownik{\'{o}}w social medi{\'{o}}w na tre{\'{s}}ci wynika w znacznie wi{\c{e}}kszym stopniu z posiadanych znajomych (jak w {\'{s}}wiecie rzeczywistym) ni{\.{z}} z algorytm{\'{o}}w i pozwala na wi{\c{e}}ksze zapoznawanie si{\c{e}} z odleg{\l}ymi pogl{\c{a}}dami, ni{\.{z}} w zwyk{\l}ych kontaktach
- du{\.{z}}y udzia{\l} spolaryzowanych tre{\'{s}}ci wynika z wi{\c{e}}kszej aktywno{\'{s}}ci spolaryzowanych u{\.{z}}ytkownik{\'{o}}w,
- (zawiera przegl{\c{a}}d {\'{z}}r{\'{o}}de{\l} dezinformacji),
- dodawanie ostrzerze{\'{n}} o 'fake news' do tre{\'{s}}ci mo{\.{z}}e przynie{\'{s}}{\'{c}} efekt odwrotny do oczekiwanego, tj. utwierdzenie si{\c{e}} w pogl{\c{a}}dach,
- rozprzestrzenianie si{\c{e}} wiadomo{\'{s}}ci w zdecydowanej wi{\c{e}}kszo{\'{s}}ci jest nap{\c{e}}dzane przez tradycyjne media i rzadko dzieje si{\c{e}} 'wiralnie',
- wys{\l}uchanie g{\l}os{\'{o}}w r{\'{o}}{\.{z}}nych stron (szczeg{\'{o}}lnie, gdy znana jest orientacja) mo{\.{z}}e utwierdza{\'{c}} odbiorc{\'{o}}w w ich pogl{\c{a}}dach,
- zwi{\c{e}}kszona ekspozycja fa{\l}szywych wiadomo{\'{s}}ci (nawet w kontek{\'{s}}cie poprawek) zwi{\c{e}}ksza wiar{\c{e}} w nie,
- zjawisko 'echo chambers' oraz zakres polaryzacji s{\c{a}} wyolbrzymiane,
- istniej{\c{a}} braki w:
-- {\'{s}}cis{\l}ych definicjach badanych zjawisk,
-- wiedzy o ich powszechno{\'{s}}ci,
-- dost{\c{e}}pnych danych: szczeg{\'{o}}lnie z sieci spo{\l}eczno{\'{s}}ciowych

O u{\.{z}}yciu ML do wykrywania fake news{\'{o}}w: "It is even possible that these methods will never work".},
author = {Tucker, Joshua A. and Guess, Andrew and Barber{\'{a}}, Pablo and Vaccari, Cristian and Siegel, Alexandra and Sanovich, Sergey and Stukal, Denis and Nyhan, Brendan},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Tucker et al. - 2018 - Social Media, Political Polarization, and Political Disinformation A Review of the Scientific Literature.pdf:pdf},
institution = {Hewlett Foundation},
title = {{Social Media, Political Polarization, and Political Disinformation: A Review of the Scientific Literature}},
url = {https://hewlett.org/library/social-media-political-polarization-political-disinformation-review-scientific-literature/},
year = {2018}
}
@inproceedings{Devlin2018,
abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI accuracy to 86.7 (5.6% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5% absolute improvement), outperforming human performance by 2.0%.},
annote = {BERT to pre-trenowany model j{\c{e}}zykowy og{\'{o}}lnego zastosowania. Wykorzystuje architektur{\c{e}} enkoder{\'{o}}w w modelu t{\l}umaczeniowym Transformera (Kwon 2017), ale o znacznie wi{\c{e}}kszych wymiarach i trenowan{\c{a}} na innych nienadzorowanych zadaniach:
- Odgadywaniu zamaskowanego s{\l}owa w zdaniu,
- Odgadywaniu czy dwa podane zdania nast{\c{e}}puj{\c{a}} po sobie.

Tak pretrenowany model mo{\.{z}}na dostroi{\'{c}} do konkretnego zadania poprzez rozbudowanie o dodatkow{\c{a}} warstw{\c{e}} na ko{\'{n}}cu specyficzn{\c{a}} do zadania i kontynuacj{\c{e}} uczenia. W ten spos{\'{o}}b sprawdzono BERTa na 11 r{\'{o}}{\.{z}}nych zadaniach (NER, QA, STS etc.), osi{\c{a}}gaj{\c{a}}c SOTA we wszystkich przypadkach. Lepiej ni{\.{z}} OpenAI GPT lub ELMo.

Mo{\.{z}}na r{\'{o}}wnie{\.{z}} u{\.{z}}y{\'{c}} go jako generatora cech, tj. konektowych osadze{\'{n}} s{\l}{\'{o}}w lub osadze{\'{n}} zda{\'{n}}. 

Zar{\'{o}}wno modele, jaki i kod wykonawczy s{\c{a}} do {\'{s}}ci{\c{a}}gni{\c{e}}cia.

Wyja{\'{s}}nienie wizualne: https://jalammar.github.io/illustrated-bert/},
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
eprint = {1810.04805},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Devlin et al. - 2018 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:pdf},
pages = {4171--4186},
publisher = {Association for Computational Linguistics},
title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
url = {http://arxiv.org/abs/1810.04805},
year = {2018}
}
@article{Zhang2020b,
abstract = {With the development of high computational devices, deep neural networks (DNNs), in recent years, have gained significant popularity in many Artificial Intelligence (AI) applications. However, prev...},
annote = {Om{\'{o}}wienie atak{\'{o}}w antagonistycznych (adversarial) na neuronowe modele przetwarzaniu j{\c{e}}zyka. Om{\'{o}}wiono:
- og{\'{o}}ln{\c{a}} taksonomi{\c{e}}
- hostoryczne zastosowanie w obrazach
- trudno{\'{s}}ci w przeniesieniu rozwi{\c{a}}za{\'{n}} dla obraz{\'{o}}w na tekst
- sposoby wektoryzacji tekstu dla dostosowania
- pomiar rozmiaru perturbacji
- kategoryzacje wg dost{\c{e}}pu do modelu, typu zadania, celu, atakowanych sieci
- przyk{\l}adowe ataki
- scenariusze wielomodalne
- strategie obrony},
author = {Zhang, Wei Emma and Sheng, Quan Z. and Alhazmi, Ahoud and Li, Chenliang},
doi = {10.1145/3374217},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2020 - Adversarial Attacks on Deep-learning Models in Natural Language Processing(2).pdf:pdf},
issn = {21576912},
journal = {ACM Transactions on Intelligent Systems and Technology (TIST)},
keywords = {Deep neural networks,adversarial examples,natural language processing,textual data},
number = {3},
publisher = {ACM PUB27 New York, NY, USA},
title = {{Adversarial Attacks on Deep-learning Models in Natural Language Processing}},
url = {https://dl.acm.org/doi/10.1145/3374217},
volume = {11},
year = {2020}
}
@inproceedings{Ettinger2017,
abstract = {This paper presents a summary of the first Workshop on Building Linguistically Generalizable Natural Language Processing Systems, and the associated Build It Break It, The Language Edition shared task. The goal of this workshop was to bring together researchers in NLP and linguistics with a shared task aimed at testing the generalizability of NLP systems beyond the distributions of their training data. We describe the motivation, setup, and participation of the shared task, provide discussion of some highlighted results, and discuss lessons learned.},
annote = {Opis shared tasku organizowanego w celu oceny podatno{\'{s}}ci system{\'{o}}w NLP na antagonistyczne modyfikacje wej{\'{s}}cia. Wykorzystano dwa zadania: oceny wyd{\'{z}}wi{\c{e}}ku i odpowiadania na pytania. Dru{\.{z}}yny 'builders' budowa{\l}y systemy wykonuj{\c{a}}ce zadania, a 'breakers' takich modyfikacji istniej{\c{a}}cych przyk{\l}ad{\'{o}}w, kt{\'{o}}re zmieniaj{\c{a}} predykcj{\c{e}}. Celem by{\l}o zaanga{\.{z}}owanie j{\c{e}}zykoznawc{\'{o}}w w zrozumienie s{\l}abo{\'{s}}ci istniej{\c{a}}cych technik. Cho{\'{c}} udawa{\l}o si{\c{e}} znale{\'{z}}{\'{c}} s{\l}abe punkty, sukces by{\l} ograniczony ze wzgl{\c{e}}du na bardzo ma{\l}{\c{a}} liczb{\c{e}} uczestnik{\'{o}}w.},
archivePrefix = {arXiv},
arxivId = {1711.01505},
author = {Ettinger, Allyson and Rao, Sudha and III, Hal Daum{\'{e}} and Bender, Emily M.},
booktitle = {Proceedings of the First Workshop on Building Linguistically Generalizable NLP Systems},
doi = {10.18653/V1/W17-5401},
eprint = {1711.01505},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Ettinger et al. - 2017 - Towards Linguistically Generalizable NLP Systems A Workshop and Shared Task.pdf:pdf},
pages = {1--10},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Towards Linguistically Generalizable NLP Systems: A Workshop and Shared Task}},
url = {https://aclanthology.org/W17-5401},
year = {2017}
}
@article{Koenders2021,
abstract = {As the spread of false information on the internet has increased dramatically in recent years, more and more attention is being paid to automated fake news detection. Some fake news detection methods are already quite successful. Nevertheless, there are still many vulnerabilities in the detection algorithms. The reason for this is that fake news publishers can structure and formulate their texts in such a way that a detection algorithm does not expose this text as fake news. This paper shows that it is possible to automatically attack state-of-the-art models that have been trained to detect Fake News, making these vulnerable. For this purpose, corresponding models were first trained based on a dataset. Then, using Text-Attack, an attempt was made to manipulate the trained models in such a way that previously correctly identified fake news was classified as true news. The results show that it is possible to automatically bypass Fake News detection mechanisms, leading to implications concerning existing policy initiatives.},
archivePrefix = {arXiv},
arxivId = {2107.07970},
author = {Koenders, Camille and Filla, Johannes and Schneider, Nicolai and Woloszyn, Vinicius},
doi = {10.48550/arxiv.2107.07970},
eprint = {2107.07970},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Koenders et al. - 2021 - How Vulnerable Are Automatic Fake News Detection Methods to Adversarial Attacks.pdf:pdf},
journal = {arXiv:2107.07970},
title = {{How Vulnerable Are Automatic Fake News Detection Methods to Adversarial Attacks?}},
url = {https://arxiv.org/abs/2107.07970v1},
year = {2021}
}
@article{Cresci2022,
abstract = {Adversarial examples are inputs to a machine learning system that result in an incorrect output from that system. Attacks launched through this type of input can cause severe consequences: for example, in the field of image recognition, a stop signal can be misclassified as a speed limit indication. However, adversarial examples also represent the fuel for a flurry of research directions in different domains and applications. Here, we give an overview of how they can be profitably exploited as powerful tools to build stronger learning models, capable of better-withstanding attacks, for two crucial tasks: fake news and social bot detection.},
author = {Cresci, Stefano and Petrocchi, Marinella and Spognardi, Angelo and Tognazzi, Stefano},
doi = {10.1109/MIC.2021.3130380},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Cresci et al. - 2022 - Adversarial Machine Learning for Protecting Against Online Manipulation.pdf:pdf},
issn = {19410131},
journal = {IEEE Internet Computing},
keywords = {Alan Turing asked this question to his audience: C,I.2.4 Knowledge representation formalisms and meth,and in his paper 'Computing Machinery and Intellig,as measured by P,if its performance at tasks in T,improves with experience E' [1],whose traditional definition is as follows: 'A com},
number = {2},
pages = {47--52},
publisher = {IEEE},
title = {{Adversarial Machine Learning for Protecting Against Online Manipulation}},
volume = {26},
year = {2022}
}
@inproceedings{Chen2022,
abstract = {Textual adversarial samples play important roles in multiple subfields of NLP research, including security, evaluation, explainability, and data augmentation. However, most work mixes all these roles, obscuring the problem definitions and research goals of the security role that aims to reveal the practical concerns of NLP models. In this paper, we rethink the research paradigm of textual adversarial samples in security scenarios. We discuss the deficiencies in previous work and propose our suggestions that the research on the Security-oriented adversarial NLP (SoadNLP) should: (1) evaluate their methods on security tasks to demonstrate the real-world concerns; (2) consider real-world attackers' goals, instead of developing impractical methods. To this end, we first collect, process, and release a security datasets collection Advbench. Then, we reformalize the task and adjust the emphasis on different goals in SoadNLP. Next, we propose a simple method based on heuristic rules that can easily fulfill the actual adversarial goals to simulate real-world attack methods. We conduct experiments on both the attack and the defense sides on Advbench. Experimental results show that our method has higher practical value, indicating that the research paradigm in SoadNLP may start from our new benchmark. All the code and data of Advbench can be obtained at \url{https://github.com/thunlp/Advbench}.},
annote = {Krytyczne podej{\'{s}}{\'{c}}ie do wcze{\'{s}}niejszych bada{\'{n}} z punktu widzenia zastosowa{\'{n}} w bezpiecze{\'{n}}stwie. Autorzy uwa{\.{z}}aj{\c{a}}, {\.{z}}e praktyczne zastosowania r{\'{o}}{\.{z}}ni{\c{a}} si{\c{e}} od tych badanych, poniewa{\.{z}} (1) dotycz{\c{a}} konkretnych zada{\'{n}}, a nie np. oceny wyd{\'{z}}wi{\c{e}}ku, (2) dost{\c{e}}pna jest tylko ostateczna decyzja modelu, a nie prawdopodobie{\'{n}}stwa lub gradienty, (3) istone jest zachowanie tre{\'{s}}ci, np. mowy nienawi{\'{s}}ci, (4) nie ma znaczenia wielko{\'{s}}{\'{c}} perturbacji, (5) konieczne jest ograniczenie liczby zapyta{\'{n}} do modelu oraz (6) atakuj{\c{a}}cy zazwyczaj nie b{\c{e}}d{\c{a}} wykorzystywali zaawansowanych modeli NLP, a proste heurystyki.

Praca opisuje kolekcj{\c{e}} zbior{\'{o}}w danych z dziedziny, np. fake newsy, fa{\l}szywe recenzje, mowa nienawi{\'{s}}ci, spam i informacje wra{\.{z}}liwe. Autorzy testuj{\c{a}} kilka znanych metod oraz w{\l}asn{\c{a}}, sk{\l}adaj{\c{a}}c{\c{a}} si{\c{e}} z prostych heurystyk z elementami r{\c{e}}cznego dostosowania. Testy korzystaj{\c{a}} ze znanych miar ale z postulatami jw. -- zachowanie tre{\'{s}}ci jest sprawdzane przez crowdworker{\'{o}}w. Okazuje si{\c{e}}, {\.{z}}e przy takich ograniczeniach bardzo trudno zaatakowa{\'{c}} niekt{\'{o}}re zadania. Proponowana metoda radzi sobie najlepiej, je{\'{s}}li dost{\c{e}}pna jest bardzo ograniczona liczba zapyta{\'{n}} modelu.},
author = {Chen, Yangyi and Gao, Hongcheng and Cui, Ganqu and Qi, Fanchao and Huang, Longtao and Liu, Zhiyuan and Sun, Maosong},
booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2022 - Why Should Adversarial Perturbations be Imperceptible Rethink the Research Paradigm in Adversarial NLP.pdf:pdf},
pages = {11222----11237},
title = {{Why Should Adversarial Perturbations be Imperceptible? Rethink the Research Paradigm in Adversarial NLP}},
url = {https://aclanthology.org/2022.emnlp-main.771},
year = {2022}
}
@inproceedings{Smith2021,
abstract = {The study of adversarial effects on AI systems is not a new concept, but much of the research has been devoted to deep learning. In this paper we explore the effects of adversarial examples on 4 machine learning classifiers and measure the effectiveness of adversarial training. Additionally, we present a novel method for selecting adversarial training examples that lead to a more robust machine learning system. Our results suggest that adversarial examples can significantly hinder the classification performance and that adversarial training is an effective defensive counter-measure.},
author = {Smith, Marcellus and Brown, Brandon and Dozier, Gerry and King, Michael},
booktitle = {2021 IEEE Congress on Evolutionary Computation, CEC 2021 - Proceedings},
doi = {10.1109/CEC45853.2021.9504723},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Smith et al. - 2021 - Mitigating Attacks on Fake News Detection Systems using Genetic-Based Adversarial Training.pdf:pdf},
isbn = {9781728183923},
keywords = {Adversarial example selection,Adversarial examples,Adversarial training,GAT,Steady-state genetic algorithm},
pages = {1265--1271},
publisher = {IEEE},
title = {{Mitigating Attacks on Fake News Detection Systems using Genetic-Based Adversarial Training}},
year = {2021}
}
@article{Carter2021,
abstract = {In the last few years, we have witnessed an explosive growth of fake content on the Internet which has significantly affected the veracity of information on many social platforms. Much of this disruption has been caused by the proliferation of advanced machine and deep learning methods. In turn, social platforms have been using the same technological methods in order to detect fake content. However, there is understanding of the strengths and weaknesses of these detection methods. In this article, we describe examples of machine and deep learning approaches that can be used to detect different types of fake content. We also discuss the characteristics and the potential for adversarial attacks on these methods that could reduce the accuracy of fake content detection. Finally, we identify and discuss some future research challenges in this area.},
author = {Carter, Matthew and Tsikerdekis, Michail and Zeadally, Sherali},
doi = {10.1109/MIC.2020.3032323},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Carter, Tsikerdekis, Zeadally - 2021 - Approaches for Fake Content Detection Strengths and Weaknesses to Adversarial Attacks.pdf:pdf},
issn = {19410131},
journal = {IEEE Internet Computing},
keywords = {adversarial,attacks,content,detection,fake},
number = {2},
pages = {73--83},
publisher = {IEEE},
title = {{Approaches for Fake Content Detection: Strengths and Weaknesses to Adversarial Attacks}},
volume = {25},
year = {2021}
}
@inproceedings{Le2020,
abstract = {In recent years, the proliferation of so-called "fake news" has caused much disruptions in society and weakened the news ecosystem. Therefore, to mitigate such problems, researchers have developed state-of-the-art models to auto-detect fake news on social media using sophisticated data science and machine learning techniques. In this work, then, we ask "what if adversaries attempt to attack such detection models?" and investigate related issues by (i) proposing a novel threat model against fake news detectors, in which adversaries can post malicious comments toward news articles to mislead fake news detectors, and (ii) developing MALCOM, an end-to-end adversarial comment generation framework to achieve such an attack. Through a comprehensive evaluation, we demonstrate that about 94% and 93.5% of the time on average MALCOM can successfully mislead five of the latest neural detection models to always output targeted real and fake news labels. Furthermore, MALCOM can also fool black box fake news detectors to always output real news labels 90% of the time on average. We also compare our attack model with four baselines across two real-world datasets, not only on attack performance but also on generated quality, coherency, transferability, and robustness.},
annote = {Spos{\'{o}}b na oszukiwanie modeli wykrywaj{\c{a}}cych fake news poprzez dopisywanie komentarzy. Dzia{\l}a na klasyfikatory, kt{\'{o}}re oceniaj{\c{a}} prawdziwo{\'{s}}c w kontek{\'{s}}cie social media, tj, wej{\'{s}}ciem jest post na twitterze z nag{\l}{\'{o}}wkiem lub fragmentem i lista komentarzy u{\.{z}}ytkownik{\'{o}}w. Praca przedstawia spos{\'{o}}b na dodanie takich komentarzy, kt{\'{o}}re zmieni{\c{a}} ocen{\c{e}} klasyfikatora. Optymalizuje strat{\c{e}} sk{\l}adaj{\c{a}}c{\c{a}} si{\c{e}} z wielu komponent{\'{o}}w. G{\l}{\'{o}}wny bazuje na gradiencie dla zmiany oceny przez klasyfikator, z uci{\c{a}}gleniem przez Gumbell-softmax. Dodatkowo promuje si{\c{e}} p{\l}ynno{\'{s}}{\'{c}} (przez model j{\c{e}}zykowy) i zgodno{\'{s}}{\'{c}} ze stylem i tre{\'{s}}ci{\c{a}} konwersacji (przez GAN). Atak black-box realizowany przez trenowanie surogatu na tych samych danych treningowych. Bardzo du{\.{z}}o eksperyment{\'{o}}w, w z r{\c{e}}czn{\c{a}} ewaluacj{\c{a}}.},
archivePrefix = {arXiv},
arxivId = {2009.01048},
author = {Le, Thai and Wang, Suhang and Lee, Dongwon},
booktitle = {Proceedings - IEEE International Conference on Data Mining, ICDM},
doi = {10.48550/arxiv.2009.01048},
eprint = {2009.01048},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Le, Wang, Lee - 2020 - MALCOM Generating Malicious Comments to Attack Neural Fake News Detection Models.pdf:pdf},
isbn = {9781728183169},
issn = {15504786},
keywords = {Adversarial Examples,Fake News Detection,Malicious Comments},
pages = {282--291},
publisher = {IEEE},
title = {{MALCOM: Generating Malicious Comments to Attack Neural Fake News Detection Models}},
url = {https://arxiv.org/abs/2009.01048v2},
year = {2020}
}
@article{Kantartopoulos2020,
abstract = {Social media has become very popular and important in people&rsquo;s lives, as personal ideas, beliefs and opinions are expressed and shared through them. Unfortunately, social networks, and specifically Twitter, suffer from massive existence and perpetual creation of fake users. Their goal is to deceive other users employing various methods, or even create a stream of fake news and opinions in order to influence an idea upon a specific subject, thus impairing the platform&rsquo;s integrity. As such, machine learning techniques have been widely used in social networks to address this type of threat by automatically identifying fake accounts. Nonetheless, threat actors update their arsenal and launch a range of sophisticated attacks to undermine this detection procedure, either during the training or test phase, rendering machine learning algorithms vulnerable to adversarial attacks. Our work examines the propagation of adversarial attacks in machine learning based detection for fake Twitter accounts, which is based on AdaBoost. Moreover, we propose and evaluate the use of k-NN as a countermeasure to remedy the effects of the adversarial attacks that we have implemented.},
author = {Kantartopoulos, Panagiotis and Pitropakis, Nikolaos and Mylonas, Alexios and Kylilis, Nicolas},
doi = {10.3390/TECHNOLOGIES8040064},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Kantartopoulos et al. - 2020 - Exploring Adversarial Attacks and Defences for Fake Twitter Account Detection.pdf:pdf},
issn = {2227-7080},
journal = {Technologies 2020},
keywords = {Twitter,adversarial attacks,machine learning,poisoning,social media},
number = {4},
pages = {64},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Exploring Adversarial Attacks and Defences for Fake Twitter Account Detection}},
url = {https://www.mdpi.com/2227-7080/8/4/64/htm https://www.mdpi.com/2227-7080/8/4/64},
volume = {8},
year = {2020}
}
@article{Llanso2020,
abstract = {Contemporary policy debates about managing the enormous volume of online content have taken a renewed focus on upload filtering, automated detection of potentially illegal content, and other “proactive measures”. Often, policymakers and tech industry players invoke artificial intelligence as the solution to complex challenges around online content, promising that AI is a scant few years away from resolving everything from hate speech to harassment to the spread of terrorist propaganda. Missing from these promises, however, is an acknowledgement that proactive identification and automated removal of user-generated content raises problems beyond issues of “accuracy” and overbreadth--problems that will not be solved with more sophisticated AI. In this commentary, I discuss how the technical realities of content filtering stack up against the protections for freedom of expression in international human rights law. As policymakers and companies around the world turn to AI for communications governance, it is crucial that we recall why legal protections for speech have included presumptions against prior censorship, and consider carefully how proactive content moderation will fundamentally re-shape the relationship between rules, people, and their speech.},
annote = {Spojrzenie na systemy filtrowania tre{\'{s}}ci w mediach spo{\l}eczno{\'{s}}ciowych z punktu widzenia praw cz{\l}owieka. Autorka zwraca uwag{\c{e}}, {\.{z}}e odrzucanie post{\'{o}}w/film{\'{o}}w etc. PRZED ich opublikowaniem ('proactive measures'), raczej ni{\.{z}} po otrzymaniu skarg lub nakazu s{\c{a}}dowego r{\'{o}}wna si{\c{e}} cenzurze prowadzonej przez s{\l}u{\.{z}}by pa{\'{n}}stwowe, kt{\'{o}}ra jest zabroniona przez mi{\c{e}}dzynarodowe standardy praw cz{\l}owieka. Nast{\c{e}}pnie omawia poszczeg{\'{o}}lne aspekty cenzury, dla kt{\'{o}}rych uznaje si{\c{e}} ja za nieakceptowaln{\c{a}}, i pokazuje, {\.{z}}e maj{\c{a}} one zastosowanie r{\'{o}}wnie{\.{z}} w przypadku medi{\'{o}}w internetowych. S{\c{a}} to: ocenianie dok{\l}adnie wszystkich wypowiedzi (niemo{\.{z}}liwe offline), zbytnie u{\l}atwienie karania (usuni{\c{e}}cie barier proceduralnych koniecznych do ukarania w drodze s{\c{a}}dowej) i ograniczenie kontroli zewn{\c{e}}trznej (niemo{\.{z}}no{\'{s}}{\'{c}} poznania czy zrozumienia zasad filtrowania i podj{\c{e}}cia obrony, jak w s{\c{a}}dzie). {\.{Z}}adnego z tych problem{\'{o}}w nie rozwi{\c{a}}{\.{z}}{\c{a}} algrytmy AI, gdy{\.{z}} skupiaj{\c{a}} si{\c{e}} na osi{\c{a}}ganiu jak najwi{\c{e}}kszej dok{\l}adno{\'{s}}ci.},
author = {Llans{\'{o}}, Emma J},
doi = {10.1177/2053951720920686},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Llans{\'{o}} - 2020 - No amount of “AI” in content moderation will solve filtering's prior-restraint problem.pdf:pdf},
issn = {2053-9517},
journal = {Big Data and Society},
keywords = {artificial intelligence,filtering,free expression,human rights,machine learning,natural language processing},
number = {1},
publisher = {SAGE Publications Ltd},
title = {{No amount of “AI” in content moderation will solve filtering's prior-restraint problem}},
url = {http://journals.sagepub.com/doi/10.1177/2053951720920686},
volume = {7},
year = {2020}
}
@inproceedings{Zeng2021,
abstract = {Textual adversarial attacking has received wide and increasing attention in recent years. Various attack models have been proposed, which are enormously distinct and implemented with different programming frameworks and settings. These facts hinder quick utilization and fair comparison of attack models. In this paper, we present an open-source textual adversarial attack toolkit named OpenAttack to solve these issues. Compared with existing other textual adversarial attack toolkits, OpenAttack has its unique strengths in support for all attack types, multilinguality, and parallel processing. Currently, OpenAttack includes 15 typical attack models that cover all attack types. Its highly inclusive modular design not only supports quick utilization of existing attack models, but also enables great flexibility and extensibility. OpenAttack has broad uses including comparing and evaluating attack models, measuring robustness of a model, assisting in developing new attack models, and adversarial training. Source code and documentation can be obtained at https://github.com/thunlp/ OpenAttack.},
annote = {Opis frameworku do ewaluacji atak{\'{o}}w antagonistycznych. Podobny do TextAttack, ale wspiera bardziej r{\'{o}}{\.{z}}norodne ataki (np. podmiany ca{\l}ych zda{\'{n}}), r{\'{o}}{\.{z}}ne j{\c{e}}zyki i r{\'{o}}wnoleg{\l}e przetwarzanie. Pozwala zdefiniowa{\'{c}} w{\l}asne modele ofiary i atakuj{\c{a}}cego. Zawiera implementacje wielu popularnych atak{\'{o}}w i miar sukcesu. Ca{\l}o{\'{s}}{\'{c}} zaimplementowana modularnie i rozszerzalnie.},
archivePrefix = {arXiv},
arxivId = {2009.09191},
author = {Zeng, Guoyang and Qi, Fanchao and Zhou, Qianrui and Zhang, Tingji and Ma, Zixian and Hou, Bairu and Zang, Yuan and Liu, Zhiyuan and Sun, Maosong},
booktitle = {ACL-IJCNLP 2021 - 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Proceedings of the System Demonstrations},
doi = {10.18653/V1/2021.ACL-DEMO.43},
eprint = {2009.09191},
file = {:Users/piotr/Library/Application Support/Mendeley Desktop/Downloaded/Zeng et al. - 2021 - OpenAttack An Open-source Textual Adversarial Attack Toolkit.pdf:pdf},
isbn = {9781954085565},
pages = {363--371},
publisher = {Association for Computational Linguistics (ACL)},
title = {{OpenAttack: An Open-source Textual Adversarial Attack Toolkit}},
url = {https://aclanthology.org/2021.acl-demo.43},
year = {2021}
}
