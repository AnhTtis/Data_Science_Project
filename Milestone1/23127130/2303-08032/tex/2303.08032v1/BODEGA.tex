\documentclass{article}
% pass the option "final" for camera-ready versions
%\usepackage[final]{nejlt}
\usepackage[final]{nejlt}

\usepackage[hyphens]{url}
%\usepackage{hyperref}
\usepackage{graphicx} 

\title{BODEGA: Benchmark for Adversarial Example Generation in Credibility Assessment}
\author{
Piotr Przyby{\l}a,\\
Universitat Pompeu Fabra, Barcelona, Spain  {\tt \small piotr.przybyla@upf.edu},\\
Institute of Computer Science, Polish Academy of Sciences, Warsaw, Poland
\and
Alexander Shvets,\\
Universitat Pompeu Fabra, Barcelona, Spain  {\tt \small alexander.shvets@upf.edu}
\and
Horacio Saggion,\\
Universitat Pompeu Fabra, Barcelona, Spain  {\tt \small horacio.saggion@upf.edu}
}
\begin{document}

\abstract{Text classification methods have been widely investigated as a way to detect content of low credibility: fake news, social media bots, propaganda, etc. Quite accurate models (likely based on deep neural networks) help in moderating public electronic platforms and often cause content creators to face rejection of their submissions or removal of already published texts. Having the incentive to evade further detection, content creators try to come up with a slightly modified version of the text (known as an attack with an adversarial example) that exploit the weaknesses of classifiers and result in a different output. Here we introduce BODEGA: a benchmark for testing both victim models and attack methods on four misinformation detection tasks in an evaluation framework designed to simulate real use-cases of content moderation. We also systematically test the robustness of popular text classifiers against available attacking techniques and discover that, indeed, in some cases barely significant changes in input text can mislead the models. We openly share the BODEGA code and data in hope of enhancing the comparability and replicability of further research in this area. }


\maketitle

\section{Introduction}

\textit{Misinformation} is one of the most commonly recognised problems in modern digital societies. Under this term, we understand the publication and spreading of information that is not \textit{credible}, including fake news, manipulative propaganda, social media bots activity, rumours, hyperpartisan and biased journalism. While these problems differ in many aspects, what they have in common is non-credible (fake or malicious) content masquerading as credible: fake news as reliable news, bots as genuine users, falsehoods as facts, etc. \citep{Tucker2018,VanderLinden2022}.

Given that each type of content is present on the Internet in abundance, the assessment of credibility has fast been recognised as a task for machine learning (ML) or wider artificial intelligence (AI) solutions \citep{Ciampaglia2018}. It is common practice among major platforms with user-generated content to use such models for moderation, either as preliminary filtering before human judgement \citep{Singhal2022}, or as an automated detection system\footnote{\url{https://support.google.com/youtube/thread/192701791/updates-on-comment-spam-abuse?hl=en}}\footnote{\url{https://www.reuters.com/technology/twitter-exec-says-moving-fast-moderation-harmful-content-surges-2022-12-03/}}.

Are the state-of-the-art techniques of ML and, in particular, Natural Language Processing (NLP), up for a task of great importance to society? The standard analysis of model implementation with traditional accuracy metrics does not suffice here as it neglects how possible it is to systematically come up with variants of malicious text, known as \textit{adversarial examples} (AEs), that fulfil the original goal but evade detection \citep{Carter2021}. A realistic analysis in such a use case has to take into account an \textit{adversary}, i.e. the author of the non-credible content, who has both motivation and opportunity to experiment with the filtering system to find out its vulnerabilities.

Looking for such weaknesses via designing AE, to assess the \textit{robustness} of an investigated model, is a well-established problem in ML. However, its application to misinformation-oriented NLP tasks is relatively rare, despite the suitability of the adversarial scenario in this domain. Moreover, similarly to the situation in other domains, the adversarial attack performance depends on a variety of factors, such as the data used for training and testing, the attack goal, disturbance constraints, attacked models and evaluation measures. As a result, it’s very hard to choose the strongest attackers among many solutions that were evaluated in different environments.

In order to fill the need for reproducibility in this field, we present BODEGA (Benchmark fOr aDversarial Example Generation in credibility Assessment), intended as a common framework for comparing AE generation solutions to serve as a foundation for the creation of ``better-defended'' content credibility classifiers. In BODEGA, we provide the following elements of the evaluation process, prepared in a way that simulates the misinformation detection scenario:
\begin{enumerate}
    \item A collection of four NLP tasks from the domain of misinformation, cast as binary text classification problems (section \ref{sec:tasks}),
    \item A training and test dataset for each of the above tasks,
    \item Two attack scenarios, specifying what information is available to an adversary and what is their goal (section \ref{sec:attackscenario}),
    \item An evaluation procedure, measuring how successful a given attack is (section \ref{sec:evaluation}).
\end{enumerate}

Apart from laying the foundation for future evaluation, we use this opportunity to check how common text classification solutions (section \ref{sec:victimclassifiers}) fare against several AE generation solutions (section \ref{sec:baselinesolutions}). The results (section \ref{sec:results}) can inform future discussion on best practices in ML-powered content filtration.

BODEGA, based on the \textit{OpenAttack} framework and existing misinformation datasets, is openly available for download and use.\footnote{\url{https://github.com/piotrmp/BODEGA}}


\section{Related work}
\label{sec:relatedwork}

\subsection{Adversarial examples in NLP}

Searching for adversarial examples can be seen within wider efforts to investigate the \textit{robustness} of ML models, i.e. their ability to maintain good performance when confronted with data instances unlike those seen in training: anomalous, rare, adversarial or edge cases. This effort is especially important for deep learning models, which are not inherently interpretable, making it harder to predict their behaviour at the design stage. The seminal work on the subject by \citet{Szegedy2013} demonstrated low robustness of neural networks used to recognise images. The adversarial examples were prepared by adding specially prepared noise to the original image, which forced the change of classifier's decision even though the changes were barely perceptible visually and the original label remained valid.

Given the prevalence of neural networks in language processing, a lot of work has been done on investigating AEs in the context of NLP tasks \citep{Zhang2020b}, but the transition from the domain of images to text is far from trivial. Firstly, it can be a challenge to make changes small enough to the text, such that the original label remains applicable -- there is no equivalent of \textit{imperceptible noise} in text. The problem has been approached on several levels: of characters, making alterations that will likely remain unnoticed by a reader \citep{Gao2018a,eger-etal-2019-text}; of words, replaced while preserving the meaning by relying on thesauri \citep{ren-etal-2019-generating} or language models \citep{DBLP:conf/aaai/JinJZS20,li-etal-2020-bert-attack} and, finally, of sentences, by employing paraphrasing techniques \citep{Iyyer2018,ribeiro-etal-2018-semantically}. Secondly, the discrete nature of text means that methods based on exploring a feature space (e.g. guided by gradient) might suggest points that do not correspond to real text. Most of the approaches solve this by only considering modifications on the text level, but there are other solutions, for example finding optimal location in the embedding space followed by choosing its nearest neighbour that is a real word \citep{Gong2018}, or generating text samples from a distribution described by continuous parameters \citep{Guo2021}.

Apart from AE generation, a public-facing text classifier may be subject to many other types of attacks, including manipulations to output desired value when a trigger word is used \citep{Bagdasaryan2022} or perform an arbitrary task chosen by the attacker \citep{Neekhara2019}. Finally, verifying trustworthiness of a model aimed for deployment should also take into account undesirable behaviours exhibited without adversarial actions, e.g. its response to modification of protected attributes, such as gender, in the input \citep{Srivastava2023}.

\subsection{Robustness of credibility assessment}

The understanding that some deployment scenarios of NLP models justify expecting adversary actions predates the popularisation of deep neural networks, with the first considerations based on spam detection \citep{Dalvi2004}. The work that followed was varied in the explored tasks, attack scenarios and approaches.

The first attempts to experimentally verify the robustness of misinformation detection were based on simple manual changes \citep{Zhou2019b}. The approach of targeting a specific weakness and manually designing rules to exploit it has been particularly popular in attacking fact-checking solutions \citep{Thorne2019,Hidey2020}.

In the domain of social media analysis, \citet{Le2020} have examined the possibility of changing the output of a text credibility classifier by concatenating it with adversarial text, e.g. added as a comment below the main text. The main solution was working in the white-box scenario, with the black-box variant made possible by training a surrogate classifier on the original training data\footnote{We explain white- and black-box scenarios in Section \ref{sec:attackscenario}.}. It has also been shown that social media bot detection using AdaBoost is vulnerable to adversarial examples \cite{Kantartopoulos2020}. Adversarial scenarios have also been considered with user-generated content classification for other tasks, e.g. hate speech or satire \citep{Alsmadi2022}.

Fake news corpora have been used to verify effectiveness of AE generation techniques, e.g. in the study introducing TextFooler \cite{DBLP:conf/aaai/JinJZS20}. Interestingly, the study has shown that the classifier for fake news was significantly more resistant to attacks compared than those for other tasks, i.e. topic detection or sentiment analysis. This task also encouraged exploration of vulnerability to manually crafted modifications of input text \citep{Jaime2022}. In general, the fake news classification task has been a common subject of robustness assessment, involving both neural networks \citep{Ali2021a,Koenders2021} and non-neural classifiers \citep{Brown2020b,Smith2021}.

To sum up, while there has been several experiments examining the vulnerability of misinformation detection to adversarial attacks, virtually each of them has used a different dataset, a different classifier and a different attack technique, making it hard to draw conclusions and make comparisons. Our study is the first to analyse credibility assessment tasks and systematically evaluate their vulnerability to various attacks.

\subsection{Resources for adversarial examples}

The efforts of finding AEs are relatively new for NLP and there exist multiple approaches to evaluation procedures and dataset. The variety of studies for the misinformation tasks is reflective of the whole domain -- see the list of datasets used for evaluation provided by \citet{Zhang2020b}. Hopefully, as the field matures, some standard practices measures will emerge, facilitating comparison of approaches. We see BODEGA as a step in this direction.

Two types of existing efforts to bring community together are worth mentioning. Firstly, some related shared tasks have been organised. The \textit{Build It Break It, The Language Edition} task \citep{Ettinger2017} covered sentiment analysis and question answering, addressed by both 'builders' (building solutions) and 'breakers' (finding adversarial examples). The FEVER 2.0 shared task \citep{Thorne19FEVER2}, focusing on fact checking, had a 'Build-It' and 'Break-It' phases with similar setup. 

Secondly, two software packages were released to aid evaluation: \textit{TextAttack} \citep{morris-etal-2020-textattack} and \textit{OpenAttack} \citep{Zeng2021}. They both provide a software skeleton for setting up the attack and implementations of several AE generation methods. A user can add implementation of their own victims and attackers and perform evaluation. BODEGA code has been developed based on OpenAttack by providing access to misinformation-specific datasets, classifiers and evaluation measures.

\section{General schema}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.8\linewidth]{fig/BODEGA_arch.pdf} 
\end{center}
\caption{An overview of the evaluation of an adversarial attack using BODEGA. See description in text.}
\label{fig:architecture}
\end{figure*}

Adversarial example generation is a task aimed to test the robustness of ML models, known as \textit{victims} in this context. The goal is to find small modifications to the input data that will change the model output even though the original meaning is preserved and the correct response remains the same. If such changed instances, known as adversarial examples, could be systematically found, it means the victim classifier is vulnerable to the attack and not robust.

In context of classification, this setup (illustrated in Figure \ref{fig:architecture}) could be formalised through the following:
\begin{itemize}
    \item A training set $X_{train}$ and an attack set $X_{attack}$, each containing instances $(x_i, y_i)$, coupling the $i$-th instance features $x_i$ with its true class $y_i$,
    \item A victim model $f$, predicting a class label $\hat{y_i}$ based on instance features: $\hat{y_i}=f(x_i)$,
    \item A modification function $m$, turning $x_i$ into an adversarial example $x^*_i=m(x_i)$.
\end{itemize}
Throughout this study, we use $y_i=1$ (positive class) to denote non-credible information and $0$ for regular, credible content.

The goal of the attacker is to come up with the $m$ function. This process typically involves generating numerous variations of $x_i$ and querying the model's response to them, until the best candidate is selected. An evaluation procedure assesses the success of the attack on the set $X_{attack}$ by comparing $x_i$ to $x^*_i$ (which should be maximally similar) and $f(x_i)$ to $f(x^*_i)$ (which should be maximally different).

For example, consider a scenario in which a foreign actor aims to incite panic in a specific country by spreading false information about a hazardous fallout, under alarming headings such as $x_i=$\textit{Radioactive dust approaching from the Mediterranean!}. If similar scenarios were explored in the past and are included in $X_{train}$, the classifier $f$ will correctly recognise this misinformation by returning $\hat{y_i}=f(x_i)=1$. But the adversary might employ a modification function $m$ based on simple paraphrasing and come up with an adversarial example $x^*_i=$\textit{Radioactive dust \textbf{coming} from the Mediterranean!}. If the classifier is not robust and returns $f(x^*_i)=0$ for this variant, the attacker succeeds.


\section{Tasks}
\label{sec:tasks}

In BODEGA we include four misinformation detection tasks:
\begin{itemize}
    \item Style-based news bias assessment (HN),
    \item Propaganda detection (PR),
    \item Fact checking (FC),
    \item Rumour detection (RD).
\end{itemize}

For each of these problems, we rely on an already established dataset with credibility labels provided by expert annotators. The tasks are all presented as text classification.

Whenever data split is released with a corpus, the training subset is included as $X_{train}$. In order to enable the evaluation of AE generation solutions that carry a high computational cost, we define the $X_{attack}$ subset which is restricted to around 400 instances taken from the test set. The rest of the cases in the original test set are left out for future use as development subset. Table \ref{tab:dataset} summarises the data obtained.

\begin{table}
    \centering
    \begin{tabular}{rrrrr}
    \hline
    \textbf{Task} & \textbf{Training} & \textbf{Attack} & \textbf{Dev.} & \textbf{Positive \%} \\
    \hline
        HN & 60,235 & 400 & 3,600 & 50.00\% \\
        PR & 12,675 & 416 & 3,320 & 29.42\% \\
        FC & 172,763 & 405 & 19,010 & 51.27\% \\
        RD & 8,694 & 415 & 2,070 & 32.68\% \\
    \hline
    \end{tabular}
    \caption{Four datasets used in BODEGA, described by the task (see descriptions in text), number of instances in training, attack and development subsets, and an overall percentage of positive (non-credible) class.}
    \label{tab:dataset}
\end{table}

In the following subsections we outline the motivation, origin and data processing within each of the tasks.

\subsection{HN: Style-based news bias assessment}

Solutions for news credibility assessment, sometimes equated with \textit{fake news} detection, usually rely on one of three factors: (1) writing style \citep{Horne2017,Przybya2020}, (2) veracity of included claims \citep{Vlachos2014,Graves2018} or (3) context of social and traditional media  \citep{Shu2019,Liu2020a}.

In this task, we focus on the writing style. This means a whole news article is provided to a classifier, which has no ability to check facts against external sources, but has been trained on enough articles to recognise stylistic cues. The training data include numerous articles coming from sources with known credibility, allowing one to learn writing styles typical for credible and non-credible outlets.

In BODEGA, we employ a corpus of news articles \citep{potthast-etal-2018-stylometric} used for the task of \textit{Hyperpartisan News Detection} at SemEval-2019 \citep{kiesel-etal-2019-semeval}. The credibility was assigned based on the overall bias of the source, assessed by journalists from \textit{BuzzFeed} and \textit{MediaBiasFactCheck.com}\footnote{\url{https://zenodo.org/record/1489920}}. We use 1/10th of the training set (60,235 articles) and assign label $1$ (non-credible) to articles from sources annotated as hyperpartisan, both right- and left-wing.

\subsection{PR: Propaganda detection}

The task of propaganda detection involves detecting text passages, whose author tries to influence the reader by means other than objective presentation of the facts, for example by appealing to emotions or exploiting common fallacies \citep{Smith1989a}. Usage of propaganda techniques does not necessarily imply falsehood, but in the context of journalism is associated with manipulative, dishonest and hyperpartisan writing. In BODEGA, we use the corpus accompanying SemEval 2020 Task 11 (\textit{Detection of Propaganda Techniques in News Articles}), with 14 propaganda techniques annotated in 371 newspapers articles by professional annotators \cite{Martino2020a}.

Propaganda detection is a fine-grained task, with SemEval data annotated on the token level, akin to a Named Entity Recognition (NER) task. In order to cast it as a text classification problem as others here, we split the text on sentence level and assign target label equal 1 to sentences overlapping with any propaganda instances and 0 to the rest. Because only the training subset is made publicly available\footnote{\url{https://zenodo.org/record/3952415}}, we randomly extract 20\% of documents for attack and development subsets.

\subsection{FC: Fact checking}

Fact checking is the most advanced way human experts can verify credibility of a given text: by assessing the veracity of the claims it includes with respect to a knowledge base (drawing from memory, reliable sources and common sense). Implementing this workflow in AI systems as computational fact checking \citep{Graves2018} is a promising direction for credibility assessment. However, it involves many challenges -- choosing check-worthy statements \citep{Nakov2022}, finding reliable sources \citep{9891941}, extracting relevant passages \citep{karpukhin-etal-2020-dense} etc. Here we focus on the claim verification stage. The input of the task is a pair of texts – target claim and relevant evidence – and the output label indicates whether the evidence supports the claim or refutes it. It essentially is Natural Language Inference (NLI) \citep{MacCartney2009} in the domain of encyclopaedic knowledge and newsworthy events.

We use the data\footnote{\url{https://fever.ai/dataset/fever.html}} from FEVER shared task \citep{Thorne2018}, aimed to evaluate fact-checking solutions through a manually created set of evidence-claim pairs. Each pair connects a one-sentence claim with a set of sentences from Wikipedia articles, including a label of SUPPORTS (the evidence justifies the claim), REFUTES (the evidence demonstrates the claim to be false) or NOT ENOUGH INFO (the evidence is not sufficient to verify the claim). For the purpose of BODEGA, we take the claims from the first two categories\footnote{NOT ENOUGH INFO was excluded to cast the task as binary classification, in line with the other ones.}, concatenating all the evidence text\footnote{Including the titles, which are often an essential part of the context in case of encyclopaedic articles.}. The labels for the test set are not openly available, so we use the development set in this role.

See below an example of a REFUTES instance:\\
\textit{\underline{David Bowie.} During his lifetime, his record sales, estimated at 140 million worldwide, made him one of the world's best-selling music artists. $\rightarrow$	David Bowie only sold records in Jamaica.}\\
The following SUPPORTS instance combines evidence from two articles:\\
\textit{\underline{Cersei Lannister.} She subsequently appeared in A Clash of Kings (1998) and A Storm of Swords (2000). \underline{A Clash of Kings.} A Clash of Kings is the second novel in A Song of Ice and Fire, an epic fantasy series by American author George R. R. Martin expected to consist of seven volumes. $\rightarrow$ Cersei Lannister appears in a series that was written by an author from the United States.}


\subsection{RD: Rumour detection}

A rumour is an information spreading between people despite not having a reliable source. In the online misinformation context, the term is used to refer to content shared between users of social media that comes from an unreliable origin, e.g. an anonymous account. Not every rumour is untrue as some of them can be later confirmed by established sources. Rumours can be detected by a variety of signals \citep{Al-Sarem2019}, but here we focus of the textual content of the original post and follow-ups from other social media users.

In BODEGA we use the Augmented dataset of rumours and non-rumours for rumour detection \citep{Han2019}, created from Twitter threads relevant to six real-world events (2013 Boston marathon bombings, 2014 Ottawa shooting, 2014 Sydney siege, 2015 Charlie Hebdo Attack, 2014 Ferguson unrest, 2015 Germanwings plane crash). The authors of the dataset started with the core threads annotated manually as rumours and non-rumours, then automatically augmented them with other threads based on textual similarity. We followed this by converting each thread to a flat feed of concatenated text fragments, including the initial post and subsequent responses. See table \ref{tab:RDexamples} for examples of tweet threads. We set aside one of the events (Charlie Hebdo attack) for attack and development subsets, while others are included in the training subset.
    
\section{Attack scenario}
\label{sec:attackscenario}

The adversarial attack scenarios are often classified according to what information is available to the attacker. The \textit{black-box} scenarios assume that no information is given on the inner workings of the targeted model and only system outputs for a given input can be observed. In \textit{white-box} scenarios, the model is fully accessible, allowing methods for generating AEs to be precisely tuned to the model weights, mainly gradient-based \citep{Zhang2020b}.

We argue neither of these scenarios is realistic in the practical misinformation detection setting, e.g. a content filter deployed in a social network. We cannot assume a model is available to the attacker since such information is usually not shared publicly; moreover, the model likely gets updated often to keep up with the current topics. On the other hand, the black-box scenario is too restrictive, as some information about the deployed model can be accessed or inferred. Also, once a certain design approach is popularised as the best performing in the NLP community (e.g a pretrained  language model), it tends to be applied to very many, if not most, solutions to related problems.

For these reasons, in BODEGA we use the \textit{grey-box} approach. The following information is considered available to an attacker preparing AEs:
\begin{itemize}
    \item A ``hidden'' classifier $f$ that for any arbitrary input returns $f(x) \in \{0, 1\}$ and a probability score $s_f(x)$, i.e. a numerical representation on how likely a given example $x$ is to be assigned a positive class. This information is more helpful to attackers than only $f(x)$, which is typically set by applying a threshold $t_f$, e.g. $f(x) = 1 \iff s_f(x)> t_f$.
    \item The general description of an architecture of classifier $f$, e.g. ‘a BERT encoder followed by a dense layer and softmax normalisation’.
    \item The training $X_{train}$, the development $X_{dev}$, and the evaluation $X_{attack}$ subsets.
\end{itemize}
This setup allows users of BODEGA to exploit weaknesses of classifiers without using the complete knowledge of the model, while maintaining some resemblance of practical scenarios.

Another choice that needs to be made concerns the goal of the attacker. Generally, adversarial actions are divided into \textit{untargeted} attacks, where any change of the victim's predictions is considered a success and \textit{targeted} attacks, which seek to obtain a specific response, aligned with the attacker's goals \citep{Zhang2020b}.

Consider a classifier $f$ that for a given instance $x_i$, with true value $y_i$, outputs class $f(x_i)$, which may be correct or incorrect. An \textit{untargeted} attack involves perturbing $x_i$ into $x^*_i$, such that $f(x_i)\neq f(x^*_i)$. A successful attack would undoubtedly show the brittleness, but may not be necessarily helpful for a malicious user, e.g. if $y_i$ corresponded to malicious content, but the original response $f(x_i)$ was incorrect.

Taking into account the misinformation scenario, we consider the \textit{targeted} attack to satisfy the following criteria:
\begin{itemize}
    \item The true class corresponds to non-credible content, i.e. $y_i=1$,
    \item The original classifier response was correct, i.e. $f(x_i)=y_i$.
\end{itemize}
Success in this attack corresponds to a scenario of the attacker preparing a piece of non-credible content that is falsely recognised as credible thanks to the adversarial modification. We use only a portion of the evaluation $X_{attack}$ subset for this kind of attack. 

By \textit{non-credible content} we mean:
\begin{itemize}
    \item In case of news bias assessment, an article from a hyperpartisan source,
    \item In case of propaganda detection, a sentence with a propaganda technique,
    \item In case of fact checking, a statement refuted by the provided evidence,
    \item In case of rumour detection, a message feed starting from a post including a rumour.
\end{itemize}

In BODEGA, both untargeted and targeted attacks can be evaluated. 

All of the text forming an instance can be modified to make an adversarial attacks. In case of fact checking, this includes both the claim and the evidence. Similarly for rumour detection, not only the original rumour, but also any of the follow-up messages in the thread. This corresponds to the real-life scenario, where all of the above content is user generated and can to some degree be influenced by an attacker.

Finally, note that BODEGA imposes no restriction on the number of queries sent to the victim, i.e. the number of variants an attacker is allowed to test for each instance before providing the final modification. This number would typically be limited, especially in a security-oriented application \citep{Chen2022}. The average number of queries for each method is computed as part of the evaluation (see the next section). 

\section{Evaluation}
\label{sec:evaluation}

Preparing adversarial examples involves balancing two goals in the adversarial attack (see Figure \ref{fig:architecture}):
\begin{enumerate}
    \item Maximising $\text{diff}(f(x_i), f(x^*_i))$ – difference between the classes predicted by the classifier for the original and perturbed instance,
    \item Maximising $\text{sim}(x_i, x^*_i)$ – similarity between the original and perturbed instance.
\end{enumerate}

If (1) is too small, the attack has failed, since the classifier preserved the correct prediction. If (2) is too small, the attack has failed, since the necessary perturbation was so large it defeated the original purpose of the text.

This makes the evaluation multi-criterion and challenging, since neither of these factors measured in isolation reflects the quality of AEs. The conundrum is usually resolved by setting the minimum similarity (2) to a fixed threshold (known as \textit{perturbation constraint}) and measuring the reduction in classification performance, i.e. accuracy reduction \citep{Zhang2020b}. This can be problematic as there are no easy ways to decide the value of the threshold that will guarantee that the class remains valid. The issue is especially valid for a task as subtle as credibility analysis --  e.g. how many word swaps can we do on a real news piece before it loses credibility?

In BODEGA we avoid this problem by inverting the approach. Instead of imposing constraints to goal (2) and using (1) as evaluation measure, we impose constraints on (1) and use (2) for evaluation. The prediction change is restricted by using a natural threshold -- if the classifier has changed its decision, the attack is valid. As a result, the similarity between the original and perturbed text becomes available to construct an evaluation measure. The higher this similarity, the better the AE generator is at performing imperceptible, yet significant, changes.

We define an adversarial modification quality score, called BODEGA score. BODEGA score always lies within 0-1 and a high value indicates good quality modification preserving the original meaning (with score=1 corresponding to no visible change), while low value indicates poor modification, altering the meaning (with score=0 corresponding to completely different text).

In the remainder of this chapter, we discuss the similarity measurement techniques we employ and outline how they are combined to form a final measure of attack success.

\subsection{BERT score}

The first element used to measure meaning preservation is \textit{BERT score} \citep{DBLP:conf/iclr/ZhangKWWA20}. BERT score was designed to compute similarity between a candidate and reference sentences in evaluating solutions for natural language generation tasks (e.g. machine translation). The score has a maximum of 1 (identical text) and is calibrated to \textit{typically} reach a minimum of 0 (no similarity).

BERT score helps to properly assess semantic similarity. That means, for example, that replacing a single word with its close synonym will yield high score value, while using a completely different one will not. However, BERT score is trained to interpret multi-word modifications (i.e. paraphrases) as well, leading to better correlation with human judgement than other popular measures, e.g. BLEU. In BODEGA, we use the variant based one the \texttt{microsoft/deberta-large-mnli} language model \citep{DBLP:conf/iclr/HeLGC21}.

\subsection{Levenshtein distance}

Levenshtein distance is used to express how different one string of characters is from another. Specifically, it computes the minimum number of elementary modifications (character additions, removals, replacements) it would take to transform one sequence into another \cite{1965}.

Levenshtein is a simple measure that does not take into account the meaning of the words. However, it is helpful to properly assess modifications that rely on graphical resemblance. For example, one family of adversarial attacks relies on replacing individual characters in text (e.g. \textit{call} to \textit{ca$||$}), altering the attacked classifier’s output. The low value of Levenshtein distance in this case represents the fact that such modification may be imperceptible for a human reader.

In order to turn Levenshtein distance $lev\_dist(a,b)$ into a similarity score,  we compute the following:
\[
\text{Lev\_score}(a, b) = 1 - \frac{lev\_dist(a, b)}{max(|a|, |b|)}
\]

$\text{Lev\_score}$ is between 0 and 1, with higher values corresponding to larger similarity, with $\text{Lev\_score}(a, b) = 1$  if $a$ and $b$ are the same and $\text{Lev\_score}(a, b) = 0$ if they have no common characters at all.

\subsection{BODEGA score}

The BODEGA score for a pair of original text $x_i$ and modified text $x^*_i$ is defined as follows:
\begin{align*}
\text{BODEGA\_score}&(x_i,x^*_i) = \\
    0\;\;\text{if } x^*_i=\varnothing &\;\;\text{or}\;\;f(x_i)= f(x^*_i)\\
    &\;\;\text{or}\;\;\text{BERT\_score}(x_i,x^*_i)<0\\
    \text{BERT\_score}&(x_i,x^*_i)  \times \text{Lev\_score}(x_i,x^*_i)\;\;\text{otherwise}
\end{align*}
The above allows to properly handle the situations when BERT score is negative, no AE is provided by a solution, or the example fails to change the victim's decision, assigning 0 in all these cases. Otherwise, a product of BERT and Levenshtein scores is returned.

The overall attack success measure is computed as an average over BODEGA scores for all instances in the attack set available in a given scenario (targeted or untargeted).

The success measure reaches 0 when the AEs bear no similarity to the originals, or they were not created at all. The value of 1 corresponds to the, unachievable in practice, situation when AEs change the victim model's output with immeasurably small perturbation.

Note that in our formulation we use measures, especially BERT score, designed for sentence level. However, two of the tasks (news bias assessment and rumour detection) involve longer texts, allowing higher-level modifications, such as reordering of sentences (or tweets), which are not explicitly taken into account by the evaluation measure. To the best of our knowledge, the discourse level has not been considered in the AE generation solutions and we leave a proper evaluation of such modifications for future work.

Many adversarial attack methods include tokenisation that does not preserve the word case or spacing between them. Our implementation of the scoring disregards such discrepancies between input and output, as they are not part of the intended adversarial modifications.

Apart from BODEGA score, expressing the overall success, three intermediate measures can be extracted from the above process to paint a fuller picture of the strengths and weaknesses of a particular solution:
\begin{itemize}
    \item Confusion score – in how many of the test cases the victim's decision was changed,
    \item BERT score – an average over the cases with changed decision,
    \item Levenshtein score – an average over the cases with changed decision.
\end{itemize}

We also report the number of queries made to the victim, averaged over all instances.

\section{Victim classifiers}
\label{sec:victimclassifiers}

A victim classifier is necessary to perform an evaluation of an AE generation solution. At this point, we include implementations of two models representing common approaches to text classification: a recurrent neural network (BiLSTM) and a fine-tuned langauge model (BERT).

This component of BODEGA could be easily replaced by newer implementations, either to test a robustness of a specific classifier architecture, or to have a better understanding of applicability of a given AE generation solution.

\subsection{BERT}

As the pretrained language model we use BERT in the \texttt{bert-base-uncased} variant \citep{Devlin2018}. The model is fine-tuned for sequence classification using Adam optimiser with linear weight decay \citep{Loshchilov}, starting from 0.00005, for 5 epochs. We use maximum input length of 512 characters and a batch size of 16. The training is implemented using the \textit{Hugging Face Transformers} library \citep{wolf-etal-2020-transformers}.

\subsection{BiLSTM}

The recurrent network is implemented using the following layers:
\begin{itemize}
    \item An embedding layer, representing each token as vector of length 32,
    \item Two LSTM \citep{Hochreiter1997a} layers (forwards and backwards), using hidden representation of length 128, returned from the edge cells and concatenated as document representation of length 256,
    \item A dense linear layer, computing two scores representing the two classes, normalised to probabilities through softmax.
\end{itemize}

Similarly to the other model, the input is tokenised using BERT uncased tokeniser. The maximum allowed input length is 512, with padding as necessary. For each of the tasks, a model instance is trained from scratch for 10 epochs by using Adam optimiser \citep{Kingma2015a}, a learning rate of 0.001 and batches of 32 examples each. The implementation uses \textit{PyTorch}.


\subsection{Classification performance}

\begin{table}
    \centering
    \begin{tabular}{rrr}
    \hline
     & \multicolumn{2}{c}{\textbf{F-score}} \\
    \textbf{Task} & \textbf{BiLSTM} & \textbf{BERT}\\
    \hline
        HN & 0.6790 & 0.7185 \\
        PR & 0.4413 & 0.6097\\
        FC & 0.7417 & 0.9308  \\
        RD & 0.6127 & 0.7247 \\
    \hline
    \end{tabular}
    \caption{Performance of the victim classifiers, expressed as F-score over combined development and attack subsets.}
    \label{tab:victims}
\end{table}

Table \ref{tab:victims} shows the performance of the victim classifiers, computed as F-score over the test data (combined development and attack subsets). As expected, the pretrained language model easily outperforms a neural network trained from scratch. The credibility assessment task is a subtle one and the amount of data available for training severely limits the performance. Thus, the BERT model has an advantage by relying on knowledge gathered during pretraining. This is demonstrated by the performance gap being the largest for the dataset with the least data available (propaganda detection) and the smallest for the most abundant corpus (hyperpartisan news).

\section{AE generation solutions}
\label{sec:baselinesolutions}
Within BODEGA, we include the AE generation solutions implemented in the \textit{OpenAttack} framework. We exclude the approaches for white-box scenario (gradient-based) and those that yielded poor performance in preliminary tests. We test 8 approaches:
\begin{itemize}
    \item BAE \citep{garg-ramakrishnan-2020-bae},
    \item BERT-ATTACK \citep{li-etal-2020-bert-attack},
    \item DeepWordBug \citep{Gao2018a},
    \item Genetic \citep{alzantot-etal-2018-generating},
    \item SememePSO \citep{zang-etal-2020-word},
    \item PWWS \citep{ren-etal-2019-generating},
    \item SCPN \citep{Iyyer2018},
    \item TextFooler \citep{DBLP:conf/aaai/JinJZS20}.
\end{itemize}

The majority of them operate on word-level, seeking replacements that would influence the classification result without modifying the meaning. The exceptions are sentence-level SCPN, performing paraphrasing of entire sentences, and character-level DeepWordBug, replacing individual characters in text to preserve superficial similarity to the original. They all use victims' scores to look for most promising modifications, except for SCPN, which operates blindly, simply generating numerous possible paraphrases. 

All of the attackers are executed with their default functionality, except for BERT-ATTACK, used without the generation of subword permutations, which is prohibitively slow for longer documents. Just like the victim classifier, the AE solution interface in BODEGA allows for the new solutions to be added and tested as the field progresses.

\section{Experiments}
\label{sec:results}

The purpose of the experiments is to test the BODEGA framework in action and establish a baseline for systematic evaluation of the robustness of credibility assessment solutions. To that end, we test each of the victim classifiers for each of the task using the available AE generation solutions. The evaluation involves both targeted and untargeted scenarios and is performed using the measures introduced in section \ref{sec:evaluation}.

\subsection{Victim: BERT}
\begin{table*}[t]
    \centering
    \begin{tabular}{rr|rrrrr|rrrrr|}
    &  & \multicolumn{5}{|c|}{\textbf{Untargeted}}  & \multicolumn{5}{|c|}{\textbf{Targeted}}\\
    \textbf{Task} & \textbf{Method} & \textbf{BODEGA} & \textbf{C-s} & \textbf{B-s} & \textbf{L-s} & \textbf{Q.}  & \textbf{BODEGA} & \textbf{C-s} & \textbf{B-s} & \textbf{L-s} & \textbf{Q.} \\
    \hline
        HN & BAE  & {0.48} & {0.60} & {0.82} & {0.96} & {606.83} & {0.26} & {0.34} & {0.81} & {0.95} & {713.42} \\
         & BERT-ATTACK  & \textbf{0.80} & \textbf{0.96} & {0.85} & {0.97} & {648.41} & \textbf{0.76} & \textbf{0.95} & {0.84} & {0.95} & {753.91} \\
         & DeepWordBug  & {0.29} & {0.32} & \textbf{0.91} & \textbf{1.00} & {395.96} & {0.15} & {0.16} & \textbf{0.90} & \textbf{1.00} & {389.77} \\
         & Genetic  & {0.68} & {0.86} & {0.81} & {0.97} & {2734.63} & {0.55} & {0.73} & {0.79} & {0.96} & {4511.63} \\
         & SememePSO  & {0.28} & {0.34} & {0.84} & {0.99} & {343.51} & {0.14} & {0.16} & {0.85} & {0.99} & {409.22} \\
         & PWWS  & {0.65} & {0.82} & {0.81} & {0.97} & {2070.78} & {0.48} & {0.64} & {0.78} & {0.94} & {2107.02} \\
         & SCPN  & {0.00} & {0.92} & {0.00} & {0.02} & {11.84} & {0.00} & \textbf{0.95} & {0.00} & {0.02} & {11.89} \\
         & TextFooler  & {0.67} & {0.92} & {0.77} & {0.93} & {660.52} & {0.55} & {0.85} & {0.71} & {0.88} & {850.79} \\
    \hline
        PR & BAE  & {0.13} & {0.18} & \textbf{0.77} & {0.93} & {33.96} & {0.14} & {0.20} & {0.77} & {0.93} & {45.68} \\
         & BERT-ATTACK  & {0.47} & {0.70} & {0.77} & {0.88} & {80.16} & \textbf{0.57} & {0.79} & \textbf{0.80} & {0.90} & {99.95} \\
         & DeepWordBug  & {0.18} & {0.34} & {0.56} & \textbf{0.95} & {27.40} & {0.40} & {0.73} & {0.57} & \textbf{0.96} & {36.13} \\
         & Genetic  & \textbf{0.51} & \textbf{0.84} & {0.69} & {0.87} & {956.23} & {0.54} & \textbf{0.88} & {0.70} & {0.85} & {1152.21} \\
         & SememePSO  & {0.41} & {0.68} & {0.69} & {0.87} & {95.43} & {0.34} & {0.51} & {0.74} & {0.90} & {181.05} \\
         & PWWS  & {0.49} & {0.75} & {0.72} & {0.89} & {131.92} & {0.47} & {0.72} & {0.73} & {0.87} & {179.68} \\
         & SCPN  & {0.08} & {0.47} & {0.30} & {0.44} & {11.47} & {0.11} & {0.79} & {0.28} & {0.37} & {11.79} \\
         & TextFooler  & {0.45} & {0.77} & {0.67} & {0.84} & {57.94} & {0.49} & {0.77} & {0.72} & {0.86} & {77.81} \\
    \hline
        FC & BAE  & {0.34} & {0.51} & {0.70} & {0.95} & {80.69} & {0.17} & {0.27} & {0.67} & {0.93} & {92.47} \\
         & BERT-ATTACK  & \textbf{0.50} & {0.77} & {0.69} & {0.94} & {146.73} & \textbf{0.37} & {0.62} & {0.65} & {0.92} & {207.23} \\
         & DeepWordBug  & {0.38} & {0.54} & \textbf{0.71} & \textbf{0.98} & {54.33} & {0.17} & {0.25} & \textbf{0.70} & \textbf{0.98} & {52.29} \\
         & Genetic  & {0.50} & {0.78} & {0.68} & {0.93} & {1223.82} & {0.37} & {0.64} & {0.64} & {0.90} & {1766.85} \\
         & SememePSO  & {0.42} & {0.64} & {0.69} & {0.95} & {150.71} & {0.23} & {0.37} & {0.68} & {0.94} & {226.37} \\
         & PWWS  & {0.46} & {0.69} & {0.70} & {0.95} & {225.27} & {0.29} & {0.47} & {0.66} & {0.92} & {226.78} \\
         & SCPN  & {0.05} & \textbf{0.90} & {0.15} & {0.30} & {11.90} & {0.05} & \textbf{0.97} & {0.14} & {0.29} & {11.97} \\
         & TextFooler  & {0.44} & {0.70} & {0.67} & {0.92} & {106.13} & {0.27} & {0.49} & {0.61} & {0.86} & {131.88} \\
    \hline
        RD & BAE  & {0.15} & {0.18} & \textbf{0.82} & {0.97} & {313.01} & {0.35} & {0.44} & \textbf{0.82} & {0.97} & {196.69} \\
         & BERT-ATTACK  & \textbf{0.33} & {0.44} & {0.80} & {0.95} & {774.31} & \textbf{0.54} & {0.69} & {0.81} & {0.97} & {366.14} \\
         & DeepWordBug  & {0.18} & {0.23} & {0.79} & \textbf{0.99} & {232.74} & {0.42} & {0.54} & {0.78} & \textbf{0.99} & {174.01} \\
         & Genetic  & {0.31} & \textbf{0.47} & {0.70} & {0.95} & {4427.71} & {0.52} & {0.78} & {0.70} & {0.95} & {2339.58} \\
         & SememePSO  & {0.15} & {0.21} & {0.71} & {0.96} & {347.59} & {0.37} & {0.54} & {0.71} & {0.96} & {223.97} \\
         & PWWS  & {0.25} & {0.38} & {0.69} & {0.94} & {1105.99} & {0.48} & {0.75} & {0.68} & {0.92} & {838.83} \\
         & SCPN  & {0.00} & {0.38} & {0.01} & {0.10} & {11.35} & {0.00} & \textbf{0.90} & {0.02} & {0.10} & {11.90} \\
         & TextFooler  & {0.24} & {0.41} & {0.65} & {0.89} & {657.15} & {0.47} & {0.70} & {0.70} & {0.95} & {358.37} \\
    \hline
    \end{tabular}
    \caption{The results of adversarial attacks on the \textbf{BERT classifier} in four misinformation detection tasks in untargeted and targeted scenario. Evaluation measures include BODEGA score (BODEGA), confusion score (C-s), BERT score (B-s), Levenshtein score (L-s) and number of queries to the attacked model (Q.). The best score in each task and scenario is in boldface. }
    \label{tab:BERTresults}
\end{table*}

Table \ref{tab:BERTresults} includes the results of the attack on the BERT classifier. 

The hyperpartisan news detection task is clearly the easiest one for generating AEs. In untargeted scenario, BERT-ATTACK achieves BODEGA score of 0.8, the best value in attacks against BERT. This is possible due to changing the decision on 96\% of the instances while preserving high similarity, both in terms of semantics and characters. However, DeepWordBug (a character-level method) provides the best results in terms of similarity, changing less than 1\% of characters on average. The only drawback of this method is that it works 29\% of the cases, failing to change the victim's decision in the remaining ones. The targeted scenario broadly shows the same situation, with the attack being slightly harder (for BERT-ATTACK) or much harder (for other methods).

The propaganda recognition task significantly differs from the previous task in terms of text length, including individual sentences rather than full articles. As a result, every word is more important and it becomes much harder to preserve the original meaning after replacement, e.g. DeepWordBug  achieves BERT score of 0.56 compared to 0.91 in hyperpartisan news detection. This setup appears to favour the Genetic method, obtaining the the best BODEGA score in the untargeted scenario: 0.51. This approach performs well across the board, but it comes at a high cost in terms of model queries. Even for the short sentences in propaganda detection, a victim model is queried around a 1000 times, compared to less than 150 for all other methods. In this case, the targeted scenario is slightly easier and favours BERT-ATTACK, achieving the best meaning preservation.

Fact checking resembles the propaganda detection in terms of relatively short text fragments and the best-performing methods -- Genetic or BERT-ATTACK. While SCPN achieves the best confusion score here, it fails to preserve the meaning, both in terms of semantics and surface level. A method that changes the victim's decision at a cost of completely transforming the instance text is not useful. Interestingly, the targeted scenario here appears to be much more challenging, resulting in the best BODEGA score of 0.37 as opposed to 0.50 in the untargeted case. In case of false statements debunked by evidence, the victims have high confidence in their outputs and are not easily swayed.

Finally, the rumour detection task, which in the untargeted scenario appears to be the hardest problem to attack of all. Here only two methods manage to exceed BODEGA score of 0.3, indicating low usability, mostly due to low confusion rates. This may be because rumour threads consist of numerous posts, each having some indication on the credibility of the news, forcing an attacker to make many modifications to change the victim's decision. The text of Twitter messages is also far from regular language, making the challenge harder for methods using models pretrained on well-formed text (e.g. BERT-ATTACK). It has to be noted however that this setup is equally problematic to the meaning preservation measurement (BERT score), thus suggesting these results should be taken cautiously.

\subsection{Victim: BiLSTM}
\begin{table*}[t]
    \centering
    \begin{tabular}{rr|rrrrr|rrrrr|}
    &  & \multicolumn{5}{|c|}{\textbf{Untargeted}}  & \multicolumn{5}{|c|}{\textbf{Targeted}}\\
    \textbf{Task} & \textbf{Method} & \textbf{BODEGA} & \textbf{C-s} & \textbf{B-s} & \textbf{L-s} & \textbf{Q.}  & \textbf{BODEGA} & \textbf{C-s} & \textbf{B-s} & \textbf{L-s} & \textbf{Q.} \\
    \hline
        HN & BAE  & {0.65} & {0.77} & {0.86} & {0.98} & {489.27} & {0.62} & {0.74} & {0.85} & {0.97} & {477.65} \\
         & BERT-ATTACK  & \textbf{0.85} & \textbf{0.98} & {0.88} & {0.99} & {487.85} & \textbf{0.82} & \textbf{0.96} & {0.87} & {0.98} & {565.05} \\
         & DeepWordBug  & {0.44} & {0.49} & \textbf{0.90} & \textbf{1.00} & {396.13} & {0.46} & {0.51} & \textbf{0.90} & \textbf{1.00} & {379.25} \\
         & Genetic  & {0.71} & {0.88} & {0.82} & {0.98} & {2085.89} & {0.64} & {0.81} & {0.81} & {0.97} & {2852.15} \\
         & SememePSO  & {0.35} & {0.42} & {0.85} & {0.99} & {317.83} & {0.24} & {0.29} & {0.83} & {0.99} & {356.63} \\
         & PWWS  & {0.76} & {0.93} & {0.83} & {0.98} & {2042.98} & {0.71} & {0.90} & {0.81} & {0.97} & {1990.75} \\
         & SCPN  & {0.00} & {0.79} & {0.00} & {0.02} & {11.86} & {0.00} & {0.80} & {0.00} & {0.02} & {11.83} \\
         & TextFooler  & {0.74} & {0.94} & {0.81} & {0.96} & {552.68} & {0.69} & {0.90} & {0.79} & {0.95} & {618.85} \\
    \hline
        PR & BAE  & {0.17} & {0.23} & \textbf{0.80} & {0.93} & {32.94} & {0.29} & {0.38} & {0.82} & {0.93} & {38.72} \\
         & BERT-ATTACK  & \textbf{0.57} & {0.80} & {0.79} & {0.90} & {61.41} & \textbf{0.73} & {0.94} & \textbf{0.83} & {0.92} & {50.14} \\
         & DeepWordBug  & {0.21} & {0.41} & {0.52} & \textbf{0.95} & {27.48} & {0.41} & {0.72} & {0.59} & \textbf{0.95} & {35.30} \\
         & Genetic  & {0.52} & {0.84} & {0.69} & {0.86} & {787.03} & {0.61} & {0.90} & {0.75} & {0.90} & {616.42} \\
         & SememePSO  & {0.46} & {0.74} & {0.70} & {0.87} & {86.14} & {0.64} & {0.90} & {0.77} & {0.92} & {75.58} \\
         & PWWS  & {0.53} & \textbf{0.84} & {0.71} & {0.88} & {130.89} & {0.66} & {0.92} & {0.78} & {0.92} & {168.56} \\
         & SCPN  & {0.09} & {0.47} & {0.33} & {0.47} & {11.55} & {0.15} & {0.86} & {0.30} & {0.44} & {11.98} \\
         & TextFooler  & {0.50} & {0.83} & {0.69} & {0.85} & {54.26} & {0.66} & \textbf{0.96} & {0.76} & {0.90} & {55.08} \\
    \hline
        FC & BAE  & {0.37} & {0.55} & {0.71} & {0.95} & {77.80} & {0.32} & {0.48} & {0.69} & {0.95} & {73.55} \\
         & BERT-ATTACK  & \textbf{0.58} & \textbf{0.85} & {0.72} & {0.94} & {135.18} & \textbf{0.57} & {0.85} & {0.70} & {0.95} & {123.24} \\
         & DeepWordBug  & {0.41} & {0.58} & \textbf{0.72} & \textbf{0.98} & {54.36} & {0.47} & {0.66} & \textbf{0.72} & \textbf{0.98} & {50.74} \\
         & Genetic  & {0.54} & {0.80} & {0.71} & {0.94} & {815.32} & {0.48} & {0.74} & {0.69} & {0.94} & {1039.01} \\
         & SememePSO  & {0.52} & {0.77} & {0.71} & {0.94} & {119.69} & {0.43} & {0.66} & {0.70} & {0.94} & {140.78} \\
         & PWWS  & {0.56} & {0.82} & {0.72} & {0.95} & {221.82} & {0.47} & {0.72} & {0.69} & {0.93} & {212.00} \\
         & SCPN  & {0.04} & {0.70} & {0.15} & {0.31} & {11.75} & {0.07} & \textbf{0.95} & {0.17} & {0.34} & {12.00} \\
         & TextFooler  & {0.51} & {0.78} & {0.70} & {0.92} & {104.16} & {0.44} & {0.68} & {0.69} & {0.93} & {107.34} \\
    \hline
        RD & BAE  & {0.17} & {0.21} & \textbf{0.82} & {0.98} & {312.86} & {0.51} & {0.64} & \textbf{0.82} & {0.98} & {123.16} \\
         & BERT-ATTACK  & \textbf{0.49} & \textbf{0.79} & {0.70} & {0.87} & {985.52} & \textbf{0.75} & {0.95} & {0.81} & {0.97} & {130.64} \\
         & DeepWordBug  & {0.19} & {0.25} & {0.78} & \textbf{0.99} & {232.75} & {0.66} & {0.87} & {0.77} & \textbf{0.99} & {153.57} \\
         & Genetic  & {0.48} & {0.70} & {0.72} & {0.95} & {3259.13} & {0.59} & {0.88} & {0.71} & {0.94} & {1551.81} \\
         & SememePSO  & {0.21} & {0.29} & {0.73} & {0.97} & {321.96} & {0.42} & {0.60} & {0.72} & {0.96} & {190.00} \\
         & PWWS  & {0.46} & {0.65} & {0.73} & {0.96} & {1059.15} & {0.64} & \textbf{0.96} & {0.71} & {0.94} & {742.60} \\
         & SCPN  & {0.00} & {0.47} & {0.01} & {0.09} & {11.53} & {0.00} & {0.47} & {0.02} & {0.10} & {11.84} \\
         & TextFooler  & {0.34} & {0.64} & {0.61} & {0.84} & {641.00} & {0.63} & {0.93} & {0.71} & {0.95} & {190.55} \\
    \hline
    \end{tabular}
    \caption{The results of adversarial attacks on the \textbf{BiLSTM} classifier in four misinformation detection tasks in untargeted and targeted scenario. Evaluation measures include BODEGA score (BODEGA), confusion score (C-s), BERT score (B-s), Levenshtein score (L-s) and number of queries to the attacked model (Q.). The best score in each task and scenario is in boldface. }
    \label{tab:BiLSTMresults}
\end{table*}


Table \ref{tab:BiLSTMresults} includes the results of the attack on the BiLSTM classifier. 

For hyperpartisan news detection, the pattern observed for BERT is largely repeated, with BERT-ATTACK offering the best overall performance, and DeepWordBug producing AEs of good quality, but with lower confusion scores. However, the recurrent network is clearly easier to attack than a pretrained model. While the character replacements of DeepWordBug are successful in flipping the decision of only 29\% of cases for BERT, that rate is up to 49\% for BiLSTM. 

In case of propaganda recognition, we see that other approaches succeed according to certain measures. For example, in untargeted scenario BAE delivers the best BERT score, while PWWS provides the best confusion score. What is more, we see a large difference of attack performance between the targeted and untargeted variant. These results have to be interpreted with caution, since BiLSTM provides relatively poor performance for this task, which reduces the number of instances used in the untargeted scenario.

In fact checking we again see that BiLSTM is much more easily attacked. What is especially worrying is the targeted scenario, which corresponds to inaccurate information being refuted by the evidence -- here the BODEGA score reaches 0.57 compared to 0.37 in BERT. The decision could be flipped for 85\% of the instances (62\% for BERT), indicating low robustness of this approach.

Rumour detection is also a task with low robustness of BiLSTM classifier. This is especially true in the targeted scenario: the character changes applied by the DeepWordBug are successful in 87\% of cases, preserving the BERT similarity of 0.77 and leaving 99\% of text unchanged. The brittleness of the classifiers output is also reflected in the number of queries made by the AE solutions to the model -- while the Genetic method on average needed to query BERT 2339 times, for BiLSTM 1552 is enough.

\subsection{Examples}
In order to demonstrate how a successful attack might look like, we revisit interactions between a relatively strong attacker (BERT-ATTACK) and a relatively weak victim (BiLSTM), randomly selecting 3 examples for each task from those with the highest BODEGA score.

The results are shown in tables \ref{tab:HNexamples} (style-based news bias assessment), \ref{tab:PRexamples} (propaganda recognition), \ref{tab:FCexamples} (fact checking) and \ref{tab:RDexamples} (rumour detection). The modified words are highlighted in bold. We can clearly see that the changes in these successful examples are minuscule. All of them involve word replacements preserving the meaning (\textit{level of success} to \textit{degree of success}) or visual similarity (\textit{Work When} to \textit{workwhen}). Some are very unlikely to be noticed in practice (\textit{.} to \textit{'}), while other can be more problematic (merging URLs with succeeding words in rumour threads). We need to emphasise that these are successful examples, with other cases including more significant changes or failing to change the classifier's decision. Nevertheless, they should be considered a warning sign when deploying such solutions in adversarial scenarios.

\begin{table*}%[t]
\small
    \centering
    \begin{tabular}{|p{0.43\linewidth}|p{0.43\linewidth}|l|}
    \textbf{Original example} & \textbf{Adversarial example} & \textbf{Label}\\
    \hline
        \textbf{Rising} Mathematician: Planned Economies Don’t \textbf{Work} \newline \textbf{When} you say “government” you often conjure an image of a magical unicorn that impartially redistributes wealth. Sadly, it’s often a corrupt bureaucracy. \newline What is so wrong with a planned economy? Many students of economics are quick to argue that in a free market, market failure is the rule rather than the exception. Some common examples of market failure include asymmetric information, monopolies, negative externalities such as pollution, and inequality. However, one particularly relevant argument concerns property rights. \newline [310 words more] & \textbf{-} mathematician: planned economies don’t \textbf{workwhen} you say “government” you often conjure an image of a magical unicorn that impartially redistributes wealth. sadly, it’s often a corrupt bureaucracy. \newline  what is so wrong with a planned economy? many students of economics are quick to argue that in a free market, market failure is the rule rather than the exception. some common examples of market failure include asymmetric information, monopolies, negative externalities such as pollution, and inequality. however, one particularly relevant argument concerns property rights. \newline [310 words more] & 1  $\Rightarrow$ 0\\
        \hline
        Diets Don’t Work. Here’s Why. \newline Posted on Jan 7, \textbf{2017} by Jon Hotchkiss in Blog | 0 comments \newline There are more than 800 different diets out there. Seriously. If one of them worked, people would be thinner. \newline Yes. Any diet you stick to for a few weeks will result in you losing weight. But, you can’t live like that. Plus: More than 95\% of people who lose weight put it back on plus MORE? within 3 years. \newline For just 75 years or so, food has been pretty abundant in most of the world. \newline [144 words more]& diets don’t work. here’s why.\newline posted on jan 7, \textbf{2016} by jon hotchkiss in blog | 0 comments  \newline there are more than 800 different diets out there. seriously. if one of them worked, people would be thinner. \newline yes. any diet you stick to for a few weeks will result in you losing weight. but, you can’t live like that. plus: more than 95\% of people who lose weight put it back on plus more? within 3 years.\newline for just 75 years or so, food has been pretty abundant in most of the world. \newline [144 words more] & 0  $\Rightarrow$ 1\\
        \hline
        Black vows reforms, more ‘transparency’ \newline Targets lobbying and campaign finance J. Andrew Curliss and Dan Kane, \textbf{Staff Writers} State House Speaker Jim Black late Monday said he will push for significant reforms in lobbying, ethics and campaign finance laws, saying changes are necessary in state government to "ensure greater access and transparency." \newline Black, a Mecklenburg County Democrat and four-term speaker, has been in the midst of controversies for the past year in his oversight of state money, creation of state jobs, the start-up of the state lottery, handling of his campaign finances and more. \newline [96 words more]& black vows reforms, more ‘transparency’ \newline targets lobbying and campaign finance j. andrew curliss and dan kane, \textbf{fellow contributors} state house speaker jim black late monday said he will push for significant reforms in lobbying, ethics and campaign finance laws, saying changes are necessary in state government to "ensure greater access and transparency." \newline black, a mecklenburg county democrat and four-term speaker, has been in the midst of controversies for the past year in his oversight of state money, creation of state jobs, the start-up of the state lottery, handling of his campaign finances and more. \newline [96 words more] & 0  $\Rightarrow$ 1\\
    \hline
    \end{tabular}
    \caption{\textbf{Style-based news bias assessment}: the successful modifications made by the BERT-ATTACK model applied to the BiLSTM classifier. The modified words are highlighted in bold. Some text (with no modifications) has been omitted.}
    \label{tab:HNexamples}
\end{table*}

\begin{table*}%[t]
\small
    \centering
    \begin{tabular}{|p{0.43\linewidth}|p{0.43\linewidth}|l|}
    \textbf{Original example} & \textbf{Adversarial example} & \textbf{Label}\\
    \hline
        But more significantly, it bolsters the otherwise risible claim that Assange is not a publisher – and thereby entitled to the protections of a free press, as enjoyed by the Guardian or the New York Times – but the \textbf{head} of an organisation engaged in espionage for a foreign power. & but more significantly, it bolsters the otherwise risible claim that assange is not a publisher – and thereby entitled to the protections of a free press, as enjoyed by the guardian or the new york times – but the \textbf{leader} of an organisation engaged in espionage for a foreign power. & 1  $\Rightarrow$ 0\\
        \hline
        [Investigators] also \textbf{discovered} that the server in question [the replacement server] was still operating under the employee’s control, contrary to the explicit instructions of the former chairman to turn over all equipment and fully cooperate with the inquiry and investigation. & [investigators] also \textbf{found} that the server in question [the replacement server] was still operating under the employee’s control, contrary to the explicit instructions of the former chairman to turn over all equipment and fully cooperate with the inquiry and investigation. & 1  $\Rightarrow$ 0\\
        \hline
        Its intention is mopping up; either to \textbf{herd} the remaining recalcitrants – those who have resisted even to the softened and conciliating degrees typical of “conservatives” – into the full post-conciliar “reform” instituted by the revolutionaries in the 1960s – or to close them. & its intention is mopping up; either to \textbf{rally} the remaining recalcitrants – those who have resisted even to the softened and conciliating degrees typical of “conservatives” – into the full post-conciliar “reform” instituted by the revolutionaries in the 1960s – or to close them. & 0  $\Rightarrow$ 1\\
    \hline
    \end{tabular}
    \caption{\textbf{Propaganda recognition}: the successfull modifications made by the BERT-ATTACK model applied to the BiLSTM classifier. The modified words are highlighted in bold.}
    \label{tab:PRexamples}
\end{table*}

\begin{table*}%[t]
\small
    \centering
    \begin{tabular}{|p{0.43\linewidth}|p{0.43\linewidth}|l|}
    \textbf{Original example} & \textbf{Adversarial example} & \textbf{Label}\\
    \hline
        Scream (franchise). Scream was one of the highest-grossing films of 1996 and became , and remains , the highest grossing slasher film in the world . $\rightarrow$ Scream has some \textbf{level} of success. & scream (franchise). scream was one of the highest-grossing films of 1996 and became, and remains, the highest grossing slasher film in the world. $\rightarrow$ scream has some \textbf{degree} of success. & 0  $\Rightarrow$ 1\\
        \hline
        Dissociative identity disorder. Dissociative disorders , including DID , have been attributed to disruptions in memory caused by trauma or other forms of stress . Psychological trauma. Trauma is often the result of an overwhelming amount of stress that exceeds one 's ability to cope , or integrate the emotions involved with that \textbf{experience} . $\rightarrow$ Dissociative identity disorder may be caused by severe stress. & dissociative identity disorder. dissociative disorders, including did, have been attributed to disruptions in memory caused by trauma or other forms of stress. psychological trauma. trauma is often the result of an overwhelming amount of stress that exceeds one's ability to cope, or integrate the emotions involved with that \textbf{experiences}. $\rightarrow$ dissociative identity disorder may be caused by severe stress. & 1  $\Rightarrow$ 0\\
        \hline
        Invasion literature. Invasion literature ( or the invasion novel ) is a literary genre most notable between 1871 and the First World War ( 1914 ) but still practised to this day \textbf{.} $\rightarrow$ Invasion literature has yet to become a genre of fiction. & invasion literature. invasion literature ( or the invasion novel ) is a literary genre most notable between 1871 and the first world war ( 1914 ) but still practised to this day \textbf{'} $\rightarrow$ invasion literature has yet to become a genre of fiction. & 1   $\Rightarrow$ 0\\
    \hline
    \end{tabular}
    \caption{\textbf{Fact checking}: the successful modifications made by the BERT-ATTACK model applied to the BiLSTM classifier. The modified words are highlighted in bold.}
    \label{tab:FCexamples}
\end{table*}

\begin{table*}%[t]
\small
    \centering
    \begin{tabular}{|p{0.43\linewidth}|p{0.43\linewidth}|l|}
    \textbf{Original example} & \textbf{Adversarial example} & \textbf{Label}\\
    \hline
        Reports: \#CharlieHebdo \textbf{suspects} killed \textbf{http://t.co/rsl4203bcQ} \newline \textbf{Damn}, this is like a movie RT @HuffingtonPost Reports: \#CharlieHebdo suspects killed http://t.co/zCuZD1cure \newline ?@HuffingtonPost: Reports: \#CharlieHebdo suspects killed http://t.co/mWCSjh3CkH? superb simultaneous response by the French tactics unit. \newline @HuffingtonPost great news! No trial, no taxpayer money spent to support them. \newline @HuffingtonPost Good news !!! Alah Akbar !!    \newline @HuffingtonPost damnit!!! That's what those fuckers wanted!! Now they will be hailed as martyrs\textbf{....} \newline \textbf{@HuffingtonPost} Can you confirm the reports that those suspects were killed by French police? \newline [71 words more] & reports: \#charliehebdo \textbf{suspected} killed \textbf{http://t.co/rsl4203bcqdamn}, this is like a movie rt @huffingtonpost reports: \#charliehebdo suspects killed http://t.co/zcuzd1cure \newline @huffingtonpost: reports: \#charliehebdo suspects killed http://t.co/mwcsjh3ckh? superb simultaneous response by the french tactics unit. \newline @huffingtonpost great news! no trial, no taxpayer money spent to support them. \newline @huffingtonpost good news !!! alah akbar !! \newline @huffingtonpost damnit!!! that's what those fuckers wanted!! now they will be hailed as martyrs\textbf{....@huffingtonpost} can you confirm the reports that those suspects were killed by french police? \newline [71 words more] & 1  $\Rightarrow$ 0\\
        \hline
        BREAKING: Three gunmen involved in \textbf{attack} on Charlie Hebdo magazine, French Interior Minister Bernard Cazeneuve says. http://t.co/ak9mTVfJdR \newline @cnni the Islamic leaders should do something about the image of islam by speaking out against the terrorists \newline @cnni expel muslims from european soil and destroy all the mosques. \newline @cnni it's not the religion.  But how the people  interpret the writings and that's what causes them to do bad things. \newline @cnni terrorism needs concerted efforts from every citizen to fight it,religion is going beyond boundaries if it can cause terror attacks & breaking: three gunmen involved in \textbf{attacks} on charlie hebdo magazine, french interior minister bernard cazeneuve says. http://t.co/ak9mtvfjdr \newline @cnni the islamic leaders should do something about the image of islam by speaking out against the terrorists \newline @cnni expel muslims from european soil and destroy all the mosques. \newline @cnni it's not the religion.  but how the people  interpret the writings and that's what causes them to do bad things. \newline @cnni terrorism needs concerted efforts from every citizen to fight it,religion is going beyond boundaries if it can cause terror attacks & 0  $\Rightarrow$ 1\\
        \hline
        \#CharlieHebdo update: \textbf{Suspects} in contact with police, say they want to die as martyrs http://t.co/7wSzrByj2n \newline @BostonGlobe as long as they do that alone, be my guests! \newline @BostonGlobe let's make their dreams come true & \#charliehebdo update: \textbf{people} in contact with police, say they want to die as martyrs http://t.co/7wszrbyj2n \newline @bostonglobe as long as they do that alone, be my guests! \newline @bostonglobe let's make their dreams come true &  1 $\Rightarrow$ 0\\
    \hline
    \end{tabular}
    \caption{\textbf{Rumour detection}: the successful modifications made by the BERT-ATTACK model applied to the BiLSTM classifier. The modified words are highlighted in bold. Some text (with no modifications) has been omitted.}
    \label{tab:RDexamples}
\end{table*}


\subsection{Summary}
We can summarise the experiments through the following general observations:
\begin{itemize}
    \item Fine-tuned BERT appears to be noticeably more robust than BiLSTM. This situation is observable across tasks and scenarios.
    \item The difficulty of attack is usually similar for untargeted and targeted scenarios. However, where the victim classifier performs poorly (BiLSTM detecting propaganda and rumours), the targeted attack becomes significantly easier.
    \item Both simple attacks based on character replacements (DeepWordBug) and complex ones utilising language models (BERT-ATTACK) can give good results in some situations.
    \item Some methods may require thousands of queries to the victim model to come up with an AE, especially for longer text. This may limit their practical applicability despite good performance.
    \item Style-based news bias assessment is clearly the most vulnerable to adversarial attacks, while fact checking is the least. This may be related to a content length: when the input is only a sentence or two, every word replacement affects the meaning, hindering the attack.
\end{itemize}


\section{Discussion}

%The work presented here has a dual purpose: providing a systematic view of vulnerability of credibility assessment solutions and laying foundation for further experiments. We shall now discuss the limitations and opportunities in these two areas.

\subsection{Reality check for credibility assessment}

While one of the principles guiding the design of BODEGA has been a realistic simulation of the misinformation detection scenarios, this is possible only to an extent. Among the obstacles are low transparency of content management platforms \citep{Gorwa2020} and the vigorous growth of the methods of attack and defence in the NLP field. 

Firstly, we have included only two victim models in our tests: BiLSTM and BERT, while in reality dozens of architectures for text classification are presented at every NLP conference, with a significant share specifically devoted to credibility assessment. However, the field has recently become surprisingly homogeneous, with the ambition to achieve the state of the art pushing researchers to reuse the common pretrained language models in virtually every application \citep{Church2022}. But these lookalike approaches share not only good performance, but also weaknesses. Thus we expect that, for example, the results of attacks on fine-tuned BERT will also apply to other solutions that use BERT as a representation layer.

Secondly, we have re-used the attacks implemented in OpenAttack to have a comprehensive view of performance of different approaches. However, the field of AEs for NLP is relatively new, with the majority of publications emerging in the recent years, which makes it very likely that subsequent solutions will provide superior performance. With the creation of BODEGA as a universal evaluation framework, such comparisons become possible.
%However, it was hard to make such comparisons with no commonly agreed evaluation framework. This is precisely the reason for the creation of BODEGA.

Thirdly, we need to consider the realism of evaluation measures. The AE evaluation framework assumes that if a modified text is very similar to the original, then the label (credible or not) still applies. Without this assumption, every evaluation would need to include manual re-annotation of the AEs. Fortunately, assessing semantic similarity  between two fragments of text is a necessary component of evaluation in many other NLP tasks, e.g. machine translation \citep{Lee2023}, and we can draw from that work. Apart from BERT score, we have experimented with SBERT cross-encoders \citep{thakur-etal-2021-augmented}, using both models tuned on semantic similarity and natural language inference and haven't found decisive evidence for the superiority of any approach. However, the problem remains open. The investigation on how subtle changes in text can invert its meaning and subvert credibility assessment is particularly vivid in the fact-checking field \citep{Jaime2022}, but is well explored for tasks involving multi-sentence inputs, e.g. news credibility.

Fourthly, we also assume that an attacker has a certain level of access to the victim classifier, being able to send unlimited queries and receive numerical scores reflecting its confidence, rather than a final decision. In practice, this is currently not the case, with platforms revealing almost nothing regarding their automatic content moderation processes. However, this may change in future due to regulatory pressure from the government organisations; cf., for example, the recently agreed EU \textit{Digital Services Act}\footnote{\url{https://ec.europa.eu/commission/presscorner/detail/en/qanda_20_2348}}.

Finally, we need to examine how realistic is that an attacker could freely modify any text included in our tasks. While this is trivial in the case of news credibility and propaganda detection, where the entire text comes from a malicious actor, the other tasks require closer consideration. In case of rumour detection, the text includes, apart from the initial information, replies from other social media users. These can indeed be manipulated by sending replies from anonymous accounts and this scenario has been already explored in the AE literature \citep{Le2020}. In the case of fact checking, the text includes, apart from the verified claim, also the relevant snippets from the knowledge base that seems non-accessible. However, it can be modified as well, when (as is usually the case) the knowledge is based on Wikipedia, which is often a subject of malicious alterations, from vandalism \citep{Kiesel_Potthast_Hagen_Stein_2017} to the generation of entire hoax articles \citep{Kumar2016}.

To sum up, we argue that despite certain assumptions, the setup of a BODEGA framework is close enough to real-life conditions to give insights about the robustness of popular classifiers in this scenario.

\subsection{Looking forward}
Beyond the exploration  of the current situation, we hope BODEGA will be useful for assessing the robustness of future classifiers and the effectiveness of new attacks. Towards this end, we make the software available openly\footnote{\url{https://github.com/piotrmp/BODEGA}}, allowing both the replication of our experiments and evaluating other solutions, both on the attack and the defence.

We also see this study as a step towards the directions recognised in the ML literature beyond NLP. For example, in security-oriented applications, there is the need to bring the evaluation of AEs closer to realistic conditions \citep{Chen2022}. Some limitations, esp. number of queries to the model, make attacks much harder. Even beyond the security field, assessing robustness is crucial for ML models that are distributed as massively-used products. This exposes them to unexpected examples, even if not generated with explicit adversarial motive. Individual spectacular failures are expected to be disproportionately influential on public opinion of technology, including AI \citep{Mannes2020}, emphasising the importance of research on AEs.
%Well-known cases include images of black people labelled as 'gorillas' by \textit{Google Photos}\footnote{\url{https://www.theverge.com/2018/1/12/16882408/google-racist-gorillas-photo-recognition-algorithm-ai}} or 'primates' by \textit{Facebook}\footnote{\url{https://www.nytimes.com/2021/09/03/technology/facebook-ai-race-primates.html}}. 

Finally, we need to acknowledge that the idea of using ML models for automatic moderation of user-generated content is not universally accepted, with some rejecting it as equivalent to censorship \citep{Llanso2020}, and calling for regulations in this area \citep{Meyer2019}.


\section{Conclusion}

Through this work, we have demonstrated that popular text classifiers, when applied for the purposes of misinformation detection, are vulnerable to manipulation through adversarial examples. We have discovered numerous cases where making a single barely perceptible change is enough to prevent a classifier from spotting non-credible information. Among the risk factors are large input lengths, poor accuracy of the victim classifier and the possibility of making numerous queries.

Nevertheless, the attack is never successful for every single instance and often entails changes that make text suspiciously malformed or ill-suited for the misinformation goal. This emphasises the need for thorough testing of the robustness of text classifiers at various stages of their development: from the initial design and experiments to the preparation for deployment, taking into account likely attack scenarios. We hope the BODEGA benchmark we contribute here, providing an environment for comprehensive and systematic tests, will be a useful tool in performing such analyses.

\section*{Acknowledgements}
This work is part of the ERINIA project, which has received funding from the European Union’s Horizon Europe research and innovation programme under grant agreement No 101060930. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union. Neither the European Union nor the granting authority can be held responsible for them.

\bibliography{BODEGA}
\bibliographystyle{nejlt_bib}

\end{document}
