@article{Llanso2020,
abstract = {Contemporary policy debates about managing the enormous volume of online content have taken a renewed focus on upload filtering, automated detection of potentially illegal content, and other “proactive measures”. Often, policymakers and tech industry players invoke artificial intelligence as the solution to complex challenges around online content, promising that AI is a scant few years away from resolving everything from hate speech to harassment to the spread of terrorist propaganda. Missing from these promises, however, is an acknowledgement that proactive identification and automated removal of user-generated content raises problems beyond issues of “accuracy” and overbreadth--problems that will not be solved with more sophisticated AI. In this commentary, I discuss how the technical realities of content filtering stack up against the protections for freedom of expression in international human rights law. As policymakers and companies around the world turn to AI for communications governance, it is crucial that we recall why legal protections for speech have included presumptions against prior censorship, and consider carefully how proactive content moderation will fundamentally re-shape the relationship between rules, people, and their speech.},
author = {Llans{\'{o}}, Emma J},
doi = {10.1177/2053951720920686},
issn = {2053-9517},
journal = {Big Data and Society},
keywords = {artificial intelligence,filtering,free expression,human rights,machine learning,natural language processing},
number = {1},
publisher = {SAGE Publications Ltd},
title = {{No amount of “AI” in content moderation will solve filtering's prior-restraint problem}},
url = {http://journals.sagepub.com/doi/10.1177/2053951720920686},
volume = {7},
year = {2020}
}
@article{Lewandowsky2017,
abstract = {The terms “post-truth” and “fake news” have become increasingly prevalent in public discourse over the last year. This article explores the growing abundance of misinformation, how it influences people, and how to counter it. We examine the ways in which misinformation can have an adverse impact on society. We summarize how people respond to corrections of misinformation, and what kinds of corrections are most effective. We argue that to be effective, scientific research into misinformation must be considered within a larger political, technological, and societal context. The post-truth world emerged as a result of societal mega-trends such as a decline in social capital, growing economic inequality, increased polarization, declining trust in science, and an increasingly fractionated media landscape. We suggest that responses to this malaise must involve technological solutions incorporating psychological principles, an interdisciplinary approach that we describe as “technocognition.” We outline a number of recommendations to counter misinformation in a post-truth world.},
author = {Lewandowsky, Stephan and Ecker, Ullrich K.H. and Cook, John},
doi = {10.1016/J.JARMAC.2017.07.008},
issn = {2211-3681},
journal = {Journal of Applied Research in Memory and Cognition},
month = {dec},
number = {4},
pages = {353--369},
publisher = {Elsevier},
title = {{Beyond Misinformation: Understanding and Coping with the “Post-Truth” Era}},
url = {https://www.sciencedirect.com/science/article/pii/S2211368117300700},
volume = {6},
year = {2017}
}
@techreport{Akers2018,
abstract = {Technology is increasingly used -- unintentionally (misinformation) or intentionally (disinformation) -- to spread false information at scale, with potentially broad-reaching societal effects. For example, technology enables increasingly realistic false images and videos, and hyper-personal targeting means different people may see different versions of reality. This report is the culmination of a PhD-level special topics course (https://courses.cs.washington.edu/courses/cse599b/18au/) in Computer Science {\&} Engineering at the University of Washington's Paul G. Allen School in the fall of 2018. The goals of this course were to study (1) how technologies and today's technical platforms enable and support the creation and spread of such mis- and disinformation, as well as (2) how technical approaches could be used to mitigate these issues. In this report, we summarize the space of technology-enabled mis- and disinformation based on our investigations, and then surface our lessons and recommendations for technologists, researchers, platform designers, policymakers, and users.},
archivePrefix = {arXiv},
arxivId = {1812.09383},
author = {Akers, John and Bansal, Gagan and Cadamuro, Gabriel and Chen, Christine and Chen, Quanze and Lin, Lucy and Mulcaire, Phoebe and Nandakumar, Rajalakshmi and Rockett, Matthew and Simko, Lucy and Toman, John and Wu, Tongshuang and Zeng, Eric and Zorn, Bill and Roesner, Franziska},
eprint = {1812.09383},
institution = {University of Washington},
title = {{Technology-Enabled Disinformation: Summary, Lessons, and Recommendations}},
url = {http://arxiv.org/abs/1812.09383},
year = {2018}
}
@inproceedings{DBLP:conf/iclr/ZhangKWWA20,
address = {Addis Ababa, Ethiopia},
author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
booktitle = {8th International Conference on Learning Representations, ICLR 2020},
title = {{BERTScore: Evaluating Text Generation with BERT}},
url = {https://openreview.net/forum?id=SkeHuCVFDr},
year = {2020}
}
@inproceedings{Kiesel_Potthast_Hagen_Stein_2017,
author = {Kiesel, Johannes and Potthast, Martin and Hagen, Matthias and Stein, Benno},
booktitle = {Proceedings of the International AAAI Conference on Web and Social Media},
doi = {10.1609/icwsm.v11i1.14900},
pages = {122--131},
title = {{Spatio-Temporal Analysis of Reverted Wikipedia Edits}},
url = {https://ojs.aaai.org/index.php/ICWSM/article/view/14900},
volume = {11},
year = {2017}
}
@article{Lee2023,
abstract = {The success of Transformer architecture has seen increased interest in machine translation (MT). The translation quality of neural network-based MT transcends that of translations derived using statistical methods. This growth in MT research has entailed the development of accurate automatic evaluation metrics that allow us to track the performance of MT. However, automatically evaluating and comparing MT systems is a challenging task. Several studies have shown that traditional metrics (e.g., BLEU, TER) show poor performance in capturing semantic similarity between MT outputs and human reference translations. To date, to improve performance, various evaluation metrics have been proposed using the Transformer architecture. However, a systematic and comprehensive literature review on these metrics is still missing. Therefore, it is necessary to survey the existing automatic evaluation metrics of MT to enable both established and new researchers to quickly understand the trend of MT evaluation over the past few years. In this survey, we present the trend of automatic evaluation metrics. To better understand the developments in the field, we provide the taxonomy of the automatic evaluation metrics. Then, we explain the key contributions and shortcomings of the metrics. In addition, we select the representative metrics from the taxonomy, and conduct experiments to analyze related problems. Finally, we discuss the limitation of the current automatic metric studies through the experimentation and our suggestions for further research to improve the automatic evaluation metrics.},
author = {Lee, Seungjun and Lee, Jungseob and Moon, Hyeonseok and Park, Chanjun and Seo, Jaehyung and Eo, Sugyeong and Koo, Seonmin and Lim, Heuiseok},
doi = {10.3390/MATH11041006},
issn = {2227-7390},
journal = {Mathematics},
keywords = {Transformer,automatic evaluation metric,deep learning,machine translation},
number = {4},
pages = {1006},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{A Survey on Evaluation Metrics for Machine Translation}},
url = {https://www.mdpi.com/2227-7390/11/4/1006/htm https://www.mdpi.com/2227-7390/11/4/1006},
volume = {11},
year = {2023}
}
@inproceedings{Loshchilov,
abstract = {L 2 regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is not the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L 2 regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by decoupling the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
address = {New Orleans, LA, USA},
archivePrefix = {arXiv},
arxivId = {1711.05101v3},
author = {Loshchilov, Ilya and Hutter, Frank},
booktitle = {7th International Conference on Learning Representations, ICLR 2019},
eprint = {1711.05101v3},
title = {{Decoupled Weight Decay Regularization}},
url = {https://openreview.net/forum?id=Bkg6RiCqY7},
year = {2019}
}
@inproceedings{DBLP:conf/iclr/HeLGC21,
author = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
booktitle = {9th International Conference on Learning Representations, {\{}ICLR{\}} 2021, Virtual Event, Austria, May 3-7, 2021},
publisher = {OpenReview.net},
title = {{Deberta: decoding-Enhanced Bert with Disentangled Attention}},
url = {https://openreview.net/forum?id=XPZIaotutsD},
year = {2021}
}
@inproceedings{Brown2020b,
abstract = {In this paper, we propose two new attacks: the Adversarial Universal False Positive (UFP) Attack and the Adversarial Universal False Negative (UFN) Attack. The objective of this research is to introduce a new class of attack using only feature vector information. The results show the potential weaknesses of five machine learning (ML) classifiers. These classifiers include k-Nearest Neighbor (KNN), Naive Bayes (NB), Random Forrest (RF), a Support Vector Machine (SVM) with a Radial Basis Function (RBF) Kernel, and XGBoost (XGB).},
author = {Brown, Brandon and Richardson, Alexicia and Smith, Marcellus and Dozier, Gerry and King, Michael C.},
booktitle = {2020 IEEE Symposium Series on Computational Intelligence, SSCI 2020},
doi = {10.1109/SSCI47803.2020.9308298},
isbn = {9781728125473},
keywords = {Fake News Detection Systems,Universal False Negative,Universal False Positive},
pages = {1523--1527},
publisher = {IEEE},
title = {{The Adversarial UFP/UFN Attack: A New Threat to ML-based Fake News Detection Systems?}},
year = {2020}
}
@inproceedings{Guo2021,
abstract = {We propose the first general-purpose gradient-based adversarial attack against transformer models. Instead of searching for a single adversarial example, we search for a distribution of adversarial examples parameterized by a continuous-valued matrix, hence enabling gradient-based optimization. We empirically demonstrate that our white-box attack attains state-of-the-art attack performance on a variety of natural language tasks, outperforming prior work in terms of adversarial success rate with matching imperceptibility as per automated and human evaluation. Furthermore, we show that a powerful black-box transfer attack, enabled by sampling from the adversarial distribution, matches or exceeds existing methods, while only requiring hard-label outputs.},
archivePrefix = {arXiv},
arxivId = {2104.13733},
author = {Guo, Chuan and Sablayrolles, Alexandre and J{\'{e}}gou, Herv{\'{e}} and Kiela, Douwe},
booktitle = {EMNLP 2021 - 2021 Conference on Empirical Methods in Natural Language Processing, Proceedings},
doi = {10.18653/V1/2021.EMNLP-MAIN.464},
eprint = {2104.13733},
isbn = {9781955917094},
pages = {5747--5757},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Gradient-based Adversarial Attacks against Text Transformers}},
url = {https://aclanthology.org/2021.emnlp-main.464},
year = {2021}
}
@article{Ali2021a,
abstract = {With the hyperconnectivity and ubiquity of the Internet, the fake news problem now presents a greater threat than ever before. One promising solution for countering this threat is to leverage deep learning (DL)-based text classification methods for fake-news detection. However, since such methods have been shown to be vulnerable to adversarial attacks, the integrity and security of DL-based fake news classifiers are under question. Although many works study text classification under the adversarial threat, to the best of our knowledge, we do not find any work in literature that specifically analyzes the performance of DL-based fake-news detectors under adversarial settings. We bridge this gap by evaluating the performance of fake-news detectors under various configurations under black-box settings. In particular, we investigate the robustness of four different DL architectural choices{\&}{\#}x2014;multilayer perceptron (MLP), convolutional neural network (CNN), recurrent neural network (RNN) and a recently proposed Hybrid CNN-RNN trained on three different state-of-the-art datasets{\&}{\#}x2014;under different adversarial attacks (Text Bugger, Text Fooler, PWWS, and Deep Word Bug) implemented using the state-of-the-art NLP attack library, Text-Attack. Additionally, we explore how changing the detector complexity, the input sequence length, and the training loss affect the robustness of the learned model. Our experiments suggest that RNNs are robust as compared to other architectures. Further, we show that increasing the input sequence length generally increases the detector{\&}{\#}x2019;s robustness. Our evaluations provide key insights to robustify fake-news detectors against adversarial attacks.},
author = {Ali, Hassan and Khan, Muhammad Suleman and AlGhadhban, Amer and Alazmi, Meshari and Alzamil, Ahmad and Al-utaibi, Khaled and Qadir, Junaid},
doi = {10.1109/ACCESS.2021.3085875},
issn = {21693536},
journal = {IEEE Access},
keywords = {Analytical models,Computer architecture,Detectors,Electronic mail,Natural language processing,Robustness,Training,adversarial attacks,adversarial robustness,deep neural networks,fake news detection},
pages = {81678--81692},
publisher = {IEEE},
title = {{All Your Fake Detector are Belong to Us: Evaluating Adversarial Robustness of Fake-News Detectors Under Black-Box Settings}},
volume = {9},
year = {2021}
}
@techreport{Meyer2019,
author = {Meyer, T and Marsden, C},
institution = {European Parliament},
title = {{Regulating disinformation with artificial intelligence: Effects of disinformation initiatives on freedom of expression and media pluralism}},
url = {https://data.europa.eu/doi/10.2861/003689},
year = {2019}
}
@techreport{Graves2018,
abstract = {The last year has seen growing attention among journalists, policymakers, and technology companies to the problem of finding effective, large-scale responses to online misinformation. The furore over so-called ‘fake news' has exacerbated long-standing concerns about political lying and online rumours in a fragmented media environment, sharpening calls for technological solutions to what is oſten seen as a technological problem. This factsheet gives an overview of efforts to automatically police false political claims and misleading content online, highlighting central research challenges in this area as well as current initiatives involving professional fact-checkers, platform companies, and artificial intelligence researchers.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Graves, Lucas},
eprint = {arXiv:1011.1669v3},
institution = {Reuters Institute, University of Oxford},
isbn = {9780874216561},
issn = {0717-6163},
pmid = {15003161},
title = {{Understanding the Promise and Limits of Automated Fact-Checking}},
url = {https://reutersinstitute.politics.ox.ac.uk/sites/default/files/2018-02/graves{\_}factsheet{\_}180226 FINAL.pdf},
year = {2018}
}
@techreport{GemmaTeam2024,
author = {{Gemma Team} and {Google DeepMind}},
institution = {Google DeepMind},
title = {{Gemma: Open Models Based on Gemini Research and Technology}},
url = {https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf},
year = {2024}
}
@inproceedings{Srivastava2023,
abstract = {AI services are known to have unstable behavior when subjected to changes in data, models or users. Such behaviors, whether triggered by omission or commission, lead to trust issues when AI works with humans. The current approach of assessing AI services in a black box setting, where the consumer does not have access to the AI's source code or training data, is limited. The consumer has to rely on the AI developer's documentation and trust that the system has been built as stated. Further, if the AI consumer reuses the service to build other services which they sell to their customers, the consumer is at the risk of the service providers (both data and model providers). Our approach, in this context, is inspired by the success of nutritional labeling in food industry to promote health and seeks to assess and rate AI services for trust from the perspective of an independent stakeholder. The ratings become a means to communicate the behavior of AI systems so that the consumer is informed about the risks and can make an informed decision. In this paper, we will first describe recent progress in developing rating methods for text-based machine translator AI services that have been found promising with user studies. Then, we will outline challenges and vision for a principled, multi-modal, causality-based rating methodologies and its implication for decision-support in real-world scenarios like health and food recommendation.},
archivePrefix = {arXiv},
arxivId = {2302.09079},
author = {Srivastava, Biplav and Lakkaraju, Kausik and Bernagozzi, Mariana and Valtorta, Marco},
booktitle = {Spring Symposium on AI Trustworthiness Assessment},
doi = {10.48550/arxiv.2302.09079},
eprint = {2302.09079},
title = {{Advances in Automatically Rating the Trustworthiness of Text Processing Services}},
url = {https://arxiv.org/abs/2302.09079v1},
year = {2023}
}
@article{Carter2021,
abstract = {In the last few years, we have witnessed an explosive growth of fake content on the Internet which has significantly affected the veracity of information on many social platforms. Much of this disruption has been caused by the proliferation of advanced machine and deep learning methods. In turn, social platforms have been using the same technological methods in order to detect fake content. However, there is understanding of the strengths and weaknesses of these detection methods. In this article, we describe examples of machine and deep learning approaches that can be used to detect different types of fake content. We also discuss the characteristics and the potential for adversarial attacks on these methods that could reduce the accuracy of fake content detection. Finally, we identify and discuss some future research challenges in this area.},
author = {Carter, Matthew and Tsikerdekis, Michail and Zeadally, Sherali},
doi = {10.1109/MIC.2020.3032323},
issn = {19410131},
journal = {IEEE Internet Computing},
keywords = {adversarial,attacks,content,detection,fake},
number = {2},
pages = {73--83},
publisher = {IEEE},
title = {{Approaches for Fake Content Detection: Strengths and Weaknesses to Adversarial Attacks}},
volume = {25},
year = {2021}
}
@inproceedings{Neekhara2019,
abstract = {In this work, we develop methods to repurpose text classification neural networks for alternate tasks without modifying the network architecture or parameters. We propose a context based vocabulary remapping method that performs a computationally inexpensive input transformation to reprogram a victim classification model for a new set of sequences. We propose algorithms for training such an input transformation in both white box and black box settings where the adversary may or may not have access to the victim model's architecture and parameters. We demonstrate the application of our model and the vulnerability of neural networks by adversarially repurposing various text-classification models including LSTM, bi-directional LSTM and CNN for alternate classification tasks.},
archivePrefix = {arXiv},
arxivId = {1809.01829},
author = {Neekhara, Paarth and Hussain, Shehzeen and Dubnov, Shlomo and Koushanfar, Farinaz},
booktitle = {EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference},
doi = {10.18653/V1/D19-1525},
eprint = {1809.01829},
isbn = {9781950737901},
pages = {5216--5225},
publisher = {Association for Computational Linguistics},
title = {{Adversarial Reprogramming of Text Classification Neural Networks}},
url = {https://aclanthology.org/D19-1525},
year = {2019}
}
@inproceedings{garg-ramakrishnan-2020-bae,
abstract = {Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model. Recent works in NLP use rule-based synonym replacement strategies to generate adversarial examples. These strategies can lead to out-of-context and unnaturally complex token replacements, which are easily identifiable by humans. We present BAE, a black box attack for generating adversarial examples using contextual perturbations from a BERT masked language model. BAE replaces and inserts tokens in the original text by masking a portion of the text and leveraging the BERT-MLM to generate alternatives for the masked tokens. Through automatic and human evaluations, we show that BAE performs a stronger attack, in addition to generating adversarial examples with improved grammaticality and semantic coherence as compared to prior work.},
address = {Online},
author = {Garg, Siddhant and Ramakrishnan, Goutham},
booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
doi = {10.18653/v1/2020.emnlp-main.498},
pages = {6174--6181},
publisher = {Association for Computational Linguistics},
title = {{BAE: BERT-based Adversarial Examples for Text Classification}},
url = {https://aclanthology.org/2020.emnlp-main.498},
year = {2020}
}
@inproceedings{wolf-etal-2020-transformers,
address = {Online},
author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'{e}}mi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M},
booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
pages = {38--45},
publisher = {Association for Computational Linguistics},
title = {{Transformers: State-of-the-Art Natural Language Processing}},
url = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
year = {2020}
}
@inproceedings{morris-etal-2020-textattack,
abstract = {While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for adversarial attacks, data augmentation, and adversarial training in NLP. TextAttack builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. TextAttack{\{}'{\}}s modular design enables researchers to easily construct attacks from combinations of novel and existing components. TextAttack provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including BERT and other transformers, and all GLUE tasks. TextAttack also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness.TextAttack is democratizing NLP: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at https://github.com/QData/TextAttack.},
address = {Online},
author = {Morris, John and Lifland, Eli and Yoo, Jin Yong and Grigsby, Jake and Jin, Di and Qi, Yanjun},
booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
doi = {10.18653/v1/2020.emnlp-demos.16},
pages = {119--126},
publisher = {Association for Computational Linguistics},
title = {{TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP}},
url = {https://aclanthology.org/2020.emnlp-demos.16},
year = {2020}
}
@inproceedings{Hidey2020,
abstract = {The increased focus on misinformation has spurred development of data and systems for detecting the veracity of a claim as well as retrieving authoritative evidence. The Fact Extraction and VERification (FEVER) dataset provides such a resource for evaluating end-to-end fact-checking, requiring retrieval of evidence from Wikipedia to validate a veracity prediction. We show that current systems for FEVER are vulnerable to three categories of realistic challenges for fact-checking -- multiple propositions, temporal reasoning, and ambiguity and lexical variation -- and introduce a resource with these types of claims. Then we present a system designed to be resilient to these "attacks" using multiple pointer networks for document selection and jointly modeling a sequence of evidence sentences and veracity relation predictions. We find that in handling these attacks we obtain state-of-the-art results on FEVER, largely due to improved evidence retrieval.},
archivePrefix = {arXiv},
arxivId = {2004.12864},
author = {Hidey, Christopher and Chakrabarty, Tuhin and Alhindi, Tariq and Varia, Siddharth and Krstovski, Kriste and Diab, Mona and Muresan, Smaranda},
booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
doi = {10.18653/V1/2020.ACL-MAIN.761},
eprint = {2004.12864},
month = {jul},
pages = {8593--8606},
publisher = {Association for Computational Linguistics (ACL)},
title = {{DeSePtion: Dual Sequence Prediction and Adversarial Examples for Improved Fact-Checking}},
url = {https://aclanthology.org/2020.acl-main.761},
year = {2020}
}
@techreport{GPT2,
author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
institution = {OpenAI},
keywords = {learning multitask},
title = {{Language Models are Unsupervised Multitask Learners}},
url = {https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf},
year = {2018}
}
@inproceedings{thakur-etal-2021-augmented,
abstract = {There are two approaches for pairwise sentence scoring: Cross-encoders, which perform full-attention over the input pair, and Bi-encoders, which map each input independently to a dense vector space. While cross-encoders often achieve higher performance, they are too slow for many practical use cases. Bi-encoders, on the other hand, require substantial training data and fine-tuning over the target task to achieve competitive performance. We present a simple yet efficient data augmentation strategy called Augmented SBERT, where we use the cross-encoder to label a larger set of input pairs to augment the training data for the bi-encoder. We show that, in this process, selecting the sentence pairs is non-trivial and crucial for the success of the method. We evaluate our approach on multiple tasks (in-domain) as well as on a domain adaptation task. Augmented SBERT achieves an improvement of up to 6 points for in-domain and of up to 37 points for domain adaptation tasks compared to the original bi-encoder performance.},
address = {Online},
author = {Thakur, Nandan and Reimers, Nils and Daxenberger, Johannes and Gurevych, Iryna},
booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
doi = {10.18653/v1/2021.naacl-main.28},
pages = {296--310},
publisher = {Association for Computational Linguistics},
title = {{Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks}},
url = {https://aclanthology.org/2021.naacl-main.28},
year = {2021}
}
@inproceedings{Thorne2019,
abstract = {Automated fact verification has been progressing owing to advancements in modeling and availability of large datasets. Due to the nature of the task, it is critical to understand the vulnerabilities of these systems against adversarial instances designed to make them predict incorrectly. We introduce two novel scoring metrics, attack potency and system resilience which take into account the correctness of the adversarial instances, an aspect often ignored in adversarial evaluations. We consider six fact verification systems from the recent Fact Extraction and VERification (FEVER) challenge: the four best-scoring ones and two baselines. We evaluate adversarial instances generated by a recently proposed state-of-the-art method, a paraphrasing method, and rule-based attacks devised for fact verification. We find that our rule-based attacks have higher potency, and that while the rankings among the top systems changed, they exhibited higher resilience than the baselines.},
author = {Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Mittal, Arpit},
booktitle = {EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference},
doi = {10.18653/V1/D19-1292},
isbn = {9781950737901},
pages = {2944--2953},
publisher = {Association for Computational Linguistics},
title = {{Evaluating adversarial attacks against multiple fact verification systems}},
url = {https://aclanthology.org/D19-1292},
year = {2019}
}
@inproceedings{potthast-etal-2018-stylometric,
abstract = {We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97{\%} of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.},
author = {Potthast, Martin and Kiesel, Johannes and Reinartz, Kevin and Bevendorff, Janek and Stein, Benno},
booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
pages = {231--240},
publisher = {Association for Computational Linguistics},
title = {{A Stylometric Inquiry into Hyperpartisan and Fake News}},
url = {https://www.aclweb.org/anthology/P18-1022},
year = {2018}
}
@inproceedings{9891941,
author = {Przyby{\l}a, Piotr and Borkowski, Piotr and Kaczy{\'{n}}ski, Konrad},
booktitle = {Proceedings of the 2022 International Joint Conference on Neural Networks (IJCNN)},
doi = {10.1109/IJCNN55064.2022.9891941},
publisher = {IEEE},
title = {{Countering Disinformation by Finding Reliable Sources: a Citation-Based Approach}},
year = {2022}
}
@article{hu2021lora,
archivePrefix = {arXiv},
arxivId = {cs.CL/2106.09685},
author = {Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
eprint = {2106.09685},
journal = {arXiv preprint arXiv:2106.09685},
primaryClass = {cs.CL},
title = {{LoRA: Low-Rank Adaptation of Large Language Models}},
year = {2021}
}
@inproceedings{NEURIPS2023_1feb8787,
author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Oh, A and Neumann, T and Globerson, A and Saenko, K and Hardt, M and Levine, S},
pages = {10088--10115},
publisher = {Curran Associates, Inc.},
title = {{QLoRA: Efficient Finetuning of Quantized LLMs}},
url = {https://proceedings.neurips.cc/paper{\_}files/paper/2023/file/1feb87871436031bdc0f2beaa62a049b-Paper-Conference.pdf},
volume = {36},
year = {2023}
}
@inproceedings{Thorne2018,
abstract = {We present the results of the first Fact Extraction and VERification (FEVER) Shared Task. The task challenged participants to classify whether human-written factoid claims could be SUPPORTED or REFUTED using evidence retrieved from Wikipedia. We received entries from 23 competing teams, 19 of which scored higher than the previously published base-line. The best performing system achieved a FEVER score of 64.21{\%}. In this paper, we present the results of the shared task and a summary of the systems, highlighting com-monalities and innovations among participating systems.},
archivePrefix = {arXiv},
arxivId = {1811.10971v1},
author = {Thorne, James and Vlachos, Andreas and Cocarascu, Oana and Christodoulopoulos, Christos and Mittal, Arpit},
booktitle = {Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)},
eprint = {1811.10971v1},
title = {{The Fact Extraction and VERification (FEVER) Shared Task}},
year = {2018}
}
@inproceedings{Bagdasaryan2022,
abstract = {We investigate a new threat to neural sequence-to-sequence (seq2seq) models: training-time attacks that cause models to 'spin' their outputs so as to support an adversary-chosen sentiment or point of view - but only when the input contains adversary-chosen trigger words. For example, a spinned 1 summarization model outputs positive summaries of any text that mentions the name of some individual or organization.Model spinning introduces a 'meta-backdoor' into a model. Whereas conventional backdoors cause models to produce incorrect outputs on inputs with the trigger, outputs of spinned models preserve context and maintain standard accuracy metrics, yet also satisfy a meta-task chosen by the adversary.Model spinning enables propaganda-as-a-service, where propaganda is defined as biased speech. An adversary can create customized language models that produce desired spins for chosen triggers, then deploy these models to generate disinformation (a platform attack), or else inject them into ML training pipelines (a supply-chain attack), transferring malicious functionality to downstream models trained by victims.To demonstrate the feasibility of model spinning, we develop a new backdooring technique. It stacks an adversarial meta-task (e.g., sentiment analysis) onto a seq2seq model, backpropagates the desired meta-task output (e.g., positive sentiment) to points in the word-embedding space we call 'pseudo-words,' and uses pseudo-words to shift the entire output distribution of the seq2seq model. We evaluate this attack on language generation, summarization, and translation models with different triggers and meta-tasks such as sentiment, toxicity, and entailment. Spinned models largely maintain their accuracy metrics (ROUGE and BLEU) while shifting their outputs to satisfy the adversary's meta-task. We also show that, in the case of a supply-chain attack, the spin functionality transfers to downstream models.Finally, we propose a black-box, meta-task-independent defense that, given a list of candidate triggers, can detect models that selectively apply spin to inputs with any of these triggers.1We use 'spinned' rather than 'spun' to match how the word is used in public relations.},
archivePrefix = {arXiv},
arxivId = {2112.05224},
author = {Bagdasaryan, Eugene and Shmatikov, Vitaly},
booktitle = {2022 IEEE Symposium on Security and Privacy (SP)},
doi = {10.1109/SP46214.2022.9833572},
eprint = {2112.05224},
isbn = {978-1-6654-1316-9},
issn = {10816011},
month = {may},
pages = {769--786},
publisher = {IEEE Computer Society},
title = {{Spinning Language Models: Risks of Propaganda-As-A-Service and Countermeasures}},
year = {2022}
}
@inproceedings{li-etal-2020-bert-attack,
abstract = {Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency. In this paper, we propose $\backslash$textbf{\{}BERT-Attack{\}}, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly. Our method outperforms state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations. The code is available at $\backslash$url{\{}https://github.com/LinyangLee/BERT-Attack{\}}.},
author = {Li, Linyang and Ma, Ruotian and Guo, Qipeng and Xue, Xiangyang and Qiu, Xipeng},
booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
doi = {10.18653/v1/2020.emnlp-main.500},
pages = {6193--6202},
publisher = {Association for Computational Linguistics},
title = {{BERT-ATTACK: Adversarial Attack Against BERT Using BERT}},
url = {https://aclanthology.org/2020.emnlp-main.500},
year = {2020}
}
@misc{liu2024robustnesstimeunderstandingadversarial,
archivePrefix = {arXiv},
arxivId = {cs.CR/2308.07847},
author = {Liu, Yugeng and Cong, Tianshuo and Zhao, Zhengyu and Backes, Michael and Shen, Yun and Zhang, Yang},
booktitle = {arXiv:2308.07847},
eprint = {2308.07847},
primaryClass = {cs.CR},
title = {{Robustness Over Time: Understanding Adversarial Examples' Effectiveness on Longitudinal Versions of Large Language Models}},
url = {https://arxiv.org/abs/2308.07847},
year = {2024}
}
@article{YAO2024100211,
abstract = {Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes the papers into “The Good” (beneficial LLM applications), “The Bad” (offensive applications), and “The Ugly” (vulnerabilities of LLMs and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, Research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs' potential to both bolster and jeopardize cybersecurity.},
author = {Yao, Yifan and Duan, Jinhao and Xu, Kaidi and Cai, Yuanfang and Sun, Zhibo and Zhang, Yue},
doi = {https://doi.org/10.1016/j.hcc.2024.100211},
issn = {2667-2952},
journal = {High-Confidence Computing},
keywords = { ChatGPT, LLM attacks, LLM privacy, LLM security, LLM vulnerabilities,Large Language Model (LLM)},
number = {2},
pages = {100211},
title = {{A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly}},
url = {https://www.sciencedirect.com/science/article/pii/S266729522400014X},
volume = {4},
year = {2024}
}
@article{Allcott2017,
doi = {10.1257/jep.31.2.211},
issn = {0895-3309},
journal = {Journal of Economic Perspectives},
number = {2},
pages = {211--236},
title = {{Social Media and Fake News in the 2016 Election}},
volume = {31},
year = {2017}
}
@article{Al-Sarem2019,
abstract = {With the rapid increase in the popularity of social networks, the propagation of rumors is also increasing. Rumors can spread among thousands of users immediately without verification and can cause serious damages. Recently, several research studies have been investigated to control online rumors automatically by mining rich text available on the open network with deep learning techniques. In this paper, we conducted a systematic literature review for rumor detection using deep neural network approaches. A total of 108 studies were retrieved using manual research from five databases (IEEE Explore, Springer Link, Science Direct, ACM Digital Library, and Google Scholar). The considered studies are then examined in our systematic review to answer the seven research questions that we have formulated to deeply understand the overall trends in the use of deep learning methods for rumor detection. Apart from this, our systematic review also presents the challenges and issues that are faced by the researchers in this area and suggests promising future research directions. Our review will be beneficial for researchers in this domain as it will facilitate researchers' comparison with the existing works due to the availability of a complete description of the used performance matrices, dataset characteristics, and the deep learning model used per each work. Our review will also assist researchers in finding the available annotated datasets that can be used as benchmarks for comparing their new proposed approaches with the existing state-of-the-art works.},
author = {Al-Sarem, Mohammed and Boulila, Wadii and Al-Harby, Muna and Qadir, Junaid and Alsaeedi, Abdullah},
doi = {10.1109/ACCESS.2019.2947855},
issn = {21693536},
journal = {IEEE Access},
keywords = {Deep learning,Rumor detection,Systematic review,Twitter analysis},
pages = {152788--152812},
publisher = {IEEE},
title = {{Deep learning-based rumor detection on microblogging platforms: A systematic review}},
volume = {7},
year = {2019}
}
@misc{Przybya2022,
author = {Przyby{\l}a, Piotr},
title = {{LAMBO: Layered Approach to Multi-level BOundary identification}},
year = {2022}
}
@inproceedings{wilby-etal-2023-gate,
abstract = {We present GATE Teamware 2: an open-source web-based platform for managing teams of annotators working on document classification tasks. GATE Teamware 2 is an entirely re-engineered successor to GATE Teamware, using contemporary web frameworks. The software allows the management of teams of multiple annotators, project managers and administrators - including the management of annotators - across multiple projects. Projects can be configured to control and monitor the annotation statistics and have a highly flexible JSON-configurable annotation display which can include arbitrary HTML. Optionally, documents can be uploaded with pre-existing annotations and documents are served to annotators in a random order by default to reduce bias. Crucially, annotators can be trained on applying the annotation guidelines correctly and then screened for quality assurance purposes, prior to being cleared for independent annotation. GATE Teamware 2 can be self-deployed, including in container orchestration environments, or provided as private, hosted cloud instances.GATE Teamware 2 is an open-source software and can be downloaded from $\backslash$url{\{}https://github.com/GATENLP/gate-teamware.A{\}} demonstration video of the system has also been made available at $\backslash$url{\{}https://youtu.be/KoXkuhc4fmM{\}}.},
address = {Dubrovnik, Croatia},
author = {Wilby, David and Karmakharm, Twin and Roberts, Ian and Song, Xingyi and Bontcheva, Kalina},
booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations},
doi = {10.18653/v1/2023.eacl-demo.17},
editor = {Croce, Danilo and Soldaini, Luca},
month = {may},
pages = {145--151},
publisher = {Association for Computational Linguistics},
title = {{GATE Teamware 2: An open-source tool for collaborative document classification annotation}},
url = {https://aclanthology.org/2023.eacl-demo.17},
year = {2023}
}
@phdthesis{MacCartney2009,
author = {MacCartney, Bill},
school = {Stanford University},
title = {{Natural Language Inference}},
type = {Ph. D. thesis},
year = {2009}
}
@article{Liu2019a,
abstract = {Language model pretraining has led to significant performance gains but
careful comparison between different approaches is challenging. Training is
computationally expensive, often done on private datasets of different sizes,
and, as we will show, hyperparameter choices have significant impact on the
final results. We present a replication study of BERT pretraining (Devlin et
al., 2019) that carefully measures the impact of many key hyperparameters and
training data size. We find that BERT was significantly undertrained, and can
match or exceed the performance of every model published after it. Our best
model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results
highlight the importance of previously overlooked design choices, and raise
questions about the source of recently reported improvements. We release our
models and code.},
archivePrefix = {arXiv},
arxivId = {1907.11692},
author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin and Allen, Paul G},
doi = {10.48550/arxiv.1907.11692},
eprint = {1907.11692},
month = {jul},
title = {{RoBERTa: A Robustly Optimized BERT Pretraining Approach}},
url = {https://arxiv.org/abs/1907.11692v1},
year = {2019}
}
@inproceedings{Le2020,
abstract = {In recent years, the proliferation of so-called "fake news" has caused much disruptions in society and weakened the news ecosystem. Therefore, to mitigate such problems, researchers have developed state-of-the-art models to auto-detect fake news on social media using sophisticated data science and machine learning techniques. In this work, then, we ask "what if adversaries attempt to attack such detection models?" and investigate related issues by (i) proposing a novel threat model against fake news detectors, in which adversaries can post malicious comments toward news articles to mislead fake news detectors, and (ii) developing MALCOM, an end-to-end adversarial comment generation framework to achieve such an attack. Through a comprehensive evaluation, we demonstrate that about 94{\%} and 93.5{\%} of the time on average MALCOM can successfully mislead five of the latest neural detection models to always output targeted real and fake news labels. Furthermore, MALCOM can also fool black box fake news detectors to always output real news labels 90{\%} of the time on average. We also compare our attack model with four baselines across two real-world datasets, not only on attack performance but also on generated quality, coherency, transferability, and robustness.},
archivePrefix = {arXiv},
arxivId = {2009.01048},
author = {Le, Thai and Wang, Suhang and Lee, Dongwon},
booktitle = {Proceedings - IEEE International Conference on Data Mining, ICDM},
doi = {10.48550/arxiv.2009.01048},
eprint = {2009.01048},
isbn = {9781728183169},
issn = {15504786},
keywords = {Adversarial Examples,Fake News Detection,Malicious Comments},
pages = {282--291},
publisher = {IEEE},
title = {{MALCOM: Generating Malicious Comments to Attack Neural Fake News Detection Models}},
url = {https://arxiv.org/abs/2009.01048v2},
year = {2020}
}
@inproceedings{Kingma2015a,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
address = {San Diego, USA},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
eprint = {1412.6980},
publisher = {ICLR},
title = {{Adam: A method for stochastic optimization}},
url = {https://arxiv.org/abs/1412.6980v9},
year = {2015}
}
@inproceedings{Kumar2016,
abstract = {Wikipedia is a major source of information for many people. However, false information onWikipedia raises concerns about its credibility. One way in which false information may be presented on Wikipedia is in the form of hoax articles, i.e., articles containing fabricated facts about nonexistent entities or events. In this paper we study false information on Wikipedia by focusing on the hoax articles that have been created throughout its history. We make several contributions. First, we assess the real-world impact of hoax articles by measuring how long they survive before being debunked, how many pageviews they receive, and how heavily they are referred to by documents on the Web. We find that, while most hoaxes are detected quickly and have little impact on Wikipedia, a small number of hoaxes survive long and are well cited across the Web. Second, we characterize the nature of successful hoaxes by comparing them to legitimate articles and to failed hoaxes that were discovered shortly after being created. We find characteristic differences in terms of article structure and content, embeddedness into the rest of Wikipedia, and features of the editor who created the hoax. Third, we successfully apply our findings to address a series of classification tasks, most notably to determine whether a given article is a hoax. And finally, we describe and evaluate a task involving humans distinguishing hoaxes from non-hoaxes. We find that humans are not good at solving this task and that our automated classifier outperforms them by a big margin.},
author = {Kumar, Srijan and West, Robert and Leskovec, Jure},
booktitle = {25th International World Wide Web Conference, WWW 2016},
doi = {10.1145/2872427.2883085},
isbn = {9781450341431},
pages = {591--602},
publisher = {International World Wide Web Conferences Steering Committee},
title = {{Disinformation on the web: Impact, characteristics, and detection of wikipedia hoaxes}},
url = {https://dl.acm.org/doi/10.1145/2872427.2883085},
year = {2016}
}
@article{Kantartopoulos2020,
abstract = {Social media has become very popular and important in people{\&}rsquo;s lives, as personal ideas, beliefs and opinions are expressed and shared through them. Unfortunately, social networks, and specifically Twitter, suffer from massive existence and perpetual creation of fake users. Their goal is to deceive other users employing various methods, or even create a stream of fake news and opinions in order to influence an idea upon a specific subject, thus impairing the platform{\&}rsquo;s integrity. As such, machine learning techniques have been widely used in social networks to address this type of threat by automatically identifying fake accounts. Nonetheless, threat actors update their arsenal and launch a range of sophisticated attacks to undermine this detection procedure, either during the training or test phase, rendering machine learning algorithms vulnerable to adversarial attacks. Our work examines the propagation of adversarial attacks in machine learning based detection for fake Twitter accounts, which is based on AdaBoost. Moreover, we propose and evaluate the use of k-NN as a countermeasure to remedy the effects of the adversarial attacks that we have implemented.},
author = {Kantartopoulos, Panagiotis and Pitropakis, Nikolaos and Mylonas, Alexios and Kylilis, Nicolas},
doi = {10.3390/TECHNOLOGIES8040064},
issn = {2227-7080},
journal = {Technologies 2020},
keywords = {Twitter,adversarial attacks,machine learning,poisoning,social media},
number = {4},
pages = {64},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Exploring Adversarial Attacks and Defences for Fake Twitter Account Detection}},
url = {https://www.mdpi.com/2227-7080/8/4/64/htm https://www.mdpi.com/2227-7080/8/4/64},
volume = {8},
year = {2020}
}
@inproceedings{Martino2020a,
abstract = {We present the results and the main findings of SemEval-2020 Task 11 on Detection of Propaganda Techniques in News Articles. The task featured two subtasks. Subtask SI is about Span Identification: given a plain-text document, spot the specific text fragments containing propaganda. Subtask TC is about Technique Classification: given a specific text fragment, in the context of a full document, determine the propaganda technique it uses, choosing from an inventory of 14 possible propaganda techniques. The task attracted a large number of participants: 250 teams signed up to participate and 44 made a submission on the test set. In this paper, we present the task, analyze the results, and discuss the system submissions and the methods they used. For both subtasks, the best systems used pre-trained Transformers and ensembles.},
archivePrefix = {arXiv},
arxivId = {2009.02696},
author = {{da San Martino}, Giovanni and Barr{\'{o}}n-Cede{\~{n}}o, Alberto and Wachsmuth, Henning and Petrov, Rostislav and Nakov, Preslav},
booktitle = {Proceedings of the Fourteenth Workshop on Semantic Evaluation (SemEval-2020)},
eprint = {2009.02696},
pages = {1377--1414},
title = {{SemEval-2020 Task 11: Detection of Propaganda Techniques in News Articles}},
url = {http://propaganda.qcri.org/annotations/definitions.html http://arxiv.org/abs/2009.02696},
year = {2020}
}
@inproceedings{Vlachos2014,
abstract = {Abstract In this paper we introduce the task of fact checking , ie the assessment of the truthfulness of a claim. The task is commonly performed manually by journalists verifying the claims made by public figures. Furthermore, ordinary citizens need to assess the ...},
author = {Vlachos, Andreas and Riedel, Sebastian},
booktitle = {Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science},
doi = {10.3115/v1/W14-2508},
isbn = {9781941643105},
pages = {18--22},
title = {{Fact Checking: Task definition and dataset construction}},
year = {2014}
}
@article{Gong2018,
abstract = {Adversarial samples for images have been extensively studied in the literature. Among many of the attacking methods, gradient-based methods are both effective and easy to compute. In this work, we propose a framework to adapt the gradient attacking methods on images to text domain. The main difficulties for generating adversarial texts with gradient methods are i) the input space is discrete, which makes it difficult to accumulate small noise directly in the inputs, and ii) the measurement of the quality of the adversarial texts is difficult. We tackle the first problem by searching for adversarials in the embedding space and then reconstruct the adversarial texts via nearest neighbor search. For the latter problem, we employ the Word Mover's Distance (WMD) to quantify the quality of adversarial texts. Through extensive experiments on three datasets, IMDB movie reviews, Reuters-2 and Reuters-5 newswires, we show that our framework can leverage gradient attacking methods to generate very high-quality adversarial texts that are only a few words different from the original texts. There are many cases where we can change one word to alter the label of the whole piece of text. We successfully incorporate FGM and DeepFool into our framework. In addition, we empirically show that WMD is closely related to the quality of adversarial texts.},
archivePrefix = {arXiv},
arxivId = {1801.07175},
author = {Gong, Zhitao and Wang, Wenlu and Li, Bo and Song, Dawn and Ku, Wei-Shinn},
doi = {10.48550/arxiv.1801.07175},
eprint = {1801.07175},
journal = {arXiv:1801.07175},
title = {{Adversarial Texts with Gradient Methods}},
url = {https://arxiv.org/abs/1801.07175v2},
year = {2018}
}
@article{Gorwa2020,
abstract = {As government pressure on major technology companies builds, both firms and legislators are searching for technical solutions to difficult platform governance puzzles such as hate speech and misinf...},
author = {Gorwa, Robert and Binns, Reuben and Katzenbach, Christian},
doi = {10.1177/2053951719897945},
issn = {20539517},
journal = {Big Data {\&} Society},
keywords = {Platform governance,algorithms,artificial intelligence,content moderation,copyright,toxic speech},
number = {1},
publisher = {SAGE Publications},
title = {{Algorithmic content moderation: Technical and political challenges in the automation of platform governance}},
url = {https://journals.sagepub.com/doi/full/10.1177/2053951719897945},
volume = {7},
year = {2020}
}
@inproceedings{Thorne19FEVER2,
author = {Thorne, James and Vlachos, Andreas and Cocarascu, Oana and Christodoulopoulos, Christos and Mittal, Arpit},
booktitle = {Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER)},
title = {{The FEVER2.0 Shared Task}},
year = {2018}
}
@inproceedings{kiesel-etal-2019-semeval,
abstract = {Hyperpartisan news is news that takes an extreme left-wing or right-wing standpoint. If one is able to reliably compute this meta information, news articles may be automatically tagged, this way encouraging or discouraging readers to consume the text. It is an open question how successfully hyperpartisan news detection can be automated, and the goal of this SemEval task was to shed light on the state of the art. We developed new resources for this purpose, including a manually labeled dataset with 1,273 articles, and a second dataset with 754,000 articles, labeled via distant supervision. The interest of the research community in our task exceeded all our expectations: The datasets were downloaded about 1,000 times, 322 teams registered, of which 184 configured a virtual machine on our shared task cloud service TIRA, of which in turn 42 teams submitted a valid run. The best team achieved an accuracy of 0.822 on a balanced sample (yes : no hyperpartisan) drawn from the manually tagged corpus; an ensemble of the submitted systems increased the accuracy by 0.048.},
address = {Minneapolis, Minnesota, USA},
author = {Kiesel, Johannes and Mestre, Maria and Shukla, Rishabh and Vincent, Emmanuel and Adineh, Payam and Corney, David and Stein, Benno and Potthast, Martin},
booktitle = {Proceedings of the 13th International Workshop on Semantic Evaluation},
doi = {10.18653/v1/S19-2145},
pages = {829--839},
publisher = {Association for Computational Linguistics},
title = {{SemEval-2019 Task 4: Hyperpartisan News Detection}},
url = {https://aclanthology.org/S19-2145},
year = {2019}
}
@article{Szegedy2013,
abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
archivePrefix = {arXiv},
arxivId = {1312.6199},
author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
doi = {10.48550/arxiv.1312.6199},
eprint = {1312.6199},
journal = {arXiv: 1312.6199},
month = {dec},
title = {{Intriguing properties of neural networks}},
url = {https://arxiv.org/abs/1312.6199v4},
year = {2013}
}
@inproceedings{Smith2021,
abstract = {The study of adversarial effects on AI systems is not a new concept, but much of the research has been devoted to deep learning. In this paper we explore the effects of adversarial examples on 4 machine learning classifiers and measure the effectiveness of adversarial training. Additionally, we present a novel method for selecting adversarial training examples that lead to a more robust machine learning system. Our results suggest that adversarial examples can significantly hinder the classification performance and that adversarial training is an effective defensive counter-measure.},
author = {Smith, Marcellus and Brown, Brandon and Dozier, Gerry and King, Michael},
booktitle = {2021 IEEE Congress on Evolutionary Computation, CEC 2021 - Proceedings},
doi = {10.1109/CEC45853.2021.9504723},
isbn = {9781728183923},
keywords = {Adversarial example selection,Adversarial examples,Adversarial training,GAT,Steady-state genetic algorithm},
pages = {1265--1271},
publisher = {IEEE},
title = {{Mitigating Attacks on Fake News Detection Systems using Genetic-Based Adversarial Training}},
year = {2021}
}
@inproceedings{Singhal2022,
abstract = {To counter online abuse and misinformation, social media platforms have been establishing content moderation guidelines and employing various moderation policies. The goal of this paper is to study these community guidelines and moderation practices, as well as the relevant research publications to identify the research gaps, differences in moderation techniques, and challenges that should be tackled by the social media platforms and the research community at large. In this regard, we study and analyze in the US jurisdiction the fourteen most popular social media content moderation guidelines and practices, and consolidate them. We then introduce three taxonomies drawn from this analysis as well as covering over one hundred interdisciplinary research papers about moderation strategies. We identified the differences between the content moderation employed in mainstream social media platforms compared to fringe platforms. We also highlight the implications of Section 230, the need for transparency and opacity in content moderation, why platforms should shift from a one-size-fits-all model to a more inclusive model, and lastly, we highlight why there is a need for a collaborative human-AI system.},
archivePrefix = {arXiv},
arxivId = {2206.14855},
author = {Singhal, Mohit and Ling, Chen and Paudel, Pujan and Thota, Poojitha and Kumarswamy, Nihal and Stringhini, Gianluca and Nilizadeh, Shirin},
booktitle = {The 8th IEEE European Symposium on Security and Privacy (EuroS{\&}P 2023)},
doi = {10.48550/arxiv.2206.14855},
eprint = {2206.14855},
isbn = {2206.14855v2},
publisher = {IEEE},
title = {{SoK: Content Moderation in Social Media, from Guidelines to Enforcement, and Research to Practice}},
url = {https://arxiv.org/abs/2206.14855v2},
year = {2022}
}
@inproceedings{sellam-etal-2020-bleurt,
abstract = {Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment. We propose BLEURT, a learned evaluation metric for English based on BERT. BLEURT can model human judgment with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG data set. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution.},
address = {Online},
author = {Sellam, Thibault and Das, Dipanjan and Parikh, Ankur},
booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
doi = {10.18653/v1/2020.acl-main.704},
month = {jul},
pages = {7881--7892},
publisher = {Association for Computational Linguistics},
title = {{BLEURT: Learning Robust Metrics for Text Generation}},
url = {https://aclanthology.org/2020.acl-main.704},
year = {2020}
}
@inproceedings{Chen2022,
abstract = {Textual adversarial samples play important roles in multiple subfields of NLP research, including security, evaluation, explainability, and data augmentation. However, most work mixes all these roles, obscuring the problem definitions and research goals of the security role that aims to reveal the practical concerns of NLP models. In this paper, we rethink the research paradigm of textual adversarial samples in security scenarios. We discuss the deficiencies in previous work and propose our suggestions that the research on the Security-oriented adversarial NLP (SoadNLP) should: (1) evaluate their methods on security tasks to demonstrate the real-world concerns; (2) consider real-world attackers' goals, instead of developing impractical methods. To this end, we first collect, process, and release a security datasets collection Advbench. Then, we reformalize the task and adjust the emphasis on different goals in SoadNLP. Next, we propose a simple method based on heuristic rules that can easily fulfill the actual adversarial goals to simulate real-world attack methods. We conduct experiments on both the attack and the defense sides on Advbench. Experimental results show that our method has higher practical value, indicating that the research paradigm in SoadNLP may start from our new benchmark. All the code and data of Advbench can be obtained at $\backslash$url{\{}https://github.com/thunlp/Advbench{\}}.},
author = {Chen, Yangyi and Gao, Hongcheng and Cui, Ganqu and Qi, Fanchao and Huang, Longtao and Liu, Zhiyuan and Sun, Maosong},
booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
pages = {11222----11237},
title = {{Why Should Adversarial Perturbations be Imperceptible? Rethink the Research Paradigm in Adversarial NLP}},
url = {https://aclanthology.org/2022.emnlp-main.771},
year = {2022}
}
@inproceedings{DBLP:conf/aaai/JinJZS20,
author = {Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter},
booktitle = {The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020,},
pages = {8018--8025},
publisher = {AAAI Press},
title = {{Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment}},
url = {https://ojs.aaai.org/index.php/AAAI/article/view/6311},
year = {2020}
}
@inproceedings{clef-checkthat:2024:task6,
address = {Grenoble, France},
author = {Przyby{\l}a, Piotr and Wu, Ben and Shvets, Alexander and Mu, Yida and Sheang, Kim Cheng and Song, Xingyi and Saggion, Horacio},
booktitle = {Working Notes of CLEF 2024 - Conference and Labs of the Evaluation Forum},
editor = {Faggioli, Guglielmo and Ferro, Nicola and Galu{\v{s}}{\v{c}}{\'{a}}kov{\'{a}}, Petra and {Garc{\'{i}}a Seco de Herrera}, Alba},
series = {CLEF{\~{}}2024},
title = {{Overview of the CLEF-2024 CheckThat! Lab Task 6 on Robustness of Credibility Assessment with Adversarial Examples (InCrediblAE)}},
year = {2024}
}
@book{Smith1989a,
abstract = {This important new book assembles the work of leading figures in contemporary propaganda scholarship. Analyzing propaganda from a multidisciplinary focus, the book presents several contemporary theoretical perspectives, explores key issues in propaganda analysis, and defines two major research traditions while providing examples of their applications. The contributors examine many of the most complicated issues in the field: the nature of suggestion, the relation of propaganda to ideology, and the interaction of pluralism and truth. Various chapters, written by scholars of communication, rhetoric, journalism, mass communication, government, history, and political science, consider both historical and contemporary issues and events in relation to propaganda. Propaganda: A Pluralistic Perspective marks the renewed development of scholarship in this fascinating field and extends the depth and range of propaganda analysis. The book begins with a focus on theoretical and definitional concerns, including a history of American propaganda analysis and traces four social responses to the subject. Further chapters develop different theoretical positions from diverse perspectives. The book concludes with a focus on key issues in propaganda research, including a study of First Amendment issues in the recent legal controversy over the classification of three Canadian films as political propaganda. Students and scholars of communication, rhetoric, journalism, history, political science, sociology, and many other disciplines will find Propaganda: A Pluralistic Perspective a provocative book full of stimulating ideas.},
author = {Smith, Ted J.},
publisher = {Praeger},
title = {{Propaganda: A Pluralistic Perspective}},
year = {1989}
}
@article{Mannes2020,
abstract = {Artificial intelligence, whether embodied as robots or Internet of Things, or disembodied as intelligent agents or decision-support systems, can enrich the human experience. It will also fail and cause harms, including physical injury and financial loss as well as more subtle harms such as instantiating human bias or undermining individual dignity. These failures could have a disproportionate impact because strange, new, and unpredictable dangers may lead to public discomfort and rejection of artificial intelligence. Two possible approaches to mitigating these risks are the hard power of regulating artificial intelligence, to ensure it is safe, and the soft power of risk communication, which engages the public and builds trust. These approaches are complementary and both should be implemented as artificial intelligence becomes increasingly prevalent in daily life.},
author = {Mannes, Aaron},
doi = {10.1609/AIMAG.V41I1.5200},
issn = {2371-9621},
journal = {AI Magazine},
keywords = {Spring 2020},
number = {1},
pages = {61--69},
publisher = {AI Access Foundation},
title = {{Governance, Risk, and Artificial Intelligence}},
url = {https://ojs.aaai.org/index.php/aimagazine/article/view/5200},
volume = {41},
year = {2020}
}
@article{Przybya2023,
abstract = {Text classification methods have been widely investigated as a way to detect content of low credibility: fake news, social media bots, propaganda, etc. Quite accurate models (likely based on deep neural networks) help in moderating public electronic platforms and often cause content creators to face rejection of their submissions or removal of already published texts. Having the incentive to evade further detection, content creators try to come up with a slightly modified version of the text (known as an attack with an adversarial example) that exploit the weaknesses of classifiers and result in a different output. Here we introduce BODEGA: a benchmark for testing both victim models and attack methods on four misinformation detection tasks in an evaluation framework designed to simulate real use-cases of content moderation. We also systematically test the robustness of popular text classifiers against available attacking techniques and discover that, indeed, in some cases barely significant changes in input text can mislead the models. We openly share the BODEGA code and data in hope of enhancing the comparability and replicability of further research in this area.},
archivePrefix = {arXiv},
arxivId = {2303.08032},
author = {Przyby{\l}a, Piotr and Shvets, Alexander and Saggion, Horacio},
eprint = {2303.08032},
journal = {arXiv preprint arXiv:2303.08032},
month = {mar},
title = {{Verifying the Robustness of Automatic Credibility Assessment}},
url = {https://arxiv.org/abs/2303.08032v1},
year = {2023}
}
@article{Alsmadi2022,
author = {Alsmadi, Izzat and Ahmad, Kashif and Nazzal, Mahmoud and Alam, Firoj and Al-Fuqaha, Ala and Khreishah, Abdallah and Algosaibi, Abdulelah},
doi = {10.1109/TCSS.2022.3218743},
issn = {2329924X},
journal = {IEEE Transactions on Computational Social Systems},
publisher = {IEEE},
title = {{Adversarial NLP for Social Network Applications: Attacks, Defenses, and Research Directions}},
year = {2022}
}
@inproceedings{ribeiro-etal-2018-semantically,
abstract = {Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs) {\{}--{\}} semantic-preserving perturbations that induce changes in the model{\{}'{\}}s predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) {\{}--{\}} simple, universal replacement rules that induce adversaries on many instances. We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question-answering, and sentiment analysis. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models using data augmentation significantly reduces bugs, while maintaining accuracy.},
address = {Melbourne, Australia},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
doi = {10.18653/v1/P18-1079},
pages = {856--865},
publisher = {Association for Computational Linguistics},
title = {{Semantically Equivalent Adversarial Rules for Debugging NLP models}},
url = {https://aclanthology.org/P18-1079},
year = {2018}
}
@article{Hochreiter1997a,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
issn = {08997667},
journal = {Neural Computation},
number = {8},
pages = {1735--1780},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}
@inproceedings{eger-etal-2019-text,
abstract = {Visual modifications to text are often used to obfuscate offensive comments in social media (e.g., {\{}``{\}}!d10t{\{}''{\}}) or as a writing style ({\{}``{\}}1337{\{}''{\}} in {\{}``{\}}leet speak{\{}''{\}}), among other scenarios. We consider this as a new type of adversarial attack in NLP, a setting to which humans are very robust, as our experiments with both simple and more difficult visual perturbations demonstrate. We investigate the impact of visual adversarial attacks on current NLP systems on character-, word-, and sentence-level tasks, showing that both neural and non-neural models are, in contrast to humans, extremely sensitive to such attacks, suffering performance decreases of up to 82{\{}$\backslash${\%}{\}}. We then explore three shielding methods{\{}---{\}}visual character embeddings, adversarial training, and rule-based recovery{\{}---{\}}which substantially improve the robustness of the models. However, the shielding methods still fall behind performances achieved in non-attack scenarios, which demonstrates the difficulty of dealing with visual attacks.},
address = {Minneapolis, Minnesota},
author = {Eger, Steffen and Şahin, G{\"{o}}zde G{\"{u}}l and R{\"{u}}ckl{\'{e}}, Andreas and Lee, Ji-Ung and Schulz, Claudia and Mesgar, Mohsen and Swarnkar, Krishnkant and Simpson, Edwin and Gurevych, Iryna},
booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
doi = {10.18653/v1/N19-1165},
month = {jun},
pages = {1634--1647},
publisher = {Association for Computational Linguistics},
title = {{Text Processing Like Humans Do: Visually Attacking and Shielding NLP Systems}},
url = {https://aclanthology.org/N19-1165},
year = {2019}
}
@inproceedings{clef-checkthat:2024-lncs,
author = {Barr{\'{o}}n-Cede{\~{n}}o, Alberto and Alam, Firoj and Stru{\ss}, Julia Maria and Nakov, Preslav and Chakraborty, Tanmoy and Elsayed, Tamer and Przyby{\l}a, Piotr and Caselli, Tommaso and {Da San Martino}, Giovanni and Haouari, Fatima and Li, Chengkai and Piskorski, Jakub and Ruggeri, Federico and Song, Xingyi and Suwaileh, Reem},
booktitle = {Experimental IR Meets Multilinguality, Multimodality, and Interaction. Proceedings of the Fifteenth International Conference of the CLEF Association (CLEF 2024)},
editor = {Goeuriot, Lorraine and Mulhem, Philippe and Qu{\'{e}}not, Georges and Schwab, Didier and Soulier, Laure and {Di Nunzio}, Giorgio Maria and Galu{\v{s}}{\v{c}}{\'{a}}kov{\'{a}}, Petra and de Herrera, Alba and Faggioli, Guglielmo and Ferro, Nicola},
title = {{Overview of the CLEF-2024 CheckThat! Lab: Check-Worthiness, Subjectivity, Persuasion, Roles, Authorities and Adversarial Robustness}},
year = {2024}
}
@inproceedings{yoo-etal-2022-detection,
abstract = {Word-level adversarial attacks have shown success in NLP models, drastically decreasing the performance of transformer-based models in recent years. As a countermeasure, adversarial defense has been explored, but relatively few efforts have been made to detect adversarial examples. However, detecting adversarial examples may be crucial for automated tasks (e.g. review sentiment analysis) that wish to amass information about a certain population and additionally be a step towards a robust defense system. To this end, we release a dataset for four popular attack methods on four datasets and four models to encourage further research in this field. Along with it, we propose a competitive baseline based on density estimation that has the highest auc on 29 out of 30 dataset-attack-model combinations. The source code is released ($\backslash$url{\{}https://github.com/bangawayoo/adversarial-examples-in-text-classification{\}}).},
address = {Dublin, Ireland},
author = {Yoo, KiYoon and Kim, Jangho and Jang, Jiho and Kwak, Nojun},
booktitle = {Findings of the Association for Computational Linguistics: ACL 2022},
doi = {10.18653/v1/2022.findings-acl.289},
editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
month = {may},
pages = {3656--3672},
publisher = {Association for Computational Linguistics},
title = {{Detection of Adversarial Examples in Text Classification: Benchmark and Baseline via Robust Density Estimation}},
url = {https://aclanthology.org/2022.findings-acl.289},
year = {2022}
}
@inproceedings{zang-etal-2020-word,
abstract = {Adversarial attacks are carried out to reveal the vulnerability of deep neural networks. Textual adversarial attacking is challenging because text is discrete and a small perturbation can bring significant change to the original input. Word-level attacking, which can be regarded as a combinatorial optimization problem, is a well-studied class of textual attack methods. However, existing word-level attack models are far from perfect, largely because unsuitable search space reduction methods and inefficient optimization algorithms are employed. In this paper, we propose a novel attack model, which incorporates the sememe-based word substitution method and particle swarm optimization-based search algorithm to solve the two problems separately. We conduct exhaustive experiments to evaluate our attack model by attacking BiLSTM and BERT on three benchmark datasets. Experimental results demonstrate that our model consistently achieves much higher attack success rates and crafts more high-quality adversarial examples as compared to baseline methods. Also, further experiments show our model has higher transferability and can bring more robustness enhancement to victim models by adversarial training. All the code and data of this paper can be obtained on https://github.com/thunlp/SememePSO-Attack.},
address = {Online},
author = {Zang, Yuan and Qi, Fanchao and Yang, Chenghao and Liu, Zhiyuan and Zhang, Meng and Liu, Qun and Sun, Maosong},
booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
doi = {10.18653/v1/2020.acl-main.540},
pages = {6066--6080},
publisher = {Association for Computational Linguistics},
title = {{Word-level Textual Adversarial Attacking as Combinatorial Optimization}},
url = {https://aclanthology.org/2020.acl-main.540},
year = {2020}
}
@inproceedings{Han2019,
abstract = {The scarcity and class imbalance of training data are known issues in current rumor detection tasks. We propose a straight-forward and general-purpose data augmentation technique which is beneficial to early rumor detection relying on event propagation patterns. The key idea is to exploit massive unlabeled event data sets on social media to augment limited labeled rumor source tweets. This work is based on rumor spreading patterns revealed by recent rumor studies and semantic relatedness between labeled and unlabeled data. A state-of-the-art neural language model (NLM) and large credibility-focused Twitter corpora are employed to learn context-sensitive representations of rumor tweets. Six different real-world events based on three publicly available rumor datasets are employed in our experiments to provide a comparative evaluation of the effectiveness of the method. The results show that our method can expand the size of an existing rumor data set nearly by 200{\%} and corresponding social context (i.e., conversational threads) by 100{\%} with reasonable quality. Preliminary experiments with a state-of-the-art deep learning-based rumor detection model show that augmented data can alleviate over-fitting and class imbalance caused by limited train data and can help to train complex neural networks (NNs). With augmented data, the performance of rumor detection can be improved by 12.1{\%} in terms of F-score. Our experiments also indicate that augmented training data can help to generalize rumor detection models on unseen rumors.},
archivePrefix = {arXiv},
arxivId = {1907.07033},
author = {Han, Sooji and Gao, Jie and Ciravegna, Fabio},
booktitle = {Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2019},
doi = {10.1145/3341161.3342892},
eprint = {1907.07033},
isbn = {9781450368681},
keywords = {Data augmentation,Rumor detection,Social media,Weak supervision},
pages = {105--112},
publisher = {Association for Computing Machinery, Inc},
title = {{Neural language model based training data augmentation for weakly supervised early rumor detection}},
url = {https://dl.acm.org/doi/10.1145/3341161.3342892},
year = {2019}
}
@article{1965,
author = {Levenshtein, Vladimir Iosifovich},
journal = {Soviet Physics Doklady},
pages = {707--710},
publisher = {Russian Academy of Sciences},
title = {{Binary codes capable of correcting deletions, insertions, and reversals}},
volume = {10},
year = {1966}
}
@inproceedings{karpukhin-etal-2020-dense,
abstract = {Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9{\{}$\backslash${\%}{\}}-19{\{}$\backslash${\%}{\}} absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.},
address = {Online},
author = {Karpukhin, Vladimir and Oguz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
doi = {10.18653/v1/2020.emnlp-main.550},
month = {nov},
pages = {6769--6781},
publisher = {Association for Computational Linguistics},
title = {{Dense Passage Retrieval for Open-Domain Question Answering}},
url = {https://aclanthology.org/2020.emnlp-main.550},
year = {2020}
}
@inproceedings{Nakov2022,
abstract = {We present an overview of CheckThat! lab 2022 Task 1, part of the 2022 Conference and Labs of the Evaluation Forum (CLEF). Task 1 asked to predict which posts in a Twitter stream are worth fact-checking, focusing on COVID-19 and politics in six languages: Arabic, Bulgarian, Dutch, English, Spanish, and Turkish. A total of 19 teams participated and most submissions managed to achieve sizable improvements over the baselines using Transformer-based models such as BERT and GPT-3. Across the four subtasks, approaches that targetted multiple languages (be it individually or in conjunction, in general obtained the best performance. We describe the dataset and the task setup, including the evaluation settings, and we give a brief overview of the participating systems. As usual in the CheckThat! lab, we release to the research community all datasets from the lab as well as the evaluation scripts, which should enable further research on finding relevant tweets that can help different stakeholders such as fact-checkers, journalists, and policymakers.},
address = {Bologna, Italy},
author = {Nakov, Preslav and Barr{\'{o}}n-Cede{\~{n}}o, Alberto and {Da San Martino}, Giovanni and Alam, Firoj and M{\'{i}}guez, Rub{\'{e}}n and Caselli, Tommaso and Kutlu, Mucahid and Zaghouani, Wajdi and Li, Chengkai and Shaar, Shaden and Mubarak, Hamdy and Nikolov, Alex and Kartal, Yavuz Selim},
booktitle = {CLEF 2022: Conference and Labs of the Evaluation Forum},
issn = {16130073},
keywords = {COVID-19,Check-Worthiness Estimation,Computational Journalism,Fact-Checking,Social Media Verification,Veracity},
pages = {368--392},
publisher = {CEUR Workshop Proceedings (CEUR-WS.org)},
title = {{Overview of the CLEF-2022 CheckThat! Lab Task 1 on Identifying Relevant Claims in Tweets}},
volume = {3180},
year = {2022}
}
@inproceedings{Horne2017,
abstract = {The problem of fake news has gained a lot of attention as it is claimed to have had a significant impact on 2016 US Presidential Elections. Fake news is not a new problem and its spread in social networks is well-studied. Often an underlying assumption in fake news discussion is that it is written to look like real news, fooling the reader who does not check for reliability of the sources or the arguments in its content. Through a unique study of three data sets and features that capture the style and the language of articles, we show that this assumption is not true. Fake news in most cases is more similar to satire than to real news, leading us to conclude that persuasion in fake news is achieved through heuristics rather than the strength of arguments. We show overall title structure and the use of proper nouns in titles are very significant in differentiating fake from real. This leads us to conclude that fake news is targeted for audiences who are not likely to read beyond titles and is aimed at creating mental associations between entities and claims.},
archivePrefix = {arXiv},
arxivId = {1703.09398},
author = {Horne, Benjamin D. and Adali, Sibel},
booktitle = {Proceedings of the 2nd International Workshop on News and Public Opinion at ICWSM},
eprint = {1703.09398},
publisher = {Association for the Advancement of Artificial Intelligence},
title = {{This Just In: Fake News Packs a Lot in Title, Uses Simpler, Repetitive Content in Text Body, More Similar to Satire than Real News}},
url = {http://arxiv.org/abs/1703.09398},
year = {2017}
}
@article{lialin2023scaling,
archivePrefix = {arXiv},
arxivId = {cs.CL/2303.15647},
author = {Lialin, Vladislav and Deshpande, Vijeta and Rumshisky, Anna},
doi = {10.48550/arXiv.2303.15647 Focus to learn more},
eprint = {2303.15647},
journal = {arXiv preprint arXiv:2303.15647},
primaryClass = {cs.CL},
title = {{Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning}},
year = {2023}
}
@inproceedings{alzantot-etal-2018-generating,
abstract = {Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations to correctly classified examples which can cause the model to misclassify. In the image domain, these perturbations can often be made virtually indistinguishable to human perception, causing humans and state-of-the-art models to disagree. However, in the natural language domain, small perturbations are clearly perceptible, and the replacement of a single word can drastically alter the semantics of the document. Given these challenges, we use a black-box population-based optimization algorithm to generate semantically and syntactically similar adversarial examples that fool well-trained sentiment analysis and textual entailment models with success rates of 97{\{}$\backslash${\%}{\}} and 70{\{}$\backslash${\%}{\}}, respectively. We additionally demonstrate that 92.3{\{}$\backslash${\%}{\}} of the successful sentiment analysis adversarial examples are classified to their original label by 20 human annotators, and that the examples are perceptibly quite similar. Finally, we discuss an attempt to use adversarial training as a defense, but fail to yield improvement, demonstrating the strength and diversity of our adversarial examples. We hope our findings encourage researchers to pursue improving the robustness of DNNs in the natural language domain.},
address = {Brussels, Belgium},
author = {Alzantot, Moustafa and Sharma, Yash and Elgohary, Ahmed and Ho, Bo-Jhang and Srivastava, Mani and Chang, Kai-Wei},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
doi = {10.18653/v1/D18-1316},
pages = {2890--2896},
publisher = {Association for Computational Linguistics},
title = {{Generating Natural Language Adversarial Examples}},
url = {https://aclanthology.org/D18-1316},
year = {2018}
}
@misc{Mierzynska2020,
author = {Mierzy{\'{n}}ska, Anna},
booktitle = {OKO press},
title = {{Chmura znad Czarnobyla - kolejna dezinformacja, kt{\'{o}}r{\c{a}} straszono Polak{\'{o}}w. Wiemy, sk{\c{a}}d si{\c{e}} wzi{\c{e}}{\l}a}},
url = {https://oko.press/radioaktywna-chmura-znad-czarnobyla-kolejna-dezinformacja-ktora-straszono-polakow},
year = {2020}
}
@inproceedings{Rangel2019,
author = {Rangel, Francisco and Rosso, Paolo},
booktitle = {CLEF 2019 Labs and Workshops, Notebook Papers. CEUR Workshop Proceedings},
editor = {Cappellato, Linda and Ferro, Nicola and Losada, David E. and M{\"{u}}ller, Henning},
publisher = {CEUR-WS.org},
title = {{Overview of the 7th Author Profiling Task at PAN 2019: Bots and Gender Profiling}},
year = {2019}
}
@inproceedings{Dalvi2004,
abstract = {Essentially all data mining algorithms assume that the data-generating process is independent of the data miner's activities. However, in many domains, including spam detection, intrusion detection, fraud detection, surveillance and counter-terrorism, this is far from the case: the data is actively manipulated by an adversary seeking to make the classifier produce false negatives. In these domains, the performance of a classifier can degrade rapidly after it is deployed, as the adversary learns to defeat it. Currently the only solution to this is repeated, manual, ad hoc reconstruction of the classifier. In this paper we develop a formal framework and algorithms for this problem. We view classification as a game between the classifier and the adversary, and produce a classifier that is optimal given the adversary's optimal strategy. Experiments in a spam detection domain show that this approach can greatly outperform a classifier learned in the standard way, and (within the parameters of the problem) automatically adapt the classifier to the adversary's evolving manipulations.},
author = {Dalvi, Nilesh and Domingos, Pedro and Mausam and Sanghai, Sumit and Verma, Deepak},
booktitle = {KDD-2004 - Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/1014052.1014066},
isbn = {1581138881},
keywords = {Cost-sensitive learning,Game theory,Integer linear programming,Naive bayes,Spam detection},
pages = {99--108},
publisher = {Association for Computing Machinery (ACM)},
title = {{Adversarial classification}},
year = {2004}
}
@article{Bakir2017,
abstract = {This paper examines the 2016 US presidential election campaign to identify problems with, causes of and solutions to the contemporary fake news phenomenon. To achieve this, we employ textual analysis and feedback from engagement, meetings and panels with technolo-gists, journalists, editors, non-profits, public relations firms, analytics firms and academics dur-ing the globally leading technology conference, South-by-South West, in March 2017. We further argue that what is most significant about the contemporary fake news furore is what it portends: the use of personally and emotionally targeted news produced by algo-journalism and what we term " empathic media " . In assessing solutions to this democratically problematic situation, we recommend that greater attention is paid to the role of digital advertising in causing, and combating, both the contemporary fake news phenomenon, and the near-horizon variant of empathically optimised automated fake news.},
author = {Bakir, Vian and McStay, Andrew},
doi = {10.1080/21670811.2017.1345645},
issn = {2167082X},
journal = {Digital Journalism},
keywords = {Facebook,Trump election,digital advertising,emotion,empathic media,fake news},
number = {2},
pages = {154--175},
title = {{Fake News and The Economy of Emotions: Problems, causes, solutions}},
volume = {6},
year = {2017}
}
@inproceedings{Zhou2019b,
abstract = {News plays a significant role in shaping people's beliefs and opinions. Fake news has always been a problem, which wasn't exposed to the mass public until the past election cycle for the 45th President of the United States. While quite a few detection methods have been proposed to combat fake news since 2015, they focus mainly on linguistic aspects of an article without any fact checking. In this paper, we argue that these models have the potential to misclassify fact-tampering fake news as well as under-written real news. Through experiments on Fakebox, a state-of-the-art fake news detector, we show that fact tampering attacks can be effective. To address these weaknesses, we argue that fact checking should be adopted in conjunction with linguistic characteristics analysis, so as to truly separate fake news from real news. A crowdsourced knowledge graph is proposed as a straw man solution to collecting timely facts about news events.},
archivePrefix = {arXiv},
arxivId = {1901.09657v1},
author = {Zhou, Zhixuan and Guan, Huankang and Bhat, Meghana Moorthy and Hsu, Justin},
booktitle = {ICAART 2019 - Proceedings of the 11th International Conference on Agents and Artificial Intelligence},
doi = {10.5220/0007566307940800},
eprint = {1901.09657v1},
keywords = {Attack,Fact Checking,Fake News Detection,NLP,Outsourced Knowledge Graph},
pages = {794--800},
publisher = {SciTePress},
title = {{Fake News Detection via NLP is Vulnerable to Adversarial Attacks}},
url = {http://arxiv.org/abs/1901.09657 http://dx.doi.org/10.5220/0007566307940800},
volume = {2},
year = {2019}
}
@inproceedings{ren-etal-2019-generating,
abstract = {We address the problem of adversarial attacks on text classification, which is rarely studied comparing to attacks on image classification. The challenge of this task is to generate adversarial examples that maintain lexical correctness, grammatical correctness and semantic similarity. Based on the synonyms substitution strategy, we introduce a new word replacement order determined by both the word saliency and the classification probability, and propose a greedy algorithm called probability weighted word saliency (PWWS) for text adversarial attack. Experiments on three popular datasets using convolutional as well as LSTM models show that PWWS reduces the classification accuracy to the most extent, and keeps a very low word substitution rate. A human evaluation study shows that our generated adversarial examples maintain the semantic similarity well and are hard for humans to perceive. Performing adversarial training using our perturbed datasets improves the robustness of the models. At last, our method also exhibits a good transferability on the generated adversarial examples.},
address = {Florence, Italy},
author = {Ren, Shuhuai and Deng, Yihe and He, Kun and Che, Wanxiang},
booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
doi = {10.18653/v1/P19-1103},
pages = {1085--1097},
publisher = {Association for Computational Linguistics},
title = {{Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency}},
url = {https://aclanthology.org/P19-1103},
year = {2019}
}
@inproceedings{Shu2019,
abstract = {Social media is becoming popular for news consumption due to its fast dissemination, easy access, and low cost. However, it also enables the wide propagation of fake news, i.e., news with intentionally false information. Detecting fake news is an important task, which not only ensures users receive authentic information but also helps maintain a trustworthy news ecosystem. The majority of existing detection algorithms focus on finding clues from news contents, which are generally not effective because fake news is often intentionally written to mislead users by mimicking true news. Therefore, we need to explore auxiliary information to improve detection. The social context during news dissemination process on social media forms the inherent tri-relationship, the relationship among publishers, news pieces, and users, which has potential to improve fake news detection. For example, partisan-biased publishers are more likely to publish fake news, and low-credible users are more likely to share fake news. In this paper, we study the novel problem of exploiting social context for fake news detection. We propose a tri-relationship embedding framework TriFN, which models publisher-news relations and user-news interactions simultaneously for fake news classification. We conduct experiments on two real-world datasets, which demonstrate that the proposed approach significantly outperforms other baseline methods for fake news detection.},
address = {New York, NY, USA},
author = {Shu, Kai and Wang, Suhang and Liu, Huan},
booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
doi = {10.1145/3289600},
isbn = {9781450359405},
publisher = {ACM},
title = {{Beyond News Contents: The Role of Social Context for Fake News Detection}},
url = {https://doi.org/10.1145/3289600.3290994},
volume = {9},
year = {2019}
}
@article{Liu2020a,
abstract = {The fast spreading of fake news stories on social media can cause inestimable social harm. Developing effective methods to detect them early is of paramount importance. A major challenge of fake ne...},
author = {Liu, Yang and Wu, Yi Fang Brook},
doi = {10.1145/3386253},
issn = {15582868},
journal = {ACM Transactions on Information Systems (TOIS)},
keywords = {Fake news detection,deep learning,social media},
number = {3},
publisher = {ACM PUB27 New York, NY, USA},
title = {{FNED: A Deep Network for Fake News Early Detection on Social Media}},
url = {https://dl.acm.org/doi/10.1145/3386253},
volume = {38},
year = {2020}
}
@article{VanderLinden2022,
abstract = {The spread of misinformation poses a considerable threat to public health and the successful management of a global pandemic. For example, studies find that exposure to misinformation can undermine vaccination uptake and compliance with public-health guidelines. As research on the science of misinformation is rapidly emerging, this conceptual Review summarizes what we know along three key dimensions of the infodemic: susceptibility, spread, and immunization. Extant research is evaluated on the questions of why (some) people are (more) susceptible to misinformation, how misinformation spreads in online social networks, and which interventions can help to boost psychological immunity to misinformation. Implications for managing the infodemic are discussed. This Review provides an overview of the psychology of misinformation, from susceptibility to spread and interventions to help boost psychological immunity.},
author = {van der Linden, Sander},
doi = {10.1038/s41591-022-01713-6},
issn = {1546-170X},
journal = {Nature Medicine 2022 28:3},
keywords = {Communication,Psychology},
number = {3},
pages = {460--467},
pmid = {35273402},
publisher = {Nature Publishing Group},
title = {{Misinformation: susceptibility, spread, and interventions to immunize the public}},
url = {https://www.nature.com/articles/s41591-022-01713-6},
volume = {28},
year = {2022}
}
@article{Ciampaglia2018,
abstract = {The deluge of online and offline misinformation is overloading the exchange of ideas upon which democracies depend. Fake news, conspiracy theories, and deceptive social bots proliferate, facilitating the manipulation of public opinion. Countering misinformation while protecting freedom of speech will require collaboration across industry, journalism, and academia. The Workshop on Digital Misinformation — held in May 2017 in conjunction with the International Conference on Web and Social Media in Montr{\'{e}}al, Qu{\'{e}}bec, Canada — was intended to foster these efforts. The meeting brought together more than 100 stakeholders from academia, media, and tech companies to discuss the research challenges implicit in building a trustworthy Web. Below we outline the main findings from the discussion.},
author = {Ciampaglia, Giovanni Luca and Mantzarlis, Alexios and Maus, Gregory and Menczer, Filippo},
doi = {10.1609/aimag.v39i1.2783},
issn = {0738-4602},
journal = {AI Magazine},
number = {1},
pages = {65},
title = {{Research Challenges of Digital Misinformation: Toward a Trustworthy Web}},
url = {https://144.208.67.177/ojs/index.php/aimagazine/article/view/2783},
volume = {39},
year = {2018}
}
@inproceedings{Jaime2022,
abstract = {With the proliferation of online misinformation, fake news detection has gained importance in the artificial intelligence community. In this paper, we propose an adversarial benchmark that tests the ability of fake news detectors to reason about real-world facts. We formulate adversarial attacks that target three aspects of "understanding": compositional semantics, lexical relations, and sensitivity to modifiers. We test our benchmark using BERT classifiers fine-tuned on the LIAR arXiv:arch-ive/1705648 and Kaggle Fake-News datasets, and show that both models fail to respond to changes in compositional and lexical meaning. Our results strengthen the need for such models to be used in conjunction with other fact checking methods.},
archivePrefix = {arXiv},
arxivId = {2201.00912},
author = {Jaime, Lorenzo and Flores, Yu and Hao, Yiding},
booktitle = {The AAAI-22 Workshop on Adversarial Machine Learning and Beyond},
doi = {10.48550/arxiv.2201.00912},
eprint = {2201.00912},
title = {{An Adversarial Benchmark for Fake News Detection Models}},
url = {https://arxiv.org/abs/2201.00912v1},
year = {2022}
}
@article{Goto_2024,
author = {Goto, Takeshi and Ono, Kensuke and Morita, Akira},
doi = {10.36227/techrxiv.171173447.70655950/v1},
journal = {techrxiv:171173447.70655950},
month = {mar},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{A Comparative Analysis of Large Language Models to Evaluate Robustness and Reliability in Adversarial Conditions}},
url = {http://dx.doi.org/10.36227/techrxiv.171173447.70655950/v1},
year = {2024}
}
@inproceedings{Iyyer2018,
abstract = {We propose syntactically controlled paraphrase networks (SCPNs) and use them to generate adversarial examples. Given a sentence and a target syntactic form (e.g., a constituency parse), SCPNs are trained to produce a paraphrase of the sentence with the desired syntax. We show it is possible to create training data for this task by first doing backtranslation at a very large scale, and then using a parser to label the syntactic transformations that naturally occur during this process. Such data allows us to train a neural encoderdecoder model with extra inputs to specify the target syntax. A combination of automated and human evaluations show that SCPNs generate paraphrases that follow their target specifications without decreasing paraphrase quality when compared to baseline (uncontrolled) paraphrase systems. Furthermore, they are more capable of generating syntactically adversarial examples that both (1) "fool" pretrained models and (2) improve the robustness of these models to syntactic variation when used to augment their training data.},
author = {Iyyer, Mohit and Wieting, John and Gimpel, Kevin and Zettlemoyer, Luke},
booktitle = {NAACL HLT 2018 - 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
doi = {10.18653/V1/N18-1170},
pages = {1875--1885},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Adversarial Example Generation with Syntactically Controlled Paraphrase Networks}},
url = {https://aclanthology.org/N18-1170},
volume = {1},
year = {2018}
}
@inproceedings{Zeng2021,
abstract = {Textual adversarial attacking has received wide and increasing attention in recent years. Various attack models have been proposed, which are enormously distinct and implemented with different programming frameworks and settings. These facts hinder quick utilization and fair comparison of attack models. In this paper, we present an open-source textual adversarial attack toolkit named OpenAttack to solve these issues. Compared with existing other textual adversarial attack toolkits, OpenAttack has its unique strengths in support for all attack types, multilinguality, and parallel processing. Currently, OpenAttack includes 15 typical attack models that cover all attack types. Its highly inclusive modular design not only supports quick utilization of existing attack models, but also enables great flexibility and extensibility. OpenAttack has broad uses including comparing and evaluating attack models, measuring robustness of a model, assisting in developing new attack models, and adversarial training. Source code and documentation can be obtained at https://github.com/thunlp/ OpenAttack.},
archivePrefix = {arXiv},
arxivId = {2009.09191},
author = {Zeng, Guoyang and Qi, Fanchao and Zhou, Qianrui and Zhang, Tingji and Ma, Zixian and Hou, Bairu and Zang, Yuan and Liu, Zhiyuan and Sun, Maosong},
booktitle = {ACL-IJCNLP 2021 - 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Proceedings of the System Demonstrations},
doi = {10.18653/V1/2021.ACL-DEMO.43},
eprint = {2009.09191},
isbn = {9781954085565},
pages = {363--371},
publisher = {Association for Computational Linguistics (ACL)},
title = {{OpenAttack: An Open-source Textual Adversarial Attack Toolkit}},
url = {https://aclanthology.org/2021.acl-demo.43},
year = {2021}
}
@inproceedings{Gao2018a,
abstract = {Although various techniques have been proposed to generate adversarial samples for white-box attacks on text, little attention has been paid to a black-box attack, which is a more realistic scenario. In this paper, we present a novel algorithm, DeepWordBug, to effectively generate small text perturbations in a black-box setting that forces a deep-learning classifier to misclassify a text input. We develop novel scoring strategies to find the most important words to modify such that the deep classifier makes a wrong prediction. Simple character-level transformations are applied to the highest-ranked words in order to minimize the edit distance of the perturbation. We evaluated DeepWordBug on two real-world text datasets: Enron spam emails and IMDB movie reviews. Our experimental results indicate that DeepWordBug can reduce the classification accuracy from 99{\%} to 40{\%} on Enron and from 87{\%} to 26{\%} on IMDB. Our results strongly demonstrate that the generated adversarial sequences from a deep-learning model can similarly evade other deep models.},
archivePrefix = {arXiv},
arxivId = {1801.04354},
author = {Gao, Ji and Lanchantin, Jack and Soffa, Mary Lou and Qi, Yanjun},
booktitle = {Proceedings - 2018 IEEE Symposium on Security and Privacy Workshops, SPW 2018},
doi = {10.1109/SPW.2018.00016},
eprint = {1801.04354},
isbn = {9780769563497},
keywords = {Adversarial samples,Black box attack,Deep learning,Misclassification,Text classification,Word embedding},
pages = {50--56},
publisher = {IEEE},
title = {{Black-box generation of adversarial text sequences to evade deep learning classifiers}},
year = {2018}
}
@inproceedings{Ettinger2017,
abstract = {This paper presents a summary of the first Workshop on Building Linguistically Generalizable Natural Language Processing Systems, and the associated Build It Break It, The Language Edition shared task. The goal of this workshop was to bring together researchers in NLP and linguistics with a shared task aimed at testing the generalizability of NLP systems beyond the distributions of their training data. We describe the motivation, setup, and participation of the shared task, provide discussion of some highlighted results, and discuss lessons learned.},
archivePrefix = {arXiv},
arxivId = {1711.01505},
author = {Ettinger, Allyson and Rao, Sudha and III, Hal Daum{\'{e}} and Bender, Emily M.},
booktitle = {Proceedings of the First Workshop on Building Linguistically Generalizable NLP Systems},
doi = {10.18653/V1/W17-5401},
eprint = {1711.01505},
pages = {1--10},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Towards Linguistically Generalizable NLP Systems: A Workshop and Shared Task}},
url = {https://aclanthology.org/W17-5401},
year = {2017}
}
@article{Church2022,
abstract = {Many papers are chasing state-of-the-art (SOTA) numbers, and more will do so in the future. SOTA-chasing comes with many costs. SOTA-chasing squeezes out more promising opportunities such as coopetition and interdisciplinary collaboration. In addition, there is a risk that too much SOTA-chasing could lead to claims of superhuman performance, unrealistic expectations, and the next AI winter. Two root causes for SOTA-chasing will be discussed: (1) lack of leadership and (2) iffy reviewing processes. SOTA-chasing may be similar to the replication crisis in the scientific literature. The replication crisis is yet another example, like evaluation, of over-confidence in accepted practices and the scientific method, even when such practices lead to absurd consequences.},
author = {Church, Kenneth Ward and Kordoni, Valia},
doi = {10.1017/S1351324922000043},
issn = {1351-3249},
journal = {Natural Language Engineering},
keywords = {Benchmarks,Evaluation,Leaderboards,Leadership,Replication crisis,Reviewing,Root causes,State-of-the-art},
month = {mar},
number = {2},
pages = {249--269},
publisher = {Cambridge University Press},
title = {{Emerging Trends: SOTA-Chasing}},
url = {https://www.cambridge.org/core/journals/natural-language-engineering/article/emerging-trends-sotachasing/5E9F9F796159040973053C52C443C1D6},
volume = {28},
year = {2022}
}
@inproceedings{Przybya2020,
abstract = {In this study we aim to explore automatic methods that can detect online documents of low credibility, especially fake news, based on the style they are written in. We show that general-purpose text classifiers, despite seemingly good performance when evaluated simplistically, in fact overfit to sources of documents in training data. In order to achieve a truly style-based prediction, we gather a corpus of 103,219 documents from 223 online sources labelled by media experts, devise realistic evaluation scenarios and design two new classifiers: a neural network and a model based on stylometric features. The evaluation shows that the proposed classifiers maintain high accuracy in case of documents on previously unseen topics (e.g. new events) and from previously unseen sources (e.g. emerging news websites). An analysis of the stylometric model indicates it indeed focuses on sensational and affective vocabulary, known to be typical for fake news.},
address = {New York, USA},
author = {Przyby{\l}a, Piotr},
booktitle = {Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20)},
doi = {10.1609/aaai.v34i01.5386},
issn = {2159-5399},
pages = {490--497},
publisher = {AAAI Press},
title = {{Capturing the Style of Fake News}},
url = {https://aaai.org/ojs/index.php/AAAI/article/view/5386},
volume = {34},
year = {2020}
}
@techreport{Tucker2018,
author = {Tucker, Joshua A. and Guess, Andrew and Barber{\'{a}}, Pablo and Vaccari, Cristian and Siegel, Alexandra and Sanovich, Sergey and Stukal, Denis and Nyhan, Brendan},
institution = {Hewlett Foundation},
title = {{Social Media, Political Polarization, and Political Disinformation: A Review of the Scientific Literature}},
url = {https://hewlett.org/library/social-media-political-polarization-political-disinformation-review-scientific-literature/},
year = {2018}
}
@inproceedings{Devlin2018,
abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4{\%} (7.6{\%} absolute improvement), MultiNLI accuracy to 86.7 (5.6{\%} absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5{\%} absolute improvement), outperforming human performance by 2.0{\%}.},
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
eprint = {1810.04805},
pages = {4171--4186},
publisher = {Association for Computational Linguistics},
title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
url = {http://arxiv.org/abs/1810.04805},
year = {2018}
}
@article{Zhang2020b,
abstract = {With the development of high computational devices, deep neural networks (DNNs), in recent years, have gained significant popularity in many Artificial Intelligence (AI) applications. However, prev...},
author = {Zhang, Wei Emma and Sheng, Quan Z. and Alhazmi, Ahoud and Li, Chenliang},
doi = {10.1145/3374217},
issn = {21576912},
journal = {ACM Transactions on Intelligent Systems and Technology (TIST)},
keywords = {Deep neural networks,adversarial examples,natural language processing,textual data},
number = {3},
publisher = {ACM PUB27 New York, NY, USA},
title = {{Adversarial Attacks on Deep-learning Models in Natural Language Processing}},
url = {https://dl.acm.org/doi/10.1145/3374217},
volume = {11},
year = {2020}
}
@article{Koenders2021,
abstract = {As the spread of false information on the internet has increased dramatically in recent years, more and more attention is being paid to automated fake news detection. Some fake news detection methods are already quite successful. Nevertheless, there are still many vulnerabilities in the detection algorithms. The reason for this is that fake news publishers can structure and formulate their texts in such a way that a detection algorithm does not expose this text as fake news. This paper shows that it is possible to automatically attack state-of-the-art models that have been trained to detect Fake News, making these vulnerable. For this purpose, corresponding models were first trained based on a dataset. Then, using Text-Attack, an attempt was made to manipulate the trained models in such a way that previously correctly identified fake news was classified as true news. The results show that it is possible to automatically bypass Fake News detection mechanisms, leading to implications concerning existing policy initiatives.},
archivePrefix = {arXiv},
arxivId = {2107.07970},
author = {Koenders, Camille and Filla, Johannes and Schneider, Nicolai and Woloszyn, Vinicius},
doi = {10.48550/arxiv.2107.07970},
eprint = {2107.07970},
journal = {arXiv:2107.07970},
title = {{How Vulnerable Are Automatic Fake News Detection Methods to Adversarial Attacks?}},
url = {https://arxiv.org/abs/2107.07970v1},
year = {2021}
}
@article{Cresci2022,
abstract = {Adversarial examples are inputs to a machine learning system that result in an incorrect output from that system. Attacks launched through this type of input can cause severe consequences: for example, in the field of image recognition, a stop signal can be misclassified as a speed limit indication. However, adversarial examples also represent the fuel for a flurry of research directions in different domains and applications. Here, we give an overview of how they can be profitably exploited as powerful tools to build stronger learning models, capable of better-withstanding attacks, for two crucial tasks: fake news and social bot detection.},
author = {Cresci, Stefano and Petrocchi, Marinella and Spognardi, Angelo and Tognazzi, Stefano},
doi = {10.1109/MIC.2021.3130380},
issn = {19410131},
journal = {IEEE Internet Computing},
keywords = {Alan Turing asked this question to his audience: C,I.2.4 Knowledge representation formalisms and meth,and in his paper 'Computing Machinery and Intellig,as measured by P,if its performance at tasks in T,improves with experience E' [1],whose traditional definition is as follows: 'A com},
number = {2},
pages = {47--52},
publisher = {IEEE},
title = {{Adversarial Machine Learning for Protecting Against Online Manipulation}},
volume = {26},
year = {2022}
}

@misc{Paul2022,
author = {Paul, Katie and Dang, Sheila},
booktitle = {Reuters},
title = {{Exclusive: Twitter leans on automation to moderate content as harmful speech surges}},
year = {2022},
url={https://www.reuters.com/technology/twitter-exec-says-moving-fast-moderation-harmful-content-surges-2022-12-03/}
}

@techreport{Graham2023,
address = {Brisbane, Australia},
author = {Graham, Timothy and FitzGerald, Katherine M.},
institution = {Digital Media Research Centre, Queensland University of Technology},
title = {{Bots, Fake News and Election Conspiracies: Disinformation During the Republican Primary Debate and the Trump Interview}},
url = {https://eprints.qut.edu.au/242533/},
year = {2023}
}
