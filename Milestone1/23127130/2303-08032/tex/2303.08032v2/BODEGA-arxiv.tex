\documentclass[preprint,12pt]{elsarticle}


\usepackage[hyphens]{url}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{atbegshi}% http://ctan.org/pkg/atbegshi
\AtBeginDocument{\AtBeginShipoutNext{\AtBeginShipoutDiscard}}

\journal{a journal.}

\begin{document}

\begin{frontmatter}

\title{Verifying the Robustness of Automatic Credibility Assessment}

\author[upf,ipi]{Piotr Przyby{\l}a\fnref{corr}}
\ead{piotr.przybyla@upf.edu}

\author[upf]{Alexander Shvets}
\ead{alexander.shvets@upf.edu}

\author[upf]{Horacio Saggion}
\ead{horacio.saggion@upf.edu}

\affiliation[upf]{organization={Universitat Pompeu Fabra},%Department and Organization
            addressline={Tànger building}, 
            city={Barcelona},
            postcode={08018}, 
%            state={},
            country={Spain}}

\affiliation[ipi]{organization={Institute of Computer Science, Polish Academy of Sciences},%Department and Organization
            addressline={ul. Jana Kazimierza 5}, 
            city={Warsaw},
            postcode={01-248}, 
%            state={},
            country={Poland}}

\fntext[corr]{Corresponding author.}
            

\begin{abstract}
Text classification methods have been widely investigated as a way to detect content of low credibility: fake news, social media bots, propaganda, etc. Quite accurate models (likely based on deep neural networks) help in moderating public electronic platforms and often cause content creators to face rejection of their submissions or removal of already published texts. Having the incentive to evade further detection, content creators try to come up with a slightly modified version of the text (known as an attack with an adversarial example) that exploit the weaknesses of classifiers and result in a different output. Here we systematically test the robustness of popular text classifiers against available attacking techniques and discover that, indeed, in some cases insignificant changes in input text can mislead the models. We also introduce BODEGA: a benchmark for testing both victim models and attack methods on four misinformation detection tasks in an evaluation framework designed to simulate real use-cases of content moderation. Finally, we manually analyse a subset adversarial examples and check what kinds of modifications are used in successful attacks. The BODEGA code and data is openly shared in hope of enhancing the comparability and replicability of further research in this area
\end{abstract}.


%Graphical abstract
%\begin{graphicalabstract}
%\includegraphics{grabs}
%\end{graphicalabstract}

%%Research highlights
%\begin{highlights}
%\item Research highlight 1
%\item Research highlight 2
%\end{highlights}

\begin{keyword}
adversarial examples \sep credibility assessment \sep robustness \sep misinformation \sep benchmark
\end{keyword}

\end{frontmatter}


\section{Introduction}

\textit{Misinformation} is one of the most commonly recognised problems in modern digital societies \cite{Lewandowsky2017,Akers2018,Tucker2018}. Under this term, we understand the publication and spreading of information that is not \textit{credible}, including fake news, manipulative propaganda, social media bots activity, rumours, hyperpartisan and biased journalism. While these problems differ in many aspects, what they have in common is non-credible (fake or malicious) content masquerading as credible: fake news as reliable news, bots as genuine users, falsehoods as facts, etc. \citep{Tucker2018,VanderLinden2022}.

Given that each type of content is present on the Internet in abundance, the assessment of credibility has fast been recognised as a task for machine learning (ML) or wider artificial intelligence (AI) solutions \citep{Ciampaglia2018}. It is common practice among major platforms with user-generated content to use such models for moderation, either as preliminary filtering before human judgement \citep{Singhal2022}, or as an automated detection system\footnote{\url{https://support.google.com/youtube/thread/192701791/updates-on-comment-spam-abuse?hl=en}}\footnote{\url{https://www.reuters.com/technology/twitter-exec-says-moving-fast-moderation-harmful-content-surges-2022-12-03/}}.

Are the state-of-the-art techniques of ML and, in particular, Natural Language Processing (NLP), up for a task of great importance to society? The standard analysis of model implementation with traditional accuracy metrics does not suffice here as it neglects how possible it is to systematically come up with variants of malicious text, known as \textit{adversarial examples} (AEs), that fulfil the original goal but evade detection \citep{Carter2021}. A realistic analysis in such a use case has to take into account an \textit{adversary}, i.e. the author of the non-credible content, who has both motivation and opportunity to experiment with the filtering system to find out its vulnerabilities.

Looking for such weaknesses via designing AE, to assess the \textit{robustness} of an investigated model, is a well-established problem in ML. However, its application to misinformation-oriented NLP tasks is relatively rare, despite the suitability of the adversarial scenario in this domain. Moreover, similarly to the situation in other domains, the adversarial attack performance depends on a variety of factors, such as the data used for training and testing, the attack goal, disturbance constraints, attacked models and evaluation measures. As a result, it’s very hard to choose the strongest attackers among many solutions that were evaluated in different environments.

In order to fill the need for reproducibility in this field, we have created BODEGA (Benchmark fOr aDversarial Example Generation in credibility Assessment), intended as a common framework for comparing AE generation solutions to serve as a foundation for the creation of ``better-defended'' content credibility classifiers. Then, we have used it to assess the robustness of the popular text classifiers by simulating attacks using various AE generation solutions.

Thus, our contributions include the following:
\begin{enumerate}
    \item The BODEGA evaluation framework, consisting of elements simulating the misinformation detection scenario:
    \begin{enumerate}
    \item A collection of four NLP tasks from the domain of misinformation, cast as binary text classification problems (section \ref{sec:tasks}),
    \item A training and test dataset for each of the above tasks,
    \item Two attack scenarios, specifying what information is available to an adversary and what is their goal (section \ref{sec:attackscenario}),
    \item An evaluation procedure, involving a success measure designed specifically for this scenario (section \ref{sec:evaluation}).
    \end{enumerate}
    \item An analysis of the robustness of common text classification solutions when attacked using various methods:
    \begin{enumerate}
    \item a systematic comparison using the automatic measures (section \ref{sec:results}),
    \item a manual analysis of the most promising cases, revealing the kinds of modifications used by the AE solutions to confuse the victim models (section \ref{sec:manual})
    \end{enumerate}
\end{enumerate}
BODEGA, based on the \textit{OpenAttack} framework and existing misinformation datasets, is openly available for download and use.\footnote{\url{https://github.com/piotrmp/BODEGA}}


\section{Related work}
\label{sec:relatedwork}

\subsection{Adversarial examples in NLP}

Searching for adversarial examples can be seen within wider efforts to investigate the \textit{robustness} of ML models, i.e. their ability to maintain good performance when confronted with data instances unlike those seen in training: anomalous, rare, adversarial or edge cases. This effort is especially important for deep learning models, which are not inherently interpretable, making it harder to predict their behaviour at the design stage. The seminal work on the subject by Szegedy et al. \cite{Szegedy2013} demonstrated the low robustness of neural networks used to recognise images. The adversarial examples were prepared by adding specially prepared noise to the original image, which forced the change of the classifier's decision even though the changes were barely perceptible visually and the original label remained valid.

Given the prevalence of neural networks in language processing, a lot of work has been done on investigating AEs in the context of NLP tasks \citep{Zhang2020b}, but the transition from the domain of images to text is far from trivial. Firstly, it can be a challenge to make changes small enough to the text, such that the original label remains applicable -- there is no equivalent of \textit{imperceptible noise} in text. The problem has been approached on several levels: of characters, making alterations that will likely remain unnoticed by a reader \citep{Gao2018a,eger-etal-2019-text}; of words, replaced while preserving the meaning by relying on thesauri \citep{ren-etal-2019-generating} or language models \citep{DBLP:conf/aaai/JinJZS20,li-etal-2020-bert-attack} and, finally, of sentences, by employing paraphrasing techniques \citep{Iyyer2018,ribeiro-etal-2018-semantically}. Secondly, the discrete nature of text means that methods based on exploring a feature space (e.g. guided by a gradient) might suggest points that do not correspond to real text. Most of the approaches solve this by only considering modifications on the text level, but there are other solutions, for example finding the optimal location in the embedding space followed by choosing its nearest neighbour that is a real word \citep{Gong2018}, or generating text samples from a distribution described by continuous parameters \citep{Guo2021}.

Apart from AE generation, a public-facing text classifier may be subject to many other types of attacks, including manipulations to output desired value when a trigger word is used \citep{Bagdasaryan2022} or perform an arbitrary task chosen by the attacker \citep{Neekhara2019}. Finally, verifying the trustworthiness of a model aimed for deployment should also take into account undesirable behaviours exhibited without adversarial actions, e.g. its response to modification of protected attributes, such as gender, in the input \citep{Srivastava2023}.

\subsection{Robustness of credibility assessment}

The understanding that some deployment scenarios of NLP models justify expecting adversary actions predates the popularisation of deep neural networks, with the first considerations based on spam detection \citep{Dalvi2004}. The work that followed was varied in the explored tasks, attack scenarios and approaches.

The first attempts to experimentally verify the robustness of misinformation detection were based on simple manual changes \citep{Zhou2019b}. The approach of targeting a specific weakness and manually designing rules to exploit it has been particularly popular in attacking fact-checking solutions \citep{Thorne2019,Hidey2020}.

In the domain of social media analysis, Le et al. \cite{Le2020} have examined the possibility of changing the output of a text credibility classifier by concatenating it with adversarial text, e.g. added as a comment below the main text. The main solution was working in the white-box scenario, with the black-box variant made possible by training a surrogate classifier on the original training data\footnote{We explain white- and black-box scenarios in Section \ref{sec:attackscenario}.}. It has also been shown that social media bot detection using AdaBoost is vulnerable to adversarial examples \cite{Kantartopoulos2020}. Adversarial scenarios have also been considered with user-generated content classification for other tasks, e.g. hate speech or satire \citep{Alsmadi2022}.

Fake news corpora have been used to verify the effectiveness of AE generation techniques, e.g. in the study introducing TextFooler \cite{DBLP:conf/aaai/JinJZS20}. Interestingly, the study has shown that the classifier for fake news was significantly more resistant to attacks compared to those for other tasks, i.e. topic detection or sentiment analysis. This task also encouraged exploration of vulnerability to manually crafted modifications of input text \citep{Jaime2022}. In general, the fake news classification task has been a common subject of robustness assessment, involving both neural networks \citep{Ali2021a,Koenders2021} and non-neural classifiers \citep{Brown2020b,Smith2021}.

To sum up, while there have been several experiments examining the vulnerability of misinformation detection to adversarial attacks, virtually each of them has used a different dataset, a different classifier and a different attack technique, making it hard to draw conclusions and make comparisons. Our study is the first to analyse credibility assessment tasks and systematically evaluate their vulnerability to various attacks.

\subsection{Resources for adversarial examples}

The efforts of finding AEs are relatively new for NLP, and there exist multiple approaches to evaluation procedures and datasets. The variety of studies for the misinformation tasks is reflective of the whole domain -- see the list of datasets used for evaluation provided by Zhang et al. \cite{Zhang2020b}. Hopefully, as the field matures, some standard practice measures will emerge, facilitating the comparison of approaches. We see BODEGA as a step in this direction.

Two types of existing efforts to bring the community together are worth mentioning. Firstly, some related shared tasks have been organised. The \textit{Build It Break It, The Language Edition} task \citep{Ettinger2017} covered sentiment analysis and question answering, addressed by both 'builders' (building solutions) and 'breakers' (finding adversarial examples). The low number of breaker teams -- four for sentiment analysis and one for question answering -- makes it difficult to draw conclusions, but the majority of deployed techniques involved manually inserted changes targeting suspected weaknesses of the classifiers. The FEVER 2.0 shared task \citep{Thorne19FEVER2}, focusing on fact checking, had a 'Build-It' and 'Break-It' phases with a similar setup, except the adversarial examples were generated and annotated from scratch, with no correspondence to existing true examples, as in \textit{Build It Break It} or BODEGA. The three valid submissions concentrated around manual introduction of issues known as challenging for automated fact checking, including multi-hop or temporal reasoning, ambiguous entities, arithmetic calculations and vague statements.

Secondly, two software packages were released to aid evaluation: \textit{TextAttack} \citep{morris-etal-2020-textattack} and \textit{OpenAttack} \citep{Zeng2021}. They both provide a software skeleton for setting up the attack and implementations of several AE generation methods. A user can add the implementation of their own victims and attackers and perform the evaluation. BODEGA code has been developed based on OpenAttack by providing access to misinformation-specific datasets, classifiers and evaluation measures.

\section{General schema}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=1.0\linewidth]{fig/BODEGA_arch.pdf} 
\end{center}
\caption{An overview of the evaluation of an adversarial attack using BODEGA. See description in the text.}
\label{fig:architecture}
\end{figure*}

Adversarial example generation is a task aimed at testing the robustness of ML models, known as \textit{victims} in this context. The goal is to find small modifications to the input data that will change the model output even though the original meaning is preserved and the correct response remains the same. If such changed instances, known as adversarial examples, could be systematically found, it means the victim classifier is vulnerable to the attack and not robust.

In the context of classification, this setup (illustrated in Figure \ref{fig:architecture}) could be formalised through the following:
\begin{itemize}
    \item A training set $X_{train}$ and an attack set $X_{attack}$, each containing instances $(x_i, y_i)$, coupling the $i$-th instance features $x_i$ with its true class $y_i$,
    \item A victim model $f$, predicting a class label $\hat{y_i}$ based on instance features: $\hat{y_i}=f(x_i)$,
    \item A modification function $m$, turning $x_i$ into an adversarial example $x^*_i=m(x_i)$.
\end{itemize}
Throughout this study, we use $y_i=1$ (positive class) to denote non-credible information and $0$ for credible content.

The goal of the attacker is to come up with the $m$ function. This process typically involves generating numerous variations of $x_i$ and querying the model's response to them until the best candidate is selected. An evaluation procedure assesses the success of the attack on the set $X_{attack}$ by comparing $x_i$ to $x^*_i$ (which should be maximally similar) and $f(x_i)$ to $f(x^*_i)$ (which should be maximally different).

For example, consider a scenario in which a foreign actor aims to incite panic in a specific country by spreading false information about a hazardous fallout, under alarming headings such as $x_i=$\textit{Radioactive dust approaching from the Mediterranean!}. If analogous scenarios were explored in the past and are included in $X_{train}$, the classifier $f$ will correctly recognise this misinformation by returning $\hat{y_i}=f(x_i)=1$. But the adversary might employ a modification function $m$ based on simple paraphrasing and come up with an adversarial example $x^*_i=$\textit{Radioactive dust \textbf{coming} from the Mediterranean!}. If the classifier is not robust and returns $f(x^*_i)=0$ for this variant, the attacker succeeds.


\section{Tasks}
\label{sec:tasks}

In BODEGA we include four misinformation detection tasks:
\begin{itemize}
    \item Style-based news bias assessment (HN),
    \item Propaganda detection (PR),
    \item Fact checking (FC),
    \item Rumour detection (RD).
\end{itemize}

For each of these problems, we rely on an already established dataset with credibility labels provided by expert annotators. The tasks are all presented as text classification.

Whenever data split is released with a corpus, the training subset is included as $X_{train}$ -- otherwise we perform a random split. In order to enable the evaluation of AE generation solutions that carry a high computational cost, we define the $X_{attack}$ subset which is restricted to around 400 instances taken from the test set. The rest of the cases in the original test set are left out for future use as a development subset. Table \ref{tab:dataset} summarises the data obtained.

\begin{table}
    \centering
    \begin{tabular}{rrrrr}
    \hline
    \textbf{Task} & \textbf{Training} & \textbf{Attack} & \textbf{Dev.} & \textbf{Positive \%} \\
    \hline
        HN & 60,235 & 400 & 3,600 & 50.00\% \\
        PR & 12,675 & 416 & 3,320 & 29.42\% \\
        FC & 172,763 & 405 & 19,010 & 51.27\% \\
        RD & 8,694 & 415 & 2,070 & 32.68\% \\
    \hline
    \end{tabular}
    \caption{Four datasets used in BODEGA, described by the task (see descriptions in text), number of instances in training, attack and development subsets, and an overall percentage of positive (non-credible) class.}
    \label{tab:dataset}
\end{table}

In the following subsections, we outline the motivation, origin and data processing within each of the tasks. Table \ref{tab:tasks} includes some examples of the credible and non-credible content in each task.

\begin{table*}
\scriptsize
    \centering
    \begin{tabular}{p{0.10\linewidth}|p{0.45\linewidth}|p{0.45\linewidth}|}
    \textbf{Task} & \textbf{Credible example} & \textbf{Non-credible example}\\
    \hline
        HN & Syria blamed for missed deadline on chemical arsenal\newline U.S. officials conceded that a Tuesday deadline for ridding Syria of hundreds of tons of liquid poisons would not be met, citing stalled progress in transporting the chemicals across war-ravaged countryside to ships that will carry them out of the region. But the officials insisted that the overall effort to destroy President Bashar Assad’s chemical arsenal was on track. "We continue to make progress, which has been the important part," State Department spokeswoman Marie Harf told reporters. "It was always an ambitious timeline, but we are still operating on the June 30th timeline for the complete destruction." (...) & Fox's Cavuto And Stein Try To Conflate 'Grubergate' With Vietnam And The Pentagon Papers \newline Over at Faux "news" this Tuesday, rather than focus on the newly released Senate torture report, it's been all Jonathan Gruber and "Grubergate" all the time and wall to wall coverage of another one of Darrell Issa's Obamacare witch hunts, otherwise known as a House Oversight Committee hearing. \newline As soon as I heard the hearing was scheduled I knew that it meant things were going to get ugly over at Fox, but not even in my wildest imagination could I have come up with this big giant turd that Neil Cavuto and his buddy Ben Stein managed to toss against the wall to attack Obamacare and Gruber. (...) \\ \hline
        PR & Leading Democratic senators like Robert Menendez, Ben Cardin and Chuck Schumer, who opposed Obama’s Iran deal may now feel that as opponents of the Trump administration, they are required to oppose any change to the Iran Nuclear Agreement Review Act. & What outcome would justify another U.S. war in a region where all the previous wars in this century have left us bleeding, bankrupt, divided and disillusioned? \\ \hline
        FC & \textit{\underline{Cersei Lannister.} She subsequently appeared in A Clash of Kings (1998) and A Storm of Swords (2000). \underline{A Clash of Kings.} A Clash of Kings is the second novel in A Song of Ice and Fire, an epic fantasy series by American author George R. R. Martin expected to consist of seven volumes. \newline $\rightarrow$ Cersei Lannister appears in a series that was written by an author from the United States.} & \textit{\underline{David Bowie.} During his lifetime, his record sales, estimated at 140 million worldwide, made him one of the world's best-selling music artists. \newline$\rightarrow$	David Bowie only sold records in Jamaica.} \\ \hline
        RD & BREAKING: Three gunmen involved in attack on Charlie Hebdo magazine, French Interior Minister Bernard Cazeneuve says. http://t.co/ak9mTVfJdR \newline @cnni the Islamic leaders should do something about the image of islam by speaking out against the terrorists \newline @cnni expel muslims from european soil and destroy all the mosques. \newline @cnni it's not the religion.  But how the people  interpret the writings and that's what causes them to do bad things. \newline @cnni terrorism needs concerted efforts from every citizen to fight it,religion is going beyond boundaries if it can cause terror attacks & Reports: \#CharlieHebdo suspects killed http://t.co/rsl4203bcQ \newline Damn, this is like a movie RT @HuffingtonPost Reports: \#CharlieHebdo suspects killed http://t.co/zCuZD1cure \newline ?@HuffingtonPost: Reports: \#CharlieHebdo suspects killed http://t.co/mWCSjh3CkH? superb simultaneous response by the French tactics unit. \newline @HuffingtonPost great news! No trial, no taxpayer money spent to support them. \newline @HuffingtonPost Good news !!! Alah Akbar !!    \newline @HuffingtonPost damnit!!! That's what those fuckers wanted!! Now they will be hailed as martyrs\textbf{....} \newline \textbf{@HuffingtonPost} Can you confirm the reports that those suspects were killed by French police? \newline (...) \\ \hline
    \hline
    \end{tabular}
    \caption{Examples of credible and non-credible content in each of the tasks: style-based news bias assessment (HN), propaganda detection (PR), fact checking (FC) and rumour detection (RD). See main text for the data sources.}
    \label{tab:tasks}
\end{table*}


\subsection{HN: Style-based news bias assessment}

Solutions for news credibility assessment, sometimes equated with \textit{fake news} detection, usually rely on one of three factors: (1) writing style \citep{Horne2017,Przybya2020}, (2) veracity of included claims \citep{Vlachos2014,Graves2018} or (3) context of social and traditional media  \citep{Shu2019,Liu2020a}.

In this task, we focus on the writing style. This means a whole news article is provided to a classifier, which has no ability to check facts against external sources, but has been trained on enough articles to recognise stylistic cues. The training data include numerous articles coming from sources with known credibility, allowing one to learn writing styles typical for credible and non-credible outlets.

In BODEGA, we employ a corpus of news articles \citep{potthast-etal-2018-stylometric} used for the task of \textit{Hyperpartisan News Detection} at SemEval-2019 \citep{kiesel-etal-2019-semeval}. The credibility was assigned based on the overall bias of the source, assessed by journalists from \textit{BuzzFeed} and \textit{MediaBiasFactCheck.com}\footnote{\url{https://zenodo.org/record/1489920}}. We use 1/10th of the training set (60,235 articles) and assign label $1$ (non-credible) to articles from sources annotated as hyperpartisan, both right- and left-wing.

See the first row of table \ref{tab:tasks} for examples: credible from \textit{Albuquerque journal}\footnote{\url{https://abqjournal.com/328734/syria-blamed-for-missed-deadline-on-weapons.html}} and non-credible from \textit{Crooks and Liars}\footnote{\url{http://crooksandliars.com/2014/12/foxs-cavuto-and-stein-try-conflate}}.

\subsection{PR: Propaganda detection}

The task of propaganda detection involves detecting text passages, whose author tries to influence the reader by means other than objective presentation of the facts, for example by appealing to emotions or exploiting common fallacies \citep{Smith1989a}. The usage of propaganda techniques does not necessarily imply falsehood, but in the context of journalism is associated with manipulative, dishonest and hyperpartisan writing. In BODEGA, we use the corpus accompanying SemEval 2020 Task 11 (\textit{Detection of Propaganda Techniques in News Articles}), with 14 propaganda techniques annotated in 371 newspapers articles by professional annotators \cite{Martino2020a}.

Propaganda detection is a fine-grained task, with SemEval data annotated on the token level, akin to a Named Entity Recognition (NER) task. In order to cast it as a text classification problem as others here, we split the text on sentence level and assign target label equal 1 to sentences overlapping with any propaganda instances and 0 to the rest. Because only the training subset is made publicly available\footnote{\url{https://zenodo.org/record/3952415}}, we randomly extract 20\% of documents for attack and development subsets.

See the second row of table \ref{tab:tasks} for examples -- the credible fragment with no propaganda technique and the non-credible, annotated as including flag-waving.

\subsection{FC: Fact checking}

Fact checking is the most advanced way human experts can verify credibility of a given text: by assessing the veracity of the claims it includes with respect to a knowledge base (drawing from memory, reliable sources and common sense). Implementing this workflow in AI systems as computational fact checking \citep{Graves2018} is a promising direction for credibility assessment. However, it involves many challenges -- choosing check-worthy statements \citep{Nakov2022}, finding reliable sources \citep{9891941}, extracting relevant passages \citep{karpukhin-etal-2020-dense} etc. Here we focus on the claim verification stage. The input of the task is a pair of texts – target claim and relevant evidence – and the output label indicates whether the evidence supports the claim or refutes it. It essentially is Natural Language Inference (NLI) \citep{MacCartney2009} in the domain of encyclopaedic knowledge and newsworthy events.

We use the data\footnote{\url{https://fever.ai/dataset/fever.html}} from FEVER shared task \citep{Thorne2018}, aimed to evaluate fact-checking solutions through a manually created set of evidence-claim pairs. Each pair connects a one-sentence claim with a set of sentences from Wikipedia articles, including a label of SUPPORTS (the evidence justifies the claim), REFUTES (the evidence demonstrates the claim to be false) or NOT ENOUGH INFO (the evidence is not sufficient to verify the claim). For the purpose of BODEGA, we take the claims from the first two categories\footnote{NOT ENOUGH INFO was excluded to cast the task as binary classification, in line with the other ones.}, concatenating all the evidence text\footnote{Including the titles, which are often an essential part of the context in case of encyclopaedic articles.}. The labels for the test set are not openly available, so we use the development set in this role.

See the examples in the third row of table \ref{tab:tasks}: the credible instance, where  combined evidence from two articles (titles underlined) supports the claim (after the arrow); and non-credible one, where the evidence refutes the claim.

\subsection{RD: Rumour detection}

A rumour is an information spreading between people despite not having a reliable source. In the online misinformation context, the term is used to refer to content shared between users of social media that comes from an unreliable origin, e.g. an anonymous account. Not every rumour is untrue as some of them can be later confirmed by established sources. Rumours can be detected by a variety of signals \citep{Al-Sarem2019}, but here we focus on the textual content of the original post and follow-ups from other social media users.

In BODEGA we use the Augmented dataset of rumours and non-rumours for rumour detection \citep{Han2019}, created from Twitter threads relevant to six real-world events (2013 Boston marathon bombings, 2014 Ottawa shooting, 2014 Sydney siege, 2015 Charlie Hebdo Attack, 2014 Ferguson unrest, 2015 Germanwings plane crash). The authors of the dataset started with the core threads annotated manually as rumours and non-rumours, then automatically augmented them with other threads based on textual similarity. We followed this by converting each thread to a flat feed of concatenated text fragments, including the initial post and subsequent responses. We set aside one of the events (Charlie Hebdo attack) for attack and development subsets, while others are included in the training subset.

See the last row of table \ref{tab:tasks} for examples, both regarding the \textit{Charlie Hebdo} shooting, but only the credible one is based on information from a credible source.
    
\section{Attack scenario}
\label{sec:attackscenario}

The adversarial attack scenarios are often classified according to what information is available to the attacker. The \textit{black-box} scenarios assume that no information is given on the inner workings of the targeted model and only system outputs for a given input can be observed. In \textit{white-box} scenarios, the model is fully accessible, allowing methods for generating AEs to be precisely tuned to the model weights, mainly gradient-based \citep{Zhang2020b}.

We argue neither of these scenarios is realistic in the practical misinformation detection setting, e.g. a content filter deployed in a social network. We cannot assume a model is available to the attacker since such information is usually not shared publicly; moreover, the model likely gets updated often to keep up with the current topics. On the other hand, the black-box scenario is too restrictive, as some information about the deployed model can be accessed or inferred. Also, once a certain design approach is popularised as the best performing in the NLP community, it tends to be applied to very many, if not most, solutions to related problems -- this is especially noticeable in case of large language models, such as BERT \cite{Devlin2018} or GPT \cite{GPT2} and their successors.

For these reasons, in BODEGA we use the \textit{grey-box} approach. The following information is considered available to an attacker preparing AEs:
\begin{itemize}
    \item A ``hidden'' classifier $f$ that for any arbitrary input returns $f(x) \in \{0, 1\}$ and a likelihood score $s_f(x)$, i.e. a numerical representation on how likely a given example $x$ is to be assigned a positive class. This information is more helpful to attackers than only $f(x)$, which is typically set by applying a threshold $t_f$, e.g. $f(x) = 1 \iff s_f(x)> t_f$.
    \item The general description of an architecture of classifier $f$, e.g. ‘a BERT encoder followed by a dense layer and softmax normalisation’.
    \item The training $X_{train}$, the development $X_{dev}$, and the evaluation $X_{attack}$ subsets.
\end{itemize}
This setup allows users of BODEGA to exploit weaknesses of classifiers without using the complete knowledge of the model, while maintaining some resemblance of practical scenarios.

Another choice that needs to be made concerns the goal of the attacker. Generally, adversarial actions are divided into \textit{untargeted} attacks, where any change of the victim's predictions is considered a success and \textit{targeted} attacks, which seek to obtain a specific response, aligned with the attacker's goals \citep{Zhang2020b}.

Consider a classifier $f$ that for a given instance $x_i$, with true value $y_i$, outputs class $f(x_i)$, which may be correct or incorrect. An \textit{untargeted} attack involves perturbing $x_i$ into $x^*_i$, such that $f(x_i)\neq f(x^*_i)$. A successful attack would undoubtedly show the brittleness of the classifier, but may not be necessarily helpful for a malicious user, e.g. if $y_i$ corresponded to malicious content, but the original response $f(x_i)$ was incorrect.

Taking into account the misinformation scenario, we consider the \textit{targeted} attack to satisfy the following criteria:
\begin{itemize}
    \item The true class corresponds to non-credible content, i.e. $y_i=1$,
    \item The original classifier response was correct, i.e. $f(x_i)=y_i$.
\end{itemize}
Success in this attack corresponds to a scenario of the attacker preparing a piece of non-credible content that is falsely recognised as credible thanks to the adversarial modification. We use only a portion of the evaluation $X_{attack}$ subset for this kind of attack. 

By \textit{non-credible content} we mean:
\begin{itemize}
    \item In case of news bias assessment, an article from a hyperpartisan source,
    \item In case of propaganda detection, a sentence with a propaganda technique,
    \item In case of fact checking, a statement refuted by the provided evidence,
    \item In case of rumour detection, a message feed starting from a post including a rumour.
\end{itemize}

In BODEGA, both untargeted and targeted attacks can be evaluated. 

All of the text forming an instance can be modified to make an adversarial attacks. In case of fact checking, this includes both the claim and the evidence. Similarly for rumour detection, not only the original rumour, but also any of the follow-up messages in the thread. This corresponds to the real-life scenario, where all of the above content is user generated and can to some degree be influenced by an attacker (see further discussion on this matter in section \ref{sec:realitycheck}).

Finally, note that BODEGA imposes no restriction on the number of queries sent to the victim, i.e. the number of variants an attacker is allowed to test for each instance before providing the final modification. This number would typically be limited, especially in a security-oriented application \citep{Chen2022}. The average number of queries for each method is computed as part of the evaluation (see the next section). 

\section{Evaluation}
\label{sec:evaluation}

Preparing adversarial examples involves balancing two goals in the adversarial attack (see Figure \ref{fig:architecture}):
\begin{enumerate}
    \item Maximising $\text{diff}(f(x_i), f(x^*_i))$ – difference between the classes predicted by the classifier for the original and perturbed instance,
    \item Maximising $\text{sim}(x_i, x^*_i)$ – similarity between the original and perturbed instance.
\end{enumerate}

If (1) is too small, the attack has failed, since the classifier preserved the correct prediction. If (2) is too small, the attack has failed, since the necessary perturbation was so large it defeated the original purpose of the text.

This makes the evaluation multi-criterion and challenging since neither of these factors measured in isolation reflects the quality of AEs. The conundrum is usually resolved by setting the minimum similarity (2) to a fixed threshold (known as \textit{perturbation constraint}) and measuring the reduction in classification performance, i.e. accuracy reduction \citep{Zhang2020b}. This can be problematic as there are no easy ways to decide the value of the threshold that will guarantee that the class remains valid. The issue is especially valid for a task as subtle as credibility analysis --  e.g. how many word swaps can we do on a real news piece before it loses credibility?

In BODEGA we avoid this problem by inverting the approach. Instead of imposing constraints on goal (2) and using (1) as evaluation measure, we impose constraints on (1) and use (2) for evaluation. The prediction change is restricted by using a natural threshold -- if the classifier has changed its decision, the attack is valid. As a result, the similarity between the original and perturbed text becomes available to construct an evaluation measure. The higher this similarity, the better the AE generator is at performing imperceptible, yet significant, changes.

We define an adversarial modification quality score, called BODEGA score. BODEGA score always lies within 0-1 and a high value indicates good quality modification preserving the original meaning (with score=1 corresponding to no visible change), while low value indicates poor modification, altering the meaning (with score=0 corresponding to completely different text).

In the remainder of this section, we discuss the similarity measurement techniques we employ and outline how they are combined to form a final measure of attack success.

\subsection{Semantic score}

The first element used to measure meaning preservation is based on \textit{BLEURT} \citep{sellam-etal-2020-bleurt}. BLEURT was designed to compute the similarity between a candidate and reference sentences in evaluating solutions for natural language generation tasks (e.g. machine translation). The score should have values between 1 (identical text) and 0 (no similarity).

BLEURT helps to properly assess semantic similarity; for example, replacing a single word with its close synonym will yield high score value, while using a completely different one will not. However, BLEURT is trained to interpret multi-word modifications (i.e. paraphrases) as well, leading to better correlation with human judgement than other popular measures, e.g. BLEU or BERTScore. This is possible thanks to fine-tuning using synthetic data covering various types of semantic differences, e.g. \textit{contradiction} as understood in the NLI (Natural Language Inference) task. This is especially important for our usecase, helping to properly handle the situations where otherwise small modifications completely change the meaning of text (e.g. a negation), rendering an AE unusable.

In BODEGA, we use the pyTorch implementation of BLEURT\footnote{\url{https://github.com/lucadiliello/bleurt-pytorch}}, choosing the recommended\footnote{\url{https://github.com/google-research/bleurt}} \texttt{BLEURT-20} variant. Since the score is only \textit{calibrated} to the 0-1 range, other numbers can be produced as well. Thus, our semantic score is equal to BLEURT except for these cases, in which the value has to be clipped to either 0 or 1. Finally, since BLEURT is a sentence-level measure and our tasks involve longer text fragments\footnote{Except propaganda detection, where input is a single sentence.}, we (1) split the text into sentences\footnote{Except fact checking, where we simply split evidence from claim.} using LAMBO \cite{Przybya2022}, (2) find an alignment between the sentences from the original and modified text using Levenshtein distance and (3) compute semantic similarities between sentence pairs, returning its average as semantic score.

\subsection{Character score}

Levenshtein distance is used to express how different one string of characters is from another. Specifically, it computes the minimum number of elementary modifications (character additions, removals, replacements) it would take to transform one sequence into another \cite{1965}.

Levenshtein is a simple measure that does not take into account the meaning of the words. However, it is helpful to properly assess modifications that rely on graphical resemblance. For example, one family of adversarial attacks relies on replacing individual characters in text (e.g. \textit{call} to \textit{ca$||$}), altering the attacked classifier’s output. The low value of Levenshtein distance in this case represents the fact that such modification may be imperceptible for a human reader.

In order to turn Levenshtein distance $lev\_dist(a,b)$ into a character similarity score,  we compute the following:
\[
\text{Char\_score}(a, b) = 1 - \frac{lev\_dist(a, b)}{max(|a|, |b|)}
\]
$\text{Char\_score}$ is between 0 and 1, with higher values corresponding to larger similarity, with $\text{Char\_score}(a, b) = 1$  if $a$ and $b$ are the same and $\text{Char\_score}(a, b) = 0$ if they have no common characters at all.

\subsection{BODEGA score}

The BODEGA score for a pair of original text $x_i$ and modified text $x^*_i$ is defined as follows:
\begin{align*}
\text{BODEGA\_score}(x_i,x^*_i)& = \text{Con\_score}(x_i,x^*_i)  \times\\
     \text{Sem\_score}&(x_i,x^*_i)  \times \text{Char\_score}(x_i,x^*_i),
\end{align*}
where $\text{Con\_score}(x_i,x^*_i)$, i.e. confusion score, takes value $1$ when an adversarial example is produced and succeeds in changing the victim's decision (i.e. $f(x_i)\neq f(x^*_i)$) and $0$ otherwise.

The overall attack success measure is computed as an average over BODEGA scores for all instances in the attack set available in a given scenario (targeted or untargeted). The success measure reaches 0 when the AEs bear no similarity to the originals, or they were not created at all. The value of 1 corresponds to the situation, unachievable in practice, when AEs change the victim model's output with immeasurably small perturbation.

%Note that in our formulation we use measures, especially BERT score, designed for sentence level. However, two of the tasks (news bias assessment and rumour detection) involve longer texts, allowing higher-level modifications, such as reordering of sentences (or tweets), which are not explicitly taken into account by the evaluation measure. To the best of our knowledge, the discourse level has not been considered in the AE generation solutions and we leave a proper evaluation of such modifications for future work.

Many adversarial attack methods include tokenisation that does not preserve the word case or spacing between them. Our implementation of the scoring disregards such discrepancies between input and output, as they are not part of the intended adversarial modifications.

Apart from BODEGA score, expressing the overall success, the intermediate measures can paint a fuller picture of the strengths and weaknesses of a particular solution:
\begin{itemize}
    \item Confusion score – in how many of the test cases the victim's decision was changed,
    \item Semantic score – an average over the cases with changed decision,
    \item Character score – an average over the cases with changed decision.
\end{itemize}

We also report the number of queries made to the victim, averaged over all instances.

\section{Victim classifiers}
\label{sec:victimclassifiers}

A victim classifier is necessary to perform an evaluation of an AE generation solution. At this point, we include implementations of two models representing common approaches to text classification: a recurrent neural network (BiLSTM) and a fine-tuned language model (BERT).

This component of BODEGA could be easily replaced by newer implementations, either to test a robustness of a specific classifier architecture, or to have a better understanding of applicability of a given AE generation solution.

\subsection{BERT}

As the pretrained language model, we use BERT in the \texttt{bert-base-uncased} variant \citep{Devlin2018}. The model is fine-tuned for sequence classification using Adam optimiser with linear weight decay \citep{Loshchilov}, starting from 0.00005, for 5 epochs. We use maximum input length of 512 characters and a batch size of 16. The training is implemented using the \textit{Hugging Face Transformers} library \citep{wolf-etal-2020-transformers}.

\subsection{BiLSTM}

The recurrent network is implemented using the following layers:
\begin{itemize}
    \item An embedding layer, representing each token as vector of length 32,
    \item Two LSTM \citep{Hochreiter1997a} layers (forwards and backwards), using hidden representation of length 128, returned from the edge cells and concatenated as document representation of length 256,
    \item A dense linear layer, computing two scores representing the two classes, normalised to probabilities through softmax.
\end{itemize}

Similarly to the other model, the input is tokenised using BERT uncased tokeniser. The maximum allowed input length is 512, with padding as necessary. For each of the tasks, a model instance is trained from scratch for 10 epochs by using Adam optimiser \citep{Kingma2015a}, a learning rate of 0.001 and batches of 32 examples each. The implementation uses \textit{PyTorch}.


\subsection{Classification performance}

\begin{table}
    \centering
    \begin{tabular}{rrr}
    \hline
     & \multicolumn{2}{c}{\textbf{F-score}} \\
    \textbf{Task} & \textbf{BiLSTM} & \textbf{BERT}\\
    \hline
        HN & 0.6790 & 0.7185 \\
        PR & 0.4413 & 0.6097\\
        FC & 0.7417 & 0.9308  \\
        RD & 0.6127 & 0.7247 \\
    \hline
    \end{tabular}
    \caption{Performance of the victim classifiers, expressed as F-score over combined development and attack subsets.}
    \label{tab:victims}
\end{table}

Table \ref{tab:victims} shows the performance of the victim classifiers, computed as F-score over the test data (combined development and attack subsets). As expected, the pretrained language model easily outperforms a neural network trained from scratch. The credibility assessment task is a subtle one and the amount of data available for training severely limits the performance. Thus, the BERT model has an advantage by relying on knowledge gathered during pretraining. This is demonstrated by the performance gap being the largest for the dataset with the least data available (propaganda detection) and the smallest for the most abundant corpus (hyperpartisan news).

\section{AE generation solutions}
\label{sec:baselinesolutions}
Within BODEGA, we include the AE generation solutions implemented in the \textit{OpenAttack} framework. We exclude the approaches for white-box scenario (gradient-based) and those that yielded poor performance in preliminary tests. We test 8 approaches:
\begin{itemize}
    \item \textbf{BAE} \citep{garg-ramakrishnan-2020-bae} uses BERT \citep{Devlin2018} as a masked language model to generate word candidates that are likely in a given context. This includes both replacing existing tokens as well as inserting new ones.
    \item \textbf{BERT-ATTACK} \citep{li-etal-2020-bert-attack} is a very similar approach, which starts with finding out if a word is vulnerable by checking victim's response to its masking. The chosen words are replaced using BERT candidates, but unlike in BAE, no new words are inserted.
    \item \textbf{DeepWordBug} \citep{Gao2018a} works at the character level, seeking modifications that are barely perceptible for humans, but will modify an important word into one unknown to the attacked model. The options include character substitutions, removal, insertion and reordering.
    \item \textbf{Genetic} \citep{alzantot-etal-2018-generating} is using the genetic algorithm framework. A \textit{population} includes variants of text built by word replacements (using GloVe representation to ensure meaning preservation), the most promising of which can replicate and combine until a successful AE is found.
    \item \textbf{SememePSO} \citep{zang-etal-2020-word} employs a related framework, namely Particle Swarm Optimisation (PSO). A group of \textit{particles}, each representing a text modification with a certain probability of further changes (\textit{velocity}), moves through the feature space until an optimal position is found.
    \item \textbf{PWWS} \citep{ren-etal-2019-generating} is a classical greedy word replacement approach. However, it differs from the majority of the solutions by using \textit{WordNet}, instead of vector representations, to obtain synonym candidates.
    \item \textbf{SCPN} \citep{Iyyer2018} performs paraphrasing of the whole text through a bespoke encoder-decoder model. In order to train this model, the authors generate a dataset of paraphrases through backtranslation from English to Czech.
    \item \textbf{TextFooler} \citep{DBLP:conf/aaai/JinJZS20} is a greedy word-substitution solution. Unlike other similar approaches, it takes into account the syntax of the attacked text, making sure the replacement is a valid word that agrees with the original regarding its part of speech. This help to make sure the AE is fluent and grammatically correct.
\end{itemize}

The main problem the presented solutions try to solve is essentially maximising a goal function (victim's decision) in a vast space of possible modifications to input text, which is further complicated by its discrete nature. Direct optimisation is not computationally feasible here, giving way to methods that are greedy (performing the change that improves the goal the most) or maintain a population of varied candidate solutions (PSO and evolutionary algorithms). The majority of the solutions operate on word-level, seeking replacements that would influence the classification result without modifying the meaning. The exceptions are sentence-level SCPN, performing paraphrasing of entire sentences, and character-level DeepWordBug, replacing individual characters in text to preserve superficial similarity to the original. They all use victims' scores to look for most promising modifications, except for SCPN, which operates blindly, simply generating numerous possible paraphrases. 

All of the attackers are executed with their default functionality, except for BERT-ATTACK, that we use without the generation of subword permutations, which is prohibitively slow for longer documents. Just like the victim classifier, the AE solution interface in BODEGA allows for new solutions to be added and tested as the field progresses.

\section{Experiments}
\label{sec:results}

The purpose of the experiments is to test the BODEGA framework in action and establish a baseline for systematic evaluation of the robustness of credibility assessment solutions. To that end, we test each of the victim classifiers for each of the task using the available AE generation solutions. The evaluation involves both targeted and untargeted scenarios and is performed using the measures introduced in section \ref{sec:evaluation}. Moreover, we perform a manual analyis of the most promising AEs.

\subsection{Victim: BERT}
\begin{table*}
\scriptsize
    \centering
    \begin{tabular}{rr|rrrrr|rrrrr|}
    &  & \multicolumn{5}{|c|}{\textbf{Untargeted}}  & \multicolumn{5}{|c|}{\textbf{Targeted}}\\
    \textbf{Task} & \textbf{Method} & \textbf{B.} & \textbf{con} & \textbf{sem} & \textbf{char} & \textbf{Q.}  & \textbf{B.} & \textbf{con} & \textbf{sem} & \textbf{char} & \textbf{Q.} \\
    \hline
        HN & BAE  & {0.34} & {0.60} & {0.58} & {0.96} & {606.83} & {0.18} & {0.34} & {0.57} & {0.95} & {713.42} \\
         & BERT-ATTACK  & \textbf{0.60} & \textbf{0.96} & {0.64} & {0.97} & {648.41} & \textbf{0.57} & \textbf{0.95} & {0.62} & {0.96} & {753.91} \\
         & DeepWordBug  & {0.22} & {0.29} & \textbf{0.78} & \textbf{1.00} & {395.94} & {0.15} & {0.20} & \textbf{0.78} & \textbf{1.00} & {389.81} \\
         & Genetic  & {0.40} & {0.86} & {0.47} & {0.98} & {2713.80} & {0.30} & {0.71} & {0.44} & {0.97} & {4502.51} \\
         & SememePSO  & {0.16} & {0.34} & {0.50} & {0.99} & {341.70} & {0.05} & {0.12} & {0.44} & {0.99} & {417.99} \\
         & PWWS  & {0.38} & {0.82} & {0.47} & {0.98} & {2070.78} & {0.27} & {0.64} & {0.44} & {0.95} & {2107.02} \\
         & SCPN  & {0.00} & {0.92} & {0.08} & {0.02} & {11.84} & {0.00} & \textbf{0.95} & {0.09} & {0.02} & {11.89} \\
         & TextFooler  & {0.39} & {0.92} & {0.44} & {0.94} & {660.52} & {0.32} & {0.85} & {0.41} & {0.90} & {850.79} \\
    \hline
        PR & BAE  & {0.11} & {0.18} & {0.69} & {0.94} & {33.96} & {0.13} & {0.20} & {0.68} & {0.94} & {45.68} \\
         & BERT-ATTACK  & {0.43} & {0.70} & {0.68} & {0.90} & {80.16} & \textbf{0.50} & {0.79} & {0.69} & {0.92} & {99.95} \\
         & DeepWordBug  & {0.28} & {0.36} & \textbf{0.79} & \textbf{0.96} & {27.43} & {0.50} & {0.64} & \textbf{0.81} & \textbf{0.96} & {36.04} \\
         & Genetic  & \textbf{0.50} & \textbf{0.84} & {0.65} & {0.89} & {962.40} & {0.49} & \textbf{0.84} & {0.65} & {0.89} & {1211.56} \\
         & SememePSO  & {0.41} & {0.68} & {0.66} & {0.90} & {96.17} & {0.35} & {0.53} & {0.71} & {0.91} & {173.71} \\
         & PWWS  & {0.47} & {0.75} & {0.68} & {0.91} & {131.92} & {0.44} & {0.72} & {0.68} & {0.89} & {179.68} \\
         & SCPN  & {0.09} & {0.47} & {0.36} & {0.46} & {11.47} & {0.11} & {0.79} & {0.32} & {0.39} & {11.79} \\
         & TextFooler  & {0.43} & {0.77} & {0.64} & {0.87} & {57.94} & {0.46} & {0.77} & {0.66} & {0.89} & {77.81} \\
    \hline
        FC & BAE  & {0.34} & {0.51} & {0.70} & {0.96} & {80.69} & {0.18} & {0.27} & {0.70} & {0.94} & {92.47} \\
         & BERT-ATTACK  & \textbf{0.53} & {0.77} & {0.73} & {0.95} & {146.73} & \textbf{0.41} & {0.62} & {0.71} & {0.93} & {207.23} \\
         & DeepWordBug  & {0.44} & {0.53} & \textbf{0.84} & \textbf{0.98} & {54.32} & {0.22} & {0.27} & \textbf{0.85} & \textbf{0.98} & {52.31} \\
         & Genetic  & {0.52} & {0.79} & {0.70} & {0.95} & {1215.19} & {0.39} & {0.63} & {0.66} & {0.92} & {1808.08} \\
         & SememePSO  & {0.44} & {0.64} & {0.71} & {0.96} & {148.20} & {0.25} & {0.37} & {0.70} & {0.94} & {230.58} \\
         & PWWS  & {0.48} & {0.69} & {0.72} & {0.96} & {225.27} & {0.31} & {0.47} & {0.70} & {0.94} & {226.78} \\
         & SCPN  & {0.09} & \textbf{0.90} & {0.29} & {0.31} & {11.90} & {0.09} & \textbf{0.97} & {0.29} & {0.30} & {11.97} \\
         & TextFooler  & {0.46} & {0.70} & {0.70} & {0.93} & {106.13} & {0.29} & {0.49} & {0.65} & {0.88} & {131.88} \\
    \hline
        RD & BAE  & {0.07} & {0.18} & {0.41} & {0.98} & {313.01} & {0.18} & {0.44} & {0.42} & {0.98} & {196.69} \\
         & BERT-ATTACK  & {0.18} & {0.44} & {0.43} & {0.96} & {774.31} & {0.30} & {0.69} & {0.45} & {0.97} & {366.14} \\
         & DeepWordBug  & {0.16} & {0.23} & \textbf{0.70} & \textbf{0.99} & {232.74} & \textbf{0.39} & {0.56} & \textbf{0.70} & \textbf{0.99} & {174.03} \\
         & Genetic  & \textbf{0.20} & \textbf{0.46} & {0.45} & {0.96} & {4425.11} & {0.35} & {0.79} & {0.46} & {0.95} & {2266.91} \\
         & SememePSO  & {0.10} & {0.21} & {0.46} & {0.97} & {345.89} & {0.27} & {0.57} & {0.49} & {0.96} & {233.88} \\
         & PWWS  & {0.16} & {0.38} & {0.45} & {0.95} & {1105.99} & {0.32} & {0.75} & {0.45} & {0.93} & {838.83} \\
         & SCPN  & {0.01} & {0.38} & {0.16} & {0.10} & {11.35} & {0.02} & \textbf{0.90} & {0.15} & {0.10} & {11.90} \\
         & TextFooler  & {0.16} & {0.41} & {0.43} & {0.91} & {657.15} & {0.31} & {0.70} & {0.47} & {0.96} & {358.37} \\
    \hline
    \end{tabular}
    \caption{The results of adversarial attacks on the \textbf{BERT classifier} in four misinformation detection tasks in untargeted and targeted scenario. Evaluation measures include BODEGA score (B.), confusion score (con), semantic score (sem), character score (char) and number of queries to the attacked model (Q.). The best score in each task and scenario is in boldface. }
    \label{tab:BERTresults}
\end{table*}

Table \ref{tab:BERTresults} includes the results of the attack on the BERT classifier. 

The hyperpartisan news detection task is clearly the easiest one for generating AEs. In untargeted scenario, BERT-ATTACK achieves BODEGA score of 0.6, the best value in attacks against BERT. This is possible due to changing the decision on 96\% of the instances while preserving high similarity, both in terms of semantics and characters. However, DeepWordBug (a character-level method) provides the best results in terms of semantic similarity, changing less than 1\% of characters on average. The only drawback of this method is that it works 29\% of the cases, failing to change the victim's decision in the remaining ones. The targeted scenario broadly shows the same situation, with the attack being slightly harder (for BERT-ATTACK) or much harder (for other methods).

The propaganda recognition task significantly differs from the previous task in terms of text length, including individual sentences rather than full articles. As a result, every word is more important and it becomes much harder to make the changes imperceptible, resulting in lower character similarity scores. This setup appears to favour the Genetic method, obtaining the best BODEGA score in the untargeted scenario: 0.50. This approach performs well across the board, but it comes at a high cost in terms of model queries. Even for the short sentences in propaganda detection, a victim model is queried around a 1000 times, compared to less than 150 for all other methods. In this case, the targeted scenario is slightly easier and favours BERT-ATTACK, achieving the best meaning preservation.

Fact checking resembles the propaganda detection in terms of relatively short text fragments and the best-performing methods -- Genetic or BERT-ATTACK. While SCPN achieves the best confusion score here, it fails to preserve the meaning, both in terms of semantics and surface level. A method that changes the victim's decision at a cost of completely transforming the instance text is not useful. Interestingly, the targeted scenario here appears to be much more challenging, resulting in the best BODEGA score of 0.41 as opposed to 0.53 in the untargeted case. In case of false statements debunked by evidence, the victims have high confidence in their outputs and are not easily swayed.

\begin{table*}
\scriptsize
    \centering
    \begin{tabular}{rr|rrrrr|rrrrr|}
    &  & \multicolumn{5}{|c|}{\textbf{Untargeted}}  & \multicolumn{5}{|c|}{\textbf{Targeted}}\\
    \textbf{Task} & \textbf{Method} & \textbf{B.} & \textbf{Con} & \textbf{Sem} & \textbf{Char} & \textbf{Q.}  & \textbf{B.} & \textbf{con} & \textbf{sem} & \textbf{char} & \textbf{Q.} \\
    \hline
        HN & BAE  & {0.48} & {0.77} & {0.64} & {0.98} & {489.27} & {0.45} & {0.74} & {0.62} & {0.98} & {477.65} \\
         & BERT-ATTACK  & \textbf{0.64} & \textbf{0.98} & {0.66} & {0.99} & {487.85} & \textbf{0.61} & \textbf{0.96} & {0.65} & {0.99} & {565.05} \\
         & DeepWordBug  & {0.41} & {0.53} & \textbf{0.77} & \textbf{1.00} & {396.18} & {0.37} & {0.47} & \textbf{0.78} & \textbf{1.00} & {379.20} \\
         & Genetic  & {0.44} & {0.94} & {0.48} & {0.98} & {2029.31} & {0.42} & {0.90} & {0.47} & {0.98} & {2882.19} \\
         & SememePSO  & {0.21} & {0.42} & {0.50} & {0.99} & {313.51} & {0.14} & {0.28} & {0.49} & {0.99} & {361.38} \\
         & PWWS  & {0.44} & {0.93} & {0.48} & {0.99} & {2044.96} & {0.42} & {0.89} & {0.48} & {0.97} & {1994.95} \\
         & SCPN  & {0.00} & {0.94} & {0.08} & {0.02} & {11.86} & {0.00} & {0.95} & {0.08} & {0.02} & {11.83} \\
         & TextFooler  & {0.43} & {0.94} & {0.47} & {0.97} & {543.68} & {0.41} & {0.91} & {0.47} & {0.96} & {598.46} \\
    \hline
        PR & BAE  & {0.15} & {0.23} & {0.72} & {0.94} & {32.94} & {0.26} & {0.38} & {0.71} & {0.94} & {38.72} \\
         & BERT-ATTACK  & {0.53} & {0.80} & {0.72} & {0.91} & {61.41} & \textbf{0.66} & {0.94} & {0.74} & {0.94} & {50.14} \\
         & DeepWordBug  & {0.29} & {0.38} & \textbf{0.79} & \textbf{0.96} & {27.45} & {0.56} & {0.72} & \textbf{0.81} & \textbf{0.96} & {35.30} \\
         & Genetic  & \textbf{0.54} & \textbf{0.88} & {0.67} & {0.89} & {782.15} & {0.62} & {0.94} & {0.71} & {0.93} & {802.20} \\
         & SememePSO  & {0.47} & {0.76} & {0.68} & {0.89} & {85.34} & {0.60} & {0.92} & {0.71} & {0.92} & {69.62} \\
         & PWWS  & {0.53} & {0.84} & {0.69} & {0.90} & {130.85} & {0.63} & {0.92} & {0.73} & {0.94} & {168.60} \\
         & SCPN  & {0.12} & {0.55} & {0.39} & {0.50} & {11.55} & {0.20} & \textbf{0.98} & {0.37} & {0.48} & {11.98} \\
         & TextFooler  & {0.51} & {0.85} & {0.67} & {0.88} & {52.59} & {0.63} & {0.94} & {0.72} & {0.92} & {54.62} \\
    \hline
        FC & BAE  & {0.36} & {0.55} & {0.69} & {0.96} & {77.76} & {0.32} & {0.48} & {0.69} & {0.96} & {73.43} \\
         & BERT-ATTACK  & {0.60} & {0.86} & {0.73} & {0.95} & {132.80} & \textbf{0.59} & {0.85} & {0.73} & {0.96} & {123.24} \\
         & DeepWordBug  & {0.48} & {0.58} & \textbf{0.85} & \textbf{0.98} & {54.36} & {0.54} & {0.64} & \textbf{0.85} & \textbf{0.98} & {50.72} \\
         & Genetic  & \textbf{0.61} & \textbf{0.90} & {0.71} & {0.95} & {840.99} & {0.57} & {0.88} & {0.69} & {0.94} & {1015.44} \\
         & SememePSO  & {0.53} & {0.76} & {0.72} & {0.96} & {112.84} & {0.46} & {0.67} & {0.72} & {0.96} & {132.28} \\
         & PWWS  & {0.57} & {0.82} & {0.73} & {0.96} & {221.60} & {0.50} & {0.73} & {0.71} & {0.95} & {211.05} \\
         & SCPN  & {0.08} & {0.75} & {0.29} & {0.32} & {11.75} & {0.11} & \textbf{1.00} & {0.30} & {0.35} & {12.00} \\
         & TextFooler  & {0.55} & {0.82} & {0.71} & {0.94} & {98.31} & {0.50} & {0.75} & {0.70} & {0.94} & {99.98} \\
    \hline
        RD & BAE  & {0.09} & {0.21} & {0.43} & {0.98} & {312.77} & {0.27} & {0.64} & {0.43} & {0.98} & {123.16} \\
         & BERT-ATTACK  & {0.29} & \textbf{0.79} & {0.41} & {0.89} & {985.52} & {0.43} & {0.95} & {0.46} & {0.97} & {130.64} \\
         & DeepWordBug  & {0.16} & {0.24} & \textbf{0.68} & \textbf{0.99} & {232.75} & \textbf{0.62} & {0.91} & \textbf{0.69} & \textbf{0.99} & {153.61} \\
         & Genetic  & \textbf{0.32} & {0.71} & {0.47} & {0.96} & {3150.24} & {0.44} & \textbf{0.96} & {0.48} & {0.95} & {1355.52} \\
         & SememePSO  & {0.15} & {0.31} & {0.48} & {0.97} & {314.63} & {0.32} & {0.67} & {0.50} & {0.97} & {185.47} \\
         & PWWS  & {0.29} & {0.64} & {0.47} & {0.97} & {1059.07} & {0.44} & {0.95} & {0.48} & {0.95} & {742.12} \\
         & SCPN  & {0.01} & {0.55} & {0.17} & {0.09} & {11.53} & {0.02} & {0.84} & {0.15} & {0.12} & {11.84} \\
         & TextFooler  & {0.24} & {0.64} & {0.41} & {0.87} & {639.97} & {0.44} & \textbf{0.96} & {0.48} & {0.96} & {184.97} \\
    \hline
    \end{tabular}
    \caption{The results of adversarial attacks on the \textbf{BiLSTM} classifier in four misinformation detection tasks in untargeted and targeted scenario. Evaluation measures include BODEGA score (B.), confusion score (con), semantic score (sem), character score (char) and number of queries to the attacked model (Q.). The best score in each task and scenario is in boldface. }
    \label{tab:BiLSTMresults}
\end{table*}

Finally, the rumour detection task, which in the untargeted scenario appears to be the hardest problem to attack of all. Here the best methods reaches BODEGA score of 0.2, indicating low usability, mostly due to low confusion rates -- less than 50\% in the untargeted scenario. This may be because rumour threads consist of numerous posts, each having some indication on the credibility of the news, forcing an attacker to make many modifications to change the victim's decision. The text of Twitter messages is also far from regular language, making the challenge harder for methods using models pretrained on well-formed text (e.g. BERT-ATTACK). It has to be noted however that this setup is equally problematic to the meaning preservation measurement (semantic score), thus suggesting these results should be taken cautiously.




\subsection{Victim: BiLSTM}


Table \ref{tab:BiLSTMresults} includes the results of the attack on the BiLSTM classifier. 

For hyperpartisan news detection, the pattern observed for BERT is largely repeated, with BERT-ATTACK offering the best overall performance, and DeepWordBug producing AEs of good quality, but with lower confusion scores. However, the recurrent network is clearly easier to attack than a pretrained model. While the character replacements of DeepWordBug are successful in flipping the decision of only 29\% of cases for BERT, that rate is up to 53\% for BiLSTM. 

In case of propaganda recognition, the patterns we see with BERT as a victim are largely repeated with BiLSTM. However, we observe a much larger difference of attack performance between the targeted and untargeted variant. These results have to be interpreted with caution, since BiLSTM provides relatively poor performance for this task, which reduces the number of instances used in the untargeted scenario.

In fact checking we again see that BiLSTM is much more easily attacked. What is especially worrying is the targeted scenario, which corresponds to inaccurate information being refuted by the evidence -- here the BODEGA score reaches 0.59 compared to 0.41 in BERT. The decision could be flipped for 85\% of the instances (62\% for BERT), indicating low robustness of this approach.

Rumour detection is also a task with low robustness of BiLSTM classifier. This is especially true in the targeted scenario: the character changes applied by the DeepWordBug are successful in 91\% of cases, preserving the semantic similarity of 0.69 and leaving 99\% of text unchanged. The brittleness of the classifier's output is also reflected in the number of queries made by the AE solutions to the model -- while the Genetic method on average needed to query BERT 2267 times, for BiLSTM 1356 is enough.

\subsection{Manual analysis}
\label{sec:manual}
In order to better understand how a successful attack might look like, we manually analyse some of them. This allows us observe what types of adversarial modifications are the weakest point of the classifier, as well as verify if attack success scoring using automatic measures is aligned with the human judgement.

For that purpose, we select 20 instances with the highest BODEGA score from the untargeted interactions between a relatively strong attacker (BERT-ATTACK) and a relatively weak victim (BiLSTM), within all tasks. Next, we label the AEs according to the degree they differ from the original text:\footnote{Note that while these categories might overlap, e.g. a typographic replacement significantly affecting the overall meaning, such cases were not encountered in practice during the analysis.}
\begin{enumerate}
    \item \textbf{Synonymous}: the text is identical in meaning to the original.
    \item \textbf{Typographic}: change of individual characters, e.g. resembling sloppy punctuation or typos, likely imperceptible.
    \item \textbf{Grammatical}: change of the syntax of the sentence, e.g. replacing a verb with a noun with the same root, possibly making the text grammatically incorrect,
    \item \textbf{Semantic-small}: changes affecting the overall meaning of the text, but to a limited degree, unlikely to affect the credibility label,
    \item \textbf{Semantic-large}: significant changes of the meaning of the text, indicating the original credibility may not apply,
    \item \textbf{Local}: changes of any degree higher than Synonymous, but present only in a few non-crucial sentences of a longer text, leaving others to carry the original meaning (applies to tasks with many sentences, i.e. RD and HN).
\end{enumerate}
The changes labelled as Semantic-large indicate attack failure, while others denote success with varying visibility of the modification.

\begin{table}
    \centering
    \begin{tabular}{r|rrrr|r}
     & \multicolumn{4}{|c|}{\textbf{number of instances}} &\\
    \textbf{AE degree} & \textbf{HN} & \textbf{PR} & \textbf{FC} & \textbf{RD} &  $\Sigma$ \\
    \hline
        Synonymous & 6 & 10 & 2 & 5 &29\% \\
        Typographic & 0 & 5 & 8 & 0 &16\%\\
        Grammatical & 0 & 4 & 3 & 2 &11\%\\
        Semantic-small & 0 & 1 & 2 & 3 &7\%\\
        Local & 13 & - & - & 2 & 19\%\\
        \hline
        Semantic-large & 1 & 0 & 5 & 8 &17\%\\
    \hline\hline
    \end{tabular}
    \caption{Number of AEs using different modifications among the best 20 instances (according to BODEGA score) in each task, using BiLSTM as victim and BERT-ATTACK as attacker.}
    \label{tab:manual}
\end{table}

Table \ref{tab:manual} shows the quantitative results of the manual analysis, while table \ref{tab:examples} includes some examples. Generally, a large majority of these attacks (82.5\%) were successful in maintaining the original meaning, confirming the high BODEGA score assigned to them. However, significant differences between the tasks are visible.

Consistently with the results of automatic analysis, rumour detection appears to be the most robust, resulting in many attacks changing the original meaning. Even though oftentimes only a word or two is changed, it affects the meaning of the whole Twitter thread, since the follow-up messages do not repeat the content, but often deviate from the topic (see EX4 in table \ref{tab:examples}). The opposite happens for news bias assessment: a singular change does not affect the overall message, as the news article are typically redundant and maintain their sentiment throughout (see EX6). As a result, the HR task is one of the most vulnerable to attacks.

It is also interesting to compare the two tasks with shorter text: fact checking and propaganda recognition. While the FC classifier shows a large vulnerability to typographic changes (esp. in punctuation, see EX2), many of the changes performed by the attackers affect important aspects of the content (e.g. names or numbers, see EX5), making the AE futile. The propaganda recognition, on the other hand, appears to rely on stylistic features, allowing the AE generation while preserving full synonymy (see EX1) or just introducing grammatical issues (see EX3).

\begin{table*}
\scriptsize
    \centering
    \begin{tabular}{p{0.12\linewidth}|p{0.44\linewidth}|p{0.44\linewidth}|}
    \textbf{Id., task, type} & \textbf{Original example} & \textbf{Adversarial example}\\
    \hline
        EX1 PR \newline Synonymous & Puerto Rico's housing secretary, Fernando Gil, says the number of \textbf{homes} destroyed by the hurricane totals about 70,000 so far, and homes with major damage have amounted to 250,000 across the island. & Puerto Rico's housing secretary, Fernando Gil, says the number of \textbf{houses} destroyed by the hurricane totals about 70,000 so far, and homes with major damage have amounted to 250,000 across the island. \\ \hline
        EX2 FC\newline Typographic & Sabbir Khan. Sabbir's second movie, Heropanti starring Tiger Shroff \& Kriti Sanon\textbf{,} released on 23 may 2014. $\rightarrow$ Sabbir Khan directed a movie. & Sabbir Khan. Sabbir's second movie, Heropanti starring Tiger Shroff \& Kriti Sanon\textbf{?} released on 23 may 2014. $\rightarrow$ Sabbir Khan directed a movie. \\ \hline
        EX3 PR\newline Grammatical & Fastiggi and Goldstein have managed to make the problem even worse in their attempt to \textbf{explain} it away. & Fastiggi and Goldstein have managed to make the problem even worse in their attempt to \textbf{explained} it away. \\ \hline
        EX4 RD\newline Semantic-small & A few of the best cartoons \textbf{drawn \& shared in solidarity} with \#charliehebdo after yesterday's massacre \#jesuischarlie http://t.co/87et0xpnwr \newline @theinquisitr war profiteers x'd \#princessdiana \& dodifayed in \#paris. pushing \#france to join war on terror video $>>$ http://t.co/tysy8ys49w \newline @theinquisitr l'amérique se tient avec la france. \#jesuischarlie & A few of the best cartoons \textbf{contributed \& held in friendship} with \#charliehebdo after yesterday's massacre \#jesuischarlie http://t.co/87et0xpnwr \newline @theinquisitr war profiteers x'd \#princessdiana \& dodifayed in \#paris. pushing \#france to join war on terror video $>>$ http://t.co/tysy8ys49w \newline @theinquisitr l'amérique se tient avec la france. \#jesuischarlie \\ \hline
        EX5 FC \newline Semantic-large & Hannah and Her Sisters. Hannah and Her Sisters is a 1986 american comedy - drama film which tells the intertwined stories of an extended family over two years that begins and ends with a family thanksgiving dinner. $\rightarrow$ Hannah and Her Sisters is an American \textbf{1986} film. & Hannah and Her Sisters. Hannah and Her Sisters is a 1986 american comedy - drama film which tells the intertwined stories of an extended family over two years that begins and ends with a family thanksgiving dinner. $\rightarrow$ Hannah and Her Sisters is an American \textbf{1987} film. \\ \hline
        EX6 HN\newline Local & Aleppo completely back under government control (GPA) Aleppo – the Syrian Arab Army (SAA) has reported today that the entirety of \textbf{east} Aleppo is fully back under government control, meaning the city is now completely liberated. The SAA has completed the evacuations of anti-government fighters and civilians looking to flee with these groups as of today. This is a major victory for the Syrian forces in Aleppo coming after almost 4 years of fighting in the city. Thousands of people have already taken to the streets to celebrate the last of the terrorists inside the city leaving.\newline [347 words more]   & Aleppo completely back under government control (GPA) Aleppo – the Syrian Arab Army (SAA) has reported today that the entirety of \textbf{south} Aleppo is fully back under government control, meaning the city is now completely liberated. The SAA has completed the evacuations of anti-government fighters and civilians looking to flee with these groups as of today. This is a major victory for the Syrian forces in Aleppo coming after almost 4 years of fighting in the city. Thousands of people have already taken to the streets to celebrate the last of the terrorists inside the city leaving.\newline [347 words more] \\ \hline
    \hline
    \end{tabular}
    \caption{Some examples of adversarial modifications that were successful (i.e. resulted in changed classifier decision), performed by BERT-ATTACK against BiLSTM, including identifier (mentions in text), task and type of modification. Changes are highlighted in boldface.}
    \label{tab:examples}
\end{table*}


\subsection{Summary}
We can summarise the experiments through the following general observations:
\begin{itemize}
    \item Fine-tuned BERT appears to be noticeably more robust than BiLSTM. This situation is observable across tasks and scenarios.
    \item The difficulty of attack is usually similar for untargeted and targeted scenarios. However, where the victim classifier performs poorly (BiLSTM detecting propaganda and rumours), the targeted attack becomes significantly easier.
    \item Both simple attacks based on character replacements (DeepWordBug) and complex ones utilising language models (BERT-ATTACK) can give good results in some situations.
    \item Some methods may require thousands of queries to the victim model to come up with an AE, especially for longer text. This may limit their practical applicability despite good performance.
    \item Style-based news bias assessment is clearly the most vulnerable to adversarial attacks, while fact checking is the least. This may be related to a content length: when the input is only a sentence or two, every word replacement affects the meaning, hindering the attack.
\end{itemize}


\section{Discussion}


\subsection{Reality check for credibility assessment}
\label{sec:realitycheck}
While one of the principles guiding the design of BODEGA has been a realistic simulation of the misinformation detection scenarios, this is possible only to an extent. Among the obstacles are low transparency of content management platforms \citep{Gorwa2020} and the vigorous growth of the methods of attack and defence in the NLP field. 

Firstly, we have included only two victim models in our tests: BiLSTM and BERT, while in reality dozens of architectures for text classification are presented at every NLP conference, with a significant share specifically devoted to credibility assessment. However, the field has recently become surprisingly homogeneous, with the ambition to achieve the state of the art pushing researchers to reuse the common pretrained language models in virtually every application \citep{Church2022}. But these lookalike approaches share not only good performance, but also weaknesses. Thus we expect that, for example, the results of attacks on fine-tuned BERT will also apply to other solutions that use BERT as a representation layer.

Secondly, we have re-used the attacks implemented in OpenAttack to have a comprehensive view of performance of different approaches. However, the field of AEs for NLP is relatively new, with the majority of publications emerging in the recent years, which makes it very likely that subsequent solutions will provide superior performance. With the creation of BODEGA as a universal evaluation framework, such comparisons become possible.
%However, it was hard to make such comparisons with no commonly agreed evaluation framework. This is precisely the reason for the creation of BODEGA.

Thirdly, we need to consider the realism of evaluation measures. The AE evaluation framework assumes that if a modified text is very similar to the original, then the label (credible or not) still applies. Without this assumption, every evaluation would need to include manual re-annotation of the AEs. Fortunately, assessing semantic similarity  between two fragments of text is a necessary component of evaluation in many other NLP tasks, e.g. machine translation \citep{Lee2023}, and we can draw from that work. Apart from BLEURT, we have experimented with SBERT cross-encoders \citep{thakur-etal-2021-augmented} and unsupervised BERT Score \citep{DBLP:conf/iclr/ZhangKWWA20}, but haven't found decisive evidence for the superiority of any approach. However, the problem remains open. The investigation on how subtle changes in text can invert its meaning and subvert credibility assessment is particularly vivid in the fact-checking field \citep{Jaime2022}, but is well explored for tasks involving multi-sentence inputs, e.g. news credibility.

Fourthly, we also assume that an attacker has a certain level of access to the victim classifier, being able to send unlimited queries and receive numerical scores reflecting its confidence, rather than a final decision. In practice, this is currently not the case, with platforms revealing almost nothing regarding their automatic content moderation processes. However, this may change in future due to regulatory pressure from the government organisations; cf., for example, the recently agreed EU \textit{Digital Services Act}\footnote{\url{https://ec.europa.eu/commission/presscorner/detail/en/qanda_20_2348}}.

Finally, we need to examine how realistic is that an attacker could freely modify any text included in our tasks. While this is trivial in the case of news credibility and propaganda detection, where the entire input comes from a malicious actor, the other tasks require closer consideration. In case of rumour detection, the text includes, apart from the initial information, replies from other social media users. These can indeed be manipulated by sending replies from anonymous accounts and this scenario has been already explored in the AE literature \citep{Le2020}. In the case of fact checking, the text includes, apart from the verified claim, also the relevant snippets from the knowledge base. However, it can be modified as well, when (as is usually the case) the knowledge is based on Wikipedia, which is often a subject of malicious alterations, from vandalism \citep{Kiesel_Potthast_Hagen_Stein_2017} to the generation of entire hoax articles \citep{Kumar2016}.

To sum up, we argue that despite certain assumptions, the setup of a BODEGA framework is close enough to real-life conditions to give insights about the robustness of popular classifiers in this scenario.

\subsection{Looking forward}
Beyond the exploration  of the current situation, we hope BODEGA will be useful for assessing the robustness of future classifiers and the effectiveness of new attacks. Towards this end, we make the software available openly\footnote{\url{https://github.com/piotrmp/BODEGA}}, allowing both the replication of our experiments and evaluating other solutions, both on the attack and the defence.

We also see this study as a step towards the directions recognised in the ML literature beyond NLP. For example, in security-oriented applications, there is the need to bring the evaluation of AEs closer to realistic conditions \citep{Chen2022}. Some limitations, esp. number of queries to the model, make attacks much harder. Even beyond the security field, assessing robustness is crucial for ML models that are distributed as massively-used products. This exposes them to unexpected examples, even if not generated with explicit adversarial motive. Individual spectacular failures are expected to be disproportionately influential on public opinion of technology, including AI \citep{Mannes2020}, emphasising the importance of research on AEs.
%Well-known cases include images of black people labelled as 'gorillas' by \textit{Google Photos}\footnote{\url{https://www.theverge.com/2018/1/12/16882408/google-racist-gorillas-photo-recognition-algorithm-ai}} or 'primates' by \textit{Facebook}\footnote{\url{https://www.nytimes.com/2021/09/03/technology/facebook-ai-race-primates.html}}. 

Finally, we need to acknowledge that the idea of using ML models for automatic moderation of user-generated content is not universally accepted, with some rejecting it as equivalent to censorship \citep{Llanso2020}, and calling for regulations in this area \citep{Meyer2019}.


\section{Conclusion}

Through this work, we have demonstrated that popular text classifiers, when applied for the purposes of misinformation detection, are vulnerable to manipulation through adversarial examples. We have discovered numerous cases where making a single barely perceptible change is enough to prevent a classifier from spotting non-credible information. Among the risk factors are large input lengths, poor accuracy of the victim classifier and the possibility of making numerous queries.

Nevertheless, the attack is never successful for every single instance and often entails changes that make text suspiciously malformed or ill-suited for the misinformation goal. This emphasises the need for thorough testing of the robustness of text classifiers at various stages of their development: from the initial design and experiments to the preparation for deployment, taking into account likely attack scenarios. We hope the BODEGA benchmark we contribute here, providing an environment for comprehensive and systematic tests, will be a useful tool in performing such analyses.

\section*{CRediT authorship contribution statement}
\textbf{Piotr Przyby{\l}a:} Conceptualization, Methodology, Software, Data Curation, Investigation, Writing - Original Draft, Writing - Review \& Editing. \textbf{Alexander Shvets:} Conceptualization, Methodology, Writing - Review \& Editing. \textbf{Horacio Saggion:} Conceptualization, Writing - Review \& Editing.

\section*{Acknowledgements}
This work is part of the ERINIA project, which has received funding from the European Union’s Horizon Europe research and innovation programme under grant agreement No 101060930.  Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union. Neither the European Union nor the granting authority can be held responsible for them. The work was also funded by the EU internal security program ISF-2021-AG-TCO under grant agreement No 101080090 (the ALLIES project) and by the Spanish State Research Agency under the Maria de Maeztu Units of Excellence Programme (CEX2021-001195-M).


\bibliographystyle{elsarticle-num}
\bibliography{BODEGA}

\end{document}
