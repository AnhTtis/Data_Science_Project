%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command
\usepackage{color,soul}
\usepackage{hyperref}
\usepackage{mwe}
\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
\usepackage{mathptmx} % assumes new font selection scheme installed
\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{subfigure}
\usepackage[noadjust]{cite}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}
\def\ie{\emph{i.e.}} \def\Ie{\emph{I.e.}\onedot}
\def\eg{\emph{e.g.}} \def\Eg{\emph{E.g.}\onedot}

% \begin{table}[h]
% \caption{An Example of a Table}
% \label{table_example}
% \begin{center}
% \begin{tabular}{|c||c|}
% \hline
% One & Two\\
% \hline
% Three & Four\\
% \hline
% \end{tabular}
% \end{center}
% \end{table}


%    \begin{figure}[thpb]
%       \centering
%       \framebox{\parbox{3in}{We suggest that you use a text box to insert a graphic (which is ideally a 300 dpi TIFF or EPS file, with all fonts embedded) because, in an document, this method is somewhat more stable than directly inserting a picture.
% }}
%       %\includegraphics[scale=1.0]{figurefile}
%       \caption{Inductance of oscillation winding on amorphous
%        magnetic core versus DC bias magnetic field}
%       \label{figurelabel}
%    \end{figure}

\title{\LARGE \bf
Mobile MoCap: Retroreflector Localization On-The-Go
}


\author{Gary Lvov$^{*}$, Mark Zolotas, Nathaniel Hanson, Austin Allison,\\ Xavier Hubbard, Michael Carvajal, Ta\c{s}kin Padir% <-this % stops a space
%\thanks{*This work was not supported by any organization}% <-this % stops a space
% \thanks{This research is supported by the Defense Advanced Research Projects Agency under award
% HR0011-22-2-0004}
\thanks{Institute for Experiential Robotics, Northeastern University, Boston, MA, USA}%}
\thanks{*Corresponding author: {\tt\small lvov.g@northeastern.edu}}%}
}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Motion capture (MoCap) through tracking retroreflectors obtains high precision pose estimation, which is frequently used in robotics. Unlike MoCap, fiducial markerbased tracking methods do not require a static camera setup to perform relative localization. Popular pose-estimating systems based on fiducial markers have lower localization accuracy than MoCap. As a solution, we propose Mobile MoCap, a system that employs inexpensive near-infrared cameras for precise relative localization in dynamic environments. We present a retroreflector feature detector that performs 6-DoF (six degrees-of-freedom) tracking and operates with minimal camera exposure times to reduce motion blur. To evaluate different localization techniques in a mobile robot setup, we mount our Mobile MoCap system, as well as a standard RGB camera, onto a precision-controlled linear rail for the purposes of retroreflective and fiducial marker tracking, respectively. We benchmark the two systems against each other, varying distance, marker viewing angle, and relative velocities. Our stereo-based Mobile MoCap approach obtains higher position and orientation accuracy than the fiducial approach.

The code for Mobile MoCap is implemented in ROS 2 and made publicly available at~\url{https://github.com/RIVeR-Lab/mobile_mocap}.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
Localization of people and robots is a critical need for automated warehouse facilities. Recognizing the 6D pose (position and orientation) of dynamic, moving objects allows robots to not only understand their current location but also to ensure an appropriate factor of safety. In this context, especially where robots and humans occupy a shared space, safety is paramount and the margin for error has to be minimized. Artificial landmarks, such as fiducial markers, provide an easily identifiable reference point for cameras to track objects in 3D space~\cite{Olson2011April,Fiala2005ARtag,Wang2016April2}. Recognizable marker geometries are used to localize fiducial markers and to discriminate between objects. This technique has applications in virtual reality \cite{simonetto2022methodological}, medicine \cite{alarcon2020upper}, and robotics \cite{menolotto2020motion,Zolotas2018}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/mobile_mocap_title_page.png}
    \caption{Mobile MoCap camera assembly with central camera for dedicated visual fiducial tracking. \textbf{(a)} Target tracking plane with fiducial marker and retroreflector tags; \textbf{(b)} Segmented tags used in tracking objects}.
    \label{fig:title_teaser}
    \vspace{-1.50em}
\end{figure}

% In robotics, relative localization of artificial landmarks or fiducials is used to perform visual servoing~\cite{Kalaitzakis2021Fiducial}. This has additional applications to autonomous vehicles, where landmarks and fiducials can be used to provide a static reference point for simultaneous localization and mapping (SLAM)~\cite{Pfrommer2019TagSLAMRS}. However, common conditions that arise in robotics applications, such as motion blur, changes in perceived lighting, large viewing angles, and large distances, can all decrease the precision of contemporary approaches. Poor relative localization leads to erroneous robot movement when vision informs control, resulting in potentially dangerous situations.

To address the limitations of current fiducial-based approaches, we present Mobile MoCap, a stereo visual localization approach that uses retroreflectors, as inspired by commercial visual motion capture systems~\cite{Field2009Motion}.   Fig.~\ref{fig:title_teaser} demonstrates an overview of our proposed system. Retroreflectors are commercially available as flat, cut-to-size, textured sheets of adhesive-backed paper or as screw-mounted, textured spheres of plastic. Affixing just three of these retroreflectors to the same rigid target body is all that is needed to extract said target's 6D (six degrees of freedom,~\ie, 6-DoF) pose by triangulating the unique three-dimensional (3D) positions of each of the attached retroreflectors. These retroreflective markers reflect near infrared (NIR) light into an NIR camera, making them easily identifiable under dim lighting conditions, or even within scenes of complete darkness. Furthermore, unlike two-dimensional (2D) fiducial markers, retroreflectors can be arranged in a non-planar layout, making retroreflector-based detection systems more robust because such markers can be detected from a wider range of IR camera viewing angles.

Our approach yields a twofold benefit over fiducial markers. First, by decreasing the exposure time of the cameras, we inherently reduce motion blur while still being able to isolate retroreflectors from the scene's background. Second, by not being constrained to planar mounting surfaces when distributing retroreflectors over the target object's unique geometry, we can achieve better tracking of the target in 3D space.

We designed a robotic experimental testbed to autonomously benchmark our proposed Mobile MoCap system against AprilTags~\cite{Olson2011April,Wang2016April2}, a popular fiducial marker tracking approach. The robotic testbed consists of a camera assembly mounted onto a servo and actuated linear rail, enabling the cameras' viewing angle and distance to be controlled relative to a static target. In our experiments, we consider different viewing angles and distances, as well as relative velocities to a target that contains both retroreflectors and an AprilTag. We compare the two marker localization techniques and determine their relative performance. The overall setup was designed to mimic a scenario in which a mobile robot aims to localize a static object of interest, such as a charging station or worker wearing a reflective safety vest.

The key contributions of this paper include:
\begin{itemize}
    \item A lightweight inexpensive visual localization stereo system capable of real-time 6-DoF tracking, which succeeds in dynamic conditions that are challenging to contemporary fiducial marker-based approaches, while offering superior precision.
    \item A robotics testing platform to benchmark our proposed approach against a pervasive fiducial marker baseline, where the cameras are mounted onto an actuated rail and perform relative localization at various distances, viewing angles, and relative velocities.
    \item An open-source software framework implemented atop the Robot Operating System (ROS) 2~\cite{Macenski2022ROS2} and OpenCV \cite{opencv_library} for easy interoperability with robotic systems and inexpensive camera hardware.
\end{itemize}

\section{Related Work}

There are numerous approaches to visually tracking objects or getting a bearing on the environment. One prominent approach is to track an object directly, without using markers or any other landmarks. These "markerless" approaches are attractive because they do not depend on extra hardware for tracking \cite{li2022bcot}. For example, Region-based Convolutional Neural Networks (R-CNN) can perform markerless tracking by combining regional classification with CNN~\cite{Girshick2014RCNN}. The YOLO network architecture~\cite{Redmon2016YOLO} can instead predict multiple object classifications and bounding boxes from a single image, in real-time. Nevertheless, learning-based markerless pose estimation requires extensive training data, and cannot demonstrate localization precision comparable to those of marker-based approaches~\cite{Nakano2020Evaluation}. 

In contrast, marker-based tracking methods rely on visually distinct landmarks that can be easily identified by an optical camera. Active markers are a common example, which require a power supply and supporting electronics to function,~\eg\ pulsed RGB LEDs during human hand tracking~\cite{Ruppel2019Low}. These markers excel under low-lighting conditions, but the requirement for a power source makes them more resource-intensive than other solutions. Another example are low-powered retroreflector tags that can be localized at longer distances~\cite{Soltanaghaei2021Millimetro}. These tags are favored in high-speed vehicle applications, but are still inhibited by the need for supporting hardware.

Passive markers are an alternative solution to remedy this issue. Passive markers require no additional electronics or power supply, and thus offer a lightweight cost-effective tracking approach. For instance, these markers are a preferable choice in full-body motion capture as they ensure that the participant is not encumbered by supporting hardware~\cite{Field2009Motion}. One drawback of passive markers is that they need direct line of sight with a camera, and occlusions can prevent effective tracking. Motion capture rigs solve this issue by leveraging multiple cameras to continuously track a single marker from all viewpoints. However, MoCap cannot translate well to robotics considering cameras are in fixed positions on the robot body looking outwards, rather than placed around the environment in a motion capture assembly.

Other common solutions used in autonomous vehicle and robotics domains are fiducial markers, which are visually distinct, flat, black and white printed images,~\eg\ the AprilTag~\cite{Olson2011April} or ARTag~\cite{Fiala2005ARtag}. These fiducials are easy and quick to localize, even when using low resolution images that are partially occluded or not orthogonal to the cameraâ€™s line of sight~\cite{Olson2011April,Wang2016April2}. Fiducial markers have a plethora of applications in robotics, most notably for visual servoing and autonomous vehicles~\cite{Pfrommer2019TagSLAMRS}. 

Passive fiducial tracking methods can be improved by incorporating retroreflective material and multiple cameras into the setup.  Monocular setups are generally lower cost and less space obtrusive ~\cite{Vagvolgyi2022Wide,Lichtenauer2011Monocular,Smith2021Method}. However, stereo cameras offer distinct advantages, such as higher precision and more effective acquisition of scene information~\cite{Azad2009Stereo}. In prior work, stereo cameras with a wide field of view and an occlusion detection algorithm were shown to rival commercially available MoCap systems, albeit not in real-time~\cite{Islam2020Stereo}. Trinocular setups with NIR cameras have also demonstrated the capacity to track passive markers at sub-tenth millimeter precision \cite{Bi2021High}. Nevertheless, a multiple camera system can rapidly become expensive and typically remains stationary. 

Considering the difficulties with marker occlusion, few existing marker tracking systems are mobile, while retaining the advantages of motion capture technology. Previous work in augmented reality for surgical applications has utilized the stereo cameras on the Microsoft HoloLens in order to track a surgical tool of known length~\cite{Gsaxner2021Inside}. The study's results were reported at sub-centimeter precision under this mobile stereo configuration. We expand on this system by generalizing it for widespread use in robotics, where stereo cameras must handle motion at significantly larger speeds and distances than in augmented reality use-cases.

\section{Stereo-based Mobile MoCap}

 Given the prevalence of the ROS 2 \cite{Macenski2022ROS2} middleware, we implemented our entire software and hardware stacks with the goals of being: 1) open-source; 2) cross-platform; and 3) easily reproducible.

\subsection{System Hardware}

A stereo NIR camera system with an integrated light source can be constructed from inexpensive off the shelf components. ArduCam\footnote{\url{https://www.arducam.com/product-category/uvc-usb-camera-module/usb-uvc-cameras-night-vision/}} offers an NIR USB camera that provides a camera stream running at 30 frames per second (FPS) and 640x480p resolution. When properly focused, the cameras have a field of view (FOV) of $100^{\circ}$, horizontal, and $138^{\circ}$, diagonal. These cameras utilize a silicon-based photodetector (CMOS) with broadband sensitivity. We permanently disable the IR cut filter by removing the transition photoresistor to allow for IR tracking in well-illuminated indoor environments, which would normally cause the camera to autoswitch to the RGB space. IR LEDs adjacent to the camera lens provide illumination at 850 nm, within the rated tolerance of the photodetector, but outside human visual range. Creating a stereo NIR camera from the ArduCams costs less than \$100, including \$20 in materials to fix the two cameras together. A full parts manifest and assembly instructions are available on the project website.

The monocular and stereo camera subsystems are combined in series on a laser cut acrylic plate to enable benchmarking of AprilTags against the proposed system. Tags are mounted on a continuous rotation servo (Dynamixel AX-12A). Four 19 mm retroreflective marker spheres (Qualisys) were asymmetrically arranged on the opposite side of the acrylic plate, with one marker placed on a 22 mm standoff for non-planar asymmetry.

\subsection{ROS 2 Software Package}

We have developed a ROS 2 package~\cite{Macenski2022ROS2} to facilitate 6-DoF tracking (shown in Fig.~\ref{fig:ros2}) of rigid retroreflector geometry, as well as 3D translation estimation of individual retroreflectors. ROS 2 ensures easy interoperability with other robotics systems and has a rich set of transform libraries available, suitable for geometric data processing. As detailed in Fig.~\ref{fig:ros2}, the core algorithm steps are encapsulated as ROS 2 nodes that occupy independent threads and can be run in parallel. The OpenCV library in C++ is used on the image processing steps to gain performance, with the pose extraction and utility scripts implemented in Python. The code is publicly available online. To the authors' best knowledge, there is no open-source, publicly available stereo retroreflector tracking package for ROS 2. 

\begin{figure}[t]
    \centering
    \vspace{0.75em}
    \includegraphics[width=\columnwidth]{figures/software_architecture_mobile_mocap.png}
    \caption{Feature detection pipeline deconstructed as series of ROS 2 nodes. \textbf{(a)} A raw camera image of four retroreflectors with exposure time minimized; \textbf{(b)} Detection of contrasting edges on the retroreflectors; \textbf{(c)} Elimination of nested contours within the detected region; and \textbf{(d)} the final detected contours with their centroids.}
    \label{fig:ros2}
\end{figure}

\subsection{Filtering and Feature Detection}

The two stereo cameras are initialized with their exposure times minimized (500 $\mu s$). The resulting images are predominantly occupied by the retroreflectors and small sections of reflected light noise, as visualized in Fig.~\ref{fig:ros2}a. When the 750 nm IR cut filter is disengaged, light sources other than the co-located IR LEDs are easily filtered out as they appear green in the RGB colorspace without IR light removal. Finally, a global intensity threshold is applied to the image, which can be tuned for the desired operating conditions. 

For each image, the pixel coordinates of each potential marker center are determined after filtering. The feature detector determines the edges in the image using a Canny Edge Detector (Fig.~\ref{fig:ros2}b)~\cite{canny1986computational}. Multiple detections are corrected for by infilling the selected contours (Fig.~\ref{fig:ros2}c), resulting in the removal of redundant marker locations. Shading in nested contours helps prevent a single marker being detected as more than one marker. After the shading in process is completed, an ellipse is fit to each contour. The marker radius is approximated as the average of the major and minor axis of the ellipse, with the center of the ellipse representing the feature location. Fitting an ellipse to the processed contours provides a better estimation of marker centers than alternative techniques when there are partial occlusions of the markers, such as fitting a minimum enclosing circle to the contours or the circle Hough Transform \cite{hough1962method}. The final feature detection results are shown in Fig.~\ref{fig:ros2}.

\subsection{Correspondence Matching}
\begin{figure}[t]
\begin{algorithm}[H]
    \label{alg:corr}
    \caption{Geometric Correspondence Matching}
    \begin{algorithmic}
    \STATE{Input: pixel coordinates $(x_i, y_i)  \forall$ features $i \in$ left camera}
    \STATE{pixel coordinates $(x_j, y_j) \forall$ features $j \in$ right camera}
    \STATE{correspondence $\leftarrow$ []}
    \STATE{assignments $\leftarrow$ []}
    \STATE{set$_i$ $\leftarrow \emptyset$, set$_j$ $\leftarrow \emptyset$}
    \STATE{geoms$_i$ $\leftarrow$ $\forall$ features $\in i$ $(x, y)$ offset to $\forall$ features $\in i$}
    \STATE{geoms$_j$ $\leftarrow$ $\forall$ features $\in j$ $(x, y)$ offset to $\forall$ features $\in j$}
    \STATE{$sort$(geoms$_i$), $sort$(geoms$_j$)}
    \STATE{shortest $\leftarrow$ $min$($ length$($ $geoms$_i$), $length$(geoms$_j$))}
    \FOR{geometry$_i$ $\in$ geoms$_i$}
        \FOR {geometry$_j$  $\in$ geoms$_j$}
            \STATE{error $\leftarrow$ $||$geometry$_i$ - geometry$_j$}$||$
            \STATE{assignments.$append$(($i$, $j$), error)}
        \ENDFOR
    \ENDFOR
    \STATE{ $sort$(assignments) by increasing error}
    %\STATE{set$_i$ $\leftarrow \emptyset$, set$_j$ $\leftarrow \emptyset$}
    
    \FOR{assignment in assignments}
        \IF{features $i, j\in$ assignment not yet encountered} 
            \STATE{correspondence.$append$(($i, j$))}
        \ENDIF

        \IF{$length$(correspondence) $=$ shortest} 
            \RETURN {correspondence}
        \ENDIF
    \ENDFOR
    \RETURN{ambiguous correspondence} 
    \end{algorithmic}
    \label{multi_point}
\end{algorithm}
\end{figure}

Once the marker centers are determined for both frames, the 3D locations of features are triangulated from corresponded feature pairs and the camera intrinsics/extrinsics. A core benefit of stereo-based tracking is that the 3D location is triangulated from a single 2D feature pair, which is not necessarily correlated to other features in the scene. Camera projection matrices are determined by a stereo calibration.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\columnwidth]{figures/assoc_cropped.png}
%     \caption{geometric alignment of features based on minimizing pixel-to-pixel distances.}
%     \label{fig:feature_corres}
% \end{figure}
An alignment method is used to match feature correspondences based on relative geometries, as summarized in Algorithm~\ref{alg:corr}. For each feature in the first frame, the offset to every other feature is calculated and stored as a relative geometry. Then, for each feature in the second frame, the association that minimizes the pixel difference for that feature's relative geometry to the rest of the features is found. Only one correspondence is assumed to be correct, so if there are several correspondences from a feature in the first frame to features in the second frame, only the association with the lowest pixel error is considered. This geometric matching approach is predicated on the assumption that the cameras have almost the same orientation, similar extrinsics, and are not far enough apart to have vastly different views of features. Using RANSAC~\cite{fischler1981random} to estimate the homography between the two images fails to determine correspondences, as keypoint detectors, such as ORB~\cite{rublee2011orb} and SURF~\cite{bay2008speeded}, do not locate enough keypoints to correctly determine correspondence in sparse low exposure time images.

Once the feature correspondence is found, it is used along with the cameras' projection matrices and detected features to iteratively triangulate the 3D locations of the features. This approach is akin to the Iterative-LS method described by \cite{Hartley1997Triangulation}. Triangulation allows a marker's 3D translation to be tracked by the cameras individually. 

Extracting a desired object's pose for multiple objects from the 3D locations of the markers is left for future work, as our current approach only supports tracking a single rigid geometry comprised of three markers arranged in a scalene triangle. This is due to the fact that when there are multiple rigid bodies, it is non-trivial to determine which 3D points belong to a specific rigid body, especially when the 3D features belonging to rigid bodies are occluded. 

The points are corresponded to a known stored geometry through an exhaustive search that correlates the longest edge between points to the longest edge in the stored scalene triangle. Once the point correspondence is determined, the translation and rotation is determined with a least-squares correspondence of two 3D point sets~\cite{arun1987least}.

\section{Experiments}

To evaluate our Mobile MoCap system, we designed a series of experiments using AprilTags with a monocular camera as a benchmark. An overview of the experimental setup is illustrated in Fig.~\ref{fig:experimental_setup}.

\begin{figure}[t]
    \vspace{0.50em}
    \centering
    \includegraphics[width=\columnwidth]{figures/mobile_mocap_system_setup.png}
    \caption{Experimental setup for benchmarking the performance of the linear rail system against visual fiducial markers. \textit{Inset}: Relative pose transforms from the motion capture setup.}
    \label{fig:experimental_setup}
    \vspace{-1.0em}
\end{figure}

\subsection{Experimental Setup}

% overall setup, include picture of testing setup here with everything labeled
% for consistent naming, I'm calling the moving camera assembly the "Mobile MoCap," the flat plate at the end of the rail the "marker plate," 

For the automated evaluation of our Mobile MoCap system, we designed a robotic testing platform that consists of a servo-controlled camera assembly that travels along a linear rail (LoPro), as shown in Fig.~\ref{fig:experimental_setup}). The linear rail, belt-driven with a high-precision stepper motor, allows for a means of reliably moving the Mobile MoCap assembly at a fixed rate with high repeatability. The architecture and system details for the rail are detailed in \cite{hanson2022occluded}. To track the position of the moving Mobile MoCap assembly relative to a static world frame, we used an OptiTrack V120:Trio (Natural Point), which recorded data at 120 Hz during all trials and serves as the ground truth for our experiments. In order to test the robustness of our system, we ran experiments where the Mobile MoCap assembly was static and moving.

Using spherical (3D) retroreflectors would give the Mobile MoCap system a non-negligible advantage over one that relied on flat (2D) fiducial markers in instances when the assembly's onboard cameras point at the markers from non-orthogonal viewing angles. Therefore, we opted to test our system only using the flat, paper-based variant of retroreflectors to allow for a fairer comparison between retroreflective marker and AprilTag technologies. To accomplish this, we first custom-designed and laser-cut a flat backing plate to adhere both types of 2D markers. The backing plate was mounted to the distal end of the linear rail in a vertical orientation perpendicular to the rail's axis of motion. We opted to produce three 30mm-wide, circular markers from a sheet of retroreflective paper (Qualisys) using a die-cutting machine (Cricut) and arranged them asymmetrically on the backing plate (see Fig.~\ref{fig:title_teaser}a); together, the trio of discs cover a cumulative surface area of about 2120 $\text{mm}^2$. We also downloaded a set of AprilTags from the publicly-available tag36h11 family, randomly selected 1 square-shaped tag to test with (tag ID\#20), proportionally scaled it to be 52mm wide, and printed it using a traditional office printer; in total, the AprilTag covered a surface area of 2704 $\text{mm}^2$. By forcing both styles of flat markers to live side-by-side on the same plane, we ensure comparable metrics between the two; in fact, given that the AprilTag is roughly 25 percent larger in surface area compared to the combined surface areas of the set of flat retroreflectors, we intentionally gave the AprilTag marker a theoretical advantage in being seen by more of the RGB camera's pixels.


\subsection{Static Trials}

As a base comparison point of estimation noise, we first tested the Mobile MoCap system at various distances and angles from the marker plate. The distances chosen were 0.9, 1.34, 1.78, and 2.23 m, and the angles chosen were 20, 10, and 0 degrees deflection from normal. Each angle was recorded for 2 seconds at 30 FPS on all three cameras. This was repeated 30 times for each angle and distance, for a total of 120 trials. 

\subsection{Constant Angular Velocity Trials}

Finally, we check the system robustness against rotational motion blur by rotating the Mobile MoCap assembly through a 40 degree range of motion with varying angular velocities. First, the Mobile MoCap assembly is fixed at a distance 1 m away from the marker plate. Then, the assembly is rotated through constant angular velocities of 0.05, 0.1, 0.2, and 0.4 rad/s. During this, the three cameras on the mobile assembly are recording at 30 Hz. Each angular velocity value is run 30 times, for a total of 120 trials. 

\subsection{Constant Linear Velocity Trials}

Next we test the tracking fidelity of our system while in motion. We moved the  back and forth along the rail at a constant velocities of 10, 20, 25, and 30 $\frac{cm}{s}$ starting at .9m and ending at 2.2m, which was repeated five times.  

\section{Results}
\label{sec:results}

To quantify estimation error in object tracking, we consider metrics related to both the position and orientation. Position error is calculated as the root mean squared error (RMSE) between the OptiTrack's (``ground truth'') observed 3D coordinate $(x_1,y_1,z_1)$ and its corresponding estimate $(x_2,y_2,z_2)$ at time $i$. Given that the OptiTrack operates at a significantly faster frequency rate than the AprilTag and Mobile MoCap tracking methods, every sequence of estimates had to be synchronized to a common rate. This yields $n$ total samples and a RMSE metric on 3D position expressed as:
\begin{equation*}
    p_{\mathrm{rmse}} = \sqrt{\sum_{i=1}^{n}\frac{(x_{1,i}-x_{2,i})^2+(y_{1,i}-y_{2,i})^2 + (z_{1,i}-z_{2,i})^2}{n}}.
\end{equation*}

To evaluate 3D rotation error, we consider an inner dot product of the ground truth and estimated unit quaternions. This error function can be viewed as a distance between orientations~\cite{huynh2009metrics}. Let OptiTrack's ground truth frame orientation be $\mathbf{q}_{\mathrm{1}}$, and the estimated quaternion as $\mathbf{q}_{\mathrm{2}}$, then the error in rotation is:
\begin{equation*}
    q_{\mathrm{err}} = \frac{\arccos{(|\mathbf{q}_{\mathrm{1,i}} \cdot \mathbf{q}_{\mathrm{2,i}}|)}}{n},
\end{equation*}
where $q_{\mathrm{err}}\in[0,\pi]$.

For each of row of measurements reported in Tables \ref{tab:static_april}-\ref{tab:dynamic_ang_vel}, these above metrics are averaged over multiple trials conducted for the same parameter set. The values for $x$, $y$, and $z$ are the mean positional error in each of the three axes plus-minus the standard deviation of that axes' relative error.

\subsection{Static}

\begin{table}[t]
\caption{AprilTags -- Static Trials Performance}
\label{tab:static_april}
\centering
\footnotesize
\setlength\tabcolsep{2.5 pt} % Essential for spacing
\begin{tabular}{cc|c @{} cccc @{} c}
\toprule
$d\,(\mathrm{m})$ & $a\,(\mathrm{rad})$ & $p_{\mathrm{rmse}}\,(\mathrm{cm})$ & $x\,(\mathrm{cm})$ & $y\,(\mathrm{cm})$ & $z\,(\mathrm{cm})$ & $q_{\mathrm{err}}\,(\mathrm{rad})$ \\
\midrule
\multirow{3}{*}{0.9} & 0.0 & 5.11 & -4.18 $\! \pm \!$ 0.1 & 7.46 $\! \pm \!$ 0.1 & -2.18 $\! \pm \!$ 0.6 & 0.19 \\
& 0.17 & 5.52 & -4.23 $\! \pm \!$ 0.3 & 7.67 $\! \pm \!$ 0.7 & 0.54 $\! \pm \!$ 3.7 & 0.18 \\ 

&-0.34 & 7.57 & -3.54 $\! \pm \!$ 2.1 & 9.02 $\! \pm \!$ 0.5 & 8.31 $\! \pm \!$ 2.0 & 0.11 \\ \hline

\multirow{3}{*}{1.3} & 0.0 & 8.82 & -1.05 $\! \pm \!$ 0.2 & 6.50 $\! \pm \!$ 0.1 & -13.67 $\! \pm \!$ 1.7 & 0.39 \\

 & 0.17 & 6.89 & -1.67 $\! \pm \!$ 1.4 & 6.57 $\! \pm \!$ 0.8 & -4.71 $\! \pm \!$ 8.5 & 0.36 \\ 

&-0.34 & 8.38 & -0.93 $\! \pm \!$ 1.8 & 8.19 $\! \pm \!$ 0.8 & 9.25 $\! \pm \!$ 7.3 & 0.15 \\ \hline

\multirow{3}{*}{1.8} & 0.0 & 16.61 & 2.80  $\! \pm \!$ 0.9 & 6.16 $\! \pm \!$ 0.4 & -26.62 $\! \pm \!$ 8.5 & 0.43 \\

 & 0.17 & 22.37 & 2.94 $\! \pm \!$ 3.4 & 6.91 $\! \pm \!$ 1.0 & -34.23 $\! \pm \!$ 16.2 & 0.42 \\ 

&-0.34 & 7.99 & -3.19 $\! \pm \!$ 2.9 & 8.54 $\! \pm \!$ 1.3 & -3.84 $\! \pm \!$ 9.2 & 0.31 \\ \hline

\multirow{3}{*}{2.2} & 0.0 & 53.43 & 9.29 $\! \pm \!$ 1.7 & 7.96 $\! \pm \!$ 0.6 & -90.40 $\! \pm \!$ 15.5 & 0.65 \\

 & 0.17 & 52.75 & 8.58 $\! \pm \!$ 4.1 & 8.04 $\! \pm \!$ 0.8 & -87.76 $\! \pm \!$ 22.2 & 0.64 \\
    
    & -0.34 & 16.48 & 1.98 $\! \pm \!$ 4.2 & 7.76 $\! \pm \!$ 1.2 & -0.07 $\! \pm \!$ 27.1 & 0.27  \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{table}[t]
\caption{Mobile MoCap -- Static Trials Performance}
\label{tab:static_mobile_mocap}
\centering
\footnotesize
\setlength\tabcolsep{1.9 pt} % Essential for spacing
\begin{tabular}{cc|c@{}ccccc@{}c}
\toprule
$d\,(\mathrm{m})$ & $a\,(\mathrm{rad})$ & $p_{\mathrm{rmse}}\,(\mathrm{cm})$ & $x\,(\mathrm{cm})$ & $y\,(\mathrm{cm})$ & $z\,(\mathrm{cm})$ & $q_{\mathrm{err}}\,(\mathrm{rad})$ \\
\midrule
\multirow{3}{*}{0.9} & 0.0 & 1.91 & $-0.79 \pm 0.0$ & $6.48 \pm 0.0$ & $0.05 \pm 0.0$ & 0.15 \\
& 0.17 & 2.70 & $-0.48 \pm 0.7$ & $6.76 \pm 0.7$ & $1.83 \pm 4.1$ & 0.15 \\
& -0.34 & 6.39 & $0.85 \pm 2.2$ & $8.10 \pm 0.5$ & $10.22 \pm 2.3$ & 0.12 \\ \hline
\multirow{3}{*}{1.3} & 0.0 & 0.96 & $0.89 \pm 0.1$ & $4.65 \pm 0.1$ & $-2.67 \pm 0.1$ & 0.17 \\
& 0.17 & 1.97 & $1.05 \pm 1.5$ & $5.09 \pm 1.0$ & $-0.22 \pm 5.8$ & 0.17 \\
& -0.34 & 5.89 & $2.01 \pm 2.3$ & $6.89 \pm 0.9$ & $8.76 \pm 7.6$ & 0.18 \\ \hline
\multirow{3}{*}{1.8} & 0.0 & 0.34 & $3.05 \pm 0.1$ & $3.58 \pm 0.0$ & $-5.60 \pm 0.07$ & 0.17 \\
& 0.17 & 1.25 & $3.40 \pm 1.5$ & $4.01 \pm 1.1$ & $-3.67 \pm 10.8$ & 0.16 \\
& -0.34 & 6.51 & $3.39 \pm 1.8$ & $6.16 \pm 1.2$ & $9.98 \pm 7.7$ & 0.15 \\ \hline
\multirow{3}{*}{2.2} & 0.0 & 1.11 & $2.30 \pm 0.1$ & $2.56 \pm 0.1$ & $-8.20 \pm 0.4$ & 0.29 \\
& 0.17 & 0.60 & $2.50 \pm 2.6$ & $3.56 \pm 2.2$ & $-4.27 \pm 10.9$ & 0.42 \\
& -0.34 & 10.08 & $-11.05 \pm 8.3$ & $9.06 \pm 3.5$ & $-28.25 \pm 17.6$ & 0.93 \\
    \bottomrule
    \end{tabular}
\end{table}

The results in Tables~\ref{tab:static_april}and~\ref{tab:static_mobile_mocap} are quite promising in favor of our system. Every $p_{rmse}$ value for the Mobile MoCap system is lower than its corresponding value for AprilTags, with sub-two centimeter error at 0.9 m from the marker plate at zero degrees. One potential outlying value is a $p_{rmse}$ value of 0.6 cm at 2.2 m and 0.17 radians in Table~\ref{tab:static_mobile_mocap}, which is lower than the preceding value, whereas for every other set of three values at each distance with increasing angular deflection, the p value shows a positively increasing trend. 

\subsection{Dynamic Angular Velocity}


\begin{table}[t]
\caption{Dynamic Angular Velocity Trials Performance}
\label{tab:dynamic_ang_vel}
\centering
\footnotesize
\setlength\tabcolsep{4 pt} % Essential for spacing
\begin{tabular}{c|cccc}
\toprule
 \multirow{2}{*}{$a_{vel}\,(\mathrm{rad/s})$} & \multicolumn{2}{c}{AprilTag} & \multicolumn{2}{c}{Mobile MoCap} \\
 & $p_{\mathrm{rmse}}\,(\mathrm{cm})$ & $q_{\mathrm{err}}\,(\mathrm{rad})$  & $p_{\mathrm{rmse}}\,(\mathrm{cm})$ & $q_{\mathrm{err}}\,(\mathrm{rad})$  \\
\midrule
0.05 & 8.86 & $0.165 \pm 0.09$ & 3.88 & $0.156 \pm 0.05$ \\
0.1 & 9.07 & $0.130 \pm 0.07$ & 5.02 & $0.143 \pm 0.05$ \\
0.2 & 9.16 & $0.146 \pm 0.08$ & 5.53 & $0.142 \pm 0.05$ \\
0.4 & 8.90 & $0.162 \pm 0.10$ & 5.04 & $0.155 \pm 0.09$ \\
    \bottomrule
    \end{tabular}
\end{table}

The results in Table~\ref{tab:dynamic_ang_vel} show a significant trend. In all cases of angular rotation, the Mobile MoCap system offers a smaller positional error than AprilTags. Although at times the improvement is marginal, the results were collected at 1 m distance, where the visual recognition of the AprilTag is still reliable. At further distances, up to the maximum range of the system, the Mobile MoCap performance is likely superior. The orientation error also trends in favor of the Mobile MoCap system, although the performance increase at this distance is still within a standard deviation of the AprilTag. For all systems, increasing $a_{vel}$ is directly correlated with an increase in pose error.

\subsection{Dynamic Linear Velocity}

\begin{table}[t]
\caption{Dynamic Linear Velocity Trials Performance}
\label{tab:dynamic_lin_vel}
\centering
\footnotesize
\setlength\tabcolsep{4 pt} % Essential for spacing
\begin{tabular}{c|cccc}
\toprule
 \multirow{2}{*}{$l_{vel}\,(\mathrm{cm/s})$} & \multicolumn{2}{c}{AprilTag} & \multicolumn{2}{c}{Mobile MoCap} \\
 & $p_{\mathrm{rmse}}\,(\mathrm{cm})$ & $q_{\mathrm{err}}\,(\mathrm{rad})$  & $p_{\mathrm{rmse}}\,(\mathrm{cm})$ & $q_{\mathrm{err}}\,(\mathrm{rad})$  \\
\midrule
10 & 9.09 & $0.170 \pm 0.10$ & 2.62 & $0.140 \pm 0.03$ \\
20 & 31.01 & $0.332 \pm 0.15$ & 1.60 & $0.178 \pm 0.07$ \\
25 & 14.93  & $0.190 \pm 0.12$ & 2.35 & $0.147 \pm 0.06$ \\
30 & 23.8 & $0.256 \pm 0.16$ & 2.10 & $0.145 \pm 0.07$ \\
    \bottomrule
    \end{tabular}
\end{table}

The values in Table~\ref{tab:dynamic_lin_vel} show some distinct trends. As the constant velocity values increase, the $p_{rmse}$ values for AprilTag show an increasing trend, while the $p_{rmse}$ values for Mobile MoCap stay more or less the same. This is an incredibly fruitful result, that suggests Mobile MoCap is extremely robust to motion blur. Additionally, the normalized $q_{err}$ values indicating orientation error are on average larger for AprilTag than they are for Mobile MoCap. These results demonstrate the full potential of the Mobile MoCap system under exposure to significant motion blur. Finally, note that the standard deviations in position and orientation errors are significantly less than those of the ApriTag method.

\section{Discussion}

Fiducial markers, such as AprilTags~\cite{Olson2011April,Wang2016April2}, are limited by the minimum resolvable distance between points in an image. As AprilTags are based on a lexographic coding system, the detection pipeline aims to detect line segments and quads within a target area. At long distance, especially when the camera has low resolution or poor focus, the number of pixels occupied by a quad is small, thus decreasing the likelihood of detection. The Mobile MoCap system overcomes this challenge by relying on simpler geometric features that are recognized individually, rather than in the context of a larger grid encoding.

The Mobile MoCap system is not without limitations. Unlike AprilTags, there is no inherent association between marker configuration and object identifier (ID). It is possible to associate a marker geometry to a specific ID, but this is not yet a built-in feature, unlike many fiducial marker libraries. Similar to other more expensive systems, our system is also subject to noise from highly reflective surfaces, such as metallic screw heads or reflective trim on articles of clothing. These items can be falsely detected as retroreflectors at close distances. Most indoor lighting is provided by LED or fluorescent lights spanning the visible light spectrum, resulting in low noise from indoor lights. However, the sun provides a broader spectrum, and incidental sunlight can be falsely detected as a marker. The current iteration of this system is best suited for indoor use, or outdoor use during the night. Nevertheless, our results demonstrate that our system is functional and accurate in dynamic environments where more expensive motion tracking systems optimally operate.

\section{Conclusions}

In this work we presented a novel system architecture for an inexpensive, ultra-portable motion capture system. We demonstrated its superior performance to fiducial markers in position and orientation accuracy. In future iterations of this work, we plan to exchange the camera system for a system capable in the Short Wave IR range, allowing for robust performance both inside and outside. Additionally, this system will be mounted to a variety of mobile robots to explore multi-vehicle tracking. Our system and methods empirically demonstrate motion capture systems can be made in cost-effective formats while still providing excellent localization accuracy in robot-centric contexts.

                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Acknowledgment}
The authors would like to thank Alina Spectorov for aid in debugging the Mobile MoCap software. This research has been supported in part by Defense Advanced Research Projects Agency (DARPA) under HR0011-22-2-0004. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. Approved for Public Release, Distribution Unlimited.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references}
\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths

\end{document}