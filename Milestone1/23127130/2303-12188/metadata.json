{
    "arxiv_id": "2303.12188",
    "paper_title": "Toward Accurate Interpretable Predictions of Materials Properties within Transformer Language Models",
    "authors": [
        "Vadim Korolev",
        "Pavel Protsenko"
    ],
    "submission_date": "2023-03-21",
    "revised_dates": [
        "2023-08-03"
    ],
    "latest_version": 1,
    "categories": [
        "cond-mat.mtrl-sci",
        "physics.comp-ph"
    ],
    "abstract": "Property prediction accuracy has long been a key parameter of machine learning in materials informatics. Accordingly, advanced models showing state-of-the-art performance turn into highly parameterized black boxes missing interpretability. Here, we present an elegant way to make their reasoning transparent. Human-readable text-based descriptions automatically generated within a suite of open-source tools are proposed as materials representation. Transformer language models pretrained on 2 million peer-reviewed articles take as input well-known terms, e.g., chemical composition, crystal symmetry, and site geometry. Our approach outperforms crystal graph networks by classifying four out of five analyzed properties if one considers all available reference data. Moreover, fine-tuned text-based models show high accuracy in the ultra-small data limit. Explanations of their internal machinery are produced using local interpretability techniques and are faithful and consistent with domain expert rationales. This language-centric framework makes accurate property predictions accessible to people without artificial-intelligence expertise.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.12188v1"
    ],
    "publication_venue": "17 pages, 5 figures, 1 table",
    "doi": "10.1016/j.patter.2023.100803"
}