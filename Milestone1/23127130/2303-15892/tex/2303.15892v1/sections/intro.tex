\section{Introduction}
% Generating 3D face~\cite{DBLP:conf/iclr/GuL0T22,chan2022efficient,DBLP:conf/cvpr/XuPYSZ22,DBLP:conf/cvpr/Or-ElLSSPK22,DBLP:conf/cvpr/XueLSL22,DBLP:conf/cvpr/ZhangZGZPY22,DBLP:journals/corr/abs-2110-09788} is a challenging task in both computer vision and computer graphics, which requires the 3D models to display photo-realistic texture and contain detailed geometry. Furthermore, many multimedia applications, such as 3D games and movies, not only require to model the frontal face, but also the complete head in 3D space, to enable view synthesis from surrounding perspectives. Whereas few works focus on the topic, this work is devoted to full head generation.

Generating high-fidelity 3D heads poses a significant challenge in the domains of computer vision and graphics, with a broad range of applications, including 3D games and movies. However, existing approaches~\cite{DBLP:conf/iclr/GuL0T22,chan2022efficient,DBLP:conf/cvpr/XuPYSZ22,DBLP:conf/cvpr/Or-ElLSSPK22,DBLP:conf/cvpr/XueLSL22,DBLP:conf/cvpr/ZhangZGZPY22,DBLP:journals/corr/abs-2110-09788,ploumpis2019combining} primarily concentrate on frontal faces, lacking the capacity of rendering the side or back views, thus their applications are significantly limited. In this work, we aim to address the issue and generate complete 3D heads with photo-realistic rendering capabilities.

%Existing approaches~\cite{DBLP:conf/iclr/GuL0T22,chan2022efficient,DBLP:conf/cvpr/XuPYSZ22,DBLP:conf/cvpr/Or-ElLSSPK22,DBLP:conf/cvpr/XueLSL22,DBLP:conf/cvpr/ZhangZGZPY22,DBLP:journals/corr/abs-2110-09788} mainly focus on frontal faces, while full-head models are needed for various applications such as 3D games and movie, requiring view synthesis from surrounding perspectives. Therefore, this work is devoted to full head generation with photo-realistic rendering. %Note that, we focus on directly generating heads for rendering, rather than generating albedos and meshes in computer graphics to form the heads.


% since they do not contain complete heads and can only be observed from a narrow field of view. Various generated full heads can be used in more extensive 3D space, even in the metaverse, while few works focus on the topic. Therefore, this work is devoted to full head generation, enabling view synthesis from surrounding perspectives.
\begin{figure}[t]
\centering
% 	\subfigure{
    	\begin{minipage}[h]{1.0\linewidth}
    	\centering
    	\includegraphics[width=1.0\linewidth]{figures/fig1_deca.png}
    	\small (a)~Example of parametric models, DECA~\cite{feng2021learning}
    	% \includegraphics[width=.95\textwidth]{figures/fig1_i3dmm.png}
    	% \scriptsize (b)~Example of 3D generator trained by 3D data, i3DMM~\cite{yenamandra2021i3dmm}
    	\includegraphics[width=1.0\linewidth]{figures/fig1_eg3d.png}
    	\small (b)~Example of 3D-aware GAN trained by 2D data, EG3D~\cite{chan2022efficient}
    	\includegraphics[width=1.0\linewidth]{figures/fig1_ours.png}
    	\small (c)~Visualization of Our Head3D
	    \end{minipage}%
	\centering
	\caption{The illustration of existing head/face generation methods and our presented Head3D. All results are rendered into RGB images and geometric form, in both frontal and side views.}
% 	We also give the example results in Appendix~\ref{example}}
\vspace{-2em}
\label{img:beginning}
\end{figure}

%predicting 3D heads from single view or multi-view images~(reconstruction methods)

Current 3D head generation methods fall into two categories: non-parametric head models and parametric head models. Non-parametric methods~\cite{ramon2021h3d,Grassal_2022_CVPR,wang2022morf,burkov2022multi} predict 3D heads from single view or multi-view images, usually considered as \textbf{``reconstruction''} methods. These methods face challenge in reproducing heads that do not exist, limiting the variety of generated results. Moreover, limited-view reconstruction results in insufficient details due to the absence of visible perspectives.
%In parametric models~(Figure~\ref{img:beginning} (b)), explicit 3D morphable models~\cite{booth20163d,paysan20093d,booth20173d,gerig2018morphable,li2017learning,tran2019towards,cao2013facewarehouse,patel20093d,ploumpis2020towards,dai20173d} represent identities, textures and expressions by low dimensional PCA parameters, which are learned from 3D scans with different expressions and identities. Similarly, implicit 3D parametric models~\cite{yenamandra2021i3dmm} typically employ an auto-decoder to learn the decoupled parameters from 3D scans. However, these methods rely on a vast of expensive 3D scans and are hard to express intricate texture and geometry. 
Parametric models~\cite{booth20163d,paysan20093d,booth20173d,gerig2018morphable,li2017learning,tran2019towards,cao2013facewarehouse,patel20093d,ploumpis2020towards,dai20173d,yenamandra2021i3dmm,giebenhain2022learning}~(Figure~\ref{img:beginning} (a)) utilize decoupled parameters to represent heads, which rely on a vast of expensive 3D scans and are hard to express intricate texture and geometry. 
Learning 3D head generation solely from images can be a more cost-effective approach to address the challenging task, and it has the potential to generate richer identities and higher-fidelity outcomes.
Recently 3D-aware GANs~\cite{DBLP:conf/iclr/GuL0T22,DBLP:conf/cvpr/Or-ElLSSPK22,chan2021pi,chan2022efficient} are learned from easily accessible in-the-wild images to generate 3D frontal faces with photo-realistic rendering and high-fidelity shapes~(Figure~\ref{img:beginning} (b)). These methods can also be used in generating heads theoretically. However, accurate camera poses are crucial for 3D consistency in these methods~\cite{chan2022efficient}, while estimating them without landmarks on the back is challenging. Hence, our objective is to address the aforementioned challenges and devise an approach capable of generating complete heads solely by training on limited images.


Motivated by the high-fidelity 3D face generator~\cite{chan2022efficient}, a question arises: can we use it as prior knowledge to generate full heads? We answer this question with \emph{YES}, but two challenges must be addressed. First, \emph{how to extract the 3D priors of heads?} The face prior is represented implicitly, making it difficult to integrate the face topology directly with the head in a re-topological manner akin to computer graphics. A straightforward idea is to directly fine-tune the 3D-aware generation model on full head data. However, fine-tuning the pre-trained model with limited data, \eg, several thousand images, often leads to mode collapse or over-fitting~\cite{gal2022stylegan}, resulting in limited face diversity and low quality. Second, \emph{how to bridge the domain gap between the frontal faces and the hair?} 
Obviously, the frontal face and back of the haired heads share two related but different distributions, respectively. 
Moreover, obtaining back-view images with accurate view direction is challenging, resulting in highly imbalanced quantities in these two distributions. This poses an extra requirement for the discriminator in the 3D-aware GAN model to not only distinguish real/fake samples, but also frontal/back views.
% It is difficult for a single discriminator to distinguish that the images from front and back view are true or fake together.


% Our goal is to train the model with a small amount of multi-view images and then complete the faces into full heads. In addition to collecting a large number of expensive scans or multi-view head images, we seek a cheaper yet effective idea. 

In this work, we propose Head3D for diverse full head generation that builds on a current SOTA 3D face generative model, \ie, EG3D~\cite{chan2022efficient}. Our goal is to transfer the facial prior generated by EG3D onto full heads with limited 3D data. 
%To address the first challenge, we systematically investigate the role of the tri-plane representation, observing that features on the $xy$ plane principally determine the character's identity, while features on the other planes represent the geometry, \ie, whether it is a face or a complete head. Based on this observation, we proposed a tri-plane feature distillation framework for detailed identity transfer, maintaining the quality of the frontal face while completing the back head. 
To extract the 3D prior, we conduct a systematic analysis of the tri-plane representation and observe a decoupling between the geometric and identity information in this representation. Based on this observation, we propose a tri-plane feature distillation framework that aims to preserve the identity information while completing the head geometry.
To address the second problem, we design dual-discriminators for frontal faces and the back of heads respectively, which not only frees the discriminator from distinguishing frontal/back images and also inherits the strong capability of discriminators from EG3D.

%Our experiments demonstrate that the proposed Head3D model can produce superior results compared to previous approaches, despite being trained with only a small amount of multi-view images and prior knowledge. Examples of our model are shown in Figure~\ref{img:beginning} (d).


To evaluate the effectiveness of our Head3D, we conduct comparative analyses with the original EG3D~\cite{chan2022efficient} and other recent baselines. Our experiments demonstrate that the proposed Head3D model can produce superior results compared to previous approaches, despite being trained with only a small amount of multi-view images and prior knowledge. Examples of our model are shown in Figure~\ref{img:beginning} (c). Additionally, we perform ablation studies to validate the effectiveness of each component in Head3D.
% Experiments on applications show the feasibility of our proposed model in multimedia applications.

The main contributions are summarized as follows: 
\textbf{(a)} We propose the novel Head3D for generating full heads with rich identity information, photo-realistic renderings, and detailed shapes with limited 3D data. \textbf{(b)} We investigate the effectiveness of tri-plane features, and propose tri-plane feature distillation to enable the transfer of identity information onto the head templates. \textbf{(c)} We propose a dual-discriminator approach to address the distribution gap and quantity imbalance between front-view and back-view images, thereby ensuring the quality of generated heads.


% \begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt]
%     \item We propose the novel Head3D for generating full heads with rich identity information, photo-realistic renderings and detailed shapes with scarce 3D data.
%     \item We investigate the efficacy of tri-plane features, and propose tri-plane feature distillation to enable transfer of identity information onto the head templates.
%     \item We propose a dual-discriminator approach to address the distribution gap and quantity imbalance between front-view and back-view images, thereby ensuring the quality of generated heads.
% \end{itemize}





% \section{Introduction}
% 3D-aware face Generative models achieve remarkable progress recently, realizing high-resolution and photo-realistic renderings and fine geometry representations. However, these generated faces are difficult to be directly applied to multimedia applications, e.g, games and movies, since they do not contain complete heads, which can only be observed from narrow field of view. Therefore, this work is devoted to complete haired head generation, enabling view synthesis from surrounding perspective.


% \begin{figure}[t]
%   \centering
%   \includegraphics[width=.5\textwidth]{figures/figure1}
%   \caption{Show the generation results of the complete human head, include mesh and rendering results, and compare with eg3d.\textbf{(To be change.)}}
%   \label{img:beginning}
% \end{figure}



% Certainly, existing 3D-aware generative models can also generate complete heads. However, training these models requires a vast of multi-view head images. The acquisition of those large number of images required to train the 3D-aware GANs can be very expensive. In addition, in-the-wild back look images are difficult to predict the pose compared to the face with much key points. Note that, parametric models, including 3DMM models, are also able to generate full heads. Yet these methods rely on a large number of 3D scans, which are not as diverse and photo-realistic as these generative models. Besides, they do not contain hair. This work aims to provide some insights into full haired head generation through overcoming the limitations of the data with only pre-trained 3D-aware face generation model and a small amount of multi-view data.


% Our method is build upon EG3D~\cite{chan2022efficient}, a recent state-of-the-art GAN model which promotes multi-view consistency 3D face generation. There are two challenges: one is that training model with a small amount of 3D multi-view images usually leads to mode collapse. The other is that directly fine-tuning the pre-trained networks with these data results in incomplete head generation and 3D inconsistency. To address these challenges, we expect to find a way to achieve joint training on 2D faces and multi-view heads, combining learned faces with 3D heads.

% We expect to explore the role of each parts in EG3D via exchanging each part one by one between original EG3D and fine-tuned EG3D for head generation. We observe that features in the x-y plane determine the character's identification, while features in the other planes represent the geometry whether it is face or complete head. 

% On this basis, we propose EG3D-KD model for diverse haired head generation. In this methods, we first fine tune a model by multi-view head images, which generate limited complete haired heads. Then, we use original EG3D as teacher for tri-planes feature distillation to head generator, which can replace face upon complete heads. The GAN loss is employed for photo-realistic rendering. 


% It is possible to generate a complete heads in this approach, while the identification changes and the definition is low and occur many artifacts. To solve these problems, multi-supervision is used for this model to ensure the authenticity of the photo. Following StyleKD~\cite{DBLP:journals/corr/abs-2208-08840}, RGB loss and LPIPS loss are utilized to keep detailed texture. Besides, concerning domain gap between faces and back views of heads, we use two discriminators for generation of faces and hair heads, respectively. These method reduce mode collapse that face is covered by the texture of hair.

% The main contributions are summarized as follows: 
% \begin{itemize}
%     \item To our best knowledge, EG3D-KD is the first 3D-aware NeRF-based model able to generate a large variety of complete haired head.
%     \item We propose knowledge-distillation based pipeline achieves photo-realistic, diverse complete head generation.
%     \item We propose a two discriminators model for xxxx
% \end{itemize}
