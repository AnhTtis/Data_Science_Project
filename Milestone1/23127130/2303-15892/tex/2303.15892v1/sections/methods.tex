
\section{Methods}
With a small number of multi-view head images, our goal is to achieve complete heads generation with pre-trained face generator. We first review an efficient and effective tri-plane-based face generator EG3D (Sec~\ref{preliminary}). Then, aiming to extract the semantic information of the face as a prior knowledge, we explore how tri-planes represent the faces (Sec~\ref{Findings}). Afterward, we describe how to apply the prior knowledge to generate full heads (Sec~\ref{pipeline}). Finally, the training process is introduced in detail (Sec~\ref{training}). %Finally, we discuss the relation between our methods and current methods \textbf{(Sec~\ref{discuss})}.


\subsection{Revisit Tri-planes for 3D Generation}
\label{preliminary}
EG3D is a tri-plane-based generative model. Similar to StyleGAN~\cite{karras2019style,Karras_2020_CVPR}, mapping networks $M$ process the input latent code $z$ and camera params $p$ to style code $w$:
\begin{equation}
	w = M(z, p).
\end{equation}

Then, feature maps $F$ are extracted via a CNN-based generator $C$, yielding three planes which are rearranged orthogonally to form a tri-plane structure:
\begin{equation}
	F_{xy}, F_{xz}, F_{yz} = C(w),
\end{equation}
where $F_{xy}$, $F_{xz}$ and $F_{yz}$ are three planes in the front, side and top views, respectively. The tri-plane features contain semantic information of faces, determining their identities. 


\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/observation2.png}
	\caption{Visualization analysis of exchanging each plane of tri-plane. ID 1 and ID 2 are generated from the same latent code $z$ and camera parameter $p$.}
	\label{fig:short-a}
\end{figure}

\begin{table}[t]
% 	\scriptsize
\small
	\centering
	\setlength{\tabcolsep}{5mm}{\begin{tabular}{ccccc}
			\hline
			$F_{xy}$ & $F_{xz}$ & $F_{yz}$ & ID 1 & ID 2  \\
			\hline
			ID 2 & ID 1 & ID 1 &0.535 & 0.968\\
			ID 1 & ID 2 & ID 1 &0.977 & 0.534\\
			ID 1 & ID 1 & ID 2 &0.988 & 0.532\\
			ID 1 & ID 1 & ID 1 &1.000 & 0.531\\
			ID 2 & ID 2 & ID 2 &0.531 & 1.000 \\
			\hline
	\end{tabular}}
	\caption{Quantitative results with tri-plane exchanged. ID 1 and ID 2 denote two different identities generated from different $z$ by EG3D. We report the average results between every pairs between 100 identities.} 	
	\label{lab:observationa}
	\vspace{-1em}
\end{table}


Afterwards, features of locations are sampled from tri-planes, and are fed into decoder to output densities $\sigma$ and colors $c$. Then, volume rendering is performed to obtain a moderate resolution images $I_{raw}$.
\begin{equation}
I_{raw}=\int_0^{\infty} p(t) \boldsymbol{c}(\boldsymbol{r}(t), \boldsymbol{d}) d t, 
\end{equation}
where $p(t)=
\exp \left(-\int_0^t \sigma(\boldsymbol{r}(s)) d s\right) \cdot \sigma(\boldsymbol{r}(t))$, $\boldsymbol{r}(t)$ represents camera ray, and $t$ is the distance from camera.

Finally, a super-resolution module $S(\cdot)$ are performed to up-sample the $I_{raw}$ to results $I$ of high resolution:
\begin{equation}
\begin{aligned}
 I = S(I_{raw}).
\end{aligned}
\end{equation}


\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/observation.png}
	\caption{Visualization analysis of exchanging $xy$-plane between face generator and head generator.}
	\label{fig:assumpt}
 \vspace{-1em}
\end{figure}

\subsection{Dientangled Representation of Tri-planes}
\label{Findings}
Insufficient multi-view head images pose a challenge for generating diverse complete heads. However, leveraging the existing EG3D model, which can generate high-quality 3D frontal faces, offers a potential solution. Similar to StyleGAN, the semantic results in EG3D are determined by the style codes $w$, which generate tri-plane features as the only output of the CNN generator $C$. As a result, all the semantic information is contained within tri-planes. To enable knowledge transfer, we conduct an in-depth analysis of tri-planes to explore how prior knowledge is represented.

To investigate the role of each plane, we exchange each plane ($F_{xy}$, $F_{xz}$ and $F_{yz}$) between two different samples, and then generate faces from the newly integrated tri-plenes for visual analysis. The results are shown in Figure~\ref{fig:short-a}. We observe that the identities are exchanged along with $F_{xy}$ changed, while the identities remain the same after exchanging $F_{yz}$ and $F_{xz}$. Additionally, we conduct numerical experiments on 100 different identities. We exchange $F_{xy}$, $F_{xz}$ and $F_{yz}$ for each identify pair, respectively, and render the tri-planes to images from the frontal view. After that, the identity consistency between the exchanged tri-plane and the original identities are measured by Arcface~\cite{deng2019arcface}, and the results are calculated by averaging all the sample pairs. Table~\ref{lab:observationa} indicates that the identities are preserved when $F_{yz}$ and $F_{xz}$ changed, whereas they exchange when $F_{xy}$ are exchanged. Based on these observations, we preliminarily assume that $F_{xy}$ plays a crucial role in determining the identity information.

To further validate our assumption, we fine-tune a head generator from EG3D and perform experiments where $F_{xy}$ is exchanged between faces and heads. 
The instanced results, as shown in Figure~\ref{fig:assumpt}, indicate that exchanging $F_{xy}$ leads to identity transfer while do not influence the geometry whether to be heads or faces.
Our findings support the notion that $F_{xy}$ primarily controls the identity information, while the other two planes, $F_{yz}$ and $F_{xz}$, mainly represent the geometric shape of the head. These results provide evidence for our proposed decoupling between identity and geometric information in the tri-plane representation.

This observation is consistent with our expectation, as the $xy$-plane is aligned with the training images to better capture the facial features, while the other two planes are orthogonal to the frontal view, which mainly represent the depth and geometric information.
The experimental results and analysis sheds some light on the effect of tri-plane features in EG3D and offer a starting point for considering how to employ this prior for complete head generation.


\begin{figure*}[htpb]
	\centering
	\includegraphics[width=.85\textwidth]{figures/overview.png}
	\caption{The overview of our proposed Head3D. First, the frozen pre-trained EG3D is served as the teacher network $G^t$, while the fine-tuned head generator is student network $G^s$. Then tri-plane feature distillation and multi-level loss functions are employed for photo-realistic and diversy full head generator. Note that, the parameter $p$ for rendering and discriminator are not displayed in this framework.}
	\label{img:overview}
	\vspace{-1em}
\end{figure*}

\subsection{Tri-plane Distillation for 3D Head Generation}
\label{pipeline}
% There exists a problem: how to generate a complete head with facial prior? 
One potential approach to generate complete heads with the frontal face priors is to directly fine-tune the face generator with a few multi-view data. However, training with a small number of data can cause mode collapse or over-fitting, resulting in limited diversity and low quality. 
%The remaining question is how to generate the complete heads with the frontal face prior.
%A simple idea is to directly fine-tune the face generator with a few multi-view data. However, training with a small number of data may cause mode collapse or over-fitting, resulting in limited diversity. 
Inspired by computer graphic methods where the face is extracted from a photo and then registered with the head template model, we can also apply the tri-plane-based face priors to an implicit head template. 
Additionally, knowledge distillation~\cite{DBLP:journals/corr/HintonVD15} is a general method to transfer dark knowledge between models, allowing for the delivery of facial priors. Therefore, we propose a tri-plane feature distillation method, as illustrated in Figure~\ref{img:overview}. First, we fine-tune a full head generator from the pre-trained EG3D using a small amount of multi-view head images. Then, served as prior knowledge, $F_{xy}$ are transferred from the face generator to the head network via knowledge distillation to ensure consistency in identity. Benefiting from the powerful presentation capability of tri-planes, we are able to integrate the head template and face priors, enabling diverse full head generation.


%  \begin{figure}[t]
% 	\centering
% 	\includegraphics[width=.45\textwidth]{figures/display.png}
% 	\caption{Partial images in the H3DH dataset.}
% 	\label{img:display}
% \end{figure}


% Now, there exists a generator with the ability to generate variety of faces. The main target is to complete the head while keep the diversity of faces. We have learned how tri-plane features, the semantic information carrier in EG3D, represent prior knowledge. 


%Our goal is to complete the full heads without reducing face diversity. A simple approach is to learn the template of a complete head with small amount of multi-view iamges, and then use knowledge distillation to transfer the face information onto the head. There are two main chanllages: can face information be transfered well and how to keep the head intact.

%For the first question, as discussed in \sec{Findings}, identity informations are represented by $F_{xy}$ in tri-plane features. We conduct a toy experiment to evalute the feasibility of only distillating $F_{xy}$.

%Following, similar to the aforementioned experiments in \sec{Findings}, $F^t_{xy}$ and $F^s_{xy}$ are exchanged, then rendering images from the changed tri-plane for validation. It can be observed in Figure~\ref{fig:short-b} that identities exchanges along with $F_{xy}$ exchanged. Meanwhile, the geometry (being a face or a full head) remains unchanged. The experimental results verify the feasibility of facial transfermation.

%To train the tri-plane distillation framework, GAN loss is indispensable for achieving photo-realistic rendering results
In order to achieve photo-realistic rendering results, GAN loss is necessary in training the tri-plane distillation framework~\cite{xu2022mind}. 
The importance of camera poses in learning correct 3D priors in the discriminator has been highlighted in EG3D~\cite{chan2022efficient}.
%As indicated in EG3D, camera poses play a significant role in learning correct 3D priors in discriminator. 
For human heads, camera poses can be categorized into front and back perspectives to guide the generation of the face and back, respectively.
A large number of single-view face datasets are available to provide front images, whereas the number of back images is limited to half of the small multi-view dataset. As a result, there is an imbalance in the quantity and a distribution gap in perception between frontal and back head images. It is hard for a single discriminator to simultaneously guide fine-grained face generation and guarantee full head geometry in two different domains. Referring to this, we propose a dual-discriminator to ensure the generation quality and maintain the head completed. In our method, two discriminators guide the generation of front and back images, respectively. Moreover, two discriminators are alternated during training to mitigate the effects of imbalanced data.






\subsection{Model Training}
\label{training}

\noindent \textbf{Face Prior Transfer.}
As depicted in Figure~\ref{img:overview}, we present Head3D, a tri-plane distillation approach for generating diverse heads. Firstly, a head generator $G_s$ is fine-tuned from a pre-trained face generator $G_t$ with scarce multi-view head images. Then, to ensure identity consistency, the transformation of $F_{xy}$ is calculated by L2-norm, represented as:
\begin{equation}
	\begin{aligned}
		L_{kd} = {||F^t_{xy} - F^s_{xy}||}_2.
	\end{aligned}
\end{equation}
where $F^t_{xy}$ and $F^s_{xy}$ are $xy$-plane features generated by head generator $G_t$ and face generator $G_s$, respectively.


 \begin{figure*}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/results.png}
	\caption{Example results of our proposed Head3D, synthesized from three different views.}
	\label{img:show}
 \vspace{-1.5em}
\end{figure*}


\noindent \textbf{Dealing with Distribution Gap.}
To realize high-fidelity front and back head generation, we employ two discriminators $D_{front}$ and $D_{back}$ for training, which share the same network structure as the discriminator in EG3D.
The purpose of $D_{front}$ is to ensure the quality of faces, and it is initialized by the pre-trained face EG3D. $D_{back}$ is to ensure the integrity of the head, which is initialized by the fine-tuned head generator. Fed together with camera parameters $p$, both discriminators support the synthesis results $I$ be 3D consistent and in a similar distribution with ground truth images $I_{gt}$. The GAN loss function can be represented as:
\begin{equation}
	\begin{aligned}
			L_{gan} &	=
		\mathbb{E}[f(D(p, \mathbf{I})) 
		+  f\left(-D\left(p, \mathbf{I}_{gt}\right)\right) ] \\
		&+ \gamma{||\nabla D\left(p, \mathbf{I}_{gt}\right) ||}^2,
	\end{aligned}
\end{equation}
where $f(x)=-{\rm log}(1+{\rm exp}(-x))$, and $\gamma$ is a hyper-parameter in R1 regularization. Dual-discriminators are trained separately according to the view from face or back of heads, whose loss functions are represented as $L^{front}_{gan}$ and $L^{back}_{gan}$, respectively. In addition, the regularization coefficient $\gamma$ of dual-discriminator is different due to the imbalanced quantity of dataset.

\noindent \textbf{Detailed Texture Learning.}
StyleKD~\cite{xu2022mind} proposes that mapping network determines semantic informations of generators. Therefore, besides tri-plane feature distillation, it is also necessary to ensure that the output $w$ of the mapping network is consistent. Following StyleKD~\cite{xu2022mind}, a mapping loss is utilized:
\begin{equation}
	\begin{aligned}
		L_{map} = {||W^t - W^s||}_1.
	\end{aligned}
\end{equation}

Moreover, in order to learn more detailed faces, RGB loss and LPIPS loss~\cite{zhang2018unreasonable} are applied only to frontal neural renderings $I_{raw}$ and super-resolutioned results $I$. Referring to the quality of the generated results of original EG3D declines in the side view, this loss function is applied only to the rendering results within a certain range.
\begin{equation}
	\begin{aligned}
		L_{rgb} = \mathbb{I}(|\Delta p|\leq \tau)[{||I^t - I^s||}_1 + {||I^t_{raw} - I^s_{raw}||}_1],
	\end{aligned}
\end{equation}
\begin{equation}
	\begin{aligned}
		L_{lpips} &= \mathbb{I}(|\Delta p| \leq \tau){||F(I^t) - F(I^s)||}_1, 
		% &+ {||F(I^t_{raw}) - F(I^s_{raw})||}_1],
	\end{aligned}
\end{equation}
where $F(\cdot)$ is a well-trained frozen VGG~\cite{DBLP:journals/corr/SimonyanZ14a} to extract multi-scale semantic information from images. $\Delta p$ is the horizontal offset angle from center, and $\tau$ is a threshold. These two loss functions work in image space and perceptual space respectively, ensuring detailed consistency between the teacher and student.

Finally, the final loss function is weighted sum with the above loss functions:
\begin{equation}
	\begin{aligned}
		L &=	\lambda_{gan_{front}}L^{front}_{gan} + \lambda_{kd}L_{kd} + \lambda_{rgb}L_{rgb} \\
		&+\lambda_{lpips}L_{lpips}+\lambda_{map}L_{map}  
		+ \lambda_{gan_{back}}L^{back}_{gan},
	\end{aligned}
\end{equation}
where $\lambda_{*}$ denotes the weights of each loss functions.


% \subsection{Discussion}
% \label{discuss}
% Here we discuss the differences from previous head generation methods. Compared with the methods of predicting heads from single or multi-view images (more often considered as reconstruction rather than generation), the diversity and computational speed are limited, while our work can quickly generate non-existent heads from the latent space. Compared with parametric models, our method does not learn from 3D models and possesses higher rendering quality. Note that H3D-Net~\cite{ramon2021h3d} employs prior knowledge for high-fidelity 3D head reconstruction. However, compared with our face prior is learned from easily obtained single images, H3D-Net learns head prior from thousands of 3D head scans, which is much more expensive. Our proposed method allows for the generation of complete heads with a limited number of multi-view images and face priors, resulting in a low-cost approach with enhanced diversity and photo-realism.


