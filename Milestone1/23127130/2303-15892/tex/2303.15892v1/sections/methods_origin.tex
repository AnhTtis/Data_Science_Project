
\section{Methods}
In the condition of a small number of multi-view head images, our goal is to make an existing face generator can generate diverse full head. We first introduce our benchmark eg3d (\sec{preliminary}) and our findings on it (\sec{Findings}), then we detail our training pipeline (\sec{pipeline}).%, and finally is the training details (\sec{detail}).

\subsection{Revisit Tri-plane for 3D Generation}
\label{preliminary}
EG3D is a 3D-aware generative model $G(\cdot)$, which can be divided as four modules: Mapping networks $M(\cdot)$, CNN generator backbone $C(\cdot)$, rendering module $R(\cdot)$ and super-resolution module $S(\cdot)$. $M(\cdot)$ maps Gaussian noise $z$ and camera params $P$ to a style vector $w$, then $M(\cdot)$ generate Tri-planes features maps $F$ through feeding $w$ into $C(\cdot)$ in the modulated way, which can be represented as:
\begin{equation}
 F_{xy}, F_{xz}, F_{yz} = C(w) = C(M(z, P)),
\end{equation}
where $F_{xy}$, $F_{xz}$ and $F_{yz}$ are plane features in the front, left and top view, respectively.

Afterwards, features are sampled from tri-planes aggregated by summation, and are fed into decoder to extract density $\sigma$ and color$c$. Then, volume rendering is implemented to obtain a moderate resolution images.

\begin{equation}
I_{raw}=\int_0^{\infty} p_(t) \boldsymbol{c}(\boldsymbol{r}(t), \boldsymbol{d}) d t, 
\end{equation}
\\
where $p(t)=
\exp \left(-\int_0^t \sigma(\boldsymbol{r}(s)) d s\right) \cdot \sigma(\boldsymbol{r}(t))$ and $\boldsymbol{r}(t)=\boldsymbol{o}+t\boldsymbol{d}$ represents camera ray.


Finally, $S(\cdot)$ are performed to up-sample the neural rendering to the final images $I$ of high resolution.
\begin{equation}
\begin{aligned}
 I = G(Z) &= S(I_{raw}) \\
          &= S(R(sample(F_{xy}, F_{xz}, F_{yz}))),
\end{aligned}
\end{equation}
where $I_{raw}$ is the neurally rendered low-resolution results.

Aiming at obtaining 3D-consistent results, a dual-Discriminator is introduced. First, the raw neural rendering results $I_{raw}$ are linearly-interpolated upsampled and concated with the super-resolved images $I$ as six-channle results $I_{concat}$. Then, together with camera parameters $P$, $I_{concat}$ is fed into the dual-discriminator $D$, which encourges $I_{concat}$ is in a similar distribution with true images $I_{truth}$ and consistency between $I$ and $I_{raw}$. The GAN loss function can be represented as:
\begin{equation}
\begin{aligned}
	L_{GAN} &=  \mathbb{E}\left[f\left(-D\left(\mathbf{I}_{\mathrm{truth}}\right)\right) -\lambda{||\nabla D\left(\mathbf{I}_{\mathrm{truth}}\right) ||}^2\right] \\
	&+
	\mathbb{E}[f(D(P, I_{concat})))],
\end{aligned}
\label{ganloss}
\end{equation}
where $f(x)=-{\rm log}(1+{\rm exp}(-x))$, and $\lambda$ is a hyper-parameter in R1 regularization.



\begin{figure}[t]
	\centering
		\includegraphics[width=.45\textwidth]{figures/observation2.png}
		\caption{Observation of a model.}
		\label{fig:short-a}
\end{figure}

	%\caption{Visualization results of exchanging Tri-plane feature in EG3D. %The results are obtained from (a) the same model trained by FFHQ, and (b) %two different models trained by FFHQ and head rendered in the surrounding %views, respectively.}
%\label{fig:short}

\begin{figure}[t]
	\centering
	\includegraphics[width=.45\textwidth]{figures/observation.png}
	\caption{Observation between two models.}
	\label{fig:short-b}
\end{figure}

\subsection{Dientangled Representation of Tri-plane}
\label{Findings}

\begin{table}[t]
	\scriptsize
	\caption{Quantitative results of rendering of tri-planes exchanged. The ID similarity is caclulated via cosine distance of ArcFace embedding. ID 1 and ID 2 means identification generated from different $z$. $F_{\{\}}$ means the plane features come from which identifacation. Note that, the ID similarity is obtained by the average between 100 different ID.} 
	\centering
	\setlength{\tabcolsep}{5mm}{\begin{tabular}{ccccc}
			\hline
			$F_{xy}$ & $F_{xz}$ & $F_{yz}$ & ID 1 & ID 2  \\
			\hline
			ID 2 & ID 1 & ID 1 &0.535 & 0.968\\
			ID 1 & ID 2 & ID 1 &0.977 & 0.534\\
			ID 1 & ID 1 & ID 2 &0.988 & 0.532\\
			ID 1 & ID 1 & ID 1 &1.000 & 0.531\\
			\hline
	\end{tabular}}
	
	\label{lab:observationa}
\end{table}


\textbf{To DO: add qualitatively results.} 

As widely proposed in StyleGAN-similar generator, the semantic results are detetmined by the mapping network and latent code $z$. Specficallhy, semantic information is represented by the tri-plane features in EG3D. Therefore, we conduct some expreiments to evaluate the effect of triplanes.

Intuitively, we hypothesize that each plane in EG3D covers a different semantic information in representing a face. We examine this hypothesis qualitatively and quantitatively by exchanging each planes $F_{xy}$, $F_{xz}$ and $F_{yz}$ between two different tri-planes sperately. As shown in Figure~\ref{fig:short-a}, the identification is exchanged anlong with $F_xy$ exchanged. While $F_{yz}$ and $F_{yz}$ changed, the identifacation remains the same. However, focusing on the width of the face in the side view, it can be observed that the width of face changes. Particularly, ears of the human ID 2 are convert to be seen in the $F_{xz}$ and $F_{yz}$ ID 1 than in itself. The similar result appears in the hair of ID 1. Therefore, we further assume that $F_{xy}$ determines the identifacation, while $F_{xz}$ and $F_{yz}$ maintain geometric information in depth (the depth of the face, even more whether it is a face or full head).

To evaluate the assumation, we conduct experiments on two EG3D models with different kinds of geometries, one is the face generator and another is a full head generator. First, we fine-tune a full head generator $G_h$ from the pre-trained face generator $G_f$ via a small amount number of surrounding-view head images. Following, similar to the aforementioned experiments, features in the xy-planes are exchanged between $F^h_{xy}$ and $F^f_{xy}$ then rendered for validation. It can be observed in Figure~\ref{fig:short-b} that . The above experiment results and analysis sheds some light on the effect of tri-planes features in EG3D. Following, we will propose a novel pipeline of full head generation via multi-level knowledge distillation of EG3D.


 \begin{figure*}[t]
	\centering
	\includegraphics[width=.95\textwidth]{figures/overview.png}
	\caption{Overview\textbf{Not complete}}
	\label{img:overview}
\end{figure*}

\subsection{Tri-plane Distillation for 3D Head Generation}
\label{pipeline}
As discussed that EG3D trained by a small amount of multi-view images can only generate limited heads. However, there exists a diverse face generator.  Combining diverse faces with full head templete leads to diverse head generation, which is knowledge distillation proposed for. Knowledge distillation forces consistent outputs between teacher and student via dark knowledge transformation, include features, logits, results and so on. However, naively using knowledge distillation results in failure. We have shown that identifacation information are represented by triplanes features $F_{xy}$. Therefore, $F_{xy}$ is distillated to the head model for diverse face. 

As pipeline shown in Figure~\ref{img:overview}, we propose a multi-level knowledge distillation method for diverse head generation. First, as mentioned in Sec.~\ref{Findings}, a full head generator $G_h$ is fine-tuned from a pretrained face generator $G_f$ via multi-view head images. Then, student $G^s$ network is initialized to $G_h$, while $G_f$ is regared as the teacher network  $G^t$. Then, aiming at identifacation learning, the transformation of $F_xy$ is calculated by L2-norm, represented as:
\begin{equation}
	\begin{aligned}
		L_{KD} = {||F^t_{xy} - F^s_{xy}||}_2
	\end{aligned}
\end{equation}

In addition, it is proposed in StyleKD~\cite{xu2022mind} that mapping networks determins semantic informations of generater images. Therefore, besides triplane distillation, it is also necessary to ensure that the output $W$ of the mapping network is consistent. Following StyleKD~\cite{xu2022mind}, a mapping loss is utillzed:
\begin{equation}
	\begin{aligned}
		L_{MAP} = {||W^t - W^s||}_1
	\end{aligned}
\end{equation}


Moreover, in order to ensure the details of the generated results, RGB loss and LPIPS loss~\cite{zhang2018unreasonable} are employed in neural renderings $I_{raw}$ and super-resolutioned results $I$:
\begin{equation}
	\begin{aligned}
		L_{RGB} = {||I^t - I^s||}_1 + {||I^t_{raw} - I^s_{raw}||}_1
	\end{aligned}
\end{equation}

\begin{equation}
	\begin{aligned}
		L_{LPIPS} = {||F(I^t) - F(I^s)||}_1 + {||F(I^t_{raw}) - F(I^s_{raw})||}_1
		\end{aligned}
	\end{equation}
where $F(\cdot)$ is a well-trained frozen network, which extracts multiscale semantic information from an image. These two loss functions work in rgb space and perceptual space respectively, ensuring detailed consistency between teacher and student.

In training, in order to ensure photo-realistic results, it is necessary to add GAN loss shown in Eq.~\ref{ganloss}. In our framework, face dataset is employed for face generation, while the back-view rendering are used for guiding the generation of the full head. However, there exists domain gap between face images and head rendering from back views. It is difficult for one discriminator to simultaneously guide fine-grained face generation and guarantee full head geometry via datasets in two different domain. Therefore, we propose a dual-discrimitor training phase to guarante the generation quality.


Finally, the final loss function is weight sum with the above loss functions:
\begin{equation}
	\begin{aligned}
    L &=	\lambda_{GAN_{front}}L^{front}_{GAN} + \lambda_{KD}L_{KD} + \lambda_{RGB}L_{RGB} \\
    &+\lambda_{LPIPS}L_{LPIPS}+\lambda_{MAP}L_{MAP}  \\
    & + \lambda_{GAN_{back}}L^{back}_{GAN}
    \end{aligned}
\end{equation}
where $\lambda_{*}$ denotes the weight of each loss functions.


%\subsection{Training Details}
%\label{detail}

