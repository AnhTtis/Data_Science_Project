\section{Related Work}

\subsection{Head Generation Model}
As previously discussed in introduction, head generation can be categorized into two types. \textbf{``Reconstruction''} methods\cite{ramon2021h3d,wang2022morf,Grassal_2022_CVPR,burkov2022multi} primarily learn the correspondence between 3D data and images to establish prior knowledge. When presented with new images, these methods optimize the difference between the reconstructed results and the images to accomplish reconstruction. However, these methods are limited to the reconstruction of heads present in the given images, and are incapable of generating novel heads.
Explicit parametric 3D morphable models~\cite{blanz1999morphable,booth20163d,paysan20093d,booth20173d,gerig2018morphable,li2017learning,tran2019towards,cao2013facewarehouse,patel20093d,ploumpis2020towards,dai20173d,ploumpis2019combining} represent identities, textures and expressions by low dimensional PCA parameters, which are learned from 3D scans with different expressions and identities. Similarly, implicit 3D parametric models~\cite{yenamandra2021i3dmm,giebenhain2022learning,hong2022headnerf} typically employ an auto-decoder to learn the decoupled parameters from 3D scans. However, these methods are trained with large number of expensive 3D scans and are hard to express detailed texture and geometry. 
Recent Rodin~\cite{wang2022rodin} employs a diffusion model to learn head generation trained by images. However, this approach requires a large dataset consisting of 100,000 3D models for rendering multi-view images, and each identity is reconstructed with tri-plane alone for training the diffusion model. The employed dataset and training procedure are particularly expensive.
In contrast to these expensive methods, our aim is to generate diverse novel and high-fidelity heads in a cost-effective manner, utilizing only implicit face priors and a limited amount of multi-view images.

% Human heads possess different characteristics, including identity, shape and texture, yet they share similar topological structures. Blanz and Vette~\cite{blanz1999morphable} first propose that faces are able to be linearly represented by low-dimensional vector obtained by principal components analysis (PCA), named as 3D morphable model (3DMM). Further works achieve better modeling results by applying more refined models, or decoupling the modeling of expression and identity~\cite{booth20163d,paysan20093d,booth20173d,gerig2018morphable,li2017learning,tran2019towards,cao2013facewarehouse,patel20093d}. Li et al. propose FLAME~\cite{li2017learning}, a widely used head model, which is obtained from a large 3D scan dataset and combines linear shape space with articulated parts to generate 3D human heads from low-dimensional vectors. Furthermore, based on FLAME, DECA~\cite{feng2021learning} can reconstruct meshes from in-the-wild images, and produce detailed shapes. %i3dmm\cite{yenamandra2021i3dmm} introduces the implicit representation, which represents the faces as signed distance function (sdf) and color of a point in the space, and realizes the generation of hair on the basis of 3DMM. 
% These models can only generate human heads linearly interpolated between 3D scans, while a large number of 3D scans are much more expensive compared to 2D images. This work learns from 2D images and limited multi-view images to enable diverse human head generation.


\subsection{NeRF-based GAN Model}
Neural radiance field (NeRF)~\cite{mildenhall2021nerf} represents 3D models by implicit networks, whose outputs are density and color under the input of position and view direction, optimized by view reconstruction between volume rendering results and ground truths. Recent methods, integrating NeRF and GANs~\cite{goodfellow2020generative}, aim at learning 3D-aware generators from a set of unconstrained images~\cite{schwarz2020graf,chan2021pi,genova2020local,meng2021gnerf,niemeyer2021giraffe,sun2022fenerf,DBLP:journals/corr/abs-2206-10535}. GRAF~\cite{schwarz2020graf} verifies NeRF-based GAN can generate 3D consistent model with high fidelity rendering results. Pi-GAN~\cite{chan2021pi} introduces SIREN~\cite{sitzmann2020implicit} and a growing strategy for higher-quality image synthesis results. To improve rendering efficiency, several works volumetrically render a low-resolution feature, then up-sample them for high-resolution view synthesis under different 3D consistency constraints~\cite{DBLP:conf/iclr/GuL0T22,chan2022efficient,DBLP:conf/cvpr/XuPYSZ22,DBLP:conf/cvpr/Or-ElLSSPK22,DBLP:conf/cvpr/XueLSL22,DBLP:conf/cvpr/ZhangZGZPY22,DBLP:journals/corr/abs-2110-09788}. Particularly, EG3D~\cite{chan2022efficient} provides a hybrid 3D representation method, which first generates a tri-plane features, then sampled features are decoded and rendered for image synthesis. Pose-related discrimination is utilized for 3D consistency generation. Surrounding head images are prerequisite for these methods to achieve complete heads generation. 
However, it is difficult to predict the view directions of in-the-wild back head images, whereas pose-accurate multi-view images are expensive. Our Head3D extends EG3D to achieve complete head generation with limited multi-view head images by facial priors transfer via tri-plane feature distillation.

% \begin{figure*}
%   \centering
%   \begin{subfigure}{0.48\linewidth}
%     \includegraphics[width=0.9\linewidth]{figures/observation2.png}
%     \caption{Observation of a model.}
%     \label{fig:short-a}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.48\linewidth}
%      \includegraphics[width=0.9\linewidth]{figures/observation.png}
%     \caption{Observation between two models.}
%     \label{fig:short-b}
%   \end{subfigure}
%   \caption{Visualization results of exchanging Tri-plane feature in EG3D. The results are obtained from (a) the same model trained by FFHQ, and (b) two different models trained by FFHQ and head rendered in the surrounding views, respectively.}
%   \label{fig:short}
% \end{figure*}


\subsection{Knowledge Distillation of GANs}
The target of knowledge distillation(KD)~\cite{DBLP:journals/corr/HintonVD15} is to transfer dark knowledge, \eg logits and features, from teacher networks to student network, which is originally used in classification~\cite{Cho2019efficacy, park2019relational,romero2014fitnets,DBLP:conf/eccv/XuLLL20,DBLP:conf/iclr/TianKI20,zhang2020prime}. Following, KD is also employed for GAN-based model compression~\cite{chen2020distilling,wang2020gan,li2020gan,liu2021content,xu2022mind,hou2021slimmable}. %GAN Slimming~\cite{wang2020gan} mainstreams knowledge distillation and the GAN minimax objective for a faster generation with little visual quality degradation. 
In GAN compression~\cite{li2020gan}, the student network learns every intermediate features and final outputs from the teacher network. Besides conditional GANs, several works focus on the study of unconditional GANs. CAGAN~\cite{liu2021content} adopts multi-Level distillation and content-aware distillation, then fine-tuned by adversarial loss. StyleKD~\cite{xu2022mind} proposes that the mapping network plays an important role in generation. In addition, a novel initialization strategy and a latent direction-based distillation loss are presented for semantic consistency between the teacher and student model. 
Our work is built upon a StyleGAN-like 3D-aware generative model EG3D~\cite{chan2022efficient}. Through experiments, we observe the disentanglement of tri-planes, which are critical in semantic representations in EG3D. Accordingly, a tri-plane feature distillation procedure is proposed to transfer facial priors in complete head generation.

