
\section{Experiments}

\subsection{Experiments setup}

\noindent\textbf{Dataset.}
We train our proposed Head3D using two datasets. One is FFHQ~\cite{karras2019style}, a large public single-view real-world face dataset, for face priors learning. And another is H3DH, our proposed multi-view human head dataset. H3DH contains multi-view images of 50 identities, who are gender- and age-balanced with a wide variety of hairstyles. Following EG3D, we employ the same off-the-shelf pose estimators~\cite{deng2019accurate} to extract approximating camera extrinsic of FFHQ. In terms of H3DH, we set the same camera intrinsic as that of FFHQ, and obtain images with the resolution of $512^2$ from surrounding perspectives under natural light.


%H3DH contains 50 high-fidelity 3D models, with 8K resolution texture and hand-refined geometry. The models are gender- and age-balanced with a wide variety of hairstyles. Following EG3D, we employ the same off-the-shelf pose estimators~\cite{deng2019accurate} to extract approximating camera extrinsic of FFHQ, in a constant camera intrinsic. In terms of H3DH, we set the same camera intrinsic as that of FFHQ while the camera extrinsic parameters to be consistent with pre-trained EG3D. Then, these models are rendered randomly with the resolution of $512^2$ from surrounding perspectives under natural light in professional 3D graphics software. 

\noindent\textbf{Implementation Details.}
We fine-tune the head generator in the proceducre from StyleGAN-ADA\cite{karras2020training} via the H3DH dataset with batch size 16. The optimizer is Adam~\cite{DBLP:journals/corr/KingmaB14} with the same learning rate as original EG3D~\cite{chan2022efficient} of 0.0025 for G and 0.002 for D. D is trained with R1 regularization with $\gamma=20$. In the knowledge distillation phase, the learning rate is converted to 0.001 for G and 0.0005 for D, with $\gamma=1$ for $D_{front}$ and $\gamma=20$ for $D_{back}$. All $\lambda_{*}$ are set to 1.0 except $\lambda_{kd}$ set to 0.5 and $\lambda_{gan_{back}}$ set to 10.0 to balance loss functions in the front view and back view. Threshold $\tau$ is set to $\pi/4$. The resolutions of neural rendering $I_{raw}$ and final generated images $I$ are ${128}^2$ and ${512}^2$, respectively. Note that, owing to H3DH rendered without background, we regard fine-tuned EG3D which generates faces in the white background as the teacher network instead of the original EG3D, which is fine-tuned with images in FFHQ whose background is removed by BiseNet~\cite{yu2018bisenet}. Training a model costs about 4 hours on two Nvidia A100 GPUs. 

\subsection{Comparisons}
 \begin{figure}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/comparison.png}
	\caption{Qualitative comparisons between Head3D and baselines. There exhibit two perspectives of two identities.}
	\label{img:comparison}
 \vspace{-1em}
\end{figure}

As few works achieve full head generation trained by images, we compare our work with some designed baselines. 1) Directly fine-tune EG3D with FFHQ and H3DH, named \emph{DFT}. 2) Train model with FFHQ and H3DH from a fine-tuned head generator, named \emph{FH}. 3) Original pre-trained EG3D, to examine the quality of faces. The results are shown both qualitatively and quantitatively, where we use Fr$\acute{e}$chet Inception Distance (FID)~\cite{heusel2017gans} and Kernel Inception Distance (KID)~\cite{binkowski2018demystifying} for evaluation. Note that, we do not implement training from scratch on the H3DH, due to the fact that training with only 50 individuals results in insufficient diversity in the identity information.

\begin{table}[t]
	\centering
     \begin{tabular}{c|ccc}
			\hline
			Type & Baselines & FID $\downarrow$ & KID $\downarrow$  \\
			\hline
			Face &  EG3D~(Fine-tuned) & \textbf{6.82} & \textbf{0.36} \\
			 \hline
             \multirow{3}{*}{Head} &\emph{DFT} & 30.46 & 1.50\\
             &\emph{FH} &46.09 & 2.11\\
             &Ours & 11.34  & 0.61    \\
			\hline
	\end{tabular}
	\caption{Quantitative comparisons between our Head3D and baselines in FID and KID$\times100$, obtained in the resolution of $512^2$.} 
	\label{lab:comparison}
	\vspace{-1.5em}
\end{table}


\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figures/withcurrent.png}
	\caption{Qualitative comparisons among our Head3D and current generative head models. Results of LYHM~\cite{ploumpis2019combining,ploumpis2020towards}, DECA~\cite{feng2021learning} and i3DMM~\cite{yenamandra2021i3dmm} are obtained via their papers and released codes.}
	\label{i3dmm}
	\vspace{-2em}
\end{figure}

Figure~\ref{img:show} provides example results synthesized by our proposed method from various viewpoints and identities, demonstrating the generation of complete head geometry and high-quality renderings. Particularly, our method is able to generate accessory items such as hats and glasses, which are not present in the H3DH dataset. The visualized comparisons are shown in Figure~\ref{img:comparison}, where images in the same column are synthesized from the same latent code $z$. Although \emph{DFT} and \emph{FH} achieve full head generation, output consistency with origin EG3D is broken. Moreover, the model is trained by naively combined FFHQ and H3DH, resulting in low-quality results and model collapse. Similar observation can also be found from Table~\ref{lab:comparison} that both FID and KID become much worse in \emph{DFT} and \emph{FH}. Original EG3D performs the best numerically, while it can not generate full heads. Our work achieves identity representation consistent with the original EG3D through knowledge distillation and can fully represent human heads with good 3D consistency. 
Although our method is numerically inferior to the original EG3D, considering information loss in knowledge distillation and no back images corresponding to FFHQ faces, the decline in generating quality is acceptable. Especially, the generated heads maintain high-fidelity rendering results as EG3D qualitatively.
In summary, our H3DH achieves high-quality complete heads generation in both visual results and quantitative evaluation.

A qualitative comparisons are conducted among our Head3D and current generative head models, LYHM~\cite{ploumpis2019combining,ploumpis2020towards}, DECA~\cite{feng2021learning} and i3DMM~\cite{yenamandra2021i3dmm}, in terms of rendering results and geometry shapes, as depicted in Figure ~\ref{i3dmm}. Notably, these models are trained on the dataset consisting of a large number of 3D scans~(several hundreds or thousands), whereas our Head3D model only uses a multi-view image dataset containing 50 individuals. However, the results reveal that our model produces superior rendering quality and more detailed geometry than these methods. Additionally, explicit head models~\cite{ploumpis2019combining,ploumpis2020towards,feng2021learning} are not capable of representing hair, while the implicit model i3DMM~\cite{yenamandra2021i3dmm} can only represent hair in low quality. In contrast, our results show the ability to generate photo-realistic and diverse hair, including various accessories such as hats. Therefore, this comparison demonstrates that our algorithm can achieve high-quality and highly-detailed complete head generation with significantly fewer data.


%In addition, we compare our Head3D with parametric head generation model i3DMM~\cite{yenamandra2021i3dmm} in rendering results and geometry shapes qualitatively, as shown in Figure~\ref{i3dmm}. As can be seen from the figure, our rendering results and geometry details are far better than i3dmm. It should be noted that i3DMM is trained with 64 subjects with each 10 expressions, totally 640 3D models, while our Head3D only use the multi-views image dataset consisting of 50 individuals. Therefore, this comparison validates that our algorithm can achieve high-quality full head generation at a much lower cost.

\subsection{Ablation Study}

\begin{figure}[t]
\centering
% 	\subfigure{
    	\begin{minipage}[h]{1.0\linewidth}
    	\centering
    	\includegraphics[width=1.0\linewidth]{figures/mix.png}
    	\small (a)~Results of applying $f^t_{xy}$ on head generator without training.
    	\includegraphics[width=1.0\linewidth]{figures/totalT.png}
    	\small (b)~Results of the model trained with total tri-plane distillation.
    	\includegraphics[width=1.0\linewidth]{figures/dualD.png}
    	\small (c)~Results of the model trained by single discriminator.
    	\includegraphics[width=1.0\linewidth]{figures/ours_ablation.png}
    	\small (d)~Results of our Head3D.
	    \end{minipage}%
	\centering
	\caption{The illustration of ablation studies on tri-plane feature distillation and dual-discriminator. }
% 	We also give the example results in Appendix~\ref{example}}
\vspace{-1em}
\label{img:triplane}
\end{figure}

\begin{table}[t]
	\centering
    \begin{tabular}{lccc}
			\hline
			& FID $\downarrow$ & KID $\downarrow$ & ID $\uparrow$ \\
			\hline
			W/O $L_{kd}$ & 44.76  & 3.07   & 0.10\\
			W/O $L_{gan_{front}}$ & 29.84 & 2.47& 0.45\\
			W/O $L_{rgb}\& L_{lpips}$ & 19.50 & 1.29& 0.45  \\
			Ours & \textbf{11.34}  & \textbf{0.61}  & \textbf{0.65} \\
			\hline
	\end{tabular}
	\caption{Quantitative results of ablation study on different loss functions, evaluated by FID, KID$\times100$ and ID consistency (ID).} 	
	\label{lab:ablation}
	\vspace{-1.5em}
\end{table}



%  \begin{figure*}[t]
% 	\centering
% 	\includegraphics[width=.90\textwidth]{figures/inversion.png}
% 	\caption{We use PTI~\cite{roich2022pivotal} to reconstruct the heads, then render them toward novel perspectives with the forms of RGB image and shape. Note that, the example views are not used in fine-tuning the models.}
% 	\label{img:inversion}
% 	\vspace{-1em}
% \end{figure*}

%  \begin{figure}[t]
% 	\centering
% 	\includegraphics[width=.48\textwidth]{figures/inversion2.png}
% 	\caption{We use PTI~\cite{roich2022pivotal} to reconstruct the heads, then render them toward novel perspectives with the forms of RGB image and shape. Note that, the example views are not used in fine-tuning the models.}
% 	\label{img:inversion}
% 	\vspace{-1em}
% \end{figure}

\noindent \textbf{Effectiveness of Tri-plane Feature Distillation.} We investigates the effectiveness of our proposed tri-plane feature distillation method. We considered two settings: (a) directly applying $f^t_{xy}$ on the head generator without training, and (b) distilling the whole tri-plane features, as results shown in Figure~\ref{img:triplane} (a) and (b), respectively. It can be concluded in the first setting, although the identity information is preserved, the generated heads suffer from severe distortions and defective head shapes. In the second setting, distilling the whole tri-plane leads to incomplete head shapes. In contrast, our proposed method, as shown in Figure 3~(d), maintains identity consistency and achieves complete head generation. These results demonstrate the effectiveness of our tri-plane feature distillation approach.

%Compared with these two settings, it can be seen in Figure~\ref{img:triplane} (d) that our method both maintains ID consistency and achieves complete head generation, which verifies the effectiveness of our tri-plane feature distillation.


\noindent \textbf{Effectiveness of Dual-discriminator.} We also conduct experiments to verify the effectiveness of our proposed dual-discriminator. As depicted in Figure~\ref{img:triplane} (c), the model trained with a single discriminator produces competitive results for face synthesis with Head3D. However, it fails to generate the back of the head resulting in a stitching of two faces. In summary, the comparison verifies the effectiveness of the proposed dual-discriminator.
%One possible reason is that the imbalance in quantity between back head and front face leads to failure.

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figures/ablations.png}
	\caption{The ablation studies on the effectiveness of each loss function. All images in a column are generated from the same latent code. Note that, the ground-truth results are generated by the pre-trained face generator without background.}
	\label{img:ablations}
	\vspace{-1.5em}
\end{figure}

\noindent \textbf{Effectiveness of Loss Functions.}
We conduct an ablation study to verify the effectiveness of each losses. Specifically, we remove each loss function individually and maintain other settings the same. The results are evaluated both qualitatively and quantitatively. The quantitative results including FID, KID and Identity Distance (ID) to the origin EG3D, calculated by Arcface~\cite{deng2019arcface} among 10000 individuals, are presented in Table~\ref{lab:ablation}. The results show that removing any loss functions results in a significant increase in FID and KID scores. Moreover, the lowest ID score is obtained when knowledge distillation is removed, which highlights the crucial role of tri-plane distillation in transferring identity information.

Figure~\ref{img:ablations} presents example results of different settings to further evaluate the effectiveness of each loss function. Although most of the textures are preserved without $L_{gan_{front}}$, the photo-realism is lost, which indicates the importance of adversarial training for maintaining image quality. While lacking $L_{kd}$ causes failure in identity transmission. $L_{rgb}$ and $L_{lpips}$ are also crucial in preserving detailed texture and ensuring texture consistency. 
Overall, the combination of these loss functions achieves the best results both quantitatively and qualitatively.



 \begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figures/stylemix.png}
	\caption{Interpolation results of our proposed Head3D model.}
	\label{img:stylemix}
	\vspace{-1.5em}
\end{figure}



% \subsection{Applications}
% \noindent \textbf{Head Reconstruction.} Figure~\ref{img:inversion} illustrates the head reconstruction results. We use GAN inversion with PTI~\cite{roich2022pivotal} to fine-tune the head generator. 
% It is worth noting that unlike face generation, reconstructing the entire head using a single image is challenging and often results in loss of details. Therefore, we use 4-view images with accurate poses as targets to reconstruct the head. Subsequently, we can synthesize novel views and extract geometric shapes.


\noindent\textbf{Analysis of Linearity in Latent Space.}
Analogous to StyleGAN-based generators \cite{karras2019style,Karras_2020_CVPR}, style codes $w$ can be linearly interpolated to achieve image manipulations. Figure~\ref{img:stylemix} demonstrate that our model remains linear separability in the latent space after knowledge distillation.