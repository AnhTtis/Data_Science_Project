%%%%%%%%% ABSTRACT
\begin{abstract}
%Generating 3D-aware faces from 2D images is a progressive task, focusing on high-quality textures and 3D shapes. However, seldom works concern head generation from images. Because it is hard to estimate the accurate pose from a single in-the-wild image in the back view, and a large number of multi-view images are expensive. Naively fine-tuning the face generator to head via a few multi-view images comes up against two challenges, which are the decrease in diversity of identity and the domain gap between the face and back of the head. To address these challenges, we present Head3D, a complete 3D head generator via tri-plane feature distillation, built upon the face generator EG3D. First, we explore the role of tri-plane in EG3D, so that identity information can be separated decoupled and serve as prior knowledge to be distillated on the head. Then, dual-discriminators are proposed to guide the face and back of head generation respectively to avoid the influence of the domain gap. Extensive experiments show the effectiveness of our proposed Head3D both qualitatively and quantitatively.

% Head generation is an important task in computer vision and computer graphics, variously applied in multi-media. However, current head generation methods either require a vast of 3D scans to train the model, or generate faces from 2D images with estimated poses. The former is expensive, while the latter lack ground-truth posed images for full head generation. In this paper, we propose Head3D, which takes advantages of both sides, and generates complete 3D heads with only single-view images and limited multi-view data. To achieve this, we first extract facial priors represented by tri-plane learned with 3D-aware generation model, and design a feature distillation module to deliver the 3D frontal identities into full heads without damaging head integrity. To address the domain gap between the face and head models, we propose dual-discriminators to guide the frontal and back of head generation respectively. Our model achieves diverse full head generation with photo-realistic renderings and high-quality geometry representation.  Extensive experiments show the effectiveness of our proposed Head3D both qualitatively and quantitatively.

Head generation with diverse identities is an important task in computer vision and computer graphics, widely used in multimedia applications. However, current full head generation methods require a large number of 3D scans or multi-view images to train the model, resulting in expensive data acquisition cost. To address this issue, we propose Head3D, a method to generate full 3D heads with limited multi-view images. Specifically, our approach first extracts facial priors represented by tri-planes learned in EG3D, a 3D-aware generative model, and then proposes feature distillation to deliver the 3D frontal faces into complete heads without compromising head integrity. To mitigate the domain gap between the face and head models, we present dual-discriminators to guide the frontal and back head generation, respectively. Our model achieves cost-efficient and diverse complete head generation with photo-realistic renderings and high-quality geometry representations. Extensive experiments demonstrate the effectiveness of our proposed Head3D, both qualitatively and quantitatively. 


\end{abstract}

%for 
%while