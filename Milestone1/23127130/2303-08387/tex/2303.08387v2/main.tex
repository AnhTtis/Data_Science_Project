%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document



% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{caption}
\usepackage{subcaption}   % : org -> uncomment
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{etoolbox}
\usepackage{placeins}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}   % : org -> uncomment
\usepackage{hyperref}   % for temp, 
\usepackage{setspace} 
\usepackage{romannum}
\usepackage{wasysym}
\let\Square\relax
\usepackage{bbding}
\usepackage{pifont}
\usepackage{xcolor}
\usepackage{array}
\usepackage{cite}
\usepackage{xspace}
% \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}\usepackage{algpseudocode}
\usepackage{hhline}
\let\labelindent\relax
\usepackage{enumitem}



\title{\LARGE \bf
Learning to Place Unseen Objects Stably using a Large-scale Simulation
}

%\author{ \parbox{3 in}{\centering Huibert Kwakernaak*
%         \thanks{*Use the $\backslash$thanks command to put information here}\\
%         Faculty of Electrical Engineering, Mathematics and Computer Science\\
%         University of Twente\\
%         7500 AE Enschede, The Netherlands\\
%         {\tt\small h.kwakernaak@autsubmit.com}}
%         \hspace*{ 0.5 in}
%         \parbox{3 in}{ \centering Pradeep Misra**
%         \thanks{**The footnote marks may be inserted manually}\\
%        Department of Electrical Engineering \\
%         Wright State University\\
%         Dayton, OH 45435, USA\\
%         {\tt\small pmisra@cs.wright.edu}}
%}

\author{Sangjun Noh$^{*}$, Raeyoung Kang$^{*}$, Taewon Kim$^{*}$, Seunghyeok Back, Seongho Bak, Kyoobin Lee†% <-this % stops a space
\thanks{$^{*}$ These authors are equally contributed.}
\thanks{All authors are with the School of Integrated Technology, Gwangju Institute of Science and Technology, Cheomdan-gwagiro 123, Buk-gu, Gwangju 61005, Republic of Korea. 
† Corresponding author: Kyoobin Lee {\tt\small kyoobinlee@gist.ac.kr}}%
}



\begin{document}

% \end{document}\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

Object placement is a crucial task for robots in unstructured environments as it enables them to manipulate and arrange objects safely and efficiently. However, existing methods for object placement have limitations, such as the requirement for a complete 3D model of the object or the inability to handle complex object shapes, which restrict the applicability of robots in unstructured scenarios. In this paper, we propose an \textbf{U}nseen \textbf{O}bject \textbf{P}lacement (\textbf{UOP}) method that directly detects stable planes of unseen objects from a single-view and partial point cloud. We trained our model on large-scale simulation data to generalize over relationships between the shape and properties of stable planes with a 3D point cloud. We verify our approach through simulation and real-world robot experiments, demonstrating state-of-the-art performance for placing single-view and partial objects. Our UOP approach enables robots to place objects stably, even when the object's shape and properties are not fully known, providing a promising solution for object placement in unstructured environments. Our research has potential applications in various domains such as manufacturing, logistics, and home automation. Additional results can be viewed on \href{https://sites.google.com/view/uop-net-anonymous/home}{https://sites.google.com/view/uop-net-anonymous/}, and we will release our code, dataset upon publication.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}


Robots need to have the ability to place unseen objects in a stable manner to operate effectively in unstructured environments, which are common in manufacturing, construction, and household tasks. Although deep learning has been applied to recognize and manipulate unseen objects, most existing researches focus on identifying\cite{xie2021unseen, back2022unseen} or grasping\cite{mahler2017dex, mousavian20196} objects. This study proposes a new task called unseen object placement (UOP), which specifically addresses the problem of placing unseen objects in the real world.

%%%%%%%%% Fig.1

\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure_1.png}
        \end{subfigure}
        
    \caption{\textbf{Comparison of UOP-Net (Ours) and previous methods.} Previous studies for object placement used (a) full-shape object models\cite{haustein2019object, trimesh, hagelskjaer2019using}, (b) completion modules\cite{gualtieri2021robotic}, or (c) fitted primitive shapes\cite{fischler1981random, Zhou2018}. In contrast, (d) the proposed UOP-Net directly detects stable planes for unseen objects from partial observations.
}    
    \label{fig:task_comparison}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%
Conventional approaches\cite{haustein2019object, trimesh, hagelskjaer2019using} for stable object placement require full 3D models and analytical calculations. These methods involve sampling stable planes after calculating the center of mass of the object, which is not feasible for all real-world objects that may be encountered. One approach\cite{gualtieri2021robotic} combines analytical methods with a 3D object completion model that can reconstruct the full shape of an object from raw perception data. However, it is challenging to employ this approach because the predicted point cloud may not be precise, and this can lead to errors in calculating stable planes. Our UOP method addresses these limitations by directly detecting stable planes of unseen objects from a single-view and partial point cloud, which helps eliminate the need for a complete 3D object model. This allows the robot to stably place the object even when the shape and properties of the object are not fully known.

In this paper, we propose a method for UOP that detects stable planes for placing objects based on partial observations. For this, we generated a large-scale synthetic dataset called UOP-Sim, which contains 3D objects and annotations of stable planes generated using a physics simulator. Unlike previous approaches \cite{jiang2012learninga, jiang2012learningb} that rely on heuristics to label the preferred placement configurations, we automatically annotate all possible planes that can support stable object poses. Our dataset includes 17.4K objects and a total of 69K annotations. We propose a convolutional neural network called UOP-Net that predicts the most stable plane from partial point-cloud data and trains it using only the UOP-Sim dataset. Then, we compare the performance of our approach with three baseline methods and show that it achieves state-of-the-art (SOTA) performance in both the simulation and real-world experiments without any fine tuning on real-world data.

The main contributions of this study are as follows:
% object (17,408 => train: 13926, val:3482 ) // anno (69,027 => train :55261, val:13766)
\begin{itemize}    
    \item{We propose a new task called UOP to place an unseen object in a stable manner from a single-view and partial point cloud.}
    
    \item{We provide a public large-scale 3D synthetic dataset 
    called UOP-Sim that contains a total of 69,027 annotations of stable planes for 17,408 different objects.}
 
    \item{We introduce a convolutional neural network named UOP-Net that directly predicts the most stable plane for an unseen object.}
    
    \item{We compare the performance of our approach with previous object placement methods and confirm that our method outperforms the SOTA methods without any fine tuning in real-world environments.}

  
\end{itemize} 


\section{Related Works}


\textbf{Stable Object Placement.}
Previous studies\cite{tournassoud1987regrasping, wan2019regrasp,lertkultanon2018certified, haustein2019object} demonstrated that robots can stably place an object with known geometrical properties by analyzing the convex hull and sampling stable planes for the object. However, this approach requires precise object prior (e.g., CAD, mass), and it may not be available in real-world scenarios with partial observations (e.g., from an RGB-D camera). Some researchers attempted to address this limitation with deep learning-based completion methods that predict invisible part of an object\cite{gualtieri2021robotic}; however, these approaches have limitations in generating the precise shapes of unseen objects. Our UOP method addresses these limitations by directly detecting stable planes from partial observations without the need for a complete 3D object model. Unlike previous methods, the UOP method is more generalizable and adaptable to real-world scenarios with partial observations, and it can detect stable planes for unseen objects. 

\textbf{Unseen Object Placement.}
Previous studies on unseen object placement focused on identifying stable placements that satisfy human preferences. For example, Jiang et al.\cite{jiang2012learninga} trained a classifier using a hand-crafted dataset for identifying such placements; this approach relies on heuristic labels and requires complete observability. Cheng et al.\cite{cheng2021learning} proposed a deep learning model based on simulations to address the issue of heuristic labels; however, this approach was limited to task-specific objects. Another common approach\cite{mitash2020task} for placing unseen objects is using bounding box fitting to determine the shape and orientation of the object. This method can be fast and effective; however, it ignores the geometry of the object and relies only on its bounding box. Although this approach can be applied to unseen objects, it may not stably place objects in all situations, and therefore, it may be less effective than methods that consider the geometry of the object. In contrast, our approach can stably place unseen rigid objects on a horizontal surface using only a single partial observation. Our method can handle a wide range of objects instead of being limited to specific object types.

\textbf{Robotic Applications of Object Placements.} 
Prior works on object placement for robotic applications focused on solving specific tasks such as constrained placement\cite{mitash2020task}, upright placement\cite{newbury2021learning, pang2022upright}, and rearrangement\cite{wada2022reorientbot, paxton2022predicting}. However, these methods have several limitations. For example, Mitash et al.'s approach\cite{mitash2020task} relies on multi-view shape fitting and requires access to object models that may not be available in some scenarios. The deep learning approach proposed in \cite{paxton2022predicting} is limited to determining the required rotation for placing objects in a stable manner in an upright orientation. Li et al.\cite{li2022stable} proposed a method that can only predict rotations that maintain objects in positions that maximize their height; these limitations restrict the applicability and potential of these methods for more general object placement tasks such as stacking and packing. In contrast, our approach addresses the fundamental problem of placing unseen objects on a horizontal surface and has the potential to be applied to a wider range of robotic applications.



%%%%%%%%% Fig.2
\begin{figure*}[hbt!]
    \centering
        \includegraphics[width=\textwidth]{figures/Figure_2.png}
  \caption{\textbf{UOP-Sim dataset generation pipeline.} The UOP-Sim dataset is a large-scale synthetic dataset that contains 3D object models (17.4K) and annotations of stable planes (69K). The dataset is generated by dropping each object on a table in 512 different configurations and by sampling stable planes that satisfy Eq.\ref{eq:stability}. The stable plane candidates are verified using a tilted table. }
    \label{fig:UOP-sim}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%

\section{Problem Statements}

%%%%%Assumptions
\subsection{Assumptions}
A robot arm equipped with a parallel-jaw gripper operates in a workspace with a planar table, assuming that the camera pose is known. The manipulating scene begins with the robot grasping an object. The scene is captured by a single-view RGB-D camera, and the partial point cloud of the object is obtained from the depth image. The point cloud is fed into the model, and the most stable plane is predicted.

%%%%%definitions
\subsection{Definitions}

\textbf{Point Cloud.}
Let ${X} \in \mathbb{R}^{N \times 3}$ be point clouds obtained by capturing the manipulating scene in which the robot grasps the object from the camera. 

\textbf{Object Instability and Stable Planes.}
Let $\mathcal{U}$ denote the instability of an object model. We define instability $\mathcal{U}_{i}$ as the average of homogeneous transformation change in world coordinates in a simulator over a discrete time step $L$ at time step ${i}$. Stable planes $\mathcal{S}$ are annotated for each object model at time step ${i}$ that first time satisfies the condition $\mathcal{U}_i < \epsilon$. A stable plane $s \in \mathcal{S}$ is represented as normal vector $\vec{V}\in \mathbb{R}^3$, and threshold $\epsilon$ indicates that the object has reached stable state. 


\textbf{Dataset and Deep Learning-based Model.}
The dataset $\mathcal{D} = \{(\mathcal{O}, \mathcal{A})_n\}_{1}^{N}$ represents the $N$ set of object models $\mathcal{O}$ and corresponding stable planes $\mathcal{A}$ as the annotations, where $\mathcal{S} \subset \mathcal{A}$. The function $\mathcal{F}: {X} \rightarrow \mathcal{S}$ denote a deep learning-based model that considers point clouds ${X}$ as the input and produces the most stable plane $\mathcal{S}$ as the output.


%\textbf {Seen and Unseen Objects}  If $\mathcal{O}_{\mathcal{D}_{train}} \cap \mathcal{O}_{\mathcal{D}_{test}} = \emptyset$, $\mathcal{O}_{\mathcal{D}_{train}}$ is a seen object and $\mathcal{O}_{\mathcal{D}_{test}}$ is an unseen object for the deep learning model $\mathcal{F}$. 

\textbf{Seen and Unseen Objects.}
The set of object models used for training and testing the function $\mathcal{F}$ are denoted as $\mathcal{O}_{train}$, $\mathcal{O}_{test}$. If $\mathcal{O}_{train} \cap \mathcal{O}_{test} = \emptyset$, then objects $\mathcal{O}_{train}$ are considered seen objects while objects $\mathcal{O}_{test}$ are unseen objects for the model $\mathcal{F}$. 

%%%%%%%objectives
\subsection{Objectives} 
Our objective is to detect the most stable plane for placing unseen objects from a single-view observation. We aim to develop a neural network $\mathcal{F}: {X} \rightarrow \mathcal{S}$ that minimizes the instability of the object $\mathcal{U}$.


\section{Learning for Unseen Object Placement}
We address the challenges of the UOP task, which is known to be difficult to solve because of the need for a large-scale dataset for approximating stable planes and the complexity of the relationship between point clouds and annotated planes. We present a novel approach by introducing the UOP-Sim dataset to mitigate these challenges; this dataset includes 17K 3D object models and 69K labeled stable planes, and a UOP-Net neural network that can detect robust stable planes from partial point clouds. We propose a general and adaptable approach to the UOP task using these tools, which enables robots to accurately place unseen objects in real-world scenarios.

\subsection{UOP-Sim Dataset}
\textbf{Overview.}
The process for generating the dataset, as illustrated in Fig. \ref{fig:UOP-sim}, involved randomly dropping a rigid object on a horizontal surface to identify stable plane candidates and to then annotate these stable planes by placing them on a tilted table. This automated approach allows us to efficiently generate a large-scale synthetic data for training and evaluating the model, and it requires no human intervention.


%%%%%%%%% Fig.3

\begin{figure*}[ht!]
   \centering
      \includegraphics[width=\textwidth]{figures/Figure_3.png}
    \caption{\textbf{Overall pipeline of UOP Method}. UOP detects the most stable plane directly from single-view and partial point cloud. UOP-Net is trained on the UOP-Sim dataset, and takes in a partial point cloud to predict the stable plane. The estimated stable plane is used to execute object placement based on the angle difference between the normal vector of the plane and the negative gravity vector.}
  \label{fig:UOP-net}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textbf{Stable Plane Annotation.}
We defined the movement of the object at time step i as $\mathcal{M}_{i}$ in terms of its translation and rotation in the world coordinates to evaluate the instability of the pose of the object in dynamic simulation. It can be represented as $\mathbf{H} =[\mathbf{R}|\mathbf{T}] \in \mathbb{SE}(3)$, where $\mathbf{R},\mathbf{T}$ are rotation and translation matrix. We tracked the pose of the object at each time step and calculated the difference between the consecutive poses (Eq.\ref{eq:movement}). Then, we took the average of these differences over a certain period ${L}$ to estimate the instability of the object at a time step ${i}$ (Eq.\ref{eq:stability}). To ensure robust annotation, we consider a range of discrete time step ${L}$ rather than only single time step ${i}$


\begin{equation}
\label{eq:movement}
\mathcal{M}_i = ||\mathbf{H}_i- \mathbf{H}_{i-1}||_2
\end{equation}

\begin{equation}
\label{eq:stability}
\mathcal{U}_{i}=
\begin{cases}
{\frac{1}{L}}\sum_{j=i-L+1}^i\mathcal{M}_{j}, & \mbox{if } i >= L \\
{\frac{1}{i}}\sum_{j=1}^i\mathcal{M}_{j}, & \mbox{otherwise } 
\end{cases}
\end{equation}

We generated 512 orientations by dividing the roll, pitch, and yaw into eight intervals to explore a wide range of possible poses for the object. The object was then placed on a table with a random pose along the normal direction of the table. We dropped the object on the table and recorded all poses where it remained stable (${\mathcal{U}_{i} < \epsilon_1}$) to find stable planes that support the object.

%Then, we used the density-based spatial clustering of applications with the noise algorithm\cite{ester1996density} to cluster the sampled poses. This allowed us to identify stable planes by clustering the poses along the z-axis; this represents the normal vector at the contact point of a stable plane with a horizontal surface. After rotating the object to align the clustered normal vector with the gravity vector, we masked the bottom 5\% of the object's 3D model along the gravity vector to indicate areas that could support the stability of the object.

Then, we used the density-based spatial clustering of applications with the noise algorithm\cite{ester1996density} to cluster the sampled poses. This allowed us to identify stable planes by clustering the poses along the z-axis; this represents the normal vector at the contact points of a stable plane with a horizontal surface. Then we masked the bottom 5\% of height along the surface normal of table to indicate areas that could support the stability of the object.


% Certain planes may not be easily generalized because real-world environments cannot be perfectly simulated. This can be a problem for planes such as a spherical model or for the sides of a cylinder. We placed each object on a flat table with a normal vector of the plane candidates and tilted the table by 10° to address this. Then, we estimated the movement of the object across each time step and eliminated any planes that did not satisfy the condition $\mathcal{U}$ less than $\epsilon_2$. This allowed us to label stable planes that were robust for application to a horizontal surface, as shown in the samples of the UOP-Sim dataset in Fig.\ref{fig:UOP-sim}. Supplementary Figure.1 contains additional sample images from UOP-Sim (YCB objects).

% The UOP-Sim contains a total of 17,408 3D object models and 69,027 stable plane annotations. Furthermore, our dataset contains both explicit and implicit planes, such as a flat surface formed by four chair legs. We captured 3D object models in UOP-Sim dataset using a depth camera with 1,000 random poses to respond appropriately to partial observations in the real world. A partial point cloud was then sampled from the depth image using a voxel downsampling method.

% To label the stable planes in partial point clouds, we aligned them with annotated 3D object models and sampled their overlapping regions, which might be smaller than the full shape object annotations. To ensure reliability, we estimated the angular error by comparing the normal vectors of the overlapped regions and the original annotations and filtered out stable planes with errors greater than 10°. This filtering process significantly improves the accuracy of our labeling method, making it suitable for partial observations in real-world scenarios (Fig.\ref{fig:UOP-sim}: Training Data Generation using UOP-Sim).

Certain planes may not be easily generalized because real-world environments cannot be perfectly simulated. This can be a problem for planes such as a spherical model or for the sides of a cylinder. We placed each object on a flat table with a normal vector of the plane candidates and tilted the table by 10° to address this. Then, we estimated the movement of the object across each time step and eliminated any planes that did not satisfy the condition $\mathcal{U}$ less than $\epsilon_2$. This allowed us to label stable planes that were robust for application to a horizontal surface, as shown in the samples of the UOP-Sim dataset in Fig.\ref{fig:UOP-sim}. The UOP-Sim contains a total of 17,408 3D object models and 69,027 stable plane annotations. Furthermore, our dataset contains both explicit and implicit planes, such as a flat surface formed by four chair legs. Supplementary Figure.1 contains additional sample images from UOP-Sim (YCB objects).

We captured 3D object models in UOP-Sim dataset using a depth camera with 1,000 random poses to respond appropriately to partial observations in the real world. A partial point cloud was then sampled from the depth image using a voxel downsampling method. To label the stable planes in partial point clouds, we aligned them with annotated 3D object models and sampled their overlapping regions, which might be smaller than the full shape object annotations. To ensure reliability, we estimated the angular error by comparing the normal vectors of the overlapped regions and the original annotations and filtered out stable planes with errors greater than 10°. This filtering process significantly improves the accuracy of our labeling method, making it suitable for partial observations in real-world scenarios (Fig.\ref{fig:UOP-sim}: Training Data Generation using UOP-Sim).


\textbf{Simulation Environment Setting.} 
We used PyRep\cite{james2019pyrep} and CoppeliaSim\cite{rohmer2013v} to build a simulation environment for computing object instability ($\mathcal{U}$). Further, we used 3D object models from three benchmark datasets (3DNet\cite{wohlkinger20123dnet}, ShapeNet\cite{chang2015shapenet}, and YCB\cite{calli2015ycb}), which yielded a total of 17,408 models. We built 64 table models in the simulation environment to facilitate the annotation process.


% real world, no fine tune 꼭 추가할 것
\subsection{UOP-Net} 
\textbf{Overview.} 
We propose a deep learning framework called UOP-Net to detect the most stable plane for placing unseen objects on a flat surface. This framework is designed to predict stable planes from the visible regions of an observed object, and this allows us to detect planes that cover all possible ways of maintaining stability based on various partial views. The model needs to learn to generalize both object shapes and plane properties, and therefore, it can be difficult to segment the stable regions of unseen objects. UOP-Net addresses this challenge by directly detecting the most stable plane from partial observations, even when objects are unseen. The input of the proposed approach is the observed point cloud of the target object. Given a partial point cloud ${X} = \mathbb{R}^{N \times 3}$, the goal is to learn a function $\mathcal{F}: {X} \rightarrow \mathcal{S}$. A rotation $\mathbf{R} \in \mathbb{SO}(3)$ can be determined for stably placing the object on a horizontal surface using $s$.


\textbf{Network Architecture.} 
The UOP-Net is based on DGCNN\cite{wang2019dynamic} architecture. The network architecture includes three EdgeConv layers which are used to extract geometric features. These three EdgeConv layers use three shared fully-connected layers with sizes 64, 128, and 256. A shared fully-connected layer with size 1024 is then used to aggregate information from the previous layers. The point cloud global feature is obtained through a Max-pooling operation, and two branches are used to transform the global features: one branch for semantic segmentation (which predicts whether a point is stable or unstable), and another branch for embedding instance features of stable planes. Both branches use fully-connected layers with sizes of 512 and 256. Before the two branches, LeakyReLU and batch normalization are applied to all layers.


A mean-shift clustering algorithm \cite{comaniciu2002mean} is applied to the predicted stable points for identifying the stable points, and RANSAC \cite{fischler1981random} is used to fit planes onto the clustered points\cite{Zhou2018}. Stability scores for each plane are calculated by the element-wise multiplication of semantic logits, predicted instance labels, and number of points composing each plane. Then, the plane with the highest score is output after fitting the planes and assigning stability scores based on the number of inliers that constitute the planes. The rotation matrix $\mathbf{R}$ is then determined by estimating the angular difference between the predicted normal vector of the stable plane and the gravity vector (negative table surface normal).

\subsection{Loss Function}
Our loss function comprise two terms; stability loss as $\mathcal{L}_{stability}$ (Eq.\ref{eq:stablility loss}) and plane loss $\mathcal{L}_{plane}$ (Eq.\ref{eq:total_dis_loss}),

\begin{equation}
\label{eq:the_sum_of_losses}
\mathcal{L} = \lambda_1 * \mathcal{L}_{stability} + \lambda_2 * \mathcal{L}_{plane},
\end{equation}

\noindent where $\lambda_1$ and $\lambda_2$ are hyper-parameters, setting as $\lambda_1=10$ and $\lambda_2=1$ respectively. The stability loss $\mathcal{L}_{stability}$ is defined by the binary cross-entropy loss to encourage predicted point label $\hat{y_i}$ to match with the ground truth label $y_i \in \{0, 1\}$, where $N$ is the number of points:

\begin{equation}
\label{eq:stablility loss}
\mathcal{L}_{stability} = {1 \over N}\sum_{i=1}^N{-[y_i\log(\hat{y_i})+(1-y_i)log(1-\hat{y_i})]}
\end{equation}

We adopted the discriminative function for 2D images\cite{de2017semantic} and 3D point cloud\cite{pham2019jsis3d} to embed instance features as follows: 

\begin{equation}
\label{eq:total_dis_loss}
\mathcal{L}_{plane} = \alpha * \mathcal{L}_{pull} + \beta * \mathcal{L}_{push} + \gamma * \mathcal{L}_{reg},
\end{equation}

%  Our plane loss function consists of three components: $\mathcal{L}_{pull}$, which pulls instance feature vectors towards the instance center; $\mathcal{L}_{push}$, which encourages instances to push against each other; and $\mathcal{L}_{reg}$, which acts as a regularization term to bound the embedding values. In our approach, $K$ represents the maximum number of stable planes (instances) in the object, while $N_k$ is the number of points in the target instance $k \in \{1,...,K\}$. The mean embedding vector of instance $k$ is denoted by $\mu_k$, and $z_p$ represents the embedding of point $x_p$. The pull loss $\mathcal{L}_{pull}$ and push loss $\mathcal{L}_{push}$ have margins $\delta_v$ and $\delta_d$, respectively. The L2-norm is represented by $||\cdot||_2$, and the function $\mathcal{G}(x) = max(0, x)$ is used. We set hyper-parameters $\alpha = \beta = 1$ and $\gamma = 0.001$, and the margin $\delta_d$ is set to be greater than 2$\delta_v$, following \cite{pham2019jsis3d}.


% \begin{equation}
% \label{eq:var_loss}
% \mathcal{L}_{pull} = {1 \over K}\sum_{k=1}^K{1 \over N_k}\sum_{p=1}^{N_k} \mathcal{G}(||\mu_k - z_p||_2 - \delta_v)^2
% \end{equation}

% \begin{equation}
% \label{eq:dist_loss}
% \mathcal{L}_{push} = {1 \over K(K-1)}\sum_{k=1}^K \sum_{q=1, k \neq q}^K \mathcal{G}(2\delta_d - ||\mu_k - \mu_q||_2)^2
% \end{equation}

% \begin{equation}
% \label{eq:reg_loss}
% \mathcal{L}_{reg} = {1 \over K} \sum_{k=1}^K ||\mu_k||_2
% \end{equation}

Our plane loss function consists of three components: $\mathcal{L}_{pull}$, which pulls instance feature vectors towards the instance center; $\mathcal{L}_{push}$, which encourages instances to push against each other; and $\mathcal{L}_{reg}$, which acts as a regularization term to bound the embedding values. In our approach, $P$ represents the maximum number of stable planes (instances) in the object, while $N_p$ is the number of points in the target plane $p \in \{1,...,P\}$. The embedding vector of plane $p$ is denoted by $\boldsymbol{z}_p$, and $\boldsymbol{\mu}_p$ represents the embedding of point $x_p \in N_p$. The pull loss $\mathcal{L}_{pull}$ and push loss $\mathcal{L}_{push}$ have margins $\delta_v$ and $\delta_d$, respectively. The L2-norm is represented by $||\cdot||_2$, and the function $\mathcal{G}(x) = max(0, x)$ is used. We set hyper-parameters $\alpha = \beta = 1$ and $\gamma = 0.001$, and the margin $\delta_d$ is set to be greater than 2$\delta_v$, following \cite{pham2019jsis3d}.


\begin{equation}
\label{eq:var_loss}
\mathcal{L}_{pull} = {1 \over P}\sum_{p=1}^P{1 \over N_p}\sum_{k=1}^{N_p} \mathcal{G}(||\boldsymbol{\mu}_p - \boldsymbol{z}_k||_2 - \delta_v)^2
\end{equation}

\begin{equation}
\label{eq:dist_loss}
\mathcal{L}_{push} = {1 \over P(P-1)}\sum_{p=1}^P \sum_{q=1, q \neq p}^P \mathcal{G}(2\delta_d - ||\boldsymbol{\mu}_p - \boldsymbol{\mu}_q||_2)^2
\end{equation}

\begin{equation}
\label{eq:reg_loss}
\mathcal{L}_{reg} = {1 \over P} \sum_{p=1}^P ||\boldsymbol{\mu}_p||_2
\end{equation}




\section{Experiments}
\textbf{Datasets.} 
We obtained a total of 152, 57, and 63 object categories in the 3DNet\cite{wohlkinger20123dnet}, ShapeNet\cite{chang2015shapenet}, and YCB\cite{calli2015ycb} datasets, respectively. We labeled the YCB object models in the simulation, but they were excluded from the training set to allow us to use the test set in both the simulation and real-world experiments. We excluded objects that had no stable planes (e.g., spherical objects) to ensure the quality of our dataset. Then, we splited the dataset into training and validation sets in a 8:2 ratio. The training set contained 13,926 objects and 55,261 annotations, while the validation set contained 3,482 objects and 13,766 annotations.

\textbf{Training Details.}  We trained UOP-Net by utilizing a dataset comprising partial point clouds of objects captured from various views in addition to stable plane annotations selected based on technical criteria for ensuring suitability. The dataset used for training was specifically curated from 3D object models sourced from 3DNet \cite{wohlkinger20123dnet} and ShapeNet \cite{chang2015shapenet}. During training, 2,048 points randomly sampled from each object and they underwent various types of data augmentation techniques such as rotation, sheering, point-wise jitter, and adding Gaussian noise to enhance the performance of the model in real-world scenarios. The model was implemented using PyTorch\cite{paszke2019pytorch} and trained on an NVIDIA Titan RTX GPU with a batch size of 32 and a total of 1,000 epochs. We employed early stopping with a patience of 50 and used the Adam optimizer at a learning rate of 1e-3 to prevent overfitting.

\textbf{Baselines.} 
We compared the performance of our method with that of the following baselines:
\begin{itemize}[leftmargin=*]
    \item \textbf{Convex hull stability analysis (CHSA)}\cite{haustein2019object, trimesh, hagelskjaer2019using}: The baseline method for determining stable object poses involves calculating the rotation matrix to allow an object to rest stably on a flat surface. The center of mass of the object is sampled, and the stable resting poses of the object on a flat surface are determined using the convex hull of the object. Then, the probabilities of the object landing in each pose are evaluated, and the pose with the highest probability is output.

    \item \textbf{Bounding box fitting (BBF)}\cite{mitash2020task, Zhou2018}: The method involves fitting an oriented bounding box to the convex hull of the object using principal component analysis (PCA) to minimize the difference between the volume of the convex hull and that of the bounding box. Then, the object is placed on a planar workspace with the largest area facing down.
    
    \item \textbf{RANSAC plane fitting (RPF)}\cite{fischler1981random, Zhou2018}: The approach segments planes in a point cloud by fitting a model of the form $ax + by + cz + d = 0$ to each point $(x, y, z)$. Then, it samples several points randomly and uses them to construct a random plane while repeating this process iteratively to determine the plane that appears most frequently.
\end{itemize}

\textbf{Evaluation metrics.}
We used two metrics to evaluate the performance of UOP-Net: object stability \textit{($\mathcal{OS})$} and success rate of object placement \textit{($\mathcal{SR}$)}. We placed an unseen object on a flat table and used the output of the model to estimate its stability for measuring $\mathcal{OS}$. This allowed us to verify whether the trained model could maintain the object in a stable pose. The $\mathcal{SR}$ was calculated as the ratio of successful stable placements to all placements, with an accumulated rotation error of less than 10°. We considered only rotational motion when evaluating object stability because rotational motion is more common than translational motion when an object placed in an unstable state falls because of the vibrations. We evaluated the performance of the model by placing the object 100 times and regarding as a failure case if no planes were detected.

\begin{itemize}[leftmargin=*]
    \item \textbf{Object Stability ($\mathcal{OS}$)}: The metric quantifies the movement of the object during a discrete time step when it is placed on a horizontal surface using the predicted plane.
    
    \item \textbf{Success Rate ($\mathcal{SR}$)}: The metric represents the percentage of placements in which the object remains stationary for a minute and the accumulated rotation is less than $10^{\circ}$.
\end{itemize}

\subsection{Discussion}
In the simulation experiments, we compared the performance of UOP-Net with the baseline methods on 3DNet\cite{wohlkinger20123dnet}, ShapeNet\cite{chang2015shapenet}, and YCB\cite{calli2015ycb} objects for complete and partial shapes. Table \ref{tab:simulation-partial} presents the results in terms simulation. Our UOP-Net achieved the best performance when using a single partial observation. We visualized the prediction results of each method and represented the object stability as a graph in Fig. \ref{fig:sim_exp_result}. The blue line in the graph (Fig. \ref{fig:sim_exp_result}) represents the case of placing the object in the plane predicted by UOP-Net, which shows that the object can be placed more reliably than predicted by other methodologies.

%%%%%%%%% Table.1
\begin{table*}[ht!]

\caption{{UOP} Performance of UOP-Net and other baselines on the three benchmark objects (partial shape) in the simulation.}
\centering
\label{tab:simulation-partial}
\resizebox{\textwidth}
{!}{%
\renewcommand{\arraystretch}{1.2}
{\LARGE
\begin{tabular}{|cccccccccccccc|}
\hline
% \multicolumn{14}{|c|}{\textbf{Stable Object Placement Evaluataion Metric}} \\ \hline
\multicolumn{2}{|c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}\\ Object type \& \\ Dataset\end{tabular}}} & \multicolumn{8}{c|}{ Object stability (S)} & \multicolumn{4}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Success rate of \\ object placement ($\mathcal{SR}$, $\%$)\end{tabular}}} \\ \cline{3-10}
\multicolumn{2}{|c|}{} & \multicolumn{4}{c|}{Rotation (R, $^{\circ}$) $\downarrow$} & \multicolumn{4}{c|}{Translation (T, $cm$) $\downarrow$} & \multicolumn{4}{c|}{} \\ \cline{3-14} 
\multicolumn{2}{|c|}{} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}UOP \\ \textbf{(Ours)}\end{tabular}} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}UOP \\ \textbf{(Ours)}\end{tabular}} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \begin{tabular}[c]{@{}c@{}}UOP \\ \textbf{(Ours)}\end{tabular} \\ \hline
% Partial Point Cloud
\multicolumn{1}{|c|}{\multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Partial \\ Point cloud\end{tabular}}} & \multicolumn{1}{c|}{3DNet\cite{wohlkinger20123dnet}} % 3DNet
& 23.19 & 28.79 & 35.64 & \multicolumn{1}{c|}{\textbf{16.40}} % Rotation
& 2.48 & 3.17 & 3.74 & \multicolumn{1}{c|}{\textbf{1.91}}  % Translation
& 54.87 & 41.17 & 50.45 & \textbf{55.47}  \\ % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{ShapeNet\cite{chang2015shapenet}} % ShapeNet
& 21.97 & 28.70 & 25.80 & \multicolumn{1}{c|}{\textbf{14.90}} % Rotation
& 2.91 & 3.56 & 2.90 & \multicolumn{1}{c|}{\textbf{1.59}} % Translation
& 55.78 & 38.92 & 58.23 & \textbf{60.79}  \\ % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{YCB\cite{calli2015ycb}} % YCB
& 35.49 & 38.46 & 39.13 & \multicolumn{1}{c|}{\textbf{15.92}} % Rotation
& 5.73 & 5.98 & 5.19 & \multicolumn{1}{c|}{\textbf{2.37}} % Translation
& 48.14 & 40.22 & 52.66 & \textbf{62.53} \\ 
\hhline{|~|=|====|====|====|} % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Total avg.} % Total avg.
& 25.78 & 31.01 & 34.49 & \multicolumn{1}{c|}{\textbf{15.97}} % Rotation
& 3.32 & 3.90 & 3.90 & \multicolumn{1}{c|}{\textbf{1.95}} % Translation
& 53.50 & 40.48 & 52.59 & \textbf{58.22} \\ \hline % Success Rate

\end{tabular}
} % LARGE
} % resizebox



\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

While the CHSA method provides optimal results for stable object placement when the object shape is fully visible, as demonstrated in Supplementary Table.1. However, these assumptions are impractical, and UOP-Net performs well even for complete object shapes. In scenarios where only partial point clouds are available, the UOP-Net demonstrates a significant advantage over the CHSA method, as shown in Table \ref{tab:simulation-partial}. This is due to the fact that the convex hull generated from partial point clouds does not provide information about the unseen parts of the object, making it challenging to accurately determine the center of mass. As a result, the CHSA may not perform well with partial observations, and may lead to unsteady placement on truncated planes, as seen in Fig.\ref{fig:sim_exp_result}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 4 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 4.
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure_4.png}
        \end{subfigure}
        
    \caption{Visualization of the prediction results obtained on the YCB\cite{calli2015ycb} dataset for each method and object stability in the simulation.}
    \label{fig:sim_exp_result}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The BBF method showed the worst performance because it did not consider the geometric properties of the object and simply placed the largest plane in the bounding box. This may work for objects with simple shapes; however, it is not suitable for complex shapes. Although RPF performs better than BBF, it still fails to detect a robust stable plane. In contrast, UOP-Net can detect a robust plane based on incomplete perception because we generate partial point clouds using UOP-Sim and train the model to learn the geometric properties of objects and planes. 

Further, UOP-Net can detect not only explicit planes visible in the point cloud but also implicit planes (e.g., a plane formed by the four legs of a chair) that are not necessarily visible but still contribute to the stability of the object. This is because our model is trained on a diverse set of objects and planes, and it can generalize new unseen objects and planes.

\textbf{Failure cases.}
UOP-Net has some limitations that can affect its performance. For example, UOP-Net may have difficulty detecting stable planes for small objects such as spoons or knives because the size of the ground truth (stable plane) is relatively small compared to the size of the object. Further, it is difficult to find the optimal hyper-parameters for the mean-shift clustering algorithm, and therefore, the performance of UOP-Net can suffer when the hyper-parameters are not well-tuned for a particular object. However, these limitations are not unique to UOP-Net and are common challenges faced by many object placement methods.

\subsection{Robot Experiments}
We conducted real-world experiments to evaluate the performance of our UOP method. Our objectives were to demonstrate its ability to: (1) perform effectively in real-world scenarios despite being trained purely in simulation, (2) robustly handle partial and noisy observations, and (3) detect stable planes in unseen objects, achieving SOTA performance

\textbf{Real environment setting.}
We conducted experiments using a universal robot (UR5) manipulator and an Azure Kinect RGB-D camera to evaluate the performance of our object placement method in a real-world scenario (Fig.\ref{fig:real_experiment} (b)). We used the MANet \cite{fan2020ma} object segmentation method with a DenseNet121 \cite{iandola2014densenet} backbone to segment the target object and the gripper. We segmented the visible region of the target object from the RGB image and cropped the depth image using a mask. Then, we sampled the point cloud from the depth image using voxel-down sampling \cite{Zhou2018} and fed it to UOP-Net. The model predicted the most stable plane and calculated the rotation value between the plane and the table. Then, the UR5 robot placed the target object on the table. We utilized the BiRRT \cite{qureshi2015intelligent} algorithm implemented with PyBullet \cite{coumans2021} and integrated with collision checking on a physics engine to ensure smooth planar motion.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 5 %%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure_5.png}
        \end{subfigure}
        
    \caption{\textbf{Unseen object placement using UOP-Net}. (a) Given a segmented target object, UOP-Net considers the 3D point clouds observed by a depth camera (Azure Kinect) as inputs and predicts the most stable plane. (b) UR5 places the target object on the table.}
    \label{fig:real_experiment}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Evaluation metrics.}
We used $\mathcal{SR}$ as the evaluation metric to evaluate the effectiveness of our approach in real-world scenarios. During the experiments, we instructed the robot to place the object on the table using the plane predicted by UOP-Net. The placement was considered successful when we could visually confirm that the robot placed the object in a fixed position on the predicted plane without the object falling or sliding. The trial was considered a failure if the model failed to detect any stable planes. We conducted a total of 10 placement trials for each object.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Table 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[ht!]
\caption{\textbf{UOP} performance of the UOP-Net and baselines on YCB\cite{calli2015ycb} in the real world.}
\label{tab:real world-eval}
\resizebox{\columnwidth}{!}{%
\LARGE{
{\renewcommand{\arraystretch}{}
\begin{tabular}{|c|cccc|}
\hline
YCB\cite{calli2015ycb} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \begin{tabular}[c]{@{}c@{}}UOP-Net \\ \textbf{(Ours)}\end{tabular} \\ \hline
Coffee can & 0 / 10 & 0 / 10 & 4 / 10 & \textbf{10} / 10 \\
Timer & 0 / 10 & 1 / 10 & \textbf{6} / 10 & \textbf{6} / 10 \\
Power drill & 0 / 10 & 1 / 10 & \textbf{5} / 10 & \textbf{5} / 10 \\
Wood block & 1 / 10 & 1 / 10 & \textbf{10} / 10 & \textbf{10} / 10 \\
Metal mug & 0 / 10 & 0 / 10 & 6 / 10 & \textbf{9} / 10 \\
Metal bowl & \textbf{10} / 10 & 5 / 10 & \textbf{10} / 10 & \textbf{10} / 10 \\
Bleach cleanser & 3 / 10 & 3 / 10 & \textbf{9} / 10 & \textbf{9} / 10 \\
Mustard container & 2 / 10 & 0 / 10 & 5 / 10 & \textbf{10} / 10 \\
Airplane toy & 0 / 10 & 3 / 10 & 0 / 10 & \textbf{4} / 10 \\
Sugar box & 2 / 10 & 3 / 10 & \textbf{10} / 10 & \textbf{10} / 10 \\
Chips can & 2 / 10 & 0 / 10 & 8 / 10 & \textbf{10} / 10 \\
Banana & 5 / 10 & 5 / 10 & \textbf{9} / 10 & \textbf{9} / 10 \\ \hhline{|=|=|=|=|=|}
Average & 2.1 / 10 & 1.8 / 10 & 6.8 / 10 & \textbf{8.5} / 10 \\ \hline
\end{tabular}%
}
}
}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 6 %%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure_6.png}
        \end{subfigure}
        
    \caption{Visualization of the prediction results for each method on YCB\cite{calli2015ycb} objects and novel objects in the real world.}
    \label{fig:comparison_pred_res}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\textbf{Results.}
We selected 12 objects from the YCB dataset. Objects with spherical shapes (e.g., apples), dimensions that were too small, or small depth values were excluded from the test set. Table \ref{tab:real world-eval} indicates that our method outperforms other baselines in terms of the success rate across all objects. Although the real-world perception is noisy, UOP-Net provides a stable plane that can be attributed to our model learning from partial point clouds captured by a depth camera and corrupted by noise. Other benchmarks (CHSA and primitive shape fitting) performed extremely poorly because they could not obtain the complete shape of the object in the real world and were unable to respond to sensor noise. We evaluated our method on completely new objects that did not have an available CAD model (a dinosaur figurine and an ice tray, as shown in Fig.\ref{fig:comparison_pred_res}) to further verify that our model can perform on unseen objects. UOP-Net detected implicit planes (e.g., the four legs of the dinosaur) although the object shapes were complex. 

% \textbf{Failure cases}
% Points corresponding to the ground truth are not sampled when the RGB-D sensor fails to capture a specific area of an object (e.g., a dark or glossy part of the object) or when the object and gripper segmentation model fails to predict a clear mask. In such a case, UOP-Net can fail to detect the stable plane. Although we demonstrated that UOP-Net can detect an implicit plane from complex shape objects (e.g., YCB airplane toy or dinosaur), it is still challenging to detect an implicit plane in an unclear point cloud.

\section{Conclusion} 
In this paper, we presented UOP-Net, a novel method for detecting stable planes of unseen objects. We also introduced an approach for automatically annotating stable planes for various objects, and the large-scale synthetic dataset, called UOP-Sim, were generated. Our dataset contains 17.4K 3D objects and 69K stable plane annotations. The effectiveness of UOP-Net was demonstrated by achieving SOTA performance on objects from three benchmark datasets, indicating its accuracy and reliability in detecting stable planes from unseen and partially observable objects.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Acknowledgment}
\begin{spacing}{0.4}
{\scriptsize This research was completely supported by a Korea Institute for Advancement of Technology (KIAT) grant funded by the Korea Government (MOTIE) (Project Name: Shared autonomy based on deep reinforcement learning for responding intelligently to unfixed environments such as robotic assembly tasks, Project Number: 20008613). This research was also partially supported by an HPC Support project of the Korea Ministry of Science and ICT and NIPA.}
\end{spacing}

{\small
\bibliographystyle{unsrt}
\bibliography{egbib}
}

\end{document}
