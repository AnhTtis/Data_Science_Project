%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{url}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{etoolbox}
\usepackage{placeins}
\usepackage{hyperref}
\usepackage{setspace} 
\usepackage{romannum}
\usepackage{wasysym}
\let\Square\relax
\usepackage{bbding}
\usepackage{pifont}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{array}
\usepackage{cite}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}\usepackage{algpseudocode}
\usepackage{hhline}


\begin{document}

\title{\LARGE \
Learning to Place Unseen Objects Stably based on a Large-scale Simulation
}



\author{Sangjun Noh$^{*}$, Raeyoung Kang$^{*}$, Taewon Kim$^{*}$, Seunghyeok Back, Seongho Bak, Kyoobin Lee†% <-this % stops a space
\thanks{\text{*} These authors contributed equally to the paper.}
\thanks{All authors are with the School of Integrated Technology, Gwangju Institute of Science and Technology, Cheomdan-gwagiro 123, Buk-gu, Gwangju 61005, Republic of Korea. 
† Corresponding author: Kyoobin Lee {\tt\small kyoobinlee@gist.ac.kr}}%
}


\maketitle
% \thispagestyle{empty}
% \pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Abstract %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
The object placement approach requires a robot to efficiently pick and place objects despite an incomplete and noisy perception in the real world. Although several previous studies have proposed various object placement methods, their applicability was limited because prior works either assumed access to a full 3D model of object or were limited to place only specific object types. Thus, we propose Unseen Object Placement (UOP), a method for directly detecting the stable planes of an unseen object from a single-view and partial point cloud. We trained our model using large-scale simulation data to generalize over the object shape and properties of the stable plane with a 3D point cloud. Our proposed method was verified through simulations and real-world robot experiments. Our approach achieves state-of-the-art performance in both simulations and the real world, where the target objects are single-view and partial, despite the model being purely trained through simulations. Our codes, dataset and additional results are available at \url{https://sites.google.com/view/ailab-uop/home}.




\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Introduction %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
Picking and placing unseen objects is an essential skill for robots operating in an unstructured environment. Previous studies have primarily focused on robots grasping previously unperceived objects\cite{mahler2017dex, mousavian20196}, but only a few studies\cite{gualtieri2018pick, mitash2020task, gualtieri2021robotic} have been conducted for the unseen objects placement. Owing to a limitation on the number and type of objects that can be modeled in a complex/dynamic environment, it is a challenging task for robots to recognize different objects and reason about their geometry and physical properties. Furthermore, when it comes to objects (e.g., mechanical equipment, glasses, plates, and bowls) that need to be handled carefully in industries or households, the robots should be designed to pick and place the objects stably. Prior studies for placing objects stably have been presented, however, they require the full shape of 3D object models\cite{haustein2019object, gualtieri2021robotic, tournassoud1987regrasping}. Though \cite{jiang2012learninga, jiang2012learningb} succeeded to place various objects (e.g, plates and cups) on supporting items (e.g., rack, pen holder, and stemware holder), but limited to task-specific categories.  


% ---- 
% Fig 1.
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure 1/Figure_1.png}
        \end{subfigure}
        
    \caption{\textbf{Comparison of UOP-Net (Ours) and prior works.} Previous object placement approaches applied (a) a complete object model\cite{haustein2019object, trimesh, hagelskjaer2019using} or (b) completion module\cite{gualtieri2021robotic} or (c) only fitted a primitive shape to the object\cite{fischler1981random, Zhou2018}. Contrarily, (d) UOP-Net can reason to detect a stable plane for unseen objects from a single partial view.} 
    \label{fig:task_comparison}
\end{figure}

% object (17,408 => train: 13926, val:3482 ) // anno (69,027 => train :55261, val:13766)
This study proposes an approach called Unseen Object Placement (UOP), by detecting a plane to place the object stably in scenarios where either partial or unseen observation of objects is given. We generate a large-scale synthetic dataset using a physics simulator called UOP-Sim by dropping various objects and sampling stable planes. While \cite{jiang2012learninga, jiang2012learningb} labeled the preferred placing configuration using a heuristic technique, we automatically annotated all possible planes where the object can sustain a stable pose based on our definition of stability. Our dataset (UOP-Sim) contains 17.4k 3D objects and a total of 69k annotations of stable planes. Furthermore, we propose a deep learning-based network named UOP-Net, that predicts the most stable plane to place an object using the partial observation data. We achieved state-of-the-art (SOTA) performance in both simulation and real-world environments, even when single partial views and unseen objects were given. We trained UOP-Net using only the simulation dataset without fine-tuning and evaluated its performance on three benchmarks (3DNet, ShapeNet, and YCB). The contributions of this study are summarized as follows:


\begin{itemize}
    \item{We propose a new task, Unseen Object Placement (UOP), to place an unseen object stably from a single and partial point cloud.}
    
    \item{We provide a public large-scale 3D synthetic dataset, called UOP-Sim, containing a total of 69,027 stable annotations for 17,408 different objects.}
    
    \item{We propose a UOP-Net that can directly detect the most stable plane from an unseen object and train the model using only synthetic dataset without fine-tuning.}
    
    \item{We compared our UOP-Net with previous object placement approaches and achieved an SOTA performance through simulations and real world environments.}

\end{itemize} 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Related Works %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Works}
\noindent\textbf{Robotic Applications of Object Placement}
Recently, a wide range of object placement problems has been investigated, such as a constrained placement\cite{mitash2020task}, upright placement\cite{newbury2021learning}, rearrangement\cite{wada2022reorientbot, paxton2022predicting} and packing\cite{wang2021dense}. Mitash et al.\cite{mitash2020task} utilized a multi-view-based primitive shape-fitting method to address the constrained placement problem in the absence of object models. In \cite{paxton2022predicting}, a deep learning network was proposed that determines the required rotation of an object so that it is stably placed in an upright orientation. Similarly, Li et al.\cite{listable} presented an approach for predicting the rotation that can stably maintain the object with the height-maximizing pose. 


\noindent\textbf{Stable Object Placement}
Robots can stably place an object when its geometrical properties are known. Analyzing the convex hull of an object using its center of mass and sampling the explicit planes where the object rests in a stable configuration can enable its stable placement on a horizontal surface\cite{tournassoud1987regrasping, wan2019regrasp,lertkultanon2018certified, haustein2019object}. However, applying the analytical method in an actual environment is impractical because it requires precise information about the object, which is inaccessible due to a partial observation (e.g., an RGB-D camera). 



In \cite{gualtieri2021robotic}, although researchers utilized a deep learning-based completion method to address the uncertainty of partially visible objects in the real world, the completion module may not be sufficiently accurate to generate the precise shape of an object when the object is unknown. On the other hand, our method can directly detect a stable plane from partial observations without requiring any other modules. Moreover, our model can detect implicit planes as well as explicit planes.


\noindent\textbf{Unseen Object Placement}
In a pioneering work, Jiang et al.\cite{jiang2012learninga} trained a classifier using a hand-crafted dataset to identify placements that are likely to be stable and satisfy human preferences. They validated the feasibility of placement and human preferences; however, full observability was required, and the dataset labels were heuristic. To overcome these limitations, Cheng et al.\cite{cheng2021learning} presented a deep learning model based on simulations; however, task-specific objects were still required. Unlike these methods, which aim to place an object stably on various surfaces (e.g., rack and stemware holder), we aim to place an unseen rigid object stably on a horizontal surface, even when only a single partial observation is accessible.  


%Pick and Place Without Geometric Object Models, gualtieri2018pick
%Task-driven Perception and Manipulation for Constrained Placement of Unknown Objects, mitash2020task
%Robotic Pick-and-Place With Uncertain Object Instance Segmentation and Shape Completion, gualtieri2021robotic




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 2.
\begin{figure*}[ht!]
    \centering
        \includegraphics[width=\textwidth]{figures/Figure 2/Figure_2.png}
  \caption{\textbf{UOP-Sim dataset generation pipeline.} (Left) The database contains a total of 17.4K 3D object mesh models, that we built to place and estimate the stability (\mathcal{S}) of the object. (Top) We drop each object on the table in 512 different poses and sample stable planes from the objects that satisfy Eq.\ref{eq:stability}. (Bottom) We verify the stable plane candidates with the tilted table. (Right) Over 17.5K 3D object models and 69K annotations of stable planes are included in the full dataset (UOP-Sim).}
    \label{fig:UOP-sim}
\end{figure*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Problem Statement %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Statements}
We considered the problem of Unseen Object Placement (UOP) from a single-view observation, where a robot picks up by a grasp generator (e.g., 6-DoF GraspNet\cite{mousavian20196}) or receives an unseen object from a human.


%%%%%definitions
\subsection{Definitions}
\noindent\textbf{States}: The manipulating scene consists of a robot $\mathcal{R}$, an object $\mathcal{O}$, and a camera $\mathcal{C}$ and pose $\mathcal{T}_\mathcal{C}$. Robot $\mathcal{R}$ grasps target object $\mathcal{O}$ with pose $\mathcal{T}_\mathcal{O}$. Thus, the state $\mathcal{Z}$ can be denoted as $\{\mathcal{O}, \mathcal{T}_\mathcal{O}, \mathcal{T}_\mathcal{C}\}$.


\noindent\textbf{Point Clouds}: Given the depth image captured by the depth camera with known intrinsic and extrinsic, we sample a set of 3D points $\{x_i | i = 1, ... ,n\}$, where each of the $n$ points is a vector of Euclidean coordinate $\mathbb{R}^3$. 


\noindent\textbf{Object Stability and Stable Planes}: Let $\mathcal{S}_\mathcal{T}^\mathcal{O}$ denote the placement stability of an object model $\mathcal {O}$ at the grasped pose $\mathcal{T}$. Although Jiang et al.\cite{jiang2012learninga} computed object stability using the kinetic energy in a simulation, they only considered the initial and final states. Thus, we define \textit{ stability } $\mathcal{S}$ as the total amount of homogeneous transformation change at the world coordinate $\mathcal{W}$ in a simulator during a discrete time step $L$. Based on \textit{ stability } $\mathcal{S}$, stable planes were annotated for each object model, which satisfy the condition of $\mathcal{S}_\mathcal{T}^\mathcal{O} < \epsilon$.

\noindent\textbf{Dataset and Deep Learning Model}: The dataset $\mathcal{D} = \{({O}_m, \mathcal{A}_m) \}_{1}^{M}$ represents the set of M object models $\mathcal{O}$ (e.g., point clouds) and the stable placement plane $\mathcal{A}$ as the ground truth. $\mathcal{D}_{train}$ and $\mathcal{D}_{test}$ are the training and test datasets, respectively, and they satisfy the following state: $\mathcal{D}_{train} \cup \mathcal{D}_{test} = \mathcal{D}$. Let function $\mathcal{F}: \mathcal{X}_p \rightarrow \mathcal{A}$ be a deep learning model that takes one object as the input and produces a stable plane $\mathcal{A}$ as the output.

\noindent\textbf{Dataset and Deep Learning Model}: The dataset $\mathcal{D} = \{({O}_m, \mathcal{A}_m) \}_{1}^{M}$ represents the set of M object models $\mathcal{O}$ (e.g., point clouds) and the stable placement plane $\mathcal{A}$ as the ground truth. $\mathcal{D}_{train}$ and $\mathcal{D}_{test}$ represent the training and test set, respectively, and they satisfy the following state: $\mathcal{D}_{train} \cup \mathcal{D}_{test} = \mathcal{D}$. Let the function $\mathcal{F}: \mathcal{X}_p \rightarrow \mathcal{A}$ be a deep learning model that adopts one object as the input and produces a stable plane $\mathcal{A}$ as the output.


\noindent\textbf {Seen and Unseen Objects}: If $\mathcal{O}_{\mathcal{D}_{train}} \cap \mathcal{O}_{\mathcal{D}_{test}} = \emptyset$, $\mathcal{O}_{\mathcal{D}_{train}}$ is the seen object and $\mathcal{O}_{\mathcal{D}_{test}}$ is unseen for the deep learning model $\mathcal{F}$. 

%%%%%%%objectives
\subsection{Objectives} 
Our goal is to detect a stable plane for the placement of arbitrary objects using only single-view observations. Thus, we aimed to develop a neural network $\mathcal{F}: \mathcal{\hat{X}} \rightarrow \methcal{A}$ that minimizes the \textit{stability} $\mathcal{S}_\mathcal{T}^\mathcal{O}$ when partial point clouds $\mathcal{\hat{X}}$ are given.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Method %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Learning Unseen Object Placement}
Solving an UOP function that predicts a robust plane is challenging for several reasons. First, large-scale samples are required to approximate the expectations of multiple possible objects. Second, learning through a simple linear or mathematical model can be difficult because the relationship between the objects and annotated metrics is complex, considering both the geometry and physics. To address these issues, we generated a synthetic dataset called UOP-Sim, containing 17k 3D object models (point clouds), and 69k labeled stable planes. Moreover, we developed UOP-Net, which learns to detect robust stable planes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Synthetic Data Generation}
As there are a vast number of different types of object models, repeating the real-data acquisition and annotation process to collect a sufficient amount of data is time-consuming and impractical. Since 3D CAD models of novel objects are freely available online (e.g., 3D-Net, ShapeNet and YCB), a synthetic dataset can be generated in a virtual environment rather than real-data collection and labeling. Thus, We used a dynamic simulator to create a synthetic dataset for the training and validation of UOP-Net, which can accelerate modeling cycles from data collection to deployment in accordance for various objects. Fig.\ref{fig:UOP-sim} illustrates our overall pipeline for synthetic dataset generation, which is made up of two processes. We begin by randomly dropping a rigid object on a horizontal surface to search for all the possible stable plane candidates. Subsequently, the stable planes for each object are annotated by placing them on a tilted table with the sampled candidates.


\noindent\textbf{Simulation Environment Setting.}
% object (17,408 => train: 13926, val:3482 ) // anno (69,027 => train :55261, val:13766)
We used the benchmark datasets, 3DNet\cite{wohlkinger20123dnet}, ShapeNet\cite{chang2015shapenet}, and Yale-CMU-Berkeley (YCB)\cite{calli2015ycb}, for a total of 17,408 3D object models. Using PyRep\cite{james2019pyrep} and CoppeliaSim\cite{rohmer2013v}, we created a learning environment to estimate the object stability during a discrete time step subsequent to the instant when the object mesh established a contact with the table plane. In the virtual environment, we built 64 table object models to accelerate the annotation process.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 3.
\begin{figure*}[ht!]
   \centering
      \includegraphics[width=\textwidth]{figures/Figure 3/Figure_3.png}

    \caption{\textbf{Overall pipeline of UOP-Net}. (Center) We generate partial point clouds using the UOP-Sim dataset since we aim to identify the stable planes directly in case of partially observable scenarios. UOP-Net then learns to predict the most stable plane. (Left) When the target object in the robot gripper is observed using an RGB-D camera, a partial point cloud is fed into UOP-Net. (Right) Using the estimated stable plane, the robot executes object placement based on the angle difference between the normal vector of the plane and the negative gravity vector.}


  \label{fig:UOP-net}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 512가지 포즈 (along eight grids of roll, pitch, and yaw).
% randomly release / drop
% 멈췄을 때 물체의 자세
% plane의 normal을 z축 하단 을 기준으로 5% 가 되는 영역의 normal vector를 저장 => plane 만들때
% plane을 horizontal surface에 놓고 정지상태가 되었을 때 기울임 => stability를 측정
% time step 0.005s 각 time step 마다 물체의 포즈 


\noindent\textbf{Stable Plane Annotation.}
To estimate object stability $\mathcal{S}$ in a rigid-body simulation, we first define the object movement $\mathcal{M}$ using a homogeneous transformation matrix $\mathcal{H}_\mathcal{O} \in \mathbb{SE}(3)$, where the transformation matrix comprises the translation $\mathbf{T}_\mathcal{O}^\mathcal{W} \in \mathbb{R}^3$ and rotation $\mathbf{R}_\mathcal{O}^\mathcal{W} \in \mathbb{SO}(3)$ matrices based on the world coordinate $\mathcal{W}$ in the simulator. Since the object pose $\mathcal{H}_i$ at each time step $i$ can be computed through simulations, the movement $\mathcal{M}$ can be calculated as object pose variance $||\mathcal{H}_i - \mathcal{H}_{i-1}||_2^\mathcal{W}$ (Eq.\ref{eq:movement}). Consequently, we define object stability $\mathcal{S}$ as the average of the accumulated movement across discrete time steps $L$ (Eq.\ref{eq:stability}) as follows:. 

\begin{equation}
\label{eq:movement}
\mathcal{M} = ||\mathcal{H}_i - \mathcal{H}_{i-1}||_2^\mathcal{W}, where \ \mathcal{H}=[\mathbf{R}|\mathbf{T}]
\end{equation}

\begin{equation}
\label{eq:stability}
\mathcal{S} = {\frac{1}{L}}\sum_{i=1}^L\mathcal{M}
\end{equation}

Considering that the pose $\mathcal{T}_\mathcal{O}$ of a rigid object in the grip of the robot is uncertain, 512 orientations were generated to estimate various poses for the object by dividing each roll, pitch, and yaw into eight grids. A rigid object $\mathcal{O}$ was set with a random pose in the normal direction of the table plane using a small value $\epsilon$. To search for all possible stable planes of the rigid object, we dropped the object on the table and sampled all poses at the moment that the object was stabilized (${\mathcal{S} < \delta_1}$). The sampled poses were then clustered using density-based spatial clustering of applications with a noise algorithm\cite{ester1996density} along the z-axis, which represents the normal vector of the contact of a stable plane with a horizontal surface. After rotating the object to align it along the direction of the clustered normal vector and gravity vector, the bottom 5\% of the regions along the z-axis (world coordinate) of the 3D object model were masked as areas that could sustain the stability of the object.


Since real environments cannot be perfectly simulated, certain planes cannot be easily generalized (e.g., a spherical model or sides of a cylinder), even if the sampled and clustered planes are reasonable in the simulation. Thus, each object was placed on a flat table with a normal vector of the plane candidates, and the table was tilted by 10 degrees. We estimated the object movement across each time step $L$ and eliminated the planes that did not satisfy ${\mathcal{S} < \delta_2}$. By its verification process, we can annotate stable planes that are robust for application to a horizontal surface; samples of the UOP-Sim dataset are shown in Figure \ref{fig:UOP-sim}. Our dataset contained both explicit and implicit planes (e.g., a flat surface formed by four chair legs), and consequently, a total of 17,408 3D object models and 69,027 stable plane annotations were generated.


To respond appropriately to partial observations in the real world, we captured 3D object models from the UOP-Sim dataset using a synthesized depth camera with 1,000 random poses. The camera was initialized with the values of the Azure Kinect intrinsic parameters. A partial point cloud (50,000 points) was sampled using a voxel down sampling method\cite{Zhou2018} from the depth image. In the case of labeling, annotations from the UOP-Sim dataset were sampled after the poses of the partial point cloud and 3D object model were aligned. However, the stable regions of the partial point cloud may be significantly small compared to those of the full object model. For example, when a four-legged chair model has only two legs in a partial point cloud from a particular perspective, it is not reasonable to create a stable plane only using the two legs. To address this problem, we estimated the angular error from the normal vectors of each stable region and removed the annotations from the partial point cloud where the error exceeded 10 degrees (Fig.\ref{fig:UOP-net} (b)).


% real world, no fine tune 꼭 추가할 것
\subsection{Deep Neural Network for Unseen Object Placement} 

We formulated the detection of the most stable plane as a process of predicting stable planes in the visible regions of the observed object so that the object can be stably placed on a flat surface for real world applications. Furthermore, the process should detect planes that cover all possible ways to maintain an object in a stable configuration for various partial views. Segmenting the stable region of unseen objects is challenging because the model should learn to generalize both object shapes and plane properties. Thus, we propose a deep learning framework called UOP-Net that directly detects the most stable plane under partial observation, even when unseen objects are given. The observed point cloud of the object, that the robot should place, is used as an input in the proposed approach.

Specifically, let us assume that a segmented object point cloud (partial point cloud) $\mathcal{\hat{X}} = \mathbb{R}^{N \times 3}$ in gripper is provided. Our objective is to learn a function $\mathcal{F}: \mathcal{\hat{X}} \rightarrow \mathcal{A}$, where A is the most stable plane in a set of predicted planes $\{a_1,...,a_n\}$. Using $\mathcal{A}$, a rotation $\mathcal{R} \in \mathbb{SO}(3)$ was determined to place the object on a horizontal surface in a stable configuration.


% Furthermore, our goal includes detecting implicit planes (e.g., a tripod or four-legged chair), whereas previous works\cite{} that segment planes from point cloud can only detect explicit planes(e.g., building plane). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Unseen Object Placement Network (UOP-Net)} 
Because there are various object shapes, and an object can have multiple stable planes, a function that generalizes the features of the stable planes is required. Therefore, we propose UOP-Net based on category-agnostic instance segmentation, which includes both the concept of object-ness and properties for stable planes from a large-scale synthetic dataset (UOP-Sim). 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Table 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% rotation(degree), translation(meter) 단위 체크, Translation * E-05
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
% \usepackage{dcolumn}
\begin{table*}[ht!]
\caption{{UOP} Performances of UOP-Net and baselines on three benchmark datasets (unseen object) in simulation.}
\centering
\label{tab:simulation-eval}
\resizebox{\textwidth}{!}{%
{\renewcommand{\arraystretch}{1.2}
\LARGE{
\begin{tabular}{|cccccccccccccc|}
\hline
% \multicolumn{14}{|c|}{\textbf{Stable Object Placement Evaluataion Metric}} \\ \hline
\multicolumn{2}{|c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}\\ Object Type \& \\ Dataset\end{tabular}}} & \multicolumn{8}{c|}{ Object Stability (S)} & \multicolumn{4}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Success Rate of \\ Object Placement (\mathcal{SR}, $\%$)\end{tabular}}} \\ \cline{3-10}
\multicolumn{2}{|c|}{} & \multicolumn{4}{c|}{Rotation (R, $^{\circ}$) \downarrow} & \multicolumn{4}{c|}{Translation (T, $cm$) \downarrow} & \multicolumn{4}{c|}{} \\ \cline{3-14} 
\multicolumn{2}{|c|}{} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}UOP \\ \textbf{(Ours)}\end{tabular}} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}UOP \\ \textbf{(Ours)}\end{tabular}} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \begin{tabular}[c]{@{}c@{}}UOP \\ \textbf{(Ours)}\end{tabular} \\ \hline
% Complete Point Cloud
\multicolumn{1}{|c|}{\multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Complete \\ Point cloud\end{tabular}}} & \multicolumn{1}{c|}{3DNet\cite{wohlkinger20123dnet}} % 3DNet
& 5.03 & 20.84 & 16.22 & \multicolumn{1}{c|}{\textbf{5.01}} % Rotation
& \textbf{0.40} & 2.09 & 1.48 & \multicolumn{1}{c|}{0.50}  % Translation
& \textbf{87.03} & 58.23  & 73.44  & 83.14  \\ % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{ShapeNet\cite{chang2015shapenet}} % ShapeNet
& \textbf{2.02} & 19.44 & 10.57 & \multicolumn{1}{c|}{2.60} % Rotation
& \textbf{0.16} & 2.41 & 1.15 & \multicolumn{1}{c|}{0.30} % Translation
& \textbf{95.17} & 54.26  & 81.51  & 88.75  \\ % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{YCB\cite{calli2015ycb}} % YCB
& 4.42 & 37.59 & 8.58 & \multicolumn{1}{c|}{\textbf{3.08}} % Rotation
& \textbf{0.35} & 5.96 & 0.87 & \multicolumn{1}{c|}{0.30} % Translation
& \textbf{89.27} & 49.89  & 83.84  & 87.84  \\ 
\hhline{|~|=|====|====|====|} % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Total avg.} % Total
& 4.26 & 24.43 & 13.27 & \multicolumn{1}{c|}{\textbf{4.06}} 
& \textbf{0.34} & 3.05 & 1.27 & \multicolumn{1}{c|}{0.41} 
& \textbf{89.25} & 55.47  & 77.54  & 85.41  \\ \hline % Success Rate
% Partial Point Cloud
\multicolumn{1}{|c|}{\multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Partial \\ Point cloud\end{tabular}}} & \multicolumn{1}{c|}{3DNet\cite{wohlkinger20123dnet}} % 3DNet
& 23.19 & 28.79 & 35.64 & \multicolumn{1}{c|}{\textbf{16.40}} % Rotation
& 2.48 & 3.17 & 3.74 & \multicolumn{1}{c|}{\textbf{1.91}}  % Translation
& 54.87 & 41.17 & 50.45 & \textbf{55.47}  \\ % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{ShapeNet\cite{chang2015shapenet}} % ShapeNet
& 21.97 & 28.70 & 25.80 & \multicolumn{1}{c|}{\textbf{14.90}} % Rotation
& 2.91 & 3.56 & 2.90 & \multicolumn{1}{c|}{\textbf{1.59}} % Translation
& 55.78 & 38.92 & 58.23 & \textbf{60.79}  \\ % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{YCB\cite{calli2015ycb}} % YCB
& 35.49 & 38.46 & 39.13 & \multicolumn{1}{c|}{\textbf{15.92}} % Rotation
& 5.73 & 5.98 & 5.19 & \multicolumn{1}{c|}{\textbf{2.37}} % Translation
& 48.14 & 40.22 & 52.66 & \textbf{62.53} \\ 
\hhline{|~|=|====|====|====|} % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Total avg.} % Total avg.
& 25.78 & 31.01 & 34.49 & \multicolumn{1}{c|}{\textbf{15.97}} % Rotation
& 3.32 & 3.90 & 3.90 & \multicolumn{1}{c|}{\textbf{1.95}} % Translation
& 53.50 & 40.48 & 52.59 & \textbf{58.22} \\ \hline % Success Rate

\end{tabular}
}
}
}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


As shown in Fig.\ref{fig:UOP-net}, the UOP-Net employes the DGCNN\cite{wang2019dynamic} network to extract the global shape features before splitting into two different branches: one for semantic segmentation, which predicts for whether the point is stable or unstable, and one for embedding instance features. Similar to the method in \cite{pham2019jsis3d, wang2019associatively}, instance labels (stable points) were determined by applying the mean-shift clustering algorithm\cite{comaniciu2002mean}. Given the predicted stable points, RANSAC\cite{fischler1981random} was used to fit a plane on the clustered points\cite{Zhou2018}. UOP-Net calculates the stability scores for each plane by element-wise multiplication of the semantic logits and predicted instance labels with the number of points composing each plane. UOP-Net produces only one plane with the highest score after fitting the planes and assigning stability scores based on the number of inlier points that constitute the planes. Finally, the rotation value $\mathcal{R}$ is determined by estimating the difference angle between the predicted normal vector of the stable plane and gravity vector (negative table surface normal).


\begin{equation}
\label{eq:the_sum_of_losses}
\mathcal{L} = \lambda_1 * \mathcal{L}_{stable} + \lambda_2 * \mathcal{L}_{plane},
\end{equation}

\noindent where $\mathcal{L}_{stable}$ is the standard binary cross-entropy loss and $\mathcal{L}_{plane}$ is the discriminative loss\cite{de2017semantic}. To allow the two losses to attain comparable values, the hyperparameter weights $\lambda_1, \lambda_2$ were set to 10, 1 respectively. 


\noindent\textbf{Training Details.} Each batch of training data for UOP-Net consisted of a rendered image of the object from a random view and stable plane annotations that were sampled using the verification process to ensure that the sampled planes are reasonable. During the training process, we randomly sampled 2,048 points for each object and utilized conventional augmentation (rotation, sheer, point-wise jittering and Gaussian noise) for real-world application. In addition, we trained the UOP-Net in the absence of real data. We employed Pytorch\cite{paszke2019pytorch} to train UOP-Net and we used one NVIDIA Titan RTX GPUs with a batch size of 32 and 1,000 epochs. Early stopping was set with patience = 50. The Adam optimizer\cite{kingma2014adam} was used at a learning rate = 1e-3. 

\section{Experiments in Simulation}
\noindent\textbf{Datasets.} 
We generated 69k stable plane annotations for a total of 17.4k 3D objects using two benchmark datasets: 3DNet\cite{wohlkinger20123dnet} and ShapeNet\cite{chang2015shapenet}. We subsequently split the synthetic data into training and validation sets at a ratio of 8:2. The YCB\cite{calli2015ycb} object models were labeled in the simulation as well, but they were excluded from the training set so that they could be utilized as a test set in both the simulation and real-world experiments. We avoided using objects that were devoid of a stable plane, such as spherical objects. Consequently, the numbers of object categories used for each benchmark dataset were 152, 57, and 63, respectively. The training set had 13,926 objects and 55,261 annotations, while the validation set had 3,482 objects and 13,766 annotations. 

\noindent\textbf{Baselines.}
We compared the performance of our method with the following baselines:
\begin{itemize}
    \item \textbf{Convex Hull Stability Analysis (CHSA)}\cite{haustein2019object, trimesh, hagelskjaer2019using} : This method computes the rotation matrix required for an object to rest in its stable pose on a planar surface. Given the object convex hull, the method samples the location of the center of mass and calculates the stable resting poses of the object on a flat surface. Then, the method evaluates the probabilities of landing in each pose and outputs the most probable pose. 

    \item \textbf{Bounding Box Fitting (BBF)}\cite{mitash2020task, Zhou2018} : This approach computes the oriented bounding box using principal component analysis (PCA), which minimizes the difference between the convex hull volume and the bounding box. Subsequently, the fitted object is placed with the largest area on a planar workspace.
    
    
    \item \textbf{RANSAC Plane Fitting (RPF)}\cite{fischler1981random, Zhou2018} : This method, given a point cloud, segments planes that satisfy $ax + by + cz + d = 0$ for each point $(x, y, z)$. Then, the method iteratively samples several points at random to construct a random plane and determines the plane with the frequency with which the same plane is founded.
    
\end{itemize}

\noindent\textbf{Evaluation Metrics.}
We used two metrics to qualitatively evaluate the efficacy of the UOP method, namely, \textit{object stability (\mathcal{S})} and \textit{ success rate of object placement (\mathcal{SR})}. For an unseen object, we estimated its stability (\mathcal{S}) when the object was placed on a flat table using the predicted results. We need to verify whether our model allows the object to preserve a stable pose after we annotate labels and learn to reason the plane features. The other evaluation metric is the ratio of successful stable placements among all predicted planes with an accumulated rotation error of less than 10 degrees. When an object is placed in an unstable state, it falls after being exposed to any kind of vibrations. At this moment, the rotational motion is more common than the translational movement. Therefore, we only considered the rotational motion for evaluating the object stability. Object placement was attempted 100 times for each object, and if no planes were detected, we abandoned the object and regarded it as a failed case.


\begin{itemize}
    \item \textbf{Object Stability (\mathcal{S})} : This metric represents the amount of movement of the object during a discrete time step when it is placed on a horizontal surface with the predicted plane.
    
    \item \textbf{Success Rate of Object Placement (\mathcal{SR})} : The percentage of placements where the object stays stationary for a minute and the accumulated rotation is less than $10^{\circ}$.
\end{itemize}

% it is ambiguous to set an appropriate threshold where the translation of an object can occur due to external environmental factors (e.g., slipping due to friction force).
% rotation error는 평면의 고유 영향만을 고려할 수 있다~


\noindent\textbf{Comparison with Baselines.}
For each 3D benchmark dataset (3DNet\cite{wohlkinger20123dnet}, ShapeNet\cite{chang2015shapenet} and YCB\cite{calli2015ycb}), we compared the performance of UOP-Net with other baselines for two object types (complete and partial shapes) and report the \textit{ object stability} and \textit{success rate of placement} in Table \ref{tab:simulation-eval}. Our UOP-Net achieves a SOTA performance in terms of evaluation metrics from a single partial observation. We visualize the prediction result of each method and represent the object stability (\mathcal{S}) as a graph in Fig.\ref{fig:sim_exp_result}.

The CHSA, which generates an object mesh (convex hull) from the point cloud and calculates mathematically stable planes based on the center of mass, performs best when the object shape is fully discernible. Though our model does not outperform CHSA, it demonstrates that it can perform well on unseen objects and that the predicted planes allow the object to retain its stability. However, the CHSA cannot perform partial observations. Because the convex hull generated in the partial point cloud does not contain the information regarding the invisible portion of the object, sampling an accurate center of mass is impractical. As illustrated in Fig.\ref{fig:sim_exp_result}, the CHSA method usually places the object in the truncated plane such that the object does not remain stationary. 
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 4 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 4.
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure 4/Figure_4.png}
        \end{subfigure}
        
    \caption{Visualization of prediction results on YCB\cite{calli2015ycb} dataset for each method and object stability in simulation.}
    \label{fig:sim_exp_result}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The BBF method exhibited the worst performance. This is because it did not consider the geometric properties of the object and placed the largest plane in the bounding box. It can be performed when the object shape is similar to a primitive shape; however, it cannot operate in case of a complex shape. Although RANSAC plane fitting (RPF) is better than BBF, it also fails to detect a robust stable plane. Conversely, our method (UOP-Net) can detect the robust plane based on incomplete perception because we generate a partial point cloud using SOP-Sim and train the model to learn the geometric properties of objects and planes. Furthermore, even if the object shape is complex, our model can detect both the implicit and explicit planes.

\noindent\textbf{Failure Cases.}
During the simulation experiments, we discovered several cases of failure. Although UOP-Net was trained with various object shapes, it could not detect stable planes of thin objects (e.g., spoons and knives) despite the existence of a predictable stable plane in case of such objects. Because the hyper-parameters of the mean-shift algorithm\cite{comaniciu2002mean} cannot be determined for all objects, UOP-Net may fail to cluster and place the object stably. % 인과관계 설명이 좀 이상함

\section{Experiments in the real world}
\noindent\textbf{Real Environment Setting.}
To demonstrate the feasibility of our proposed approach and the potential of Sim2Real transfer, we implemented our object placement method on a universal robot (UR5) manipulator using a single Azure Kinect RGB-D camera (Fig.\ref{fig:real_experiment} (b)). We used MAnet\cite{fan2020ma} with Densenet121\cite{iandola2014densenet} as the backbone to segment the target object and the gripper. We began by segmenting the visible region of the target object from the RGB image, and then cropped the depth image with the mask. The point cloud is then sampled from the depth image using voxel-down sampling\cite{Zhou2018}. For the model input, 2,048 points were randomly sampled and fed into UOP-Net. The most stable plane is predicted, and the rotation value between the plane and the table was calculated. The UR5 robot then placed the target object on the table. For planar motion, we utilized BiRRT\cite{qureshi2015intelligent} implemented with Pybullet\cite{coumans2021} and integrated with collision checking on a physics engine.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 5 %%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure 5/Figure_5.png}
        \end{subfigure}
        
    \caption{\textbf{Unseen object placement using UOP-Net}. (a) Given a segmented target object, UOP-Net takes 3D point clouds observed by a depth camera (Azure kinect) as input and predicts the most stable plane. (b) The UR5 robot places the target object on the table.}
    \label{fig:real_experiment}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Table 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Table 2
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[ht!]
\caption{\textbf{UOP} performances of UOP-Net and baselines on YCB\cite{calli2015ycb} in the real world.}
\label{tab:real world-eval}
\resizebox{\columnwidth}{!}{%
\LARGE{
{\renewcommand{\arraystretch}{}
\begin{tabular}{|c|cccc|}
\hline
YCB\cite{calli2015ycb} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \begin{tabular}[c]{@{}c@{}}UOP-Net \\ \textbf{(Ours)}\end{tabular} \\ \hline
Coffee Can & 0 / 10 & 0 / 10 & 4 / 10 & \textbf{10} / 10 \\
Timer & 0 / 10 & 1 / 10 & \textbf{6} / 10 & \textbf{6} / 10 \\
Power Drill & 0 / 10 & 1 / 10 & \textbf{5} / 10 & \textbf{5} / 10 \\
Wood Block & 1 / 10 & 1 / 10 & \textbf{10} / 10 & \textbf{10} / 10 \\
Metal Mug & 0 / 10 & 0 / 10 & 6 / 10 & \textbf{9} / 10 \\
Metal Bowl & \textbf{10} / 10 & 5 / 10 & \textbf{10} / 10 & \textbf{10} / 10 \\
Bleach Cleanser & 3 / 10 & 3 / 10 & \textbf{9} / 10 & \textbf{9} / 10 \\
Mustard Container & 2 / 10 & 0 / 10 & 5 / 10 & \textbf{10} / 10 \\
Airplane Toy & 0 / 10 & 3 / 10 & 0 / 10 & \textbf{4} / 10 \\
Sugar Box & 2 / 10 & 3 / 10 & \textbf{10} / 10 & \textbf{10} / 10 \\
Chips Can & 2 / 10 & 0 / 10 & 8 / 10 & \textbf{10} / 10 \\
Banana & 5 / 10 & 5 / 10 & \textbf{9} / 10 & \textbf{9} / 10 \\ \hhline{|=|=|=|=|=|}
Average & 2.1 / 10 & 1.8 / 10 & 6.8 / 10 & \textbf{8.5} / 10 \\ \hline
\end{tabular}%
}
}
}
\end{table}

\noindent\textbf{Evaluation Metrics.}
Because the object stability (\mathcal{S}) can not be computed accurately in the real world, we only considered the \textit{success rate of object placement (\mathcal{SR})}. While evaluating success rate in the simulations, we set the position of the object on the table and determined that the placement was successful if the target object did not fall and remained stationary when the robot place it on the table with the predicted plane by the UOP-Net. Also, if no planes can be detected, we considered the trial to be a failure. For each object, we ran 10 trials to place. 


\noindent\textbf{Comparison with Baselines.}
We selected 12 objects from the YCB objects. Objects with spherical shapes(e.g., apples), dimensions that were too small, or low depth values were excluded from the test set. Table \ref{tab:real world-eval} shows that our method outperforms the other baselines in terms of the success rate across all objects. Even though real-world perception is noisy, UOP-Net provides a stable plane. This can be attributed to the fact that our model learned from a large number of partial point clouds that were captured by a depth camera and trained with noise. Other benchmarks (CHSA and primitive shape fitting) perform very poorly because they cannot obtain the full shape of an object in the real world and cannot respond to sensor noise. To further verify that our model can perform as efficiently in case of unseen objects, we experimented our method on novel objects that did not have a CAD model (a dinosaur product and an ice tray in Fig.\ref{fig:comparison_pred_res}). Although the object shape is complex, UOP-Net detects an implicit plane (e.g., four legs). 


\noindent\textbf{Failure Cases.}
Since we did not consider the robot gripper in the process of generating partial point cloud in the simulation, UOP-Net sometimes fails to detect the stable plane when the target object is densely occluded by the robot gripper. Though we showed that UOP-Net can detect an implicit plane from complex object shapes (e.g., YCB airplane toy, dinosaur), the recall is not high. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 7 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 7. -> 6
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure 6/Figure_6.png}
        \end{subfigure}
        
    \caption{Visualization of prediction results for each method on YCB\cite{calli2015ycb} objects and novel objects in the real world.}
    \label{fig:comparison_pred_res}
\end{figure}





% object (17,408 => train: 13926, val:3482 ) // anno (69,027 => train :55261, val:13766)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Conclusion %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion and Future Work} 
This work proposes a method, UOP-Net, for detecting the stable planes of an object in the real world, including unseen objects. We first introduced the UOP-Sim pipeline for generating synthetic datasets, which comprise of 17.4k 3D objects and 69k stable plane annotations. Our model learns to detect stable planes directly from various objects under partial observation by using only a synthetic dataset (UOP-Sim). We demonstrated the accuracy and reliability of UOP-Net in detecting stable planes from unseen and partially observable objects on three benchmark datasets with SOTA performance. 

The proposed method can be extended and improved in several ways. First, our model requires post-processing to cluster and fit the plane when segmenting each instance plane. Because the hyperparameters should be set for experimental environments, post-processing algorithms are heuristic. Thus, we hope to improve the model architecture to directly generate planes as an end-to-end pipeline. Second, our system only considers object placement on a horizontal surface, but we hope to extend and learn to predict a plane for object placement on other surrounding environments so that the robot can pack and stack.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Acknowledgement %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgement}
\begin{spacing}{0.4}
{\scriptsize This work was fully supported by the Korea Institute for Advancement of Technology (KIAT) grant funded by the Korea Government (MOTIE) (Project Name: Shared autonomy based on deep reinforcement learning for responding intelligently to unfixed environments such as robotic assembly tasks, Project Number: 20008613). This work was also partially supported by the HPC Support project of the Korea Ministry of Science and ICT and NIPA.}
\end{spacing}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{references.bib}
\bibliographystyle{IEEEtran}

\end{document}