%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{url}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{etoolbox}
\usepackage{placeins}
\usepackage{hyperref}
\usepackage{setspace} 
\usepackage{romannum}
\usepackage{wasysym}
\let\Square\relax
\usepackage{bbding}
\usepackage{pifont}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{array}
\usepackage{cite}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}\usepackage{algpseudocode}
\usepackage{hhline}


\begin{document}

\title{\LARGE \
Learning to Place Unseen Objects Stably via a Large-scale Simulation
}



\author{Sangjun Noh$^{*}$, Raeyoung Kang$^{*}$, Taewon Kim$^{*}$, Seunghyeok Back, Seongho Bak, Kyoobin Lee†% <-this % stops a space
\thanks{\text{*} These authors contributed equally to the study.}
\thanks{All authors are with the School of Integrated Technology, Gwangju Institute of Science and Technology, Cheomdan-gwagiro 123, Buk-gu, Gwangju 61005, Republic of Korea. 
† Corresponding author: Kyoobin Lee {\tt\small kyoobinlee@gist.ac.kr}}%
}


\maketitle
% \thispagestyle{empty}
% \pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Abstract %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Object placement is a critical skill for robots operating in unstructured environments, as it enables them to manipulate and arrange objects in a way that is safe and efficient. However, existing methods for object placement often have limitations that restrict their applicability, such as the need for a complete 3D model of the object or the inability to handle complex object shapes. In this paper, we present Unseen Object Placement (UOP), a novel method for directly detecting the stable planes of an unseen object from a single-view and partial point cloud. Our approach is particularly useful for scenarios where the shape and properties of the object are not fully known, as it allows the robot to adapt and place the object accurately. We train our model on large-scale simulation data to generalize over the shape and properties of the stable plane with a 3D point cloud. Our approach is verified through simulations and real-world robot experiments, achieving state-of-the-art performance for placing single-view and partial objects. Our code, dataset, and additional results are available at \url{https://sites.google.com/view/ailab-uop/home}.
\end{abstract}

% Object placement is a crucial skill for robots operating in unstructured environments. However, existing methods for object placement often have limitations that restrict their applicability, such as the need for a complete 3D model of the object or the inability to handle complex object shapes. In this paper, we present Unseen Object Placement (UOP), a novel method for directly detecting the stable planes of an unseen object from a single-view and partial point cloud. We train our model on large-scale simulation data to generalize over the shape and properties of the stable plane with a 3D point cloud. Our approach is verified through simulations and real-world robot experiments, achieving state-of-the-art performance for placing single-view and partial objects. Our code, dataset, and additional results are available at \url{https://sites.google.com/view/ailab-uop/home}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Introduction %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction} 
The ability to place unseen objects in a stable manner is essential for robots to operate effectively in unstructured environments, such as manufacturing, construction, and household tasks. However, it is infeasible to assume that all real-world objects have been modeled beforehand. Deep learning has been applied to recognize and manipulate unseen objects, but most of this research has focused on how to identify\cite{xie2021unseen, back2021unseen} or grasp\cite{mahler2017dex, mousavian20196} these objects. In this work, we propose a new task called Unseen Object Placement (UOP), which specifically addresses the problem of placing unseen objects in the real world.

Conventional approaches\cite{haustein2019object, trimesh, hagelskjaer2019using} for stable object placement typically require full 3D models and analytical calculations of stable planes. While these approaches can be effective in certain scenarios, they are not always applicable in the real world. One approach\cite{gualtieri2021robotic} is to combine analytical methods with a 3D object completion model, which can reconstruct the full shape of an object from raw perception data. However, this approach can still be challenging as the completed object may not be precise enough, leading to errors in the calculation of stable planes. Our UOP method addresses these limitations by directly detecting stable planes of unseen objects from a single-view and partial point cloud, eliminating the need for a complete 3D object model. This allows the robot to adapt and place the object accurately, even in situations where the shape and properties of the object are not fully known.

% -------------------------------------------------------------------------------------------------------------------------------------------
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure 1/Figure_1.png}
        \end{subfigure}
        
    \caption{\textbf{Comparison of UOP-Net (ours) and previous methods.} Previous studies for object placement have used (a) full-shape object models\cite{haustein2019object, trimesh, hagelskjaer2019using}, (b) completion modules\cite{gualtieri2021robotic}, or (c) fitted primitive shapes\cite{fischler1981random, Zhou2018}. In contrast, (d) our UOP-Net directly detects stable planes for unseen objects from partial observations.
}

    
    \label{fig:task_comparison}
\end{figure}

% -------------------------------------------------------------------------------------------------------------------------------------------

In this paper, we propose a method for Unseen Object Placement (UOP) that detects stable planes for placing objects based on partial observations. We train and evaluate our approach using a large-scale synthetic dataset called UOP-Sim, which contains 3D objects and annotations of stable planes generated using a physics simulator. Unlike previous approaches\cite{jiang2012learninga, jiang2012learningb} that rely on heuristics to label preferred placement configurations, we automatically annotate all possible planes that can support stable object poses. Our dataset includes 17.4k objects and a total of 69k annotations. We propose a convolutional neural network called UOP-Net that predicts the most stable plane from partial point cloud data, and train it using only the UOP-Sim dataset. We compare the performance of our approach with three baseline methods and show that it achieves state-of-the-art (SOTA) performance in both simulation and real-world experiments without any fine-tuning on real-world data. 

The main contributions of this study are as follows:
% object (17,408 => train: 13926, val:3482 ) // anno (69,027 => train :55261, val:13766)
\begin{itemize}    
    \item{We propose a new task called Unseen Object Placement (UOP) to place an unseen object stably from a single and partial point cloud.}
    
    \item{We provide a public large-scale 3D synthetic dataset,
    called UOP-Sim that contains a total of 69,027 annotations of stable planes for 17,408 different objects.}
 
    \item{We introduce a convolutional neural network named UOP-Net that can directly predict the most stable plane for an unseen object.}
    
    \item{We compare the performance of our approach with previous object placement methods and show that it achieves state-of-the-art results without any fine-tuning on both simulation and real-world environements.}

    
\end{itemize} 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Related Works %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Works}
\noindent\textbf{Stable Object Placement} Previous studies\cite{tournassoud1987regrasping, wan2019regrasp,lertkultanon2018certified, haustein2019object} have demonstrated that robots can stably place an object with known geometrical properties by analyzing the convex hull and sampling stable planes for the object. However, this approach requires precise object information, which may not be available in real-world scenarios with partial observations (e.g., from an RGB-D camera). Some researchers have tried to address this limitation with deep learning-based completion methods, but these approaches still have limitations in generating precise shapes of unseen objects. In contrast, our UOP method addresses these limitations by directly detecting stable planes from partial observations without the need for a complete 3D object model. Unlike deep learning-based completion methods, the UOP method is more generalizable and adaptable to real-world scenarios with partial observations, and can detect stable planes for unseen objects. 


\noindent\textbf{Unseen Object Placement} Previous studies on unseen object placement have focused on identifying stable placements that satisfy human preferences. For example, Jiang et al.\cite{jiang2012learninga} trained a classifier using a hand-crafted dataset to identify such placements. However, this approach relied on heuristic labels and requires complete observability. Cheng et al.\cite{cheng2021learning} proposed a deep learning model based on simulations to address the issue of heuristic labels, but this approach was limited to task-specific objects. Another common approach to placing unseen objects is to use bounding box fitting to determine the object's orientation and position. This method can be fast and effective, but it ignores the object's geometry and relies only on its bounding box. While this approach can be applied to unseen objects, it may not be able to stably place objects in all situations and may be less effective than methods that consider the object's geometry. In contrast, our approach is able to stably place unseen rigid objects on a horizontal surface using only a single partial observation. Furthermore, our method can handle a wide range of objects, rather than being limited to specific object types.

\noindent\textbf{Robotic Applications of Object Placements} Prior works on object placement for robotic applications have focused on solving specific tasks, such as constrained placement\cite{mitash2020task}, upright placement\cite{newbury2021learning, pang2022upright}, and rearrangement\cite{wada2022reorientbot, paxton2022predicting}. However, these methods have several limitations. For example, Mitash et al.'s approach\cite{mitash2020task} relies on multi-view shape-fitting and requires access to object models, which may not be available in some scenarios. The deep learning approach proposed in \cite{paxton2022predicting} is limited to determining the required rotation for stably placing objects in an upright orientation. Li et al.\cite{listable}'s method is only able to predict rotations that maintain objects in positions that maximize their height. These limitations restrict the applicability and potential of these methods for more general object placement tasks, such as stacking and packing. In contrast, our approach addresses the fundamental problem of placing unseen objects on a horizontal surface and has the potential to be applied to a wider range of robotic applications.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 2.
\begin{figure*}[ht!]
    \centering
        \includegraphics[width=\textwidth]{figures/Figure 2/Figure_2.png}
  \caption{\textbf{UOP-Sim dataset generation pipeline.} The UOP-Sim dataset is a large-scale synthetic dataset that contains 3D object models (point cloud) and annotations of stable planes for use in the UOP method. The dataset was generated by dropping each object on a table in 512 different configurations and sampling stable planes that satisfied Eq.\ref{eq:stability}. The stable plane candidates were then verified using a tilted table. In total, the UOP-Sim dataset includes 17.5K 3D object models and 69K annotations of stable planes.}
    \label{fig:UOP-sim}
\end{figure*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Problem Statement %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Statements}
% The problem of Unseen Object Placement involves placing unseen rigid objects from a single view and partial point cloud.

%%%%%Assumptions
\subsection{Assumptions}
A robot arm $\mathcal{R}$ equipped with a parallel-jaw gripper and is operating in a workspace with a planar table. The manipulating scene begins with the robot grasping an object $\mathcal{O}$. The scene is captured by a single-view RGB-D camera, and the partial point cloud of the object is obtained from the depth image. The point cloud $\mathcal{X}$ is fed into the model and the most stable plane is predicted.

%%%%%definitions
\subsection{Definitions}

\noindent\textbf{Point Cloud}: Let $\mathcal{X}$ be a point cloud obtained by capturing the manipulating scene in which the robot $\mathcal{R}$ grasps the object $\mathcal{O}$ from the camera $\psi$. 

\noindent\textbf{Object Unstability and Stable Planes}: Let $\mathcal{U}_\mathcal{O}$ denote the unstability of an object model $\mathcal {O}$. We define unstability $\mathcal{U}$ as the total amount of homogeneous transformation change in world coordinates $\mathcal{W}$ in a simulator over a discrete time step $L$. Based on this definition, stable planes $\mathcal{S}$ are annotated for each object model that satisfies the condition $\mathcal{U}_\mathcal{O} < \epsilon$.

\noindent\textbf{Dataset and Deep Learning Model}: Notably, the dataset $\mathcal{D} = \{({\mathcal{X}}_n, \mathcal{S})_n\}_{1}^{N}$ represents the set of $\mathcal{N}$ point clouds $\mathcal{X}$ and corresponding stable planes as the ground truth. $\mathcal{D}_{train}$ and $\mathcal{D}_{test}$ are the training and test datasets, respectively, and they satisfy the following condition: $\mathcal{D}_{train} \cup \mathcal{D}_{test} = \mathcal{D}$. Let the function $\mathcal{F}: \mathcal{X} \rightarrow \mathcal{S}$ denote a deep learning model that considers a partial point cloud $\matchal{X}$ as the input and produces the most stable plane $\mathcal{S}$ as the output.

\noindent\textbf {Seen and Unseen Objects}: If $\mathcal{O}_{\mathcal{D}_{train}} \cap \mathcal{O}_{\mathcal{D}_{test}} = \emptyset$, $\mathcal{O}_{\mathcal{D}_{train}}$ is a seen object, and $\mathcal{O}_{\mathcal{D}_{test}}$ is an unseen object for the deep learning model $\mathcal{F}$. 

%%%%%%%objectives
\subsection{Objectives} 
Our objective is to detect the most stable plane for the placement of unseen objects from a single-view observation. To achieve this, we aime to develop a neural network $\mathcal{F}: \mathcal{X} \rightarrow \methcal{S}$ that minimizes the unstability of the object $\mathcal{U}_\mathcal{O}$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Method %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Learning for Unseen Object Placement}
In this paper, we address the challenges of the Unseen Object Placement (UOP) task, which is known to be difficult to solve due to the need for a large-scale dataset to approximate stable planes, and the complexity of the relationship between point clouds and annotated planes. To mitigate these challenges, we present a novel approach by introducing the UOP-Sim dataset, comprising 17k 3D object models and 69k labeled stable planes, and the UOP-Net neural network, capable of detecting robust stable planes from partial point clouds. Through the use of these tools, we propose a general and adaptable approach to the UOP task, enabling robots to accurately place unseen objects in real-world scenarios.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{UOP-Sim Dataset Generation}
\noindent\textbf{Overview.}
To generate a synthetic dataset for training and validating our UOP-Net, we utilized a dynamic simulator and 3D CAD models from online resources such as 3D-Net \cite{wohlkinger20123dnet}, ShapeNet \cite{chang2015shapenet}, and YCB \cite{calli2015ycb}. The process for generating the dataset, as illustrated in Fig. \ref{fig:UOP-sim}, involved randomly dropping a rigid object on a horizontal surface to identify stable plane candidates, and then annotating these stable planes by placing them on a tilted table. This automated approach allowed us to efficiently generate a large-scale synthetic data for training and evaluating the model, and it requires no human intervention.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 3.
\begin{figure*}[ht!]
   \centering
      \includegraphics[width=\textwidth]{figures/Figure 3/Figure_3.png}

    \caption{\textbf{Overall pipeline of UOP-Net}. Our goal is to identify stable planes for object placement directly from partial observations, using the UOP-Sim dataset to generate partial point clouds. UOP-Net is trained to predict the most stable plane from these partial point clouds. When the target object is observed using an RGB-D camera, a partial point cloud is fed to UOP-Net, which uses the estimated stable plane to execute object placement based on the angular difference between the normal vector of the plane and the negative gravity vector.
    }


  \label{fig:UOP-net}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 512가지 포즈 (along eight grids of roll, pitch, and yaw).
% randomly release / drop
% 멈췄을 때 물체의 자세
% plane의 normal을 z축 하단 을 기준으로 5% 가 되는 영역의 normal vector를 저장 => plane 만들때
% plane을 horizontal surface에 놓고 정지상태가 되었을 때 기울임 => stability를 측정
% time step 0.005s 각 time step 마다 물체의 포즈 


\noindent\textbf{Stable Plane Annotation.}
In order to evaluate the unstability of an object's pose in our dynamic simulation, we defined the object's movement $\mathcal{M}$ in terms of its translation and rotation in the simulator's world coordinates. It can be represented as $\mathcal{H}_\mathcal{O} =[\mathbf{R}_\mathcal{O}|\mathbf{T}_\mathcal{O}] \in \mathbb{SE}(3)$. We tracked the object's pose at each time step and calculated the difference between consecutive poses (Equation \ref{eq:movement}). We then took the average of these differences over a certain period of time ${L}$, to estimate the overall unstability of the object in the simulation environment.


% To determine the stability of an object's pose in a dynamic simulation, we used a method that involves defining the object's movement $\mathcal{M}$ using a homogeneous transformation matrix $\mathcal{H}_\mathcal{O} =[\mathbf{R}_\mathcal{O}|\mathbf{T}_\mathcal{O}] \in \mathbb{SE}(3)$. This matrix encodes the object's translation $\mathbf{T}_\mathcal{O}^\mathcal{W} \in \mathbb{R}^3$ and rotation $\mathbf{R}_\mathcal{O}^\mathcal{W} \in \mathbb{SO}(3)$ in the world coordinates of the simulator. At each time step, we calculated the object's pose difference as shown in Equation \ref{eq:movement} to determine its movement $\mathcal{M}$. We then took the average of last ${L}$ movements to define the object's unstability $\mathcal{U}$ as shown in Equation \ref{eq:stability}. This allowed us to estimate the stability of the object in the simulated environment.


\begin{equation}
\label{eq:movement}
\mathcal{M} = ||\mathcal{H}_i^\mathcal{W} - \mathcal{H}_{i-1}^\mathcal{W}||_2
\end{equation}

\begin{equation}
\label{eq:stability}
\mathcal{U}_{i}=
\begin{cases}
{\frac{1}{L}}\sum_{j=i-L}^i\mathcal{M}_{j}, & \mbox{if } i >= L \\
{\frac{1}{i}}\sum_{j=1}^i\mathcal{M}_{j}, & \mbox{otherwise } 
\end{cases}
\end{equation}


%To explore a wide range of possible poses for the object, we generated 512 orientations by dividing the roll, pitch, and yaw into eight intervals. The object was then placed on a table with a random pose along the normal direction of the table, with a small value $\epsilon$ added to introduce uncertainty in the object's pose. To find stable planes that could support the object, we dropped the object on the table and recorded all poses where it remained stable (${\mathcal{U} < \delta_1}$).

To explore a wide range of possible poses for the object, we generated 512 orientations by dividing the roll, pitch, and yaw into eight intervals. The object was then placed on a table with a random pose along the normal direction of the table. To find stable planes that could support the object, we dropped the object on the table and recorded all poses where it remained stable (${\mathcal{U} < \delta_1}$).

% To comprehensively consider various possible object poses, we generated 512 orientations by dividing the roll, pitch, and yaw into eight intervals. The object was then randomly positioned on a table along the normal direction of the table plane, with a small value $\epsilon$ added to introduce uncertainty in the pose. In order to find stable planes that could support the object, we dropped it on the table and recorded all poses where it remained stable (${\mathcal{U} < \delta_1}$).

We then used the density-based spatial clustering of applications with noise algorithm\cite{ester1996density} to cluster the sampled poses. This allowed us to identify stable planes by clustering the poses along the z-axis, which represents the normal vector at the contact point of a stable plane with the horizontal surface. After rotating the object to align clustered normal vector with the gravity vector, we masked the bottom 5\% of the object's 3D model along the gravity vector to indicate the areas that could support the object's stability.

Because real-world environments cannot be perfectly simulated, certain planes may not be easily generalized. This can be a problem, especially for planes like a spherical model or the sides of a cylinder. To address this, we placed each object on a flat table with a normal vector of the plane candidates and tilted the table by 10°. We then estimated the object's movement across each time step and eliminated any planes that did not satisfy the condition of $\mathcal{U}$ less than $\delta_2$. This allowed us to label the stable planes that were robust for application to a horizontal surface, as shown in the samples of the UOP-Sim dataset in Fig.\ref{fig:UOP-sim}.

Our dataset contains both explicit and implicit planes, such as a flat surface formed by four chair legs. This results in a total of 17,408 3D object models and 69,027 stable plane annotations. To respond appropriately to partial observations in the real world, we captured 3D object models from the UOP-Sim dataset using a synthesized depth camera with 1,000 random poses. A partial point cloud was then sampled from the depth image using a voxel down sampling method.

For labeling purposes, annotations from the UOP-Sim dataset were sampled after aligning the poses of the partial point cloud and 3D object model. However, it is worth noting that the stable regions of a partial point cloud may be much smaller than those of the complete object model. In these cases, it is not reasonable to create a stable plane using only the partial point cloud. To address this issue, we estimated the angular error from the normal vectors of each stable region and eliminated any annotations from the partial point cloud for which the error exceeded 10°. This ensured that our labeling method was robust and applicable to partial observations in the real world.

\noindent\textbf{Simulation Environment Setting.}
To compute the object unstability ($\mathcal{U}$), we used PyRep and CoppeliaSim to build a simulation environment. We used 3D object models from 3 benchmark datasets: 3DNet, Shapenet, and YCB, yielding a total of 17,408 models. To facilitate the annotation process, we built 64 table models in the simulation environment.


% real world, no fine tune 꼭 추가할 것
\subsection{Deep Neural Network for Unseen Object Placement} 
\noindent\textbf{Overview.}
To detect the most stable plane for placing unseen objects on a flat surface, we propose a deep learning framework called UOP-Net. This framework is designed to predict stable planes from the visible regions of an observed object, allowing for the detection of planes that cover all the possible ways of maintaining stability based on various partial views. Segmenting the stable regions of unseen objects can be difficult, as the model should learn to generalize both object shapes and plane properties. To address this challenge, UOP-Net directly detects the most stable plane from partial observations, even when the objects are unseen. The input to the proposed approach is the observed point cloud of the target object. Given a partial point cloud $\mathcal{X} = \mathbb{R}^{N \times 3}$, the goal is to learn a function $\mathcal{F}: \mathcal{X} \rightarrow \mathcal{S}$, where $\mathcal{S}$ is the most stable plane in the set of predicted planes ${s_1,...,s_n}$. Using $\mathcal{S}$, a rotation $\mathcal{R} \in \mathbb{SO}(3)$ can be determined for stably placing the object on a horizontal surface.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Unseen Object Placement-Network (UOP-Net)} 
The UOP-Net model is designed to detect stable planes for object placement on a flat surface. It takes in a partial point cloud from a single depth image and processes it using a dynamic graph convolutional neural network (DGCNN)\cite{wang2019dynamic} to extract global shape features. These features are then split into two branches: one for semantic segmentation (predicting whether a point is stable or unstable) and one for embedding instance features.


% Our UOP-Net is a model designed for detecting stable planes for object placement on a flat surface. Given a partial point cloud sampled from a single depth image, the model first extracts the global shape features using a dynamic graph convolutional neural network (DGCNN) \cite{wang2019dynamic}. These features are then split into two branches: one for semantic segmentation, which predicts whether a point is stable or unstable, and one for embedding instance features.
% Because objects can have various shapes and multiple stable planes, a function that generalizes the features of the stable planes is required. Therefore, we propose UOP-Net based on category-agnostic instance segmentation, which includes both the concept of objectness and properties for stable planes obtained from the UOP-Sim dataset. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Table 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% rotation(degree), translation(meter) 단위 체크, Translation * E-05
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
% \usepackage{dcolumn}
\begin{table*}[ht!]
\caption{{UOP} Performance of UOP-Net and other baselines on the three benchmark datasets (unseen objects) in the simulation.}
\centering
\label{tab:simulation-eval}
\resizebox{\textwidth}{!}{%
{\renewcommand{\arraystretch}{1.2}
\LARGE{
\begin{tabular}{|cccccccccccccc|}
\hline
% \multicolumn{14}{|c|}{\textbf{Stable Object Placement Evaluataion Metric}} \\ \hline
\multicolumn{2}{|c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}\\ Object Type \& \\ Dataset\end{tabular}}} & \multicolumn{8}{c|}{ Object Stability (S)} & \multicolumn{4}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Success Rate of \\ Object Placement (\mathcal{SR}, $\%$)\end{tabular}}} \\ \cline{3-10}
\multicolumn{2}{|c|}{} & \multicolumn{4}{c|}{Rotation (R, $^{\circ}$) \downarrow} & \multicolumn{4}{c|}{Translation (T, $cm$) \downarrow} & \multicolumn{4}{c|}{} \\ \cline{3-14} 
\multicolumn{2}{|c|}{} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}UOP \\ \textbf{(Ours)}\end{tabular}} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}UOP \\ \textbf{(Ours)}\end{tabular}} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \begin{tabular}[c]{@{}c@{}}UOP \\ \textbf{(Ours)}\end{tabular} \\ \hline
% Partial Point Cloud
\multicolumn{1}{|c|}{\multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Partial \\ Point cloud\end{tabular}}} & \multicolumn{1}{c|}{3DNet\cite{wohlkinger20123dnet}} % 3DNet
& 23.19 & 28.79 & 35.64 & \multicolumn{1}{c|}{\textbf{16.40}} % Rotation
& 2.48 & 3.17 & 3.74 & \multicolumn{1}{c|}{\textbf{1.91}}  % Translation
& 54.87 & 41.17 & 50.45 & \textbf{55.47}  \\ % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{ShapeNet\cite{chang2015shapenet}} % ShapeNet
& 21.97 & 28.70 & 25.80 & \multicolumn{1}{c|}{\textbf{14.90}} % Rotation
& 2.91 & 3.56 & 2.90 & \multicolumn{1}{c|}{\textbf{1.59}} % Translation
& 55.78 & 38.92 & 58.23 & \textbf{60.79}  \\ % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{YCB\cite{calli2015ycb}} % YCB
& 35.49 & 38.46 & 39.13 & \multicolumn{1}{c|}{\textbf{15.92}} % Rotation
& 5.73 & 5.98 & 5.19 & \multicolumn{1}{c|}{\textbf{2.37}} % Translation
& 48.14 & 40.22 & 52.66 & \textbf{62.53} \\ 
\hhline{|~|=|====|====|====|} % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Total avg.} % Total avg.
& 25.78 & 31.01 & 34.49 & \multicolumn{1}{c|}{\textbf{15.97}} % Rotation
& 3.32 & 3.90 & 3.90 & \multicolumn{1}{c|}{\textbf{1.95}} % Translation
& 53.50 & 40.48 & 52.59 & \textbf{58.22} \\ \hline % Success Rate

% Complete Point Cloud
\multicolumn{1}{|c|}{\multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Complete \\ Point cloud\end{tabular}}} & \multicolumn{1}{c|}{3DNet\cite{wohlkinger20123dnet}} % 3DNet
& 5.03 & 20.84 & 16.22 & \multicolumn{1}{c|}{\textbf{5.01}} % Rotation
& \textbf{0.40} & 2.09 & 1.48 & \multicolumn{1}{c|}{0.50}  % Translation
& \textbf{87.03} & 58.23  & 73.44  & 83.14  \\ % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{ShapeNet\cite{chang2015shapenet}} % ShapeNet
& \textbf{2.02} & 19.44 & 10.57 & \multicolumn{1}{c|}{2.60} % Rotation
& \textbf{0.16} & 2.41 & 1.15 & \multicolumn{1}{c|}{0.30} % Translation
& \textbf{95.17} & 54.26  & 81.51  & 88.75  \\ % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{YCB\cite{calli2015ycb}} % YCB
& 4.42 & 37.59 & 8.58 & \multicolumn{1}{c|}{\textbf{3.08}} % Rotation
& \textbf{0.35} & 5.96 & 0.87 & \multicolumn{1}{c|}{0.30} % Translation
& \textbf{89.27} & 49.89  & 83.84  & 87.84  \\ 
\hhline{|~|=|====|====|====|} % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Total avg.} % Total
& 4.26 & 24.43 & 13.27 & \multicolumn{1}{c|}{\textbf{4.06}} 
& \textbf{0.34} & 3.05 & 1.27 & \multicolumn{1}{c|}{0.41} 
& \textbf{89.25} & 55.47  & 77.54  & 85.41  \\ \hline % Success Rate


\end{tabular}
}
}
}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To identify stable points, the mean-shift clustering algorithm \cite{comaniciu2002mean} is applied to the predicted stable points, and RANSAC \cite{fischler1981random} is used to fit planes onto the clustered points\cite{Zhou2018}. The stability scores for each plane are calculated by element-wise multiplication of the semantic logits, predicted instance labels, and number of points composing each plane. The plane with the highest score is then outputted after fitting the planes and assigning stability scores based on the number of inliers that constitute the planes. The rotation value $\mathcal{R}$ is then determined by estimating the angular difference between the predicted normal vector of the stable plane and the gravity vector (negative table surface normal).

\begin{equation}
\label{eq:the_sum_of_losses}
\mathcal{L} = \lambda_1 * \mathcal{L}_{stable} + \lambda_2 * \mathcal{L}_{plane},
\end{equation}

To improve the robustness and generalization of UOP-Net, it is trained using a category-agnostic instance segmentation approach that allows it to learn the features of stable planes without being restricted to specific object categories. The model is trained using a loss function that is a combination of the binary cross-entropy loss (stable loss) for semantic segmentation and the discriminative loss (plane loss) for instance embedding. The hyperparameters $\lambda_1$ and $\lambda_2$ are set to 10 and 1, respectively, to ensure comparable values for the two loss terms.


\noindent\textbf{Training Details.} To train UOP-Net, we utilized a dataset consisting of partial point clouds of objects captured from various views, along with stable plane annotations that were selected based on technical criteria to ensure their suitability. The dataset used for training was specifically curated from 3D object models sourced from 3DNet \cite{wohlkinger20123dnet} and ShapeNet \cite{chang2015shapenet}. The training batches were comprised of 2,048 points randomly sampled from each object and underwent various types of data augmentation techniques, such as rotation, sheering, point-wise jittering, and adding Gaussian noise, to enhance the model's performance in real-world scenarios. The model was implemented using PyTorch\cite{paszke2019pytorch} and trained on an NVIDIA Titan RTX GPU, with a batch size of 32 and a total of 1,000 epochs. To prevent overfitting, we employed early stopping with a patience of 50 and used the Adam optimizer at a learning rate of 1e-3.

\section{Simulation Experiments}
\noindent\textbf{Datasets.} 
We compared our methods and previous object placement methods on the three benchmark object models: 3DNet \cite{wohlkinger20123dnet}, ShapeNet \cite{chang2015shapenet} and YCB \cite{calli2015ycb}. 3D


To evaluate our method, we conducted experiments using synthetic data generated from two benchmark datasets: 3DNet \cite{wohlkinger20123dnet} and ShapeNet \cite{chang2015shapenet}. We generated 69,000 stable plane annotations for a total of 17,400 3D objects using these datasets, and split the synthetic data into training and validation sets with a 8:2 ratio. We also labeled the YCB \cite{calli2015ycb} object models in the simulation, but excluded them from the training set to use as a test set in both the simulation and real-world experiments. To ensure the quality of our dataset, we excluded objects that had no stable planes, such as spherical objects, and ended up with a total of 152, 57, and 63 object categories in the 3DNet, ShapeNet, and YCB datasets, respectively. The training set contained 13,926 objects and 55,261 annotations, while the validation set contained 3,482 objects and 13,766 annotations.

\noindent\textbf{Baselines.}
We compared the performance of our method with that of the following baselines:
\begin{itemize}
    \item \textbf{Convex Hull Stability Analysis (CHSA)}\cite{haustein2019object, trimesh, hagelskjaer2019using}: The baseline method for determining stable object poses involves calculating the rotation matrix needed to allow an object to rest stably on a flat surface. To do this, it starts by sampling the location of the object's center of mass and finding the stable resting poses of the object on a flat surface using the object's convex hull. It then evaluates the probabilities of the object landing in each pose and outputs the pose with the highest probability.

    \item \textbf{Bounding Box Fitting (BBF)}\cite{mitash2020task, Zhou2018}: The method involves fitting an oriented bounding box to the object's convex hull using principal component analysis, in order to minimize the difference between the volume of the convex hull and that of the bounding box. The object is then placed on a planar workspace with the largest area facing down.
    
    \item \textbf{RANSAC Plane Fitting (RPF)}\cite{fischler1981random, Zhou2018}: The approach segments planes in a point cloud by fitting a model of the form $ax + by + cz + d = 0$ to each point $(x, y, z)$. It then samples several points randomly and uses them to construct a random plane, repeating this process iteratively to determine the plane that appears most frequently.
    
\end{itemize}

\noindent\textbf{Evaluation Metrics.}
To evaluate the performance of UOP-Net, we used two metrics: \textit{object stability (\mathcal{OS})} and \textit{success rate of object placement (\mathcal{SR})}. To measure \textit{object stability (\mathcal{OS})}, we placed an unseen object on a flat table and used the output of the model to estimate its stability. This allowed us to verify whether the trained model was able to maintain the object in a stable pose. The \textit{success rate of object placement (\mathcal{SR})} was calculated as the ratio of successful stable placements to all placements, with an accumulated rotation error of less than 10°. As rotational motion is more common than translation when an object is placed in an unstable state and falls due to vibrations, we only considered rotational motion when evaluating object stability. To evaluate the model's performance, we attempted to place the object 100 times and terminated the evaluation if no planes were detected, treating it as a failed case.

% We used two metrics to qualitatively evaluate the efficiency of the UOP method, namely the \textit{object stability (\mathcal{OS})} and \textit{success rate of object placement (\mathcal{SR})}. For an unseen object, we estimated its stability (\mathcal{OS}) when the object was placed on a flat table using the output of the model. Our aim was to verify whether our trained model allowed the object to maintain a stable pose. The other evaluation metric was the ratio of successful stable placements to all placements with an accumulated rotation error of less than 10°. Typically, when an object is placed in an unstable state, it falls after being exposed to any kind of vibration. In this case, rotational motion is more common than translation. Therefore, we only considered the rotational motion when evaluating object stability. Object placement was attempted 100 times for each object, and if no planes were detected, we terminated the evaluation and regarded it as a failed case.


\begin{itemize}
    \item \textbf{Object Stability (\mathcal{OS})}: The metric quantifies the movement of the object during a discrete time step when it is placed on a horizontal surface using the predicted stable plane.
    
    \item \textbf{Success Rate of Object Placement (\mathcal{SR})}: The metric represents the percentage of placements in which the object remains stationary for a minute and the accumulated rotation is less than $10^{\circ}$.
\end{itemize}


\noindent\textbf{Comparison with the Baselines.}
In the simulation experiments, we compared the performance of UOP-Net with the baseline methods on three 3D benchmark datasets (3DNet\cite{wohlkinger20123dnet}, ShapeNet\cite{chang2015shapenet}, and YCB\cite{calli2015ycb}) for two object types (complete and partial shapes). The results in terms of the object stability (OS) and success rate of placement (SR) are reported in Table \ref{tab:simulation-eval}. Our UOP-Net achieved the best performance in these evaluation metrics when using a single partial observation. We visualized the prediction results of each method and represented the object stability as a graph in Fig. \ref{fig:sim_exp_result}. The blue line in the graph (Fig. \ref{fig:sim_exp_result}) is the case of placing the object in the plane predicted by UOP-Net, which shows that the object can be placed more reliably than predicted by other methodologies.

The CHSA, which generates an object mesh (convex hull) from the point cloud and obtains mathematically stable planes based on the center of mass, performs the best when the object shape is fully visible. Although our model does not outperform the CHSA in this case, it still performs well for unseen objects and the predicted planes allow the object to remain stable. However, the CHSA does not perform well when only partial observations are available, as the convex hull generated from the partial point cloud does not contain information about the invisible portion of the object, making it difficult to accurately sample the center of mass. As shown in Fig.\ref{fig:sim_exp_result}, the CHSA often places the object on a truncated plane, causing it to become unsteady.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 4 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 4.
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure 4/Figure_4.png}
        \end{subfigure}
        
    \caption{Visualization of the prediction results obtained on the YCB\cite{calli2015ycb} dataset for each method and object stability in the simulation.}
    \label{fig:sim_exp_result}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The BBF method had the worst performance because it did not consider the geometric properties of the object and simply placed the largest plane in the bounding box. This may work for objects with simple shapes, but it is not suitable for complex shapes. Although RPF performs better than BBF, it still fails to detect a robust stable plane. In contrast, our method (UOP-Net) can detect a robust plane based on incomplete perception, as we generate partial point clouds using UOP-Sim and train the model to learn the geometric properties of objects and planes. 

UOP-Net is able to detect not only explicit planes that are explicitly visible in the point cloud, but also implicit planes (e.g., a plane formed by the four legs of a chair) that are not necessarily visible but still contribute to the stability of the object. This is because our model is trained on a diverse set of objects and planes and is able to generalize to new unseen objects and planes.

\noindent\textbf{Failure Cases.}
UOP-Net has some limitations that may affect its performance. For small objects, such as spoons or knives, UOP-Net may have difficulty detecting stable planes because the size of the ground truth (stable plane) is relatively small compared to the size of the object. Additionally, finding the optimal hyperparameters for the mean-shift clustering algorithm may be challenging, as the performance of UOP-Net may suffer if the hyperparameters are not well-tuned for a particular object. These limitations may result in UOP-Net failing to accurately detect stable planes for some objects. However, these limitations are not unique to UOP-Net and are common challenges faced by many object placement methods.

\section{Experiments in the Real World}
\noindent\textbf{Real Environment Setting.}
To evaluate the performance of our object placement method in real-world scenarios, we conducted experiments using a universal robot (UR5) manipulator and an Azure Kinect RGB-D camera (Fig.\ref{fig:real_experiment} (b)). We used the MANet \cite{fan2020ma} object segmentation method with a DenseNet121 \cite{iandola2014densenet} backbone to segment the target object and the gripper. The process began by segmenting the visible region of the target object from the RGB image, after which the depth image was cropped using a mask. The point cloud was then sampled from the depth image using voxel-down sampling \cite{Zhou2018} and fed to UOP-Net. The model predicted the most stable plane and calculated the rotation value between the plane and the table. The UR5 robot then placed the target object on the table. To ensure smooth planar motion, we utilized the BiRRT \cite{qureshi2015intelligent} algorithm implemented with PyBullet \cite{coumans2021} and integrated with collision checking on a physics engine.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 5 %%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure 5/Figure_5.png}
        \end{subfigure}
        
    \caption{\textbf{Unseen object placement using UOP-Net}. (a) Given a segmented target object, UOP-Net considers the 3D point clouds observed by a depth camera (Azure Kinect) as inputs and predicts the most stable plane. (b) UR5 places the target object on the table.}
    \label{fig:real_experiment}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Table 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Table 2
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[ht!]
\caption{\textbf{UOP} performance of UOP-Net and baselines on YCB\cite{calli2015ycb} in the real world.}
\label{tab:real world-eval}
\resizebox{\columnwidth}{!}{%
\LARGE{
{\renewcommand{\arraystretch}{}
\begin{tabular}{|c|cccc|}
\hline
YCB\cite{calli2015ycb} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \begin{tabular}[c]{@{}c@{}}UOP-Net \\ \textbf{(Ours)}\end{tabular} \\ \hline
Coffee Can & 0 / 10 & 0 / 10 & 4 / 10 & \textbf{10} / 10 \\
Timer & 0 / 10 & 1 / 10 & \textbf{6} / 10 & \textbf{6} / 10 \\
Power Drill & 0 / 10 & 1 / 10 & \textbf{5} / 10 & \textbf{5} / 10 \\
Wood Block & 1 / 10 & 1 / 10 & \textbf{10} / 10 & \textbf{10} / 10 \\
Metal Mug & 0 / 10 & 0 / 10 & 6 / 10 & \textbf{9} / 10 \\
Metal Bowl & \textbf{10} / 10 & 5 / 10 & \textbf{10} / 10 & \textbf{10} / 10 \\
Bleach Cleanser & 3 / 10 & 3 / 10 & \textbf{9} / 10 & \textbf{9} / 10 \\
Mustard Container & 2 / 10 & 0 / 10 & 5 / 10 & \textbf{10} / 10 \\
Airplane Toy & 0 / 10 & 3 / 10 & 0 / 10 & \textbf{4} / 10 \\
Sugar Box & 2 / 10 & 3 / 10 & \textbf{10} / 10 & \textbf{10} / 10 \\
Chips Can & 2 / 10 & 0 / 10 & 8 / 10 & \textbf{10} / 10 \\
Banana & 5 / 10 & 5 / 10 & \textbf{9} / 10 & \textbf{9} / 10 \\ \hhline{|=|=|=|=|=|}
Average & 2.1 / 10 & 1.8 / 10 & 6.8 / 10 & \textbf{8.5} / 10 \\ \hline
\end{tabular}%
}
}
}
\end{table}

\noindent\textbf{Evaluation Metrics.}
To evaluate the effectiveness of our approach in real-world scenarios, we used the success rate of object placement (\mathcal{SR}) as the evaluation metric. During the experiments, we instructed the robot to place the object on the table using the plane predicted by UOP-Net. The placement was deemed successful if the robot was able to place the object in a fixed position on the predicted plane without it falling or sliding, as judged by visual inspection. If the model failed to detect any stable planes, the trial was considered a failure. We conducted a total of 10 placement trials for each object.


\noindent\textbf{Comparison with the Baselines.}
We selected 12 objects from the YCB dataset. Objects with spherical shapes (e.g., apples), dimensions that were too small, or small depth values were excluded from the test set. Table \ref{tab:real world-eval} indicates that our method outperforms the other baselines in terms of the success rate across all objects. Even though real-world perception is noisy, UOP-Net provides a stable plane. This can be attributed to the fact that our model learned from numerous partial point clouds that were captured by a depth camera and corrupted by noise. Other benchmarks (CHSA and primitive shape fitting) performed extremely poorly because they could not obtain the complete shape of an object in the real world and were unable to respond to sensor noise. To further verify that our model can perform as efficiently on unseen objects, we evaluated our method on novel objects that did not have an available CAD model (a dinosaur figurine and an ice tray, as shown in Fig.\ref{fig:comparison_pred_res}). Although the object shapes were complex, UOP-Net detected implicit planes (e.g., the four legs of the dinosaur). 


\noindent\textbf{Failure Cases.}
When an RGB-D senor fail to capture a specific area of an object (e.g., a dark or glossy part of the object) or when the object and gripper segmentation model fails to predict clear mask, the points corresponding to ground truth are not sampled, UOP-Net may fail to detect the stable plane. We also demonstrated that UOP-Net can detect an implicit plane from complex shape objects (e.g., YCB airplane toy or dinosaur), but detecting an implicit plane in unclear point cloud is still challenging.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 7 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 7. -> 6
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure 6/Figure_6.png}
        \end{subfigure}
        
    \caption{Visualization of the prediction results for each method on YCB\cite{calli2015ycb} objects and novel objects in the real world.}
    \label{fig:comparison_pred_res}
\end{figure}





% object (17,408 => train: 13926, val:3482 ) // anno (69,027 => train :55261, val:13766)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Conclusion %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion and Future Scope} 
This paper proposes a method called UOP-Net for detecting stable planes of unseen objects. We first introduced the UOP-Sim pipeline for generating synthetic datasets, which consisted of 17.4k 3D objects and 69k stable plane annotations. Our model learned to detect stable planes directly from various objects under partial observations using only a synthetic dataset called UOP-Sim. We demonstrated the accuracy and reliability of UOP-Net in detecting stable planes from unseen and partially observable objects on three benchmark datasets with SOTA performance. 

% The proposed method could be extended and improved in several ways. First, our model requires post-processing to cluster and fit the plane when segmenting each instance plane. Because the hyperparameters should be set for experimental environments, the post-processing algorithms are heuristic. Thus, we hope to improve the model architecture to directly generate planes as an end-to-end pipeline. Second, our system only considers object placement on a horizontal surface, but we hope to extend our model and learn to predict a plane for object placement in other environments so that the robot can pack and stack.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Acknowledgement %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgment}
\begin{spacing}{0.4}
{\scriptsize This research was completely supported by a Korea Institute for Advancement of Technology (KIAT) grant funded by the Korea Government (MOTIE) (Project Name: Shared autonomy based on deep reinforcement learning for responding intelligently to unfixed environments such as robotic assembly tasks, Project Number: 20008613). This research was also partially supported by an HPC Support project of the Korea Ministry of Science and ICT and NIPA.}
\end{spacing}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{references.bib}
\bibliographystyle{IEEEtran}

\end{document}
