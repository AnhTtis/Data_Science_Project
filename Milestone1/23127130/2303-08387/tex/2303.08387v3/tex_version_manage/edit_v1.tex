%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{url}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{etoolbox}
\usepackage{placeins}
\usepackage{hyperref}
\usepackage{setspace} 
\usepackage{romannum}
\usepackage{wasysym}
\let\Square\relax
\usepackage{bbding}
\usepackage{pifont}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{array}
\usepackage{cite}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{algpseudocode}

\title{\LARGE \
Learning to Place Unseen Objects Stably based on a Large-scale Simulation
}

\author{Sangjun Noh$^{*}$, Raeyoung Kang$^{*}$, Taewon Kim$^{*}$, Seunghyeok Back, Seongho Bak, Kyoobin Lee†% <-this % stops a space
\thanks{\text{*} These authors contributed equally to the paper.}
\thanks{All authors are with the School of Integrated Technology, Gwangju Institute of Science and Technology, Cheomdan-gwagiro 123, Buk-gu, Gwangju 61005, Republic of Korea. 
† Corresponding author: Kyoobin Lee {\tt\small kyoobinlee@gist.ac.kr}}%
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Abstract %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

In this paper, we propose \textbf{\textit{SOP}} that 

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Introduction %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

The ability to place objects stably can be significantly useful for robots performing a variety of tasks, such as moving mechanical equipment on industrial sites, which should be handled carefully, or organizing fragile objects (e.g., glass, plate, bowl) in household environments. Previous works have predominantly focused on manipulation tasks, such as grasping\cite{mahler2017dex, morrison2018closing, mousavian20196, sundermeyer2021contact}, picking and placing\cite{harada2012object, gualtieri2018learning}, upright placement of\cite{newbury2021learning}, or rearranging\cite{wada2022reorientbot} objects. A few studies have considered stability in object placement, which is required for maintaining the full shape of 3D object models\cite{haustein2019object, gualtieri2021robotic, tournassoud1987regrasping}; alternatively, few studies were only focused on performing placement on task-specific objects (e.g., plates, cups) and environments (e.g., rack, pen holder, stemware holder) \cite{jiang2012learninga, jiang2012learningb}. However, relying on the availability of complete 3D models serves as a limitation in real-world scenarios where a robot physically interacts with objects based on partially observed visual data (e.g., depth camera). To overcome this limitation, the utilization of multiview-based methods for representing 3D object models\cite{newbury2021learning} or a shape completion module\cite{gualtieri2021robotic} for generating a full-object model have been proposed; however, both methods may not be sufficiently accurate for generating 3D object models. Here, we consider the problem of stable object placement (\textbf\textit{SOP}) based on the 3D point cloud of the depth camera. Our goal is to directly predict a stable plane from partial observation, which will enable the object to physically maintain a stable pose. 

% ---- 
% Fig 1.
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure 1/Figure_1.png}
        \end{subfigure}
        
    \caption{\textbf{Comparison between stable object placement (SOP) network (SOP-NET) and prior works.} Placement algorithms based on 3D models can detect a plane using the analytical method, primitive shape fitting, or completion methodology, whereas SOP-NET can detect a stable plane directly from partial observations without any other modules.}
    \label{fig:task_comparison}
\end{figure}

In this work, we investigated SOP, the first deep learning-based approach for detecting a plane to place object stably, when either partial observation from a depth image or unseen objects are given. To achieve this, we defined the stability for object placement by considering both geometrical and physical properties of objects. Thus, we generated a large-scale synthetic dataset using a physical simulator, called SOP-SIM, by sampling and inspecting candidate areas for various objects. While \cite{jiang2012learninga, jiang2012learningb} labeled the preferred placing configuration using a heuristic technique, we automatically annotated all possible regions where the object can sustain stable pose based on the geometric shape and physical characteristics. Furthermore, we propose SOP network (SOP-NET), an end-to-end deep learning model that directly predicts the most stable plane to place an object based on partial observations. SOP-NET is trained in a category-agnostic fashion on the SOP-SIM dataset, which contains 12.5K 3D objects that are annotated with a total of 20K stable areas. % 실험 결과 나오면, 실험 설명 추가 / 그림 1 어디 문장에 넣을 것인지 확인할 것 

% In other words, this task requires the robot to reason about the geometry and physical properties of objects based on partially observed visual data, infer a stable plane(3D orientation), and move the robot to the desired stability configuration for execution. 


% abstract 쓰면서 최종적으로 수정 필요 !!
To summarize, the contributions of this paper are as follows:
\begin{itemize}
    \item{We propose a new deep learning-based framework for (\textbf\textit{SOP}) by detecting a plane that allows an object to sustain a stable pose even when only a partial observation of the object or an unseen object is given.}
    
    \item{We provide a large-scale 3D synthetic dataset, called SOP-SIM, containing a total of 20,790 stable regions for 12,480 different objects from ShapeNet\cite{chang2015shapenet} and 3DNet\cite{wohlkinger20123dnet}. The stable regions for SOP are automatically annotated in a physical simulator.}
    
    \item{We validated SOP-NET by comparing it to an analytical method for predicting stable planes, and demonstrated that the plane detected by our model is robust in real-world conditions. Our model is only trained with synthetic data and performs well in the absence of any additional real-world data.} % 수정 필요 !!

\end{itemize} 

% The organization of the paper is as follows. We describe the related works in Section \uppercase\expandafter{\romannumeral2} and describe the problem statements in Section \uppercase\expandafter{\romannumeral3}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Related Works %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Works}
\noindent\textbf{Robotic Object Placement} Since the early days of robotic research, object placement approaches have been widely studied in various tasks in the entire pipeline of robotic picking and placement\cite{tremblay2018deep, morgan2019benchmarking, gualtieri2018learning, gualtieri2020learning}, rearrangement\cite{qureshi2021nerp, danielczuk2021object, yuan2018rearrangement}, or reorientation\cite{wada2022reorientbot, newbury2021learning} of objects. Although these previous works have shown a strong capability in object placement, only objects in a given pose or those fitted with primitive shapes (e.g., cuboids, cylinders) were placed. Moreover, after the object was released by a robot, it was placed at the target location without considering the stability of the pose.

Analyzing the convex hull of the object with the center of mass and sampling the explicit faces where the object rests in a stable configuration can enable stable placement on a horizontal surface\cite{tournassoud1987regrasping, wan2019regrasp,lertkultanon2018certified, haustein2019object}. When the geometrical and physical properties of an object are known, the robot can perform well in terms of stable placement performance. However, this process is difficult because it requires precise information about the object, whereas in the real world, only partial observation with noise is accessible (e.g., RGB-depth (RGB-D) camera). To overcome this limitation, several groups have demonstrated integrated vision-based placement using stable object states that are sampled with known or abstracted geometry of the object or predicted by a learned model. In \cite{gualtieri2021robotic}, although researchers utilized the deep learning-based completion module and an analytical method to address the uncertainty for partially visible objects in the real world, the completion module might not be sufficiently accurate to generate the full shape. Jiang et al.\cite{jiang2012learninga} trained a classifier using a hand-crafted dataset to identify placements that are likely to be stable and satisfy human preference. They validated the physical feasibility of placement and human preferences, but full observability was required, and dataset labels were heuristic. Recently, Cheng et al.\cite{cheng2021learning} demonstrated good performance by training a deep learning model with a synthetic dataset, but task-specific objects were still required.

In this paper, we investigate a deep learning-based method for SOP in the real world, even when partially observable, unseen objects are given. We learned by annotating all possible stable planes based on geometrical shapes and physical characteristics rather than human preference in the simulation. Moreover, to respond to partial observations, we propose a model that can predict a plane in the visible region of an object using a partial point cloud as an input. Thus, our method does not require a complete shape to predict a plane. \\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 2.
\begin{figure*}[ht!]
    \centering
        \includegraphics[width=\textwidth]{figures/Figure 2/Figure_2_final.png}
    \caption{\textbf{Overall pipeline for SOP}. (Center) The SOP-NET is trained using SOP-SIM to predict candidate stable planes from depth images with 12K 3D synthesized point clouds, 25K annotations for SOP. (Left) When the robot observes the target objects in the RGB-depth images, a sampled partial point cloud is input to SOP-NET and instances of stable planes are returned. (Right) SOP-NET determines the most robust stable planes, and the target object that is grasped by the Universal Robots UR5 robot is rotated and placed based on the angle difference between the table-based z-axis and the normal vector of a stable plane.}
    \label{fig:overall_pipeline}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent\textbf{Unseen Object Perception for Robotic Manipulation}
To work in an industrial domain, a robot should be able to recognize and manipulate new objects that have never been seen before. Detecting unseen objects is a challenging task because modeling all objects in the real world is impractical and infeasible. Many object perception approaches\cite{xie2021unseen, back2021unseen, xie2020best, xiang2020learning} have been proposed to segment the visible parts of objects, although unseen objects and clutter scenes are given. In robotic manipulation, Grasp Quality Convolutional Neural Network (GQ-CNN)\cite{mahler2017dex} and six degrees-of-freedom (6DoF) GraspNet\cite{mousavian20196} are representative approaches for detecting grasp points from depth images and point clouds of unseen objects. These prior works used synthetic data to train category-agnostic object models to learn generalization of objects. In this work, we also trained our proposed method for SOP in a category-agnostic fashion with the objective of detecting regions for stably placing an unseen object from a 3D point cloud.\\


\noindent\textbf{Deep Neural Networks for Learning from 3D Data} 
% 추가 가능 : partial analysis or completion works 
With the considerable success of deep learning on image recognition, object detection, and semantic segmentation using 2D images, researchers have presented learning-based approaches using a variety of 3D representations, including multiview images\cite{boulch2018snapnet, su2015multi, boulch2017unstructured}, 2.5D depth images\cite{gupta2014learning}, 3D voxels\cite{maturana2015voxnet, riegler2017octnet, wang2017cnn}, and 3D point clouds\cite{qi2017pointnet, qi2017pointnet++, liu2019relation}. Qi et al. proposed new architectures, called PointNet\cite{qi2017pointnet} and PointNet++\cite{qi2017pointnet++}, that can process raw point clouds directly and extract the geometric features efficiently, achieving good performance on classification and semantic segmentation tasks. The success of PointNet and PointNet++ prompted the development of various network architectures that represent 3D data, leading to significant improvements not only in 3D-object pose estimation\cite{tremblay2018deep, brachmann2014learning, oberweger2018making} and instance segmentation\cite{wang2018sgpn, pham2019jsis3d, yi2019gspn, lahoud20193d, yang2019learning}, but also in deep learning-based robotic manipulations\cite{mousavian20196, sundermeyer2021contact, fang2020graspnet}. To detect a stable plane to place an object, global as well as local geometrical information is required. Accurate geometric information of objects cannot be obtained by performing operations on a single RGB image as it is based on 2D coordinates. Thus, we used 3D point clouds and PointNet\cite{qi2017pointnet}/PointNet++\cite{qi2017pointnet++} as the backbone to produce a stable plane in the special Euclidean group SE(3). \\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Problem Statement %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Statements}
We considered the problem of (\textbf\textit{SOP}) from a single-view depth image, where a robot picks up an unseen rigid object with the grasp points generated by the grasp generator (e.g., GQ-CNN\cite{morrison2018closing}, 6DoF GraspNet\cite{mousavian20196}). 
%%%%assumptions
% \subsection{Assumptions}
% We assume a rigid object in gripper(e.g., parallel-jaw gripper, five-finger gripper) and a horizontal surface to stably place objects. A depth image from the fixed view depth camera is organized to point cloud and fed into the model. The model outputs a plane for stable object placement. We train our model with labeled stable planes within object point clouds, which are generated in a physics engine and used to stably object placement. For generating datasets, we assume the pose, shape and category of the objects are unknown.

%%%%%definitions
% the center of object -> World coordinate
\subsection{Definitions}
\textbf{\textit{States}}: Let $\mathcal{Z} = \{\mathcal{R}, \mathcal{O}, \mathcal{T}_\mathcal{O}, \mathcal{T}_\mathcal{C}\}$ denote a state describing the properties of a robot, objects, and a camera in the manipulating scene, where the robot $\mathcal{R}$ grasps the target object $\mathcal{O}$, and $\mathcal{T}_\mathcal{O}$, $\mathcal{T}_\mathcal{C}$ are the 3D poses of the object in the robot gripper and camera respectively.

% H -> 호모니지어스 transformation(translation, rotation) 언급할 것
\textbf{\textit{Point Clouds}}: Let $\mathcal{X} = \mathbb{R}^{H \times W}_{+}$ be a 2.5D point cloud represented as a depth image with height $H$ and width $W$ captured by a camera with known intrinsic characteristics\cite{page2005multiple} and 3D pose of the camera $\mathcal{T}_\mathcal{C}$. The input of our model is $\mathcal{\hat{X}} = \mathbb{R}^{N \times 3}$, where $N$ points are sampled from $\mathcal{X}$. 

\textbf{\textit{Object Stability}}: Let $\mathcal{S}_\mathcal{T}^\mathcal{O}$ denote the placement stability of an object model $\mathcal {O}$ at the grasped pose $\mathcal{T}$. We define \textit{stability} $\mathcal{S} = {\frac{1}{L}}\sum_{i=1}^L{\parallel \mathcal{H}_i - \mathcal{H}_{i-1}\parallel}_2^\mathcal{W}$ as the amount of transformation change at the world coordinate $\mathcal{W}$ in a simulator during a discrete time step $L$.

\textbf{\textit{Stable Plane}}: In the simulation, the ground truth is generated as the stable plane $\mathcal{A}$ for each object model $\{{O}_i | i=1,..,M, O_i \in \mathcal{O} \}$ by placing the object at uniformly determined poses $\matchal{T}$ and $\mathcal{A}_\mathcal{T}^\mathcal{O}$ is regarded as a stable placement plane where \textbf{stability} $\mathcal{S}_\mathcal{T}^\mathcal{O} < \epsilon$.

\textbf{\textit{Dataset and Deep Learning Model}}: The dataset $\mathcal{D} = \{({O}_m, \mathcal{A}_m) \}_{1}^{M}$ represents the set of M object models $\mathcal{O}$ (e.g., point clouds) and the stable placement plane $\mathcal{A}$ as the ground truth. $\mathcal{D}_{train}$ and $\mathcal{D}_{test}$ are the training and test datasets, respectively, and they satisfy the following state: $\mathcal{D}_{train} \cup \mathcal{D}_{test} = \mathcal{D}$. Let function $\mathcal{F}: \mathcal{X}_p \rightarrow \mathcal{A}$ be a deep learning model that takes one object as the input and produces a stable plane as the output.

\textbf {\textit{Seen and Unseen Objects}}: If $\mathcal{O}_{\mathcal{D}_{train}} \cap \mathcal{O}_{\mathcal{D}_{train}} = \emptyset$, $\mathcal{O}_{\mathcal{D}_{train}}$ is the seen object and $\mathcal{O}_{\mathcal{D}_{train}}$ is unseen for the deep learning model $\mathcal{F}$. \\

%%%%%%%objectives
\subsection{Objectives}
Our goal was to detect a stable plane for the stable placement of arbitrary objects using only single-view observations. Thus, we aimed to develop a neural network $\mathcal{F}: \mathcal{\hat{X}} \rightarrow \methcal{A}$ that maximizes the \textit{stability} $\mathcal{S}_\mathcal{T}^\mathcal{O}$ when partial point clouds $\mathcal{\hat{X}}$ are given.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Method %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Learning Stable Object Placement Function}
Solving an SOP function that predicts a robust plane is challenging owing to several reasons. First, large-scale samples may be required to approximate the expectation over multiple possible objects. Thus, we generated a synthetic dataset, called SOP-SIM, containing 12k 3D models (point clouds), 25K annotations of stable planes, and robust SOP metrics. Another reason is that learning using a simple linear or mathematical model can be difficult as the relationship between the point cloud of objects and the annotated metrics is complex, considering both the geometric and physical properties. Consequently, we developed SOP-NET, which learns to detect robust stable planes using SOP-SIM.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 3.
\begin{figure*}[ht!]
   \centering
      \includegraphics[width=\textwidth]{figures/Figure 3/Figure_3_final.png}
  \caption{\textbf{SOP-SIM dataset generation pipeline for training SOP-NET.} (Left) The database contains a total of 12K 3D object mesh models, and we built 64 tables in the simulation to place and annotate each object. (Top) We attempted to place each object on a table with 512 poses and sample stable poses of the objects were determined using (A). (Bottom) We labeled the stable plane on the point cloud based on the sampled poses using an inspection process that eliminates the poses in which an object falls from the table when it is tilted. (Right) Over 12K 3D object models and 24K annotations of stable planes are included in the full dataset.}

  \label{fig:data_generation}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Dataset Generation}
For SOP, anticipating the geometrically and physically stable pose of an object is critical. While prior works\cite{jiang2012learninga, jiang2012learningb, haustein2019object, gualtieri2021robotic} only considered the geometrical stability of an object, we used a dynamic simulator (CoppeliaSim (formerly known as V-REP)\cite{rohmer2013v}) and introduce a synthetic dataset generation pipeline to automatically annotate a stable plane from several objects.


\subsubsection{Environment Setting}
% 몇개의 점을 샘플링하였는지 확인할 것
We used the benchmark datasets, 3DNet\cite{wohlkinger20123dnet}, ShapeNet\cite{chang2015shapenet}, and Yale-CMU-Berkeley (YCB)\cite{calli2015ycb}, as 3D object models and sampled 50,000 points using random Poisson sampling method\cite{yuksel2015sample} for all objects. We built the benchmark and learning environments using CoppeliaSim/V-REP\cite{rohmer2013v} and the PyRep\cite{james2019pyrep} interface. Fig. \ref{fig:data_generation} shows the simulation environment, which contains eight flat tables where various objects can be placed.

\subsubsection{Annotating Point Cloud}
% eq 설명 추가할 것
% along 8 grid of each roll, pitch, yaw -> quantization 표현으로 수정하면 좋을듯 !
The SOP process defines a transformation matrix $\mathcal{H}_\mathcal{O} \in \mathbb{SE}(3)$, where the transformation matrix contains translation $\mathbf{T}_\mathcal{O}^\mathcal{W} \in \mathbb{R}^3$ and rotation $\mathbf{R}_\mathcal{O}^\mathcal{W} \in \mathbb{SO}(3)$ matrices based on the world coordinate in the simulator. 
%(raeyo) Jiang et al. computed the kinetic energy change of object between discrete time step and compare rotation and translation difference between the initial state where the object made contact with supporter and the final state where the object stop moving, but~ 
Jiang et al.\cite{jiang2012learninga} computed the kinetic energy change between the state where the object was in contact with the supporter and the final state where the object stopped moving; however, this method was only valid because the objects were known. Based on (\ref{eq:movement}), we averaged the accumulated movements of the object after placement (${1 \over L}\sum_{i=1}^L \mathcal{M}$) during discrete time steps $L$, as a robot may cause the rotation of a target object that can be stably placed on a flat table. We placed each object with 512 poses (along the eight grids of roll, pitch, and yaw) on a horizontal table and considered the sample stable poses as those that satisfy ${1 \over L}\sum_{i=1}^L \mathcal{M} < \delta_1$. The sampled poses were then clustered using the Density-Based Spatial Clustering of Applications with Noise algorithm\cite{ester1996density} along the z-axis, which represents the normal vector of the contact of a stable plane with a horizontal surface. As real environments cannot be perfectly simulated, there are planes that are not easily generalized (e.g., a spherical model or sides of a cylinder), even if the sampled and clustered poses are reasonable in simulation. To generate a robust stable plane, we placed each object on an oblique table in a pose that belongs to the clustered stable plane and removed the planes where the pose of the object was unstable (${1 \over L}\sum_{i=1}^L \mathcal{M} < \delta_2$).

\begin{equation}
\label{eq:movement}
\mathcal{M} = ||\mathcal{H}_i - \mathcal{H}_{i-1}||_2^\mathcal{W}, where \ \mathcal{H}=[\mathbf{R}||\mathbf{T}]
\end{equation}

To annotate each point cloud with a sampled plane, we calculated the dot product of the point cloud along each normal vector of the stable plane, masking the points with lower dot product values ($<\delta_3)$. Consequently, our generated dataset contains 16,195 point clouds and 43,820 stable planes annotated by segmenting masks. 

\subsection{Deep Neural Network for Stable Object Placement} %도입부 지울 것
In this section, we present an end-to-end deep learning framework named SOP-NET that directly detects a stable plane from partial point clouds of objects. We learned stable plane prediction by jointly considering geometric and physical properties so that an object can rest in a stable configuration when a robot releases it on a horizontal surface. Fig. 2 illustrates the overall pipeline of SOP-NET.
% instance segmentation 후, the highest quaility 출력
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The stable plane is not considered because the parts of the object that are captured by the camera change depending on the pose of the object grasped by the robot. Thus, we used the SOP-SIM dataset and an instance segmentation scheme\cite{pham2019jsis3d, wang2019associatively} to segment all possible stable planes from partial observations. SOP-NET produces only one plane with the highest score after fitting the planes from the predicted instances and assigning stability scores based on the number of inlier points that constitute the planes.


%hidden point removal
In our implementation, we represent the partial observation of the object $\mathcal{\hat{X}} = \mathbb{R}^{N \times 3}$ as a point cloud, which is generated based on \cite{katz2007direct}, where the number of 3D point clouds $N$ is 2048. We employed PointNet encoder\cite{qi2017pointnet} to extract the global point cloud features $z \in \mathbb{R}^{128}$. Then, SOP-NET diverges into two different branches that predicts the point-wise semantics (stable or unstable) and embeds instance features for a stable plane.

\begin{equation}
\label{eq:the_sum_of_losses}
\mathcal{L} = \lambda_1 * \mathcal{L}_{stable} + \lambda_2 * \mathcal{L}_{plane},
\end{equation}
\noindent where $\mathcal{L}_{stable}$ is the standard binary cross-entropy loss and $\mathcal{L}_{plane}$ is the discriminative loss\cite{de2017semantic}.

During the training time, the semantic segmentation branch is supervised by the stable loss denoted as $\mathcal{L}_{stable}$. We adopted the discriminative function for 2D images\cite{de2017semantic} and 3D point clouds\cite{pham2019jsis3d, wang2019associatively} to embed instance features. While these studies used the discriminative loss to supervise both semantics for object categories and instances based on semantic information, we modified the loss to learn instances of stable planes based in a category-agnostic fashion. The total loss for training SOP-NET is determined according to (\ref{eq:the_sum_of_losses}) and the plane loss $\mathcal{L}_{plane}$ for instance learning is formulated as follows:

\begin{equation}
\label{eq:total_dis_loss}
\mathcal{L}_{plane} = \alpha * \mathcal{L}_{var} + \beta * \mathcal{L}_{dist} + \gamma * \mathcal{L}_{reg},
\end{equation}


\noindent where $\mathcal{L}_{var}$ aims to pull instance feature vectors toward the mean embedding vectors of the instance, i.e., the instance center, $\mathcal{L}_{dist}$ leads instances to push against each other, and $\mathcal{L}_{reg}$ is a regularization term to keep the embedding values bounded ((\ref{eq:var_loss}), (\ref{eq:dist_loss}), (\ref{eq:reg_loss})). We set $\lambda_1 = \lambda_2 = 1$ and $\alpha = \beta = 1, \gamma = 0.001$. Moreover, $K$ is the number of maximum instances (stable planes) in objects and $N_P^k$ is the number of points in target instance $k$; $\mu_k$ is the mean embedding vector of instance $k$; $z_j$ is the embedding of point $x_j$; $||\cdot||_2$ is the l2-norm and $\mathcal{V}(x) = max(0, x)$.

\begin{equation}
\label{eq:var_loss}
\mathcal{L}_{var} = {1 \over K}\sum_{k=1}^K{1 \over N_\mathcal{X}^K}\sum_{j=1}^{N_\mathcal{X}^k} \mathcal{V}(||\mu_k - z_j||_2 - \delta_v)^2
\end{equation}

\begin{equation}
\label{eq:dist_loss}
\mathcal{L}_{dist} = {1 \over K(K-1)}\sum_{k=1}^K \sum_{q=1, k \neq q}^K \mathcal{V}(2\delat_d - ||\mu_k - \mu_q||_2)^2
\end{equation}

\begin{equation}
\label{eq:reg_loss}
\mathcal{L}_{reg} = {1 \over K} \sum_{k=1}^K ||\mu_k||_2
\end{equation}

At the test time, we utilized mean shift clustering algorithm\cite{comaniciu2002mean} on instance embeddings to obtain instance labels and random sample consensus (RANSAC)\cite{fischler1981random} to fit a plane on clustered points. Then, we calculated the stability scores for each instance by element-wise multiplication of the semantic logits $\mathcal{P}_s$ and predicted instance labels $\mathcal{P}_s$ with the number of points composing each plane. Thus, the SOP-NET method determines the plane with the highest score.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 4 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 4.
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure 4/Figure_4_final.png}
        \end{subfigure}
        
    \caption{\textbf{Stability comparison of SOP in different simulations.} These are sample results of inference on the test data using SOP-NET and Trimesh. The observation (left) shows the visual appearance of the target object, and after creating a partial point cloud, an inference is conducted. The columns titled Trimesh and Ours (the two columns at the center) show the results of visualizing the stable region during inference. In the graph on the right, the different colors represent Trimesh (orange) and SOP-NET (blue, Ours) as a result of calculating the rotation immediately after placement.}
    \label{fig:real_experiment}
\end{figure}

% SOP-NET과 Trimesh를 이용하여 Test data를 inference한 결과이다. 왼쪽 그림은 target object에 대한 visual observation을 나타내며 이를 partial point cloud화 한 뒤, inference를 진행한다. 중앙의 두 그림(Trimesh, Ours)은 inference 시, Stable region을 시각화 한 결과 이며, 오른쪽의 Graph에서 주황색은 Trimesh에 대하여, 파란색은 SOP-NET(Ours)에 대하여 stability를 측정하기 위해 placement직후 rotation을 accumulate한 결과로 수치가 낮을수록 stable하다.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 5 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 5.
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure 5/Figure_5_final.png}
        \end{subfigure}
        
    \caption{\textbf{Comparison of the prediction results for each method}. This is a sample.}
    \label{fig:comparison_pred_res}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Experiments %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
In this work, we develop a deep learning-based framework for a robot to place an object stably on horizontal surface. Our experiments focus on evaluating (1) how stably SOP-NET can place objects when compared to baseline methods, (2) how robustly SOP-NET can predict a stable plane in various environments, and (3) how effectively SOP-NET can apply in real-world scenarios.

\subsection{Experimental Setup}
\subsubsection{Dataset} We generate the SOP-SIM dataset by combining ShapeNet\cite{}, 3DNet\cite{} and the YCB\cite{} benchmark datasets. As training dataset, we use custom ShapeNet and 3DNet, and as test dataset, we use custom YCB. During training, no YCB datasets were observed. 


\subsubsection{Baselines} %RANSAC/Primitive Shape fiiting/Trimesh/ours 
\subsubsection{Settings} We set up the simulation environment in Coppeliasim (V-REP) \cite{rohmer2013v} for evaluating our overall pipeline on synthetic data. 

\subsection{Experiments on Simulation}
\subsubsection{Metric for evaluation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 6 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 6.
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure 6./Figure_6_final.png}
        \end{subfigure}
        
    \caption{\textbf{Real-world experiment of SOP}. Pipeline of robot application. The RGB-D image is observed from the camera installed in the external view. And then, a point cloud is created and cropped for the target object. Thereafter, the cropped point cloud is used as an input of SOP-Net and a stable plane is output as Output. Finally, execute a rotation for robot action to place the target object stably.}
    \label{fig:real_experiment}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Table 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% rotation(degree), translation(meter) 단위 체크, Translation * E-05
% Please add the following required packages to your document preamble:
\begin{table*}[ht!]
\centering
\caption{}
\label{tab:unseen_sop_eval}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|cccccccccccccc|}
\hline
\multicolumn{14}{|c|}{\textbf{Stable Object Placement Evaluataion Metric (Unseen)}} \\ \hline
\multicolumn{2}{|c|}{\multirow{3}{*}{\textbf{Object Type}}} & \multicolumn{8}{c|}{\textbf{Stability (S)}} & \multicolumn{4}{c|}{\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Success Rate of \\ Stable Placement (<5)\end{tabular}}}} \\ \cline{3-10}
\multicolumn{2}{|c|}{} & \multicolumn{4}{c|}{\textbf{Rotation (R) \downarrow}} & \multicolumn{4}{c|}{\textbf{Translation (T) \downarrow}} & \multicolumn{4}{c|}{} \\ \cline{3-14} 
\multicolumn{2}{|c|}{} & Trimesh\cite{} & Bbox & RANSAC\cite{} & \multicolumn{1}{c|}{SOP-Net(\textbf{Ours})} & Trimesh\cite{} & Bbox & RANSAC\cite{} & \multicolumn{1}{c|}{SOP-Net(\textbf{Ours})} & Trimesh\cite{} & Bbox & RANSAC\cite{} & SOP-Net(\textbf{Ours}) \\ \hline
% Complete Point Cloud
\multicolumn{1}{|c|}{\multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Complete \\ Point cloud\end{tabular}}} & \multicolumn{1}{c|}{Easy} % Easy
& 36.74 & 37.34 & 5.32 & \multicolumn{1}{c|}{1.35} % Rotation
& 0.28 & 2.99 & 0.20 & \multicolumn{1}{c|}{0.19} % Translation
& 90.48 & 33.33 & 95.24 & 95.24 \\ % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Normal} % Normal
& 0.74 & 14.17 & 2.65 & \multicolumn{1}{c|}{0.78} % Rotation
& 0.12 & 2.36 & 0.30 & \multicolumn{1}{c|}{0.13} % Translation
& 100 & 52.38 & 85.71 & 85.71 \\ % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Hard} % Hard
& 11.04 & 33.19 & 12.14 & \multicolumn{1}{c|}{2.86} % Rotation
& 1.10 & 4.21 & 1.52 & \multicolumn{1}{c|}{0.37} % Translation
& 52.38 & 33.33 & 47.62 & 33.33 \\ \cline{2-14} % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Total avg.} % Total
& 4.55 & 21.84 & 5.46 & \multicolumn{1}{c|}{1.66} 
& 0.50 & 3.19 & 0.67 & \multicolumn{1}{c|}{0.23} 
& 80.95 & 39.68 & 76.19 & 71.43 \\ \hline % Success Rate
% Partial Point Cloud
\multicolumn{1}{|c|}{\multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Partial \\ Point cloud\end{tabular}}} & \multicolumn{1}{c|}{Easy} % Easy
& 36.74 & 37.34 & 5.32 & \multicolumn{1}{c|}{2.65} % Rotation
& 7.48 & 6.76 & 0.75 & \multicolumn{1}{c|}{0.38} % Translation
& 38.95 & 4.90 & 86.90 & 85.24 \\ % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Normal} % Normal
& 66.04 & 67.43 & 36.84 & \multicolumn{1}{c|}{3.23} % Rotation
& 11.94 & 12.20 & 6.37 & \multicolumn{1}{c|}{0.46} % Translation
& 33.05 & 15.67 & 52.24 & 79.95 \\ % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Hard} % Hard
& 28.50 & 37.71 & 27.84 & \multicolumn{1}{c|}{14.24} % Rotation
& 3.34 & 4.14 & 2.97 & \multicolumn{1}{c|}{1.84} % Translation
& 43.71 & 26.43 & 40.14 & 29.33 \\ \cline{2-14} % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Total avg.} % Total avg.
& 43.76 & 47.49 & 23.33 & \multicolumn{1}{c|}{6.71} % Rotation
& 7.59 & 7.70 & 3.36 & \multicolumn{1}{c|}{0.89} % Translation
& 38.57 & 15.67 & 59.76 & 64.84 \\ \hline % Success Rate

\end{tabular}%
}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Table2%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[ht!]
\centering
\caption{}
\label{tab:seen_sop_eval}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|ccccccccccccc|}
\hline
\multicolumn{13}{|c|}{\textbf{Stable Object Placement Evaluataion Metric (Seen)}} \\ \hline
\multicolumn{2}{|c|}{\multirow{3}{*}{\textbf{Object Type}}} & \multicolumn{8}{c|}{\textbf{Stability (S)}} & \multicolumn{3}{c|}{\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Success Rate of \\ Stable Placement\end{tabular}}}} \\ \cline{3-10}
\multicolumn{2}{|c|}{} & \multicolumn{4}{c|}{\textbf{Rotation (R) \downarrow}} & \multicolumn{4}{c|}{\textbf{Translation (T) \downarrow}} & \multicolumn{3}{c|}{} \\ \cline{3-13} 
\multicolumn{2}{|c|}{} & GT & Trimesh\cite{} & RANSAC\cite{} & \multicolumn{1}{c|}{SOP-Net(\textbf{Ours})} & GT & Trimesh\cite{} & RANSAC\cite{} & \multicolumn{1}{c|}{SOP-Net(\textbf{Ours})} & Trimesh\cite{} & RANSAC\cite{} & SOP-Net(\textbf{Ours}) \\ \hline
\multicolumn{1}{|c|}{\multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Complete \\ Point cloud\end{tabular}}} & \multicolumn{1}{c|}{Easy} & 0 & 0 & 0 &
\multicolumn{1}{c|}{0} & 0 & 0 & 0 & \multicolumn{1}{c|}{0} & 0 & 0 & 0 \\
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Normal} & 0 & 0 & 0 & \multicolumn{1}{c|}{0} & 0 & 0 & 0 & \multicolumn{1}{c|}{0} & 0 & 0 & 0 \\
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Hard} & 0 & 0 & 0 & \multicolumn{1}{c|}{0} & 0 & 0 & 0 & \multicolumn{1}{c|}{0} & 0 & 0 & 0 \\ \cline{2-13}
% \multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Extremly Hard} & 0 & 0 & 0 & \multicolumn{1}{c|}{0} & 0 & 0 & 0 & \multicolumn{1}{c|}{0} & 0 & 0 & 0 \\ \cline{2-13} 
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Total avg.} & 0 & 0 & 0 & \multicolumn{1}{c|}{0} & 0 & 0 & 0 & \multicolumn{1}{c|}{0} & 0 & 0 & 0 \\ \hline
\multicolumn{1}{|c|}{\multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Partial \\ Point cloud\end{tabular}}} & \multicolumn{1}{c|}{Easy} & 0.0463 & 0.626 & 0 & \multicolumn{1}{c|}{0.0446} & 7.051 & 9.325 & 0 & \multicolumn{1}{c|}{6.733} & 0 & 0 & 0 \\
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Normal} & 0.0534 & 0.0806 & 0 & \multicolumn{1}{c|}{0.0670} & 8.870 & 9.275 & 0 & \multicolumn{1}{c|}{9.723} & 0 & 0 & 0 \\
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Hard} & 0.0806 & 0.0001 & 0 & \multicolumn{1}{c|}{0.0612} & 12.42 & 10.39 & 0 & \multicolumn{1}{c|}{6.849} & 0 & 0 & 0 \\ \cline{2-13}
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Total avg.} & 0.0601 & 0.0478 & 0 & \multicolumn{1}{c|}{0.0576} & 9.447 & 10.34 & 0 & \multicolumn{1}{c|}{7.768} & 0 & 0 & 0 \\ \hline
\end{tabular}%
}
\end{table*}


\subsection{Experiments on Real-World}
\subsubsection{Object segmentation }



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Conclusion %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusions}

In this paper,




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Appendix & acknowledgement %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{APPENDIX}

Appendixes should appear before the acknowledgment.

\section*{Acknowledgement} % 비정형
\begin{spacing}{0.4}
{\scriptsize This work was fully supported by the Korea Institute for Advancement of Technology (KIAT) grant funded by the Korea Government (MOTIE) (Project Name: Shared autonomy based on deep reinforcement learning for responding intelligently to unfixed environments such as robotic assembly tasks, Project Number: 20008613). This work was also partially supported by the HPC Support project of the Korea Ministry of Science and ICT and NIPA.}
\end{spacing}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{references.bib}
\bibliographystyle{IEEEtran}

\end{document}

