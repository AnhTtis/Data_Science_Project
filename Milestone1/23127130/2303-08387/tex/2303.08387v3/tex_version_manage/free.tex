Stable object placement is essential for robots performing a variety tasks. Since objects (e.g., mechanical equipment, glass, plate, bowl) that must be handled carefully in industrial fields or at household can be damaged or broken if not placed carefully. Previous works have primarily focused on robotic systems, such as grasping\cite{mahler2017dex, morrison2018closing, mousavian20196, sundermeyer2021contact}, picking and placing\cite{harada2012object, gualtieri2018learning}, upright placement of\cite{newbury2021learning}, or rearranging\cite{wada2022reorientbot} objects. 

Only a few studies have considered stability in object placement, which is required for maintaining the full shape of 3D object models\cite{haustein2019object, gualtieri2021robotic, tournassoud1987regrasping}; alternatively, several studies were only focused on performing placement on task-specific objects (e.g., plates, cups) and supporting items (e.g., rack, pen holder, stemware holder) \cite{jiang2012learninga, jiang2012learningb}. However, relying on the availability of complete 3D models is a major drawback in real-world scenarios where a robot physically interacts with objects based on partially observed visual data (e.g., depth camera). To overcome this limitation, the utilization of multiview-based methods for representing 3D object models\cite{newbury2021learning} or a shape completion module\cite{gualtieri2021robotic} for generating a full-object model have been proposed; however, both methods may not be sufficiently accurate for generating 3D object models. Here, we consider the problem of stable object placement (\textbf\textit{SOP}) based on the 3D point cloud of the depth camera. Our goal is to directly predict a stable plane from partial observation, which will enable the object to physically maintain a stable pose. 



% related works

\noindent\textbf{Deep Neural Networks for Learning from 3D Data} 
% 추가 가능 : partial analysis or completion works 
With the considerable success of deep learning on image recognition, object detection, and semantic segmentation using 2D images, researchers have presented learning-based approaches using a variety of 3D representations, including multiview images\cite{boulch2018snapnet, su2015multi, boulch2017unstructured}, 2.5D depth images\cite{gupta2014learning}, 3D voxels\cite{maturana2015voxnet, riegler2017octnet, wang2017cnn}, and 3D point clouds\cite{qi2017pointnet, qi2017pointnet++, liu2019relation}. Qi et al. proposed new architectures, called PointNet\cite{qi2017pointnet} and PointNet++\cite{qi2017pointnet++}, that can process raw point clouds directly and extract the geometric features efficiently, achieving good performance on classification and semantic segmentation tasks. The success of PointNet and PointNet++ prompted the development of various network architectures that represent 3D data, leading to significant improvements not only in 3D-object pose estimation\cite{tremblay2018deep, brachmann2014learning, oberweger2018making} and instance segmentation\cite{wang2018sgpn, pham2019jsis3d, yi2019gspn, lahoud20193d, yang2019learning}, but also in deep learning-based robotic manipulations\cite{mousavian20196, sundermeyer2021contact, fang2020graspnet}. To detect a stable plane to place an object, global as well as local geometrical information is required. Accurate geometric information of objects cannot be obtained by performing operations on a single RGB image as it is based on 2D coordinates. Thus, we used 3D point clouds and PointNet\cite{qi2017pointnet}/PointNet++\cite{qi2017pointnet++} as the backbone to produce a stable plane in the special Euclidean group SE(3). 


% related works
% sop 관련 내용 작성
Analyzing the convex hull of the object with the center of mass and sampling the explicit faces where the object rests in a stable configuration can enable stable placement on a horizontal surface\cite{tournassoud1987regrasping, wan2019regrasp,lertkultanon2018certified, haustein2019object}. When the geometrical and physical properties of an object are known, the robot can perform well in terms of stable placement performance. However, this process is difficult because it requires precise information about the object, whereas in the real world, only partial observation with noise is accessible (e.g., RGB-depth (RGB-D) camera). To overcome this limitation, several groups have demonstrated integrated vision-based placement using stable object states that are sampled with known or abstracted geometry of the object or predicted by a learned model. 

In \cite{gualtieri2021robotic}, although researchers utilized the deep learning-based completion module and an analytical method to address the uncertainty for partially visible objects in the real world, the completion module might not be sufficiently accurate to generate the full shape. 



Jiang et al.\cite{jiang2012learninga} trained a classifier using a hand-crafted dataset to identify placements that are likely to be stable and satisfy human preference. They validated the physical feasibility of placement and human preferences, but full observability was required, and dataset labels were heuristic. Recently, Cheng et al.\cite{cheng2021learning} present a deep learning model training with a synthetic dataset, but task-specific objects were still required.



% statement

Let $\mathcal{Z} = \{\mathcal{R}, \mathcal{O}, \mathcal{T}_\mathcal{O}, \mathcal{T}_\mathcal{C}\}$ denote a state describing the properties of a robot, objects, and a camera in the manipulating scene, where the robot $\mathcal{R}$ grasps the target object $\mathcal{O}$, and $\mathcal{T}_\mathcal{O}$, $\mathcal{T}_\mathcal{C}$ are the 3D poses of the object in the robot gripper and camera respectively.



% Let $\mathcal{X} = \mathbb{R}^{H \times W}_{+}$ be a 2.5D point cloud represented as a depth image with height $H$ and width $W$ captured by a camera with known intrinsic characteristics\cite{page2005multiple} and 3D pose of the camera $\mathcal{T}_\mathcal{C}$. The input of our model is $\mathcal{\hat{X}} = \mathbb{R}^{N \times 3}$, where $N$ points are sampled from $\mathcal{X}$. 



We define \textit{stability} $\mathcal{S} = {\frac{1}{L}}\sum_{i=1}^L{\parallel \mathcal{H}_i - \mathcal{H}_{i-1}\parallel}_2^\mathcal{W}$ as the amount of homogeneous transformation change at the world coordinate $\mathcal{W}$ in a simulator during a discrete time step $L$. Based on \textit{stability}, stable planes $\mathcal{A}_\mathcal{T}^\mathcal{O}$ were generated for each object model $\{{O}_i | i=1,..,M, O_i \in \mathcal{O} \}$ by placing at uniformly determined poses $\matchal{T}_\mathcal{O}$, where satisfy $\mathcal{S}_\mathcal{T}^\mathcal{O} < \epsilon$.



% method

%category-agnostic 어떻게 서술할 것인지 생각할 것

% Sim2real transfer
% 1. noise 추가한 것 서술
% 3. category-agnostic




% The stable plane is not considered because the parts of the object that are captured by the camera change depending on the pose of the object grasped by the robot. Thus, we used the SOP-Sim dataset and an instance segmentation scheme\cite{pham2019jsis3d, wang2019associatively} to segment all possible stable planes from partial observations. SOP-Net produces only one plane with the highest score after fitting the planes from the predicted instances and assigning stability scores based on the number of inlier points that constitute the planes.


% %hidden point removal
% In our implementation, we represent the partial observation of the object $\mathcal{\hat{X}} = \mathbb{R}^{N \times 3}$ as a point cloud, which is generated based on \cite{katz2007direct}, where the number of 3D point clouds $N$ is 2048. We employed PointNet encoder\cite{qi2017pointnet} to extract the global point cloud features $z \in \mathbb{R}^{128}$. Then, SOP-Net diverges into two different branches that predicts the point-wise semantics (stable or unstable) and embeds instance features for a stable plane.



% During the training time, the semantic segmentation branch is supervised by the stable loss denoted as $\mathcal{L}_{stable}$. We adopted the discriminative function for 2D images\cite{de2017semantic} and 3D point clouds\cite{pham2019jsis3d, wang2019associatively} to embed instance features. While these studies used the discriminative loss to supervise both semantics for object categories and instances based on semantic information, we modified the loss to learn instances of stable planes based in a category-agnostic fashion. 
% The total loss for training SOP-Net is determined according to (\ref{eq:the_sum_of_losses}) and the plane loss $\mathcal{L}_{plane}$ for instance learning is formulated as follows:

% \begin{equation}
% \label{eq:total_dis_loss}
% \mathcal{L}_{plane} = \alpha * \mathcal{L}_{var} + \beta * \mathcal{L}_{dist} + \gamma * \mathcal{L}_{reg},
% \end{equation}


% \noindent where $\mathcal{L}_{var}$ aims to pull instance feature vectors toward the mean embedding vectors of the instance, i.e., the instance center, $\mathcal{L}_{dist}$ leads instances to push against each other, and $\mathcal{L}_{reg}$ is a regularization term to keep the embedding values bounded ((\ref{eq:var_loss}), (\ref{eq:dist_loss}), (\ref{eq:reg_loss})). We set $\lambda_1 = \lambda_2 = 1$ and $\alpha = \beta = 1, \gamma = 0.001$. Moreover, $K$ is the number of maximum instances (stable planes) in objects and $N_P^k$ is the number of points in target instance $k$; $\mu_k$ is the mean embedding vector of instance $k$; $z_j$ is the embedding of point $x_j$; $||\cdot||_2$ is the l2-norm and $\mathcal{V}(x) = max(0, x)$.

% \begin{equation}
% \label{eq:var_loss}
% \mathcal{L}_{var} = {1 \over K}\sum_{k=1}^K{1 \over N_\mathcal{X}^K}\sum_{j=1}^{N_\mathcal{X}^k} \mathcal{V}(||\mu_k - z_j||_2 - \delta_v)^2
% \end{equation}

% \begin{equation}
% \label{eq:dist_loss}
% \mathcal{L}_{dist} = {1 \over K(K-1)}\sum_{k=1}^K \sum_{q=1, k \neq q}^K \mathcal{V}(2\delat_d - ||\mu_k - \mu_q||_2)^2
% \end{equation}

% \begin{equation}
% \label{eq:reg_loss}
% \mathcal{L}_{reg} = {1 \over K} \sum_{k=1}^K ||\mu_k||_2
% \end{equation}

% At the test time, we utilized mean shift clustering algorithm\cite{comaniciu2002mean} on instance embeddings to obtain instance labels and random sample consensus (RANSAC)\cite{fischler1981random} to fit a plane on clustered points. Then, we calculated the stability scores for each instance by element-wise multiplication of the semantic logits $\mathcal{P}_s$ and predicted instance labels $\mathcal{P}_s$ with the number of points composing each plane. Thus, the SOP-Net method determines the plane with the highest score.
