%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, journal, twoside]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{url}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{etoolbox}
\usepackage{placeins}
\usepackage{hyperref}
\usepackage{setspace} 
\usepackage{romannum}
\usepackage{wasysym}
\let\Square\relax
\usepackage{bbding}
\usepackage{pifont}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{array}
\usepackage{cite}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}\usepackage{algpseudocode}
\usepackage{hhline}

\begin{document}


\title{Enhanced Unseen Object Placement Method to Ensure Stable Installation of Objects by Robots }

\author{Sangjun Noh$^{*}$, Raeyoung Kang$^{*}$, Taewon Kim$^{*}$, Seunghyeok Back, Seongho Bak, Kyoobin Lee†% <-this % stops a space
\thanks{\text{*} These authors contributed equally to the paper.}
\thanks{All authors are with the School of Integrated Technology, Gwangju Institute of Science and Technology, Cheomdan-gwagiro 123, Buk-gu, Gwangju 61005, Republic of Korea. 
† Corresponding author: Kyoobin Lee { \\ tt \\ small kyoobinlee@gist.ac.kr}}. %
}
\thanks{Manuscript received April 19, 2021; revised August 16, 2021.}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.




\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Abstract %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
The unseen object placement (UOP) approach requires a robot to efficiently pick and place objects despite an incomplete and noisy perception in the real world. Although several previous studies have proposed various UOP methods, their applicability was limited owing to their requirement of a complete object shape or a completion module to perform partial observation. Thus, we propose a method for directly detecting the stable planes of an unseen object when the robot can only obtain partial observations. Furthermore, we consider the stability of the object after placing it on a planar workspace because the robot is required to place the object without toppling it. We trained our model using large-scale simulation data to generalize the object shape and properties of the stable plane with a 3D point cloud. The superiority of our proposed method was verified through simulations and real-world robot experiments. Our approach achieves state-of-the-art performance in both simulations and the real world, where the target objects are partially visible, despite the model being purely trained through simulations. 




\end{abstract}

\begin{IEEEkeywords}
Unseen object placement network, deep learning-based network, stable object plane, grasp generator convolutional neural network.
\end{IEEEkeywords}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Introduction %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
Owing to a limitation on the number and type of objects that can be modeled in a complex/dynamic environment, it is a challenging task for robots to recognize different objects and reason about their geometry and physical properties. Furthermore, when it comes to objects (e.g., mechanical equipment, glasses, plates, and bowls) that need to be handled carefully in industries or households, the robots should be designed to pick and place the objects stably. Previous studies have primarily focused on robots grasping previously unperceived objects \cite{mahler2017dex, mousavian20196}, but only a few studies\cite{gualtieri2018pick, mitash2020task, paxton2022predicting} have been conducted for testing the efficiency of robots in ensuring the stability of an object after releasing it. 


Prior studies for placing objects stably have been presented, which are required for maintaining the full shape of 3D object models\cite{haustein2019object, gualtieri2021robotic, tournassoud1987regrasping}. Additionally, several studies have tested the performance of placement only for task-specific objects (e.g., plates and cups) and supporting items (e.g., rack, pen holder, and stemware holder) \cite{jiang2012learninga, jiang2012learningb}. However, relying on complete 3D models is a major drawback for real-world applicability because, in the real world, the robot physically interacts with objects based on partially perceivable visual data. 


% ---- 
% Fig 1.
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure 1/Figure_1.png}
        \end{subfigure}
        
    \caption{Comparison of unseen object placement (UOP)-Net proposed in this study and prior UP approaches. Previous studies applied (a) a complete object model\cite{haustein2019object, trimesh, hagelskjaer2019using} or (b) completion module\cite{gualtieri2021robotic} or (c) only fitted a primitive shape to the object\cite{fischler1981random, Zhou2018}. Contrarily, (d) UOP-Net can reason to detect a stable plane for various objects from a single partial view.} 
    \label{fig:task_comparison}
\end{figure}

% object (17,408 => train: 13926, val:3482 ) // anno (69,027 => train :55261, val:13766)
This study proposes an approach for performing \textit{ UOP}, by detecting a plane to place the object stably in case of either partial or unseen observation of objects. Here, we defined the stability of object placement by considering the physical properties of the objects. This was executed by generating a large-scale synthetic dataset using a physics simulator called UOP-Sim by dropping and sampling stable planes of various objects. While \cite{jiang2012learninga, jiang2012learningb} labeled the preferred placing configuration using a heuristic technique, we automatically annotated all possible planes where the object can sustain a stable pose based on our definition of stability. Our dataset contains 17.4k 3D objects and a total of 69k annotations of stable planes. Furthermore, we propose a deep learning-based network named UOP-Net, that predicts the most stable plane to place an object using the partial observation data. We trained UOP-Net only using the simulation dataset (UOP-Sim) with no fine-tuning and evaluated its performance on three benchmarks (3DNet, ShapeNet, and YCB). We achieved state-of-the-art (SOTA) performance in both simulation and real-world environments, even when single partial views and unseen objects were given. The contributions of this study are summarized as follows:

\begin{itemize}
    \item{We propose a method for (\textbf\textit{UOP}) by identifying a plane that allows an object to sustain its stability in case of partial observation of the object or a completely unseen object.}
    
    \item{We provide a public large-scale 3D synthetic dataset, called UOP-Sim, containing a total of 69,027 stable annotations for 17,408 different objects.}

    \item{We trained UOP-Net a using synthetic dataset without applying any fine-tuning, and achieved an SOTA performance verified through simulations and real world environments. Our results show that UOP-Net can detect implicit planes as well as explicit planes}
    
\end{itemize} 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Related Works %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Works}
\noindent\textbf{Unseen Object Placement}
% unseen object grasping : dexnet2.0, graspnet, contact-graspent, ggcnn 등등
Very few studies have been conducted that focus on UOP, in contrast to grasping unseen objects \cite{mahler2017dex, morrison2018closing, mousavian20196}. In a pioneering work, Jiang et al.\cite{jiang2012learninga} trained a classifier using a hand-crafted dataset to identify placements that are likely to be stable and satisfy human preferences. They validated the feasibility of placement and human preferences; however, full observability was required, and the dataset labels were heuristic. To overcome these limitations, Cheng et al.\cite{cheng2021learning} presented a deep learning model based on simulations; however, task-specific objects were still required. Unlike these methods, which aim to place an object stably on various surfaces (e.g., rack and stemware holder), we aim to place an unseen rigid object stably on a horizontal surface, even when only a single partial observation is accessible. 


\noindent\textbf{Stable Object Placement}
Robots can stably install an object when its geometrical properties are known. Analyzing the convex hull of an object using its center of mass and sampling the explicit planes where the object rests in a stable configuration can enable its stable placement on a horizontal surface\cite{tournassoud1987regrasping, wan2019regrasp,lertkultanon2018certified, haustein2019object}. However, applying the analytical method in an actual environment is difficult because it requires precise information about the object, which is inaccessible owing to a partial observation (e.g., an RGB-D camera). In \cite{gualtieri2021robotic}, although researchers utilized a deep learning-based completion method to address the uncertainty of partially visible objects in the real world, the completion module may not be sufficiently accurate to generate the precise shape of an object. Conversely, our method can directly detect a stable plane from partial observations without requiring any other modules.

\noindent\textbf{Robotic Applications based on Object Placement}
Recently, a wide range of object placement problems has been investigated, such as a constrained placement\cite{mitash2020task}, upright placement\cite{newbury2021learning}, rearrangement\cite{wada2022reorientbot, paxton2022predicting} and packing\cite{wang2021dense}. Mitash et al.\cite{mitash2020task} utilized a multi-view-based primitive shape-fitting method to address the constrained placement problem in the absence of object models. In \cite{paxton2022predicting}, a deep learning network was proposed that determines the required rotation of an object so that it is stably placed in an upright orientation. Similarly, Li et al.\cite{listable} presented an approach for predicting the rotation that can stably maintain the object with the height-maximizing pose.


% object (17,408 => train: 13926, val:3482 ) // anno (69,027 => train :55261, val:13766)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 2.
\begin{figure*}[ht!]
    \centering
        \includegraphics[width=\textwidth]{figures/Figure 2/Figure_2.png}
  \caption{UOP-Sim dataset generation pipeline. (Left) The database contains a total of 17.4K 3D object mesh models, that we built to place and estimate the stability of the object (\mathcal{S}). (Top) We drop each object on the table in 512 different poses and sample stable planes from the objects that satisfy eq.\ref{eq:stability}. (Bottom) We verify the stable plane candidates with the tilted table. (Right) Over 17.5K 3D object models and 69K annotations of stable planes are included in the full dataset (UOP-Sim).}
    \label{fig:UOP-sim}
\end{figure*}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \noindent\textbf{Unseen Object Perception}
% To work in an industrial domain, a robot should be able to recognize and manipulate new objects that have never been seen before. Detecting unseen objects is a challenging task because modeling all objects in the real world is impractical and infeasible. Many object perception approaches\cite{xie2021unseen, back2021unseen, xie2020best, xiang2020learning} have been proposed to segment the visible parts of objects, although unseen objects and cluttered scenes have been neglected here. In robotic manipulation, the grasp quality convolutional neural network (GQ-CNN)\cite{mahler2017dex} and six-degrees-of-freedom (6DoF) GraspNet\cite{mousavian20196} are representative approaches for detecting grasp points from depth images and point clouds of unseen objects. These prior studies used synthetic data to train category-agnostic object models to learn the generalization of objects. In this study, we also trained our proposed method for UOP in a category-agnostic fashion with the objective of detecting regions that can be utilized for stably placing an unseen object using a 3D point cloud.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Problem Statement %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Statements}
We considered the problem of (\textbf{UOP}) from a single-view observation, where a robot picks up an unseen rigid object using grasp points generated by a grasp generator (e.g., GQ-CNN\cite{mahler2017dex}, GG-CNN\cite{morrison2018closing}, or 6-DoF GraspNet\cite{mousavian20196}). 

%%%%%definitions
\subsection{Definitions}
\noindent\textbf{States}: The manipulating scene consists of a robot $\mathcal{R}$, an object $\mathcal{O}$, and a camera $\mathcal{C}$ with a known intrinsic parameter and pose $\mathcal{T}_\mathcal{C}$. Robot $\mathcal{R}$ grasps target object $\mathcal{O}$ with pose $\mathcal{T}_\mathcal{O}$. Thus, the state $\mathcal{Z}$ can be denoted as $\{\mathcal{R}, \mathcal{O}, \mathcal{T}_\mathcal{O}, \mathcal{T}_\mathcal{C}\}$.


\noindent\textbf{Point Clouds}: Given the depth image captured by the depth camera with known intrinsic and extrinsic parameters, we sampled a set of 3D points $\{x_i | i = 1, ... ,n\}$, where each of the $n$ points is a vector of the Euclidean coordinate $\mathbb{R}^3$. 


\noindent\textbf{Object Stability and Stable Planes}: Let $\mathcal{S}_\mathcal{T}^\mathcal{O}$ denote the placement stability of an object model $\mathcal {O}$ at the grasped pose $\mathcal{T}$. Although Jiang et al.\cite{jiang2012learninga} computed object stability using the kinetic energy in a simulation, they only considered the initial and final states. Thus, we define \textit{ stability } $\mathcal{S}$ as the total amount of homogeneous transformation change at the world coordinate $\mathcal{W}$ in a simulator during a discrete time step $L$. Based on \textit{ stability } $\mathcal{S}$, stable planes were annotated for each object model, which satisfy the condition of $\mathcal{S}_\mathcal{T}^\mathcal{O} < \epsilon$.


\noindent\textbf{Dataset and Deep Learning Model}: The dataset $\mathcal{D} = \{({O}_m, \mathcal{A}_m) \}_{1}^{M}$ represents the set of M object models $\mathcal{O}$ (e.g., point clouds) and the stable placement plane $\mathcal{A}$ as the ground truth. $\mathcal{D}_{train}$ and $\mathcal{D}_{test}$ represent the training and test datasets, respectively, and they satisfy the following state: $\mathcal{D}_{train} \cup \mathcal{D}_{test} = \mathcal{D}$. Let the function $\mathcal{F}: \mathcal{X}_p \rightarrow \mathcal{A}$ be a deep learning model that adopts one object as the input and produces a stable plane $\mathcal{A}$ as the output.

\noindent\textbf {Seen and Unseen Objects}: If $\mathcal{O}_{\mathcal{D}_{train}} \cap \mathcal{O}_{\mathcal{D}_{train}} = \emptyset$, $\mathcal{O}_{\mathcal{D}_{train}}$ denotes a seen object and $\mathcal{O}_{\mathcal{D}_{train}}$ denotes an unseen object in the deep learning model $\mathcal{F}$. 

%%%%%%%objectives
\subsection{Objectives} 
We aim to detect a stable plane for the placement of arbitrary objects using only single-view observations. Thus, we aimed to develop a neural network $\mathcal{F}: \mathcal{\hat{X}} \rightarrow \methcal{A}$ that minimizes \textit{stability} $\mathcal{S}_\mathcal{T}^\mathcal{O}$ when partial point clouds $\mathcal{\hat{X}}$ are given.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Method %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Learning UOP}
Solving an UOP function that predicts a robust plane is challenging for several reasons. First, large-scale samples are required to approximate the expectations of multiple possible objects. Second, learning through a simple linear or mathematical model can be difficult because the relationship between the point cloud of objects and annotated metrics is complex, considering both the geometry and physics. To address these issues, we generated a synthetic dataset called UOP-Sim, containing 17k 3D object models (point clouds), and 69k labeled stable planes. Furthermore, we developed UOP-Net, which learns to detect robust stable planes using the UOP-Sim.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Synthetic Data Generation}
As there are a vast number of different types of object models, repeating the real-data acquisition and annotation processes to collect a sufficient amount of data is time-consuming and impractical. Because 3D CAD models of novel objects are freely available online (e.g., 3D-Net, ShapeNet, and YCB), a synthetic dataset can be generated in a virtual environment rather than real-data collection and labeling. Thus, we used a dynamic simulator to create a synthetic dataset for the training and validation of UOP-Net, which can accelerate modeling cycles from data collection to deployment in accordance with various objects. Fig.\ref{fig:UOP-sim} illustrates our overall pipeline for synthetic dataset generation, which is made up of two processes. We begin by randomly dropping a rigid object on a horizontal surface to search for all the possible stable plane candidates. Subsequently, the stable planes for each object are annotated by placing them on a tilted table with the sampled candidates.


\noindent\textbf{Simulation Environment Setting}
% object (17,408 => train: 13926, val:3482 ) // anno (69,027 => train :55261, val:13766)
We used the benchmark datasets, namely, 3DNet\cite{wohlkinger20123dnet}, ShapeNet\cite{chang2015shapenet}, and Yale-CMU-Berkeley (YCB)\cite{calli2015ycb}, for a total of 17,408 3D object models. Using PyRep\cite{james2019pyrep} and CoppeliaSim\cite{rohmer2013v}, we created a learning environment to estimate object stability during a discrete time step subsequent to the instant when the object mesh established a contact with the table plane. In the virtual environment, we built 64 table object models to accelerate the annotation process.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 3.
\begin{figure*}[ht!]
   \centering
      \includegraphics[width=\textwidth]{figures/Figure 3/Figure_3.png}

    \caption{Overall pipeline of UOP. (Center) We generate partial point clouds using the UOP-Sim dataset since we aim to identify the stable planes directly in case of partially observable scenarios. UOP-Net then learns to predict the most stable plane. (Left) When the target object in the robot gripper is observed using an RGB-D camera, a partial point cloud is fed into UOP-Net. (Right) Using the estimated stable plane, the robot executes object placement based on the angle difference between the normal vector of the plane and the negative gravity vector.}

  \label{fig:UOP-net}
\end{figure*}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 512가지 포즈 (along eight grids of roll, pitch, and yaw).
% randomly release / drop
% 멈췄을 때 물체의 자세
% plane의 normal을 z축 하단 을 기준으로 5% 가 되는 영역의 normal vector를 저장 => plane 만들때
% plane을 horizontal surface에 놓고 정지상태가 되었을 때 기울임 => stability를 측정
% time step 0.005s 각 time step 마다 물체의 포즈 


\noindent\textbf{Stable Plane Annotation.}
To estimate object stability $\mathcal{S}$ in a rigid-body simulation, we first define the object movement $\mathcal{M}$ using a homogeneous transformation matrix $\mathcal{H}_\mathcal{O} \in \mathbb{SE}(3)$, where the transformation matrix comprises the translation $\mathbf{T}_\mathcal{O}^\mathcal{W} \in \mathbb{R}^3$ and rotation $\mathbf{R}_\mathcal{O}^\mathcal{W} \in \mathbb{SO}(3)$ matrices based on the world coordinate $\mathcal{W}$ in the simulator. Because the object pose $\mathcal{H}_i$ at each time step $i$ can be computed through simulations, the movement $\mathcal{M}$ can be calculated as the object pose variance $||\mathcal{H}_i - \mathcal{H}_{i-1}||_2^\mathcal{W}$ (Eq.\ref{eq:movement}). Consequently, we define the object stability $\mathcal{S}$ as the average of the accumulated movement across discrete time steps $L$ (Eq.\ref{eq:stability}) as follows:

\begin{equation}
\label{eq:movement}
\mathcal{M} = ||\mathcal{H}_i - \mathcal{H}_{i-1}||_2^\mathcal{W}, where \ \mathcal{H}=[\mathbf{R}|\mathbf{T}]
\end{equation}

\begin{equation}
\label{eq:stability}
\mathcal{S} = {\frac{1}{L}}\sum_{i=1}^L\mathcal{M}
\end{equation}

Considering that the pose $\mathcal{T}_\mathcal{O}$ of a rigid object in the grip of the robot is uncertain, 512 orientations were generated to estimate various poses for the object by dividing each roll, pitch, and yaw into eight grids. A rigid object $\mathcal{O}$ was set with a random pose in the normal direction of the table plane using a small value $\epsilon$. To search for all possible stable planes of the rigid object, we dropped the object on the table and sampled all poses at the moment that the object was stabilized (${\mathcal{S} < \delta_1}$). The sampled poses were then clustered using density-based spatial clustering of applications with a noise algorithm\cite{ester1996density} along the z-axis, which represents the normal vector of the contact of a stable plane with a horizontal surface. After rotating the object to align it along the direction of the clustered normal vector and gravity vector, the bottom 5\% of the regions along the z-axis (world coordinate) of the 3D object model were masked as areas that could sustain the stability of the object.


Because real environments cannot be perfectly simulated, certain planes cannot be easily generalized (e.g., a spherical model or sides of a cylinder), even if the sampled and clustered planes are reasonable in the simulation. Thus, each object was placed on a flat table with a normal vector of the plane candidates, and the table was tilted by 10 degrees. We estimated the object movement across each time step $L$ and eliminated the planes that did not satisfy ${\mathcal{S} < \delta_2}$. Thus, we verified that our stable plane annotations are robust for application to a horizontal surface; samples of the UOP-Sim dataset are shown in Figure \ref{fig:UOP-sim}. Furthermore, our dataset contained both explicit and implicit planes (e.g., a flat surface formed by four chair legs), and consequently, a total of 17,408 3D object models and 69,027 stable plane annotations were generated.

To respond appropriately to partial observations in the real world, we captured 3D object models from the UOP-Sim dataset using a synthesized depth camera with 1,000 random poses. The camera was initialized with the values of the Azure Kinect intrinsic parameters. A partial point cloud (50,000 points) was sampled using a voxel down sampling method\cite{Zhou2018} from the depth image. In the case of labeling, annotations from the UOP-Sim dataset were sampled after the poses of the partial point cloud and 3D object model were aligned. However, the stable regions of the partial point cloud may be significantly small compared to those of the full object model. For example, when a four-legged chair model has only two legs in a partial point cloud from a particular perspective, it is not reasonable to create a stable plane only using the two legs. To address this problem, we estimated the angular error from the normal vectors of each stable region and removed the annotations from the partial point cloud where the error exceeded 10 degrees (Fig.\ref{fig:UOP-net} (b)).


% real world, no fine tune 꼭 추가할 것
\subsection{Deep Neural Network for UOP} 
We formulated the detection of the most stable plane as a process of predicting stable planes in the visible regions of the observed object so that the object can be stably placed on a flat surface for real world applications. Furthermore, the process should detect planes that cover all possible ways to maintain an object in a stable configuration for various partial views. Segmenting the stable region of unseen objects is challenging because the model should learn to generalize both object shapes and plane properties. Thus, we propose a deep learning framework called UOP-Net that directly detects the most stable plane under partial observation, even when unseen objects are given. The  observed point cloud of the object, that the robot should place, is used as an input in the proposed approach.

Specifically, let us assume that a segmented object point cloud (partial point cloud) $\mathcal{\hat{X}} = \mathbb{R}^{N \times 3}$ is provided in the gripper. Our objective is to learn a function $\mathcal{F}: \mathcal{\hat{X}} \rightarrow \mathcal{A}$, where $ A$ is the most stable plane in a set of predicted planes $\{a_1,...,a_n\}$. Using $\mathcal{A}$, a rotation $\mathcal{R} \in \mathbb{SO}(3)$ was determined to place the object on a horizontal surface in a stable configuration.

% Furthermore, our goal includes detecting implicit planes (e.g., a tripod or four-legged chair), whereas previous works\cite{} that segment planes from point cloud can only detect explicit planes(e.g., building plane). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{ UOP-Net} 
Because there are various object shapes, and an object can have multiple stable planes, a function that generalizes the features of the stable planes is required. Therefore, we propose UOP-Net based on category-agnostic instance segmentation, which includes both the concept of object-ness and properties for stable planes from a large-scale synthetic dataset (UOP-Sim). 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Table 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% rotation(degree), translation(meter) 단위 체크, Translation * E-05
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
% \usepackage{dcolumn}
\begin{table*}[ht!]
\caption{{UOP} Performances of UOP-Net and baselines on three benchmark datasets (unseen object) in simulation.}
\centering
\label{tab:simulation-eval}
\resizebox{\textwidth}{!}{%
{\renewcommand{\arraystretch}{1.2}
\LARGE{
\begin{tabular}{|cccccccccccccc|}
\hline
% \multicolumn{14}{|c|}{\textbf{Stable Object Placement Evaluataion Metric}} \\ \hline
\multicolumn{2}{|c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}\\ Object Type \& \\ Dataset\end{tabular}}} & \multicolumn{8}{c|}{ Object Stability (S)} & \multicolumn{4}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Success Rate of \\ Object Placement (\mathcal{SR}, $\%$)\end{tabular}}} \\ \cline{3-10}
\multicolumn{2}{|c|}{} & \multicolumn{4}{c|}{Rotation (R, $^{\circ}$) \downarrow} & \multicolumn{4}{c|}{Translation (T, $cm$) \downarrow} & \multicolumn{4}{c|}{} \\ \cline{3-14} 
\multicolumn{2}{|c|}{} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}UOP \\ \textbf{(Ours)}\end{tabular}} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}UOP \\ \textbf{(Ours)}\end{tabular}} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \begin{tabular}[c]{@{}c@{}}UOP \\ \textbf{(Ours)}\end{tabular} \\ \hline
% Complete Point Cloud
\multicolumn{1}{|c|}{\multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Complete \\ Point cloud\end{tabular}}} & \multicolumn{1}{c|}{3DNet\cite{wohlkinger20123dnet}} % 3DNet
& 5.03 & 20.84 & 16.22 & \multicolumn{1}{c|}{\textbf{5.01}} % Rotation
& \textbf{0.40} & 2.09 & 1.48 & \multicolumn{1}{c|}{0.50}  % Translation
& \textbf{87.03} & 58.23  & 73.44  & 83.14  \\ % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{ShapeNet\cite{chang2015shapenet}} % ShapeNet
& \textbf{2.02} & 19.44 & 10.57 & \multicolumn{1}{c|}{2.60} % Rotation
& \textbf{0.16} & 2.41 & 1.15 & \multicolumn{1}{c|}{0.30} % Translation
& \textbf{95.17} & 54.26  & 81.51  & 88.75  \\ % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{YCB\cite{calli2015ycb}} % YCB
& 4.42 & 37.59 & 8.58 & \multicolumn{1}{c|}{\textbf{3.08}} % Rotation
& \textbf{0.35} & 5.96 & 0.87 & \multicolumn{1}{c|}{0.30} % Translation
& \textbf{89.27} & 49.89  & 83.84  & 87.84  \\ 
\hhline{|~|=|====|====|====|} % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Total avg.} % Total
& 4.26 & 24.43 & 13.27 & \multicolumn{1}{c|}{\textbf{4.06}} 
& \textbf{0.34} & 3.05 & 1.27 & \multicolumn{1}{c|}{0.41} 
& \textbf{89.25} & 55.47  & 77.54  & 85.41  \\ \hline % Success Rate
% Partial Point Cloud
\multicolumn{1}{|c|}{\multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Partial \\ Point cloud\end{tabular}}} & \multicolumn{1}{c|}{3DNet\cite{wohlkinger20123dnet}} % 3DNet
& 23.19 & 28.79 & 35.64 & \multicolumn{1}{c|}{\textbf{16.40}} % Rotation
& 2.48 & 3.17 & 3.74 & \multicolumn{1}{c|}{\textbf{1.91}}  % Translation
& 54.87 & 41.17 & 50.45 & \textbf{55.47}  \\ % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{ShapeNet\cite{chang2015shapenet}} % ShapeNet
& 21.97 & 28.70 & 25.80 & \multicolumn{1}{c|}{\textbf{14.90}} % Rotation
& 2.91 & 3.56 & 2.90 & \multicolumn{1}{c|}{\textbf{1.59}} % Translation
& 55.78 & 38.92 & 58.23 & \textbf{60.79}  \\ % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{YCB\cite{calli2015ycb}} % YCB
& 35.49 & 38.46 & 39.13 & \multicolumn{1}{c|}{\textbf{15.92}} % Rotation
& 5.73 & 5.98 & 5.19 & \multicolumn{1}{c|}{\textbf{2.37}} % Translation
& 48.14 & 40.22 & 52.66 & \textbf{62.53} \\ 
\hhline{|~|=|====|====|====|} % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Total avg.} % Total avg.
& 25.78 & 31.01 & 34.49 & \multicolumn{1}{c|}{\textbf{15.97}} % Rotation
& 3.32 & 3.90 & 3.90 & \multicolumn{1}{c|}{\textbf{1.95}} % Translation
& 53.50 & 40.48 & 52.59 & \textbf{58.22} \\ \hline % Success Rate

\end{tabular}
}
}
}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


As shown in Fig.\ref{fig:UOP-net}, UOP-Net employs the disordered graph convolutional neural network (DGCNN)\cite{wang2019dynamic} network to extract the global shape features before splitting into two different branches: one for semantic segmentation, which predicts the stability of a point, and one for embedding the instance features. Similar to the method in \cite{pham2019jsis3d, wang2019associatively}, instance labels (stable points) were determined by applying the mean-shift clustering algorithm\cite{comaniciu2002mean}. Given the predicted stable points, random sampling and consensus (RANSAC)\cite{fischler1981random} was used to fit a plane on the clustered points\cite{Zhou2018}. UOP-Net calculates the stability scores for each plane by element-wise multiplication of the semantic and predicted instance labels with the number of points composing each plane. UOP-Net produces only one plane with the highest score after fitting the planes and assigning stability scores based on the number of inlier points that constitute the planes. Finally, the rotation value $\mathcal{R}$ is determined by estimating the difference angle between the predicted normal vector of the stable plane and gravity vector (negative table surface normal).

% The UOP-Net, shown in Fig.\ref{fig:UOP-net}, takes a partial point cloud $\mathcal{\hat{X}} \in \mathbb{R}^{N \times 3}$ as the input and predicts the most stable plane $\mathcal{A}$. We implemented a point cloud instance segmentation scheme\cite{} and category-agnostic segmentation method\cite{pham2019jsis3d, wang2019associatively} to distinguish stable planes from unseen objects. While they\cite{} performed instance segmentation at the object level, our model operates at the plane level. 


% This is a challenging task because our goal is to cover all possible planes for various shape objects (unseen objects). Thus, a point cloud instance segmentation scheme\cite{pham2019jsis3d, wang2019associatively} and category-agnostic fashion\cite{} were employed to distinguish stable planes from unseen objects.

% To generalize the stable planes over various object shapes, we trained the UOP-Net via Sim2Real transfer. Using a large-scale synthetic dataset (UOP-Sim), UOP-Net learns to generalize the object shapes and detect plane instances. During the training process, the point-wise features($z$) size of 1024 for the randomly sampled partial point cloud $\mathcal{\hat{X}} = \mathbb{R}^{N \times 3}$ were embedded by the DGCNN\cite{} network, where $N=2048$. Then the features were fed into two fully connected (FC) layers to predict semantic logits and update instance features. The loss for semantic and instance segmentation is \ref{eq:the_sum_of_losses}

\begin{equation}
\label{eq:the_sum_of_losses}
\mathcal{L} = \lambda_1 * \mathcal{L}_{stable} + \lambda_2 * \mathcal{L}_{plane},
\end{equation}

\noindent where $\mathcal{L}_{stable}$ is the standard binary cross-entropy loss, and $\mathcal{L}_{plane}$ is the discriminative loss\cite{de2017semantic}. To allow the two losses to attain comparable values, the hyperparameter weights $\lambda_1 and \lambda_2$ were set to 10 and 1, respectively. 


\noindent\textbf{Training Details.} Each batch of training data for UOP-Net consisted of a rendered image of the object from a random view and stable plane annotations that were sampled using the verification process to ensure that the sampled planes are reasonable. During the training process, we randomly sampled 2,048 points for each object and utilized conventional augmentation (rotation, sheer, point-wise jittering, and Gaussian noise) for real-world application. In addition, we trained UOP-Net in the absence of real data. We employed Pytorch\cite{paszke2019pytorch} to train UOP-Net, and used one NVIDIA Titan RTX GPUs with a batch size of 32 and 1, 000 epochs. Early stopping was set with patience = 50. The Adam optimizer\cite{kingma2014adam} was used at a learning rate = 1e-3. 


\section{Simulations}
\noindent\textbf{Datasets.} 
% object (17,408 => train: 13926, val:3482 ) // anno (69,027 => train :55261, val:13766)
We generated 69k stable plane annotations for a total of 17.4k 3D objects using two benchmark datasets: 3DNet\cite{wohlkinger20123dnet} and ShapeNet\cite{chang2015shapenet}. We subsequently split the synthetic data into training and validation sets at a ratio of 8:2. The YCB\cite{calli2015ycb} object models were labeled in the simulation as well, but they were excluded from the training set so that they could be utilized as a test set in both the simulation and real-world experiments. We avoided using objects that were devoid of a stable plane, such as spherical objects. Consequently, the numbers of object categories used for each benchmark dataset were 152, 57, and 63, respectively. The training set had 13,926 objects and 55,261 annotations, while the validation set had 3,482 objects and 13,766 annotations. 

\noindent\textbf{Baselines.}
We compared the performance of our method with the following baselines:
\begin{itemize}
    \item \textbf{Convex hull stability analysis (CHSA)}\cite{haustein2019object, trimesh, hagelskjaer2019using}: This method computes the rotation matrix required for an object to rest in its stable pose on a planar surface. Given the object convex hull, the method samples the location of the center of mass and calculates the stable resting poses of the object on a flat surface. Then, the method evaluates the probabilities of landing in each pose and outputs the most probable pose. 
    
    \item \textbf{Bounding Box Fitting (BBF)}\cite{mitash2020task, Zhou2018}: This approach computes the oriented bounding box using principal component analysis (PCA), which minimizes the difference between the convex hull volume and the bounding box. Subsequently, the fitted object is placed with the largest area on a planar surface.
    
    
    \item \textbf{RANSAC Plane Fitting (RPF)}\cite{fischler1981random, Zhou2018}: This method, given a point cloud, segments planes that satisfy $ax + by + cz + d = 0$ for each point $(x, y, z)$. Then, the method iteratively samples several points at random to construct a random plane and determines the plane with the frequency with which the same plane is founded.
    
\end{itemize}

\noindent\textbf{Evaluation Metrics}
We used two metrics to qualitatively evaluate the efficacy of the UOP method, namely, \textit{object stability (\mathcal{S})} and \textit{ success rate of stable placement }. For an unseen object, we estimated its stability (\mathcal{S}) when the object was placed on a flat table using the predicted results. We need to verify whether our model allows the object to preserve a stable pose after we annotate labels and learn to reason the plane features. The other evaluation metric is the ratio of successful stable placements among all predicted planes with an accumulated rotation error of less than 10 degrees. When an object is placed in an unstable state, it falls after being exposed to any kind of vibrations. At this moment, the rotational motion is more common than the translational movement. Therefore, we only considered the rotational motion for evaluating the object stability. Object placement was attempted 100 times for each object, and if no planes were detected, we abandoned the object and regarded it as a failed case.

\begin{itemize}
    \item \textbf{Object Stability (\mathcal{S})}: This metric represents the amount of movement of the object during a discrete time step when it is placed on a horizontal surface on the predicted plane.
    
    \item \textbf{Success Rate of Object Placement (\mathcal{SR})}: The percentage of placements where the object stays stationary for a minute and the accumulated rotation is less than $10^{\circ}$.
\end{itemize}

% it is ambiguous to set an appropriate threshold where the translation of an object can occur due to external environmental factors (e.g., slipping due to friction force).
% rotation error는 평면의 고유 영향만을 고려할 수 있다~


\noindent\textbf{Comparison with Baselines}
For each 3D benchmark dataset (3DNet\cite{wohlkinger20123dnet}, ShapeNet\cite{chang2015shapenet} and YCB\cite{calli2015ycb}), we compared the performance of UOP-Net with other baselines for two object types (complete and partial shapes). We report the \textit{ object stability } and \textit{ success rate of placement } for different scenarios in Table \ref{tab:simulation-eval}. We can see that UOP-Net achieves a SOTA performance in terms of evaluation metrics from a single partial observation. We visualize the prediction result of each method and represent the object stability (\mathcal{S}) as a graph in Fig.\ref{fig:sim_exp_result}.
 

The CHSA, which generates an object mesh from the point cloud and calculates mathematically stable planes based on the center of mass, performs best when the object shape is fully discernible. Although our model does not outperform CHSA, it demonstrates that it can perform well on unseen objects and that the predicted planes allow the object to retain its stability. However, the CHSA cannot perform partial observations. Because the convex hull generated in the partial point cloud does not contain the information regarding the invisible portion of the object, sampling an accurate center of mass is impractical. As illustrated in Fig.\ref{fig:sim_exp_result}, the CHSA method usually places the object in the truncated plane such that the object does not remain stationary. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 4 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 4.
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure 4/Figure_4.png}
        \end{subfigure}
        
    \caption{Visualization of prediction results on YCB\cite{calli2015ycb} dataset for each method and object stability in simulation.}
    \label{fig:sim_exp_result}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The BBF method exhibited the worst performance. This is because it did not consider the geometric properties of the object and placed the largest plane in the bounding box. It can be performed when the object shape is similar to a primitive shape; however, it cannot operate in case of a complex shape. Although RANSAC plane fitting (RPF) is better than BBF, it also fails to detect a robust stable plane. Conversely, our method (UOP-Net) can detect the robust plane based on incomplete perception because we generate a partial point cloud using SOP-Sim and train the model to learn the geometric properties of objects and planes. Furthermore, even if the object shape is complex, our model can detect both the implicit and explicit planes.

\noindent\textbf{Failure Cases}
During the simulation experiments, we discovered several cases of failure. Although UOP-Net was trained with various object shapes, it could not detect stable planes of thin objects (e.g., spoons and knives) despite the existence of a predictable stable plane in case of such objects. Because the hyper-parameters of the mean-shift algorithm\cite{comaniciu2002mean} cannot be determined for all objects, UOP-Net may fail to cluster and place the object stably.


\section{Real World Application}
\noindent\textbf{Real Environment Setting.}
To demonstrate the feasibility of our proposed approach and the potential of Sim2Real transfer, we implemented our object placement method on a universal robot (UR5) manipulator using a single Azure Kinect RGB-D camera (Fig.\ref{fig:real_experiment} (b)). We used MAnet\cite{fan2020ma} with Densenet121\cite{iandola2014densenet} as the backbone to segment the target object and the gripper. We began by segmenting the visible region of the target object from the RGB image, and then cropped the depth image with the mask. The point cloud is then sampled from the depth image using voxel-down sampling\cite{Zhou2018}. For the model input, 2,048 points were randomly sampled and fed into UOP-Net. The most stable plane is predicted, and the rotation value between the plane and the table was calculated. The UR5 robot then placed the target object on the table. For planar motion, we utilized BiRRT\cite{qureshi2015intelligent} implemented with Pybullet\cite{coumans2021} and integrated with collision checking on a physics engine.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 5 %%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure 5/Figure_5.png}
        \end{subfigure}
        
    \caption{\textbf{Object placement using  UOP-Net}. (a) Given a segmented target object, UOP-Net takes 3D point clouds observed by a depth camera (Azure kinect) as input and predicts the most stable plane. (b) The UR5 robot places the target object on the table.}
    \label{fig:real_experiment}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Table 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Table 2
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[ht!]
\caption{\textbf{UOP} performances of UOP-Net and baselines on YCB\cite{calli2015ycb} in the real world.}
\label{tab:real world-eval}
\resizebox{\columnwidth}{!}{%
\LARGE{
{\renewcommand{\arraystretch}{}
\begin{tabular}{|c|cccc|}
\hline
YCB\cite{calli2015ycb} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \begin{tabular}[c]{@{}c@{}}UOP-Net \\ \textbf{(Ours)}\end{tabular} \\ \hline
Coffee Can & 0 / 10 & 0 / 10 & 4 / 10 & \textbf{10} / 10 \\
Timer & 0 / 10 & 1 / 10 & \textbf{6} / 10 & \textbf{6} / 10 \\
Power Drill & 0 / 10 & 1 / 10 & \textbf{5} / 10 & \textbf{5} / 10 \\
Wood Block & 1 / 10 & 1 / 10 & \textbf{10} / 10 & \textbf{10} / 10 \\
Metal Mug & 0 / 10 & 0 / 10 & 6 / 10 & \textbf{9} / 10 \\
Metal Bowl & \textbf{10} / 10 & 5 / 10 & \textbf{10} / 10 & \textbf{10} / 10 \\
Bleach Cleanser & 3 / 10 & 3 / 10 & \textbf{9} / 10 & \textbf{9} / 10 \\
Mustard Container & 2 / 10 & 0 / 10 & 5 / 10 & \textbf{10} / 10 \\
Airplane Toy & 0 / 10 & 3 / 10 & 0 / 10 & \textbf{4} / 10 \\
Sugar Box & 2 / 10 & 3 / 10 & \textbf{10} / 10 & \textbf{10} / 10 \\
Chips Can & 2 / 10 & 0 / 10 & 8 / 10 & \textbf{10} / 10 \\
Banana & 5 / 10 & 5 / 10 & \textbf{9} / 10 & \textbf{9} / 10 \\ \hhline{|=|=|=|=|=|}
Average & 2.1 / 10 & 1.8 / 10 & 6.8 / 10 & \textbf{8.5} / 10 \\ \hline
\end{tabular}%
}
}
}
\end{table}

\noindent\textbf{Evaluation Metrics}
Because object stability (\mathcal{S}) cannot be computed accurately in the real world, we only considered the \textit{success rate of placement}. However, when evaluating the success rate in the simulations, we set the position of the object on the table and determined that the placement was successful if the target object did not fall and remained stationary when the robot placed it on the table with the plane predicted by UOP-Net. In addition, if no planes could be detected, we considered the trial to be a failure. In total, we ran ten trials for each object.


\noindent\textbf{Comparison with Baselines}
We selected 12 objects from the YCB objects. Objects with spherical shapes(e.g., apples), dimensions that were too small, or low depth values were excluded from the test set. Table \ref{tab:real world-eval} shows that our method outperforms the other baselines in terms of the success rate across all objects. Even though real-world perception is noisy, UOP-Net provides a stable plane. This can be attributed to the fact that our model learned from a large number of partial point clouds that were captured by a depth camera and trained with noise. Other benchmarks (CHSA and primitive shape fitting) perform very poorly because they cannot obtain the full shape of an object in the real world and cannot respond to sensor noise. To further verify that our model can perform as efficiently in case of unseen objects, we experimented our method on novel objects that did not have a CAD model (a dinosaur product and an ice tray in Fig.\ref{fig:comparison_pred_res}). Although the object shape is complex, UOP-Net detects an implicit plane (e.g., four legs). 


\noindent\textbf{Failure Cases.}

Because we did not consider the robot gripper in the process of generating a partial point cloud in the simulation, UOP-Net sometimes fails to detect a stable plane when the target object is densely occluded by the robot gripper. Although we showed that UOP-Net can detect an implicit plane from complex object shapes (e.g., YCB airplane toy and dinosaur), the recall is not high. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 7 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 7. -> 6
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure 6/Figure_6.png}
        \end{subfigure}
        
    \caption{Visualization of prediction results for each method on YCB\cite{calli2015ycb} objects and novel objects in the real world.}
    \label{fig:comparison_pred_res}
\end{figure}





% object (17,408 => train: 13926, val:3482 ) // anno (69,027 => train :55261, val:13766)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Conclusion %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion and Future Work} 
In this paper we propose a method, UOP-Net, for detecting the stable planes of an object in the real world, including unseen objects. We first introduced the UOP-Sim pipeline for generating synthetic datasets, which comprise of 17.4k 3D objects and 69k stable plane annotations. Our model learns to detect stable planes directly from various objects under partial observation by using only a synthetic dataset (UOP-Sim). We demonstrated the accuracy and reliability of UOP-Net in detecting and generating stable planes from unseen and partially observable objects on three benchmark datasets with SOTA performance. 

The proposed method can be extended and improved in several ways. First, our model requires post-processing to cluster and fit the plane when segmenting each instance plane. Because the hyperparameters should be set for experimental environments, post-processing algorithms are heuristic. Thus, we hope to improve the model architecture to directly generate planes as an end-to-end pipeline. Second, our system only considers object placement on a horizontal surface, but we hope to extend and learn to predict a plane for object placement on other surrounding objects or workspaces so that the robot can pack and stack.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Acknowledgement %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgements}
{This work was fully supported by the Korea Institute for Advancement of Technology (KIAT) grant funded by the Korean Government (MOTIE) (Project Name: Shared autonomy based on deep reinforcement learning for responding intelligently to unfixed environments such as robotic assembly tasks, Project Number: 20008613). This work was also partially supported by the HPC Support Project of the Korean Ministry of Science, ICT, and NIPA.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{References Section}

\bibliography{references.bib}
\bibliographystyle{IEEEtran}

\section{Biography Section}

\vspace{11pt}
\bf{If you include a photo:}\vspace{-33pt}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Michael Shell}
Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
Use the author name as the 3rd argument followed by the biography text.
\end{IEEEbiography}

\vspace{11pt}

\bf{If you will not include a photo:}\vspace{-33pt}
\begin{IEEEbiographynophoto}{John Doe}
Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
\end{IEEEbiographynophoto}




\vfill



\end{document}
