%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{url}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{etoolbox}
\usepackage{placeins}
\usepackage{hyperref}
\usepackage{setspace} 
\usepackage{romannum}
\usepackage{wasysym}
\let\Square\relax
\usepackage{bbding}
\usepackage{pifont}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{array}
\usepackage{cite}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{algpseudocode}
\usepackage{hhline}

\title{\LARGE \
Learning to Place Unseen Objects Stably based on a Large-scale Simulation
}

\author{Sangjun Noh$^{*}$, Raeyoung Kang$^{*}$, Taewon Kim$^{*}$, Seunghyeok Back, Seongho Bak, Kyoobin Lee†% <-this % stops a space
\thanks{\text{*} These authors contributed equally to the paper.}
\thanks{All authors are with the School of Integrated Technology, Gwangju Institute of Science and Technology, Cheomdan-gwagiro 123, Buk-gu, Gwangju 61005, Republic of Korea. 
† Corresponding author: Kyoobin Lee {\tt\small kyoobinlee@gist.ac.kr}}%
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Abstract %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Unseen object placement (UOP) requires the robot reasoning about the object based on incomplete and noisy perception in the real world. Although previous works presented UOP methods, they were limited in that they required a complete object shape or a completion module to perform on a partial observation. We thus propose a method for directly detecting a plane from an unseen object, when a robot is only partially observable. Furthermore, we consider an object stability after placing on a planar workspace because the robot should place the object stably. We train our model from a large-scale simulation data to generalize object shape and the properties of stable plane from 3D point cloud. Evaluation is conducted in simulation and real world robot experiments. Our approach achieves state-of-the-art performance in both simulation and real world, where the target objects are partially visible. Our model is trained purely in simulation and works in the real world. Our codes, dataset and additional results are available at \url{https://sites.google.com/view/ailab-uop/home}.




\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Introduction %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
Picking and placing unseen objects is an essential skill for robots operating in an unstructured environment. Because not all objects in the environment can be modeled, this task requires robots to recognize objects and reason about their geometry and physical properties in order to execute. Furthermore, when it comes to objects (e.g., mechanical equipment, glass, plate, bowel) that need to be handled carefully in industrial fields or at household, robots should place the objects stably. Researchers have primarily focused on detecting grasps for unseen object\cite{mahler2017dex, mousavian20196}, but only a few studies\cite{gualtieri2018pick, mitash2020task, paxton2022predicting} for placement has been investigated. However, they have not dealt with stability that can maintain an object in a stable pose after placement.


Prior works for placing object stably have been presented, which is required for maintaining the full shape of 3D object models\cite{haustein2019object, gualtieri2021robotic, tournassoud1987regrasping}; alternatively, several studies were only focused on performing placement on task-specific objects (e.g., plates, cups) and supporting items (e.g., rack, pen holder, stemware holder) \cite{jiang2012learninga, jiang2012learningb}. However, relying on the availability of complete 3D models is a major drawback in real world scenarios where a robot physically interacts with objects based on partially observed visual data. 


% ---- 
% Fig 1.
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure 1/Figure_1.png}
        \end{subfigure}
        
    \caption{\textbf{Comparison of UOP-Net (Ours) and prior works.} Previous works required (a) full object model\cite{haustein2019object, trimesh, hagelskjaer2019using} or (b) completion module\cite{gualtieri2021robotic} or (c) only fitted primitive shape to the object\cite{fischler1981random, Zhou2018}. In contrast, (d) UOP-Net can reason to detect a stable plane for various objects from only a single partial view.} 
    \label{fig:task_comparison}
\end{figure}

% object (17,408 => train: 13926, val:3482 ) // anno (69,027 => train :55261, val:13766)
This work proposed an approach for \textit{Unseen Object Placement (UOP)} that detects a plane to place object stably, when either partial observation or unseen objects are given. To achieve this, we defined the stability for object placement by considering physical properties of objects. We generated a large-scale synthetic dataset using a physics simulator, called UOP-Sim, by dropping and sampling stable planes for various objects. While \cite{jiang2012learninga, jiang2012learningb} labeled the preferred placing configuration using a heuristic technique, we automatically annotated all possible planes where the object can sustain stable pose based on stability definition. Our dataset contains 17.4K 3D objects and total 69K annotations for stable planes. Furthermore, we propose a deep learning based network, named UOP-Net, that predicts the most stable plane to place an object from partial observation. We trained UOP-Net using only simulation dataset (UOP-Sim), with no fine-tuning and evaluated its performance on three benchmarks (3DNet, ShapeNet and YCB). We achieved the state-of-the-art (SOTA) performance on both simulation and real world environments, even single partial view and unseen objects are given. Our contributions are as follows:

\begin{itemize}
    \item{We propose a method for (\textbf\textit{UOP}) by detecting a plane that allows an object to sustain a stable pose even when only a single partial observation of the object or an unseen object is given.}
    
    \item{We provide a public large-scale 3D synthetic dataset, called UOP-Sim, containing a total of 69,027 stable annotations for 17,408 different objects.}

    \item{We trained UOP-Net using only synthetic dataset without fine-tuning and achieved SOTA performance in simulation and real world environment. UOP-Net can detect implicit planes as well as explicit planes}
    
\end{itemize} 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Related Works %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Works}
\noindent\textbf{Unseen Object Placement}
% unseen object grasping : dexnet2.0, graspnet, contact-graspent, ggcnn 등등
While grasping unseen object\cite{mahler2017dex, morrison2018closing, mousavian20196} have been proposed, there is little work on unseen object placement. As a pioneer work, Jiang et al.\cite{jiang2012learninga} trained a classifier using a hand-crafted dataset to identify placements that are likely to be stable and satisfy human preference. They validated the physical feasibility of placement and human preferences, but full observability was required, and dataset labels were heuristic. To overcome the limitations, Cheng et al.\cite{cheng2021learning} present a deep learning model by learning from a simulation, but task-specific objects were still required. Unlike these methods aim to place an object stably on various placing areas (e.g., rack, stemware holder), our majority is to learn to generalize a plane that can place an unseen rigid object stably on a horizontal surface, even only a single partial observation is given. 


\noindent\textbf{Stable Object Placement}
When the geometrical properties of an object are known, robots can perform well in terms of stable object placement. Analyzing the convex hull of the object with the center of mass and sampling the explicit planes where the object rests in a stable configuration can enable stable placement on a horizontal surface\cite{tournassoud1987regrasping, wan2019regrasp,lertkultanon2018certified, haustein2019object}. However, applying the analytical method in an real environment is difficult because it requires precise information about the object, whereas in the real world, only partial observation with noise is accessible (e.g., RGB-D camera). In \cite{gualtieri2021robotic}, though researchers utilized the deep learning-based completion method to address the uncertainty for partially visible objects in the real world, the completion module might not be sufficiently accurate to generate the precise full shape object. In contrast, our method can directly detect a stable plane from partial observation without any other modules.

\noindent\textbf{Robotic Applications based on Object Placement}
Recently, a wide range of object placement problems have been investigated so far, such as constrained placement\cite{mitash2020task}, upright placement\cite{newbury2021learning}, rearrangement\cite{wada2022reorientbot, paxton2022predicting} or packing\cite{wang2021dense}. Mitash et al.\cite{mitash2020task} utilized a multi-view based primitive shape fitting method to address the constrained placement problem in the absence of the object models. In \cite{paxton2022predicting}, a deep learning network that finds the required object rotation so that the object is stably placed in the upright orientation. Similarly, Li et al.\cite{listable} presents an approach to predict the rotation that will maintain the object with the height-maximizing pose stably.


% object (17,408 => train: 13926, val:3482 ) // anno (69,027 => train :55261, val:13766)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 2.
\begin{figure*}[ht!]
    \centering
        \includegraphics[width=\textwidth]{figures/Figure 2/Figure_2.png}
  \caption{\textbf{UOP-Sim dataset generation pipeline.} (Left) The database contains a total of 17.4K 3D object mesh models, and we built to place and estimate the object stability (\mathcal{S}). (Top) We drop each object on the table with 512 poses and sample stable planes from the objects that satisfy eq.\ref{eq:stability}. (Bottom) We verify the stable plane candidates with the tilted table. (Right) Over 17.5K 3D object models and 69K annotations of stable planes are included in the full dataset (UOP-Sim).}
    \label{fig:UOP-sim}
\end{figure*}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \noindent\textbf{Unseen Object Perception}
% To work in an industrial domain, a robot should be able to recognize and manipulate new objects that have never been seen before. Detecting unseen objects is a challenging task because modeling all objects in the real world is impractical and infeasible. Many object perception approaches\cite{xie2021unseen, back2021unseen, xie2020best, xiang2020learning} have been proposed to segment the visible parts of objects, although unseen objects and clutter scenes are given. In robotic manipulation, Grasp Quality Convolutional Neural Network (GQ-CNN)\cite{mahler2017dex} and six degrees-of-freedom (6DoF) GraspNet\cite{mousavian20196} are representative approaches for detecting grasp points from depth images and point clouds of unseen objects. These prior works used synthetic data to train category-agnostic object models to learn generalization of objects. In this work, we also trained our proposed method for UOP in a category-agnostic fashion with the objective of detecting regions for stably placing an unseen object from a 3D point cloud.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Problem Statement %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Statements}
We considered the problem of (\textbf{UOP}) from a single-view observation, where a robot picks up an unseen rigid object with the grasp points generated by the grasp generator (e.g., GQ-CNN\cite{mahler2017dex} or 6-DoF GraspNet\cite{mousavian20196}). 

%%%%%definitions
\subsection{Definitions}
\noindent\textbf{States}: The manipulating scene consists of a robot $\mathcal{R}$, an object $\mathcal{O}$, and a camera $\mathcal{C}$ with known intrinsic and pose $\mathcal{T}_\mathcal{C}$. The robot $\mathcal{R}$ grasps the target object $\mathcal{O}$ with the pose $\mathcal{T}_\mathcal{O}$. Thus, state $\mathcal{Z}$ can be denoted as $\{\mathcal{R}, \mathcal{O}, \mathcal{T}_\mathcal{O}, \mathcal{T}_\mathcal{C}\}$.


\noindent\textbf{Point Clouds}: Given the depth image captured by the depth camera with known intrinsic and extrinsic, we sample a set of 3D points $\{x_i | i = 1, ... ,n\}$, where each of the $n$ points is a vector of Euclidean coordinate $\mathbb{R}^3$. 


\noindent\textbf{Object Stability and Stable Planes}: Let $\mathcal{S}_\mathcal{T}^\mathcal{O}$ denote the placement stability of an object model $\mathcal {O}$ at the grasped pose $\mathcal{T}$. Though Jiang et al.\cite{jiang2012learninga} computed an object stability by kinetic energy in simulation, they only consider the initial and final state. Thus, we define \textit{Stability} $\mathcal{S}$ as the total amount of homogeneous transformation change at the world coordinate $\mathcal{W}$ in a simulator during a discrete time step $L$. Based on \textit{Stability} $\mathcal{S}$, stable planes were annotated for each object model, where satisfy $\mathcal{S}_\mathcal{T}^\mathcal{O} < \epsilon$.


\noindent\textbf{Dataset and Deep Learning Model}: The dataset $\mathcal{D} = \{({O}_m, \mathcal{A}_m) \}_{1}^{M}$ represents the set of M object models $\mathcal{O}$ (e.g., point clouds) and the stable placement plane $\mathcal{A}$ as the ground truth. $\mathcal{D}_{train}$ and $\mathcal{D}_{test}$ are the training and test datasets, respectively, and they satisfy the following state: $\mathcal{D}_{train} \cup \mathcal{D}_{test} = \mathcal{D}$. Let function $\mathcal{F}: \mathcal{X}_p \rightarrow \mathcal{A}$ be a deep learning model that takes one object as the input and produces a stable plane $\mathcal{A}$ as the output.

\noindent\textbf {Seen and Unseen Objects}: If $\mathcal{O}_{\mathcal{D}_{train}} \cap \mathcal{O}_{\mathcal{D}_{train}} = \emptyset$, $\mathcal{O}_{\mathcal{D}_{train}}$ is the seen object and $\mathcal{O}_{\mathcal{D}_{train}}$ is unseen for the deep learning model $\mathcal{F}$. 

%%%%%%%objectives
\subsection{Objectives} 
Our goal was to detect a stable plane for the stable placement of arbitrary objects using only single-view observations. Thus, we aimed to develop a neural network $\mathcal{F}: \mathcal{\hat{X}} \rightarrow \methcal{A}$ that minimizes the \textit{stability} $\mathcal{S}_\mathcal{T}^\mathcal{O}$ when partial point clouds $\mathcal{\hat{X}}$ are given.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Method %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Learning Unseen Object Placement}
Solving an UOP function that predicts a robust plane is challenging owing to several reasons. First, large-scale samples may be required to approximate the expectation over multiple possible objects. Another reason is that learning using a simple linear or mathematical model can be difficult as the relationship between the point cloud of objects and the annotated metrics is complex, considering both the geometry and physics. To address issues, we generated a synthetic dataset, called UOP-Sim, containing 17K 3D object models (point clouds), 69K labeled stable planes. Furthermore we developed UOP-Net, which learns to detect robust stable planes from UOP-Sim.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Synthetic Data Generation}
As there are a vast number of different types of object models, repeating the real-data acquisition and annotation process to collect a sufficient amount of dataset is time-consuming and impractical. Since 3D CAD models of novel objects are freely available online (e.g., 3D-Net, ShapeNet and YCB), a synthetic dataset can be generated in a virtual environment for a much less than real-data collection and labeling. Thus, We use a dynamic simulator to create a synthetic dataset for training and validation of UOP-Net, which can accelerate modeling cycles from data collection to deployment in accordance for various objects. Fig.\ref{fig:UOP-sim} illustrates our overall pipeline for synthetic dataset generation, which is made up of two processes. We begin by randomly dropping a rigid object on a horizontal surface to search for all possible stable plane candidates. Then, for each object, we annotate stable planes by placing them on the tilted table with the sampled candidates.


\noindent\textbf{Simulation Environment Setting.}
% object (17,408 => train: 13926, val:3482 ) // anno (69,027 => train :55261, val:13766)
We used the benchmark datasets, 3DNet\cite{wohlkinger20123dnet}, ShapeNet\cite{chang2015shapenet}, and Yale-CMU-Berkeley (YCB)\cite{calli2015ycb}, as a total of 17,408 3D object models. Using PyRep\cite{james2019pyrep} and CoppeliaSim\cite{rohmer2013v}, We created a learning environment to estimate the object stability during a discrete time step after the moment where the object mesh in contact with the table plane. In the virtual environment, we build 64 table object models to accelerate annotation process.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 3.
\begin{figure*}[ht!]
   \centering
      \includegraphics[width=\textwidth]{figures/Figure 3/Figure_3.png}

    \caption{\textbf{Overall pipeline of UOP}. (Center) As our objective is to detect a stable plane directly when the robot is only partially observable, we generate partial point clouds with the UOP-Sim dataset. The UOP-Net then learns to predict the most stable plane. (Left) When the target object in the robot gripper is observed by RGB-D camera, a partial point cloud is fed into the UOP-Net. (Right) Using the determined stable plane, the robot executes the target object placement for the angle difference between the normal vector of the plane and the negative gravity vector.}

  \label{fig:UOP-net}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 512가지 포즈 (along eight grids of roll, pitch, and yaw).
% randomly release / drop
% 멈췄을 때 물체의 자세
% plane의 normal을 z축 하단 을 기준으로 5% 가 되는 영역의 normal vector를 저장 => plane 만들때
% plane을 horizontal surface에 놓고 정지상태가 되었을 때 기울임 => stability를 측정
% time step 0.005s 각 time step 마다 물체의 포즈 


\noindent\textbf{Stable Plane Annotation.}
To estimate object stability $\mathcal{S}$ in rigid-body simulation, we first define object movement $\mathcal{M}$ using a homogeneous transformation matrix $\mathcal{H}_\mathcal{O} \in \mathbb{SE}(3)$, where the transformation matrix contains translation $\mathbf{T}_\mathcal{O}^\mathcal{W} \in \mathbb{R}^3$ and rotation $\mathbf{R}_\mathcal{O}^\mathcal{W} \in \mathbb{SO}(3)$ matrices based on the world coordinate $\mathcal{W}$ in the simulator. Since the object pose $\mathcal{H}_i$ at each time step $i$ can be computed by the simulation, the movement $\mathcal{M}$ can be calculated as object pose variance $||\mathcal{H}_i - \mathcal{H}_{i-1}||_2^\mathcal{W}$ (eq.\ref{eq:movement}). As a result, we define object stability $\mathcal{S}$ as the average of accumulated movement during the discrete time steps $L$ (eq.\ref{eq:stability}).

\begin{equation}
\label{eq:movement}
\mathcal{M} = ||\mathcal{H}_i - \mathcal{H}_{i-1}||_2^\mathcal{W}, where \ \mathcal{H}=[\mathbf{R}|\mathbf{T}]
\end{equation}

\begin{equation}
\label{eq:stability}
\mathcal{S} = {\frac{1}{L}}\sum_{i=1}^L\mathcal{M}
\end{equation}

Considering the pose $\mathcal{T}_\mathcal{O}$ of a rigid object in the robot gripper is uncertain, we generated 512 orientations to cover various poses for the object by dividing each roll, pitch, and yaw into 8 grids. A rigid object $\mathcal{O}$ was set with a random pose in the normal direction of the table plane by a small value $\epsilon$. To search all possible stable planes from the rigid object, we dropped the object on the table and sampled all poses at the moment that the object has stabilized (${\mathcal{S} < \delta_1}$). The sampled poses were then clustered using the Density-Based Spatial Clustering of Applications with Noise algorithm\cite{ester1996density} along the z-axis, which represents the normal vector of the contact of a stable plane with a horizontal surface. After rotating the object to align with the direction of the clustered normal vector and the gravity vector, the bottom 5\% regions along the Z-axis (world coordinate) of 3D object model were masked as the areas that can sustain the object stably.


As real environments cannot be perfectly simulated, there are planes that are not easily generalized (e.g., a spherical model or sides of a cylinder), even if the sampled and clustered planes are reasonable in simulation. Thus, each object was placed on the flat table with the normal vector of plane candidates and the table was tilted by 10 degrees. We estimate the object movement during the time steps $L$ again and remove the planes that did not satisfy ${\mathcal{S} < \delta_2}$. By using the verification process, our stable plane annotations are robust for a horizontal surface and samples of UOP-Sim dataset can be shown in Figure \ref{fig:UOP-sim}. Furthermore, our dataset contains not only explicit planes but also implicit planes (e.g., a flat surface formed by four chair legs), and as a result, a total of 17,408 3D object models and 69,027 stable plane annotations were generated.

To respond appropriately to partial observation in the real world, we captured 3D object models from the UOP-Sim dataset using a synthesized depth camera with 1,000 random poses, with the camera set to the same parameters as the Azure Kinect intrinsic parameters. A partial point cloud (50,000 points) was sampled using a voxel down sampling method\cite{Zhou2018} from the depth image. In the case of labeling, annotations from the UOP-Sim dataset were sampled after the poses of the partial point cloud and 3D object model were aligned. However, the stable regions of the partial point cloud might be quite small in comparison to the label of the full object model. For example, when a four-legged chair model has only two legs in a partial point cloud due to the perspective view, it is not reasonable to create a stable plane with the two legs. To address the problem, we estimated an angular error from the normal vectors of each stable regions and removed the annotations from the partial point cloud where the error exceeded 10 degrees (Fig.\ref{fig:UOP-net} (b)).


% real world, no fine tune 꼭 추가할 것
\subsection{Deep Neural Network for Unseen Object Placement} 
We formulate detecting the most stable plane as a process of predicting stable plane instances in the visible regions of observed object so that the object can be stably placed on a flat surface in the real world. Furthermore, the process should detect planes that cover all possible ways to keep an object in a stable configuration for various partial views. Segmenting the stable region on unseen objects is challenging because the model should learn to generalize both object shapes and plane properties. Thus, we propose a deep learning framework named, UOP-Net that directly detects the most stable plane under partial observation, even unseen objects are given. The input to our approach is an observed point cloud of the object the robot should place.

Specifically, let us assume that a segmented object point cloud (partial point cloud) $\mathcal{\hat{X}} = \mathbb{R}^{N \times 3}$ in gripper is provided. Our objective is to learn a function $\mathcal{F}: \mathcal{\hat{X}} \rightarrow \mathcal{A}$, where A is the most stable plane in a set of predicted planes $\{a_1,...,a_n\}$. Using $\mathcal{A}$, a rotation $\mathcal{R} \in \mathbb{SO}(3)$ is found to place object on a horizontal surface in a stable configuration.

% Furthermore, our goal includes detecting implicit planes (e.g., a tripod or four-legged chair), whereas previous works\cite{} that segment planes from point cloud can only detect explicit planes(e.g., building plane). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Unseen Object Placement Network (UOP-Net)} 
Since there are various object shapes and an object can have multiple stable planes, a function that generalizes the features of the stable planes is required. For this, we propose the UOP-Net via category-agnostic instance segmentation by learning both a concept of object-ness and properties for stable planes from large-scale synthetic dataset (UOP-Sim). 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Table 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% rotation(degree), translation(meter) 단위 체크, Translation * E-05
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
% \usepackage{dcolumn}
\begin{table*}[ht!]
\centering
\caption{\textbf{UOP} performances of UOP-Net and baselines on three benchmark datasets (unseen object) in simulation.}
\label{tab:simulation-eval}
\resizebox{\textwidth}{!}{%
{\renewcommand{\arraystretch}{1.2}
\LARGE{
\begin{tabular}{|cccccccccccccc|}
\hline
% \multicolumn{14}{|c|}{\textbf{Stable Object Placement Evaluataion Metric}} \\ \hline
\multicolumn{2}{|c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}\\ Object Type \& \\ Dataset\end{tabular}}} & \multicolumn{8}{c|}{ Object Stability (S)} & \multicolumn{4}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Success Rate of \\ Object Placement (\mathcal{SR}, $\%$)\end{tabular}}} \\ \cline{3-10}
\multicolumn{2}{|c|}{} & \multicolumn{4}{c|}{Rotation (R, $^{\circ}$) \downarrow} & \multicolumn{4}{c|}{Translation (T, $cm$) \downarrow} & \multicolumn{4}{c|}{} \\ \cline{3-14} 
\multicolumn{2}{|c|}{} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}UOP \\ \textbf{(Ours)}\end{tabular}} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}UOP \\ \textbf{(Ours)}\end{tabular}} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \begin{tabular}[c]{@{}c@{}}UOP \\ \textbf{(Ours)}\end{tabular} \\ \hline
% Complete Point Cloud
\multicolumn{1}{|c|}{\multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Complete \\ Point cloud\end{tabular}}} & \multicolumn{1}{c|}{3DNet\cite{wohlkinger20123dnet}} % 3DNet
& 5.03 & 20.84 & 16.22 & \multicolumn{1}{c|}{\textbf{5.01}} % Rotation
& \textbf{0.40} & 2.09 & 1.48 & \multicolumn{1}{c|}{0.50}  % Translation
& \textbf{87.03} & 58.23  & 73.44  & 83.14  \\ % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{ShapeNet\cite{chang2015shapenet}} % ShapeNet
& \textbf{2.02} & 19.44 & 10.57 & \multicolumn{1}{c|}{2.60} % Rotation
& \textbf{0.16} & 2.41 & 1.15 & \multicolumn{1}{c|}{0.30} % Translation
& \textbf{95.17} & 54.26  & 81.51  & 88.75  \\ % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{YCB\cite{calli2015ycb}} % YCB
& 4.42 & 37.59 & 8.58 & \multicolumn{1}{c|}{\textbf{3.08}} % Rotation
& \textbf{0.35} & 5.96 & 0.87 & \multicolumn{1}{c|}{0.30} % Translation
& \textbf{89.27} & 49.89  & 83.84  & 87.84  \\ 
\hhline{|~|=|====|====|====|} % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Total avg.} % Total
& 4.26 & 24.43 & 13.27 & \multicolumn{1}{c|}{\textbf{4.06}} 
& \textbf{0.34} & 3.05 & 1.27 & \multicolumn{1}{c|}{0.41} 
& \textbf{89.25} & 55.47  & 77.54  & 85.41  \\ \hline % Success Rate
% Partial Point Cloud
\multicolumn{1}{|c|}{\multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Partial \\ Point cloud\end{tabular}}} & \multicolumn{1}{c|}{3DNet\cite{wohlkinger20123dnet}} % 3DNet
& 23.19 & 28.79 & 35.64 & \multicolumn{1}{c|}{\textbf{16.40}} % Rotation
& 2.48 & 3.17 & 3.74 & \multicolumn{1}{c|}{\textbf{1.91}}  % Translation
& 54.87 & 41.17 & 50.45 & \textbf{55.47}  \\ % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{ShapeNet\cite{chang2015shapenet}} % ShapeNet
& 21.97 & 28.70 & 25.80 & \multicolumn{1}{c|}{\textbf{14.90}} % Rotation
& 2.91 & 3.56 & 2.90 & \multicolumn{1}{c|}{\textbf{1.59}} % Translation
& 55.78 & 38.92 & 58.23 & \textbf{60.79}  \\ % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{YCB\cite{calli2015ycb}} % YCB
& 35.49 & 38.46 & 39.13 & \multicolumn{1}{c|}{\textbf{15.92}} % Rotation
& 5.73 & 5.98 & 5.19 & \multicolumn{1}{c|}{\textbf{2.37}} % Translation
& 48.14 & 40.22 & 52.66 & \textbf{62.53} \\ 
\hhline{|~|=|====|====|====|} % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Total avg.} % Total avg.
& 25.78 & 31.01 & 34.49 & \multicolumn{1}{c|}{\textbf{15.97}} % Rotation
& 3.32 & 3.90 & 3.90 & \multicolumn{1}{c|}{\textbf{1.95}} % Translation
& 53.50 & 40.48 & 52.59 & \textbf{58.22} \\ \hline % Success Rate

\end{tabular}%
}
}
}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


As shown in Fig.\ref{fig:UOP-net}, the UOP-Net employed the DGCNN\cite{wang2019dynamic} network to extract the global shape features before splitting into two different branches: one for semantic segmentation, which predicts for whether the point is stable or unstable, and one for embedding instance features. Similar to \cite{pham2019jsis3d, wang2019associatively}, instance labels (stable points) are determined by applying the mean shift clustering algorithm\cite{comaniciu2002mean}. Given the predicted stable points, random sampling and consensus (RANSAC)\cite{fischler1981random} is operated to fit a plane on clustered points\cite{Zhou2018}. The UOP-Net calculates the stability scores for each plane by element-wise multiplication of the semantic logits and predicted instance labels with the number of points composing each plane. The UOP-Net produces only one plane with the highest score after fitting the planes and assinging stability scores based on the number of inlier points that constitute the planes. Finally, the rotation value $\mathcal{R}$ is determined by estimating difference angle between the predicted normal vector of the stable plane and the gravity vector (negative table surface normal).

% The UOP-Net, shown in Fig.\ref{fig:UOP-net}, takes as input a partial point cloud $\mathcal{\hat{X}} \in \mathbb{R}^{N \times 3}$ and predicts the most stable plane $\mathcal{A}$. We implemented point cloud instance segmentation scheme\cite{} and category-agnostic segmentation method\cite{pham2019jsis3d, wang2019associatively} to distinguish the stable planes from an unseen objects. While they\cite{} performs instance segmentation on object-level, our model does it at the plane-level. 


% It is challenging task because our goal is to cover all possible planes for various shape objects (unseen objects). For this, point cloud instance segmentation scheme\cite{pham2019jsis3d, wang2019associatively} and category-agnostic fashion\cite{} were employed to distinguish the stable planes from unseen object.

% To generalize the stable planes over various object shapes, we train the UOP-Net via Sim2Real transfer. Using large-scale synthetic dataset (UOP-Sim), UOP-Net learns to generalize object shapes as well as detect plane instances. During training process, the point-wise features($z$) size of 1024 for the randomly sampled partial point cloud $\mathcal{\hat{X}} = \mathbb{R}^{N \times 3}$ were embedded by DGCNN\cite{} network, where $N=2048$. Then the features are fed into two fully connected(FC) layers to predict semantic logits and update instance features. The loss for semantic and instance segmentation is \ref{eq:the_sum_of_losses}

\begin{equation}
\label{eq:the_sum_of_losses}
\mathcal{L} = \lambda_1 * \mathcal{L}_{stable} + \lambda_2 * \mathcal{L}_{plane},
\end{equation}

\noindent where $\mathcal{L}_{stable}$ is the standard binary cross-entropy loss and $\mathcal{L}_{plane}$ is the discriminative loss\cite{de2017semantic}. To allow the two loss values similar, the hyper-parameter weights $\lambda_1, \lambda_2$ are set to 10, 1 respectively. 


\noindent\textbf{Training Details.} Each batch of the training data for the UOP-Net consists of a rendering of the object from a random view and stable plane annotations that sampled using the verifying process to make sure the sampled planes are reasonable. During training process, we randomly sampled 2,048 points for each object and utilized conventional augmentation (rotation, sheer, point-wise jittering and Gaussian noise) to apply in the real world. Also, we trained the UOP-Net in the absence of the real data. We employed Pytorch\cite{paszke2019pytorch} to train UOP-Net and we used 1 NVIDIA Titan RTX GPUs with batch size = 32 and epoch = 1,000. With patience = 50, an early stopping is set. The Adam optimizer\cite{kingma2014adam} is used with the parameters of learning rate = 1e-3.  


\section{Experiments in Simulation}
\noindent\textbf{Datasets.} 
% object (17,408 => train: 13926, val:3482 ) // anno (69,027 => train :55261, val:13766)
We generated 69K stable plane annotations for a total of 17.4K 3D objects using two benchmark dataset; 3DNet\cite{wohlkinger20123dnet} and ShapeNet\cite{chang2015shapenet}. Then, we split the synthetic data with a 8:2 ratio for the training and validation sets. The YCB\cite{calli2015ycb} object models were labeled in simulation as well, but they were excluded for the training set so that they could be utilized as test set in both simulation and real world experiments. We avoided objects that did not contain a plane that could be placed stably such as spherical objects. As a result, the number of object categories used for each benchmark dataset is 152, 57, and 63, respectively, and the training set has 13,926 objects and 55,261 annotations, while the validation set has 3,482 objects and 13,766 annotations. 

\noindent\textbf{Baselines.}
We compare the performance our method and the following baselines:
\begin{itemize}
    \item \textbf{Convex Hull Stability Analysis (CHSA)}\cite{haustein2019object, trimesh, hagelskjaer2019using} : The method computes rotation matrix required for the object to rest in a stable pose on a planar workspace. Given the object convex hull, they samples the location of the center of mass and calculates the stable resting poses of the object on a flat surface. Then, they evaluates the probabilities of landing in each pose and returns the most probable pose. 
    
    \item \textbf{Bounding Box Fitting (BBF)}\cite{mitash2020task, Zhou2018} : The method computes the oriented bounding box using Principal Component Analysis (PCA), which minimizes the difference between the convex hull volume and the bounding box. Then, the fitted object is placed with the largest area on a planar workspace.
    
    
    \item \textbf{RANSAC Plane Fitting (RPF)}\cite{fischler1981random, Zhou2018} : The method, given a point cloud, segments planes that satisfy $ax + by + cz + d = 0$ for each point $(x, y, z)$. They iteratively sample several points at random to construct a random plane and determine the plane with the frequency with which the same plane is founded.
    
\end{itemize}

\noindent\textbf{Evaluation Metrics.}
We used two metrics to qualitatively evaluate unseen object placement methods; \textit{Object Stability (\mathcal{S})} and \textit{Success Rate of Stable Placement}. Given an unseen object, we estimated the object stability (\mathcal{S}) when an object was placed on a flat table with the predicted result. We need to verify whether our model allows the object preserves a stable pose after we annotated labels and learned to reason about plane features. The other evaluation metrics is the ratio of successful stable placements among all predicted planes with an accumulated rotation error of less than 10 degrees. When an object is placed with in an unstable state, it will fall after vibrating. At this moment, rotation movement is more common than translation movement. As a result, we only consider the rotation value for evaluating an object stability. Object placement was attempted 100 times for each object and if no planes were detected, we give up the current object and regard it as a failure case.

\begin{itemize}
    \item \textbf{Object Stability (\mathcal{S})} : This metric represents the amount of movement of the object during a discrete time step when it was placed on a horizontal surface with the predicted plane.
    
    \item \textbf{Success Rate of Object Placement (\mathcal{SR})} : The percentage of placements where the object stays stationary for a minute and the accumulated rotation is less than $10^{\circ}$.
\end{itemize}

% it is ambiguous to set an appropriate threshold where the translation of an object can occur due to external environmental factors (e.g., slipping due to friction force).
% rotation error는 평면의 고유 영향만을 고려할 수 있다~


\noindent\textbf{Comparison with Baselines.}
On each 3D benchmark dataset (3DNet\cite{wohlkinger20123dnet}, ShapeNet\cite{chang2015shapenet} and YCB\cite{calli2015ycb}), we compare performance to other baselines in two object types (complete shape and partial shape). We report \textit{Object Stability} and \textit{Success Rate of Placement} for different scenarios in Table \ref{tab:simulation-eval}. We can see that UOP-Net achieves the state-of-the-art (SOTA) performance in evaluation metrics from a single partial observation. We visualize each prediction result of each method and represent the object stability (\mathcal{S}) as a graph after it was placed in Fig.\ref{fig:sim_exp_result}.
 

The Convex Hull Stability Analysis (CHSA), which generates object mesh from the point cloud and calculates mathematical stable planes based on a center of mass, performs best when the object shape is complete. Though our model does not outperform the CHSA, it demonstrates that it can perform well enough on unseen objects and that predicted planes allow the object to be in a stable pose. The CHSA, however, cannot perform on partial observations. Because the convex hull generated in the partial point cloud does not contain the invisible part information for the object, sampling the accurate center of mass is impractical. As illustrate in Fig.\ref{fig:sim_exp_result}, the CHSA method usually place the object with the truncated plane so that the object cannot stay stationary. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 4 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 4.
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure 4/Figure_4.png}
        \end{subfigure}
        
    \caption{Visualization of prediction results on YCB\cite{calli2015ycb} dataset for each method and object stability in simulation.}
    \label{fig:sim_exp_result}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The Bounding Box Fitting (BBF) method performed the worst. Because they did not consider geometric properties and place with the largest plane in the bounding box. It can perform when the object shape is similar to a primitive shape, but it cannot operate when the object shape becomes complex. Though RANSAC Plane Fitting (RPF) is better to BBF, it also fails to detect the robust stable plane. On the other hand, our mothod (UOP-Net) can detect the robust plane based on incomplete perception because we generate partial point cloud using SOP-Sim and train the model to learn about the geometric properties of objects and planes. Furthermore, even if the object shape is complex, our model can detect implicit planes as well as explicit planes.

\noindent\textbf{Failure Cases.}
During simulation experiments, we discovered some failure cases. Though UOP-Net was trained with various object shapes, they could not detect stable planes from thin objects (e.g., spoons, knives) despite having a predictable stable plane. Since hyper-parameters of mean-shift algorithm\cite{comaniciu2002mean} cannot be determined for all objects, UOP-Net may fail to cluster and place the object stably.


\section{Experiments in the real world}
\noindent\textbf{Real Environment Setting.}
To demonstrate the feasibility of our proposed approach and the potential of Sim2Real transfer, we implemented our object placement method on a Universal Robot (UR5) manipulator with a single Azure kinect RGB-D camera (Fig.\ref{fig:real_experiment} (b)). We used MAnet\cite{fan2020ma} with Densenet121\cite{iandola2014densenet} as the backbone to segment a target object and gripper. We begin by segmenting the visible region of the target object from the RGB image, and then crop the depth image with the mask. The point cloud is then sampled from the depth image using the voxel down sampling\cite{Zhou2018}. For the model input, 2,048 points were randomly sampled and fed into the UOP-Net. The most stable plane is predicted and the rotation value between the plane and the table were calculated. Then, the UR5 robot place the target object on the table. For the motion planar, we utilized BiRRT\cite{qureshi2015intelligent} implemented with Pybullet\cite{coumans2021} integrating with the collision checking on the physics engine.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 5 %%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure 5/Figure_5.png}
        \end{subfigure}
        
    \caption{\textbf{Unseen object placement using UOP-Net}. (a) Given a segmented target object, UOP-Net takes 3D point clouds observed by a depth camera (Azure kinect) as input and predicts the most stable plane. (b) The UR5 robot places the target object on the table.}
    \label{fig:real_experiment}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Table 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Table 2
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[ht!]
\caption{\textbf{UOP} performances of UOP-Net and baselines on YCB\cite{calli2015ycb} in the real world.}
\label{tab:real world-eval}
\resizebox{\columnwidth}{!}{%
\LARGE{
{\renewcommand{\arraystretch}{}
\begin{tabular}{|c|cccc|}
\hline
YCB\cite{calli2015ycb} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \begin{tabular}[c]{@{}c@{}}UOP-Net \\ \textbf{(Ours)}\end{tabular} \\ \hline
Coffee Can & 0 / 10 & 0 / 10 & 4 / 10 & \textbf{10} / 10 \\
Timer & 0 / 10 & 1 / 10 & \textbf{6} / 10 & \textbf{6} / 10 \\
Power Drill & 0 / 10 & 1 / 10 & \textbf{5} / 10 & \textbf{5} / 10 \\
Wood Block & 1 / 10 & 1 / 10 & \textbf{10} / 10 & \textbf{10} / 10 \\
Metal Mug & 0 / 10 & 0 / 10 & 6 / 10 & \textbf{9} / 10 \\
Metal Bowl & \textbf{10} / 10 & 5 / 10 & \textbf{10} / 10 & \textbf{10} / 10 \\
Bleach Cleanser & 3 / 10 & 3 / 10 & \textbf{9} / 10 & \textbf{9} / 10 \\
Mustard Container & 2 / 10 & 0 / 10 & 5 / 10 & \textbf{10} / 10 \\
Airplane Toy & 0 / 10 & 3 / 10 & 0 / 10 & \textbf{4} / 10 \\
Sugar Box & 2 / 10 & 3 / 10 & \textbf{10} / 10 & \textbf{10} / 10 \\
Chips Can & 2 / 10 & 0 / 10 & 8 / 10 & \textbf{10} / 10 \\
Banana & 5 / 10 & 5 / 10 & \textbf{9} / 10 & \textbf{9} / 10 \\ \hhline{|=|=|=|=|=|}
Average & 2.1 / 10 & 1.8 / 10 & 6.8 / 10 & \textbf{8.5} / 10 \\ \hline
\end{tabular}%
}
}
}
\end{table}

\noindent\textbf{Evaluation Metrics.}
Since the object stability (\mathcal{S}) can not be computed accurately in the real world, we only considered the \textit{success rate of placement}. Whereas when evaluating success rate in simulation experiments, we set the position of the object on the table and determined that the placement was successful if the target object did not fall and remained stationary when the robot place it on the table with the predicted plane by the UOP-Net. Also, if no planes can be detected we consider the trial a failure. In total we ran 10 trials for each object.


\noindent\textbf{Comparison with Baselines.}
We choose a total of 12 objects from the YCB objects. Objects with spherical shapes(e.g., apple), too small dimensions, or low depth values were excluded from the test set. Table \ref{tab:real world-eval} shows that our method outperforms other baselines on success rate across all objects. Even though real world perception is noisy, UOP-Net perform the stable plane. One of the reasons is that our model was learned from large amount of partial point clouds that captured by depth camera and trained with noise. Other benchmarks (CHSA and Primitive Shape Fitting) perform very poorly because they cannot obtain the full shape of the object in the real world and cannot respond to sensor noise. To further verify that our model can perform on unseen objects, we experimented with novel objects that did not have a CAD model (a dinosaur product, and an ice tray in Fig.\ref{fig:comparison_pred_res}). Even though the object shape is complex, UOP-Net detect an implicit plane (e.g., four legs). 


\noindent\textbf{Failure Cases.}

Since we did not consider the robot gripper in the process of generating partial point cloud in simulation, UOP-Net sometimes fails to detect the stable plane when the target object is densely occluded by the robot gripper. Though we showed that UOP-Net can detect an implicit plane from complex object shapes (e.g., YCB airplane toy, dinosaur), the recall is not high. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 7 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 7. -> 6
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure 6/Figure_6.png}
        \end{subfigure}
        
    \caption{Visualization of prediction results for each method on YCB\cite{calli2015ycb} objects and novel objects in the real world.}
    \label{fig:comparison_pred_res}
\end{figure}





% object (17,408 => train: 13926, val:3482 ) // anno (69,027 => train :55261, val:13766)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Conclusion %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion and Future Work} 
This paper proposed UOP-Net, a method for detecting a stable plane in the real world, including unseen objects. We first introduced the UOP-Sim pipeline for generating synthetic datasets, which contains 17.4K 3D objects and 69K stable plane annotations. Our model learns to detect stable planes directly from various objects under partial observation using only synthetic dataset (UOP-Sim). On three benchmark datasets, we demonstrated that UOP-Net could detect and reason stable planes from unseen and partially observable objects with SOTA performance. We showed a real robot demo and demonstrated that it could function in a real-world environment. 

In several ways, our method can be extended and improved. First, our model required post-processing to cluster and fit the plane when segmenting each instance plane. Since the hyper parameters should be set for experiment environments, post process algorithms were heuristic. Thus, we hope to improve the model architecture to generate planes directly as an end-to-end pipeline. Second, our system only considers object placement on a horizontal surface, but we hope to extend and learn to predict a plane for object placement on other surrounding objects or workspace so that the robot can pack or stack.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Acknowledgement %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgement}
\begin{spacing}{0.4}
{\scriptsize This work was fully supported by the Korea Institute for Advancement of Technology (KIAT) grant funded by the Korea Government (MOTIE) (Project Name: Shared autonomy based on deep reinforcement learning for responding intelligently to unfixed environments such as robotic assembly tasks, Project Number: 20008613). This work was also partially supported by the HPC Support project of the Korea Ministry of Science and ICT and NIPA.}
\end{spacing}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{references.bib}
\bibliographystyle{IEEEtran}

\end{document}