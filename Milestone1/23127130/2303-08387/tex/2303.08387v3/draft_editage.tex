%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{url}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{etoolbox}
\usepackage{placeins}
\usepackage{hyperref}
\usepackage{setspace} 
\usepackage{romannum}
\usepackage{wasysym}
\let\Square\relax
\usepackage{bbding}
\usepackage{pifont}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{array}
\usepackage{cite}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}\usepackage{algpseudocode}
\usepackage{hhline}


\begin{document}

\title{\LARGE \
Learning to Place Unseen Objects Stably Using Large-scale Simulations
}



\author{Sangjun Noh$^{*}$, Raeyoung Kang$^{*}$, Taewon Kim$^{*}$, Seunghyeok Back, Seongho Bak, Kyoobin Lee†% <-this % stops a space
\thanks{\text{*} These authors contributed equally to the study.}
\thanks{All authors are with the School of Integrated Technology, Gwangju Institute of Science and Technology, Cheomdan-gwagiro 123, Buk-gu, Gwangju 61005, Republic of Korea. 
† Corresponding author: Kyoobin Lee {\tt\small kyoobinlee@gist.ac.kr}}%
}


\maketitle
% \thispagestyle{empty}
% \pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Abstract %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Object placement is a fundamental task for robots, yet it remains challenging for partially observed objects. Existing methods for object placement have limitations, such as the requirement for a complete three-dimensional model of the object or the inability to handle complex shapes and novel objects that restrict the applicability of robots in the real world. Herein, we focus on addressing the \textbf{U}nseen \textbf{O}bject \textbf{P}lacement (\textbf{UOP}) problem. We tackled the UOP problem using two methods: (1) UOP-Sim, a large-scale dataset used to accommodate various shapes and novel objects, and (2) UOP-Net, a point-cloud segmentation-based approach that directly detects the most stable plane from partial point clouds. Our UOP approach enables robots to place objects stably, even when the object's shape and properties are not fully known, thus providing a promising solution for object placement in various environments. We verify our approach using simulations and real-world robot experiments, thus demonstrating state-of-the-art performance for single-view and partial-object placements. Robot demonstrations, codes, and datasets are available at \href{https://gistailab.github.io/uop/}{https://gistailab.github.io/uop/}.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Introduction %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction} 
Robots need to have the ability to manipulate unseen objects to operate effectively in various environments to execute various tasks, including (among others) manufacturing, construction, and household. While deep learning has progressed and improved its capability to recognize and handle unseen objects, most of the current research focuses on identifying \cite{xie2021unseen, back2022unseen} or grasping \cite{mahler2017dex, mousavian20196} them. However, it is important to note that when a robot picks up an object from a cluttered container or receives it from a human, the robot must be able to place the object stably. Thus, this study addresses the \textbf{U}nseen \textbf{O}bject \textbf{P}lacement (\textbf{UOP}) problem, which involves the stable placement of novel objects in a real-world environment. 


% Robots need to have the ability to manipulate unseen objects to operate effectively in various environments, which are common in manufacturing, construction, and household tasks. Although deep learning has been applied to recognize and manipulate unseen objects, most existing researches focus on identifying\cite{xie2021unseen, back2022unseen} or grasping\cite{mahler2017dex, mousavian20196} objects. This study proposes a new task called unseen object placement (UOP), which addresses the problem of placing unseen objects in the real world.


% -------------------------------------------------------------------------------------------------------------------------------------------
\begin{figure}[ht]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure 1/Figure_1.png}
        \end{subfigure}
        
    \caption{\textbf{Comparison of the Unseen Object Placement network (UOP-Net) (ours) and previous methods.} Previous studies for object placement used (a) full-shape object models \cite{haustein2019object, trimesh, hagelskjaer2019using}, (b) completion modules \cite{gualtieri2021robotic}, or (c) fitted primitive shapes \cite{fischler1981random, Zhou2018}. In contrast, (d) the proposed UOP-Net directly detects stable planes for unseen objects from partial observations.
}    
    \label{fig:task_comparison}
\end{figure}

% -----------------------------------------------------------------------------------------------------------

Conventional approaches \cite{haustein2019object, trimesh, hagelskjaer2019using} for stable object placement require full three-dimensional (3D) models and analytical calculations. These methods involve sampling stable planes after calculating the center of mass of the object, which is not feasible for all real-world objects that may be encountered. One approach \cite{gualtieri2021robotic} combines analytical methods with a 3D object completion model that can reconstruct the full shape of an object from raw perception data. However, using this approach is difficult as the predicted point cloud may not be precise, thus resulting in inaccuracies in its effort to determine stable planes. Our UOP method addresses these limitations by directly detecting stable planes of unseen objects from single views and partial-point clouds, thus eliminating the need for a full 3D object model. This enables the robot to stably place the object even when the shape and properties of the object are not fully known.

In this study, we propose a method for UOP that detects stable planes from complex shapes and novel objects. To achieve this, we generated a large-scale synthetic dataset referred to as UOP-Sim, which contains various 3D objects and annotations of stable planes generated using a physics simulator. Unlike previous approaches \cite{jiang2012learninga, jiang2012learningb} that rely on heuristics to label the preferred placement configurations, we automatically annotate all feasible planes that can support stable object poses. Our dataset includes 17.4 K objects and 69 K annotations. We propose a point-cloud, instance-segmentation-based network referred to as UOP-Net that predicts stable planes from the partial point cloud and trains it using only the UOP-Sim dataset. We compare the performance of our approach with three baselines and learning-based methods. We demonstrate that it achieves state-of-the-art (SOTA) performance in both the simulation and real-world experiments without any fine-tuning on real-world data.
% (comment 반영한거) In this paper, we propose two methods for UOP that detects stable planes from complex shape and novel objects. First, we generated a large-scale synthetic dataset called UOP-Sim, which contains various 3D objects and annotations of stable planes generated using a physics simulator. Unlike previous approaches \cite{jiang2012learninga, jiang2012learningb} that rely on heuristics to label the preferred placement configurations, we automatically annotate all feasible planes that can support stable object poses. Our dataset includes 17.4 K objects and a total of 69 K annotations. Second we propose a point cloud instance segmentation based network called UOP-Net that predicts stable planes from partial point-cloud and trains it using only the UOP-Sim dataset. To verify our methods we compare the performance of our approach with three baseline methods, learning based methods and demonstrate that it achieves state-of-the-art (SOTA) performance in both the simulation and real-world experiments without any fine tuning on real-world data.



The main contributions of this study are as follows:
% object (17,408 => train: 13926, val:3482 ) // anno (69,027 => train :55261, val:13766)
\begin{itemize}    
    \item{We propose a new task called UOP to place an unseen object stably from single views and partial point clouds}
    
    \item{We provide a public, large-scale, 3D synthetic dataset 
    called UOP-Sim that contains a total of 69027 annotations of stable planes for 17408 different objects}
 
    \item{We introduce a point-cloud instance, segmentation-based network named UOP-Net that predicts stable planes for partially observed unseen objects}

    \item{We compare the performance of our approach with previous object placement methods and confirm that our method outperforms the SOTA methods without any fine-tuning in real-world environments}

  
\end{itemize} 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Related Works %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Works}
\noindent\textbf{Stable object placement.} Previous studies\cite{tournassoud1987regrasping, wan2019regrasp,lertkultanon2018certified, haustein2019object} demonstrated that robots can stably place an object with known geometrical properties by analyzing the convex hull and sampling stable planes of the object. However, this approach requires precise object priors (e.g., CAD, mass), and it may not be available in real-world scenarios with partial observations (e.g., from a red, green, red, depth (RGB-D) camera). Several researchers attempted to address this limitation with deep-learning-based methods that predict the invisible parts of an object \cite{gualtieri2021robotic}; however, these approaches have limitations in generating the precise shapes of unseen objects. Our UOP method addresses these limitations by directly detecting stable planes from partial observations without the need for complete 3D object models. Unlike previous methods, the UOP method is more generalizable and adaptable to real-world scenarios with partial observations.

\noindent\textbf{UOP.} Previous studies on unseen object placement focused on the identification of stable placements that satisfy human preferences. For example, Jiang et al. \cite{jiang2012learninga} trained a classifier using a hand-crafted dataset to identify these placements; this approach relies on heuristic labels and requires complete observability. Cheng et al. \cite{cheng2021learning} proposed a deep-learning model based on simulations to address the issue of heuristic labels; however, this approach was limited to task-specific objects. Another common approach \cite{mitash2020task} for placing unseen objects is using bounding box fitting to determine the shape and orientation of the object. This method can be fast and effective; however, it ignores the geometry of the object and relies only on its bounding box. Although this approach can be applied to unseen objects, it may not stably place objects in all situations; therefore, it may be less effective than methods that consider the geometry of the object. In contrast, our approach can stably place unseen objects on a horizontal surface using only a single partial observation. Our method can handle a broad range of objects instead of being limited to specific object types.

\noindent\textbf{Robotic applications of object placements.} Prior studies on object placement for robotic applications focused on solving specific tasks, such as constrained placement \cite{mitash2020task}, upright placement \cite{newbury2021learning, pang2022upright}, and rearrangement \cite{wada2022reorientbot, paxton2022predicting}. However, these methods have several limitations. For example, the approach proposed by Mitash et al. \cite{mitash2020task} relies on multiview shape fitting and requires access to object models that may not be available in some scenarios. The deep-learning approach proposed in \cite{paxton2022predicting} is limited to the determination of the required rotation for stable placements of objects in an upright orientation. Li et al. \cite{li2022stable} proposed a method that can only predict rotations that maintain objects in positions that maximize their heights; these limitations restrict the applicability and potential of these methods for more general object placement tasks, such as stacking and packing. In contrast, our approach addresses the fundamental problem of placing unseen objects on a horizontal surface and has the potential to be applied to a broader range of robotic applications.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 2.
\begin{figure*}[ht!]
    \centering
        \includegraphics[width=\textwidth]{figures/Figure 2/Figure_2.png}
  \caption{\textbf{UOP-Sim dataset generation pipeline.} The UOP-Sim dataset is a large-scale synthetic dataset that contains three-dimensional (3D) object models (17.4 K) and annotations of stable planes (69 K). The dataset is generated by dropping each object on a table in 512 different configurations and by sampling stable planes that satisfy Eq.\ref{eq:stability}. The stable plane candidates are verified using a tilted table. }
    \label{fig:UOP-sim}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Problem Statement %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Problem Statements}

%%%%%Assumptions
\subsection{Assumptions}
% A robot arm equipped with a parallel-jaw gripper operates in a workspace with a planar table, assuming that the camera pose is known. The manipulating scene begins with the robot grasping an object. The scene is captured by a single-view RGB-D camera, and the partial point cloud of the object is obtained from the depth image. The point cloud is fed into the model, and the most stable plane is predicted.

The suggested approach is used when a robot must retrieve unseen objects that are mixed up in a container, or when a person hands a novel object to a robot and the robot does not know its correct orientation. The camera pose is assumed to be known in the workspace. The robot begins by grasping an object and capturing the scene using a single-view RGB-D camera. The resulting partial point cloud of the object is fed into the model to predict the most stable plane.

%%%%%definitions
\subsection{Definitions}

\noindent\textbf{Point cloud}: Let ${X} \in \mathbb{R}^{N \times 3}$ be point clouds obtained by capturing the manipulating scene in which the robot grasps the object from the camera. 

%-----orignal \noindent\textbf{Object instability and stable planes}: Let $\mathcal{U}$ denote the instability of an object model. We define instability $\mathcal{U}_{i}$ as the average of homogeneous transformation change in world coordinates in a simulator over a discrete time step $L$ at time step ${i}$. Stable planes $\mathcal{S}$ are annotated for each object model at time step ${i}$ that first time satisfies the condition $\mathcal{U}_i < \epsilon$. A stable plane $s \in \mathcal{S}$ is represented as normal vector $\vec{V}\in \mathbb{R}^3$, and threshold $\epsilon$ indicates that the object has reached a stable state. 
\noindent\textbf{Object instability and stable planes}: Let $\mathcal{U}$ denote the instability of an object model. We define the instability of an object model as the average of movements in a simulator over a discrete time step $L$. Stable planes $\mathcal{S}$ are annotated for each object model so that the condition $\mathcal{U} < \epsilon$ is satisfied. A stable plane $s \in \mathcal{S}$ is represented as a normal vector $\vec{V}\in \mathbb{R}^3$, and threshold $\epsilon$ indicates that the object has stopped. 

\noindent\textbf{Dataset and deep learning model}: The dataset $\mathcal{D} = \{(\mathcal{O}, \mathcal{S})_n\}_{1}^{N}$ represents the $N$ set of object models $\mathcal{O}$ and corresponding stable planes $\mathcal{S}$ as the annotations. The function $\mathcal{F}: {X} \rightarrow s$ denotes a deep-learning-based model that considers point clouds ${X}$ as the input and produces the most stable plane $s$ as the output.

% \noindent\textbf{Dataset and deep learning model}: The dataset $\mathcal{D} = \{(\mathcal{O}, \mathcal{A})_n\}_{1}^{N}$ represents the $N$ set of object models $\mathcal{O}$ and corresponding stable planes $\mathcal{A}$ as the annotations, where $\mathcal{S} \subset \mathcal{A}$. The function $\mathcal{F}: {X} \rightarrow \mathcal{S}$ denotes a deep learning-based model that considers point clouds ${X}$ as the input and produces the most stable plane $\mathcal{S}$ as the output.

\noindent\textbf {Seen and unseen objects}: The set of object models used for training and testing the function $\mathcal{F}$ are denoted as $\mathcal{O}_{train}$, $\mathcal{O}_{test}$. If $\mathcal{O}_{train} \cap \mathcal{O}_{test} = \emptyset$, then objects $\mathcal{O}_{train}$ are considered seen objects while objects $\mathcal{O}_{test}$ are unseen objects for the model $\mathcal{F}$. 

%%%%%%%objectives
\subsection{Objectives} 
Our objective is to detect the most stable plane for placing unseen objects from a single-view observation. We aim to develop a function $\mathcal{F}: {X} \rightarrow s$ that minimizes the instability of the object $\mathcal{U}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Method %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Learning for Unseen Object Placements}
We address the challenges of the UOP task, which is difficult to address because of a) the need for a large-scale dataset to approximate stable planes and b) the complexity of the relationship between point clouds and annotated planes. We present a novel approach by introducing the UOP-Sim dataset to mitigate these challenges; this dataset includes 17 K 3D object models and 69 K labeled stable planes, and a UOP-Net neural network that can detect robust stable planes from partial point clouds. We propose a general and adaptable approach to the UOP task using these tools that enables robots to place accurately unseen objects in real-world scenarios.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{UOP-Sim Dataset Generation}
% \noindent\textbf{Overview.}
% The process for generating the dataset, as illustrated in Fig. \ref{fig:UOP-sim}, involved randomly dropping a rigid object on a horizontal surface to identify stable plane candidates and then annotate these stable planes by placing them on a tilted table. This automated approach allows us to efficiently generate a large-scale synthetic data for training and evaluating the model, and it requires no human intervention.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 3.
\begin{figure*}[ht!]
   \centering
      \includegraphics[width=\textwidth]{figures/Figure 3/Figure_3.png}

    \caption{\textbf{Overall pipeline of UOP method}. UOP detects the most stable plane directly from single views and partial-point clouds. The UOP-Net is trained on the UOP-Sim dataset and takes in a partial point cloud to predict the stable plane. The estimated (stable) plane is used to execute object placement based on the angle difference between the normal vector of the plane and the negative gravity vector.
    }


  \label{fig:UOP-net}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Stable Plane Annotation.}
We defined the movement of the object at time step i as $\mathcal{M}_{i}$ in terms of its translation and rotation change in the world coordinates to evaluate the instability of the pose of the object in dynamic simulations. The pose can be represented as $\mathbf{H} =[\mathbf{R}|\mathbf{T}] \in \mathbb{SE}(3)$, where $\mathbf{R},\mathbf{T}$ are the rotation and translation matrices, respectively. We tracked the pose of the object at each time step and calculated the difference between the consecutive poses (Eq.\ref{eq:movement}). Subsequently, we estimated the average of these differences over a certain period ${L}$ to estimate the instability of the object at a time step ${i}$ (Eq.\ref{eq:stability}). To ensure robust annotation, we consider a range of discrete time steps ${L}$ instead of using only a single time step ${i}$.

\begin{equation}
\label{eq:movement}
\mathcal{M}_i = ||\mathbf{H}_i- \mathbf{H}_{i-1}||_2
\end{equation}

\begin{equation}
\label{eq:stability}
\mathcal{U}_{i}=
\begin{cases}
{\frac{1}{L}}\sum_{j=i-L+1}^i\mathcal{M}_{j}, & \mbox{if } i >= L \\
{\frac{1}{i}}\sum_{j=1}^i\mathcal{M}_{j}, & \mbox{otherwise } 
\end{cases}
\end{equation}


We generated 512 orientations by dividing the roll, pitch, and yaw into eight intervals to explore a broad range of possible poses for the object. The object was then placed on a table with a random pose along the normal direction of the table. We dropped the object on the table and recorded all poses in which it remained stable (${\mathcal{U}_{i} < \epsilon_1}$) to identify stable planes that support the object.

We then used the density-based spatial clustering noise algorithm \cite{ester1996density} to cluster the sampled poses. This allowed us to identify stable planes by clustering the poses along the z-axis; this represents the normal vector at the contact points of a stable plane with a horizontal surface. Subsequently, we masked the lower 5\% of the height (along the surface normal of the table) to indicate areas that could support the stability of the object.

Specific planes may not be easily generalized because real-world environments cannot be perfectly simulated. This can be a problem for spherical model planes or the sides of a cylinder. We placed each object on a flat table and tilted the table by 10° to address this. We then estimated the movement of the object across each time step and eliminated any planes that did not satisfy the condition $\mathcal{U}$ less than $\epsilon_2$. This allowed us to label stable planes that were robust for applications to horizontal surfaces, as shown in the samples of the UOP-Sim dataset in Fig. \ref{fig:UOP-sim}. The UOP-Sim contained a total of 17408 3D object models and 69027 stable plane annotations. Furthermore, our dataset contains both explicit and implicit planes, such as a flat surface formed by four chair legs. Supplementary Fig. 1 contains additional sample images from UOP-Sim (YCB objects).

% We captured 3D object models in the UOP-Sim dataset using a depth camera with 1,000 random poses to respond appropriately to partial observations in the real world. A partial point cloud was then sampled from the depth image using a voxel downsampling method. To label the stable planes in partial point clouds, we aligned them with annotated 3D object models and sampled their overlapping regions, which might be smaller than the full-shape object annotations. To ensure reliability, we estimated the angular error by comparing the normal vectors of the overlapped regions and the original annotations and filtered out stable planes with errors greater than 10°. This filtering process significantly improves the accuracy of our labeling method, making it suitable for partial observations in real-world scenarios (Fig.\ref{fig:UOP-sim}: Training Data Generation using UOP-Sim).


% In the UOP-Sim dataset, we employed a depth camera for capturing 3D models with 1,000 random poses, addressing partial real-world observations. From this, we extracted partial point clouds via voxel downsampling. To label stable planes, we aligned these clouds with annotated 3D models and identified smaller overlapping regions. By comparing normal vectors, we filtered out planes with errors over 10°, enhancing labeling accuracy for real-world partial observations (Fig.\ref{fig:UOP-sim}).


\noindent\textbf{Simulation Environment Setting}
We used PyRep \cite{james2019pyrep} and CoppeliaSim \cite{rohmer2013v} to construct a simulation environment to compute object instability ($\mathcal{U}$). We employed the bullet engine as the physics engine. Further, we used 3D object models from three benchmark datasets (3DNet\cite{wohlkinger20123dnet}, ShapeNet\cite{chang2015shapenet}, and YCB \cite{calli2015ycb}), which yielded a total of 17408 models. We built 64 table models in the simulation environment to facilitate the annotation process.


% real world, no fine tune 꼭 추가할 것
\subsection{UOP-Net} 
% \noindent\textbf{Overview.}
% We propose a learning based framework called UOP-Net to detect the most stable plane for placing unseen objects on a flat surface. This framework is designed to predict stable planes from the visible regions of an observed object, and this allows us to detect planes that cover all possible ways of maintaining stability based on various partial views. The model needs to learn to generalize both object shapes and plane properties, and therefore, it can be difficult to segment the stable regions of unseen objects. UOP-Net addresses this challenge by directly detecting the most stable plane from partial observations, even when objects are unseen. 
% The input of the proposed approach is the observed point cloud of the target object. Given a partial point cloud ${X} = \mathbb{R}^{N \times 3}$, the goal is to learn a function $\mathcal{F}: {X} \rightarrow \mathcal{S}$. A rotation $\mathbf{R} \in \mathbb{SO}(3)$ can be determined for stably placing the object on a horizontal surface using $s$.

\noindent\textbf{Network Architecture.} 
The UOP-Net is based on the dynamic graph convolutional neural network \cite{wang2019dynamic} architecture and JSIS3D \cite{pham2019jsis3d} model. The network architecture includes three EdgeConv layers which are used to extract geometric features. These three EdgeConv layers use three shared fully connected layers with sizes equal to 64, 128, and 256. A shared fully connected layer with size 1024 was then used to aggregate information from the previous layers. The global point-cloud feature was obtained using the max-pooling operation, and two branches were used to transform the global features: one branch for semantic segmentation (which predicts whether a point is stable or unstable), and another branch for embedding instance features of stable planes. Both branches used fully connected layers with sizes of 512 and 256. Before the two branches, LeakyReLU and batch normalization were applied to all layers.

A mean-shift clustering algorithm \cite{comaniciu2002mean} was applied to the predicted stable points to identify stable points, and RANSAC \cite{fischler1981random} was used to fit planes onto the clustered points \cite{Zhou2018}. Stability scores for each plane were calculated by the element-wise multiplication of semantic logits, predicted instance labels, and number of points composing each plane. The plane with the highest score was output after fitting the planes and assigning stability scores based on the number of inliers that constitute the planes. The rotation matrix $\mathbf{R}$ was then determined by estimating the angular difference between the predicted normal vector of the stable plane and the gravity vector (negative table surface normal).

\subsection{Loss Function}
Our loss function comprises two terms; stability loss $\mathcal{L}_{stability}$ and plane loss $\mathcal{L}_{plane}$,

\begin{equation}
\label{eq:the_sum_of_losses}
\mathcal{L} = \lambda_1 * \mathcal{L}_{stability} + \lambda_2 * \mathcal{L}_{plane},
\end{equation}

\noindent where $\lambda_1$ and $\lambda_2$ are hyperparameters respectively set as $\lambda_1 = 10$ and $\lambda_2 = 1$. 
The stability loss $\mathcal{L}_{stability}$ is defined by the binary cross-entropy loss to encourage the predicted point label to match the ground truth label. We adopted the discriminative function for two-dimensional images \cite{de2017semantic} and 3D point cloud \cite{pham2019jsis3d} to embed instance features for the plane loss $\mathcal{L}_{plane}$.




% \noindent where $\lambda_1$ and $\lambda_2$ are hyper-parameters, setting as $\lambda_1=10$ and $\lambda_2=1$ respectively. The stability loss $\mathcal{L}_{stability}$ is defined by the binary cross-entropy loss to encourage predicted point label $\hat{y_i}$ to match with the ground truth label $y_i \in \{0, 1\}$, where $N$ is the number of points:

% \begin{equation}
% \label{eq:stablility loss}
% \mathcal{L}_{stability} = {1 \over N}\sum_{i=1}^N{-[y_i\log(\hat{y_i})+(1-y_i)log(1-\hat{y_i})]}
% \end{equation}

% We adopted the discriminative function for 2D images\cite{de2017semantic} and 3D point cloud\cite{pham2019jsis3d} to embed instance features as follows: 

% \begin{equation}
% \label{eq:total_dis_loss}
% \mathcal{L}_{plane} = \alpha * \mathcal{L}_{pull} + \beta * \mathcal{L}_{push} + \gamma * \mathcal{L}_{reg},
% \end{equation}

% Our plane loss function consists of three components: $\mathcal{L}_{pull}$, which pulls instance feature vectors towards the instance center; $\mathcal{L}_{push}$, which encourages instances to push against each other; and $\mathcal{L}_{reg}$, which acts as a regularization term to bound the embedding values. In our approach, $P$ represents the maximum number of stable planes (instances) in the object, while $N_p$ is the number of points in the target plane $p \in \{1,...,P\}$. The embedding vector of plane $p$ is denoted by $\boldsymbol{z}_p$, and $\boldsymbol{\mu}_p$ represents the embedding of point $x_p \in N_p$. The pull loss $\mathcal{L}_{pull}$ and push loss $\mathcal{L}_{push}$ have margins $\delta_v$ and $\delta_d$, respectively. The L2-norm is represented by $||\cdot||_2$, and the function $\mathcal{G}(x) = max(0, x)$ is used. We set hyper-parameters $\alpha = \beta = 1$ and $\gamma = 0.001$, and the margin $\delta_d$ is set to be greater than 2$\delta_v$, following \cite{pham2019jsis3d}.


% \begin{equation}
% \label{eq:var_loss}
% \mathcal{L}_{pull} = {1 \over P}\sum_{p=1}^P{1 \over N_p}\sum_{k=1}^{N_p} \mathcal{G}(||\boldsymbol{\mu}_p - \boldsymbol{z}_k||_2 - \delta_v)^2
% \end{equation}

% \begin{equation}
% \label{eq:dist_loss}
% \mathcal{L}_{push} = {1 \over P(P-1)}\sum_{p=1}^P \sum_{q=1, q \neq p}^P \mathcal{G}(2\delta_d - ||\boldsymbol{\mu}_p - \boldsymbol{\mu}_q||_2)^2
% \end{equation}

% \begin{equation}
% \label{eq:reg_loss}
% \mathcal{L}_{reg} = {1 \over P} \sum_{p=1}^P ||\boldsymbol{\mu}_p||_2
% \end{equation}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \noindent\textbf{Unseen object placement-network (UOP-Net)} 
% The UOP-Net model is designed to detect stable planes for object placement on a flat surface. The model considers a partial point cloud from a single depth image and processes it using a dynamic graph convolutional neural network (DGCNN)\cite{wang2019dynamic} to extract global shape features. These features are then split into two branches: one for semantic segmentation (predicting whether a point is stable or unstable) and the other for embedding instance features.

% Our UOP-Net is a model designed for detecting stable planes for object placement on a flat surface. Given a partial point cloud sampled from a single depth image, the model first extracts the global shape features using a dynamic graph convolutional neural network (DGCNN) \cite{wang2019dynamic}. These features are then split into two branches: one for semantic segmentation, which predicts whether a point is stable or unstable, and one for embedding instance features.
% Because objects can have various shapes and multiple stable planes, a function that generalizes the features of the stable planes is required. Therefore, we propose UOP-Net based on category-agnostic instance segmentation, which includes both the concept of objectness and properties for stable planes obtained from the UOP-Sim dataset. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Table 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% rotation(degree), translation(meter) 단위 체크, Translation * E-05
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
% \usepackage{dcolumn}

\begin{table*}[ht!]
\caption{Performance of Unseen Object Placement network (UOP-Net) and other baselines on the three benchmark objects (partial shape) in simulations.}
\centering
\label{tab:simulation-partial}
\resizebox{\textwidth}{!}{%
{\renewcommand{\arraystretch}{1.2}
\LARGE{
\begin{tabular}{|cccccccccccccc|}
\hline
% \multicolumn{14}{|c|}{\textbf{Stable Object Placement Evaluation Metric}} \\ \hline
\multicolumn{2}{|c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}\\ Object type \& \\ Dataset\end{tabular}}} & \multicolumn{8}{c|}{ Object stability (S)} & \multicolumn{4}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Success rate of \\ object placement (\mathcal{SR}, $\%$)\end{tabular}}} \\ \cline{3-10}
\multicolumn{2}{|c|}{} & \multicolumn{4}{c|}{Rotation (R, $^{\circ}$) \downarrow} & \multicolumn{4}{c|}{Translation (T, $cm$) \downarrow} & \multicolumn{4}{c|}{} \\ \cline{3-14} 
\multicolumn{2}{|c|}{} & Convex hull stability analysis (CHSA) \cite{haustein2019object} & Bounding box fitting (BBF) \cite{mitash2020task} & RANSAC plane fitting (RPF) \cite{fischler1981random} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}UOP \\ \textbf{(Ours)}\end{tabular}} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF \cite{fischler1981random} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}UOP \\ \textbf{(Ours)}\end{tabular}} & CHSA \cite{haustein2019object} & BBF\cite{mitash2020task} & RPF \cite{fischler1981random} & \begin{tabular}[c]{@{}c@{}}UOP \\ \textbf{(Ours)}\end{tabular} \\ \hline
% Partial Point Cloud
\multicolumn{1}{|c|}{\multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Partial \\ point cloud\end{tabular}}} & \multicolumn{1}{c|}{Three-dimensional network (3DNet) \cite{wohlkinger20123dnet}} % 3DNet
& 28.81 & 33.10 & 25.40 & \multicolumn{1}{c|}{\textbf{8.68}} % Rotation
& 3.02 & 3.45 & 2.49 & \multicolumn{1}{c|}{\textbf{0.77}}  % Translation
& 52.60 & 36.12 & 64.34 & \textbf{65.24}  \\ % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{ShapeNet\cite{chang2015shapenet}} % ShapeNet
& 31.65 & 38.14 & 21.83 & \multicolumn{1}{c|}{\textbf{7.91}} % Rotation
& 3.97 & 4.53 & 2.46 & \multicolumn{1}{c|}{\textbf{0.86}} % Translation
& 51.05 & 29.95 & 69.70 & \textbf{72.82}  \\ % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{YCB \cite{calli2015ycb}} % YCB
& 34.85 & 41.69 & 22.41 & \multicolumn{1}{c|}{\textbf{5.92}} % Rotation
& 5.07 & 5.89 & 2.55 & \multicolumn{1}{c|}{\textbf{0.56}} % Translation
& 42.48 & 30.41 & 62.13 & \textbf{73.32} \\ 
\hhline{|~|=|====|====|====|} % Success Rate
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Total average} % Total avg.
& 31.18 & 36.72 & 23.69 & \multicolumn{1}{c|}{\textbf{7.73}} % Rotation
& 3.82 & 4.39 & 2.50 & \multicolumn{1}{c|}{\textbf{0.74}} % Translation
& 49.43 & 33.01 & 65.07 & \textbf{69.35} \\ \hline % Success Rate

\end{tabular}
}
}
}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% A mean-shift clustering algorithm \cite{comaniciu2002mean} is applied to the predicted stable points for identifying the stable points, and RANSAC \cite{fischler1981random} is used to fit planes onto the clustered points\cite{Zhou2018}. Stability scores for each plane are calculated by the element-wise multiplication of semantic logits, predicted instance labels, and a number of points composing each plane. Then, the plane with the highest score is output after fitting the planes and assigning stability scores based on the number of inliers that constitute the planes. The rotation value $\mathcal{R}$ is then determined by estimating the angular difference between the predicted normal vector of the stable plane and the gravity vector (negative table surface normal).

% \begin{equation}
% \label{eq:the_sum_of_losses}
% \mathcal{L} = \lambda_1 * \mathcal{L}_{stable} + \lambda_2 * \mathcal{L}_{plane}.
% \end{equation}

% The robustness and generalization of UOP-Net is improved by training it using a category-agnostic instance segmentation approach that allows it to learn the features of the stable planes without being restricted to specific object categories. The model is trained using a loss function that is a combination of the binary cross-entropy loss (stable loss) for semantic segmentation and the discriminative loss (plane loss) for instance embedding. Hyperparameters $\lambda_1$ and $\lambda_2$ are respectively set to 10 and 1 to ensure comparable values for the two loss terms.

% \noindent\textbf{Training details} We trained UOP-Net by utilizing a dataset comprising partial point clouds of objects captured from various views in addition to stable plane annotations selected based on technical criteria for ensuring suitability. The dataset used for training was specifically curated from 3D object models sourced from 3DNet \cite{wohlkinger20123dnet} and ShapeNet \cite{chang2015shapenet}. Training batches comprised 2,048 points randomly sampled from each object and they underwent various types of data augmentation techniques such as rotation, sheering, point-wise jittering, and adding Gaussian noise to enhance the performance of the model in real-world scenarios. The model was implemented using PyTorch\cite{paszke2019pytorch} and trained on an NVIDIA Titan RTX GPU with a batch size of 32 and a total of 1,000 epochs. We employed early stopping with a patience of 50 and used the Adam optimizer at a learning rate of 1e-3 to prevent overfitting.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Experiments
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Experiments}
\subsection{Comparison with Traditional Method in Simulations}
\noindent\textbf{Datasets.} 
We obtained 152, 57, and 63 object categories in the 3DNet \cite{wohlkinger20123dnet}, ShapeNet\cite{chang2015shapenet}, and YCB \cite{calli2015ycb} datasets, respectively. We labeled the YCB object models in the simulations, but these were excluded from the training set to allow us to use the test set in both the simulation and real-world experiments. We excluded objects that had no stable planes (e.g., spherical objects) to ensure the quality of our dataset. We then split the dataset into training and validation sets (8:2 ratio). The training set contained 13926 objects and 55261 annotations, while the validation set contained 3482 objects and 13766 annotations.

\noindent\textbf{Training Details.} We trained the UOP-Net using partial point clouds sampled from the UOP-Sim dataset. During training, 2048 points were randomly sampled for each object and they underwent various types of data augmentation techniques, such as rotation, sheering, point-wise jitter, and Gaussian noise was added to improve the performance of the model in real-world scenarios. The model was implemented using PyTorch \cite{paszke2019pytorch} and trained on an NVIDIA Titan RTX graphics processing unit (GPU) with a batch size of 32 and a total of 1000 epochs. We employed early stopping with a patience of 50 and used the Adam optimizer at a learning rate of 1e-3 to prevent overfitting.

% We trained UOP-Net by utilizing a dataset comprising partial point clouds of objects captured from various views in addition to stable plane annotations selected based on technical criteria for ensuring suitability. The dataset used for training was specifically curated from 3D object models sourced from 3DNet \cite{wohlkinger20123dnet} and ShapeNet \cite{chang2015shapenet}. During training, 2,048 points randomly sampled from each object and they underwent various types of data augmentation techniques such as rotation, sheering, point-wise jitter, and adding Gaussian noise to enhance the performance of the model in real-world scenarios. The model was implemented using PyTorch\cite{paszke2019pytorch} and trained on an NVIDIA Titan RTX GPU with a batch size of 32 and a total of 1,000 epochs. We employed early stopping with a patience of 50 and used the Adam optimizer at a learning rate of 1e-3 to prevent overfitting.

\noindent\textbf{Baselines.} 
We compared the performance of our method with those of the following baselines:
\begin{itemize}
    \item \textbf{Convex hull stability analysis (CHSA)} \cite{haustein2019object, trimesh, hagelskjaer2019using}: The baseline method for determining stable object poses involves the calculation of the rotation matrix to allow an object to rest stably on a flat surface. The center of mass of the object is sampled, and the stable resting poses of the object on a flat surface are determined using the convex hull of the object. The probabilities of the object landing in each pose were evaluated, and the pose with the highest probability was output.

    \item \textbf{Bounding box fitting (BBF)} \cite{mitash2020task, Zhou2018}: The method involves the fitting of an oriented bounding box to the convex hull of the object using principal component analysis to minimize the difference between the volume of the convex hull and that of the bounding box. The object was placed on a planar workspace with the largest area facing down.
    
    \item \textbf{RANSAC plane fitting (RPF)} \cite{fischler1981random, Zhou2018}: The approach segments planes in point clouds by fitting a model of the form $ax + by + cz + d = 0$ to each point $(x, y, z)$. Subsequently, it samples several points randomly and uses them to construct a random plane while repeating this process iteratively to determine the plane that appears most frequently.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Table 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table}[ht!]
\caption{UOP Performance of UOP-Net and other baselines in the simulation (whole shape). The best and second-best results are indicated in \textbf{bold} and \underline{underline}, respectively.}
\label{tab:simulation-whole}
\tiny
\resizebox{0.48\textwidth}{!}{%
\begin{tabular}{cccccc}
\hline
\multicolumn{2}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}} \\Object type \&\\ Dataset\end{tabular}}} & \multicolumn{4}{c}{\begin{tabular}[c]{@{}c@{}}Success rate of \\ object placement (SR), \%)\end{tabular}} \\ \cline{3-6} 
\multicolumn{2}{c}{}                                                                                  
& CHSA\cite{haustein2019object}          & BBF\cite{mitash2020task}         & RPF\cite{fischler1981random}         & \begin{tabular}[c]{@{}c@{}}UOP-Net\\ (Ours)\end{tabular}         \\ \hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Whole\\ Point cloud\end{tabular}}       
& 3DNet\cite{wohlkinger20123dnet}   & \textbf{83.28}    & 55.48    & 70.89    & \underline{80.16}  \\ \cline{2-6}
& ShapeNet\cite{chang2015shapenet}  & \textbf{92.37}    & 49.09    & 79.00    & \underline{86.14}  \\ \cline{2-6}
& YCB\cite{calli2015ycb}            & \textbf{86.08}    & 45.86    & 78.11    & \underline{84.32}  \\ \cline{2-6}
& Total avg.                        & \textbf{86.31}    & 51.24    & 74.90    & \underline{82.79}  \\ \hline
\end{tabular}%
}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\noindent\textbf{Evaluation Metrics.} 
We used two metrics to evaluate the performance of UOP-Net: object stability \textit{(\mathcal{OS})} and success rate of object placement \textit{(\mathcal{SR})}. We placed an object on a flat table and used the output of the model to estimate its stability for conducting measurements \textit{\mathcal{OS}}. We considered only rotational motion when we evaluated object stability because rotational motion is more common than translational motion when an object placed in an unstable state falls owing to vibrations. We evaluated the performance of the model by placing the object 100 times; we considered cases as failures when no planes were detected.
% We used two metrics to evaluate the performance of UOP-Net: object stability \textit{(\mathcal{OS})} and success rate of object placement \textit{(\mathcal{SR})}. We placed an unseen object on a flat table and used the output of the model to estimate its stability for measuring \mathcal{OS}. This allowed us to verify whether the trained model could maintain the object in a stable pose. The \mathcal{SR} was calculated as the ratio of successful stable placements to all placements, with an accumulated rotation error of less than 10°. We considered only rotational motion when evaluating object stability because rotational motion is more common than translational motion when an object placed in an unstable state falls because of the vibrations. We evaluated the performance of the model by placing the object 100 times and regarding as a failure case if no planes were detected.


\begin{itemize}
    \item \textbf{Object stability (\mathcal{OS})}: The metric quantifies the movement of the object during a discrete time step when it is placed on a horizontal surface using the predicted plane
    \item \textbf{Success rate (\mathcal{SR})}: The metric indicates the percentage of placements where the object remains stationary for 1 min with an accumulated rotation of $10^{\circ}$
    % \item \textbf{Success Rate (\mathcal{SR})}: The metric represents the percentage of placements in which the object remains stationary for a minute and the accumulated rotation is less than $10^{\circ}$. 
\end{itemize}





%\subsection{Discussion}
\noindent\textbf{Discussion.} In the simulation experiments, we compared the performance of UOP-Net with the baseline methods on 3DNet \cite{wohlkinger20123dnet}, ShapeNet\cite{chang2015shapenet}, and YCB \cite{calli2015ycb} objects for partial shapes. Table \ref{tab:simulation-partial} presents the simulation results. Our UOP-Net achieved the best performance when single partial observations were used. We visualized the prediction results of each method and represented the object stability as a graph in Fig. \ref{fig:sim_exp_result}. The blue line in the graph (Fig. \ref{fig:sim_exp_result}) represents the case in which the object is placed in the plane predicted by the UOP-Net, and shows that the object can be placed more reliably compared with other methodologies.


% While the CHSA method provides optimal results for stable object placement when the object shape is fully visible, as demonstrated in Supplementary Table 1. However, these assumptions are impractical, and UOP-Net performs well even for complete object shapes. In scenarios where only partial point clouds are available, the UOP-Net demonstrates a significant advantage over the CHSA method, as shown in Table \ref{tab:simulation-partial}. This is due to the fact that the convex hull generated from partial point clouds does not provide information about the unseen parts of the object, making it challenging to accurately determine the center of mass. As a result, the CHSA may not perform well with partial observations, and may lead to unsteady placement on truncated planes, as seen in Fig.\ref{fig:sim_exp_result}.


When the object shape is fully visible, the CHSA method provides optimal results for stable object placements (Table \ref{tab:simulation-whole}). While the UOP-Net appears to be less effective in fully observed scenarios, the assumption is impractical. As shown in Table \ref{tab:simulation-partial}, the UOP-Net method outperforms the CHSA method when only partial point clouds are available. This is because a convex hull derived from a partial point cloud lacks information about the invisible part of the object, thus making the determination of its center of mass more difficult. Additionally, we found that the CHSA method frequently predicts truncated planes. This is due to the truncated plane having the largest area within the convex hull, thus making it the most likely candidate for stable object placements (Fig.\ref{fig:sim_exp_result}). 


% While the CHSA method provides optimal results for stable object placement when the object shape is fully visible (Table \ref{tab:simulation-whole}), these assumptions are impractical. In scenarios where only partial point clouds are available, the UOP-Net outperforms the CHSA method, as shown in Table \ref{tab:simulation-partial}. This is because the convex hull generated from partial point cloud does not provide information about the invisible part of the object, making determining the center of mass difficult. As a result, the CHSA may perform poorly with partial observations, and may lead to unsteady placement on truncated planes, as seen in Fig.\ref{fig:sim_exp_result}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 4 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fig 4.
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure 4/Figure_4.png}
        \end{subfigure}
        
    \caption{Visualization of the prediction results obtained on the YCB \cite{calli2015ycb} dataset for each method and object stability in simulations.}
    \label{fig:sim_exp_result}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% The BBF method performed the worst because it did not consider the geometric properties of the object and simply placed the largest plane in the bounding box. This may work for objects with simple shapes; however, it is not suitable for complex shapes. Although RPF performs better than BBF, it still fails to detect a robust stable plane.  In contrast, UOP-Net can detect the most stable plane from partial observation, because we train the model to predict planes from only visible parts directly.

The BBF method had the worst performance because it did not consider the geometric properties of the object and did not rely on the placement of the largest plane within the bounding box. This may work for objects with simple shapes; however, it is not suitable for complex shapes. Despite the improved performance of the RPF method compared with BBF, it is still insufficient to identify consistently stable planes. Even though the RPF method eventually generates one of the most frequently sampled planes, its inability to select a reliably stable plane prevents successful object placement. In contrast, the UOP-Net excels in the identification of the most stable plane (even from partial observations) achieved by training the model to predict directly planes based solely on visible components.

Further, UOP-Net can detect explicit planes visible in the point cloud and implicit planes (e.g., a plane formed by the four legs of a chair) that are not necessarily visible but still contribute to the stability of the object. This is because our model was trained on a diverse set of objects and planes and can generalize new unseen objects and planes.

% \textbf{Failure cases.}
% UOP-Net has some limitations that can affect its performance. For example, UOP-Net may have difficulty detecting stable planes for small objects such as spoons or knives because the size of the ground truth (stable plane) is relatively small compared to the size of the object. Further, it is difficult to find the optimal hyper-parameters for the mean-shift clustering algorithm, and therefore, the performance of UOP-Net can suffer when the hyper-parameters are not well-tuned for a particular object. However, these limitations are not unique to UOP-Net and are common challenges faced by many object placement methods.


\subsection{Comparison with Learning-based Method in Simulations}

We compared the performance of UOP-Net, Upright-Net \cite{pang2022upright}, and the CHSA method alongside the point-cloud completion method \cite{yu2021pointr}. To ensure a fair comparison with Upright-Net, we conducted the experiments on a subset of UOP-Sim, where we used the same categorization scheme of Upright-Net. The dataset was divided into three splits: seen objects for training, seen objects for evaluation, and unseen objects for evaluation. Each category included 40 objects for training and 10 for evaluation. The categories are the following:

\begin{itemize}
    \item \textbf{\textit{Seen}}: bed, bench, bottle, bowl, bus, cabinet, camera, cap, car, chair, jar, laptop, mug, printer, and table
    \item \textbf{\textit{Unseen}}: basket, helicopter, lamp, pot, skateboard, sofa, and tower
\end{itemize}

For the completion method, we used Pointr \cite{yu2021pointr} that was trained on our subset of UOP-Sim; we confirmed that this model performs better than their pretrained models owing to the difference in input preprocessing. Table \ref{tab:Additional Experiments} shows each method's object placement success rate in simulations. The rate is the ratio of successful placements to the total number of inference trials. Each object underwent 60 trials.

% Table 3. ----------------------------------------------
%{14pt plus 2pt minus 2pt}
\begin{table}[ht!]

\centering
\caption{Placement success rates of UOP-Net, Upright-Net, CHSA with and without completion (C: completion \cite{yu2021pointr}).}
\label{tab:Additional Experiments}
\resizebox{0.48\textwidth}{!}{%
\begin{tabular}{cccccc}
\hline
\multirow{2}{*}{}        & \multirow{2}{*}{Data type} & \multicolumn{4}{c}{Method}            \\ \cline{3-6} 
                         &                            & CHSA without C & CHSA w/ C & Upright-Net \cite{pang2022upright} & UOP-Net (Ours) \\ \hline
\multirow{2}{*}{Whole}   & Seen                       & \textbf{92.27}          & -        & 77.97       & 87.59   \\ \cline{2-6} 
                         & Unseen                     & 86.76          & -        & 84.26       & \textbf{87.77}   \\ \hline
\multirow{2}{*}{Partial} & Seen                       & 57.04          & 67.06        & 17.04       & \textbf{76.75}   \\ \cline{2-6} 
                         & Unseen                     & 51.68          & 63.92        & 25.21       & \textbf{73.78}   \\ \hline
\end{tabular}%
}
\end{table}


% %-----------------------------------------



\noindent\textbf{Comparison with Upright-Net}. Our proposed UOP-Net consistently outperformed Upright-Net \cite{pang2022upright} under all conditions. Upright-Net demonstrates comparable performance with the entire point-cloud input; however, its performance was significantly lower than ours when the partial point cloud was input. This is owing to Upright-Net's design specifically constructed to predict only the upright orientation from the entire point cloud input. Consequently, it tends to fail when upright planes are invisible within the input partial point cloud. On the contrary, our UOP-Net was designed to detect all stable planes from the partial point cloud, thus leading to better performance. 

\noindent\textbf{Comparison with CHSA and Completion}. Our UOP-Net outperforms CHSA in all cases except for seen objects provided with a complete point cloud. While the completion method does enhance CHSA's performance, it still tends to underperform compared with our method. This is primarily because the completion method often struggles to generate an accurate and detailed point cloud (Fig\ref{fig:ablation_pred_res}). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 7 %%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=0.8\textwidth]{figures/Figure7/uop_re_fig.png}
        \end{subfigure}
        
    \caption{Visualization of the predictions for partial point clouds (Top: Jar/Down: Lamp).}
    \label{fig:ablation_pred_res}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\subsection{Robot Experiments}
We conducted real-world experiments to evaluate the performance of our UOP method. Our objectives were to demonstrate its ability to (1) perform effectively in real-world scenarios despite being trained purely in simulation, (2) robustly handle partial and noisy observations, and (3) detect stable planes in unseen objects, thus achieving SOTA performance.

\noindent\textbf{Real Environment Settings.}
We conducted experiments using a universal robot (UR5) manipulator and an Azure Kinect RGB-D camera to evaluate the performance of our object placement method in a real-world scenario (Fig.\ref{fig:real_experiment} (b)). We used the MANet \cite{fan2020ma} object segmentation method with a DenseNet121 \cite{iandola2014densenet} backbone to segment the target object and the gripper. We segmented the visible region of the target object from the RGB image and cropped the depth image using a mask. We then sampled the point cloud from the depth image using voxel-down sampling \cite{Zhou2018} and fed it to UOP-Net. The model predicted the most stable plane and calculated the rotation value between the plane and the table. The UR5 robot placed the target object on the table. We utilized the BiRRT algorithm \cite{qureshi2015intelligent} implemented with PyBullet \cite{coumans2021} and integrated it with collision checking in a physics engine to ensure smooth planar motion.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 5 %%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure 5/Figure_5.png}
        \end{subfigure}
        
    \caption{\textbf{Unseen object placement using UOP-Net}. (a) Given a segmented target object, UOP-Net considers the 3D point clouds observed using a depth camera (Azure Kinect) as inputs and predicts the most stable plane. (b) The universal robot places the target object on the table.}
    \label{fig:real_experiment}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent\textbf{Evaluation metrics.} To ensure a fair comparison, we strived to standardize the object grasp configurations across various methodologies. Subsequently, we adopted the \mathcal{SR} metric to assess quantitatively the efficacy of our proposed approach in real-world scenarios. Throughout the experiments, we instructed the robotic system to execute object placements onto the surfaces predicted by the UOP-Net model. The successful placement was predicted based on visual confirmation of the object maintaining a stable, nonsliding position on the predicted plane. Conversely, if the model failed to identify any viable stable planes, the trial was classified as unsuccessful. For each distinct object, we conducted a series of 10 placement trials to ensure a comprehensive evaluation outcome.

% We used \mathcal{SR} as the evaluation metric to evaluate the effectiveness of our approach in real-world scenarios. During the experiments, we instructed the robot to place the object on the table using the plane predicted by UOP-Net. The placement was considered successful when we could visually confirm that the robot placed the object in a fixed position on the predicted plane without the object falling or sliding. The trial was considered a failure if the model failed to detect any stable planes. We conducted a total of 10 placement trials for each object.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Table 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{table}[ht!]
% \caption{\textbf{UOP} performance of the UOP-Net and baselines on YCB\cite{calli2015ycb} in the real world.}
% \label{tab:real world-eval}
% \resizebox{\columnwidth}{!}{%
% \LARGE{
% {\renewcommand{\arraystretch}{}
% \begin{tabular}{|c|cccc|}
% \hline
% YCB\cite{calli2015ycb} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \begin{tabular}[c]{@{}c@{}}UOP-Net \\ \textbf{(Ours)}\end{tabular} \\ \hline
% Coffee can & 0 / 10 & 0 / 10 & 4 / 10 & \textbf{10} / 10 \\
% Timer & 0 / 10 & 1 / 10 & \textbf{6} / 10 & \textbf{6} / 10 \\
% Power drill & 0 / 10 & 1 / 10 & \textbf{5} / 10 & \textbf{5} / 10 \\
% Wood block & 1 / 10 & 1 / 10 & \textbf{10} / 10 & \textbf{10} / 10 \\
% Metal mug & 0 / 10 & 0 / 10 & 6 / 10 & \textbf{9} / 10 \\
% Metal bowl & \textbf{10} / 10 & 5 / 10 & \textbf{10} / 10 & \textbf{10} / 10 \\
% Bleach cleanser & 3 / 10 & 3 / 10 & \textbf{9} / 10 & \textbf{9} / 10 \\
% Mustard container & 2 / 10 & 0 / 10 & 5 / 10 & \textbf{10} / 10 \\
% Airplane toy & 0 / 10 & 3 / 10 & 0 / 10 & \textbf{4} / 10 \\
% Sugar box & 2 / 10 & 3 / 10 & \textbf{10} / 10 & \textbf{10} / 10 \\
% Chips can & 2 / 10 & 0 / 10 & 8 / 10 & \textbf{10} / 10 \\
% Banana & 5 / 10 & 5 / 10 & \textbf{9} / 10 & \textbf{9} / 10 \\ \hhline{|=|=|=|=|=|}
% Average & 2.1 / 10 & 1.8 / 10 & 6.8 / 10 & \textbf{8.5} / 10 \\ \hline
% \end{tabular}%
% }
% }
% }
% \end{table}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Table 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[ht!]
\centering
\caption{UOP performance of the UOP-Net and baselines on YCB \cite{calli2015ycb} in the real world.}
\label{tab:real world-eval}
\tiny
\resizebox{0.48\textwidth}{!}{%
\begin{tabular}{ccccc}
\hline
YCB & CHSA \cite{haustein2019object}  & BBF \cite{mitash2020task}  & RPF \cite{fischler1981random} & \begin{tabular}[c]{@{}c@{}}UOP-Net\\ (Ours)\end{tabular} \\ \hline
Coffee can & 0 & 0 & 4 & \textbf{10} \\
Timer & 0 & 1 & \textbf{6} & \textbf{6} \\
Power drill & 0 & 1 & \textbf{5} & \textbf{5} \\
Wood block & 1 & 1 & \textbf{10} & \textbf{10} \\
Metal mug & 0 & 0 & 6 & \textbf{9} \\
Metal bowl & \textbf{10} & 5 & \textbf{10} & \textbf{10} \\
Bleach cleanser & 3 & 3 & \textbf{9} & \textbf{9} \\
Mustard container & 2 & 0 & 5 & \textbf{10} \\
Airplane toy & 0 & 3 & 0 & \textbf{4} \\
Sugar box & 2 & 3 & \textbf{10} & \textbf{10} \\
Chips can & 2 & 0 & 8 & \textbf{10} \\
Banana & 5 & 5 & \textbf{9} & \textbf{9} \\ \hline
Average & 2.1 & 1.8 & 6.8 & \textbf{8.5} \\ \hline
\end{tabular}%
}
\end{table}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 6 %%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht!]
    \centering
        \begin{subfigure}[t]{\columnwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/Figure 6/Figure_6.png}
        \end{subfigure}
        
    \caption{Visualization of the prediction results for each method on YCB \cite{calli2015ycb} objects and novel objects in the real world.}
    \label{fig:comparison_pred_res}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\noindent\textbf{Results.}
We selected 12 objects from the YCB dataset. Objects with spherical shapes (e.g., apples), dimensions that were extremely small, or small depth values, were excluded from the test set. Table \ref{tab:real world-eval} indicates that our method outperforms other baselines in terms of the success rate across all objects. Although real-world perceptions are noisy, UOP-Net provides a stable plane that can be attributed to our model learning from partial point clouds captured by a depth camera and corrupted by noise. Other benchmarks (CHSA and primitive shape fitting) performed extremely poorly because they could not obtain the complete shape of the object in the real world and were unable to respond to sensor noise. We evaluated our method on completely new objects that did not have an available CAD model (a dinosaur figurine and an ice tray, as shown in Fig. \ref{fig:comparison_pred_res}) to verify further that our model can perform on unseen objects. UOP-Net detected implicit planes (e.g., the four legs of the dinosaur) even though the object shapes were complex. 

% \textbf{Failure cases}
% Points corresponding to the ground truth are not sampled when the RGB-D sensor fails to capture a specific area of an object (e.g., a dark or glossy part of the object) or when the object and gripper segmentation model fails to predict a clear mask. In such a case, UOP-Net can fail to detect the stable plane. Although we demonstrated that UOP-Net can detect an implicit plane from complex-shaped objects (e.g., YCB airplane toy or dinosaur), it is still challenging to detect an implicit plane in an unclear point cloud.



\section{Conclusion} 
In this study, we presented UOP-Net, a novel method developed to detect stable planes of unseen objects. We also introduced an approach to annotate automatically stable planes for various objects, and the large-scale synthetic dataset, called UOP-Sim, was generated. Our dataset contains 17.4 K 3D objects and 69 K stable plane annotations. The effectiveness of UOP-Net was demonstrated by achieving SOTA performance on objects from three benchmark datasets, thus confirming its accuracy and reliability in detecting stable planes from unseen and partially observable objects.















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Table 2%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{table*}[ht!]
% \caption{{UOP} Performance of UOP-Net and other baselines on the three benchmark objects (complete shape) in the simulation.}
% \centering
% \label{tab:simulation-complete}
% \resizebox{\textwidth}{!}{%
% {\renewcommand{\arraystretch}{1.2}
% \LARGE{
% \begin{tabular}{|cccccccccccccc|}
% \hline
% % \multicolumn{14}{|c|}{\textbf{Stable Object Placement Evaluataion Metric}} \\ \hline
% \multicolumn{2}{|c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}\\ Object type \& \\ Dataset\end{tabular}}} & \multicolumn{8}{c|}{ Object stability (S)} & \multicolumn{4}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Success rate of \\ object placement (\mathcal{SR}, $\%$)\end{tabular}}} \\ \cline{3-10}
% \multicolumn{2}{|c|}{} & \multicolumn{4}{c|}{Rotation (R, $^{\circ}$) \downarrow} & \multicolumn{4}{c|}{Translation (T, $cm$) \downarrow} & \multicolumn{4}{c|}{} \\ \cline{3-14} 
% \multicolumn{2}{|c|}{} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}UOP \\ \textbf{(Ours)}\end{tabular}} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}UOP \\ \textbf{(Ours)}\end{tabular}} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \begin{tabular}[c]{@{}c@{}}UOP \\ \textbf{(Ours)}\end{tabular} \\ \hline


% % Complete Point Cloud
% \multicolumn{1}{|c|}{\multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Complete \\ point cloud\end{tabular}}} & \multicolumn{1}{c|}{3DNet\cite{wohlkinger20123dnet}} % 3DNet
% & \underline{5.03} & 20.84 & 16.22 & \multicolumn{1}{c|}{\textbf{5.01}} % Rotation
% & \textbf{0.40} & 2.09 & 1.48 & \multicolumn{1}{c|}{\underline{0.50}}  % Translation
% & \textbf{87.03} & 58.23  & 73.44  & \underline{83.14}  \\ % Success Rate
% \multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{ShapeNet\cite{chang2015shapenet}} % ShapeNet
% & \textbf{2.02} & 19.44 & 10.57 & \multicolumn{1}{c|}{\underline{2.60}} % Rotation
% & \textbf{0.16} & 2.41 & 1.15 & \multicolumn{1}{c|}{\underline{0.30}} % Translation
% & \textbf{95.17} & 54.26  & 81.51  & \underline{88.75}  \\ % Success Rate
% \multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{YCB\cite{calli2015ycb}} % YCB
% & \underline{4.42} & 37.59 & 8.58 & \multicolumn{1}{c|}{\textbf{3.08}} % Rotation
% & \textbf{0.35} & 5.96 & 0.87 & \multicolumn{1}{c|}{\underline{0.30}} % Translation
% & \textbf{89.27} & 49.89  & 83.84  & \underline{87.84}  \\ 
% \hhline{|~|=|====|====|====|} % Success Rate
% \multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{Total avg.} % Total
% & \underline{4.26} & 24.43 & 13.27 & \multicolumn{1}{c|}{\textbf{4.06}} 
% & \textbf{0.34} & 3.05 & 1.27 & \multicolumn{1}{c|}{\underline{0.41}} 
% & \textbf{89.25} & 55.47  & 77.54  & \underline{85.41}  \\ \hline % Success Rate


% \end{tabular}
% }
% }
% }
% \end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \section{Simulation Experiments}
% \noindent\textbf{Datasets} 
% We evaluated our method by conducting experiments using synthetic data generated from 3DNet \cite{wohlkinger20123dnet} and ShapeNet \cite{chang2015shapenet}. We generated 69,000 stable plane annotations for a total of 17,400 3D objects using these datasets and split the synthetic data into training and validation sets in a 8:2 ratio. Although we labeled the YCB \cite{calli2015ycb} object models in the simulation, they were excluded from the training set to allow us to use the test set in both the simulation and real-world experiments. Further, we excluded objects that had no stable planes (e.g., spherical objects) to ensure the quality of our dataset. We obtained a total of 152, 57, and 63 object categories in the 3DNet, ShapeNet, and YCB datasets, respectively. The training set contained 13,926 objects and 55,261 annotations, while the validation set contained 3,482 objects and 13,766 annotations.

% \noindent\textbf{Baselines}
% We compared the performance of our method with that of the following baselines:
% \begin{itemize}
%     \item \textbf{Convex hull stability analysis (CHSA)}\cite{haustein2019object, trimesh, hagelskjaer2019using}: The baseline method for determining stable object poses involves calculating the rotation matrix to allow an object to rest stably on a flat surface. To this end, the location of the center of mass of the object is sampled, and the stable resting poses of the object on a flat surface are determined using the convex hull of the object. Then, the probabilities of the object landing in each pose are evaluated, and the pose with the highest probability is output.

%     \item \textbf{Bounding box fitting (BBF)}\cite{mitash2020task, Zhou2018}: The method involves fitting an oriented bounding box to the convex hull of the object using principal component analysis to minimize the difference between the volume of the convex hull and that of the bounding box. Then, the object is placed on a planar workspace with the largest area facing down.
    
%     \item \textbf{RANSAC plane fitting (RPF)}\cite{fischler1981random, Zhou2018}: The approach segments planes in a point cloud by fitting a model of the form $ax + by + cz + d = 0$ to each point $(x, y, z)$. Then, it samples several points randomly and uses them to construct a random plane while repeating this process iteratively to determine the plane that appears most frequently.
    
% \end{itemize}

% \noindent\textbf{Evaluation metrics}
% We used two metrics to evaluate the performance of UOP-Net: \textit{object stability (\mathcal{OS})} and \textit{success rate of object placement (\mathcal{SR})}. We placed an unseen object on a flat table and used the output of the model to estimate its stability for measuring \mathcal{OS}. This allowed us to verify whether the trained model could maintain the object in a stable pose. The \mathcal{SR} was calculated as the ratio of successful stable placements to all placements, with an accumulated rotation error of less than 10°. We considered only rotational motion when evaluating object stability because rotational motion is more common than translational motion when an object placed in an unstable state falls because of the vibrations. We evaluated the performance of the model by placing the object 100 times and terminating the evaluation if no planes were detected; we treated such an instance as a failed case.



% \begin{itemize}
%     \item \textbf{Object stability (\mathcal{OS})}: The metric quantifies the movement of the object during a discrete time step when it is placed on a horizontal surface using the predicted stable plane.
    
%     \item \textbf{ \mathcal{SR}}: The metric represents the percentage of placements in which the object remains stationary for a minute and the accumulated rotation is less than $10^{\circ}$.
% \end{itemize}

% \noindent\textbf{Comparison with the baselines}
% In the simulation experiments, we compared the performance of UOP-Net with the baseline methods on 3DNet\cite{wohlkinger20123dnet}, ShapeNet\cite{chang2015shapenet}, and YCB\cite{calli2015ycb} for complete and partial object shapes. Table \ref{tab:simulation-partial} presents the results in terms of OS and SR. Our UOP-Net achieved the best performance when using a single partial observation. We visualized the prediction results of each method and represented the object stability as a graph in Fig. \ref{fig:sim_exp_result}. The blue line in the graph (Fig. \ref{fig:sim_exp_result}) represents the case of placing the object in the plane predicted by UOP-Net, which shows that the object can be placed more reliably than predicted by other methodologies.

% While the CHSA method provides optimal results for stable object placement when the object shape is fully visible, as demonstrated in Table \ref{tab:simulation-complete}, our proposed UOP-Net is highly competitive and performs nearly as well. In scenarios where only partial point clouds are available, the UOP-Net demonstrates a significant advantage over the CHSA method, as shown in Table \ref{tab:simulation-partial}. This is due to the fact that the convex hull generated from partial point clouds does not provide information about the unseen parts of the object, making it challenging to accurately determine the center of mass. As a result, the CHSA may not perform well with partial observations, and may lead to unsteady placement on truncated planes, as seen in Fig.\ref{fig:sim_exp_result}.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 4 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% % Fig 4.
% \begin{figure}[ht!]
%     \centering
%         \begin{subfigure}[t]{\columnwidth}
%             \centering
%             \includegraphics[width=\textwidth]{figures/Figure 4/Figure_4.png}
%         \end{subfigure}
        
%     \caption{Visualization of the prediction results obtained on the YCB\cite{calli2015ycb} dataset for each method and object stability in the simulation.}
%     \label{fig:sim_exp_result}
% \end{figure}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% The BBF method showed the worst performance because it did not consider the geometric properties of the object and simply placed the largest plane in the bounding box. This may work for objects with simple shapes; however, it is not suitable for complex shapes. Although RPF performs better than BBF, it still fails to detect a robust stable plane. In contrast, UOP-Net can detect a robust plane based on incomplete perception because we generate partial point clouds using UOP-Sim and train the model to learn the geometric properties of objects and planes. 

% Further, UOP-Net can detect not only explicit planes visible in the point cloud but also implicit planes (e.g., a plane formed by the four legs of a chair) that are not necessarily visible but still contribute to the stability of the object. This is because our model is trained on a diverse set of objects and planes, and it can generalize new unseen objects and planes.

% \noindent\textbf{Failure cases}
% UOP-Net has some limitations that can affect its performance. For example, UOP-Net may have difficulty detecting stable planes for small objects such as spoons or knives because the size of the ground truth (stable plane) is relatively small compared to the size of the object. Further, it is difficult to find the optimal hyperparameters for the mean-shift clustering algorithm, and therefore, the performance of UOP-Net can suffer when the hyperparameters are not well-tuned for a particular object. 

% These limitations can cause UOP-Net to fail in accurately detecting stable planes for some objects. However, these limitations are not unique to UOP-Net and are common challenges faced by many object placement methods.

% \section{Experiments in the Real World}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 5 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure}[ht!]
%     \centering
%         \begin{subfigure}[t]{\columnwidth}
%             \centering
%             \includegraphics[width=\textwidth]{figures/Figure 5/Figure_5.png}
%         \end{subfigure}
        
%     \caption{\textbf{Unseen object placement using UOP-Net}. (a) Given a segmented target object, UOP-Net considers the 3D point clouds observed by a depth camera (Azure Kinect) as inputs and predicts the most stable plane. (b) UR5 places the target object on the table.}
%     \label{fig:real_experiment}
% \end{figure}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \noindent\textbf{Real environment setting}
% We conducted experiments using a universal robot (UR5) manipulator and an Azure Kinect RGB-D camera to evaluate the performance of our object placement method in a real-world scenario (Fig.\ref{fig:real_experiment} (b)). We used the MANet \cite{fan2020ma} object segmentation method with a DenseNet121 \cite{iandola2014densenet} backbone to segment the target object and the gripper. To this end, we segmented the visible region of the target object from the RGB image and cropped the depth image using a mask. Then, we sampled the point cloud from the depth image using voxel-down sampling \cite{Zhou2018} and fed it to UOP-Net. The model predicted the most stable plane and calculated the rotation value between the plane and the table. Then, the UR5 robot placed the target object on the table. We utilized the BiRRT \cite{qureshi2015intelligent} algorithm implemented with PyBullet \cite{coumans2021} and integrated with collision checking on a physics engine to ensure smooth planar motion.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Table 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% % Table 3
% % Please add the following required packages to your document preamble:
% % \usepackage{graphicx}
% \begin{table}[ht!]
% \caption{\textbf{UOP} performance of the UOP-Net and baselines on YCB\cite{calli2015ycb} in the real world.}
% \label{tab:real world-eval}
% \resizebox{\columnwidth}{!}{%
% \LARGE{
% {\renewcommand{\arraystretch}{}
% \begin{tabular}{|c|cccc|}
% \hline
% YCB\cite{calli2015ycb} & CHSA\cite{haustein2019object} & BBF\cite{mitash2020task} & RPF\cite{fischler1981random} & \begin{tabular}[c]{@{}c@{}}UOP-Net \\ \textbf{(Ours)}\end{tabular} \\ \hline
% Coffee can & 0 / 10 & 0 / 10 & 4 / 10 & \textbf{10} / 10 \\
% Timer & 0 / 10 & 1 / 10 & \textbf{6} / 10 & \textbf{6} / 10 \\
% Power drill & 0 / 10 & 1 / 10 & \textbf{5} / 10 & \textbf{5} / 10 \\
% Wood block & 1 / 10 & 1 / 10 & \textbf{10} / 10 & \textbf{10} / 10 \\
% Metal mug & 0 / 10 & 0 / 10 & 6 / 10 & \textbf{9} / 10 \\
% Metal bowl & \textbf{10} / 10 & 5 / 10 & \textbf{10} / 10 & \textbf{10} / 10 \\
% Bleach cleanser & 3 / 10 & 3 / 10 & \textbf{9} / 10 & \textbf{9} / 10 \\
% Mustard container & 2 / 10 & 0 / 10 & 5 / 10 & \textbf{10} / 10 \\
% Airplane toy & 0 / 10 & 3 / 10 & 0 / 10 & \textbf{4} / 10 \\
% Sugar box & 2 / 10 & 3 / 10 & \textbf{10} / 10 & \textbf{10} / 10 \\
% Chips can & 2 / 10 & 0 / 10 & 8 / 10 & \textbf{10} / 10 \\
% Banana & 5 / 10 & 5 / 10 & \textbf{9} / 10 & \textbf{9} / 10 \\ \hhline{|=|=|=|=|=|}
% Average & 2.1 / 10 & 1.8 / 10 & 6.8 / 10 & \textbf{8.5} / 10 \\ \hline
% \end{tabular}%
% }
% }
% }
% \end{table}




% \noindent\textbf{Evaluation metrics}
% We used \mathcal{SR} as the evaluation metric to evaluate the effectiveness of our approach in real-world scenarios. During the experiments, we instructed the robot to place the object on the table using the plane predicted by UOP-Net. The placement was considered successful when we could visually confirm that the robot placed the object in a fixed position on the predicted plane without the object falling or sliding. The trial was considered a failure if the model failed to detect any stable planes. We conducted a total of 10 placement trials for each object.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Figure 6 %%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure}[ht!]
%     \centering
%         \begin{subfigure}[t]{\columnwidth}
%             \centering
%             \includegraphics[width=\textwidth]{figures/Figure 6/Figure_6.png}
%         \end{subfigure}
        
%     \caption{Visualization of the prediction results for each method on YCB\cite{calli2015ycb} objects and novel objects in the real world.}
%     \label{fig:comparison_pred_res}
% \end{figure}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \noindent\textbf{Comparison with the baselines}
% We selected 12 objects from the YCB dataset. Objects with spherical shapes (e.g., apples), dimensions that were too small, or small depth values were excluded from the test set. Table \ref{tab:real world-eval} indicates that our method outperforms other baselines in terms of the success rate across all objects. Although the real-world perception is noisy, UOP-Net provides a stable plane that can be attributed to our model learning from numerous partial point clouds captured by a depth camera and corrupted by noise. Other benchmarks (CHSA and primitive shape fitting) performed extremely poorly because they could not obtain the complete shape of the object in the real world and were unable to respond to sensor noise. We evaluated our method on novel objects that did not have an available CAD model (a dinosaur figurine and an ice tray, as shown in Fig.\ref{fig:comparison_pred_res}) to further verify that our model can perform as efficiently on unseen objects. UOP-Net detected implicit planes (e.g., the four legs of the dinosaur) although the object shapes were complex. 

% \noindent\textbf{Failure cases}
% Points corresponding to the ground truth are not sampled when the RGB-D sensor fails to capture a specific area of an object (e.g., a dark or glossy part of the object) or when the object and gripper segmentation model fails to predict a clear mask. In such a case, UOP-Net can fail to detect the stable plane. Although we demonstrated that UOP-Net can detect an implicit plane from complex shape objects (e.g., YCB airplane toy or dinosaur), it is still challenging to detect an implicit plane in an unclear point cloud.

% % object (17,408 => train: 13926, val:3482 ) // anno (69,027 => train :55261, val:13766)
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Conclusion %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section{Conclusion and Future Scope} 
% This paper proposed a method called UOP-Net for detecting the stable planes of unseen objects. We introduced the  P-Sim pipeline for generating synthetic datasets, which consisted of 17.4K 3D objects and 69K stable plane annotations. Our model detected stable planes directly from various objects under partial observations using only a synthetic dataset called UOP-Sim. We demonstrated the accuracy and reliability of UOP-Net in detecting stable planes from unseen and partially observable objects on three benchmark datasets with SOTA performance. 













%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Acknowledgement %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgment}
\begin{spacing}{0.4}
{\scriptsize This research was completely supported by a Korea Institute for Advancement of Technology (KIAT) grant funded by the Korean Government (MOTIE) (Project name: Shared autonomy based on deep reinforcement learning for responding intelligently to unfixed environments such as robotic assembly tasks, Project Number: 20008613). This research was also partially supported by an HPC Support Project of the Korea Ministry of Science and ICT and NIPA.}
\end{spacing}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{references.bib}
\bibliographystyle{IEEEtran}

\end{document}

