

\section{Neural Graph Databases}
\label{sec:ngdb}




Traditional databases (including graph databases) are designed around two crucial modules: the storage layer for data and the query engine to process queries over the stored data.
From that perspective, neural database \emph{per se} is not a novel term and many machine learning systems already operate in this paradigm when data is encoded into model parameters and querying is equivalent to a forward pass that can output a new representation or prediction for a downstream task.
One of the first examples of neural databases is \emph{vector databases}. In vector databases, the storage module consists of domain-agnostic vector representations of the data, which can be muliti-modal \eg, text paragraphs or images.
Vector databases belong to the family of storage-oriented systems commonly built around approximate nearest neighbor libraries (ANN) like Faiss~\citep{faiss} or ScaNN~\citep{scann} to answer distance-based queries (like maximum inner product search, MIPS). 
Being encoder-independent (that is, any encoder yielding vector representations can be a source), vector databases lack graph reasoning and complex query answering capabilities. 
Still, ANN systems are a convenient choice for implementing certain layers of Neural Graph Databases as we describe below.

With the recent rise of large-scale pretrained models (\ie, \emph{foundation models}~\citep{Bommasani2021FoundationModels}), we have witnessed their huge success in natural language processing and computer vision tasks. We argue that such foundation models are also a prominent example of neural databases. 
% 
In foundation models, the ``storage module'' might be presented directly with model parameters or outsourced to an external index often used in retrieval-augmented models~\citep{rag, realm, alon2022neuro} since encoding all world knowledge even into billions of model parameters is hard. The ``query module'' performs in-context learning either via filling in the blanks in encoder models (BERT or T5 style) or via prompts in decoder-only models (GPT-style) that can span multiple modalities, \eg, learnable tokens for vision applications~\citep{uvim} or even calling external tools~\citep{mialon2023augmented}. 
Applied to the text modality, \citet{thorne2021acl, thorne2021vldb} devise \emph{Natural Language Databases (NLDB)} where atomic elements are textual facts encoded to a vector via a pre-trained language model (LM). 
Queries to NLDB are sent as natural language utterances that get encoded to vectors and query processing employs the \emph{retriever-reader} approach.
First, a dense neural \emph{retriever} returns a support set of candidate facts, and a fine-grained neural \emph{reader} performs a \emph{join} operation over the candidates (as a sequence-to-sequence task). 

The amount of graph-structured data is huge and spans numerous domains like general knowledge with Freebase~\citep{bollacker2008freebase}, DBpedia~\citep{lehmann2015dbpedia}, Wikidata~\citep{wikidata}, YAGO~\citep{yago}, commonsense knowledge with ConceptNet~\citep{conceptnet}, ATOMIC~\citep{atomic}, and biomedical knowledge such as 
Bio2RDF~\citep{bio2rdf} and PrimeKG~\citep{primekg}. 
%
With the growing sizes, the incompleteness of those graphs grows simultaneously.
At this scale, symbolic methods struggle to provide a meaningful approach to deal with incompleteness. 
Therefore, we argue that neural reasoning and graph representation learning methods are capable of addressing incompleteness at scale while maintaining high expressiveness and supporting complex queries beyond simple link prediction.
We propose to study those methods under the framework of \emph{Neural Graph Databases (\ngdb)}.
The concept of \ngdb extends the ideas of neural databases to the graph domain. \ngdb combines the advantages of traditional graph databases (graphs as a first-class citizen, efficient storage, and uniform querying interface) with modern graph machine learning (geometric and physics-inspired vector representations, ability to work with incomplete and noisy inputs by default, large-scale pre-training and fine-tuning on downstream applications).
In contrast to the work of \citet{lpg2vec} that proposed LPG2Vec, a featurization strategy for labeled property graphs to be then used in standard graph database pipelines, we design the \ngdb concept to be \emph{neural-first}. 

Using the definition of Approximate Graph Query Answering (\autoref{def:approx_gqa}), we define Neural Graph Databases as follows.

\begin{definition}[Neural Graph Database, \ngdb]\label{def:ngdb}
	A Neural Graph Database (see \autoref{fig:ngd}) is a tuple $(S, E, f_{\theta})$.
$S$ is a Neural Graph Storage (see \autoref{sec:storage}), E is a Neural Query Engine (see \autoref{sec:neural_qe}), and $f_{\theta}$ is a
 parameterized Approximate Graph Query Answering function, where $\theta$ represents a set of parameters. 
\end{definition}

In particular, our \ngdb design agenda includes:
\begin{itemize}
    \item The \textbf{data incompleteness assumption}, \ie, the underlying data might have missing information on node-, link-, and graph-levels which we would like to infer and leverage in query answering;
    \item \textbf{Inductiveness and updatability}, \ie, similar to traditional databases that allow updates and instant querying, representation learning algorithms for building graph latents have to be inductive and generalize to unseen data (new entities and relation at inference time) in the zero-shot (or few-shot) manner to prevent costly re-training (for instance, of shallow node embeddings);
    \item \textbf{Expressiveness}, \ie, the ability of latent representations to encode logical and semantic relations in the data akin to FOL (or its fragments) and leverage them in query answering. Practically, the set of supported logical operators for neural reasoning should be close to or equivalent to standard graph database languages like SPARQL or Cypher;
    \item \textbf{Multimodality} beyond KGs, \ie, any graph-structured data that can be stored as a node or record in classical databases (consisting, for example, of images, texts, molecular graphs, or timestamped sequences) and can be imbued with a vector representation is a valid source for the Neural Graph Storage and Neural Query Engine.
\end{itemize}

The key methods to address the \ngdb design agenda are:
\begin{itemize}
    \item \textbf{Vector representation as the atomic element}, \ie, while traditional graph DBs hash the adjacency matrix (or edge list) in many indexes, the incompleteness assumption implies that both given edges \textbf{and} graph latents (vector representations) become the \emph{sources of truth} in the \emph{Neural Graph Storage};
    \item \textbf{Neural query execution in the latent space}, \ie, basic operations such as edge traversal cannot be performed solely symbolically due to the incompleteness assumption. Instead, the \emph{Neural Query Engine} operates on both adjacency and graph latents to incorporate possibly missing data into query answering;
\end{itemize}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.7\linewidth]{figs/NGDB_v6.pdf}
	\caption{A conceptual scheme of Neural Graph Databases. An input query is processed by the \emph{Neural Query Engine} where the \emph{Planner} derives a computation graph of the query and the \emph{Executor} executes the query in the latent space. \emph{Neural Graph Storage} employs \emph{Graph Store} and \emph{Feature Store} to obtain latent representations in the \emph{Embedding Store}. The \emph{Executor} communicates with the embedding store via the \emph{Approximate Graph Query Answering Function $f_\theta$ (\autoref{def:ngdb})} to retrieve and return results.}
	\label{fig:ngd}
\end{figure}

A conceptual scheme of \ngdb is presented in \autoref{fig:ngd}. 
On a higher level, \ngdb contains two main components, \emph{Neural Graph Storage} and \emph{Neural Query Engine} that we describe in the following \autoref{sec:storage} and \autoref{sec:neural_qe}, respectively.
The processing pipeline starts with the query sent by some application or downstream task already in a structured format (obtained, for example, via semantic parsing~\citep{drozdov2022compositional} if an initial query is in natural language).
The query first arrives to the \emph{Neural Query Engine}, and, in particular, to the \emph{Query Planner} module. 
The task of the Query Planner is to derive an efficient computation graph of atomic operations (\eg, projections and logical operations)  with respect to the query complexity, prediction tasks, and underlying data storage such as possible graph partitioning. We elaborate on similarities and differences of the planning mechanism to standard query planners in classic databases in \autoref{sec:neural_qe}.
The derived plan is then sent to the \emph{Query Executor} that encodes the query in a latent space, executes the atomic operations over the underlying graph and its latent representations, and aggregates the results of atomic operations into a final answer set.
The execution is done via the \emph{Retrieval} module that communicates with the \emph{Neural Graph Storage}. 
The storage layer consists of (1) \emph{Graph Store} for keeping the multi-relational adjacency matrix in space- and time-efficient manner (\eg, in various sparse formats like COO and CSR; (2) \emph{Feature Store} for keeping node- and edge-level multimodal features associated with the underlying graph; 
(3) \emph{Embedding Store} that leverages an \emph{Encoder} module to produce graph representations in a latent space based on the underlying adjacency and associated features. 
%
The Retrieval module queries the encoded graph representations to build a distribution of potential answers to atomic operations. 
In the following subsections, we describe the Neural Graph Storage and Neural Query Engine in more detail.

\subsection{Neural Graph Storage}
\label{sec:storage}

In traditional graph databases, storage design often depends on the graph modeling paradigm. 
The two most popular paradigms are Resource Description Framework (RDF) graphs~\citep{brickley2014rdf} and Labeled Property Graphs (LPG)~\citep{lpg2vec}.
While the detailed comparison between those frameworks is out of scope of this work, the principal difference consists in that RDF is a triple-based model allowing for some formal \emph{semantics} suitable for symbolic reasoning whereas LPG allows literal attributes over edges but has no formal semantics.
We posit that the new RDF-star paradigm~\citep{hartig2022rdfstar} would be a convergence point of graph modeling combining the best of both worlds, \ie, attributes over edges (enabling other nodes and relations to be in key-value attributes) together with expressive formal semantics. 
We note that hyper-relational KGs (\autoref{def:hyper_relational_kg}) and Wikidata Statement Model~\citep{wikidata} are conceptually close to the RDF-star paradigm.
%
On a physical level, graph databases store edges employing various indexes and data structures optimized for read (\eg, B+ trees~\citep{rdf3x} or HDT~\citep{hdt}) or write applications (such as LSM trees~\citep{sagi2022design}).

\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\linewidth]{figs/ngdb_storages_v2.pdf}
	\caption{The storage and execution pipeline of \ngdb (left) and traditional databases (right). Traditional DBs store the graph in a collection of lookup indexes and each query pattern from the query tree plan is answered by some of those indexes. In {\ngdb}s, due to graph incompleteness, the graph and its features are first encoded in a latent space. Queries (or their atomic patterns) are encoded in a latent space either and probed using the Retrieval module (\eg, can be implemented with MIPS).}
	\label{fig:ngdb_storage}
\end{figure}

However, unlike the above methods, we propose the concept of a \emph{neural} graph storage (\autoref{fig:ngdb_storage}) where both the input graph and its vector representations are sources of truth. 
Physically, it consists of the (1) \emph{Graph Store} that stores a (multi-relational) adjacency matrix that in the basic form can be implemented with sparse COO, CSR, or more efficient compressed formats, (2) \emph{Feature Store} that stores node- and edge-level feature vectors of various formats and modalities, \eg, numerical, categorical, text data, or already given vectors.
(3) \emph{Embedding Store} that leverages an \emph{Encoder} to produce graph representations in a latent space based on the underlying adjacency and associated features. 
%  
The embedding store is one of the biggest differences between neural graph storage and its counterpart in traditional graph DBs.
The embedding process can be viewed as a compression step but the semantic and structure similarity of entities/relations is kept. The distance between entities/relations in the embedding space should be positively correlated with the semantic/structure similarity. There are several options for the architecture of the encoder.

First, we may implement the encoder as a matrix lookup, of which each row corresponds to the embedding of an entity/relation. The benefit of such modeling is that it provides the most flexibility and the most number of free parameters -- any entry of the embedding matrix is trainable. However, it comes at the cost that it is challenging to edit the storage. We cannot easily add new entities/relations to the storage because any novel entity/relation may need a new embedding that requires learning from scratch. If we remove an entity and later add it back, such embedding matrix modeling also cannot recover the original learned embedding.

Another option for the encoder is a graph neural network (GNN) that can be analyzed through the lens of message passing and neighborhood aggregation (\autoref{sec:grl}). The idea is that we may learn the entity/relation embeddings by aggregating its neighbor information. The GNN parameters are shared for all the entities and relations on the graph. 
Such modeling provides clear benefit with much better generalization capability then shallow embedding matrices.


One important process in the neural graph storage is the \textbf{retrieval} step. 
%
Traditional graph databases directly perform identity-based exact match retrieval from the indexes.
In contrast, in a neural graph storage, since we store each entity and relation in the latent space, besides performing retrieval by the entity/relation id, users can also input an vector, and the retrieval process may return ``relevant'' entities/relations by measuring the distance in the embedding space. The retrieval process can be seen as a nearest neighbor search of the input vector in the embedding space. There are three direct benefits of a neural graph storage compared with traditional storages. The first advantage is that since the retrieval is operated in the embedding space with a predefined distance function, each retrieved item naturally comes with a score which may represent the confidence/relevance of the input vector in a retrieval step. 
Besides, NGDB allows for different definitions of the latent space and the distance function (which will be detailed in \autoref{sec:models}), such that NGDB is  flexible and users may customize the latent space and distance function based on different desired properties and user need.
Lastly, with the whole literature on efficient nearest neighbor search, we have the opportunity to implement the retrieval step on extremely large graphs with billions of nodes and edges with high efficiency and scalability. Existing frameworks including Faiss~\citep{faiss}, ScaNN~\citep{scann} provide scalable implementation of nearest neighbor search. 
%Note one 
A major limitation, however, is that existing frameworks are only applicable to L1, L2 and cosine distance functions. It is still an open research problem how to design efficient scalable nearest neighbor search algorithms for more complex distance functions such as KL divergence so that we can retrieve with much better efficiency for different \clqa methods.

\subsection{Neural Query Engine}
\label{sec:neural_qe}


In traditional databases, a typical query engine~\citep{rdf3x, endris2018querying} performs three major operations. (1) Query parsing to verify syntax correctness (often enriched with a deeper semantic analysis of query terms); (2) Query planning and optimization to derive an efficient query plan (usually, a tree of relational operators) that minimizes computational costs; (3) Query execution that scans the storage and processes intermediate results according to the query plan.

In {\ngdb}s, the sources of truth are both graph adjacency (possibly incomplete) and latent graph representations (\eg, vector representations of entities and relations). 
% 
Similarly, queries (or their atomic operations) are encoded into a latent space. 
To answer encoded queries over latent spaces, we devise \emph{Neural Query Engines} that include two modules: (1) \emph{Query Planner} to derive an efficient query plan of atomic operations (\eg, projections and logical operators) maximizing completeness (all answers over existing edges must be returned) and inference (of missing edges predicted on the fly) taking into account query complexity, prediction tasks, and partitioning of sources; (2) \emph{Query Executor} that encodes the derived plan in a latent space, executes the atomic operations (or the whole query) over the graph and its latent representations, and aggregates intermediate results into a final answer set with possible postprocessing.



Broadly in Deep Learning, prompting large language models (LLMs) can be seen as a form of a neural query engine where queries are  unstructured texts. 
Recent prompting techniques like Chain of Thought~\citep{chain_of_thought} or Program of Thought~\citep{program_of_thought} achieved remarkable progress in natural language reasoning~\citep{prompt_survey} by providing few prompts with ``common sense'' examples of solving a given problem step-by-step. 
Such step-by-step instructions resemble a  \emph{query plan} designed manually by a prompt engineer and optimized for \emph{query execution} against a particular LLM where query execution  is framed as the language model objective (predicting next few tokens). 
Prompting often does not require any additional LLM fine-tuning or training and works in the inference regime (often called \emph{in-context learning}). 
Pre-trained LLMs can thus be seen as \emph{neural databases} that can be queried with a sequence of prompts organized in a domain-specific query language~\citep{lmql}.
Recently, a similar prompting technique~\citep{drozdov2022compositional} demonstrated strong systematic generalization skills by solving a long-standing semantic parsing task of converting natural language questions to SPARQL queries. 
Several major drawbacks of such querying techniques are: (1) a limited context window (usually, less than 8192 tokens) that does not allow prompting the whole database content; (2) issues with factual correctness and hallucinating of the generated reponses.


\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\linewidth]{figs/ngdb_query_engines_v3.pdf}
	\caption{Query planning in {\ngdb}s (left) and traditional graph DBs (right). The \ngdb planning (assuming incomplete graphs) can be performed autoregressively step-by-step (1) or generated entirely in one step (2). The traditional DB planning is cost-based and resorts to metadata (assuming complete graphs and extracted from them) such as the number of intermediate answers to build a tree of join operators.
 }
	\label{fig:ngdb_query_planning}
\end{figure}

\paragraph{Query Planner} 
In {\ngdb}s, an input query is expected to arrive in the structured form, \eg, as generic as FOL -- we leave the discussion on possible query languages out of scope of this work emphasizing the breadth of general and domain-specific languages. 
For illustrating example queries, however, we use the SPARQL notation as one of the standard query languages for graph databases. 
Broadly, principal differences between symbolic and neural query planning are illustrated in \autoref{fig:ngdb_query_planning}.
While traditional graph DBs are symbolic and deductive, {\ngdb}s are neural and inductive (\ie, can generalize to unseen data). 
As we assume the graph data is incomplete, neural reasoning is expected to process more intermediate results that makes query planning even more important in deriving the most efficient plan.

The task of the Query Planner is to optimize the execution process by deriving the most efficient query plan, \ie, the execution order of atomic and logical operations, given the query complexity, expected prediction task, and configuration of the storage.
%
We envision the storage configuration to play an important role in very large graphs that cannot be stored entirely in the main memory. 
A typical configuration would 
resort to partitioning of graphs and latent representations across many devices in a distributed fashion. 
Therefore, the planner has to identify which parts of the input query to send to a relevant partition. 
Training and inference of ML models in distributed environments (with possible privacy restrictions) is at the core of \emph{federated learning}~\citep{li2020federated} and \emph{differential privacy}~\citep{diff_privacy} and we posit they would be important components of very large {\ngdb}s.



Common metrics of evaluating the quality of plans are \emph{execution time} (in units of time) and \emph{throughput} (number of queries per unit of time). 
In traditional graph DBs, different query plans return the same answers but require different execution time ~\citep{endris2018querying} due to the order of join operators (\eg, nested loop joins or hash joins), the number of calls to indexes, and intermediate results to process.
%
In contrast, {\ngdb}s execute queries in the latent space together with link prediction and, depending on the approach, time complexity of atomic operations (relation projection and logical operators) might depend on the hidden dimension $\gO(d)$ or the number of entities $\gO(\gE)$ (we elaborate on complexity in \autoref{sec:complexity}).


To date, query planning is still a challenge for neural query answering methods as all existing approaches (\autoref{sec:learning}) execute an already given sequence of operators and all existing datasets (\autoref{sec:datasets}) provide such a hardcoded sequence without optimizations. 
Furthermore, some existing approaches, \eg, CQD~\citep{cqd}, are susceptible to the issue when changing a query plan might result in a different answer set.


We hypothesize that principled approaches for deriving efficient query plans taking into account missing edges and incomplete graphs might be framed as \emph{Neural Program Synthesis}~\citep{neural_program_synth} often used to derive query plans for LLMs to solve complex numerical reasoning tasks~\citep{neural_module_nets, nerd}.
In the complex query answering domain, the first attempt to apply neural program synthesis for generating query plans was made by the Latent Execution-Guided Reasoning (LEGO) framework~\citep{lego}. 
LEGO iteratively expands the initial query tree root by sampling from the space of relation projection and logical operators. 
The sampling is based on learnable heuristics and pruning mechanisms. 


\paragraph{Query Executor} 
Once the query plan is finalized, the \emph{Query Executor} module encodes the query (or its parts) into a latent space, communicates with the Graph Storage and its Retrieval module, and aggregates intermediate results into the final answer set. Following the \autoref{def:easy_hard_ans}, there exists easy answers and hard answers. Query Executors are expected to return the compelte set of easy answers and recover missing edges to return the missing hard answers.

There exist two common mechanisms for neural query execution described in \autoref{sec:learning}: 
%
(1) \emph{atomic}, resembling traditional DBs, when a query plan is executed sequentially by encoding atomic patterns (such as relation projections), retrieving their answers, and executing logical operators as intermediate steps; (2) \emph{global}, when the entire query graph is encoded and executed in a latent space in one step. 
For example (\autoref{fig:ngdb_storage}), given a query plan $q = U_? . \exists V: \textit{win}(\mathtt{TuringAward}, V) \land \textit{field}(\mathtt{DeepLearning}, V) \land \textit{university}(V, U_?)$, the atomic mechanism executes separate relation projections, \eg, $\textit{win}(\mathtt{TuringAward}, V)$, sequentially while the global mechanism encodes the whole query graph and probes it against the latent space of the graph.

Direct implications of the chosen mechanism include computational complexity (\autoref{sec:complexity}) and supported logical operators (\autoref{sec:queries}), \ie, fully latent mechanisms are mostly limited to conjunctive and intersection queries and have worse generalization qualities. 
To date, most methods follow the atomic mechanism.



The main challenge for neural query execution is matching query expressiveness to that of symbolic languages like SPARQL or Cypher.
The challenge includes three aspects:
(1) handling more expressive query operators such as \texttt{FILTER} or aggregations like \texttt{COUNT} or \texttt{SUM}; (2) supporting  query patterns more complex than trees; (3) supporting several projected variables and operations on them.
We elaborate in \autoref{sec:queries}.


\paragraph{A Taxonomy of Query Reasoning Methods}
\label{sec:taxonomy}

In the following sections, we devise a taxonomy of query answering methods as a component of the \emph{Neural Query Engine (NQE)}. 
We categorize existing and future approaches along three main directions: (i) \textbf{Graphs} -- what is the underlying structure against which we answer queries; (ii) \textbf{Modeling} -- how we answer queries and which inductive biases are employed; (iii) \textbf{Queries} -- what we answer, what are the query structures and what are the expected answers. 
The taxonomy is presented in \autoref{fig:taxonomy}. 
In the following sections, we describe each direction in more detail and illustrate them with examples covering the whole existing literature on complex query answering (more than 40 papers).
