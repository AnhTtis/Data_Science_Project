\section{Summary and Future Opportunities}
\label{sec:future}

We presented a summary of the literature of the \longclqa (\clqa) task, which is at the core of our proposed neural query engine and neural graph database (NGDB) concepts. {\ngdb}s marry the idea of neural database and graph database where we have a unique latent space for information (including entities, relations, facts). NGDBs perform query planning, execution, and result retrieval all in the latent space using graph representation learning. Such a design choice allows for a unified, more efficient and robust interface for storage and querying. We envision {\ngdb}s to be the next generation of databases tailored for predictive queries over sheer volumes of incomplete graph-structured data.

We also proposed a deep and detailed taxonomy of methods 
tackling certain aspects of the envisioned neural query engine.
Going forward, there is still much room to unlock the full power of NGDB by addressing the open challenges in Neural Query Engines and Neural Storage domains. Pertaining to Neural Query Engines and adhering to the taxonomy in \autoref{sec:taxonomy}, we summarize the challenges in three main areas: Graphs, Modeling, and Queries. 

Along the \textbf{Graph} branch:
\begin{itemize}
    \item \textbf{Modality:} Supporting more graph modalities: from classic triple-only graphs to hyper-relational graphs, hypergraphs, and multimodal sources combining graphs, texts, images, and more.
    \item \textbf{Reasoning Domain:} Supporting logical reasoning and neural query answering over temporal and continuous (\eg, textual and numerical) data -- literals constitute a major portion of graphs as well as relevant queries over literals.
    \item \textbf{Background Semantics:} Supporting complex axioms and formal semantics that encode higher-order relationships between (latent) classes of entities and their hierarchies, \eg, enabling neural reasoning over description logics and OWL fragments.
\end{itemize}

In the \textbf{Modeling} branch:
\begin{itemize}
    \item \textbf{Encoder:} Inductive encoders supporting unseen relation at inference time -- this a key for (1) \textbf{updatability} of neural databases without the need of retraining; (2) enabling the \textbf{pretrain-finetune} strategy generalizing query answering to custom graphs with custom relational schema.
    \item \textbf{Processor:} Expressive processor networks able to effectively and efficiently execute complex query operators akin to SPARQL and Cypher operators. Improving sample efficiency of neural processors is crucial for the \emph{training time vs quality} tradeoff, \ie, reducing training time while maintaining high predictive qualities. 
    \item \textbf{Decoder:} So far, all neural query answering decoders operate exclusively on discrete nodes. Extending the range of answers to continuous outputs is crucial for answering real-world queries.
    \item \textbf{Complexity:} As the main computational bottleneck of processor networks is the dimensionality of embedding space (for purely neural models) and/or the number of nodes (for neuro-symbolic), new efficient algorithms for neural logical operators and retrieval methods are the key to scaling {\ngdb}s  to billions of nodes and trillions of edges.  
\end{itemize}

In \textbf{Queries}:

\begin{itemize}
    \item \textbf{Operators:} Neuralizing more complex query operators matching the expressiveness of declarative graph query languages, e.g., supporting Kleene plus and star, property paths, filters.
    \item \textbf{Patterns:} Answering more complex  patterns beyond tree-like queries. This includes DAGs and cyclic graphs.
    \item \textbf{Projected Variables:} Allowing projecting more than a final leaf node entity, that is, allowing returning intermediate variables, relations, and multiple variables organized in tuples (bindings).
    \item \textbf{Expressiveness:} Answering queries outside simple EPFO and EFO fragments, extending the expressiveness to the union of conjunctive queries with negations (UCQ and $\text{UCQ}_{\text{neg}}$ and aiming for the expressiveness of database languages. 
\end{itemize}


In \textbf{Datasets} and \textbf{Evaluation}:

\begin{itemize}
    \item The need for larger and diverse \textbf{benchmarks} covering more graph modalities, more expressive query semantics, more query operators, and query patterns.
    \item As the existing evaluation protocol appears to be limited (focusing only on inferring \emph{hard} answers) there is a need for a more principled \textbf{evaluation framework} and \textbf{metrics} covering various aspects of the query answering workflow.
\end{itemize}

Pertaining to the Neural Graph Storage and {\ngdb} in general, we identify the following challenges:

\begin{itemize}
    \item The need for a \textbf{scalable retrieval} mechanism to scale neural reasoning to graphs of billions of nodes. Retrieval is tightly connected to the \emph{Query Processor} and its modeling priors (as shown in \autoref{sec:storage}). Existing scalable  ANN libraries can only work with basic L1, L2, and cosine distances that limit the space of possible processors in the neural query engine.
    \item Currently, all complex query datasets listed in \autoref{sec:datasets} provide a hardcoded query execution plan that might not be optimal for neural processors. There is a need for a \textbf{neural query planner} that would transform an input query into an optimal execution sequence taking into account prediction tasks, query complexity, type of the neural processor, and configuration of the Storage layer (that can be federated as well).
    \item Due to encoder inductiveness and updatability without retraining, there is a need to alleviate the issues of \textbf{continual learning}~\citep{cont_learning_thrun, cont_learning_ring}, \textbf{catastrophic forgetting}~\citep{mccloskey1989catastrophic}, and \textbf{size generalization} when running inference on much larger graphs than training ones~\citep{DBLP:conf/icml/YehudaiFMCM21,buffelli2022sizeshiftreg}.
\end{itemize}

Finally, in the era of foundation models, many research works have demonstrated various capabilities hidden inside these large language models often with hundreds of billions of parameters, and most importantly how to unleash these capabilities \citep{chain_of_thought,wang2022self,zhou2022least}. We envision an important future direction that is to design a natural language interface so that we can better harness the reasoning capabilities of these large language models for the \clqa task along the directions we mentioned ahead \citep{drozdov2022compositional}. Besides, NGDB also provides an exciting future opportunity to improve foundation models at various stages, especially inference. Since not all the downstream tasks require a full billion-parameter model call, it's promising to research how NGDB can accelerate or compress foundation models while still keeping the emergent behaviors that are essential for these extremely large models.
