\section{Datasets and Metrics}
\label{sec:datasets}

\begin{figure}[!t]
	\centering
    \includegraphics[width=0.7\textwidth]{figs/query.pdf}
	\caption{Standard query patterns with names, where \emph{p} is projection, \emph{i} is intersection, \emph{u} is union, \emph{n} is negation. In a pattern, blue node represents a non-variable entity, grey node represents a variable node, and the green node represents the answer node. In a typical training protocol, models are trained on 10 patterns (first and third rows) and evaluated on all patterns. In the hardest generalization case, models are only trained on \emph{1p} queries. Some datasets further modify the patterns with additional features like qualifiers or temporal timestamps.}
	\label{fig:query_graph}
\end{figure}


\subsection{Evaluation Setup}
\label{sec:evaluation}

Multiple datasets have been proposed for evaluation of query reasoning models. Here we introduce the common setup for \clqa task. Given a knowledge graph $\gG=(\gE,\gR,\gS)$, the standard practice is to split $\gG$ into a training graph $\gtrain$, a validation graph $\gval$ and a test graph $\gtest$ (simulating the unobserved complete graph $\hat{\gG}$ from \autoref{sec:prelim}). The standard experiment protocol is to train a query reasoning model only on the training graph $\gtrain$, and evaluate the model on answering queries over the validation graph $\gval$ and the test graph $\gtest$. Given a query $q$, denote the answers of this query on training, validation and test graph as $\dqtr$, $\dqv$ and $\dqte$. During evaluation, queries may have missing answers, \eg, a validation query $q$ may have answers $\dqv$ that are not in $\dqtr$, a test query $q$ may have answers $\dqte$ that are not in $\dqv$. 
The overall goal of \clqa task is to find these missing answers.
The details of typical training queries, training protocol, inference and evaluation metrics are introduced in \autoref{sec:dataset_query_types}, \autoref{sec:graph_training}, \autoref{sec:graph_inference}, and \autoref{sec:metrics}, respectively.

\subsection{Query Types}
\label{sec:dataset_query_types}

The standard set of graph queries used in many datasets includes 14 types: \emph{1p/2p/3p/2i/3i/ip/pi/2u/up/2in/3in/inp/pni/pin} where \emph{p} denotes relation projection, \emph{i} is intersection, \emph{u} is union, \emph{n} is negation, and a number denotes the number of hops for projection queries or number of branches to be merged by a logical operator. 
\autoref{fig:query_graph} illustrates common query patterns. 
For example, \emph{3p} is a chain-like query of three consecutive relation projections, \emph{2i} is an intersection of two relation projections, \emph{3in} is an intersection of three relation projections where one of the branches contains negation, \emph{up} is a union of two relation projections followed by another projection.
The original GQE by \citet{gqe} introduced 7 query patterns with projection and intersection \emph{1p/2p/3p/2i/3i/ip/pi}, Query2Box~\citep{q2b} added union queries \emph{2u/up}, and BetaE~\citep{betae} added five types with negation.

Subsequent works modified the standard set of query types in several ways, \eg, hyper-relational queries~\citep{starqe,nqe} with entity-relation qualifiers on relation projections, or temporal operators on edges~\citep{lin2022tflex}. 
New query patterns include queries with regular expressions of relations (property paths)~\citep{regex}, more tree-like queries~\citep{biqe}, and more combinations of projections, intersections, and unions~\citep{wang2021benchmarking, gnnq}.
We summarize existing query answering datasets and their properties in \autoref{tab:datasets1} covering supported query operators, inference setups, and additional features like temporal timestamps, class hierarchies, or complex ontological axioms.

Commonly, query datasets are sampled from different KGs to study model performance under different graph distributions, for example, BetaE datasets include sets of queries from denser Freebase~\citep{bollacker2008freebase} with average node degree of 18 and sparser WordNet~\citep{miller1998wordnet} and NELL~\citep{nell} with average node degree of 2. 
Hyper-relational datasets WD50K~\citep{starqe} and WD50K-NFOL~\citep{nqe} were sampled from Wikidata~\citep{wikidata} where qualifiers are natural.
TAR datasets with class hierarchy~\citep{abin_abductive2022} were sampled from YAGO 4~\citep{yago} and DBpedia~\citep{lehmann2015dbpedia} where class hierarchies are well-curated.
Q2B Onto datasets with ontological axioms~\citep{q2b_onto} were sampled from LUBM~\citep{guo2005lubm} and NELL. 
Temporal TFLEX datasets~\citep{lin2022tflex} were sampled from ICEWS~\citep{icews_dataset} and GDELT~\citep{leetaru2013gdelt} that maintain event information.
InductiveQE datasets~\citep{galkin2022} were sampled from Freebase and Wikidata, while inductive GNNQ datasets~\citep{gnnq} were sampled from the WatDiv benchmark~\citep{watdiv} and Freebase.

\subsection{Training}
\label{sec:graph_training}
Query reasoning methods are trained on the given $\gtrain$ with different objectives/losses and different datasets. Following the standard protocol, methods are trained on 10 query patterns \emph{1p/2p/3p/2i/3i/2in/3in/inp/pni/pin} and evaluated on all 14 patterns including generalization to unseen \emph{ip/pi/2u/up} patterns. That is, the training protocol assumes that models trained on atomic logical operators would learn to compositionally generalize to patterns using several operators such as \emph{ip} and \emph{pi} queries that use both intersection and projection. 

We summarize different training objectives in \autoref{tab:loss}. Most methods that learn a representation of the queries and entities on the graph optimize a contrastive loss, \ie, minimizing the distance between the representation of a query $q$ and its positive answers $e$ while maximizing that between the representation of a query and negative answers $e'$. Various objectives include: (1) max-margin loss (first column in \autoref{tab:loss}) with the goal that the distance of negative answers should be larger than that of positive answers at least by the margin $\gamma$. Such loss is often of the form as the equation below.
$$
\ell = \max(0, \gamma - \texttt{dist}(\Em{q}, \Em{e}) + \texttt{dist}(\Em{q}, \Em{e'}));
$$
(2) LogSigmoid loss (second column in \autoref{tab:loss}) with a similar goal that pushes the distance of negatives up and vice versa. Often the loss also includes a margin term and the gradient will gradually decrease when the margin is satisfied.
$$
\ell = -\log\sigma (\gamma - \texttt{dist}(\Em{q}, \Em{e})) - \sum \frac{1}{k} \log\sigma(\texttt{dist}(\Em{q}, \Em{e'}) - \gamma),
$$
where $k$ is the number of negative answers.
Other methods (third column in \autoref{tab:loss}) that directly model a logit vector over all the nodes on the graph may optimize a cross entropy loss instead of a contrastive loss.
Besides, methods such as the two variants of CQD~\citep{cqd} or QTO~\citep{qto} only optimize the link prediction loss since they do not learn a representation of the query.

\input{tables/tab_loss.tex}

Almost all the datasets including GQE~\citep{gqe}, Q2B~\citep{q2b}, BetaE~\citep{betae}, RegEx~\citep{regex}, BiQE~\citep{biqe}, Query2Onto~\citep{q2b_onto}, TAR~\citep{abin_abductive2022}, StarQE~\citep{starqe}, GNNQ~\citep{gnnq}, TeMP~\citep{temp2022}, TFLEX~\citep{lin2022tflex}, InductiveQE~\citep{galkin2022}, SQE~\citep{sqe} provide a set of training queries of given structures sampled from the $\gtrain$. The benefit is that during training, methods do not need to sample queries online. However, it often means that only a portion of information is utilized from the $\gtrain$ since exponentially more multi-hop queries exist on $\gtrain$ and the dataset can never pre-generate all offline. SMORE~\citep{smore} proposes a bidirectional online query sampler such that methods can directly do online sampling efficiently without the need to pre-generate a training set offline. Alternatively, methods that do not have parameterized ways to handle logical operations, \eg, CQD~\citep{cqd}, only require one-hop edges to train the overall system.  


\subsection{Inference}
\label{sec:graph_inference}

\input{tables/tab_datasets}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=\linewidth]{figs/nqe_graph_inference.pdf}
	\caption{Inference Scenarios. In the \emph{Transductive} case, training and inference graphs are the same and share the same nodes ($\einf = \etrain$). \emph{Inductive} cases can be split into \emph{superset} where the inference graph extends the training one ($\etrain \subseteq \einf$, missing links cover both seen and unseen nodes) and \emph{disjoint}  where the inference graph is disconnected ($\etrain \cap \einf = \emptyset$, missing links are among unseen nodes).}
	\label{fig:graph_inference}
\end{figure}

By \emph{Inference} we understand testing scenarios on which a trained query answering model will be deployed and evaluated. 
Following the literature, we distinguish \emph{Transductive} and \emph{Inductive} inference (\autoref{fig:graph_inference}). In the transductive case, inference is performed on the graph with the same set of nodes and relation types as in training but with different edges. Any other scenario when either the number of nodes or relation types of an inference graph is different from that of the training is deemed inductive. 
The inference scenario plays a major role in designing query answering models, that is, transductive models can learn a shallow entity embedding matrix thanks to the fixed entity set whereas inductive models have to rely on other \emph{invariances} available in the underlying graph in order to generalize to unseen entity/relation types. 
We discuss many transductive and inductive models in \autoref{sec:models}. 
Below, we categorize existing datasets from the \emph{Inferece} perspective.
The overview of existing \clqa datasets is presented in \autoref{tab:datasets1} through the lens of supported query operators, inference scenario, graph domain, and other features like types or qualifiers.

\paragraph{Transductive Inference.}
Formally, given a training graph $\gtrain = (\etrain, \rtrain, \strain)$, the transductive inference graph $\ginf$~\footnote{Below we use $\ginf$ to refer to the graphs we use during inference, it can be $\gval$ or $\gtest$ without loss of generalization.} contains the same set of entities and relation types, that is, $\etrain = \einf$ and $\rtrain=\rinf$, while the edge set on $\gtrain$ is a subset of that on the inference graph $\ginf$, \ie, $\strain\subset\sinf$.
In this setup, query answering is performed on the same nodes and edges seen during training. 
From the entity set perspective, the prediction pattern is \emph{seen-to-seen} -- missing links are predicted between known entities. 

Traditionally, KG link prediction focused more on the transductive task. In \clqa, therefore, the majority of existing datasets (\autoref{tab:inference}) follow the transductive scenario. Starting from simple triple-based graphs with fixed query patterns in GQE datasets~\citep{gqe}, Query2Box datasets~\citep{q2b}, and BetaE datasets~\citep{betae} that became de-facto standard benchmarks for query answering approaches, newer datasets include  regex queries~\citep{regex}, wider set of query patterns~\citep{biqe, wang2021benchmarking, sqe}, entity type information~\citep{abin_abductive2022}, ontological axioms~\citep{q2b_onto}, hyper-relational queries with qualifiers~\citep{starqe,nqe}, temporal queries~\citep{lin2022tflex}, or very large graphs up to 100M nodes~\citep{smore}.  

\paragraph{Inductive Inference.} 
Formally, given a training graph $\gtrain = (\etrain, \rtrain, \strain)$, the inductive inference graph $\ginf =(\einf, \rinf, \sinf) $ is different from the training graph in either the entity set or the relation set or both. 
The nature of this difference explains several subtypes of inductive inference. 
First, the set of relations might or might not be shared at inference time, that is, $\rinf \subseteq \rtrain$ or $|\rinf \setminus \rtrain| > 0$. 
Most of the literature on inductive link prediction~\citep{teru2020inductive,zhu2021neural,galkin2022nodepiece} in KGs assumes the set of relations is shared whereas the setup where new relations appear at inference time is still highly non-trivial~\citep{huang2022fewshot,gao2023double,chen2023generalizing}.

On the other hand, the inference graph might be either a superset of the training graph after adding new nodes and edges, $\etrain \subseteq \einf$, or a disjoint graph with completely new entities as a disconnected component, $\einf \cap \etrain = \emptyset$ as illustrated in \autoref{fig:graph_inference}.
From the node set perspective, the superset inductive inference case might contain both \emph{unseen-to-seen} and \emph{unseen-to-unseen} missing links whereas in the disjoint inference graph only \emph{unseen-to-unseen} links are naturally appearing.

In \clqa, inductive reasoning is still an emerging area as it has a direct impact on the space of possible variables $\gV$, constants $\gC$, and answers $A$ that might now include entities unseen at training time. 
Several most recent works started to explore inductive query answering (\autoref{tab:inference}). 
InductiveQE datasets~\citep{galkin2022} focus on the inductive superset case where a training graph can be extended with up to 500\% new unseen nodes. Test queries start from unseen constants and answering therefore requires reasoning over both seen and unseen nodes. Similarly, training queries can have many new correct answers when answered against the extended inference graph.
GNNQ datasets~\citep{gnnq} focus on the disjoint inductive inference case where constants, variables, and answers all belong to a new entity set.
TeMP datasets~\citep{temp2022} focus on the disjoint inductive inference as well but offer to leverage an additional class hierarchy as a learnable \emph{invariant}. That is, the set of classes at training and inference time does not change.

As stated in \autoref{sec:ngdb}, inductive inference is crucial for NGDBs to enable running models over updatable graphs without retraining. 
We conjecture that inductive datasets and models are likely to be the major contribution area in the future work.

\input{tables/tab_inference}



\subsection{Metrics}
\label{sec:metrics}

Several metrics have been proposed to evaluate the performance of query reasoning models that can be broadly classified into \textbf{generalization}, \textbf{entailment}, and \textbf{query representation quality} metrics.

\paragraph{Generalization Metrics.}
Since the aim of query reasoning models is to perform reasoning over massive incomplete graphs, 
most metrics are designed to evaluate models' \emph{generalization} capabilities in discovering missing answers, \ie, $\dqte \backslash \dqv$ for a given test query $q$.
As one of the first works in the field, GQE~\citep{gqe} proposes ROC-AUC and average percentile rank (APR). The idea is that for a given test query $q$, GQE calculates a score for all its missing answers $e\in\dqte\backslash\dqv$ and the negatives $e'\notin \dqte$. The model's performance is the ROC-AUC score and APR, where they rank a missing answer against at most 1000 randomly sampled negatives of the same entity type. Besides GQE, GQE+hashing~\citep{gqe_bin}, CGA~\citep{cga} and TractOR~\citep{tractor2020} use the same evaluation metrics.

However, the above metrics do not reflect the real world setting where we often have orders of magnitude more negatives than the missing answers. Instead of ROC-AUC or APR, Query2Box~\citep{q2b} proposes ranking-based metrics, such as mean reciprocal rank (MRR) and hits@$k$. Given a test query $q$, for each missing answer $e\in\dqte\backslash\dqv$, we rank it against all the other negatives $e'\notin \dqte$. Given the ranking $r$, MRR is calculated as $\frac{1}{r}$ and hits@$k$ is $1[r\leq k]$. This has been the most used metrics for the task.
Note that the final rankings are computed only for the \emph{hard} answers that require predicting at least one missing link. 
Rankings for \emph{easy} answers reachable by edge traversal are usually discarded.

\paragraph{Representation Quality Metrics.}
Besides evaluating model's capability of finding missing answers, another aspect is to evaluate the quality of the learned query representation for all models. BetaE~\citep{betae} proposes to evaluate whether the learned query representation can model the cardinality of a query's answer set, and view this as a proxy of the quality of the query representation. For models with a sense of ``volume'' (\eg, differential entropy for Beta embeddings), the goal is to measure the Spearman's rank correlation coefficient and Pearson's correlation coefficient between the ``volume'' of a query (calculated from the query representation) and the cardinality of the answer set. BetaE also proposed to evaluate an ability to model queries without answers using ROC-AUC. 

\paragraph{Entailment Metrics.}
The other evaluation protocol is about whether a model is also able to discover the existing answers, \eg, $\dqv$ for test queries, that does not require inferring missing links but focuses on memorizing the graph structure (\emph{easy} answers in the common terminology). 
This is referred to as faithfulness (or \emph{entailment}) in EmQL~\citep{emql}. 
Natural for database querying tasks, it is expected that query answering models first recover \emph{easy} answers already existing in the graph (reachable by edge traversal) and then enrich the answer set with predicted \emph{hard} answers inferred with link prediction. 
A natural metric is therefore an ability to rank easy answers higher than hard answers -- this was studied by InductiveQE~\citep{galkin2022} that proposed to use ROC-AUC as the main metric for this task.

Still, we would argue that existing metrics might not fully capture the nature of neural query answering and new metrics might be needed.
For example, some under-explored but potentially useful metrics include (1) studying \emph{reasonable} answers (in between easy and hard answers) that can be deduced by symbolic reasoners using a higher-level graph schema (ontology)~\citep{q2b_onto}. 
A caveat in computing reasonable answers is a potentially infinite processing time of symbolic reasoners that have to be limited by time or expressiveness in order to complete in a finite time. Hence, the set of reasonable answers might still be incomplete; (2) evaluation in light of the \emph{open-world assumption} (OWA) stating that unknown triples in the graph might not necessarily be false (as postulated by the standard \emph{closed-world assumption} used a lot in link prediction). 
Practically, OWA means that even the test set might be incomplete and some high-rank predictions deemed incorrect by the test set might in fact be correct in the (possibly unobservable) complete graph. 
Initial experiments of \citet{yang2022rethinking} with OWA evaluation of link prediction explain the saturation of ranking metrics (\eg, MRR) on common datasets by the performance of neural link predictors able to predict the answers from the complete graph missed in the test set.
For example, saturated MRR of 0.4 on the test set might correspond to MRR of 0.9 on the true complete graph.
Studying OWA evaluation in the query answering task in both transductive and inductive setups is a solid avenue for future work.

