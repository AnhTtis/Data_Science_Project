



\section{Queries}
\label{sec:queries}



The third direction to segment the methods is from the queries point of view. Under the queries category, we have three subcategories: \emph{Query Operators}, \emph{Query Patterns}, and \emph{Projected Variables}. For query operators, methods have different operator expressiveness, which means the set of query operators one model is able to handle including existential quantification ($\exists$), conjunction ($\wedge$), disjunction ($\vee$), negation ($\neg$), Kleene plus (+), filter and various aggregation operators. For query patterns, we refer to the structure/pattern of the (optimized) query plan, ranging from paths and trees to arbitrary directed acyclic graphs (DAGs) and cyclic patterns. 
%
As to projected variables (by \emph{projected} we refer to target variables that have to be bound to particular graph elements like entity or relation), queries might have a different number (zero or more) of target variables.
We are interested in the complexity of such projections as binding of two and more variables involves relational algebra~\citep{codd1970relational} and might result in a Cartesian product of all retrieved answers.

\subsection{Query Operators}
\label{sec:query_ops}

Different query processors have different expressiveness in terms of operators a method can handle. Throughout all the works, we compiled \autoref{tab:proc_query_ops} that classifies all methods based on the supported operators.

\input{tables/tab_query_ops}

We start from simplest conjunctive queries that involve only existential quantification ($\exists$) and conjunction ($\wedge$) and gradually increase the complexity of supported operators.

\paragraph{Existential Quantification ($\exists$).}
When $\exists$ appears in a query, this means that there exists at least one existentially quantified variable. For example, given a query ``At what universities do the Turing Award winners work?'' and its logical form $q = U_?\:.\: \exists V\::\:\textit{win}(\texttt{TuringAward}, V)\wedge \textit{university}(V, U_?)$, here $V$ is the existentially quantified variable. Query processors model existential quantification by using a relation projection operator. 
Mapping to query languages like SPARQL, relation projection is equivalent to a \emph{triple pattern} with one variable, \eg, \texttt{\{TuringAward win ?v\}} (\autoref{fig:query_ops1}).
Generally, as introduced in \autoref{sec:models}, query embedding methods embed a query in a bottom-up fashion, starting with the embedding of the anchors (leaf nodes) and gradually traversing the query tree up to the root. In such a way, query embedding methods (\eg, geometric or probabilistic) explicitly obtain an embedding for the existentially quantified variables. The embeddings/representations of these variables are calculated by a relation projection function implemented as shallow vector operations~\citep{gqe,q2b,hype,cqd,smore} or deep neural nets~\citep{betae,cone,query2particles2022,mlpmix}. 
Another set of methods based on GNNs and Transformers directly assigns a learnable initial embedding for the existentially quantified variables. These embeddings are then updated through several message passing layers over the query plan~\citep{mpqe,starqe,gnnq,lmpnn} or attention layers over the serialized query plan~\citep{biqe,kgtrans2022}.


The major drawback of existing neural query processors is the assumption of at least one anchor entity in the query from which the answering process starts and relation projections can be executed. 
It remains an open challenge and an avenue for future work to support queries without anchor entities, \eg, $q_1 = U_? . \exists v_1, v_2 : \textit{win}(v_1, v_2) \wedge \textit{university}(v_2, U_?)$, and queries where relations are variables, \eg, $q_2 = r_? : r_?(\texttt{TuringAward}, \texttt{Bengio})$, that can be framed as the relation prediction task.

\paragraph{Universal Quantification ($\forall$).}
A universally quantified variable $\forall x.P(x)$ means that a logical formula $P(x)$ holds for all possible $x$. 
Usually, the universal quantifier does not appear in facts-only ABox graphs without types and complex axioms (see \autoref{sec:bg_sem}) as it would imply that some entity is connected to all other entities. 
For example, $\forall V. \textit{win}(\texttt{TuringAward}, V)$ without other constraints implies that all entities are connected to \texttt{TuringAward} by the \textit{win} relation (which does not occur in practice). 
However, universal quantifiers are more useful when paired with the class hierarchy (unary relations), \eg, $\exists \textit{?paper}, \forall r \in \texttt{Researcher}: \texttt{Researcher}(r) \wedge \textit{authored}(r, \textit{?paper})$ means that the \textit{authored} relation projection would be applied only to entities of class \texttt{Researcher}.

Currently, existing \clqa approaches do not support universal quantifiers explicitly nor the datasets include queries with the $\forall$ quantifier. 
Still, using the basic identity  $\forall x. P(x) \equiv \neg (\exists x. \neg P(x))$ it is possible to model the universal quantifier by any approach supporting existential quantification $\exists$ and negation $\neg$.
By default, we assume the closed-world assumption in {\ngdb}s.
We leave the implications of universal quantification pertaining to the open-world assumption (OWA) out of the scope of this work.

\paragraph{Conjunction ($\wedge$).}
We elaborate on the differences of those query patterns in the following \autoref{sec:query_pat} and emphasize our focus on more complex intersection queries going beyond simpler path-like queries.

Query processors, as described in \autoref{sec:models}, employ different parameterizations of conjunctions as permutation-invariant set functions. 
A family of neural processors~\citep{gqe, gqe_bin, smore} often resort to the DeepSet architecture~\citep{deepsets} that first projects each set element independently and then pools representations together with a permutation-invariant function (\eg, sum, mean) followed by an MLP. Alternatively,~\citep{cga, query2particles2022} self-attention, can serve as a replacement of the DeepSet where set elements are weighted with the attention operation. 
The other family of neural processors combine projection and intersection by processing the whole query graph with GNNs~\citep{mpqe, starqe, gnnq, lmpnn} or with the Transformer over linearized query sequences~\citep{biqe, kgtrans2022}. 
Geometric processors~\citep{q2b, newlook, hype, cone} implement conjunction as the attention-based average of centroids and offsets of respective geometric objects (boxes, hyperboloids, or cones).
Probabilistic processors~\citep{betae, perm, line2022, gammae} implement intersection as a weighted sum of parametric distributions that represent queries and variables.

\begin{figure}[t]
	\centering
    \includegraphics[width=\textwidth]{figs/query_operators_p1.pdf}
	\caption{Query operators (relation projection and intersection), corresponding SPARQL basic graph patterns (BGP), and their computation graphs. Relation Projection (left) corresponds to a triple pattern. 
    }
	\label{fig:query_ops1}
\end{figure}

Fuzzy-logic processors~\citep{cqd,logic_e, fuzz_qe, gnn_qe} commonly resort to \emph{t-norms}, generalized versions of conjunctions in the continuous $[0,1]$ space
and corresponding \emph{t-conorms} for modeling unions (\autoref{sec:tnorms}). 
Often, due to the absence of a principal study, the choice of the fuzzy logic is a hyperparameter. 
We posit that such a study is an important avenue for future works in fuzzy processors.
More exotic neuro-symbolic methods for modeling conjunctions include element-wise product of count-min sketches~\citep{emql} or as a weighted sum in the \emph{feature logic}~\citep{lin2022flex, lin2022tflex}.
Finally, some processors~\citep{abin_abductive2022, enesy} perform conjunctions both in the embedding and symbolic space with neural and fuzzy operators.

We note that certain neural query processors that embed queries directly~\citep{gqe,gqe_bin, cga,tractor2020, smore} or via GNN/Transformer encoder over a query graph~\citep{mpqe, biqe, sheaves, starqe, cbr_subg2022, gnnq} support only projections and intersections, that is, their extensions to more complex logical operators are non-trivial and might require changing the underlying assumptions of modeling entities, variables, and queries. 
In some cases, support of unions might be enabled when re-writing a query to the disjunctive normal form (discussed below).

\paragraph{Disjunction ($\vee$).}

Query processors implement the disjunction operator in several ways. However, modeling disjunction is notoriously hard since it requires modeling any powerset of entities\footnote{Here the most rigorous description should be powerset of sets of ``isomorphic'' entities on the graph. By ``isomorphic'' entities, it refer to an entity set where each elemen} on the graph in a vector space.
Before delving into details about different ways of modeling disjunction, we first refer the readers to the Theorem 1 in Query2Box~\citep{q2b}. The theorem proves that we need the VC dimension of the function class of the distance function to be around the number of entities on the graph.  


The theorem shows that in order to accurately model \emph{any} EPFO query with the existing framework, the complexity of the distance function measured by the VC dimension needs to be as large as the number of KG entities. This implies that if we use common distance functions based on hyper-plane, Euclidean sphere, or axis-aligned rectangle,\footnote{For the detailed VC dimensions of these function classes, see \cite{vapnik2013nature}. Crucially, their VC dimensions are all linear with respect to the number of parameters $d$.} their parameter dimensionality needs to be $\Theta(M)$, which is $\Theta(|\gE|)$ for real KGs we are interested in. In other words, the dimensionality of the logical query embeddings needs to be $\Theta(|\gE|)$, which is not low-dimensional; thus not scalable to large KGs and not generalizable in the presence of unobserved KG edges. 


The first idea proposed in Query2Box~\citep{q2b} is that given a model has defined a distance function between a query representation and the entity representation, then a query can be transformed (or re-written) into its equivalent \emph{disjunctive normal form} (DNF), \ie, a disjunction of conjunctive queries. For example, we can safely convert a query $(A\vee B)\wedge (C\vee D)$ to $((A\wedge C) \vee (A\wedge D) \vee (B \wedge C) \vee (B \wedge D))$, where $A,B,C,D$ are atomic formulas. In such a way, we only need to process disjunction $\vee$ at the very last step. For models that have defined a distance function between the query representation and entity representation $d(\Em{q}, \Em{e})$ (such as geometric processors~\citep{q2b, q2b_onto, regex, hype, cone, query2geom} and some neural processors~\citep{kgtrans2022, mlpmix, lmpnn}, the idea of using DNF to handle disjunction is to (1) embed each atomic formula / conjunctive query in the DNF into a vector $\Em{q_i}$, (2) calculate the distance between the representation/embedding of each atomic formula / conjunctive query and the entity $d(\Em{q_i}, \Em{e})$, (3) take the minimum of the distances $\textit{min}_i(d(\Em{q_i}, \Em{e}))$. 
The intuition is that since disjunction models the union operation, as long as the node is close to one atomic formula / conjunctive query, it should be close to the whole query.
Potentially, many neural processors with the defined distance function and originally supporting only intersection and projection can be extended to supporting unions with DNF. 
One notable downside of this modeling is that it is exponentially expensive (to the number of disjunctions) in the worst case when converting a query to its DNF.

Another category of mostly probabilistic processors~\citep{perm, gammae} proposes a neural disjunction operator implemented with the permutation-invariant attention over the input set with the \emph{closure} assumption that the result of attention weighting  union remains in the same probabilistic space as its inputs.  %
Such models design a more black-box framework to handle the disjunction operation under the strong closture assumption that might not be true in all cases. 
%

The third way of modeling disjunction is based on the De Morgan's laws~\citep{betae}. According to the De Morgan's laws (DM), the disjunction is equivalent to the negation of the conjunction of the negation of the statements making up the disjunction, \ie, $A\vee B = \neg (\neg A \wedge \neg B)$. For methods that can handle the negation operator (detailed in the following paragraph), they model disjunction by using three negation operations and one conjunction operation. 
DM conversion was explicitly probed in probabilistic~\citep{betae}, geometric~\citep{cone}, and fuzzy~\citep{logic_e} processors.

Finally, most fuzzy-logic~\citep{cqd, fuzz_qe, abin_abductive2022, gnn_qe, qto, cqda, var2vec} processors employ \emph{t-conorms}, generalized versions of disjunctions in the continuous $[0,1]$ space (\autoref{sec:tnorms}).
%
More exotic versions of neuro-symbolic disjunctions include element-wise summation of count-min sketches~\citep{emql}, feature logic operations~\citep{lin2022flex,lin2022tflex}, as well as performing a union in both embedding and symbolic spaces~\citep{abin_abductive2022,enesy} with fuzzy operators.

\begin{figure}[!t]
	\centering
    \includegraphics[width=\textwidth]{figs/query_operators_p2.pdf}
	\caption{Query operators (union, negation, Kleene plus), corresponding SPARQL basic graph patterns (BGP), and their computation graphs.}
	\label{fig:query_ops2}
\end{figure}

\paragraph{Negation ($\neg$).}
For negation operation, the goal is to model the complement set, \ie, the answers $\gA_q$ to a query $q=V_?: \neg r(v, V_?)$ are the exact complement of the answers $\gA_{q'}$ to query $q'=V_?:r(v,V_?)$: $\gA_q = \gV / \gA_{q'}$. 
Correspondingly, negation in SPARQL can be implemented with \texttt{FILTER NOT EXISTS} or \texttt{MINUS} clauses. 
For example (\autoref{fig:query_ops2}), a logical formula with negation $\neg\textit{field}(\texttt{DeepLearning, V})$ is equivalent to the SPARQL BGP \texttt{\{?s ?p ?v. FILTER NOT EXISTS \{DeepLearning field ?v\}\}} where \texttt{\{?s ?p ?v\}} models the \emph{universe set} ($\Em{1}$) of all facts that gets filtered by the triple pattern.

Modeling the \emph{universe set} ($\Em{1}$) and its complement is the key problem when designing a negation operator in neural query processors, \eg, an arbitrary real $\RR$ or complex $\CC$ space is unbounded such that $\Em{1}$ is not defined.
For that reason, many neural processors do not support the negation operator. 
Still, there exist several approaches to handle negation. 

The first line of works~\citep{query2particles2022, mlpmix, nmp_qem} designs a purely neural MLP-based negation operator over the query representation avoiding the universe set altogether. 
Similarly, a token of the negation operator can be included into the linearized query representation~\citep{sqe} to be encoded with Transformer or recurrent network.
A step aside from purely neural operators is taken by GNN-based processors~\citep{lmpnn} that treat a negation edge as a new edge type during message passing over the query computation graph.

The second line is customized to different embedding spaces and aims to simulate the calculation of the universe and complement in the embedding space, \eg, using geometric cones~\citep{cone}, parameters are angles $\Em{\theta}$ such that the space (and, hence, $\Em{1}$) is bounded to $2\pi$ and the complement is straight $2\pi - \Em{\theta}$. 
Probabilistic methods~\citep{betae, line2022, gammae} naturally represent negation as an inverse of distribution parameters.
%

Thirdly, fuzzy logic processors explicitly model the universe set $\Em{1}$ and the complement % 
over the same real valued logic space. 
For instance, LogicE~\citep{logic_e} and FuzzQE~\citep{fuzz_qe} restrict the query embedding space to the range $[0,1]^d$ where each query $\Em{q} \in [0,1]^d$ is a vector. This way, the universe $\Em{1}$ is represented with a vector of all ones (in the embedding space $\Em{1}^d$) and negation is simply $\Em{1} - \Em{q}$.
TAR~\citep{abin_abductive2022} and GNN-QE~\citep{gnn_qe} operate over fuzzy sets where each entity has a corresponding scalar $\Em{q} \in [0,1]$ in the bounded range. Therefore, the universe $\Em{1}$ can still be a vector of all ones (in the entity space $\Em{1}^{|\gE|}$) and negation is $\Em{1} - \Em{q}$. 
ENeSy~\citep{enesy} defines the universe as the uniform distribution over the entity space with each element weighting $\frac{\alpha}{|\gE|}$ ($\alpha$ is a hyperparameter). 
$\text{CQD}^{\gA}$~\citep{cqda} employs a strict cosine fuzzy negation $\frac{1}{2}(1+\cos(\pi \Em{q}))$ over scalar scores $\Em{q}$.
More exotic processors~\citep{lin2022flex, lin2022tflex} employ feature logic for modeling negation.
We also note that the \emph{difference} operator introduced in \citet{newlook} is in fact a common intersection-negation (\emph{2in}) query pattern used in all standard benchmarks (\autoref{sec:datasets}).

\paragraph{Kleene Plus (+) and Property Paths.}
Kleene Plus is an operator that applies compositionally and recursively to any regular expression (RegEx) that denotes \emph{one or more} occurrence of the specified pattern. 
Regular expressions exhibit a direct connection to \emph{property paths} in SPARQL.
We defined a very basic regular graph query in \autoref{def:regular_path_query}, here we generalize that further to property paths.
To define property paths more formally, given a set of relations $\gR$ and operators $\{+, *, ?, !, \hat{},  /, | \}$, a property path $p$ can be obtained from the recursive grammar 
$p::= r \: | \: p^+ \:|\: p^* \:|\: p? \:|\: !p \: |\: \hat{p} \:|\: p_1 / p_2 \:|\: ``p_1|p_2" $
Here, $r$ is any element of $\gR$, $+$ is a Kleene Plus denoting \emph{one or more} occurrences, $*$ is a Kleene Star denoting \emph{zero or more} occurrences, $?$  denotes \emph{zero or one} occurences, $!$ denotes negation of the relation or path, $\hat{p}$ 
traverses an edge of type $p$ in the opposite direction,
$p_1/p_2$ is a sequence of relations (corresponds to \emph{relation projection}), and $p_1 | p_2$ denotes an alternative path of $p_1$ or $p_2$ (corresponds to a \emph{union} operation).
For example (\autoref{fig:query_ops2}), an expression with Kleene plus $\textit{knows}(\texttt{JohnDoe}, V)^+$ can be represented as a SPARQL property path \texttt{\{JohnDoe knows+ ?v.\}}.

Property paths are non-trivial to model for neural query processors due to compositionalilty and recursive nature. To the best of our knowledge, RotatE-Box~\citep{regex} is the only geometric processor that handles the Kleene Plus operator implementing the subset of operators $\{ +, /, | \}$.
RotatE-Box provides two ways to handle Kleene Plus. The first method is to define a $\Em{r^+}$ embedding for each relation $r\in\gR$, note this is independent and separate from the regular relation embedding for $r$; another way is to use a trainable matrix to transform the relation embedding $\Em{r}$ to the $\Em{r^+}$ embedding. 
Note the two methods do not support Kleene Plus over paths.
RotatE-Box also implements relation projection (as a rotation in the complex space) and union (with DeepSets or DNF) but does not support the intersection operator.
 
We hypothesize that better support of the property paths vocabulary might be one of main focuses in future neural query processors.
Particularly for Kleene Plus, some unresolved issues include supporting
idempotence ($(r^+)^+=r^+$) and infinite union of sets ($r^+=r | (r/r) | (r/r/r)\dots$).


\begin{figure}[!t]
	\centering
    \includegraphics[width=\textwidth]{figs/query_operators_p3.pdf}
	\caption{Query operators (\texttt{Filter}, \texttt{Count} Aggregation, \texttt{Optional}), corresponding SPARQL basic graph patterns (BGP), and their computation graphs.}
	\label{fig:query_ops3}
\end{figure}

\paragraph{Filter.}
Filter is an operation that can be inserted in a SPARQL query. %
It takes any expression of boolean type as input and aims to filter the results based on the boolean value, \ie, only the results rendered \texttt{True} under the expression will be returned. The boolean expression can thus be seen as a condition that the answers to the query should follow. For the filter operator, we can do filter on values/literals/attributes, \eg, $\texttt{Filter}(V_\text{date} \geq ``2000-01-01" \&\& V_\text{date} \leq ``2000-12-31")$ means we would like to filter dates not in the year 2000; $\texttt{Filter}(\texttt{LANG}(V_\text{book}) = ``en")$ means we would like to filter books not written in English, $\texttt{Filter}(?\text{pages} > 100)$ means returning the books that have more than 100 pages (as illustrated in \autoref{fig:query_ops3}). To the best of our knowledge, there does not exist a reasoning model that claims to handle Filters, which leaves room for future work on this direction.

We envision several possibilities to support filtering in neural query engines: (1) the simplest option used by  \citet{thorne2021acl,thorne2021vldb} in natural language engines is to defer filtering to the postprocessing stage when the set of candidate nodes is identified and their attributes can be extracted by a lookup. 
(2) Filtering often implies reasoning over literal values and numerical node attributes, that is, processors supporting continuous values (as described in \autoref{sec:modality}) might be able to perform filtering in the latent space by attaching, for instance, a parametric regressor decoder (\autoref{sec:decoder}) when predicting $?\text{pages} > 100$.
%

\paragraph{Aggregation.}
Aggregation is a set of operators in SPARQL queries including \texttt{COUNT} (return the number of elements), \texttt{MIN}, \texttt{MAX}, \texttt{SUM}, \texttt{AVG} (return the minimum / maximum / sum / average value of all elements), \texttt{SAMPLE} (return any sample from the set). 
For example (\autoref{fig:query_ops3}), given a triple pattern \texttt{\{StephenKing wrote ?book.\}}, the clause $\texttt{COUNT (?book) as ?n)}$ returns the total number of books written by \texttt{StephenKing}.

Most aggregation operators require reasoning over sets of numerical values/literals. Such symbolic operations have long been considered a challenge for neural models~\citep{hendrycksmath2021}. How to design a better representation for numerical values / literals requires remains an open question.
Some neural query processors~\citep{q2b, betae, cone, gnn_qe}, however, have the means to estimate the cardinality of the answer set (including predicted hard answers) that directly corresponds to the \texttt{COUNT} aggregation over the target projected variable (assumed to be an entity, not a literal).
For example, GNN-QE~\citep{gnn_qe} returns a fuzzy set, \ie, a scalar likelihood value for each entity, that, after thresholding, 
%
has low mean absolute percentage error (MAPE) of the number of ground truth answers.
Answer cardinality estimation is thus obtained as a byproduct of the neural query processor without tailored predictors.
Alternatively, when models cannot predict the exact count, Spearman's rank correlation is a surrogate metric to evaluate the correlation between model predictions and the exact count.
%
Spearman's rank correlation and MAPE of the number of ground truth answers are common metrics to evaluate the performance of neural query processors and we elaborate on the metrics in \autoref{sec:metrics}.

\paragraph{Optional and Solution Modifiers.}
SPARQL offers many features yet to be incorporated into neural query engines to extend their expressiveness.
Some of those common features include the \texttt{OPTIONAL} clause that is essentially a \texttt{LEFT JOIN} operator.
For example (\autoref{fig:query_ops3}), given a triple pattern \texttt{\{King wrote ?book.\}} that returns books, the optional clause \texttt{\{King wrote ?book. OPTIONAL \{King award ?a.\}\}} enriches the answer set with any existing awards received by \texttt{King}. 
Importantly, if there are no bindings to the optional clause, the query still returns the values of \texttt{?book}.
%
In the query's computation graph, the optional clause corresponds to the optional branch of a relation projection.
A particular challenge for neural query processors operating on incomplete graphs is that the absence of the queried edge in the graph does not mean that there are no bindings -- instead, the edge might be missing and might be predicted during query processing.

Solution modifiers, \eg, \texttt{GROUP BY}, \texttt{ORDER BY}, \texttt{LIMIT}, apply further postprocessing of  projected (returned) results and are particularly important when projecting several variables in the query. 
So far, all existing neural query processors are tailored for only one return variable. We elaborate on this matter in \autoref{sec:vars}.

\paragraph{A General Note on Incompleteness.}
Finally, we would like to stress out that neural query engines performing all the described operators (\textbf{Projection}, \textbf{Intersection}, \textbf{Union}, \textbf{Negation}, \textbf{Property Paths}, \textbf{Filters}, \textbf{Aggregations}, \textbf{Optionals}, and \textbf{Modifiers}) assume the underlying graph is incomplete and queries might have some missing answers to be predicted, hence, all the operators should incorporate predicted \emph{hard answers} in addition to \emph{easy answers} reachable by graph traversal as in symbolic graph databases.
Evaluation of query performance with those operators in light of incompleteness is still an open challenge (we elaborate on that in \autoref{sec:metrics}), \eg, having an \emph{Optional} clause, it might be unclear when there is no true answer (even predicted ones are in fact false) or a model is not able to predict them.
%

\subsection{Query Patterns}
\label{sec:query_pat}

Here, we introduce several types of query patterns commonly used in practical tasks and sort them in the increasing order of complexity. Starting with chain-like \emph{Path} queries known in the literature for years, we move to \emph{Tree-Structured} queries (the main supported pattern in modern \clqa systems). Then, we overview \emph{DAG} and \emph{cyclic} patterns which currently are not supported by any neural query answering system and represent a solid avenue for future work.

\begin{figure}[!t]
	\centering
    \includegraphics[width=\textwidth]{figs/query_patterns.pdf}
	\caption{Query patterns: path, tree-like, DAG, and cyclic queries. A DAG query has two branches from the intermediate variable, a cyclic query contains a 3-cycle. Existing neural query processors support path and tree-like patterns.}
	\label{fig:query_pats1}
\end{figure}

\begin{figure}[!t]
	\centering
    \includegraphics[width=\textwidth]{figs/query_patterns_graph_ex.pdf}
	\caption{Answers to example tree-like, DAG, and cyclic query patterns given a toy graph. Note the difference in the answer set to the tree-like and DAG queries -- in the DAG query, a variable $v$ must have two outgoing edges from the same node.}
	\label{fig:query_pats2}
\end{figure}

\paragraph{Path Queries.} As introduced in \autoref{sec:multihop_vs_complex}, previous literature starts with path queries (\emph{aka} multi-hop queries), where the goal is simply to go beyond one-hop queries such as $q=V_? . r(v, V_?)$, where $r\in\gR, v\in\gV$ and $V_?$ represents the answer variable. 
As shown in \autoref{fig:query_patterns} and \autoref{fig:query_pats1}, there is no logical operator such as branch intersection or union involved. 
Therefore, in order to answer such a query, we simply find or infer the neighbors of the entity $v$ with relation $r$. 
%
Path queries are a natural extension of one-hop queries.
Formally, we denote a path query as follows. $q_\text{path}=V_? . \exists V_1,\dots,V_{k-1} : r_1(v, V_1)\wedge r_2(V_1, V_2) \wedge \dots \wedge r_k(V_{k-1}, V_?)$, where $r_i \in \gR, \forall i\in[1,k]$, $v\in\gV$, $V_i$ are all existentially quantified variables. We denote a $k$-hop path query if it has $k$ atomic formulas. 
The query plan of a $k$-hop path query is a chain of length $k$ starting from the anchor entity. 
For example (\autoref{fig:query_pats1}), a 2-hop path query is $V_?.\exists v:\textit{student}(\texttt{Stanford}, v) \wedge \textit{roommate}(v, V_?)$ where \texttt{Stanford} is the starting anchor node, $v$ is a tail variable of the first projection $\textit{student}$ and at the same time is the head variable of the second projection $\textit{roommate}$ thus forming a chain.

As shown in the definition, in order to handle path queries, it is necessary to develop a method to handle existential quantification $\exists$ and conjunction $\wedge$ operators. 
Several query reasoning methods~\citep{guu-2015-traversing, das-2017-chains} aim to answer the path queries with sequence models using either chainable KG embeddings (\eg, TransE) in \citet{guu-2015-traversing} or LSTM in \citet{das-2017-chains}. These methods initiated one of the first efforts that use embeddings and neural methods to answer multi-hop path queries.
We acknowledge the efforts in this domain but emphasize their limitations in terms of query expressiveness and, therefore, focus our attention in this work on more expressive query answering methods that operate on tree-like and more complex patterns.


\paragraph{Tree-Structured Queries.} Path queries only have one anchor entity and one answer variable. 
Such queries have limited expressiveness and are far away from real-world query complexity seen in the logs~\citep{wikidata_query_logs} of real-world KGs like Wikidata.
One direct extension to increase the expressiveness and complexity is to support tree-structured (tree-like) queries.
Tree-like queries may have multiple anchor entities, and different branches (from different anchors) will merge at the final single answer node, thus forming a tree structured query plan. 
Such merge can be achieved by intersection, union, or negation operators. For example, as shown in \autoref{fig:clqa_1}, the query plan of ``At what universities do the Turing Award winners in the field of Deep Learning work?'' is not a path but a tree. 
Alternatively, the example in \autoref{fig:query_pats1} depicts a query $q = V_?, \exists v_1, v_2 : \textit{student}(\texttt{Stanford}, v_1) \wedge \textit{roommate}(v_1, V_?) \wedge \textit{student}(\texttt{Stanford}, v_2) \wedge \textit{classmate}(v_2, V_?)$ that consists of two branches of 2-hop path queries joined by the intersection operator at the end.

Tree-like queries pose more challenges to the previous models that are only able to handle path (multi-hop) queries since a sequence model no longer applies to tree-structured execution plans with logical operators. 
In light of the challenges, neural and neuro-symbolic query processors (described in \autoref{sec:learning}) %
are designed to execute more complex query patterns.
These processors design neural set/logic operators and do a bottom-up traversal of the tree up to the single root node. 

\paragraph{Arbitrary DAGs.}
Based on tree-structured queries, one can further increase the complexity of the query pattern to arbitrary directed acyclic graphs (DAGs). The key difference between the two types of queries is that for DAG-structured queries, one variable node in the query plan (that represents a set of entities) may be split and routed to different reasoning paths, while the number of branches/reasoning paths in the query plan always decreases from the anchor nodes to the answer node. We show one example in \autoref{fig:query_pats1} and in \autoref{fig:query_pats2}. 
Consider the tree-like query from the previous paragraph $q_1=V_?, \exists v_1, v_2 : \textit{student}(\texttt{Stanford}, v_1) \wedge \textit{roommate}(v_1, V_?) \wedge \textit{student}(\texttt{Stanford}, v_2) \wedge \textit{classmate}(v_2, V_?)$ and the DAG query $q_2=V_?, \exists v : \textit{student}(\texttt{Stanford}, v) \wedge \textit{roommate}(v, V_?) \wedge \textit{classmate}(v, V_?)$.
The two queries search for $V_?$ who are \emph{roommate} and \emph{classmate} with \texttt{Stanford} students.
However, the answer sets of the two queries are different (illustrated in \autoref{fig:query_pats2}).
That is, the answer to the DAG query $V_{q_2} = \{\texttt{A}\}$ is the subset of the answers to the tree-like query $V_{q_1} = \{\texttt{A,E}\}$ because the answers to $q_2$ have to be both \textit{roommate} and \textit{classmate} with the \textbf{same} Stanford student in the intermediate variable $v$. 
On the other hand, the two branches of the tree-like query $q_1$ are independent such that intermediate variables $v_1$ and $v_2$ need not be the same entities, hence, the query has more valid intermediate answers and more correct answers.
To the best of our knowledge, there still does not exist a neural query processor that can faithfully handle any DAG query.
Although BiQE~\citep{biqe} claims to support DAG queries, the mined dataset consists of tree-like queries. 
Nevertheless, we hypothesize that, potentially, processors with message passing or Transformer architectures that consider the entire query graph structure $\gG_q$ may be capable of handling DAG queries and leave this question for future work.

\paragraph{Cyclic Queries.}
Cyclic queries are more complex than DAG-structured queries. 
A cycle in a query naturally entails no particular order to traverse the query plan.
An example of the cyclic query is illustrated in \autoref{fig:query_pats1} and \autoref{fig:query_pats2}: $q = V_?, \exists v_1, v_2, v_3 : \textit{student}(\texttt{Stanford}, v_1) \wedge \textit{roommate}(v_1, v_2) \wedge \textit{roommate}(v_2, v_3) \wedge \textit{roommate}(v_3, v_1) \wedge \textit{classmate}(v_1, V_?)$. 
In $q$, three variables form a triangle cycle $\textit{roommate}(v_1, v_2) \wedge \textit{roommate}(v_2, v_3) \wedge \textit{roommate}(v_3, V_?)$.
Given a graph in \autoref{fig:query_pats2}, the cycle starts and ends at node \texttt{C}, hence the only correct answer is obtained after performing the \textit{classmate} relation projection from \texttt{C} ending in \texttt{D}, $V_{q} = \{\texttt{D}\}$. 

Reasoning methods and query processors that assume a particular traversal or node ordering on the query plan, therefore, cannot faithfully answer cyclic queries. 
It remains an open question how to effectively model a query with cyclic structures.
Moreover, cyclic structures often appear when processing queries with regular expressions and \emph{property paths} (\autoref{sec:query_ops}). 
We posit that supporting cycles might be a necessary condition to fully enable property paths in neural query engines.



\subsection{Projected Variables}
\label{sec:vars}

By \emph{projected variables} we understand target query variables that have to be bound to particular graph elements such as entity, relation, or literals.
For example, a query in \autoref{fig:clqa_1} $q = V_?. \exists v : \textit{win}(\texttt{TuringAward}, v)\wedge \textit{field}(\texttt{DeepLearning}, v) \wedge \textit{university}(v, V_?)$ has one projected variable $V_?$ that can be bound to three answer nodes in the graph, $V_? = \{\texttt{UofT}, \texttt{UdeM}, \texttt{NYU}\}$.
In the SPARQL literature~\citep{sparql2013}, the \texttt{SELECT} query specifies which existentially quantified variables to project as final answers.
The pairs of projected variables and answers form \emph{bindings} as the result of the \texttt{SELECT} query.
Generally, queries might have zero, one, or multiple projected variables, and we align our categorization with this notion. 
Examples of such queries and their possible answers are provided in \autoref{fig:query_projected_vars}.
Currently, most neural query processors focus on the setting where queries have only one answer variable -- the leaf node of the computation graph, as shown in \autoref{fig:query_projected_vars} (center). 

\begin{figure}[!t]
	\centering
    \includegraphics[width=\textwidth]{figs/query_projected_vars.pdf}
	\caption{Projected variables of the tree-like query from \autoref{fig:query_pats2}. Current neural query processors support the single-variable \texttt{DISTINCT} mode (center) whereas queries might have zero return variables akin to a subgraph matching Boolean \texttt{ASK} query (left) or multiple projected variables (right) that imply returning intermediate answers and form output tuples.}
	\label{fig:query_projected_vars}
\end{figure}

\paragraph{Zero Projected Variables.}
Queries with zero projected variables do not return any bindings but rather probe the graph on the presence of a certain subgraph or relational pattern where the answer is Boolean \texttt{True} or \texttt{False}.
In SPARQL, the equivalent of zero-variable queries is the \texttt{ASK} clause.
Zero-variable queries might have all entities and relations instantiated with constants, \eg, $q = \textit{student}(\texttt{S}, \texttt{D}) \wedge \textit{roommate}(\texttt{D}, \texttt{E})$ as in \autoref{fig:query_projected_vars} (left) is equivalent to the SPARQL query \texttt{ASK WHERE \{S student D. D roommate E.\}}. 
The query probes whether a graph contains a particular subgraph (path) induced by the constants. Such a path exists, so the answer is $q = \{\texttt{True}\}$. 

Alternatively, zero-variable queries might have existentially quantified variables that are never projected (up to the cases where all subjects, predicates, or objects are variables). 
For example, a query $q_1 = \exists v_1, v_2, v_3 : \textit{student}(v_1, v_2) \wedge \textit{roommate}(v_2, v_3)$ probes whether there exist any nodes forming a relational path $v_1 \xrightarrow{\textit{student}} v_2 \xrightarrow{\textit{roommate}} v_3$.
In a general case, a query $q_2 = \exists p, s, o : p(s, o)$ asks if a graph contains at least one edge.

We note that in the main considered setting with incomplete graphs and missing edges zero-variable queries are still non-trivial to answer.
Particularly, a subfield of \emph{neural subgraph matching}~\citep{rex2020neural,huang2022fewshot} implies having incomplete graphs. 
We hypothesize such approaches might be found useful for neural query processors to support answering zero-variable queries.

\paragraph{One Projected Variable.}
Queries with one projected variable return bindings for one (of possibly many) existentially quantified variable. 
In SPARQL, the projected variable is specified in the \texttt{SELECT} clause, \eg, \texttt{SELECT DISTINCT ?v} in \autoref{fig:query_projected_vars} (center).
Although SPARQL allows projecting variables from any part of a query, most neural query engines covered in \autoref{sec:learning} follow the task formulation of GQE~\citep{gqe} and allow the projected target variable to be only the \textbf{leaf node} of the query computation graph. 
This limitation is illustrated in \autoref{fig:query_projected_vars} (center) where the target variable $?v$ is the leaf node of the query graph and has two bindings $v=\{\texttt{A}, \texttt{E}\}$.

It is worth noting that existing neural query processors are designed to return a \emph{unique set} of answers to the input query, \ie, it corresponds to the \texttt{SELECT DISTINCT} clause in SPARQL.
In contrast, the default \texttt{SELECT} returns \emph{multisets} with possible duplicates.
For example, the same query in \autoref{fig:query_projected_vars} (center) without \texttt{DISTINCT} would have bindings $v=\{\texttt{A},\texttt{A},\texttt{E}\}$ as there exist two matching graph patterns ending in \texttt{A}. 
Implementing non-\texttt{DISTINCT} query answering remains an open challenge.

Most neural query processors have a notion of intermediate variables and model their distribution in the entity space.
For instance, having a defined distance function, geometric processors~\citep{q2b,hype} can find nearest entities as intermediate variables.
Similarly, fuzzy-logic processors operating on fuzzy sets~\citep{abin_abductive2022,gnn_qe} already maintain a scalar distribution over all entities after each execution step.
Finally, GNN-based~\citep{mpqe,starqe} and Transformer-based~\citep{kgtrans2022} processors  explicitly include intermediate variables as nodes in the query graph (or tokens in the query sequence) and can therefore decode  their representations to the entity space.
The main drawback of all those methods is the lack of filtering mechanisms for the sets of intermediate variables \emph{after} the leaf node has been identified.
That is, in order to filter and project only those intermediate variables that lead to the final answer, some notion of \emph{backward pass} is required. 
The first step in this direction is taken by  QTO~\citep{qto} that runs the pruning backward pass after reaching the answer leaf node. 


\paragraph{Multiple Projected Variables.}
The most general and complex case for queries is to have multiple projected variables as illustrated in \autoref{fig:query_projected_vars} (right).
In SPARQL, all projected variables are specified in the \texttt{SELECT} clause (with the possibility to project all variables in the query via \texttt{SELECT *}).
In the logical form, a query has several target variables $q = ?v_1, ?v_2, ?v : \textit{student}(\texttt{Stanford}, ?v_1) \wedge \textit{roommate}(?v_1, ?v) \wedge \textit{student}(\texttt{Stanford}, ?v_2) \wedge \textit{classmate}(?v_2, ?v)$ such that the output bindings are organized in \emph{tuples}.
For example, one possible answer tuple is $\{?v_1: \texttt{F}, ?v_2: \texttt{F}, ?v: \texttt{A}\}$ denotes particular nodes (variable bindings) that satisfy the query pattern.

As shown in the previous paragraph about one-variable queries, some neural query processors have the means to keep track of the intermediate variables.
However, none of them have the means to construct answer tuples with variables bindings and it remains an open challenge how to incorporate multiple projected variables into such processors.
Furthermore, some common caveats to be taken into account include (1) dealing with unbound variables that often emerge, for example, in \texttt{OPTIONAL} queries covered in \autoref{sec:query_ops}, where answer tuples might contain an empty value ($\emptyset$ or \texttt{NULL}) for some variables; (2) the growing complexity issue where the answer set might potentially be polynomially large depending on the number of projected variables.
