
\section{Modeling}
\label{sec:learning}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=\linewidth]{figs/nqe_enc_proc_dec.pdf}
	\caption{\emph{Neural Query Execution} through the \emph{Encoder}-\emph{Processor}-\emph{Decoder} modules. Encoder function $f$ builds representations of inputs (query, target graph, auxiliary data) in the latent space. Processor $P$ executes the query with its logical operators against the graph conditioned on other inputs. Decoder function $g$ builds requested outputs that might be discrete or continuous.}
	\label{fig:enc_proc_dec}
\end{figure}

In this section, we discuss the literature from the perspective of \emph{Modeling}. Following the common methodology~\citep{battaglia2018relational}, we segment the \emph{Modeling} methods through the lens of \emph{Encoder-Processor-Decoder} modules (illustrated in \autoref{fig:enc_proc_dec}).
(1) The \emph{Encoder} $\textsc{Enc}()$ takes an input query $q$, target graph $\gG$ with its entities and relations, and auxiliary inputs (\eg, node, edge, graph features) to build their representations in the latent space.
(2) The \emph{Processor} $P$ leverages the chosen inductive biases to process representations of the query with its logical operators in the latent or symbolic space.
(3) The \emph{Decoder} $\textsc{Dec}()$ takes the processed latents and builds desired outputs such as a distribution over discrete entities or regression predictions in case of continuous tasks.
Generally, encoder, processor, and decoder can be parameterized with a neural network $\theta$ or be non-parametric.
Finally, we analyze computational complexity of existing processors.

\subsection{Encoder}
\label{sec:trans_ind}

We start the modeling section with encoders, \ie, how different methods encode and represent entities and relations from the KG. There are three different categories, \emph{Shallow Embedding}, \emph{Transductive Encoder}, and \emph{Inductive Encoder} each representing a different way of producing the neural representation of the entities/relations.
%
Different encoding methods are suitable in different inference setups (details in \Secref{sec:graph_inference}), and may further require different logical operator methods (details in \Secref{sec:models}).
\autoref{fig:model_encoders} illustrates the three common encoding approaches.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=\linewidth]{figs/nqe_encoders_v2.pdf}
	\caption{Categorization of \emph{Encoders}. Shallow encoders perform entity and relation embedding lookup and send them to the processor. Transductive encoders additionally enrich the  representations with query, graph, classes, or other latents. Inductive encoders do not need learnable entity embeddings.}
	\label{fig:model_encoders}
\end{figure}


\input{tables/tab_encoder}

\paragraph{Shallow Embeddings.} The first line of approaches encodes each entity/relation on the graph as a low-dimensional vector, and thus we achieve an entity embedding matrix $\mE$ and a relation embedding matrix $\mR$. The shape of the entity embedding matrix is $|\gE|\times d$ ($|\gR|\times d$), where $d$ is the dimension of the embedding. Shallow embedding methods assume independence of the representation of all the nodes on the graph. This independence assumption gives the model much freedom, free parameters to learn. %
Such modeling origins from the KG completion literature, where the idea is to learn the entity and relation embedding matrices by optimizing a pre-defined distance/score function over all edges on the graph, \eg, a triplet fact $\texttt{dist}(\Em{e_s}, \Em{r}, \Em{e_o})$. The majority of query answering literature follows the same paradigm with various different embedding spaces and distance functions to learn the entity and relation embedding matrices.
% 
%
Multiple embedding spaces have been proposed. For example, GQE~\citep{gqe} and Query2Box~\citep{q2b} embed into $\mathbb{R}^d$ (point vector in the Euclidean space); FuzzQE~\citep{fuzz_qe} embeds into the space of real numbers in range $[0,1]$ (fuzzy logic score); BetaE~\citep{betae} uses Beta distribution, a probabilistic embedding space; ConE~\cite{cone} on the other hand embeds entities as a point on a unit circle. Each design choice motivates the inductive bias for executing logical operators (as we show in \Secref{sec:models}).
Some approaches employ shallow entity and relation embeddings already pre-trained on a simple link prediction task and just apply on top of them a query answering decoder with non-parametric logical operators. 
For example, CQD~\citep{cqd}, LMPNN~\citep{lmpnn}, Var2Vec~\citep{var2vec}, and $\text{CQD}^{\gA}$~\citep{cqda} take pre-trained embeddings in the complex space $\mathbb{C}^d$ and apply non-parametric \emph{t-norms} and \emph{t-conorms} to model intersection and union, respectively. 
QTO~\citep{qto} goes even further and fully materializes scores of all possible triples in one $[0,1]^{|\gR| \times |\gE| \times |\gE|}$ matrix given pre-trained entity and relation embeddings at preprocessing stage.

Despite being the mainstream design choice, the downside of shallow methods is that (1) shallow embeddings do not use any inductive bias and prior knowledge of the entity or its neighboring structure since the parameters of all entities/relations are free parameters learned from scratch; (2) they are not applicable in the inductive inference setting since these methods do not have a representation/embedding for those unseen novel entities by design. 
One possible solution is to randomly initialize one embedding vector for a novel entity and finetune the embedding vector by sampling queries involving the novel entity (detailed in \autoref{sec:graph_training}) 
However, such a solution requires gradient steps during inference, rendering it not ideal.


\paragraph{Transductive Encoder.} Similar to shallow embedding methods, transductive encoder methods learn the same entity embedding matrix $\gE$. Besides, they learn an additional encoder $\textsc{Enc}_{\theta}(q, \mE, \mR, \dots)$ (parameterized with $\theta$) on top of the query $q$, entity and relation embedding matrices (and, optionally, other available inputs). The goal is to apply the encoder to the embeddings of entities in the query $q$ in order to capture dependencies between neighboring entities in the graph. 
Specifically, the additional encoder may take several rows of the feature matrix as input and further apply transformations. For example, BiQE~\citep{biqe} and kgTransformer~\citep{kgtrans2022} linearize a query graph $\gG_q$ into a sequence and apply a Transformer~\citep{transformer} encoder that attends to all other embeddings in the query and obtain the final representation of the \texttt{[MASK]} token as the target query. MPQE~\citep{mpqe} and StarQE~\citep{starqe} run a message passing GNN architecture on top of the query graph $\gG_q$ to enrich entity and relation embeddings and extract the final node representation as the query embedding. These methods share similar benefit and disadvantage of the shallow embeddings. Namely, there are many free parameters in the method to train. Unlike the shallow embeddings, the additional encoder leverages relational inductive bias between an entity and its neighboring entities or other entities in a query, allowing for a better learned entity representation and generalization capacity. However, since at its core the method is still based on the large look-up matrix of entity embeddings, it still exhibits the same downside that all such methods cannot be directly applied to an inductive setting where we may observe new entities.

\paragraph{Inductive Encoder.}
In order to address the aforementioned challenges of shallow embeddings and transductive encoders, inductive encoder methods aim to avoid learning an embedding matrix $\mE$ for a fixed number of entities. Instead, inductive representations are often calculated by leveraging certain \emph{invariances}, that is, the features that remain the same when transferred onto different graphs with new entities at inference time. 
As we describe in \autoref{sec:graph_inference}, inductive encoders might employ different invariances albeit the majority of inductive encoders rely on the assumption of the fixed set of relation types $\gR$.
Formally, following Definition~\ref{def:basic_graph_query}, given a complex query $\gQ = (\gE', \gR', \gS', \bar{\gS'})$ composed of entity and relation terms $\gE', \gR'$ (that, in turn, contain constants $\con$ and variables $\variable$), relation projections $R(a,b) \in \gS$ (and, optionally, in $\bar{\gS'}$), a target graph $\gG$ (and, optionally, other inputs), inductive encoders learn a conditional representation function $\textsc{Enc}_\theta(e|\gE', \gR', \gG, \dots)$ for each entity $e \in \gE$.
\citet{galkin2022} devise two families of inductive representations, \ie, (1) inductive \emph{node representations} and (2) inductive \emph{relational structure} representations. 

Inductive \textbf{node representation} approaches parameterize $\textsc{Enc}_\theta$ as a function of a fixed-size invariant vocabulary.
For instance, NodePiece-QE~\citep{galkin2022} employs the invariant vocabulary of relation types and parameterizes each entity through the set of incident relations.
TeMP~\citep{temp2022} employs the invariant vocabulary of entity types and class hierarchy and injects their representations into entity representations.
Inductive node representation approaches reconstruct embeddings of new entities and can be used as a drop-in replacement of shallow lookup tables paired with any \emph{processor} method, \eg, NodePiece-QE used CQD as the processor while TeMP was probed with GQE, Query2Box, BetaE, and LogicE processors.

Inductive \textbf{relational structure} representation methods parameterize $\textsc{Enc}_\theta$ as a function of the relative relational structure that only requires learning of relation embeddings and uses relations as invariants. 
Such methods often employ various \emph{labeling tricks}~\citep{labeling_trick} to label constants (anchor entities) of the input query $\gQ$ such that after the message passing procedure all other nodes would encode a graph structure relative to starting nodes.
In particular, GNN-QE~\citep{gnn_qe} labels anchor nodes with the embedding vector of the queried relations, \eg, for a projection query $(h, r, ?)$ a node $h$ will be initialized with the embedding of relation $r$, whereas all other nodes are initialized with the zero vector. In this way, GNN-QE learns only relation embeddings $\mR$ and GNN weights. 
GNNQ~\citep{gnnq} represents a query with its variables and relations as a hypergraph and learns a relational structure through applying graph convolutions on hyperedges. Hyperedges are parameterized with multi-hot feature vectors of participating relations, so the only learnable parameters are GNN weights.

Still, there exists a set of open problems for inductive models. 
As the majority of inductive methods rely on learning relation embeddings, they cannot be easily used in setups where at inference time KGs are updated with new, unseen relation types, that is, relations are not invariant. 
This fact might require exploration of novel invariances and featurization strategies~\citep{huang2022fewshot,gao2023double,chen2023generalizing}.
Inductive models are more expensive to train in terms of both time and memory than shallow models and cannot yet be easily extended to large-scale graphs.
We conjecture that inductive encoders will be in the focus of the future work in \clqa as generalization to unseen entities and graphs at inference time without re-training is crucial for updatability of \ngdb.
Furthermore, updatability might increase the role of \emph{continual learning}~\citep{cont_learning_thrun, cont_learning_ring} and amplify the negative effects of \emph{catastrophic forgetting}~\citep{mccloskey1989catastrophic} that have to be addressed by the encoders.
Larger inference graphs also present a major \emph{size generalization} issue~\citep{DBLP:conf/icml/YehudaiFMCM21,buffelli2022sizeshiftreg,zhou2022ood} when performance of GNNs trained on small graphs decreases when running inference on much larger graphs. The phenomenon has been  observed by \citet{galkin2022} in the inductive complex query answering setup.

\subsection{Processor}
\label{sec:models}

Having encoded the query and other available inputs, the \emph{Processor} $P$ executes the query in the latent (or symbolic) space against the input graph. 
Recall that a query $q$ is defined as $q(\gE', \gR', \gS, \bar{\gS})$ where $\gE'$ and $\gR'$ terms include constants $\con$ and variables $\variable$, statements in $\gS$ and $\bar{\gS}$ include relation projections $R(a,b)$, and logical operators $\textit{ops}$ over the variables.
We define \emph{Processor} $P$ as a collection of modules that perform relation projections $R(a,b)$ given constants $\con$ and logical operators $\textit{ops} \subseteq \{\wedge, \vee, \neg, \dots \}$ over variables $\variable$ (we elaborate on the logical operators in \autoref{sec:query_ops}). 
Depending on the chosen inductive biases and parameterization strategies behind those modules, we categorize \emph{Processors} into \emph{Neural} and \emph{Neuro-Symbolic} (\autoref{tab:processor1}).
Furthermore, we break down the Neuro-Symbolic processors into \emph{Geometric}, \emph{Probabilistic}, and \emph{Fuzzy Logic} (\autoref{tab:processor2}). 
Note that in this section we omit pure encoder approaches like TeMP~\citep{temp2022} and NodePiece-QE~\citep{galkin2022} that can be paired with any neural or neuro-symbolic processor.
To describe processor models more formally, we denote $\Em{e}$ as an entity vector, $\Em{r}$ as a relation vector, and $\Em{q}$ as the query embedding that is often a function of $\Em{e}$ and $\Em{r}$. We use $\gG_q$ as the query graph.

\input{tables/tab_processor_all}

\input{tables/tab_processor_neurosym}


\paragraph{Neural Processors.}
Neural processors execute relation projections and logical operators directly in the latent space $\mathbb{R}^d$ parameterizing them with neural networks. 
To date, most existing purely neural approaches operate exclusively on the query graph $\gG_q$ only executing operators within a single query and do not condition the execution process on the full underlying graph structure $\gG$. 
Since the query processing is performed in the latent space with neural networks where Union ($\vee$) and Negation ($\neg$) are not well-defined, the majority of neural processors implement only relation projection $(R(a,b))$ and intersection $(\wedge)$ operators. 
We aggregate the characteristics of neural processors as to their embedding space, the way of executing relation projection, logical operators, and the final decoding distance function in \autoref{tab:proc_neural}.
We illustrate the difference between two families of neural processors (sequential execution and joint query encoding) in \autoref{fig:proc_neural}.


The original GQE~\citep{gqe} is the first example of the neural processor. 
That is, queries $\Em{q}$, entities $\Em{e}$, and relations $\Em{r}$ are vectors in $\mathbb{R}^d$.
Query embedding starts with embeddings of constants $\gC$ (anchor nodes $\Em{e}$) and they get progressively refined through relation projection and intersection, \ie, it is common to assume that query embedding at the initial step 0 is equivalent to embedding(s) of anchor node(s), $\Em{q}^{(0)}=\Em{e}$.
Relation projection is executed in the latent space with the translation function $\Em{q}+\Em{r}$, and intersection is modeled with the permutation-invariant DeepSet~\citep{deepsets} neural network.
Several follow-up works improved GQE to work with hashed binary vectors $\{+1, -1\}^d$~\citep{gqe_bin} or replaced DeepSet with self-attention and translation-based projection to a matrix-vector product~\citep{cga}.
Recently, \citet{smore} proposed DistMult-m, ComplEx-m, and RotatE-m, extensions of simple link prediction models for complex queries that, inspired by GQE, perform relation projection by the respective composition function and model the intersection operator with DeepSet and, optionally, L2 norm.

\begin{figure}[t]
	\centering
	\begin{subfigure}[b]{0.4\textwidth}
	    \centering
	    \includegraphics[width=\linewidth]{figs/nqe_proc_neural_3.pdf}
	\end{subfigure}
	\begin{subfigure}[b]{0.55\textwidth}
	    \centering
	    \includegraphics[width=\linewidth]{figs/nqe_proc_neural_2_3.pdf}
	\end{subfigure}
	\caption{Neural Processors. (a) Relation projections $R_\theta(a,b)$ and logical operators (non-parametric or parameterized with $\theta$) are executed sequentially in the latent space; (b) a query is encoded to a graph or linearized to a sequence and passed through the encoder (GNN or Transformer, respectively). A pooled representation denotes the query embedding.}
	\label{fig:proc_neural}
\end{figure}

The other line of works apply neural encoders to whole query graphs $\gG_q$ without explicit execution of logical operators. Depending on the query graph representation, such encoders are often GNNs, Transformers, or MLPs. 
It is assumed that neural encoders can implicitly capture logical operators in the latent space during optimization.
For instance, MPQE~\citep{mpqe}, StarQE~\citep{starqe}, and LMPNN~\citep{lmpnn} represent queries as relational graphs (optionally, hyper-relational graphs for StarQE) where each edge is a relation projection and intersection is modeled as two incoming projections to the same variable node. 
All constants $\gC$ and known relation types are initialized from the respective embedding matrices. 
All variable nodes in all query graphs are initialized with the same learnable \texttt{[VAR]} feature vector while all target nodes are initialized with the same \texttt{[TAR]} vector.
Then, the query graph is passed through a GNN encoder (R-GCN~\citep{rgcn} for MPQE, StarE~\citep{stare} for StarQE, GIN~\citep{gin} for LMPNN), and the final state of the \texttt{[TAR]} target node is considered the final query embedding ready for decoding.
A recent LMPNN extends query graph encoding with an additional edge feature indicating whether a given projection $R(a,b)$ has a negation or not and derives a closed-form solution for the merged projection and negation operator for the ComplEx composition function.
A different approach is taken by GNNQ~\citep{gnnq} that frames query answering as a subgraph classification task.
That is, an input query is not directly executed over a given graph $\gG$, but, instead, the task is to classify whether a given precomputed subgraph $\gG' \subset \gG$ satisfies a given conjunctive query. 
For that, GNNQ first augments the graph with Datalog-derived triples and converts the subgraph to a hypergraph where only hyperedges are parameterized with learnable vectors. 
On the one hand, this strategy allows GNNQ to be inductive and not learn entity embeddings. On the other hand, GNNQ is limited to conjunctive queries only and extensions to union and negation queries are not defined.

A more exotic approach by \citet{sheaves} is based on the sheaf theory and algebraic topology~\citep{hansen2019sheaf}. There, a graph is represented as a cellular sheaf and conjunctive queries are modeled as chains of relations (0-cochains). A sheaf is induced over the query graph and relevant answers should be consistent with the induced sheaf and entity embeddings. The optimization problem is a harmonic extension of a 0-cochain using \emph{sheaf Laplacian} and \emph{Schur complement} of the sheaf Laplacian. Conceptually, this approach merges execution of projection and intersection operators as functions over topological structures. 

Considering Transformer encoders, BiQE~\citep{biqe}, kgTransformer~\citep{kgtrans2022}, and SQE~\citep{sqe} linearize a conjunctive query graph into a sequence of relational paths composed of entity constants $\gC$ and relation tokens. 
The order of tokens in paths and intersections of paths are marked with positional encodings.
The target node (present in many paths) is marked with the \texttt{[MASK]} token (optionally, kgTransformer also annotates existentially quantified variables with \texttt{[MASK]}). 
SQE does not model variables explicitly but instead relies on auxiliary \emph{bracket} tokens that separate branches of the computation graph.
Passing the sequence through the Transformer encoder, the final query embedding is the aggregated representation of the target node.
BiQE only supports conjunctive queries while kgTransformer converts queries with unions to the Disjunctive Normal Form (DNF) with post-processing of score distributions (we elaborate on query rewritings and normal forms in \autoref{sec:query_ops}).
SQE explicitly includes all operator tokens into the linearized sequence and thus supports negations.

Finally, MLPMix~\citep{mlpmix} sequentially executes operations of the query where projection, intersection, and negation operators are modeled as separate learnable MLPs. Union queries are converted to DNF such that they can be answered with projection and intersection operators with the final post-processing of scores as a union operator.
Similarly, Query2Particles~\citep{query2particles2022} represents each query as a set of vectors in the embedding space and models projection, intersection, and negation operators as attention over the set of particles followed by an MLP. Union is a concatenation of query particles.

\input{tables/tab_method_neural}


\paragraph{Neuro-Symbolic Processors.}
In contrast to purely neural and symbolic models, we define \emph{neuro-symbolic} processors as those who (1) explicitly design logic modules (or neural logical operators) that simulate the real logic/set operations, or rely on various kinds of fuzzy logic formulations to provide a probabilistic view of the query execution process, and (2) execute relation traversal in the latent space. The key difference between neuro-symbolic processors and the previous two is that neuro-symbolic processors explicitly model the logical operations with strong inductive bias so that the processing / execution is better aligned with the symbolic operation (\eg, by imposing restrictions on the embedding space) and more interpretable. We further segment these methods into the following categories.

\paragraph{Geometric Processors.}

\begin{figure}[!t]
	\centering
	\includegraphics[width=\linewidth]{figs/nqe_proc_geometric.pdf}
	\caption{Geometric Processors and their inductive biases.}
	\label{fig:proc_geometric}
\end{figure}


Geometric processors design an entity/query embedding space with different geometric intuitions and further customize neuro-symbolic operators that directly simulate their logical counterparts with similar properties (as illustrated in \autoref{fig:proc_geometric}). 
We aggregate the characteristics of geometric models as to their embedding space and inductive biases for logical operators in \autoref{tab:proc_geometric}.

Query2Box~\citep{q2b} embeds queries $\Em{q}$ as hyper-rectangles (high-dimensional boxes) in the Euclidean space. To achieve that, entities $\Em{e}$ and relations $\Em{r}$ are embedded as points in the Euclidean space where each relation has an additional learnable offset vector $\Em{r_o}$ (entities' offsets are zeros). The projection operator is modeled as an element-wise summation $\Em{q} + \Em{r}$ of centers and offsets of the query and relation, that is, the initial box is obtained by projecting the original anchor node embedding $\Em{e}$ (with zero offset) with the relation embedding $\Em{r}$ and relation offset $\Em{r_o}$. Accordingly, an attention-based neuro-intersection operator is designed to simulate the set intersection of the query boxes in the Euclidean space. The operator is closed, permutation invariant and aligns well with the intuition that the size of the intersected set is smaller than that of all input sets. 
The union operator is achieved via DNF, that is, union is the final step of concatenating results of operand boxes. 
Several works extend Query2Box, \ie, Query2Onto~\citep{q2b_onto} attempts to model complex ontological axioms by materializing entailed triples and enforcing hierarchical relationship using inclusion of the box embeddings; RotatE-Box~\citep{regex} designs an additional rotation-based Kleene plus ($+$) operator denoting relational paths (we elaborate on the Kleene plus operator in \autoref{sec:query_ops}); NewLook~\citep{newlook} adds symbolic lookup from the adjacency tensor\footnote{Incorrect implementation led to the major test set leakage and incorrect reported results.} to the operators, modifies projection with MLPs and models the \emph{difference} operator as attention over centers and offsets (note that the difference operator is a particular case of the \emph{2in} negation query, we elaborate on that in \autoref{sec:query_ops}); Query2Geom~\citep{query2geom} replaces an attention-based intersection operator with a simple non-parametric closed-form geometric intersection of boxes. 

\input{tables/tab_method_geometric}

HypE~\citep{hype} extends the idea of Query2Box and embeds a query as a hyperboloid (two parallel pairs of arc-aligned horocycles) in a Poincar\'e hyperball to better capture the hierarchical information. A similar attention-based neuro-intersection operator is designed for the hyperboloid embeddings with the goal to shrink the limits with DeepSets. 

ConE~\citep{cone}, on the other hand, embeds queries on the surface of a set of unit circles. Each query is represented as a cone section and the benefit is that in most cases the intersection of cones is still a cone, and the negation/complement of a cone is also a cone thanks to the angular space bounded by $2\pi$. Based on this intuition, they design geometric neuro-intersection and negation operators.

To sum up, the geometric-based processors are often designed with a strong geometric prior such that %one or more 
properties of the logical/set operations can be better simulated or satisfied.

\paragraph{Probabilistic Processors.}

\begin{figure}[!t]
	\centering
	\includegraphics[width=\linewidth]{figs/prob.pdf}
	\caption{Probabilistic Processors and their inductive biases.}
	\label{fig:proc_probabilistic}
\end{figure}

Instead of a geometric embedding space, probabilistic processors aim to model the query and the logic/set operations in a probabilistic space. Such methods are similar to the geometric based methods in that they also require a probabilistic space on which one can design a neuro-symbolic logic/set operator that aligns well with the real one. Some examples of implementing logical operators in a probabilistic space are illustrated in \autoref{fig:proc_probabilistic}. The aggregated characteristics are presented in \autoref{tab:proc_prob}.

BetaE~\citep{betae} builds upon the Beta distribution and embeds entities/queries as high-dimensional Beta distribution with learnable parameters. The benefit is that one can design a parameterized neuro-intersection operator over two Beta embeddings where the output is still a Beta embedding with more concentrated density function. A neuro-negation operator can be designed by simply taking the reciprocal of the parameters in order to flip the density. 
PERM~\citep{perm} looks at the Gaussian distribution space and embeds queries as a multivariate Gaussian distribution. Since the product of Gaussian probability density functions (PDFs) is still a Gaussian PDF, the neuro-intersection operator accordingly calculates the parameters of the Gaussian embedding of the intersected set. 
NMP-QEM~\citep{nmp_qem} develops this idea further and represents a query as a mixture of Gaussians where logical operators are modeled with MLP or attention over distribution parameters. 
LinE~\citep{line2022} transforms the Beta distribution into a discrete sequence of values. A similar neuro-negation operator is introduced by taking the reciprocal as BetaE while designing a new neuro-intersection/union operator by taking element-wise min/max. % 
GammaE~\citep{gammae} replaces Beta distribution with Gamma distribution as entity and query embedding space. 
Parameterizing logical operators with operations over mixtures of Gamma distributions, union and negation become closed, do not need DNF or DM transformations, and can be executed sequentially along the query computation graph.
Overall, probabilistic processors are similar to geometric processors since they are all inspired by certain properties of the probability and geometry used for embeddings and customize neuro-logic operators.

\input{tables/tab_method_prob}

\paragraph{Fuzzy-Logic Processors.}


Unlike the methods above, fuzzy-logic processors directly model all logical operations using existing fuzzy logic theory~\citep{tnorm,vankrieken_fuzzy} where intersection can be expressed via \emph{t-norms}
%
and union via corresponding \emph{t-conorms} (\autoref{sec:tnorms}). 
%
In such a way, fuzzy-logic processors avoid the need to manually design or learn neural logical operators as in the previous two processors but rather directly use established fuzzy operators commonly expressed as differentiable, element-wise algebraic operators over vectors (\autoref{fig:proc_fuzzy}). While intersection, union, and negation are non-parametric, the projection operator might still be parameterized with a neural network. The aggregated characteristics of fuzzy-logic processors are presented in \autoref{tab:proc_fuzzy}. Generally, fuzzy processors aim to combine execution in \emph{embedding space} (vectors) with \emph{entity space} (symbols). The described methods are different in designing such a combination.

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figs/nqe_proc_fuzzy_2.pdf}
	\caption{Fuzzy-Logic Processors and fuzzy logical operators.}
        %
	\label{fig:proc_fuzzy}
\end{figure}

One of the first fuzzy processors is EmQL~\citep{emql} that imbues entity and relation embeddings with a count-min sketch~\citep{count_min_sketch}. There, projection, intersection, and union are performed both in the embedding space and in the symbolic sketch space, \eg, intersection is modeled as an element-wise multiplication and union is an element-wise summation of two sketches.
CQD~\citep{cqd} scores each atomic formula in a query with a pretrained neural link predictor and uses t-norms and t-conorms to compute the final score of a query directly in the \emph{embedding space}. CQD does not train any neural logical operators and only requires pretraining the entity and relation embeddings with one-hop links such that the projection operator is equivalent to \emph{top-k} results of the chosen scoring function, \eg, ComplEx~\citep{lacroix2018complex}. 
The idea was then extended in several directions: Query Tree Optimization (QTO)~\citep{qto} added a look-up from the materialized tensor of scores of all possible triples $\Em{M}\in [0,1]^{|\gR|\times |\gE| \times |\gE|}$ to the relation projection step; $\text{CQD}^{\gA}$~\citep{cqda} and Var2Vec~\citep{var2vec} learn an additional linear transformation of the entity-relation concatenation $\Em{W}[\Em{q},\Em{r}]$.

LogicE~\citep{logic_e} designs logic embeddings for each entity with a list of \emph{lower bound -- upper bound} pairs in range $[0,1]$, which can be interpreted as a uniform distribution between the lower and upper bound. LogicE executes negation and conjunction with continuous t-norms over the lower and upper bounds. FuzzQE~\citep{fuzz_qe} and TAR~\citep{abin_abductive2022} embed query to a high-dimensional fuzzy space $[0,1]^d$ and similarly use G\"odel t-norm and \L{}ukasiewicz t-norm to model disjunction, conjunction and negation.
FuzzQE models relation projection as a relation-specific MLP whereas TAR uses a geometric translation (element-wise sum). FLEX~\citep{lin2022flex} and TFLEX~\citep{lin2022tflex} embed a query as a mixture of feature and logic embedding. For the logic part, both methods use the real logical operations in vector logic~\citep{mizraji2008vector}. TFLEX adds a temporal module conditioning logical operators on the time embedding.

GNN-QE~\citep{gnn_qe} models the likelihood of all entities for each relation projection step with a graph neural network NBFNet~\citep{zhu2021neural}. It further adopts product logic to directly model the set operations (intersection, union, and negation) over the fuzzy set obtained after a relation projection. GNN-QE employs a node labeling technique where a starting node is initialized with the relation vector (while other nodes are initialized with zeros). This allows GNN-QE to be inductive and not rely on trainable entity embeddings.  
ENeSy~\citep{enesy}, on the other hand, maintains both vector and symbolic representations for queries, entities, and relations (where symbolic relations are encoded into $\Em{M}_r$ sparse adjacency matrices). Logical operators are executed first in the neural space, \eg, relation projection is RotatE composition function~\citep{sun2018rotate},  and then get intertwined with symbolic representations. Logical operators in the symbolic space employ a generalized version of the product logic and corresponding t-(co)norms. 

In summary, fuzzy-logic processors directly rely on established fuzzy logic formalisms to perform all the logical operations in the query and avoid manually designing and learning neural operators in (possibly) unbounded embedding space.
The fuzzy logic space is continuous but bounded within $[0, 1]$ -- this is both the advantage and weakness of such processors. The bounded space is beneficial for closed logical operators as their output values still belong to the same bounded space.
On the other hand, most of the known t-norms (and corresponding t-conorms) still lead to vanishing gradients and only the Product logic norms are stable~\citep{vankrieken_fuzzy,badreddine2022logic}.
Another caveat is designing an effective and differentiable interaction mechanism between the fuzzy space $[0,1]^d$ and unbounded embedding space $\RR^d$ (or $\CC^d$) where relation representations are often initialized from. 
That is, re-scaling and squashing of vector values when processing a computation graph might lead to noisy gradients and unstable training which is observed, for instance, by GNN-QE that has to turn off gradients from all but last projection step.


\input{tables/tab_method_fuzzy}


\subsection{Decoder}
\label{sec:decoder}
The goal of decoding is to obtain the final set of answers or a ranking of all the entities. It is the final step of the query answering task after processing. Here we categorize the methods into two buckets: \emph{non-parametric} and \emph{parametric}. Parametric methods require a parameterized method to score an entity (or predict a regression target from the processed latents) while non-parametric methods can directly measure the similarity (or distance) between a pair of query and entity on the graph. 
Most of the methods belong to the non-parametric category as shown in the \emph{Distance} column of processor tables \autoref{tab:proc_neural}, \autoref{tab:proc_geometric}, \autoref{tab:proc_prob}, \autoref{tab:proc_fuzzy}. For instance, geometric models~\citep{q2b, q2b_onto, regex, hype, cone} pre-define a distance function between the representation of the query and that of an entity. Commonly employed distance functions are L1~\citep{gqe, smore, sheaves, mlpmix, abin_abductive2022, enesy,nmp_qem},  L2~\citep{line2022, fuzz_qe}, or their variations~\citep{logic_e, lin2022flex, lin2022tflex}, cosine similarity~\citep{gqe_bin, cga, mpqe, lmpnn}, dot product~\citep{emql, smore, starqe,kgtrans2022,query2particles2022,nqe}, 
or naturally model the likelihood of all the entities without the need of a distance function~\citep{cqd,biqe,gnn_qe,gnnq,qto,cqda,var2vec}. 
Probabilistic models often employ KL divergence~\citep{betae, gammae} or Mahalanobis distance~\citep{perm}.

One important direction (orthogonal to the distance function) that current methods largely ignore is how to perform efficient answer entity retrieval over extremely large graphs with billions of entities. A scalable and approximate nearest neighbor (ANN) search algorithm is necessary. Existing frameworks including FAISS~\citep{faiss} or ScaNN~\citep{scann} provide scalable implementations of ANN. However, ANN is limited to L1, L2 and cosine distance and mostly optimized for CPUs. It is still an open research problem how to design efficient scalable ANN search algorithms for more complex distance functions such as KL divergences so that we can retrieve with much better efficiency for different \clqa methods with different distance functions (preferably, using GPUs).

We conjecture that parametric decoders are to gain more traction in numerical tasks on top of plain entity retrieval for query answering. Such tasks might involve numerical and categorical features on node-, edge-, and graph levels, \eg, training a regressor to predict numerical values for node attributes like \emph{age}, \emph{length}, etc. Besides a parametric decoder gives new opportunities to generalize to inductive settings where we may have unseen entities during evaluation. 
SE-KGE~\citep{se_kge} takes a step in this direction by predicting geospatial coordinates of query targets.
