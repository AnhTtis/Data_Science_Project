
\section{Introduction}

Graph databases (graph DBs) are key architectures to capture, organize and navigate structured relational information over real-world entities. 
Unlike traditional relational DBs storing information in tables with a rigid schema, graph DBs store information in the form of heterogeneous graphs, where nodes represent entities and edges represent relationships between entities.
In graph DBs, a relation (i.e. heterogeneous connection between entities) is the first-class citizen. With the graph structure and a more flexible schema, graph DBs allow for a more efficient and expressive way to handle higher-order relationships between distant entities, especially navigating through multi-hop hierarchies.
While traditional DBs require expensive join operations to retrieve information, graph DBs can directly traverse the graph and navigate through links more efficiently with the adjacency matrix.
Due to its capabilities, graph databases serve as the backbone of many critical industrial applications including question answering in virtual assistants \citep{alexa,siri}, recommender systems in marketplaces \citep{amazon,uber}, social networking in mobile applications \citep{facebook}, and fraud detection in financial industries~\citep{ibmfraud,pourhabibi2020fraud}.

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figs/Fig1_CLQA_Overview_v5.pdf}
	\caption{A complex logical query \protect\encirclered{a} and its execution over an incomplete graph \protect\encirclered{b}. Symbolic engines like  SPARQL \protect\encirclered{c} perform edge traversal and retrieve an incomplete set of \emph{easy} answers directly reachable in the graph, i.e., \{\texttt{UofT}\}. Neural query execution \protect\encirclered{d} recovers missing ground truth edges (dashed) and returns an additional set of \emph{hard} answers \{\texttt{UdeM}, \texttt{NYU}\} unattainable by symbolic methods.}
	\label{fig:clqa_1}
 \vspace{-1em}
\end{figure}

Given a downstream task, one of the most important tasks of graph DBs is to perform complex query answering. The goal is to retrieve the answers of a given input query of interest from the graph database. Given the query, graph DBs first translate and optimize the query into a more efficient graph traversal pattern with a query planner, and then execute the pattern on the graph database to retrieve the answers from the graph storage using the query executor.
The storage compresses the graphs into symbolic indexes suitable for fast table lookups. 
Querying is thus fast and efficient under the assumption of completeness, i.e., stored graphs have no missing edges.

However, most real-world graphs are notoriously incomplete, e.g., in Freebase, 93.8\% of people have no place of birth and 78.5\% have no nationality~ \citep{mintz2009distant}, about 68\% of people do not have any profession~\citep{DBLP:conf/www/WestGMSGL14}, while in Wikidata, about 50\% of artists have no date of birth~\citep{zhang2022enriching}, and only 0.4\% of known buildings have information about height~\citep{DBLP:conf/www/Ho0MSW22}.
Na\"ively traversing the graph in light of incompleteness leads to a significant miss of relevant results and the issue further exacerbates with an increasing number of hops.
This inherently hinders the application of graph databases.
Link prediction is a challenging task, prior works predict links by learning a latent representation of each link \citep{transe,distmult,complex,rotate} or mining rules~\citep{Galrraga2013AMIEAR,Xiong2017DeepPathAR,Lin2018MultiHopKG, qu2021rnnlogic}. However, it is always a trade-off between possibly incomplete results and decidability -- with a denser graph, some SPARQL entailment regimes~\citep{sparql2013} do not guarantee that query execution terminates in finite time.

On the other hand, recent advances in graph machine learning enabled expressive reasoning over large graphs in a latent space without facing decidability bottlenecks. 
The seminal work of \citet{gqe} on Graph Query Embedding (GQE) laid foundations of answering complex, database-like logical queries over incomplete KGs where inferring missing links during query execution is achieved via parameterization of entities, relations, and logical operators with learnable vector representations and neural networks.
For the incomplete knowledge graph in (\autoref{fig:clqa_1}), given a complex query \emph{``At what universities do the Turing Award winners in the field of Deep Learning work?''}, traditional symbolic graph DBs (SPARQL- or Cypher-like) would return only one answer (\texttt{UofT}), reachable by edge traversal. 
In contrast, neural query embedding parameterizes the graph and the query with learnable vectors in the embedding space.
Neural query execution is akin to \emph{traversing the graph and executing logical operators in the embedding space} that infers missing links and enriches the answer set with two more relevant answers \texttt{UdeM} and \texttt{NYU} unattainable by symbolic DBs. 

Since then, the area has seen a surge of interest with numerous improvements of supported logical operators, query types, graph modalities, and modeling approaches.
In our view, those improvements have been rather scattered, without an overall aim.
There still lacks a unifying framework to organize the existing works and guide future research.
To this end, we propose to present one of the first holistic studies about the field. We devise a taxonomy classifying existing works along three main axes, \ie, 
\begin{inparaenum}[(i)]
\item \textbf{Graphs} (\autoref{sec:graphs} --
logical formalisms behind the underlying graph and its schema)
\item \textbf{Modeling} (\autoref{sec:learning} -- what are the neural approaches to answer queries)
\item \textbf{Queries} (\autoref{sec:queries} -- what queries can be answered).
\end{inparaenum}
We then discuss \textbf{Datasets and Metrics} (\autoref{sec:datasets} -- how we measure performance of \ngdb engines).
Each of these dimensions is further divided into fine-grained aspects.
%
Finally, we list \clqa applications (\autoref{sec:apps}) and summarize open challenges for future research (\autoref{sec:future}).

We further propose a novel framework \emph{Neural Graph Databases} (NGDB, \autoref{sec:ngdb}), where complex approximate query answering (\autoref{sec:prelim}) is the core task and approached by the neural query embedding methods.
NGDB consists of a neural graph storage and a neural query engine, which correspond to a neural version of both counterparts in a regular graph DB respectively.
Besides a graph store and feature store that store the graph structure as well as the multimodal node-/edge-level features, neural graph storage assumes an encoder that further compresses the raw information in a semantic-preserving latent embedding space, \ie, similar entities/relations are mapped to similar embeddings. During retrieval, such a design choice instantly allows for both exact identity match as well as approximate nearest neighbor search.
As another key component, the neural query engine is responsible for optimizing, planning and executing a given input query in the embedding space, and finally returning the query answers by interacting with the neural graph storage.
We provide a detailed conceptual scheme for an NGDB with various levels of design principles and assumptions over the two components.
We hope this sheds light on the current state of the art and provides a roadmap for future research on NGDBs.
%

\textbf{Related Work.} 
While there exist insightful surveys on general graph machine learning~\citep{chami2022survey}, simple link prediction in KGs~\citep{ali2021light,chen2023generalizing}, and logic-based link prediction~\citep{zhang2022_logic_emb_survey,delong2023Neurosymbolic}, the complex query answering area remained uncovered so far.
With our work, we close this gap and provide a holistic view on the state of affairs in this emerging field.