\section{Goldfish}
\label{sec:goldfish}




$\GF$ is a distributed algorithm that each node runs locally to optimize its  
%\sbv{what is visibility?} 
%\bx{shortest topological distances } 
shortest topological distances to other nodes in the P2P network. $\GF$ mimics an intelligent entity optimizing for its own improvements: it collects surrounding information, organizes it and recognizes pattern in order to make next move to fit  in a ever-changing environment. They correspond to four components, as shown in Fig~\ref{fig:pipeline}. They are Network-Storage, Matrix Constructor, K-NN Matrix completer and Peer Selector. 

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.42\textwidth]{figs/pipeline.png}
    \caption{Goldfish components flowchart}
    \label{fig:pipeline}
\end{figure}

We refer to the data forwarded by a node as a message, and refer to the content of a message as a block. A node can receive multiple messages of the same block. The Network-Storage component observes and keeps a memory about any delivered data in the local storage and formats it as a (peer, block, delivery time) tuple. The delivery time is measured with relative timestamp, see Section \ref{sec:system_model}.

At initial, $\GF$ is triggered to collect data tuple, and uses a Matrix Constructor to organize observations from the local storage for creating data batches. A batch contains messages observed in a contiguous time span without connection changes, we use epoch $E_1,..E_\ell$ to denote each of the $\ell$ batches 
corresponding to each contiguous time span.

For messages collected in a single epoch, we can create a 2D matrix, where each block has its own row vector, and each peer has its own column vector. The intersection of row and column stores the recorded latency. But to concatenate blocks from multiple epochs into a single matrix, every block vector has to insert empty cells for peers only present in other epochs. The \textbf{Matrix Constructor} creates a synthesized matrix by concatenating epochs along the block axis, and its peer axis is union of connections across epochs. A synthesized matrix from 2 epochs is shown in Table(\ref{table:constructor}) where $\star$ are missing observations, which is further discussed in the following sections. 


% To mentally complete a network picture, a 
$\GF$ uses Matrix completer to estimate the missing cells. First, the completer classifies all cells in the matrix into 4 categories: \textbf{observed}, \textbf{symbolically observed}, \textbf{estimable} and \textbf{inestimable}. For all estimable missing cells, they are formulated as optimization variables in a non-convex optimization formulation. While designing the loss function, we have a key insight that block vectors exhibiting similar peer delivery pattern are more likely from the source-nodes close to each other. We can compare those block(row) vectors and use K-NN to interpolate the missing cell using nearest neighbor. To solve for solution, we create a pytorch tensor graph (see Section~\ref{sec:related}) to empirically optimize for the missing values. 

After estimating the missing cells,
$\GF$ can use any peer exploration algorithm. One particular selection algorithm that helps to evaluate $\GF$ performance is the depleting pool, where exploration peers are drawn from a depletion pool which includes all nodes in the network at the beginning; the peers are taken out of the pool without replacement, and resets itself until the pool is emptied. 
%\sbv{The exploitation part below is not crisp.}
Exploitation peers are chosen based on an altruistic heuristics. We first identify a set of contributing peers who always deliver blocks fast to us, and we credit them scores by counting the number of benefiting peers who receives block fast from us. The altruistic principle is based on an intuition: if a $\GF$ can help so many other nodes, then itself must already be well connected, so a $\GF$ should improve itself by the measure how helpful it becomes to the others. A $\GF$ can occasionally skip recognition step if it wants to speedup exploration as represented by the diagonal in Fig.~\ref{fig:pipeline}, and this is implemented inside Scheduler submodule.
%information from benefiting peers who receives some blocks firstly from the $\GF$ (who earlier just received the blocks from the contributing peer). $\GF$ selector uses
%$\GF$ first identifies a set of contributing peers from whom $\GF$ first receives some blocks recorded in the matrix. Associating to each contributing peer is a list of benefiting peers who receives some blocks firstly from the $\GF$ (who earlier just received the blocks from the contributing peer). $\GF$ selector uses an altruistic principle by choosing a set of exploitation peers that maximizes benefiting peers; it comes from an intuition that if a $\GF$ can help so many other nodes, then itself must already be well connected. A $\GF$ can occasionally skip recognition step if it wants to speedup exploration, and this is achieved by a Scheduler represented by the diagonal in Fig.~\ref{fig:pipeline}.   

Since the system is remarked by its short memory only to appreciate recent data (because historic data is not reflective to current network condition), the completer only needs to process a few epochs at a time. It greatly reduces the space and computation complexity. 
%and by its fairly simple pattern recognizing logic requiring much less tensor (neuron) than other popular Neural Network applications, a small creature with decent intelligence is well fitted for the system, and hence $\GF$. 
For the rest of the section, we characterize every component of $\GF$ with greate details.
 
 %uses the saved data dating back from the earliest connections to the latest network connections for creating a matrix whose rows represent every received block and whose columns represent ever connected peers. Then the Matrix Completer fills the missing entries, which is used by the Peer selector to decide what outgoing peers potentially improve its broadcast latency; the output of the selector are exploitation peers from currently active connections (both incoming and outgoing), whereas the exploration peers are chosen randomly. Algorithm can optionally fix exploitation set for more explorations, which is shown in the diagonal line.





%Since each epoch can be represented by a  matrix containing block deliver latency, whose rows are unique blocks, and whose columns are active peers (incoming and outgoing), merging multiple matrix together is simply done by concatenating matrix along block axis, and expanding the peer axis by union of active peers in each epochs. Table \ref{table:completer} (a) is a merged partially observed matrix constructed for epoch $c=1, d=2$. In both epochs, the number of active connection is 3 (epoch $E_1$ connects with $n_1, n_2, n_5$, whereas $E_2$ connects with $n_1, n_2, n_4$); peer $n_5, n_6$ are not connected in neither epoch. Next, we discuss the value contained in each cells of the matrix.. The node in each epoch perceives 4 blocks, $m_{1-4}$ in $E_1$ and $m_{5-8}$ in $E_2$. Each block has an implicit publishing node marked by superscript. Note that $\GF$ does not need this information, it is used only to elaborate the example.



\begin{table*}[!hbt]
\renewcommand{\arraystretch}{1.35}
\centering
%\begin{threeparttable}
\caption{Transformation from (a) Synthesized matrix to (b) Output of Matrix completer }
\begin{subtable}[h]{.385\linewidth}
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|l| c c c c|}
\hline
 \multirow{2}{*}{}  &  Block    & \multicolumn{4}{c|}{Peers} \\
   &   & $n_1$ & $n_2$ & $n_3$ & $n_4$\\ 
\hline
\multirow{3}[0]{*}{ $E_1$ } &
  $m_1^{n_5}$ & $0$       & $t_{1,2}$ & $\star$ & $t_{1,4}$\\
& $m_2^{n_2}$ & $\PLUS$   & $0$       & $\star$ & $\PLUS$\\
& $m_3^{n_5}$ & $0$       & $t_{3,2}$ & $\star$ & $t_{3,4}$\\
& $m_4^{n_6}$ & $t_{4,1}$ & $t_{4,2}$ & $\star$ & $0$\\
\hline
\multirow{3}[0]{*}{ $E_2$ } & 
  $m_5^{n_2}$ & $\PLUS$   & $0$   & $\PLUS$ & $\star$  \\
& $m_6^{n_6}$ & $0$       & $t_{6,2}$ & $t_{6,3}$ & $\star$ \\
& $m_7^{n_6}$ & $0$       & $t_{7,2}$   & $t_{7,3}$ & $\star$\\
& $m_8^{n_7}$ & $\dagger$ & $0$ & $t_{8,3}$ & $\star$\\
\hline
\end{tabular}
}
\caption{$\star$ is missing cell. $\dagger$ is symbolically known cell when a peer gets the first block from local node }
\label{table:constructor}
\end{subtable}
$\Longrightarrow$
\begin{subtable}[h]{0.548\linewidth} 
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|l| c c c c|}
\hline
 \multirow{2}{*}{}  &  Block   & \multicolumn{4}{c|}{Peers} \\
   &   & $n_1$ & $n_2$ & $n_3$ & $n_4$\\ 
\hline
\multirow{3}[0]{*}{ $E_1$ } &
$m_1^{n_5}$ & $c_1$       & $c_1+t_{1,2}$ & $a_1$ & $t_{1,4}+c_1$\\
& $m_2^{n_6}$ & $\PLUS$   & $c_2$       & $\times$ & $\PLUS$\\
& $m_3^{n_6}$ & $c_3$     & $t_{3,2}+c_3$ & $a_2$ & $t_{3,4}+c_3$\\
& $m_4^{n_6}$ & $t_{4,1}+c_4$ & $t_{4,2}+c_4$ & $a_3$ & $c_4$\\
\hline
\multirow{3}[0]{*}{ $E_2$ } & 
  $m_5^{n_2}$ & $\PLUS$ & $c_5$   & $\PLUS$ & $\times$  \\
& $m_6^{n_6}$ & $c_6$   & $t_{6,2}+c_6$ & $t_{6,3}+c_6$ & $a_4$  \\
& $m_7^{n_6}$ & $c_7$   & $t_{7,2}+c_7$ & $t_{7,3}+c_7$ & $a_5$ \\
& $m_8^{n_7}$ & $\dagger$ & $c_8$   & $t_{8,3}+c_8$ & $\times$\\
\hline
\end{tabular}}
\caption{Completed matrix. $\times$ is infeasible cell. $\dagger$ is symbolically known cell. $\ddagger$ is ambiguous cell. $a_1-a_5$ are estimated missing values. $c_1-c_8$ are offset.}
\label{table:completer}
\end{subtable}

%\end{threeparttable}
\end{table*}


\subsection{Matrix Constructor}
Matrix constructor transforms the tuple data 
from multiple epochs to a single partially observed matrix. A concatenated matrix with 2 epochs in shown in Table (\ref{table:constructor}). In both epochs, the number of active connections is 3. In epoch $E_1$, the node is connecting to peers $n_1, n_2, n_4$; whereas in epoch $E_2$, the node is connecting to peer $n_1, n_2, n_3$. There are 4 blocks in each epoch being delivered to the node, with notation $m_1, ..., m_8$. Each superscript on the block denotes the original publisher for the block, which is used only to elaborate the algorithm, and 
%$n_5, n_6, n_7$
%are seven nodes $n_1,\ldots,n_7$ % $n_{1-7}$ 
%(excluding the local $\GF$ node) in the matrix, and peers $n_5, n_6, n_7$ % $n_{5-7}$ 
%are not connected in neither epoch. The $\GF$ node perceives blocks $m_1,\dots,m_4$ in $E_1$ and $m_4.\ldots,m_8$
% $m_{1-4}$ in $E_1$ and $m_{5-8}$ 
%in $E_2$. Each block has an implicit publishing node marked by its superscript; 
$\GF$ does not need this information for execution.
%, the information is used solely for the purpose to elaborate $\GF$.
As shown in Table(\ref{table:constructor}), after combining 2 epochs, each epoch contains a missing sub-column because those peers are not connected when the corresponding blocks were perceived. In the next step, $\GF$ categorizes cells in order to formulate the optimization problem.

%Matrix constructor transforms network delivery data collected from $E_1$ to $E_\ell$ during its active period. Associating with epoch is a list of connections (incoming and outgoing) and a list of blocks the node processed during that epoch. Matrix constructor takes a list of contiguous epochs, $[E_c, E_d]$, where $1\le c < d \le \ell$ to merge them into a single partially filled matrix. Since each epoch can be represented by a  matrix containing block deliver latency, whose rows are unique blocks, and whose columns are active peers (incoming and outgoing), merging multiple matrix together is simply done by concatenating matrix along block axis, and expanding the peer axis by union of active peers in each epochs. Table \ref{table:completer} (a) is a merged partially observed matrix constructed for epoch $c=1, d=2$. In both epochs, the number of active connection is 3 (epoch $E_1$ connects with $n_1, n_2, n_5$, whereas $E_2$ connects with $n_1, n_2, n_4$); peer $n_5, n_6$ are not connected in neither epoch. Next, we discuss the value contained in each cells of the matrix.. The node in each epoch perceives 4 blocks, $m_{1-4}$ in $E_1$ and $m_{5-8}$ in $E_2$. Each block has an implicit publishing node marked by superscript. Note that $\GF$ does not need this information, it is used only to elaborate the example.


\subsubsection{Preliminary Cell classification}
%\noindent {\bf Preliminary Cell classification.}
A partially observed matrix, $T$, of size $(p,q)$ contains 3 types of cells: observed, missing, and symbolically known cells. Every cell in the matrix is associated with 2 attributes: the delivering peer and the delivered block. A cell $(i,j)$, where $0<i\le p, 0<j\le q$, is an observed cell if there is a peer $n_i$ delivered a block $m_j$ to the node, and $t_{i,j}$ is the associated measurement. The measurement for an observed cell $(i,j)$ is computed as 
%\sbv{elaborate on what is measured time} \bx{
%numeric relative time observation computed as 
time difference between the peer $n_i$'s delivery time and the earliest delivery time for block $m_j$ among all $q$ peers. 
If a peer $n_i$ does not send the block $m_j$ to the node, the cell $(i,j)$ is categorized either as a missing cell($\star$), or as a symbolically known cell($\dagger$). 
A missing cell is identified if the cell is synthetically created as the result of merging multiple matrices along the block axis; a symbolically known cell is identified when the cell's associating peer does not forward the block to the $\GF$ node; 
in which case, we can infer the local node (who is creating the table) is the first node who forwards the block to that peer (since a node should not send back the block to where it comes from). 
For example in Table(\ref{table:constructor}), cell $(2,1)$ and $(2,4)$ are symbolically known, and we can infer the local node is the first one that is delivering the block $m_2$ to peer $n_2, n_4$. 
%from whom $n_1, n_4$ firstly receives $m_2$ in $E_1$, and similarly $m_6$ with $n_1, n_3$ in $E_2$ 
The table suggest a situation where $n_1, n_3, n_4$ are far away from the original publisher $n_2$ for $m_2$, and the local node is on the faster path to deliver the block to other peers. %Although the symbolically observed cells are missing values, but we do know that its value must be greater than 0. Symbolically known cells are unworthy to estimate
%\sbv{why? they could still be helpful to connect to},
Although the symbolically known cells are missing measurement data, they still worth to consider because they convey the associated peer cannot be the fastest ones to deliver similar blocks to the local node.
%that they are not the earliest deliveries, and they do not have numeric definition because the peer attributed by the cell can only keeping receiving first blocks from the $\GF$ node, unless the underlying network context changes.
%since they are not defined due to topological condition.
These 3 cell types partition $p \times q$ cells, and each cell type induces its own binary masking matrix on $T$. %Let $B$ be a binary masking matrix for the observed cell type where $B_{i,j}=1$ if $T_{i,j}$ contains a numeric observed time. 
%and let $R$ be the masking matrix for the missing cell type where $R_{i,j}=1$ if $T_{i,j}$ is a cell of type missing, and similarly $P$ for symbolically observed cells. $B+P+R$ is a $p\times q$ matrix with all $1$.


\subsubsection{Scheduler}
%\noindent {\bf Scheduler}
A local node can combine arbitrary number of epochs into a matrix for cell interpolation. But as network continues to change, historical data might damage the completion accuracy because %those data are not inherently consistent 
%\sbv{unclear. why are they inconsistent?}. \bx{
the network topology might have changed so much that those date are no longer useful to represent the current network. There are 2 design spaces to tune learning behavior: the size of the synthesized matrix determined by the number of epoch, and frequency to initiate the algorithm. For example, a node can set the matrix to contain 3 epochs, and choose to run matrix completion every 2 epoch. In which case, instead of running completion every time when peers change, the node can skip 1 learning epoch and use it to explore more peers. This speeds up node exploration, and saves resources on running computational tasks. 



%and run completer every 2 epoch. Then the second epoch in the synthesized matrix is always the exploration epoch, which keeps its exploitation peers intact, and replaces the exploration peers with a list of new exploration peers from the depletion pool.

\subsection{K-NN Matrix Completer and Missing Cell Classification}
After data is formatted into a matrix, $\GF$ uses K nearest neighbor to interpolate the missing cells. We begin by treating every row of the synthesized matrix, $T$, as a vector in the space $\mathbb{R}^q$, ($T_{i,j} = m_i[j]$), the goal is to cluster similar rows and use the available data to fill the missing ones.  


%To do that, we need a metric that can take 2 vectors and output a scalar signifying how close 2 vectors are. Ideally, if 2 vectors are generated by 2 miners topologically close to each other, the output should be a small non-negative value. 
%Another way to measure closeness is by tagging publisher information on every block. But such design cannot group together blocks whose publishers are close to each other. Before diving into method to estimate the missing values, we note that only a portion of  missing cells are estimable.

\subsubsection{Estimable Missing Cells}
%\noindent {\bf Missing Cell Categories }
The missing cells are further classified into 3 categories: estimable, 
ambiguous ($\ddagger$) and infeasible($\times$). A missing cell attributed by block $r$ peer $u$, $m_r[u]$, is estimable if there exists another block $m_i$ which has measured data from peer $u$, and  both blocks $m_i, m_r$ have measured data from least 2 common peers. The intuition is that, we can use the closeness of the observed value to derive the unobserved value.
But since we use K nearest neighbor to interpolate the missing dimensions, we need to find  K such blocks to complete the estimation. However, not all missing cells can find enough $K$ blocks 
to satisfy the requirements. Those cells are deemed as ambiguous ($\ddagger$). Infeasible cells are missing values that cannot be estimated, its definition is not central to peer selection, so we move it to the end.

\subsubsection{Distance Metrics for Selecting K Nearest Neighbor}
\label{example}
%\noindent {\bf Distance Metrics for Selecting K Nearest Neighbor. }
A metric function $g: \mathbb{R}^p \times \mathbb{R}^p \rightarrow \mathbb{R}^+$ 
takes two block vectors and produce a non-negative value.
A metrics is good if it produces a small value when   
two block sources are close and they have similar delivery viewed from a perspective of the local node.
Let's first focus on a simple case where all publishers have perfect synchronous clocks and block creation time is printed on all messages. Then if a node receives multiple messages about the same block, it can deduce about its peers' topological positions in the network based on 
the messages arrival time. 
%Immediately we know how topologically close 2 publishers are by comparing how long and similar in the 2 block vectors in every dimension reflecting different paths leading to the local node. 
%it is a much easier task to group blocks coming from close publishers, because time scale on the block hints on topological closeness, and block delivery order among direct peers tells which direction the block is coming from (in topological sense). 
But in the real world, we cannot get accurate time measurement because time synchronization precision among computer can be low, and bad data can significantly damage the model. In Section~\ref{sec:system_model}, we discussed to use relative time to solve the problem, and it has an effect to offset the fastest route to 0. It appears we miss some information, because all values subtracts a common constant, but subtracting a offset does not change the differential value among vector's dimensions. Therefore, we can still use the differential characteristics of the block vector to infer if 2 block vectors are coming from close publishers.
%But for each block, although they are offset, the relative difference 
%the relative time difference among directly connected peers are unchanged (except the missing peer), i
%f we assume network context is roughly stable. 
To construct the distance metrics, 
%Therefore we can evaluate closeness of 2 block vectors directly by subtracting 
we subtract 2 vectors and take the unbiased variance on the peer dimensions where the local node has observations to both blocks, i.e. $g(m_r, m_i) = \VAR(D_i)$ where $D_i = \{m_r[j] - m_i[j]: 0<j\le q, B_{r,j}=1, B_{i,j}=1\}$ ($B$ is a binary matrix indicating if a cell has observation). 
%The intuition comes from the idea that if publisher of two distinct blocks are close and have used similar network topology to deliver the block to the node, the peers would react to the block similarly by exhibiting similar forwarding time to the local node. 
At least 2 values are required to compute unbiased variance. %that is why we have infeasible missing cells, which is more carefully defined later.
%the question from the last paragraph. 
%Variance metrics helps $\GF$ reliably recover good performing peers even if the good peers are dropped by accident or by random exploration. 

The differential variance metrics has a good property that groups blocks whose miners are close to each other. The intuition behind is that publishers with close topological locations generate blocks that exhibit similar differential variance. We illustrate this property by an example. From the Table(\ref{table:constructor}), since $m_4[4] = 0$ we know $n_4$ is close to $n_6$  compared to other nodes, such that $n_4$ is the first one to deliver $m_4$ (mined by $n_6$) to the local node, but $n_4$ is not fast enough to make the local node be the first forwarding node to $n_1, n_2$, i.e. $n_1, n_2$ have their separate but slightly worse path than $n_4$ relative to the miner $n_6$. In epoch $E_2$, peer $n_4$ is dropped by the local node  to connect with $n_3$. %but $n_3$ is the worse peer since it is not the first relay peer for block $m_7$ which is also mined by $n_6$. 
Suppose this is the only connection change in the entire network, then relative time difference for $n_1, n_2$ to deliver the block is identical as before, i.e. $t_{4,2} - t_{4,1} = t_{6,2} = t_{7,2}$. The differential variance metrics $g(m_4, m_6)$ is computed as $\VAR(\{t_{4,1}, t_{4,2}-4_{6,2}\}) = \VAR(\{t_{4,1}, t_{4,1}\}) = 0$,  and similarly $g(m_4, m_7) = 0$. Therefore $m_4, m_6, m_7$ are grouped together in the next section for optimizing the missing cells.

\subsubsection{Optimization Formulation.}
%\noindent {\bf Optimization Formulation. } 
%After determining which missing cells can be estimated and criteria to find their K nearest neighbor, this sections describes how to formulate an optimization problem to interpolate them. 
After defining the optimization space $\mathbb{R}^q$ and the metric to find $K$ nearest neighbors, we start to formulate the unsupervised optimization problem.
Suppose there are in total $s$ estimable missing values, whose set is denoted as $S$. In the partially observed matrix $T$ of dimension $p\times q$, we associate each missing value with an optimization variable, and denote them as $A =a_1, ... , a_s$.
Each missing value in the matrix has a row and column indices, which can be encoded by 2 functions: 
$
    \textbf{row}: [s] \rightarrow [p] \quad \textbf{col}: [s] \rightarrow [q] 
$. Notation $[n]$ denotes a integer set $\{1,..,n\}$.
For each missing cell $a_v \in S$, its $k$ nearest vectors can be pre-processed using the metrics defined earlier, and the processed result can be summarized by 2 functions. Function $\textbf{N}: [s] \rightarrow [p]^k$ encodes for each missing cell $a_v$, which $k$ block vectors out of $p$ block vectors are closest to the block $m_{row(a_v)}$ where $a_v$ is a part of. Furthermore, the loss calculated by metrics $Var(D)$ between $m_{row(a_v)}$ and any of its $k$ nearest vector can be encoded as a function $\textbf{w}: [s, p] \rightarrow \mathbb{R}$. Since we only assign weight to $k$ such vector, other vectors has a weight of $0$. We further normalize the $k$ weight so that their sum equals to 1 using softmax \cite{softmax}. We also define a helper function $\textbf{L}: [p, p] \rightarrow \{0,1\}^q$ which takes 2 integers $i,j$ and returns a $q$ dimensional binary vector indicating peers who deliver block $m_i, m_j$ to the local node.

%in case when there are multiple missing variable in a single row (block), we have to normalize the effect 
%and so the weight is normalized. 
%need only be computed once before the gradient ascent loop
%Function, $L$, maps from a missing value to a $q$ dimensional indicator vector representing if the local node has observations from each of $q$ peers. Note these functions do not change as optimization variable changes, so they only need to compute once.
To illustrate how to construct the loss function, we begin with a simpler task by optimizing for a single missing cell $a_v$. We can get its $k$ closest vectors and the corresponding weight using the function defined earlier. $N(v)$ gives $k$ peers and $w(row(v),o)$ gives their weights. Since all latency are measured with relative time, we have a problem that any 2 row vectors are not comparable to each other due to a lack of common reference time. To illustrate it, 
we performed a step-by-step cells completion 
for both blocks $m_4$ and $m_6$ in the Table(\ref{table:constructor}). 
Suppose $m_4, m_6$ are  grouped together and  they are generated from one publisher,
we can calculate the vector difference based on measured time and use that to complete the missing ones. In this case, the difference are $t_{4,1} - 0 = t_{4,2} - t_{6,2}$.
%different based on their difference. We notice the performance becomes worst in block $m_4$, since $n_1$ becomes the earliest relayer after the change; 
%Since block vector $m_6$ is only a offset of $m_4$ by $t_{4,1}$. 
As the result, the missing cell $m_4[3]$ should equal to $t_{4,1} + t_{7,3}$ and the missing cell $m_7[4]$ should equal to $- t_{4,1}$. 
As we can see, although we can compute those missing values, we are not able to compare latency across rows. Specifically block  $m_7$ is slower than $m_4$ by $t_{4,1}$, but the negative value in $m_7[4]$ is the lowest number.
Consequently, it is much more helpful if there is a common reference such that we can easily compare latency among multiple row vectors based on absolute number. For that purpose, we introduce extra optimization variable, $C = c_1, ..., c_p$ for each row. We show an example of a compensated matrix with all optimization variables in Table(\ref{table:completer}). With all variable defined, the  optimization formulation is shown below
\begin{align}
    \argmin_{A, C} & \sum_{v = 1}^{s}
    \sum_{o \in N(v)} w(v,o) \| I( \Delta_{v,o} , \Theta_{v,o} ) \|_2 
                        +\|A\|_2 + \|C\|_2  \label{eq1} \\ 
	\textrm{s.t.} \quad 
	                & \Delta_{v,o} = L(row(a_v), o) + \ele_{col(a_v)} \label{eq2}\\
				    & \Theta_{v,o} = T_{o} + c_{o}\mathbf{1}^q - T_{row(a_v)} - c_{row(a_v)}\mathbf{1}^q \nonumber\\
                    & \quad \quad \quad  - (a_{v}  - c_{row(a_v)}) \ele_{col(v)} \label{eq3}
\end{align}
We use this formulation to continue to optimize for a single cell $a_v$. After we identify $k$ nearest vectors and their weight, we optimize for the $\ell_2$ norm on the difference between the 2 vectors. The vectors subtraction is shown in equation \ref{eq3}, where $T$ is a matrix containing all observed latency data, and all unobserved data are filled with 0; notation $T_{o}$ is the $o$-th row of the matrix $T$. As shown in the equation \ref{eq3}, $\Theta_{v,o}$ is the difference between the row vector $m_{row(v)}$ where $a_v$ comes from, and the neighboring vector $o$.
But since each of 2 vectors contains many other missing cells, and those cells do not contribute to the estimation of $a_v$. They are not considered in the metrics weight calculation. It is done by creating an indicator vector $\Delta_{v,o}$ in equation \ref{eq2} with the helper function $L(row(a_v), o)$ and an elementary vector $\ele_{col(v)}$ to specify which dimensions should be contributing to the loss.
We enable it with a mask selector function $I(a, b) = \{b_j: 0<j\le q, a_j=1\}$ function to combine equation \ref{eq2} and \ref{eq3} in equation \ref{eq1}. To optimize for all optimization variable, we optimize for sum of loss and add two regularization terms to keep optimization solution bounded.



%In the formulation, $w(v,o)$ decides how much weight the other nodes have influence to the vector where the missing variable $a_v$ belongs to ($v$ is index). Since $w$ is computed using a softmax function, summation over $N(v)$ is a normalized sum over every close neighbor denoted as $o$. 
% Clearly the dimension $\ele_{col(v)}$ of the missing variable should be added, and we also include dimensions which are commonly observed by the two vectors. 
%in both block $m_{o}$ and $m_{row(v)}$ are relevant to interpolate the variable $v$; it contains the column of the missing cell $\ele_{col(v)}$, and columns of common peers who forward block $m_{row(v)}$ and $m_{o}$ to the $\GF$ node; 


\begin{figure*}[hbt]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/optimal-single-star.png}
    \caption{Histogram of number of non-optimal epochs}
    \label{fig:optimal}
\end{figure*}

\subsubsection{Optimization Solver.}
%\noindent {\bf Optimization Solver.}  
The stated optimization problem is an unconstraint non-convex optimization problem, which is difficult to solve by analytically deriving gradients for each variables. Instead, we use a popular pytorch autograd package proven to be successful in many machine learning tasks. See Section~\ref{sec:related} for pytorch tensor graph. Solving the formulated problem requires 3 components: node definitions, a loss function that defines graph edges, and an optimizer. 
We define $A, C$ as the edge nodes that requires gradients and each observation row vector $T_i$ as constant nodes, so that the back-propagation only optimizes variables $A,C$. The loss function defined in the last section gives straightforward instructions to construct DAG tensor graph: the binary vector $\Delta_{v,o}$ decides edges for the graph, and the computation logic is contained in $\Theta_{v,o}$; in the end, we regularize $A, C$ by adding them to the final cost. Since  none of variables $A, C$ is constrained, we can use optimizer provided from pytorch like adam, which has been highly optimized, and gives us a better performance in short time. It is important to balance the complexity of edges among tensor nodes. %A high value of $K$ for KNN can create a densely connected tensor graph, taking very long time to compute. A low value of $K=1$ or unbalanced graph, (when a few nodes are highly connected, while others are not) gives poor result, too. To stop the optimization loop early when the variables are sufficiently good, we implement an early stopper that decide when the result loss function is small, and it stops early to reduce the running time. 

\subsubsection{Infeasible Missing Cells}
A missing cell can be infeasible ($\times$) if its value cannot be estimated, which can occur in 2 situations. First, when there is only 1 numeric observation in vector $m_r$. %(the observed value has to be 0 since we use relative time and the first is always 0).
It happens when the local node is the fastest relaying node. 
In the Table(\ref{table:constructor}), cell $m_2[3], m_5[4]$ are infeasible. Second, a missing cell, $m_r[u]$, is infeasible if there is no  vector $m_i$ satisfying 2 properties: the local node has numeric observation for block $i$ from peer $u$; the local node has numeric observations for block $i$ and $r$ from at least 2 peers (number of 2 is discussed in Distance metrics).

%Since each variables in $A$ is weighted by k rows, where $k$ is user chosen. It is important to randomize the order of the k rows if there are multiple ties for the score, which implies a much diverse graph. 







%\subsection{System Advantages}
%$\GF$ is highly adaptable to large networks that require  anonymous and permissionless properties while achieving high performance. $\GF$ needs little information about the network to generate intelligence for optimizing its connections. No block needs to provide extra information about its publisher or timestamp for $\GF$ to make inference. Furthermore, any publisher can freely change its identity, and still $\GF$ can optimize a best way to connect to them by using local time observation. $\GF$ keeps improving performance while allowing faster and more comprehensive neighbors selection, because $\GF$ synthesizes the entire view from the several past epoch to create virtual connections while remembering good performing peers; it has a risk-tolerant property that re-catches good performing peers that were dropped by accident or by exploration in recent history. As a consequence, when the number of publishers is less or equal to number of exploitation limitation, hence a global optimal connections is defined as direct connections to all publishers, a $\GF$ node only need to use a short memory to attain those connections with high probability by exploring every peer only once. Otherwise for a heuristic that explores peers iteratively and uses only current connections to make selection, it succeeds only when all publishers are selected together at once with extremely low probability.


%simultaneous comparison. $\GF$ inherently is fault tolerant because it can pickup a good peer in its memory several epoch ago, when the peer is dropped by random or by exploration. As a consequence, when the number of publishers is less or equal to number of exploitation quota, $\GF$ can locates those peers with high probability and establish direct connections with them, which achieve the optimal network broadcast latency.


