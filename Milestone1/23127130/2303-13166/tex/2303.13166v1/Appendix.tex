\renewcommand{\bldStatement}{. Best results per column are in bold and $\pm$ indicates the standard deviation across \fivetext{} runs.}
\renewcommand{\imgbldStatement}{ for \resnet{} on \imgnetheader using the pretrained dense model.. Best results per column are in bold and $\pm$ indicates the standard deviation across \fourtext{} runs.}
\clearpage
\appendix


\section{Appendix}
    
In this appendix, we provide implementation details and standard deviations for the experiments.
Additionally, the pseudocode for \glsentrylong{glmsaga} is shown.
Finally, we present the feature visualization technique and more ablations on~\gls{customLoss}.

\section{Detailed Results}

\begin{table*}
\resizebox{\linewidth}{!}{
\centering
\begin{tabular}{c|ccccc|ccccc|ccccc|ccccc}
\toprule
\fullheader
\xmark &\thead{ \textbf{86.6} \\$\pm$0.4}&\thead{ 81.8 \\$\pm$0.3}&\thead{ 85.3 \\$\pm$0.2}&\thead{ 79.5 \\$\pm$0.3}&\thead{ 83.4 \\$\pm$0.2}&\thead{ 90.0 \\$\pm$0.3}&\thead{ 88.4 \\$\pm$0.3}&\thead{ 89.4 \\$\pm$0.2}&\thead{ 87.3 \\$\pm$0.4}&\thead{ 88.1 \\$\pm$0.3}&\thead{ 84.2 \\$\pm$0.1}&\thead{ 79.5 \\$\pm$0.3}&\thead{ 83.3 \\$\pm$0.1}&\thead{ 77.3 \\$\pm$0.3}&\thead{ 80.7 \\$\pm$0.2}&\thead{ 93.2 \\$\pm$0.1}&\thead{ 90.9 \\$\pm$0.2}&\thead{ 92.6 \\$\pm$0.1}&\thead{ 89.3 \\$\pm$0.3}&\thead{ 91.1 \\$\pm$0.1}\\
\cmark &\thead{ \textbf{86.6} \\$\pm$0.2}&\thead{ \textbf{84.0} \\$\pm$0.2}&\thead{ \textbf{86.5} \\$\pm$0.1}&\thead{ \textbf{81.7} \\$\pm$0.2}&\thead{ \textbf{84.0} \\$\pm$0.3}&\thead{ \textbf{91.4} \\$\pm$0.2}&\thead{ \textbf{90.7} \\$\pm$0.3}&\thead{ \textbf{91.1} \\$\pm$0.2}&\thead{ \textbf{89.8} \\$\pm$0.4}&\thead{ \textbf{90.1} \\$\pm$0.1}&\thead{ \textbf{84.4} \\$\pm$0.2}&\thead{ \textbf{81.0} \\$\pm$0.2}&\thead{ \textbf{84.0} \\$\pm$0.0}&\thead{ \textbf{79.8} \\$\pm$0.2}&\thead{ \textbf{81.7} \\$\pm$0.1}&\thead{ \textbf{93.6} \\$\pm$0.2}&\thead{ \textbf{92.1} \\$\pm$0.3}&\thead{ \textbf{93.3} \\$\pm$0.1}&\thead{ \textbf{91.1} \\$\pm$0.1}&\thead{ \textbf{92.0} \\$\pm$0.2}\\
\bottomrule
\end{tabular}
}
\caption{Accuracy\inpercent{}\tablefinisher{\gls{customLoss}} for \resnet{}\bldStatement}
\label{table:Accuracy in stdldiv}
\end{table*}

\begin{table*}
\resizebox{\linewidth}{!}{
\centering
\begin{tabular}{c|ccccc|ccccc|ccccc|ccccc}
\toprule
\bbheader
\densenet &\thead{ 86.3 \\$\pm$0.0}&\thead{ 76.2 \\$\pm$0.5}&\thead{ 82.9 \\$\pm$0.5}&\thead{ 75.7 \\$\pm$0.7}&\thead{ 83.1 \\$\pm$0.1}&\thead{ \textbf{91.5} \\$\pm$0.1}&\thead{ 88.2 \\$\pm$0.5}&\thead{ 89.8 \\$\pm$0.4}&\thead{ 88.1 \\$\pm$0.2}&\thead{ 90.0 \\$\pm$0.4}&\thead{ 84.1 \\$\pm$0.1}&\thead{ 72.8 \\$\pm$0.4}&\thead{ 64.6 \\$\pm$22.8}&\thead{ 71.0 \\$\pm$0.5}&\thead{ 80.5 \\$\pm$0.3}&\thead{ 93.3 \\$\pm$0.1}&\thead{ 87.3 \\$\pm$0.4}&\thead{ 91.7 \\$\pm$0.1}&\thead{ 85.8 \\$\pm$0.3}&\thead{ 91.4 \\$\pm$0.2}\\
\incv &\thead{ 82.3 \\$\pm$0.1}&\thead{ 78.0 \\$\pm$0.3}&\thead{ 80.3 \\$\pm$0.4}&\thead{ 74.0 \\$\pm$0.7}&\thead{ 78.3 \\$\pm$0.4}&\thead{ 88.9 \\$\pm$0.1}&\thead{ 87.5 \\$\pm$0.2}&\thead{ 88.1 \\$\pm$0.2}&\thead{ 85.9 \\$\pm$0.3}&\thead{ 87.4 \\$\pm$0.2}&\thead{ 79.0 \\$\pm$0.1}&\thead{ 75.8 \\$\pm$0.2}&\thead{ 77.3 \\$\pm$0.3}&\thead{ 73.1 \\$\pm$0.2}&\thead{ 76.5 \\$\pm$0.1}&\thead{ 91.5 \\$\pm$0.1}&\thead{ 88.9 \\$\pm$0.2}&\thead{ 90.3 \\$\pm$0.2}&\thead{ 86.3 \\$\pm$0.2}&\thead{ 89.4 \\$\pm$0.2}\\
\resnet &\thead{ \textbf{86.6} \\$\pm$0.2}&\thead{ \textbf{84.0} \\$\pm$0.2}&\thead{ \textbf{86.5} \\$\pm$0.1}&\thead{ \textbf{81.7} \\$\pm$0.2}&\thead{ \textbf{84.0} \\$\pm$0.3}&\thead{ 91.4 \\$\pm$0.2}&\thead{ \textbf{90.7} \\$\pm$0.3}&\thead{ \textbf{91.1} \\$\pm$0.2}&\thead{ \textbf{89.8} \\$\pm$0.4}&\thead{ \textbf{90.1} \\$\pm$0.1}&\thead{ \textbf{84.4} \\$\pm$0.2}&\thead{ \textbf{81.0} \\$\pm$0.2}&\thead{ \textbf{84.0} \\$\pm$0.0}&\thead{ \textbf{79.8} \\$\pm$0.2}&\thead{ \textbf{81.7} \\$\pm$0.1}&\thead{ \textbf{93.6} \\$\pm$0.2}&\thead{ \textbf{92.1} \\$\pm$0.3}&\thead{ \textbf{93.3} \\$\pm$0.1}&\thead{ \textbf{91.1} \\$\pm$0.1}&\thead{ \textbf{92.0} \\$\pm$0.2}\\
\bottomrule
\end{tabular}
}
\caption{Accuracy\inpercent{}\tablefinisher{backbone}\bldStatement}
\label{table:Accuracy in stdbackbone}
\end{table*}
\begin{table*}
\centering
\begin{tabular}{ccccc}
\toprule
 \multicolumn{5}{c}{\imgnetheader}\\
 \multicolumn{3}{c|}{ $\gls{nReducedFeatures}=2048$ }&\multicolumn{2}{c}{ $\gls{nReducedFeatures}=50$ }\\
Dense  & Sparse  & Finet. & Sparse  & Finet. \\\midrule
80.9&\thead{ \imgsunl \\$\pm$0.0}&\thead{{\imgfunl} \\$\pm$0.0}&\thead{ \imgsl \\$\pm$0.1}&\thead{ \imgfl \\$\pm$0.1}\\
\bottomrule
\end{tabular}
\caption{Accuracy\inpercent{}\imgbldStatement}
\label{table:Accuracy imgnet}
\end{table*}

The full results of Section~\ref{sec:results} with the standard deviations are presented in
\suppt
Tables~\ref{table:Accuracy in stdldiv} to~\ref{table:loc5 incmpLoss}. 
The reported standard deviations are, except for \densenet{} on \birdsheader{}, as mentioned in Section~\ref{sec:results},  generally rather small compared to the differences in means, which supports our conclusions. 
We also show exemplary images with the 5 most important features for the dense and sparse conventional model in Figures~\ref{app:fig:vizExamples0} to~\ref{app:fig:vizExamples8}. The comparison with the finetuned~\gls{layerName} shows an improved localization and interpretability of the features.
The finetuned \gls{layerName} in Figure~\ref{app:fig:vizExamples3} seems to use features that each individually focus more on chest, lower belly, head, bill or crown, whereas for the dense and sparse models the different features focus on the same regions. This increased \loc{5} and with it interpretability was also measured in Section~\ref{sec:results}.
\subsection{Implementation Details}
\label{app:ImpDetails}
We use Pytorch~\citep{Pytorch} to implement our methods and on ImageNet pretrained models 
as backbone feature extractor. We utilized \glm{} and \textit{robustness}~\citep{robustness}.
The images are resized to $448\times448$ ($299\times299$ for \incv, $224\times224$ for \imgnetheader), normalized, randomly horizontally flipped and jitter is applied.
The model is finetuned using stochastic gradient descent on the specific dataset for $150$ ($100$ for \birdsheader) epochs with a batch size of $16$ ($64$ for \gls{imgnetheader}), starting with $5\cdot10^{-3}$ as learning rate for the pretrained layer and $0.01$ for the final linear layer. 
Both get multiplied by $0.4$ every $30$ epochs. Additionally, we used momentum of $0.9$, $\ell_2$-regularization of $5\cdot10^{-4}$ and apply a dropout rate of $0.2$ on the features to reduce dependencies. \gls{cLW} was set to $0.196$ for \resnet{}, $0.098$ for \densenet{} and $0.049$ for \incv.
For the feature selection, we set $\gls{elaW} = 0.8$ and reduce the regularization strength \gls{elaWeight} by $90\,\%$ as we found it sped up the process without decreasing performance.\\
We use \glm{} to compute the regularization path with $\gls{elaW}=0.99$ and all other parameters set to default with a lookbehind of $T=5$.
From this path, the solution with maximum $\gls{nperClass} \leq 10$ is selected. 
Then the non-zero entries with the lowest absolute value get zeroed out until we are left with $\gls{nperClass} = 5$,
as we empirically found that they do not improve test accuracy after finetuning.\\
This selected solution replaces the final layer of our model. 
Then we train for $40$ epochs, starting with the final learning rate of the initial training multiplied by $100$ ($\frac{1}{100}$ of that for \imgnetheader), and decrease it by $60\,\%$ every $10$ epochs. 
Dropout on the features was set to $0.1$ and momentum was increased to $0.95$. Note that, while the increased momentum has been important for the stability of the final training, the hyperparameters were not thoroughly optimized for the sparse case. 
\subsubsection{Competitors}
\label{app:cbmjoint}
For creating the accuracy for \resnet{} and \textit{\glsentrylong{cbm} - joint} in Table~\ref{tab:competitors} we resized the images to $448\times448$ and used a batch size of $16$. The remaining used hyperparameters were almost identical to the \gls{cbm} experiments with \incv, but we only trained for up to $400$ epochs, as $650$ led to decreased accuracy ($-0.8$ percent points). 
Additionally, the learning rate was not decayed, mirroring the published code.
The reported accuracy stems from three runs with a standard deviation of $0.7$.\\
For MCL, we used the reported hyperparameters of $\mu=0.005$ and $\lambda=10$. For finetuning, we assigned every feature to every class that was using it. We optimized the hyperparameters for FRL based on accuracy, leading to $K = 10$ and $\lambda=0.01$.\\ 

\section{\Gls{glmsaga}}
\label{app:glmsaga}
\begin{algorithm}[!t]
	\caption{Pseudocode from \glsentrylong{glmsaga}
	}
	\label{alg:solver}
	\begin{algorithmic}[1]
		\STATE Initialize table of scalars $a_i' = 0$ for $i \in [n]$
		\STATE Initialize average gradient of table $g_{avg}=0$ and 
		$g_{0avg}=0$
		\FOR{minibatch $B\subset [n]$}
		\FOR{$i \in B$}
		\STATE $a_i = x_i^T\beta + \beta_0 - y_i$
		\STATE $g_i = a_i \cdot x_i$ \textit{// calculate new gradient information}
		\STATE $g_i' = a_i' \cdot x_i$ \textit{// calculate stored gradient 
			information}
		\ENDFOR
		\STATE $g = \frac{1}{|B|}\sum_{i \in B} g_i$
		\STATE $g' = \frac{1}{|B|}\sum_{i \in B} g_i'$
		
		\STATE $g_0 = \frac{1}{|B|}\sum_{i \in B} a_i$
		\STATE $g_0' = \frac{1}{|B|}\sum_{i \in B} a_i'$
		
		\STATE $\beta = \beta - \gamma(g - g' + g_{avg})$
		\STATE $\beta_0 = \beta_0 - \gamma(g_0 - g_0' + g_{0avg})$
		\STATE $\beta = \textrm{Prox}_{\gamma\lambda\alpha, 
			\gamma\lambda(1-\alpha)}(\beta)$
		\FOR{$i\in B$}
		\STATE $a_i' = a_i$ \textit{// update table}
		\STATE $g_{avg} = g_{avg} + \frac{|B|}{n}(g - g')$ \textit{// update 
			average}
		\STATE $g_{0avg} = g_{0avg} + \frac{|B|}{n}(g_0 - g_0')$ 
		\ENDFOR
		\ENDFOR
	\end{algorithmic}
\end{algorithm}
This section includes the Pseudocode for \glsentrylong{glmsaga} in algorithm~\ref{alg:solver}. The proximal operator $\textrm{Prox}_{\lambda_1, \lambda_2}(\beta)$ is defined as:
\begin{equation}
\textrm{Prox}_{\lambda_1, \lambda_2}(\beta) = \begin{cases}
\frac{\beta - \lambda_1}{1+\lambda_2} &\text{if } \beta > \lambda_1 \\
\frac{\beta + \lambda_1}{1+\lambda_2} &\text{if } \beta < \lambda_1 \\
0 &\text{otherwise}
\end{cases}
\end{equation}
\section{Visualization of Features}
\label{app:featureViz}
For feature visualization, we follow a masking approach. We systematically blur, following~\citep{fong2017interpretable}, one patch of size $p\times p$ of the image and measure the difference in feature activation between the augmented image and not augmented image. The actual localization map $\glsentrylong{LocalizationMaps}$ for that square size is computed by 
\begin{align}
    \gls{LocalizationMaps}_{pxy} =  \text{ReLU}(\glsentrylong{featureVector}(I) - \glsentrylong{featureVector}(I_{pxy}))
\end{align}
where $I_{pxy}$ indicates the image where a $p$-sized patch starting at position $(x *p,y*p)$ is blurred and the ReLU suppresses parts that increased the feature activation, since blur should not be injecting a feature. The final localization map is the combination of different square sizes $p\in\{28,56,64,112,224\}$ to accommodate for differently sized features:
\begin{equation}
    \gls{LocalizationMaps}= \sum_{p}\frac{\gls{LocalizationMaps}_p}{\max(\gls{LocalizationMaps}_p)}
\end{equation}
Notably, $\gls{LocalizationMaps}_p$ has to be resized according to the smallest $p$ and we only show $\gls{LocalizationMaps}^i$ for one feature $i$.
\section{Feature Alignment}
\label{app:sec:alignment}
In this section, we use the alignment of the shown feature in Figure~\ref{fig:4Strahlig}  with four-engine aircraft to exemplary show how one can align a feature manually. We first visualize the distribution of the feature over the training data in Figure~\ref{fig:distair}. This indicates a more binary attribute and by investigating the images and saliency maps, we observed an alignment with four-engine aircraft. 
We test the hypothesis by filtering for the classes "A340-500" and "BAE 146-300", for which the feature corresponds to the four engines. 
Figure~\ref{fig:lowestair} shows that the lowest activating examples of this group do not clearly show the four-engines which supports our hypothesis.
Note that one feature can correspond to multiple concepts, as another hypothesis is an alignment with the propeller.
Whether all correlating concepts have to be understood or how exact the analysis has to be depends on the application.
\begin{figure*}[t]
\begin{center}
  \includegraphics[width=\linewidth]{plots/1388_6.png}
\end{center}
  \caption{Top: Distribution of feature activation over the training data. Bottom: Different examples of this distribution with their scaled feature localization. The activation and class name is given above the image. }
   
\vspace{-0.55cm}
\label{fig:distair}
\end{figure*}
\begin{figure*}
     \begin{subfigure}[t]{.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{plots/2.jpg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{.3\textwidth}
         \centering
       \includegraphics[width=\textwidth]{plots/4.jpg}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t]{.3\textwidth}
         \centering
       \includegraphics[width=\textwidth]{plots/6.jpg}
     \end{subfigure}
        \caption{The three least activating training examples of classes "A340-500" and "BAE-16-300" for the given feature. }
        \label{fig:lowestair}
\end{figure*}
\section{Ablations on Feature Diversity Loss}
\label{app:AblCustom}
In this section, we present an additional analysis of the factors in~\gls{customLoss} and the impact of \gls{cLW}. 
\begin{table*}
\resizebox{\linewidth}{!}{
\centering
\begin{tabular}{c|ccccc|ccccc|ccccc|ccccc}
\toprule
\fullheader
\xmark &\thead{ 50.2 \\$\pm$0.2}&\thead{ 46.0 \\$\pm$0.2}&\thead{ 43.4 \\$\pm$0.3}&\thead{ 48.0 \\$\pm$0.5}&\thead{ 46.5 \\$\pm$0.3}&\thead{ 46.5 \\$\pm$0.5}&\thead{ 43.8 \\$\pm$0.7}&\thead{ 40.9 \\$\pm$0.4}&\thead{ 45.9 \\$\pm$0.6}&\thead{ 44.0 \\$\pm$0.6}&\thead{ 41.5 \\$\pm$0.2}&\thead{ 38.4 \\$\pm$0.1}&\thead{ 34.9 \\$\pm$0.1}&\thead{ 40.8 \\$\pm$0.6}&\thead{ 35.1 \\$\pm$0.7}&\thead{ 45.0 \\$\pm$0.2}&\thead{ 41.7 \\$\pm$0.3}&\thead{ 39.1 \\$\pm$0.1}&\thead{ 43.6 \\$\pm$0.6}&\thead{ 43.7 \\$\pm$0.4}\\
\cmark &\thead{ \textbf{98.9} \\$\pm$0.1}&\thead{ \textbf{69.9} \\$\pm$0.5}&\thead{ \textbf{71.9} \\$\pm$0.4}&\thead{ \textbf{65.2} \\$\pm$1.4}&\thead{ \textbf{72.6} \\$\pm$0.3}&\thead{ \textbf{98.8} \\$\pm$0.2}&\thead{ \textbf{85.7} \\$\pm$1.5}&\thead{ \textbf{86.6} \\$\pm$1.4}&\thead{ \textbf{69.3} \\$\pm$0.8}&\thead{ \textbf{73.9} \\$\pm$1.3}&\thead{ \textbf{98.7} \\$\pm$0.1}&\thead{ \textbf{69.5} \\$\pm$1.1}&\thead{ \textbf{81.0} \\$\pm$1.2}&\thead{ \textbf{70.7} \\$\pm$1.9}&\thead{ \textbf{85.3} \\$\pm$1.0}&\thead{ \textbf{99.2} \\$\pm$0.1}&\thead{ \textbf{72.4} \\$\pm$2.0}&\thead{ \textbf{74.6} \\$\pm$1.5}&\thead{ \textbf{63.7} \\$\pm$1.3}&\thead{ \textbf{74.8} \\$\pm$0.9}\\
\bottomrule
\end{tabular}
}
\caption{\loc{5}\inpercent{}\tablefinisher{\gls{customLoss}} for \resnet{}\bldStatement}
\label{table:loc5 in stdldiv}
\end{table*}

\begin{table*}
\resizebox{\linewidth}{!}{
\centering
\begin{tabular}{c|ccccc|ccccc|ccccc|ccccc}
\toprule
\bbheader
\densenet &\thead{ 98.5 \\$\pm$0.1}&\thead{ 69.1 \\$\pm$1.7}&\thead{ 64.2 \\$\pm$1.2}&\thead{ \textbf{76.2} \\$\pm$1.0}&\thead{ 71.2 \\$\pm$0.8}&\thead{ \textbf{99.0} \\$\pm$0.1}&\thead{ 40.4 \\$\pm$0.7}&\thead{ 39.9 \\$\pm$0.9}&\thead{ 64.3 \\$\pm$2.0}&\thead{ 62.6 \\$\pm$2.1}&\thead{ 98.6 \\$\pm$0.1}&\thead{ 45.7 \\$\pm$1.1}&\thead{ 42.3 \\$\pm$3.5}&\thead{ 63.1 \\$\pm$3.1}&\thead{ 66.2 \\$\pm$2.2}&\thead{ 98.7 \\$\pm$0.1}&\thead{ 47.4 \\$\pm$1.3}&\thead{ 46.3 \\$\pm$1.0}&\thead{ \textbf{71.8} \\$\pm$1.5}&\thead{ 68.0 \\$\pm$1.2}\\
\incv &\thead{ 86.3 \\$\pm$0.5}&\thead{ \textbf{74.9} \\$\pm$0.9}&\thead{ 65.1 \\$\pm$0.6}&\thead{ 53.2 \\$\pm$0.8}&\thead{ 52.7 \\$\pm$0.4}&\thead{ 95.1 \\$\pm$0.2}&\thead{ \textbf{87.2} \\$\pm$1.6}&\thead{ 72.2 \\$\pm$1.4}&\thead{ 53.7 \\$\pm$1.1}&\thead{ 56.1 \\$\pm$0.9}&\thead{ 81.8 \\$\pm$1.0}&\thead{ 47.9 \\$\pm$1.0}&\thead{ 47.0 \\$\pm$0.8}&\thead{ 42.7 \\$\pm$1.6}&\thead{ 45.6 \\$\pm$1.0}&\thead{ 92.0 \\$\pm$0.3}&\thead{ \textbf{76.3} \\$\pm$0.6}&\thead{ 63.7 \\$\pm$0.6}&\thead{ 52.0 \\$\pm$1.2}&\thead{ 50.7 \\$\pm$0.8}\\
\resnet &\thead{ \textbf{98.9} \\$\pm$0.1}&\thead{ 69.9 \\$\pm$0.5}&\thead{ \textbf{71.9} \\$\pm$0.4}&\thead{ 65.2 \\$\pm$1.4}&\thead{ \textbf{72.6} \\$\pm$0.3}&\thead{ 98.8 \\$\pm$0.2}&\thead{ 85.7 \\$\pm$1.5}&\thead{ \textbf{86.6} \\$\pm$1.4}&\thead{ \textbf{69.3} \\$\pm$0.8}&\thead{ \textbf{73.9} \\$\pm$1.3}&\thead{ \textbf{98.7} \\$\pm$0.1}&\thead{ \textbf{69.5} \\$\pm$1.1}&\thead{ \textbf{81.0} \\$\pm$1.2}&\thead{ \textbf{70.7} \\$\pm$1.9}&\thead{ \textbf{85.3} \\$\pm$1.0}&\thead{ \textbf{99.2} \\$\pm$0.1}&\thead{ 72.4 \\$\pm$2.0}&\thead{ \textbf{74.6} \\$\pm$1.5}&\thead{ 63.7 \\$\pm$1.3}&\thead{ \textbf{74.8} \\$\pm$0.9}\\
\bottomrule
\end{tabular}
}
\caption{\loc{5}\inpercent{}\tablefinisher{backbone}\bldStatement}
\label{table:loc5 in stdbackbone}
\end{table*}


\begin{table*}
\resizebox{\linewidth}{!}{
\centering
\begin{tabular}{c|ccccc|ccccc|ccccc}
\toprule
\fullablheader
\xmark &\thead{ \textbf{86.6} \\$\pm$0.4}&\thead{ 81.8 \\$\pm$0.3}&\thead{ 85.3 \\$\pm$0.2}&\thead{ 79.5 \\$\pm$0.3}&\thead{ 83.4 \\$\pm$0.2}&\thead{ 90.0 \\$\pm$0.3}&\thead{ 88.4 \\$\pm$0.3}&\thead{ 89.4 \\$\pm$0.2}&\thead{ 87.3 \\$\pm$0.4}&\thead{ 88.1 \\$\pm$0.3}&\thead{ 93.2 \\$\pm$0.1}&\thead{ 90.9 \\$\pm$0.2}&\thead{ 92.6 \\$\pm$0.1}&\thead{ 89.3 \\$\pm$0.3}&\thead{ 91.1 \\$\pm$0.1}\\
\cmark &\thead{ \textbf{86.6} \\$\pm$0.2}&\thead{ \textbf{84.0} \\$\pm$0.2}&\thead{ \textbf{86.5} \\$\pm$0.1}&\thead{ \textbf{81.7} \\$\pm$0.2}&\thead{ \textbf{84.0} \\$\pm$0.3}&\thead{ \textbf{91.4} \\$\pm$0.2}&\thead{ \textbf{90.7} \\$\pm$0.3}&\thead{ \textbf{91.1} \\$\pm$0.2}&\thead{ \textbf{89.8} \\$\pm$0.4}&\thead{ \textbf{90.1} \\$\pm$0.1}&\thead{ \textbf{93.6} \\$\pm$0.2}&\thead{ \textbf{92.1} \\$\pm$0.3}&\thead{ \textbf{93.3} \\$\pm$0.1}&\thead{ \textbf{91.1} \\$\pm$0.1}&\thead{ \textbf{92.0} \\$\pm$0.2}\\
\bottomrule MCL~\citep{DevilChannels} & \thead{86.1\\$\pm$0.2} & \thead{81.9\\$\pm$0.3} & \thead{85.1\\$\pm$0.2} & \thead{79.4\\$\pm$0.2} & \thead{82.8\\$\pm$0.1} & \thead{90.1\\$\pm$0.1} & \thead{88.4\\$\pm$0.2} & \thead{89.0\\$\pm$0.2} & \thead{87.2\\$\pm$0.5} & \thead{88.1\\$\pm$0.1} & \thead{93.1\\$\pm$0.1} & \thead{91.0\\$\pm$0.2} & \thead{92.5\\$\pm$0.2} & \thead{89.0\\$\pm$0.5} & \thead{90.7\\$\pm$0.3}\\FRL~\citep{ClassUniqueCorr} & \thead{86.4\\$\pm$0.2} & \thead{81.5\\$\pm$0.2} & \thead{85.3\\$\pm$0.2} & \thead{78.9\\$\pm$0.5} & \thead{82.6\\$\pm$0.5} & \thead{90.0\\$\pm$0.1} & \thead{88.5\\$\pm$0.2} & \thead{89.4\\$\pm$0.4} & \thead{87.5\\$\pm$0.2} & \thead{88.2\\$\pm$0.2} & \thead{93.3\\$\pm$0.1} & \thead{90.8\\$\pm$0.3} & \thead{92.6\\$\pm$0.2} & \thead{89.4\\$\pm$0.2} & \thead{90.9\\$\pm$0.2}\\
\end{tabular}
}
\caption{Accuracy\inpercent{} for \resnet{} \cmpLoss}
\label{table:Accuracy incmpLoss}
\end{table*}

\begin{table*}
\resizebox{\linewidth}{!}{
\centering
\begin{tabular}{c|ccccc|ccccc|ccccc}
\toprule
\fullablheader
\xmark &\thead{ 50.2 \\$\pm$0.2}&\thead{ 46.0 \\$\pm$0.2}&\thead{ 43.4 \\$\pm$0.3}&\thead{ 48.0 \\$\pm$0.5}&\thead{ 46.5 \\$\pm$0.3}&\thead{ 46.5 \\$\pm$0.5}&\thead{ 43.8 \\$\pm$0.7}&\thead{ 40.9 \\$\pm$0.4}&\thead{ 45.9 \\$\pm$0.6}&\thead{ 44.0 \\$\pm$0.6}&\thead{ 45.0 \\$\pm$0.2}&\thead{ 41.7 \\$\pm$0.3}&\thead{ 39.1 \\$\pm$0.1}&\thead{ 43.6 \\$\pm$0.6}&\thead{ 43.7 \\$\pm$0.4}\\
\cmark &\thead{ \textbf{98.9} \\$\pm$0.1}&\thead{ \textbf{69.9} \\$\pm$0.5}&\thead{ \textbf{71.9} \\$\pm$0.4}&\thead{ \textbf{65.2} \\$\pm$1.4}&\thead{ \textbf{72.6} \\$\pm$0.3}&\thead{ \textbf{98.8} \\$\pm$0.2}&\thead{ \textbf{85.7} \\$\pm$1.5}&\thead{ \textbf{86.6} \\$\pm$1.4}&\thead{ \textbf{69.3} \\$\pm$0.8}&\thead{ \textbf{73.9} \\$\pm$1.3}&\thead{ \textbf{99.2} \\$\pm$0.1}&\thead{ \textbf{72.4} \\$\pm$2.0}&\thead{ \textbf{74.6} \\$\pm$1.5}&\thead{ \textbf{63.7} \\$\pm$1.3}&\thead{ \textbf{74.8} \\$\pm$0.9}\\
\bottomrule MCL~\citep{DevilChannels} & \thead{52.5\\$\pm$0.3} & \thead{51.4\\$\pm$0.7} & \thead{48.9\\$\pm$0.8} & \thead{56.7\\$\pm$1.0} & \thead{52.3\\$\pm$1.0} & \thead{50.1\\$\pm$0.7} & \thead{50.6\\$\pm$0.8} & \thead{48.3\\$\pm$1.2} & \thead{51.7\\$\pm$1.8} & \thead{50.1\\$\pm$1.9} & \thead{49.0\\$\pm$0.6} & \thead{49.0\\$\pm$0.9} & \thead{46.2\\$\pm$0.8} & \thead{51.8\\$\pm$1.2} & \thead{49.5\\$\pm$1.3}\\FRL~\citep{ClassUniqueCorr} & \thead{51.1\\$\pm$0.3} & \thead{47.1\\$\pm$0.5} & \thead{44.1\\$\pm$0.4} & \thead{49.0\\$\pm$0.7} & \thead{46.3\\$\pm$0.6} & \thead{48.2\\$\pm$0.3} & \thead{44.6\\$\pm$0.3} & \thead{41.2\\$\pm$0.4} & \thead{44.9\\$\pm$0.8} & \thead{43.1\\$\pm$0.5} & \thead{46.2\\$\pm$0.1} & \thead{43.0\\$\pm$0.4} & \thead{40.0\\$\pm$0.4} & \thead{43.0\\$\pm$1.1} & \thead{41.7\\$\pm$1.1}\\
\end{tabular}
}
\caption{\loc{5}\inpercent{} for \resnet{} \cmpLoss}
\label{table:loc5 incmpLoss}
\end{table*}





\subsection{Factor Importance}
\begin{table*}
\resizebox{\linewidth}{!}{
\centering
\begin{tabular}{c|ccccc|ccccc|ccccc}
\toprule
\lossheader
\gls{customLoss} &\thead{ \textbf{86.6} \\$\pm$0.2}&\thead{ \textbf{84.0} \\$\pm$0.2}&\thead{ \textbf{86.5} \\$\pm$0.1}&\thead{ \textbf{81.7} \\$\pm$0.2}&\thead{ \textbf{84.0} \\$\pm$0.3}&\thead{ \textbf{91.4} \\$\pm$0.2}&\thead{ \textbf{90.7} \\$\pm$0.3}&\thead{ \textbf{91.1} \\$\pm$0.2}&\thead{ \textbf{89.8} \\$\pm$0.4}&\thead{ \textbf{90.1} \\$\pm$0.1}&\thead{ \textbf{93.6} \\$\pm$0.2}&\thead{ \textbf{92.1} \\$\pm$0.3}&\thead{ \textbf{93.3} \\$\pm$0.1}&\thead{ \textbf{91.1} \\$\pm$0.1}&\thead{ \textbf{92.0} \\$\pm$0.2}\\
\SoftmaxName &\thead{ 86.3 \\$\pm$0.2}&\thead{ 82.7 \\$\pm$0.4}&\thead{ 85.6 \\$\pm$0.3}&\thead{ 80.2 \\$\pm$0.5}&\thead{ 83.4 \\$\pm$0.3}&\thead{ 90.8 \\$\pm$0.4}&\thead{ 89.7 \\$\pm$0.3}&\thead{ 90.1 \\$\pm$0.2}&\thead{ 88.7 \\$\pm$0.4}&\thead{ 89.1 \\$\pm$0.3}&\thead{ 93.2 \\$\pm$0.1}&\thead{ 91.0 \\$\pm$0.2}&\thead{ 92.8 \\$\pm$0.2}&\thead{ 89.9 \\$\pm$0.1}&\thead{ 91.5 \\$\pm$0.2}\\
\classWeightsName &\thead{ 86.3 \\$\pm$0.2}&\thead{ 82.0 \\$\pm$0.2}&\thead{ 85.4 \\$\pm$0.3}&\thead{ 79.0 \\$\pm$0.2}&\thead{ 83.1 \\$\pm$0.2}&\thead{ 90.2 \\$\pm$0.3}&\thead{ 88.6 \\$\pm$0.4}&\thead{ 89.6 \\$\pm$0.3}&\thead{ 87.3 \\$\pm$0.2}&\thead{ 88.5 \\$\pm$0.4}&\thead{ 93.2 \\$\pm$0.2}&\thead{ 90.8 \\$\pm$0.2}&\thead{ 92.6 \\$\pm$0.2}&\thead{ 89.1 \\$\pm$0.3}&\thead{ 91.0 \\$\pm$0.3}\\
\bottomrule
\end{tabular}
}
\caption{Impact of factors in~\gls{customLoss} on accuracy\inpercent{} for \resnet{}\bldStatement}
\label{table:Impact of factors in customLoss on Impact of factors in customLoss on Accuracy in std}
\end{table*}
\begin{table*}
\resizebox{\linewidth}{!}{
\centering
\begin{tabular}{c|ccccc|ccccc|ccccc}
\toprule
\lossheader
\gls{customLoss} &\thead{ 98.9 \\$\pm$0.1}&\thead{ \textbf{69.9} \\$\pm$0.5}&\thead{ \textbf{71.9} \\$\pm$0.4}&\thead{ 65.2 \\$\pm$1.4}&\thead{ 72.6 \\$\pm$0.3}&\thead{ 98.8 \\$\pm$0.2}&\thead{ \textbf{85.7} \\$\pm$1.5}&\thead{ \textbf{86.6} \\$\pm$1.4}&\thead{ \textbf{69.3} \\$\pm$0.8}&\thead{ \textbf{73.9} \\$\pm$1.3}&\thead{ 99.2 \\$\pm$0.1}&\thead{ \textbf{72.4} \\$\pm$2.0}&\thead{ \textbf{74.6} \\$\pm$1.5}&\thead{ 63.7 \\$\pm$1.3}&\thead{ \textbf{74.8} \\$\pm$0.9}\\
\SoftmaxName &\thead{ \textbf{99.3} \\$\pm$0.0}&\thead{ 67.6 \\$\pm$0.9}&\thead{ 66.9 \\$\pm$0.5}&\thead{ \textbf{70.5} \\$\pm$2.0}&\thead{ \textbf{73.3} \\$\pm$1.8}&\thead{ \textbf{99.7} \\$\pm$0.0}&\thead{ 72.4 \\$\pm$1.2}&\thead{ 74.6 \\$\pm$1.6}&\thead{ 68.5 \\$\pm$4.3}&\thead{ 73.6 \\$\pm$2.1}&\thead{ \textbf{99.7} \\$\pm$0.0}&\thead{ 64.9 \\$\pm$0.8}&\thead{ 65.3 \\$\pm$0.6}&\thead{ \textbf{68.3} \\$\pm$3.6}&\thead{ 73.5 \\$\pm$2.0}\\
\classWeightsName &\thead{ 50.6 \\$\pm$0.3}&\thead{ 46.2 \\$\pm$0.3}&\thead{ 43.5 \\$\pm$0.2}&\thead{ 48.1 \\$\pm$1.1}&\thead{ 46.8 \\$\pm$0.6}&\thead{ 47.6 \\$\pm$0.6}&\thead{ 44.3 \\$\pm$0.5}&\thead{ 41.2 \\$\pm$0.3}&\thead{ 45.0 \\$\pm$1.7}&\thead{ 43.7 \\$\pm$1.6}&\thead{ 45.5 \\$\pm$0.2}&\thead{ 42.2 \\$\pm$0.3}&\thead{ 39.5 \\$\pm$0.2}&\thead{ 42.7 \\$\pm$1.1}&\thead{ 42.5 \\$\pm$1.1}\\
\bottomrule
\end{tabular}
}
\caption{Impact of factors in~\gls{customLoss} on \loc{5}\inpercent{} for \resnet{}\bldStatement}
\label{table:Impact of factors in customLoss on Impact of factors in customLoss on loc5 in std}
\end{table*}



We analyzed the impact of the two factors in Equation~\ref{eq:ScaleDiv} with for accuracy optimized \gls{cLW}, shown in Tables~\ref{table:Impact of factors in customLoss on Impact of factors in customLoss on Accuracy in std} and~\ref{table:Impact of factors in customLoss on Impact of factors in customLoss on loc5 in std}. 
The label \classWeightsName{} indicates not using the weights of the predicted class and \SoftmaxName{} refers to not maintaining their relative mean. 
We used $\gls{cLW}= 0.001\cdot\frac{196}{2048}\approx1\mathrm{e}{-5}$ for
\classWeightsName{}, since we use the size of the feature maps with $196=\gls{featuresMapwidth}\cdot\gls{featuresMapheigth}$ and the number of features of the
baseline model$~\gls{nFeatures}=2048$ as scaling factors in order to be less dependent of model architecture and image size, and $\gls{cLW}= 0.1$ for \SoftmaxName.
Only the combination of both factors leads to an improved accuracy, validating our idea that it is important to only enforce diversity of features that are found in the input and used in conjunction. 





\subsection{Loss Weighting}
This section is concerned with the impact of the weighting factor~\gls{cLW} for the feature diversity loss~\gls{customLoss}. For our proposed method, we use $\gls{cLW}=0.196$.
Tables~\ref{table:Accuracy in stdbeta} and~\ref{table:loc5 in stdbeta} show that
\gls{customLoss} improves the \loc{5} and accuracy 
across all datasets
in the sparse case with increasing~\gls{cLW} up to a maximum roughly around $\gls{cLW}=1$. Setting the value higher leads to~\gls{customLoss} dominating the training. To ensure that the network is still mainly optimized for classification, we choose $\gls{cLW}=0.196$, even though in some cases we could still observe a slight gain in accuracy with a slight increase of~\gls{cLW}.
The positive relation between~\loc{5} and accuracy, visualized in
\suppt
Figure~\ref{fig:DivAcc}, supports our approach of enforcing varied features for the extremely sparse case.
\begin{table*}
\resizebox{\linewidth}{!}{
\centering
\begin{tabular}{c|ccccc|ccccc|ccccc}
\toprule
\cLWheader
0 &\thead{ \textbf{86.6} \\$\pm$0.4}&\thead{ 81.8 \\$\pm$0.3}&\thead{ 85.3 \\$\pm$0.2}&\thead{ 79.5 \\$\pm$0.3}&\thead{ 83.4 \\$\pm$0.2}&\thead{ 90.0 \\$\pm$0.3}&\thead{ 88.4 \\$\pm$0.3}&\thead{ 89.4 \\$\pm$0.2}&\thead{ 87.3 \\$\pm$0.4}&\thead{ 88.1 \\$\pm$0.3}&\thead{ 93.2 \\$\pm$0.1}&\thead{ 90.9 \\$\pm$0.2}&\thead{ 92.6 \\$\pm$0.1}&\thead{ 89.3 \\$\pm$0.3}&\thead{ 91.1 \\$\pm$0.1}\\
0.00196 &\thead{ 86.4 \\$\pm$0.2}&\thead{ 82.0 \\$\pm$0.3}&\thead{ 85.5 \\$\pm$0.3}&\thead{ 79.3 \\$\pm$0.3}&\thead{ 83.3 \\$\pm$0.2}&\thead{ 90.2 \\$\pm$0.2}&\thead{ 88.7 \\$\pm$0.3}&\thead{ 89.5 \\$\pm$0.3}&\thead{ 87.6 \\$\pm$0.2}&\thead{ 88.5 \\$\pm$0.2}&\thead{ 93.1 \\$\pm$0.1}&\thead{ 90.9 \\$\pm$0.3}&\thead{ 92.6 \\$\pm$0.1}&\thead{ 89.0 \\$\pm$0.2}&\thead{ 90.8 \\$\pm$0.2}\\
0.0196 &\thead{ 86.4 \\$\pm$0.2}&\thead{ 82.2 \\$\pm$0.3}&\thead{ 85.3 \\$\pm$0.3}&\thead{ 79.4 \\$\pm$0.6}&\thead{ 83.2 \\$\pm$0.3}&\thead{ 90.6 \\$\pm$0.4}&\thead{ 89.0 \\$\pm$0.3}&\thead{ 89.7 \\$\pm$0.3}&\thead{ 87.4 \\$\pm$0.4}&\thead{ 88.5 \\$\pm$0.4}&\thead{ 93.1 \\$\pm$0.1}&\thead{ 91.0 \\$\pm$0.2}&\thead{ 92.6 \\$\pm$0.2}&\thead{ 89.0 \\$\pm$0.2}&\thead{ 90.9 \\$\pm$0.2}\\
\underline{0.196} &\thead{ \textbf{86.6} \\$\pm$0.2}&\thead{ \textbf{84.0} \\$\pm$0.2}&\thead{ \textbf{86.5} \\$\pm$0.1}&\thead{ \textbf{81.7} \\$\pm$0.2}&\thead{ \textbf{84.0} \\$\pm$0.3}&\thead{ \textbf{91.4} \\$\pm$0.2}&\thead{ \textbf{90.7} \\$\pm$0.3}&\thead{ \textbf{91.1} \\$\pm$0.2}&\thead{ \textbf{89.8} \\$\pm$0.4}&\thead{ \textbf{90.1} \\$\pm$0.1}&\thead{ \textbf{93.6} \\$\pm$0.2}&\thead{ \textbf{92.1} \\$\pm$0.3}&\thead{ \textbf{93.3} \\$\pm$0.1}&\thead{ \textbf{91.1} \\$\pm$0.1}&\thead{ \textbf{92.0} \\$\pm$0.2}\\
\bottomrule
\end{tabular}
}
\caption{Accuracy\inpercent{}\tablefinisher{\gls{cLW}} for \resnet{}\bldStatement\undlinstmt}
\label{table:Accuracy in stdbeta}
\end{table*}
\begin{table*}
\resizebox{\linewidth}{!}{
\centering
\begin{tabular}{c|ccccc|ccccc|ccccc}
\toprule
\cLWheader
0 &\thead{ 50.2 \\$\pm$0.2}&\thead{ 46.0 \\$\pm$0.2}&\thead{ 43.4 \\$\pm$0.3}&\thead{ 48.0 \\$\pm$0.5}&\thead{ 46.5 \\$\pm$0.3}&\thead{ 46.5 \\$\pm$0.5}&\thead{ 43.8 \\$\pm$0.7}&\thead{ 40.9 \\$\pm$0.4}&\thead{ 45.9 \\$\pm$0.6}&\thead{ 44.0 \\$\pm$0.6}&\thead{ 45.0 \\$\pm$0.2}&\thead{ 41.7 \\$\pm$0.3}&\thead{ 39.1 \\$\pm$0.1}&\thead{ 43.6 \\$\pm$0.6}&\thead{ 43.7 \\$\pm$0.4}\\
0.00196 &\thead{ 51.0 \\$\pm$0.1}&\thead{ 46.1 \\$\pm$0.2}&\thead{ 43.7 \\$\pm$0.2}&\thead{ 49.2 \\$\pm$0.6}&\thead{ 47.6 \\$\pm$0.4}&\thead{ 48.1 \\$\pm$0.4}&\thead{ 44.6 \\$\pm$0.4}&\thead{ 41.5 \\$\pm$0.4}&\thead{ 45.6 \\$\pm$0.5}&\thead{ 43.8 \\$\pm$0.3}&\thead{ 46.3 \\$\pm$0.0}&\thead{ 42.6 \\$\pm$0.3}&\thead{ 39.7 \\$\pm$0.2}&\thead{ 43.0 \\$\pm$0.7}&\thead{ 43.3 \\$\pm$0.5}\\
0.0196 &\thead{ 64.0 \\$\pm$0.6}&\thead{ 50.8 \\$\pm$0.6}&\thead{ 47.8 \\$\pm$0.4}&\thead{ 50.2 \\$\pm$1.0}&\thead{ 48.6 \\$\pm$0.9}&\thead{ 75.4 \\$\pm$0.5}&\thead{ 54.4 \\$\pm$0.7}&\thead{ 50.2 \\$\pm$0.6}&\thead{ 50.2 \\$\pm$1.9}&\thead{ 48.5 \\$\pm$1.0}&\thead{ 66.1 \\$\pm$0.7}&\thead{ 48.7 \\$\pm$0.6}&\thead{ 45.2 \\$\pm$0.4}&\thead{ 46.6 \\$\pm$2.0}&\thead{ 46.0 \\$\pm$1.3}\\
\underline{0.196} &\thead{ \textbf{98.9} \\$\pm$0.1}&\thead{ \textbf{69.9} \\$\pm$0.5}&\thead{ \textbf{71.9} \\$\pm$0.4}&\thead{ \textbf{65.2} \\$\pm$1.4}&\thead{ \textbf{72.6} \\$\pm$0.3}&\thead{ \textbf{98.8} \\$\pm$0.2}&\thead{ \textbf{85.7} \\$\pm$1.5}&\thead{ \textbf{86.6} \\$\pm$1.4}&\thead{ \textbf{69.3} \\$\pm$0.8}&\thead{ \textbf{73.9} \\$\pm$1.3}&\thead{ \textbf{99.2} \\$\pm$0.1}&\thead{ \textbf{72.4} \\$\pm$2.0}&\thead{ \textbf{74.6} \\$\pm$1.5}&\thead{ \textbf{63.7} \\$\pm$1.3}&\thead{ \textbf{74.8} \\$\pm$0.9}\\
\bottomrule
\end{tabular}
}
\caption{\loc{5}\inpercent{}\tablefinisher{\gls{cLW}} for \resnet{}\bldStatement\undlinstmt}
\label{table:loc5 in stdbeta}
\end{table*}




\begin{figure*}[t]
\begin{center}
  \includegraphics[width=.6\linewidth]{plots/in_cfnDivAccFinetune.png}
\end{center}
  \caption{Relationship between finetuned \loc{5} and accuracy for varying \gls{cLW} for \resnet{}, portrayed via color, on \fgvcheader. Each dot represents an increase by a factor of $\sqrt{10}$ and the standard deviation is indicated by the shaded area. $\gls{cLW}=1.96$ \gls{customLoss} is not shown, as it dominates the training. }
   
\vspace{-0.55cm}
\label{fig:DivAcc}
\end{figure*}


\comparison{Scarlet Tanager}{0}
\comparison{White Crowned Sparrow}{3}
\comparison{Brown Creeper}{1}
\comparison{Cape Glossy Starling}{2}
\comparison{Ford Freestar Minivan}{4}
\comparison{Audi S5 Coupe 2012}{5}
\comparison{Ford Ranger SubCab2011}{6}
\comparison{Tornado}{7}
\comparison{Fokker 100}{8}

        
        
        
        
\end{document}
