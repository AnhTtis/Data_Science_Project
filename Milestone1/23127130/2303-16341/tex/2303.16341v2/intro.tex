\section{Introduction}
Videos are composed of groups of pixels spanning spatially and temporally. 
Semantically related groups of pixels form the objects in the visual scenes and their changes through space and time vividly show the action and interactions of physical world. 
Scene switching further complicates the video story line and finally depicts attractive video stories.
The similar structures also appear in the paragraphs when they come to describe the videos.
Captions are built from the basic grammar components such as nouns and verbs, and sentences are concatenated to describe complex scenes.

Modern video-language models (VLMs), however, mostly neglect the fine-grained structures of such video-text pairs during the development.
%
Video-language pre-training typically follows the pipeline: (1) encoding video and text pairs into latent representations, (2) modality fusion, and (3) pre-training on specific objectives.
%
Existing methods typically optimize these three components in the pre-training pipeline by designing expressive encoders~\citep{bain2021frozen,li2022align,ge2022bridging,nagrani2022learning,ma2023temporal}, fusing two modalities via a cross-encoder~\citep{ge2022bridging,li2022align,lei2021less,li2020hero,luo2020univl,xu2021vlm,zhu2020actbert}, or adopting a combination of various pre-training tasks such as contrastive learning and masked modeling~\citep{li2022align,ge2022bridging,fu2021violet,zellers2022merlot,cao2022locvtp,ge2022miles}.
%
While these modifications benefit the pre-trained model, their lack of local discriminative modeling poses challenges for VLMs to further understand complex videos.

It has been shown that most video-language pre-training methods merely perform well on learning holistic representations to match a $\langle \textit{video, caption} \rangle$ pair while neglect fine-grained information such as region-object correspondences, or scene/action changes along the time in a video~\citep{akbari2021vatt,bain2021frozen,lei2021less,li2020hero,luo2020univl,miech2019howto100m,xu2021videoclip,nagrani2022learning}.
%
However, such regional or temporal fine-grained information has been demonstrated to play a vital role in localization and reasoning tasks~\citep{li2022align,ge2022bridging,zhang2022unsupervised,ma2023temporal,yuan2022contextualized}. 
Motivated by aforementioned observations, we revive the strong connectivity between basic components of video clips and languages during self-supervised video-language pre-training.
We approach the video-language pre-training task from a different perspective with a focus on exploiting spatiotemporally fine-grained structures.
%
% For example, in Figure \ref{fig:motivation}, given a video of a woman applying facial cream, per-frame features from the model pre-trained with only global contrastive loss are relatively hard to distinguish, 
% % the model  would output features  
% which makes it challenging to find boundaries in tasks like temporal action localization. 
% %
% Furthermore, region-object alignment frequently appears in a video and caption pair as shown in Figure \ref{fig:motivation}, where we highlight regions and their associated objects with the same color.
% Few methods take such alignment into consideration.

In this work, we incorporate structured video-language interactions into the pre-training stage and propose a novel framework,  
% Text-\textbf{G}rounded \textbf{Vi}deo-\textbf{L}anguage \textbf{M}odeling, named \ours.
namely \textbf{S}tructured \textbf{Vi}deo-\textbf{L}anguage \textbf{M}odeling (\ours), with temporal grouping and spatial grounding.
% \textbf{G}rounding-\textbf{G}rouping \textbf{Vi}deo-\textbf{L}anguage \textbf{M}odeling, 
\ours\ encourages instance-level video-caption  alignment, fine-grained  region-object  alignment, and learns temporal-aware video representations, simultaneously.
%
As shown in Figure \ref{fig:framework}, \ours consists of three primary training objectives: inter-clip spatial grounding, intra-clip temporal grouping, and global contrastive learning. 
%
Given a video-caption pair as the input, a classical dual-encoder model is leveraged to extract the representation for each modality, respectively. 
%
% Specifically, spatiotemporal grounding maintains a set of group tokens shared among all videos (\textbf{inter-video}). These tokens aggregate semantically similar video patches and are updated via alignment with noun phrases extracted from the caption to promote region-noun correspondences.
%
% On the other hand, temporal grouping leverages cut-and-paste to manually create temporal scene changes and then learns distinguishable features from different scenes in one video (\textbf{intra-video}). 
%
Videos are pre-processed with the cut-and-paste operation, inspired by ~\citep{zhang2022unsupervised,yun2019cutmix}, i.e., pasting one clip in a video onto the other background video, to explicitly introduce temporal scene changes.
%
We further adopt grouping blocks~\citep{xu2022groupvit,yu2022k} to aggregate semantically similar video patches to represent regions without off-the-shelf detectors via a set of group tokens shared among all videos.
%
In \textit{inter-clip spatial grounding}, we align grouped video tokens with objects represented by nouns in the caption by minimizing our designed grounding loss. % among a batch of video clips. 
%
In \textit{intra-clip temporal grouping}, we improve features temporal granularity by distinguishing foreground and background representations within one clip. 
%
Finally, the model is trained by a global video-caption contrastive loss to match instance-level video-caption pairs.
We evaluate our proposed method comprehensively on four representative tasks, including text-video retrieval, video question answering, video action recognition, and temporal action localization. 
Our strong experimental results demonstrate that exploiting fine-grained video-text structures during pre-training effectively improves VLM's video understanding and reasoning capabilities.

Our key contributions are summarized as follows:
% \begin{compactitem}
\begin{itemize}
    \item We propose \ours, a dual-encoder video-language modeling framework, making use of structured video-caption interactions to learn more expressive spatiotemporal features. 
    % Two novel designs are introduced: temporal grouping aims at learning temporal-aware features while spatial-temporal grounding facilitates alignment between regions and noun concepts.
    \item We leverage a cut-and-paste operation to introduce scene changes into videos during pre-training, and propose an intra-clip grouping module to learn more temporal-aware features.
    %
    \item We design an inter-clip spatial grounding module to capture fine-grained correspondences by aligning objects from the caption and regions from the video in a self-supervised manner.
    % \item In addition to global contrastive learning, we introduce two novel pre-training designs, temporal grouping and spatial-temporal grounding. Temporal grouping aims at learning temporal-aware features while spatial-temporal grounding facilitates alignment between regions and noun concepts.
    \item Experimental results have demonstrated the effectiveness of \ours on four downstream tasks, including text-video retrieval, video question answering, video action recognition, and temporal action localization. For example, \ours outperforms SOTA by 3\% in R@1 in zero-shot video-text retrieval on MSR-VTT and 5\% in accuracy in action recognition on UCF101, showing its advantages over both multi-modal and single-modal tasks.
    % performs competitive on other downstream tasks.
    % \item We show that the intrinsic properties of pre-training datasets play an important role in performance of downstream tasks: less noisy datasets such as VideoCC~\cite{bain2021frozen} benefits retrieval which requires strong vision-language alignment and more action-related datasets like HowTo100M~\cite{miech2019howto100m} contributes to learning temporal-aware features.
% Text-video retrieval: Improve by 9\% in R@10 in zero-shot performance. Video question answering: Achieve better or similar performance with less training data. Video action recognition: SOTA linear evaluation performance on UCF and HMDB. Temporal action localization: Outperform unsupervised or supervised baselines
% \end{compactitem}
\end{itemize}