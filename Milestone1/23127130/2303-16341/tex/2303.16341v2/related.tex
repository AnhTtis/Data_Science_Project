\section{Related Work}
\noindent\textbf{Video-language pre-training.} Video-language pre-training is an emerging research area that aims to develop machine learning models capable of jointly understanding visual and textual content.
Representations learned from large scale noisy datasets such as HowTo100M~\citep{miech2019howto100m}, WebVid~\citep{bain2021frozen}, and VideoCC~\citep{nagrani2022learning} have demonstrated great potentials in adapting to downstream tasks, including but not limited to text-video retrieval, video question answering, and video captioning. 
Elaborately designed pre-training objectives ranging from generative~\citep{chen2020uniter,fu2021violet,li2019visualbert,liu2022ts2} to discriminative~{\citep{bain2021frozen,lei2021less,akbari2021vatt,sun2022long,li2022align,ge2022bridging,ma2022x,wang2023parameter, wang2023all, wang2023unified}} have been proposed, among which contrastive learning is prevalent and widely adopted to attract paired video-caption instances and repelling unpaired ones. 
However, their primary focus is still on learning holistic global representations to align instance-level $\langle \textit{video, caption}\rangle$ pairs. 
Recently, some approaches have been proposed to leverage finer-grained information such as nouns/verb phrases from a caption.
%
ALPRO~\citep{li2022align} extracts pseudo entity labels by feeding noun prompts into a frozen model and use contrastive objective to align cropped visual regions and the corresponding textual labels.
%
In~\citet{ge2022bridging}, MCQ recovers randomly masked noun/verb tokens via resorting to global video features, which implicitly improves text entity association in visual encoding.
%
{LAVILA~\citep{zhao2023learning} constructed temporally dense captions by automatic annotation from large language models to describe activities more comprehensively.}
{In addition, TemPVL~\citep{ma2023temporal} enables temporal and semantic alignment such that the trained model can accurately perceive temporal boundaries in videos given the text description.
}
Despite these efforts, correspondences between visual regions and objects from noun concepts in captions and temporal scene shifts in a video are still neglected and not modeled explicitly in existing video-language pre-training methods. 
In this work, we propose two novel designs, spatial grounding and temporal grouping, to leverage fine-grained information in the pre-training stage.
% there has been a surge of interest in video-language pre-training, with researchers proposing various methods for pre-training models on video and language data. These methods range from simple fusion techniques to more complex multimodal architectures based on Transformer models. 
% Despite the promising results achieved by some of these methods, there are still many challenges to overcome in the field of video-language pre-training.


\noindent\textbf{Vision language grounding.} The goal of visual grounding (VG) is to locate the most relevant object or region in a visual input based on a natural language query~\citep{fang2015captions,rohrbach2016grounding,fukui2016multimodal,ghiasi2022scaling,gupta2020contrastive}. 
%
Recently, visual grounding has been adapted to pre-training tasks in a self-supervised manner for open-vocabulary image segmentation~\citep{ghiasi2022scaling,xu2022groupvit}. 
%
For example, OpenSeg~\citep{ghiasi2022scaling} semantically aligns a caption with extracted image regions via a grounding loss. 
%
Moreover, without the off-the-shelf object detectors, GroupViT~\citep{xu2022groupvit} learns to group together semantic regions from text supervision by contrastive learning. 
%
Note that visual grounding is mostly discussed in the image domain and its success motivates us to extend visual-semantic alignment to video-language pre-training.
%
To achieve this, we integrate a novel spatial grounding module in our framework to promote visual and textual entity correspondences in a self-supervised manner.
% Recently, visual grounding has been adapted to a pre-training task in a self-supervised manner for open-vocabulary image segmentation. For example, OpenSeg semantically aligns a caption with extracted image regions via a grounding loss while GroupViT .


\noindent\textbf{Video temporal modeling.} In contrast to images, videos contain a sequence of dynamic frames and how to model temporal information is critical in video understanding~\citep{feichtenhofer2019slowfast,bertasius2021space,tran2014c3d,alwassel2021tsp,zhang2022unsupervised,qian2022teg}. 
Specifically, TSP~\citep{alwassel2021tsp} learns temporal information via predicting clips inside or outside the action with substantial annotations. PAL~\citep{zhang2022unsupervised} aligns features of pasted pseudo action regions from two synthetic videos. BSP~\citep{xu2021boundary} introduces a novel boundary-sensitive pretext task via classifying the boundary types of synthetic videos. 
These techniques are elaborately designed for training models on long videos such as movies or TV dramas, which contains natural scene changes.
%
However, few of them have been considered in  video-language pre-training since the majority of video-language datasets contains short videos with repeated frames and are lacking in temporal differences. 
%
Instead, we develop a temporal grouping method to learn temporal-aware clip features in a self-supervised manner.
%
We show that features extracted from explicitly temporal modeling achieve significant improvements in not only temporal action localization tasks, but also coarse-grained reasoning and understanding tasks such as video question answering and video action recognition.
