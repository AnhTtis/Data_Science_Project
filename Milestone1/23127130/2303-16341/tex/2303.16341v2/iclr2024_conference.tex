
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage[sort,nocompress]{cite}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{relsize}
% \usepackage{subcaption}
\usepackage{float}
\usepackage{multirow}
\usepackage{textcomp}
\usepackage{xspace}
\usepackage{comment}
\usepackage{caption}

% \newcommand{\ours}{G-ViLM\xspace}
% \newcommand{\ours}{G$^2$ViLM\xspace}
\newcommand{\ours}{S-ViLM\xspace}
\newcommand{\rebuttal}[1]{\textcolor{blue}{#1}}
\newcommand{\authorskip}{\quad}

\title{Structured Video-Language Modeling with Temporal Grouping and Spatial Grounding}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\author{Yuanhao Xiong$^{1,3}\thanks{Work done as a student researcher at Google Research.}$ \authorskip Long Zhao$^{1}$ \authorskip Boqing Gong$^{1}$ \authorskip Ming-Hsuan Yang$^{1}$ \\
\vspace{2pt}
\textbf{Florian Schroff$^{1}$ \authorskip Ting Liu$^{1}$  \authorskip Cho-Jui Hsieh$^{2,3}$ \authorskip Liangzhe Yuan$^{1}$} \\
\vspace{2pt}
$^{1}$Google Research \authorskip $^{2}$Google \authorskip $^{3}$UCLA \\
}
% \author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Existing video-language pre-training methods primarily focus on instance-level alignment between video clips and captions via global contrastive learning but neglect rich fine-grained local information in both videos and text, which is of importance to downstream tasks requiring temporal localization and semantic reasoning.
A powerful model is expected to be capable of capturing region-object correspondences and recognizing scene changes in a video clip, reflecting spatial and temporal granularity, respectively.
%
To strengthen model's understanding into such fine-grained details, we propose a simple yet effective video-language modeling framework, \ours, by exploiting the intrinsic structures of these two modalities.
% to learn spatiotemporal features from both intra- and inter-clip perspectives.
% promote learning temporal-aware features and local region-noun alignment at the same time. 
% Two novel designs involving spatiotemporal grounding and temporal grouping promote learning local region-noun alignment and temporal-aware features simultaneously.
It includes two novel designs, inter-clip spatial grounding and intra-clip temporal grouping, to promote learning region-object alignment and temporal-aware features, simultaneously.
%
% Specifically, spatiotemporal grounding maintains a set of group tokens shared among all videos (\textbf{inter-video}). These tokens aggregate semantically similar video patches and are updated via alignment with noun phrases extracted from the caption to promote region-noun correspondences.
%
% On the other hand, temporal grouping leverages cut-and-paste to manually create temporal scene changes and then learns distinguishable features from different scenes in one video (\textbf{intra-video}). 
%
Comprehensive evaluations demonstrate that \ours performs favorably against existing approaches in learning more expressive representations.
Specifically, \ours surpasses the state-of-the-art methods substantially on four representative downstream tasks, covering text-video retrieval, video question answering, video action recognition, and temporal action localization.
% \ours performs competitively on all evaluated tasks and in particular achieves R@10 of 65.1 on zero-shot MSR-VTT retrieval, over 9\% higher than the state-of-the-art method.
\end{abstract}

\input{./intro.tex}
\input{./related.tex}
\input{./method.tex}
\input{./exp.tex}
\input{./conclusion.tex}

\section*{Acknowledgement}
We thank the reviewers for their invaluable feedbacks.
Cho-Jui Hsieh is partially supported by NSF 2331966, 2325121, 2330830, 2244760, 2008173, and ONR 20230936.

\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\clearpage
\input{./appendix.tex}
% \appendix
% \section{Appendix}
% You may include other additional sections here.

\end{document}
