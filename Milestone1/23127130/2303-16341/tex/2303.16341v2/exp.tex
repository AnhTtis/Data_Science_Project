\section{Experiments}
\subsection{Downstream Tasks}
\label{sec:downstream}
\noindent\textbf{Text-video retrieval.} We adopt the widely used text-video retrieval benchmark MSR-VTT~\citep{xu2016msr} for evaluation. 
%
It consists of 10K YouTube video clips with 200K captions. 
%
We conduct experiments in both zero-shot and fine-tuning settings.
%
For fine-tuning setup, we follow \citet{bain2021frozen} and \citet{ge2022bridging}, and train and test the model on the split of 9K and 1K videos.

\noindent\textbf{Video question answering~(VQA).} We consider open-ended VQA settings with two representative datasets: (1) MSRVTT-QA~\citep{xu2017video} with 1,500 answer candidates and (2) MSVD-QA~\citep{xu2017video} with 2,423 answer candidates. 

\noindent\textbf{Video action recognition.} We select HMDB51~\citep{kuehne2011hmdb} containing 6,766 videos with 51 categories and UCF101~\citep{soomro2012ucf101} containing 13,320 videos with 101 categories. Both linear probing and fine-tuning the whole model are explored.

\noindent\textbf{Temporal action localization~(TAL).} TAL aims at predicting the temporal extent and the labels of action instances. We evaluate the performance on ActivityNet~\citep{heilbron2015activitynet}, an action understanding dataset of 19,994 temporally  annotated  untrimmed  videos  with  200  action categories.

\subsection{Implementation Details}
\label{sec:implement}
\noindent\textbf{Input.} Following \citet{nagrani2022learning}, we sample 32 frames for each video and resize them into $224\times224$ with the same augmentations.
Each caption is tokenized into 32 tokens including $\texttt{[CLS]}$. 
$K=2$ noun phrases are extracted for each caption and then prompted with a set of prompt templates such as \textit{``It is a video of \{noun\}''}.
We include the full list of prompt templates in Appendix~\ref{appendix:detail}.

\noindent\textbf{Model architecture.} We use a 12-layer ViT-base model with the patch size of $2\times16\times16$ as the video encoder and initialize it with weights pre-trained on Kinetics-400.
We adopt 32 learnable group tokens and 3 grouping blocks featuring K-means attention~\citep{xu2022groupvit,yu2022k}.
Grouping blocks are inserted at the 6th, 9th and last layers of the video encoder~\citep{xu2022groupvit, yu2022k}. 
% and kMaX-DeepLab~\citep{}. 
The text encoder is initialized from the pre-trained BERT-base model.
% with uncased wordpiece tokenization. 
All representations are projected into the common space with the dimension of 256.

\noindent\textbf{Pre-training datasets.} We pre-train \ours\ with the VideoCC~\citep{nagrani2022learning} dataset, which contains about 3.3M video-caption pairs. 
%
We also include ActivityNet-Caption~\citep{krishna2017dense} with 20K well-aligned pairs into the pre-training corpus. 
We note the commonly-used WebVid~\citep{bain2021frozen} is unavailable to us due to the restricted data access policy. 
To illustrate the effectiveness of our proposed method and how the pre-training datasets contribute to the final results, we designed fair studies on dataset impacts.
Details could be found in Section~\ref{sec:ablation}.
%

\noindent\textbf{Pre-training and fine-tuning setups.} 
We implement \ours\ in JAX and train all models on TPU accelerators. During pre-training, SGD with momentum 0.9 and initial learning rate 0.1 is used for optimization. 
We train \ours for 10 epochs with a batch size 1024 and adopt a cosine learning rate decay schedule with a warmup ratio 0.05. It takes about one day for the whole pre-training stage.
%
In terms of fine-tuning, different tasks are trained independently with their own set of hyperparameters on the target dataset and more details can be found in Appendix \ref{appendix:detail}. 
%
For temporal action localization, we fix weights of the pre-trained video encoder and its grouping blocks to extract video features, which are then evaluated by G-TAD~\citep{xu2020g}, a commonly used method for TAL.
% \vspace{-1em}
\subsection{Evaluation Results}
\label{sec:results}
\begin{table*}[t]
\centering
	\caption{Zero-shot (top) and fine-tuning evaluation (bottom) of text-video retrieval on MSR-VTT test set with 1K videos. \textbf{Higher} R@k and \textbf{lower} MedR (Median Rank) indicate better performance.} 
	\vspace{-0.5em}
	\label{tab:msrvtt}
	\scalebox{0.7}{
		\begin{tabular}{@{}c|ccc|cccc@{}}
			\toprule[1pt]
			\multirow{1}*{\textbf{Method}}& \textbf{Video Encoder Input} &\textbf{PT Dataset}& \bf $\#$Pairs PT & \bf R@1& \bf R@5& \bf R@10& \bf MedR\\
			\midrule	\multirow{1}*{MIL-NCE~\citep{miech2019howto100m}}&Raw Videos&HowTo100M&120M&9.9&24.0&32.4&29.6\\
			\multirow{1}*{VATT~\citep{akbari2021vatt}}  &Raw Videos&HowTo100M, AudioSet&138M&-&-&29.7&49.0\\
			\multirow{1}*{VideoCLIP~\citep{xu2021videoclip}}  &S3D&HowTo100M&110M&10.4&22.2&30.0&-\\
			\multirow{1}*{SupportSet~\citep{patrick2020support}}  &R(2+1)D-34&HowTo100M&120M&12.7&27.5&36.2&24.0\\
			\multirow{1}*{Frozen~\citep{bain2021frozen}}  &Raw Videos&CC3M, WebVid-2M&5.5M&18.7&39.5&51.6&10.0\\
			\multirow{1}*{AVLnet~\citep{rouditchenko2020avlnet}}  &ResNeXt-101&HowTo100M&120M&19.6&40.8&50.7&9.0\\
  DemoVLP~\citep{cai2022revitalize} & Raw Videos & CC3M, WebVid-2M & 5.5M & 24.0 & 44.0 &52.6 & 8.0\\
  ALPRO~\citep{li2022align} & Raw Videos & CC3M, WebVid-2M & 5.5M & 24.1 & 44.7 & 55.4 & 8.0 \\
			\multirow{1}*{MCQ~\citep{ge2022bridging}}  &Raw Videos&CC3M, WebVid-2M&5.5M&26.0&46.4&56.4&7.0\\
			\multirow{1}*{VCC~\citep{nagrani2022learning}}  &Raw Videos&VideoCC &3.3M&18.9&37.5&47.1&-\\
			\multirow{1}*{\bf\ours}&Raw Videos&VideoCC, ActivityNet& 3.3M &\textbf{28.6}&\textbf{53.6}&\textbf{65.1}&\textbf{5.0}\\

			\midrule
	\multirow{1}*{UniVL~\citep{luo2020univl}} &S3D&HowTo100M&110M&21.2&49.6&63.1&6.0\\
			\multirow{1}*{MMT~\citep{gabeur2020multi}} &S3D&HowTo100M&120M&26.6&57.1&69.6&4.0\\
			\multirow{1}*{ClipBERT~\citep{lei2021less}}  &Raw Videos&COCO, VisGenome&5.6M&22.0&46.8&59.9&6.0\\
			\multirow{1}*{AVLnet~\citep{rouditchenko2020avlnet}}  &ResNeXt-101&HowTo100M&120M&27.1&55.6&66.6&4.0\\

			\multirow{1}*{SupportSet~\citep{patrick2020support}}  &R(2+1)D-34&HowTo100M&120M&30.1&58.5&69.3&3.0\\	
			\multirow{1}*{VideoCLIP~\citep{xu2021videoclip}}  &S3D&HowTo100M&110M&30.9&55.4&66.8&-\\
			\multirow{1}*{Frozen~\citep{bain2021frozen}}  &Raw Videos&CC3M, WebVid-2M&5.5M&31.0&59.5&70.5&3.0\\
  DemoVLP~\citep{cai2022revitalize} & Raw Videos & CC3M, WebVid-2M & 5.5M & 36.0 & 61.0 &71.8 & 3.0\\
  ALPRO~\citep{li2022align} & Raw Videos & CC3M, WebVid-2M & 5.5M & 33.9 & 60.7 & 73.2 & 3.0 \\
			\multirow{1}*{MCQ~\citep{ge2022bridging}}  &Raw Videos&CC3M,  WebVid-2M&5.5M&37.6&{64.8}&{75.1}&3.0\\	
		    \multirow{1}*{VIOLETv2~\citep{fu2023empirical}}  &Raw Videos&CC3M, WebVid-2M&5.5M&37.2&64.8&75.8&-\\
		   { \multirow{1}*{All-in-One~\citep{wang2023all}} } & {Raw Videos} & {HowTo100M, WebVid-2M}&{112M}&{37.1}&{\bf66.7}&{75.9}&{-}\\
			\multirow{1}*{VCC~\citep{nagrani2022learning}}  &Raw Videos&VideoCC &3.3M&35.0&63.1&75.1&-\\
  			\multirow{1}*{\bf\ours}&Raw Videos&VideoCC, ActivityNet& 3.3M &\textbf{38.4}&{65.7}&\textbf{76.3}&\textbf{2.0}\\
			\bottomrule[1pt]
	\end{tabular}
	}
	% \vspace{-1em}
\end{table*}

\subsubsection{Text-Video Retrieval}
We evaluate \ours for the text-video retrieval task on MSR-VTT under both zero-shot and fine-tuning settings, and compare it with existing prevalent methods in Table~\ref{tab:msrvtt}.
\ours outperforms other methods significantly for zero-shot evaluation with R@10 of 65.1, yielding approximately 9\% improvement over the best-performing baseline MCQ. 
The superior results demonstrate that our pre-trained model builds up a good alignment between video and language and generalizes well to unseen datasets. 
%
\ours also achieves performance gain when the model is fine-tuned on the target MSR-VTT dataset, which further validates advantages of the pre-trained model.
%
Note that \ours performs favorably against existing methods despite the much smaller size of the pre-training data used in \ours than those in baselines, such as HowTo100M and WebVid-2M. 
%

\subsubsection{Video Question Answering}
VQA results on two open-ended datasets are shown in Table \ref{tab:qa}.
%
To enable \ours to deal with the VQA task, we add a fusion head adapted from BUTD~\citep{anderson2018bottom} by integrating video and text features with simple linear layers. Then a classifier is inserted after the fusion module to perform question answering as a classification problem.
Compared with previous methods which leverage particular architectures for VQA or include a complicated fusion encoder, \ours is the most efficient and flexible for various vision-language tasks. 
\ours achieves better performance than competing methods with the accuracy of 43.5\% (+1.4\%) and 46.4\% (+0.5\%) on MSRVTT-QA and MSVD-QA, respectively. 
%
\begin{table}[!t]
\begin{center}
    \caption
	{
	Top-1 accuracy (\%) of Video Question Answering on MSRVTT-QA and MSVD-QA.
	}
	\vspace{-0.5em}
	\label{tab:qa}
    \scalebox{0.65}{
% \resizebox{0.48\textwidth}{!}{
	\centering	
		\begin{tabular}	{@{}c | c |  c c@{}}
			\toprule
			\textbf{Method}  & \textbf{PT Dataset} & \textbf{MSRVTT-QA}     & \textbf{MSVD-QA} \\
			\midrule
		HGA~\citep{jiang2020reasoning} & - & 35.5 & 34.7 \\
		QUEST~\citep{jiang2020divide} & - & 34.6 & 36.1 \\	
		HCRN~\citep{le2020hierarchical} & - & 35.6 & 36.1 \\
		ClipBERT~\citep{lei2021less} & COCO, VG & 37.4 & - \\
		SSML~\citep{amrani2021noise}& HowTo100M & 35.1 & 35.1 \\
	CoMVT~\citep{seo2021look} & HowTo100M & 39.5 & 42.6 \\
 DemoVLP~\citep{cai2022revitalize} & CC3M, WebVid-2M & 38.3 & 39.5 \\
		ALPRO~\citep{li2022align} & CC3M, WebVid-2M & 42.1 & 45.9 \\
  \bf\ours & VideoCC, ActivityNet & \textbf{43.5} & \textbf{46.4} \\
			\bottomrule
		\end{tabular}
  }
\end{center}
\end{table}

\subsubsection{Video Action Recognition}
For video action recognition, we only keep the video encoder together with its grouping blocks to extract single-modality video representations for evaluation. 
%
Two evaluation settings are considered: (1) linear probing where the backbone encoder is frozen and only the last linear classifier is trained and (2) end-to-end fine-tuning where both the backbone and the classifier are trained. 
Top-1 accuracy on UCF101 and HMDB51 is reported in Table \ref{tab:action}.
We observe that in linear probing, \ours outperforms other baselines, with 3.0\% and 2.9\% higher than current SOTA, 
%
MMV that leverages audio and text modalities in addition on UCF101 and HMDB51.
\ours also achieves consistently superior performance under the fine-tuning evaluation. 
Outstanding performance of \ours demonstrates that leveraging fine-grained video language structures during pre-training contributes to meaningful video representations.
This aligns with our intuition because finer-grained video-text alignment improves video understanding.

\begin{table}
% \captionsetup{font=\footnotesize}
\begin{minipage}{.46\textwidth}
\centering
 	\caption{Experiments of action recognition on UCF101 and HMDB51 with linear evaluation (Lin) and fully fine-tuning evaluation (FT).}
%  	``Modal'' denotes the modality used for pre-training in addition to videos, \textit{i.e.}, optical flow (OF), motion vector (MV), audio (A), text (T). } 
	\vspace{-5pt}
% 	\vspace{-10pt}
	\label{tab:action}
	\scalebox{0.65}{
		\begin{tabular}{@{}c|c|cc|cc@{}}
			\toprule
\multirow{2}*{\textbf{Method}} & \multirow{2}*{\textbf{Modal}} & \multicolumn{2}{c|}{\textbf{UCF101}} & \multicolumn{2}{c}{\textbf{HMDB51}} \\
			&&Lin&FT &Lin&FT \\
			\midrule
			\multirow{1}*{CoCLR~\citep{han2020self}}&OF&77.8&90.6 & 52.4 & 62.9\\
			\multirow{1}*{MVCGC~\citep{huo2021compressed}}&MV&78.0&90.8 & 53.0 & 63.4\\
			\multirow{1}*{XDC$\_$R~\citep{alwassel2020self}}&A&80.7&88.8 & 49.9 & 61.2\\
			\multirow{1}*{XDC$\_$K~\citep{alwassel2020self}}&A&85.3&91.5 & 56.0 & 63.1\\
			\multirow{1}*{MIL-NCE~\citep{miech2019howto100m}}&T&83.4&89.1 & 54.8 & 59.2\\
			\multirow{1}*{Frozen~\citep{bain2021frozen}}&T&87.8&89.8 & 61.3 & 66.3\\	
			\multirow{1}*{VATT~\citep{akbari2021vatt}}&A, T&89.2&- & 63.3 & -\\
			\multirow{1}*{ELO~\citep{piergiovanni2020evolving}}&A, OF&-&93.8 & 64.5 & 67.4\\
			\multirow{1}*{MMV~\citep{alayrac2020self}}&A&77.1&- & 53.6 & - \\
			\multirow{1}*{MMV~\citep{alayrac2020self}}&T&86.8&- & 55.1 & -\\
			\multirow{1}*{MMV~\citep{alayrac2020self}}&A, T&91.8&95.2 & 67.1 & 75.0 \\
			\multirow{1}*{MCQ~\citep{ge2022bridging}}&T&89.1&92.3 & 65.8 & 69.8\\
   \bf\ours & T & \textbf{94.8} & \textbf{96.5} & \textbf{70.0} & \textbf{76.9} \\
			\bottomrule[1pt]
	\end{tabular}
 }
    % \vspace{2pt}

\end{minipage}
\hspace{0.3in}
\begin{minipage}{.45\textwidth}
% \begin{table}[t]
\caption{Comparison to SOTA methods on temporal action localization~(\textbf{TAL}).}
\label{tab:tad}
\vspace{-5pt}
\begin{center}
\scalebox{0.65}{
\begin{tabular}{@{}c|cccc@{}}
\toprule
\multirow{2}*{\textbf{Method}}  & \multicolumn{4}{c}{\textbf{TAL Task (G-TAD)}} \\
% \cmidrule(lr){2-5}
& mAP@0.5 & @0.75 & @0.95 & Avg \\
\midrule
CoCLR~\citep{han2020self}  & 47.9 & 32.3 & 7.3 & 31.9 \\
XDC~\citep{alwassel2020self} &  48.4 & 32.6 & 7.6 & 32.3 \\
MoCo-v2~\citep{chen2020improved} &  46.6 & 30.7 & 6.3 & 30.3 \\
VideoMoCo~\citep{pan2021videomoco} &  47.8 & 32.1 & 7.0 & 31.7 \\
RSPNet~\citep{chen2021rspnet} & 47.1 & 31.2 & 7.1 & 30.9 \\
AoT~\citep{wei2018learning} & 44.1 & 28.9 & 5.9 & 28.8 \\
SpeedNet~\citep{benaim2020speednet}  & 44.5 & 29.5 & 6.1 & 29.4 \\
PAL~\citep{zhang2022unsupervised}  & 50.7 & 35.5 & 8.7 & 34.6 \\
TAC~\citep{xu2020g}  & 48.5 & 32.9 & 7.2 & 32.5 \\
BSP~\citep{xu2021boundary}  & 50.9 & 35.6 & 8.0 & 34.8 \\
LoFi~\citep{xu2021low} & 50.4 & 35.4 & 8.9 & 34.4 \\
TSP~\citep{alwassel2021tsp}  & 51.3 & \bf 37.1 & 9.3 & \bf 35.8 \\
\bf\ours & \bf 51.7 & 36.4 & \bf 9.7 & 35.6 \\
\bottomrule
\end{tabular}
}
% \vspace{3pt}
\end{center}
\end{minipage}
\vspace{-1em}
\end{table}

\subsubsection{Temporal Action Localization}
We report the mean average precision (mAP) under different temporal Intersection over Union (tIoU) thresholds on ActivityNet in Table \ref{tab:tad}.
For temporal action localization, the model is pre-trained on HowTo100M only, which is observed to be beneficial to TAL compared with VideoCC + ActivityNet (see the ablation study below).
We directly use pre-trained models to extract video features as the input to G-TAD and do not further train the encoder.
%
\ours consistently exceeds other self-supervised competitors and even fully supervised approaches such as LoFi and BSP. This observation again consolidates the conclusion that vision-language pre-training can not only be applied to specific VL problems like text-video retrieval, but also benefit single-modal downstream tasks.

\subsubsection{Ablation Studies}
\label{sec:ablation}
\noindent \textbf{Pre-training datasets.} To analyze of effects of pre-training datasets, we report the model performances on selected downstream tasks in Table \ref{tab:pt}.
%
In particular, the same model pre-trained on VideoCC achieves the best performance in zero-shot retrieval on MSR-VTT, compared with HowTo100M and WebVid-2M. 
These results coincide with findings in~\citet{nagrani2022learning}, where HowTo100M has been pointed out not appropriate for vision-language tasks requiring strong alignment. 
%
\ours trained on VideoCC alone significantly outperforms VCC on both tasks, showing the effectiveness of our proposed techniques.
In particular, when pre-trained on the same VideoCC dataset, \ours leads to better performance than MCQ. The significant improvement over MCQ shows that our techniques do help to learn better features for downstream tasks.
%
It is also worth noting that pre-training on VideoCC and ActivityNet performs consistently better than using only one dataset, and thus we choose this setup in the main experiments. 

\begin{figure}[t!]
    \centering
    \vspace{-2em}
    \includegraphics[width=0.96\textwidth]{figures/visual.pdf}
    \vspace{-0.4em}
    \caption{Visualization of \ours. Left: Similarity scores of features derived from the baseline and our method. Right: Attention maps between region and object with spatial grounding.
    }
    \label{fig:visual}
\end{figure}

\begin{table}[t]
\begin{center}
    \caption{Effects on different choices of pre-training datasets. }
    \vspace{-0.8em}
	\label{tab:pt}
    \scalebox{0.65}{
% \resizebox{0.48\textwidth}{!}{
	\centering	
\begin{tabular}	{@{}c|c|ccc|cccc@{}}
			\toprule
\multirow{2}*{\textbf{Method}} & \multirow{2}*{\textbf{PT Dataset}}  & \multicolumn{3}{c|}{\textbf{MSRVTT-ZS}}  & \multicolumn{4}{c}{\textbf{TAL}}\\
& &  R@1 & R@5 & R@10  & mAP@0.5 & 0.75 & 0.95 & Avg \\
 \midrule
 \multirow{3}*{VCC} & HowTo100M & 10.4 & 22.2 & 30.0 & \multicolumn{4}{c}{-} \\
& WebVid & 15.4 & 33.6 & 44.1 & \multicolumn{4}{c}{-}\\
& VideoCC & 18.9 & 37.5 & 47.1 & 49.9 & 34.3 & 8.7 &  33.7 \\
\midrule
  \multirow{1}*{MCQ} & VideoCC & 22.5 & 43.8 & 54.8 & \multicolumn{4}{c}{-} \\
\midrule
\multirow{4}*{\textbf{\ours}} & HowTo100M & 9.4 & 22.9 & 31.3 & 51.7 & 36.4 & 9.7 & 35.6 \\
& ActivityNet & 14.4 & 33.5 & 44.0 & 50.5 & 35.3 & 8.7 & 34.5\\
& VideoCC & 24.7 & 47.4 & 59.0 & 50.5 & 35.0 & 9.2 & 34.2 \\
& VideoCC, ActivityNet & 28.6 & 53.6 & 65.1 & 50.8 & 35.6 & 9.3 & 34.7 \\
			\bottomrule
		\end{tabular}
  }
  % }
    % \vspace{2pt}
	\vspace{-10pt}
\end{center}
\end{table}

\noindent \textbf{Training objectives.} Without loss of generality, the model in this ablation is pre-trained on VideoCC only. For better understanding \ours, we start with the contrastive baseline represented in Scenario~$1$ in Table~\ref{tab:objective}.
Then we add our proposed spatial grouping module during the pre-training phase. This module is driven by the grouping loss $\mathcal{L}_g$ in Scenario $2$, and we observe consistent improvements on all tasks across the board comparing to Scenario $1$.
Similarly, we introduce the temporal grouping module in $\mathcal{L}_t$ to encourage more temporal discriminative video representation.
After comparing Scenario $3$ to Scenario $1$ in Table~\ref{tab:objective}, we also observe noticeable improvements on different downstream tasks.
These phenomenons suggest both spatially and temporally fine-grained features improve video understanding tasks.
After combining everything together in Scenario $4$, we show significant performance improvements on all tasks, which demonstrates the effectiveness of \ours pre-training.
Moreover, we visualize effects of temporal grouping and spatial grounding in Figure~\ref{fig:visual}. It can be observed from similarity scores among frames that with temporal grouping, features from different scenes are much easier to distinguish. Besides, attention maps from spatial grounding indicates the alignment between the region and the noun phrase has been learned during the pre-training stage without any fine-grained annotations. More examples can be found in Appendix \ref{appendix:visual}.


\begin{table*}[ht]
\begin{center}
    \caption
	{Ablation study on training objectives.
	We validate that our proposed spatial grounding loss and temporal grouping loss both benefit downstream tasks.
	}
	\label{tab:objective}
	\vspace{-0.5em}
    \scalebox{0.7}{
% \resizebox{0.48\textwidth}{!}{
	\centering	
\begin{tabular}	{@{}c|ccc|ccc|c|c|cccc@{}}
			\toprule
% \textbf{Scenario}  & $\gL_c$  & $\gL_g$     & $\gL_t$ & xxx \\
\multirow{2}*{\textbf{Scenario}} & \multirow{2}*{$\gL_\text{contrast}$} & \multirow{2}*{$\gL_\text{g}$} & \multirow{2}*{$\gL_\text{t}$} & \multicolumn{3}{c|}{\textbf{MSRVTT-ZS}} & \textbf{MSVD-QA} & \textbf{UCF101} & \multicolumn{4}{c}{\textbf{TAL}}\\
 & & & & R@1 & R@5 & R@10 & Acc & Acc & mAP@0.5 & 0.75 & 0.95 & Avg \\
 \midrule
 1 & \checkmark & & & 22.7 & 45.9 & 57.0 & 43.6 & 90.5 & 49.9 & 34.3 & 8.7 &  33.7  \\
 2 & \checkmark & \checkmark & & 23.3 & 46.6 & 58.6 & 44.1 & 90.6 &  50.2 &  34.7 & 8.7 &  34.0\\
 3 & \checkmark & & \checkmark & 24.2 & 46.7 & 58.2 & 43.9 & 90.9 &  50.1 & 34.6 & 8.8 & 34.0\\
 4 & \checkmark & \checkmark & \checkmark & 24.7 & 47.4 & 59.0 & 44.9 & 91.0 &  50.5 & 35.0 & 9.2 & 34.2\\
			% \midrule
			\bottomrule
		\end{tabular}
%   }
   }
    % \vspace{2pt}

% \vspace{-15pt}
\end{center}
\end{table*}