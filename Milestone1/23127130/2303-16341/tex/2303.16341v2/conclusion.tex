\section{Conclusion}
In this paper, we present a novel video-language pre-training framework, named \ours, that aims to utilize fine-grained structures in video and languages to learn region-object correspondences and temporal-aware features simultaneously.
Spatial grounding and temporal grouping are introduced to achieve the goal of local region-object alignment and temporal distinction in a self-supervised manner. 
%
The proposed framework outperforms existing methods significantly on downstream tasks, including text-video retrieval, video question answering, video action recognition, and temporal action localization.
% 
The superior performance validates our design and our method could be easily scaled up, as it is self-contained and does not rely on other artifacts.
