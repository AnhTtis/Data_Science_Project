\appendix
\section{Implementation Details}
\label{appendix:detail}
\subsection{Text Prompt Templates}
As mentioned in Section \ref{sec:grounding}, we extract noun tokens by prompting noun phrases with pre-defined templates. We randomly select one from 12 templates to generate the prompt and details are presented in Table \ref{tbl:template}.

\begin{table}[ht]
\centering
% \vspace{-4ex}
% \begin{minipage}[b]{0.37\textwidth}
% \resizebox{0.48\textwidth}{!}
% \centering	
    \caption
	{Prompt templates used for generating noun tokens.}
\scalebox{0.9}{
    %  \setlength\tabcolsep{4pt}
		\begin{tabular}	{@{}c|l@{}}
			\toprule
			\textbf{Template} & \textbf{Prompts for noun phrases} \\
% 			\midrule
% 			\multirow{2}{*}{ZS} & CLIP~\cite{clip} & & & \\
% 			& ALBEF & & & \\
			\midrule
1&	A footage of a \{\}. \\
2&	A footage of the \{\}. \\
3&	A footage of one \{\}. \\
4&	A video of a \{\}. \\
5&	A video of the \{\}. \\
6&	A video of one \{\}. \\
7&	A portrait of a \{\}. \\
8&	A portrait of the \{\}.\\
9&	A portrait of one \{\}.\\
10&	A video footage of a \{\}.\\
11&	A video footage of the \{\}.\\
12&	A video footage of one \{\}.\\
			\bottomrule
		\end{tabular}}
% \vspace{2pt}

	\label{tbl:template}
% 	\vspace{-10pt}
\end{table}

\subsection{Structure of Grouping Block}
We demonstrate the structure of a grouping block in Figure \ref{fig:grouping-block}. It features a K-means clustering attention layer, in which attention scores are computed between group tokens as query and video tokens as value. The cluster assignment is computed via gumbel softmax over group tokens and converted into a one-hot hard assignment.
\begin{figure}[h] 
    \centering
    \includegraphics[width=0.9\linewidth]{figures/grouping.pdf}
    \caption{The structure of a grouping block. It is inserted at different layers of the video encoder to update group tokens by merging semantically similar video tokens.}
    % {\color{red}(arrow, our features})}
    \vspace{-0.8em}
    \label{fig:grouping-block}
\end{figure}

\subsection{Downstream tasks}
Implementation details of fine-tuning the pre-trained model on downstream tasks are described in this section.  During fine-tuning, we resize video frames to $224 \times 224$ and sample 32 frames for each video. The maximum length for each caption is 32 by default, the same as the value in the pre-training stage. Specific optimization settings for each dataset are shown in Tables \ref{tbl:msrvtt-ret} to \ref{tbl:tal}. 
\begin{table}[!h]

	\centering	
    %  \setlength\tabcolsep{4pt}
    \caption
	{End-to-end fine-tuning configurations on MSR-VTT for text-to-video retrieval.
	}
    \scalebox{0.85}{
		\begin{tabular}	{@{}l| l@{}}
			\toprule
			\textbf{Config} & \textbf{MSR-VTT} \\
			\midrule
	optimizer & SGD  \\
	base learning rate & 2.5e-1  \\
	optimizer momentum & 0.9 \\
	learning rate schedule & cosine decay  \\
	batch size & 512 \\
	warmup ratio & 0.1 \\
	training epochs & 20 \\
			\bottomrule
		\end{tabular}}
    % \vspace{2pt}

	\label{tbl:msrvtt-ret}
% \vspace{-10pt}
\end{table}

\begin{table}[h]

	\centering
    \caption
	{End-to-end fine-tuning configurations on MSRVTT-QA and MSVD-QA for VQA.
	}
	\scalebox{0.85}{
    %  \setlength\tabcolsep{4pt}
		\begin{tabular}	{@{}l| l | l@{}}
			\toprule
			\textbf{Config} & \textbf{MSRVTT-QA} & \textbf{MSVD-QA}\\
			\midrule
	optimizer & SGD & SGD \\
	base learning rate & 1e-1 & 5e-5\\
	optimizer momentum & 0.9 & 0.9 \\
	learning rate schedule & cosine decay & cosine decay\\
	batch size & 64 & 64\\
	warmup ratio & 0.1 & 0.1\\
	training epochs & 30 & 30\\
			\bottomrule
		\end{tabular}}
    % \vspace{2pt}

	\label{tbl:msrvtt-qa}
% 	\vspace{-10pt}
\end{table}

\begin{table}[h]

	\centering
    \caption
	{Fine-tuning configurations on UCF101 and HMDB51 for video action recognition.
	}
	\scalebox{0.85}{
    %  \setlength\tabcolsep{4pt}
		\begin{tabular}	{@{}l| l | l@{}}
			\toprule
			\textbf{Config} & \textbf{UCF101} & \textbf{HMDB51}\\
			\midrule
	optimizer & SGD & SGD \\
	base learning rate & 1e-1 & 1e-1\\
	optimizer momentum & 0.9 & 0.9 \\
	learning rate schedule & cosine decay & cosine decay\\
	batch size & 64 & 64\\
	warmup ratio & 0.1 & 0.1\\
	training epochs & 30 & 60 \\
			\bottomrule
		\end{tabular}}
    % \vspace{2pt}

	\label{tbl:action-recog}
% 	\vspace{-10pt}
\end{table}

\begin{table}[!h]

	\centering	
    \caption
	{Fine-tuning configurations on ActivityNet for temporal action localization.
	}
    %  \setlength\tabcolsep{4pt}
    \scalebox{0.85}{
		\begin{tabular}	{@{}l| l@{}}
			\toprule
			\textbf{Config} & \textbf{ActivityNet} \\
			\midrule
	optimizer & Adam  \\
	base learning rate & 4e-3  \\
	weight decay & 1e-4  \\	optimizer momentum & $\beta_1$=0.9, $\beta_2$=0.999 \\
	learning rate schedule & decay by $\gamma$=0.1 every 5 epochs   \\
	batch size & 16 \\
	training epochs & 10 \\
			\bottomrule
		\end{tabular}}
    % \vspace{2pt}

	\label{tbl:tal}
% \vspace{-10pt}
\end{table}

\clearpage
\section{Additional results}
\noindent \textbf{Number of frames.} The number of frames used in pre-training vary among different methods.
% ALPRO and MCQ adopted 4 frames while Frozen utilized 8 frames and DemoVLP uses 16 frames respectively.
We follow the setting in \citet{nagrani2022learning} to sample 32 frames for each video. 
% The comparison of VideoCC model and Frozen was conducted under the 32-frame setting, and VideoCC outperformed Frozen as well. 
% Besides, people care more about inference time than pre-training time.
% Despite using 4-frame in pre-training, ALPRO and MCQ use up to 16 frames for downstream tasks.
% 8 and 16 frames respectively for retrieval and VQA. 
Note that \ours uses a vanilla vision transformer with temporal patch size 2, which effectively downsamples the video frames at the first layer.
Since no downsampling happens in ALPRO and MCQ's encoder, their computational cost of 16 frames is comparable to \ours with 32-frame input.
% with temporal patch size 2 of 16(32)-frame input. 
In Table~\ref{tab:flop} we report 16 and 32-frame results and \ours outperforms ALPRO and MCQ in both settings.
% We report model performance given similar inference time in Table~\ref{tab:flop} and \ours still outperforms ALPRO and MCQ. 
As most methods use 16 frames during evaluation, we thus select to sample 32 frames to ensure a fair comparison.
\begin{table}[H]
\begin{center}
    \caption{Results with different number of frames.}
    \scalebox{0.8}{
% \resizebox{0.48\textwidth}{!}{
	\centering	
\begin{tabular}	{@{}c|c|ccc|cc@{}}
			\toprule
% \textbf{Scenario}  & $\gL_c$  & $\gL_g$     & $\gL_t$ & xxx \\
\multirow{2}*{\textbf{Method}}  & \multirow{2}*{\textbf{Input}} & \multicolumn{3}{c|}{\textbf{MSRVTT-ZS}} & \multicolumn{2}{c}{\textbf{UCF101}}\\
 & &  R@1 & R@5 & R@10 & Lin & FT \\
 \midrule
ALPRO & 8-frame & 24.1 & 44.7 & 55.4 & - & - \\
ALPRO & 16-frame & 24.7 & - & 55.0 & - & - \\
MCQ & 16-frame & - & - & - & 89.1 & 92.3 \\
% \ours & 16-frame & \bf 28.5 & \bf 53.5 & \bf 64.0 & \bf 93.1 & \bf 96.0 \\
\ours & 16-frame & 28.5 & 53.5 & 64.0 & 93.1 & 96.0 \\
\ours & 32-frame & \bf 28.6 & \bf 53.5 & \bf 65.1 & \bf94.8 & \bf96.5\\
		\bottomrule
		\end{tabular}
  }
  % }

% 	\vspace{-3em}
	\label{tab:flop}
\end{center}
\end{table}

\noindent \textbf{Spatiotemporal action localization.} We report experiment results of spatial temporal action localization task on AVA v2.2 dataset. 
Results are presented in the table below:
\begin{table}[H]
\begin{center}
    \caption{Results of spatiotemporal action localization on AVA v2.2. Two columns indicates using Detected and Grounded-truth boxes respectively.}
    \scalebox{0.8}{
% \resizebox{0.48\textwidth}{!}{
	\centering	
\begin{tabular}	{@{}c|cc@{}}
			\toprule
% \textbf{Scenario}  & $\gL_c$  & $\gL_g$     & $\gL_t$ & xxx \\
\multirow{2}*{\textbf{Method}} & \multicolumn{2}{c}{\textbf{mAP@0.5}}\\
 & Detected & Ground-truth  \\
 \midrule
SlowFast & 23.80 & -  \\
MViT-B & 24.50 & - \\
\ours (contrastive only) & 21.95 & 26.55 \\
% \ours & 16-frame & \bf 28.5 & \bf 53.5 & \bf 64.0 & \bf 93.1 & \bf 96.0 \\
\ours & \bf 25.00 & \bf 30.15  \\
		\bottomrule
		\end{tabular}
  }
  % }

% 	\vspace{-3em}
	\label{tab:ava}
\end{center}
\end{table}
We can observe that \ours outperforms other models and the variant with contrastive loss only when either detected boxes or ground-truth boxes are used for evaluation. This experiment demonstrates the effectiveness of the spatiotemporal learning design of our method.

\section{Visualization}
\label{appendix:visual}
% \subsection{Spatiotemporal Grounding}
We present the visualization of spatial grounding in Figure \ref{fig:ground-visual}. For each example, we choose the group token which has the maximum similarity score of the target noun phrase, and compute the attention heatmap based on corresponding video tokens assigned to that group token. It can be observed that the alignment between the region and the noun phrase has been learned during the pre-training stage without any fine-grained annotations. In addition, more comparisons between similarity scores of baseline features and temporal-aware features are provided in Figure \ref{fig:t1}. With temporal grouping, features from different scenes are much easier to distinguish.

% \newpage

\begin{figure*}[th] 
    \centering
    \includegraphics[width=0.9\linewidth]{figures/grounding-visual.png}
    \caption{Visualization of spatial grounding. The attention feature map of each example is computed from the corresponding regions assigned to the group token which achieves the highest similarity score with respect to the target noun phrase.}
    \vspace{-0.8em}
    \label{fig:ground-visual}
\end{figure*}
\begin{figure*}[th] 
    \centering
    \includegraphics[width=0.9\linewidth]{figures/temporal-visual.pdf}
    \caption{Visualization of temporal grouping.}
    \vspace{-0.8em}
    \label{fig:t1}
\end{figure*}

% \subsection{Temporal Grouping}







