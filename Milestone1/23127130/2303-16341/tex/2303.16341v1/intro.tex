\section{Introduction}
Video-language pre-training with the goal of learning transferable multi-modal representations has recently attracted much attention that finds numerous applications~\cite{akbari2021vatt,bain2021frozen,ge2022bridging,lei2021less,li2020hero,li2022align,luo2020univl,miech2019howto100m,xu2021videoclip}. 
%
Such a model trained on those web-crawled unlabeled noisy data achieves promising performance on various downstream tasks, ranging from single-modal video action recognition to multi-modal text-video retrieval.
%
Video-language pre-training typically follows the pipeline: a) encoding video and text pairs into latent representations; b) modality fusion and interaction; c) pre-training on specific objectives.

Existing methods typically optimize these three components in the pre-training pipeline by designing expressive encoders~\cite{bain2021frozen,li2022align,ge2022bridging,nagrani2022learning,ma2023temporal}, fusing two modalities via a cross-encoder~\cite{ge2022bridging,li2022align,lei2021less,li2020hero,luo2020univl,xu2021vlm,zhu2020actbert}, or adopting a combination of various pre-training tasks such as contrastive learning and masked modeling~\cite{li2022align,ge2022bridging,fu2021violet,zellers2022merlot,cao2022locvtp,ge2022miles}.
%
While these modifications benefit the pre-trained model, they lack local discriminative capabilities.
%
Instead, we approach the video-language pre-training task from a different perspective with a focus on local fine-grained information.


\begin{figure}[t] 
    \centering
    \includegraphics[width=\linewidth]{figures/fine-grained.pdf}
    \caption{A video example from the pre-training dataset with scene shift and region-noun correspondences. 
    % We highlight words with the same color as their associated objects. 
    Matrices below show frame-wise similarity scores computed from baseline and temporal-aware features respectively.}
    % {\color{red}(arrow, our features})}
    \vspace{-0.8em}
    \label{fig:motivation}
\end{figure}

It has been shown that most video-language pre-training methods merely perform well on learning holistic representations to match a $\langle \textit{video, caption} \rangle$ pair and neglect fine-grained information (e.g., region-noun correspondences, or scene/action changes along the time in a video)~\cite{akbari2021vatt,bain2021frozen,lei2021less,li2020hero,luo2020univl,miech2019howto100m,xu2021videoclip,nagrani2022learning}.
%
However, such regional or temporal fine-grained information has been demonstrated to play an important role in localization and reasoning tasks~\cite{li2022align,ge2022bridging,zhang2022unsupervised,ma2023temporal,yuan2022contextualized}. 
%
For example, in Figure \ref{fig:motivation}, given a video of a woman applying facial cream, per-frame features from the model pre-trained with only global contrastive loss are relatively hard to distinguish, 
% the model  would output features  
which makes it challenging to find boundaries in tasks like temporal action localization. 
%
Furthermore, region-noun alignment frequently appears in a video and caption pair as shown in Figure \ref{fig:motivation}, where we highlight objects and their associated noun phrases with the same color.
Few methods take such alignment into consideration.

% Learning discriminative spatio-temporal representations is critical in solving video understanding tasks, as such fine-grained representations have demonstrated to be effective in localization and reasoning ~\cite{li2022align,ge2022bridging,zhang2022unsupervised,ma2023temporal}.
% However, most existing video-launguage pre-training methods only explicitly model holistic multi-modal alignment, which matches a video to its text descriptions and neglects rich fine-grained information residing in video-text pairs~\cite{akbari2021vatt,bain2021frozen,lei2021less,li2020hero,luo2020univl,miech2019howto100m,xu2021videoclip,nagrani2022learning}.
% As shown in Figure \ref{fig:motivation}, given a video of a woman applying facial cleanser and its descriptive caption, noun phrases like \textit{woman, bottle and facial cream} in the caption could be visually grounded in the corresponding frames.
% These visual entities form the basics when human watch and understand the video, while few existing methods take advantage of this fine-grained alignment during their pre-training stage.
% Furthermore, the model pre-trained with global contrastive objective would yield highly similar features for each frame which makes it challenging to capture scene changes in this video.
% % boundaries in tasks like temporal action localization. 

% In this work, we incorporate fine-grained video-caption interactions into the pre-training stage and propose a novel framework, \textbf{Vi}deo-\textbf{L}anguage \textbf{M}odeling with \textbf{G}rouping and \textbf{G}rounding, named \ours.
In this work, we incorporate fine-grained video-caption interactions into the pre-training stage and propose a novel framework,  Text-\textbf{G}rounded \textbf{Vi}deo-\textbf{L}anguage \textbf{M}odeling, named \ours.
\ours\ encourages instance-level video-caption  alignment, fine-grained  region-noun  alignment, and temporal-aware video representations simultaneously.
%
As Figure \ref{fig:framework} shows, \ours consists of three primary training objectives: spatiotemporal grounding, temporal grouping, and global contrastive learning. 
%
With the video-caption pair as the input, a classical dual-encoder is leveraged to extract the representation for each modality respectively. 
%
Note that videos are pre-processed with the cut-and-paste operation~\cite{zhang2022unsupervised,yun2019cutmix}, i.e., randomly selecting a clip in one video as the foreground and pasting it onto the other background video, to explicitly introduce temporal scene changes, as shown in Figure \ref{fig:framework}.
%
We first adopt the structure of grouping blocks~\cite{xu2022groupvit,yu2022k} to aggregate semantically similar video patches to represent objects without off-the-shelf detectors. 
%
In spatiotemporal grounding, we align grouped video tokens with noun concepts extracted from the caption via our designed grounding loss. 
%
Furthermore, we design a temporal grouping objective to capture temporal information by distinguishing foreground and background representations. 
%
Finally, the model is trained by a global video-caption contrastive loss to match instance-level video-caption pairs.

Our key contributions are summarized as follows:
\begin{compactitem}
% \begin{itemize}
    % \item We propose an efficient dual-encoder pre-training framework \ours, which makes use of fine-grained information without extra fusion or interaction. 
    % Two novel designs are introduced: temporal grouping aims at learning temporal-aware features while spatial-temporal grounding facilitates alignment between regions and noun concepts.
    \item We design a spatiotemporal grounding module to capture fine-grained correspondences by aligning nouns from the caption and regions from the video in a self-supervised manner.
    %
    \item We leverage a cut-and-paste operation to introduce temporal scene changes into videos during pre-training, and propose the temporal grouping module to learn more temporal-aware features.
    % \item In addition to global contrastive learning, we introduce two novel pre-training designs, temporal grouping and spatial-temporal grounding. Temporal grouping aims at learning temporal-aware features while spatial-temporal grounding facilitates alignment between regions and noun concepts.
    \item \ours is evaluated comprehensively on four representative downstream tasks, including text-video retrieval, video question answering, video action recognition and temporal action localization. 
    %
    \item Experimental results have shown the effectiveness of \ours and in particular, it outperforms SOTA by 9\% in R@10 in zero-shot text-video retrieval and performs competitive on other downstream tasks.
    % \item We show that the intrinsic properties of pre-training datasets play an important role in performance of downstream tasks: less noisy datasets such as VideoCC~\cite{bain2021frozen} benefits retrieval which requires strong vision-language alignment and more action-related datasets like HowTo100M~\cite{miech2019howto100m} contributes to learning temporal-aware features.
% Text-video retrieval: Improve by 9\% in R@10 in zero-shot performance. Video question answering: Achieve better or similar performance with less training data. Video action recognition: SOTA linear evaluation performance on UCF and HMDB. Temporal action localization: Outperform unsupervised or supervised baselines
\end{compactitem}
% \end{itemize}