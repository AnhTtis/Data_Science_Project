% \clearpage
\section{Experiments}
\begin{table*}\centering
	\scalebox{0.9}{
		\begin{tabular}{c|ccc|cccc}
			\toprule[1pt]
			\multirow{1}*{\textbf{Method}}& \textbf{Video Encoder Input} &\textbf{PT Dataset}& \bf $\#$Pairs PT & \bf R@1& \bf R@5& \bf R@10& \bf MedR\\
			\midrule
			\multirow{1}*ActBERT~\cite{zhu2020actbert}&ResNet-3D&HowTo100M&120M&8.6&23.4&33.1&36.0\\
			\multirow{1}*{MMV~\cite{alayrac2020self}}&Raw Videos&HowTo100M, AudioSet&138M&9.3&23.0&31.1&38.0\\
			\multirow{1}*{MIL-NCE~\cite{miech2020end}}&Raw Videos&HowTo100M&120M&9.9&24.0&32.4&29.6\\
			\multirow{1}*{VATT~\cite{akbari2021vatt}}  &Raw Videos&HowTo100M, AudioSet&138M&-&-&29.7&49.0\\
			\multirow{1}*{NoiseEst~\cite{amrani2021noise}}  &ResNeXt-101&HowTo100M&110M&8.0&21.3&29.3&33.0\\
			\multirow{1}*{TACo~\cite{yang2021taco}}  &I3D, S3D&HowTo100M&120M&9.8&25.0&33.4&29.0\\
			\multirow{1}*{VideoCLIP~\cite{xu2021videoclip}}  &S3D&HowTo100M&110M&10.4&22.2&30.0&-\\
			\multirow{1}*{MCN~\cite{chen2021multimodal}}  &ResNeXt-101&HowTo100M&120M&10.5&25.2&33.8&-\\	
			\multirow{1}*{SupportSet~\cite{patrick2020support}}  &R(2+1)D-34&HowTo100M&120M&12.7&27.5&36.2&24.0\\
			\multirow{1}*{Frozen~\cite{bain2021frozen}}  &Raw Videos&CC3M, WebVid-2M&5.5M&18.7&39.5&51.6&10.0\\
			\multirow{1}*{AVLnet~\cite{rouditchenko2020avlnet}}  &ResNeXt-101&HowTo100M&120M&19.6&40.8&50.7&9.0\\
   DemoVLP~\cite{cai2022revitalize} & Raw Videos & CC3M, WebVid-2M & 5.5M & 24.0 & 44.0 &52.6 & -\\
   ALPRO~\cite{li2022align} & Raw Videos & CC3M, WebVid-2M & 5.5M & 24.1 & 44.7 & 55.4 & 8.0 \\
			\multirow{1}*{MCQ~\cite{ge2022bridging}}  &Raw Videos&CC3M, WebVid-2M&5.5M&26.0&46.4&56.4&7.0\\
			\multirow{1}*{\bf\ours}&Raw Videos&VideoCC, ActivityNet& 3.3M &\textbf{28.6}&\textbf{53.6}&\textbf{65.1}&\textbf{5.0}\\
			\bottomrule[1pt]
	\end{tabular}
	}
	\vspace{2pt}
	\caption{Zero-shot performance of text-video retrieval on MSR-VTT test set with 1K videos, where \textbf{higher} R@k and \textbf{lower} MedR (Median Rank) indicate better performance.} 
% 	\vspace{-1em}
	\label{tab:msrvtt}
\end{table*}

\begin{table*}\centering
	\scalebox{0.9}{
		\begin{tabular}{c|ccc|cccc}
			\toprule[1pt]
			\multirow{1}*{\textbf{Method}}& \textbf{Video Encoder Input} &\textbf{PT Dataset}& \bf $\#$Pairs PT & \bf R@1& \bf R@5& \bf R@10& \bf MedR\\
			\midrule
			\multirow{1}*{ActBERT~\cite{zhu2020actbert}} &ResNet-3D&HowTo100M&120M&16.3&42.8&56.9&10.0\\
			\multirow{1}*{UniVL~\cite{luo2020univl}} &S3D&HowTo100M&110M&21.2&49.6&63.1&6.0\\
			\multirow{1}*{MMT~\cite{gabeur2020multi}} &S3D&HowTo100M&120M&26.6&57.1&69.6&4.0\\
			\multirow{1}*{HERO~\cite{li2020hero}}  &SlowFast&TV and HowTo100M&120M&16.8&43.4&57.7&-\\	
			\multirow{1}*{NoiseEst~\cite{amrani2021noise}}  &ResNeXt-101&HowTo100M&110M&17.4&41.6&53.6&8.0\\
			\multirow{1}*{ClipBert~\cite{lei2021less}}  &Raw Videos&COCO, VisGenome&5.6M&22.0&46.8&59.9&6.0\\
			\multirow{1}*{AVLnet~\cite{rouditchenko2020avlnet}}  &ResNeXt-101&HowTo100M&120M&27.1&55.6&66.6&4.0\\
			\multirow{1}*{VLM~\cite{xu2021vlm}}  &S3D&HowTo100M&110M&28.1&55.5&67.4&4.0\\
			\multirow{1}*{TACo~\cite{yang2021taco}}  &I3D, S3D&HowTo100M&120M&28.4&57.8&71.2&4.0\\
			\multirow{1}*{SupportSet~\cite{patrick2020support}}  &R(2+1)D-34&HowTo100M&120M&30.1&58.5&69.3&3.0\\	
			\multirow{1}*{VideoCLIP~\cite{xu2021videoclip}}  &S3D&HowTo100M&110M&30.9&55.4&66.8&-\\
			\multirow{1}*{Frozen~\cite{bain2021frozen}}  &Raw Videos&CC3M, WebVid-2M&5.5M&31.0&59.5&70.5&3.0\\
   DemoVLP~\cite{cai2022revitalize} & Raw Videos & CC3M, WebVid-2M & 5.5M & 24.0 & 44.0 &52.6 & -\\
   ALPRO~\cite{li2022align} & Raw Videos & CC3M, WebVid-2M & 5.5M & 24.1 & 44.7 & 55.4 & 8.0 \\
			\multirow{1}*{MCQ~\cite{ge2022bridging}}  &Raw Videos&CC3M,  WebVid-2M&5.5M&37.6&{64.8}&{75.1}&3.0\\	
   			\multirow{1}*{\bf\ours}&Raw Videos&VideoCC, ActivityNet& 3.3M &\textbf{38.4}&\textbf{65.7}&\textbf{76.3}&\textbf{2.0}\\
			\bottomrule[1pt]
	\end{tabular}}
	\vspace{2pt}
	\caption{Fine-tuning performance of text-video retrieval on MSR-VTT test set with 1K videos, where \textbf{higher} R@k and \textbf{lower} MedR (Median Rank) indicate better performance.} 
	\vspace{-1em}
	\label{tab:msrvtt-ft}
\end{table*}

We conduct comprehensive evaluations of \ours against the state-of-the-art methods. 
%
First, we introduce the pre-training datasets used in our method and four selected downstream tasks. 
%
Next, the implementation details of both pre-training and fine-tuning procedures are presented. 
%
We compare the performance of \ours with existing methods and demonstrate the effectiveness of incorporating fine-grained information.
%
In addition, we present ablation results on choices of pre-training datasets and training objectives.

\subsection{Pre-training Datasets}
\label{sec:pre}
We pre-train \ours\ with VideoCC~\cite{nagrani2022learning} dataset, which contains about 3.3M video-caption pairs. Specifically, VideoCC is mined online using the Conceptual Captions~\cite{sharma2018conceptual} as a seed dataset, and has shown to be more effective in retrieval and captioning tasks than commonly-adopted datasets such as HowTo100M~\cite{miech2019howto100m} and WebVid-2M~\cite{bain2021frozen}.
%
In addition, we include ActivityNet-Caption~\cite{krishna2017dense} with 20K well-aligned pairs into the pre-training corpus. 
Note that for temporal action localization, the model is pre-trained on HowTo100M only, which is observed to benefit to TAL compared with VideoCC + ActivityNet.

\subsection{Downstream Tasks}
\label{sec:downstream}
\noindent\textbf{Text-Video Retrieval.} We adopt the widely used text-video retrieval benchmark MSR-VTT~\cite{xu2016msr} for evaluation. 
%
It consists of 10K YouTube video clips with 200K captions. 
%
Similar to existing methods~\cite{bain2021frozen, ge2022bridging}, we train and test the model on the split of 9K and 1K videos.

\noindent\textbf{Video Question Answering~(VQA).} We consider open-ended VQA settings with two representative datasets: 1) MSRVTT-QA~\cite{xu2017video} with 1500 answer candidates; 2) MSVD-QA~\cite{xu2017video} with 2423 answer candidates. 
To comply with the data policy, the size of these two datasets that we are using are smaller than the original ones and detailed statistics could be found in Table~\ref{tab:stats}.

\noindent\textbf{Video Action Recognition.} We select HMDB51~\cite{kuehne2011hmdb} with 6,766 videos from 51 categories and UCF101~\cite{soomro2012ucf101} with 13,320 videos from 101 categories. Both linear probing and fine-tuning the whole model are explored.

\noindent\textbf{Temporal Action Localization~(TAL).} TAL aims for predicting the temporal extent and the  class labels of action instances. We evaluate the performance on ActivityNet~\cite{heilbron2015activitynet}, an action understanding dataset of 19,994 temporally  annotated  untrimmed  videos  with  200  action categories.

\subsection{Implementation Details}
\label{sec:implement}
\noindent\textbf{Input.} We sample 32 frames for each video and resize them into $224\times224$ as input with the same augmentations in \cite{nagrani2022learning}.  Each caption is tokenized into 32 tokens including $\texttt{[CLS]}$ during training. $K=2$ noun phrases are extracted for each caption and then prompted with a set of manually designed templates such as \textit{``It is a video of \{noun\}''}.

\noindent\textbf{Model architecture.} We use a 12-layer ViT-base model with the patch size of $2\times16\times16$ as the video encoder and initialize it with weights pre-trained on Kinetics-400.
We adopt 32 learnable group tokens and 3 grouping blocks featuring K-means attention~\cite{xu2022groupvit,yu2022k}.
Grouping blocks are inserted at the 6th, 9th and last layers of the video encoder, following GroupViT~\cite{xu2022groupvit} and kMaX-DeepLab~\cite{yu2022k}. 
The text encoder is initialized from the pre-trained BERT-base model.
% with uncased wordpiece tokenization. 
All representations are projected into the common space with the dimension of 256.

\noindent\textbf{Pre-training and fine-tuning setups.} 
We implement \ours\ in JAX and train all models on TPU accelerators. During pre-training, a synchronous SGD with momentum 0.9 and initial learning rate 0.1 is used for optimization. 
We train \ours for 10 epochs with a batch size 1024 and adopt a cosine learning rate decay schedule with a warmup ratio 0.05. It takes about one day for the whole pre-training stage.
We use the same text templates as in \cite{li2022align} to generate text prompts. 
%
In terms of fine-tuning, different tasks are trained independently with their own set of hyperparameters on the target dataset and more details can be found in Appendix B. 
%
For temporal action localization, we fix weights of the pre-trained video encoder and its grouping blocks to extract video features, which are then evaluated by G-TAD~\cite{xu2020g}, a commonly used method for TAL.
% \vspace{-1em}
\subsection{Evaluation Results}
\label{sec:results}
\subsubsection{Text-Video Retrieval}
We evaluate \ours for the task of text-video retrieval on MSR-VTT under both zero-shot and fine-tuning settings, and present detailed results in Table \ref{tab:msrvtt} and \ref{tab:msrvtt-ft}. \ours outperform other methods significantly for zero-shot evaluation with R@10 of 65.1, yielding approximately 9\% improvement over the best-performing baseline MCQ. 
The superior results demonstrate that our pre-trained model builds up a good alignment between video and language and has great generalization to unseen datasets. 
%
\ours also achieves performance gain when the model is fine-tuned on the target MSR-VTT dataset, which further validates advantages of the pre-trained model.
%
Note that \ours performs favorably against existing methods despite the much smaller size of the pre-training data used in \ours than those in baselines, such as HowTo100M and WebVid-2M. 
%
These results are consistent with the findings in \cite{nagrani2022learning} and demonstrate the importance of high-quality video-caption pairs in retrieval tasks.
%
\ours leverages a dual-encoder design and does not include a fusion encoder during the pre-training stage which saves much computation cost. 
On the other hand, retrieval can be achieved efficiently by computing dot-product between video and caption features without feeding every combination into the fusion model, compared with models such as ClipBERT~\cite{lei2021less}.
% xxxxx compared with previous methods with a joint video-text fusion encoder, which xxxx.

% We begin by analysing the re-sults with fine-tuning for text-video retrieval on the MSR-VTT dataset, presented in Table 2.  We note that pretrain-ing on VideoCC3M provides a significant boost to perfor-mance  over  HowTo100M,  with  far  less  data,  and  for  anRGB-only model, yields a 5\% improvement over trainingfrom scratch on R@1.  This effect is even more profoundin the zero-shot case,  where for an RGB-only model,  us-ing VideoCC3M more than doubles the R@1 performancecompared to HowTo100M pretraining.   This is done with100x fewer captions and 20x less video data.  We believethat this shows the value in high-quality video-captioningpairs. HowTo100M,  with  far  less  data,  and  for  anRGB-only model, yields a 5\% improvement over trainingfrom scratch on R@1.  This effect is even more profoundin the zero-shot case,  where for an RGB-only model,  us-ing VideoCC3M more than doubles the R@1 performancecompared to HowTo100M pretraining.   This is done with100x fewer captions and 20x less video data.  We believethat this shows the value in high-quality video-captioningpairs.

% \vspace{-1em}
\subsubsection{Video Question Answering}
VQA results on two open-ended datasets MSRVTT-QA and MSVD-QA are shown in Table \ref{tab:qa}. 
To enable \ours to deal with the VQA task, we add a fusion head adapted from BUTD~\cite{anderson2018bottom} by integrating video and text features with simple linear layers. Then a classifier is inserted after the fusion module to perform question answering as a classification problem.
Compared with previous methods which leverage particular architectures for VQA or include a complicated fusion encoder, \ours is the most efficient and flexible for various vision-language tasks. 
Meanwhile, \ours achieves on-par or even better performance with selected baselines, with the accuracy of 43.5\% (1.4\% lift) and 45.2\% (0.7\% drop) on MSRVTT-QA and MSVD-QA respectively. 
Note that due to data restrictions shown in Table \ref{tab:stats}, the training sets are not complete as original ones and we believe the performance of our method can be further improved if trained on more VQA pairs.
\begin{table}[!t]
\begin{center}
    \scalebox{0.8}{
% \resizebox{0.48\textwidth}{!}{
	\centering	
		\begin{tabular}	{c | c |  c c }
			\toprule
			\textbf{Method}  & \textbf{PT Dataset} & \textbf{MSRVTT-QA}     & \textbf{MSVD-QA} \\
			\midrule
	E-SA~\cite{xu2017video} & - & 29.3 & 27.6 \\
			ST-TP~\cite{jang2017tgif} & - & 30.9 & 31.3 \\	
	AMU~\cite{xu2017video} & - & 32.5 & 32.0 \\
		Co-mem~\cite{gao2018motion} & -& 32.0 & 31.7 \\
		HME~\cite{fan2019heterogeneous} & - & 33.0 & 33.7 \\
		LAGCN~\cite{huang2020location} & - & - & 34.3 \\
		HGA~\cite{jiang2020reasoning} & - & 35.5 & 34.7 \\
		QUEST~\cite{jiang2020divide} & - & 34.6 & 36.1 \\	
		HCRN~\cite{le2020hierarchical} & - & 35.6 & 36.1 \\
		ClipBERT~\cite{lei2021less} & COCO, VG & 37.4 & - \\
		SSML~\cite{amrani2021noise}& HowTo100M & 35.1 & 35.1 \\
	CoMVT~\cite{seo2021look} & HowTo100M & 39.5 & 42.6 \\
 DemoVLP~\cite{cai2022revitalize} & CC3M, WebVid-2M & 38.3 & 39.5 \\
		ALPRO~\cite{li2022align} & CC3M, WebVid-2M & 42.1 & \textbf{45.9} \\
  \bf\ours & VideoCC, ActivityNet & \textbf{43.5} & 45.2 \\
			\bottomrule
		\end{tabular}
  }
  % }
    \vspace{2pt}
    \caption
	{
	Experiment results on Video Question Answering on MSRVTT-QA and MSVD-QA datasets in top-1 accuracy (\%).
	}
	\label{tab:qa}
% \vspace{-1.5em}
    \vspace{-5pt}
\end{center}
\end{table}

\begin{table}[!t]
\begin{center}
    \scalebox{0.9}{
\centering	
\begin{tabular}	{c|cc}
\toprule
\bf Method  &  \textbf{MSRVTT-QA}     & \textbf{MSVD-QA} \\
\midrule
Original & 158,581 & 30,933\\
Ours  & 117,210 & 25,115\\ 
\midrule
Reduction ratio & 26.1\% & 18.8\%  \\
\bottomrule
\end{tabular}
  }
  % }
    % \vspace{-5pt}
    \vspace{2pt}
    \caption
	{Statistics of training examples in MSRVTT-QA and MSVD-QA. 
	Due to the data policy, we use reduced number of QA pairs in our experiments.
	}
	\label{tab:stats}
	\vspace{-5pt}
\end{center}
\end{table}

\subsubsection{Video Action Recognition}
For video action recognition, we only keep the video encoder together with its grouping blocks to extract single-modality video representations for evaluation. 
Two evaluation settings are considered: (1) linear probing where the backbone encoder is frozen and only the last linear classifier is trained and (2) end-to-end fine-tuning where both the backbone and the classifier are trained. 
Top-1 accuracy on UCF101 and HMDB51 is reported in Table \ref{tab:action}. 
We can observe that in linear probing, \ours performs well against the other methods, with 3.0\% and 2.9\% higher than current SOTA, 
%
MMV with audio and text on UCF101 and HMDB51.
\ours also achieves consistently superior performance under fine-tuning evaluation. 
Outstanding performance of \ours demonstrates that learning the alignment between videos and captions with fine-grained information contributes to meaningful video representations for tasks involved in a single modality.
% For video action recognition, we only keep the video encoder together with its grouping blocks to extract single-modality video representations for evaluation. Two evaluation settings are considered: (1) linear probing where the backbone encoder is frozen and only the last linear classifier is trainable and (2) fully fine-tuning where both the backbone and the classifier are trained.
% We  further  evaluate  thesingle-modality  video  repre-sentationsof our model via action recognition with linearand fully fine-tuning evaluation as shown in Table. 5, wherethe representations from VideoFormer are extracted as theinput of a trainable linear classifier.  Our method achieveshigher accuracy than some previous work that pre-train theirmodel on datasets with considerably longer video time (e.g.14longer  in  XDC  [3],  10longer  in  MIL-NCE  [28]and VATT [1]), showing the effectiveness of our method inlearning transferable video representations for action recog-nition.  Despite MMV [2] performs better than our methodwhen  pre-training  on  datasets  11longer  than  ours  withmultiple modalities including audio and text besides video,its performance lags far behind ours when only audio andvideo or text and video are used. We can conclude that ourmethod utilizes the language modality more efficiently tolearn stronger video representations with fewer video hours.
\begin{table}\centering
	\scalebox{0.9}{
		\begin{tabular}{c|c|cc|cc}
			\toprule
\multirow{2}*{\textbf{Method}} & \multirow{2}*{\textbf{Modal}} & \multicolumn{2}{c|}{\textbf{UCF101}} & \multicolumn{2}{c}{\textbf{HMDB51}} \\
			&&Lin&FT &Lin&FT \\
			\midrule
			\multirow{1}*{CCL~\cite{kong2020cycle}}&-&54.0&69.4 & 29.5 & 37.8\\
			\multirow{1}*{CBT~\cite{sun2019learning}}&-&54.0&79.5 & 29.5 & 44.5\\
			\multirow{1}*{MemDPC~\cite{han2020memory}}&OF&54.1&86.1 & 30.5 & 54.5 \\
			\multirow{1}*{CoCLR~\cite{han2020self}}&OF&77.8&90.6 & 52.4 & 62.9\\
			\multirow{1}*{MVCGC~\cite{huo2021compressed}}&MV&78.0&90.8 & 53.0 & 63.4\\
			\multirow{1}*{XDC$\_$R~\cite{alwassel2020self}}&A&80.7&88.8 & 49.9 & 61.2\\
			\multirow{1}*{XDC$\_$K~\cite{alwassel2020self}}&A&85.3&91.5 & 56.0 & 63.1\\
			\multirow{1}*{MIL-NCE~\cite{miech2020end}}&T&83.4&89.1 & 54.8 & 59.2\\
			\multirow{1}*{Frozen~\cite{bain2021frozen}}&T&87.8&89.8 & 61.3 & 66.3\\	
			\multirow{1}*{VATT~\cite{akbari2021vatt}}&A, T&89.2&- & 63.3 & -\\
			\multirow{1}*{ELO~\cite{piergiovanni2020evolving}}&A, OF&-&93.8 & 64.5 & 67.4\\
			\multirow{1}*{MMV~\cite{alayrac2020self}}&A&77.1&- & 53.6 & - \\
			\multirow{1}*{MMV~\cite{alayrac2020self}}&T&86.8&- & 55.1 & -\\
			\multirow{1}*{MMV~\cite{alayrac2020self}}&A, T&91.8&95.2 & 67.1 & \textbf{75.0} \\
			\multirow{1}*{MCQ~\cite{ge2022bridging}}&T&89.1&92.3 & 65.8 & 69.8\\
   \bf\ours & T & \textbf{94.8} & \textbf{96.5} & \textbf{70.0} & 73.9 \\
			\bottomrule[1pt]
	\end{tabular}
 }
    \vspace{2pt}
 	\caption{Experiments of action recognition on UCF101 and HMDB51 with linear evaluation (Lin) and fully fine-tuning evaluation (FT). ``Modal'' denotes the modality used for pre-training in addition to videos, \textit{i.e.}, optical flow (OF), motion vector (MV), audio (A), text (T). } 
	\vspace{-5pt}
% 	\vspace{-10pt}
	\label{tab:action}
\end{table}


% It should be emphasized that 
% including fully supervised approaches such as LoFi and BSP.
% We report mean Average Precision (mAP) values under different temporal Intersection over Union (tIoU) thresholds on ActivityNet in Table \ref{tab:tad}. As mentioned in Section , we directly use their released pre-trained models to extract the video features. 
\begin{table*}[ht]
\begin{center}
    \scalebox{0.9}{
% \resizebox{0.48\textwidth}{!}{
	\centering	
\begin{tabular}	{c|ccc|ccc|c|cccc}
			\toprule
% \textbf{Scenario}  & $\gL_c$  & $\gL_g$     & $\gL_t$ & xxx \\
\multirow{2}*{\textbf{Scenario}} & \multirow{2}*{$\gL_\text{contrast}$} & \multirow{2}*{$\gL_\text{g}$} & \multirow{2}*{$\gL_\text{t}$} & \multicolumn{3}{c|}{\textbf{MSRVTT-ZS}} & \textbf{MSVD-QA} & \multicolumn{4}{c}{\textbf{TAL}}\\
 & & & & R@1 & R@5 & R@10 & Acc & mAP@0.5 & 0.75 & 0.95 & Avg \\
 \midrule
 1 & \checkmark & \checkmark & \checkmark & 24.7 & 47.4 & 59.0 & 44.9 &  50.5 & 35.0 & 9.2 & 34.2\\
 2 & \checkmark & \checkmark & & 23.3 & 46.6 & 58.6 & 44.1 & 50.2 &  34.7 & 8.7 &  34.0\\
 3 & \checkmark & & \checkmark & 24.2 & 46.7 & 58.2 & 43.9 & 50.1 & 34.6 & 8.8 & 34.0\\
 4 & \checkmark & & & 22.7 & 45.9 & 57.0 & 43.6 & 49.9 & 34.3 & 8.7 &  33.7  \\
			% \midrule
			\bottomrule
		\end{tabular}
%   }
   }
    \vspace{2pt}
    \caption
	{Ablation study on training objective.
	We validate that our proposed spatiotemporal grounding loss and temporal grouping loss both benefit downstream tasks.
	}
	\label{tab:objective}
\vspace{-15pt}
\end{center}
\end{table*}



\begin{table}[t]
\begin{center}
\scalebox{0.9}{
\begin{tabular}{c|cccc}
\toprule
\multirow{2}*{\textbf{Method}}  & \multicolumn{4}{c}{\textbf{TAL Task (G-TAD)}} \\
% \cmidrule(lr){2-5}
& mAP@0.5 & @0.75 & @0.95 & Avg \\
\midrule
CoCLR~\cite{han2020self}  & 47.9 & 32.3 & 7.3 & 31.9 \\
XDC~\cite{alwassel2020self} &  48.4 & 32.6 & 7.6 & 32.3 \\
MoCo-v2~\cite{chen2020improved} &  46.6 & 30.7 & 6.3 & 30.3 \\
VideoMoCo~\cite{pan2021videomoco} &  47.8 & 32.1 & 7.0 & 31.7 \\
RSPNet~\cite{chen2021rspnet} & 47.1 & 31.2 & 7.1 & 30.9 \\
AoT~\cite{wei2018learning} & 44.1 & 28.9 & 5.9 & 28.8 \\
SpeedNet~\cite{benaim2020speednet}  & 44.5 & 29.5 & 6.1 & 29.4 \\
PAL~\cite{zhang2022unsupervised}  & 50.7 & 35.5 & 8.7 & 34.6 \\
TAC~\cite{xu2020g}  & 48.5 & 32.9 & 7.2 & 32.5 \\
BSP~\cite{xu2021boundary}  & 50.9 & 35.6 & 8.0 & 34.8 \\
LoFi-E2E~\cite{xu2021low} & 50.4 & 35.4 & 8.9 & 34.4 \\
TSP~\cite{alwassel2021tsp}  & 51.3 & \bf 37.1 & 9.3 & \bf 35.8 \\
\bf\ours & \bf 51.7 & 36.4 & \bf 9.7 & 35.6 \\
\bottomrule
\end{tabular}
}
\vspace{3pt}
\caption{Comparison to state-of-the-art pre-training methods on temporal action localization~(\textbf{TAL}).}
\label{tab:tad}
\vspace{-5pt}
\end{center}
\end{table}
\subsubsection{Temporal Action Localization}
We report mean Average Precision (mAP) values under different temporal Intersection over Union (tIoU) thresholds on ActivityNet in Table \ref{tab:tad}.
As mentioned in Section \ref{sec:implement}, we directly use pre-trained models to extract the video features as the input to G-TAD and do not further train the encoder. \ours consistently exceeds other self-supervised competitors and even fully supervised approaches such as LoFi~\cite{xu2021low} and BSP~\cite{xu2021boundary} in three tIoU thresholds. This observation again consolidates the conclusion that vision-language pre-training can not only be applied to specific VL problems like text-video retrieval and VQA, but also benefit single-modal downstream tasks.

\subsection{Ablation Study}
\label{sec:ablation}
We analyze the effects of the choice of pre-training datasets  and different combinations of three pre-training objectives of \ours in this section.

%\subsubsection{Pre-training dataset}
\noindent \textbf{Pre-training datasets.}
\label{sec:dataset}
\begin{table}[t]
\begin{center}
    \scalebox{0.7}{
% \resizebox{0.48\textwidth}{!}{
	\centering	
\begin{tabular}	{c|ccc|cccc}
			\toprule
% \textbf{Scenario}  & $\gL_c$  & $\gL_g$     & $\gL_t$ & xxx \\
\multirow{2}*{\textbf{PT Dataset}}  & \multicolumn{3}{c|}{\textbf{MSRVTT-ZS}}  & \multicolumn{4}{c}{\textbf{TAL}}\\
 &  R@1 & R@5 & R@10  & mAP@0.5 & 0.75 & 0.95 & Avg \\
 \midrule
HowTo100M & 9.4 & 22.9 & 31.3 & 51.7 & 36.4 & 9.7 & 35.6 \\
ActivityNet & 14.4 & 33.5 & 44.0 & 50.5 & 35.3 & 8.7 & 34.5\\
VideoCC & 24.7 & 47.4 & 59.0 & 50.5 & 35.0 & 9.2 & 34.2 \\
VideoCC, ActivityNet & 28.6 & 53.6 & 65.1 & 50.8 & 35.6 & 9.3 & 34.7 \\
			\bottomrule
		\end{tabular}
  }
  % }
    \vspace{2pt}
    \caption{Ablation study on pre-training datasets. We observe that the combination of VideoCC and ActivityNet yields the best performance on retrieval task, while using HowTo100M achieves the best results on temporal action localization.}
	\label{tab:pt}
	\vspace{-10pt}
\end{center}
\end{table}
To analyze of effect of the pre-training datasets, we evaluate the performance on selected downstream tasks and present detailed results in Table \ref{tab:pt}. 
%
Four choices of PT datasets are considered, including HowTo100M, ActivityNet, VideoCC, and VideoCC + ActivityNet. %
%MH: this sentence does not make much sense. check the revised sentence 
%It can be found that HowTo100M performs poorly in zero-shot text-video retrieval on MSR-VTT while the combination of VideoCC and ActivityNet leads to best performance.
%
\ours performs performs poorly in the zero-shot text-video settings on MSR-VTT when pre-trained on Howto100M. 
%
On the other hand, \ours performs best on the same task when pre-trained with the combination of VideoCC and ActivityNet. 
%
These empirical results coincide with the findings in~\cite{nagrani2022learning}, in which HowTo100M has been pointed out not appropriate for such vision-language tasks requiring strong alignment. 
It is also worth noting that pre-training on ActivityNet and VideoCC performs consistently better than using only one dataset, which validates our choice for PT dataset. 
%
On the other hand, HowTo100M contains a large number of action-related videos, which contributes to learning temporal-aware features and demonstrates better capacities in temporal action localization and thus is used for all TAL experiments.

% We evaluate the performance of models pre-trained on different datasets  selected downstream tasks.

%\subsubsection{Training objective}
%\label{sec:objective}
\noindent \textbf{Training objective.}
We analyze the role of our proposed training objectives and present results of this ablation study in Table \ref{tab:objective}.
%
We evaluate four scenarios and find that in video-language tasks like zero-shot text-video retrieval and VQA, both temporal grouping loss $\gL_\text{t}$ and spatiotemporal grounding loss $\gL_\text{g}$ contribute to performance gain. And the combination of $\gL_\text{t}$ and $\gL_\text{g}$ can further improve the performance to 28.6 for R@1 on MSRVTT-ZS and 45.2 accuracy on MSVD-QA. 
%
For the temporal action localization task, $\gL_\text{g}$ would not 
significant achieve performance gain in mAP compared to scenario 2 and 4. 
%
We hypothesize that HowTo100M is a noisy dataset with weakly-aligned video-caption pairs and grounding in a self-supervised manner might not benefit representation learning on such data. 