\section{Related Work}
\noindent\textbf{Video-language pre-training.} Video-language pre-training is an emerging research area that aims to develop machine learning models capable of jointly understanding visual and textual content.
Representations learned from large scale noisy datasets such as HowTo100M~\cite{miech2019howto100m}, WebVid~\cite{bain2021frozen} and VideoCC~\cite{nagrani2022learning} have demonstrated great potentials in adapting to downstream tasks, including but not limited to text-video retrieval, video question answering and video captioning. 
Elaborately designed pre-training objectives ranging from generative~\cite{chen2020uniter,fu2021violet,li2019visualbert,liu2022ts2} to discriminative~\cite{bain2021frozen,lei2021less,akbari2021vatt, li2022align,ge2022bridging} have been proposed, among which contrastive learning is prevalent and widely adopted to attract paired video-caption instances and repelling unpaired ones. 
Early approaches~\cite{li2020hero,luo2020univl,miech2020end,miech2019howto100m,sun2019videobert,xu2021videoclip,zhu2020actbert} merely leverage offline video features extracted from frozen backbone models, and are less effective in adaptation to various domains. 
Recently, end-to-end training~\cite{lei2021less,bain2021frozen,akbari2021vatt,ge2022bridging,li2022align,ge2022miles,yan2021video,cai2022revitalize} enables video and language features to be learned from raw pixels and captions, respectively. 
For instance, Frozen~\cite{bain2021frozen} adopts a vision transformer as the visual encoder taking both raw images and videos as input and updates the visual and text encoder via contrastive learning.
In addition, some methods~\cite{yan2021video,cai2022revitalize,fu2021violet} attempt to make encoders more expressive by introducing richer information from raw data like region features, adding a multi-modal fusion encoder to facilitate modality interaction, or adopting a combination of contrastive learning and masked modeling. 
However, their primary focus is still on learning holistic global representations to align instance-level $\langle \textit{video, caption}\rangle$ pairs. 

\begin{figure*}[htp]
    \centering
    \includegraphics[width=1\textwidth]{figures/framework-new.pdf}
    \caption{Framework of \ours\ with three training objectives: temporal grouping, spatiotemporal grounding, and global contrastive learning. Cut-and-paste is leveraged to introduce temporal changes manually. The example involves two videos with 8 frames with $s=1$ and $e=6$, leading to the masking as $(0, 1, 1, 1, 1, 1, 1, 0)$. These objectives promote modality interaction from both local and global perspectives: (1) spatiotemporal grounding focuses on local correspondences between regions and nouns; (2) temporal grouping learns temporal-aware features by distinguishing whether clips are from background or foreground; (3) global contrastive learning matches instance-level
    $\langle \textit{video, caption}\rangle$ pairs.
    }
    \label{fig:framework}
    % \vspace{-1em}
\end{figure*}


Recently, some approaches have been proposed to leverage finer-grained information such as nouns/verbs phrases from a caption.
%
ALPRO~\cite{li2022align} extracts pseudo entity labels by feeding noun prompts into a frozen model and use contrastive objective to align cropped visual regions and the corresponding textual labels.
%
In~\cite{ge2022bridging}, MCQ recovers randomly masked noun/verbs tokens via resorting to global video features, which implicitly improves text entity association in visual encoding.
%
Despite these efforts, correspondences between visual regions and noun concepts in captions and temporal scene shifts in a video, is still neglected and not modeled explicitly in existing video-language pre-training methods. 
In this work, we propose two novel designs, spatiotemporal grounding and temporal grouping, to leverage fine-grained information in pre-training stage.
% there has been a surge of interest in video-language pre-training, with researchers proposing various methods for pre-training models on video and language data. These methods range from simple fusion techniques to more complex multimodal architectures based on Transformer models. 
% Despite the promising results achieved by some of these methods, there are still many challenges to overcome in the field of video-language pre-training.


\noindent\textbf{Vision language grounding.} The goal of Visual Grounding (VG) is to locate the most relevant object or region in a visual input based on a natural language query~\cite{fang2015captions,rohrbach2016grounding,fukui2016multimodal,ghiasi2022scaling,gupta2020contrastive}. 
%
Recently, visual grounding has been adapted to a pre-training task in a self-supervised manner for open-vocabulary image segmentation~\cite{ghiasi2022scaling,xu2022groupvit}. 
%
For example, OpenSeg~\cite{ghiasi2022scaling} semantically aligns a caption with extracted image regions via a grounding loss. 
%
Moreover, without the off-the-shelf object detectors, GroupViT~\cite{xu2022groupvit} learns to group together semantic regions from text supervision by contrastive learning. 
%
Note that visual grounding is mostly discussed in the image domain and its success motivates us to extend visual-semantic alignment to video-language pre-training.
%
We integrate a novel spatiotemporal grounding module in our framework to promote visual and textual entity correspondences in a self-supervised manner.
% Recently, visual grounding has been adapted to a pre-training task in a self-supervised manner for open-vocabulary image segmentation. For example, OpenSeg semantically aligns a caption with extracted image regions via a grounding loss while GroupViT .


\noindent\textbf{Video temporal modeling.} In contrast to images, videos contain a sequence of dynamic frames and how to model temporal information is critical in video understanding~\cite{feichtenhofer2019slowfast,bertasius2021space,tran2014c3d,alwassel2021tsp,zhang2022unsupervised,qian2022teg}. 
Specifically, TSP~\cite{alwassel2021tsp} learns temporal information via predicting clips inside or outside the action with substantial annotations while PAL~\cite{zhang2022unsupervised} aligns features of pasted pseudo action regions from two synthetic videos. 
These techniques are elaborately designed for training models on long videos such as movies or TV dramas, which contains natural scene changes.
%
However, few of them have been considered in  video-language pre-training since the majority of the dataset contains short videos which feature repeated frames and are lacking in temporal differences. 
%
In this work, we develop a temporal grouping method to learn temporal-aware clip features in video-language self-supervised learning.
%
We show that features extracted from explicitly temporal modeling achieve significant improvements in not only temporal action localization tasks, but also coarse-grained reasoning and understanding tasks such as video question answering and video action recognition.
