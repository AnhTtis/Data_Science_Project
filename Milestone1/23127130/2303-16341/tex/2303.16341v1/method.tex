\section{Method}
\subsection{Overview}
% In this section, we elaborate on details of our approach. 
The framework of \ours is presented in Figure \ref{fig:framework}. 
We adopt the dual encoder architecture for video-language pre-training, and there are three primary objectives used in the pre-training stage: 1) spatiotemporal grounding, 2) temporal grouping, and 3) global contrastive learning. 

As shown in Figure \ref{fig:framework}, temporal changes are first artificially introduced into training examples through cut-and-paste, and then a set of spatiotemporal video tokens are obtained given the patch size and a linear projection layer.
%
Video tokens are processed in two branches: 
(1) group tokens aggregate semantically similar video tokens via grouping blocks to promote region-noun groundingness; % and are updated based on intermediate outputs from the video encoder at different stages. 
(2) the output tokens from the last layer of the video encoder are utilized in temporal grouping to improve temporal discriminativeness.
In contrast to previous methods where regions are extracted with pre-trained object detectors~\cite{cai2022revitalize,li2022align,yan2021video}, we leverage the learned group tokens to cluster and organize semantically similar regions in a self-supervised manner, which is more effective and reduces the artifacts of any detectors.
%
For the language branch, the original captions are tokenized into a sequence of text tokens, which are then fed into a text encoder to extract the corresponding representation from the preceding \texttt{[CLS]} token. 
Noun tokens are extracted in the same way given a set of noun prompts. 

We model the interaction between region features and noun tokens using spatiotemporal grounding loss.
%
To further promote temporal awareness, we use masks derived from the cut-and-paste operations as the ground-truth for temporal grouping.
%
Finally, a global contrastive loss is computed between the video and the caption representations to match the instance-level $\langle \textit{video, caption}\rangle$ pair.

% The rest of methodology is organized as follows: temporal grouping is presented in Section \ref{sec:temporal-grouping}; Section \ref{sec:grounding} discusses spatial-temporal grounding; Section \ref{sec:global} summarizes the overall pre-training objective.
% encoder at different stages. The final group tokens are regarded as region features instead of extraction with pre-trained object detectors. For the side of language, the original caption are tokenized into a sequence of text tokens, which are fed into the text encoder to extract the corresponding representation from the preceding [CLS] token. Noun tokens are extracted in the same way provided that xxxx a set of noun prompts. To achieve the fine-grainedmulti-modal alignment, we introduce a novel text-video lo-calization pre-training task
% However, the contrastive learning only en-courages the global video-text matching, lacking correspon-dence  between  the  individual  frames  and  words.   On  theone  hand,  the  temporal  modeling  on  video  frames  is  notwell established with coarse text-video contrastive learning.On the other hand, the representations are not well aligned,limiting the performance on downstream extensions, suchas the text-to-video retrieval.   To achieve the fine-grainedmulti-modal alignment, we introduce a novel text-video lo-calization pre-training task

% Video-language pre-training is an emerging research area that aims to develop machine learning models capable of jointly understanding visual and language information.
% Representations learned from large scale noisy datasets such as HowTo100M, WebVid and VideoCC have demonstrated great potentials in different downstream tasks including text-video retrieval, video question answering and the like. Elaborately designed pre-training objectives ranging from generative to discriminative methods have been proposed, among which constrastive learning is prevalent and widely adopted to xxxx. Early approaches merely leverage offline video features extracted from frozen backbone models, and are less effective in adaptation to various domains. Recently, end-to-end training enables video and language features to be learned from raw pixels and captions respectively. For instance, Frozen as visual encoder and can be flexibly trained using both im-age and video datasets due to the sequential nature of trans-former. In addition, some methods attempt to make encoders more expressive by introducing richer information from raw datasets like region features, adding a fusion encoder to facilitate modality interaction, or adopting a combination of contrastive learning and masked modeling. However, they still focus on learning holistic global representations to match instance-level (video, caption) pairs, and neglect fine-grained information such as temporal differences in a video, and correspondences of regions in a video and noun concepts in a caption. In this work, we propose two novel designed to incorporate fine-grained information into pre-training.

% Video-language pre-training is an emerging research area that aims to develop machine learning models capable of jointly understanding visual and language information. Representations learned from large scale noisy datasets such as HowTo100M, WebVid and VideoCC have demonstrated great potentials in different downstream tasks including text-video retrieval, video question answering and the like. Elaborately designed pre-training objectives ranging from generative to discriminative methods have been proposed, among which constrastive learning is prevalent and widely adopted to xxxx. Early approaches merely leverage offline video features extracted from frozen backbone models, and are less effective in adaptation to various domains. Recently, end-to-end training enables video and language features to be learned from raw pixels and captions respectively. For instance, Frozen as visual encoder and can be flexibly trained using both im-age and video datasets due to the sequential nature of trans-former. In addition, some methods attempt to make encoders more expressive by introducing richer information from raw datasets like region features, adding a fusion encoder to facilitate modality interaction, or adopting a combination of contrastive learning and masked modeling. However, they still focus on learning holistic global representations to match instance-level (video, caption) pairs, and neglect fine-grained information such as temporal differences in a video, and correspondences of regions in a video and noun concepts in a caption. In this work, we propose two novel designed to incorporate fine-grained information into pre-training.
\subsection{Grounding with Group Tokens}
\label{sec:grounding}
% In the previous section, grouping is conducted temporal-wise.  
Observing the correspondences between visual regions in a video and noun phrases in a caption, as demonstrated in Figure \ref{fig:motivation}, we model such fine-grained alignment for more expressive encoders. 
%
% We extend temporal grouping to the spatiotemporal domain to facilitate learning fine-grained spatiotemporal representations.
%
% Unlike temporal grouping, 
In practice, it is infeasible to pool tokens of interest as cluster centers since we do not have ground-truth spatiotemporal segmentation. 
%
Thus, we adopt $M$ learnable group tokens to cluster semantic similar regions in a self-supervised manner.
%
Note that group tokens are randomly initialized and shared among different videos.
% Taking the output of the last layer in the video encoder $z_i^v$ as an example, 
% We leverage grouping blocks to merge semantically similar tokens and update group tokens.
% by $\{z_i^{g_m}\}=\text{GroupingBlock}(\{z_i^p\}, \{g_m\})$.
%MH: what is Figure
The detailed structure of a grouping block is presented in Appendix A and multiple grouping blocks are placed at different layers of the video encoder to update group tokens progressively. 
%
% 
Final group tokens denoted as
% $\{h_i^m\}_{m=1}^{M}$
$\mathcal{G} = \{g^m_i\}_{m=1}^{M}$
% $\{f_i^{g_m}\}_{m=1}^{M}$ 
aggregate semantically similar voxels and represent different regions in the video $v_i$.


% To facilitate the training of group tokens, we turn to text supervision. 
% Observing the correspondences between visual regions in a video and noun phrases in a caption, as demonstrated in Figure x, we aims for modeling such fine-grained matching for more powerful encoders. 
%MH: not good to mention package, instead, you should mention how nous are extracted (method). 
We obtain noun tokens as follows: for each caption $c_i$ of a video, we extract $K$ noun phrases using noun chunking in spaCy\footnote{\url{https://spacy.io/}} and  prompt each of them with a set of handcrafted sentence templates, e.g.,
``\textit{A photo of a \{noun\}}''. 
Such prompted noun phrases are fed into the text encoder to extract noun tokens
$\{n_i^{k}\}_{k=1}^K$.
% $\{f_i^{n_k}\}_{k=1}^K$.
% Then group tokens and noun tokens are projected to a common space via two separate linear layers as final res. 
% , $ \{f_i^{g_m}\} = h_v(\{z_i^{g_m}\}), \{f_i^{n_k}\} = h_c(\{z_i^{n_k}\})$.

% \end{equation}
% \begin{equation}
%     G(v, c) = \frac{1}{K}\sum_{k=1}^K \sum_{m=1}^{N_g} \frac{\exp(\langle f^{n_k}, f^{g_m}\rangle)}{\sum_{i=1}^{N_g} \exp(\langle f^{n_k}, f^{g_i}\rangle)} \cdot \langle f^{n_k}, f^{g_m} \rangle
% \end{equation}

We define the notation for softmax on a vector $\mathbf{x}$ at the $i$-th element as:
% \begin{equation}
    $\sigma(\mathbf{x})_i = \frac{\exp(x_i)/\tau}{\sum_j \exp(x_j)/\tau}$,
% \end{equation}
where $\tau$ is the temperature to scale logits. 
The similarity of all group tokens $\mathcal{G}$ to a noun token $n^k$ is $s( \mathcal{G}, n^k)=[\langle g^1, n^k \rangle, \dots, \langle g^M, n^k \rangle] \in \sR^{M}$, where $\langle\cdot, \cdot \rangle$ is the cosine similarity.
Since ground-truth correspondences between regions and nouns are inaccessible, we compute the grounding similarity between all group and noun tokens by:
% \begin{equation}
%     G(v, c) = \frac{1}{K}\sum_{k=1}^K \left\langle f^{n_k}, \mathlarger{\sum_{m=1}^{N_g}} \frac{\exp( \langle f^{n_k}, f^{g_m}\rangle)}{\sum_{i=1}^{N_g} \exp(\langle f^{n_k}, f^{g_i}\rangle)} \cdot f^{g_m} \right \rangle.
% \end{equation}
\begin{equation}
    G(v, c) = \frac{1}{K}\sum_{k=1}^K \left\langle n^k, \mathlarger{\sum_{m=1}^{M}} \sigma\left(s(\mathcal{G},n^k)\right)_m \cdot g^m \right \rangle.
\end{equation} $G(v,c)$ encourages each noun to be grounded to one or a few regions and avoids penalizing regions that cannot find any relevant nouns. 

Similarity scores over a batch of size $B$ are computed as: $G(\mathcal{V}, c_i)=[G(v_1, c_i), \dots, G(v_B, c_i)] \in \sR^{B}$ and  $G(v_i, \mathcal{C})=[G(v_i, c_1), \dots, G(v_i, c_B] \in \sR^{B}$, where $\mathcal{V}=\{v_i\}_{i=1}^B$ and $\mathcal{C}=\{c_i\}_{i=1}^B$ denote the set of videos and captions in a batch respectively.
%
Spatiotemporal grounding loss $\gL_\text{g}$ is then defined 
% by replacing the original cosine similarity with our grounding similarity 
to enable nouns to be matched with regions for each positive $\langle \textit{video, caption}\rangle$ pair.
$\gL_\text{g} = \gL^{v\rightarrow c}_\text{g} + \gL^{c\rightarrow v}_\text{g}$, consists of a video-to-caption grounding loss
\begin{equation}
   \gL^{v\rightarrow c}_\text{g} = -\frac{1}{B}\sum_{i=1}^B \log \sigma\left(G\left(v_i, \mathcal{C}\right)\right)_i,
\end{equation}
and a caption-to-video grounding loss
% \begin{equation}
%     \gL^{c\rightarrow v}_\text{g} = -\frac{1}{B}\sum_{i=1}^B\sum_{p\in P_i^c}w_{i,p}^c\log\frac{\exp(G(v_p, c_i)/\tau)}{\sum_{j=1}^B\exp(G(v_j, c_i)/\tau)},
% \end{equation}
% $\gL^{c\rightarrow v}_\text{g} = -\frac{1}{B}\sum_{i=1}^B \log \sigma\left(G\left(\mathcal{V}, c_i\right)\right)_i$.
\begin{equation}
\gL^{c\rightarrow v}_\text{g} = -\frac{1}{B}\sum_{i=1}^B \log \sigma\left(G\left(\mathcal{V}, c_i\right)\right)_i.
\end{equation}

\subsection{Temporal Grouping with Cut-and-Paste}
\label{sec:temporal-grouping}
% Videos in the pre-training dataset are usually very short and contain multiple repeated scenes, which are hard to distinguish. 
Training data in pre-training stage are usually short video clips with repetitive scenes.
To simulate scene shifts, we design a cut-and-paste operation inspired from image augmentations~\cite{yun2019cutmix,zhang2022unsupervised} to introduce temporal changes manually as augmentation to further improve video representations. 

Given a target video $v_i$ with $T$ frames as the foreground and a randomly sampled video $v_{p_i}$ with the index $p_i$ as the background from the same batch of size $B$,
% (both $v_i, v_{p_i} \in \sR^{T\times H \times W \times 3}$),
we divide each video into $N_t = T/t$ clips with the temporal window size $t$. 
%
We then sample the start and end clip indices $s$ and $e$ from $(0, N_t)$, and paste the corresponding region from $v_i$ into the background video $v_{p_i}$ to form a blended video $\hat{v}_i$. 
We define the foreground-background mask as $
    m_i \in \sR^{N_t} = \{ \mathbf{1}(j \in [s, e]) | j \in [0, N_t) \}$,
where $\mathbf{1}(\cdot)$ is the indicator function. This operation is illustrated in Figure \ref{fig:framework}.
% and we can observe that $\hat{v}_i$ contains at least two scenes from $v_i$ and $v_{p_i}$. 

% \begin{figure}[t]
% \begin{center}
% % \fbox{\rule{0pt}{1.5in} \rule{0.8\linewidth}{0pt}}
%   \includegraphics[width=0.9\linewidth]{figures/cut-paste.pdf}
% \end{center}
%   \caption{An illustration of cut-and-paste operation.}
% \label{fig:cut-paste}
% \end{figure}
% The whole weight matrix $W^v \in \sR^{B\times B}$ can be constructed from Eq.2 with the remaining elements as $0$. From the perspective of $c_i$, we can obtain $P_i^c$ and $W^c$ by transposing $W^v$.
% With the patch size of $t\times h \times w$, each $\hat{v}_i$ is divided into $N_\text{patch}=N_t*N_h*N_w$ patches, where $N_h=H/h$ and $N_w=W/w$.
A video is first flattened into $N$ non-overlapping voxels.
After projected by a linear layer, these voxel tokens are fed into the transformer encoder to obtain transformed tokens $z_i^v \in \sR^{N \times d}$, where $d$ is the feature dimension.
% To reflect the cut\&paste operation, we reshape $\{z_i^v\}$ into $N_t\times(N_h*N_w)\times d$ and extract clip features $z_i^\text{clip}\in \sR^{N_t \times d}$ via average pooling.
To obtain clip-level representations $z_i^{\text{clip}} \in R^{N_t \times d}$, we average-pool over $z_i^v$ along spatial dimension after recovering the feature map's 3D shape.
% \lzyuan{TODO: rename $z^{clip}$}
% conduct the following transformation to extract representations for each clip $z_i^\text{clip} = \text{AvgPool}(\text{Reshape}(\{z_i^{v}\}, N_t \times (N_h*N_w) \times d))$.
Two cluster centers, $z^b_i$ for the background and $z^f_i$ for the foreground, are further computed by averaging features from $z_i^v$ on the corresponding position based on the mask $m_i$.
% average pooling is applied to $z_i^\text{clip}$ based on background and foreground indices
% $z_i^b = \text{AvgPool}(\{z_i^\text{clip}[k] | k \in [0, s) \cup [e, N_t)\}),
%         z_i^f = \text{AvgPool}(\{z_i^\text{clip}[k] | k \in [s, e)\})$.
% $z^b$ and $z^f$ are regarded as two cluster centers for all $N_t$ video clips. 
% To distinguish whether these clips are from background or foreground, 
To assign each clip to background or foreground,
% obtain the group assignment $a_i$, i.e.,  
we compute $a_i$ via dot product with an element-wise softmax function applying on the last dimension:
\begin{equation}
    a_i = \text{Softmax}(z_i^\text{clip} \cdot [z_i^b; z_i^f]^T) \in \sR^{N_t \times 2}.
\end{equation}
Finally, the temporal grouping loss can be computed within a batch between $a_i$ and the ground-truth masking $m_i$ of one-hot version using mean squared error as
\begin{equation}
    \gL_\text{t} = \frac{1}{B}\sum_{i}^B \ell_\text{MSE}(a_i, \text{One-hot}(m_i)).
\end{equation}
%
In addition, the cut-and-paste operation indicates that $\hat{v}_i$ has another positive caption $c_{p_i}$ apart from its original $c_i$, and the positive indices of captions with weights become $W^v\in \sR^{B\times B}=\{w^v_{i,j}\}$ satisfying
\begin{equation}
    w^v_{i,j} = 
    \begin{cases}
    \beta_i, & j = i \\
    1 - \beta_i, & j = p_i \\
    0, & \text{otherwise}
    \end{cases},
\end{equation}
where $\beta_i=(e-s)/N_t$ is the ratio of the foreground in the cut-and-paste video $\hat{v}_i$. 
From the perspective of captions, we can obtain $W^c=(W^v)^\top$. 
We can derive the augmented grounding loss $\gL_\text{g}$ with the video-to-caption loss as
\begin{equation}
    \gL^{v\rightarrow c}_\text{g} = -\frac{1}{B}\sum_{i=1}^B \sum_{j=1}^B w^v_{i,j}\log \sigma\left(G\left(\hat{v}_i, \mathcal{C}\right)\right)_j,
\end{equation}
and the caption-to-video loss with $\mathcal{\hat{V}}=\{\hat{v}_i\}_{i=1}^B$ as
\begin{equation}
    \gL^{c\rightarrow v}_\text{g} = -\frac{1}{B}\sum_{i=1}^B \sum_{j=1}^B w^c_{i,j}\log \sigma\left(G\left(\mathcal{\hat{V}}, c_i\right)\right)_j.
\end{equation}


\subsection{Overall Pre-training Objective}
\label{sec:global}
We also include a global contrastive learning objective for instance-level video-caption alignment. $f_i^v$, the video representation of $\hat{v}_i$, is extracted from average-pooled group tokens and $f_i^c$, the caption representation $c_i$, is selected as \texttt{[CLS]} token from the original caption.
Instance similarity scores are defined as: $s(\mathcal{V}, c_i)=[\langle f^v_1, f^c_i\rangle, \dots, \langle f^v_B, f^c_i\rangle] \in \sR^{B}$ and $s(\hat{v}_i, \mathcal{C})=[\langle f^v_i, f^c_1\rangle, \dots, \langle f^v_i, f^c_B\rangle] \in \sR^{B}$.
The global contrastive loss is defined as
$    \gL_\text{contrast} = \gL^{v\rightarrow c}_\text{contrast} + \gL^{c\rightarrow v}_\text{contrast}$,
with the video-to-caption view of
% \begin{equation}
%     \gL_\text{c}^{v\rightarrow c} = -\frac{1}{B}\sum_{i=1}^B\sum_{p\in P_i^{v}}w_{i,p}^v\log\frac{\exp(f_i^v\cdot f_p^c/\tau)}{\sum_{j=1}^B\exp(f_i^v\cdot f_j^c/\tau)},
% \end{equation}
\begin{equation}
    \gL_\text{contrast}^{v\rightarrow c} = -\frac{1}{B}\sum_{i=1}^B\sum_{j=1}^B w_{i,j}^v\log \sigma(s(\hat{v}_i, \mathcal{C}))_j,
\end{equation}
and the caption-to-video view of
% \begin{equation}
%     \gL_\text{c}^{c\rightarrow v} = -\frac{1}{B}\sum_{i=1}^B\sum_{p\in P_i^{c}}w_{i,p}^c\log\frac{\exp(f_i^c\cdot f_p^v/\tau)}{\sum_{j=1}^B\exp(f_i^c\cdot f_j^v/\tau)}.
% \end{equation}
\begin{equation}
    \gL_\text{contrast}^{c\rightarrow v} = -\frac{1}{B}\sum_{i=1}^B\sum_{j=1}^B w_{i,j}^c\log \sigma(s(\mathcal{V}, c_i))_j.
\end{equation}
The overall pre-training objective is a combination of weighted sum of grouping loss,  grounding loss, and global contrastive loss:
% \begin{equation}
   $\gL = \omega_1 \gL_\text{t} + \omega_2 \gL_\text{g} + \omega_3 \gL_\text{contrast}$.
% \end{equation}
We set three weights equal to one in our experiments for brevity.