\documentclass[10pt,twocolumn,letterpaper]{article}
\input{math_commands}
\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[sort,nocompress]{cite}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{relsize}
% \usepackage{subcaption}
\usepackage{float}
\usepackage{multirow}
\usepackage{textcomp}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
% % \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,linkcolor=blue,citecolor=green,bookmarks=false]{hyperref}
% \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,citecolor=blue,linkcolor=blue,bookmarks=false]{hyperref}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{4280} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
% \newcommand{\ours}{G$^2$-ViLM\xspace}
\newcommand{\ours}{G-ViLM\xspace}
\newcommand{\authorskip}{\quad}

\newcommand{\lzyuan}[1]{\textcolor{blue}{Liangzhe: #1}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
% \title{Grouping-based Video-Language Pre-training with Fine-Grained Information}
\title{Spatiotemporally Discriminative Video-Language Pre-Training \\ with Text Grounding}

\author{Yuanhao Xiong$^{1,2}\thanks{Work done as a student researcher at Google}$ \authorskip Long Zhao$^{1}$ \authorskip Boqing Gong$^{1}$ \authorskip Ming-Hsuan Yang$^{1}$ \authorskip Florian Schroff$^{1}$ \\
Ting Liu$^{1}$  \authorskip Cho-Jui Hsieh$^{2}$ \authorskip Liangzhe Yuan$^{1}$ \\ [2mm]
$^{1}$Google Research \authorskip $^{2}$UCLA \\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
% Past few years has witnessed the success of video-language pre-training in learning transferable representations and improving performance of various downstream tasks. 
% However, most of existing methods focus on instance-level (video, caption) alignment via global contrastive learning, and neglect fine-grained local information which proves important for tasks requiring temporal localization and semantic reasoning. 
Most of existing video-language pre-training methods focus on instance-level alignment between video clips and captions via global contrastive learning but neglect rich fine-grained local information, which is of importance to downstream tasks requiring temporal localization and semantic reasoning.
%
In this work, we propose a simple yet effective video-language pre-training framework, namely \ours, to learn discriminative spatiotemporal features.
% promote learning temporal-aware features and local region-noun alignment at the same time. 
Two novel designs involving spatiotemporal grounding and  temporal grouping promote learning local region-noun alignment and temporal-aware features simultaneously.
%
Specifically, spatiotemporal grounding aggregates semantically similar video tokens and aligns them with noun phrases extracted from the caption to promote local region-noun correspondences.
%
Moreover, temporal grouping leverages cut-and-paste to manually create temporal scene changes and then learns distinguishable features from different scenes. 
%
Comprehensive evaluations demonstrate that \ours performs favorably against existing approaches on four representative downstream tasks, covering text-video retrieval, video question answering, video action recognition and temporal action localization.
\ours performs competitively on all evaluated tasks and in particular achieves R@10 of 65.1 on zero-shot MSR-VTT retrieval, over 9\% higher than the state-of-the-art method.
\end{abstract}

\input{intro}
\input{related}
\input{method}
\input{exp}
\input{conclusion}

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

% \newpage
\clearpage
\input{appendix}

\end{document}