\section{Conclusion}
In this paper, we present a novel video-language pre-training framework, named \ours, that aims to utilize fine-grained local information to capture region-noun correspondences and temporal-aware features at the same time. 
Spatiotemporal grounding and temporal grouping are introduced to achieve the goal of local region-noun alignment and temporal distinguishment in a self-supervised manner. 
%
The proposed framework outperforms existing methods significantly on several downstream tasks, such as text-video retrieval, video question answering, video action recognition, and temporal action localization.

% While the proposed GroundVL framework shows promising results, there are several limitations to this study. Firstly, the evaluation is limited to a few representative downstream tasks and may not be generalizable to other tasks. Secondly, the proposed framework relies on the availability of caption information, which may not always be present in real-world scenarios. Finally, the manual creation of temporal changes using cut-and-paste may not fully capture the complexity of real-world video data. Future studies can address these limitations and further explore the effectiveness and generalizability of the proposed framework.
