
\section{Reducing the computational costs: reduced \algo}\label{sec:reduced}
While \algo\ provides optimal theoretical guarantees, it does so by computing, at every iteration, a projection onto a $n-n_h$-dimensional vector space. For $n -n_h \gg 1$, such a projection might be computational expensive. In this section, we therefore propose a modification of \algo\ that only projects onto a hyperplane, which comes essentially for free. The main idea is to reparametrize our problem by noting that $\cM =\{ x \in \bbR^n: H(x) := \norm{h(x)}^2 / 2 = 0\}$.
Introducing the vector spaces $\tilde{V}(x) := \{v\in \bbR^n : \nabla H(x)^{\top} v = 0 \}$, the iterates of \redalgo\ are defined as follows:
\begin{equation}
  x_{k+1} = x_k - \alpha(x_k) \gamma_k \nabla H(x_k) - \gamma_k \nabla_{\tilde V}f(x_k) + \gamma_k \eta_{k+1}\, ,
\end{equation}
where, as previously, $\nabla_{\tilde V} f(x)$ denotes the projection of $\nabla f(x)$ onto $\tilde V(x)$, $(\eta_{k+1})$ is a perturbation sequence, and $\alpha(x)$ corresponds to the choice $A(x) = \alpha(x)\cI$ and is specified in \Cref{th:reduced_rates} below.

Note that, as soon as $\nabla H(x) \neq 0$, $\tilde{V}(x)$ is a hyperplane. Therefore, the computation of $\nabla_{\tilde V} f(x)$ is straightforward, preserving, at the same time, its orthogonality to $\nabla H(x)$. Thus, \redalgo\ follows the same idea as \algo\, while significantly reducing the computational costs. Unfortunately, this construction damages the continuity of $\tilde{V}(x)$ near $\cM$. Indeed, since $\nabla H(x) = \nabla h(x) \cdot h(x)$, we obtain $\nabla H(x) = 0$ and $\tilde{V}(x)= \bbR^n$ on $\cM$. This observation shows that the field associated with \redalgo\ is non-smooth. The inherent non-smoothness of \redalgo\ deteriorates its convergence properties, but we can still derive a $\cO(\epsilon^{-3})$ rate of convergence in deterministic environments and a $\cO(\varepsilon^{-4})$ rate of convergence in stochastic environments. The latter is reminiscent of the convergence rate of subgradient methods in non-smooth environments (see \citet{dav_dru_weakconv_rate19}). 

To properly analyze \redalgo, and due to a non-smooth choice of $\alpha(x)$, we consider assumptions that are slightly different from \Cref{hyp:cont_model}. More precisely, we assume \Cref{hyp:cont_model} for $A(x) = \cI$. We will call this set of assumptions \Cref{hyp:cont_model}', and we denote the smallest eigenvalue of $\nabla h(x)^\top \nabla h(x)$ as $\mu_h^2$. %\dan{Probably it is not a good idea to redefine quantities, but $\alpha_m$ with $\alpha$ in one equation will be too confusing.}

We note that the compactness of $K$ and Lipchitz-continuity of $\nabla f$ and $\nabla h$ (\Cref{hyp:disc_model}-\ref{hyp:fh_Lipgrad}) implies that $f$, $h$, and $\nabla H = \nabla h \cdot h$ are Lipschitz-continuous with Lipchitz constants $C_f$, $C_h$, and $L_H$ respectively. Moreover, since $\nabla h$ is continuous and $K$ is compact, we have $\sup_{x \in K}\norm{\nabla h(x)}_2 \leq M_h$. 

% \begin{assumption}\label{hyp:reduced_model}
%    \begin{enumerate}[label=\roman*), nosep]
%      \item\label{hyp:reduced_k_comp} The set $K$ is compact.
%      \item\label{hyp:reduced_k_fullr}  $\nabla h$ is of full rank on $K$.
%      \item\label{hyp:reduced_fh_lip} The functions $f$ and $h$ are Lipschitz on $K$ with constants $C_f$ and $C_h$ respectively.
%      \item\label{hyp:h_singular_values}  There are two constants $M_h \geq \mu_h > 0$ such that the singular values of the matrix $\nabla h(x)$ are lower-bounded by $\mu_h$ and upper-bounded by $M_h$.
%    \end{enumerate}
%  \end{assumption}
 

% Notice that \Cref{hyp:reduced_model}-\ref{hyp:reduced_fh_lip} automatically follows from compactness of $K$ and Lipchitz-continuity of $\nabla f$ and $\nabla h$ (\Cref{hyp:disc_model}-\ref{hyp:fh_Lipgrad}). Additionally, under the set of assumptions \Cref{hyp:disc_model}-\ref{hyp:reduced_model} we automatically have that the gradients of $H$ are Lipschitz-continuous with some constant $L_H$ since $\nabla H(x) = \nabla h(x) \cdot h(x)$.

\begin{theorem}\label{th:reduced_rates}
    Assume \Cref{hyp:cont_model}'-\ref{hyp:disc_model}. Let $\bar D, \alpha >0$ and $\alpha(x) = \alpha \cdot H(x) / \norm{\nabla H(x)}^2$. Denote $D_0 = f(x_0) - \inf_{x \in K} f(x)$, $\gamma_{\max} = \min(\alpha^{-1}, (L_f + \alpha \mu_h^{-2} L_H)^{-1})$ and $\tilde C = B_f M_h \mu_h^{-2}$. Finally, assume that $x_0 \in \cM$ and fix $N>0$ the number of iterations. The following holds:
     \begin{enumerate}
        \item If $\sigma^2=0$  and for all $k \in \bbN: \gamma_k \equiv \gamma$ for $\gamma = \min(\gamma_{\max}, \bar D \cdot N^{-1/3})$, then choosing $\alpha = \gamma$, we obtain
        \[
            \inf_{k=0,\ldots,N-1} \left\{\norm{\nabla_{\tV} f(x_k)}^2 + \frac{1}{2} \norm{h(x_k)}^2\right\} \leq \frac{8D_0 (L_f + L_h \mu_h^{-2})}{N} + \frac{\left(\frac{8 D_0}{\bar D} + 8 \widetilde{C} L_H \cdot \bar D \right)}{N^{2/3}}\,.
        \]
        \item Otherwise, if for all $k \in \bbN: \gamma_k \equiv \gamma$, with $\gamma = \min(\gamma_{\max}, \bar D \cdot N^{-1/2})$, we obtain by choosing $\alpha = \gamma$ and $\hat k$ uniformly sampled in $\{0,\ldots, N-1\}$
        \begin{align*}
            \bbE\left[\norm{\nabla_{\tV} f(x_{\hat k})}^2 + \frac{1}{2}\norm{h(x_{\hat k})}^2\right] &\leq \frac{4D_0 (L_f + L_h \mu_h^{-2})}{N} + \frac{4 D_0}{\bar D \cdot \sqrt{N}}  +  \frac{4 \widetilde{C}^2 \bar D^2 \cdot L_H}{N}\\
            &+ \frac{\bar D}{\sqrt{N}}\left( 2\left(L_f + \gamma L_H \mu_h^{-2}  \right) \cdot \sigma^2 + 2 \widetilde{C} \cdot \sqrt{ \frac{L_H \sigma^2}{2}} \right)\, .
        \end{align*}
    \end{enumerate}
\end{theorem}
The main difficulty in establishing this result relies in the lack of a suitable Lyapunov function for \redalgo. The latter comes from its inherent non-smoothness and the fact that the Lagrange multipliers that arise in the problem of projection on $\tV$ are unbounded. A complete proof of this theorem is provided in \Cref{app:reduced_proofs}.

In the deterministic setting, \redalgo\ outputs $\hat x$ such that $\norm{h(\hat x)} \leq \varepsilon$ and $\norm{\nabla_{\tV} f(\hat x)} \leq \varepsilon$ in $\cO(\varepsilon^{-3})$ iterations. In the stochastic setting, \redalgo\ outputs a point $\hat x = x_{\hat k}$ such that $\bbE[\norm{h(\hat x)}] \leq \varepsilon$ and $\bbE[\norm{\nabla_{\tV} f(\hat x)}] \leq \varepsilon$ in $\cO(\varepsilon^{-4})$ iterations. One drawback of such a method is that we are no longer guaranteed to converge towards the feasible set. Nevertheless, the condition $\norm{\nabla_{\tV} f(\hat x)} \leq \varepsilon$, could be rewritten as $\varepsilon$-1o point with appropriate Lagrange multipliers proportional to $h(\hat x)$ (see \cite{xie_wright_PAL19} for the definition of an $\varepsilon$-1o point). %\sho{just to be sure, do we mean here that we don't have the convergence of $x_k$ towards $\cM$?} \dan{I'm not sure that we can guarantee it due to discontinuity, so we can just not claim convergence or lack of it.}