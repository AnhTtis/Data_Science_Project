\section{Numerical experiments}\label{sec:numerics}
We showcase the efficiency of the proposed algorithms on different optimization problems.
\paragraph{Procrustes problem}
Let $A,B$ be matrices with $A \in \bbR^{q \times q}$ and $B \in \bbR^{p \times q}$, where $p \geq q$. We consider the orthogonal Procrustes problem of finding a matrix $X \in \bbR^{p \times q}$ with orthonormal columns solving the minimization problem
$\min _{X^{\top} X= \cI_q}  \left\|A X-B\right\|^2_{\mathrm{F}}$,
where $\|\cdot\|_{\mathrm{F}}$ is the Frobenius norm. This is referred to as the Procrustes problem on the Stiefel manifold; see \citep{elden1999procrustes}.
We compare \algo, \redalgo, \geomalgo\ with Riemannian gradient descent with two different choices of Riemannian metric: Euclidean and Canonical. The results are shown in \Cref{fig:procrustes_small_scale} in log-log scale for $p=60, q=40$. The results are averaged over $n=100$ draws for the matrices $A$ and $B$ [the entries of the matrices are sampled from a standard normal distributions]. For this experiment, we choose $A(x) = 5 \cI$; we use a constant step size $\gamma_k = 10^{-2}$ for \algo\ and \geomalgo, and decreasing step size for \redalgo\ $\gamma_k = 10^{-2} \cdot k^{-1/3}$.
\begin{figure}
    \centering
    \includegraphics[width=0.98\linewidth]{media/procrutes_small.pdf}
    \caption{Comparison of \algo\ (blue), \redalgo\ (orange), and \geomalgo\ (green) with Riemannian gradient descent with two different Riemannian metrics (red and purple). The upper plot shows the orthogonality error for $X$, the lower plot shows the convergence of the objective function (averages over 100 seeds).}
    \label{fig:procrustes_small_scale}
\end{figure}
In particular, we find that \geomalgo\ outperforms the Riemannian gradient descent methods for both the Euclidean and canonical Riemannian metrics, and achieves the orthogonality error at the level of machine accuracy. We also see numerical confirmation of the $\cO(\varepsilon^{-2})$ convergence of \algo\ and \geomalgo\ and the slower convergence of \redalgo. Additional experiments on a large instance of the problem are presented in \Cref{app:numerics}.
\paragraph{Hanging chain}
As a second non-convex and nonlinear example, we compute the shape of a hanging chain. The problem can be formulated as follows:
\begin{multline}
\min_{(\xi_1,\dots,\xi_N)\in\mathbb{R}^{2N}} \frac{1}{N^3} \sum_{i=1}^N \left( \frac{k_\text{s}}{r^4}(\xi_{i-1}-\xi_i)\T (\xi_{i+1}-\xi_i) + y_i \right) \\ \text{s.t.} \quad \sqrt{(\xi_{k-1}-\xi_k)\T (\xi_{k-1}-\xi_k)}\leq r,~ k=1,2,\dots,N+1, \label{eq:numEx}
\end{multline}
where $\xi_k=(x_k,y_k)$ denotes the $xy$-position of the $k$-th element, $k=1,\dots N$, and $\xi_{0}=(0,0)$ and $\xi_{N+1}=(9,0)$ are the two endpoints. Further details are given in \Cref{app:numerics}. We compare the results of \algo~ with $A(x) = \alpha (\nabla h(x)^{\top} \nabla h(x))^{-1}$ (hereafter abbreviated as \algo),  \redalgo~, and an augmented Lagrangian method. The results are summarized in \Cref{fig:objfunconst2} for the case $N=10,000$, which leads to $20,000$ decision variables and $10,001$ nonlinear constraints.
We note that  \redalgo~ and augmented Lagrangian converge much more slowly than \algo. We also find that fine-tuning the augmented Lagrangian method is quite difficult, while the time steps of \algo~ and  \redalgo~ are easy to set (see \Cref{app:numerics} for details). The execution time per iteration of \algo~ is about five times that of \redalgo~ and the augmented Lagrangian. To demonstrate the potential of \redalgo~, we run the same example for $N=2 \times 10^5$, resulting in a large optimization problem with $4 \times 10^5$ decision variables and $2\times10^5$ nonlinear constraints. Under these conditions, solving the Karush-Kuhn-Tucker system becomes challenging at each iteration, which is required for \algo~. However, the  \redalgo~ still performs well, requiring only about 0.85 seconds to execute a single iteration.
\begin{figure}
\newlength{\figurewidth}
\newlength{\figureheight}
\setlength{\figurewidth}{.35\columnwidth}
\setlength{\figureheight}{.2\columnwidth}
\input{media/fvalueUpdated.tikz} %
\input{media/gnormFinal.tikz}
\vspace{-10pt}
\caption{The figure compares the results of \algo~ (black) with \redalgo~ (red) and an augmented Lagrangian method (blue). \redalgo~ is performed with a decreasing step size of $\cO(k^{-1/2})$. The left plot shows the convergence in objective function, while the right plot gives the mean square error (denoted by rms) of the constraint violations. We note that \algo~ converges much faster than \redalgo~ and the augmented Lagrangian.}
\label{fig:objfunconst2}
\end{figure}
