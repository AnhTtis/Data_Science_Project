\subsection{Proof of \Cref{th:reduced_rates}}\label{app:reduced_proofs}

We start from the observation that explains how looks the solution to the projection on $\tilde{V}(x) = \{ v \in \bbR^n : \nabla H(x)^\top v = 0 \}$, where $H(x) = \norm{h(x)}^2 / 2$. 
\begin{corollary}\label{cor:tildeV_projections}
    Let $f\colon \bbR^n \to \bbR$ be a differentiable function and let $\cM = \{ x \in \bbR^n : H(x) = 0\}$. Then for any $x$ such that $\nabla h(x)$ is of full rank it holds
    \[
        \nabla_{\tV} f(x) = \nabla f(x) + \lambda(x) \cdot \nabla H(x),
    \]
    where 
    \[
        \lambda(x) = \begin{cases}
            0, & x \in \cM \\
            -\frac{\nabla H(x)^\top \nabla f(x)}{\norm{\nabla H(x)}^2}, & x \not \in \cM
        \end{cases}
    \]
\end{corollary}
\begin{proof}
    Apply \Cref{lm:aff_proj} with $n_h = 1, W = \nabla H(x)$ and $b = 0$ for $x \not \in \cM$.
\end{proof}
\begin{remark}
    Even through $\lambda(x) \to -\infty$ as $x \to \cM$, we have that projected gradients are always bounded:
    \[
        \norm{\nabla_{\tV} f(x)} \leq \norm{\nabla f(x)} + \frac{\vert \nabla H(x)^\top \nabla f(x) \vert }{\norm{\nabla H(x)}^2} \cdot \norm{\nabla H(x)} \leq 2 \norm{\nabla f(x)}.
    \]
\end{remark}

Next we provide a lemma that guarantees for $H(x_k)$ that under the specific choice of $A(x) = \alpha(x)\cI$ where $\alpha(x) =  \alpha \cdot \frac{H(x)}{\norm{\nabla H(x)}^2}$ for a constant $\alpha > 0$ if $x \not \in \cM$ and $\alpha(x) = 0$ if $x \in \cM$. 

\begin{lemma}\label{lm:reduced_constrain_rate}
    Assume \Cref{hyp:cont_model}'-\ref{hyp:disc_model}. Assume that for any $k > 0$, $\alpha \gamma_k \leq 1$. Define $v_k = -\alpha(x_k) \nabla H(x_k) - \nabla_{\tV} f(x_k)$. It holds
    \begin{align*}
        \bbE[H(x_{k+1})] &\leq H(x_0)  \cdot \prod_{j=0}^k (1 - \alpha \gamma_j)  + \bbE\left[\frac{L_H}{2} \sum_{j=0}^k \gamma_j^2 (\norm{v_j}^2 +\sigma^2)\prod_{\ell=j+1}^k (1 - \alpha \gamma_\ell)\right]\, .
    \end{align*}
    Furthermore, if $(\gamma_k)$ is a non-increasing sequence, then for all $N \in \bbN$,
    \begin{equation*}
        \bbE\left[\sum_{k=0}^{N-1} \gamma_k H(x_{k})\right] \leq \frac{1}{\alpha} H(x_0) + \frac{L_H}{2 \alpha} \bbE\left[ \sum_{k=0}^{N-1} \gamma_k^2 \norm{v_k}^2\right]  + \frac{L_H}{2\alpha} \sum_{k=0}^{N-1} \gamma_k^2 \sigma^2 \, .
    \end{equation*}
\end{lemma}
\begin{proof}
    Since $h(x)$ is Lispchitz and has Lipschitz-continuous gradients we have that $H(x)$ also has Lipschitz-continuous gradients with constant $L_H$, thus for any $k \in \bbN$
    \[
        \bbE_k[H(x_{k+1})] \leq H(x_k) + \gamma_k \nabla H(x_k)^\top v_k + \frac{L_H \gamma_k^2}{2} \bbE_k[\norm{v_k + \eta_{k+1}}^2].
    \]
    Notice that $\nabla_{\tV} f(x_k)$ is orthogonal to $\nabla H(x_k)$, thus 
    \[
        \nabla H(x_k)^\top v_k  = -\alpha(x_k) \norm{\nabla H(x_k)}^2 = -\alpha H(x_k)
    \] by definition of $\alpha(x_k)$. Also notice that $\bbE_k[\norm{v_k + \eta_{k+1}}^2] \leq \norm{v_k}^2 + \sigma^2$. Therefore
    \[
        \bbE_k[H(x_{k+1})] \leq (1 - \alpha \gamma_k) H(x_k) + \frac{L_H \gamma_k^2}{2} \left(\norm{v_k}^2 + \sigma^2 \right).
    \]
    Rolling out this inequality we conclude the first statement. Next we sum all inequalities for all $k=0,\ldots,N-1$ with weights $\gamma_k$
    \[
        \bbE\left[\sum_{k=0}^{N-1} \gamma_k H(x_k)\right] \leq  H(x_0) \cdot \sum_{k=0}^{N-1} \gamma_k \prod_{j=0}^{k-1} (1 - \alpha \gamma_j)  + \frac{L_H}{2} \sum_{k=0}^{N-1} \gamma_k \sum_{j=0}^{k-1} \gamma_j^2 (\norm{v_j}^2 + \sigma^2) \prod_{\ell=j+1}^{k-1} (1 - \alpha \gamma_\ell)\, .
    \]
    First we apply \Cref{lm:sums_gamma_rev_bound} for the first term. Next we change the order of summation and apply \Cref{lm:sums_gamma_rev_bound} again
    \begin{align*}
        \sum_{k=0}^{N-1} \gamma_k \sum_{j=0}^{k-1} \gamma_j^2 (\norm{v_j}^2 + \sigma^2) \prod_{\ell=j+1}^{k-1} (1 - \alpha \gamma_\ell) &=
        \sum_{j=0}^{N-1} \gamma_j^2 (\norm{v_j}^2 + \sigma^2) \sum_{k=j+1}^{N-1} \gamma_k  \prod_{\ell=j+1}^{k-1} (1 - \alpha \gamma_\ell)
        \\
        &\leq \frac{1}{\alpha} \sum_{j=0}^{N-1} \gamma_j^2 (\norm{v_j}^2 + \sigma^2).
    \end{align*}
\end{proof}

\begin{lemma}\label{lm:sums_gamma_rev_bound}
    Let $b > 0$ and $(\gamma_k)_{k \geq 0}$ be a non-increasing sequence such that $\gamma_0 \leq 1/b$. Then
    \[
        \sum_{k=0}^{n} \gamma_k \prod_{j=0}^{k-1} (1 - b \gamma_j) = \frac{1 - \prod_{j=0}^n (1 - b \gamma_j)}{b}
    \]
\end{lemma}
\begin{proof}
    Introduce $u_{i:j} = \prod_{\ell=i}^j (1- b \gamma_{\ell})$. Notice that $u_{0:k-1} - u_{0:k} = u_{0:k-1} \cdot b \gamma_k$. Summing this equation from $0$ to $n$ we conclude the statement.
\end{proof}

To provide rates of convergence for the final algorithm we have to proof the following proposition.
\begin{proposition}\label{prop:reduced_rates}
    Assume \Cref{hyp:cont_model}'-\ref{hyp:disc_model}. Let $x_0 \in \cM$. If for all $k \in \bbN$ $\gamma_k \equiv \gamma$ where $\gamma \leq \min(\alpha^{-1}, (L_f + \alpha L_H \mu_h^{-2})^{-1})$, then for any $N \in \bbN$ the following holds
    \begin{align*}
        \bbE\left[ \sum_{k=0}^{N-1} \gamma \norm{v_k}^2 \right] &\leq 4\Delta_N + 2\left(L_f + \alpha L_H \mu_h^{-2}  \right) \cdot \sigma^2 \cdot \gamma^2 N \\
        &+ 4 \widetilde{C}^2 \cdot \alpha L_H \cdot \gamma^2 N  + 4 \widetilde{C} \cdot \sqrt{\frac{\alpha \gamma N}{2}} \sqrt{\frac{L_H \sigma^2}{2} \cdot \gamma^2 N},
    \end{align*}
    where $D_0 = f(x_0) - \inf_{x\in K} f(x)$ and $\widetilde{C} = B_f M_h \mu_h^{-2}$. 
\end{proposition}
\begin{proof}
    First we use the definition of smoothness of the function $f$
    \[
        \bbE_k[f(x_{k+1})] \leq f(x_k) + \gamma_k \nabla f(x_k)^\top v_k + \frac{L_f \gamma_k^2}{2} (\norm{v_k}^2 + \sigma^2).
    \]
    Next we notice that $\nabla f(x_k) + (\alpha(x_k) + \lambda(x_k)) \nabla H(x_k) = -v_k$, thus
    \begin{equation}\label{eq:reduced_f_difference}
        \begin{split}
            \bbE_k[f(x_{k+1})] &\leq f(x_k) - \gamma_k \left( 1 - \frac{L_f \gamma_k}{2} \right) \norm{v_k}^2 + \frac{L_f \gamma_k^2 \sigma^2}{2} \\
            &- \gamma_k (\alpha(x_k) + \lambda(x_k)) \nabla H(x_k)^\top v_k.
        \end{split}
    \end{equation}
    By orthogonality property and choice of $\alpha(x_k)$ we have
    \[
        \nabla H(x_k)^\top v_k = - \alpha(x_k) \norm{\nabla H(x_k)}^2 = -\alpha H(x_k),
    \]
    therefore, rolling out inequality for any $N \in \bbN$
    \begin{equation}\label{eq:reduced_f_difference_final}
        \begin{split}
        \bbE[f(x_N)] &\leq f(x_0) + \bbE\left[\sum_{k=0}^{N-1}\left( \frac{L_f \gamma_k^2}{2} - \gamma_k\right) \norm{v_k}^2 \bbE\right] + \sum_{k=0}^{N-1} \frac{L_f \gamma_k^2 \sigma^2}{2} \\
        &- \alpha \bbE\left[\sum_{k=0}^{N-1}  \gamma_k (\alpha(x_k) + \lambda(x_k)) H(x_k)\right].
        \end{split}
    \end{equation}
    Next we have to analyze the last sum. To do it, we start from definitions of $\alpha(x_k)$, $\lambda(x_k)$ and $H(x_k)$
    \[
        \left\vert \sum_{k=0}^{N-1} \gamma_k (\alpha(x_k) + \lambda(x_k)) H(x_k) \right\vert  \leq \sum_{k=0}^{N-1} \gamma_k \frac{\vert \alpha H(x_k) - \nabla f(x_k)^\top \nabla H(x_k)\vert }{\norm{ \nabla h(x) h(x_k)}^2 } \cdot \frac{1}{2} \norm{h(x_k)}^2.
    \]
    Next we apply Cauchy-Schwartz inequality combined with definition of $B_f$, $\mu_h$ and $M_h$ we have $\norm{ \nabla h(x) h(x_k)}^2 \geq \mu_h \norm{h(x)}^2$ and
    \[
        \left\vert \sum_{k=0}^{N-1} \gamma_k (\alpha(x_k) + \lambda(x_k)) H(x_k) \right\vert \leq \frac{\alpha}{2\mu_h^{2}} \sum_{k=0}^{N-1} \gamma_k H(x_k) + \frac{B_f M_h}{\sqrt{2} \cdot \mu_h^{2}} \sum_{k=0}^{N-1} \gamma_k \sqrt{H(x_k)}.
    \]
    By Cauchy-Schwartz inequality
    \[
        \sum_{k=0}^{N-1} \gamma_k \sqrt{H(x_k)} \leq \sqrt{\sum_{k=0}^{N-1} \gamma_k} \cdot \sqrt{\sum_{k=0}^{N-1} \gamma_k H(x_k)}.
    \]
    Next we are going to deal with expectation. By Jensen's inequality applied to a square root
    \begin{align*}
        \bbE\left[\biggl\vert \sum_{k=0}^{N-1} \gamma_k (\alpha(x_k) + \lambda(x_k)) H(x_k) \biggl\vert\right] &\leq \frac{\alpha}{2\mu_h^2} \bbE\left[ \sum_{k=0}^{N-1} \gamma_k H(x_k) \right] \\
        &+ \frac{B_f M_h}{\sqrt{2} \cdot \mu_h^2} \sqrt{\sum_{k=0}^{N-1} \gamma_k} \sqrt{\bbE\left[ \sum_{k=0}^{N-1} \gamma_k H(x_k) \right]}
    \end{align*}
    By assumption we have $\gamma_k \leq \alpha^{-1}$, so we can apply \Cref{lm:reduced_constrain_rate} and obtain
    \begin{align*}
        \bbE\biggl[\biggl\vert \sum_{k=0}^{N-1} \gamma_k &(\alpha(x_k) + \lambda(x_k)) H(x_k) \biggl\vert\biggl] \leq \frac{1}{2 \mu_h^2}\left( H(x_0) +  \frac{L_H}{2} \bbE\left[\sum_{k=0}^{N-1} \gamma_k^2 \norm{v_k}^2\right] + \frac{L_H}{2} \sum_{k=0}^{N-1} \gamma_k^2 \sigma^2 \right) \\
        &+ \frac{B_f M_h}{\sqrt{2\alpha} \mu_h^{2}} \sqrt{\sum_{k=0}^{N-1} \gamma_k} \cdot \sqrt{H(x_0) + \frac{L_H}{2} \bbE\left[\sum_{k=0}^{N-1}  \gamma^2_k \norm{v_k}^2\right] + \frac{L_H}{2} \sum_{k=0}^{N-1} \gamma_k^2 \sigma^2 }.
    \end{align*}
    For simplicity we assume $H(x_0) = 0$ and that $\gamma_k \equiv \gamma$ that satisfies the following inequality
    \[
        \gamma \leq \frac{1}{L_f + \alpha L_H \mu_h^{-2}}.
    \]
    Define $\Delta f_N = f(x_0) - f(x_N)$ and $S_N = \bbE\left[\sum_{k=0}^{N-1}\norm{v_k}^2\right]$, then by rearranging term in \eqref{eq:reduced_f_difference_final} and applying inequality $\sqrt{a+b} \leq \sqrt{a} + \sqrt{b}$ for positive $a,b$
    \begin{align*}
        \frac{\gamma}{2} S_N &\leq \Delta f_N + \frac{(L_f + \alpha L_H \mu_h^{-2}) \cdot \sigma^2}{2} \gamma^2 N \\
        &+ \frac{B_f M_h L_H}{2\mu_h^2} \cdot (\alpha^{1/2} \gamma^{3/2} N) \cdot \sigma  + \frac{B_f M_h}{\mu_h^2} \cdot \sqrt{\frac{\alpha \gamma^2 N \cdot L_H}{2}} \sqrt{\gamma S_N}.
    \end{align*}
    We have the quadratic inequality in $\sqrt{\gamma S_N}$ that could be easily solved. Using the fact that if $x^2  \leq 2ax + 2b$ then $x \leq a + \sqrt{a^2 + 2b} \leq 2a + \sqrt{2b}$ and a numeric inequality $(2a+\sqrt{2b})^2 \leq 8 a^2 + 4b$
    \begin{align*}
        \bbE\left[ \sum_{k=0}^{N-1} \gamma \norm{v_k}^2 \right] &\leq 4 \Delta f_N + 2\left( L_f + \alpha L_H \mu_h^{-2} \right)\cdot \sigma^2  \cdot \gamma^2 N \\
        &+ \left(\frac{4B_f^2 M_h^2 \cdot \alpha L_H}{\mu_h^{4}}\right) \cdot \gamma^2 N  + \frac{2B_f M_h L_H}{\mu_h^2} \cdot (\alpha^{1/2} \gamma^{3/2} N) \cdot \sigma
    \end{align*}
    Finally, we notice that $D_0 = f(x_0) - \inf_{x\in K} f(x)$ is an upper bound on $\Delta f_N$.
\end{proof}

Now we are ready to prove the main convergence results. It will be divided into two independent propositions.

\begin{proposition}[Convergence in deterministic case]\label{prop:reduced_deterministic_rates}
    Assume \Cref{hyp:cont_model}'-\ref{hyp:disc_model} and let $x_0 \in \cM$. 
     Let $\sigma^2 = 0$ and also define $\bar D$ as a known constant. If for all $k \in \bbN$, $\gamma_k \equiv \bar\gamma$ where $\bar\gamma =\min(\alpha^{-1}, (L_f + \alpha L_H \mu_h^{-2})^{-1}, \bar D \cdot N^{-1/3})$,  and $\alpha = \bar\gamma$ then for any $N \in \bbN$ the following holds
    \[
        \min_{k=0,\ldots,N-1} \left\{\norm{\nabla_{\tV} f(x_k)}^2 + \frac{1}{2} \norm{h(x_k)}^2\right\} \leq \frac{8D_0 (L_f + L_H \mu_h^{-2})}{N} + \left(\frac{8 D_0}{\bar D} + 8 \widetilde{C} L_H \cdot \bar D \right) \cdot N^{-2/3},
    \]
    where $D_0 = f(x_0) - \inf_{x\in K} f(x)$ and $\widetilde{C} = B_f M_h \mu_h^{-2}$. 
    
    In particular, \redalgo\ outputs a point $\hat x$ for which the minima in the left-hand side attains such that $\norm{h(\hat x)} \leq \varepsilon$ and $\norm{\nabla_{\tV} f(\hat x)} \leq \varepsilon$ in $\cO(\varepsilon^{-3})$ iterations.
\end{proposition}
\begin{proof}
    Apply \Cref{prop:reduced_rates} with $\sigma^2 = 0$ and without expectations
    \[
        \sum_{k=0}^{N-1} \gamma \norm{v_k}^2 \leq 4D_0 + 4 \widetilde{C}^2 \cdot \alpha L_H \cdot  \gamma^2 N.
    \]
    and, at the same time, by combination of \Cref{lm:reduced_constrain_rate} and \Cref{prop:reduced_rates}
    \[
        \sum_{k=0}^{N-1} \gamma \left( \norm{v_k}^2 + H(x_k) \right) \leq 4D_0\left(1 + \frac{\gamma}{\alpha}\right) + 4 \widetilde{C}^2 \cdot \alpha L_H \cdot  \gamma^2 N \left( 1 + \frac{\gamma}{\alpha} \right).
    \]
    By taking $\alpha = \gamma$ and using orthongonality property of $v_k$ we have
    \[
        \min_{k=0,\ldots,N-1} \left\{\norm{\nabla_{\tV} f(x_k)}^2 + \frac{1}{2} \norm{h(x_k)}^2\right\} \leq \frac{8 D_0}{\gamma N} + 8 \widetilde{C}^2 L_H \cdot \gamma^2.
    \]
    To balance these two terms we choose $\gamma_k \equiv \bar\gamma = \min(1, (L_f + \bar\gamma L_H \mu_h^{-2})^{-1}, \bar D \cdot N^{1/3})$ and obtain
    \[
        \min_{k=0,\ldots,N-1} \left\{\norm{\nabla_{\tV} f(x_k)}^2 + \frac{1}{2} \norm{h(x_k)}^2\right\} \leq \frac{8D_0 (L_f + L_H \mu_h^{-2})}{N} + \left(\frac{8 D_0}{\bar D} + 8 \widetilde{C} L_H \cdot \bar D \right) \cdot N^{-2/3}.
    \]
\end{proof}


\begin{proposition}[Convergence in stochastic case]\label{prop:reduced_stochastic_rates}
    Assume \Cref{hyp:cont_model}'-\ref{hyp:disc_model} and let $x_0 \in \cM$. 
    Let $\sigma^2 > 0$ and also define $\bar D$ as a known constant. If for all $k \in \bbN$ $\gamma_k \equiv \bar\gamma$ where $\bar\gamma =\min(\alpha^{-1}, (L_f + \alpha L_H \mu_h^{-2})^{-1}, \bar D \cdot N^{-1/2})$. Fix number of steps $N > 0$. Let $\hat k$ be a uniform index sampled from the set $\{0,\ldots, N-1\}$ then the following holds
    \begin{align*}
        \bbE\left[\norm{\nabla_{\tV} f(x_{\hat k})}^2 + \frac{1}{2}\norm{h(x_{\hat k})}^2\right] &\leq \frac{4D_0 (L_f + L_h \mu_h^{-2})}{N} + \frac{4 D_0}{\bar D \cdot N^{1/2}}  +  \frac{4 \widetilde{C}^2 \bar D^2 \cdot L_H}{N}\\
        &+ \frac{\bar D}{N^{1/2}}\left( 2\left(L_f + \gamma L_H \mu_h^{-2}  \right) \cdot \sigma^2 + 2 \widetilde{C} \cdot \sqrt{ \frac{L_H \sigma^2}{2}} \right)
    \end{align*}
    where $D_0 = f(x_0) - \inf_{x\in K} f(x)$ and $\widetilde{C} = B_f M_h \mu_h^{-2}$. 
    
    In particular, \redalgo\ outputs a point $\hat x = x_{\hat k}$ such that $\bbE[\norm{h(\hat x)}] \leq \varepsilon$ and $\bbE[\norm{\nabla_{\tV} f(\hat x)}] \leq \varepsilon$ in $\cO(\varepsilon^{-4})$ iterations.
\end{proposition}
\begin{proof}
    Let us start from \Cref{prop:reduced_rates} with taking $\alpha = \gamma$
    \begin{align*}
        \bbE\left[ \sum_{k=0}^{N-1} \gamma \norm{v_k}^2 \right] &\leq 4D_0 + 2\left(L_f + \gamma L_H \mu_h^{-2}  \right) \cdot \sigma^2 \cdot \gamma^2 N \\
        &+ 4 \widetilde{C}^2 \cdot L_H \cdot \gamma^3 N  + 2 \widetilde{C} \cdot \sqrt{ \frac{L_H \sigma^2}{2}} \cdot \gamma^2 N.
    \end{align*}
    Combining \Cref{lm:reduced_constrain_rate} with \Cref{prop:reduced_rates} and using orthogonality property
    \begin{align*}
        \bbE\left[\frac{1}{N}\sum_{k=0}^{N-1}\left\{ \norm{\nabla_{\tV} f(x_k)}^2 + \frac{1}{2}\norm{h(x_k)}^2\right\} \right] &\leq \frac{4 D_0}{\gamma N} + \gamma \cdot \left( 2\left(L_f + \gamma L_H \mu_h^{-2}  \right) \cdot \sigma^2 + 2 \widetilde{C} \cdot \sqrt{ \frac{L_H \sigma^2}{2}} \right) \\
        &+ 4 \widetilde{C}^2 \cdot L_H \cdot \gamma^2.
    \end{align*}
    Notice that in the left-hand side we have exactly expectation over $\hat k$. Thus taking $\gamma_k \equiv \bar \gamma = \min(1, (L_f + \bar \gamma L_H \mu_h^{-2})^{-1}, \bar D \cdot N^{-1/2})$ we obtain
    \begin{align*}
        \bbE\left[\norm{\nabla_{\tV} f(x_{\hat k})}^2 + \frac{1}{2}\norm{h(x_{\hat k})}^2\right] &\leq \frac{4D_0 (L_f + L_h \mu_h^{-2})}{N} + \frac{4 D_0}{\bar D \cdot N^{1/2}}  +  \frac{4 \widetilde{C}^2 \bar D^2 \cdot L_H}{N}\\
        &+ \frac{\bar D}{N^{1/2}}\left( 2\left(L_f + \gamma L_H \mu_h^{-2}  \right) \cdot \sigma^2 + 2 \widetilde{C} \cdot \sqrt{ \frac{L_H \sigma^2}{2}} \right)
    \end{align*}
\end{proof}