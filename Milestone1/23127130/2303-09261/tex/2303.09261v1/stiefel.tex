\section{A geometry aware version of \algo}\label{sec:stiefel}
As mentioned earlier, a drawback of \algo\ lies in the fact that at each iteration the method evaluates a projection on $V(x)$.  \redalgo\ requires only one projection onto a hyperplane but does not exhibit optimal convergence guarantees. In fact, since the main feature of our analysis was to exploit the orthogonality of $\nabla h(x)$ and $V(x)$, one might think that the projection onto $V(x)$ (and thus $\nabla_V f(x)$) is not necessarily defined through the canonical metric. This observation is the main idea behind our \textit{Orthogonal Directions Riemannian Gradient Method} (\geomalgo), where the type of projection might depend on $x$. This implicitly provides the ambient space with a Riemannian metric and turns out to be particularly interesting for optimization over the Stiefel manifold. In fact, by a specific choice of metric, the projection has a closed form which recovers the \texttt{landing} algorithm of \citet{ablin2022fast}.  In particular, our results imply near-optimal rates of \texttt{landing}, significantly improving the ones presented in \cite{ablin2022fast}.

Before proceeding, let us introduce some notations. Let $Q : \bbR^n \rightarrow \bbR^{n \times n}$ be such that for all $x \in \bbR^n$, $Q(x)$ is a positive definite matrix. Given $v, u \in \bbR^n$, we set $q_x(u,v) = u^{\top} Q(x)v$. As a result, we change the geometry of $\bbR^n$ and transform it into a Riemannian manifold with $q_x$ as the Riemannian inner product. For $v \in \bbR^n$, we will denote $\norm{v}_{q_x} =\sqrt{ q_x(v,v)}$.
We are now ready to present the \emph{geometry-aware orthogonal directions field}
\begin{equation}\label{eq:geometric_algorithm}
  \GOF(x) = - \nabla h(x) A(x) h(x) + \argmin_{v \in V(x)} \frac{1}{2} \norm{ v + Q^{-1}(x)\nabla f(x)}^2_{q_x} \, .
\end{equation}
By replacing $\OF$ with $\GOF$ in the algorithms of \Cref{sec:Main}, we obtain geometry-aware deterministic and stochastic algorithms. 

To describe a more geometric viewpoint on this algorithm, let us define a family of manifolds $\cM_{h(x)} = \{ y \in \bbR^n : h(y) = h(x) \}$ with Riemannian metric $g^{h(x)}$ such that $g^{h(x)}_x = q_x$ parameterized by $x \in \bbR^n$. In this case, we can prove that the projection in \eqref{eq:geometric_algorithm} exactly corresponds to the negative Riemannian gradient (see Lemma~\ref{lem:riemannian_gradient_by_projection} in Appendix~\ref{app:geometric}):
\begin{equation}\label{eq:riemannian_gradient_by_projections}
    -\Grad_{\cM_{h(x)}} f(x) = \argmin_{v \in V(x)} \frac{1}{2} \norm{ v + Q^{-1}(x)\nabla f(x)}^2_{q_x} \, .
\end{equation}
In particular, if the problem at hand has a geometrical structure, one might hope that a particular choice of $Q$ might reduce the computational costs (or even exhibit a closed form solution) of the right-hand side of \eqref{eq:riemannian_gradient_by_projections}. This idea explains the ``geometry-aware" nature of the algorithm. 

The main motivation for \geomalgo\ is the example of the orthogonal, or, more generally, Stiefel manifold. In this case, for $X \in \bbR^{n \times p}$, following the recent work of \citet{gao2022optimization}, the constraints are defined by $h(X) = X^\top X - \cI$ and the manifolds $\cM_{h(X)}$ correspond to $\mathrm{St}_{X^\top X}(p,n)$. For any of such $\cM_{h(X)}$, we obtain a natural Riemannian metric inherited from the Stiefel manifold $\mathrm{St}(p,n)$ through a family of diffeomorphisms. This provides us with a natural way of defining $Q$ and we obtain (see \cite{gao2022optimization} for a detailed discussion):
\[
    \Grad_{\cM_{h(X)}} f(X)  = \psi(X) X, \quad \text{where } \psi(X) = \left(\nabla f(X) X^\top - X (\nabla f(X))^\top\right).
\]
In particular, by setting $A(x) = \lambda \cI$, our algorithm exactly recovers the \texttt{landing} algorithm \citep{ablin2022fast,gao2022optimization}
\[
    X_{k+1} = X_k - \lambda \gamma_k \nabla H(X_k) - \gamma_k \psi(X_k) X_k.
\]
In other words, our approach is a generalization of the \texttt{landing} algorithm beyond the orthogonal and Stiefel manifolds.

Next we analyze \geomalgo\ under the following assumption. 
\begin{assumption}\label{hyp:riem_proj}
  There is a constant $C_q >0$ such that
  \begin{equation*}
    \sup_{x \in K}\max(\norm{Q^{-1}(x)}, \norm{Q(x)}) \leq C_q \, .
  \end{equation*}
\end{assumption}
The following theorem shows that \geomalgo\ exhibits the same type of rates than \algo. We emphasize that all our analysis automatically holds for the \texttt{landing} algorithms as a special case. In particular, we obtain new and better rates for \texttt{landing}, where only an $\cO(\epsilon^{-6})$ rate was previously proven for the deterministic (and with decreasing step-sizes) version of the algorithm. Furthermore, we establish the convergence of the deterministic version of \texttt{landing} to the Stiefel manifold, which was only conjectured in \cite{ablin2022fast}. A full proof is provided in \Cref{app:geometric}.
\begin{theorem}\label{th:stief_rates}
  Let \Cref{hyp:cont_model}--\ref{hyp:riem_proj} hold. Then there exists $\overline{M}_q$ (detailed in the proof) such that for all $M \geq \overline{M}_q$, denoting $\gamma_{\max}>0$ as $\gamma_{\max} = \min(\alpha_m^{-1}, C_q^{-1}(L_f + M L_h)^{-1})$, the following holds:
  \begin{enumerate}
      \item If $\sigma = 0$ and $\gamma_k \equiv \gamma$, with $\gamma \leq \gamma_{\max}$, then:
        \begin{equation*}
\inf_{0 \leq k \leq N-1} \norm{\GOF(x_k)}^2 \leq    \inf_{0 \leq k \leq N-1}{\norm{v_k}^2} \leq 2 C_q\frac{\Lambda_M(x_0) - \Lambda_{M}(x_n)}{N \gamma} \, .
  \end{equation*}
  Furthermore, $\norm{\GOF(x_k)} \rightarrow 0$ and any accumulation point of $(x_k)$ is a critical point of Problem~\ref{eq:main_opt_prob}.
  \item Otherwise, fix some constant $\bar D$, $N >0$ and $\gamma = \min(\gamma_{\max}, \bar D (\sigma \sqrt{N})^{-1})$. If $\gamma_k \equiv \gamma $ and $\hat k$ is uniformly sampled in $\{0, \dots, N-1\}$, then 
  \begin{equation*}
      \bbE_k [\norm{\GOF(x_{\hat k})}] \leq \frac{2C_qD_M(L_f + M L_h+ \alpha_m)}{N} + \frac{C_q\sigma}{\sqrt{N}}\left( \bar D (L_f + M L_h) + \frac{2 D_M}{\bar D}\right) \, .
  \end{equation*}
  \end{enumerate}

\end{theorem}

