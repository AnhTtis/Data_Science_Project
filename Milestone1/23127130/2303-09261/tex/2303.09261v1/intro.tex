\section{Introduction}
%\sho{There is a mix between $\alpha$, $\alpha_m$ in the paper (e.g. \Cref{hyp:cont_model}-\ref{hyp:hA_eigenv}), choose one and uniformize. non convex, nonconvex, or non-convex?}\dan{I saw non-convex more often in optimization papers, let's stuck on it.}

% \dan{Algorithm name suggestions:
% \begin{itemize}
%     \item Infeasible Riemannian Flow IRF (to underline infeasibility);
%     \item Orthongonal Directions Constrained/Riemannian/- Gradient Flow/Method ODCGF/ODRGF/ODGF (ADMM-like name, very long but exact enough);
%     \item Orthogonalized Riemannian/Gradiennt Flow ORF/OFG (seems to be better than Orthongonal/Orthogonalization);
% \end{itemize}

% Paper name suggestions:
% \begin{itemize}
%     \item Near-optimal first-order/gradient-based infeasible method for non-convex constrained optimization: from non-linear equality constraints to Stiefel manifold. (We want to underline infeasibility and near-optimality, in the current name we don't have any of them.)
% \end{itemize}

% }

Given a continuously differentiable function $f\colon \bbR^n \rightarrow \bbR$, we consider the following optimization problem:
\begin{equation}\label{eq:main_opt_prob}
  \min_{x \in \cM} f(x) , \quad \textrm{ with } \cM := \{x \in \bbR^n : h(x) = 0 \} \, ,
\end{equation}
where $h: \bbR^n \rightarrow \bbR^{n_h}$ is continuously differentiable, non-convex, $n_h>0$ represents the number of constraints and $\cM$ denotes the feasible set. Optimization problems with nonlinear constraints naturally arise in a number of areas in machine learning,  with a specific emphasis on matrix manifold optimization (see \cite{li2019orthogonal,yang2007globally,sato2021riemannian}). Examples include independent component analysis \citep{hyvarinen2009independent,ablin2018faster}, Procrustes estimation  \citep{bojanczyk1999procrustes,turaga2008statistical,turaga2011statistical}
and the orthogonally normalized neural networks in deep learning \citep{bengio_unitaryNN16,li2019orthogonal,bansal18,qi2020deep}.

When the projection to $\cM$ is computationally tractable, projected gradient method -- in which a gradient descent step on $f$ is combined with the projection to $\cM$ --  is often the preferred option. The convergence guarantees for projected gradient methods are similar to those for an unconstrained gradient descent. Moreover, projected gradients are a first-order procedure and efficiently handle the stochastic case where only one estimator of $\nabla f$ is known (see, e.g., \cite{gha_lan13, gha_lan16}). When $\cM$ is a submanifold, a typical approach is to determine a search direction in the tangent space and then apply a retraction (see e.g. \cite{absil2012projection, bonn_riem13,boumal2019global,boumal2020intromanifolds,sato2021riemannian}). Similarly, retraction-based gradient algorithms have optimal convergence rates in both deterministic and stochastic settings (see \cite{riem_first_zha16, riem_sto_zha19}). These methods are feasible, i.e., the iterates always belong to $\cM$. In most cases, however, computing the retraction is expensive and requires solving a nontrivial optimization problem.

Infeasible methods (i.e. the iterates do not remain on $\cM$) such as augmented Lagrangian and proximally guided methods seek a solution to~\eqref{eq:main_opt_prob} by solving a sequence of optimization problems (see \cite{zichong2020rate, lin_ma_xu22, xie_wright_PAL19, hong17a}). Here, the iterates are not  feasible but are gradually pushed towards $\cM$. Nevertheless, each of the optimization problems in the inner loop might be computationally involved. Moreover, these methods are sensitive to the choice of hyperparameters both in theory (often the sub-problems in the inner loop are required to be convex) and in practice.  

In this work, we propose \algo, which stands for \emph{Orthogonal Directions Constrained Gradient Method}, a new class of algorithms that are both easy to implement and computationally inexpensive while retaining the good convergence properties of gradient descent. \algo\ realizes a trade-off between two opposite goals: minimizing $f$ and guaranteeing feasibility  of solutions. In order to set up the stage, we define for each $x \in \bbR^n$ \emph{i)}  $\nabla H(x) := (1/2) \nabla (\norm{h(x)}^2)$, and \emph{ii)}  $V(x)$ the vector space  orthogonal  to $\operatorname{span}(\{\nabla h_i(x) \}_{i=1}^{n_h})$. Then, a vanilla version of \algo\ produces iterates as follows:
\begin{equation}\label{eq:alg_int} x_{k+1} = x_k - \gamma_k \nabla H(x_k) - \gamma_k \nabla_V f(x_k) \, ,
\end{equation}
where $\gamma_k > 0$ is a step size and $\nabla_V f$ denotes the orthogonal projection of $\nabla f$ onto $V(x)$. Since $\nabla H(x)$ is orthogonal to $V(x)$ by construction, the iterates, even if allowed to be infeasible, are constantly shifted in the direction of $\cM$. Moreover, $-\nabla_V f$ strives to be as close as possible to $-\nabla f$, which is the direction of descent for $f$, and thus tends to minimize $f$; see \Cref{fig:orth_dir}.

%\begin{wrapfigure}{r}{0pt}
%\resizebox{60mm}{!}{\input{drawing.pdf_tex}}
%\caption{Orthogonal directions construction}
%\label{fig:orth_dir}
%\end{wrapfigure}
\begin{comment}
\begin{figure}[h]
\centering{
\resizebox{60mm}{!}{\input{drawing.pdf_tex}}
\caption{Construction of the orthogonal directions.}
\label{fig:orth_dir}
}
\end{figure}
\end{comment}

\begin{figure}[h]
  \centering{

  \resizebox{60mm}{!}{\input{drawing_test2.pdf_tex}}\hspace{30pt}
\resizebox{60mm}{!}{\input{drawing_test.pdf_tex}} 
\caption{Construction of the orthogonal directions}\label{fig:orth_dir}
  }
\end{figure}


\algo\ is a first-order algorithm that only requires a projection onto the vector space $V(x)$ at each iteration. It is scalable, simple to implement, and can be easily generalized to the stochastic setting. In addition, we provide \algo\ with strong theoretical guarantees: we establish convergence bounds that are equivalent to those of (unconstrained) gradient descent in both deterministic and stochastic settings: $\cO(\varepsilon^{-2})$ and $\cO(\varepsilon^{-4})$, respectively. We also present \redalgo\, a computationally cheaper version of \algo\ where $V(x)$ is replaced by a hyperplane orthogonal to $\nabla H(x)$. The advantage \redalgo\ is that the projection (i.e. the computation of $\nabla_V f(x)$) now comes essentially for free, which has the potential to efficiently solve high-dimensional problems, $n-n_h \gg 1$.
This version of \algo\ is inherently non-smooth and we obtain a $\cO(\varepsilon^{-3})$ convergence rate in the deterministic setting and $\cO(\varepsilon^{-4})$ in the stochastic setting. 

\algo\ is closely related to two recently proposed methods. First, the algorithm developed in \citet{muehlebach2022constraints}, when applied to equality constraints, is a special instance of \algo. Our convergence finite-time complexity analysis extends \citet{muehlebach2022constraints} to the non-convex setting (see also \cite{stmjm_ifac22, askew_lsm22}). 
Second, \algo\ is closely related to the \texttt{landing} algorithm proposed by \citet{ablin2022fast,gao2022optimization}. The \texttt{landing} algorithm deals with the case where $\cM$ is the Stiefel (or orthogonal) manifold: it avoids retractions and requires only a few matrix multiplications at each iteration. For the orthogonal manifold case (and in the deterministic setting), \cite{ablin2022fast} provides convergence guarantees, however, with a suboptimal convergence rate. Following \cite{gao2022optimization}, we show that by choosing an appropriate metric for the projection on $V(x)$, we obtain a closed-form solution for $\nabla_V f$, and we recover \texttt{landing} as a specific instance of \algo. As a consequence, when $\cM$ is the Stiefel manifold, we significantly extend the analysis of \cite{ablin2022fast} by establishing near-optimal rates both in the deterministic and stochastic framework. In particular, we show that \texttt{landing} indeed converges to $\cM$, which was only conjectured in \cite{ablin2022fast}.\\
\vspace{5pt}
\noindent\textbf{Main contributions.}
\vspace{-10pt}
\begin{itemize}%[nosep]%,leftmargin=0pt]
    \item We propose \algo, a novel family of algorithms that do not require projections or retractions to the feasible set $\cM$.
    \item We establish convergence rates that coincide with the one of gradient descent in the non-convex setting: $\cO(\varepsilon^{-2})$ in the deterministic and $\cO(\varepsilon^{-4})$ in the stochastic cases; see \Cref{sec:Main}.
    \item We propose \redalgo\ which significantly decreases the computational cost per iteration. The cost of this computational reduction is a slightly degraded convergence rate: $\cO(\varepsilon^{-3})$ in the deterministic case and $\cO(\varepsilon^{-4})$ in the stochastic case; see \Cref{sec:reduced}.
    \item We introduce \geomalgo, a geometry-aware version of \algo, which is applicable when an underlying geometrical structure of the problem is available. In particular, the \texttt{landing} method of \cite{ablin2022fast} is a particular version of \geomalgo. Convergence guarantees of \geomalgo\ are identical to the one \algo; see \Cref{sec:stiefel}.
    \item We perform various numerical experiments on high-dimensional problems that highlight the claim on efficiency of our method; see \Cref{sec:numerics}.
\end{itemize}

\noindent\textbf{Notations.} For a smooth function $f : \bbR^n \rightarrow \bbR$, $\nabla f(x) \in \bbR^n$ denotes its gradient. For a smooth function $h: \bbR^n \rightarrow \bbR^{n_h}$, we denote $\nabla h(x) \in \bbR^{n \times n_h}$ the matrix in which the $i$-th column is $\nabla h_i(x)$. Given a matrix $A$, $\ker A$ denotes its kernel.
 Given a probability space $(\Omega, \cA, \bbP)$ and a filtration $(\mcF_k)$, $\bbE_k[\cdot]$ is denoted as $\bbE[\cdot | \mcF_k]$. $P_V$ denotes the orthogonal projector on the linear subspace $V$.

\noindent \textbf{Submanifolds.} A set $\cM \subset \bbR^n$ is called a submanifold of dimension $n-n_h$, with $n_h \leq n$, if for every point $x\in \cM$ there is a neighborhood $U \subset \bbR^n$ of $x$ and a smooth function $h: U \rightarrow \bbR^{n_h}$ such that $h^{-1}(0) = U \cap \cM$ and $\nabla h$ is of full rank on $U$. 
The tangent plane of $\cM$ at $x$ is $\cT_{x}\cM = \ker (\nabla h(x)^{\top})$.
For a smooth function $f : \bbR^n \rightarrow \bbR$ and $x \in \cM$, $\Grad f(x) = P_{\cT_x \cM} \nabla f(x)$ denotes the Riemannian gradient of $f$ at $x$ in the case when the Riemannian metric is inherited from the ambient space.
More generally, for  $(\cM, g)$ a manifold equipped with a Riemannian metric $g$, $\Grad_{\cM} f(x) \in \cT_x \cM$ denotes the Riemannian gradient: a vector in the tangent plane such that for any $\xi \in \cT_x \cM$,  $g_x(\Grad_{\cM} f(x), \xi) = \nabla f(x)^{\top} \xi$.

 
