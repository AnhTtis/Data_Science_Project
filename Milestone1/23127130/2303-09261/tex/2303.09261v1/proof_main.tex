
\section{Supplementary proofs}
\subsection{Proof of \Cref{th:gen_rates}}\label{proof:gen_rates}
We preface the proof with two elementary results.
\begin{lemma}\label{lm:aff_proj}
  Consider $n_h \leq n$, $y \in \bbR^n$, $b \in \bbR^{n_h}$ and $W \in \bbR^{n \times n_h}$, with $W$ being of full rank. It holds that:
  \begin{equation*}
    \argmin_{v \in \bbR^n : W^{\top} v = b} \frac{1}{2} \norm{ v + y}^2 = - y + W(W^{\top} W)^{-1} (W^{\top} y + b)
  \end{equation*}
\end{lemma}
\begin{comment}
Eric: this is trivial
\begin{proof}
Denote $v_\star$ the solution of the optimization problem. To compute it we write down the  Lagrangian, $\cL(v, \lambda)$, with $\lambda \in \bbR^{n_h}$:
\begin{equation*}
  \cL(v, \lambda) = \frac{1}{2} \norm{ v + y}^2 + \lambda^{\top} (W^{\top} v - b) \, .
\end{equation*}
Since $v_\star$ is a solution of a quadratic problem with affine constraints, we obtain from the KKT conditions that there is $\lambda_\star \in \bbR^{n_h}$ such that:
\begin{equation*}
  \begin{split}
        v_\star &= \argmin_{v \in \bbR^n} \cL(v, \lambda_\star) \, .
  \end{split}
\end{equation*}
Thus, $\frac{\partial}{\partial v}\cL(v_\star, \lambda_\star) = 0$, which implies $v_\star = - y - W \lambda_\star^{\top}$. Since $W^{\top} v_\star = b$, we obtain:
\begin{equation*}
-  W^{\top} W \lambda_\star^{\top} - W^{\top} y = b \, .
\end{equation*}
Thanks to the fact that $W$ is of full rank, the matrix $W^{\top} W$ is invertible and thus:
\begin{equation*}
  \lambda_\star^{\top} = - (W^{\top} W)^{-1} (W^{\top} y+ b) \, .
\end{equation*}
Substituting $\lambda_\star$ with this expression in the equality $v_\star = - y - W \lambda_\star^{\top}$ completes the proof.
\end{proof}
\end{comment}
\begin{corollary}\label{cor:proj_Vx}
If $x \in \bbR^n$ is such that $\nabla h(x)$ is of full rank, then:
  \begin{equation}\label{eq:proj_auxtan}
    -\nabla_V f(x) = \argmin_{v \in V(x)} \frac{1}{2} \norm{v + \nabla f(x)}^2 = - \nabla f + \nabla h(x) D_h(x) \nabla h(x)^{\top} \nabla f(x)\, ,
  \end{equation}
  where $D_h(x) := (\nabla h(x)^{\top} \nabla h(x))^{-1}$.
\end{corollary}



The following proposition is the key element in our proof. It mainly follows from a Taylor expansion of $\Lambda_M$.


\begin{proposition}[Discrete Lyapunov function]\label{prop:lyap_stoch}
    Let \Cref{hyp:cont_model}--\ref{hyp:disc_model} hold and let $\overline{M}$ be the one of Theorem~\ref{th:cont_time}. If for all $k \in \bbN$, $\gamma_k \leq \alpha_m^{-1}$, then for all $M \geq \overline{M}$, it holds:
\begin{equation}\label{eq:lyap_sto_dec}
    \bbE_k [\Lambda_M(x_{k+1})]- \Lambda_M(x_k) \leq - \gamma_k \norm{ v_k}^2 \left( 1 - \frac{L_f + M L_h}{2} \gamma_k\right) + \frac{L_f + ML_h}{2}\sigma^2 \gamma_k^2 \, .
\end{equation}
\end{proposition}
\begin{proof}
Since $f$ is gradient Lipschitz continuous on $K$ and $\bbE_k[\eta_{k+1}] = 0$, it holds that
\begin{equation}\label{eq:f_disc_decr}
    \begin{split}
        \bbE_k[f(x_{k+1})] - f(x_k) &\leq \gamma_k \nabla f(x_k)^{\top}  v_k + \frac{L_f}{2}\gamma_k^2 \bbE_k[\norm{v_k + \eta_{k+1}}^2] \\
        &\leq \gamma_k \nabla f(x_k)^{\top}  v_k  + \frac{L_f}{2}\gamma_k^2 (\norm{v_k}^2 + \sigma^2) \\
        &\leq - \gamma_k \norm{v_k}^2\left( 1 - \frac{L_f}{2}\gamma_k\right) + \gamma_k(\nabla f(x_k) + v_k)^{\top} v_k + \frac{L_f}{2}\gamma_k^2\sigma^2\\
        &\leq - \gamma_k \norm{v_k}^2\left( 1 - \frac{L_f}{2}\gamma_k\right) + \gamma_k M_1 \norm{h(x_k)} + \frac{L_f}{2}\gamma_k^2\sigma^2 \, ,
    \end{split}
\end{equation}
where the second inequality follows from \Cref{hyp:disc_model}-\ref{hyp:var_bound} and the last inequality follows from Equation~\eqref{eq:err_f}.

Similarly, since $h$ is gradient Lipschitz on $K$, we obtain:
\begin{equation}\label{eq:h_disc_decr}
\begin{split}
        \bbE_k[\norm{h(x_{k+1})}]  &\leq \bbE_k[\norm{h(x_{k}) + \gamma_k \nabla h(x_k)^{\top}( v_k + \eta_{k+1})}] + \frac{L_h}{2} \gamma^2_k \bbE_k[\norm{v_k + \eta_{k+1}}^2] \\
            &\leq \norm{h(x_{k}) + \gamma_k \nabla h(x_k)^{\top}v_k} +  \frac{L_h}{2} \gamma^2_k \norm{v_k}^2 + \frac{L_h}{2} \gamma_k^2 \sigma^2 \\
            &\leq \norm{ h(x_k) - \gamma_k \nabla h(x_k)^{\top} \nabla h(x_k) A(x_k) h(x_k)} + \frac{L_h}{2} \gamma^2_k (\norm{v_k}^2 + \sigma^2) \\
            &\leq (1 - \alpha_m \gamma_k) \norm{h(x_k)} + \frac{L_h}{2} \gamma^2_k (\norm{v_k}^2 + \sigma^2) \, ,
\end{split}    
\end{equation}
where in the second inequality we have used that $\eta_{k+1} \in V(x_k)$ and in the last inequality our choice of $(\gamma_k)$ with \Cref{hyp:cont_model}-\ref{hyp:hA_eigenv}.

Combining Equations~\eqref{eq:f_disc_decr} and \eqref{eq:h_disc_decr} with the fact that $M \geq \overline{M} = M_1/\alpha_m$ completes the proof.
\end{proof}

The following corollary is obtained by telescoping Equation~\eqref{eq:lyap_sto_dec}.
\begin{corollary}\label{cor:lyap_telesc}
    Under the assumptions of Theorem~\ref{th:gen_rates}, for $N >0$, it holds that: 
    \begin{equation}
        \sum_{i=0}^{N-1} \bbE[\norm{v_i}^2] \leq 2 \frac{\bbE[\Lambda_M(x_0)] - \bbE[\Lambda_M(x_{N-1})]}{\gamma} + N \gamma (L_f + M L_h) \sigma^2 \,.
    \end{equation}
\end{corollary}
\subsubsection{Deterministic case: $\sigma = 0$}\label{proof:det_rates}
Fix $\sigma = 0$, from Corollary~\ref{cor:lyap_telesc} we obtain:
 \begin{equation*}
    \gamma \sum_{i=0}^{N-1} \norm{v_i}^2 \leq 2(\Lambda_{M}(x_0) - \Lambda_{M}(x_{N-1}))\, .
  \end{equation*}
  This implies Equation~\eqref{eq:det_rates} and shows that $ \norm{v_k} \rightarrow 0$. Now notice that
  \begin{equation*}
    \norm{v_k}^2 = \norm{\nabla_V f(x_k)}^2 + \norm{\nabla h(x_k) A(x_k) h(x_k)}^2 \, .
  \end{equation*}
  Since by ~\Cref{hyp:cont_model} both $A$ and $\nabla h$ are of full rank on $K$, this implies that $\norm{h(x_k)} \rightarrow 0$. Thus, if $x^*$ is an accumulation point of $(x_k)$, then it must satisfy $h(x^*) = 0$, or, in other words, $x^* \in \cM$. Finally, by continuity of $\OF$, we also have $0 = \lim_{k \rightarrow \infty}\norm{\OF(x_k)} = \norm{\OF(x^*)} = \norm{\Grad f(x^*)}$, which completes the proof.


\subsubsection{The general case}

Using Corollary~\ref{cor:lyap_telesc}, we obtain:
\begin{equation*}
\begin{split}
        \bbE\left[\norm{v_{\hat k}}^2\right] = \frac{1}{N}\sum_{i=0}^{N-1} \bbE[\norm{v_i}^2] &\leq 2\frac{\bbE[\Lambda_M(x_0)] - \bbE[\Lambda_M(x_{N-1})]}{N \gamma} + \gamma(L_f + M L_h) \sigma^2 \\
        &\leq  \frac{2 D_M}{N  \gamma} + \gamma(L_f + M L_h) \sigma^2
\end{split}
\end{equation*}
Therefore, 
\begin{equation*}
    \begin{split}
        \bbE\left[\norm{v_{\hat k}}^2\right] &\leq \frac{2 D_M}{N  \gamma} + \bar D (L_f + M L_h) \frac{\sigma}{\sqrt{N}} \\
        &\leq \frac{2 D_M}{N} \max(\alpha_m, L_f + M L_h, \bar D^{-1} \sigma \sqrt{N}) + \bar D (L_f + M L_h) \frac{\sigma}{\sqrt{N}} \\
        &\leq \frac{2 D_M}{N}\left( \alpha_m + L_f + M L_h\right) + \frac{\sigma}{\sqrt{N}} \left( \bar D (L_f + M L_h) + \frac{2 D_M}{\bar D}\right) \, ,
    \end{split}
\end{equation*}
which completes the proof.


\subsection{Safe step size}\label{proof:safe_step}
In this section, we discuss \Cref{hyp:disc_model}-\ref{hyp:iter_bound}. We establish that if the sequence $(\eta_{k+1})$ is bounded (which is the case in both the deterministic and finite-sum settings), a sufficiently small step size forces the algorithm to stay in $K$. To formulate this theorem, we denote $C_h, C_f$ as the Lipschitz constants of $h,f$ on $K$ and define $ C_A:= \sup_{x \in K} \norm{\nabla h A}$.


\begin{proposition}[safe step-size]\label{prop:safe_step}
  Assume \Cref{hyp:cont_model} and \Cref{hyp:disc_model}-\ref{hyp:fh_Lipgrad} and that there is a constant $b>0$ such that $\sup_{k} \norm{\eta_{k+1}} \leq b$. Consider $\delta >0$ and let $\gamma$ be defined as
  \begin{equation}\label{eq:safe_step}
    \gamma =  \min\left(\frac{1}{\alpha_m}, \frac{\delta}{C_h \sqrt{2 \left(C^2_A r_1^2 + C_f^2 + b^2\right)}}, \frac{\alpha_m}{2 L_h C_A^2 r_1}, \frac{2\alpha_m (r_1-\delta)}{3 L_h (C_f^2+ b^2)}\right)\, .
  \end{equation}
  If $(\gamma_k)$ is bounded by $\gamma$ and $\norm{h(x_0)} \leq r_1- \delta$, then the sequence $(x_k)$ produced by \algo\ remains in $K$.
\end{proposition}
\begin{proof}
Let $k \in \bbN$ be such that $\norm{h(x_k)} \leq r_1 - \delta$, we will show that $\norm{h(x_{k+1})} \leq r_1 - \delta$, which will complete the proof by an immediate induction. 

Denote $C_h$ the Lipschitz constant of $h$ on $K$ and notice that if $\norm{x_{k+1} - x_k} \leq \delta/C_h$, then $x_{k+1} \in K$. Indeed, assume that $x_{k+1} \notin K$ and denote for $t \in [0, 1]$, $x_t = x_k + t(x_{k+1}- x_k)$. Let $u = \inf \{t \in [0,1]: x_t \notin K \}$ and note that by continuity of $h$, $\norm{h(x_u)} = r_1$. This implies that
\begin{equation*}
    \delta \leq \norm{h(x_u)} - \norm{h(x_k)} \leq \norm{h(x_u) -h(x_k)} \leq C_h\norm{x_u - x_k} \leq u \delta \, .
\end{equation*}
Thus, $u$ must be equal to $1$, which is a contradiction. 

Now, 
\begin{equation*}
    \norm{x_{k+1} - x_k}^2 \leq 2\gamma_k^2 (\norm{v_k}^2 + b^2) \, ,
\end{equation*}
and since $\nabla_V f$ is orthogonal to $\nabla h A h$, we obtain:
\begin{equation}\label{eq:safe_vkbound}
    \norm{v_k}^2 \leq \norm{\nabla h(x_k) A(x_k) h(x_k)}^2 + \norm{\nabla_V f}^2 \leq C^2_A \norm{h(x_k)}^2 + C_f^2  \leq C^2_A r_1^2+ C_f^2\, ,
\end{equation}
Hence, we get,
\begin{equation*}
    \norm{x_{k+1} - x_k}^2 \leq 2 \gamma^2 (C^2_A r_1^2+ C_f^2 + b^2) \leq \frac{\delta^2}{C_h^2} \, ,
\end{equation*}
which shows that $x_{k+1}$ remain in $K$. Now, since $x_k, x_{k+1} \in K$ and $\nabla h$ is $L_h$-Lipschitz on $K$, it holds that:
\begin{equation*}
  \norm{h(x_{k+1})- h(x_k) - \gamma_k \nabla h(x_k)^{\top} v_k} \leq \frac{L_h}{2} \norm{x_{k+1} - x_k}^2 \, ,
\end{equation*}
where we have used the fact that $\eta_{k+1} \in V(x_k)$.
Thus,
\begin{equation*}
  \norm{h(x_{k+1})} \leq \norm{h(x_k) + \gamma_k \nabla h(x_k)^{\top} v_k} +\frac{L_h}{2} \norm{x_{k+1} - x_k}^2  \, .
\end{equation*}
Recall that $\cI \in \bbR^{n_h\times n_h}$ denotes the identity matrix, it holds that:
\begin{equation*}
  \begin{split}
        \norm{h(x_{k+1})} &\leq \norm{( \cI - \gamma_k \nabla h(x_k)^{\top} \nabla h(x_k) A(x_k)) h(x_k)}  +\frac{L_h}{2} \norm{x_{k+1} - x_k}^2  \\
         &\leq (1 - \alpha_m \gamma_k) \norm{h(x_k)}  +\frac{L_h}{2} \norm{x_{k+1} - x_k}^2 \,.
  \end{split}
\end{equation*}
Examining Equation~\eqref{eq:safe_vkbound} we can actually obtain a tighter upper bound on $\norm{x_{k+1} - x_k}^2$
\begin{equation*}
    \norm{x_{k+1} - x_k}^2 \leq 2 b^2 + 2 C^2_A r_1 \norm{h(x_k)} + 2 C_f^2\, .
\end{equation*}
And finally: 
\begin{equation*}
    \begin{split}
           \norm{h(x_{k+1})} \leq \norm{h(x_k)} + \gamma_k \norm{h(x_k)}( \gamma_k L_h C_A^2 r_1 -  \alpha_m ) + \gamma_k^2 L_h(C_f^2 + b^2) \, ,
    \end{split}
\end{equation*}
Since $\gamma \leq \alpha_m / (2L_h C_A^2 r_1)$, it holds that:
\begin{equation*}
  \norm{h(x_{k+1})} \leq \norm{h(x_k)} - \alpha_m \gamma_k \norm{h(x_k)}/2 + \gamma_k^2 L_h(C_f^2 + b^2) \, .
\end{equation*}
Therefore, if $\alpha_m \norm{h(x_{k}) } \geq 2 \gamma_k(L_h C_f^2 + b^2)$, then $\norm{h(x_{k+1})} \leq \norm{h(x_k)}$. Otherwise,
\begin{equation*}
  \norm{h(x_{k+1})} \leq \norm{h(x_{k})} +\gamma_k^2 L_h(C_f^2 + b^2)  \leq (L_h C_f^2 +b^2)\gamma_k \left(\frac{2}{\alpha_m} + \gamma_k \right) \leq \frac{3(L_h C_f^2 +b^2) \gamma}{2\alpha_m} \leq r_1 -\delta \, ,
\end{equation*}
where the last inequality comes from our choice of $\gamma$.
\end{proof}
\begin{remark}\label{rm:safe_step}
Although equation ~\eqref{eq:safe_step} may be intractable, it shows that the iterates remain in $K$ for a sufficiently small $\gamma$. Therefore, we can combine our algorithm with standard line search techniques (see \cite{NoceWrig06}). For example, if we set a threshold $\overline{\gamma}$, we check whether iterates with step sizes smaller than the threshold remain in $K$. If this is not the case, the threshold is divided by $2$. Such a change of the threshold value can only occur finitely often, so that the convergence rates of \Cref{th:gen_rates} remain true.
\end{remark}
