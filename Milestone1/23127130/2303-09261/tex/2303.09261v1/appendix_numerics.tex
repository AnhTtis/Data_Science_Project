\section{Numerical experiments}\label{app:numerics}

\subsection{Procrustes problem}

We provide additional numerical experiment on matrices $A \in \bbR^{q \times q},B \in \bbR^{p\times q}$ for $p=1000$ and $q=500$. Plots are presented in Figure~\ref{fig:procrustes_large_scale}. The large scale of the problem add a lot of challenges for all algorithms and notably it affects \redalgo\, convergence for constraints. However, we see that \geomalgo\ again outperforms all the baselines. All experiments for the Procrustes problem are performed in PyTorch on CPU with Intel Core-i7 processor. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.98\linewidth]{media/procrutes_large.pdf}
    \caption{Comparison of \algo\ (blue), \redalgo\ (orange), and \geomalgo\ (green) with Riemannian gradient descent with two different Riemannian metrics (red and purple). The upper plot shows the orthogonality error for $X$, the lower plot shows the convergence of the objective function (averages over 128 seeds).}
    \label{fig:procrustes_large_scale}
\end{figure}

\paragraph{Euclidean projection on tangent space}

Additionally, in the case of optimization over Stiefel manifold $\cM = \{ X \in \bbR^{p \times q}: X^\top X = \cI \}$, we discuss a way of projecting onto $V(x)$ (necessary for \algo) whihc is more efficient than solving a linear system of size $pq$. 

First, we notice that the tangent space can be described as follows (see \cite{gao2022optimization}):
\[
    V(X) = \{ Y \in \bbR^{p \times q} : Y^\top X + X^\top Y = 0 \}\,.
\]
To optimize over this set, we apply the Lagrange multipliers method
\begin{equation}\label{eq:stiefel_valpha_proj}
    \min_{Y \in \bbR^{p \times q}} \frac{1}{2} \norm{Y - U}_2^2, \quad \text{s.t. } Y^\top X + X^\top Y = 0.
\end{equation}
To solve this problem, we start from reparametrization $Y = (X^+)^\top Z$, with $Z$ a skew-symmetric matrix, and where $X^+$ denotes the pseudoinverse. In this way, we obtain:
\begin{equation}\label{eq:stiefel_valpha_proj_skew_sym}
    \min_{Z \in \bbR^{n \times p}} \frac{1}{2} \norm{(X^+)^\top Z - U}_2^2, \quad \text{s.t. } Z^\top + Z = 0\,.
\end{equation}
First order optimality condition implies
$
    X^+ ( (X^+)^\top Z - U) - (\Lambda + \Lambda^\top)= 0 ,
$
where $\Lambda$ are Lagrange multipliers. By properties of pseudoinverse for full column rank matrices we have $X^+ ( (X^+)^\top = (X^\top X)^{-1}$ and thus
\[
    Z = (X^\top X) (\Lambda + \Lambda^\top) + X^\top U.
\]
Next, we have to choose $M$ to satisfy $Z^\top + Z = 0$:
\[
    (X^\top X) (\Lambda + \Lambda^\top) + (\Lambda + \Lambda^\top) (X^\top X) = - (X^\top U + U^\top X)\, .
\]
Since the right-hand side is symmetric, we only need to compute, over $P$, any solution to the following \textit{Sylvester's equation}:
\[
    (X^\top X) P + P(X^\top X) = - 2(X^\top U + U^\top X)
\]
and symmetrize it: $\Lambda = (P + P^\top)/2$. The solution to this system could be easily found using SVD of a matrix $X$. Notice that since $X^\top X \to I$ we may expect that all operations will be numerically stable. The total complexity of is equal to $\cO(pq^2)$.




\subsection{Hanging chain}
We compute the shape of a hanging chain as a numerical example. The chain has length $l=10$ and is divided into $N+1$ segments of equal length $r=10/(N+1)$. Each two segments are connected by a joint and a torsion spring, which models the stiffness of the chain. The torsion spring has a spring constant of $k_\text{s}=100$. The chain is suspended at positions $(0,0)$ and $(9,0)$, and an example with three nodes ($N=3$) is shown in \Cref{fig:chain}. The optimization variables are given by the coordinates $\xi_i=(x_i,y_i)$ of the nodes, $i=1,\dots,N$, and a non-convex distance constraint restricts the length of each segment to $r$. We compute the shape of the chain by minimizing its potential energy, i.e.,
\begin{equation}
\begin{split}
    \min_{(\xi_1,\dots,\xi_N)\in\mathbb{R}^{2N}} &\frac{1}{N^3} \sum_{i=1}^N  \left( \frac{k_\text{s}}{r^4}(\xi_{i-1}-\xi_i)\T (\xi_{i+1}-\xi_i) + y_i \right) \\ \text{s.t.}& \quad \sqrt{(\xi_{k-1}-\xi_k)\T (\xi_{k-1}-\xi_k)}\leq r,~ k=1,2,\dots,N+1,
\end{split}
 \label{eq:numEx_app}
\end{equation}
where $\xi_{0}=(0,0)$ and $\xi_{N+1}=(9,0)$ are the two endpoints. We note that the first term of the objective function contains a discrete approximation to the curvature of the chain that models the spring potential, while the second term corresponds to the gravitational potential. This example is motivated by the fact that it leads to a simple problem formulation that includes non-convex distance constraints, but also allows us to scale $N$ to values of $10^5$ or more. Finally, Euler-Bernoulli beam theory gives us a reasonable initial estimate for the start of the optimization. All calculations are performed in MATLAB on a standard laptop (Dell XPS 15 with an Intel Core-i7 processor, 32 gigabytes of RAM, and a Windows operating system).

We start with a chain of $10,001$ segments, leading to an optimization problem with $20,000$ decision variables and $10,001$ nonlinear constraints. We compare the three algorithms: \algo~ with $A(x) = \alpha (\nabla h(x)^{\top} \nabla h(x))^{-1}$ (denoted simply by \algo~), \redalgo~, and an extended Lagrangian approach. \Cref{fig:chain} (right) shows the initial estimate and the final result as computed by \algo~ (the result of the other algorithms is similar). 
The step size for \algo~ is set to $\gamma_k=T$, where $T=0.1/k_\text{s}$ and $\alpha=0.05/T$; the step size for \redalgo\ is set to  $\gamma_k=T$ for $k\leq 100$ and $\gamma_k=T/\sqrt{k-100}$ for $k > 100$ (the scaling with $1/k_\text{s}$ results from the Hessian of \eqref{eq:numEx_app}). \Cref{fig:objfunconst2} (main text) shows the value of the objective function and the root mean square error of the constraint violation over the course of the optimization.
We find that \algo~ leads to fast convergence in terms of constraint violations and function value, while the convergence of the augmented Lagrangian approach and \redalgo~ is much slower. Moreover, the performance of the augmented Lagrangian is quite sensitive to the initial value of the dual variable, which may even lead to divergence. In contrast, setting the step size of \algo~ and \redalgo~ is very simple. 
\Cref{fig:objfunconst} illustrates that \redalgo~ must be executed with decreasing step size; if a constant step size is used, the constraint violations remain as shown in the left panel, which is also consistent with our theoretical analysis. 
The right panel shows the execution time per iteration of the different algorithms by computing a moving average over past iterations. We conclude that  \redalgo~ and the augmented Lagrangian require only about one-fifth of the time of \algo\ for a single iteration. 
This can be explained by the fact that \algo\ requires the solution of a linear system of size $30,0001 \times 30,0001$ at each iteration (we have exploited parsimony but have not taken into account the special structure of the equality constraints in \eqref{eq:numEx_app}). 
Although  \redalgo~ (and the augmented Lagrangian) have lower execution time per iteration, it also converges much more slowly.

In order to highlight the potential of \redalgo~, we run the same example for $N=200,000$, which results in a large-scale optimization problem with $400,000$ decision variables and $200,001$ non-convex equality constraints. In this case, solving the resulting Karush-Kuhn-Tucker system at every iteration, which is required for \algo~, becomes challenging. However, \redalgo~ can still be applied and requires only about 0.085 seconds for executing a single iteration. The resulting function values and the evolution of the constraint violations are shown in \Cref{fig:large}.

\begin{figure}
%\newlength{\figurewidth}
%\newlength{\figureheight}
\setlength{\figurewidth}{.45\columnwidth}
\setlength{\figureheight}{.125\columnwidth}
\resizebox{75mm}{!}{\input{drawingChain.pdf_tex}} \hspace{-20pt}%
\input{media/hangingChain.tikz}
\caption{The figure shows a sketch of the hanging chain (left), the results arising from optimizing \eqref{eq:numEx_app} (black, right) and the results predicted by the Euler-Bernoulli beam theory (red, right). The predictions from the Euler-Bernoulli beam theory are used as initial guess.}
\label{fig:chain}
\end{figure}

\begin{figure}
%\newlength{\figurewidth}
%\newlength{\figureheight}
\setlength{\figurewidth}{.35\columnwidth}
\setlength{\figureheight}{.2\columnwidth}
\input{media/gnormA.tikz} %
\input{media/exectimeAverage.tikz}
\vspace{-10pt}
\caption{The figure on the left shows that \redalgo~ with a constant step size $\gamma_k=T$ does not converge and leads to significant constraint violations. The figure on the right shows the execution time per iteration of the different algorithms (moving average over past iterations). We note that the curve of the augmented Lagrangian and \redalgo~ are essentially superimposed.}
\label{fig:objfunconst}
\end{figure}

%\begin{figure}
%\setlength{\figurewidth}{.25\columnwidth}
%\setlength{\figureheight}{.2\columnwidth}
%\input{media/exectimeAverage.tikz}%
%\input{media/fvalue2.tikz}%
%\input{media/gnorm2.tikz}
%\caption{The figure compares the execution time per iteration of the MJ orthogonalizing field Algorithm and the vanilla orthogonalizing field Algorithm (left). It also shows the convergence of the vanilla orthogonalizing field Algorithm both in terms of function value (center) as well as in terms of constraint violation (right) with a step size of $\gamma_k \sim 1/\sqrt{k}$ for large $k$. We note that the convergence is relatively slow.}
%\label{fig:decreasingStep}
%\end{figure}

\begin{figure}
\setlength{\figurewidth}{.35\columnwidth}
\setlength{\figureheight}{.2\columnwidth}
\input{media/fvalueLarge.tikz}%
\input{media/gnormLarge.tikz}
\caption{This figure shows the results from applying \redalgo~ with $\gamma_k \sim 1/\sqrt{k}$ for large $k$ to \eqref{eq:numEx_app} with $N=200,000$. This leads to an optimization problem with $400,000$ decision variables and $200,001$ non-convex constraints. The evolution of the function value is shown on the left, whereas the evolution of the constraint violations is shown on the right.}
\label{fig:large}
\end{figure}