\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{todonotes}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{amsthm,thmtools}

\newcommand{\ab}{{\boldsymbol a}}
\newcommand{\bb}{{\boldsymbol b}}
\newcommand{\cb}{{\boldsymbol c}}
\newcommand{\s}{{\boldsymbol s}}
\newcommand{\xb}{{\boldsymbol x}}
\newcommand{\yb}{{\boldsymbol y}}
\newcommand{\zb}{{\boldsymbol z}}
\newcommand{\wb}{{\boldsymbol w}}
\newcommand{\nb}{{\boldsymbol n}}
\newcommand{\fb}{{\boldsymbol f}}
\newcommand{\gb}{{\boldsymbol g}}

\newcommand{\Ab}{{\boldsymbol A}}
\newcommand{\Db}{{\boldsymbol D}}
\newcommand{\Ib}{{\boldsymbol I}}

\newcommand{\Ed}{{\mathbb E}}
\newcommand{\Rd}{{\mathbb R}}
\newcommand{\Cd}{{\mathbb C}}

\newcommand{\Nc}{{\mathcal N}}
\newcommand{\Sc}{{\mathcal S}}
\newcommand{\Pc}{{\mathcal P}}

\DeclareMathOperator*{\argmin}{\arg\!\min}

\newcommand{\code}[1] {\texttt{#1}}

%------------tikz plot helpers------------------------
\newcommand{\tickYtop}[1]{\raisebox{0ex}[1ex][1ex]{#1}}
\newcommand{\tickYtopD}[1]{\raisebox{-1.5ex}[0ex][0ex]{#1}}
\newcommand{\tickPSNR}{\tickYtopD{PSNR[db]}}
\newcommand{\tickNview}[1]{\smash{\# view$=$}{$#1$}\hspace*{1em}}

\definecolor{C0}{rgb}{0.121569, 0.466667, 0.705882}
\definecolor{C1}{rgb}{1.000000, 0.498039, 0.054902}
\definecolor{C2}{rgb}{0.172549, 0.627451, 0.172549}
\definecolor{C3}{rgb}{0.839216, 0.152941, 0.156863}
\definecolor{C4}{rgb}{0.580392, 0.403922, 0.741176}
\definecolor{C5}{rgb}{0.549020, 0.337255, 0.294118}
\definecolor{C6}{rgb}{0.890196, 0.466667, 0.760784}
\definecolor{C7}{rgb}{0.498039, 0.498039, 0.498039}
\definecolor{C8}{rgb}{0.737255, 0.741176, 0.133333}
\definecolor{C9}{rgb}{0.090196, 0.745098, 0.811765}
\definecolor{trolleygrey}{rgb}{0.5, 0.5, 0.5}

\newtheorem{theorem}{\bf{Theorem}}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{\bf{Lemma}}
\newtheorem{proposition}{\bf{Proposition}}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{\bf{Definition}}
\newtheorem{remark}{\bf{Remark}}
\newtheorem{example}{\bf{Example}}
\def\QED{~\rule[-1pt]{5pt}{5pt}\par\medskip}


\newcommand{\add}[1] {\textcolor{blue}{#1}} % for revision
\newcommand{\rev}[1] {\textcolor{red}{#1}} % for revision


% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{TPDM: Solving 3D Inverse Problems and Generate Voxels Volume with Pre-Trained Two Perpendicular 2D Diffusion Models}

\author{Suhyeon Lee$^*$\\
KAIST\\
Daejeon, Republic of Korea\\
{\tt\small suhyeon.lee@kaist.ac.kr}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Hyungjin Chung$^*$\\
KAIST\\
Daejeon, Republic of Korea\\
{\tt\small hj.chung@kaist.ac.kr}
\and
JLK1
\and
JLK2
\and
Professor
}

\maketitle
\def\thefootnote{*}\footnotetext{These authors contributed equally to this work}\def\thefootnote{\arabic{footnote}}
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% TITLE FIGURE
\begin{figure*}[ht]
\begin{center}
\includegraphics[width=\textwidth]{tpdm_title.png}
\end{center}
   \caption{The visualization of the proposed method (left) and the results of solving the 3D inverse problem (center: MR-ZSR, right: CS-MRI) using it. First row: measurement, second row: proposed method, third row: ground truth. MR-ZSR: slice thickness 5mm$\rightarrow$1mm super-resolution, CS-MRI: Poisson sub-sampling of $\times48$ acceleration.}
\label{fig:tpdm_title}
\end{figure*}


%%%%%%%%% ABSTRACT
\begin{abstract}
The diffusion model emerged as the cutting-edge and mainstream image generation model due to its numerous advantages. In particular, the conditional sampling capabilities of diffusion models have provided a way to solve inverse problems using diffusion models prior to data distributions, and many studies have validated its effectiveness. Nevertheless, most of the diffusion-based inverse problem-solving methods deal with 2D images, and even the recently published 3D inverse problem-solving methods still do not fully exploit the 3D distribution prior to a global volumetric view. By modeling the 3D data distribution as a product distribution of 2D distributions sliced in different directions, we propose a novel method to solve the 3D inverse problem and generate 3D voxel volumes with only two perpendicular trained 2D diffusion models. This method effectively circumvents the curse of dimensionality, has been shown to perform highly effectively on three-dimensional medical image reconstruction tasks such as MRI Z-axis super-resolution, compressed sensing MRI, and sparse-view CT highly effectively, and demonstrated that high-quality voxel volumes can also be generated unconditionally without any guidance. In particular, MRI Z-axis super-resolution was pioneered for the primary time, with remarkable results both technically and clinically.
\end{abstract}






%%%%%%%%% BODY TEXT
\section{Introduction}

Since introducing the diffusion probabilistic model~\cite{sohl2015deep, ho2020denoising, song2021scorebased} (DPM), it has been considered a mainstream generative model in the image field. The diffusion model has already achieved state-of-the-art performance~\cite{dhariwal2021diffusion, rombach2022high} compared to other generative models like GAN and is being actively studied as a generative model in fields other than image generation tasks such as speech, text, video, and graph~\cite{popov2021grad, li2022diffusionml, molad2023dreamix, vignac2022digress}. The diffusion model operates by explicitly learning the gradient of the log probability distribution $\nabla_{\bm{x}} \log p_{data}(\bm{x})$ (\ie score function) with neural networks. Sampling is performed by using Langevin dynamics~\cite{song2019generative} or solving the reverse SDE using the learned score function \cite{song2021scorebased}. 

The diffusion model has a slow sampling speed because it has to perform sequential sampling over several time steps (\ie noise levels), but it has a significant number of advantages over other generative models. Particularly noteworthy is its scalability, such as conditional sampling using Bayes' theorem for the score function during the sampling stage without retraining the score function model. Using this scalability and the conditional sampling method, beyond performing class conditional sampling~\cite{dhariwal2021diffusion, ho2021classifierfree}, the idea of solving a general inverse problem through a diffusion model was proposed by Song \etal~\cite{song2021scorebased}. Since that time, several enhancements and applications have emerged~\cite{kawar2022denoising, chung2022improving, chung2023diffusion, chung2022parallel, song2022solving}, and methodology for Diffusion prior-based Inverse problem-Solving (DIS) is regarded as state-of-the-art within the relevant field.

Most forgoing DIS methods focus on 2D applications, on the other hand, a recent work dubbed DiffusionMBIR~\cite{chung2022solving} proposes a method to tackle 3D inverse problems that arise in medical imaging. In essence, \cite{chung2022solving} leverages the diffusion model trained on the primary XY-plane as the prior, and augments the generative prior with the model-based prior, namely total variation (TV), to impose smoothness to the adjacent slices (Z-axis). While shown to be effective for various tasks, this idea is still limited in the sense that it does not fully learn the 3D prior distribution of the data. Namely, TV prior can only impose {\em local} dependencies that are derived from finite difference operators, while the true 3D prior should model {\em global} dependencies.

To tackle this drawback, we propose to drop the model-based prior completely and fully resort to 3D generative prior by modeling the 3D data distribution with the {\em product distribution} of 2D constituents. By such a strategy, our method, TPDM\footnote{\textbf{T}wo \textbf{P}erpendicular 2D \textbf{D}iffusion \textbf{M}odels for (un)conditional 3D generation.}, can effectively learn the 3D prior using only two 2D diffusion models: the primary model that acts on the XY-plane, and the auxiliary model that learns the YZ-plane. Notably, we are now able to model the global dependencies of the 3D structure which was impossible with the previous DiffusionMBIR approach and break free from performing sub-optimization schemes that were required to impose the TV constraint.

Posterior sampling using TDPM can be naturally derived from Bayes' rule without any additional computation costs arising from the auxiliary model\footnote{To clarify, there only exists an additional cost for training the auxiliary model. At inference, the cost remains the same or rather less than the previous DIS methods.}. It is worth mentioning that unlike DiffusionMBIR which is designed specifically for inverse problem solving, TDPM is a {\em fully general 3D generative model}, which can be used both for conditional and unconditional sampling.

TPDM has been tested in various 3D medical imaging reconstruction problems such as MRI Z-axis (\ie slice direction) super-resolution (MR-ZSR), compressed sensing MRI (CS-MRI), and sparse view CT (SV-CT), and has obtained the best results compared to existing methods. In particular, to the best of our knowledge, super-resolution of the Z-axis resolution of MRI images (MR-ZSR) is the first attempt, and excellent results have been obtained both technically and clinically (Fig.~\ref{fig:mrzsr_result_main}). We also demonstrated that TPDM can generate a very high-quality, complete 3D voxels volume as a pure generative model (Fig.~\ref{fig:uncond_result_main}).

In short, \textbf{1)} we developed a novel, simple, yet effective method to solve the 3D voxel inverse problem with two perpendicular 2D diffusion models as a 3D prior, which is a {\em fully unsupervised fashion} and has {\em no need to re-train} and \textbf{2)} applied it to various medical imaging reconstruction problems and achieved the best-known performance.  \textbf{3)} In particular, MRI Z-axis super-resolution was attempted and succeeded for the first time with the TPDM approach. \textbf{4)} Finally, we showed that TPDM also can be operated as a 3D generative model that generates high-quality 3D voxels volume.






%%%%%%%%%%
\section{Background}


\subsection{Score-based diffusion models}
The diffusion model~\cite{sohl2015deep, ho2020denoising, song2021scorebased} is a model family that defines a process that noises the original data gradually, called a \textit{forward process}, and expresses the generation process by performing the learned \textit{reverse process} of this noising process. Among them, the score-based diffusion model introduced by Song \etal~\cite{song2021scorebased} defines the forward process through the following stochastic differential equation (SDE). Throughout the diffusion process, the data $\bm{x}$ can be represented by $\bm{x}(t)=\bm{x}_t$, with continuous time index $t\in[0, 1]$. $\bm{x}_0 \sim p_{data}$ is the raw data distribution, and $\bm{x}_1 \sim p_0$ is the predefined prior distribution.
\begin{equation} \label{forward-sde}
    d\bm{x} = \bm{f}(\bm{x},t)dt + g(t)d\bm{w},
\end{equation}
where the function $\bm{f}: \mathbb{R}^d \times \mathbb{R} \rightarrow \mathbb{R}^d$ is the \textit{drift function}, and the function $g: \mathbb{R} \rightarrow \mathbb{R}$ is the \textit{diffusion coefficient}. $\bm{w}$ is the standard Wiener process, also called Brownian motion.

The reverse-time SDE of Eq.~\eqref{forward-sde} can be expressed as follows~\cite{anderson1982reverse, song2021scorebased} where $\bm{\Bar{w}}$ is also the standard Wiener process from time $1$ to $0$.
\begin{equation} \label{reverse-sde}
    d\bm{x} = [\bm{f}(\bm{x}, t) - g(t)^2 \nabla_{\bm{x}_t} \log p(\bm{x}_t)] dt + g(t)d\bm{\Bar{w}}
\end{equation}

In order to solve the reverse-time SDE for the generation process, a time-dependent score function $\nabla_{\bm{x}_t} \log p(\bm{x}_t)$ is required, which can be obtained by training the neural networks score function estimator $\bm{s_\theta}$ through the denoising score matching (DSM) objective~\cite{vincent2011connection, song2021scorebased} 
\begin{multline} \label{denoising-score-matching}
    \min_{\bm{\theta}} \mathbb{E}_{\xb_t|\xb_0,\xb_0} [ \lVert \bm{s_\theta} (\bm{x}(t), t) - \nabla_{\bm{x}_t} \log p(\bm{x}_t | \bm{x}_0) \rVert _2^2 ]
\end{multline}

Setting $\fb(\bm{x}, t)=0$ and $g(t) = \sqrt{ \frac{d[\sigma^2(t)]} {dt} }$ with the positive time-dependent increasing noise scale function $\sigma(t)$, we achieve the so-called variance exploding SDE (VE-SDE). The sampling process of VE-SDE can be effectively solved by replacing the score function with the score network which is trained by DSM objective and using e.g. predictor-corrector (PC) sampler introduced by Song \etal~\cite{song2021scorebased}.


\subsection{Diffusion posterial sampling}
Diffusion posterior sampling (DPS) is the state-of-the-art method to solve the general noisy inverse problem introduced by Chung \etal~\cite{chung2023diffusion} by using the diffusion model as a prior. The general forward model of the inverse problem can be defined as:
\begin{equation}
    \bm{y} = \bm{A}(\bm{x}_0) + \bm{n}, \qquad \bm{y}, \bm{n} \in \mathbb{R}^n, \bm{x} \in \mathbb{R}^d
\end{equation}
where $\bm{A}$ is the forward measurement function and $\bm{n}$ is the measurement noise. To solve the inverse problem using the diffusion prior, we can use Bayes' rule to obtain
\begin{multline} \label{cond-sample-diff}
    \nabla_{\bm{x}_t} \log p(\bm{x}_t | \bm{y}) = \nabla_{\bm{x}_t} \log p(\bm{x}_t) + \nabla_{\bm{x}_t} \log p(\bm{y} | \bm{x}_t) \\
                                                \simeq \bm{s_{\theta^*}}(\bm{x}_t, t) + \nabla_{\bm{x}_t} \log p(\bm{y} | \bm{x}_t).
\end{multline}

Nonetheless, since there is no explicit relationship between $\xb_t$ and $\yb$, we cannot use \eqref{cond-sample-diff} directly. To circumvent this problem, \cite{chung2023diffusion} proposes an approximation with a theoretically guaranteed upper bound on the approximation error
\begin{align}
    \label{dps-main}
    \nabla_{\bm{x}_t} \log p(\bm{y}|\bm{x}_t) \simeq \nabla_{\bm{x}_t} \log p(\bm{y} |\hat{\bm{x}}_0),
\end{align}
where
\begin{align}
\label{dps-twee}
    \hat{\bm{x}}_0 \coloneqq \mathbb{E}[\bm{x}_0|\bm{x}_t] = \bm{x}_t + \sigma^2(t) \nabla_{\bm{x}_t} \log p(\bm{x}_t)
\end{align}
is the Tweedie denoised estimate~\cite{efron2011tweedie}. When the measurement noise is Gaussian, one can use:
\begin{equation}
        \nabla_{\bm{x}_t} \log p(\bm{x}_t | \bm{y}) \simeq  \bm{s_{\theta^*}}(\bm{x}_t, t) - \lambda \nabla_{\bm{x}_t} \lVert \bm{A}(\Hat{\bm{x}}_0) - \bm{y} \rVert ^2 _2.
\end{equation}






%%%%%%%%%%
\section{Methods}


\subsection{TPDM}
% The 3D inverse problem is still very difficult to solve directly with a 3D diffusion model due to the problem represented by the curse of dimensionality. A na\"ive but quite effective way to circumvent this problem is to use a 2D diffusion model-based inverse problem-solving method by slice by slice. However, since these methods treat one axis of a 3D volumne as just batch direction, inconsistencies in the batch direction inevitably occur. That is, the diffusion model cannot use 3D information at all.
% To solve these problems, Chung. \etal \cite{chung2022solving} applied ADMM-TV projection between sampling steps of the unconditioned 2D diffusion model to meet measurement constraints and apply total variation loss in the batch direction (DiffusionMBIR). This method showed best performance than any other existing methods for various 3D inverse problems such as SV/LA-CT and CS-MRI. Although the total variation loss is an effective prior that can be applied in the batch direction, it is a very simple prior compared to the distribution prior in real 3D space. In particular, this method did not produce any results when used in a newly designed forward measurement model to solve the MR-ZSR problem described later.
Inspired by the drawbacks of DiffusionMBIR, we tried to develop a new method of applying priors that is closer to the actual 3D distribution than the batch direction total variation while maintaining the unsupervised and unnecessary retraining properties of DiffusionMBIR. Our simple yet effective solution is, by modeling the 3D data distribution as the product distribution, to additionally use an auxiliary diffusion model trained on slices in different directions of the volume, in addition to the primary 2D diffusion model to solve the inverse problem. If conditioning is not performed in the diffusion model sampling process, it can also be used to generate a 3D voxels volume, unlike DiffusionMBIR. This is called \textit{\textbf{TPDM}: a method of solving a 3D inverse problem and generating 3D voxels volume using pretrained Two Perpendicular 2D Diffusion Models}.~(Fig. \ref{fig:tpdm_title})


\subsection{Modeling data distribution} 
Our proposal is to model the data distribution as the {\em product} distribution given by
\begin{align}
\label{eq:prod}
p_{\theta, \phi}(\xb) = q_\theta^{(p)}(\xb)^\alpha q_\phi^{(a)}(\xb)^{\beta} \qquad\\
\begin{aligned}
=  [q_\theta^{(p)}(\xb_{[:,:,1]}) q_\theta^{(p)}(\xb_{[:,:,2]}) \cdots q_\theta^{(p)}(\xb_{[:,:,d_3]})]^\alpha \cdot \\ [q_\phi^{(a)}(\xb_{[1,:,:]}) q_\phi^{(a)}(\xb_{[2,:,:]}) \cdots 
q_\phi^{(a)}(\xb_{[d_1,:,:]})]^{\beta},
\end{aligned}
\end{align}
where $q_\theta^{(p)}(\xb)$ is the distribution modeled by the primary model parameterized with $\theta$, and $q_\phi^{(a)}(\xb)$ is the distribution modeled by the auxiliary model parameterized with $\phi$ for $\xb \in \mathbb{R}^{d_1 \times d_2 \times d_3}$. Moreover, $\alpha, \beta$ induces weighting between the two distributions according to the importance. Nevertheless, as all distributions must be expressed in 2D distributions for using 2D diffusion models, we assume that both $q_\theta^{(p)}$ and $q_\phi^{(a)}$ can be decomposed into independent 2D (slice) distributions.

When performing sampling from the prior distribution $p_{\theta, \phi}(\xb)$, we can directly use
\begin{multline}
    \nabla_{\xb_t} \log p(\xb_t) = \alpha \nabla_{\xb_t} \log q^{(p)}(\xb_t) + \beta \nabla_{\xb_t} \log q^{(a)}(\xb_t) \\
    = \alpha \Sigma_{i=1}^{d_3} \nabla_{\xb_t} \log q^{(p)}(\xb_{t, [:,:,i]}) + \beta \Sigma_{i=1}^{d_1} \nabla_{\xb_t} \log q^{(a)}(\xb_{t, [i,:,:]}) \\
    \simeq \alpha \Sigma_{i=1}^{d_3} \s^{3D}_{\theta^*}(\xb_{t, [:,:,i]}) + \beta \Sigma_{i=1}^{d_1} \s^{3D}_{\phi^*}(\xb_{t, [i,:,:]}),
\end{multline}
\begin{equation}
\begin{aligned}
\text{where} \quad
\begin{cases}
    \s^{3D}(\xb_{t, [:,:,i]})_{[:,:,i]} = \s(\xb_{t, [:,:,i]}) \\
    \s^{3D}(\xb_{t, [:,:,i]})_{[\text{otherwise}]} = 0
\end{cases} \\
\text{and} \quad 
\begin{cases}
    \s^{3D}(\xb_{t, [i,:,:]})_{[i,:,:]} = \s(\xb_{t, [i,:,:]}) \\
    \s^{3D}(\xb_{t, [i,:,:]})_{[\text{otherwise}]} = 0
\end{cases}
\end{aligned}
\end{equation}
 % which used the plug-in approximation $\s_{\theta^*}(\xb_{t,[:,:,i]}) \simeq \nabla_{\bm{x}_{t,[:,:,i]}} \log q_\theta^{(p)}(\xb_{t,[:,:,i]})$, $\s_{\phi^*}(\xb_{t,[i,:,:]}) \simeq \nabla_{\bm{x}_{t,[i,:,:]}} \log q_\phi^{(a)}(\xb_{t,[i,:,:]})$ with 2D score estimator $\bm{s}$ and $\nabla_{\bm{x}_{t,[:,:,j]}} \log q_\theta^{(p)}(\xb_{t,[:,:,i]}) = 0$, $\nabla_{\bm{x}_{t,[j,:,:]}} \log q_\phi^{(a)}(\xb_{t,[i,:,:]}) = 0$ when $i \neq j$ by our slice independence assumption.
 which used the trained 2D score estimator $\bm{s}(\cdot)$ and 2D-to-3D zero-fill score lifting function $\bm{s}^{3D}(\cdot)$ due to our 2D slice indenpdence assumption. However, care must be taken since simply using this approximation would be compute-heavy, as one would have to evaluate two forward passes {\em per} each iteration. In this regard, we propose a simple fix to this problem by using alternating updates
\begin{equation}
\begin{aligned}
\begin{cases}
\Sigma \s^{3D}_{\theta^*}(\xb_{t, [:,:,i]}), &\text{with $\mathbb{P} = \alpha / (\alpha + \beta)$}\\
\Sigma \s^{3D}_{\phi^*}(\xb_{t, [i,:,:]}), &\text{with $\mathbb{P} = \beta / (\alpha + \beta)$}
\end{cases}
\end{aligned}
\label{eq:alternating_uncond}
\end{equation}
where $\mathbb{P}$ denotes the probability of each step to be performed. \eqref{eq:alternating_uncond} can be implemented in regularly structured intervals or in a stochastic fashion, which we discuss in detail in \ref{solving-3d-inverse-problem-with-tpdm}.
In order to solve the inverse problem, we can leverage the following result:
\begin{equation}
\begin{aligned}
    &\nabla_{\xb_t} \log p(\xb_t|\yb) \simeq \alpha \nabla_{\xb_t} \log q^{(p)}(\xb_t)\\ &+ \beta \nabla_{\xb_t} \log q^{(a)}(\xb_t) + \nabla_{\xb_t} \log p(\yb|\hat\xb_0),
\end{aligned}
\end{equation}
where by construction, we have the same Jensen gap~\cite{gao2017bounds} as in~\cite{chung2023diffusion}. Moreover, similar to unconditional sampling in \eqref{eq:alternating_uncond}, we can use
\begin{equation}
\begin{aligned}
\begin{cases}
\Sigma \s^{3D}_{\theta^*}(\xb_{t, [:,:,i]}) + \gamma_t \nabla_{\xb_t} \log p(\yb|\hat\xb_0), &\text{with $\mathbb{P} = \alpha / (\alpha + \beta)$}\\
\Sigma \s^{3D}_{\phi^*}(\xb_{t, [i,:,:]}), &\text{with $\mathbb{P} = \beta / (\alpha + \beta)$}
\end{cases}
\end{aligned}
\label{eq:alternating_cond}
\end{equation}
$\gamma_t$ is the step size that also absorbs the weighting factor induced by $\alpha$ and $\beta$.


 \subsection{Training TPDM}
Training of TPDM is performed by the method of Algorithm~\ref{alg:train_tpdm}. The primary 2D diffusion model $\bm{s}_{\theta^*}$ (\ie \textit{prim model}) selects an appropriate plane when solving the inverse problem and is trained with sliced images of 3D volumes  $\{\bm{X}_i\}_1^M$ into the corresponding plane. For example, in the case of CS-MRI and SV-CT, it is the axial plane, and in the case of MR-ZSR, it is the sagittal plane or the coronal plane. An auxiliary 2D diffusion model $\bm{s}_{\phi^*}$ (\ie \textit{aux model}) is trained by selecting one of the two remaining planes of the volumes.
\begin{algorithm}
\caption{Training TPDM}\label{alg:train_tpdm}
\begin{algorithmic}
\Require $\{\bm{X}_i \in \mathbb{N}^{d_1 \times d_2 \times d_3} \}_1^M$, $\{\sigma_i\}_0^1$

\State $D_{prim} \gets \{\}$, $D_{aux} \gets \{\}$ \Comment{Create 2D datasets}
\For{$i$ \textbf{in} $1:M$}
\For{$j$ \textbf{in} $1:d_{3}$}
\State $D_{prim}$.add($\bm{X}_i[:, :, j]$)
\EndFor
\For{$j$ \textbf{in} $1:d_{1}$}
\State $D_{aux}$.add($\bm{X}_i[j, :, :]$)
\EndFor
\EndFor

\State $\bm{s}_{\theta^*} \gets $ train\_2D\_DPM($D_{prim}$, $\{\sigma_i\}_0^1$) \Comment{Train DPMs}
\State $\bm{s}_{\phi^*} \gets $ train\_2D\_DPM($D_{aux}$, $\{\sigma_i\}_0^1$)

\State \Return $\bm{s}_{\theta^*}$, $\bm{s}_{\phi^*}$
\end{algorithmic}
\end{algorithm}


\subsection{Solving 3D inverse problem with TPDM} \label{solving-3d-inverse-problem-with-tpdm}
\begin{algorithm}
\caption{Solving 3D Inverse Problem with TPDM}\label{alg:inference_tpdm}
\begin{algorithmic}
\Require $\bm{Y} \in \mathbb{N}^{d_1^{\prime} \times d_2^{\prime} \times d_3}$, $\bm{A}(\cdot): \mathbb{N}^{d_1 \times d_2} \rightarrow \mathbb{N}^{d_1^{\prime} \times d_2^{\prime}}$,  $\bm{s}_{\theta^*}$, $\bm{s}_{\phi^*}$, $\{\sigma_i\}_0^1$, $N$, $K$, $\lambda$

\State $\bm{X}_N \sim \mathcal{N}(\bm{0}, \sigma^2_1\bm{I}) \in \mathbb{N}^{d_1 \times d_2 \times d_3}$
\For{$i$ \textbf{in} $N-1:0$}
\State $t \gets \frac{i}{N}$
\State $\bm{X}_i \gets$ torch.empty\_like($\bm{X}_N$)
\If{$\mod(i, K) \neq 0$}
\For{$j$ \textbf{in} $1:d_{3}$}
\State $\bm{x} \gets \bm{X}_{i+1}[:,:,j]$
\State $\bm{y} \gets \bm{Y}[:,:,j]$
\State $\Hat{\bm{x}}_0 \gets \bm{x} + \sigma_t^2 \cdot \bm{s}_{\theta^*}(\bm{x}, t)$ 
\State $\bm{x}^{\prime} \gets$ step\_2D\_DPM($\bm{x}$, $\bm{s}_{\theta^*}$, $\sigma_t$, $t$)
\State $\bm{x}^{\prime\prime} \gets \bm{x}^{\prime} - \lambda \nabla_{\bm{x}} \lVert \bm{A}(\Hat{\bm{x}}_0) - \bm{y} \rVert ^2 _2$
\State $\bm{X}_{i}[:,:,j] \gets \bm{x}^{\prime\prime}$
\EndFor
\Else
\For{$j$ \textbf{in} $1:d_{1}$}
\State $\bm{x} \gets \bm{X}_{i+1}[j,:,:]$
\State $\bm{x}^{\prime} \gets$ step\_2D\_DPM($\bm{x}$, $\bm{s}_{\phi^*}$, $\sigma_t$, $t$)
\State $\bm{X}_i[j,:,:] \gets \bm{x}^{\prime}$
\EndFor
\EndIf
\EndFor
\State \Return $\bm{X}_0$
\end{algorithmic}
\end{algorithm}
In order to solve the inverse problem, a method of performing condition sampling is used alternately using the trained TPDM for each step of time-step denoising (Algorithm~\ref{alg:inference_tpdm}). Algorithms are not batched for ease of representation but can be easily batched for computational efficiency. One-denoising step progress through the primary diffusion model $\bm{s}_{\theta^*}$ plays the role of constraining measurement $\bm{Y}$ consistency to an image to be created in combination with DPS method~\cite{chung2023diffusion}. At this time, the hyperparameter $\lambda$ is the step size of the DPS and serves to adjust the strength of the measurement consistency. One denoising step through the auxiliary diffusion model $\bm{s}_{\phi^*}$ plays a role in correcting the inconsistency in the batch direction caused by the step of the main diffusion model by using the trained other slice direction distribution prior. The degree of contribution of the two models can be adjusted through the integer hyperparameter $K$ (for real number $K$, see Appendix~\ref{real-value-k}). For example, if $K=4$, the primary model and the auxiliary model contribute to the generation at a ratio of $3:1$, respectively.


\subsection{3D voxels volume generation with TPDM}
In addition to solving the 3D inverse problem that applies condition sampling of the diffusion model, TPDM can also be used to generate unconditioned high-fidelity 3D voxels volumes. This can be done as a simple way to remove the conditioning step using DPS in the inverse problem-solving method. For the algorithm, see Appendix~\ref{tpdm-uncond}.


\subsection{Forward Measurement Model}
\noindent \textbf{MR-ZSR.} Considering the slice selection process of MRI, the forward measurement kernel can be modeled by averaging and combining adjacent voxels in the Z-axis direction. For example, if the goal is performing super-resolution of a 5mm slice thickness MRI image to 1mm, the forward model is an operation of degrading a 1mm slice image by grouping 5 adjacent XY-plane along the Z-axis direction and averaging each group to get a 5mm slice image. Here, we defined the number of pixels in the Z-axis direction of a group to be merged as \textit{merge size} ($M$).

We used the forward kernel just presented when creating the retrospective degraded MRI dataset (1mm $\rightarrow$ 2/5mm), but used a different forward measurement kernel used in the DPS step when solving the inverse problem MR-ZSR. The kernel is similar to the averaging process, but when averaging, divide by $\sqrt{M}$ instead of dividing by $M$, which is is inspired by Song \etal~\cite{song2021scorebased} and Chung \etal~\cite{chung2022improving}'s diffusion model-based image colorization method.\\

\noindent \textbf{CS-MRI and SV-CT.} The forward measurement kernel of CS-MRI is defined as applying a compressed sensing mask after transforming $\hat{\bm{x}}_0$ into k-space through a 2D Fourier transform. Measurement $\bm{y}$ is also a sub-sampled k-space image by the same mask applied.

The forward measurement kernel of SV-CT converts $\hat{\bm{x}}_0$ into a sinogram by 2D Radon transform under a given acquisition condition. As an instance, the Radon transform is performed with a sparse set of angles. Measurements are also given in sinogram space.






%%%%%%%%%%
\section{Experimental setup}
In addition to 1) MRI Z-axis (slice direction) super-resolution (MR-ZSR), which was first-ever attempted, 2) compressed sensing MRI (CS-MRI), and 3) sparse view CT (SV-CT) were performed to verify the effectiveness of TPDM. Finally, through 5) unconditional Brain MRI volume generation, TPDM as a 3D voxel volume generative model was also demonstrated.

\subsection{Dataset}
The MR-ZSR, CS-MRI and the 3D volume voxel generation task used our private brain MRI image dataset (\ie \verb|BMR-ZSR-1mm| dataset). It has a slice thickness of 1mm and is composed of a cube shape of 256$\times$256$\times$256 per volume. 923 volumes (236,288 2D images) were used as a training dataset, and 1 volume was used as a test dataset with the retrospective slice thickness degradation. A prospective dataset acquired at a slice thickness of 5 mm was also used for the clinical evaluation (\ie \verb|BMR-ZSR-5mm| dataset). Since the score model was pretrained on \verb|BMR-ZSR-1mm| dataset, we did not perform training-test data separation on \verb|BMR-ZSR-5mm| dataset.

\todo[inline]{TODO: Add clinical/modality information of MRI Dataset freely (1mm\&5mm)}

SV-CT task used the public CT dataset provided in the AAPM 2016 CT low-dose grand challenge~\cite{mccollough2017low}. The dataset consists of a total of 10 volumes of contrast-enhanced abdominal CT. To make the volume a $256\times256\times256$ cube, we resized the XY-plane to $256\times256$ and cropped the common part in the Z-direction to make the length 256 (\ie \verb|AAPM-CUBE| Dataset). One of the 10 volumes was used as the test dataset with the retrospective measurement simulation, and the remaining 9 were used as the training dataset. The data we used for training was \textit{only 2304 2D images}, so we were able to test when the training data was extremely small.


\subsection{DPM training and inference}
Both the MRI model and the CT model were trained and inferred under the common model setup and algorithms. The 2D image diffusion model constituting the TPDM used \verb|ncsnpp|~\cite{song2021scorebased} using VE-SDE. In the MR-ZSR problem, the YZ-plane (coronal) was used for the primary model and the XY-plane (axial) was used for the auxiliary model. In all other problems, the XY-plane (axial) was used for the primary model and the YZ plane (coronal) for the auxiliary model. 

The training was conducted with two NVIDIA Geforce RTX 3090 and batch size 8, and the MRI model and CT model performed 300K and 100K training iterations, respectively. For the sampling stage, $N=2000$ and predictor-corrector (PC) sampler~\cite{song2021scorebased} method with batch size 6 were employed and required about 24$\sim$36 hours per volume depending on the type of problem.

\subsection{Comparison methods and evaluation}
For the 3D medical inverse problem, our method was compared with DiffusionMBIR~\cite{chung2022solving}, DPS~\cite{chung2023diffusion}, MCG~\cite{chung2022improving}, L1-Wavelet~\cite{lustig2007sparse}, FBPConvNet~\cite{jin2017deep} and ADMM-TV. DiffusionMBIR is the state-of-the-art method to solve general 3D inverse problems which outperformed existing methods such as Score-MRI~\cite{chung2022score}, DuDoRNet~\cite{zhou2020dudornet}, U-Net~\cite{ronneberger2015unet} and Zero-filled in CS-MRI and outperformed previous methods such as MCG, Lahiri~\etal~\cite{lahiri2023sparse}, FBPConvNet~\cite{jin2017deep}, and ADMM-TV in SV-CT. As the BMR-ZSR problem is a new endeavor, no learning-based method has been devised to address it specifically.

Quantitative evaluation was performed using peak-signal-to-noise-ratio (PSNR) and structural similarity index measure (SSIM)~\cite{wang2004image} for the retrospective test dataset. PSNR was evaluated in a 3D volume, and SSIM measured the average value of results of 2D slices for each slice direction (axial, coronal and sagittal).

\todo[inline]{TODO: Method of MR-ZSR Clinical Evaluation of Prospective Dataset}







%%%%%%%%%%
\section{Results}

\subsection{MRI Z-axis super-resolution (MR-ZSR)} 

\begin{figure*}
\begin{center}
\includegraphics[width=\textwidth]{mrzsr_result_main.png}
\end{center}
\caption{Slice thickness 5mm$\rightarrow$1mm ($\times$5) MR-ZSR results of the retrospective test volume (first row: coronal slice, second row: axial slice, third row: sagittal slice). (a) measurement, (b) DiffusionMBIR~\cite{chung2022solving}, (c) DPS~\cite{chung2023diffusion}, (d) proposed method, (e) ground truth. For (d), first row: primary plane, second row: auxiliary plane.}
\label{fig:mrzsr_result_main}
\end{figure*}

\begin{table}
\begin{center}
\setlength{\tabcolsep}{0.45em}
\begin{tabular}{lllll}
\Xhline{3\arrayrulewidth}
                                      & \multirow{2}{*}{\textbf{PSNR} $\uparrow$} & \multicolumn{3}{c}{\textbf{SSIM $\uparrow$}}   \\ \cline{3-5}
\textbf{Method}                       &       & {\footnotesize Axial$^+$} & {\footnotesize Coronal$^{*}$} & {\footnotesize Sagittal} \\ \hline
TPDM (ours)                           & \textbf{35.97} & \textbf{0.970} & \textbf{0.966} & \textbf{0.964} \\
TPDM-MEAN                             & 32.84 & 0.963 & 0.957 & 0.955 \\
TPDM-MCG                       & 34.48 & 0.961 & 0.955 & 0.954 \\ \hline
DiffusionMBIR~\cite{chung2022solving} & \multicolumn{4}{c}{N/W} \\
DPS~\cite{chung2023diffusion}         & 34.77 & 0.965 & 0.963 & 0.960 \\
MCG~\cite{chung2022improving}         & 32.72 & 0.951 & 0.948 & 0.944 \\ \Xhline{3\arrayrulewidth}
\end{tabular}
\end{center}
\caption{Quantitative evaluation (PSNR, SSIM) of MR-ZSR (5mm$\rightarrow$1mm; $\times$5) on the BMR-ZSR test set. TPDM-MCG: TPDM uses MCG instead of DPS, TPDM-MEAN: The forward model used to create the retrospective dataset is used. N/W: Not Working. $^*$: Primary Plane, $^+$: Auxiliary Plane.}
\label{tab:mrzsr_result_main}
\end{table}


We first conducted MRI Z-axis $\times$5 super-resolution images of 5mm slices, which are mainly taken in clinical practice, to 1mm with the retrospective 5mm test dataset, and the results are in the Table~\ref{tab:mrzsr_result_main} and Fig.~\ref{fig:mrzsr_result_main}. For other merge sizes, see Appendix~\ref{mr-zsr-result-supp}. MR-ZSR using TPDM showed quantitatively better results than any other diffusion-based 3D/2D inverse problem-solving methods~\cite{chung2022solving, chung2023diffusion, chung2022improving}, and no artifacts occurred in any slice direction of the volume. In addition, the use of the auxiliary model not only improves the quality of the slice in the auxiliary direction but also has the effect of improving the detail of the entire slice directions (see (c) and (d) of the Fig.~\ref{fig:mrzsr_result_main}).

Notably, DiffusionMBIR~\cite{chung2022solving}, which is known to be the highest-performing general linear 3D inverse problem solver, did not work at all for our custom-designed MR-ZSR forward measurement kernel. This problem is caused by the total variation loss term, which is a key point loss term that gives consistency in the batch direction in DiffusionMBIR.


As a forward model of TPDM, when merging slices, dividing the sum of slices by $\sqrt{M}$ (TPDM) instead of using $N$ (TPDM-MEAN) yielded superior results. In addition, as a method for imposing measurement consistency constraints on the generation of the main model, DPS (TPDM) exhibited superior outcomes than MCG (TPDM-MCG), which is consistent with~\cite{chung2023diffusion}.

\todo[inline]{TODO: Result of the MR-ZSR Clinical Evaluation of the Prospective Dataset}


\subsection{Compressed-sensing MRI (CS-MRI)}

\begin{figure*}
\begin{center}
\includegraphics[width=\textwidth]{csmri_result_main.png}
\end{center}
   \caption{$\times48$ acceleration Poisson sub-sampled CS-MRI reconstruction results of the retrospective test volume (first row: axial slice, second row: coronal slice, third row: sagittal slice). (a) measurement, (b) DPS~\cite{chung2023diffusion}, (c) DiffusionMBIR~\cite{chung2022solving}, (d) proposed method, (e) ground truth. For (d), first row: primary plane, second row: auxiliary plane.}
\label{fig:csmri_result_main}
\end{figure*}

\begin{table}
\begin{center}
\setlength{\tabcolsep}{0.45em}
\begin{tabular}{lllll}
\Xhline{3\arrayrulewidth}
                                      & \multirow{2}{*}{\textbf{PSNR} $\uparrow$} & \multicolumn{3}{c}{\textbf{SSIM $\uparrow$}}   \\ \cline{3-5}
\textbf{Method}                       &       & {\footnotesize Axial$^{*}$} & {\footnotesize Coronal$^+$} & {\footnotesize Sagittal} \\ \hline
TPDM (ours)                           & \textbf{37.17} & \textbf{0.966} & \textbf{0.967} & \textbf{0.965} \\ \hline
DiffusionMBIR~\cite{chung2022solving} & 34.83 & 0.907 & 0.909 & 0.906 \\
ADMM-TV                               & 27.01 & 0.812 & 0.802 & 0.812 \\ \hline
DPS~\cite{chung2023diffusion}         & 35.30 & 0.950 & 0.951 & 0.949 \\
L1-Wavelet~\cite{lustig2007sparse}    & 23.15 & 0.557 & 0.530 & 0.535 \\\Xhline{3\arrayrulewidth}
\end{tabular}
\end{center}
\caption{Quantitative evaluation (PSNR, SSIM) of CS-MRI (Poisson, $\times$48 acc.) on the BMR-ZSR test set. $^*$: Primary Plane, $^+$: Auxiliary Plane.}
\label{tab:csmri_result_main}
\end{table}

We also evaluated TPDM by performing reconstruction on retrospective $\times$48 acceleration Poisson sub-sampled CS-MRI volumes (Fig.~\ref{fig:csmri_result_main}, Table~\ref{tab:csmri_result_main}). For other acceleration factors, see Appendix~\ref{cs-mri-result-supp}. Similarly to the outcomes produced by MR-ZSR, TPDM showed the best results compared to the prior art 2D/3D reverse problem-solving methods. Not only quantitative evaluations, of course, Fig.~\ref{fig:csmri_result_main} shows that TPDM accurately reconstructed the details, surpassing all other methods.



\subsection{Sparse-view CT (SV-CT)}

\begin{figure}
\begin{center}
\includegraphics[width=\columnwidth]{svct_result_main.png}
\end{center}
   \caption{36-view SV-CT reconstruction results of the retrospective test volume (first row: axial slice, second row: coronal slice, third row: sagittal slice). (a) measurement, (b) DiffusionMBIR~\cite{chung2022solving}, (c) proposed method, (d) ground truth. For (c), first row: primary plane, second row: auxiliary plane. Note that \emph{only 2304 2D images} were used for training.}
\label{fig:svct_result_main}
\end{figure}

\begin{table}
\begin{center}
\setlength{\tabcolsep}{0.45em}
\begin{tabular}{lllll}
\Xhline{3\arrayrulewidth}
                                      & \multirow{2}{*}{\textbf{PSNR} $\uparrow$} & \multicolumn{3}{c}{\textbf{SSIM $\uparrow$}}   \\ \cline{3-5}
\textbf{Method}                       &       & {\footnotesize Axial$^{*}$} & {\footnotesize Coronal$^+$} & {\footnotesize Sagittal} \\ \hline
TPDM (ours)                           & \textbf{38.25} & \textbf{0.947} & \textbf{0.951} & \textbf{0.949} \\ \hline
DiffusionMBIR~\cite{chung2022solving} & 34.78 & 0.857 & 0.856 & 0.861 \\
ADMM-TV                               & 30.33 & 0.856 & 0.894 & 0.867 \\  \hline
DPS~\cite{chung2023diffusion}         & 38.20 & 0.942 & 0.943 & 0.941 \\
FBPConvNet~\cite{jin2017deep}         & 32.09 & 0.945 & 0.932 & 0.931 \\  \Xhline{3\arrayrulewidth}
\end{tabular}
\end{center}
\caption{Quantitative evaluation (PSNR, SSIM) of SV-CT (36-view) on the AAPM-CUBE test set. $^*$: Primary Plane, $^+$: Auxiliary Plane. Note that \emph{only 2304 2D images} were used for training.}
\label{tab:svct_result_main}
\end{table}

The CT problem was used for only 9 volumes (about 2000 2D images) as a train dataset data to test the performance of TPDM in extremely small data conditions. The experimental results for 36-view SV-CT are shown in Table~\ref{tab:svct_result_main}, Fig.~\ref{fig:svct_result_main}. Despite training with a highly limited dataset, the TPDM model performed well compared to the other models. Although the margin with DPS~\cite{chung2023diffusion} is not large, TPDM outperforms DPS significantly due to DPS being a 2D inverse problem solver which introduces causes numerous artifacts in the batch direction when applied to 3D inverse problems. In the case of FBPConvNet~\cite{jin2017deep}, the SSIM in the axial direction has a small margin, but since it is also a 2D model, it exhibits poor performance for other slice directions, and the blurry results characteristic of convolutional networks trained with supervised methods are noticeable.

\subsection{Unconditional 3D voxels volume generation}

Finally, with TPDM, we tried to unconditionally generate a full 3D voxels volume. Based on the TPDM model trained with the BMR-ZSR data set, the MRI volume of the human head generation was conducted, and the results are shown in Fig.~\ref{fig:uncond_result_main}. Without any measurement guidance, a complete three-dimensional full voxels volume was created with high resolution and quality. It is interpreted that the 3D volume aware capability of TPDM does not originate from the 2D image order guidance of the measurement applied in the DPS step but from alternative denoising algorithms of two diffusion models, which eventually induce sampling along the true three-dimensional data distribution represented by the product distribution.

\begin{figure}
\begin{center}
\includegraphics[width=\columnwidth]{uncond_result_main.png}
\end{center}
   \caption{Results of the human head MRI volume generation using unconditioned TPDM. To visualize the volume, after removing a quarter of the volume, the iso-surface contour is expressed as a surface.}
\label{fig:uncond_result_main}
\end{figure}






%%%%%%%%%%
\section{Conclusion}

In this study, we introduced TPDM, a method for solving the general 3D inverse problem and generating voxels volume with pre-trained two perpendicular 2D diffusion models. TPDM works in a completely unsupervised manner and does not require any fine-tuning for individual inverse problems. It handles 3D volume without using any 3D diffusion model by assuming a 3D distribution as the product distribution of 2D distributions, effectively avoiding the curse of dimensionality while still utilizing probability distributions of 3D volume. Our findings indicate that TPDM outperforms existing state-of-the-art 3D inverse problem-solving methods on several medical 3D reconstruction problems, even when trained with a significantly limited amount of data. Finally, using TPDM and a novel forward measurement model, we first-ever attempted Z-directional super-resolution of MRI images and demonstrated exceptional outcomes in both technical and clinical aspects.






%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}






%%%%%%%%% SUPPLEMENTARY_MATERIAL
\clearpage
\appendix

\section*{Supplementary Material}

\section{Real Value $K$} \label{real-value-k}



\section{3D Voxel Volume Generation with TPDM} \label{tpdm-uncond}

\begin{algorithm}
\caption{3D Voxel Volume Generation with TPDM} \label{alg:uncond_tpdm}
\begin{algorithmic}
\State todo!
\end{algorithmic}
\end{algorithm}




\section{Additonal Results}
\subsection{BMR-ZSR} \label{mr-zsr-result-supp}

\begin{table}[h]
\begin{center}
\setlength{\tabcolsep}{0.45em}
\begin{tabular}{lllll}
\Xhline{3\arrayrulewidth}
                                      & \multirow{2}{*}{\textbf{PSNR} $\uparrow$} & \multicolumn{3}{c}{\textbf{SSIM $\uparrow$}}   \\ \cline{3-5}
\textbf{Method}                       &       & {\footnotesize Axial$^+$} & {\footnotesize Coronal$^{*}$} & {\footnotesize Sagittal} \\ \hline
TPDM (ours)                           & \textbf{38.76} & \textbf{0.982} & \textbf{0.979} & \textbf{0.978} \\ \hline
DiffusionMBIR \cite{chung2022solving} & \multicolumn{4}{c}{N/W} \\ \Xhline{3\arrayrulewidth}
\end{tabular}
\end{center}
\caption{Quantitative evaluation (PSNR, SSIM) of MR-ZSR (3mm$\rightarrow$1mm; $\times$3) on the BMR-ZSR-256 test set. N/W: Not Working. $^*$: Main Plane, $^+$: Auxiliary Plane.}
\label{tab:mrzsr_supp_result}
\end{table}

\subsection{Compressed-sensing MRI (CS-MRI)} \label{cs-mri-result-supp}

\begin{table}[h]
\begin{center}
\setlength{\tabcolsep}{0.45em}
\begin{tabular}{lllll}
\Xhline{3\arrayrulewidth}
                                      & \multirow{2}{*}{\textbf{PSNR} $\uparrow$} & \multicolumn{3}{c}{\textbf{SSIM $\uparrow$}}   \\ \cline{3-5}
\textbf{Method}                       &       & {\footnotesize Axial$^{*}$} & {\footnotesize Coronal$^+$} & {\footnotesize Sagittal} \\ \hline
TPDM (ours)                           & 44.96 & 0.988 & 0.989 & 0.988 \\ \hline
DiffusionMBIR \cite{chung2022solving} & 41.21 & 0.934 & 0.934 & 0.934 \\
DPS \cite{chung2023diffusion}         & \textbf{47.10} & \textbf{0.991} & \textbf{0.991} & \textbf{0.991} \\ \Xhline{3\arrayrulewidth}
\end{tabular}
\end{center}
\caption{Quantitative evaluation (PSNR, SSIM) of CS-MRI (Poisson, $\times$8 acc) on the BMR-ZSR-256 test set. $^*$: Main Plane, $^+$: Auxiliary Plane.}
\end{table}

\begin{table}[h]
\begin{center}
\setlength{\tabcolsep}{0.45em}
\begin{tabular}{lllll}
\Xhline{3\arrayrulewidth}
                                      & \multirow{2}{*}{\textbf{PSNR} $\uparrow$} & \multicolumn{3}{c}{\textbf{SSIM $\uparrow$}}   \\ \cline{3-5}
\textbf{Method}                       &       & {\footnotesize Axial$^{*}$} & {\footnotesize Coronal$^+$} & {\footnotesize Sagittal} \\ \hline
TPDM (ours)                           & \textbf{40.34} & \textbf{0.979} & \textbf{0.978} & \textbf{0.978} \\ \hline
DiffusionMBIR \cite{chung2022solving} & 37.48 & 0.895 & 0.899 & 0.897 \\
DPS \cite{chung2023diffusion}         & 39.06 & 0.965 & 0.967 & 0.965 \\ \Xhline{3\arrayrulewidth}
\end{tabular}
\end{center}
\caption{Quantitative evaluation (PSNR, SSIM) of CS-MRI (Poisson, $\times$24 acc) on the BMR-ZSR-256 test set. $^*$: Main Plane, $^+$: Auxiliary Plane.}
\end{table}

\todo[inline]{K and LAMBDA}






\end{document}
