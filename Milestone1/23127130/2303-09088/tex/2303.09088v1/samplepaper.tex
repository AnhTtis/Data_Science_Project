% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
\usepackage{amsmath,amsfonts}
%\usepackage{amsmath,amsfonts,amssymb,amsthm}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%
\usepackage{xcolor}
%\newcommand{\fixme}[1]{{\color{red}{#1}}}
%\newcommand{\fixed}[1]{{\color{blue}{#1}}}

\usepackage{hyperref}
\hypersetup{
  colorlinks,
  citecolor=blue,
  linkcolor=red,
  urlcolor=blue
}

\usepackage{enumitem}
\setlist{nolistsep}

\begin{document}
%
% \title{Metamorphic Image Registration using Flows of Diffeomorphisms\thanks{Supported by organization x.}}
%\title{Metamorphic Image Registration using Flows of Diffeomorphisms}
\title{MetaRegNet: Metamorphic Image Registration Using Flow-Driven Residual Networks}
%
\titlerunning{MetaRegNet}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
% \author{First Author\inst{1}\orcidID{0000-1111-2222-3333} \and
% Second Author\inst{2,3}\orcidID{1111-2222-3333-4444} \and
% Third Author\inst{3}\orcidID{2222--3333-4444-5555}}
\author{Ankita Joshi\inst{1}, Yi Hong\inst{2}}
%
\authorrunning{Joshi and Hong}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
% \institute{Princeton University, Princeton NJ 08544, USA \and
% Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
% \email{lncs@springer.com}\\
% \url{http://www.springer.com/gp/computer-science/lncs} \and
% ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
% \email{\{abc,lncs\}@uni-heidelberg.de}}
\institute{Department of Computer Science, University of Georgia, Athens, GA, 30602, USA \and Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China\\
\email{yi.hong@sjtu.edu.cn}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
% Deep learning based methods have revolutionised medical image registration by providing efficient solutions to the problem of diffeomorphic image registration. 
Deep learning based methods provide efficient solutions to medical image registration, including the challenging problem of diffeomorphic image registration. However, most methods register normal image pairs, facing difficulty handling those with missing correspondences, e.g., in the presence of pathology like tumors. We desire an efficient solution to jointly account for spatial deformations and appearance changes in the pathological regions where the correspondences are missing, i.e., finding a solution to metamorphic image registration. Some approaches are proposed to tackle this problem, but they cannot properly handle large pathological regions and deformations around pathologies. In this paper, we propose a deep metamorphic image registration network (MetaRegNet), which adopts time-varying flows to drive spatial diffeomorphic deformations and generate intensity variations. We evaluate MetaRegNet on two datasets, i.e., BraTS 2021 with brain tumors and 3D-IRCADb-01 with liver tumors, showing promising results in registering a healthy and tumor image pair. The source code is available online. 
%Our method sheds new lights on providing an efficient metamorphic image registration solution. 
% a pair of healthy images, since for pathological image regions there is no clear definition of their pixel/voxel level
% correspondences. Not only this, but existing registration methods also do not account for changes in image appearance typically in the presence of pathology such as tumors or oedema. Building on the theory of image metamorphosis that obtain transformations while simultaneously accounting for the appearance changes, in this paper, we propose a deep learning based diffeomorphic image registration approach that jointly learns to estimate spatial transformations for images in the presence of pathology while simultaneously learning the changes in the intensity variations between the source and target images. We use residual networks to model the flows of diffeomorphisms in order to learn the diffeomorphic deformations as well as model the incremental mappings in the intensity variations to learn the appearance changes. We restrict the intensity variations learned to be within the pathological areas of the images using the tumor masks. The ability of the proposed architecture to learn the spatial transformations and the intensity variations is illustrated on two real-world datasets namely, BraTS 2021 brain tumor dataset and the 3D-IRCADb-01 liver tumor dataset, thus facilitating novel directions in deep learning based pathological image registration. Our source code is available online at ***.
\keywords{Pathological Image Registration \and  Image Metamorphosis \and Diffeomorphisms \and Residual Networks}
\end{abstract}
%
%
%
\section{Introduction}

Deformable image registration (DIR) establishes pixel/voxel dense correspondences between 2D/3D images using a deformation that transforms images into a common space for fusion or comparison~\cite{sotiras2013deformable}. Existing DIR methods include classical registration models, e.g. SyN~\cite{avants2011reproducible}, Large Deformation Diffeomorphic Metric Mapping (LDDMM)~\cite{beg2005computing,cao2005large}, Stationary Velocity Fields (SVF)~\cite{arsigny2006log}, and recent deep-learning-based methods, e.g., QuickSilver~\cite{yang2017quicksilver}, VoxelMorph~\cite{dalca2018unsupervised}, SYMNet~\cite{mok2020fast}. These methods focus on registering image pairs with no missing correspondences, i.e., all pixels or voxels in the source image are matched with those in the target image using a bijective mapping function. Such assumptions limit their ability to tackle registration between image pairs with appearing or disappearing structures, e.g., developing brain scans during myelination, a healthy image and a tumor one, etc. These image pairs have both spatial deformation caused by the movements of shared structures and intensity changes caused by missing correspondences between unshared ones. Such intensity changes challenge existing methods, including diffeomorphic image registration that provides smooth transformation with a smooth inverse to preserve topology between image pairs.   

To capture spatial and intensity changes simultaneously, we turn to metamorphic image registration~\cite{trouve2005metamorphoses}, which introduces a source term to simulate the intensity changes in the diffeomorphic image registration framework~\cite{hong2012metamorphic}. The model complexity of traditional metamorphic image registration makes them impractical when handling large-scale and high-resolution images. To reduce the computational cost of existing methods, researchers propose some deep-learning-based solutions to handle pathological image registration, a special case of deep metamorphic image registration, e.g., the existence of tumor regions as in~\cite{maillard2022deep,mok2022unsupervised}. Existing methods use either a cost function masking (CFM) strategy, which completely separates deformations and intensity changes in the pathological and non-pathological regions, or a clean/healthy source or target image where a tumor is simulated to match with the other one with tumor. These methods ignore the deformations of healthy regions effected by the pathological regions, which causes large artifacts within and surrounding the pathological regions. 
% However, existing methods are not truly metamorphic in their approaches, in that, they either use cost function masking methods which are not suitable for large tumors or use a fixed healthy template as either the source or the target image with the other image being the one with lesions or pathologies, making them limited in their applications.

\begin{figure}[t]
\includegraphics[width=0.99\textwidth]{figures/MetaRegNet.pdf}
\caption{Overview of our metamorphic image registration network, MetaRegNet.}
%, which has two branches to integrate the spatial deformations $\phi$ and the intensity variations $q$. Its output adds the deformed source image with intensity changes in tumor regions.}
\label{fig:metaregnet}
\end{figure}

To address this challenge, we reformulate traditional metamorphic image registration and propose a deep metamorphic solution MetaRegNet, see Fig.~\ref{fig:metaregnet}. Based on Lipschitz continuous ResNet blocks proposed in~\cite{joshi2021diffeomorphic}, MetaRegNet consists of two pathways for jointly integrating spatial deformations and intensity changes along the trajectory from the source to target images. That is, MetaRegNet produces diffeomorphic deformations to account for spatial deformations between images, and jointly learns intensity changes to account for the tumor appearance between them. In this paper, we simplify the problem and limit our task to registering a healthy source to a pathological target, leaving other cases for future work. Our contributions in this paper are summarized below:
\begin{itemize}
    \item We propose a novel registration network for image-to-image metamorphosis, which uses the time-varying flow to drive diffeomorphic spatial mappings and simultaneously learns the incremental intensity variations between a healthy source image and a target image with one or more pathological regions.
    \item We conduct experiments on the BraTS 2021 brain tumor MRI~\cite{menze2014multimodal} and 3D-IRCADb-01 liver CT~\cite{3dircadb} datasets and compare to a metamorphic version of VoxelMorph~\cite{dalca2018unsupervised}. Our method produces significantly better results qualitatively and quantitatively, for both estimations of diffeomorphic mappings for spatial alignment and intensity variations in pathological regions. 
    % Results demonstrate that our framework not only produces diffeomorphic mappings but also accounts for the intensity variations in the tumor regions. Compared to a metamorphic version of the VoxelMorph \cite{dalca2018unsupervised} diffeomorphic registration algorithm, our approach presents much better results qualitatively and quantitatively. 
\end{itemize}




%\cite{describe our method in one or two sentences.} 

% Such a deformation desires a good property of diffeomorphisms, a smooth transformation with a smooth inverse, to ensure the preservation of topology when warping images. 

% but are computationally expensive requiring optimization over millions of parameters in $3D$. Deep learning based solutions proposed to alleviate the computational complexities of the traditional methods have been successful in tackling the diffeomorphic image registration problem using both supervised~\cite{sokooti2017nonrigid,rohe2017svf,yang2017quicksilver} and unsupervised methods ~\cite{mok2020large,dalca2018unsupervised,krebs2018unsupervised,krebs2019learning,mok2020fast,wu2022nodeo}. 

% Since image registration is based on structural similarity between source and target images, for an alignment between healthy images and images in the presence of pathologies, the assumptions of structural and intensity similarity may not hold. Therefore, standard diffeomorphic image registration methods which are designed to register normal neuroanatomies may fail to produce results due to the absence of one-to-one correspondences and tissue appearance changes. This suggests the need for better methods because they are necessary for applications in disease diagnosis and treatment planning, treatment monitoring and image guided neurosurgery \cite{brock2017use}. These limitations gave rise to the theory of metamorphosis \cite{trouve2005metamorphoses,holm2009euler,miller2001group} which capture spatial and intensity changes simultaneously. 

%However these models face two main challenges for practical applications providing fast solutions and handling possible missing correspondences. 
%Recently, deep learning based approaches open an alternative to possibly address the above challenges in one framework, which inspires our work in this paper. Existing deep registration models mainly tackle the efficiency challenge using supervised \cite{} or unsupervised techniques\cite{} in terms of providing deformations. Supervised approaches maintain the diffeomorphic property
%%%%%%

%Our main contribution in this paper is a novel diffeomorphic image registration algorithm for capturing large deformations based on time-dependent velocity fields which handle pathological images. We test our registration model on 2D MRI scans collected from BraTS brain MRI dataset, The experimental results demonstrate that our framework has better registration performance compared to the baselines. 



\subsection{Related Work}
%The existing methods that tackle the pathological image registration problem can be roughly divided into three categories, i.e., mask-based methods, reconstruction-based methods, and other methods like feature-disentangled approaches. 

\noindent
\textbf{Mask-based Methods.} 
%Masking out pathological regions is a commonly-used technique to handle the absent correspondences. 
This category assumes that pathological masks are available for learning; so that, the healthy and pathological regions can be treated separately. In~\cite{brett2001spatial}, masks are used to exclude the pathological regions from measuring the image similarity loss. The geometric metamorphosis~\cite{niethammer2011geometric} uses masks to separate the foreground and background deformation models for capturing intensity and spatial changes separately. 
%The combination of both foreground and background deformations produce the final transformations for pathological image pairs. 
Similarly, masking strategies have been used in~\cite{clatz2005robust,mok2022unsupervised}. Joint registration and segmentation approaches are also proposed in~\cite{chitphakdithai2010non,gooya2011joint,mok2022unsupervised}. These approaches assume that both source and target images have tumors and their models are hard to optimize and need a long time to converge.
%, which use the registration deformation to deform the source image to match the target image and the source image segmentation mask to match the target one. 

Differently, we automatically learns the amount of intensity changes and how to balance them with spatial deformations. The mask {\it softly} restricts the learning of intensity changes within the pathological regions, resulting continuous deformations and greatly reduced artifacts surrounding the pathological regions. 

%This simple separation of spatial and intensity changes ignore the impact on the healthy regions that is produced by pathologies like tumors, yielding unsatisfactory alignment results, especially around the pathological regions. 

% Multiple works have been proposed to handle the problem of pathological image registration. Most registration algorithms deal with the absent correspondences by using the technique of cost function masking \cite{brett2001spatial} wherein the pathological regions are excluded from the measurements of image similarity. Authors  Both the foreground and background deformations are combined to produce the final deformation. However, in this work both source and target images are needed to be with tumors. There are multiple conventional and deep learning pathological image registration methods based on cost function masking techniques \cite{mok2022unsupervised,clatz2005robust}. However, cost function masking as used in \cite{mok2022unsupervised} and other deep learning based methods, only masks the pathology in the loss function calculation assuming that the pathology does not have an impact on the healthy regions. However, in the real-world scenario the pathologies have an obvious impact on the deformations of the healthy regions, especially for images containing big lesions and this approach might not yield good results.  

\noindent
\textbf{Reconstruction-based Methods.} Another choice for metamorphic image registration is to reconstruct a pathological image into a clean quasi-normal one before registration~\cite{han2018brain,bone2020learning,han2020deep}. A variety of techniques have been proposed, such as image in-painting~\cite{sdika2009nonrigid}, Variational AutoEncoder (VAE) based approaches~\cite{bone2020learning}, and a low-rank decomposition model to learn a normal image appearance~\cite{liu2014low}. These approaches do not require masks of pathological regions; however, their registration quality is limited by the imperfect or over-smoothed reconstructions. Also, these methods require extra healthy image scans for training and work well only if the size of the pathological region, like tumor or lesion, is relatively small. 

% Another popular approach is a data augmentation technique of reconstructing the pathological images into a clean quasi-normal image \cite{han2018brain,bone2020learning,han2020deep} using a variety of techniques such as image in-painting\cite{sdika2009nonrigid}, using a variety of Variational AutoEncoders (VAEs) based architectures \cite{bone2020learning} or using statistical models which estimate a low rank/sparse decomposition to learn a normal image appearance \cite{liu2014low}. Although such approaches have the advantage of not needing a prior pathological segmentation, they suffer from producing over-smoothed reconstructions, or requiring extra image scans of healthy images for training and work well only if the tumor sizes are small. Image inpainting techniques are also used to reconstruct clean images from noisy ones which have also shown to work only in cases of small lesions.

\noindent
\textbf{Other Methods}. In~\cite{bone2020learning,shu2018deforming,maillard2022deep}, researchers disentangle the shape and appearance changes of an image when performing registration.  In~\cite{gooya2012glistr,franccois2022weighted}, a biophysical model is adopted to introduce a growing tumor into a healthy image and perform image registration with another tumor image; however, modeling the growth of a tumor is a non-trivial task. In \cite{franccois2021metamorphic}, a semi-langrangian scheme is proposed to carry out registration for each pair but has limited ability to handle a large-scale dataset. 

% Another approach of pathological image registration is using a biophysical model to grow tumors into a healthy image and then perform image registration with another pathological image. In  \cite{gooya2012glistr,franccois2022weighted}. Such an approach may not always give good results since it is difficult to model specific types of tumors and it is also difficult to understand the intensities of the tumor regions, and modelling the growth of tumors along with the deformation adds to the complexity. In deep learning based approaches like \cite{bone2020learning,shu2018deforming} authors try to disentangle the shape and appearance, however, such methods again need large amounts of datasets to be trained on and are still unable to handle the disentanglement properly. Joint registration and segmentation approaches have also been proposed \cite{chitphakdithai2010non,gooya2011joint}, wherein the registration accuracy is tried to be improved by generating the segmentation mask and using them in the loss function with cost function masking, and then improving the segmentation accuracy by registering the masks with the generated deformation fields. However such approaches are difficult to optimize and take a lot of time to converge \cite{mok2022unsupervised}. In \cite{franccois2021metamorphic}, the authors introduce a semi-langrangian scheme for image registration which is faster than the Eulerian scheme, however, the authors need to carry out the metamorphic registration for each image pair, and for large scale high dimensional medical images, this can get very computationally expensive.

The model in~\cite{maillard2022deep} is the closest one to ours. It also uses residual networks and deforms a pathological source image to a fixed healthy atlas, which is a special case of ours, since we allow choosing different healthy images. More importantly, unlike them, we do not need a hyper-parameter to balance spatial deformations and intensity changes, which is not practical since it varies with different inputs.

%On the contrary, we use two control variables to capture deformation and intensity changes separately, with no need of setting this hyperparameter.  



%\fixme{Also, they follow the formulation in \cite{franccois2022weighted} based on the metamorphosis model very closely, wherein they model the deformations based on the momentum that represents both the additive intensity and the parameter of the deformation. Our architecture however separates the two control variables and learns the deformation and intensity variations separately by modelling them as numerical approximations of the underlying continuous diffeomorphic settings governed by the separated ordinary differential equations.}%whereas we change the original metamorphosis formulation \cite{trouve2005metamorphoses} by separating the two dynamics in the metamorphosis model.}
 
%Furthermore, in~\cite{maillard2022deep} the tumor mask in the source image takes the same 

%However, our method is different in several crucial ways. Firstly, their architecture is used to deform different source images to a fixed target image (clean atlas). However, we use different healthy source images and register it to variable target images, making our method an image-to-image metamorphic solution. Next, the authors use a tumor mask to limit the region where intensities can be modified, however, the authors also assume that during deformation the tumor of the source image will also undergo the same deformation, i.e., they deform the source segmentation mask using the generated deformation field. However, it is unclear as to what this deformed source segmentation mask is matched with since it has no valid correspondences in the healthy target image. Such a solution is not metamorphic since during image metamorphosis the appearance or disappearance of a tumor region is only be accounted for by the intensity changes and not the deformation. %\fixme{please describe this method and discuss the difference between ours and theirs.}

% , moreover, the authors also do not learn the embedded features of their inputs and directly work on the original images. Not only this, in order to disentangle the topological changes from the spatial changes, the authors introduce the source segmentation mask, and constraint the generated deformation field to also deform this source tumor segmentation mask. It is unclear as to what this deformed source segmentation mask is matched with since it has no valid correspondences in the healthy target image.

% However they apply the proposed algorithm to deform a pathological source image to a single healthy target image, moreover, the authors also do not learn the embedded features of their inputs and directly work on the original images. Not only this, in order to disentangle the topological changes from the spatial changes, the authors introduce the source segmentation mask, and constraint the generated deformation field to also deform this source tumor segmentation mask. It is unclear as to what this deformed source segmentation mask is matched with since it has no valid correspondences in the healthy target image.

\section{Background and Reformulation}
\label{sec:background}
In the LDDMM framework~\cite{beg2005computing}, diffeomorphic image registration is formulated as a minimization problem, which estimates a smooth deformation field $\phi: \Omega \rightarrow \Omega$, where $\Omega \subseteq R^ {n_{x} \times n_{y} \times n_{z}}$ (i.e., the size of a 3D image). The formulation is given as
\begin{equation}
    E(v) = \frac{1}{2}  \int_{0}^{1} {\| v \|_{L}^{2}}\,dt + \frac{1}{\sigma^{2}} {\|I_{1} - I(1) \|}_2^2, \quad s.t. \ I_{t} + \nabla I^Tv = 0, I(0)= I_{0}.
\label{eq:lddmm}    
\end{equation}
% \begin{equation}
%     E(v) = \frac{1}{2}  \int_{0}^{1} {\| v \|_{L}^{2}}\,dt + \frac{1}{\sigma^{2}} {\|I_{1} - (\phi \circ I_{0}) \|}_2^2, \quad s.t. \ I_{t} + \nabla I^Tv = 0, I(0)= I_{0}.
% \label{eq:lddmm}    
% \end{equation}
Here, $v$ is the time-dependent velocity field, $L$ is a spatial regularizer to ensure the smoothness of $v$; $I_0$ and $I_1$ are the source and target images, $I(0)$ and $I(1)$ are the deformed images at $t=0$ and $t=1$, respectively; $\sigma$ controls the influence of the regularization term and the image matching term. This formulation is an image-based version of LDDMM, where the image intensity is driven by the velocity field $v$. We can also use map-based implementation, which uses a deformation $\phi$ that is driven by the velocity field and then used to warp the source image. 

Metamorphic registration can be formulated based on the diffeomorphic LDDMM model~\cite{hong2012metamorphic}, by introducing a control variable $q$ in the image transport equation, as shown in Eq.~\eqref{eq:metamorphosis}. This introduced variable $q$ simulates an intensity source term and models intensity changes caused by appearing or disappearing objects in the image. The new optimization function is formulated as 
% where $v$ is the time-dependent velocity field, the integral of which produces the diffeomorphic deformation $\phi$ used to warp the source image $I_{0}$ to the target image $I_{1}$. $L$ is the spatial regularizer of the velocity fields to ensure their smoothness, and $\sigma$ controls the influence of the image matching term. Such a deformation of the source image only allows its deformation and not a change in the appearance of $I_{0}$. This standard image registration model is modified with a control variable $q$, which will allow to adjust the image intensities of $I_{0}$ changing the above optimization problem in Equation~\ref{eq:lddmm} to \cite{hong2012metamorphic},
\begin{equation}
    E(v,q) = \frac{1}{2}\int_{0}^{1} {\| v \|_{L}^{2}}\,dt + \rho{\| q \|_{Q}^{2}}\,dt \ s.t. \ I_{t} + \nabla I^Tv = q, I(0)= I_{0}, I(1) = I_{1},
\label{eq:metamorphosis}    
\end{equation}
where $\rho$ is a constant value to balance the intensity variations introduced by the control variable $q$, $Q$ is a smooth operator applied on $q$. Different from LDDMM, metamorphic registration moves the image matching term into the constraints and can achieve a {\it perfect matching} between warped image $I(1)$ and the target image $I_1$, due to the introduced $q$. The solution in~\cite{hong2012metamorphic} presents a tight coupling of velocity fields that drive spatial deformations and additive intensity changes. 
%More detailed descriptions on metamorphic registration can be found in~\cite{trouve2005metamorphoses,hong2012metamorphic,maillard2022deep}. 
% By using an augmented Lagrangian approach the solution is given in \cite{hong2012metamorphic} 
% as,
% \begin{equation}
% \begin{aligned}
%          I_{t} + \nabla I^{T}v &= q,\ I(0) = I_{0}, \\
%              - p_{t} - div(pv) &= 0, \  p(1) = r - \mu(I(1) -I_{1}) \\ 
%      L^{\dagger}Lv + p\nabla I &= 0
% \end{aligned}
% \end{equation}
% where $p$ is the added dynamical constraint through the momentum variable. $\mu > 0$ and $r$ is the langrangian multiplier for the final image match constraint. 
%This formulation tells us that there is a tight coupling between spatial deformation driven by the flows of velocity fields and the additive intensity changes over time. 

\noindent
\textbf{Reformulation.} In the deep learning framework, we use the map-based image registration and disentangle the velocity fields into two parts, i.e., $v_t^{sd}$ that drives the spatial deformation $\phi$ and $v_t^{iv}$ that drives the intensity variation $q$. Hence, we replace the transport equation in Eq.~\eqref{eq:metamorphosis} with a reformulation of two separate dynamics, using the following two ordinary differential equations (ODEs):
\begin{equation}
    \frac{d \phi}{dt} = v_t^{sd} \circ \phi(t), \phi(0) = id \quad and \quad \frac{d q}{dt} = v_t^{iv} \circ q(t), q(0) = 0, 
\label{eq:deepMeta}
\end{equation}
where $id$ is the identity map, the spatial deformation $\phi$ lies in a vector space that has the same size as the velocity field $v$, and the intensity variation $q$ is scalar and has the same size as the image $I$. At $t=1$, the transported image $I(1)$ is the combination of the deformed source image and the total intensity changes. To restrict intensity changes within the pathological regions, we use its mask $\mathcal{M}$ and obtain
%As a result, $I(1)$ is estimated as:
%\begin{equation}
    $I(1) = \phi(1) \circ I_0 + q(1) \otimes \mathcal{M}$.
%\label{eq:imageat1}
%\end{equation}
%where $\mathcal{M}$ is the mask of the pathological regions. 
This formulation models the appearance of a tumor from a clean source image to a pathological target image.
%In this paper, we focus on registering a healthy source image $I_0$ to a pathological target image $I_1$, because this will help us study the evolution of the tumor from a clean source image to the pathological target image. %Therefore, we only have one mask at the target, which is used to mask out other unwanted intensity variations at the end.  (we do not mask anything out)

% where $I(1)$ is the warped and intensity adjusted source image and $\rho$ controls the balance of the amount of intensity variations and the spatial deformations. The term $\nabla I^Tv$ represents the spatial variation of the moving image $I^T$ in the direction of $v_{t}$. And the moving intensity scalar field $I_{t}$ is basically obtained by the diffeomorphism $\phi_{t}$ on the source image $I_{0}$. This is the formulation of metamorphic image registration where the solution of the minimization problem in Equation~\ref{eq:metamorphosis} provides an efficient and robust estimation of intensity variation and diffeomorphisms in the presence of large deformations \cite{trouve2005metamorphoses}.
% For a thorough derivation to the solutions of the above equations we refer the reader to \cite{hong2012metamorphic,maillard2022deep}. The solution for LDDMM registration is identical to that of the metamorphosis model which shows that there is a tight coupling between the flows of velocity fields producing the deformations and the additive intensity changes. 



\section{Deep Metamorphic Image Registration}
%We follow the formulation of metamorphic image registration in Eq.~\ref{eq:metamorphosis} faithfully,
Based on the above reformulation, we propose a metamorphic image registration network (MetaRegNet) to jointly model spatial deformations and intensity changes. Overall, MetaRegNet adopts a UNet network to estimate two initial velocity fields $v_0^{sd}$ and $v_0^{iv}$, which drive the spatial deformation $\phi$ and the intensity variations $q$ over time. The combination of the deformed source image and estimated intensity changes matches the target image. The proposed architecture is presented in Fig.~\ref{fig:metaregnet}, with each component described below.   

% We utilize the LDDMM based formulation of ResNets as given in \cite{joshi2021diffeomorphic}, wherein we modify the architecture to adapt to the metamorphosis extension of the LDDMM framework \cite{trouve2005local,cao2005large}. It should also be noted that our choice of using the baseline architecture in \cite{joshi2021diffeomorphic} is motivated due to the fact that the diffemorphisms generated in this work are modeled as time-varying velocity fields,  unlike most other work \cite{mok2020fast,mok2020large,mok2022unsupervised} which are based on stationary velocity fields. 

% \subsection{Architecture Overview}
% Let $I_{0}$ and $I_{1}$ be a healthy source image and a pathological target image respectively,  defined over $\Omega$ $\subseteq$ $\mathbb{R}^n$ where $\Omega$ is the spatial domain and $n=2$,$3$. The proposed architecture is as shown in Fig.~\ref{fig1}, named as MetaRegNet, has important components described briefly as below.

\noindent
\textbf{Estimation of Initial Flows.} To obtain spatial deformations and intensity variations, we first estimate their initial driven flows. We adopt a UNet~\cite{ronneberger2015u} to estimate the initial values $v_0^{sd}$ and $v_0^{iv}$ from an image pair. As shown in Fig.~\ref{fig:metaregnet}, the network includes a non-probabilistic U-Net architecture, which directly outputs the flow estimation, without sampling from the mean and variance as in~\cite{dalca2018unsupervised}. This non-probabilistic approach is simpler and works better in our experiments.

%However, other UNet based architecture could also be used to estimate the initial values, although that is not the focus of this paper.  

%  In order to estimate this embedding we make use of an encoder-and-decoder scheme in the form of a UNet architecture  which takes in the image pairs and outputs the embedding of the total deformation in the context of metamorphic image registration. This forms the parameter estimating phase of the total deformation in our architecture. %From this total deformation we disentangle the diffeomorphisms which model the shape changes with the intensity variations which model the appearance changes in our architecture.

\noindent
\textbf{Integration of Spatial Deformations and Intensity Variations.} To solve Eq.~\eqref{eq:deepMeta} with the estimated initial flows, we utilize Lipschitz Continuous ResNet Blocks (LC-RB) proposed in~\cite{joshi2021diffeomorphic}. These LC-RB blocks are used as numerical integration schemes for solving ODEs. We use the version without sharing weights among seven blocks, which models time-varying velocity fields and produces diffeomorphic deformations. 
%As shown in Fig.~\ref{fig:metaregnet}, two branches are used to integrate spatial deformations and intensity variations separately, using seven LC-RB blocks. 
That is, we obtain the diffeomorphic deformation $\phi(1)$, which captures the spatial transformations between the source and target images and deforms the healthy source image to generate parts of the final image. 

Another branch with the same number of LC-RB blocks produces incremental residual mappings of additive intensity changes between the input pair, starting from $q_{0}$. The intensity variations produced at the end, $q(1)$, are added onto the deformed source to approximate the target image $I_1$. Naively, if $q(1)$ was a simple pixel-/voxel-wise subtraction of the input pair, it would perfectly reconstruct the target image; however, in such a case, the anatomical matching constraint of image registration will not be met. 
%The anatomical structures between the source and target image must be solely matched using the deformation fields. 
To avoid this issue, we add a regularization similar to~\cite{maillard2022deep}, which restricts the learned intensity variations within the pathological region of the target image. That is, missing correspondences only happen within the pathological regions. We assume the availability of the binary mask $\mathcal{M}$ of the pathological region, like a tumor, and use it to mask out intensity variations in non-pathological regions, i.e., $q(1) \otimes \mathcal{M}$.

%perform pixel-/voxel-wise multiplication on the integrated intensity variations, i.e., $q(1) \otimes \mathcal{M}$, resulting in the estimated intensity variations.  

% using which we use a per voxel multiplication of the final intensity variations $q_{T}$ produced with $\mathcal{M}$ to give the output of this phase $\psi$.

% We utilize the LDDMM based formulation of ResNets as given in \cite{joshi2021diffeomorphic}, wherein we modify the architecture to adapt to the metamorphosis extension of the LDDMM framework \cite{trouve2005local,cao2005large}. It should also be noted that our choice of using the baseline architecture in \cite{joshi2021diffeomorphic} is motivated due to the fact that the diffemorphisms generated in this work are modeled as time-varying velocity fields,  unlike most other work \cite{mok2020fast,mok2020large,mok2022unsupervised} which are based on stationary velocity fields. 

% The total deformation generated above forms the input of this phase and parameterizes the velocity fields starting from $v_{0}$. We aim to produce smooth time-varying velocity fields which are integrated using ResNets as numerical schemes of differential equations \cite{joshi2021diffeomorphic}. 

% A series of ResNet blocks (like eight blocks used in the paper) are used to compute the velocity fields which are constrained to be smooth (by using Lipschitz continuous blocks) which form the diffeomorphic deformation $\phi$. In this way, we disentangle the shape from the total deformation embedding, and learn a diffeomorphic deformation which is used to deform the healthy source image.

% \textbf{Generation of Intensity Variations}
% The total deformation estimated also forms the input of this phase in order to produce infinitesimal incremental residual mappings of additive intensity changes between the source and target images starting from $q_{0}$. We use a series of ResNet blocks (same number of blocks as used in the previous phase) to compute the additive intensity changes from the source to the target image. The intensity variations produced at the end of the ResNet blocks $\psi$ are modeled as as a per-voxel addition in the spatial frame of the source image. Naively, if $\psi$ was a simple per-voxel subtraction of the source and target image it would perfectly reconstruct the target image, however, in such a case, the anatomical matching constraint of image registration will not be met. The anatomical structures between the source and target image must be solely matched using the deformation fields generated. In order to impose this we need added regularization\cite{maillard2022deep} so that the learned intensity variations are restricted to the region of the brain where the tumor is present in the target image. We assume that we have the binary segmentation mask $\mathcal{M}$ of the tumor in the pathological target image, using which we use a per voxel multiplication of the final intensity variations $q_{T}$ produced with $\mathcal{M}$ to give the output of this phase $\psi$.

% In the traditional setting of metamorphic image registration, $v_{0}$ is computed from $q_{0}$ and the source image $I_{0}$ making them the only parameters of the system. In this work, taking advantage of this tight coupling, we model the parameters of the total deformation of the image registration system first, which is then divided into two phases; one phase which models the deformation from the obtained $v_{0}$ and the other phase that models the intensity changes from the obtained $q_{0}$, as explained above. This is a simultaneous process where both deformations and intensity variations are learned at the same time.

\noindent
\textbf{Interpolation and Output.} In this step, we generate the final output to approximate the target image $I_1$. Firstly, with $\phi(1)$ we generate the deformed source image using a differentiable interpolation layer. For each voxel $p$ in the target image, this layer computes its location given at $\phi(p)$ in the source image and obtains its intensity value using linear interpolation. Upon this, we produce our final metamorphic output by adding the generated intensity variations, which performs a pixel-wise addition of $q(1) \otimes \mathcal{M}$ to the deformed image $\phi(1)\circ I_0$.

\noindent
\textbf{Loss Function.} Similar to metamorphic image registration formulated in Eq.~\eqref{eq:metamorphosis}, we have the image matching between our metamorphic output and the target image, and the regularization on the spatial deformation and the intensity variation. The overall loss function is formulated as
\begin{align*}
   %\begin{split}
   \mathcal{L} = & \lambda_{1}\mathcal{L}_{sim}(\frac{1}{|\Omega|} {\|I_{1} - \phi \circ I_{0} \|}_2^2) +  \lambda_{2}\mathcal{L}_{reg}(\nabla (q(1)\otimes \mathcal{M}) \\ 
     + & \lambda_{3}\mathcal{L}_{Jdet}(0.5(|\mathbb{J}(\phi(1))| - \mathbb{J}(\phi(1)))),
   %\end{split}
    %\label{eq:lossfunction}
\end{align*}
where $\Omega$ is the image spatial domain and $|\Omega|$ indicates the number of pixels or voxels in an image, and $\lambda_{1}$, $\lambda_{2}$ and $\lambda_{3}$ are the balancing weights, which are set to $[1.0, 1.0, 0.001]$, respectively, in our experiments.
We use mean squared error (MSE) as the image similarity metric $\mathcal{L}_{sim}$ to measure the goodness of image matching. We also discourage dramatic intensity changes within the learned intensity values by using a diffusion regularizer $\mathcal{L}_{reg}$ on the estimated intensity changes, where $\nabla$ is the spatial gradient operator. Besides, to restrict the learned deformations to be diffeomorphic we use $\mathcal{L}_{Jdet}$, where we penalize the total number of locations where the Jacobian determinants $|\mathbb{J}(\phi(1))|$ are negative. 


% In particular, we use the Mean Square Error (MSE) to measure the image matching result, the gradient penalty on the intensity variation to ensure its smoothness, the Jacobian penalty on the spatial deformation.



\section{Experiments}

\noindent
\textbf{Datasets.} 
% \noindent
{\it (1) BraTS 2021~\cite{baid2021rsna,menze2009menze,kuzilek2017open}.} This dataset includes image scans collected from 1251 subjects. Each scan is pre-processed by skull-strpping, co-registering to a common anatomical template, and being interpolated to the same resolution of $1 \times 1 \times 1 mm^{3}$, which is followed by an intensity normalization between $0$ to $1$. We select 120 slices with no tumor as our healthy image set and 120 slices with tumors as our pathological set, by checking their corresponding tumor masks. We keep aside 20 images from each set for testing and 5 for validation. As a result, we have 240 random image pairs for training, 5 pairs for validation, and 20 pairs for testing. 
%None of these subjects are overlapped in the creation of the training, validation, and test sets. %\fixme{do you normalize the image intensity to [0, 1]?}
%Since this dataset does not have any segmentation masks, except for tumor masks, we do not report dice score. 
%provided within the varied anatomical regions of the brain due to which we do not report the dice score.
{\it (2) 3D-IRCADb-01~\cite{3dircadb}.} This database is composed of 3D CT scans of 20 different patients with hepatic tumors. Each image has 74$\sim$260 slices with size of $512 \times 512$, which is resampled to a pixel spacing of 1mm. Since the pixel values are in Hounsfield and in the range of $[-1000,4000]$, we perform a color depth conversion using the mapping as in~\cite{almotairi2020liver}: $g = \frac{h - m_{1}}{m_{2} - m_{1}} \times 255$. Here, $g$ is the converted gray level value, $h$ is the Hounsfield value in the raw image, and $m_{1}$ and $m_{2}$ are the minimum and maximum of the Hounsfield range, respectively. Then, we crop the liver region using the provided liver mask and normalize the image intensity to [0, 1].
We collect 20 slices that contain healthy liver regions and 6 slices that have a pathological regions in the liver. We take 12 slices from the healthy set and 3 slices from the unhealthy set to make our training set, resulting in 36 image pairs, and take 3 healthy slices and 1 unhealthy slice to make 3 pairs for validation, and 5 healthy slices and 2 unhealthy slices to make 10 pairs for testing. 
% We keep aside 5 healthy slices, and 2 unhealthy slices to make 10 image pairs for testing; , making a pair of 10 images in our test set.  
Due to the limited training samples, we extend the training set to 144 pairs, via rotating each image by 90, 180 and 270 degrees. For both datasets, we have {\it subject-wise} splitting for training, validation, and test sets.   %random flips of the image scans. \fixme{what do you mean by "random flips"?}
% And finally, we take the remaining 3 slices from the healthy set and 1 slice in the unhealthy set to make 3 image pairs for the validation set. 

 %and pair the images to prepare the non-overlapping training, validation and test sets. 

\noindent
\textbf{Baseline Methods.} For comparison, we choose the diffeomorphic version of VoxelMorph~\cite{dalca2018unsupervised}, a deep-learning-based image registration model, as our baseline. Since VoxelMorph cannot handle the metamorphic image registration problem, we modify it and adopt the cost-function-masking (CFM) strategy~\cite{brett2001spatial} to exclude the similarity measure of the tumor regions using their masks during training. This modified VoxelMorph is denoted as VM-CFM.

% the tumor segmentation mask in the loss function to exclude the similarity measure of the tumor region during the training phase. This is not required in the test phase.

\noindent
\textbf{Implementation and Settings.} We implement our MetaRegNet using Keras and TensorFlow and train it in an end-to-end fashion, with the Adam optimizer and a fixed learning rate of $1e^{-4}$. Both our architecture and the baseline model VM-CFM have been deployed on the same machine with an Nvidia TITAN X GPU. We build our method on top of the R2Net implementation with default parameters reported in~\cite{joshi2021diffeomorphic}. All models are trained from scratch. 

%It is worth to mention that our source could be any healthy image and is not limited to a healthy atlas. 

%It is important to note that we train our algorithm to register any healthy source image to any pathological target image. We do not restrict our model to register only from an atlas.


\noindent
\textbf{Evaluation Metrics.} We measure the average Sum of Squared Distance (SSD) between the deformed source and target images, including the whole image (SSD-total) and the healthy region only (SSD-healthy). 
To measure the number of foldings in the estimated spatial deformations, we report the number of voxels with negative Jacobian determinants. Also, we measure the segmentation Dice score by using estimated deformations and the inference time as well. 








% \NEED TO ADD SOURCE TGT AND VOX OURS IN TABLES

\begin{figure}[t]
\centering
\setlength{\tabcolsep}{0.5pt}
    \begin{tabular}{lllllll}
        % Image Pair & $\phi \cdot I_{0}$ & $\phi \cdot I_{0} - I_{1}$  & $\phi$ \\
         \rotatebox[origin=l]{90}{\scriptsize Image Pair} &
         \includegraphics[width=0.14\textwidth]{figures/output4/source.pdf}  &
         \includegraphics[width=0.14\textwidth]{figures/output4/target.pdf} &
         &
         \;
         \includegraphics[width=0.14\textwidth]{figures/output6/source.pdf}  & \includegraphics[width=0.14\textwidth]{figures/output6/target.pdf}  & \\
         \rotatebox[origin=l]{90}{\scriptsize VM-CFM} &
         \includegraphics[width=0.14\textwidth]{figures/output4/vox.pdf} 
         &
        \includegraphics[width=0.14\textwidth]{figures/output4/voxdiff.pdf} &
        \includegraphics[width=0.14\textwidth]{figures/output4/voxphi.pdf} & \;
        \includegraphics[width=0.14\textwidth]{figures/output6/vox.pdf}     &
        \includegraphics[width=0.14\textwidth]{figures/output6/voxdiff.pdf} &
        \includegraphics[width=0.14\textwidth]{figures/output6/voxphi.pdf}
        \\
        \rotatebox[origin=l]{90}{ \scriptsize Ours} &
        \includegraphics[width=0.14\textwidth]{figures/output4/nsvf.pdf}    &
        \includegraphics[width=0.14\textwidth]{figures/output4/nsvfdiff.pdf} &
        \includegraphics[width=0.14\textwidth]{figures/output4/nsvfphi.pdf} & \;
        \includegraphics[width=0.14\textwidth]{figures/output6/nsvf.pdf}    &
        \includegraphics[width=0.14\textwidth]{figures/output6/nsvfdiff.pdf} &
        \includegraphics[width=0.14\textwidth]{figures/output6/nsvfphi.pdf} \\
         \rotatebox[origin=l]{90}{\scriptsize Image Pair} &
         \includegraphics[width=0.14\textwidth]{liver/output1/source.pdf}  &
         \includegraphics[width=0.14\textwidth]{liver/output1/target.pdf} & & \;
         \includegraphics[width=0.14\textwidth]{liver/output2/source.pdf}  & \includegraphics[width=0.14\textwidth]{liver/output2/target.pdf}  & \\
         \rotatebox[origin=l]{90}{\scriptsize VM-CFM} &
         \includegraphics[width=0.14\textwidth]{liver/output1/vox.pdf} 
         &
        \includegraphics[width=0.17\textwidth]{liver/output1/voxdiff.pdf} &
        \includegraphics[width=0.14\textwidth]{liver/output1/voxphi.pdf} & \;
        \includegraphics[width=0.14\textwidth]{liver/output2/vox.pdf}     &
        \includegraphics[width=0.17\textwidth]{liver/output2/voxdiff.pdf} &
        \includegraphics[width=0.14\textwidth]{liver/output2/voxphi.pdf}
        \\
        \rotatebox[origin=l]{90}{\scriptsize Ours} &
        \includegraphics[width=0.14\textwidth]{liver/output1/nsvf.pdf}    &
        \includegraphics[width=0.17\textwidth]{liver/output1/nsvfdiff.pdf} &
        \includegraphics[width=0.14\textwidth]{liver/output1/nsvfphi.pdf} & \;
        \includegraphics[width=0.14\textwidth]{liver/output2/nsvf.pdf}    &
        \includegraphics[width=0.17\textwidth]{liver/output2/nsvfdiff.pdf} &
        \includegraphics[width=0.14\textwidth]{liver/output2/nsvfphi.pdf}
    \end{tabular}
    \caption{Qualitative comparison between VM-CFM and our MetaRegNet on brain and liver datasets. Given an input pair on the top, the final output, its intensity difference to the target, and the spatial deformation are shown from left to right for each sample.}
    %Left: shown on two examples of clean source and pathological target image pairs (Top Row). For both algorithms VM-Diff-CFM and Ours (MetaRegNet) from Left to Right: output deformed image, image intensity difference map, and generated deformations.}
    \label{fig:comparison_BraTS}
\end{figure}


% \begin{figure}[t]
% \centering
% \setlength{\tabcolsep}{0.5pt}
%     \begin{tabular}{lllllll}
%         % Image Pair & $\phi \cdot I_{0}$ & $\phi \cdot I_{0} - I_{1}$  & $\phi$ \\
%          \rotatebox[origin=l]{90}{\scriptsize Image Pair} &
%          \includegraphics[height=0.15\textwidth]{liver/output1/source.pdf}  &
%          \includegraphics[height=0.15\textwidth]{liver/output1/target.pdf} & & \;
%          \includegraphics[height=0.15\textwidth]{liver/output2/source.pdf}  & \includegraphics[height=0.15\textwidth]{liver/output2/target.pdf}  & \\
%          \rotatebox[origin=l]{90}{\scriptsize VM-Diff-CFM} &
%          \includegraphics[height=0.15\textwidth]{liver/output1/vox.pdf} 
%          &
%         \includegraphics[height=0.15\textwidth]{liver/output1/voxdiff.pdf} &
%         \includegraphics[height=0.15\textwidth]{liver/output1/voxphi.pdf} & \;
%         \includegraphics[height=0.15\textwidth]{liver/output2/vox.pdf}     &
%         \includegraphics[height=0.15\textwidth]{liver/output2/voxdiff.pdf} &
%         \includegraphics[height=0.15\textwidth]{liver/output2/voxphi.pdf}
%         \\
%         \rotatebox[origin=l]{90}{\scriptsize Ours} &
%         \includegraphics[height=0.15\textwidth]{liver/output1/nsvf.pdf}    &
%         \includegraphics[height=0.15\textwidth]{liver/output1/nsvfdiff.pdf} &
%         \includegraphics[height=0.15\textwidth]{liver/output1/nsvfphi.pdf} & \;
%         \includegraphics[height=0.15\textwidth]{liver/output2/nsvf.pdf}    &
%         \includegraphics[height=0.15\textwidth]{liver/output2/nsvfdiff.pdf} &
%         \includegraphics[height=0.15\textwidth]{liver/output2/nsvfphi.pdf}
%     \end{tabular}
%     \caption{Qualitative comparison between VM-Diff-CFM on our MetaRegNet using two liver samples. The clean source and pathological target image pairs are shown on the top row. For each sample, the final output, its intensity difference to the target image, and the spatial deformation are shown from left to right in the bottom two rows.}
%     %Visualization results of metamorphic registration for the 3D-IRCADb-01 dataset shown on two examples of clean source and pathological target image pairs (Top Row). For both algorithms VM-Diff-CFM and Ours (MetaRegNet) from Left to Right: output deformed image, image intensity difference map, and generated deformations.}
%     \label{fig:comparison_liver}
% \end{figure}


% \begin{figure}[h]
% \centering
% \setlength{\tabcolsep}{0.5pt}
%     \begin{tabular}{lllllll}
%         % Image Pair & $\phi \cdot I_{0}$ & $\phi \cdot I_{0} - I_{1}$  & $\phi$ \\
%          \includegraphics[height=0.20\textwidth]{liver/output1/source.pdf}  & 
%          \includegraphics[height=0.20\textwidth]{liver/output1/vox.pdf}     &
%         \includegraphics[height=0.20\textwidth]{liver/output1/voxdiff.pdf} &
%          \includegraphics[height=0.20\textwidth]{liver/output1/voxphi.pdf}  \\ %\rotatebox{270}{\small{VM-CFM}}  \\
%         \includegraphics[height=0.20\textwidth]{liver/output1/target.pdf}  & 
%         \includegraphics[height=0.20\textwidth]{liver/output1/nsvf.pdf}    &
%         \includegraphics[height=0.20\textwidth]{liver/output1/nsvfdiff.pdf} &
%         \includegraphics[height=0.20\textwidth]{liver/output1/nsvfphi.pdf}  \\
%         %  \rotatebox{270}{\small{MetaRegNet}} \\
            
            
%         \includegraphics[height=0.20\textwidth]{liver/output2/source.pdf}  & 
%         \includegraphics[height=0.20\textwidth]{liver/output2/vox.pdf}     &
%         \includegraphics[height=0.20\textwidth]{liver/output2/voxdiff.pdf} &
%         \includegraphics[height=0.20\textwidth]{liver/output2/voxphi.pdf}  \\         %& \rotatebox{270}{\small{VM-CFM}}  \\
%         \includegraphics[height=0.20\textwidth]{liver/output2/target.pdf}  & 
%         \includegraphics[height=0.20\textwidth]{liver/output2/nsvf.pdf}    &
%         \includegraphics[height=0.20\textwidth]{liver/output2/nsvfdiff.pdf} &
%         \includegraphics[height=0.20\textwidth]{liver/output2/nsvfphi.pdf} 
%     \end{tabular}
%     \caption{3D-IRCADb-01 visualization Results of metamorphic registration.}
%     \label{fig:comparison_BraTS}
% \end{figure}

% \begin{figure}[H]
% \centering
% \setlength{\tabcolsep}{0.5pt}
%     \begin{tabular}{cccc}
%          Image Pair & $\phi \cdot I_{0}$ & $\phi \cdot I_{0} - I_{1}$  & $\phi$ \\
   
%          \includegraphics[height=0.20\textwidth]{figures/output4/source.pdf}  &
%          \includegraphics[height=0.20\textwidth]{figures/output4/vox.pdf} 
%          &
%         \includegraphics[height=0.20\textwidth]{figures/output4/voxdiff.pdf} &
%         \includegraphics[height=0.20\textwidth]{figures/output4/voxphi.pdf}
%       \\
%         \includegraphics[height=0.20\textwidth]{figures/output4/target.pdf}  & 
%         \includegraphics[height=0.20\textwidth]{figures/output4/nsvf.pdf}    &
%         \includegraphics[height=0.20\textwidth]{figures/output4/nsvfdiff.pdf} &
%         \includegraphics[height=0.20\textwidth]{figures/output4/nsvfphi.pdf}  \\
%         \includegraphics[height=0.20\textwidth]{figures/output6/source.pdf}  & 
%         \includegraphics[height=0.20\textwidth]{figures/output6/vox.pdf}     &
%         \includegraphics[height=0.20\textwidth]{figures/output6/voxdiff.pdf} &
%         \includegraphics[height=0.20\textwidth]{figures/output6/voxphi.pdf}  \\         %& \rotatebox{270}{\small{VM-CFM}}  \\
%         \includegraphics[height=0.20\textwidth]{figures/output6/target.pdf}  & 
%         \includegraphics[height=0.20\textwidth]{figures/output6/nsvf.pdf}    &
%         \includegraphics[height=0.20\textwidth]{figures/output6/nsvfdiff.pdf} &
%         \includegraphics[height=0.20\textwidth]{figures/output6/nsvfphi.pdf}  \\
%         %\hline \\

%          \includegraphics[height=0.20\textwidth]{liver/output1/source.pdf}  & 
%          \includegraphics[height=0.20\textwidth]{liver/output1/vox.pdf}     &
%         \includegraphics[height=0.20\textwidth]{liver/output1/voxdiff.pdf} &
%          \includegraphics[height=0.20\textwidth]{liver/output1/voxphi.pdf}  \\ %\rotatebox{270}{\small{VM-CFM}}  \\
%         \includegraphics[height=0.20\textwidth]{liver/output1/target.pdf}  & 
%         \includegraphics[height=0.20\textwidth]{liver/output1/nsvf.pdf}    &
%         \includegraphics[height=0.20\textwidth]{liver/output1/nsvfdiff.pdf} &
%         \includegraphics[height=0.20\textwidth]{liver/output1/nsvfphi.pdf}  \\
%         %  \rotatebox{270}{\small{MetaRegNet}} \\
            
            
%         \includegraphics[height=0.20\textwidth]{liver/output2/source.pdf}  & 
%         \includegraphics[height=0.20\textwidth]{liver/output2/vox.pdf}     &
%         \includegraphics[height=0.20\textwidth]{liver/output2/voxdiff.pdf} &
%         \includegraphics[height=0.20\textwidth]{liver/output2/voxphi.pdf}  \\         %& \rotatebox{270}{\small{VM-CFM}}  \\
%         \includegraphics[height=0.20\textwidth]{liver/output2/target.pdf}  & 
%         \includegraphics[height=0.20\textwidth]{liver/output2/nsvf.pdf}    &
%         \includegraphics[height=0.20\textwidth]{liver/output2/nsvfdiff.pdf} &
%         \includegraphics[height=0.20\textwidth]{liver/output2/nsvfphi.pdf}  \\
            
%     \end{tabular}
%     \caption{3D-IRCADb-01 visualization Results of metamorphic registration.}
%     \label{fig:comparison_BraTS}
% \end{figure}


\begin{table}[t]
  \caption{Comparison between VM-CFM and our method on brain and liver datasets.} %image matching over the entire image (SSD-total) and over only the healthy regions by masking out tumor regions (SSD-healthy), the number of foldings in deformation maps by counting their number of locations with negative determinant of Jacobian, the inference time for a pair of images, and the segmentation dice score by using the estimated spatial deformations.}
    \label{tab:quant}
 \setlength{\tabcolsep}{3pt}
 \centering
  \begin{tabular}{ccccccc}
  
  \bfseries Data & \bfseries Method & \bfseries SSD ($e^{-1}$) & \bfseries SSD ($e^{-1}$) & \bfseries \#Foldings & \bfseries Time (ms) & \bfseries Dice \\
  & & SSD-total & SSD-healthy &  & (per img. pair)\\
 \hline  
 
Brain & VM-CFM & $0.13_{\pm0.003}$ & $0.1_{\pm 0.002}$ & $88.585_{\pm76.63}$ &21 & -\\
   & MetaRegNet & $\textbf{0.08}_{\pm0.002}$ & $\textbf{0.07}_{\pm0.002}$ & $\textbf{3.62}_{\pm6.34}$  & 23 & -\\
  \hline
   
   Liver & VM-CFM & $1.5_{\pm0.004}$ & $0.07_{\pm0.003}$  &  $23.16_{\pm34.56}$ & 22 & 0.88\\
 & MetaRegNet & $\textbf{0.40}_{\pm0.012}$ & $\textbf{0.04}_{\pm0.002}$ &  $\textbf{1.00}_{\pm4.90}$  & 24 &  \textbf{0.95} \\
 \hline
  \end{tabular}
\end{table}

\begin{figure}[t]
\centering
\setlength{\tabcolsep}{0.5pt}
    \begin{tabular}{llllll}
         $I_0$ & $I_1$ & $\mathcal{M}$ & $q(1) \otimes  \mathcal{M}$  & $\phi(1) \circ I_0$ & Final  \\
           \includegraphics[width=0.15\textwidth]{figures/output4/source.pdf}  &
            \includegraphics[width=0.15\textwidth]{figures/output4/target.pdf}  & \includegraphics[width=0.15\textwidth]{figures/output4/mask.pdf}    &
            \includegraphics[width=0.15\textwidth]{figures/output4/app2.pdf} &
            \includegraphics[width=0.15\textwidth]{figures/output4/onlydeformation.pdf} &
             \includegraphics[width=0.15\textwidth]{figures/output4/nsvf.pdf} \\ 
             
          \includegraphics[width=0.15\textwidth]{liver/output1/source.pdf}  & 
        \includegraphics[width=0.15\textwidth]{liver/output1/target.pdf}  & \includegraphics[width=0.15\textwidth]{liver/output1/mask.pdf}     &
        \includegraphics[width=0.185\textwidth]{liver/output1/app2.pdf} &
          \includegraphics[width=0.15\textwidth]{liver/output1/onlydeformation.pdf} &
        \includegraphics[width=0.15\textwidth]{liver/output1/nsvf.pdf} \\
            
            %&\rotatebox{270}{\small{VM-CFM}}  \\
    \end{tabular}
    \caption{Image appearance separation learned by MetaRegNet. Left to right: clean source image $I_{0}$, pathological target image $I_{1}$, tumor mask $\mathcal{M}$, learned intensity changes within the tumor region $q(1) \otimes \mathcal{M}$, the deformed source image $\phi(1) \circ I_0$, and the final output.}
    \label{fig:appearance}
\end{figure}



\noindent
\textbf{Experimental Results.}
Table~\ref{tab:quant} reports our quantitative results. Compared to VM-CFM, our model provides a better matching, not only in the healthy region but also in the whole image, for both brain and liver datasets. Fig.~\ref{fig:comparison_BraTS} presents the visual improvement, showing better matching in both tumor and healthy regions, for images with either large or small tumors. Our deformations are much smoother, as demonstrated by the much fewer foldings reported in Table~\ref{tab:quant} and smoother maps within tumors and their surrounding regions in Fig.~\ref{fig:comparison_BraTS}. 
% Our improvement in image matching is clearly shown in . For images with either large or small tumors, our method presents much better matching results in the tumor regions, and visible improvements in the healthy regions. 
% Also, the few numbers of foldings reported in Table~\ref{tab:quant} and the smooth deformation maps in Fig.~\ref{fig:comparison_BraTS} show that we produce much smoother/diffeomorphic mappings than VM-CFM, especially within tumors and their surrounding regions. 
Unlike VM-CFM, we have spatial deformations going under the tumor regions, and the rest appearance changes are contributed by the intensity variance, indicating the appearing tumors from a healthy source to a pathological target, which is also observed in Fig.~\ref{fig:appearance}.
%via the appearance map learned within the tumor regions ($\phi_{iv} \otimes  \mathcal{M}$) and the deformed image ($\phi_{sd}(1) \circ I_0$). 
%That is, the deformed images have contributions to the appearance of the tumor regions, and the appearance map is used to adjust the intensity up or down to match the final appearance. 
To further evaluate the effectiveness of MetaRegNet, we use deformation maps to transfer the segmentation mask of the source image to match the target one. Since only liver masks are available, we apply our method on liver segmentation and obtain a mean Dice score of 0.95, compared to 0.88 produced by VM-CFM. And we only takes 2ms more for registering one pair.


%The image intensity difference maps clearly show the per pixel intensity variation between the generated images by both methods compared to the expected target image. In this, our method shows much whiter results (which basically means intensity difference is 0) or lighter shades (intensity difference is close to 0) as compared to VoxelMorph-CFM. It should be noted that our method produces good image matching results even for cases where the target image has very big tumors. This shows that the addition of intensity variations (which is only restricted to the tumor region) as well as the deformation of the healthy regions of the brains have been successful in both examples of registration with small and big tumors. This is an especially difficult task for registration methods which are dependent on cost function masking. 


%Figure~\ref{fig:appearance} shows our appearance maps learned in the specific regions of the target images where there are tumors. 

%These maps are added to the deformed images generated to produce the final outputs.

%The smoothness and regularity of the deformation fields can be seen by the number of foldings reported in Table~\ref{tab:quant}. The average value of the total number of foldings across the test set are shown. Our method produces much less number of foldings than VoxelMorph-CFM. This shows that our method is diffeomorphic. Inspite of having really large deformations due to the movement because of the presence of big tumors (in BraTS2021) or multiple tumors in different locations throughout the anatomy (in 3D-IRCADb-01),  our method still produces smooth deformation fields.  

%The runtime of our method is comparable to that of VoxelMorph-Diff-CFM \cite{dalca2018unsupervised} since it only takes a few more milliseconds to register a new image pair on both the datasets. It is important to note that VoxelMorph uses the scaling and squaring \cite{arsigny2006log} for integration of stationary velocity fields, in that, they work on half-scale velocity fields. On the other hand, we employ ResNets as flows of diffeomorphisms to integrate time-varying velocity fields and we work on the original full-scale velocity fields. This means that our method handles the task of registration at the original scale while only taking a few more milliseconds.




\section{Conclusion and Future Work}


% Overall, our method produces good matching and smooth deformations, and our spatial deformations look more realistic than those generated by the cost function masking strategy. We can handle challenging cases like having large deformations due to the presence of big tumors as shown in the brain tumor case or having multiple tumors at different locations throughout the anatomy in the liver tumor case.

In this paper, we propose a metamorphic image registration framework, MetaRegNet, which utilizes LC-ResNet blocks as flow integrator and allows for joint estimation of spatial deformation and intensity variation between a healthy source image and a pathological target image. MetaRegNet is successfully applied on two real datasets with big tumors or multiple ones in the same scan, by producing more realistic matching results and smoother deformations compared to exist methods. 
%, BraTS 2021 and 3D-IRCADb-01, which contain 
%Our method can account for image deformations and also simultaneously estimate changes in the form of intensity variations to images due to pathologies. 
% We utilize LC-ResNet blocks as integrators for flows of diffeomorphisms to estimate the spatial deformations and use the same to generate the additive intensity changes accordingly.
%One limitation of our method is the possibility of generating negative intensity values in the final output, because of the intensity adjustments introduced by the learned intensity variations. This is observed in the second example of Fig.~\ref{fig:comparison_liver}, which shows a gray background not a black one because the generated tumor region has negative values, which is mapped to the black color. A possible solution is to add some constraints on the intensity range of the final output, which is left as our future work. 
Limited by few available 3D medical datasets with both normal and tumor images, we currently work on 2D images. However, the model is developed for 3D metamorphic image registration and we will perform experiments on $3D$ medical scans in future work, as well as other applications with considering more anatomies and image modalities. Another possible extension of our work is the adaption to a model that registers any pairs of healthy and pathological images, including the registration between two pathological image scans. 

%\fixme{A limitation of our method is, the learned intensity variations added to the deformed image can contain negative values. This is evident from  Fig.~\ref{fig:comparison_liver}, example on the right. Due to this the background is grey, since the tumor region has some negative values seen as black. A simplistic solution such as adding a sigmoid layer to renormalize the added intensity values between $0$ and $1$ might not work, since this will also change the intensity values in the healthy region of the brain (which is learned via the deformation alone), and as such will not be a metamorphic solution. Hence, we need a more sophisticated solution and the authors leave this aspect as an extension of the current work.}



%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%

%\newpage

\bibliographystyle{splncs04}
\bibliography{refs}
%

\end{document}