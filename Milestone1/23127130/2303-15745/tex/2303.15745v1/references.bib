@misc{ma18,
  doi = {10.48550/ARXIV.1806.06144},
  url = {https://arxiv.org/abs/1806.06144},
  author = {Ma, Siyuan and Belkin, Mikhail},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Kernel machines that adapt to GPUs for effective large batch training},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{liu20,
    author = {Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
    title = {On the Linearity of Large Non-Linear Models: When and Why the Tangent Kernel is Constant},
    year = {2020},
    isbn = {9781713829546},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {The goal of this work is to shed light on the remarkable phenomenon of "transition to linearity" of certain neural networks as their width approaches infinity. We show that the "transition to linearity" of the model and, equivalently, constancy of the (neural) tangent kernel (NTK) result from the scaling properties of the norm of the Hessian matrix of the network as a function of the network width. We present a general framework for understanding the constancy of the tangent kernel via Hessian scaling applicable to the standard classes of neural networks. Our analysis provides a new perspective on the phenomenon of constant tangent kernel, which is different from the widely accepted "lazy training". Furthermore, we show that the "transition to linearity" is not a general property of wide neural networks and does not hold when the last layer of the network is non-linear. It is also not necessary for successful optimization by gradient descent.},
    booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
    articleno = {1338},
    numpages = {11},
    location = {Vancouver, BC, Canada},
    series = {NIPS'20}
}

@article{hofmann08,
    author = {Thomas Hofmann and Bernhard Sch{\"o}lkopf and Alexander J. Smola},
    title = {{Kernel methods in machine learning}},
    volume = {36},
    journal = {The Annals of Statistics},
    number = {3},
    publisher = {Institute of Mathematical Statistics},
    pages = {1171 -- 1220},
    keywords = {graphical models, machine learning, reproducing kernels, Support vector machines},
    year = {2008},
    doi = {10.1214/009053607000000677},
    URL = {https://doi.org/10.1214/009053607000000677}
}

@article{radhakrishnan23,
    author = {Radhakrishnan, Adityanarayanan and Beaglehole, Daniel and  Pandit, Parthe and Belkin, Mikhail},
    title = {Feature learning in neural networks and kernel machines that recursively learn features},
    publisher = {arXiv},
    year = {2023},
    copyright = {arXiv.org perpetual, non-exclusive license},
    url = {https://arxiv.org/abs/2212.13881},
    journal = {arXiv}
}

@inproceedings{Nak19,
    title={Deep Double Descent: Where Bigger Models and More Data Hurt},
    author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=B1g5sA4twr}
}

@inproceedings{Paszke2019,
  title={PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle={Advances in Neural Information Processing Systems 32},
  editor={Wallach, H. and Larochelle, H. and Beygelzimer, A. and "d'Alché-Buc", F. and Fox, E. and Garnett, R.},
  pages={8024-8035},
  year={2019},
  organization={Curran Associates, Inc.},
  url={http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@inproceedings{MahanolobisDistance,
  title={On the generalized distance in statistics},
  author={Mahalanobis, P.C.},
  booktitle={Proceedings of the National Institute of Science of India},
  year={1936}
}

@inproceedings{NTK,
  title={Neural Tangent Kernel: Convergence and generalization in neural networks},
  author={Jacot, A. and Gabriel, F. and Hongler, C.},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
}

@inproceedings{DALLE,
  title={Zero-shot text-to-image generation},
  author={Ramesh, A. and Pavlov, M. and Goh, G. and Gray, S. and Voss, C. and Radford, A. and Chen, M. and Sutskever, I.},
  booktitle={International Conference on Machine Learning},
  year={2021}
}

@inproceedings{GPT3,
  title={Language models are few-shot learners},
  author={Brown, T. and Mann, B. and Ryder, N. and Subbiah, M. and Kaplan, J. and Dhariwal, P. and Neelakantan, A. and Shyam, P. and Sastry, G. and Askell, A. and Agarwal, S. and Herbert-Voss, A. and Krueger, G. and Henighan, T. and Child, R. and Ramesh, A. and Ziegler, D. and Wu, J. and Winter, C. and Amodei, D.},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}

@inproceedings{Midjourney,
    doi = {10.1145/3569219.3569352},
    url = {https://doi.org/10.1145\%2F3569219.3569352},
    year = {2022},
    month = {nov},
    publisher = {{ACM}},
    author = {Jonas Oppenlaender},
    title = {The Creativity of Text-to-Image Generation},
    booktitle = {25th International Academic Mindtrek conference}
}

@misc{llama,
    title={LLaMA: Open and Efficient Foundation Language Models}, 
    author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
    year={2023},
    eprint={2302.13971},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias–variance trade-off},
  author={Belkin, M. and Hsu, D. and Ma, S. and Mandal, S.},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019}
}

@inproceedings{belkin2018understand,
  title={To understand deep learning we need to understand kernel learning},
  author={Belkin, M. and Ma, S. and Mandal, S.},
  booktitle={International Conference on Machine Learning},
  pages={541--549},
  year={2018},
  organization={PMLR}
}

@inproceedings{FeatureLearningEmergenceShi,
  title={A theoretical analysis on feature learning in neural networks: Emergence from inputs and advantage over fixed features},
  author={Shi, Z. and Wei, J. and Lian, Y.},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@misc{mei2020generalization,
  title={The generalization error of random features regression: Precise asymptotics and double descent curve}, 
  author={Song Mei and Andrea Montanari},
  year={2020},
  eprint={1908.05355},
  archivePrefix={arXiv},
  primaryClass={math.ST}
}

@inproceedings{RR08,
    author = {Rahimi, Ali and Recht, Benjamin},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Random Features for Large-Scale Kernel Machines},
    url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf},
    volume = {20},
    year = {2007}
}
