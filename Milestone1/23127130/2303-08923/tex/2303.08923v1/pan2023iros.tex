 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BEFORE YOU START:
%%
%% 1. Rename the paper.tex file into your paper name. Use the BibTeX key policy
%%    for the naming convention (see end of this file)
%%
%% 2. Change line 3 in the Makefile from "TARGET=paper" to "TARGET=name-of-tex-file"
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}
\IEEEoverridecommandlockouts    % This command is only needed if
\overrideIEEEmargins            % Needed to meet printer requirements.

\input{stachnisslab-latex}
\input{stachnisslab-math}

%% Aligns the last page but causes errors on 
%% some machines (such as OSX), so don't use it for now.
\usepackage{flushend}
\usepackage{siunitx}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{subfigure}
\usepackage{tabularx}
\usepackage{comment}

\usepackage{stmaryrd}
\usepackage{multirow}
\usepackage{pifont}

\newcommand{\rgbd}{\mbox{RGB-D}\ } 
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newcommand{\equsize}{\normalsize} % may change to \small to save space

\renewcommand{\and}{\hspace{1.0cm}} %% better to adjust the space between the authors.

% %%%%%%%%% FOR CVPR STYLE
\title{\LARGE \bf  Panoptic Mapping with Fruit Completion \\ and Pose Estimation for Horticultural Robots}
% % Change the title later, hightlight the contribution (such as shape and pose joint estimation)
\author{Yue Pan \and Federico Magistri \and Thomas LÃ¤be \and Elias Marks \\ Claus Smitt \and  Chris McCool \and Jens Behley \and Cyrill Stachniss}

\begin{document}

\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\begin{center}
    \centering
    \vspace{-2em}
    \captionsetup{type=figure}
    \includegraphics[width=0.94\linewidth]{pics/teaser_v2.pdf}
    \setlength{\abovecaptionskip}{1pt}
    \captionof{figure}{Our method is able to build a multi-resolution panoptic map (top) of a challenging commercial glasshouse environment online using a mobile horticultural robot equipped with RGB-D cameras (left). Furthermore, our method manages to jointly estimate the complete shape and pose of each fruit in the map (bottom).}
    \label{fig:mot}
    \vspace{2mm}
\end{center}%
}]

\makeatletter{\renewcommand*{\@makefnmark}{}
\footnotetext{All authors are with the University of Bonn, Germany. Cyrill Stachniss is additionally with the Department of Engineering Science at the University of Oxford, UK, and with the Lamarr Institute for Machine Learning and Artificial Intelligence, Germany.}\makeatother
\footnotetext{This work has partially been funded 
  by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy, EXC-2070 -- 390732324 -- PhenoRob and under STA~1051/5-1 within the FOR 5351~--~459376902~(AID4Crops).}\makeatother
}


%% Style hacks to save space between floating objects and text
\setlength{\textfloatsep}{1.3em}
\setlength{\dbltextfloatsep}{1.3em}

\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
  Monitoring plants and fruits at high resolution play a key role in the future of agriculture. Accurate 3D information can pave the way to a diverse number of robotic applications in agriculture ranging from autonomous harvesting to precise yield estimation. Obtaining such 3D information is non-trivial as agricultural environments are often repetitive and cluttered, and one has to account for the partial observability of fruit and plants.
  In this paper, we address the problem of jointly estimating complete 3D shapes of fruit and their pose in a 3D multi-resolution map built by a mobile robot. 
  To this end, we propose an online multi-resolution panoptic mapping system where regions of interest are represented with a higher resolution. We exploit data to learn a general fruit shape representation that we use at inference time together with an occlusion-aware differentiable rendering pipeline to complete partial fruit observations and estimate the 7 DoF pose of each fruit in the map.
  The experiments presented in this paper, evaluated both in the controlled environment and in a commercial greenhouse, show that our novel algorithm yields higher completion and pose estimation accuracy than existing methods, with an improvement of 41\% in completion accuracy and 52\% in pose estimation accuracy while keeping a low inference time of 0.6\,s in average.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

%%%%%%%%%%%%%%%%%%%
%% WHY: 
% First, answer the WHY question: Why is that relevant? Why should I be
% motivated to read the paper? Why should I care? (1 paragraph, 2-5 sentences)
% \the\linewidth
To feed an ever-growing world population, the whole agricultural sector needs to increase its productivity while reducing its negative impact on the environment. Autonomous robots have the potential to support addressing both issues.
For example, robots can automate labor-intensive tasks such as harvesting~\cite{arad2020jfr, lehnert2017ral}, weeding~\cite{mccool2018ral}, or pruning~\cite{botterill2017jfr}. 
They can also provide plant-specific treatment of herbicides and pesticides~\cite{ahmadi2022iros} reducing the required amount of agro-chemicals. 
Thus increasing the likelihood of meeting the population demands in producing food, feed, fiber, and fuel and at the same time decreasing the use of agro-chemicals.

%%%%%%%%%%%%%%%%%%%
%% WHICH PROBLEM
% Second, explain WHICH problem you are solving/address to solve.
Robots can continuously monitor orchards or arable fields to detect early stages of plant stress~\cite{yi2020sensors}, support phenotyping activities~\cite{zermas2020cea}, and provide detailed yield estimates~\cite{kierdorf2022fai}.  
Robots working in arable fields or horticulture environments can seldomly observe the whole scene due to the cluttered nature of the environments. This means that data obtained with any agricultural robot is partial and incomplete.
In this paper, we address the problem of building multi-resolution 3D maps of such scenes and estimating the non-visible parts of fruit to obtain more complete 3D models. \figref{fig:mot} depicts an example of the resulting map together with the 3D shape and pose estimated for each fruit.

% TODO: update this
\begin{figure*}[ht]
  \centering  
  \includegraphics[width=0.96\linewidth]{pics/pipeline.pdf}
  % \setlength{\abovecaptionskip}{4pt}
  \caption{Overview pipeline of the proposed mapping system: the input to the mapping system is a stream of the \rgbd images collected by the horticultural mobile robot. We predict the fruit instance using Mask R-CNN~\cite{he2017iccv-mr}. Then we conduct panoptic volumetric mapping by tracking the instance lables as the temporal consistent submap IDs. For each fruit submap, we jointly complete its shape and estimate its pose using offline learned shape prior and the differentiable rendering pipeline.}
  \label{fig:overview}
  \vspace{-10pt}
\end{figure*}
%%%%%%%%%%%%%%%%%%%
%% HOW & WHAT
% Third, explain briefly how one can address the problem in general and mention 
% briefly what others/we before have done. Prepare the reader for your contribution 
% that comes in the next section (and not here!).
To recover the shapes of occluded objects, a typical solution is the usage of templates whose deformations are able to represent different instances of the same category. A template can be represented as a 3D triangular mesh~\cite{magistri2021icra} or encoded in the weights of a neural network~\cite{park2019cvpr}. In recent years, templates have been used to recover shapes of human bodies~\cite{gall2009cvpr} and hands~\cite{tagliasacchi2015cgf}, but also in the agricultural context to estimate shapes of fruits~\cite{magistri2022ral-iros} and plants~\cite{marks2022icra}.

%%%%%%%%%%%%%%%%%%%
%% MAIN CONTRIBUTION & WHAT FOLLOWS FROM THAT
% Explain your contribution in one paragraph. This is a very important paragraph. 
% Always start that paragraph with: ``The main contribution of this paper is''
The main contribution of this paper is a novel method to jointly estimate the 3D shape of fruits and their pose. We build multi-resolution maps in which we place the predicted 3D shapes of fruits correctly posed in a globally consistent representation. Additionally, we exploit high-resolution 3D data to encode a general fruit representation into the weights of a neural network. In this way, we can recover details of the fruit's shape with a low inference time during operations.

%%%%%%%%%%%%%%%%%%%
%% OUR KEY CLAIMS (can be merged with the main contribution above if desired)
% Explicitly(!) state your claims in one (short) paragraph and make
% sure you pick them up again in the experiments and support every claim.
In sum, we make three key claims:
our approach is able to jointly
%
(i) predict the 3D shape of fruits even under substantial occlusions in real commercial glasshouse environments and
%
(ii) estimate the pose of each fruit in the 3D map.
%
(iii) Additionally, our multi-resolution map representation yields substantial improvement in the shape completion task over fixed-resolution maps.
%
These claims are backed up by our experimental evaluation.
We will furthermore release the code and pre-trained model upon the acceptance of the paper.

% Pollination, pruning, and harvesting require the manipulation of plants, flowers, and fruits. To successfully execute such actions, an appropriate model of the scene must be available. Such models must include semantic and geometric information and answer the question "what is where?".

% (Our target can also be the flowers)

% The panoptic mapping system can build a global, medium-accurat 3D model of the whole row of plants together with the high-resolution submap or models of the individual fruits and flowers.

% Monitoring plants and furits for high-throughput phenotyping and autonomous harvesting.

% Benefits and applications of the complete 3D model: detailed yield and ripeness estimation by providing crop volume or autonomous harvesting providing precise fruit size, shape and pose.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec:related}

% Discuss the main related work and cite around 15-25 papers in sum. 
% The related work section should be approx. 1 column long, assuming 
% a 6-page paper.  Structure the section in paragraphs, grouping the 
% papers, and describing the key approaches with 1-2 sentences. If 
% applicable, describe the key difference to your approach at the end 
% of each paragraph briefly. Avoid adding subsections, al least for a 
% conference paper.

In recent years, agricultural robotics has become an increasingly popular research area due to the aforementioned challenges and the prospect of deploying such robots for more efficient and sustainable agri- and horticulture. More specifically, in the horticulture context we have seen deployed robotic systems for monitoring~\cite{smitt2021icra} and harvesting~\cite{arad2020jfr,lehnert2017ral}. In both cases, a fundamental build block of such robots is an instance segmentation network~\cite{he2017iccv-mr} that can robustly segment fruits~\cite{halstead2020dicta,mccool2016icra}, peduncles~\cite{sa2017icra}, and plants~\cite{halstead2021fps}, often starting from 2D images.
Neural networks can also be used to estimate ripeness~\cite{halstead2018ral} and poses~\cite{wagner2021icra} of fruits.
Based on such networks, it is possible to build semantically-aware 3D maps~\cite{schmid2022icra}. However, in the agricultural context, classical volumetric mapping pipelines, such as KinectFusion~\cite{newcombe2011ismar} or Voxblox~\cite{oleynikova2017iros}, yield incomplete fruits representations as they do not deal explicitly with the occlusions present in this context. Instead, we build upon a multi-resolution map from prior work~\cite{pan2022iros} and propose a pipeline to jointly estimate the pose and the complete shape of fruits in the 3D map resulting in a complete and better representation of fruits.

Recently, a variety of works tackle the problem of estimating the shape of non-visible fruits parts of plants or fruits. Such works can be generally divided into three categories: geometry-based, mesh-based, and deep learning-based approaches.
In the first category, a closed-form geometric model is fit into the collected data. Marangoz~\etalcite{marangoz2022case} use such an approach for fruit monitoring, while Lehnert~\etalcite{lehnert2016icra} show that fitting a superellipsoid improves robotic grasping performances. These approaches estimate complete shapes quickly but the model used cannot represent details in the 3D shapes. 

In the second category, the model is represented by a 3D mesh that represents the general appearance of a target object. We used this approach to estimate the shapes of plants~\cite{magistri2021icra} and leaves~\cite{marks2022icra}. While such methods can provide precise reconstructions, a shortcoming is the high inference time required for obtaining the final mesh. This is often not practical for real-world operations. 

In the last category, the general appearance of an object is encoded in the weights of a neural network that produces either point clouds~\cite{yuan2018threedv} or triangular meshes~\cite{park2019cvpr}. In our previous work~\cite{magistri2022ral-iros}, we use DeepSDF~\cite{park2019cvpr} to learn a prior over fruits shapes and map a single \rgbd frame to the network's latent space to avoid the online optimization. While such approaches provide plausible 3D shapes at a low inference time, they lack the ability to estimate 7 degrees of freedom (DoF) poses and the performance declines when processing the fruits with non-canonical poses in the real world. The effectiveness of DeepSDF-based methods~\cite{magistri2022ral-iros} on in-the-wild settings is thus limited by the requirement of having the input partial shapes in the same canonical pose and scale as in the training set. 

Inspired by the recent works in the computer vision community including Frodo~\cite{runz2020cvpr} and DSP-SLAM~\cite{wang2021threedv} we propose a novel method to jointly estimate the shape and the pose of fruits with a low inference time. We combine a completion network based on a neural shape prior and an occlusion-aware differentiable rendering pipeline. In this way, the shape completion and reconstruction are still conducted in the canonical pose of the fruit with high quality while obtaining the transformation to the world coordinate system and allowing for its embedding in a 3D model of the glasshouse and its plants.


%\todo{@Yue I don't know this works, you can add few words whenever you have time}
%Node-SLAM~\cite{sucar2020threedv}, Frodo~\cite{runz2020cvpr}, DSP-SLAM~\cite{wang2021threedv}, , VMap~\cite{kong2023arxiv}

% Downstream tasks:
% Planning, Monitoring, Localization, Harvesting, Volume estimation


%% BRIEFLY SUMMARIZE OWN CONTRIBUTION 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Our Approach to Fruit Mapping}
\label{sec:main}

%% Describe your approach. It is okay to divide the main section
%%  into a few subsections (e.g., 2-4 subsections).

In this paper, we study the problem of panoptic volumetric mapping for horticultural applications while estimating complete 3D shapes including scale and 7 DoF pose of fruits from a sequence of \rgbd frames.

As shown in \figref{fig:overview}, we build a panoptic volumetric map that decomposes the scene into the background and individual foreground submaps. The background submap is represented at a lower resolution than the foreground submaps to ensure accurate and high-fidelity reconstructions of the fruits. Based on the foreground submaps integrating partial observations, we estimate complete shapes and 7 DoF poses of each individual fruit using a deep neural network that jointly estimates the shape of the fruit in a canonical coordinate system and a transformation into the world coordinate frame.

% TODO: optimizing 7 DoF pose, but the scale is just for the domain gap between the training data and the testing data. We only evaluate the 6 DoF pose.  
\subsection{Multi-resolution Panoptic Mapping}
\label{sec:mapping}
The input to our mapping system is a stream of the \rgbd images collected by a horticultural mobile robot. The pose of the robot can be estimated by wheel odometry or a tracking camera and refined online with a standard \rgbd odometry system. % add reference here?
We train a Mask R-CNN~\cite{he2017iccv-mr} model to perform online instance segmentation of the fruits based on the RGB information resulting in per-fruit instance masks $\mathcal{M}$. We assign a submap to each panoptic entity, either the thing (fruit) instance or the stuff (background). For each submap, we apply a non-projective TSDF integration~\cite{pan2022iros} as well as a mesh reconstruction based on marching cubes algorithm~\cite{lorensen1987siggraph} incrementally using the \rgbd image stream and the estimated robot's poses. 

Meanwhile, we track the segmented instances as temporal consistent submap IDs through a data association. We project the mesh vertices of active submaps within the visual frustum onto the image planes to generate submap masks for newly collected image frames. Based on the intersection over union~(IoU) ratio between the rendered and predicted masks and the difference between the rendered and measured depth, we either assign a segmented instance to an existing submap or allocate a new submap for it. 

% describe data association with more details
As shown in \figref{fig:multires}, our volumetric mapping allows the fruits %or flowers
 to be reconstructed
at a small voxel size ($3\,\mathrm{mm}$) and with high accuracy, while using
larger voxel sizes ($1\,\mathrm{cm}$) to efficiently map the background or less
relevant classes such as leaves and stems. 
Therefore, the final panoptic map consists of a low-resolution submap for the background and high-resolution submaps for each fruit instance. 

Once a fruit submap is not observed for $G$ frames consecutively, it is not updated anymore, marked as frozen, and used for joint shape completion and pose estimation.


% TODO: update this
\begin{figure}
  \centering  
  \includegraphics[width=0.95\linewidth]{pics/multi_res_compressed.pdf}
  % \setlength{\abovecaptionskip}{2pt}
  \caption{On the left, we show an example of our multi-resolution volumetric map consisting of a lower resolution background (with larger voxel size) and several higher resolution fruit submaps (with smaller voxel size). On the right, we zoom in to show the high resolution submap representing a fruit.}
  \label{fig:multires}
  \vspace{2mm}
% \end{figure}
% \begin{figure}
  \centering  
  \includegraphics[width=0.97\linewidth]{pics/peppers_deepsdf_training_clear.pdf}
  % \setlength{\abovecaptionskip}{0pt}
  \caption{On the left, we show examples of the sweet pepper models with a canonical pose measured using a sub-millimeter precision laser scanner. On the right, for one example sweet pepper, we show both the close-to-surface sample points and the uniform free space sample points with their color representing the SDF value label. We use these sample points and their SDF value labels to train the DeepSDF model and learn the shape prior of the specific fruit species (in this case the sweet pepper). }
  \label{fig:deep_sdf_training}
  % \vspace{-10pt}
\end{figure}

\subsection{Fruit Completion and Pose Estimation in the Wild}
\label{sec:optimization}

By exploiting the fruit shape prior of a pre-trained DeepSDF~\cite{park2019cvpr} model and an occlusion-aware differentiable rendering technique, for each frozen fruit submap $\mathcal{S}$ from the panoptic volumetric map in \secref{sec:mapping}, we aim to jointly estimate its latent shape code $\v{z}$ and 7 DoF transformation $\mq{T}_{o w} \in \operatorname{Sim}(3)$ from the world coordinate system to the fruit's canonical coordinate system. $\mq{T}_{o w}$ is represented by an homogeneous transformation matrix with $s \in \RR$, $\m{R} \in\RR^{3\times3}$, $\v{t} \in \RR^3$ representing the scale, rotation,  and translation, respectively.
We sample a point cloud $\mathcal{P}_\mathcal{S}$ from the mesh of each instance submap $\mathcal{S}$. Additionally, from all the images in which $\mathcal{S}$ is visible, we get the 2D mask $\mathcal{M}$ predicted by Mask R-CNN, the extended 2D bounding box $\mathcal{B}$ with a padding of $h$ pixels on each side.






\textbf{DeepSDF pretraining:} The DeepSDF~\cite{park2019cvpr} model takes as input a query position $\v{x} \in \RR^3$ and a latent shape code $\v{z} \in \RR^{C}$, and predicts the SDF value $v \in \RR$ at $\v{x}$ through a decoder network $D_{\theta}$ as: 
% \begin{align}
  $v = D_\theta(\v{x}, \v{z})$.
% \end{align}
With a pre-trained DeepSDF decoder $D_{\theta}$ and optimized shape code $\v{z}$, we compute a dense SDF volume by querying it at a regular 3D grid of points, which we use for a complete mesh reconstruction via marching cubes~\cite{lorensen1987siggraph}. 

%% todo: think about a better motivation.
% 
In line with Magistri \etal~\cite{magistri2022ral-iros}, we aim at having an accurate and complete model of the fruits predicted by $D_\theta$ for which we use high-resolution 3D laser scans collected in a controlled laboratory environment using a sub-millimeter accurate Perceptron V5 laser scanner and a Romer Infinite measuring arm.
We train a DeepSDF model for each type of fruit using the measured 3D scans and instead of only sampling points close to the surface along the normal direction~\cite{park2019cvpr,magistri2022ral-iros}, we also sample points uniformly in a sphere surrounding the object as shown in \figref{fig:deep_sdf_training}. By doing so, the model better represents the free space, which is beneficial for differentiable rendering and pose estimation. Regarding model training, we adopt a latent code size of $C=32$ and adhere to the network architecture and hyperparameter configurations specified in the original work of DeepSDF~\cite{park2019cvpr}.
% (TODO) Also add an ablation study on DeepSDF training sample strategy
% Fix the sdf notation here, avoid confusing with the scale notation s

\begin{figure}
  \centering  
  \includegraphics[width=0.84\linewidth]{pics/loss_explain.pdf}
  % \setlength{\abovecaptionskip}{2pt}
  \caption{Geometric relationships for the joint optimization of the shape $\v{z}$ and the pose $\mq{T}_{o w}$ of a target fruit using the submap point cloud in the world frame $w$ and the image collected in the camera frame $c$. The black points represent the sampled points from the point cloud $\mathcal{P}_\mathcal{S}$ of the target fruit submaps. The orange points represent the sample points on each ray corresponding to each sampled pixel. The red, blue, and purple squares represent the sampled pixels from the foreground mask $\mathcal{M}$, the unmasked background region in the extended bounding box $\mathcal{B}$ and the potential occluded background region, respectively.}
  \label{fig:losses}
  % \vspace{-10pt}
\end{figure}

\textbf{Surface consistency loss:} Our first objective is to achieve precise alignment between the reconstructed fruit and the fused 3D observations from the \rgbd sensor. To accomplish this, as shown in~\figref{fig:losses}, we aim to keep the points from the target fruit submap's point cloud $\mathcal{P}_\mathcal{S}$ close to the iso-surface of the SDF predicted by $D_{\theta}$, which minimizes the following surface reconstruction loss $\mathcal{L}_{\mathrm{s}}$ given by:
\begin{align}
  \mathcal{L}_{\mathrm{s}} &= \frac{1}{\left|\mathcal{P}_\mathcal{S}\right|} \sum_{\q{p}^{w} \in \mathcal{P}_\mathcal{S}} D_\theta^2\left(\mq{T}_{o w}\q{p}^{w}, \v{z}\right).
\end{align}

\textbf{Occlusion-aware differentiable rendering:} 
As we use depth images and instance segments, we additionally propose a depth rendering loss and a mask rendering loss using differentiable SDF rendering.
As shown in~\figref{fig:losses}, for each image corresponding to a visible fruit submap, we sample $G_f$ foreground pixels $\boldsymbol{\Omega}_f$ from the mask $\mathcal{M}$ and $G_b$  background pixels $\boldsymbol{\Omega}_b$ from the unmasked region $\mathcal{B}\setminus\mathcal{M}$ in the extended bounding box $\mathcal{B}$. For each sampled pixel \mbox{$\v{u} \in \boldsymbol{\Omega} = \boldsymbol{\Omega}_f \cup \boldsymbol{\Omega}_b$}, we calculate the corresponding ray $\q{r}$ in the camera coordinate system. We then sample $N+1$ points with a fixed interval $\Delta d$ on each ray, resulting in per sample depths $d_i$:
\begin{align}
d_i &= d_{\min}+(i-1) \Delta d, \qquad i=1,\ldots, N+1,
\end{align}
along the ray $\q{r}$ from the projection center to the fruit, where $d_{\min}$ and $\Delta d$ are determined by the distance from the camera to the submap's bounding box center and the approximate fruit size. By transforming each sample point to the world coordinate system, we obtain:
\begin{align}
\q{p}^{w}_{i} = \mq{T}_{w c}\left(d_i \mq{K}^{-1} \q{u}\right),
\end{align}
where $\mq{K}$ and $\mq{T}_{w c}$ are the camera intrinsic and extrinsic matrices, respectively. Then,  the SDF prediction $v_i$ at the sample point $\q{p}^{w}_{i}$ is given by:
\begin{align}
  v_{i} &= D_\theta(\mq{T}_{o w}\q{p}^{w}_{i}, \v{z}),
\end{align}
which can then be converted to an estimated occupancy probability $o_{i}$ by a logistic function with a surface noise threshold $\sigma$:
\vspace{-1em}
\begin{align}
  o_{i}=\frac{1}{1+e^{v_{i} / \sigma}}.
\end{align}

Then, the so-called ray termination weight $\alpha_i$ for each sample point can be calculated from the occupancy probability as:
\vspace{-1em}
{\equsize 
\begin{equation}
\alpha_i = \begin{cases}o_i \prod_{j=1}^{i-1}\left(1-o_j\right) & i=1, \ldots, N \\ \prod_{j=1}^{N}\left(1-o_j\right) & i=N+1. \end{cases}
\end{equation}
}

Note that $\sum_{i=1}^{N+1} \alpha_i = 1$ always holds. We can then get the rendered mask $\hat{M}$ and depth $\hat{D}$ by integrating over all the samples along the ray as: 
{\equsize 
\begin{equation}
 \hat{M}=\sum_{i=1}^N \alpha_i, \quad \hat{D}=\sum_{i=1}^{N+1} \alpha_i d_i. 
\end{equation}
}

The depth rendering loss $\mathcal{L}_{\mathrm{d}}$ and mask rendering loss $\mathcal{L}_{\mathrm{m}}$ for $K$ image frames observing the fruit submap are calculated by comparing the rendered result $\hat{D}$ and $\hat{M}$ with the depth camera's measurement $D$ and the binary mask $M$ of the target fruit for each of the $G$ sampled pixels as:
% \vspace{-1em}
\begin{eqnarray}
  \mathcal{L}_{\mathrm{d}}=\frac{1}{KG} \sum_{k}\sum_{\mathbf{u} \in (\boldsymbol{\Omega}^k \setminus \boldsymbol{\Omega}^k_o)}\left(\hat{D}_{\mathbf{u}}-D_{\mathbf{u}}\right)^2,\\
  \mathcal{L}_{\mathrm{m}}=\frac{1}{KG} \sum_{k}\sum_{\mathbf{u} \in (\boldsymbol{\Omega}^k \setminus \boldsymbol{\Omega}^k_o)}\left(\hat{M}_{\mathbf{u}}-M_{\mathbf{u}}\right)^2.
  \label{equ:mask}
\end{eqnarray}


For background pixels, if $\hat{D}_{\mathbf{u}}-D_{\mathbf{u}}>d_o$, where $d_o$ is a small threshold, the pixel is regarded as lying in a potential occluded region of the fruit caused by leaves or other fruits, as shown in~\figref{fig:losses}. Such occluded pixels $\boldsymbol{\Omega}_o \subset \boldsymbol{\Omega}_b$ are not taken into account in the rendering loss $\mathcal{L}_{\mathrm{d}}$ and $\mathcal{L}_{\mathrm{m}}$. For the rest of background pixels $\boldsymbol{\Omega}_b \setminus \boldsymbol{\Omega}_o$, we take the termination depth $d_{\max}=d_{\min}+N\Delta d$ as the virtual depth measurements for depth rendering loss calculation so that we can enforce the silhouette consistency at the target fruit.

% TODO: add initial guess of the pose and the shape code
\subsection{Optimization}
%\textbf{Levenberg-Marquardt optimization:} 
With an additional shape code regularization term \mbox{$\mathcal{L}_{\mathrm{r}} = \|\v{z}\|^2$}, we use as our final loss function:
\begin{equation}
\mathcal{L}=w_{\mathrm{s}} \mathcal{L}_{\mathrm{s}}+w_{\mathrm{d}} \mathcal{L}_{\mathrm{d}}+w_{\mathrm{m}} \mathcal{L}_{\mathrm{m}}+w_{\mathrm{r}}\mathcal{L}_{\mathrm{r}},
\label{equ:all_loss}
\end{equation}
\noindent where $w_{\mathrm{s}}$, $w_{\mathrm{d}}$, $w_{\mathrm{m}}$, $w_{\mathrm{r}}$ are the weights for each loss term. % TODO:write about their value in the experiment section in a parameter setting table

Our goal is to solve $\v{\xi}_{o w}^*,\v{z}^*=\argmin \mathcal{L}$, where  $\v{\xi}_{o w} \in \mathbb{R}^7$ is the corresponding $\mathfrak{s i m}(3)$ Lie Algebra of $\mq{T}_{o w}$. Instead of using first-order optimization such as gradient descent, we use  Levenberg-Marquardt with analytical Jacobians for faster and more stable convergence. The latent shape code $\v{z}$ is initialized as $\v{0}_{C}$ while $\v{\xi}_{o w}$ is initialized as an identity rotation, scaling of 1, and a translation from the bounding box center of the submap point cloud to the origin. For each iteration, with a damping parameter $\lambda$, the increment $\delta \v{x}$ to the estimated parameter vector $\left[\v{\xi}_{o w}, \v{z} \right]^{\tr}$ is given by:
\begin{equation} % check the function, change the LM part, not an I mat, actually the diag(diag(H)), H=J'J
  \delta \v{x} = \left[\delta \v{\xi}_{o w}, \delta \v{z} \right]^{\tr} = (\m{H} + \lambda \mathrm{diag}(\m{H}))^{-1} \v{g}
\label{equ:lm}
\end{equation}


The approximate Hessian matrix is given by $\m{H}=\m{J}^{\tr} \m{P} \m{J}$ and the gradient of the target function is $\v{g}=\m{J}^{\tr} \m{P} \v{b}$, where $\m{J}$, $\m{P}$, $\v{b}$ are the Jacobian matrix, weight matrix and residual vector, respectively.

Since both, the submap point cloud and the camera depth measurements are noisy, we apply a Huber robust kernel for the surface reconstruction and depth rendering residual resulting in the weight $w^{h}_{i}$ for each observation:
% {\equsize 
% \begin{equation}
%   w^{h}_{i}= \begin{cases}1 & \left|e_i\right|\leq\tau \\ (2\tau\left|e_i\right|-\tau^2)/e_i^2 & \text{otherwise} \end{cases},
% \end{equation}
% }
\begin{equation}
  w^{h}_{i}= \begin{cases}1 &, \text{if } \left|e_i\right|\leq\tau \\ \tau /\left|e_i\right| & \text{, otherwise} \end{cases},
\end{equation}
where $e_i$ and $\tau$ are the corresponding residual and kernel threshold, respectively. The weight matrix is formulated as $\m{P}=\mathrm{diag}\left(w_{s}w_{1}^{h}, \cdots, w_{d}w_{n}^{h}\right)$ with both the loss specific weight $w_{\mathrm{s}}$, $w_{\mathrm{d}}$ and the Huber weight $w^{h}$.
% Add jacobians here , or don't list them to save the space

The residual and Jacobian of the surface reconstruction term for each submap point is given by:
{\equsize 
\begin{eqnarray}
\v{b}_s=-D_\theta\left(\v{p}^{o}, \v{z}\right), \qquad 
\m{J}_s=\frac{\partial D_\theta\left(\v{p}^{o}, \v{z}\right)}{\partial\left[\v{\xi}_{o w}, \v{z}\right]^{\tr}}.
\end{eqnarray}
}
\vspace{-0.5em}

Applying the chain rule, we obtain the derivative of $\v{\xi}_{o w}$:
{\equsize
\begin{eqnarray}
\frac{\partial D_\theta\left(\v{p}^{o}, \v{z}\right)}{\partial \v{\xi}_{o w}}=\frac{\partial D_\theta\left(\v{p}^{o}, \v{z}\right)}{\partial \v{p}^{o}} \frac{\partial \v{p}^{o}}{\partial \v{\xi}_{o w}},\\
\frac{\partial \v{p}^{o}}{\partial \v{\xi}_{o w}} = \frac{\partial (\mq{T}_{o w}\q{p}^{w})}{\partial \v{\xi}_{o w}} = \left[\begin{array}{lll}
    \m{I}_{3\times3} & -\v{p}_{\times}^{o} & \v{p}^{o}
    \end{array}\right],
\end{eqnarray}
}
\vspace{-0.5em}

\noindent where $_{\times}$ refers to the skew symmetric matrix of a vector. Note that both, $\frac{\partial D_\theta\left(\v{p}^{o}, \v{z}\right)}{\partial\v{p}^{o}}$ and $\frac{\partial D_\theta\left(\v{p}^{o}, \v{z}\right)}{\partial\v{z}}$ can be obtained through automatic differentiation of the DeepSDF model $D_\theta$.
The residuals of the rendering term for each sampled pixel are simply:
\vspace{-0.5em}
\begin{equation}
\v{b}_{d} = \hat{D} - D, \qquad \v{b}_{m} = \hat{M} - M,
\end{equation}
\noindent and the Jacobians can also be obtained using the chain rule:
{\equsize
\begin{flalign}
  \m{J}_d & =\sum_{i=1}^N \frac{\partial \hat{D}}{\partial o_i} \frac{\partial o_i}{\partial v_i} \frac{\partial D_\theta\left(\v{p}^{o}_{i}, \v{z}\right)}{\partial\left[\v{\xi}_{o w}, \v{z}\right]^{\tr}},\\
  \m{J}_m & =\sum_{i=1}^N \frac{\partial \hat{M}}{\partial o_i} \frac{\partial o_i}{\partial v_i} \frac{\partial D_\theta\left(\v{p}^{o}_{i}, \v{z}\right)}{\partial\left[\v{\xi}_{o w}, \v{z}\right]^{\tr}},
\end{flalign}
}

\noindent in which $\frac{\partial D_\theta\left(\v{p}^{o}_{i}, \v{z}\right)}{\partial\left[\v{\xi}_{o w}, \v{z}\right]^{\tr}}$ is in the same form as $\m{J}_{s}$ and the other three derivative terms are derived as:
{\equsize
\begin{flalign}
\frac{\partial \hat{D}}{\partial o_i} &= \frac{\Delta d}{1-o_i}\sum_{j=i}^N \prod_{k=1}^j (1-o_k), \\
\frac{\partial \hat{M}}{\partial o_i} &= \prod_{j=1, j \neq i}^N\left(1-o_j\right), \\
\frac{\partial o_i}{\partial v_i} &= \frac{o_i(1-o_i)}{\sigma}.
\end{flalign}
\vspace{-0.5em}
}

Lastly, the residual and Jacobian of the shape code regularization term are:
\begin{align}
  \v{b}_r &=-\v{z}\\
  \m{J}_r &= \left[\begin{array}{ll}
    \frac{\partial \v{z}}{\partial \boldsymbol{\xi}_{o w}} & \frac{\partial \v{z}}{\partial \v{z}} 
    \end{array}\right]=\left[\begin{array}{ll} \v{0}_{1\times7} & \v{1}_{1\times C} \end{array}\right]
\end{align}

With all Jacobians and residuals available, we are able to solve \eqref{equ:lm} and update:
%\equsize
\begin{equation}
\left[\v{\xi}_{o w}^{(t+1)}, \v{z}^{(t+1)} \right]^{\tr} = \left[\v{\xi}_{o w}^{(t)}, \v{z}^{(t)} \right]^{\tr} + \delta \v{x}^{(t)},
\end{equation}
until convergence. 
% reaching a certain maximum iteration $N_c$ or the maximum element in gradient $\left|\v{g}\right|$ is smaller than a threshold $\epsilon_{c}$. 
After convergence, the complete fruit model can be reconstructed using marching cubes with the optimized $\v{z}^{*}$ at 3D grid queries in the fruit's canonical coordinate system. The reconstruction can then be transformed into the world coordinate system using $\mq{T}_{o w}^{*} = \exp (\v{\xi}_{o w}^{*})$.  

\begin{figure}
  \centering  
  \includegraphics[width=0.93\linewidth]{pics/cka_dataset_v2.pdf}
  % \setlength{\abovecaptionskip}{2pt}
  \caption{Example from our dataset. Left: Offline 3D reconstruction using bundle adjustment. Middle: shape completion ground truth models with pose alignment in the bundle adjustment result. Right: Ground truth sweet pepper obatined by harvesting the fruits and scanning then in the lab with a high-precision laser scanner.}
  \label{fig:dataset}
  % \vspace{-14pt}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Evaluation}
\label{sec:exp}

\begin{figure*}[ht]
  \centering  
  \includegraphics[width=0.9\linewidth]{pics/row_quali_v3.pdf}
  % \setlength{\abovecaptionskip}{2pt}
  % \vspace{0mm}
  \caption{Qualitative results of the panoptic mapping with fruit shape completion and pose estimation in a sweet pepper greenhouse. Our method successfully achieves convincing reconstructions and poses, even with partial observations and despite heavy occlusions. We also show a failure case in the orange box on the right, where noisy partial input leads to wrong pose of the sweet pepper on the top.}
  \label{fig:row_quali}
  % \vspace{-6pt}
\end{figure*}

\begin{table*}
    \centering
     \caption{Reconstruction results in the commercial glasshouse.  The $\downarrow$ and $\uparrow$ indicate that lower or higher values mean better performance.}
    \label{tab:results_wild_pepper}
     % \resizebox{0.\linewidth}{!}{
    \begin{tabular}{cccccccc} 
      \toprule
      \multirow{2}{*}{\textbf{Approach}} & \textbf{$\boldsymbol{D}_\text{C}$} [mm] & \textbf{f-score} [\%] & \textbf{precision} [\%] & \textbf{recall} [\%] & \textbf{$\boldsymbol{E}_\text{rot}$} [$\deg$] & \textbf{$\boldsymbol{E}_\text{tran}$} [mm] & \textbf{inference time} [s] \\
       & $\downarrow$ avg & $\uparrow$ avg & $\uparrow$ avg & $\uparrow$ avg  &  $\downarrow$ avg &  $\downarrow$ avg &  $\downarrow$ avg\\
      \midrule
      CPD~\cite{myronenko2010pami}           & 25.38 & 3.09 & 8.10 & 1.92 &  26.79 & 27.74 & 0.57 \\    
      PF-SGD~\cite{marks2022icra}            & 9.28 & 35.03 & 37.32 & 33.21 & 29.61 & 19.73  & 30.21  \\  
      % SE-fit~\cite{marangoz2022case}         & xxx & xxx & xxx & xxx & xxx & xxx & xxx \\  
      \midrule
      DeepSDF~\cite{park2019cvpr}            & 9.33 & 35.24 & 32.38 & 38.77 & \xmark & \xmark & 16.01 \\  
      CoRe~\cite{magistri2022ral-iros}       & 6.90 & 41.47 & 43.17 & 41.64 & \xmark & \xmark & \b{0.004} \\                    
      \midrule
      % \midrule
      Ours           &   \b{5.29}  & \b{58.56} & \b{61.28} & \b{56.26} & \b{11.48} & \b{11.20} & 0.62 \\  
      \bottomrule
    
    \end{tabular}
    % }
    % \vspace{-1em}
   \end{table*}

%% Repeat the main focus/objective with one single(!) sentence starting with:
%
The main focus of this work is a pipeline for multi-resolution mapping in orchard environments including fruits shape completion and pose estimation.
%% Explain the reader that the experiments with support all claims
%% (same list as in the intro!) starting the paragraph with:}
%
We present our experiments to show the capabilities of our method. The results of our experiments also support the claims that our approach is able to jointly
%
(i) predict the 3D shape of fruits even under substantial occlusions in real commercial glasshouse environments and
%
(ii) estimate the pose of each fruit in the 3D map.
%
(iii) Additionally, our multi-resolution map representation yield substantial improvement in the shape completion task over fixed-resolution maps.

\subsection{Experimental Setup}

%% If needed (and only then!) say also a few words about the experimental
%% setup, the datasets, and used parameters. You can use a separate subsection if you
%% want to put the focus on that but often that is not needed.}
% The experiment plaform we used is the PathoBot (phenotyping autonomous Trolley for Horticultural), an automated robotic platform capable of operating in protected cropping environments. The robot obtain observation of the plants in the glasshouse using an array of the \textit{Intel RealSense D435i cameras}, which record RGB, NIR and depth information with a framerate at \SI{15}{\hertz}.
To showcase the capability of our proposed pipeline, we consider two typical and challenging crops, sweet pepper and strawberry, both of which have irregular shapes.

\textbf{Dataset:} For training and testing our shape completion network in a controlled environment we use the strawberries and sweet peppers dataset also used in our previous work~\cite{magistri2022ral-iros}. We, additionally, collected a sweet pepper dataset with the robot shown in \figref{fig:mot} in a commercial greenhouse near Bonn, Germany, that we use for testing and evaluating our proposed solution in real conditions. This dataset contains \rgbd frames collected with the robot using an Intel RealSense d435i camera with a framerate of \SI{15}{\hertz}. We then harvested the sweet peppers present in the greenhouse and scanned them with a high-precision LiDAR system as in Schunk~\etalcite{schunck2021plosone}. In this way, after registering the data obtain with the two different sensors, we obtain the ground truth shape of each sweet pepper and also ground truth poses with respect to the fruit canonical pose, \ie, with the peduncle point upwards. See \figref{fig:dataset} for a visual impression.

\textbf{Metrics:} 
To evaluate our shape completion solution, we use the Chamfer distance $D_\text{C}$, \ie, the average symmetric squared distance of each point to its nearest neighbor in the other point cloud.
% % $D_\text{C}(\mathcal{G},\mathcal{R}) = \frac{\bar{d^2}(\mathcal{G},\mathcal{R})}{2}  + \frac{\bar{d^2}(\mathcal{R},\mathcal{G})}{2}$
% \vspace{-1em}
% {\equsize
% \begin{eqnarray}
%     D_\text{C}(\mathcal{G},\mathcal{R})&= \frac{\bar{d^2}(\mathcal{G},\mathcal{R})}{2}  + \frac{\bar{d^2}(\mathcal{R},\mathcal{G})}{2},
%     \label{eq:chamfer}
% \end{eqnarray}
% % \vspace{-0.5em}
% }
% \noindent with $\bar{d^2}(\mathcal{P}_i,\mathcal{P}_j) = \frac{1}{|\mathcal{P}_i|} \sum_{\v{x}_i \in \mathcal{P}_i} \min_{\v{x}_j \in \mathcal{P}_j} \norm{\v{x}_i-\v{x}_j}_2^2$, where $\mathcal{G}$ and $\mathcal{R}$ are respectively the ground truth point cloud and the point cloud obtained by sampling the reconstructed mesh. 
We, additionally, use the f-score, precision, and recall at a fixed threshold ($\rho$\,=\,5\,mm in our experiments) as proposed by Knapitsch \etalcite{Knapitsch2017tog}.
 %Given the estimated fruit pose as $\mq{T}_{w o}$ and the ground truth fruit pose as $\widetilde{\mq{T}}_{w o}$. 
To evaluate the pose estimated by our approach we report separate metrics for translations and rotations.
We report the average translation error $E_\text{tran}$, \ie, the Euclidean distance between the predicted and the ground truth center, for of each fruit. We define the rotation error $E_\text{rot}$ as the the intersection angle between the \textit{z}-axis of predicted and ground truth pose. This metric ignores rotations around the fruits main axis as fruits are almost symmetric around it and focuses instead on the difference between the predicted and the ground truth \textit{z}-axis. Additionally, we report the average inference time needed to obtain the complete 3D shape. In our experiments, we used an NVIDIA Quadro RTX A5000 GPU.

\textbf{Parameter settings:} 
We tune the hyperparameters of our system for better performance. We set the weight for each loss term in \eqref{equ:all_loss} as $w_s=1.0$, $w_d=0.05$, $w_m=0.0002$, $w_r=0.0005$ to ensure that the Hessian matrices of every loss term have the same order of magnitude. We set the damping factor $\lambda=0.1$ in \eqref{equ:lm}. To balance the efficiency and the performance, we sample $\left|\mathcal{P}_\mathcal{S}\right|=2,000$ points from each fruit submap. For each fruit instance on the image, we sample $G_f=300$ and $G_b=300$ pixels from the foreground and the background, respectively. For each ray, we sample $N=30$ points on it. We set the occlusion and surface noise thresholds to $d_o=3$\,cm and $\sigma=1$\,mm.

% % TODO Not used
% \begin{tabularx}{\columnwidth}{c||c}
%   \toprule
%   \textbf{Parameter}         & \textbf{Value}         \\ \midrule
%   weight for surface loss $w_s$       & 1.0                  \\
%   weight for depth rendering loss $w_d$                     & 0.05                 \\
%   weight for mask rendering loss $w_m$                     & 0.0002   \\
%   weight for shape code regularization   $w_r$                     & 0.0005 \\
%   \bottomrule
% \end{tabularx}
% \label{tab:params}
% \vspace{-0.5cm}

\begin{table*}
  \centering
   \caption{Reconstruction results in \emph{controlled environment}. The $\downarrow$ and $\uparrow$ indicate that lower or higher values mean better performance. }
  \label{tab:results_lab}
  % May redo the experiment on DeepSDF
   \resizebox{\linewidth}{!}{
  \begin{tabular}{cccccc|ccccc|cc} 
    \toprule
    \multirow{3}{*}{\textbf{Approach}} & \multicolumn{5}{c|}{\normalsize{\textbf{Sweet Pepper}}}  & \multicolumn{5}{c|}{{\normalsize\textbf{Strawberry}}} & \multirow{3}{*}{\textbf{learning}} & \multirow{3}{*}{\textbf{pose}}\\
    & \textbf{$\boldsymbol{D}_\text{C}$} [mm] & \textbf{f-score} [\%] & \textbf{precision} [\%] & \textbf{recall} [\%] & \textbf{time} [s] & \textbf{$\boldsymbol{D}_\text{C}$} [mm] & \textbf{f-score} [\%] & \textbf{precision} [\%] & \textbf{recall} [\%] & \textbf{time} [s] \\
    & $\downarrow$ avg & $\uparrow$ avg & $\uparrow$ avg & $\uparrow$ avg &  $\downarrow$ avg & $\downarrow$ avg & $\uparrow$ avg & $\uparrow$ avg & $\uparrow$ avg &  $\downarrow$ avg \\
    \midrule
    CPD~\cite{myronenko2010pami}           & 12.36  & 39.84 & 76.68 & 27.07 & 15.62 & 5.13 & 57.93 & 94.09 & 42.34  & 0.57 & \xmark & \cmark \\    
    PF-SGD~\cite{marks2022icra}            & 3.97 & 68.95  & 71.20 & 66.94  & 17.48 & 2.71  & 86.08  & 88.82  & 83.90  & 8.10 & \xmark & \cmark \\  
    % SE-fit~\cite{marangoz2022case}         & 4.57  & 65.53  & 62.21  & 69.82  & 0.13 & 3.56  & 79.97  & 74.67  & 91.43  & 0.03 & \xmark & \cmark \\  
    \midrule
    DeepSDF~\cite{park2019cvpr}            & 29.78  & 37.12  & 32.96  & 46.06 & 44.13 & 3.61  & 74.01  & 83.76  & 68.32 & 36.84 & \cmark & \xmark \\  
    CoRe~\cite{magistri2022ral-iros}       &  7.83 & 52.85  & 47.38  & 60.00  & \b{0.004} & 2.67  & 86.01 & 87.97  & 84.85  & \b{0.004} & \cmark & \xmark \\    
    \midrule
    % \midrule
    % Ours (single)                         & 3.98  & \underline{71.16} & 72.41  & \underline{70.10}  & \underline{0.11} & 2.77  & \underline{87.50} & 92.00  & 84.14 & 0.10 & \cmark & \cmark \\  
    Ours                            & \b{3.16} & \b{80.86}  & \b{82.14}  & \b{79.72}  & 0.60  & \b{2.42} & \b{92.81} & \b{94.38} & \b{91.53} & 0.53 & \cmark & \cmark \\  
    \bottomrule
    % I used the 3D_metrics incorrectly (reinitialize the class every time) so that the previous result is actually only from the last sample. Now I update the new results. 
  
  \end{tabular}
  }
 \end{table*} 

\begin{table*}
    \centering
     \caption{Results of different maps in the glasshouse. The $\downarrow$ and $\uparrow$ indicate that lower or higher values mean better performance.}
    \label{tab:results_wild_pepper_maps}
     \resizebox{\linewidth}{!}{
    \begin{tabular}{cccccccc|c} 
      \toprule
      \multirow{2}{*}{\textbf{Approach}} & \textbf{$\boldsymbol{D}_\text{C}$} [mm] & \textbf{f-score} [\%] & \textbf{precision} [\%] & \textbf{recall} [\%] & \textbf{$\boldsymbol{E}_\text{rot}$} [$\deg$] & \textbf{$\boldsymbol{E}_\text{tran}$} [mm] & \textbf{inference time} [s] & \multirow{2}{*}{\textbf{Online Map}}\\
       & $\downarrow$ avg & $\uparrow$ avg & $\uparrow$ avg & $\uparrow$ avg  &  $\downarrow$ avg &  $\downarrow$ avg &  $\downarrow$ avg\\
      \midrule
      Bundle Adjustment           & \b{4.70} & \b{67.71} & \b{69.07} & \b{66.52} & \b{10.23} & \b{8.71} & 0.66 & \xmark \\     
      \midrule
      RGB-D Single Frame                     & 9.62 & 40.56 & 41.98 & 39.52 & 25.91 & 18.21 & \b{0.16} & \cmark\\  
      Fixed-Resolution Map                   & 7.29 & 44.22 & 46.29 & 42.62 & 17.48 & 16.02 & 0.75 & \cmark\\  
      Multi-Resolution Map (Ours)           &  5.29  & 58.56 & 61.28 & 56.26 & 11.48 & 11.20 & 0.62 & \cmark\\  
      \bottomrule
    
    \end{tabular}
    }
    \vspace{-1em}
   \end{table*}


%% Note 1: It MUST be always crystal clear (a) WHY an experiment is there
%% (e.g., to support a claim, to show that the approaches useful for real-word
%% systems, to show the performance, or to provide a baseline comparison), (b)
%% WHAT it wants to show (which claim/property exactly), and (c) HOW it aims at 
%% showing this. This is ESSENTIAL for a good evaluation. Think about when BEFORE
%% designing an experiment.  IMPORTANT: Every experiment MUST start with something 
%% like:  The next experiment is presented to show \dots and thus for supporting our 
%% first claim.


%% Note 2: Start with the most important/impressive experiment first. Make
%% his a key story of the paper. Keep the order of the claims, i.e., re-order
%% claims in the intro/before if needed. 

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Shape Completion and Pose Estimation}
%% First experiment - most impressive, important or the most important
%% claim supporting experiments comes first.
The first experiment evaluates the performance of our approach and supports the claims that our approach can predict the 3D shape of fruits
under substantial occlusions and can estimate the pose of each fruit in the 3D map. The qualitative results are shown in~\figref{fig:row_quali}.
We report in~\tabref{tab:results_wild_pepper} the metrics regarding shape completion and fruit pose estimation in real greenhouses. Our approach yields better performances in all metrics except for the inference time where we can still estimate poses and shapes of two fruit per second. The most competitive baselines for shape completion~\cite{magistri2022ral-iros, park2019cvpr} provide Chamfer distance performances of 1.6\,mm  and 4.0\,mm worse than our approach and can not estimate fruit poses, thus limiting their applicability in this scenario. On the other hand, the best baseline that can estimate both shape and pose provide a Chamfer distance of 9.28\,mm against our 5.29\,mm, a rotation error of 29.61$^{\circ}$ against our 19.73$^{\circ}$ and a translation error of 19.73\,mm against 11.20\,mm.

For a deeper comparison with the baselines and for testing our approach on a different fruit species, we evaluate the shape completion accuracy in a controlled environment where we estimate shapes of sweet peppers and strawberries, see~\tabref{tab:results_lab}. Our approach yields better reconstruction performances on both species with an f-score of 80.86\% on the sweet pepper and of 92.81\% on the strawberry, while the closest baseline~\cite{marks2022icra} reaches 68.95\% and 86.08\% respectively. In terms of execution time, our approach can estimate the 3D shape of roughly two fruit per second, note that the baseline~\cite{magistri2022ral-iros} with lower inference time produce worst 3D shapes.

% \begin{figure}
%   \centering  
%   \includegraphics[width=0.95\linewidth]{pics/place_holder_qualitative.pdf}
%   \setlength{\abovecaptionskip}{2pt}
%   \caption{TBA. Placeholder for some qualitative results in-the-wild}
%   \label{fig:qualitative}
%   \vspace{-10pt}
% \end{figure}

   % Update second best performance result

% Reconstruction experiments in the wild based on the panoptic map
% Ablation studies on different kind of loss, used frame number and DeepSDF latent code size.

\subsection{Influence of Map Representation on Completion Results}
The second experiment evaluates the effects of the map representation on estimating shapes and poses of fruits. This experiment illustrates that our multi-resolution map representation yield substantial improvement in the shape completion task over fixed-resolution maps. In~\tabref{tab:results_wild_pepper_maps}, we compare the shape completion results obtained with using our multi-resolution mapping strategy against the results of fixed-resolution maps and single \rgbd frames. Note that our multi-resolution mapping system can run in real-time. We additionally report the shape completion results obtained with a map obtained offline using bundle-adjustment software.  For the Chamfer distance, our map yields a 28\% improvement over fixed-resolution map and a 45\% improvement over single \rgbd frames. Considering the estimated poses, using our map representation yields an improvement of 6$^{\circ}$ and 5\,mm  with respect to fixed-resolution map and an improvement of 14$^{\circ}$ and 7\,mm for over single \rgbd frames. Notably, by using our online mapping strategy, we obtain competitive results with respect to using an offline photogrammetric reconstruction method, \ie, bundle adjustment. Our completion results are only 0.59\,mm less accurate than the one obtained by using the offline method. Similarly, our predicted pose is around 1$^{\circ}$ and 3\,mm less accurate.

\subsection{Ablation Study}
We ran several ablation studies to evaluate the impact of each loss term in~\eqref{equ:all_loss}. From the metrics in~\tabref{tab:ablation_loss}, it is clear that our loss design with all four terms yields the best performances in both shape completion and pose estimation. As expected, the regularization term $\mathcal{L}_\mathrm{r}$ has the lowest impact in the final results. Instead, the biggest impact on the performances is given by the mask term $\mathcal{L}_\mathrm{m}$ defined in~\eqref{equ:mask}. Without this term, the f-score drops to 35.89\% instead of 58.56\%, the rotation error increases from 11.48$^{\circ}$ to 19.16$^{\circ}$ and the translation error from 11.20\,mm to 23.34\,mm.


\begin{table}
  \caption{Ablation studies on the loss terms} 
  \label{tab:ablation_loss}
  \centering
  \resizebox{0.95\linewidth}{!}{
  \begin{tabular}{ccccc} 
    \toprule
    \multirow{2}{*}{\textbf{Loss}} & \textbf{$\boldsymbol{D}_\text{C}$} [mm] & \textbf{f-score} [\%] & \textbf{$\boldsymbol{E}_\text{rot}$} [$\deg$] & \textbf{$\boldsymbol{E}_\text{tran}$} [mm]  \\
     & $\downarrow$ avg  & $\uparrow$ avg & $\downarrow$ avg  & $\downarrow$ avg  \\
    \midrule
    % $\mathcal{L}_{\text{reg}}$                                                                  & 18.79 (2.09) & 11.63 (2.55) \\
    % $\mathcal{L}_{\text{c}}$                                                                    & 11.27 (5.49) & 30.43 (9.14) \\
    No $\mathcal{L}_\mathrm{s}$                           & 8.17 & 38.71 & 15.80 & 17.99 \\
    No $\mathcal{L}_\mathrm{d}$                         & 8.93 & 36.12 & 17.11 & 19.78  \\
    No $\mathcal{L}_\mathrm{m}$                           & 10.09 & 35.89 & 19.16 & 23.34 \\
    No $\mathcal{L}_\mathrm{r}$                          & 7.64 & 40.24 & 15.42 & 12.34 \\
    All Loss (Ours)                                      & \b{5.29} & \b{58.56} & \b{11.48} & \b{11.20} \\
    \bottomrule
  \end{tabular}}
  \vspace{-1em}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\label{sec:conclusion}

We presented a novel approach for panoptic mapping in real agricultural glasshouses using a mobile robot equipped with \rgbd cameras. We build a multi-resolution map to represent fruits with higher resolution and jointly estimate complete 3D shapes of fruits and their pose in the 3D map. Our method exploits high-precision 3D scanning to learn a general fruit shape prior that we use at inference time together with an occlusion-aware differentiable rendering pipeline. This allows us to successfully complete partial fruit observations and estimate the 7 DoF pose of each fruit in the map. We solve the joint shape and pose optimization efficiently with a second-order optimization with analytical Jacobians, allowing for its application online. We evaluated our approach on different datasets and provided comparisons to other existing techniques and supported all our claims made. The experiments suggest that the proposed approach yields higher shape completion and pose estimation accuracy than existing baselines. It provides precise and complete fruits models, allowing farmers to assess the status of the orchard. Furthermore, our map providing fruits shape and pose explicitly can serve as a basis for planning autonomous fruit harvesting missions.

%\vspace{-1em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Future work: Use only if applicable -- but if so, use the following
%% sentence to start:
% Despite these encouraging results, there is further space for improvements. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only if applicable
%\section*{Acknowledgments}
%We thank XXX for fruitful discussions and for \dots


\bibliographystyle{plain_abbrv}


% All new citations should go to new.bib. The file glorified.bib should go
% be the one from the ipb server. After paper or related work has been
% written merge the entries from new.bib to glorified.bib ON THE SERVER,
% replace the glorified.bib in this repository and empty the new.bib
\bibliography{glorified,new}

\end{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% NOTES ON PAPER WRITING
%
% Rename the paper.tex file into your paper name. Use the BibTeX key policy (see below)
%
% I recommend using grammarly (www.grammarly.com) as a first check 
%
% Use a Spell Checker with US English as spelling language
%
% Use Academic Writing Check: https://github.com/devd/Academic-Writing-Check
%
% Use GIT for version control. Use our gitlab sever!
%
% Make sure your Makefile is working correctly and compiles the documents
%
% All images go to the subfolder pics and reviews into the reviews folder
%
% Make sure the source files for images are in the pics folder as well (unless they are huge)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% NOTES ON BIB ENTRIES
%
% Bibtex Key Policy
%
%    All in lower case
%    Use the key structure: <lastnamefirstauthor><4 digit year><conference/journal><extra>
%    Use <extra>:
%        only to disambiguate: - first chars of the first 4 words f the title
%    Examples: stachniss2008icra, stachniss2008icraws, stachniss2008icra-adhc
%    Use the BibTeX key also at the filename for the paper, e.g., stachniss2008icra.pdf
%
% Bibtex Entries
%
%    Use strings for conferences and journal name in order to keep obtain consistent entries
%    Use the identical abbreviations for conference name, e.g., âProc. of the IEEE Int. Conf. on Robotics and Automation (ICRA)â
%    Avoid adding the location in addition to the city or street of the conference.
%    Use doi for the official document on the publisher webpage.
%    Abbreviate the first name of the authors, e.g., C. Stachniss instead of Cyrill Stachniss
%    In case of a first name and a middle name, use no space between them, e.g., C.J. Stachniss
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


