\section{Multi-domain Data Augmentation}
\label{sec:method}

\subsection{Data Augmentation using McAdams coefficient}
\label{subsec:mcadams}

We use data augmentation to increase the amount of accented training data. Our approach consists of the alteration of the McAdams coefficient~\cite{mcadams1984spectral}, and is described in \cite{patino2020speaker} in the context of speaker anonymization. It is based upon simple signal processing techniques and is particularly relevant because it is simple to implement and does not require any additional training data. By applying the McAdams transformation to a speech signal with different McAdams coefficients $\alpha$, we generate new signals with the same transcription but uttered by a pseudo voice with a different timbre.

We use values of $\alpha$ ranging from 0.7 to 1 with a step of 0.1 to generate the augmented dataset; note that using $\alpha=1$ does not change the sample. We do not use values of $\alpha \leq 0.6$ because it deteriorates too much the intelligibility. Applying this augmentation to AAF results in a dataset that is $4\times$ bigger than the original set, leading to a total of $\simeq 32$ hours of speech, matching the audio quantity of CV. We denote this augmented dataset \textbf{AAFaug}.

\subsection{Multi-domain mix of datasets}
\label{subsec:multidom}

We want to study the impact of multi-accent fine-tuning on the recognition of individual accents. To do so we design several experiments which consist of varying the number of domains and their associated quantity of speech data.

In our first experiment, we create train sets using various amounts of CV and AAF. Starting with a set containing only AAF, we keep increasing its size by including increasingly larger subsets of CV. In a reverse manner, we also create train sets by starting with CV and progressively adding AAF. We denote these sets \textbf{CV - \bm{$x$}\%}, where $x$ represents the proportion of CV data in the set. In addition, we create a \textit{FullCV} train set consisting of the whole CV dataset (train, dev, test) together with the AAF train split. See Table~\ref{tab:training_sets} for details on the individual training sets.

In a second experiment, we select the best train sets from experiment 1 (i.e. the sets which led to the lowest WER) and combine them with either CaFE (whole) or CFPB (train split). This leads to slightly larger train sets which have three accents instead of two.

In our third and last experiment, we repeat experiment 1 using a fixed number of hours in the training set. To do so, we make use of the augmented dataset AAFaug~\ref{subsec:mcadams}. Similarly to experiment 1, we define 11 train sets, starting with a 31h subset of AAFaug, and gradually replacing more and more AAFaug data by CV data with a 10\% proportion increment, ending with the full CV train set. Finally, we create two larger training sets using the augmented data, namely CV $\cup$ AAFaug and FullCV $\cup$ AAFaug.