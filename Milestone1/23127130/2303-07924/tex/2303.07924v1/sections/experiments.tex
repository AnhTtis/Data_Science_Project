\section{Experimental context}
\label{sec:experiments}

\input{tables/datasets.tex}

\input{tables/training_sets_xp1.tex}

\subsection{Datasets}
\label{subsec:datasets}

We use four different datasets of French speech representing different accents. All the datasets are supplied with transcripts. Their main statistics are reported in Table~\ref{tab:datasets}.

\noindent \textbf{CV} (Common Voice~\cite{commonvoice2020}) is a large crowd-sourced multilingual corpus of read speech. We use the French subset of the CommonVoice~3 database. We use the official splits of the dataset - train: 31h, dev: 12h, test: 13h. This is our reference corpus for (accent-free) French speech.

\noindent \textbf{AAF} (African Accented French~\cite{african_accented_french}) is a corpus of read speech. Speakers originate from five African countries (Cameroon, Chad, Congo, Gabon, and Niger) where French is (one of) the official language(s), however, their accent is clearly audible. We split this dataset as such - train: 8h, dev: 3h, test: 3h. This is our target corpus, that is, we want to obtain the best performance on this dataset.

\noindent \textbf{CaFE} (Canadian French Emotional~\cite{cafe}) is a small corpus of acted emotional speech. Speakers have a distinguishing \textit{Québécois} accent. Due to the low amount of audio of this dataset, we do not split it and use it solely for testing.

\noindent \textbf{CFPB} (\textit{Corpus de Français Parlé à Bruxelles} (Corpus of French as Spoken in Brussels)~\cite{cfpb}) is a small corpus of interviews with Brussels speakers with a Belgian accent. We split this dataset as such - train: 3h, test: 1h.

\subsection{Model}
\label{subsec:model}

We use the following wav2vec~2.0 models from the \textit{LeBenchmark}~\cite{evain2021task} initiative: LB-7K-base and LB-7K-large, which were pre-trained on 7,739 hours of French audio. The \textit{base} variant refers to the standard model architecture from~\cite{baevski2020wav2vec} that has 95 million parameters, while the \textit{large} refers to their larger architecture that presents greater capacity (317 million parameters). We use the \textit{LB-7K} variants of the models since previous work~\cite{maison2022promises} has shown that for this task, models pre-trained using the greater quantity of audio performed best.

Each pre-trained wav2vec~2.0 model acts as a speech encoder, which is fine-tuned for the ASR task together with an additional feed-forward network. This head network consists of three linear layers with 768 or 1,024 neurons for a \textit{base} or \textit{large} model, respectively. Each linear layer is followed by batch normalization and a Leaky ReLU~\cite{Maas2013RectifierNI} activation function. We use dropout with $p=0.15$ between each linear layer.
\noindent At last, a final linear layer projects the output into token space, and log-softmax is applied to obtain probabilities of each token.

\subsection{Training}
\label{subsec:training}

We use the \texttt{SpeechBrain}~\cite{speechbrain} toolkit for all our experiments. All our models are fine-tuned during 50 epochs using the CTC loss. Adam~\cite{adam} and Adadelta~\cite{Zeiler2012ADADELTAAA} optimizers with learning rates $10^{-4}$ and $1.0$ are used to update the weights of the wav2vec 2.0~model and the additional top layers respectively. Learning rates are reduced at each epoch in which the validation loss does not improve.
During training, we apply on-the-fly data augmentation using the \texttt{SpeechBrain} time-domain approximation of the SpecAugment~\cite{Park2019specaugment} algorithm: it disrupts audio speed, and randomly drops chunks of audio and frequency bands.

For fine-tuning we use several different training sets, which are formed using varying amounts of audio data from one or more \textit{speech domains}~(accents). We detail the formation of these training sets in section~\ref{sec:method}. We also use a validation set (dev set) for early stopping; this set is composed of 5 hours of audio, evenly distributed between CV and AAF. We believe that using such a validation set favors the selection of a model with good performance in both domains. Note that we take care of separating by speaker when creating splits of the data; this way validation and testing are always done on unknown speakers.

\subsection{Evaluation}
\label{subsec:evaluation}

We evaluate our trained models on four test sets, which stay identical for all the experiments: CV, AAF, CFPB (test splits), and CaFE (whole set). We use the Word Error Rate~(WER) as our test metric; lower is better. Note that we do not use any language model to avoid introducing a bias during evaluation.