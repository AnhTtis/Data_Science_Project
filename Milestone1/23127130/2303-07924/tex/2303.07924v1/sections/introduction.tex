\section{Introduction and Related Work}
\label{sec:intro}

In recent years, self-supervised learning~(SSL) has proven useful in a variety of domains, including computer vision, NLP, and speech processing. For the latter, SSL is used to leverage huge amounts of unannotated speech to build powerful speech representations that can be used to tackle downstream tasks like automatic speech recognition~(ASR). Systems pre-trained with an SSL approach and then fine-tuned on a target domain reach state-of-the-art performance. However, there is growing evidence that these systems are not robust to domain shifts~\cite{Hsu2021, gomez2023ATC}. For example, a model fine-tuned on speech uttered with accent A is likely to have reduced performance when evaluated on accent B, although the language is the same in both cases. It is known that adding out-of-domain data to the training dataset improves robustness to domain shifts~\cite{likhomanenko21_interspeech}; however, it is often the case that out-of-domain data is scarce, making it hard to gather amounts sufficient for training. Regarding the peculiar case of accented speech, the vast majority of corpora consist of speech spoken with the dominant accent.

In order to tackle the lack of accented data, we can use data augmentation to artificially increase the amount of training data. There exist several ways to do this, for instance, it is possible to add noise to the training data, modify voice speed, or transform voice by manipulating the vocal-source and vocal-tract characteristics~\cite{Fukuda2018}. Other approaches include applying speaker normalization or anonymization methods in a reverse manner, for example using Vocal Tract Length Perturbation~\cite{Jaitly2013} or voice conversion using X-vectors~\cite{fang2019speaker}.

There exist several methods for reducing the effects of domain shifting. It has been shown that adding new domains to the pre-training dataset improves the model's robustness~\cite{likhomanenko21_interspeech}. In order to avoid expensive retraining of large SSL models, Fan et al. introduce DRAFT~\cite{Fan2022}, a method for learning domain-specific knowledge using residual adapters. Viglino et al. use multi-task learning and accents embeddings~\cite{viglino19_interspeech} to tackle accented speech recognition. In this work, we learn domain-specific knowledge during the fine-tuning stage by carefully designing multi-domain training sets.