\section{Results and Discussion}
\label{sec:results}

\begin{figure}[h]
    \begin{center}
    \includegraphics[width=\columnwidth]{figures/wer_cv_aaf_tradeoff.pdf}
    \caption{A representation of the performance trade-off between CV and AAF. Each point represents a model fine-tuned on a different train set. All models are \textit{Large} unless specified otherwise. Details on the experiments can be found in section~\ref{subsec:multidom}.}
    \label{fig:wer_cv_aaf_tradeoff}    
    \end{center}
\end{figure}

In this section, we summarize and analyze the results of our experiments. Numerical results are reported in Table~\ref{tab:results}; for the sake of brevity, we only list the best training sets. All models use the \textit{Large} architecture unless specified otherwise.

\subsection{Experiment 1: Mixing two domains}

We study the impact of mixing two domains during training on performance. We observe U-shaped curves of WER with respect to CV proportion in the training set, meaning that WER first decreases when adding CV to the mix (from 0 to 80\%), then slightly increases again when removing AAF (from 80 to 100\%). Going from one to two domains causes both in-domain and out-of-domain improvements. 
When adding a new domain B to a train set containing only domain A, not only do we improve on domain B as expected, but we also improve on domain A and on unseen domains C and D as well. We obtain our best results on CV~(9.4 WER) and on CaFE~(36.8 WER) using CV 90\% and CV 50\% respectively, while also reaching competitive performance on the other test sets compared to the baselines. Remarkably, training on a CV/AAF mixed set allows us to reach WER of 34.6 and 36.8 on CFPB and CaFE respectively, which is a lot better than the scores reached when training on CV or AAF alone, see table~\ref{tab:results}.

\noindent We observe similar trends on base models although with significantly higher WER (+40\% on average relative to large models). However, we see lesser improvements from the domain augmentation compared to large models. We believe the base model is less able to take advantage of multi-domain due to its reduced capacity.

It should be noted that the \emph{LB-7K} models were pre-trained using several different corpora, including AAF and CaFE~(see~\cite{evain2021task}). However, this does not prevent the catastrophic forgetting phenomenon which happens during fine-tuning. For instance, the LB-7K-large model fine-tuned on CV scores 18.7 on AAF, despite AAF being included in the pre-training data. This highlights the importance of mixing domains during fine-tuning.

\subsection{Experiment 2: Mixing three domains}

We select the five best train sets from experiment 1, which are CV 0\%~(i.e. AAF only), CV 80\% (i.e. CV $\cup$ AAF), CV 90\%, CV 100\% (i.e. CV only) and \textit{FullCV}. When adding CaFE to those sets, we observe no significant changes on CV or AAF, as we can see in figure~\ref{fig:wer_cv_aaf_tradeoff}. We observe the same trend with CFPB; however, its addition results in a massive boost on the CFPB test set~(-30\% WER). The best performance on CFPB is reached on the train set CV 90\% $\cup$ CFPB, with a WER of 24.8 and very good scores on CV and AAF.

\noindent Moreover, the best average WER on all four test sets is reached on the train set CV 80\% $\cup$ CFPB, see Table~\ref{tab:results}, last row. This shows the benefits of training on multiple domains, as we can leverage speech from other domains to improve on each domain with respect to the single-domain baseline.

\begin{figure}[h]
    \begin{center}
    \includegraphics[width=\columnwidth]{figures/results_augmentation.pdf}
    \caption{Graph of the WER with respect to the CV proportion. Plain and dashed lines correspond to experiments 1 and 3 respectively (without or with augmentation).}
    \label{fig:results_augmentation}    
    \end{center}
\end{figure}

\subsection{Experiment 3: using data augmentation}

We repeat experiment 1 using an augmented AAF dataset. Results are shown in figure~\ref{fig:results_augmentation}. We clearly see the advantage of using the augmented train set: performance improves on CV and AAF while staying nearly identical on CaFE and CFPB. This improvement is also visible in figure~\ref{fig:wer_cv_aaf_tradeoff}, bottom-left corner. In addition, this figure highlights the existing trade-off between error rates on different domains. When training using a fixed number of hours, an improvement on one domain usually comes at the expense of a (possibly slight) degradation in another domain.

\noindent The best performance on AAF among these 31h sets is reached using CV 10\% (which comprises 28h of African speech) with a WER of 5.7, but its performance is poor on CV~(16.7 WER). However, if we train on CV $\cup$ AAFaug (63h of speech), we obtain a WER of 5.3 and 9.7 on AAF and CV respectively (see "top result" in figure~\ref{fig:results_augmentation}), which is our best score on African accent.

These results on augmentation are encouraging: thanks to it, we are able to significantly reduce the WER on African accent ($-14$\% WER, relative) compared to the best result without augmentation. However, the augmentation method we describe (which is based on the variation of the McAdams coefficient) may not be particularly relevant to the task. Indeed, we ran an ablation study, training using only on-the-fly SpecAugment on the same quantity of data, and obtained nearly the same results. Further investigation is needed to assess more precisely the impact of the different augmentation methods on ASR performance, and the relevance of McAdams pseudo-voices.

Finally, we discuss the impact of using a substantially larger CV domain for training, forming the \textit{FullCV} family of train sets, which are the largest ones. Models trained on these sets tend to perform better on average on CaFE~($-6.1$\%) and CFPB~($-1.4$\%) compared to those trained on the second biggest sets, but also show a performance degradation on AAF~($+1.7$\%). Besides, no model trained on \textit{FullCV} sets has reached top performance on any of the test sets. This is likely due to the discrepancies between the accents in the training data, and indicates that blindly maximizing the amount of training data may not always be the best choice for minimizing WER, neither across domains nor for a single domain.

\input{tables/results.tex}