
\documentclass[10pt,twocolumn,letterpaper]{article}

\input{mycommands.tex}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage[accsupp]{axessibility}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{float}
\usepackage{tablefootnote}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{7985} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Supplementary Material of HS-Pose: Hybrid Scope Feature Extraction for Category-level Object Pose Estimation}

\author{
Linfang Zheng$^{1,4}$ \and Chen Wang$^{1,2}$ \and Yinghan Sun$^{1}$ \and Esha Dasgupta$^{4}$ \and Hua Chen$^{1}$ \and Ale\v{s} Leonardis$^{4}$ \and Wei Zhang\thanks{The corresponding author.} $^{1,3}$ \and Hyung Jin Chang$^{4}$ \and\\$^{1}$Department of Mechanical and Energy Engineering, Southern University of Science and Technology\\
$^{2}$Department of Computer Science, the University of Hong Kong\\
$^{3}$Peng Cheng Laboratory, Shenzhen, China\\
$^{4}$School of Computer Science, University of Birmingham\\
{\tt\small$\{$lxz948,exd949$\}$@student.bham.ac.uk, cwang5@cs.hku.hk, sunyh2021@mail.sustech.edu.cn}\\
{\tt\small $\{$chenh6,zhangw3$\}$@sustech.edu.cn,$\{$a.leonadis,h.j.chang$\}$@bham.ac.uk}
}

\maketitle

\section{Choice of the Baseline Method.}
We choose GPV-Pose~\cite{GPV-Pose_2022_CVPR} as the baseline for the following reasons:
\begin{itemize}
    \vspace{-1mm}
    \item \textbf{To demonstrate the effectiveness of the components in the HS-layer regarding pose estimation.} GPV-Pose is one of the state-of-the-art \textbf{3D-GC~\cite{3dgcn_lin_CVPR_2020} based} category-level object pose estimation methods. It is suitable for us to show how each component of the HS-layer incrementally added onto the 3D-GC layer influences the performance of pose estimation.
    \vspace{-2mm}
    \item \textbf{To compare the HS-layer with the strategies proposed by other methods.} For example, SSP-Pose~\cite{SSP-Pose_IROS_22} and RBP-Pose \cite{RBP-Pose_ECCV_2022} are also developed based upon GPV-Pose. The former leverages the prior-shape information and uses a shape deformation module to improve performance. The latter enhances GPV-Pose by a \textit{residual bounding box projection} (SPRV) module and a shape deformation module. We compare with SSP-Pose to demonstrate the effectiveness of STE. We also show the influence of the RF-F approach by comparing it with RBP-Pose. In the experiments, our simple STE and RF-F method outperform their counterparts in strict metrics (\eg, $\IoUSevenFive$, $\fiveDtwoC$, and $\fiveDfiveC$ metrics) and achieve competitive results in other metrics.
    \vspace{-2mm}
\end{itemize}

\section{About the Object Detector}
% {\color{red} TODO: rewrite this part.}
For a fair comparison, as when compared against other methods~\cite{GPV-Pose_2022_CVPR, RBP-Pose_ECCV_2022, SSP-Pose_IROS_22}, we also utilize the MaskRCNN~\cite{Maskrcnn_2017} to detect the objects in our experiments. It is worth noting that our method is not limited to MaskRCNN~\cite{Maskrcnn_2017}. Other object detectors such as SD-MaskRCNN~\cite{SD-MaskRCNN_ICRA_2019} and PointNet~\cite{Pointnet_2017_CVPR} can also be used. 

\section{About the Speed} 
Since the speed can be different when performed on different machines, we only use the results of the speed to demonstrate that our method can achieve real-time performance and do not emphasise a speed comparison with other methods. 

\subsection{The Speed of GPV-Pose}
For a fair speed comparison with the baseline, GPV-Pose~\cite{GPV-Pose_2022_CVPR}, we report the speed of GPV-Pose on our machine with the same evaluation code as ours. The speed of GPV-Pose achieved on our machine (69 FPS) is faster than the original paper (20 FPS) due to the following reasons:
\begin{itemize}
    \vspace{-1mm}
    \item  \textbf{The difference between the machines.} The original paper of GPV-Pose reports the speed test on a single TITAN X GPU, while we test GPV-Pose on a single RTX 3090 GPU with an Intel(R) Core(TM) i9-10900K CPU, 32 GB RAM. The speed is 33 FPS on our machine.
    \vspace{-2mm}
    \item \textbf{The difference in the evaluation code.} Our evaluation code is a refactored version of GPV-Pose's code. We change some for-loop operations to batch operations and remove unnecessary calculations (\eg the bounding box voting and symmetric point cloud reconstruction) during inference. These changes significantly boost the speed from 33 FPS to 69 FPS. All the changes have passed unit tests to ensure they get the same results as the original code.  
\end{itemize}

\input{CVPR2023/tbl/quantitative/real275_full_result}
\input{CVPR2023/fig/mAP_cirve/item.tex}
\section{Comparison with State-of-the-Arts Methods}
We add the comparison between the proposed HS-Pose with methods that use different data modalities (\eg RGB and RGB-D) in this section.

\subsection{Results on REAL275 dataset.} 
The comparison between the proposed method and the state-of-the-art methods on the REAL275 dataset is shown in Table~\ref{tbl:cat_perform_all}. Our method outperforms the depth-only methods in 7 out of 8 pose estimation and size estimation metrics and achieves comparable performance in the remaining metric. Our depth-only method also achieves competitive results with the RGB-D-based approaches and outperforms them in several pose estimation metrics (\eg $\textbf{56.1\%}$ (ours) vs. $50.7\%$ on $\fiveDfiveC$ metric). It is worth noting that many of them are trained with synthetic data or using CAMERA25 and REAL275 for mixed training, which results in a large number of training images and many more objects (over 1K objects for CAMERA25 and Real275 mixed training) for training. In contrast, our method is trained on 1.6K real images of 18 objects. In Figure~\ref{fig:mAP_cirve}, we present the average precision of each category under different thresholds and compare it with the GPV-Pose. 


\input{CVPR2023/tbl/quantitative/camera25_full}
\input{CVPR2023/tbl/quantitative/per_cat_real275}
\input{CVPR2023/tbl/quantitative/per_cat_camera25}
\subsection{Results on CAMERA25 dataset.}
We test the proposed method on the CAMERA25 dataset and show the comparison results of the proposed method with other approaches in Table~\ref{tbl:cat_perform_camera_full}. We achieved top and second scores on 5 out of 6 metrics (4 tops and 1 second) with no need for RGB data. 
% It is also worth noting that CAMERA25 is a synthetic dataset that contains no noise, so one main contribution of the proposed method, noise robustness, is not reflected in this dataset. However, this contribution can be identified by comparing the proposed and the state-of-the-art methods' performance on the CAMERA25 and the REAL275 dataset. The REAL275 dataset contains the same object categories as the CAMERA25 but is real-world collected and contains complex noise. It can be observed that the performance drop of our method is much less than other methods when encountering real-world noises in the REAL275. This demonstrates that our method is more noise-robust compared with other methods.


\section{Per-category Results}
The per-category results trained on the REAL275 and CAMERA25 datasets are shown in Table~\ref{tbl:per_cat_real275} and Table~\ref{tbl:per_cat_camera25}, respectively. 

\section{Settings of Noise Resistance Experiments}
In the ablation study [AS-6], we compared the outlier robustness of the proposed method and the baseline. We define \textit{outliers} as the points that do not belong to the target object. The \textit{outlier ratio} is defined as the ratio of the outliers' number to the total point number of the input point cloud. We use the REAL275 dataset for testing and generate the noisy input data by sampling points from the background and the object region according to the outlier ratio.
% Since in real applications the outlier of the input data mainly comes from inaccurate object mask detection, we randomly enlarged the object masks and sampled outliers from the enlarged area when generating the noisy data. 
To ensure a fair comparison, the noisy data used for testing the proposed and baseline methods is the same. 

\input{CVPR2023/fig/noise_resistance_ORL/item}
\section{Ablation Study on ORL}
We demonstrate the effectiveness of the proposed outlier robust feature extraction layer (ORL) in Fig.~\ref{fig:noise_resistance_ORL}. We test the noise resistance of the proposed HS-Pose with and without ORL using different outlier ratios (from 0.0\% to 40.0\%) in the input point cloud. The figure shows that the ORL successfully enhances the performance on the size-pose-joint metric ($\text{IoU}_{75}$), translation metric ($2\text{cm}$), and rotation metric ($10^\circ$) across different noise levels.
% \subsection{The robustness to noise with and without ORL}
% Add the descriptions about the points and the outliers.

\section{Ablation Study on the Neighbor Numbers}
% Since the main contribution of our work is on the structural design of the HS-layer, we put the ablation study of the neighbor numbers used in RF-F and ORL in this supplementary. 
To investigate the influences of the neighbor numbers, we test the performance of the proposed method using different neighbor numbers (from 3 to 40) in the RF-F and ORL\footnote{Due to the limit of the GPU memory size, we set the batch size to 16, 16, and 8 for 20, 30, and 40 neighbors, respectively.}. The experiments are separated into three groups to evaluate the impact: 1) change the RF-F's neighbor number with the ORL's neighbor number fixed, 2) change the ORL's neighbor number with the RF-F's neighbor number fixed, and 3) change the neighbor numbers of the RF-F and ORL simultaneously. 
% The experiment results are presented in Table~\ref{tbl:RFF_neighbor}, Table~\ref{tbl:ORL_neighbor}, and Table~\ref{tbl:ORL_RFF_neighbor}, respectively. We discuss each of them in the following.

% When assessing the neighbor number for RF-F, we maintained the ORL's neighbors at 10. Similarly, when testing the ORL, we fixed the RF-F's neighbors to 10.

\subsection{Change RF-F's Neighbor Number Only}
\input{CVPR2023/tbl/neighbor_num/RF-F}
Table~\ref{tbl:RFF_neighbor} shows the performance of the proposed method using different neighbor numbers in the RF-F with the ORL's neighbor number fixed to 10. As seen from the table, finding more neighboring 3D points using feature distance requires a longer time, while a certain range of neighbor numbers (around 10-20 neighbors) produces better precision than other numbers. Specifically, the speed decreased from 64 FPS to 30 FPS when increasing the neighbor number from 3 to 40. In the meantime, the performance on $5^\circ 2\text{cm}$, which starts at $39.8\%$, reaches its best at $46.5\%$ when using 10 neighbors, after which it begins to decline and ultimately reaches a score of $41.6\%$ at 40 neighbors. Generally, using 10 neighbors for RF-F achieves the overall best performance while maintaining fast speed. The reason why insufficient and excessive neighbor numbers adversely affect the precision might be that fewer neighbors cannot fully characterize the global geometric feature, whereas an excessive number of neighbors may obscure the geometric structural information in the formed receptive field. 


\subsection{Change ORL's Neighbor Number Only}
\input{CVPR2023/tbl/neighbor_num/ORL}
Table~\ref{tbl:ORL_neighbor} shows the performance of the proposed method using different neighbor numbers in the ORL with the RF-F's neighbor number fixed to 10. According to the table, the speed for finding neighboring points in 3D space is relatively stable, which only dropped by 4 FPS when the neighbor number increased from 3 to 40. Compared to the RF-F, the neighbor number impacts the speed less. The reason is that in RF-F, the nearest neighbors are found in higher dimensional feature space. In terms of precision, an appropriate range of neighbor numbers is beneficial for ORL to balance finding the reliable points and outliers. Similar to RF-R, using 10 neighbors performs better than other values in our experiments.

\subsection{Change the Neighbor Number Simultaneously}
\input{CVPR2023/tbl/neighbor_num/ORL_RFF}
Table~\ref{tbl:ORL_RFF_neighbor} shows the performance of the proposed method with the neighbor numbers of the ORL and RF-F changing simultaneously.
As shown in the table, when the neighbor numbers are around 10-30, the performance of our method is best. Moreover, in this range, using the same number of neighbors leads to better precision compared to fixing one of the neighbor numbers to 10. The reason might be that with increasing neighbor number in RF-F, more global geometric structure information can be found, and the possibility to include uninformed points is also increased. Therefore, with the neighbor number in ORL also increased, the effect brought by these uninformed points can be compensated, thus resulting a better performance. However, with too many neighbors, the performance still deteriorates because the balance between identifying reliable points and rejecting outliers is hurt.

% As shown in the table, when the neighbor number exceeds 5, using the same number of neighbors for RF-F and ORL leads to better precision compared to fixing one neighbor number to 10. 


% We find that when the neighbor numbers are around 10-30, the performance of our method is best. 


% The expressiveness of the global structural information is impacted when using too few neighbors. Here is our explanation of why the same neighbor number for ORL and RF-F improves the performance: 

% \input{CVPR2023/fig/qualitative_supplementary/item}


% \section{Performance Calculated by the CATRE Evaluation Code.}
% We report the results using the official bench marking code\footnote{\href{https://github.com/hughw19/NOCS_CVPR2019/blob/master/utils.py}{https://github.com/hughw19/NOCS\_CVPR2019/blob/master/utils.py}} to ensure fair comparison with the state-of-the-art methods in our paper, as all of the compared methods are evaluated using the official bench marking code. However, Liu \etal \cite{liu_2022_catre} reported in their GitHub repository\footnote{\href{https://github.com/THU-DA-6D-Pose-Group/CATRE}{https://github.com/THU-DA-6D-Pose-Group/CATRE}} that there is a small error in that bench marking code for the IoU score calculation. For correctness, we also report the results calculated using the corrected version\footnote{\href{https://github.com/THU-DA-6D-Pose-Group/CATRE/blob/main/core/catre/engine/test_utils.py}{https://github.com/THU-DA-6D-Pose-Group/CATRE/blob/main/core/catre/engine/test\_utils.py}} provided by Liu \etal in Table~\ref{tbl:cat_perform_CATRE}. It is worth noting that the two versions show similar ranking in the results: the method which has a better performance gain higher score in both of the versions.  the pose estimation results only show slightly difference with the original version. 
% \input{CVPR2023/tbl/quantitative/refine_real275}

% \input{CVPR2023/fig/qualitative_supplementary/item}

\section{Qualitative Results}
\input{CVPR2023/fig/qualitative_supplementary/item}
More qualitative results comparing our method with the GPV-pose are shown in Fig.\ref{fig:qualitative}.

% \section{Performance on Instance-level Datasets}
% \input{CVPR2023/tbl/quantitative/linemod}
% We additionally tested the proposed method on the LineMOD~\cite{LineMod} dataset, which is commonly used for instance-level object pose estimation. Follow~\cite{GPV-Pose_2022_CVPR, FSNET_2021_CVPR}, we use ADD(-S) metric to evaluate the performance. The comparison between the proposed and the state-of-the-art is shown in Table~\ref{tbl:linemod}.

% {\color{red} Add something here if the experiment results have come out}
% \input{CVPR2023/tbl/quantitative/refine_real275}

%%%%%%%%% REFERENCES
\newpage
{\small
\bibliographystyle{ieee_fullname}
% \bibliographystyle{ieeetr}
\bibliography{supbib}
}

\end{document}
