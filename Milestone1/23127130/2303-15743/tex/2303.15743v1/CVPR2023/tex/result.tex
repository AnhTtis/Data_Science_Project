\section{Experiments}
\input{CVPR2023/tbl/ablation/complete_ablation.tex}

\textbf{Implementation details:}
% {\color{red}@Chen wang: could you please check this part and see if it reads okay? The original version can be found in the following comments.}
To rigorously verify the effectiveness of the proposed HS-layer and ensure a fair comparison with the baseline GPV-Pose, we construct the HS-Pose by replacing GPV-Pose's 3D-GC layer with the HS-layer while keeping the overall network structure and network parameters identical to the GPV-Pose, as shown in Figure~\ref{fig:framework}. For a fair comparison, we choose 10 neighbors for the RF-F, consistent with the RF-P in GPV-Pose. The neighbor number of ORL is the same as the RF-F. No other parameters need to be set for the HS-layer as they only depend on the input and output. We also keep the settings, data augmentation strategy, loss terms, and their parameters, the same as those in GPV-Pose's official code\footnote{\label{foot:gpv_cite}\href{https://github.com/lolrudy/GPV_Pose}{https://github.com/lolrudy/GPV\_Pose}}. Following GPV-Pose, the off-the-shelf object detector MaskRCNN~\cite{Maskrcnn_2017} is employed to generate instance segmentation masks, and 1028 points are randomly sampled as the input to the network. The code is developed using \texttt{PyTorch}. We run all experiments on a computer equipped with an Intel(R) Core(TM) i9-10900K CPU, 32 GB RAM, and an NVIDIA GeForce RTX 3090 GPU. All categories are trained together with a batch size of 32, and the training epochs are set to 150 and 300 for REAL275 and CAMERA25 datasets, respectively. The Ranger optimizer\cite{ranger_optimizer_1_ICLA_2019, Ranger_optimizer_2_ECCV_2020, Ranger_optimizer_3_2019_NIPS} is used with the learning rate starting at $1e^{-4}$ and then decreasing based on a cosine schedule for the last $28\%$ training phase.


\textbf{Baseline methods: }
We use GPV-Pose~\cite{GPV-Pose_2022_CVPR} as the baseline for the ablation study. Since GPV-Pose did not provide the performance of $10^\circ2\text{cm}$, $2\text{cm}$, and $5^\circ$, we generate them using their official code\footnoteref{foot:gpv_cite}. To ensure a fair comparison of their relative speeds, we report GPV-Pose's speed on our machine using the same evaluation code as ours. The results of the other methods are taken directly from the corresponding papers. 

\textbf{Datasets: }
We evaluate our method on REAL275~\cite{NOCS_2019_CVPR} and CAMERA25~\cite{NOCS_2019_CVPR}, the two most popular benchmark datasets for category-level object pose estimation. REAL275 is a real-world dataset that provides 7k RGB-D images in 13 scenes. It contains 6 categories of objects (can, laptop, mug, bowl, camera, and bottle), and every category contains 6 instances. The training data comprises 4.3k images from 7 scenes, with 3 objects from each category shown in different scenes. The testing data includes 2.7k images from 6 scenes and 3 objects from each category. CAMERA25 is a synthetic RGB-D dataset that contains the same categories as REAL275. It provides 1085 objects for training and 184 for testing. The training set contains 275K images, and the testing set contains 25K.

\textbf{Evaluation metrics:} 
Following~\cite{RBP-Pose_ECCV_2022, GPV-Pose_2022_CVPR}, we use the mean average precision (mAP) of the \textit{3D Intersection over Union (IoU)} with thresholds of $25\%$, $50\%$, and $75\%$ to evaluate the object's size and pose together. We evaluate the rotation and translation estimation performance using the metrics of $5^\circ$, $10^\circ$, $2\text{cm}$ and $5\text{cm}$, which means an estimation is considered correct if its corresponding error is lower than the threshold. The pose estimation performance is also evaluated using the combination of rotation and translation thresholds: $5^\circ2\text{cm}$, $5^\circ5\text{cm}$, $10^\circ2\text{cm}$, and $10^\circ5\text{cm}$.

\subsection{Ablation Study}
To validate the proposed architecture, we conduct intensive ablation studies using the REAL275~\cite{NOCS_2019_CVPR} dataset. We incrementally add the proposed strategies (STE, RF-F, and ORL) on the baseline (GPV-Pose) to study their influences. The full ablation study results are shown in Table~\ref{tbl:ablation_full}.

\textbf{[AS-1] Scale and translation encoding (STE).}
To demonstrate the effectiveness of STE and highlight the significance of scale and translation awareness when extracting latent features, we parallelly connected a single linear layer to each 3D-GC layer in the encoder of the GPV-Pose. The results in Table~\ref{tbl:ablation_full}, specifically the [B0] row, indicate that the inclusion of STE has a significant positive impact on scale and translation estimation ($\textbf{8.7\%}$ improvement on $\text{IoU}_{75}$ and $\textbf{5.9\%}$ improvement on $2\text{cm}$) while also slightly improving rotation estimation ($2.7\%$ improvement on $5^\circ$).
% More specifically, compared to our baseline [A0], the use of STE leads to an $\textbf{8.7\%}$ increase on $\text{IoU}_{75}$ metric and a $\textbf{5.9\%}$ increase on $2\text{cm}$ metric.
% Furthermore, STE improves the rotation estimation by $2.7\%$ on the $5^\circ$ metric.
% As shown in the [B0] row of Table~\ref{tbl:ablation_full}, our simple manner boosts the size estimation mAP by $\textbf{8.7\%}$ on $\text{IoU}_{75}$ metric compared to our baseline [A0]. The translation mAP on $2\text{cm}$ is also increased by $\textbf{5.9\%}$. Interestingly, encoding of translation and scale also increased the rotation estimation by $2.7\%$. 
As shown in Table~\ref{tbl:cat_perform}, such a simple addition even outperforms the SSP-Pose in several strict metrics ($\IoUSevenFive$, $\fiveDtwoC$, and $\fiveDfiveC$) and shows a notable improvement of $6.8\%$ on the $\IoUSevenFive$ metric, despite that the SSP-Pose extends the GPV-Pose using a much more complex shape deformation module. The experiment results demonstrate the effectiveness of STE.


% ----------------------------------------------------
\textbf{[AS-2] Receptive field with feature distance (RF-F).}
To show the usefulness of the proposed RF-F strategy and to demonstrate the importance of the global geometric relationships, we apply RF-F on GPV-Pose. From the results in Table~\ref{tbl:ablation_full} ([B1]), we see that RF-F has a substantial impact on rotation estimation and brings a performance leap by $\textbf{11.4\%}$ on $5^\circ$ metric. In addition, it improves the performance on $\text{IoU}_{75}$ and $2\text{cm}$ by $3.3\%$ and $2.0\%$, respectively, thanks to the fact that having a sense of the global geometric relationships is helpful in finding the object's center and shape boundary. When comparing the experimental results with the state-of-the-art methods in Table~\ref{tbl:cat_perform}, our simple RF-F strategy achieves 
comparable performance with the state-of-the-art methods and outperforms them on the stricter metrics (\eg $5^\circ2\text{cm}$ and $5^\circ5\text{cm}$).

\textbf{[AS-3] The combination of RF-F and STE.}
To exhibit the benefit of leveraging global geometric relationships and size-translation awareness, we conduct an experiment that combines RF-F and STE. As shown in [B2], the cooperation of RF-F and STE enhances each other and contributes to a better performance than their individual results. When compared with the baseline method, GPV-Pose, the combination of RF-F and STE improves $5^\circ5\text{cm}$ by $\textbf{10.8\%}$, $5^\circ$ by $\textbf{12.3\%}$ and $\text{IoU}_{75}$ by $\textbf{7.6\%}$.

% pose estimation performance by $\textbf{10.8\%}$($5^\circ5\text{cm}$) with the rotation increased by $\textbf{12.3\%}$ and $\text{IoU}_{75}$ increased by $\textbf{7.6\%}$.
\textbf{[AS-4] Outlier robust feature extraction layer (ORL).}
To demonstrate the effectiveness of the ORL, we add the ORL on top of [AS-3]. The results shown in the [D0] row of Table~\ref{tbl:ablation_full} demonstrate that using global features to adjust per-point feature extraction is helpful for both pose and size estimation with an improvement of ${5.2\%}$ ($10^\circ2\text{cm}$) and ${2.7\%}$ ($\text{IoU}_{75}$), respectively. To check the effectiveness of the outlier robust global feature, we further conduct two experiments by replacing the outlier robust global feature with two popular global pooling methods: average pooling [C0] and max pooling [C1]. The results of [D0], [C0], and [C1] all show the contribution of global information to pose estimation. The comparison between [D0] and [C0, C1] shows that the outlier robust global feature plays a positive role and enhances the overall performance.

\input{CVPR2023/fig/bar/item.tex}
\textbf{[AS-5] Capability of handling complex shapes.} \label{ab:complex_shape}
To exhibit the proposed method's capability in handling complex geometric shapes, we compare the rotation estimation results of the three proposed strategies (STE, RF-F, and ORL) and GPV-Pose on categories with different shape complexity in Figure~\ref{fig:bar}. As shown in the figure, the proposed method increases the mAP of categories with complex shapes (\ie mug and camera) and handles simple shapes (\ie bowl) with ease. The figure also demonstrates the effectiveness of leveraging global geometric relationships (STE+RF-F \vs STE) and shows the usefulness of outlier robust global information guided feature extraction in ORL (STE+RF-F+ORL \vs STE+RF-F).

\textbf{[AS-6] Noise resistance.}
To demonstrate the outlier robustness of the proposed method, we tested GPV-Pose and our method under different outlier ratios. As shown in Figure~\ref{fig:noise_resistance}, our method outperforms GPV-Pose by a large margin across a range of outlier ratios and is steadier when the outlier ratio increases. More details are in the Supplementary.

\textbf{[AS-7] Neighbor numbers.}
We investigate the influence of neighbor numbers used in ORL and RF-F on the performance. The details are presented in the supplementary. The results show that the performance is best when the neighbor numbers are in a certain range. We also observed that using the same neighbor numbers in ORL and RF-F enhances the performance: the precision results are best when the neighbor numbers for both ORL and RF-F are 20 or 30. The results for 20 neighbor numbers are shown in row [E0] of Table~\ref{tbl:ablation_full}, which outperforms the results with 10 neighbors. It should be noted that, for a fair comparison with GPV-Pose and focusing on the HS-layer's structural design, we use the results with 10 neighbors (as GPV-Pose) in all tables and figures if not specified.

\input{CVPR2023/fig/noisy_resistance/item.tex}


\input{CVPR2023/tbl/quantitative/real275_depth_only_results.tex}
\input{CVPR2023/fig/qualitative/item.tex}
%---------------------------------------------------------------------
\subsection{Comparison With State-of-the-Art Methods}
\textbf{Results on REAL275 dataset: }
We compare the performance of the proposed HS-Pose with the state-of-the-art methods in Table~\ref{tbl:cat_perform}, which shows the mAP scores in different metrics. We choose methods that use depth only for pose estimation for a fair comparison. As shown in the table, our method outperforms the state-of-the-art methods in all metrics except the $\text{IoU}_{50}$ in which our method also have comparable performance. Besides, our method can run in real-time. It is worth noting that our method outperforms the second rank on strict metrics by a large margin, with $\textbf{8.3\%}$ improvement on $5^\circ2\text{cm}$, and $\textbf{7.1\%}$ on $5^\circ5\text{cm}$, and $\textbf{6.9\%}$ on $\text{IoU}_{75}$. We also provide the comparison with methods \cite{Self-DPDN_ECCV_2022, SGPA_2021_ICCV, DualPoseNet_2021_ICCV, Tian_ShapePrior_2020_ECCV, NOCS_2019_CVPR, CASS_2020_CVPR, CR-Net_2021_IROS} and that by using other data modalities (\eg RGB and RGB-D) in the supplementary, we outperform the state-of-the-art on 5 metrics out of 9 and achieved the second rank on 3 metrics. Notably, most of them leverage synthetic data, whose datasets contain many more images and objects for training purposes, and also exhibit a limited inference speed. Our method is trained using REAL275 with only 1.6k images and 18 objects while achieving real-time performance. A qualitative comparison between GPV-Pose and our method is shown in Figure~\ref{fig:qualitative}. Our method achieves a better size and pose estimation (\eg the first three columns), shows robustness to occlusion (\eg the laptop in the last column), and handles complex shapes better (\eg the cameras and mugs in each column). 
\input{CVPR2023/tbl/quantitative/camera25}


\textbf{Results on CAMERA25 Dataset:}
The performance comparison of the proposed method and the state-of-the-art is shown in Table~\ref{tbl:cat_perform_camera}. Our method ranks top and second on all the metrics without prior information. Of the four scores ranked second, three are close to the tops with negligible differences ($0.1\%$ on $\tenDfiveC$ and $\IoUSevenFive$ metrics, and $0.2\%$ on $\fiveDtwoC$ metric). It is also worth noting that CAMERA25 is a synthetic dataset that contains no noise, so one main contribution of the proposed method, noise robustness, is not reflected in this dataset. However, this contribution can be identified by comparing the proposed and the state-of-the-art methods' performance on the CAMERA25 and the REAL275 dataset. The REAL275 dataset contains the same object categories as the CAMERA25 but is real-world collected and contains complex noise. It can be observed that the performance drop of our method is much less than other methods when encountering real-world noises in the REAL275. This demonstrates that our method is more noise-robust compared with other methods. A more comprehensive comparison with methods using RGB and RGB-D data is included in the supplementary, in which our method still shows competitive results despite using depth-only data.


% \input{CVPR2023/tbl/quantitative/camera25_full}
% \input{CVPR2023/tbl/quantitative/real275_full_result.tex}

