
\section{Methodology}
\input{CVPR2023/fig/framework/item.tex}
This paper considers the category-level pose estimation problem of estimating the 6D pose and 3D size of an arbitrary instance in the same category based on visual observation. In particular, our approach estimates the 3D rotation $\rotation \in SO(3) $, the 3D translation $\translation \in \realR^3$, and the size $\size \in \realR^3$ of object instances based on a depth image, the objects' categories, and segmentation masks. The segmentation mask and category information can be generated by object detectors (\eg MaskRCNN\cite{Maskrcnn_2017}). We use point cloud data $\pointCloud \in \realR^{N\times3}$ as the direct input of our network, which is achieved by back-projecting the segmented depth data and downsampling.

Due to the fact that geometric features are essential for determining an object's pose across different shapes, the 3D graph convolution (3D-GC) \cite{3dgcn_lin_CVPR_2020} is widely adopted in recent category-level object pose estimation methods \cite{FSNET_2021_CVPR, GPV-Pose_2022_CVPR, SSP-Pose_IROS_22, RBP-Pose_ECCV_2022, SAR-Net_Lin_2022_CVPR}. In particular, GPV-Pose\cite{GPV-Pose_2022_CVPR} uses a 3D-GCN encoder, formed by 3D-GC layers, together with geometric consistency terms for category-level object pose estimation and achieves state-of-the-art performance.
However, 3D-GC cannot perceive global geometric features, limiting its capability to handle complex geometric shapes and being sensitive to noise. Also, it is invariant to scale and translation, which contradicts category-level pose estimation tasks (\ie, size and translation estimation).

In this paper, we propose the hybrid scope geometric feature extraction layer (HS-layer) which is based on 3D-GC and keeps its local geometric sensitivity while extending it to have the following characteristics: 1) perception of global geometric structural relationships, 2) robustness to noise, and 3) encoding of size and translation information, particularly for category-level object pose estimation tasks. 

\subsection{Background of 3D-GC}\label{sec:3dgc}
The core unit of 3D-GC is a deformable kernel that generalizes the convolution kernel used in 2D image processing to deal with unstructured point cloud data. In particular, a 3D-GC kernel $K^S$ is defined as:
\begin{equation}
    K^S = \{(\kernelPoint_C, \weight_C), (\kernelPoint_1, \weight_1), \dots, (\kernelPoint_S, \weight_S)\},
\end{equation}
where $S$ is the total number of support vectors, $\kernelPoint_C=[0, 0, 0]^T$ is the central kernel point, $\{\kernelPoint_s\in \realR^3\}_{s=1}^S$ are the support kernel vectors and ${\weight}$ is the weight associated with each kernel vector. The 3D-GC kernel performs a convolution on the receptive field $R^M(\point_i)$, which is the point along with its neighbors and their associated features $\feature$:
\begin{equation}
    \label{eq:receptive_field}
    R^M(\point_i) = \{(\point_i, \feature_i), (\point_m, \feature_m) | \point_m \in \neighbor^M(\point_i)\}.
\end{equation}
Here $\neighbor^M(\point_i)$ is the set of the $M$ nearest neighbor points of $\point_i$. In particular, in \cite{3dgcn_lin_CVPR_2020} the receptive field with point distance metric (RF-P) is used for finding which of the nearest neighbors is within the point distance metric:
\begin{equation}
  \distance_p(\point_i, \point_j) = \norm{\point_i - \point_j}.
\end{equation}

For more details, the readers can refer to the original work \cite{3dgcn_lin_CVPR_2020}. It should be noted that 3D-GC has size and translation invariance by design. Although this invariance may be benefit tasks like segmentation and classification, it harms the pose estimation task as the size and translation are the targets to estimate. 
\subsection{Overall Framework}
The overview of the framework, HS-Pose, is shown in Figure~\ref{fig:framework}. We use the proposed HS-layer to form an encoder (HS-encoder) to extract the hybrid scope latent features from the input point cloud data. 
Then, the extracted latent features are fed into the downstream branches for object pose estimation. 
To demonstrate the effectiveness of the proposed HS-layer, which can be inserted into any category-level object pose estimation method, we construct our hybrid scope pose estimation network (HS-Pose) based on the state-of-the-art 3D-GC based GPV-Pose with minimal modification. Specifically, we only replace the 3D-GC layers of the 3D-GCN encoder of GPV-Pose with the HS-layer and keep all the other settings the same as the original GPV-Pose, which include network layers, network connection structure, and the downstream branches. Therefore, the extracted features from the encoder along with the input point cloud are fed into three modules for object pose regression, symmetric-based point cloud reconstruction, and bounding box voting. During inference, only the encoder and the pose regression module are used.

% \subsubsection{Overview of HS-layer}
Inside the HS-layer, we extract the hybrid scope latent features of the input using two parallel paths. The first path performs scale and translation encoding (STE), which provides essential information for size and translation estimation. The second path extracts outlier-robust geometric features by leveraging local and global geometric relationships, as well as global information in two phases. In the first phase, we form the receptive fields of points based on their feature distances (RF-F), then feed them to a graph convolution (GC) layer to extract high-level geometric features. The output of the GC layer is taken as the second phase's input and passes through an outlier-robust feature extraction layer (ORL), where each point feature is adjusted by an outlier robust global information. 
The final output of the HS-layer is the element-wise summation of the features of both paths. 


\subsection{Scale and Translation Encoding (STE)}
As mentioned earlier, even though 3D-GC provides geometric features crucial in rotation estimation, it loses the essential translation and scale information necessary for pose estimation. To address this problem, existing 3D-GC-based methods try to use another network for translation and size estimation \cite{FSNET_2021_CVPR} or concatenate the point cloud data with the extracted features for downstream estimation tasks with the assistance of other modules (\ie, bounding box voting)\cite{GPV-Pose_2022_CVPR, SSP-Pose_IROS_22, RBP-Pose_ECCV_2022}. While these methods are effective and all achieve improvements from the baseline, we emphasize the scale and translation information is beneficial during the latent feature extraction phase. 

As shown in Figure~\ref{fig:framework}, our suggestion is to connect in parallel a linear layer (see STE in HS-layer in the figure) to the geometric extraction path and then perform element-wise summation for their output features:
\begin{equation}
    \outputFeature_n = \geometricLayer(\feature_n) + \linearLayer(\feature_n),
\end{equation}
where $\linearLayer$ and $\geometricLayer$ apply linear transformation and geometric feature extraction on the features of the points, respectively, and $\feature_n$ is the $n$-th point's feature. In particular, we use the points' positions for size and translation encoding in the first layer since there are no features in the original point cloud. Our ablation study in Table~\ref{tbl:ablation_full} shows that this design choice keeps the advantage of geometric feature extraction, and boosts the performance of translation and scale estimation.

% Our ablation study in Table~\ref{tbl:ablation_full} shows that this simple manner does not affect the geometric feature extraction, but greatly improves overall performance and outperforms one of the state-of-the-art method, SSP-Pose\cite{SSP-Pose_IROS_22}.

\subsection{Receptive field with feature distance (RF-F)}
\input{CVPR2023/fig/reception_field_explanation/item.tex}
As introduced in Sec.~\ref{sec:3dgc}, 3D-GC learns awareness of local geometric features by forming receptive fields with point Euclidean distance metric (RF-P) and then using the deformable kernel-based graph convolution to extract geometric features for the receptive fields. However, RF-P restricts the perception to small local regions. Even though the perceived regions can be enlarged when cooperating with 3D graph pooling, it can not perceive the global geometric relationships essential for complex geometric structures. This limitation is also exhibited in the performance of category-level object pose estimation tasks \cite{GPV-Pose_2022_CVPR}, where the methods show impressive capability in handling simple geometric shapes (\eg bowl) while encountering difficulty with more complex shapes (\eg mug and camera). However, this limitation has not been well addressed. To this end, we extend the 3D-GC and propose a simple manner to leverage global geometric structural relationships.

We suggest forming the receptive field with the feature distance metric (RF-F). Specifically, we find $\point_i$'s neighbors using the feature distance metric:
\begin{equation}
\distance_f(\point_i, \point_m) = \norm{\feature_i -  \feature_m}.  
\end{equation} In other words, with the feature distance metric, the distance between two points is the Euclidean distance between their associated features. We denote the corresponding receptive fields as $R_{f}^M(\point_i)$. 
%This receptive field has the advantage that the following graph convolution is not restricted to only local regions, and distant points with similar features can also be included. 
This receptive field has the advantage that it is not restricted to local regions; distant points with similar features can also be included.

Figure~\ref{fig:reception_field} shows the difference between RF-P and RF-F. RF-F can capture a larger receptive field and, therefore, can capture geometric relationships in a larger area, while the RF-P always formed with local regions. 
% Note that we use the point Euclidean distance to form the receptive field in the first layer to extract the local geometric features.
% Since the input point cloud does not have corresponding features, a special setting is needed for the first layer of the HS-encoder. We take the points as features for scale and translation encoding and set $\feature_n$ and $\weight$ to $1$ for the graph convolution. RF-P is specially used in the first layer to extract local geometric features. 
For initialization, in the first layer, we use RF-P and set all the features $\feature$ to 1. The RF-F is used in the following layers for extracting higher-level geometric relationships. 
% Note that we keep RF-P in the first layer to extract the local geometric features and use RF-F in the following layers for extracting higher-level geometric relationships.


% Our experiments in Table~\ref{tbl:ablation_full} show that RP-F significantly boosts the rotation estimation performance and even outperforms the state-of-the-art method, RBG-Pose \cite{RBP-Pose_ECCV_2022}. The ablation study [AS-5] also demonstrates the effectiveness of RF-P in handling complex shapes.


\subsection{Outlier robust feature extraction layer (ORL) }
\input{CVPR2023/fig/noise_illustration/item.tex} 
3D-GC's sensitivity to noise influences the category-level methods \cite{FSNET_2021_CVPR, GPV-Pose_2022_CVPR, RBP-Pose_ECCV_2022, SSP-Pose_IROS_22} that are based on it. 
% \cite{liu_2022_catre, TransNet_ECCV_2022} also observed the noisy sensitivity of 3D-GC in their experiments. 
To address this problem, we introduce an outlier robust feature extraction layer (ORL) on top of the 3D-GC layer, which enhances the method's robustness to noise.
The ORL is constructed as follows. Denote the input to this layer as $\{(\point_1, \feature_1), \dots, (\point_N, \feature_N)\}$, where $\feature_n \in \realR^D$ is the feature of point $\point_n$. 
%As illustrated in Figure~\ref{fig:noise_illus}, the input point cloud is partial and contains outliers. The outliers carry little useful information and the features $\feature$ associated with the outlier points should not be trusted. To obtain the global feature that contains the whole geometric information, we need to have a mechanism that can alleviate the deviation caused by the outliers. By simply using the average pooling or max pooling, all points are taken equally in the pooling procedure, even if they are far away from the more reliable part.
As illustrated in Figure~\ref{fig:noise_illus}, outliers are distractive, and their features $\feature$ should not be trusted. To focus on the global information of the more reliable part, we need a mechanism to alleviate the deviation caused by the outliers. Using the global average or maximum pooling directly is limited in addressing this, as all points are taken equally in the pooling procedure.

% To address this problem, we propose to use the local region as a guide to extract the global feature, while lowering the influence of the outliers.
To lower outliers' influence, we propose using the local region as a guide to extract the global feature.
As shown in Figure~\ref{fig:framework} (see ORL), we first use RF-P to find the $M$ nearest neighbors of each point $\neighbor_p^M(\point_n)$. Then, we extract the channel-wise max features of $\neighbor_p^M(\point_n)$ using a maximum pooling layer. It should be noted that the points in the reliable parts are more likely to be presented in other points' receptive fields and thus contribute more to the results of the max pooling. The output of the max pooling layer is then passed to a global average pooling layer to get the global feature $\globalFeature$. We then generate an adjusting feature using the $\globalFeature$ and the original input per-point feature $\feature_n$ by first concatenating them and then feeding them to a linear layer. The final output of ORL is the result of the summation of the adjusting feature and the input features $\feature_n$ of this layer. 

% The ORL improves the robustness in presence of outliers. Our experiment shows that this layer improves the overall pose estimation performance with $3.4\%$ on $5^\circ2\text{cm}$, and our network exhibits robustness to noise under different ranges of noisy levels.  





% \input{CVPR2023/fig/gpv_pose_framework/item.tex}
% \subsection{Network Architecture and Losses}
% The network architecture is shown in Fig.~\ref{fig:framework}. Inside the HS-encoder, the downsampling rate for each 3D graph pooling layer \cite{3dgcn_lin_CVPR_2020} is $\frac{1}{4}$. We use the same loss terms as our baseline \cite{GPV-Pose_2022_CVPR}.

% \textbf{Initialization of the first HS-layer.} {\color{red}discuss this part}
% Since the input point cloud does not have corresponding features, a special setting is needed for the first layer of the HS-encoder. We take the points as features for scale and translation encoding and set $\feature_n$ and $\weight$ to $1$ for the graph convolution. RF-P is specially used in the first layer to extract local geometric features. 


