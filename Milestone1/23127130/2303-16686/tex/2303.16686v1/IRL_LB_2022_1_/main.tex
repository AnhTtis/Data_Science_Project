%\documentclass[draftclsnofoot,11pt, onecolumn]{IEEEtran}
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts,bbm}
%\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{subcaption}
%\usepackage{caption}
\captionsetup{font=footnotesize}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\usepackage[numbers]{natbib}
\newtheorem{lemma}[theorem]{Lemma}
\renewcommand*{\bibfont}{\footnotesize}
\usepackage[utf8]{inputenc}
%\usepackage{algorithm}
\usepackage[ruled,vlined]{algorithm2e}
\DontPrintSemicolon
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{setspace}
\usepackage{array}
\setlength{\intextsep}{5pt plus 2pt minus 2pt}

\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newcommand{\edited}[1]{\textcolor{blue}{#1}}
\newcommand{\edit}[1]{\textcolor{green}{#1}}

\begin{document}

\title{Communication Load Balancing via Efficient Inverse Reinforcement Learning}

\author{\IEEEauthorblockN{Abhisek Konar$^*$, Di Wu$^*$, Yi Tian Xu$^*$, Seowoo Jang$^\dagger$, Steve Liu$^*$, Gregory Dudek$^*$}
\IEEEauthorblockA{$^*$Samsung Electronics, Canada,  $^\dagger$Samsung Electronics, Korea (South)\\
%\IEEEauthorblockA{Samsung Electronics\\
\{abhisek.k, di.wu1, seowoo.jang, steve.liu, greg.dudek\}@samsung.com}
}

\maketitle

\begin{abstract}

%Load balancing aims to balance the load between different communication resources. It plays a critical role in determining the quality of service for modern communication networks. %Most of the existing machine learning-based approaches rely on reinforcement learning (RL) to address this problem.
%Reinforcement learning (RL) has recently shown effective on addressing this problem. The performance of RL algorithms is greatly dictated by their reward function the creation of which often requires careful calibration and expert guidance. In this work, we propose an inverse reinforcement learning (IRL) based approach to address the load balancing problem. Using a set of trivial demonstrations and access to pairwise ranking among them, we learn a reward function which, in turn, is used in a downstream task of training a policy network using reinforcement learning (RL). We test our method on a simulated environment and show that the policy trained on the learned reward function can significantly outperform the original demonstrations on multiple scenarios. This showcases the effectiveness of our method to infer and extrapolate a reward function in the domain of 5G load balancing.  

Communication load balancing aims to balance the load between different available resources, and thus improve the quality of service for network systems. After formulating the load balancing (LB) as a Markov decision process problem, reinforcement learning (RL) has recently proven effective in addressing the LB problem. To leverage the benefits of classical RL for load balancing, however, we need an explicit reward definition. Engineering this reward function is challenging, because it involves the need for expert knowledge and there lacks a general consensus on the form of an \textit{optimal} reward function. In this work, we tackle the communication load balancing problem from an inverse reinforcement learning (IRL) approach. To the best of our knowledge, this is the first time IRL has been successfully applied in the field of communication load balancing. Specifically, first, we infer a reward function from a set of demonstrations, and then learn a reinforcement learning load balancing policy with the inferred reward function. Compared to classical RL-based solution, the proposed solution can be more general and more suitable for real-world scenarios.  Experimental evaluations implemented on different simulated traffic scenarios have shown our method to be effective and better than other baselines by a considerable margin.

\end{abstract}

\input{intro}
%Fueled by the ubiquitous availability of smartphones or user equipment (UEs) and higher data consumption per device, the reliance of modern day society on internet continues to grow exponentially. 

The volume of wireless communication data has been snowballing in recent years. As reported in~\cite{yahoo}, the annual mobile data has increased seven times in the past six years and it is expected to reach 220.8 million terabytes per month by 2026. %We can expect that with the advancement in high-quality video streaming, and other AR/VR technologies, mobile data will keep on increasing in the next few years.
Alongside surge in mobile traffic, their distribution is also very uneven.  As reported in ~\cite{holma2012LTE-Advanced}, over $50\%$ of the mobile traffic is being routed through a number as low as $15\%$ of the existing cells, severely hampering the quality of experience of the users being served by these overloaded cells. Load balancing aims to balance the traffic distribution within the network, improving the quality of service (QoS) of the systems and the quality of experience (QoE) for the customers. %The need for better load balancing methods becomes even more apparent from studies conducted by

%Advances in the media industry have also added addtion
%gifted us high-resolution audio and video quality which, in turn, has led to an explosion of a myriad of data-intense applications like video conferencing, and streaming services. 
%According to \cite{ciscoreport2017}, the global mobile data was expected to grow seven-fold between 2017 and 2022. But the recent impact of COVID-19 has accelerated the process, with peak traffic growing at a rate of \textcolor{red}{annual growing rate?} $47\%$ as compared to the previously predicted $28\%$ for 2019-2020 \cite{globalinternetmap}. Such precedent growth in the demand for data, if not addressed, is bound to adversely impact the quality of service(QoS) of the connected devices. 

%Internet data traffic is communicated via cells: a combination of radio frequencies and sectors. 
%The problem of load balancing (LB) can be defined as finding an optimal strategy for redistributing the internet traffic from an overloaded cell to its neighboring cells to improve the quality of service (QoS) for the connected users. 

Load balancing algorithms can be classified into two broad categories: rule-based methods, and reinforcement learning-based methods. Rule-based methods aim to balance the load distribution using pre-determined rules but they usually lack the ability to adapt to rapidly evolving network conditons. Reinforcement learning (RL) is a class of learning algorithms, where a controller is optimized by interacting with an environment. RL has recently shown impressive performance on communication load balancing \cite{yuexu2019drlMLB, wu2021dataefficientRL, feriani2022multiobjective, ma2022coordinated,kang2021hierarchical}. 
It is particularly suited for solving intricate tasks with well-defined rewards like Atari games \cite{mnih2013playing}. It has also been applied for real-world tasks such as self-driving cars \cite{sallab2017deep}, smart grid~\cite{wu2018optimizing,, fu2022reinforcement,zhang2022metaems,wu2018machine}, traffic control~\cite{huang2021modellight}. 
The outcome of an RL policy is contingent upon the design choices of the associated reward~\cite{fucloser}. Reward-engineering, i.e., the design of the RL reward function, for real-world tasks can be quite challenging. Especially, for tasks like load balancing in communication networks, where improvement in the QoE of the customers is gaining significant traction. But QoE can be vague and different network performance indicators can contribute to varying degrees to form the desired reward function that can effectively capture the QoE for a demographic. Inverse reinforcement learning (IRL)~\cite{abbeel2004irl} can act as a potential solution by circumventing the need for tedious reward designing instead inferring it from expert data.


IRL leverages a set of expert demonstrations to infer the underlying reward function that best explains the behavior of the expert.
%It is often used for real-world problems where there is a lack of a well-structured reward function and has been heavily applied to tasks like self-driving \cite{sharifzadeh2016learning} and robot manipulation \cite{finn2016guided}. 
It has been used for real-world problems that lack a well-defined reward function like in robotics \cite{finn2016guided, zhang2021learning} and autonomous driving \cite{zou2018inverse}. 
%%%
%In autonomous driving, \cite{zou2018inverse, sharifzadeh2016learning} use expert demonstrations to train a policy that behaves like a human driver. \cite{kuderer2015learning} uncovers a set of reward functions, each of which can be trained to generate a policy suited for different environments.  For robotic manipulation, IRL has been greatly explored in the area of dexterous manipulation from operator demonstrations \cite{finn2016guided, zhang2021learning}. \cite{xie2019learning} trains a model to learn grasping from failed demonstrations. 
%Other areas where IRL has found applications include medical robotics where \cite{human_motion_analysis_Li_2020} uses IRL for human motion analysis of patients undergoing therapy for spinal cord injury, user intention inference in online platforms like Reddit \cite{das2014effects}, route prediction for taxis \cite{ziebart2008navigate} and activity forecasting from first-person vision \cite{activity_forecast_rhinehard_2017}.
%%%%
\begin{figure}[htp]
    \centering
    \includegraphics[width=0.85\columnwidth]{irl_lb_figures/TCS_TREX_blockdiagram_2.pdf}
    \caption{Overview of our IRL-based pipeline for load balancing. }
    \label{fig:tcs-trex-overview}
\end{figure}
Traditionally, IRL algorithms are generally 
formulated under the assumption that ``expert'' demonstrations are optimal and, the goal of such algorithms is to recover the reward function that best explains the observed expert behavior. This assumption, however, limits the performance of the output policy to that of the expert demonstrations, which in reality are rarely actually optimal. This limitation is addressed in a recent work on reward extrapolation~\cite{brown2019extrapolating}. Reward extrapolation is able is exploit sub-optimal demonstrations to infer a reward function which is used to train policies that can outperform the demonstrator. %In a recent work, T-REX (short for TRajectory EXtrapolation), \cite{brown2019extrapolating} proposed and showed the capability to outperform the demonstrator by leveraging sub-optimal demonstrations. 

In this paper, we present, the first attempt of using IRL and reward extrapolation for communication load balancing. The contributions of this paper are two-fold: (1) an IRL-based learning framework (see Fig.\ref{fig:tcs-trex-overview}) to train a reward function that accurately captures the latent reward function from a set of ranked sub-optimal demonstrations. This reward function is used in a downstream RL task to train a policy function that significantly outperforms the demonstrations; and (2) a trajectory sub-sampling technique, Temporally Consistent Sampling (TCS), that is suited for load balancing.
%The key contributions are:
%\begin{enumerate}
%\item An IRL-based learning framework to train a reward function that accurately captures the latent reward function from a set of ranked sub-optimal demonstrations. This reward function can be used in a downstream RL task to train a policy function that significantly outperforms the demonstrations.
%\item A trajectory sub-sampling technique, Temporally Consistent Sampling(TCS), that is suited for 5G load balancing.
%\end{enumerate}

\input{background}
\subsection{Terminology and notation}
For consistency, we define a few terms that will be used throughout this paper as well as some mild simplifying assumptions. A communication network is composed of a number of base stations (eNB). An eNB is a physical site containing radio access devices. Each eNB in turn consists of $N_S$ sectors. Each sector is made up of $N_C$ cells ($c_1, c_2, ... c_{N_{c}}$), one for each frequency in the sector. A cell serves user equipment (UEs) of a particular carrier and is directional in nature. The sectors of an eNB are designed in a non-geographically overlapping fashion to maximize coverage. A schematic diagram of a base station is shown in Fig.\ref{fig:hex7-layout}.  In practice, although actual networks may violate some of these assumptions, they are needed only for explanatory purposes.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\columnwidth]{irl_lb_figures/hex7_layout.pdf}
    \caption{The simulated communication network layout with seven eNBs along with a graphical representation of a base station within the network.}
    \label{fig:hex7-layout}
\end{figure}

\subsection{Load balancing mechanisms}
Out of the many network load balancing mechanisms presently in practice, in this work, we focus on idle mode user equipment based load balancing and mobility load balancing. They differ by the status of the UEs that they redistribute, targeting idle and active UEs respectively.
%The first one targets idle UEs, which are UEs that are not actively consuming network resources and can receive limited communications such as paging messages or receiving broadcast warning messages. The second one handles active UEs, which are UEs that are currently requesting network resources for different applications such as video streaming or online gaming.

\subsubsection*{Idle mode UE based Load Balancing}
IULB deals with offloading idle UEs from an overloaded cell to its neighbors by adjusting cell re-selection priorities. Each cell $i$ has an associated IULB weight ($w_i$). Once the load of a given cell breaches a specific threshold, idle UEs from that cell move to nearby cells with probability proportional to their current IULB weight. As IULB deals with idle UEs, improvements are perceived when the UEs becomes active.

\subsubsection*{Mobility Load Balancing}
MLB focuses on migrating active UEs from an overloaded cell to their less-loaded counterparts through handovers. 
% Like IMLB, 
MLB gets triggered when the IP throughput of a given cell (source cell) falls below a predetermined threshold value. Then, the source cell determines the set of UEs to transfer and their corresponding target cells according to the signal quality from the UEs and the IP throughput of the neighbouring cells. 
% Target cells are determined as a function of their current IP throughput. Finally, UEs get paired with target cells which again relies on the IP throughput of the target cell.
Unlike IULB, MLB has immediate effect on the network.
%load distribution across the network and may increase transmission delay.

\subsection{Metrics for load balancing}
\label{subsection:kpis}
Many metrics, focusing on different aspects of the network, can be used to quantify the condition of a network. In this work, we use the IRL approach that, in theory, should bypass the need for reward engineering in favour of pairwise trajectory ranking provided by an expert. However, due to the lack of such infrastructure, we resort to using an engineered reward function. We focus on a set of metrics based on the throughput of the network following the practice used in a few recent works~\cite{wu2021dataefficientRL,kangHierarchy2021,Li2022clusteringRL}. %These are referred to as the key performance indicators (KPIs) of the network and has been frequently used in the literature \cite{Li2022clusteringRL, kangHierarchy2021}.
\begin{enumerate}
    \item $\mathbf{T}_{min}$ = $\min_{i \in \{1,...N_{c}\}} x_i$ denotes the minimum IP throughput among the cells of the sector under consideration. Here, $x_i$ denotes the IP throughput of cell $i$. Higher $\mathbf{T}_{min}$ indicates better worst case cell performance.
    \item $\mathbf{T}_{std}$ = $\sqrt{\frac{1}{N_{c}} \sum^{N_c}_{i=1}(x_i- \frac{1}{N_c}\sum^{N_c}_{i=1}x_i})^2$ is the standard deviation in IP throughput of the cells. 
    Lower $\mathbf{T}_{std}$ implies fairer services across the cells. 
    \item $\mathbf{T}_{cc}$ = $\sum_{i=1}^{N_{c}} \mathbbm{1}(x_i < \mathbbm{x})$ counts the number of cells in the sector that have a throughput lower than a given threshold value $\mathbbm{x}$. Here $\mathbbm{x}$ is chosen to be a small constant. Lower $\mathbf{T}_{cc}$ suggests less congested cells.
\end{enumerate}
In addition to gauging performance, the above KPIs individually or as a combination can also serve other roles like creation of a reward function in an RL setting \cite{Li2022clusteringRL}.

\subsection{Load balancing algorithms}
%\textcolor{red}{Intelligent Communication Load }}
%The literature on load balancing in communication networks is rich and extensive.
Classic load balancing algorithms are rule-based and use expert domain knowledge to make informed decisions. \cite{kwan2010} performs intra-frequency load balancing in Long Term Evolution (LTE) networks by automatically adjusting the cell-specific offset based on the current cell loads. \cite{yingyang2012} focuses on the use of adaptive step-size to adjust the offset between neighboring cells. 
%However, their dependence on expert domain knowledge often makes them expensive to implement and hard to scale for bigger complicated networks.
\cite{elbv_shorabi2021} exploits the geometric properties of the network infrastructure and uses Voronoi tessellations over a geographical area to compute UE association with nearby edge servers.
Data-driven approaches have recently enjoyed considerable success. Various learning-based methods, including supervised and reinforcement learning, have been successfully applied to the problem of load balancing.
\cite{zhang2021STD} use spatio-temporal network data to predict future traffic patterns and subsequently use this information to perform proactive user association.
%\cite{munoz2012fuzzy} focuses on the use of RL for load balancing in femtocells.
\cite{mwanjeSON2016}and \cite{kudoHnets2014} use Q-learning to address the load balancing in self-organizing networks, and heterogeneous networks respectively. 
%\cite{xuUDN2019} takes a 2-stage approach, where they use a centralized clustering algorithm in conjunction with an RL-based load balancing method in ultra-dense networks(UDN). 
\cite{yuexu2019drlMLB} proposes a multi-agent actor-critic network model in a model-free off-policy setting to obtain an optimal policy for MLB.
%\cite{mwanje2013Q-learn} tunes the cell individual offset (CIO) to trigger handovers between neighboring cells based on factors like current cell load and user distribution of the service cell. 
%\cite{baros2019D2D} restructures load balancing in a device-to-device (D2D) communication network as a network partitioning problem and use deep Q-learning (DQN) to solve it.
\cite{gupta2021DRL} uses RNN to understand past SINR measurements as a function of UE trajectory and number of HOs. \cite{wu2021dataefficientRL} propose a data-efficient transfer learning-based RL approach that is robust to environmental fluctuations.

A thorough scrutiny, however, will reveal a lack in consensus for the choice of reward function. \cite{mwanjeSON2016} considers the change in load while optimizing for MLB. 
\cite{yuexu2019drlMLB} use the inverse of the maximum load of a cell in a given neighborhood and, \cite{kudoHnets2014} relies on the change in overload among neighboring cells. The choice of the reward function, in RL, defines the task \cite{Ng00algorithmsfor} and plays a pivotal role in determining the final performance of the controller policy. There are different opinions in the community on what an \textit{ideal} reward function should be. 

\subsection{Inverse RL and reward extrapolation}
Inverse RL aims to recover a reward function from expert demonstrations that best explains the expert's behavior \cite{abbeel2004irl}. 
%Fundamentally an under-constrained problem, \cite{ziebart2008maxent} uses an entropy-based constraint to provide a more rigorous formulation. 
While the use of IRL is yet to receive traction in network load balancing community, there has been recent work that use it in the field of cellular networks. \cite{zhang2022} use IRL to optimally allocate power in multi-user cellular networks. Other real-world applications where IRL has seen significant success are autonomous driving and dexterous robotic manipulation. In autonomous driving, expert demonstrations are used to train a policy that drives  \cite{zou2018inverse} and parks \cite{fang2021maximum} like a human driver.
%\cite{kuderer2015learning} uncovers a set of reward functions, each trained to generate a policy suited for a different environment. 
For robotic manipulation, \cite{finn2016guided} uses IRL to train a robot to perform dexterous tasks. \cite{xie2019learning} trains a model to learn grasping from failed demonstrations. Recent works like TRajectory EXtrapolation (T-REX) \cite{brown2019extrapolating} aim towards reward extrapolation. The goal being able to leverage the goodness of expert data to come up with a reward function that can infer rewards from unseen states.  It is a relatively new topic, with applications limited to environments like Atari and Mujoco \cite{mujoco}.
%Historically, an implicit assumption made by the IRL algorithms is the optimality of the expert. To that end, the goodness of the recovered reward function is contingent upon the quality of the demonstrations used. Reward extrapolation \cite{brown2019extrapolating} relaxes this assumption. Instead, from a set of pairwise ranked demonstrations, is able to recover a reward function that is capable of capturing and extrapolating performance beyond that of the demonstrator.


\input{method}
Our proposed method, TRajectory EXtrapolation(T-REX) \cite{brown2019extrapolating} using Temporally Consistent Sampling (TCS) follows a similar training pipeline of T-REX along with the inclusion of a data-augmentation module, that uses TCS, to generate training data for better extrapolation results. We start by defining the problem in Section \ref{subsection:problem_formulation}, followed by the individual components of the IRL-based learning framework in Sections \ref{subsection:reward_learning} and \ref{subsection:data_augmentation}.

\subsection{Problem formulation}
\label{subsection:problem_formulation}
The problem is modelled as an Markov decision process (MDP) represented by a 4-tuple ($\mathcal{S}$, $\mathcal{A}$, $\mathcal{P}$, $\mathcal{R}$) and are defined as follows:
\begin{enumerate}
    \item $\mathcal{S}$ is the set of states. Each state, s $\in \mathcal{R}^{3 \text{x}N_C}$, consists of 3 components: the number of active UEs $s_{ue} \in \mathcal{R}$, the average throughput per cell $s_{ip} \in \mathcal{R}$, and the average percentage of the physical resources of each cell used $s_{prb} \in \mathcal{R}$ of every cells of a given sector. The number of cells in our system, $N_C$, is 4. Hence s $\in \mathcal{R}^{12}$.
    \item $\mathcal{A}$ is set of actions. Each action, $a$ consists of two parts, one to initiate IULB ($a_{IULB} \in \mathcal{R}^{N_C}$) and one that controls handover thresholds to trigger MLB ($a_{MLB} \in \mathcal{R}^{3 \text{x} N_C}$). All the actions are discrete with a step size of $1$.
    \item $\mathcal{P}(s,a,s')=P(s'=s_{t+1}|s=s_t, a=a_t)$  are the state transition probabilities. 
    \item $\mathcal{R} : \mathcal{S} \rightarrow \mathbb{R}$ is the reward function.
\end{enumerate}
For IRL, $\mathcal{R}$ is unavailable and the objective is to learn the reward function from expert observations. A policy $\pi_{\phi} : \pi_{\phi}(a|s) \in [0, 1]$, parameterized by $\phi$, is a function that maps a given state, $s \in \mathcal{S}$, to a distribution over actions, $a \in \mathcal{A}$. A trajectory $\tau$ of length $n$ is represented by a sequence of states ${\{s_1, s_2, ... s_n\}}$.

 
Given a set of $m$ ranked trajectories, ${\{\tau_1, \tau_2, ... \tau_m\}}$ where  $\tau_j$ is better than $\tau_i$ (i.e., $\tau_i \prec \tau_j$) if $ i < j$, the objective is to find a parameterized reward network $\hat{r}_\theta$ that is able to capture the relative ranking of the demonstrated trajectories. In the process, it has to extrapolate the underlying reward function the demonstrations are trying to maximize. Once, $\hat{r}_\theta$ is obtained, it is used to train a policy that has the potential to outperform the demonstrations.

\subsection{Reward extrapolation using T-REX}
\label{subsection:reward_learning}
The goal is to train a reward network, $\hat{r}_{\theta}$, that maintains $\hat{J}_\theta(\tau_i) < \hat{J}_\theta(\tau_j)$ when $\tau_i \prec \tau_j$, where $\hat{J}_\theta(\tau_i) = \sum_{s \in \tau_i} \hat{r}_\theta(s)$ denote the total reward obtained by trajectory $\tau_i$ using $\hat{r}_\theta$. For such a model, the general loss function can be given by Equation \ref{eqn:trex-loss-general}.
\begin{equation}
    \label{eqn:trex-loss-general}
    \mathcal{L}(\theta) = \mathbf{E}_{\tau_i, \tau_j \in \Pi}\bigg{[} \xi(P(\hat{J}_\theta(\tau_i) < \hat{J}_\theta(\tau_j)), \tau_i \prec \tau_j \bigg{]}   
\end{equation}
where $\Pi$ is the set of ranked demonstrations, and $\xi$ is a binary classification loss. Following the classic models of preference \cite{bradley1952rank} (Equation \ref{eqn:preferencemodel}), and using a cross-entropy loss for $\xi$, $\mathcal{L}$ can be rewritten as Equation \ref{eqn:trex-loss-fn}.
 
\begin{equation}
    \label{eqn:preferencemodel}
    P\bigg{(}\hat{J}_\theta(\tau_i) < \hat{J}_\theta(\tau_j)\bigg{)} \approx \frac{\exp{\sum\limits_{s \in \tau_j} \hat{r}_\theta(s)} } { \exp{\sum\limits_{s \in \tau_i} \hat{r}_\theta(s)} + \exp{\sum\limits_{s \in \tau_j} \hat{r}_\theta(s)}}
\end{equation}
\begin{equation}
    \label{eqn:trex-loss-fn}
    \mathcal{L}(\theta) = - \sum_{\tau_i \prec \tau_j} \log \bigg{(} \frac{\exp{\sum\limits_{s \in \tau_j} \hat{r}_\theta(s)}} {\exp{\sum\limits_{s \in \tau_i} \hat{r}_\theta(s)} + \exp{\sum\limits_{s \in \tau_j} \hat{r}_\theta(s)} } \bigg{)}
\end{equation}

Once a reward network $\hat{r}_\theta$ is trained, it is used to train a policy using Proximal Policy Optimization (PPO) \cite{schulman2017proximal}. Additional details about the training parameters for both reward and policy learning are provided in Section \ref{subsection:training_details}.

\subsection{Temporally Consistent Sampling}
\label{subsection:data_augmentation}
The collection of expert demonstrations is expensive, and reward extrapolation relies on data augmentation making the sub-sampling technique employed for the data augmentation a vital component of the training pipeline. \cite{brown2019extrapolating} proposes the sampling of contiguous blocks of states of equal length from randomly selected starting points in trajectories. The method assumes that the relative ranking of these sub-trajectories match that of the complete trajectories they were sampled from. While this assumption is reasonable in certain domains such as Atari and Mujoco, we argue that it can be detrimental in the load balancing domain when the performance is mainly determined by the throughput-based metrics as presented in Section~\ref{subsection:kpis}. It is commonly observed that the network throughput varies significantly through time due to regular daily high and low usage periods. For example, Fig. \ref{fig:trajectory_over_timesteps} shows the variation of the minimum IP throughput among the cells ($T_{min}$) in the span of one week. Clearly, sampling pairs of sub-trajectories from different time intervals would often result in an inconsistency between the assumption from \cite{brown2019extrapolating} and the performance metric. Indeed, we find such inconsistency in $35\%$ of our samples in our experiment. Its adverse effect on the learning capabilities of the reward network is discussed in detail in Section \ref{subsection:reward_extrapolation}.
% While this technique works well in the domains of Atari and Mujoco, the periodic nature of the trajectories in 5G networks as shown in Fig. \ref{fig:trajectory_over_timesteps} and the high variation (Fig. \ref{fig:kpi_hist}) in network KPIs over different stages of a given trajectory makes the problem of generating a reliable training sample a challenge. To elaborate, the loss function (Eqn. \ref{eqn:trex-loss-fn}) aims to learn a trajectory-level classifier. To that end, an effective training sample is a pair of sub-trajectories where the sum of rewards obtained by the states sampled from the `better' sub-trajectory is more than that of its `worse' counterpart. Sampling $10000$ training pairs of lengths $10-20$ over different traffic scenarios and network KPIs (as rewards), we found the above condition was only satisfied on an average of $65\%$ over multiple runs, adversely affecting the learning capabilities of the reward network as discussed in detail in Section \ref{subsection:reward_extrapolation}.

To mitigate the problem, we propose a new sampling procedure: Temporally Consistent Sampling(TCS). TCS introduces two changes to the previously proposed sub-sampling technique. 
Firstly, to account the temporal variation in the network throughput, 
% Firstly, to account for the periodic nature of the trajectories, 
we opt for sampling over random time indexes rather than contiguous blocks. Secondly, as the variation
in the network KPIs across different trajectories at a given time index is relatively lower and fairly consistent with the original relative ranking (Figure \ref{fig:indiv_hist-miniptput}), we mandate temporal consistency,
% temporal consistency 
i.e., given a pair of ranked trajectories, a sub-trajectory should be sampled from the same time indexes for both the trajectories maintaining temporal consistency. Empirically, TCS can reduce the aforementioned inconsistency from $35\%$ to $15\%$, significantly increasing the effectiveness of the training samples.
% placing the aforementioned constraints in the sampling process leads to the generation of samples that are far more likely ($\approx 85\%$) to generate effective training samples. 
Our approach is summarized in Algorithm \ref{alg:tcs-trex}.

%This is not optimal for the problem at hand because of the following reason. The loss function of T-REX (Eqn. \ref{eqn:trex-loss-fn}), aims to learn a trajectory-level classifier. To that end, it successfully learns to differentiate between the states of a good and a bad trajectory. However, the reward distribution over the states of a single trajectory is not explicitly taken into consideration.


\begin{figure}
    \centering
    \includegraphics[width=0.85\columnwidth]{irl_lb_plots/min_ip_throughput_scenario_23.pdf}
    \caption{Plot of all trajectories from the demonstration set from a load balancing scenario. The trajectories exhibit a periodic behavior over the minimum IP throughput. Similar trends are observed across different network KPIs.}
    \label{fig:trajectory_over_timesteps}
\end{figure}

\begin{figure}
    \centering
        \includegraphics[width=0.85\columnwidth]{irl_lb_plots/indiv_state_scatter_20_50_scenario_23.pdf}
        \caption{Scatter plot of the minimum IP throughput at different timesteps of two trajectories. While the value across different timesteps varies widely, for a given time, minimum Ip throughput of the state from the better trajectory is consistently (9/10) higher than its worse counterpart. Only 10 such states are shown for visual clarity. } %Over the entire trajectory, this order is maintained 128/168 times.
        \label{fig:trajectory_over_timestep_zoom}
    \label{fig:indiv_hist-miniptput}
    
\end{figure}
\begin{algorithm}[t]
\caption{Temporally consistent sampling}
\label{alg:tcs-trex}

\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}

\Input{Ranked demonstrations: $D_E$, Number of samples to generate: $t_{sub}$ 
}
\Begin{
    \For{$i=1$ to $t_{sub}$}{
    Randomly select a pair of trajectories, $t_x$ and $t_y$, from $D_E$ \\
    Define the length of a sample sub-trajectory, $l$. \\
    Randomly select a set of $l$ indexes, $n_l$. \\
    $t_{x}^{samp} \leftarrow t_x[n_l]$, $t_{y}^{samp} \leftarrow t_y[n_l]$ \\
        \eIf{rank($t_x$) $>$ rank($t_y$)}{
            $t_{label} \leftarrow 0$ 
        }{
            $t_{label} \leftarrow 1$ 
        } 
    Append $\{(t_{x}^{samp}, t_{y}^{samp}), t_{label}\}$ to $T_s$. 
    }
    Return $T_s$
}
\Output{Training set, $T_s$}
\end{algorithm}
% \begin{figure}[!htbp]
%     \centering
%     \includegraphics[width=\columnwidth]{irl_lb_plots/plots_32sector_sector4_new_training/agg_window_info/plot_mean_std_snip_len_1.pdf}
%     \caption{Plot of all the trajectories from the demonstration set of a single load balancing scenario. There exists a high variation in the rewards obtained by the states of a given trajectory with significant parts of the worst trajectory comprised of states (the crests) that are better than the states from the best trajectory (the troughs).}
%     \label{fig:trajectory-over-timesteps}
% \end{figure}

%This poses no issue for a task where the individual states from the good trajectory are on average better than their counterparts from a bad one. But, for load balancing, the rewards from any given trajectory can vary widely as shown in Figure \ref{fig:trajectory-over-timesteps}. 
%As a result, in a pair of ranked trajectories, randomly sampling states from the better trajectory no longer guarantees the generation of a sub-trajectory that is also better than its counterpart obtained from sampling the worse trajectory. To account for this, we propose an alternate method of trajectory sub-sampling specifically designed for the problem at hand. Along with consistency in the length of the sub-sample, we also mandate temporal consistency among the sub-trajectories being sampled from a given pair of trajectories. For a given pair, we sample $n$ time indexes at random, where $n$ ranges between $3-5$, and select the states corresponding to those time-indexes from both the trajectories in the pair. The motivation behind this stems from the observation that, at a given time-step the state from the better trajectory is consistently better than the state from a worse trajectory.


% \subsection{Policy learning using Proximal Policy Optimization (PPO)}
% Once the reward
% PPO falls under the umbrella of policy gradient methods. Policy gradient methods are a class of reinforcement learning algorithms that optimize a policy by calculating the gradient of the policy network using expected return. One of the main drawbacks of policy gradient methods is that it is susceptible to large updates in the policy which can lead to a deterioration in the performance, a phenomenon commonly termed as \textit{policy collapse}. PPO addresses this problem by clipping the gradient of the policy loss to prevent the method from taking arbitrarily large updates. 

% \begin{algorithm}[t]
% \caption{Load balancing using reward extrapolation in 5G communication networks. \textcolor{red}{Di: this might be heavy for the reviewers, can you also prepare a figure to showcase the overview of the proposed method} \textcolor{red}{Di: Also we should have a name for our solution.}}
% \label{alg:tcs-trex}

% \SetKwInOut{Input}{Input}
% \SetKwInOut{Output}{Output}

% \Input{Ranked expert demonstrations $D_E$ \newline
% Number of trajectory pairs to form $t_{full}$ \newline 
% Number of sub-trajectory pairs to form $t_{sub}$ \newline 
% Number of training iterations $N$ \newline
% %maximum iterations $M$\newline
% %training intervals  $i_{RL}$, $i_{IRL}$ %\newline
% % RL training interval $i_{RL}$ \newline
% % num. IRL expert and buffer samples $n_E$, $n_B$
% % num. IRL buffer samples $n_B$
% }
% \Output{Reward function $R_{\theta}$, Policy function $\pi_{\phi}$}

% \SetKwComment{Comment}{$\triangleright$\ }{}
% \SetKwFunction{Backprop}{Backprop}
% \SetKwFunction{SolveMDP}{SolveMDP}
% \SetKwFunction{GenerateTrainingSet}{MakeTrainingSet}
% \SetKwFunction{UpdatePolicy}{UpdatePolicy}
% \SetKwFunction{TrainRewardNetwork}{TrainRewardNetwork}


% \SetKwProg{dataaug}{Procedure}{}{}
% \dataaug{\GenerateTrainingSet{$D_E$, $t_{full}$, $t_{sub}$}}{
%     \For{$i=1$ \KwTo $t_{full}$}{
%         sample a pair of trajectories $\tau_{x}$ and $\tau_{y} \in D_E$ \;
        
%         $TS_{obs} \gets (\tau_x, \tau_{y})$ \;
%         $TS_{label} \gets [1/0]$ based on relative ranking of $\tau_x$ and $\tau_{y}$ \;
        
%      }  
    
%     \For{$i=1$ \KwTo $t_{sub}$}{
%         sample a pair of trajectories $\tau_{x}$ and $\tau_{y} \in D_E$ \; 
%         sample sub trajectory $\tau^{sub}_{x}$, $\tau^{sub}_{y}$ from $\tau_{x}, \tau_{y} $ using TCS\;
%         $TS_{obs} \gets (\tau^{sub}_{x}, \tau^{sub}_{y})$ \;
%         $TS_{label} \gets [1/0]$ based on relative ranking of $\tau_x$ and $\tau_{y}$ \;

%     }

% }
% %\setcounter{AlgoLine}{0}
% \SetKwProg{trex}{Procedure}{}{}
% \trex{\TrainRewardNetwork{TS}}{
%     \For{$i=1$ \KwTo $N$}{
%         \For{$(\tau_x, \tau_y) \in TS_{obs}$}{
%             $\mathcal{L}(\theta) = - \log \frac{\exp(R_{\theta}(\tau_x))}{\exp(R_{\theta}(\tau_x)) + \exp(R_{\theta}(\tau_y))}$ \;
%             $\theta \gets \theta - \alpha \nabla_{\theta}\mathcal{L}(\theta)$ \;
%             }
%     }   
% }
% \BlankLine
% \Begin{
% % \SetKwProg{main}{Algorithm}{}{}
% % \main{\SESNO{}}{
% % \setcounter{AlgoLine}{0}
% Initialize $R_{\theta}$ \;
% ($TS_{ob}$, $TS_{lbl}$) $\gets$ \GenerateTrainingSet{$D_E$, $t_{full}$, $t_{sub}$} \;
% $R_{\theta} \gets $\TrainRewardNetwork{} \; 
% $\pi_{\phi} \gets $ \SolveMDP{$R_{\theta}$} \;
% }
% \end{algorithm}


\section{Experiments and results}
\subsection{Simulation environment}
The experiments are conducted on a  system-level RAN  simulator\cite{slsstart,slsend} consisting of seven eNBs. Each base station consists of three sectors, which in turn comprises of four cells with different carrier frequencies. Six eNBs are arranged in a uniform hexagonal ring with one eNB at the center as shown in Figure \ref{fig:hex7-layout}. The UEs in the system are randomly distributed over the geographical area and are assumed to be points with fixed velocities and random directions drawn from a uniform distribution. We test the extrapolation performance of the reward network in two traffic scenarios and the load balancing performance of the policy network in four scenarios (ID 1-4). These scenarios are determined by different number of UEs, request packet size and interval distributions. 

\subsection{Collecting demonstrations}
For each scenario, we collect a set of $100$ trajectories using a random controller. A random controller randomly samples an action at each hour. In the absence of an external critic to rank the demonstrations, we resort to an ad-hoc ranking function, $R_{f}$, based on a weighted combination of a set of KPIs. We use the reward function from \cite{Li2022clusteringRL} as our ranking function. We would like to emphasise the fact that the use of ranked trajectories provides a significantly low resolution image of the reward landscape than using a reward function to learn a controller in an RL setting which needs access to the reward corresponding to each state it encounters. For a given pair of expert demonstrations $\tau_i$ and $\tau_j$, $\tau_i \prec \tau_j$ when $\sum_{s \in \tau_i} R_{f}(s) < \sum_{s \in \tau_j} R_{f}(s)$. From the set of $100$ trajectories, we select the worst $70\%$ to build our training set and leave the rest to test the extrapolation capabilities of the trained reward network.

%The KPIs include: minimum IP throughput (minIPTput), standard IP throughput(stdIPTput), ICU count and dead cell count. The KPIs are defined in Table \ref{tab:KPI-table}. The ranking function, $R_f$ is defined in Equation \ref{eqn:reward_func}.
% \begin{multline}
%     \label{eqn:reward_func}
%     R_{f} = 0.3 \times \texttt{minIpTput} + 0.2 \times \frac{1}{1 + \texttt{stdIpTput}} - \\
%      0.25 \times \texttt{ICUCount} - 0.25 \times \texttt{deadCellCount} 
% \end{multline}

% \begin{table}[tbh]
%     \centering
%     \begin{tabular}{|p{2.5cm}|p{5cm}|}
%     \hline
%     \textbf{KPI} & \textbf{Description} \\
%     \hline    
%     \hline
%     minIPTput & Minimum IP throughput (IPTput) per UE, per cell. \\
%     \hline
%     stdIPTput & Standard deviation of the IPTput per UE per cell. \\
%     \hline
%     ICU count & Number of cells with IPTput less than 1.\\
%     \hline
%     dead cell count & Number of cells with IPTput less than 0.5\\
%     \hline
%     \end{tabular}
%     \caption{Different KPIs used in the ranking function and their description.}
%     \label{tab:KPI-table}
% \end{table}



\subsection{Training details}
\label{subsection:training_details}
The reward network consists of 3 fully-connected layers with a leaky ReLU activation function \cite{nairRectifiedLinearUnits2010} for the input and the first hidden layer. Other hyperparameters of the training the reward and the policy networks are listed in Table \ref{tab:trex-parameters}. The training time for the reward network took on average roughly 45 hours on an NVIDIA A6000. The policy network is trained using PPO. Both the actor and value network consist of 3 fully-connected layers with $256$ neurons in each layer and use the tanh activation function. All the networks are optimized using Adam \cite{kingma2014adam}. Hyperparameters used for training the policy network are listed in Table \ref{tab:rl-parameters}.

\begin{table}[hbp]
\centering
% \begin{tabular}{ |p{3cm}||p{3cm}|p{3cm}|p{3cm}|}
    \begin{subtable}[b]{0.52\columnwidth}
        \begin{tabular}{|p{2.8cm}|l|}
        \hline
        Learning rate &  $1e-5$\\
        \hline
        Weight decay & $1e-4$ \\ 
        \hline
        No. of epochs &  $1200$ \\ 
        \hline
        No. trajectory pairs & $1000$ \\ 
        \hline
        No. sub-trajectory pairs & $50000$ \\
        \hline
        \end{tabular}
        \caption{Reward learning.}
        \label{tab:trex-parameters}

    \end{subtable} %
    \begin{subtable}[b]{0.46\columnwidth}
        \begin{tabular}{|c|c|}
        \hline
        Learning rate &  $0.0003$\\ 
        \hline
        Weight decay & $1e-4$ \\ 
        \hline
        Total timesteps & $200k$\\
        \hline
        Gamma & $0.97$ \\
        \hline
        Clip range & $0.15$ \\
        \hline
        Batch size & 64\\
        \hline
        \end{tabular}
        \caption{Policy learning.}
        \label{tab:rl-parameters}

    \end{subtable}
\caption{Training hyperparameters for different parts of the learning pipeline.}
\end{table}

\begin{table}[hb]
    \centering
    %\begin{tabular}{|p{2cm}|p{1.5cm}|p{1cm}|p{1.5cm}|p{1cm}|p{1.5cm}|p{1cm}|p{1.5cm}|p{1cm}|}
    \begin{tabular}{|p{0.5cm}|p{3.5cm}|p{1.1cm}|p{2cm}|}
    \hline 
    \textbf{ID} & \textbf{Method} & Training & Extrapolation \\
    \hline
    \multirow{2}{*}{1} & T-REX (Original) &  \textbf{0.98} & $0.83$ \\
    &  T-REX + TCS(ours)  & $0.94$  & \textbf{0.94} \\
    \hline
    \hline
    \multirow{2}{*}{4} & T-REX (Original)  & \textbf{0.99} & 0.53 \\
    &  T-REX + TCS(ours)   & 0.98  & \textbf{0.93} \\
    \hline
   \end{tabular}
    \caption{Pearson correlation on the training and extrapolation set for different sub-trajectory sampling techniques. Maintaining temporal consistency between candidate sub-trajectories during sampling from a given pair of trajectories consistently outperforms its counterpart across different scenarios. For reference: a higher value is better.}
    \label{tab:pcorr-table}
\end{table}


% \begin{table*}[t]
%     \centering
%     %\begin{tabular}{|p{2cm}|p{1.5cm}|p{1cm}|p{1.5cm}|p{1cm}|p{1.5cm}|p{1cm}|p{1.5cm}|p{1cm}|}
%     \begin{tabular}{|p{2cm}|p{1.5cm}|p{1cm}|p{1.5cm}|p{1cm}|p{1.5cm}|p{1cm}|p{1.5cm}|p{1cm}|}
%     \hline 
%     \multirow{}{}{} & \multicolumn{4}{c|}{Scenario 4}& \multicolumn{4}{c|}{Scenario 23} \\
%     \hline 
%      & \multicolumn{2}{c|}{Training set} & \multicolumn{2}{c|}{Extrapolation (test) set} & \multicolumn{2}{c|}{Training set} & \multicolumn{2}{c|}{Extrapolation (test) set}  \\
%     \cline{2-9}
%         & Trajectory space & State space & Trajectory space & State space & Trajectory space & State space & Trajectory space & State space \\
%     \hline
%     Original (T-REX) &  \textbf{0.98} & $0.16$ & $0.83$ & $-0.63$  & \textbf{0.99} & 0.38 & 0.53 & -0.47\\
%     \hline
%     Ours  & $0.94$ & \textbf{0.48} & \textbf{0.94} & \textbf{-0.46} & 0.98 & \textbf{0.5} & \textbf{0.93} & \textbf{-0.33}\\
%     \hline
%     \end{tabular}
%     \caption{\textcolor{red}{can you try to remove this table}Pearson correlation values on the training and extrapolation set for different sub-trajectory sampling techniques. Maintaining a temporal consistency between candidate sub-trajectories during sampling from a given pair of trajectories consistently outperforms its counterpart across different scenarios. For reference: a higher value is better.}
%     \label{tab:pcorr-table}
% \end{table*}

\subsection{Results: Reward extrapolation}
\label{subsection:reward_extrapolation}
To test the performance of the algorithm, we use the Pearson correlation coefficient~\cite{wikipedia-contributors-2022}. It calculates the linear relationship between two datasets and outputs a value in the range of $[-1,$ $1]$ which is proportional to their correlation. Table \ref{tab:pcorr-table} shows the correlation values obtained between the ranking reward function and the reward predicted by the trained reward network. From Table \ref{tab:pcorr-table}, we see that while the original method (T-REX) outperforms in the training set, using TCS shows consistent improvement in the extrapolation or test set across different scenarios. This indicates that the model trained from samples generated from the original sampling technique learns a reward function that lacks generalization. A possible explanation for this stems from the empirical observations that the probability of mislabeling a sub-sample pair using TCS drops from $0.35$ to $0.13$, helping the model better capture the latent ranking reward and thus contributing to better extrapolation. A qualitative overview of the extrapolation performance is shown in Figure \ref{fig:pcorr-sector23}.

% The formula for the coefficient, $\rho_{XY}$ between two datasets $X$ and $Y$ is given in Equation \ref{eqn:pearson-corr}

% \begin{equation}
%     \label{eqn:pearson-corr}
%     \rho_{X,Y} = \frac{\sum_{i=1}^{n} (x_i-\bar{x})(y_i- \bar{y})}{\sqrt{\sum_{i=1}^{n} (x - \bar{x})^2} \sqrt{\sum_{i=1}^{n} (y-\bar{y})^2)}}
% \end{equation}
% where, $\bar{x}$ and $\bar{y}$ are the means of $X$ and $Y$ respectively.\\

% \begin{figure}[!htbp]
%     \centering
%     \begin{subfigure}{\columnwidth}
%         \includegraphics[width=\textwidth]{irl_lb_plots/plots_32sector_sector4_old_training/gt_predicted_reward_correlation.pdf}
%         \caption{Sector 4: Without temporal consistency.}
%     \end{subfigure}
%     \begin{subfigure}{\columnwidth}
%       \includegraphics[width=\textwidth]{irl_lb_plots/plots_32sector_sector4_new_training/gt_predicted_reward_correlation.pdf}
%       \caption{Sector 4: With temporal consistency.}
%     \end{subfigure}
%     \caption{Scatter plots qualitatively showing the correlation between the predicted and ground truth reward for the demonstration trajectories in the training and test (extrapolation) set for sector 4.}
%     \label{fig:pcorr-sector4}
% \end{figure}

\begin{figure}[!htp]
    \centering
    \begin{subfigure}[t]{0.49\columnwidth}
        \includegraphics[width=\textwidth]{irl_lb_plots/plots_32sector_sector23_old_training/gt_predicted_reward_correlation.pdf}
        \subcaption{Without TCS.}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\columnwidth}
        \includegraphics[width=\textwidth]{irl_lb_plots/plots_32sector_sector23_new_training/gt_predicted_reward_correlation.pdf}
        \caption{With TCS.}
    \end{subfigure}
    \caption{Scatter plots qualitatively showing the correlation between the predicted and ad-hoc reward for the demonstration trajectories in the training and test (extrapolation) set.}
    \label{fig:pcorr-sector23}
\end{figure}

\subsection{Results: Model performance}
We compare the load balancing performance of the trained policy with four baselines, i.e., demonstrations, fixed rule-based method, adaptive rule-based method and the original TREX method. Demonstrations are the set of trajectories generated from the random controller on which the reward network was trained. Fixed rule-based method is a rule-based method consisting of a set of fixed IULB and MLB parameters as used in \cite{wu2021dataefficientRL}. Adaptive Rule-based method is a rule-based load balancing algorithm \cite{YangAdaptiveRule2012} that dynamically adjusts the IULB and MLB parameters according to the difference in the loads between neighboring cells. We do not include RL based methods in the comparison because these methods usually require access to rewards from individual states as compared to pairwise trajectory ranks utilized by our method. Pairwise ranking on a trajectory level is far more accessible as compared to the former, especially in the real world, and in this work we focus on methods that can be trained and deployed in such conditions.

%We argue that having access to pairwise ranking of trajectories provides a very low resolution image of the reward landscape as compared to access to rewards from individual states and comparing the two methods would 

%The parameters have been fine-tuned for optimal performance.
% \begin{table}[]
%     \centering
%     \begin{tabular}{|c|c|c|}
%         \hline
%          & Parameter & Value \\
%          \hline
%          \multirow{4}{*}{IMLB} & CCCV 2300 A & 75\\
%          & CCCV 2300 B & 37\\
%          & CCCV 1800 & 25 \\
%          & CCCV 850 & 25 \\
%         \hline
%         \multirow{3}{*}{MLB} & intraGroupOffloadThForIpTput(x10) & 15\\
%          & deltaOffloadThForIpTput (x10) & 5 \\
%          & rateLBTarget (x100) & 5\\
%          \hline
%     \end{tabular}
%     \caption{IMLB and MLB event parameters for Standard LB baseline.}
%     \label{tab:default-imlb-mlb-values}
% \end{table}


We evaluate the performance of the  methods on a single sector of the central base station on the three metrics introduced in Section \ref{subsection:kpis}.
Table \ref{tab:kpi-performance-table} reports these metrics averaged over the entire sequence of a given scenario at each hour.
%%%%%THE OLD TABLE%%%%%%
% \begin{table*}[!h]
%     \centering
%     \begin{tabular}{|c|c|c|c|c|c|c|}
%         \hline
%         \multirow{2}{*}{Method} & \multicolumn{3}{c|}{Sector 4} & \multicolumn{3}{c|}{Sector 23} \\
%         \cline{2-7}
%          & min IP Tput & std IP Tput & GT reward & min IP Tput & std IP Tput & GT reward\\ 
%         \hline
%         Demonstrations & $1.45\pm0.13$ & $1.74\pm0.28$ & $0.26\pm0.10$ & $2.09\pm0.15$ & $2.61\pm0.29$ & $0.64\pm0.07$\\ 
%         Fixed rule-based & $1.50\pm0.01$ & $1.51\pm0.02$ & $0.35\pm0.01$ & $2.05\pm0.01$ & $2.58\pm0.02$ & $0.66\pm0.01$\\ 
%         Adaptive rule-based & $1.44\pm0.07$ & $1.89\pm0.10$ & $0.22\pm0.05$ & $2.27\pm0.07$ & $2.41\pm0.12$ & $0.70\pm0.03$\\ 
%         Temporal TREX(ours) & \bm{$1.97\pm0.02$} & \bm{$1.07\pm0.01$} & \bm{$0.64\pm0.01$} & \bm{$2.77\pm0.01$} & \bm{$1.73\pm0.02$} & \bm{$0.90\pm0.01$}\\ 
%         \hline
%     \end{tabular}
%     \caption{Evaluation of all the competing methods on different network traffic scenarios. For min IP Tput and GT reward, a higher value translates to a better performance and for std IP Tput, lower is better. Our method enjoys an an average improvement of $26.6\%$, $28.6\%$ and $55.71\%$ percent in min IP Tput, std IP Tput and GT reward over the next best competitor respectively across different scenarios. }
%     \label{tab:kpi-performance-table}
% \end{table*}

\begin{table}[!h]\small
    \centering
    \def\arraystretch{1}
    \setlength{\tabcolsep}{0.2em}
    \begin{tabular}{|P{1cm}|P{2cm}|P{1.65cm}|P{1.65cm}|P{1.64cm}|}
        \hline
         \textbf{ID} & \textbf{Method} & $\mathbf{T}_{min}$ & $\mathbf{T}_{std}$& $\mathbf{T}_{cc}$\\
         \cline{2-5}
         \hline
         \multirow{4}{*}{1} & Ours & \bm{$2.69\pm0.01$} & \bm{$1.46\pm0.02$} & \bm{$0.00\pm0.00$} \\%     & $0.90\pm0.01$\\ 
         & Fixed rule & $1.60\pm0.01$ & $2.55\pm0.02$ & $0.09\pm0.01$\\%     & $0.39\pm0.01$\\ 
         & Demonstrations & $1.89\pm0.19$ & $2.29\pm0.29$ & $0.05\pm0.07$ \\%     & $0.52\pm0.11$\\ 
         & Adaptive rule & $2.15\pm0.05$ & $2.05\pm0.05$ & $0.03\pm0.01$\\%     & $0.66\pm0.02$\\ 
         & TREX(Original) & $2.51\pm0.02$ & $1.66\pm0.02$ & \bm{$0.00\pm0.00$} \\
        \hline
        \hline
        \multirow{4}{*}{2} & Ours & \bm{$1.97\pm0.02$} & \bm{$1.07\pm0.01$} & \bm{$0.02\pm0.01$}\\%     & $0.64\pm0.01$\\ 
        & Fixed rule & $1.50\pm0.01$ & $1.51\pm0.02$ & $0.03\pm0.01$ \\%     & $0.35\pm0.0$\\ 
        & Demonstrations & $1.45\pm0.13$ & $1.74\pm0.28$ & $0.25\pm0.19$\\%     & $0.26\pm0.1$\\ 
        & Adaptive rule & $1.44\pm0.07$ & $1.89\pm0.10$ & $0.32\pm0.07$\\%     & $0.22\pm0.1$\\ 
        & TREX(Original) & $1.68\pm0.02$ & $1.34\pm0.01$ & \bm{$0.02\pm0.01$} \\
        \hline 
        \hline
        \multirow{4}{*}{3} & Ours & \bm{$1.63\pm0.02$} & \bm{$1.63\pm0.04$} & \bm{$0.20\pm0.02$} \\%    & $0.31\pm0.02$\\ 
        & Fixed rule & $1.47\pm0.02$ & $2.31\pm0.04$ & $0.31\pm0.02$\\%    & $0.22\pm0.0$\\
        & Demonstrations & $1.54\pm0.08$ & $2.07\pm0.15$ & $0.30\pm0.10$ \\%    & $0.23\pm0.1$\\ 
        & Adaptive rule & $1.57\pm0.04$ & $2.06\pm0.06$ & $0.28\pm0.03$\\%    & $0.22\pm0.0$\\ 
        & TREX(Original) & $1.62\pm0.02$ & $1.74\pm0.04$ & $0.21\pm0.02$ \\
        \hline
        \hline
        \multirow{4}{*}{4} & Ours & \bm{$2.77\pm0.01$} & \bm{$1.73\pm0.02$} & $0.01\pm0.00$\\%    & $0.90\pm0.01$\\ 
        & Fixed rule & $2.05\pm0.01$ & $2.58\pm0.02$ & \bm{$0.00\pm0.00$}\\%     & $0.66\pm0.0$\\ 
        & Demonstrations & $2.09\pm0.15$ & $2.61\pm0.29$ & $0.02\pm0.03$\\ %     & $0.64\pm0.1$\\ 
        & Adaptive rule & $2.27\pm0.07$ & $2.41\pm0.12$ & $0.02\pm0.01$\\%     & $0.70\pm0.0$\\ 
        & TREX(Original) & $2.38\pm0.01$ & $2.14\pm0.02$ & $0.01\pm0.00$ \\
        \hline
    \end{tabular}
    \caption{Evaluation results on different traffic scenarios. For $T_{min}$, a higher value translates to a better performance and for $T_{std}$ and $T_{cc}$, lower is better. Our method enjoys an an average improvement of $19.6\%$, $26.7\%$ and $32.3\%$in $T_{min}$, $T_{std}$ and $T_{cc}$ respectively over the next best method.}
    \label{tab:kpi-performance-table}
\end{table}

Table \ref{tab:kpi-performance-table}, shows that our proposed method consistently outperforms the other methods across different network scenarios. For $T_{min}$, we achieve on average an improvement of $10.3\%$ over the second best performing method across different scenarios. Figure \ref{fig:ts-miniptput} shows that our method is able to obtain significantly higher $T_{min}$ values at times of moderately high traffic. At times when the network traffic is low, the performance of all the methods is comparable due to the lack of any room for further optimization to wring out more from the existing network infrastructure. Our method also attains a substantial reduction in $T_{std}$. It outperforms its nearest competitor by approximately $14.4\%$ averaged across all the scenarios, and it shows consistent improvement across both periods of high and low traffic as seen from Figure \ref{fig:ts-stdiptput}.




%From figure \ref{fig:policy-performance-sec4} and \ref{fig:policy-performance-sec23} we that the trained policy not only outperforms the original demonstrator but also outperforms standard load balancing baselines by a heavy margin across all the scenarios. 



\begin{figure}[!htbp]
    \centering
    \begin{subfigure}{\columnwidth}
        \centering        

        \includegraphics[width=1\columnwidth]{irl_lb_plots/timeseries_minIptput_sec4.pdf}
        \caption{Minimum IP throughput obtained by different methods over the period of one week. From the trends in the minimum IP throughput, the timeline of each week can be divided into low and high traffic intervals. The line plots show that our proposed method better capitalizes on the network resources during moderately low network traffic hours when compared to the other baselines improving the minimum IP throughput in the network.}
        \label{fig:ts-miniptput}
    \end{subfigure}
    \begin{subfigure}{\columnwidth}
    \centering
        \setlength{\belowcaptionskip}{-5pt}
        \includegraphics[width=1\columnwidth]{irl_lb_plots/timeseries_stdIptput_sec4.pdf}
        \caption{Standard deviation in IP throughput over time. The plot shows that our method consistently maintains a lower standard deviation across the entire week.}
        \label{fig:ts-stdiptput}
    \end{subfigure}
    \caption{Performance of competing methods in different network KPIs.}
    \label{fig:ts-kpi}
\end{figure}


% \begin{figure}[!htbp]{\columnwidth}
%     \centering
%     \begin{subfigure}{\columnwidth}
%         \includegraphics[width=\columnwidth]{irl_lb_plots/timeseries_stdIptput_sec4.pdf}
%         \caption{Sector 4}
%     \end{subfigure}
%     \begin{subfigure}{\columnwidth}
%         \includegraphics[width=\columnwidth]{irl_lb_plots/timeseries_stdIptput_sec23.pdf}
%         \caption{Sector 23}
%     \end{subfigure}
%     \caption{Big caption.}
%     \label{fig:ts-stdiptput}
% \end{figure}
\section{Conclusion and Future work}
%In this work, we explore the application of inverse reinforcement learning in load balancing for a 5G communication network: a real-world task with a hard-to-define reward function. 
%We present a learning framework, which successfully infers and extrapolates an underlying reward function using a neural network. We use this reward network, in a downstream task, to train a policy that not only significantly outperforms the demonstrations but also other load balancing algorithms standard in the community.\\
%In the future, we aim to conduct more rigorous tests across a wide variety of network traffic scenarios. Exploring different ways to reformulate the reward learning problem to take into account the wide distribution of rewards obtained from states of a single trajectory is also something we plan to investigate in the future.

With the rapid increase and uneven distribution of communication traffic, communication load balancing has become a pressing problem in maintaining the quality of experience for customers. In this work, we showcase the first attempt to use inverse reinforcement learning for communication load balancing, bypassing the need for an explicitly defined reward signal. We can learn a reward function from a collection of system demonstrations and then utilize that to train a reinforcement learning-based load balancing control policy. Experimental results on different traffic scenarios have showcased the the proposed solution can help significantly improve the system performance. We believe that this work has showcased the effectiveness of inverse reinforcement learning and provides a new direction for future load balancing research.  In the future, we plan to further improve the data efficiency of the proposed solution and investigate the applicability of the proposed solution to other networking optimization problems, e.g., energy saving, network slicing.

\bibliographystyle{IEEEtran}
\bibliography{references}
\end{document}
