% \let\hl=\undefined
% \newcommand\hl[1]{\textcolor{blue}{(#1)}}

% \smallskip\noindent\textbf{Cluster-based AL for classification.}
% Active learning for classification employs clustering to ensure the diversity in the selected samples, which helps to prevent redundancy in annotations \cite{nguyen2004active, hong2015clustering}.
% As active learning begins with an unlabeled dataset, we first generate an embedding feature space for the unlabeled samples through representation learning.
% Clustering is then progressed to select the most typical samples with the shortest distance to neighboring samples for each cluster \cite{hacohen2022active}.
% To consider both uncertainty and diversity, the samples with high uncertainty are initially selected, followed by a random selection from the cluster generated through hierarchical agglomerative clustering, to which the high uncertainty samples belong \cite{citovsky2021batch}.
% Similar to the clustering approaches, for segmentation models in active learning, superpixel algorithms that locally cluster adjacent pixels together are recently used.
% However, differently from the previous works, ...

%\smallskip\noindent\textbf{Image-level, dense pixel-wise annotation} Active learning aims to produce the most informative subset of labels to help model training. One way is to serve labels in pixel-wise granularity within the entire image unit \cite{yang2017suggestive,xie2020deal,desai2022active,sinha2019variational,chengliang2020suggestive}. Suggestive annotation (SA) proposed in \cite{yang2017suggestive} samples highly uncertain and representative images, and queries dense pixel annotations for them. It estimates uncertainty by bootstrapping, and representativeness by similarity measure. However, bootstrapping depends on training a model on multiple subsets of dataset, which incurs computational burden. Moreover, finding most representative images, modeled by max $k$-cover problem, is NP-hard, and even its approximation is still time-consuming. Therefore, it is hard to extend SA for large-scale dataset. \cite{chengliang2020suggestive} gradient-guided... \cite{sinha2019variational} ... Differently, \cite{xie2020deal} proposed new way of measuring uncertainty and entropy scores based on the learned difficulty estimator for sample acquisition. To this end, it introduced semantic difficulty network that learns proportion of misclassified pixels in image. However, those image-level sampling approaches have common pitfalls. By aggregating acquisition heuristics (e.g., uncertainty, entropy, difficulty, etc.) into image level, the disparity in each image can be ignored when the images are ranked by acquisition scores \cite{mackowiak2018cereals}. For example, different semantic areas should have different semantic difficulty, but the resulting acquisition score is based on the aggregated one, and thereby makes it hard to capture the difference between semantic areas. Also, annotation efforts can be wasted when uncertainty score is involved, since models tend to confident on large areas in the images (cite?). That is, even if a model showed highest uncertainty on some image, most of the large areas have low uncertainty and they don't need to be annotated. Finally, even with active learning, image-level annotation is still cost-expensive and is apart from the realistic scenario, since annotators should classify each of the pixels in the image.

% \iffalse
% \smallskip\noindent\textbf{Superpixel-based active learning.}
% Researchers start the active learning for segmentation models with an image or square unit samples, where acquisition scores of the samples are aggregated per-pixel scores \cite{yang2017suggestive,xie2020deal,sinha2019variational, qiao2022cpral,mackowiak2018cereals,golestaneh2020importance,casanova2019reinforced}.
% However, the high cost of annotating each pixel leads to the introduction of superpixels, which clusters adjacent pixels with similar perceptual pixels into a superpixel, significantly reducing annotation costs \cite{siddiqui2020viewal, mackowiak2018cereals,casanova2019reinforced}.
% The dominant label of a superpixel is propagated to its constituent pixels and the class imbalance of the selected samples is addressed by class-balanced sampling \cite{cai2021revisiting} or regional Gaussian attention \cite{qiao2022cpral}.

% (Cluster-based al for classification, Active learning for classification employs clustering to ensure the diversity in the selected samples, which helps to prevent redundancy in annotations \cite{nguyen2004active, hong2015clustering}.
% As active learning begins with an unlabeled dataset, we first generate an embedding feature space for the unlabeled samples through representation learning.
% Clustering is then progressed to select the most typical samples with the shortest distance to neighboring samples for each cluster \cite{hacohen2022active}.
% To consider both uncertainty and diversity, the samples with high uncertainty are initially selected, followed by a random selection from the cluster generated through hierarchical agglomerative clustering, to which the high uncertainty samples belong \cite{citovsky2021batch}.
% Similar to the clustering approaches, for segmentation models in active learning, superpixel algorithms that locally cluster adjacent pixels together are recently used.
% However, differently from the previous works, ...)

% However, clustering is only utilized for the initial superpixels, and as the round progresses, the initial pool remains fixed and no further clustering takes place.
% Our work suggests additional clustering, i.e., merging adjacent superpixels every round, to adaptively change the pool.
% \fi

\section{Related work}


% =====================================================
% 
% + Labeling unit in Active learning for semantic segmentation
%   - “whole image sampling”
%   - “Region-based method”: data variability 높임
%   - "Patch-based method"
%   - "Suerpixel-based method"
% + 제안된 방법은 이전 웍들과 달리 사전에 계산된 superpixel 에 의존적이지 않고, 모델이 학습함에 따라 superpixel 을 적절히 merging 함으로써 annotation efficiency 를 증가시킨다.
% + Acquisition function
% + Clustering-based Al for classification 과의 유사점
% ===================================================== 
%\hsh{Conventional AL for segmentation approaches  }
% \ok{Need to start with high annotation cost for segmentation.}

\noindent\textbf{Active learning for segmentation.}
To reduce the labeling cost of semantic segmentation, active learning for segmentation selectively collects labels among unlabeled samples, and they utilize different predefined labeling units.
% To reduce the labeling cost of semantic segmentation, AL for segmentation selectively collects labels among unlabeled samples.
% They utilize different predefined labeling units.
Early approaches~\cite{sinha2019variational,yang2017suggestive} perform image-wise selection and mask labeling.
% Early approaches~\cite{yang2017suggestive,sinha2019variational} perform image-wise selection and obtain a ground-truth mask for a whole image.
% Point-based approach~\cite{shin2021labor} tries to select the most informative points among all pixels and assign a class label to the selected one.
% Patch-based methods~\cite{casanova2019reinforced,golestaneh2020importance,xie2022towards} divide images into rectangular patches and annotate pixels within the patch.
Patch-based methods~\cite{casanova2019reinforced,colling2020metabox+,golestaneh2020importance,mackowiak2018cereals,xie2022towards} divide images into rectangular patches and provide mask label~\cite{casanova2019reinforced,golestaneh2020importance,xie2022towards} or polygon overlay of an object~\cite{colling2020metabox+,mackowiak2018cereals} within the selected patch.
% Furthermore, polygon-based approaches~\cite{colling2020metabox+,mackowiak2018cereals} obtain a class corresponding to the patch and intersections with object boundaries, rather than pixel-wise annotations.
% Superpixel-based approaches~\cite{colling2020metabox+, cai2021revisiting, siddiqui2020viewal} divide the image through an over-segmentation algorithm and utilize each divided superpixel as a labeling unit.
Recently, superpixel-based approaches~\cite{cai2021revisiting, siddiqui2020viewal} split images to perceptually meaningful regions called superpixel by running an off-the-shelf over-segmentation algorithm~\cite{achanta2012slic, ren2003learning, van2012seeds}.
Each superpixel is labeled with a single dominant class, and thus it can be obtained efficiently~\cite{cai2021revisiting}, while a label noise may occur depending on the quality of the superpixel.
We present a new efficient labeling unit, that is initialized with the superpixel but its quality continuously improves by the proposed merging algorithm.
To the best of our knowledge, the proposed method is the first approach to improve the labeling units during active learning for segmentation.
% However, the high cost of annotating each pixel leads to the introduction of superpixels, which clusters adjacent pixels with similar perceptual pixels into a superpixel, significantly reducing annotation costs \cite{siddiqui2020viewal, mackowiak2018cereals,casanova2019reinforced}.
% The dominant label of a superpixel is propagated to its constituent pixels and the class imbalance of the selected samples is addressed by class-balanced sampling \cite{cai2021revisiting} or regional Gaussian attention \cite{qiao2022cpral}.

% \hsh{[TEMP] Cluster-based AL~\cite{nguyen2004active, hong2015clustering} is close to ours in spirit, as both of them employ clustering for labeling efficiency.}
% Clustering to ensure the diversity in the selected samples
% Superpixel algorithms locally cluster adjacent pixels together are recently used.
% Moreover, we suggest adaptive clustering, i.e., merging adjacent superpixels for every round, to reflect the model update.
% Similar to the clustering approaches, for segmentation models in active learning, superpixel algorithms that locally cluster adjacent pixels together are recently used.
% However, clustering is only utilized for the initial superpixels, and as the round progresses, the initial pool remains fixed and no further clustering takes place.
% Our work suggests additional clustering, i.e., merging adjacent superpixels every round, to adaptively change the pool.

% \khy{adaptive clustering}
% (Cluster-based al for classification, Active learning for classification employs clustering to ensure the diversity in the selected samples, which helps to prevent redundancy in annotations \cite{nguyen2004active, hong2015clustering}.
% As active learning begins with an unlabeled dataset, we first generate an embedding feature space for the unlabeled samples through representation learning.
% Clustering is then progressed to select the most typical samples with the shortest distance to neighboring samples for each cluster \cite{hacohen2022active}.
% To consider both uncertainty and diversity, the samples with high uncertainty are initially selected, followed by a random selection from the cluster generated through hierarchical agglomerative clustering, to which the high uncertainty samples belong \cite{citovsky2021batch}.
% Similar to the clustering approaches, for segmentation models in active learning, superpixel algorithms that locally cluster adjacent pixels together are recently used.
% However, differently from the previous works, ...)

% However, clustering is only utilized for the initial superpixels, and as the round progresses, the initial pool remains fixed and no further clustering takes place.
% Our work suggests additional clustering, i.e., merging adjacent superpixels every round, to adaptively change the pool.

%\vspace{-3mm}
% \iffalse
% \smallskip\noindent\textbf{Learning from noisy labels for segmentation.}
% Recent studies address the issue of annotation noise in segmentation tasks, as noise is inevitable due to the nature of hand-crafted \cite{yaolearning, li2021superpixel, zhang2020robust}.
% One approach captures the timing when different classes memorize noisy labels, and uses this information to adaptively correct noisy labels \cite{liu2022adaptive}.
% Another method is to introduce a boundary-thinning layer with a corresponding loss, as noisy annotations often occur at the boundaries of objects \cite{acuna2019devil}.
% In order to train a model that is robust to noisy labels, researches propose the use of noise-aware loss \cite{oh2021background} and different loss functions for background and foreground instances \cite{yang2020learning}.
% However, we handle noisy labels caused by superpixels in active learning situations where only a few labels exist. 
% \fi

\smallskip
\noindent\textbf{Learning from noisy labels for segmentation.}
Considering the difficulty in acquiring high-quality labels~\cite{Cordts2016Cityscapes}, semantic segmentation often suffers from noisy annotations.
Previous studies address the label noise by using
gradient similarity to the clean label~\cite{yaolearning},
structural constraints~\cite{acuna2019devil,li2021superpixel}, and
noise-aware loss~\cite{oh2021background, yang2020learning}.
A recent approach captures the moment when different classes memorize noisy labels~\cite{liu2022adaptive}.
Most of these methods~\cite{li2021superpixel, oh2021background, yang2020learning} utilize a single confidence threshold to detect label noise within data.
Unlike previous approaches, we propose to detect an adaptive confidence threshold 
per every superpixel queried,
using the Kneedle algorithm~\cite{satopaa2011finding} (Section~\ref{sec:sieving-technique}).
Filtering with the sample-adaptive threshold prevents superpixels with low overall confidence or superpixels containing minor classes from being ignored.
% , we introduce  to detect superpixel-adaptive confidence threshold, and sieve noisy region within the superpixel.
% We propose the sieving algorithm that introduces kneedle algorithm~\cite{satopaa2011finding} to detect}
% However, we handle noisy labels caused by superpixels in active learning situations where only a few labels exist. 

% Meanwhile, \cite{xie2022towards} noticed that dividing each image into non-overlapping regions (either rectangles or superpixels) is not suitable for region-based query strategy, and introduced $k$-square-neighbors of each pixel as a region. Also, it proposed a sampling strategy including region impurity as well as traditional uncertainty. Region impurity was designed to prefer the regions composed of more diverse semantic classes. Therefore, most diverse and uncertain regions are selected and queried. ...impurity and class-balancing? However, ...

% Using superpixels as regional units \cite{siddiqui2020viewal,cai2021revisiting} is another alternative to defeat the limitations of rectangle-based querying frameworks. Since superpixels do better capture the boundary of each semantic areas in image,  
% Instead of using regularly shaped image patches \cite{mackowiak2018cereals,casanova2019reinforced,qiao2022cpral}, other works \cite{siddiqui2020viewal,qiao2022cpral} adopted irregularly shaped regions (e.g., superpixels). \cite{qiao2022cpral} ... \cite{siddiqui2020viewal} require different views of each images (not available for every datasets). 

% \cite{qiao2022cpral,mackowiak2018cereals,golestaneh2020importance,casanova2019reinforced}.
% In \cite{colling2020metabox+}, for example, each image is partitioned with multiple boxes of fixed size, and the score for each box is obtained by simply summing up the pixel-wise acquisition scores.
% Since rectangles are likely to contain different semantic areas, multiple pixels, or even every pixels in the regions, are required for annotations, thereby increasing annotation costs.

% \cite{qiao2022cpral,mackowiak2018cereals,golestaneh2020importance,casanova2019reinforced}.
% In \cite{colling2020metabox+}, for example, each image is partitioned with multiple boxes of fixed size, and the score for each box is obtained by simply summing up the pixel-wise acquisition scores.
% Since rectangles are likely to contain different semantic areas, multiple pixels, or even every pixels in the regions, are required for annotations, thereby increasing annotation costs.
% However, different regions in each image has different effects for model training, and the disparity can be easily ignored with image-level queries.
% Previous works \cite{qiao2022cpral,mackowiak2018cereals,golestaneh2020importance,casanova2019reinforced} have demonstrated benefits of querying samples in rectangular units. 
% Therefore, using smaller queries in rectangular units is one way of considering multiple different semantic regions more effectively. 
% In \cite{colling2020metabox+}, for example, each image is partitioned with multiple boxes of fixed size, and the score for each box is obtained by simply summing up the pixel-wise acquisition scores. 

% In \cite{mackowiak2018cereals}, multiple region proposals are created with Non-Maximum Suppression~(NMS) on the aggregated region score map. 
% \hl{Also, to form score map, annotation cost for each pixel is directly estimated and blended with Vote Entropy for minimal annotation burden. However, training the cost model is hard to achieve because too few datasets provide detailed information on human annotations, thereby limiting the extensibility of the proposed method.}
% In \cite{casanova2019reinforced}, the modified deep Q-network~(DQN) was trained on the pre-partitioned set of regions, to directly learn acquisition scores of the regions. 
% \hl{However, using subset of labeled data pool to train DQN at each cycle is often irrealizable due to the limited budgets.} 
% Besides the aforementioned issues, rectangle-based active learning methods share common pitfalls. Since rectangles are likely to contain different semantic areas, multiple pixels, or even every pixels in the regions, are required for annotations, thereby increasing annotation costs.

%Instead of using regularly shaped image patches \cite{mackowiak2018cereals,casanova2019reinforced,qiao2022cpral}, other works \cite{siddiqui2020viewal,qiao2022cpral} adopted irregularly shaped regions (e.g., superpixels). \cite{qiao2022cpral} ... \cite{siddiqui2020viewal} require different views of each images (not available for every datasets). Meanwhile, \cite{xie2022towards} noticed that dividing each image into non-overlapping regions (either rectangles or superpixels) is not suitable for region-based query strategy, and introduced $k$-square-neighbors of each pixel as a region. Also, it proposed a sampling strategy including region impurity as well as traditional uncertainty. Region impurity was designed to prefer the regions composed of more diverse semantic classes. Therefore, most diverse and uncertain regions are selected and queried. ...impurity and class-balancing? However, ... [Robustness]

% There are lots of works regarding how to divide an image into non-overlapping superpixels aligned with the approximate semantic boundaries. One family of methods represents each pixels with nodes and the similarities between neighbors with edges, and then gradually adds multiple cuts to find boundary-preserving partition \cite{jianbo2000normalized,veksler2010superpixels,zhang2011superpixels,liu2011entropy}.
% Taking all nodes in consideration and normalizing the cut costs, \cite{jianbo2000normalized} solved the issue that cuts in the smaller set of nodes are preferred.
% Differently, superpixels can be formed from the initial set of points, each growing towards homogeneous cluster \cite{levinshtein2009turbopixels,achanta2012slic}. In \cite{levinshtein2009turbopixels}, superpixel generation was formulated by geometric-flow problem.
% ---
% \emph{However, finding a minimum cut can sometimes produce bad partitioning on graph, since such criteria prefers isolating small set of nodes (\hl{over-partitioning}). Therefore, \cite{jianbo2000normalized} introduced normalized-cut as an alternative measure of disassociation, thereby producing better grouping of nodes.}
% ---

%\vspace{-3mm}
\smallskip\noindent\textbf{
Superpixel mechanisms and their evaluation metric.}
Numerous studies segment an image into superpixels to reduce the computation burden of pixels.
Cut-based approaches~\cite{liu2011entropy,jianbo2000normalized,veksler2010superpixels,zhang2011superpixels} create superpixels by adding multiple minimum cuts into a graph with pixel nodes.
Other methods evolve homogeneous clusters from the initial set of points~\cite{achanta2012slic,levinshtein2009turbopixels}.
For real-time applications, a simple hill-climbing optimization is utilized to enforce color similarity~\cite{van2012seeds}.
Most of methods aim at generating superpixels of predefined size or shape,
and 
the generated superpixels are evaluated by achievable segmentation accuracy and boundary recall compared with ground truth~\cite{liu2011entropy, van2012seeds} or by examining the regularity in superpixel shape~\cite{giraud2017robust,machairas2014waterpixels,schick2012measuring}.
To save labeling costs in active learning, 
it is more important to obtain superpixels as close to the ground-truth segments as possible
without such constraints on the shape or size of superpixels. 
% In addition, the regularity of superpixels accelerates a consistent size of superpixels.
To this end, we propose the merging method (Section~\ref{sec:adaptive-merging}), and a new evaluation metric of superpixel mechanism, that
also takes account of the size of ground-truth segments (Section~\ref{sec:confusion-matrix}).
The proposed metric not only highlights the difference of the ideal superpixel required in active learning
than that in the previous context, but also gives a guideline to develop superpixel algorithms for active learning. 
% To evaluate the suitability of superpixels in active learning, we propose achievable precision and recall metrics that consider the size of superpixels.

% However, the tendency of superpixel algorithms to produce over-segmented and homogeneous superpixels makes them unsuitable for active learning, which requires a large amount of labels in a single click. 
% We propose acheivable metrics
% All the generated superpixels are evaluated 
% with various metrics including achievable segmentation accuracy and regularity \cite{schick2012measuring, giraud2017robust, machairas2014waterpixels}
% calculate the similarity between adjacent pixels, followed by gradually adding multiple minimum cuts.
% to achieve a boundary-preserving partition.
% To enforce compact and homogeneous superpixels, superpixels are created by maximizing the entropy rate of a random walk on a graph with a balancing term \cite{liu2011entropy}.
% For real-time applications, SEEDS \cite{van2012seeds} leverages a simple hill-climbing optimization to enforce color similarity.

% if superpixels are of size one (single pixel) then ASA should be 100\%.
% With dominant-labeling, the uniformity prior must be relaxed for increased throughput of annotation budget, and what is followed is that larger superpixels have more noise and smaller ones have less noise. The trade-off is not invovled in ASA. 
% Superpixels are evaluated with various metrics. Since superpixels are uniform in size, conventionally the most famous is Achievable Semantic Accuracy~(ASA)~
% When superpixels are involved in active learning framework with dominant-labeling as querying system, the goal is to increase both accuracy and throughput; 
% Numerous studies focus on the challenge of dividing an image into non-overlapping superpixels that align with semantic boundaries.
% Cut-based approaches \cite{jianbo2000normalized,veksler2010superpixels,zhang2011superpixels,liu2011entropy} represent each pixel as a node and assign the similarity between adjacent pixels, followed by gradually adding multiple minimum cuts to achieve a boundary-preserving partition.
% Other approaches develop homogeneous clusters from the initial set of points \cite{levinshtein2009turbopixels, achanta2012slic}.
% To enforce compact and homogeneous superpixels, superpixels are created by maximizing the entropy rate of a random walk on a graph with a balancing term \cite{liu2011entropy}.
% For real-time applications, SEEDS \cite{van2012seeds} leverages a simple hill-climbing optimization to enforce color similarity.
% However, the tendency of superpixel algorithms to produce over-segmented and homogeneous superpixels makes them unsuitable for active learning, which requires a large amount of labels in a single click. 

% Recently, there exists attempts to use deep neural networks with ground truth for superpixel algorithms \cite{tu2018learning, jampani2018superpixel, yang2020superpixel}. 
% However, these methods are far from performing active learning starting from an unlabeled dataset.
% (Superpixel evaluation) 

% Superpixels are evaluated with various metrics. Since superpixels are uniform in size, conventionally the most famous is Achievable Semantic Accuracy~(ASA)~
%(cite), which is the ratio of ..., puts more focus on reducing noise of superpixels. 
% However, ASA is not suitable for active learning, if annotaions are responded with dominant-labeling (cite?). 
% With dominant-labeling, the uniformity prior must be relaxed for increased throughput of annotation budget, and what is followed is that larger superpixels have more noise and smaller ones have less noise. The trade-off is not invovled in ASA. 
% Average Precision~(AP) and Recall~(AR)~(cite) ...
% Conventional superpixels genration - uniform size, ASA measure
% ASA is not suitable for adaptive active learning. ASA compares generated superpixels for an image against the ground-truth oracle superpixels... as a ratio of ... ASA put more focus on accuracy. 
%if superpixels are of size one (single pixel) then ASA should be 100\%. 
% When superpixels are involved in active learning framework with dominant-labeling as querying system, the goal is to increase both accuracy and throughput; 
% if two superpixels have same noise ratio, then the larger one must be preferred.

\omh{
% A superpixel refers to a set of pxiels, where the semantic categories are perceptually equivalent. 
% To generate superpixels, cut-based algorithms \cite{jianbo2000normalized,veksler2010superpixels,zhang2011superpixels,liu2011entropy} represent each pixel as a node and assign the similarity between adjacent pixels to the edge connecting them, followed by gradually adding multiple minimum cuts to achieve a boundary-preserving partition.
% Other approaches develop homogeneous clusters from the initial set of points \cite{levinshtein2009turbopixels, achanta2012slic}.
% To enforce compact and homogeneous superpixels, superpixels are created by maximizing the entropy rate of a random walk on a graph with a balancing term \cite{liu2011entropy}.
% For real-time applications, SEEDS \cite{van2012seeds} leverages a simple hill-climbing optimization to enforce color similarity.
% However, the tendency of superpixel algorithms to produce over-segmented and homogeneous superpixels makes them unsuitable for active learning, which requires a large amount of labels in a single click. 
% Recently, there exists attempts to use deep neural networks with ground truth for superpixel algorithms \cite{tu2018learning, jampani2018superpixel, yang2020superpixel}.

% Increasing precision is analogous to reducing noise.
% Increasing recall is to increase throughput.
% Achievable Precision~(AP) and Recall~(AR) - our contribution (is it right to mention it in related works?)
% Average Precision and Recall
% Achievable Precision and Recall
% Why AP, AR, and AF?

}

% However, as minimum cuts tend to isolate a small set of pixels, normalized-cut is introduced to address the issue of over-segmentation \cite{jianbo2000normalized}.
% Another technique covers images with overlapping square patches and assigns each pixel to one of them by minimizing an energy function \cite{veksler2010superpixels}.

% There exists lots of research into the problem of dividing an image into non-overlapping superpixels that align with the semantic boundaries. 
% One family of methods represents each pixel as a node and calculates the similarity between neighboring pixels, then gradually adds multiple minimum cuts to achieve a boundary-preserving partition \cite{jianbo2000normalized,veksler2010superpixels,zhang2011superpixels,liu2011entropy}.
% Since the minimum cuts prefer isolating a small set of nodes, normalized-cut is introduced to solve the problem of over-partitioning.
% In \cite{veksler2010superpixels}, images are covered by overlapping square patches, and each pixel is assigned to one of those by minimizing an energy function.
% To reduce the computational burden of \cite{veksler2010superpixels}, pseudo-boolean optimization is introduced \cite{zhang2011superpixels}.
% In \cite{liu2011entropy}, superpixels are generated by maximizing the entropy rate of a random walk on a graph, while enforcing compact and homogeneous superpixels through a balancing term.
% However, the over-segmented and homogeneous superpixels are unsuitable for active learning.
% In addition to the different sizes of the objects in the image, active learning also benefits from receiving a large amount of labels in a single click. We generate superpixels of different sizes and explore which superpixels are suitable for active learning.



% \smallskip\noindent\textbf{Rectangle-based Active Learning.} 
%\cite{qiao2022cpral,siddiqui2020viewal,xie2022towards,mackowiak2018cereals,colling2020metabox+,casanova2019reinforced,cai2021revisiting} queried annotations in the regional units. They divide each image into multiple non-overlapping regions, and sample most informative regions instead of the images themselves. In \cite{casanova2019reinforced}, deep Q-network \cite{mnih2013atari} was incorporated to model the sampling strategy network; i.e., acquisition function is modeled by deep reinforcement learning instead of hand-crafted heuristics. \cite{golestaneh2020importance} fused uncertainty and equivalent augmentations (e.g., horizontal flipping). It simply samples highest-uncertainty regions but self-consistency loss is incorporated to enforce generalization to invariance. [High cost]
% Previous works \cite{qiao2022cpral,mackowiak2018cereals,golestaneh2020importance,casanova2019reinforced} have demonstrated benefits of querying samples in rectangular units. For image-level queries, acquisition scores must be assigned to each image, simply aggregating per-pixel scores \cite{yang2017suggestive,xie2020deal,sinha2019variational}. However, different regions in each image has different effects for model training, and the disparity can be easily ignored with image-level queries. Therefore, using smaller queries in rectangular units is one way of considering multiple different semantic regions more effectively. In \cite{colling2020metabox+}, for example, each image is partitioned with multiple boxes of fixed size, and the score for each box is obtained by simply summing up the pixel-wise acquisition scores. In \cite{mackowiak2018cereals}, multiple region proposals are created with Non-Maximum Suppression~(NMS) on the aggregated region score map. \hl{Also, to form score map, annotation cost for each pixel is directly estimated and blended with Vote Entropy for minimal annotation burden. However, training the cost model is hard to achieve because too few datasets provide detailed information on human annotations, thereby limiting the extensibility of the proposed method.} In \cite{casanova2019reinforced}, the modified deep Q-network~(DQN) was trained on the pre-partitioned set of regions, to directly learn acquisition scores of the regions. \hl{However, using subset of labeled data pool to train DQN at each cycle is often irrealizable due to the limited budgets.} Besides the aforementioned issues, rectangle-based active learning methods share common pitfalls. Since rectangles are likely to contain different semantic areas, multiple pixels, or even every pixels in the regions, are required for annotations, thereby increasing annotation costs.