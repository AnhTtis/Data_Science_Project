\section{Related Work}
\label{sec:related_works}

\noindent\textbf{Hand Grasp Synthesis.} 
Synthesizing realistic hand grasps is a challenging problem, and with the advancement of deep learning, many works have been proposed to approach the task, including \cite{brahmbhatt2019contactgrasp, jiang2021graspTTA, kar2020grasp, GRAB:2020, turpin2022graspd, grady2021contactopt}. Taheri et al. \cite{GRAB:2020} propose a conditional variational autoencoder (cVAE) \cite{sohn2015cvae} based network to predict coarse MANO \cite{romero2017mano} parameters and then apply a refinement step to optimize them. Jiang et al. \cite{jiang2021graspTTA} present a method to estimate the hand pose together with a contact map, which is then used to refine the grasp. In contrast to sampling-based grasp synthesis methods, Turpin et al. \cite{turpin2022graspd} present a gradient-based optimization scheme with a differentiable contact simulation to synthesize contact-rich grasps. Going beyond the synthesis of static object grasps, some previous studies \cite{zhang2021manipnet, christen2022dgrasp} also explore the generation of hand grasp motions. Christen et al. \cite{christen2022dgrasp} suggest using reinforcement learning to synthesize physically plausible grasping motions. Given the object and wrists trajectories, Zhang et al. \cite{zhang2021manipnet} propose an autoregressive model to synthesize the hand-object manipulation motions.

While these works have well-addressed hand grasp synthesis, they only focus on the hands in isolation from the body. Our work differs from the earlier works in that we propose to generate whole-body human-object interactions, which lie in a higher parameter dimension and require consistency of all body parts.


%------------------------------------------------------------------------
\noindent\textbf{Human Motion Synthesis.} Human motion synthesis has become a prevailing research topic in recent years and has drawn attention from both computer vision and computer graphics 
\cite{martinez2017recurrent, gopalakrishan2019rnn, martinez2017rnn, kaufmann2020convolutional, li2021taskgeneric, cai2020unified, mao2020history, li2021choreographer, petrovich2021actor, tang2018longterm}.
Kaufmann et al. \cite{kaufmann2020convolutional} propose considering motion sequences as images and utilizing convolutional neural networks to inpaint the missing parts. Some studies \cite{li2021taskgeneric, cai2020unified} suggest the use of VAEs to generate stochastic human motions. Some more recent works \cite{mao2020history, li2021choreographer, petrovich2021actor, tang2018longterm} make use of the attention \cite{vaswani2017attention} mechanism to model human motions. Even though these works have made significant progress in human motion synthesis, they only focus on modeling humans regardless of the scenes and objects around them.

There are some existing works proposed to tackle human motion synthesis involving the 3D scenes, including 
\cite{cao2020longterm, harvey2020robust, li2019affordance, makansi2019overcoming, rempe2021humor, sadeghian2019sophie, starke2019neural,  wang2021synthesizing, zhang2021priors, wang2022sceneaware, hassan2021stochastic, corona2020context, taheri2021goal, wu2022saga}. 
Wang et al. \cite{wang2021synthesizing} propose a hierarchical motion synthesis framework, which first synthesizes several sub-goal poses in 3D scenes, then infills the whole motion sequence, and finally refines the motion sequence with an optimization scheme. Hassan et al. \cite{hassan2021stochastic} suggest first estimating a goal pose and contact of the object to interact and a trajectory for the human to approach and then generating the human motion with an autoregressive module. Corona et al. \cite{corona2020context} propose a context-aware human-object interaction motion synthesis framework, while the human is represented in skeletons and contacts cannot be precisely estimated. The works that are the most relevant to ours are GOAL \cite{taheri2021goal} and SAGA \cite{wu2022saga}, both are whole-body grasping motion generation pipelines. GOAL suggests first estimating a grasping pose and then synthesizing the motion of the human from its initial position to the goal with an autoregressive model. Similarly, SAGA first predicts a grasping pose and uses a CNN-based cVAE to infill the motion. While both works generate realistic motions, they only model the human approaching the objects and stopping at the 'touching' points, ignoring human-object interactions. Unlike the methods mentioned above, our framework synthesizes complete human-object interactions, including object motions.

%------------------------------------------------------------------------
\noindent\textbf{Implicit Neural Representations.} Implicit neural representations have gained considerable attention recently with the success of SIREN \cite{sitzmann2020inr}, and NeRF \cite{mildenhall2020nerf}. The critical insight of INR is that a complex signal can be represented by a function of spatial or temporal coordinates at its corresponding position, and high-frequency details can be well preserved through this mapping. INR has demonstrated its efficacy on multiple tasks, including image synthesis \cite{inr_gan, anokhin2021image}, video generation \cite{yu2022digan}, time-varying 3D geometries \cite{niemeyer2019occupancy}, and dynamic scenes \cite{pumarola2021dnerf, li2022neural3d}. Recently, He et al. \cite{he2022nemf} propose a task-agnostic INR-based representation to interpret human motion as a function of time and conduct tasks through per-sequence optimization similar to NeRF. Inspired by these works, we propose an INR-based generative model to infill the motion between two arbitrary human poses and positions.

%------------------------------------------------------------------------
\noindent\textbf{Concurrent work.} Ghosh et al. \cite{ghosh2023imos} develop the IMoS method to generate human-object interaction motions. IMoS employs an autoregressive prediction to generate 15 frames for one motion clip and linearly interpolate them into 30 frames. Unlike IMoS, our method generates continuous motions with a standard output of 64 frames and can be upsampled to much higher frames. Besides, IMoS assumes a stable grasp is established and only generates motions from the grasping point, while we generate complete human-object motion from T-poses to manipulation.