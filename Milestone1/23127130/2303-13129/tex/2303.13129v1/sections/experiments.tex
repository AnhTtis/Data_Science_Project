\section{Experiments}


We train our full pipeline on the GRAB dataset, similar to \cite{taheri2021goal, ghosh2023imos}. We follow the same train/valid/test/ set split of objects as in \cite{taheri2021goal}. 
The GRAB dataset has 51 different objects and four intents of manipulating them, namely pass, lift, use, and offhand. For the use intent, there are 26 sub-tasks in total. However, some sub-tasks are specific to certain objects and have only one sequence for a subject in the dataset, and some sub-tasks are similar to each other though having different names. Thus, we merge some of the tasks into 6 tasks that have distinct behaviors to demonstrate the effectiveness of our method.
We label the representative frames of each task in the dataset, which requires minimal labor work. For training our pose predictor, we additionally label frames that the human first grasps the object from the table and the following 20 frames as a 'touch' task to indicate the grasping poses.
For the motion-infilling net, we downsample the framerate of GRAB from 120 fps to 30 fps, slide over each sequence with a skip frame of 16, and chop the sequences into 64 frames as unit training sequences. 

We conduct qualitative and quantitative experiments to demonstrate the effectiveness of our pipeline. We encourage readers to check our supplementary video for the complete generated sequences.



%------------------------------------------------------------------------
\subsection{Qualitative Results}


\noindent\textbf{Full pipeline.} 
A natural human-object interaction motion requires the human to first walk towards the object, grasp it, and then conduct the task. 
Fig. \ref{fig:full_pipeline} shows generated sequences of three humans of different shapes performing distinct tasks with novel objects unseen in the training data.



\begin{figure}
\centering
\begin{subfigure}{0.23\textwidth}
    \includegraphics[width=\textwidth]{figures/upsampling_a.pdf}
    \caption{64 frames.}
    \label{fig:up_a}
\end{subfigure}
\hfill
\begin{subfigure}{0.23\textwidth}
    \includegraphics[width=\textwidth]{figures/upsampling_b.pdf}
    \caption{128 frames.}
    \label{fig:up_b}
\end{subfigure}
\hfill
\begin{subfigure}{0.23\textwidth}
    \includegraphics[width=\textwidth]{figures/upsampling_c.pdf}
    \caption{256 frames.}
    \label{fig:up_c}
\end{subfigure}
\hfill
\begin{subfigure}{0.23\textwidth}
    \includegraphics[width=\textwidth]{figures/upsampling_d.pdf}
    \caption{512 frames.}
    \label{fig:up_d}
\end{subfigure}
\caption{Examples of upsampling generated motion sequences to higher frames. Fig. (a) to (d) are the results of 64, 128, 256, and 512 frames, respectively. The results demonstrate our method can generate motions of framerates well beyond the training data.  We offer the results with a skip frame of 16 and include the last frames. }
\label{figure:upsampling}
\end{figure}




\noindent\textbf{Upsampling} As our INR-based motion-infilling net generates continuous motions, it is theoretically guaranteed to allow upsampling to arbitrary frame rates. Fig. \ref{figure:upsampling} shows the results of upsampling the generated sequence to higher frame rates. Although the model is trained on 64 frame motion sequences only, we demonstrate that it can generate smooth motion sequences even at 512 frames.  For clarity of visualization, we only show the motion from the grasping pose to the final pose.

\noindent\textbf{Velocity Modification} Another advantage of our method is that by designing the temporal coordinate $\tau$, users can generate motions with different velocities at specific parts of the sequence with the same model weights. This allows for affluent post-artworks to output motions reflecting divergent states of the human. Fig. \ref{figure:velocity} shows the motions generated with the same inferenced model weights but different temporal inputs. The four inputs include a uniform sampling of $\tau$ on $[0, 1]$, a uniform sampling that is two times sparser, a non-uniform sampling that is denser at the start, and a sampling denser at the end. Here, we design the lengths of intervals of the non-uniform sampling as geometric sequences.


\begin{figure}
\centering
\begin{subfigure}{0.23\textwidth}
    \includegraphics[width=\textwidth]{figures/vel_a.pdf}
    \caption{Normal speed}
    \label{fig:vel_a}
\end{subfigure}
\hfill
\begin{subfigure}{0.23\textwidth}
    \includegraphics[width=\textwidth]{figures/vel_b.pdf}
    \caption{$\times 2$ speed}
    \label{fig:vel_b}
\end{subfigure}
\hfill
\begin{subfigure}{0.23\textwidth}
    \includegraphics[width=\textwidth]{figures/vel_c.pdf}
    \caption{Fast to slow}
    \label{fig:vel_c}
\end{subfigure}
\hfill
\begin{subfigure}{0.23\textwidth}
    \includegraphics[width=\textwidth]{figures/vel_d.pdf}
    \caption{Slow to fast}
    \label{fig:vel_d}
\end{subfigure}
\caption{Examples of motion velocity adjustment by modifying the temporal coordinate $\tau$. a) A 64-frame generated result of normal speed. b) Speed up the sequence by uniformly sampling two times fewer values from $\tau \in [0, 1]$, which gives a result of doubled velocities. c) The human swiftly lifts the object and then slowly passes it, which is done by sampling sparsely near $0$ and densely near $1$. d) The human slowly lifts the object and swiftly passes it, which uses a reverse sampling scheme of c).}
\label{figure:velocity}
\end{figure}

%------------------------------------------------------------------------
\subsection{Quantitative Results}
\subsubsection{Motion Diversity}
We calculate the Average L2 Pairwise Distance (APD) \cite{yuan2020dlow} of the object translation offsets predicted by our object position estimator, given the same objects and task types but different human shapes, to be 0.19 ($m$). This indicates our model generates diverse ending object positions for different human shapes within a reasonable range. We report the APD of our full generated motions on GRAB to be 0.34.


\subsubsection{Motion Inbetweening}

\input{tables/table1}

We conduct experiments to demonstrate the effectiveness of our INR-based motion inbetweening model. To our knowledge, our model is the first generalizable motion-infilling model that generates continuous motions.

\noindent\textbf{Baselines.} SAGA \cite{wu2022saga} proposed a CNN-based motion-infilling network that predicts both the human poses and their root translations. Wang et al. \cite{wang2021synthesizing} proposed an LSTM-based model, namely Route+PoseNet, to infill motion sequences. We take these two as our baselines as they are the closest to our settings. We also compare our model with some existing works, including the convolution autoencoder network (CNN-AE) \cite{kaufmann2020convolutional}, LEMO \cite{zhang2021priors}, and PoseNet \cite{wang2021synthesizing}, that take the ground truth trajectory as inputs and infill the local motions. We use the same body markers as in \cite{wu2022saga} for fair comparisons.

\noindent\textbf{Evaluation Metrics.} We use the same evaluation metrics in \cite{wu2022saga}. 1) \textit{3D marker accuracy.} We compute the Average L2 Distance (ADE) between the reconstructed marker sequences and the ground truth. 2) \textit{Foot skating.} We follow \cite{yuan2020dlow} to decide on skating frames and report their ratio in the full sequences. 3) \textit{Motion smoothness.} We measure the smoothness of the generated sequences by computing the Power Spectrum KL Divergence of their joints (PSKL-J) \cite{zhang2021priors} to compare with the ground truth. We report the scores of PSKL-J in both directions as they are asymmetric. Note a lower PSKL-J score represents a closer generated distribution to the ground truth.

\noindent\textbf{Results.} In Table \ref{table:motion}, we show the results of our method compared to the previous works mentioned above. We experiment on both GRAB \cite{GRAB:2020} and AMASS \cite{AMASS:ICCV:2019} datasets following the settings of \cite{wu2022saga} to test our motion inbetweening model. We show the results of using the ground truth trajectory for \cite{kaufmann2020convolutional, zhang2021priors} as they only infill local motions. We also show the results of PoseNet and SAGA fed with ground truth trajectory to predict local poses. Then, we evaluate both the predicted pose and trajectory of our method against SAGA and Route+PoseNet. The results show that our model gives better scores of PSKL-J in both directions than previous works, which indicates that the continuity of our model facilitates the generation of smoother sequences. Our method also yields lower skating effects than SAGA and Route+PoseNet and achieves a comparable ADE score to SAGA. The results demonstrate that our method is on par with state-of-the-art methods while providing the benefits of upsampling and velocity adjustment through its continuity property.

%------------------------------------------------------------------------
\subsubsection{Human-Object Motion}

\input{tables/table2}

We conduct experiments to evaluate the realisticness of our human-object motions by computing the hand-object contact ratio \cite{zhang2020withoutpeople} and the largest hand-object interpenetration depth along the whole motion sequence. We report the maximum, minimum, and average contact ratio and interpenetration depth as they reflect the patterns of the hand-object status along the movement. Our object motion estimation algorithm captures the stable grasp at the initial frame and keeps this hand-object relationship across the sequence, so we also report the contact ratio and interpenetration depth of our pose predictor generated results. 
Table \ref{table:object} shows that our method well preserves good grasps during the human-object interaction and yields results that are comparable to the ground truth.


