\section{Introduction}
\label{sec:intro}

Humans are in constant interactions with objects around them, and with different using intents, we conduct distinct motions with the objects. 
Generating such sequences in simulation is of great interest and value in various fields, from computer vision to robotics. Despite the tremendous progress in this topic in recent years, human-object interaction motion synthesis remains an under-studied problem. 
Previous works primarily focus on synthesizing human motions regardless of the scene and objects or generating human interactions only with static objects, ignoring the object motions and the using intents. Moreover, existing works can only generate discrete frames and neglect actual motions' intrinsic diversity, making the synthesized motions hard to be manipulated in applications.

Synthesizing task-oriented human-object interaction motions is challenging due to various reasons. 
First, given an object with a task like lifting, the directions of moving the object are neither known nor unique and should be estimated by the given information. 
Second, a complete human-object interaction sequence should have the object move consistently with the human without floating around or staying still. Third, to deploy in applications that have various framerate requirements, generated motions should be flexible in length instead of having fixed framerates. Existing works of human-object interaction synthesis primarily focus on modeling humans regardless of the objects, and they can only generate discrete motion sequences that are hard to be manipulated. Previous methods that are the most relevant to ours \cite{taheri2021goal, wu2022saga} only generate motions stopping at the grasping points, ignoring object motions and using intents, and cannot produce continuous results. Currently, synthesizing continuous task-oriented human-object interaction motions remains an unsolved problem.

Inspired by recent advances in human motion synthesis and implicit neural representations, we propose TOHO, a novel task-oriented human-object interaction motion generation framework. 
A natural human-object motion requires the human first to approach the object, grasp it, and then use it to accomplish the task, sequentially. TOHO addresses each part of the motion synthesis process, taking only the task type, the object at its initial position, and the initial human status as inputs. It generates natural and realistic human motions conducting the task with the object while allowing the generated results to be of arbitrary lengths. 

We generate complete human-object motions with TOHO in three steps. 1) First, we formulate the problem as a motion inbetweening task and generate the keyframes of a human conducting the given task. To generate the keyframes, we design an object position estimator that predicts the ending object's position with the task label and human shape and a pose predictor to generate human poses given the object information and task type. 2) Then, we design an INR-based motion inbetweening network that generates continuous motions to infill the missing in-between part of two frames. 2) Finally, we present a fast and compact object motion estimation algorithm that outputs realistic and natural object motions based on the generated human motions. With the algorithm, we synthesize perceptually realistic human-object manipulations with objects moving consistently with the human. 

With the three steps, our framework generates complete task-oriented human-object interaction motions. Unlike autoregressive and CNN-based methods, our framework generates motions that are continuous and parameterized only by the temporal coordinate. The continuity reflects what motions in the real world are like and allows for velocity adjustments and upsampling of sequences to arbitrary frames. Also, at inference time, as a frame is not conditioned on previous frames, all frames can be inferred parallelly with the same model. 



We summarize our contributions as 1) we present a unified framework to generate complete human-object interaction motions with intents; 
2) our framework generates continuous motions parameterized only by the temporal coordinates, exploring the intrinsic diversity of actual motions; 3) our generated results are flexible in length and speed, allowing upsampling and velocity adjustments in downstream applications.
 


