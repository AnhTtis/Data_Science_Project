\section{Introduction}
\label{sec:intro}
 
Humans are in constant interactions with objects around them, and with different using intents, we conduct distinct motions with the objects. 
Generating such sequences in simulation is of great interest and value in various fields, from computer vision to robotics. Despite the tremendous progress in this topic in recent years, human-object interaction motion synthesis remains an under-studied problem. 
Previous works primarily focus on synthesizing human motions regardless of the objects or generating human interactions only with known objects already attached to the human hands. Moreover, existing works can only generate discrete frames and neglect actual motions' intrinsic diversity, making the synthesized motions hard to be manipulated in applications.

Synthesizing task-oriented human-object interaction motions is challenging due to various reasons. 
First, given an object with a task like lifting, the directions of moving the object are neither known nor unique and should be estimated by the given information. 
Second, a complete human-object interaction sequence should have the human approaching and grasping the object and making it move consistently with the human without floating around or staying still. Third, to deploy in applications that have various framerate requirements, generated motions should be flexible in length instead of having fixed framerates. 
% Existing works of human-object interaction synthesis primarily focus on modeling humans regardless of the objects, and they can only generate discrete motion sequences that are hard to be manipulated. 
Previous methods that are the most relevant to ours either only generate motions stopping at the grasping points, ignoring object motions and using intents \cite{taheri2021goal, wu2022saga}, or only generate object manipulation sequences with objects already attached to the human hands \cite{ghosh2023imos}, and neither can produce continuous results. Currently, synthesizing complete continuous task-oriented human-object interaction motions remains an unsolved problem.

Inspired by recent advances in human motion synthesis and implicit neural representations, we propose TOHO, a novel task-oriented human-object interaction motion generation framework. 
A natural human-object motion requires the human first to approach the object, grasp it, and then use it to accomplish the task, sequentially. TOHO addresses each part of the motion synthesis process, taking only the task type, the object at its initial position, and the initial human status as inputs. It generates natural and realistic human motions conducting a task with an unseen object while allowing the generated results to be of arbitrary lengths. 

We generate complete human-object motions with TOHO in four steps. 1) First, we design an object parameters sampler that estimates the object's translation and orientation offsets from its initial position with the task type and human shape, giving the final object position and orientation. 2) Similar to \cite{taheri2021goal, wu2022saga}, we use a goal pose generation network to generate grasping poses given the object information. 3) Then, we propose an INR-based motion inbetweening network that generates continuous motions to infill the missing in-between part of two frames. 4) Finally, we present a fast and compact object motion estimation algorithm that outputs realistic and natural object motions based on the generated human motions. With the algorithm, we synthesize perceptually realistic human-object manipulations with objects moving consistently with the human. 

With the four steps, our framework generates complete task-oriented human-object interaction motions. Unlike autoregressive and CNN-based methods, our framework generates motions that are continuous and parameterized only by the temporal coordinate. The continuity reflects what motions in the real world are like and allows for velocity adjustments and upsampling of sequences to arbitrary frames. Also, at inference time, as a frame is not conditioned on previous frames, all frames can be inferred parallelly with the same model. In Table \ref{table:checkbox}, we compare our problem setting with previous methods.



We summarize our contributions as 1) we present a unified framework to generate complete human-object interaction motions with intents; 
2) our framework generates continuous motions parameterized only by the temporal coordinates, exploring the intrinsic diversity of actual motions; 3) our generated results are flexible in length and speed, allowing upsampling and velocity adjustments in downstream applications.


