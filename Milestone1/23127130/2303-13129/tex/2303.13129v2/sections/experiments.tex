\section{Experiments}


We train our pipeline on the GRAB dataset \cite{GRAB:2020}, following the same split as \cite{taheri2021goal}. The dataset consists of 51 objects and four manipulation intents: pass, lift, offhand, and use. For the use intent, there are 26 sub-tasks in total. However, some sub-tasks are specific to certain objects and have only one sequence for a subject in the dataset, and some sub-tasks are similar to each other though having different names. Thus, we merge some of the tasks into 6 tasks that have distinct behaviors to demonstrate the effectiveness of our method.
We label the representative frames of each task in the dataset, which requires minimal labor work. For training our pose predictor, we additionally label frames that the human first grasps the object from the table and the following 20 frames as a 'touch' task to indicate the grasping poses.
For the motion inbetweening model, we downsample the framerate of GRAB from 120 fps to 30 fps, slide over each sequence with a skip frame of 16, and chop the sequences into 64 frames as unit training sequences. 



%------------------------------------------------------------------------
\subsection{Qualitative Results}


\noindent\textbf{Full pipeline.} 
A natural human-object interaction motion requires the human to first walk towards the object, grasp it, and then conduct the task. 
Fig. \ref{fig:full_pipeline} shows generated sequences of three humans of \textit{different shapes} performing \textit{distinct tasks} with \textit{unseen objects}.

\noindent\textbf{Upsampling} As our motion inbetweening model generates continuous motions, it is theoretically guaranteed to allow upsampling to arbitrary frames. In the supplementary materials, we show our model generates smooth motion even at 512 frames although trained only with 64 frames sequences.

\noindent\textbf{Velocity Modification} Another advantage of our method is that by designing the temporal coordinate $\tau$, users can generate motions with different velocities at specific parts of the sequence with the same model weights. This allows for affluent post-artworks to output motions reflecting divergent states of the human. Fig. \ref{figure:velocity} shows the motions generated with the same inferenced model weights but different temporal inputs. The four inputs include a uniform sampling of $\tau$ on $[0, 1]$, a uniform sampling that is two times sparser, a non-uniform sampling that is denser at the end, and a sampling denser at the start. Here, we design the lengths of intervals of the non-uniform sampling as geometric sequences.


\begin{figure}
\centering
\begin{subfigure}{0.23\textwidth}
    \includegraphics[width=\textwidth]{figures/vel_a.pdf}
    \caption{Normal speed}
    \label{fig:vel_a}
\end{subfigure}
\hfill
\begin{subfigure}{0.23\textwidth}
    \includegraphics[width=\textwidth]{figures/vel_b.pdf}
    \caption{$\times 2$ speed}
    \label{fig:vel_b}
\end{subfigure}
\hfill
\begin{subfigure}{0.23\textwidth}
    \includegraphics[width=\textwidth]{figures/vel_c.pdf}
    \caption{Fast to slow}
    \label{fig:vel_c}
\end{subfigure}
\hfill
\begin{subfigure}{0.23\textwidth}
    \includegraphics[width=\textwidth]{figures/vel_d.pdf}
    \caption{Slow to fast}
    \label{fig:vel_d}
\end{subfigure}
\caption{Examples of motion velocity adjustment by modifying the temporal coordinate $\tau$. a) A 64-frame generated result of normal speed. b) Speed up the sequence by uniformly sampling two times fewer values from $\tau \in [0, 1]$, which gives a result of doubled velocities. c) The human swiftly lifts the object and then slowly passes it, which is done by sampling sparsely near $0$ and densely near $1$. d) The human slowly lifts the object and swiftly passes it, which uses a reverse sampling scheme of c).}
\label{figure:velocity}
\end{figure}

%------------------------------------------------------------------------
\subsection{Quantitative Results}
\subsubsection{Motion Diversity}
We calculate the Average L2 Pairwise Distance (APD) \cite{yuan2020dlow} of the object translation offsets predicted by our object position estimator, given the same objects and task types but different human shapes, to be 0.19 ($m$). This indicates our model generates diverse ending object positions for different human shapes within a reasonable range. We report the APD of our full generated motions on GRAB to be 0.34.


\subsubsection{Motion Inbetweening}

\input{tables/table1}

We conduct experiments to demonstrate the effectiveness of our motion inbetweening model. To our knowledge, our model is the first generalizable motion inbetweening model that generates continuous motions.

\noindent\textbf{Baselines.} SAGA \cite{wu2022saga} proposed a CNN-based motion-infilling network that predicts both the human poses and their root translations. Wang et al. \cite{wang2021synthesizing} proposed an LSTM-based model, namely Route+PoseNet, to infill motion sequences. We take these two as our baselines as they are the closest to our settings. We also compare our model with some existing works, including the convolution autoencoder network (CNN-AE) \cite{kaufmann2020convolutional}, LEMO \cite{zhang2021priors}, and PoseNet \cite{wang2021synthesizing}, that take the ground truth trajectory as inputs and infill the local motions. We use the same body markers as in \cite{wu2022saga} for fair comparisons.

\noindent\textbf{Evaluation Metrics.} We use the same evaluation metrics in \cite{wu2022saga}. 1) \textit{3D marker accuracy.} We compute the Average L2 Distance (ADE) between the reconstructed marker sequences and the ground truth. 2) \textit{Foot skating.} We follow \cite{yuan2020dlow} to decide on skating frames and report their ratio in the full sequences. 3) \textit{Motion smoothness.} We measure the smoothness of the generated sequences by computing the Power Spectrum KL Divergence of their joints (PSKL-J) \cite{zhang2021priors} to compare with the ground truth. We report the scores of PSKL-J in both directions as they are asymmetric. Note a lower PSKL-J score represents a closer generated distribution to the ground truth.

\noindent\textbf{Results.} In Table \ref{table:motion}, we show the results of our method compared to the previous works mentioned above. We experiment on both GRAB \cite{GRAB:2020} and AMASS \cite{AMASS:ICCV:2019} datasets following the settings of \cite{wu2022saga} to test our motion inbetweening model. We show the results of using the ground truth trajectory for \cite{kaufmann2020convolutional, zhang2021priors} as they only infill local motions. We also show the results of PoseNet and SAGA fed with ground truth trajectory to predict local poses. Then, we evaluate both the predicted pose and trajectory of our method against SAGA and Route+PoseNet. The results show that our model gives better scores of PSKL-J in both directions than previous works, which indicates that the continuity of our model facilitates the generation of smoother sequences. Our method also yields lower skating effects than SAGA and Route+PoseNet and achieves a comparable ADE score to SAGA. The results show that our method is on par with SOTA methods while providing the benefits of upsampling and velocity adjustment through its continuity property.

We report the results of IMoS-generated sequences to give a sense of how our method is compared with IMoS \cite{ghosh2023imos}. As IMoS is trained only on the GRAB dataset with sequences starting from the grasping poses, we also report our results tested in the same setting. We uniformly sample 60 frames from TOHO as motion sequences and interpolate the 15 generated frames of IMoS into 60-frame motion clips, such that all motion clips have the same length. Note this is not an exact comparison as IMoS is an autoregressive model while ours is a motion-infilling model. Sequences starting from the grasping poses tend to have small root translation movements, which leads to a relatively higher skating ratio.

%------------------------------------------------------------------------
\subsubsection{Human-Object Motion}

\input{tables/table2}

We conduct experiments to evaluate the realisticness of our human-object motions by computing the hand-object contact ratio \cite{zhang2020withoutpeople} and the largest hand-object interpenetration depth along the whole motion sequence. We report the maximum, minimum, and average contact ratio and interpenetration depth along object movements. Our object motion estimation algorithm captures the stable grasp at the initial grasping frame and keeps this hand-object relationship across the sequence, so we also report the contact ratio and interpenetration depth of our pose predictor generated results. 
Table \ref{table:object} shows that our method well preserves good grasps during the human-object interaction and yields results that are comparable to the ground truth.

%------------------------------------------------------------------------
\subsubsection{Ablation Study}

\textbf{(1) Human shape for object parameters sampler.} To evaluate the effects of human shapes in estimating final object positions, we conduct experiments by training an object parameters sampler which does not take the human shape as input. We report the average distance from the predicted object positions to the ground truth of this model as 0.073 (m), while ours taking the human shape information as input during training has a result of 0.048 (m). The results suggest that human shape is an impactful factor in estimating the final object position given a task type. \textbf{(2) Losses of the motion inbetweening model.} In Table \ref{table:motion_ablation}, we show the results of our motion inbetweening model trained without the foot-ground contact loss/surface marker loss. All three models are trained and tested on the GRAB dataset. \textbf{(3) Object motion estimation.} To demonstrate the effectiveness of aligning the orientation of the object to the fingertip rotations, we compare our method with one without $R_n$, i.e., simply averaging the fingertip trajectories as the object trajectory and keeping the object orientation still. We show the results in Table \ref{table:object_ablation}. For both methods, we use the ground truth human motions to compute the object trajectories.


\input{tables/table3}

\input{tables/table4}
