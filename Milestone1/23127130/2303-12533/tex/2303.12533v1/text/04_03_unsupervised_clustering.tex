\vspace{-0.8em}
\subsubsection{Time series clustering} \label{sec:unsupclus}

\begin{table*}[!t]
  \vspace{-.5em}
  \renewcommand{\arraystretch}{1.05}
  \addtolength{\tabcolsep}{1pt}
  %\scriptsize
  \centering
  \begin{tabular*}{\linewidth}{@{}lr@{\hskip 11pt}@{\extracolsep{\fill}}*{4}{c}@{\extracolsep{\fill}}*{2}{c}|@{\extracolsep{\fill}}*{2}{c}@{}}
  \toprule
    & \#param &\multicolumn{2}{c}{PASTIS} & \multicolumn{2}{c}{TimeSen2Crop} & \multicolumn{2}{c|}{SA} & \multicolumn{2}{c}{DENETHOR}\\
    \cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}\cmidrule(lr){9-10}
    Method & (x1000) & {\color{ACC}OA$\uparrow$} & MA$\uparrow$ & {\color{ACC}OA$\uparrow$} & MA$\uparrow$ & {\color{ACC}OA$\uparrow$} & MA$\uparrow$ & {\color{ACC}OA$\uparrow$} & MA$\uparrow$\\
    \midrule
    K-means-DTW~\cite{Petitjean2011} & $520$ & {\color{ACC}---} & --- & {\color{ACC}$40.5 \hidestd{1.9}$} & $26.8 \hidestd{1.4}$ & {\color{ACC}---} & --- & {\color{ACC}---} & --- \rule{0pt}{2.6ex}\\
    USRL~\cite{franceschi2019unsupervised}+K-means & $290$ & {\color{ACC}$63.9 \hidestd{0.4}$} & $20.4 \hidestd{1.4}$ & {\color{ACC}$34.9 \hidestd{0.9}$} & $23.6 \hidestd{0.7}$ & {\color{ACC}$60.9 \hidestd{1.1}$} & $48.6 \hidestd{0.8}$ & {\color{ACC}$54.0 \hidestd{3.5}$} & $46.4 \hidestd{2.2}$\\
    DTAN~\cite{shapira2019diffeomorphic}+K-means & $646$ & {\color{ACC}$65.6 \hidestd{1.4}$} & $21.4 \hidestd{1.0}$ & {\color{ACC}$47.7 \hidestd{4.5}$} & $29.3 \hidestd{2.6}$ & {\color{ACC}$60.5 \hidestd{1.6}$} & $48.6 \hidestd{2.7}$ & {\color{ACC}$46.3 \hidestd{1.2}$} & $36.9 \hidestd{1.2}$\rule[-1.2ex]{0pt}{0pt}\\
    \midrule
    K-means~\cite{bottou1994convergence} & $520$ & {\color{ACC}$69.0 \hidestd{0.6}$} & $29.8 \hidestd{1.3}$ & {\color{ACC}$49.5 \hidestd{2.3}$} & $32.5 \hidestd{1.9}$ & {\color{ACC}$61.9 \hidestd{0.9}$} & $47.8 \hidestd{1.1}$ & {\color{ACC}$57.2 \hidestd{1.9}$} & $48.5 \hidestd{2.2}$\rule{0pt}{2.6ex}\\
    ~~$+$~time warping    
        & $1\ 209$ & {\color{ACC}$\mathbf{69.1} \hidestd{0.5}$} & $\mathbf{30.4} \hidestd{1.2}$ & {\color{ACC}$\mathbf{52.3} \hidestd{1.8}$} & $\mathbf{36.0} \hidestd{0.9}$ & {\color{ACC}$\mathbf{64.1} \hidestd{1.1}$} & $\mathbf{51.7} \hidestd{2.3}$ & {\color{ACC}$57.6 \hidestd{2.0}$} & $51.1 \hidestd{2.7}$\\
    ~~~~$+$~offset    
        & $1\ 373$ & {\color{ACC}$67.7 \hidestd{0.4}$} & $28.6 \hidestd{0.9}$ & {\color{ACC}$52.0 \hidestd{1.4}$} & $35.5 \hidestd{1.4}$ & {\color{ACC}$63.6 \hidestd{0.2}$} & $50.4 \hidestd{1.6}$ & {\color{ACC}$\mathbf{58.5} \hidestd{3.6}$} & $\mathbf{52.6} \hidestd{2.9}$\rule[-1.2ex]{0pt}{0pt}\\ \bottomrule
  \end{tabular*}
  \vspace{-.8em}
  \caption{\textbf{Performance comparison for clustering on all datasets.} We report for our method and competing methods the number of trainable parameters (\#param), the accuracy (OA) and the mean class accuracy (MA). K-means clustering is run with 32 clusters for all methods for fair comparison. We separate with a vertical line the DENETHOR datasets where train and test splits are acquired during different periods (right) from the others (left). K-means-DTW is tested on TimeSen2Crop dataset only, due to the expensive cost of the algorithm.}
  \label{tab:unsup}
    \vspace{-1em}
\end{table*}

In this section, we demonstrate clear boosts provided by our method on the four SITS datasets we study. We compare our method to other clustering approaches applied on learned features or directly on the time series. Our results are summarized in Table~\ref{tab:unsup} and the different methods are detailed below:
\begin{itemize}
    \vspace{-0.2em}
    \item\textbf{K-means~\cite{bottou1994convergence}.} We apply the classic K-means algorithm on the multivariate pixel time series directly. Clustering is performed on all splits (train, val and test). Then we determine the most frequently occurring class in each cluster, considering training data only. The result is used as label for the entire cluster. We use the gradient descent version~\cite{bottou1994convergence} K-means with empty cluster reassignment~\cite{caron2018deep, monnier2020deep}.
    \vspace{-0.2em}
    \item\textbf{K-means-DTW~\cite{Petitjean2011}.} The K-means algorithm is applied in this case with a dynamic time warping metric instead of the usual Euclidean distance. To this end, we use the differentiable Soft-DTW~\cite{Cuturi2017} version of DTW and its Pytorch implementation~\cite{maghoumi2021deepnag}.
    \vspace{-0.2em}
    \item\textbf{USRL~\cite{franceschi2019unsupervised} + K-means.} USRL is an encoder trained in an unsupervised manner to represent time series by a 320-dimensional vector. We train USRL on all splits of each dataset, then apply K-means in the feature space. We use the official implementation\footnote{\noindent github.com/White-Link/UnsupervisedScalableRepresentationLearni\-ngTimeSeries} of USRL with default parameters. 
    \item\textbf{DTAN~\cite{shapira2019diffeomorphic} + K-means.} DTAN is an unsupervised method for aligning temporally all the time series of a given set. K-means is applied on data from all splits after alignment with DTAN. We use the official implementation\footnote{github.com/BGU-CS-VIL/dtan} of DTAN with default parameters.
\end{itemize}
We evaluate all methods with $K=32$ clusters.

Our method outperforms all the other baselines on the four datasets, always achieving the best mean accuracy. In particular, our time warping transformation appears to be the best way to handle temporal information when clustering agricultural time series. Indeed, DTAN+K-means leads to a significantly less accurate clustering than simple K-means. It confirms that temporal information is crucial when clustering agricultural time series: aligning temporally all the sequences of a given dataset leads to a loss of discriminative information. The same conclusion can be drawn from the results of K-means-DTW on TimeSen2Crop. In contrast, our time warping appears as constrained enough to both reach satisfying scores and account for the temporal diversity of the data. 

Using an offset transformation on the spectral intensities consistently results in improved sample reconstruction using our prototypes, as demonstrated in Table~\ref{tab:super_ablation}. However, it only increases classification scores for DENETHOR.
We attribute this improvement to the offset transformation's ability to better handle the domain shift between the training and testing data on the DENETHOR dataset. The results on the other datasets suggest that this transformation accounts for more than just intra-class variability, leading to less accurate classification scores.