\vspace{-0.2em}
\section{Method}
\label{sec:method}
In this section, we explain how we adapt the DTI framework~\cite{monnier2020deep} to pixel-wise SITS classification. First, we explain our model and network architecture (Sec.~\ref{sec:model}). Second, we present our training losses in the supervised and unsupervised cases and give implementation and optimization details (Sec.~\ref{sec:losstrain}). 

\paragraph{Notation.} We use bold letters for multivariate time series (e.g., $\mathbf{a}$, $\mathbf{A}$), brackets $[.]$ to index time series dimensions and we write $a_{1:N}$ for the set $\{a_1,...,a_n\}$.

\subsection{Model} \label{sec:model}

\paragraph{Overview.} An overview of our model is presented in Figure~\ref{fig:method}. 
We consider a pixel time series $\inputseq$ in $\mathbb{R}^{T\times C}$ of temporal length $T$ with $C$ spectral bands and we reconstruct it as a transformation of a prototypical time series. 
We will consider a set of $K$ prototypical time series $\proto_{1:K}$, each one being a time series $\protok\in \mathbb{R}^{T\times C}$ of same size as $\inputseq$ and each intuitively corresponding to a different crop type.
 
We consider a family of multivariate time series transformations $\mathcal{T}_\beta:\mathbb{R}^{T\times C}\longrightarrow\mathbb{R}^{T\times C}$ parametrized by $\beta$. Our main assumption is that we can faithfully reconstruct the sequence $\inputseq$ by applying to a prototype $\proto_k$  a transformation $\mathcal{T}_{g_k(\inputseq)}$ with some input-dependent and prototype-specific parameters $g_k(\inputseq)$ .

We denote by $\recons_k(\inputseq)\in \mathbb{R}^{T\times C}$ the reconstruction of the time series $\inputseq$ obtained using a specific prototype $\proto_k$ and the prototype-specific parameters $g_k(\inputseq)$:
 
\begin{equation}
    \recons_k\big(\inputseq\big)=\mathcal{T}_{g_k(\inputseq)}\big(\proto_k\big).
    \label{eq:reconsk}
\end{equation}
Intuitively, a prototype corresponds to a type of crop (wheat, oat, etc) and a given input should be best reconstructed by the prototype of the corresponding class. For this reason, we want the transformations to only account for intra-class variability, which requires defining an adapted transformation model.

\paragraph{Transformation model.}
 We have designed a transformation model specific to SITS and based on two transformations: an offset along the spectral dimension and a time warping.

The 'offset' transformation allows the prototypes to be shifted in the spectral dimension to best reconstruct a given input time series (Figure~\ref{fig:transformationsa}). More formally, the deformation with parameters $\beta^{\text{offset}}$ in $\mathbb{R}^C$ applied to a prototype $\proto$ can be written as:
\begin{equation}
\toff_{\beta^{\text{offset}}}\big(\proto\big)=\beta^{\text{offset}} + \proto,
    \label{eq:offset}
\end{equation}
where the addition is to be understood channel-wise.

The 'time warping' deformation aims at modeling intra-class temporal variability (Figure~\ref{fig:transformationsb}) and is defined using a thin-plate spline~\cite{bookstein1989principal} transformation along the temporal dimension of the time series. More formally, we start by defining a set of $M$ uniformly spaced landmark time steps $(t_1, ..., t_M)^T$. Given $M$ target time steps $\beta^{\text{tw}}=(\beta^{\text{tw}}_1, ..., \beta^{\text{tw}}_M)^T$, we denote by $h_{\beta^{\text{tw}}}$ the unique 1D thin-plate spline that maps each $t_m$ to $t_m'=t_m+\beta^{\text{tw}}_m$. 
Now, given an input pixel time series $\inputseq$ and $\beta^{\text{tw}} \in \mathbb{R}^M$, we define the time warping deformation applied to a prototype $\proto$ as:
\begin{equation}
    \ttw_{\beta^{\text{tw}}}\big(\proto\big)[t]=\proto\big[h_{\beta^{\text{tw}}}(t)\big],
    \label{eq:tw}
\end{equation}
for $t\in [1, T]$. Note that the offset is time-independent and that the time warping is channel-independent.\\

To define our full transformation model, we compose these two transformations, which leads to reconstructions: 
\begin{equation}
    \recons_k\big(\inputseq\big)= \toff_{\beta^{\text{offset}}} \circ \ttw_{\beta^{\text{tw}}} \big(\proto_k\big) \text{, with } (\beta^{\text{offset}},\beta^{\text{tw}})=g_k(\inputseq).
    \label{eq:reconskfull}
\end{equation}


\paragraph{Architecture.} We implement the functions $g_{1:K}$ predicting the transformation parameters as a neural network composed of a shared {encoder}, for which we use the convolutional network architecture proposed by~\cite{wang2017time}, and a final linear layer with $K\times(C+M)$ outputs followed by the hyperbolic tangent (tanh) function as activation layer. We interpret this output as $K$ sets of $(C+M)$ parameters for the transformations of the $K$ prototypes. By design, these transformation parameters take values in $[-1,1]$. This is adapted for the offset transformation since we normalize the time series before processing, but not for the time warping. We thus multiply the outputs of the network corresponding to the time warping parameters so that the maximum shift of the control time step corresponds to a week. We choose $M$ for each dataset so that we have a landmark time-step every month. In the supervised case, we choose $K$ equal to the number of crop classes in each dataset and we set $K$ to 32 in the unsupervised case. 

\begin{figure*}[t]
    \centering
    \begin{center}
    \begin{tabular}{ccc}
    \begin{subfigure}[t]{0.46\linewidth}
        \includegraphics[trim={0 0 0 0}, clip, width=\linewidth]{figures/offset_4.pdf}
        \subcaption{Offset with $\beta^{\text{offset}}=0.3$}
        \label{fig:transformationsa}
    \end{subfigure}
    &
    \begin{subfigure}[t]{0.46\linewidth}
        \includegraphics[trim={0 0 0 0}, clip, width=\linewidth]{figures/time_warping.pdf}
        \subcaption{Time warping with $M=3$, $\beta_1^{\text{tw}}=-7$, $\beta_2^{\text{tw}}=0$ and  $\beta_3^{\text{tw}}=7$}
        \label{fig:transformationsb}
    \end{subfigure}
    \end{tabular}
    \end{center}
    \vspace{-1em}
    \caption{
    \textbf{Prototype deformations.}~We show the visual interpretations of our time series deformations. The offset deformation is time-independent and performed on each spectral band separately. On the other hand, time warping is channel-independent and achieved by translating landmark time-steps, allowing targeted temporal adjustments.}
    \label{fig:transformations}
\end{figure*}
\subsection{Losses and training} \label{sec:losstrain}

We learn the prototypes $\proto_{1:K}$ and the deformation prediction networks $g_{1:K}$ by minimizing a mean loss on a dataset of $N$ multivariate pixel time series $\inputseq_{1:N}$. We define this loss below in the supervised and unsupervised scenarios. % respectively.

\paragraph{Unsupervised case.} In this scenario, our loss is composed of two terms. The first one is a reconstruction loss and corresponds to the mean squared error between the input time series and the transformed prototype that best reconstructs it for all pixels $\inputseq$ of the studied dataset:
\begin{equation}
    \mathcal{L}_\text{rec}(\proto_{1:K}, g_{1:K}) = \frac{1}{NTC}\sum_{i=1}^N \underset{k}{\min}\Big|\Big|\inputseq_i-\reconsk(\inputseq_i)\Big|\Big|_2^2.
    \label{eq:lrec_unsup}
\end{equation}
The second loss is a regularization term, which prevents high frequencies in the learned prototypes. Indeed, the time warping module allows interpolations between prototype values at consecutive time steps $t$ and $t+1$, and our network could thus use temporal shifts together with high-frequencies in the prototypes to obtain better reconstructions. To avoid these unwanted high-frequency artifacts, we add a total variation regularization~\cite{rudin1992nonlinear}:
\begin{equation}
    \mathcal{L}_{\text{tv}}(\proto_{1:K}) = \frac{1}{(KT-1)C}\sum_{k=1}^K \sum_{t=1}^{T-1} \Big|\Big|\protok[t+1] - \protok[t]\Big|\Big|_2.
    \label{eq:ltvh}
\end{equation}
The full training loss without supervision is thus:
\begin{equation}
    \mathcal{L}_\text{unsup}(\proto_{1:K}, g_{1:K})=\mathcal{L}_\text{rec}(\proto_{1:K}, g_{1:K}) + \lambda\mathcal{L}_{\text{tv}}(\proto_{1:K}),
    \label{eq:l_unsup}
\end{equation}
with $\lambda$ a scalar hyperparameter set to $0.01$ in all our experiments.

\paragraph{Supervised case.} In the supervised scenario, we choose $K$ as the true number of classes in the studied dataset, and set a one-to-one correspondence between each prototype and one class. We leverage this knowledge of the class labels to define two losses. Let $y_i\in\{1,...,K\}$ be the class label of input pixel $\inputseq_i$. First, a reconstruction loss similar to~(\ref{eq:lrec_unsup}) penalizes the mean squared error between an input and its reconstruction using the true-class prototype:

\begin{equation}
    \mathcal{L}_\text{rec\_sup}(\proto_{1:K}, g_{1:K}) = \frac{1}{NTC}\sum_{i=1}^N\Big|\Big|\inputseq_i-\recons_{y_i}(\inputseq)\Big|\Big|_2^2.
    \label{eq:lrec_sup}
\end{equation}
Second, in order to boost the discriminative power of our model, we add a contrastive loss~\cite{loiseau22amodelyoucanhear} based on the reconstruction error:

\begin{equation}
\begin{split}
    &\mathcal{L}_{\text{cont}}(\proto_{1:K}, g_{1:K}) =\\ &-\frac{1}{N}\sum_{i=1}^N \log\Bigg(\frac{\exp\big(-\Big|\Big|\inputseq_i-\recons_{y_i}(\inputseq)\Big|\Big|_2^2\big)}{\sum_{k=1}^{K}\exp\big(-\Big|\Big|\inputseq_i-\recons_k(\inputseq)\Big|\Big|_2^2\big)}\Bigg).
    \end{split}
    \label{eq:lce_sup}
\end{equation}
We also use the same total variation regularization as in the unsupervised case, and the full training loss under supervision is:

\begin{equation}
    \begin{split}
    \mathcal{L}_\text{sup}(\proto_{1:K}, g_{1:K})=&\mathcal{L}_\text{rec\_sup}(\proto_{1:K}, g_{1:K}) + \mu\mathcal{L}_{\text{tv}}(\proto_{1:K}) \\&+ \nu\mathcal{L}_{\text{cont}}(\proto_{1:K}, g_{1:K}),
    \label{eq:l_sup}
    \end{split}
\end{equation}
with $\mu$ and $\nu$ two hyperparameters equal to $0.01$ in all our experiments.

\paragraph{Optimization.} We use the ADAM~\cite{kingma2014adam} optimizer with a learning rate of $10^{-5}$. We train our model following a curriculum modeling scheme~\cite{elman1993learning, monnier2020deep}: we progressively increase the model complexity by first training without deformation, then adding the time warp deformation and finally the offset deformation. We add transformations when the mean accuracy does not increase in the supervised setting and, in the unsupervised setting, when the reconstruction loss does not decrease, after 1500 iterations. Note that the contrastive loss is only activated at the end of the curriculum in the supervised setting.