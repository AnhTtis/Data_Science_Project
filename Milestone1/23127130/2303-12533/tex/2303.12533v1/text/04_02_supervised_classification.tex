\subsection{Quantitative evaluation}

We validate our approach in two setups: supervised (classification) and unsupervised (clustering). In this section, we first compare our method to top-performing supervised methods proposed in the literature for MTSC~(Sec.~\ref{sec:supclass}). We then demonstrate that our method outperforms the K-means baseline on all four datasets~(Sec.~\ref{sec:unsupclus}) thanks to the design choices for our time series deformations. 

\subsubsection{Time series classification} \label{sec:supclass}

The purpose of this section is to benchmark classic and state-of-the-art MTSC methods for crop classification in SITS data. Results are summarized in Table~\ref{tab:super}, where the different methods are:
\begin{itemize}
\item\textbf{NCC~\cite{duda1973pattern}.} The nearest centroid classifier (NCC) assigns to a test sample the label of the closest class average time series using the Euclidean distance. We also report the extension of NCC with our method to add invariance to time warping and sequence offset, as well as adding our contrastive loss.
\item\textbf{1NN~\cite{cover1967nearest} and 1NN-DTW~\cite{seto2015multivariate}.} The first nearest neighbor algorithm assigns to a test sample the label of its closest neighbor in the train set, with respect to a given distance. This algorithm is computationally costly and, since the datasets under study typically contain millions of pixel time series, we search for neighbors of test samples in a random 0.1\% subset of the train set and report the average over 5 runs with different subsets. We evaluate the nearest neighbor algorithm using the Euclidean distance (1NN) as well as using the dynamic time warping (1NN-DTW) metric on the TimeSen2Crop dataset which is small enough to compute it in a reasonable time.
\item\textbf{MLSTM-FCN~\cite{karim2019multivariate}.} MLST-FCN is a two-branch neural network concatenating the ouputs of an LSTM and a 1D-CNN to better encode time series. We use a non-official PyTorch implementation\footnote{github.com/timeseriesAI/tsai} of MLSTM-FCN.
\item\textbf{TapNet~\cite{zhang2020tapnet}.} TapNet uses a similar architecture to MLST-FCN to learn a low-dimensional representation of the data. Additionally, \cite{zhang2020tapnet} learns class prototypes in this latent space using the softmin of the euclidian distances of the embedding to the different class prototypes as  classification scores. We use the official PyTorch implementation\footnote{github.com/xuczhang/tapnet} with default parameters.
\item\textbf{OS-CNN~\cite{tang2020rethinking}.} The Omni-Scale CNN is a 1D convolutional neural network that has shown ability to robustly capture the best time scale because it covers all the receptive filed sizes in an efficient manner. We use the official implementation\footnote{github.com/Wensi-Tang/OS-CNN} with default parameters. 
\item\textbf{MLP+LTAE~\cite{garnot2020lightweight}.} The Lightweight Temporal Attention Encoder (LTAE) is an attention-based network. Used along with a Pixel Set Encoder (PSE)~\cite{garnot2020satellite}, LTAE achieves good performances on images. To adapt it to time series, we instead use a MLP as encoder. We refer to this method as MLP+LTAE and we use the official PyTorch implementation\footnote{github.com/VSainteuf/lightweight-temporal-attention-pytorch} of LTAE. 
\item\textbf{UTAE~\cite{garnot2020satellite}.} In addition to SITS methods, we also report the scores of U-net with Temporal Attention Encoder (UTAE) on PASTIS dataset. This method leverages complete (constant-size) images. Since it can learn from the spatial context of a given pixel this state-of-the-art image sequence segmentation approach is expected to perform better than pixel-based MTSC approaches and is reported for reference.
\end{itemize}

We provide two metrics for evaluating classification accuracy: overall accuracy (OA) and mean accuracy (MA). OA is computed as the ratio of correct and total predictions:
\begin{equation}
    \text{OA} = \frac{\text{TP}+\text{TN}}{\text{TP}+\text{TN}+\text{FP}+\text{FN}},
\end{equation}

where $\text{TP}$, $\text{TN}$, $\text{FP}$ and $\text{FN}$ correspond to true positive, true negative, false positive and false negative, respectively. MA is the class-averaged classification accuracy:
\begin{equation}
    \text{MA} = \frac{1}{K}\sum_{k=1}^K \text{OA}(\{\inputseq_i|y_i=k\}).
\end{equation}

It is important to note that the datasets under consideration show a high degree of imbalance, making MA a more appropriate and informative metric for evaluating classification performance. For this reason, OA scores are shown in grey in Table~\ref{tab:super}, \ref{tab:unsup} and \ref{tab:super_ablation}.

\begin{table*}[!t]
  \vspace{-.5em}
  \renewcommand{\arraystretch}{1.05}
  \addtolength{\tabcolsep}{1pt}
  \centering
  %\scriptsize
  \begin{tabular*}{\linewidth}{@{}lr@{\hskip 11pt}@{\extracolsep{\fill}}*{4}{c}@{\extracolsep{\fill}}*{2}{c}|@{\extracolsep{\fill}}*{2}{c}@{}}
  \toprule
    & \#param &\multicolumn{2}{c}{PASTIS} & \multicolumn{2}{c}{TimeSen2Crop} & \multicolumn{2}{c|}{SA} & \multicolumn{2}{c}{DENETHOR}\\
    \cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}\cmidrule(lr){9-10}
    Method & (x1000) & {\color{ACC}OA$\uparrow$} & MA$\uparrow$ & {\color{ACC}OA$\uparrow$} & MA$\uparrow$ & {\color{ACC}OA$\uparrow$} & MA$\uparrow$ & {\color{ACC}OA$\uparrow$} & MA$\uparrow$\\
    \midrule
    UTAE~\cite{Garnot2021}                  & $1\ 087$ & \underline{\color{ACC}$\mathbf{83.4}$} & \underline{$\mathbf{77.7}$} & {\color{ACC}---} & --- & {\color{ACC}---} & --- & {\color{ACC}---} & --- \rule{0pt}{2.6ex}\rule[-1.2ex]{0pt}{0pt}\\
    \midrule
    MLP + LTAE~\cite{garnot2020lightweight} & $320$    & {\color{ACC}$80.6$} & $65.9$ & \underline{\color{ACC}$\mathbf{88.7}$} & $80.9$ & {\color{ACC}$67.4$} & \underline{$\mathbf{63.7}$} & {\color{ACC}$55.6$} & $43.6$\rule{0pt}{2.6ex}\\
    OS-CNN~\cite{tang2020rethinking}        & $4\ 729$ & {\color{ACC}$\mathbf{81.3}$} & $68.1$ & {\color{ACC}$87.9$} & $81.2$ & {\color{ACC}$64.6$} & $60.3$ & {\color{ACC}$49.0$} & $39.2$\\
    TapNet~\cite{zhang2020tapnet}           & $1\ 882$ & {\color{ACC}$77.4$} & $\mathbf{69.5}$ & {\color{ACC}$83.9$} & \underline{$\mathbf{83.0}$} & \underline{\color{ACC}$\mathbf{69.4}$} & $62.5$ & {\color{ACC}$\mathbf{61.5}$} & $\mathbf{60.6}$\\
    MLSTM-FCN~\cite{karim2019multivariate}  & $490$    & {\color{ACC}$44.4$} & $10.9$ & {\color{ACC}$58.7$} & $44.0$ & {\color{ACC}$56.1$} & $47.9$ & {\color{ACC}$58.2$} & $48.3$\\
    1NN-DTW~\cite{seto2015multivariate}     & $0$      & {\color{ACC}---}    & ---    & {\color{ACC}$32.2$} & $23.0$ & {\color{ACC}---}    & --- & {\color{ACC}---} & ---\rule[-1.2ex]{0pt}{0pt}\\
    1NN~\cite{cover1967nearest}             & $0$      & {\color{ACC}$65.8$} & $40.1$ & {\color{ACC}$43.9$} & $35.0$ & {\color{ACC}$60.7$} & $54.9$ & {\color{ACC}$56.7$} & $48.2$\\
    \midrule
    NCC~\cite{duda1973pattern}              & $77$     & {\color{ACC}$56.5$} & $48.4$ & {\color{ACC}$57.4$} & $49.5$ & {\color{ACC}$51.3$} & $46.4$ & {\color{ACC}$61.3$} & $55.5$ \rule{0pt}{2.6ex}\\
    ~~$+$~time warping                      & $427$    & {\color{ACC}$56.2$} & $51.4$ & {\color{ACC}$59.9$} & $52.3$ & {\color{ACC}$54.5$} & $49.7$ & \underline{\color{ACC}$\mathbf{62.4}$} & $56.4$\\
    ~~~~$+$~offset                          & $451$    & {\color{ACC}$53.5$} & $53.8$ & {\color{ACC}$57.3$} & $55.0$ & {\color{ACC}$60.6$} & $50.0$ & {\color{ACC}$59.8$} & \underline{$\mathbf{62.9}$}\\
	~~~~~~$+$~contrastive loss              & $451$    & {\color{ACC}$\mathbf{73.7}$} & $\mathbf{59.1}$ & {\color{ACC}$\mathbf{78.5}$} & $\mathbf{70.5}$ & {\color{ACC}$\mathbf{62.3}$} & $\mathbf{54.9}$ & {\color{ACC}$56.5$} & $54.2$\rule[-1.2ex]{0pt}{0pt}\\ 
  \bottomrule
  \end{tabular*}
  \vspace{-.8em}
  \caption{\textbf{Performance comparison for classification on all datasets.} We report for our method and competing methods, the number of trainable parameters (\#param), the accuracy (OA) and the mean class accuracy (MA). We separate with a vertical line the DENETHOR dataset where train and test splits are acquired during different periods (right) from the others (left). 1NN-DTW is tested on TimeSen2Crop dataset only, due to the expensive cost of the algorithm. We separate results in 3 parts: the image level method UTAE, MTSC methods and NCC and its extension with our proposed method. We put in bold the best method in each of the 3 parts and underline the absolute best for each dataset.}
  \label{tab:super}
    \vspace{-1em}
\end{table*}

Results on the DENETHOR dataset are qualitatively very different from the results on the other datasets. We believe this is because DENETHOR has train and test splits corresponding to two distinct years. We thus analyze it separately.

\paragraph{Results on PASTIS, TimeSen2Crop and SA.} As expected, since UTAE can leverage knowledge on the spatial context of each pixel, it achieves the best score on PASTIS dataset by $2.1\%$ in overall accuracy and $8.2\%$ in mean accuracy.
Our improvements over the NCC method~\cite{duda1973pattern} - adding time warping deformation, offset deformations and contrastive loss~(\ref{eq:lce_sup}) - consistently boost the mean accuracy (MA). The improvement obtained by adding transformation modeling comes from a better capability to model the data, as confirmed by the detailed results reported in the left part of Table~\ref{tab:super_ablation}, where one can see the reconstruction error (i.e. $\mathcal{L}_\text{rec}$) significantly decreases when adding these transformations. Note that on the contrary, adding the discriminative loss increase the accuracy at the cost of decreasing the quality of the reconstruction error. Our complete supervised approach outperforms both the nearest neighbor based methods and MLSTM-FCN. However, it is still significantly outperformed by top MTSC methods. This is not surprising, since these methods are able to learn complex embeddings that capture subtle signal variations, e.g. thanks to a temporal attention mechanism~\cite{garnot2020lightweight} or to multiple-sized receptive fields~\cite{tang2020rethinking}. Note however that in doing so, they loose the interpretability of simpler approaches such as 1NN or NCC, which our method is designed to keep.

\paragraph{Results on DENETHOR.} Because the data we use is highly dependent on weather conditions, subsets acquired on distinct years follow significantly different distributions~\cite{Kondmann2021denethor}. Because of their complexity, other methods struggle to deal with this domain shift. In this setting, our extension of NCC to incorporate specific meaningful deformations achieves better performances than all the other MTSC methods we evaluated. However, adding the contrastive loss significantly degrades the results. We believe this is again due to the temporal domain shift between train and test data. This analysis is supported by results reported in Table~\ref{tab:super_ablation} which show that on the validation set of DENETHOR, which is sampled from the same year as the training data, adding the constrative loss significantly boost the results, similar to the other dataset. One can also see again on DENETHOR the benefits of modeling the deformations in term of reconstruction error. 
Note these results emphasize the need for more multi-year datasets to reliably evaluate the potential of automatic methods for practical crop segmentation scenarios, for which our deformation modeling approach seems to provide significant advantages. 

