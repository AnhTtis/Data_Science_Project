\subsection{Time series clustering} \label{sec:unsupclus}

\begin{table}[!t]
  \centering
  \caption{\textbf{Performance comparison for clustering on all datasets.} We report for our method and competing methods the number of trainable parameters (\#param) when trained on PASTIS, the accuracy (OA) and the mean class accuracy (MA). K-means clustering is run with 32 clusters for all methods for fair comparison. We distinguish with a background color the DENETHOR dataset - where train and test splits are acquired during different periods - from the others. K-means-DTW is tested on TimeSen2Crop dataset only, due to the expensive cost of the algorithm. We report the average inference time of each method to process a batch of 2,048 time series from TS2C on a single NVIDIA GeForce RTX 2080 Ti GPU.\\}
  \resizebox{\linewidth}{!}{
  \begin{tabular}{lrrccccccaa}
  \toprule
    & \#param & Inf. time & \multicolumn{2}{c}{PASTIS} & \multicolumn{2}{c}{TS2C} & \multicolumn{2}{c}{SA} & \multicolumn{2}{a}{DENETH.}\\
    \cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}
    Method & (x1000) & (ms/batch) & {\color{ACC}OA$\uparrow$} & MA$\uparrow$ & {\color{ACC}OA$\uparrow$} & MA$\uparrow$ & {\color{ACC}OA$\uparrow$} & MA$\uparrow$ & {\color{ACC}OA$\uparrow$} & MA$\uparrow$\\
    \midrule
    K-means-DTW~\citep{Petitjean2011} & $130$ & $>$10$^4$ & {\color{ACC}---} & --- & {\color{ACC}$40.5 \hidestd{1.9}$} & $26.8 \hidestd{1.4}$ & {\color{ACC}---} & --- & {\color{ACC}---} & ---\\
    USRL~\citep{franceschi2019unsupervised}+K-means & $259$ & 193 & {\color{ACC}$63.9 \hidestd{0.4}$} & $20.4 \hidestd{1.4}$ & {\color{ACC}$34.9 \hidestd{0.9}$} & $23.6 \hidestd{0.7}$ & {\color{ACC}$60.9 \hidestd{1.1}$} & $48.6 \hidestd{0.8}$ & {\color{ACC}$54.0 \hidestd{3.5}$} & $46.4 \hidestd{2.2}$\\
    DTAN~\citep{shapira2019diffeomorphic}+K-means & $256$ & 28 & {\color{ACC}$65.6 \hidestd{1.4}$} & $21.4 \hidestd{1.0}$ & {\color{ACC}$47.7 \hidestd{4.5}$} & $29.3 \hidestd{2.6}$ & {\color{ACC}$60.5 \hidestd{1.6}$} & $48.6 \hidestd{2.7}$ & {\color{ACC}$46.3 \hidestd{1.2}$} & $36.9 \hidestd{1.2}$\\
    K-means~\citep{bottou1994convergence} & $130$ & 7 & {\color{ACC}$69.0 \hidestd{0.6}$} & $29.8 \hidestd{1.3}$ & {\color{ACC}$49.5 \hidestd{2.3}$} & $32.5 \hidestd{1.9}$ & {\color{ACC}$61.9 \hidestd{0.9}$} & $47.8 \hidestd{1.1}$ & {\color{ACC}$57.2 \hidestd{1.9}$} & $48.5 \hidestd{2.2}$\\
    \midrule
    DTI-TS: K-means~$+$~time warping    
        & $471$ & 13 & {\color{ACC}$\mathbf{69.1} \hidestd{0.5}$} & $\mathbf{30.4} \hidestd{1.2}$ & {\color{ACC}$\mathbf{52.3} \hidestd{1.8}$} & $\mathbf{36.0} \hidestd{0.9}$ & {\color{ACC}$\mathbf{64.1} \hidestd{1.1}$} & $\mathbf{51.7} \hidestd{2.3}$ & {\color{ACC}$57.6 \hidestd{2.0}$} & $51.1 \hidestd{2.7}$\\
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~$+$~offset    
        & $512$ & 18 & {\color{ACC}$67.7 \hidestd{0.4}$} & $28.6 \hidestd{0.9}$ & {\color{ACC}$52.0 \hidestd{1.4}$} & $35.5 \hidestd{1.4}$ & {\color{ACC}$63.6 \hidestd{0.2}$} & $50.4 \hidestd{1.6}$ & {\color{ACC}$\mathbf{58.5} \hidestd{3.6}$} & $\mathbf{52.6} \hidestd{2.9}$\\ \bottomrule
  \end{tabular}
  }
  \label{tab:unsup}
\end{table}
In this section, we demonstrate clear boosts provided by our method on the four SITS datasets we study. We report the performance of \modelname~and competing methods in Table~\ref{tab:unsup}. Our method outperforms all the other baselines on the four datasets, always achieving the best mean accuracy. In particular, our time warping transformation appears to be the best way to handle temporal information when clustering agricultural time series. Indeed, DTAN+K-means leads to a significantly less accurate clustering than simple K-means. It confirms that temporal information is crucial when clustering agricultural time series: when DTAN aligns temporally all the sequences of a given dataset, it probably discards discriminative information, leading to poor performance. The same conclusion can be drawn from the results of K-means-DTW on TimeSen2Crop. In contrast, our time warping appears as constrained enough to both reach satisfying scores and account for the temporal diversity of the data. 

Using an offset transformation on the spectral intensities consistently results in improved sample reconstruction using our prototypes, as demonstrated in Table~\ref{tab:super_ablation}. However, it only increases classification scores for DENE\-THOR. We attribute this improvement to the offset transformation's ability to better handle the domain shift between the training and testing data on the DENETHOR dataset. The results on the other datasets suggest that this transformation accounts for more than just intra-class variability, leading to less accurate classification scores, as discussed in Section~\ref{sec:discussion}.
\begin{figure}[t]
    \includegraphics[trim={0 0 0 0}, clip, width=0.55\linewidth]{figures/ts2c_nb_ts_assignment.pdf}
    \centering
    \caption{\textbf{Number of labeled pixel times series used to assign prototypes.} We label each of the $K=32$ prototypes obtained on TS2C training set using the 1, 5 and 10 closest - or 1, 5 and 10 random - pixel time series in its cluster on the training set. We report the mean accuracy averaged over 5 runs and compare it to when using all annotated pixel time series of the training set.}
    \label{fig:num_ts_used}
\end{figure}
For all the methods compared above, we label the clusters with the most frequently occurring class in each of them on the train set. This can correspond to millions of annotated pixel time series being used, but our method works with far less annotations. We report in Figure~\ref{fig:num_ts_used} the MA of our method on TS2C when only 1, 5 or 10 annotated pixel time series used to decide for each cluster's label. We sample either random time series in each cluster ('Random') or select the time series that are best reconstructed by the given prototype ('Closest'). There is a clear 5\% performance drop when a single time series is used to label each cluster. However, using 5 time series per cluster is already enough to recover scores similar to the ones obtained using the full training dataset. For TS2C, this amounts to 0.001\% of all the training data.