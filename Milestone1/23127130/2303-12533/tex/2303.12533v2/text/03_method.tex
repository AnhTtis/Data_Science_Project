\section{Method}
\label{sec:method}
In this section, we explain how we adapt the DTI framework~\citep{monnier2020deep} to pixel-wise SITS classification. First, we explain our model and network architecture (Sec.~\ref{sec:model}). Second, we present our training losses in the supervised and unsupervised cases and give implementation and optimization details (Sec.~\ref{sec:losstrain}). We refer to our method as DTI-TS.

\paragraph{Notation} We use bold letters for multivariate time series (e.g., $\mathbf{a}$, $\mathbf{A}$), brackets $[.]$ to index time series dimensions and we write $a_{1:N}$ for the set $\{a_1,...,a_n\}$.

\subsection{Model} \label{sec:model}

\paragraph{Overview.} An overview of our model is presented in Figure~\ref{fig:method}. 
We consider a pixel time series $\inputseq$ in $\mathbb{R}^{T\times C}$ of temporal length $T$ with $C$ spectral bands and we reconstruct it as a transformation of a prototypical time series. 
We will consider a set of $K$ prototypical time series $\proto_{1:K}$, each one being a time series $\protok\in \mathbb{R}^{T\times C}$ of same size as $\inputseq$ and each intuitively corresponding to a different crop type.
 
We consider a family of multivariate time series transformations $\mathcal{T}_\beta:\mathbb{R}^{T\times C}\longrightarrow\mathbb{R}^{T\times C}$ parametrized by $\beta$. Our main assumption is that we can faithfully reconstruct the sequence $\inputseq$ by applying to a prototype $\proto_k$  a transformation $\mathcal{T}_{g_k(\inputseq)}$ with some input-dependent and prototype-specific parameters $g_k(\inputseq)$ .

We denote by $\recons_k(\inputseq)\in \mathbb{R}^{T\times C}$ the reconstruction of the time series $\inputseq$ obtained using a specific prototype $\proto_k$ and the prototype-specific parameters $g_k(\inputseq)$:
\begin{equation}
    \recons_k\big(\inputseq\big)=\mathcal{T}_{g_k(\inputseq)}\big(\proto_k\big).
    \label{eq:reconsk}
\end{equation}
Intuitively, a prototype corresponds to a type of crop (wheat, oat, etc.) and a given input should be best reconstructed by the prototype of the corresponding class. For this reason, we want the transformations to only account for intra-class variability, which requires defining an adapted transformation model.

\paragraph{Transformation model.} We have designed a transformation model specific to SITS and based on two transformations: an offset along the spectral dimension and a time warping.

The 'offset' transformation allows the prototypes to be shifted in the spectral dimension to best reconstruct a given input time series (Figure~\hyperref[fig:transformations]{\ref*{fig:transformations}a}). More formally, the deformation with parameters $\beta^{\text{offset}}$ in $\mathbb{R}^C$ applied to a prototype $\proto$ can be written as:
\begin{equation}
\toff_{\beta^{\text{offset}}}\big(\proto\big)=\beta^{\text{offset}} + \proto,
    \label{eq:offset}
\end{equation}
where the addition is to be understood channel-wise.

The 'time warping' deformation aims at modeling intra-class temporal variability (Figure~\hyperref[fig:transformations]{\ref*{fig:transformations}b}) and is defined using a thin-plate spline~\citep{bookstein1989principal} transformation along the temporal dimension of the time series. More formally, we start by defining a set of $M$ uniformly spaced landmark time steps $(t_1, ..., t_M)^\top$. Given $M$ target shifts $\beta^{\text{tw}}=(\beta^{\text{tw}}_1, ..., \beta^{\text{tw}}_M)^\top$, we denote by $h_{\beta^{\text{tw}}}$ the unique 1D thin-plate spline that maps each $t_m$ to $t_m'=t_m+\beta^{\text{tw}}_m$. 
Now, given an input pixel time series $\inputseq$ and $\beta^{\text{tw}} \in \mathbb{R}^M$, we define the time warping deformation applied to a prototype $\proto$ as:
\begin{equation}
    \ttw_{\beta^{\text{tw}}}\big(\proto\big)[t]=\proto\big[h_{\beta^{\text{tw}}}(t)\big],
    \label{eq:tw}
\end{equation}
for $t\in [1, T]$. Note that the offset is time-independent and that the time warping is channel-independent.\\

To define our full transformation model, we compose these two transformations, which leads to reconstructions: 
\begin{equation}
    \recons_k\big(\inputseq\big)= \toff_{\beta^{\text{offset}}} \circ \ttw_{\beta^{\text{tw}}} \big(\proto_k\big) \text{, with } (\beta^{\text{offset}},\beta^{\text{tw}})=g_k(\inputseq).
    \label{eq:reconskfull}
\end{equation}

\paragraph{Architecture.} The prototypes are multivariate time series whose values in all channels and for all time stamps are free parameters learned through the optimization of a training objective (see Section~\ref{sec:losstrain}). We implement the functions $g_{1:K}$ predicting the transformation parameters as a neural network composed of a shared {encoder}, for which we use the convolutional network architecture proposed by~\cite{wang2017time}, and a final linear layer with $K\times(C+M)$ outputs followed by the hyperbolic tangent (tanh) function as activation layer. We interpret this output as $K$ sets of $(C+M)$ parameters for the transformations of the $K$ prototypes. By design, these transformation parameters take values in $[-1,1]$. This is appropriate for the offset transformation since we normalize the time series before processing, but not for the time warping. We thus multiply the outputs of the network corresponding to the time warping parameters so that the maximum shift of the landmark time steps corresponds to a week. We choose $M$ for each dataset so that we have a landmark time step every month. In the supervised case, we choose $K$ equal to the number of crop classes in each dataset and we set $K$ to 32 in the unsupervised case. 
\begin{figure}[t]
    \centering
    \begin{tabular}{cc}
        \includegraphics[trim={0 0 0 0}, clip, width=0.46\linewidth]{figures/offset.pdf}
        &
        \includegraphics[trim={0 0 0 0}, clip, width=0.46\linewidth]{figures/time_warping.pdf}\\
        (a) Offset with $\beta^{\text{offset}}=0.3$
        &
        (b) Time warping with $M=3$, $\beta_1^{\text{tw}}=-7$,\\
        & $\beta_2^{\text{tw}}=0$ and $\beta_3^{\text{tw}}=7$\\
        
    \end{tabular}
    \caption{\textbf{Prototype deformations.}~We show the visual interpretations of our time series deformations. The offset deformation is time-independent and performed on each spectral band separately. On the other hand, the time warping is channel-independent and achieved by translating landmark time-steps, allowing targeted temporal adjustments.}
    \label{fig:transformations}
\end{figure}

\subsection{Losses and training} \label{sec:losstrain}

We learn the prototypes $\proto_{1:K}$ and the deformation prediction networks $g_{1:K}$ by minimizing a mean loss on a dataset of $N$ multivariate pixel time series $\inputseq_{1:N}$. We define this loss below in the supervised and unsupervised scenarios. 

\paragraph{Unsupervised case.} In this scenario, our loss is composed of two terms. The first one is a reconstruction loss and corresponds to the mean squared error between the input time series and the transformed prototype that best reconstructs it for all pixels $\inputseq$ of the studied dataset:
\begin{equation}
    \mathcal{L}_\text{rec}(\proto_{1:K}, g_{1:K}) = \frac{1}{NTC}\sum_{i=1}^N \underset{k}{\min}\Big|\Big|\inputseq_i-\reconsk(\inputseq_i)\Big|\Big|_2^2.
    \label{eq:lrec_unsup}
\end{equation}
The second loss is a regularization term, which prevents high frequencies in the learned prototypes. Indeed, the time warping module allows interpolations between prototype values at consecutive time steps $t$ and $t+1$, and our network could thus use temporal shifts together with high-frequencies in the prototypes to obtain better reconstructions. To avoid these unwanted high-frequency artifacts, we add a total variation regularization~\citep{rudin1992nonlinear}:
\begin{equation}
    \mathcal{L}_{\text{tv}}(\proto_{1:K}) = \frac{1}{K(T-1)C}\sum_{k=1}^K \sum_{t=1}^{T-1} \Big|\Big|\protok[t+1] - \protok[t]\Big|\Big|_2.
    \label{eq:ltvh}
\end{equation}
The full training loss without supervision is thus:
\begin{equation}
    \mathcal{L}_\text{unsup}(\proto_{1:K}, g_{1:K})=\mathcal{L}_\text{rec}(\proto_{1:K}, g_{1:K}) + \lambda\mathcal{L}_{\text{tv}}(\proto_{1:K}),
    \label{eq:l_unsup}
\end{equation}
with $\lambda$ a scalar hyperparameter set to $1$ in all our experiments.

\paragraph{Supervised case.} In the supervised scenario, we choose $K$ as the true number of classes in the studied dataset, and set a one-to-one correspondence between each prototype and one class. We leverage this knowledge of the class labels to define two losses. Let $y_i\in\{1,...,K\}$ be the class label of input pixel $\inputseq_i$. First, a reconstruction loss similar to~(\ref{eq:lrec_unsup}) penalizes the mean squared error between an input and its reconstruction using the true-class prototype:
\begin{equation}
    \mathcal{L}_\text{rec\_sup}(\proto_{1:K}, g_{1:K}) = \frac{1}{NTC}\sum_{i=1}^N\Big|\Big|\inputseq_i-\recons_{y_i}(\inputseq)\Big|\Big|_2^2.
    \label{eq:lrec_sup}
\end{equation}
Second, in order to boost the discriminative power of our model, we add a contrastive loss~\citep{loiseau22amodelyoucanhear} based on the reconstruction error:
\begin{equation}
\begin{split}
 &\mathcal{L}_{\text{cont}}(\proto_{1:K}, g_{1:K}) =-\frac{1}{N}\sum_{i=1}^N \log\Bigg(\frac{\exp\big(-\Big|\Big|\inputseq_i-\recons_{y_i}(\inputseq)\Big|\Big|_2^2\big)}{\sum_{k=1}^{K}\exp\big(-\Big|\Big|\inputseq_i-\recons_k(\inputseq)\Big|\Big|_2^2\big)}\Bigg).
    \end{split}
    \label{eq:lce_sup}
\end{equation}
We also use the same total variation regularization as in the unsupervised case, and the full training loss under supervision is:
\begin{equation}
    \mathcal{L}_\text{sup}(\proto_{1:K}, g_{1:K})=\mathcal{L}_\text{rec\_sup}(\proto_{1:K}, g_{1:K}) +\mu\mathcal{L}_{\text{tv}}(\proto_{1:K}) + \nu\mathcal{L}_{\text{cont}}(\proto_{1:K}, g_{1:K}),
    \label{eq:l_sup}
\end{equation}
with $\mu$ and $\nu$ two hyperparameters equal to $1$ and $0.01$ respectively in all our experiments.

\paragraph{Initialization.} The learnable parameters of our model are (i) the prototypes, (ii) the encoder and (iii) the time warping and offset decoders. We initialize our prototypes with the centroids learned by NCC (resp. K-means) in the supervised (resp. unsupervised) case. Default Kaming He initialization~\citep{he2015delving} is used for the encoder while the parameters of both decoders are set to zero. This ensures that at initialization the predicted transformations are the identity.

\paragraph{Optimization.} Parameters are learned using the ADAM optimizer~\citep{kingma2014adam} with a learning rate of $10^{-5}$. We train our model following a curriculum modeling scheme~\citep{elman1993learning, monnier2020deep}: we progressively increase the model complexity by first training without deformation, then adding the time warp deformation and finally the offset deformation. We add transformations when the mean accuracy does not increase in the supervised setting and, in the unsupervised setting, when the reconstruction loss does not decrease, after 5 validation steps. Note that the contrastive loss is only activated at the end of the curriculum in the supervised setting.


\subsection{Handling missing data}
\label{sec:missing_data}

Our method, as presented in Section~\ref{sec:method}, is designed for uniformly sampled constant-sized time series. While satellite time series from PlanetScope are pre-processed to obtain such regular data, time series acquired by Sentinel 2 have at most a data point every 5 days due to a lower revisit frequency, and additional missing dates because of clouds or shadows. To handle such non-regularly sampled time series, the remote sensing literature proposes several gap filling methods~\citep{belda2020datimes, julien2019optimizing, bg-10-4055-2013}. Instead, since our method is distance-based, we propose (i) to filter the input data to prevent possible outliers and (ii) to only compare inputs and prototypes on time stamps for which the input is defined.

Let us consider a specific time series, acquired over a period of length $T$ but with missing data. We define the associated raw time series $\inputseq_\text{raw}\in \mathbb{R}^{T\times C}$ by setting zero values for missing time stamps and the associated binary mask $\mask_\text{raw}\in \{0,1\}^{T}$, equal to $0$ for missing time stamps and $1$ otherwise. We define the filtered time series $\inputseq$ extracted from $\inputseq_\text{raw}$ and $\mask_\text{raw}$ through Gaussian filtering for $t\in [1, T]$ by:
\begin{equation}
    \inputseq[t] = \frac{1}{\mask[t]}\sum_{t'=1}^T \mathcal{G}_{t, \sigma}[t'] \cdot \inputseq_\text{raw}[t'],
    \label{eq:gaussian_filtering}
\end{equation}
with
\begin{equation}
    \mathcal{G}_{t, \sigma}[t'] = \exp\Big(-\frac{(t'-t)^2}{2\sigma^2}\Big),
    \label{eq:gaussian_filter}
\end{equation}
where $\sigma$ is a hyperparameter set to 7 days in our experiments. We also define the associated filtered mask $\mask$ for $t\in [1, T]$ by:
\begin{equation}
    \mask[t] = \sum_{t'=1}^T \mathcal{G}_{t, \sigma}[t'] \cdot \mask_\text{raw}[t'],
    \label{eq:gaussian_filtering_mask}
\end{equation}
for $t\in [1, T]$ and with the same hyperparameter $\sigma$.

Using directly this filtered time series to compute our mean square errors would lead to large errors, because data might be missing for long time periods. Thus, we modify the losses $\mathcal{L}_\text{rec}$ and $\mathcal{L}_\text{rec\_sup}$ by replacing the reconstruction error between a time series $\inputseq$  and reconstruction $\recons$,
\begin{equation}
    \frac{1}{TC}\Big|\Big|\inputseq-\recons\Big|\Big|_2^2= \frac{1}{C}\sum_{t=1}^T\frac{1}{T}\Big|\Big|\inputseq[t]-\recons[t]\Big|\Big|_2^2,
\end{equation}
in Equations~(\ref{eq:lrec_unsup}) and~(\ref{eq:lrec_sup}) by a weighted mean squared error:
\begin{equation}
\frac{1}{C}\sum_{t=1}^T\frac{\mask[t]}{\sum_{t'=1}^T \mask[t']}\Big|\Big|\inputseq[t]-\recons[t]\Big|\Big|_2^2.
\label{eq:lrec_unsup_mask}
\end{equation}
This adapted loss gives more weight to time stamps $t$ corresponding to true data acquisitions. 

In Appendix~\hyperref[sec:appA]{A}, we justify these design choices and demonstrate that they result in superior performance when compared to alternative standard filtering schemes, both for our method and NCC.