\setcounter{table}{0}
\renewcommand{\thetable}{C\arabic{table}}
\setcounter{equation}{0}
\renewcommand{\theequation}{C\arabic{equation}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{C\arabic{figure}}
\section*{Appendix C - Prediction aggregation}
\label{sec:appC}

Pixel-wise methods in the scope of this paper do not leverage any spatial information or context. Thus, it is expected for whole-image based approaches like UTAE to reach better performance. However it is interesting to look for simple, yet effective fashions to aggregate pixel-wise predictions at the field level in a post-processing step. In this section, we aggregate predictions using (i) ground truth instance segmentation maps, (ii) sliding windows or (iii) instance segmentation maps obtained with Segment Anything Model~\citep{kirillov2023segment}. The following study is performed on PASTIS Fold 2.

\paragraph{Ground truth instance segmentation (GTI).} We can use the ground truth segmentation of agricultural parcels provided with PASTIS dataset to have an upper bound of what can be achieved in terms of field-level aggregation. For each given parcel, we use majority voting to assign to all pixels of the instance the corresponding label.  

\paragraph{Sliding patches (SW).} We assign to a pixel the majority label inside a patch of size 5$\times$5 centered on it.

\begin{figure}[t]
    \centering
    \begin{tabular}{ccccc}
    \qualitativemaps{39}
    \qualitativemaps{51}
    \qualitativemaps{58}
    (a) & (b) & (c) & (d) & (e)\\
    \end{tabular}
    \caption{
    \textbf{SAM for SITS instance segmentation.} For randomly selected SITS from Fold 2 test set of PASTIS (a), we first show the fine segmentation $\text{SAM}_\text{raw}$ obtained when intersecting all temporal SAM outputs (b). Then we show the filtered maps $\text{SAM}_\text{filt}$ (c) where dark grey pixels correspond to remaining pixels. We show the final SAM-based instance maps (d) where all remaining pixels are assigned to one of the filtered instance and compare to the ground truth instance map (e).} 
    \label{fig:visuinstancesam}
\end{figure}


\begin{figure*}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccccc}
    \qualitativepp{1}
    \qualitativepp{45}
    \qualitativepp{51}
    \multirow{2}{*}{(a) Input} & \multirow{2}{*}{(b) GT} & \multirow{2}{*}{(c) UTAE}  & \multirow{2}{*}{(d) \modelname} & (e) \modelname & (f) \modelname & (g) \modelname\\
    & & & & +SW & +SAM & +GTI
    \end{tabular}
    }
    \caption{
    \textbf{Qualitative comparisons of post-processing methods for field-level prediction aggregation.}~{We show predicted segmentation maps for UTAE image-based method (c) and compared it them to those obtained with our method with and without image-level post-processing methods (d-g) for randomly selected SITS from Fold 2 test set of PASTIS (a). Grey segments correspond to the \textit{void} class and are ignored by all methods.}}
    \label{fig:comparative_visu_super_pp}
\end{figure*}

\paragraph{Segment Anything Model (SAM).} SAM~\citep{kirillov2023segment} is an image segmentation model trained on 1 billion image masks. It is able to generate masks for an entire image or from a given prompt. Here, we use it off-the-shelf without any additional learning to generate instance segmentation maps for each image of a given SITS. Combining these possibly contradictory segmentation maps is not easy and is the subject of several related works~\citep{franek2010image, li2012segmentation, khelifi2016novel, lefevre2019generic}. As in~\citet{lefevre2019generic}, we first produce a fine segmentation $\text{SAM}_\text{raw}$ by intersecting all the obtained maps: two pixels $p_1$ and $p_2$ belong in the same instance in the final result if and only if they belong in the same instance for all images of the time series \textit{i.e.}:
\begin{equation}
    d(p1,p2)=0,
    \label{eq:sal_dist}
\end{equation}
with $d$ the number of images in the SITS where pixels $p_1$ and $p_2$ belong to different instances. Then we propose to only keep instances that are not empty when eroded with a 3$\times$3 kernel. We distinguish the \textit{filtered instances} from the \textit{remaining pixels} on these $\text{SAM}_\text{filt}$ filtered instance segmentation maps. Examples of $\text{SAM}_\text{raw}$ and $\text{SAM}_\text{filt}$ maps can be found on Figure~\hyperref[fig:visuinstancesam]{\ref*{fig:visuinstancesam}b} and~\hyperref[fig:visuinstancesam]{\ref*{fig:visuinstancesam}c} respectively. In Table~\ref{tab:sam_on_sub_pixels}, we show that our method is more accurate on pixels of filtered instances than on remaining pixels, confirming that these instances correspond to clear and consistent spatial structures. Finally, a remaining pixel $p$ is assigned to the closest filtered instance containing a pixel $p'$ that minimizes $d(p,p')$. Example of final SAM-based instance maps are shown on~\hyperref[fig:visuinstancesam]{\ref*{fig:visuinstancesam}d}. We again use majority voting to assign to all pixels of an instance the corresponding label.  

\begin{table}[!t]
  \centering
  \caption{\textbf{Quality of SAM instances.} We investigate performance of our method on all the pixels, on remaining pixels and on pixels in SAM filtered instances for Fold 2 test set of PASTIS.\\}
  \begin{tabular}{lcc}
  \toprule
    & OA & MA\\
    \midrule
    All                        & 75.2 & 60.1\\
    Remaining pixels           & 63.3 & 48.3\\
    Filtered instances         & 80.4 & 66.7\\
  \bottomrule
  \end{tabular}
  \label{tab:sam_on_sub_pixels}
\end{table}

\begin{table}[!t]
  \centering
  \caption{\textbf{Bridging the gap between pixel-wise and whole-image methods.} We investigate how post-processing approaches can help our pixel-wise method be on par with a whole-image method like UTAE~\citep{Garnot2021}.\\}
  \begin{tabular}{llcc}
  \toprule
    Method & Post-processing & OA & MA\\
    \midrule
    UTAE & None            & 83.8 & 73.7\\
    \modelname & None            & 75.2 & 60.1\\
    \modelname & SW              & 74.8 & 61.3\\
    \modelname & SAM             & 77.3 & 62.0\\
    \modelname & GTI    & 86.4 & 67.5\\
  \bottomrule
  \end{tabular}
  \label{tab:results_pp}
\end{table}

We now compare quantitatively and qualitatively the prediction aggregation methods described above applied to our supervised method to the whole-image based approach UTAE~\citep{Garnot2021} on Fold 2 test set of PASTIS. On Figure~\ref{fig:comparative_visu_super_pp}, see how such post-processing steps allow to leverage spatial context in order to remove noisy predictions. While SW seems to rather smooth the raw semantic predictions than actually aggregating them at the field-level, SAM leads to results that are visually close to those obtained when using the ground-truth instances. Quantitatively though, we observe in Table~\ref{tab:results_pp} a neat gap between SAM and GTI (9.1\% in OA and 5.5\% in MA) which encourage to search for better instance proposing methods. Still, SAM post-processing lead to a +2\% increase in both OA and MA, demonstrating that obtained segments are semantically consistent. Finally, using GTI, our approach outperforms UTAE by +2.6\% in OA but is behind by -6.2\% in MA. Here the post-processing especially help classify the majority classes.