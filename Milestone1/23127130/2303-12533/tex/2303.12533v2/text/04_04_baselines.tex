\subsection{Baselines} \label{sec:baselines}

We validate our approach in two settings: supervised (classification) and unsupervised (clustering). For each setting, we describe below the methods evaluated in this work.

\subsubsection{Time series classification}\label{sec:mehtsup}

The purpose of this section is to benchmark classic and state-of-the-art MTSC methods for crop classification in SITS data:
\begin{itemize}
\item\textbf{NCC~\citep{duda1973pattern}.} The nearest centroid classifier (NCC) assigns to a test sample the label of the closest class average time series using the Euclidean distance. We also report the extension of NCC with our method to add invariance to time warping and sequence offset, as well as adding our contrastive loss.
\item\textbf{1NN~\citep{cover1967nearest} and 1NN-DTW~\citep{seto2015multivariate}.} The first nearest neighbor algorithm assigns to a test sample the label of its closest neighbor in the train set, with respect to a given distance. This algorithm is computationally costly and, since the datasets under study typically contain millions of pixel time series, we search for neighbors of test samples in a random 0.1\% subset of the train set and report the average over 5 runs with different subsets. We evaluate the nearest neighbor algorithm using the Euclidean distance (1NN) as well as using the dynamic time warping (1NN-DTW) measure on the TimeSen2Crop dataset which is small enough to compute it in a reasonable time.
\item\textbf{SVM~\citep{cortes1995support}.} We trained a linear support vector machine (SVM) in the input space using scikit-learn library~\citep{scikit-learn}.
\item\textbf{Random Forest~\citep{ho1995random}.} We evaluated the performance of a Random Forest of a hundred trees built in the input space using scikit-learn library~\cite{scikit-learn}.
\item\textbf{MLSTM-FCN~\citep{karim2019multivariate}.} MLSTM-FCN is a two-branch neural network concatenating the ouputs of an LSTM and a 1D-CNN to better encode time series. We use a non-official PyTorch implementation\footnote{github.com/timeseriesAI/tsai} of MLSTM-FCN.
\item\textbf{TapNet~\citep{zhang2020tapnet}.} TapNet uses a similar architecture to MLST-FCN to learn a low-dimensional representation of the data. Additionally, \citet{zhang2020tapnet} learn class prototypes in this latent space using the softmin of the euclidian distances of the embedding to the different class prototypes as  classification scores. The official PyTorch implementation\footnote{github.com/kdd2019-tapnet/tapnet} is designed for datasets with a size range from 27 to 10,992 only, while our datasets contain millions of time series. Thus, based on the official implementation, we implemented a batch version of TapNet which we use for our experiments.
\item\textbf{OS-CNN~\citep{tang2020rethinking}.} The Omni-Scale CNN is a 1D convolutional neural network that has shown ability to robustly capture the best time scale because it covers all the receptive field sizes in an efficient manner. We use the official implementation\footnote{github.com/Wensi-Tang/OS-CNN} with default parameters. 
\item\textbf{MLP+LTAE~\citep{garnot2020lightweight}.} The Lightweight Temporal Attention Encoder (LTAE) is an attention-based network. Used along with a Pixel Set Encoder (PSE)~\citep{garnot2020satellite}, LTAE achieves good performances on images. To adapt it to time series, we instead use a MLP as encoder. We refer to this method as MLP+LTAE and we use the official PyTorch implementation\footnote{github.com/VSainteuf/lightweight-temporal-attention-pytorch} of LTAE. 
\item\textbf{UTAE~\citep{garnot2020satellite}.} In addition to SITS methods, we also report the scores of U-net with Temporal Attention Encoder (UTAE) on PASTIS dataset. This method leverages complete (constant-size) images. Since it can learn from the spatial context of a given pixel, this state-of-the-art image sequence segmentation approach is expected to perform better than pixel-based MTSC approaches and is reported for reference.
\end{itemize}

\subsubsection{Time series clustering}\label{sec:methunsup}

In the unsupervised setting, we compare our method to other clustering approaches applied on learned features or directly on the time series:
\begin{itemize}
    \item\textbf{K-means~\citep{bottou1994convergence}.} We apply the classic K-means algorithm on the multivariate pixel time series directly. Clustering is performed on all splits (train, val and test). Then we determine the most frequently occurring class in each cluster, considering training data only. The result is used as label for the entire cluster. We use the gradient descent version~\citep{bottou1994convergence} K-means with empty cluster reassignment~\citep{caron2018deep, monnier2020deep}.
    \item\textbf{K-means-DTW~\citep{Petitjean2011}.} The K-means algorithm is applied in this case with a dynamic time warping measure instead of the usual Euclidean distance. To this end, we use the differentiable Soft-DTW~\citep{Cuturi2017} version of DTW and its Pytorch implementation~\citep{maghoumi2021deepnag}.
    \item\textbf{USRL~\citep{franceschi2019unsupervised} + K-means.} USRL is an encoder trained in an unsupervised manner to represent time series by a 320-dimensional vector. We train USRL on all splits of each dataset, then apply K-means in the feature space. We use the official implementation\footnote{\noindent github.com/White-Link/UnsupervisedScalableRepresentationLearni\-ngTimeSeries} of USRL with default parameters. 
    \item\textbf{DTAN~\citep{shapira2019diffeomorphic} + K-means.} DTAN is an unsupervised method for aligning temporally all the time series of a given set. K-means is applied on data from all splits after alignment with DTAN. We use the official implementation\footnote{github.com/BGU-CS-VIL/dtan} of DTAN with default parameters.
\end{itemize}
We evaluate all methods with $K=32$ clusters. We discuss this choice in Appendix~\hyperref[sec:appB]{B}.
