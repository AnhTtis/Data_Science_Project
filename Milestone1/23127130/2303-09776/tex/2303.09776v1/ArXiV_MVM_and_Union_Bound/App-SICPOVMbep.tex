% \textcolor{red}{\tiny JK: I rewrote this section to account for the fact that, as expressed, the argument only works for even $m$. I also articulated the logic and streamlined the proof.  TO DO: Coordinate the error probability notation in this section and the main body.}

Given an even natural number $k$, we can use the $M=2^k$ binary words (all possible sequences of $k$ {\it zeros} and {\it ones}) to (arbitrarily) label the   $N^2$ points of the SIC-POVM constellation for $N:=2^{k/2}$. 
 %\textcolor{gray}{As the constellation is equi-angular, we observe that $\Peb^{m' \vert m}$ is constant across all pairs $m' \neq m$.} 

We derive a formula linking the average symbol error probability $\bar{P}_{e|s}$ %, now denoted by $P_{e|s}$,
and the average bit error probability $\bar{P}_{e|b}$ for the transmission of a random binary sequence. 

Consider a randomly chosen single bit of the message to be transmitted. It will be randomly incorporated into the binary word of a sent symbol $s$, which we may assume without loss of generality to be the all-{\it zero} word. The probability that our bit is flipped if a specific erroneous $s' \neq s$ is received equals $\frac{k}{m}$ where $k$ is the number of bits {\it one} in the word of $s'$.
 %Since each of the $M-1$ such $s'$ appears independently with probability $\frac{\Pe}{(M-1)}$,
The  average bit error probability is obtained by summing over all  $M-1$ such $s'$, each  appearing independently with probability
$\frac{\Pe}{(M-1)}$, and thus equals 
\begin{equation}
  \label{eq:singleBitFlipProb}
  	\Pbit = \sum_{k=1}^m \binom{m}{k} \frac{k}{m} \frac{\Pe}{(M-1)} = \frac{2^{m-1}}{M-1} \Pe.
\end{equation}
To evaluate the sum we used the identity
\begin{equation}
\label{binomAux:eq}
\sum_{k=1}^m \binom{m}{k}\frac{k}{m} \frac{1}{2^m} = \frac{1}{2},
\end{equation}
which expresses the fact that, on average, 50\% of bits of $s'$ are flipped with respect to $s$.
(Of course, \eqref{binomAux:eq} is also equivalent to $\sum_{k=1}^m \binom{m}{k}k = m 2^{m-1}$, 
 a standard fact obtained by differentiating
$(1+x)^m=\sum_{k=1}^m \binom{m}{k}x^k$ and setting $x=1$.) 




% \textcolor{red}{\tiny JK: scrutinize how the formula above is used ... and generally think more bout the hypotheses underlying the annealing.}
% % Without loss of generality, assume that we send the all-zero word. 
% % Then an incorrect decision on our signal has an equal probability to lead to the remaining  $M-1$  signals.


% % There are    $\binom{m}{k}$ words that differ by k bits from the all zero-word. The average bit error probability is therefore
% % \begin{equation}
% % 	\Pbit=\left[\frac{1}{m(M-1)}\sum_{k=1}^m \binom{m}{k}k\right] \Pe .
% % \end{equation} 

% % We will show that the above expression can be rewritten as
% % \begin{equation}
% % 	\Pbit=\frac{2^{m-1}}{M-1} \Pe .
% % \end{equation} 

% % \begin{proof}
% % From the binomial expansion formula for $(1 + x)^m $ we have
% % \begin{equation}
% % 	(1+x)^m=\sum_{k=1}^m \binom{m}{k}x^k .
% % \end{equation} 
% % Taking the first derivative with respect to $x$ gives
% % \begin{equation}
% % 	\frac{d}{dx}(1+x)^m=m(1+x)^{m-1}
% % 	=\sum_{k=1}^m \binom{m}{k}k x^{k-1} .
% % \end{equation} 
% % Upon setting $x=1$ we obtain
% % \begin{equation}
% % 	2^{m-1}m
% % 	=\sum_{k=1}^m \binom{m}{k}k.
% % \end{equation} 

% % We therefore obtain the desired result
% % \begin{equation}
% % 	\Pbit=\frac{2^{m-1}}{M-1} \Pe . \qedhere
% % \end{equation} 
% % \end{proof}

% \textcolor{red}{\tiny JK: Why repeat the union bound in different notation? Kill or Coordinate with main body?}

% \textcolor{gray}{
% From Proakis \cite{Proakis}, eq. (5-2-25), the union bound for the symbol error probability gives 
% \begin{equation}
% 	{P}_{e|s}\leq (M-1)P_{e|s|{\rm pair}} .
% \end{equation} 
% Substituting to the expressions for the average bit error probability, we obtain
% \begin{equation}
% 	\Pbit\leq 2^{m-1}P_{e|s|{\rm pair}} .
% \end{equation}}




% \bigskip
% \textcolor{red}{BELOW EXTRA DRAFT MATERIAL by JK with example analysis for all $N$. This is not really checked by JK and meant as an indication of reasoning one can make (short of full on FEC considerations).} 
% \bigskip

% For arbitrary $N \geq 1$ the above scenario can be generalized as follows.
% Let $m_-:=\lfloor \log_2 M \rfloor$  and randomly assign to each $m_-$-word a symbol of the SIC POVM. [We fix one such assignment but will take ensamble expected value over all assignment possibilities.] All utilized  {\it code symbols} will constitute at least 50\% of SIC POVMs since $M \leq 2^{m_+}$ where $m_+:=\lceil \log_2 M \rceil$ differs from $m_-$ by at most $1$.

% A random binary  data stream of large length $L$ is then divided into circa $L/m_-$ binary words of length $m_-$ and  each word is transmitted as the associated  symbol. If a code symbol $s$ is transmitted, with probability  $\Pe/(M-1)$ it will arrive as some $s' \neq s$. If $s'$ is not a code symbol, it is discarded and the transmission of the word is considered lost. % [or maybe better should be replaced with the code word of one of the nearest symbols (randomly selected)].
% (However, see the improvement  below.) %If $s'$ is a code symbol, its binary word is obviously different from the one sent and we are facing bit errors. 

% Consider a random bit of the data stream. It is one of the $m_-$ bits of a code word $w$. We may assume that $w$ is all {\it zero} bits. The probability that the symbol $s$ associated to $w$ is transmitted and recovered without error is $1-\Pe$. If a symbol error occurs, the receiver sees a symbol $s' \neq s$. There are two possibilities: $s'$  is one of the remainig $2^{m_-}-1$ code symbols (beside $s$) among the $M-1$ symbols different than $s$, which happens with probability  $(2^{m_-}-1)/(M-1)$); or $s'$ is not a code symbol, which happens with the complementary probability $(M-2^{m_-})/(M-1)$. Therefore the probability that our bit is lost or flipped is   
% \begin{equation}
%  \Prob(\text{bit error} | s') = \frac{M-2^{m_-}}{M-1} +  \frac{2^{m_-}-1}{M-1}\frac{k(s')}{m_-}
% \end{equation}
% where $k(s')$ is the number of {\it one} bits in $s'$. 
% The probability of a bit error is then evaluated by aggregating over all $s'\neq s$:
% \begin{align}
%   \Prob(\text{bit error})
%   &= \sum_{s' \neq s} \Prob(\text{bit error} | s')\Prob(s'|s) \notag \\
%   &= \sum_{s' \neq s}  \left(\frac{M-2^{m_-}}{M-1} +  \frac{2^{m_-}-1}{M-1}\frac{k(s')}{m_-}\right) \frac{\Pe}{M-1} \notag \\
%   &=\frac{M-2^{m_-}}{M-1}\Pe + \frac{2^{m_-}-1}{M-1} \frac{\Pe}{M-1} \sum_{s' \neq s}  \frac{k(s')}{m_-}.  %\notag
% \end{align}
% Now, since the assignement of words was random, the (encoding ensamble) expected proportion of {\it one} bits in the word associated to $s'$ is $1/2$, which is to say that the following holds (in the sense of ensamble expected values)  
% \begin{equation}
%   \frac{1}{M} \sum_{s'}  \frac{k(s')}{m_-} %= \frac{1}{M} \sum_{s'}  \frac{k(s')}{m_-}
%   =\frac{1}{2}.
% \end{equation}
% (Note that the summation above being over all $s'$ or all $s' \neq s$ is immatrial since $k(s)=0$.)
% Hence
% \begin{align}
%   \Prob(\text{bit error})
%   &=\frac{M-2^{m_-}}{M-1}\Pe + \frac{2^{m_-}-1}{M-1}\frac{M}{M-1}\Pe \frac{1}{2} \notag \\
%   &=\frac{2M^2-(2^{m_-}+3)M +2^{m_-+1}}{2(M-1)^2}\Pe.
% \end{align}

% \bigskip


% To add an obvious improvement, note that discarding a non-code symbol $s'$ can be profitably (but at some processing cost) modified. Suppose that when  $s'$ is not a code symbol we assign to it a random word $w'$.
% Then the probability that our bit is lost or flipped is a bit lower then before: 
% \begin{equation}
%  \Prob(\text{bit error} | s') = \frac{M-2^{m_-}}{M-1}\frac{1}{2} +  \frac{2^{m_-}-1}{M-1}\frac{k(s')}{m_-}
% \end{equation}
% Thus 
% \begin{align}
%   \Prob(\text{bit error})
%   &=\frac{M-2^{m_-}}{M-1}\Pe\frac{1}{2} + \frac{2^{m_-}-1}{M-1}\frac{M}{M-1}\Pe \frac{1}{2} \notag \\
%   &=\frac{M^2-(2^{m_-}+2)M + 2^{m_-+1}}{2(M-1)^2}\Pe.
% \end{align}
