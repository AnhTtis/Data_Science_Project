%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Fitting procedure}
\label{sec:setup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[t!]
\setlength\tabcolsep{8pt}
  \begin{center}
    \begin{tabular}{llcll}
      \toprule
      parameter & \textsc{Pythia8} setting & Variation range & \textsc{Monash} & Tune-3D \cite{Amoroso:2018qga} \\
      \midrule
      $\sigma_{\perp}$~[GeV] & \verb|StringPT:Sigma|   & 0.0 -- 1.0 & 0.335 & 0.3174 \\
      $a$              & \verb|StringZ:aLund|    & 0.0 -- 2.0 & 0.68 & 0.5999 \\
      $b$              & \verb|StringZ:bLund|    & 0.2 -- 2.0 & 0.98 & -- \\
      $\left<z_\rho\right>$       & \verb|StringZ:avgZLund| & 0.3 -- 0.7 & (0.55) & 0.5278 \\
      $a_{\rm DiQuark}$ & \verb|StringZ:aExtraDiquark| & 0.0 -- 2.0 & 0.97 & 0.97 \\
      \bottomrule
    \end{tabular}
  \end{center}
  \caption{\label{tab:ranges} Parameter ranges used for the \textsc{Pythia} 8 tuning,
    and their corresponding values in the \textsc{Monash} tune \cite{Skands:2014pea} and in a tune performed in \cite{Amoroso:2018qga}.}
\end{table}

In this study, we use \textsc{Pythia8} version 8.244 \cite{Sjostrand:2014zea} to generate Monte Carlo samples for different values of the string fragmentation function parameters assuming the \textsc{Monash} tune \cite{Skands:2014pea} as our baseline. The different measurements used in our tunings are implemented in the validation package \textsc{Rivet} version 3.1.3 \cite{Buckley:2010ar, Bierlich:2019rhm}. Frequentist-type tunings are performed using the optimisation tool \textsc{Professor} version 2.3.3 \cite{Buckley:2009bj}.  Analytical expressions for the physical dependence of the observables on the different parameters are derived by fitting the Monte Carlo predictions to a set of points in the parameter space (called anchor points). The best-fit points for the parameters are determined by a standard $\chi^2$-minimisation method -- \textsc{Minuit} \cite{James:1975dr} --  implemented in \textsc{Professor} and which uses the analytical polynomial 
interpolations.

The parameters of interest are four which are $a$, and $\langle z_\rho\rangle$ which controls the longitudinal momentum taking by the hadrons inside the QCD jets, $a_{\rm Diquark}$ which is mainly connected to production of baryons in QCD jets. Finally, the $\sigma$ parameter is connected to the mean transverse momentum taken away by a hadron in the fragmentation process (see Table \ref{tab:ranges} for their default values and their allowed range in \textsc{Pythia8}). Two baseline retunings are performed throughout this study: 
\begin{itemize}
    \item In the first tune, we fix the parameters $a, \langle z_\rho \rangle$ and $\sigma$ to the values derived in a previous study \cite{Amoroso:2018qga}, \emph{i.e.}
    \begin{eqnarray}
    \verb|StringZ:aLund| &=& 0.5999^{+0.2000}_{-0.2000}, \nonumber \\
    \verb|StringZ:avgZLund| &=& 0.5278^{+0.0270}_{-0.0230}, \\
    \verb|StringPT:sigma| &=& 0.3174^{+0.0420}_{-0.0370}, \nonumber
    \label{eq:tune2018}
    \end{eqnarray}
    and tune $a_{\rm Diquark}$ to a set of constraining measurements compromising of proton, and $\Lambda^0$ spectra. This tuning is called a one-dimensional tuning. The set of the measurements used in this optimisation part is summarised in Table \ref{tab:measurements:protons}. 
    
    \item In a second tune, we fit the four-dimensional parameter space using the measurements listed in Tables \ref{tab:measurements:protons}-\ref{tab:measurements:eventshapes}. The additional measurements used in the tunes include meson scaled momenta, event shapes, jet rates, and charged multiplicities, and identified particle multiplicities.
\end{itemize}

For comparison and as an extra check of the results of the frequensit fit, we further perform a Bayesian fit using \textsc{MultiNest} \cite{Feroz:2008xx}. To increase the precision of the posteriors we generate one thousand live points with a tolerance of $10^{-3}$. Since the probability distribution functions (PDFs) associated to data are unimodal Gaussian PDFs, we expect that the results of frequentist and Bayesian fits to have a perfect agreement. 

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.49\textwidth]{Figures/resval.pdf}
    \hfill
    \includegraphics[width=0.49\textwidth]{Figures/reserr.pdf}
    \caption{The distributions for the interpolated observable values ({\it left}) and their errors ({\it right}). These distributions are evaluated over all the 1000 MC runs and sum over all the observables used in the fit (defined in Tables \ref{tab:measurements:protons}-\ref{tab:measurements:eventshapes}). Here, we show the first order (red dotted), third order (blue), fourth order (cyan) and eighth order (magenta) interpolation polynomial. The residual for the central values of the observables show clearly that the eighth order polynomial performs better than the other interpolations. The residuals for the errors are independent of the order we used in the interpolations.}
    \label{fig:residuals}
\end{figure}


The goodness-of-fit is defined as 
\begin{equation}
 \chi^2 = \sum_{\mathcal{O}} \sum_{b\in \mathcal{O}} \bigg(\frac{f_{(b)}(\{p_i\}) - \mathcal{R}_b}{\Delta_b}\bigg)^2,
\label{eq:GoF}
\end{equation}
where $\mathcal{R}_b$ is the central value for the experimental measurement $\mathcal{O}$ at a bin $b$, $f_{(b)}(\{p_i\})$ is the analytical expression of the response function which is a polynomial of the parameters, and $\Delta_b$ is the total error. The number of degrees-of-freedom ($N_{\rm df}$) is the number of degrees-of-freedom which is defined as the number of measurements minus the number of independent parameters, {\it i.e.}
\begin{eqnarray}
N_{\rm df} = \sum_{\mathcal{O}} |b \in \mathcal{O}| - N_{\rm parameters}.
\label{eq:NDF}
\end{eqnarray}
We turn now to a brief discussion of the treatment of the errors ($\Delta_b$) used in the goodness-of-fit definition. Experimental errors are the quadratic sum of statistical and systematic uncertainties. Besides, we do not assume any correlations between the different measurements as this information is not provided by the experimental collaborations. The Monte Carlo uncertainties connected to the size of the samples used in our interpolations are summed in quadrature with the experimental errors. Finally, we add a flat $5\%$ uncertainty on each bin and for each observable which is used as a protection against overfitting effects and as sanity limit for the accuracy in both the perturbative (high order corrections,...) and non-perturbative (high twist terms, ...) regimes. We note that the introduction of this flat uncertainty will also reduce the value of $\chi^2/N_{\rm df}$ to be consistent with unity. The resulting variations around the best-fit points can reasonably defines conservative estimates of the QCD uncertainties on the predictions. The total error per bin $b$ is defined by
\begin{eqnarray}
\Delta_b = \sqrt{\sigma_{b, \rm exp}^2 + \sigma_{b, {\rm MC}}^2 + \sigma_{b, {\rm th}}^2},
\end{eqnarray}
with $\sigma_{b, {\rm th}} = 0.05 \times f_{(b)}(\{p_i\})$. The polynomial dependence of the true Monte Carlo response is parametrised as follows
\begin{eqnarray}
    f_{(b)}(\{p_i\}) &=& \alpha_0^{(b)} + \sum_{i=1}^4 \beta_i^{(b)} p_i + \sum_{i,j=1}^4 \gamma_{ij}^{(b)} p_i p_j + \sum_{i,j,k = 1}^4 \delta_{ijk}^{(b)} p_i p_j p_k + \sum_{i,j,k,\ell=1}^4 \epsilon_{ijk\ell}^{(b)} p_i p_j p_k p_\ell \nonumber \\
    &+& \sum_{i,j,k,\ell,m=1}^4 \zeta_{ijk \ell m}^{(b)} p_i p_j p_k p_\ell p_m + \sum_{i,j,k,\ell,m,n=1}^4 \theta_{ijk \ell m n}^{(b)} p_i p_j p_k p_\ell p_m p_n \\
    &+& \sum_{i,j,k,\ell,m,n,r=1}^4 \omega_{ijk \ell m n r}^{(b)} p_i p_j p_k p_\ell p_m p_n p_r + \sum_{i,j,k,\ell,m,n,r,s=1}^4 \rho_{ijk \ell m n r s}^{(b)} p_i p_j p_k p_\ell p_m p_n p_r p_s, \nonumber
    \label{eq:interp}
\end{eqnarray}
with $\alpha, \beta, \gamma, \delta, \epsilon, \zeta, \theta, \omega,~{\rm and}~\rho$ are the polynomial coefficients determined in the fit and $\{p_i\} = \{a, \langle z_\rho \rangle, \sigma_\perp, a_{\rm Diquark} \}$ are the parameters of the  Lund fragmentation function. The order of the polynomial function plays a crucial role in both the quality of the fits and the consistency of the interpolated results with the true MC response at the minimum of the model parameters. To see which polynomial is most suitable in our tune, we compute the distributions of for the interpolated values (called residuals) and their errors for few polynomial functions. The results of these are shown in figure \ref{fig:residuals} where we display the residuals for $1^{\rm st}$ order (dashed red), $3^{\rm rd}$ order (navy), $4^{\rm th}$ order (cyan) and $8^{\rm th}$ order (magenta) polynomial functions. We can see that the $8^{\rm th}$ order polynomial performs better than the others as most of the density of the residuals is within $5\%$ of the true MC response. Nevertheless, the $4^{\rm th}$ order polynomial interpolation has a good performance as well (we also checked that the differences between the predictions for the polynomials of order $4$ and $8$ interpolations at the minimum and found good agreement). We must stress out that using polynomials with orders higher than $4$ for the interpolation will cause additional overfitting effects. Therefore, we will use the $4^{\rm th}$ order interpolation polynomial throughout this study and set $\zeta = \theta = \omega = \rho = 0$ in equation \ref{eq:interp}. Finally, the distributions of the residuals for the errors are showing similar behavior which may be explained by the fact that we have used a large number of events for our MC sampling ($2$ million events per parameter point).