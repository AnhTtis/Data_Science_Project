\documentclass{article}
\usepackage[utf8]{inputenc}
%% text
\usepackage[english]{babel}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{xcolor}
\usepackage{authblk}
%% Math symbol
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{bbm}
\usepackage{subcaption}
%% MATLAB listing
\usepackage{listings}
\usepackage{verbatim}
% comment
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}
% Tree diagram
\usepackage{tikz}
\usetikzlibrary{trees}

% Algorithm
\usepackage{algorithm}
\usepackage{algpseudocode}
\newenvironment{claimproof}[1]{\par\noindent{Proof:}\space#1}{\hfill $\blacksquare$}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}{Lemma}

\newtheorem{notation}[theorem]{Notational Convention}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{solution}[theorem]{Solution}
\newcommand{\ve}{{\varepsilon}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\EE}{{\mathbb{E}}}
\newcommand{\Prob}{{\mathbb{P}}}
\newcommand{\bphi}{{\boldsymbol{\phi}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\tri}{{\triangle}}
\newcommand{\ceil}{\mathrm{ceil}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\del}{{\partial}}

\def\beq{ \begin{equation} }
\def\eeq{ \end{equation} }
\def\mn{\medskip\noindent}
\def\ms{\medskip}
\def\bs{\bigskip}
\def\bn{\bigskip\noindent}
\def\ep{\epsilon}
\def\nass{\noalign{\smallskip}}

\def\nams{\noalign{\smallskip}}
\def\square{\vcenter{\vbox{\hrule height .4pt
  \hbox{\vrule width .4pt height 5pt \kern 5pt
        \vrule width .4pt} \hrule height .4pt}}}
\def\eopt{\hfill$\square$}
\def\RR{\mathbb{R}}
\def\ZZ{\mathbb{Z}}
\def\eqd{\,{\buildrel d \over =}\,}
\def\var{\hbox{var}\,}
\def\cov{\hbox{cov}\,}
\def\hbr{\hfill\break}
\def\sqz{\kern-0.2em}


%% Shortcut
\newcommand{\E}{\mathbb{E}}
\newcommand{\indi}{\mathbbm{1}}


%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------
% \usepackage{titling} % Customizing the title section
% \setlength{\droptitle}{-4\baselineskip} % Move the title up

% \pretitle{\begin{center}\Huge\bfseries} % Article title formatting
% \posttitle{\end{center}} % Article title closing formatting
% \title{Inferring sub-population Structure in Branching Processes} % Article title
% \author{%
% \textsc{K.Leder and C.Wu}\thanks{A thank you or further information} \\[1ex] % Your name
% \normalsize University of Minnesota \\ % Your institution
% \normalsize \href{mailto:john@smith.com}{john@smith.com} % Your email address
% %\and % Uncomment if 2 authors are required, duplicate these 4 lines if more
% %\textsc{Jane Smith}\thanks{Corresponding author} \\[1ex] % Second author's name
% %\normalsize University of Utah \\ % Second author's institution
% %\normalsize \href{mailto:jane@smith.com}{jane@smith.com} % Second author's email address
% }
% \date{\today} % Leave empty to omit a date
% \renewcommand{\maketitlehookd}{%
% \begin{abstract}
% \noindent \blindtext % Dummy abstract text - replace \blindtext with your abstract text
% \end{abstract}
% }



\title{Using birth-death processes to infer tumor subpopulation structure from live-cell imaging drug screening data}
\author[1]{Wu C.}
\author[2]{Gunnarsson E.B.}
\author[3]{Myklebust E.M.}
\author[3,4]{K\"{o}hn-Luque A.}
\author[5,6]{Tadele D.S.}
\author[7,8,9]{Enserink J.M.}
\author[3,4]{Frigessi A.}
\author[2]{Foo J.}
\author[1]{Leder K.}

\affil[1]{%
Department of Industrial and Systems Engineering, University of Minnesota, Twin Cities, MN 55455, USA.}
\affil[2]{%
School of Mathematics, University of Minnesota, Twin Cities, MN 55455, USA.}
\affil[3]{%
Oslo Centre for Biostatistics and Epidemiology, Faculty of Medicine, University of Oslo, 0372 Oslo, Norway}
\affil[4]{%
Oslo Centre for Biostatistics and Epidemiology, Oslo University Hospital, Oslo, Norway}
\affil[5]{%
Department of Medical Genetics, Oslo University Hospital, 0424 Oslo, Norway}
\affil[6]{%
Translational Hematology and Oncology Research, Cleveland Clinic, Cleveland, OH 44131, USA}
\affil[7]{%
Department of Molecular Cell Biology, Institute for Cancer Research, Oslo University Hospital, Oslo, Norway}
\affil[8]{%
Centre for Cancer Cell Reprogramming, Institute of Clinical Medicine, Faculty of Medicine, University of Oslo, Norway}
\affil[9]{%
Section for Biochemistry and Molecular Biology, Faculty of Mathematics and Natural Sciences, University of Oslo, Oslo, Norway}

\date{\today}

\graphicspath{{Plots/}}

\begin{document}
%% MATLAB setting
\lstset{language=Matlab,%
    %basicstyle=\color{red},
    breaklines=true,%
    morekeywords={matlab2tikz},
    keywordstyle=\color{blue},%
    morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},
    identifierstyle=\color{black},%
    stringstyle=\color{mylilas},
    commentstyle=\color{mygreen},%
    showstringspaces=false,%without this there will be a symbol in the places where there is a space
    numbers=left,%
    numberstyle={\tiny \color{black}},% size of the numbers
    numbersep=9pt, % this defines how far the numbers are from the text
    emph=[1]{for,end,break},emphstyle=[1]\color{red}, %some words to emphasise
    %emph=[2]{word1,word2}, emphstyle=[2]{style},    
}






\maketitle



\begin{abstract}
    Tumor heterogeneity is a complex and widely recognized trait that poses significant challenges in developing effective cancer therapies. Characterizing the heterogeneous subpopulation structure within a tumor will enable a more precise and successful treatment strategy for cancer therapy. A possible strategy to uncover the structure of subpopulations is to examine how they respond differently to different drugs. For instance, PhenoPop was proposed as a means to unravel the subpopulation structure within a tumor from high-throughput drug screening data. However, the deterministic nature of PhenoPop restricts the model fit and the information it can extract from the data. As an advancement, we proposed a stochastic model based on the linear birth-death process to address this limitation. Our model can formulate a dynamic variance along the horizon of the experiment so that the model uses more information from the data to provide a more robust estimation. In addition, the newly proposed model can be readily adapted to situations where the experimental data exhibits a positive time correlation. We concluded our study by testing our model on simulated data (\textit{In Silico}) and experimental data (\textit{In Vitro}), which supports our argument about its advantages.
\end{abstract}



\section{Introduction}

    
In recent years the design of personalized anti-cancer therapies has been greatly aided by the use of high throughput drug screens (HTDS) \cite{pauli2017personalized,grandori2018personalized}. In these studies a large panel of drugs is tested against a patient's tumor sample to identify the most effective treatment \cite{pemovska2013individualized,pozdeyev2016integrating,matulis2019functional,bonolo2020direct}. This exciting development has been brought about by technological breakthroughs in cell biology that have led to greater ease of growing patient tumor cells \textit{in vitro}, and robotics and computer vision improvements that have allowed for very large numbers of experiments to be run  and analyzed simultaneously.  After an HTDS is performed on a patient tumor sample, the clinician will see estimated cell counts after the tumor cells have been exposed to various drugs at a variety of doses. In principle, this information can then be used to guide the clinician's choice of therapy and dosage for the cancer patient; thus resulting in truly personalized cancer therapy. 

However, due to the evolutionary process by which they develop, tumors often harbor many different subpopulations with distinct drug-response characteristics by the time of diagnosis \cite{marusyk2012intra}. This tumor heterogeneity can confound results from HTDS since the combined signal from multiple tumor subpopulations results in a bulk drug sensitivity profile that may not reflect the true drug response characteristics of any individual cell in the tumor.  Small clones of drug-resistant subpopulations may be difficult to detect in a bulk drug response profile, but these clones may be clinically significant and drive tumor recurrence after drug-sensitive populations are depleted. As a result of the complex heterogeneities present in most tumors, care must be taken in the analysis and design of HTDS to ensure that beneficial treatments result from the HTDS. 

In our recent work \cite{kohn2022phenotypic} we have developed an method named PhenoPop that uses HTDS data from a single drug to study phenotypic substructure with respect to that drug. In particular we i) identify the number of phenotypically distinct subpopulations present, ii) characterize the relative abundance of those subpopulations at the beginning of the experiments, and iii) characterize each subpopulations dose response.

While the PhenoPop method has excellent performance there are some important theoretical and practical limitations. First, PhenoPop assumes only two levels of additive noise, limiting the explanation of data variance to only two levels of observation noise. However, when looking at experimental data, it is often clear that noise is proportional to cell count, (see Figure \ref{fig:ImatinibData}). Next, the PhenoPop method assumes that all observations are statistically independent. This is not a valid statistical assumption when studying data generated using live-cell imaging techniques where the same cellular population is studied at multiple time points; such a procedure will of course have observations that are correlated in time. Our goal in this work is to introduce an improved version of PhenoPop that addresses these issues by using linear birth-death processes to build our model. By using these stochastic processes we automatically get an improved variance structure that is proportional to the cellular population count. In addition, the linear birth-death process model has a natural correlation structure that can be used to model live-cell imaging experiments. We find that this improved version is able to more accurately model correlated data, giving estimators with smaller confidence intervals, and more accurately estimating small subpopulations. This continues a long tradition of using linear birth-death processes to model the dynamics of growing cellular populations \cite{pakes2003ch,kimmelbranching,Durrett}.

\begin{figure}[ht]
    \centering
    \includegraphics[scale = 0.175]{figure_sensitive_resistant_cells_without_outliers.jpg}
    \caption{Imatinib-sensitive and resistant Ba/F3 total cell count DATA. Data trajectories were collected at 14 different time points shown on the x-axis. The different trajectories correspond to different imatinib concentrations, as indicated in the legend. Note that in this figure the outliers due to imaging artifacts were removed.}
    \label{fig:ImatinibData}
\end{figure}

The rest of the paper is organized as follows. In Section \ref{sec:Model}, we review the existing PhenoPop method and introduce the novel stochastic model. Based on the stochastic model, we propose end-points and live cell image methods corresponding to two different assumptions of data. In Section \ref{sec:Experiment}, we conduct a comprehensive investigation of our newly proposed methods and compare them with the PhenoPop method on both \textit{in silico} and \textit{in vitro} data. Finally, we conclude the results from the experiments and discuss the insight about the new model in the Section \ref{sec:Discussion}
    


\section{Data and Model Formulation}

\label{sec:Model}
In this work, we are using drug response data of total cellular populations to infer the presence of subpopulations with different drug sensitivities. The term `total cellular population' refers to the aggregate of all subpopulations within the tumor. We will assume that our data is cell counts at a specified collection of times and drug concentrations for a specific drug being studied. In particular, we will assume that the cell counts are observed under drug concentrations $\mathcal{D}=\{d_1,\ldots, d_{N_D}\}$ and the cell count is taken at times in the set $\mathcal{T}=\{t_1,\ldots, t_{N_T}\}$. Furthermore, we will assume that for each dose-time pair, there are $N_R$ statistically identical and independent replicates. We will denote the observed cell count of replicate $r$ at dose $d$ and time $t$ by $x_{t,d,r}$.  We will denote our total dataset by
$$
\mathbf{x}=\left\{x_{t,d,r}; t\in \mathcal{T}, d\in \mathcal{D}, r\in\{1,\ldots, N_R\}\right\}.
$$
\subsection{PhenoPop}
In \cite{kohn2022phenotypic} we introduced a statistical framework for identifying the subpopulation structure based on drug screen measurements of the entire population, we referred to the resulting method as PhenoPop. Here we will briefly review the statistical framework and the resulting method.
We define the Hill equation with parameters $(b,E,m)$ as 
$$
H(d;b,E,m)=b+\frac{1-b}{1+(d/E)^m},
$$
this function will be used to model the effect of dose, $d$, on cellular growth rate. The Hill equation is a usual way to fit the logarithmic drug dose-response relationship \cite{di2015automated}. Specifically, $b$, $E$, and $m$ represent the maximum effect, the midpoint of the effect, and the rate of change at the midpoint, respectively. To model the negative drug effect, we usually assume that $0<b<1$ and take a logarithm of the Hill equation. Assuming a homogeneous population growing exponentially with initial size $C_0$ and growth rate $\alpha$ in the absence of a drug, we model the population at time $t$ under $d$ units of the drug given by
$$
C_0\exp\left[t\left(\alpha+\log(H(d;b,E,m))\right)\right].
$$

The expression in the previous display assumes that the cellular population is homogeneous. However, most cellular populations have some heterogeneity present. To include this heterogeneity assume that there are $S$ distinct subpopulations and that under $d$ units of the drug, the number of cells in population $i$ at time $t$ is given by 
$$
f_i(t,d)=f_i(0)\exp\left[t\left(\alpha_i+\log(H(d;b_i,E_i,m_i))\right)\right].
$$ 
In the above display $\alpha_i$ is the subpopulation specific cellular growth rate in the absence of the drug and $H(\cdot;b_i,E_i,m_i)$ is the subpopulation specific dose response Hill function, and we denote it as $H_i(\cdot)$ for simplicity. The initial population size is $f_i(0)=np_i$ where $n$ is the known initial total population and $p_i$ is the unknown initial fraction of population $i$.
We can then write the total population at time $t$ by
$$
f(t,d)=\sum_{i=1}^Sf_i(t,d)=n\sum_{i=1}^Sp_i\exp\left[t\left(\alpha_i+\log(H_i(d))\right)\right].
$$
In order to develop a statistical model for the observed data $\mathbf{x}$, we assume independent additive mean zero Gaussian noise for each observation. Based on the variance in the data, PhenoPop assume a high variance when there is low drug concentration and the cells are allowed to grow for a long time and otherwise use a lower variance. In particular, we define the variance as
$$
\sigma_{hl}^2(t,d)=\begin{cases}\sigma_H^2,&\enskip t\geq T_L\mbox{ and }d\leq D_L\\
\sigma_L^2,&\enskip\mbox{ otherwise.}\end{cases}
$$

As a result, the statistical model of observation $x_{t,d,r}$ is given by
$$
x_{t,d,r}=f(t,d)+Z^{(r)}(t,d),
$$
where $\{Z^{(r)}(t,d);r\in\{1,\ldots, N_R\}\}$ are independent random variables with normal distribution $N(0,\sigma_{hl}^2(t,d))$.  

This model has the parameter set
$$
\theta_{PP}(S)=\left\{(p_i,\alpha_i,b_i,E_i,m_i),\sigma_H,\sigma_L;i\in\{1,\ldots,S\}\right\}.
$$
In particular, we do not know the initial fractions of the $S$ subpopulations $\{p_i:i\in\{1,\ldots S\}\}$ and the parameters governing each subpopulations dose response curve $\{(\alpha_i,b_i,E_i,m_i):i\in\{1,\ldots,S\}\}$. In addition, we assume no prior knowledge about the observation noise, which means we do not know the value of $\sigma_H$ and $\sigma_L$. However, for simplicity, we assume the threshold $T_L$ and $D_L$ are known for simplicity in contrast to our previous work \cite{kohn2022phenotypic}. 

The goal of the PhenoPop algorithm is to use the data $\mathbf{x}$ to estimate the unknown parameters $\theta_{PP}(S)$, and in fact to also estimate the number of subpopulations $S$. It employs the well-established maximum likelihood estimation process to estimate the parameters $\theta_{PP}(S)$. The likelihood function for the parameters $\theta_{PP}(S)$ given the observations $\mathbf{x}$ is given by
\begin{equation}
\label{eq:PhenoPop Likelihood}
L_{PP}\left(\theta_{PP}(S)|\mathbf{x}\right)=\prod_{r=1}^{N_R}\prod_{(t,d)\in\mathcal{T}\times\mathcal{D}}\frac{1}{\sqrt{2\pi\sigma^2_{hl}(t,d)}}\exp\left[-\frac{\left(x_{t,d,r}-f(t,d)\right)^2}{2\sigma^2_{hl}(t,d)}\right].
\end{equation}
% In \cite{kohn2022phenotypic} we optimized the above likelihood function to derive a maximum likelihood estimator (MLE) for the unknown parameters $\theta_{PP}(S)$. Note that in the above expression, the right hand side depends on the parameter set $\theta_{PP}(S)$ via the function $f(t,d)$.
The Likelihood function describes the probability of obtaining the observation $\mathbf{x}$ with respect to the parameter set $\theta_{PP}$. PhenoPop employs various assumptions about the number of subpopulations $S$ to estimate the number of subpopulations from the data. It then plots the negative log-likelihood against $S$, which is typically a decreasing function as the number of parameters increases. Finally, PhenoPop applies the elbow method to determine the estimated value of $S$, where the negative log-likelihood no longer decreases significantly.

There are two shortcomings of the PhenoPop algorithm that we will address in the current manuscript. First, the PhenoPop algorithm only allows for two levels of noise in cell count observations. This is an unnatural restriction since there is no physical reason that there should only be two possible variance levels. A more natural assumption is to assume that noise is proportional to cell counts, and in fact, in Figure \ref{fig:ImatinibData} we see that the noise levels seem proportional to the cell counts. The second shortcoming is that PhenoPop assumes all observations are statistically independent. However, if the cell counts are performed using a live-cell imaging technique then observations of the same well at different times will of course be positively correlated. As we will see we can simultaneously address both of these issues by modeling the cellular populations with a stochastic process.



\subsection{Linear Birth-Death Process}
We extend the PhenoPop method of \cite{kohn2022phenotypic} by using a linear birth-death process, which is a stochastic process, to model the cell population dynamics. In the model, a sub-type $i$ cell divides into two cells at rate $\beta_i \geq 0$ and dies at rate $\nu_i \geq 0$. In particular, a type $i$ cell has a probability of $\beta_i \Delta t$ for dividing into two cells and a probability of $\nu_i \Delta t$ for dying within an infinitesimally short time interval of length $\Delta t > 0$. Using this stochastic model has two important benefits, first, it allows for a more complex variance model, and second, it gives a natural time based correlation structure to the observations. 

As in the PhenoPop method, we start with the assumption that there are $S$ distinct subpopulations. We assume that cells in subpopulation $i$ have birth rate $\beta_i$, and death rate
$$
\nu_i(d) = \nu_i - \log(H_i(d))=\nu_i-\log\left(b_i+\frac{1-b_i}{1+(d/E_i)^{n_i}}\right).
$$
Note that we are in effect assuming the drug under study acts via a cyto-toxic mechanism, i.e., higher doses lead to a higher death rate. This assumption is done purely for convenience and one could easily adapt this model to the study of a cyto-static drug where higher doses lead to lower cell division rates.

If we denote the number of cells in subpopulation $i$ by $\{X_i(t,d);t\geq 0\}$, we can calculate 
\begin{equation}
\label{eq:BD_Mean}
E[X_i(t,d)]\doteq n p_i \mu_i(t,d)=n p_i e^{\lambda_i(d)t}
\end{equation}
\begin{equation}
\label{eq:BD_Variance}
\Var[X_i(t,d)]\doteq n p_i\sigma^2_i(t,d)=n p_i\frac{\beta_i+\nu_i(d)}{\lambda_i(d)}\left(e^{2\lambda_i(d)t}-e^{\lambda_i(d)t}\right),
\end{equation}
where $\lambda_i(d)=\beta_i-\nu_i(d) \neq 0$ is the net growth rate under $d$ units of drug. Note that if we let $\beta_i - \nu_i = \alpha_i$, it is easy to see that $\lambda_i(d) = \alpha_i + \log(H_i(d))$ is the drug affected growth rate in PhenoPop method. We then denote the total population by 
$$
X(t,d)=\sum_{i=1}^SX_i(t,d).
$$
For what follows we define the mean and variance of the total population process as
\begin{align*}
E[X(t,d)]\doteq\mu(t,d)&=\sum_{i=1}^Snp_i\mu_i(t,d)\\
\Var[X(t,d)]\doteq n \sigma^2(t,d)&=n\sum_{i=1}^Sp_i\sigma^2_i(t,d).
\end{align*}
Note that the mean is the same as in PhenoPop, but the variance is of course now a more complex function. As PhenoPop, we assume independent additive mean zero Gaussian noise for each observation. However, the variance from the total population process in the linear birth-death model enables us to assume constant variance for observation error at each data point.

As a result, the statistical model for the observation is 
\begin{align}
\label{eq:stat_model}
x_{t,d,r}=X^{(r)}(t,d)+Z_{t,d,r},
\end{align}
where $\{Z_{t,d,r};d\in\mathcal{D},t\in\mathcal{T},r\in\{1,\ldots,N_R\}\}$ are i.i.d. random variables with normal distribution $N(0,c^2)$. The random variables $Z_{t,d,r}$ are used to represent independent observation noise, which is independent of $X^{(r)}(t,d)$. 

We then have the following parameter set
$$
\theta_{BD}(S)=\left\{\left(p_i,\beta_i,\nu_i,b_i,E_i,m_i\right),c;i\in\{1,\ldots,S\}\right\}.
$$
In comparison with PhenoPop, we now have an extra parameter per subpopulation but three fewer parameters for describing the observation noise.

A challenging aspect of this new model will be evaluating the likelihood function:
\begin{align}
\label{eq:BD_LikelihoodFun}
L_{BD}\left(\theta_{BD}(S)|\mathbf{x}\right)&=P\left(X^{(r)}(t,d)+Z_{t,d,r}=x_{t,d,r}, d\in\mathcal{D},t\in\mathcal{T},r\in\{1,\ldots,N_R\}|\theta_{BD}(S)\right)\\
&=
\prod_{r=1}^{N_R}\prod_{d\in\mathcal{D}}P\left(X^{(r)}(t,d)+Z_{t,d,r}=x_{t,d,r},t\in\mathcal{T}|\theta_{BD}(S)\right),\nonumber
\end{align}
where in the above equality we assume that observations at different doses are independent and that replicates are independent.

\subsubsection{End-Point Experiments}
In many experimental methods cells are only observed at a single time point. As a result, data points where cells are allowed to grow for different time periods are from different experiments and are assumed to be independent. For these experiments, we can thus write the likelihood function as
$$
L_{BD}\left(\theta_{BD}(S)|\mathbf{x}\right)=\prod_{r=1}^{N_R}\prod_{d\in\mathcal{D}}\prod_{t\in\mathcal{T}}P\left(X^{(r)}(t,d)+Z_{t,d,r}=x_{t,d,r}|\theta_{BD}(S)\right).
$$
The right hand side of the previous display is just the probability density function of a sum of a fixed number of linear birth-death processes and observation noise, and can therefore be evaluated exactly. For faster computation, we replace the exact likelihood function of the $X(t,d)$ with a Gaussian approximation. In particular, consider the centered and renormalized process
\begin{equation}
\label{eq:centered process}
    W_{n}(t,d)=\frac{1}{\sqrt{n}}\sum_{i=1}^S\left(X_i(t,d)-np_ie^{\lambda_i(d)t}\right).
\end{equation}

A straightforward application of the central limit theorem gives us the following result.
\begin{proposition}
\label{prop:EndPointCLT}
For $t>0$ and $d\geq 0$, 
$W_{n}(t,d)\Rightarrow N(0,\sigma^2(t,d))$, as $n\to\infty.$
\end{proposition}
Note that `$\Rightarrow$' means converge in distribution. We will not prove this result since it is a consequence of the more general Proposition \ref{prop:LiveImageCLT}.

Based on Proposition \ref{prop:EndPointCLT} we work with the likelihood function,
\begin{align}
\label{eq:EndPointsMethod}
L_{EP}\left(\theta_{BD}(S)|\mathbf{x}\right)=\prod_{r=1}^{N_R}\prod_{(t,d)\in\mathcal{T}\times\mathcal{D}}\frac{1}{\sqrt{2\pi(n\sigma^2(t,d)+c^2)}}\exp\left[-\frac{\left(x_{t,d,r}-\mu(t,d)\right)^2}{2\left(n\sigma^2(t,d)+c^2\right)}\right].
\end{align}
The right hand side in the previous display depends on the parameters $\theta_{BD}(S)$ via the mean and variance functions $\mu(t,d)$ and $\sigma^2(t,d)$. As in \cite{kohn2022phenotypic} we can maximize this expression over the parameter set $\theta_{BD}(S)$ to obtain maximum likelihood estimates of the model parameters. By using $L_{EP}$ instead of $L_{PP}$ we are increasing the number of model parameters to describe the cell growth dynamic. Note that when we use $L_{EP}$ we now have a more realistic model for the dynamic variance, and we assume all observations are independent and identically distributed. In principle, $L_{EP}$ is harder to optimize than $L_{PP}$ since the variance term now also depends on the model parameters. However, we find that the numerical optimization software we use (see Section \ref{sec: Environment setting}) is able to deal with this more complex dependence on the model parameters.

\subsubsection{Live-Cell Imaging Techniques}
A widely used technique in modern cell biology is live-cell imaging, which enables the counting of the cellular population at multiple time points. In particular, when using these methods there will be a positive correlation between cell counts at different time points. This is in contrast to the earlier discussed end-points method where we assume that measurements at different times are independent. As a result of these positive correlations we now must evaluate the likelihood function \eqref{eq:BD_LikelihoodFun} at multiple time points. In particular, we need to evaluate the likelihood of the form
\begin{equation}
\label{eq:LC path likelihood}
P\left(X^{(r)}(t,d)+Z_{t,d,r}=x_{t,d,r},t\in\mathcal{T}|\theta_{BD}(S)\right)
\end{equation}
for each $d\in\mathcal{D}$. Note that to ease notation we will temporarily suppress dependence on the dose. Unfortunately, due to the hidden subpopulations the total population process is not a Markov process and therefore the precise evaluation of the likelihood of a trajectory is a computationally intensive task. We discuss the detail of our attempt on evaluating the equation \eqref{eq:LC path likelihood} precisely in the Appendix \ref{appx:Exact Path likelihood}. As a result, we reduce the computational complexity of computing one joint likelihood to $\Omega(\min_{t\in \mathcal{T}} x_{t}^{2})$ many of transition probability computing, where the computation of transition probability contributes the main computational burden due to the factorial terms. Despite this, the computational workload for computing exact joint probability remains substantial because $\min_{t\in \mathcal{T}}x_t \approx 5000$, which requires 1 second to compute the likelihood once. 


To get a sufficient reduction in the computational burden of evaluating the likelihood function we use a Central Limit Theorem approach. We define the centered and re-scaled total population process $W_{n}(t)$ by removing the dependence of the drug in equation \eqref{eq:centered process}. Then define the observations of the centered process at all the time points by
$$
\mathbf{W}_{n}=\{W_{n}(t);t\in\mathcal{T}\}.
$$
By assuming the set $\mathcal{T}$, number of subpopulations $S$ and initial proportion $p_i$ of each sub-type $i$ are independent of the initial total cell count $n$, we derive the following approximation of $\mathbf{W}_{n}$:
\begin{proposition}
\label{prop:LiveImageCLT}
As $n\to\infty$,
$$
\mathbf{W}_{n}\Rightarrow \mathbf{Y}=\{Y(t);t\in\mathcal{T}\}\sim N(0,\Sigma),
$$
where the $(i,j)$ element of the covariance matrix $\Sigma$ is given by
$$
\Sigma_{i,j}=\sum_{\ell=1}^{\min(i,j)}\sum_{k=1}^{S}p_ke^{(\lambda_k t_i-\lambda_kt_\ell)}e^{(\lambda_kt_j- \lambda_kt_\ell)}e^{\lambda_kt_{\ell-1}}\sigma^2_k(t_\ell -t_{\ell-1}).
$$
\end{proposition}

In Appendix \ref{appx:LiveImageCLT Extension}, we relax our assumption that the initial proportion $p_i$ is independent of the initial total cell count, and a similar result still follows. In future work, we plan to relax the assumption that $\mathcal{T}$ is independent of $n$.
% Note that in practice, we will define the covariance vector by summing over all sub-populations. This is because in practice we only have $n<\infty$ and we cannot actually assume sub-populations have zero contribution to the variance.

We now re-introduce the dose dependence notation, and explicitly state how we can use the previous proposition to evaluate our likelihood function. As in the end-points method, we will let $c^2$ denote the variance of our observation noise. In addition, for each $d\in\mathcal{D}$ we define the $N_T\times 1$ vector 
$$
\mu(d)=\{\mu(t,d);t\in\mathcal{T}\}
$$
and the $N_T\times N_T$ identity matrix $I$. Based on Proposition \ref{prop:LiveImageCLT} we will then use the following approximation to calculate the likelihood in equation \eqref{eq:BD_LikelihoodFun}
$$
\mathbf{x}_{\cdot,d,r}=(x_{t,d,r};t\in\mathcal{T})\approx\mu(d)+N(0,n\Sigma(d))+N(0,c^2 I).
$$
We can now write down the likelihood function as
\begin{align}
\label{eq:LiveCellLikelihood}
L_{LC}\left(\theta_{BD}(S)|\mathbf{x}\right)=\prod_{r=1}^{N_R}\prod_{d\in\mathcal{D}}\frac{\exp\left[-\frac{1}{2}\left(\mathbf{x}_{\cdot,d,r}-\mu(d)\right)^\top\left(n\Sigma(d)+c^2 I\right)^{-1}\left(\mathbf{x}_{\cdot,d,r}-\mu(d)\right)\right]}{\left(\det\left(2\pi\left(n\Sigma(d)+c^2 I\right)\right)\right)^{1/2}}.
\end{align}
Importantly note that the computational complexity of evaluating the above likelihood is independent of $\min_{t\in\mathcal{T}}x_{t}$, so we do not run into the same problems as when we tried to evaluate the likelihood exactly. 
%The issue is that when we attempt to evaluate the likelihood of a trajectory exactly using the Markov property we need to consider all the paths the sub-populations might take that lead to the observed trajectory of the total population. However, when we use the central limit theorem based approach, we no longer rely on the Markov property and thus are no longer concerned with the paths of the sub-populations. Once we know that the entire trajectory is well approximated by a Gaussian vector we can simply write out the approximate likelihood function of the observed trajectory.

It should be noted that the likelihood function $L_{LC}$ is quite similar to $L_{EP}$ the only difference being the structure of the covariance matrix. In particular, for $L_{EP}$ we can view the covariance matrix of a cell count trajectory as being diagonal, whereas in the live-cell imaging case the covariance matrix is not a diagonal matrix. Accurately accounting for these time correlations will provide some benefits for the accuracy of the parameter inference. However, it does come at a cost. In particular, it is obviously more computationally difficult to calculate the inverse and determinants present in $L_{LC}$. As a result, the optimization of $L_{LC}$ can be more difficult than the optimization of $L_{EP}$.

\subsubsection{Accuracy of Gaussian Approximation}
%%%%%%%%%%%%%%%%%%%%%%%% We can include the covariance test for different drug dosage %%%%%%%%%%%%%%%%%%%%%%%%%
Proposition \ref{prop:LiveImageCLT} states that the centered process $\mathbf{W}_n$ is approximately Gaussian $N(0,n\Sigma(d))$. However, when we derive the likelihood function \eqref{eq:LiveCellLikelihood} we are in fact approximating the total cell number $\mathbf{X}(d)=\{X(t,d);t\in\mathcal{T}\}$ with a Gaussian distribution $N(\mu(d),n\Sigma(d) + c^2 I)$. Due to the normalization term $\sqrt{n}$ in $\mathbf{W}_{n}$, the later approximation is stronger than the result in Proposition \ref{prop:LiveImageCLT}. We numerically compare the distribution of $\mathbf{X}(d)$ and the correspondent Gaussian distribution to see if the approximation still holds. To compare these two distributions, we will use the energy distance as a metric of difference.


%\textbf{Energy distance and its background:}
%%%%%%%%% E-statistic citation need to check %%%%%%%%%%%%%
    Introduced in \cite{szekely2003statistics}, the energy distance is a metric on probability distributions. Since it was proven \cite{szekely2003statistics} to be twice Harald Cramer's distance \cite{Cramer1928}, energy distance has also been used in several statistical tests \cite{BARINGHAUS2004}. This metric is preferable because it is easily computed for multivariate distributions. For probability distributions $F$ and $G$ on $\mathbb{R}^d$ we define their energy distance as
$$
D(F,G) = \sqrt{2 \E[\|X - Y\|] - \E[\|X - X'\|] - \E[\|Y - Y'\|]}
$$
where all random variables are independent, $X$ and $X'$, and $Y$ and $Y'$ are distributed according to $F$ and $G$ respectively, and $\|\cdot\|$ denotes the Euclidean norm.

Note that for two sets of i.i.d. realization $\{X_1,\cdots, X_k\}, X_i \sim F, \{Y_1,\cdots, Y_m\},Y_i \sim G$, one can obtain the empirical energy distance by
\begin{align}
\label{eq:EmpiricalEnergy}
D_{E}(F,G) = \sqrt{\frac{2}{k m} \sum_{i = 1}^{k} \sum_{j = 1}^{m} \|X_i - Y_j\| - \frac{1}{k^2} \sum_{i = 1}^{k} \sum_{j = 1}^{k} \|X_i - X_j\| - \frac{1}{m^2}\sum_{i = 1}^{m} \sum_{j = 1}^{m} \|Y_i - Y_j\|}
\end{align}
As a metric, the empirical energy distance will converge to 0 if and only if $F = G$.

According to the Proposition \ref{prop:LiveImageCLT}, the centered process $\mathbf{W}_n$ will converge to $\mathbf{Y}\sim N(0,\Sigma)$ in distribution as $n\rightarrow \infty$. We now check whether the $\mathbf{X}(d)$ has distribution approximated by $N(\mu(d),n\Sigma(d) + c^2 I )$. We will denote the distribution of $\mathbf{X}(d)$ by $F_{BD}$ and the normal distribution $N(\mu(d),n\Sigma(d) + c^2 I)$ by $F_N$.  Let $\{X_i\}_{i=1}^k$ be $k$ i.i.d. samples from the distribution $F_{BD}$, and $\{Y_i\}_{i=1}^m$ by $m$ i.i.d samples from $F_N$. We can then compute $D_E(F_{BD},F_N)$ using \eqref{eq:EmpiricalEnergy}. In Figure \ref{fig:ED}, we plot $D_E(F_{BD},F_N)$ with varying initial cell counts. This plot shows a monotonic decrease in the empirical energy distance, indicating that the distribution of $\mathbf{X}(d)$ is growing closer to a Gaussian distribution as the number of initial cells increases.

\begin{figure}[ht]
    \centering
    \includegraphics[scale = 0.17]{ED_100000_noise_free.jpg}
    \caption{Empirical energy distance between the linear birth-death simulated data and multivariate normal distributed data with different initial cell counts $[10,20,50,100,500,1000]$. In order to obtain the concentrated result, we use $N_R=100000$ replicates each from both distributions, and the data consists of 7 time points $\mathcal{T} = [1,2,3,4,5,6,7]$. Without loss of generality, we assume no drug effect here. The parameter used to generated this experiment is $p_1 = 0.4629, \beta_1 = 0.9058,\nu_1 = 0.8101,p_2 = 0.5371, \beta_2 = 0.2785,\nu_2 = 0.2300$. The box-plot records the value from 10 identical experiments.}
    \label{fig:ED}
\end{figure}



\section{Numerical Results}
\label{sec:Experiment}

In this section, we investigate the accuracy of the inference when using $L_{LC}$ and $L_{EP}$. Furthermore, we compare these new methods with the inference from the existing PhenoPop likelihood function $L_{PP}$. To make this comparison, we use both simulated (\textit{in silico}) and experimental (\textit{in vitro}) data. In all of our data we assume that the observation at time $t=0$ is accurate, i.e., $x_{0,d,r} = n$. When we estimate the model parameters using  the likelihood function $L_{LC}$ we call the resulting method the `live cell image method', when we use $L_{EP}$ we will use the phrase `end-points method', and finally when we use $L_{PP}$ we will simply say the PhenoPop method. 


\subsection{Preliminary Definitions}
\label{sec: Environment setting}
Here we will define some of the basic algorithms and formulas we used in gathering our numerical results.

\subsubsection*{Data Generation:}

For simulated data, we will select the generating parameter set $\theta_{BD(G)}(S)$ uniformly at random from a subset of the parameter space given in the Table \ref{table: Generating_table_2_pop_illustrative}. Note that one can obtain the generating parameter set $\theta_{PP(G)}(S)$ from $\theta_{BD(G)}(S)$ directly by setting $\alpha_i=\beta_i-\nu_i$ for each subpopulation. Based on the parameter set $\theta_{BD(G)}(S)$, we generate the simulated data according to the statistical model specified in equation \eqref{eq:stat_model}. It is important to note that we collect data continually during the course of the experiment to replicate the live-cell imaging experiments, thus the data should be in the greatest agreement with the live cell image method. For the \textit{in vitro} data, we apply our methods to mixtures data of Imatinib-sensitive and -resistant Ba/F3 cells that are collected using the live-cell imaging technique, see the experimental methods section of \cite{kohn2022phenotypic}. 


\subsubsection*{Maximum likelihood estimation (MLE):}

The maximum likelihood estimation was conducted by minimizing the negative log-likelihood, subject to constraints that were placed on the range of each parameter. The optimization process to find the minimum point was based on the MATLAB Optimization Toolbox \cite{MatlabOTB} function fmincon with sequential quadratic programming (sqp) solver. Due to the non-convexity of the negative log-likelihood function, we performed the optimization starting from 100 uniformly sampled initial points within a feasible region. The feasible region sets limitations on the parameters based on prior knowledge about them. For simulation studies, the feasible region is given by Table \ref{table:OptimizationConstraintsInSilico}, and for the \textit{in vitro} data the feasible region is specified by Table \ref{table:OptimizationConstriantsInVitro}. Among all the resulting optima, we selected the one with the lowest negative log-likelihood as our estimated result. 

\subsubsection*{Bootstrapping:}

In the simulated experiments, we use bootstrapping to quantify the uncertainty in the MLE estimator. In particular, we generated $20$ independent replicates of data measured at 11 concentration values $\mathcal{D}$ and 13 time points $\mathcal{T}$ from the generating parameter $\theta_{BD(G)}(S)$ at the beginning of the experiment. Then we employ bootstrapping to randomly re-sample $13$ replicates from those $20$ replicates with replacement $100$ times. These $13$ randomly re-sampled replicates are assumed to be the whole data set sampled from `nature'. Based on $100$ re-sampled data sets, we perform the MLE process to obtain $100$ estimators for every generating parameter $\theta_{BD(G)}(S)$.


\subsubsection*{$GR_{50}$:}


As proposed in \cite{kohn2022phenotypic}, rather than directly look at the precision of estimating the vector $(p_i,\beta_i,\nu_i,b_i,E_i,m_i),$ we will focus on the precision of the estimation of initial mixture proportion $p_i$ and $GR_{50}$ for each subpopulation cell. The $GR_{50}$, introduced in \cite{Hafner2016}, is the concentration at which a drug's effect on cell growth is half the observed effect. Note that at the maximum concentration level, the drug may not reach its theoretical maximum effect. We denote the maximum dosage applied as $d_m$, and we define the half effect for subpopulation $i$ as $r_i = (\nu_i(d_m) + \nu_i(0))/2$. Then we can derive the explicit formula for the $GR_{50}$ for subpopulation $i$:
\[GR_{50} = E_i\left(\frac{1 - e^{\nu_i - r_i}}{e^{\nu_i - r_i} - b_i} \right)^{1/m_i}\]
When we assume that $S=2$, we denote the higher $GR_{50}$ as either the resistant $GR_{50}$ or $GR_{r}$, and the lower $GR_{50}$ as either the sensitive $GR_{50}$ or $GR_{s}$. We will refer to the parameter for the sensitive subpopulation and the resistant subpopulation, respectively, by the letters subscript $s$ and subscript $r$, e.g. $E_s$ and $E_r$.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 2022/8/24 %%%%%%%%%%%%%%%%%%%%%











%%%%%%%%% Use only one data %%%%%%%%







\subsection{\textit{In Silico} Experiments} 

% \subsubsection{Parameter setting:}
% \label{Parameter setting}
\textbf{Experimental Conditions:}
We assume the initial number of cells is $n = 1000$, and we set the initial number of each subpopulation cell by rounding $n p_i$ to the nearest integer for subpopulation $i$. We used the following drug concentration levels: 
$$
\mathcal{D} = [0, 0.0313, 0.0625, 0.1250, 0.2500, 0.3750, 0.5, 1.25, 2.5, 3.75, 5]
$$
and we collect the cell count data at the time points: 
$$
\mathcal{T}= [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36].
$$

%\textbf{Generating parameter:}

% We select the parameters uniformly from the intervals given in table \ref{Generating_table1}:

% 

% Such region for generating parameter was selected to satisfy
% \begin{itemize}
%     \item Concentration level $Conc$ cover the $GR_{50}$ of all sub-population cells 
%     \item The $GR_{50}$ for two distinctive sub-populations do not locate on the same interval of the concentration level $Conc$.
%     \item Each sub-population has or will have significant amount of population. Otherwise, it may be confused with the noise.
% \end{itemize}

\noindent\textbf{Optimization Constraints:}
When running the numerical optimization we restricted the parameters to ensure they were physically realistic.
Unless otherwise noted, we performed the optimization using 100 uniformly sampled initial points from the Table \ref{table:OptimizationConstraintsInSilico} to perform the optimization.

\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
         & $p_s$ &$p_r$& $\beta_{s,r}$ & $\nu_{s,r}$ & $b_{s,r}$ & $E_{s,r}$ & $m_{s,r}$ & $\sigma_L,\sigma_H$ & $c$  \\
         \hline
         Range & $[0,0.5]$ &$1-p_s$& $[0,1]$ & $[\beta-0.1,\beta]$ & $[0.27,1]$ & $[0,10]$& $[0,10]$ & $[0,2500]$ & $[0,10]$ \\
         \hline
    \end{tabular}
    \caption{Feasible interval for each parameter.}
    \label{table:OptimizationConstraintsInSilico}
\end{table}



\subsubsection{Illustrative example with 2 subpopulations}




\label{sec:Illustrative example}
We first present an illustrative example with 2 subpopulations using estimators based on $L_{LC}$ and $L_{EP}$, i.e., the live cell image method and end-points method. We focus on a scenario with one drug sensitive subpopulation and one drug resistant subpopulation. Therefore, we restrict the range of $E_{s},E_{r}$, which control the $GR_{50}$ of each subpopulation, separately  at low and high drug concentration intervals respectively.
The parameters $\theta_{BD(G)}(2)$ used to generate the data of this example are randomly selected from Table \ref{table: Generating_table_2_pop_illustrative}.

 \begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
         & $p_{s}$&$p_{r}$ & $\beta_{s,r}$ & $\nu_{s,r}$ & $b_{s,r}$ & $E_s$& $E_r$ & $m_{s,r}$& $c$  \\
         \hline
         Range & $[0.3 , 0.5]$ &$1-p_s$& $[0, 1]$ & $[\beta-0.1 , \beta]$ & $[0.8 , 0.9]$ & $[0.05 , 0.1]$  &$[0.75 , 2.5]$& $[1.5 , 5]$ & $[0, 10]$ \\
         \hline
    \end{tabular}
    \caption{Range for parameter generation of experiments with 2 subpopulations}
    \label{table: Generating_table_2_pop_illustrative}
\end{table}
In order to check the robustness of the estimator, we perform the MLE process on multiple bootstrapped data as described in \ref{sec: Environment setting} to obtain multiple estimators. The details of the experiment are provided in the caption of Figure \ref{fig:Illustrative}.




\begin{figure}[ht]
    \makebox[\textwidth][c]{\includegraphics[scale = 0.2]{Illustrative41.jpg}}
    \caption{Illustrative example to show the performance of the end-points method and live cell image method for estimating the initial proportion and $GR_{50}$ for 2 subpopulations. The subpopulation attribute $\theta_{BD(G)}(2)$ and observation noise $c$ used in this example is $p_s = 0.4856, \beta_s = 0.1163,\nu_s = 0.0176,b_s = 0.8262,E_s = 0.0674,m_s = 4.5404,p_r = 0.5144,\beta_r = 0.4624,\nu_r = 0.3978,b_r = 0.8062,E_r = 1.5776,m_r = 4.2002,c = 1.2103$. The pie chart shows the average estimation for initial proportion $p_s,p_r$, and the box-plot shows all estimators for $GR_{50}$. Different color represents the $GR_{50}$ for different subpopulation, and the outliers are also colored differently, i.e., red for sensitive $GR_{50}$ and blue for resistant $GR_{50}$. The vertical dash line in the box-plot represents the true $GR_{50}$ used to generate the data. Notice that in the box-plot, we use the median of our bootstrapped estimators, but in the pie-chart we use the mean of the estimators.} 
    \label{fig:Illustrative}
\end{figure}




In Figure \ref{fig:Illustrative} we show the results of the parameter estimation of the two newly proposed methods. We can see that both the live cell image method and end-points method can recover the initial proportion and $GR_{50}$ for both subpopulations accurately, i.e., the interquartile ranges (IQR) from the boxplot cover the true $GR_{50}$ values. 

Given that both methods are able to accurately estimate this one example we next look at a large number of simulated examples to ensure that the methods perform well in a wide range of parameter settings.
Specifically, we generated 30 random samples of $\theta_{BD(G)}^i(2)$ according to Table \ref{table: Generating_table_2_pop_illustrative}. We then compute the accuracy of estimation of $\{p_s,GR_{s},GR_{r}\}$ from PhenoPop method ($PP$), end-points method ($EP$), and live cell image method ($LC$). For $i \in \{1,\cdots,30\}$ and $j \in \{PP,EP,LC\}$ we will denote these estimates by 
\[\hat{\theta}_i^j = \left(\hat{p}_{i,s}^j, \hat{GR}_{i,s}^j, \hat{GR}_{i,r}^j\right)\]
We obtain these estimates by taking the mean of MLE estimators from 100 Bootstrapping data sets. Given the true parameter $\{p_{i,s},GR_{i,s},GR_{i,r}\}$ from $\theta_{BD(G)}^i(2)$, we then compute the error of estimation. For the metric of error, we use the absolute log ratio, i.e.,
\begin{equation}
    \begin{split}
    Er_{i}^{j}(p_s) &= \left\lvert\log\left(\frac{p_{i,s}}{\hat{p}_{i,s}^j} \right) \right\rvert\\
    Er_{i}^{j}(GR_{s}) &= \left\lvert\log\left(\frac{GR_{i,s}}{\hat{GR}_{i,s}^j} \right)\right\rvert\\
    Er_{i}^{j}(GR_{r}) &= \left\lvert\log\left(\frac{GR_{i,r}}{\hat{GR}_{i,r}^j} \right)\right\rvert.
    \end{split}
    \label{eq:Log_Ratio}
\end{equation}
These metrics are chosen for addressing the logarithmic scale of concentration level.
In Figure \ref{fig:Log_Ratio_Accuracy}, we present a boxplot of errors from 30 experiments.

\begin{figure}[ht]
    \centering
    \includegraphics[width = \linewidth]{Log_Ratio_Precision_30.jpg}
    \caption{The absolute log ratio accuracy plot for all three parameters estimated from all three methods. The x-axis is three different methods, and the y-axis is the absolute log ratio for each parameter.}
    \label{fig:Log_Ratio_Accuracy}
\end{figure}

We can observe that all three methods have good accuracy in estimating all three parameters. In addition, we have the error of estimating sensitive $GR_{50}$ higher than the error of estimating resistant $GR_{50}$. A possible reason is that in Table \ref{table: Generating_table_2_pop_illustrative}, we have the initial proportion of sensitive cell $p_s \in [0.3,0.5]$ so that the population of sensitive cells is less than the population of resistant cells, and as a result, there is less information available from the sensitive cells. Later experiments will also provide evidence to support this hypothesis. 

As seen in the result of Figure \ref{fig:Log_Ratio_Accuracy}, the newly proposed methods do not provide a notable advantage in the estimation accuracy compared to the existing PhenoPop method. However, if we look at the precision of the estimators we will notice a statistically significant difference. Specifically, we look at the confidence interval width as a measure of estimator precision, i.e., a narrower confidence interval width indicates a more precise estimator. In particular, for each of the 30 samples of $\theta_{BD(G)}(2)$, we collect the width of the $95\%$ confidence interval generated from 100 MLE estimators. It's important to mention that the MLE is conducted on all three methods using the same bootstrapping data. Due to the randomness in $\theta_{BD(G)}(2)$, we will compare the normalized confidence interval width, i.e., for each estimator we consider its confidence interval width divided by the sum of the confidence interval widths for all three methods. 
 

 
\begin{figure}[ht]
\begin{subfigure}{.33\textwidth}
  \centering
  % include first image
  \includegraphics[width=\linewidth]{CI_p_prctile.jpg}  
  \caption{Initial proportion}
  \label{fig:CI_p}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  % include second image
  \includegraphics[width=\linewidth]{CI_GR1_prctile.jpg}  
  \caption{Sensitive $GR_{50}$}
  \label{fig:CI_GR1}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{CI_GR2_prctile.jpg}
  \caption{Resistant $GR_{50}$}
  \label{fig:CI_GR2}
\end{subfigure}
\caption{Comparison of the normalized CI width for 3 methods. The x-axis is three different methods, and the y-axis is the normalized CI width. The significance bar is based on the Wilcoxon Test with significant level $*** \leq  0.001 \leq  ** \leq   0.01 \leq * \leq 0.05$.}
\label{fig:CI width}
\end{figure}



 

In Figure \ref{fig:CI width}, we compare the normalized confidence interval widths across our three methods for measuring the three quantities: initial proportion, sensitive $GR_{50}$ value, and resistant $GR_{50}$ value. First, note that for recovering the initial proportion the live cell image method has significantly narrower confidence intervals than the other two approaches. Additionally, there is a small but statistically significant difference between the confidence interval widths for the PhenoPop method and the end-points method. When estimating the sensitive $GR_{50}$ index we see that the live cell image method has significantly narrower confidence intervals than the other two methods. Furthermore, for this parameter, the end-points method has significantly narrower confidence intervals than the PhenoPop method. Finally, note that when estimating the resistant $GR_{50}$ value we see that there is a statistically significant difference in  CI widths among the three different approaches. More precisely, the CI widths of the live cell image method are the tightest among the three, and the CI width of PhenoPop is the widest among the three. It is worth mentioning that in at least 28 out of 30 experiments we conducted, the true parameters were located within the confidence intervals from all three methods. In summary, we see that the end-points and live cell image methods provide a significant improvement in estimator precision over PhenoPop when estimating all three quantities, and furthermore, the live cell image method has the best performance.






\subsubsection{Illustrative example with 3 subpopulation}

In this section, we would like to investigate the performance of three methods when inferring parameters for a mixture tumor with 3 subpopulations. In order to generate data with three distinct subpopulations, we modify the sampling range of $p_{s},p_{m}, E_s,E_m,E_r$ (see Table \ref{table: modified range}). Here we refer to the parameter for the moderate subpopulation by subscript $m$. We then sample other parameters in the range same as that in Table \ref{table: Generating_table_2_pop_illustrative}. 

 \begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
         & $p_{s},p_{m}$&$p_{r}$ &$E_s$& $E_m$ &$E_r$  \\
         \hline
         Range & $[0.167 , 0.333]$ & $1-p_{s} - p_{m}$& $[0.0313 , 0.0625]$  &$[0.25 , 0.375]$&$[1.25,2.5]$\\
         \hline
    \end{tabular}
    \caption{modified range of parameters}
    \label{table: modified range}
\end{table}



\begin{figure}[ht]
    \centering
    \makebox[\textwidth][c]{\includegraphics[scale = 0.2]{Illustrative_3pop.jpg}}
    \caption{Illustrative example to show the performance of three methods when estimating parameters from a mixture of three subpopulations. The parameters are given by $\theta_{BD(G)}(3)$ and the observation noise in this example is $p_s = 0.2135,\beta_s = 0.3214,\nu_s = 0.2773,b_s = 0.8782, E_s = 0.0344, m_s = 2.5998, p_m = 0.2718, \beta_m = 0.7334, \nu_m = 0.6776, b_m = 0.8506, E_m = 0.3558, m_m = 4.6055, p_r = 0.5147, \beta_r = 0.0683, \nu_r = 0.0253, b_r = 0.8614, E_r = 1.5764, m_r = 4.4706, c = 9.5209$. The pie chart shows the mean estimation for the initial proportion $p_s,p_m,p_r$, and the box-plot shows the estimation for $GR_{50}$ of each subpopulations respectively. Different colors represented the different $GR_{50}$, and the outliers for that estimation were also colored differently. The vertical dash line in the box-plot represents the true $GR_{50}$ used to generate the data. }
    \label{fig:Illustrative_3}
\end{figure}

In Figure \ref{fig:Illustrative_3}, we showed the \textit{in silico} experiment results of estimating initial proportion and $GR_{50}$ of 3 subpopulations from the mixture data. We can see that the end-points and live cell image methods provided more accurate estimates of the initial proportion for each subpopulation. On estimating the $GR_{50}$, we can see that all three methods have their IQR correctly covering the true $GR_{50}$. However, the estimation of the moderate subpopulation with $E_m = 0.3558$ is less precise than the estimation of the other two subpopulation, i.e., the IQR is wider. We suppose that is due to the confounding between the moderate subpopulation and the other two subpopulations. It is worth noting that in this illustrative example, we still apply the same dimension of the data and the same setting for optimization, which means the same amount of information and the same effort as the 2 subpopulation experiment. However, the model is of course more complicated when we have more subpopulations. This might be a reason why this example shows less precision and accuracy in estimation. In fact, we did observe an improvement in precision and accuracy as we increased the number of initial starting points to $300$ for optimization.

Overall, our conclusion is that all three methods can provide reasonable estimates of the true initial proportion and $GR_{50}$ of each subpopulation from mixture data of 3 subpopulations. However, when assuming the existence of 3 subpopulations, achieving equivalent levels of accuracy and precision as in experiments involving 2 subpopulations may require greater effort or more information.







\subsubsection{Algorithm Performance In Challenging Conditions}
\label{sec:Parameter recovering under the extreme case}
In our previous work \cite{kohn2022phenotypic}, we discovered three different scenarios that were challenging for the PhenoPop method. We investigate these three challenging scenarios in this section: high observation noise, small resistant fraction, and similar subpopulation sensitivity. Specifically, we evaluate the performance of PhenoPop, end-points, and live cell image methods in these scenarios.\\

\textbf{Observation noise:}

We first consider the scenario where there is a substantial amount of observation noise. It should be noted that in PhenoPop method variance affects the estimation of additive noise terms. However, in the end-points and live cell image methods, there is an underlying stochastic process governing the birth-death dynamics of the cells in addition to an additive noise term. As a result, the end-points and live cell image methods can have difficulty with data with high levels of observation noise. In particular, these methods may try to adjust the dose-response parameters to accommodate the high levels of noise, whereas PhenoPop can simply adjust the variance level of the additive noise to deal with high levels of observation errors.

Recall that when we generated data for Figures \ref{fig:Illustrative} - \ref{fig:CI width} we used $\theta_{BD(G)}(2)$ selected in the range from Table \ref{table: Generating_table_2_pop_illustrative} and required that $c\in [0,10]$. In addition, we assumed that the initial cell count is given by $n=1000$. This means the standard deviation is below $1\%$ of the initial cell count. This is in fact lower than the levels used in \cite{kohn2022phenotypic}; we use this lower level because there is already noise from the stochastic population dynamics. 

In Figure \ref{fig:ObserationNoise} we show the results of using the three methods in a higher noise setting. In particular, we generated data with additive noise set at $c=500$, while the rest of the parameters were chosen uniformly at random according to Table \ref{table: Generating_table_2_pop_illustrative}. In Figure \ref{fig:ObserationNoise} we see that all three methods can estimate the initial proportion with good accuracy and provide IQR that contains the true $GR_{s}$ values. However, compared with the estimation in Figure \ref{fig:Illustrative}, the estimation precision of the end-points method and live cell image method have degraded. In addition, we can observe that the IQR of the three methods have about the same width, which implies the advantage we observed in Section \ref{sec:Illustrative example} disappears under a high observation environment. %We did realize that the estimation will get worse as we increase the observation noise, i.e. the estimation for resistant GR-50 in the figure \ref{fig:ObserationNoise} is not as precise as in the figure \ref{fig:Illustrative}.


\begin{figure}[ht]
    \centering
    \makebox[\textwidth][c]{\includegraphics[scale = 0.2]{sup_No91.jpg}}
    \caption{Pie chart and box-plot to show the estimation result in the High observation noise experiment. In this experiment, we have $ p_s = 0.3690, \beta_s = 0.4380,\nu_s = 0.3422, b_s = 0.8398, E_s = 0.0813, m_s = 3.9647, p_r = 0.6310, \beta_r = 0.5320, \nu_r = 0.4767, b_r = 0.8674, E_r = 1.9793, m_r = 4.8357, c = 500$. The rest notions are same as Figure \ref{fig:Illustrative}}
    \label{fig:ObserationNoise}
\end{figure}

We next study the effects of observation noise on estimator accuracy in a more quantitative fashion. The results are shown in Figure \ref{fig:sup_noise_quant}. Due to one more dimension, i.e., observation noise, we only plot the average of the absolute log ratio in Figure \ref{fig:sup_noise_quant} rather than the boxplots of all results in Figure \ref{fig:Log_Ratio_Accuracy}. 

%Specifically, we again generate 30 random samples of $\theta_{BD(G)}(2)$ uniformly from the range in Table \ref{table: Generating_table_2_pop_illustrative} with different observation noise parameters $c\in \mathcal{C} =  \{100,200,300,400,500\}$. 


\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{sup_noise_quant_30.jpg}
    \caption{Quantitative $(p_s,GR_s,GR_r)$ estimation accuracy experiment when high observation noise involves. The metric of estimation error is the absolute log ratio. The mean absolute log ratio was obtained from 30 independent experiments with 30 randomly generated $\theta_{BD(G)}(2)$ sets. The value of the observation noise parameter, $c$, in these 30 generating parameter sets was assigned to 5 different values in the set $\mathcal{C} = \{100,200,300,400,500\}$ to generate the line plot. Three different line plots correspond to three different methods, as indicated in the legend.}
    \label{fig:sup_noise_quant}
\end{figure}
In Figure \ref{fig:sup_noise_quant} we can see that the estimation error increases for all three methods and that the three methods seem to have a similar response to increasing levels of noise.

We next want to look at how estimator precision, in terms of confidence interval width, behaves with increasing levels of observation noise. In particular, we compute 30 normalized widths of the confidence interval from the result in the quantitative experiment with two different values of $c$, namely $100$ and $500$, for all three methods. Then we record them in Figure \ref{fig:CI width 10 noise} and Figure \ref{fig:CI width 50 noise}.
% We generated 30 random samples of $\theta_{BD(G)}(2)$ according to Table \ref{table: Generating_table_2_pop_illustrative}, we then replaced the observation noise parameter $c$ by 100 and 500, i.e. the observation noise has standard deviation $c$ equal to $10\%$ and $50\%$ of initial cell count respectively. 

\begin{figure}[ht]
\begin{subfigure}{.33\textwidth}
  \centering
  % include first image
  \includegraphics[width=\linewidth]{10_noise_CI_p_30.jpg}  
  \caption{Initial proportion}
  \label{fig:10_noise_CI_p}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  % include second image
  \includegraphics[width=\linewidth]{10_noise_CI_GR1_30.jpg}  
  \caption{Sensitive $GR_{50}$}
  \label{fig:10_noise_CI_GR1}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{10_noise_CI_GR2_30.jpg}
  \caption{Resistant $GR_{50}$}
  \label{fig:10_noise_CI_GR2}
\end{subfigure}
\caption{Comparison of the normalized CI width under observation noise $c = 100$ for 3 methods. The x-axis is three different methods, and the y-axis is the normalized CI width. The significance bar is based on the Wilcoxon Test with significant level $*** \leq  0.001 \leq  ** \leq   0.01 \leq * \leq 0.05$. }
\label{fig:CI width 10 noise}
\end{figure}

\begin{figure}[ht]
\begin{subfigure}{.33\textwidth}
  \centering
  % include first image
  \includegraphics[width=\linewidth]{50_noise_CI_p_30.jpg}  
  \caption{Initial proportion}
  \label{fig:50_noise_CI_p}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  % include second image
  \includegraphics[width=\linewidth]{50_noise_CI_GR1_30.jpg}  
  \caption{Sensitive $GR_{50}$}
  \label{fig:50_noise_CI_GR1}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{50_noise_CI_GR2_30.jpg}
  \caption{Resistant $GR_{50}$}
  \label{fig:50_noise_CI_GR2}
\end{subfigure}
\caption{Comparison of the normalized CI width under observation noise $c = 500$ for 3 methods. The x-axis is three different methods, and the y-axis is the normalized CI width. The significance bar is based on the Wilcoxon Test with significant level $*** \leq  0.001 \leq  ** \leq   0.01 \leq * \leq 0.05$. }
\label{fig:CI width 50 noise}
\end{figure}

We can see that the confidence interval advantage of the live cell image method becomes less significant in estimating the Resistant $GR_{50}$ in Figure \ref{fig:CI width 10 noise}. Such degradation becomes more significant when we increase the observation noise to $c = 500$ as seen in Figure \ref{fig:CI width 50 noise}. Nevertheless, the advantage of estimation precision still remains significant when the standard deviation of observation noise is as large as $10\%$ of the initial cell count, while the standard deviation of observation noise reported from the common automated or semi-automated cell counting techniques ranges from $1-15\%$ \cite{CADENAHERRERA20159,mumenthaler2011evolutionary}. \\


\textbf{Small Resistant subpopulation:}


In our earlier results, we have constrained the initial proportion of the sensitive subpopulation to the interval $[0.3,0.5]$. This balanced initial condition ensures generating data with enough information about each subpopulation. We now modify this constraint and investigate the performance of the three methods with less balanced initial conditions, in particular, we constrain the initial proportion of the sensitive subpopulation to the interval $[0.85, 0.99]$. An illustrative example is shown in Figure \ref{fig:High proportion}.
\begin{figure}[ht]
    \centering
    \makebox[\textwidth][c]{\includegraphics[scale = 0.2]{high_proportion_10.jpg}}
    \caption{Pie chart and box-plot to show the estimation result in the unbalanced initial proportion experiment. In this experiment, we have $ p_s = 0.9900, \beta_s = 0.4301,\nu_s = 0.4199, b_s = 0.8644, E_s = 0.0768, m_s = 4.3186, p_r = 0.0100, \beta_r = 0.1458, \nu_r = 0.1258, b_r = 0.8565, E_r = 0.5348, m_r = 3.7518, c = 4.8400$. The rest notions are the same as Figure \ref{fig:Illustrative}.}
    \label{fig:High proportion}
\end{figure}
For both sensitive and resistant subpopulation, we can clearly see that the IQR of PhenoPop does not cover the true $GR_{50}$, whereas the IQR of the live cell image methods covers the true $GR_{50}$. The IQR of the end-points method does cover the true resistant $GR_{50}$, but only lightly touches the true sensitive $GR_{50}$. In addition, the end-points and live cell image methods have significantly narrower confidence intervals in estimating the $GR_{50}$. Finally, note that the estimate of the initial proportion of resistant cells is more accurate for the end-points and live cell image methods. 

These unbalanced initial conditions provide a significant amount of information about the dominant subpopulation. As a result, PhenoPop is still able to report information about the dominant subpopulation. However, for the minority subpopulation, there is really limited information available from the data, and in order to infer anything about the minority subpopulation it is necessary to use the more powerful end-points or live cell image methods.
We will now quantify this increased performance. 

We performed a quantitative experiment, same as in the high observation noise section to validate the observation that the end-points method and the live cell image method outperform PhenoPop in the presence of small resistant subpopulations. The results are shown in Figure \ref{fig:high_prop_Log_Ratio}.

%Specifically, we generated 100 random samples of $\theta_{BD(G)}(2)$ according to Table \ref{table: Generating_table_2_pop_illustrative}, we then discard the component corresponding to the initial proportion of sensitive cells. For each of the randomly sampled $\theta_{BD(G)}(2)$ we consider four possible values for the initial proportion of sensitive cells $\mathcal{P}=\{0.85,0.9, 0.95, 0.99\}$. 
%For each $p\in \mathcal{P}$ and $i\in \{1,\ldots, 100\}$ we compute the absolute log ratio $Er_i^j(p), Er_i^j(GR_s),Er_i^j(GR_r)$ defined in equation \eqref{eq:Log_Ratio}, and then we plot the mean absolute log ratio with respect to the $p\in \mathcal{P}$ in the Figure \ref{fig:high_prop_Log_Ratio}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{High_proportion_quant.jpg}
    \caption{Quantitative $(p_s,GR_s,GR_r)$ estimation accuracy experiment when limited resistant subpopulation information provided. The metric of estimation error is the absolute log ratio. The mean absolute log ratio was obtained from 100 independent experiments with 100 randomly generated $\theta_{BD(G)}(2)$ sets. The value of $p_s$ in these 100 generating parameter sets was assigned to 4 different values in the set $\mathcal{P} = \{0.85,0.90,0.95,0.99\}$ to generate the line plot. Three different line plots correspond to three different methods, as indicated in the legend.}
    \label{fig:high_prop_Log_Ratio}
\end{figure}

In Figure \ref{fig:high_prop_Log_Ratio}, we can observe that both the end-points method and the live cell image method have smaller errors than the PhenoPop method in the small resistant subpopulation scenario. Furthermore, we can see that as the initial proportion of sensitive cells increases the accuracy benefit of the live-cell and the end-points methods increases. Compared to Figure \ref{fig:Log_Ratio_Accuracy}, one may notice that the error of sensitive $GR_{50}$ estimation is now smaller than the error of resistant $GR_{50}$ estimation. This observation supports the conclusion we made in Section \ref{sec:Illustrative example} that the initial proportion of subpopulation will influence the precision of estimation of the $GR_{50}$ of that subpopulation. \\








\textbf{Similar subpopulation sensitivity}


In the previous 2 subpopulations experiments, we let the $GR_{50}$ of two subpopulations be at least 3 concentration intervals apart, i.e., there are at least 4 concentration levels between the $GR_{s}$ and $GR_{r}$. In this experiment, we would like to test whether these three methods can identify both $GR_s$ and $GR_r$ precisely if we allow $E_s$ and $E_r$ to be closer to each other. More specifically, we let the $GR_{50}$ of two subpopulations be located 1 or 2 concentration intervals apart or next to each other. These results are presented in Figure \ref{fig:Similar_GR50}, which is the case when $GR_{50}$ of two subpopulations are next to each other. The $\theta_{BD(G)}(2)$ of this experiment was generated from the range given in the Table \ref{table: Generating_table_2_pop_illustrative} except $E_s \in [0.05,0.1],E_r \in [0.15,0.5]$.

\begin{figure}[ht]
    \centering
    \makebox[\textwidth][c]{\includegraphics[scale = 0.2]{Close_GR33.jpg}}
    \caption{Pie chart and box-plot to show the estimation result in the similar subpopulation experiment. In this experiment, we have $ p_s = 0.3263, \beta_s = 0.8896,\nu_s = 0.8215, b_s = 0.8820, E_s = 0.0654, m_s = 3.8539, p_r = 0.6737, \beta_r = 0.0925, \nu_r = 0.0661, b_r = 0.8171, E_r = 0.1500, m_r = 3.6015, c = 7.6660$. The rest notions are same as Figure \ref{fig:Illustrative}.}
    \label{fig:Similar_GR50}
\end{figure}

In Figure \ref{fig:Similar_GR50}, we can see that all three methods can recover the parameters. Note that this experiment was conducted with the noise parameter less than $10$, which is the $1\%$ of the initial cell count. Therefore, the conclusion might be different from the conclusion in the paper \cite{kohn2022phenotypic}. In general, we can conclude that in this example, the IQR of all three methods may contain the true $GR_{50}$ when the $GR_{50}$ of two subpopulations are located at consecutive concentration intervals. We believe that this is good evidence for all three methods can distinguish the $GR_{50}$ from the mixture data generated by the linear birth-death process. 

For completeness, we also perform a quantitative experiment for this extreme scenario. The results are shown in Figure \ref{fig:Close_GR_Log_Ratio}

%As we did in the previous section, we randomly generate 80 $\theta_{BD(G)}(2)$ from Table \ref{table: Generating_table_2_pop_illustrative}. For a similar sub-population scenario, we would like to substitute the value of $E_r$ from the randomly generated $\theta_{BD(G)}(2)$ to control the position of resistant $GR_{50}$. Specifically, we consider 5 values of $E_r$, which are $\mathcal{E}=\{0.15,0.3,0.45,0.85,2.0\}$. Then, for each $i\in \{1,\cdots,80\}$ and $E_r \in \mathcal{E} $ we compute the $Er_i^j(p),Er_i^j(GR_s),Er_i^j(GR_r)$ defined in the equation \eqref{eq:Log_Ratio} and plot the mean of them for each $j\in \{PP,EP,LC\}$ with respect to $E_r\in \mathcal{E}$ in the Figure \ref{fig:Close_GR_Log_Ratio}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{Close_GR_quant.jpg}
    \caption{Quantitative $(p_s,GR_s,GR_r)$ estimation accuracy experiment when high observation noise involves. The metric of estimation error is the absolute log ratio. The mean absolute log ratio was obtained from 80 independent experiments with 80 randomly generated $\theta_{BD(G)}(2)$ sets. The value of $E_r$ in these 80 generating parameter sets was assigned to 5 different values in the set $\mathcal{E} = \{0.15,0.3,0.45,0.85,2\}$ to generate the line plot. Three different line plots correspond to three different methods, as indicated in the legend.}
    \label{fig:Close_GR_Log_Ratio}
\end{figure}

In Figure \ref{fig:Close_GR_Log_Ratio}, we can observe that the accuracy is improving as we increase the distance between the sensitive $GR_{50}$ and resistant $GR_{50}$, which is a reasonable observation because two subpopulations are getting more distinguishable. 
We also observe the live cell image method has the lowest mean error when estimating model parameters, but all three methods have similar degradation as the two subpopulations become more phenotypically similar.

% Besides, we also notice that the Log Ratio of estimating resistant $GR50$ is increasing when $E_2>0.5$. We consider this phenomenon may not be due to the noise of the experiment. One possible reason is that the concentration interval we select may not cover enough information about the resistant $GR50$ as we increase the value of $E_2$. Nevertheless, we can observe that the Log Ratio of resistant $GR50$ estimation is around $0.03$ when $E_2 = 0.45$, which is the lowest among all three parameters estimation. Thus, we suppose the estimation of resistant $GR50$ may reach its optimal precision when $E_2 = 0.45$ under the current concentration design. In general, we consider the error in this scenario is acceptable, i.e. the largest mean error is around $0.2$. This result means in most of the time, three models can successfully identify the correct concentration interval where the true $GR50$ locate from synthetic data as we observed in Figure \ref{fig:Similar_GR50}. 

 








\subsection{Method comparison with \textit{in vitro} Data}
It is important to compare the performance of our three methods on experimentally generated data. Here we use data from \cite{kohn2022phenotypic} that consisted of mixtures of Imatinib sensitive and resistant \textit{Ba/F3} cells, see \cite{kohn2022phenotypic} for further details on the experimental methods for generating the data.

The cells were exposed to 11 different concentrations of imatinib and were observed at 14 different time points. There are 14 independent replicates of data collected via a live-cell imaging technique with roughly 1000 initial total cell count. The [BF11, BF12, BF21, BF41] are 4 different data sets collected from the ratio setting between sensitive and resistant equal to $[1:1,1:2,2:1,4:1]$ respectively.


In the paper \cite{kohn2022phenotypic}, the authors saw that the PhenoPop method can accurately identify the initial proportion of sensitive cells and the concentration intervals of both subpopulations' $GR_{50}$ values. Therefore, in this paper, we mainly focus on the comparison experiment among all three methods. For the metric of performance, we use the Akaike Information Criterion (AIC) to estimate the prediction error. For a statistical model with parameters $\theta_{BD}(S)$ and likelihood function $\mathcal{L}(\theta|\mathbf{x})$, the AIC is given by
\begin{align*}
    AIC &= -2 log(\mathcal{L}(\theta^*|\mathbf{x})) + 2|\theta^*|
\end{align*}
where $\theta^*$ is the optimal solution obtained in MLE process and $|v|$ is the cardinality of the vector $v$. When comparing all three methods, the one with a lower AIC is preferred. 

\textbf{\textit{Ba/F3} data feasible region for optimization}

The optimization feasible region is the same as the feasible region used in paper \cite{kohn2022phenotypic} for \textit{Ba/F3} data, i.e.,
\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
         & $p$ & $\beta$ & $\nu$ & $b$ & $E$ & $m$ & $\sigma_{L},\sigma_{H}$& c \\
         \hline
         Range & $[0,1]$ & $[0,1]$ & $[\beta-0.06,\beta]$ & $[0.878,1]$ & $[0,50]$& $[0.001,20]$ & $[0,2500]$& $[0,100]$. \\
         \hline
    \end{tabular}
    \caption{Optimization Feasible region}
    \label{table:OptimizationConstriantsInVitro}
\end{table}


\begin{table}[ht]
    \centering
    \begin{tabular}{||c|c|c|c||}
         DATA & PP(AIC) & EP(AIC) & LC(AIC)   \\
         BF11& 28502 & 26300 & \textbf{25294} \\
         BF12& 30485 & \textbf{26816} & 27311 \\
         BF21& 27928 & \textbf{24064} & 24182 \\
         BF41& 28912 & \textbf{24066} & 24574 \\
    \end{tabular}
    \caption{AIC scores of three methods: PhenoPop method(PP), end-points method(EP), and live cell image method(LC). These scores are obtained from optimization with 500 randomly chosen feasible initial points. The feasible region is described in Table \ref{table:OptimizationConstriantsInVitro}.}
    \label{table:AICofBa/F3}
\end{table}



The AIC values of the end-points method (EP) and live cell image method (LC) in Table \ref{table:AICofBa/F3} are clearly lower than that of the PhenoPop method (PP), indicating that these methods are superior for fitting this data set. As discussed in the modeling section, the newly proposed model has a more sophisticated variance model and we believe this is the reason why they are able to provide a better fit to this dataset. Finally, we do see that the end-points method has superior AIC scores to the live cell image method for three out of four of the experimental conditions. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Therefore, this observation does not deliver a clear message about the difference between the end-points method and the live cell image method. 




\section{Discussion}
\label{sec:Discussion}

In this work, we have proposed two methods for analyzing data from heterogeneous cell mixtures. In particular, we are interested in the setting where a mixture of at least two distinct cell subpopulations is exposed to a given drug at various concentrations. We then use the dose response curve of the composite population to learn about the two subpopulations. In particular, we are interested in estimates of the different subpopulations' initial prevalence and also their distinct dose response curves. The challenge of this problem is that we do not observe direct information about the subpopulations, but instead only information about the dose response of the composite population. 

This work is an extension of our prior work in \cite{kohn2022phenotypic}. The novelty in the current work is that we introduce a more realistic variance structure to our statistical model. We create a new variance structure by building our model using linear birth-death processes. In particular, we model each subpopulation as a linear birth-death process with a unique birth rate and a unique dose-dependent death rate. The dose dependence of the death rate is captured using a 3-parameter Hill function. Our observed process is then a sum of independent birth-death processes. Our goal is then to estimate the initial proportion of the subpopulations, as well as their birth-rates and the parameters governing the dose response in their death rates.

Counting cells in \textit{in vitro} experiments can generally be conducted in one of two fashions. In the first approach, cell numbers can only be estimated at the end of the experiment because the mechanism for estimating cell numbers requires killing the cells. In the second approach, cells are counted via live imaging techniques and the cells can be counted at multiple time points. When dealing with multiple time point data from cells collected via the first approach we can assume that observations at different time points are independent because they are the result of different experiments. However, when dealing with data from the second approach we can no longer make that assumption because the cell counts at different time points are from the same population and there is a positive correlation between those measurements. As a result of this differing structure we develop two methods, one that assumes independent observations at each time point, and one that assumes all the time points for a given dose are correlated. Evaluating the likelihood function under this second approach is not trivial at first glance since it requires evaluating the likelihood function of a sample path of a non-Markovian process (the total cell count). We are able to get around this difficulty by using a central limit theorem argument to approximate the exact likelihood function with a Gaussian likelihood.

In this work we compared three different methods: PhenoPop method from \cite{kohn2022phenotypic}, end-points method (assumes measurements are independent in time), and live cell image method (assumes time correlations). We first performed this comparison using simulated data. We generate our data by simulating linear birth-death processes and then adding independent Gaussian noise terms to the simulations. Throughout our tests we were interested in the estimation of three features of the mixed population: initial proportion, $GR_{50}$ of the sensitive cells, and $GR_{50}$ of the resistant cells.
Our first test for the simulated data was to look at confidence interval width as a measure of estimator precision. In this study, we found that the live cell image method had significantly narrower confidence intervals than the other methods for estimating all three features. We next investigated the performance of our three methods in the setting of small resistant subpopulations, i.e., less than 15\% of initial cells are resistant. We find that in this small resistant fraction setting that the live cell image method provides a significant improvement in accuracy over the original PhenoPop method. Furthermore, this improvement increases as the initial fraction of resistant cells goes to zero. We also compare the performance of the methods for simulations with increased levels of additive noise and subpopulations with close dose response curves. In these simulations, we found that the two new methods perform comparably with the original PhenoPop method.



We also compared the three methods using \textit{in vitro} data. In particular, we used data from our previous work \cite{kohn2022phenotypic} that considered different seeding mixtures of imatinib sensitive and resistant tumor cells. We then used all three methods to fit this data and used AIC as a model selection tool. We found that live cell image and end-points methods had significantly better scores than PhenoPop for all four initial mixtures studied. Interestingly the end-points method had lower AIC scores for three out of the four mixtures studied even though this data was generated using live-cell imaging techniques. 

In our statistical model, there are several important features of cell biology that we have left out.  For example, one type of cell may transition to another type of cell via a phenotypic switching mechanism (see e.g., \cite{gunnarsson2020understanding}). We believe that our current methods should be able to handle this type of switching with little modification since the underlying stochastic model will be very similar, i.e., a multi-type branching process. Another way the cell types can interact is via competition for scarce resources as the populations approach their carrying capacity. These types of interactions will require new statistical models since the underlying stochastic processes will no longer be a linear birth-death processes. Another interesting direction of future work is to quantify the limits of when we can identify distinct subpopulations. For example, if the resistant subpopulation is present at fraction $\epsilon$, what observation set would allow us to identify the presence of this subpopulation? Finally our stochastic model assumed that the time between cell divisions were exponential, but this is of course a great simplifcation. At the cost of a more complex model it would be possible to incorporate states for the different stages of the cell cycle. We leave this open as a question for future investigation.




























    
    
\section{Appendix}



%%%%%%%%%%%%%%%%%%%% Proof of multi-type CLT %%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Proposition \ref{prop:LiveImageCLT}}
\label{appx:LiveImageCLT Proof}

Given a set of time points $\mathcal{T} = \{t_1,\cdots,t_k\}$, we first show for any $t_j, 1\leq j\leq k$ that:
\[W_{n}(t_j) \Rightarrow Y(t_j) =  \sum_{\ell = 1}^j \sum_{g=1}^S \sqrt{p_g} e^{\lambda_g (t_j - t_{\ell})} e^{\lambda_g t_{\ell - 1}/2}V_g(t_{\ell} - t_{\ell -1}) \text{ as } n \to \infty\]
Where $V_g(t)$ is a random variable that has normal distribution $N(0,\sigma_g^2(t))$ and the $\sigma_g^2(t)$ here is the variance of subpopulation $g$ linear birth-death process defined in equation \eqref{eq:BD_Variance}.
Following an argument from Either and Kurtz \cite{ethier2009markov}, we have the decomposition
\begin{align*}
    W_{n}(t_j) &= \frac{1}{\sqrt{n}} \sum_{g = 1}^{S} \sum_{\ell = 1}^{j} e^{\lambda_g (t_j - t_{\ell})} X_g(t_{\ell}) - e^{\lambda_g (t_j - t_{\ell - 1})} X_g(t_{\ell - 1})\\
    &= \sum_{\ell = 1}^{j} \sum_{g = 1}^{S} \frac{1}{\sqrt{ n}} e^{\lambda_g(t_j - t_{\ell})} \left[X_g(t_{\ell}) - e^{\lambda_g(t_{\ell} - t_{\ell - 1})} X_g(t_{\ell - 1})\right]\\
    &= \sum_{\ell = 1}^{j} \sum_{g = 1}^{S} e^{\lambda_g(t_j - t_{\ell})} \left(\frac{p_g  n}{n} \right)^{1/2} \left(\frac{X_g(t_{\ell - 1})}{p_g  n} \right)^{1/2} X_g(t_{\ell - 1})^{-1/2} \sum_{m = 1}^{X_g(t_{\ell - 1})} \left[ B_g(t_{\ell} - t_{\ell - 1}) - e^{\lambda_g(t_{\ell} - t_{\ell - 1})} \right]\\
\end{align*}
By assuming the initial proportion $p_g$ for sub-type $g$ is independent of $n$ and using the Law of large numbers, as $n\rightarrow \infty$, we have
\[\left( \frac{X_g(t_{\ell - 1})}{p_g  n}  \right) \rightarrow \E[B_g(t_{\ell - 1})] = e^{\lambda_g t_{\ell - 1}} \quad a.s.\]
By assuming that the maximum number of time point $N_T$ and the length of time interval $t_i - t_j$ for any $i\geq j$ are both bounded and not depend on $n$, the Law of large numbers also assures that for any $\ell \in \{1,\cdots,N_T\}$ the $X_g(t_{\ell-1})$ will diverge to infinity when $n\rightarrow \infty$. Therefore, we may apply the Central Limit Theorem to the following term:
\[X_g(t_{\ell - 1})^{-1/2} \sum_{m = 1}^{X_g(t_{\ell - 1})} \left[  B_g(t_{\ell} - t_{\ell - 1}) - e^{\lambda_g (t_{\ell} - t_{\ell - 1})} \right] \Rightarrow V_g(t_{\ell} - t_{\ell - 1}) \sim N(0, \sigma_{g}^2(t_{\ell} - t_{\ell - 1})). \]
Thus, we conclude that
\[W_{n}(t_j) \Rightarrow \sum_{\ell = 1}^j \sum_{g =1}^S \sqrt{p_g} e^{\lambda_g (t_j - t_{\ell})} e^{\lambda_g t_{\ell - 1}/2} V_g(t_{\ell} - t_{\ell -1}) \text{ as } n \to \infty\]
Next we show that the random vector $\mathbf{W}$ converges to the random vector $\mathbf{Y}$, which has the multivariate normal distribution. We can obtain the distribution for $\mathbf{Y}$ from the independence between $V_i(t_{\ell} - t_{\ell - 1})$ and $V_j(t_{m} - t_{m - 1})$ for all $i,j \in \{1,\cdots,N_g\}, \ell,m\in \{1,\cdots, k\}$:
\[\mathbf{Y}= [Y(t_1),\cdots, Y(t_k)] \sim N(0,\Sigma)\]
where
\[\Sigma_{i,j} =  \sum_{\ell = 1}^{\min(i,j)} \sum_{g =1}^{S} p_g e^{\lambda_g (t_i - t_{\ell})} e^{\lambda_g (t_j - t_{\ell})} e^{\lambda_g t_{\ell - 1}} \sigma_{k}^2(t_{\ell} - t_{\ell - 1})\]
Then we use the Cramer Wold device, given a constant vector $a\in \mathbb{R}^k < \infty$, we have
\begin{align*}
    \langle a, \mathbf{W}\rangle &= \sum_{j = 1}^k a_j \sum_{\ell = 1}^{j} \sum_{g =1}^{S} \sqrt{p_g} e^{\lambda_g (t_j - t_{\ell})} \left(\frac{X_g(t_{\ell - 1})}{p_g  n} \right)^{1/2} X_g(t_{\ell - 1})^{-1/2} \sum_{m = 1}^{X_g(t_{\ell - 1})} \left[B_m(t_{\ell} - t_{\ell - 1}) - e^{\lambda_g (t_{\ell} - t_{\ell - 1})} \right]\\
    & \Rightarrow \sum_{j = 1}^{k} a_j \sum_{\ell = 1}^{j} \sum_{g =1}^{S} \sqrt{p_g} e^{\lambda_g (t_{j} - t_{\ell})} e^{\lambda_g t_{\ell - 1}/2} V_{i}(t_{\ell} - t_{\ell - 1})\\
    &= \sum_{j = 1}^{k} a_j Y(t_j) = \langle a , \mathbf{Y} \rangle
\end{align*}
Thus, we show that $\mathbf{W} \Rightarrow \mathbf{Y}$.

\subsection{Proof of Proposition \ref{prop:LiveImageCLT} if initial proportions can go to 0 with $n$ }

\label{appx:LiveImageCLT Extension}
In proving Proposition \ref{prop:LiveImageCLT}, we made the assumption that the initial proportions $p_i$ for $i=1,\cdots,S$ are not dependent on the initial cell count $n$. In this sub-section, we aim to relax this assumption and demonstrate a similar result. Note we will assume that all other inputs are still independent of $n$, i.e., $S$ and $\mathcal{T}$.

In particular, we will allow $p_i$ to depend on $n$, and allow for $\limsup_{n}p_in<\infty.$ We denote two sets of subpopulations $F$ and $I$, where 
\begin{align*}
    F &= \{i \in \{1,\cdots,S\};\limsup_{n\to\infty}p_i n < \infty \}\\
    I &= \{i \in \{1,\cdots,S\};\lim_{n\to\infty}p_i n = \infty \}
\end{align*}
Due to $\sum_{i = 1}^S p_i = 1$, and $S$ being fixed with $n$, we know that the set $I$ must not be an empty set. Then following a similar pattern as the proof of the Proposition \ref{prop:LiveImageCLT}, we derive:

\begin{align*}
    W_{n}(t_j) &= \sum_{\ell = 1}^{j} \sum_{g = 1}^{S} e^{\lambda_g(t_j - t_{\ell})} \left(\frac{p_g  n}{ n} \right)^{1/2} \left(\frac{X_g(t_{\ell - 1})}{p_g  n} \right)^{1/2} X_g(t_{\ell - 1})^{-1/2} \sum_{m = 1}^{X_g(t_{\ell - 1})} \left[ B_g(t_{\ell} - t_{\ell - 1}) - e^{\lambda_g(t_{\ell} - t_{\ell - 1})} \right]\\
    &= \sum_{\ell = 1}^{j} \sum_{g \in I} e^{\lambda_g(t_j - t_{\ell})} \left(\frac{p_g  n}{ n} \right)^{1/2} \left(\frac{X_g(t_{\ell - 1})}{p_g  n} \right)^{1/2} X_g(t_{\ell - 1})^{-1/2} \sum_{m = 1}^{X_g(t_{\ell - 1})} \left[ B_g(t_{\ell} - t_{\ell - 1}) - e^{\lambda_g(t_{\ell} - t_{\ell - 1})} \right]\\
    & + \sum_{\ell = 1}^{j} \sum_{g \in F} e^{\lambda_g(t_j - t_{\ell})} \left(\frac{p_g  n}{ n} \right)^{1/2} \left(\frac{X_g(t_{\ell - 1})}{p_g  n} \right)^{1/2} X_g(t_{\ell - 1})^{-1/2} \sum_{m = 1}^{X_g(t_{\ell - 1})} \left[ B_g(t_{\ell} - t_{\ell - 1}) - e^{\lambda_g(t_{\ell} - t_{\ell - 1})} \right]\\
\end{align*}

Next, we may consider these two double sums separately. For $g\in I$, because the $p_g n$ diverges to infinity as $n\rightarrow \infty$, the first double sum will converge to the same limit we established in Proposition \ref{prop:LiveImageCLT}. For $g\in F$, $p_g n$ will stay bounded. Therefore, in the second double sum, we will have $\frac{p_g n}{n}$ converge to 0 as $n\rightarrow \infty$, which makes the second double sum vanish. In conclusion, we have
\[W_{n}(t_j) \Rightarrow \sum_{\ell = 1}^{j} \sum_{g\in I} \sqrt{p_g} e^{\lambda_g (t_j - t_{\ell})} e^{\lambda_g t_{\ell - 1}/2} V_g(t_{\ell} - t_{\ell -1}) \text{ as } n \to \infty.\]
Note that this convergence result will lead to the same realization in practice, i.e., we will define the covariance by summing over all subtypes. This is because in practice we only have $n<\infty$ and we cannot actually assume subpopulations have zero contribution to the covariance.






%%%%%%%%%%%%%%%%%%% End of the proof %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Exact Path likelihood computation}
\label{appx:Exact Path likelihood}
Here we show how to calculate the exact likelihood of a sample path observation of the total cell count of multiple heterogeneous birth-death processes. While we do not use this approach for likelihood evaluation in the current manuscript we report it here to show that it is not feasible.

We first consider the following joint probability of a homogeneous linear birth-death process:
$$
    \mathbb{P}(X(t_1) = x_1,\cdots, X(t_k) = x_k|X(t_0) = n, \theta_{BD}(2))  = \prod_{k = 1}^{N_t} \mathbb{P}(X(t_k) = x_k | X(t_{k-1}) = x_{k-1},\theta_{BD}(2)).
$$
For ease of notation define the transition probability $p_{i, j}(t_k - t_{k-1}) =\mathbb{P}(X(t_k) = j | X(t_{k-1}) = i,\theta_{BD}(2))$. It is important to note that evaluating $p_{i,j}(t_k - t_{k-1})$ is the most computation-demanding task when evaluating the joint probability. As a result, we mainly consider the number of evaluations of this transition probability. The analytical form for this transition probability was derived in \cite{bailey1991elements}:
\begin{align}
    \label{eq:BD_transition}
    p_{i,j}(t) = \sum_{k = 0}^{\min(i,j)} \binom{i}{k} \binom{i+j-k-1}{i-1} a(t)^{i-k} b(t)^{j -k} (1 - a(t) - b(t))^{k},
\end{align}
where 
\[a(t) = \frac{\nu(e^{(\beta-\nu)t} - 1)}{\beta e^{(\beta-\nu)t} -\nu}, b(t) = \frac{\beta(e^{(\beta - \nu)t} - 1)}{\beta e^{(\beta - \nu)t} - \nu}.\]
Note that numerical evaluation \eqref{eq:BD_transition} will be computationally expensive due to the presence of multiple factorial terms. We use a Gosper refined version of the Stirling formula \cite{gosper1978decision} to approximate these factorials
\[n! \approx \sqrt{\left(2n + \frac{1}{3}\right)\pi} n^n e^{-n}.\]
We find that this approximation leads to good performance in our examples. It is thus straightforward to evaluate the path likelihood for the case of a homogeneous linear birth-death process.

If we instead have observations of a sum of birth-death processes, the evaluation of the path likelihood is much more difficult. In particular, the sum of the birth-death processes is no longer a Markov process and we must therefore sum over possible values of our unobserved subpopulations.
Specifically, if we have two subpopulations we can formulate the equation \eqref{eq:LC path likelihood}, as 
\begin{equation}
\label{eq:naive_transition}
\begin{split}
&P(X^{(r)}(t)=x_t, t\in\mathcal{T}|\theta_{BD}(2))\\
&=\sum_{i_1=0}^{x_1}\cdots\sum_{i_{N_T}=0}^{x_{N_T}}P\left(X_1(t_1)=i_1,X_2(t_1)=x_1-i_1,\ldots,X_1(t_{N_T})=i_{N_T},X_2(t_{N_T})=x_{N_T}-i_{N_T}|\theta_{BD}(2)\right)\\
&=
\sum_{i_1=0}^{x_1}\cdots\sum_{i_{N_T}=0}^{x_{N_T}}P\left(X_1(t_1)=i_1,X_1(t_2)=i_2,\cdots,X_1(t_{N_T}) = i_{N_T}|\theta_{BD}(2)\right)\\
&\quad\times P\left(X_2(t_1)=x_1 - i_1,X_2(t_2)=x_2-i_2,\cdots, X_2(t_{N_T}) = x_{N_{T}} - i_{N_T}|\theta_{BD}(2)\right).
\end{split}
\end{equation}
Note that the last equality is due to the assumption that subpopulation grow independently, and we can decompose the joint probability of mixture cell count into a summation of multiple joint probabilities of the homogeneous linear birth-death process. It is not hard to see that if we naively evaluate the above sum, the number of computations of the homogeneous joint probability is around $\Omega(\min_{t\in\tau} x_t^{N_T})$.
Since many examples have $N_T\approx 10$, and $x_t\approx 100$ this is clearly an infeasible approach.




% \subsection{Hidden Markov Model}
% \label{HMM}



%%%%%%%%%%%%%%%%%%%%%%%%%%% HMM send to appendix %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In order to avoid the exponential dependence on the number of time points, one option is to use techniques from Hidden Markov Models (HMM). The main assumption of HMM is the Markov property of the hidden process, and that the hidden process relates to the observable process according to a specified distribution $B$. Recall that the time series of observed total cell count is given by $\{X(t_i);i\in\mathcal{T}\}$ and denote the time series of the subpopulations as $\{(X_1(t_i),\ldots,X_S(t_i);i\in\mathcal{T}\}$. Then $\{X(t_i);i\in\mathcal{T}\}$ in the HMM is the observable process, and $\{(X_1(t_i),\ldots,X_S(t_i));i\in\mathcal{T}\}$ is the hidden Markov process due to the Markov property of the linear birth-death process. Notice that the relationship between the hidden process $\{(X_1(t_i),\ldots,X_S(t_i));i\in\mathcal{T}\}$ and the observable process $\{X(t_i);i\in\mathcal{T}\}$ can be defined as \[\mathbb{P}(X(t) = x | (X_1(t),\ldots,X_S(t)) = (x_1,\ldots,x_S)) = \begin{cases}
1 & \text{ if } x_1 +\ldots+ x_S = x\\
0 & \text{ o.w. }
\end{cases}\] 



We can translate the live-cell imaging experiment into an HMM and significantly improve the computational complexity of evaluating the exact likelihood function \eqref{eq:BD_LikelihoodFun}. In particular, we can use popular HMM techniques, such as the forward-backward procedure to reduce the total number of transition probability, i.e., equation \eqref{eq:BD_transition}, computation to $\Theta(H^2 N_T)$ for one replicate at one dosage level, where $H$ is the number of hidden states. In particular, we need to calculate the $H$ by $H$ transition matrix for every time point, and if we assume the length of time intervals are identical, we can reduce the upper bound to $\Theta(H^2)$. However, the number of hidden states depends on both the maximum total number of cells observed at each time point $x$ and the number of subpopulations $S$. As we will now show, this is unfortunately not a sufficient reduction in computational complexity. In particular, assume we observe $x$ total cells. In that case, the number of hidden states is given by $\binom{x-1}{S-1}$, and assuming that $x\gg S$, we have that $\binom{x-1}{S-1}=\Theta\left(x^{S-1}\right)$ as $x\to\infty$. With only two subpopulations this results in computational complexity of $\Theta(x^2)$, and in many experiments, we might have $x\approx 10^5$ leading to an extremely high computational burden. If $S=3$ we would end up with the far worse computational complexity of $\Theta(x^4)$.




In conclusion, using a naive approach to compute the joint probability of mixture cell count would require $\Omega\left(\min_{t\in\mathcal{T}}x_t^{N_T}\right)$ many computations of transition probability, which is clearly infeasible when many cell counts are in the thousands with over 10 observed time points. We also discuss an HMM based approach to evaluating the likelihood that results in a significant reduction in the computational burden for evaluating this likelihood. In particular, with this approach we can reduce the number of computations of the transition probability to $\Omega\left(\min_{t\in\mathcal{T}}x_t^2\right)$. Unfortunately we might have $\min_{t\in\mathcal{T}}x_t\approx 5000$. Note that this will be the complexity for evaluating the likelihood of one single replicate at a single dose, so taking into account that we can have more than 10 different doses, with more than 10 replicates at each dose we see that unfortunately, this HMM approach is computationally infeasible. In addition, this HMM approach will have significantly worse computational complexity after we include observation noise terms and/or more than 2 subpopulations.

\subsection{Significance test for confidence interval width}
In Figures \ref{fig:CI width}, \ref{fig:CI width 10 noise}, and \ref{fig:CI width 50 noise}, statistics were done using R version 4.2.1 \cite{R4.2.1} with ggsignif package \cite{ggsignif}.


\section*{Acknowledgements}
The work of C. Wu was supported in part  with funds from the Norwegian Centennial Chair Program.
The work of J.M. Enserink and D.S. Tadele was supported by grants from the Norwegian Health Authority South-East, grant numbers 2017064, 2018012, and 2019096; the Norwegian Cancer Society, grant numbers 182524 and 208012; and the Research Council of Norway through its Centers of Excellence funding scheme (262652) and through grants and 261936, 294916 and 314811

    \bibliographystyle{plain}
    \bibliography{main.bib} 
    



\end{document}
