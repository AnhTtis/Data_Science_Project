\section{Introduction}
\label{sec:introduction}

Supporting modern applications that rely on accurate and real-time analytics  computed over large and continuously evolving databases is a challenging data management problem~\cite{LB:SIGMOD:2015}. Special cases are the classical problems of incremental view maintenance (IVM)~\cite{Chirkova:Views:2012:FTD,DBT:VLDBJ:2014} and stream query processing~\cite{abadi2005design,madden2005tinydb}. 

Recent efforts studied the problem of computing machine learning (ML) tasks over {\em static} databases. The predominant approach loo\-sely integrates the database systems with the statistical packages \cite{MADlib:2012,Rusu:2015,MLlib:JMLR:2016,Polyzotis:SIGMOD:Tutorial:17,Kumar:SIGMOD:Tutorial:17}: First, the database system computes the input to the statistical package by joining the database relations. It then exports the join result to the statistical package for training ML models. This approach precludes real-time analytics due to the expensive export/import steps. 
Systems like Morpheus~\cite{KuNaPa15} and LMFAO~\cite{LMFAO:SIGMOD:2019} push the ML task inside the database and learn ML models over static normalized data. 
In particular, LMFAO, and its precursors F~\cite{SOC:SIGMOD:2016} and AC/DC~\cite{ANNOS:TODS:2020}, decompose the task of learning classification and regression models over arbitrary joins into factorized computation of aggregates over joins and fixpoint computation of model parameters. This factorization may significantly lower the complexity by avoiding the computation of Cartesian products lurking within joins~\cite{BKOZ:PVLDB:2013,Olteanu:FactBounds:2015:TODS}. Both the tight integration of the database computation step and of the statistical computation step as well as the factorized computation are pre-requisites for real-time analytics.


This article describes \DF\footnote{\url{https://github.com/fdbresearch/FIVM}.}, a unified approach for maintaining analytics over changing relational data.  We exemplify its versatility in four disciplines: processing queries with group-by aggregates and joins; learning linear regression models using the covariance matrix of the input features; building Chow-Liu trees using pairwise mutual information matrix of the input features; and matrix chain multiplication. 

\DF was introduced in prior work~\cite{FIVM:SIGMOD:2018}. This article revisits and extends this prior work with: a more refined analysis of \DF for the $q$-hierarchical and free-connex acyclic  queries in the presence of functional dependencies; the covariance ring over continuous and categorical features; an overview of the design of \DF; further experiments on: the covariance matrix; end-to-end linear regression models; Chow-Liu trees; $q$-hierarchical queries with eager and lazy approaches and payloads carrying the listing or the factorized representation of the query result; and path queries of increasing length on graph data to stress-test the scalability of the IVM engines.

\DF has three main ingredients: higher-order incremental view maintenance (IVM); factorized computation and data representation; and ring abstraction.

The first ingredient reduces the maintenance task to that of a hierarchy of simple views. Such views are functions mapping keys, which are tuples of input values, to payloads, which are elements from a ring. In contrast to classical (first-order) IVM, which computes changes to the query result on the fly and does not use extra views, \DF can significantly speed up the maintenance task and lower its complexity by using carefully chosen views. Yet \DF can use substantially fewer views than the fully-recursive IVM, which is used by the state-of-the-art IVM system DBToaster~\cite{DBT:VLDBJ:2014}. In our experiments, \DF outperforms first-order and higher-order IVM by up to two orders of magnitude in both runtime and memory requirements.

The second ingredient supports efficient computation and representation for keys, payloads, and updates. \DF exploits insights from query evaluation algorithms with best known complexity and optimizations that push aggregates past joins~\cite{BKOZ:PVLDB:2013,Olteanu:FactBounds:2015:TODS,FAQ:PODS:2016}. It can process bulk updates expressed as low-rank decompositions~\cite{TensorDecomp:2009,TensorDecomposition:2017} and maintain a factorized representation of query results, which is essential to achieve low complexity for free-connex acyclic and $q$-hierarchical queries.

The third ingredient allows \DF to treat uniformly seemingly disparate tasks. In the key space, all tasks require joins and variable marginalization. In the payload space, tasks differ in the ring operations. To maintain linear regression models and Chow-Liu trees under updates, \DF uses a new ring that captures the maintenance of a covariance matrix over continuous and categorical features from the input database. Furthermore, it composes rings to capture the data-dependent computation for complex analytics. Thanks to the ring abstraction, \DF is highly extensible: efficient maintenance for new analytics over relational databases is readily available as long as they come with appropriate sum and product ring operations.

\subsection{\DF by Example}
\label{ex:sql_sum_aggregate_intro}

Consider the following SQL query over a database $\db$ with relations $R(A,B)$, $S(A,C,E)$, and $T(C,D)$:
\begin{lstlisting}[language=SQL, mathescape, columns=fullflexible]
  Q := SELECT A,$\;$C,$\;$SUM(B$\,$*$\,$D$\,$*$\,$E) 
       $\,$FROM   R NATURAL$\;$JOIN S NATURAL$\;$JOIN T
       $\,$GROUP$\;$BY A,$\;$C;
\end{lstlisting}
%
A na\"{i}ve query evaluation approach first computes the join and then the aggregate. This takes $\bigO{N^3}$ time, where $N$ is the size of $\db$.
An alternative approach exploits the distributivity of {\tt SUM} over multiplication to partially push the aggregate past joins and then combine the partial aggregates. For instance, one such partial sum over $S$ can be expressed as the view V$_\texttt{S}$:
\begin{lstlisting}[language=SQL, mathescape, columns=fullflexible]
  V$_\texttt{S}$ := SELECT A,$\;$C,$\;$SUM(E)$\;$AS$\;$S$_\texttt{E}$ 
        $\,$FROM S GROUP$\;$BY A,$\;$C;
\end{lstlisting}
% 
In the view V$_\texttt{S}$, we identify keys, which are tuples over $(A,C)$, and payloads, which are aggregate values S$_\texttt{E}$. 
Similarly, we compute partial sums over \texttt{R} and \texttt{T} as views V$_\texttt{R}$ and V$_\texttt{T}$. These views are joined as depicted by the {\em view tree} in Figure~\ref{fig:view_tree_sql}, which is akin to a query plan with aggregates pushed past joins. This view tree computes the result of $Q$ in $\bigO{N}$ time.

\begin{figure}[t]
  \centering   
  \includegraphics[width=0.5\columnwidth]{figures/ViewTreeDelta5}
  % \vspace{-2em}
  \caption{View tree for the query in Example~\ref{ex:sql_sum_aggregate_intro}. The propagation paths for updates to $S$ (right red) and to $T$ (left blue).}
  \label{fig:view_tree_sql}
\end{figure}

Consider now the problem of learning, for each pair $(a,c)$ of $(A,C)$-values in the natural join of $R$, $S$, and $T$, a linear function $f_{a,c}$ with parameters $\theta_{0}$, $\theta_{D}$ and $\theta_{E}$ that predicts the label $B$ given features $D$ and $E$:
\begin{align*}
f(D, E) = \theta_{0} + \theta_{D} \cdot D + \theta_{E} \cdot E
\end{align*}
Our insight is that the same view tree in Figure~\ref{fig:view_tree_sql} can compute the gradient vector used for learning $f_{a,c}$, where we replace the SQL \texttt{SUM} and \texttt{*} operators.

As shown in Section~\ref{sec:application-lr}, the gradient of the square loss objective function needs the computation of three types of aggregates: the scalar $\LRringC$ that is the count aggregate \texttt{SUM(1)}; the vector $\LRringS$ of linear aggregates \texttt{SUM(i)}, for $i\in\{\texttt{B},\texttt{D},\texttt{E}\}$; and the matrix $\LRringQ$ of quadratic aggregates \texttt{SUM($i*j$)}, where $i,j\in\{\texttt{B},\texttt{D},\texttt{E}\}$. These aggregates capture the correlation between the features and the label.

We treat these aggregates as one compound aggregate $(\LRringC,\LRringS,\LRringQ)$ so we can share computation across them. This compound aggregate can be partially pushed past joins similarly to the \texttt{SUM} aggregate discussed before. Its values are carried in the key payloads of views in the view tree from Figure~\ref{fig:view_tree_sql}. For instance, the partial compound aggregate $(\LRringC_\texttt{T},\LRringS_\texttt{T},\LRringQ_\texttt{T})$ at the view V$_\texttt{T}$ computes, for each $C$-value, the count, sum, and sum of squares of the $D$-values in $T$. Similarly, the partial aggregate $(\LRringC_\texttt{S},\LRringS_\texttt{S},\LRringQ_\texttt{S})$ at the view V$_\texttt{S}$ computes, for each pair $(A,C)$, the count, sum, and sum of squares of $E$-values in $S$. In the view V$_\texttt{ST}$, which is the join of V$_\texttt{T}$ and V$_\texttt{S}$, each key $(a,c)$ is associated with the multiplication of the payloads for the keys $c$ in V$_\texttt{T}$ and $(a,c)$ in V$_\texttt{S}$. This multiplication works on compound aggregates: The scalar $\LRringC_\texttt{ST}$ is the arithmetic multiplication of $\LRringC_\texttt{T}$ and $\LRringC_\texttt{S}$; the vector of linear aggregates $\LRringS_\texttt{ST}$ is the sum of the scalar-vector products $\LRringC_\texttt{T}\LRringS_\texttt{S}$ and $\LRringC_\texttt{S}\LRringS_\texttt{T}$; finally, the matrix 
$\LRringQ_\texttt{ST}$ of quadratic aggregates is the sum of the scalar-matrix products $\LRringC_\texttt{T}\LRringQ_\texttt{S}$ and $\LRringC_\texttt{S}\LRringQ_\texttt{T}$, and of the outer products of the vectors $\LRringS_\texttt{T}$ and the transpose of $\LRringS_\texttt{S}$ and also of $\LRringS_\texttt{S}$ and the transpose of $\LRringS_\texttt{T}$. Our approach shares the computation across the aggregates: The scalar aggregates are used to scale up the linear and quadratic aggregates, while the linear aggregates are used to compute the quadratic aggregates.

We now turn to incremental view maintenance. \DF operates over view trees. Whereas for non-incre\-mental computation we only materialize the top view in the tree and the input relations, for incremental computation we may materialize additional views to speed up the maintenance task. Our approach is an instance of higher-order IVM, where an update to one relation may trigger the maintenance of several views. 

Figure~\ref{fig:view_tree_sql} shows the leaf-to-root maintenance paths under changes to $\texttt{S}$ and $\texttt{T}$. For updates $\delta{\texttt{S}}$ to $\texttt{S}$, each delta view $\delta{V_\texttt{S}}$, $\delta{V_\texttt{ST}}$, and $\delta{\texttt{Q}}$, is computed using delta rules:
% 
% \vspace{-0.25em}
\begin{lstlisting}[language=SQL, mathescape, columns=flexible] 
 $\delta$V$_\texttt{S}$ := $\,$SELECT A,$\;$C,$\;$SUM(E)$\;$AS$\;$S$_\texttt{E}$ 
        $\,\,$FROM $\delta$S GROUP$\;$BY A,$\;$C;
$\;\;\delta$V$_\texttt{ST}\,$:= SELECT A,$\;$C,$\;$SUM(S$_\texttt{D}$$\,$*$\,$S$_\texttt{E}$)$\;$AS$\;$S$_\texttt{C}$ 
        $\,\,$FROM V$_\texttt{T}$ NATURAL JOIN $\delta$V$_\texttt{S}$ GROUP$\;$BY A,$\;$C;
  $\delta$Q := SELECT A,$\;$C,$\;$SUM(S$_\texttt{B}$$\,$*$\,$S$_\texttt{C}$)
        $\,\,$FROM V$_\texttt{R}$ NATURAL JOIN $\delta$V$_\texttt{ST}$ GROUP$\;$BY A,$\;$C;
\end{lstlisting}
% \vspace{-0.25em}
%
An update may consist of both inserts and deletes, which are encoded as keys with positive and respectively negative payloads. For the count aggregate, the payload is $1$ for an insert and   
$-1$ for a delete. For the compound aggregate, the payload is $(1, {\bf 0}_{5 \times 1}, {\bf 0}_{5 \times 5})$ for an insert and $(-1, {\bf 0}_{5 \times 1}, {\bf 0}_{5 \times 5})$ for a delete, where ${\bf 0}_{n \times m}$ is the $n$-by-$m$ matrix with all zero values.

\DF materializes and maintains views depending on the update workload. For updates to all input relations, it materializes each view in the view tree. For updates to \texttt{R} only, it materializes V$_\texttt{ST}$; for updates to \texttt{S} only, it materializes V$_\texttt{R}$ and V$_\texttt{T}$; for updates to \texttt{T} only, it materializes  V$_\texttt{R}$ and V$_\texttt{S}$. \DF takes constant time for updates to \texttt{S} and linear time for updates to \texttt{R} and \texttt{T}; these complexities are in the number of distinct keys in the views.
In contrast, the first-order IVM computes one delta query per each updated relation and without the use of extra views. It takes linear time for updates to any of the three relations for our example query. The fully-recursive higher-order IVM constructs a view tree for each delta query, so overall more views, including the view materializing the join of V$_\texttt{R}$, V$_\texttt{S}$, and $\delta{\texttt{T}}$.

\DF thus needs the same view tree and views for our query with one SUM aggregate and even for the learning task with the ten SUM aggregates. In contrast, the first-order IVM needs to compute a distinct delta query for each of these aggregates for updates to any of the three relations. DBToaster, which is the state-of-the-art fully recursive IVM, computes 31 views, ten top views and 21 auxiliary ones. Whereas \DF shares the computation across these aggregates, the other IVM approaches do not. This significantly widens the performance gap between \DF and its competitors.
