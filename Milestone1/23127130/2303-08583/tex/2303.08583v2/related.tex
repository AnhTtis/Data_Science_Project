\section{Related Work}
\label{sec:related_work}

To the best of our knowledge, ours is the first approach to propose factorized IVM for a range of distinct applications. It extends non-trivially two lines of prior work: higher-order delta-based IVM and factorized computation of in-database analytics. 

Our view language is modelled on functional aggregate queries over semirings~\cite{FAQ:PODS:2016} and generalized multiset relations over rings~\cite{DBT:VLDBJ:2014}; the latter allowed us to adapt DBToaster to factorized IVM.


{\bf IVM.} IVM is a well-studied area spanning more than three deca\-des~\cite{Chirkova:Views:2012:FTD,SalemBCL00,TangSEKF20}. Prior work extensively studied IVM for various query languages and showed that the time complexity of IVM is lower than of recomputation. We go beyond prior work on higher-order IVM for queries with joins and aggregates, as realized in DBToaster~\cite{DBT:VLDBJ:2014}, and propose a unified approach for factorized computation of aggregates over joins~\cite{BKOZ:PVLDB:2013}, factorized incremental computation of linear algebra~\cite{NEK:SIGMOD:2014}, and in-database machine learning over database joins~\cite{SOC:SIGMOD:2016}. DBToaster uses one materialization hierarchy per relation in the query, whereas \DF uses one view tree for all relations. DBToaster can thus have much higher space requirements and update times. As we observed experimentally, it does not consider the maintenance of composite aggregates such as the covariance matrix.
IVM over array data~\cite{Zhao:2017:ArrayIVM} targets scientific workloads but without exploiting data factorization.

\DF over the relational payload ring strict\-ly subsumes prior work on factorized IVM for acyclic joins \cite{DynYannakakis:SIGMOD:2017} as it can support arbitrary joins.
\DF has efficient support for free-connex acyclic~\cite{DynYannakakis:SIGMOD:2017} and $q$-hierarchical queries~\cite{Nicole:PODS:2017}.
Exploiting key attributes to enable succinct delta representations and accelerate maintenance complements our approach~\cite{Katsis:idIVM:2015}.
Our framework generalizes the main idea of the LINVIEW approach~\cite{NEK:SIGMOD:2014} for maintaining matrix computation over arbitrary joins. 
Unlike approaches that exploit the {\em append-only} nature of data streams~\cite{YangGO17}, \DF allows for both data insertions and deletions.
\DF can be used to improve the memory-efficiency of systems that integrate IVM into compilers to speed up the search in abstract syntax trees~\cite{BalakrishnanNKZ21}.
Such systems  suffer from the high storage overhead of systems such as DBToaster that maintain significantly more views than F-IVM.


Commercial DBMSs support IVM for restricted cla\-sses of queries, e.g., Oracle~\cite{Oracle:RestrictionsIVM} and SQLServer \cite{SQLServer:RestrictionsIVM}.
LogicBlox supports higher-order IVM for Datalog meta-programs~\cite{LB:SIGMOD:2015,GOW:PVLDB:2015}. Trill is a streaming engine that supports incremental processing of relational-style que\-ries but no complex aggregates like covariance matrices~\cite{chandramouli2014trill}. Differential Dataflow~\cite{McSherry:DiffDataflow:2013} supports incremental processing for  programs with recursion. There is a distinct line of work on maintenance for recursive Datalog~\cite{Motik:FBF:2019}.

{\bf Static In-DB analytics.} The emerging area of in-data\-base analytics has been overviewed in two tutorials~\cite{Polyzotis:SIGMOD:Tutorial:17,Kumar:SIGMOD:Tutorial:17} and a recent keynote~\cite{Olteanu:VLDBKeynote:2020}.  Several systems support analytics over normalized data via a tight integration of databases and machine learning~\cite{MLlib:JMLR:2016,MADlib:2012,Rusu:2015,Polyzotis:SIGMOD:Tutorial:17,Kumar:SIGMOD:Tutorial:17}. Other systems integrate with R to enable in-situ data processing using domain-specia\-lized routines~\cite{ZCDDMMFSS12,Brown:SciDB:2010:SIGMOD}. The closest in spirit to our approach is work on learning models over factorized joins \cite{Rendle13,SOC:SIGMOD:2016,OS:PVLDB:16,ANNOS:TODS:2020}, pushing ML tasks past joins~\cite{Kumar:InDBMS:2012,LMFAO:SIGMOD:2019} and on in-database linear algebra~\cite{Boehm:VLDB:2016,Arun:VLDB:2017,Figaro:SIGMOD:2022}, yet they do not consider incremental maintenance.

{\bf Learning.} There is a wealth of work in the ML community on incremental or online learning over {\em arbitrary} relations~\cite{OnlineML:2011}. Our approach learns over {\em joins} and crucially exploits the join dependencies in the underlying training dataset to improve the runtime performance. 


