%!TEX root = main.tex

\section{Experiments}
\label{sec:experiments}

We compare \DF (factorized IVM) against \IVM (first-order IVM) and \DBT (DBToaster's fully recursive higher-order IVM). Our experimental results can be summarized as follows:
% \begin{itemize}[leftmargin=3mm,itemsep=0pt,topsep=1pt]
\begin{itemize}
    \item Factorized updates lead to two orders of magnitude speedup for \DF over competitors for matrix chain multiplication by propagating factorized deltas and avoiding matrix multiplication.
%
    \item For cofactor matrices used in regression models, \DF exhibits the lowest memory utilization and up to two orders of magnitude better performance than \IVM and \DBT.  

    \item 
    For conjunctive query evaluation, factorized payloads can speed up view maintenance and reduce memory by up to two orders of magnitude compared to the listing representation of payloads.
\end{itemize}

\subsection{Experimental Setting}

{\bf Runtime.}
\IVM and \DBT are supported by DBToaster~\cite{DBT:VLDBJ:2014}, a system that compiles a given SQL query into code that maintains the query result under updates to input relations. The generated code represents an in-memory stream processor that is standalone and independent of any database system. DBToaster's performance on decision support and financial workloads can be several orders of magnitude better than state-of-the-art commercial databases and stream processing systems~\cite{DBT:VLDBJ:2014}. We implemented \DF as a program that maintains a set of materialized views for a given variable order and a set of updatable relations. We use the intermediate language of DBToaster to encode this program and then feed it into DBToaster's code generator. We modified the backend of DBToaster v2.2 to enable arbitrary ring payloads and limit the amount of memory over-provisioning to at most one million records.
Unless stated otherwise, all the benchmarked approaches use the same runtime and store views as multi-indexed maps with memory-pooled records. The algorithms and record types used in these approaches, however, can differ greatly.

{\bf Workload. }
We run experiments over three datasets:

% \begin{itemize}[leftmargin=0mm,itemsep=0pt,topsep=1pt,itemindent=1.5em]
% \begin{itemize}[leftmargin=0.5em,itemindent=1.5em]
% \begin{itemize}[leftmargin=0mm,itemindent=26pt]
  \begin{itemize}
    \item {\em Retailer} is a real-world dataset from an industrial collaborator and used by a retailer for business decision support and forecasting user demands. 
    The dataset has a snowflake schema with one fact relation {\tt Inventory} with $84$M records, storing information about the inventory units for products in a location, at a given date.
    The {\tt Inventory} relation joins along three dimension hierarchies: {\tt Item} (on product id), {\tt Weather} (on location and date), and {\tt Location} (on location) with its lookup relation {\tt Census} (on zip code). 
    The natural join of these five relations is acyclic and has $43$ attributes. We consider a view tree in which the variables of each relation form a distinct root-to-leaf path, and the partial order on join variables is: location - $\{$ date - $\{$ product id $\}$, zip $\}$.
     
    \item {\em Housing} is a synthetic dataset modeling a house price market~\cite{SOC:SIGMOD:2016}.
    It consists of six relations: {\tt House}, {\tt Shop}, {\tt Institution}, {\tt Restaurant}, {\tt Demographics}, and {\tt Transport}, arranged into a star schema and with $1.4$M tuples in total (scale factor 20). The natural join of all relations is on the common attribute (postcode) and has $27$ attributes. We consider an optimal view tree that has each root-to-leaf path consisting of query variables of one relation.

    \item {\em Twitter} represents friends/followers relationships among users who were active on Twitter during the discovery of Higgs boson~\cite{Higgs:TwitterDataset}. We split the first $3$M records from the dataset into three equally-sized relations, $R(A,B)$, $S(B,C)$, and $T(C,A)$, and consider the triangle query over them and the variable order $A - B - C$.
\end{itemize}

We run the systems over data streams synthesized from the above datasets by interleaving insertions to the input relations in a round-robin fashion. We group insertions into batches of different sizes and place no restriction on the order of records in input relations. 
In all experiments, we use payloads defined over rings with additive inverse, thus processing deletions is similar to that of insertions.

{\bf Queries.} 
We next present the queries used in the experiments. 

\begin{itemize}
\item 
{\em Matrix Chain Multiplication:}
The query in standard SQL is defined over tables $A_1(I,J,P_1)$, $A_2(J,K,P_2)$, $A_3(K,L,P_3)$:
\begin{lstlisting}[language=SQL,columns=flexible, basicstyle=\linespread{1.1}\ttfamily\small]
SELECT A1.I, A3.L, SUM(A1.P1 * A2.P2 * A3.P3)
FROM A1 NATURAL JOIN A2 NATURAL JOIN A3
GROUP BY A1.I, A3.L;
\end{lstlisting}
In our formalism, each relation maps pairs of indices to matrix values, all lifting functions map values to $1$, and the query is: 
$\VIEW[I,L]{Q}=\VSUM_{J}\VSUM_{K} \VIEW[I,\textsf{$J$}]{A_1} \VPROD \VIEW[\textsf{$J$},K]{A_2} \VPROD \VIEW[K,L]{A_3}$.

\item 
{\em Cofactor Matrix Computation:}
For the {\em Retailer} schema, the query has one regression aggregate over the natural join of its relations:
\begin{lstlisting}[language=SQL,columns=flexible, mathescape, basicstyle=\linespread{1.1}\ttfamily\small]
SELECT SUM(g$_1$(X$_1$) * ... * g$_{43}$(X$_{43}$))
FROM Inventory NATURAL JOIN Item NATURAL JOIN Weather
                $\;$NATURAL JOIN Location NATURAL JOIN Census;
\end{lstlisting}
where $\{ X_i \}_{i \in [43]}$ are all the variables from the {\em Retailer} schema, 
the {\tt SUM} operator uses $+$ and $*$ from the degree-$43$ matrix ring, 
and each lifting function $g_i$ maps a value $x$ to $g_i(x) = (\LRringC_i=1, \LRringS_i = x, \LRringQ_{(i,i)} = x^2$) (see Example~\ref{ex:gradient-computation}).
Similarly, the queries over {\em Housing} and {\em Twitter} use the degree-$27$ and respectively degree-$3$ matrix ring. 
We consider all variables to be continuous; categorical variables can be treated using group-by queries as explained in related work~\cite{ANNOS:PODS:2018}.

\item
{\em Factorized Computation of Conjunctive Queries:}
We consider two full conjunctive queries joining all the relations in the {\em Retailer} and respectively {\em Housing} datasets. 
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
{\bf Experimental Setup. }
We run all experiments on a Microsoft Azure DS14 instance, Intel(R) Xeon(R) CPU E5-2673 v3 @ 2.40GHz, 112GB RAM, with Ubuntu Server 14.04. 
We use DBToaster v2.2 for the IVM competitors and code generation in our approach. 
The generated C++ code is single-threaded and compiled using g++ 6.3.0 with the -O3 flag. 
We set a one-hour timeout on query execution and report wall-clock times by averaging three best results out of four runs. 
We profile memory utilization using gperftools, not counting the memory used for storing input streams.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[t]
\begin{center}
{
\renewcommand{\arraystretch}{1.3}
\begin{small}
\begin{tabular}{@{}l@{~}c@{~}c@{~}c@{~}c@{~}c@{}}
\toprule
 & \DF &  \DBT & \IVM & \DFRE & \DBTRE \\
% \cmidrule{2-4} \cmidrule{6-7} 
\midrule
Retailer \quad & $2,955,045$ &  $1,250,262$ & $2,925,828$ & $3,785^{*}$ & $3,491^{*}$ \\
Housing  \quad & $22,857,143$ & $17,834,395$ & $2,403,433$ & $79,226$ & $364^{*}$ \\
\bottomrule
\end{tabular}
\end{small}
}
\end{center}
\caption{The average throughput (tuples/sec) of reevaluation and incremental maintenance of a sum aggregate under updates of size $1,000$ to all relations of the {\em Retailer} and {\em Housing} datasets with a one-hour timeout (denoted by the symbol$^{*}$).}
\label{table:sum_aggregate_cost_comparison}
% \vspace*{-1.5em}
\end{figure}

\subsection{Maintenance of Sum Aggregates}
We analyze different strategies for maintaining a sum of one variable on top of a natural join. We measure the average throughput of reevaluation and incremental maintenance under updates of size $1,000$ to all the relations of {\em Retailer} and {\em Housing}. For the former dataset, we sum the inventory units for products in {\tt Inventory}; for the latter, we sum over the common join variable. We also benchmark two reevaluation strategies that recompute the results from scratch on every update: \DFRE denotes reevaluation using variable orders and \DBTRE denotes reevaluation using DBToaster. Table~\ref{table:sum_aggregate_cost_comparison} summarizes the results. 
 

\DF achieves the highest average throughput in both cases. For {\em Retailer}, the maintenance cost is dominated by the update on {\tt Inventory}. 
\DBT's recursive delta compilation materializes $13$ views representing connected subqueries: five group-by aggregates over the input relations, {\tt Inv}, {\tt It}, {\tt W}, {\tt L}, and {\tt C}; one group-by aggregate joining {\tt L} and {\tt C}; six views joining {\tt Inv} with subsets of the others, namely \{{\tt It}\}, \{{\tt It}, {\tt W}\}, \{{\tt It}, {\tt W}, {\tt L}\}, \{{\tt W}\}, \{{\tt W}, {\tt L}\}, and \{{\tt W}, {\tt L}, {\tt C}\}; and the final aggregate.
The two views joining {\tt Inv} with \{ {\tt W}, {\tt L} \} and \{ {\tt It}, {\tt W}, {\tt L} \} require linear maintenance for a single-tuple change in {\tt Inventory}.
\IVM recomputes deltas from scratch on each update using only the input relations with no aggregates on top of them. Updates to {\tt Inventory} are efficient due to small sizes of the other relations. 
\DF uses the given variable order to materialize $9$ views, four of them over {\tt Inventory}, \{{\tt Inv}\}, \{{\tt Inv}, {\tt It}\}, \{ {\tt Inv}, {\tt It}, {\tt W} \}, and the final sum, but each with constant maintenance for single-tuple updates to this relation.
In contrast to \IVM, our approach materializes precomputed views in which all nonjoin variables are aggregated away. 
In the {\em Housing} schema, both \DF and \DBT benefit from this preaggregation, and since the query is a star join, both materialize the same views. \DBT computes {\tt SUM(1)} and {\tt SUM(postcode)} for each {\tt postcode} in the delta for {\tt Inventory}, although only the count suffices.
Figure~\ref{table:sum_aggregate_cost_comparison} also shows that the reevaluation strategies significantly underperform the incremental approaches.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}[t]
  \centering   
  \includegraphics[width=0.47\textwidth]{Figure_MCM1}
  \qquad
  \includegraphics[width=0.47\textwidth]{Figure_MCM2}
  % \vspace*{-1em}
  \caption{Incremental maintenance and reevaluation of the product of three $(n \times n)$ matrices, $A = A_1 \, A_2 \, A_3$: (left) one-row updates in $A_2$; (right) rank-$r$ updates in $A_2$ for $n=4,096$ using the DBToaster and Octave runtime environments. }
  \label{fig:MCM}
\end{figure*}

\subsection{Matrix Chain Multiplication with Factorized Updates}

We consider the problem of maintaining the multiplication $A = A_1 \, A_2 \, A_3$ of three $(n \times n)$ matrices under changes to $A_2$. We compare \DF with factorized updates, \IVM that recomputes the delta $\delta{A} = A_1 \, \delta{A_2} 
\, A_3$ from scratch, and REEVAL that recomputes the entire product from scratch on every update. 
\DBT becomes \IVM in this particular setting.
We consider two different implementations of these maintenance strategies: The first uses DBToaster's hash maps to store matrices, while the second uses Octave, a numerical tool that stores matrices in dense arrays and offers highly-optimized BLAS routines for matrix multiplication~\cite{Whaley1999}. In both cases, matrix-matrix multiplication takes $\bigO{n^{\alpha}}$ for $\alpha > 2$; for instance, $\alpha = 2.8074$ for 
Strassen's algorithm.


We first consider updates to one row in $A_2$. For \IVM, the delta $\delta{A_{12}} = A_1 \, \delta{A_2}$ might  contain non-zero changes to all $n^2$ matrix entries, thus computing $\delta{A} = \delta{A_{12}} \, A_3$ requires full matrix-matrix multiplication. REEVAL updates $A_2$ first before computing two matrix-matrix multiplications. \DF factorizes $\delta{A_2}$ into a product of two vectors $\delta{A_2} = u \TR{v}$, which are used to compute $\delta{A_{12}} = (A_1 \, u) \, \TR{v} = u_1 \, \TR{v}$ and $\delta{A} = u_1 \, (\TR{v} \, A_3) = u_1 \, v_1$. Both deltas involve only matrix-vector multiplications computed in $\bigO{n^2}$ time. Figure~\ref{fig:MCM} (left) shows the average time needed to process an update to one randomly selected row in $A_2$ for different matrix sizes. REEVAL performs two matrix-matrix multiplications, while \IVM performs only one. In the hash-based implementation, the gap between \DF and \IVM grows from $28$x for $n=256$ to $92$x for $n=4,096$; similarly, in the Octave implementation, the same gap grows from $16$x for $n=256$ to $236$x for $n=16,384$. This confirms the difference in the asymptotic complexity of these strategies.

Our next experiment considers rank-$r$ updates to $A_2$, which can be decomposed into a sum of $r$ rank-$1$ tensors, $\delta{A_2} = \sum_{i\in[r]} u_i \TR{v_i}$. \DF processes $\delta{A_2}$ as a sequence of $r$ rank-$1$ updates in $\bigO{rn^2}$ time, while both REEVAL and \IVM take as input one full matrix $\delta{A_2}$ and maintain the product in $\bigO{n^3}$ time per each rank-$r$ update. \IVM has the same performance as REEVAL. Figure~\ref{fig:MCM} (right) shows that the average time \DF takes to process a rank-$r$ update for different $r$ values and the matrix size $4,096$ is linear in the tensor rank $r$. 
Under both implementations in DBToaster and Octave, incremental computation is faster than reevaluation for updates with rank $r\leq 96$.
With larger matrix sizes, the gap between reevaluation and incremental computation increases, which enables incremental maintenance for updates of higher ranks.


\begin{figure*}[t]
  \centering   
  \includegraphics[width=0.47\textwidth]{Figure_Cofactor_Retailer_IVM_trace_ALL_vs_INVENTORY_unified}
  \quad\;\;
  \includegraphics[width=0.47\textwidth]{Figure_Cofactor_Housing_IVM_trace_ALL_unified}
  % \vspace*{-0.8em}
  \caption{Incremental maintenance of the cofactor matrix over the {\em Retailer} dataset (left) and {\em Housing} dataset (right) under updates of size $1,000$ to all relations with a one-hour timeout. The ONE plots consider updates to the largest relation only. }
  \label{fig:cofactor_IVM_trace_ALL}
\end{figure*}

\subsection{Cofactor Matrix Computation} 

We benchmark the performance of maintaining a cofactor matrix for learning regression models over a natural join. We compute the cofactor matrix over all variables of the join query (i.e., over all attributes of the input database), which suffices to learn linear regression models over {\em any label and set of features} that is a subset of the set of variables~\cite{OS:PVLDB:16}. This is achieved by specializing the convergence step to the relevant restriction of the cofactor matrix. In end-to-end learning of regression models over factorized joins in the {\em Retailer} and {\em Housing} datasets, the convergence step takes orders of magnitude less time compared to the data-dependent cofactor matrix computation~\cite{SOC:SIGMOD:2016}.

In addition to the three incremental strategies from before, we now also benchmark 
\DBTRING, DBToaster's recursive IVM strategy with payloads from the degree-$m$ ring (cf.~Section~\ref{sec:application-lr}) instead of scalars, 
and \SQLOPT, an optimized SQL encoding of cofactor matrix computation. 
The latter arranges regression aggregates -- recall there are quadratically many such aggregates in the number of query variables -- into a {\em single} aggregate column indexed by the degree of each query variable. 
This encoding takes as input a variable order and constructs one SQL query that intertwines join and aggregate computation by pushing (partial) regression aggregates (counts, sums, and cofactors) past joins~\cite{Olteanu:FactorizedDB:2016:SIGREC}. 


We consider updates to all relations in the {\em Retailer} and {\em Housing} datasets. 
In the {\em Retailer} schema, \DF and \SQLOPT rely on a given variable order.
These two strategies store $9$ views each: five views over the input relations, three intermediate views, and the top-level view; \DBTRING stores four additional views, $13$ in total. These views are identical to those used for maintaining a sum aggregate but have different payloads.
DBToaster's recursive higher-order IVM and first-order IVM use scalar payloads and fail to effectively share the computation of regression aggregates, materializing linearly many views in the size of the cofactor matrix: \DBT and \IVM use $3,814$ and respectively $995$ views to maintain $990$ aggregates.
In the {\em Housing} schema, where all relations join on one variable, \DF and \SQLOPT materialize one view per relation and the root view, $7$ in total, while \DBT and \IVM use $702$ and respectively $412$ views to maintain $406$ aggregates.
\DF and \DBTRING use identical strategies for the {\em Housing} dataset.

Figure~\ref{fig:cofactor_IVM_trace_ALL} shows the throughput of these techniques as they process an increasing fraction of the stream of tuple inserts. 
The {\em Retailer} stream consists of inserts into the largest relation mostly, and since the variables of this relation form a root-to-leaf path in the variable order, processing a single-tuple update takes $\bigO{1}$ time for \DF and \SQLOPT. 
The former outperforms the latter due to efficient encoding of triples of aggregates $(\LRringC,\LRringS,\LRringQ)$ as payloads containing vectors and matrices. 
\DBTRING's additional views cause non-constant update times to the largest relation, which means $8.7$x lower average throughput than \DF.
The two approaches with scalar payloads, \DBT and \IVM, need to maintain too many views  and fail to process the entire stream within a one-hour limit.


The query for {\em Housing} is a star join with all relations joining on the common variable, which is the root in our variable order. Thus, \DF and \SQLOPT can process a single tuple in $\bigO{1}$ time. \DBTRING and \DF use the same strategy in this case.
\DBT exploits the conditional independence in the derived deltas to materialize each input relation separately such that all non-join variables are aggregated away. 
Although each materialized view has $\bigO{1}$ maintenance cost per update tuple, the large number of such views in \DBT is the main reason for its poor performance. 
In contrast, \IVM stores entire tuples of the input relations including non-join variables.
On each update, \IVM recomputes an aggregate on top of the join of these input relations and the update.
Since an update tuple binds the value of the common join variable, the hypergraph of the delta query consists of disconnected components. 
DBToaster optimizes such a delta query by placing an aggregate around each component, 
that is, the delta first aggregates over each relation and then joins together the partial aggregates.
Even with this optimization, \IVM takes linear time, which explains its poorer performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


{\bf Memory Consumption.}
Figure~\ref{fig:cofactor_IVM_trace_ALL} shows that \DF achie\-ves the lowest memory utilization on both datasets while providing orders of magnitude better performance than its competitors! 
The reason behind the memory efficiency of \DF is twofold. 
First, it uses complex aggregates and factorization structures to express the cofactor matrix computation over a smaller set of views compared to \DBTRING and, even more, to \DBT and \IVM. 
Second, it encodes regression aggregates implicitly using vectors and matrices rather than explicitly using variable degrees, like in \SQLOPT. 
The occasional throughput hiccups in the plot are due to expansion of the underlying data structures used for storing views.

{\bf The Effect of Update Workload.}
Our next experiment studies the effect of different update workloads on performance. 
We consider the {\em Retailer} dataset and two possible update scenarios: (1) all relations can change, in which case every view in the view tree needs to be materialized; (2) only the largest relation changes, while all others are static (denoted as ONE in Figure~\ref{fig:cofactor_IVM_trace_ALL}). 
In the latter scenario, we can precompute the views that are unaffected by changes and avoid materialization of those views that do not directly join with the updated relation. Thus, restricting updates to only one relation leads to materializing fewer views, which in turn reduces the maintenance overhead. 
Figure~\ref{fig:cofactor_IVM_trace_ALL} shows the throughput of processing updates for  the incremental maintenance of the cofactor matrix in these two scenarios. If we restrict updates only to a relation, we can avoid materializing all the views on the leaf-to-root path covered by that relation.  
This corresponds to a streaming scenario where we compute a continuous query and do not store the stream. Restricting updates to only one relation improves the average throughput, $3.2$x in \DF and $1.3$x in \SQLOPT, and also decreases memory consumption (note the log $y$-axis).
The latter also reflects in smoother throughput curves for the ONE variants. 
In \DBT, restricting updates brings $\bigO{1}$ time updates per view, yet the number of views is still large.
\DF and \DBTRING use identical materialization strategies here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}[t]
\begin{minipage}[t]{.475\textwidth}
  \centering   
  \includegraphics[width=\columnwidth]{Figure_Cofactor_Triangle_IVM_trace_ALL_unified}
  \caption{Incremental maintenance of the cofactor matrix on top of the triangle query on {\em Twitter} for updates of size $1,000$ to all input relations.}
  \label{fig:cofactor_Triangle_IVM_trace_ALL}
\end{minipage}
\quad
\begin{minipage}[t]{.475\textwidth}
  \centering   
  \includegraphics[width=\columnwidth]{Figure_Batch_sizes_unified2}
  \caption{Incremental maintenance of the cofactor matrix under batch updates of different sizes to all input relations. }
  % \vspace*{-1em}
  \label{fig:cofactor_IVM_batch_sizes_ALL}
\end{minipage}%
% \subfloat[Cofactor matrix on Twitter]{
%   \centering   
%   \includegraphics[width=0.475\columnwidth]{Figure_Cofactor_Triangle_IVM_trace_ALL_unified}  
%   \label{fig:cofactor_Triangle_IVM_trace_ALL}
% }
% % 
% \subfloat[Batch Size Effect]{
%   \centering
%   \includegraphics[width=0.475\columnwidth]{Figure_Batch_sizes_unified2}
%   \label{fig:cofactor_IVM_batch_sizes_ALL}
% }
% \caption{(a) The performance of incremental maintenance of the cofactor matrix on top of the triangle query over the {\em Twitter} dataset for updates of size $1,000$ to all input relations. (b) Incremental view maintenance of the cofactor matrix under batch updates of different sizes to all input relations.}
\end{figure*}

% \begin{figure}[t]
%   \centering   
%   \includegraphics[width=0.475\columnwidth]{Figure_Cofactor_Triangle_IVM_trace_ALL_unified}
%   \caption{The performance of incremental maintenance of the cofactor matrix on top of the triangle query over the {\em Twitter} dataset for updates of size $1,000$ to all input relations.}
%   \label{fig:cofactor_Triangle_IVM_trace_ALL}
% \end{figure}

{\bf Cofactor Matrix Computation over the Triangle Query.}
We analyze the cofactor matrix computation over the triangle query on the {\em Twitter} dataset and updates of size $1,000$ to all the relations.
\DF uses the view tree from Figure~\ref{fig:triangle_hypergraph_viewtree}~(right) without the indicator projection and materializes the join of $S$ and $T$ of size $\bigO{N^2}$. 
Its time complexity for a single-tuple update to $R$ is $\bigO{1}$, but updating the join of $S$ and $T$ takes $\bigO{N}$. 
\DBTRING uses payloads from the degree-$3$ ring and materializes all three such pairwise joins, each requiring linear time maintenance. 
\DBT uses scalar payloads and materializes $21$ views (to maintain $6$ aggregates), out of which $12$ views are over two relations. Its time complexity for processing single-tuple updates to either of the three relations is also $\bigO{N}$.
The \IVM strategy maintains just the input relations and recomputes the delta upon each update in linear time.

The throughput rate of the strategies that materialize views of quadratic size declines sharply as the input stream progresses. \DBT exhibits the highest processing and memory overheads caused by storing $12$ auxiliary views of quadratic size. \DBTRING underperforms \DF due to maintaining two extra views of quadratic size, which contribute to a $2.3$x higher peak memory utilization. \IVM exhibits a $42$\% decline in performance after processing the entire trace due to its linear time maintenance. The extent of this decrease is much lower compared to the other approaches with the quadratic space complexity. For updates to $R$ only, \DFONE (which is identical to \DBTRING's ONE variant) requires one lookup in the materialized join of $S$ and $T$ per update. This strategy has two orders of magnitude higher throughput than \IVM at the cost of using $23$x more memory.

Clique queries like triangles provide no factorization opportunities. Materializing auxiliary views to speed up incremental view maintenance increases memory and processing overheads. However, \DF can exploit indicator projections to bound the size of such materialized views, as described in Section~\ref{sec:cyclic_queries}. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{figure}[t]
%   \centering   
%   \includegraphics[width=0.475\columnwidth]{Figure_Batch_sizes_unified}
%   \caption{Incremental view maintenance of the cofactor matrix under batch updates of different sizes to all input relations. }
%   % \vspace*{-1em}
%   \label{fig:cofactor_IVM_batch_sizes_ALL}
% \end{figure}


{\bf The Effect of Batch Size on IVM.}
This experiment evaluates the performance of maintaining a cofactor matrix for batch updates of different sizes. Figure~\ref{fig:cofactor_IVM_batch_sizes_ALL} shows the throughput of batched incremental processing for batch sizes varying from $100$ to $100,000$ on the {\em Retailer}, {\em Housing}, and {\em Twitter} datasets for updates to all relations.
We show only the best three approaches for each dataset.

We observe that using very large or small batch sizes can have negative performance effects: Iterating over large batches invalidates previously cached data resulting in future cache misses, whereas using small batches cannot offset the overhead associated with processing each batch. 
Using batches with $1,000-10,000$ tuples delivers best performance in most cases, except when needed to incrementally maintain a large number of views. This conclusion about cofactor matrix computation is in line with similar findings on batched delta processing in decision support workloads~\cite{Nikolic:Batching:2016:SIGMOD}.

Batched incremental processing is also beneficial for one-off computation of the entire cofactor matrix. Using medium-sized updates can bring better performance, cf.\@ Figure~\ref{fig:cofactor_IVM_batch_sizes_ALL}, but can also lower memory requirements and improve cache locality during query processing. 
For instance, incrementally processing the {\em Retailer} dataset in chunks of $1,000$ tuples can bring up to $2.45$x better performance compared to processing the entire dataset at once.



\begin{figure*}[t]
  \centering   
  \includegraphics[width=0.48\textwidth]{Figure_FullJoin_Retailer}
  \quad
  \includegraphics[width=0.48\textwidth]{Figure_FullJoin_Housing}
  % \vspace*{-1em}
  \caption{Incremental maintenance using relational and factorized payloads for the natural joins of the {\em Retailer}  (left) and of the {\em Housing} (right) datasets under updates of size $1,000$ to the largest relation ({\em Retailer}) and all input relations ({\em Housing}).}
  \label{fig:FullJoin_Factorized_Relational}
\end{figure*}

\subsection{Factorized Computation of Conjunctive Queries}

We analyze \DF on queries whose results are stored as keys with integer multiplicities using listing representation ({\tt List$\;$keys}) and as relational payloads using factorized and listing representations ({\tt Fact$\;$payloads} and {\tt List$\;$payloads}).
Figure~\ref{fig:FullJoin_Factorized_Relational} (left) considers the natural join of {\em Retailer} under updates to the largest relation. The factorized payloads reduce the memory consumption by $4.4$x, from $34$GB to $7.8$GB, improve the average throughput by $2.8$x and $3.7$x (and the overall run time by $3.2$x and $4.2$x) compared to using the two listing encodings.
Figure~\ref{fig:FullJoin_Factorized_Relational} (right) considers the natural join of {\em Housing} under updates to all input relations.  The number of tuples in the dataset varies from $150,000$ (scale 1) to $1,400,000$ (scale 20), while the size of the listing (factorized) representation of natural join grows cubically (linearly) with the scale factor. The two listing encodings blow up the memory consumption and computation time for large scales. Storing tuples in the listing representation using payloads instead of keys avoids the need for hashing wide keys, which makes the joins slightly cheaper. For {\em Housing} and factorized representation, the root view stores $25,000$ values of the join variable regardless of the scale. The root's children map these values to relational payloads for each relation. For the largest scale, {\tt Fact$\;$payloads} is $481$x faster and takes $548$x less memory than {\tt List$\;$payloads} ($410$ms vs. $197$s, $195$MB vs. $104$GB), and {\tt List$\;$keys} exceeds the available memory.


\nop{In this setting, DBToaster's recursive IVM is equivalent to the {\tt List$\;$keys} strategy.}

\nop{The compression effect is significant but limited due to the key constraints in {\em Retailer}. 
The hash maps used for storing the views maintain memory pools of increasingly larger sizes, which reflects in occasional throughput hiccups that correspond to the expansion of the underlying data structures. }


