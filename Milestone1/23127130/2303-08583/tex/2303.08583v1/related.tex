%!TEX root = main.tex

\section{Related Work}
\label{sec:related_work}

To the best of our knowledge, ours is the first approach to propose factorized IVM for a range of distinct applications. It extends non-trivially two lines of prior work: higher-order delta-based IVM and factorized computation of in-database analytics. 

Our view language is modelled on functional aggregate queries over semirings~\cite{FAQ:PODS:2016} and generalized multiset relations over rings~\cite{DBT:VLDBJ:2014}; the latter allowed us to adapt DBToaster to factorized IVM.


{\bf IVM.} IVM is a well-studied area spanning more than three deca\-des~\cite{Chirkova:Views:2012:FTD}. Prior work extensively studied IVM for various query languages and showed that the time complexity of IVM is lower than of recomputation. We go beyond prior work on higher-order IVM for queries with joins and aggregates, as realized in DBToaster~\cite{DBT:VLDBJ:2014}, and propose a unified approach for factorized computation of aggregates over joins~\cite{BKOZ:PVLDB:2013}, factorized incremental computation of linear algebra~\cite{NEK:SIGMOD:2014}, and in-database machine learning over database joins~\cite{SOC:SIGMOD:2016}. DBToaster uses one materialization hierarchy per relation in the query, whereas \DF uses one view tree for all relations. DBToaster can thus have much higher space requirements and update times. As we observed experimentally, it does not consider the maintenance of composite aggregates such as the covariance matrix.
IVM over array data~\cite{Zhao:2017:ArrayIVM} targets scientific workloads but without exploiting data factorization.

Our approach over the relational payload ring strict\-ly subsumes prior work on factorized IVM for acyclic joins \cite{DynYannakakis:SIGMOD:2017} as it can support arbitrary joins.
\DF supports efficiently free-connex acyclic que\-ries~\cite{DynYannakakis:SIGMOD:2017} and $q$-hierarchical queries~\cite{Nicole:PODS:2017}.
\nop{Recent work on in-data\-base maintenance of linear regression models shows how to compute such models using previously computed mo\-dels over distinct sets of features~\cite{Gupta:CORR:2015}. Its contribution is complementary to ours and shares a similar goal with prior work on reusing gradient computation to efficiently explore the space of possible regression models~\cite{OS:PVLDB:16}.} Exploiting key attributes to enable succinct delta representations and accelerate maintenance complements our approach~\cite{Katsis:idIVM:2015}.
Our framework generalizes the main idea of the LINVIEW approach~\cite{NEK:SIGMOD:2014} for maintaining matrix computation over arbitrary joins. 

Most commercial databases support IVM for restricted classes of queries, e.g., Oracle~\cite{Oracle:RestrictionsIVM} and SQLServer \cite{SQLServer:RestrictionsIVM}.
LogicBlox supports higher-order IVM for Datalog (meta)programs~\cite{LB:SIGMOD:2015,GOW:PVLDB:2015}. Trill is a streaming engine that supports incremental processing of relational-style que\-ries but no complex aggregates like cofactor matrices~\cite{chandramouli2014trill}. Differential Dataflow~\cite{McSherry:DiffDataflow:2013} supports incremental processing for expressive programs with recursion. There is an entire line of work on maintenance for recursive Datalog, a summary is provided in a recent article~\cite{Motik:FBF:2019}.

{\bf Static In-DB analytics.} The emerging area of in-data\-base analytics has been recently overviewed in two tutorials~\cite{Polyzotis:SIGMOD:Tutorial:17,Kumar:SIGMOD:Tutorial:17}.  Several systems support analytics over normalized data via a tight integration of databases and machine learning~\cite{MLlib:JMLR:2016,MADlib:2012,Rusu:2015,Polyzotis:SIGMOD:Tutorial:17,Kumar:SIGMOD:Tutorial:17}. Other systems integrate with R to enable in-situ data processing using domain-specia\-lized routines~\cite{ZCDDMMFSS12,Brown:SciDB:2010:SIGMOD}. The closest in spirit to our approach is work on learning models over factorized joins \cite{Rendle13,SOC:SIGMOD:2016,OS:PVLDB:16,ANNOS:TODS:2020}, pushing ML tasks past joins~\cite{Kumar:InDBMS:2012,LMFAO:SIGMOD:2019} and on in-database linear algebra~\cite{Boehm:VLDB:2016,Arun:VLDB:2017,Figaro:SIGMOD:2022}, yet they do not consider incremental maintenance.

{\bf Learning.} There is a wealth of work in the ML community on incremental or online learning over {\em arbitrary} relations~\cite{OnlineML:2011}. Our approach learns over {\em joins} and crucially exploits the join dependencies in the underlying training dataset to improve the runtime performance. 

\nop{
Our framework can be used for learning regression models over joins, which follows a recent line of research on marrying databases and machine learning~\cite{MADlib:2012,CMPW13,BTRSTBV14, KuNaPa15,MLlib:JMLR:2016,Neumann15,HBTRTR15,CGLPVJ14,SSMBRE15,RABCJKR15,Rusu:2015} and in particular builds on {\em static} factorized in-database learning~\cite{Kumar:InDBMS:2012,Rendle13,SOC:SIGMOD:2016,OS:PVLDB:16}. Our factorization approach is that from prior work~\cite{SOC:SIGMOD:2016,OS:PVLDB:16}. Limited forms of factorized learning have been also used by Rendle~\cite{} and Kumar et al.~\cite{KuNaPa15}. The former considers zero-suppresed design matrices for high-degree regression models called factorization machines. The latter proposes a framework for learning generalized linear models over key-foreign key joins in a distributed environment.
}
\nop{
Most efforts in the database community are on designing systems to support large-scale machine learning libraries on distributed architectures~\cite{Kumar:InDBMS:2012}, e.g., MLLib~\cite{MLlib:JMLR:2016} and DeepDist~\cite{Neumann15} on Spark \cite{ZCDDMMFSS12}, GLADE~\cite{Rusu:2015}, MADlib \cite{MADlib:2012} on PostgreSQL, SystemML \cite{HBTRTR15,BTRSTBV14}, system benchmarking~\cite{CGLPVJ14} and sample generator for cross-validate
learning~\cite{SSMBRE15}. 
}

