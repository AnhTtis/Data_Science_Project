
\section{Proofs}\label{app:proof}
\subsection{Proof of Theorem~\ref{thero:emb}}\label{proof:emb}

\noindent \textit{\textbf{Proof:}}
We concentrate on the center node's embedding obtained from a $K$-layer GCN with 1-hop polynomial filter $\Phi (L)=I d-L$ whch is the common used GNN model. We denote the embedding of node $x_i \forall i=1, \cdots, n$ from a node-wise view in the final layer of the GCN as
$$
z_i^{(K)}=\sigma\left(\sum_{j \in \mathcal{N}\left(x_i\right)} a_{i j} z_j^{(K-1)^T} \theta^{(K)}\right) \in \mathbb{R}^d,
$$
where $a_{i j}=[\Phi (L)]_{i j} \in \mathbb{R}$ the weighted link between node $i$ and $j$; and $\theta^{(K)} \in \mathbb{R}^{d \times d}$ is the weight for the $K$ th layer sharing across nodes. Then $\theta=\left\{\theta^{(\ell)}\right\}_{\ell=1}^K$. We may denote $z_i^{(\ell)} \in \mathbb{R}^d$ similarly for $\ell=1, \cdots, K-1$, and $z_i^0=x_i \in \mathbb{R}^d$ as the node feature of center node $x_i$. With the assumption of GCN in the statement, we consider that only the k-hop ego-graph $g_i$ centered at $x_i$ is needed to compute $z_i^{(K)}$ for any $i=1, \cdots, n$ instead of the whole of $G$.

Denote $L_{g_i}$ as the out-degree normalised graph Laplacian of $g_i$, which is defined with respect to the direction from leaves  to centre node in $g_i$. We write the $\ell$ th layer embedding in following form
\begin{equation}
\left[z_i^{(\ell)}\right]_{K-\ell+1}=\sigma\left(\left[\Phi\left(L_{g_i}\right)\right]_{K-\ell+1}\left[z_i^{(\ell-1)}\right]_{K-\ell+1} \theta^{(\ell)}\right).
\end{equation}

Assume that $\forall i$, $\max _{\ell}\left\|z_i^{(\ell)}\right\|_2 \leq c_z $, and $\max _{\ell}\left\|\theta^{(\ell)}\right\|_2 \leq c_\theta$. Suppose that the activation function $\sigma$ is $\kappa_\sigma$-Lipschitz function Then, for $\ell=1, \cdots, K-1$, we have
\begin{equation}
\begin{aligned}
&\left\|\left[z_{i}^{(\ell)}\right]_{K-\ell}-\left[z_{i^{\prime}}^{(\ell)}\right]_{K-\ell}\right\|_2 \\
&\leq\|[\sigma\left(\left[\Phi\left(L_{g_i}\right)\right]_{K-\ell+1}\left[z_i^{(\ell-1)}\right]_{K-\ell+1} \theta^{(\ell)}\right) \\
&-\sigma\left(\left[\Phi\left(L_{g_{i^{\prime}}}\right)\right]_{K-\ell+1}\left[z_{i^{\prime}}^{(\ell-1)}\right]_{K-\ell+1} \theta^{(\ell)}\right)]_{K-\ell}) \|_2 \\
& \leq \kappa_\sigma c_\theta\left\|\Phi\left(L_{g_i}\right)\right\|_2\left\|\left[z_i^{(\ell-1)}\right]_{K-\ell+1}-\left[z_{i^{\prime}}^{(\ell-1)}\right]_{K-\ell+1}\right\|_2 \\
&+\kappa_\sigma c_\theta c_z\left\|\Phi\left(L_{g_i}\right)-\Phi\left(L_{g_{i^{\prime}}}\right)\right\|_2 .
\end{aligned}
\end{equation}
Since $\left[\Phi\left(L_{g_i}\right)\right]_{K-\ell+1}$ is the principle submatrix of $\Phi\left(L_{g_i}\right)$. We equivalently write the above equation as $A_{\ell} \leq a A_{\ell-1}+b$, where $a$ and $b$ is the coefficient. And we have
\begin{equation}
\begin{aligned}
A_{\ell} &\leq  a A_{\ell-1}+b \\
&\leq  a^{2} A_{\ell-2}+b(1+a) \\
& \cdots\\
&\leq a^{\ell} A_1+\frac{a^{\ell}+1}{a-1} b.\\
\end{aligned}
\end{equation}

Therefore, for any $\ell=1, \cdots, K$, we have an upper bound for the hidden representation difference between $g_i$ and $g_i^{\prime}$ by substitute coefficient $a$ and $b$,
\begin{equation}
\begin{aligned}
\left\|\left[z_i^{(\ell)}\right]_{K-\ell}-\left[z_{i^{\prime}}^{(\ell)}\right]_{K-\ell}\right\|_2 & \leq\left(\kappa_\sigma c_\theta\right)^{\ell}\left\|\Phi\left(L_{g_i}\right)\right\|_2^{\ell}\left\|\left[x_i\right]-\left[x_{i^{\prime}}\right]\right\|_2 \\
& +\frac{\left(\kappa_\sigma c_\theta\right)^{\ell}\left\|\Phi\left(L_{g_i}\right)\right\|_2^{\ell}+1}{\kappa_\sigma c_\theta\left\|\Phi\left(L_{g_i}\right)\right\|_2-1} \kappa_\sigma c_\theta c_z\left\|\Phi\left(L_{g_i}\right)-\Phi\left(L_{g_{i^{\prime}}}\right)\right\|_2 .
\end{aligned}
\end{equation}
Specifically, for $\ell=K$, we obtain the upper bound for center node embedding $\left\|\left[z_i^{(K)}\right]_0-\left[z_{i^{\prime}}^{(K)}\right]_0\right\| \equiv$ $\left\|z_i-z_{i^{\prime}}\right\|$. Since the attribute of each node as a scalar 1, we therefore have $\left\|\left[x_i\right]-\left[x_{i^{\prime}}\right]\right\|_2 =0$. Suppose that $\forall i$, $\left\|\Phi\left(L_{g_i}\right)\right\|_2 \leq c_L$ 
because that the the graph Laplacians are normalised. Since $\Phi$ is a linear function for $L$, We have 
\begin{equation}\label{eq:divergence}
\begin{aligned}
\left\|z_i-z_{i^{\prime}}\right\|_2 & \leq\frac{\left(\kappa_\sigma c_\theta c_L\right)^K+1}{\kappa_\sigma c_\theta c_L-1} 
c_\theta c_z\left\|\Phi\left(L_{g_i}\right)-\Phi\left(L_{g_{i^{\prime}}}\right)\right\|_2 \\
& \leq \kappa\left\|L_{g_i}-L_{g_{i^{\prime}}}\right\|_2, 
\end{aligned}
\end{equation}
where $\kappa=\frac{\left(\kappa_\sigma c_\theta c_L\right)^K+1}{\kappa_\sigma c_\theta c_L-1} 
c_\theta c_z$.

Therefore, we sequentially compute the representation divergence between node $i$ and node $j^{\prime}$ in $G_\text{train}$ and $G_\text{down}$ respectively. By means of Eq.(\ref{eq:divergence}), we have
\begin{equation}
\begin{aligned}
\left\|e(G_\text{train})-e(G_\text{down})\right\|_2
& = \frac{1}{mn}  \sum_{i=1}^m \sum_{j=1}^n \left\| z_i-z_{j^{\prime}}\right\|_2\\
&\leq \frac{\kappa}{mn}  \sum_{i=1}^m \sum_{j=1}^n \left\|L_{g_i}-L_{g_{j^{\prime}}}\right\|_2. \\
\end{aligned}
\end{equation}




 \subsection{Proof of Theorem \ref{theor:dis-prob}}\label{proof:dis-pro}
We first show the following lemma, which would be used in the proof of Theorem \ref{theor:dis-prob}, 
\begin{lemma}\label{lemma:convergent}
(Proposition 11.32 in~\cite{lovasz2012large}) For every graphon $W$, generating a $W$-random graph $\mathbb{G}(n, W)$ for $n=1,2, \ldots$ we get a graph sequence such that $\mathbb{G}(n, W) \rightarrow W$ with probability 1.
\end{lemma} 
\noindent \textit{\textbf{Proof of Theorem \ref{theor:dis-prob} :}} Since we assume $\mathcal{G}_\text{down}$ can be generated from $f(\{\alpha_i\},\{B_i\})$ with  probability 1, according to Lemma~\ref{lemma:convergent}, we can have that $\mathcal{G}_\text{down} \rightarrow f(\{\alpha_i\},\{B_i\} )$ with  probability 1.
On the other hand, since $B_\text{down}$ is the  graphon fitted by  $\mathcal{G}_\text{down}$, which means $\mathcal{G}_\text{down}$ is convergent to graphon $B_\text{down}$ and we have $\mathcal{G}_\text{down} \rightarrow B_\text{down} $. Hence $B_\text{down}$ is equivalent to $f$, then dist($f, B_\text{down}$) =0.

\subsection{Proof of Theorem~\ref{theor-preserve}}
\noindent \textit{\textbf{Proof:}}
Given an arbitrary set of graphons $X=\{B_i\}$ and $i=1...k$ the 
 general form of the convex combination of  graphons in $X$ can be represented as:
\begin{equation}
f(\{\alpha_i\},\{B_i\}) = \sum _{i=1}^k \alpha_i B_i,\quad \quad
(\sum _{i=1}^k \alpha_i = 1,\alpha_i \geq 0).
\end{equation}
Next we prove that our generator preserves the properties of graphons.
the  convex combination of  graphons is still a bounded symmetric function $[0,1]^2 \to [0,1]$.
First, we prove  that the convex combination $f(\{\alpha_i\},\{B_i\})$ is still symmetric. Since $f(\{\alpha_i\},\{B_i\})$ is symmetric if and only if it satisfies that $f(\{\alpha_i\},\{B_i\}) = f(\{\alpha_i\},\{B_i\})^T$, we have the following derivations:
\begin{equation}
\begin{aligned}
    {f(\{\alpha_i\},\{B_i\})}^T&=
(\alpha_1B_1+...+ \alpha_kB_k
)^T\\
&= \alpha_1B_1^T+...+ \alpha_kB_k^T
\\&=\alpha_1B_1+...+ \alpha_kB_k = f(\{\alpha_i\},\{B_i\}).
\end{aligned}
\end{equation}
Then we prove that the convex combination $f(\{\alpha_i\},\{B_i\})$ is still in $[0,1]$
Let $B_{max}$ indicates the maximum $B_i$, meanwhile $B_{min}$ indicates the minimum $B_i$
\begin{equation}
\begin{aligned}
 f(\{\alpha_i\},\{B_i\}) = (\alpha_1B_1+...+ \alpha_kB_k
) \leq \sum_{i=1}^k{\alpha_i}B_{max} = B_{max} \leq 1.
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
f(\{\alpha_i\},\{B_i\}) = (\alpha_1B_1+...+ \alpha_kB_k
) \geq  \sum_{i=1}^k{\alpha_i}B_{min} \geq B_{min} \ge 0.
\end{aligned}
\end{equation}
% Hence we have proved that the convex combination of graphons preserve the properties of graphon and is still a bounded symmetric function $[0,1]^2 \to [0,1]$.
Thus, for a set of graphon basis $X$, the generator space $\Omega$ is the set of all convex combinations of basis elements in $X$, $\Omega$ is the convex hull of $X$.

\subsection{Proof of Theorem~\ref{theor:diff}}

We first show the following two lemmas, which would be used in the proof of  Theorem~\ref{theor:diff}.

The first lemma is known as counting lemma for graphons, provided in Lemma 10.23 in~\cite{lovasz2012large}.
\begin{lemma}\label{lemma:counting}
Let $F$ be a graph motif and let $B$, $B'$ be two graphon . Then
we have
\begin{equation}
\left|t(F, B)-t\left(F, B^{\prime}\right)\right| \leq |F|\left\|B-B^{\prime}\right\|_{\square}.
\end{equation}
\end{lemma}
\noindent \textit{\textbf{Proof of Lemma~\ref{lemma:counting}:}}
\begin{equation}
\begin{aligned}
& \left|t(F, B)-t\left(F, B^{\prime}\right)\right| \\
& =\left|\int\left(\prod_{u_{i} v_{i} \in E} B\left(u_{i}, v_{i}\right)-\prod_{u_{i} v_{i} \in E} B^{\prime}\left(u_{i}, v_{i}\right)\right) \prod_{v \in V} d v\right| \\
& \leq \sum_{i=1}^{|E|}\left|\int\left(\prod_{j=1}^{i-1} B^{\prime}\left(u_{j}, v_{j}\right)\left(B\left(u_{i}, v_{i}\right)-B^{\prime}\left(u_{i}, v_{i}\right)\right) \prod_{k=i+1}^{|E|} B\left(u_{k}, v_{k}\right)\right) \prod_{v \in V} d v\right|.
\end{aligned}
\end{equation}


Each absolute value term in the sum is bounded by the cut norm $\left\|B-B^{\prime}\right\|_{\square}$. When we fix all other irrelavant variables (everything except $u_{i}$ and $v_{i}$ for the $i$-th term), altogether implying that

\begin{equation}
\left|t(F, B)-t\left(F, B^{\prime}\right)\right| \leq |F|\left\|B-B^{\prime}\right\|_{\square}.
\end{equation}
\begin{lemma}\label{lemma:cut norm}
The cut norm  of a graphon $\|B\|_{\square}$ is defined as 
\begin{equation}
\|B\|_{\square}=\sup _{S, T \subseteq[0,1]}\left|\int_{S \times T} B\right|,
\end{equation}
where the supremum is taken over all measurable subsets $S$ and $T$.
Obviously, suppose $\alpha \in \mathbb{R}$, we have
\begin{equation}
\|\alpha B\|_{\square}=\sup _{S, T \subseteq[0,1]}\left|\int_{S \times T} \alpha B\right|=\sup _{S, T \subseteq[0,1]}\left|\alpha \int_{S \times T} B\right|=\alpha\|B\|_{\square}.
\end{equation}

\end{lemma}
\noindent \textit{\textbf{Proof of Theorem~\ref{theor:diff}:}} Based on the Lemma~\ref{lemma:counting} and Lemma~\ref{lemma:cut norm}, we have the following derivations.
The $a-$th element $B_a$  in graphon basis has its corresponding motif set. Each  motif $F_a$ is expected to be preserved and to exhibit similar frequency (\emph{i.e., homomorphism density}) in  $f(\{\alpha_i\},\{B_i\})$. 

Applying Lemma~\ref{lemma:counting}, we have the following derivations:
\begin{equation}
\begin{aligned}
&|t(F_a,f(\{\alpha_i\},\{B_i\}))-t(F_a,B_a)| \leq |F_a|||f(\{\alpha_i\},\{B_i\}) -B_a|| _\square\\
&|t(F_a,f(\{\alpha_i\},\{B_i\}))-t(F_a,B_a)| \leq |F_a| ||\sum _{b=1}^k\alpha_b B_b - \sum _{b=1}^k \alpha_b B_a||_\square\\
&|t(F_a,f(\{\alpha_i\},\{B_i\}))-t(F_a,B_a)| \leq |F_a| ||\sum _{b=1}^k\alpha_b (B_b -B_a)||_\square.
\end{aligned}
\end{equation}
Combining with the triangle inequality, we have:
\begin{equation}
|t(F_a,f(\{\alpha_i\},\{B_i\}))-t(F_a,B_a)|\leq \sum _{b=1,b\neq a}^k  |F_a| \alpha_b || B_b -B_a||_\square.
\end{equation}

where a=1...k, $|F|$ represents the number of nodes in motif $F$, and $||
\cdot||_\square$ is the cut norm.


\subsection{Proof of Theorem~\ref{thm:down}}

\begin{lemma}\label{lemma:random}
    (Corollary $10.4$ in~\cite{lovasz2012large}). Let $W$ be a graphon, $n \geq 1,0<\varepsilon<1$, and let $F$ be a simple graph, then the $W$-random graph $\mathbb{G}=\mathbb{G}(n, W)$ satisfies
\begin{equation}
    \mathrm{P}(|t(F, \mathbb{G})-t(F, W)|>\varepsilon) \leq 2 \exp \left(-\frac{\varepsilon^2 n}{8 \mathrm{v}(F)^2}\right).
\end{equation}

\end{lemma}

\noindent \textit{\textbf{Proof of Theorem~\ref{thm:down}:}}
Apply Lemma~\ref{lemma:random} in our setting, let graphon $B$ as $W$ Lemma~\ref{lemma:random}, we have 
\begin{equation}
\mathrm{P}(|t(F, \mathbb{G})-t(F, B)|>\varepsilon) \leq 2 \exp \left(-\frac{\varepsilon^2 n}{8 \mathrm{v}(F)^2}\right).
\end{equation}





\section{Datasets}\label{app:data}
\vpara{Datasets for node classification.}
For pre-training datasets, following~\cite{Qiu2020GCCGC},  we collect the graph data from Academia (NetRep)~\cite{Ritchie2016ASP}, DBLP (SNAP)~\cite{Yang2012DefiningAE} and DBLP (NetRep)~\cite{Ritchie2016ASP} as the academic domain data. For movie domain, we utilize the graph from IMDB~\cite{Ritchie2016ASP}. As for social domain, the graph data from Facebook~\cite{Ritchie2016ASP} and LiveJournal~\cite{Backstrom2006GroupFI} is leveraged.
For the downstream dataset, we use data from transportation, academic and web domains.
Specifically, we collect the datasets from H-Index~\cite{Zhang2019OAGTL} and Chamelon~\cite{Rozemberczki2021MultiscaleAN} for academic and web domains, respectively.
The datasets in transportation domain are collected from US-Airport and Europe-Airport~\cite{Ribeiro2017struc2vecLN}.
Detailed statistics of pre-training and downstream datasets for node classification is summarized in Table~\ref{tab:dataset}.

\begin{table*}[!ht]
\renewcommand\arraystretch{1.2}
\centering
 \resizebox{0.95\linewidth}{!}
 {
    \setlength\tabcolsep{2pt} 
    \begin{threeparttable}
    \begin{tabular}{ccccccccccc}
     \toprule
        & \multicolumn{1}{c}{Type}  &  \multicolumn{1}{c}{Name}  &  \multicolumn{1}{c}{$|{V}|$}  &  \multicolumn{1}{c}{$|{E}|$}  &  avg deg & deg var& avg clustering coef & density & assortativity coef & transitivity \\
     \midrule
     \multirow{6}*{\setlength\tabcolsep{1pt}\rotatebox{90}{\textbf{pre-training}}} & {\textcolor[rgb]{0.404,0.173,0.075}{\textbf{academic}}}  &  Academia &  137,969  &  739,384  &  5.36 & 10.11 & 1.42e-01 & 3.88e-05 & 8.39e-03 & 7.65e-02 \\
     & ~   &  DBLP(SNAP) & 317,080 & 2,099,732 & 6.62 & 10.01& 6.32e-01 & 2.09e-05 & 2.67e-01 & 3.06e-01 \\
     & ~   &  DBLP(NetRep) & 540,686  & 30,491,458 & 56.41 & 66.24& 8.02e-01 & 1.04e-04 & 5.10e-01 & 6.56e-01  \\
     & {\textcolor[rgb]{0.765,0.4078,0.145}{\textbf{social}}}  &  Facebook  & 3,097,165 & 47,334,788 & 15.28 & 45.17 & 9.70e-02& 4.93e-06 & -5.57e-02 & 4.77e-02  \\
     & ~   &  LiveJournal & 4,843,953 & 85,691,368 & 17.69 & 52.02 & 2.74e-01& 3.65e-06 & 2.10e-02 & 1.18e-01 \\
     & {\textcolor[rgb]{0.878,0.690,0.169}{\textbf{movie}}}  & IMDB &  896,305  &  3,782,447 &  8.44 & 17.27& 5.79e-05 & 9.42e-06 & -5.30e-02 & 8.08e-05 \\
     \midrule
     \multirow{2}*{\rotatebox{90}{\textbf{downstream}}} &
    {\textcolor[rgb]{0.404,0.173,0.075}{\textbf{academic}}}  &  H-index & 5000 & 44,020 & 17.61& 31.91& 7.10e-01 & 3.52e-03 & 1.18e-01 & 5.66-01 \\
     & {\textcolor[rgb]{0.9,0.2,0.3294}{\textbf{web}}} &  Chameleon  & 2277  & 36,101 & 27.58 & 46.40 & 4.81e-01& 1.21e-02 & -1.99e-01 & 3.14e-01 \\
     & {\textcolor[rgb]{0.075,0.275,0.514}{\textbf{transportation}}}   & US-Airport & 1,190  & 13,599 & 22.86 & 40.45& 5.01e-01 & 1.92e-02 & 3.13e-02 & 4.26e-01 \\ 
     & ~ & Europe-Airport & 399 & 5,995 & 30.05 & 34.65& 5.39e-01 & 7.55e-02 & -2.25e-01 & 3.34e-01 \\
    \bottomrule
    \end{tabular}
    \end{threeparttable}
 }
\caption{The statistics of pre-training and downstream datasets (avg deg: average degree; deg var: degree variance; coef: coefficient) for node classification.}
\label{tab:dataset}
\vspace{-0.15in}
\end{table*}
\vpara{Datasets for graph classification.}
For pre-training datasets, to enrich the follow-up experimental analysis, we use scaffold split to partition the ZINC15 into five datasets (ZINC15-0,ZINC15-1,ZINC15-2,ZINC15-3 and ZINC15-4) according to their scaffolds~\cite{hu2019strategies}, such that the scaffolds are different in each dataset.
{For downstream datasets, we use BACE, BBBP, MUV, HIV, and ClinTox provided in~\cite{wu2018moleculenet}. }
BACE provides quantitative ($IC_{50}$) and qualitative (binary label) binding results for a set of inhibitors of human $\beta$-secretase 1, which merged a collection of 1522 compounds with their 2D structures and binary labels in MoleculeNet. BBBP includes binary labels for over 2000 compounds on their permeability properties. MUV contains 17 tasks for around 90 thousand compounds, designed for validation of virtual screening techniques. HIV was introduced to test the ability to inhibit HIV replication for over 40,000 compounds. ClinTox includes two classification tasks for 1491 drug compounds with known chemical structures.
Detailed statistics of pre-training and downstream datasets for graph classification is summarized in Table~\ref{tab:dataset_graph}.




\begin{table*}[!ht]
\renewcommand\arraystretch{1.2}
% \vspace{-0.1in}
\centering
 \resizebox{0.95\linewidth}{!}
 {
    \setlength\tabcolsep{2pt} 
    \begin{threeparttable}
    \begin{tabular}{ccccccccccc}
     \toprule
        &  \multicolumn{1}{c}{Name}  & avg node & avg edge & avg deg & deg var & density & closeness & assortativity coef & transitivity & avg clustering coef \\
     \midrule
     \multirow{6}*{\setlength\tabcolsep{1pt}\rotatebox{90}{\textbf{pre-training}}} &  ZINC15-0 &  26.95 & 29.24 & 2.16 & 0.70 & 0.08 & 0.19 & -0.31 & 4.22e-03 & 4.43e-03\\
     & ZINC15-1 & 25.69 & 27.67 & 2.14 & 0.71 & 0.09 & 0.20 & -0.33 & 3.08e-03 & 3.38e-03\\
     & ZINC15-2 & 26.86 & 29.21 & 2.17 & 0.69 & 0.08 & 0.19 & -0.31 & 3.66e-03 & 4.07e-03 \\
     & ZINC15-3 & 26.91 & 29.19 & 2.16 & 0.70 & 0.08 & 0.19 & -0.31 & 3.30e-03 & 3.66e-03\\
     & ZINC15-4 & 26.83 & 29.18 & 2.17 & 0.69 & 0.08 & 0.19 & -0.31 & 3.71e-02 & 4.09e-03\\
     \midrule
     \multirow{2}*{\rotatebox{90}{\textbf{downstream}}} &
        BACE & 34.08 & 36.85 & 2.16 & 0.76 & 0.07 & 0.17 & -0.36 & 6.10e-03 & 6.54e-03\\
     & BBBP & 24.06 & 25.95 & 2.13 & 0.75 & 0.11 & 0.24 & -0.27 & 2.21e-03 & 2.56e-03 \\
     & MUV & 24.23 & 26.27 & 2.16 & 0.68 & 0.09 & 0.20 & -0.27 & 8.75e-04 & 1.01e-03 \\ 
     & HIV & 25.51 & 27.46 & 2.14 & 0.74 & 0.10 & 0.22 & -0.26 & 2.11e-03 & 1.96e-03 \\
     & ClinTox & 26.15 & 27.88 & 2.10 & 0.77 & 0.11 & 0.23 & -0.34 & 2.98e-03 & 3.21e-03 \\
    \bottomrule
    \end{tabular}
    \end{threeparttable}
 }
\caption{The statistics of pre-training and downstream datasets (avg deg: average degree; deg var: degree variance; coef: coefficient) for graph classification. All these datasets are from molecular domain.}
% \JR{avg closeness?}}
\label{tab:dataset_graph}
\vspace{-0.15in}
\end{table*}


\section{Additional Results}

\subsection{Results of Pre-training Feasibility }\label{app:graph_class_res}
We here give additional plots to show the detailed correlation between estimated pre-training feasibility and the best downstream performance.
Figure~\ref{fig:corre_node_3} shows the results on node classification  when the selection budget is  3.
Figure~\ref{fig:corre_graph} shows the results on graph classification when the selection budget is 2 and 3 respectively.
We find that in all cases, there is a strong positive correlation between
pre-training feasibility and the best downstream performance.

\begin{figure*}[!h]
 \centering\setlength{\abovecaptionskip}{0.15cm}{
 \includegraphics[width=\linewidth]
 {figure/Figure_3.pdf}}
\caption{Pre-training feasibility vs. the best downstream performance on node classification when the selection buget is 3.}
\label{fig:corre_node_3}
\vspace{-0.1in}
\end{figure*}


 \begin{figure*}[!h]
 \centering\setlength{\abovecaptionskip}{0.15cm}{\includegraphics[width=\linewidth]{figure/corr_graph_2.pdf}}
 \centering{\includegraphics[width=\linewidth]{figure/corr_graph_3.pdf}}
\caption{Pre-training feasibility vs. best downstream performance on graph classification when the selection buget is 2 and 3. }
\vspace{-0.1in}
\label{fig:corre_graph}
\end{figure*}

\begin{table*}[h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccccccccc}
\toprule[1pt]
\multicolumn{1}{c}{\multirow{2}{*}{}} & \multicolumn{6}{c}{$N=2$}      & \multicolumn{6}{c}{$N=3$}    \\
\cmidrule[1pt](r){2-7}\cmidrule[1pt](r){8-13} \multicolumn{1}{c}{}                         & BACE  & BBBP  & MUV   & HIV   & ClinTox & Rank & BACE  & BBBP  & MUV   & HIV   & ClinTox & Rank \\ \bottomrule[1pt]
\cellcolor{ggray!90}All Datasets                                      & \cellcolor{ggray!90}74.86 & \cellcolor{ggray!90}62.67 & \cellcolor{ggray!90}73.80 & \cellcolor{ggray!90}68.31 & \cellcolor{ggray!90}60.58   & \cellcolor{ggray!90}-    & \cellcolor{ggray!90}74.86 & \cellcolor{ggray!90}62.67 & \cellcolor{ggray!90}73.80 & \cellcolor{ggray!90}68.31 & \cellcolor{ggray!90}60.58  & \cellcolor{ggray!90}-    \\
\toprule
Graph Statistics                             &  71.52&  58.47 & 69.42 & 70.31    &62.99 & 2    & 65.95&  62.36 & 68.98 & 68.83    &59.86 & 5    \\
EGI                                          & 68.46 & \textbf{65.65}&  69.42 & 68.29  &  \textbf{65.21}  & 3    & 68.48 &  61.00 & 68.98  &68.83    &60.35 & 3    \\
Clustering Coefficient                       &  71.52 & 60.06 & 69.42 & 70.31  &  58.91  & 5    &65.95 & 61.00 & 68.98&  68.83 &   60.35   & 6    \\
Spectrum of Graph Laplacian                  & 69.43&  \textbf{65.65}  &68.57 & 68.88  &  59.82  & 6    & 70.21  &61.00 & 69.14 & 69.18    &\textbf{63.02}& 2    \\
Betweenness Centrality           &  71.52 & 63.20 & 69.42 & \textbf{71.72} &   58.91 & 4    & 67.08  &\textbf{67.41} & 68.98  &68.83  &  60.35  & 4    \\ \midrule
W2PGNN                                       & \textbf{73.33} & 65.46  &\textbf{74.17} & 70.69  &  64.21   & 1    & \textbf{73.54} & 65.02  &\textbf{72.49}  &\textbf{71.18}&    62.26 & 1 \\ \bottomrule[1pt]   
\end{tabular}
}
\caption{Graph classification results when performing pre-training on different selected pre-training data. We also provide the results of using all pre-training data without selection for your reference (see ``All Datasets'' in the table).}
\label{tab:graph_classification_data_selection_results}
% \vspace{-0.2in}
\end{table*}
% \vspace{-0.25in}
\subsection{Results of Pre-Training Data Selection}\label{app:graph_class_res2}
Table~\ref{tab:graph_classification_data_selection_results} shows the results of pre-training data selection on graph classification tasks. 
The backbone pre-training model used here is GraphCL~\cite{you2020graph}.
We can see that the pre-training data selected by W2PGNN
ranks the first, which suggests that the effectiveness of our strategy on the graph classification task is still significant.


\vspace{-0.05in}
\section{Computation Complexity Analysis}\label{app:complexity}
We show the time complexity of W2PNN and the traditional  solution.
Suppose that we have $n_1$  and $n_2$ (sub)graphs sampled from pre-training data and  downstream data respectively. Denote $|V|$ and $|E|$ as the average number of nodes and edges per (sub)graph.

The time complexity of W2PGNN consists of three components: 
computation of three graphon base ($\{B_i\}_\text{topo}$, $\{B_i\}_\text{domain}$, $\{B_i\}_\text{integr}$), graphon estimation of downstream data (\emph{i.e.}, $B_\text{down}$), computation of GW distance (\emph{i.e.}, $\operatorname{dist}(\cdot, \cdot)$):
(1) Among the estimation of three graphon base, the topological feature extraction is the most time-consuming one. It mainly involves the topological feature extractor, K-means clustering and graphon estimation of pre-training data, which costs {$O(n_1|E|)$} (which is taken as the complexity of the most costly property closeness)~\cite{freeman2002centrality}, $O(n_1)$~\cite{Kaiser2008MeanCC} and $O(n_1|V|^2)$~\cite{channarond2012classification}, respectively. The domain and integrated graphon basis only include the graphon estimation of pre-training data and cost $O(n_1|V|^2)$~\cite{channarond2012classification}.
(2) The graphon estimation of downstream data costs $O(n_2|V|^2)$~\cite{channarond2012classification}.
(3) The computation of GW distance costs $O(|V|^3)$ ~\cite{peyre2016gromov}, which can be ignored because we have $|V| \ll n_1+n_2$
(For node-level transferable patterns, extracting the ego-network of sampled nodes via breadth first search costs $O((n_1+n_2)(|V|+|E|))$, which can be ignored).
So the overall time complexity of W2PGNN is $O((n_1+n_2)|V|^2)$.



For comparison, traditional solution make $l_1 \times l_2$ ``pre-train and fine-tune'' attempts, if there are $l_1$ pre-training models and $l_2$ fine-tuning strategies. Suppose the batch size of pre-training as $b$ and the representation dimension as $d$. The time complexity of each pre-training model (taking the most general graph pre-training model GCC~\cite{Qiu2020GCCGC} as example) is typically from data augmentation, GNN encoder and  contrastive loss, which costs $O\left(n_1|V|^3\right)$ (subgraphs sampled from random walk with restarts as augmentation)~\cite{xia2019random}, {$O\left(n_1|E|d\right)$} (GIN as graph encoder) and $O\left(n_1bd\right)$~\cite{li2022let}, respectively. 
(Note that other data augmentations like dropping nodes cost $O(|V|^2)$, but they cannot achieve good performance on node classification in our pre-training experiments.)
The time complexity of each fine-tuning strategy involves the inference of pre-trained model  $O\left(n_2 (|V|^3 + |E|d)\right)$ and downstream predictor  $O\left(n_2d\right)$ (which can be ignored), under the simple freezing mode.
% (\emph{i.e}, the pre-trained model without any changes in parameters is directly applied to the downstream tasks via learnable downstream predictor).
Thus the overall time complexity of traditional solution is 
 $O\left(l_1l_2((n_1+n_2)(|V|^3 + |E|d)+n_1bd)\right)$.
