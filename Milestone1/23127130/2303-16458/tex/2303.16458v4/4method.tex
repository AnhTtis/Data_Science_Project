% \vspace{-0.05in}
\section{Methodology}
In this section, we first present our proposed framework W2PGNN to answer when to pre-train GNNs in Section~\ref{subsec:overview}.
Based on the framework, we further introduce the measure of
the feasibility of performing pre-training in Section~\ref{subsec:fea}.
Then in Section~\ref{subsec:3.3}, we discuss our approximation to the feasibility of pre-training.
Finally, the complexity analysis of W2PGNN is provided in Section~\ref{subsec:cost}.

\begin{figure}[t]
\centerline{\includegraphics[width=0.9\linewidth]{figure/fig_framework.pdf}} 
\caption{Illustration of our proposed framework W2PGNN to answer when to pre-train GNNs.}
\vspace{-0.1in}
\label{fig:model}
\end{figure}
\vspace{-0.05in}
\subsection{Framework Overview}
\label{subsec:overview}

 W2PGNN framework provides a guide to answer
\emph{\underline{w}hen \underline{to} \underline{p}re-train \underline{GNN}s from a data generation perspective}.
The key insight is that if downstream data can be generated with high probability by a graph generator that summarizes the pre-training data, the downstream data would
 present high feasibility of performing pre-training.

The overall framework of W2PGNN can be found in Figure~\ref{fig:model}.
Given the \emph{input space} consisting of pre-training graphs, we fit them into a graph generator in the \emph{generator space}, from which the graphs generated constitute the \emph{possible downstream space}.
More specifically,
an ideal graph generator should inherit different kinds of topological patterns, based on which new graphs can be induced. Therefore, we first construct a graphon basis $\mathcal{B}=\{B_1, B_2, \cdots, B_k\}$, where each element $B_i$ represents a graphon fitted from a set of (sub)graphs with similar patterns (\emph{i.e.}, the blue dots $\vcenter{\hbox{\includegraphics[width=2.1ex,height=2.1ex]{figure/point.pdf}}}$). 
To access different combinations of generator basis,
each $B_i$ is assigned with a corresponding weight $\alpha_i$ (\emph{i.e.}, the width of blue arrow $\vcenter{\hbox{\includegraphics[width=2.3ex,height=2.1ex]{figure/arrow.pdf}}}$) and their combination gives rise to a graph generator (\emph{i.e.}, the blue star $\vcenter{\hbox{\includegraphics[width=2.1ex,height=2.1ex]{figure/star.pdf}}}$). All weighted combinations compose the generator space $\Omega$ (\emph{i.e.}, the gray surface $\vcenter{\hbox{\includegraphics[width=3ex,height=2ex]{figure/surface.pdf}}}$), from which graphs generated form the possible solution space of downstream data (shorted as possible downstream space). 
{The generated graphs are those that could benefit from the pre-training data, we say that they exhibit \emph{high feasibility} of performing pre-training.}
 
In the following, we introduce the workflow of W2PGNN in the input space, the generator space and the possible downstream space in detail.
Then, the application cases of W2PGNN are given for different practical use.
 

\vpara{Input space.}
The input space of W2PGNN is composed of nodes' ego-networks or graphs. For node-level pre-training, we take the nodes' ego-networks to constitute the input space; For graph-level pre-training, we take the graphs  (\emph{e.g.}, small molecular graphs) as input space.


\vpara{Generator space.}
As illustrated in Figure~\ref{fig:model}, each point (\emph{i.e.}, graph generator) in the generator space $\Omega$ is a  convex combination of  generator basis $\mathcal{B}=\{B_1, B_2, \cdots, B_k\}$. Formally, we define the graph generator as
 % \vspace{-0.05in}
\begin{equation}
f(\{\alpha_i\},\{B_i\}) = \sum _{i=1}^k \alpha_i B_i, \ \ \text{where} \  \sum _{i=1}^k \alpha_i = 1, \alpha_i \geq 0.
% , 1 \leq i \leq k,
 \vspace{-0.05in}
\end{equation}
Different choices of $\{\alpha_i\},\{B_i\}$ comprise different graph generators.
All possible generators constitute the \emph{generator space}
$\Omega=\{ f(\{\alpha_i\},\{B_i\}) \mid \forall \ \{\alpha_i\},\{B_i\} \}$.

We shall also note that, the graph generator $f(\{\alpha_i\},\{B_i\})$ is indeed a mixed graphon, (\emph{i.e.}, mixture of $k$ graphons $\{B_1, B_2, \cdots, B_k\}$), where
{each element $B_i$ represents a graphon estimated from a set of similar pre-training (sub)graphs.
Furthermore, it can be theoretically justified that the mixed version still preserve the properties of graphons (c.f. Theorem~\ref{theor-preserve}) and the key transferable patterns    inherited in $B_i$ (c.f. Theorem~\ref{theor:diff}).}
Thus the graph generator $f(\{\alpha_i\},\{B_i\})$, \emph{i.e.}, mixed graphon, serves as a representative and comprehensive summary of pre-training data,  from which unseen graphs with different combinations of transferable patterns can be induced.

\vpara{Possible downstream space.}
All the graphs produced by the generators in the generator space $\Omega$ could benefit from the pre-training, and finally form the possible downstream space.

Formally, for each generator in the generator space $\Omega$
(we denote it as $f$ for simplicity), we can generate a $n$-node graph as follows.
First, we independently sample a random latent variable for each node. Then for each pair of nodes, we assign an edge between them with the probability equal to the value of the graphon at their randomly sampled points.
This process can be formulated as:
\begin{equation}
\begin{aligned}
& v_1, \cdots, v_n \sim \textrm{Uniform}([0,1]), \\
& A_{ij} \sim \textrm{Bernouli}(
f(v_i,v_j))
, \quad \forall i,j \in \{1,2,...,n\},
\end{aligned}
\end{equation}
where $f(v_i,v_j) \in [0,1]$ indicates the corresponding value of the graphon  at point $(v_i ,v_j)$~\footnote{For simplicity, we slightly abuse the notations $f(\cdot,\cdot)$. Note that $f(\{\alpha_i\},\{B_i\})$ is a function of $\{\alpha_i\}$ and $\{B_i\}$, representing that the generator depends on $\{\alpha_i\}, \{B_i\}$; while for each generator (\emph{i.e.}, mixed graphon) $f$ given $\{\alpha_i\}, \{B_i\}$, it can be represented as a continuous, bounded and symmetric function $f: [0,1]^2 \rightarrow [0,1]$.}, and $A_{ij} \in \{0,1\}$ indicates the existence of edge between $i$-th node and $j$-th node.
The adjacency matrix of the sampled graph $G$ is denoted as $A = [A_{ij}] \in \{0,1\}^{n \times n}, \forall i,j \in[n]$.
We summarize this generation process as $G \shortleftarrow f$.


Therefore, with all generators from the generator space $\Omega$, 
the possible downstream space is defined as $\mathcal{D} = \{G \shortleftarrow f |f \in \Omega\}$.
Note that for each ${\{\alpha_i\},\{B_i\}}$,
we have a generator $f$; and for each generator, we also have different generated graphs.
Besides, we theoretically justify that the generated graphs in the possible downstream space can inherit key transferable graph patterns in our generator  (c.f. Theorem~\ref{thm:down}).


\vpara{Application cases.}
The proposed framework is flexible to be adopted in different application scenarios when discussing {the problem of} when to pre-train GNNs.


\begin{itemize}[leftmargin=*,topsep=0pt]
    \item \emph{Use case 1: provide a user guide of a graph pre-trained model.} The possible downstream space $\mathcal{D}$ serves as a user guide of a graph pre-trained model, telling the application scope  of graph pre-trained models (\emph{i.e.}, the possible downstream graphs that can benefit from the pre-training data).
    

 \item \emph{Use case 2:  estimate the feasibility of performing pre-training from pre-training data to downstream data.}
Given a collection of pre-training graphs and a downstream graph, one can directly measure the feasibility of performing pre-training on pre-training data, before conducting costly pre-training and fine-tuning attempts. By making such pre-judgement of a kind of transferability, some unnecessary and expensive parameter optimization steps during model training and evaluation can be avoided.

 \item \emph{Use case 3: select pre-training data to benefit the downstream.}
In some practical scenarios {where} the downstream data is provided (\emph{e.g.}, a company needs to boost downstream performance of its business data), the feasibility of pre-training inferred by W2PGNN can be used to select data for pre-training to maximize the downstream performance with limited resources.
\end{itemize}

Use case 1 can be directly given by our produced possible downstream space $\mathcal{D}$.
However, how to measure the feasibility of pre-training in use case 2 and 3 still remains a key challenge.
In the following sections, we introduce the formal definition of the feasibility of pre-training and its approximate solution.


\subsection{Feasibility of Pre-training}
~\label{subsec:fea}
If a downstream graph can be generated with a higher probability from any generator in the generator space $\Omega$, then the graph could benefit more from the pre-training data. 
We therefore define the feasibility of performing pre-training as the highest probability of the downstream data generated from a generator in $\Omega$, which can be formulated as an optimization problem as follows.

\begin{definition}[Feasibility of graph pre-training]\label{def:fea}
Given the pre-training data $\mathcal G_\text{train}$ and downstream data $\mathcal G_\text{down}$, we have the feasibility of performing pre-training on $\mathcal G_\text{train}$ to benefit $\mathcal G_\text{down}$ as
\begin{equation} \label{eq:fea}
    \zeta(\mathcal G_\text{train} 
    \shortrightarrow \mathcal G_\text{down}) = \underset{\{\alpha_i\},\{B_i\}}{\sup } \operatorname{Pr}\left(\mathcal G_\text{down} \mid f(\{\alpha_i\},\{B_i\})\right),
\end{equation}
where $\operatorname{Pr}\left(\mathcal G_\text{down} \mid f(\{\alpha_i\},\{B_i\}) \right)$ denotes the probability of 
the graph sequence sampled from $\mathcal G_\text{down}$ being generated by graph generator $f(\{\alpha_i\},\{B_i\})$; each (sub)graph represents an ego-network  (for node-level task) or a graph (for graph-level task) sampled from the downstream data $\mathcal{G}_\text{down}$.
\end{definition}

However, the probability $\operatorname{Pr}\left(\mathcal G_\text{down} \mid f(\{\alpha_i\},\{B_i\})\right)$ of generating the downstream graph from a generator is extremely hard to compute, we therefore {turn to converting} the optimization problem~\eqref{eq:fea} to a tractable problem.
Intuitively, if generator $f(\{\alpha_i\},\{B_i\})$ can generate the downstream data with higher probability, it potentially means that the underlying generative patterns of pre-training data (characterized by $f(\{\alpha_i\},\{B_i\})$) and downstream data (characterized by the graphon $B_\text{down}$ fitted from $\mathcal{G}_\text{down}$) are more similar.
Accordingly, we turn to figure out the infimum of the distance between $f(\{\alpha_i\},\{B_i\})$ and $B_\text{down}$ as the feasibility, \emph{i.e.},
\begin{equation}~\label{eq:dist}
  \zeta(\mathcal G_\text{train} 
    \shortrightarrow \mathcal G_\text{down}) =  - \underset{\{\alpha_i\},\{B_i\}}{\inf }  \operatorname{dist}(f(\{\alpha_i\},\{B_i\}), B_\text{down}).
\end{equation}
Following~\cite{Xu2021LearningGA}, we hire the 2-order Gromov-Wasserstein (GW) distance
as our distance function $\operatorname{dist}(\cdot,\cdot)$, as
GW distance is commonly used to measure the difference between structured data.

Additionally, we establish a theoretical connection between the above-mentioned distance and the probability of generating the downstream data in extreme case, which further adds to the integrity and rationality of our solution. Detailed proof of the following theorem can be found in Appendix~\ref{app:proof}.
\begin{theorem}\label{theor:dis-prob}
    Given the graph sequence sampled from downstream data $\mathcal{G}_\text{down}$, we estimate its corresponding graphon as $B_\text{down}$. If a generator $f$ can generate the downstream graph sequence with probability 1, then $\operatorname{dist}(f, B_\text{down})=0$.
\end{theorem}
% }

\subsection{Choose Graphon Basis to Approximate Feasibility}
\label{subsec:3.3}

Although the feasibility has been converted to the optimization problem~\eqref{eq:dist},
exhausting all possible $\{\alpha_i\},\{B_i\}$ to find the infimum is impractical. 
An intuitive idea is that we can choose some appropriate graphon {basis} $\{B_i\}$, which can not only prune the search space but also accelerate the optimization process.
Therefore, we aim to first reduce the search space of graphon basis $\{B_i\}$ and then learn the optimal $\{\alpha_i\}$ in the reduced search space.

Considering that the downstream data may be formed via {different generation mechanisms (implying various transferable patterns)}, a single graphon basis might have limited expressivity and completeness to cover all patterns. 
We therefore argue that a good reduced search space of graphon basis should cover a set of graphon bases. Here, we introduce three candidates of them as follows.


\vpara{Integrated graphon basis.}
The first candidate of graphon basis is the integrated graphon basis $\{B_i\}_\text{integr}$. This graphon basis is introduced based on the assumption that the pre-training and the downstream graphs share very similar patterns. For example, the pre-training and the downstream graphs might come from social networks of different time spans~\cite{Hu2020GPTGNNGP}. In the situation, almost all patterns involved in the pre-training data might be useful for the downstream.
To achieve this, we directly utilize all (sub)graphs sampled from the pre-training data to estimate one graphon as the graphon basis. This integrated graphon basis serves as a special case of the graphon basis introduced below.





\vpara{Domain graphon basis.}
The second candidate is the domain graphon basis $\{B_i\}_\text{domain}$. The domain information that pre-training data comes from is important prior knowledge to indicate the transferability from the pre-training to downstream data.
For example, when the downstream data is molecular network, it is more likely to benefit from the pre-training data from specific domains like biochemistry. This is because the specificity of molecules makes it difficult to learn transferable patterns from other domains, \emph{e.g.},
 closed triangle structure represents diametrically opposite meanings (stable vs unstable) in social network and molecular network.
Therefore, we propose to split the (sub)graphs sampled from  pre-training data according to their domains, and each split of (sub)graphs will be used to estimate a graphon as a basis element. In this way, each basis element reflects transferable patterns from a specific domain, and all basis elements construct the domain graphon basis $\{B_i\}_\text{domain}$.







\vpara{Topological graphon basis.}
The third candidate is the topological graphon basis  $\{B_i\}_\text{topo}$. The topological similarity between the pre-training and the downstream data serves as a crucial indicator of transferability. For example, a downstream social network might benefit from the similar topological patterns in academic or web networks (\emph{e.g.}, closed triangle structure indicates stable relationship in all these networks). 
Then, the problem of finding topological graphon basis can be converted to
partition $n$ (sub)graphs sampled from pre-training data into $k$-split according to their topology similarity, where each split contains (sub)graphs with similar topology. Each element of graphon basis (\emph{i.e.}, graphon) fitted from each split of (sub)graphs is expected to characterize a specific kind of topological transferable pattern.

However, the challenge {is} that for graph structured data that is irregular and complex, we cannot directly measure the topological similarity between graphs.
To tackle this problem, we introduce a \emph{graph feature extractor} that maps arbitrary graph into a fixed-length vector representation.
To approach a comprehensive and representative set of topological features,
we here consider both node-level and graph-level properties.



For node-level topological features, we first apply a set of node-level property functions $[\phi_1(v),\cdots,\phi_{m_1}(v)]$ for each node $v$ in graph $G$ to capture the local topological features around it. 
Considering that the numbers of nodes of two graphs are possibly different, we introduce an aggregation function $\operatorname{AGG}$ to summarize the node-level property of all nodes over $G$ to a real number $\operatorname{AGG}(\{{\phi}_i(v), v \in G\})$.
We can thus obtain the node-level topological vector representation as follows.
$$
h_\text{node} (G) =  [\operatorname{AGG}(\{{\phi}_1(v), v \in G\}), \cdots, \operatorname{AGG}(\{{\phi}_{m_1}(v), v \in G\})].
$$
In practice, we calculate degree~\cite{borner2007network}, clustering coefficient~\cite{Kaiser2008MeanCC} and closeness centrality~\cite{freeman2002centrality} for each node and instantiate the aggregation function $\operatorname{AGG}$ as the mean aggregator.


For graph-level topological features, we also employ a set of graph-level property functions for each graph $G$ to serve as the vector representation
$$
h_\text{graph}(G) = [\psi_1(G),\cdots,\psi_{m_2}(G)],
$$
where   density~\cite{wasserman1994social}, assortativity~\cite{newman2003mixing}, transitivity~\cite{wasserman1994social} are adopted as graph-level properties here.
~\footnote{Other graph-level properties can also be utilized like  \emph{diameter and Wiener index}, but we do not include them due to their high computational complexity.}.

Finally, the final representation of $G$ produced by the graph feature extractor is
$$
h=[h_\text{local}(G)||h_\text{global}(G)] \in \mathbb{R}^{m_1+m_2},
$$
where $||$  is the concatenation function that combines both node-level and graph-level features.
Given the topological vector representation, we leverage an efficient clustering algorithm  K-Means~\cite{macqueen1967classification} to obtain k-splits of (sub)graphs and finally fit each split into a graphon as one element of topological graphon basis.



\vpara{Optimization solution.}
Given the above-mentioned three graphon bases, the choice of graphon basis $\{B_i\}$ can be specified to one of them.
In this way, the pre-training feasibility (simplified as $\zeta$) could be approximated in the reduced search space of graphon basis as
\begin{equation}~\label{eq:dist3}
\begin{aligned}
\zeta \leftarrow -  \operatorname{MIN}  ( \{ \underset{\{\alpha_i\}}{\inf }  \operatorname{dist}(f(\{\alpha_i\},\{B_i\}), B_\text{down}),  \forall \{B_i\} \in \mathcal{B} \}),
\end{aligned}
\end{equation}
where $\mathcal{B}$$=$$\{\{B_i\}_\text{topo}, \{B_i\}_\text{domain}, \{B_i\}_\text{integr}\} $ is the reduced search space of $\{B_i\}$.
Thus, the problem can be naturally splitted into three sub-problems with objective of  $\operatorname{dist}(f(\{\alpha_i\},\{B_i\}_\text{topo}), B_\text{down}))$, $\operatorname{dist}\\(f(\{\alpha_i\}, \{B_i\}_\text{domain}), B_\text{down}))$ and $\operatorname{dist}(f(\{\alpha_i\},\{B_i\}_\text{integr}), B_\text{down}))$ respectively.
Each sub-problem can be solved by updating the corresponding learnable parameters $\{\alpha_i\}$ with multiple gradient descent steps. Taking one step as an example, we have
\begin{equation} \label{eq.2}
\begin{aligned}
    \{\alpha_i\} = \{\alpha_i\}-\eta \nabla_{\{\alpha_i\}} \operatorname{dist}(f(\{\alpha_i\},\{B_i\}), B_\text{down})
\end{aligned}
\end{equation}
where $\eta$ is the learning rate. 
Finally, we achieve three infimum distances under different $ \{B_i\} \in \mathcal B$ respectively, the minimum value among them is the approximation of pre-training feasibility.
In practice, we adopt an efficient and differential approximation of GW distance, \emph{i.e.}, entropic regularization GW distance~\cite{peyre2016gromov}, as the distance function.
{For graphon estimation, we use the ``largest gap'' method as to estimate graphon $B_i$.}


\subsection{Computation Complexity}~\label{subsec:cost}
We now show that the time complexity of W2PNN is much lower than traditional  solution.
Suppose that we have $n_1$  and $n_2$ (sub)graphs sampled from pre-training data and  downstream data respectively, and denote $|V|$ and $|E|$ as the average number of nodes and edges per (sub)graph.
The overall time complexity of W2PGNN is $O((n_1+n_2)|V|^2)$.
For comparison, traditional solution in Figure~\ref{fig:example}(a) to estimate the pre-training feasibility should make $l_1 \times l_2$ ``pre-train and fine-tune'' attempts, if there exist $l_1$ pre-training models and $l_2$ fine-tuning strategies. Suppose the batch size of pre-training as $b$ and the representation dimension as $d$. 
The overall time complexity of traditional solution is 
 $O\left(l_1l_2((n_1+n_2)(|V|^3 + |E|d)+n_1bd)\right)$.
Detailed analysis can be found in Appendix~\ref{app:complexity}.


