\section{Problem Formulation}\label{sec:problem}

In this section, we first formally define the problem of when to pre-train GNNs. Then, we provide a brief theoretical analysis of the transferable patterns in the problem we study, and finally discuss some non-transferable patterns.


\begin{definition}[When to pre-train GNNs]
    Given the pretraining graph data $\mathcal G_\text{train}$ and the downstream graph data $\mathcal G_\text{down}$, our main goal is to answer to what extent the ``pre-train and fine-tune'' paradigm can benefit the downstream data.
\end{definition}
Note that in addition to this main problem, our proposed framework can also serve other scenarios, such as providing the application scope of graph pre-trained models, and helping select pre-training data to benefit the downstream (please refer to the \emph{application cases} in Section~\ref{subsec:overview} for details).

\vpara{Transferable graph patterns.}
The success of ``pre-train and fine-tune'' paradigm is typically attributed to the commonplace between pre-training and downstream data. However, in real-world scenarios, there possibly exists a significant divergence between the pre-training data and the downstream data. To answer the problem of when to pre-train GNNs, the primary task is to define the transferable patterns across graphs.

We here theoretically explore which patterns are transferable between pre-training and downstream data under the performance guarantee of graph pre-training model (with GNN as the backbone).
\begin{theorem}[Transferability of graph pre-training model]\label{thero:emb}
Let $G_\text{train}$ and $G_\text{down}$ be two (sub)graphs sampled from $\mathcal G_\text{train}$ and $\mathcal{G}_\text{down}$, and assume the  attribute of each node as a scalar $1$ without loss of generality. Given a graph pre-training model $e$ (instantiated as a GNN)  with $K$ layers and $1-$hop graph filter $ \Phi(L)$ (which is a function of the normalized graph Laplacian matrix $L$), we have 

\begin{equation}
\left\|e(G_\text{train})-e(G_\text{down})\right\|_2
\leq \kappa
\Delta_\text{topo}\left(G_\text{train}, G_\text{down}\right)
\end{equation}
where {$\Delta_\text{topo}\left(G_\text{train}, G_\text{down}\right) =  \frac{1}{mn}  \sum_{i=1}^m \sum_{j^{\prime}=1}^n \|
L_{g_i}-L_{g_j^{\prime}} \|_2$} measures the topological divergence between $G_\text{train}$ and $G_\text{down}$, where
$g_i$ is the $K$-hop ego-network of node $i$ from $G_\text{train}$ and $L_{g_i}$ is its corresponding normalized graph Laplacian matrix, $m$ and $n$ are the number of nodes of $G_\text{train}$ and $G_\text{down}$. $e(G_\text{train})$ and $e(G_\text{down})$ are the output representations of $G_\text{train}$ and $G_\text{down}$ from graph pre-training model,
$\kappa$ is a constant relevant to  $K$, graph filter $\Phi$, learnable parameters of GNN and the activation function used in GNN.
\end{theorem}

Detailed proofs and descriptions can be found in Appendix~\ref{proof:emb}.
Theorem~\ref{thero:emb} suggests that two (sub)graphs sampled from pre-training and downstream data with similar topology are transferable via graph pre-training model (\emph{i.e.}, sharing similar representations produced by the model).


{Hence we consider the transferable graph pattern as the topology of a (sub)graph, either node-level or graph-level. Specifically, the node-level transferable pattern could be the topology of the ego-network of a node (or the structural role of a node), irrespective of the node's exact location in the graph. The graph-level transferable pattern is the topology of the entire graph itself (\emph{e.g.}, molecular network). Such transferable patterns constitute the input space introduced in Section~\ref{subsec:overview}. }

\vpara{Discussion of non-transferable graph patterns.}
As a remark, we show that two important pieces of information (\emph{i.e.}, attributes and proximity) commonly used in graph learning are not necessarily transferable across pre-training and downstream data in most real-world scenarios, thus we do not discuss them in this paper.

First, although the attributes carry important semantic meaning in one graph, 
it can be shown that the attribute space of different graphs typically has little or no overlap at all. For example, if the pre-training and downstream data come from different domains, their nodes would indicate different types of entities and the corresponding attributes may be completely irrelevant. Even for graphs from the similar/same domain, the dimensions/meaning of their node attributes can also be totally different and result in misalignment. 

The proximity, on the other hand, 
assumes that closely connected nodes are similar,
which also cannot be transferred across graphs. 
Obviously, this proximity assumption depends on the overlaps in neighborhoods and thus only works on graphs with the same or overlapped node set.