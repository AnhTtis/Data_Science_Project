\section{Introduction}
Graph neural networks (GNNs) have undergone rapid development and become increasingly popular for learning graph data \cite{welling2016semi, velivckovic2017graph, xu2018powerful}.
GNNs are usually trained in an end-to-end manner while getting enough labeled data is arduously expensive and sometimes even impractical to access. This motivates some recent advances in pre-training GNNs ~\cite{hu2019strategies,Hu2020GPTGNNGP,Qiu2020GCCGC,Lu2021LearningTP}. 
The key insight of pre-training GNNs is to learn transferable knowledge from a collection of unlabeled graph data, hoping that the learned knowledge can be easily adapted to downstream tasks.
In view of the great success of pre-training in other fields like computer vision and natural language processing~\cite{devlin2018bert,he2020momentum},  graph pre-training is {highly expected} to be an effective means to improve downstream performance.


\begin{figure}[t]
    \centering
    {\includegraphics[width=1\columnwidth]{figure/motivation.pdf}}
    \caption{Comparison of {existing methods} and {proposed W2PGNN} to answer \emph{when to pre-train} GNNs.}    
    \label{fig:example}
\end{figure}

However, the intuition that graph pre-trained model would ideally benefit the downstream is far from the truth in the area of graph pre-training.
Instead, graph pre-trained models can lead to \emph{negative transfer} on many downstream tasks, especially when the graphs used for pre-training are not necessarily from the same domain as the {downstream} data~\cite{hu2019strategies, Qiu2020GCCGC}.
For example, the closed triangles ($\vcenter{\hbox{\includegraphics[width=2.4ex,height=2.4ex]{figure/s2.pdf}}}$) and open triangles  ($\vcenter{\hbox{\includegraphics[width=2.4ex,height=2.4ex]{figure/s1.pdf}}}$) might yield different interpretations in molecular networks (unstable vs. stable in terms of chemical property) from those in social networks (stable vs. unstable in terms of social relationship); such distinct or reversed semantics does not contribute to transferability, and even exacerbates the problem of negative transfer.


To avoid the negative transfer, recent efforts focus on  \emph{what to pre-train} and \emph{how to pre-train},  \emph{i.e.}, design/adopt graph pre-training models with a variety of self-supervised tasks to capture different patterns~\cite{Qiu2020GCCGC,you2020graph,Lu2021LearningTP} and fine-tuning strategies to enhance downstream performance~\cite{Hu2019PreTrainingGN,Han2021AdaptiveTL,Zhang2022FineTuningGN,Xia2022TowardsEA}.
However, there do exist some cases that no matter how advanced the pre-training/fine-tuning method is, the transferability from pre-training data to downstream data still cannot be guaranteed. This is because the underlying assumption of deep learning models is that the test data should share a similar distribution as the training data.
Therefore, it is a necessity to understand \emph{when to pre-train}, \emph{i.e.}, under what situations the ``graph pre-train and fine-tune'' paradigm should be adopted.

Towards the answer of when to pre-train GNNs, one straight-forward way illustrated in Figure~\ref{fig:example}(a) is to train and evaluate on all candidates of pre-training models and fine-tuning strategies, and then the resulting best downstream performance would tell us whether pre-training
% ``pre-train and fine-tune'' 
is a sensible choice. If there exist $l_1$ pre-training models and $l_2$ fine-tuning strategies,  such a process would be very costly as you should make $l_1 \times l_2$ ``pre-train and fine-tune'' attempts.
Another approach is to utilize graph metrics to measure the similarity between pre-training and downstream data, \emph{e.g.}, density, clustering coefficient and etc. However, it is a daunting task to enumerate all hand-engineered graph features or find the dominant features that influenced similarity.
Moreover, the graph metrics only measure the pair-wise similarity between two graphs, which cannot be directly and accurately applied to the practical scenario where pre-training data contains multiple graphs.


In this paper, we propose a W2PGNN framework to answer
\emph{\underline{w}hen \underline{to} \underline{p}re-train \underline{GNN}s from a graph data generation perspective}.
% aim to address the problem of when to pre-train GNNs 
The high-level idea is that instead of performing effortful graph pre-training/fine-tuning or making comparisons between the pre-training and downstream data, we study the complex generative mechanism from the pre-training data to the downstream data (Figure~\ref{fig:example}(b)).
We say that downstream data can benefit from pre-training data (\emph{i.e.}, has high feasibility of performing pre-training), 
if it can be generated with high probability by a graph generator that summarizes the topological characteristic of pre-training data.



The major challenge is how to obtain an appropriate graph generator, hoping that it not only inherits the transferable topological patterns of the pre-training data, but also is endowed with the ability to generate feasible downstream graphs.
To tackle the challenge, we propose to design a graph generator based on graphons.
We first fit the pre-training graphs into different graphons to construct a \emph{graphon basis}, where each graphon (\emph{i.e.}, element of the graphon basis) identifies a collection of graphs that share common transferable patterns. We then define a \emph{graph generator} as {a convex combination of elements in a graphon basis}, which serves as a comprehensive and representative summary of pre-training data.  All of these possible generators constitute the \emph{generator space}, from which graphs generated form the solution space for the downstream data that can benefit from pre-training.

Accordingly, the feasibility of performing pre-training can be measured as the highest probability of downstream data being generated from any graph generator in the generator space, which can be formulated as an optimization problem.
However, this problem is still difficult to solve due to the large search space of graphon basis. We propose to reduce the search space to three candidates of graphon basis, \emph{i.e.,} topological graphon basis, domain graphon basis, and integrated graphon basis, to mimic different {generation mechanisms} from pre-training to downstream data. Built upon the reduced search space, the feasibility can be approximated efficiently.





Our major contributions are concluded as follows:
\begin{itemize}[leftmargin=*,topsep=0pt]
\item \textbf{Problem and method.} To the best of our knowledge, we are the first work to study the problem of when to pre-train GNNs. We propose a W2PGNN framework
to answer the question from a data generation perspective, which tells us the feasibility of performing graph pre-training before conducting effortful pre-training and fine-tuning.




\item \textbf{Broad applications.}
W2PGNN provides several practical applications: (1) provide the application scope of a graph pre-trained model, (2) measure the feasibility of performing pre-training for a downstream data
and (3) choose the pre-training data so as to maximize downstream performance with limited resources.


\item \textbf{Theory and Experiment.} 
We theoretically and empirically justify the effectiveness of W2PGNN.
Extensive experiments {on real-world graph datasets from multiple domains} show that the proposed method can provide an accurate estimation of pre-training feasibility and the selected pre-training data can benefit the downstream performance.


\end{itemize}

