\section{Preliminary and Related Works}
\vpara{Graphons.}
A Graphon (short for graph function)~\cite{Airoldi2013StochasticBA} is a bounded symmetric function $B: [0,1]^{2} \rightarrow [0,1]$ (we use different subscripts of $B$ to denote different graphons), which can be interpreted as the weighted matrix of an arbitrary undirected graph with uncountable number of nodes\cite{lovasz2012large}. Literaturelly, graphon has been studied from two perspectives: as limit of graph sequence, and as graph generators\cite{lovasz2006limits,Airoldi2013StochasticBA, Han2022GMixupGD}. \emph{We utilize both perspectives in our framework.}

On one hand, a graphon can be considered as the limit objects of graph sequence, where the density of certain ``graph motifs'' can be preserved;
and every convergent graph sequence would converge to a graphon\cite{lovasz2012large}.
Thus, a graphon provides a comprehensive summary of an entire collection of arbitrary size graphs. These graphs can be considered topologically similar in the sense that they belong to the same graphon. \emph{In this paper, we utilize a set of graphons as a comprehensive and representative summary of pre-training data.}




Taking graphon 
as a graph generator, we can associate nodes $i$ and $j$ with points $v_i$ and $v_j$ in $[0,1]$, and then $B(v_i , v_j)$ serves as the probability to generate the edge between these two nodes. Therefore, a graphon $B$ can generate unweighted graphs of arbitrary sizes, which can be taken as those induced graphs  potentially inheriting the topological patterns implied in graphon. \emph{Thus, the generation capability of graphon can help us  generate feasible downstream graphs that can benefit from pre-training.}

\vpara{Graph pre-training and fine-tuning.}
Graph pre-training models first learn universal knowledge from large-scale graph datasets with self-supervised or unsupervised objectives (\emph{i.e.}, pre-training stage), and then transfer the knowledge to deal with specific downstream tasks (\emph{i.e.}, fine-tuning stage).
Among them, some researchers design some pre-training tasks based on the neighborhood similarity assumption~\cite{Perozzi2014DeepWalkOL,Kipf2016VariationalGA,Sun2020MultiStageSL,Grover2016node2vecSF,Donnat2018LearningSN,Zhang2019ProNEFA,Tang2015LINELI,Zhao2021DataAF}. The learned graph-specific patterns/knowledge could benefit downstream tasks on the same graphs, but cannot be generalized to unseen graphs.
To enhance the transferability of the pre-trained graph models, some works try to utilize graph data from the same (or similar) domains as the pre-training data~\cite{Hafidi2020GraphCLCS,hassani2020contrastive,Sun2020InfoGraphUA,Narayanan2017graph2vecLD,Zhu2020DeepGC,hu2019strategies,you2020graph,You2020WhenDS,Hu2020GPTGNNGP,Li2021PairwiseHD,Lu2021LearningTP,Sun2021MoCLDM,Zhang2021MotifbasedGS}, or explore cross-domain pre-training strategies~\cite{Qiu2020GCCGC,you2020graph,Lu2021LearningTP} as well as fine-tuning strategies~\cite{Hu2019PreTrainingGN,Han2021AdaptiveTL,Zhang2022FineTuningGN,Xia2022TowardsEA}.
Nevertheless, all these efforts focus on addressing the problem of  \emph{what to pre-train and how to pre-train} by developing pre-training or fine-tuning methods.
To the first time, we aim to study \emph{when to pre-train} GNNs, \emph{i.e.}, in what situations the graph pre-training should be leveraged.

\vpara{Transferability measure.}
There are several attempts to measure the transferability of GNNs. 
The most  straightforword way is to train and evaluate on 
all candidates of pre-training models and fine-tuning strategies,
and then the resulting best downstream performance as the transferability measure.
However, as depicted in Figure~\ref{fig:example}(a), such approach would be very costly to perform effortful pre-training and fine-tuning. 
Another way is based on graph properties, which leverage the graph  properties (\emph{e.g.,} degree~\cite{borner2007network}, density~\cite{wasserman1994social}, assortativity~\cite{newman2003mixing} and etc.) to measure the similarities between pre-training and downstream graphs, potentially can be utilized to approximate the transferability. 
Some other works also focus on analyzing the transferability of GNNs theoritically~\cite{levie2021transferability,Ruiz2021TransferabilityPO}. Nevertheless, they are limited to measure the transferability of GNNs on a single graph or when training and testing data are from the same dataset~\cite{levie2021transferability,Ruiz2021TransferabilityPO}  , which are inapplicable to our setting.
A recent work, EGI~\cite{Zhu2021TransferLO}  addresses the transferability measure problem of GNNs across graphs. However, EGI is a model-specific measure and depend on its own framework. 
For the first time, we study the transferability of graph pre-training from the data perspective, without performing any pre-training and fine-tuning.
