\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}

\usepackage[left=1.5in, right=1.5in, top=1.25in, bottom=1.25in]{geometry}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tcolorbox}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{bbm}
\usepackage{bm}
\newcommand{\R}{\mathbb{R}}

\usepackage{xurl}
\usepackage{authblk}

\usepackage{todonotes}

\usepackage[style=authoryear, maxcitenames=2, maxbibnames=99]{biblatex}
\addbibresource{refs.bib}

\usepackage{titlesec}
%
\titleformat*{\section}{\large\bfseries}
\titlespacing*{\section}{0pt}{16pt}{8pt}
%
\titleformat*{\subsection}{\bfseries}
\titleformat*{\subsubsection}{\bfseries}
%
\renewcommand*{\Affilfont}{\normalsize\itshape}
\newcommand{\mat}[1]{\mathbf{#1}}

\definecolor{midnightblue}{rgb}{0.1, 0.1, 0.44}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    linktoc = all,
    citecolor=midnightblue,
    filecolor=black,
    linkcolor=midnightblue,
    urlcolor=black
}

\usepackage{cleveref}
\usepackage[absolute,overlay]{textpos}

\newcommand{\greytext}[1]{\textcolor{gray}{#1}}
\title{%
\vspace{-0.5in}
%
%
%
%
%
%
%
%
%
%
%
%
%
The Quantization Model of Neural Scaling
}
%
%
\author[1, 2]{Eric J. Michaud\footnote{ericjm@mit.edu}}
\author[1, 2]{Ziming Liu}
\author[3]{Uzay Girit}
\author[1, 2, 4]{Max Tegmark}
\affil[1]{Department of Physics, MIT}
\affil[2]{NSF AI Institute for AI and Fundamental Interactions}
\affil[3]{Department of EECS, MIT}
\affil[4]{Center for Brains, Minds and Machines, MIT}
\date{\vspace{-0.3in}}

\begin{document}

%
%
%

\maketitle

\begin{abstract}

%

We propose the \emph{Quantization Model} of neural scaling laws, explaining both the observed power law dropoff of loss with model and data size, and also the sudden emergence of new capabilities with scale. 
We derive this model from what we call the \emph{Quantization Hypothesis}, 
where learned network capabilities are quantized into discrete chunks (\emph{quanta}).
We show that when quanta are learned in order of decreasing use frequency, then a power law in use frequencies explains observed power law scaling of loss. We validate this prediction on toy datasets, then study  how scaling curves decompose for large language models. Using language model internals, we auto-discover diverse model capabilities (quanta) and find tentative evidence that the distribution over corresponding subproblems in the prediction of natural text is compatible with the power law predicted from the neural scaling exponent as predicted from our theory.






%

%

%

%

\vspace{0.2in}
\end{abstract}

%

%
%
%
%

\section{Introduction}

%

%

In the aggregate, larger neural networks trained on more data perform better than smaller neural networks trained on less data, in a predictable way. Across a range of studies, mean test loss has been observed to decrease as a power law in both the number of network parameters ($L \propto N^{-\alpha_N}$) and the number of training samples ($L \propto D^{-\alpha_D}$) \parencite{hestness2017deep, rosenfeld2019constructive, kaplan2020scaling, henighan2020scaling, gordon2021data, zhai2022scaling, hoffmann2022training}. Although aggregate performance changes predictably with scale, when particular capabilities are examined, larger models often have emergent abilities, i.e., unexpected and qualitatively different behavior than smaller models~\parencite{wei2022emergent}. Understanding both facets of scaling -- the predictable power law decrease in loss and the emergence of new capabilities at scale -- is not just of theoretical interest, but highly relevant to the near-term future of deep learning~\parencite{ganguli2022predictability}. Understanding the precise way in which larger models are different from smaller ones is entangled with basic questions about \emph{what} deep neural networks are doing internally and \emph{whether} they will continue to improve with scale. 

Recent studies of the internal workings of neural networks have found a variety of impressive algorithms learned by gradient descent~\parencite{olah2020zoom, olsson2022context, nanda2023progress}. As more work is put into understanding the internal computations performed by neural networks (the task of so-called \textit{mechanistic interpretability}), we may find more and more 
%
``circuits''~\parencite{elhage2021mathematical} in models -- intelligible computations for accomplishing prediction in specific contexts. A natural question is whether such circuits are learned universally across models with different random initializations and across scales. \textcite{olsson2022context} find evidence for universality of ``induction heads'', a type of circuit that may underlie in-context learning. In this paper, we will put forth the \emph{Quantization Hypothesis}, a set of conjectures about the universality of computations performed across model scales and about how properties of the data distribution produce power law neural scaling. 

In particular, we hypothesize that to many prediction problems, there corresponds a \emph{universal} and \emph{discrete} set of computations which are instrumental for reducing loss, and that model performance is determined by \emph{which} of these computations are successfully learned. 
%
We call these basic building blocks of model performance the \textbf{quanta}.
%
We then argue that an intrinsic power law distribution in how frequently the quanta are useful for prediction leads to a power law marginal improvement in loss from learning additional quanta. If the effect of scaling is to simply learn \emph{more} quanta, then this leads to power law neural scaling. Under the Quantization Hypothesis, neural scaling exponents are determined by the exponent in a power law distribution over subtasks in data. We describe this \emph{Quantization Model} of neural scaling power laws in detail in \Cref{sec:THEORY}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/llm-spectral-cluster-examples.pdf}
    \caption{From network internals, we auto-discover \emph{quanta} -- discrete units of model capability -- for the task of language modeling. Samples from the left cluster all involve continuing a numerical sequence. Samples from the right cluster involve predicting a newline in order to maintain text width. We indicate the token which was predicted from the context before it with a red highlight.
    %
    We indicate the prediction of a newline with a highlighted \texttt{\textbackslash n} character. See~\Cref{sec:llm-quanta-discovery} for explanation.}
    \label{fig:cluster-examples}
\end{figure}

%
%
%
%
%
%

As a proof of concept, in \Cref{sec:SPARSEPARITY} we construct toy datasets consisting of many subtasks, and find that power law neural scaling emerges according to the Quantization Model. We then investigate scaling in large language models in \Cref{sec:llms}. We first analyze how power law scaling in mean loss decomposes into scaling on individual tokens, and find diverse behavior. Since it is unclear how to break up natural language prediction into quanta a priori, we develop a method which we call QDG (for ``quanta discovery with gradients'') based on spectral clustering with model gradients, to sort language model inputs into clusters. With this method, we auto-discover diverse and abstract language model behaviors, some of which we show in~\Cref{fig:cluster-examples}. We find that the distribution over these auto-discovered clusters in natural text roughly follows the power law our theory would expect given the empirical scaling exponent, though bias inherent in our clustering algorithm makes our measurement fairly uncertain.

If correct, the Quantization Hypothesis could have many implications for understanding neural networks. If model performance can be understood in terms of the presence or absence of a discrete set of computations in models, then we may be able to mechanistically understand large neural networks by enumerating the quanta they learn. Furthermore, we may be able to predict when certain capabilities will emerge at scale by estimating the frequency at which the relevant quanta for that capability are useful for prediction in the natural distribution of data.

%

%



%

%




%

%
%
%

%
%
%

%

%


%

%


%
%

%


%

%

%


%

%



%






%

%
%
%

%

%
    
%

%

%
    
%
   
    
%



%
%
%

\section{Theory}\label{sec:THEORY}


%

%
%

%

%

%
    
%

Consider the task of modeling the distribution of text on the internet. Successful prediction requires an immense amount of knowledge, and the ability to perform diverse computations, due to the immense complexity and diversity of the world and therefore of human language. For instance, in order to predict what word will come next in a conversation between two physicists, one must ``know'' much about physics. In order to continue the text \mbox{``\texttt{2534 + 7261 = }''}, one must be able to perform arithmetic (for large enough numbers, memorization becomes a highly inefficient strategy). A great many distinct types of computations are present in the world in the processes that \emph{produce} text, and so \emph{predicting} text requires those computations to be present in our models.

In this paper, we conjecture the Quantization Hypothesis:
\begin{center}
\begin{tcolorbox}[colframe=black, boxrule=1pt, colback=white, width=0.85\textwidth]
\begin{enumerate}
    \item[QH1] Many natural prediction problems involve a discrete set of computations which are natural to learn and instrumental for reducing loss. We call these ``quanta''. Model performance is determined by \emph{which} quanta have been learned.

    \item[QH2] Some abilities are more useful for reducing loss than others, leading to a natural ordering of the quanta. We call the ordered quanta the \textbf{Q Sequence}. Optimally trained networks should therefore learn the quanta in that order. The effect of scaling is to learn \emph{more} of the quanta in the Q Sequence, so scaling performance is simply determined by \emph{how many} quanta are successfully learned. 
    
    \item[QH3] The frequencies at which the quanta are used for prediction drop off as a power law.

\end{enumerate}
\end{tcolorbox}
\end{center}

We will show that together these result in power law neural scaling. The power law governing the frequency that the quanta are used from QH3 will determine the exponent of neural scaling laws.
%
Note that we use the word ``quanta'' to refer interchangeably to both model behavior (indivisible units of model capability) and to the corresponding computations implemented by the model which enable that behavior.

\def\q{{\bf q}}

We model the Quantization Hypothesis as follows.
Let $\q$ denote a bit string whose $k^{\rm th}$ bit $q_k=1$ if the $k^{\rm th}$ quantum in the Q Sequence has been learned, and $q_k=0$ otherwise.
QH1 implies that the mean loss $L$ is simply a function of $\q$.
QH2 implies that when $n\equiv\sum_k q_k$ quanta have been learned, we have $q_k=1$ for $k\le n$. Let $L_n$ denote the mean loss in this case.

From QH3, we have that the $k^{\rm th}$ quantum benefits prediction on a randomly chosen sample with probability 
\begin{equation}
p_k = \frac{1}{\zeta(\alpha+1)} k^{-(\alpha+1)}\propto k^{-(\alpha+1)}
\end{equation}
for a Zipf power law $\alpha>0$,
where $\zeta(s)\equiv\sum_{k=1}^\infty k^{-s}$.
Let us also assume that learning the $k^{\rm th}$ quantum reduces average loss from $b_k$ before it is learned to $a_k$ after it is learned on the samples where it is utilized.

If $a_k$ and $b_k$ are $k$-independent ($a_k=a$, $b_k=b$), then a model that has learned the first $n$ quanta will have expected loss
\begin{eqnarray}
    L_n &=&\sum_{k=1}^n a p_k + \sum_{k=n+1}^\infty b p_k
        =\sum_{k=1}^\infty a p_k + \sum_{k=n+1}^\infty (b-a) p_k\nonumber\\
        &\approx&a + \frac{b-a}{\zeta(\alpha+1)}\int_n^\infty k^{-(\alpha+1)} dk =
    a + \frac{b-a}{\alpha\zeta(\alpha+1)}n^{-\alpha}.
\end{eqnarray}
In other words, $L_\infty=a$ and $(L_n-L_\infty)\propto n^{-\alpha}$ is a power law.

In Appendix A, we provide analogous derivations for other assumptions for $a_k$ and $b_k$, including the case where $b_k\propto -\log\ p_k$, the entropy for a baseline model whose predictions use no other aspects of the data besides token frequencies (assuming that quanta involve the prediction of a particular token).
Interestingly, we find that the power law prediction is quite robust, in the sense that the broad range of assumptions we explore all produce curves $(L_n-L_\infty)$ that are exact or approximate power laws --- the latter include a small logarithmic correction.


An implicit assumption above is that all quanta are what we will refer to as 
{\it monogenic}, meaning that token predictions rely on at most a single quantum, 
akin to how monogenic traits in biology (e.g.\ cystic fibrosis) depend on a single gene.
Many real-world prediction problems are likely to involve both monogenic and polygenic quanta, a topic we explore in~\Cref{sec:llm-scaling-taxonomy}.
%

When all learned quanta are monogenic, the expected loss (which involves an average over all predicted tokens) transforms into an average over quanta, by simply grouping together all tokens predicted using each quantum, and summing their token probabilities to obtain the quantum use probabilities $p_k$ discussed above. A rigorous generalization of our formalism to the polygenic case is an interesting challenge for future work. However, it should be noted that polygenic quanta that reduce the loss of certain tokens by a fixed number of bits regardless of what other tokens have been learned will still produce power law scaling. For example, If one quantum predicts that the next word is an adjective and another quantum predicts that the next word relates to sports, they may each reduce the entropy by a fixed number of bits regardless of the order in which they are learned.


%


%
For the following derivations, we use $a = 0$ and $b = 1$ resulting in the simple formula $L_n \approx \frac{1}{\alpha \zeta(\alpha+1)} n^{-\alpha}$. Scaling in model parameters ($N$), training samples ($D$), and training time ($S$) can translate into scaling in $n$ and therefore loss $L$ as follows:

\bigbreak
\noindent\textbf{Parameter scaling}: In networks of finite size, only finitely many quanta can be learned -- network capacity is a bottleneck. If we assume that all quanta require the same capacity of $C$ network parameters, and we have a network with $N$ total parameters, roughly $n = N / C$ elements in the Q Sequence can be learned. We therefore expect loss to depend on the number of model parameters $N$ like so:
\begin{equation}
    L(N) = L_{N/C} \approx \frac{1}{\alpha\zeta(\alpha+1)}\left(\frac{N}{C}\right)^{-\alpha} \propto N^{-\alpha}. 
\end{equation} 
Given a power law distribution over quanta with exponent $\alpha+1$, we get power law neural scaling in parameters with exponent $\alpha_N = \alpha$. Note that we have also assumed that all quanta require the same model capacity. This is surely an unrealistic assumption (some computations, enabling certain model capabilities, probably require more capacity than others to implement), although if the average capacity consumed per quanta is small enough, fluctuations away from the mean capacity will be averaged out and the number of quanta learned $n$ will still be roughly proportional to model size $N$.

\bigbreak
\noindent\textbf{Data scaling (multi-epoch)}: For data scaling, we assume that a threshold of $\tau$ examples utilizing quantum $k$ are needed in the training set in order for quantum $k$ to be learned. $\tau$ can perhaps be thought of as the minimum number of examples on average requiring quantum $k$ needed to uniquely specify its computation. Assuming network capacity is not a bottleneck, how many quanta will be learned? If we have a training set of $D$ samples, then it will contain roughly $Dp_1$ samples utilizing quantum $1$, $Dp_2$ samples utilizing quantum $2$, and so on. If $p_k = \frac{1}{\zeta(\alpha+1)} k^{-(\alpha+1)}$, the last quantum $n$ learned in the Q Sequence will then roughly be $n$ such that $D\frac{1}{\zeta(\alpha+1)} n^{-(\alpha+1)} = \tau$ and so $n = (D / {\tau \zeta(\alpha+1)})^{1/(1+\alpha)}$. Under this model of how the training set size $D$ influences which quanta are learned, we would therefore expect data scaling:
\begin{equation}
    L(D) = L_{(D / {\tau \zeta(\alpha+1)})^{1/(1+\alpha)}} \approx \frac{1}{\alpha\zeta(\alpha+1)} \left( \frac{D}{\tau \zeta(\alpha+1)} \right)^{-\frac{\alpha}{\alpha+1}} \propto D^{-\frac{\alpha}{\alpha+1}}.
\end{equation}
This mechanism of data scaling therefore predicts that a power law distribution over quanta with exponent $\alpha+1$ translates into a data scaling exponent $\alpha_D = \alpha / (\alpha + 1)$. From our earlier result that $\alpha_N = \alpha$, we would predict that $\alpha_D = \alpha_N / (\alpha_N + 1)$. We discuss whether this relationship holds empirically for data and parameter scaling exponents observed across a variety of studies in \Cref{sec:parameter-vs-data-scaling-review}.

\bigbreak
\noindent\textbf{Data scaling (single-epoch)}: In multi-epoch training, the information contained in the training dataset can bottleneck which quanta are learned. However, the rate of convergence of SGD can also bottleneck performance. For single-epoch training, a greater number of training samples allows one to train for longer. Assume that batches are large and that they contain effectively perfect gradient information. If quanta each reduce mean loss by an amount given by a power law, then the gradients incentivizing each quantum to form may also roughly follow a power law in magnitude. We might therefore expect that the number of training steps $S$ to learn quantum $k$ to be inversely proportional to use frequency $p_k$ (more commonly useful quanta have larger gradients and are learned faster). Therefore if the first quantum requires $T$ steps to be learned, then quantum $n$ will require $T n^{\alpha+1}$ steps to converge. As a function of the number of training steps $S$, the number of quanta learned is therefore $n = (S/T)^{1/(\alpha+1)}$, and so:
\begin{equation}
    L(S) = L_{(S/T)^{1/(\alpha+1)}} \approx \frac{1}{\alpha \zeta(\alpha+1)} \left( \frac{S}{T} \right)^{-\frac{\alpha}{\alpha + 1}} \propto S^{-\frac{\alpha}{\alpha+1}}.
\end{equation}
The scaling exponent $\alpha_S$ of loss w.r.t\ steps $S$ is therefore the same as the multi-epoch data scaling exponent $\alpha_D$.
%


\bigbreak 
\noindent\textbf{Review of prior work}: Several models of power law neural scaling have been proposed in prior work. \textcite{sharma2022scaling} develop a model of power law scaling w.r.t.\ model parameters which describes networks as performing a piecewise-linear approximation of a function on a data manifold of intrinsic dimension $d$. Under their model, the scaling exponent $\alpha_N$ is determined by the dimension of the data manifold via $\alpha_N \leq \frac{4}{d}$. \textcite{michaud2023precision} point out the effective dimension $d$ could be generalized to the maximum arity of the task computation graph for sparse compositional problems. The model of \textcite{sharma2022scaling} was also generalized by \textcite{bahri2021explaining} to account for power law scaling in training data and who additionally relate scaling exponents to a power law spectrum of certain kernels. \textcite{maloney2022solvable} develop a random-feature model of scaling, in which power law scaling comes from power law spectra of the data feature-feature covariance matrix, and scaling exponents are determined by the power law exponent over these spectra. \textcite{hutter2021learning} propose a toy model of data scaling in which features are learned based on whether they've been seen during training, and a Zipfian distribution over features produces power law data scaling.

%


%

%

%







%


%


%

%

%

%

%

%

%

%
    
%

%

%

%



%
    
    
%

%

%

%

%





%


%



%



%

\section{Proof of concept: a toy dataset}\label{sec:SPARSEPARITY}

In this section, we will describe a toy dataset transparently consisting of distinct subtasks which are power law distributed in frequency. We observe power law neural scaling in data and parameters on this task, and find that the mechanism of neural scaling coincides with our theory from~\Cref{sec:THEORY}. It is therefore possible for power law neural scaling to arise from the Quantization Model. We leave a study of whether natural datasets (e.g. natural language) possess such structure to~\Cref{sec:llms}.

%
    
%
%

\subsection{The ``multitask sparse parity'' dataset}

The toy task we will construct consists of many subtasks -- distinct types of inputs which each require corresponding distinct computations (quanta). For each subtask, we choose a variant of the ``sparse parity'' problem, recently studied in \cite{barak2022hidden}. The sparse parity prediction problem is simple: given a bit string of length $n$, compute the parity (sum mod 2) of a fixed subset of $k$ of those bits. We introduce an extension of this task, which we call ``multitask sparse parity''. Beyond $n$ and $k$, multitask sparse parity adds an additional parameter $n_\text{tasks}$, the number of subtasks (number of distinct versions of sparse parity) present in the dataset. To construct the task, we first choose $n_\text{tasks}$ random subsets $S_i$ of $k$ indices from $\{1, 2, \ldots, n\}$: $S_i \subset \{1, 2, \ldots, n\}$ and $|S_i| = k$, where $i = 1, 2, \ldots, n_\text{tasks}$. Input bit strings are length $n_\text{tasks} + n$. We call the first $n_\text{tasks}$ bits the \emph{control bits} and the last $n$ bits the \emph{task bits}. If control bit $i$ is active, then the parity is computed from the subset $S_i$ of the task bits. The control bits 1-hot encode the task number: on a given input, only one control bit is set to $1$ at a time -- the rest are zero. For the sample shown below, since control bit $2$ is active, the answer is the parity of the task bits $S_2 = \{2, 7\}$, which is $0$ for this input:

\begin{center}
    \includegraphics[width=\textwidth]{figures/sparse-parity-diagram.pdf}
\end{center}

We impose a uniform distribution over the task bits. On the control bits, we impose a Zipfian distribution: the probability that a sample has control bit $i$ active (and therefore the parity must be computed from the subset $S_i$ of the task bits) is $\frac{1}{Z} i^{-(\alpha+1)}$ where $Z = \sum_{i=1}^{n_\text{tasks}} i^{-(\alpha+1)}$. This imposes a power law distribution over subtasks in data. Since answers are parities, this task can be treated as a binary classification problem on the subset of bit strings $\{0, 1\}^{n_\text{tasks} + n}$ where for each string all but one bit of the first $n_\text{tasks}$ bits are zero.



%
    
%

%


%

%
    
%

\subsection{Power law scaling and emergence}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/parameters-steps-data-emergence-and-scaling-scalingtop.png}
    \caption{\textbf{Top:} Neural networks exhibit power law neural scaling parameters $N$, training time $S$, and training samples $D$ (for multi-epoch training) when trained on the multitask sparse parity dataset. Here $\alpha = 0.4$ and we plot lines $\propto N^{-\alpha}$, $\propto S^{-\alpha/(\alpha+1)}$, $\propto D^{-\alpha / (\alpha+1)}$. \textbf{Bottom:} neural scaling broken down by subtask. Scaling behavior on individual subtasks exhibits emergence, where subtasks are not learned below a certain scale and then suddenly learned beyond a certain scale. Power law neural scaling of mean test loss averages over a large number of qualitative changes in network performance (when broken down by subtask), with loss being driven to zero on an increasing number of subtasks which are power law distributed in frequency, a realization of the mechanism of neural scaling discussed in \Cref{sec:THEORY}.
    %
    }
    \label{fig:data-parameter-parity-scaling}
\end{figure}

We train ReLU MLPs with a single hidden layer to solve this task. The input dimension is $n_\text{tasks} + n$ and we use cross-entropy loss, so the output dimension is $2$. We use the Adam optimizer with a learning rate of $10^{-3}$. To study scaling with respect to the number of model parameters, we train networks of varying width by sampling batches online. For high enough $n$ (e.g.\ 100) it is unlikely that the network will encounter the same sample twice during training. Within an individual single-epoch training run, we can study scaling in steps $S$. To study scaling with respect to multi-epoch training dataset size $D$, we use a network of sufficient width for capacity to not be a bottleneck, and for varying $D$ we sample a training set of $D$ samples and train for multiple epochs, recording model performance when mean test loss is lowest (early-stopping). 

Training dynamics on the multitask sparse parity problem are highly nontrivial -- on each individual subtask, loss follows a reverse-S curve, undergoing a ``phase transition'' after an initial plateau. However, this transition happens at different times for different subtasks, so the overall loss decreases smoothly, averaging over these phase transitions. We leave a more detailed discussion of training dynamics to \Cref{sec:appendix-multitask-parity}.

\Cref{fig:data-parameter-parity-scaling} shows scaling curves on the multitask sparse parity problem. For the results shown, we used $n_\text{tasks} = 500$, $n = 100$, $k = 3$, $\alpha=0.4$, and a batch size of $20000$. We vary training dataset size from 1e4 to 5e6 and vary hidden-layer width from 10 to 500 neurons. We train for 2e5 steps. In line with the theory from~\Cref{sec:THEORY}, we find that as we scale training data and parameters, networks learn more and more quanta (reducing loss on more and more subtasks), roughly in order of their frequency, and that this is what drives neural scaling. We see that mean loss decreases as a power law with $\alpha_N \approx \alpha$ and $\alpha_D \approx \alpha / (\alpha + 1)$, although $\alpha_S$ is somewhat greater than $\alpha / (\alpha + 1)$. We see that scaling w.r.t.\ parameters is noisier than data scaling, possibly due to model initialization having some influence on which computational quanta are learned (for our data scaling experiments, we use the same seed and same model size for all runs, eliminating this effect). We also see that when we look at scaling on individual subtasks, there is a rough scale of data or parameters below which networks do not learn the task, and above which they do. Smooth power law scaling therefore averages over a large number of phase transitions in model performance when properly decomposed by subtask, a proof of concept that the Quantization Model can be the mechanism of neural scaling for data with the right structure. For additional discussion of training dynamics and results on how the empirical scaling exponents $\alpha_N, \alpha_S, \alpha_D$ relate to quanta distribution power law exponent $\alpha + 1$ for a variety of $\alpha$ (beyond just $\alpha = 0.4$) see \Cref{sec:appendix-multitask-parity}.

%

%


%

%

%

%

    
%



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
 
%




%
\section{Decomposing empirical LLM scaling}\label{sec:llms}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/pythia-scaling-sixpanel.png}
    \caption{\textbf{Top left:} Scaling of mean test loss w.r.t.\ non-embedding parameters for the Eleuther Pythia models. The parameter scaling exponent $\alpha_N$ is measured to be $\approx 0.083$ from the first six points along the curve (the seventh model appears to break the trend), roughly similar to the parameter scaling exponent measured in~\cite{kaplan2020scaling}. \textbf{Top center:} the distribution $p(L)$ over losses on individual tokens for models of different size. Token losses $\approx 0$ are by far the most common, and larger models achieve $\approx 0$ loss on an increasing fraction of tokens. \textbf{Top right:} the expected loss integrand $L \cdot p(L)$ for models of different sizes.  Despite their very high prevalence, low-loss tokens contribute minimal mass to the mean loss, which is instead dominated by tokens with much higher loss of 5-10 bits (depending on scale). \textbf{Bottom left:} Training curves (scaling w.r.t.\ steps $S$) of mean test loss for Pythia models. We measure exponents $\alpha_S$ between 0.037 and 0.06. \textbf{Bottom center:} the distribution $p(L)$ over time. Over time, models achieve $\approx 0$ loss on an increasing fraction of tokens, similar to scaling in model size. \textbf{Bottom right:} The distribution $L \cdot p(L)$ over time.
    %
    }
    \label{fig:mean-and-distribution-llm-scaling}
\end{figure}

We now study how scaling curves for large language models decompose. For our experiments, we use the ``Pythia'' model sequence from Eleuther~\parencite{eleutherai2023pythia}. These are decoder-only transformers of varying size trained on the same data in the same order -- approximately 300 billion tokens of the train set of The Pile~\parencite{gao2020pile}. Eleuther released 143 checkpoints for these models, spaced 1000 optimization steps apart. We can therefore study scaling w.r.t.\ model parameters $N$ and training steps $S$. We evaluate the first seven models in the sequence, which range from 19m to 6.4b non-embedding parameters, on approximately 10 million tokens from the test set of The Pile. We record cross-entropy loss on every token. With this collection of loss values, we are able to study how neural scaling decomposes -- rather than looking just at how mean test loss changes with scale, we can see how the distribution over losses changes with scale.

\subsection{The distribution over per-token losses}

In \Cref{fig:mean-and-distribution-llm-scaling}, we plot some basic facts about how neural scaling decomposes in LLMs. First, we find that for the first six models in Pythia sequence, the mean loss of the final model  against the number of non-embedding model parameters is well-fit by a power law with exponent $\alpha_N = 0.083$. This is roughly in line with the parameter scaling exponent of $0.076$ measured in \cite{kaplan2020scaling}\footnote{\cite{kaplan2020scaling} also use non-embedding parameters when studying scaling w.r.t.\ parameters}. The 6.4b model does not fit the scaling curve well, so we excluded its loss when measuring the scaling exponent. Next, we plot the probability distribution over per-token losses $p(L)$. We find that losses close to zero are by far the most common, and that scaling increases the portion of approximately-zero losses. We also plot $L \cdot p(L)$, the probability density over losses weighted by loss. The mean loss is the area under this curve. We see that despite approximately-zero-loss tokens being by far the most common, they do not contribute much mass to the mean loss. We also plot mean loss as well as $p(L)$ and $L \cdot p(L)$ versus optimization steps rather than model size.

We see immediately that neural scaling in the wild is somewhat more complicated than our theoretical model. Notably, the distribution over losses is not bimodal like it was for multitask sparse parity. 
%
Nevertheless, we do see that losses of approximately zero are by far the most common and that models of increasing scale achieve approximately zero loss on an increasing fraction of the dataset. We leave a detailed study of whether the statistics of neural scaling in LLMs are compatible with prior models of neural scaling to future work.

\subsection{A taxonomy: monogenic versus polygenic behaviors}\label{sec:llm-scaling-taxonomy}

In our introduction of the Quantization Hypothesis in~\Cref{sec:THEORY} and our multitask sparse parity study in~\Cref{sec:SPARSEPARITY} we modeled network performance on individual samples as benefitting from a single quanta -- all samples belong to a single subtask, which is either solved or not solved in a binary fashion based on the presence or absence of some computation in the network. In our model and on multitask sparse parity, scaling curves on individual examples all exhibit emergence -- loss on individual examples undergoes a phase transition at a particular scale of parameters or data. Do we observe this in large language models?

Manually inspecting a large number of per-token scaling curves, we observe a variety of scaling behaviors. We see that not all loss scaling curves on individual tokens undergo a phase transition, or a single drop at a particular model scale. More commonly, loss improves at more than one model scale.

If it were true, as we conjectured earlier, that the effect of scaling is to simply add computations to the network, while still learning quanta present in smaller networks, then for scaling curves on individual prediction problems to show progress at multiple scales, it must be the case that prediction on those problems benefits from multiple quanta additively. 

As first mentioned in~\Cref{sec:THEORY}, we borrow terminology from genetics and refer to prediction problems for which the model's loss is influenced by multiple quanta as \emph{polygenic} (in analogy to when multiple genes contribute to a trait) and problems for which performance is determined by a single quantum as \emph{monogenic} (akin to when a single gene determines a trait). In multitask sparse parity, all prediction problems are monogenic. In natural language, we observe that the majority of tokens are polygenic but that we can indeed find monogenic tokens for which loss drops as a single phase transition in scale. Polygenicity forms a spectrum: the smoothness of the loss curve can vary substantially between examples, presumably with some prediction problems only using few quanta and others using many. In~\Cref{fig:llm-diverse-scaling-behaviors}, we show extreme examples of both monogenic and polygenic prediction problems.

Note that our monogenic/polygenic taxonomy of model behaviors assumes that QH1 and QH2 are true, that larger networks contain the quanta of smaller networks. However, it could be the case that very little is similar between large networks from small networks. It is encouraging that some structures such as induction heads have been found across many models at many scales~\parencite{olsson2022context}, but whether other computations performed across models are truly universal, and whether scaling has the effect we described, will have to be investigated in future studies of the internals of neural networks. 

\begin{figure}[th!]
    \centering
    \includegraphics[width=5.5in]{figures/diverse-scaling-behaviors.pdf}
    \caption{Scaling on individual tokens can have diverse behavior. Here we show examples of scaling curves on examples which we call \emph{monogenic} and \emph{polygenic}. Scaling curves on monogenic examples display emergence: there is a particular model scale at which the model's performance improves rather abruptly. Scaling on polygenic curves displays gradual progress, since (we conjecture) many quanta, emerging at different scales, marginally contribute to the loss.
    %
    }
    \label{fig:llm-diverse-scaling-behaviors}
\end{figure}


\subsection{Auto-discovering quanta with language model internals}\label{sec:llm-quanta-discovery}

We will now attempt to auto-discover quanta in language modeling. While for multitask sparse parity it was clear how to partition the prediction task into subtasks, it is unclear a priori how to do so for the task of predicting natural language. For instance, partitioning samples based on the correct output token is suboptimal since the same token can occur for different reasons depending on the context and where prediction relies separate quanta. Partitioning inputs based on the final $n$-gram of the context is also suboptimal, since prediction often relies on information contained throughout the whole context and on abstract patterns within it. Clustering based on inputs or outputs therefore seems unlikely to discover quanta in language modeling. We therefore use the internals of trained language models to cluster samples. An ideal clustering scheme would group samples based on which internal mechanism(s) models use for prediction on those examples.

\textbf{Quanta Discovery from Gradients (QDG)}: For the discovery of quanta, we propose a method based on spectral clustering with model gradients. This method clusters samples together based on whether gradients on those samples point in similar directions. In particular, given a set of samples $(x_i, y_i)$ and a model $f_\theta$, we compute gradients $g_i = \nabla_\theta L(f_\theta(x_i), y_i)$. We then normalize these gradients $g_i \mapsto \hat{g}_i$ so that $\hat{g}_i \cdot \hat{g}_i = 1$. Let $A$ be a matrix whose rows are the normalized gradients: $A_{i, \cdot} = \hat{g}_i$. We can define an affinity matrix $C = A A^T$, so that $C_{ij} = \hat{g}_i \cdot \hat{g}_j$, the cosine similarity between gradients $g_i, g_j$. One can then define an affinity matrix $\hat{C}$ of angular similarities (which take values in $[0, 1]$) via $\hat{C}_{ij} = 1 - \arccos(C_{ij})/\pi$. We perform spectral clustering with $\hat{C}$ to cluster samples $(x_i, y_i)$.

One challenge of QDG is that it is expensive to compute when gradients are very high dimensional. When applying QDG to language models, we therefore use only the smallest model in the Pythia sequence, which has 19m non-embedding parameters. We use gradients within self-attention and MLP layers, but do not include embed, unembed, or layer norm gradients when we flatten and concatenate gradients into a vector $g$.\footnote{We exclude gradients for embed and unembed parameters because they are high dimensional and also because they may contain information more about the input and output rather than the computations the model performs internally. We exclude layer norm gradients because they appeared to contain less information about clusters in toy experiments.} We choose samples $(x_i, y_i)$ for which our 19m-parameter model achieves a cross-entropy loss less than $0.1$ nats. We filter based on this criteria since (1) we cannot cluster samples based on model mechanism if the model does not have such a mechanism for performing prediction correctly on those samples and (2) our intuition that samples with particularly low loss are more likely to be monogenic. We further exclude samples which can be solved via induction on the context\footnote{We filter (copying) induction problems by excluding samples where the token which is to be predicted is the last token in a trigram which occurred earlier in the context. This is not a very comprehensive filtering scheme.}, since such samples are quite common (possibly interfering with our task of finding diverse quanta) and since early experiments indicated that QDG had trouble clustering such samples together. We choose 10000 such samples to perform clustering on from the test set of The Pile. After computing the affinity matrix $\hat{C}$, we use the spectral clustering implementation from SciPy~\parencite{2020SciPy-NMeth} with labels assigned via k-means.

We find that QDG discovers many clusters of coherent model behavior. We show examples from clusters in \Cref{fig:cluster-examples} and \Cref{fig:cluster-examples-more}. These clusters were found with the spectral clustering hyperparameter \texttt{n\_clusters = 400}. While most clusters involve the prediction of the same token, manually inspecting these clusters we find that they usually involve predicting the same token for a coherent reason, rather than being based merely on having the same output. We also find clusters for more abstract prediction rules. For instance, the quantum shown on the left column of~\Cref{fig:cluster-examples} is for continuing a numerical sequence, and the examples involve prediction for a variety of numbers.

\subsection{The natural distribution over language modeling quanta}\label{sec:llm-subtask-distribution}

\begin{figure}[t!]
    \centering
    \includegraphics{figures/similarity-matrix-and-rank-frequency-envelope.png}
    \caption{\textbf{Left:} angular similarity between model gradients for a variety of natural language samples. Samples are reordered according to their QDG cluster (with 400 clusters) to reveal the block-diagonal structure of the similarity matrix. We visualize a small part of the overall similarity matrix in this plot -- note that not all clusters are as visibly distinct as the ones shown. 
    \textbf{Right:} rank-frequency plot of clusters computed with spectral clustering from the similarity matrix of model gradients. We measure the slope of the envelope of the rank-frequency curves from cluster rank 100-1000 to be $\approx -1.24$, which is a steeper than the slope of -1.08 expected from the measured parameter-scaling exponent from~\Cref{fig:mean-and-distribution-llm-scaling}, though within margin of error given the uncertainty of our clustering methodology. See~\Cref{sec:appendix-clustering-analysis} for a discussion of the bias/uncertainty of our method.}
    \label{fig:llm-similarity-and-frequency}
\end{figure}

Some quanta of prediction ability are more frequently relied upon than others. Earlier, we hypothesized that a power law in how frequently the quanta are utilized is the origin of power law scaling. When we cluster samples with QDG, do we find that a power law governs the size of the clusters? The measured scaling exponent of $\alpha_N = 0.083$ implies a power law distribution over quanta with exponent $-1.083$, and so we would hope to recover a power law with this exponent governing the relative size of the clusters.

\Cref{fig:llm-similarity-and-frequency} shows rank-frequency curves for clusters discovered with QDG for varying choices of \texttt{n\_clusters}. These curves rank the clusters according to their size and then plot size against cluster index. We plot rank-frequency curves for many choices of \texttt{n\_clusters} since it is unclear a priori which \texttt{n\_clusters} to use. When we measure the slope of the rank-frequency curve, we measure it from the envelope formed by the many rank-frequency curves, a practice which we discuss in~\Cref{sec:appendix-clustering-analysis}. Biases in the clustering algorithm and inherent noise in model gradients make clustering imperfect, and lead to high uncertainty of our the measured power law exponent. From an argument in~\Cref{sec:appendix-clustering-analysis}, we think that extracting the power law exponent over quanta utilization frequency by measuring the slope of the rank-frequency curve should have uncertainty of at least 0.2. We measure a slope of $\approx -1.24$, about $0.16$ off our expected slope of $-1.08$, and so within the margin of error. While less naive clustering schemes could help sharpen this measurement in future work, we are encouraged that the size of our discovered clusters seems to decay at a rate compatible with the power law predicted from the Quantization Model given the empirical scaling exponents for language modeling on The Pile. 


%
%

%

%

%

    
%




%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%



%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%


\section{Discussion}

We have articulated the \emph{Quantization Model} of neural scaling laws. This relied on the \emph{Quantization Hypothesis} which posits that neural network performance can be understood with respect to a discrete set of computations and the associated capabilities they enable, which networks can either succeed or fail at learning. We called these computations/capabilities the \emph{quanta} of the prediction problem, and sorted them into the Q Sequence according to how frequently they are used for prediction in the data distribution. We saw that when the use frequencies of the quanta are given by a power law, we can get power law neural scaling as networks learn additional quanta. For the multitask sparse parity problem, we found that the Quantization Hypothesis holds, and that power law neural scaling averages over the emergence of quanta (network capabilities on subtasks). We then decomposed LLM scaling curves by token and auto-discovered quanta for language prediction with a method we called \emph{QDG}. Beyond understanding neural scaling laws, we speculate that our perspective could have a number of other implications for understanding deep neural networks:

%
%

%
%

\bigbreak
\noindent\textbf{Understanding Emergence:} \textcite{srivastava2022beyond} study how model capabilities scale on a variety of tasks, and find diverse scaling behavior: some tasks display high ``linearity'' where model performance improves gradually with scale and others display ``breakthroughness'' where model performance improves sharply at a particular scale. Under the Quantization Hypothesis, the linearity or breakthroughness of a task would be influenced by how the quanta relevant to the task are distributed along the Q Sequence. If performance relies on a single quantum of knowledge or computation, or on multiple quanta close together in the Q Sequence, we should expect high breakthroughness. On the other hand, if the relevant quanta are numerous and distributed widely across the Q Sequence, we would expect performance to improve gradually across scale. The Quantization Hypothesis also suggests that we may be able to predict when certain capabilities will arise with scale if we could know where their corresponding quanta lie on the Q Sequence. This could in theory be estimated if we could compute how frequently those quanta would be useful for prediction in the training distribution. 

\bigbreak
\noindent\textbf{Mechanistic Interpretability:} If it were true that computations were learned universally across model scales, then the task of mechanistically understanding neural networks might simplify. If performance is determined by the quanta -- a particular, enumerable set of computations -- then understanding the network could reduce to enumerating the quanta. Having done this, the learned knowledge and abilities of our networks could perhaps then be translated into a more interpretable format (something like code), studied in this format, and eventually executed in this format, rather than via the operation of the network. 

\bigbreak
\noindent\textbf{The Science of Deep Learning:} If we can understand model performance with respect to a particular set of computations, then perhaps these become natural objects of study in deep learning. Instead of studying as a black box how engineering choices like architecture, optimization hyperparameters, and scale affect model performance, we could instead study at an intermediate level how these choices influence the building blocks of model performance -- the quanta. For instance, instead of studying the training dynamics at the level of individual parameters, or at the level of the whole-network performance, one could study how the quanta emerge over training. This \emph{mesoscale} understanding of networks, in terms of the internal computations which collectively constitute their performance, could act like statistical physics for deep learning, perhaps allowing us to bridge our microscale understanding of low-level training dynamics and our macroscale understanding of model performance. 

\bigbreak

%

%


\bigbreak
\bigbreak

\begin{center}
\rule{0.5\textwidth}{1.5pt}
\end{center}

%
%

%

%

%
    
%


\bigbreak

\noindent\textbf{Acknowledgements}: We thank Tamay Besiroglu, Neel Nanda, Tony Wang, Ben Edelman, Wes Gurnee, Eleni Shor, Max Nadeau, and Xander Davies for helpful conversations and feedback. We thank Lauro Langosco for helping with code to visualize samples from The Pile. This work was supported by the Foundational Questions Institute, the Rothberg Family Fund for Cognitive Science, the NSF Graduate Research Fellowship (Grant No. 2141064), and IAIFI through NSF grant PHY-2019786.


\printbibliography

\appendix

\section{More general scaling laws}\label{sec:appendix-general-scaling-laws}

If one learns the first $n$ quanta, reducing the loss from $b_k$ to $a_k$ ($1\leq k\leq n$), while the loss remains $b_k$ for $k>n$. The expected loss is given by:
\begin{equation}
L_n = \sum_{k=1}^n a_kp_k + \sum_{k=n+1}^\infty b_kp_k.
\end{equation}
In the main text, we used $a_k=a$ and $b_k=b$ for our model. However, one can imagine a variety of other choices for $a_k$ and $b_k$.

\begin{comment}
{\bf Case 1 (main text)} $b_k=b$ and $a_k=a\ (0<a<b)$. The expected loss is given by
\begin{equation}
L_n = \sum_{k=1}^n bp_k + \sum_{k=n+1}^\infty a\cdot p_k  %
\approx a + \frac{b-a}{\alpha\zeta(\alpha+1)}n^{-\alpha},
\end{equation}
which is power law $n^{-\alpha}$ plus a constant irreducible loss term $c$.
\end{comment}

{\bf Case 1} $b_k=-{\rm log}\ p_k$ and $a_k=0$, where $p_k=k^{-(\alpha+1)}/\zeta(\alpha+1)$. The expected loss is given by:
\begin{equation}
L_n = \sum_{k=1}^n 0 \cdot p_k + \sum_{k=n+1}^\infty (-{\rm log\ }p_k)\cdot p_k \approx \frac{1+\alpha+\alpha{\rm log}\zeta(\alpha+1)}{\alpha^2\zeta(\alpha+1)}n^{-\alpha} + \frac{\alpha+1}{\alpha\zeta(\alpha+1)}n^{-\alpha}{\rm log\ }n,
\end{equation}
which contains a power law term $n^{-\alpha}$ plus a log term $n^{-\alpha}{\rm log\ }n$. For very large $n$, the log term can be ignored, so $L$ is still approximately a power law of $n$ with exponent $-\alpha$, shown in Figure~\ref{fig:log_power_law}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/log_power_law.pdf}
    \caption{Comparing different scaling laws. Setting $a_k=0$, we compare $b_k=-{\rm log}\ p_k$ (solid lines) and $b_k=1$ (dashed lines) for different alphas. Although the $b_k=-{\rm log}\ p_k$ case would cause an extra loss term $n^{-\alpha}{\rm log}n$ in additional to the power law term $n^{-\alpha}$, the loss becomes a power law asymptotically when $n$ becomes large.} 
    \label{fig:log_power_law}
\end{figure}

{\bf Case 2} $b_k=-{\rm log}\ p_k$ and $ a_k=-{\rm log}\ (Cp_k)\ (C>1)$, where $p_k=k^{-(\alpha+1)}/\zeta(\alpha+1)$. The expected loss is given by:
\begin{equation}
L_n = \sum_{k=1}^n (-{\rm log\ }(Cp_k)) \cdot p_k + \sum_{k=n+1}^\infty (-{\rm log\ }p_k)\cdot p_k \approx \frac{{\rm log}C}{\alpha\zeta(\alpha+1)}n^{-\alpha}-{\rm log}C+\frac{1+\alpha+\alpha{\rm log}\zeta(\alpha+1)}{\alpha^2\zeta(\alpha+1)},
\end{equation}
which is a power law $n^{-\alpha}$ plus a constant. 


\section{Additional results on multitask sparse parity}\label{sec:appendix-multitask-parity}

\noindent\textbf{Training dynamics}: When loss is broken down by subtask on multitask sparse parity, learning curves consist of many reverse-S shaped curves, and mean loss decreases smoothly as an average over these curves. In~\Cref{fig:sparse-parity-training-dynamics}, we show loss versus time for each subtask for training runs in both the single-epoch and multi-epoch regimes. In~\Cref{fig:sparse-parity-convergence-time} we show how convergence time for each subtask relates to the frequency of that subtask.

\begin{figure}[h!]
    \centering
    \includegraphics[width=5.5in]{figures/parity-subtask-timeseries-infinite-and-finite-data.jpg}
    \caption{Training dynamics on the multitask sparse parity dataset consist of many ``phase transitions'' when decomposed by subtask -- the loss curve for each subtask drops following an initial plateau of no apparent progress, in line with~\cite{barak2022hidden}. The mean loss decreases smoothly, averaging over these phase transitions in the model's performance on subtasks. We show curves for single-epoch training (top) and multi-epoch training on 5 million samples (bottom). The dashed red line indicates the early stopping point where mean test loss is minimized. For these runs, $\alpha = 0.4$.}
    \label{fig:sparse-parity-training-dynamics}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics{figures/sparse-parity-convergence-time.pdf}
    \caption{Convergence time for each subtask versus the frequency of that subtask. We see that convergence time $S_k$ on subtask $k$ is $S_k \propto p_k^{-0.81}$ rather than $S_k \propto p_k^{-1}$ as we had expected. This leads to a steeper scaling w.r.t.\ $S$ than expected from theory. For these experiments, we used $\alpha = 0.4$, and so we would have predicted $\alpha_S \approx 0.29$ but instead we get $\alpha_S \approx 0.45$. We consider the model to have converged on a subtask once it gets mean test loss less than 0.1 bits on that subtask.}
    \label{fig:sparse-parity-convergence-time}
\end{figure}

\bigbreak
\noindent\textbf{Scaling for varying $\alpha$}: In~\Cref{fig:sparse-parity-varying-alpha-scaling} we show scaling curves on multitask sparse parity in $N, S, D$ for a variety of quanta distribution parameters $\alpha$. While all scaling curves appear to be power laws, the relationship between $\alpha_N, \alpha_S, \alpha_D$ and $\alpha$ is not precisely as predicted by theory:

\begin{enumerate}
    \item \textbf{Parameter scaling:} We observe that the relationship between $\alpha_N$ and $\alpha$ deviates a bit from the prediction $\alpha_N = \alpha$, with $\alpha_N < \alpha$ for small $\alpha$ and $\alpha_N > \alpha$ for large $\alpha$. Perhaps model size does not influence learning just by changing capacity, but also by affecting optimization.

    \item \textbf{Step scaling:} We observe that $\alpha_S$ is consistently higher than the theoretical prediction $\alpha / (\alpha + 1)$. In~\Cref{fig:sparse-parity-convergence-time}, we saw that the number of steps to convergence for each subtask did not precisely follow $S_k \propto p_k^{-1}$, but was closer to $S_k \propto p_k^{-0.81}$. This means that many subtasks converge faster than we would expect, producing a steeper scaling curve.

    \item \textbf{Data scalaing:} We observe that $\alpha_D$ is substantially higher than the theoretical prediction $\alpha / (\alpha + 1)$ for small $\alpha$. We think this may be related to the fact that early-stopping cuts off training before all subtasks are learned as observed in~\Cref{fig:sparse-parity-training-dynamics}. In~\Cref{fig:sparse-parity-varying-alpha-n-scaling}, we show how the number of subtasks learned $n$, when we include subtasks learned after early-stopping, seems to be in line with theory: $n \propto D^{1/(\alpha+1)}$. 

\end{enumerate}

Better understanding the precise nature of power law scaling on multitask sparse parity is an interesting avenue for future work.

%

\begin{figure}
    \centering
    \includegraphics{figures/sparse-parity-data-scaling-dependence-n.pdf}
    \caption{Number of subtasks learned ($n$), including subtasks learned after early-stopping would terminated the training run, versus training samples $D$ for a variety of $\alpha$. We see that the relation $n \propto D^{1/(\alpha+1)}$ approximately holds, in line with theory. Deviation from theory for the scaling exponent of loss $L$ w.r.t.\ $D$ therefore likely originates from our failure to regularize network training, leading to early-stopping ending training before some subtasks can be learned.}
    \label{fig:sparse-parity-varying-alpha-n-scaling}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics{figures/sparse-parity-all-scaling-varying-alpha.pdf}
    \caption{Scaling in parameters ($N$), single-epoch training time ($S$), and multi-epoch training samples $(D)$ for varying quanta power law distribution parameter $\alpha$ on multitask sparse parity. We notice that scaling curves in steps $S$ are typically steeper than the $\alpha_S = \alpha / (\alpha + 1)$ predicted from theory, and that for low $\alpha$ the scaling curves in $D$ also deviate from theory substantially.}
    \label{fig:sparse-parity-varying-alpha-scaling}
\end{figure}





%

%
%

%


\newpage
\section{Additional results on language models}

In~\Cref{fig:cluster-examples-more} we show additional examples from clusters discovered with QDG.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/llm-spectral-cluster-more-examples.pdf}
    \caption{Additional examples of clusters of inputs discovered by QDG. Like in~\Cref{fig:cluster-examples}, we used 10000 samples and \texttt{n\_clusters} of 400.}
    \label{fig:cluster-examples-more}
\end{figure}


\newpage
\section{The difficulty of estimating the power law exponent from clusters}\label{sec:appendix-clustering-analysis}

In~\Cref{sec:llm-subtask-distribution}, when we looked at the distribution over elements in each cluster, we did not perfectly recover a Zipf distribution with exponent $\approx 1.08$ that we expected from our theory. In this section, we describe the difficulty of accurately estimating such an exponent with our method. 

\subsection{QDG on multitask sparse parity}

\begin{figure}[h!]
    \centering
    \includegraphics{figures/similarity-matrix-and-rank-frequency-envelope-sparseparity.png}
    \caption{Similarity matrix and rank-frequency plots from QDG on multitask sparse parity. Despite sparse parity having a known decomposition into subtasks which are power law distributed in frequency, we do not recover this same power law from samples. We used $\alpha = 0.4$ for the frequency distribution for an expected rank-frequency power law exponent of -1.4, but measure a rank-frequency envelope slope closer to -1.1.}
    \label{fig:sparse-parity-similarity-and-envelope}
\end{figure}

As a first experiment, we performed QDG on multitask sparse parity, where there is a known, artificially-imposed power law distribution over subtasks. We train a width-500 single-hidden-layer ReLU MLP on multitask sparse parity with $\alpha = 0.4$ and with $n=100$, $k=3$, and $n_\text{tasks} = 500$. We then took 10000 samples which the network achieves $\approx 0$ loss on (sampled from the Zipf distribution over subtasks with exponent 1.4). We compute gradients of cross-entropy loss w.r.t.\ all model parameters for these samples, and then perform QDG just like for LLMs. We show results in~\Cref{fig:sparse-parity-similarity-and-envelope}. We plot the full similarity matrix where samples are ordered according to their a priori known subtask, rather than their cluster from QDG, and  see a clear pattern where elements from the same subtask have on average higher angular similarity than elements between subtasks. However, from the rank-frequency plot of the clusters, we do not recover a slope of -1.4, but rather a lower slope of $\approx -1.1$. This shows that even when there is an exact decomposition of inputs into subtasks with a known Zipf distribution over these subtasks, that we do not perfectly recover this Zipf distribution from QDG. 

\subsection{A toy model of QDG uncertainty and bias}

{\bf A toy model:} To understand the bias of spectral clustering, we develop the following toy model. We assume the dataset has $N=1000$ subtasks, each subtask containing $n_i=\lfloor\frac{A}{i^\alpha}\rfloor (1\leq i\leq N)$ tokens ($A=1000$). We use a Gaussian distribution $\mathcal{N}(\mat{m}_i,\sigma^2\mat{I}_{d\times d})$ to model gradients within a subtask $i$, where $d$ is the embedding dimension, $\sigma$ is the noise level, and $\mat{m}_i$ is the Gaussian mean. $\mat{m}_i$ itself is drawn from the standard Gaussian distribution $\mat{m}_i\sim \mathcal{N}(\mat{0}, \mat{I}_{d\times d})$. We define the similarity between two vectors $\mat{x}, \mat{y}$ to be ${\rm sim}\equiv 1+\frac{\mat{x}}{|\mat{x}|}\cdot \frac{\mat{y}}{|\mat{y}|}$. We compute pairwise similarity between all $\sum_{i=1}^N n_i$ tokens, and input the similarity matrix to the spectral clustering algorithm. We also need to specify the number of clusters $k$.


We have two hyperparameters in the toy model, the embedding dimension $d$ and the noise level $\sigma$. We need to determine them such that this toy model can decently imitate LLM results (Figure~\ref{fig:llm-similarity-and-frequency}). We fix $\alpha=1$, sweeping $d=\{30,100,1000\}$, $\sigma=\{0,0.5,2.0\}$, and $k=\{100,200,500\}$. As shown in Figure~\ref{fig:toy_clustering_d_noise}, the high-dimension ($d=1000$) large-noise ($\sigma=2.0$) scheme seem to best agree with the LLM results, since the $k=200$ curve can reproduce the sag and the cliff present in LLM curves.

Estimating $\alpha$ from the frequency curve is hard, in fact, the slope depends on $k$ and the region used to estimate it. However, we observe that different $k$ curves form a clear envelope, whose slope is robust in a reasonably wide region. The envelope slope seems to indicate $\alpha$. We fix $d=1000$ and $\sigma=2.0$, sweeping $\alpha=\{0.8,0.9,1.0,1.1,1.2,1.3,1.4,1.5\}$. For each $\alpha$, we estimate the slope of the envelope. Although there is clear correlation between the estimated envelope and $\alpha$, if we use the envelope slope to estimate $\alpha$, the error is on the order of 0.2, as shown in Figure~\ref{fig:toy_clustering_envelope}.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/toy_clustering_d_noise.pdf}
    \caption{To understand the bias of spectral clustering, we apply spectral clustering to a toy model with different embedding dimension $d$, noise scale $\sigma$ and number of cluster $k$. The high-dimension ($d=1000$) large-noise ($\sigma=2.0$) scheme seems to best agree with the LLM results (Figure~\ref{fig:llm-similarity-and-frequency}).}
    \label{fig:toy_clustering_d_noise}
\end{figure}

\begin{figure}
    \centering\includegraphics[width=0.8\linewidth]{figures/toy_clustering_envelope_notparity.pdf}\includegraphics[width=0.19\linewidth]{figures/toy_clustering_compare.pdf}
    \caption{The difficulty of measuring $\alpha$ from curves. We apply spectral clustering to a toy model with different $\alpha$ and number of clusters $k$. For a fixed  $\alpha$, different $k$ curves define an envelope. One could use the envelope slope to infer $\alpha$, but this incurs errors around 0.2.}
    \label{fig:toy_clustering_envelope}
\end{figure}


\newpage
\section{Parameter and data scaling exponents across studies}\label{sec:parameter-vs-data-scaling-review}

In~\Cref{fig:parameter-data-scaling-exponents-review}, we show $\alpha_N$ and $\alpha_D$ (or possibly $\alpha_S$, depending on the study) for a variety of prior studies of deep learning scaling, as compiled by~\textcite{epoch2023scalinglawsliteraturereview}. While the data is messy, it is intriguing that most of the \textcite{rosenfeld2019constructive} samples lie below the $\alpha_D = \alpha_N$ line, as our model would predict. The scaling exponents from \textcite{hoffmann2022training} are also closer to our prediction than the relation $\alpha_D = \alpha_N$, which has been proposed by other models of neural scaling laws. Overall though, the existing empirical results are too messy to definitively support or contradict our model.

\begin{figure}[h!]
    \centering
    \includegraphics{figures/scaling-scatter-linear-scale.pdf}
    \caption{Parameter and data scaling exponents from various studies of deep learning scaling, compiled from the database of neural scaling laws from~\cite{epoch2023scalinglawsliteraturereview}. Our model of scaling predicts that $\alpha_D = \alpha_N / (\alpha_N + 1)$, indicated with the solid black line. Visible points are from \cite{rosenfeld2019constructive, kaplan2020scaling, hoffmann2022training, gordon2021data, droppo2021scaling}. \cite{ardalani2022understanding} is above the visible window of the figure.}
    \label{fig:parameter-data-scaling-exponents-review}
\end{figure}



\end{document}



