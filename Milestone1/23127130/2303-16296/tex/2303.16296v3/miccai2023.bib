
@article{AOWSBermanCVPR2020, 
year = {2020}, 
title = {{AOWS: Adaptive and Optimal Network Width Search with Latency Constraints}}, 
author = {Berman, Maxim and Pishchulin, Leonid and Xu, Ning and Blaschko, Matthew and Medioni, Gerard}, 
journal = {CVPR}, 
keywords = {}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Berman-AOWS-%20Adaptive%20and%20Optimal%20Network%20Width%20Search%20with%20Latency%20Constraints​-2020-CVPR.pdf}
}
@article{UniversallyYuICCV2019, 
year = {2019}, 
title = {{Universally slimmable networks and improved training techniques}}, 
author = {Yu, Jiahui and Huang, Thomas S}, 
journal = {ICCV}, 
keywords = {}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yu-Universally%20slimmable%20networks%20and%20improved%20training%20techniques-2019-ICCV.pdf}
}
@article{SNNYuICLR2019, 
year = {2019}, 
title = {{Slimmable neural networks}}, 
author = {Yu, Jiahui and Yang, Linjie and Xu, Ning and Yang, Jianchao and Huang, Thomas}, 
journal = {ICLR}, 
keywords = {}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yu-Slimmable%20neural%20networks-2019-ICLR.pdf}
}
@article{AutoSlimYuNeurIPSWorkshop2019, 
year = {2019}, 
title = {{AutoSlim: Towards One-Shot Architecture Search for Channel Numbers}}, 
author = {Yu, Jiahui and Huang, Thomas}, 
journal = {NeurIPS Workshop}, 
keywords = {}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yu-AutoSlim-%20Towards%20One-Shot%20Architecture%20Search%20for%20Channel%20Numbers-2019-NeurIPS%20Workshop.pdf}
}
@article{NASZophICLR2017, 
year = {2017}, 
title = {{Neural architecture search with reinforcement learning}}, 
author = {Zoph, Barret and Le, Quoc V}, 
journal = {ICLR}, 
abstract = {{Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zoph-Neural%20architecture%20search%20with%20reinforcement%20learning-2017-ICLR.pdf}
}
@article{LearningZophCVPR2018, 
year = {2018}, 
title = {{Learning transferable architectures for scalable image recognition}}, 
author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V}, 
journal = {CVPR}, 
abstract = {{Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the "NASNet search space") which enables transferability. In our experiments, we search for the best convolutional layer (or "cell") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named "NASNet architecture". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4\% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7\% top-1 and 96.2\% top-5 on ImageNet. Our model is 1.2\% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28\% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74\% top-1 accuracy, which is 3.1\% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0\% achieving 43.1\% mAP on the COCO dataset.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zoph-Learning%20transferable%20architectures%20for%20scalable%20image%20recognition-2018-CVPR.pdf}
}
@article{ENASPhamICML2018, 
year = {2018}, 
title = {{Efficient neural architecture search via parameter sharing}}, 
author = {Pham, Hieu and Guan, Melody Y and Zoph, Barret and Le, Quoc V and Dean, Jeff}, 
journal = {ICML}, 
abstract = {{We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89\%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65\%.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Pham-Efficient%20neural%20architecture%20search%20via%20parameter%20sharing-2018-ICML.pdf}
}
@article{DARTSLiuICLR2019, 
year = {2019}, 
title = {{DARTS: Differentiable Architecture Search}}, 
author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming}, 
journal = {ICLR}, 
abstract = {{This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-DARTS-%20Differentiable%20Architecture%20Search-2019-ICLR.pdf}
}
@article{NetAdaptYangECCV2018, 
year = {2018}, 
title = {{NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications}}, 
author = {Yang, Tien-Ju and Howard, Andrew and Chen, Bo and Zhang, Xiao and Go, Alec and Sandler, Mark and Sze, Vivienne and Adam, Hartwig}, 
journal = {ECCV}, 
abstract = {{This work proposes an algorithm, called NetAdapt, that automatically adapts a pre-trained deep neural network to a mobile platform given a resource budget. While many existing algorithms simplify networks based on the number of MACs or weights, optimizing those indirect metrics may not necessarily reduce the direct metrics, such as latency and energy consumption. To solve this problem, NetAdapt incorporates direct metrics into its adaptation algorithm. These direct metrics are evaluated using empirical measurements, so that detailed knowledge of the platform and toolchain is not required. NetAdapt automatically and progressively simplifies a pre-trained network until the resource budget is met while maximizing the accuracy. Experiment results show that NetAdapt achieves better accuracy versus latency trade-offs on both mobile CPU and mobile GPU, compared with the state-of-the-art automated network simplification algorithms. For image classification on the ImageNet dataset, NetAdapt achieves up to a 1.7\$\textbackslashtimes\$ speedup in measured inference latency with equal or higher accuracy on MobileNets (V1\&V2).}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-NetAdapt-%20Platform-Aware%20Neural%20Network%20Adaptation%20for%20Mobile%20Applications-2018-ECCV.pdf}
}
@article{MnasNetTanCVPR2019, 
year = {2019}, 
title = {{MnasNet: Platform-Aware Neural Architecture Search for Mobile}}, 
author = {Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V}, 
journal = {CVPR}, 
abstract = {{Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2\% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8× faster than MobileNetV2 with 0.5\% higher accuracy and 2.3× faster than NASNet with 1.2\% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet.}}, 
pages = {2815--2823}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tan-MnasNet-%20Platform-Aware%20Neural%20Architecture%20Search%20for%20Mobile-2019-CVPR.pdf}
}
@article{MobileNetsHowardarXiv2017, 
year = {2017}, 
title = {{MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications}}, 
author = {Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig}, 
journal = {arXiv}, 
eprint = {1704.04861}, 
abstract = {{We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Howard-MobileNets-%20Efficient%20Convolutional%20Neural%20Networks%20for%20Mobile%20Vision%20Applications-2017-arXiv.pdf}
}
@article{MobileNetV2SandlerCVPR2018, 
year = {2018}, 
title = {{MobileNetV2: Inverted Residuals and Linear Bottlenecks}}, 
author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh}, 
journal = {CVPR}, 
abstract = {{In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the in-put/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classification, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Sandler-MobileNetV2-%20Inverted%20Residuals%20and%20Linear%20Bottlenecks-2018-CVPR.pdf}
}
@article{MobileNetV3HowardICCV2019, 
year = {2019}, 
title = {{Searching for MobileNetV3}}, 
author = {Howard, Andrew and Sandler, Mark and Chen, Bo and Wang, Weijun and Chen, Liang-Chieh and Tan, Mingxing and Chu, Grace and Vasudevan, Vijay and Zhu, Yukun and Pang, Ruoming and Adam, Hartwig and Le, Quoc}, 
journal = {ICCV}, 
abstract = {{We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 20\% compared to MobileNetV2. MobileNetV3-Small is 6.6\% more accurate compared to a MobileNetV2 model with comparable latency. MobileNetV3-Large detection is over 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LRASPP is 34\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.}}, 
pages = {1314--1324}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Howard-Searching%20for%20MobileNetV3-2019-ICCV.pdf}
}
@article{BNIoffeICML2015, 
year = {2015}, 
title = {{Batch normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}}, 
author = {Ioffe, Sergey and Szegedy, Christian}, 
journal = {ICML}, 
abstract = {{Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ioffe-Batch%20normalization-%20Accelerating%20Deep%20Network%20Training%20by%20Reducing%20Internal%20Covariate%20Shift-2015-ICML.pdf}
}
@article{HowSanturkarNeurIPS2018, 
year = {2018}, 
title = {{How does batch normalization help optimization?}}, 
author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander}, 
journal = {NeurIPS}, 
abstract = {{Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called "internal covariate shift". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Santurkar-How%20does%20batch%20normalization%20help%20optimization--2018-NeurIPS.pdf}
}
@article{ShuffleNetZhangCVPR2018, 
year = {2018}, 
title = {{ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices}}, 
author = {Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian}, 
journal = {CVPR}, 
eprint = {1706.00384}, 
abstract = {{We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10–150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8\%) than recent MobileNet [12] on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves ∼13× actual speedup over AlexNet while maintaining comparable accuracy.}}, 
pages = {6848--6856}, 
keywords = {}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-ShuffleNet-%20An%20Extremely%20Efficient%20Convolutional%20Neural%20Network%20for%20Mobile%20Devices-2018-CVPR.pdf}
}
@article{ShuffleNetV2MaECCV2018, 
year = {2018}, 
title = {{ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design}}, 
author = {Ma, Ningning and Zhang, Xiangyu and Zheng, Hai-Tao and Sun, Jian}, 
journal = {ECCV}, 
abstract = {{Currently, the neural network architecture design is mostly guided by the \textbackslashemph\{indirect\} metric of computation complexity, i.e., FLOPs. However, the \textbackslashemph\{direct\} metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical \textbackslashemph\{guidelines\} for efficient network design. Accordingly, a new architecture is presented, called \textbackslashemph\{ShuffleNet V2\}. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ma-ShuffleNet%20V2-%20Practical%20Guidelines%20for%20Efficient%20CNN%20Architecture%20Design-2018-ECCV.pdf}
}
@article{KDHintonNeurIPSWorkshop2015, 
year = {2015}, 
title = {{Distilling the knowledge in a neural network}}, 
author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff}, 
journal = {NeurIPS Workshop}, 
abstract = {{A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hinton-Distilling%20the%20knowledge%20in%20a%20neural%20network-2015-NeurIPS%20Workshop.pdf}
}
@article{BNNHubaraNeurIPS2016, 
year = {2016}, 
title = {{Binarized neural networks: Training Neural Networks with Weights and Activations Constrained to +1 or -1}}, 
author = {Hubara, Itay and Soudry, Daniel and Yaniv, Ran El}, 
journal = {NeurIPS}, 
abstract = {{We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time and when computing the parameters' gradient at train-time. We conduct two sets of experiments, each based on a different framework, namely Torch7 and Theano, where we train BNNs on MNIST, CIFAR-10 and SVHN, and achieve nearly state-of-the-art results. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which might lead to a great increase in power-efficiency. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hubara-Binarized%20neural%20networks-%20Training%20Neural%20Networks%20with%20Weights%20and%20Activations%20Constrained%20to%20+1%20or%20-1-2016-NeurIPS.pdf}
}
@article{XNOR-NetRastegariECCV2016, 
year = {2016}, 
title = {{XNOR-Net: Imagenet Classification Using Binary Convolutional Neural Networks}}, 
author = {Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali}, 
journal = {ECCV}, 
issn = {0302-9743}, 
abstract = {{We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values resulting in 32× memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58× faster convolutional operations (in terms of number of the high precision operations) and 32× memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is the same as the full-precision AlexNet. We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than 16\% in top-1 accuracy. Our code is available at: http://allenai.org/plato/xnornet.}}, 
pages = {525--542}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Rastegari-XNOR-Net-%20Imagenet%20Classification%20Using%20Binary%20Convolutional%20Neural%20Networks-2016-ECCV.pdf}
}
@article{ProxylessNASCaiICLR2019, 
year = {2019}, 
title = {{ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware}}, 
author = {Cai, Han and Zhu, Ligeng and Han, Song}, 
journal = {ICLR}, 
abstract = {{Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. \$10\textasciicircum4\$ GPU hours) makes it difficult to \textbackslashemph\{directly\} search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize\textbackslashtextasciitilde\textbackslashemph\{proxy\} tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present \textbackslashemph\{ProxylessNAS\} that can \textbackslashemph\{directly\} learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08\textbackslash\% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\$\textbackslashtimes\$ fewer parameters. On ImageNet, our model achieves 3.1\textbackslash\% better top-1 accuracy than MobileNetV2, while being 1.2\$\textbackslashtimes\$ faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Cai-ProxylessNAS-%20Direct%20Neural%20Architecture%20Search%20on%20Target%20Task%20and%20Hardware-2019-ICLR.pdf}
}
@article{Any-precisionYuAAAI2021, 
year = {2021}, 
title = {{Any-precision deep neural networks}}, 
author = {Yu, Haichao and Li, Haoxiang and Shi, Honghui and Huang, Thomas S and Hua, Gang}, 
journal = {AAAI}, 
eprint = {1911.07346}, 
abstract = {{We present Any-Precision Deep Neural Networks (Any-Precision DNNs), which are trained with a new method that empowers learned DNNs to be flexible in any numerical precision during inference. The same model in runtime can be flexibly and directly set to different bit-width, by truncating the least significant bits, to support dynamic speed and accuracy trade-off. When all layers are set to low-bits, we show that the model achieved accuracy comparable to dedicated models trained at the same precision. This nice property facilitates flexible deployment of deep learning models in real-world applications, where in practice trade-offs between model accuracy and runtime efficiency are often sought. Previous literature presents solutions to train models at each individual fixed efficiency/accuracy trade-off point. But how to produce a model flexible in runtime precision is largely unexplored. When the demand of efficiency/accuracy trade-off varies from time to time or even dynamically changes in runtime, it is infeasible to re-train models accordingly, and the storage budget may forbid keeping multiple models. Our proposed framework achieves this flexibility without performance degradation. More importantly, we demonstrate that this achievement is agnostic to model architectures. We experimentally validated our method with different deep network backbones (AlexNet-small, Resnet-20, Resnet-50) on different datasets (SVHN, Cifar-10, ImageNet) and observed consistent results. Code and models will be available at https://github.com/haichaoyu.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yu-Any-precision%20deep%20neural%20networks-2019-arXiv.pdf}
}
@article{SwitchableGuerraarXiv2020, 
year = {2020}, 
title = {{Switchable precision neural networks}}, 
author = {Guerra, Luis and Zhuang, Bohan and Reid, Ian and Drummond, Tom}, 
journal = {arXiv}, 
eprint = {2002.02815}, 
abstract = {{Instantaneous and on demand accuracy-efficiency trade-off has been recently explored in the context of neural networks slimming. In this paper, we propose a flexible quantization strategy, termed Switchable Precision neural Networks (SP-Nets), to train a shared network capable of operating at multiple quantization levels. At runtime, the network can adjust its precision on the fly according to instant memory, latency, power consumption and accuracy demands. For example, by constraining the network weights to 1-bit with switchable precision activations, our shared network spans from BinaryConnect to Binarized Neural Network, allowing to perform dot-products using only summations or bit operations. In addition, a self-distillation scheme is proposed to increase the performance of the quantized switches. We tested our approach with three different quantizers and demonstrate the performance of SP-Nets against independently trained quantized models in classification accuracy for Tiny ImageNet and ImageNet datasets using ResNet-18 and MobileNet architectures.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Guerra-Switchable%20precision%20neural%20networks-2020-arXiv.pdf}
}
@article{FromDuBMVC2020, 
year = {2020}, 
title = {{From quantized DNNs to quantizable DNNs}}, 
author = {Du, Kunyuan and Zhang, Ya and Guan, Haibing}, 
journal = {BMVC}, 
eprint = {2004.05284}, 
abstract = {{This paper proposes Quantizable DNNs, a special type of DNNs that can flexibly quantize its bit-width (denoted as `bit modes' thereafter) during execution without further re-training. To simultaneously optimize for all bit modes, a combinational loss of all bit modes is proposed, which enforces consistent predictions ranging from low-bit mode to 32-bit mode. This Consistency-based Loss may also be viewed as certain form of regularization during training. Because outputs of matrix multiplication in different bit modes have different distributions, we introduce Bit-Specific Batch Normalization so as to reduce conflicts among different bit modes. Experiments on CIFAR100 and ImageNet have shown that compared to quantized DNNs, Quantizable DNNs not only have much better flexibility, but also achieve even higher classification accuracy. Ablation studies further verify that the regularization through the consistency-based loss indeed improves the model's generalization performance.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Du-From%20quantized%20DNNs%20to%20quantizable%20DNNs-2020-BMVC.pdf}
}
@article{SingleGuoECCV2020, 
year = {2020}, 
title = {{Single path one-shot neural architecture search with uniform sampling}}, 
author = {Guo, Zichao and Zhang, Xiangyu and Mu, Haoyuan and Heng, Wen and Liu, Zechun and Wei, Yichen and Sun, Jian}, 
journal = {ECCV}, 
eprint = {1904.00420}, 
abstract = {{One-shot method is a powerful Neural Architecture Search (NAS) framework, but its training is non-trivial and it is difficult to achieve competitive results on large scale datasets like ImageNet. In this work, we propose a Single Path One-Shot model to address its main challenge in the training. Our central idea is to construct a simplified supernet, Single Path Supernet, which is trained by an uniform path sampling method. All underlying architectures (and their weights) get trained fully and equally. Once we have a trained supernet, we apply an evolutionary algorithm to efficiently search the best-performing architectures without any fine tuning. Comprehensive experiments verify that our approach is flexible and effective. It is easy to train and fast to search. It effortlessly supports complex search spaces (e.g., building blocks, channel, mixed-precision quantization) and different search constraints (e.g., FLOPs, latency). It is thus convenient to use for various needs. It achieves start-of-the-art performance on the large dataset ImageNet.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Guo-Single%20path%20one-shot%20neural%20architecture%20search%20with%20uniform%20sampling-2020-ECCV.pdf}
}
@article{UnderstandingBenderICML2018, 
year = {2018}, 
title = {{Understanding and simplifying one-shot architecture search}}, 
author = {Bender, Gabriel and Kindermans, Pieter-Jan and Zoph, Barret and Vasudevan, Vijay and Le, Quoc}, 
journal = {ICML}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bender-Understanding%20and%20simplifying%20one-shot%20architecture%20search-2018-ICML.pdf}
}
@article{SMASHBrockICLR2018, 
year = {2018}, 
title = {{SMASH: One-Shot Model Architecture Search through HyperNetworks}}, 
author = {Brock, Andrew and Lim, Theodore and Ritchie, J M and Weston, Nick}, 
journal = {ICLR}, 
abstract = {{Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized hand-designed networks. Our code is available at https://github.com/ajbrock/SMASH}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Brock-SMASH-%20One-Shot%20Model%20Architecture%20Search%20through%20HyperNetworks-2018-ICLR.pdf}
}
@article{LearningVeniatCVPR2018, 
year = {2018}, 
title = {{Learning time/memory-efficient deep architectures with budgeted super networks}}, 
author = {Veniat, Tom and Denoyer, Ludovic}, 
journal = {CVPR}, 
abstract = {{We propose to focus on the problem of discovering neural network architectures efficient in terms of both prediction quality and cost. For instance, our approach is able to solve the following tasks: learn a neural network able to predict well in less than 100 milliseconds or learn an efficient model that fits in a 50 Mb memory. Our contribution is a novel family of models called Budgeted Super Networks (BSN). They are learned using gradient descent techniques applied on a budgeted learning objective function which integrates a maximum authorized cost, while making no assumption on the nature of this cost. We present a set of experiments on computer vision problems and analyze the ability of our technique to deal with three different costs: the computation cost, the memory consumption cost and a distributed computation cost. We particularly show that our model can discover neural network architectures that have a better accuracy than the ResNet and Convolutional Neural Fabrics architectures on CIFAR-10 and CIFAR-100, at a lower cost.}}, 
pages = {3492--3500}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Veniat-Learning%20time-memory-efficient%20deep%20architectures%20with%20budgeted%20super%20networks-2018-CVPR.pdf}
}
@article{AreLabelsLiuECCV2020, 
year = {2020}, 
title = {{Are labels necessary for neural architecture search?}}, 
author = {Liu, Chenxi and Dollár, Piotr and He, Kaiming and Girshick, Ross and Yuille, Alan and Xie, Saining}, 
journal = {ECCV}, 
abstract = {{Existing neural network architectures in computer vision --- whether designed by humans or by machines --- were typically found using both images and their associated labels. In this paper, we ask the question: can we find high-quality neural architectures using only images, but no human-annotated labels? To answer this question, we first define a new setup called Unsupervised Neural Architecture Search (UnNAS). We then conduct two sets of experiments. In sample-based experiments, we train a large number (500) of diverse architectures with either supervised or unsupervised objectives, and find that the architecture rankings produced with and without labels are highly correlated. In search-based experiments, we run a well-established NAS algorithm (DARTS) using various unsupervised objectives, and report that the architectures searched without labels can be competitive to their counterparts searched with labels. Together, these results reveal the potentially surprising finding that labels are not necessary, and the image statistics alone may be sufficient to identify good neural architectures.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-Are%20labels%20necessary%20for%20neural%20architecture%20search--2020-ECCV.pdf}
}
@article{NAS-Bench-101YingICML2019, 
year = {2019}, 
title = {{NAS-Bench-101: Towards Reproducible Neural Architecture Search}}, 
author = {Ying, Chris and Klein, Aaron and Real, Esteban and Christiansen, Eric and Murphy, Kevin and Hutter, Frank}, 
journal = {ICML}, 
eprint = {1902.09635}, 
abstract = {{Recent advances in neural architecture search (NAS) demand tremendous computational resources, which makes it difficult to reproduce experiments and imposes a barrier-to-entry to researchers without access to large-scale computation. We aim to ameliorate these problems by introducing NAS-Bench-101, the first public architecture dataset for NAS research. To build NAS-Bench-101, we carefully constructed a compact, yet expressive, search space, exploiting graph isomorphisms to identify 423k unique convolutional architectures. We trained and evaluated all of these architectures multiple times on CIFAR-10 and compiled the results into a large dataset of over 5 million trained models. This allows researchers to evaluate the quality of a diverse range of models in milliseconds by querying the pre-computed dataset. We demonstrate its utility by analyzing the dataset as a whole and by benchmarking a range of architecture optimization algorithms.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ying-NAS-Bench-101-%20Towards%20Reproducible%20Neural%20Architecture%20Search-2019-ICML.pdf}
}
@article{EfficientNetTanICML2019, 
year = {2019}, 
title = {{EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks}}, 
author = {Tan, Mingxing and Le, Quoc V}, 
journal = {ICML}, 
eprint = {1905.11946}, 
abstract = {{Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4\% top-1 / 97.1\% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tan-EfficientNet-%20Rethinking%20Model%20Scaling%20for%20Convolutional%20Neural%20Networks-2019-ICML.pdf}
}
@article{DesigningRadosavovicCVPR2020, 
year = {2020}, 
title = {{Designing network design spaces}}, 
author = {Radosavovic, Ilija and Kosaraju, Raj Prateek and Girshick, Ross and He, Kaiming and Dollár, Piotr}, 
journal = {CVPR}, 
eprint = {2003.13678}, 
abstract = {{In this work, we present a new network design paradigm. Our goal is to help advance the understanding of network design and discover design principles that generalize across settings. Instead of focusing on designing individual network instances, we design network design spaces that parametrize populations of networks. The overall process is analogous to classic manual design of networks, but elevated to the design space level. Using our methodology we explore the structure aspect of network design and arrive at a low-dimensional design space consisting of simple, regular networks that we call RegNet. The core insight of the RegNet parametrization is surprisingly simple: widths and depths of good networks can be explained by a quantized linear function. We analyze the RegNet design space and arrive at interesting findings that do not match the current practice of network design. The RegNet design space provides simple and fast networks that work well across a wide range of flop regimes. Under comparable training settings and flops, the RegNet models outperform the popular EfficientNet models while being up to 5x faster on GPUs.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Radosavovic-Designing%20network%20design%20spaces-2020-CVPR.pdf}
}
@article{HypernetworksHaICLR2017, 
year = {2017}, 
title = {{Hypernetworks}}, 
author = {Ha, David and Dai, Andrew and Le, Quoc V}, 
journal = {ICLR}, 
eprint = {1609.09106}, 
abstract = {{This work explores hypernetworks: an approach of using a one network, also known as a hypernetwork, to generate the weights for another network. Hypernetworks provide an abstraction that is similar to what is found in nature: the relationship between a genotype - the hypernetwork - and a phenotype - the main network. Though they are also reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to-end with backpropagation and thus are usually faster. The focus of this work is to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as relaxed form of weight-sharing across layers. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve near state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks. Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ha-Hypernetworks-2017-ICLR.pdf}
}
@article{FBNetWuCVPR2019, 
year = {2019}, 
title = {{FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search}}, 
author = {Wu, Bichen and Keutzer, Kurt and Dai, Xiaoliang and Zhang, Peizhao and Wang, Yanghan and Sun, Fei and Wu, Yiming and Tian, Yuandong and Vajda, Peter and Jia, Yangqing}, 
journal = {CVPR}, 
abstract = {{Designing accurate and efficient ConvNets for mobile devices is challenging because the design space is combinatorially large. Due to this, previous neural architecture search (NAS) methods are computationally expensive. ConvNet architecture optimality depends on factors such as input resolution and target devices. However, existing approaches are too resource demanding for case-by-case redesigns. Also, previous work focuses primarily on reducing FLOPs, but FLOP count does not always reflect actual latency. To address these, we propose a differentiable neural architecture search (DNAS) framework that uses gradient-based methods to optimize ConvNet architectures, avoiding enumerating and training individual architectures separately as in previous methods. FBNets (Facebook-Berkeley-Nets), a family of models discovered by DNAS surpass state-of-the-art models both designed manually and generated automatically. FBNet-B achieves 74.1\% top-1 accuracy on ImageNet with 295M FLOPs and 23.1 ms latency on a Samsung S8 phone, 2.4x smaller and 1.5x faster than MobileNetV2-1.3[17] with similar accuracy. Despite higher accuracy and lower latency than MnasNet[20], we estimate FBNet-B’s search cost is 420x smaller than MnasNet’s, at only 216 GPUhours. Searched for different resolutions and channel sizes, FBNets achieve 1.5\% to 6.4\% higher accuracy than MobileNetV2. The smallest FBNet achieves 50.2\% accuracy and 2.9 ms latency (345 frames per second) on a Samsung S8. Over a Samsung-optimized FBNet, the iPhone-Xoptimized model achieves a 1.4x speedup on an iPhone X. FBNet models are open-sourced at https://github.com/facebookresearch/mobile-vision.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wu-FBNet-%20Hardware-Aware%20Efficient%20ConvNet%20Design%20via%20Differentiable%20Neural%20Architecture%20Search-2019-CVPR.pdf}
}
@article{FBNetV2WanCVPR2020, 
year = {2020}, 
title = {{FBNetV2: Differentiable Neural Architecture Search for Spatial and Channel Dimensions}}, 
author = {Wan, Alvin and Dai, Xiaoliang and Zhang, Peizhao and He, Zijian and Tian, Yuandong and Xie, Saining and Wu, Bichen and Yu, Matthew and Xu, Tao and Chen, Kan and Vajda, Peter and Gonzalez, Joseph E}, 
journal = {CVPR}, 
eprint = {2004.05565}, 
abstract = {{Differentiable Neural Architecture Search (DNAS) has demonstrated great success in designing state-of-the-art, efficient neural networks. However, DARTS-based DNAS's search space is small when compared to other search methods', since all candidate network layers must be explicitly instantiated in memory. To address this bottleneck, we propose a memory and computationally efficient DNAS variant: DMaskingNAS. This algorithm expands the search space by up to \$10\textasciicircum\{14\}\textbackslashtimes\$ over conventional DNAS, supporting searches over spatial and channel dimensions that are otherwise prohibitively expensive: input resolution and number of filters. We propose a masking mechanism for feature map reuse, so that memory and computational costs stay nearly constant as the search space expands. Furthermore, we employ effective shape propagation to maximize per-FLOP or per-parameter accuracy. The searched FBNetV2s yield state-of-the-art performance when compared with all previous architectures. With up to 421\$\textbackslashtimes\$ less search cost, DMaskingNAS finds models with 0.9\% higher accuracy, 15\% fewer FLOPs than MobileNetV3-Small; and with similar accuracy but 20\% fewer FLOPs than Efficient-B0. Furthermore, our FBNetV2 outperforms MobileNetV3 by 2.6\% in accuracy, with equivalent model size. FBNetV2 models are open-sourced at https://github.com/facebookresearch/mobile-vision.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wan-FBNetV2-%20Differentiable%20Neural%20Architecture%20Search%20for%20Spatial%20and%20Channel%20Dimensions-2020-CVPR.pdf}
}
@article{OnRadosavovicICCV2019, 
year = {2019}, 
title = {{On network design spaces for visual recognition}}, 
author = {Radosavovic, Ilija and Johnson, Justin and Xie, Saining and Lo, Wan-Yen and Dollar, Piotr}, 
journal = {ICCV}, 
abstract = {{Over the past several years progress in designing better neural network architectures for visual recognition has been substantial. To help sustain this rate of progress, in this work we propose to reexamine the methodology for comparing network architectures. In particular, we introduce a new comparison paradigm of distribution estimates, in which network design spaces are compared by applying statistical techniques to populations of sampled models, while controlling for confounding factors like network complexity. Compared to current methodologies of comparing point and curve estimates of model families, distribution estimates paint a more complete picture of the entire design landscape. As a case study, we examine design spaces used in neural architecture search (NAS). We find significant statistical differences between recent NAS design space variants that have been largely overlooked. Furthermore, our analysis reveals that the design spaces for standard model families like ResNeXt can be comparable to the more complex ones used in recent NAS work. We hope these insights into distribution analysis will enable more robust progress toward discovering better networks for visual recognition.}}, 
pages = {1882--1890}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Radosavovic-On%20network%20design%20spaces%20for%20visual%20recognition-2019-ICCV.pdf}
}
@article{NBDTWanICLR2021, 
year = {2021}, 
title = {{NBDT: Neural-Backed Decision Trees}}, 
author = {Wan, Alvin and Dunlap, Lisa and Ho, Daniel and Yin, Jihan and Lee, Scott and Jin, Henry and Petryk, Suzanne and Bargal, Sarah Adel and Gonzalez, Joseph E}, 
journal = {ICLR}, 
eprint = {2004.00221}, 
abstract = {{Deep learning is being adopted in settings where accurate and justifiable predictions are required, ranging from finance to medical imaging. While there has been recent work providing post-hoc explanations for model predictions, there has been relatively little work exploring more directly interpretable models that can match state-of-the-art accuracy. Historically, decision trees have been the gold standard in balancing interpretability and accuracy. However, recent attempts to combine decision trees with deep learning have resulted in models that (1) achieve accuracies far lower than that of modern neural networks (e.g. ResNet) even on small datasets (e.g. MNIST), and (2) require significantly different architectures, forcing practitioners pick between accuracy and interpretability. We forgo this dilemma by creating Neural-Backed Decision Trees (NBDTs) that (1) achieve neural network accuracy and (2) require no architectural changes to a neural network. NBDTs achieve accuracy within 1\% of the base neural network on CIFAR10, CIFAR100, TinyImageNet, using recently state-of-the-art WideResNet; and within 2\% of EfficientNet on ImageNet. This yields state-of-the-art explainable models on ImageNet, with NBDTs improving the baseline by \textbackslashtextasciitilde14\% to 75.30\% top-1 accuracy. Furthermore, we show interpretability of our model's decisions both qualitatively and quantitatively via a semi-automatic process. Code and pretrained NBDTs can be found at https://github.com/alvinwan/neural-backed-decision-trees.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wan-NBDT-%20Neural-Backed%20Decision%20Trees-2020-arXiv.pdf}
}
@article{RegularizedRealAAAI2019, 
year = {2019}, 
title = {{Regularized evolution for image classifier architecture search}}, 
author = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V}, 
journal = {AAAI}, 
issn = {2159-5399}, 
abstract = {{The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier— AmoebaNet-A—that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-theart 83.9\% top-1 / 96.6\% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.}}, 
pages = {4780--4789}, 
volume = {33}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Real-Regularized%20evolution%20for%20image%20classifier%20architecture%20search-2019-AAAI.pdf}
}
@article{DifferentiableMenschICML2018, 
year = {2018}, 
title = {{Differentiable dynamic programming for structured prediction and attention}}, 
author = {Mensch, Arthur and Blondel, Mathieu}, 
journal = {ICML}, 
eprint = {1802.03676}, 
abstract = {{Dynamic programming (DP) solves a variety of structured combinatorial problems by iteratively breaking them down into smaller subproblems. In spite of their versatility, DP algorithms are usually non-differentiable, which hampers their use as a layer in neural networks trained by backpropagation. To address this issue, we propose to smooth the max operator in the dynamic programming recursion, using a strongly convex regularizer. This allows to relax both the optimal value and solution of the original combinatorial problem, and turns a broad class of DP algorithms into differentiable operators. Theoretically, we provide a new probabilistic perspective on backpropagating through these DP operators, and relate them to inference in graphical models. We derive two particular instantiations of our framework, a smoothed Viterbi algorithm for sequence prediction and a smoothed DTW algorithm for time-series alignment. We showcase these instantiations on two structured prediction tasks and on structured and sparse attention for neural machine translation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Mensch-Differentiable%20dynamic%20programming%20for%20structured%20prediction%20and%20attention-2018-ICML.pdf}
}
@article{Single-PathNASStamoulisECML-PKDD2019, 
year = {2019}, 
title = {{Single-Path NAS: Designing Hardware-Efficient ConvNets in Less Than 4 Hours}}, 
author = {Stamoulis, Dimitrios and Ding, Ruizhou and Wang, Di and Lymberopoulos, Dimitrios and Priyantha, Bodhi and Liu, Jie and Marculescu, Diana}, 
journal = {ECML-PKDD}, 
eprint = {1904.02877}, 
abstract = {{Can we automatically design a Convolutional Network (ConvNet) with the highest image classification accuracy under the runtime constraint of a mobile device? Neural architecture search (NAS) has revolutionized the design of hardware-efficient ConvNets by automating this process. However, the NAS problem remains challenging due to the combinatorially large design space, causing a significant searching time (at least 200 GPU-hours). To alleviate this complexity, we propose Single-Path NAS, a novel differentiable NAS method for designing hardware-efficient ConvNets in less than 4 hours. Our contributions are as follows: 1. Single-path search space: Compared to previous differentiable NAS methods, Single-Path NAS uses one single-path over-parameterized ConvNet to encode all architectural decisions with shared convolutional kernel parameters, hence drastically decreasing the number of trainable parameters and the search cost down to few epochs. 2. Hardware-efficient ImageNet classification: Single-Path NAS achieves 74.96\% top-1 accuracy on ImageNet with 79ms latency on a Pixel 1 phone, which is state-of-the-art accuracy compared to NAS methods with similar constraints (<80ms). 3. NAS efficiency: Single-Path NAS search cost is only 8 epochs (30 TPU-hours), which is up to 5,000x faster compared to prior work. 4. Reproducibility: Unlike all recent mobile-efficient NAS methods which only release pretrained models, we open-source our entire codebase at: https://github.com/dstamoulis/single-path-nas.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Stamoulis-Single-Path%20NAS-%20Designing%20Hardware-Efficient%20ConvNets%20in%20Less%20Than%204%20Hours-2019-ECML-PKDD.pdf}
}
@article{WeightGaierNeurIPS2019, 
year = {2019}, 
title = {{Weight agnostic neural networks}}, 
author = {Gaier, Adam and Ha, David}, 
journal = {NeurIPS}, 
eprint = {1906.04358}, 
abstract = {{Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures that achieve much higher than chance accuracy on MNIST using random weights. Interactive version of this paper at https://weightagnostic.github.io/}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Gaier-Weight%20agnostic%20neural%20networks-2019-NeurIPS.pdf}
}
@article{OneKaiserarXix2017, 
year = {2017}, 
title = {{One model to learn them all}}, 
author = {Kaiser, Lukasz and Gomez, Aidan N and Shazeer, Noam and Vaswani, Ashish and Parmar, Niki and Jones, Llion and Uszkoreit, Jakob}, 
journal = {arXiv}, 
eprint = {1706.05137}, 
abstract = {{Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kaiser-One%20model%20to%20learn%20them%20all-2017-arXiv.pdf}
}
@article{BigNASYuECCV2020, 
year = {2020}, 
title = {{BigNAS: Scaling Up Neural Architecture Search with Big Single-Stage Models}}, 
author = {Yu, Jiahui and Jin, Pengchong and Liu, Hanxiao and Bender, Gabriel and Kindermans, Pieter-Jan and Tan, Mingxing and Huang, Thomas and Song, Xiaodan and Pang, Ruoming and Le, Quoc}, 
journal = {ECCV}, 
eprint = {2003.11142}, 
abstract = {{Neural architecture search (NAS) has shown promising results discovering models that are both accurate and fast. For NAS, training a one-shot model has become a popular strategy to rank the relative quality of different architectures (child models) using a single set of shared weights. However, while one-shot model weights can effectively rank different network architectures, the absolute accuracies from these shared weights are typically far below those obtained from stand-alone training. To compensate, existing methods assume that the weights must be retrained, finetuned, or otherwise post-processed after the search is completed. These steps significantly increase the compute requirements and complexity of the architecture search and model deployment. In this work, we propose BigNAS, an approach that challenges the conventional wisdom that post-processing of the weights is necessary to get good prediction accuracies. Without extra retraining or post-processing steps, we are able to train a single set of shared weights on ImageNet and use these weights to obtain child models whose sizes range from 200 to 1000 MFLOPs. Our discovered model family, BigNASModels, achieve top-1 accuracies ranging from 76.5\% to 80.9\%, surpassing state-of-the-art models in this range including EfficientNets and Once-for-All networks without extra retraining or post-processing. We present ablative study and analysis to further understand the proposed BigNASModels.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yu-BigNAS-%20Scaling%20Up%20Neural%20Architecture%20Search%20with%20Big%20Single-Stage%20Models-2020-ECCV.pdf}
}
@article{AccurateGoyalarXiv2017, 
year = {2017}, 
title = {{Accurate, large minibatch SGD: Training ImageNet in 1 Hour}}, 
author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming}, 
journal = {arXiv}, 
eprint = {1706.02677}, 
abstract = {{Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves \textbackslashtextasciitilde90\% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Goyal-Accurate,%20large%20minibatch%20SGD-%20Training%20ImageNet%20in%201%20Hour-2017-arXiv.pdf}
}
@article{StabilizingChenICML2020, 
year = {2020}, 
title = {{Stabilizing differentiable architecture search via perturbation-based regularization}}, 
author = {Chen, Xiangning and Hsieh, Cho-Jui}, 
journal = {ICML}, 
eprint = {2002.05283}, 
abstract = {{Differentiable architecture search (DARTS) is a prevailing NAS solution to identify architectures. Based on the continuous relaxation of the architecture space, DARTS learns a differentiable architecture weight and largely reduces the search cost. However, its stability has been challenged for yielding deteriorating architectures as the search proceeds. We find that the precipitous validation loss landscape, which leads to a dramatic performance drop when distilling the final architecture, is an essential factor that causes instability. Based on this observation, we propose a perturbation-based regularization - SmoothDARTS (SDARTS), to smooth the loss landscape and improve the generalizability of DARTS-based methods. In particular, our new formulations stabilize DARTS-based methods by either random smoothing or adversarial attack. The search trajectory on NAS-Bench-1Shot1 demonstrates the effectiveness of our approach and due to the improved stability, we achieve performance gain across various search spaces on 4 datasets. Furthermore, we mathematically show that SDARTS implicitly regularizes the Hessian norm of the validation loss, which accounts for a smoother loss landscape and improved performance.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Stabilizing%20differentiable%20architecture%20search%20via%20perturbation-based%20regularization-2020-ICML.pdf}
}
@article{SGDRLoshchilovICLR2017, 
year = {2017}, 
title = {{SGDR: Stochastic Gradient Descent with Warm Restarts}}, 
author = {Loshchilov, Ilya and Hutter, Frank}, 
journal = {ICLR}, 
eprint = {1608.03983}, 
abstract = {{Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Loshchilov-SGDR-%20Stochastic%20Gradient%20Descent%20with%20Warm%20Restarts-2017-ICLR.pdf}
}
@article{CARSYangCVPR2020, 
year = {2020}, 
title = {{CARS: Continuous Evolution for Efficient Neural Architecture Search}}, 
author = {Yang, Zhaohui and Wang, Yunhe and Chen, Xinghao and Shi, Boxin and Xu, Chao and Xu, Chunjing and Tian, Qi and Xu, Chang}, 
journal = {CVPR}, 
eprint = {1909.04977}, 
abstract = {{Searching techniques in most of existing neural architecture search (NAS) algorithms are mainly dominated by differentiable methods for the efficiency reason. In contrast, we develop an efficient continuous evolutionary approach for searching neural networks. Architectures in the population that share parameters within one SuperNet in the latest generation will be tuned over the training dataset with a few epochs. The searching in the next evolution generation will directly inherit both the SuperNet and the population, which accelerates the optimal network generation. The non-dominated sorting strategy is further applied to preserve only results on the Pareto front for accurately updating the SuperNet. Several neural networks with different model sizes and performances will be produced after the continuous search with only 0.4 GPU days. As a result, our framework provides a series of networks with the number of parameters ranging from 3.7M to 5.1M under mobile settings. These networks surpass those produced by the state-of-the-art methods on the benchmark ImageNet dataset.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-CARS-%20Continuous%20Evolution%20for%20Efficient%20Neural%20Architecture%20Search-2020-CVPR.pdf}
}
@article{MixedMicikeviciusICLR2018, 
year = {2018}, 
title = {{Mixed precision training}}, 
author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao}, 
journal = {ICLR}, 
eprint = {1710.03740}, 
abstract = {{Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Micikevicius-Mixed%20precision%20training-2018-ICLR.pdf}
}
@article{SemiNASLuoNeurIPS2020, 
year = {2020}, 
title = {{Semi-supervised neural architecture search}}, 
author = {Luo, Renqian and Tan, Xu and Wang, Rui and Qin, Tao and Chen, Enhong and Liu, Tie-Yan}, 
journal = {NeurIPS}, 
eprint = {2002.10389}, 
abstract = {{Neural architecture search (NAS) relies on a good controller to generate better architectures or predict the accuracy of given architectures. However, training the controller requires both abundant and high-quality pairs of architectures and their accuracy, while it is costly to evaluate an architecture and obtain its accuracy. In this paper, we propose SemiNAS, a semi-supervised NAS approach that leverages numerous unlabeled architectures (without evaluation and thus nearly no cost) to improve the controller. Specifically, SemiNAS 1) trains an initial controller with a small set of architecture-accuracy data pairs; 2) uses the trained controller to predict the accuracy of large amount of architectures\textbackslashtextasciitilde(without evaluation); and 3) adds the generated data pairs to the original data to further improve the controller. SemiNAS has two advantages: 1) It reduces the computational cost under the same accuracy guarantee. 2) It achieves higher accuracy under the same computational cost. On NASBench-101 benchmark dataset, it discovers a top 0.01\% architecture after evaluating roughly 300 architectures, with only 1/7 computational cost compared with regularized evolution and gradient-based methods. On ImageNet, it achieves a state-of-the-art top-1 error rate of \$23.5\textbackslash\%\$ (under the mobile setting) using 4 GPU-days for search. We further apply it to LJSpeech text to speech task and it achieves 97\% intelligibility rate in the low-resource setting and 15\% test error rate in the robustness setting, with 9\%, 7\% improvements over the baseline respectively. Our code is available at https://github.com/renqianluo/SemiNAS.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Luo-Semi-supervised%20neural%20architecture%20search-2020-NeurIPS.pdf}
}
@article{NAOLuoNeurIPS2018, 
year = {2018}, 
title = {{Neural architecture optimization}}, 
author = {Luo, Renqian and Tian, Fei and Qin, Tao and Chen, Enhong and Liu, Tie-Yan}, 
journal = {NeurIPS}, 
eprint = {1808.07233}, 
abstract = {{Automatic neural architecture design has shown its potential in discovering powerful neural network architectures. Existing methods, no matter based on reinforcement learning or evolutionary algorithms (EA), conduct architecture search in a discrete space, which is highly inefficient. In this paper, we propose a simple and efficient method to automatic neural architecture design based on continuous optimization. We call this new approach neural architecture optimization (NAO). There are three key components in our proposed approach: (1) An encoder embeds/maps neural network architectures into a continuous space. (2) A predictor takes the continuous representation of a network as input and predicts its accuracy. (3) A decoder maps a continuous representation of a network back to its architecture. The performance predictor and the encoder enable us to perform gradient based optimization in the continuous space to find the embedding of a new architecture with potentially better accuracy. Such a better embedding is then decoded to a network by the decoder. Experiments show that the architecture discovered by our method is very competitive for image classification task on CIFAR-10 and language modeling task on PTB, outperforming or on par with the best results of previous architecture search methods with a significantly reduction of computational resources. Specifically we obtain 1.93\% test set error rate for CIFAR-10 image classification task and 56.0 test set perplexity of PTB language modeling task. Furthermore, combined with the recent proposed weight sharing mechanism, we discover powerful architecture on CIFAR-10 (with error rate 2.93\%) and on PTB (with test set perplexity 56.6), with very limited computational resources (less than 10 GPU hours) for both tasks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Luo-Neural%20architecture%20optimization-2018-NeurIPS.pdf}
}
@article{ResNetHeCVPR2016, 
year = {2016}, 
title = {{Deep residual learning for image recognition}}, 
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian}, 
journal = {CVPR}, 
abstract = {{Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers-8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions11http://image-net.org/challenges/LSVRC/2015/ and http://mscoco.org/dataset/\#detections-challenge2015., where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. http://image-net.org/challenges/LSVRC/2015/ and http://mscoco.org/dataset/\#detections-challenge2015.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/He-Deep%20residual%20learning%20for%20image%20recognition-2016-CVPR.pdf}
}
@article{GCNKipfICLR2017, 
year = {2017}, 
title = {{Semi-supervised classification with graph convolutional networks}}, 
author = {Kipf, Thomas N and Welling, Max}, 
journal = {ICLR}, 
eprint = {1609.02907}, 
abstract = {{We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kipf-Semi-supervised%20classification%20with%20graph%20convolutional%20networks-2017-ICLR.pdf}
}
@article{Distillationn-basedPhuongICCV2019, 
year = {2019}, 
title = {{Distillation-based training for multi-exit architectures}}, 
author = {Phuong, Mary and Lampert, Christoph H.}, 
journal = {ICCV}, 
abstract = {{Multi-exit architectures, in which a stack of processing layers is interleaved with early output layers, allow the processing of a test example to stop early and thus save computation time and/or energy. In this work, we propose a new training procedure for multi-exit architectures based on the principle of knowledge distillation. The method encourages early exits to mimic later, more accurate exits, by matching their output probabilities. Experiments on CIFAR100 and ImageNet show that distillation-based training significantly improves the accuracy of early exits while maintaining state-of-the-art accuracy for late ones. The method is particularly beneficial when training data is limited and it allows a straightforward extension to semi-supervised learning, i.e. making use of unlabeled data at training time. Moreover, it takes only a few lines to implement and incurs almost no computational overhead at training time, and none at all at test time.}}, 
pages = {1355--1364}, 
volume = {00}, 
keywords = {}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Phuong-Distillation-based%20training%20for%20multi-exit%20architectures-2019-ICCV.pdf}
}
@article{SearchLiuCVPR2020, 
year = {2020}, 
title = {{Search to distill: Pearls are Everywhere but not the Eyes}}, 
author = {Liu, Yu and Jia, Xuhui and Tan, Mingxing and Vemulapalli, Raviteja and Zhu, Yukun and Green, Bradley and Wang, Xiaogang}, 
journal = {CVPR}, 
abstract = {{Standard Knowledge Distillation (KD) approaches distill the knowledge of a cumbersome teacher model into the parameters of a student model with a pre-defined architecture. However, the knowledge of a neural network, which is represented by the network's output distribution conditioned on its input, depends not only on its parameters but also on its architecture. Hence, a more generalized approach for KD is to distill the teacher's knowledge into both the parameters and architecture of the student. To achieve this, we present a new Architecture-aware Knowledge Distillation (AKD) approach that finds student models (pearls for the teacher) that are best for distilling the given teacher model. In particular, we leverage Neural Architecture Search (NAS), equipped with our KD-guided reward, to search for the best student architectures for a given teacher. Experimental results show our proposed AKD consistently outperforms the conventional NAS plus KD approach, and achieves state-of-the-art results on the ImageNet classification task under various latency settings. Furthermore, the best AKD student architecture for the ImageNet classification task also transfers well to other tasks such as million level face recognition and ensemble learning.}}, 
pages = {7536--7545}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-Search%20to%20distill-%20Pearls%20are%20Everywhere%20but%20not%20the%20Eyes-2020-CVPR.pdf}
}
@article{FitNetsRomeroICLR2015, 
year = {2015}, 
title = {{FitNets: Hints for Thin Deep Nets}}, 
author = {Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua}, 
journal = {ICLR}, 
eprint = {1412.6550}, 
abstract = {{While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Romero-FitNets-%20Hints%20for%20Thin%20Deep%20Nets-2015-ICLR.pdf}
}
@article{AttentionTransferZagoruykoICLR2017, 
year = {2017}, 
title = {{Paying more attention to attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer}}, 
author = {Zagoruyko, Sergey and Komodakis, Nikos}, 
journal = {ICLR}, 
eprint = {1612.03928}, 
abstract = {{Attention plays a critical role in human visual experience. Furthermore, it has recently been demonstrated that attention can also play an important role in the context of applying artificial neural networks to a variety of tasks from fields such as computer vision and NLP. In this work we show that, by properly defining attention for convolutional neural networks, we can actually use this type of information in order to significantly improve the performance of a student CNN network by forcing it to mimic the attention maps of a powerful teacher network. To that end, we propose several novel methods of transferring attention, showing consistent improvement across a variety of datasets and convolutional neural network architectures. Code and models for our experiments are available at https://github.com/szagoruyko/attention-transfer}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zagoruyko-Paying%20more%20attention%20to%20attention-%20Improving%20the%20Performance%20of%20Convolutional%20Neural%20Networks%20via%20Attention%20Transfer-2017-ICLR.pdf}
}
@article{AutomaticallyHuTNNLS2019, 
year = {2019}, 
title = {{Automatically design convolutional neural networks by optimization with submodularity and supermodularity}}, 
author = {Hu, Wenzheng and Jin, Junqi and Liu, Tie-Yan and Zhang, Changshui}, 
journal = {TNNLS}, 
abstract = {{The architecture of convolutional neural networks (CNNs) is a key factor of influencing their performance. Although deep CNNs perform well in many difficult problems, how to intelligently design the architecture is still a challenging problem. Focusing on two practical architectural design problems: to maximize the accuracy with a given forward running time and to minimize the forward running time with a given accuracy requirement, we innovatively utilize prior knowledge to convert architecture optimization problems into submodular optimization problems. We propose efficient Greedy algorithms to solve them and give theoretical bounds of our algorithms. Specifically, we employ the techniques on some public data sets and compare our algorithms with some other hyperparameter optimization methods. Experiments show our algorithms’ efficiency.}}, 
pages = {3215--3229}, 
number = {9}, 
volume = {31}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hu-Automatically%20design%20convolutional%20neural%20networks%20by%20optimization%20with%20submodularity%20and%20supermodularity-2019-TNNLS.pdf}
}
@article{DMLZhangCVPR2018, 
year = {2018}, 
title = {{Deep mutual learning}}, 
author = {Zhang, Ying and Xiang, Tao and Hospedales, Timothy M. and Lu, Huchuan}, 
journal = {CVPR}, 
abstract = {{Model distillation is an effective and widely used technique to transfer knowledge from a teacher to a student network. The typical application is to transfer from a powerful large network or ensemble to a small network, in order to meet the low-memory or fast execution requirements. In this paper, we present a deep mutual learning (DML) strategy. Different from the one-way transfer between a static pre-defined teacher and a student in model distillation, with DML, an ensemble of students learn collaboratively and teach each other throughout the training process. Our experiments show that a variety of network architectures benefit from mutual learning and achieve compelling results on both category and instance recognition tasks. Surprisingly, it is revealed that no prior powerful teacher network is necessary mutual learning of a collection of simple student networks works, and moreover outperforms distillation from a more powerful yet static teacher.}}, 
pages = {4320--4328}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Deep%20mutual%20learning-2018-CVPR.pdf}
}
@article{NetworkSlimmingLiuICCV2017, 
year = {2017}, 
title = {{Learning efficient convolutional networks through network slimming}}, 
author = {Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui}, 
journal = {ICCV}, 
abstract = {{The deployment of deep convolutional neural networks (CNNs) in many real world applications is largely hindered by their high computational cost. In this paper, we propose a novel learning scheme for CNNs to simultaneously 1) reduce the model size; 2) decrease the run-time memory footprint; and 3) lower the number of computing operations, without compromising accuracy. This is achieved by enforcing channel-level sparsity in the network in a simple but effective way. Different from many existing approaches, the proposed method directly applies to modern CNN architectures, introduces minimum overhead to the training process, and requires no special software/hardware accelerators for the resulting models. We call our approach network slimming, which takes wide and large networks as input models, but during training insignificant channels are automatically identified and pruned afterwards, yielding thin and compact models with comparable accuracy. We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet and DenseNet, on various image classification datasets. For VGGNet, a multi-pass version of network slimming gives a 20× reduction in model size and a 5× reduction in computing operations.}}, 
pages = {2755--2763}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-Learning%20efficient%20convolutional%20networks%20through%20network%20slimming-2017-ICCV.pdf}
}
@article{BagHeCVPR2019, 
year = {2019}, 
title = {{Bag of tricks for image classification with convolutional neural networks}}, 
author = {He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu}, 
journal = {CVPR}, 
abstract = {{Much of the recent progress made in image classification research can be credited to training procedure refinements, such as changes in data augmentations and optimization methods. In the literature, however, most refinements are either briefly mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such refinements and empirically evaluate their impact on the final model accuracy through ablation study. We will show that, by combining these refinements together, we are able to improve various CNN models significantly. For example, we raise ResNet-50’s top-1 validation accuracy from 75.3\% to 79.29\% on ImageNet. We will also demonstrate that improvement on image classification accuracy leads to better transfer learning performance in other application domains such as object detection and semantic segmentation.}}, 
pages = {558--567}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/He-Bag%20of%20tricks%20for%20image%20classification%20with%20convolutional%20neural%20networks-2019-CVPR.pdf}
}
@article{Once-for-AllCaiICLR2020, 
year = {2020}, 
title = {{Once-for-All: Train One Network and Specialize it for Efficient Deployment}}, 
author = {Cai, Han and Gan, Chuang and Wang, Tianzhe and Zhang, Zhekai and Han, Song}, 
journal = {ICLR}, 
eprint = {1908.09791}, 
abstract = {{We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices. Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing \$CO\_2\$ emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks (\$> 10\textasciicircum\{19\}\$) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0\% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and \$CO\_2\$ emission. In particular, OFA achieves a new SOTA 80.0\% ImageNet top-1 accuracy under the mobile setting (\$<\$600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and 50 pre-trained models (for many devices \& many latency constraints) are released at https://github.com/mit-han-lab/once-for-all.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Cai-Once-for-All-%20Train%20One%20Network%20and%20Specialize%20it%20for%20Efficient%20Deployment-2020-ICLR.pdf}
}
@article{PDARTSChenICCV2019, 
year = {2019}, 
title = {{Progressive differentiable architecture search: Bridging the Depth Gap between Search and Evaluation}}, 
author = {Chen, Xin and Xie, Lingxi and Wu, Jun and Tian, Qi}, 
journal = {ICCV}, 
abstract = {{Recently, differentiable search methods have made major progress in reducing the computational costs of neural architecture search. However, these approaches often report lower accuracy in evaluating the searched architecture or transferring it to another dataset. This is arguably due to the large gap between the architecture depths in search and evaluation scenarios. In this paper, we present an efficient algorithm which allows the depth of searched architectures to grow gradually during the training procedure. This brings two issues, namely, heavier computational overheads and weaker search stability, which we solve using search space approximation and regularization, respectively. With a significantly reduced search time (\$\textbackslashsim\$7 hours on a single GPU), our approach achieves state-of-the-art performance on both the proxy dataset (CIFAR10 or CIFAR100) and the target dataset (ImageNet). Code is available at https://github.com/chenxin061/pdarts}}, 
pages = {1294--1303}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Progressive%20differentiable%20architecture%20search-%20Bridging%20the%20Depth%20Gap%20between%20Search%20and%20Evaluation-2019-ICCV.pdf}
}
@article{DARTS+LiangarXiv2019, 
year = {2019}, 
title = {{DARTS+: Improved Differentiable Architecture Search with Early Stopping}}, 
author = {Liang, Hanwen and Zhang, Shifeng and Sun, Jiacheng and He, Xingqiu and Huang, Weiran and Zhuang, Kechen and Li, Zhenguo}, 
journal = {arXiv}, 
eprint = {1909.06035}, 
abstract = {{Recently, there has been a growing interest in automating the process of neural architecture design, and the Differentiable Architecture Search (DARTS) method makes the process available within a few GPU days. However, the performance of DARTS is often observed to collapse when the number of search epochs becomes large. Meanwhile, lots of "\{\textbackslashem skip-connect\}s" are found in the selected architectures. In this paper, we claim that the cause of the collapse is that there exists overfitting in the optimization of DARTS. Therefore, we propose a simple and effective algorithm, named "DARTS+", to avoid the collapse and improve the original DARTS, by "early stopping" the search procedure when meeting a certain criterion. We also conduct comprehensive experiments on benchmark datasets and different search spaces and show the effectiveness of our DARTS+ algorithm, and DARTS+ achieves \$2.32\textbackslash\%\$ test error on CIFAR10, \$14.87\textbackslash\%\$ on CIFAR100, and \$23.7\textbackslash\%\$ on ImageNet. We further remark that the idea of "early stopping" is implicitly included in some existing DARTS variants by manually setting a small number of search epochs, while we give an \{\textbackslashem explicit\} criterion for "early stopping".}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liang-DARTS+-%20Improved%20Differentiable%20Architecture%20Search%20with%20Early%20Stopping-2019-arXiv.pdf}
}
@article{PC-DARTSXuICLR2020, 
year = {2020}, 
title = {{PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search}}, 
author = {Xu, Yuhui and Xie, Lingxi and Zhang, Xiaopeng and Chen, Xin and Qi, Guo-Jun and Tian, Qi and Xiong, Hongkai}, 
journal = {ICLR}, 
eprint = {1907.05737}, 
abstract = {{Differentiable architecture search (DARTS) provided a fast solution in finding effective network architectures, but suffered from large memory and computing overheads in jointly training a super-network and searching for an optimal architecture. In this paper, we present a novel approach, namely, Partially-Connected DARTS, by sampling a small part of super-network to reduce the redundancy in exploring the network space, thereby performing a more efficient search without comprising the performance. In particular, we perform operation search in a subset of channels while bypassing the held out part in a shortcut. This strategy may suffer from an undesired inconsistency on selecting the edges of super-net caused by sampling different channels. We alleviate it using edge normalization, which adds a new set of edge-level parameters to reduce uncertainty in search. Thanks to the reduced memory cost, PC-DARTS can be trained with a larger batch size and, consequently, enjoys both faster speed and higher training stability. Experimental results demonstrate the effectiveness of the proposed method. Specifically, we achieve an error rate of 2.57\% on CIFAR10 with merely 0.1 GPU-days for architecture search, and a state-of-the-art top-1 error rate of 24.2\% on ImageNet (under the mobile setting) using 3.8 GPU-days for search. Our code has been made available at: https://github.com/yuhuixu1993/PC-DARTS.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xu-PC-DARTS-%20Partial%20Channel%20Connections%20for%20Memory-Efficient%20Architecture%20Search-2020-ICLR.pdf}
}
@article{FairDARTSChuECCV2020, 
year = {2020}, 
title = {{Fair DARTS: Eliminating Unfair Advantages in Differentiable Architecture Search}}, 
author = {Chu, Xiangxiang and Zhou, Tianbao and Zhang, Bo and Li, Jixiang}, 
journal = {ECCV}, 
eprint = {1911.12126}, 
abstract = {{Differentiable Architecture Search (DARTS) is now a widely disseminated weight-sharing neural architecture search method. However, it suffers from well-known performance collapse due to an inevitable aggregation of skip connections. In this paper, we first disclose that its root cause lies in an unfair advantage in exclusive competition. Through experiments, we show that if either of two conditions is broken, the collapse disappears. Thereby, we present a novel approach called Fair DARTS where the exclusive competition is relaxed to be collaborative. Specifically, we let each operation's architectural weight be independent of others. Yet there is still an important issue of discretization discrepancy. We then propose a zero-one loss to push architectural weights towards zero or one, which approximates an expected multi-hot solution. Our experiments are performed on two mainstream search spaces, and we derive new state-of-the-art results on CIFAR-10 and ImageNet. Our code is available on https://github.com/xiaomi-automl/fairdarts .}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chu-Fair%20DARTS-%20Eliminating%20Unfair%20Advantages%20in%20Differentiable%20Architecture%20Search-2020-ECCV.pdf}
}
@article{FairNASChuICCV2021, 
year = {2021}, 
title = {{FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search}}, 
author = {Chu, Xiangxiang and Zhang, Bo and Xu, Ruijun}, 
journal = {ICCV}, 
eprint = {1907.01845}, 
abstract = {{One of the most critical problems in two-stage weight-sharing neural architecture search is the evaluation of candidate models. A faithful ranking certainly leads to accurate searching results. However, current methods are prone to making misjudgments. In this paper, we prove that they inevitably give biased evaluations due to inherent unfairness in the supernet training. In view of this, we propose two levels of constraints: expectation fairness and strict fairness. Particularly, strict fairness ensures equal optimization opportunities for all choice blocks throughout the training, which neither overestimates nor underestimates their capacity. We demonstrate this is crucial to improving confidence in models' ranking. Incorporating our supernet trained under fairness constraints with a multi-objective evolutionary search algorithm, we obtain various state-of-the-art models on ImageNet. Especially, FairNAS-A attains 77.5\% top-1 accuracy. The models and their evaluation codes are made publicly available online http://github.com/fairnas/FairNAS .}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chu-FairNAS-%20Rethinking%20Evaluation%20Fairness%20of%20Weight%20Sharing%20Neural%20Architecture%20Search-2021-ICCV.pdf}
}
@article{SNASXieICLR2019, 
year = {2019}, 
title = {{SNAS: Stochastic Neural Architecture Search}}, 
author = {Xie, Sirui and Zheng, Hehui and Liu, Chunxiao and Lin, Liang}, 
journal = {ICLR}, 
eprint = {1812.09926}, 
abstract = {{We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets. We have released our implementation at https://github.com/SNAS-Series/SNAS-Series.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xie-SNAS-%20Stochastic%20Neural%20Architecture%20Search-2019-ICLR.pdf}
}
@article{UnderstandingZelaICLR2020, 
year = {2020}, 
title = {{Understanding and robustifying differentiable architecture search}}, 
author = {Zela, Arber and Elsken, Thomas and Saikia, Tonmoy and Marrakchi, Yassine and Brox, Thomas and Hutter, Frank}, 
journal = {ICLR}, 
eprint = {1909.09656}, 
abstract = {{Differentiable Architecture Search (DARTS) has attracted a lot of attention due to its simplicity and small search costs achieved by a continuous relaxation and an approximation of the resulting bi-level optimization problem. However, DARTS does not work robustly for new problems: we identify a wide range of search spaces for which DARTS yields degenerate architectures with very poor test performance. We study this failure mode and show that, while DARTS successfully minimizes validation loss, the found solutions generalize poorly when they coincide with high validation loss curvature in the architecture space. We show that by adding one of various types of regularization we can robustify DARTS to find solutions with less curvature and better generalization properties. Based on these observations, we propose several simple variations of DARTS that perform substantially more robustly in practice. Our observations are robust across five search spaces on three image classification tasks and also hold for the very different domains of disparity estimation (a dense regression task) and language modelling.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zela-Understanding%20and%20robustifying%20differentiable%20architecture%20search-2020-ICLR.pdf}
}
@article{FCNLongCVPR2015, 
year = {2015}, 
title = {{Fully convolutional networks for semantic segmentation}}, 
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor}, 
journal = {CVPR}, 
eprint = {1411.4038}, 
abstract = {{Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Long-Fully%20convolutional%20networks%20for%20semantic%20segmentation-2015-CVPR.pdf}
}
@article{U-NetRonnebergerMICCAI2015, 
year = {2015}, 
title = {{U-Net: Convolutional Networks for Biomedical Image Segmentation}}, 
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas}, 
journal = {MICCAI}, 
eprint = {1505.04597}, 
abstract = {{There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ronneberger-U-Net-%20Convolutional%20Networks%20for%20Biomedical%20Image%20Segmentation-2015-MICCAI.pdf}
}
@article{GNWuECCV2018, 
year = {2018}, 
title = {{Group normalization}}, 
author = {Wu, Yuxin and He, Kaiming}, 
journal = {ECCV}, 
abstract = {{Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6\% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wu-Group%20normalization-2018-ECCV.pdf}
}
@article{DMCPGuoCVPR2020, 
year = {2020}, 
title = {{DMCP: Differentiable Markov Channel Pruning for Neural Networks}}, 
author = {Guo, Shaopeng and Wang, Yujie and Li, Quanquan and Yan, Junjie}, 
journal = {CVPR}, 
abstract = {{Recent works imply that the channel pruning can be regarded as searching optimal sub-structure from unpruned networks. However, existing works based on this observation require training and evaluating a large number of structures, which limits their application. In this paper, we propose a novel differentiable method for channel pruning, named Differentiable Markov Channel Pruning (DMCP), to efficiently search the optimal sub-structure. Our method is differentiable and can be directly optimized by gradient descent with respect to standard task loss and budget regularization (e.g. FLOPs constraint). In DMCP, we model the channel pruning as a Markov process, in which each state represents for retaining the corresponding channel during pruning, and transitions between states denote the pruning process. In the end, our method is able to implicitly select the proper number of channels in each layer by the Markov process with optimized transitions. To validate the effectiveness of our method, we perform extensive experiments on ImageNet with ResNet and MobilenetV2. Results show our method can achieve consistent improvement than state-of-the-art pruning methods in various FLOPs settings.}}, 
pages = {1536--1544}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Guo-DMCP-%20Differentiable%20Markov%20Channel%20Pruning%20for%20Neural%20Networks-2020-CVPR.pdf}
}
@article{VAEKingmaICLR2014, 
year = {2014}, 
title = {{Auto-encoding variational Bayes}}, 
author = {Kingma, Diederik P and Welling, Max}, 
journal = {ICLR}, 
eprint = {1312.6114}, 
abstract = {{How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kingma-Auto-encoding%20variational%20Bayes-2014-ICLR.pdf}
}
@article{LogPartitionWainwrightTIT2005, 
year = {2005}, 
title = {{A new class of upper bounds on the log partition function}}, 
author = {Wainwright, Martin J. and Jaakkola, Tommi S. and Willsky, Alan S.}, 
journal = {TIT}, 
issn = {0018-9448}, 
abstract = {{We introduce a new class of upper bounds on the log partition function of a Markov random field (MRF). This quantity plays an important role in various contexts, including approximating marginal distributions, parameter estimation, combinatorial enumeration, statistical decision theory, and large-deviations bounds. Our derivation is based on concepts from convex duality and information geometry: in particular, it exploits mixtures of distributions in the exponential domain, and the Legendre mapping between exponential and mean parameters. In the special case of convex combinations of tree-structured distributions, we obtain a family of variational problems, similar to the Bethe variational problem, but distinguished by the following desirable properties: i) they are convex, and have a unique global optimum; and ii) the optimum gives an upper bound on the log partition function. This optimum is defined by stationary conditions very similar to those defining fixed points of the sum–product algorithm, or more generally, any local optimum of the Bethe variational problem. As with sum–product fixed points, the elements of the optimizing argument can be used as approximations to the marginals of the original model. The analysis extends naturally to convex combinations of hypertree-structured distributions, thereby establishing links to Kikuchi approximations and variants.}}, 
pages = {2313--2335}, 
number = {7}, 
volume = {51}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wainwright-A%20new%20class%20of%20upper%20bounds%20on%20the%20log%20partition%20function-2005-TIT.pdf}
}
@article{TRBPWainwrightAISTATS2003, 
year = {2003}, 
title = {{Tree-reweighted belief propagation algorithms and approximate ML estimation by pseudo-moment matching}}, 
author = {Wainwright, Martin J. and Jaakkola, Tommi S. and Willsky, Alan S.}, 
journal = {AISTATS}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wainwright-Tree-reweighted%20belief%20propagation%20algorithms%20and%20approximate%20ML%20estimation%20by%20pseudo-moment%20matching-2003-AISTATS.pdf}
}
@article{TRMPKolmogorovTPAMI2006, 
year = {2006}, 
title = {{Convergent tree-reweighted message passing for energy minimization}}, 
author = {Kolmogorov, Vladimir}, 
journal = {TPAMI}, 
issn = {0162-8828}, 
pmid = {16986540}, 
abstract = {{Algorithms for discrete energy minimization are of fundamental importance in computer vision. In this paper, we focus on the recent technique proposed by Wainwright et al. [33]-tree-reweighted max-product message passing (TRW). It was inspired by the problem of maximizing a lower bound on the energy. However, the algorithm is not guaranteed to increase this bound―it may actually go down. In addition, TRW does not always converge. We develop a modification of this algorithm which we call sequential tree-reweighted message passing. Its main property is that the bound is guaranteed not to decrease. We also give a weak tree agreement condition which characterizes local maxima of the bound with respect to TRW algorithms. We prove that our algorithm has a limit point that achieves weak tree agreement. Finally, we show that, our algorithm requires half as much memory as traditional message passing approaches. Experimental results demonstrate that on certain synthetic and real problems, our algorithm outperforms both the ordinary belief propagation and tree-reweighted algorithm in [33]. In addition, on stereo problems with Potts interactions, we obtain a lower energy than graph cuts.}}, 
pages = {1568--1583}, 
number = {10}, 
volume = {28}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kolmogorov-Convergent%20tree-reweighted%20message%20passing%20for%20energy%20minimization-2006-TPAMI.pdf}
}
@article{CNFSaxenaNeurIPS2016, 
year = {2016}, 
title = {{Convolutional neural fabrics}}, 
author = {Saxena, Shreyas and Verbeek, Jakob}, 
journal = {NeurIPS}, 
eprint = {1606.02492}, 
abstract = {{Despite the success of CNNs, selecting the optimal architecture for a given task remains an open problem. Instead of aiming to select a single optimal architecture, we propose a "fabric" that embeds an exponentially large number of architectures. The fabric consists of a 3D trellis that connects response maps at different layers, scales, and channels with a sparse homogeneous local connectivity pattern. The only hyper-parameters of a fabric are the number of channels and layers. While individual architectures can be recovered as paths, the fabric can in addition ensemble all embedded architectures together, sharing their weights where their paths overlap. Parameters can be learned using standard methods based on back-propagation, at a cost that scales linearly in the fabric size. We present benchmark results competitive with the state of the art for image classification on MNIST and CIFAR10, and for semantic segmentation on the Part Labels dataset.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Saxena-Convolutional%20neural%20fabrics-2016-NeurIPS.pdf}
}
@article{PNASLiuECCV2018, 
year = {2018}, 
title = {{Progressive neural architecture search}}, 
author = {Liu, Chenxi and Zoph, Barret and Neumann, Maxim and Shlens, Jonathon and Hua, Wei and Li, Li-Jia and Fei-Fei, Li and Yuille, Alan and Huang, Jonathan and Murphy, Kevin}, 
journal = {ECCV}, 
eprint = {1712.00559}, 
abstract = {{We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space. Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al. (2018) in terms of number of models evaluated, and 8 times faster in terms of total compute. The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-Progressive%20neural%20architecture%20search-2018-ECCV.pdf}
}
@article{Self-trainingXieCVPR2020, 
year = {2020}, 
title = {{Self-training with noisy student improves ImageNet classification}}, 
author = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V.}, 
journal = {CVPR}, 
abstract = {{We present a simple self-training method that achieves 88.4\% top-1 accuracy on ImageNet, which is 2.0\% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0\% to 83.7\%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. To achieve this result, we first train an EfficientNet model on labeled ImageNet images and use it as a teacher to generate pseudo labels on 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the generation of the pseudo labels, the teacher is not noised so that the pseudo labels are as accurate as possible. However, during the learning of the student, we inject noise such as dropout, stochastic depth and data augmentation via RandAugment to the student so that the student generalizes better than the teacher.11Models are available at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet. Code is available at https://github.com/google-research/noisystudent. Models are available at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet. Code is available at https://github.com/google-research/noisystudent.}}, 
keywords = {}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xie-Self-training%20with%20noisy%20student%20improves%20ImageNet%20classification-2020-CVPR.pdf}
}
@article{EfficienntCaiAAAI2018, 
year = {2018}, 
title = {{Efficient architecture search by network transformation}}, 
author = {Cai, Han and Chen, Tianyao and Zhang, Weinan and Yu, Yong and Wang, Jun}, 
journal = {AAAI}, 
eprint = {1707.04873}, 
abstract = {{Techniques for automatically designing deep neural network architectures such as reinforcement learning based approaches have recently shown promising results. However, their success is based on vast computational resources (e.g. hundreds of GPUs), making them difficult to be widely used. A noticeable limitation is that they still design and train each network from scratch during the exploration of the architecture space, which is highly inefficient. In this paper, we propose a new framework toward efficient architecture search by exploring the architecture space based on the current network and reusing its weights. We employ a reinforcement learning agent as the meta-controller, whose action is to grow the network depth or layer width with function-preserving transformations. As such, the previously validated networks can be reused for further exploration, thus saves a large amount of computational cost. We apply our method to explore the architecture space of the plain convolutional neural networks (no skip-connections, branching etc.) on image benchmark datasets (CIFAR-10, SVHN) with restricted computational resources (5 GPUs). Our method can design highly competitive networks that outperform existing networks using the same design scheme. On CIFAR-10, our model without skip-connections achieves 4.23\textbackslash\% test error rate, exceeding a vast majority of modern architectures and approaching DenseNet. Furthermore, by applying our method to explore the DenseNet architecture space, we are able to achieve more accurate networks with fewer parameters.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Cai-Efficient%20architecture%20search%20by%20network%20transformation-2018-AAAI.pdf}
}
@article{FastFangICLR2020, 
year = {2020}, 
title = {{Fast neural network adaptation via parameter remapping and architecture search}}, 
author = {Fang, Jiemin and Sun, Yuzhu and Peng, Kangjian and Zhang, Qian and Li, Yuan and Liu, Wenyu and Wang, Xinggang}, 
journal = {ICLR}, 
eprint = {2001.02525}, 
abstract = {{Deep neural networks achieve remarkable performance in many computer vision tasks. Most state-of-the-art (SOTA) semantic segmentation and object detection approaches reuse neural network architectures designed for image classification as the backbone, commonly pre-trained on ImageNet. However, performance gains can be achieved by designing network architectures specifically for detection and segmentation, as shown by recent neural architecture search (NAS) research for detection and segmentation. One major challenge though, is that ImageNet pre-training of the search space representation (a.k.a. super network) or the searched networks incurs huge computational cost. In this paper, we propose a Fast Neural Network Adaptation (FNA) method, which can adapt both the architecture and parameters of a seed network (e.g. a high performing manually designed backbone) to become a network with different depth, width, or kernels via a Parameter Remapping technique, making it possible to utilize NAS for detection/segmentation tasks a lot more efficiently. In our experiments, we conduct FNA on MobileNetV2 to obtain new networks for both segmentation and detection that clearly out-perform existing networks designed both manually and by NAS. The total computation cost of FNA is significantly less than SOTA segmentation/detection NAS approaches: 1737\$\textbackslashtimes\$ less than DPC, 6.8\$\textbackslashtimes\$ less than Auto-DeepLab and 7.4\$\textbackslashtimes\$ less than DetNAS. The code is available at https://github.com/JaminFong/FNA.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Fang-Fast%20neural%20network%20adaptation%20via%20parameter%20remapping%20and%20architecture%20search-2020-ICLR.pdf}
}
@article{AtomNASMeiICLR2020, 
year = {2020}, 
title = {{AtomNAS: Fine-Grained End-to-End Neural Architecture Search}}, 
author = {Mei, Jieru and Li, Yingwei and Lian, Xiaochen and Jin, Xiaojie and Yang, Linjie and Yuille, Alan and Yang, Jianchao}, 
journal = {ICLR}, 
eprint = {1912.09640}, 
abstract = {{Search space design is very critical to neural architecture search (NAS) algorithms. We propose a fine-grained search space comprised of atomic blocks, a minimal search unit that is much smaller than the ones used in recent NAS algorithms. This search space allows a mix of operations by composing different types of atomic blocks, while the search space in previous methods only allows homogeneous operations. Based on this search space, we propose a resource-aware architecture search framework which automatically assigns the computational resources (e.g., output channel numbers) for each operation by jointly considering the performance and the computational cost. In addition, to accelerate the search process, we propose a dynamic network shrinkage technique which prunes the atomic blocks with negligible influence on outputs on the fly. Instead of a search-and-retrain two-stage paradigm, our method simultaneously searches and trains the target architecture. Our method achieves state-of-the-art performance under several FLOPs configurations on ImageNet with a small searching cost. We open our entire codebase at: https://github.com/meijieru/AtomNAS.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Mei-AtomNAS-%20Fine-Grained%20End-to-End%20Neural%20Architecture%20Search-2020-ICLR.pdf}
}
@article{EvaluatingYuICLR2020, 
year = {2020}, 
title = {{Evaluating the search phase of neural architecture search}}, 
author = {Yu, Kaicheng and Sciuto, Christian and Jaggi, Martin and Musat, Claudiu and Salzmann, Mathieu}, 
journal = {ICLR}, 
eprint = {1902.08142}, 
abstract = {{Neural Architecture Search (NAS) aims to facilitate the design of deep networks for new tasks. Existing techniques rely on two stages: searching over the architecture space and validating the best architecture. NAS algorithms are currently compared solely based on their results on the downstream task. While intuitive, this fails to explicitly evaluate the effectiveness of their search strategies. In this paper, we propose to evaluate the NAS search phase. To this end, we compare the quality of the solutions obtained by NAS search policies with that of random architecture selection. We find that: (i) On average, the state-of-the-art NAS algorithms perform similarly to the random policy; (ii) the widely-used weight sharing strategy degrades the ranking of the NAS candidates to the point of not reflecting their true performance, thus reducing the effectiveness of the search process. We believe that our evaluation framework will be key to designing NAS strategies that consistently discover architectures superior to random ones.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yu-Evaluating%20the%20search%20phase%20of%20neural%20architecture%20search-2020-ICLR.pdf}
}
@article{NASYangICLR2020, 
year = {2020}, 
title = {{NAS evaluation is frustratingly hard}}, 
author = {Yang, Antoine and Esperança, Pedro M and Carlucci, Fabio M}, 
journal = {ICLR}, 
eprint = {1912.12522}, 
abstract = {{Neural Architecture Search (NAS) is an exciting new field which promises to be as much as a game-changer as Convolutional Neural Networks were in 2012. Despite many great works leading to substantial improvements on a variety of tasks, comparison between different methods is still very much an open issue. While most algorithms are tested on the same datasets, there is no shared experimental protocol followed by all. As such, and due to the under-use of ablation studies, there is a lack of clarity regarding why certain methods are more effective than others. Our first contribution is a benchmark of \$8\$ NAS methods on \$5\$ datasets. To overcome the hurdle of comparing methods with different search spaces, we propose using a method's relative improvement over the randomly sampled average architecture, which effectively removes advantages arising from expertly engineered search spaces or training protocols. Surprisingly, we find that many NAS techniques struggle to significantly beat the average architecture baseline. We perform further experiments with the commonly used DARTS search space in order to understand the contribution of each component in the NAS pipeline. These experiments highlight that: (i) the use of tricks in the evaluation protocol has a predominant impact on the reported performance of architectures; (ii) the cell-based search space has a very narrow accuracy range, such that the seed has a considerable impact on architecture rankings; (iii) the hand-designed macro-structure (cells) is more important than the searched micro-structure (operations); and (iv) the depth-gap is a real phenomenon, evidenced by the change in rankings between \$8\$ and \$20\$ cell architectures. To conclude, we suggest best practices, that we hope will prove useful for the community and help mitigate current NAS pitfalls. The code used is available at https://github.com/antoyang/NAS-Benchmark.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-NAS%20evaluation%20is%20frustratingly%20hard-2020-ICLR.pdf}
}
@article{UnderstandingShuICLR2020, 
year = {2020}, 
title = {{Understanding architectures learnt by cell-based neural architecture search}}, 
author = {Shu, Yao and Wang, Wei and Cai, Shaofeng}, 
journal = {ICLR}, 
eprint = {1909.09569}, 
abstract = {{Neural architecture search (NAS) searches architectures automatically for given tasks, e.g., image classification and language modeling. Improving the search efficiency and effectiveness have attracted increasing attention in recent years. However, few efforts have been devoted to understanding the generated architectures. In this paper, we first reveal that existing NAS algorithms (e.g., DARTS, ENAS) tend to favor architectures with wide and shallow cell structures. These favorable architectures consistently achieve fast convergence and are consequently selected by NAS algorithms. Our empirical and theoretical study further confirms that their fast convergence derives from their smooth loss landscape and accurate gradient information. Nonetheless, these architectures may not necessarily lead to better generalization performance compared with other candidate architectures in the same search space, and therefore further improvement is possible by revising existing NAS algorithms.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Shu-Understanding%20architectures%20learnt%20by%20cell-based%20neural%20architecture%20search-2020-ICLR.pdf}
}
@article{DeepLabChenTPAMI2017, 
year = {2017}, 
title = {{DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs}}, 
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.}, 
journal = {TPAMI}, 
issn = {0162-8828}, 
pmid = {28463186}, 
abstract = {{In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or atrousconvolution, as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrousspatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed DeepLab system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.}}, 
pages = {834--848}, 
number = {4}, 
volume = {40}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-DeepLab-%20Semantic%20Image%20Segmentation%20with%20Deep%20Convolutional%20Nets,%20Atrous%20Convolution,%20and%20Fully%20Connected%20CRFs-2017-TPAMI.pdf}
}
@article{Auto-DeepLabLiuCVPR2019, 
year = {2019}, 
title = {{Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation}}, 
author = {Liu, Chenxi and Chen, Liang-Chieh and Schroff, Florian and Adam, Hartwig and Hua, Wei and Yuille, Alan and Fei-Fei, Li}, 
journal = {CVPR}, 
abstract = {{Recently, Neural Architecture Search (NAS) has successfully identified neural network architectures that exceed human designed ones on large-scale image classification. In this paper, we study NAS for semantic image segmentation. Existing works often focus on searching the repeatable cell structure, while hand-designing the outer network structure that controls the spatial resolution changes. This choice simplifies the search space, but becomes increasingly problematic for dense image prediction which exhibits a lot more network level architectural variations. Therefore, we propose to search the network level structure in addition to the cell level structure, which forms a hierarchical architecture search space. We present a network level search space that includes many popular designs, and develop a formulation that allows efficient gradient-based architecture search (3 P100 GPU days on Cityscapes images). We demonstrate the effectiveness of the proposed method on the challenging Cityscapes, PASCAL VOC 2012, and ADE20K datasets. Auto-DeepLab, our architecture searched specifically for semantic image segmentation, attains state-of-the-art performance without any ImageNet pretraining.1}}, 
keywords = {}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-Auto-DeepLab-%20Hierarchical%20Neural%20Architecture%20Search%20for%20Semantic%20Image%20Segmentation-2019-CVPR.pdf}
}
@article{DeepLabChenICLR2015, 
year = {2015}, 
title = {{Semantic image segmentation with deep convolutional nets and fully connected CRFs}}, 
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L}, 
journal = {ICLR}, 
eprint = {1412.7062}, 
abstract = {{Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called "semantic image segmentation"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our "DeepLab" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6\% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Semantic%20image%20segmentation%20with%20deep%20convolutional%20nets%20and%20fully%20connected%20CRFs-2015-ICLR.pdf}
}
@article{CRFZhengICCV2015, 
year = {2015}, 
title = {{Conditional random fields as recurrent neural networks}}, 
author = {Zheng, Shuai and Jayasumana, Sadeep and Romera-Paredes, Bernardino and Vineet, Vibhav and Su, Zhizhong and Du, Dalong and Huang, Chang and Torr, Philip H. S.}, 
journal = {ICCV}, 
eprint = {1502.03240}, 
abstract = {{Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate Conditional Random Fields with Gaussian pairwise potentials and mean-field approximate inference as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our systemfully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation. We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.}}, 
pages = {1529--1537}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zheng-Conditional%20random%20fields%20as%20recurrent%20neural%20networks-2015-ICCV.pdf}
}
@article{ExploringXieICCV2019, 
year = {2019}, 
title = {{Exploring randomly wired neural networks for image recognition}}, 
author = {Xie, Saining and Kirillov, Alexander and Girshick, Ross and He, Kaiming}, 
journal = {ICCV}, 
abstract = {{Neural networks for image recognition have evolved through extensive manual design from simple chain-like models to structures with multiple wiring paths. The success of ResNets [12] and DenseNets [17] is due in large part to their innovative wiring plans. Now, neural architecture search (NAS) studies are exploring the joint optimization of wiring and operation types, however, the space of possible wirings is constrained and still driven by manual design despite being searched. In this paper, we explore a more diverse set of connectivity patterns through the lens of randomly wired neural networks. To do this, we first define the concept of a stochastic network generator that encapsulates the entire network generation process. Encapsulation provides a unified view of NAS and randomly wired networks. Then, we use three classical random graph models to generate randomly wired graphs for networks. The results are surprising: several variants of these random generators yield network instances that have competitive accuracy on the ImageNet benchmark. These results suggest that new efforts focusing on designing better network generators may lead to new breakthroughs by exploring less constrained search spaces with more room for novel design. The code is publicly available online1 1https://github.com/facebookresearch/RandWire https://github.com/facebookresearch/RandWire}}, 
pages = {1284--1293}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xie-Exploring%20randomly%20wired%20neural%20networks%20for%20image%20recognition-2019-ICCV.pdf}
}
@article{DeepLabV3ChenarXiv2017, 
year = {2017}, 
title = {{Rethinking Atrous convolution for semantic image segmentation}}, 
author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig}, 
journal = {arXiv}, 
eprint = {1706.05587}, 
abstract = {{In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Rethinking%20Atrous%20convolution%20for%20semantic%20image%20segmentation-2017-arXiv.pdf}
}
@article{DeepLabV3+ChenECCV2018, 
year = {2018}, 
title = {{Encoder-decoder with Atrous separable convolution for semantic image segmentation}}, 
author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig}, 
journal = {ECCV}, 
eprint = {1802.02611}, 
abstract = {{Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0\textbackslash\% and 82.1\textbackslash\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at \textbackslashurl\{https://github.com/tensorflow/models/tree/master/research/deeplab\}.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Encoder-decoder%20with%20Atrous%20separable%20convolution%20for%20semantic%20image%20segmentation-2018-ECCV.pdf}
}
@article{SearchingChenNeurIPS2018, 
year = {2018}, 
title = {{Searching for efficient multi-scale architectures for dense image prediction}}, 
author = {Chen, Liang-Chieh and Collins, Maxwell D and Zhu, Yukun and Papandreou, George and Zoph, Barret and Schroff, Florian and Adam, Hartwig and Shlens, Jonathon}, 
journal = {NeurIPS}, 
eprint = {1809.04184}, 
abstract = {{The design of neural network architectures is an important component for achieving state-of-the-art performance with machine learning systems across a broad array of tasks. Much work has endeavored to design and build architectures automatically through clever construction of a search space paired with simple learning algorithms. Recent progress has demonstrated that such meta-learning methods may exceed scalable human-invented architectures on image classification tasks. An open question is the degree to which such methods may generalize to new domains. In this work we explore the construction of meta-learning techniques for dense image prediction focused on the tasks of scene parsing, person-part segmentation, and semantic image segmentation. Constructing viable search spaces in this domain is challenging because of the multi-scale representation of visual information and the necessity to operate on high resolution imagery. Based on a survey of techniques in dense image prediction, we construct a recursive search space and demonstrate that even with efficient random search, we can identify architectures that outperform human-invented architectures and achieve state-of-the-art performance on three dense prediction tasks including 82.7\textbackslash\% on Cityscapes (street scene parsing), 71.3\textbackslash\% on PASCAL-Person-Part (person-part segmentation), and 87.9\textbackslash\% on PASCAL VOC 2012 (semantic image segmentation). Additionally, the resulting architecture is more computationally efficient, requiring half the parameters and half the computational cost as previous state of the art systems.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Searching%20for%20efficient%20multi-scale%20architectures%20for%20dense%20image%20prediction-2018-NeurIPS.pdf}
}
@article{NAS-UnetWengAccess2019, 
year = {2019}, 
title = {{NAS-Unet: Neural Architecture Search for Medical Image Segmentation}}, 
author = {Weng, Yu and Zhou, Tianbao and Li, Yujie and Qiu, Xiaoyu}, 
journal = {IEEE Access}, 
abstract = {{Neural architecture search (NAS) has significant progress in improving the accuracy of image classification. Recently, some works attempt to extend NAS to image segmentation which shows preliminary feasibility. However, all of them focus on searching architecture for semantic segmentation in natural scenes. In this paper, we design three types of primitive operation set on search space to automatically find two cell architecture DownSC and UpSC for semantic image segmentation especially medical image segmentation. Inspired by the U-net architecture and its variants successfully applied to various medical image segmentation, we propose NAS-Unet which is stacked by the same number of DownSC and UpSC on a U-like backbone network. The architectures of DownSC and UpSC updated simultaneously by a differential architecture strategy during the search stage. We demonstrate the good segmentation results of the proposed method on Promise12, Chaos, and ultrasound nerve datasets, which collected by magnetic resonance imaging, computed tomography, and ultrasound, respectively. Without any pretraining, our architecture searched on PASCAL VOC2012, attains better performances and much fewer parameters (about 0.8M) than U-net and one of its variants when evaluated on the above three types of medical image datasets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Weng-NAS-Unet-%20Neural%20Architecture%20Search%20for%20Medical%20Image%20Segmentation-2019-IEEE%20Access.pdf}
}
@article{3DU-NetCicekMICCAI2016, 
year = {2016}, 
title = {{3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation}}, 
author = {Çiçek, Özgün and Abdulkadir, Ahmed and Lienkamp, Soeren S and Brox, Thomas and Ronneberger, Olaf}, 
journal = {MICCAI}, 
eprint = {1606.06650}, 
abstract = {{This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup, the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup, we assume that a representative, sparsely annotated training set exists. Trained on this data set, the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch, i.e., no pre-trained network is required. We test the performance of the proposed method on a complex, highly variable 3D structure, the Xenopus kidney, and achieve good results for both use cases.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Çiçek-3D%20U-Net-%20Learning%20Dense%20Volumetric%20Segmentation%20from%20Sparse%20Annotation-2016-MICCAI.pdf}
}
@article{ConvolutionalTajbakhshTMI2016, 
year = {2016}, 
title = {{Convolutional neural networks for medical image analysis: Full Training or Fine Tuning?}}, 
author = {Tajbakhsh, Nima and Shin, Jae Y. and Gurudu, Suryakanth R. and Hurst, R. Todd and Kendall, Christopher B. and Gotway, Michael B. and Liang, Jianming}, 
journal = {TMI}, 
issn = {0278-0062}, 
pmid = {26978662}, 
eprint = {1706.00712}, 
abstract = {{Training a deep convolutional neural network (CNN) from scratch is difficult because it requires a large amount of labeled training data and a great deal of expertise to ensure proper convergence. A promising alternative is to fine-tune a CNN that has been pre-trained using, for instance, a large set of labeled natural images. However, the substantial differences between natural and medical images may advise against such knowledge transfer. In this paper, we seek to answer the following central question in the context of medical image analysis: Can the use of pre-trained deep CNNs with sufficient fine-tuning eliminate the need for training a deep CNN from scratch? To address this question, we considered four distinct medical imaging applications in three specialties (radiology, cardiology, and gastroenterology) involving classification, detection, and segmentation from three different imaging modalities, and investigated how the performance of deep CNNs trained from scratch compared with the pre-trained CNNs fine-tuned in a layer-wise manner. Our experiments consistently demonstrated that 1) the use of a pre-trained CNN with adequate fine-tuning outperformed or, in the worst case, performed as well as a CNN trained from scratch; 2) fine-tuned CNNs were more robust to the size of training sets than CNNs trained from scratch; 3) neither shallow tuning nor deep tuning was the optimal choice for a particular application; and 4) our layer-wise fine-tuning scheme could offer a practical way to reach the best performance for the application at hand based on the amount of available data.}}, 
pages = {1299--1312}, 
number = {5}, 
volume = {35}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tajbakhsh-Convolutional%20neural%20networks%20for%20medical%20image%20analysis-%20Full%20Training%20or%20Fine%20Tuning--2016-TMI.pdf}
}
@article{UNet++ZhouTMI2020, 
year = {2020}, 
title = {{UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation}}, 
author = {Zhou, Zongwei and Siddiquee, Md Mahfuzur Rahman and Tajbakhsh, Nima and Liang, Jianming}, 
journal = {TMI}, 
abstract = {{The state-of-the-art models for medical image segmentation are variants of U-Net and fully convolutional networks (FCN). Despite their success, these models have two limitations: (1) their optimal depth is apriori unknown, requiring extensive architecture search or inefficient ensemble of models of varying depths; and (2) their skip connections impose an unnecessarily restrictive fusion scheme, forcing aggregation only at the same-scale feature maps of the encoder and decoder sub-networks. To overcome these two limitations, we propose UNet++, a new neural architecture for semantic and instance segmentation, by (1) alleviating the unknown network depth with an efficient ensemble of U-Nets of varying depths, which partially share an encoder and co-learn simultaneously using deep supervision; (2) redesigning skip connections to aggregate features of varying semantic scales at the decoder sub-networks, leading to a highly flexible feature fusion scheme; and (3) devising a pruning scheme to accelerate the inference speed of UNet++. We have evaluated UNet++ using six different medical image segmentation datasets, covering multiple imaging modalities such as computed tomography (CT), magnetic resonance imaging (MRI), and electron microscopy (EM), and demonstrating that (1) UNet++ consistently outperforms the baseline models for the task of semantic segmentation across different datasets and backbone architectures; (2) UNet++ enhances segmentation quality of varying-size objects—an improvement over the fixed-depth U-Net; (3) Mask RCNN++ (Mask R-CNN with UNet++ design) outperforms the original Mask R-CNN for the task of instance segmentation; and (4) pruned UNet++ models achieve significant speedup while showing only modest performance degradation. Our implementation and pre-trained models are available at https://github.com/MrGiovanni/UNetPlusPlus.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhou-UNet++-%20Redesigning%20Skip%20Connections%20to%20Exploit%20Multiscale%20Features%20in%20Image%20Segmentation-2020-TMI.pdf}
}
@article{DeepCompressionHanICLR2016, 
year = {2016}, 
title = {{Deep compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding}}, 
author = {Han, Song and Mao, Huizi and Dally, William J}, 
journal = {ICLR}, 
eprint = {1510.00149}, 
abstract = {{Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Han-Deep%20compression-%20Compressing%20Deep%20Neural%20Networks%20with%20Pruning,%20Trained%20Quantization%20and%20Huffman%20Coding-2016-ICLR.pdf}
}
@article{DynamicRoutingLiCVPR2020, 
year = {2020}, 
title = {{Learning dynamic routing for semantic segmentation}}, 
author = {Li, Yanwei and Song, Lin and Chen, Yukang and Li, Zeming and Zhang, Xiangyu and Wang, Xingang and Sun, Jian}, 
journal = {CVPR}, 
abstract = {{Recently, numerous handcrafted and searched networks have been applied for semantic segmentation. However, previous works intend to handle inputs with various scales in pre-defined static architectures, such as FCN, U-Net, and DeepLab series. This paper studies a conceptually new method to alleviate the scale variance in semantic representation, named dynamic routing. The proposed framework generates data-dependent routes, adapting to the scale distribution of each image. To this end, a differentiable gating function, called soft conditional gate, is proposed to select scale transform paths on the fly. In addition, the computational cost can be further reduced in an end-to-end manner by giving budget constraints to the gating function. We further relax the network level routing space to support multipath propagations and skip-connections in each forward, bringing substantial network capacity. To demonstrate the superiority of the dynamic property, we compare with several static architectures, which can be modeled as special cases in the routing space. Extensive experiments are conducted on Cityscapes and PASCAL VOC 2012 to illustrate the effectiveness of the dynamic framework. Code is available at https://github.com/yanwei-li/DynamicRouting. 11Work was done in Megvii Research. liyanwei2017@ia.ac.cn Work was done in Megvii Research. liyanwei2017@ia.ac.cn}}, 
pages = {8550--8559}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Learning%20dynamic%20routing%20for%20semantic%20segmentation-2020-CVPR.pdf}
}
@article{ASAPNoyAISTATS2020, 
year = {2020}, 
title = {{ASAP: Architecture Search, Anneal and Prune}}, 
author = {Noy, Asaf and Nayman, Niv and Ridnik, Tal and Zamir, Nadav and Doveh, Sivan and Friedman, Itamar and Giryes, Raja and Zelnik-Manor, Lihi}, 
journal = {AISTATS}, 
eprint = {1904.04123}, 
abstract = {{Automatic methods for Neural Architecture Search (NAS) have been shown to produce state-of-the-art network models. Yet, their main drawback is the computational complexity of the search process. As some primal methods optimized over a discrete search space, thousands of days of GPU were required for convergence. A recent approach is based on constructing a differentiable search space that enables gradient-based optimization, which reduces the search time to a few days. While successful, it still includes some noncontinuous steps, e.g., the pruning of many weak connections at once. In this paper, we propose a differentiable search space that allows the annealing of architecture weights, while gradually pruning inferior operations. In this way, the search converges to a single output network in a continuous manner. Experiments on several vision datasets demonstrate the effectiveness of our method with respect to the search cost and accuracy of the achieved model. Specifically, with \$0.2\$ GPU search days we achieve an error rate of \$1.68\textbackslash\%\$ on CIFAR-10.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Noy-ASAP-%20Architecture%20Search,%20Anneal%20and%20Prune-2020-AISTATS.pdf}
}
@article{XNASNaymanNeurIPS2019, 
year = {2019}, 
title = {{XNAS: Neural Architecture Search with Expert Advice}}, 
author = {Nayman, Niv and Noy, Asaf and Ridnik, Tal and Friedman, Itamar and Jin, Rong and Zelnik-Manor, Lihi}, 
journal = {NeurIPS}, 
eprint = {1906.08031}, 
abstract = {{This paper introduces a novel optimization method for differential neural architecture search, based on the theory of prediction with expert advice. Its optimization criterion is well fitted for an architecture-selection, i.e., it minimizes the regret incurred by a sub-optimal selection of operations. Unlike previous search relaxations, that require hard pruning of architectures, our method is designed to dynamically wipe out inferior architectures and enhance superior ones. It achieves an optimal worst-case regret bound and suggests the use of multiple learning-rates, based on the amount of information carried by the backward gradients. Experiments show that our algorithm achieves a strong performance over several image classification datasets. Specifically, it obtains an error rate of 1.6\% for CIFAR-10, 24\% for ImageNet under mobile settings, and achieves state-of-the-art results on three additional datasets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Nayman-XNAS-%20Neural%20Architecture%20Search%20with%20Expert%20Advice-2019-NeurIPS.pdf}
}
@article{NetworkAdjustmentChenCVPR2020, 
year = {2020}, 
title = {{Network adjustment: Channel Search Guided by FLOPs Utilization Ratio}}, 
author = {Chen, Zhengsu and Niu, Jianwei and Xie, Lingxi and Liu, Xuefeng and Wei, Longhui and Tian, Qi}, 
journal = {CVPR}, 
abstract = {{Automatic designing computationally efficient neural networks has received much attention in recent years. Existing approaches either utilize network pruning or leverage the network architecture search methods. This paper presents a new framework named network adjustment, which considers network accuracy as a function of FLOPs, so that under each network configuration, one can estimate the FLOPs utilization ratio (FUR) for each layer and use it to determine whether to increase or decrease the number of channels on the layer. Note that FUR, like the gradient of a non-linear function, is accurate only in a small neighborhood of the current network. Hence, we design an iterative mechanism so that the initial network undergoes a number of steps, each of which has a small ‘adjusting rate’ to control the changes to the network. The computational overhead of the entire search process is reasonable, i.e., comparable to that of re-training the final model from scratch. Experiments on standard image classification datasets and a wide range of base networks demonstrate the effectiveness of our approach, which consistently outperforms the pruning counterpart. The code is available at https://github.com/danczs/NetworkAdjustment.}}, 
pages = {10655--10664}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Network%20adjustment-%20Channel%20Search%20Guided%20by%20FLOPs%20Utilization%20Ratio-2020-CVPR.pdf}
}
@article{CreamPengNeurIPS2020, 
year = {2020}, 
title = {{Cream of the crop: Distilling Prioritized Paths For One-Shot Neural Architecture Search}}, 
author = {Peng, Houwen and Du, Hao and Yu, Hongyuan and Li, Qi and Liao, Jing and Fu, Jianlong}, 
journal = {NeurIPS}, 
eprint = {2010.15821}, 
abstract = {{One-shot weight sharing methods have recently drawn great attention in neural architecture search due to high efficiency and competitive performance. However, weight sharing across models has an inherent deficiency, i.e., insufficient training of subnetworks in hypernetworks. To alleviate this problem, we present a simple yet effective architecture distillation method. The central idea is that subnetworks can learn collaboratively and teach each other throughout the training process, aiming to boost the convergence of individual models. We introduce the concept of prioritized path, which refers to the architecture candidates exhibiting superior performance during training. Distilling knowledge from the prioritized paths is able to boost the training of subnetworks. Since the prioritized paths are changed on the fly depending on their performance and complexity, the final obtained paths are the cream of the crop. We directly select the most promising one from the prioritized paths as the final architecture, without using other complex search methods, such as reinforcement learning or evolution algorithms. The experiments on ImageNet verify such path distillation method can improve the convergence ratio and performance of the hypernetwork, as well as boosting the training of subnetworks. The discovered architectures achieve superior performance compared to the recent MobileNetV3 and EfficientNet families under aligned settings. Moreover, the experiments on object detection and more challenging search space show the generality and robustness of the proposed method. Code and models are available at https://github.com/microsoft/cream.git.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Peng-Cream%20of%20the%20crop-%20Distilling%20Prioritized%20Paths%20For%20One-Shot%20Neural%20Architecture%20Search-2020-NeurIPS.pdf}
}
@article{RecurrentU-NetWangICCV2019, 
year = {2019}, 
title = {{Recurrent U-Net for resource-constrained segmentation}}, 
author = {Wang, Wei and Yu, Kaicheng and Hugonot, Joachim and Fua, Pascal and Salzmann, Mathieu}, 
journal = {ICCV}, 
abstract = {{State-of-the-art segmentation methods rely on very deep networks that are not always easy to train without very large training datasets and tend to be relatively slow to run on standard GPUs. In this paper, we introduce a novel recurrent U-Net architecture that preserves the compactness of the original U-Net [33], while substantially increasing its performance to the point where it outperforms the state of the art on several benchmarks. We will demonstrate its effectiveness for several tasks, including hand segmentation, retina vessel segmentation, and road segmentation. We also introduce a large-scale dataset for hand segmentation.}}, 
pages = {2142--2151}, 
volume = {00}, 
keywords = {}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Recurrent%20U-Net%20for%20resource-constrained%20segmentation-2019-ICCV.pdf}
}
@article{TernausNetIglovikovarXiv2018, 
year = {2018}, 
title = {{TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation}}, 
author = {Iglovikov, Vladimir and Shvets, Alexey}, 
journal = {arXiv}, 
eprint = {1801.05746}, 
abstract = {{Pixel-wise image segmentation is demanding task in computer vision. Classical U-Net architectures composed of encoders and decoders are very popular for segmentation of medical images, satellite images etc. Typically, neural network initialized with weights from a network pre-trained on a large data set like ImageNet shows better performance than those trained from scratch on a small dataset. In some practical applications, particularly in medicine and traffic safety, the accuracy of the models is of utmost importance. In this paper, we demonstrate how the U-Net type architecture can be improved by the use of the pre-trained encoder. Our code and corresponding pre-trained weights are publicly available at https://github.com/ternaus/TernausNet. We compare three weight initialization schemes: LeCun uniform, the encoder with weights from VGG11 and full network trained on the Carvana dataset. This network architecture was a part of the winning solution (1st out of 735) in the Kaggle: Carvana Image Masking Challenge.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Iglovikov-TernausNet-%20U-Net%20with%20VGG11%20Encoder%20Pre-Trained%20on%20ImageNet%20for%20Image%20Segmentation-2018-arXiv.pdf}
}
@article{TernausNetV2IglovikovCVPRWorkshop2018, 
year = {2018}, 
title = {{TernausNetV2: Fully Convolutional Network for Instance Segmentation}}, 
author = {Iglovikov, Vladimir and Seferbekov, Selim and Buslaev, Alexander and Shvets, Alexey}, 
journal = {CVPR Workshop}, 
abstract = {{The most common approaches to instance segmentation are complex and use two-stage networks with object proposals, conditional random-fields, template matching or recurrent neural networks. In this work we present Ternaus-NetV2 - a simple fully convolutional network that allows extracting objects from a high-resolution satellite imagery on an instance level. The network has popular encoder-decoder type of architecture with skip connections but has a few essential modifications that allows using for semantic as well as for instance segmentation tasks. This approach is universal and allows to extend any network that has been successfully applied for semantic segmentation to perform instance segmentation task. In addition, we generalize network encoder that was pre-trained for RGB images to use additional input channels. It makes possible to use transfer learning from visual to a wider spectral range. For DeepGlobe-CVPR 2018 building detection sub-challenge, based on public leaderboard score, our approach shows superior performance in comparison to other methods.}}, 
pages = {228--232}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Iglovikov-TernausNetV2-%20Fully%20Convolutional%20Network%20for%20Instance%20Segmentation-2018-CVPR%20Workshop.pdf}
}
@article{DNALiCVPR2020, 
year = {2020}, 
title = {{Block-wisely supervised neural architecture search with knowledge distillation}}, 
author = {Li, Changlin and Peng, Jiefeng and Yuan, Liuchun and Wang, Guangrun and Liang, Xiaodan and Lin, Liang and Chang, Xiaojun}, 
journal = {CVPR}, 
abstract = {{Neural Architecture Search (NAS), aiming at automatically designing network architectures by machines, is expected to bring about a new revolution in machine learning. Despite these high expectation, the effectiveness and efficiency of existing NAS solutions are unclear, with some recent works going so far as to suggest that many existing NAS solutions are no better than random architecture selection. The ineffectiveness of NAS solutions may be attributed to inaccurate architecture evaluation. Specifically, to speed up NAS, recent works have proposed under-training different candidate architectures in a large search space concurrently by using shared network parameters; however, this has resulted in incorrect architecture ratings and furthered the ineffectiveness of NAS. In this work, we propose to modularize the large search space of NAS into blocks to ensure that the potential candidate architectures are fully trained; this reduces the representation shift caused by the shared parameters and leads to the correct rating of the candidates. Thanks to the blockwise search, we can also evaluate all of the candidate architectures within each block. Moreover, we find that the knowledge of a network model lies not only in the network parameters but also in the network architecture. Therefore, we propose to distill the neural architecture (DNA) knowledge from a teacher model to supervise our block-wise architecture search, which significantly improves the effectiveness of NAS. Remarkably, the performance of our searched architectures has exceeded the teacher model, demonstrating the practicability of our method. Finally, our method achieves a state-of-the-art 78.4\% top-1 accuracy on ImageNet in a mobile setting. All of our searched models along with the evaluation code are available at https://github.com/changlin31/DNA.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Block-wisely%20supervised%20neural%20architecture%20search%20with%20knowledge%20distillation-2020-CVPR.pdf}
}
@article{SCARLET-NASChuICCV2021, 
year = {2021}, 
title = {{SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search}}, 
author = {Chu, Xiangxiang and Zhang, Bo and Li, Jixiang and Li, Qingyuan and Xu, Ruijun}, 
journal = {ICCV}, 
eprint = {1908.06022}, 
abstract = {{To discover powerful yet compact models is an important goal of neural architecture search. Previous two-stage one-shot approaches are limited by search space with a fixed depth. It seems handy to include an additional skip connection in the search space to make depths variable. However, it creates a large range of perturbation during supernet training and it has difficulty giving a confident ranking for subnetworks. In this paper, we discover that skip connections bring about significant feature inconsistency compared with other operations, which potentially degrades the supernet performance. Based on this observation, we tackle the problem by imposing an equivariant learnable stabilizer to homogenize such disparities. Experiments show that our proposed stabilizer helps to improve the supernet's convergence as well as ranking performance. With an evolutionary search backend that incorporates the stabilized supernet as an evaluator, we derive a family of state-of-the-art architectures, the SCARLET series of several depths, especially SCARLET-A obtains 76.9\% top-1 accuracy on ImageNet. The models and evaluation code are released online https://github.com/xiaomi-automl/ScarletNAS.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chu-SCARLET-NAS-%20Bridging%20the%20gap%20between%20Stability%20and%20Scalability%20in%20Weight-sharing%20Neural%20Architecture%20Search-2019-arXiv.pdf}
}
@article{MoGAChuICASSP2020, 
year = {2020}, 
title = {{MoGA: Searching Beyond MobileNetV3}}, 
author = {Chu, Xiangxiang and Zhang, Bo and Xu, Ruijun}, 
journal = {ICASSP}, 
abstract = {{The evolution of MobileNets has laid a solid foundation for neural network applications on mobile end. With the latest MobileNetV3, neural architecture search again claimed its supremacy in network design. Unfortunately, till today all mobile methods mainly focus on CPU latencies instead of GPU, the latter, however, is much preferred in practice because it has faster speed, lower overhead and less interference. Bearing the target hardware in mind, we propose the first Mobile GPU-Aware (MoGA) neural architecture search in order to be precisely tailored for real-world applications. Further, the ultimate objective to devise a mobile network lies in achieving better performance by maximizing the utilization of bounded resources. Urging higher capability while restraining time consumption is not reconcilable. We alleviate this tension by weighted evolution techniques. Moreover, we encourage increasing the number of parameters for higher representational power. With 200× fewer GPU days than MnasNet, we obtain a series of models that outperform MobileNetV3 under the similar latency constraints1.}}, 
pages = {4042--4046}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chu-MoGA-%20Searching%20Beyond%20MobileNetV3-2020-ICASSP.pdf}
}
@article{MixPathChuarXiv2020, 
year = {2020}, 
title = {{MixPath: A Unified Approach for One-shot Neural Architecture Search}}, 
author = {Chu, Xiangxiang and Li, Xudong and Lu, Shun and Zhang, Bo and Li, Jixiang}, 
journal = {arXiv}, 
eprint = {2001.05887}, 
abstract = {{Blending multiple convolutional kernels is proved advantageous in neural architectural design. However, current neural architecture search approaches are mainly limited to stacked single-path search space. How can the one-shot doctrine search for multi-path models remains unresolved. Specifically, we are motivated to train a multi-path supernet to accurately evaluate the candidate architectures. In this paper, we discover that in the studied search space, feature vectors summed from multiple paths are nearly multiples of those from a single path, which perturbs supernet training and its ranking ability. In this regard, we propose a novel mechanism called Shadow Batch Normalization(SBN) to regularize the disparate feature statistics. Extensive experiments prove that SBN is capable of stabilizing the training and improving the ranking performance (e.g. Kendall Tau 0.597 tested on NAS-Bench-101). We call our unified multi-path one-shot approach as MixPath, which generates a series of models that achieve state-of-the-art results on ImageNet.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chu-MixPath-%20A%20Unified%20Approach%20for%20One-shot%20Neural%20Architecture%20Search-2020-arXiv.pdf}
}
@article{PC-NASLiCVPR2020, 
year = {2020}, 
title = {{Improving one-shot NAS by suppressing the posterior fading}}, 
author = {Li, Xiang and Lin, Chen and Li, Chuming and Sun, Ming and Wu, Wei and Yan, Junjie and Ouyang, Wanli}, 
journal = {CVPR}, 
abstract = {{Neural architecture search (NAS) has demonstrated much success in automatically designing effective neural network architectures. To improve the efficiency of NAS, previous approaches adopt weight sharing method to force all models share the same set of weights. However, it has been observed that a model performing better with shared weights does not necessarily perform better when trained alone. In this paper, we analyse existing weight sharing one-shot NAS approaches from a Bayesian point of view and identify the Posterior Fading problem, which compromises the effectiveness of shared weights. To alleviate this problem, we present a novel approach to guide the parameter posterior towards its true distribution. Moreover, a hard latency constraint is introduced during the search so that the desired latency can be achieved. The resulted method, namely Posterior Convergent NAS (PC-NAS), achieves state-of-the-art performance under standard GPU latency constraint on ImageNet.}}, 
pages = {13833--13842}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Improving%20one-shot%20NAS%20by%20suppressing%20the%20posterior%20fading-2020-CVPR.pdf}
}
@article{DeepGlobeDemirCVPRWorkshop2018, 
year = {2018}, 
title = {{DeepGlobe 2018: A Challenge to Parse the Earth through Satellite Images}}, 
author = {Demir, Ilke and Koperski, Krzysztof and Lindenbaum, David and Pang, Guan and Huang, Jing and Basu, Saikat and Hughes, Forest and Tuia, Devis and Raskar, Ramesh}, 
journal = {CVPR Workshop}, 
abstract = {{We present the DeepGlobe 2018 Satellite Image Understanding Challenge, which includes three public competitions for segmentation, detection, and classification tasks on satellite images. Similar to other challenges in computer vision domain such as DAVIS and COCO, DeepGlobe proposes three datasets and corresponding evaluation methodologies, coherently bundled in three competitions with a dedicated workshop co-located with CVPR 2018. We observed that satellite imagery is a rich and structured source of information, yet it is less investigated than everyday images by computer vision researchers. However, bridging modern computer vision with remote sensing data analysis could have critical impact to the way we understand our environment and lead to major breakthroughs in global urban planning or climate change research. Keeping such bridging objective in mind, DeepGlobe aims to bring together researchers from different domains to raise awareness of remote sensing in the computer vision community and vice-versa. We aim to improve and evaluate state-of-the-art satellite image understanding approaches, which can hopefully serve as reference benchmarks for future research in the same topic. In this paper, we analyze characteristics of each dataset, define the evaluation criteria of the competitions, and provide baselines for each task.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Demir-DeepGlobe%202018-%20A%20Challenge%20to%20Parse%20the%20Earth%20through%20Satellite%20Images-2018-CVPR%20Workshop.pdf}
}
@article{StackedU-NetsGhoshCVPRWorkshop2018, 
year = {2018}, 
title = {{Stacked U-Nets for ground material segmentation in remote sensing imagery}}, 
author = {Ghosh, Arthita and Ehrlich, Max and Shah, Sohil and Davis, Larry and Chellappa, Rama}, 
journal = {CVPR Workshop}, 
abstract = {{We present a semantic segmentation algorithm for RGB remote sensing images. Our method is based on the Dilated Stacked U-Nets architecture. This state-of-the-art method has been shown to have good performance in other applications. We perform additional post-processing by blending image tiles and degridding the result. Our method gives competitive results on the DeepGlobe dataset.}}, 
pages = {252--256}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ghosh-Stacked%20U-Nets%20for%20ground%20material%20segmentation%20in%20remote%20sensing%20imagery-2018-CVPR%20Workshop.pdf}
}
@article{RuntimeNeuralPrunningLinNeurIPS2017, 
year = {2017}, 
title = {{Runtime neural pruning}}, 
author = {Lin, Ji and Rao, Yongming and Lu, Jiwen and Zhou, Jie}, 
journal = {NeurIPS}, 
abstract = {{In this paper, we propose a Runtime Neural Pruning (RNP) framework which prunes the deep neural network dynamically at the runtime. Unlike existing neural pruning methods which produce a fixed pruned model for deployment, our method preserves the full ability of the original network and conducts pruning according to the input image and current feature maps adaptively. The pruning is performed in a bottom-up, layer-by-layer manner, which we model as a Markov decision process and use reinforcement learning for training. The agent judges the importance of each convolutional kernel and conducts channel-wise pruning conditioned on different samples, where the network is pruned more when the image is easier for the task. Since the ability of network is fully preserved, the balance point is easily adjustable according to the available resources. Our method can be applied to off-the-shelf network structures and reach a better tradeoff between speed and accuracy, especially with a large pruning rate.}}, 
pages = {2181---2191}, 
keywords = {}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lin-Runtime%20neural%20pruning-2017-NeurIPS.pdf}
}
@article{OptimizationTMI2020, 
year = {2020}, 
title = {{Optimization for medical image segmentation: Theory and Practice When Evaluating With Dice Score or Jaccard Index}}, 
author = {Eelbode, Tom and Bertels, Jeroen and Berman, Maxim and Vandermeulen, Dirk and Maes, Frederik and Bisschops, Raf and Blaschko, Matthew B.}, 
journal = {TMI}, 
abstract = {{In many medical imaging and classical computer vision tasks, the Dice score and Jaccard index are used to evaluate the segmentation performance. Despite the existence and great empirical success of metric-sensitive losses, i.e. relaxations of these metrics such as soft Dice, soft Jaccard and Lovász-Softmax, many researchers still use per-pixel losses, such as (weighted) cross-entropy to train CNNs for segmentation. Therefore, the target metric is in many cases not directly optimized. We investigate from a theoretical perspective, the relation within the group of metric-sensitive loss functions and question the existence of an optimal weighting scheme for weighted cross-entropy to optimize the Dice score and Jaccard index at test time. We find that the Dice score and Jaccard index approximate each other relatively and absolutely, but we find no such approximation for a weighted Hamming similarity. For the Tversky loss, the approximation gets monotonically worse when deviating from the trivial weight setting where soft Tversky equals soft Dice. We verify these results empirically in an extensive validation on six medical segmentation tasks and can confirm that metric-sensitive losses are superior to cross-entropy based loss functions in case of evaluation with Dice Score or Jaccard Index. This further holds in a multi-class setting, and across different object sizes and foreground/background ratios. These results encourage a wider adoption of metric-sensitive loss functions for medical segmentation tasks where the performance measure of interest is the Dice score or Jaccard index.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Eelbode-Optimization%20for%20medical%20image%20segmentation-%20Theory%20and%20Practice%20When%20Evaluating%20With%20Dice%20Score%20or%20Jaccard%20Index-2020-TMI.pdf}
}
@article{GraphYouICML2020, 
year = {2020}, 
title = {{Graph structure of neural networks}}, 
author = {You, Jiaxuan and Leskovec, Jure and He, Kaiming and Xie, Saining}, 
journal = {ICML}, 
eprint = {2007.06559}, 
abstract = {{Neural networks are often represented as graphs of connections between neurons. However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance. Here we systematically investigate how does the graph structure of neural networks affect their predictive performance. To this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural network computation correspond to rounds of message exchange along the graph structure. Using this representation we show that: (1) a "sweet spot" of relational graphs leads to neural networks with significantly improved predictive performance; (2) neural network's performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph; (3) our findings are consistent across many different tasks and datasets; (4) the sweet spot can be identified efficiently; (5) top-performing neural networks have graph structure surprisingly similar to those of real biological neural networks. Our work opens new directions for the design of neural architectures and the understanding on neural networks in general.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/You-Graph%20structure%20of%20neural%20networks-2020-ICML.pdf}
}
@article{DeepYuCVPR2018, 
year = {2018}, 
title = {{Deep layer aggregation}}, 
author = {Yu, Fisher and Wang, Dequan and Shelhamer, Evan and Darrell, Trevor}, 
journal = {CVPR}, 
abstract = {{Visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improves inference of what and where. Architectural efforts are exploring many dimensions for network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across a network deserves further attention. Although skip connections have been incorporated to combine layers, these connections have been “shallow” themselves, and only fuse by simple, one-step operations. We augment standard architectures with deeper aggregation to better fuse information across layers. Our deep layer aggregation structures iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters. Experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes.}}, 
pages = {2403--2412}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yu-Deep%20layer%20aggregation-2018-CVPR.pdf}
}
@article{Lovasz-softmaxLossBermanCVPR2018, 
year = {2018}, 
title = {{The Lovasz-softmax loss: A Tractable Surrogate for the Optimization of the Intersection-Over-Union Measure in Neural Networks}}, 
author = {Berman, Maxim and Triki, Amal Rannen and Blaschko, Matthew B.}, 
journal = {CVPR}, 
abstract = {{The Jaccard index, also referred to as the intersection-over-union score, is commonly employed in the evaluation of image segmentation results given its perceptual qualities, scale invariance-which lends appropriate relevance to small objects, and appropriate counting of false negatives, in comparison to per-pixel losses. We present a method for direct optimization of the mean intersection-over-union loss in neural networks, in the context of semantic image segmentation, based on the convex Lovász extension of submodular losses. The loss is shown to perform better with respect to the Jaccard index measure than the traditionally used cross-entropy loss. We show quantitative and qualitative differences between optimizing the Jaccard index per image versus optimizing the Jaccard index taken over an entire dataset. We evaluate the impact of our method in a semantic segmentation pipeline and show substantially improved intersection-over-union segmentation scores on the Pascal VOC and Cityscapes datasets using state-of-the-art deep learning segmentation architectures.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Berman-The%20Lovasz-softmax%20loss-%20A%20Tractable%20Surrogate%20for%20the%20Optimization%20of%20the%20Intersection-Oyer-Union%20Measure%20in%20Neural%20Networks-2018-CVPR.pdf}
}
@article{V-NetMilletari3DV2016, 
year = {2016}, 
title = {{V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation}}, 
author = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad}, 
journal = {3DV}, 
abstract = {{Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Milletari-V-Net-%20Fully%20Convolutional%20Neural%20Networks%20for%20Volumetric%20Medical%20Image%20Segmentation-2016-3DV.pdf}
}
@article{RobNetsGuoCVPR2020, 
year = {2020}, 
title = {{When NAS meets robustness: In Search of Robust Architectures against Adversarial Attacks}}, 
author = {Guo, Minghao and Yang, Yuzhe and Xu, Rui and Liu, Ziwei and Lin, Dahua}, 
journal = {CVPR}, 
abstract = {{Recent advances in adversarial attacks uncover the intrinsic vulnerability of modern deep neural networks. Since then, extensive efforts have been devoted to enhancing the robustness of deep networks via specialized learning algorithms and loss functions. In this work, we take an architectural perspective and investigate the patterns of network architectures that are resilient to adversarial attacks. To obtain the large number of networks needed for this study, we adopt one-shot neural architecture search, training a large network for once and then finetuning the sub-networks sampled therefrom. The sampled architectures together with the accuracies they achieve provide a rich basis for our study. Our “robust architecture Odyssey” reveals several valuable observations: 1) densely connected patterns result in improved robustness; 2) under computational budget, adding convolution operations to direct connection edge is effective; 3) flow of solution procedure (FSP) matrix is a good indicator of network robustness. Based on these observations, we discover a family of robust architectures (RobNets). On various datasets, including CIFAR, SVHN, Tiny-ImageNet, and ImageNet, RobNets exhibit superior robustness performance to other widely used architectures. Notably, RobNets substantially improve the robust accuracy (∼5\% absolute gains) under both white-box and black-box attacks, even with fewer parameter numbers. Code is available at https://github.com/gmh14/RobNets.}}, 
pages = {628--637}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Guo-When%20NAS%20meets%20robustness-%20In%20Search%20of%20Robust%20Architectures%20against%20Adversarial%20Attacks-2020-CVPR.pdf}
}
@article{GDASDongCVPR2019, 
year = {2019}, 
title = {{Searching for a robust neural architecture in four GPU hours}}, 
author = {Dong, Xuanyi and Yang, Yi}, 
journal = {CVPR}, 
abstract = {{Conventional neural architecture search (NAS) approaches are based on reinforcement learning or evolutionary strategy, which take more than 3000 GPU hours to find a good model on CIFAR-10. We propose an efficient NAS approach learning to search by gradient descent. Our approach represents the search space as a directed acyclic graph (DAG). This DAG contains billions of sub-graphs, each of which indicates a kind of neural architecture. To avoid traversing all the possibilities of the sub-graphs, we develop a differentiable sampler over the DAG. This sampler is learnable and optimized by the validation loss after training the sampled architecture. In this way, our approach can be trained in an end-to-end fashion by gradient descent, named Gradient-based search using Differentiable Architecture Sampler (GDAS). In experiments, we can finish one searching procedure in four GPU hours on CIFAR-10, and the discovered model obtains a test error of 2.82\% with only 2.5M parameters, which is on par with the state-of-the-art.}}, 
pages = {1761--1770}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Dong-Searching%20for%20a%20robust%20neural%20architecture%20in%20four%20GPU%20hours-2019-CVPR.pdf}
}
@article{FPNASCuiICCV2019, 
year = {2019}, 
title = {{Fast and practical neural architecture search}}, 
author = {Cui, Jiequan and Chen, Pengguang and Li, Ruiyu and Liu, Shu and Shen, Xiaoyong and Jia, Jiaya}, 
journal = {ICCV}, 
abstract = {{In this paper, we propose a fast and practical neural architecture search (FPNAS) framework for automatic network design. FPNAS aims to discover extremely efficient networks with less than 300M FLOPs. Different from previous NAS methods, our approach searches for the whole network architecture to guarantee block diversity instead of stacking a set of similar blocks repeatedly. We model the search process as a bi-level optimization problem and propose an approximation solution. On CIFAR-10, our approach is capable of design networks with comparable performance to state-of-the-arts while using orders of magnitude less computational resource with only 20 GPU hours. Experimental results on ImageNet and ADE20K datasets further demonstrate transferability of the searched networks.}}, 
pages = {6508--6517}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Cui-Fast%20and%20practical%20neural%20architecture%20search-2019-ICCV.pdf}
}
@article{TG-SAGEBashivanICCV2019, 
year = {2019}, 
title = {{Teacher guided architecture search}}, 
author = {Bashivan, Pouya and Tensen, Mark and DiCarlo, James J}, 
journal = {ICCV}, 
abstract = {{Much of the recent improvement in neural networks for computer vision has resulted from discovery of new networks architectures. Most prior work has used the performance of candidate models following limited training to automatically guide the search in a feasible way. Could further gains in computational efficiency be achieved by guiding the search via measurements of a high performing network with unknown detailed architecture (e.g. the primate visual system)? As one step toward this goal, we use representational similarity analysis to evaluate the similarity of internal activations of candidate networks with those of a (fixed, high performing) teacher network. We show that adopting this evaluation metric could produce up to an order of magnitude in search efficiency over performanceguided methods. Our approach finds a convolutional cell structure with similar performance as was previously found using other methods but at a total computational cost that is two orders of magnitude lower than Neural Architecture Search (NAS) and more than four times lower than progressive neural architecture search (PNAS). We further show that measurements from only \$\textbackslashsim\$300 neurons from primate visual system provides enough signal to find a network with an Imagenet top-1 error that is significantly lower than that achieved by performance-guided architecture search alone. These results suggest that representational matching can be used to accelerate network architecture search in cases where one has access to some or all of the internal representations of a teacher network of interest, such as the brain’s sensory processing networks.}}, 
pages = {5319--5328}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bashivan-Teacher%20guided%20architecture%20search-2019-ICCV.pdf}
}
@article{SETNDongICCV2019, 
year = {2019}, 
title = {{One-shot neural architecture search via self-evaluated template network}}, 
author = {Dong, Xuanyi and Yang, Yi}, 
journal = {ICCV}, 
eprint = {1910.05733}, 
abstract = {{Neural architecture search (NAS) aims to automate the search procedure of architecture instead of manual design. Even if recent NAS approaches finish the search within days, lengthy training is still required for a specific architecture candidate to get the parameters for its accurate evaluation. Recently one-shot NAS methods are proposed to largely squeeze the tedious training process by sharing parameters across candidates. In this way, the parameters for each candidate can be directly extracted from the shared parameters instead of training them from scratch. However, they have no sense of which candidate will perform better until evaluation so that the candidates to evaluate are randomly sampled and the top-1 candidate is considered the best. In this paper, we propose a Self-Evaluated Template Network (SETN) to improve the quality of the architecture candidates for evaluation so that it is more likely to cover competitive candidates. SETN consists of two components: (1) an evaluator, which learns to indicate the probability of each individual architecture being likely to have a lower validation loss. The candidates for evaluation can thus be selectively sampled according to this evaluator. (2) a template network, which shares parameters among all candidates to amortize the training cost of generated candidates. In experiments, the architecture found by SETN achieves state-of-the-art performance on CIFAR and ImageNet benchmarks within comparable computation costs. Code is publicly available on GitHub: https://github.com/D-X-Y/AutoDL-Projects.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Dong-One-shot%20neural%20architecture%20search%20via%20self-evaluated%20template%20network-2019-ICCV.pdf}
}
@article{ResourceXiongICCV2019, 
year = {2019}, 
title = {{Resource constrained neural network architecture search: Will a Submodularity Assumption Help?}}, 
author = {Xiong, Yunyang and Mehta, Ronak and Singh, Vikas}, 
journal = {ICCV}, 
abstract = {{The design of neural network architectures is frequently either based on human expertise using triallerror and empirical feedback or tackled via large scale reinforcement learning strategies performed over distinct discrete architecture choices. In the latter case, the optimization is often non-differentiable and also not very amenable to derivative-free optimization methods. Most methods in use today require sizable computational resources. And if we want networks that additionally satisfy resource constraints, the above challenges are exacerbated because the search must now balance accuracy with certain budget constraints on resources. We formulate this problem as the optimization of a set function–we find that the empirical behavior of this set function often (but not always) satisfies marginal gain and monotonicity principles-properties central to the idea of submodularity. Based on this observation, we adapt algorithms within discrete optimization to obtain heuristic schemes for neural network architecture search, where we have resource constraints on the architecture. This simple scheme when applied on CIFAR-100 and ImageNet, identifies resource-constrained architectures with quantifiably better performance than current state-of-the-art models designed for mobile devices. Specifically, we find high-performing architectures with fewer parameters and computations by a search method that is much faster.}}, 
pages = {1901--1910}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xiong-Resource%20constrained%20neural%20network%20architecture%20search-%20Will%20a%20Submodularity%20Assumption%20Help--2019-ICCV.pdf}
}
@article{MDENASZhengICCV2019, 
year = {2019}, 
title = {{Multinomial distribution learning for effective neural architecture search}}, 
author = {Zheng, Xiawu and Ji, Rongrong and Tang, Lang and Zhang, Baochang and Liu, Jianzhuang and Tian, Qi}, 
journal = {ICCV}, 
abstract = {{Architectures obtained by Neural Architecture Search (NAS) have achieved highly competitive performance in various computer vision tasks. However, the prohibitive computation demand of forward-backward propagation in deep neural networks and searching algorithms makes it difficult to apply NAS in practice. In this paper, we propose a Multinomial Distribution Learning for extremely effective NAS, which considers the search space as a joint multinomial distribution, i.e., the operation between two nodes is sampled from this distribution, and the optimal network structure is obtained by the operations with the most likely probability in this distribution. Therefore, NAS can be transformed to a multinomial distribution learning problem, i.e., the distribution is optimized to have high expectation of the performance. Besides, a hypothesis that the performance ranking is consistent in every training epoch is proposed and demonstrated to further accelerate the learning process. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of our method. On CIFAR-10, the structure searched by our method achieves 2.55\% test error, while being 6.0\$\textbackslashtimes\$ (only 4 GPU hours on GTX1080Ti) faster compared with state-of-the-art NAS algorithms. On ImageNet, our model achieves 75.2\% top1 accuracy under MobileNet settings (MobileNet V1/V2), while being 1.2\$\textbackslashtimes\$ faster with measured GPU latency. Test code with pre-trained models are available at https://github.com/tanglang96/MDENAS}}, 
pages = {1304--1313}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zheng-Multinomial%20distribution%20learning%20for%20effective%20neural%20architecture%20search-2019-ICCV.pdf}
}
@article{TASDongNeurIPS2019, 
year = {2019}, 
title = {{Network pruning via transformable architecture search}}, 
author = {Dong, Xuanyi and Yang, Yi}, 
journal = {NeurIPS}, 
abstract = {{Network pruning reduces the computation costs of an over-parameterized network without performance damage. Prevailing pruning algorithms pre-define the width and depth of the pruned networks, and then transfer parameters from the unpruned network to pruned networks. To break the structure limitation of the pruned networks, we propose to apply neural architecture search to search directly for a network with flexible channel and layer sizes. The number of the channels/layers is learned by minimizing the loss of the pruned networks. The feature map of the pruned network is an aggregation of K feature map fragments (generated by K networks of different sizes), which are sampled based on the probability distribution.The loss can be back-propagated not only to the network weights, but also to the parameterized distribution to explicitly tune the size of the channels/layers. Specifically, we apply channel-wise interpolation to keep the feature map with different channel sizes aligned in the aggregation procedure. The maximum probability for the size in each distribution serves as the width and depth of the pruned network, whose parameters are learned by knowledge transfer, e.g., knowledge distillation, from the original networks. Experiments on CIFAR-10, CIFAR-100 and ImageNet demonstrate the effectiveness of our new perspective of network pruning compared to traditional network pruning algorithms. Various searching and knowledge transfer approaches are conducted to show the effectiveness of the two components. Code is at: https://github.com/D-X-Y/NAS-Projects.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Dong-Network%20pruning%20via%20transformable%20architecture%20search-2019-NeurIPS.pdf}
}
@article{EcoNASZhouCVPR2020, 
year = {2020}, 
title = {{EcoNAS: Finding Proxies for Economical Neural Architecture Search}}, 
author = {Zhou, Dongzhan and Zhou, Xinchi and Zhang, Wenwei and Loy, Chen Change and Yi, Shuai and Zhang, Xuesen and Ouyang, Wanli}, 
journal = {CVPR}, 
abstract = {{Neural Architecture Search (NAS) achieves significant progress in many computer vision tasks. While many methods have been proposed to improve the efficiency of NAS, the search progress is still laborious because training and evaluating plausible architectures over large search space is time-consuming. Assessing network candidates under a proxy (i.e., computationally reduced setting) thus becomes inevitable. In this paper, we observe that most existing proxies exhibit different behaviors in maintaining the rank consistency among network candidates. In particular, some proxies can be more reliable - the rank of candidates does not differ much comparing their reduced setting performance and final performance. In this paper, we systematically investigate some widely adopted reduction factors and report our observations. Inspired by these observations, we present a reliable proxy and further formulate a hierarchical proxy strategy. The strategy spends more computations on candidate networks that are potentially more accurate, while discards unpromising ones in early stage with a fast proxy. This leads to an economical evolutionary-based NAS (EcoNAS), which achieves an impressive 400× search time reduction in comparison to the evolutionary-based state of the art [19] (8 vs. 3150 GPU days). Some new proxies led by our observations can also be applied to accelerate other NAS methods while still able to discover good candidate networks with performance matching those found by previous proxy strategies.}}, 
pages = {11393--11401}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhou-EcoNAS-%20Finding%20Proxies%20for%20Economical%20Neural%20Architecture%20Search-2020-CVPR.pdf}
}
@article{GreedyNASYouCVPR2020, 
year = {2020}, 
title = {{GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet}}, 
author = {You, Shan and Huang, Tao and Yang, Mingmin and Wang, Fei and Qian, Chen and Zhang, Changshui}, 
journal = {CVPR}, 
abstract = {{Training a supernet matters for one-shot neural architecture search (NAS) methods since it serves as a basic performance estimator for different architectures (paths). Current methods mainly hold the assumption that a supernet should give a reasonable ranking over all paths. They thus treat all paths equally, and spare much effort to train paths. However, it is harsh for a single supernet to evaluate accurately on such a huge-scale search space (e.g., 721). In this paper, instead of covering all paths, we ease the burden of supernet by encouraging it to focus more on evaluation of those potentially-good ones, which are identified using a surrogate portion of validation data. Concretely, during training, we propose a multi-path sampling strategy with rejection, and greedily filter the weak paths. The training efficiency is thus boosted since the training space has been greedily shrunk from all paths to those potentially-good ones. Moreover, we further adopt an exploration and exploitation policy by introducing an empirical candidate path pool. Our proposed method GreedyNAS is easy-to-follow, and experimental results on ImageNet dataset indicate that it can achieve better Top-1 accuracy under same search space and FLOPs or latency level, but with only ∼60\% of supernet training cost. By searching on a larger space, our GreedyNAS can also obtain new state-of-the-art architectures.}}, 
pages = {1996--2005}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/You-GreedyNAS-%20Towards%20Fast%20One-Shot%20NAS%20with%20Greedy%20Supernet-2020-CVPR.pdf}
}
@article{SGASLiCVPR2020, 
year = {2020}, 
title = {{SGAS: Sequential Greedy Architecture Search}}, 
author = {Li, Guohao and Qiau, Guocheng and Delgadillo, Itzel C. and Müller, Matthias and Thabet, Ali and Ghanem, Bernard}, 
journal = {CVPR}, 
abstract = {{Architecture design has become a crucial component of successful deep learning. Recent progress in automatic neural architecture search (NAS) shows a lot of promise. However, discovered architectures often fail to generalize in the final evaluation. Architectures with a higher validation accuracy during the search phase may perform worse in the evaluation (see Figure 1). Aiming to alleviate this common issue, we introduce sequential greedy architecture search (SGAS), an efficient method for neural architecture search. By dividing the search procedure into subproblems, SGAS chooses and prunes candidate operations in a greedy fashion. We apply SGAS to search architectures for Convolutional Neural Networks (CNN) and Graph Convolutional Networks (GCN). Extensive experiments show that SGAS is able to find state-of-the-art architectures for tasks such as image classification, point cloud classification and node classification in protein-protein interaction graphs with minimal computational cost.}}, 
pages = {1617--1627}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-SGAS-%20Sequential%20Greedy%20Architecture%20Search-2020-CVPR.pdf}
}
@article{MiLeNASHeCVPR2020, 
year = {2020}, 
title = {{MiLeNAS: Efficient Neural Architecture Search via Mixed-Level Reformulation}}, 
author = {He, Chaoyang and Ye, Haishan and Shen, Li and Zhang, Tong}, 
journal = {CVPR}, 
abstract = {{Many recently proposed methods for Neural Architecture Search (NAS) can be formulated as bilevel optimization. For efficient implementation, its solution requires approximations of second-order methods. In this paper, we demonstrate that gradient errors caused by such approximations lead to suboptimality, in the sense that the optimization procedure fails to converge to a (locally) optimal solution. To remedy this, this paper proposes MiLeNAS, a mixed-level reformulation for NAS that can be optimized efficiently and reliably. It is shown that even when using a simple first-order method on the mixed-level formulation, MiLeNAS can achieve a lower validation error for NAS problems. Consequently, architectures obtained by our method achieve consistently higher accuracies than those obtained from bilevel optimization. Moreover, MiLeNAS proposes a framework beyond DARTS. It is upgraded via model size-based search and early stopping strategies to complete the search process in around 5 hours. Extensive experiments within the convolutional architecture search space validate the effectiveness of our approach.}}, 
pages = {11990--11999}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/He-MiLeNAS-%20Efficient%20Neural%20Architecture%20Search%20via%20Mixed-Level%20Reformulation-2020-CVPR.pdf}
}
@article{DSNASHuCVPR2020, 
year = {2020}, 
title = {{DSNAS: Direct Neural Architecture Search without Parameter Retraining}}, 
author = {Hu, Shoukang and Xie, Sirui and Zheng, Hehui and Liu, Chunxiao and Shi, Jianping and Liu, Xunying and Lin, Dahua}, 
journal = {CVPR}, 
abstract = {{If NAS methods are solutions, what is the problem? Most existing NAS methods require two-stage parameter optimization. However, performance of the same architecture in the two stages correlates poorly. In this work, we propose a new problem definition for NAS, task-specific end-to-end, based on this observation. We argue that given a computer vision task for which a NAS method is expected, this definition can reduce the vaguely-defined NAS evaluation to i) accuracy of this task and ii) the total computation consumed to finally obtain a model with satisfying accuracy. Seeing that most existing methods do not solve this problem directly, we propose DSNAS, an efficient differentiable NAS framework that simultaneously optimizes architecture and parameters with a low-biased Monte Carlo estimate. Child networks derived from DSNAS can be deployed directly without parameter retraining. Comparing with two-stage methods, DSNAS successfully discovers networks with comparable accuracy (74.4\%) on ImageNet in 420 GPU hours, reducing the total time by more than 34\%.}}, 
pages = {12081--12089}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hu-DSNAS-%20Direct%20Neural%20Architecture%20Search%20without%20Parameter%20Retraining-2020-CVPR.pdf}
}
@article{Hit-DetectorGuoCVPR2020, 
year = {2020}, 
title = {{Hit-Detector: Hierarchical Trinity Architecture Search for Object Detection}}, 
author = {Guo, Jianyuan and Han, Kai and Wang, Yunhe and Zhang, Chao and Yang, Zhaohui and Wu, Han and Chen, Xinghao and Xu, Chang}, 
journal = {CVPR}, 
abstract = {{Neural Architecture Search (NAS) has achieved great success in image classification task. Some recent works have managed to explore the automatic design of efficient backbone or feature fusion layer for object detection. However, these methods focus on searching only one certain component of object detector while leaving others manually designed. We identify the inconsistency between searched component and manually designed ones would withhold the detector of stronger performance. To this end, we propose a hierarchical trinity search framework to simultaneously discover efficient architectures for all components (i.e. backbone, neck, and head) of object detector in an end-to-end manner. In addition, we empirically reveal that different parts of the detector prefer different operators. Motivated by this, we employ a novel scheme to automatically screen different sub search spaces for different components so as to perform the end-to-end search for each component on the corresponding sub search space efficiently. Without bells and whistles, our searched architecture, namely Hit-Detector, achieves 41.4\% mAP on COCO minival set with 27M parameters. Our implementation is available at https://github.com/ggjy/HitDet.pytorch.}}, 
pages = {11402--11411}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Guo-Hit-Detector-%20Hierarchical%20Trinity%20Architecture%20Search%20for%20Object%20Detection-2020-CVPR.pdf}
}
@article{E2GANTianECCV2020, 
year = {2020}, 
title = {{Off-policy reinforcement learning for efficient and effective GAN architecture search}}, 
author = {Tian, Yuan and Wang, Qin and Huang, Zhiwu and Li, Wen and Dai, Dengxin and Yang, Minghao and Wang, Jun and Fink, Olga}, 
journal = {ECCV}, 
abstract = {{In this paper, we introduce a new reinforcement learning (RL) based neural architecture search (NAS) methodology for effective and efficient generative adversarial network (GAN) architecture search. The key idea is to formulate the GAN architecture search problem as a Markov decision process (MDP) for smoother architecture sampling, which enables a more effective RL-based search algorithm by targeting the potential global optimal architecture. To improve efficiency, we exploit an off-policy GAN architecture search algorithm that makes efficient use of the samples generated by previous policies. Evaluation on two standard benchmark datasets (i.e., CIFAR-10 and STL-10) demonstrates that the proposed method is able to discover highly competitive architectures for generally better image generation results with a considerably reduced computational burden: 7 GPU hours. Our code is available at https://github.com/Yuantian013/E2GAN.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tian-Off-policy%20reinforcement%20learning%20for%20efficient%20and%20effective%20GAN%20architecture%20search-2020-ECCV.pdf}
}
@article{MSDNetHuangICLR2018, 
year = {2017}, 
title = {{Multi-scale dense networks for resource efficient image classification}}, 
author = {Huang, Gao and Chen, Danlu and Li, Tianhong and Wu, Felix and Maaten, Laurens van der and Weinberger, Kilian Q}, 
journal = {ICLR}, 
eprint = {1703.09844}, 
abstract = {{In this paper we investigate image classification with computational resource limits at test time. Two such settings are: 1. anytime classification, where the network's prediction for a test example is progressively updated, facilitating the output of a prediction at any time; and 2. budgeted batch classification, where a fixed amount of computation is available to classify a set of examples that can be spent unevenly across "easier" and "harder" inputs. In contrast to most prior work, such as the popular Viola and Jones algorithm, our approach is based on convolutional neural networks. We train multiple classifiers with varying resource demands, which we adaptively apply during test time. To maximally re-use computation between the classifiers, we incorporate them as early-exits into a single deep convolutional neural network and inter-connect them with dense connectivity. To facilitate high quality classification early on, we use a two-dimensional multi-scale network architecture that maintains coarse and fine level features all-throughout the network. Experiments on three image-classification tasks demonstrate that our framework substantially improves the existing state-of-the-art in both settings.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Huang-Multi-scale%20dense%20networks%20for%20resource%20efficient%20image%20classification-2017-ICLR.pdf}
}
@article{S2DNASYuanECCV2020, 
year = {2019}, 
title = {{S2DNAS: Transforming Static CNN Model for Dynamic Inference via Neural Architecture Search}}, 
author = {Yuan, Zhihang and Wu, Bingzhe and Liang, Zheng and Zhao, Shiwan and Bi, Weichen and Sun, Guangyu}, 
journal = {ECCV}, 
eprint = {1911.07033}, 
abstract = {{Recently, dynamic inference has emerged as a promising way to reduce the computational cost of deep convolutional neural network (CNN). In contrast to static methods (e.g. weight pruning), dynamic inference adaptively adjusts the inference process according to each input sample, which can considerably reduce the computational cost on "easy" samples while maintaining the overall model performance. In this paper, we introduce a general framework, S2DNAS, which can transform various static CNN models to support dynamic inference via neural architecture search. To this end, based on a given CNN model, we first generate a CNN architecture space in which each architecture is a multi-stage CNN generated from the given model using some predefined transformations. Then, we propose a reinforcement learning based approach to automatically search for the optimal CNN architecture in the generated space. At last, with the searched multi-stage network, we can perform dynamic inference by adaptively choosing a stage to evaluate for each sample. Unlike previous works that introduce irregular computations or complex controllers in the inference or re-design a CNN model from scratch, our method can generalize to most of the popular CNN architectures and the searched dynamic network can be directly deployed using existing deep learning frameworks in various hardware devices.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yuan-S2DNAS-%20Transforming%20Static%20CNN%20Model%20for%20Dynamic%20Inference%20via%20Neural%20Architecture%20Search-2019-ECCV.pdf}
}
@article{FBSGaoICLR2019, 
year = {2019}, 
title = {{Dynamic channel pruning: Feature Boosting and Suppression}}, 
author = {Gao, Xitong and Zhao, Yiren and Dudziak, Łukasz and Mullins, Robert and Xu, Cheng-zhong}, 
journal = {ICLR}, 
abstract = {{Making deep convolutional neural networks more accurate typically comes at the cost of increased computational and memory resources. In this paper, we reduce this cost by exploiting the fact that the importance of features computed by convolutional layers is highly input-dependent, and propose feature boosting and suppression (FBS), a new method to predictively amplify salient convolutional channels and skip unimportant ones at run-time. FBS introduces small auxiliary connections to existing convolutional layers. In contrast to channel pruning methods which permanently remove channels, it preserves the full network structures and accelerates convolution by dynamically skipping unimportant input and output channels. FBS-augmented networks are trained with conventional stochastic gradient descent, making it readily available for many state-of-the-art CNNs. We compare FBS to a range of existing channel pruning and dynamic execution schemes and demonstrate large improvements on ImageNet classification. Experiments show that FBS can respectively provide \$5\textbackslashtimes\$ and \$2\textbackslashtimes\$ savings in compute on VGG-16 and ResNet-18, both with less than \$0.6\textbackslash\%\$ top-5 accuracy loss.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Gao-Dynamic%20channel%20pruning-%20Feature%20Boosting%20and%20Suppression-2019-ICLR.pdf}
}
@article{RANetYangCVPR2020, 
year = {2020}, 
title = {{Resolution adaptive networks for efficient inference}}, 
author = {Yang, Le and Han, Yizeng and Chen, Xi and Song, Shiji and Dai, Jifeng and Huang, Gao}, 
journal = {CVPR}, 
abstract = {{Adaptive inference is an effective mechanism to achieve a dynamic tradeoff between accuracy and computational cost in deep networks. Existing works mainly exploit architecture redundancy in network depth or width. In this paper, we focus on spatial redundancy of input samples and propose a novel Resolution Adaptive Network (RANet), which is inspired by the intuition that low-resolution representations are sufficient for classifying “easy” inputs containing large objects with prototypical features, while only some “hard” samples need spatially detailed information. In RANet, the input images are first routed to a lightweight sub-network that efficiently extracts low-resolution representations, and those samples with high prediction confidence will exit early from the network without being further processed. Meanwhile, high-resolution paths in the network maintain the capability to recognize the “hard” samples. Therefore, RANet can effectively reduce the spatial redundancy involved in inferring high-resolution inputs. Empirically, we demonstrate the effectiveness of the proposed RANet on the CIFAR-10, CIFAR-100 and ImageNet datasets in both the anytime prediction setting and the budgeted batch classification setting.}}, 
pages = {2366--2375}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-Resolution%20adaptive%20networks%20for%20efficient%20inference-2020-CVPR.pdf}
}
@article{BlockDropWuCVPR2018, 
year = {2018}, 
title = {{BlockDrop: Dynamic Inference Paths in Residual Networks}}, 
author = {Wu, Zuxuan and Nagarajan, Tushar and Kumar, Abhishek and Rennie, Steven and Davis, Larry S. and Grauman, Kristen and Feris, Rogerio}, 
journal = {CVPR}, 
abstract = {{Very deep convolutional neural networks offer excellent recognition results, yet their computational expense limits their impact for many real-world applications. We introduce BlockDrop, an approach that learns to dynamically choose which layers of a deep network to execute during inference so as to best reduce total computation without degrading prediction accuracy. Exploiting the robustness of Residual Networks (ResNets) to layer dropping, our framework selects on-the-fly which residual blocks to evaluate for a given novel image. In particular, given a pretrained ResNet, we train a policy network in an associative reinforcement learning setting for the dual reward of utilizing a minimal number of blocks while preserving recognition accuracy. We conduct extensive experiments on CIFAR and ImageNet. The results provide strong quantitative and qualitative evidence that these learned policies not only accelerate inference but also encode meaningful visual information. Built upon a ResNet-101 model, our method achieves a speedup of 20\% on average, going as high as 36\% for some images, while maintaining the same 76.4\% top-1 accuracy on ImageNet.}}, 
pages = {8817--8826}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wu-BlockDrop-%20Dynamic%20Inference%20Paths%20in%20Residual%20Networks-2018-CVPR.pdf}
}
@article{MUXConvLuCVPR2020, 
year = {2020}, 
title = {{MUXConv: Information Multiplexing in Convolutional Neural Networks}}, 
author = {Lu, Zhichao and Deb, Kalyanmoy and Boddeti, Vishnu Naresh}, 
journal = {CVPR}, 
abstract = {{Convolutional neural networks have witnessed remarkable improvements in computational efficiency in recent years. A key driving force has been the idea of trading-off model expressivity and efficiency through a combination of \$1\textbackslashtimes 1\$ and depth-wise separable convolutions in lieu of a standard convolutional layer. The price of the efficiency, however, is the sub-optimal flow of information across space and channels in the network. To overcome this limitation, we present MUXConv, a layer that is designed to increase the flow of information by progressively multiplexing channel and spatial information in the network, while mitigating computational complexity. Furthermore, to demonstrate the effectiveness of MUXConv, we integrate it within an efficient multi-objective evolutionary algorithm to search for the optimal model hyper-parameters while simultaneously optimizing accuracy, compactness, and computational efficiency. On ImageNet, the resulting models, dubbed MUXNets, match the performance (75.3\% top-1 accuracy) and multiply-add operations (218M) of MobileNetV3 while being \$\textbackslashmathit\{1.6\}\textbackslashtimes\$ more compact, and outperform other mobile models in all the three criteria. MUXNet also performs well under transfer learning and when adapted to object detection. On the ChestX-Ray 14 benchmark, its accuracy is comparable to the state-of-the-art while being \$3.3\textbackslashtimes\$ more compact and \$14\textbackslashtimes\$ more efficient. Similarly, detection on PASCAL VOC 2007 is 1.2\% more accurate, 28\% faster and 6\% more compact compared to MobileNetV2. The code is available from https://github.com/human-analysis/MUXConv.}}, 
pages = {12041--12050}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lu-MUXConv-%20Information%20Multiplexing%20in%20Convolutional%20Neural%20Networks-2020-CVPR.pdf}
}
@article{Perturb-and-MAPPapandreouICCV2011, 
year = {2011}, 
title = {{Perturb-and-MAP random fields: Using Discrete Optimization to Learn and Sample from Energy Models}}, 
author = {Papandreou, George and Yuille, Alan L.}, 
journal = {ICCV}, 
abstract = {{We propose a novel way to induce a random field from an energy function on discrete labels. It amounts to locally injecting noise to the energy potentials, followed by finding the global minimum of the perturbed energy function. The resulting Perturb-and-MAP random fields harness the power of modern discrete energy minimization algorithms, effectively transforming them into efficient random sampling algorithms, thus extending their scope beyond the usual deterministic setting. In this fashion we can enjoy the benefits of a sound probabilistic framework, such as the ability to represent the solution uncertainty or learn model parameters from training data, while completely bypassing costly Markov-chain Monte-Carlo procedures typically associated with discrete label Gibbs Markov random fields (MRFs). We study some interesting theoretical properties of the proposed model in juxtaposition to those of Gibbs MRFs and address the issue of principled design of the perturbation process. We present experimental results in image segmentation and scene labeling that illustrate the new qualitative aspects and the potential of the proposed model for practical computer vision applications.}}, 
pages = {193--200}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Papandreou-Perturb-and-MAP%20random%20fields-%20Using%20Discrete%20Optimization%20to%20Learn%20and%20Sample%20from%20Energy%20Models-2011-ICCV.pdf}
}
@article{TRWPXuACCV2020, 
year = {2019}, 
title = {{Fast and differentiable message passing on pairwise Markov random fields}}, 
author = {Xu, Zhiwei and Ajanthan, Thalaiyasingam and Hartley, Richard}, 
journal = {ACCV}, 
eprint = {1910.10892}, 
abstract = {{Despite the availability of many Markov Random Field (MRF) optimization algorithms, their widespread usage is currently limited due to imperfect MRF modelling arising from hand-crafted model parameters and the selection of inferior inference algorithm. In addition to differentiability, the two main aspects that enable learning these model parameters are the forward and backward propagation time of the MRF optimization algorithm and its inference capabilities. In this work, we introduce two fast and differentiable message passing algorithms, namely, Iterative Semi-Global Matching Revised (ISGMR) and Parallel Tree-Reweighted Message Passing (TRWP) which are greatly sped up on a GPU by exploiting massive parallelism. Specifically, ISGMR is an iterative and revised version of the standard SGM for general pairwise MRFs with improved optimization effectiveness, and TRWP is a highly parallel version of Sequential TRW (TRWS) for faster optimization. Our experiments on the standard stereo and denoising benchmarks demonstrated that ISGMR and TRWP achieve much lower energies than SGM and Mean-Field (MF), and TRWP is two orders of magnitude faster than TRWS without losing effectiveness in optimization. We further demonstrated the effectiveness of our algorithms on end-to-end learning for semantic segmentation. Notably, our CUDA implementations are at least \$7\$ and \$700\$ times faster than PyTorch GPU implementations for forward and backward propagation respectively, enabling efficient end-to-end learning with message passing.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xu-Fast%20and%20differentiable%20message%20passing%20on%20pairwise%20Markov%20random%20fields-2019-ACCV.pdf}
}
@article{AdaptiveSrivastavaBMVC2019, 
year = {2019}, 
title = {{Adaptive compression-based lifelong learning}}, 
author = {Srivastava, Shivangi and Berman, Maxim and Blaschko, Matthew B and Tuia, Devis}, 
journal = {BMVC}, 
eprint = {1907.09695}, 
abstract = {{The problem of a deep learning model losing performance on a previously learned task when fine-tuned to a new one is a phenomenon known as Catastrophic forgetting. There are two major ways to mitigate this problem: either preserving activations of the initial network during training with a new task; or restricting the new network activations to remain close to the initial ones. The latter approach falls under the denomination of lifelong learning, where the model is updated in a way that it performs well on both old and new tasks, without having access to the old task's training samples anymore. Recently, approaches like pruning networks for freeing network capacity during sequential learning of tasks have been gaining in popularity. Such approaches allow learning small networks while making redundant parameters available for the next tasks. The common problem encountered with these approaches is that the pruning percentage is hard-coded, irrespective of the number of samples, of the complexity of the learning task and of the number of classes in the dataset. We propose a method based on Bayesian optimization to perform adaptive compression/pruning of the network and show its effectiveness in lifelong learning. Our method learns to perform heavy pruning for small and/or simple datasets while using milder compression rates for large and/or complex data. Experiments on classification and semantic segmentation demonstrate the applicability of learning network compression, where we are able to effectively preserve performances along sequences of tasks of varying complexity.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Srivastava-Adaptive%20compression-based%20lifelong%20learning-2019-BMVC.pdf}
}
@article{DiverseMBestBatraECCV2012, 
year = {2012}, 
title = {{Diverse M-best solutions in Markov random fields}}, 
author = {Batra, Dhruv and Yadollahpour, Payman and Guzman-Rivera, Abner and Shakhnarovich, Gregory}, 
journal = {ECCV}, 
abstract = {{Much effort has been directed at algorithms for obtaining the highest probability (MAP) configuration in probabilistic (random field) models. In many situations, one could benefit from additional high-probability solutions. Current methods for computing the M most probable configurations produce solutions that tend to be very similar to the MAP solution and each other. This is often an undesirable property. In this paper we propose an algorithm for the Diverse M-Best problem, which involves finding a diverse set of highly probable solutions under a discrete probabilistic model. Given a dissimilarity function measuring closeness of two solutions, our formulation involves maximizing a linear combination of the probability and dissimilarity to previous solutions. Our formulation generalizes the M-Best MAP problem and we show that for certain families of dissimilarity functions we can guarantee that these solutions can be found as easily as the MAP solution.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Batra-Diverse%20M-best%20solutions%20in%20Markov%20random%20fields-2012-ECCV.pdf}
}
@article{SparseMatrixBixlerUAI2018, 
year = {2018}, 
title = {{Sparse-matrix belief propagation}}, 
author = {Bixler, Reid and Huang, Bert}, 
journal = {UAI}, 
abstract = {{We propose sparse-matrix belief propagation, which executes loopy belief propagation in pair- wise Markov random fields by replacing in- dexing over graph neighborhoods with sparse- matrix operations. This abstraction allows for seamless integration with optimized sparse lin- ear algebra libraries, including those that per- form matrix and tensor operations on modern hardware such as graphical processing units (GPUs). The sparse-matrix abstraction allows the implementation of belief propagation in a high-level language (e.g., Python) that is also able to leverage the power of GPU paralleliza- tion. We demonstrate sparse-matrix belief prop- agation by implementing it in a modern deep learning framework (PyTorch), measuring the resulting massive improvement in running time, and facilitating future integration into deep learning models.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bixler-Sparse-matrix%20belief%20propagation-2018-UAI.pdf}
}
@article{DynamicConvolutionVerelstCVPR2020, 
year = {2020}, 
title = {{Dynamic convolutions: Exploiting Spatial Sparsity for Faster Inference}}, 
author = {Verelst, Thomas and Tuytelaars, Tinne}, 
journal = {CVPR}, 
abstract = {{Modern convolutional neural networks apply the same operations on every pixel in an image. However, not all image regions are equally important. To address this inefficiency, we propose a method to dynamically apply convolutions conditioned on the input image. We introduce a residual block where a small gating branch learns which spatial positions should be evaluated. These discrete gating decisions are trained end-to-end using the Gumbel-Softmax trick, in combination with a sparsity criterion. Our experiments on CIFAR, ImageNet, Food-101 and MPII show that our method has better focus on the region of interest and better accuracy than existing methods, at a lower computational complexity. Moreover, we provide an efficient CUDA implementation of our dynamic convolutions using a gather-scatter approach, achieving a significant improvement in inference speed on MobileNetV2 and ShuffleNetV2. On human pose estimation, a task that is inherently spatially sparse, the processing speed is increased by 60\% with no loss in accuracy.}}, 
pages = {2317--2326}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Verelst-Dynamic%20convolutions-%20Exploiting%20Spatial%20Sparsity%20for%20Faster%20Inference-2020-CVPR.pdf}
}
@article{DARTS-PTWangICLR2021, 
year = {2021}, 
title = {{Rethinking architecture selection in differentiable NAS}}, 
author = {Wang, Ruochen and Cheng, Minhao and Chen, Xiangning and Tang, Xiaocheng and Hsieh, Cho-Jui}, 
journal = {ICLR}, 
abstract = {{Differentiable Neural Architecture Search is one of the most popular Neural Ar- chitecture Search (NAS) methods for its search efficiency and simplicity, accom- plished by jointly optimizing the model weight and architecture parameters in a weight-sharing supernet via gradient-based algorithms. At the end of the search phrase, the operations with the largest architecture parameters will be selected to form the final architecture, with the implicit assumption that the values of archi- tecture parameters reflect the operation strength. While much has been discussed about the supernet’s optimization, the architecture selection process has received little attention. We provide empirical and theoretical analysis to show that the magnitude of architecture parameters does not necessarily indicate how much the operation contributes to the supernet’s performance. We propose an alternative perturbation-based architecture selection that directly measures each operation’s influence on the supernet. We re-evaluate several differentiable NAS methods with the proposed architecture selection and find that it is able to extract significantly improved architectures from the underlying supernets consistently. Furthermore, we find that several failure modes of Darts can be greatly alleviated with the pro- posed selection method, indicating that much of the poor generalization observed in Darts can be attributed to the failure of magnitude-based architecture selec- tion rather than entirely the optimization of its supernet. Our code will be made publicly available shortly.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Rethinking%20architecture%20selection%20in%20differentiable%20NAS-2021-ICLR.pdf}
}
@article{DropNASHongIJCAI2020, 
year = {2020}, 
title = {{DropNAS: Grouped Operation Dropout for Differentiable Architecture Search}}, 
author = {Hong, Weijun and Li, Guilin and Zhang, Weinan and Tang, Ruiming and Wang, Yunhe and Li, Zhenguo and Yu, Yong}, 
journal = {IJCAI}, 
abstract = {{Neural architecture search (NAS) has shown encouraging results in automating the architecture design. Recently, DARTS relaxes the search process with a differentiable formulation that leverages weight-sharing and SGD for cost reduction of NAS. In DARTS, all candidate operations are trained simultaneously during the network weight training step. Our empirical results show that this training procedure leads to the co-adaption problem and Matthew Effect: operations with fewer parameters would be trained maturely earlier. This causes two problems: firstly, the operations with more parameters may never have the chance to express the desired function since those with less have already done the job; secondly, the system will punish those underperforming operations by lowering their architecture parameter and backward smaller loss gradients, this causes the Matthew Effect. In this paper, we systematically study these problems and propose a novel grouped operation dropout algorithm named DropNAS to fix the problems with DARTS. Extensive experiments demonstrate that DropNAS solves the above issues and achieves promising performance. Specifically, DropNAS achieves 2.26\% test error on CIFAR-10, 16.39\% on CIFAR-100 and 23.4\% on ImageNet (with the same training hyperparameters as DARTS for a fair comparison). It is also observed that DropNAS is robust across variants of the DARTS search space. Code is available at https://github.com/wiljohnhong/DropNAS.}}, 
pages = {2326--2332}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hong-DropNAS-%20Grouped%20Operation%20Dropout%20for%20Differentiable%20Architecture%20Search-2020-IJCAI.pdf}
}
@article{DARTS-ChuICLR2021, 
year = {2020}, 
title = {{DARTS-: Robustly Stepping out of Performance Collapse Without Indicators}}, 
author = {Chu, Xiangxiang and Wang, Xiaoxing and Zhang, Bo and Lu, Shun and Wei, Xiaolin and Yan, Junchi}, 
journal = {ICLR}, 
eprint = {2009.01027}, 
abstract = {{Despite the fast development of differentiable architecture search (DARTS), it suffers from long-standing performance instability, which extremely limits its application. Existing robustifying methods draw clues from the resulting deteriorated behavior instead of finding out its causing factor. Various indicators such as Hessian eigenvalues are proposed as a signal to stop searching before the performance collapses. However, these indicator-based methods tend to easily reject good architectures if the thresholds are inappropriately set, let alone the searching is intrinsically noisy. In this paper, we undertake a more subtle and direct approach to resolve the collapse. We first demonstrate that skip connections have a clear advantage over other candidate operations, where it can easily recover from a disadvantageous state and become dominant. We conjecture that this privilege is causing degenerated performance. Therefore, we propose to factor out this benefit with an auxiliary skip connection, ensuring a fairer competition for all operations. We call this approach DARTS-. Extensive experiments on various datasets verify that it can substantially improve robustness. Our code is available at https://github.com/Meituan-AutoML/DARTS- .}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chu-DARTS--%20Robustly%20Stepping%20out%20of%20Performance%20Collapse%20Without%20Indicators-2020-ICLR.pdf}
}
@article{PR-DARTSZhouNeurIPS2020, 
year = {2020}, 
title = {{Theory-inspired path-regularized differential network architecture search}}, 
author = {Zhou, Pan and Xiong, Caiming and Socher, Richard and Hoi, Steven C H}, 
journal = {NeurIPS}, 
eprint = {2006.16537}, 
abstract = {{Despite its high search efficiency, differential architecture search (DARTS) often selects network architectures with dominated skip connections which lead to performance degradation. However, theoretical understandings on this issue remain absent, hindering the development of more advanced methods in a principled way. In this work, we solve this problem by theoretically analyzing the effects of various types of operations, e.g. convolution, skip connection and zero operation, to the network optimization. We prove that the architectures with more skip connections can converge faster than the other candidates, and thus are selected by DARTS. This result, for the first time, theoretically and explicitly reveals the impact of skip connections to fast network optimization and its competitive advantage over other types of operations in DARTS. Then we propose a theory-inspired path-regularized DARTS that consists of two key modules: (i) a differential group-structured sparse binary gate introduced for each operation to avoid unfair competition among operations, and (ii) a path-depth-wise regularization used to incite search exploration for deep architectures that often converge slower than shallow ones as shown in our theory and are not well explored during the search. Experimental results on image classification tasks validate its advantages.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhou-Theory-inspired%20path-regularized%20differential%20network%20architecture%20search-2020-NeurIPS.pdf}
}
@article{CurveLane-NASXuECCV2020, 
year = {2020}, 
title = {{CurveLane-NAS: Unifying Lane-Sensitive Architecture Search and Adaptive Point Blending}}, 
author = {Xu, Hang and Wang, Shaoju and Cai, Xinyue and Zhang, Wei and Liang, Xiaodan and Li, Zhenguo}, 
journal = {ECCV}, 
eprint = {2007.12147}, 
abstract = {{We address the curve lane detection problem which poses more realistic challenges than conventional lane detection for better facilitating modern assisted/autonomous driving systems. Current hand-designed lane detection methods are not robust enough to capture the curve lanes especially the remote parts due to the lack of modeling both long-range contextual information and detailed curve trajectory. In this paper, we propose a novel lane-sensitive architecture search framework named CurveLane-NAS to automatically capture both long-ranged coherent and accurate short-range curve information while unifying both architecture search and post-processing on curve lane predictions via point blending. It consists of three search modules: a) a feature fusion search module to find a better fusion of the local and global context for multi-level hierarchy features; b) an elastic backbone search module to explore an efficient feature extractor with good semantics and latency; c) an adaptive point blending module to search a multi-level post-processing refinement strategy to combine multi-scale head prediction. The unified framework ensures lane-sensitive predictions by the mutual guidance between NAS and adaptive point blending. Furthermore, we also steer forward to release a more challenging benchmark named CurveLanes for addressing the most difficult curve lanes. It consists of 150K images with 680K labels.The new dataset can be downloaded at github.com/xbjxh/CurveLanes (already anonymized for this submission). Experiments on the new CurveLanes show that the SOTA lane detection methods suffer substantial performance drop while our model can still reach an 80+\% F1-score. Extensive experiments on traditional lane benchmarks such as CULane also demonstrate the superiority of our CurveLane-NAS, e.g. achieving a new SOTA 74.8\% F1-score on CULane.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xu-CurveLane-NAS-%20Unifying%20Lane-Sensitive%20Architecture%20Search%20and%20Adaptive%20Point%20Blending-2020-ECCV.pdf}
}
@article{TF-NASHuECCV2020, 
year = {2020}, 
title = {{TF-NAS: Rethinking Three Search Freedoms of Latency-Constrained Differentiable Neural Architecture Search}}, 
author = {Hu, Yibo and Wu, Xiang and He, Ran}, 
journal = {ECCV}, 
eprint = {2008.05314}, 
abstract = {{With the flourish of differentiable neural architecture search (NAS), automatically searching latency-constrained architectures gives a new perspective to reduce human labor and expertise. However, the searched architectures are usually suboptimal in accuracy and may have large jitters around the target latency. In this paper, we rethink three freedoms of differentiable NAS, i.e. operation-level, depth-level and width-level, and propose a novel method, named Three-Freedom NAS (TF-NAS), to achieve both good classification accuracy and precise latency constraint. For the operation-level, we present a bi-sampling search algorithm to moderate the operation collapse. For the depth-level, we introduce a sink-connecting search space to ensure the mutual exclusion between skip and other candidate operations, as well as eliminate the architecture redundancy. For the width-level, we propose an elasticity-scaling strategy that achieves precise latency constraint in a progressively fine-grained manner. Experiments on ImageNet demonstrate the effectiveness of TF-NAS. Particularly, our searched TF-NAS-A obtains 76.9\% top-1 accuracy, achieving state-of-the-art results with less latency. The total search time is only 1.8 days on 1 Titan RTX GPU. Code is available at https://github.com/AberHu/TF-NAS.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hu-TF-NAS-%20Rethinking%20Three%20Search%20Freedoms%20of%20Latency-Constrained%20Differentiable%20Neural%20Architecture%20Search-2020-ECCV.pdf}
}
@article{MorphNetGordonCVPR2020, 
year = {2020}, 
title = {{MorphNet: Fast \& Simple Resource-Constrained Structure Learning of Deep Networks}}, 
author = {Gordon, Ariel and Eban, Elad and Nachum, Ofir and Chen, Bo and Wu, Hao and Yang, Tien-Ju and Choi, Edward}, 
journal = {CVPR}, 
abstract = {{We present MorphNet, an approach to automate the design of neural network structures. MorphNet iteratively shrinks and expands a network, shrinking via a resource-weighted sparsifying regularizer on activations and expanding via a uniform multiplicative factor on all layers. In contrast to previous approaches, our method is scalable to large networks, adaptable to specific resource constraints (e.g. the number of floating-point operations per inference), and capable of increasing the network's performance. When applied to standard network architectures on a wide variety of datasets, our approach discovers novel structures in each domain, obtaining higher performance while respecting the resource constraint.}}, 
pages = {1586--1595}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Gordon-MorphNet-%20Fast%20&%20Simple%20Resource-Constrained%20Structure%20Learning%20of%20Deep%20Networks-2020-CVPR.pdf}
}
@article{BP-NASYuECCV2020, 
year = {2020}, 
title = {{Search what you want: Barrier Panelty NAS for Mixed Precision Quantization}}, 
author = {Yu, Haibao and Han, Qi and Li, Jianbo and Shi, Jianping and Cheng, Guangliang and Fan, Bin}, 
journal = {ECCV}, 
eprint = {2007.10026}, 
abstract = {{Emergent hardwares can support mixed precision CNN models inference that assign different bitwidths for different layers. Learning to find an optimal mixed precision model that can preserve accuracy and satisfy the specific constraints on model size and computation is extremely challenge due to the difficult in training a mixed precision model and the huge space of all possible bit quantizations. In this paper, we propose a novel soft Barrier Penalty based NAS (BP-NAS) for mixed precision quantization, which ensures all the searched models are inside the valid domain defined by the complexity constraint, thus could return an optimal model under the given constraint by conducting search only one time. The proposed soft Barrier Penalty is differentiable and can impose very large losses to those models outside the valid domain while almost no punishment for models inside the valid domain, thus constraining the search only in the feasible domain. In addition, a differentiable Prob-1 regularizer is proposed to ensure learning with NAS is reasonable. A distribution reshaping training strategy is also used to make training more stable. BP-NAS sets new state of the arts on both classification (Cifar-10, ImageNet) and detection (COCO), surpassing all the efficient mixed precision methods designed manually and automatically. Particularly, BP-NAS achieves higher mAP (up to 2.7\textbackslash\% mAP improvement) together with lower bit computation cost compared with the existing best mixed precision model on COCO detection.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yu-Search%20what%20you%20want-%20Barrier%20Panelty%20NAS%20for%20Mixed%20Precision%20Quantization-2020-ECCV.pdf}
}
@article{AM-LFSLiICCV2019, 
year = {2019}, 
title = {{AM-LFS: AutoML for Loss Function Search}}, 
author = {Li, Chuming and Yuan, Xin and Lin, Chen and Guo, Minghao and Wu, Wei and Yan, Junjie and Ouyang, Wanli}, 
journal = {ICCV}, 
abstract = {{Designing an effective loss function plays an important role in visual analysis. Most existing loss function designs rely on hand-crafted heuristics that require domain experts to explore the large design space, which is usually suboptimal and time-consuming. In this paper, we propose AutoML for Loss Function Search (AM-LFS) which leverages REINFORCE to search loss functions during the training process. The key contribution of this work is the design of search space which can guarantee the generalization and transferability on different vision tasks by including a bunch of existing prevailing loss functions in a unified formulation. We also propose an efficient optimization framework which can dynamically optimize the parameters of loss function’s distribution during training. Extensive experimental results on four benchmark datasets show that, without any tricks, our method outperforms existing hand-crafted loss functions in various computer vision tasks.}}, 
pages = {8409--8418}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-AM-LFS-%20AutoML%20for%20Loss%20Function%20Search-2019-ICCV.pdf}
}
@article{AMCHeECCV2018, 
year = {2018}, 
title = {{AMC: AutoML for Model Compression and Acceleration on Mobile Devices}}, 
author = {He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song}, 
journal = {ECCV}, 
eprint = {1802.03494}, 
abstract = {{Model compression is a critical technique to efficiently deploy neural network models on mobile devices which have limited computation resources and tight power budgets. Conventional model compression techniques rely on hand-crafted heuristics and rule-based policies that require domain experts to explore the large design space trading off among model size, speed, and accuracy, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Model Compression (AMC) which leverage reinforcement learning to provide the model compression policy. This learning-based compression policy outperforms conventional rule-based compression policy by having higher compression ratio, better preserving the accuracy and freeing human labor. Under 4x FLOPs reduction, we achieved 2.7\% better accuracy than the handcrafted model compression policy for VGG-16 on ImageNet. We applied this automated, push-the-button compression pipeline to MobileNet and achieved 1.81x speedup of measured inference latency on an Android phone and 1.43x speedup on the Titan XP GPU, with only 0.1\% loss of ImageNet Top-1 accuracy.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/He-AMC-%20AutoML%20for%20Model%20Compression%20and%20Acceleration%20on%20Mobile%20Devices-2018-ECCV.pdf}
}
@article{MCUNetLinNeurIPS2020, 
year = {2020}, 
title = {{MCUNet: Tiny Deep Learning on IoT Devices}}, 
author = {Lin, Ji and Chen, Wei-Ming and Lin, Yujun and Cohn, John and Gan, Chuang and Han, Song}, 
journal = {NeurIPS}, 
eprint = {2007.10319}, 
abstract = {{Machine learning on tiny IoT devices based on microcontroller units (MCU) is appealing but challenging: the memory of microcontrollers is 2-3 orders of magnitude smaller even than mobile phones. We propose MCUNet, a framework that jointly designs the efficient neural architecture (TinyNAS) and the lightweight inference engine (TinyEngine), enabling ImageNet-scale inference on microcontrollers. TinyNAS adopts a two-stage neural architecture search approach that first optimizes the search space to fit the resource constraints, then specializes the network architecture in the optimized search space. TinyNAS can automatically handle diverse constraints (i.e.device, latency, energy, memory) under low search costs.TinyNAS is co-designed with TinyEngine, a memory-efficient inference library to expand the search space and fit a larger model. TinyEngine adapts the memory scheduling according to the overall network topology rather than layer-wise optimization, reducing the memory usage by 4.8x, and accelerating the inference by 1.7-3.3x compared to TF-Lite Micro and CMSIS-NN. MCUNet is the first to achieves >70\% ImageNet top1 accuracy on an off-the-shelf commercial microcontroller, using 3.5x less SRAM and 5.7x less Flash compared to quantized MobileNetV2 and ResNet-18. On visual\&audio wake words tasks, MCUNet achieves state-of-the-art accuracy and runs 2.4-3.4x faster than MobileNetV2 and ProxylessNAS-based solutions with 3.7-4.1x smaller peak SRAM. Our study suggests that the era of always-on tiny machine learning on IoT devices has arrived. Code and models can be found here: https://tinyml.mit.edu.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lin-MCUNet-%20Tiny%20Deep%20Learning%20on%20IoT%20Devices-2020-NeurIPS.pdf}
}
@article{TinyTLCaiNeurIPS2020, 
year = {2020}, 
title = {{TinyTL: Reduce Activations, Not Trainable Parameters for Efficient On-Device Learning}}, 
author = {Cai, Han and Gan, Chuang and Zhu, Ligeng and Han, Song}, 
journal = {NeurIPS}, 
eprint = {2007.11622}, 
abstract = {{On-device learning enables edge devices to continually adapt the AI models to new data, which requires a small memory footprint to fit the tight memory constraint of edge devices. Existing work solves this problem by reducing the number of trainable parameters. However, this doesn't directly translate to memory saving since the major bottleneck is the activations, not parameters. In this work, we present Tiny-Transfer-Learning (TinyTL) for memory-efficient on-device learning. TinyTL freezes the weights while only learns the bias modules, thus no need to store the intermediate activations. To maintain the adaptation capacity, we introduce a new memory-efficient bias module, the lite residual module, to refine the feature extractor by learning small residual feature maps adding only 3.8\% memory overhead. Extensive experiments show that TinyTL significantly saves the memory (up to 6.5x) with little accuracy loss compared to fine-tuning the full network. Compared to fine-tuning the last layer, TinyTL provides significant accuracy improvements (up to 34.1\%) with little memory overhead. Furthermore, combined with feature extractor adaptation, TinyTL provides 7.3-12.9x memory saving without sacrificing accuracy compared to fine-tuning the full Inception-V3.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Cai-TinyTL-%20Reduce%20Activations,%20Not%20Trainable%20Parameters%20for%20Efficient%20On-Device%20Learning-2020-NeurIPS.pdf}
}
@article{DiffAugmentZhaoNeurIPS2020, 
year = {2020}, 
title = {{Differentiable augmentation for data-efficient GAN training}}, 
author = {Zhao, Shengyu and Liu, Zhijian and Lin, Ji and Zhu, Jun-Yan and Han, Song}, 
journal = {NeurIPS}, 
eprint = {2006.10738}, 
abstract = {{The performance of generative adversarial networks (GANs) heavily deteriorates given a limited amount of training data. This is mainly because the discriminator is memorizing the exact training set. To combat it, we propose Differentiable Augmentation (DiffAugment), a simple method that improves the data efficiency of GANs by imposing various types of differentiable augmentations on both real and fake samples. Previous attempts to directly augment the training data manipulate the distribution of real images, yielding little benefit; DiffAugment enables us to adopt the differentiable augmentation for the generated samples, effectively stabilizes training, and leads to better convergence. Experiments demonstrate consistent gains of our method over a variety of GAN architectures and loss functions for both unconditional and class-conditional generation. With DiffAugment, we achieve a state-of-the-art FID of 6.80 with an IS of 100.8 on ImageNet 128x128 and 2-4x reductions of FID given 1,000 images on FFHQ and LSUN. Furthermore, with only 20\% training data, we can match the top performance on CIFAR-10 and CIFAR-100. Finally, our method can generate high-fidelity images using only 100 images without pre-training, while being on par with existing transfer learning algorithms. Code is available at https://github.com/mit-han-lab/data-efficient-gans.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhao-Differentiable%20augmentation%20for%20data-efficient%20GAN%20training-2020-NeurIPS.pdf}
}
@article{FasterSegChenICLR2020, 
year = {2019}, 
title = {{FasterSeg: Searching for Faster Real-time Semantic Segmentation}}, 
author = {Chen, Wuyang and Gong, Xinyu and Liu, Xianming and Zhang, Qian and Li, Yuan and Wang, Zhangyang}, 
journal = {ICLR}, 
eprint = {1912.10917}, 
abstract = {{We present FasterSeg, an automatically designed semantic segmentation network with not only state-of-the-art performance but also faster speed than current methods. Utilizing neural architecture search (NAS), FasterSeg is discovered from a novel and broader search space integrating multi-resolution branches, that has been recently found to be vital in manually designed segmentation models. To better calibrate the balance between the goals of high accuracy and low latency, we propose a decoupled and fine-grained latency regularization, that effectively overcomes our observed phenomenons that the searched networks are prone to "collapsing" to low-latency yet poor-accuracy models. Moreover, we seamlessly extend FasterSeg to a new collaborative search (co-searching) framework, simultaneously searching for a teacher and a student network in the same single run. The teacher-student distillation further boosts the student model's accuracy. Experiments on popular segmentation benchmarks demonstrate the competency of FasterSeg. For example, FasterSeg can run over 30\% faster than the closest manually designed competitor on Cityscapes, while maintaining comparable accuracy.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-FasterSeg-%20Searching%20for%20Faster%20Real-time%20Semantic%20Segmentation-2019-ICLR.pdf}
}
@article{AssembleNetRyooICLR2020, 
year = {2019}, 
title = {{AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures}}, 
author = {Ryoo, Michael S and Piergiovanni, AJ and Tan, Mingxing and Angelova, Anelia}, 
journal = {ICLR}, 
eprint = {1905.13209}, 
abstract = {{Learning to represent videos is a very challenging task both algorithmically and computationally. Standard video CNN architectures have been designed by directly extending architectures devised for image understanding to include the time dimension, using modules such as 3D convolutions, or by using two-stream design to capture both appearance and motion in videos. We interpret a video CNN as a collection of multi-stream convolutional blocks connected to each other, and propose the approach of automatically finding neural architectures with better connectivity and spatio-temporal interactions for video understanding. This is done by evolving a population of overly-connected architectures guided by connection weight learning. Architectures combining representations that abstract different input types (i.e., RGB and optical flow) at multiple temporal resolutions are searched for, allowing different types or sources of information to interact with each other. Our method, referred to as AssembleNet, outperforms prior approaches on public video datasets, in some cases by a great margin. We obtain 58.6\% mAP on Charades and 34.27\% accuracy on Moments-in-Time.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ryoo-AssembleNet-%20Searching%20for%20Multi-Stream%20Neural%20Connectivity%20in%20Video%20Architectures-2019-ICLR.pdf}
}
@article{T-NASZhengICLR2020, 
year = {2020}, 
title = {{Towards fast adaptation of neural architectures with meta learning}}, 
author = {Lian, Dongze and Zheng, Yin and Xu, Yintao and Lu, Yanxiong and Lin, Leyu and Zhao, Peilin and Huang, Junzhou and Gao, Shenghua}, 
journal = {ICLR}, 
abstract = {{Recently, Neural Architecture Search (NAS) has been successfully applied to mul- tiple artificial intelligence areas and shows better performance compared with hand-designed networks. However, the existing NAS methods only target a spe- cific task. Most of them usually do well in searching an architecture for single task but are troublesome for multiple datasets or multiple tasks. Generally, the ar- chitecture for a new task is either searched from scratch, which is neither efficient nor flexible enough for practical application scenarios, or borrowed from the ones searched on other tasks, which might be not optimal. In order to tackle the trans- ferability of NAS and conduct fast adaptation of neural architectures, we propose a novel Transferable Neural Architecture Search method based on meta-learning in this paper, which is termed as T-NAS. T-NAS learns a meta-architecture that is able to adapt to a new task quickly through a few gradient steps, which makes the transferred architecture suitable for the specific task. Extensive experiments show that T-NAS achieves state-of-the-art performance in few-shot learning and comparable performance in supervised learning but with 50x less searching cost, which demonstrates the effectiveness of our method.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lian-Towards%20fast%20adaptation%20of%20neural%20architectures%20with%20meta%20learning-2020-ICLR.pdf}
}
@article{MTL-NASGaoCVPR2020, 
year = {2020}, 
title = {{MTL-NAS: Task-Agnostic Neural Architecture Search towards General-Purpose Multi-Task Learning}}, 
author = {Gao, Yuan and Bai, Haoping and Jie, Zequn and Ma, Jiayi and Jia, Kui and Liu, Wei}, 
journal = {CVPR}, 
abstract = {{We propose to incorporate neural architecture search (NAS) into general-purpose multi-task learning (GP-MTL). Existing NAS methods typically define different search spaces according to different tasks. In order to adapt to different task combinations (i.e., task sets), we disentangle the GP-MTL networks into single-task backbones (option-ally encode the task priors), and a hierarchical and layer-wise features sharing/fusing scheme across them. This enables us to design a novel and general task-agnostic search space, which inserts cross-task edges (i.e., feature fusion connections) into fixed single-task network backbones. Moreover, we also propose a novel single-shot gradient-based search algorithm that closes the performance gap between the searched architectures and the final evaluation architecture. This is realized with a minimum entropy regularization on the architecture weights during the search phase, which makes the architecture weights converge to near-discrete values and therefore achieves a single model. As a result, our searched model can be directly used for evaluation without (re-)training from scratch. We perform extensive experiments using different single-task backbones on various task sets, demonstrating the promising performance obtained by exploiting the hierarchical and layerwise features, as well as the desirable generalizability to different i) task sets and ii) single-task backbones. The code of our paper is available at https://github.com/bhpfelix/MTLNAS.}}, 
pages = {11540--11549}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Gao-MTL-NAS-%20Task-Agnostic%20Neural%20Architecture%20Search%20towards%20General-Purpose%20Multi-Task%20Learning-2020-CVPR.pdf}
}
@article{P3DQiuICCV2017, 
year = {2017}, 
title = {{Learning spatio-temporal representation with pseudo-3D residual networks}}, 
author = {Qiu, Zhaofan and Yao, Ting and Mei, Tao}, 
journal = {ICCV}, 
abstract = {{Convolutional Neural Networks (CNN) have been regarded as a powerful class of models for image recognition problems. Nevertheless, it is not trivial when utilizing a CNN for learning spatio-temporal video representation. A few studies have shown that performing 3D convolutions is a rewarding approach to capture both spatial and temporal dimensions in videos. However, the development of a very deep 3D CNN from scratch results in expensive computational cost and memory demand. A valid question is why not recycle off-the-shelf 2D networks for a 3D CNN. In this paper, we devise multiple variants of bottleneck building blocks in a residual learning framework by simulating \$3\textbackslashtimes3\textbackslashtimes3\$ convolutions with \$1\textbackslashtimes3\textbackslashtimes3\$ convolutional filters on spatial domain (equivalent to 2D CNN) plus \$3\textbackslashtimes1\textbackslashtimes1\$ convolutions to construct temporal connections on adjacent feature maps in time. Furthermore, we propose a new architecture, named Pseudo-3D Residual Net (P3D ResNet), that exploits all the variants of blocks but composes each in different placement of ResNet, following the philosophy that enhancing structural diversity with going deep could improve the power of neural networks. Our P3D ResNet achieves clear improvements on Sports-1M video classification dataset against 3D CNN and frame-based 2D CNN by 5.3\% and 1.8\%, respectively. We further examine the generalization performance of video representation produced by our pre-trained P3D ResNet on five different benchmarks and three different tasks, demonstrating superior performances over several state-of-the-art techniques.}}, 
pages = {5534--5542}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Qiu-Learning%20spatio-temporal%20representation%20with%20pseudo-3D%20residual%20networks-2017-ICCV.pdf}
}
@article{V-NASZhu3DV2019, 
year = {2019}, 
title = {{V-NAS: Neural Architecture Search for Volumetric Medical Image Segmentation}}, 
author = {Zhu, Zhuotun and Liu, Chenxi and Yang, Dong and Yuille, Alan and Xu, Daguang}, 
journal = {3DV}, 
abstract = {{Deep learning algorithms, in particular 2D and 3D fully convolutional neural networks (FCNs), have rapidly become the mainstream methodology for volumetric medical image segmentation. However, 2D convolutions cannot fully leverage the rich spatial information along the third axis, while 3D convolutions suffer from the demanding computation and high GPU memory consumption. In this paper, we propose to automatically search the network architecture tailoring to volumetric medical image segmentation problem. Concretely, we formulate the structure learning as differentiable neural architecture search, and let the network itself choose between 2D, 3D or Pseudo-3D (P3D) convolutions at each layer. We evaluate our method on 3 public datasets, i.e., the NIH Pancreas dataset, the Lung and Pancreas dataset from the Medical Segmentation Decathlon (MSD) Challenge. Our method, named V-NAS, consistently outperforms other state-of-the-arts on the segmentation tasks of both normal organ (NIH Pancreas) and abnormal organs (MSD Lung tumors and MSD Pancreas tumors), which shows the power of chosen architecture. Moreover, the searched architecture on one dataset can be well generalized to other datasets, which demonstrates the robustness and practical use of our proposed method.}}, 
pages = {240--248}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhu-V-NAS-%20Neural%20Architecture%20Search%20for%20Volumetric%20Medical%20Image%20Segmentation-2019-3DV.pdf}
}
@article{MDCWeiCVPR2018, 
year = {2018}, 
title = {{Revisiting dilated convolution: A Simple Approach for Weakly- and Semi-Supervised Semantic Segmentation}}, 
author = {Wei, Yunchao and Xiao, Huaxin and Shi, Honghui and Jie, Zequn and Feng, Jiashi and Huang, Thomas S.}, 
journal = {CVPR}, 
abstract = {{Despite the remarkable progress, weakly supervised segmentation approaches are still inferior to their fully supervised counterparts. We obverse the performance gap mainly comes from their limitation on learning to produce high-quality dense object localization maps from image-level supervision. To mitigate such a gap, we revisit the dilated convolution [1] and reveal how it can be utilized in a novel way to effectively overcome this critical limitation of weakly supervised segmentation approaches. Specifically, we find that varying dilation rates can effectively enlarge the receptive fields of convolutional kernels and more importantly transfer the surrounding discriminative information to non-discriminative object regions, promoting the emergence of these regions in the object localization maps. Then, we design a generic classification network equipped with convolutional blocks of different dilated rates. It can produce dense and reliable object localization maps and effectively benefit both weakly- and semi-supervised semantic segmentation. Despite the apparent simplicity, our proposed approach obtains superior performance over state-of-the-arts. In particular, it achieves 60.8\% and 67.6\% mIoU scores on Pascal VOC 2012 test set in weakly- (only image-level labels are available) and semi- (1,464 segmentation masks are available) supervised settings, which are the new state-of-the-arts.}}, 
pages = {7268--7277}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wei-Revisiting%20dilated%20convolution-%20A%20Simple%20Approach%20for%20Weakly-%20and%20Semi-Supervised%20Semantic%20Segmentation-2018-CVPR.pdf}
}
@article{AdversarialRobustnessChenCVPR2020, 
year = {2020}, 
title = {{Adversarial robustness: From Self-Supervised Pre-Training to Fine-Tuning}}, 
author = {Chen, Tianlong and Liu, Sijia and Chang, Shiyu and Cheng, Yu and Amini, Lisa and Wang, Zhangyang}, 
journal = {CVPR}, 
abstract = {{Pretrained models from self-supervision are prevalently used in fine-tuning downstream tasks faster or for better accuracy. However, gaining robustness from pretraining is left unexplored. We introduce adversarial training into self-supervision, to provide general-purpose robust pretrained models for the first time. We find these robust pretrained models can benefit the subsequent fine-tuning in two ways: i) boosting final model robustness; ii) saving the computation cost, if proceeding towards adversarial fine-tuning. We conduct extensive experiments to demonstrate that the proposed framework achieves large performance margins (e.g., 3.83\% on robust accuracy and 1.3\% on standard accuracy, on the CIFAR-10 dataset), compared with the conventional end-to-end adversarial training baseline. Moreover, we find that different self-supervised pretrained models have diverse adversarial vulnerability. It inspires us to ensemble several pretraining tasks, which boosts robustness more. Our ensemble strategy contributes to a further improvement of 3.59\% on robust accuracy, while maintaining a slightly higher standard accuracy on CIFAR-10. Our codes are available at https://github.com/TAMU-VITA/Adv-SS-Pretraining.}}, 
pages = {696--705}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Adversarial%20robustness-%20From%20Self-Supervised%20Pre-Training%20to%20Fine-Tuning-2020-CVPR.pdf}
}
@article{AutoAugmentCubukCVPR2019, 
year = {2018}, 
title = {{AutoAugment: Learning Augmentation Policies from Data}}, 
author = {Cubuk, Ekin D and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V}, 
journal = {CVPR}, 
eprint = {1805.09501}, 
abstract = {{Data augmentation is an effective technique for improving the accuracy of modern image classifiers. However, current data augmentation implementations are manually designed. In this paper, we describe a simple procedure called AutoAugment to automatically search for improved data augmentation policies. In our implementation, we have designed a search space where a policy consists of many sub-policies, one of which is randomly chosen for each image in each mini-batch. A sub-policy consists of two operations, each operation being an image processing function such as translation, rotation, or shearing, and the probabilities and magnitudes with which the functions are applied. We use a search algorithm to find the best policy such that the neural network yields the highest validation accuracy on a target dataset. Our method achieves state-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without additional data). On ImageNet, we attain a Top-1 accuracy of 83.5\% which is 0.4\% better than the previous record of 83.1\%. On CIFAR-10, we achieve an error rate of 1.5\%, which is 0.6\% better than the previous state-of-the-art. Augmentation policies we find are transferable between datasets. The policy learned on ImageNet transfers well to achieve significant improvements on other datasets, such as Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Cubuk-AutoAugment-%20Learning%20Augmentation%20Policies%20from%20Data-2018-CVPR.pdf}
}
@article{AdversarialAutoAugmentZhangICLR2020, 
year = {2019}, 
title = {{Adversarial AutoAugment}}, 
author = {Zhang, Xinyu and Wang, Qiang and Zhang, Jian and Zhong, Zhao}, 
journal = {ICLR}, 
eprint = {1912.11188}, 
abstract = {{Data augmentation (DA) has been widely utilized to improve generalization in training deep neural networks. Recently, human-designed data augmentation has been gradually replaced by automatically learned augmentation policy. Through finding the best policy in well-designed search space of data augmentation, AutoAugment can significantly improve validation accuracy on image classification tasks. However, this approach is not computationally practical for large-scale problems. In this paper, we develop an adversarial method to arrive at a computationally-affordable solution called Adversarial AutoAugment, which can simultaneously optimize target related object and augmentation policy search loss. The augmentation policy network attempts to increase the training loss of a target network through generating adversarial augmentation policies, while the target network can learn more robust features from harder examples to improve the generalization. In contrast to prior work, we reuse the computation in target network training for policy evaluation, and dispense with the retraining of the target network. Compared to AutoAugment, this leads to about 12x reduction in computing cost and 11x shortening in time overhead on ImageNet. We show experimental results of our approach on CIFAR-10/CIFAR-100, ImageNet, and demonstrate significant performance improvements over state-of-the-art. On CIFAR-10, we achieve a top-1 test error of 1.36\%, which is the currently best performing single model. On ImageNet, we achieve a leading performance of top-1 accuracy 79.40\% on ResNet-50 and 80.00\% on ResNet-50-D without extra data.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Adversarial%20AutoAugment-2019-ICLR.pdf}
}
@article{meta-VIZhangAISTATS2021, 
year = {2020}, 
title = {{Meta-learning for variational inference}}, 
author = {Zhang, Ruqi and Li, Yingzhen and Sa, Christopher De and Devlin, Sam and Zhang, Cheng}, 
journal = {AISTATS}, 
eprint = {2007.02912}, 
abstract = {{Variational inference (VI) plays an essential role in approximate Bayesian inference due to its computational efficiency and broad applicability. Crucial to the performance of VI is the selection of the associated divergence measure, as VI approximates the intractable distribution by minimizing this divergence. In this paper we propose a meta-learning algorithm to learn the divergence metric suited for the task of interest, automating the design of VI methods. In addition, we learn the initialization of the variational parameters without additional cost when our method is deployed in the few-shot learning scenarios. We demonstrate our approach outperforms standard VI on Gaussian mixture distribution approximation, Bayesian neural network regression, image generation with variational autoencoders and recommender systems with a partial variational autoencoder.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Meta-learning%20for%20variational%20inference-2020-AISTATS.pdf}
}
@article{EnsuringSaICML2016, 
year = {2016}, 
title = {{Ensuring rapid mixing and low bias for asynchronous Gibbs sampling}}, 
author = {Sa, Christopher De and Olukotun, Kunle and Ré, Christopher}, 
journal = {ICML}, 
eprint = {1602.07415}, 
abstract = {{Gibbs sampling is a Markov chain Monte Carlo technique commonly used for estimating marginal distributions. To speed up Gibbs sampling, there has recently been interest in parallelizing it by executing asynchronously. While empirical results suggest that many models can be efficiently sampled asynchronously, traditional Markov chain analysis does not apply to the asynchronous case, and thus asynchronous Gibbs sampling is poorly understood. In this paper, we derive a better understanding of the two main challenges of asynchronous Gibbs: bias and mixing time. We show experimentally that our theoretical results match practical outcomes.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Sa-Ensuring%20rapid%20mixing%20and%20low%20bias%20for%20asynchronous%20Gibbs%20sampling-2016-ICML.pdf}
}
@article{MAMLFinnICML2017, 
year = {2017}, 
title = {{Model-agnostic meta-learning for fast adaptation of deep networks}}, 
author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey}, 
journal = {ICML}, 
eprint = {1703.03400}, 
abstract = {{We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Finn-Model-agnostic%20meta-learning%20for%20fast%20adaptation%20of%20deep%20networks-2017-ICML.pdf}
}
@article{CR-NASLiangICLR2020, 
year = {2019}, 
title = {{Computation reallocation for object detection}}, 
author = {Liang, Feng and Lin, Chen and Guo, Ronghao and Sun, Ming and Wu, Wei and Yan, Junjie and Ouyang, Wanli}, 
journal = {ICLR}, 
eprint = {1912.11234}, 
abstract = {{The allocation of computation resources in the backbone is a crucial issue in object detection. However, classification allocation pattern is usually adopted directly to object detector, which is proved to be sub-optimal. In order to reallocate the engaged computation resources in a more efficient way, we present CR-NAS (Computation Reallocation Neural Architecture Search) that can learn computation reallocation strategies across different feature resolution and spatial position diectly on the target detection dataset. A two-level reallocation space is proposed for both stage and spatial reallocation. A novel hierarchical search procedure is adopted to cope with the complex search space. We apply CR-NAS to multiple backbones and achieve consistent improvements. Our CR-ResNet50 and CR-MobileNetV2 outperforms the baseline by 1.9\% and 1.7\% COCO AP respectively without any additional computation budget. The models discovered by CR-NAS can be equiped to other powerful detection neck/head and be easily transferred to other dataset, e.g. PASCAL VOC, and other vision tasks, e.g. instance segmentation. Our CR-NAS can be used as a plugin to improve the performance of various networks, which is demanding.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liang-Computation%20reallocation%20for%20object%20detection-2019-ICLR.pdf}
}
@article{HowHongICLR2020, 
year = {2020}, 
title = {{How to 0wn NAS in your spare time}}, 
author = {Hong, Sanghyun and Davinroy, Michael and Kaya, Yiğitcan and Dachman-Soled, Dana and Dumitraş, Tudor}, 
journal = {ICLR}, 
eprint = {2002.06776}, 
abstract = {{New data processing pipelines and novel network architectures increasingly drive the success of deep learning. In consequence, the industry considers top-performing architectures as intellectual property and devotes considerable computational resources to discovering such architectures through neural architecture search (NAS). This provides an incentive for adversaries to steal these novel architectures; when used in the cloud, to provide Machine Learning as a Service, the adversaries also have an opportunity to reconstruct the architectures by exploiting a range of hardware side channels. However, it is challenging to reconstruct novel architectures and pipelines without knowing the computational graph (e.g., the layers, branches or skip connections), the architectural parameters (e.g., the number of filters in a convolutional layer) or the specific pre-processing steps (e.g. embeddings). In this paper, we design an algorithm that reconstructs the key components of a novel deep learning system by exploiting a small amount of information leakage from a cache side-channel attack, Flush+Reload. We use Flush+Reload to infer the trace of computations and the timing for each computation. Our algorithm then generates candidate computational graphs from the trace and eliminates incompatible candidates through a parameter estimation process. We implement our algorithm in PyTorch and Tensorflow. We demonstrate experimentally that we can reconstruct MalConv, a novel data pre-processing pipeline for malware detection, and ProxylessNAS- CPU, a novel network architecture for the ImageNet classification optimized to run on CPUs, without knowing the architecture family. In both cases, we achieve 0\% error. These results suggest hardware side channels are a practical attack vector against MLaaS, and more efforts should be devoted to understanding their impact on the security of deep learning systems.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hong-How%20to%200wn%20NAS%20in%20your%20spare%20time-2020-ICLR.pdf}
}
@article{WhyDoDemontisUSENIX2019, 
year = {2018}, 
title = {{Why do adversarial attacks transfer? Explaining transferability of evasion and poisoning attacks}}, 
author = {Demontis, Ambra and Melis, Marco and Pintor, Maura and Jagielski, Matthew and Biggio, Battista and Oprea, Alina and Nita-Rotaru, Cristina and Roli, Fabio}, 
journal = {USENIX}, 
eprint = {1809.02861}, 
abstract = {{Transferability captures the ability of an attack against a machine-learning model to be effective against a different, potentially unknown, model. Empirical evidence for transferability has been shown in previous work, but the underlying reasons why an attack transfers or not are not yet well understood. In this paper, we present a comprehensive analysis aimed to investigate the transferability of both test-time evasion and training-time poisoning attacks. We provide a unifying optimization framework for evasion and poisoning attacks, and a formal definition of transferability of such attacks. We highlight two main factors contributing to attack transferability: the intrinsic adversarial vulnerability of the target model, and the complexity of the surrogate model used to optimize the attack. Based on these insights, we define three metrics that impact an attack's transferability. Interestingly, our results derived from theoretical analysis hold for both evasion and poisoning attacks, and are confirmed experimentally using a wide range of linear and non-linear classifiers and datasets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Demontis-Why%20do%20adversarial%20attacks%20transfer-%20Explaining%20transferability%20of%20evasion%20and%20poisoning%20attacks-2018-USENIX.pdf}
}
@article{SearchingShenICCVWorkshop, 
year = {2019}, 
title = {{Searching for accurate binary neural architectures}}, 
author = {Shen, Mingzhu and Han, Kai and Xu, Chunjing and Wang, Yunhe}, 
journal = {ICCV Workshop}, 
abstract = {{Binary neural networks have attracted tremendous attention due to the efficiency for deploying them on mobile devices. Since the weak expression ability of binary weights and features, their accuracy is usually much lower than that of full-precision (i.e. 32-bit) models. Here we present a new frame work for automatically searching for compact but accurate binary neural networks. In practice, number of channels in each layer will be encoded into the search space and optimized using the evolutionary algorithm. Experiments conducted on benchmark datasets and neural architectures demonstrate that our searched binary networks can achieve the performance of full-precision models with acceptable increments on model sizes and calculations.}}, 
pages = {2041--2044}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Shen-Searching%20for%20accurate%20binary%20neural%20architectures-2019-ICCV%20Workshop.pdf}
}
@article{IR-NetQinCVPR2020, 
year = {2020}, 
title = {{Forward and backward information retention for accurate binary neural networks}}, 
author = {Qin, Haotong and Gong, Ruihao and Liu, Xianglong and Shen, Mingzhu and Wei, Ziran and Yu, Fengwei and Song, Jingkuan}, 
journal = {CVPR}, 
abstract = {{Weight and activation binarization is an effective approach to deep neural network compression and can accelerate the inference by leveraging bitwise operations. Although many binarization methods have improved the accuracy of the model by minimizing the quantization error in forward propagation, there remains a noticeable performance gap between the binarized model and the full-precision one. Our empirical study indicates that the quantization brings information loss in both forward and backward propagation, which is the bottleneck of training accurate binary neural networks. To address these issues, we propose an Information Retention Network (IR-Net) to retain the information that consists in the forward activations and backward gradients. IR-Net mainly relies on two technical contributions: (1) Libra Parameter Binarization (Libra-PB): simultaneously minimizing both quantization error and information loss of parameters by balanced and standardized weights in forward propagation; (2) Error Decay Estimator (EDE): minimizing the information loss of gradients by gradually approximating the sign function in backward propagation, jointly considering the updating ability and accurate gradients. We are the first to investigate both forward and backward processes of binary networks from the unified information perspective, which provides new insight into the mechanism of network binarization. Comprehensive experiments with various network structures on CIFAR-10 and ImageNet datasets manifest that the proposed IR-Net can consistently outperform state-of-the-art quantization methods.}}, 
pages = {2247--2256}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Qin-Forward%20and%20backward%20information%20retention%20for%20accurate%20binary%20neural%20networks-2020-CVPR.pdf}
}
@article{Bi-realNetLiuECCV2018, 
year = {2018}, 
title = {{Bi-real net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm}}, 
author = {Liu, Zechun and Wu, Baoyuan and Luo, Wenhan and Yang, Xin and Liu, Wei and Cheng, Kwang-Ting}, 
journal = {ECCV}, 
eprint = {1808.00278}, 
abstract = {{In this work, we study the 1-bit convolutional neural networks (CNNs), of which both the weights and activations are binary. While being efficient, the classification accuracy of the current 1-bit CNNs is much worse compared to their counterpart real-valued CNN models on the large-scale dataset, like ImageNet. To minimize the performance gap between the 1-bit and real-valued CNN models, we propose a novel model, dubbed Bi-Real net, which connects the real activations (after the 1-bit convolution and/or BatchNorm layer, before the sign function) to activations of the consecutive block, through an identity shortcut. Consequently, compared to the standard 1-bit CNN, the representational capability of the Bi-Real net is significantly enhanced and the additional cost on computation is negligible. Moreover, we develop a specific training algorithm including three technical novelties for 1- bit CNNs. Firstly, we derive a tight approximation to the derivative of the non-differentiable sign function with respect to activation. Secondly, we propose a magnitude-aware gradient with respect to the weight for updating the weight parameters. Thirdly, we pre-train the real-valued CNN model with a clip function, rather than the ReLU function, to better initialize the Bi-Real net. Experiments on ImageNet show that the Bi-Real net with the proposed training algorithm achieves 56.4\% and 62.2\% top-1 accuracy with 18 layers and 34 layers, respectively. Compared to the state-of-the-arts (e.g., XNOR Net), Bi-Real net achieves up to 10\% higher top-1 accuracy with more memory saving and lower computational cost. Keywords: binary neural network, 1-bit CNNs, 1-layer-per-block}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-Bi-real%20net-%20Enhancing%20the%20Performance%20of%201-bit%20CNNs%20With%20Improved%20Representational%20Capability%20and%20Advanced%20Training%20Algorithm-2018-ECCV.pdf}
}
@article{Gumbel-SoftmaxJangICLR2017, 
year = {2016}, 
title = {{Categorical reparameterization with Gumbel-softmax}}, 
author = {Jang, Eric and Gu, Shixiang and Poole, Ben}, 
journal = {ICLR}, 
eprint = {1611.01144}, 
abstract = {{Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Jang-Categorical%20reparameterization%20with%20Gumbel-softmax-2016-ICLR.pdf}
}
@article{CONCRETEMaddisonICLR2017, 
year = {2017}, 
title = {{The concrete distribution: A Continuous Relaxation of Discrete Random Variables}}, 
author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye}, 
journal = {ICLR}, 
abstract = {{The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce CONCRETE random variables—CONtinuous relaxations of disCRETE random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Maddison-The%20concrete%20distribution-%20A%20Continuous%20Relaxation%20of%20Discrete%20Random%20Variables-2017-ICLR.pdf}
}
@article{GRMCPaulusICLR2021, 
year = {2020}, 
title = {{Rao-Blackwellizing the straight-through Gumbel-softmax gradient estimator}}, 
author = {Paulus, Max B and Maddison, Chris J and Krause, Andreas}, 
journal = {ICLR}, 
eprint = {2010.04838}, 
abstract = {{Gradient estimation in models with discrete latent variables is a challenging problem, because the simplest unbiased estimators tend to have high variance. To counteract this, modern estimators either introduce bias, rely on multiple function evaluations, or use learned, input-dependent baselines. Thus, there is a need for estimators that require minimal tuning, are computationally cheap, and have low mean squared error. In this paper, we show that the variance of the straight-through variant of the popular Gumbel-Softmax estimator can be reduced through Rao-Blackwellization without increasing the number of function evaluations. This provably reduces the mean squared error. We empirically demonstrate that this leads to variance reduction, faster convergence, and generally improved performance in two unsupervised latent variable models.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Paulus-Rao-Blackwellizing%20the%20straight-through%20Gumbel-softmax%20gradient%20estimator-2020-ICLR.pdf}
}
@article{AdamKingmaICLR2015, 
year = {2014}, 
title = {{Adam: A Method for Stochastic Optimization}}, 
author = {Kingma, Diederik P and Ba, Jimmy}, 
journal = {ICLR}, 
eprint = {1412.6980}, 
abstract = {{We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kingma-Adam-%20A%20Method%20for%20Stochastic%20Optimization-2014-ICLR.pdf}
}
@article{BinaryConnectCourbariauxNeurIPS2015, 
year = {2015}, 
title = {{BinaryConnect: Training Deep Neural Networks with Binary Weights during Propagations}}, 
author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre}, 
journal = {NeurIPS}, 
eprint = {1511.00363}, 
abstract = {{Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and power-hungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Courbariaux-BinaryConnect-%20Training%20Deep%20Neural%20Networks%20with%20Binary%20Weights%20during%20Propagations-2015-NeurIPS.pdf}
}
@article{ASNG-NASAkimotoICML2019, 
year = {2019}, 
title = {{Adaptive Stochastic Natural Gradient Method for One-Shot Neural Architecture Search}}, 
author = {Akimoto, Youhei and Shirakawa, Shinichi and Yoshinari, Nozomu and Uchida, Kento and Saito, Shota and Nishida, Kouhei}, 
journal = {ICML}, 
eprint = {1905.08537}, 
abstract = {{High sensitivity of neural architecture search (NAS) methods against their input such as step-size (i.e., learning rate) and search space prevents practitioners from applying them out-of-the-box to their own problems, albeit its purpose is to automate a part of tuning process. Aiming at a fast, robust, and widely-applicable NAS, we develop a generic optimization framework for NAS. We turn a coupled optimization of connection weights and neural architecture into a differentiable optimization by means of stochastic relaxation. It accepts arbitrary search space (widely-applicable) and enables to employ a gradient-based simultaneous optimization of weights and architecture (fast). We propose a stochastic natural gradient method with an adaptive step-size mechanism built upon our theoretical investigation (robust). Despite its simplicity and no problem-dependent parameter tuning, our method exhibited near state-of-the-art performances with low computational budgets both on image classification and inpainting tasks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Akimoto-Adaptive%20Stochastic%20Natural%20Gradient%20Method%20for%20One-Shot%20Neural%20Architecture%20Search-2019-ICML.pdf}
}
@article{Auto-ReIDQuanICCV2019, 
year = {2019}, 
title = {{Auto-ReID: Searching for a Part-Aware ConvNet for Person Re-Identification}}, 
author = {Quan, Ruijie and Dong, Xuanyi and Wu, Yu and Zhu, Linchao and Yang, Yi}, 
journal = {ICCV}, 
abstract = {{Prevailing deep convolutional neural networks (CNNs) for person re-IDentification (reID) are usually built upon ResNet or VGG backbones, which were originally designed for classification. Because reID is different from classification, the architecture should be modified accordingly. We propose to automatically search for a CNN architecture that is specifically suitable for the reID task. There are three aspects to be tackled. First, body structural information plays an important role in reID but it is not encoded in backbones. Second, Neural Architecture Search (NAS) automates the process of architecture design without human effort, but no existing NAS methods incorporate the structure information of input images. Third, reID is essentially a retrieval task but current NAS algorithms are merely designed for classification. To solve these problems, we propose a retrieval-based search algorithm over a specifically designed reID search space, named Auto-ReID. Our Auto-ReID enables the automated approach to find an efficient and effective CNN architecture for reID. Extensive experiments demonstrate that the searched architecture achieves state-of-the-art performance while reducing 50\% parameters and 53\% FLOPs compared to others.}}, 
pages = {3749--3758}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Quan-Auto-ReID-%20Searching%20for%20a%20Part-Aware%20ConvNet%20for%20Person%20Re-Identification-2019-ICCV.pdf}
}
@article{OpeningSchwartz-ZivarXiv2017, 
year = {2017}, 
title = {{Opening the Black Box of Deep Neural Networks via Information}}, 
author = {Shwartz-Ziv, Ravid and Tishby, Naftali}, 
journal = {arXiv}, 
eprint = {1703.00810}, 
abstract = {{Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work proposed to analyze DNNs in the \textbackslashtextit\{Information Plane\}; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. Our main results are: (i) most of the training epochs in standard DL are spent on \{\textbackslashemph compression\} of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the IB self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Shwartz-Ziv-Opening%20the%20Black%20Box%20of%20Deep%20Neural%20Networks%20via%20Information-2017-arXiv.pdf}
}
@article{DeepTishbyITWorkshop2015, 
year = {2015}, 
title = {{Deep learning and the information bottleneck principle}}, 
author = {Tishby, Naftali and Zaslavsky, Noga}, 
journal = {IT Workshop}, 
abstract = {{Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.}}, 
pages = {1--5}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tishby-Deep%20learning%20and%20the%20information%20bottleneck%20principle-2015-IT%20Workshop.pdf}
}
@article{UnderstandingZhangICLR2017, 
year = {2016}, 
title = {{Understanding deep learning requires rethinking generalization}}, 
author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol}, 
journal = {ICLR}, 
eprint = {1611.03530}, 
abstract = {{Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Understanding%20deep%20learning%20requires%20rethinking%20generalization-2016-ICLR.pdf}
}
@article{RethinkingLiuICLR2019, 
year = {2018}, 
title = {{Rethinking the value of network pruning}}, 
author = {Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor}, 
journal = {ICLR}, 
eprint = {1810.05270}, 
abstract = {{Network pruning is widely used for reducing the heavy inference cost of deep models in low-resource settings. A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning. During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all state-of-the-art structured pruning algorithms we examined, fine-tuning a pruned model only gives comparable or worse performance than training that model with randomly initialized weights. For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch. Our observations are consistent for multiple network architectures, datasets, and tasks, which imply that: 1) training a large, over-parameterized model is often not necessary to obtain an efficient final model, 2) learned "important" weights of the large model are typically not useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited "important" weights, is more crucial to the efficiency in the final model, which suggests that in some cases pruning can be useful as an architecture search paradigm. Our results suggest the need for more careful baseline evaluations in future research on structured pruning methods. We also compare with the "Lottery Ticket Hypothesis" (Frankle \& Carbin 2019), and find that with optimal learning rate, the "winning ticket" initialization as used in Frankle \& Carbin (2019) does not bring improvement over random initialization.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-Rethinking%20the%20value%20of%20network%20pruning-2018-ICLR.pdf}
}
@article{LearningChenICML2015, 
year = {2014}, 
title = {{Learning deep structured models}}, 
author = {Chen, Liang-Chieh and Schwing, Alexander G and Yuille, Alan L and Urtasun, Raquel}, 
journal = {ICML}, 
eprint = {1407.2538}, 
abstract = {{Many problems in real-world applications involve predicting several random variables which are statistically related. Markov random fields (MRFs) are a great mathematical tool to encode such relationships. The goal of this paper is to combine MRFs with deep learning algorithms to estimate complex representations while taking into account the dependencies between the output random variables. Towards this goal, we propose a training algorithm that is able to learn structured models jointly with deep features that form the MRF potentials. Our approach is efficient as it blends learning and inference and makes use of GPU acceleration. We demonstrate the effectiveness of our algorithm in the tasks of predicting words from noisy images, as well as multi-class classification of Flickr photographs. We show that joint learning of the deep features and the MRF parameters results in significant performance gains.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Learning%20deep%20structured%20models-2014-ICML.pdf}
}
@article{BayesNASZhouICML2019, 
year = {2019}, 
title = {{BayesNAS: A Bayesian Approach for Neural Architecture Search}}, 
author = {Zhou, Hongpeng and Yang, Minghao and Wang, Jun and Pan, Wei}, 
journal = {ICML}, 
eprint = {1905.04919}, 
abstract = {{One-Shot Neural Architecture Search (NAS) is a promising method to significantly reduce search time without any separate training. It can be treated as a Network Compression problem on the architecture parameters from an over-parameterized network. However, there are two issues associated with most one-shot NAS methods. First, dependencies between a node and its predecessors and successors are often disregarded which result in improper treatment over zero operations. Second, architecture parameters pruning based on their magnitude is questionable. In this paper, we employ the classic Bayesian learning approach to alleviate these two issues by modeling architecture parameters using hierarchical automatic relevance determination (HARD) priors. Unlike other NAS methods, we train the over-parameterized network for only one epoch then update the architecture. Impressively, this enabled us to find the architecture on CIFAR-10 within only 0.2 GPU days using a single GPU. Competitive performance can be also achieved by transferring to ImageNet. As a byproduct, our approach can be applied directly to compress convolutional neural networks by enforcing structural sparsity which achieves extremely sparse networks without accuracy deterioration.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhou-BayesNAS-%20A%20Bayesian%20Approach%20for%20Neural%20Architecture%20Search-2019-ICML.pdf}
}
@article{MPLPhamCVPR2021, 
year = {2020}, 
title = {{Meta pseudo labels}}, 
author = {Pham, Hieu and Dai, Zihang and Xie, Qizhe and Luong, Minh-Thang and Le, Quoc V}, 
journal = {CVPR}, 
eprint = {2003.10580}, 
abstract = {{We present Meta Pseudo Labels, a semi-supervised learning method that achieves a new state-of-the-art top-1 accuracy of 90.2\% on ImageNet, which is 1.6\% better than the existing state-of-the-art. Like Pseudo Labels, Meta Pseudo Labels has a teacher network to generate pseudo labels on unlabeled data to teach a student network. However, unlike Pseudo Labels where the teacher is fixed, the teacher in Meta Pseudo Labels is constantly adapted by the feedback of the student's performance on the labeled dataset. As a result, the teacher generates better pseudo labels to teach the student. Our code will be available at https://github.com/google-research/google-research/tree/master/meta\_pseudo\_labels.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Pham-Meta%20pseudo%20labels-2020-CVPR.pdf}
}
@article{BATSBulatECCV2020, 
year = {2020}, 
title = {{BATS: Binary ArchitecTure Search}}, 
author = {Bulat, Adrian and Martinez, Brais and Tzimiropoulos, Georgios}, 
journal = {ECCV}, 
eprint = {2003.01711}, 
abstract = {{This paper proposes Binary ArchitecTure Search (BATS), a framework that drastically reduces the accuracy gap between binary neural networks and their real-valued counterparts by means of Neural Architecture Search (NAS). We show that directly applying NAS to the binary domain provides very poor results. To alleviate this, we describe, to our knowledge, for the first time, the 3 key ingredients for successfully applying NAS to the binary domain. Specifically, we (1) introduce and design a novel binary-oriented search space, (2) propose a new mechanism for controlling and stabilising the resulting searched topologies, (3) propose and validate a series of new search strategies for binary networks that lead to faster convergence and lower search times. Experimental results demonstrate the effectiveness of the proposed approach and the necessity of searching in the binary space directly. Moreover, (4) we set a new state-of-the-art for binary neural networks on CIFAR10, CIFAR100 and ImageNet datasets. Code will be made available https://github.com/1adrianb/binary-nas}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bulat-BATS-%20Binary%20ArchitecTure%20Search-2020-ECCV.pdf}
}
@article{MS-NASYanMICCAI2020, 
year = {2020}, 
title = {{MS-NAS: Multi-Scale Neural Architecture Search for Medical Image Segmentation}}, 
author = {Yan, Xingang and Jiang, Weiwen and Shi, Yiyu and Zhuo, Cheng}, 
journal = {MICCAI}, 
eprint = {2007.06151}, 
abstract = {{The recent breakthroughs of Neural Architecture Search (NAS) have motivated various applications in medical image segmentation. However, most existing work either simply rely on hyper-parameter tuning or stick to a fixed network backbone, thereby limiting the underlying search space to identify more efficient architecture. This paper presents a Multi-Scale NAS (MS-NAS) framework that is featured with multi-scale search space from network backbone to cell operation, and multi-scale fusion capability to fuse features with different sizes. To mitigate the computational overhead due to the larger search space, a partial channel connection scheme and a two-step decoding method are utilized to reduce computational overhead while maintaining optimization quality. Experimental results show that on various datasets for segmentation, MS-NAS outperforms the state-of-the-art methods and achieves 0.6-5.4\% mIOU and 0.4-3.5\% DSC improvements, while the computational resource consumption is reduced by 18.0-24.9\%.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yan-MS-NAS-%20Multi-Scale%20Neural%20Architecture%20Search%20for%20Medical%20Image%20Segmentation-2020-MICCAI.pdf}
}
@article{ENetPaszkearXiv2016, 
year = {2016}, 
title = {{ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation}}, 
author = {Paszke, Adam and Chaurasia, Abhishek and Kim, Sangpil and Culurciello, Eugenio}, 
journal = {arXiv}, 
eprint = {1606.02147}, 
abstract = {{The ability to perform pixel-wise semantic segmentation in real-time is of paramount importance in mobile applications. Recent deep neural networks aimed at this task have the disadvantage of requiring a large number of floating point operations and have long run-times that hinder their usability. In this paper, we propose a novel deep neural network architecture named ENet (efficient neural network), created specifically for tasks requiring low latency operation. ENet is up to 18\$\textbackslashtimes\$ faster, requires 75\$\textbackslashtimes\$ less FLOPs, has 79\$\textbackslashtimes\$ less parameters, and provides similar or better accuracy to existing models. We have tested it on CamVid, Cityscapes and SUN datasets and report on comparisons with existing state-of-the-art methods, and the trade-offs between accuracy and processing time of a network. We present performance measurements of the proposed architecture on embedded systems and suggest possible software improvements that could make ENet even faster.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Paszke-ENet-%20A%20Deep%20Neural%20Network%20Architecture%20for%20Real-Time%20Semantic%20Segmentation-2016-arXiv.pdf}
}
@article{BiSeNetYuECCV2018, 
year = {2018}, 
title = {{BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation}}, 
author = {Yu, Changqian and Wang, Jingbo and Peng, Chao and Gao, Changxin and Yu, Gang and Sang, Nong}, 
journal = {ECCV}, 
eprint = {1808.00897}, 
abstract = {{Semantic segmentation requires both rich spatial information and sizeable receptive field. However, modern approaches usually compromise spatial resolution to achieve real-time inference speed, which leads to poor performance. In this paper, we address this dilemma with a novel Bilateral Segmentation Network (BiSeNet). We first design a Spatial Path with a small stride to preserve the spatial information and generate high-resolution features. Meanwhile, a Context Path with a fast downsampling strategy is employed to obtain sufficient receptive field. On top of the two paths, we introduce a new Feature Fusion Module to combine features efficiently. The proposed architecture makes a right balance between the speed and segmentation performance on Cityscapes, CamVid, and COCO-Stuff datasets. Specifically, for a 2048x1024 input, we achieve 68.4\% Mean IOU on the Cityscapes test dataset with speed of 105 FPS on one NVIDIA Titan XP card, which is significantly faster than the existing methods with comparable performance.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yu-BiSeNet-%20Bilateral%20Segmentation%20Network%20for%20Real-time%20Semantic%20Segmentation-2018-ECCV.pdf}
}
@article{ICNetZhaoECCV2018, 
year = {2017}, 
title = {{ICNet for real-time semantic segmentation on high-resolution images}}, 
author = {Zhao, Hengshuang and Qi, Xiaojuan and Shen, Xiaoyong and Shi, Jianping and Jia, Jiaya}, 
journal = {ECCV}, 
eprint = {1704.08545}, 
abstract = {{We focus on the challenging task of real-time semantic segmentation in this paper. It finds many practical applications and yet is with fundamental difficulty of reducing a large portion of computation for pixel-wise label inference. We propose an image cascade network (ICNet) that incorporates multi-resolution branches under proper label guidance to address this challenge. We provide in-depth analysis of our framework and introduce the cascade feature fusion unit to quickly achieve high-quality segmentation. Our system yields real-time inference on a single GPU card with decent quality results evaluated on challenging datasets like Cityscapes, CamVid and COCO-Stuff.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhao-ICNet%20for%20real-time%20semantic%20segmentation%20on%20high-resolution%20images-2017-ECCV.pdf}
}
@article{SparseMaskWuICCV2019, 
year = {2019}, 
title = {{SparseMask: Differentiable Connectivity Learning for Dense Image Prediction}}, 
author = {Wu, Huikai and Zhang, Junge and Huang, Kaiqi}, 
journal = {ICCV}, 
abstract = {{In this paper, we aim at automatically searching an efficient network architecture for dense image prediction. Particularly, we follow the encoder-decoder style and focus on designing a connectivity structure for the decoder. To achieve that, we design a densely connected network with learnable connections, named Fully Dense Network, which contains a large set of possible final connectivity structures. We then employ gradient descent to search the optimal connectivity from the dense connections. The search process is guided by a novel loss function, which pushes the weight of each connection to be binary and the connections to be sparse. The discovered connectivity achieves competitive results on two segmentation datasets, while runs more than three times faster and requires less than half parameters compared to the state-of-the-art methods. An extensive experiment shows that the discovered connectivity is compatible with various backbones and generalizes well to other dense image prediction tasks.}}, 
pages = {6767--6776}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wu-SparseMask-%20Differentiable%20Connectivity%20Learning%20for%20Dense%20Image%20Prediction-2019-ICCV.pdf}
}
@article{FastNekrasovCVPR2019, 
year = {2019}, 
title = {{Fast neural architecture search of compact semantic segmentation models via auxiliary cells}}, 
author = {Nekrasov, Vladimir and Chen, Hao and Shen, Chunhua and Reid, Ian}, 
journal = {CVPR}, 
abstract = {{Automated design of neural network architectures tailored for a specific task is an extremely promising, albeit inherently difficult, avenue to explore. While most results in this domain have been achieved on image classification and language modelling problems, here we concentrate on dense per-pixel tasks, in particular, semantic image segmentation using fully convolutional networks. In contrast to the aforementioned areas, the design choices of a fully convolutional network require several changes, ranging from the sort of operations that need to be used—e.g., dilated convolutions—to a solving of a more difficult optimisation problem. In this work, we are particularly interested in searching for high-performance compact segmentation architectures, able to run in real-time using limited resources. To achieve that, we intentionally over-parameterise the architecture during the training time via a set of auxiliary cells that provide an intermediate supervisory signal and can be omitted during the evaluation phase. The design of the auxiliary cell is emitted by a controller, a neural network with the fixed structure trained using reinforcement learning. More crucially, we demonstrate how to efficiently search for these architectures within limited time and computational budgets. In particular, we rely on a progressive strategy that terminates non-promising architectures from being further trained, and on Polyak averaging coupled with knowledge distillation to speed-up the convergence. Quantitatively, in 8 GPU-days our approach discovers a set of architectures performing on-par with state-of-the-art among compact models on the semantic segmentation, pose estimation and depth prediction tasks. Code will be made available here: https://github.com/drsleep/nas-segm-pytorch}}, 
pages = {9118--9127}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Nekrasov-Fast%20neural%20architecture%20search%20of%20compact%20semantic%20segmentation%20models%20via%20auxiliary%20cells-2019-CVPR.pdf}
}
@article{BiSeNetV2YuIJCV2021, 
year = {2021}, 
title = {{BiSeNet V2: Bilateral Network with Guided Aggregation for Real-time Semantic Segmentation}}, 
author = {Yu, Changqian and Gao, Changxin and Wang, Jingbo and Yu, Gang and Shen, Chunhua and Sang, Nong}, 
journal = {IJCV}, 
eprint = {2004.02147}, 
abstract = {{The low-level details and high-level semantics are both essential to the semantic segmentation task. However, to speed up the model inference, current approaches almost always sacrifice the low-level details, which leads to a considerable accuracy decrease. We propose to treat these spatial details and categorical semantics separately to achieve high accuracy and high efficiency for realtime semantic segmentation. To this end, we propose an efficient and effective architecture with a good trade-off between speed and accuracy, termed Bilateral Segmentation Network (BiSeNet V2). This architecture involves: (i) a Detail Branch, with wide channels and shallow layers to capture low-level details and generate high-resolution feature representation; (ii) a Semantic Branch, with narrow channels and deep layers to obtain high-level semantic context. The Semantic Branch is lightweight due to reducing the channel capacity and a fast-downsampling strategy. Furthermore, we design a Guided Aggregation Layer to enhance mutual connections and fuse both types of feature representation. Besides, a booster training strategy is designed to improve the segmentation performance without any extra inference cost. Extensive quantitative and qualitative evaluations demonstrate that the proposed architecture performs favourably against a few state-of-the-art real-time semantic segmentation approaches. Specifically, for a 2,048x1,024 input, we achieve 72.6\% Mean IoU on the Cityscapes test set with a speed of 156 FPS on one NVIDIA GeForce GTX 1080 Ti card, which is significantly faster than existing methods, yet we achieve better segmentation accuracy.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yu-BiSeNet%20V2-%20Bilateral%20Network%20with%20Guided%20Aggregation%20for%20Real-time%20Semantic%20Segmentation-2020-arXiv.pdf}
}
@article{DCNASZhangCVPR2021, 
year = {2020}, 
title = {{DCNAS: Densely Connected Neural Architecture Search for Semantic Image Segmentation}}, 
author = {Zhang, Xiong and Xu, Hongmin and Mo, Hong and Tan, Jianchao and Yang, Cheng and Wang, Lei and Ren, Wenqi}, 
journal = {CVPR}, 
eprint = {2003.11883}, 
abstract = {{Neural Architecture Search (NAS) has shown great potentials in automatically designing scalable network architectures for dense image predictions. However, existing NAS algorithms usually compromise on restricted search space and search on proxy task to meet the achievable computational demands. To allow as wide as possible network architectures and avoid the gap between target and proxy dataset, we propose a Densely Connected NAS (DCNAS) framework, which directly searches the optimal network structures for the multi-scale representations of visual information, over a large-scale target dataset. Specifically, by connecting cells with each other using learnable weights, we introduce a densely connected search space to cover an abundance of mainstream network designs. Moreover, by combining both path-level and channel-level sampling strategies, we design a fusion module to reduce the memory consumption of ample search space. We demonstrate that the architecture obtained from our DCNAS algorithm achieves state-of-the-art performances on public semantic image segmentation benchmarks, including 84.3\% on Cityscapes, and 86.9\% on PASCAL VOC 2012. We also retain leading performances when evaluating the architecture on the more challenging ADE20K and Pascal Context dataset.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-DCNAS-%20Densely%20Connected%20Neural%20Architecture%20Search%20for%20Semantic%20Image%20Segmentation-2020-CVPR.pdf}
}
@article{CASZhangCVPR2019, 
year = {2019}, 
title = {{Customizable architecture search for semantic segmentation}}, 
author = {Zhang, Yiheng and Qiu, Zhaofan and Liu, Jingen and Yao, Ting and Liu, Dong and Mei, Tao}, 
journal = {CVPR}, 
abstract = {{In this paper, we propose a Customizable Architecture Search (CAS) approach to automatically generate a network architecture for semantic image segmentation. The generated network consists of a sequence of stacked computation cells. A computation cell is represented as a directed acyclic graph, in which each node is a hidden representation (i.e., feature map) and each edge is associated with an operation (e.g., convolution and pooling), which transforms data to a new layer. During the training, the CAS algorithm explores the search space for an optimized computation cell to build a network. The cells of the same type share one architecture but with different weights. In real applications, however, an optimization may need to be conducted under some constraints such as GPU time and model size. To this end, a cost corresponding to the constraint will be assigned to each operation. When an operation is selected during the search, its associated cost will be added to the objective. As a result, our CAS is able to search an optimized architecture with customized constraints. The approach has been thoroughly evaluated on Cityscapes and CamVid datasets, and demonstrates superior performance over several stateof-the-art techniques. More remarkably, our CAS achieves 72.3\% mIoU on the Cityscapes dataset with speed of 108 FPS on an Nvidia TitanXp GPU.}}, 
pages = {11633--11642}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Customizable%20architecture%20search%20for%20semantic%20segmentation-2019-CVPR.pdf}
}
@article{GASLinCVPR2020, 
year = {2020}, 
title = {{Graph-guided architecture search for real-time semantic segmentation}}, 
author = {Lin, Peiwen and Sun, Peng and Cheng, Guangliang and Xie, Sirui and Li, Xi and Shi, Jianping}, 
journal = {CVPR}, 
abstract = {{Designing a lightweight semantic segmentation network often requires researchers to find a trade-off between performance and speed, which is always empirical due to the limited interpretability of neural networks. In order to release researchers from these tedious mechanical trials, we propose a Graph-guided Architecture Search (GAS) pipeline to automatically search real-time semantic segmentation networks. Unlike previous works that use a simplified search space and stack a repeatable cell to form a network, we introduce a novel search mechanism with a new search space where a lightweight model can be effectively explored through the cell-level diversity and latency-oriented constraint. Specifically, to produce the cell-level diversity, the cell-sharing constraint is eliminated through the cell-independent manner. Then a graph convolution network (GCN) is seamlessly integrated as a communication mechanism between cells. Finally, a latency-oriented constraint is endowed into the search process to balance the speed and performance. Extensive experiments on Cityscapes and CamVid datasets demonstrate that GAS achieves the new state-of-the-art trade-off between accuracy and speed. In particular, on Cityscapes dataset, GAS achieves the new best performance of 73.5\% mIoU with speed of 108.4 FPS on Titan Xp.}}, 
pages = {4202--4211}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lin-Graph-guided%20architecture%20search%20for%20real-time%20semantic%20segmentation-2020-CVPR.pdf}
}
@article{SegNetBadrinarayananTPAMI2016, 
year = {2015}, 
title = {{SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}}, 
author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto}, 
journal = {TPAMI}, 
issn = {0162-8828}, 
pmid = {28060704}, 
abstract = {{We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.}}, 
pages = {2481--2495}, 
number = {12}, 
volume = {39}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Badrinarayanan-SegNet-%20A%20Deep%20Convolutional%20Encoder-Decoder%20Architecture%20for%20Image%20Segmentation-2015-TPAMI.pdf}
}
@article{UNet3+HuangICASSP2020, 
year = {2020}, 
title = {{UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation}}, 
author = {Huang, Huimin and Lin, Lanfen and Tong, Ruofeng and Hu, Hongjie and Zhang, Qiaowei and Iwamoto, Yutaro and Han, Xianhua and Chen, Yen-Wei and Wu, Jian}, 
journal = {ICASSP}, 
abstract = {{Recently, a growing interest has been seen in deep learning-based semantic segmentation. UNet, which is one of deep learning networks with an encoder-decoder architecture, is widely used in medical image segmentation. Combining multi-scale features is one of important factors for accurate segmentation. UNet++ was developed as a modified Unet by designing an architecture with nested and dense skip connections. However, it does not explore sufficient information from full scales and there is still a large room for improvement. In this paper, we propose a novel UNet 3+, which takes advantage of full-scale skip connections and deep supervisions. The full-scale skip connections incorporate low-level details with high-level semantics from feature maps in different scales; while the deep supervision learns hierarchical representations from the full-scale aggregated feature maps. The proposed method is especially benefiting for organs that appear at varying scales. In addition to accuracy improvements, the proposed UNet 3+ can reduce the network parameters to improve the computation efficiency. We further propose a hybrid loss function and devise a classification-guided module to enhance the organ boundary and reduce the over-segmentation in a non-organ image, yielding more accurate segmentation results. The effectiveness of the proposed method is demonstrated on two datasets. The code is available at: github.com/ZJUGiveLab/UNet-Version}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Huang-UNet%203+-%20A%20Full-Scale%20Connected%20UNet%20for%20Medical%20Image%20Segmentation-2020-ICASSP.pdf}
}
@article{MixConvTanBMVC2019, 
year = {2019}, 
title = {{MixConv: Mixed Depthwise Convolutional Kernels}}, 
author = {Tan, Mingxing and Le, Quoc V}, 
journal = {BMVC}, 
eprint = {1907.09595}, 
abstract = {{Depthwise convolution is becoming increasingly popular in modern efficient ConvNets, but its kernel size is often overlooked. In this paper, we systematically study the impact of different kernel sizes, and observe that combining the benefits of multiple kernel sizes can lead to better accuracy and efficiency. Based on this observation, we propose a new mixed depthwise convolution (MixConv), which naturally mixes up multiple kernel sizes in a single convolution. As a simple drop-in replacement of vanilla depthwise convolution, our MixConv improves the accuracy and efficiency for existing MobileNets on both ImageNet classification and COCO object detection. To demonstrate the effectiveness of MixConv, we integrate it into AutoML search space and develop a new family of models, named as MixNets, which outperform previous mobile models including MobileNetV2 [20] (ImageNet top-1 accuracy +4.2\%), ShuffleNetV2 [16] (+3.5\%), MnasNet [26] (+1.3\%), ProxylessNAS [2] (+2.2\%), and FBNet [27] (+2.0\%). In particular, our MixNet-L achieves a new state-of-the-art 78.9\% ImageNet top-1 accuracy under typical mobile settings (<600M FLOPS). Code is at https://github.com/ tensorflow/tpu/tree/master/models/official/mnasnet/mixnet}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tan-MixConv-%20Mixed%20Depthwise%20Convolutional%20Kernels-2019-BMVC.pdf}
}
@article{IterNetLiWACV2020, 
year = {2020}, 
title = {{IterNet: Retinal Image Segmentation Utilizing Structural Redundancy in Vessel Networks}}, 
author = {Li, Liangzhi and Verma, Manisha and Nakashima, Yuta and Nagahara, Hajime and Kawasaki, Ryo}, 
journal = {WACV}, 
abstract = {{Retinal vessel segmentation is of great interest for diagnosis of retinal vascular diseases. To further improve the performance of vessel segmentation, we propose IterNet, a new model based on UNet [1], with the ability to find obscured details of the vessel from the segmented vessel image itself, rather than the raw input image. IterNet consists of multiple iterations of a mini-UNet, which can be 4× deeper than the common UNet. IterNet also adopts the weight-sharing and skip-connection features to facilitate training; therefore, even with such a large architecture, IterNet can still learn from merely 10\textbackslashtextasciitilde20 labeled images, without pre-training or any prior knowledge. IterNet achieves AUCs of 0.9816, 0.9851, and 0.9881 on three mainstream datasets, namely DRIVE, CHASE-DB1, and STARE, respectively, which currently are the best scores in the literature. The source code is available1.}}, 
pages = {3645--3654}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-IterNet-%20Retinal%20Image%20Segmentation%20Utilizing%20Structural%20Redundancy%20in%20Vessel%20Networks-2020-WACV.pdf}
}
@article{UDAXieNeurIPS2020, 
year = {2019}, 
title = {{Unsupervised data augmentation for consistency training}}, 
author = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V}, 
journal = {NeurIPS}, 
eprint = {1904.12848}, 
abstract = {{Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAugment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10\% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xie-Unsupervised%20data%20augmentation%20for%20consistency%20training-2019-NeurIPS.pdf}
}
@article{ImprovedBatraCVPR2019, 
year = {2019}, 
title = {{Improved road connectivity by joint learning of orientation and segmentation}}, 
author = {Batra, Anil and Singh, Suriya and Pang, Guan and Basu, Saikat and Jawahar, C.V. and Paluri, Manohar}, 
journal = {CVPR}, 
abstract = {{Road network extraction from satellite images often produce fragmented road segments leading to road maps unfit for real applications. Pixel-wise classification fails to predict topologically correct and connected road masks due to the absence of connectivity supervision and difficulty in enforcing topological constraints. In this paper, we propose a connectivity task called Orientation Learning, motivated by the human behavior of annotating roads by tracing it at a specific orientation. We also develop a stacked multi-branch convolutional module to effectively utilize the mutual information between orientation learning and segmentation tasks. These contributions ensure that the model predicts topologically correct and connected road masks. We also propose Connectivity Refinement approach to further enhance the estimated road networks. The refinement model is pre-trained to connect and refine the corrupted ground-truth masks and later fine-tuned to enhance the predicted road masks. We demonstrate the advantages of our approach on two diverse road extraction datasets SpaceNet [30] and DeepGlobe [11]. Our approach improves over the state-of-the-art techniques by 9\% and 7.5\% in road topology metric on SpaceNet and DeepGlobe, respectively.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Batra-Improved%20road%20connectivity%20by%20joint%20learning%20of%20orientation%20and%20segmentation-2019-CVPR.pdf}
}
@article{ICTVermaIJCAI2019, 
year = {2019}, 
title = {{Interpolation consistency training for semi-supervised learning}}, 
author = {Verma, Vikas and Lamb, Alex and Kannala, Juho and Bengio, Yoshua and Lopez-Paz, David}, 
journal = {IJCAI}, 
abstract = {{We introduce Interpolation Consistency Training (ICT), a simple and computation efficient algorithm for training Deep Neural Networks in the semi-supervised learning paradigm. ICT encourages the prediction at an interpolation of unlabeled points to be consistent with the interpolation of the predictions at those points. In classification problems, ICT moves the decision boundary to low-density regions of the data distribution. Our experiments show that ICT achieves state-of-the-art performance when applied to standard neural network architectures on the CIFAR-10 and SVHN benchmark dataset.}}, 
pages = {3635--3641}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Verma-Interpolation%20consistency%20training%20for%20semi-supervised%20learning-2019-IJCAI.pdf}
}
@article{Pseudo-labelLeeICMLWorkshop2013, 
year = {2013}, 
title = {{Pseudo-label: The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks}}, 
author = {Lee, Dong-Hyun}, 
journal = {ICML Workshop}, 
abstract = {{We propose the simple and efficient method of semi-supervised learning for deep neural networks. Basically, the proposed network is trained in a supervised fashion with labeled and unlabeled data simultaneously. For un- labeled data, Pseudo-Labels, just picking up the class which has the maximum predicted probability, are used as if they were true la- bels. This is in effect equivalent to Entropy Regularization. It favors a low-density sepa- ration between classes, a commonly assumed prior for semi-supervised learning. With De- noising Auto-Encoder and Dropout, this sim- ple method outperforms conventional meth- ods for semi-supervised learning with very small labeled data on the MNIST handwrit- ten digit dataset.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lee-Pseudo-label-%20The%20Simple%20and%20Efficient%20Semi-Supervised%20Learning%20Method%20for%20Deep%20Neural%20Networks-2013-ICML%20Workshop.pdf}
}
@article{SoftJaccardRahmanISVC2016, 
year = {2016}, 
title = {{Optimizing intersection-over-union in deep neural networks for image segmentation}}, 
author = {Rahman, Md Atiqur and Wang, Yang}, 
journal = {ISVC}, 
abstract = {{We consider the problem of learning deep neural networks(DNNs) for object category segmentation, where the goal is to label each pixel in an image as being part of a given object (foreground) or not (back- ground). Deep neural networks are usually trained with simple loss functions (e.g., softmax loss). These loss functions are appropriate for standard classification problems where the performance is measured by the overall classification accuracy. For object category segmentation, the two classes (foreground and background) are very imbalanced. The intersection- over-union (IoU) is usually used to measure the performance of any object category segmentation method. In this paper, we propose an approach for directly optimizing this IoU measure in deep neural networks. Our experimental results on two object category segmentation datasets demonstrate that our approach outperforms DNNs trained with standard softmax loss.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Rahman-Optimizing%20intersection-over-union%20in%20deep%20neural%20networks%20for%20image%20segmentation-2016-ISVC.pdf}
}
@article{CHAOSKavurMIA2021, 
year = {2021}, 
title = {{CHAOS Challenge - combined (CT-MR) healthy abdominal organ segmentation}}, 
author = {Kavur, A. Emre and Gezer, N. Sinem and Barış, Mustafa and Aslan, Sinem and Conze, Pierre-Henri and Groza, Vladimir and Pham, Duc Duy and Chatterjee, Soumick and Ernst, Philipp and Özkan, Savaş and Baydar, Bora and Lachinov, Dmitry and Han, Shuo and Pauli, Josef and Isensee, Fabian and Perkonigg, Matthias and Sathish, Rachana and Rajan, Ronnie and Sheet, Debdoot and Dovletov, Gurbandurdy and Speck, Oliver and Nürnberger, Andreas and Maier-Hein, Klaus H. and Akar, Gözde Bozdağı and Ünal, Gözde and Dicle, Oğuz and Selver, M. Alper}, 
journal = {MIA}, 
eprint = {2001.06535}, 
abstract = {{Segmentation of abdominal organs has been a comprehensive, yet unresolved, research field for many years. In the last decade, intensive developments in deep learning (DL) introduced new state-of-the-art segmentation systems. Despite outperforming the overall accuracy of existing systems, the effects of DL model properties and parameters on the performance are hard to interpret. This makes comparative analysis a necessary tool towards interpretable studies and systems. Moreover, the performance of DL for emerging learning approaches such as cross-modality and multi-modal semantic segmentation tasks has been rarely discussed. In order to expand the knowledge on these topics, the CHAOS – Combined (CT-MR) Healthy Abdominal Organ Segmentation challenge was organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI), 2019, in Venice, Italy. Abdominal organ segmentation from routine acquisitions plays an important role in several clinical applications, such as pre-surgical planning or morphological and volumetric follow-ups for various diseases. These applications require a certain level of performance on a diverse set of metrics such as maximum symmetric surface distance (MSSD) to determine surgical error-margin or overlap errors for tracking size and shape differences. Previous abdomen related challenges are mainly focused on tumor/lesion detection and/or classification with a single modality. Conversely, CHAOS provides both abdominal CT and MR data from healthy subjects for single and multiple abdominal organ segmentation. Five different but complementary tasks were designed to analyze the capabilities of participating approaches from multiple perspectives. The results were investigated thoroughly, compared with manual annotations and interactive methods. The analysis shows that the performance of DL models for single modality (CT / MR) can show reliable volumetric analysis performance (DICE: 0.98 ± 0.00 / 0.95 ± 0.01), but the best MSSD performance remains limited (21.89 ± 13.94 / 20.85 ± 10.63 mm). The performances of participating models decrease dramatically for cross-modality tasks both for the liver (DICE: 0.88 ± 0.15 MSSD: 36.33 ± 21.97 mm). Despite contrary examples on different applications, multi-tasking DL models designed to segment all organs are observed to perform worse compared to organ-specific ones (performance drop around 5\%). Nevertheless, some of the successful models show better performance with their multi-organ versions. We conclude that the exploration of those pros and cons in both single vs multi-organ and cross-modality segmentations is poised to have an impact on further research for developing effective algorithms that would support real-world clinical applications. Finally, having more than 1500 participants and receiving more than 550 submissions, another important contribution of this study is the analysis on shortcomings of challenge organizations such as the effects of multiple submissions and peeking phenomenon.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kavur-CHAOS%20Challenge%20-%20combined%20(CT-MR)%20healthy%20abdominal%20organ%20segmentation-2021-MIA.pdf}
}
@article{SpaceNetEttenarXiv2018, 
year = {2018}, 
title = {{SpaceNet: A Remote Sensing Dataset and Challenge Series}}, 
author = {Etten, Adam Van and Lindenbaum, Dave and Bacastow, Todd M}, 
journal = {arXiv}, 
eprint = {1807.01232}, 
abstract = {{Foundational mapping remains a challenge in many parts of the world, particularly in dynamic scenarios such as natural disasters when timely updates are critical. Updating maps is currently a highly manual process requiring a large number of human labelers to either create features or rigorously validate automated outputs. We propose that the frequent revisits of earth imaging satellite constellations may accelerate existing efforts to quickly update foundational maps when combined with advanced machine learning techniques. Accordingly, the SpaceNet partners (CosmiQ Works, Radiant Solutions, and NVIDIA), released a large corpus of labeled satellite imagery on Amazon Web Services (AWS) called SpaceNet. The SpaceNet partners also launched a series of public prize competitions to encourage improvement of remote sensing machine learning algorithms. The first two of these competitions focused on automated building footprint extraction, and the most recent challenge focused on road network extraction. In this paper we discuss the SpaceNet imagery, labels, evaluation metrics, prize challenge results to date, and future plans for the SpaceNet challenge series.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Etten-SpaceNet-%20A%20Remote%20Sensing%20Dataset%20and%20Challenge%20Series-2018-arXiv.pdf}
}
@article{AdaBitsJinCVPR2020, 
year = {2020}, 
title = {{AdaBits: Neural Network Quantization with Adaptive Bit-Widths}}, 
author = {Jin, Qing and Yang, Linjie and Liao, Zhenyu}, 
journal = {CVPR}, 
abstract = {{Deep neural networks with adaptive configurations have gained increasing attention due to the instant and flexible deployment of these models on platforms with different resource budgets. In this paper, we investigate a novel option to achieve this goal by enabling adaptive bit-widths of weights and activations in the model. We first examine the benefits and challenges of training quantized model with adaptive bit-widths, and then experiment with several approaches including direct adaptation, progressive training and joint training. We discover that joint training is able to produce comparable performance on the adaptive model as individual models. We also propose a new technique named Switchable Clipping Level (S-CL) to further improve quantized models at the lowest bit-width. With our proposed techniques applied on a bunch of models including MobileNet V1/V2 and ResNet50, we demonstrate that bit-width of weights and activations is a new option for adaptively executable deep neural networks, offering a distinct opportunity for improved accuracy-efficiency trade-off as well as instant adaptation according to the platform constraints in real-world applications.}}, 
pages = {2143--2153}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Jin-AdaBits-%20Neural%20Network%20Quantization%20with%20Adaptive%20Bit-Widths-2020-CVPR.pdf}
}
@article{TiramisuJegouCVPRWorkshop2017, 
year = {2017}, 
title = {{The one hundred layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation}}, 
author = {Jégou, Simon and Drozdzal, Michal and Vazquez, David and Romero, Adriana and Bengio, Yoshua}, 
journal = {CVPR Workshop}, 
abstract = {{State-of-the-art approaches for semantic image segmentation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, (c) a post-processing module (e.g. Conditional Random Fields) to refine the model predictions. Recently, a new CNN architecture, Densely Connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train. In this paper, we extend DenseNets to deal with the problem of semantic segmentation. We achieve state-of-the-art results on urban scene benchmark datasets such as Cam Vid and Gatech, without any further post-processing module nor pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently published best entries for these datasets. Code to reproduce the experiments is publicly available here: https://github.com/SimJeg/FC-DenseNet}}, 
pages = {1175--1183}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Jégou-The%20one%20hundred%20layers%20Tiramisu-%20Fully%20Convolutional%20DenseNets%20for%20Semantic%20Segmentation-2017-CVPR%20Workshop.pdf}
}
@article{EvaNetPiergiovanniICCV2019, 
year = {2019}, 
title = {{Evolving space-time neural architectures for videos}}, 
author = {Piergiovanni, AJ and Angelova, Anelia and Toshev, Alexander and Ryoo, Michael S.}, 
journal = {ICCV}, 
abstract = {{We present a new method for finding video CNN architectures that capture rich spatio-temporal information in videos. Previous work, taking advantage of 3D convolutions, obtained promising results by manually designing video CNN architectures. We here develop a novel evolutionary search algorithm that automatically explores models with different types and combinations of layers to jointly learn interactions between spatial and temporal aspects of video representations. We demonstrate the generality of this algorithm by applying it to two meta-architectures, obtaining new architectures superior to manually designed architectures:EvaNet. Further, we propose a new component, the iTGM layer, which more efficiently utilizes its parameters to allow learning of space-time interactions over longer time horizons. The iTGM layer is often preferred by the evolutionary algorithm and allows building cost-efficient networks. The proposed approach discovers new and diverse video architectures that were previously unknown. More importantly they are both more accurate and faster than prior models, and outperform the state-of-the-art results on multiple datasets we test, including HMDB, Kinetics, and Moments in Time. We will open source the code and models, to encourage future model development 1. 1Code and models: https://sites.google.com/corp/view/evanet-video Code and models: https://sites.google.com/corp/view/evanet-video}}, 
pages = {1793--1802}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Piergiovanni-Evolving%20space-time%20neural%20architectures%20for%20videos-2019-ICCV.pdf}
}
@article{Auto-FPNXuICCV2019, 
year = {2019}, 
title = {{Auto-FPN: Automatic Network Architecture Adaptation for Object Detection Beyond Classification}}, 
author = {Xu, Hang and Yao, Lewei and Zhang, Wei and Liang, Xiaodan and Li, Zhenguo}, 
journal = {ICCV}, 
abstract = {{Neural architecture search (NAS) has shown great potential in automating the manual process of designing a good CNN architecture for image classification. In this paper, we study NAS for object detection, a core computer vision task that classifies and localizes object instances in an image. Existing works focus on transferring the searched architecture from classification task (ImageNet) to the detector backbone, while the rest of the architecture of the detector remains unchanged. However, this pipeline is not task-specific or data-oriented network search which cannot guarantee optimal adaptation to any dataset. Therefore, we propose an architecture search framework named Auto-FPN specifically designed for detection beyond simply searching a classification backbone. Specifically, we propose two auto search modules for detection: Auto-fusion to search a better fusion of the multi-level features; Auto-head to search a better structure for classification and bounding-box (bbox) regression. Instead of searching for one repeatable cell structure, we relax the constraint and allow different cells. The search space of both modules covers many popular designs of detectors and allows efficient gradient-based architecture search with resource constraint (2 days for COCO on 8 GPUs). Extensive experiments on Pascal VOC, COCO, BDD, VisualGenome and ADE demonstrate the effectiveness of the proposed method, e.g., achieving around 5\% improvement than FPN in terms of mAP while requiring around 50\% fewer parameters on the searched modules.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xu-Auto-FPN-%20Automatic%20Network%20Architecture%20Adaptation%20for%20Object%20Detection%20Beyond%20Classification-2019-ICCV.pdf}
}
@article{BraTSMenzeTMI2014, 
year = {2015}, 
title = {{The multimodal brain tumor image segmentation benchmark (BRATS)}}, 
author = {Menze, Bjoern H. and others}, 
journal = {TMI}, 
abstract = {{In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients—manually annotated by up to four raters—and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74\%–85\%), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Menze-The%20multimodal%20brain%20tumor%20image%20segmentation%20benchmark%20(BRATS)-2015-TMI.pdf}
}
@article{ElasticSimardICDAR2003, 
year = {2003}, 
title = {{Best practices for convolutional neural networks applied to visual document analysis}}, 
author = {Simard, Patrice Y. and Steinkraus, Dave and Platt, John C.}, 
journal = {ICDAR}, 
abstract = {{Neural networks are a powerful technology for classification of visual inputs arising from documents. However, there is a confusing plethora of different neural network methods that are used in the literature and in industry. This paper describes a set of concrete best practices that document analysis researchers can use to get good results with neural networks. The most important practice is getting a training set as large as possible: we expand the training set by adding a new form of distorted data. The next most important practice is that convolutional neural networks are better suited for visual document tasks than fully connected networks. We propose that a simple “do-it-yourself” implementation of convolution with a flexible architecture is suitable for many visual document problems. This simple convolutional neural network does not require complex methods, such as momentum, weight decay, structure-dependent learning rates, averaging layers, tangent prop, or even finely-tuning the architecture. The end result is a very simple yet general architecture which can yield state-of-the-art performance for document analysis. We illustrate our claims on the MNIST set of English digit images.}}, 
pages = {958--963}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Simard-Best%20practices%20for%20convolutional%20neural%20networks%20applied%20to%20visual%20document%20analysis-2003-ICDAR.pdf}
}
@article{AdvancesZhangTPAMI2018, 
year = {2017}, 
title = {{Advances in variational inference}}, 
author = {Zhang, Cheng and Bütepage, Judith and Kjellström, Hedvig and Mandt, Stephan}, 
journal = {TPAMI}, 
issn = {0162-8828}, 
pmid = {30596568}, 
abstract = {{Many modern unsupervised or semi-supervised machine learning algorithms rely on Bayesian probabilistic models. These models are usually intractable and thus require approximate inference. Variational inference (VI) lets us approximate a high-dimensional Bayesian posterior with a simpler variational distribution by solving an optimization problem. This approach has been successfully applied to various models and large-scale applications. In this review, we give an overview of recent trends in variational inference. We first introduce standard mean field variational inference, then review recent advances focusing on the following aspects: (a) scalable VI, which includes stochastic approximations, (b) generic VI, which extends the applicability of VI to a large class of otherwise intractable models, such as non-conjugate models, (c) accurate VI, which includes variational models beyond the mean field approximation or with atypical divergences, and (d) amortized VI, which implements the inference over local latent variables with inference networks. Finally, we provide a summary of promising future research directions.}}, 
pages = {2008--2026}, 
number = {8}, 
volume = {41}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Advances%20in%20variational%20inference-2017-TPAMI.pdf}
}
@article{WRNZagoruykoBMVC2016, 
year = {2016}, 
title = {{Wide residual networks}}, 
author = {Zagoruyko, Sergey and Komodakis, Nikos}, 
journal = {BMVC}, 
abstract = {{Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer- deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at https: //github.com/szagoruyko/wide-residual-networks.}}, 
pages = {87.1--87.12}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zagoruyko-Wide%20residual%20networks-2016-BMVC.pdf}
}
@article{NAS-CountHuECCV2020, 
year = {2020}, 
title = {{NAS-Count: Counting-by-Density with Neural Architecture Search}}, 
author = {Hu, Yutao and Jiang, Xiaolong and Liu, Xuhui and Zhang, Baochang and Han, Jungong and Cao, Xianbin and Doermann, David}, 
journal = {ECCV}, 
eprint = {2003.00217}, 
abstract = {{Most of the recent advances in crowd counting have evolved from hand-designed density estimation networks, where multi-scale features are leveraged to address the scale variation problem, but at the expense of demanding design efforts. In this work, we automate the design of counting models with Neural Architecture Search (NAS) and introduce an end-to-end searched encoder-decoder architecture, Automatic Multi-Scale Network (AMSNet). Specifically, we utilize a counting-specific two-level search space. The encoder and decoder in AMSNet are composed of different cells discovered from micro-level search, while the multi-path architecture is explored through macro-level search. To solve the pixel-level isolation issue in MSE loss, AMSNet is optimized with an auto-searched Scale Pyramid Pooling Loss (SPPLoss) that supervises the multi-scale structural information. Extensive experiments on four datasets show AMSNet produces state-of-the-art results that outperform hand-designed models, fully demonstrating the efficacy of NAS-Count.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hu-NAS-Count-%20Counting-by-Density%20with%20Neural%20Architecture%20Search-2020-ECCV.pdf}
}
@article{GTNSuchICML2020, 
year = {2019}, 
title = {{Generative Teaching Networks: Accelerating Neural Architecture Search by Learning to Generate Synthetic Training Data}}, 
author = {Such, Felipe Petroski and Rawal, Aditya and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff}, 
journal = {ICML}, 
eprint = {1912.07768}, 
abstract = {{This paper investigates the intriguing question of whether we can create learning algorithms that automatically generate training data, learning environments, and curricula in order to help AI agents rapidly learn. We show that such algorithms are possible via Generative Teaching Networks (GTNs), a general approach that is, in theory, applicable to supervised, unsupervised, and reinforcement learning, although our experiments only focus on the supervised case. GTNs are deep neural networks that generate data and/or training environments that a learner (e.g. a freshly initialized neural network) trains on for a few SGD steps before being tested on a target task. We then differentiate through the entire learning process via meta-gradients to update the GTN parameters to improve performance on the target task. GTNs have the beneficial property that they can theoretically generate any type of data or training environment, making their potential impact large. This paper introduces GTNs, discusses their potential, and showcases that they can substantially accelerate learning. We also demonstrate a practical and exciting application of GTNs: accelerating the evaluation of candidate architectures for neural architecture search (NAS), which is rate-limited by such evaluations, enabling massive speed-ups in NAS. GTN-NAS improves the NAS state of the art, finding higher performing architectures when controlling for the search proposal mechanism. GTN-NAS also is competitive with the overall state of the art approaches, which achieve top performance while using orders of magnitude less computation than typical NAS methods. Speculating forward, GTNs may represent a first step toward the ambitious goal of algorithms that generate their own training data and, in doing so, open a variety of interesting new research questions and directions.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Such-Generative%20Teaching%20Networks-%20Accelerating%20Neural%20Architecture%20Search%20by%20Learning%20to%20Generate%20Synthetic%20Training%20Data-2019-ICML.pdf}
}
@article{NeuralWenECCV2020, 
year = {2019}, 
title = {{Neural predictor for neural architecture search}}, 
author = {Wen, Wei and Liu, Hanxiao and Li, Hai and Chen, Yiran and Bender, Gabriel and Kindermans, Pieter-Jan}, 
journal = {ECCV}, 
eprint = {1912.00848}, 
abstract = {{Neural Architecture Search methods are effective but often use complex algorithms to come up with the best architecture. We propose an approach with three basic steps that is conceptually much simpler. First we train N random architectures to generate N (architecture, validation accuracy) pairs and use them to train a regression model that predicts accuracy based on the architecture. Next, we use this regression model to predict the validation accuracies of a large number of random architectures. Finally, we train the top-K predicted architectures and deploy the model with the best validation result. While this approach seems simple, it is more than 20 times as sample efficient as Regularized Evolution on the NASBench-101 benchmark and can compete on ImageNet with more complex approaches based on weight sharing, such as ProxylessNAS.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wen-Neural%20predictor%20for%20neural%20architecture%20search-2019-ECCV.pdf}
}
@article{GroSSJenkinsECCV2020, 
year = {2019}, 
title = {{GroSS: Group-Size Series Decomposition for Grouped Architecture Search}}, 
author = {Howard-Jenkins, Henry and Li, Yiwen and Prisacariu, Victor A}, 
journal = {ECCV}, 
eprint = {1912.00673}, 
abstract = {{We present a novel approach which is able to explore the configuration of grouped convolutions within neural networks. Group-size Series (GroSS) decomposition is a mathematical formulation of tensor factorisation into a series of approximations of increasing rank terms. GroSS allows for dynamic and differentiable selection of factorisation rank, which is analogous to a grouped convolution. Therefore, to the best of our knowledge, GroSS is the first method to enable simultaneous training of differing numbers of groups within a single layer, as well as all possible combinations between layers. In doing so, GroSS is able to train an entire grouped convolution architecture search-space concurrently. We demonstrate this through architecture searches with performance objectives on multiple datasets and networks. GroSS enables more effective and efficient search for grouped convolutional architectures.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Howard-Jenkins-GroSS-%20Group-Size%20Series%20Decomposition%20for%20Grouped%20Architecture%20Search-2019-ECCV.pdf}
}
@article{DA-NASDaiECCV2020, 
year = {2020}, 
title = {{DA-NAS: Data Adapted Pruning for Efficient Neural Architecture Search}}, 
author = {Dai, Xiyang and Chen, Dongdong and Liu, Mengchen and Chen, Yinpeng and Yuan, Lu}, 
journal = {ECCV}, 
eprint = {2003.12563}, 
abstract = {{Efficient search is a core issue in Neural Architecture Search (NAS). It is difficult for conventional NAS algorithms to directly search the architectures on large-scale tasks like ImageNet. In general, the cost of GPU hours for NAS grows with regard to training dataset size and candidate set size. One common way is searching on a smaller proxy dataset (e.g., CIFAR-10) and then transferring to the target task (e.g., ImageNet). These architectures optimized on proxy data are not guaranteed to be optimal on the target task. Another common way is learning with a smaller candidate set, which may require expert knowledge and indeed betrays the essence of NAS. In this paper, we present DA-NAS that can directly search the architecture for large-scale target tasks while allowing a large candidate set in a more efficient manner. Our method is based on an interesting observation that the learning speed for blocks in deep neural networks is related to the difficulty of recognizing distinct categories. We carefully design a progressive data adapted pruning strategy for efficient architecture search. It will quickly trim low performed blocks on a subset of target dataset (e.g., easy classes), and then gradually find the best blocks on the whole target dataset. At this time, the original candidate set becomes as compact as possible, providing a faster search in the target task. Experiments on ImageNet verify the effectiveness of our approach. It is 2x faster than previous methods while the accuracy is currently state-of-the-art, at 76.2\% under small FLOPs constraint. It supports an argument search space (i.e., more candidate blocks) to efficiently search the best-performing architecture.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Dai-DA-NAS-%20Data%20Adapted%20Pruning%20for%20Efficient%20Neural%20Architecture%20Search-2020-ECCV.pdf}
}
@article{LCRankNetWistubaICML2020, 
year = {2020}, 
title = {{Learning to rank learning curves}}, 
author = {Wistuba, Martin and Pedapati, Tejaswini}, 
journal = {ICML}, 
eprint = {2006.03361}, 
abstract = {{Many automated machine learning methods, such as those for hyperparameter and neural architecture optimization, are computationally expensive because they involve training many different model configurations. In this work, we present a new method that saves computational budget by terminating poor configurations early on in the training. In contrast to existing methods, we consider this task as a ranking and transfer learning problem. We qualitatively show that by optimizing a pairwise ranking loss and leveraging learning curves from other datasets, our model is able to effectively rank learning curves without having to observe many or very long learning curves. We further demonstrate that our method can be used to accelerate a neural architecture search by a factor of up to 100 without a significant performance degradation of the discovered architecture. In further experiments we analyze the quality of ranking, the influence of different model components as well as the predictive behavior of the model.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wistuba-Learning%20to%20rank%20learning%20curves-2020-ICML.pdf}
}
@article{AutoML-ZeroRealICML2020, 
year = {2020}, 
title = {{AutoML-Zero: Evolving Machine Learning Algorithms From Scratch}}, 
author = {Real, Esteban and Liang, Chen and So, David R and Le, Quoc V}, 
journal = {ICML}, 
eprint = {2003.03384}, 
abstract = {{Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks---or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Real-AutoML-Zero-%20Evolving%20Machine%20Learning%20Algorithms%20From%20Scratch-2020-ICML.pdf}
}
@article{AutoSTRZhangECCV2020, 
year = {2020}, 
title = {{AutoSTR: Efficient Backbone Search for Scene Text Recognition}}, 
author = {Zhang, Hui and Yao, Quanming and Yang, Mingkun and Xu, Yongchao and Bai, Xiang}, 
journal = {ECCV}, 
eprint = {2003.06567}, 
abstract = {{Scene text recognition (STR) is very challenging due to the diversity of text instances and the complexity of scenes. The community has paid increasing attention to boost the performance by improving the pre-processing image module, like rectification and deblurring, or the sequence translator. However, another critical module, i.e., the feature sequence extractor, has not been extensively explored. In this work, inspired by the success of neural architecture search (NAS), which can identify better architectures than human-designed ones, we propose automated STR (AutoSTR) to search data-dependent backbones to boost text recognition performance. First, we design a domain-specific search space for STR, which contains both choices on operations and constraints on the downsampling path. Then, we propose a two-step search algorithm, which decouples operations and downsampling path, for an efficient search in the given space. Experiments demonstrate that, by searching data-dependent backbones, AutoSTR can outperform the state-of-the-art approaches on standard benchmarks with much fewer FLOPS and model parameters.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-AutoSTR-%20Efficient%20Backbone%20Search%20for%20Scene%20Text%20Recognition-2020-ECCV.pdf}
}
@article{ABSHuECCV2020, 
year = {2020}, 
title = {{Angle-based search space shrinking for neural architecture search}}, 
author = {Hu, Yiming and Liang, Yuding and Guo, Zichao and Wan, Ruosi and Zhang, Xiangyu and Wei, Yichen and Gu, Qingyi and Sun, Jian}, 
journal = {ECCV}, 
eprint = {2004.13431}, 
abstract = {{In this work, we present a simple and general search space shrinking method, called Angle-Based search space Shrinking (ABS), for Neural Architecture Search (NAS). Our approach progressively simplifies the original search space by dropping unpromising candidates, thus can reduce difficulties for existing NAS methods to find superior architectures. In particular, we propose an angle-based metric to guide the shrinking process. We provide comprehensive evidences showing that, in weight-sharing supernet, the proposed metric is more stable and accurate than accuracy-based and magnitude-based metrics to predict the capability of child models. We also show that the angle-based metric can converge fast while training supernet, enabling us to get promising shrunk search spaces efficiently. ABS can easily apply to most of NAS approaches (e.g. SPOS, FairNAS, ProxylessNAS, DARTS and PDARTS). Comprehensive experiments show that ABS can dramatically enhance existing NAS approaches by providing a promising shrunk search space.}}, 
keywords = {}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hu-Angle-based%20search%20space%20shrinking%20for%20neural%20architecture%20search-2020-ECCV.pdf}
}
@article{AutoGANGongICCV2019, 
year = {2019}, 
title = {{AutoGAN: Neural Architecture Search for Generative Adversarial Networks}}, 
author = {Gong, Xinyu and Chang, Shiyu and Jiang, Yifan and Wang, Zhangyang}, 
journal = {ICCV}, 
abstract = {{Neural architecture search (NAS) has witnessed prevailing success in image classification and (very recently) segmentation tasks. In this paper, we present the first preliminary study on introducing the NAS algorithm to generative adversarial networks (GANs), dubbed AutoGAN. The marriage of NAS and GANs faces its unique challenges. We define the search space for the generator architectural variations and use an RNN controller to guide the search, with parameter sharing and dynamic-resetting to accelerate the process. Inception score is adopted as the reward, and a multi-level search strategy is introduced to perform NAS in a progressive way. Experiments validate the effectiveness of AutoGAN on the task of unconditional image generation. Specifically, our discovered architectures achieve highly competitive performance compared to current state-of-the-art hand-crafted GANs, e.g., setting new state-of-theart FID scores of 12.42 on CIFAR-10, and 31.01 on STL-10, respectively. We also conclude with a discussion of the current limitations and future potential of AutoGAN. The code is avaliable at https://github.com/TAMU-VITA/AutoGAN.}}, 
pages = {3223--3233}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Gong-AutoGAN-%20Neural%20Architecture%20Search%20for%20Generative%20Adversarial%20Networks-2019-ICCV.pdf}
}
@article{AGDFuICML2020, 
year = {2020}, 
title = {{AutoGAN-Distiller: Searching to Compress Generative Adversarial Networks}}, 
author = {Fu, Yonggan and Chen, Wuyang and Wang, Haotao and Li, Haoran and Lin, Yingyan and Wang, Zhangyang}, 
journal = {ICML}, 
eprint = {2006.08198}, 
abstract = {{The compression of Generative Adversarial Networks (GANs) has lately drawn attention, due to the increasing demand for deploying GANs into mobile devices for numerous applications such as image translation, enhancement and editing. However, compared to the substantial efforts to compressing other deep models, the research on compressing GANs (usually the generators) remains at its infancy stage. Existing GAN compression algorithms are limited to handling specific GAN architectures and losses. Inspired by the recent success of AutoML in deep compression, we introduce AutoML to GAN compression and develop an AutoGAN-Distiller (AGD) framework. Starting with a specifically designed efficient search space, AGD performs an end-to-end discovery for new efficient generators, given the target computational resource constraints. The search is guided by the original GAN model via knowledge distillation, therefore fulfilling the compression. AGD is fully automatic, standalone (i.e., needing no trained discriminators), and generically applicable to various GAN models. We evaluate AGD in two representative GAN tasks: image translation and super resolution. Without bells and whistles, AGD yields remarkably lightweight yet more competitive compressed models, that largely outperform existing alternatives. Our codes and pretrained models are available at https://github.com/TAMU-VITA/AGD.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Fu-AutoGAN-Distiller-%20Searching%20to%20Compress%20Generative%20Adversarial%20Networks-2020-ICML.pdf}
}
@article{TGMPiergiovanniICML2019, 
year = {2019}, 
title = {{Temporal Gaussian mixture layer for videos}}, 
author = {Piergiovanni, AJ and Ryoo, Michael S}, 
journal = {ICML}, 
abstract = {{We introduce a new convolutional layer named the Temporal Gaussian Mixture (TGM) layer and present how it can be used to efficiently capture longer-term temporal information in continuous activity videos. The TGM layer is a temporal convolutional layer governed by a much smaller set of parameters (e.g., location/variance of Gaussians) that are fully differentiable. We present our fully convolutional video models with multiple TGM layers for activity detection. The extensive experiments on multiple datasets, including Charades and MultiTHUMOS, confirm the effectiveness of TGM layers, significantly outperforming the state-of-the-arts.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Piergiovanni-Temporal%20Gaussian%20mixture%20layer%20for%20videos-2019-ICML.pdf}
}
@article{HW-NAS-BenchLiICLR2021, 
year = {2021}, 
title = {{HW-NAS-Bench: Hardware-Aware Neural Architecture Search Benchmark}}, 
author = {Li, Chaojian and Yu, Zhongzhi and Fu, Yonggan and Zhang, Yongan and Zhao, Yang and You, Haoran and Yu, Qixuan and Wang, Yue and Lin, Yingyan}, 
journal = {ICLR}, 
eprint = {2103.10584}, 
abstract = {{HardWare-aware Neural Architecture Search (HW-NAS) has recently gained tremendous attention by automating the design of DNNs deployed in more resource-constrained daily life devices. Despite its promising performance, developing optimal HW-NAS solutions can be prohibitively challenging as it requires cross-disciplinary knowledge in the algorithm, micro-architecture, and device-specific compilation. First, to determine the hardware-cost to be incorporated into the NAS process, existing works mostly adopt either pre-collected hardware-cost look-up tables or device-specific hardware-cost models. Both of them limit the development of HW-NAS innovations and impose a barrier-to-entry to non-hardware experts. Second, similar to generic NAS, it can be notoriously difficult to benchmark HW-NAS algorithms due to their significant required computational resources and the differences in adopted search spaces, hyperparameters, and hardware devices. To this end, we develop HW-NAS-Bench, the first public dataset for HW-NAS research which aims to democratize HW-NAS research to non-hardware experts and make HW-NAS research more reproducible and accessible. To design HW-NAS-Bench, we carefully collected the measured/estimated hardware performance of all the networks in the search spaces of both NAS-Bench-201 and FBNet, on six hardware devices that fall into three categories (i.e., commercial edge devices, FPGA, and ASIC). Furthermore, we provide a comprehensive analysis of the collected measurements in HW-NAS-Bench to provide insights for HW-NAS research. Finally, we demonstrate exemplary user cases to (1) show that HW-NAS-Bench allows non-hardware experts to perform HW-NAS by simply querying it and (2) verify that dedicated device-specific HW-NAS can indeed lead to optimal accuracy-cost trade-offs. The codes and all collected data are available at https://github.com/RICE-EIC/HW-NAS-Bench.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-HW-NAS-Bench-%20Hardware-Aware%20Neural%20Architecture%20Search%20Benchmark-2021-ICLR.pdf}
}
@article{LinkNetChaurasiaVCIP2017, 
year = {2017}, 
title = {{LinkNet: Exploiting Encoder Representations for Efficient Semantic Segmentation}}, 
author = {Chaurasia, Abhishek and Culurciello, Eugenio}, 
journal = {VCIP}, 
eprint = {1707.03718}, 
abstract = {{Pixel-wise semantic segmentation for visual scene understanding not only needs to be accurate, but also efficient in order to find any use in real-time application. Existing algorithms even though are accurate but they do not focus on utilizing the parameters of neural network efficiently. As a result they are huge in terms of parameters and number of operations; hence slow too. In this paper, we propose a novel deep neural network architecture which allows it to learn without any significant increase in number of parameters. Our network uses only 11.5 million parameters and 21.2 GFLOPs for processing an image of resolution 3x640x360. It gives state-of-the-art performance on CamVid and comparable results on Cityscapes dataset. We also compare our networks processing time on NVIDIA GPU and embedded system device with existing state-of-the-art architectures for different image resolutions.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chaurasia-LinkNet-%20Exploiting%20Encoder%20Representations%20for%20Efficient%20Semantic%20Segmentation-2017-VCIP.pdf}
}
@article{CafeNetSuICLR2021, 
year = {2021}, 
title = {{Locally free weight sharing for network width search}}, 
author = {Su, Xiu and You, Shan and Huang, Tao and Wang, Fei and Qian, Chen and Zhang, Changshui and Xu, Chang}, 
journal = {ICLR}, 
eprint = {2102.05258}, 
abstract = {{Searching for network width is an effective way to slim deep neural networks with hardware budgets. With this aim, a one-shot supernet is usually leveraged as a performance evaluator to rank the performance \textbackslashwrt\textbackslashtextasciitildedifferent width. Nevertheless, current methods mainly follow a manually fixed weight sharing pattern, which is limited to distinguish the performance gap of different width. In this paper, to better evaluate each width, we propose a locally free weight sharing strategy (CafeNet) accordingly. In CafeNet, weights are more freely shared, and each width is jointly indicated by its base channels and free channels, where free channels are supposed to loCAte FrEely in a local zone to better represent each width. Besides, we propose to further reduce the search space by leveraging our introduced FLOPs-sensitive bins. As a result, our CafeNet can be trained stochastically and get optimized within a min-min strategy. Extensive experiments on ImageNet, CIFAR-10, CelebA and MS COCO dataset have verified our superiority comparing to other state-of-the-art baselines. For example, our method can further boost the benchmark NAS network EfficientNet-B0 by 0.41\textbackslash\% via searching its width more delicately.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Su-Locally%20free%20weight%20sharing%20for%20network%20width%20search-2021-ICLR.pdf}
}
@article{MetaD2ALeeICLR2021, 
year = {2021}, 
title = {{Rapid neural architecture search by learning to generate graphs from datasets}}, 
author = {Lee, Hayeon and Hyung, Eunyoung and Hwang, Sung Ju}, 
journal = {ICLR}, 
abstract = {{Despite the success of recent Neural Architecture Search (NAS) methods on var- ious tasks which have shown to output networks that largely outperform human- designed networks, conventional NAS methods have mostly tackled the optimization of searching for the network architecture for a single task (dataset), which does not generalize well across multiple tasks (datasets). Moreover, since such task-specific methods search for a neural architecture from scratch for every given task, they incur a large computational cost, which is problematic when the time and monetary budget are limited. In this paper, we propose an efficient NAS framework that is trained once on a database consisting of datasets and pretrained networks and can rapidly search for a neural architecture for a novel dataset. The proposed MetaD2A (Meta Dataset-to-Architecture) model can stochastically generate graphs (architectures) from a given set (dataset) via a cross-modal latent space learned with amortized meta-learning. Moreover, we also propose a meta-performance predictor to estimate and select the best architecture without direct training on target datasets. The experimental results demonstrate that our model meta-learned on subsets of ImageNet-1K and architectures from NAS-Bench 201 search space successfully generalizes to multiple unseen datasets including CIFAR-10 and CIFAR-100, with an average search time of 33 GPU seconds. Even under MobileNetV3 search space, MetaD2A is 5.5K times faster than NSGANetV2, a transferable NAS method, with comparable performance. We believe that the MetaD2A proposes a new research direction for rapid NAS as well as ways to utilize the knowledge from rich databases of datasets and architectures accumulated over the past years. Code is available at https://github.com/HayeonLee/MetaD2A.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lee-Rapid%20neural%20architecture%20search%20by%20learning%20to%20generate%20graphs%20from%20datasets-2021-ICLR.pdf}
}
@article{Zero-costAbdelfattahICLR2021, 
year = {2021}, 
title = {{Zero-cost proxies for lightweight NAS}}, 
author = {Abdelfattah, Mohamed S and Mehrotra, Abhinav and Dudziak, Łukasz and Lane, Nicholas D}, 
journal = {ICLR}, 
eprint = {2101.08134}, 
abstract = {{Neural Architecture Search (NAS) is quickly becoming the standard methodology to design neural network models. However, NAS is typically compute-intensive because multiple models need to be evaluated before choosing the best one. To reduce the computational power and time needed, a proxy task is often used for evaluating each model instead of full training. In this paper, we evaluate conventional reduced-training proxies and quantify how well they preserve ranking between multiple models during search when compared with the rankings produced by final trained accuracy. We propose a series of zero-cost proxies, based on recent pruning literature, that use just a single minibatch of training data to compute a model's score. Our zero-cost proxies use 3 orders of magnitude less computation but can match and even outperform conventional proxies. For example, Spearman's rank correlation coefficient between final validation accuracy and our best zero-cost proxy on NAS-Bench-201 is 0.82, compared to 0.61 for EcoNAS (a recently proposed reduced-training proxy). Finally, we use these zero-cost proxies to enhance existing NAS search algorithms such as random search, reinforcement learning, evolutionary search and predictor-based search. For all search methodologies and across three different NAS datasets, we are able to significantly improve sample efficiency, and thereby decrease computation, by using our zero-cost proxies. For example on NAS-Bench-101, we achieved the same accuracy 4\$\textbackslashtimes\$ quicker than the best previous result. Our code is made public at: https://github.com/mohsaied/zero-cost-nas.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Abdelfattah-Zero-cost%20proxies%20for%20lightweight%20NAS-2021-ICLR.pdf}
}
@article{TE-NASChenICLR2021, 
year = {2021}, 
title = {{Neural architecture search on ImageNet in four GPU hours: A Theoretically Inspired Perspective}}, 
author = {Chen, Wuyang and Gong, Xinyu and Wang, Zhangyang}, 
journal = {ICLR}, 
eprint = {2102.11535}, 
abstract = {{Neural Architecture Search (NAS) has been explosively studied to automate the discovery of top-performer neural networks. Current works require heavy training of supernet or intensive architecture evaluations, thus suffering from heavy resource consumption and often incurring search bias due to truncated training or approximations. Can we select the best neural architectures without involving any training and eliminate a drastic portion of the search cost? We provide an affirmative answer, by proposing a novel framework called training-free neural architecture search (TE-NAS). TE-NAS ranks architectures by analyzing the spectrum of the neural tangent kernel (NTK) and the number of linear regions in the input space. Both are motivated by recent theory advances in deep networks and can be computed without any training and any label. We show that: (1) these two measurements imply the trainability and expressivity of a neural network; (2) they strongly correlate with the network's test accuracy. Further on, we design a pruning-based NAS mechanism to achieve a more flexible and superior trade-off between the trainability and expressivity during the search. In NAS-Bench-201 and DARTS search spaces, TE-NAS completes high-quality search but only costs 0.5 and 4 GPU hours with one 1080Ti on CIFAR-10 and ImageNet, respectively. We hope our work inspires more attempts in bridging the theoretical findings of deep networks and practical impacts in real NAS applications. Code is available at: https://github.com/VITA-Group/TENAS.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Neural%20architecture%20search%20on%20ImageNet%20in%20four%20GPU%20hours-%20A%20Theoretically%20Inspired%20Perspective-2021-ICLR.pdf}
}
@article{NADSArdywibowoICML2020, 
year = {2020}, 
title = {{NADS: Neural Architecture Distribution Search for Uncertainty Awareness}}, 
author = {Ardywibowo, Randy and Boluki, Shahin and Gong, Xinyu and Wang, Zhangyang and Qian, Xiaoning}, 
journal = {ICML}, 
eprint = {2006.06646}, 
abstract = {{Machine learning (ML) systems often encounter Out-of-Distribution (OoD) errors when dealing with testing data coming from a distribution different from training data. It becomes important for ML systems in critical applications to accurately quantify its predictive uncertainty and screen out these anomalous inputs. However, existing OoD detection approaches are prone to errors and even sometimes assign higher likelihoods to OoD samples. Unlike standard learning tasks, there is currently no well established guiding principle for designing OoD detection architectures that can accurately quantify uncertainty. To address these problems, we first seek to identify guiding principles for designing uncertainty-aware architectures, by proposing Neural Architecture Distribution Search (NADS). NADS searches for a distribution of architectures that perform well on a given task, allowing us to identify common building blocks among all uncertainty-aware architectures. With this formulation, we are able to optimize a stochastic OoD detection objective and construct an ensemble of models to perform OoD detection. We perform multiple OoD detection experiments and observe that our NADS performs favorably, with up to 57\% improvement in accuracy compared to state-of-the-art methods among 15 different testing configurations.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ardywibowo-NADS-%20Neural%20Architecture%20Distribution%20Search%20for%20Uncertainty%20Awareness-2020-ICML.pdf}
}
@article{K-shotNASSuICML2021, 
year = {2021}, 
title = {{K-shot NAS: Learnable Weight-Sharing for NAS with K-shot Supernets}}, 
author = {Su, Xiu and You, Shan and Zheng, Mingkai and Wang, Fei and Qian, Chen and Zhang, Changshui and Xu, Chang}, 
journal = {ICML}, 
eprint = {2106.06442}, 
abstract = {{In one-shot weight sharing for NAS, the weights of each operation (at each layer) are supposed to be identical for all architectures (paths) in the supernet. However, this rules out the possibility of adjusting operation weights to cater for different paths, which limits the reliability of the evaluation results. In this paper, instead of counting on a single supernet, we introduce \$K\$-shot supernets and take their weights for each operation as a dictionary. The operation weight for each path is represented as a convex combination of items in a dictionary with a simplex code. This enables a matrix approximation of the stand-alone weight matrix with a higher rank (\$K>1\$). A \textbackslashtextit\{simplex-net\} is introduced to produce architecture-customized code for each path. As a result, all paths can adaptively learn how to share weights in the \$K\$-shot supernets and acquire corresponding weights for better evaluation. \$K\$-shot supernets and simplex-net can be iteratively trained, and we further extend the search to the channel dimension. Extensive experiments on benchmark datasets validate that K-shot NAS significantly improves the evaluation accuracy of paths and thus brings in impressive performance improvements.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Su-K-shot%20NAS-%20Learnable%20Weight-Sharing%20for%20NAS%20with%20K-shot%20Supernets-2021-ICML.pdf}
}
@article{arch2vecYanNeurIPS2020, 
year = {2020}, 
title = {{Does Unsupervised Architecture Representation Learning Help Neural Architecture Search?}}, 
author = {Yan, Shen and Zheng, Yu and Ao, Wei and Zeng, Xiao and Zhang, Mi}, 
journal = {NeurIPS}, 
eprint = {2006.06936}, 
abstract = {{Existing Neural Architecture Search (NAS) methods either encode neural architectures using discrete encodings that do not scale well, or adopt supervised learning-based methods to jointly learn architecture representations and optimize architecture search on such representations which incurs search bias. Despite the widespread use, architecture representations learned in NAS are still poorly understood. We observe that the structural properties of neural architectures are hard to preserve in the latent space if architecture representation learning and search are coupled, resulting in less effective search performance. In this work, we find empirically that pre-training architecture representations using only neural architectures without their accuracies as labels considerably improve the downstream architecture search efficiency. To explain these observations, we visualize how unsupervised architecture representation learning better encourages neural architectures with similar connections and operators to cluster together. This helps to map neural architectures with similar performance to the same regions in the latent space and makes the transition of architectures in the latent space relatively smooth, which considerably benefits diverse downstream search strategies.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yan-Does%20Unsupervised%20Architecture%20Representation%20Learning%20Help%20Neural%20Architecture%20Search--2020-NeurIPS.pdf}
}
@article{PARSECCasalearXiv2019, 
year = {2019}, 
title = {{Probabilistic neural architecture search}}, 
author = {Casale, Francesco Paolo and Gordon, Jonathan and Fusi, Nicolo}, 
journal = {arXiv}, 
eprint = {1902.05116}, 
abstract = {{In neural architecture search (NAS), the space of neural network architectures is automatically explored to maximize predictive accuracy for a given task. Despite the success of recent approaches, most existing methods cannot be directly applied to large scale problems because of their prohibitive computational complexity or high memory usage. In this work, we propose a Probabilistic approach to neural ARchitecture SEarCh (PARSEC) that drastically reduces memory requirements while maintaining state-of-the-art computational complexity, making it possible to directly search over more complex architectures and larger datasets. Our approach only requires as much memory as is needed to train a single architecture from our search space. This is due to a memory-efficient sampling procedure wherein we learn a probability distribution over high-performing neural network architectures. Importantly, this framework enables us to transfer the distribution of architectures learnt on smaller problems to larger ones, further reducing the computational cost. We showcase the advantages of our approach in applications to CIFAR-10 and ImageNet, where our approach outperforms methods with double its computational cost and matches the performance of methods with costs that are three orders of magnitude larger.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Casale-Probabilistic%20neural%20architecture%20search-2019-arXiv.pdf}
}
@article{ReNASXuCVPR2021, 
year = {2019}, 
title = {{ReNAS: Relativistic Evaluation of Neural Architecture Search}}, 
author = {Xu, Yixing and Wang, Yunhe and Han, Kai and Tang, Yehui and Jui, Shangling and Xu, Chunjing and Xu, Chang}, 
journal = {CVPR}, 
abstract = {{An effective and efficient architecture performance evaluation scheme is essential for the success of Neural Architecture Search (NAS). To save computational cost, most of existing NAS algorithms often train and evaluate intermediate neural architectures on a small proxy dataset with limited training epochs. But it is difficult to expect an accurate performance estimation of an architecture in such a coarse evaluation way. This paper advocates a new neural architecture evaluation scheme, which aims to determine which architecture would perform better instead of accurately predict the absolute architecture performance. Therefore, we propose a \textbackslashtextbf\{relativistic\} architecture performance predictor in NAS (ReNAS). We encode neural architectures into feature tensors, and further refining the representations with the predictor. The proposed relativistic performance predictor can be deployed in discrete searching methods to search for the desired architectures without additional evaluation. Experimental results on NAS-Bench-101 dataset suggests that, sampling 424 (\$0.1\textbackslash\%\$ of the entire search space) neural architectures and their corresponding validation performance is already enough for learning an accurate architecture performance predictor. The accuracies of our searched neural architectures on NAS-Bench-101 and NAS-Bench-201 datasets are higher than that of the state-of-the-art methods and show the priority of the proposed method.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xu-ReNAS-%20Relativistic%20Evaluation%20of%20Neural%20Architecture%20Search-2019-CVPR.pdf}
}
@article{HourNASYangCVPR2021, 
year = {2020}, 
title = {{HourNAS: Extremely Fast Neural Architecture Search Through an Hourglass Lens}}, 
author = {Yang, Zhaohui and Wang, Yunhe and Chen, Xinghao and Guo, Jianyuan and Zhang, Wei and Xu, Chao and Xu, Chunjing and Tao, Dacheng and Xu, Chang}, 
journal = {CVPR}, 
eprint = {2005.14446}, 
abstract = {{Neural Architecture Search (NAS) refers to automatically design the architecture. We propose an hourglass-inspired approach (HourNAS) for this problem that is motivated by the fact that the effects of the architecture often proceed from the vital few blocks. Acting like the narrow neck of an hourglass, vital blocks in the guaranteed path from the input to the output of a deep neural network restrict the information flow and influence the network accuracy. The other blocks occupy the major volume of the network and determine the overall network complexity, corresponding to the bulbs of an hourglass. To achieve an extremely fast NAS while preserving the high accuracy, we propose to identify the vital blocks and make them the priority in the architecture search. The search space of those non-vital blocks is further shrunk to only cover the candidates that are affordable under the computational resource constraints. Experimental results on the ImageNet show that only using 3 hours (0.1 days) with one GPU, our HourNAS can search an architecture that achieves a 77.0\% Top-1 accuracy, which outperforms the state-of-the-art methods.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-HourNAS-%20Extremely%20Fast%20Neural%20Architecture%20Search%20Through%20an%20Hourglass%20Lens-2020-CVPR.pdf}
}
@article{FP-NASYanCVPR2021, 
year = {2021}, 
title = {{FP-NAS: Fast Probabilistic Neural Architecture Search}}, 
author = {Yan, Zhicheng and Dai, Xiaoliang and Zhang, Peizhao and Tian, Yuandong and Wu, Bichen and Feiszli, Matt}, 
journal = {CVPR}, 
eprint = {2011.10949}, 
abstract = {{Differential Neural Architecture Search (NAS) requires all layer choices to be held in memory simultaneously; this limits the size of both search space and final architecture. In contrast, Probabilistic NAS, such as PARSEC, learns a distribution over high-performing architectures, and uses only as much memory as needed to train a single model. Nevertheless, it needs to sample many architectures, making it computationally expensive for searching in an extensive space. To solve these problems, we propose a sampling method adaptive to the distribution entropy, drawing more samples to encourage explorations at the beginning, and reducing samples as learning proceeds. Furthermore, to search fast in the multi-variate space, we propose a coarse-to-fine strategy by using a factorized distribution at the beginning which can reduce the number of architecture parameters by over an order of magnitude. We call this method Fast Probabilistic NAS (FP-NAS). Compared with PARSEC, it can sample 64\% fewer architectures and search 2.1x faster. Compared with FBNetV2, FP-NAS is 1.9x - 3.5x faster, and the searched models outperform FBNetV2 models on ImageNet. FP-NAS allows us to expand the giant FBNetV2 space to be wider (i.e. larger channel choices) and deeper (i.e. more blocks), while adding Split-Attention block and enabling the search over the number of splits. When searching a model of size 0.4G FLOPS, FP-NAS is 132x faster than EfficientNet, and the searched FP-NAS-L0 model outperforms EfficientNet-B0 by 0.7\% accuracy. Without using any architecture surrogate or scaling tricks, we directly search large models up to 1.0G FLOPS. Our FP-NAS-L2 model with simple distillation outperforms BigNAS-XL with advanced in-place distillation by 0.7\% accuracy using similar FLOPS.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yan-FP-NAS-%20Fast%20Probabilistic%20Neural%20Architecture%20Search-2021-CVPR.pdf}
}
@article{EnTranNASYangCVPR2021, 
year = {2021}, 
title = {{Towards improving the consistency, efficiency, and flexibility of differentiable neural architecture search}}, 
author = {Yang, Yibo and You, Shan and Li, Hongyang and Wang, Fei and Qian, Chen and Lin, Zhouchen}, 
journal = {CVPR}, 
eprint = {2101.11342}, 
abstract = {{Most differentiable neural architecture search methods construct a super-net for search and derive a target-net as its sub-graph for evaluation. There exists a significant gap between the architectures in search and evaluation. As a result, current methods suffer from an inconsistent, inefficient, and inflexible search process. In this paper, we introduce EnTranNAS that is composed of Engine-cells and Transit-cells. The Engine-cell is differentiable for architecture search, while the Transit-cell only transits a sub-graph by architecture derivation. Consequently, the gap between the architectures in search and evaluation is significantly reduced. Our method also spares much memory and computation cost, which speeds up the search process. A feature sharing strategy is introduced for more balanced optimization and more efficient search. Furthermore, we develop an architecture derivation method to replace the traditional one that is based on a hand-crafted rule. Our method enables differentiable sparsification, and keeps the derived architecture equivalent to that of Engine-cell, which further improves the consistency between search and evaluation. Besides, it supports the search for topology where a node can be connected to prior nodes with any number of connections, so that the searched architectures could be more flexible. For experiments on CIFAR-10, our search on the standard space requires only 0.06 GPU-day. We further have an error rate of 2.22\% with 0.07 GPU-day for the search on an extended space. We can also directly perform the search on ImageNet with topology learnable and achieve a top-1 error rate of 23.8\% in 2.1 GPU-day.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-Towards%20improving%20the%20consistency,%20efficiency,%20and%20flexibility%20of%20differentiable%20neural%20architecture%20search-2021-CVPR.pdf}
}
@article{RLNASZhangCVPR2021, 
year = {2021}, 
title = {{Neural architecture search with random labels}}, 
author = {Zhang, Xuanyang and Hou, Pengfei and Zhang, Xiangyu and Sun, Jian}, 
journal = {CVPR}, 
eprint = {2101.11834}, 
abstract = {{In this paper, we investigate a new variant of neural architecture search (NAS) paradigm -- searching with random labels (RLNAS). The task sounds counter-intuitive for most existing NAS algorithms since random label provides few information on the performance of each candidate architecture. Instead, we propose a novel NAS framework based on ease-of-convergence hypothesis, which requires only random labels during searching. The algorithm involves two steps: first, we train a SuperNet using random labels; second, from the SuperNet we extract the sub-network whose weights change most significantly during the training. Extensive experiments are evaluated on multiple datasets (e.g. NAS-Bench-201 and ImageNet) and multiple search spaces (e.g. DARTS-like and MobileNet-like). Very surprisingly, RLNAS achieves comparable or even better results compared with state-of-the-art NAS methods such as PC-DARTS, Single Path One-Shot, even though the counterparts utilize full ground truth labels for searching. We hope our finding could inspire new understandings on the essential of NAS.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Neural%20architecture%20search%20with%20random%20labels-2021-CVPR.pdf}
}
@article{AlphaNetWangICML2021, 
year = {2021}, 
title = {{AlphaNet: Improved Training of Supernets with Alpha-Divergence}}, 
author = {Wang, Dilin and Gong, Chengyue and Li, Meng and Liu, Qiang and Chandra, Vikas}, 
journal = {ICML}, 
eprint = {2102.07954}, 
abstract = {{Weight-sharing neural architecture search (NAS) is an effective technique for automating efficient neural architecture design. Weight-sharing NAS builds a supernet that assembles all the architectures as its sub-networks and jointly trains the supernet with the sub-networks. The success of weight-sharing NAS heavily relies on distilling the knowledge of the supernet to the sub-networks. However, we find that the widely used distillation divergence, i.e., KL divergence, may lead to student sub-networks that over-estimate or under-estimate the uncertainty of the teacher supernet, leading to inferior performance of the sub-networks. In this work, we propose to improve the supernet training with a more generalized alpha-divergence. By adaptively selecting the alpha-divergence, we simultaneously prevent the over-estimation or under-estimation of the uncertainty of the teacher model. We apply the proposed alpha-divergence based supernets training to both slimmable neural networks and weight-sharing NAS, and demonstrate significant improvements. Specifically, our discovered model family, AlphaNet, outperforms prior-art models on a wide range of FLOPs regimes, including BigNAS, Once-for-All networks, and AttentiveNAS. We achieve ImageNet top-1 accuracy of 80.0\% with only 444M FLOPs. Our code and pretrained models are available at https://github.com/facebookresearch/AlphaNet.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-AlphaNet-%20Improved%20Training%20of%20Supernets%20with%20Alpha-Divergence-2021-ICML.pdf}
}
@article{HardCoRe-NASNaymanICML2021, 
year = {2021}, 
title = {{HardCoRe-NAS: Hard Constrained diffeRentiable Neural Architecture Search}}, 
author = {Nayman, Niv and Aflalo, Yonathan and Noy, Asaf and Zelnik-Manor, Lihi}, 
journal = {ICML}, 
eprint = {2102.11646}, 
abstract = {{Realistic use of neural networks often requires adhering to multiple constraints on latency, energy and memory among others. A popular approach to find fitting networks is through constrained Neural Architecture Search (NAS), however, previous methods enforce the constraint only softly. Therefore, the resulting networks do not exactly adhere to the resource constraint and their accuracy is harmed. In this work we resolve this by introducing Hard Constrained diffeRentiable NAS (HardCoRe-NAS), that is based on an accurate formulation of the expected resource requirement and a scalable search method that satisfies the hard constraint throughout the search. Our experiments show that HardCoRe-NAS generates state-of-the-art architectures, surpassing other NAS methods, while strictly satisfying the hard resource constraints without any tuning required.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Nayman-HardCoRe-NAS-%20Hard%20Constrained%20diffeRentiable%20Neural%20Architecture%20Search-2021-ICML.pdf}
}
@book{PGMKoller2009, 
year = {2009}, 
title = {{Probabilistic graphical models: Principles and Techniques}}, 
author = {Koller, Daphne and Friedman, Nir}, 
publisher = {MIT press}
}
@article{DenseNetHuangCVPR2017, 
year = {2017}, 
title = {{Densely connected convolutional networks}}, 
author = {Huang, Gao and Liu, Zhuang and Maaten, Laurens van der and Weinberger, Kilian Q.}, 
journal = {CVPR}, 
abstract = {{Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with \$L\$ layers have \$L\$ connections—one between each layer and its subsequent layer—our network has \$\textbackslashfrac\{L(L+1)\}\{2\}\$ direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Huang-Densely%20connected%20convolutional%20networks-2017-CVPR.pdf}
}
@article{EfficientNetV2TanICML2021, 
year = {2021}, 
title = {{EfficientNetV2: Smaller Models and Faster Training}}, 
author = {Tan, Mingxing and Le, Quoc V}, 
journal = {ICML}, 
eprint = {2104.00298}, 
abstract = {{This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller. Our training can be further sped up by progressively increasing the image size during training, but it often causes a drop in accuracy. To compensate for this accuracy drop, we propose to adaptively adjust regularization (e.g., dropout and data augmentation) as well, such that we can achieve both fast training and good accuracy. With progressive learning, our EfficientNetV2 significantly outperforms previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, our EfficientNetV2 achieves 87.3\% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0\% accuracy while training 5x-11x faster using the same computing resources. Code will be available at https://github.com/google/automl/tree/master/efficientnetv2.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tan-EfficientNetV2-%20Smaller%20Models%20and%20Faster%20Training-2021-ICML.pdf}
}
@article{OPANASLiangCVPR2021, 
year = {2021}, 
title = {{OPANAS: One-Shot Path Aggregation Network Architecture Search for Object Detection}}, 
author = {Liang, Tingting and Wang, Yongtao and Tang, Zhi and Hu, Guosheng and Ling, Haibin}, 
journal = {CVPR}, 
eprint = {2103.04507}, 
abstract = {{Recently, neural architecture search (NAS) has been exploited to design feature pyramid networks (FPNs) and achieved promising results for visual object detection. Encouraged by the success, we propose a novel One-Shot Path Aggregation Network Architecture Search (OPANAS) algorithm, which significantly improves both searching efficiency and detection accuracy. Specifically, we first introduce six heterogeneous information paths to build our search space, namely top-down, bottom-up, fusing-splitting, scale-equalizing, skip-connect and none. Second, we propose a novel search space of FPNs, in which each FPN candidate is represented by a densely-connected directed acyclic graph (each node is a feature pyramid and each edge is one of the six heterogeneous information paths). Third, we propose an efficient one-shot search method to find the optimal path aggregation architecture, that is, we first train a super-net and then find the optimal candidate with an evolutionary algorithm. Experimental results demonstrate the efficacy of the proposed OPANAS for object detection: (1) OPANAS is more efficient than state-of-the-art methods (e.g., NAS-FPN and Auto-FPN), at significantly smaller searching cost (e.g., only 4 GPU days on MS-COCO); (2) the optimal architecture found by OPANAS significantly improves main-stream detectors including RetinaNet, Faster R-CNN and Cascade R-CNN, by 2.3-3.2 \% mAP comparing to their FPN counterparts; and (3) a new state-of-the-art accuracy-speed trade-off (52.2 \% mAP at 7.6 FPS) at smaller training costs than comparable state-of-the-arts. Code will be released at https://github.com/VDIGPKU/OPANAS.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liang-OPANAS-%20One-Shot%20Path%20Aggregation%20Network%20Architecture%20Search%20for%20Object%20Detection-2021-CVPR.pdf}
}
@article{XGBoostChenKDD2016, 
year = {2016}, 
title = {{XGBoost: A Scalable Tree Boosting System}}, 
author = {Chen, Tianqi and Guestrin, Carlos}, 
journal = {KDD}, 
abstract = {{Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end- to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-XGBoost-%20A%20Scalable%20Tree%20Boosting%20System-2016-KDD.pdf}
}
@article{MRFKomodakisICCV2007, 
year = {2007}, 
title = {{MRF optimization via dual decomposition: Message-Passing Revisited}}, 
author = {Komodakis, N. and Paragios, N. and Tziritas, G.}, 
journal = {ICCV}, 
issn = {1550-5499}, 
abstract = {{A new message-passing scheme for MRF optimization is proposed in this paper. This scheme inherits better theoretical properties than all other state-of-the-art message passing methods and in practice performs equally well/outperforms them. It is based on the very powerful technique of Dual Decomposition [1] and leads to an elegant and general framework for understanding/designing message-passing algorithms that can provide new insights into existing techniques. Promising experimental results and comparisons with the state of the art demonstrate the extreme theoretical and practical potentials of our approach.}}, 
pages = {1--8}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Komodakis-MRF%20optimization%20via%20dual%20decomposition-%20Message-Passing%20Revisited-2007-ICCV.pdf}
}
@article{LISTAMengICLR2021, 
year = {2021}, 
title = {{A design space study for LISTA and beyond}}, 
author = {Meng, Tianjian and Chen, Xiaohan and Jiang, Yifan and Wang, Zhangyang}, 
journal = {ICLR}, 
eprint = {2104.04110}, 
abstract = {{In recent years, great success has been witnessed in building problem-specific deep networks from unrolling iterative algorithms, for solving inverse problems and beyond. Unrolling is believed to incorporate the model-based prior with the learning capacity of deep learning. This paper revisits the role of unrolling as a design approach for deep networks: to what extent its resulting special architecture is superior, and can we find better? Using LISTA for sparse recovery as a representative example, we conduct the first thorough design space study for the unrolled models. Among all possible variations, we focus on extensively varying the connectivity patterns and neuron types, leading to a gigantic design space arising from LISTA. To efficiently explore this space and identify top performers, we leverage the emerging tool of neural architecture search (NAS). We carefully examine the searched top architectures in a number of settings, and are able to discover networks that are consistently better than LISTA. We further present more visualization and analysis to "open the black box", and find that the searched top architectures demonstrate highly consistent and potentially transferable patterns. We hope our study to spark more reflections and explorations on how to better mingle model-based optimization prior and data-driven learning.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Meng-A%20design%20space%20study%20for%20LISTA%20and%20beyond-2021-ICLR.pdf}
}
@article{BSQYangICLR2021, 
year = {2021}, 
title = {{BSQ: Exploring Bit-Level Sparsity for Mixed-Precision Neural Network Quantization}}, 
author = {Yang, Huanrui and Duan, Lin and Chen, Yiran and Li, Hai}, 
journal = {ICLR}, 
eprint = {2102.10462}, 
abstract = {{Mixed-precision quantization can potentially achieve the optimal tradeoff between performance and compression rate of deep neural networks, and thus, have been widely investigated. However, it lacks a systematic method to determine the exact quantization scheme. Previous methods either examine only a small manually-designed search space or utilize a cumbersome neural architecture search to explore the vast search space. These approaches cannot lead to an optimal quantization scheme efficiently. This work proposes bit-level sparsity quantization (BSQ) to tackle the mixed-precision quantization from a new angle of inducing bit-level sparsity. We consider each bit of quantized weights as an independent trainable variable and introduce a differentiable bit-sparsity regularizer. BSQ can induce all-zero bits across a group of weight elements and realize the dynamic precision reduction, leading to a mixed-precision quantization scheme of the original model. Our method enables the exploration of the full mixed-precision space with a single gradient-based optimization process, with only one hyperparameter to tradeoff the performance and compression. BSQ achieves both higher accuracy and higher bit reduction on various model architectures on the CIFAR-10 and ImageNet datasets comparing to previous methods.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-BSQ-%20Exploring%20Bit-Level%20Sparsity%20for%20Mixed-Precision%20Neural%20Network%20Quantization-2021-ICLR.pdf}
}
@article{DDWangarXiv2018, 
year = {2018}, 
title = {{Dataset distillation}}, 
author = {Wang, Tongzhou and Zhu, Jun-Yan and Torralba, Antonio and Efros, Alexei A}, 
journal = {arXiv}, 
eprint = {1811.10959}, 
abstract = {{Model distillation aims to distill the knowledge of a complex model into a simpler one. In this paper, we consider an alternative formulation called dataset distillation: we keep the model fixed and instead attempt to distill the knowledge from a large training dataset into a small one. The idea is to synthesize a small number of data points that do not need to come from the correct data distribution, but will, when given to the learning algorithm as training data, approximate the model trained on the original data. For example, we show that it is possible to compress 60,000 MNIST training images into just 10 synthetic distilled images (one per class) and achieve close to original performance with only a few gradient descent steps, given a fixed network initialization. We evaluate our method in various initialization settings and with different learning objectives. Experiments on multiple datasets show the advantage of our approach compared to alternative methods.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Dataset%20distillation-2018-arXiv.pdf}
}
@article{DCZhaoICLR2021, 
year = {2021}, 
title = {{Dataset condensation with gradient matching}}, 
author = {Zhao, Bo and Mopuri, Konda Reddy and Bilen, Hakan}, 
journal = {ICLR}, 
eprint = {2006.05929}, 
abstract = {{As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing datasets and training models on them become significantly more expensive. This paper proposes a training set synthesis technique for data-efficient learning, called Dataset Condensation, that learns to condense large dataset into a small set of informative synthetic samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of deep neural network weights that are trained on the original and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and report promising gains when limited memory and computations are available.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhao-Dataset%20condensation%20with%20gradient%20matching-2021-ICLR.pdf}
}
@article{CompOFASahniICLR2021, 
year = {2021}, 
title = {{CompOFA: Compound Once-For-All Networks for Faster Multi-Platform Deployment}}, 
author = {Sahni, Manas and Varshini, Shreya and Khare, Alind and Tumanov, Alexey}, 
journal = {ICLR}, 
eprint = {2104.12642}, 
abstract = {{The emergence of CNNs in mainstream deployment has necessitated methods to design and train efficient architectures tailored to maximize the accuracy under diverse hardware \& latency constraints. To scale these resource-intensive tasks with an increasing number of deployment targets, Once-For-All (OFA) proposed an approach to jointly train several models at once with a constant training cost. However, this cost remains as high as 40-50 GPU days and also suffers from a combinatorial explosion of sub-optimal model configurations. We seek to reduce this search space -- and hence the training budget -- by constraining search to models close to the accuracy-latency Pareto frontier. We incorporate insights of compound relationships between model dimensions to build CompOFA, a design space smaller by several orders of magnitude. Through experiments on ImageNet, we demonstrate that even with simple heuristics we can achieve a 2x reduction in training time and 216x speedup in model search/extraction time compared to the state of the art, without loss of Pareto optimality! We also show that this smaller design space is dense enough to support equally accurate models for a similar diversity of hardware and latency targets, while also reducing the complexity of the training and subsequent extraction algorithms.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Sahni-CompOFA-%20Compound%20Once-For-All%20Networks%20for%20Faster%20Multi-Platform%20Deployment-2021-ICLR.pdf}
}
@article{LovaszHingeYuTPAMI2018, 
year = {2018}, 
title = {{The Lovász Hinge: A Novel Convex Surrogate for Submodular Losses}}, 
author = {Yu, Jiaqian and Blaschko, Matthew B.}, 
journal = {TPAMI}, 
abstract = {{Learning with non-modular losses is an important problem when sets of predictions are made simultaneously. The main tools for constructing convex surrogate loss functions for set prediction are margin rescaling and slack rescaling. In this work, we show that these strategies lead to tight convex surrogates iff the underlying loss function is increasing in the number of incorrect predictions. However, gradient or cutting-plane computation for these functions is NP-hard for non-supermodular loss functions. We propose instead a novel surrogate loss function for submodular losses, the Lovász hinge, which leads to \$\textbackslashmathcal \{O\}(p\textbackslash; \textbackslashlog p)\$O(plogp) complexity with \$\textbackslashmathcal \{O\}(p)\$O(p) oracle accesses to the loss function to compute a gradient or cutting-plane. We prove that the Lovász hinge is convex and yields an extension. As a result, we have developed the first tractable convex surrogates in the literature for submodular losses. We demonstrate the utility of this novel convex surrogate through several set prediction tasks, including on the PASCAL VOC and Microsoft COCO datasets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yu-The%20Lovász%20Hinge-%20A%20Novel%20Convex%20Surrogate%20for%20Submodular%20Losses-2018-TPAMI.pdf}
}
@article{DeepEnsemblesLakshminarayananNeurIPS2017, 
year = {2017}, 
title = {{Simple and scalable predictive uncertainty estimation using deep ensembles}}, 
author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles}, 
journal = {NeurIPS}, 
eprint = {1612.01474}, 
abstract = {{Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lakshminarayanan-Simple%20and%20scalable%20predictive%20uncertainty%20estimation%20using%20deep%20ensembles-2017-NeurIPS.pdf}
}
@article{DeepVIBAlemiICLR2017, 
year = {2017}, 
title = {{Deep variational information bottleneck}}, 
author = {Alemi, Alexander A and Fischer, Ian and Dillon, Joshua V and Murphy, Kevin}, 
journal = {ICLR}, 
eprint = {1612.00410}, 
abstract = {{We present a variational approximation to the information bottleneck of Tishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method "Deep Variational Information Bottleneck", or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Alemi-Deep%20variational%20information%20bottleneck-2017-ICLR.pdf}
}
@article{DrNASChenICLR2021, 
year = {2021}, 
title = {{DrNAS: Dirichlet Neural Architecture Search}}, 
author = {Chen, Xiangning and Wang, Ruochen and Cheng, Minhao and Tang, Xiaocheng and Hsieh, Cho-Jui}, 
journal = {ICLR}, 
eprint = {2006.10355}, 
abstract = {{This paper proposes a novel differentiable architecture search method by formulating it into a distribution learning problem. We treat the continuously relaxed architecture mixing weight as random variables, modeled by Dirichlet distribution. With recently developed pathwise derivatives, the Dirichlet parameters can be easily optimized with gradient-based optimizer in an end-to-end manner. This formulation improves the generalization ability and induces stochasticity that naturally encourages exploration in the search space. Furthermore, to alleviate the large memory consumption of differentiable NAS, we propose a simple yet effective progressive learning scheme that enables searching directly on large-scale tasks, eliminating the gap between search and evaluation phases. Extensive experiments demonstrate the effectiveness of our method. Specifically, we obtain a test error of 2.46\% for CIFAR-10, 23.7\% for ImageNet under the mobile setting. On NAS-Bench-201, we also achieve state-of-the-art results on all three datasets and provide insights for the effective design of neural architecture search algorithms.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-DrNAS-%20Dirichlet%20Neural%20Architecture%20Search-2021-ICLR.pdf}
}
@article{Joint-DetNASYaoCVPR2021, 
year = {2021}, 
title = {{Joint-DetNAS: Upgrade Your Detector with NAS, Pruning and Dynamic Distillation}}, 
author = {Yao, Lewei and Pi, Renjie and Xu, Hang and Zhang, Wei and Li, Zhenguo and Zhang, Tong}, 
journal = {CVPR}, 
eprint = {2105.12971}, 
abstract = {{We propose Joint-DetNAS, a unified NAS framework for object detection, which integrates 3 key components: Neural Architecture Search, pruning, and Knowledge Distillation. Instead of naively pipelining these techniques, our Joint-DetNAS optimizes them jointly. The algorithm consists of two core processes: student morphism optimizes the student's architecture and removes the redundant parameters, while dynamic distillation aims to find the optimal matching teacher. For student morphism, weight inheritance strategy is adopted, allowing the student to flexibly update its architecture while fully utilize the predecessor's weights, which considerably accelerates the search; To facilitate dynamic distillation, an elastic teacher pool is trained via integrated progressive shrinking strategy, from which teacher detectors can be sampled without additional cost in subsequent searches. Given a base detector as the input, our algorithm directly outputs the derived student detector with high performance without additional training. Experiments demonstrate that our Joint-DetNAS outperforms the naive pipelining approach by a great margin. Given a classic R101-FPN as the base detector, Joint-DetNAS is able to boost its mAP from 41.4 to 43.9 on MS COCO and reduce the latency by 47\%, which is on par with the SOTA EfficientDet while requiring less search cost. We hope our proposed method can provide the community with a new way of jointly optimizing NAS, KD and pruning.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yao-Joint-DetNAS-%20Upgrade%20Your%20Detector%20with%20NAS,%20Pruning%20and%20Dynamic%20Distillation-2021-CVPR.pdf}
}
@article{NAS-BOWLRuICLR2021, 
year = {2021}, 
title = {{Interpretable neural architecture search via Bayesian optimization with Weisfeiler-Lehman kernels}}, 
author = {Ru, Binxin and Wan, Xingchen and Dong, Xiaowen and Osborne, Michael}, 
journal = {ICLR}, 
eprint = {2006.07556}, 
abstract = {{Current neural architecture search (NAS) strategies focus only on finding a single, good, architecture. They offer little insight into why a specific network is performing well, or how we should modify the architecture if we want further improvements. We propose a Bayesian optimisation (BO) approach for NAS that combines the Weisfeiler-Lehman graph kernel with a Gaussian process surrogate. Our method optimises the architecture in a highly data-efficient manner: it is capable of capturing the topological structures of the architectures and is scalable to large graphs, thus making the high-dimensional and graph-like search spaces amenable to BO. More importantly, our method affords interpretability by discovering useful network features and their corresponding impact on the network performance. Indeed, we demonstrate empirically that our surrogate model is capable of identifying useful motifs which can guide the generation of new architectures. We finally show that our method outperforms existing NAS approaches to achieve the state of the art on both closed- and open-domain search spaces.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ru-Interpretable%20neural%20architecture%20search%20via%20Bayesian%20optimization%20with%20Weisfeiler-Lehman%20kernels-2021-ICLR.pdf}
}
@article{GAEALiICLR2021, 
year = {2021}, 
title = {{Geometry-aware gradient algorithms for neural architecture search}}, 
author = {Li, Liam and Khodak, Mikhail and Balcan, Maria-Florina and Talwalkar, Ameet}, 
journal = {ICLR}, 
eprint = {2004.07802}, 
abstract = {{Recent state-of-the-art methods for neural architecture search (NAS) exploit gradient-based optimization by relaxing the problem into continuous optimization over architectures and shared-weights, a noisy process that remains poorly understood. We argue for the study of single-level empirical risk minimization to understand NAS with weight-sharing, reducing the design of NAS methods to devising optimizers and regularizers that can quickly obtain high-quality solutions to this problem. Invoking the theory of mirror descent, we present a geometry-aware framework that exploits the underlying structure of this optimization to return sparse architectural parameters, leading to simple yet novel algorithms that enjoy fast convergence guarantees and achieve state-of-the-art accuracy on the latest NAS benchmarks in computer vision. Notably, we exceed the best published results for both CIFAR and ImageNet on both the DARTS search space and NAS-Bench201; on the latter we achieve near-oracle-optimal performance on CIFAR-10 and CIFAR-100. Together, our theory and experiments demonstrate a principled way to co-design optimizers and continuous relaxations of discrete NAS search spaces.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Geometry-aware%20gradient%20algorithms%20for%20neural%20architecture%20search-2021-ICLR.pdf}
}
@article{PostRousseauISBI2021, 
year = {2021}, 
title = {{Post training uncertainty calibration of deep networks for medical image segmentation}}, 
author = {Rousseau, Axel-Jan and Becker, Thijs and Bertels, Jeroen and Blaschko, Matthew B. and Valkenborg, Dirk}, 
journal = {ISBI}, 
abstract = {{Neural networks for automated image segmentation are typically trained to achieve maximum accuracy, while less attention has been given to the calibration of their confidence scores. However, well-calibrated confidence scores provide valuable information towards the user. We investigate several post hoc calibration methods that are straightforward to implement, some of which are novel. They are compared to Monte Carlo (MC) dropout and are applied to neural networks trained with cross-entropy (CE) and soft Dice (SD) losses on BraTS 2018 and ISLES 2018. Surprisingly, models trained on SD loss are not necessarily less calibrated than those trained on CE loss. In all cases, at least one post hoc method improves the calibration. There is limited consistency across the results, so we can’t conclude on one method being superior. In all cases, post hoc calibration is competitive with MC dropout. Although average calibration improves compared to the base model, subject-level variance of the calibration remains similar.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Rousseau-Post%20training%20uncertainty%20calibration%20of%20deep%20networks%20for%20medical%20image%20segmentation-2021-ISBI.pdf}
}
@article{TheoreticalBertelsMIA2021, 
year = {2021}, 
title = {{Theoretical analysis and experimental validation of volume bias of soft Dice optimized segmentation maps in the context of inherent uncertainty}}, 
author = {Bertels, Jeroen and Robben, David and Vandermeulen, Dirk and Suetens, Paul}, 
journal = {MIA}, 
abstract = {{The clinical interest is often to measure the volume of a structure, which is typically derived from a segmentation. In order to evaluate and compare segmentation methods, the similarity between a segmentation and a predefined ground truth is measured using popular discrete metrics, such as the Dice score. Recent segmentation methods use a differentiable surrogate metric, such as soft Dice, as part of the loss function during the learning phase. In this work, we first briefly describe how to derive volume estimates from a segmentation that is, potentially, inherently uncertain or ambiguous. This is followed by a theoretical analysis and an experimental validation linking the inherent uncertainty to common loss functions for training CNNs, namely cross-entropy and soft Dice. We find that, even though soft Dice optimization leads to an improved performance with respect to the Dice score and other measures, it may introduce a volume bias for tasks with high inherent uncertainty. These findings indicate some of the method’s clinical limitations and suggest doing a closer ad-hoc volume analysis with an optional re-calibration step.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bertels-Theoretical%20analysis%20and%20experimental%20validation%20of%20volume%20bias%20of%20soft%20Dice%20optimized%20segmentation%20maps%20in%20the%20context%20of%20inherent%20uncertainty-2021-MIA.pdf}
}
@article{DisentanglingZhangNeurIPS2020, 
year = {2020}, 
title = {{Disentangling human error from the ground truth in segmentation of medical images}}, 
author = {Zhang, Le and Tanno, Ryutaro and Xu, Mou-Cheng and Jin, Chen and Jacob, Joseph and Ciccarelli, Olga and Barkhof, Frederik and Alexander, Daniel C}, 
journal = {NeurIPS}, 
eprint = {2007.15963}, 
abstract = {{Recent years have seen increasing use of supervised learning methods for segmentation tasks. However, the predictive performance of these algorithms depends on the quality of labels. This problem is particularly pertinent in the medical image domain, where both the annotation cost and inter-observer variability are high. In a typical label acquisition process, different human experts provide their estimates of the "true" segmentation labels under the influence of their own biases and competence levels. Treating these noisy labels blindly as the ground truth limits the performance that automatic segmentation algorithms can achieve. In this work, we present a method for jointly learning, from purely noisy observations alone, the reliability of individual annotators and the true segmentation label distributions, using two coupled CNNs. The separation of the two is achieved by encouraging the estimated annotators to be maximally unreliable while achieving high fidelity with the noisy training data. We first define a toy segmentation dataset based on MNIST and study the properties of the proposed algorithm. We then demonstrate the utility of the method on three public medical imaging segmentation datasets with simulated (when necessary) and real diverse annotations: 1) MSLSC (multiple-sclerosis lesions); 2) BraTS (brain tumours); 3) LIDC-IDRI (lung abnormalities). In all cases, our method outperforms competing methods and relevant baselines particularly in cases where the number of annotations is small and the amount of disagreement is large. The experiments also show strong ability to capture the complex spatial characteristics of annotators' mistakes.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Disentangling%20human%20error%20from%20the%20ground%20truth%20in%20segmentation%20of%20medical%20images-2020-NeurIPS.pdf}
}
@article{BANANASWhiteAAAI2021, 
year = {2021}, 
title = {{BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture Search}}, 
author = {White, Colin and Neiswanger, Willie and Savani, Yash}, 
journal = {AAAI}, 
eprint = {1910.11858}, 
abstract = {{Over the past half-decade, many methods have been considered for neural architecture search (NAS). Bayesian optimization (BO), which has long had success in hyperparameter optimization, has recently emerged as a very promising strategy for NAS when it is coupled with a neural predictor. Recent work has proposed different instantiations of this framework, for example, using Bayesian neural networks or graph convolutional networks as the predictive model within BO. However, the analyses in these papers often focus on the full-fledged NAS algorithm, so it is difficult to tell which individual components of the framework lead to the best performance. In this work, we give a thorough analysis of the "BO + neural predictor" framework by identifying five main components: the architecture encoding, neural predictor, uncertainty calibration method, acquisition function, and acquisition optimization strategy. We test several different methods for each component and also develop a novel path-based encoding scheme for neural architectures, which we show theoretically and empirically scales better than other encodings. Using all of our analyses, we develop a final algorithm called BANANAS, which achieves state-of-the-art performance on NAS search spaces. We adhere to the NAS research checklist (Lindauer and Hutter 2019) to facilitate best practices, and our code is available at https://github.com/naszilla/naszilla.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/White-BANANAS-%20Bayesian%20Optimization%20with%20Neural%20Architectures%20for%20Neural%20Architecture%20Search-2021-AAAI.pdf}
}
@article{CalibrationGuoICML2017, 
year = {2017}, 
title = {{On calibration of modern neural networks}}, 
author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q}, 
journal = {ICML}, 
eprint = {1706.04599}, 
abstract = {{Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Guo-On%20calibration%20of%20modern%20neural%20networks-2017-ICML.pdf}
}
@article{ProbabilisticU-netKohlNeurIPS2018, 
year = {2018}, 
title = {{A probabilistic U-net for segmentation of ambiguous images}}, 
author = {Kohl, Simon A A and Romera-Paredes, Bernardino and Meyer, Clemens and Fauw, Jeffrey De and Ledsam, Joseph R and Maier-Hein, Klaus H and Eslami, S M Ali and Rezende, Danilo Jimenez and Ronneberger, Olaf}, 
journal = {NeurIPS}, 
eprint = {1806.05034}, 
abstract = {{Many real-world vision problems suffer from inherent ambiguities. In clinical applications for example, it might not be clear from a CT scan alone which particular region is cancer tissue. Therefore a group of graders typically produces a set of diverse but plausible segmentations. We consider the task of learning a distribution over segmentations given an input. To this end we propose a generative segmentation model based on a combination of a U-Net with a conditional variational autoencoder that is capable of efficiently producing an unlimited number of plausible hypotheses. We show on a lung abnormalities segmentation task and on a Cityscapes segmentation task that our model reproduces the possible segmentation variants as well as the frequencies with which they occur, doing so significantly better than published approaches. These models could have a high impact in real-world applications, such as being used as clinical decision-making algorithms accounting for multiple plausible semantic segmentation hypotheses to provide possible diagnoses and recommend further actions to resolve the present ambiguities.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kohl-A%20probabilistic%20U-net%20for%20segmentation%20of%20ambiguous%20images-2018-NeurIPS.pdf}
}
@article{EncodingWhiteNeurIPS2020, 
year = {2020}, 
title = {{A study on encodings for neural architecture search}}, 
author = {White, Colin and Neiswanger, Willie and Nolen, Sam and Savani, Yash}, 
journal = {NeurIPS}, 
eprint = {2007.04965}, 
abstract = {{Neural architecture search (NAS) has been extensively studied in the past few years. A popular approach is to represent each neural architecture in the search space as a directed acyclic graph (DAG), and then search over all DAGs by encoding the adjacency matrix and list of operations as a set of hyperparameters. Recent work has demonstrated that even small changes to the way each architecture is encoded can have a significant effect on the performance of NAS algorithms. In this work, we present the first formal study on the effect of architecture encodings for NAS, including a theoretical grounding and an empirical study. First we formally define architecture encodings and give a theoretical characterization on the scalability of the encodings we study Then we identify the main encoding-dependent subroutines which NAS algorithms employ, running experiments to show which encodings work best with each subroutine for many popular algorithms. The experiments act as an ablation study for prior work, disentangling the algorithmic and encoding-based contributions, as well as a guideline for future work. Our results demonstrate that NAS encodings are an important design decision which can have a significant impact on overall performance. Our code is available at https://github.com/naszilla/nas-encodings.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/White-A%20study%20on%20encodings%20for%20neural%20architecture%20search-2020-NeurIPS.pdf}
}
@article{ConstrainedMailearXiv2021, 
year = {2021}, 
title = {{On constrained optimization in differentiable neural architecture search}}, 
author = {Maile, Kaitlin and Lecarpentier, Erwan and Luga, Herve and Wilson, Dennis G.}, 
journal = {arXiv}, 
abstract = {{Differentiable  Architecture  Search  (DARTS)  is  a  recently proposed neural architecture search (NAS) method based on a differentiable relaxation. Due to its success, numerous variants analyzing and improving parts of the DARTS framework have recently been proposed.By considering the problem as a constrained bilevel optimization, we propose and analyze three improvements to architectural weight com-petition, update scheduling, and regularization towards discretization. First, we introduce a new  approach  to  the  activation  of  architecture weights, which prevents confounding competition within an edge and allows for fair comparison across edges to aid in discretization. Next, we propose a dynamic schedule based on per-minibatch network information to make architecture updates more informed. Finally, we consider two regularizations, based on proximity to discretization and the Alternating Directions Method of Multipliers (ADMM) algorithm, to promote early discretization. Our results show that this new activation scheme reduces final architecture size and the regularizations improve reliability in search results while maintaining comparable performance to state-of-the-art in NAS, especially when used with our new dynamic informed schedule.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Maile-On%20constrained%20optimization%20in%20differentiable%20neural%20architecture%20search-2021-arXiv.pdf}
}
@article{ExploringWhiteUAI2021, 
year = {2021}, 
title = {{Exploring the loss landscape in neural architecture search}}, 
author = {White, Colin and Nolen, Sam and Savani, Yash}, 
journal = {UAI}, 
eprint = {2005.02960}, 
abstract = {{Neural architecture search (NAS) has seen a steep rise in interest over the last few years. Many algorithms for NAS consist of searching through a space of architectures by iteratively choosing an architecture, evaluating its performance by training it, and using all prior evaluations to come up with the next choice. The evaluation step is noisy - the final accuracy varies based on the random initialization of the weights. Prior work has focused on devising new search algorithms to handle this noise, rather than quantifying or understanding the level of noise in architecture evaluations. In this work, we show that (1) the simplest hill-climbing algorithm is a powerful baseline for NAS, and (2), when the noise in popular NAS benchmark datasets is reduced to a minimum, hill-climbing to outperforms many popular state-of-the-art algorithms. We further back up this observation by showing that the number of local minima is substantially reduced as the noise decreases, and by giving a theoretical characterization of the performance of local search in NAS. Based on our findings, for NAS research we suggest (1) using local search as a baseline, and (2) denoising the training pipeline when possible.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/White-Exploring%20the%20loss%20landscape%20in%20neural%20architecture%20search-2021-UAI.pdf}
}
@article{NESZaidiICMLWorkshop2020, 
year = {2020}, 
title = {{Neural ensemble search for uncertainty estimation and dataset shift}}, 
author = {Zaidi, Sheheryar and Zela, Arber and Elsken, Thomas and Holmes, Chris and Hutter, Frank and Teh, Yee Whye}, 
journal = {ICML Workshop}, 
eprint = {2006.08573}, 
abstract = {{Ensembles of neural networks achieve superior performance compared to stand-alone networks in terms of accuracy, uncertainty calibration and robustness to dataset shift. \textbackslashemph\{Deep ensembles\}, a state-of-the-art method for uncertainty estimation, only ensemble random initializations of a \textbackslashemph\{fixed\} architecture. Instead, we propose two methods for automatically constructing ensembles with \textbackslashemph\{varying\} architectures, which implicitly trade-off individual architectures' strengths against the ensemble's diversity and exploit architectural variation as a source of diversity. On a variety of classification tasks and modern architecture search spaces, we show that the resulting ensembles outperform deep ensembles not only in terms of accuracy but also uncertainty calibration and robustness to dataset shift. Our further analysis and ablation studies provide evidence of higher ensemble diversity due to architectural variation, resulting in ensembles that can outperform deep ensembles, even when having weaker average base learners.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zaidi-Neural%20ensemble%20search%20for%20uncertainty%20estimation%20and%20dataset%20shift-2020-ICML%20Workshop.pdf}
}
@article{DIPUlyanovCVPR2018, 
year = {2018}, 
title = {{Deep image prior}}, 
author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor}, 
journal = {CVPR}, 
abstract = {{Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, super-resolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs. Apart from its diverse applications, our approach highlights the inductive bias captured by standard generator network architectures. It also bridges the gap between two very popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity.}}, 
pages = {9446--9454}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ulyanov-Deep%20image%20prior-2018-CVPR.pdf}
}
@article{NAS-DIPChenECCV2020, 
year = {2020}, 
title = {{NAS-DIP: Learning Deep Image Prior with Neural Architecture Search}}, 
author = {Chen, Yun-Chun and Gao, Chen and Robb, Esther and Huang, Jia-Bin}, 
journal = {ECCV}, 
eprint = {2008.11713}, 
abstract = {{Recent work has shown that the structure of deep convolutional neural networks can be used as a structured image prior for solving various inverse image restoration tasks. Instead of using hand-designed architectures, we propose to search for neural architectures that capture stronger image priors. Building upon a generic U-Net architecture, our core contribution lies in designing new search spaces for (1) an upsampling cell and (2) a pattern of cross-scale residual connections. We search for an improved network by leveraging an existing neural architecture search algorithm (using reinforcement learning with a recurrent neural network controller). We validate the effectiveness of our method via a wide variety of applications, including image restoration, dehazing, image-to-image translation, and matrix factorization. Extensive experimental results show that our algorithm performs favorably against state-of-the-art learning-free approaches and reaches competitive performance with existing learning-based methods in some cases.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-NAS-DIP-%20Learning%20Deep%20Image%20Prior%20with%20Neural%20Architecture%20Search-2020-ECCV.pdf}
}
@article{IMAGENETCPHendrycksICLR2019, 
year = {2019}, 
title = {{Benchmarking neural network robustness to common corruptions and perturbations}}, 
author = {Hendrycks, Dan and Dietterich, Thomas}, 
journal = {ICLR}, 
eprint = {1903.12261}, 
abstract = {{In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hendrycks-Benchmarking%20neural%20network%20robustness%20to%20common%20corruptions%20and%20perturbations-2019-ICLR.pdf}
}
@article{OnPopordanoskaMICCAI2021, 
year = {2021}, 
title = {{On the relationship between calibrated predictors and unbiased volume estimation}}, 
author = {Popordanoska, Teodora and Bertels, Jeroen and Vandermeulen, Dirk and Maes, Frederik and Blaschko, Matthew B.}, 
journal = {MICCAI}, 
abstract = {{Machine learning driven medical image segmentation has become standard in medical image analysis. However, deep learning models are prone to overconfident predictions. This has lead to a renewed focus on calibrated predictions in the medical imaging and broader machine learning communities. Calibrated predictions are estimates of the prob- ability of a label that correspond to the true expected value of the label conditioned on the confidence. Such calibrated predictions have utility in a range of medical imaging applications, including surgical planning under uncertainty and active learning systems. At the same time it is often an accurate volume measurement that is of real importance for many medical applications. This work investigates the relationship between model calibration and volume estimation. We demonstrate both mathematically and empirically that if the predictor is calibrated per image, we can obtain the correct volume by taking an expectation of the probability scores per pixel/voxel of the image. Furthermore, we show that linear combinations of calibrated classifiers preserve volume estimation, but do not preserve calibration. Therefore, we conclude that having a calibrated predictor is a sufficient, but not necessary condition for obtaining an unbiased estimate of the volume. We validate our theoretical findings empirically on a collection of 18 different (calibrated) training strategies on the tasks of glioma volume estimation on BraTS 2018, and ischemic stroke lesion volume estimation on ISLES 2018 datasets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Popordanoska-On%20the%20relationship%20between%20calibrated%20predictors%20and%20unbiased%20volume%20estimation-2021-MICCAI.pdf}
}
@article{CanOvadiaNeurIPS2019, 
year = {2019}, 
title = {{Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift}}, 
author = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, D and Nowozin, Sebastian and Dillon, Joshua V and Lakshminarayanan, Balaji and Snoek, Jasper}, 
journal = {NeurIPS}, 
eprint = {1906.02530}, 
abstract = {{Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive \{\textbackslashem uncertainty\}. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ovadia-Can%20you%20trust%20your%20model's%20uncertainty-%20Evaluating%20predictive%20uncertainty%20under%20dataset%20shift-2019-NeurIPS.pdf}
}
@article{CryptoNASGhodsiNeurIPS2020, 
year = {2020}, 
title = {{CryptoNAS: Private Inference on a ReLU Budget}}, 
author = {Ghodsi, Zahra and Veldanda, Akshaj and Reagen, Brandon and Garg, Siddharth}, 
journal = {NeurIPS}, 
eprint = {2006.08733}, 
abstract = {{Machine learning as a service has given raise to privacy concerns surrounding clients' data and providers' models and has catalyzed research in private inference (PI): methods to process inferences without disclosing inputs. Recently, researchers have adapted cryptographic techniques to show PI is possible, however all solutions increase inference latency beyond practical limits. This paper makes the observation that existing models are ill-suited for PI and proposes a novel NAS method, named CryptoNAS, for finding and tailoring models to the needs of PI. The key insight is that in PI operator latency cost are non-linear operations (e.g., ReLU) dominate latency, while linear layers become effectively free. We develop the idea of a ReLU budget as a proxy for inference latency and use CryptoNAS to build models that maximize accuracy within a given budget. CryptoNAS improves accuracy by 3.4\% and latency by 2.4x over the state-of-the-art.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ghodsi-CryptoNAS-%20Private%20Inference%20on%20a%20ReLU%20Budget-2020-NeurIPS.pdf}
}
@article{CTNASChenCVPR2021, 
year = {2021}, 
title = {{Contrastive neural architecture search with neural architecture comparators}}, 
author = {Chen, Yaofo and Guo, Yong and Chen, Qi and Li, Minli and Zeng, Wei and Wang, Yaowei and Tan, Mingkui}, 
journal = {CVPR}, 
eprint = {2103.05471}, 
abstract = {{One of the key steps in Neural Architecture Search (NAS) is to estimate the performance of candidate architectures. Existing methods either directly use the validation performance or learn a predictor to estimate the performance. However, these methods can be either computationally expensive or very inaccurate, which may severely affect the search efficiency and performance. Moreover, as it is very difficult to annotate architectures with accurate performance on specific tasks, learning a promising performance predictor is often non-trivial due to the lack of labeled data. In this paper, we argue that it may not be necessary to estimate the absolute performance for NAS. On the contrary, we may need only to understand whether an architecture is better than a baseline one. However, how to exploit this comparison information as the reward and how to well use the limited labeled data remains two great challenges. In this paper, we propose a novel Contrastive Neural Architecture Search (CTNAS) method which performs architecture search by taking the comparison results between architectures as the reward. Specifically, we design and learn a Neural Architecture Comparator (NAC) to compute the probability of candidate architectures being better than a baseline one. Moreover, we present a baseline updating scheme to improve the baseline iteratively in a curriculum learning manner. More critically, we theoretically show that learning NAC is equivalent to optimizing the ranking over architectures. Extensive experiments in three search spaces demonstrate the superiority of our CTNAS over existing methods.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Contrastive%20neural%20architecture%20search%20with%20neural%20architecture%20comparators-2021-CVPR.pdf}
}
@article{SGNASHuangCVPR2021, 
year = {2021}, 
title = {{Searching by generating: Flexible and Efficient One-Shot NAS with Architecture Generator}}, 
author = {Huang, Sian-Yao and Chu, Wei-Ta}, 
journal = {CVPR}, 
eprint = {2103.07289}, 
abstract = {{In one-shot NAS, sub-networks need to be searched from the supernet to meet different hardware constraints. However, the search cost is high and \$N\$ times of searches are needed for \$N\$ different constraints. In this work, we propose a novel search strategy called architecture generator to search sub-networks by generating them, so that the search process can be much more efficient and flexible. With the trained architecture generator, given target hardware constraints as the input, \$N\$ good architectures can be generated for \$N\$ constraints by just one forward pass without re-searching and supernet retraining. Moreover, we propose a novel single-path supernet, called unified supernet, to further improve search efficiency and reduce GPU memory consumption of the architecture generator. With the architecture generator and the unified supernet, we propose a flexible and efficient one-shot NAS framework, called Searching by Generating NAS (SGNAS). With the pre-trained supernt, the search time of SGNAS for \$N\$ different hardware constraints is only 5 GPU hours, which is \$4N\$ times faster than previous SOTA single-path methods. After training from scratch, the top1-accuracy of SGNAS on ImageNet is 77.1\%, which is comparable with the SOTAs. The code is available at: https://github.com/eric8607242/SGNAS.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Huang-Searching%20by%20generating-%20Flexible%20and%20Efficient%20One-Shot%20NAS%20with%20Architecture%20Generator-2021-CVPR.pdf}
}
@article{AIFTZhouCVPR2017, 
year = {2017}, 
title = {{Fine-tuning convolutional neural networks for biomedical image analysis: Actively and Incrementally}}, 
author = {Zhou, Zongwei and Shin, Jae and Zhang, Lei and Gurudu, Suryakanth and Gotway, Michael and Liang, Jianming}, 
journal = {CVPR}, 
abstract = {{Intense interest in applying convolutional neural networks (CNNs) in biomedical image analysis is wide spread, but its success is impeded by the lack of large annotated datasets in biomedical imaging. Annotating biomedical images is not only tedious and time consuming, but also demanding of costly, specialty-oriented knowledge and skills, which are not easily accessible. To dramatically reduce annotation cost, this paper presents a novel method called AIFT (active, incremental fine-tuning) to naturally integrate active learning and transfer learning into a single framework. AIFT starts directly with a pre-trained CNN to seek “worthy” samples from the unannotated for annotation, and the (fine-tuned) CNN is further fine-tuned continuously by incorporating newly annotated samples in each iteration to enhance the CNN's performance incrementally. We have evaluated our method in three different biomedical imaging applications, demonstrating that the cost of annotation can be cut by at least half. This performance is attributed to the several advantages derived from the advanced active and incremental capability of our AIFT method.}}, 
pages = {4761--4772}, 
volume = {2017}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhou-Fine-tuning%20convolutional%20neural%20networks%20for%20biomedical%20image%20analysis-%20Actively%20and%20Incrementally-2017-CVPR.pdf}
}
@article{RepVGGDingCVPR2021, 
year = {2021}, 
title = {{RepVGG: Making VGG-style ConvNets Great Again}}, 
author = {Ding, Xiaohan and Zhang, Xiangyu and Ma, Ningning and Han, Jungong and Ding, Guiguang and Sun, Jian}, 
journal = {CVPR}, 
eprint = {2101.03697}, 
abstract = {{We present a simple but powerful architecture of convolutional neural network, which has a VGG-like inference-time body composed of nothing but a stack of 3x3 convolution and ReLU, while the training-time model has a multi-branch topology. Such decoupling of the training-time and inference-time architecture is realized by a structural re-parameterization technique so that the model is named RepVGG. On ImageNet, RepVGG reaches over 80\% top-1 accuracy, which is the first time for a plain model, to the best of our knowledge. On NVIDIA 1080Ti GPU, RepVGG models run 83\% faster than ResNet-50 or 101\% faster than ResNet-101 with higher accuracy and show favorable accuracy-speed trade-off compared to the state-of-the-art models like EfficientNet and RegNet. The code and trained models are available at https://github.com/megvii-model/RepVGG.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ding-RepVGG-%20Making%20VGG-style%20ConvNets%20Great%20Again-2021-CVPR.pdf}
}
@article{KDJiNeurIPS2020, 
year = {2020}, 
title = {{Knowledge distillation in wide neural networks: Risk Bound, Data Efficiency and Imperfect Teacher}}, 
author = {Ji, Guangda and Zhu, Zhanxing}, 
journal = {NeurIPS}, 
eprint = {2010.10090}, 
abstract = {{Knowledge distillation is a strategy of training a student network with guide of the soft output from a teacher network. It has been a successful method of model compression and knowledge transfer. However, currently knowledge distillation lacks a convincing theoretical understanding. On the other hand, recent finding on neural tangent kernel enables us to approximate a wide neural network with a linear model of the network's random features. In this paper, we theoretically analyze the knowledge distillation of a wide neural network. First we provide a transfer risk bound for the linearized model of the network. Then we propose a metric of the task's training difficulty, called data inefficiency. Based on this metric, we show that for a perfect teacher, a high ratio of teacher's soft labels can be beneficial. Finally, for the case of imperfect teacher, we find that hard labels can correct teacher's wrong prediction, which explains the practice of mixing hard and soft labels.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ji-Knowledge%20distillation%20in%20wide%20neural%20networks-%20Risk%20Bound,%20Data%20Efficiency%20and%20Imperfect%20Teacher-2020-NeurIPS.pdf}
}
@article{ISTA-NASYangNeurIPS2020, 
year = {2020}, 
title = {{ISTA-NAS: Efficient and Consistent Neural Architecture Search by Sparse Coding}}, 
author = {Yang, Yibo and Li, Hongyang and You, Shan and Wang, Fei and Qian, Chen and Lin, Zhouchen}, 
journal = {NeurIPS}, 
eprint = {2010.06176}, 
abstract = {{Neural architecture search (NAS) aims to produce the optimal sparse solution from a high-dimensional space spanned by all candidate connections. Current gradient-based NAS methods commonly ignore the constraint of sparsity in the search phase, but project the optimized solution onto a sparse one by post-processing. As a result, the dense super-net for search is inefficient to train and has a gap with the projected architecture for evaluation. In this paper, we formulate neural architecture search as a sparse coding problem. We perform the differentiable search on a compressed lower-dimensional space that has the same validation loss as the original sparse solution space, and recover an architecture by solving the sparse coding problem. The differentiable search and architecture recovery are optimized in an alternate manner. By doing so, our network for search at each update satisfies the sparsity constraint and is efficient to train. In order to also eliminate the depth and width gap between the network in search and the target-net in evaluation, we further propose a method to search and evaluate in one stage under the target-net settings. When training finishes, architecture variables are absorbed into network weights. Thus we get the searched architecture and optimized parameters in a single run. In experiments, our two-stage method on CIFAR-10 requires only 0.05 GPU-day for search. Our one-stage method produces state-of-the-art performances on both CIFAR-10 and ImageNet at the cost of only evaluation time.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-ISTA-NAS-%20Efficient%20and%20Consistent%20Neural%20Architecture%20Search%20by%20Sparse%20Coding-2020-NeurIPS.pdf}
}
@article{BOCompressionMaICCV2019, 
year = {2019}, 
title = {{A Bayesian optimization framework for neural network compression}}, 
author = {Ma, Xingchen and Triki, Amal Rannen and Berman, Maxim and Sagonas, Christos and Cali, Jacques and Blaschko, Matthew B.}, 
journal = {ICCV}, 
abstract = {{Neural network compression is an important step for deploying neural networks where speed is of high importance, or on devices with limited memory. It is necessary to tune compression parameters in order to achieve the desired trade-off between size and performance. This is often done by optimizing the loss on a validation set of data, which should be large enough to approximate the true risk and therefore yield sufficient generalization ability. However, using a full validation set can be computationally expensive. In this work, we develop a general Bayesian optimization framework for optimizing functions that are computed based on U-statistics. We propagate Gaussian uncertainties from the statistics through the Bayesian optimization framework yielding a method that gives a probabilistic approximation certificate of the result. We then apply this to parameter selection in neural network compression. Compression objectives that can be written as U-statistics are typically based on empirical risk and knowledge distillation for deep discriminative models. We demonstrate our method on VGG and ResNet models, and the resulting system can find optimal compression parameters for relatively high-dimensional parametrizations in a matter of minutes on a standard desktop machine, orders of magnitude faster than competing methods.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ma-A%20Bayesian%20optimization%20framework%20for%20neural%20network%20compression-2019-ICCV.pdf}
}
@article{PROMISE12LitjensMIA2014, 
year = {2014}, 
title = {{Evaluation of prostate segmentation algorithms for MRI: The PROMISE12 Challenge}}, 
author = {Litjens, Geert and Toth, Robert and Ven, Wendy van de and Hoeks, Caroline and Kerkstra, Sjoerd and Ginneken, Bram van and Vincent, Graham and Guillard, Gwenael and Birbeck, Neil and Zhang, Jindang and Strand, Robin and Malmberg, Filip and Ou, Yangming and Davatzikos, Christos and Kirschner, Matthias and Jung, Florian and Yuan, Jing and Qiu, Wu and Gao, Qinquan and Edwards, Philip “Eddie” and Maan, Bianca and Heijden, Ferdinand van der and Ghose, Soumya and Mitra, Jhimli and Dowling, Jason and Barratt, Dean and Huisman, Henkjan and Madabhushi, Anant}, 
journal = {MIA}, 
abstract = {{Prostate MRI image segmentation has been an area of intense research due to the increased use of MRI as a modality for the clinical workup of prostate cancer. Segmentation is useful for various tasks, e.g. to accurately localize prostate boundaries for radiotherapy or to initialize multi-modal registration algorithms. In the past, it has been difficult for research groups to evaluate prostate segmentation algorithms on multi-center, multi-vendor and multi-protocol data. Especially because we are dealing with MR images, image appearance, resolution and the presence of artifacts are affected by differences in scanners and/or protocols, which in turn can have a large influence on algorithm accuracy. The Prostate MR Image Segmentation (PROMISE12) challenge was setup to allow a fair and meaningful comparison of segmentation methods on the basis of performance and robustness. In this work we will discuss the initial results of the online PROMISE12 challenge, and the results obtained in the live challenge workshop hosted by the MICCAI2012 conference. In the challenge, 100 prostate MR cases from 4 different centers were included, with differences in scanner manufacturer, field strength and protocol. A total of 11 teams from academic research groups and industry participated. Algorithms showed a wide variety in methods and implementation, including active appearance models, atlas registration and level sets. Evaluation was performed using boundary and volume based metrics which were combined into a single score relating the metrics to human expert performance. The winners of the challenge where the algorithms by teams Imorphics and ScrAutoProstate, with scores of 85.72 and 84.29 overall. Both algorithms where significantly better than all other algorithms in the challenge (p<0.05) and had an efficient implementation with a run time of 8min and 3s per case respectively. Overall, active appearance model based approaches seemed to outperform other approaches like multi-atlas registration, both on accuracy and computation time. Although average algorithm performance was good to excellent and the Imorphics algorithm outperformed the second observer on average, we showed that algorithm combination might lead to further improvement, indicating that optimal performance for prostate segmentation is not yet obtained. All results are available online at http://promise12.grand-challenge.org/.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Litjens-Evaluation%20of%20prostate%20segmentation%20algorithms%20for%20MRI-%20The%20PROMISE12%20Challenge-2014-MIA.pdf}
}
@article{BiX-NASWangMICCAI2021, 
year = {2021}, 
title = {{BiX-NAS: Searching Efficient Bi-directional Architecture for Medical Image Segmentation}}, 
author = {Wang, Xinyi and Xiang, Tiange and Zhang, Chaoyi and Song, Yang and Liu, Dongnan and Huang, Heng and Cai, Weidong}, 
journal = {MICCAI}, 
eprint = {2106.14033}, 
abstract = {{The recurrent mechanism has recently been introduced into U-Net in various medical image segmentation tasks. Existing studies have focused on promoting network recursion via reusing building blocks. Although network parameters could be greatly saved, computational costs still increase inevitably in accordance with the pre-set iteration time. In this work, we study a multi-scale upgrade of a bi-directional skip connected network and then automatically discover an efficient architecture by a novel two-phase Neural Architecture Search (NAS) algorithm, namely BiX-NAS. Our proposed method reduces the network computational cost by sifting out ineffective multi-scale features at different levels and iterations. We evaluate BiX-NAS on two segmentation tasks using three different medical image datasets, and the experimental results show that our BiX-NAS searched architecture achieves the state-of-the-art performance with significantly lower computational cost.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-BiX-NAS-%20Searching%20Efficient%20Bi-directional%20Architecture%20for%20Medical%20Image%20Segmentation-2021-MICCAI.pdf}
}
@article{BiO-NetXiangMICCAI2020, 
year = {2020}, 
title = {{BiO-Net: Learning Recurrent Bi-directional Connections for Encoder-Decoder Architecture}}, 
author = {Xiang, Tiange and Zhang, Chaoyi and Liu, Dongnan and Song, Yang and Huang, Heng and Cai, Weidong}, 
journal = {MICCAI}, 
abstract = {{U-Net has become one of the state-of-the-art deep learning- based approaches for modern computer vision tasks such as semantic segmentation, super resolution, image denoising, and inpainting. Previous extensions of U-Net have focused mainly on the modification of its existing building blocks or the development of new functional modules for performance gains. As a result, these variants usually lead to an unneglectable increase in model complexity. To tackle this issue in such U-Net variants, in this paper, we present a novel Bi-directional O-shape network (BiO-Net) that reuses the building blocks in a recurrent manner without introducing any extra parameters. Our proposed bi-directional skip connections can be directly adopted into any encoder-decoder architecture to further enhance its capabilities in various task domains. We evaluated our method on various medical image analysis tasks and the results show that our BiO-Net significantly outperforms the vanilla U-Net as well as other state-of-the-art methods. Our code is available at https://github.com/tiangexiang/BiO-Net.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xiang-BiO-Net-%20Learning%20Recurrent%20Bi-directional%20Connections%20for%20Encoder-Decoder%20Architecture-2020-MICCAI.pdf}
}
@article{NEASChenCVPR2021, 
year = {2021}, 
title = {{One-shot neural ensemble architecture search by diversity-guided search space shrinking}}, 
author = {Chen, Minghao and Peng, Houwen and Fu, Jianlong and Ling, Haibin}, 
journal = {CVPR}, 
eprint = {2104.00597}, 
abstract = {{Despite remarkable progress achieved, most neural architecture search (NAS) methods focus on searching for one single accurate and robust architecture. To further build models with better generalization capability and performance, model ensemble is usually adopted and performs better than stand-alone models. Inspired by the merits of model ensemble, we propose to search for multiple diverse models simultaneously as an alternative way to find powerful models. Searching for ensembles is non-trivial and has two key challenges: enlarged search space and potentially more complexity for the searched model. In this paper, we propose a one-shot neural ensemble architecture search (NEAS) solution that addresses the two challenges. For the first challenge, we introduce a novel diversity-based metric to guide search space shrinking, considering both the potentiality and diversity of candidate operators. For the second challenge, we enable a new search dimension to learn layer sharing among different models for efficiency purposes. The experiments on ImageNet clearly demonstrate that our solution can improve the supernet's capacity of ranking ensemble architectures, and further lead to better search results. The discovered architectures achieve superior performance compared with state-of-the-arts such as MobileNetV3 and EfficientNet families under aligned settings. Moreover, we evaluate the generalization ability and robustness of our searched architecture on the COCO detection benchmark and achieve a 3.1\% improvement on AP compared with MobileNetV3. Codes and models are available at https://github.com/researchmm/NEAS.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-One-shot%20neural%20ensemble%20architecture%20search%20by%20diversity-guided%20search%20space%20shrinking-2021-CVPR.pdf}
}
@article{DiNTSHeCVPR2021, 
year = {2021}, 
title = {{DiNTS: Differentiable Neural Network Topology Search for 3D Medical Image Segmentation}}, 
author = {He, Yufan and Yang, Dong and Roth, Holger and Zhao, Can and Xu, Daguang}, 
journal = {CVPR}, 
eprint = {2103.15954}, 
abstract = {{Recently, neural architecture search (NAS) has been applied to automatically search high-performance networks for medical image segmentation. The NAS search space usually contains a network topology level (controlling connections among cells with different spatial scales) and a cell level (operations within each cell). Existing methods either require long searching time for large-scale 3D image datasets, or are limited to pre-defined topologies (such as U-shaped or single-path). In this work, we focus on three important aspects of NAS in 3D medical image segmentation: flexible multi-path network topology, high search efficiency, and budgeted GPU memory usage. A novel differentiable search framework is proposed to support fast gradient-based search within a highly flexible network topology search space. The discretization of the searched optimal continuous model in differentiable scheme may produce a sub-optimal final discrete model (discretization gap). Therefore, we propose a topology loss to alleviate this problem. In addition, the GPU memory usage for the searched 3D model is limited with budget constraints during search. Our Differentiable Network Topology Search scheme (DiNTS) is evaluated on the Medical Segmentation Decathlon (MSD) challenge, which contains ten challenging segmentation tasks. Our method achieves the state-of-the-art performance and the top ranking on the MSD challenge leaderboard.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/He-DiNTS-%20Differentiable%20Neural%20Network%20Topology%20Search%20for%203D%20Medical%20Image%20Segmentation-2021-CVPR.pdf}
}
@article{DSRNAHosseiniCVPR2021, 
year = {2021}, 
title = {{DSRNA: Differentiable Search of Robust Neural Architectures}}, 
author = {Hosseini, Ramtin and Yang, Xingyi and Xie, Pengtao}, 
journal = {CVPR}, 
eprint = {2012.06122}, 
abstract = {{In deep learning applications, the architectures of deep neural networks are crucial in achieving high accuracy. Many methods have been proposed to search for high-performance neural architectures automatically. However, these searched architectures are prone to adversarial attacks. A small perturbation of the input data can render the architecture to change prediction outcomes significantly. To address this problem, we propose methods to perform differentiable search of robust neural architectures. In our methods, two differentiable metrics are defined to measure architectures' robustness, based on certified lower bound and Jacobian norm bound. Then we search for robust architectures by maximizing the robustness metrics. Different from previous approaches which aim to improve architectures' robustness in an implicit way: performing adversarial training and injecting random noise, our methods explicitly and directly maximize robustness metrics to harvest robust architectures. On CIFAR-10, ImageNet, and MNIST, we perform game-based evaluation and verification-based evaluation on the robustness of our methods. The experimental results show that our methods 1) are more robust to various norm-bound attacks than several robust NAS baselines; 2) are more accurate than baselines when there are no attacks; 3) have significantly higher certified lower bounds than baselines.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hosseini-DSRNA-%20Differentiable%20Search%20of%20Robust%20Neural%20Architectures-2021-CVPR.pdf}
}
@article{BANFurlanelloICML2018, 
year = {2018}, 
title = {{Born-again neural networks}}, 
author = {Furlanello, Tommaso and Lipton, Zachary C and Tschannen, Michael and Itti, Laurent and Anandkumar, Anima}, 
journal = {ICML}, 
eprint = {1805.04770}, 
abstract = {{Knowledge distillation (KD) consists of transferring knowledge from one machine learning model (the teacher\}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness. \%we desire a compact model with performance close to the teacher's. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these \{Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5\%) and CIFAR-100 (15.5\%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between DenseNets and ResNets in either direction.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Furlanello-Born-again%20neural%20networks-2018-ICML.pdf}
}
@article{ACNetDingICCV2019, 
year = {2019}, 
title = {{ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks}}, 
author = {Ding, Xiaohan and Guo, Yuchen and Ding, Guiguang and Han, Jungong}, 
journal = {ICCV}, 
eprint = {1908.03930}, 
abstract = {{As designing appropriate Convolutional Neural Network (CNN) architecture in the context of a given application usually involves heavy human works or numerous GPU hours, the research community is soliciting the architecture-neutral CNN structures, which can be easily plugged into multiple mature architectures to improve the performance on our real-world applications. We propose Asymmetric Convolution Block (ACB), an architecture-neutral structure as a CNN building block, which uses 1D asymmetric convolutions to strengthen the square convolution kernels. For an off-the-shelf architecture, we replace the standard square-kernel convolutional layers with ACBs to construct an Asymmetric Convolutional Network (ACNet), which can be trained to reach a higher level of accuracy. After training, we equivalently convert the ACNet into the same original architecture, thus requiring no extra computations anymore. We have observed that ACNet can improve the performance of various models on CIFAR and ImageNet by a clear margin. Through further experiments, we attribute the effectiveness of ACB to its capability of enhancing the model's robustness to rotational distortions and strengthening the central skeleton parts of square convolution kernels.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ding-ACNet-%20Strengthening%20the%20Kernel%20Skeletons%20for%20Powerful%20CNN%20via%20Asymmetric%20Convolution%20Blocks-2019-ICCV.pdf}
}
@article{TransNAS-Bench-101DuanCVPR2021, 
year = {2021}, 
title = {{TransNAS-Bench-101: Improving Transferability and Generalizability of Cross-Task Neural Architecture Search}}, 
author = {Duan, Yawen and Chen, Xin and Xu, Hang and Chen, Zewei and Liang, Xiaodan and Zhang, Tong and Li, Zhenguo}, 
journal = {CVPR}, 
eprint = {2105.11871}, 
abstract = {{Recent breakthroughs of Neural Architecture Search (NAS) extend the field's research scope towards a broader range of vision tasks and more diversified search spaces. While existing NAS methods mostly design architectures on a single task, algorithms that look beyond single-task search are surging to pursue a more efficient and universal solution across various tasks. Many of them leverage transfer learning and seek to preserve, reuse, and refine network design knowledge to achieve higher efficiency in future tasks. However, the enormous computational cost and experiment complexity of cross-task NAS are imposing barriers for valuable research in this direction. Existing NAS benchmarks all focus on one type of vision task, i.e., classification. In this work, we propose TransNAS-Bench-101, a benchmark dataset containing network performance across seven tasks, covering classification, regression, pixel-level prediction, and self-supervised tasks. This diversity provides opportunities to transfer NAS methods among tasks and allows for more complex transfer schemes to evolve. We explore two fundamentally different types of search space: cell-level search space and macro-level search space. With 7,352 backbones evaluated on seven tasks, 51,464 trained models with detailed training information are provided. With TransNAS-Bench-101, we hope to encourage the advent of exceptional NAS algorithms that raise cross-task search efficiency and generalizability to the next level. Our dataset file will be available at Mindspore, VEGA.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Duan-TransNAS-Bench-101-%20Improving%20Transferability%20and%20Generalizability%20of%20Cross-Task%20Neural%20Architecture%20Search-2021-CVPR.pdf}
}
@article{DOTSGuCVPR2021, 
year = {2021}, 
title = {{DOTS: Decoupling Operation and Topology in Differentiable Architecture Search}}, 
author = {Gu, Yu-Chao and Wang, Li-Juan and Liu, Yun and Yang, Yi and Wu, Yu-Huan and Lu, Shao-Ping and Cheng, Ming-Ming}, 
journal = {CVPR}, 
eprint = {2010.00969}, 
abstract = {{Differentiable Architecture Search (DARTS) has attracted extensive attention due to its efficiency in searching for cell structures. DARTS mainly focuses on the operation search and derives the cell topology from the operation weights. However, the operation weights can not indicate the importance of cell topology and result in poor topology rating correctness. To tackle this, we propose to Decouple the Operation and Topology Search (DOTS), which decouples the topology representation from operation weights and makes an explicit topology search. DOTS is achieved by introducing a topology search space that contains combinations of candidate edges. The proposed search space directly reflects the search objective and can be easily extended to support a flexible number of edges in the searched cell. Existing gradient-based NAS methods can be incorporated into DOTS for further improvement by the topology search. Considering that some operations (e.g., Skip-Connection) can affect the topology, we propose a group operation search scheme to preserve topology-related operations for a better topology search. The experiments on CIFAR10/100 and ImageNet demonstrate that DOTS is an effective solution for differentiable NAS.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Gu-DOTS-%20Decoupling%20Operation%20and%20Topology%20in%20Differentiable%20Architecture%20Search-2021-CVPR.pdf}
}
@article{FBNetV3DaiCVPR2021, 
year = {2021}, 
title = {{FBNetV3: Joint Architecture-Recipe Search using Predictor Pretraining}}, 
author = {Dai, Xiaoliang and Wan, Alvin and Zhang, Peizhao and Wu, Bichen and He, Zijian and Wei, Zhen and Chen, Kan and Tian, Yuandong and Yu, Matthew and Vajda, Peter and Gonzalez, Joseph E}, 
journal = {CVPR}, 
eprint = {2006.02049}, 
abstract = {{Neural Architecture Search (NAS) yields state-of-the-art neural networks that outperform their best manually-designed counterparts. However, previous NAS methods search for architectures under one set of training hyper-parameters (i.e., a training recipe), overlooking superior architecture-recipe combinations. To address this, we present Neural Architecture-Recipe Search (NARS) to search both (a) architectures and (b) their corresponding training recipes, simultaneously. NARS utilizes an accuracy predictor that scores architecture and training recipes jointly, guiding both sample selection and ranking. Furthermore, to compensate for the enlarged search space, we leverage "free" architecture statistics (e.g., FLOP count) to pretrain the predictor, significantly improving its sample efficiency and prediction reliability. After training the predictor via constrained iterative optimization, we run fast evolutionary searches in just CPU minutes to generate architecture-recipe pairs for a variety of resource constraints, called FBNetV3. FBNetV3 makes up a family of state-of-the-art compact neural networks that outperform both automatically and manually-designed competitors. For example, FBNetV3 matches both EfficientNet and ResNeSt accuracy on ImageNet with up to 2.0x and 7.1x fewer FLOPs, respectively. Furthermore, FBNetV3 yields significant performance gains for downstream object detection tasks, improving mAP despite 18\% fewer FLOPs and 34\% fewer parameters than EfficientNet-based equivalents.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Dai-FBNetV3-%20Joint%20Architecture-Recipe%20Search%20using%20Predictor%20Pretraining-2021-CVPR.pdf}
}
@article{NetAdaptV2YangCVPR2021, 
year = {2021}, 
title = {{NetAdaptV2: Efficient Neural Architecture Search with Fast Super-Network Training and Architecture Optimization}}, 
author = {Yang, Tien-Ju and Liao, Yi-Lun and Sze, Vivienne}, 
journal = {CVPR}, 
eprint = {2104.00031}, 
abstract = {{Neural architecture search (NAS) typically consists of three main steps: training a super-network, training and evaluating sampled deep neural networks (DNNs), and training the discovered DNN. Most of the existing efforts speed up some steps at the cost of a significant slowdown of other steps or sacrificing the support of non-differentiable search metrics. The unbalanced reduction in the time spent per step limits the total search time reduction, and the inability to support non-differentiable search metrics limits the performance of discovered DNNs. In this paper, we present NetAdaptV2 with three innovations to better balance the time spent for each step while supporting non-differentiable search metrics. First, we propose channel-level bypass connections that merge network depth and layer width into a single search dimension to reduce the time for training and evaluating sampled DNNs. Second, ordered dropout is proposed to train multiple DNNs in a single forward-backward pass to decrease the time for training a super-network. Third, we propose the multi-layer coordinate descent optimizer that considers the interplay of multiple layers in each iteration of optimization to improve the performance of discovered DNNs while supporting non-differentiable search metrics. With these innovations, NetAdaptV2 reduces the total search time by up to \$5.8\textbackslashtimes\$ on ImageNet and \$2.4\textbackslashtimes\$ on NYU Depth V2, respectively, and discovers DNNs with better accuracy-latency/accuracy-MAC trade-offs than state-of-the-art NAS works. Moreover, the discovered DNN outperforms NAS-discovered MobileNetV3 by 1.8\% higher top-1 accuracy with the same latency. The project website is http://netadapt.mit.edu.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-NetAdaptV2-%20Efficient%20Neural%20Architecture%20Search%20with%20Fast%20Super-Network%20Training%20and%20Architecture%20Optimization-2021-CVPR.pdf}
}
@article{MCT-NASSuCVPR2021, 
year = {2021}, 
title = {{Prioritized architecture sampling with Monto-Carlo tree search}}, 
author = {Su, Xiu and Huang, Tao and Li, Yanxi and You, Shan and Wang, Fei and Qian, Chen and Zhang, Changshui and Xu, Chang}, 
journal = {CVPR}, 
eprint = {2103.11922}, 
abstract = {{One-shot neural architecture search (NAS) methods significantly reduce the search cost by considering the whole search space as one network, which only needs to be trained once. However, current methods select each operation independently without considering previous layers. Besides, the historical information obtained with huge computation cost is usually used only once and then discarded. In this paper, we introduce a sampling strategy based on Monte Carlo tree search (MCTS) with the search space modeled as a Monte Carlo tree (MCT), which captures the dependency among layers. Furthermore, intermediate results are stored in the MCT for the future decision and a better exploration-exploitation balance. Concretely, MCT is updated using the training loss as a reward to the architecture performance; for accurately evaluating the numerous nodes, we propose node communication and hierarchical node selection methods in the training and search stages, respectively, which make better uses of the operation rewards and hierarchical information. Moreover, for a fair comparison of different NAS methods, we construct an open-source NAS benchmark of a macro search space evaluated on CIFAR-10, namely NAS-Bench-Macro. Extensive experiments on NAS-Bench-Macro and ImageNet demonstrate that our method significantly improves search efficiency and performance. For example, by only searching \$20\$ architectures, our obtained architecture achieves \$78.0\textbackslash\%\$ top-1 accuracy with 442M FLOPs on ImageNet. Code (Benchmark) is available at: \textbackslashurl\{https://github.com/xiusu/NAS-Bench-Macro\}.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Su-Prioritized%20architecture%20sampling%20with%20Monto-Carlo%20tree%20search-2021-CVPR.pdf}
}
@article{LandmarkRegularizationYuCVPR2021, 
year = {2021}, 
title = {{Landmark regularization: Ranking Guided Super-Net Training in Neural Architecture Search}}, 
author = {Yu, Kaicheng and Ranftl, Rene and Salzmann, Mathieu}, 
journal = {CVPR}, 
eprint = {2104.05309}, 
abstract = {{Weight sharing has become a de facto standard in neural architecture search because it enables the search to be done on commodity hardware. However, recent works have empirically shown a ranking disorder between the performance of stand-alone architectures and that of the corresponding shared-weight networks. This violates the main assumption of weight-sharing NAS algorithms, thus limiting their effectiveness. We tackle this issue by proposing a regularization term that aims to maximize the correlation between the performance rankings of the shared-weight network and that of the standalone architectures using a small set of landmark architectures. We incorporate our regularization term into three different NAS algorithms and show that it consistently improves performance across algorithms, search-spaces, and tasks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yu-Landmark%20regularization-%20Ranking%20Guided%20Super-Net%20Training%20in%20Neural%20Architecture%20Search-2021-CVPR.pdf}
}
@article{MRNetJiCVPR2021, 
year = {2021}, 
title = {{Learning calibrated medical image segmentation via multi-rater agreement modeling}}, 
author = {Ji, Wei and Yu, Shuang and Wu, Junde and Ma, Kai and Bian, Cheng and Bi, Qi and Li, Jingjing and Liu, Hanruo and Cheng, Li and Zheng, Yefeng}, 
journal = {CVPR}, 
abstract = {{In medical image analysis, it is typical to collect multiple annotations, each from a different clinical expert or rater, in the expectation that possible diagnostic errors could be mitigated. Meanwhile, from the computer vision practitioner viewpoint, it has been a common practice to adopt the ground-truth labels obtained via either the majority- vote or simply one annotation from a preferred rater. This process, however, tends to overlook the rich information of agreement or disagreement ingrained in the raw multi- rater annotations. To address this issue, we propose to explicitly model the multi-rater (dis-)agreement, dubbed MR- Net, which has two main contributions. First, an expertise- aware inferring module or EIM is devised to embed the expertise level of individual raters as prior knowledge, to form high-level semantic features. Second, our approach is capable of reconstructing multi-rater gradings from coarse predictions, with the multi-rater (dis-)agreement cues being further exploited to improve the segmentation performance. To our knowledge, our work is the first in producing calibrated predictions under different expertise levels for med- ical image segmentation. Extensive empirical experiments are conducted across five medical segmentation tasks of diverse imaging modalities. In these experiments, superior performance of our MRNet is observed comparing to the state-of-the-arts, indicating the effectiveness and applicability of our MRNet toward a wide range of medical segmentation tasks. Source code is publicly available.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ji-Learning%20calibrated%20medical%20image%20segmentation%20via%20multi-rater%20agreement%20modeling-2021-CVPR.pdf}
}
@article{PracticalSnoekNeurIPS2012, 
year = {2012}, 
title = {{Practical Bayesian optimization of machine learning algorithms}}, 
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P}, 
journal = {NeurIPS}, 
eprint = {1206.2944}, 
abstract = {{Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters. Unfortunately, this tuning is often a "black art" that requires expert experience, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). The tractable posterior distribution induced by the GP leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next. Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization. We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms. We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Snoek-Practical%20Bayesian%20optimization%20of%20machine%20learning%20algorithms-2012-NeurIPS.pdf}
}
@article{PixIoUYuICML2021, 
year = {2021}, 
title = {{Learning generalized intersection over union for dense pixelwise prediction}}, 
author = {Yu, Jiaqian and Xu, Jingtao and Chen, Yiwei and Li, Weiming and Wang, QIang and Yoo, Byung In and Han, Jae-Joon}, 
journal = {ICML}, 
abstract = {{Intersection over union (IoU) score, also named Jaccard Index, is one of the most fundamental evaluation methods in machine learning. The original IoU computation cannot provide non-zero gradients and thus cannot be directly optimized by nowadays deep learning methods. Several recent works generalized IoU for bounding box regression, but they are not straightforward to adapt for pixelwise prediction. In particular, the original IoU fails to provide effective gradients for the non- overlapping and location-sensitive cases, which results in performance plateau. In this paper, we propose PixIoU, a generalized IoU for pixelwise prediction that is sensitive to the distance for non- overlapping cases and the locations in prediction. We provide proofs that PixIoU holds nice proper- ties as the original IoU. To optimize the PixIoU, we also propose a loss function that is proved to be submodular, hence we can apply the Lovasz functions, the efficient surrogates for submodular functions for learning this loss. Experimental results show consistent performance improvements by learning PixIoU over the original IoU for several different pixelwise prediction tasks on Pascal VOC, VOT-2020 and Cityscapes.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yu-Learning%20generalized%20intersection%20over%20union%20for%20dense%20pixelwise%20prediction-2021-ICML.pdf}
}
@article{DrQKostrikovICLR2021, 
year = {2021}, 
title = {{Image augmentation is all you need: Regularizing Deep Reinforcement Learning from Pixels}}, 
author = {Kostrikov, Ilya and Yarats, Denis and Fergus, Rob}, 
journal = {ICLR}, 
eprint = {2004.13649}, 
abstract = {{We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC), are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based (Dreamer, PlaNet, and SLAC) methods and recently proposed contrastive learning (CURL). Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at https://sites.google.com/view/data-regularized-q.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kostrikov-Image%20augmentation%20is%20all%20you%20need-%20Regularizing%20Deep%20Reinforcement%20Learning%20from%20Pixels-2021-ICLR.pdf}
}
@article{LearningNatarajanNeurIPS2013, 
year = {2013}, 
title = {{Learning with noisy labels}}, 
author = {Natarajan, Nagarajan and Dhillon, Inderjit S. and Ravikumar, Pradeep}, 
journal = {NeurIPS}, 
abstract = {{In this paper, we theoretically study the problem of binary classification in the presence of random classification noise — the learner, instead of seeing the true la- bels, sees labels that have independently been flipped with some small probability. Moreover, random label noise is class-conditional — the flip probability depends on the class. We provide two approaches to suitably modify any given surrogate loss function. First, we provide a simple unbiased estimator of any loss, and obtain performance bounds for empirical risk minimization in the presence of iid data with noisy labels. If the loss function satisfies a simple symmetry condition, we show that the method leads to an efficient algorithm for empirical minimization. Second, by leveraging a reduction of risk minimization under noisy labels to classification with weighted 0-1 loss, we suggest the use of a simple weighted surrogate loss, for which we are able to obtain strong empirical risk bounds. This approach has a very remarkable consequence — methods used in practice such as biased SVM and weighted logistic regression are provably noise-tolerant. On a synthetic non-separable dataset, our methods achieve over 88\% accuracy even when 40\% of the labels are corrupted, and are competitive with respect to recently proposed methods for dealing with label noise in several benchmark datasets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Natarajan-Learning%20with%20noisy%20labels-2013-NeurIPS.pdf}
}
@article{LearningLiICCV2017, 
year = {2017}, 
title = {{Learning from noisy labels with distillation}}, 
author = {Li, Yuncheng and Yang, Jianchao and Song, Yale and Cao, Liangliang and Luo, Jiebo and Li, Li-Jia}, 
journal = {ICCV}, 
abstract = {{The ability of learning from noisy labels is very useful in many visual recognition tasks, as a vast amount of data with noisy labels are relatively easy to obtain. Traditionally, label noise has been treated as statistical outliers, and techniques such as importance re-weighting and bootstrapping have been proposed to alleviate the problem. According to our observation, the real-world noisy labels exhibit multimode characteristics as the true labels, rather than behaving like independent random outliers. In this work, we propose a unified distillation framework to use “side” information, including a small clean dataset and label relations in knowledge graph, to “hedge the risk” of learning from noisy labels. Unlike the traditional approaches evaluated based on simulated label noises, we propose a suite of new benchmark datasets, in Sports, Species and Artifacts domains, to evaluate the task of learning from noisy labels in the practical setting. The empirical study demonstrates the effectiveness of our proposed method in all the domains.}}, 
pages = {1928--1936}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Learning%20from%20noisy%20labels%20with%20distillation-2017-ICCV.pdf}
}
@article{SMPHanICCV2019, 
year = {2019}, 
title = {{Deep self-learning from noisy labels}}, 
author = {Han, Jiangfan and Luo, Ping and Wang, Xiaogang}, 
journal = {ICCV}, 
abstract = {{ConvNets achieve good results when training from clean data, but learning from noisy labels significantly degrades performances and remains challenging. Unlike previous works constrained by many conditions, making them infeasible to real noisy cases, this work presents a novel deep self-learning framework to train a robust network on the real noisy datasets without extra supervision. The proposed approach has several appealing benefits. (1) Different from most existing work, it does not rely on any assumption on the distribution of the noisy labels, making it robust to real noises. (2) It does not need extra clean supervision or accessorial network to help training. (3) A self-learning framework is proposed to train the network in an iterative end-to-end manner, which is effective and efficient. Extensive experiments in challenging benchmarks such as Clothing1M and Food101-N show that our approach outperforms its counterparts in all empirical settings.}}, 
pages = {5137--5146}, 
volume = {00}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Han-Deep%20self-learning%20from%20noisy%20labels-2019-ICCV.pdf}
}
@article{Test-timeAyhanMIDL2018, 
year = {2018}, 
title = {{Test-time Data Augmentation for Estimation of Heteroscedastic Aleatoric Uncertainty in Deep Neural Networks}}, 
author = {Ayhan, Murat Seçkin and Berens, Philipp}, 
journal = {MIDL}, 
abstract = {{Deep neural networks (DNNs) have revolutionized medical image analysis and disease diagnosis. Despite their impressive increase in performance, it is difficult to generate well-calibrated probabilistic outputs for such networks such that state-of-the-art networks fail to provide reliable uncertainty estimates regarding their decisions. We propose a simple but effective method using traditional data augmentation methods such as geometric and color transformations at test time. This allows to examine how much the network output varies in the vicinity of examples in the input spaces. Despite its simplicity, our method yields useful estimates for the input-dependent predictive uncertainties of deep neural networks. We showcase the impact of our method via the well-known collection of fundus images obtained from a previous Kaggle competition.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ayhan-Test-time%20Data%20Augmentation%20for%20Estimation%20of%20Heteroscedastic%20Aleatoric%20Uncertainty%20in%20Deep%20Neural%20Networks-2018-MIDL.pdf}
}
@article{UnderstandingRajagopalMIDL2021, 
year = {2021}, 
title = {{Understanding and Visualizing Generalization in UNets}}, 
author = {Rajagopal, Abhejit and Madala, Vamshi C. and Hope, Thomas A. and Larson, Peder E. Z.}, 
journal = {MIDL}, 
abstract = {{Fully-convolutional neural networks, such as 2D and 3D UNets, are now pervasive in medical imaging for semantic segmentation, classification, image denoising, domain translation, and reconstruction. However, evaluation of UNet performance, as with most CNNs, has mostly been relegated to evaluation of a few performance metrics (e.g. accuracy, IoU, SSIM, etc.) using the network’s final predictions, which provides little insight into important issues such as generalization and dataset shift that can occur in clinical applications. In this paper, we propose techniques for understanding and visualizing the generalization performance of UNets in image classification and regression tasks, as well as metrics that are indicative of performance on unseen, unlabeled data.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Rajagopal-Understanding%20and%20Visualizing%20Generalization%20in%20UNets-2021-MIDL.pdf}
}
@article{MeasuringNixonCVPRWorkshop2019, 
year = {2019}, 
title = {{Measuring Calibration in Deep Learning}}, 
author = {Nixon, Jeremy and Dusenberry, Mike and Jerfel, Ghassen and Nguyen, Timothy and Liu, Jeremiah and Zhang, Linchuan and Tran, Dustin}, 
journal = {CVPR Workshop}, 
eprint = {1904.01685}, 
abstract = {{Overconfidence and underconfidence in machine learning classifiers is measured by calibration: the degree to which the probabilities predicted for each class match the accuracy of the classifier on that prediction. How one measures calibration remains a challenge: expected calibration error, the most popular metric, has numerous flaws which we outline, and there is no clear empirical understanding of how its choices affect conclusions in practice, and what recommendations there are to counteract its flaws. In this paper, we perform a comprehensive empirical study of choices in calibration measures including measuring all probabilities rather than just the maximum prediction, thresholding probability values, class conditionality, number of bins, bins that are adaptive to the datapoint density, and the norm used to compare accuracies to confidences. To analyze the sensitivity of calibration measures, we study the impact of optimizing directly for each variant with recalibration techniques. Across MNIST, Fashion MNIST, CIFAR-10/100, and ImageNet, we find that conclusions on the rank ordering of recalibration methods is drastically impacted by the choice of calibration measure. We find that conditioning on the class leads to more effective calibration evaluations, and that using the L2 norm rather than the L1 norm improves both optimization for calibration metrics and the rank correlation measuring metric consistency. Adaptive binning schemes lead to more stablity of metric rank ordering when the number of bins vary, and is also recommended. We open source a library for the use of our calibration measures.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Nixon-Measuring%20Calibration%20in%20Deep%20Learning-2019-CVPR%20Workshop.pdf}
}
@article{Self-DistillationMobahiNeurIPS2020, 
year = {2020}, 
title = {{Self-Distillation Amplifies Regularization in Hilbert Space}}, 
author = {Mobahi, Hossein and Farajtabar, Mehrdad and Bartlett, Peter L}, 
journal = {NeurIPS}, 
eprint = {2002.05715}, 
abstract = {{Knowledge distillation introduced in the deep learning context is a method to transfer knowledge from one architecture to another. In particular, when the architectures are identical, this is called self-distillation. The idea is to feed in predictions of the trained model as new target values for retraining (and iterate this loop possibly a few times). It has been empirically observed that the self-distilled model often achieves higher accuracy on held out data. Why this happens, however, has been a mystery: the self-distillation dynamics does not receive any new information about the task and solely evolves by looping over training. To the best of our knowledge, there is no rigorous understanding of this phenomenon. This work provides the first theoretical analysis of self-distillation. We focus on fitting a nonlinear function to training data, where the model space is Hilbert space and fitting is subject to \$\textbackslashell\_2\$ regularization in this function space. We show that self-distillation iterations modify regularization by progressively limiting the number of basis functions that can be used to represent the solution. This implies (as we also verify empirically) that while a few rounds of self-distillation may reduce over-fitting, further rounds may lead to under-fitting and thus worse performance.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Mobahi-Self-Distillation%20Amplifies%20Regularization%20in%20Hilbert%20Space-2020-NeurIPS.pdf}
}
@article{LearningLampertCVPR2009, 
year = {2009}, 
title = {{Learning to detect unseen object classes by between-class attribute transfer}}, 
author = {Lampert, Christoph H and Nickisch, Hannes and Harmeling, Stefan}, 
journal = {CVPR}, 
abstract = {{We study the problem of object classification when training and test classes are disjoint, i.e. no training examples of the target classes are available. This setup has hardly been studied in computer vision research, but it is the rule rather than the exception, because the world contains tens of thousands of different object classes and for only a very few of them image, collections have been formed and annotated with suitable class labels. In this paper, we tackle the problem by introducing attribute-based classification. It performs object detection based on a human-specified high-level description of the target objects instead of training images. The description consists of arbitrary semantic attributes, like shape, color or even geographic information. Because such properties transcend the specific learning task at hand, they can be pre-learned, e.g. from image datasets unrelated to the current task. Afterwards, new classes can be detected based on their attribute representation, without the need for a new training phase. In order to evaluate our method and to facilitate research in this area, we have assembled a new large-scale dataset, “Animals with Attributes”, of over 30,000 animal images that match the 50 classes in Osherson's classic table of how strongly humans associate 85 semantic attributes with animal classes. Our experiments show that by using an attribute layer it is indeed possible to build a learning object detection system that does not require any training images of the target classes.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lampert-Learning%20to%20detect%20unseen%20object%20classes%20by%20between-class%20attribute%20transfer-2009-CVPR.pdf}
}
@article{SoftTverskyLossSalehiMICCAIWorkshop2017, 
year = {2017}, 
title = {{Tversky loss function for image segmentation using 3D fully convolutional deep networks}}, 
author = {Salehi, Seyed Sadegh Mohseni and Erdogmus, Deniz and Gholipour, Ali}, 
journal = {MICCAI Workshop}, 
eprint = {1706.05721}, 
abstract = {{Fully convolutional deep neural networks carry out excellent potential for fast and accurate image segmentation. One of the main challenges in training these networks is data imbalance, which is particularly problematic in medical imaging applications such as lesion segmentation where the number of lesion voxels is often much lower than the number of non-lesion voxels. Training with unbalanced data can lead to predictions that are severely biased towards high precision but low recall (sensitivity), which is undesired especially in medical applications where false negatives are much less tolerable than false positives. Several methods have been proposed to deal with this problem including balanced sampling, two step training, sample re-weighting, and similarity loss functions. In this paper, we propose a generalized loss function based on the Tversky index to address the issue of data imbalance and achieve much better trade-off between precision and recall in training 3D fully convolutional deep neural networks. Experimental results in multiple sclerosis lesion segmentation on magnetic resonance images show improved F2 score, Dice coefficient, and the area under the precision-recall curve in test data. Based on these results we suggest Tversky loss function as a generalized framework to effectively train deep neural networks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Salehi-Tversky%20loss%20function%20for%20image%20segmentation%20using%203D%20fully%20convolutional%20deep%20networks-2017-MICCAI%20Workshop.pdf}
}
@article{FocalTverskyLossAbrahamISBI2019, 
year = {2019}, 
title = {{A Novel Focal Tversky Loss Function with Improved Attention U-Net for Lesion Segmentation}}, 
author = {Abraham, Nabila and Khan, Naimul Mefraz}, 
journal = {ISBI}, 
abstract = {{We propose a generalized focal loss function based on the Tversky index to address the issue of data imbalance in medical image segmentation. Compared to the commonly used Dice loss, our loss function achieves a better trade off between precision and recall when training on small structures such as lesions. To evaluate our loss function, we improve the attention U-Net model by incorporating an image pyramid to preserve contextual features. We experiment on the BUS 2017 dataset and ISIC 2018 dataset where lesions occupy 4.84\% and 21.4\% of the images area and improve segmentation accuracy when compared to the standard U-Net by 25.7\% and 3.6\%, respectively.}}, 
keywords = {}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Abraham-A%20Novel%20Focal%20Tversky%20Loss%20Function%20with%20Improved%20Attention%20U-Net%20for%20Lesion%20Segmentation-2019-ISBI.pdf}
}
@article{FocalLossLinTPAMI2018, 
year = {2018}, 
title = {{Focal Loss for Dense Object Detection}}, 
author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr}, 
journal = {TPAMI}, 
abstract = {{The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lin-Focal%20Loss%20for%20Dense%20Object%20Detection-2018-TPAMI.pdf}
}
@article{FocalCalibrationMukhotiNeurIPS2020, 
year = {2020}, 
title = {{Calibrating Deep Neural Networks Using Focal Loss}}, 
author = {Mukhoti, Jishnu and Kulharia, Viveka and Sanyal, Amartya and Golodetz, Stuart and Torr, Philip H S and Dokania, Puneet K}, 
journal = {NeurIPS}, 
eprint = {2002.09437}, 
abstract = {{Miscalibration - a mismatch between a model's confidence and its correctness - of Deep Neural Networks (DNNs) makes their predictions hard to rely on. Ideally, we want networks to be accurate, calibrated and confident. We show that, as opposed to the standard cross-entropy loss, focal loss [Lin et. al., 2017] allows us to learn models that are already very well calibrated. When combined with temperature scaling, whilst preserving accuracy, it yields state-of-the-art calibrated models. We provide a thorough analysis of the factors causing miscalibration, and use the insights we glean from this to justify the empirically excellent performance of focal loss. To facilitate the use of focal loss in practice, we also provide a principled approach to automatically select the hyperparameter involved in the loss function. We perform extensive experiments on a variety of computer vision and NLP datasets, and with a wide variety of network architectures, and show that our approach achieves state-of-the-art calibration without compromising on accuracy in almost all cases. Code is available at https://github.com/torrvision/focal\_calibration.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Mukhoti-Calibrating%20Deep%20Neural%20Networks%20Using%20Focal%20Loss-2020-NeurIPS.pdf}
}
@article{CS-KDYunCVPR2020, 
year = {2020}, 
title = {{Regularizing Class-wise Predictions via Self-Knowledge Distillation}}, 
author = {Yun, Sukmin and Park, Jongjin and Lee, Kimin and Shin, Jinwoo}, 
journal = {CVPR}, 
abstract = {{Deep neural networks with millions of parameters may suffer from poor generalization due to overfitting. To mitigate the issue, we propose a new regularization method that penalizes the predictive distribution between similar samples. In particular, we distill the predictive distribution between different samples of the same label during training. This results in regularizing the dark knowledge (i.e., the knowledge on wrong predictions) of a single network (i.e., a self-knowledge distillation) by forcing it to produce more meaningful and consistent predictions in a class-wise manner. Consequently, it mitigates overconfident predictions and reduces intra-class variations. Our experimental results on various image classification tasks demonstrate that the simple yet powerful method can significantly improve not only the generalization ability but also the calibration performance of modern convolutional neural networks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yun-Regularizing%20Class-wise%20Predictions%20via%20Self-Knowledge%20Distillation-2020-CVPR.pdf}
}
@article{OptimalNowozinCVPR2014, 
year = {2014}, 
title = {{Optimal Decisions from Probabilistic Models: the Intersection-over-Union Case}}, 
author = {Nowozin, Sebastian}, 
journal = {CVPR}, 
abstract = {{A probabilistic model allows us to reason about the world and make statistically optimal decisions using Bayesian decision theory. However, in practice the intractability of the decision problem forces us to adopt simplistic loss functions such as the 0/1 loss or Hamming loss and as result we make poor decisions through MAP estimates or through low-order marginal statistics. In this work we investigate optimal decision making for more realistic loss functions. Specifically we consider the popular intersection-over-union (IoU) score used in image segmentation benchmarks and show that it results in a hard combinatorial decision problem. To make this problem tractable we propose a statistical approximation to the objective function, as well as an approximate algorithm based on parametric linear programming. We apply the algorithm on three benchmark datasets and obtain improved intersection-over-union scores compared to maximum-posterior-marginal decisions. Our work points out the difficulties of using realistic loss functions with probabilistic computer vision models.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Nowozin-Optimal%20Decisions%20from%20Probabilistic%20Models-%20the%20Intersection-over-Union%20Case-2014-CVPR.pdf}
}
@article{VerifiedKumarNeurIPS2019, 
year = {2019}, 
title = {{Verified Uncertainty Calibration}}, 
author = {Kumar, Ananya and Liang, Percy and Ma, Tengyu}, 
journal = {NeurIPS}, 
eprint = {1909.10155}, 
abstract = {{Applications such as weather forecasting and personalized medicine demand models that output calibrated probability estimates---those representative of the true likelihood of a prediction. Most models are not calibrated out of the box but are recalibrated by post-processing model outputs. We find in this work that popular recalibration methods like Platt scaling and temperature scaling are (i) less calibrated than reported, and (ii) current techniques cannot estimate how miscalibrated they are. An alternative method, histogram binning, has measurable calibration error but is sample inefficient---it requires \$O(B/\textbackslashepsilon\textasciicircum2)\$ samples, compared to \$O(1/\textbackslashepsilon\textasciicircum2)\$ for scaling methods, where \$B\$ is the number of distinct probabilities the model can output. To get the best of both worlds, we introduce the scaling-binning calibrator, which first fits a parametric function to reduce variance and then bins the function values to actually ensure calibration. This requires only \$O(1/\textbackslashepsilon\textasciicircum2 + B)\$ samples. Next, we show that we can estimate a model's calibration error more accurately using an estimator from the meteorological community---or equivalently measure its calibration error with fewer samples (\$O(\textbackslashsqrt\{B\})\$ instead of \$O(B)\$). We validate our approach with multiclass calibration experiments on CIFAR-10 and ImageNet, where we obtain a 35\% lower calibration error than histogram binning and, unlike scaling methods, guarantees on true calibration. In these experiments, we also estimate the calibration error and ECE more accurately than the commonly used plugin estimators. We implement all these methods in a Python library: https://pypi.org/project/uncertainty-calibration}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kumar-Verified%20Uncertainty%20Calibration-2019-NeurIPS.pdf}
}
@article{TSDYangAAAI2019, 
year = {2019}, 
title = {{Training Deep Neural Networks in Generations: A More Tolerant Teacher Educates Better Students}}, 
author = {Yang, Chenglin and Xie, Lingxi and Qiao, Siyuan and Yuille, Alan L}, 
journal = {AAAI}, 
abstract = {{We focus on the problem of training a deep neural network in generations. The flowchart is that, in order to optimize the target network (student), another network (teacher) with the same architecture is first trained, and used to provide part of supervision signals in the next stage. While this strategy leads to a higher accuracy, many aspects (e.g., why teacher-student optimization helps) still need further explorations.This paper studies this problem from a perspective of controlling the strictness in training the teacher network. Existing approaches mostly used a hard distribution (e.g., one-hot vectors) in training, leading to a strict teacher which itself has a high accuracy, but we argue that the teacher needs to be more tolerant, although this often implies a lower accuracy. The implementation is very easy, with merely an extra loss term added to the teacher network, facilitating a few secondary classes to emerge and complement to the primary class. Consequently, the teacher provides a milder supervision signal (a less peaked distribution), and makes it possible for the student to learn from inter-class similarity and potentially lower the risk of over-fitting. Experiments are performed on standard image classification tasks (CIFAR100 and ILSVRC2012). Although the teacher network behaves less powerful, the students show a persistent ability growth and eventually achieve higher classification accuracies than other competitors. Model ensemble and transfer feature extraction also verify the effectiveness of our approach.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-Training%20Deep%20Neural%20Networks%20in%20Generations-%20A%20More%20Tolerant%20Teacher%20Educates%20Better%20Students-2019-AAAI.pdf}
}
@article{BetaSmoothingZhangNeurIPS2020, 
year = {2020}, 
title = {{Self-Distillation as Instance-Specific Label Smoothing}}, 
author = {Zhang, Zhilu and Sabuncu, Mert R}, 
journal = {NeurIPS}, 
eprint = {2006.05065}, 
abstract = {{It has been recently demonstrated that multi-generational self-distillation can improve generalization. Despite this intriguing observation, reasons for the enhancement remain poorly understood. In this paper, we first demonstrate experimentally that the improved performance of multi-generational self-distillation is in part associated with the increasing diversity in teacher predictions. With this in mind, we offer a new interpretation for teacher-student training as amortized MAP estimation, such that teacher predictions enable instance-specific regularization. Our framework allows us to theoretically relate self-distillation to label smoothing, a commonly used technique that regularizes predictive uncertainty, and suggests the importance of predictive diversity in addition to predictive uncertainty. We present experimental results using multiple datasets and neural network architectures that, overall, demonstrate the utility of predictive diversity. Finally, we propose a novel instance-specific label smoothing technique that promotes predictive diversity without the need for a separately trained teacher model. We provide an empirical evaluation of the proposed method, which, we find, often outperforms classical label smoothing.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Self-Distillation%20as%20Instance-Specific%20Label%20Smoothing-2020-NeurIPS.pdf}
}
@article{ImageNetDengCVPR2009, 
year = {2009}, 
title = {{ImageNet: A Large-Scale Hierarchical Image Database}}, 
author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li}, 
journal = {CVPR}, 
abstract = {{The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Deng-ImageNet-%20A%20Large-Scale%20Hierarchical%20Image%20Database-2009-CVPR.pdf}
}
@article{LqLossZhangNeurIPS2018, 
year = {2018}, 
title = {{Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels}}, 
author = {Zhang, Zhilu and Sabuncu, Mert R}, 
journal = {NeurIPS}, 
eprint = {1805.07836}, 
abstract = {{Deep neural networks (DNNs) have achieved tremendous success in a variety of applications across many disciplines. Yet, their superior performance comes with the expensive cost of requiring correctly annotated large-scale datasets. Moreover, due to DNNs' rich capacity, errors in training labels can hamper performance. To combat this problem, mean absolute error (MAE) has recently been proposed as a noise-robust alternative to the commonly-used categorical cross entropy (CCE) loss. However, as we show in this paper, MAE can perform poorly with DNNs and challenging datasets. Here, we present a theoretically grounded set of noise-robust loss functions that can be seen as a generalization of MAE and CCE. Proposed loss functions can be readily applied with any existing DNN architecture and algorithm, while yielding good performance in a wide range of noisy label scenarios. We report results from experiments conducted with CIFAR-10, CIFAR-100 and FASHION-MNIST datasets and synthetically generated noisy labels.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Generalized%20Cross%20Entropy%20Loss%20for%20Training%20Deep%20Neural%20Networks%20with%20Noisy%20Labels-2018-NeurIPS.pdf}
}
@article{AStatisticalMenonICML2021, 
year = {2021}, 
title = {{A Statistical Perspective on Distillation}}, 
author = {Menon, Aditya Krishna and Rawat, Ankit Singh and Reddi, Sashank J and Kim, Seungyeon and Kumar, Sanjiv}, 
journal = {ICML}, 
eprint = {2005.10419}, 
abstract = {{Knowledge distillation is a technique for improving the performance of a simple "student" model by replacing its one-hot training labels with a distribution over labels obtained from a complex "teacher" model. While this simple approach has proven widely effective, a basic question remains unresolved: why does distillation help? In this paper, we present a statistical perspective on distillation which addresses this question, and provides a novel connection to extreme multiclass retrieval techniques. Our core observation is that the teacher seeks to estimate the underlying (Bayes) class-probability function. Building on this, we establish a fundamental bias-variance tradeoff in the student's objective: this quantifies how approximate knowledge of these class-probabilities can significantly aid learning. Finally, we show how distillation complements existing negative mining techniques for extreme multiclass retrieval, and propose a unified objective which combines these ideas.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Menon-Why%20Distillation%20Helps-%20A%20Statistical%20Perspective-2021-ICML.pdf}
}
@article{WhenMullerNeurIPS2019, 
year = {2019}, 
title = {{When Does Label Smoothing Help?}}, 
author = {Müller, Rafael and Kornblith, Simon and Hinton, Geoffrey}, 
journal = {NeurIPS}, 
eprint = {1906.02629}, 
abstract = {{The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Müller-When%20Does%20Label%20Smoothing%20Help--2019-NeurIPS.pdf}
}
@article{SupermodularBermanarXiv2018, 
year = {2018}, 
title = {{Supermodular Locality Sensitive Hashes}}, 
author = {Berman, Maxim and Blaschko, Matthew B}, 
journal = {arXiv}, 
eprint = {1807.06686}, 
abstract = {{In this work, we show deep connections between Locality Sensitive Hashability and submodular analysis. We show that the LSHablility of the most commonly analyzed set similarities is in one-to-one correspondance with the supermodularity of these similarities when taken with respect to the symmetric difference of their arguments. We find that the supermodularity of equivalent LSHable similarities can be dependent on the set encoding. While monotonicity and supermodularity does not imply the metric condition necessary for supermodularity, this condition is guaranteed for the more restricted class of supermodular Hamming similarities that we introduce. We show moreover that LSH preserving transformations are also supermodular-preserving, yielding a way to generate families of similarities both LSHable and supermodular. Finally, we show that even the more restricted family of cardinality-based supermodular Hamming similarities presents promising aspects for the study of the link between LSHability and supermodularity. We hope that the several bridges that we introduce between LSHability and supermodularity paves the way to a better understanding both of supermodular analysis and LSHability, notably in the context of large-scale supermodular optimization.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Berman-Supermodular%20Locality%20Sensitive%20Hashes-2018-arXiv.pdf}
}
@article{SimSiamChenCVPR2021, 
year = {2021}, 
title = {{Exploring Simple Siamese Representation Learning}}, 
author = {Chen, Xinlei and He, Kaiming}, 
journal = {CVPR}, 
abstract = {{Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These models maximize the similarity between two augmentations of one image, subject to certain conditions for avoiding collapsing solutions. In this paper, we report surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Our experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing. We provide a hypothesis on the implication of stop-gradient, and further show proof-of-concept experiments verifying it. Our "SimSiam" method achieves competitive results on ImageNet and downstream tasks. We hope this simple baseline will motivate people to rethink the roles of Siamese architectures for unsupervised representation learning. Code will be made available.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Exploring%20Simple%20Siamese%20Representation%20Learning-2021-CVPR.pdf}
}
@article{COCOLinECCV2014, 
year = {2014}, 
title = {{Microsoft COCO: Common Objects in Context}}, 
author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C Lawrence and Dollár, Piotr}, 
journal = {ECCV}, 
eprint = {1405.0312}, 
abstract = {{We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lin-Microsoft%20COCO-%20Common%20Objects%20in%20Context-2014-ECCV.pdf}
}
@article{MoCoHeCVPR2020, 
year = {2020}, 
title = {{Momentum Contrast for Unsupervised Visual Representation Learning}}, 
author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross}, 
journal = {CVPR}, 
abstract = {{We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning [29] as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/He-Momentum%20Contrast%20for%20Unsupervised%20Visual%20Representation%20Learning-2020-CVPR.pdf}
}
@article{SimCLRChenICML2020, 
year = {2020}, 
title = {{A Simple Framework for Contrastive Learning of Visual Representations}}, 
author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey}, 
journal = {ICML}, 
eprint = {2002.05709}, 
abstract = {{This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-A%20Simple%20Framework%20for%20Contrastive%20Learning%20of%20Visual%20Representations-2020-ICML.pdf}
}
@article{BYOLGrillNeurIPS2020, 
year = {2020}, 
title = {{Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning}}, 
author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre H and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, Rémi and Valko, Michal}, 
journal = {NeurIPS}, 
eprint = {2006.07733}, 
abstract = {{We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches \$74.3\textbackslash\%\$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and \$79.6\textbackslash\%\$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Grill-Bootstrap%20Your%20Own%20Latent-%20A%20New%20Approach%20to%20Self-Supervised%20Learning-2020-NeurIPS.pdf}
}
@article{PointRendKirillovCVPR2020, 
year = {2020}, 
title = {{PointRend: Image Segmentation as Rendering}}, 
author = {Kirillov, Alexander and Wu, Yuxin and He, Kaiming and Girshick, Ross}, 
journal = {CVPR}, 
abstract = {{We present a new method for efficient high-quality image segmentation of objects and scenes. By analogizing classical computer graphics methods for efficient rendering with over- and undersampling challenges faced in pixel labeling tasks, we develop a unique perspective of image segmentation as a rendering problem. From this vantage, we present the PointRend (Point-based Rendering) neural network module: a module that performs point-based segmentation predictions at adaptively selected locations based on an iterative subdivision algorithm. PointRend can be flexibly applied to both instance and semantic segmentation tasks by building on top of existing state-of-the-art models. While many concrete implementations of the general idea are possible, we show that a simple design already achieves excellent results. Qualitatively, PointRend outputs crisp object boundaries in regions that are over-smoothed by previous methods. Quantitatively, PointRend yields significant gains on COCO and Cityscapes, for both instance and semantic segmentation. PointRend's efficiency enables output resolutions that are otherwise impractical in terms of memory or computation compared to existing approaches. Code has been made available at https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kirillov-PointRend-%20Image%20Segmentation%20as%20Rendering-2020-CVPR.pdf}
}
@article{MultigridWuCVPR2020, 
year = {2020}, 
title = {{A Multigrid Method for Efficiently Training Video Models}}, 
author = {Wu, Chao-Yuan and Girshick, Ross and He, Kaiming and Feichtenhofer, Christoph and Krähenbühl, Philipp}, 
journal = {CVPR}, 
abstract = {{Training competitive deep video models is an order of magnitude slower than training their counterpart image models. Slow training causes long research cycles, which hinders progress in video understanding research. Following standard practice for training image models, video model training has used a fixed mini-batch shape: a specific number of clips, frames, and spatial size. However, what is the optimal shape? High resolution models perform well, but train slowly. Low resolution models train faster, but are less accurate. Inspired by multigrid methods in numerical optimization, we propose to use variable mini-batch shapes with different spatial-temporal resolutions that are varied according to a schedule. The different shapes arise from resampling the training data on multiple sampling grids. Training is accelerated by scaling up the mini-batch size and learning rate when shrinking the other dimensions. We empirically demonstrate a general and robust grid schedule that yields a significant out-of-the-box training speedup without a loss in accuracy for different models (I3D, nonlocal, SlowFast), datasets (Kinetics, Something-Something, Charades), and training settings (with and without pretraining, 128 GPUs or 1 GPU). As an illustrative example, the proposed multigrid method trains a ResNet-50 SlowFast network 4.5× faster (wall-clock time, same hardware) while also improving accuracy (+0.8\% absolute) on Kinetics-400 compared to baseline training. Code is available online.11github.com/facebookresearch/SlowFast/blob/master/projects/multigrid github.com/facebookresearch/SlowFast/blob/master/projects/multigrid}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wu-A%20Multigrid%20Method%20for%20Efficiently%20Training%20Video%20Models-2020-CVPR.pdf}
}
@article{ConvexYuAISTATS2016, 
year = {2016}, 
title = {{A Convex Surrogate Operator for General Non-Modular Loss Functions}}, 
author = {Yu, Jiaqian and Blaschko, Matthew}, 
journal = {AISTATS}, 
eprint = {1604.03373}, 
abstract = {{Empirical risk minimization frequently employs convex surrogates to underlying discrete loss functions in order to achieve computational tractability during optimization. However, classical convex surrogates can only tightly bound modular loss functions, sub-modular functions or supermodular functions separately while maintaining polynomial time computation. In this work, a novel generic convex surrogate for general non-modular loss functions is introduced, which provides for the first time a tractable solution for loss functions that are neither super-modular nor submodular. This convex surro-gate is based on a submodular-supermodular decomposition for which the existence and uniqueness is proven in this paper. It takes the sum of two convex surrogates that separately bound the supermodular component and the submodular component using slack-rescaling and the Lov\{\textbackslash'a\}sz hinge, respectively. It is further proven that this surrogate is convex , piecewise linear, an extension of the loss function, and for which subgradient computation is polynomial time. Empirical results are reported on a non-submodular loss based on the S\{\{\textbackslasho\}\}rensen-Dice difference function, and a real-world face track dataset with tens of thousands of frames, demonstrating the improved performance, efficiency, and scalabil-ity of the novel convex surrogate.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yu-A%20Convex%20Surrogate%20Operator%20for%20General%20Non-Modular%20Loss%20Functions-2016-AISTATS.pdf}
}
@article{BayesianSegNetKendallBMVC2017, 
year = {2017}, 
title = {{Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding}}, 
author = {Kendall, Alex and Badrinarayanan, Vijay and Cipolla, Roberto}, 
journal = {BMVC}, 
eprint = {1511.02680}, 
abstract = {{We present a deep learning framework for probabilistic pixel-wise semantic segmentation, which we term Bayesian SegNet. Semantic segmentation is an important tool for visual scene understanding and a meaningful measure of uncertainty is essential for decision making. Our contribution is a practical system which is able to predict pixel-wise class labels with a measure of model uncertainty. We achieve this by Monte Carlo sampling with dropout at test time to generate a posterior distribution of pixel class labels. In addition, we show that modelling uncertainty improves segmentation performance by 2-3\% across a number of state of the art architectures such as SegNet, FCN and Dilation Network, with no additional parametrisation. We also observe a significant improvement in performance for smaller datasets where modelling uncertainty is more effective. We benchmark Bayesian SegNet on the indoor SUN Scene Understanding and outdoor CamVid driving scenes datasets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kendall-Bayesian%20SegNet-%20Model%20Uncertainty%20in%20Deep%20Convolutional%20Encoder-Decoder%20Architectures%20for%20Scene%20Understanding-2017-BMVC.pdf}
}
@article{LTSDingICCV2021, 
year = {2021}, 
title = {{Local Temperature Scaling for Probability Calibration}}, 
author = {Ding, Zhipeng and Han, Xu and Liu, Peirong and Niethammer, Marc}, 
journal = {ICCV}, 
eprint = {2008.05105}, 
abstract = {{For semantic segmentation, label probabilities are often uncalibrated as they are typically only the by-product of a segmentation task. Intersection over Union (IoU) and Dice score are often used as criteria for segmentation success, while metrics related to label probabilities are not often explored. However, probability calibration approaches have been studied, which match probability outputs with experimentally observed errors. These approaches mainly focus on classification tasks, but not on semantic segmentation. Thus, we propose a learning-based calibration method that focuses on multi-label semantic segmentation. Specifically, we adopt a convolutional neural network to predict local temperature values for probability calibration. One advantage of our approach is that it does not change prediction accuracy, hence allowing for calibration as a post-processing step. Experiments on the COCO, CamVid, and LPBA40 datasets demonstrate improved calibration performance for a range of different metrics. We also demonstrate the good performance of our method for multi-atlas brain segmentation from magnetic resonance images.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ding-Local%20Temperature%20Scaling%20for%20Probability%20Calibration-2021-ICCV.pdf}
}
@article{Mix-n-MatchZhangICML2020, 
year = {2020}, 
title = {{Mix-n-Match: Ensemble and Compositional Methods for Uncertainty Calibration in Deep Learning}}, 
author = {Zhang, Jize and Kailkhura, Bhavya and Han, T Yong-Jin}, 
journal = {ICML}, 
eprint = {2003.07329}, 
abstract = {{This paper studies the problem of post-hoc calibration of machine learning classifiers. We introduce the following desiderata for uncertainty calibration: (a) accuracy-preserving, (b) data-efficient, and (c) high expressive power. We show that none of the existing methods satisfy all three requirements, and demonstrate how Mix-n-Match calibration strategies (i.e., ensemble and composition) can help achieve remarkably better data-efficiency and expressive power while provably maintaining the classification accuracy of the original classifier. Mix-n-Match strategies are generic in the sense that they can be used to improve the performance of any off-the-shelf calibrator. We also reveal potential issues in standard evaluation practices. Popular approaches (e.g., histogram-based expected calibration error (ECE)) may provide misleading results especially in small-data regime. Therefore, we propose an alternative data-efficient kernel density-based estimator for a reliable evaluation of the calibration performance and prove its asymptotically unbiasedness and consistency. Our approaches outperform state-of-the-art solutions on both the calibration as well as the evaluation tasks in most of the experimental settings. Our codes are available at https://github.com/zhang64-llnl/Mix-n-Match-Calibration.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Mix-n-Match-%20Ensemble%20and%20Compositional%20Methods%20for%20Uncertainty%20Calibration%20in%20Deep%20Learning-2020-ICML.pdf}
}
@article{KD-MRIMurugesanMIDL2020, 
year = {2020}, 
title = {{KD-MRI: A knowledge distillation framework for image reconstruction and image restoration in MRI workflow}}, 
author = {Murugesan, Balamurali and Vijayarangan, Sricharan and Sarveswaran, Kaushik and Ram, Keerthi and Sivaprakasam, Mohanasankar}, 
journal = {MIDL}, 
eprint = {2004.05319}, 
abstract = {{Deep learning networks are being developed in every stage of the MRI workflow and have provided state-of-the-art results. However, this has come at the cost of increased computation requirement and storage. Hence, replacing the networks with compact models at various stages in the MRI workflow can significantly reduce the required storage space and provide considerable speedup. In computer vision, knowledge distillation is a commonly used method for model compression. In our work, we propose a knowledge distillation (KD) framework for the image to image problems in the MRI workflow in order to develop compact, low-parameter models without a significant drop in performance. We propose a combination of the attention-based feature distillation method and imitation loss and demonstrate its effectiveness on the popular MRI reconstruction architecture, DC-CNN. We conduct extensive experiments using Cardiac, Brain, and Knee MRI datasets for 4x, 5x and 8x accelerations. We observed that the student network trained with the assistance of the teacher using our proposed KD framework provided significant improvement over the student network trained without assistance across all the datasets and acceleration factors. Specifically, for the Knee dataset, the student network achieves \$65\textbackslash\%\$ parameter reduction, 2x faster CPU running time, and 1.5x faster GPU running time compared to the teacher. Furthermore, we compare our attention-based feature distillation method with other feature distillation methods. We also conduct an ablative study to understand the significance of attention-based distillation and imitation loss. We also extend our KD framework for MRI super-resolution and show encouraging results.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Murugesan-KD-MRI-%20A%20knowledge%20distillation%20framework%20for%20image%20reconstruction%20and%20image%20restoration%20in%20MRI%20workflow-2020-MIDL.pdf}
}
@article{EMKDQinTMI2021, 
year = {2021}, 
title = {{Efficient Medical Image Segmentation Based on Knowledge Distillation}}, 
author = {Qin, Dian and Bu, Jia-Jun and Liu, Zhe and Shen, Xin and Zhou, Sheng and Gu, Jing-Jun and Wang, Zhi-Hua and Wu, Lei and Dai, Hui-Fen}, 
journal = {TMI}, 
eprint = {2108.09987}, 
abstract = {{Recent advances have been made in applying convolutional neural networks to achieve more precise prediction results for medical image segmentation problems. However, the success of existing methods has highly relied on huge computational complexity and massive storage, which is impractical in the real-world scenario. To deal with this problem, we propose an efficient architecture by distilling knowledge from well-trained medical image segmentation networks to train another lightweight network. This architecture empowers the lightweight network to get a significant improvement on segmentation capability while retaining its runtime efficiency. We further devise a novel distillation module tailored for medical image segmentation to transfer semantic region information from teacher to student network. It forces the student network to mimic the extent of difference of representations calculated from different tissue regions. This module avoids the ambiguous boundary problem encountered when dealing with medical imaging but instead encodes the internal information of each semantic region for transferring. Benefited from our module, the lightweight network could receive an improvement of up to 32.6\% in our experiment while maintaining its portability in the inference phase. The entire structure has been verified on two widely accepted public CT datasets LiTS17 and KiTS19. We demonstrate that a lightweight network distilled by our method has non-negligible value in the scenario which requires relatively high operating speed and low storage usage.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Qin-Efficient%20Medical%20Image%20Segmentation%20Based%20on%20Knowledge%20Distillation-2021-TMI.pdf}
}
@article{VIDAhnCVPR2019, 
year = {2019}, 
title = {{Variational Information Distillation for Knowledge Transfer}}, 
author = {Ahn, Sungsoo and Hu, Shell Xu and Damianou, Andreas and Lawrence, Neil D. and Dai, Zhenwen}, 
journal = {CVPR}, 
abstract = {{Transferring knowledge from a teacher neural network pretrained on the same or a similar task to a student neural network can significantly improve the performance of the student neural network. Existing knowledge transfer approaches match the activations or the corresponding handcrafted features of the teacher and the student networks. We propose an information-theoretic framework for knowledge transfer which formulates knowledge transfer as maximizing the mutual information between the teacher and the student networks. We compare our method with existing knowledge transfer methods on both knowledge distillation and transfer learning tasks and show that our method consistently outperforms existing methods. We further demonstrate the strength of our method on knowledge transfer across heterogeneous network architectures by transferring knowledge from a convolutional neural network (CNN) to a multi-layer perceptron (MLP) on CIFAR-10. The resulting MLP significantly outperforms the-state-of-the-art methods and it achieves similar performance to the CNN with a single convolutional layer.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ahn-Variational%20Information%20Distillation%20for%20Knowledge%20Transfer-2019-CVPR.pdf}
}
@article{ConfidenceMehrtashTMI2020, 
year = {2020}, 
title = {{Confidence Calibration and Predictive Uncertainty Estimation for Deep Medical Image Segmentation}}, 
author = {Mehrtash, Alireza and Wells, William M. and Tempany, Clare M. and Abolmaesumi, Purang and Kapur, Tina}, 
journal = {TMI}, 
abstract = {{Fully convolutional neural networks (FCNs), and in particular U-Nets, have achieved state-of-the-art results in semantic segmentation for numerous medical imaging applications. Moreover, batch normalization and Dice loss have been used successfully to stabilize and accelerate training. However, these networks are poorly calibrated i.e. they tend to produce overconfident predictions for both correct and erroneous classifications, making them unreliable and hard to interpret. In this paper, we study predictive uncertainty estimation in FCNs for medical image segmentation. We make the following contributions: 1) We systematically compare cross-entropy loss with Dice loss in terms of segmentation quality and uncertainty estimation of FCNs; 2) We propose model ensembling for confidence calibration of the FCNs trained with batch normalization and Dice loss; 3) We assess the ability of calibrated FCNs to predict segmentation quality of structures and detect out-of-distribution test examples. We conduct extensive experiments across three medical image segmentation applications of the brain, the heart, and the prostate to evaluate our contributions. The results of this study offer considerable insight into the predictive uncertainty estimation and out-of-distribution detection in medical image segmentation and provide practical recipes for confidence calibration. Moreover, we consistently demonstrate that model ensembling improves confidence calibration.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Mehrtash-Confidence%20Calibration%20and%20Predictive%20Uncertainty%20Estimation%20for%20Deep%20Medical%20Image%20Segmentation-2020-TMI.pdf}
}
@article{MoCoV2ChenarXiv2020, 
year = {2020}, 
title = {{Improved Baselines with Momentum Contrastive Learning}}, 
author = {Chen, Xinlei and Fan, Haoqi and Girshick, Ross and He, Kaiming}, 
journal = {arXiv}, 
eprint = {2003.04297}, 
abstract = {{Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Improved%20Baselines%20with%20Momentum%20Contrastive%20Learning-2020-arXiv.pdf}
}
@article{MMCEKumarICML2018, 
year = {2018}, 
title = {{Trainable Calibration Measures For Neural Networks From Kernel Mean Embeddings}}, 
author = {Kumar, Aviral and Sarawagi, Sunita and Jain, Ujjwal}, 
journal = {ICML}, 
abstract = {{Modern neural networks have recently been found to be poorly calibrated, primarily in the direction of over-confidence. Methods like entropy penalty and temperature smoothing improve calibration by clamping confidence, but in doing so compromise the many legitimately confident predictions. We propose a more principled fix that minimizes an explicit calibration error during training. We present MMCE, a RKHS kernel based measure of calibration that is efficiently trainable alongside the negative likelihood loss without careful hyper-parameter tuning. Theoretically too, MMCE is a sound measure of calibration that is minimized at perfect calibration, and whose finite sample estimates are consistent and enjoy fast convergence rates. Extensive experiments on several network architectures demonstrate that MMCE is a fast, stable, and accurate method to minimize calibration error metrics while maximally preserving the number of high confidence predictions.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kumar-Trainable%20Calibration%20Measures%20For%20Neural%20Networks%20From%20Kernel%20Mean%20Embeddings-2018-ICML.pdf}
}
@article{DLGZhuNeurIPS2019, 
year = {2019}, 
title = {{Deep Leakage from Gradients}}, 
author = {Zhu, Ligeng and Liu, Zhijian and Han, Song}, 
journal = {NeurIPS}, 
eprint = {1906.08935}, 
abstract = {{Exchanging gradients is a widely used method in modern multi-node machine learning system (e.g., distributed training, collaborative learning). For a long time, people believed that gradients are safe to share: i.e., the training data will not be leaked by gradient exchange. However, we show that it is possible to obtain the private training data from the publicly shared gradients. We name this leakage as Deep Leakage from Gradient and empirically validate the effectiveness on both computer vision and natural language processing tasks. Experimental results show that our attack is much stronger than previous approaches: the recovery is pixel-wise accurate for images and token-wise matching for texts. We want to raise people's awareness to rethink the gradient's safety. Finally, we discuss several possible strategies to prevent such deep leakage. The most effective defense method is gradient pruning.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhu-Deep%20Leakage%20from%20Gradients-2019-NeurIPS.pdf}
}
@article{ICKDLiuICCV2021, 
year = {2021}, 
title = {{Exploring Inter-Channel Correlation for Diversity-preserved Knowledge Distillation}}, 
author = {Liu, Li and Huang, Qingle and Lin, Sihao and Xie, Hongwei and Wang, Bing and Chang, Xiaojun and Liang, Xiaodan}, 
journal = {ICCV}, 
abstract = {{Knowledge Distillation has shown very promising ability in transferring learned representation from the larger model (teacher) to the smaller one (student). Despite many efforts, prior methods ignore the important role of retaining inter-channel correlation of features, leading to the lack of capturing intrinsic distribution of the feature space and sufficient diversity properties of features in the teacher network. To solve the issue, we propose the novel Inter-Channel Correlation for Knowledge Distillation (ICKD), with which the diversity and homology of the feature space of the student network can align with that of the teacher network. The correlation between these two channels is interpreted as diversity if they are irrelevant to each other, otherwise homology. Then the student is required to mimic the correlation within its own embedding space. In addition, we introduce the grid-level inter-channel correlation, making it capable of dense prediction tasks. Extensive experiments on two vision tasks, including ImageNet classification and Pascal VOC segmentation, demonstrate the superiority of our ICKD, which consistently outperforms many existing methods, advancing the state-of-the-art in the fields of Knowledge Distillation. To our knowledge, we are the first method based on knowledge distillation boosts ResNet18 beyond 72\% Top-1 ac- curacy on ImageNet classification. Code is available at: https://github.com/ADLab-AutoDrive/ICKD.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-Exploring%20Inter-Channel%20Correlation%20for%20Diversity-preserved%20Knowledge%20Distillation-2021-ICCV.pdf}
}
@article{VGGSimonyanICLR2015, 
year = {2015}, 
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}}, 
author = {Simonyan, Karen and Zisserman, Andrew}, 
journal = {ICLR}, 
eprint = {1409.1556}, 
abstract = {{In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Simonyan-Very%20Deep%20Convolutional%20Networks%20for%20Large-Scale%20Image%20Recognition-2015-ICLR.pdf}
}
@article{DoKornblithCVPR2019, 
year = {2019}, 
title = {{Do Better ImageNet Models Transfer Better?}}, 
author = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.}, 
journal = {CVPR}, 
abstract = {{Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classification networks on 12 image classification datasets. We find that, when networks are used as fixed feature extractors or fine-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy (r = 0.99 and 0. 96, respectively). In the former setting, we find that this relationship is very sensitive to the way in which networks are trained on ImageNet; many common forms of regularization slightly improve ImageNet accuracy but yield penultimate layer features that are much worse for transfer learning. Additionally, we find that, on two small fine-grained image classification datasets, pretraining on ImageNet provides minimal benefits, indicating the learned features from ImageNet do not transfer well to fine-grained tasks. Together, our results show that ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kornblith-Do%20Better%20ImageNet%20Models%20Transfer%20Better--2019-CVPR.pdf}
}
@article{SKDLiuCVPR2019, 
year = {2019}, 
title = {{Structured Knowledge Distillation for Semantic Segmentation}}, 
author = {Liu, Yifan and Chen, Ke and Liu, Chris and Qin, Zengchang and Luo, Zhenbo and Wang, Jingdong}, 
journal = {CVPR}, 
abstract = {{In this paper, we investigate the knowledge distillation strategy for training small semantic segmentation networks by making use of large networks. We start from the straightforward scheme, pixel-wise distillation, which applies the distillation scheme adopted for image classification and performs knowledge distillation for each pixel separately. We further propose to distill the structured knowledge from large networks to small networks, which is motivated by that semantic segmentation is a structured prediction problem. We study two structured distillation schemes: (i) pair-wise distillation that distills the pairwise similarities, and (ii) holistic distillation that uses GAN to distill holistic knowledge. The effectiveness of our knowledge distillation approaches is demonstrated by extensive experiments on three scene parsing datasets: Cityscapes, Camvid and ADE20K.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-Structured%20Knowledge%20Distillation%20for%20Semantic%20Segmentation-2019-CVPR.pdf}
}
@article{Seq2SeqSutskeverNeurIPS2014, 
year = {2014}, 
title = {{Sequence to Sequence Learning with Neural Networks}}, 
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V}, 
journal = {NeurIPS}, 
eprint = {1409.3215}, 
abstract = {{Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Sutskever-Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks-2014-NeurIPS.pdf}
}
@article{AttentionBahdanauICLR2014, 
year = {2014}, 
title = {{Neural Machine Translation by Jointly Learning to Align and Translate}}, 
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua}, 
journal = {ICLR}, 
eprint = {1409.0473}, 
abstract = {{Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bahdanau-Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate-2014-ICLR.pdf}
}
@article{TransformerVaswaniNeurIPS2017, 
year = {2017}, 
title = {{Attention Is All You Need}}, 
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia}, 
journal = {NeurIPS}, 
eprint = {1706.03762}, 
abstract = {{The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Vaswani-Attention%20Is%20All%20You%20Need-2017-NeurIPS.pdf}
}
@article{ViTDosovitskiyICLR2021, 
year = {2021}, 
title = {{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}}, 
author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil}, 
journal = {ICLR}, 
eprint = {2010.11929}, 
abstract = {{While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Dosovitskiy-An%20Image%20is%20Worth%2016x16%20Words-%20Transformers%20for%20Image%20Recognition%20at%20Scale-2021-ICLR.pdf}
}
@article{CRDTianICLR2020, 
year = {2020}, 
title = {{Contrastive Representation Distillation}}, 
author = {Tian, Yonglong and Krishnan, Dilip and Isola, Phillip}, 
journal = {ICLR}, 
eprint = {1910.10699}, 
abstract = {{Often we wish to transfer representational knowledge from one neural network to another. Examples include distilling a large network into a smaller one, transferring knowledge from one sensory modality to a second, or ensembling a collection of models into a single estimator. Knowledge distillation, the standard approach to these problems, minimizes the KL divergence between the probabilistic outputs of a teacher and student network. We demonstrate that this objective ignores important structural knowledge of the teacher network. This motivates an alternative objective by which we train a student to capture significantly more information in the teacher's representation of the data. We formulate this objective as contrastive learning. Experiments demonstrate that our resulting new objective outperforms knowledge distillation and other cutting-edge distillers on a variety of knowledge transfer tasks, including single model compression, ensemble distillation, and cross-modal transfer. Our method sets a new state-of-the-art in many transfer tasks, and sometimes even outperforms the teacher network when combined with knowledge distillation. Code: http://github.com/HobbitLong/RepDistiller.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tian-Contrastive%20Representation%20Distillation-2020-ICLR.pdf}
}
@article{TAMirzadehAAAI2020, 
year = {2020}, 
title = {{Improved Knowledge Distillation via Teacher Assistant}}, 
author = {Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Li, Ang and Levine, Nir and Matsukawa, Akihiro and Ghasemzadeh, Hassan}, 
journal = {AAAI}, 
abstract = {{Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Mirzadeh-Improved%20Knowledge%20Distillation%20via%20Teacher%20Assistant-2020-AAAI.pdf}
}
@article{BN-NASChenICCV2021, 
year = {2021}, 
title = {{BN-NAS: Neural Architecture Search with Batch Normalization}}, 
author = {Chen, Boyu and Li, Peixia and Li, Baopu and Lin, Chen and Li, Chuming and Sun, Ming and Yan, Junjie and Ouyang, Wanli}, 
journal = {ICCV}, 
eprint = {2108.07375}, 
abstract = {{We present BN-NAS, neural architecture search with Batch Normalization (BN-NAS), to accelerate neural architecture search (NAS). BN-NAS can significantly reduce the time required by model training and evaluation in NAS. Specifically, for fast evaluation, we propose a BN-based indicator for predicting subnet performance at a very early training stage. The BN-based indicator further facilitates us to improve the training efficiency by only training the BN parameters during the supernet training. This is based on our observation that training the whole supernet is not necessary while training only BN parameters accelerates network convergence for network architecture search. Extensive experiments show that our method can significantly shorten the time of training supernet by more than 10 times and shorten the time of evaluating subnets by more than 600,000 times without losing accuracy.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-BN-NAS-%20Neural%20Architecture%20Search%20with%20Batch%20Normalization-2021-ICCV.pdf}
}
@article{DCALiangBMVC2020, 
year = {2020}, 
title = {{Improved Trainable Calibration Method for Neural Networks on Medical Imaging Classification}}, 
author = {Liang, Gongbo and Zhang, Yu and Wang, Xiaoqin and Jacobs, Nathan}, 
journal = {BMVC}, 
eprint = {2009.04057}, 
abstract = {{Recent works have shown that deep neural networks can achieve super-human performance in a wide range of image classification tasks in the medical imaging domain. However, these works have primarily focused on classification accuracy, ignoring the important role of uncertainty quantification. Empirically, neural networks are often miscalibrated and overconfident in their predictions. This miscalibration could be problematic in any automatic decision-making system, but we focus on the medical field in which neural network miscalibration has the potential to lead to significant treatment errors. We propose a novel calibration approach that maintains the overall classification accuracy while significantly improving model calibration. The proposed approach is based on expected calibration error, which is a common metric for quantifying miscalibration. Our approach can be easily integrated into any classification task as an auxiliary loss term, thus not requiring an explicit training round for calibration. We show that our approach reduces calibration error significantly across various architectures and datasets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liang-Improved%20Trainable%20Calibration%20Method%20for%20Neural%20Networks%20on%20Medical%20Imaging%20Classification-2020-BMVC.pdf}
}
@article{RegularizingPereyraICLRWorkshop2017, 
year = {2017}, 
title = {{Regularizing Neural Networks by Penalizing Confident Output Distributions}}, 
author = {Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, Łukasz and Hinton, Geoffrey}, 
journal = {ICLR Workshop}, 
eprint = {1701.06548}, 
abstract = {{We systematically explore regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. Furthermore, we connect a maximum entropy based confidence penalty to label smoothing through the direction of the KL divergence. We exhaustively evaluate the proposed confidence penalty and label smoothing on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and the confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyperparameters, suggesting the wide applicability of these regularizers.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Pereyra-Regularizing%20Neural%20Networks%20by%20Penalizing%20Confident%20Output%20Distributions-2017-ICLR%20Workshop.pdf}
}
@article{U-TimePerslevNeurIPS2019, 
year = {2019}, 
title = {{U-Time: A Fully Convolutional Network for Time Series Segmentation Applied to Sleep Staging}}, 
author = {Perslev, Mathias and Jensen, Michael Hejselbak and Darkner, Sune and Jennum, Poul Jørgen and Igel, Christian}, 
journal = {NeurIPS}, 
eprint = {1910.11162}, 
abstract = {{Neural networks are becoming more and more popular for the analysis of physiological time-series. The most successful deep learning systems in this domain combine convolutional and recurrent layers to extract useful features to model temporal relations. Unfortunately, these recurrent models are difficult to tune and optimize. In our experience, they often require task-specific modifications, which makes them challenging to use for non-experts. We propose U-Time, a fully feed-forward deep learning approach to physiological time series segmentation developed for the analysis of sleep data. U-Time is a temporal fully convolutional network based on the U-Net architecture that was originally proposed for image segmentation. U-Time maps sequential inputs of arbitrary length to sequences of class labels on a freely chosen temporal scale. This is done by implicitly classifying every individual time-point of the input signal and aggregating these classifications over fixed intervals to form the final predictions. We evaluated U-Time for sleep stage classification on a large collection of sleep electroencephalography (EEG) datasets. In all cases, we found that U-Time reaches or outperforms current state-of-the-art deep learning models while being much more robust in the training process and without requiring architecture or hyperparameter adaptation across tasks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Perslev-U-Time-%20A%20Fully%20Convolutional%20Network%20for%20Time%20Series%20Segmentation%20Applied%20to%20Sleep%20Staging-2019-NeurIPS.pdf}
}
@article{BERTDevlinNAACL2019, 
year = {2019}, 
title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}}, 
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina}, 
journal = {NAACL}, 
eprint = {1810.04805}, 
abstract = {{We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Devlin-BERT-%20Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding-2019-NAACL.pdf}
}
@article{ImprovingXieBMVC2018, 
year = {2018}, 
title = {{Improving Fast Segmentation With Teacher-student Learning}}, 
author = {Xie, Jiafeng and Shuai, Bing and Hu, Jian-Fang and Lin, Jingyang and Zheng, Wei-Shi}, 
journal = {BMVC}, 
eprint = {1810.08476}, 
abstract = {{Recently, segmentation neural networks have been significantly improved by demonstrating very promising accuracies on public benchmarks. However, these models are very heavy and generally suffer from low inference speed, which limits their application scenarios in practice. Meanwhile, existing fast segmentation models usually fail to obtain satisfactory segmentation accuracies on public benchmarks. In this paper, we propose a teacher-student learning framework that transfers the knowledge gained by a heavy and better performed segmentation network (i.e. teacher) to guide the learning of fast segmentation networks (i.e. student). Specifically, both zero-order and first-order knowledge depicted in the fine annotated images and unlabeled auxiliary data are transferred to regularize our student learning. The proposed method can improve existing fast segmentation models without incurring extra computational overhead, so it can still process images with the same fast speed. Extensive experiments on the Pascal Context, Cityscape and VOC 2012 datasets demonstrate that the proposed teacher-student learning framework is able to significantly boost the performance of student network.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xie-Improving%20Fast%20Segmentation%20With%20Teacher-student%20Learning-2018-BMVC.pdf}
}
@article{CityscapesCordtsCVPR2016, 
year = {2016}, 
title = {{The Cityscapes Dataset for Semantic Urban Scene Understanding}}, 
author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt}, 
journal = {CVPR}, 
abstract = {{Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Cordts-The%20Cityscapes%20Dataset%20for%20Semantic%20Urban%20Scene%20Understanding-2016-CVPR.pdf}
}
@article{CalibratedKuleshovNeurIPS2015, 
year = {2015}, 
title = {{Calibrated Structured Prediction}}, 
author = {Kuleshov, Volodymyr and Liang, Percy}, 
journal = {NeurIPS}, 
abstract = {{In user-facing applications, displaying calibrated confidence measures— probabilities that correspond to true frequency—can be as important as obtaining high accuracy. We are interested in calibration for structured prediction problems such as speech recognition, optical character recognition, and medical diagnosis. Structured prediction presents new challenges for calibration: the output space is large, and users may issue many types of probability queries (e.g., marginals) on the structured output. We extend the notion of calibration so as to handle various subtleties pertaining to the structured setting, and then provide a simple recalibration method that trains a binary classifier to predict probabilities of interest. We explore a range of features appropriate for structured recalibration, and demonstrate their efficacy on three real-world datasets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kuleshov-Calibrated%20Structured%20Prediction-2015-NeurIPS.pdf}
}
@article{GPTRadfordarXiv2018, 
year = {2018}, 
title = {{Improving Language Understanding by Generative Pre-Training}}, 
author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya}, 
journal = {arXiv}, 
abstract = {{Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Radford-Improving%20Language%20Understanding%20by%20Generative%20Pre-Training-2018-arXiv.pdf}
}
@article{GPT-2RadfordarXiv2019, 
year = {2019}, 
title = {{Language Models are Unsupervised Multitask Learners}}, 
author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya}, 
journal = {arXiv}, 
abstract = {{Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on task- specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and in- creasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Radford-Language%20Models%20are%20Unsupervised%20Multitask%20Learners-2019-arXiv.pdf}
}
@article{SwinTransformerLiuICCV2021, 
year = {2021}, 
title = {{Swin Transformer: Hierarchical Vision Transformer using Shifted Windows}}, 
author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining}, 
journal = {ICCV}, 
eprint = {2103.14030}, 
abstract = {{This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbackslashtextbf\{S\}hifted \textbackslashtextbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at\textbackslashtextasciitilde\textbackslashurl\{https://github.com/microsoft/Swin-Transformer\}.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-Swin%20Transformer-%20Hierarchical%20Vision%20Transformer%20using%20Shifted%20Windows-2021-ICCV.pdf}
}
@article{BoundaryLossKervadecMIA2021, 
year = {2021}, 
title = {{Boundary loss for highly unbalanced segmentation}}, 
author = {Kervadec, Hoel and Bouchtiba, Jihene and Desrosiers, Christian and Granger, Eric and Dolz, Jose and Ayed, Ismail Ben}, 
journal = {MIA}, 
abstract = {{Widely used loss functions for CNN segmentation, e.g., Dice or cross-entropy, are based on integrals over the segmentation regions. Unfortunately, for highly unbalanced segmentations, such regional summations have values that differ by several orders of magnitude across classes, which affects training performance and stability. We propose a boundary loss, which takes the form of a distance metric on the space of contours, not regions. This can mitigate the difficulties of highly unbalanced problems because it uses integrals over the interface between regions instead of unbalanced integrals over the regions. Furthermore, a boundary loss complements regional information. Inspired by graph-based optimization techniques for computing active-contour flows, we express a non-symmetric L 2 distance on the space of contours as a regional integral, which avoids completely local differential computations involving contour points. This yields a boundary loss expressed with the regional softmax probability outputs of the network, which can be easily combined with standard regional losses and implemented with any existing deep network architecture for N-D segmentation. We report comprehensive evaluations and comparisons on different unbalanced problems, showing that our boundary loss can yield significant increases in performances while improving training stability. Our code is publicly available 1 1 https://github.com/LIVIAETS/surface-loss .}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kervadec-Boundary%20loss%20for%20highly%20unbalanced%20segmentation-2021-MIA.pdf}
}
@article{iGPTChenICML2020, 
year = {2020}, 
title = {{Generative Pretraining from Pixels}}, 
author = {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeff and Jun, Heewoo and Dhariwal, Prafulla and Luan, David and Sutskever, Ilya}, 
journal = {ICML}, 
abstract = {{Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Trans- former to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet with- out labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3\% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0\% accuracy with full fine- tuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0\% top-1 accuracy on a linear probe of our features.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Generative%20Pretraining%20from%20Pixels-2020-ICML.pdf}
}
@article{GPT-3BrownNeurIPS2020, 
year = {2020}, 
title = {{Language Models are Few-Shot Learners}}, 
author = {Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario}, 
journal = {NeurIPS}, 
eprint = {2005.14165}, 
abstract = {{Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Brown-Language%20Models%20are%20Few-Shot%20Learners-2020-NeurIPS.pdf}
}
@article{DynamicBrabandereNeurIPS2016, 
year = {2016}, 
title = {{Dynamic Filter Networks}}, 
author = {Brabandere, Bert De and Jia, Xu and Tuytelaars, Tinne and Gool, Luc Van}, 
journal = {NeurIPS}, 
eprint = {1605.09673}, 
abstract = {{In a traditional convolutional layer, the learned filters stay fixed after training. In contrast, we introduce a new framework, the Dynamic Filter Network, where filters are generated dynamically conditioned on an input. We show that this architecture is a powerful one, with increased flexibility thanks to its adaptive nature, yet without an excessive increase in the number of model parameters. A wide variety of filtering operations can be learned this way, including local spatial transformations, but also others like selective (de)blurring or adaptive feature extraction. Moreover, multiple such layers can be combined, e.g. in a recurrent architecture. We demonstrate the effectiveness of the dynamic filter network on the tasks of video and stereo prediction, and reach state-of-the-art performance on the moving MNIST dataset with a much smaller model. By visualizing the learned filters, we illustrate that the network has picked up flow information by only looking at unlabelled training data. This suggests that the network can be used to pretrain networks for various supervised tasks in an unsupervised way, like optical flow and depth estimation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Brabandere-Dynamic%20Filter%20Networks-2016-NeurIPS.pdf}
}
@article{DeepSimonyanICLRWorkshop2014, 
year = {2014}, 
title = {{Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps}}, 
author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew}, 
journal = {ICLR Workshop}, 
eprint = {1312.6034}, 
abstract = {{This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Simonyan-Deep%20Inside%20Convolutional%20Networks-%20Visualising%20Image%20Classification%20Models%20and%20Saliency%20Maps-2014-ICLR%20Workshop.pdf}
}
@article{IntriguingSzegedyICLR2014, 
year = {2014}, 
title = {{Intriguing properties of neural networks}}, 
author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob}, 
journal = {ICLR}, 
eprint = {1312.6199}, 
abstract = {{Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.}}, 
keywords = {}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Szegedy-Intriguing%20properties%20of%20neural%20networks-2014-ICLR.pdf}
}
@article{GANsGoodfellowNeurIPS2014, 
year = {2014}, 
title = {{Generative Adversarial Nets}}, 
author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua}, 
journal = {NeurIPS}, 
abstract = {{We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 21 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Goodfellow-Generative%20Adversarial%20Nets-2014-NeurIPS.pdf}
}
@article{VisualizingZeilerECCV2014, 
year = {2014}, 
title = {{Visualizing and Understanding Convolutional Networks}}, 
author = {Zeiler, Matthew D and Fergus, Rob}, 
journal = {ECCV}, 
eprint = {1311.2901}, 
abstract = {{Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \textbackslashetal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zeiler-Visualizing%20and%20Understanding%20Convolutional%20Networks-2014-ECCV.pdf}
}
@article{ELUsClevertICLR2016, 
year = {2016}, 
title = {{Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)}}, 
author = {Clevert, Djork-Arné and Unterthiner, Thomas and Hochreiter, Sepp}, 
journal = {ICLR}, 
eprint = {1511.07289}, 
abstract = {{We introduce the "exponential linear unit" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10\% classification error for a single crop, single model network.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Clevert-Fast%20and%20Accurate%20Deep%20Network%20Learning%20by%20Exponential%20Linear%20Units%20(ELUs)-2016-ICLR.pdf}
}
@article{GELUsHendrycksarXiv2016, 
year = {2016}, 
title = {{Gaussian Error Linear Units (GELUs)}}, 
author = {Hendrycks, Dan and Gimpel, Kevin}, 
journal = {arXiv}, 
eprint = {1606.08415}, 
abstract = {{We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is \$x\textbackslashPhi(x)\$, where \$\textbackslashPhi(x)\$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (\$x\textbackslashmathbf\{1\}\_\{x>0\}\$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hendrycks-Gaussian%20Error%20Linear%20Units%20(GELUs)-2016-arXiv.pdf}
}
@article{DistillingFrosstarXiv2017, 
year = {2017}, 
title = {{Distilling a Neural Network Into a Soft Decision Tree}}, 
author = {Frosst, Nicholas and Hinton, Geoffrey}, 
journal = {arXiv}, 
eprint = {1711.09784}, 
abstract = {{Deep neural networks have proved to be a very effective way to perform classification tasks. They excel when the input data is high dimensional, the relationship between the input and the output is complicated, and the number of labeled training examples is large. But it is hard to explain why a learned network makes a particular classification decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead, explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Frosst-Distilling%20a%20Neural%20Network%20Into%20a%20Soft%20Decision%20Tree-2017-arXiv.pdf}
}
@article{InPlace-ABNBuloCVPR2018, 
year = {2018}, 
title = {{In-Place Activated BatchNorm for Memory-Optimized Training of DNNs}}, 
author = {Bulò, Samuel Rota and Porzi, Lorenzo and Kontschieder, Peter}, 
journal = {CVPR}, 
eprint = {1712.02616}, 
abstract = {{In this work we present In-Place Activated Batch Normalization (InPlace-ABN) - a novel approach to drastically reduce the training memory footprint of modern deep neural networks in a computationally efficient way. Our solution substitutes the conventionally used succession of BatchNorm + Activation layers with a single plugin layer, hence avoiding invasive framework surgery while providing straightforward applicability for existing deep learning frameworks. We obtain memory savings of up to 50\% by dropping intermediate results and by recovering required information during the backward pass through the inversion of stored forward results, with only minor increase (0.8-2\%) in computation time. Also, we demonstrate how frequently used checkpointing approaches can be made computationally as efficient as InPlace-ABN. In our experiments on image classification, we demonstrate on-par results on ImageNet-1k with state-of-the-art approaches. On the memory-demanding task of semantic segmentation, we report results for COCO-Stuff, Cityscapes and Mapillary Vistas, obtaining new state-of-the-art results on the latter without additional training data but in a single-scale and -model scenario. Code can be found at https://github.com/mapillary/inplace\_abn .}}, 
pages = {5639--5647}, 
keywords = {}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bulò-In-Place%20Activated%20BatchNorm%20for%20Memory-Optimized%20Training%20of%20DNNs-2018-CVPR.pdf}
}
@article{TrainingChenarXiv2016, 
year = {2016}, 
title = {{Training Deep Nets with Sublinear Memory Cost}}, 
author = {Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos}, 
journal = {arXiv}, 
eprint = {1604.06174}, 
abstract = {{We propose a systematic approach to reduce the memory consumption of deep neural network training. Specifically, we design an algorithm that costs O(sqrt(n)) memory to train a n layer network, with only the computational cost of an extra forward pass per mini-batch. As many of the state-of-the-art models hit the upper bound of the GPU memory, our algorithm allows deeper and more complex models to be explored, and helps advance the innovations in deep learning research. We focus on reducing the memory cost to store the intermediate feature maps and gradients during training. Computation graph analysis is used for automatic in-place operation and memory sharing optimizations. We show that it is possible to trade computation for memory - giving a more memory efficient training algorithm with a little extra computation cost. In the extreme case, our analysis also shows that the memory consumption can be reduced to O(log n) with as little as O(n log n) extra cost for forward computation. Our experiments show that we can reduce the memory cost of a 1,000-layer deep residual network from 48G to 7G with only 30 percent additional running time cost on ImageNet problems. Similarly, significant memory cost reduction is observed in training complex recurrent neural networks on very long sequences.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Training%20Deep%20Nets%20with%20Sublinear%20Memory%20Cost-2016-arXiv.pdf}
}
@article{PKTPassalisECCV2018, 
year = {2018}, 
title = {{Learning Deep Representations with Probabilistic Knowledge Transfer}}, 
author = {Passalis, Nikolaos and Tefas, Anastasios}, 
journal = {ECCV}, 
eprint = {1803.10837}, 
abstract = {{Knowledge Transfer (KT) techniques tackle the problem of transferring the knowledge from a large and complex neural network into a smaller and faster one. However, existing KT methods are tailored towards classification tasks and they cannot be used efficiently for other representation learning tasks. In this paper a novel knowledge transfer technique, that is capable of training a student model that maintains the same amount of mutual information between the learned representation and a set of (possible unknown) labels as the teacher model, is proposed. Apart from outperforming existing KT techniques, the proposed method allows for overcoming several limitations of existing methods providing new insight into KT as well as novel KT applications, ranging from knowledge transfer from handcrafted feature extractors to \{cross-modal\} KT from the textual modality into the representation extracted from the visual modality of the data.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Passalis-Learning%20Deep%20Representations%20with%20Probabilistic%20Knowledge%20Transfer-2018-ECCV.pdf}
}
@article{AttentiveNASWangCVPR2021, 
year = {2021}, 
title = {{AttentiveNAS: Improving Neural Architecture Search via Attentive Sampling}}, 
author = {Wang, Dilin and Li, Meng and Gong, Chengyue and Chandra, Vikas}, 
journal = {CVPR}, 
abstract = {{Neural architecture search (NAS) has shown great promise in designing state-of-the-art (SOTA) models that are both accurate and efficient. Recently, two-stage NAS, e.g. BigNAS, decouples the model training and searching process and achieves remarkable search efficiency and accuracy. Two-stage NAS requires sampling from the search space during training, which directly impacts the accuracy of the final searched models. While uniform sampling has been widely used for its simplicity, it is agnostic of the model performance Pareto front, which is the main focus in the search process, and thus, misses opportunities to further improve the model accuracy. In this work, we propose AttentiveNAS that focuses on improving the sampling strategy to achieve better performance Pareto. We also propose algorithms to efficiently and effectively identify the networks on the Pareto during training. Without extra re-training or post-processing, we can simultaneously obtain a large number of networks across a wide range of FLOPs. Our discovered model family, AttentiveNAS models, achieves top-1 accuracy from 77.3\% to 80.7\% on ImageNet, and outperforms SOTA models, including BigNAS, Once-for-All networks and FBNetV3. We also achieve ImageNet accuracy of 80.1\% with only 491 MFLOPs. Our training code and pretrained models are available at https://github.com/facebookresearch/AttentiveNAS.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-AttentiveNAS-%20Improving%20Neural%20Architecture%20Search%20via%20Attentive%20Sampling-2021-CVPR.pdf}
}
@article{SimCLRv2ChenNeurIPS2020, 
year = {2020}, 
title = {{Big Self-Supervised Models are Strong Semi-Supervised Learners}}, 
author = {Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey}, 
journal = {NeurIPS}, 
eprint = {2006.10029}, 
abstract = {{One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\% ImageNet top-1 accuracy with just 1\% of the labels (\$\textbackslashle\$13 labeled images per class) using ResNet-50, a \$10\textbackslashtimes\$ improvement in label efficiency over the previous state-of-the-art. With 10\% of labels, ResNet-50 trained with our method achieves 77.5\% top-1 accuracy, outperforming standard supervised training with all of the labels.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Big%20Self-Supervised%20Models%20are%20Strong%20Semi-Supervised%20Learners-2020-NeurIPS.pdf}
}
@article{DANNGaninJMLR2016, 
year = {2016}, 
title = {{Domain-Adversarial Training of Neural Networks}}, 
author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, François and Marchand, Mario and Lempitsky, Victor}, 
journal = {JMLR}, 
eprint = {1505.07818}, 
abstract = {{We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ganin-Domain-Adversarial%20Training%20of%20Neural%20Networks-2016-JMLR.pdf}
}
@article{SwAVCaronNeurIPS2020, 
year = {2020}, 
title = {{Unsupervised Learning of Visual Features by Contrasting Cluster Assignments}}, 
author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand}, 
journal = {NeurIPS}, 
eprint = {2006.09882}, 
abstract = {{Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our findings by achieving 75.3\% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Caron-Unsupervised%20Learning%20of%20Visual%20Features%20by%20Contrasting%20Cluster%20Assignments-2020-NeurIPS.pdf}
}
@article{DeepClusterCaronECCV2018, 
year = {2018}, 
title = {{Deep Clustering for Unsupervised Learning of Visual Features}}, 
author = {Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs}, 
journal = {ECCV}, 
eprint = {1807.05520}, 
abstract = {{Clustering is a class of unsupervised learning methods that has been extensively applied and studied in computer vision. Little work has been done to adapt it to the end-to-end training of visual features on large scale datasets. In this work, we present DeepCluster, a clustering method that jointly learns the parameters of a neural network and the cluster assignments of the resulting features. DeepCluster iteratively groups the features with a standard clustering algorithm, k-means, and uses the subsequent assignments as supervision to update the weights of the network. We apply DeepCluster to the unsupervised training of convolutional neural networks on large datasets like ImageNet and YFCC100M. The resulting model outperforms the current state of the art by a significant margin on all the standard benchmarks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Caron-Deep%20Clustering%20for%20Unsupervised%20Learning%20of%20Visual%20Features-2018-ECCV.pdf}
}
@article{MCDSaitoCVPR2018, 
year = {2018}, 
title = {{Maximum Classifier Discrepancy for Unsupervised Domain Adaptation}}, 
author = {Saito, Kuniaki and Watanabe, Kohei and Ushiku, Yoshitaka and Harada, Tatsuya}, 
journal = {CVPR}, 
abstract = {{In this work, we present a method for unsupervised domain adaptation. Many adversarial learning methods train domain classifier networks to distinguish the features as either a source or target and train a feature generator network to mimic the discriminator. Two problems exist with these methods. First, the domain classifier only tries to distinguish the features as a source or target and thus does not consider task-specific decision boundaries between classes. Therefore, a trained generator can generate ambiguous features near class boundaries. Second, these methods aim to completely match the feature distributions between different domains, which is difficult because of each domain's characteristics. To solve these problems, we introduce a new approach that attempts to align distributions of source and target by utilizing the task-specific decision boundaries. We propose to maximize the discrepancy between two classifiers' outputs to detect target samples that are far from the support of the source. A feature generator learns to generate target features near the support to minimize the discrepancy. Our method outperforms other methods on several datasets of image classification and semantic segmentation. The codes are available at https://github.com/mil-tokyo/MCD\_DA}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Saito-Maximum%20Classifier%20Discrepancy%20for%20Unsupervised%20Domain%20Adaptation-2018-CVPR.pdf}
}
@article{DIRT-TShuICLR2018, 
year = {2018}, 
title = {{A DIRT-T Approach to Unsupervised Domain Adaptation}}, 
author = {Shu, Rui and Bui, Hung H and Narui, Hirokazu and Ermon, Stefano}, 
journal = {ICLR}, 
eprint = {1802.08735}, 
abstract = {{Domain adaptation refers to the problem of leveraging labeled data in a source domain to learn an accurate model in a target domain where labels are scarce or unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin \& Lempitsky, 2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high-capacity, then feature distribution matching is a weak constraint, 2) in non-conservative domain adaptation (where no single classifier can perform well in both the source and target domains), training the model to do well on the source domain hurts performance on the target domain. In this paper, we address these issues through the lens of the cluster assumption, i.e., decision boundaries should not cross high-density data regions. We propose two novel and related models: 1) the Virtual Adversarial Domain Adaptation (VADA) model, which combines domain adversarial training with a penalty term that punishes the violation the cluster assumption; 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T) model, which takes the VADA model as initialization and employs natural gradient steps to further minimize the cluster assumption violation. Extensive empirical results demonstrate that the combination of these two models significantly improve the state-of-the-art performance on the digit, traffic sign, and Wi-Fi recognition domain adaptation benchmarks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Shu-A%20DIRT-T%20Approach%20to%20Unsupervised%20Domain%20Adaptation-2018-ICLR.pdf}
}
@article{ATIBustoICCV2017, 
year = {2017}, 
title = {{Open Set Domain Adaptation}}, 
author = {Busto, Pau Panareda and Gall, Juergen}, 
journal = {ICCV}, 
abstract = {{When the training and the test data belong to different domains, the accuracy of an object classifier is significantly reduced. Therefore, several algorithms have been proposed in the last years to diminish the so called domain shift between datasets. However, all available evaluation protocols for domain adaptation describe a closed set recognition task, where both domains, namely source and target, contain exactly the same object classes. In this work, we also explore the field of domain adaptation in open sets, which is a more realistic scenario where only a few categories of interest are shared between source and target data. Therefore, we propose a method that fits in both closed and open set scenarios. The approach learns a mapping from the source to the target domain by jointly solving an assignment problem that labels those target instances that potentially belong to the categories of interest present in the source dataset. A thorough evaluation shows that our approach outperforms the state-of-the-art.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Busto-Open%20Set%20Domain%20Adaptation-2017-ICCV.pdf}
}
@article{OSDASaitoECCV2018, 
year = {2018}, 
title = {{Open Set Domain Adaptation by Backpropagation}}, 
author = {Saito, Kuniaki and Yamamoto, Shohei and Ushiku, Yoshitaka and Harada, Tatsuya}, 
journal = {ECCV}, 
eprint = {1804.10427}, 
abstract = {{Numerous algorithms have been proposed for transferring knowledge from a label-rich domain (source) to a label-scarce domain (target). Almost all of them are proposed for a closed-set scenario, where the source and the target domain completely share the class of their samples. We call the shared class the \textbackslashdoublequote\{known class.\} However, in practice, when samples in target domain are not labeled, we cannot know whether the domains share the class. A target domain can contain samples of classes that are not shared by the source domain. We call such classes the \textbackslashdoublequote\{unknown class\} and algorithms that work well in the open set situation are very practical. However, most existing distribution matching methods for domain adaptation do not work well in this setting because unknown target samples should not be aligned with the source. In this paper, we propose a method for an open set domain adaptation scenario which utilizes adversarial training. A classifier is trained to make a boundary between the source and the target samples whereas a generator is trained to make target samples far from the boundary. Thus, we assign two options to the feature generator: aligning them with source known samples or rejecting them as unknown target samples. This approach allows extracting features that separate unknown target samples from known target samples. Our method was extensively evaluated in domain adaptation setting and outperformed other methods with a large margin in most settings.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Saito-Open%20Set%20Domain%20Adaptation%20by%20Backpropagation-2018-ECCV.pdf}
}
@article{UANYouCVPR2019, 
year = {2019}, 
title = {{Universal Domain Adaptation}}, 
author = {You, Kaichao and Long, Mingsheng and Cao, Zhangjie and Wang, Jianmin and Jordan, Michael I.}, 
journal = {CVPR}, 
abstract = {{Domain adaptation aims to transfer knowledge in the presence of the domain gap. Existing domain adaptation methods rely on rich prior knowledge about the relationship between the label sets of source and target domains, which greatly limits their application in the wild. This paper introduces Universal Domain Adaptation (UDA) that requires no prior knowledge on the label sets. For a given source label set and a target label set, they may contain a common label set and hold a private label set respectively, bringing up an additional category gap. UDA requires a model to either (1) classify the target sample correctly if it is associated with a label in the common label set, or (2) mark it as "unknown" otherwise. More importantly, a UDA model should work stably against a wide spectrum of commonness (the proportion of the common label set over the complete label set) so that it can handle real-world problems with unknown target label sets. To solve the universal domain adaptation problem, we propose Universal Adaptation Network (UAN). It quantifies sample-level transferability to discover the common label set and the label sets private to each domain, thereby promoting the adaptation in the automatically discovered common label set and recognizing the "unknown" samples successfully. A thorough evaluation shows that UAN outperforms the state of the art closed set, partial and open set domain adaptation methods in the novel UDA setting.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/You-Universal%20Domain%20Adaptation-2019-CVPR.pdf}
}
@article{HowIslamICLR2020, 
year = {2020}, 
title = {{How Much Position Information Do Convolutional Neural Networks Encode?}}, 
author = {Islam, Md Amirul and Jia, Sen and Bruce, Neil D B}, 
journal = {ICLR}, 
eprint = {2001.08248}, 
abstract = {{In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Islam-How%20Much%20Position%20Information%20Do%20Convolutional%20Neural%20Networks%20Encode--2020-ICLR.pdf}
}
@article{I-MaxPatelICLR2021, 
year = {2021}, 
title = {{Multi-Class Uncertainty Calibration via Mutual Information Maximization-based Binning}}, 
author = {Patel, Kanil and Beluch, William and Yang, Bin and Pfeiffer, Michael and Zhang, Dan}, 
journal = {ICLR}, 
eprint = {2006.13092}, 
abstract = {{Post-hoc multi-class calibration is a common approach for providing high-quality confidence estimates of deep neural network predictions. Recent work has shown that widely used scaling methods underestimate their calibration error, while alternative Histogram Binning (HB) methods often fail to preserve classification accuracy. When classes have small prior probabilities, HB also faces the issue of severe sample-inefficiency after the conversion into K one-vs-rest class-wise calibration problems. The goal of this paper is to resolve the identified issues of HB in order to provide calibrated confidence estimates using only a small holdout calibration dataset for bin optimization while preserving multi-class ranking accuracy. From an information-theoretic perspective, we derive the I-Max concept for binning, which maximizes the mutual information between labels and quantized logits. This concept mitigates potential loss in ranking performance due to lossy quantization, and by disentangling the optimization of bin edges and representatives allows simultaneous improvement of ranking and calibration performance. To improve the sample efficiency and estimates from a small calibration set, we propose a shared class-wise (sCW) calibration strategy, sharing one calibrator among similar classes (e.g., with similar class priors) so that the training sets of their class-wise calibration problems can be merged to train the single calibrator. The combination of sCW and I-Max binning outperforms the state of the art calibration methods on various evaluation metrics across different benchmark datasets and models, using a small calibration set (e.g., 1k samples for ImageNet).}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Patel-Multi-Class%20Uncertainty%20Calibration%20via%20Mutual%20Information%20Maximization-based%20Binning-2021-ICLR.pdf}
}
@article{DirichletCalibrationKullNeurIPS2019, 
year = {2019}, 
title = {{Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with Dirichlet calibration}}, 
author = {Kull, Meelis and Perello-Nieto, Miquel and Kängsepp, Markus and Filho, Telmo Silva and Song, Hao and Flach, Peter}, 
journal = {NeurIPS}, 
eprint = {1910.12656}, 
abstract = {{Class probabilities predicted by most multiclass classifiers are uncalibrated, often tending towards over-confidence. With neural networks, calibration can be improved by temperature scaling, a method to learn a single corrective multiplicative factor for inputs to the last softmax layer. On non-neural models the existing methods apply binary calibration in a pairwise or one-vs-rest fashion. We propose a natively multiclass calibration method applicable to classifiers from any model class, derived from Dirichlet distributions and generalising the beta calibration method from binary classification. It is easily implemented with neural nets since it is equivalent to log-transforming the uncalibrated probabilities, followed by one linear layer and softmax. Experiments demonstrate improved probabilistic predictions according to multiple measures (confidence-ECE, classwise-ECE, log-loss, Brier score) across a wide range of datasets and classifiers. Parameters of the learned Dirichlet calibration map provide insights to the biases in the uncalibrated model.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kull-Beyond%20temperature%20scaling-%20Obtaining%20well-calibrated%20multiclass%20probabilities%20with%20Dirichlet%20calibration-2019-NeurIPS.pdf}
}
@article{QuocNetLeICML2012, 
year = {2012}, 
title = {{Building high-level features using large scale unsupervised learning}}, 
author = {Le, Quoc V and Ranzato, Marc'Aurelio and Monga, Rajat and Devin, Matthieu and Chen, Kai and Corrado, Greg S and Dean, Jeff and Ng, Andrew Y}, 
journal = {ICML}, 
eprint = {1112.6209}, 
abstract = {{We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting with these learned features, we trained our network to obtain 15.8\% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70\% relative improvement over the previous state-of-the-art.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Le-Building%20high-level%20features%20using%20large%20scale%20unsupervised%20learning-2012-ICML.pdf}
}
@article{AlexNetKrizhevskyNeurIPS2012, 
year = {2012}, 
title = {{ImageNet classification with deep convolutional neural networks}}, 
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.}, 
journal = {NeurIPS}, 
abstract = {{We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Krizhevsky-ImageNet%20classification%20with%20deep%20convolutional%20neural%20networks-2012-NeurIPS.pdf}
}
@article{ExplainingGoodfellowICLR2015, 
year = {2015}, 
title = {{Explaining and Harnessing Adversarial Examples}}, 
author = {Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian}, 
journal = {ICLR}, 
eprint = {1412.6572}, 
abstract = {{Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Goodfellow-Explaining%20and%20Harnessing%20Adversarial%20Examples-2015-ICLR.pdf}
}
@article{TowardsMadryICLR2018, 
year = {2018}, 
title = {{Towards Deep Learning Models Resistant to Adversarial Attacks}}, 
author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian}, 
journal = {ICLR}, 
eprint = {1706.06083}, 
abstract = {{Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist\_challenge and https://github.com/MadryLab/cifar10\_challenge.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Madry-Towards%20Deep%20Learning%20Models%20Resistant%20to%20Adversarial%20Attacks-2018-ICLR.pdf}
}
@article{BilateralTomasiICCV1998, 
year = {1998}, 
title = {{Bilateral filtering for gray and color images}}, 
author = {Tomasi, C. and Manduchi, R.}, 
journal = {ICCV}, 
abstract = {{Bilateral filtering smooths images while preserving edges, by means of a nonlinear combination of nearby image values. The method is noniterative, local, and simple. It combines gray levels or colors based on both their geometric closeness and their photometric similarity, and prefers near values to distant values in both domain and range. In contrast with filters that operate on the three bands of a color image separately, a bilateral filter can enforce the perceptual metric underlying the CIE-Lab color space, and smooth colors and preserve edges in a way that is tuned to human perception. Also, in contrast with standard filtering, bilateral filtering produces no phantom colors along edges in color images, and reduces phantom colors where they appear in the original image.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tomasi-Bilateral%20filtering%20for%20gray%20and%20color%20images-1998-ICCV.pdf}
}
@article{KAHeCVPR2019, 
year = {2019}, 
title = {{Knowledge Adaptation for Efficient Semantic Segmentation}}, 
author = {He, Tong and Shen, Chunhua and Tian, Zhi and Gong, Dong and Sun, Changming and Yan, Youliang}, 
journal = {CVPR}, 
abstract = {{Both accuracy and efficiency are of significant importance to the task of semantic segmentation. Existing deep FCNs suffer from heavy computations due to a series of high-resolution feature maps for preserving the detailed knowledge in dense estimation. Although reducing the feature map resolution (i.e., applying a large overall stride) via subsampling operations (e.g., polling and convolution striding) can instantly increase the efficiency, it dramatically decreases the estimation accuracy. To tackle this dilemma, we propose a knowledge distillation method tailored for semantic segmentation to improve the performance of the compact FCNs with large overall stride. To handle the inconsistency between the features of the student and teacher network, we optimize the feature similarity in a transferred latent domain formulated by utilizing a pre-trained autoencoder. Moreover, an affinity distillation module is proposed to capture the long-range dependency by calculating the non-local interactions across the whole image. To validate the effectiveness of our proposed method, extensive experiments have been conducted on three popular benchmarks: Pascal VOC, Cityscapes and Pascal Context. Built upon a highly competitive baseline, our proposed method can improve the performance of a student network by 2.5\% (mIOU boosts from 70.2 to 72.7 on the cityscapes test set) and can train a better compact model with only 8\% float operations (FLOPS) of a model that achieves comparable performances.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/He-Knowledge%20Adaptation%20for%20Efficient%20Semantic%20Segmentation-2019-CVPR.pdf}
}
@article{IFVDWangECCV2020, 
year = {2020}, 
title = {{Intra-class Feature Variation Distillation for Semantic Segmentation}}, 
author = {Wang, Yukang and Zhou, Wei and Jiang, Tao and Bai, Xiang and Xu, Yongchao}, 
journal = {ECCV}, 
abstract = {{Current state-of-the-art semantic segmentation methods usually require high computational resources for accurate segmentation. One promising way to achieve a good trade-off between segmentation accuracy and efficiency is knowledge distillation. In this paper, different from previous methods performing knowledge distillation for densely pairwise relations, we propose a novel intra-class feature variation distillation (IFVD) to transfer the intra-class feature variation (IFV) of the cumbersome model (teacher) to the compact model (student). Concretely, we compute the feature center (regarded as the prototype) of each class and characterize the IFV with the set of similarity between the feature on each pixel and its corresponding class-wise prototype. The teacher model usually learns more robust intra-class feature representation than the student model, making them have different IFV. Transferring such IFV from teacher to student could make the student mimic the teacher better in terms of feature distribution, and thus improve the segmentation accuracy. We evaluate the proposed approach on three widely adopted benchmarks: Cityscapes, CamVid and Pascal VOC 2012, consistently improving state-of-the-art methods. The code is available at https://github.com/YukangWang/IFVD.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Intra-class%20Feature%20Variation%20Distillation%20for%20Semantic%20Segmentation-2020-ECCV.pdf}
}
@article{CDShuICCV2021, 
year = {2021}, 
title = {{Channel-wise Knowledge Distillation for Dense Prediction}}, 
author = {Shu, Changyong and Liu, Yifan and Gao, Jianfei and Yan, Zheng and Shen, Chunhua}, 
journal = {ICCV}, 
eprint = {2011.13256}, 
abstract = {{Knowledge distillation (KD) has been proven to be a simple and effective tool for training compact models. Almost all KD variants for dense prediction tasks align the student and teacher networks' feature maps in the spatial domain, typically by minimizing point-wise and/or pair-wise discrepancy. Observing that in semantic segmentation, some layers' feature activations of each channel tend to encode saliency of scene categories (analogue to class activation mapping), we propose to align features channel-wise between the student and teacher networks. To this end, we first transform the feature map of each channel into a probabilty map using softmax normalization, and then minimize the Kullback-Leibler (KL) divergence of the corresponding channels of the two networks. By doing so, our method focuses on mimicking the soft distributions of channels between networks. In particular, the KL divergence enables learning to pay more attention to the most salient regions of the channel-wise maps, presumably corresponding to the most useful signals for semantic segmentation. Experiments demonstrate that our channel-wise distillation outperforms almost all existing spatial distillation methods for semantic segmentation considerably, and requires less computational cost during training. We consistently achieve superior performance on three benchmarks with various network structures. Code is available at: https://git.io/Distiller}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Shu-Channel-wise%20Knowledge%20Distillation%20for%20Dense%20Prediction-2021-ICCV.pdf}
}
@article{PSPNetZhaoCVPR2017, 
year = {2017}, 
title = {{Pyramid Scene Parsing Network}}, 
author = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya}, 
journal = {CVPR}, 
abstract = {{Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper; we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4\% on PASCAL VOC 2012 and accuracy 80.2\% on Cityscapes.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhao-Pyramid%20Scene%20Parsing%20Network-2017-CVPR.pdf}
}
@article{PACSuCVPR2019, 
year = {2019}, 
title = {{Pixel-Adaptive Convolutional Neural Networks}}, 
author = {Su, Hang and Jampani, Varun and Sun, Deqing and Gallo, Orazio and Learned-Miller, Erik and Kautz, Jan}, 
journal = {CVPR}, 
abstract = {{Convolutions are the fundamental building blocks of CNNs. The fact that their weights are spatially shared is one of the main reasons for their widespread use, but it is also a major limitation, as it makes convolutions content- agnostic. We propose a pixel-adaptive convolution (PAC) operation, a simple yet effective modification of standard convolutions, in which the filter weights are multiplied with a spatially varying kernel that depends on learnable, local pixel features. PAC is a generalization of several popular filtering techniques and thus can be used for a wide range of use cases. Specifically, we demonstrate state-of- the-art performance when PAC is used for deep joint image upsampling. PAC also offers an effective alternative to fully-connected CRF (Full-CRF), called PAC-CRF, which performs competitively compared to Full-CRF, while being considerably faster. In addition, we also demonstrate that PAC can be used as a drop-in replacement for convolution layers in pre-trained networks, resulting in consistent performance improvements.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Su-Pixel-Adaptive%20Convolutional%20Neural%20Networks-2019-CVPR.pdf}
}
@article{GuidedHeTPAMI2013, 
year = {2013}, 
title = {{Guided Image Filtering}}, 
author = {He, Kaiming and Sun, Jian and Tang, Xiaoou}, 
journal = {TPAMI}, 
abstract = {{In this paper, we propose a novel explicit image filter called guided filter. Derived from a local linear model, the guided filter computes the filtering output by considering the content of a guidance image, which can be the input image itself or another different image. The guided filter can be used as an edge-preserving smoothing operator like the popular bilateral filter [1], but it has better behaviors near edges. The guided filter is also a more generic concept beyond smoothing: It can transfer the structures of the guidance image to the filtering output, enabling new filtering applications like dehazing and guided feathering. Moreover, the guided filter naturally has a fast and nonapproximate linear time algorithm, regardless of the kernel size and the intensity range. Currently, it is one of the fastest edge-preserving filters. Experiments show that the guided filter is both effective and efficient in a great variety of computer vision and computer graphics applications, including edge-aware smoothing, detail enhancement, HDR compression, image matting/feathering, dehazing, joint upsampling, etc.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/He-Guided%20Image%20Filtering-2013-TPAMI.pdf}
}
@article{EfficientZhangCVPR2015, 
year = {2015}, 
title = {{Efficient and Accurate Approximations of Nonlinear Convolutional Networks}}, 
author = {Zhang, Xiangyu and Zou, Jianhua and Ming, Xiang and He, Kaiming and Sun, Jian}, 
journal = {CVPR}, 
eprint = {1411.4229}, 
abstract = {{This paper aims to accelerate the test-time computation of deep convolutional neural networks (CNNs). Unlike existing methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We minimize the reconstruction error of the nonlinear responses, subject to a low-rank constraint which helps to reduce the complexity of filters. We develop an effective solution to this constrained nonlinear optimization problem. An algorithm is also presented for reducing the accumulated error when multiple layers are approximated. A whole-model speedup ratio of 4x is demonstrated on a large network trained for ImageNet, while the top-5 error rate is only increased by 0.9\%. Our accelerated model has a comparably fast speed as the "AlexNet", but is 4.7\% more accurate.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Efficient%20and%20Accurate%20Approximations%20of%20Nonlinear%20Convolutional%20Networks-2015-CVPR.pdf}
}
@article{SelectiveSearchUijlings2013, 
year = {2013}, 
title = {{Selective Search for Object Recognition}}, 
author = {Uijlings, J R R and Sande, K E A van de and Gevers, T and Smeulders, A W M}, 
journal = {IJCV}, 
abstract = {{This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 \% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/\textbackslashtextasciitildeuijlings/SelectiveSearch.html).}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Uijlings-Selective%20Search%20for%20Object%20Recognition-2013-IJCV.pdf}
}
@article{R-CNNGirshickCVPR2014, 
year = {2014}, 
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}}, 
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra}, 
journal = {CVPR}, 
abstract = {{Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012—achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. Source code for the complete system is available at http://www.cs.berkeley.edu/\textbackslashtextasciitilderbg/rcnn.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Girshick-Rich%20feature%20hierarchies%20for%20accurate%20object%20detection%20and%20semantic%20segmentation-2014-CVPR.pdf}
}
@article{SDSHariharanECCV2014, 
year = {2014}, 
title = {{Simultaneous Detection and Segmentation}}, 
author = {Hariharan, Bharath and Arbeláez, Pablo and Girshick, Ross and Malik, Jitendra}, 
journal = {ECCV}, 
eprint = {1407.1808}, 
abstract = {{We aim to detect all instances of a category in an image and, for each instance, mark the pixels that belong to it. We call this task Simultaneous Detection and Segmentation (SDS). Unlike classical bounding box detection, SDS requires a segmentation and not just a box. Unlike classical semantic segmentation, we require individual object instances. We build on recent work that uses convolutional neural networks to classify category-independent region proposals (R-CNN [16]), introducing a novel architecture tailored for SDS. We then use category-specific, top- down figure-ground predictions to refine our bottom-up proposals. We show a 7 point boost (16\% relative) over our baselines on SDS, a 5 point boost (10\% relative) over state-of-the-art on semantic segmentation, and state-of-the-art performance in object detection. Finally, we provide diagnostic tools that unpack performance and provide directions for future work.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hariharan-Simultaneous%20Detection%20and%20Segmentation-2014-ECCV.pdf}
}
@article{SPP-NetHeTPAMI2015, 
year = {2015}, 
title = {{Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition}}, 
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian}, 
journal = {TPAMI}, 
abstract = {{Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224\$\textbackslashtimes\$ 224) input image. This requirement is artificial and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, spatial pyramid pooling, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102 \$\textbackslashtimes\$ faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank \#2 in object detection and \#3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/He-Spatial%20Pyramid%20Pooling%20in%20Deep%20Convolutional%20Networks%20for%20Visual%20Recognition-2015-TPAMI.pdf}
}
@article{CFMDaiCVPR2015, 
year = {2015}, 
title = {{Convolutional Feature Masking for Joint Object and Stuff Segmentation}}, 
author = {Dai, Jifeng and He, Kaiming and Sun, Jian}, 
journal = {CVPR}, 
eprint = {1412.1283}, 
abstract = {{The topic of semantic segmentation has witnessed considerable progress due to the powerful features learned by convolutional neural networks (CNNs) [13]. The current leading approaches for semantic segmentation exploit shape information by extracting CNN features from masked image regions. This strategy introduces artificial boundaries on the images and may impact the quality of the extracted features. Besides, the operations on the raw image domain require to compute thousands of networks on a single image, which is time-consuming. In this paper, we propose to exploit shape information via masking convolutional features. The proposal segments (e.g., superpixels) are treated as masks on the convolutional feature maps. The CNN features of segments are directly masked out from these maps and used to train classifiers for recognition. We further propose a joint method to handle objects and “stuff” (e.g., grass, sky, water) in the same framework. State-of-the-art results are demonstrated on benchmarks of PASCAL VOC and new PASCAL-CONTEXT, with a compelling computational speed.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Dai-Convolutional%20Feature%20Masking%20for%20Joint%20Object%20and%20Stuff%20Segmentation-2015-CVPR.pdf}
}
@article{DEFXuICML2015, 
year = {2015}, 
title = {{Deep Edge-Aware Filters}}, 
author = {Xu, Li and Ren, Jimmy SJ. and Yan, Qiong and Liao, Renjie and Jia, Jiaya}, 
journal = {ICML}, 
abstract = {{There are many edge-aware filters varying in their construction forms and filtering properties. It seems impossible to uniformly represent and accelerate them in a single framework. We made the attempt to learn a big and important family of edge-aware operators from data. Our method is based on a deep convolutional neural network with a gradient domain training procedure, which gives rise to a powerful tool to approximate various filters without knowing the original models and implementation details. The only difference among these operators in our system becomes merely the learned parameters. Our system enables fast approximation for complex edge-aware filters and achieves up to 200x acceleration, regardless of their originally very different implementation. Fast speed can also be achieved when creating new effects using spatially varying filter or filter combination, bearing out the effectiveness of our deep edge-aware filters.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xu-Deep%20Edge-Aware%20Filters-2015-ICML.pdf}
}
@article{DGFWuCVPR2018, 
year = {2018}, 
title = {{Fast End-to-End Trainable Guided Filter}}, 
author = {Wu, Huikai and Zheng, Shuai and Zhang, Junge and Huang, Kaiqi}, 
journal = {CVPR}, 
abstract = {{Image processing and pixel-wise dense prediction have been advanced by harnessing the capabilities of deep learning. One central issue of deep learning is the limited capacity to handle joint upsampling. We present a deep learning building block for joint upsampling, namely guided filtering layer. This layer aims at efficiently generating the high-resolution output given the corresponding low-resolution one and a high-resolution guidance map. The proposed layer is composed of a guided filter, which is reformulated as a fully differentiable block. To this end, we show that a guided filter can be expressed as a group of spatial varying linear transformation matrices. This layer could be integrated with the convolutional neural networks (CNNs) and jointly optimized through end-to-end training. To further take advantage of end-to-end training, we plug in a trainable transformation function that generates task-specific guidance maps. By integrating the CNNs and the proposed layer, we form deep guided filtering networks. The proposed networks are evaluated on five advanced image processing tasks. Experiments on MIT-Adobe FiveK Dataset demonstrate that the proposed approach runs \$10-100\textbackslashtimes faster\$ and achieves the state-of-the-art performance. We also show that the proposed guided filtering layer helps to improve the performance of multiple pixel-wise dense prediction tasks. The code is available at https://github.com/wuhuikai/DeepGuidedFilter.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wu-Fast%20End-to-End%20Trainable%20Guided%20Filter-2018-CVPR.pdf}
}
@article{ConvolutionalHeCVPR2015, 
year = {2015}, 
title = {{Convolutional Neural Networks at Constrained Time Cost}}, 
author = {He, Kaiming and Sun, Jian}, 
journal = {CVPR}, 
abstract = {{Though recent advanced convolutional neural networks (CNNs) have been improving the image recognition accuracy, the models are getting more complex and time-consuming. For real-world applications in industrial and commercial scenarios, engineers and developers are often faced with the requirement of constrained time budget. In this paper, we investigate the accuracy of CNNs under constrained time cost. Under this constraint, the designs of the network architectures should exhibit as trade-offs among the factors like depth, numbers of filters, filter sizes, etc. With a series of controlled comparisons, we progressively modify a baseline model while preserving its time complexity. This is also helpful for understanding the importance of the factors in network designs. We present an architecture that achieves very competitive accuracy in the ImageNet dataset (11.8\% top-5 error, 10-view test), yet is 20\% faster than “AlexNet” [14] (16.0\% top-5 error, 10-view test).}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/He-Convolutional%20Neural%20Networks%20at%20Constrained%20Time%20Cost-2015-CVPR.pdf}
}
@article{SemanticBrabanderearXiv2017, 
year = {2017}, 
title = {{Semantic Instance Segmentation with a Discriminative Loss Function}}, 
author = {Brabandere, Bert De and Neven, Davy and Gool, Luc Van}, 
journal = {arXiv}, 
eprint = {1708.02551}, 
abstract = {{Semantic instance segmentation remains a challenging task. In this work we propose to tackle the problem with a discriminative loss function, operating at the pixel level, that encourages a convolutional network to produce a representation of the image that can easily be clustered into instances with a simple post-processing step. The loss function encourages the network to map each pixel to a point in feature space so that pixels belonging to the same instance lie close together while different instances are separated by a wide margin. Our approach of combining an off-the-shelf network with a principled loss function inspired by a metric learning objective is conceptually simple and distinct from recent efforts in instance segmentation. In contrast to previous works, our method does not rely on object proposals or recurrent mechanisms. A key contribution of our work is to demonstrate that such a simple setup without bells and whistles is effective and can perform on par with more complex methods. Moreover, we show that it does not suffer from some of the limitations of the popular detect-and-segment approaches. We achieve competitive performance on the Cityscapes and CVPPP leaf segmentation benchmarks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Brabandere-Semantic%20Instance%20Segmentation%20with%20a%20Discriminative%20Loss%20Function-2017-arXiv.pdf}
}
@article{SRCNNDongECCV2014, 
year = {2014}, 
title = {{Learning a Deep Convolutional Network for Image Super-Resolution}}, 
author = {Dong, Chao and Loy, Chen Change and He, Kaiming and Tang, Xiaoou}, 
journal = {ECCV}, 
abstract = {{We propose a deep learning method for single image super- resolution (SR). Our method directly learns an end-to-end mapping be- tween the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) [15] that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Dong-Learning%20a%20Deep%20Convolutional%20Network%20for%20Image%20Super-Resolution-2014-ECCV.pdf}
}
@article{EncNetZhangCVPR2018, 
year = {2018}, 
title = {{Context Encoding for Semantic Segmentation}}, 
author = {Zhang, Hang and Dana, Kristin and Shi, Jianping and Zhang, Zhongyue and Wang, Xiaogang and Tyagi, Ambrish and Agrawal, Amit}, 
journal = {CVPR}, 
abstract = {{Recent work has made significant progress in improving spatial resolution for pixelwise labeling with Fully Convolutional Network (FCN) framework by employing Dilated/Atrous convolution, utilizing multi-scale features and refining boundaries. In this paper, we explore the impact of global contextual information in semantic segmentation by introducing the Context Encoding Module, which captures the semantic context of scenes and selectively highlights class-dependent featuremaps. The proposed Context Encoding Module significantly improves semantic segmentation results with only marginal extra computation cost over FCN. Our approach has achieved new state-of-the-art results 51.7\% mIoU on PASCAL-Context, 85.9\% mIoU on PASCAL VOC 2012. Our single model achieves a final score of 0.5567 on ADE20K test set, which surpasses the winning entry of COCO-Place Challenge 2017. In addition, we also explore how the Context Encoding Module can improve the feature representation of relatively shallow networks for the image classification on CIFAR-10 dataset. Our 14 layer network has achieved an error rate of 3.45\%, which is comparable with state-of-the-art approaches with over 10× more layers. The source code for the complete system are publicly available11Links can be found at http://hangzh.com/. Links can be found at http://hangzh.com/}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Context%20Encoding%20for%20Semantic%20Segmentation-2018-CVPR.pdf}
}
@article{AdversarialKurakinICLR2017, 
year = {2017}, 
title = {{Adversarial Machine Learning at Scale}}, 
author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy}, 
journal = {ICLR}, 
eprint = {1611.01236}, 
abstract = {{Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a "label leaking" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kurakin-Adversarial%20Machine%20Learning%20at%20Scale-2017-ICLR.pdf}
}
@article{AdamWLoshchilovICLR2019, 
year = {2019}, 
title = {{Decoupled Weight Decay Regularization}}, 
author = {Loshchilov, Ilya and Hutter, Frank}, 
journal = {ICLR}, 
eprint = {1711.05101}, 
abstract = {{L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslashemph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \textbackslashemph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Loshchilov-Decoupled%20Weight%20Decay%20Regularization-2019-ICLR.pdf}
}
@article{ResidualVeitNeurIPS2016, 
year = {2016}, 
title = {{Residual Networks Behave Like Ensembles of Relatively Shallow Networks}}, 
author = {Veit, Andreas and Wilber, Michael and Belongie, Serge}, 
journal = {NeurIPS}, 
eprint = {1605.06431}, 
abstract = {{In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Veit-Residual%20Networks%20Behave%20Like%20Ensembles%20of%20Relatively%20Shallow%20Networks-2016-NeurIPS.pdf}
}
@article{ResNet-RSBelloICLR2021, 
year = {2021}, 
title = {{Revisiting ResNets: Improved Training and Scaling Strategies}}, 
author = {Bello, Irwan and Fedus, William and Du, Xianzhi and Cubuk, Ekin D and Srinivas, Aravind and Lin, Tsung-Yi and Shlens, Jonathon and Zoph, Barret}, 
journal = {ICLR}, 
eprint = {2103.07579}, 
abstract = {{Novel computer vision architectures monopolize the spotlight, but the impact of the model architecture is often conflated with simultaneous changes to training methodology and scaling strategies. Our work revisits the canonical ResNet (He et al., 2015) and studies these three aspects in an effort to disentangle them. Perhaps surprisingly, we find that training and scaling strategies may matter more than architectural changes, and further, that the resulting ResNets match recent state-of-the-art models. We show that the best performing scaling strategy depends on the training regime and offer two new scaling strategies: (1) scale model depth in regimes where overfitting can occur (width scaling is preferable otherwise); (2) increase image resolution more slowly than previously recommended (Tan \& Le, 2019). Using improved training and scaling strategies, we design a family of ResNet architectures, ResNet-RS, which are 1.7x - 2.7x faster than EfficientNets on TPUs, while achieving similar accuracies on ImageNet. In a large-scale semi-supervised learning setup, ResNet-RS achieves 86.2\% top-1 ImageNet accuracy, while being 4.7x faster than EfficientNet NoisyStudent. The training techniques improve transfer performance on a suite of downstream tasks (rivaling state-of-the-art self-supervised algorithms) and extend to video classification on Kinetics-400. We recommend practitioners use these simple revised ResNets as baselines for future research.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bello-Revisiting%20ResNets-%20Improved%20Training%20and%20Scaling%20Strategies-2021-ICLR.pdf}
}
@article{EfficientDetTanCVPR2020, 
year = {2020}, 
title = {{EfficientDet: Scalable and Efficient Object Detection}}, 
author = {Tan, Mingxing and Pang, Ruoming and Le, Quoc V}, 
journal = {CVPR}, 
eprint = {1911.09070}, 
abstract = {{Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multiscale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and better backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with single model and single-scale, our EfficientDet-D7 achieves state-of-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs, being 4x - 9x smaller and using 13x - 42x fewer FLOPs than previous detectors. Code is available at https://github.com/google/automl/tree/master/efficientdet.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tan-EfficientDet-%20Scalable%20and%20Efficient%20Object%20Detection-2020-CVPR.pdf}
}
@article{FastFCNWuarXiv2019, 
year = {2019}, 
title = {{FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation}}, 
author = {Wu, Huikai and Zhang, Junge and Huang, Kaiqi and Liang, Kongming and Yu, Yizhou}, 
journal = {arXiv}, 
eprint = {1903.11816}, 
abstract = {{Modern approaches for semantic segmentation usually employ dilated convolutions in the backbone to extract high-resolution feature maps, which brings heavy computation complexity and memory footprint. To replace the time and memory consuming dilated convolutions, we propose a novel joint upsampling module named Joint Pyramid Upsampling (JPU) by formulating the task of extracting high-resolution feature maps into a joint upsampling problem. With the proposed JPU, our method reduces the computation complexity by more than three times without performance loss. Experiments show that JPU is superior to other upsampling modules, which can be plugged into many existing approaches to reduce computation complexity and improve performance. By replacing dilated convolutions with the proposed JPU module, our method achieves the state-of-the-art performance in Pascal Context dataset (mIoU of 53.13\%) and ADE20K dataset (final score of 0.5584) while running 3 times faster.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wu-FastFCN-%20Rethinking%20Dilated%20Convolution%20in%20the%20Backbone%20for%20Semantic%20Segmentation-2019-arXiv.pdf}
}
@article{3DShapeNetsWuCVPR2015, 
year = {2015}, 
title = {{3D ShapeNets: A deep representation for volumetric shapes}}, 
author = {Wu, Zhirong and Song, Shuran and Khosla, Aditya and Yu, Fisher and Zhang, Linguang and Tang, Xiaoou and Xiao, Jianxiong}, 
journal = {CVPR}, 
abstract = {{3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5D depth sensors (e.g. Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop. Apart from category recognition, recovering full 3D shapes from view-based 2.5D depth maps is also a critical part of visual understanding. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model, 3D ShapeNetqs, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representation automatically. It naturally supports joint object recognition and shape completion from 2.5D depth maps, and it enables active object recognition through view planning. To train our 3D deep learning model, we construct ModelNet-a large-scale 3D CAD model dataset. Extensive experiments show that our 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wu-3D%20ShapeNets-%20A%20deep%20representation%20for%20volumetric%20shapes-2015-CVPR.pdf}
}
@article{VolumetricQiCVPR2016, 
year = {2016}, 
title = {{Volumetric and Multi-View CNNs for Object Classification on 3D Data}}, 
author = {Qi, Charles R. and Su, Hao and Nießner, Matthias and Dai, Angela and Yan, Mengyuan and Guibas, Leonidas J.}, 
journal = {CVPR}, 
abstract = {{3D shape models are becoming widely available and easier to capture, making available 3D information crucial for progress in object classification. Current state-of-the-art methods rely on CNNs to address this problem. Recently, we witness two types of CNNs being developed: CNNs based upon volumetric representations versus CNNs based upon multi-view representations. Empirical results from these two types of CNNs exhibit a large gap, indicating that existing volumetric CNN architectures and approaches are unable to fully exploit the power of 3D representations. In this paper, we aim to improve both volumetric CNNs and multi-view CNNs according to extensive analysis of existing approaches. To this end, we introduce two distinct network architectures of volumetric CNNs. In addition, we examine multi-view CNNs, where we introduce multi-resolution filtering in 3D. Overall, we are able to outperform current state-of-the-art methods for both volumetric CNNs and multi-view CNNs. We provide extensive experiments designed to evaluate underlying design choices, thus providing a better understanding of the space of methods available for object classification on 3D data.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Qi-Volumetric%20and%20Multi-View%20CNNs%20for%20Object%20Classification%20on%203D%20Data-2016-CVPR.pdf}
}
@article{PointNetQiCVPR2017, 
year = {2017}, 
title = {{PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation}}, 
author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.}, 
journal = {CVPR}, 
abstract = {{Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Qi-PointNet-%20Deep%20Learning%20on%20Point%20Sets%20for%203D%20Classification%20and%20Segmentation-2017-CVPR.pdf}
}
@article{PSANetZhaoECCV2018, 
year = {2018}, 
title = {{PSANet: Point-wise Spatial Attention Network for Scene Parsing}}, 
author = {Zhao, Hengshuang and Zhang, Yi and Liu, Shu and Shi, Jianping and Loy, Chen Change and Lin, Dahua and Jia, Jiaya}, 
journal = {ECCV}, 
abstract = {{We notice information flow in convolutional neural networks is restricted inside local neighborhood regions due to the physical design of convolutional filters, which limits the overall understanding of complex scenes. In this paper, we propose the point-wise spatial attention network (PSANet) to relax the local neighborhood constraint. Each position on the feature map is connected to all the other ones through a self-adaptively learned attention mask. Moreover, information propagation in bi-direction for scene parsing is enabled. Information at other positions can be collected to help the prediction of the current position and vice versa, information at the current position can be distributed to assist the prediction of other ones. Our proposed approach achieves top performance on various competitive scene parsing datasets, including ADE20K, PASCAL VOC 2012 and Cityscapes, demonstrating its effectiveness and generality.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhao-PSANet-%20Point-wise%20Spatial%20Attention%20Network%20for%20Scene%20Parsing-2018-ECCV.pdf}
}
@article{BillionYalnizarXiv2019, 
year = {2019}, 
title = {{Billion-scale semi-supervised learning for image classification}}, 
author = {Yalniz, I Zeki and Jégou, Hervé and Chen, Kan and Paluri, Manohar and Mahajan, Dhruv}, 
journal = {arXiv}, 
eprint = {1905.00546}, 
abstract = {{This paper presents a study of semi-supervised learning with large convolutional networks. We propose a pipeline, based on a teacher/student paradigm, that leverages a large collection of unlabelled images (up to 1 billion). Our main goal is to improve the performance for a given target architecture, like ResNet-50 or ResNext. We provide an extensive analysis of the success factors of our approach, which leads us to formulate some recommendations to produce high-accuracy models for image classification with semi-supervised learning. As a result, our approach brings important gains to standard architectures for image, video and fine-grained classification. For instance, by leveraging one billion unlabelled images, our learned vanilla ResNet-50 achieves 81.2\% top-1 accuracy on the ImageNet benchmark.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yalniz-Billion-scale%20semi-supervised%20learning%20for%20image%20classification-2019-arXiv.pdf}
}
@article{SKNetLiCVPR2019, 
year = {2019}, 
title = {{Selective Kernel Networks}}, 
author = {Li, Xiang and Wang, Wenhai and Hu, Xiaolin and Yang, Jian}, 
journal = {CVPR}, 
abstract = {{In standard Convolutional Neural Networks (CNNs), the receptive fields of artificial neurons in each layer are designed to share the same size. It is well-known in the neuroscience community that the receptive field size of visual cortical neurons are modulated by the stimulus, which has been rarely considered in constructing CNNs. We propose a dynamic selection mechanism in CNNs that allows each neuron to adaptively adjust its receptive field size based on multiple scales of input information. A building block called Selective Kernel (SK) unit is designed, in which multiple branches with different kernel sizes are fused using softmax attention that is guided by the information in these branches. Different attentions on these branches yield different sizes of the effective receptive fields of neurons in the fusion layer. Multiple SK units are stacked to a deep network termed Selective Kernel Networks (SKNets). On the ImageNet and CIFAR benchmarks, we empirically show that SKNet outperforms the existing state-of-the-art architectures with lower model complexity. Detailed analyses show that the neurons in SKNet can capture target objects with different scales, which verifies the capability of neurons for adaptively adjusting their receptive field sizes according to the input. The code and models are available at https://github.com/implus/SKNet.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Selective%20Kernel%20Networks-2019-CVPR.pdf}
}
@article{SqueezeNetIandolaarXiv2016, 
year = {2016}, 
title = {{SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size}}, 
author = {Iandola, Forrest N and Han, Song and Moskewicz, Matthew W and Ashraf, Khalid and Dally, William J and Keutzer, Kurt}, 
journal = {arXiv}, 
eprint = {1602.07360}, 
abstract = {{Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet). The SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Iandola-SqueezeNet-%20AlexNet-level%20accuracy%20with%2050x%20fewer%20parameters%20and%20-0.5MB%20model%20size-2016-arXiv.pdf}
}
@article{FastR-CNNGirshickICCV2015, 
year = {2015}, 
title = {{Fast R-CNN}}, 
author = {Girshick, Ross}, 
journal = {ICCV}, 
abstract = {{This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network \$9\textbackslashtimes\$ faster than R-CNN, is \$213\textbackslashtimes\$ faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 \$3\textbackslashtimes\$ faster, tests \$10\textbackslashtimes\$ faster, and is more accurate. Fast R-CNN is implemented in Python and \$C\$++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rCNN.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Girshick-Fast%20R-CNN-2015-ICCV.pdf}
}
@article{YOLORedmonCVPR2016, 
year = {2016}, 
title = {{You Only Look Once: Unified, Real-Time Object Detection}}, 
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali}, 
journal = {CVPR}, 
abstract = {{We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Redmon-You%20Only%20Look%20Once-%20Unified,%20Real-Time%20Object%20Detection-2016-CVPR.pdf}
}
@article{SCKDZhuICCV2021, 
year = {2021}, 
title = {{Student Customized Knowledge Distillation: Bridging the Gap Between Student and Teacher}}, 
author = {Zhu, Yichen and Wang, Yi}, 
journal = {ICCV}, 
abstract = {{Knowledge distillation (KD) transfers the dark knowledge from cumbersome networks (teacher) to lightweight (student) networks and expects the student to achieve more promising performance than training without the teacher’s knowledge. However, a counter-intuitive argument is that better teachers do not make better students due to the capacity mismatch. To this end, we present a novel adaptive knowledge distillation method to complement traditional approaches. The proposed method, named as Student Customized Knowledge Distillation (SCKD), examines the capacity mismatch between teacher and student from the perspective of gradient similarity. We formulate the knowledge distillation as a multi-task learning problem so that the teacher transfers knowledge to the student only if the student can benefit from learning such knowledge. We validate our methods on multiple datasets with various teacher- student configurations on image classification, object detection, and semantic segmentation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhu-Student%20Customized%20Knowledge%20Distillation-%20Bridging%20the%20Gap%20Between%20Student%20and%20Teacher-2021-ICCV.pdf}
}
@article{PASCALContextMottaghiCVPR2014, 
year = {2014}, 
title = {{The Role of Context for Object Detection and Semantic Segmentation in the Wild}}, 
author = {Mottaghi, Roozbeh and Chen, Xianjie and Liu, Xiaobai and Cho, Nam-Gyu and Lee, Seong-Whan and Fidler, Sanja and Urtasun, Raquel and Yuille, Alan}, 
journal = {CVPR}, 
abstract = {{In this paper we study the role of context in existing state-of-the-art detection and segmentation approaches. Towards this goal, we label every pixel of PASCAL VOC 2010 detection challenge with a semantic category. We believe this data will provide plenty of challenges to the community, as it contains 520 additional classes for semantic segmentation and object detection. Our analysis shows that nearest neighbor based approaches perform poorly on semantic segmentation of contextual classes, showing the variability of PASCAL imagery. Furthermore, improvements of existing contextual models for detection is rather modest. In order to push forward the performance in this difficult scenario, we propose a novel deformable part-based model, which exploits both local context around each candidate detection as well as global context at the level of the scene. We show that this contextual reasoning significantly helps in detecting objects at all scales.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Mottaghi-The%20Role%20of%20Context%20for%20Object%20Detection%20and%20Semantic%20Segmentation%20in%20the%20Wild-2014-CVPR.pdf}
}
@article{OnMixupTrainingThulasidasanNeurIPS2019, 
year = {2019}, 
title = {{On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks}}, 
author = {Thulasidasan, Sunil and Chennupati, Gopinath and Bilmes, Jeff and Bhattacharya, Tanmoy and Michalak, Sarah}, 
journal = {NeurIPS}, 
eprint = {1905.11001}, 
abstract = {{Mixup\textbackslashtextasciitilde\textbackslashcite\{zhang2017mixup\} is a recently proposed method for training deep neural networks where additional samples are generated during training by convexly combining random pairs of images and their associated labels. While simple to implement, it has been shown to be a surprisingly effective method of data augmentation for image classification: DNNs trained with mixup show noticeable gains in classification performance on a number of image classification benchmarks. In this work, we discuss a hitherto untouched aspect of mixup training -- the calibration and predictive uncertainty of models trained with mixup. We find that DNNs trained with mixup are significantly better calibrated -- i.e., the predicted softmax scores are much better indicators of the actual likelihood of a correct prediction -- than DNNs trained in the regular fashion. We conduct experiments on a number of image classification architectures and datasets -- including large-scale datasets like ImageNet -- and find this to be the case. Additionally, we find that merely mixing features does not result in the same calibration benefit and that the label smoothing in mixup training plays a significant role in improving calibration. Finally, we also observe that mixup-trained DNNs are less prone to over-confident predictions on out-of-distribution and random-noise data. We conclude that the typical overconfidence seen in neural networks, even on in-distribution data is likely a consequence of training with hard labels, suggesting that mixup be employed for classification tasks where predictive uncertainty is a significant concern.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Thulasidasan-On%20Mixup%20Training-%20Improved%20Calibration%20and%20Predictive%20Uncertainty%20for%20Deep%20Neural%20Networks-2019-NeurIPS.pdf}
}
@article{mixupZhangICLR2018, 
year = {2018}, 
title = {{mixup: Beyond Empirical Risk Minimization}}, 
author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David}, 
journal = {ICLR}, 
eprint = {1710.09412}, 
abstract = {{Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-mixup-%20Beyond%20Empirical%20Risk%20Minimization-2018-ICLR.pdf}
}
@article{CAMixupWenICLR2021, 
year = {2021}, 
title = {{Combining Ensembles and Data Augmentation can Harm your Calibration}}, 
author = {Wen, Yeming and Jerfel, Ghassen and Muller, Rafael and Dusenberry, Michael W and Snoek, Jasper and Lakshminarayanan, Balaji and Tran, Dustin}, 
journal = {ICLR}, 
eprint = {2010.09875}, 
abstract = {{Ensemble methods which average over multiple neural network predictions are a simple approach to improve a model's calibration and robustness. Similarly, data augmentation techniques, which encode prior information in the form of invariant feature transformations, are effective for improving calibration and robustness. In this paper, we show a surprising pathology: combining ensembles and data augmentation can harm model calibration. This leads to a trade-off in practice, whereby improved accuracy by combining the two techniques comes at the expense of calibration. On the other hand, selecting only one of the techniques ensures good uncertainty estimates at the expense of accuracy. We investigate this pathology and identify a compounding under-confidence among methods which marginalize over sets of weights and data augmentation techniques which soften labels. Finally, we propose a simple correction, achieving the best of both worlds with significant accuracy and calibration gains over using only ensembles or data augmentation individually. Applying the correction produces new state-of-the art in uncertainty calibration across CIFAR-10, CIFAR-100, and ImageNet.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wen-Combining%20Ensembles%20and%20Data%20Augmentation%20can%20Harm%20your%20Calibration-2021-ICLR.pdf}
}
@article{RevisitingMindererNeurIPS2021, 
year = {2021}, 
title = {{Revisiting the Calibration of Modern Neural Networks}}, 
author = {Minderer, Matthias and Djolonga, Josip and Romijnders, Rob and Hubis, Frances and Zhai, Xiaohua and Houlsby, Neil and Tran, Dustin and Lucic, Mario}, 
journal = {NeurIPS}, 
eprint = {2106.07998}, 
abstract = {{Accurate estimation of predictive uncertainty (model calibration) is essential for the safe application of neural networks. Many instances of miscalibration in modern neural networks have been reported, suggesting a trend that newer, more accurate models produce poorly calibrated predictions. Here, we revisit this question for recent state-of-the-art image classification models. We systematically relate model calibration and accuracy, and find that the most recent models, notably those not using convolutions, are among the best calibrated. Trends observed in prior model generations, such as decay of calibration with distribution shift or model size, are less pronounced in recent architectures. We also show that model size and amount of pretraining do not fully explain these differences, suggesting that architecture is a major determinant of calibration properties.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Minderer-Revisiting%20the%20Calibration%20of%20Modern%20Neural%20Networks-2021-NeurIPS.pdf}
}
@article{Tf-KDYuanCVPR2020, 
year = {2020}, 
title = {{Revisiting Knowledge Distillation via Label Smoothing Regularization}}, 
author = {Yuan, Li and Tay, Francis EH and Li, Guilin and Wang, Tao and Feng, Jiashi}, 
journal = {CVPR}, 
abstract = {{Knowledge Distillation (KD) aims to distill the knowledge of a cumbersome teacher model into a lightweight student model. Its success is generally attributed to the privileged information on similarities among categories provided by the teacher model, and in this sense, only strong teacher models are deployed to teach weaker students in practice. In this work, we challenge this common belief by following experimental observations: 1) beyond the acknowledgment that the teacher can improve the student, the student can also enhance the teacher significantly by reversing the KD procedure; 2) a poorly-trained teacher with much lower accuracy than the student can still improve the latter significantly. To explain these observations, we provide a theoretical analysis of the relationships between KD and label smoothing regularization. We prove that 1) KD is a type of learned label smoothing regularization and 2) label smoothing regularization provides a virtual teacher model for KD. From these results, we argue that the success of KD is not fully due to the similarity information between categories from teachers, but also to the regularization of soft targets, which is equally or even more important. Based on these analyses, we further propose a novel Teacher-free Knowledge Distillation (Tf-KD) framework, where a student model learns from itself or manually-designed regularization distribution. The Tf-KD achieves comparable performance with normal KD from a superior teacher, which is well applied when a stronger teacher model is unavailable. Meanwhile, Tf-KD is generic and can be directly deployed for training deep neural networks. Without any extra computation cost, Tf-KD achieves up to 0.65\% improvement on ImageNet over well-established baseline models, which is superior to label smoothing regularization.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yuan-Revisiting%20Knowledge%20Distillation%20via%20Label%20Smoothing%20Regularization-2020-CVPR.pdf}
}
@article{DoesLukasikICML2020, 
year = {2020}, 
title = {{Does label smoothing mitigate label noise?}}, 
author = {Lukasik, Michal and Bhojanapalli, Srinadh and Menon, Aditya Krishna and Kumar, Sanjiv}, 
journal = {ICML}, 
eprint = {2003.02819}, 
abstract = {{Label smoothing is commonly used in training deep learning models, wherein one-hot training labels are mixed with uniform label vectors. Empirically, smoothing has been shown to improve both predictive performance and model calibration. In this paper, we study whether label smoothing is also effective as a means of coping with label noise. While label smoothing apparently amplifies this problem --- being equivalent to injecting symmetric noise to the labels --- we show how it relates to a general family of loss-correction techniques from the label noise literature. Building on this connection, we show that label smoothing is competitive with loss-correction under label noise. Further, we show that when distilling models from noisy data, label smoothing of the teacher is beneficial; this is in contrast to recent findings for noise-free problems, and sheds further light on settings where label smoothing is beneficial.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lukasik-Does%20label%20smoothing%20mitigate%20label%20noise--2020-ICML.pdf}
}
@article{DoBaNeurIPS2014, 
year = {2014}, 
title = {{Do Deep Nets Really Need to be Deep?}}, 
author = {Ba, Lei Jimmy and Caruana, Rich}, 
journal = {NeurIPS}, 
eprint = {1312.6184}, 
abstract = {{Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this extended abstract, we show that shallow feed-forward networks can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, in some cases the shallow neural nets can learn these deep functions using a total number of parameters similar to the original deep model. We evaluate our method on the TIMIT phoneme recognition task and are able to train shallow fully-connected nets that perform similarly to complex, well-engineered, deep convolutional architectures. Our success in training shallow neural nets to mimic deeper models suggests that there probably exist better algorithms for training shallow feed-forward nets than those currently available.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ba-Do%20Deep%20Nets%20Really%20Need%20to%20be%20Deep--2014-NeurIPS.pdf}
}
@article{WiderorDeeperWuPR2019, 
year = {2019}, 
title = {{Wider or Deeper: Revisiting the ResNet Model for Visual Recognition}}, 
author = {Wu, Zifeng and Shen, Chunhua and Hengel, Anton van den}, 
journal = {PR}, 
abstract = {{The community has been going deeper and deeper in designing one cutting edge network after another, yet some works are there suggesting that we may have gone too far in this dimension. Some researchers unravelled a residual network into an exponentially wider one, and assorted the success of residual networks to fusing a large amount of relatively shallow models. Since some of their early claims are still not settled, we in this paper dig more on this topic, i.e., the unravelled view of residual networks. Based on that, we try to find a good compromise between the depth and width. Afterwards, we walk through a typical pipeline of developing a deep-learning-based algorithm. We start from a group of relatively shallow networks, which perform as well or even better than the current (much deeper) state-of-the-art models on the ImageNet classification dataset. Then, we initialize fully convolutional networks (FCNs) using our pre-trained models, and tune them for semantic image segmentation. Results show that the proposed networks, as pre-trained features, can boost existing methods a lot. Even without exhausting the sophistical techniques to improve the classic FCN model, we achieve comparable results with the best performers on four widely-used datasets, i.e., Cityscapes, PASCAL VOC, ADE20k and PASCAL-Context. The code and pre-trained models are released for public access 1 1 https://github.com/itijyou/ademxapp .}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wu-Wider%20or%20Deeper-%20Revisiting%20the%20ResNet%20Model%20for%20Visual%20Recognition-2019-PR.pdf}
}
@article{LearningLiICCV2017, 
year = {2017}, 
title = {{Learning from Noisy Labels with Distillation}}, 
author = {Li, Yuncheng and Yang, Jianchao and Song, Yale and Cao, Liangliang and Luo, Jiebo and Li, Li-Jia}, 
journal = {ICCV}, 
abstract = {{The ability of learning from noisy labels is very useful in many visual recognition tasks, as a vast amount of data with noisy labels are relatively easy to obtain. Traditionally, label noise has been treated as statistical outliers, and techniques such as importance re-weighting and bootstrapping have been proposed to alleviate the problem. According to our observation, the real-world noisy labels exhibit multimode characteristics as the true labels, rather than behaving like independent random outliers. In this work, we propose a unified distillation framework to use “side” information, including a small clean dataset and label relations in knowledge graph, to “hedge the risk” of learning from noisy labels. Unlike the traditional approaches evaluated based on simulated label noises, we propose a suite of new benchmark datasets, in Sports, Species and Artifacts domains, to evaluate the task of learning from noisy labels in the practical setting. The empirical study demonstrates the effectiveness of our proposed method in all the domains.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Learning%20from%20Noisy%20Labels%20with%20Distillation-2017-ICCV_1.pdf}
}
@article{ESKDChoICCV2019, 
year = {2019}, 
title = {{On the Efficacy of Knowledge Distillation}}, 
author = {Cho, Jang Hyun and Hariharan, Bharath}, 
journal = {ICCV}, 
abstract = {{In this paper, we present a thorough evaluation of the efficacy of knowledge distillation and its dependence on student and teacher architectures. Starting with the observation that more accurate teachers often don’t make good teachers, we attempt to tease apart the factors that affect knowledge distillation performance. We find crucially that larger models do not often make better teachers. We show that this is a consequence of mismatched capacity, and that small students are unable to mimic large teachers. We find typical ways of circumventing this (such as performing a sequence of knowledge distillation steps) to be ineffective. Finally, we show that this effect can be mitigated by stopping the teacher’s training early. Our results generalize across datasets and models.}}
}
@article{SPKDTungICCV2019, 
year = {2019}, 
title = {{Similarity-Preserving Knowledge Distillation}}, 
author = {Tung, Frederick and Mori, Greg}, 
journal = {ICCV}, 
abstract = {{Knowledge distillation is a widely applicable technique for training a student neural network under the guidance of a trained teacher network. For example, in neural network compression, a high-capacity teacher is distilled to train a compact student; in privileged learning, a teacher trained with privileged data is distilled to train a student without access to that data. The distillation loss determines how a teacher’s knowledge is captured and transferred to the student. In this paper, we propose a new form of knowledge distillation loss that is inspired by the observation that semantically similar inputs tend to elicit similar activation patterns in a trained network. Similarity-preserving knowledge distillation guides the training of a student network such that input pairs that produce similar (dissimilar) activations in the teacher network produce similar (dissimilar) activations in the student network. In contrast to previous distillation methods, the student is not required to mimic the representation space of the teacher, but rather to preserve the pairwise similarities in its own representation space. Experiments on three public datasets demonstrate the potential of our approach.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tung-Similarity-Preserving%20Knowledge%20Distillation-2019-ICCV.pdf}
}
@article{UnderstandingTangarXiv2020, 
year = {2020}, 
title = {{Understanding and Improving Knowledge Distillation}}, 
author = {Tang, Jiaxi and Shivanna, Rakesh and Zhao, Zhe and Lin, Dong and Singh, Anima and Chi, Ed H and Jain, Sagar}, 
journal = {arXiv}, 
eprint = {2002.03532}, 
abstract = {{Knowledge Distillation (KD) is a model-agnostic technique to improve model quality while having a fixed capacity budget. It is a commonly used technique for model compression, where a larger capacity teacher model with better quality is used to train a more compact student model with better inference efficiency. Through distillation, one hopes to benefit from student's compactness, without sacrificing too much on model quality. Despite the large success of knowledge distillation, better understanding of how it benefits student model's training dynamics remains under-explored. In this paper, we categorize teacher's knowledge into three hierarchical levels and study its effects on knowledge distillation: (1) knowledge of the `universe', where KD brings a regularization effect through label smoothing; (2) domain knowledge, where teacher injects class relationships prior to student's logit layer geometry; and (3) instance specific knowledge, where teacher rescales student model's per-instance gradients based on its measurement on the event difficulty. Using systematic analyses and extensive empirical studies on both synthetic and real-world datasets, we confirm that the aforementioned three factors play a major role in knowledge distillation. Furthermore, based on our findings, we diagnose some of the failure cases of applying KD from recent studies.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tang-Understanding%20and%20Improving%20Knowledge%20Distillation-2020-arXiv.pdf}
}
@article{RethinkingZhouICLR2021, 
year = {2021}, 
title = {{Rethinking Soft Labels for Knowledge Distillation: A Bias-Variance Tradeoff Perspective}}, 
author = {Zhou, Helong and Song, Liangchen and Chen, Jiajie and Zhou, Ye and Wang, Guoli and Yuan, Junsong and Zhang, Qian}, 
journal = {ICLR}, 
eprint = {2102.00650}, 
abstract = {{Knowledge distillation is an effective approach to leverage a well-trained network or an ensemble of them, named as the teacher, to guide the training of a student network. The outputs from the teacher network are used as soft labels for supervising the training of a new network. Recent studies \textbackslashcitep\{muller2019does,yuan2020revisiting\} revealed an intriguing property of the soft labels that making labels soft serves as a good regularization to the student network. From the perspective of statistical learning, regularization aims to reduce the variance, however how bias and variance change is not clear for training with soft labels. In this paper, we investigate the bias-variance tradeoff brought by distillation with soft labels. Specifically, we observe that during training the bias-variance tradeoff varies sample-wisely. Further, under the same distillation temperature setting, we observe that the distillation performance is negatively associated with the number of some specific samples, which are named as regularization samples since these samples lead to bias increasing and variance decreasing. Nevertheless, we empirically find that completely filtering out regularization samples also deteriorates distillation performance. Our discoveries inspired us to propose the novel weighted soft labels to help the network adaptively handle the sample-wise bias-variance tradeoff. Experiments on standard evaluation benchmarks validate the effectiveness of our method. Our code is available at \textbackslashurl\{https://github.com/bellymonster/Weighted-Soft-Label-Distillation\}.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhou-Rethinking%20Soft%20Labels%20for%20Knowledge%20Distillation-%20A%20Bias-Variance%20Tradeoff%20Perspective-2021-ICLR.pdf}
}
@article{FasterR-CNNRenNeurIPS2015, 
year = {2015}, 
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}}, 
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian}, 
journal = {NeurIPS}, 
eprint = {1506.01497}, 
abstract = {{State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2\% mAP) and 2012 (70.4\% mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster\_rcnn.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ren-Faster%20R-CNN-%20Towards%20Real-Time%20Object%20Detection%20with%20Region%20Proposal%20Networks-2015-NeurIPS.pdf}
}
@article{FPNLinCVPR2017, 
year = {2017}, 
title = {{Feature Pyramid Networks for Object Detection}}, 
author = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge}, 
journal = {CVPR}, 
abstract = {{Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lin-Feature%20Pyramid%20Networks%20for%20Object%20Detection-2017-CVPR.pdf}
}
@article{MaskR-CNNHeICCV2017, 
year = {2017}, 
title = {{Mask R-CNN}}, 
author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross}, 
journal = {ICCV}, 
abstract = {{We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/He-Mask%20R-CNN-2017-ICCV.pdf}
}
@article{InstanceNevenCVPR2019, 
year = {2019}, 
title = {{Instance Segmentation by Jointly Optimizing Spatial Embeddings and Clustering Bandwidth}}, 
author = {Neven, Davy and Brabandere, Bert De and Proesmans, Marc and Gool, Luc Van}, 
journal = {CVPR}, 
abstract = {{Current state-of-the-art instance segmentation methods are not suited for real-time applications like autonomous driving, which require fast execution times at high accuracy. Although the currently dominant proposal-based methods have high accuracy, they are slow and generate masks at a fixed and low resolution. Proposal-free methods, by contrast, can generate masks at high resolution and are often faster, but fail to reach the same accuracy as the proposal-based methods. In this work we propose a new clustering loss function for proposal-free instance segmentation. The loss function pulls the spatial embeddings of pixels belonging to the same instance together and jointly learns an instance-specific clustering bandwidth, maximizing the intersection-over-union of the resulting instance mask. When combined with a fast architecture, the network can perform instance segmentation in real-time while maintaining a high accuracy. We evaluate our method on the challenging Cityscapes benchmark and achieve top results (5\% improvement over Mask R-CNN) at more than 10 fps on 2MP images.1}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Neven-Instance%20Segmentation%20by%20Jointly%20Optimizing%20Spatial%20Embeddings%20and%20Clustering%20Bandwidth-2019-CVPR.pdf}
}
@article{HowSanturkarNeurIPS2018, 
year = {2018}, 
title = {{How Does Batch Normalization Help Optimization?}}, 
author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander}, 
journal = {NeurIPS}, 
eprint = {1805.11604}, 
abstract = {{Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called "internal covariate shift". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Santurkar-How%20Does%20Batch%20Normalization%20Help%20Optimization--2018-NeurIPS_1.pdf}
}
@article{UPerNetXiaoECCV2018, 
year = {2018}, 
title = {{Unified Perceptual Parsing for Scene Understanding}}, 
author = {Xiao, Tete and Liu, Yingcheng and Zhou, Bolei and Jiang, Yuning and Sun, Jian}, 
journal = {ECCV}, 
eprint = {1807.10221}, 
abstract = {{Humans recognize the visual world at multiple levels: we effortlessly categorize scenes and detect objects inside, while also identifying the textures and surfaces of the objects along with their different compositional parts. In this paper, we study a new task called Unified Perceptual Parsing, which requires the machine vision systems to recognize as many visual concepts as possible from a given image. A multi-task framework called UPerNet and a training strategy are developed to learn from heterogeneous image annotations. We benchmark our framework on Unified Perceptual Parsing and show that it is able to effectively segment a wide range of concepts from images. The trained networks are further applied to discover visual knowledge in natural scenes. Models are available at \textbackslashurl\{https://github.com/CSAILVision/unifiedparsing\}.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xiao-Unified%20Perceptual%20Parsing%20for%20Scene%20Understanding-2018-ECCV.pdf}
}
@article{SOLOWangECCV2020, 
year = {2020}, 
title = {{SOLO: Segmenting Objects by Locations}}, 
author = {Wang, Xinlong and Kong, Tao and Shen, Chunhua and Jiang, Yuning and Li, Lei}, 
journal = {ECCV}, 
eprint = {1912.04488}, 
abstract = {{We present a new, embarrassingly simple approach to instance segmentation in images. Compared to many other dense prediction tasks, e.g., semantic segmentation, it is the arbitrary number of instances that have made instance segmentation much more challenging. In order to predict a mask for each instance, mainstream approaches either follow the 'detect-thensegment' strategy as used by Mask R-CNN, or predict category masks first then use clustering techniques to group pixels into individual instances. We view the task of instance segmentation from a completely new perspective by introducing the notion of "instance categories", which assigns categories to each pixel within an instance according to the instance's location and size, thus nicely converting instance mask segmentation into a classification-solvable problem. Now instance segmentation is decomposed into two classification tasks. We demonstrate a much simpler and flexible instance segmentation framework with strong performance, achieving on par accuracy with Mask R-CNN and outperforming recent singleshot instance segmenters in accuracy. We hope that this very simple and strong framework can serve as a baseline for many instance-level recognition tasks besides instance segmentation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-SOLO-%20Segmenting%20Objects%20by%20Locations-2020-ECCV.pdf}
}
@article{DeiTTouvronICML2021, 
year = {2021}, 
title = {{Training data-efficient image transformers \& distillation through attention}}, 
author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jégou, Hervé}, 
journal = {ICML}, 
eprint = {2012.12877}, 
abstract = {{Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1\% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2\% accuracy) and when transferring to other tasks. We share our code and models.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Touvron-Training%20data-efficient%20image%20transformers%20&%20distillation%20through%20attention-2021-ICML.pdf}
}
@article{LONDONShangICCV2021, 
year = {2021}, 
title = {{Lipschitz Continuity Guided Knowledge Distillation}}, 
author = {Shang, Yuzhang and Duan, Bin and Zong, Ziliang and Nie, Liqiang and Yan, Yan}, 
journal = {ICCV}, 
eprint = {2108.12905}, 
abstract = {{Knowledge distillation has become one of the most important model compression techniques by distilling knowledge from larger teacher networks to smaller student ones. Although great success has been achieved by prior distillation methods via delicately designing various types of knowledge, they overlook the functional properties of neural networks, which makes the process of applying those techniques to new tasks unreliable and non-trivial. To alleviate such problem, in this paper, we initially leverage Lipschitz continuity to better represent the functional characteristic of neural networks and guide the knowledge distillation process. In particular, we propose a novel Lipschitz Continuity Guided Knowledge Distillation framework to faithfully distill knowledge by minimizing the distance between two neural networks' Lipschitz constants, which enables teacher networks to better regularize student networks and improve the corresponding performance. We derive an explainable approximation algorithm with an explicit theoretical derivation to address the NP-hard problem of calculating the Lipschitz constant. Experimental results have shown that our method outperforms other benchmarks over several knowledge distillation tasks (e.g., classification, segmentation and object detection) on CIFAR-100, ImageNet, and PASCAL VOC datasets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Shang-Lipschitz%20Continuity%20Guided%20Knowledge%20Distillation-2021-ICCV.pdf}
}
@article{OFDHeoICCV2019, 
year = {2019}, 
title = {{A Comprehensive Overhaul of Feature Distillation}}, 
author = {Heo, Byeongho and Kim, Jeesoo and Yun, Sangdoo and Park, Hyojin and Kwak, Nojun and Choi, Jin Young}, 
journal = {ICCV}, 
abstract = {{We investigate the design aspects of feature distillation methods achieving network compression and propose a novel feature distillation method in which the distillation loss is designed to make a synergy among various aspects: teacher transform, student transform, distillation feature position and distance function. Our proposed distillation loss includes a feature transform with a newly designed margin ReLU, a new distillation feature position, and a partial L2 distance function to skip redundant information giving adverse effects to the compression of student. In ImageNet, our proposed method achieves 21.65\% of top-l error with ResNet50, which outperforms the performance of the teacher network, ResNet152. Our proposed method is evaluated on various tasks such as image classification, object detection and semantic segmentation and achieves a significant performance improvement in all tasks. The code is available at bhheo.github.ioloverhaul}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Heo-A%20Comprehensive%20Overhaul%20of%20Feature%20Distillation-2019-ICCV.pdf}
}
@article{BatchRenormalizationIoffeNeurIPS2017, 
year = {2017}, 
title = {{Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models}}, 
author = {Ioffe, Sergey}, 
journal = {NeurIPS}, 
eprint = {1702.03275}, 
abstract = {{Batch Normalization is quite effective at accelerating and improving the training of deep models. However, its effectiveness diminishes when the training minibatches are small, or do not consist of independent samples. We hypothesize that this is due to the dependence of model layer inputs on all the examples in the minibatch, and different activations being produced between training and inference. We propose Batch Renormalization, a simple and effective extension to ensure that the training and inference models generate the same outputs that depend on individual examples rather than the entire minibatch. Models trained with Batch Renormalization perform substantially better than batchnorm when training with small or non-i.i.d. minibatches. At the same time, Batch Renormalization retains the benefits of batchnorm such as insensitivity to initialization and training efficiency.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ioffe-Batch%20Renormalization-%20Towards%20Reducing%20Minibatch%20Dependence%20in%20Batch-Normalized%20Models-2017-NeurIPS.pdf}
}
@article{MSEKimIJCAI2021, 
year = {2021}, 
title = {{Comparing Kullback-Leibler Divergence and Mean Squared Error Loss in Knowledge Distillation}}, 
author = {Kim, Taehyeon and Oh, Jaehoon and Kim, Nak Yil and Cho, Sangwook and Yun, Se-Young}, 
journal = {IJCAI}, 
abstract = {{Knowledge distillation (KD), transferring knowledge from a cumbersome teacher model to a lightweight student model, has been investigated to design efficient neural architectures. Generally, the objective function of KD is the Kullback-Leibler (KL) divergence loss between the softened probability distributions of the teacher model and the student model with the temperature scaling hyperparameter τ. Despite its widespread use, few studies have discussed how such softening influences generalization. Here, we theoretically show that the KL divergence loss focuses on the logit matching when τ increases and the label matching when τ goes to 0 and empirically show that the logit matching is positively correlated to performance improvement in general. From this observation, we consider an intuitive KD loss function, the mean squared error (MSE) between the logit vectors, so that the student model can directly learn the logit of the teacher model. The MSE loss outperforms the KL divergence loss, explained by the penultimate layer representations difference between the two losses. Furthermore, we show that sequential distillation can improve performance and that KD, using the KL divergence loss with small τ particularly, mitigates the label noise. The code to reproduce the experiments is publicly available online at https://github.com/jhoon-oh/kd\_data/.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kim-Comparing%20Kullback-Leibler%20Divergence%20and%20Mean%20Squared%20Error%20Loss%20in%20Knowledge%20Distillation-2021-IJCAI.pdf}
}
@article{LearningHanNeurIPS2015, 
year = {2015}, 
title = {{Learning both Weights and Connections for Efficient Neural Networks}}, 
author = {Han, Song and Pool, Jeff and Tran, John and Dally, William J}, 
journal = {NeurIPS}, 
eprint = {1506.02626}, 
abstract = {{Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Han-Learning%20both%20Weights%20and%20Connections%20for%20Efficient%20Neural%20Networks-2015-NeurIPS.pdf}
}
@article{Meta-CalMaICML2021, 
year = {2021}, 
title = {{Meta-Cal: Well-controlled Post-hoc Calibration by Ranking}}, 
author = {Ma, Xingchen and Blaschko, Matthew B}, 
journal = {ICML}, 
eprint = {2105.04290}, 
abstract = {{In many applications, it is desirable that a classifier not only makes accurate predictions, but also outputs calibrated posterior probabilities. However, many existing classifiers, especially deep neural network classifiers, tend to be uncalibrated. Post-hoc calibration is a technique to recalibrate a model by learning a calibration map. Existing approaches mostly focus on constructing calibration maps with low calibration errors, however, this quality is inadequate for a calibrator being useful. In this paper, we introduce two constraints that are worth consideration in designing a calibration map for post-hoc calibration. Then we present Meta-Cal, which is built from a base calibrator and a ranking model. Under some mild assumptions, two high-probability bounds are given with respect to these constraints. Empirical results on CIFAR-10, CIFAR-100 and ImageNet and a range of popular network architectures show our proposed method significantly outperforms the current state of the art for post-hoc multi-class classification calibration.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ma-Meta-Cal-%20Well-controlled%20Post-hoc%20Calibration%20by%20Ranking-2021-ICML.pdf}
}
@article{BEITBaoICLR2021, 
year = {2021}, 
title = {{BEiT: BERT Pre-Training of Image Transformers}}, 
author = {Bao, Hangbo and Dong, Li and Wei, Furu}, 
journal = {ICLR}, 
eprint = {2106.08254}, 
abstract = {{We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first "tokenize" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2\% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8\%) with the same setup. Moreover, large-size BEiT obtains 86.3\% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2\%). The code and pretrained models are available at https://aka.ms/beit.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bao-BEiT-%20BERT%20Pre-Training%20of%20Image%20Transformers-2021-ICLR.pdf}
}
@article{EndXuICCV2021, 
year = {2021}, 
title = {{End-to-End Semi-Supervised Object Detection with Soft Teacher}}, 
author = {Xu, Mengde and Zhang, Zheng and Hu, Han and Wang, Jianfeng and Wang, Lijuan and Wei, Fangyun and Bai, Xiang and Liu, Zicheng}, 
journal = {ICCV}, 
eprint = {2106.09018}, 
abstract = {{This paper presents an end-to-end semi-supervised object detection approach, in contrast to previous more complex multi-stage methods. The end-to-end training gradually improves pseudo label qualities during the curriculum, and the more and more accurate pseudo labels in turn benefit object detection training. We also propose two simple yet effective techniques within this framework: a soft teacher mechanism where the classification loss of each unlabeled bounding box is weighed by the classification score produced by the teacher network; a box jittering approach to select reliable pseudo boxes for the learning of box regression. On the COCO benchmark, the proposed approach outperforms previous methods by a large margin under various labeling ratios, i.e. 1\textbackslash\%, 5\textbackslash\% and 10\textbackslash\%. Moreover, our approach proves to perform also well when the amount of labeled data is relatively large. For example, it can improve a 40.9 mAP baseline detector trained using the full COCO training set by +3.6 mAP, reaching 44.5 mAP, by leveraging the 123K unlabeled images of COCO. On the state-of-the-art Swin Transformer based object detector (58.9 mAP on test-dev), it can still significantly improve the detection accuracy by +1.5 mAP, reaching 60.4 mAP, and improve the instance segmentation accuracy by +1.2 mAP, reaching 52.4 mAP. Further incorporating with the Object365 pre-trained model, the detection accuracy reaches 61.3 mAP and the instance segmentation accuracy reaches 53.0 mAP, pushing the new state-of-the-art.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xu-End-to-End%20Semi-Supervised%20Object%20Detection%20with%20Soft%20Teacher-2021-ICCV.pdf}
}
@article{ANNZhuICCV2019, 
year = {2019}, 
title = {{Asymmetric Non-local Neural Networks for Semantic Segmentation}}, 
author = {Zhul, Zhen and Xu, Mengde and Bai, Song and Huang, Tengteng and Bai, Xiang}, 
journal = {ICCV}, 
abstract = {{The non-local module works as a particularly useful technique for semantic segmentation while criticized for its prohibitive computation and GPU memory occupation. In this paper, we present Asymmetric Non-local Neural Network to semantic segmentation, which has two prominent components: Asymmetric Pyramid Non-local Block (APNB) and Asymmetric Fusion Non-local Block (AFNB). APNB leverages a pyramid sampling module into the nonlocal block to largely reduce the computation and memory consumption without sacrificing the performance. AFNB is adapted from APNB to fuse the features of different levels under a sufficient consideration of long range dependencies and thus considerably improves the performance. Extensive experiments on semantic segmentation benchmarks demonstrate the effectiveness and efficiency of our work. In particular, we report the state-of-the-art performance of 81.3 mIoU on the Cityscapes test set. For a 256 × 128 input, APNB is around 6 times faster than a non-local block on GPU while 28 times smaller in GPU running memory occupation. Code is available at: https://github.com/MendelXu/ANN.git.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhul-Asymmetric%20Non-local%20Neural%20Networks%20for%20Semantic%20Segmentation-2019-ICCV.pdf}
}
@article{ResNeXtXieCVPR2017, 
year = {2017}, 
title = {{Aggregated Residual Transformations for Deep Neural Networks}}, 
author = {Xie, Saining and Girshick, Ross and Dollár, Piotr and Tu, Zhuowen and He, Kaiming}, 
journal = {CVPR}, 
abstract = {{We present a simple, highly modularized network archi tecture for image classification. Our network is constructed by repeating a building block that aggregates a set of trans formations with the same topology. Our simple design re sults in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call “cardinality” (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, in creasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online11https://github.com/facebookresearch/ResNeXt. https://github.com/facebookresearch/ResNeXt}}
}
@article{PASCALVOCEveringhamIJCV2009, 
year = {2009}, 
title = {{The Pascal Visual Object Classes (VOC) Challenge}}, 
author = {Everingham, Mark and Gool, Luc Van and Williams, Christopher K I and Winn, John and Zisserman, Andrew}, 
journal = {IJCV}, 
abstract = {{The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Everingham-The%20Pascal%20Visual%20Object%20Classes%20(VOC)%20Challenge-2009-IJCV.pdf}
}
@article{SBDHariharanICCV2011, 
year = {2011}, 
title = {{Semantic contours from inverse detectors}}, 
author = {Hariharan, Bharath and Arbelaez, Pablo and Bourdev, Lubomir and Maji, Subhransu and Malik, Jitendra}, 
journal = {ICCV}, 
abstract = {{We study the challenging problem of localizing and classifying category-specific object contours in real world images. For this purpose, we present a simple yet effective method for combining generic object detectors with bottom-up contours to identify object contours. We also provide a principled way of combining information from different part detectors and across categories. In order to study the problem and evaluate quantitatively our approach, we present a dataset of semantic exterior boundaries on more than 20,000 object instances belonging to 20 categories, using the images from the VOC2011 PASCAL challenge [7].}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hariharan-Semantic%20contours%20from%20inverse%20detectors-2011-ICCV.pdf}
}
@article{iCaRLRebuffiCVPR2017, 
year = {2017}, 
title = {{iCaRL: Incremental Classifier and Representation Learning}}, 
author = {Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H}, 
journal = {CVPR}, 
abstract = {{A major open problem on the road to artificial intelligence is the development of incrementally learning systems that learn about more and more concepts over time from a stream of data. In this work, we introduce a new training strategy, iCaRL, that allows learning in such a class-incremental way: only the training data for a small number of classes has to be present at the same time and new classes can be added progressively. iCaRL learns strong classifiers and a data representation simultaneously. This distinguishes it from earlier works that were fundamentally limited to fixed data representations and therefore incompatible with deep learning architectures. We show by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can learn many classes incrementally over a long period of time where other strategies quickly fail.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Rebuffi-iCaRL-%20Incremental%20Classifier%20and%20Representation%20Learning-2017-CVPR.pdf}
}
@article{ConfidenceGongICCV2021, 
year = {2021}, 
title = {{Confidence Calibration for Domain Generalization under Covariate Shift}}, 
author = {Gong, Yunye and Lin, Xiao and Yao, Yi and Dietterich, Thomas G and Divakaran, Ajay and Gervasio, Melinda}, 
journal = {ICCV}, 
eprint = {2104.00742}, 
abstract = {{Existing calibration algorithms address the problem of covariate shift via unsupervised domain adaptation. However, these methods suffer from the following limitations: 1) they require unlabeled data from the target domain, which may not be available at the stage of calibration in real-world applications and 2) their performance depends heavily on the disparity between the distributions of the source and target domains. To address these two limitations, we present novel calibration solutions via domain generalization. Our core idea is to leverage multiple calibration domains to reduce the effective distribution disparity between the target and calibration domains for improved calibration transfer without needing any data from the target domain. We provide theoretical justification and empirical experimental results to demonstrate the effectiveness of our proposed algorithms. Compared against state-of-the-art calibration methods designed for domain adaptation, we observe a decrease of 8.86 percentage points in expected calibration error or, equivalently, an increase of 35 percentage points in improvement ratio for multi-class classification on the Office-Home dataset.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Gong-Confidence%20Calibration%20for%20Domain%20Generalization%20under%20Covariate%20Shift-2021-ICCV.pdf}
}
@article{Pix2SeqChenICLR2022, 
year = {2022}, 
title = {{Pix2seq: A Language Modeling Framework for Object Detection}}, 
author = {Chen, Ting and Saxena, Saurabh and Li, Lala and Fleet, David J and Hinton, Geoffrey}, 
journal = {ICLR}, 
eprint = {2109.10852}, 
abstract = {{This paper presents Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, we simply cast object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural net to perceive the image and generate the desired sequence. Our approach is based mainly on the intuition that if a neural net knows about where and what the objects are, we just need to teach it how to read them out. Beyond the use of task-specific data augmentations, our approach makes minimal assumptions about the task, yet it achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Pix2seq-%20A%20Language%20Modeling%20Framework%20for%20Object%20Detection-2022-ICLR.pdf}
}
@article{DeepAbadiCCS2016, 
year = {2016}, 
title = {{Deep Learning with Differential Privacy}}, 
author = {Abadi, Martin and Weippl, Edgar and Katzenbeisser, Stefan and Kruegel, Christopher and Myers, Andrew and Halevi, Shai and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li}, 
journal = {CCS}, 
eprint = {1607.00133}, 
abstract = {{Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Abadi-Deep%20Learning%20with%20Differential%20Privacy-2016-CCS.pdf}
}
@article{MINILMWangNeurIPS2020, 
year = {2020}, 
title = {{MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers}}, 
author = {Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming}, 
journal = {NeurIPS}, 
eprint = {2002.10957}, 
abstract = {{Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99\% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50\% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-MINILM-%20Deep%20Self-Attention%20Distillation%20for%20Task-Agnostic%20Compression%20of%20Pre-Trained%20Transformers-2020-NeurIPS.pdf}
}
@article{LiteTransformerWuICLR2020, 
year = {2020}, 
title = {{Lite Transformer with Long-Short Range Attention}}, 
author = {Wu, Zhanghao and Liu, Zhijian and Lin, Ji and Lin, Yujun and Han, Song}, 
journal = {ICLR}, 
eprint = {2004.11886}, 
abstract = {{Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT'14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years. Code has been made available at https://github.com/mit-han-lab/lite-transformer.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wu-Lite%20Transformer%20with%20Long-Short%20Range%20Attention-2020-ICLR.pdf}
}
@article{TowardsPhuongICML2019, 
year = {2019}, 
title = {{Towards Understanding Knowledge Distillation}}, 
author = {Phuong, Mary and Lampert, Christoph H}, 
journal = {ICML}, 
eprint = {2105.13093}, 
abstract = {{Knowledge distillation, i.e., one classifier being trained on the outputs of another classifier, is an empirically very successful technique for knowledge transfer between classifiers. It has even been observed that classifiers learn much faster and more reliably if trained with the outputs of another classifier as soft labels, instead of from ground truth data. So far, however, there is no satisfactory theoretical explanation of this phenomenon. In this work, we provide the first insights into the working mechanisms of distillation by studying the special case of linear and deep linear classifiers. Specifically, we prove a generalization bound that establishes fast convergence of the expected risk of a distillation-trained linear classifier. From the bound and its proof we extract three key factors that determine the success of distillation: * data geometry -- geometric properties of the data distribution, in particular class separation, has a direct influence on the convergence speed of the risk; * optimization bias -- gradient descent optimization finds a very favorable minimum of the distillation objective; and * strong monotonicity -- the expected risk of the student classifier always decreases when the size of the training set grows.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Phuong-Towards%20Understanding%20Knowledge%20Distillation-2019-ICML.pdf}
}
@article{FromTsiprasICML2020, 
year = {2020}, 
title = {{From ImageNet to Image Classification: Contextualizing Progress on Benchmarks}}, 
author = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Ilyas, Andrew and Madry, Aleksander}, 
journal = {ICML}, 
eprint = {2005.11295}, 
abstract = {{Building rich machine learning datasets in a scalable manner often necessitates a crowd-sourced data collection pipeline. In this work, we use human studies to investigate the consequences of employing such a pipeline, focusing on the popular ImageNet dataset. We study how specific design choices in the ImageNet creation process impact the fidelity of the resulting dataset---including the introduction of biases that state-of-the-art models exploit. Our analysis pinpoints how a noisy data collection pipeline can lead to a systematic misalignment between the resulting benchmark and the real-world task it serves as a proxy for. Finally, our findings emphasize the need to augment our current model training and evaluation toolkit to take such misalignments into account. To facilitate further research, we release our refined ImageNet annotations at https://github.com/MadryLab/ImageNetMultiLabel.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tsipras-From%20ImageNet%20to%20Image%20Classification-%20Contextualizing%20Progress%20on%20Benchmarks-2020-ICML.pdf}
}
@article{BetaDARTSYeCVPR2022, 
year = {2022}, 
title = {{\$\textbackslashbeta\$-DARTS: Beta-Decay Regularization for Differentiable Architecture Search}}, 
author = {Ye, Peng and Li, Baopu and Li, Yikang and Chen, Tao and Fan, Jiayuan and Ouyang, Wanli}, 
journal = {CVPR}, 
eprint = {2203.01665}, 
abstract = {{Neural Architecture Search\textbackslashtextasciitilde(NAS) has attracted increasingly more attention in recent years because of its capability to design deep neural networks automatically. Among them, differential NAS approaches such as DARTS, have gained popularity for the search efficiency. However, they suffer from two main issues, the weak robustness to the performance collapse and the poor generalization ability of the searched architectures. To solve these two problems, a simple-but-efficient regularization method, termed as Beta-Decay, is proposed to regularize the DARTS-based NAS searching process. Specifically, Beta-Decay regularization can impose constraints to keep the value and variance of activated architecture parameters from too large. Furthermore, we provide in-depth theoretical analysis on how it works and why it works. Experimental results on NAS-Bench-201 show that our proposed method can help to stabilize the searching process and makes the searched network more transferable across different datasets. In addition, our search scheme shows an outstanding property of being less dependent on training time and data. Comprehensive experiments on a variety of search spaces and datasets validate the effectiveness of the proposed method.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ye-$-beta$-DARTS-%20Beta-Decay%20Regularization%20for%20Differentiable%20Architecture%20Search-2022-CVPR.pdf}
}
@article{NCPDingICLR2022, 
year = {2022}, 
title = {{Learning Versatile Neural Architectures by Propagating Network Codes}}, 
author = {Ding, Mingyu and Huo, Yuqi and Lu, Haoyu and Yang, Linjie and Wang, Zhe and Lu, Zhiwu and Wang, Jingdong and Luo, Ping}, 
journal = {ICLR}, 
eprint = {2103.13253}, 
abstract = {{This work explores how to design a single neural network capable of adapting to multiple heterogeneous vision tasks, such as image segmentation, 3D detection, and video recognition. This goal is challenging because both network architecture search (NAS) spaces and methods in different tasks are inconsistent. We solve this challenge from both sides. We first introduce a unified design space for multiple tasks and build a multitask NAS benchmark (NAS-Bench-MR) on many widely used datasets, including ImageNet, Cityscapes, KITTI, and HMDB51. We further propose Network Coding Propagation (NCP), which back-propagates gradients of neural predictors to directly update architecture codes along the desired gradient directions to solve various tasks. In this way, optimal architecture configurations can be found by NCP in our large search space in seconds. Unlike prior arts of NAS that typically focus on a single task, NCP has several unique benefits. (1) NCP transforms architecture optimization from data-driven to architecture-driven, enabling joint search an architecture among multitasks with different data distributions. (2) NCP learns from network codes but not original data, enabling it to update the architecture efficiently across datasets. (3) In addition to our NAS-Bench-MR, NCP performs well on other NAS benchmarks, such as NAS-Bench-201. (4) Thorough studies of NCP on inter-, cross-, and intra-tasks highlight the importance of cross-task neural architecture design, i.e., multitask neural architectures and architecture transferring between different tasks. Code is available at https://github.com/dingmyu/NCP.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ding-Learning%20Versatile%20Neural%20Architectures%20by%20Propagating%20Network%20Codes-2022-ICLR.pdf}
}
@article{OnWanICLR2022, 
year = {2022}, 
title = {{On Redundancy and Diversity in Cell-based Neural Architecture Search}}, 
author = {Wan, Xingchen and Ru, Binxin and Esperança, Pedro M and Li, Zhenguo}, 
journal = {ICLR}, 
eprint = {2203.08887}, 
abstract = {{Searching for the architecture cells is a dominant paradigm in NAS. However, little attention has been devoted to the analysis of the cell-based search spaces even though it is highly important for the continual development of NAS. In this work, we conduct an empirical post-hoc analysis of architectures from the popular cell-based search spaces and find that the existing search spaces contain a high degree of redundancy: the architecture performance is minimally sensitive to changes at large parts of the cells, and universally adopted designs, like the explicit search for a reduction cell, significantly increase the complexities but have very limited impact on the performance. Across architectures found by a diverse set of search strategies, we consistently find that the parts of the cells that do matter for architecture performance often follow similar and simple patterns. By explicitly constraining cells to include these patterns, randomly sampled architectures can match or even outperform the state of the art. These findings cast doubts into our ability to discover truly novel architectures in the existing cell-based search spaces, and inspire our suggestions for improvement to guide future NAS research. Code is available at https://github.com/xingchenwan/cell-based-NAS-analysis.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wan-On%20Redundancy%20and%20Diversity%20in%20Cell-based%20Neural%20Architecture%20Search-2022-ICLR.pdf}
}
@article{YooLearningCVPR2019, 
year = {2019}, 
title = {{Learning Loss for Active Learning}}, 
author = {Yoo, Donggeun and Kweon, In So}, 
journal = {CVPR}, 
abstract = {{The performance of deep neural networks improves with more annotated data. The problem is that the budget for annotation is limited. One solution to this is active learning, where a model asks human to annotate data that it perceived as uncertain. A variety of recent methods have been proposed to apply active learning to deep networks but most of them are either designed specific for their target tasks or computationally inefficient for large networks. In this paper, we propose a novel active learning method that is simple but task-agnostic, and works efficiently with the deep networks. We attach a small parametric module, named "loss prediction module," to a target network, and learn it to predict target losses of unlabeled inputs. Then, this module can suggest data that the target model is likely to produce a wrong prediction. This method is task-agnostic as networks are learned from a single loss regardless of target tasks. We rigorously validate our method through image classification, object detection, and human pose estimation, with the recent network architectures. The results demonstrate that our method consistently outperforms the previous methods over the tasks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yoo-Learning%20Loss%20for%20Active%20Learning-2019-CVPR.pdf}
}
@article{DIRYangICML2021, 
year = {2021}, 
title = {{Delving into Deep Imbalanced Regression}}, 
author = {Yang, Yuzhe and Zha, Kaiwen and Chen, Ying-Cong and Wang, Hao and Katabi, Dina}, 
journal = {ICML}, 
eprint = {2102.09554}, 
abstract = {{Real-world data often exhibit imbalanced distributions, where certain target values have significantly fewer observations. Existing techniques for dealing with imbalanced data focus on targets with categorical indices, i.e., different classes. However, many tasks involve continuous targets, where hard boundaries between classes do not exist. We define Deep Imbalanced Regression (DIR) as learning from such imbalanced data with continuous targets, dealing with potential missing data for certain target values, and generalizing to the entire target range. Motivated by the intrinsic difference between categorical and continuous label space, we propose distribution smoothing for both labels and features, which explicitly acknowledges the effects of nearby targets, and calibrates both label and learned feature distributions. We curate and benchmark large-scale DIR datasets from common real-world tasks in computer vision, natural language processing, and healthcare domains. Extensive experiments verify the superior performance of our strategies. Our work fills the gap in benchmarks and techniques for practical imbalanced regression problems. Code and data are available at https://github.com/YyzHarry/imbalanced-regression.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-Delving%20into%20Deep%20Imbalanced%20Regression-2021-ICML.pdf}
}
@article{BalancedMSERenCVPR2022, 
year = {2022}, 
title = {{Balanced MSE for Imbalanced Visual Regression}}, 
author = {Ren, Jiawei and Zhang, Mingyuan and Yu, Cunjun and Liu, Ziwei}, 
journal = {CVPR}, 
eprint = {2203.16427}, 
abstract = {{Data imbalance exists ubiquitously in real-world visual regressions, e.g., age estimation and pose estimation, hurting the model's generalizability and fairness. Thus, imbalanced regression gains increasing research attention recently. Compared to imbalanced classification, imbalanced regression focuses on continuous labels, which can be boundless and high-dimensional and hence more challenging. In this work, we identify that the widely used Mean Square Error (MSE) loss function can be ineffective in imbalanced regression. We revisit MSE from a statistical view and propose a novel loss function, Balanced MSE, to accommodate the imbalanced training label distribution. We further design multiple implementations of Balanced MSE to tackle different real-world scenarios, particularly including the one that requires no prior knowledge about the training label distribution. Moreover, to the best of our knowledge, Balanced MSE is the first general solution to high-dimensional imbalanced regression. Extensive experiments on both synthetic and three real-world benchmarks demonstrate the effectiveness of Balanced MSE.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ren-Balanced%20MSE%20for%20Imbalanced%20Visual%20Regression-2022-CVPR.pdf}
}
@article{WhatByrdICML2019, 
year = {2019}, 
title = {{What is the Effect of Importance Weighting in Deep Learning?}}, 
author = {Byrd, Jonathon and Lipton, Zachary C}, 
journal = {ICML}, 
eprint = {1812.03372}, 
abstract = {{Importance-weighted risk minimization is a key ingredient in many machine learning algorithms for causal inference, domain adaptation, class imbalance, and off-policy reinforcement learning. While the effect of importance weighting is well-characterized for low-capacity misspecified models, little is known about how it impacts over-parameterized, deep neural networks. This work is inspired by recent theoretical results showing that on (linearly) separable data, deep linear networks optimized by SGD learn weight-agnostic solutions, prompting us to ask, for realistic deep networks, for which many practical datasets are separable, what is the effect of importance weighting? We present the surprising finding that while importance weighting impacts models early in training, its effect diminishes over successive epochs. Moreover, while L2 regularization and batch normalization (but not dropout), restore some of the impact of importance weighting, they express the effect via (seemingly) the wrong abstraction: why should practitioners tweak the L2 regularization, and by how much, to produce the correct weighting effect? Our experiments confirm these findings across a range of architectures and datasets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Byrd-What%20is%20the%20Effect%20of%20Importance%20Weighting%20in%20Deep%20Learning--2019-ICML.pdf}
}
@article{RethinkingHeICCV2019, 
year = {2019}, 
title = {{Rethinking ImageNet Pre-training}}, 
author = {He, Kaiming and Girshick, Ross and Dollár, Piotr}, 
journal = {ICCV}, 
abstract = {{We report competitive results on object detection and instance segmentation on the COCO dataset using standard models trained from random initialization. The results are no worse than their ImageNet pre-training counterparts even when using the hyper-parameters of the baseline system (Mask R-CNN) that were optimized for fine-tuning pretrained models, with the sole exception of increasing the number of training iterations so the randomly initialized models may converge. Training from random initialization is surprisingly robust; our results hold even when: (i) using only 10\% of the training data, (ii) for deeper and wider models, and (iii) for multiple tasks and metrics. Experiments show that ImageNet pre-training speeds up convergence early in training, but does not necessarily provide regularization or improve final target task accuracy. To push the envelope we demonstrate 50.9 AP on COCO object detection without using any external data—a result on par with the top COCO 2017 competition results that used ImageNet pre-training. These observations challenge the conventional wisdom of ImageNet pre-training for dependent tasks and we expect these discoveries will encourage people to rethink the current de facto paradigm of ‘pretraining and fine-tuning’ in computer vision.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/He-Rethinking%20ImageNet%20Pre-training-2019-ICCV.pdf}
}
@article{TensorMaskChenICCV2019, 
year = {2019}, 
title = {{TensorMask: A Foundation for Dense Object Segmentation}}, 
author = {Chen, Xinlei and Girshick, Ross and He, Kaiming and Dollár, Piotr}, 
journal = {ICCV}, 
eprint = {1903.12174}, 
abstract = {{Sliding-window object detectors that generate bounding-box object predictions over a dense, regular grid have advanced rapidly and proven popular. In contrast, modern instance segmentation approaches are dominated by methods that first detect object bounding boxes, and then crop and segment these regions, as popularized by Mask R-CNN. In this work, we investigate the paradigm of dense sliding-window instance segmentation, which is surprisingly under-explored. Our core observation is that this task is fundamentally different than other dense prediction tasks such as semantic segmentation or bounding-box object detection, as the output at every spatial location is itself a geometric structure with its own spatial dimensions. To formalize this, we treat dense instance segmentation as a prediction task over 4D tensors and present a general framework called TensorMask that explicitly captures this geometry and enables novel operators on 4D tensors. We demonstrate that the tensor view leads to large gains over baselines that ignore this structure, and leads to results comparable to Mask R-CNN. These promising results suggest that TensorMask can serve as a foundation for novel advances in dense mask prediction and a more complete understanding of the task. Code will be made available.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-TensorMask-%20A%20Foundation%20for%20Dense%20Object%20Segmentation-2019-ICCV.pdf}
}
@article{HRNetWangTPAMI2020, 
year = {2020}, 
title = {{Deep High-Resolution Representation Learning for Visual Recognition}}, 
author = {Wang, Jingdong and Sun, Ke and Cheng, Tianheng and Jiang, Borui and Deng, Chaorui and Zhao, Yang and Liu, Dong and Mu, Yadong and Tan, Mingkui and Wang, Xinggang and Liu, Wenyu and Xiao, Bin}, 
journal = {TPAMI}, 
abstract = {{High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions in series (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams in parallel and (ii) repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at https://github.com/HRNet.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Deep%20High-Resolution%20Representation%20Learning%20for%20Visual%20Recognition-2020-TPAMI.pdf}
}
@article{CAMZhouCVPR2016, 
year = {2016}, 
title = {{Learning Deep Features for Discriminative Localization}}, 
author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio}, 
journal = {CVPR}, 
abstract = {{In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1\% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation. We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classification task11Code and models are available at http://cnnlocalization.csail.mit.edu. Code and models are available at http://cnnlocalization.csail.mit.edu}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhou-Learning%20Deep%20Features%20for%20Discriminative%20Localization-2016-CVPR.pdf}
}
@article{NINLinICLR2014, 
year = {2014}, 
title = {{Network In Network}}, 
author = {Lin, Min and Chen, Qiang and Yan, Shuicheng}, 
journal = {ICLR}, 
eprint = {1312.4400}, 
abstract = {{We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lin-Network%20In%20Network-2014-ICLR.pdf}
}
@article{Non-localWangCVPR2018, 
year = {2018}, 
title = {{Non-local Neural Networks}}, 
author = {Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming}, 
journal = {CVPR}, 
eprint = {1711.07971}, 
abstract = {{Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our non-local models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code is available at https://github.com/facebookresearch/video-nonlocal-net .}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Non-local%20Neural%20Networks-2018-CVPR.pdf}
}
@article{SEAMWangCVPR2020, 
year = {2020}, 
title = {{Self-supervised Equivariant Attention Mechanism for Weakly Supervised Semantic Segmentation}}, 
author = {Wang, Yude and Zhang, Jie and Kan, Meina and Shan, Shiguang and Chen, Xilin}, 
journal = {CVPR}, 
abstract = {{Image-level weakly supervised semantic segmentation is a challenging problem that has been deeply studied in recent years. Most of advanced solutions exploit class activation map (CAM). However, CAMs can hardly serve as the object mask due to the gap between full and weak supervisions. In this paper, we propose a self-supervised equivariant attention mechanism (SEAM) to discover additional supervision and narrow the gap. Our method is based on the observation that equivariance is an implicit constraint in fully supervised semantic segmentation, whose pixel-level labels take the same spatial transformation as the input images during data augmentation. However, this constraint is lost on the CAMs trained by image-level supervision. Therefore, we propose consistency regularization on predicted CAMs from various transformed images to provide self-supervision for network learning. Moreover, we propose a pixel correlation module (PCM), which exploits context appearance information and refines the prediction of current pixel by its similar neighbors, leading to further improvement on CAMs consistency. Extensive experiments on PASCAL VOC 2012 dataset demonstrate our method outperforms state-of-the-art methods using the same level of supervision. The code is released online11https://github.com/YudeWang/SEAM. https://github.com/YudeWang/SEAM}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Self-supervised%20Equivariant%20Attention%20Mechanism%20for%20Weakly%20Supervised%20Semantic%20Segmentation-2020-CVPR.pdf}
}
@article{DANetFuCVPR2019, 
year = {2019}, 
title = {{Dual Attention Network for Scene Segmentation}}, 
author = {Fu, Jun and Liu, Jing and Tian, Haijie and Li, Yong and Bao, Yongjun and Fang, Zhiwei and Lu, Hanqing}, 
journal = {CVPR}, 
abstract = {{In this paper, we address the scene segmentation task by capturing rich contextual dependencies based on the self-attention mechanism. Unlike previous works that capture contexts by multi-scale feature fusion, we propose a Dual Attention Network (DANet) to adaptively integrate local features with their global dependencies. Specifically, we append two types of attention modules on top of dilated FCN, which model the semantic interdependencies in spatial and channel dimensions respectively. The position attention module selectively aggregates the feature at each position by a weighted sum of the features at all positions. Similar features would be related to each other regardless of their distances. Meanwhile, the channel attention module selectively emphasizes interdependent channel maps by integrating associated features among all channel maps. We sum the outputs of the two attention modules to further improve feature representation which contributes to more precise segmentation results. We achieve new state-of-theart segmentation performance on three challenging scene segmentation datasets, i.e., Cityscapes, PASCAL Context and COCO Stuff dataset. In particular, a Mean IoU score of 81.5\% on Cityscapes test set is achieved without using coarse data.1.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Fu-Dual%20Attention%20Network%20for%20Scene%20Segmentation-2019-CVPR.pdf}
}
@article{CaiTTouvronICCV2021, 
year = {2021}, 
title = {{Going deeper with Image Transformers}}, 
author = {Touvron, Hugo and Cord, Matthieu and Sablayrolles, Alexandre and Synnaeve, Gabriel and Jégou, Hervé}, 
journal = {ICCV}, 
abstract = {{Transformers have been recently adapted for large scale image classification, achieving high scores shaking up the long supremacy of convolutional neural networks. However the optimization of vision transformers has been little studied so far. In this work, we build and optimize deeper transformer networks for image classification. In particular, we investigate the interplay of architecture and optimization of such dedicated transformers. We make two architecture changes that significantly improve the accuracy of deep transformers. This leads us to produce models whose performance does not saturate early with more depth, for in-stance we obtain 86.5\% top-1 accuracy on Imagenet when training with no external data, we thus attain the current sate of the art with less floating-point operations and parameters. Our best model establishes the new state of the art on Imagenet with Reassessed labels and Imagenet-V2 / match frequency, in the setting with no additional training data. We share our code and models1.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Touvron-Going%20deeper%20with%20Image%20Transformers-2021-ICCV.pdf}
}
@article{ALarge-ScaleFeichtenhoferCVPR2021, 
year = {2021}, 
title = {{A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning}}, 
author = {Feichtenhofer, Christoph and Fan, Haoqi and Xiong, Bo and Girshick, Ross and He, Kaiming}, 
journal = {CVPR}, 
abstract = {{We present a large-scale study on unsupervised spatiotemporal representation learning from videos. With a unified perspective on four recent image-based frameworks, we study a simple objective that can easily generalize all these methods to space-time. Our objective encourages temporally-persistent features in the same video, and in spite of its simplicity, it works surprisingly well across: (i) different unsupervised frameworks, (ii) pre-training datasets, (iii) downstream datasets, and (iv) backbone architectures. We draw a series of intriguing observations from this study, e.g., we discover that encouraging long-spanned persistency can be effective even if the timespan is 60 seconds. In addition to state-of-the-art results in multiple benchmarks, we report a few promising cases in which unsupervised pre-training can outperform its supervised counterpart. Code will be made available at https://github.com/facebookresearch/SlowFast.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Feichtenhofer-A%20Large-Scale%20Study%20on%20Unsupervised%20Spatiotemporal%20Representation%20Learning-2021-CVPR.pdf}
}
@article{AnEmpiricalChenICCV2021, 
year = {2021}, 
title = {{An Empirical Study of Training Self-Supervised Vision Transformers}}, 
author = {Chen, Xinlei and Xie, Saining and He, Kaiming}, 
journal = {ICCV}, 
abstract = {{This paper does not describe a novel method. Instead, it studies a straightforward, incremental, yet must-know baseline given the recent progress in computer vision: self-supervised learning for Vision Transformers (ViT). While the training recipes for standard convolutional networks have been highly mature and robust, the recipes for ViT are yet to be built, especially in the self-supervised scenarios where training becomes more challenging. In this work, we go back to basics and investigate the effects of several fundamental components for training self-supervised ViT. We observe that instability is a major issue that degrades accuracy, and it can be hidden by apparently good results. We reveal that these results are indeed partial failure, and they can be improved when training is made more stable. We benchmark ViT results in MoCo v3 and several other self-supervised frameworks, with ablations in various aspects. We discuss the currently positive evidence as well as challenges and open questions. We hope that this work will provide useful data points and experience for future research.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-An%20Empirical%20Study%20of%20Training%20Self-Supervised%20Vision%20Transformers-2021-ICCV.pdf}
}
@article{DALL-ERameshICML2021, 
year = {2021}, 
title = {{Zero-Shot Text-to-Image Generation}}, 
author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya}, 
journal = {ICML}, 
eprint = {2102.12092}, 
abstract = {{Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ramesh-Zero-Shot%20Text-to-Image%20Generation-2021-ICML.pdf}
}
@article{DCRChengECCV2018, 
year = {2018}, 
title = {{Revisiting RCNN: On Awakening the Classification Power of Faster RCNN}}, 
author = {Cheng, Bowen and Wei, Yunchao and Shi, Honghui and Feris, Rogerio and Xiong, Jinjun and Huang, Thomas}, 
journal = {ECCV}, 
eprint = {1803.06799}, 
abstract = {{Recent region-based object detectors are usually built with separate classification and localization branches on top of shared feature extraction networks. In this paper, we analyze failure cases of state-of-the-art detectors and observe that most hard false positives result from classification instead of localization. We conjecture that: (1) Shared feature representation is not optimal due to the mismatched goals of feature learning for classification and localization; (2) multi-task learning helps, yet optimization of the multi-task loss may result in sub-optimal for individual tasks; (3) large receptive field for different scales leads to redundant context information for small objects.We demonstrate the potential of detector classification power by a simple, effective, and widely-applicable Decoupled Classification Refinement (DCR) network. DCR samples hard false positives from the base classifier in Faster RCNN and trains a RCNN-styled strong classifier. Experiments show new state-of-the-art results on PASCAL VOC and COCO without any bells and whistles.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Cheng-Revisiting%20RCNN-%20On%20Awakening%20the%20Classification%20Power%20of%20Faster%20RCNN-2018-ECCV.pdf}
}
@article{ExploringNairMICCAI2018, 
year = {2018}, 
title = {{Exploring Uncertainty Measures in Deep Networks for Multiple Sclerosis Lesion Detection and Segmentation}}, 
author = {Nair, Tanya and Precup, Doina and Arnold, Douglas L and Arbel, Tal}, 
journal = {MICCAI}, 
eprint = {1808.01200}, 
abstract = {{Deep learning (DL) networks have recently been shown to outperform other segmentation methods on various public, medical-image challenge datasets [3,11,16], especially for large pathologies. However, in the context of diseases such as Multiple Sclerosis (MS), monitoring all the focal lesions visible on MRI sequences, even very small ones, is essential for disease staging, prognosis, and evaluating treatment efficacy. Moreover, producing deterministic outputs hinders DL adoption into clinical routines. Uncertainty estimates for the predictions would permit subsequent revision by clinicians. We present the first exploration of multiple uncertainty estimates based on Monte Carlo (MC) dropout [4] in the context of deep networks for lesion detection and segmentation in medical images. Specifically, we develop a 3D MS lesion segmentation CNN, augmented to provide four different voxel-based uncertainty measures based on MC dropout. We train the network on a proprietary, large-scale, multi-site, multi-scanner, clinical MS dataset, and compute lesion-wise uncertainties by accumulating evidence from voxel-wise uncertainties within detected lesions. We analyze the performance of voxel-based segmentation and lesion-level detection by choosing operating points based on the uncertainty. Empirical evidence suggests that uncertainty measures consistently allow us to choose superior operating points compared only using the network's sigmoid output as a probability.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Nair-Exploring%20Uncertainty%20Measures%20in%20Deep%20Networks%20for%20Multiple%20Sclerosis%20Lesion%20Detection%20and%20Segmentation-2018-MICCAI.pdf}
}
@article{Uncertainty-AwareDingMIDL2020, 
year = {2020}, 
title = {{Uncertainty-Aware Training of Neural Networks for Selective Medical Image Segmentation}}, 
author = {Ding, Yukun and Liu, Jinglan and Xu, Xiaowei and Huang, Meiping and Zhuang, Jian and Xiong, Jinjun and Shi, Yiyu}, 
journal = {MIDL}, 
abstract = {{State-of-the-art deep learning based methods have achieved remarkable performance on medical image segmentation. Their applications in the clinical setting are, however, limited due to the lack of trustworthiness and reliability. Selective image segmentation has been proposed to address this issue by letting a DNN model process instances with high confidence while referring difficult ones with high uncertainty to experienced radiologists. As such, the model performance is only affected by the predictions on the high confidence subset rather than the whole dataset. Existing selective segmentation methods, however, ignore this unique property of selective segmentation and train their DNN models by optimizing accuracy on the entire dataset. Motivated by such a discrepancy, we present a novel method in this paper that considers such uncertainty in the training process to maximize the accuracy on the confident subset rather than the accuracy on the whole dataset. Experimental results using the whole heart and great vessel segmentation and gland segmentation show that such a training scheme can significantly improve the performance of selective segmentation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ding-Uncertainty-Aware%20Training%20of%20Neural%20Networks%20for%20Selective%20Medical%20Image%20Segmentation-2020-MIDL.pdf}
}
@article{FCOSTianICCV2019, 
year = {2019}, 
title = {{FCOS: Fully Convolutional One-Stage Object Detection}}, 
author = {Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong}, 
journal = {ICCV}, 
abstract = {{We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the predefined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44:7\% in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at: tinyurl:com/FCOSv1}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tian-FCOS-%20Fully%20Convolutional%20One-Stage%20Object%20Detection-2019-ICCV.pdf}
}
@article{RepLKNetDingCVPR2022, 
year = {2022}, 
title = {{Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs}}, 
author = {Ding, Xiaohan and Zhang, Xiangyu and Zhou, Yizhuang and Han, Jungong and Ding, Guiguang and Sun, Jian}, 
journal = {CVPR}, 
eprint = {2203.06717}, 
abstract = {{We revisit large kernel design in modern convolutional neural networks (CNNs). Inspired by recent advances in vision transformers (ViTs), in this paper, we demonstrate that using a few large convolutional kernels instead of a stack of small kernels could be a more powerful paradigm. We suggested five guidelines, e.g., applying re-parameterized large depth-wise convolutions, to design efficient high-performance large-kernel CNNs. Following the guidelines, we propose RepLKNet, a pure CNN architecture whose kernel size is as large as 31x31, in contrast to commonly used 3x3. RepLKNet greatly closes the performance gap between CNNs and ViTs, e.g., achieving comparable or superior results than Swin Transformer on ImageNet and a few typical downstream tasks, with lower latency. RepLKNet also shows nice scalability to big data and large models, obtaining 87.8\% top-1 accuracy on ImageNet and 56.0\% mIoU on ADE20K, which is very competitive among the state-of-the-arts with similar model sizes. Our study further reveals that, in contrast to small-kernel CNNs, large-kernel CNNs have much larger effective receptive fields and higher shape bias rather than texture bias. Code \& models at https://github.com/megvii-research/RepLKNet.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ding-Scaling%20Up%20Your%20Kernels%20to%2031x31-%20Revisiting%20Large%20Kernel%20Design%20in%20CNNs-2022-CVPR.pdf}
}
@article{LDZhengCVPR2022, 
year = {2022}, 
title = {{Localization Distillation for Dense Object Detection}}, 
author = {Zheng, Zhaohui and Ye, Rongguang and Wang, Ping and Ren, Dongwei and Zuo, Wangmeng and Hou, Qibin and Cheng, Ming-Ming}, 
journal = {CVPR}, 
eprint = {2102.12252}, 
abstract = {{Knowledge distillation (KD) has witnessed its powerful capability in learning compact models in object detection. Previous KD methods for object detection mostly focus on imitating deep features within the imitation regions instead of mimicking classification logit due to its inefficiency in distilling localization information and trivial improvement. In this paper, by reformulating the knowledge distillation process on localization, we present a novel localization distillation (LD) method which can efficiently transfer the localization knowledge from the teacher to the student. Moreover, we also heuristically introduce the concept of valuable localization region that can aid to selectively distill the semantic and localization knowledge for a certain region. Combining these two new components, for the first time, we show that logit mimicking can outperform feature imitation and localization knowledge distillation is more important and efficient than semantic knowledge for distilling object detectors. Our distillation scheme is simple as well as effective and can be easily applied to different dense object detectors. Experiments show that our LD can boost the AP score of GFocal-ResNet-50 with a single-scale 1x training schedule from 40.1 to 42.1 on the COCO benchmark without any sacrifice on the inference speed. Our source code and trained models are publicly available at https://github.com/HikariTJU/LD}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zheng-Localization%20Distillation%20for%20Dense%20Object%20Detection-2022-CVPR.pdf}
}
@article{SECKolesnikovECCV2016, 
year = {2016}, 
title = {{Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation}}, 
author = {Kolesnikov, Alexander and Lampert, Christoph H}, 
journal = {ECCV}, 
eprint = {1603.06098}, 
abstract = {{We introduce a new loss function for the weakly-supervised training of semantic image segmentation models based on three guiding principles: to seed with weak localization cues, to expand objects based on the information about which classes can occur in an image, and to constrain the segmentations to coincide with object boundaries. We show experimentally that training a deep convolutional neural network using the proposed loss function leads to substantially better segmentations than previous state-of-the-art methods on the challenging PASCAL VOC 2012 dataset. We furthermore give insight into the working mechanism of our method by a detailed experimental study that illustrates how the segmentation quality is affected by each term of the proposed loss function as well as their combinations.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kolesnikov-Seed,%20Expand%20and%20Constrain-%20Three%20Principles%20for%20Weakly-Supervised%20Image%20Segmentation-2016-ECCV.pdf}
}
@article{Lite-HRNetYuCVPR2021, 
year = {2021}, 
title = {{Lite-HRNet: A Lightweight High-Resolution Network}}, 
author = {Yu, Changqian and Xiao, Bin and Gao, Changxin and Yuan, Lu and Zhang, Lei and Sang, Nong and Wang, Jingdong}, 
journal = {CVPR}, 
abstract = {{We present an efficient high-resolution network, Lite-HRNet, for human pose estimation. We start by simply applying the efficient shuffle block in ShuffleNet to HRNet (high-resolution network), yielding stronger performance over popular lightweight networks, such as MobileNet, ShuffleNet, and Small HRNet. We find that the heavily-used pointwise (1 × 1) convolutions in shuffle blocks become the computational bottleneck. We introduce a lightweight unit, conditional channel weighting, to replace costly pointwise (1 × 1) convolutions in shuffle blocks. The complexity of channel weighting is linear w.r.t the number of channels and lower than the quadratic time complexity for pointwise convolutions. Our solution learns the weights from all the channels and over multiple resolutions that are readily available in the parallel branches in HRNet. It uses the weights as the bridge to exchange information across channels and resolutions, compensating the role played by the pointwise (1 × 1) convolution. Lite-HRNet demonstrates superior results on human pose estimation over popular lightweight networks. Moreover, Lite-HRNet can be easily applied to semantic segmentation task in the same lightweight manner. The code and models have been publicly available at https://github.com/HRNet/Lite-HRNet.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yu-Lite-HRNet-%20A%20Lightweight%20High-Resolution%20Network-2021-CVPR.pdf}
}
@article{ConvNeXtLiuCVPR2022, 
year = {2022}, 
title = {{A ConvNet for the 2020s}}, 
author = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining}, 
journal = {CVPR}, 
eprint = {2201.03545}, 
abstract = {{The "Roaring 20s" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually "modernize" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8\% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-A%20ConvNet%20for%20the%202020s-2022-CVPR.pdf}
}
@article{ResNetStrikesBackWightmanNeurIPSWorkshop2021, 
year = {2021}, 
title = {{ResNet strikes back: An improved training procedure in timm}}, 
author = {Wightman, Ross and Touvron, Hugo and Jégou, Hervé}, 
journal = {NeurIPS Workshop}, 
eprint = {2110.00476}, 
abstract = {{The influential Residual Networks designed by He et al. remain the gold-standard architecture in numerous scientific publications. They typically serve as the default architecture in studies, or as baselines when new architectures are proposed. Yet there has been significant progress on best practices for training neural networks since the inception of the ResNet architecture in 2015. Novel optimization \& data-augmentation have increased the effectiveness of the training recipes. In this paper, we re-evaluate the performance of the vanilla ResNet-50 when trained with a procedure that integrates such advances. We share competitive training settings and pre-trained models in the timm open-source library, with the hope that they will serve as better baselines for future work. For instance, with our more demanding training setting, a vanilla ResNet-50 reaches 80.4\% top-1 accuracy at resolution 224x224 on ImageNet-val without extra data or distillation. We also report the performance achieved with popular models with our training procedure.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wightman-ResNet%20strikes%20back-%20An%20improved%20training%20procedure%20in%20timm-2021-NeurIPS%20Workshop.pdf}
}
@article{CoAtNetDaiNeurIPS2021, 
year = {2021}, 
title = {{CoAtNet: Marrying Convolution and Attention for All Data Sizes}}, 
author = {Dai, Zihang and Liu, Hanxiao and Le, Quoc V and Tan, Mingxing}, 
journal = {NeurIPS}, 
eprint = {2106.04803}, 
abstract = {{Transformers have attracted increasing interests in computer vision, but they still fall behind state-of-the-art convolutional networks. In this work, we show that while Transformers tend to have larger model capacity, their generalization can be worse than convolutional networks due to the lack of the right inductive bias. To effectively combine the strengths from both architectures, we present CoAtNets(pronounced "coat" nets), a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention; (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that our CoAtNets achieve state-of-the-art performance under different resource constraints across various datasets: Without extra data, CoAtNet achieves 86.0\% ImageNet top-1 accuracy; When pre-trained with 13M images from ImageNet-21K, our CoAtNet achieves 88.56\% top-1 accuracy, matching ViT-huge pre-trained with 300M images from JFT-300M while using 23x less data; Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88\% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Dai-CoAtNet-%20Marrying%20Convolution%20and%20Attention%20for%20All%20Data%20Sizes-2021-NeurIPS.pdf}
}
@article{PETSchickEACL2021, 
year = {2021}, 
title = {{Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference}}, 
author = {Schick, Timo and Schütze, Hinrich}, 
journal = {EACL}, 
eprint = {2001.07676}, 
abstract = {{Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with "task descriptions" in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Schick-Exploiting%20Cloze%20Questions%20for%20Few%20Shot%20Text%20Classification%20and%20Natural%20Language%20Inference-2021-EACL.pdf}
}
@article{CLIPRadfordICML2021, 
year = {2021}, 
title = {{Learning Transferable Visual Models From Natural Language Supervision}}, 
author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya}, 
journal = {ICML}, 
eprint = {2103.00020}, 
abstract = {{State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Radford-Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision-2021-ICML.pdf}
}
@article{CCNNPathakICCV2015, 
year = {2015}, 
title = {{Constrained Convolutional Neural Networks for Weakly Supervised Segmentation}}, 
author = {Pathak, Deepak and Krahenbuhl, Philipp and Darrell, Trevor}, 
journal = {ICCV}, 
abstract = {{We present an approach to learn a dense pixel-wise labeling from image-level tags. Each image-level tag imposes constraints on the output labeling of a Convolutional Neural Network (CNN) classifier. We propose Constrained CNN (CCNN), a method which uses a novel loss function to optimize for any set of linear constraints on the output space (i.e. predicted label distribution) of a CNN. Our loss formulation is easy to optimize and can be incorporated directly into standard stochastic gradient descent optimization. The key idea is to phrase the training objective as a biconvex optimization for linear models, which we then relax to nonlinear deep networks. Extensive experiments demonstrate the generality of our new learning framework. The constrained loss yields state-of-the-art results on weakly supervised semantic image segmentation. We further demonstrate that adding slightly more supervision can greatly improve the performance of the learning algorithm.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Pathak-Constrained%20Convolutional%20Neural%20Networks%20for%20Weakly%20Supervised%20Segmentation-2015-ICCV.pdf}
}
@article{Constrained-CNNKervadecMIA2019, 
year = {2019}, 
title = {{Constrained-CNN losses for weakly supervised segmentation}}, 
author = {Kervadec, Hoel and Dolz, Jose and Tang, Meng and Granger, Eric and Boykov, Yuri and Ayed, Ismail Ben}, 
journal = {MIA}, 
abstract = {{Weakly-supervised learning based on, e.g., partially labelled images or image-tags, is currently attracting significant attention in CNN segmentation as it can mitigate the need for full and laborious pixel/voxel annotations. Enforcing high-order (global) inequality constraints on the network output (for instance, to constrain the size of the target region) can leverage unlabeled data, guiding the training process with domain-specific knowledge. Inequality constraints are very flexible because they do not assume exact prior knowledge. However, constrained Lagrangian dual optimization has been largely avoided in deep networks, mainly for computational tractability reasons. To the best of our knowledge, the method of Pathak et al. (2015a) is the only prior work that addresses deep CNNs with linear constraints in weakly supervised segmentation. It uses the constraints to synthesize fully-labeled training masks (proposals) from weak labels, mimicking full supervision and facilitating dual optimization. We propose to introduce a differentiable penalty, which enforces inequality constraints directly in the loss function, avoiding expensive Lagrangian dual iterates and proposal generation. From constrained-optimization perspective, our simple penalty-based approach is not optimal as there is no guarantee that the constraints are satisfied. However, surprisingly, it yields substantially better results than the Lagrangian-based constrained CNNs in Pathak et al. (2015a), while reducing the computational demand for training. By annotating only a small fraction of the pixels, the proposed approach can reach a level of segmentation performance that is comparable to full supervision on three separate tasks. While our experiments focused on basic linear constraints such as the target-region size and image tags, our framework can be easily extended to other non-linear constraints, e.g., invariant shape moments (Klodt and Cremers, 2011) and other region statistics (Lim et al., 2014). Therefore, it has the potential to close the gap between weakly and fully supervised learning in semantic medical image segmentation. Our code is publicly available.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kervadec-Constrained-CNN%20losses%20for%20weakly%20supervised%20segmentation-2019-MIA.pdf}
}
@article{OnRegularizedLossesTangECCV2018, 
year = {2018}, 
title = {{On Regularized Losses for Weakly-supervised CNN Segmentation}}, 
author = {Tang, Meng and Perazzi, Federico and Djelouah, Abdelaziz and Ayed, Ismail Ben and Schroers, Christopher and Boykov, Yuri}, 
journal = {ECCV}, 
eprint = {1803.09569}, 
abstract = {{Minimization of regularized losses is a principled approach to weak supervision well-established in deep learning, in general. However, it is largely overlooked in semantic segmentation currently dominated by methods mimicking full supervision via "fake" fully-labeled training masks (proposals) generated from available partial input. To obtain such full masks the typical methods explicitly use standard regularization techniques for "shallow" segmentation, e.g. graph cuts or dense CRFs. In contrast, we integrate such standard regularizers directly into the loss functions over partial input. This approach simplifies weakly-supervised training by avoiding extra MRF/CRF inference steps or layers explicitly generating full masks, while improving both the quality and efficiency of training. This paper proposes and experimentally compares different losses integrating MRF/CRF regularization terms. We juxtapose our regularized losses with earlier proposal-generation methods using explicit regularization steps or layers. Our approach achieves state-of-the-art accuracy in semantic segmentation with near full-supervision quality.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tang-On%20Regularized%20Losses%20for%20Weakly-supervised%20CNN%20Segmentation-2018-ECCV.pdf}
}
@article{IBLossParkICCV2021, 
year = {2021}, 
title = {{Influence-Balanced Loss for Imbalanced Visual Classification}}, 
author = {Park, Seulki and Lim, Jongin and Jeon, Younghan and Choi, Jin Young}, 
journal = {ICCV}, 
abstract = {{In this paper, we propose a balancing training method to address problems in imbalanced data learning. To this end, we derive a new loss used in the balancing training phase that alleviates the influence of samples that cause an overfitted decision boundary. The proposed loss efficiently improves the performance of any type of imbalance learning methods. In experiments on multiple benchmark data sets, we demonstrate the validity of our method and reveal that the proposed loss outperforms the state-of-the-art cost-sensitive loss methods. Furthermore, since our loss is not restricted to a specific task, model, or training method, it can be easily used in combination with other recent resampling, meta-learning, and cost-sensitive learning methods for class-imbalance problems. Our code is made available at https://github.com/pseulki/IB-Loss.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Park-Influence-Balanced%20Loss%20for%20Imbalanced%20Visual%20Classification-2021-ICCV.pdf}
}
@article{UnderstandingXuICLR2021, 
year = {2021}, 
title = {{Understanding the role of importance weighting for deep learning}}, 
author = {Xu, Da and Ye, Yuting and Ruan, Chuanwei}, 
journal = {ICLR}, 
eprint = {2103.15209}, 
abstract = {{The recent paper by Byrd \& Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xu-Understanding%20the%20role%20of%20importance%20weighting%20for%20deep%20learning-2021-ICLR.pdf}
}
@article{RW-KDLuEMNLPFindings2021, 
year = {2021}, 
title = {{RW-KD: Sample-wise Loss Terms Re-Weighting for Knowledge Distillation}}, 
author = {Lu, Peng and Ghaddar, Abbas and Rashid, Ahmad and Rezagholizadeh, Mehdi and Ghodsi, Ali and Langlais, Philippe}, 
journal = {EMNLP Findings}, 
abstract = {{Knowledge Distillation (KD) is extensively used in Natural Language Processing to com- press the pre-training and task-specific fine- tuning phases of large neural language mod- els. A student model is trained to minimize a convex combination of the prediction loss over the labels and another over the teacher output. However, most existing works either fix the interpolating weight between the two losses a priorior vary the weight using heuristics. In this work, we propose a novel sample- wise loss weighting method, RW-KD. A meta- learner, simultaneously trained with the student, adaptively re-weights the two losses for each sample. We demonstrate, on 7 datasets of the GLUE benchmark, that RW-KD outperforms other loss re-weighting methods for KD.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lu-RW-KD-%20Sample-wise%20Loss%20Terms%20Re-Weighting%20for%20Knowledge%20Distillation-2021-EMNLP%20Findings.pdf}
}
@article{Multi-TaskLearningKendallCVPR2018, 
year = {2018}, 
title = {{Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics}}, 
author = {Kendall, Alex and Gal, Yarin and Cipolla, Roberto}, 
journal = {CVPR}, 
abstract = {{Numerous deep learning applications benefit from multitask learning with multiple regression and classification objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task's loss. Tuning these weights by hand is a difficult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kendall-Multi-Task%20Learning%20Using%20Uncertainty%20to%20Weigh%20Losses%20for%20Scene%20Geometry%20and%20Semantics-2018-CVPR.pdf}
}
@article{Multi-TaskLearningSenerNeurIPS2018, 
year = {2018}, 
title = {{Multi-Task Learning as Multi-Objective Optimization}}, 
author = {Sener, Ozan and Koltun, Vladlen}, 
journal = {NeurIPS}, 
eprint = {1810.04650}, 
abstract = {{In multi-task learning, multiple tasks are solved jointly, sharing inductive bias between them. Multi-task learning is inherently a multi-objective problem because different tasks may conflict, necessitating a trade-off. A common compromise is to optimize a proxy objective that minimizes a weighted linear combination of per-task losses. However, this workaround is only valid when the tasks do not compete, which is rarely the case. In this paper, we explicitly cast multi-task learning as multi-objective optimization, with the overall objective of finding a Pareto optimal solution. To this end, we use algorithms developed in the gradient-based multi-objective optimization literature. These algorithms are not directly applicable to large-scale learning problems since they scale poorly with the dimensionality of the gradients and the number of tasks. We therefore propose an upper bound for the multi-objective loss and show that it can be optimized efficiently. We further prove that optimizing this upper bound yields a Pareto optimal solution under realistic assumptions. We apply our method to a variety of multi-task deep learning problems including digit classification, scene understanding (joint semantic segmentation, instance segmentation, and depth estimation), and multi-label classification. Our method produces higher-performing models than recent multi-task learning formulations or per-task training.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Sener-Multi-Task%20Learning%20as%20Multi-Objective%20Optimization-2018-NeurIPS.pdf}
}
@article{UnderstandingKohICML2017, 
year = {2017}, 
title = {{Understanding Black-box Predictions via Influence Functions}}, 
author = {Koh, Pang Wei and Liang, Percy}, 
journal = {ICML}, 
eprint = {1703.04730}, 
abstract = {{How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Koh-Understanding%20Black-box%20Predictions%20via%20Influence%20Functions-2017-ICML.pdf}
}
@article{LearningToDownsampleJinICLR2022, 
year = {2022}, 
title = {{Learning to Downsample for Segmentation of Ultra-High Resolution Images}}, 
author = {Jin, Chen and Tanno, Ryutaro and Mertzanidou, Thomy and Panagiotaki, Eleftheria and Alexander, Daniel C}, 
journal = {ICLR}, 
eprint = {2109.11071}, 
abstract = {{Many computer vision systems require low-cost segmentation algorithms based on deep learning, either because of the enormous size of input images or limited computational budget. Common solutions uniformly downsample the input images to meet memory constraints, assuming all pixels are equally informative. In this work, we demonstrate that this assumption can harm the segmentation performance because the segmentation difficulty varies spatially. We combat this problem by introducing a learnable downsampling module, which can be optimised together with the given segmentation model in an end-to-end fashion. We formulate the problem of training such downsampling module as optimisation of sampling density distributions over the input images given their low-resolution views. To defend against degenerate solutions (e.g. over-sampling trivial regions like the backgrounds), we propose a regularisation term that encourages the sampling locations to concentrate around the object boundaries. We find the downsampling module learns to sample more densely at difficult locations, thereby improving the segmentation performance. Our experiments on benchmarks of high-resolution street view, aerial and medical images demonstrate substantial improvements in terms of efficiency-and-accuracy trade-off compared to both uniform downsampling and two recent advanced downsampling techniques.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Jin-Learning%20to%20Downsample%20for%20Segmentation%20of%20Ultra-High%20Resolution%20Images-2022-ICLR.pdf}
}
@article{SeLaAsanoICLR2020, 
year = {2020}, 
title = {{Self-labelling via simultaneous clustering and representation learning}}, 
author = {Asano, Yuki Markus and Rupprecht, Christian and Vedaldi, Andrea}, 
journal = {ICLR}, 
eprint = {1911.05371}, 
abstract = {{Combining clustering and representation learning is one of the most promising approaches for unsupervised learning of deep neural networks. However, doing so naively leads to ill posed learning problems with degenerate solutions. In this paper, we propose a novel and principled learning formulation that addresses these issues. The method is obtained by maximizing the information between labels and input data indices. We show that this criterion extends standard crossentropy minimization to an optimal transport problem, which we solve efficiently for millions of input images and thousands of labels using a fast variant of the Sinkhorn-Knopp algorithm. The resulting method is able to self-label visual data so as to train highly competitive image representations without manual labels. Our method achieves state of the art representation learning performance for AlexNet and ResNet-50 on SVHN, CIFAR-10, CIFAR-100 and ImageNet and yields the first self-supervised AlexNet that outperforms the supervised Pascal VOC detection baseline. Code and models are available.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Asano-Self-labelling%20via%20simultaneous%20clustering%20and%20representation%20learning-2020-ICLR.pdf}
}
@article{STEGOHamiltonICLR2022, 
year = {2022}, 
title = {{Unsupervised Semantic Segmentation by Distilling Feature Correspondences}}, 
author = {Hamilton, Mark and Zhang, Zhoutong and Hariharan, Bharath and Snavely, Noah and Freeman, William T}, 
journal = {ICLR}, 
eprint = {2203.08414}, 
abstract = {{Unsupervised semantic segmentation aims to discover and localize semantically meaningful categories within image corpora without any form of annotation. To solve this task, algorithms must produce features for every pixel that are both semantically meaningful and compact enough to form distinct clusters. Unlike previous works which achieve this with a single end-to-end framework, we propose to separate feature learning from cluster compactification. Empirically, we show that current unsupervised feature learning frameworks already generate dense features whose correlations are semantically consistent. This observation motivates us to design STEGO (\$\textbackslashtextbf\{S\}\$elf-supervised \$\textbackslashtextbf\{T\}\$ransformer with \$\textbackslashtextbf\{E\}\$nergy-based \$\textbackslashtextbf\{G\}\$raph \$\textbackslashtextbf\{O\}\$ptimization), a novel framework that distills unsupervised features into high-quality discrete semantic labels. At the core of STEGO is a novel contrastive loss function that encourages features to form compact clusters while preserving their relationships across the corpora. STEGO yields a significant improvement over the prior state of the art, on both the CocoStuff (\$\textbackslashtextbf\{+14 mIoU\}\$) and Cityscapes (\$\textbackslashtextbf\{+9 mIoU\}\$) semantic segmentation challenges.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hamilton-Unsupervised%20Semantic%20Segmentation%20by%20Distilling%20Feature%20Correspondences-2022-ICLR.pdf}
}
@article{DCNDaiICCV2017, 
year = {2017}, 
title = {{Deformable Convolutional Networks}}, 
author = {Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen}, 
journal = {ICCV}, 
abstract = {{Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in their building modules. In this work, we introduce two new modules to enhance the transformation modeling capability of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from the target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the performance of our approach. For the first time, we show that learning dense spatial transformation in deep CNNs is effective for sophisticated vision tasks such as object detection and semantic segmentation. The code is released at https://github.com/msracver/Deformable-ConvNets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Dai-Deformable%20Convolutional%20Networks-2017-ICCV.pdf}
}
@article{UnderstandingLuoNeurIPS2016, 
year = {2016}, 
title = {{Understanding the Effective Receptive Field in Deep Convolutional Neural Networks}}, 
author = {Luo, Wenjie and Li, Yujia and Urtasun, Raquel and Zemel, Richard}, 
journal = {NeurIPS}, 
eprint = {1701.04128}, 
abstract = {{We study characteristics of receptive fields of units in deep convolutional networks. The receptive field size is a crucial issue in many visual tasks, as the output must respond to large enough areas in the image to capture information about large objects. We introduce the notion of an effective receptive field, and show that it both has a Gaussian distribution and only occupies a fraction of the full theoretical receptive field. We analyze the effective receptive field in several architecture designs, and the effect of nonlinear activations, dropout, sub-sampling and skip connections on it. This leads to suggestions for ways to address its tendency to be too small.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Luo-Understanding%20the%20Effective%20Receptive%20Field%20in%20Deep%20Convolutional%20Neural%20Networks-2016-NeurIPS.pdf}
}
@article{AnytimeLiuICLR2022, 
year = {2022}, 
title = {{Anytime Dense Prediction with Confidence Adaptivity}}, 
author = {Liu, Zhuang and Xu, Zhiqiu and Wang, Hung-Ju and Darrell, Trevor and Shelhamer, Evan}, 
journal = {ICLR}, 
abstract = {{Anytime inference requires a model to make a progression of predictions which might be halted at any time. Prior research on anytime visual recognition has mostly focused on image classification.We propose the first unified and end-to- end approach for anytime dense prediction. A cascade of “exits” is attached to the model to make multiple predictions. We redesign the exits to account for the depth and spatial resolution of the features for each exit. To reduce total computation, and make full use of prior predictions, we develop a novel spatially adaptive approach to avoid further computation on regions where early predictions are al- ready sufficiently confident. Our full method, named anytime dense prediction with confidence (ADP-C), achieves the same level of final accuracy, and mean- while significantly reduces total computation. We evaluate ADP-C on Cityscapes semantic segmentation and MPII human pose estimation: our method enables any- time inference without sacrificing accuracy while also reducing the total FLOPs of its base models by 44.4\% and 59.1\%. We compare with anytime inference by deep equilibrium networks and feature-based stochastic sampling, showing that ADP-C dominates both across the accuracy-computation curve. Our code is available at https://github.com/liuzhuang13/anytime.}}
}
@article{ShapeDescriptorsKervadecMIDL2021, 
year = {2021}, 
title = {{Beyond pixel-wise supervision for segmentation: A few global shape descriptors might be surprisingly good!}}, 
author = {Kervadec, Hoel and Bahig, Houda and Letourneau-Guillon, Laurent and Dolz, Jose and Ayed, Ismail Ben}, 
journal = {MIDL}, 
eprint = {2105.00859}, 
abstract = {{Standard losses for training deep segmentation networks could be seen as individual classifications of pixels, instead of supervising the global shape of the predicted segmentations. While effective, they require exact knowledge of the label of each pixel in an image. This study investigates how effective global geometric shape descriptors could be, when used on their own as segmentation losses for training deep networks. Not only interesting theoretically, there exist deeper motivations to posing segmentation problems as a reconstruction of shape descriptors: Annotations to obtain approximations of low-order shape moments could be much less cumbersome than their full-mask counterparts, and anatomical priors could be readily encoded into invariant shape descriptions, which might alleviate the annotation burden. Also, and most importantly, we hypothesize that, given a task, certain shape descriptions might be invariant across image acquisition protocols/modalities and subject populations, which might open interesting research avenues for generalization in medical image segmentation. We introduce and formulate a few shape descriptors in the context of deep segmentation, and evaluate their potential as standalone losses on two different challenging tasks. Inspired by recent works in constrained optimization for deep networks, we propose a way to use those descriptors to supervise segmentation, without any pixel-level label. Very surprisingly, as little as 4 descriptors values per class can approach the performance of a segmentation mask with 65k individual discrete labels. We also found that shape descriptors can be a valid way to encode anatomical priors about the task, enabling to leverage expert knowledge without additional annotations. Our implementation is publicly available and can be easily extended to other tasks and descriptors: https://github.com/hkervadec/shape\_descriptors}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kervadec-Beyond%20pixel-wise%20supervision%20for%20segmentation-%20A%20few%20global%20shape%20descriptors%20might%20be%20surprisingly%20good!-2021-MIDL.pdf}
}
@article{ReCoLiuICLR2022, 
year = {2022}, 
title = {{Bootstrapping Semantic Segmentation with Regional Contrast}}, 
author = {Liu, Shikun and Zhi, Shuaifeng and Johns, Edward and Davison, Andrew J}, 
journal = {ICLR}, 
eprint = {2104.04465}, 
abstract = {{We present ReCo, a contrastive learning framework designed at a regional level to assist learning in semantic segmentation. ReCo performs semi-supervised or supervised pixel-level contrastive learning on a sparse set of hard negative pixels, with minimal additional memory footprint. ReCo is easy to implement, being built on top of off-the-shelf segmentation networks, and consistently improves performance in both semi-supervised and supervised semantic segmentation methods, achieving smoother segmentation boundaries and faster convergence. The strongest effect is in semi-supervised learning with very few labels. With ReCo, we achieve high-quality semantic segmentation models, requiring only 5 examples of each semantic class. Code is available at https://github.com/lorenmt/reco.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-Bootstrapping%20Semantic%20Segmentation%20with%20Regional%20Contrast-2022-ICLR.pdf}
}
@article{DDPMSegmentationBaranchukICLR2022, 
year = {2022}, 
title = {{Label-Efficient Semantic Segmentation with Diffusion Models}}, 
author = {Baranchuk, Dmitry and Rubachev, Ivan and Voynov, Andrey and Khrulkov, Valentin and Babenko, Artem}, 
journal = {ICLR}, 
eprint = {2112.03126}, 
abstract = {{Denoising diffusion probabilistic models have recently received much research attention since they outperform alternative approaches, such as GANs, and currently provide state-of-the-art generative performance. The superior performance of diffusion models has made them an appealing tool in several applications, including inpainting, super-resolution, and semantic editing. In this paper, we demonstrate that diffusion models can also serve as an instrument for semantic segmentation, especially in the setup when labeled data is scarce. In particular, for several pretrained diffusion models, we investigate the intermediate activations from the networks that perform the Markov step of the reverse diffusion process. We show that these activations effectively capture the semantic information from an input image and appear to be excellent pixel-level representations for the segmentation problem. Based on these observations, we describe a simple segmentation method, which can work even if only a few training images are provided. Our approach significantly outperforms the existing alternatives on several datasets for the same amount of human supervision.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Baranchuk-Label-Efficient%20Semantic%20Segmentation%20with%20Diffusion%20Models-2022-ICLR.pdf}
}
@article{AdaptiveBolukbasiICML2017, 
year = {2017}, 
title = {{Adaptive Neural Networks for Efficient Inference}}, 
author = {Bolukbasi, Tolga and Wang, Joseph and Dekel, Ofer and Saligrama, Venkatesh}, 
journal = {ICML}, 
eprint = {1702.07811}, 
abstract = {{We present an approach to adaptively utilize deep neural networks in order to reduce the evaluation time on new examples without loss of accuracy. Rather than attempting to redesign or approximate existing networks, we propose two schemes that adaptively utilize networks. We first pose an adaptive network evaluation scheme, where we learn a system to adaptively choose the components of a deep network to be evaluated for each example. By allowing examples correctly classified using early layers of the system to exit, we avoid the computational time associated with full evaluation of the network. We extend this to learn a network selection system that adaptively selects the network to be evaluated for each example. We show that computational time can be dramatically reduced by exploiting the fact that many examples can be correctly classified using relatively efficient networks and that complex, computationally costly networks are only necessary for a small fraction of examples. We pose a global objective for learning an adaptive early exit or network selection policy and solve it by reducing the policy learning problem to a layer-by-layer weighted binary classification problem. Empirically, these approaches yield dramatic reductions in computational cost, with up to a 2.8x speedup on state-of-the-art networks from the ImageNet image recognition challenge with minimal (<1\%) loss of top5 accuracy.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bolukbasi-Adaptive%20Neural%20Networks%20for%20Efficient%20Inference-2017-ICML.pdf}
}
@article{AnytimeKarayevCVPR2014, 
year = {2014}, 
title = {{Anytime Recognition of Objects and Scenes}}, 
author = {Karayev, Sergey and Fritz, Mario and Darrell, Trevor}, 
journal = {CVPR}, 
abstract = {{Humans are capable of perceiving a scene at a glance, and obtain deeper understanding with additional time. Similarly, visual recognition deployments should be robust to varying computational budgets. Such situations require Anytime recognition ability, which is rarely considered in computer vision research. We present a method for learning dynamic policies to optimize Anytime performance in visual architectures. Our model sequentially orders feature computation and performs subsequent classification. Crucially, decisions are made at test time and depend on observed data and intermediate results. We show the applicability of this system to standard problems in scene and object recognition. On suitable datasets, we can incorporate a semantic back-off strategy that gives maximally specific predictions for a desired level of accuracy; this provides a new view on the time course of human visual perception.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Karayev-Anytime%20Recognition%20of%20Objects%20and%20Scenes-2014-CVPR.pdf}
}
@article{RelatIFBarshanAISTATS2020, 
year = {2020}, 
title = {{RelatIF: Identifying Explanatory Training Examples via Relative Influence}}, 
author = {Barshan, Elnaz and Brunet, Marc-Etienne and Dziugaite, Gintare Karolina}, 
journal = {AISTATS}, 
eprint = {2003.11630}, 
abstract = {{In this work, we focus on the use of influence functions to identify relevant training examples that one might hope "explain" the predictions of a machine learning model. One shortcoming of influence functions is that the training examples deemed most "influential" are often outliers or mislabelled, making them poor choices for explanation. In order to address this shortcoming, we separate the role of global versus local influence. We introduce RelatIF, a new class of criteria for choosing relevant training examples by way of an optimization objective that places a constraint on global influence. RelatIF considers the local influence that an explanatory example has on a prediction relative to its global effects on the model. In empirical evaluations, we find that the examples returned by RelatIF are more intuitive when compared to those found using influence functions.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Barshan-RelatIF-%20Identifying%20Explanatory%20Training%20Examples%20via%20Relative%20Influence-2020-AISTATS.pdf}
}
@article{ConvNet-AIGVeitECCV2018, 
year = {2018}, 
title = {{Convolutional Networks with Adaptive Inference Graphs}}, 
author = {Veit, Andreas and Belongie, Serge}, 
journal = {ECCV}, 
eprint = {1711.11503}, 
abstract = {{Do convolutional networks really need a fixed feed-forward structure? What if, after identifying the high-level concept of an image, a network could move directly to a layer that can distinguish fine-grained differences? Currently, a network would first need to execute sometimes hundreds of intermediate layers that specialize in unrelated aspects. Ideally, the more a network already knows about an image, the better it should be at deciding which layer to compute next. In this work, we propose convolutional networks with adaptive inference graphs (ConvNet-AIG) that adaptively define their network topology conditioned on the input image. Following a high-level structure similar to residual networks (ResNets), ConvNet-AIG decides for each input image on the fly which layers are needed. In experiments on ImageNet we show that ConvNet-AIG learns distinct inference graphs for different categories. Both ConvNet-AIG with 50 and 101 layers outperform their ResNet counterpart, while using 20\% and 38\% less computations respectively. By grouping parameters into layers for related classes and only executing relevant layers, ConvNet-AIG improves both efficiency and overall classification quality. Lastly, we also study the effect of adaptive inference graphs on the susceptibility towards adversarial examples. We observe that ConvNet-AIG shows a higher robustness than ResNets, complementing other known defense mechanisms.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Veit-Convolutional%20Networks%20with%20Adaptive%20Inference%20Graphs-2018-ECCV.pdf}
}
@article{LSegLiICLR2022, 
year = {2022}, 
title = {{Language-driven Semantic Segmentation}}, 
author = {Li, Boyi and Weinberger, Kilian Q and Belongie, Serge and Koltun, Vladlen and Ranftl, René}, 
journal = {ICLR}, 
eprint = {2201.03546}, 
abstract = {{We present LSeg, a novel model for language-driven semantic image segmentation. LSeg uses a text encoder to compute embeddings of descriptive input labels (e.g., "grass" or "building") together with a transformer-based image encoder that computes dense per-pixel embeddings of the input image. The image encoder is trained with a contrastive objective to align pixel embeddings to the text embedding of the corresponding semantic class. The text embeddings provide a flexible label representation in which semantically similar labels map to similar regions in the embedding space (e.g., "cat" and "furry"). This allows LSeg to generalize to previously unseen categories at test time, without retraining or even requiring a single additional training sample. We demonstrate that our approach achieves highly competitive zero-shot performance compared to existing zero- and few-shot semantic segmentation methods, and even matches the accuracy of traditional segmentation algorithms when a fixed label set is provided. Code and demo are available at https://github.com/isl-org/lang-seg.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Language-driven%20Semantic%20Segmentation-2022-ICLR.pdf}
}
@article{LearningHuAAAI2019, 
year = {2019}, 
title = {{Learning Anytime Predictions in Neural Networks via Adaptive Loss Balancing}}, 
author = {Hu, Hanzhang and Dey, Debadeepta and Hebert, Martial and Bagnell, J Andrew}, 
journal = {AAAI}, 
abstract = {{This work considers the trade-off between accuracy and testtime computational cost of deep neural networks (DNNs) via anytime predictions from auxiliary predictions. Specifically, we optimize auxiliary losses jointly in an adaptive weighted sum, where the weights are inversely proportional to average of each loss. Intuitively, this balances the losses to have the same scale. We demonstrate theoretical considerations that motivate this approach from multiple viewpoints, including connecting it to optimizing the geometric mean of the expectation of each loss, an objective that ignores the scale of losses. Experimentally, the adaptive weights induce more competitive anytime predictions on multiple recognition data-sets and models than non-adaptive approaches including weighing all losses equally. In particular, anytime neural networks (ANNs) can achieve the same accuracy faster using adaptive weights on a small network than using static constant weights on a large one. For problems with high performance saturation, we also show a sequence of exponentially deepening ANNs can achieve near-optimal anytime results at any budget, at the cost of a const fraction of extra computation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hu-Learning%20Anytime%20Predictions%20in%20Neural%20Networks%20via%20Adaptive%20Loss%20Balancing-2019-AAAI.pdf}
}
@article{URNetLeeAAAI2020, 
year = {2020}, 
title = {{URNet : User-Resizable Residual Networks with Conditional Gating Module}}, 
author = {Lee, Sang-ho and Chang, Simyung and Kwak, Nojun}, 
journal = {AAAI}, 
eprint = {1901.04687}, 
abstract = {{Convolutional Neural Networks are widely used to process spatial scenes, but their computational cost is fixed and depends on the structure of the network used. There are methods to reduce the cost by compressing networks or varying its computational path dynamically according to the input image. However, since a user can not control the size of the learned model, it is difficult to respond dynamically if the amount of service requests suddenly increases. We propose User-Resizable Residual Networks (URNet), which allows users to adjust the scale of the network as needed during evaluation. URNet includes Conditional Gating Module (CGM) that determines the use of each residual block according to the input image and the desired scale. CGM is trained in a supervised manner using the newly proposed scale loss and its corresponding training methods. URNet can control the amount of computation according to user's demand without degrading the accuracy significantly. It can also be used as a general compression method by fixing the scale size during training. In the experiments on ImageNet, URNet based on ResNet-101 maintains the accuracy of the baseline even when resizing it to approximately 80\% of the original network, and demonstrates only about 1\% accuracy degradation when using about 65\% of the computation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lee-URNet%20-%20User-Resizable%20Residual%20Networks%20with%20Conditional%20Gating%20Module-2020-AAAI.pdf}
}
@article{DFSShenAAAI2020, 
year = {2020}, 
title = {{Fractional Skipping: Towards Finer-Grained Dynamic CNN Inference}}, 
author = {Shen, Jianghao and Fu, Yonggan and Wang, Yue and Xu, Pengfei and Wang, Zhangyang and Lin, Yingyan}, 
journal = {AAAI}, 
eprint = {2001.00705}, 
abstract = {{While increasingly deep networks are still in general desired for achieving state-of-the-art performance, for many specific inputs a simpler network might already suffice. Existing works exploited this observation by learning to skip convolutional layers in an input-dependent manner. However, we argue their binary decision scheme, i.e., either fully executing or completely bypassing one layer for a specific input, can be enhanced by introducing finer-grained, "softer" decisions. We therefore propose a Dynamic Fractional Skipping (DFS) framework. The core idea of DFS is to hypothesize layer-wise quantization (to different bitwidths) as intermediate "soft" choices to be made between fully utilizing and skipping a layer. For each input, DFS dynamically assigns a bitwidth to both weights and activations of each layer, where fully executing and skipping could be viewed as two "extremes" (i.e., full bitwidth and zero bitwidth). In this way, DFS can "fractionally" exploit a layer's expressive power during input-adaptive inference, enabling finer-grained accuracy-computational cost trade-offs. It presents a unified view to link input-adaptive layer skipping and input-adaptive hybrid quantization. Extensive experimental results demonstrate the superior tradeoff between computational cost and model expressive power (accuracy) achieved by DFS. More visualizations also indicate a smooth and consistent transition in the DFS behaviors, especially the learned choices between layer skipping and different quantizations when the total computational budgets vary, validating our hypothesis that layer quantization could be viewed as intermediate variants of layer skipping. Our source code and supplementary material are available at \textbackslashlink\{https://github.com/Torment123/DFS\}.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Shen-Fractional%20Skipping-%20Towards%20Finer-Grained%20Dynamic%20CNN%20Inference-2020-AAAI.pdf}
}
@article{DifferentiableTschiatschekIJCAI2018, 
year = {2018}, 
title = {{Differentiable Submodular Maximization}}, 
author = {Tschiatschek, Sebastian and Sahin, Aytunc and Krause, Andreas}, 
journal = {IJCAI}, 
eprint = {1803.01785}, 
abstract = {{We consider learning of submodular functions from data. These functions are important in machine learning and have a wide range of applications, e.g. data summarization, feature selection and active learning. Despite their combinatorial nature, submodular functions can be maximized approximately with strong theoretical guarantees in polynomial time. Typically, learning the submodular function and optimization of that function are treated separately, i.e. the function is first learned using a proxy objective and subsequently maximized. In contrast, we show how to perform learning and optimization jointly. By interpreting the output of greedy maximization algorithms as distributions over sequences of items and smoothening these distributions, we obtain a differentiable objective. In this way, we can differentiate through the maximization algorithms and optimize the model to work well with the optimization algorithm. We theoretically characterize the error made by our approach, yielding insights into the tradeoff of smoothness and accuracy. We demonstrate the effectiveness of our approach for jointly learning and optimizing on synthetic maximum cut data, and on real world applications such as product recommendation and image collection summarization.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tschiatschek-Differentiable%20Submodular%20Maximization-2018-IJCAI.pdf}
}
@article{DDPMHoNeurIPS2020, 
year = {2020}, 
title = {{Denoising Diffusion Probabilistic Models}}, 
author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter}, 
journal = {NeurIPS}, 
eprint = {2006.11239}, 
abstract = {{We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ho-Denoising%20Diffusion%20Probabilistic%20Models-2020-NeurIPS.pdf}
}
@article{WDPruningYuAAAI2022, 
year = {2022}, 
title = {{Width \& Depth Pruning for Vision Transformer}}, 
author = {Yu, Fang and Huang, Kun and Wang, Meng and Cheng, Yuan and Chu, Wei and Cui, Li}, 
journal = {AAAI}, 
abstract = {{Transformer models have demonstrated their promising po- tential and achieved excellent performance on a series of
computer vision tasks. However, the huge computational cost
of vision transformers hinders their deployment and application to edge devices. Recent works have proposed to find
and remove the unimportant units of vision transformers. De-
spite achieving remarkable results, these methods take one dimension of network width into consideration and ignore net-
work depth, which is another important dimension for pruning vision transformers. Therefore, we propose a Width \&
Depth Pruning (WDPruning) framework that reduces both
width and depth dimensions simultaneously. Specifically, for
width pruning, a set of learnable pruning-related parameters
is used to adaptively adjust the width of transformer. For
depth pruning, we introduce several shallow classifiers by using the intermediate information of the transformer blocks,
which allows images to be classified by shallow classifiers
instead of the deeper classifiers. In the inference period, all
of the blocks after shallow classifiers can be dropped so they
don’t bring additional parameters and computation. Experimental results on benchmark datasets demonstrate that the
proposed method can significantly reduce the computational
costs of mainstream vision transformers such as DeiT and
Swin Transformer with a minor accuracy drop. In particular,
on ILSVRC-12, we achieve over 22\% pruning ratio of FLOPs
by compressing DeiT-Base, even with an increase of 0.14\%
Top-1 accuracy.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yu-Width%20&%20Depth%20Pruning%20for%20Vision%20Transformer-2022-AAAI.pdf}
}
@article{DIVNETMarietICLR2016, 
year = {2016}, 
title = {{Diversity Networks: Neural Network Compression Using Determinantal Point Processes}}, 
author = {Mariet, Zelda and Sra, Suvrit}, 
journal = {ICLR}, 
eprint = {1511.05077}, 
abstract = {{We introduce Divnet, a flexible technique for learning networks with diverse neurons. Divnet models neuronal diversity by placing a Determinantal Point Process (DPP) over neurons in a given layer. It uses this DPP to select a subset of diverse neurons and subsequently fuses the redundant neurons into the selected ones. Compared with previous approaches, Divnet offers a more principled, flexible technique for capturing neuronal diversity and thus implicitly enforcing regularization. This enables effective auto-tuning of network architecture and leads to smaller network sizes without hurting performance. Moreover, through its focus on diversity and neuron fusing, Divnet remains compatible with other procedures that seek to reduce memory footprints of networks. We present experimental results to corroborate our claims: for pruning neural networks, Divnet is seen to be notably superior to competing approaches.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Mariet-Diversity%20Networks-%20Neural%20Network%20Compression%20Using%20Determinantal%20Point%20Processes-2016-ICLR.pdf}
}
@article{DPPMLKuleszaarXiv2012, 
year = {2012}, 
title = {{Determinantal point processes for machine learning}}, 
author = {Kulesza, Alex and Taskar, Ben}, 
journal = {arXiv}, 
eprint = {1207.6083}, 
abstract = {{Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that arise in quantum physics and random matrix theory. In contrast to traditional structured models like Markov random fields, which become intractable and hard to approximate in the presence of negative correlations, DPPs offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. We provide a gentle introduction to DPPs, focusing on the intuitions, algorithms, and extensions that are most relevant to the machine learning community, and show how DPPs can be applied to real-world applications like finding diverse sets of high-quality search results, building informative summaries by selecting diverse sentences from documents, modeling non-overlapping human poses in images or video, and automatically building timelines of important news stories.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kulesza-Determinantal%20point%20processes%20for%20machine%20learning-2012-arXiv.pdf}
}
@article{SPPIyerAISTATS2015, 
year = {2015}, 
title = {{Submodular Point Processes with Applications to Machine Learning}}, 
author = {Iyer, Rishabh and Bilmes, Jeff}, 
journal = {AISTATS}, 
abstract = {{We introduce a class of discrete point processes that we call the Submodular Point Processes (SPPs). These processes are characterized via a submodular (or supermodular) function, and naturally model notions of information, coverage and diversity, as well as cooperation. Unlike Log-submodular and Log-supermodular distributions (Log-SPPs) such as determinantal point processes (DPPs), SPPs are themselves submodular (or super-modular). In this paper, we analyze the computational complexity of probabilistic inference in SPPs. We show that computing the partition function for SPPs (and Log-SPPs), requires exponential complexity in the worst
case, and also provide algorithms which approximate SPPs up to polynomial factors. Moreover, for several subclasses of interesting
submodular functions that occur in applications, we show how we can provide efficient closed form expressions for the partition functions, and thereby marginals and conditional distributions. We also show how SPPs are closed under mixtures, thus enabling maximum likelihood based strategies for learning mixtures of submodular functions. Finally, we argue how SPPs complement existing Log-SPP distributions, and are a natural model for several applications.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Iyer-Submodular%20Point%20Processes%20with%20Applications%20to%20Machine%20Learning-2015-AISTATS.pdf}
}
@article{DifferentiableDjolongaNeurIPS2017, 
year = {2017}, 
title = {{Differentiable Learning of Submodular Models}}, 
author = {Djolonga, Josip and Krause, Andreas}, 
journal = {NeurIPS}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Djolonga-Differentiable%20Learning%20of%20Submodular%20Models-2017-NeurIPS.pdf}
}
@article{EncoderRannenICCV2017, 
year = {2017}, 
title = {{Encoder Based Lifelong Learning}}, 
author = {Rannen, Amal and Aljundi, Rahaf and Blaschko, Matthew B. and Tuytelaars, Tinne}, 
journal = {ICCV}, 
eprint = {1704.01920}, 
abstract = {{This paper introduces a new lifelong learning solution where a single model is trained for a sequence of tasks. The main challenge that vision systems face in this context is catastrophic forgetting: as they tend to adapt to the most recently seen task, they lose performance on the tasks that were learned previously. Our method aims at preserving the knowledge of the previous tasks while learning a new one by using autoencoders. For each task, an under-complete autoencoder is learned, capturing the features that are crucial for its achievement. When a new task is presented to the system, we prevent the reconstructions of the features with these autoencoders from changing, which has the effect of preserving the information on which the previous tasks are mainly relying. At the same time, the features are given space to adjust to the most recent environment as only their projection into a low dimension submanifold is controlled. The proposed system is evaluated on image classification tasks and shows a reduction of forgetting over the state-of-the-art.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Rannen-Encoder%20Based%20Lifelong%20Learning-2017-ICCV.pdf}
}
@article{SubmodularItoNeurIPS2019, 
year = {2019}, 
title = {{Submodular Function Minimization with Noisy Evaluation Oracle}}, 
author = {Ito, Shinji}, 
journal = {NeurIPS}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ito-Submodular%20Function%20Minimization%20with%20Noisy%20Evaluation%20Oracle-2019-NeurIPS.pdf}
}
@article{AGeneralizationSomaNeurIPS2015, 
year = {2015}, 
title = {{A Generalization of Submodular Cover via the Diminishing Return Property on the Integer Lattice}}, 
author = {Soma, Tasuku and Yoshida, Yuichi}, 
journal = {NeurIPS}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Soma-A%20Generalization%20of%20Submodular%20Cover%20via%20the%20Diminishing%20Return%20Property%20on%20the%20Integer%20Lattice-2015-NeurIPS.pdf}
}
@article{ContinuousDR-submodularBianNeurIPS2017, 
year = {2017}, 
title = {{Continuous DR-submodular Maximization: Structure and Algorithms}}, 
author = {Bian, An and Levy, Kfir Y and Krause, Andreas and Buhmann, Joachim M}, 
journal = {NeurIPS}, 
eprint = {1711.02515}, 
abstract = {{DR-submodular continuous functions are important objectives with wide real-world applications spanning MAP inference in determinantal point processes (DPPs), and mean-field inference for probabilistic submodular models, amongst others. DR-submodularity captures a subclass of non-convex functions that enables both exact minimization and approximate maximization in polynomial time. In this work we study the problem of maximizing non-monotone DR-submodular continuous functions under general down-closed convex constraints. We start by investigating geometric properties that underlie such objectives, e.g., a strong relation between (approximately) stationary points and global optimum is proved. These properties are then used to devise two optimization algorithms with provable guarantees. Concretely, we first devise a "two-phase" algorithm with \$1/4\$ approximation guarantee. This algorithm allows the use of existing methods for finding (approximately) stationary points as a subroutine, thus, harnessing recent progress in non-convex optimization. Then we present a non-monotone Frank-Wolfe variant with \$1/e\$ approximation guarantee and sublinear convergence rate. Finally, we extend our approach to a broader class of generalized DR-submodular continuous functions, which captures a wider spectrum of applications. Our theoretical findings are validated on synthetic and real-world problem instances.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bian-Continuous%20DR-submodular%20Maximization-%20Structure%20and%20Algorithms-2017-NeurIPS.pdf}
}
@article{GuaranteedBianAISTATS2017, 
year = {2017}, 
title = {{Guaranteed Non-convex Optimization: Submodular Maximization over Continuous Domains}}, 
author = {Bian, Andrew An and Mirzasoleiman, Baharan and Buhmann, Joachim M and Krause, Andreas}, 
journal = {AISTATS}, 
eprint = {1606.05615}, 
abstract = {{Submodular continuous functions are a category of (generally) non-convex/non-concave functions with a wide spectrum of applications. We characterize these functions and demonstrate that they can be maximized efficiently with approximation guarantees. Specifically, i) We introduce the weak DR property that gives a unified characterization of submodularity for all set, integer-lattice and continuous functions; ii) for maximizing monotone DR-submodular continuous functions under general down-closed convex constraints, we propose a Frank-Wolfe variant with \$(1-1/e)\$ approximation guarantee, and sub-linear convergence rate; iii) for maximizing general non-monotone submodular continuous functions subject to box constraints, we propose a DoubleGreedy algorithm with \$1/3\$ approximation guarantee. Submodular continuous functions naturally find applications in various real-world settings, including influence and revenue maximization with continuous assignments, sensor energy management, multi-resolution data summarization, facility location, etc. Experimental results show that the proposed algorithms efficiently generate superior solutions compared to baseline algorithms.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bian-Guaranteed%20Non-convex%20Optimization-%20Submodular%20Maximization%20over%20Continuous%20Domains-2017-AISTATS.pdf}
}
@article{OnlineChenAISTATS2018, 
year = {2018}, 
title = {{Online Continuous Submodular Maximization}}, 
author = {Chen, Lin and Hassani, Hamed and Karbasi, Amin}, 
journal = {AISTATS}, 
eprint = {1802.06052}, 
abstract = {{In this paper, we consider an online optimization process, where the objective functions are not convex (nor concave) but instead belong to a broad class of continuous submodular functions. We first propose a variant of the Frank-Wolfe algorithm that has access to the full gradient of the objective functions. We show that it achieves a regret bound of \$O(\textbackslashsqrt\{T\})\$ (where \$T\$ is the horizon of the online optimization problem) against a \$(1-1/e)\$-approximation to the best feasible solution in hindsight. However, in many scenarios, only an unbiased estimate of the gradients are available. For such settings, we then propose an online stochastic gradient ascent algorithm that also achieves a regret bound of \$O(\textbackslashsqrt\{T\})\$ regret, albeit against a weaker \$1/2\$-approximation to the best feasible solution in hindsight. We also generalize our results to \$\textbackslashgamma\$-weakly submodular functions and prove the same sublinear regret bounds. Finally, we demonstrate the efficiency of our algorithms on a few problem instances, including non-convex/non-concave quadratic programs, multilinear extensions of submodular set functions, and D-optimal design.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Online%20Continuous%20Submodular%20Maximization-2018-AISTATS.pdf}
}
@article{GradientHassaniNeurIPS2017, 
year = {2017}, 
title = {{Gradient Methods for Submodular Maximization}}, 
author = {Hassani, Hamed and Soltanolkotabi, Mahdi and Karbasi, Amin}, 
journal = {NeurIPS}, 
eprint = {1708.03949}, 
abstract = {{In this paper, we study the problem of maximizing continuous submodular functions that naturally arise in many learning applications such as those involving utility functions in active learning and sensing, matrix approximations and network inference. Despite the apparent lack of convexity in such functions, we prove that stochastic projected gradient methods can provide strong approximation guarantees for maximizing continuous submodular functions with convex constraints. More specifically, we prove that for monotone continuous DR-submodular functions, all fixed points of projected gradient ascent provide a factor \$1/2\$ approximation to the global maxima. We also study stochastic gradient and mirror methods and show that after \$\textbackslashmathcal\{O\}(1/\textbackslashepsilon\textasciicircum2)\$ iterations these methods reach solutions which achieve in expectation objective values exceeding \$(\textbackslashfrac\{\textbackslashtext\{OPT\}\}\{2\}-\textbackslashepsilon)\$. An immediate application of our results is to maximize submodular functions that are defined stochastically, i.e. the submodular function is defined as an expectation over a family of submodular functions with an unknown distribution. We will show how stochastic gradient methods are naturally well-suited for this setting, leading to a factor \$1/2\$ approximation when the function is monotone. In particular, it allows us to approximately maximize discrete, monotone submodular optimization problems via projected gradient descent on a continuous relaxation, directly connecting the discrete and continuous domains. Finally, experiments on real data demonstrate that our projected gradient methods consistently achieve the best utility compared to other continuous baselines while remaining competitive in terms of computational effort.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hassani-Gradient%20Methods%20for%20Submodular%20Maximization-2017-NeurIPS.pdf}
}
@article{ContinuousFeldmanNeurIPS2020, 
year = {2020}, 
title = {{Continuous Submodular Maximization: Beyond DR-Submodularity}}, 
author = {Feldman, Moran and Karbasi, Amin}, 
journal = {NeurIPS}, 
eprint = {2006.11726}, 
abstract = {{In this paper, we propose the first continuous optimization algorithms that achieve a constant factor approximation guarantee for the problem of monotone continuous submodular maximization subject to a linear constraint. We first prove that a simple variant of the vanilla coordinate ascent, called Coordinate-Ascent+, achieves a \$(\textbackslashfrac\{e-1\}\{2e-1\}-\textbackslashvarepsilon)\$-approximation guarantee while performing \$O(n/\textbackslashvarepsilon)\$ iterations, where the computational complexity of each iteration is roughly \$O(n/\textbackslashsqrt\{\textbackslashvarepsilon\}+n\textbackslashlog n)\$ (here, \$n\$ denotes the dimension of the optimization problem). We then propose Coordinate-Ascent++, that achieves the tight \$(1-1/e-\textbackslashvarepsilon)\$-approximation guarantee while performing the same number of iterations, but at a higher computational complexity of roughly \$O(n\textasciicircum3/\textbackslashvarepsilon\textasciicircum\{2.5\} + n\textasciicircum3 \textbackslashlog n / \textbackslashvarepsilon\textasciicircum2)\$ per iteration. However, the computation of each round of Coordinate-Ascent++ can be easily parallelized so that the computational cost per machine scales as \$O(n/\textbackslashsqrt\{\textbackslashvarepsilon\}+n\textbackslashlog n)\$.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Feldman-Continuous%20Submodular%20Maximization-%20Beyond%20DR-Submodularity-2020-NeurIPS.pdf}
}
@article{Data-EfficientHalabiNeurIPS2022, 
year = {2022}, 
title = {{Data-Efficient Structured Pruning via Submodular Optimization}}, 
author = {Halabi, Marwa El and Srinivas, Suraj and Lacoste-Julien, Simon}, 
journal = {NeurIPS}, 
eprint = {2203.04940}, 
abstract = {{Structured pruning is an effective approach for compressing large pre-trained neural networks without significantly affecting their performance, which involves removing redundant regular regions of weights. However, current structured pruning methods are highly empirical in nature, do not provide any theoretical guarantees, and often require fine-tuning, which makes them inapplicable in the limited-data regime. We propose a principled data-efficient structured pruning method based on submodular optimization. In particular, for a given layer, we select neurons/channels to prune and corresponding new weights for the next layer, that minimize the change in the next layer's input induced by pruning. We show that this selection problem is a weakly submodular maximization problem, thus it can be provably approximated using an efficient greedy algorithm. Our method is one of the few in the literature that uses only a limited-number of training data and no labels. Our experimental results demonstrate that our method outperforms popular baseline methods in various one-shot pruning settings.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Halabi-Data-Efficient%20Structured%20Pruning%20via%20Submodular%20Optimization-2022-arXiv.pdf}
}
@article{StochasticKarimiNeurIPS2017, 
year = {2017}, 
title = {{Stochastic Submodular Maximization: The Case of Coverage Functions}}, 
author = {Karimi, Mohammad Reza and Lucic, Mario and Hassani, Hamed and Krause, Andreas}, 
journal = {NeurIPS}, 
eprint = {1711.01566}, 
abstract = {{Stochastic optimization of continuous objectives is at the heart of modern machine learning. However, many important problems are of discrete nature and often involve submodular objectives. We seek to unleash the power of stochastic continuous optimization, namely stochastic gradient descent and its variants, to such discrete problems. We first introduce the problem of stochastic submodular optimization, where one needs to optimize a submodular objective which is given as an expectation. Our model captures situations where the discrete objective arises as an empirical risk (e.g., in the case of exemplar-based clustering), or is given as an explicit stochastic model (e.g., in the case of influence maximization in social networks). By exploiting that common extensions act linearly on the class of submodular functions, we employ projected stochastic gradient ascent and its variants in the continuous domain, and perform rounding to obtain discrete solutions. We focus on the rich and widely used family of weighted coverage functions. We show that our approach yields solutions that are guaranteed to match the optimal approximation guarantees, while reducing the computational cost by several orders of magnitude, as we demonstrate empirically.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Karimi-Stochastic%20Submodular%20Maximization-%20The%20Case%20of%20Coverage%20Functions-2017-NeurIPS.pdf}
}
@article{MultivariateSantiagoICML2019, 
year = {2019}, 
title = {{Multivariate Submodular Optimization}}, 
author = {Santiago, Richard and Shepherd, F Bruce}, 
journal = {ICML}, 
eprint = {1612.05222}, 
abstract = {{Submodular functions have found a wealth of new applications in data science and machine learning models in recent years. This has been coupled with many algorithmic advances in the area of submodular optimization: (SO) \$\textbackslashmin/\textbackslashmax\textbackslashtextasciitildef(S): S \textbackslashin \textbackslashmathcal\{F\}\$, where \$\textbackslashmathcal\{F\}\$ is a given family of feasible sets over a ground set \$V\$ and \$f:2\textasciicircumV \textbackslashrightarrow \textbackslashmathbb\{R\}\$ is submodular. In this work we focus on a more general class of \textbackslashemph\{multivariate submodular optimization\} (MVSO) problems: \$\textbackslashmin/\textbackslashmax\textbackslashtextasciitildef (S\_1,S\_2,\textbackslashldots,S\_k): S\_1 \textbackslashuplus S\_2 \textbackslashuplus \textbackslashcdots \textbackslashuplus S\_k \textbackslashin \textbackslashmathcal\{F\}\$. Here we use \$\textbackslashuplus\$ to denote disjoint union and hence this model is attractive where resources are being allocated across \$k\$ agents, who share a `joint' multivariate nonnegative objective \$f(S\_1,S\_2,\textbackslashldots,S\_k)\$ that captures some type of submodularity (i.e. diminishing returns) property. We provide some explicit examples and potential applications for this new framework. For maximization, we show that practical algorithms such as accelerated greedy variants and distributed algorithms achieve good approximation guarantees for very general families (such as matroids and \$p\$-systems). For arbitrary families, we show that monotone (resp. nonmonotone) MVSO admits an \$\textbackslashalpha (1-1/e)\$ (resp. \$\textbackslashalpha \textbackslashcdot 0.385\$) approximation whenever monotone (resp. nonmonotone) SO admits an \$\textbackslashalpha\$-approximation over the multilinear formulation. This substantially expands the family of tractable models for submodular maximization. For minimization, we show that if SO admits a \$\textbackslashbeta\$-approximation over \textbackslashemph\{modular\} functions, then MVSO admits a \$\textbackslashfrac\{\textbackslashbeta \textbackslashcdot n\}\{1+(n-1)(1-c)\}\$-approximation where \$c\textbackslashin [0,1]\$ denotes the curvature of \$f\$, and this is essentially tight. Finally, we prove that MVSO has an \$\textbackslashalpha k\$-approximation whenever SO admits an \$\textbackslashalpha\$-approximation over the convex formulation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Santiago-Multivariate%20Submodular%20Optimization-2019-ICML_1.pdf}
}
@article{SubmodularBachMP2019, 
year = {2019}, 
title = {{Submodular Functions: from Discrete to Continous Domains}}, 
author = {Bach, Francis}, 
journal = {Mathematical Programming}, 
eprint = {1511.00394}, 
abstract = {{Submodular set-functions have many applications in combinatorial optimization, as they can be minimized and approximately maximized in polynomial time. A key element in many of the algorithms and analyses is the possibility of extending the submodular set-function to a convex function, which opens up tools from convex optimization. Submodularity goes beyond set-functions and has naturally been considered for problems with multiple labels or for functions defined on continuous domains, where it corresponds essentially to cross second-derivatives being nonpositive. In this paper, we show that most results relating submodularity and convexity for set-functions can be extended to all submodular functions. In particular, (a) we naturally define a continuous extension in a set of probability measures, (b) show that the extension is convex if and only if the original function is submodular, (c) prove that the problem of minimizing a submodular function is equivalent to a typically non-smooth convex optimization problem, and (d) propose another convex optimization problem with better computational properties (e.g., a smooth dual problem). Most of these extensions from the set-function situation are obtained by drawing links with the theory of multi-marginal optimal transport, which provides also a new interpretation of existing results for set-functions. We then provide practical algorithms to minimize generic submodular functions on discrete domains, with associated convergence rates.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bach-Submodular%20Functions-%20from%20Discrete%20to%20Continous%20Domains-2019-Mathematical%20Programming.pdf}
}
@article{CBLossCuiCVPR2019, 
year = {2019}, 
title = {{Class-Balanced Loss Based on Effective Number of Samples}}, 
author = {Cui, Yin and Jia, Menglin and Lin, Tsung-Yi and Song, Yang and Belongie, Serge}, 
journal = {CVPR}, 
abstract = {{With the rapid increase of large-scale, real-world datasets, it becomes critical to address the problem of long-tailed data distribution (i.e., a few classes account for most of the data, while most classes are under-represented). Existing solutions typically adopt class re-balancing strategies such as re-sampling and re-weighting based on the number of observations for each class. In this work, we argue that as the number of samples increases, the additional benefit of a newly added data point will diminish. We introduce a novel theoretical framework to measure data overlap by associating with each sample a small neighboring region rather than a single point. The effective number of samples is defined as the volume of samples and can be calculated by a simple formula (1−βn/(1−β), where n is the of number samples and β ∈ [0, 1) is a hyperparameter. We design a re-weighting scheme that uses the effective number of samples for each class to re-balance the loss, thereby yielding a class-balanced loss. Comprehensive experiments are conducted on artificially induced long-tailed CIFAR datasets and large-scale datasets including ImageNet and iNaturalist. Our results show that when trained with the proposed class-balanced loss, the network is able to achieve significant performance gains on long-tailed datasets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Cui-Class-Balanced%20Loss%20Based%20on%20Effective%20Number%20of%20Samples-2019-CVPR.pdf}
}
@article{DY-CNNChenCVPR2020, 
year = {2020}, 
title = {{Dynamic Convolution: Attention Over Convolution Kernels}}, 
author = {Chen, Yinpeng and Dai, Xiyang and Liu, Mengchen and Chen, Dongdong and Yuan, Lu and Liu, Zicheng}, 
journal = {CVPR}, 
abstract = {{Light-weight convolutional neural networks (CNNs) suffer performance degradation as their low computational budgets constrain both the depth (number of convolution layers) and the width (number of channels) of CNNs, resulting in limited representation capability. To address this issue, we present Dynamic Convolution, a new design that increases model complexity without increasing the network depth or width. Instead of using a single convolution kernel per layer, dynamic convolution aggregates multiple parallel convolution kernels dynamically based upon their attentions, which are input dependent. Assembling multiple kernels is not only computationally efficient due to the small kernel size, but also has more representation power since these kernels are aggregated in a non-linear way via attention. By simply using dynamic convolution for the state-of-the-art architecture MobileNetV3-Small, the top-1 accuracy of ImageNet classification is boosted by 2.9\% with only 4\% additional FLOPs and 2.9 AP gain is achieved on COCO keypoint detection.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Dynamic%20Convolution-%20Attention%20Over%20Convolution%20Kernels-2020-CVPR.pdf}
}
@article{Don'tBaiICML2021, 
year = {2021}, 
title = {{Don't Just Blame Over-parametrization for Over-confidence: Theoretical Analysis of Calibration in Binary Classification}}, 
author = {Bai, Yu and Mei, Song and Wang, Huan and Xiong, Caiming}, 
journal = {ICML}, 
eprint = {2102.07856}, 
abstract = {{Modern machine learning models with high accuracy are often miscalibrated -- the predicted top probability does not reflect the actual accuracy, and tends to be over-confident. It is commonly believed that such over-confidence is mainly due to over-parametrization, in particular when the model is large enough to memorize the training data and maximize the confidence. In this paper, we show theoretically that over-parametrization is not the only reason for over-confidence. We prove that logistic regression is inherently over-confident, in the realizable, under-parametrized setting where the data is generated from the logistic model, and the sample size is much larger than the number of parameters. Further, this over-confidence happens for general well-specified binary classification problems as long as the activation is symmetric and concave on the positive part. Perhaps surprisingly, we also show that over-confidence is not always the case -- there exists another activation function (and a suitable loss function) under which the learned classifier is under-confident at some probability values. Overall, our theory provides a precise characterization of calibration in realizable binary classification, which we verify on simulations and real data experiments.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bai-Don't%20Just%20Blame%20Over-parametrization%20for%20Over-confidence-%20Theoretical%20Analysis%20of%20Calibration%20in%20Binary%20Classification-2021-ICML.pdf}
}
@article{OnBisubmodularSinghAISTATS2012, 
year = {2012}, 
title = {{On Bisubmodular Maximization}}, 
author = {Singh, Ajit P. and Guillory, Andrew and Bilmes, Jeff}, 
journal = {AISTATS}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Singh-On%20Bisubmodular%20Maximization-2012-AISTATS.pdf}
}
@article{RethinkingZophNeurIPS2020, 
year = {2020}, 
title = {{Rethinking Pre-training and Self-training}}, 
author = {Zoph, Barret and Ghiasi, Golnaz and Lin, Tsung-Yi and Cui, Yin and Liu, Hanxiao and Cubuk, Ekin D and Le, Quoc V}, 
journal = {NeurIPS}, 
eprint = {2006.06882}, 
abstract = {{Pre-training is a dominant paradigm in computer vision. For example, supervised ImageNet pre-training is commonly used to initialize the backbones of object detection and segmentation models. He et al., however, show a surprising result that ImageNet pre-training has limited impact on COCO object detection. Here we investigate self-training as another method to utilize additional data on the same setup and contrast it against ImageNet pre-training. Our study reveals the generality and flexibility of self-training with three additional insights: 1) stronger data augmentation and more labeled data further diminish the value of pre-training, 2) unlike pre-training, self-training is always helpful when using stronger data augmentation, in both low-data and high-data regimes, and 3) in the case that pre-training is helpful, self-training improves upon pre-training. For example, on the COCO object detection dataset, pre-training benefits when we use one fifth of the labeled data, and hurts accuracy when we use all labeled data. Self-training, on the other hand, shows positive improvements from +1.3 to +3.4AP across all dataset sizes. In other words, self-training works well exactly on the same setup that pre-training does not work (using ImageNet to help COCO). On the PASCAL segmentation dataset, which is a much smaller dataset than COCO, though pre-training does help significantly, self-training improves upon the pre-trained model. On COCO object detection, we achieve 54.3AP, an improvement of +1.5AP over the strongest SpineNet model. On PASCAL segmentation, we achieve 90.5 mIOU, an improvement of +1.5\% mIOU over the previous state-of-the-art result by DeepLabv3+.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zoph-Rethinking%20Pre-training%20and%20Self-training-2020-NeurIPS.pdf}
}
@article{BilevelFranceschiICML2018, 
year = {2018}, 
title = {{Bilevel Programming for Hyperparameter Optimization and Meta-Learning}}, 
author = {Franceschi, Luca and Frasconi, Paolo and Salzo, Saverio and Grazzi, Riccardo and Pontil, Massimilano}, 
journal = {ICML}, 
eprint = {1806.04910}, 
abstract = {{We introduce a framework based on bilevel programming that unifies gradient-based hyperparameter optimization and meta-learning. We show that an approximate version of the bilevel problem can be solved by taking into explicit account the optimization dynamics for the inner objective. Depending on the specific setting, the outer variables take either the meaning of hyperparameters in a supervised learning problem or parameters of a meta-learner. We provide sufficient conditions under which solutions of the approximate problem converge to those of the exact problem. We instantiate our approach for meta-learning in the case of deep learning where representation layers are treated as hyperparameters shared across a set of training episodes. In experiments, we confirm our theoretical findings, present encouraging results for few-shot learning and contrast the bilevel approach against classical approaches for learning-to-learn.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Franceschi-Bilevel%20Programming%20for%20Hyperparameter%20Optimization%20and%20Meta-Learning-2018-ICML.pdf}
}
@article{OnMutualInformationTschannenICLR2020, 
year = {2020}, 
title = {{On Mutual Information Maximization for Representation Learning}}, 
author = {Tschannen, Michael and Djolonga, Josip and Rubenstein, Paul K and Gelly, Sylvain and Lucic, Mario}, 
journal = {ICLR}, 
eprint = {1907.13625}, 
abstract = {{Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tschannen-On%20Mutual%20Information%20Maximization%20for%20Representation%20Learning-2020-ICLR.pdf}
}
@article{LearningLiECCV2016, 
year = {2016}, 
title = {{Learning without Forgetting}}, 
author = {Li, Zhizhong and Hoiem, Derek}, 
journal = {ECCV}, 
eprint = {1606.09282}, 
abstract = {{When building a unified vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Learning%20without%20Forgetting-2016-ECCV.pdf}
}
@article{GIoURezatofighiCVPR2019, 
year = {2019}, 
title = {{Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression}}, 
author = {Rezatofighi, Hamid and Tsoi, Nathan and Gwak, JunYoung and Sadeghian, Amir and Reid, Ian and Savarese, Silvio}, 
journal = {CVPR}, 
abstract = {{Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks. However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value. The optimal objective for a metric is the metric itself. In the case of axis- aligned 2D bounding boxes, it can be shown that IoU can be directly used as a regression loss. However, IoU has a plateau making it infeasible to optimize in the case of nonoverlapping bounding boxes. In this paper, we address the weaknesses of IoU by introducing a generalized version as both a new loss and a new metric. By incorporating this generalized IoU (GIoU) as a loss into the state-of-the art object detection frameworks, we show a consistent improvement on their performance using both the standard, IoU based, and new, GIoU based, performance measures on popular object detection benchmarks such as PASCAL VOC and MS COCO.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Rezatofighi-Generalized%20Intersection%20over%20Union-%20A%20Metric%20and%20A%20Loss%20for%20Bounding%20Box%20Regression-2019-CVPR.pdf}
}
@article{MetaOptNetLeeCVPR2019, 
year = {2019}, 
title = {{Meta-Learning with Differentiable Convex Optimization}}, 
author = {Lee, Kwonjoon and Maji, Subhransu and Ravichandran, Avinash and Soatto, Stefano}, 
journal = {CVPR}, 
abstract = {{Many meta-learning approaches for few-shot learning rely on simple base learners such as nearest-neighbor classifiers. However, even in the few-shot regime, discriminatively trained linear predictors can offer better generalization. We propose to use these predictors as base learners to learn representations for few-shot learning and show they offer better tradeoffs between feature size and performance across a range of few-shot recognition benchmarks. Our objective is to learn feature embeddings that generalize well under a linear classification rule for novel categories. To efficiently solve the objective, we exploit two properties of linear classifiers: implicit differentiation of the optimality conditions of the convex problem and the dual formulation of the optimization problem. This allows us to use high-dimensional embeddings with improved generalization at a modest increase in computational overhead. Our approach, named MetaOptNet, achieves state-of-the-art performance on miniImageNet, tieredImageNet, CIFAR-FS, and FC100 few-shot learning benchmarks. Our code is available on-line1.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lee-Meta-Learning%20with%20Differentiable%20Convex%20Optimization-2019-CVPR.pdf}
}
@article{LearningHuCVPR2018, 
year = {2018}, 
title = {{Learning to Segment Every Thing}}, 
author = {Hu, Ronghang and Dollár, Piotr and He, Kaiming and Darrel, Trevor and Girshick, Ross}, 
journal = {CVPR}, 
abstract = {{Most methods for object instance segmentation require all training examples to be labeled with segmentation masks. This requirement makes it expensive to annotate new categories and has restricted instance segmentation models to \textbackslashtextasciitilde 100 well-annotated classes. The goal of this paper is to propose a new partially supervised training paradigm, together with a novel weight transfer function, that enables training instance segmentation models on a large set of categories all of which have box annotations, but only a small fraction of which have mask annotations. These contributions allow us to train Mask R-CNN to detect and segment 3000 visual concepts using box annotations from the Visual Genome dataset and mask annotations from the 80 classes in the COCO dataset. We evaluate our approach in a controlled study on the COCO dataset. This work is a first step towards instance segmentation models that have broad comprehension of the visual world.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hu-Learning%20to%20Segment%20Every%20Thing-2018-CVPR.pdf}
}
@article{DataDistillationRadosavovicCVPR2018, 
year = {2018}, 
title = {{Data Distillation: Towards Omni-Supervised Learning}}, 
author = {Radosavovic, Ilija and Dollár, Piotr and Girshick, Ross and Gkioxari, Georgia and He, Kaiming}, 
journal = {CVPR}, 
abstract = {{We investigate omni-supervised learning, a special regime of semi-supervised learning in which the learner exploits all available labeled data plus internet-scale sources of unlabeled data. Omni-supervised learning is lower-bounded by performance on existing labeled datasets, offering the potential to surpass state-of-the-art fully supervised methods. To exploit the omni-supervised setting, we propose data distillation, a method that ensembles predictions from multiple transformations of unlabeled data, using a single model, to automatically generate new training annotations. We argue that visual recognition models have recently become accurate enough that it is now possible to apply classic ideas about self-training to challenging realworld data. Our experimental results show that in the cases of human keypoint detection and general object detection, state-of-the-art models trained with data distillation surpass the performance of using labeled data from the COCO dataset alone.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Radosavovic-Data%20Distillation-%20Towards%20Omni-Supervised%20Learning-2018-CVPR.pdf}
}
@article{Data-DrivenHuangECCV2018, 
year = {2018}, 
title = {{Data-Driven Sparse Structure Selection for Deep Neural Networks}}, 
author = {Huang, Zehao and Wang, Naiyan}, 
journal = {ECCV}, 
eprint = {1707.01213}, 
abstract = {{Deep convolutional neural networks have liberated its extraordinary power on various tasks. However, it is still very challenging to deploy state-of-the-art models into real-world applications due to their high computational complexity. How can we design a compact and effective network without massive experiments and expert knowledge? In this paper, we propose a simple and effective framework to learn and prune deep models in an end-to-end manner. In our framework, a new type of parameter -- scaling factor is first introduced to scale the outputs of specific structures, such as neurons, groups or residual blocks. Then we add sparsity regularizations on these factors, and solve this optimization problem by a modified stochastic Accelerated Proximal Gradient (APG) method. By forcing some of the factors to zero, we can safely remove the corresponding structures, thus prune the unimportant parts of a CNN. Comparing with other structure selection methods that may need thousands of trials or iterative fine-tuning, our method is trained fully end-to-end in one training pass without bells and whistles. We evaluate our method, Sparse Structure Selection with several state-of-the-art CNNs, and demonstrate very promising results with adaptive depth and width selection.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Huang-Data-Driven%20Sparse%20Structure%20Selection%20for%20Deep%20Neural%20Networks-2018-ECCV.pdf}
}
@article{PanopticSegmentationKirillovCVPR2019, 
year = {2019}, 
title = {{Panoptic Segmentation}}, 
author = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Dollar, Piotr}, 
journal = {CVPR}, 
abstract = {{We propose and study a task we name panoptic segmentation (PS). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified view of image segmentation. For more analysis and up-to-date results, please check the arXiv version of the paper: https://arxiv.org/abs/1801.00868.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kirillov-Panoptic%20Segmentation-2019-CVPR.pdf}
}
@article{ExploringMahajanECCV2018, 
year = {2018}, 
title = {{Exploring the Limits of Weakly Supervised Pretraining}}, 
author = {Mahajan, Dhruv and Girshick, Ross and Ramanathan, Vignesh and He, Kaiming and Paluri, Manohar and Li, Yixuan and Bharambe, Ashwin and Maaten, Laurens van der}, 
journal = {ECCV}, 
eprint = {1805.00932}, 
abstract = {{State-of-the-art visual perception models for a wide range of tasks rely on supervised pretraining. ImageNet classification is the de facto pretraining task for these models. Yet, ImageNet is now nearly ten years old and is by modern standards "small". Even so, relatively little is known about the behavior of pretraining with datasets that are multiple orders of magnitude larger. The reasons are obvious: such datasets are difficult to collect and annotate. In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4\% (97.6\% top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Mahajan-Exploring%20the%20Limits%20of%20Weakly%20Supervised%20Pretraining-2018-ECCV.pdf}
}
@article{DGMRFSidenICML2020, 
year = {2020}, 
title = {{Deep Gaussian Markov Random Fields}}, 
author = {Sidén, Per and Lindsten, Fredrik}, 
journal = {ICML}, 
eprint = {2002.07467}, 
abstract = {{Gaussian Markov random fields (GMRFs) are probabilistic graphical models widely used in spatial statistics and related fields to model dependencies over spatial structures. We establish a formal connection between GMRFs and convolutional neural networks (CNNs). Common GMRFs are special cases of a generative model where the inverse mapping from data to latent variables is given by a 1-layer linear CNN. This connection allows us to generalize GMRFs to multi-layer CNN architectures, effectively increasing the order of the corresponding GMRF in a way which has favorable computational scaling. We describe how well-established tools, such as autodiff and variational inference, can be used for simple and efficient inference and learning of the deep GMRF. We demonstrate the flexibility of the proposed model and show that it outperforms the state-of-the-art on a dataset of satellite temperatures, in terms of prediction and predictive uncertainty.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Sidén-Deep%20Gaussian%20Markov%20Random%20Fields-2020-ICML.pdf}
}
@article{LearningRenICML2018, 
year = {2018}, 
title = {{Learning to Reweight Examples for Robust Deep Learning}}, 
author = {Ren, Mengye and Zeng, Wenyuan and Yang, Bin and Urtasun, Raquel}, 
journal = {ICML}, 
eprint = {1803.09050}, 
abstract = {{Deep neural networks have been shown to be very powerful modeling tools for many supervised learning tasks involving complex input patterns. However, they can also easily overfit to training set biases and label noises. In addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they require careful tuning of additional hyperparameters, such as example mining schedules and regularization hyperparameters. In contrast to past reweighting methods, which typically consist of functions of the cost value of each example, in this work we propose a novel meta-learning algorithm that learns to assign weights to training examples based on their gradient directions. To determine the example weights, our method performs a meta gradient descent step on the current mini-batch example weights (which are initialized from zero) to minimize the loss on a clean unbiased validation set. Our proposed method can be easily implemented on any type of deep network, does not require any additional hyperparameter tuning, and achieves impressive performance on class imbalance and corrupted label problems where only a small amount of clean validation data is available.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ren-Learning%20to%20Reweight%20Examples%20for%20Robust%20Deep%20Learning-2018-ICML.pdf}
}
@article{DirectPredTianICML2021, 
year = {2021}, 
title = {{Understanding self-supervised Learning Dynamics without Contrastive Pairs}}, 
author = {Tian, Yuandong and Chen, Xinlei and Ganguli, Surya}, 
journal = {ICML}, 
eprint = {2102.06810}, 
abstract = {{While contrastive approaches of self-supervised learning (SSL) learn representations by minimizing the distance between two augmented views of the same data point (positive pairs) and maximizing views from different data points (negative pairs), recent \textbackslashemph\{non-contrastive\} SSL (e.g., BYOL and SimSiam) show remarkable performance \{\textbackslashit without\} negative pairs, with an extra learnable predictor and a stop-gradient operation. A fundamental question arises: why do these methods not collapse into trivial representations? We answer this question via a simple theoretical study and propose a novel approach, DirectPred, that \textbackslashemph\{directly\} sets the linear predictor based on the statistics of its inputs, without gradient training. On ImageNet, it performs comparably with more complex two-layer non-linear predictors that employ BatchNorm and outperforms a linear predictor by \$2.5\textbackslash\%\$ in 300-epoch training (and \$5\textbackslash\%\$ in 60-epoch). DirectPred is motivated by our theoretical study of the nonlinear learning dynamics of non-contrastive SSL in simple linear networks. Our study yields conceptual insights into how non-contrastive SSL methods learn, how they avoid representational collapse, and how multiple factors, like predictor networks, stop-gradients, exponential moving averages, and weight decay all come into play. Our simple theory recapitulates the results of real-world ablation studies in both STL-10 and ImageNet. Code is released https://github.com/facebookresearch/luckmatters/tree/master/ssl.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tian-Understanding%20self-supervised%20Learning%20Dynamics%20without%20Contrastive%20Pairs-2021-ICML.pdf}
}
@article{RDI-NetWangICCV2021, 
year = {2021}, 
title = {{RDI-Net: Relational Dynamic Inference Networks}}, 
author = {Wang, Huanyu and Li, Songyuan and Su, Shihao and Qin, Zequn and Li, Xi}, 
journal = {ICCV}, 
abstract = {{Dynamic inference networks, aimed at promoting computational efficiency, go along an adaptive executing path for a given sample. Prevalent methods typically assign a router for each convolutional block and sequentially make block-by-block executing decisions, without considering the relations during the dynamic inference. In this paper, we model the relations for dynamic inference from two aspects: the routers and the samples. We design a novel type of router called the relational router to model the relations among routers for a given sample. In principle, the current relational router aggregates the contextual features of preceding routers by graph convolution and propagates its router features to subsequent ones, making the executing decision for the current block in a long-range manner. Furthermore, we model the relation between samples by introducing a Sample Relation Module (SRM), encouraging correlated samples to go along correlated executing paths. As a whole, we call our method the Relational Dynamic Inference Network (RDI-Net). Extensive experiments on CIFAR-10/100 and ImageNet show that RDI-Net achieves state-of-the-art performance and computational cost reduction.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-RDI-Net-%20Relational%20Dynamic%20Inference%20Networks-2021-ICCV.pdf}
}
@article{DynamicGuoNeurIPS2016, 
year = {2016}, 
title = {{Dynamic Network Surgery for Efficient DNNs}}, 
author = {Guo, Yiwen and Yao, Anbang and Chen, Yurong}, 
journal = {NeurIPS}, 
eprint = {1608.04493}, 
abstract = {{Deep learning has become a ubiquitous technology to improve machine intelligence. However, most of the existing deep models are structurally very complex, making them difficult to be deployed on the mobile platforms with limited computational power. In this paper, we propose a novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning. Unlike the previous methods which accomplish this task in a greedy way, we properly incorporate connection splicing into the whole process to avoid incorrect pruning and make it as a continual network maintenance. The effectiveness of our method is proved with experiments. Without any accuracy loss, our method can efficiently compress the number of parameters in LeNet-5 and AlexNet by a factor of \$\textbackslashbm\{108\}\textbackslashtimes\$ and \$\textbackslashbm\{17.7\}\textbackslashtimes\$ respectively, proving that it outperforms the recent pruning method by considerable margins. Code and some models are available at https://github.com/yiwenguo/Dynamic-Network-Surgery.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Guo-Dynamic%20Network%20Surgery%20for%20Efficient%20DNNs-2016-NeurIPS.pdf}
}
@article{DB3KDWangICML2021, 
year = {2021}, 
title = {{Zero-Shot Knowledge Distillation from a Decision-Based Black-Box Model}}, 
author = {Wang, Zi}, 
journal = {ICML}, 
eprint = {2106.03310}, 
abstract = {{Knowledge distillation (KD) is a successful approach for deep neural network acceleration, with which a compact network (student) is trained by mimicking the softmax output of a pre-trained high-capacity network (teacher). In tradition, KD usually relies on access to the training samples and the parameters of the white-box teacher to acquire the transferred knowledge. However, these prerequisites are not always realistic due to storage costs or privacy issues in real-world applications. Here we propose the concept of decision-based black-box (DB3) knowledge distillation, with which the student is trained by distilling the knowledge from a black-box teacher (parameters are not accessible) that only returns classes rather than softmax outputs. We start with the scenario when the training set is accessible. We represent a sample's robustness against other classes by computing its distances to the teacher's decision boundaries and use it to construct the soft label for each training sample. After that, the student can be trained via standard KD. We then extend this approach to a more challenging scenario in which even accessing the training data is not feasible. We propose to generate pseudo samples distinguished by the teacher's decision boundaries to the largest extent and construct soft labels for them, which are used as the transfer set. We evaluate our approaches on various benchmark networks and datasets and experiment results demonstrate their effectiveness. Codes are available at: https://github.com/zwang84/zsdb3kd.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Zero-Shot%20Knowledge%20Distillation%20from%20a%20Decision-Based%20Black-Box%20Model-2021-ICML.pdf}
}
@article{GLoMoYangNeurIPS2018, 
year = {2018}, 
title = {{GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations}}, 
author = {Yang, Zhilin and Zhao, Jake and Dhingra, Bhuwan and He, Kaiming and Cohen, William W and Salakhutdinov, Ruslan and LeCun, Yann}, 
journal = {NeurIPS}, 
eprint = {1806.05662}, 
abstract = {{Modern deep transfer learning approaches have mainly focused on learning generic feature vectors from one task that are transferable to other tasks, such as word embeddings in language and pretrained convolutional features in vision. However, these approaches usually transfer unary features and largely ignore more structured graphical representations. This work explores the possibility of learning generic latent relational graphs that capture dependencies between pairs of data units (e.g., words or pixels) from large-scale unlabeled data and transferring the graphs to downstream tasks. Our proposed transfer learning framework improves performance on various tasks including question answering, natural language inference, sentiment analysis, and image classification. We also show that the learned graphs are generic enough to be transferred to different embeddings on which the graphs have not been trained (including GloVe embeddings, ELMo embeddings, and task-specific RNN hidden unit), or embedding-free units such as image pixels.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-GLoMo-%20Unsupervisedly%20Learned%20Relational%20Graphs%20as%20Transferable%20Representations-2018-NeurIPS.pdf}
}
@article{FeatureXieCVPR2019, 
year = {2019}, 
title = {{Feature Denoising for Improving Adversarial Robustness}}, 
author = {Xie, Cihang and Wu, Yuxin and Maaten, Laurens van der and Yuille, Alan and He, Kaiming}, 
journal = {CVPR}, 
abstract = {{Adversarial attacks to image classification systems present challenges to convolutional networks and opportunities for understanding them. This study suggests that adversarial perturbations on images lead to noise in the features constructed by these networks. Motivated by this observation, we develop new network architectures that increase adversarial robustness by performing feature denoising. Specifically, our networks contain blocks that denoise the features using non-local means or other filters; the entire networks are trained end-to-end. When combined with adversarial training, our feature denoising networks substantially improve the state-of-the-art in adversarial robustness in both white-box and black-box attack settings. On ImageNet, under 10-iteration PGD white-box attacks where prior art has 27.9\% accuracy, our method achieves 55.7\%; even under extreme 2000-iteration PGD white-box attacks, our method secures 42.6\% accuracy. Our method was ranked first in Competition on Adversarial Attacks and Defenses (CAAD) 2018 — it achieved 50.6\% classification accuracy on a secret, ImageNet-like test dataset against 48 unknown attackers, surpassing the runner-up approach by \textbackslashtextasciitilde10\%. Code is available at https://github.com/facebookresearch/ImageNet-Adversarial-Training.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xie-Feature%20Denoising%20for%20Improving%20Adversarial%20Robustness-2019-CVPR.pdf}
}
@article{PanopticFPNKirillovCVPR2019, 
year = {2019}, 
title = {{Panoptic Feature Pyramid Networks}}, 
author = {Kirillov, Alexander and Girshick, Ross and He, Kaiming and Dollár, Piotr}, 
journal = {CVPR}, 
eprint = {1901.02446}, 
abstract = {{The recently introduced panoptic segmentation task has renewed our community's interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff classes). However, current state-of-the-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, top-performing method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kirillov-Panoptic%20Feature%20Pyramid%20Networks-2019-CVPR.pdf}
}
@article{PCLLiICLR2021, 
year = {2021}, 
title = {{Prototypical Contrastive Learning of Unsupervised Representations}}, 
author = {Li, Junnan and Zhou, Pan and Xiong, Caiming and Hoi, Steven C H}, 
journal = {ICLR}, 
eprint = {2005.04966}, 
abstract = {{This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that addresses the fundamental limitations of instance-wise contrastive learning. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it implicitly encodes semantic structures of the data into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks with substantial improvement in low-resource transfer learning. Code and pretrained models are available at https://github.com/salesforce/PCL.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Prototypical%20Contrastive%20Learning%20of%20Unsupervised%20Representations-2021-ICLR.pdf}
}
@article{OSLSMShabanBMVC2017, 
year = {2017}, 
title = {{One-Shot Learning for Semantic Segmentation}}, 
author = {Shaban, Amirreza and Bansal, Shray and Liu, Zhen and Essa, Irfan and Boots, Byron}, 
journal = {BMVC}, 
eprint = {1709.03410}, 
abstract = {{Low-shot learning methods for image classification support learning from sparse data. We extend these techniques to support dense semantic image segmentation. Specifically, we train a network that, given a small set of annotated images, produces parameters for a Fully Convolutional Network (FCN). We use this FCN to perform dense pixel-level prediction on a test image for the new semantic class. Our architecture shows a 25\% relative meanIoU improvement compared to the best baseline methods for one-shot segmentation on unseen classes in the PASCAL VOC 2012 dataset and is at least 3 times faster.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Shaban-One-Shot%20Learning%20for%20Semantic%20Segmentation-2017-BMVC.pdf}
}
@article{GFS-SegTianCVPR2022, 
year = {2022}, 
title = {{Generalized Few-shot Semantic Segmentation}}, 
author = {Tian, Zhuotao and Lai, Xin and Jiang, Li and Shu, Michelle and Zhao, Hengshuang and Jia, Jiaya}, 
journal = {CVPR}, 
eprint = {2010.05210}, 
abstract = {{Training semantic segmentation models requires a large amount of finely annotated data, making it hard to quickly adapt to novel classes not satisfying this condition. Few-Shot Segmentation (FS-Seg) tackles this problem with many constraints. In this paper, we introduce a new benchmark, called Generalized Few-Shot Semantic Segmentation (GFS-Seg), to analyze the generalization ability of simultaneously segmenting the novel categories with very few examples and the base categories with sufficient examples. It is the first study showing that previous representative state-of-the-art FS-Seg methods fall short in GFS-Seg and the performance discrepancy mainly comes from the constrained setting of FS-Seg. To make GFS-Seg tractable, we set up a GFS-Seg baseline that achieves decent performance without structural change on the original model. Then, since context is essential for semantic segmentation, we propose the Context-Aware Prototype Learning (CAPL) that significantly improves performance by 1) leveraging the co-occurrence prior knowledge from support samples, and 2) dynamically enriching contextual information to the classifier, conditioned on the content of each query image. Both two contributions are experimentally shown to have substantial practical merit. Extensive experiments on Pascal-VOC and COCO manifest the effectiveness of CAPL, and CAPL generalizes well to FS-Seg by achieving competitive performance. Code will be made publicly available.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tian-Generalized%20Few-shot%20Semantic%20Segmentation-2022-CVPR.pdf}
}
@article{ADELELiuCVPR2022, 
year = {2022}, 
title = {{Adaptive Early-Learning Correction for Segmentation from Noisy Annotations}}, 
author = {Liu, Sheng and Liu, Kangning and Zhu, Weicheng and Shen, Yiqiu and Fernandez-Granda, Carlos}, 
journal = {CVPR}, 
eprint = {2110.03740}, 
abstract = {{Deep learning in the presence of noisy annotations has been studied extensively in classification, but much less in segmentation tasks. In this work, we study the learning dynamics of deep segmentation networks trained on inaccurately-annotated data. We discover a phenomenon that has been previously reported in the context of classification: the networks tend to first fit the clean pixel-level labels during an "early-learning" phase, before eventually memorizing the false annotations. However, in contrast to classification, memorization in segmentation does not arise simultaneously for all semantic categories. Inspired by these findings, we propose a new method for segmentation from noisy annotations with two key elements. First, we detect the beginning of the memorization phase separately for each category during training. This allows us to adaptively correct the noisy annotations in order to exploit early learning. Second, we incorporate a regularization term that enforces consistency across scales to boost robustness against annotation noise. Our method outperforms standard approaches on a medical-imaging segmentation task where noises are synthesized to mimic human annotation errors. It also provides robustness to realistic noisy annotations present in weakly-supervised semantic segmentation, achieving state-of-the-art results on PASCAL VOC 2012. Code is available at https://github.com/Kangningthu/ADELE}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-Adaptive%20Early-Learning%20Correction%20for%20Segmentation%20from%20Noisy%20Annotations-2022-CVPR.pdf}
}
@article{WeaklySupervisedDuCVPR2022, 
year = {2022}, 
title = {{Weakly Supervised Semantic Segmentation by Pixel-to-Prototype Contrast}}, 
author = {Du, Ye and Fu, Zehua and Liu, Qingjie and Wang, Yunhong}, 
journal = {CVPR}, 
eprint = {2110.07110}, 
abstract = {{Though image-level weakly supervised semantic segmentation (WSSS) has achieved great progress with Class Activation Maps (CAMs) as the cornerstone, the large supervision gap between classification and segmentation still hampers the model to generate more complete and precise pseudo masks for segmentation. In this study, we propose weakly-supervised pixel-to-prototype contrast that can provide pixel-level supervisory signals to narrow the gap. Guided by two intuitive priors, our method is executed across different views and within per single view of an image, aiming to impose cross-view feature semantic consistency regularization and facilitate intra(inter)-class compactness(dispersion) of the feature space. Our method can be seamlessly incorporated into existing WSSS models without any changes to the base networks and does not incur any extra inference burden. Extensive experiments manifest that our method consistently improves two strong baselines by large margins, demonstrating the effectiveness. Specifically, built on top of SEAM, we improve the initial seed mIoU on PASCAL VOC 2012 from 55.4\% to 61.5\%. Moreover, armed with our method, we increase the segmentation mIoU of EPS from 70.8\% to 73.6\%, achieving new state-of-the-art.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Du-Weakly%20Supervised%20Semantic%20Segmentation%20by%20Pixel-to-Prototype%20Contrast-2022-CVPR.pdf}
}
@article{CSLBen-BaruchCVPR2022, 
year = {2022}, 
title = {{Multi-label Classification with Partial Annotations using Class-aware Selective Loss}}, 
author = {Ben-Baruch, Emanuel and Ridnik, Tal and Friedman, Itamar and Ben-Cohen, Avi and Zamir, Nadav and Noy, Asaf and Zelnik-Manor, Lihi}, 
journal = {CVPR}, 
eprint = {2110.10955}, 
abstract = {{Large-scale multi-label classification datasets are commonly, and perhaps inevitably, partially annotated. That is, only a small subset of labels are annotated per sample. Different methods for handling the missing labels induce different properties on the model and impact its accuracy. In this work, we analyze the partial labeling problem, then propose a solution based on two key ideas. First, un-annotated labels should be treated selectively according to two probability quantities: the class distribution in the overall dataset and the specific label likelihood for a given data sample. We propose to estimate the class distribution using a dedicated temporary model, and we show its improved efficiency over a naive estimation computed using the dataset's partial annotations. Second, during the training of the target model, we emphasize the contribution of annotated labels over originally un-annotated labels by using a dedicated asymmetric loss. With our novel approach, we achieve state-of-the-art results on OpenImages dataset (e.g. reaching 87.3 mAP on V6). In addition, experiments conducted on LVIS and simulated-COCO demonstrate the effectiveness of our approach. Code is available at https://github.com/Alibaba-MIIL/PartialLabelingCSL.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ben-Baruch-Multi-label%20Classification%20with%20Partial%20Annotations%20using%20Class-aware%20Selective%20Loss-2022-CVPR.pdf}
}
@article{PS-MTLiuCVPR2022, 
year = {2022}, 
title = {{Perturbed and Strict Mean Teachers for Semi-supervised Semantic Segmentation}}, 
author = {Liu, Yuyuan and Tian, Yu and Chen, Yuanhong and Liu, Fengbei and Belagiannis, Vasileios and Carneiro, Gustavo}, 
journal = {CVPR}, 
eprint = {2111.12903}, 
abstract = {{Consistency learning using input image, feature, or network perturbations has shown remarkable results in semi-supervised semantic segmentation, but this approach can be seriously affected by inaccurate predictions of unlabelled training images. There are two consequences of these inaccurate predictions: 1) the training based on the "strict" cross-entropy (CE) loss can easily overfit prediction mistakes, leading to confirmation bias; and 2) the perturbations applied to these inaccurate predictions will use potentially erroneous predictions as training signals, degrading consistency learning. In this paper, we address the prediction accuracy problem of consistency learning methods with novel extensions of the mean-teacher (MT) model, which include a new auxiliary teacher, and the replacement of MT's mean square error (MSE) by a stricter confidence-weighted cross-entropy (Conf-CE) loss. The accurate prediction by this model allows us to use a challenging combination of network, input data and feature perturbations to improve the consistency learning generalisation, where the feature perturbations consist of a new adversarial perturbation. Results on public benchmarks show that our approach achieves remarkable improvements over the previous SOTA methods in the field. Our code is available at https://github.com/yyliu01/PS-MT.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-Perturbed%20and%20Strict%20Mean%20Teachers%20for%20Semi-supervised%20Semantic%20Segmentation-2022-CVPR.pdf}
}
@article{WILSONCermelliCVPR2022, 
year = {2022}, 
title = {{Incremental Learning in Semantic Segmentation from Image Labels}}, 
author = {Cermelli, Fabio and Fontanel, Dario and Tavera, Antonio and Ciccone, Marco and Caputo, Barbara}, 
journal = {CVPR}, 
eprint = {2112.01882}, 
abstract = {{Although existing semantic segmentation approaches achieve impressive results, they still struggle to update their models incrementally as new categories are uncovered. Furthermore, pixel-by-pixel annotations are expensive and time-consuming. This paper proposes a novel framework for Weakly Incremental Learning for Semantic Segmentation, that aims at learning to segment new classes from cheap and largely available image-level labels. As opposed to existing approaches, that need to generate pseudo-labels offline, we use an auxiliary classifier, trained with image-level labels and regularized by the segmentation model, to obtain pseudo-supervision online and update the model incrementally. We cope with the inherent noise in the process by using soft-labels generated by the auxiliary classifier. We demonstrate the effectiveness of our approach on the Pascal VOC and COCO datasets, outperforming offline weakly-supervised methods and obtaining results comparable with incremental learning methods with full supervision. Code can be found at https://github.com/fcdl94/WILSON.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Cermelli-Incremental%20Learning%20in%20Semantic%20Segmentation%20from%20Image%20Labels-2022-CVPR.pdf}
}
@article{NCDSSZhaoCVPR2022, 
year = {2022}, 
title = {{Novel Class Discovery in Semantic Segmentation}}, 
author = {Zhao, Yuyang and Zhong, Zhun and Sebe, Nicu and Lee, Gim Hee}, 
journal = {CVPR}, 
eprint = {2112.01900}, 
abstract = {{We introduce a new setting of Novel Class Discovery in Semantic Segmentation (NCDSS), which aims at segmenting unlabeled images containing new classes given prior knowledge from a labeled set of disjoint classes. In contrast to existing approaches that look at novel class discovery in image classification, we focus on the more challenging semantic segmentation. In NCDSS, we need to distinguish the objects and background, and to handle the existence of multiple classes within an image, which increases the difficulty in using the unlabeled data. To tackle this new setting, we leverage the labeled base data and a saliency model to coarsely cluster novel classes for model training in our basic framework. Additionally, we propose the Entropy-based Uncertainty Modeling and Self-training (EUMS) framework to overcome noisy pseudo-labels, further improving the model performance on the novel classes. Our EUMS utilizes an entropy ranking technique and a dynamic reassignment to distill clean labels, thereby making full use of the noisy data via self-supervised learning. We build the NCDSS benchmark on the PASCAL-5\$\textasciicircumi\$ dataset and COCO-20\$\textasciicircumi\$ dataset. Extensive experiments demonstrate the feasibility of the basic framework (achieving an average mIoU of 49.81\% on PASCAL-5\$\textasciicircumi\$) and the effectiveness of EUMS framework (outperforming the basic framework by 9.28\% mIoU on PASCAL-5\$\textasciicircumi\$).}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhao-Novel%20Class%20Discovery%20in%20Semantic%20Segmentation-2022-CVPR.pdf}
}
@article{ReCAMChenCVPR2022, 
year = {2022}, 
title = {{Class Re-Activation Maps for Weakly-Supervised Semantic Segmentation}}, 
author = {Chen, Zhaozheng and Wang, Tan and Wu, Xiongwei and Hua, Xian-Sheng and Zhang, Hanwang and Sun, Qianru}, 
journal = {CVPR}, 
eprint = {2203.00962}, 
abstract = {{Extracting class activation maps (CAM) is arguably the most standard step of generating pseudo masks for weakly-supervised semantic segmentation (WSSS). Yet, we find that the crux of the unsatisfactory pseudo masks is the binary cross-entropy loss (BCE) widely used in CAM. Specifically, due to the sum-over-class pooling nature of BCE, each pixel in CAM may be responsive to multiple classes co-occurring in the same receptive field. As a result, given a class, its hot CAM pixels may wrongly invade the area belonging to other classes, or the non-hot ones may be actually a part of the class. To this end, we introduce an embarrassingly simple yet surprisingly effective method: Reactivating the converged CAM with BCE by using softmax cross-entropy loss (SCE), dubbed \textbackslashtextbf\{ReCAM\}. Given an image, we use CAM to extract the feature pixels of each single class, and use them with the class label to learn another fully-connected layer (after the backbone) with SCE. Once converged, we extract ReCAM in the same way as in CAM. Thanks to the contrastive nature of SCE, the pixel response is disentangled into different classes and hence less mask ambiguity is expected. The evaluation on both PASCAL VOC and MS\textbackslashtextasciitildeCOCO shows that ReCAM not only generates high-quality masks, but also supports plug-and-play in any CAM variant with little overhead.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Class%20Re-Activation%20Maps%20for%20Weakly-Supervised%20Semantic%20Segmentation-2022-CVPR.pdf}
}
@article{DAFormerHoyerCVPR2022, 
year = {2022}, 
title = {{DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation}}, 
author = {Hoyer, Lukas and Dai, Dengxin and Gool, Luc Van}, 
journal = {CVPR}, 
eprint = {2111.14887}, 
abstract = {{As acquiring pixel-wise annotations of real-world images for semantic segmentation is a costly process, a model can instead be trained with more accessible synthetic data and adapted to real images without requiring their annotations. This process is studied in unsupervised domain adaptation (UDA). Even though a large number of methods propose new adaptation strategies, they are mostly based on outdated network architectures. As the influence of recent network architectures has not been systematically studied, we first benchmark different network architectures for UDA and newly reveal the potential of Transformers for UDA semantic segmentation. Based on the findings, we propose a novel UDA method, DAFormer. The network architecture of DAFormer consists of a Transformer encoder and a multi-level context-aware feature fusion decoder. It is enabled by three simple but crucial training strategies to stabilize the training and to avoid overfitting to the source domain: While (1) Rare Class Sampling on the source domain improves the quality of the pseudo-labels by mitigating the confirmation bias of self-training toward common classes, (2) a Thing-Class ImageNet Feature Distance and (3) a learning rate warmup promote feature transfer from ImageNet pretraining. DAFormer represents a major advance in UDA. It improves the state of the art by 10.8 mIoU for GTA-to-Cityscapes and 5.4 mIoU for Synthia-to-Cityscapes and enables learning even difficult classes such as train, bus, and truck well. The implementation is available at https://github.com/lhoyer/DAFormer.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hoyer-DAFormer-%20Improving%20Network%20Architectures%20and%20Training%20Strategies%20for%20Domain-Adaptive%20Semantic%20Segmentation-2022-CVPR.pdf}
}
@article{CONTAZhangNeurIPS2020, 
year = {2020}, 
title = {{Causal Intervention for Weakly-Supervised Semantic Segmentation}}, 
author = {Zhang, Dong and Zhang, Hanwang and Tang, Jinhui and Hua, Xiansheng and Sun, Qianru}, 
journal = {NeurIPS}, 
eprint = {2009.12547}, 
abstract = {{We present a causal inference framework to improve Weakly-Supervised Semantic Segmentation (WSSS). Specifically, we aim to generate better pixel-level pseudo-masks by using only image-level labels -- the most crucial step in WSSS. We attribute the cause of the ambiguous boundaries of pseudo-masks to the confounding context, e.g., the correct image-level classification of "horse" and "person" may be not only due to the recognition of each instance, but also their co-occurrence context, making the model inspection (e.g., CAM) hard to distinguish between the boundaries. Inspired by this, we propose a structural causal model to analyze the causalities among images, contexts, and class labels. Based on it, we develop a new method: Context Adjustment (CONTA), to remove the confounding bias in image-level classification and thus provide better pseudo-masks as ground-truth for the subsequent segmentation model. On PASCAL VOC 2012 and MS-COCO, we show that CONTA boosts various popular WSSS methods to new state-of-the-arts.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Causal%20Intervention%20for%20Weakly-Supervised%20Semantic%20Segmentation-2020-NeurIPS.pdf}
}
@article{SIPEChenCVPR2022, 
year = {2022}, 
title = {{Self-supervised Image-specific Prototype Exploration for Weakly Supervised Semantic Segmentation}}, 
author = {Chen, Qi and Yang, Lingxiao and Lai, Jianhuang and Xie, Xiaohua}, 
journal = {CVPR}, 
eprint = {2203.02909}, 
abstract = {{Weakly Supervised Semantic Segmentation (WSSS) based on image-level labels has attracted much attention due to low annotation costs. Existing methods often rely on Class Activation Mapping (CAM) that measures the correlation between image pixels and classifier weight. However, the classifier focuses only on the discriminative regions while ignoring other useful information in each image, resulting in incomplete localization maps. To address this issue, we propose a Self-supervised Image-specific Prototype Exploration (SIPE) that consists of an Image-specific Prototype Exploration (IPE) and a General-Specific Consistency (GSC) loss. Specifically, IPE tailors prototypes for every image to capture complete regions, formed our Image-Specific CAM (IS-CAM), which is realized by two sequential steps. In addition, GSC is proposed to construct the consistency of general CAM and our specific IS-CAM, which further optimizes the feature representation and empowers a self-correction ability of prototype exploration. Extensive experiments are conducted on PASCAL VOC 2012 and MS COCO 2014 segmentation benchmark and results show our SIPE achieves new state-of-the-art performance using only image-level labels. The code is available at https://github.com/chenqi1126/SIPE.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Self-supervised%20Image-specific%20Prototype%20Exploration%20for%20Weakly%20Supervised%20Semantic%20Segmentation-2022-CVPR.pdf}
}
@article{W-OoDLeeCVPR2022, 
year = {2022}, 
title = {{Weakly Supervised Semantic Segmentation using Out-of-Distribution Data}}, 
author = {Lee, Jungbeom and Oh, Seong Joon and Yun, Sangdoo and Choe, Junsuk and Kim, Eunji and Yoon, Sungroh}, 
journal = {CVPR}, 
eprint = {2203.03860}, 
abstract = {{Weakly supervised semantic segmentation (WSSS) methods are often built on pixel-level localization maps obtained from a classifier. However, training on class labels only, classifiers suffer from the spurious correlation between foreground and background cues (e.g. train and rail), fundamentally bounding the performance of WSSS. There have been previous endeavors to address this issue with additional supervision. We propose a novel source of information to distinguish foreground from the background: Out-of-Distribution (OoD) data, or images devoid of foreground object classes. In particular, we utilize the hard OoDs that the classifier is likely to make false-positive predictions. These samples typically carry key visual features on the background (e.g. rail) that the classifiers often confuse as foreground (e.g. train), so these cues let classifiers correctly suppress spurious background cues. Acquiring such hard OoDs does not require an extensive amount of annotation efforts; it only incurs a few additional image-level labeling costs on top of the original efforts to collect class labels. We propose a method, W-OoD, for utilizing the hard OoDs. W-OoD achieves state-of-the-art performance on Pascal VOC 2012.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lee-Weakly%20Supervised%20Semantic%20Segmentation%20using%20Out-of-Distribution%20Data-2022-CVPR.pdf}
}
@article{U2PLWangCVPR2022, 
year = {2022}, 
title = {{Semi-Supervised Semantic Segmentation Using Unreliable Pseudo-Labels}}, 
author = {Wang, Yuchao and Wang, Haochen and Shen, Yujun and Fei, Jingjing and Li, Wei and Jin, Guoqiang and Wu, Liwei and Zhao, Rui and Le, Xinyi}, 
journal = {CVPR}, 
eprint = {2203.03884}, 
abstract = {{The crux of semi-supervised semantic segmentation is to assign adequate pseudo-labels to the pixels of unlabeled images. A common practice is to select the highly confident predictions as the pseudo ground-truth, but it leads to a problem that most pixels may be left unused due to their unreliability. We argue that every pixel matters to the model training, even its prediction is ambiguous. Intuitively, an unreliable prediction may get confused among the top classes (i.e., those with the highest probabilities), however, it should be confident about the pixel not belonging to the remaining classes. Hence, such a pixel can be convincingly treated as a negative sample to those most unlikely categories. Based on this insight, we develop an effective pipeline to make sufficient use of unlabeled data. Concretely, we separate reliable and unreliable pixels via the entropy of predictions, push each unreliable pixel to a category-wise queue that consists of negative samples, and manage to train the model with all candidate pixels. Considering the training evolution, where the prediction becomes more and more accurate, we adaptively adjust the threshold for the reliable-unreliable partition. Experimental results on various benchmarks and training settings demonstrate the superiority of our approach over the state-of-the-art alternatives.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Semi-Supervised%20Semantic%20Segmentation%20Using%20Unreliable%20Pseudo-Labels-2022-CVPR.pdf}
}
@article{BAMLangCVPR2022, 
year = {2022}, 
title = {{Learning What Not to Segment: A New Perspective on Few-Shot Segmentation}}, 
author = {Lang, Chunbo and Cheng, Gong and Tu, Binfei and Han, Junwei}, 
journal = {CVPR}, 
eprint = {2203.07615}, 
abstract = {{Recently few-shot segmentation (FSS) has been extensively developed. Most previous works strive to achieve generalization through the meta-learning framework derived from classification tasks; however, the trained models are biased towards the seen classes instead of being ideally class-agnostic, thus hindering the recognition of new concepts. This paper proposes a fresh and straightforward insight to alleviate the problem. Specifically, we apply an additional branch (base learner) to the conventional FSS model (meta learner) to explicitly identify the targets of base classes, i.e., the regions that do not need to be segmented. Then, the coarse results output by these two learners in parallel are adaptively integrated to yield precise segmentation prediction. Considering the sensitivity of meta learner, we further introduce an adjustment factor to estimate the scene differences between the input image pairs for facilitating the model ensemble forecasting. The substantial performance gains on PASCAL-5i and COCO-20i verify the effectiveness, and surprisingly, our versatile scheme sets a new state-of-the-art even with two plain learners. Moreover, in light of the unique nature of the proposed approach, we also extend it to a more realistic but challenging setting, i.e., generalized FSS, where the pixels of both base and novel classes are required to be determined. The source code is available at github.com/chunbolang/BAM.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lang-Learning%20What%20Not%20to%20Segment-%20A%20New%20Perspective%20on%20Few-Shot%20Segmentation-2022-CVPR.pdf}
}
@article{RCAZhouCVPR2022, 
year = {2022}, 
title = {{Regional Semantic Contrast and Aggregation for Weakly Supervised Semantic Segmentation}}, 
author = {Zhou, Tianfei and Zhang, Meijie and Zhao, Fang and Li, Jianwu}, 
journal = {CVPR}, 
eprint = {2203.09653}, 
abstract = {{Learning semantic segmentation from weakly-labeled (e.g., image tags only) data is challenging since it is hard to infer dense object regions from sparse semantic tags. Despite being broadly studied, most current efforts directly learn from limited semantic annotations carried by individual image or image pairs, and struggle to obtain integral localization maps. Our work alleviates this from a novel perspective, by exploring rich semantic contexts synergistically among abundant weakly-labeled training data for network learning and inference. In particular, we propose regional semantic contrast and aggregation (RCA) . RCA is equipped with a regional memory bank to store massive, diverse object patterns appearing in training data, which acts as strong support for exploration of dataset-level semantic structure. Particularly, we propose i) semantic contrast to drive network learning by contrasting massive categorical object regions, leading to a more holistic object pattern understanding, and ii) semantic aggregation to gather diverse relational contexts in the memory to enrich semantic representations. In this manner, RCA earns a strong capability of fine-grained semantic understanding, and eventually establishes new state-of-the-art results on two popular benchmarks, i.e., PASCAL VOC 2012 and COCO 2014.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhou-Regional%20Semantic%20Contrast%20and%20Aggregation%20for%20Weakly%20Supervised%20Semantic%20Segmentation-2022-CVPR.pdf}
}
@article{CPSLLiCVPR2022, 
year = {2022}, 
title = {{Class-Balanced Pixel-Level Self-Labeling for Domain Adaptive Semantic Segmentation}}, 
author = {Li, Ruihuang and Li, Shuai and He, Chenhang and Zhang, Yabin and Jia, Xu and Zhang, Lei}, 
journal = {CVPR}, 
eprint = {2203.09744}, 
abstract = {{Domain adaptive semantic segmentation aims to learn a model with the supervision of source domain data, and produce satisfactory dense predictions on unlabeled target domain. One popular solution to this challenging task is self-training, which selects high-scoring predictions on target samples as pseudo labels for training. However, the produced pseudo labels often contain much noise because the model is biased to source domain as well as majority categories. To address the above issues, we propose to directly explore the intrinsic pixel distributions of target domain data, instead of heavily relying on the source domain. Specifically, we simultaneously cluster pixels and rectify pseudo labels with the obtained cluster assignments. This process is done in an online fashion so that pseudo labels could co-evolve with the segmentation model without extra training rounds. To overcome the class imbalance problem on long-tailed categories, we employ a distribution alignment technique to enforce the marginal class distribution of cluster assignments to be close to that of pseudo labels. The proposed method, namely Class-balanced Pixel-level Self-Labeling (CPSL), improves the segmentation performance on target domain over state-of-the-arts by a large margin, especially on long-tailed categories.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Class-Balanced%20Pixel-Level%20Self-Labeling%20for%20Domain%20Adaptive%20Semantic%20Segmentation-2022-CVPR.pdf}
}
@article{USRNGuanCVPR2022, 
year = {2022}, 
title = {{Unbiased Subclass Regularization for Semi-Supervised Semantic Segmentation}}, 
author = {Guan, Dayan and Huang, Jiaxing and Xiao, Aoran and Lu, Shijian}, 
journal = {CVPR}, 
eprint = {2203.10026}, 
abstract = {{Semi-supervised semantic segmentation learns from small amounts of labelled images and large amounts of unlabelled images, which has witnessed impressive progress with the recent advance of deep neural networks. However, it often suffers from severe class-bias problem while exploring the unlabelled images, largely due to the clear pixel-wise class imbalance in the labelled images. This paper presents an unbiased subclass regularization network (USRN) that alleviates the class imbalance issue by learning class-unbiased segmentation from balanced subclass distributions. We build the balanced subclass distributions by clustering pixels of each original class into multiple subclasses of similar sizes, which provide class-balanced pseudo supervision to regularize the class-biased segmentation. In addition, we design an entropy-based gate mechanism to coordinate learning between the original classes and the clustered subclasses which facilitates subclass regularization effectively by suppressing unconfident subclass predictions. Extensive experiments over multiple public benchmarks show that USRN achieves superior performance as compared with the state-of-the-art.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Guan-Unbiased%20Subclass%20Regularization%20for%20Semi-Supervised%20Semantic%20Segmentation-2022-CVPR.pdf}
}
@article{TELLiangCVPR2022, 
year = {2022}, 
title = {{Tree Energy Loss: Towards Sparsely Annotated Semantic Segmentation}}, 
author = {Liang, Zhiyuan and Wang, Tiancai and Zhang, Xiangyu and Sun, Jian and Shen, Jianbing}, 
journal = {CVPR}, 
eprint = {2203.10739}, 
abstract = {{Sparsely annotated semantic segmentation (SASS) aims to train a segmentation network with coarse-grained (i.e., point-, scribble-, and block-wise) supervisions, where only a small proportion of pixels are labeled in each image. In this paper, we propose a novel tree energy loss for SASS by providing semantic guidance for unlabeled pixels. The tree energy loss represents images as minimum spanning trees to model both low-level and high-level pair-wise affinities. By sequentially applying these affinities to the network prediction, soft pseudo labels for unlabeled pixels are generated in a coarse-to-fine manner, achieving dynamic online self-training. The tree energy loss is effective and easy to be incorporated into existing frameworks by combining it with a traditional segmentation loss. Compared with previous SASS methods, our method requires no multistage training strategies, alternating optimization procedures, additional supervised data, or time-consuming post-processing while outperforming them in all SASS settings. Code is available at https://github.com/megvii-research/TreeEnergyLoss.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liang-Tree%20Energy%20Loss-%20Towards%20Sparsely%20Annotated%20Semantic%20Segmentation-2022-CVPR.pdf}
}
@article{RegProxyZhangCVPR2022, 
year = {2022}, 
title = {{Semantic Segmentation by Early Region Proxy}}, 
author = {Zhang, Yifan and Pang, Bo and Lu, Cewu}, 
journal = {CVPR}, 
eprint = {2203.14043}, 
abstract = {{Typical vision backbones manipulate structured features. As a compromise, semantic segmentation has long been modeled as per-point prediction on dense regular grids. In this work, we present a novel and efficient modeling that starts from interpreting the image as a tessellation of learnable regions, each of which has flexible geometrics and carries homogeneous semantics. To model region-wise context, we exploit Transformer to encode regions in a sequence-to-sequence manner by applying multi-layer self-attention on the region embeddings, which serve as proxies of specific regions. Semantic segmentation is now carried out as per-region prediction on top of the encoded region embeddings using a single linear classifier, where a decoder is no longer needed. The proposed RegProxy model discards the common Cartesian feature layout and operates purely at region level. Hence, it exhibits the most competitive performance-efficiency trade-off compared with the conventional dense prediction methods. For example, on ADE20K, the small-sized RegProxy-S/16 outperforms the best CNN model using 25\% parameters and 4\% computation, while the largest RegProxy-L/16 achieves 52.9mIoU which outperforms the state-of-the-art by 2.1\% with fewer resources. Codes and models are available at https://github.com/YiF-Zhang/RegionProxy.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Semantic%20Segmentation%20by%20Early%20Region%20Proxy-2022-CVPR.pdf}
}
@article{ProtoSegZhouCVPR2022, 
year = {2022}, 
title = {{Rethinking Semantic Segmentation: A Prototype View}}, 
author = {Zhou, Tianfei and Wang, Wenguan and Konukoglu, Ender and Gool, Luc Van}, 
journal = {CVPR}, 
eprint = {2203.15102}, 
abstract = {{Prevalent semantic segmentation solutions, despite their different network designs (FCN based or attention based) and mask decoding strategies (parametric softmax based or pixel-query based), can be placed in one category, by considering the softmax weights or query vectors as learnable class prototypes. In light of this prototype view, this study uncovers several limitations of such parametric segmentation regime, and proposes a nonparametric alternative based on non-learnable prototypes. Instead of prior methods learning a single weight/query vector for each class in a fully parametric manner, our model represents each class as a set of non-learnable prototypes, relying solely on the mean features of several training pixels within that class. The dense prediction is thus achieved by nonparametric nearest prototype retrieving. This allows our model to directly shape the pixel embedding space, by optimizing the arrangement between embedded pixels and anchored prototypes. It is able to handle arbitrary number of classes with a constant amount of learnable parameters. We empirically show that, with FCN based and attention based segmentation models (i.e., HRNet, Swin, SegFormer) and backbones (i.e., ResNet, HRNet, Swin, MiT), our nonparametric framework yields compelling results over several datasets (i.e., ADE20K, Cityscapes, COCO-Stuff), and performs well in the large-vocabulary situation. We expect this work will provoke a rethink of the current de facto semantic segmentation model design.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhou-Rethinking%20Semantic%20Segmentation-%20A%20Prototype%20View-2022-CVPR.pdf}
}
@article{TheMinisumSpathOR1981, 
year = {1981}, 
title = {{The minisum location problem for the Jaccard metric}}, 
author = {Späth, H.}, 
journal = {OR Spektrum}, 
abstract = {{The well-known Jaccard metric for binary vectors can also be used for nonnegative vectors. This metric is a special case of a whole class of metrics obtainable from a given one by a certain transformation. For the Jaccard metric, the continuous optimization problem of finding an optimal location is reduced to a discrete one. The impact of this result on a multiple location-allocation problem from cluster analysis is discussed.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Späth-The%20minisum%20location%20problem%20for%20the%20Jaccard%20metric-1981-OR%20Spektrum.pdf}
}
@article{iFSLKangCVPR2022, 
year = {2022}, 
title = {{Integrative Few-Shot Learning for Classification and Segmentation}}, 
author = {Kang, Dahyun and Cho, Minsu}, 
journal = {CVPR}, 
eprint = {2203.15712}, 
abstract = {{We introduce the integrative task of few-shot classification and segmentation (FS-CS) that aims to both classify and segment target objects in a query image when the target classes are given with a few examples. This task combines two conventional few-shot learning problems, few-shot classification and segmentation. FS-CS generalizes them to more realistic episodes with arbitrary image pairs, where each target class may or may not be present in the query. To address the task, we propose the integrative few-shot learning (iFSL) framework for FS-CS, which trains a learner to construct class-wise foreground maps for multi-label classification and pixel-wise segmentation. We also develop an effective iFSL model, attentive squeeze network (ASNet), that leverages deep semantic correlation and global self-attention to produce reliable foreground maps. In experiments, the proposed method shows promising performance on the FS-CS task and also achieves the state of the art on standard few-shot segmentation benchmarks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kang-Integrative%20Few-Shot%20Learning%20for%20Classification%20and%20Segmentation-2022-CVPR.pdf}
}
@article{ELNKwonCVPR2022, 
year = {2022}, 
title = {{Semi-supervised Semantic Segmentation with Error Localization Network}}, 
author = {Kwon, Donghyeon and Kwak, Suha}, 
journal = {CVPR}, 
eprint = {2204.02078}, 
abstract = {{This paper studies semi-supervised learning of semantic segmentation, which assumes that only a small portion of training images are labeled and the others remain unlabeled. The unlabeled images are usually assigned pseudo labels to be used in training, which however often causes the risk of performance degradation due to the confirmation bias towards errors on the pseudo labels. We present a novel method that resolves this chronic issue of pseudo labeling. At the heart of our method lies error localization network (ELN), an auxiliary module that takes an image and its segmentation prediction as input and identifies pixels whose pseudo labels are likely to be wrong. ELN enables semi-supervised learning to be robust against inaccurate pseudo labels by disregarding label noises during training and can be naturally integrated with self-training and contrastive learning. Moreover, we introduce a new learning strategy for ELN that simulates plausible and diverse segmentation errors during training of ELN to enhance its generalization. Our method is evaluated on PASCAL VOC 2012 and Cityscapes, where it outperforms all existing methods in every evaluation setting.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kwon-Semi-supervised%20Semantic%20Segmentation%20with%20Error%20Localization%20Network-2022-CVPR.pdf}
}
@article{L2GJiangCVPR2022, 
year = {2022}, 
title = {{L2G: A Simple Local-to-Global Knowledge Transfer Framework for Weakly Supervised Semantic Segmentation}}, 
author = {Jiang, Peng-Tao and Yang, Yuqi and Hou, Qibin and Wei, Yunchao}, 
journal = {CVPR}, 
eprint = {2204.03206}, 
abstract = {{Mining precise class-aware attention maps, a.k.a, class activation maps, is essential for weakly supervised semantic segmentation. In this paper, we present L2G, a simple online local-to-global knowledge transfer framework for high-quality object attention mining. We observe that classification models can discover object regions with more details when replacing the input image with its local patches. Taking this into account, we first leverage a local classification network to extract attentions from multiple local patches randomly cropped from the input image. Then, we utilize a global network to learn complementary attention knowledge across multiple local attention maps online. Our framework conducts the global network to learn the captured rich object detail knowledge from a global view and thereby produces high-quality attention maps that can be directly used as pseudo annotations for semantic segmentation networks. Experiments show that our method attains 72.1\% and 44.2\% mIoU scores on the validation set of PASCAL VOC 2012 and MS COCO 2014, respectively, setting new state-of-the-art records. Code is available at https://github.com/PengtaoJiang/L2G.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Jiang-L2G-%20A%20Simple%20Local-to-Global%20Knowledge%20Transfer%20Framework%20for%20Weakly%20Supervised%20Semantic%20Segmentation-2022-CVPR.pdf}
}
@article{CIRKDYangCVPR2022, 
year = {2022}, 
title = {{Cross-Image Relational Knowledge Distillation for Semantic Segmentation}}, 
author = {Yang, Chuanguang and Zhou, Helong and An, Zhulin and Jiang, Xue and Xu, Yongjun and Zhang, Qian}, 
journal = {CVPR}, 
eprint = {2204.06986}, 
abstract = {{Current Knowledge Distillation (KD) methods for semantic segmentation often guide the student to mimic the teacher's structured information generated from individual data samples. However, they ignore the global semantic relations among pixels across various images that are valuable for KD. This paper proposes a novel Cross-Image Relational KD (CIRKD), which focuses on transferring structured pixel-to-pixel and pixel-to-region relations among the whole images. The motivation is that a good teacher network could construct a well-structured feature space in terms of global pixel dependencies. CIRKD makes the student mimic better structured semantic relations from the teacher, thus improving the segmentation performance. Experimental results over Cityscapes, CamVid and Pascal VOC datasets demonstrate the effectiveness of our proposed approach against state-of-the-art distillation methods. The code is available at https://github.com/winycg/CIRKD.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-Cross-Image%20Relational%20Knowledge%20Distillation%20for%20Semantic%20Segmentation-2022-CVPR.pdf}
}
@article{SDRMichieliCVPR2021, 
year = {2021}, 
title = {{Continual Semantic Segmentation via Repulsion-Attraction of Sparse and Disentangled Latent Representations}}, 
author = {Michieli, Umberto and Zanuttigh, Pietro}, 
journal = {CVPR}, 
abstract = {{Deep neural networks suffer from the major limitation of catastrophic forgetting old tasks when learning new ones. In this paper we focus on class incremental continual learning in semantic segmentation, where new categories are made available over time while previous training data is not retained. The proposed continual learning scheme shapes the latent space to reduce forgetting whilst improving the recognition of novel classes. Our framework is driven by three novel components which we also combine on top of existing techniques effortlessly. First, prototypes matching enforces latent space consistency on old classes, constraining the encoder to produce similar latent representation for previously seen classes in the subsequent steps. Second, features sparsification allows to make room in the latent space to accommodate novel classes. Finally, contrastive learning is employed to cluster features according to their semantics while tearing apart those of different classes. Extensive evaluation on the Pascal VOC2012 and ADE20K datasets demonstrates the effectiveness of our approach, significantly outperforming state-of-the-art methods.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Michieli-Continual%20Semantic%20Segmentation%20via%20Repulsion-Attraction%20of%20Sparse%20and%20Disentangled%20Latent%20Representations-2021-CVPR.pdf}
}
@article{MESSKourisECCV2022, 
year = {2022}, 
title = {{Multi-Exit Semantic Segmentation Networks}}, 
author = {Kouris, Alexandros and Venieris, Stylianos I and Laskaridis, Stefanos and Lane, Nicholas D}, 
journal = {ECCV}, 
eprint = {2106.03527}, 
abstract = {{Semantic segmentation arises as the backbone of many vision systems, spanning from self-driving cars and robot navigation to augmented reality and teleconferencing. Frequently operating under stringent latency constraints within a limited resource envelope, optimising for efficient execution becomes important. To this end, we propose a framework for converting state-of-the-art segmentation models to MESS networks; specially trained CNNs that employ parametrised early exits along their depth to save computation during inference on easier samples. Designing and training such networks naively can hurt performance. Thus, we propose a two-staged training process that pushes semantically important features early in the network. We co-optimise the number, placement and architecture of the attached segmentation heads, along with the exit policy, to adapt to the device capabilities and application-specific requirements. Optimising for speed, MESS networks can achieve latency gains of up to 2.83x over state-of-the-art methods with no accuracy degradation. Accordingly, optimising for accuracy, we achieve an improvement of up to 5.33 pp, under the same computational budget.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kouris-Multi-Exit%20Semantic%20Segmentation%20Networks-2021-arXiv.pdf}
}
@article{EvaluationHuiICLR2021, 
year = {2021}, 
title = {{Evaluation of Neural Architectures Trained with Square Loss vs Cross-Entropy in Classification Tasks}}, 
author = {Hui, Like and Belkin, Mikhail}, 
journal = {ICLR}, 
eprint = {2006.07322}, 
abstract = {{Modern neural architectures for classification tasks are trained using the cross-entropy loss, which is widely believed to be empirically superior to the square loss. In this work we provide evidence indicating that this belief may not be well-founded. We explore several major neural architectures and a range of standard benchmark datasets for NLP, automatic speech recognition (ASR) and computer vision tasks to show that these architectures, with the same hyper-parameter settings as reported in the literature, perform comparably or better when trained with the square loss, even after equalizing computational resources. Indeed, we observe that the square loss produces better results in the dominant majority of NLP and ASR experiments. Cross-entropy appears to have a slight edge on computer vision tasks. We argue that there is little compelling empirical or theoretical evidence indicating a clear-cut advantage to the cross-entropy loss. Indeed, in our experiments, performance on nearly all non-vision tasks can be improved, sometimes significantly, by switching to the square loss. Furthermore, training with square loss appears to be less sensitive to the randomness in initialization. We posit that training using the square loss for classification needs to be a part of best practices of modern deep learning on equal footing with cross-entropy.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hui-Evaluation%20of%20Neural%20Architectures%20Trained%20with%20Square%20Loss%20vs%20Cross-Entropy%20in%20Classification%20Tasks-2021-ICLR.pdf}
}
@article{NCPapyanPNAS2020, 
year = {2020}, 
title = {{Prevalence of neural collapse during the terminal phase of deep learning training}}, 
author = {Papyan, Vardan and Han, X. Y. and Donoho, David L.}, 
journal = {PNAS}, 
eprint = {2008.08186}, 
abstract = {{Modern practice for training classification deepnets involves a terminal phase of training (TPT), which begins at the epoch where training error first vanishes. During TPT, the training error stays effectively zero, while training loss is pushed toward zero. Direct measurements of TPT, for three prototypical deepnet architectures and across seven canonical classification datasets, expose a pervasive inductive bias we call neural collapse (NC), involving four deeply interconnected phenomena. (NC1) Cross-example within-class variability of last-layer training activations collapses to zero, as the individual activations themselves collapse to their class means. (NC2) The class means collapse to the vertices of a simplex equiangular tight frame (ETF). (NC3) Up to rescaling, the last-layer classifiers collapse to the class means or in other words, to the simplex ETF (i.e., to a self-dual configuration). (NC4) For a given activation, the classifier’s decision collapses to simply choosing whichever class has the closest train class mean (i.e., the nearest class center [NCC] decision rule). The symmetric and very simple geometry induced by the TPT confers important benefits, including better generalization performance, better robustness, and better interpretability.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Papyan-Prevalence%20of%20neural%20collapse%20during%20the%20terminal%20phase%20of%20deep%20learning%20training-2020-PNAS.pdf}
}
@article{NCMSEHanICLR2022, 
year = {2022}, 
title = {{Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path}}, 
author = {Han, X Y and Papyan, Vardan and Donoho, David L}, 
journal = {ICLR}, 
eprint = {2106.02073}, 
abstract = {{The recently discovered Neural Collapse (NC) phenomenon occurs pervasively in today's deep net training paradigm of driving cross-entropy (CE) loss towards zero. During NC, last-layer features collapse to their class-means, both classifiers and class-means collapse to the same Simplex Equiangular Tight Frame, and classifier behavior collapses to the nearest-class-mean decision rule. Recent works demonstrated that deep nets trained with mean squared error (MSE) loss perform comparably to those trained with CE. As a preliminary, we empirically establish that NC emerges in such MSE-trained deep nets as well through experiments on three canonical networks and five benchmark datasets. We provide, in a Google Colab notebook, PyTorch code for reproducing MSE-NC and CE-NC: at https://colab.research.google.com/github/neuralcollapse/neuralcollapse/blob/main/neuralcollapse.ipynb. The analytically-tractable MSE loss offers more mathematical opportunities than the hard-to-analyze CE loss, inspiring us to leverage MSE loss towards the theoretical investigation of NC. We develop three main contributions: (I) We show a new decomposition of the MSE loss into (A) terms directly interpretable through the lens of NC and which assume the last-layer classifier is exactly the least-squares classifier; and (B) a term capturing the deviation from this least-squares classifier. (II) We exhibit experiments on canonical datasets and networks demonstrating that term-(B) is negligible during training. This motivates us to introduce a new theoretical construct: the central path, where the linear classifier stays MSE-optimal for feature activations throughout the dynamics. (III) By studying renormalized gradient flow along the central path, we derive exact dynamics that predict NC.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Han-Neural%20Collapse%20Under%20MSE%20Loss-%20Proximity%20to%20and%20Dynamics%20on%20the%20Central%20Path-2022-ICLR.pdf}
}
@article{ZSKDNayakICML2019, 
year = {2019}, 
title = {{Zero-Shot Knowledge Distillation in Deep Networks}}, 
author = {Nayak, Gaurav Kumar and Mopuri, Konda Reddy and Shaj, Vaisakh and Babu, R Venkatesh and Chakraborty, Anirban}, 
journal = {ICML}, 
eprint = {1905.08114}, 
abstract = {{Knowledge distillation deals with the problem of training a smaller model (Student) from a high capacity source model (Teacher) so as to retain most of its performance. Existing approaches use either the training data or meta-data extracted from it in order to train the Student. However, accessing the dataset on which the Teacher has been trained may not always be feasible if the dataset is very large or it poses privacy or safety concerns (e.g., bio-metric or medical data). Hence, in this paper, we propose a novel data-free method to train the Student from the Teacher. Without even using any meta-data, we synthesize the Data Impressions from the complex Teacher model and utilize these as surrogates for the original training data samples to transfer its learning to Student via knowledge distillation. We, therefore, dub our method "Zero-Shot Knowledge Distillation" and demonstrate that our framework results in competitive generalization performance as achieved by distillation using the actual training data samples on multiple benchmark datasets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Nayak-Zero-Shot%20Knowledge%20Distillation%20in%20Deep%20Networks-2019-ICML.pdf}
}
@article{FunMatchBeyerCVPR2022, 
year = {2022}, 
title = {{Knowledge distillation: A good teacher is patient and consistent}}, 
author = {Beyer, Lucas and Zhai, Xiaohua and Royer, Amélie and Markeeva, Larisa and Anil, Rohan and Kolesnikov, Alexander}, 
journal = {CVPR}, 
eprint = {2106.05237}, 
abstract = {{There is a growing discrepancy in computer vision between large-scale models that achieve state-of-the-art performance and models that are affordable in practical applications. In this paper we address this issue and significantly bridge the gap between these two types of models. Throughout our empirical investigation we do not aim to necessarily propose a new method, but strive to identify a robust and effective recipe for making state-of-the-art large scale models affordable in practice. We demonstrate that, when performed correctly, knowledge distillation can be a powerful tool for reducing the size of large models without compromising their performance. In particular, we uncover that there are certain implicit design choices, which may drastically affect the effectiveness of distillation. Our key contribution is the explicit identification of these design choices, which were not previously articulated in the literature. We back up our findings by a comprehensive empirical study, demonstrate compelling results on a wide range of vision datasets and, in particular, obtain a state-of-the-art ResNet-50 model for ImageNet, which achieves 82.8\textbackslash\% top-1 accuracy.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Beyer-Knowledge%20distillation-%20A%20good%20teacher%20is%20patient%20and%20consistent-2022-CVPR.pdf}
}
@article{DECOREAlwaniCVPR2022, 
year = {2022}, 
title = {{DECORE: Deep Compression with Reinforcement Learning}}, 
author = {Alwani, Manoj and Wang, Yang and Madhavan, Vashisht}, 
journal = {CVPR}, 
eprint = {2106.06091}, 
abstract = {{Deep learning has become an increasingly popular and powerful methodology for modern pattern recognition systems. However, many deep neural networks have millions or billions of parameters, making them untenable for real-world applications due to constraints on memory size or latency requirements. As a result, efficient network compression techniques are often required for the widespread adoption of deep learning methods. We present DECORE, a reinforcement learning-based approach to automate the network compression process. DECORE assigns an agent to each channel in the network along with a light policy gradient method to learn which neurons or channels to be kept or removed. Each agent in the network has just one parameter (keep or drop) to learn, which leads to a much faster training process compared to existing approaches. DECORE provides state-of-the-art compression results on various network architectures and various datasets. For example, on the ResNet-110 architecture, it achieves a 64.8\% compression and 61.8\% FLOPs reduction as compared to the baseline model without any accuracy loss on the CIFAR-10 dataset. It can reduce the size of regular architectures like the VGG network by up to 99\% with just a small accuracy drop of 2.28\%. For a larger dataset like ImageNet with just 30 epochs of training, it can compress the ResNet-50 architecture by 44.7\% and reduce FLOPs by 42.3\%, with just a 0.69\% drop on Top-5 accuracy of the uncompressed model. We also demonstrate that DECORE can be used to search for compressed network architectures based on various constraints, such as memory and FLOPs.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Alwani-DECORE-%20Deep%20Compression%20with%20Reinforcement%20Learning-2022-CVPR.pdf}
}
@article{EPIShenCVPR2022, 
year = {2022}, 
title = {{When to Prune? A Policy towards Early Structural Pruning}}, 
author = {Shen, Maying and Molchanov, Pavlo and Yin, Hongxu and Alvarez, Jose M}, 
journal = {CVPR}, 
eprint = {2110.12007}, 
abstract = {{Pruning enables appealing reductions in network memory footprint and time complexity. Conventional post-training pruning techniques lean towards efficient inference while overlooking the heavy computation for training. Recent exploration of pre-training pruning at initialization hints on training cost reduction via pruning, but suffers noticeable performance degradation. We attempt to combine the benefits of both directions and propose a policy that prunes as early as possible during training without hurting performance. Instead of pruning at initialization, our method exploits initial dense training for few epochs to quickly guide the architecture, while constantly evaluating dominant sub-networks via neuron importance ranking. This unveils dominant sub-networks whose structures turn stable, allowing conventional pruning to be pushed earlier into the training. To do this early, we further introduce an Early Pruning Indicator (EPI) that relies on sub-network architectural similarity and quickly triggers pruning when the sub-network's architecture stabilizes. Through extensive experiments on ImageNet, we show that EPI empowers a quick tracking of early training epochs suitable for pruning, offering same efficacy as an otherwise ``oracle'' grid-search that scans through epochs and requires orders of magnitude more compute. Our method yields \$1.4\textbackslash\%\$ top-1 accuracy boost over state-of-the-art pruning counterparts, cuts down training cost on GPU by \$2.4\textbackslashtimes\$, hence offers a new efficiency-accuracy boundary for network pruning during training.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Shen-When%20to%20Prune-%20A%20Policy%20towards%20Early%20Structural%20Pruning-2022-CVPR.pdf}
}
@article{SRRLYangICLR2021, 
year = {2021}, 
title = {{Knowledge distillation via softmax regression representation learning}}, 
author = {Yang, Jing and Marinez, Brais and Bulat, Adrian and Tzimiropoulos, Georgios}, 
journal = {ICLR}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-Knowledge%20distillation%20via%20softmax%20regression%20representation%20learning-2021-ICLR.pdf}
}
@article{MbLSLiuCVPR2022, 
year = {2022}, 
title = {{The Devil is in the Margin: Margin-based Label Smoothing for Network Calibration}}, 
author = {Liu, Bingyuan and Ayed, Ismail Ben and Galdran, Adrian and Dolz, Jose}, 
journal = {CVPR}, 
eprint = {2111.15430}, 
abstract = {{In spite of the dominant performances of deep neural networks, recent works have shown that they are poorly calibrated, resulting in over-confident predictions. Miscalibration can be exacerbated by overfitting due to the minimization of the cross-entropy during training, as it promotes the predicted softmax probabilities to match the one-hot label assignments. This yields a pre-softmax activation of the correct class that is significantly larger than the remaining activations. Recent evidence from the literature suggests that loss functions that embed implicit or explicit maximization of the entropy of predictions yield state-of-the-art calibration performances. We provide a unifying constrained-optimization perspective of current state-of-the-art calibration losses. Specifically, these losses could be viewed as approximations of a linear penalty (or a Lagrangian) imposing equality constraints on logit distances. This points to an important limitation of such underlying equality constraints, whose ensuing gradients constantly push towards a non-informative solution, which might prevent from reaching the best compromise between the discriminative performance and calibration of the model during gradient-based optimization. Following our observations, we propose a simple and flexible generalization based on inequality constraints, which imposes a controllable margin on logit distances. Comprehensive experiments on a variety of image classification, semantic segmentation and NLP benchmarks demonstrate that our method sets novel state-of-the-art results on these tasks in terms of network calibration, without affecting the discriminative performance. The code is available at https://github.com/by-liu/MbLS .}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-The%20Devil%20is%20in%20the%20Margin-%20Margin-based%20Label%20Smoothing%20for%20Network%20Calibration-2022-CVPR.pdf}
}
@article{RDCLiCVPR2022, 
year = {2022}, 
title = {{Ranking Distance Calibration for Cross-Domain Few-Shot Learning}}, 
author = {Li, Pan and Gong, Shaogang and Wang, Chengjie and Fu, Yanwei}, 
journal = {CVPR}, 
eprint = {2112.00260}, 
abstract = {{Recent progress in few-shot learning promotes a more realistic cross-domain setting, where the source and target datasets are from different domains. Due to the domain gap and disjoint label spaces between source and target datasets, their shared knowledge is extremely limited. This encourages us to explore more information in the target domain rather than to overly elaborate training strategies on the source domain as in many existing methods. Hence, we start from a generic representation pre-trained by a cross-entropy loss and a conventional distance-based classifier, along with an image retrieval view, to employ a re-ranking process for calibrating a target distance matrix by discovering the reciprocal k-nearest neighbours within the task. Assuming the pre-trained representation is biased towards the source, we construct a non-linear subspace to minimise task-irrelevant features therewithin while keep more transferrable discriminative information by a hyperbolic tangent transformation. The calibrated distance in this target-aware non-linear subspace is complementary to that in the pre-trained representation. To impose such distance calibration information onto the pre-trained representation, a Kullback-Leibler divergence loss is employed to gradually guide the model towards the calibrated distance-based distribution. Extensive evaluations on eight target domains show that this target ranking calibration process can improve conventional distance-based classifiers in few-shot learning.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Ranking%20Distance%20Calibration%20for%20Cross-Domain%20Few-Shot%20Learning-2022-CVPR.pdf}
}
@article{KDEPHeCVPR2022, 
year = {2022}, 
title = {{Knowledge Distillation as Efficient Pre-training: Faster Convergence, Higher Data-efficiency, and Better Transferability}}, 
author = {He, Ruifei and Sun, Shuyang and Yang, Jihan and Bai, Song and Qi, Xiaojuan}, 
journal = {CVPR}, 
eprint = {2203.05180}, 
abstract = {{Large-scale pre-training has been proven to be crucial for various computer vision tasks. However, with the increase of pre-training data amount, model architecture amount, and the private/inaccessible data, it is not very efficient or possible to pre-train all the model architectures on large-scale datasets. In this work, we investigate an alternative strategy for pre-training, namely Knowledge Distillation as Efficient Pre-training (KDEP), aiming to efficiently transfer the learned feature representation from existing pre-trained models to new student models for future downstream tasks. We observe that existing Knowledge Distillation (KD) methods are unsuitable towards pre-training since they normally distill the logits that are going to be discarded when transferred to downstream tasks. To resolve this problem, we propose a feature-based KD method with non-parametric feature dimension aligning. Notably, our method performs comparably with supervised pre-training counterparts in 3 downstream tasks and 9 downstream datasets requiring 10x less data and 5x less pre-training time. Code is available at https://github.com/CVMI-Lab/KDEP.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/He-Knowledge%20Distillation%20as%20Efficient%20Pre-training-%20Faster%20Convergence,%20Higher%20Data-efficiency,%20and%20Better%20Transferability-2022-CVPR.pdf}
}
@article{IPWimmerCVPR2022, 
year = {2022}, 
title = {{Interspace Pruning: Using Adaptive Filter Representations to Improve Training of Sparse CNNs}}, 
author = {Wimmer, Paul and Mehnert, Jens and Condurache, Alexandru Paul}, 
journal = {CVPR}, 
eprint = {2203.07808}, 
abstract = {{Unstructured pruning is well suited to reduce the memory footprint of convolutional neural networks (CNNs), both at training and inference time. CNNs contain parameters arranged in \$K \textbackslashtimes K\$ filters. Standard unstructured pruning (SP) reduces the memory footprint of CNNs by setting filter elements to zero, thereby specifying a fixed subspace that constrains the filter. Especially if pruning is applied before or during training, this induces a strong bias. To overcome this, we introduce interspace pruning (IP), a general tool to improve existing pruning methods. It uses filters represented in a dynamic interspace by linear combinations of an underlying adaptive filter basis (FB). For IP, FB coefficients are set to zero while un-pruned coefficients and FBs are trained jointly. In this work, we provide mathematical evidence for IP's superior performance and demonstrate that IP outperforms SP on all tested state-of-the-art unstructured pruning methods. Especially in challenging situations, like pruning for ImageNet or pruning to high sparsity, IP greatly exceeds SP with equal runtime and parameter costs. Finally, we show that advances of IP are due to improved trainability and superior generalization ability.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wimmer-Interspace%20Pruning-%20Using%20Adaptive%20Filter%20Representations%20to%20Improve%20Training%20of%20Sparse%20CNNs-2022-CVPR.pdf}
}
@article{DKDZhaoCVPR2022, 
year = {2022}, 
title = {{Decoupled Knowledge Distillation}}, 
author = {Zhao, Borui and Cui, Quan and Song, Renjie and Qiu, Yiyu and Liang, Jiajun}, 
journal = {CVPR}, 
eprint = {2203.08679}, 
abstract = {{State-of-the-art distillation methods are mainly based on distilling deep features from intermediate layers, while the significance of logit distillation is greatly overlooked. To provide a novel viewpoint to study logit distillation, we reformulate the classical KD loss into two parts, i.e., target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD). We empirically investigate and prove the effects of the two parts: TCKD transfers knowledge concerning the "difficulty" of training samples, while NCKD is the prominent reason why logit distillation works. More importantly, we reveal that the classical KD loss is a coupled formulation, which (1) suppresses the effectiveness of NCKD and (2) limits the flexibility to balance these two parts. To address these issues, we present Decoupled Knowledge Distillation (DKD), enabling TCKD and NCKD to play their roles more efficiently and flexibly. Compared with complex feature-based methods, our DKD achieves comparable or even better results and has better training efficiency on CIFAR-100, ImageNet, and MS-COCO datasets for image classification and object detection tasks. This paper proves the great potential of logit distillation, and we hope it will be helpful for future research. The code is available at https://github.com/megvii-research/mdistiller.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhao-Decoupled%20Knowledge%20Distillation-2022-CVPR.pdf}
}
@article{MDCAHebbalaguppeCVPR2022, 
year = {2022}, 
title = {{A Stitch in Time Saves Nine: A Train-Time Regularizing Loss for Improved Neural Network Calibration}}, 
author = {Hebbalaguppe, Ramya and Prakash, Jatin and Madan, Neelabh and Arora, Chetan}, 
journal = {CVPR}, 
eprint = {2203.13834}, 
abstract = {{Deep Neural Networks ( DNN s) are known to make overconfident mistakes, which makes their use problematic in safety-critical applications. State-of-the-art ( SOTA ) calibration techniques improve on the confidence of predicted labels alone and leave the confidence of non-max classes (e.g. top-2, top-5) uncalibrated. Such calibration is not suitable for label refinement using post-processing. Further, most SOTA techniques learn a few hyper-parameters post-hoc, leaving out the scope for image, or pixel specific calibration. This makes them unsuitable for calibration under domain shift, or for dense prediction tasks like semantic segmentation. In this paper, we argue for intervening at the train time itself, so as to directly produce calibrated DNN models. We propose a novel auxiliary loss function: Multi-class Difference in Confidence and Accuracy ( MDCA ), to achieve the same MDCA can be used in conjunction with other application/task-specific loss functions. We show that training with MDCA leads to better-calibrated models in terms of Expected Calibration Error ( ECE ), and Static Calibration Error ( SCE ) on image classification, and segmentation tasks. We report ECE ( SCE ) score of 0.72 (1.60) on the CIFAR 100 dataset, in comparison to 1.90 (1.71) by the SOTA. Under domain shift, a ResNet-18 model trained on PACS dataset using MDCA gives an average ECE ( SCE ) score of 19.7 (9.7) across all domains, compared to 24.2 (11.8) by the SOTA. For the segmentation task, we report a 2X reduction in calibration error on PASCAL - VOC dataset in comparison to Focal Loss. Finally, MDCA training improves calibration even on imbalanced data, and for natural language classification tasks. We have released the code here: code is available at https://github.com/mdca-loss}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hebbalaguppe-A%20Stitch%20in%20Time%20Saves%20Nine-%20A%20Train-Time%20Regularizing%20Loss%20for%20Improved%20Neural%20Network%20Calibration-2022-CVPR.pdf}
}
@article{SimKDChenCVPR2022, 
year = {2022}, 
title = {{Knowledge Distillation with the Reused Teacher Classifier}}, 
author = {Chen, Defang and Mei, Jian-Ping and Zhang, Hailin and Wang, Can and Feng, Yan and Chen, Chun}, 
journal = {CVPR}, 
eprint = {2203.14001}, 
abstract = {{Knowledge distillation aims to compress a powerful yet cumbersome teacher model into a lightweight student model without much sacrifice of performance. For this purpose, various approaches have been proposed over the past few years, generally with elaborately designed knowledge representations, which in turn increase the difficulty of model development and interpretation. In contrast, we empirically show that a simple knowledge distillation technique is enough to significantly narrow down the teacher-student performance gap. We directly reuse the discriminative classifier from the pre-trained teacher model for student inference and train a student encoder through feature alignment with a single \$\textbackslashell\_2\$ loss. In this way, the student model is able to achieve exactly the same performance as the teacher model provided that their extracted features are perfectly aligned. An additional projector is developed to help the student encoder match with the teacher classifier, which renders our technique applicable to various teacher and student architectures. Extensive experiments demonstrate that our technique achieves state-of-the-art results at the modest cost of compression ratio due to the added projector.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Knowledge%20Distillation%20with%20the%20Reused%20Teacher%20Classifier-2022-CVPR.pdf}
}
@article{AdaTripletNguyenMICCAI2022, 
year = {2022}, 
title = {{AdaTriplet: Adaptive Gradient Triplet Loss with Automatic Margin Learning for Forensic Medical Image Matching}}, 
author = {Nguyen, Khanh and Nguyen, Huy Hoang and Tiulpin, Aleksei}, 
journal = {MICCAI}, 
eprint = {2205.02849}, 
abstract = {{This paper tackles the challenge of forensic medical image matching (FMIM) using deep neural networks (DNNs). FMIM is a particular case of content-based image retrieval (CBIR). The main challenge in FMIM compared to the general case of CBIR, is that the subject to whom a query image belongs may be affected by aging and progressive degenerative disorders, making it difficult to match data on a subject level. CBIR with DNNs is generally solved by minimizing a ranking loss, such as Triplet loss (TL), computed on image representations extracted by a DNN from the original data. TL, in particular, operates on triplets: anchor, positive (similar to anchor) and negative (dissimilar to anchor). Although TL has been shown to perform well in many CBIR tasks, it still has limitations, which we identify and analyze in this work. In this paper, we introduce (i) the AdaTriplet loss -- an extension of TL whose gradients adapt to different difficulty levels of negative samples, and (ii) the AutoMargin method -- a technique to adjust hyperparameters of margin-based losses such as TL and our proposed loss dynamically. Our results are evaluated on two large-scale benchmarks for FMIM based on the Osteoarthritis Initiative and Chest X-ray-14 datasets. The codes allowing replication of this study have been made publicly available at \textbackslashurl\{https://github.com/Oulu-IMEDS/AdaTriplet\}.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Nguyen-AdaTriplet-%20Adaptive%20Gradient%20Triplet%20Loss%20with%20Automatic%20Margin%20Learning%20for%20Forensic%20Medical%20Image%20Matching-2022-MICCAI.pdf}
}
@article{SVAEXuCVPR2022, 
year = {2022}, 
title = {{Generating Representative Samples for Few-Shot Classification}}, 
author = {Xu, Jingyi and Le, Hieu}, 
journal = {CVPR}, 
eprint = {2205.02918}, 
abstract = {{Few-shot learning (FSL) aims to learn new categories with a few visual samples per class. Few-shot class representations are often biased due to data scarcity. To mitigate this issue, we propose to generate visual samples based on semantic embeddings using a conditional variational autoencoder (CVAE) model. We train this CVAE model on base classes and use it to generate features for novel classes. More importantly, we guide this VAE to strictly generate representative samples by removing non-representative samples from the base training set when training the CVAE model. We show that this training scheme enhances the representativeness of the generated samples and therefore, improves the few-shot classification results. Experimental results show that our method improves three FSL baseline methods by substantial margins, achieving state-of-the-art few-shot classification performance on miniImageNet and tieredImageNet datasets for both 1-shot and 5-shot settings. Code is available at: https://github.com/cvlab-stonybrook/fsl-rsvae.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xu-Generating%20Representative%20Samples%20for%20Few-Shot%20Classification-2022-CVPR.pdf}
}
@article{RandomPruningLiCVPR2022, 
year = {2022}, 
title = {{Revisiting Random Channel Pruning for Neural Network Compression}}, 
author = {Li, Yawei and Adamczewski, Kamil and Li, Wen and Gu, Shuhang and Timofte, Radu and Gool, Luc Van}, 
journal = {CVPR}, 
eprint = {2205.05676}, 
abstract = {{Channel (or 3D filter) pruning serves as an effective way to accelerate the inference of neural networks. There has been a flurry of algorithms that try to solve this practical problem, each being claimed effective in some ways. Yet, a benchmark to compare those algorithms directly is lacking, mainly due to the complexity of the algorithms and some custom settings such as the particular network configuration or training procedure. A fair benchmark is important for the further development of channel pruning. Meanwhile, recent investigations reveal that the channel configurations discovered by pruning algorithms are at least as important as the pre-trained weights. This gives channel pruning a new role, namely searching the optimal channel configuration. In this paper, we try to determine the channel configuration of the pruned models by random search. The proposed approach provides a new way to compare different methods, namely how well they behave compared with random pruning. We show that this simple strategy works quite well compared with other channel pruning methods. We also show that under this setting, there are surprisingly no clear winners among different channel importance evaluation methods, which then may tilt the research efforts into advanced channel configuration searching methods.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Revisiting%20Random%20Channel%20Pruning%20for%20Neural%20Network%20Compression-2022-CVPR.pdf}
}
@article{ImageXuTOG2011, 
year = {2011}, 
title = {{Image smoothing via L0 gradient minimization}}, 
author = {Xu, Li and Lu, Cewu and Xu, Yi and Jia, Jiaya}, 
journal = {TOG}, 
abstract = {{We present a new image editing method, particularly effective for sharpening major edges by increasing the steepness of transition while eliminating a manageable degree of low-amplitude structures. The seemingly contradictive effect is achieved in an optimization framework making use of L0 gradient minimization, which can globally control how many non-zero gradients are resulted in to approximate prominent structure in a sparsity-control manner. Unlike other edge-preserving smoothing approaches, our method does not depend on local features, but instead globally locates important edges. It, as a fundamental tool, finds many applications and is particularly beneficial to edge extraction, clip-art JPEG artifact removal, and non-photorealistic effect generation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xu-Image%20smoothing%20via%20L0%20gradient%20minimization-2011-TOG.pdf}
}
@article{BINGOXuICLR2022, 
year = {2022}, 
title = {{Bag of Instances Aggregation Boosts Self-supervised Distillation}}, 
author = {Xu, Haohang and Fang, Jiemin and Zhang, Xiaopeng and Xie, Lingxi and Wang, Xinggang and Dai, Wenrui and Xiong, Hongkai and Tian, Qi}, 
journal = {ICLR}, 
eprint = {2107.01691}, 
abstract = {{Recent advances in self-supervised learning have experienced remarkable progress, especially for contrastive learning based methods, which regard each image as well as its augmentations as an individual class and try to distinguish them from all other images. However, due to the large quantity of exemplars, this kind of pretext task intrinsically suffers from slow convergence and is hard for optimization. This is especially true for small-scale models, in which we find the performance drops dramatically comparing with its supervised counterpart. In this paper, we propose a simple but effective distillation strategy for unsupervised learning. The highlight is that the relationship among similar samples counts and can be seamlessly transferred to the student to boost the performance. Our method, termed as BINGO, which is short for Bag of InstaNces aGgregatiOn, targets at transferring the relationship learned by the teacher to the student. Here bag of instances indicates a set of similar samples constructed by the teacher and are grouped within a bag, and the goal of distillation is to aggregate compact representations over the student with respect to instances in a bag. Notably, BINGO achieves new state-of-the-art performance on small-scale models, i.e., 65.5\% and 68.9\% top-1 accuracies with linear evaluation on ImageNet, using ResNet-18 and ResNet-34 as the backbones respectively, surpassing baselines (52.5\% and 57.4\% top-1 accuracies) by a significant margin. The code is available at https://github.com/haohang96/bingo.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xu-Bag%20of%20Instances%20Aggregation%20Boosts%20Self-supervised%20Distillation-2022-ICLR.pdf}
}
@article{LossMaMIA2021, 
year = {2021}, 
title = {{Loss odyssey in medical image segmentation}}, 
author = {Ma, Jun and Chen, Jianan and Ng, Matthew and Huang, Rui and Li, Yu and Li, Chen and Yang, Xiaoping and Martel, Anne L.}, 
journal = {MIA}, 
abstract = {{The loss function is an important component in deep learning-based segmentation methods. Over the past five years, many loss functions have been proposed for various segmentation tasks. However, a systematic study of the utility of these loss functions is missing. In this paper, we present a comprehensive review of segmentation loss functions in an organized manner. We also conduct the first large-scale analysis of 20 general loss functions on four typical 3D segmentation tasks involving six public datasets from 10+ medical centers. The results show that none of the losses can consistently achieve the best performance on the four segmentation tasks, but compound loss functions (e.g. Dice with TopK loss, focal loss, Hausdorff distance loss, and boundary loss) are the most robust losses. Our code and segmentation results are publicly available and can serve as a loss function benchmark. We hope this work will also provide insights on new loss function development for the community.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ma-Loss%20odyssey%20in%20medical%20image%20segmentation-2021-MIA.pdf}
}
@article{HDLossKarimiTMI2020, 
year = {2020}, 
title = {{Reducing the Hausdorff Distance in Medical Image Segmentation With Convolutional Neural Networks}}, 
author = {Karimi, Davood and Salcudean, Septimiu E.}, 
journal = {TMI}, 
abstract = {{The Hausdorff Distance (HD) is widely used in evaluating medical image segmentation methods. However, the existing segmentation methods do not attempt to reduce HD directly. In this paper, we present novel loss functions for training convolutional neural network (CNN)-based segmentation methods with the goal of reducing HD directly. We propose three methods to estimate HD from the segmentation probability map produced by a CNN. One method makes use of the distance transform of the segmentation boundary. Another method is based on applying morphological erosion on the difference between the true and estimated segmentation maps. The third method works by applying circular/spherical convolution kernels of different radii on the segmentation probability maps. Based on these three methods for estimating HD, we suggest three loss functions that can be used for training to reduce HD. We use these loss functions to train CNNs for segmentation of the prostate, liver, and pancreas in ultrasound, magnetic resonance, and computed tomography images and compare the results with commonly-used loss functions. Our results show that the proposed loss functions can lead to approximately 18–45\% reduction in HD without degrading other segmentation performance criteria such as the Dice similarity coefficient. The proposed loss functions can be used for training medical image segmentation methods in order to reduce the large segmentation errors.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Karimi-Reducing%20the%20Hausdorff%20Distance%20in%20Medical%20Image%20Segmentation%20With%20Convolutional%20Neural%20Networks-2020-TMI.pdf}
}
@article{DistanceMapLossCalivaarXiv2019, 
year = {2019}, 
title = {{Distance Map Loss Penalty Term for Semantic Segmentation}}, 
author = {Caliva, Francesco and Iriondo, Claudia and Martinez, Alejandro Morales and Majumdar, Sharmila and Pedoia, Valentina}, 
journal = {arXiv}, 
eprint = {1908.03679}, 
abstract = {{Convolutional neural networks for semantic segmentation suffer from low performance at object boundaries. In medical imaging, accurate representation of tissue surfaces and volumes is important for tracking of disease biomarkers such as tissue morphology and shape features. In this work, we propose a novel distance map derived loss penalty term for semantic segmentation. We propose to use distance maps, derived from ground truth masks, to create a penalty term, guiding the network's focus towards hard-to-segment boundary regions. We investigate the effects of this penalizing factor against cross-entropy, Dice, and focal loss, among others, evaluating performance on a 3D MRI bone segmentation task from the publicly available Osteoarthritis Initiative dataset. We observe a significant improvement in the quality of segmentation, with better shape preservation at bone boundaries and areas affected by partial volume. We ultimately aim to use our loss penalty term to improve the extraction of shape biomarkers and derive metrics to quantitatively evaluate the preservation of shape.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Caliva-Distance%20Map%20Loss%20Penalty%20Term%20for%20Semantic%20Segmentation-2019-arXiv.pdf}
}
@article{AutoSegLossLiICLR2021, 
year = {2021}, 
title = {{Auto Seg-Loss: Searching Metric Surrogates for Semantic Segmentation}}, 
author = {Li, Hao and Tao, Chenxin and Zhu, Xizhou and Wang, Xiaogang and Huang, Gao and Dai, Jifeng}, 
journal = {ICLR}, 
eprint = {2010.07930}, 
abstract = {{Designing proper loss functions is essential in training deep networks. Especially in the field of semantic segmentation, various evaluation metrics have been proposed for diverse scenarios. Despite the success of the widely adopted cross-entropy loss and its variants, the mis-alignment between the loss functions and evaluation metrics degrades the network performance. Meanwhile, manually designing loss functions for each specific metric requires expertise and significant manpower. In this paper, we propose to automate the design of metric-specific loss functions by searching differentiable surrogate losses for each metric. We substitute the non-differentiable operations in the metrics with parameterized functions, and conduct parameter search to optimize the shape of loss surfaces. Two constraints are introduced to regularize the search space and make the search efficient. Extensive experiments on PASCAL VOC and Cityscapes demonstrate that the searched surrogate losses outperform the manually designed loss functions consistently. The searched losses can generalize well to other datasets and networks. Code shall be released.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Auto%20Seg-Loss-%20Searching%20Metric%20Surrogates%20for%20Semantic%20Segmentation-2021-ICLR_1.pdf}
}
@article{OntheInterplayd’AscoliNeurIPS2021, 
year = {2021}, 
title = {{On the interplay between data structure and loss function in classification problems}}, 
author = {d'Ascoli, Stéphane and Gabrié, Marylou and Sagun, Levent and Biroli, Giulio}, 
journal = {NeurIPS}, 
eprint = {2103.05524}, 
abstract = {{One of the central puzzles in modern machine learning is the ability of heavily overparametrized models to generalize well. Although the low-dimensional structure of typical datasets is key to this behavior, most theoretical studies of overparametrization focus on isotropic inputs. In this work, we instead consider an analytically tractable model of structured data, where the input covariance is built from independent blocks allowing us to tune the saliency of low-dimensional structures and their alignment with respect to the target function. Using methods from statistical physics, we derive a precise asymptotic expression for the train and test error achieved by random feature models trained to classify such data, which is valid for any convex loss function. We study in detail how the data structure affects the double descent curve, and show that in the over-parametrized regime, its impact is greater for logistic loss than for mean-squared loss: the easier the task, the wider the gap in performance at the advantage of the logistic loss. Our insights are confirmed by numerical experiments on MNIST and CIFAR10.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/d'Ascoli-On%20the%20interplay%20between%20data%20structure%20and%20loss%20function%20in%20classification%20problems-2021-NeurIPS.pdf}
}
@article{WhyDoBetterKornblithNeurIPS2021, 
year = {2021}, 
title = {{Why Do Better Loss Functions Lead to Less Transferable Features?}}, 
author = {Kornblith, Simon and Chen, Ting and Lee, Honglak and Norouzi, Mohammad}, 
journal = {NeurIPS}, 
eprint = {2010.16402}, 
abstract = {{Previous work has proposed many new loss functions and regularizers that improve test accuracy on image classification tasks. However, it is not clear whether these loss functions learn better representations for downstream tasks. This paper studies how the choice of training objective affects the transferability of the hidden representations of convolutional neural networks trained on ImageNet. We show that many objectives lead to statistically significant improvements in ImageNet accuracy over vanilla softmax cross-entropy, but the resulting fixed feature extractors transfer substantially worse to downstream tasks, and the choice of loss has little effect when networks are fully fine-tuned on the new tasks. Using centered kernel alignment to measure similarity between hidden representations of networks, we find that differences among loss functions are apparent only in the last few layers of the network. We delve deeper into representations of the penultimate layer, finding that different objectives and hyperparameter combinations lead to dramatically different levels of class separation. Representations with higher class separation obtain higher accuracy on the original task, but their features are less useful for downstream tasks. Our results suggest there exists a trade-off between learning invariant features for the original task and features relevant for transfer tasks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kornblith-Why%20Do%20Better%20Loss%20Functions%20Lead%20to%20Less%20Transferable%20Features--2021-NeurIPS.pdf}
}
@article{BASNetQinCVPR2019, 
year = {2019}, 
title = {{BASNet: Boundary-Aware Salient Object Detection}}, 
author = {Qin, Xuebin and Zhang, Zichen and Huang, Chenyang and Gao, Chao and Dehghan, Masood and Jagersand, Martin}, 
journal = {CVPR}, 
abstract = {{Deep Convolutional Neural Networks have been adopted for salient object detection and achieved the state-of-the-art performance. Most of the previous works however focus on region accuracy but not on the boundary quality. In this paper, we propose a predict-refine architecture, BASNet, and a new hybrid loss for Boundary-Aware Salient object detection. Specifically, the architecture is composed of a densely supervised Encoder-Decoder network and a residual refinement module, which are respectively in charge of saliency prediction and saliency map refinement. The hybrid loss guides the network to learn the transformation between the input image and the ground truth in a three-level hierarchy -pixel-, patch-and map-level -by fusing Binary Cross Entropy (BCE), Structural SIMilarity (SSIM) and Intersection-over-Union (IoU) losses. Equipped with the hybrid loss, the proposed predict-refine architecture is able to effectively segment the salient object regions and accurately predict the fine structures with clear boundaries. Experimental results on six public datasets show that our method outperforms the state-of-the-art methods both in terms of regional and boundary evaluation measures. Our method runs at over 25 fps on a single GPU. The code is available at: https://github.com/NathanUA/BASNet.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Qin-BASNet-%20Boundary-Aware%20Salient%20Object%20Detection-2019-CVPR.pdf}
}
@article{CIDDengNeurIPS2021, 
year = {2021}, 
title = {{Comprehensive Knowledge Distillation with Causal Intervention}}, 
author = {Deng, Xiang and Zhang, Zhongfei}, 
journal = {NeurIPS}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Deng-Comprehensive%20Knowledge%20Distillation%20with%20Causal%20Intervention-2021-NeurIPS.pdf}
}
@article{LargeLossMattersKimCVPR2022, 
year = {2022}, 
title = {{Large Loss Matters in Weakly Supervised Multi-Label Classification}}, 
author = {Kim, Youngwook and Kim, Jae Myung and Akata, Zeynep and Lee, Jungwoo}, 
journal = {CVPR}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kim-Large%20Loss%20Matters%20in%20Weakly%20Supervised%20Multi-Label%20Classification-2022-CVPR.pdf}
}
@article{WhatCsurkaBMVC2013, 
year = {2013}, 
title = {{What is a good evaluation measure for semantic segmentation?}}, 
author = {Csurka, Gabriela and Larlus, Diane and Perronnin, Florent}, 
journal = {BMVC}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Csurka-What%20is%20a%20good%20evaluation%20measure%20for%20semantic%20segmentation--2013-BMVC.pdf}
}
@article{AdwU-NetHuangMIDL2022, 
year = {2022}, 
title = {{AdwU-Net: Adaptive Depth and Width U-Net for Medical Image Segmentation by Differentiable Neural Architecture Search}}, 
author = {Huang, Ziyan and Wang, Zehua and Yang, Zhikai and Gu, Lixu}, 
journal = {MIDL}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Huang-AdwU-Net-%20Adaptive%20Depth%20and%20Width%20U-Net%20for%20Medical%20Image%20Segmentation%20by%20Differentiable%20Neural%20Architecture%20Search-2022-MIDL.pdf}
}
@article{LP-FTKumarICLR2022, 
year = {2022}, 
title = {{Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution}}, 
author = {Kumar, Ananya and Raghunathan, Aditi and Jones, Robbie and Ma, Tengyu and Liang, Percy}, 
journal = {ICLR}, 
eprint = {2202.10054}, 
abstract = {{When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer -- the "head"). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR \$\textbackslashto\$ STL, CIFAR10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2\% higher accuracy ID but 7\% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head -- this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1\% better ID, 10\% better OOD than full fine-tuning).}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kumar-Fine-Tuning%20can%20Distort%20Pretrained%20Features%20and%20Underperform%20Out-of-Distribution-2022-ICLR.pdf}
}
@article{DiffStrideRiadICLR2022, 
year = {2022}, 
title = {{Learning strides in convolutional neural networks}}, 
author = {Riad, Rachid and Teboul, Olivier and Grangier, David and Zeghidour, Neil}, 
journal = {ICLR}, 
eprint = {2202.01653}, 
abstract = {{Convolutional neural networks typically contain several downsampling operators, such as strided convolutions or pooling layers, that progressively reduce the resolution of intermediate representations. This provides some shift-invariance while reducing the computational complexity of the whole architecture. A critical hyperparameter of such layers is their stride: the integer factor of downsampling. As strides are not differentiable, finding the best configuration either requires cross-validation or discrete optimization (e.g. architecture search), which rapidly become prohibitive as the search space grows exponentially with the number of downsampling layers. Hence, exploring this search space by gradient descent would allow finding better configurations at a lower computational cost. This work introduces DiffStride, the first downsampling layer with learnable strides. Our layer learns the size of a cropping mask in the Fourier domain, that effectively performs resizing in a differentiable way. Experiments on audio and image classification show the generality and effectiveness of our solution: we use DiffStride as a drop-in replacement to standard downsampling layers and outperform them. In particular, we show that introducing our layer into a ResNet-18 architecture allows keeping consistent high performance on CIFAR10, CIFAR100 and ImageNet even when training starts from poor random stride configurations. Moreover, formulating strides as learnable variables allows us to introduce a regularization term that controls the computational complexity of the architecture. We show how this regularization allows trading off accuracy for efficiency on ImageNet.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Riad-Learning%20strides%20in%20convolutional%20neural%20networks-2022-ICLR.pdf}
}
@article{CURLLuoCVPR2020, 
year = {2020}, 
title = {{Neural Network Pruning with Residual-Connections and Limited-Data}}, 
author = {Luo, Jian-Hao and Wu, Jianxin}, 
journal = {CVPR}, 
abstract = {{Filter level pruning is an effective method to accelerate the inference speed of deep CNN models. Although numerous pruning algorithms have been proposed, there are still two open issues. The first problem is how to prune residual connections. We propose to prune both channels inside and outside the residual connections via a KL-divergence based criterion. The second issue is pruning with limited data. We observe an interesting phenomenon: directly pruning on a small dataset is usually worse than fine-tuning a small model which is pruned or trained from scratch on the large dataset. Knowledge distillation is an effective approach to compensate for the weakness of limited data. However, the logits of a teacher model may be noisy. In order to avoid the influence of label noise, we propose a label refinement approach to solve this problem. Experiments have demonstrated the effectiveness of our method (CURL, Compression Using Residual-connections and Limited-data). CURL significantly outperforms previous state-of-the-art methods on ImageNet. More importantly, when pruning on small datasets, CURL achieves comparable or much better performance than fine-tuning a pretrained small model.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Luo-Neural%20Network%20Pruning%20with%20Residual-Connections%20and%20Limited-Data-2020-CVPR.pdf}
}
@article{MLTIYaoICLR2022, 
year = {2022}, 
title = {{Meta-Learning with Fewer Tasks through Task Interpolation}}, 
author = {Yao, Huaxiu and Zhang, Linjun and Finn, Chelsea}, 
journal = {ICLR}, 
eprint = {2106.02695}, 
abstract = {{Meta-learning enables algorithms to quickly learn a newly encountered task with just a few labeled examples by transferring previously learned knowledge. However, the bottleneck of current meta-learning algorithms is the requirement of a large number of meta-training tasks, which may not be accessible in real-world scenarios. To address the challenge that available tasks may not densely sample the space of tasks, we propose to augment the task set through interpolation. By meta-learning with task interpolation (MLTI), our approach effectively generates additional tasks by randomly sampling a pair of tasks and interpolating the corresponding features and labels. Under both gradient-based and metric-based meta-learning settings, our theoretical analysis shows MLTI corresponds to a data-adaptive meta-regularization and further improves the generalization. Empirically, in our experiments on eight datasets from diverse domains including image recognition, pose prediction, molecule property prediction, and medical image classification, we find that the proposed general MLTI framework is compatible with representative meta-learning algorithms and consistently outperforms other state-of-the-art strategies.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yao-Meta-Learning%20with%20Fewer%20Tasks%20through%20Task%20Interpolation-2022-ICLR.pdf}
}
@article{ProsPrAlizadehICLR2022, 
year = {2022}, 
title = {{Prospect Pruning: Finding Trainable Weights at Initialization using Meta-Gradients}}, 
author = {Alizadeh, Milad and Tailor, Shyam A and Zintgraf, Luisa M and Amersfoort, Joost van and Farquhar, Sebastian and Lane, Nicholas Donald and Gal, Yarin}, 
journal = {ICLR}, 
eprint = {2202.08132}, 
abstract = {{Pruning neural networks at initialization would enable us to find sparse models that retain the accuracy of the original network while consuming fewer computational resources for training and inference. However, current methods are insufficient to enable this optimization and lead to a large degradation in model performance. In this paper, we identify a fundamental limitation in the formulation of current methods, namely that their saliency criteria look at a single step at the start of training without taking into account the trainability of the network. While pruning iteratively and gradually has been shown to improve pruning performance, explicit consideration of the training stage that will immediately follow pruning has so far been absent from the computation of the saliency criterion. To overcome the short-sightedness of existing methods, we propose Prospect Pruning (ProsPr), which uses meta-gradients through the first few steps of optimization to determine which weights to prune. ProsPr combines an estimate of the higher-order effects of pruning on the loss and the optimization trajectory to identify the trainable sub-network. Our method achieves state-of-the-art pruning performance on a variety of vision classification tasks, with less data and in a single shot compared to existing pruning-at-initialization methods.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Alizadeh-Prospect%20Pruning-%20Finding%20Trainable%20Weights%20at%20Initialization%20using%20Meta-Gradients-2022-ICLR.pdf}
}
@article{RDIAKongICLR2022, 
year = {2022}, 
title = {{Resolving Training Biases via Influence-based Data Relabeling}}, 
author = {Kong, Shuming and Shen, Yanyan and Huang, Linpeng}, 
journal = {ICLR}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kong-Resolving%20Training%20Biases%20via%20Influence-based%20Data%20Relabeling-2022-ICLR.pdf}
}
@article{EstimatingBengioarXiv2013, 
year = {2013}, 
title = {{Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation}}, 
author = {Bengio, Yoshua and Léonard, Nicholas and Courville, Aaron}, 
journal = {arXiv}, 
eprint = {1308.3432}, 
abstract = {{Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we "back-propagate" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of \{\textbackslashem conditional computation\}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bengio-Estimating%20or%20Propagating%20Gradients%20Through%20Stochastic%20Neurons%20for%20Conditional%20Computation-2013-arXiv.pdf}
}
@article{UnderstandingSTEYinICLR2019, 
year = {2019}, 
title = {{Understanding Straight-Through Estimator in Training Activation Quantized Neural Nets}}, 
author = {Yin, Penghang and Lyu, Jiancheng and Zhang, Shuai and Osher, Stanley and Qi, Yingyong and Xin, Jack}, 
journal = {ICLR}, 
eprint = {1903.05662}, 
abstract = {{Training activation quantized neural networks involves minimizing a piecewise constant function whose gradient vanishes almost everywhere, which is undesirable for the standard back-propagation or chain rule. An empirical way around this issue is to use a straight-through estimator (STE) (Bengio et al., 2013) in the backward pass only, so that the "gradient" through the modified chain rule becomes non-trivial. Since this unusual "gradient" is certainly not the gradient of loss function, the following question arises: why searching in its negative direction minimizes the training loss? In this paper, we provide the theoretical justification of the concept of STE by answering this question. We consider the problem of learning a two-linear-layer network with binarized ReLU activation and Gaussian input data. We shall refer to the unusual "gradient" given by the STE-modifed chain rule as coarse gradient. The choice of STE is not unique. We prove that if the STE is properly chosen, the expected coarse gradient correlates positively with the population gradient (not available for the training), and its negation is a descent direction for minimizing the population loss. We further show the associated coarse gradient descent algorithm converges to a critical point of the population loss minimization problem. Moreover, we show that a poor choice of STE leads to instability of the training algorithm near certain local minima, which is verified with CIFAR-10 experiments.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yin-Understanding%20Straight-Through%20Estimator%20in%20Training%20Activation%20Quantized%20Neural%20Nets-2019-ICLR.pdf}
}
@article{Top-LabelGuptaICLR2022, 
year = {2022}, 
title = {{Top-label calibration and multiclass-to-binary reductions}}, 
author = {Gupta, Chirag and Ramdas, Aaditya K}, 
journal = {ICLR}, 
eprint = {2107.08353}, 
abstract = {{We investigate the relationship between commonly considered notions of multiclass calibration and the calibration algorithms used to achieve these notions, leading to two broad contributions. First, we propose a new and arguably natural notion of top-label calibration, which requires the reported probability of the most likely label to be calibrated. Along the way, we highlight certain philosophical issues with the closely related and popular notion of confidence calibration. Second, we outline general 'wrapper' multiclass-to-binary (M2B) algorithms that can be used to achieve confidence, top-label, and class-wise calibration, using underlying binary calibration routines. Our wrappers can also be generalized to other notions of calibration, if required for certain practical applications. We instantiate these wrappers with the binary histogram binning (HB) algorithm, and show that the overall procedure has distribution-free calibration guarantees. In an empirical evaluation, we find that with the right M2B wrapper, HB performs significantly better than other calibration approaches. Code for this work has been made publicly available at https://github.com/aigen/df-posthoc-calibration.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Gupta-Top-label%20calibration%20and%20multiclass-to-binary%20reductions-2022-ICLR.pdf}
}
@article{IsLabelSmoothingShenICLR2021, 
year = {2021}, 
title = {{Is Label Smoothing Truly Incompatible with Knowledge Distillation: An Empirical Study}}, 
author = {Shen, Zhiqiang and Liu, Zechun and Xu, Dejia and Chen, Zitian and Cheng, Kwang-Ting and Savvides, Marios}, 
journal = {ICLR}, 
eprint = {2104.00676}, 
abstract = {{This work aims to empirically clarify a recently discovered perspective that label smoothing is incompatible with knowledge distillation. We begin by introducing the motivation behind on how this incompatibility is raised, i.e., label smoothing erases relative information between teacher logits. We provide a novel connection on how label smoothing affects distributions of semantically similar and dissimilar classes. Then we propose a metric to quantitatively measure the degree of erased information in sample's representation. After that, we study its one-sidedness and imperfection of the incompatibility view through massive analyses, visualizations and comprehensive experiments on Image Classification, Binary Networks, and Neural Machine Translation. Finally, we broadly discuss several circumstances wherein label smoothing will indeed lose its effectiveness. Project page: http://zhiqiangshen.com/projects/LS\_and\_KD/index.html.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Shen-Is%20Label%20Smoothing%20Truly%20Incompatible%20with%20Knowledge%20Distillation-%20An%20Empirical%20Study-2021-ICLR.pdf}
}
@article{RevisitingLabelSmoothingChandrasegaranICML2022, 
year = {2022}, 
title = {{Revisiting Label Smoothing and Knowledge Distillation Compatibility: What was Missing?}}, 
author = {Chandrasegaran, Keshigeyan and Tran, Ngoc-Trung and Zhao, Yunqing and Cheung, Ngai-Man}, 
journal = {ICML}, 
eprint = {2206.14532}, 
abstract = {{This work investigates the compatibility between label smoothing (LS) and knowledge distillation (KD). Contemporary findings addressing this thesis statement take dichotomous standpoints: Muller et al. (2019) and Shen et al. (2021b). Critically, there is no effort to understand and resolve these contradictory findings, leaving the primal question -- to smooth or not to smooth a teacher network? -- unanswered. The main contributions of our work are the discovery, analysis and validation of systematic diffusion as the missing concept which is instrumental in understanding and resolving these contradictory findings. This systematic diffusion essentially curtails the benefits of distilling from an LS-trained teacher, thereby rendering KD at increased temperatures ineffective. Our discovery is comprehensively supported by large-scale experiments, analyses and case studies including image classification, neural machine translation and compact student distillation tasks spanning across multiple datasets and teacher-student architectures. Based on our analysis, we suggest practitioners to use an LS-trained teacher with a low-temperature transfer to achieve high performance students. Code and models are available at https://keshik6.github.io/revisiting-ls-kd-compatibility/}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chandrasegaran-Revisiting%20Label%20Smoothing%20and%20Knowledge%20Distillation%20Compatibility-%20What%20was%20Missing--2022-ICML.pdf}
}
@article{TreeFilteringBaoTIP2013, 
year = {2013}, 
title = {{Tree Filtering: Efficient Structure-Preserving Smoothing with a Minimum Spanning Tree}}, 
author = {Bao, Linchao and Song, Yibing and Yang, Qingxiong and Yuan, Hao and Wang, Gang}, 
journal = {TIP}, 
abstract = {{We present a new efficient edge-preserving filter—“tree filter”—to achieve strong image smoothing. The proposed filter can smooth out high-contrast details while preserving major edges, which is not achievable for bilateral-filter-like techniques. Tree filter is a weighted-average filter, whose kernel is derived by viewing pixel affinity in a probabilistic framework simultaneously considering pixel spatial distance, color/intensity difference, as well as connectedness. Pixel connectedness is acquired by treating pixels as nodes in a minimum spanning tree (MST) extracted from the image. The fact that an MST makes all image pixels connected through the tree endues the filter with the power to smooth out high-contrast, fine-scale details while preserving major image structures, since pixels in small isolated region will be closely connected to surrounding majority pixels through the tree, while pixels inside large homogeneous region will be automatically dragged away from pixels outside the region. The tree filter can be separated into two other filters, both of which turn out to have fast algorithms. We also propose an efficient linear time MST extraction algorithm to further improve the whole filtering speed. The algorithms give tree filter a great advantage in low computational complexity (linear to number of image pixels) and fast speed: it can process a 1-megapixel 8-bit image at \$\{\textbackslashsim\}\{\textbackslashrm 0.25\}\textbackslashtextasciitilde\{\textbackslashrm s\}\$ on an Intel 3.4 GHz Core i7 CPU (including the construction of MST). The proposed tree filter is demonstrated on a variety of applications.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bao-Tree%20Filtering-%20Efficient%20Structure-Preserving%20Smoothing%20with%20a%20Minimum%20Spanning%20Tree-2013-TIP.pdf}
}
@article{TAWTChenICLR2022, 
year = {2022}, 
title = {{Weighted Training for Cross-Task Learning}}, 
author = {Chen, Shuxiao and Crammer, Koby and He, Hangfeng and Roth, Dan and Su, Weijie J}, 
journal = {ICLR}, 
eprint = {2105.14095}, 
abstract = {{In this paper, we introduce Target-Aware Weighted Training (TAWT), a weighted training algorithm for cross-task learning based on minimizing a representation-based task distance between the source and target tasks. We show that TAWT is easy to implement, is computationally efficient, requires little hyperparameter tuning, and enjoys non-asymptotic learning-theoretic guarantees. The effectiveness of TAWT is corroborated through extensive experiments with BERT on four sequence tagging tasks in natural language processing (NLP), including part-of-speech (PoS) tagging, chunking, predicate detection, and named entity recognition (NER). As a byproduct, the proposed representation-based task distance allows one to reason in a theoretically principled way about several critical aspects of cross-task learning, such as the choice of the source data and the impact of fine-tuning.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Weighted%20Training%20for%20Cross-Task%20Learning-2022-ICLR.pdf}
}
@article{AdversarialXieICCV2017, 
year = {2017}, 
title = {{Adversarial Examples for Semantic Segmentation and Object Detection}}, 
author = {Xie, Cihang and Wang, Jianyu and Zhang, Zhishuai and Zhou, Yuyin and Xie, Lingxi and Yuille, Alan}, 
journal = {ICCV}, 
abstract = {{It has been well demonstrated that adversarial examples, i.e., natural images with visually imperceptible perturbations added, cause deep networks to fail on image classification. In this paper, we extend adversarial examples to semantic segmentation and object detection which are much more difficult. Our observation is that both segmentation and detection are based on classifying multiple targets on an image (e.g., the target is a pixel or a receptive field in segmentation, and an object proposal in detection). This inspires us to optimize a loss function over a set of targets for generating adversarial perturbations. Based on this, we propose a novel algorithm named Dense Adversary Generation (DAG), which applies to the state-of-the-art networks for segmentation and detection. We find that the adversarial perturbations can be transferred across networks with different training data, based on different architectures, and even for different recognition tasks. In particular, the transfer ability across networks with the same architecture is more significant than in other cases. Besides, we show that summing up heterogeneous perturbations often leads to better transfer performance, which provides an effective method of black-box adversarial attack.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xie-Adversarial%20Examples%20for%20Semantic%20Segmentation%20and%20Object%20Detection-2017-ICCV.pdf}
}
@article{H-DenseUNetLiTMI2018, 
year = {2018}, 
title = {{H-DenseUNet: Hybrid Densely Connected UNet for Liver and Tumor Segmentation From CT Volumes}}, 
author = {Li, Xiaomeng and Chen, Hao and Qi, Xiaojuan and Dou, Qi and Fu, Chi-Wing and Heng, Pheng-Ann}, 
journal = {TMI}, 
abstract = {{Liver cancer is one of the leading causes of cancer death. To assist doctors in hepatocellular carcinoma diagnosis and treatment planning, an accurate and automatic liver and tumor segmentation method is highly demanded in the clinical practice. Recently, fully convolutional neural networks (FCNs), including 2-D and 3-D FCNs, serve as the backbone in many volumetric image segmentation. However, 2-D convolutions cannot fully leverage the spatial information along the third dimension while 3-D convolutions suffer from high computational cost and GPU memory consumption. To address these issues, we propose a novel hybrid densely connected UNet (H-DenseUNet), which consists of a 2-D DenseUNet for efficiently extracting intra-slice features and a 3-D counterpart for hierarchically aggregating volumetric contexts under the spirit of the auto-context algorithm for liver and tumor segmentation. We formulate the learning process of the H-DenseUNet in an end-to-end manner, where the intra-slice representations and inter-slice features can be jointly optimized through a hybrid feature fusion layer. We extensively evaluated our method on the data set of the MICCAI 2017 Liver Tumor Segmentation Challenge and 3DIRCADb data set. Our method outperformed other state-of-the-arts on the segmentation results of tumors and achieved very competitive performance for liver segmentation even with a single model.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-H-DenseUNet-%20Hybrid%20Densely%20Connected%20UNet%20for%20Liver%20and%20Tumor%20Segmentation%20From%20CT%20Volumes-2018-TMI.pdf}
}
@article{TrainingMartinezICLR2020, 
year = {2020}, 
title = {{Training Binary Neural Networks with Real-to-Binary Convolutions}}, 
author = {Martinez, Brais and Yang, Jing and Bulat, Adrian and Tzimiropoulos, Georgios}, 
journal = {ICLR}, 
eprint = {2003.11535}, 
abstract = {{This paper shows how to train binary networks to within a few percent points (\$\textbackslashsim 3-5 \textbackslash\%\$) of the full precision counterpart. We first show how to build a strong baseline, which already achieves state-of-the-art accuracy, by combining recently proposed advances and carefully adjusting the optimization procedure. Secondly, we show that by attempting to minimize the discrepancy between the output of the binary and the corresponding real-valued convolution, additional significant accuracy gains can be obtained. We materialize this idea in two complementary ways: (1) with a loss function, during training, by matching the spatial attention maps computed at the output of the binary and real-valued convolutions, and (2) in a data-driven manner, by using the real-valued activations, available during inference prior to the binarization process, for re-scaling the activations right after the binary convolution. Finally, we show that, when putting all of our improvements together, the proposed model beats the current state of the art by more than 5\% top-1 accuracy on ImageNet and reduces the gap to its real-valued counterpart to less than 3\% and 5\% top-1 accuracy on CIFAR-100 and ImageNet respectively when using a ResNet-18 architecture. Code available at https://github.com/brais-martinez/real2binary.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Martinez-Training%20Binary%20Neural%20Networks%20with%20Real-to-Binary%20Convolutions-2020-ICLR.pdf}
}
@article{OntheDifficultyDziedzicICML2022, 
year = {2022}, 
title = {{On the Difficulty of Defending Self-Supervised Learning against Model Extraction}}, 
author = {Dziedzic, Adam and Dhawan, Nikita and Kaleem, Muhammad Ahmad and Guan, Jonas and Papernot, Nicolas}, 
journal = {ICML}, 
eprint = {2205.07890}, 
abstract = {{Self-Supervised Learning (SSL) is an increasingly popular ML paradigm that trains models to transform complex inputs into representations without relying on explicit labels. These representations encode similarity structures that enable efficient learning of multiple downstream tasks. Recently, ML-as-a-Service providers have commenced offering trained SSL models over inference APIs, which transform user inputs into useful representations for a fee. However, the high cost involved to train these models and their exposure over APIs both make black-box extraction a realistic security threat. We thus explore model stealing attacks against SSL. Unlike traditional model extraction on classifiers that output labels, the victim models here output representations; these representations are of significantly higher dimensionality compared to the low-dimensional prediction scores output by classifiers. We construct several novel attacks and find that approaches that train directly on a victim's stolen representations are query efficient and enable high accuracy for downstream models. We then show that existing defenses against model extraction are inadequate and not easily retrofitted to the specificities of SSL.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Dziedzic-On%20the%20Difficulty%20of%20Defending%20Self-Supervised%20Learning%20against%20Model%20Extraction-2022-ICML.pdf}
}
@article{ShiftAddNASYouICML2022, 
year = {2022}, 
title = {{ShiftAddNAS: Hardware-Inspired Search for More Accurate and Efficient Neural Networks}}, 
author = {You, Haoran and Li, Baopu and Shi, Huihong and Fu, Yonggan and Lin, Yingyan}, 
journal = {ICML}, 
eprint = {2205.08119}, 
abstract = {{Neural networks (NNs) with intensive multiplications (e.g., convolutions and transformers) are capable yet power hungry, impeding their more extensive deployment into resource-constrained devices. As such, multiplication-free networks, which follow a common practice in energy-efficient hardware implementation to parameterize NNs with more efficient operators (e.g., bitwise shifts and additions), have gained growing attention. However, multiplication-free networks usually under-perform their vanilla counterparts in terms of the achieved accuracy. To this end, this work advocates hybrid NNs that consist of both powerful yet costly multiplications and efficient yet less powerful operators for marrying the best of both worlds, and proposes ShiftAddNAS, which can automatically search for more accurate and more efficient NNs. Our ShiftAddNAS highlights two enablers. Specifically, it integrates (1) the first hybrid search space that incorporates both multiplication-based and multiplication-free operators for facilitating the development of both accurate and efficient hybrid NNs; and (2) a novel weight sharing strategy that enables effective weight sharing among different operators that follow heterogeneous distributions (e.g., Gaussian for convolutions vs. Laplacian for add operators) and simultaneously leads to a largely reduced supernet size and much better searched networks. Extensive experiments and ablation studies on various models, datasets, and tasks consistently validate the efficacy of ShiftAddNAS, e.g., achieving up to a +7.7\% higher accuracy or a +4.9 better BLEU score compared to state-of-the-art NN, while leading to up to 93\% or 69\% energy and latency savings, respectively. Codes and pretrained models are available at https://github.com/RICE-EIC/ShiftAddNAS.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/You-ShiftAddNAS-%20Hardware-Inspired%20Search%20for%20More%20Accurate%20and%20Efficient%20Neural%20Networks-2022-ICML.pdf}
}
@article{DNNRNaderICML2022, 
year = {2022}, 
title = {{DNNR: Differential Nearest Neighbors Regression}}, 
author = {Nader, Youssef and Sixt, Leon and Landgraf, Tim}, 
journal = {ICML}, 
eprint = {2205.08434}, 
abstract = {{K-nearest neighbors (KNN) is one of the earliest and most established algorithms in machine learning. For regression tasks, KNN averages the targets within a neighborhood which poses a number of challenges: the neighborhood definition is crucial for the predictive performance as neighbors might be selected based on uninformative features, and averaging does not account for how the function changes locally. We propose a novel method called Differential Nearest Neighbors Regression (DNNR) that addresses both issues simultaneously: during training, DNNR estimates local gradients to scale the features; during inference, it performs an n-th order Taylor approximation using estimated gradients. In a large-scale evaluation on over 250 datasets, we find that DNNR performs comparably to state-of-the-art gradient boosting methods and MLPs while maintaining the simplicity and transparency of KNN. This allows us to derive theoretical error bounds and inspect failures. In times that call for transparency of ML models, DNNR provides a good balance between performance and interpretability.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Nader-DNNR-%20Differential%20Nearest%20Neighbors%20Regression-2022-ICML.pdf}
}
@article{InterpretationGhorbaniAAAI2019, 
year = {2019}, 
title = {{Interpretation of Neural Networks Is Fragile}}, 
author = {Ghorbani, Amirata and Abid, Abubakar and Zou, James}, 
journal = {AAAI}, 
abstract = {{In order for machine learning to be trusted in many applications, it is critical to be able to reliably explain why the machine learning algorithm makes certain predictions. For this reason, a variety of methods have been developed recently to interpret neural network predictions by providing, for example, feature importance maps. For both scientific robustness and security reasons, it is important to know to what extent can the interpretations be altered by small systematic perturbations to the input data, which might be generated by adversaries or by measurement biases. In this paper, we demonstrate how to generate adversarial perturbations that produce perceptively indistinguishable inputs that are assigned the same predicted label, yet have very different interpretations. We systematically characterize the robustness of interpretations generated by several widely-used feature importance interpretation methods (feature importance maps, integrated gradients, and DeepLIFT) on ImageNet and CIFAR-10. In all cases, our experiments show that systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly susceptible to adversarial attack. Our analysis of the geometry of the Hessian matrix gives insight on why robustness is a general challenge to current interpretation approaches.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ghorbani-Interpretation%20of%20Neural%20Networks%20Is%20Fragile-2019-AAAI.pdf}
}
@article{InverseFormBorseCVPR2021, 
year = {2021}, 
title = {{InverseForm: A Loss Function for Structured Boundary-Aware Segmentation}}, 
author = {Borse, Shubhankar and Wang, Ying and Zhang, Yizhe and Porikli, Fatih}, 
journal = {CVPR}, 
abstract = {{We present a novel boundary-aware loss term for semantic segmentation using an inverse-transformation network, which efficiently learns the degree of parametric transformations between estimated and target boundaries. This plug-in loss term complements the cross-entropy loss in capturing boundary transformations and allows consistent and significant performance improvement on segmentation backbone models without increasing their size and computational complexity. We analyze the quantitative and qualitative effects of our loss function on three indoor and outdoor segmentation benchmarks, including Cityscapes, NYU-Depth-v2, and PASCAL, integrating it into the training phase of several backbone networks in both single-task and multi-task settings. Our extensive experiments show that the proposed method consistently outperforms base-lines, and even sets the new state-of-the-art on two datasets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Borse-InverseForm-%20A%20Loss%20Function%20for%20Structured%20Boundary-Aware%20Segmentation-2021-CVPR.pdf}
}
@article{DeepRoadMapperMattyusICCV2017, 
year = {2017}, 
title = {{DeepRoadMapper: Extracting Road Topology from Aerial Images}}, 
author = {Mattyus, Gellert and Luo, Wenjie and Urtasun, Raquel}, 
journal = {ICCV}, 
abstract = {{Creating road maps is essential for applications such as autonomous driving and city planning. Most approaches in industry focus on leveraging expensive sensors mounted on top of a fleet of cars. This results in very accurate estimates when exploiting a user in the loop. However, these solutions are very expensive and have small coverage. In contrast, in this paper we propose an approach that directly estimates road topology from aerial images. This provides us with an affordable solution with large coverage. Towards this goal, we take advantage of the latest developments in deep learning to have an initial segmentation of the aerial images. We then propose an algorithm that reasons about missing connections in the extracted road topology as a shortest path problem that can be solved efficiently. We demonstrate the effectiveness of our approach in the challenging Toron-toCity dataset [23] and show very significant improvements over the state-of-the-art.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Mattyus-DeepRoadMapper-%20Extracting%20Road%20Topology%20from%20Aerial%20Images-2017-ICCV.pdf}
}
@article{DeepCenICCV2021, 
year = {2021}, 
title = {{Deep Metric Learning for Open World Semantic Segmentation}}, 
author = {Cen, Jun and Yun, Peng and Cai, Junhao and Wang, Michael Yu and Liu, Ming}, 
journal = {ICCV}, 
abstract = {{Classical close-set semantic segmentation networks have limited ability to detect out-of-distribution (OOD) objects, which is important for safety-critical applications such as autonomous driving. Incrementally learning these OOD objects with few annotations is an ideal way to enlarge the knowledge base of the deep learning models. In this paper, we propose an open world semantic segmentation system that includes two modules: (1) an open-set semantic segmentation module to detect both in-distribution and OOD objects. (2) an incremental few-shot learning module to gradually incorporate those OOD objects into its existing knowledge base. This open world semantic segmentation system behaves like a human being, which is able to identify OOD objects and gradually learn them with corresponding supervision. We adopt the Deep Metric Learning Network (DMLNet) with contrastive clustering to implement open-set semantic segmentation. Compared to other open-set semantic segmentation methods, our DMLNet achieves state-of-the-art performance on three challenging open-set semantic segmentation datasets without using additional data or generative models. On this basis, two incremental few-shot learning methods are further proposed to progressively improve the DMLNet with the annotations of OOD objects.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Cen-Deep%20Metric%20Learning%20for%20Open%20World%20Semantic%20Segmentation-2021-ICCV.pdf}
}
@article{MRF-UNetsWangECML-PKDD2022, 
year = {2022}, 
title = {{MRF-UNets: Searching UNet with Markov Random Fields}}, 
author = {Wang, Zifu and Blaschko, Matthew B}, 
journal = {ECML-PKDD}, 
eprint = {2207.06168}, 
abstract = {{UNet [27] is widely used in semantic segmentation due to its simplicity and effectiveness. However, its manually-designed architecture is applied to a large number of problem settings, either with no architecture optimizations, or with manual tuning, which is time consuming and can be sub-optimal. In this work, firstly, we propose Markov Random Field Neural Architecture Search (MRF-NAS) that extends and improves the recent Adaptive and Optimal Network Width Search (AOWS) method [4] with (i) a more general MRF framework (ii) diverse M-best loopy inference (iii) differentiable parameter learning. This provides the necessary NAS framework to efficiently explore network architectures that induce loopy inference graphs, including loops that arise from skip connections. With UNet as the backbone, we find an architecture, MRF-UNet, that shows several interesting characteristics. Secondly, through the lens of these characteristics, we identify the sub-optimality of the original UNet architecture and further improve our results with MRF-UNetV2. Experiments show that our MRF-UNets significantly outperform several benchmarks on three aerial image datasets and two medical image datasets while maintaining low computational costs. The code is available at: https://github.com/zifuwanggg/MRF-UNets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-MRF-UNets-%20Searching%20UNet%20with%20Markov%20Random%20Fields-2022-ECML-PKDD.pdf}
}
@article{Severity-awareLiuCVPR2020, 
year = {2020}, 
title = {{Severity-Aware Semantic Segmentation with Reinforced Wasserstein Training}}, 
author = {Liu, Xiaofeng and Ji, Wenxuan and You, Jane and Fakhri, Georges El and Woo, Jonghye}, 
journal = {CVPR}, 
abstract = {{Semantic segmentation is a class of methods to classify each pixel in an image into semantic classes, which is critical for autonomous vehicles and surgery systems. Cross-entropy (CE) loss-based deep neural networks (DNN) achieved great success w.r.t. the accuracy-based metrics, e.g., mean Intersection-over Union. However, the CE loss has a limitation in that it ignores varying degrees of severity of pair-wise misclassified results. For instance, classifying a car into the road is much more terrible than recognizing it as a bus. To sidestep this, in this work, we propose to incorporate the severity-aware inter-class correlation into our Wasserstein training framework by configuring its ground distance matrix. In addition, our method can adaptively learn the ground metric in a high-fidelity simulator, following a reinforcement alternative optimization scheme. We evaluate our method using the CARLA simulator with the Deeplab backbone, demonstraing that our method significantly improves the survival time in the CARLA simulator. In addition, our method can be readily applied to existing DNN architectures and algorithms while yielding superior performance. We report results from experiments carried out with the CamVid and Cityscapes datasets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-Severity-Aware%20Semantic%20Segmentation%20with%20Reinforced%20Wasserstein%20Training-2020-CVPR.pdf}
}
@article{ADVENTVuCVPR2019, 
year = {2019}, 
title = {{ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation}}, 
author = {Vu, Tuan-Hung and Jain, Himalaya and Bucher, Maxime and Cord, Matthieu and Pérez, Patrick}, 
journal = {CVPR}, 
abstract = {{Semantic segmentation is a key problem for many computer vision tasks. While approaches based on convolutional neural networks constantly break new records on different benchmarks, generalizing well to diverse testing environments remains a major challenge. In numerous real world applications, there is indeed a large gap between data distributions in train and test domains, which results in severe performance loss at run-time. In this work, we address the task of unsupervised domain adaptation in semantic segmentation with losses based on the entropy of the pixel-wise predictions. To this end, we propose two novel, complementary methods using (i) an entropy loss and (ii) an adversarial loss respectively. We demonstrate state-of-the-art performance in semantic segmentation on two challenging "synthetic-2-real" set-ups1 and show that the approach can also be used for detection.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Vu-ADVENT-%20Adversarial%20Entropy%20Minimization%20for%20Domain%20Adaptation%20in%20Semantic%20Segmentation-2019-CVPR.pdf}
}
@article{PSIXuICCV2021, 
year = {2021}, 
title = {{Scribble-Supervised Semantic Segmentation Inference}}, 
author = {Xu, Jingshan and Zhou, Chuanwei and Cui, Zhen and Xu, Chunyan and Huang, Yuge and Shen, Pengcheng and Li, Shaoxin and Yang, Jian}, 
journal = {ICCV}, 
abstract = {{In this paper, we propose a progressive segmentation inference (PSI) framework to tackle with scribble-supervised semantic segmentation. In virtue of latent contextual dependency, we encapsulate two crucial cues, contextual pattern propagation and semantic label diffusion, to enhance and refine pixel-level segmentation results from partially known seeds. In contextual pattern propagation, different-granular contextual patterns are correlated and leveraged to properly diffuse pattern information based on graphical model, so as to increase the inference confidence of pixel label prediction. Further, depending on high-confidence scores of estimated pixels, the initial annotated seeds are progressively spread over the image through dynamically learning an adaptive decision strategy. The two cues are finally modularized to form a close-looping update process during pixel-wise label inference. Extensive experiments demonstrate that our proposed progressive segmentation inference can benefit from the combination of spatial and semantic context cues, and meantime achieve the state-of-the-art performance on two public scribble segmentation datasets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xu-Scribble-Supervised%20Semantic%20Segmentation%20Inference-2021-ICCV.pdf}
}
@article{Compositing-awareZhaoECCV2018, 
year = {2018}, 
title = {{Compositing-aware Image Search}}, 
author = {Zhao, Hengshuang and Shen, Xiaohui and Lin, Zhe and Sunkavalli, Kalyan and Price, Brian and Jia, Jiaya}, 
journal = {ECCV}, 
abstract = {{We present a new image search technique that, given a background image, returns compatible foreground objects for image compositing tasks. The compatibility of a foreground object and a background scene depends on various aspects such as semantics, surrounding context, geometry, style and color. However, existing image search techniques measure the similarities on only a few aspects, and may return many results that are not suitable for compositing. Moreover, the importance of each factor may vary for different object categories and image content, making it difficult to manually define the matching criteria. In this paper, we propose to learn feature representations for foreground objects and background scenes respectively, where image content and object category information are jointly encoded during training. As a result, the learned features can adaptively encode the most important compatibility factors. We project the features to a common embedding space, so that the compatibility scores can be easily measured using the cosine similarity, enabling very efficient search. We collect an evaluation set consisting of eight object categories commonly used in compositing tasks, on which we demonstrate that our approach significantly outperforms other search techniques.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhao-Compositing-aware%20Image%20Search-2018-ECCV.pdf}
}
@article{UFOZhaoICCV2019, 
year = {2019}, 
title = {{Unconstrained Foreground Object Search}}, 
author = {Zhao, Yinan and Price, Brian and Cohen, Scott and Gurari, Danna}, 
journal = {ICCV}, 
abstract = {{Many people search for foreground objects to use when editing images. While existing methods can retrieve candidates to aid in this, they are constrained to returning objects that belong to a pre-specified semantic class. We instead propose a novel problem of unconstrained foreground object (UFO) search and introduce a solution that supports efficient search by encoding the background image in the same latent space as the candidate foreground objects. A key contribution of our work is a cost-free, scalable approach for creating a large-scale training dataset with a variety of foreground objects of differing semantic categories per image location. Quantitative and human-perception experiments with two diverse datasets demonstrate the advantage of our UFO search solution over related baselines.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhao-Unconstrained%20Foreground%20Object%20Search-2019-ICCV.pdf}
}
@article{DeepImageCompositingZhangWACV2021, 
year = {2021}, 
title = {{Deep Image Compositing}}, 
author = {Zhang, He and Zhang, Jianming and Perazzi, Federico and Lin, Zhe and Patel, Vishal M}, 
journal = {WACV}, 
abstract = {{Image compositing is a task of combining regions from different images to compose a new image. A common use case is background replacement of portrait images. To obtain high quality composites, professionals typically manually perform multiple editing steps such as segmentation, matting and foreground color decontamination, which is very time consuming even with sophisticated photo editing tools. In this paper, we propose a new method which can automatically generate high-quality image compositing with-out any user input. Our method can be trained end-to-end to optimize exploitation of contextual and color information of both foreground and background images, where the com-positing quality is considered in the optimization. Specifically, inspired by Laplacian pyramid blending, a dense-connected multi-stream fusion network is proposed to effectively fuse the information from the foreground and back-ground images at different scales. In addition, we intro-duce a self-taught strategy to progressively train from easy to complex cases to mitigate the lack of training data. Experiments show that the proposed method can automatically generate high-quality composites and outperforms existing methods both qualitatively and quantitatively.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Deep%20Image%20Compositing-2021-WACV.pdf}
}
@article{DeepImageBlendingZhangWACV2020, 
year = {2020}, 
title = {{Deep Image Blending}}, 
author = {Zhang, Lingzhi and Wen, Tarmily and Shi, Jianbo}, 
journal = {WACV}, 
eprint = {1910.11495}, 
abstract = {{Image composition is an important operation to create visual content. Among image composition tasks, image blending aims to seamlessly blend an object from a source image onto a target image with lightly mask adjustment. A popular approach is Poisson image blending, which enforces the gradient domain smoothness in the composite image. However, this approach only considers the boundary pixels of target image, and thus can not adapt to texture of target image. In addition, the colors of the target image often seep through the original source object too much causing a significant loss of content of the source object. We propose a Poisson blending loss that achieves the same purpose of Poisson image blending. In addition, we jointly optimize the proposed Poisson blending loss as well as the style and content loss computed from a deep network, and reconstruct the blending region by iteratively updating the pixels using the L-BFGS solver. In the blending image, we not only smooth out gradient domain of the blending boundary but also add consistent texture into the blending region. User studies show that our method outperforms strong baselines as well as state-of-the-art approaches when placing objects onto both paintings and real-world images.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Deep%20Image%20Blending-2020-WACV.pdf}
}
@article{TheDiceLossTilborghsMICCAI2022, 
year = {2022}, 
title = {{The Dice loss in the context of missing or empty labels: Introducing $\Phi$ and $\epsilon$}}, 
author = {Tilborghs, Sofie and Bertels, Jeroen and Robben, David and Vandermeulen, Dirk and Maes, Frederik}, 
journal = {MICCAI}, 
eprint = {2207.09521}, 
abstract = {{Albeit the Dice loss is one of the dominant loss functions in medical image segmentation, most research omits a closer look at its derivative, i.e. the real motor of the optimization when using gradient descent. In this paper, we highlight the peculiar action of the Dice loss in the presence of missing or empty labels. First, we formulate a theoretical basis that gives a general description of the Dice loss and its derivative. It turns out that the choice of the reduction dimensions \$\textbackslashPhi\$ and the smoothing term \$\textbackslashepsilon\$ is non-trivial and greatly influences its behavior. We find and propose heuristic combinations of \$\textbackslashPhi\$ and \$\textbackslashepsilon\$ that work in a segmentation setting with either missing or empty labels. Second, we empirically validate these findings in a binary and multiclass segmentation setting using two publicly available datasets. We confirm that the choice of \$\textbackslashPhi\$ and \$\textbackslashepsilon\$ is indeed pivotal. With \$\textbackslashPhi\$ chosen such that the reductions happen over a single batch (and class) element and with a negligible \$\textbackslashepsilon\$, the Dice loss deals with missing labels naturally and performs similarly compared to recent adaptations specific for missing labels. With \$\textbackslashPhi\$ chosen such that the reductions happen over multiple batch elements or with a heuristic value for \$\textbackslashepsilon\$, the Dice loss handles empty labels correctly. We believe that this work highlights some essential perspectives and hope that it encourages researchers to better describe their exact implementation of the Dice loss in future work.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tilborghs-The%20Dice%20loss%20in%20the%20context%20of%20missing%20or%20empty%20labels-%20Introducing%20$-Phi$%20and%20$-epsilon$-2022-MICCAI.pdf}
}
@article{SegmenterStrudelICCV2021, 
year = {2021}, 
title = {{Segmenter: Transformer for Semantic Segmentation}}, 
author = {Strudel, Robin and Garcia, Ricardo and Laptev, Ivan and Schmid, Cordelia}, 
journal = {ICCV}, 
abstract = {{Image segmentation is often ambiguous at the level of individual image patches and requires contextual information to reach label consensus. In this paper we introduce Segmenter, a transformer model for semantic segmentation. In contrast to convolution-based methods, our approach allows to model global context already at the first layer and throughout the network. We build on the recent Vision Transformer (ViT) and extend it to semantic segmentation. To do so, we rely on the output embeddings corresponding to image patches and obtain class labels from these embed-dings with a point-wise linear decoder or a mask trans-former decoder. We leverage models pre-trained for image classification and show that we can fine-tune them on moderate sized datasets available for semantic segmentation. The linear decoder allows to obtain excellent results already, but the performance can be further improved by a mask transformer generating class masks. We conduct an extensive ablation study to show the impact of the different parameters, in particular the performance is better for large models and small patch sizes. Segmenter attains excellent results for semantic segmentation. It outperforms the state of the art on both ADE20K and Pascal Context datasets and is competitive on Cityscapes.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Strudel-Segmenter-%20Transformer%20for%20Semantic%20Segmentation-2021-ICCV.pdf}
}
@article{LogitAdjustmentMenonICLR2021, 
year = {2021}, 
title = {{Long-tail learning via logit adjustment}}, 
author = {Menon, Aditya Krishna and Jayasumana, Sadeep and Rawat, Ankit Singh and Jain, Himanshu and Veit, Andreas and Kumar, Sanjiv}, 
journal = {ICLR}, 
eprint = {2007.07314}, 
abstract = {{Real-world classification problems typically exhibit an imbalanced or long-tailed label distribution, wherein many labels are associated with only a few samples. This poses a challenge for generalisation on such labels, and also makes na\textbackslash"ive learning biased towards dominant labels. In this paper, we present two simple modifications of standard softmax cross-entropy training to cope with these challenges. Our techniques revisit the classic idea of logit adjustment based on the label frequencies, either applied post-hoc to a trained model, or enforced in the loss during training. Such adjustment encourages a large relative margin between logits of rare versus dominant labels. These techniques unify and generalise several recent proposals in the literature, while possessing firmer statistical grounding and empirical performance.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Menon-Long-tail%20learning%20via%20logit%20adjustment-2020-ICLR.pdf}
}
@article{Trans10K-v2XieIJCAI2021, 
year = {2021}, 
title = {{Segmenting Transparent Object in the Wild with Transformer}}, 
author = {Xie, Enze and Wang, Wenjia and Wang, Wenhai and Sun, Peize and Xu, Hang and Liang, Ding and Luo, Ping}, 
journal = {IJCAI}, 
eprint = {2101.08461}, 
abstract = {{This work presents a new fine-grained transparent object segmentation dataset, termed Trans10K-v2, extending Trans10K-v1, the first large-scale transparent object segmentation dataset. Unlike Trans10K-v1 that only has two limited categories, our new dataset has several appealing benefits. (1) It has 11 fine-grained categories of transparent objects, commonly occurring in the human domestic environment, making it more practical for real-world application. (2) Trans10K-v2 brings more challenges for the current advanced segmentation methods than its former version. Furthermore, a novel transformer-based segmentation pipeline termed Trans2Seg is proposed. Firstly, the transformer encoder of Trans2Seg provides the global receptive field in contrast to CNN's local receptive field, which shows excellent advantages over pure CNN architectures. Secondly, by formulating semantic segmentation as a problem of dictionary look-up, we design a set of learnable prototypes as the query of Trans2Seg's transformer decoder, where each prototype learns the statistics of one category in the whole dataset. We benchmark more than 20 recent semantic segmentation methods, demonstrating that Trans2Seg significantly outperforms all the CNN-based methods, showing the proposed algorithm's potential ability to solve transparent object segmentation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xie-Segmenting%20Transparent%20Object%20in%20the%20Wild%20with%20Transformer-2021-IJCAI.pdf}
}
@article{Topology-awareHuICLR2021, 
year = {2021}, 
title = {{Topology-Aware Segmentation Using Discrete Morse Theory}}, 
author = {Hu, Xiaoling and Wang, Yusu and Fuxin, Li and Samaras, Dimitris and Chen, Chao}, 
journal = {ICLR}, 
eprint = {2103.09992}, 
abstract = {{In the segmentation of fine-scale structures from natural and biomedical images, per-pixel accuracy is not the only metric of concern. Topological correctness, such as vessel connectivity and membrane closure, is crucial for downstream analysis tasks. In this paper, we propose a new approach to train deep image segmentation networks for better topological accuracy. In particular, leveraging the power of discrete Morse theory (DMT), we identify global structures, including 1D skeletons and 2D patches, which are important for topological accuracy. Trained with a novel loss based on these global structures, the network performance is significantly improved especially near topologically challenging locations (such as weak spots of connections and membranes). On diverse datasets, our method achieves superior performance on both the DICE score and topological metrics.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hu-Topology-Aware%20Segmentation%20Using%20Discrete%20Morse%20Theory-2021-ICLR.pdf}
}
@article{PEBALTianECCV2022, 
year = {2022}, 
title = {{Pixel-wise Energy-biased Abstention Learning for Anomaly Segmentation on Complex Urban Driving Scenes}}, 
author = {Tian, Yu and Liu, Yuyuan and Pang, Guansong and Liu, Fengbei and Chen, Yuanhong and Carneiro, Gustavo}, 
journal = {ECCV}, 
eprint = {2111.12264}, 
abstract = {{State-of-the-art (SOTA) anomaly segmentation approaches on complex urban driving scenes explore pixel-wise classification uncertainty learned from outlier exposure, or external reconstruction models. However, previous uncertainty approaches that directly associate high uncertainty to anomaly may sometimes lead to incorrect anomaly predictions, and external reconstruction models tend to be too inefficient for real-time self-driving embedded systems. In this paper, we propose a new anomaly segmentation method, named pixel-wise energy-biased abstention learning (PEBAL), that explores pixel-wise abstention learning (AL) with a model that learns an adaptive pixel-level anomaly class, and an energy-based model (EBM) that learns inlier pixel distribution. More specifically, PEBAL is based on a non-trivial joint training of EBM and AL, where EBM is trained to output high-energy for anomaly pixels (from outlier exposure) and AL is trained such that these high-energy pixels receive adaptive low penalty for being included to the anomaly class. We extensively evaluate PEBAL against the SOTA and show that it achieves the best performance across four benchmarks. Code is available at https://github.com/tianyu0207/PEBAL.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tian-Pixel-wise%20Energy-biased%20Abstention%20Learning%20for%20Anomaly%20Segmentation%20on%20Complex%20Urban%20Driving%20Scenes-2021-ECCV.pdf}
}
@article{PolyLossLengICLR2022, 
year = {2022}, 
title = {{PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions}}, 
author = {Leng, Zhaoqi and Tan, Mingxing and Liu, Chenxi and Cubuk, Ekin Dogus and Shi, Xiaojie and Cheng, Shuyang and Anguelov, Dragomir}, 
journal = {ICLR}, 
eprint = {2204.12511}, 
abstract = {{Cross-entropy loss and focal loss are the most common choices when training deep neural networks for classification problems. Generally speaking, however, a good loss function can take on much more flexible forms, and should be tailored for different tasks and datasets. Motivated by how functions can be approximated via Taylor expansion, we propose a simple framework, named PolyLoss, to view and design loss functions as a linear combination of polynomial functions. Our PolyLoss allows the importance of different polynomial bases to be easily adjusted depending on the targeting tasks and datasets, while naturally subsuming the aforementioned cross-entropy loss and focal loss as special cases. Extensive experimental results show that the optimal choice within the PolyLoss is indeed dependent on the task and dataset. Simply by introducing one extra hyperparameter and adding one line of code, our Poly-1 formulation outperforms the cross-entropy loss and focal loss on 2D image classification, instance segmentation, object detection, and 3D object detection tasks, sometimes by a large margin.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Leng-PolyLoss-%20A%20Polynomial%20Expansion%20Perspective%20of%20Classification%20Loss%20Functions-2022-ICLR.pdf}
}
@article{EvaluatingVaicenaviciusAISTATS2019, 
year = {2019}, 
title = {{Evaluating model calibration in classification}}, 
author = {Vaicenavicius, Juozas and Widmann, David and Andersson, Carl and Lindsten, Fredrik and Roll, Jacob and Schön, Thomas B}, 
journal = {AISTATS}, 
eprint = {1902.06977}, 
abstract = {{Probabilistic classifiers output a probability distribution on target classes rather than just a class prediction. Besides providing a clear separation of prediction and decision making, the main advantage of probabilistic models is their ability to represent uncertainty about predictions. In safety-critical applications, it is pivotal for a model to possess an adequate sense of uncertainty, which for probabilistic classifiers translates into outputting probability distributions that are consistent with the empirical frequencies observed from realized outcomes. A classifier with such a property is called calibrated. In this work, we develop a general theoretical calibration evaluation framework grounded in probability theory, and point out subtleties present in model calibration evaluation that lead to refined interpretations of existing evaluation techniques. Lastly, we propose new ways to quantify and visualize miscalibration in probabilistic classification, including novel multidimensional reliability diagrams.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Vaicenavicius-Evaluating%20model%20calibration%20in%20classification-2019-AISTATS.pdf}
}
@article{TaTKDLinCVPR2022, 
year = {2022}, 
title = {{Knowledge Distillation via the Target-aware Transformer}}, 
author = {Lin, Sihao and Xie, Hongwei and Wang, Bing and Yu, Kaicheng and Chang, Xiaojun and Liang, Xiaodan and Wang, Gang}, 
journal = {CVPR}, 
eprint = {2205.10793}, 
abstract = {{Knowledge distillation becomes a de facto standard to improve the performance of small neural networks. Most of the previous works propose to regress the representational features from the teacher to the student in a one-to-one spatial matching fashion. However, people tend to overlook the fact that, due to the architecture differences, the semantic information on the same spatial location usually vary. This greatly undermines the underlying assumption of the one-to-one distillation approach. To this end, we propose a novel one-to-all spatial matching knowledge distillation approach. Specifically, we allow each pixel of the teacher feature to be distilled to all spatial locations of the student features given its similarity, which is generated from a target-aware transformer. Our approach surpasses the state-of-the-art methods by a significant margin on various computer vision benchmarks, such as ImageNet, Pascal VOC and COCOStuff10k. Code will be released soon.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lin-Knowledge%20Distillation%20via%20the%20Target-aware%20Transformer-2022-CVPR.pdf}
}
@article{RankSimGongICML2022, 
year = {2022}, 
title = {{RankSim: Ranking Similarity Regularization for Deep Imbalanced Regression}}, 
author = {Gong, Yu and Mori, Greg and Tung, Frederick}, 
journal = {ICML}, 
eprint = {2205.15236}, 
abstract = {{Data imbalance, in which a plurality of the data samples come from a small proportion of labels, poses a challenge in training deep neural networks. Unlike classification, in regression the labels are continuous, potentially boundless, and form a natural ordering. These distinct features of regression call for new techniques that leverage the additional information encoded in label-space relationships. This paper presents the RankSim (ranking similarity) regularizer for deep imbalanced regression, which encodes an inductive bias that samples that are closer in label space should also be closer in feature space. In contrast to recent distribution smoothing based approaches, RankSim captures both nearby and distant relationships: for a given data sample, RankSim encourages the sorted list of its neighbors in label space to match the sorted list of its neighbors in feature space. RankSim is complementary to conventional imbalanced learning techniques, including re-weighting, two-stage training, and distribution smoothing, and lifts the state-of-the-art performance on three imbalanced regression benchmarks: IMDB-WIKI-DIR, AgeDB-DIR, and STS-B-DIR.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Gong-RankSim-%20Ranking%20Similarity%20Regularization%20for%20Deep%20Imbalanced%20Regression-2022-ICML.pdf}
}
@article{DifferentiablePetersenICML2022, 
year = {2022}, 
title = {{Differentiable Top-k Classification Learning}}, 
author = {Petersen, Felix and Kuehne, Hilde and Borgelt, Christian and Deussen, Oliver}, 
journal = {ICML}, 
eprint = {2206.07290}, 
abstract = {{The top-k classification accuracy is one of the core metrics in machine learning. Here, k is conventionally a positive integer, such as 1 or 5, leading to top-1 or top-5 training objectives. In this work, we relax this assumption and optimize the model for multiple k simultaneously instead of using a single k. Leveraging recent advances in differentiable sorting and ranking, we propose a differentiable top-k cross-entropy classification loss. This allows training the network while not only considering the top-1 prediction, but also, e.g., the top-2 and top-5 predictions. We evaluate the proposed loss function for fine-tuning on state-of-the-art architectures, as well as for training from scratch. We find that relaxing k does not only produce better top-5 accuracies, but also leads to top-1 accuracy improvements. When fine-tuning publicly available ImageNet models, we achieve a new state-of-the-art for these models.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Petersen-Differentiable%20Top-k%20Classification%20Learning-2022-ICML.pdf}
}
@article{MakingZhangICML2019, 
year = {2019}, 
title = {{Making Convolutional Networks Shift-Invariant Again}}, 
author = {Zhang, Richard}, 
journal = {ICML}, 
eprint = {1904.11486}, 
abstract = {{Modern convolutional networks are not shift-invariant, as small input shifts or translations can cause drastic changes in the output. Commonly used downsampling methods, such as max-pooling, strided-convolution, and average-pooling, ignore the sampling theorem. The well-known signal processing fix is anti-aliasing by low-pass filtering before downsampling. However, simply inserting this module into deep networks degrades performance; as a result, it is seldomly used today. We show that when integrated correctly, it is compatible with existing architectural components, such as max-pooling and strided-convolution. We observe \textbackslashtextit\{increased accuracy\} in ImageNet classification, across several commonly-used architectures, such as ResNet, DenseNet, and MobileNet, indicating effective regularization. Furthermore, we observe \textbackslashtextit\{better generalization\}, in terms of stability and robustness to input corruptions. Our results demonstrate that this classical signal processing technique has been undeservingly overlooked in modern deep networks. Code and anti-aliased versions of popular networks are available at https://richzhang.github.io/antialiased-cnns/ .}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Making%20Convolutional%20Networks%20Shift-Invariant%20Again-2019-ICML.pdf}
}
@article{TResNetRidnikWACV2021, 
year = {2021}, 
title = {{TResNet: High Performance GPU-Dedicated Architecture}}, 
author = {Ridnik, Tal and Lawen, Hussam and Noy, Asaf and Baruch, Emanuel Ben and Sharir, Gilad and Friedman, Itamar}, 
journal = {WACV}, 
eprint = {2003.13630}, 
abstract = {{Many deep learning models, developed in recent years, reach higher ImageNet accuracy than ResNet50, with fewer or comparable FLOPS count. While FLOPs are often seen as a proxy for network efficiency, when measuring actual GPU training and inference throughput, vanilla ResNet50 is usually significantly faster than its recent competitors, offering better throughput-accuracy trade-off. In this work, we introduce a series of architecture modifications that aim to boost neural networks' accuracy, while retaining their GPU training and inference efficiency. We first demonstrate and discuss the bottlenecks induced by FLOPs-optimizations. We then suggest alternative designs that better utilize GPU structure and assets. Finally, we introduce a new family of GPU-dedicated models, called TResNet, which achieve better accuracy and efficiency than previous ConvNets. Using a TResNet model, with similar GPU throughput to ResNet50, we reach 80.8 top-1 accuracy on ImageNet. Our TResNet models also transfer well and achieve state-of-the-art accuracy on competitive single-label classification datasets such as Stanford cars (96.0\%), CIFAR-10 (99.0\%), CIFAR-100 (91.5\%) and Oxford-Flowers (99.1\%). They also perform well on multi-label classification and object detection tasks. Implementation is available at: https://github.com/mrT23/TResNet.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ridnik-TResNet-%20High%20Performance%20GPU-Dedicated%20Architecture-2021-WACV.pdf}
}
@article{ImageNet-21KRidnikNeurIPSTrackonDatasetsandBenchmarks2021, 
year = {2021}, 
title = {{ImageNet-21K Pretraining for the Masses}}, 
author = {Ridnik, Tal and Ben-Baruch, Emanuel and Noy, Asaf and Zelnik-Manor, Lihi}, 
journal = {NeurIPS Track on Datasets and Benchmarks}, 
eprint = {2104.10972}, 
abstract = {{ImageNet-1K serves as the primary dataset for pretraining deep learning models for computer vision tasks. ImageNet-21K dataset, which is bigger and more diverse, is used less frequently for pretraining, mainly due to its complexity, low accessibility, and underestimation of its added value. This paper aims to close this gap, and make high-quality efficient pretraining on ImageNet-21K available for everyone. Via a dedicated preprocessing stage, utilization of WordNet hierarchical structure, and a novel training scheme called semantic softmax, we show that various models significantly benefit from ImageNet-21K pretraining on numerous datasets and tasks, including small mobile-oriented models. We also show that we outperform previous ImageNet-21K pretraining schemes for prominent new models like ViT and Mixer. Our proposed pretraining pipeline is efficient, accessible, and leads to SoTA reproducible results, from a publicly available dataset. The training code and pretrained models are available at: https://github.com/Alibaba-MIIL/ImageNet21K}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ridnik-ImageNet-21K%20Pretraining%20for%20the%20Masses-2021-NeurIPS%20Track%20on%20Datasets%20and%20Benchmarks.pdf}
}
@article{ZAP-HPOOzturkICML2022, 
year = {2022}, 
title = {{Zero-Shot AutoML with Pretrained Models}}, 
author = {Öztürk, Ekrem and Ferreira, Fabio and Jomaa, Hadi S and Schmidt-Thieme, Lars and Grabocka, Josif and Hutter, Frank}, 
journal = {ICML}, 
eprint = {2206.08476}, 
abstract = {{Given a new dataset D and a low compute budget, how should we choose a pre-trained model to fine-tune to D, and set the fine-tuning hyperparameters without risking overfitting, particularly if D is small? Here, we extend automated machine learning (AutoML) to best make these choices. Our domain-independent meta-learning approach learns a zero-shot surrogate model which, at test time, allows to select the right deep learning (DL) pipeline (including the pre-trained model and fine-tuning hyperparameters) for a new dataset D given only trivial meta-features describing D such as image resolution or the number of classes. To train this zero-shot model, we collect performance data for many DL pipelines on a large collection of datasets and meta-train on this data to minimize a pairwise ranking objective. We evaluate our approach under the strict time limit of the vision track of the ChaLearn AutoDL challenge benchmark, clearly outperforming all challenge contenders.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Öztürk-Zero-Shot%20AutoML%20with%20Pretrained%20Models-2022-ICML.pdf}
}
@article{AMELinICML2022, 
year = {2022}, 
title = {{Measuring the Effect of Training Data on Deep Learning Predictions via Randomized Experiments}}, 
author = {Lin, Jinkun and Zhang, Anqi and Lecuyer, Mathias and Li, Jinyang and Panda, Aurojit and Sen, Siddhartha}, 
journal = {ICML}, 
eprint = {2206.10013}, 
abstract = {{We develop a new, principled algorithm for estimating the contribution of training data points to the behavior of a deep learning model, such as a specific prediction it makes. Our algorithm estimates the AME, a quantity that measures the expected (average) marginal effect of adding a data point to a subset of the training data, sampled from a given distribution. When subsets are sampled from the uniform distribution, the AME reduces to the well-known Shapley value. Our approach is inspired by causal inference and randomized experiments: we sample different subsets of the training data to train multiple submodels, and evaluate each submodel's behavior. We then use a LASSO regression to jointly estimate the AME of each data point, based on the subset compositions. Under sparsity assumptions (\$k \textbackslashll N\$ datapoints have large AME), our estimator requires only \$O(k\textbackslashlog N)\$ randomized submodel trainings, improving upon the best prior Shapley value estimators.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lin-Measuring%20the%20Effect%20of%20Training%20Data%20on%20Deep%20Learning%20Predictions%20via%20Randomized%20Experiments-2022-ICML.pdf}
}
@article{POEMMingICML2022, 
year = {2022}, 
title = {{POEM: Out-of-Distribution Detection with Posterior Sampling}}, 
author = {Ming, Yifei and Fan, Ying and Li, Yixuan}, 
journal = {ICML}, 
eprint = {2206.13687}, 
abstract = {{Out-of-distribution (OOD) detection is indispensable for machine learning models deployed in the open world. Recently, the use of an auxiliary outlier dataset during training (also known as outlier exposure) has shown promising performance. As the sample space for potential OOD data can be prohibitively large, sampling informative outliers is essential. In this work, we propose a novel posterior sampling-based outlier mining framework, POEM, which facilitates efficient use of outlier data and promotes learning a compact decision boundary between ID and OOD data for improved detection. We show that POEM establishes state-of-the-art performance on common benchmarks. Compared to the current best method that uses a greedy sampling strategy, POEM improves the relative performance by 42.0\% and 24.2\% (FPR95) on CIFAR-10 and CIFAR-100, respectively. We further provide theoretical insights on the effectiveness of POEM for OOD detection.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ming-POEM-%20Out-of-Distribution%20Detection%20with%20Posterior%20Sampling-2022-ICML.pdf}
}
@article{CutMixYunICCV2019, 
year = {2019}, 
title = {{CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features}}, 
author = {Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon}, 
journal = {ICCV}, 
eprint = {1905.04899}, 
abstract = {{Regional dropout strategies have been proposed to enhance the performance of convolutional neural network classifiers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout remove informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it leads to information loss and inefficiency during training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on the ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classifier, when used as a pretrained model, results in consistent performance gains in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances. Source code and pretrained models are available at https://github.com/clovaai/CutMix-PyTorch .}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yun-CutMix-%20Regularization%20Strategy%20to%20Train%20Strong%20Classifiers%20with%20Localizable%20Features-2019-ICCV.pdf}
}
@article{InceptionSzegedyCVPR2015, 
year = {2015}, 
title = {{Going Deeper with Convolutions}}, 
author = {Szegedy, Christian and Wei, Liu and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Dumitru, Erhan and Vanhoucke, Vincent and Rabinovich, Andrew}, 
journal = {CVPR}, 
abstract = {{We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the Im-ageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Christian-Going%20Deeper%20with%20Convolutions-2015-CVPR.pdf}
}
@article{InceptionV2V3SzegedyCVPR2016, 
year = {2016}, 
title = {{Rethinking the Inception Architecture for Computer Vision}}, 
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew}, 
journal = {CVPR}, 
eprint = {1512.00567}, 
abstract = {{Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error on the validation set (3.6\% error on the test set) and 17.3\% top-1 error on the validation set.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Szegedy-Rethinking%20the%20Inception%20Architecture%20for%20Computer%20Vision-2016-CVPR.pdf}
}
@article{InceptionV4SzegedyAAAI2017, 
year = {2017}, 
title = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}}, 
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex}, 
journal = {AAAI}, 
eprint = {1602.07261}, 
abstract = {{Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Szegedy-Inception-v4,%20Inception-ResNet%20and%20the%20Impact%20of%20Residual%20Connections%20on%20Learning-2017-AAAI.pdf}
}
@article{ADE20KZhouCVPR2017, 
year = {2017}, 
title = {{Scene Parsing Through ADE20K Dataset}}, 
author = {Zhou, Bolei and Zhao, Hang and Puig, Xavier and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio}, 
journal = {CVPR}, 
abstract = {{Scene parsing, or recognizing and segmenting objects and stuff in an image, is one of the key problems in computer vision. Despite the community's efforts in data collection, there are still few image datasets covering a wide range of scenes and object categories with dense and detailed annotations for scene parsing. In this paper, we introduce and analyze the ADE20K dataset, spanning diverse annotations of scenes, objects, parts of objects, and in some cases even parts of parts. A scene parsing benchmark is built upon the ADE20K with 150 object and stuff classes included. Several segmentation baseline models are evaluated on the benchmark. A novel network design called Cascade Segmentation Module is proposed to parse a scene into stuff, objects, and object parts in a cascade and improve over the baselines. We further show that the trained scene parsing networks can lead to applications such as image content removal and scene synthesis11Dataset and pretrained models are available at http://groups.csail.mit.edu/vision/datasets/ADE20K/. Dataset and pretrained models are available at http://groups.csail.mit.edu/vision/datasets/ADE20K/}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhou-Scene%20Parsing%20Through%20ADE20K%20Dataset-2017-CVPR.pdf}
}
@article{OHEMShrivastavaCVPR2016, 
year = {2016}, 
title = {{Training Region-Based Object Detectors with Online Hard Example Mining}}, 
author = {Shrivastava, Abhinav and Gupta, Abhinav and Girshick, Ross}, 
journal = {CVPR}, 
abstract = {{The field of object detection has made significant advances riding on the wave of region-based ConvNets, but their training procedure still includes many heuristics and hyperparameters that are costly to tune. We present a simple yet surprisingly effective online hard example mining (OHEM) algorithm for training region-based ConvNet detectors. Our motivation is the same as it has always been — detection datasets contain an overwhelming number of easy examples and a small number of hard examples. Automatic selection of these hard examples can make training more effective and efficient. OHEM is a simple and intuitive algorithm that eliminates several heuristics and hyperparameters in common use. But more importantly, it yields consistent and significant boosts in detection performance on benchmarks like PASCAL VOC 2007 and 2012. Its effectiveness increases as datasets become larger and more difficult, as demonstrated by the results on the MS COCO dataset. Moreover, combined with complementary advances in the field, OHEM leads to state-of-the-art results of 78.9\% and 76.3\% mAP on PASCAL VOC 2007 and 2012 respectively.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Shrivastava-Training%20Region-Based%20Object%20Detectors%20with%20Online%20Hard%20Example%20Mining-2016-CVPR.pdf}
}
@article{DUCWangWACV2018, 
year = {2018}, 
title = {{Understanding Convolution for Semantic Segmentation}}, 
author = {Wang, Panqu and Chen, Pengfei and Yuan, Ye and Liu, Ding and Huang, Zehua and Hou, Xiaodi and Cottrell, Garrison}, 
journal = {WACV}, 
abstract = {{Recent advances in deep learning, especially deep convolutional neural networks (CNNs), have led to significant improvement over previous semantic segmentation systems. Here we show how to improve pixel-wise semantic segmentation by manipulating convolution-related operations that are of both theoretical and practical value. First, we design dense upsampling convolution (DUC) to generate pixel-level prediction, which is able to capture and decode more detailed information that is generally missing in bilinear upsampling. Second, we propose a hybrid dilated convolution (HDC) framework in the encoding phase. This framework 1) effectively enlarges the receptive fields (RF) of the network to aggregate global information; 2) alleviates what we call the “gridding issue” caused by the standard dilated convolution operation. We evaluate our approaches thoroughly on the Cityscapes dataset, and achieve a state-of-art result of 80.1\% mIOU in the test set at the time of submission. We also have achieved state-of-the-art overall on the KITTI road estimation benchmark and the PASCAL VOC2012 segmentation task. Our source code can be found at https://github.com/TuSimple/TuSimple-DUC.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Understanding%20Convolution%20for%20Semantic%20Segmentation-2018-WACV.pdf}
}
@article{t-CEFengIJCAI2020, 
year = {2020}, 
title = {{Can Cross Entropy Loss Be Robust to Label Noise?}}, 
author = {Feng, Lei and Shu, Senlin and Lin, Zhuoyi and Lv, Fengmao and Li, Li and An, Bo}, 
journal = {IJCAI}, 
abstract = {{Trained with the standard cross entropy loss, deep neural networks can achieve great performance on correctly labeled data. However, if the training data is corrupted with label noise, deep models tend to overfit the noisy labels, thereby achieving poor generation performance. To remedy this issue, several loss functions have been proposed and demonstrated to be robust to label noise. Although most of the robust loss functions stem from Categorical Cross Entropy (CCE) loss, they fail to embody the intrinsic relationships between CCE and other loss functions. In this paper, we propose a general framework dubbed Taylor cross entropy loss to train deep models in the presence of label noise. Specifically, our framework enables to weight the extent of fitting the training labels by controlling the order of Taylor Series for CCE, hence it can be robust to label noise. In addition, our framework clearly reveals the intrinsic relationships between CCE and other loss functions, such as Mean Absolute Error (MAE) and Mean Squared Error (MSE). Moreover, we present a detailed theoretical analysis to certify the robustness of this framework. Extensive experimental results on benchmark datasets demonstrate that our proposed approach significantly outperforms the state-of-the-art counterparts.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Feng-Can%20Cross%20Entropy%20Loss%20Be%20Robust%20to%20Label%20Noise--2020-IJCAI.pdf}
}
@article{NoFrostWangICML2022, 
year = {2022}, 
title = {{Removing Batch Normalization Boosts Adversarial Training}}, 
author = {Wang, Haotao and Zhang, Aston and Zheng, Shuai and Shi, Xingjian and Li, Mu and Wang, Zhangyang}, 
journal = {ICML}, 
eprint = {2207.01156}, 
abstract = {{Adversarial training (AT) defends deep neural networks against adversarial attacks. One challenge that limits its practical application is the performance degradation on clean samples. A major bottleneck identified by previous works is the widely used batch normalization (BN), which struggles to model the different statistics of clean and adversarial training samples in AT. Although the dominant approach is to extend BN to capture this mixture of distribution, we propose to completely eliminate this bottleneck by removing all BN layers in AT. Our normalizer-free robust training (NoFrost) method extends recent advances in normalizer-free networks to AT for its unexplored advantage on handling the mixture distribution challenge. We show that NoFrost achieves adversarial robustness with only a minor sacrifice on clean sample accuracy. On ImageNet with ResNet50, NoFrost achieves \$74.06\textbackslash\%\$ clean accuracy, which drops merely \$2.00\textbackslash\%\$ from standard training. In contrast, BN-based AT obtains \$59.28\textbackslash\%\$ clean accuracy, suffering a significant \$16.78\textbackslash\%\$ drop from standard training. In addition, NoFrost achieves a \$23.56\textbackslash\%\$ adversarial robustness against PGD attack, which improves the \$13.57\textbackslash\%\$ robustness in BN-based AT. We observe better model smoothness and larger decision margins from NoFrost, which make the models less sensitive to input perturbations and thus more robust. Moreover, when incorporating more data augmentations into NoFrost, it achieves comprehensive robustness against multiple distribution shifts. Code and pre-trained models are public at https://github.com/amazon-research/normalizer-free-robust-training.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Removing%20Batch%20Normalization%20Boosts%20Adversarial%20Training-2022-ICML.pdf}
}
@article{HowMahmoodCVPR2022, 
year = {2022}, 
title = {{How Much More Data Do I Need? Estimating Requirements for Downstream Tasks}}, 
author = {Mahmood, Rafid and Lucas, James and Acuna, David and Li, Daiqing and Philion, Jonah and Alvarez, Jose M and Yu, Zhiding and Fidler, Sanja and Law, Marc T}, 
journal = {CVPR}, 
eprint = {2207.01725}, 
abstract = {{Given a small training data set and a learning algorithm, how much more data is necessary to reach a target validation or test performance? This question is of critical importance in applications such as autonomous driving or medical imaging where collecting data is expensive and time-consuming. Overestimating or underestimating data requirements incurs substantial costs that could be avoided with an adequate budget. Prior work on neural scaling laws suggest that the power-law function can fit the validation performance curve and extrapolate it to larger data set sizes. We find that this does not immediately translate to the more difficult downstream task of estimating the required data set size to meet a target performance. In this work, we consider a broad class of computer vision tasks and systematically investigate a family of functions that generalize the power-law function to allow for better estimation of data requirements. Finally, we show that incorporating a tuned correction factor and collecting over multiple rounds significantly improves the performance of the data estimators. Using our guidelines, practitioners can accurately estimate data requirements of machine learning systems to gain savings in both development time and data acquisition costs.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Mahmood-How%20Much%20More%20Data%20Do%20I%20Need-%20Estimating%20Requirements%20for%20Downstream%20Tasks-2022-CVPR.pdf}
}
@article{KFYangECCV2022, 
year = {2022}, 
title = {{Factorizing Knowledge in Neural Networks}}, 
author = {Yang, Xingyi and Ye, Jingwen and Wang, Xinchao}, 
journal = {ECCV}, 
eprint = {2207.03337}, 
abstract = {{In this paper, we explore a novel and ambitious knowledge-transfer task, termed Knowledge Factorization\textbackslashtextasciitilde(KF). The core idea of KF lies in the modularization and assemblability of knowledge: given a pretrained network model as input, KF aims to decompose it into several factor networks, each of which handles only a dedicated task and maintains task-specific knowledge factorized from the source network. Such factor networks are task-wise disentangled and can be directly assembled, without any fine-tuning, to produce the more competent combined-task networks. In other words, the factor networks serve as Lego-brick-like building blocks, allowing us to construct customized networks in a plug-and-play manner. Specifically, each factor network comprises two modules, a common-knowledge module that is task-agnostic and shared by all factor networks, alongside with a task-specific module dedicated to the factor network itself. We introduce an information-theoretic objective, InfoMax-Bottleneck\textbackslashtextasciitilde(IMB), to carry out KF by optimizing the mutual information between the learned representations and input. Experiments across various benchmarks demonstrate that, the derived factor networks yield gratifying performances on not only the dedicated tasks but also disentanglement, while enjoying much better interpretability and modularity. Moreover, the learned common-knowledge representations give rise to impressive results on transfer learning.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-Factorizing%20Knowledge%20in%20Neural%20Networks-2022-ECCV.pdf}
}
@article{LeGRChinCVPR2020, 
year = {2020}, 
title = {{Towards Efficient Model Compression via Learned Global Ranking}}, 
author = {Chin, Ting-Wu and Ding, Ruizhou and Zhang, Cha and Marculescu, Diana}, 
journal = {CVPR}, 
abstract = {{Pruning convolutional filters has demonstrated its effectiveness in compressing ConvNets. Prior art in filter pruning requires users to specify a target model complexity (e.g., model size or FLOP count) for the resulting architecture. However, determining a target model complexity can be difficult for optimizing various embodied AI applications such as autonomous robots, drones, and user-facing applications. First, both the accuracy and the speed of ConvNets can affect the performance of the application. Second, the performance of the application can be hard to assess without evaluating ConvNets during inference. As a consequence, finding a sweet-spot between the accuracy and speed via filter pruning, which needs to be done in a trial-and-error fashion, can be time-consuming. This work takes a first step toward making this process more efficient by altering the goal of model compression to producing a set of ConvNets with various accuracy and latency trade-offs instead of producing one ConvNet targeting some pre-defined latency constraint. To this end, we propose to learn a global ranking of the filters across different layers of the ConvNet, which is used to obtain a set of ConvNet architectures that have different accuracy/latency trade-offs by pruning the bottom-ranked filters. Our proposed algorithm, LeGR, is shown to be 2× to 3× faster than prior work while having comparable or better performance when targeting seven pruned ResNet-56 with different accuracy/FLOPs profiles on the CIFAR-100 dataset. Additionally, we have evaluated LeGR on ImageNet and Bird-200 with ResNet-50 and Mo-bileNetV2 to demonstrate its effectiveness. Code available at https://github.com/cmu-enyac/LeGR.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chin-Towards%20Efficient%20Model%20Compression%20via%20Learned%20Global%20Ranking-2020-CVPR.pdf}
}
@article{H-divergenceZhaoICLR2022, 
year = {2022}, 
title = {{Comparing Distributions by Measuring Differences that Affect Decision Making}}, 
author = {Zhao, Shengjia and Sinha, Abhishek and He, Yutong and Perreault, Aidan and Song, Jiaming and Ermon, Stefano}, 
journal = {ICLR}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhao-Comparing%20Distributions%20by%20Measuring%20Differences%20that%20Affect%20Decision%20Making-2022-ICLR.pdf}
}
@article{UniversalMetzenICCV2017, 
year = {2017}, 
title = {{Universal Adversarial Perturbations Against Semantic Image Segmentation}}, 
author = {Metzen, Jan Hendrik and Kumar, Mummadi Chaithanya and Brox, Thomas and Fischer, Volker}, 
journal = {ICCV}, 
eprint = {1704.05712}, 
abstract = {{While deep learning is remarkably successful on perceptual tasks, it was also shown to be vulnerable to adversarial perturbations of the input. These perturbations denote noise added to the input that was generated specifically to fool the system while being quasi-imperceptible for humans. More severely, there even exist universal perturbations that are input-agnostic but fool the network on the majority of inputs. While recent work has focused on image classification, this work proposes attacks against semantic image segmentation: we present an approach for generating (universal) adversarial perturbations that make the network yield a desired target segmentation as output. We show empirically that there exist barely perceptible universal noise patterns which result in nearly the same predicted segmentation for arbitrary inputs. Furthermore, we also show the existence of universal noise which removes a target class (e.g., all pedestrians) from the segmentation while leaving the segmentation mostly unchanged otherwise.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Metzen-Universal%20Adversarial%20Perturbations%20Against%20Semantic%20Image%20Segmentation-2017-ICCV.pdf}
}
@article{PiCOWangICLR2022, 
year = {2022}, 
title = {{PiCO: Contrastive Label Disambiguation for Partial Label Learning}}, 
author = {Wang, Haobo and Xiao, Ruixuan and Li, Yixuan and Feng, Lei and Niu, Gang and Chen, Gang and Zhao, Junbo}, 
journal = {ICLR}, 
eprint = {2201.08984}, 
abstract = {{Partial label learning (PLL) is an important problem that allows each training example to be labeled with a coarse candidate set, which well suits many real-world data annotation scenarios with label ambiguity. Despite the promise, the performance of PLL often lags behind the supervised counterpart. In this work, we bridge the gap by addressing two key research challenges in PLL -- representation learning and label disambiguation -- in one coherent framework. Specifically, our proposed framework PiCO consists of a contrastive learning module along with a novel class prototype-based label disambiguation algorithm. PiCO produces closely aligned representations for examples from the same classes and facilitates label disambiguation. Theoretically, we show that these two components are mutually beneficial, and can be rigorously justified from an expectation-maximization (EM) algorithm perspective. Extensive experiments demonstrate that PiCO significantly outperforms the current state-of-the-art approaches in PLL and even achieves comparable results to fully supervised learning. Code and data available: https://github.com/hbzju/PiCO.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-PiCO-%20Contrastive%20Label%20Disambiguation%20for%20Partial%20Label%20Learning-2022-ICLR.pdf}
}
@article{OnSutskeverICML2013, 
year = {2013}, 
title = {{On the importance of initialization and momentum in deep learning}}, 
author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey}, 
journal = {ICML}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Sutskever-On%20the%20importance%20of%20initialization%20and%20momentum%20in%20deep%20learning-2013-ICML.pdf}
}
@article{MarginCalibrationYuIJCV2021, 
year = {2021}, 
title = {{Distribution-Aware Margin Calibration for Semantic Segmentation in Images}}, 
author = {Yu, Litao and Li, Zhibin and Xu, Min and Gao, Yongsheng and Luo, Jiebo and Zhang, Jian}, 
journal = {IJCV}, 
eprint = {2112.11554}, 
abstract = {{The Jaccard index, also known as Intersection-over-Union (IoU), is one of the most critical evaluation metrics in image semantic segmentation. However, direct optimization of IoU score is very difficult because the learning objective is neither differentiable nor decomposable. Although some algorithms have been proposed to optimize its surrogates, there is no guarantee provided for the generalization ability. In this paper, we propose a margin calibration method, which can be directly used as a learning objective, for an improved generalization of IoU over the data-distribution, underpinned by a rigid lower bound. This scheme theoretically ensures a better segmentation performance in terms of IoU score. We evaluated the effectiveness of the proposed margin calibration method on seven image datasets, showing substantial improvements in IoU score over other learning objectives using deep segmentation models.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yu-Distribution-Aware%20Margin%20Calibration%20for%20Semantic%20Segmentation%20in%20Images-2021-IJCV.pdf}
}
@article{MemoryNetworksWestonICLR2015, 
year = {2015}, 
title = {{Memory Networks}}, 
author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine}, 
journal = {ICLR}, 
eprint = {1410.3916}, 
abstract = {{We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Weston-Memory%20Networks-2015-ICLR.pdf}
}
@article{EfficientTompsonCVPR2015, 
year = {2015}, 
title = {{Efficient object localization using Convolutional Networks}}, 
author = {Tompson, Jonathan and Goroshin, Ross and Jain, Arjun and LeCun, Yann and Bregler, Christoph}, 
journal = {CVPR}, 
abstract = {{Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet architectures include pooling and sub-sampling layers which reduce computational requirements, introduce invariance and prevent over-training. These benefits of pooling come at the cost of reduced localization accuracy. We introduce a novel architecture which includes an efficient ‘position refinement’ model that is trained to estimate the joint offset location within a small region of the image. This refinement model is jointly trained in cascade with a state-of-the-art ConvNet model [21] to achieve improved accuracy in human joint location estimation. We show that the variance of our detector approaches the variance of human annotations on the FLIC [20] dataset and outperforms all existing approaches on the MPlI-human-pose dataset [1].}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tompson-Efficient%20object%20localization%20using%20Convolutional%20Networks-2015-CVPR.pdf}
}
@article{LayerNormalizationBaNeurIPSDeepLearningSymposium2016, 
year = {2016}, 
title = {{Layer Normalization}}, 
author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E}, 
journal = {NeurIPS Deep Learning Symposium}, 
eprint = {1607.06450}, 
abstract = {{Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ba-Layer%20Normalization-2016-NeurIPS%20Deep%20Learning%20Symposium.pdf}
}
@article{DeeplyHouTPAMI2018, 
year = {2018}, 
title = {{Deeply Supervised Salient Object Detection with Short Connections}}, 
author = {Hou, Qibin and Cheng, Ming-Ming and Hu, Xiaowei and Borji, Ali and Tu, Zhuowen and Torr, Philip H. S.}, 
journal = {TPAMI}, 
issn = {0162-8828}, 
doi = {10.1109/tpami.2018.2815688}, 
pmid = {29993862}, 
eprint = {1611.04849}, 
abstract = {{Recent progress on salient object detection is substantial, benefiting mostly from the explosive development of Convolutional Neural Networks (CNNs). Semantic segmentation and salient object detection algorithms developed lately have been mostly based on Fully Convolutional Neural Networks (FCNs). There is still a large room for improvement over the generic FCN models that do not explicitly deal with the scale-space problem. The Holistically-Nested Edge Detector (HED) provides a skip-layer structure with deep supervision for edge and boundary detection, but the performance gain of HED on saliency detection is not obvious. In this paper, we propose a new salient object detection method by introducing short connections to the skip-layer structures within the HED architecture. Our framework takes full advantage of multi-level and multi-scale features extracted from FCNs, providing more advanced representations at each layer, a property that is critically needed to perform segment detection. Our method produces state-of-the-art results on 5 widely tested salient object detection benchmarks, with advantages in terms of efficiency (0.08 seconds per image), effectiveness, and simplicity over the existing algorithms. Beyond that, we conduct an exhaustive analysis of the role of training data on performance. We provide a training set for future research and fair comparisons.}}, 
pages = {815--828}, 
number = {4}, 
volume = {41}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hou-Deeply%20Supervised%20Salient%20Object%20Detection%20with%20Short%20Connections-2018-TPAMI.pdf}
}
@article{ClassMixOlssonWACV2021, 
year = {2021}, 
title = {{ClassMix: Segmentation-Based Data Augmentation for Semi-Supervised Learning}}, 
author = {Olsson, Viktor and Tranheden, Wilhelm and Pinto, Juliano and Svensson, Lennart}, 
journal = {WACV}, 
abstract = {{The state of the art in semantic segmentation is steadily increasing in performance, resulting in more precise and reliable segmentations in many different applications. However, progress is limited by the cost of generating labels for training, which sometimes requires hours of manual labor for a single image. Because of this, semi-supervised methods have been applied to this task, with varying degrees of success. A key challenge is that common augmentations used in semi-supervised classification are less effective for semantic segmentation. We propose a novel data augmentation mechanism called ClassMix, which generates augmentations by mixing unlabelled samples, by leveraging on the network’s predictions for respecting object boundaries. We evaluate this augmentation technique on two common semi-supervised semantic segmentation benchmarks, showing that it attains state-of-the-art results. Lastly, we also provide extensive ablation studies comparing different design decisions and training regimes.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Olsson-ClassMix-%20Segmentation-Based%20Data%20Augmentation%20for%20Semi-Supervised%20Learning-2021-WACV.pdf}
}
@article{MeanTeacherTarvainenNeurIPS2017, 
year = {2017}, 
title = {{Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results}}, 
author = {Tarvainen, Antti and Valpola, Harri}, 
journal = {NeurIPS}, 
eprint = {1703.01780}, 
abstract = {{The recently proposed Temporal Ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets. To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional benefit, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling. Without changing the network architecture, Mean Teacher achieves an error rate of 4.35\% on SVHN with 250 labels, outperforming Temporal Ensembling trained with 1000 labels. We also show that a good network architecture is crucial to performance. Combining Mean Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with 4000 labels from 10.55\% to 6.28\%, and on ImageNet 2012 with 10\% of the labels from 35.24\% to 9.11\%.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tarvainen-Mean%20teachers%20are%20better%20role%20models-%20Weight-averaged%20consistency%20targets%20improve%20semi-supervised%20deep%20learning%20results-2017-NeurIPS.pdf}
}
@article{MixMatchBerthelotNeurIPS2019, 
year = {2019}, 
title = {{MixMatch: A Holistic Approach to Semi-Supervised Learning}}, 
author = {Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin}, 
journal = {NeurIPS}, 
eprint = {1905.02249}, 
abstract = {{Semi-supervised learning has proven to be a powerful paradigm for leveraging unlabeled data to mitigate the reliance on large labeled datasets. In this work, we unify the current dominant approaches for semi-supervised learning to produce a new algorithm, MixMatch, that works by guessing low-entropy labels for data-augmented unlabeled examples and mixing labeled and unlabeled data using MixUp. We show that MixMatch obtains state-of-the-art results by a large margin across many datasets and labeled data amounts. For example, on CIFAR-10 with 250 labels, we reduce error rate by a factor of 4 (from 38\% to 11\%) and by a factor of 2 on STL-10. We also demonstrate how MixMatch can help achieve a dramatically better accuracy-privacy trade-off for differential privacy. Finally, we perform an ablation study to tease apart which components of MixMatch are most important for its success.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Berthelot-MixMatch-%20A%20Holistic%20Approach%20to%20Semi-Supervised%20Learning-2019-NeurIPS.pdf}
}
@article{TemporalEnsemblingLaineICLR2017, 
year = {2017}, 
title = {{Temporal Ensembling for Semi-Supervised Learning}}, 
author = {Laine, Samuli and Aila, Timo}, 
journal = {ICLR}, 
eprint = {1610.02242}, 
abstract = {{In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44\% to 7.05\% in SVHN with 500 labels and from 18.63\% to 16.55\% in CIFAR-10 with 4000 labels, and further to 5.12\% and 12.16\% by enabling the standard augmentations. We additionally obtain a clear improvement in CIFAR-100 classification accuracy by using random images from the Tiny Images dataset as unlabeled extra inputs during training. Finally, we demonstrate good tolerance to incorrect labels.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Laine-Temporal%20Ensembling%20for%20Semi-Supervised%20Learning-2017-ICLR.pdf}
}
@article{DistillationPapernotS&P2016, 
year = {2016}, 
title = {{Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks}}, 
author = {Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram}, 
journal = {S\&P}, 
abstract = {{Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95\% to less than 0.5\% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800\% on one of the DNNs we tested.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Papernot-Distillation%20as%20a%20Defense%20to%20Adversarial%20Perturbations%20Against%20Deep%20Neural%20Networks-2016-S&P.pdf}
}
@article{AGiftYimCVPR2017, 
year = {2017}, 
title = {{A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning}}, 
author = {Yim, Junho and Joo, Donggyu and Bae, Jihoon and Kim, Junmo}, 
journal = {CVPR}, 
abstract = {{We introduce a novel technique for knowledge transfer, where knowledge from a pretrained deep neural network (DNN) is distilled and transferred to another DNN. As the DNN maps from the input space to the output space through many layers sequentially, we define the distilled knowledge to be transferred in terms of flow between layers, which is calculated by computing the inner product between features from two layers. When we compare the student DNN and the original network with the same size as the student DNN but trained without a teacher network, the proposed method of transferring the distilled knowledge as the flow between two layers exhibits three important phenomena: (1) the student DNN that learns the distilled knowledge is optimized much faster than the original model; (2) the student DNN outperforms the original DNN; and (3) the student DNN can learn the distilled knowledge from a teacher DNN that is trained at a different task, and the student DNN outperforms the original DNN that is trained from scratch.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yim-A%20Gift%20from%20Knowledge%20Distillation-%20Fast%20Optimization,%20Network%20Minimization%20and%20Transfer%20Learning-2017-CVPR.pdf}
}
@article{pix2pixIsolaCVPR2017, 
year = {2017}, 
title = {{Image-to-Image Translation with Conditional Adversarial Networks}}, 
author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.}, 
journal = {CVPR}, 
abstract = {{We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Isola-Image-to-Image%20Translation%20with%20Conditional%20Adversarial%20Networks-2017-CVPR.pdf}
}
@article{CPCOordarXiv2018, 
year = {2018}, 
title = {{Representation Learning with Contrastive Predictive Coding}}, 
author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol}, 
journal = {arXiv}, 
eprint = {1807.03748}, 
abstract = {{While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Oord-Representation%20Learning%20with%20Contrastive%20Predictive%20Coding-2018-arXiv.pdf}
}
@article{AdderNetChenCVPR2020, 
year = {2020}, 
title = {{AdderNet: Do We Really Need Multiplications in Deep Learning?}}, 
author = {Chen, Hanting and Wang, Yunhe and Xu, Chunjing and Shi, Boxin and Xu, Chao and Tian, Qi and Xu, Chang}, 
journal = {CVPR}, 
abstract = {{Compared with cheap addition operation, multiplication operation is of much higher computation complexity. The widely-used convolutions in deep neural networks are exactly cross-correlation to measure the similarity between input feature and convolution filters, which involves massive multiplications between float values. In this paper, we present adder networks (AdderNets) to trade these massive multiplications in deep neural networks, especially convolutional neural networks (CNNs), for much cheaper additions to reduce computation costs. In AdderNets, we take the \$\textbackslashell\_\{1\}\$-norm distance between filters and input feature as the output response. The influence of this new similarity measure on the optimization of neural network have been thoroughly analyzed. To achieve a better performance, we develop a special back-propagation approach for Adder-Nets by investigating the full-precision gradient. We then propose an adaptive learning rate strategy to enhance the training procedure of AdderNets according to the magnitude of each neuron's gradient. As a result, the proposed AdderNets can achieve 74.9\% Top-1 accuracy 91.7\% Top-5 accuracy using ResNet-50 on the ImageNet dataset without any multiplication in convolutional layer. The codes are publicly available at: https://github.com/huawei-noah/AdderNet.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-AdderNet-%20Do%20We%20Really%20Need%20Multiplications%20in%20Deep%20Learning--2020-CVPR.pdf}
}
@article{HierarchicalTaoarXiv2020, 
year = {2020}, 
title = {{Hierarchical Multi-Scale Attention for Semantic Segmentation}}, 
author = {Tao, Andrew and Sapra, Karan and Catanzaro, Bryan}, 
journal = {arXiv}, 
eprint = {2005.10821}, 
abstract = {{Multi-scale inference is commonly used to improve the results of semantic segmentation. Multiple images scales are passed through a network and then the results are combined with averaging or max pooling. In this work, we present an attention-based approach to combining multi-scale predictions. We show that predictions at certain scales are better at resolving particular failures modes, and that the network learns to favor those scales for such cases in order to generate better predictions. Our attention mechanism is hierarchical, which enables it to be roughly 4x more memory efficient to train than other recent approaches. In addition to enabling faster training, this allows us to train with larger crop sizes which leads to greater model accuracy. We demonstrate the result of our method on two datasets: Cityscapes and Mapillary Vistas. For Cityscapes, which has a large number of weakly labelled images, we also leverage auto-labelling to improve generalization. Using our approach we achieve a new state-of-the-art results in both Mapillary (61.1 IOU val) and Cityscapes (85.1 IOU test).}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tao-Hierarchical%20Multi-Scale%20Attention%20for%20Semantic%20Segmentation-2020-arXiv.pdf}
}
@article{SCANGansbekeECCV2020, 
year = {2020}, 
title = {{SCAN: Learning to Classify Images without Labels}}, 
author = {Gansbeke, Wouter Van and Vandenhende, Simon and Georgoulis, Stamatios and Proesmans, Marc and Gool, Luc Van}, 
journal = {ECCV}, 
eprint = {2005.12320}, 
abstract = {{Can we automatically group images into semantically meaningful clusters when ground-truth annotations are absent? The task of unsupervised image classification remains an important, and open challenge in computer vision. Several recent approaches have tried to tackle this problem in an end-to-end fashion. In this paper, we deviate from recent works, and advocate a two-step approach where feature learning and clustering are decoupled. First, a self-supervised task from representation learning is employed to obtain semantically meaningful features. Second, we use the obtained features as a prior in a learnable clustering approach. In doing so, we remove the ability for cluster learning to depend on low-level features, which is present in current end-to-end learning approaches. Experimental evaluation shows that we outperform state-of-the-art methods by large margins, in particular +26.6\% on CIFAR10, +25.0\% on CIFAR100-20 and +21.3\% on STL10 in terms of classification accuracy. Furthermore, our method is the first to perform well on a large-scale dataset for image classification. In particular, we obtain promising results on ImageNet, and outperform several semi-supervised learning methods in the low-data regime without the use of any ground-truth annotations. The code is made publicly available at https://github.com/wvangansbeke/Unsupervised-Classification.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Gansbeke-SCAN-%20Learning%20to%20Classify%20Images%20without%20Labels-2020-ECCV.pdf}
}
@article{SETRZhengCVPR2021, 
year = {2021}, 
title = {{Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers}}, 
author = {Zheng, Sixiao and Lu, Jiachen and Zhao, Hengshuang and Zhu, Xiatian and Luo, Zekun and Wang, Yabiao and Fu, Yanwei and Feng, Jianfeng and Xiang, Tao and Torr, Philip H.S. and Zhang, Li}, 
journal = {CVPR}, 
abstract = {{Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (i.e., without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28\% mIoU), Pascal Context (55.83\% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zheng-Rethinking%20Semantic%20Segmentation%20from%20a%20Sequence-to-Sequence%20Perspective%20with%20Transformers-2021-CVPR.pdf}
}
@article{UnderstandingEthayarajhICML2022, 
year = {2022}, 
title = {{Understanding Dataset Difficulty with \$\textbackslashmathcal\{V\}\$-Usable Information}}, 
author = {Ethayarajh, Kawin and Choi, Yejin and Swayamdipta, Swabha}, 
journal = {ICML}, 
eprint = {2110.08420}, 
abstract = {{Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty -- w.r.t. a model \$\textbackslashmathcal\{V\}\$ -- as the lack of \$\textbackslashmathcal\{V\}\$-\$\textbackslashtextit\{usable information\}\$ (Xu et al., 2019), where a lower value indicates a more difficult dataset for \$\textbackslashmathcal\{V\}\$. We further introduce \$\textbackslashtextit\{pointwise \$\textbackslashmathcal\{V\}\$-information\}\$ (PVI) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, \$\textbackslashmathcal\{V\}\$-\$\textbackslashtextit\{usable information\}\$ and PVI also permit the converse: for a given model \$\textbackslashmathcal\{V\}\$, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used NLP benchmarks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ethayarajh-Understanding%20Dataset%20Difficulty%20with%20$-mathcal{V}$-Usable%20Information-2022-ICML.pdf}
}
@article{G-MixupHanICML2022, 
year = {2022}, 
title = {{G-Mixup: Graph Data Augmentation for Graph Classification}}, 
author = {Han, Xiaotian and Jiang, Zhimeng and Liu, Ninghao and Hu, Xia}, 
journal = {ICML}, 
eprint = {2202.07179}, 
abstract = {{This work develops \textbackslashemph\{mixup for graph data\}. Mixup has shown superiority in improving the generalization and robustness of neural networks by interpolating features and labels between two random samples. Traditionally, Mixup can work on regular, grid-like, and Euclidean data such as image or tabular data. However, it is challenging to directly adopt Mixup to augment graph data because different graphs typically: 1) have different numbers of nodes; 2) are not readily aligned; and 3) have unique typologies in non-Euclidean space. To this end, we propose \$\textbackslashmathcal\{G\}\$-Mixup to augment graphs for graph classification by interpolating the generator (i.e., graphon) of different classes of graphs. Specifically, we first use graphs within the same class to estimate a graphon. Then, instead of directly manipulating graphs, we interpolate graphons of different classes in the Euclidean space to get mixed graphons, where the synthetic graphs are generated through sampling based on the mixed graphons. Extensive experiments show that \$\textbackslashmathcal\{G\}\$-Mixup substantially improves the generalization and robustness of GNNs.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Han-G-Mixup-%20Graph%20Data%20Augmentation%20for%20Graph%20Classification-2022-ICML.pdf}
}
@article{MGDYangECCV2022, 
year = {2022}, 
title = {{Masked Generative Distillation}}, 
author = {Yang, Zhendong and Li, Zhe and Shao, Mingqi and Shi, Dachuan and Yuan, Zehuan and Yuan, Chun}, 
journal = {ECCV}, 
eprint = {2205.01529}, 
abstract = {{Knowledge distillation has been applied to various tasks successfully. The current distillation algorithm usually improves students' performance by imitating the output of the teacher. This paper shows that teachers can also improve students' representation power by guiding students' feature recovery. From this point of view, we propose Masked Generative Distillation (MGD), which is simple: we mask random pixels of the student's feature and force it to generate the teacher's full feature through a simple block. MGD is a truly general feature-based distillation method, which can be utilized on various tasks, including image classification, object detection, semantic segmentation and instance segmentation. We experiment on different models with extensive datasets and the results show that all the students achieve excellent improvements. Notably, we boost ResNet-18 from 69.90\% to 71.69\% ImageNet top-1 accuracy, RetinaNet with ResNet-50 backbone from 37.4 to 41.0 Boundingbox mAP, SOLO based on ResNet-50 from 33.1 to 36.2 Mask mAP and DeepLabV3 based on ResNet-18 from 73.20 to 76.02 mIoU. Our codes are available at https://github.com/yzd-v/MGD.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-Masked%20Generative%20Distillation-2022-ECCV.pdf}
}
@article{LogitNormWeiICML2022, 
year = {2022}, 
title = {{Mitigating Neural Network Overconfidence with Logit Normalization}}, 
author = {Wei, Hongxin and Xie, Renchunzi and Cheng, Hao and Feng, Lei and An, Bo and Li, Yixuan}, 
journal = {ICML}, 
eprint = {2205.09310}, 
abstract = {{Detecting out-of-distribution inputs is critical for safe deployment of machine learning models in the real world. However, neural networks are known to suffer from the overconfidence issue, where they produce abnormally high confidence for both in- and out-of-distribution inputs. In this work, we show that this issue can be mitigated through Logit Normalization (LogitNorm) -- a simple fix to the cross-entropy loss -- by enforcing a constant vector norm on the logits in training. Our method is motivated by the analysis that the norm of the logit keeps increasing during training, leading to overconfident output. Our key idea behind LogitNorm is thus to decouple the influence of output's norm during network optimization. Trained with LogitNorm, neural networks produce highly distinguishable confidence scores between in- and out-of-distribution data. Extensive experiments demonstrate the superiority of LogitNorm, reducing the average FPR95 by up to 42.30\% on common benchmarks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wei-Mitigating%20Neural%20Network%20Overconfidence%20with%20Logit%20Normalization-2022-ICML.pdf}
}
@article{MobileOneKumarCVPR2023, 
year = {2023}, 
title = {{An Improved One millisecond Mobile Backbone}}, 
author = {Vasu, Pavan Kumar Anasosalu and Gabriel, James and Zhu, Jeff and Tuzel, Oncel and Ranjan, Anurag}, 
journal = {CVPR}, 
eprint = {2206.04040}, 
abstract = {{Efficient neural network backbones for mobile devices are often optimized for metrics such as FLOPs or parameter count. However, these metrics may not correlate well with latency of the network when deployed on a mobile device. Therefore, we perform extensive analysis of different metrics by deploying several mobile-friendly networks on a mobile device. We identify and analyze architectural and optimization bottlenecks in recent efficient neural networks and provide ways to mitigate these bottlenecks. To this end, we design an efficient backbone MobileOne, with variants achieving an inference time under 1 ms on an iPhone12 with 75.9\% top-1 accuracy on ImageNet. We show that MobileOne achieves state-of-the-art performance within the efficient architectures while being many times faster on mobile. Our best model obtains similar performance on ImageNet as MobileFormer while being 38x faster. Our model obtains 2.3\% better top-1 accuracy on ImageNet than EfficientNet at similar latency. Furthermore, we show that our model generalizes to multiple tasks - image classification, object detection, and semantic segmentation with significant improvements in latency and accuracy as compared to existing efficient architectures when deployed on a mobile device.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Vasu-An%20Improved%20One%20millisecond%20Mobile%20Backbone-2022-arXiv.pdf}
}
@article{IntriguingXieICLR2020, 
year = {2020}, 
title = {{Intriguing properties of adversarial training at scale}}, 
author = {Xie, Cihang and Yuille, Alan}, 
journal = {ICLR}, 
eprint = {1906.03787}, 
abstract = {{Adversarial training is one of the main defenses against adversarial attacks. In this paper, we provide the first rigorous study on diagnosing elements of adversarial training, which reveals two intriguing properties. First, we study the role of normalization. Batch normalization (BN) is a crucial element for achieving state-of-the-art performance on many vision tasks, but we show it may prevent networks from obtaining strong robustness in adversarial training. One unexpected observation is that, for models trained with BN, simply removing clean images from training data largely boosts adversarial robustness, i.e., 18.3\%. We relate this phenomenon to the hypothesis that clean images and adversarial images are drawn from two different domains. This two-domain hypothesis may explain the issue of BN when training with a mixture of clean and adversarial images, as estimating normalization statistics of this mixture distribution is challenging. Guided by this two-domain hypothesis, we show disentangling the mixture distribution for normalization, i.e., applying separate BNs to clean and adversarial images for statistics estimation, achieves much stronger robustness. Additionally, we find that enforcing BNs to behave consistently at training and testing can further enhance robustness. Second, we study the role of network capacity. We find our so-called "deep" networks are still shallow for the task of adversarial learning. Unlike traditional classification tasks where accuracy is only marginally improved by adding more layers to "deep" networks (e.g., ResNet-152), adversarial training exhibits a much stronger demand on deeper networks to achieve higher adversarial robustness. This robustness improvement can be observed substantially and consistently even by pushing the network capacity to an unprecedented scale, i.e., ResNet-638.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xie-Intriguing%20properties%20of%20adversarial%20training%20at%20scale-2020-ICLR.pdf}
}
@article{AdvPropXieCVPR2020, 
year = {2020}, 
title = {{Adversarial Examples Improve Image Recognition}}, 
author = {Xie, Cihang and Tan, Mingxing and Gong, Boqing and Wang, Jiang and Yuille, Alan and Le, Quoc V.}, 
journal = {CVPR}, 
abstract = {{Adversarial examples are commonly viewed as a threat to ConvNets. Here we present an opposite perspective: adversarial examples can be used to improve image recognition models if harnessed in the right manner. We propose AdvProp, an enhanced adversarial training scheme which treats adversarial examples as additional examples, to prevent overfitting. Key to our method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples. We show that AdvProp improves a wide range of models on various image recognition tasks and performs better when the models are bigger. For instance, by applying AdvProp to the latest EfficientNet-B7 [41] on ImageNet, we achieve significant improvements on ImageNet (+0.7\%), ImageNet-C (+6.5\%), ImageNet-A (+ 7.0\%) and Stylized-ImageNet (+4.8\%). With an enhanced EfficientNet-B8, our method achieves the state-of-the-art 85.5\% ImageNet top-1 accuracy without extra data. This result even surpasses the best model in [24] which is trained with 3.5B Instagram images (∼3000× more than ImageNet) and ∼9.4× more parameters. Models are available at https://github.com/tensorflow/tpu/tree/master/models/official/EfficientNet.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xie-Adversarial%20Examples%20Improve%20Image%20Recognition-2020-CVPR.pdf}
}
@article{LongformerBeltagyarXiv2020, 
year = {2020}, 
title = {{Longformer: The Long-Document Transformer}}, 
author = {Beltagy, Iz and Peters, Matthew E and Cohan, Arman}, 
journal = {arXiv}, 
eprint = {2004.05150}, 
abstract = {{Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Beltagy-Longformer-%20The%20Long-Document%20Transformer-2020-arXiv.pdf}
}
@article{LinformerWangarXiv2020, 
year = {2020}, 
title = {{Linformer: Self-Attention with Linear Complexity}}, 
author = {Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao}, 
journal = {arXiv}, 
eprint = {2006.04768}, 
abstract = {{Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses \$O(n\textasciicircum2)\$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from \$O(n\textasciicircum2)\$ to \$O(n)\$ in both time and space. The resulting linear transformer, the \textbackslashtextit\{Linformer\}, performs on par with standard Transformer models, while being much more memory- and time-efficient.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Linformer-%20Self-Attention%20with%20Linear%20Complexity-2020-arXiv.pdf}
}
@article{SSKDXuECCV2020, 
year = {2020}, 
title = {{Knowledge Distillation Meets Self-Supervision}}, 
author = {Xu, Guodong and Liu, Ziwei and Li, Xiaoxiao and Loy, Chen Change}, 
journal = {ECCV}, 
eprint = {2006.07114}, 
abstract = {{Knowledge distillation, which involves extracting the "dark knowledge" from a teacher network to guide the learning of a student network, has emerged as an important technique for model compression and transfer learning. Unlike previous works that exploit architecture-specific cues such as activation and attention for distillation, here we wish to explore a more general and model-agnostic approach for extracting "richer dark knowledge" from the pre-trained teacher model. We show that the seemingly different self-supervision task can serve as a simple yet powerful solution. For example, when performing contrastive learning between transformed entities, the noisy predictions of the teacher network reflect its intrinsic composition of semantic and pose information. By exploiting the similarity between those self-supervision signals as an auxiliary task, one can effectively transfer the hidden information from the teacher to the student. In this paper, we discuss practical ways to exploit those noisy self-supervision signals with selective transfer for distillation. We further show that self-supervision signals improve conventional distillation with substantial gains under few-shot and noisy-label scenarios. Given the richer knowledge mined from self-supervision, our knowledge distillation approach achieves state-of-the-art performance on standard benchmarks, i.e., CIFAR100 and ImageNet, under both similar-architecture and cross-architecture settings. The advantage is even more pronounced under the cross-architecture setting, where our method outperforms the state of the art CRD by an average of 2.3\% in accuracy rate on CIFAR100 across six different teacher-student pairs.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xu-Knowledge%20Distillation%20Meets%20Self-Supervision-2020-ECCV.pdf}
}
@article{BigBirdZaheerNeurIPS2020, 
year = {2020}, 
title = {{Big Bird: Transformers for Longer Sequences}}, 
author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr}, 
journal = {NeurIPS}, 
eprint = {2007.14062}, 
abstract = {{Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having \$O(1)\$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zaheer-Big%20Bird-%20Transformers%20for%20Longer%20Sequences-2020-NeurIPS.pdf}
}
@article{ASLRidnikICCV2021, 
year = {2021}, 
title = {{Asymmetric Loss For Multi-Label Classification}}, 
author = {Ridnik, Tal and Ben-Baruch, Emanuel and Zamir, Nadav and Noy, Asaf and Friedman, Itamar and Protter, Matan and Zelnik-Manor, Lihi}, 
journal = {ICCV}, 
abstract = {{In a typical multi-label setting, a picture contains on average few positive labels, and many negative ones. This positive-negative imbalance dominates the optimization process, and can lead to under-emphasizing gradients from positive labels during training, resulting in poor accuracy. In this paper, we introduce a novel asymmetric loss ("ASL"), which operates differently on positive and negative samples. The loss enables to dynamically down-weights and hard-thresholds easy negative samples, while also discarding possibly mislabeled samples. We demonstrate how ASL can balance the probabilities of different samples, and how this balancing is translated to better mAP scores. With ASL, we reach state-of-the-art results on multiple popular multi-label datasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate ASL applicability for other tasks, such as single-label classification and object detection. ASL is effective, easy to implement, and does not increase the training time or complexity. Implementation is available at: https://github.com/Alibaba-MIIL/ASL.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ridnik-Asymmetric%20Loss%20For%20Multi-Label%20Classification-2021-ICCV.pdf}
}
@article{BAPPSZhangCVPR2018, 
year = {2018}, 
title = {{The Unreasonable Effectiveness of Deep Features as a Perceptual Metric}}, 
author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver}, 
journal = {CVPR}, 
abstract = {{While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, andfail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called “percep-tual losses”? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (su-pervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-The%20Unreasonable%20Effectiveness%20of%20Deep%20Features%20as%20a%20Perceptual%20Metric-2018-CVPR.pdf}
}
@article{MixSKDYangECCV2022, 
year = {2022}, 
title = {{MixSKD: Self-Knowledge Distillation from Mixup for Image Recognition}}, 
author = {Yang, Chuanguang and An, Zhulin and Zhou, Helong and Cai, Linhang and Zhi, Xiang and Wu, Jiwen and Xu, Yongjun and Zhang, Qian}, 
journal = {ECCV}, 
eprint = {2208.05768}, 
abstract = {{Unlike the conventional Knowledge Distillation (KD), Self-KD allows a network to learn knowledge from itself without any guidance from extra networks. This paper proposes to perform Self-KD from image Mixture (MixSKD), which integrates these two techniques into a unified framework. MixSKD mutually distills feature maps and probability distributions between the random pair of original images and their mixup images in a meaningful way. Therefore, it guides the network to learn cross-image knowledge by modelling supervisory signals from mixup images. Moreover, we construct a self-teacher network by aggregating multi-stage feature maps for providing soft labels to supervise the backbone classifier, further improving the efficacy of self-boosting. Experiments on image classification and transfer learning to object detection and semantic segmentation demonstrate that MixSKD outperforms other state-of-the-art Self-KD and data augmentation methods. The code is available at https://github.com/winycg/Self-KD-Lib.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-MixSKD-%20Self-Knowledge%20Distillation%20from%20Mixup%20for%20Image%20Recognition-2022-ECCV.pdf}
}
@article{FS-BBTNguyenECCV2022, 
year = {2022}, 
title = {{Black-box Few-shot Knowledge Distillation}}, 
author = {Nguyen, Dang and Gupta, Sunil and Do, Kien and Venkatesh, Svetha}, 
journal = {ECCV}, 
eprint = {2207.12106}, 
abstract = {{Knowledge distillation (KD) is an efficient approach to transfer the knowledge from a large "teacher" network to a smaller "student" network. Traditional KD methods require lots of labeled training samples and a white-box teacher (parameters are accessible) to train a good student. However, these resources are not always available in real-world applications. The distillation process often happens at an external party side where we do not have access to much data, and the teacher does not disclose its parameters due to security and privacy concerns. To overcome these challenges, we propose a black-box few-shot KD method to train the student with few unlabeled training samples and a black-box teacher. Our main idea is to expand the training set by generating a diverse set of out-of-distribution synthetic images using MixUp and a conditional variational auto-encoder. These synthetic images along with their labels obtained from the teacher are used to train the student. We conduct extensive experiments to show that our method significantly outperforms recent SOTA few/zero-shot KD methods on image classification tasks. The code and models are available at: https://github.com/nphdang/FS-BBT}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Nguyen-Black-box%20Few-shot%20Knowledge%20Distillation-2022-ECCV.pdf}
}
@article{APE-LossXuCVPR2022, 
year = {2022}, 
title = {{Revisiting AP Loss for Dense Object Detection: Adaptive Ranking Pair Selection}}, 
author = {Xu, Dongli and Deng, Jinhong and Li, Wen}, 
journal = {CVPR}, 
eprint = {2207.12042}, 
abstract = {{Average precision (AP) loss has recently shown promising performance on the dense object detection task. However,a deep understanding of how AP loss affects the detector from a pairwise ranking perspective has not yet been developed.In this work, we revisit the average precision (AP)loss and reveal that the crucial element is that of selecting the ranking pairs between positive and negative samples.Based on this observation, we propose two strategies to improve the AP loss. The first of these is a novel Adaptive Pairwise Error (APE) loss that focusing on ranking pairs in both positive and negative samples. Moreover,we select more accurate ranking pairs by exploiting the normalized ranking scores and localization scores with a clustering algorithm. Experiments conducted on the MSCOCO dataset support our analysis and demonstrate the superiority of our proposed method compared with current classification and ranking loss. The code is available at https://github.com/Xudangliatiger/APE-Loss.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xu-Revisiting%20AP%20Loss%20for%20Dense%20Object%20Detection-%20Adaptive%20Ranking%20Pair%20Selection-2022-CVPR.pdf}
}
@article{APLossChenCVPR2019, 
year = {2019}, 
title = {{Towards Accurate One-Stage Object Detection with AP-Loss}}, 
author = {Chen, Kean and Li, Jianguo and Lin, Weiyao and See, John and Wang, Ji and Duan, Lingyu and Chen, Zhibo and He, Changwei and Zou, Junni}, 
journal = {CVPR}, 
abstract = {{One-stage object detectors are trained by optimizing classification-loss and localization-loss simultaneously, with the former suffering much from extreme foreground-background class imbalance issue due to the large number of anchors. This paper alleviates this issue by proposing a novel framework to replace the classification task in one-stage detectors with a ranking task, and adopting the Average-Precision loss (AP-loss) for the ranking problem. Due to its non-differentiability and non-convexity, the AP-loss cannot be optimized directly. For this purpose, we develop a novel optimization algorithm, which seamlessly combines the error-driven update scheme in perceptron learning and backpropagation algorithm in deep networks. We verify good convergence property of the proposed algorithm theoretically and empirically. Experimental results demonstrate notable performance improvement in state-of-the-art one-stage detectors based on AP-loss over different kinds of classification-losses on various benchmarks, without changing the network architectures.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Towards%20Accurate%20One-Stage%20Object%20Detection%20with%20AP-Loss-2019-CVPR.pdf}
}
@article{RankNetBurgesICML2005, 
year = {2005}, 
title = {{Learning to rank using gradient descent}}, 
author = {Burges, Chris and Shaked, Tal and Renshaw, Erin and Lazier, Ari and Deeds, Matt and Hamilton, Nicole and Hullender, Greg}, 
journal = {ICML}, 
abstract = {{We investigate using gradient descent methods for learning ranking functions; we propose a simple probabilistic cost function, and we introduce RankNet, an implementation of these ideas using a neural network to model the underlying ranking function. We present test results on toy data and on data from a commercial internet search engine.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Burges-Learning%20to%20rank%20using%20gradient%20descent-2005-ICML.pdf}
}
@article{LambdaRankBurgesNeurIPS2006, 
year = {2006}, 
title = {{Learning to Rank with Nonsmooth Cost Functions}}, 
author = {Burges, Christopher J.C. and Ragno, Robert and Le, Quoc Viet}, 
journal = {NeurIPS}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Burges-Learning%20to%20Rank%20with%20Nonsmooth%20Cost%20Functions-2006-NeurIPS.pdf}
}
@article{jSTABLDorentMIA2020, 
year = {2020}, 
title = {{Learning joint segmentation of tissues and brain lesions from task-specific hetero-modal domain-shifted datasets}}, 
author = {Dorent, Reuben and Booth, Thomas and Li, Wenqi and Sudre, Carole H. and Kafiabadi, Sina and Cardoso, Jorge and Ourselin, Sebastien and Vercauteren, Tom}, 
journal = {MIA}, 
eprint = {2009.04009}, 
abstract = {{Brain tissue segmentation from multimodal MRI is a key building block of many neuroimaging analysis pipelines. Established tissue segmentation approaches have, however, not been developed to cope with large anatomical changes resulting from pathology, such as white matter lesions or tumours, and often fail in these cases. In the meantime, with the advent of deep neural networks (DNNs), segmentation of brain lesions has matured significantly. However, few existing approaches allow for the joint segmentation of normal tissue and brain lesions. Developing a DNN for such a joint task is currently hampered by the fact that annotated datasets typically address only one specific task and rely on task-specific imaging protocols including a task-specific set of imaging modalities. In this work, we propose a novel approach to build a joint tissue and lesion segmentation model from aggregated task-specific hetero-modal domain-shifted and partially-annotated datasets. Starting from a variational formulation of the joint problem, we show how the expected risk can be decomposed and optimised empirically. We exploit an upper bound of the risk to deal with heterogeneous imaging modalities across datasets. To deal with potential domain shift, we integrated and tested three conventional techniques based on data augmentation, adversarial learning and pseudo-healthy generation. For each individual task, our joint approach reaches comparable performance to task-specific and fully-supervised models. The proposed framework is assessed on two different types of brain lesions: White matter lesions and gliomas. In the latter case, lacking a joint ground-truth for quantitative assessment purposes, we propose and use a novel clinically-relevant qualitative assessment methodology.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Dorent-Learning%20joint%20segmentation%20of%20tissues%20and%20brain%20lesions%20from%20task-specific%20hetero-modal%20domain-shifted%20datasets-2020-MIA.pdf}
}
@article{RandAugmentCubukNeurIPS2020, 
year = {2020}, 
title = {{RandAugment: Practical automated data augmentation with a reduced search space}}, 
author = {Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V}, 
journal = {NeurIPS}, 
eprint = {1909.13719}, 
abstract = {{Recent work has shown that data augmentation has the potential to significantly improve the generalization of deep learning models. Recently, automated augmentation strategies have led to state-of-the-art results in image classification and object detection. While these strategies were optimized for improving validation accuracy, they also led to state-of-the-art results in semi-supervised learning and improved robustness to common corruptions of images. An obstacle to a large-scale adoption of these methods is a separate search phase which increases the training complexity and may substantially increase the computational cost. Additionally, due to the separate search phase, these approaches are unable to adjust the regularization strength based on model or dataset size. Automated augmentation policies are often found by training small models on small datasets and subsequently applied to train larger models. In this work, we remove both of these obstacles. RandAugment has a significantly reduced search space which allows it to be trained on the target task with no need for a separate proxy task. Furthermore, due to the parameterization, the regularization strength may be tailored to different model and dataset sizes. RandAugment can be used uniformly across different tasks and datasets and works out of the box, matching or surpassing all previous automated augmentation approaches on CIFAR-10/100, SVHN, and ImageNet. On the ImageNet dataset we achieve 85.0\% accuracy, a 0.6\% increase over the previous state-of-the-art and 1.0\% increase over baseline augmentation. On object detection, RandAugment leads to 1.0-1.3\% improvement over baseline augmentation, and is within 0.3\% mAP of AutoAugment on COCO. Finally, due to its interpretable hyperparameter, RandAugment may be used to investigate the role of data augmentation with varying model and dataset size. Code is available online.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Cubuk-RandAugment-%20Practical%20automated%20data%20augmentation%20with%20a%20reduced%20search%20space-2020-NeurIPS.pdf}
}
@article{PerformersChoromanskiICLR2021, 
year = {2021}, 
title = {{Rethinking Attention with Performers}}, 
author = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy and Weller, Adrian}, 
journal = {ICLR}, 
eprint = {2009.14794}, 
abstract = {{We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Choromanski-Rethinking%20Attention%20with%20Performers-2021-ICLR.pdf}
}
@article{NFNetBrockICML2021, 
year = {2021}, 
title = {{High-Performance Large-Scale Image Recognition Without Normalization}}, 
author = {Brock, Andrew and De, Soham and Smith, Samuel L and Simonyan, Karen}, 
journal = {ICML}, 
eprint = {2102.06171}, 
abstract = {{Batch normalization is a key component of most image classification models, but it has many undesirable properties stemming from its dependence on the batch size and interactions between examples. Although recent work has succeeded in training deep ResNets without normalization layers, these models do not match the test accuracies of the best batch-normalized networks, and are often unstable for large learning rates or strong data augmentations. In this work, we develop an adaptive gradient clipping technique which overcomes these instabilities, and design a significantly improved class of Normalizer-Free ResNets. Our smaller models match the test accuracy of an EfficientNet-B7 on ImageNet while being up to 8.7x faster to train, and our largest models attain a new state-of-the-art top-1 accuracy of 86.5\%. In addition, Normalizer-Free models attain significantly better performance than their batch-normalized counterparts when finetuning on ImageNet after large-scale pre-training on a dataset of 300 million labeled images, with our best models obtaining an accuracy of 89.2\%. Our code is available at https://github.com/deepmind/ deepmind-research/tree/master/nfnets}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Brock-High-Performance%20Large-Scale%20Image%20Recognition%20Without%20Normalization-2021-ICML.pdf}
}
@article{SwinTransformerV2LiuCVPR2022, 
year = {2022}, 
title = {{Swin Transformer V2: Scaling Up Capacity and Resolution}}, 
author = {Liu, Ze and Hu, Han and Lin, Yutong and Yao, Zhuliang and Xie, Zhenda and Wei, Yixuan and Ning, Jia and Cao, Yue and Zhang, Zheng and Dong, Li and Wei, Furu and Guo, Baining}, 
journal = {CVPR}, 
eprint = {2111.09883}, 
abstract = {{Large-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536\$\textbackslashtimes\$1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training time. Code is available at \textbackslashurl\{https://github.com/microsoft/Swin-Transformer\}.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-Swin%20Transformer%20V2-%20Scaling%20Up%20Capacity%20and%20Resolution-2022-CVPR.pdf}
}
@article{LiDARDistillationWeiECCV2022, 
year = {2022}, 
title = {{LiDAR Distillation: Bridging the Beam-Induced Domain Gap for 3D Object Detection}}, 
author = {Wei, Yi and Wei, Zibu and Rao, Yongming and Li, Jiaxin and Zhou, Jie and Lu, Jiwen}, 
journal = {ECCV}, 
eprint = {2203.14956}, 
abstract = {{In this paper, we propose the LiDAR Distillation to bridge the domain gap induced by different LiDAR beams for 3D object detection. In many real-world applications, the LiDAR points used by mass-produced robots and vehicles usually have fewer beams than that in large-scale public datasets. Moreover, as the LiDARs are upgraded to other product models with different beam amount, it becomes challenging to utilize the labeled data captured by previous versions' high-resolution sensors. Despite the recent progress on domain adaptive 3D detection, most methods struggle to eliminate the beam-induced domain gap. We find that it is essential to align the point cloud density of the source domain with that of the target domain during the training process. Inspired by this discovery, we propose a progressive framework to mitigate the beam-induced domain shift. In each iteration, we first generate low-beam pseudo LiDAR by downsampling the high-beam point clouds. Then the teacher-student framework is employed to distill rich information from the data with more beams. Extensive experiments on Waymo, nuScenes and KITTI datasets with three different LiDAR-based detectors demonstrate the effectiveness of our LiDAR Distillation. Notably, our approach does not increase any additional computation cost for inference.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wei-LiDAR%20Distillation-%20Bridging%20the%20Beam-Induced%20Domain%20Gap%20for%203D%20Object%20Detection-2022-ECCV.pdf}
}
@article{CDSKDZhangECCV2022, 
year = {2022}, 
title = {{Contrastive Deep Supervision}}, 
author = {Zhang, Linfeng and Chen, Xin and Zhang, Junbo and Dong, Runpei and Ma, Kaisheng}, 
journal = {ECCV}, 
eprint = {2207.05306}, 
abstract = {{The success of deep learning is usually accompanied by the growth in neural network depth. However, the traditional training method only supervises the neural network at its last layer and propagates the supervision layer-by-layer, which leads to hardship in optimizing the intermediate layers. Recently, deep supervision has been proposed to add auxiliary classifiers to the intermediate layers of deep neural networks. By optimizing these auxiliary classifiers with the supervised task loss, the supervision can be applied to the shallow layers directly. However, deep supervision conflicts with the well-known observation that the shallow layers learn low-level features instead of task-biased high-level semantic features. To address this issue, this paper proposes a novel training framework named Contrastive Deep Supervision, which supervises the intermediate layers with augmentation-based contrastive learning. Experimental results on nine popular datasets with eleven models demonstrate its effects on general image classification, fine-grained image classification and object detection in supervised learning, semi-supervised learning and knowledge distillation. Codes have been released in Github.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Contrastive%20Deep%20Supervision-2022-ECCV.pdf}
}
@article{KCDLiECCV2022, 
year = {2022}, 
title = {{Knowledge Condensation Distillation}}, 
author = {Li, Chenxin and Lin, Mingbao and Ding, Zhiyuan and Lin, Nie and Zhuang, Yihong and Huang, Yue and Ding, Xinghao and Cao, Liujuan}, 
journal = {ECCV}, 
eprint = {2207.05409}, 
abstract = {{Knowledge Distillation (KD) transfers the knowledge from a high-capacity teacher network to strengthen a smaller student. Existing methods focus on excavating the knowledge hints and transferring the whole knowledge to the student. However, the knowledge redundancy arises since the knowledge shows different values to the student at different learning stages. In this paper, we propose Knowledge Condensation Distillation (KCD). Specifically, the knowledge value on each sample is dynamically estimated, based on which an Expectation-Maximization (EM) framework is forged to iteratively condense a compact knowledge set from the teacher to guide the student learning. Our approach is easy to build on top of the off-the-shelf KD methods, with no extra training parameters and negligible computation overhead. Thus, it presents one new perspective for KD, in which the student that actively identifies teacher's knowledge in line with its aptitude can learn to learn more effectively and efficiently. Experiments on standard benchmarks manifest that the proposed KCD can well boost the performance of student model with even higher distillation efficiency. Code is available at https://github.com/dzy3/KCD.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Knowledge%20Condensation%20Distillation-2022-ECCV.pdf}
}
@article{BayesCapUpadhyayECCV2022, 
year = {2022}, 
title = {{BayesCap: Bayesian Identity Cap for Calibrated Uncertainty in Frozen Neural Networks}}, 
author = {Upadhyay, Uddeshya and Karthik, Shyamgopal and Chen, Yanbei and Mancini, Massimiliano and Akata, Zeynep}, 
journal = {ECCV}, 
eprint = {2207.06873}, 
abstract = {{High-quality calibrated uncertainty estimates are crucial for numerous real-world applications, especially for deep learning-based deployed ML systems. While Bayesian deep learning techniques allow uncertainty estimation, training them with large-scale datasets is an expensive process that does not always yield models competitive with non-Bayesian counterparts. Moreover, many of the high-performing deep learning models that are already trained and deployed are non-Bayesian in nature and do not provide uncertainty estimates. To address these issues, we propose BayesCap that learns a Bayesian identity mapping for the frozen model, allowing uncertainty estimation. BayesCap is a memory-efficient method that can be trained on a small fraction of the original dataset, enhancing pretrained non-Bayesian computer vision models by providing calibrated uncertainty estimates for the predictions without (i) hampering the performance of the model and (ii) the need for expensive retraining the model from scratch. The proposed method is agnostic to various architectures and tasks. We show the efficacy of our method on a wide variety of tasks with a diverse set of architectures, including image super-resolution, deblurring, inpainting, and crucial application such as medical image translation. Moreover, we apply the derived uncertainty estimates to detect out-of-distribution samples in critical scenarios like depth estimation in autonomous driving. Code is available at https://github.com/ExplainableML/BayesCap.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Upadhyay-BayesCap-%20Bayesian%20Identity%20Cap%20for%20Calibrated%20Uncertainty%20in%20Frozen%20Neural%20Networks-2022-ECCV.pdf}
}
@article{AR-AdaLSQinNeurIPS2021, 
year = {2021}, 
title = {{Improving Calibration through the Relationship with Adversarial Robustness}}, 
author = {Qin, Yao and Wang, Xuezhi and Beutel, Alex and Chi, Ed H}, 
journal = {NeurIPS}, 
eprint = {2006.16375}, 
abstract = {{Neural networks lack adversarial robustness, i.e., they are vulnerable to adversarial examples that through small perturbations to inputs cause incorrect predictions. Further, trust is undermined when models give miscalibrated predictions, i.e., the predicted probability is not a good indicator of how much we should trust our model. In this paper, we study the connection between adversarial robustness and calibration and find that the inputs for which the model is sensitive to small perturbations (are easily attacked) are more likely to have poorly calibrated predictions. Based on this insight, we examine if calibration can be improved by addressing those adversarially unrobust inputs. To this end, we propose Adversarial Robustness based Adaptive Label Smoothing (AR-AdaLS) that integrates the correlations of adversarial robustness and calibration into training by adaptively softening labels for an example based on how easily it can be attacked by an adversary. We find that our method, taking the adversarial robustness of the in-distribution data into consideration, leads to better calibration over the model even under distributional shifts. In addition, AR-AdaLS can also be applied to an ensemble model to further improve model calibration.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Qin-Improving%20Calibration%20through%20the%20Relationship%20with%20Adversarial%20Robustness-2021-NeurIPS.pdf}
}
@article{DropoutGalICML2016, 
year = {2016}, 
title = {{Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}}, 
author = {Gal, Yarin and Ghahramani, Zoubin}, 
journal = {ICML}, 
eprint = {1506.02142}, 
abstract = {{Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Gal-Dropout%20as%20a%20Bayesian%20Approximation-%20Representing%20Model%20Uncertainty%20in%20Deep%20Learning-2016-ICML.pdf}
}
@article{AccurateKuleshovICML2018, 
year = {2018}, 
title = {{Accurate Uncertainties for Deep Learning Using Calibrated Regression}}, 
author = {Kuleshov, Volodymyr and Fenner, Nathan and Ermon, Stefano}, 
journal = {ICML}, 
eprint = {1807.00263}, 
abstract = {{Methods for reasoning under uncertainty are a key building block of accurate and reliable machine learning systems. Bayesian methods provide a general framework to quantify uncertainty. However, because of model misspecification and the use of approximate inference, Bayesian uncertainty estimates are often inaccurate -- for example, a 90\% credible interval may not contain the true outcome 90\% of the time. Here, we propose a simple procedure for calibrating any regression algorithm; when applied to Bayesian and probabilistic models, it is guaranteed to produce calibrated uncertainty estimates given enough data. Our procedure is inspired by Platt scaling and extends previous work on classification. We evaluate this approach on Bayesian linear regression, feedforward, and recurrent neural networks, and find that it consistently outputs well-calibrated credible intervals while improving performance on time series forecasting and model-based reinforcement learning tasks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kuleshov-Accurate%20Uncertainties%20for%20Deep%20Learning%20Using%20Calibrated%20Regression-2018-ICML.pdf}
}
@article{DistributionCalibrationSongICML2019, 
year = {2019}, 
title = {{Distribution Calibration for Regression}}, 
author = {Song, Hao and Diethe, Tom and Kull, Meelis and Flach, Peter}, 
journal = {ICML}, 
eprint = {1905.06023}, 
abstract = {{We are concerned with obtaining well-calibrated output distributions from regression models. Such distributions allow us to quantify the uncertainty that the model has regarding the predicted target value. We introduce the novel concept of distribution calibration, and demonstrate its advantages over the existing definition of quantile calibration. We further propose a post-hoc approach to improving the predictions from previously trained regression models, using multi-output Gaussian Processes with a novel Beta link function. The proposed method is experimentally verified on a set of common regression models and shows improvements for both distribution-level and quantile-level calibration.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Song-Distribution%20Calibration%20for%20Regression-2019-ICML.pdf}
}
@article{TowardsJelassiICML2022, 
year = {2022}, 
title = {{Towards understanding how momentum improves generalization in deep learning}}, 
author = {Jelassi, Samy and Li, Yuanzhi}, 
journal = {ICML}, 
eprint = {2207.05931}, 
abstract = {{Stochastic gradient descent (SGD) with momentum is widely used for training modern deep learning architectures. While it is well-understood that using momentum can lead to faster convergence rate in various settings, it has also been observed that momentum yields higher generalization. Prior work argue that momentum stabilizes the SGD noise during training and this leads to higher generalization. In this paper, we adopt another perspective and first empirically show that gradient descent with momentum (GD+M) significantly improves generalization compared to gradient descent (GD) in some deep learning problems. From this observation, we formally study how momentum improves generalization. We devise a binary classification setting where a one-hidden layer (over-parameterized) convolutional neural network trained with GD+M provably generalizes better than the same network trained with GD, when both algorithms are similarly initialized. The key insight in our analysis is that momentum is beneficial in datasets where the examples share some feature but differ in their margin. Contrary to GD that memorizes the small margin data, GD+M still learns the feature in these data thanks to its historical gradients. Lastly, we empirically validate our theoretical findings.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Jelassi-Towards%20understanding%20how%20momentum%20improves%20generalization%20in%20deep%20learning-2022-ICML.pdf}
}
@article{CalEnsembleKumarUAI2022, 
year = {2022}, 
title = {{Calibrated ensembles can mitigate accuracy tradeoffs under distribution shift}}, 
author = {Kumar, Ananya and Ma, Tengyu and Liang, Percy and Raghunathan, Aditi}, 
journal = {UAI}, 
eprint = {2207.08977}, 
abstract = {{We often see undesirable tradeoffs in robust machine learning where out-of-distribution (OOD) accuracy is at odds with in-distribution (ID) accuracy: a robust classifier obtained via specialized techniques such as removing spurious features often has better OOD but worse ID accuracy compared to a standard classifier trained via ERM. In this paper, we find that ID-calibrated ensembles -- where we simply ensemble the standard and robust models after calibrating on only ID data -- outperforms prior state-of-the-art (based on self-training) on both ID and OOD accuracy. On eleven natural distribution shift datasets, ID-calibrated ensembles obtain the best of both worlds: strong ID accuracy and OOD accuracy. We analyze this method in stylized settings, and identify two important conditions for ensembles to perform well both ID and OOD: (1) we need to calibrate the standard and robust models (on ID data, because OOD data is unavailable), (2) OOD has no anticorrelated spurious features.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kumar-Calibrated%20ensembles%20can%20mitigate%20accuracy%20tradeoffs%20under%20distribution%20shift-2022-UAI.pdf}
}
@article{iNatLoc500ColeECCV2022, 
year = {2022}, 
title = {{On Label Granularity and Object Localization}}, 
author = {Cole, Elijah and Wilber, Kimberly and Horn, Grant Van and Yang, Xuan and Fornoni, Marco and Perona, Pietro and Belongie, Serge and Howard, Andrew and Aodha, Oisin Mac}, 
journal = {ECCV}, 
eprint = {2207.10225}, 
abstract = {{Weakly supervised object localization (WSOL) aims to learn representations that encode object location using only image-level category labels. However, many objects can be labeled at different levels of granularity. Is it an animal, a bird, or a great horned owl? Which image-level labels should we use? In this paper we study the role of label granularity in WSOL. To facilitate this investigation we introduce iNatLoc500, a new large-scale fine-grained benchmark dataset for WSOL. Surprisingly, we find that choosing the right training label granularity provides a much larger performance boost than choosing the best WSOL algorithm. We also show that changing the label granularity can significantly improve data efficiency.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Cole-On%20Label%20Granularity%20and%20Object%20Localization-2022-ECCV.pdf}
}
@article{UFOXiECCV2022, 
year = {2022}, 
title = {{UFO: Unified Feature Optimization}}, 
author = {Xi, Teng and Sun, Yifan and Yu, Deli and Li, Bi and Peng, Nan and Zhang, Gang and Zhang, Xinyu and Wang, Zhigang and Chen, Jinwen and Wang, Jian and Liu, Lufei and Feng, Haocheng and Han, Junyu and Liu, Jingtuo and Ding, Errui and Wang, Jingdong}, 
journal = {ECCV}, 
eprint = {2207.10341}, 
abstract = {{This paper proposes a novel Unified Feature Optimization (UFO) paradigm for training and deploying deep models under real-world and large-scale scenarios, which requires a collection of multiple AI functions. UFO aims to benefit each single task with a large-scale pretraining on all tasks. Compared with the well known foundation model, UFO has two different points of emphasis, i.e., relatively smaller model size and NO adaptation cost: 1) UFO squeezes a wide range of tasks into a moderate-sized unified model in a multi-task learning manner and further trims the model size when transferred to down-stream tasks. 2) UFO does not emphasize transfer to novel tasks. Instead, it aims to make the trimmed model dedicated for one or more already-seen task. With these two characteristics, UFO provides great convenience for flexible deployment, while maintaining the benefits of large-scale pretraining. A key merit of UFO is that the trimming process not only reduces the model size and inference consumption, but also even improves the accuracy on certain tasks. Specifically, UFO considers the multi-task training and brings two-fold impact on the unified model: some closely related tasks have mutual benefits, while some tasks have conflicts against each other. UFO manages to reduce the conflicts and to preserve the mutual benefits through a novel Network Architecture Search (NAS) method. Experiments on a wide range of deep representation learning tasks (i.e., face recognition, person re-identification, vehicle re-identification and product retrieval) show that the model trimmed from UFO achieves higher accuracy than its single-task-trained counterpart and yet has smaller model size, validating the concept of UFO. Besides, UFO also supported the release of 17 billion parameters computer vision (CV) foundation model which is the largest CV model in the industry.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xi-UFO-%20Unified%20Feature%20Optimization-2022-ECCV.pdf}
}
@article{TinyViTWuECCV2022, 
year = {2022}, 
title = {{TinyViT: Fast Pretraining Distillation for Small Vision Transformers}}, 
author = {Wu, Kan and Zhang, Jinnian and Peng, Houwen and Liu, Mengchen and Xiao, Bin and Fu, Jianlong and Yuan, Lu}, 
journal = {ECCV}, 
eprint = {2207.10666}, 
abstract = {{Vision transformer (ViT) recently has drawn great attention in computer vision due to its remarkable model capability. However, most prevailing ViT models suffer from huge number of parameters, restricting their applicability on devices with limited resources. To alleviate this issue, we propose TinyViT, a new family of tiny and efficient small vision transformers pretrained on large-scale datasets with our proposed fast distillation framework. The central idea is to transfer knowledge from large pretrained models to small ones, while enabling small models to get the dividends of massive pretraining data. More specifically, we apply distillation during pretraining for knowledge transfer. The logits of large teacher models are sparsified and stored in disk in advance to save the memory cost and computation overheads. The tiny student transformers are automatically scaled down from a large pretrained model with computation and parameter constraints. Comprehensive experiments demonstrate the efficacy of TinyViT. It achieves a top-1 accuracy of 84.8\% on ImageNet-1k with only 21M parameters, being comparable to Swin-B pretrained on ImageNet-21k while using 4.2 times fewer parameters. Moreover, increasing image resolutions, TinyViT can reach 86.5\% accuracy, being slightly better than Swin-L while using only 11\% parameters. Last but not the least, we demonstrate a good transfer ability of TinyViT on various downstream tasks. Code and models are available at https://github.com/microsoft/Cream/tree/main/TinyViT.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wu-TinyViT-%20Fast%20Pretraining%20Distillation%20for%20Small%20Vision%20Transformers-2022-ECCV.pdf}
}
@article{OnDAPanagiotakopoulosECCV2022, 
year = {2022}, 
title = {{Online Domain Adaptation for Semantic Segmentation in Ever-Changing Conditions}}, 
author = {Panagiotakopoulos, Theodoros and Dovesi, Pier Luigi and Härenstam-Nielsen, Linus and Poggi, Matteo}, 
journal = {ECCV}, 
eprint = {2207.10667}, 
abstract = {{Unsupervised Domain Adaptation (UDA) aims at reducing the domain gap between training and testing data and is, in most cases, carried out in offline manner. However, domain changes may occur continuously and unpredictably during deployment (e.g. sudden weather changes). In such conditions, deep neural networks witness dramatic drops in accuracy and offline adaptation may not be enough to contrast it. In this paper, we tackle Online Domain Adaptation (OnDA) for semantic segmentation. We design a pipeline that is robust to continuous domain shifts, either gradual or sudden, and we evaluate it in the case of rainy and foggy scenarios. Our experiments show that our framework can effectively adapt to new domains during deployment, while not being affected by catastrophic forgetting of the previous domains.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Panagiotakopoulos-Online%20Domain%20Adaptation%20for%20Semantic%20Segmentation%20in%20Ever-Changing%20Conditions-2022-ECCV.pdf}
}
@article{GOLAlexandridisECCV2022, 
year = {2022}, 
title = {{Long-tailed Instance Segmentation using Gumbel Optimized Loss}}, 
author = {Alexandridis, Konstantinos Panagiotis and Deng, Jiankang and Nguyen, Anh and Luo, Shan}, 
journal = {ECCV}, 
eprint = {2207.10936}, 
abstract = {{Major advancements have been made in the field of object detection and segmentation recently. However, when it comes to rare categories, the state-of-the-art methods fail to detect them, resulting in a significant performance gap between rare and frequent categories. In this paper, we identify that Sigmoid or Softmax functions used in deep detectors are a major reason for low performance and are sub-optimal for long-tailed detection and segmentation. To address this, we develop a Gumbel Optimized Loss (GOL), for long-tailed detection and segmentation. It aligns with the Gumbel distribution of rare classes in imbalanced datasets, considering the fact that most classes in long-tailed detection have low expected probability. The proposed GOL significantly outperforms the best state-of-the-art method by 1.1\% on AP , and boosts the overall segmentation by 9.0\% and detection by 8.0\%, particularly improving detection of rare classes by 20.3\%, compared to Mask-RCNN, on LVIS dataset. Code available at: https://github.com/kostas1515/GOL}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Alexandridis-Long-tailed%20Instance%20Segmentation%20using%20Gumbel%20Optimized%20Loss-2022-ECCV.pdf}
}
@article{SegPGDGuECCV2022, 
year = {2022}, 
title = {{SegPGD: An Effective and Efficient Adversarial Attack for Evaluating and Boosting Segmentation Robustness}}, 
author = {Gu, Jindong and Zhao, Hengshuang and Tresp, Volker and Torr, Philip}, 
journal = {ECCV}, 
eprint = {2207.12391}, 
abstract = {{Deep neural network-based image classifications are vulnerable to adversarial perturbations. The image classifications can be easily fooled by adding artificial small and imperceptible perturbations to input images. As one of the most effective defense strategies, adversarial training was proposed to address the vulnerability of classification models, where the adversarial examples are created and injected into training data during training. The attack and defense of classification models have been intensively studied in past years. Semantic segmentation, as an extension of classifications, has also received great attention recently. Recent work shows a large number of attack iterations are required to create effective adversarial examples to fool segmentation models. The observation makes both robustness evaluation and adversarial training on segmentation models challenging. In this work, we propose an effective and efficient segmentation attack method, dubbed SegPGD. Besides, we provide a convergence analysis to show the proposed SegPGD can create more effective adversarial examples than PGD under the same number of attack iterations. Furthermore, we propose to apply our SegPGD as the underlying attack method for segmentation adversarial training. Since SegPGD can create more effective adversarial examples, the adversarial training with our SegPGD can boost the robustness of segmentation models. Our proposals are also verified with experiments on popular Segmentation model architectures and standard segmentation datasets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Gu-SegPGD-%20An%20Effective%20and%20Efficient%20Adversarial%20Attack%20for%20Evaluating%20and%20Boosting%20Segmentation%20Robustness-2022-ECCV.pdf}
}
@article{TPPWangECCV2022, 
year = {2022}, 
title = {{Trainability Preserving Neural Structured Pruning}}, 
author = {Wang, Huan and Fu, Yun}, 
journal = {ECCV}, 
eprint = {2207.12534}, 
abstract = {{Several recent works empirically find finetuning learning rate is critical to the final performance in neural network structured pruning. Further researches find that the network trainability broken by pruning answers for it, thus calling for an urgent need to recover trainability before finetuning. Existing attempts propose to exploit weight orthogonalization to achieve dynamical isometry for improved trainability. However, they only work for linear MLP networks. How to develop a filter pruning method that maintains or recovers trainability and is scalable to modern deep networks remains elusive. In this paper, we present trainability preserving pruning (TPP), a regularization-based structured pruning method that can effectively maintain trainability during sparsification. Specifically, TPP regularizes the gram matrix of convolutional kernels so as to de-correlate the pruned filters from the kept filters. Beside the convolutional layers, we also propose to regularize the BN parameters for better preserving trainability. Empirically, TPP can compete with the ground-truth dynamical isometry recovery method on linear MLP networks. On non-linear networks (ResNet56/VGG19, CIFAR datasets), it outperforms the other counterpart solutions by a large margin. Moreover, TPP can also work effectively with modern deep networks (ResNets) on ImageNet, delivering encouraging performance in comparison to many top-performing filter pruning methods. To our best knowledge, this is the first approach that effectively maintains trainability during pruning for the large-scale deep neural networks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Trainability%20Preserving%20Neural%20Structured%20Pruning-2022-ECCV.pdf}
}
@article{ZipfLSLiangECCV2022, 
year = {2022}, 
title = {{Efficient One Pass Self-distillation with Zipf's Label Smoothing}}, 
author = {Liang, Jiajun and Li, Linze and Bing, Zhaodong and Zhao, Borui and Tang, Yao and Lin, Bo and Fan, Haoqiang}, 
journal = {ECCV}, 
eprint = {2207.12980}, 
abstract = {{Self-distillation exploits non-uniform soft supervision from itself during training and improves performance without any runtime cost. However, the overhead during training is often overlooked, and yet reducing time and memory overhead during training is increasingly important in the giant models' era. This paper proposes an efficient self-distillation method named Zipf's Label Smoothing (Zipf's LS), which uses the on-the-fly prediction of a network to generate soft supervision that conforms to Zipf distribution without using any contrastive samples or auxiliary parameters. Our idea comes from an empirical observation that when the network is duly trained the output values of a network's final softmax layer, after sorting by the magnitude and averaged across samples, should follow a distribution reminiscent to Zipf's Law in the word frequency statistics of natural languages. By enforcing this property on the sample level and throughout the whole training period, we find that the prediction accuracy can be greatly improved. Using ResNet50 on the INAT21 fine-grained classification dataset, our technique achieves +3.61\% accuracy gain compared to the vanilla baseline, and 0.88\% more gain against the previous label smoothing or self-distillation strategies. The implementation is publicly available at https://github.com/megvii-research/zipfls.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liang-Efficient%20One%20Pass%20Self-distillation%20with%20Zipf's%20Label%20Smoothing-2022-ECCV.pdf}
}
@article{STMOhICCV2019, 
year = {2019}, 
title = {{Video Object Segmentation Using Space-Time Memory Networks}}, 
author = {Oh, Seoung Wug and Lee, Joon-Young and Xu, Ning and Kim, Seon Joo}, 
journal = {ICCV}, 
abstract = {{We propose a novel solution for semi-supervised video object segmentation. By the nature of the problem, available cues (e.g. video frame(s) with object masks) become richer with the intermediate predictions. However, the existing methods are unable to fully exploit this rich source of information. We resolve the issue by leveraging memory networks and learn to read relevant information from all available sources. In our framework, the past frames with object masks form an external memory, and the current frame as the query is segmented using the mask information in the memory. Specifically, the query and the memory are densely matched in the feature space, covering all the space-time pixel locations in a feed-forward fashion. Contrast to the previous approaches, the abundant use of the guidance information allows us to better handle the challenges such as appearance changes and occlussions. We validate our method on the latest benchmark sets and achieved the state-of-the-art performance (overall score of 79.4 on Youtube-VOS val set, J \$\textbackslashmathcal \{J\}\$ of 88.7 and 79.2 on DAVIS 2016/2017 val set respectively) while having a fast runtime (0.16 second/frame on DAVIS 2016 val set).}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Oh-Video%20Object%20Segmentation%20Using%20Space-Time%20Memory%20Networks-2019-ICCV.pdf}
}
@article{PCVOSParkCVPR2022, 
year = {2022}, 
title = {{Per-Clip Video Object Segmentation}}, 
author = {Park, Kwanyong and Woo, Sanghyun and Oh, Seoung Wug and Kweon, In So and Lee, Joon-Young}, 
journal = {CVPR}, 
abstract = {{Recently, memory-based approaches show promising results on semi-supervised video object segmentation. These methods predict object masks frame-by-frame with the help of frequently updated memory of the previous mask. Different from this per-frame inference, we investigate an alternative perspective by treating video object segmentation as clip-wise mask propagation. In this per-clip inference scheme, we update the memory with an interval and simul-taneously process a set of consecutive frames (i.e. clip) between the memory updates. The scheme provides two potential benefits: accuracy gain by clip-level optimization and efficiency gain by parallel computation of multiple frames. To this end, we propose a new method tailored for the perclip inference. Specifically, we first introduce a clip-wise operation to refine the features based on intra-clip correlation. In addition, we employ a progressive matching mechanism for efficient information-passing within a clip. With the synergy of two modules and a newly proposed perclip based training, our network achieves state-of-the-art performance on Youtube-VOS 2018/2019 val (84.6\% and 84.6\%) and DAVIS 2016/2017 val (91.9\% and 86.1\%). Fur-thermore, our model shows a great speed-accuracy trade-off with varying memory update intervals, which leads to huge flexibility.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Park-Per-Clip%20Video%20Object%20Segmentation-2022-CVPR.pdf}
}
@article{DLSAXuECCV2022, 
year = {2022}, 
title = {{Constructing Balance from Imbalance for Long-tailed Image Recognition}}, 
author = {Xu, Yue and Li, Yong-Lu and Li, Jiefeng and Lu, Cewu}, 
journal = {ECCV}, 
eprint = {2208.02567}, 
abstract = {{Long-tailed image recognition presents massive challenges to deep learning systems since the imbalance between majority (head) classes and minority (tail) classes severely skews the data-driven deep neural networks. Previous methods tackle with data imbalance from the viewpoints of data distribution, feature space, and model design, etc.In this work, instead of directly learning a recognition model, we suggest confronting the bottleneck of head-to-tail bias before classifier learning, from the previously omitted perspective of balancing label space. To alleviate the head-to-tail bias, we propose a concise paradigm by progressively adjusting label space and dividing the head classes and tail classes, dynamically constructing balance from imbalance to facilitate the classification. With flexible data filtering and label space mapping, we can easily embed our approach to most classification models, especially the decoupled training methods. Besides, we find the separability of head-tail classes varies among different features with different inductive biases. Hence, our proposed model also provides a feature evaluation method and paves the way for long-tailed feature learning. Extensive experiments show that our method can boost the performance of state-of-the-arts of different types on widely-used benchmarks. Code is available at https://github.com/silicx/DLSA.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xu-Constructing%20Balance%20from%20Imbalance%20for%20Long-tailed%20Image%20Recognition-2022-ECCV.pdf}
}
@article{NCELiECCV2022, 
year = {2022}, 
title = {{Neighborhood Collective Estimation for Noisy Label Identification and Correction}}, 
author = {Li, Jichang and Li, Guanbin and Liu, Feng and Yu, Yizhou}, 
journal = {ECCV}, 
eprint = {2208.03207}, 
abstract = {{Learning with noisy labels (LNL) aims at designing strategies to improve model performance and generalization by mitigating the effects of model overfitting to noisy labels. The key success of LNL lies in identifying as many clean samples as possible from massive noisy data, while rectifying the wrongly assigned noisy labels. Recent advances employ the predicted label distributions of individual samples to perform noise verification and noisy label correction, easily giving rise to confirmation bias. To mitigate this issue, we propose Neighborhood Collective Estimation, in which the predictive reliability of a candidate sample is re-estimated by contrasting it against its feature-space nearest neighbors. Specifically, our method is divided into two steps: 1) Neighborhood Collective Noise Verification to separate all training samples into a clean or noisy subset, 2) Neighborhood Collective Label Correction to relabel noisy samples, and then auxiliary techniques are used to assist further model optimization. Extensive experiments on four commonly used benchmark datasets, i.e., CIFAR-10, CIFAR-100, Clothing-1M and Webvision-1.0, demonstrate that our proposed method considerably outperforms state-of-the-art methods.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Neighborhood%20Collective%20Estimation%20for%20Noisy%20Label%20Identification%20and%20Correction-2022-ECCV.pdf}
}
@article{SCorrSANHuangECCV2022, 
year = {2022}, 
title = {{Learning Semantic Correspondence with Sparse Annotations}}, 
author = {Huang, Shuaiyi and Yang, Luyu and He, Bo and Zhang, Songyang and He, Xuming and Shrivastava, Abhinav}, 
journal = {ECCV}, 
eprint = {2208.06974}, 
abstract = {{Finding dense semantic correspondence is a fundamental problem in computer vision, which remains challenging in complex scenes due to background clutter, extreme intra-class variation, and a severe lack of ground truth. In this paper, we aim to address the challenge of label sparsity in semantic correspondence by enriching supervision signals from sparse keypoint annotations. To this end, we first propose a teacher-student learning paradigm for generating dense pseudo-labels and then develop two novel strategies for denoising pseudo-labels. In particular, we use spatial priors around the sparse annotations to suppress the noisy pseudo-labels. In addition, we introduce a loss-driven dynamic label selection strategy for label denoising. We instantiate our paradigm with two variants of learning strategies: a single offline teacher setting, and mutual online teachers setting. Our approach achieves notable improvements on three challenging benchmarks for semantic correspondence and establishes the new state-of-the-art. Project page: https://shuaiyihuang.github.io/publications/SCorrSAN.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Huang-Learning%20Semantic%20Correspondence%20with%20Sparse%20Annotations-2022-ECCV.pdf}
}
@article{TextPMsZhangTPAMI2022, 
year = {2022}, 
title = {{Arbitrary Shape Text Detection via Segmentation with Probability Maps}}, 
author = {Zhang, Shi-Xue and Zhu, Xiaobin and Chen, Lei and Hou, Jie-Bo and Yin, Xu-Cheng}, 
journal = {TPAMI}, 
abstract = {{Arbitrary shape text detection is a challenging task due to the significantly varied sizes and aspect ratios, arbitrary orientations or shapes, inaccurate annotations, etc. Due to the scalability of pixel-level prediction, segmentation-based methods can adapt to various shape texts and hence attracted considerable attention recently. However, accurate pixel-level annotations of texts are formidable, and the existing datasets for scene text detection only provide coarse-grained boundary annotations. Consequently, numerous misclassified text pixels or background pixels inside annotations always exist, degrading the performance of segmentation-based text detection methods. Generally speaking, whether a pixel belongs to text or not is highly related to the distance with the adjacent annotation boundary. With this observation, in this paper, we propose an innovative and robust segmentation-based detection method via probability maps for accurately detecting text instances. To be concrete, we adopt a Sigmoid Alpha Function (SAF) to transfer the distances between boundaries and their inside pixels to a probability map. However, one probability map can not cover complex probability distributions well because of the uncertainty of coarse-grained text boundary annotations. Therefore, we adopt a group of probability maps computed by a series of Sigmoid Alpha Functions to describe the possible probability distributions. In addition, we propose an iterative model to learn to predict and assimilate probability maps for providing enough information to reconstruct text instances. Finally, simple region growth algorithms are adopted to aggregate probability maps to complete text instances. Experimental results demonstrate that our method achieves state-of-the-art performance in terms of detection accuracy on several benchmarks. Notably, our method with Watershed Algorithm as post-processing achieves the best F-measure on Total-Text (88.79\%), CTW1500 (85.75\%), and MSRA-TD500 (88.93\%). Besides, our method achieves promising performance on multi-oriented datasets (ICDAR2015) and multilingual datasets (ICDAR2017-MLT). Code is available at: https://github.com/GXYM/TextPMs.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Arbitrary%20Shape%20Text%20Detection%20via%20Segmentation%20with%20Probability%20Maps-2022-TPAMI.pdf}
}
@article{FSTDuNeurIPS2022, 
year = {2022}, 
title = {{Learning from Future: A Novel Self-Training Framework for Semantic Segmentation}}, 
author = {Du, Ye and Shen, Yujun and Wang, Haochen and Fei, Jingjing and Li, Wei and Wu, Liwei and Zhao, Rui and Fu, Zehua and Liu, Qingjie}, 
journal = {NeurIPS}, 
eprint = {2209.06993}, 
abstract = {{Self-training has shown great potential in semi-supervised learning. Its core idea is to use the model learned on labeled data to generate pseudo-labels for unlabeled samples, and in turn teach itself. To obtain valid supervision, active attempts typically employ a momentum teacher for pseudo-label prediction yet observe the confirmation bias issue, where the incorrect predictions may provide wrong supervision signals and get accumulated in the training process. The primary cause of such a drawback is that the prevailing self-training framework acts as guiding the current state with previous knowledge, because the teacher is updated with the past student only. To alleviate this problem, we propose a novel self-training strategy, which allows the model to learn from the future. Concretely, at each training step, we first virtually optimize the student (i.e., caching the gradients without applying them to the model weights), then update the teacher with the virtual future student, and finally ask the teacher to produce pseudo-labels for the current student as the guidance. In this way, we manage to improve the quality of pseudo-labels and thus boost the performance. We also develop two variants of our future-self-training (FST) framework through peeping at the future both deeply (FST-D) and widely (FST-W). Taking the tasks of unsupervised domain adaptive semantic segmentation and semi-supervised semantic segmentation as the instances, we experimentally demonstrate the effectiveness and superiority of our approach under a wide range of settings. Code will be made publicly available.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Du-Learning%20from%20Future-%20A%20Novel%20Self-Training%20Framework%20for%20Semantic%20Segmentation-2022-NeurIPS.pdf}
}
@article{CondConvYangNeurIPS, 
year = {2019}, 
title = {{CondConv: Conditionally Parameterized Convolutions for Efficient Inference}}, 
author = {Yang, Brandon and Bender, Gabriel and Le, Quoc V and Ngiam, Jiquan}, 
journal = {NeurIPS}, 
eprint = {1904.04971}, 
abstract = {{Convolutional layers are one of the basic building blocks of modern deep neural networks. One fundamental assumption is that convolutional kernels should be shared for all examples in a dataset. We propose conditionally parameterized convolutions (CondConv), which learn specialized convolutional kernels for each example. Replacing normal convolutions with CondConv enables us to increase the size and capacity of a network, while maintaining efficient inference. We demonstrate that scaling networks with CondConv improves the performance and inference cost trade-off of several existing convolutional neural network architectures on both classification and detection tasks. On ImageNet classification, our CondConv approach applied to EfficientNet-B0 achieves state-of-the-art performance of 78.3\% accuracy with only 413M multiply-adds. Code and checkpoints for the CondConv Tensorflow layer and CondConv-EfficientNet models are available at: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/condconv.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-CondConv-%20Conditionally%20Parameterized%20Convolutions%20for%20Efficient%20Inference-2019-NeurIPS.pdf}
}
@article{ESOLLiNeurIPS2022, 
year = {2022}, 
title = {{Expansion and Shrinkage of Localization for Weakly-Supervised Semantic Segmentation}}, 
author = {Li, Jinlong and Jie, Zequn and Wang, Xu and Wei, Xiaolin and Ma, Lin}, 
journal = {NeurIPS}, 
eprint = {2209.07761}, 
abstract = {{Generating precise class-aware pseudo ground-truths, a.k.a, class activation maps (CAMs), is essential for weakly-supervised semantic segmentation. The original CAM method usually produces incomplete and inaccurate localization maps. To tackle with this issue, this paper proposes an Expansion and Shrinkage scheme based on the offset learning in the deformable convolution, to sequentially improve the recall and precision of the located object in the two respective stages. In the Expansion stage, an offset learning branch in a deformable convolution layer, referred as "expansion sampler" seeks for sampling increasingly less discriminative object regions, driven by an inverse supervision signal that maximizes image-level classification loss. The located more complete object in the Expansion stage is then gradually narrowed down to the final object region during the Shrinkage stage. In the Shrinkage stage, the offset learning branch of another deformable convolution layer, referred as "shrinkage sampler", is introduced to exclude the false positive background regions attended in the Expansion stage to improve the precision of the localization maps. We conduct various experiments on PASCAL VOC 2012 and MS COCO 2014 to well demonstrate the superiority of our method over other state-of-the-art methods for weakly-supervised semantic segmentation. Code will be made publicly available here https://github.com/TyroneLi/ESOL\_WSSS.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Expansion%20and%20Shrinkage%20of%20Localization%20for%20Weakly-Supervised%20Semantic%20Segmentation-2022-NeurIPS.pdf}
}
@article{GARSDCLiangECCV2022, 
year = {2022}, 
title = {{A Large-scale Multiple-objective Method for Black-box Attack against Object Detection}}, 
author = {Liang, Siyuan and Li, Longkang and Fan, Yanbo and Jia, Xiaojun and Li, Jingzhi and Wu, Baoyuan and Cao, Xiaochun}, 
journal = {ECCV}, 
eprint = {2209.07790}, 
abstract = {{Recent studies have shown that detectors based on deep models are vulnerable to adversarial examples, even in the black-box scenario where the attacker cannot access the model information. Most existing attack methods aim to minimize the true positive rate, which often shows poor attack performance, as another sub-optimal bounding box may be detected around the attacked bounding box to be the new true positive one. To settle this challenge, we propose to minimize the true positive rate and maximize the false positive rate, which can encourage more false positive objects to block the generation of new true positive bounding boxes. It is modeled as a multi-objective optimization (MOP) problem, of which the generic algorithm can search the Pareto-optimal. However, our task has more than two million decision variables, leading to low searching efficiency. Thus, we extend the standard Genetic Algorithm with Random Subset selection and Divide-and-Conquer, called GARSDC, which significantly improves the efficiency. Moreover, to alleviate the sensitivity to population quality in generic algorithms, we generate a gradient-prior initial population, utilizing the transferability between different detectors with similar backbones. Compared with the state-of-art attack methods, GARSDC decreases by an average 12.0 in the mAP and queries by about 1000 times in extensive experiments. Our codes can be found at https://github.com/LiangSiyuan21/ GARSDC.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liang-A%20Large-scale%20Multiple-objective%20Method%20for%20Black-box%20Attack%20against%20Object%20Detection-2022-ECCV.pdf}
}
@article{ODConvLiICLR2022, 
year = {2022}, 
title = {{Omni-Dimensional Dynamic Convolution}}, 
author = {Li, Chao and Zhou, Aojun and Yao, Anbang}, 
journal = {ICLR}, 
eprint = {2209.07947}, 
abstract = {{Learning a single static convolutional kernel in each convolutional layer is the common training paradigm of modern Convolutional Neural Networks (CNNs). Instead, recent research in dynamic convolution shows that learning a linear combination of \$n\$ convolutional kernels weighted with their input-dependent attentions can significantly improve the accuracy of light-weight CNNs, while maintaining efficient inference. However, we observe that existing works endow convolutional kernels with the dynamic property through one dimension (regarding the convolutional kernel number) of the kernel space, but the other three dimensions (regarding the spatial size, the input channel number and the output channel number for each convolutional kernel) are overlooked. Inspired by this, we present Omni-dimensional Dynamic Convolution (ODConv), a more generalized yet elegant dynamic convolution design, to advance this line of research. ODConv leverages a novel multi-dimensional attention mechanism with a parallel strategy to learn complementary attentions for convolutional kernels along all four dimensions of the kernel space at any convolutional layer. As a drop-in replacement of regular convolutions, ODConv can be plugged into many CNN architectures. Extensive experiments on the ImageNet and MS-COCO datasets show that ODConv brings solid accuracy boosts for various prevailing CNN backbones including both light-weight and large ones, e.g., 3.77\%\textbackslashtextasciitilde5.71\%|1.86\%\textbackslashtextasciitilde3.72\% absolute top-1 improvements to MobivleNetV2|ResNet family on the ImageNet dataset. Intriguingly, thanks to its improved feature learning ability, ODConv with even one single kernel can compete with or outperform existing dynamic convolution counterparts with multiple kernels, substantially reducing extra parameters. Furthermore, ODConv is also superior to other attention modules for modulating the output features or the convolutional weights.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Omni-Dimensional%20Dynamic%20Convolution-2022-ICLR.pdf}
}
@article{MADDoNeurIPS2022, 
year = {2022}, 
title = {{Momentum Adversarial Distillation: Handling Large Distribution Shifts in Data-Free Knowledge Distillation}}, 
author = {Do, Kien and Le, Hung and Nguyen, Dung and Nguyen, Dang and Harikumar, Haripriya and Tran, Truyen and Rana, Santu and Venkatesh, Svetha}, 
journal = {NeurIPS}, 
eprint = {2209.10359}, 
abstract = {{Data-free Knowledge Distillation (DFKD) has attracted attention recently thanks to its appealing capability of transferring knowledge from a teacher network to a student network without using training data. The main idea is to use a generator to synthesize data for training the student. As the generator gets updated, the distribution of synthetic data will change. Such distribution shift could be large if the generator and the student are trained adversarially, causing the student to forget the knowledge it acquired at previous steps. To alleviate this problem, we propose a simple yet effective method called Momentum Adversarial Distillation (MAD) which maintains an exponential moving average (EMA) copy of the generator and uses synthetic samples from both the generator and the EMA generator to train the student. Since the EMA generator can be considered as an ensemble of the generator's old versions and often undergoes a smaller change in updates compared to the generator, training on its synthetic samples can help the student recall the past knowledge and prevent the student from adapting too quickly to new updates of the generator. Our experiments on six benchmark datasets including big datasets like ImageNet and Places365 demonstrate the superior performance of MAD over competing methods for handling the large distribution shift problem. Our method also compares favorably to existing DFKD methods and even achieves state-of-the-art results in some cases.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Do-Momentum%20Adversarial%20Distillation-%20Handling%20Large%20Distribution%20Shifts%20in%20Data-Free%20Knowledge%20Distillation-2022-NeurIPS.pdf}
}
@article{LearningBacharXiv2013, 
year = {2013}, 
title = {{Learning with Submodular Functions: A Convex Optimization Perspective}}, 
author = {Bach, Francis}, 
journal = {arXiv}, 
eprint = {1111.6453}, 
abstract = {{Submodular functions are relevant to machine learning for at least two reasons: (1) some problems may be expressed directly as the optimization of submodular functions and (2) the lovasz extension of submodular functions provides a useful set of regularization functions for supervised and unsupervised learning. In this monograph, we present the theory of submodular functions from a convex analysis perspective, presenting tight links between certain polyhedra, combinatorial optimization and convex optimization problems. In particular, we show how submodular function minimization is equivalent to solving a wide variety of convex optimization problems. This allows the derivation of new efficient algorithms for approximate and exact submodular function minimization with theoretical guarantees and good practical performance. By listing many examples of submodular functions, we review various applications to machine learning, such as clustering, experimental design, sensor placement, graphical model structure learning or subset selection, as well as a family of structured sparsity-inducing norms that can be derived and used from submodular functions.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bach-Learning%20with%20Submodular%20Functions-%20A%20Convex%20Optimization%20Perspective-2013-arXiv.pdf}
}
@article{IRNetAhnCVPR2019, 
year = {2019}, 
title = {{Weakly Supervised Learning of Instance Segmentation with Inter-pixel Relations}}, 
author = {Ahn, Jiwoon and Cho, Sunghyun and Kwak, Suha}, 
journal = {CVPR}, 
eprint = {1904.05044}, 
abstract = {{This paper presents a novel approach for learning instance segmentation with image-level class labels as supervision. Our approach generates pseudo instance segmentation labels of training images, which are used to train a fully supervised model. For generating the pseudo labels, we first identify confident seed areas of object classes from attention maps of an image classification model, and propagate them to discover the entire instance areas with accurate boundaries. To this end, we propose IRNet, which estimates rough areas of individual instances and detects boundaries between different object classes. It thus enables to assign instance labels to the seeds and to propagate them within the boundaries so that the entire areas of instances can be estimated accurately. Furthermore, IRNet is trained with inter-pixel relations on the attention maps, thus no extra supervision is required. Our method with IRNet achieves an outstanding performance on the PASCAL VOC 2012 dataset, surpassing not only previous state-of-the-art trained with the same level of supervision, but also some of previous models relying on stronger supervision.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ahn-Weakly%20Supervised%20Learning%20of%20Instance%20Segmentation%20with%20Inter-pixel%20Relations-2019-CVPR.pdf}
}
@article{NoisedTop-KGarcinICML2022, 
year = {2022}, 
title = {{Stochastic smoothing of the top-K calibrated hinge loss for deep imbalanced classification}}, 
author = {Garcin, Camille and Servajean, Maximilien and Joly, Alexis and Salmon, Joseph}, 
journal = {ICML}, 
eprint = {2202.02193}, 
abstract = {{In modern classification tasks, the number of labels is getting larger and larger, as is the size of the datasets encountered in practice. As the number of classes increases, class ambiguity and class imbalance become more and more problematic to achieve high top-1 accuracy. Meanwhile, Top-K metrics (metrics allowing K guesses) have become popular, especially for performance reporting. Yet, proposing top-K losses tailored for deep learning remains a challenge, both theoretically and practically. In this paper we introduce a stochastic top-K hinge loss inspired by recent developments on top-K calibrated losses. Our proposal is based on the smoothing of the top-K operator building on the flexible "perturbed optimizer" framework. We show that our loss function performs very well in the case of balanced datasets, while benefiting from a significantly lower computational time than the state-of-the-art top-K loss function. In addition, we propose a simple variant of our loss for the imbalanced case. Experiments on a heavy-tailed dataset show that our loss function significantly outperforms other baseline loss functions.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Garcin-Stochastic%20smoothing%20of%20the%20top-K%20calibrated%20hinge%20loss%20for%20deep%20imbalanced%20classification-2022-ICML.pdf}
}
@article{EvaluatingCroceICML2022, 
year = {2022}, 
title = {{Evaluating the Adversarial Robustness of Adaptive Test-time Defenses}}, 
author = {Croce, Francesco and Gowal, Sven and Brunner, Thomas and Shelhamer, Evan and Hein, Matthias and Cemgil, Taylan}, 
journal = {ICML}, 
eprint = {2202.13711}, 
abstract = {{Adaptive defenses, which optimize at test time, promise to improve adversarial robustness. We categorize such adaptive test-time defenses, explain their potential benefits and drawbacks, and evaluate a representative variety of the latest adaptive defenses for image classification. Unfortunately, none significantly improve upon static defenses when subjected to our careful case study evaluation. Some even weaken the underlying static model while simultaneously increasing inference computation. While these results are disappointing, we still believe that adaptive test-time defenses are a promising avenue of research and, as such, we provide recommendations for their thorough evaluation. We extend the checklist of Carlini et al. (2019) by providing concrete steps specific to adaptive defenses.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Croce-Evaluating%20the%20Adversarial%20Robustness%20of%20Adaptive%20Test-time%20Defenses-2022-ICML.pdf}
}
@article{WhyHeinCVPR2019, 
year = {2019}, 
title = {{Why ReLU networks yield high-confidence predictions far away from the training data and how to mitigate the problem}}, 
author = {Hein, Matthias and Andriushchenko, Maksym and Bitterwolf, Julian}, 
journal = {CVPR}, 
abstract = {{Classifiers used in the wild, in particular for safety-critical systems, should not only have good generalization properties but also should know when they don’t know, in particular make low confidence predictions far away from the training data. We show that ReLU type neural networks which yield a piecewise linear classifier function fail in this regard as they produce almost always high confidence predictions far away from the training data. For bounded domains like images we propose a new robust optimization technique similar to adversarial training which enforces low confidence predictions far away from the training data. We show that this technique is surprisingly effective in reducing the confidence of predictions far away from the training data while maintaining high confidence predictions and test error on the original classification task compared to standard training.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hein-Why%20ReLU%20networks%20yield%20high-confidence%20predictions%20far%20away%20from%20the%20training%20data%20and%20how%20to%20mitigate%20the%20problem-2019-CVPR.pdf}
}
@article{FANZhouICML2022, 
year = {2022}, 
title = {{Understanding The Robustness in Vision Transformers}}, 
author = {Zhou, Daquan and Yu, Zhiding and Xie, Enze and Xiao, Chaowei and Anandkumar, Anima and Feng, Jiashi and Alvarez, Jose M}, 
journal = {ICML}, 
eprint = {2204.12451}, 
abstract = {{Recent studies show that Vision Transformers(ViTs) exhibit strong robustness against various corruptions. Although this property is partly attributed to the self-attention mechanism, there is still a lack of systematic understanding. In this paper, we examine the role of self-attention in learning robust representations. Our study is motivated by the intriguing properties of the emerging visual grouping in Vision Transformers, which indicates that self-attention may promote robustness through improved mid-level representations. We further propose a family of fully attentional networks (FANs) that strengthen this capability by incorporating an attentional channel processing design. We validate the design comprehensively on various hierarchical backbones. Our model achieves a state of-the-art 87.1\% accuracy and 35.8\% mCE on ImageNet-1k and ImageNet-C with 76.8M parameters. We also demonstrate state-of-the-art accuracy and robustness in two downstream tasks: semantic segmentation and object detection. Code will be available at https://github.com/NVlabs/FAN.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhou-Understanding%20The%20Robustness%20in%20Vision%20Transformers-2022-ICML.pdf}
}
@article{LatentBENamICML2022, 
year = {2022}, 
title = {{Improving Ensemble Distillation With Weight Averaging and Diversifying Perturbation}}, 
author = {Nam, Giung and Lee, Hyungi and Heo, Byeongho and Lee, Juho}, 
journal = {ICML}, 
eprint = {2206.15047}, 
abstract = {{Ensembles of deep neural networks have demonstrated superior performance, but their heavy computational cost hinders applying them for resource-limited environments. It motivates distilling knowledge from the ensemble teacher into a smaller student network, and there are two important design choices for this ensemble distillation: 1) how to construct the student network, and 2) what data should be shown during training. In this paper, we propose a weight averaging technique where a student with multiple subnetworks is trained to absorb the functional diversity of ensemble teachers, but then those subnetworks are properly averaged for inference, giving a single student network with no additional inference cost. We also propose a perturbation strategy that seeks inputs from which the diversities of teachers can be better transferred to the student. Combining these two, our method significantly improves upon previous methods on various image classification tasks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Nam-Improving%20Ensemble%20Distillation%20With%20Weight%20Averaging%20and%20Diversifying%20Perturbation-2022-ICML.pdf}
}
@article{BatchEnsembleWenICLR2020, 
year = {2020}, 
title = {{BatchEnsemble: An Alternative Approach to Efficient Ensemble and Lifelong Learning}}, 
author = {Wen, Yeming and Tran, Dustin and Ba, Jimmy}, 
journal = {ICLR}, 
eprint = {2002.06715}, 
abstract = {{Ensembles, where multiple neural networks are trained individually and their predictions are averaged, have been shown to be widely successful for improving both the accuracy and predictive uncertainty of single neural networks. However, an ensemble's cost for both training and testing increases linearly with the number of networks, which quickly becomes untenable. In this paper, we propose BatchEnsemble, an ensemble method whose computational and memory costs are significantly lower than typical ensembles. BatchEnsemble achieves this by defining each weight matrix to be the Hadamard product of a shared weight among all ensemble members and a rank-one matrix per member. Unlike ensembles, BatchEnsemble is not only parallelizable across devices, where one device trains one member, but also parallelizable within a device, where multiple ensemble members are updated simultaneously for a given mini-batch. Across CIFAR-10, CIFAR-100, WMT14 EN-DE/EN-FR translation, and out-of-distribution tasks, BatchEnsemble yields competitive accuracy and uncertainties as typical ensembles; the speedup at test time is 3X and memory reduction is 3X at an ensemble of size 4. We also apply BatchEnsemble to lifelong learning, where on Split-CIFAR-100, BatchEnsemble yields comparable performance to progressive neural networks while having a much lower computational and memory costs. We further show that BatchEnsemble can easily scale up to lifelong learning on Split-ImageNet which involves 100 sequential learning tasks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wen-BatchEnsemble-%20An%20Alternative%20Approach%20to%20Efficient%20Ensemble%20and%20Lifelong%20Learning-2020-ICLR.pdf}
}
@book{NatureVapnik1995, 
year = {1995}, 
title = {{The Nature of Statistical Learning Theory}}, 
author = {Vapnik, Vladimir N}, 
publisher = {Springer}
}
@article{SoftDiceLossSudreMICCAIWorkshop2017, 
year = {2017}, 
title = {{Generalised Dice overlap as a deep learning loss function for highly unbalanced segmentations}}, 
author = {Sudre, Carole H and Li, Wenqi and Vercauteren, Tom and Ourselin, Sébastien and Cardoso, M Jorge}, 
journal = {MICCAI Workshop}, 
eprint = {1707.03237}, 
abstract = {{Deep-learning has proved in recent years to be a powerful tool for image analysis and is now widely used to segment both 2D and 3D medical images. Deep-learning segmentation frameworks rely not only on the choice of network architecture but also on the choice of loss function. When the segmentation process targets rare observations, a severe class imbalance is likely to occur between candidate labels, thus resulting in sub-optimal performance. In order to mitigate this issue, strategies such as the weighted cross-entropy function, the sensitivity function or the Dice loss function, have been proposed. In this work, we investigate the behavior of these loss functions and their sensitivity to learning rate tuning in the presence of different rates of label imbalance across 2D and 3D segmentation tasks. We also propose to use the class re-balancing properties of the Generalized Dice overlap, a known metric for segmentation assessment, as a robust and accurate deep-learning loss function for unbalanced tasks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Sudre-Generalised%20Dice%20overlap%20as%20a%20deep%20learning%20loss%20function%20for%20highly%20unbalanced%20segmentations-2017-MICCAI%20Workshop.pdf}
}
@article{KDE-XEPopordanoskaNeurIPS2022, 
year = {2022}, 
title = {{A Consistent and Differentiable Lp Canonical Calibration Error Estimator}}, 
author = {Popordanoska, Teodora and Sayer, Raphael and Blaschko, Matthew B}, 
journal = {NeurIPS}, 
eprint = {2210.07810}, 
abstract = {{Calibrated probabilistic classifiers are models whose predicted probabilities can directly be interpreted as uncertainty estimates. It has been shown recently that deep neural networks are poorly calibrated and tend to output overconfident predictions. As a remedy, we propose a low-bias, trainable calibration error estimator based on Dirichlet kernel density estimates, which asymptotically converges to the true \$L\_p\$ calibration error. This novel estimator enables us to tackle the strongest notion of multiclass calibration, called canonical (or distribution) calibration, while other common calibration methods are tractable only for top-label and marginal calibration. The computational complexity of our estimator is \$\textbackslashmathcal\{O\}(n\textasciicircum2)\$, the convergence rate is \$\textbackslashmathcal\{O\}(n\textasciicircum\{-1/2\})\$, and it is unbiased up to \$\textbackslashmathcal\{O\}(n\textasciicircum\{-2\})\$, achieved by a geometric series debiasing scheme. In practice, this means that the estimator can be applied to small subsets of data, enabling efficient estimation and mini-batch updates. The proposed method has a natural choice of kernel, and can be used to generate consistent estimates of other quantities based on conditional expectation, such as the sharpness of a probabilistic classifier. Empirical results validate the correctness of our estimator, and demonstrate its utility in canonical calibration error estimation and calibration error regularized risk minimization.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Popordanoska-A%20Consistent%20and%20Differentiable%20Lp%20Canonical%20Calibration%20Error%20Estimator-2022-NeurIPS.pdf}
}
@article{ANoteKosubPRL2019, 
year = {2019}, 
title = {{A note on the triangle inequality for the Jaccard distance}}, 
author = {Kosub, Sven}, 
journal = {PRL}, 
abstract = {{ Two simple proofs of the triangle inequality for the Jaccard distance in terms of nonnegative, monotone, submodular functions are given and discussed.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kosub-A%20note%20on%20the%20triangle%20inequality%20for%20the%20Jaccard%20distance-2019-PRL.pdf}
}
@article{scSERoyMICCAI2018, 
year = {2018}, 
title = {{Concurrent Spatial and Channel Squeeze \& Excitation in Fully Convolutional Networks}}, 
author = {Roy, Abhijit Guha and Navab, Nassir and Wachinger, Christian}, 
journal = {MICCAI}, 
eprint = {1803.02579}, 
abstract = {{Fully convolutional neural networks (F-CNNs) have set the state-of-the-art in image segmentation for a plethora of applications. Architectural innovations within F-CNNs have mainly focused on improving spatial encoding or network connectivity to aid gradient flow. In this paper, we explore an alternate direction of recalibrating the feature maps adaptively, to boost meaningful features, while suppressing weak ones. We draw inspiration from the recently proposed squeeze \& excitation (SE) module for channel recalibration of feature maps for image classification. Towards this end, we introduce three variants of SE modules for image segmentation, (i) squeezing spatially and exciting channel-wise (cSE), (ii) squeezing channel-wise and exciting spatially (sSE) and (iii) concurrent spatial and channel squeeze \& excitation (scSE). We effectively incorporate these SE modules within three different state-of-the-art F-CNNs (DenseNet, SD-Net, U-Net) and observe consistent improvement of performance across all architectures, while minimally effecting model complexity. Evaluations are performed on two challenging applications: whole brain segmentation on MRI scans (Multi-Atlas Labelling Challenge Dataset) and organ segmentation on whole body contrast enhanced CT scans (Visceral Dataset).}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Roy-Concurrent%20Spatial%20and%20Channel%20Squeeze%20&%20Excitation%20in%20Fully%20Convolutional%20Networks-2018-MICCAI.pdf}
}
@article{DISTHuangNeurIPS2022, 
year = {2022}, 
title = {{Knowledge Distillation from A Stronger Teacher}}, 
author = {Huang, Tao and You, Shan and Wang, Fei and Qian, Chen and Xu, Chang}, 
journal = {NeurIPS}, 
eprint = {2205.10536}, 
abstract = {{Unlike existing knowledge distillation methods focus on the baseline settings, where the teacher models and training strategies are not that strong and competing as state-of-the-art approaches, this paper presents a method dubbed DIST to distill better from a stronger teacher. We empirically find that the discrepancy of predictions between the student and a stronger teacher may tend to be fairly severer. As a result, the exact match of predictions in KL divergence would disturb the training and make existing methods perform poorly. In this paper, we show that simply preserving the relations between the predictions of teacher and student would suffice, and propose a correlation-based loss to capture the intrinsic inter-class relations from the teacher explicitly. Besides, considering that different instances have different semantic similarities to each class, we also extend this relational match to the intra-class level. Our method is simple yet practical, and extensive experiments demonstrate that it adapts well to various architectures, model sizes and training strategies, and can achieve state-of-the-art performance consistently on image classification, object detection, and semantic segmentation tasks. Code is available at: https://github.com/hunto/DIST\_KD .}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Huang-Knowledge%20Distillation%20from%20A%20Stronger%20Teacher-2022-NeurIPS.pdf}
}
@article{LandRakhlinCVPRWorkshop2018, 
year = {2018}, 
title = {{Land Cover Classification from Satellite Imagery With U-Net and Lovász-Softmax Loss}}, 
author = {Rakhlin, Alexander and Davydow, Alex and Nikolenko, Sergey}, 
journal = {CVPR Workshop}, 
abstract = {{The land cover classification task of the DeepGlobe Challenge presents significant obstacles even to state of the art segmentation models due to a small amount of data, incomplete and sometimes incorrect labeling, and highly imbalanced classes. In this work, we show an approach based on the U-Net architecture with the Lovász-Softmax loss that successfully alleviates these problems; we compare several different convolutional architectures for U-Net encoders.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Rakhlin-Land%20Cover%20Classification%20from%20Satellite%20Imagery%20With%20U-Net%20and%20Lovász-Softmax%20Loss-2018-CVPR%20Workshop.pdf}
}
@article{nnU-NetIsenseeNatureMethods2021, 
year = {2021}, 
title = {{nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation}}, 
author = {Isensee, Fabian and Jaeger, Paul F. and Kohl, Simon A. A. and Petersen, Jens and Maier-Hein, Klaus H.}, 
journal = {Nature Methods}, 
abstract = {{Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training. nnU-Net is a deep learning-based image segmentation method that automatically configures itself for diverse biological and medical image segmentation tasks. nnU-Net offers state-of-the-art performance as an out-of-the-box tool.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Isensee-nnU-Net-%20a%20self-configuring%20method%20for%20deep%20learning-based%20biomedical%20image%20segmentation-2021-Nature%20Methods.pdf}
}
@article{BoundaryIoUChengCVPR2021, 
year = {2021}, 
title = {{Boundary IoU: Improving Object-Centric Image Segmentation Evaluation}}, 
author = {Cheng, Bowen and Girshick, Ross and Dollár, Piotr and Berg, Alexander C and Kirillov, Alexander}, 
journal = {CVPR}, 
eprint = {2103.16562}, 
abstract = {{We present Boundary IoU (Intersection-over-Union), a new segmentation evaluation measure focused on boundary quality. We perform an extensive analysis across different error types and object sizes and show that Boundary IoU is significantly more sensitive than the standard Mask IoU measure to boundary errors for large objects and does not over-penalize errors on smaller objects. The new quality measure displays several desirable characteristics like symmetry w.r.t. prediction/ground truth pairs and balanced responsiveness across scales, which makes it more suitable for segmentation evaluation than other boundary-focused measures like Trimap IoU and F-measure. Based on Boundary IoU, we update the standard evaluation protocols for instance and panoptic segmentation tasks by proposing the Boundary AP (Average Precision) and Boundary PQ (Panoptic Quality) metrics, respectively. Our experiments show that the new evaluation metrics track boundary quality improvements that are generally overlooked by current Mask IoU-based evaluation metrics. We hope that the adoption of the new boundary-sensitive evaluation metrics will lead to rapid progress in segmentation methods that improve boundary quality.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Cheng-Boundary%20IoU-%20Improving%20Object-Centric%20Image%20Segmentation%20Evaluation-2021-CVPR.pdf}
}
@article{MasKDHuangICLR2023, 
year = {2023}, 
title = {{Masked Distillation with Receptive Tokens}}, 
author = {Huang, Tao and Zhang, Yuan and You, Shan and Wang, Fei and Qian, Chen and Cao, Jian and Xu, Chang}, 
journal = {ICLR}, 
eprint = {2205.14589}, 
abstract = {{Distilling from the feature maps can be fairly effective for dense prediction tasks since both the feature discriminability and localization priors can be well transferred. However, not every pixel contributes equally to the performance, and a good student should learn from what really matters to the teacher. In this paper, we introduce a learnable embedding dubbed receptive token to localize those pixels of interests (PoIs) in the feature map, with a distillation mask generated via pixel-wise attention. Then the distillation will be performed on the mask via pixel-wise reconstruction. In this way, a distillation mask actually indicates a pattern of pixel dependencies within feature maps of teacher. We thus adopt multiple receptive tokens to investigate more sophisticated and informative pixel dependencies to further enhance the distillation. To obtain a group of masks, the receptive tokens are learned via the regular task loss but with teacher fixed, and we also leverage a Dice loss to enrich the diversity of learned masks. Our method dubbed MasKD is simple and practical, and needs no priors of tasks in application. Experiments show that our MasKD can achieve state-of-the-art performance consistently on object detection and semantic segmentation benchmarks. Code is available at: https://github.com/hunto/MasKD .}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Huang-Masked%20Distillation%20with%20Receptive%20Tokens-2022-arXiv.pdf}
}
@article{AdaFocalGhoshNeurIPS2022, 
year = {2022}, 
title = {{AdaFocal: Calibration-aware Adaptive Focal Loss}}, 
author = {Ghosh, Arindam and Schaaf, Thomas and Gormley, Matt}, 
journal = {NeurIPS}, 
eprint = {2211.11838}, 
abstract = {{Much recent work has been devoted to the problem of ensuring that a neural network's confidence scores match the true probability of being correct, i.e. the calibration problem. Of note, it was found that training with focal loss leads to better calibration than cross-entropy while achieving similar level of accuracy \textbackslashcite\{mukhoti2020\}. This success stems from focal loss regularizing the entropy of the model's prediction (controlled by the parameter \$\textbackslashgamma\$), thereby reining in the model's overconfidence. Further improvement is expected if \$\textbackslashgamma\$ is selected independently for each training sample (Sample-Dependent Focal Loss (FLSD-53) \textbackslashcite\{mukhoti2020\}). However, FLSD-53 is based on heuristics and does not generalize well. In this paper, we propose a calibration-aware adaptive focal loss called AdaFocal that utilizes the calibration properties of focal (and inverse-focal) loss and adaptively modifies \$\textbackslashgamma\_t\$ for different groups of samples based on \$\textbackslashgamma\_\{t-1\}\$ from the previous step and the knowledge of model's under/over-confidence on the validation set. We evaluate AdaFocal on various image recognition and one NLP task, covering a wide variety of network architectures, to confirm the improvement in calibration while achieving similar levels of accuracy. Additionally, we show that models trained with AdaFocal achieve a significant boost in out-of-distribution detection.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ghosh-AdaFocal-%20Calibration-aware%20Adaptive%20Focal%20Loss-2022-NeurIPS.pdf}
}
@article{OnTheConsistencyYangICML2020, 
year = {2020}, 
title = {{On the Consistency of Top-k Surrogate Losses}}, 
author = {Yang, Forest and Koyejo, Sanmi}, 
journal = {ICML}, 
eprint = {1901.11141}, 
abstract = {{The top-\$k\$ error is often employed to evaluate performance for challenging classification tasks in computer vision as it is designed to compensate for ambiguity in ground truth labels. This practical success motivates our theoretical analysis of consistent top-\$k\$ classification. Surprisingly, it is not rigorously understood when taking the \$k\$-argmax of a vector is guaranteed to return the \$k\$-argmax of another vector, though doing so is crucial to describe Bayes optimality; we do both tasks. Then, we define top-\$k\$ calibration and show it is necessary and sufficient for consistency. Based on the top-\$k\$ calibration analysis, we propose a class of top-\$k\$ calibrated Bregman divergence surrogates. Our analysis continues by showing previously proposed hinge-like top-\$k\$ surrogate losses are not top-\$k\$ calibrated and suggests no convex hinge loss is top-\$k\$ calibrated. On the other hand, we propose a new hinge loss which is consistent. We explore further, showing our hinge loss remains consistent under a restriction to linear functions, while cross entropy does not. Finally, we exhibit a differentiable, convex loss function which is top-\$k\$ calibrated for specific \$k\$.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-On%20the%20Consistency%20of%20Top-k%20Surrogate%20Losses-2020-ICML.pdf}
}
@article{UnderstandingLiECCV2022, 
year = {2022}, 
title = {{Understanding Collapse in Non-Contrastive Siamese Representation Learning}}, 
author = {Li, Alexander C and Efros, Alexei A and Pathak, Deepak}, 
journal = {ECCV}, 
eprint = {2209.15007}, 
abstract = {{Contrastive methods have led a recent surge in the performance of self-supervised representation learning (SSL). Recent methods like BYOL or SimSiam purportedly distill these contrastive methods down to their essence, removing bells and whistles, including the negative examples, that do not contribute to downstream performance. These "non-contrastive" methods work surprisingly well without using negatives even though the global minimum lies at trivial collapse. We empirically analyze these non-contrastive methods and find that SimSiam is extraordinarily sensitive to dataset and model size. In particular, SimSiam representations undergo partial dimensional collapse if the model is too small relative to the dataset size. We propose a metric to measure the degree of this collapse and show that it can be used to forecast the downstream task performance without any fine-tuning or labels. We further analyze architectural design choices and their effect on the downstream performance. Finally, we demonstrate that shifting to a continual learning setting acts as a regularizer and prevents collapse, and a hybrid between continual and multi-epoch training can improve linear probe accuracy by as many as 18 percentage points using ResNet-18 on ImageNet. Our project page is at https://alexanderli.com/noncontrastive-ssl/.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Understanding%20Collapse%20in%20Non-Contrastive%20Siamese%20Representation%20Learning-2022-ECCV.pdf}
}
@article{SmoothBerradaICLR2018, 
year = {2018}, 
title = {{Smooth Loss Functions for Deep Top-k Classification}}, 
author = {Berrada, Leonard and Zisserman, Andrew and Kumar, M Pawan}, 
journal = {ICLR}, 
eprint = {1802.07595}, 
abstract = {{The top-k error is a common measure of performance in machine learning and computer vision. In practice, top-k classification is typically performed with deep neural networks trained with the cross-entropy loss. Theoretical results indeed suggest that cross-entropy is an optimal learning objective for such a task in the limit of infinite data. In the context of limited and noisy data however, the use of a loss function that is specifically designed for top-k classification can bring significant improvements. Our empirical evidence suggests that the loss function must be smooth and have non-sparse gradients in order to work well with deep neural networks. Consequently, we introduce a family of smoothed loss functions that are suited to top-k optimization via deep learning. The widely used cross-entropy is a special case of our family. Evaluating our smooth loss functions is computationally challenging: a na\textbackslash"ive algorithm would require \$\textbackslashmathcal\{O\}(\textbackslashbinom\{n\}\{k\})\$ operations, where n is the number of classes. Thanks to a connection to polynomial algebra and a divide-and-conquer approach, we provide an algorithm with a time complexity of \$\textbackslashmathcal\{O\}(k n)\$. Furthermore, we present a novel approximation to obtain fast and stable algorithms on GPUs with single floating point precision. We compare the performance of the cross-entropy loss and our margin-based losses in various regimes of noise and data size, for the predominant use case of k=5. Our investigation reveals that our loss is more robust to noise and overfitting than cross-entropy.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Berrada-Smooth%20Loss%20Functions%20for%20Deep%20Top-k%20Classification-2018-ICLR.pdf}
}
@article{LearningBerthetNeurIPS2020, 
year = {2020}, 
title = {{Learning with Differentiable Perturbed Optimizers}}, 
author = {Berthet, Quentin and Blondel, Mathieu and Teboul, Olivier and Cuturi, Marco and Vert, Jean-Philippe and Bach, Francis}, 
journal = {NeurIPS}, 
eprint = {2002.08676}, 
abstract = {{Machine learning pipelines often rely on optimization procedures to make discrete decisions (e.g., sorting, picking closest neighbors, or shortest paths). Although these discrete decisions are easily computed, they break the back-propagation of computational graphs. In order to expand the scope of learning problems that can be solved in an end-to-end fashion, we propose a systematic method to transform optimizers into operations that are differentiable and never locally constant. Our approach relies on stochastically perturbed optimizers, and can be used readily together with existing solvers. Their derivatives can be evaluated efficiently, and smoothness tuned via the chosen noise amplitude. We also show how this framework can be connected to a family of losses developed in structured prediction, and give theoretical guarantees for their use in learning tasks. We demonstrate experimentally the performance of our approach on various tasks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Berthet-Learning%20with%20Differentiable%20Perturbed%20Optimizers-2020-NeurIPS.pdf}
}
@article{SWAIzmailovUAI2018, 
year = {2018}, 
title = {{Averaging Weights Leads to Wider Optima and Better Generalization}}, 
author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon}, 
journal = {UAI}, 
eprint = {1803.05407}, 
abstract = {{Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Izmailov-Averaging%20Weights%20Leads%20to%20Wider%20Optima%20and%20Better%20Generalization-2018-UAI.pdf}
}
@article{SVLSIslamIPMI2021, 
year = {2021}, 
title = {{Spatially Varying Label Smoothing: Capturing Uncertainty from Expert Annotations}}, 
author = {Islam, Mobarakol and Glocker, Ben}, 
journal = {IPMI}, 
eprint = {2104.05788}, 
abstract = {{The task of image segmentation is inherently noisy due to ambiguities regarding the exact location of boundaries between anatomical structures. We argue that this information can be extracted from the expert annotations at no extra cost, and when integrated into state-of-the-art neural networks, it can lead to improved calibration between soft probabilistic predictions and the underlying uncertainty. We built upon label smoothing (LS) where a network is trained on 'blurred' versions of the ground truth labels which has been shown to be effective for calibrating output predictions. However, LS is not taking the local structure into account and results in overly smoothed predictions with low confidence even for non-ambiguous regions. Here, we propose Spatially Varying Label Smoothing (SVLS), a soft labeling technique that captures the structural uncertainty in semantic segmentation. SVLS also naturally lends itself to incorporate inter-rater uncertainty when multiple labelmaps are available. The proposed approach is extensively validated on four clinical segmentation tasks with different imaging modalities, number of classes and single and multi-rater expert annotations. The results demonstrate that SVLS, despite its simplicity, obtains superior boundary prediction with improved uncertainty and model calibration.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Islam-Spatially%20Varying%20Label%20Smoothing-%20Capturing%20Uncertainty%20from%20Expert%20Annotations-2021-IPMI.pdf}
}
@article{LossLapinCVPR2016, 
year = {2016}, 
title = {{Loss Functions for Top-k Error: Analysis and Insights}}, 
author = {Lapin, Maksim and Hein, Matthias and Schiele, Bernt}, 
journal = {CVPR}, 
eprint = {1512.00486}, 
abstract = {{In order to push the performance on realistic computer vision tasks, the number of classes in modern benchmark datasets has significantly increased in recent years. This increase in the number of classes comes along with increased ambiguity between the class labels, raising the question if top-1 error is the right performance measure. In this paper, we provide an extensive comparison and evaluation of established multiclass methods comparing their top-k performance both from a practical as well as from a theoretical perspective. Moreover, we introduce novel top-k loss functions as modifications of the softmax and the multiclass SVM losses and provide efficient optimization schemes for them. In the experiments, we compare on various datasets all of the proposed and established methods for top-k error optimization. An interesting insight of this paper is that the softmax loss yields competitive top-k performance for all k simultaneously. For a specific top-k error, our new top-k losses lead typically to further improvements while being faster to train than the softmax.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lapin-Loss%20Functions%20for%20Top-k%20Error-%20Analysis%20and%20Insights-2016-CVPR_1.pdf}
}
@article{ReLossHuangICLR2022, 
year = {2022}, 
title = {{Relational Surrogate Loss Learning}}, 
author = {Huang, Tao and Li, Zekang and Lu, Hua and Shan, Yong and Yang, Shusheng and Feng, Yang and Wang, Fei and You, Shan and Xu, Chang}, 
journal = {ICLR}, 
eprint = {2202.13197}, 
abstract = {{Evaluation metrics in machine learning are often hardly taken as loss functions, as they could be non-differentiable and non-decomposable, e.g., average precision and F1 score. This paper aims to address this problem by revisiting the surrogate loss learning, where a deep neural network is employed to approximate the evaluation metrics. Instead of pursuing an exact recovery of the evaluation metric through a deep neural network, we are reminded of the purpose of the existence of these evaluation metrics, which is to distinguish whether one model is better or worse than another. In this paper, we show that directly maintaining the relation of models between surrogate losses and metrics suffices, and propose a rank correlation-based optimization method to maximize this relation and learn surrogate losses. Compared to previous works, our method is much easier to optimize and enjoys significant efficiency and performance gains. Extensive experiments show that our method achieves improvements on various tasks including image classification and neural machine translation, and even outperforms state-of-the-art methods on human pose estimation and machine reading comprehension tasks. Code is available at: https://github.com/hunto/ReLoss.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Huang-Relational%20Surrogate%20Loss%20Learning-2022-ICLR.pdf}
}
@article{AdaptiveTSJoyAAAI2023, 
year = {2023}, 
title = {{Sample-dependent Adaptive Temperature Scaling for Improved Calibration}}, 
author = {Joy, Tom and Pinto, Francesco and Lim, Ser-Nam and Torr, Philip H S and Dokania, Puneet K}, 
journal = {AAAI}, 
eprint = {2207.06211}, 
abstract = {{It is now well known that neural networks can be wrong with high confidence in their predictions, leading to poor calibration. The most common post-hoc approach to compensate for this is to perform temperature scaling, which adjusts the confidences of the predictions on any input by scaling the logits by a fixed value. Whilst this approach typically improves the average calibration across the whole test dataset, this improvement typically reduces the individual confidences of the predictions irrespective of whether the classification of a given input is correct or incorrect. With this insight, we base our method on the observation that different samples contribute to the calibration error by varying amounts, with some needing to increase their confidence and others needing to decrease it. Therefore, for each input, we propose to predict a different temperature value, allowing us to adjust the mismatch between confidence and accuracy at a finer granularity. Furthermore, we observe improved results on OOD detection and can also extract a notion of hardness for the data-points. Our method is applied post-hoc, consequently using very little computation time and with a negligible memory footprint and is applied to off-the-shelf pre-trained classifiers. We test our method on the ResNet50 and WideResNet28-10 architectures using the CIFAR10/100 and Tiny-ImageNet datasets, showing that producing per-data-point temperatures is beneficial also for the expected calibration error across the whole test set. Code is available at: https://github.com/thwjoy/adats.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Joy-Sample-dependent%20Adaptive%20Temperature%20Scaling%20for%20Improved%20Calibration-2022-arXiv.pdf}
}
@article{OnTheRobustnessChhabraNeurIPS2022, 
year = {2022}, 
title = {{On the Robustness of Deep Clustering Models: Adversarial Attacks and Defenses}}, 
author = {Chhabra, Anshuman and Sekhari, Ashwin and Mohapatra, Prasant}, 
journal = {NeurIPS}, 
doi = {10.48550/arxiv.2210.01940}, 
eprint = {2210.01940}, 
abstract = {{Clustering models constitute a class of unsupervised machine learning methods which are used in a number of application pipelines, and play a vital role in modern data science. With recent advancements in deep learning -- deep clustering models have emerged as the current state-of-the-art over traditional clustering approaches, especially for high-dimensional image datasets. While traditional clustering approaches have been analyzed from a robustness perspective, no prior work has investigated adversarial attacks and robustness for deep clustering models in a principled manner. To bridge this gap, we propose a blackbox attack using Generative Adversarial Networks (GANs) where the adversary does not know which deep clustering model is being used, but can query it for outputs. We analyze our attack against multiple state-of-the-art deep clustering models and real-world datasets, and find that it is highly successful. We then employ some natural unsupervised defense approaches, but find that these are unable to mitigate our attack. Finally, we attack Face++, a production-level face clustering API service, and find that we can significantly reduce its performance as well. Through this work, we thus aim to motivate the need for truly robust deep clustering models.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chhabra-On%20the%20Robustness%20of%20Deep%20Clustering%20Models-%20Adversarial%20Attacks%20and%20Defenses-2022-NeurIPS.pdf}
}
@article{DeepReinforcementLearningChristianoNeurIPS2017, 
year = {2017}, 
title = {{Deep reinforcement learning from human preferences}}, 
author = {Christiano, Paul and Leike, Jan and Brown, Tom B and Martic, Miljan and Legg, Shane and Amodei, Dario}, 
journal = {NeurIPS}, 
eprint = {1706.03741}, 
abstract = {{For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Christiano-Deep%20reinforcement%20learning%20from%20human%20preferences-2017-NeurIPS.pdf}
}
@article{RLHFStiennonNeurIPS2020, 
year = {2020}, 
title = {{Learning to summarize from human feedback}}, 
author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeff and Ziegler, Daniel M and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul}, 
journal = {NeurIPS}, 
eprint = {2009.01325}, 
abstract = {{As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Stiennon-Learning%20to%20summarize%20from%20human%20feedback-2020-NeurIPS.pdf}
}
@article{LocalFGRenICLR2023, 
year = {2023}, 
title = {{Scaling Forward Gradient With Local Losses}}, 
author = {Ren, Mengye and Kornblith, Simon and Liao, Renjie and Hinton, Geoffrey}, 
journal = {ICLR}, 
eprint = {2210.03310}, 
abstract = {{Forward gradient learning computes a noisy directional gradient and is a biologically plausible alternative to backprop for learning deep neural networks. However, the standard forward gradient algorithm, when applied naively, suffers from high variance when the number of parameters to be learned is large. In this paper, we propose a series of architectural and algorithmic modifications that together make forward gradient learning practical for standard deep learning benchmark tasks. We show that it is possible to substantially reduce the variance of the forward gradient estimator by applying perturbations to activations rather than weights. We further improve the scalability of forward gradient by introducing a large number of local greedy loss functions, each of which involves only a small number of learnable parameters, and a new MLPMixer-inspired architecture, LocalMixer, that is more suitable for local learning. Our approach matches backprop on MNIST and CIFAR-10 and significantly outperforms previously proposed backprop-free algorithms on ImageNet.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ren-Scaling%20Forward%20Gradient%20With%20Local%20Losses-2022-arXiv.pdf}
}
@article{CW-RGPWengNeurIPS2022, 
year = {2022}, 
title = {{An Investigation into Whitening Loss for Self-supervised Learning}}, 
author = {Weng, Xi and Huang, Lei and Zhao, Lei and Anwer, Rao Muhammad and Khan, Salman and Khan, Fahad Shahbaz}, 
journal = {NeurIPS}, 
eprint = {2210.03586}, 
abstract = {{A desirable objective in self-supervised learning (SSL) is to avoid feature collapse. Whitening loss guarantees collapse avoidance by minimizing the distance between embeddings of positive pairs under the conditioning that the embeddings from different views are whitened. In this paper, we propose a framework with an informative indicator to analyze whitening loss, which provides a clue to demystify several interesting phenomena as well as a pivoting point connecting to other SSL methods. We reveal that batch whitening (BW) based methods do not impose whitening constraints on the embedding, but they only require the embedding to be full-rank. This full-rank constraint is also sufficient to avoid dimensional collapse. Based on our analysis, we propose channel whitening with random group partition (CW-RGP), which exploits the advantages of BW-based methods in preventing collapse and avoids their disadvantages requiring large batch size. Experimental results on ImageNet classification and COCO object detection reveal that the proposed CW-RGP possesses a promising potential for learning good representations. The code is available at https://github.com/winci-ai/CW-RGP.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Weng-An%20Investigation%20into%20Whitening%20Loss%20for%20Self-supervised%20Learning-2022-NeurIPS.pdf}
}
@article{C-MixupYaoNeurIPS2022, 
year = {2022}, 
title = {{C-Mixup: Improving Generalization in Regression}}, 
author = {Yao, Huaxiu and Wang, Yiping and Zhang, Linjun and Zou, James and Finn, Chelsea}, 
journal = {NeurIPS}, 
eprint = {2210.05775}, 
abstract = {{Improving the generalization of deep networks is an important open challenge, particularly in domains without plentiful data. The mixup algorithm improves generalization by linearly interpolating a pair of examples and their corresponding labels. These interpolated examples augment the original training set. Mixup has shown promising results in various classification tasks, but systematic analysis of mixup in regression remains underexplored. Using mixup directly on regression labels can result in arbitrarily incorrect labels. In this paper, we propose a simple yet powerful algorithm, C-Mixup, to improve generalization on regression tasks. In contrast with vanilla mixup, which picks training examples for mixing with uniform probability, C-Mixup adjusts the sampling probability based on the similarity of the labels. Our theoretical analysis confirms that C-Mixup with label similarity obtains a smaller mean square error in supervised regression and meta-regression than vanilla mixup and using feature similarity. Another benefit of C-Mixup is that it can improve out-of-distribution robustness, where the test distribution is different from the training distribution. By selectively interpolating examples with similar labels, it mitigates the effects of domain-associated information and yields domain-invariant representations. We evaluate C-Mixup on eleven datasets, ranging from tabular to video data. Compared to the best prior approach, C-Mixup achieves 6.56\%, 4.76\%, 5.82\% improvements in in-distribution generalization, task generalization, and out-of-distribution robustness, respectively. Code is released at https://github.com/huaxiuyao/C-Mixup.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yao-C-Mixup-%20Improving%20Generalization%20in%20Regression-2022-NeurIPS.pdf}
}
@article{WENOQuNeurIPS2022, 
year = {2022}, 
title = {{Bi-directional Weakly Supervised Knowledge Distillation for Whole Slide Image Classification}}, 
author = {Qu, Linhao and Luo, Xiaoyuan and Wang, Manning and Song, Zhijian}, 
journal = {NeurIPS}, 
eprint = {2210.03664}, 
abstract = {{Computer-aided pathology diagnosis based on the classification of Whole Slide Image (WSI) plays an important role in clinical practice, and it is often formulated as a weakly-supervised Multiple Instance Learning (MIL) problem. Existing methods solve this problem from either a bag classification or an instance classification perspective. In this paper, we propose an end-to-end weakly supervised knowledge distillation framework (WENO) for WSI classification, which integrates a bag classifier and an instance classifier in a knowledge distillation framework to mutually improve the performance of both classifiers. Specifically, an attention-based bag classifier is used as the teacher network, which is trained with weak bag labels, and an instance classifier is used as the student network, which is trained using the normalized attention scores obtained from the teacher network as soft pseudo labels for the instances in positive bags. An instance feature extractor is shared between the teacher and the student to further enhance the knowledge exchange between them. In addition, we propose a hard positive instance mining strategy based on the output of the student network to force the teacher network to keep mining hard positive instances. WENO is a plug-and-play framework that can be easily applied to any existing attention-based bag classification methods. Extensive experiments on five datasets demonstrate the efficiency of WENO. Code is available at https://github.com/miccaiif/WENO.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Qu-Bi-directional%20Weakly%20Supervised%20Knowledge%20Distillation%20for%20Whole%20Slide%20Image%20Classification-2022-NeurIPS.pdf}
}
@article{DKDBaekNeurIPS2022, 
year = {2022}, 
title = {{Decomposed Knowledge Distillation for Class-Incremental Semantic Segmentation}}, 
author = {Baek, Donghyeon and Oh, Youngmin and Lee, Sanghoon and Lee, Junghyup and Ham, Bumsub}, 
journal = {NeurIPS}, 
eprint = {2210.05941}, 
abstract = {{Class-incremental semantic segmentation (CISS) labels each pixel of an image with a corresponding object/stuff class continually. To this end, it is crucial to learn novel classes incrementally without forgetting previously learned knowledge. Current CISS methods typically use a knowledge distillation (KD) technique for preserving classifier logits, or freeze a feature extractor, to avoid the forgetting problem. The strong constraints, however, prevent learning discriminative features for novel classes. We introduce a CISS framework that alleviates the forgetting problem and facilitates learning novel classes effectively. We have found that a logit can be decomposed into two terms. They quantify how likely an input belongs to a particular class or not, providing a clue for a reasoning process of a model. The KD technique, in this context, preserves the sum of two terms (i.e., a class logit), suggesting that each could be changed and thus the KD does not imitate the reasoning process. To impose constraints on each term explicitly, we propose a new decomposed knowledge distillation (DKD) technique, improving the rigidity of a model and addressing the forgetting problem more effectively. We also introduce a novel initialization method to train new classifiers for novel classes. In CISS, the number of negative training samples for novel classes is not sufficient to discriminate old classes. To mitigate this, we propose to transfer knowledge of negatives to the classifiers successively using an auxiliary classifier, boosting the performance significantly. Experimental results on standard CISS benchmarks demonstrate the effectiveness of our framework.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Baek-Decomposed%20Knowledge%20Distillation%20for%20Class-Incremental%20Semantic%20Segmentation-2022-NeurIPS.pdf}
}
@article{GATSHsuNeurIPS2022, 
year = {2022}, 
title = {{What Makes Graph Neural Networks Miscalibrated?}}, 
author = {Hsu, Hans Hao-Hsun and Shen, Yuesong and Tomani, Christian and Cremers, Daniel}, 
journal = {NeurIPS}, 
eprint = {2210.06391}, 
abstract = {{Given the importance of getting calibrated predictions and reliable uncertainty estimations, various post-hoc calibration methods have been developed for neural networks on standard multi-class classification tasks. However, these methods are not well suited for calibrating graph neural networks (GNNs), which presents unique challenges such as accounting for the graph structure and the graph-induced correlations between the nodes. In this work, we conduct a systematic study on the calibration qualities of GNN node predictions. In particular, we identify five factors which influence the calibration of GNNs: general under-confident tendency, diversity of nodewise predictive distributions, distance to training nodes, relative confidence level, and neighborhood similarity. Furthermore, based on the insights from this study, we design a novel calibration method named Graph Attention Temperature Scaling (GATS), which is tailored for calibrating graph neural networks. GATS incorporates designs that address all the identified influential factors and produces nodewise temperature scaling using an attention-based architecture. GATS is accuracy-preserving, data-efficient, and expressive at the same time. Our experiments empirically verify the effectiveness of GATS, demonstrating that it can consistently achieve state-of-the-art calibration results on various graph datasets for different GNN backbones.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hsu-What%20Makes%20Graph%20Neural%20Networks%20Miscalibrated--2022-NeurIPS.pdf}
}
@article{CheckpointKDWangNeurIPS2022, 
year = {2022}, 
title = {{Efficient Knowledge Distillation from Model Checkpoints}}, 
author = {Wang, Chaofei and Yang, Qisen and Huang, Rui and Song, Shiji and Huang, Gao}, 
journal = {NeurIPS}, 
eprint = {2210.06458}, 
abstract = {{Knowledge distillation is an effective approach to learn compact models (students) with the supervision of large and strong models (teachers). As empirically there exists a strong correlation between the performance of teacher and student models, it is commonly believed that a high performing teacher is preferred. Consequently, practitioners tend to use a well trained network or an ensemble of them as the teacher. In this paper, we make an intriguing observation that an intermediate model, i.e., a checkpoint in the middle of the training procedure, often serves as a better teacher compared to the fully converged model, although the former has much lower accuracy. More surprisingly, a weak snapshot ensemble of several intermediate models from a same training trajectory can outperform a strong ensemble of independently trained and fully converged models, when they are used as teachers. We show that this phenomenon can be partially explained by the information bottleneck principle: the feature representations of intermediate models can have higher mutual information regarding the input, and thus contain more "dark knowledge" for effective distillation. We further propose an optimal intermediate teacher selection algorithm based on maximizing the total task-related mutual information. Experiments verify its effectiveness and applicability.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Efficient%20Knowledge%20Distillation%20from%20Model%20Checkpoints-2022-NeurIPS.pdf}
}
@article{WeightedIliopoulosNeurIPS2022, 
year = {2022}, 
title = {{Weighted Distillation with Unlabeled Examples}}, 
author = {Iliopoulos, Fotis and Kontonis, Vasilis and Baykal, Cenk and Menghani, Gaurav and Trinh, Khoa and Vee, Erik}, 
journal = {NeurIPS}, 
eprint = {2210.06711}, 
abstract = {{Distillation with unlabeled examples is a popular and powerful method for training deep neural networks in settings where the amount of labeled data is limited: A large ''teacher'' neural network is trained on the labeled data available, and then it is used to generate labels on an unlabeled dataset (typically much larger in size). These labels are then utilized to train the smaller ''student'' model which will actually be deployed. Naturally, the success of the approach depends on the quality of the teacher's labels, since the student could be confused if trained on inaccurate data. This paper proposes a principled approach for addressing this issue based on a ''debiasing'' reweighting of the student's loss function tailored to the distillation training paradigm. Our method is hyper-parameter free, data-agnostic, and simple to implement. We demonstrate significant improvements on popular academic datasets and we accompany our results with a theoretical analysis which rigorously justifies the performance of our method in certain settings.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Iliopoulos-Weighted%20Distillation%20with%20Unlabeled%20Examples-2022-NeurIPS.pdf}
}
@article{AMDKDBiNeurIPS2022, 
year = {2022}, 
title = {{Learning Generalizable Models for Vehicle Routing Problems via Knowledge Distillation}}, 
author = {Bi, Jieyi and Ma, Yining and Wang, Jiahai and Cao, Zhiguang and Chen, Jinbiao and Sun, Yuan and Chee, Yeow Meng}, 
journal = {NeurIPS}, 
eprint = {2210.07686}, 
abstract = {{Recent neural methods for vehicle routing problems always train and test the deep models on the same instance distribution (i.e., uniform). To tackle the consequent cross-distribution generalization concerns, we bring the knowledge distillation to this field and propose an Adaptive Multi-Distribution Knowledge Distillation (AMDKD) scheme for learning more generalizable deep models. Particularly, our AMDKD leverages various knowledge from multiple teachers trained on exemplar distributions to yield a light-weight yet generalist student model. Meanwhile, we equip AMDKD with an adaptive strategy that allows the student to concentrate on difficult distributions, so as to absorb hard-to-master knowledge more effectively. Extensive experimental results show that, compared with the baseline neural methods, our AMDKD is able to achieve competitive results on both unseen in-distribution and out-of-distribution instances, which are either randomly synthesized or adopted from benchmark datasets (i.e., TSPLIB and CVRPLIB). Notably, our AMDKD is generic, and consumes less computational resources for inference.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bi-Learning%20Generalizable%20Models%20for%20Vehicle%20Routing%20Problems%20via%20Knowledge%20Distillation-2022-NeurIPS.pdf}
}
@article{NastyTeacherMaICLR2021, 
year = {2021}, 
title = {{Undistillable: Making A Nasty Teacher That CANNOT teach students}}, 
author = {Ma, Haoyu and Chen, Tianlong and Hu, Ting-Kuei and You, Chenyu and Xie, Xiaohui and Wang, Zhangyang}, 
journal = {ICLR}, 
eprint = {2105.07381}, 
abstract = {{Knowledge Distillation (KD) is a widely used technique to transfer knowledge from pre-trained teacher models to (usually more lightweight) student models. However, in certain situations, this technique is more of a curse than a blessing. For instance, KD poses a potential risk of exposing intellectual properties (IPs): even if a trained machine learning model is released in 'black boxes' (e.g., as executable software or APIs without open-sourcing code), it can still be replicated by KD through imitating input-output behaviors. To prevent this unwanted effect of KD, this paper introduces and investigates a concept called Nasty Teacher: a specially trained teacher network that yields nearly the same performance as a normal one, but would significantly degrade the performance of student models learned by imitating it. We propose a simple yet effective algorithm to build the nasty teacher, called self-undermining knowledge distillation. Specifically, we aim to maximize the difference between the output of the nasty teacher and a normal pre-trained network. Extensive experiments on several datasets demonstrate that our method is effective on both standard KD and data-free KD, providing the desirable KD-immunity to model owners for the first time. We hope our preliminary study can draw more awareness and interest in this new practical problem of both social and legal importance.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ma-Undistillable-%20Making%20A%20Nasty%20Teacher%20That%20CANNOT%20teach%20students-2021-ICLR.pdf}
}
@article{DAFLChenICCV2019, 
year = {2019}, 
title = {{Data-Free Learning of Student Networks}}, 
author = {Chen, Hanting and Wang, Yunhe and Xu, Chang and Yang, Zhaohui and Liu, Chuanjian and Shi, Boxin and Xu, Chunjing and Xu, Chao and Tian, Qi}, 
journal = {ICCV}, 
abstract = {{Learning portable neural networks is very essential for computer vision for the purpose that pre-trained heavy deep models can be well applied on edge devices such as mobile phones and micro sensors. Most existing deep neural network compression and speed-up methods are very effective for training compact deep models, when we can directly access the training dataset. However, training data for the given deep network are often unavailable due to some practice problems (e.g. privacy, legal issue, and transmission), and the architecture of the given network are also unknown except some interfaces. To this end, we propose a novel framework for training efficient deep neural networks by exploiting generative adversarial networks (GANs). To be specific, the pre-trained teacher networks are regarded as a fixed discriminator and the generator is utilized for derivating training samples which can obtain the maximum response on the discriminator. Then, an efficient network with smaller model size and computational complexity is trained using the generated data and the teacher network, simultaneously. Efficient student networks learned using the proposed Data-Free Learning (DAFL) method achieve 92.22\% and 74.47\% accuracies using ResNet-18 without any training data on the CIFAR-10 and CIFAR-100 datasets, respectively. Meanwhile, our student network obtains an 80.56\% accuracy on the CelebA benchmark.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Data-Free%20Learning%20of%20Student%20Networks-2019-ICCV.pdf}
}
@article{NastyAttacksJandialECCV2022, 
year = {2022}, 
title = {{Distilling the Undistillable: Learning from a Nasty Teacher}}, 
author = {Jandial, Surgan and Khasbage, Yash and Pal, Arghya and Balasubramanian, Vineeth N and Krishnamurthy, Balaji}, 
journal = {ECCV}, 
eprint = {2210.11728}, 
abstract = {{The inadvertent stealing of private/sensitive information using Knowledge Distillation (KD) has been getting significant attention recently and has guided subsequent defense efforts considering its critical nature. Recent work Nasty Teacher proposed to develop teachers which can not be distilled or imitated by models attacking it. However, the promise of confidentiality offered by a nasty teacher is not well studied, and as a further step to strengthen against such loopholes, we attempt to bypass its defense and steal (or extract) information in its presence successfully. Specifically, we analyze Nasty Teacher from two different directions and subsequently leverage them carefully to develop simple yet efficient methodologies, named as HTC and SCM, which increase the learning from Nasty Teacher by upto 68.63\% on standard datasets. Additionally, we also explore an improvised defense method based on our insights of stealing. Our detailed set of experiments and ablations on diverse models/settings demonstrate the efficacy of our approach.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Jandial-Distilling%20the%20Undistillable-%20Learning%20from%20a%20Nasty%20Teacher-2022-ECCV.pdf}
}
@article{IPWDNiuNeurIPS2022, 
year = {2022}, 
title = {{Respecting Transfer Gap in Knowledge Distillation}}, 
author = {Niu, Yulei and Chen, Long and Zhou, Chang and Zhang, Hanwang}, 
journal = {NeurIPS}, 
eprint = {2210.12787}, 
abstract = {{Knowledge distillation (KD) is essentially a process of transferring a teacher model's behavior, e.g., network response, to a student model. The network response serves as additional supervision to formulate the machine domain, which uses the data collected from the human domain as a transfer set. Traditional KD methods hold an underlying assumption that the data collected in both human domain and machine domain are both independent and identically distributed (IID). We point out that this naive assumption is unrealistic and there is indeed a transfer gap between the two domains. Although the gap offers the student model external knowledge from the machine domain, the imbalanced teacher knowledge would make us incorrectly estimate how much to transfer from teacher to student per sample on the non-IID transfer set. To tackle this challenge, we propose Inverse Probability Weighting Distillation (IPWD) that estimates the propensity score of a training sample belonging to the machine domain, and assigns its inverse amount to compensate for under-represented samples. Experiments on CIFAR-100 and ImageNet demonstrate the effectiveness of IPWD for both two-stage distillation and one-stage self-distillation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Niu-Respecting%20Transfer%20Gap%20in%20Knowledge%20Distillation-2022-NeurIPS.pdf}
}
@article{CouldLinNeurIPS2022, 
year = {2022}, 
title = {{Could Giant Pretrained Image Models Extract Universal Representations?}}, 
author = {Lin, Yutong and Liu, Ze and Zhang, Zheng and Hu, Han and Zheng, Nanning and Lin, Stephen and Cao, Yue}, 
journal = {NeurIPS}, 
eprint = {2211.02043}, 
abstract = {{Frozen pretrained models have become a viable alternative to the pretraining-then-finetuning paradigm for transfer learning. However, with frozen models there are relatively few parameters available for adapting to downstream tasks, which is problematic in computer vision where tasks vary significantly in input/output format and the type of information that is of value. In this paper, we present a study of frozen pretrained models when applied to diverse and representative computer vision tasks, including object detection, semantic segmentation and video action recognition. From this empirical analysis, our work answers the questions of what pretraining task fits best with this frozen setting, how to make the frozen setting more flexible to various downstream tasks, and the effect of larger model sizes. We additionally examine the upper bound of performance using a giant frozen pretrained model with 3 billion parameters (SwinV2-G) and find that it reaches competitive performance on a varied set of major benchmarks with only one shared frozen base network: 60.0 box mAP and 52.2 mask mAP on COCO object detection test-dev, 57.6 val mIoU on ADE20K semantic segmentation, and 81.7 top-1 accuracy on Kinetics-400 action recognition. With this work, we hope to bring greater attention to this promising path of freezing pretrained image models.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lin-Could%20Giant%20Pretrained%20Image%20Models%20Extract%20Universal%20Representations--2022-NeurIPS.pdf}
}
@article{SMCPHumbleECCV2022, 
year = {2022}, 
title = {{Soft Masking for Cost-Constrained Channel Pruning}}, 
author = {Humble, Ryan and Shen, Maying and Latorre, Jorge Albericio and Darve1, Eric and Alvarez, Jose M}, 
journal = {ECCV}, 
eprint = {2211.02206}, 
abstract = {{Structured channel pruning has been shown to significantly accelerate inference time for convolution neural networks (CNNs) on modern hardware, with a relatively minor loss of network accuracy. Recent works permanently zero these channels during training, which we observe to significantly hamper final accuracy, particularly as the fraction of the network being pruned increases. We propose Soft Masking for cost-constrained Channel Pruning (SMCP) to allow pruned channels to adaptively return to the network while simultaneously pruning towards a target cost constraint. By adding a soft mask re-parameterization of the weights and channel pruning from the perspective of removing input channels, we allow gradient updates to previously pruned channels and the opportunity for the channels to later return to the network. We then formulate input channel pruning as a global resource allocation problem. Our method outperforms prior works on both the ImageNet classification and PASCAL VOC detection datasets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Humble-Soft%20Masking%20for%20Cost-Constrained%20Channel%20Pruning-2022-ECCV.pdf}
}
@article{MOVEBielskiNeurIPS2022, 
year = {2022}, 
title = {{MOVE: Unsupervised Movable Object Segmentation and Detection}}, 
author = {Bielski, Adam and Favaro, Paolo}, 
journal = {NeurIPS}, 
eprint = {2210.07920}, 
abstract = {{We introduce MOVE, a novel method to segment objects without any form of supervision. MOVE exploits the fact that foreground objects can be shifted locally relative to their initial position and result in realistic (undistorted) new images. This property allows us to train a segmentation model on a dataset of images without annotation and to achieve state of the art (SotA) performance on several evaluation datasets for unsupervised salient object detection and segmentation. In unsupervised single object discovery, MOVE gives an average CorLoc improvement of 7.2\% over the SotA, and in unsupervised class-agnostic object detection it gives a relative AP improvement of 53\% on average. Our approach is built on top of self-supervised features (e.g. from DINO or MAE), an inpainting network (based on the Masked AutoEncoder) and adversarial training.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bielski-MOVE-%20Unsupervised%20Movable%20Object%20Segmentation%20and%20Detection-2022-NeurIPS.pdf}
}
@article{AFNetZhangNeurIPS2022, 
year = {2022}, 
title = {{Look More but Care Less in Video Recognition}}, 
author = {Zhang, Yitian and Bai, Yue and Wang, Huan and Xu, Yi and Fu, Yun}, 
journal = {NeurIPS}, 
eprint = {2211.09992}, 
abstract = {{Existing action recognition methods typically sample a few frames to represent each video to avoid the enormous computation, which often limits the recognition performance. To tackle this problem, we propose Ample and Focal Network (AFNet), which is composed of two branches to utilize more frames but with less computation. Specifically, the Ample Branch takes all input frames to obtain abundant information with condensed computation and provides the guidance for Focal Branch by the proposed Navigation Module; the Focal Branch squeezes the temporal size to only focus on the salient frames at each convolution block; in the end, the results of two branches are adaptively fused to prevent the loss of information. With this design, we can introduce more frames to the network but cost less computation. Besides, we demonstrate AFNet can utilize fewer frames while achieving higher accuracy as the dynamic selection in intermediate features enforces implicit temporal modeling. Further, we show that our method can be extended to reduce spatial redundancy with even less cost. Extensive experiments on five datasets demonstrate the effectiveness and efficiency of our method.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Look%20More%20but%20Care%20Less%20in%20Video%20Recognition-2022-NeurIPS.pdf}
}
@article{CALSLiuCVPR2023, 
year = {2023}, 
title = {{Class Adaptive Network Calibration}}, 
author = {Liu, Bingyuan and Rony, Jérôme and Galdran, Adrian and Dolz, Jose and Ayed, Ismail Ben}, 
journal = {CVPR}, 
eprint = {2211.15088}, 
abstract = {{Recent studies have revealed that, beyond conventional accuracy, calibration should also be considered for training modern deep neural networks. To address miscalibration during learning, some methods have explored different penalty functions as part of the learning objective, alongside a standard classification loss, with a hyper-parameter controlling the relative contribution of each term. Nevertheless, these methods share two major drawbacks: 1) the scalar balancing weight is the same for all classes, hindering the ability to address different intrinsic difficulties or imbalance among classes; and 2) the balancing weight is usually fixed without an adaptive strategy, which may prevent from reaching the best compromise between accuracy and calibration, and requires hyper-parameter search for each application. We propose Class Adaptive Label Smoothing (CALS) for calibrating deep networks, which allows to learn class-wise multipliers during training, yielding a powerful alternative to common label smoothing penalties. Our method builds on a general Augmented Lagrangian approach, a well-established technique in constrained optimization, but we introduce several modifications to tailor it for large-scale, class-adaptive training. Comprehensive evaluation and multiple comparisons on a variety of benchmarks, including standard and long-tailed image classification, semantic segmentation, and text classification, demonstrate the superiority of the proposed method. The code is available at https://github.com/by-liu/CALS.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-Class%20Adaptive%20Network%20Calibration-2022-arXiv.pdf}
}
@article{NVDRuizNeurIPS2022, 
year = {2022}, 
title = {{Finding Differences Between Transformers and ConvNets Using Counterfactual Simulation Testing}}, 
author = {Ruiz, Nataniel and Bargal, Sarah Adel and Xie, Cihang and Saenko, Kate and Sclaroff, Stan}, 
journal = {NeurIPS}, 
doi = {10.48550/arxiv.2211.16499}, 
eprint = {2211.16499}, 
abstract = {{Modern deep neural networks tend to be evaluated on static test sets. One shortcoming of this is the fact that these deep neural networks cannot be easily evaluated for robustness issues with respect to specific scene variations. For example, it is hard to study the robustness of these networks to variations of object scale, object pose, scene lighting and 3D occlusions. The main reason is that collecting real datasets with fine-grained naturalistic variations of sufficient scale can be extremely time-consuming and expensive. In this work, we present Counterfactual Simulation Testing, a counterfactual framework that allows us to study the robustness of neural networks with respect to some of these naturalistic variations by building realistic synthetic scenes that allow us to ask counterfactual questions to the models, ultimately providing answers to questions such as "Would your classification still be correct if the object were viewed from the top?" or "Would your classification still be correct if the object were partially occluded by another object?". Our method allows for a fair comparison of the robustness of recently released, state-of-the-art Convolutional Neural Networks and Vision Transformers, with respect to these naturalistic variations. We find evidence that ConvNext is more robust to pose and scale variations than Swin, that ConvNext generalizes better to our simulated domain and that Swin handles partial occlusion better than ConvNext. We also find that robustness for all networks improves with network scale and with data scale and variety. We release the Naturalistic Variation Object Dataset (NVD), a large simulated dataset of 272k images of everyday objects with naturalistic variations such as object pose, scale, viewpoint, lighting and occlusions. Project page: https://counterfactualsimulation.github.io}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ruiz-Finding%20Differences%20Between%20Transformers%20and%20ConvNets%20Using%20Counterfactual%20Simulation%20Testing-2022-NeurIPS.pdf}
}
@article{AttentionU-NetOktayMIDL2018, 
year = {2018}, 
title = {{Attention U-Net: Learning Where to Look for the Pancreas}}, 
author = {Oktay, Ozan and Schlemper, Jo and Folgoc, Loic Le and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Hammerla, Nils Y and Kainz, Bernhard and Glocker, Ben and Rueckert, Daniel}, 
journal = {MIDL}, 
eprint = {1804.03999}, 
abstract = {{We propose a novel attention gate (AG) model for medical imaging that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules of cascaded convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN architectures such as the U-Net model with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed Attention U-Net architecture is evaluated on two large CT abdominal datasets for multi-class image segmentation. Experimental results show that AGs consistently improve the prediction performance of U-Net across different datasets and training sizes while preserving computational efficiency. The code for the proposed architecture is publicly available.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Oktay-Attention%20U-Net-%20Learning%20Where%20to%20Look%20for%20the%20Pancreas-2018-MIDL.pdf}
}
@article{SelectiveScalingWangCVPR2023, 
year = {2023}, 
title = {{On Calibrating Semantic Segmentation Models: Analysis and An Algorithm}}, 
author = {Wang, Dongdong and Gong, Boqing and Wang, Liqiang}, 
journal = {CVPR}, 
eprint = {2212.12053}, 
abstract = {{We study the problem of semantic segmentation calibration. For image classification, lots of existing solutions are proposed to alleviate model miscalibration of confidence. However, to date, confidence calibration research on semantic segmentation is still limited. We provide a systematic study on the calibration of semantic segmentation models and propose a simple yet effective approach. First, we find that model capacity, crop size, multi-scale testing, and prediction correctness have impact on calibration. Among them, prediction correctness, especially misprediction, is more important to miscalibration due to over-confidence. Next, we propose a simple, unifying, and effective approach, namely selective scaling, by separating correct/incorrect prediction for scaling and more focusing on misprediction logit smoothing. Then, we study popular existing calibration methods and compare them with selective scaling on semantic segmentation calibration. We conduct extensive experiments with a variety of benchmarks on both in-domain and domain-shift calibration, and show that selective scaling consistently outperforms other methods.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-On%20Calibrating%20Semantic%20Segmentation%20Models-%20Analysis%20and%20An%20Algorithm-2022-arXiv.pdf}
}
@article{NeRFMildenhallECCV2020, 
year = {2020}, 
title = {{NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis}}, 
author = {Mildenhall, Ben and Srinivasan, Pratul P and Tancik, Matthew and Barron, Jonathan T and Ramamoorthi, Ravi and Ng, Ren}, 
journal = {ECCV}, 
eprint = {2003.08934}, 
abstract = {{We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$(\textbackslashtheta, \textbackslashphi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Mildenhall-NeRF-%20Representing%20Scenes%20as%20Neural%20Radiance%20Fields%20for%20View%20Synthesis-2020-ECCV.pdf}
}
@article{JMCCuiNeurocomputing2021, 
year = {2021}, 
title = {{Joint structured pruning and dense knowledge distillation for efficient transformer model compression}}, 
author = {Cui, Baiyun and Li, Yingming and Zhang, Zhongfei}, 
journal = {Neurocomputing}, 
issn = {0925-2312}, 
doi = {10.1016/j.neucom.2021.05.084}, 
abstract = {{In this paper, we develop a novel Joint Model Compression (referred to as JMC) method by combining structured pruning and dense knowledge distillation techniques to significantly compress original large language model into a deep compressed shallow network. In particular, a new Direct Importance-aware Structured Pruning (referred as DISP) approach is proposed to structurally prune the redundant structures in the Transformer networks directly based on the corresponding parameter matrices in the model. Besides, a Dense Knowledge Distillation (referred to as DKD) method is developed with a many-to-one layer mapping strategy to leverage more comprehensive layer-wise linguistic knowledge for the distillation. Further, the proposed structured pruning and dense knowledge distillation are integrated together to perform the joint compression, which enables us to achieve a significant compression without sacrificing model accuracy. The extensive experimental results across four NLP tasks on seven datasets demonstrate its effectiveness and superiority to the baselines, while maintaining similar performance to original large model with further remarkable benefits for inference-time speedup and memory efficiency.}}, 
pages = {56--69}, 
volume = {458}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Cui-Joint%20structured%20pruning%20and%20dense%20knowledge%20distillation%20for%20efficient%20transformer%20model%20compression-2021-Neurocomputing.pdf}
}
@article{PruneParkECCV2022, 
year = {2022}, 
title = {{Prune Your Model Before Distill It}}, 
author = {Park, Jinhyuk and No, Albert}, 
journal = {ECCV}, 
eprint = {2109.14960}, 
abstract = {{Knowledge distillation transfers the knowledge from a cumbersome teacher to a small student. Recent results suggest that the student-friendly teacher is more appropriate to distill since it provides more transferable knowledge. In this work, we propose the novel framework, "prune, then distill," that prunes the model first to make it more transferrable and then distill it to the student. We provide several exploratory examples where the pruned teacher teaches better than the original unpruned networks. We further show theoretically that the pruned teacher plays the role of regularizer in distillation, which reduces the generalization error. Based on this result, we propose a novel neural network compression scheme where the student network is formed based on the pruned teacher and then apply the "prune, then distill" strategy. The code is available at https://github.com/ososos888/prune-then-distill}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Park-Prune%20Your%20Model%20Before%20Distill%20It-2022-ECCV.pdf}
}
@article{LotteryTicketHypothesisFrankleICLR2019, 
year = {2019}, 
title = {{The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks}}, 
author = {Frankle, Jonathan and Carbin, Michael}, 
journal = {ICLR}, 
eprint = {1803.03635}, 
abstract = {{Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Frankle-The%20Lottery%20Ticket%20Hypothesis-%20Finding%20Sparse,%20Trainable%20Neural%20Networks-2019-ICLR.pdf}
}
@article{SparseBERTXuNAACL2021, 
year = {2021}, 
title = {{Rethinking Network Pruning -- under the Pre-train and Fine-tune Paradigm}}, 
author = {Xu, Dongkuan and Yen, Ian E H and Zhao, Jinxi and Xiao, Zhibin}, 
journal = {NAACL}, 
eprint = {2104.08682}, 
abstract = {{Transformer-based pre-trained language models have significantly improved the performance of various natural language processing (NLP) tasks in the recent years. While effective and prevalent, these models are usually prohibitively large for resource-limited deployment scenarios. A thread of research has thus been working on applying network pruning techniques under the pretrain-then-finetune paradigm widely adopted in NLP. However, the existing pruning results on benchmark transformers, such as BERT, are not as remarkable as the pruning results in the literature of convolutional neural networks (CNNs). In particular, common wisdom in pruning CNN states that sparse pruning technique compresses a model more than that obtained by reducing number of channels and layers (Elsen et al., 2020; Zhu and Gupta, 2017), while existing works on sparse pruning of BERT yields inferior results than its small-dense counterparts such as TinyBERT (Jiao et al., 2020). In this work, we aim to fill this gap by studying how knowledge are transferred and lost during the pre-train, fine-tune, and pruning process, and proposing a knowledge-aware sparse pruning process that achieves significantly superior results than existing literature. We show for the first time that sparse pruning compresses a BERT model significantly more than reducing its number of channels and layers. Experiments on multiple data sets of GLUE benchmark show that our method outperforms the leading competitors with a 20-times weight/FLOPs compression and neglectable loss in prediction accuracy.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xu-Rethinking%20Network%20Pruning%20--%20under%20the%20Pre-train%20and%20Fine-tune%20Paradigm-2021-NAACL.pdf}
}
@article{GANEnsemblingChaiCVPR2021, 
year = {2021}, 
title = {{Ensembling with Deep Generative Views}}, 
author = {Chai, Lucy and Zhu, Jun-Yan and Shechtman, Eli and Isola, Phillip and Zhang, Richard}, 
journal = {CVPR}, 
eprint = {2104.14551}, 
abstract = {{Recent generative models can synthesize "views" of artificial images that mimic real-world variations, such as changes in color or pose, simply by learning from unlabeled image collections. Here, we investigate whether such views can be applied to real images to benefit downstream analysis tasks such as image classification. Using a pretrained generator, we first find the latent code corresponding to a given real input image. Applying perturbations to the code creates natural variations of the image, which can then be ensembled together at test-time. We use StyleGAN2 as the source of generative augmentations and investigate this setup on classification tasks involving facial attributes, cat faces, and cars. Critically, we find that several design decisions are required towards making this process work; the perturbation procedure, weighting between the augmentations and original image, and training the classifier on synthesized images can all impact the result. Currently, we find that while test-time ensembling with GAN-based augmentations can offer some small improvements, the remaining bottlenecks are the efficiency and accuracy of the GAN reconstructions, coupled with classifier sensitivities to artifacts in GAN-generated images.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chai-Ensembling%20with%20Deep%20Generative%20Views-2021-CVPR.pdf}
}
@article{AttentionToScaleChenCVPR2016, 
year = {2016}, 
title = {{Attention to Scale: Scale-Aware Semantic Image Segmentation}}, 
author = {Chen, Liang-Chieh and Yang, Yi and Wang, Jiang and Xu, Wei and Yuille, Alan L.}, 
journal = {CVPR}, 
abstract = {{Incorporating multi-scale features in fully convolutional neural networks (FCNs) has been a key element to achieving state-of-the-art performance on semantic image segmentation. One common way to extract multi-scale features is to feed multiple resized input images to a shared deep network and then merge the resulting features for pixel-wise classification. In this work, we propose an attention mechanism that learns to softly weight the multi-scale features at each pixel location. We adapt a state-of-the-art semantic image segmentation model, which we jointly train with multi-scale input images and the attention model. The proposed attention model not only outperforms average-and max-pooling, but allows us to diagnostically visualize the importance of features at different positions and scales. Moreover, we show that adding extra supervision to the output at each scale is essential to achieving excellent performance when merging multi-scale features. We demonstrate the effectiveness of our model with extensive experiments on three challenging datasets, including PASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Attention%20to%20Scale-%20Scale-Aware%20Semantic%20Image%20Segmentation-2016-CVPR.pdf}
}
@article{DifferentiableCamarasaMIDL2022, 
year = {2022}, 
title = {{Differentiable Boundary Point Extraction for Weakly Supervised Star-shaped Object Segmentation}}, 
author = {Camarasa, Robin and Kervadec, Hoel and Bos, Daniel and Bruijne, Marleen de}, 
journal = {MIDL}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Camarasa-Differentiable%20Boundary%20Point%20Extraction%20for%20Weakly%20Supervised%20Star-shaped%20Object%20Segmentation-2022-MIDL.pdf}
}
@article{DATZhangUAI2022, 
year = {2022}, 
title = {{Distributed Adversarial Training to Robustify Deep Neural Networks at Scale}}, 
author = {Zhang, Gaoyuan and Lu, Songtao and Zhang, Yihua and Chen, Xiangyi and Chen, Pin-Yu and Fan, Quanfu and Martie, Lee and Horesh, Lior and Hong, Mingyi and Liu, Sijia}, 
journal = {UAI}, 
eprint = {2206.06257}, 
abstract = {{Current deep neural networks (DNNs) are vulnerable to adversarial attacks, where adversarial perturbations to the inputs can change or manipulate classification. To defend against such attacks, an effective and popular approach, known as adversarial training (AT), has been shown to mitigate the negative impact of adversarial attacks by virtue of a min-max robust training method. While effective, it remains unclear whether it can successfully be adapted to the distributed learning context. The power of distributed optimization over multiple machines enables us to scale up robust training over large models and datasets. Spurred by that, we propose distributed adversarial training (DAT), a large-batch adversarial training framework implemented over multiple machines. We show that DAT is general, which supports training over labeled and unlabeled data, multiple types of attack generation methods, and gradient compression operations favored for distributed optimization. Theoretically, we provide, under standard conditions in the optimization theory, the convergence rate of DAT to the first-order stationary points in general non-convex settings. Empirically, we demonstrate that DAT either matches or outperforms state-of-the-art robust accuracies and achieves a graceful training speedup (e.g., on ResNet-50 under ImageNet). Codes are available at https://github.com/dat-2022/dat.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Distributed%20Adversarial%20Training%20to%20Robustify%20Deep%20Neural%20Networks%20at%20Scale-2022-UAI.pdf}
}
@article{RP2EykholtCVPR2018, 
year = {2018}, 
title = {{Robust Physical-World Attacks on Deep Learning Visual Classification}}, 
author = {Eykholt, Kevin and Evtimov, Ivan and Fernandes, Earlence and Li, Bo and Rahmati, Amir and Xiao, Chaowei and Prakash, Atul and Kohno, Tadayoshi and Song, Dawn}, 
journal = {CVPR}, 
abstract = {{Recent studies show that the state-of-the-art deep neural networks (DNNs) are vulnerable to adversarial examples, resulting from small-magnitude perturbations added to the input. Given that that emerging physical systems are using DNNs in safety-critical situations, adversarial examples could mislead these systems and cause dangerous situations. Therefore, understanding adversarial examples in the physical world is an important step towards developing resilient learning algorithms. We propose a general attack algorithm, Robust Physical Perturbations (RP2), to generate robust visual adversarial perturbations under different physical conditions. Using the real-world case of road sign classification, we show that adversarial examples generated using RP2 achieve high targeted misclassification rates against standard-architecture road sign classifiers in the physical world under various environmental conditions, including viewpoints. Due to the current lack of a standardized testing method, we propose a two-stage evaluation methodology for robust physical adversarial examples consisting of lab and field tests. Using this methodology, we evaluate the efficacy of physical adversarial manipulations on real objects. With a perturbation in the form of only black and white stickers, we attack a real stop sign, causing targeted misclassification in 100\% of the images obtained in lab settings, and in 84.8\% of the captured video frames obtained on a moving vehicle (field test) for the target classifier.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Eykholt-Robust%20Physical-World%20Attacks%20on%20Deep%20Learning%20Visual%20Classification-2018-CVPR.pdf}
}
@article{PVDHouCVPR2022, 
year = {2022}, 
title = {{Point-to-Voxel Knowledge Distillation for LiDAR Semantic Segmentation}}, 
author = {Hou, Yuenan and Zhu, Xinge and Ma, Yuexin and Loy, Chen Change and Li, Yikang}, 
journal = {CVPR}, 
abstract = {{This article addresses the problem of distilling knowledge from a large teacher model to a slim student network for LiDAR semantic segmentation. Directly employing previous distillation approaches yields inferior results due to the intrinsic challenges of point cloud, i.e., sparsity, randomness and varying density. To tackle the aforementioned problems, we propose the Point-to-Voxel Knowledge Distillation (PVD), which transfers the hidden knowledge from both point level and voxel level. Specifically, we first leverage both the pointwise and voxelwise output distillation to complement the sparse supervision signals. Then, to better exploit the structural information, we divide the whole point cloud into several supervoxels and design a difficulty-aware sampling strategy to more frequently sample supervoxels containing less-frequent classes and faraway objects. On these supervoxels, we propose inter-point and inter-voxel affinity distillation, where the similarity information between points and voxels can help the student model better capture the structural information of the surrounding environment. We conduct extensive experiments on two popular LiDAR segmentation benchmarks, i.e., nuScenes [3] and SemanticKITTI [1]. On both benchmarks, our PVD-consistently outperforms previous distillation approaches by a large margin on three representative backbones, i.e., Cylinder3D [36], [37], SPVNAS [25] and MinkowskiNet [5]. Notably, on the challenging nuScenes and SemanticKITTI datasets, our method can achieve roughly 75\% MACs reduction and 2× speedup on the competitive Cylinder3D model and rank 1st on the SemanticKITTI leaderboard among all published algorithms11https://competitions.codalab.org/competitions/20331\#results (single-scan competition) till 2021-11-18 04:00 Pacific Time, and our method is termed Point-Voxel-KD. Our method (PV-KD) ranks 3rd on the multi-scan challenge till 2021-12-1 00:00 Pacific Time.. Our code is available at https://github.com/cardwing/Codes-for-PVKD. https://competitions.codalab.org/competitions/20331\#results (single-scan competition) till 2021-11-18 04:00 Pacific Time, and our method is termed Point-Voxel-KD. Our method (PV-KD) ranks 3rd on the multi-scan challenge till 2021-12-1 00:00 Pacific Time.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hou-Point-to-Voxel%20Knowledge%20Distillation%20for%20LiDAR%20Semantic%20Segmentation-2022-CVPR.pdf}
}
@article{BilinearCNNLinICCV2015, 
year = {2015}, 
title = {{Bilinear CNN Models for Fine-Grained Visual Recognition}}, 
author = {Lin, Tsung-Yu and RoyChowdhury, Aruni and Maji, Subhransu}, 
journal = {ICCV}, 
abstract = {{We propose bilinear models, a recognition architecture that consists of two feature extractors whose outputs are multiplied using outer product at each location of the image and pooled to obtain an image descriptor. This architecture can model local pairwise feature interactions in a translationally invariant manner which is particularly useful for fine-grained categorization. It also generalizes various orderless texture descriptors such as the Fisher vector, VLAD and 02P. We present experiments with bilinear models where the feature extractors are based on convolutional neural networks. The bilinear form simplifies gradient computation and allows end-to-end training of both networks using image labels only. Using networks initialized from the ImageNet dataset followed by domain specific fine-tuning we obtain 84.1\% accuracy of the CUB-200-2011 dataset requiring only category labels at training time. We present experiments and visualizations that analyze the effects of fine-tuning and the choice two networks on the speed and accuracy of the models. Results show that the architecture compares favorably to the existing state of the art on a number of fine-grained datasets while being substantially simpler and easier to train. Moreover, our most accurate model is fairly efficient running at 8 frames/sec on a NVIDIA Tesla K40 GPU. The source code for the complete system will be made available at http://vis-www.cs.umass.edu/bCNN}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lin-Bilinear%20CNN%20Models%20for%20Fine-Grained%20Visual%20Recognition-2015-ICCV.pdf}
}
@article{RefineNetLinCVPR2017, 
year = {2017}, 
title = {{RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation}}, 
author = {Lin, Guosheng and Milan, Anton and Shen, Chunhua and Reid, Ian}, 
journal = {CVPR}, 
eprint = {1611.06612}, 
abstract = {{Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense classification problems such as semantic segmentation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular, we achieve an intersection-over-union score of 83.4 on the challenging PASCAL VOC 2012 dataset, which is the best reported result to date.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lin-RefineNet-%20Multi-Path%20Refinement%20Networks%20for%20High-Resolution%20Semantic%20Segmentation-2017-CVPR.pdf}
}
@article{FromBurgesTechnicalReport2010, 
year = {2010}, 
title = {{From RankNet to LambdaRank to LambdaMART: An Overview}}, 
author = {Burges, Christopher J.C.}, 
journal = {Technical Report}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Burges-From%20RankNet%20to%20LambdaRank%20to%20LambdaMART-%20An%20Overview-2010-Technical%20Report.pdf}
}
@article{EvaluatingNestiWACV2022, 
year = {2022}, 
title = {{Evaluating the Robustness of Semantic Segmentation for Autonomous Driving against Real-World Adversarial Patch Attacks}}, 
author = {Nesti, Federico and Rossolini, Giulio and Nair, Saasha and Biondi, Alessandro and Buttazzo, Giorgio}, 
journal = {WACV}, 
eprint = {2108.06179}, 
abstract = {{Deep learning and convolutional neural networks allow achieving impressive performance in computer vision tasks, such as object detection and semantic segmentation (SS). However, recent studies have shown evident weaknesses of such models against adversarial perturbations. In a real-world scenario instead, like autonomous driving, more attention should be devoted to real-world adversarial examples (RWAEs), which are physical objects (e.g., billboards and printable patches) optimized to be adversarial to the entire perception pipeline. This paper presents an in-depth evaluation of the robustness of popular SS models by testing the effects of both digital and real-world adversarial patches. These patches are crafted with powerful attacks enriched with a novel loss function. Firstly, an investigation on the Cityscapes dataset is conducted by extending the Expectation Over Transformation (EOT) paradigm to cope with SS. Then, a novel attack optimization, called scene-specific attack, is proposed. Such an attack leverages the CARLA driving simulator to improve the transferability of the proposed EOT-based attack to a real 3D environment. Finally, a printed physical billboard containing an adversarial patch was tested in an outdoor driving scenario to assess the feasibility of the studied attacks in the real world. Exhaustive experiments revealed that the proposed attack formulations outperform previous work to craft both digital and real-world adversarial patches for SS. At the same time, the experimental results showed how these attacks are notably less effective in the real world, hence questioning the practical relevance of adversarial attacks to SS models for autonomous/assisted driving.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Nesti-Evaluating%20the%20Robustness%20of%20Semantic%20Segmentation%20for%20Autonomous%20Driving%20against%20Real-World%20Adversarial%20Patch%20Attacks-2022-WACV.pdf}
}
@article{CAPHeWACV2021, 
year = {2021}, 
title = {{CAP: Context-Aware Pruning for Semantic Segmentation}}, 
author = {He, Wei and Wu, Meiqing and Liang, Mingfu and Lam, Siew-Kei}, 
journal = {WACV}, 
abstract = {{Network pruning for deep convolutional neural networks (CNNs) has recently achieved notable research progress on image-level classification. However, most existing pruning methods are not catered to or evaluated on semantic segmentation networks. In this paper, we advocate the importance of contextual information during channel pruning for semantic segmentation networks by presenting a novel Context-aware Pruning framework. Concretely, we formulate the embedded contextual information by leveraging the layer-wise channels interdependency via the Context-aware Guiding Module (CAGM) and introduce the Context-aware Guided Sparsification (CAGS) to adaptively identify the informative channels on the cumbersome model by inducing channel-wise sparsity on the scaling factors in batch normalization (BN) layers. The resulting pruned models require significantly lesser operations for inference while maintaining comparable performance to (at times outperforming) the original models. We evaluated our framework on widely-used benchmarks and showed its effectiveness on both large and lightweight models. On Cityscapes dataset, our framework reduces the number of parameters by 32\%, 47\%, 54\%, and 63\%, on PSPNet101, PSPNet50, ICNet, and SegNet, respectively, while preserving the performance.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/He-CAP-%20Context-Aware%20Pruning%20for%20Semantic%20Segmentation-2021-WACV.pdf}
}
@article{RethinkingHuangNeurIPS2021, 
year = {2021}, 
title = {{Rethinking the Pruning Criteria for Convolutional Neural Network}}, 
author = {Huang, Zhongzhan and Shao, Wenqi and Wang, Xinjiang and Lin, Liang and Luo, Ping}, 
journal = {NeurIPS}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Huang-Rethinking%20the%20Pruning%20Criteria%20for%20Convolutional%20Neural%20Network-2021-NeurIPS.pdf}
}
@article{SupervisedContrastiveLearningKhoslaNeurIPS2020, 
year = {2020}, 
title = {{Supervised Contrastive Learning}}, 
author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip}, 
journal = {NeurIPS}, 
eprint = {2004.11362}, 
abstract = {{Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches subsume or significantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4\% on the ImageNet dataset, which is 0.8\% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows benefits for robustness to natural corruptions and is more stable to hyperparameter settings such as optimizers and data augmentations. Our loss function is simple to implement, and reference TensorFlow code is released at https://t.ly/supcon.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Khosla-Supervised%20Contrastive%20Learning-2020-NeurIPS.pdf}
}
@article{DistanceMetricLearningXingNeurIPS2002, 
year = {2002}, 
title = {{Distance Metric Learning, with Application to Clustering with Side-Information}}, 
author = {Xing, Eric P. and Ng, Andrew Y. and Jordan, Michael I. and Russell, Stuart}, 
journal = {NeurIPS}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xing-Distance%20Metric%20Learning,%20with%20Application%20to%20Clustering%20with%20Side-Information-2002-NeurIPS.pdf}
}
@article{OnTheRelationshipNarasimhanNeurIPS2013, 
year = {2013}, 
title = {{On the Relationship Between Binary Classification, Bipartite Ranking, and Binary Class Probability Estimation}}, 
author = {Narasimhan, Harikrishna and Agarwal, Shivani}, 
journal = {NeurIPS}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Narasimhan-On%20the%20Relationship%20Between%20Binary%20Classification,%20Bipartite%20Ranking,%20and%20Binary%20Class%20Probability%20Estimation-2013-NeurIPS.pdf}
}
@article{ConvLSTMShiNeurIPS2015, 
year = {2015}, 
title = {{Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting}}, 
author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and Woo, Wang-chun}, 
journal = {NeurIPS}, 
eprint = {1506.04214}, 
abstract = {{The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Shi-Convolutional%20LSTM%20Network-%20A%20Machine%20Learning%20Approach%20for%20Precipitation%20Nowcasting-2015-NeurIPS.pdf}
}
@article{WeaklySupervisedContrastiveLearningZhengICCV2021, 
year = {2021}, 
title = {{Weakly Supervised Contrastive Learning}}, 
author = {Zheng, Mingkai and Wang, Fei and You, Shan and Qian, Chen and Zhang, Changshui and Wang, Xiaogang and Xu, Chang}, 
journal = {ICCV}, 
eprint = {2110.04770}, 
abstract = {{Unsupervised visual representation learning has gained much attention from the computer vision community because of the recent achievement of contrastive learning. Most of the existing contrastive learning frameworks adopt the instance discrimination as the pretext task, which treating every single instance as a different class. However, such method will inevitably cause class collision problems, which hurts the quality of the learned representation. Motivated by this observation, we introduced a weakly supervised contrastive learning framework (WCL) to tackle this issue. Specifically, our proposed framework is based on two projection heads, one of which will perform the regular instance discrimination task. The other head will use a graph-based method to explore similar samples and generate a weak label, then perform a supervised contrastive learning task based on the weak label to pull the similar images closer. We further introduced a K-Nearest Neighbor based multi-crop strategy to expand the number of positive samples. Extensive experimental results demonstrate WCL improves the quality of self-supervised representations across different datasets. Notably, we get a new state-of-the-art result for semi-supervised learning. With only 1\textbackslash\% and 10\textbackslash\% labeled examples, WCL achieves 65\textbackslash\% and 72\textbackslash\% ImageNet Top-1 Accuracy using ResNet50, which is even higher than SimCLRv2 with ResNet101.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zheng-Weakly%20Supervised%20Contrastive%20Learning-2021-ICCV.pdf}
}
@article{OREPAHuCVPR2022, 
year = {2022}, 
title = {{Online Convolutional Re-parameterization}}, 
author = {Hu, Mu and Feng, Junyi and Hua, Jiashen and Lai, Baisheng and Huang, Jianqiang and Gong, Xiaojin and Hua, Xiansheng}, 
journal = {CVPR}, 
eprint = {2204.00826}, 
abstract = {{Structural re-parameterization has drawn increasing attention in various computer vision tasks. It aims at improving the performance of deep models without introducing any inference-time cost. Though efficient during inference, such models rely heavily on the complicated training-time blocks to achieve high accuracy, leading to large extra training cost. In this paper, we present online convolutional re-parameterization (OREPA), a two-stage pipeline, aiming to reduce the huge training overhead by squeezing the complex training-time block into a single convolution. To achieve this goal, we introduce a linear scaling layer for better optimizing the online blocks. Assisted with the reduced training cost, we also explore some more effective re-param components. Compared with the state-of-the-art re-param models, OREPA is able to save the training-time memory cost by about 70\% and accelerate the training speed by around 2x. Meanwhile, equipped with OREPA, the models outperform previous methods on ImageNet by up to +0.6\%.We also conduct experiments on object detection and semantic segmentation and show consistent improvements on the downstream tasks. Codes are available at https://github.com/JUGGHM/OREPA\_CVPR2022 .}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hu-Online%20Convolutional%20Re-parameterization-2022-CVPR.pdf}
}
@article{CFNNorooziECCV2016, 
year = {2016}, 
title = {{Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles}}, 
author = {Noroozi, Mehdi and Favaro, Paolo}, 
journal = {ECCV}, 
eprint = {1603.09246}, 
abstract = {{In this paper we study the problem of image representation learning without human annotation. By following the principles of self-supervision, we build a convolutional neural network (CNN) that can be trained to solve Jigsaw puzzles as a pretext task, which requires no manual labeling, and then later repurposed to solve object classification and detection. To maintain the compatibility across tasks we introduce the context-free network (CFN), a siamese-ennead CNN. The CFN takes image tiles as input and explicitly limits the receptive field (or context) of its early processing units to one tile at a time. We show that the CFN includes fewer parameters than AlexNet while preserving the same semantic learning capabilities. By training the CFN to solve Jigsaw puzzles, we learn both a feature mapping of object parts as well as their correct spatial arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. Our proposed method for learning visual representations outperforms state of the art methods in several transfer learning benchmarks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Noroozi-Unsupervised%20Learning%20of%20Visual%20Representations%20by%20Solving%20Jigsaw%20Puzzles-2016-ECCV.pdf}
}
@article{FourierFeaturesTancikNeurIPS2020, 
year = {2020}, 
title = {{Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains}}, 
author = {Tancik, Matthew and Srinivasan, Pratul P and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan T and Ng, Ren}, 
journal = {NeurIPS}, 
eprint = {2006.10739}, 
abstract = {{We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tancik-Fourier%20Features%20Let%20Networks%20Learn%20High%20Frequency%20Functions%20in%20Low%20Dimensional%20Domains-2020-NeurIPS.pdf}
}
@article{NTKJacotNeurIPS2018, 
year = {2018}, 
title = {{Neural Tangent Kernel: Convergence and Generalization in Neural Networks}}, 
author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clément}, 
journal = {NeurIPS}, 
eprint = {1806.07572}, 
abstract = {{At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function \$f\_\textbackslashtheta\$ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We prove the positive-definiteness of the limiting NTK when the data is supported on the sphere and the non-linearity is non-polynomial. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function \$f\_\textbackslashtheta\$ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Jacot-Neural%20Tangent%20Kernel-%20Convergence%20and%20Generalization%20in%20Neural%20Networks-2018-NeurIPS.pdf}
}
@article{IsOODFangNeurIPS2022, 
year = {2022}, 
title = {{Is Out-of-Distribution Detection Learnable?}}, 
author = {Fang, Zhen and Li, Yixuan and Lu, Jie and Dong, Jiahua and Han, Bo and Liu, Feng}, 
journal = {NeurIPS}, 
eprint = {2210.14707}, 
abstract = {{Supervised learning aims to train a classifier under the assumption that training and test data are from the same distribution. To ease the above assumption, researchers have studied a more realistic setting: out-of-distribution (OOD) detection, where test data may come from classes that are unknown during training (i.e., OOD data). Due to the unavailability and diversity of OOD data, good generalization ability is crucial for effective OOD detection algorithms. To study the generalization of OOD detection, in this paper, we investigate the probably approximately correct (PAC) learning theory of OOD detection, which is proposed by researchers as an open problem. First, we find a necessary condition for the learnability of OOD detection. Then, using this condition, we prove several impossibility theorems for the learnability of OOD detection under some scenarios. Although the impossibility theorems are frustrating, we find that some conditions of these impossibility theorems may not hold in some practical scenarios. Based on this observation, we next give several necessary and sufficient conditions to characterize the learnability of OOD detection in some practical scenarios. Lastly, we also offer theoretical supports for several representative OOD detection works based on our OOD theory.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Fang-Is%20Out-of-Distribution%20Detection%20Learnable--2022-NeurIPS.pdf}
}
@article{DBBDingCVPR2021, 
year = {2021}, 
title = {{Diverse Branch Block: Building a Convolution as an Inception-like Unit}}, 
author = {Ding, Xiaohan and Zhang, Xiangyu and Han, Jungong and Ding, Guiguang}, 
journal = {CVPR}, 
eprint = {2103.13425}, 
abstract = {{We propose a universal building block of Convolutional Neural Network (ConvNet) to improve the performance without any inference-time costs. The block is named Diverse Branch Block (DBB), which enhances the representational capacity of a single convolution by combining diverse branches of different scales and complexities to enrich the feature space, including sequences of convolutions, multi-scale convolutions, and average pooling. After training, a DBB can be equivalently converted into a single conv layer for deployment. Unlike the advancements of novel ConvNet architectures, DBB complicates the training-time microstructure while maintaining the macro architecture, so that it can be used as a drop-in replacement for regular conv layers of any architecture. In this way, the model can be trained to reach a higher level of performance and then transformed into the original inference-time structure for inference. DBB improves ConvNets on image classification (up to 1.9\% higher top-1 accuracy on ImageNet), object detection and semantic segmentation. The PyTorch code and models are released at https://github.com/DingXiaoH/DiverseBranchBlock.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ding-Diverse%20Branch%20Block-%20Building%20a%20Convolution%20as%20an%20Inception-like%20Unit-2021-CVPR.pdf}
}
@article{AOFPDingICML2019, 
year = {2019}, 
title = {{Approximated Oracle Filter Pruning for Destructive CNN Width Optimization}}, 
author = {Ding, Xiaohan and Ding, Guiguang and Guo, Yuchen and Han, Jungong and Yan, Chenggang}, 
journal = {ICML}, 
eprint = {1905.04748}, 
abstract = {{It is not easy to design and run Convolutional Neural Networks (CNNs) due to: 1) finding the optimal number of filters (i.e., the width) at each layer is tricky, given an architecture; and 2) the computational intensity of CNNs impedes the deployment on computationally limited devices. Oracle Pruning is designed to remove the unimportant filters from a well-trained CNN, which estimates the filters' importance by ablating them in turn and evaluating the model, thus delivers high accuracy but suffers from intolerable time complexity, and requires a given resulting width but cannot automatically find it. To address these problems, we propose Approximated Oracle Filter Pruning (AOFP), which keeps searching for the least important filters in a binary search manner, makes pruning attempts by masking out filters randomly, accumulates the resulting errors, and finetunes the model via a multi-path framework. As AOFP enables simultaneous pruning on multiple layers, we can prune an existing very deep CNN with acceptable time cost, negligible accuracy drop, and no heuristic knowledge, or re-design a model which exerts higher accuracy and faster inference.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ding-Approximated%20Oracle%20Filter%20Pruning%20for%20Destructive%20CNN%20Width%20Optimization-2019-ICML.pdf}
}
@article{C-SGDDingCVPR2019, 
year = {2019}, 
title = {{Centripetal SGD for Pruning Very Deep Convolutional Networks with Complicated Structure}}, 
author = {Ding, Xiaohan and Ding, Guiguang and Guo, Yuchen and Han, Jungong}, 
journal = {CVPR}, 
eprint = {1904.03837}, 
abstract = {{The redundancy is widely recognized in Convolutional Neural Networks (CNNs), which enables to remove unimportant filters from convolutional layers so as to slim the network with acceptable performance drop. Inspired by the linear and combinational properties of convolution, we seek to make some filters increasingly close and eventually identical for network slimming. To this end, we propose Centripetal SGD (C-SGD), a novel optimization method, which can train several filters to collapse into a single point in the parameter hyperspace. When the training is completed, the removal of the identical filters can trim the network with NO performance loss, thus no finetuning is needed. By doing so, we have partly solved an open problem of constrained filter pruning on CNNs with complicated structure, where some layers must be pruned following others. Our experimental results on CIFAR-10 and ImageNet have justified the effectiveness of C-SGD-based filter pruning. Moreover, we have provided empirical evidences for the assumption that the redundancy in deep neural networks helps the convergence of training by showing that a redundant CNN trained using C-SGD outperforms a normally trained counterpart with the equivalent width.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ding-Centripetal%20SGD%20for%20Pruning%20Very%20Deep%20Convolutional%20Networks%20with%20Complicated%20Structure-2019-CVPR.pdf}
}
@article{PruningLiICLR2017, 
year = {2017}, 
title = {{Pruning Filters for Efficient ConvNets}}, 
author = {Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter}, 
journal = {ICLR}, 
eprint = {1608.08710}, 
abstract = {{The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34\% and ResNet-110 by up to 38\% on CIFAR10 while regaining close to the original accuracy by retraining the networks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Pruning%20Filters%20for%20Efficient%20ConvNets-2017-ICLR.pdf}
}
@article{ResRepDingICCV2021, 
year = {2021}, 
title = {{ResRep: Lossless CNN Pruning via Decoupling Remembering and Forgetting}}, 
author = {Ding, Xiaohan and Hao, Tianxiang and Tan, Jianchao and Liu, Ji and Han, Jungong and Guo, Yuchen and Ding, Guiguang}, 
journal = {ICCV}, 
eprint = {2007.03260}, 
abstract = {{We propose ResRep, a novel method for lossless channel pruning (a.k.a. filter pruning), which slims down a CNN by reducing the width (number of output channels) of convolutional layers. Inspired by the neurobiology research about the independence of remembering and forgetting, we propose to re-parameterize a CNN into the remembering parts and forgetting parts, where the former learn to maintain the performance and the latter learn to prune. Via training with regular SGD on the former but a novel update rule with penalty gradients on the latter, we realize structured sparsity. Then we equivalently merge the remembering and forgetting parts into the original architecture with narrower layers. In this sense, ResRep can be viewed as a successful application of Structural Re-parameterization. Such a methodology distinguishes ResRep from the traditional learning-based pruning paradigm that applies a penalty on parameters to produce sparsity, which may suppress the parameters essential for the remembering. ResRep slims down a standard ResNet-50 with 76.15\% accuracy on ImageNet to a narrower one with only 45\% FLOPs and no accuracy drop, which is the first to achieve lossless pruning with such a high compression ratio. The code and models are at https://github.com/DingXiaoH/ResRep.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ding-ResRep-%20Lossless%20CNN%20Pruning%20via%20Decoupling%20Remembering%20and%20Forgetting-2021-ICCV.pdf}
}
@article{W-MSEErmolovICML2021, 
year = {2021}, 
title = {{Whitening for Self-Supervised Representation Learning}}, 
author = {Ermolov, Aleksandr and Siarohin, Aliaksandr and Sangineto, Enver and Sebe, Nicu}, 
journal = {ICML}, 
eprint = {2007.06346}, 
abstract = {{Most of the current self-supervised representation learning (SSL) methods are based on the contrastive loss and the instance-discrimination task, where augmented versions of the same image instance ("positives") are contrasted with instances extracted from other images ("negatives"). For the learning to be effective, many negatives should be compared with a positive pair, which is computationally demanding. In this paper, we propose a different direction and a new loss function for SSL, which is based on the whitening of the latent-space features. The whitening operation has a "scattering" effect on the batch samples, avoiding degenerate solutions where all the sample representations collapse to a single point. Our solution does not require asymmetric networks and it is conceptually simple. Moreover, since negatives are not needed, we can extract multiple positive pairs from the same image instance. The source code of the method and of all the experiments is available at: https://github.com/htdt/self-supervised.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ermolov-Whitening%20for%20Self-Supervised%20Representation%20Learning-2021-ICML.pdf}
}
@article{EfficientFormerLiNeurIPS2022, 
year = {2022}, 
title = {{EfficientFormer: Vision Transformers at MobileNet Speed}}, 
author = {Li, Yanyu and Yuan, Geng and Wen, Yang and Hu, Ju and Evangelidis, Georgios and Tulyakov, Sergey and Wang, Yanzhi and Ren, Jian}, 
journal = {NeurIPS}, 
eprint = {2206.01191}, 
abstract = {{Vision Transformers (ViT) have shown rapid progress in computer vision tasks, achieving promising results on various benchmarks. However, due to the massive number of parameters and model design, \textbackslashtextit\{e.g.\}, attention mechanism, ViT-based models are generally times slower than lightweight convolutional networks. Therefore, the deployment of ViT for real-time applications is particularly challenging, especially on resource-constrained hardware such as mobile devices. Recent efforts try to reduce the computation complexity of ViT through network architecture search or hybrid design with MobileNet block, yet the inference speed is still unsatisfactory. This leads to an important question: can transformers run as fast as MobileNet while obtaining high performance? To answer this, we first revisit the network architecture and operators used in ViT-based models and identify inefficient designs. Then we introduce a dimension-consistent pure transformer (without MobileNet blocks) as a design paradigm. Finally, we perform latency-driven slimming to get a series of final models dubbed EfficientFormer. Extensive experiments show the superiority of EfficientFormer in performance and speed on mobile devices. Our fastest model, EfficientFormer-L1, achieves \$79.2\textbackslash\%\$ top-1 accuracy on ImageNet-1K with only \$1.6\$ ms inference latency on iPhone 12 (compiled with CoreML), which runs as fast as MobileNetV2\$\textbackslashtimes 1.4\$ (\$1.6\$ ms, \$74.7\textbackslash\%\$ top-1), and our largest model, EfficientFormer-L7, obtains \$83.3\textbackslash\%\$ accuracy with only \$7.0\$ ms latency. Our work proves that properly designed transformers can reach extremely low latency on mobile devices while maintaining high performance.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-EfficientFormer-%20Vision%20Transformers%20at%20MobileNet%20Speed-2022-NeurIPS.pdf}
}
@article{PNALYeICCV2021, 
year = {2021}, 
title = {{Learning with Noisy Labels for Robust Point Cloud Segmentation}}, 
author = {Ye, Shuquan and Chen, Dongdong and Han, Songfang and Liao, Jing}, 
journal = {ICCV}, 
doi = {10.48550/arxiv.2107.14230}, 
eprint = {2107.14230}, 
abstract = {{Point cloud segmentation is a fundamental task in 3D. Despite recent progress on point cloud segmentation with the power of deep networks, current deep learning methods based on the clean label assumptions may fail with noisy labels. Yet, object class labels are often mislabeled in real-world point cloud datasets. In this work, we take the lead in solving this issue by proposing a novel Point Noise-Adaptive Learning (PNAL) framework. Compared to existing noise-robust methods on image tasks, our PNAL is noise-rate blind, to cope with the spatially variant noise rate problem specific to point clouds. Specifically, we propose a novel point-wise confidence selection to obtain reliable labels based on the historical predictions of each point. A novel cluster-wise label correction is proposed with a voting strategy to generate the best possible label taking the neighbor point correlations into consideration. We conduct extensive experiments to demonstrate the effectiveness of PNAL on both synthetic and real-world noisy datasets. In particular, even with \$60\textbackslash\%\$ symmetric noisy labels, our proposed method produces much better results than its baseline counterpart without PNAL and is comparable to the ideal upper bound trained on a completely clean dataset. Moreover, we fully re-labeled the validation set of a popular but noisy real-world scene dataset ScanNetV2 to make it clean, for rigorous experiment and future research. Our code and data are available at \textbackslashurl\{https://shuquanye.com/PNAL\_website/\}.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ye-Learning%20with%20Noisy%20Labels%20for%20Robust%20Point%20Cloud%20Segmentation-2021-ICCV.pdf}
}
@article{LeViTGrahamICCV2021, 
year = {2021}, 
title = {{LeViT: a Vision Transformer in ConvNet’s Clothing for Faster Inference}}, 
author = {Graham, Ben and El-Nouby, Alaaeldin and Touvron, Hugo and Stock, Pierre and Joulin, Armand and Jégou, Hervé and Douze, Matthijs}, 
journal = {ICCV}, 
abstract = {{We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers. As a result, we propose LeViT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80\% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at https://github.com/facebookresearch/LeViT.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Graham-LeViT-%20a%20Vision%20Transformer%20in%20ConvNet’s%20Clothing%20for%20Faster%20Inference-2021-ICCV.pdf}
}
@article{Mobile-FormerChenCVPR2022, 
year = {2022}, 
title = {{Mobile-Former: Bridging MobileNet and Transformer}}, 
author = {Chen, Yinpeng and Dai, Xiyang and Chen, Dongdong and Liu, Mengchen and Dong, Xiaoyi and Yuan, Lu and Liu, Zicheng}, 
journal = {CVPR}, 
eprint = {2108.05895}, 
abstract = {{We present Mobile-Former, a parallel design of MobileNet and transformer with a two-way bridge in between. This structure leverages the advantages of MobileNet at local processing and transformer at global interaction. And the bridge enables bidirectional fusion of local and global features. Different from recent works on vision transformer, the transformer in Mobile-Former contains very few tokens (e.g. 6 or fewer tokens) that are randomly initialized to learn global priors, resulting in low computational cost. Combining with the proposed light-weight cross attention to model the bridge, Mobile-Former is not only computationally efficient, but also has more representation power. It outperforms MobileNetV3 at low FLOP regime from 25M to 500M FLOPs on ImageNet classification. For instance, Mobile-Former achieves 77.9\textbackslash\% top-1 accuracy at 294M FLOPs, gaining 1.3\textbackslash\% over MobileNetV3 but saving 17\textbackslash\% of computations. When transferring to object detection, Mobile-Former outperforms MobileNetV3 by 8.6 AP in RetinaNet framework. Furthermore, we build an efficient end-to-end detector by replacing backbone, encoder and decoder in DETR with Mobile-Former, which outperforms DETR by 1.1 AP but saves 52\textbackslash\% of computational cost and 36\textbackslash\% of parameters.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Mobile-Former-%20Bridging%20MobileNet%20and%20Transformer-2022-CVPR.pdf}
}
@article{ViT-SlimChavanCVPR2022, 
year = {2022}, 
title = {{Vision Transformer Slimming: Multi-Dimension Searching in Continuous Optimization Space}}, 
author = {Chavan, Arnav and Shen, Zhiqiang and Liu, Zhuang and Liu, Zechun and Cheng, Kwang-Ting and Xing, Eric}, 
journal = {CVPR}, 
abstract = {{This paper explores the feasibility of finding an optimal sub-model from a vision transformer and introduces a pure vision transformer slimming (ViT-Slim) framework. It can search a sub-structure from the original model end-to-end across multiple dimensions, including the input tokens, MHSA and MLP modules with state-of-the-art performance. Our method is based on a learnable and unified ℓ1sparsity constraint with pre-defined factors to reflect the global importance in the continuous searching space of different dimensions. The searching process is highly efficient through a single-shot training scheme. For instance, on DeiT-S, ViT-Slim only takes \textbackslashtextasciitilde43 GPU hours for the searching process, and the searched structure is flexible with diverse dimensionalities in different modules. Then, a budget threshold is employed according to the requirements of accuracy-FLOPs trade-off on running devices, and a retraining process is performed to obtain the final model. The extensive experiments show that our ViT-Slim can compress up to 40\% of parameters and 40\% FLOPs on various vision transformers while increasing the accuracy by \textbackslashtextasciitilde0.6\% on ImageNet. We also demonstrate the advantage of our searched models on several downstream datasets. Our code is available at https://github.com/Arnav0400/ViT-Slim.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chavan-Vision%20Transformer%20Slimming-%20Multi-Dimension%20Searching%20in%20Continuous%20Optimization%20Space-2022-CVPR.pdf}
}
@article{DynamicReLUChenECCV2020, 
year = {2020}, 
title = {{Dynamic ReLU}}, 
author = {Chen, Yinpeng and Dai, Xiyang and Liu, Mengchen and Chen, Dongdong and Yuan, Lu and Liu, Zicheng}, 
journal = {ECCV}, 
eprint = {2003.10027}, 
abstract = {{Rectified linear units (ReLU) are commonly used in deep neural networks. So far ReLU and its generalizations (non-parametric or parametric) are static, performing identically for all input samples. In this paper, we propose dynamic ReLU (DY-ReLU), a dynamic rectifier of which parameters are generated by a hyper function over all in-put elements. The key insight is that DY-ReLU encodes the global context into the hyper function, and adapts the piecewise linear activation function accordingly. Compared to its static counterpart, DY-ReLU has negligible extra computational cost, but significantly more representation capability, especially for light-weight neural networks. By simply using DY-ReLU for MobileNetV2, the top-1 accuracy on ImageNet classification is boosted from 72.0\% to 76.2\% with only 5\% additional FLOPs.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Dynamic%20ReLU-2020-ECCV.pdf}
}
@article{MaxoutGoodfellowICML2013, 
year = {2013}, 
title = {{Maxout Networks}}, 
author = {Goodfellow, Ian J and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua}, 
journal = {ICML}, 
eprint = {1302.4389}, 
abstract = {{We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Goodfellow-Maxout%20Networks-2013-ICML.pdf}
}
@article{BISKDLiuMICCAI2022, 
year = {2022}, 
title = {{Efficient Biomedical Instance Segmentation via Knowledge Distillation}}, 
author = {Liu, Xiaoyu and Hu, Bo and Huang, Wei and Zhang, Yueyi and Xiong, Zhiwei}, 
journal = {MICCAI}, 
abstract = {{Biomedical instance segmentation is vulnerable to complicated instance morphology, resulting in over-merge and over-segmentation. Recent advanced methods apply convolutional neural networks to predict pixel embeddings to overcome this problem. However, these methods suffer from heavy computational burdens and massive storage. In this paper, we present the first knowledge distillation method tailored for biomedical instance segmentation to transfer the knowledge from a cumbersome teacher network to a lightweight student one. Different from existing distillation methods on other tasks, we consider three kinds of essential knowledge of the instance segmentation task, i.e., instance-level features, instance relationships in the feature space and pixel-level instance boundaries. Specifically, we devise two distillation schemes: (i) instance graph distillation that transfers the knowledge of instance-level features and instance relationships by the instance graphs built from embeddings of the teacher-student pair, respectively, and (ii) pixel affinity distillation that converts pixel embeddings into pixel affinities and explicitly transfers the structured knowledge of instance boundaries encoded in affinities. Experimental results on a 3D electron microscopy dataset (CREMI) and a 2D plant phenotype dataset (CVPPP) demonstrate that the student models trained through our distillation method use fewer than 1\% parameters and less than 10\% inference time while achieving promising performance compared with corresponding teacher models. Code is available at https://github.com/liuxy1103/BISKD.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-Efficient%20Biomedical%20Instance%20Segmentation%20via%20Knowledge%20Distillation-2022-MICCAI.pdf}
}
@article{DeSDYeMICCAI2022, 
year = {2022}, 
title = {{DeSD: Self-Supervised Learning with Deep Self-Distillation for 3D Medical Image Segmentation}}, 
author = {Ye, Yiwen and Zhang, Jianpeng and Chen, Ziyang and Xia, Yong}, 
journal = {MICCAI}, 
abstract = {{Self-supervised learning (SSL), enabling advanced performance with few annotations, has demonstrated a proven successful in medical image segmentation. Usually, SSL relies on measuring the similarity of features obtained at the deepest layer to attract the features of positive pairs or repulse the features of negative pairs, and then may suffer from the weak supervision at shallow layers. To address this issue, we reformulate SSL in a Deep Self-Distillation (DeSD) manner to improve the representation quality of both shallow and deep layers. Specifically, the DeSD model is composed of an online student network and a momentum teacher network, both being stacked by multiple sub-encoders. The features produced by each sub-encoder in the student network are trained to match the features produced by the teacher network. Such a deep self-distillation supervision is able to improve the representation quality of all sub-encoders, including both shallow ones and deep ones. We pre-train the DeSD model on a large-scale unlabeled dataset and evaluate it on seven downstream segmentation tasks. Our results indicate that the proposed DeSD model achieves superior pre-training performance over existing SSL methods, setting the new state of the art. The code is available at https://github.com/yeerwen/DeSD.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ye-DeSD-%20Self-Supervised%20Learning%20with%20Deep%20Self-Distillation%20for%203D%20Medical%20Image%20Segmentation-2022-MICCAI.pdf}
}
@article{LiTSBilicMIA2023, 
year = {2023}, 
title = {{The Liver Tumor Segmentation Benchmark (LiTS)}}, 
author = {Bilic, Patrick and others}, 
journal = {MIA}, 
abstract = {{In this work, we report the set-up and results of the Liver Tumor Segmentation Benchmark (LITS) organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI) 2016 and International Conference On Medical Image Computing Computer Assisted Intervention (MICCAI) 2017. Twenty four valid state-of-the-art liver and liver tumor segmentation algorithms were applied to a set of 131 computed tomography (CT) volumes with different types of tumor contrast levels (hyper-/hypo-intense), abnormalities in tissues (metastasectomie) size and varying amount of lesions. The submitted algorithms have been tested on 70 undisclosed volumes. The dataset is created in collaboration with seven hospitals and research institutions and manually reviewed by independent three radiologists. We found that not a single algorithm performed best for liver and tumors. The best liver segmentation algorithm achieved a Dice score of 0.96(MICCAI) whereas for tumor segmentation the best algorithm evaluated at 0.67(ISBI) and 0.70(MICCAI). The LITS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bilic-The%20Liver%20Tumor%20Segmentation%20Benchmark%20(LiTS)-2023-MIA.pdf}
}
@article{KiTSHellerMIA2021, 
year = {2021}, 
title = {{The state of the art in kidney and kidney tumor segmentation in contrast-enhanced CT imaging: Results of the KiTS19 challenge}}, 
author = {Heller, Nicholas and others}, 
journal = {MIA}, 
abstract = {{There is a large body of literature linking anatomic and geometric characteristics of kidney tumors to perioperative and oncologic outcomes. Semantic segmentation of these tumors and their host kidneys is a promising tool for quantitatively characterizing these lesions, but its adoption is limited due to the manual effort required to produce high-quality 3D segmentations of these structures. Recently, methods based on deep learning have shown excellent results in automatic 3D segmentation, but they require large datasets for training, and there remains little consensus on which methods perform best. The 2019 Kidney and Kidney Tumor Segmentation challenge (KiTS19) was a competition held in conjunction with the 2019 International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) which sought to address these issues and stimulate progress on this automatic segmentation problem. A training set of 210 cross sectional CT images with kidney tumors was publicly released with corresponding semantic segmentation masks. 106 teams from five continents used this data to develop automated systems to predict the true segmentation masks on a test set of 90 CT images for which the corresponding ground truth segmentations were kept private. These predictions were scored and ranked according to their average Sørensen-Dice coefficient between the kidney and tumor across all 90 cases. The winning team achieved a Dice of 0.974 for kidney and 0.851 for tumor, approaching the inter-annotator performance on kidney (0.983) but falling short on tumor (0.923). This challenge has now entered an “open leaderboard” phase where it serves as a challenging benchmark in 3D semantic segmentation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Heller-The%20state%20of%20the%20art%20in%20kidney%20and%20kidney%20tumor%20segmentation%20in%20contrast-enhanced%20CT%20imaging-%20Results%20of%20the%20KiTS19%20challenge-2021-MIA.pdf}
}
@article{UNETRHatamizadehWACV2022, 
year = {2022}, 
title = {{UNETR: Transformers for 3D Medical Image Segmentation}}, 
author = {Hatamizadeh, Ali and Tang, Yucheng and Nath, Vishwesh and Yang, Dong and Myronenko, Andriy and Landman, Bennett and Roth, Holger and Xu, Daguang}, 
journal = {WACV}, 
eprint = {2103.10504}, 
abstract = {{Fully Convolutional Neural Networks (FCNNs) with contracting and expanding paths have shown prominence for the majority of medical image segmentation applications since the past decade. In FCNNs, the encoder plays an integral role by learning both global and local features and contextual representations which can be utilized for semantic output prediction by the decoder. Despite their success, the locality of convolutional layers in FCNNs, limits the capability of learning long-range spatial dependencies. Inspired by the recent success of transformers for Natural Language Processing (NLP) in long-range sequence learning, we reformulate the task of volumetric (3D) medical image segmentation as a sequence-to-sequence prediction problem. We introduce a novel architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information, while also following the successful "U-shaped" network design for the encoder and decoder. The transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the final semantic segmentation output. We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation tasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV leaderboard. Code: https://monai.io/research/unetr}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hatamizadeh-UNETR-%20Transformers%20for%203D%20Medical%20Image%20Segmentation-2022-WACV.pdf}
}
@article{DBNHuangCVPR2018, 
year = {2018}, 
title = {{Decorrelated Batch Normalization}}, 
author = {Huang, Lei and Yang, Dawei and Lang, Bo and Deng, Jia}, 
journal = {CVPR}, 
eprint = {1804.08450}, 
abstract = {{Batch Normalization (BN) is capable of accelerating the training of deep models by centering and scaling activations within mini-batches. In this work, we propose Decorrelated Batch Normalization (DBN), which not just centers and scales activations but whitens them. We explore multiple whitening techniques, and find that PCA whitening causes a problem we call stochastic axis swapping, which is detrimental to learning. We show that ZCA whitening does not suffer from this problem, permitting successful learning. DBN retains the desirable qualities of BN and further improves BN's optimization efficiency and generalization ability. We design comprehensive experiments to show that DBN can improve the performance of BN on multilayer perceptrons and convolutional neural networks. Furthermore, we consistently improve the accuracy of residual networks on CIFAR-10, CIFAR-100, and ImageNet.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Huang-Decorrelated%20Batch%20Normalization-2018-CVPR.pdf}
}
@article{On-DeviceTrainingLinNeurIPS2022, 
year = {2022}, 
title = {{On-Device Training Under 256KB Memory}}, 
author = {Lin, Ji and Zhu, Ligeng and Chen, Wei-Ming and Wang, Wei-Chen and Gan, Chuang and Han, Song}, 
journal = {NeurIPS}, 
eprint = {2206.15472}, 
abstract = {{On-device training enables the model to adapt to new data collected from the sensors by fine-tuning a pre-trained model. Users can benefit from customized AI models without having to transfer the data to the cloud, protecting the privacy. However, the training memory consumption is prohibitive for IoT devices that have tiny memory resources. We propose an algorithm-system co-design framework to make on-device training possible with only 256KB of memory. On-device training faces two unique challenges: (1) the quantized graphs of neural networks are hard to optimize due to low bit-precision and the lack of normalization; (2) the limited hardware resource does not allow full back-propagation. To cope with the optimization difficulty, we propose Quantization-Aware Scaling to calibrate the gradient scales and stabilize 8-bit quantized training. To reduce the memory footprint, we propose Sparse Update to skip the gradient computation of less important layers and sub-tensors. The algorithm innovation is implemented by a lightweight training system, Tiny Training Engine, which prunes the backward computation graph to support sparse updates and offload the runtime auto-differentiation to compile time. Our framework is the first solution to enable tiny on-device training of convolutional neural networks under 256KB SRAM and 1MB Flash without auxiliary memory, using less than 1/1000 of the memory of PyTorch and TensorFlow while matching the accuracy on tinyML application VWW. Our study enables IoT devices not only to perform inference but also to continuously adapt to new data for on-device lifelong learning. A video demo can be found here: https://youtu.be/XaDCO8YtmBw.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lin-On-Device%20Training%20Under%20256KB%20Memory-2022-NeurIPS.pdf}
}
@article{DCNV2ZhuCVPR2019, 
year = {2019}, 
title = {{Deformable ConvNets v2: More Deformable, Better Results}}, 
author = {Zhu, Xizhou and Hu, Han and Lin, Stephen and Dai, Jifeng}, 
journal = {CVPR}, 
eprint = {1811.11168}, 
abstract = {{The superior performance of Deformable Convolutional Networks arises from its ability to adapt to the geometric variations of objects. Through an examination of its adaptive behavior, we observe that while the spatial support for its neural features conforms more closely than regular ConvNets to object structure, this support may nevertheless extend well beyond the region of interest, causing features to be influenced by irrelevant image content. To address this problem, we present a reformulation of Deformable ConvNets that improves its ability to focus on pertinent image regions, through increased modeling power and stronger training. The modeling power is enhanced through a more comprehensive integration of deformable convolution within the network, and by introducing a modulation mechanism that expands the scope of deformation modeling. To effectively harness this enriched modeling capability, we guide network training via a proposed feature mimicking scheme that helps the network to learn features that reflect the object focus and classification power of R-CNN features. With the proposed contributions, this new version of Deformable ConvNets yields significant performance gains over the original model and produces leading results on the COCO benchmark for object detection and instance segmentation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhu-Deformable%20ConvNets%20v2-%20More%20Deformable,%20Better%20Results-2019-CVPR.pdf}
}
@article{InternImageWangCVPR2023, 
year = {2023}, 
title = {{InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions}}, 
author = {Wang, Wenhai and Dai, Jifeng and Chen, Zhe and Huang, Zhenhang and Li, Zhiqi and Zhu, Xizhou and Hu, Xiaowei and Lu, Tong and Lu, Lewei and Li, Hongsheng and Wang, Xiaogang and Qiao, Yu}, 
journal = {CVPR}, 
eprint = {2211.05778}, 
abstract = {{Compared to the great progress of large-scale vision transformers (ViTs) in recent years, large-scale models based on convolutional neural networks (CNNs) are still in an early state. This work presents a new large-scale CNN-based foundation model, termed InternImage, which can obtain the gain from increasing parameters and training data like ViTs. Different from the recent CNNs that focus on large dense kernels, InternImage takes deformable convolution as the core operator, so that our model not only has the large effective receptive field required for downstream tasks such as detection and segmentation, but also has the adaptive spatial aggregation conditioned by input and task information. As a result, the proposed InternImage reduces the strict inductive bias of traditional CNNs and makes it possible to learn stronger and more robust patterns with large-scale parameters from massive data like ViTs. The effectiveness of our model is proven on challenging benchmarks including ImageNet, COCO, and ADE20K. It is worth mentioning that InternImage-H achieved a new record 65.4 mAP on COCO test-dev and 62.9 mIoU on ADE20K, outperforming current leading CNNs and ViTs. The code will be released at https://github.com/OpenGVLab/InternImage.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-InternImage-%20Exploring%20Large-Scale%20Vision%20Foundation%20Models%20with%20Deformable%20Convolutions-2022-arXiv.pdf}
}
@article{BNextGuoarXiv2022, 
year = {2022}, 
title = {{Join the High Accuracy Club on ImageNet with A Binary Neural Network Ticket}}, 
author = {Guo, Nianhui and Bethge, Joseph and Meinel, Christoph and Yang, Haojin}, 
journal = {arXiv}, 
eprint = {2211.12933}, 
abstract = {{Binary neural networks are the extreme case of network quantization, which has long been thought of as a potential edge machine learning solution. However, the significant accuracy gap to the full-precision counterparts restricts their creative potential for mobile applications. In this work, we revisit the potential of binary neural networks and focus on a compelling but unanswered problem: how can a binary neural network achieve the crucial accuracy level (e.g., 80\%) on ILSVRC-2012 ImageNet? We achieve this goal by enhancing the optimization process from three complementary perspectives: (1) We design a novel binary architecture BNext based on a comprehensive study of binary architectures and their optimization process. (2) We propose a novel knowledge-distillation technique to alleviate the counter-intuitive overfitting problem observed when attempting to train extremely accurate binary models. (3) We analyze the data augmentation pipeline for binary networks and modernize it with up-to-date techniques from full-precision models. The evaluation results on ImageNet show that BNext, for the first time, pushes the binary model accuracy boundary to 80.57\% and significantly outperforms all the existing binary networks. Code and trained models are available at: https://github.com/hpi-xnor/BNext.git.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Guo-Join%20the%20High%20Accuracy%20Club%20on%20ImageNet%20with%20A%20Binary%20Neural%20Network%20Ticket-2022-arXiv.pdf}
}
@article{WSBDNKimIJCV2022, 
year = {2022}, 
title = {{Learning to Detect Semantic Boundaries with Image-level Class Labels}}, 
author = {Kim, Namyup and Hwang, Sehyun and Kwak, Suha}, 
journal = {IJCV}, 
eprint = {2212.07579}, 
abstract = {{This paper presents the first attempt to learn semantic boundary detection using image-level class labels as supervision. Our method starts by estimating coarse areas of object classes through attentions drawn by an image classification network. Since boundaries will locate somewhere between such areas of different classes, our task is formulated as a multiple instance learning (MIL) problem, where pixels on a line segment connecting areas of two different classes are regarded as a bag of boundary candidates. Moreover, we design a new neural network architecture that can learn to estimate semantic boundaries reliably even with uncertain supervision given by the MIL strategy. Our network is used to generate pseudo semantic boundary labels of training images, which are in turn used to train fully supervised models. The final model trained with our pseudo labels achieves an outstanding performance on the SBD dataset, where it is as competitive as some of previous arts trained with stronger supervision.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kim-Learning%20to%20Detect%20Semantic%20Boundaries%20with%20Image-level%20Class%20Labels-2022-IJCV.pdf}
}
@article{AffinityNetAhnCVPR2018, 
year = {2018}, 
title = {{Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation}}, 
author = {Ahn, Jiwoon and Kwak, Suha}, 
journal = {CVPR}, 
eprint = {1803.10464}, 
abstract = {{The deficiency of segmentation labels is one of the main obstacles to semantic segmentation in the wild. To alleviate this issue, we present a novel framework that generates segmentation labels of images given their image-level class labels. In this weakly supervised setting, trained models have been known to segment local discriminative parts rather than the entire object area. Our solution is to propagate such local responses to nearby areas which belong to the same semantic entity. To this end, we propose a Deep Neural Network (DNN) called AffinityNet that predicts semantic affinity between a pair of adjacent image coordinates. The semantic propagation is then realized by random walk with the affinities predicted by AffinityNet. More importantly, the supervision employed to train AffinityNet is given by the initial discriminative part segmentation, which is incomplete as a segmentation annotation but sufficient for learning semantic affinities within small image areas. Thus the entire framework relies only on image-level class labels and does not require any extra data or annotations. On the PASCAL VOC 2012 dataset, a DNN learned with segmentation labels generated by our method outperforms previous models trained with the same level of supervision, and is even as competitive as those relying on stronger supervision.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ahn-Learning%20Pixel-level%20Semantic%20Affinity%20with%20Image-level%20Supervision%20for%20Weakly%20Supervised%20Semantic%20Segmentation-2018-CVPR.pdf}
}
@article{MaximallyMoultonICDMWorkshop2018, 
year = {2018}, 
title = {{Maximally Consistent Sampling and the Jaccard Index of Probability Distributions}}, 
author = {Moulton, Ryan and Jiang, Yunjiang}, 
journal = {ICDM Workshop}, 
eprint = {1809.04052}, 
abstract = {{We introduce simple, efficient algorithms for computing a MinHash of a probability distribution, suitable for both sparse and dense data, with equivalent running times to the state of the art for both cases. The collision probability of these algorithms is a new measure of the similarity of positive vectors which we investigate in detail. We describe the sense in which this collision probability is optimal for any Locality Sensitive Hash based on sampling. We argue that this similarity measure is more useful for probability distributions than the similarity pursued by other algorithms for weighted MinHash, and is the natural generalization of the Jaccard index.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Moulton-Maximally%20Consistent%20Sampling%20and%20the%20Jaccard%20Index%20of%20Probability%20Distributions-2018-ICDM%20Workshop.pdf}
}
@article{ContinuationKDJafariEMNLPFindings2022, 
year = {2022}, 
title = {{Continuation KD: Improved Knowledge Distillation through the Lens of Continuation Optimization}}, 
author = {Jafari, Aref and Kobyzev, Ivan and Rezagholizadeh, Mehdi and Poupart, Pascal and Ghodsi, Ali}, 
journal = {EMNLP Findings}, 
eprint = {2212.05998}, 
abstract = {{Knowledge Distillation (KD) has been extensively used for natural language understanding (NLU) tasks to improve a small model's (a student) generalization by transferring the knowledge from a larger model (a teacher). Although KD methods achieve state-of-the-art performance in numerous settings, they suffer from several problems limiting their performance. It is shown in the literature that the capacity gap between the teacher and the student networks can make KD ineffective. Additionally, existing KD techniques do not mitigate the noise in the teacher's output: modeling the noisy behaviour of the teacher can distract the student from learning more useful features. We propose a new KD method that addresses these problems and facilitates the training compared to previous techniques. Inspired by continuation optimization, we design a training procedure that optimizes the highly non-convex KD objective by starting with the smoothed version of this objective and making it more complex as the training proceeds. Our method (Continuation-KD) achieves state-of-the-art performance across various compact architectures on NLU (GLUE benchmark) and computer vision tasks (CIFAR-10 and CIFAR-100).}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Jafari-Continuation%20KD-%20Improved%20Knowledge%20Distillation%20through%20the%20Lens%20of%20Continuation%20Optimization-2022-EMNLP%20Findings.pdf}
}
@article{LoLSamAAAI2023, 
year = {2023}, 
title = {{Losses over Labels: Weakly Supervised Learning via Direct Loss Construction}}, 
author = {Sam, Dylan and Kolter, J Zico}, 
journal = {AAAI}, 
eprint = {2212.06921}, 
abstract = {{Owing to the prohibitive costs of generating large amounts of labeled data, programmatic weak supervision is a growing paradigm within machine learning. In this setting, users design heuristics that provide noisy labels for subsets of the data. These weak labels are combined (typically via a graphical model) to form pseudolabels, which are then used to train a downstream model. In this work, we question a foundational premise of the typical weakly supervised learning pipeline: given that the heuristic provides all ``label" information, why do we need to generate pseudolabels at all? Instead, we propose to directly transform the heuristics themselves into corresponding loss functions that penalize differences between our model and the heuristic. By constructing losses directly from the heuristics, we can incorporate more information than is used in the standard weakly supervised pipeline, such as how the heuristics make their decisions, which explicitly informs feature selection during training. We call our method Losses over Labels (LoL) as it creates losses directly from heuristics without going through the intermediate step of a label. We show that LoL improves upon existing weak supervision methods on several benchmark text and image classification tasks and further demonstrate that incorporating gradient information leads to better performance on almost every task.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Sam-Losses%20over%20Labels-%20Weakly%20Supervised%20Learning%20via%20Direct%20Loss%20Construction-2023-AAAI.pdf}
}
@article{MedISegZhangarXiv2022, 
year = {2022}, 
title = {{Deep Learning for Medical Image Segmentation: Tricks, Challenges and Future Directions}}, 
author = {Zhang, Dong and Lin, Yi and Chen, Hao and Tian, Zhuotao and Yang, Xin and Tang, Jinhui and Cheng, Kwang Ting}, 
journal = {arXiv}, 
eprint = {2209.10307}, 
abstract = {{Over the past few years, the rapid development of deep learning technologies for computer vision has greatly promoted the performance of medical image segmentation (MedISeg). However, the recent MedISeg publications usually focus on presentations of the major contributions (e.g., network architectures, training strategies, and loss functions) while unwittingly ignoring some marginal implementation details (also known as "tricks"), leading to a potential problem of the unfair experimental result comparisons. In this paper, we collect a series of MedISeg tricks for different model implementation phases (i.e., pre-training model, data pre-processing, data augmentation, model implementation, model inference, and result post-processing), and experimentally explore the effectiveness of these tricks on the consistent baseline models. Compared to paper-driven surveys that only blandly focus on the advantages and limitation analyses of segmentation models, our work provides a large number of solid experiments and is more technically operable. With the extensive experimental results on both the representative 2D and 3D medical image datasets, we explicitly clarify the effect of these tricks. Moreover, based on the surveyed tricks, we also open-sourced a strong MedISeg repository, where each of its components has the advantage of plug-and-play. We believe that this milestone work not only completes a comprehensive and complementary survey of the state-of-the-art MedISeg approaches, but also offers a practical guide for addressing the future medical image processing challenges including but not limited to small dataset learning, class imbalance learning, multi-modality learning, and domain adaptation. The code has been released at: https://github.com/hust-linyi/MedISeg}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Deep%20Learning%20for%20Medical%20Image%20Segmentation-%20Tricks,%20Challenges%20and%20Future%20Directions-2022-arXiv.pdf}
}
@article{AdaiXieICML2022, 
year = {2022}, 
title = {{Adaptive Inertia: Disentangling the Effects of Adaptive Learning Rate and Momentum}}, 
author = {Xie, Zeke and Wang, Xinrui and Zhang, Huishuai and Sato, Issei and Sugiyama, Masashi}, 
journal = {ICML}, 
eprint = {2006.15815}, 
abstract = {{Adaptive Moment Estimation (Adam), which combines Adaptive Learning Rate and Momentum, would be the most popular stochastic optimizer for accelerating the training of deep neural networks. However, it is empirically known that Adam often generalizes worse than Stochastic Gradient Descent (SGD). The purpose of this paper is to unveil the mystery of this behavior in the diffusion theoretical framework. Specifically, we disentangle the effects of Adaptive Learning Rate and Momentum of the Adam dynamics on saddle-point escaping and flat minima selection. We prove that Adaptive Learning Rate can escape saddle points efficiently, but cannot select flat minima as SGD does. In contrast, Momentum provides a drift effect to help the training process pass through saddle points, and almost does not affect flat minima selection. This partly explains why SGD (with Momentum) generalizes better, while Adam generalizes worse but converges faster. Furthermore, motivated by the analysis, we design a novel adaptive optimization framework named Adaptive Inertia, which uses parameter-wise adaptive inertia to accelerate the training and provably favors flat minima as well as SGD. Our extensive experiments demonstrate that the proposed adaptive inertia method can generalize significantly better than SGD and conventional adaptive gradient methods.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xie-Adaptive%20Inertia-%20Disentangling%20the%20Effects%20of%20Adaptive%20Learning%20Rate%20and%20Momentum-2022-ICML.pdf}
}
@article{JMLsWangNeurIPS2023, 
year = {2023}, 
title = {{Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels}}, 
author = {Wang, Zifu and Ning, Xuefei and Blaschko, Matthew B.}, 
journal = {NeurIPS}
}
@article{RelaxedGrageraTCS2018, 
year = {2018}, 
title = {{Relaxed triangle inequality ratio of the Sørensen–Dice and Tversky indexes}}, 
author = {Gragera, Alonso and Suppakitpaisarn, Vorapong}, 
journal = {TCS}, 
abstract = {{ In this work, we calculate a tight relaxed triangle inequality ratio for some of the most well-known indexes used in finding dissimilarities between two finite sets known as the Sørensen–Dice and Tversky indexes. This relaxed triangle inequality ratio affects efficiency and approximation ratios of recent algorithms for many combinatorial problems such as traveling salesman and nearest neighbor search. Because of that, there are many works providing ratios for several other indexes. In this work, we focus on the Tversky index, which is a generalization of many dissimilarity indexes commonly used in practice. We provide the tight ratio of the Tversky index in this paper. Because the Sørensen–Dice index is a special case of the Tversky index, we know from the results that the tight ratio for the Sørensen–Dice index is equal to 1.5.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Gragera-Relaxed%20triangle%20inequality%20ratio%20of%20the%20Sørensen–Dice%20and%20Tversky%20indexes-2018-TCS.pdf}
}
@article{AutoLoss-ZeroLiCVPR2022, 
year = {2022}, 
title = {{AutoLoss-Zero: Searching Loss Functions from Scratch for Generic Tasks}}, 
author = {Li, Hao and Fu, Tianwen and Dai, Jifeng and Li, Hongsheng and Huang, Gao and Zhu, Xizhou}, 
journal = {CVPR}, 
eprint = {2103.14026}, 
abstract = {{Significant progress has been achieved in automating the design of various components in deep networks. However, the automatic design of loss functions for generic tasks with various evaluation metrics remains under-investigated. Previous works on handcrafting loss functions heavily rely on human expertise, which limits their extendibility. Meanwhile, existing efforts on searching loss functions mainly focus on specific tasks and particular metrics, with task-specific heuristics. Whether such works can be extended to generic tasks is not verified and questionable. In this paper, we propose AutoLoss-Zero, the first general framework for searching loss functions from scratch for generic tasks. Specifically, we design an elementary search space composed only of primitive mathematical operators to accommodate the heterogeneous tasks and evaluation metrics. A variant of the evolutionary algorithm is employed to discover loss functions in the elementary search space. A loss-rejection protocol and a gradient-equivalence-check strategy are developed so as to improve the search efficiency, which are applicable to generic tasks. Extensive experiments on various computer vision tasks demonstrate that our searched loss functions are on par with or superior to existing loss functions, which generalize well to different datasets and networks. Code shall be released.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-AutoLoss-Zero-%20Searching%20Loss%20Functions%20from%20Scratch%20for%20Generic%20Tasks-2022-CVPR.pdf}
}
@article{ImprovedIoffeICDM2010, 
year = {2010}, 
title = {{Improved Consistent Sampling, Weighted Minhash and L1 Sketching}}, 
author = {Ioffe, Sergey}, 
journal = {ICDM}, 
abstract = {{We propose a new Consistent Weighted Sampling method, where the probability of drawing identical samples for a pair of inputs is equal to their Jaccard similarity. Our method takes deterministic constant time per non-zero weight, improving on the best previous approach which takes expected constant time. The samples can be used as Weighted Minhash for efficient retrieval and compression (sketching) under Jaccard or L1 metric. A method is presented for using simple data statistics to reduce the running time of hash computation by two orders of magnitude. We compare our method with the random projection method and show that it has better characteristics for retrieval under L1. We present a novel method of mapping hashes to short bit-strings, apply it to Weighted Minhash, and achieve more accurate distance estimates from sketches than existing methods, as long as the inputs are sufficiently distinct. We show how to choose the optimal number of bits per hash for sketching, and demonstrate experimental results which agree with the theoretical analysis.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ioffe-Improved%20Consistent%20Sampling,%20Weighted%20Minhash%20and%20L1%20Sketching-2010-ICDM.pdf}
}
@article{JIoUFengTITS2022, 
year = {2022}, 
title = {{Labels are Not Perfect: Inferring Spatial Uncertainty in Object Detection}}, 
author = {Feng, Di and Wang, Zining and Zhou, Yiyang and Rosenbaum, Lars and Timm, Fabian and Dietmayer, Klaus and Tomizuka, Masayoshi and Zhan, Wei}, 
journal = {TITS}, 
abstract = {{The availability of many real-world driving datasets is a key reason behind the recent progress of object detection algorithms in autonomous driving. However, there exist ambiguity or even failures in object labels due to error-prone annotation process or sensor observation noise. Current public object detection datasets only provide deterministic object labels without considering their inherent uncertainty, as does the common training process or evaluation metrics for object detectors. As a result, an in-depth evaluation among different object detection methods remains challenging, and the training process of object detectors is sub-optimal, especially in probabilistic object detection. In this work, we infer the uncertainty in bounding box labels from LiDAR point clouds based on a generative model, and define a new representation of the probabilistic bounding box through a spatial uncertainty distribution. Comprehensive experiments show that the proposed model reflects complex environmental noises in LiDAR perception and the label quality. Furthermore, we propose Jaccard IoU (JIoU) as a new evaluation metric that extends IoU by incorporating label uncertainty. We conduct an in-depth comparison among several LiDAR-based object detectors using the JIoU metric. Finally, we incorporate the proposed label uncertainty in a loss function to train a probabilistic object detector and to improve its detection accuracy. We verify our proposed methods on two public datasets (KITTI, Waymo), as well as on simulation data. Code is released at https://github.com/ ZiningWang/Inferring-Spatial-Uncertainty-in-Object-Detection.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Feng-Labels%20are%20Not%20Perfect-%20Inferring%20Spatial%20Uncertainty%20in%20Object%20Detection-2022-TITS.pdf}
}
@article{CoLabLiTMI2022, 
year = {2022}, 
title = {{Context Label Learning: Improving Background Class Representations in Semantic Segmentation}}, 
author = {Li, Zeju and Kamnitsas, Konstantinos and Ouyang, Cheng and Chen, Chen and Glocker, Ben}, 
journal = {TMI}, 
eprint = {2212.08423}, 
abstract = {{Background samples provide key contextual information for segmenting regions of interest (ROIs). However, they always cover a diverse set of structures, causing difficulties for the segmentation model to learn good decision boundaries with high sensitivity and precision. The issue concerns the highly heterogeneous nature of the background class, resulting in multi-modal distributions. Empirically, we find that neural networks trained with heterogeneous background struggle to map the corresponding contextual samples to compact clusters in feature space. As a result, the distribution over background logit activations may shift across the decision boundary, leading to systematic over-segmentation across different datasets and tasks. In this study, we propose context label learning (CoLab) to improve the context representations by decomposing the background class into several subclasses. Specifically, we train an auxiliary network as a task generator, along with the primary segmentation model, to automatically generate context labels that positively affect the ROI segmentation accuracy. Extensive experiments are conducted on several challenging segmentation tasks and datasets. The results demonstrate that CoLab can guide the segmentation model to map the logits of background samples away from the decision boundary, resulting in significantly improved segmentation accuracy. Code is available.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Context%20Label%20Learning-%20Improving%20Background%20Class%20Representations%20in%20Semantic%20Segmentation-2022-TMI.pdf}
}
@article{GradientDescentChandraNeurIPS2022, 
year = {2022}, 
title = {{Gradient Descent: The Ultimate Optimizer}}, 
author = {Chandra, Kartik and Xie, Audrey and Ragan-Kelley, Jonathan and Meijer, Erik}, 
journal = {NeurIPS}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chandra-Gradient%20Descent-%20The%20Ultimate%20Optimizer-2022-NeurIPS.pdf}
}
@article{MapillaryVistasNeuholdICCV2017, 
year = {2017}, 
title = {{The Mapillary Vistas Dataset for Semantic Understanding of Street Scenes}}, 
author = {Neuhold, Gerhard and Ollmann, Tobias and Bulò, Samuel Rota and Kontschieder, Peter}, 
journal = {ICCV}, 
abstract = {{The Mapillary Vistas Dataset is a novel, large-scale street-level image dataset containing 25 000 high-resolution images annotated into 66 object categories with additional, instance-specific labels for 37 classes. Annotation is performed in a dense and fine- grained style by using polygons for delineating individual objects. Our dataset is \$5\textbackslashtimes\$ larger than the total amount of fine annotations for c ityscapes and contains images from all around the world, captured at various conditions regarding weather, season and daytime. Images come from different imaging devices (mobile phones, tablets, action cameras, professional capturing rigs) and differently experienced photographers. In such a way, our dataset has been designed and compiled to cover diversity, richness of detail and geographic extent. As default benchmark tasks, we define semantic image segmentation and instance-specific image segmentation, aiming to significantly further the development of state-of-the-art methods for visual road-scene understanding.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Neuhold-The%20Mapillary%20Vistas%20Dataset%20for%20Semantic%20Understanding%20of%20Street%20Scenes-2017-ICCV.pdf}
}
@article{BARLemaireCVPR2019, 
year = {2019}, 
title = {{Structured Pruning of Neural Networks with Budget-Aware Regularization}}, 
author = {Lemaire, Carl and Achkar, Andrew and Jodoin, Pierre-Marc}, 
journal = {CVPR}, 
eprint = {1811.09332}, 
abstract = {{Pruning methods have shown to be effective at reducing the size of deep neural networks while keeping accuracy almost intact. Among the most effective methods are those that prune a network while training it with a sparsity prior loss and learnable dropout parameters. A shortcoming of these approaches however is that neither the size nor the inference speed of the pruned network can be controlled directly; yet this is a key feature for targeting deployment of CNNs on low-power hardware. To overcome this, we introduce a budgeted regularized pruning framework for deep CNNs. Our approach naturally fits into traditional neural network training as it consists of a learnable masking layer, a novel budget-aware objective function, and the use of knowledge distillation. We also provide insights on how to prune a residual network and how this can lead to new architectures. Experimental results reveal that CNNs pruned with our method are more accurate and less compute-hungry than state-of-the-art methods. Also, our approach is more effective at preventing accuracy collapse in case of severe pruning; this allows us to attain pruning factors up to 16x without significant accuracy drop.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lemaire-Structured%20Pruning%20of%20Neural%20Networks%20with%20Budget-Aware%20Regularization-2019-CVPR.pdf}
}
@article{ComplementaryLabelsIshidaNeurIPS2017, 
year = {2017}, 
title = {{Learning from Complementary Labels}}, 
author = {Ishida, Takashi and Niu, Gang and Hu, Weihua and Sugiyama, Masashi}, 
journal = {NeurIPS}, 
eprint = {1705.07541}, 
abstract = {{Collecting labeled data is costly and thus a critical bottleneck in real-world classification tasks. To mitigate this problem, we propose a novel setting, namely learning from complementary labels for multi-class classification. A complementary label specifies a class that a pattern does not belong to. Collecting complementary labels would be less laborious than collecting ordinary labels, since users do not have to carefully choose the correct class from a long list of candidate classes. However, complementary labels are less informative than ordinary labels and thus a suitable approach is needed to better learn from them. In this paper, we show that an unbiased estimator to the classification risk can be obtained only from complementarily labeled data, if a loss function satisfies a particular symmetric condition. We derive estimation error bounds for the proposed method and prove that the optimal parametric convergence rate is achieved. We further show that learning from complementary labels can be easily combined with learning from ordinary labels (i.e., ordinary supervised learning), providing a highly practical implementation of the proposed method. Finally, we experimentally demonstrate the usefulness of the proposed methods.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ishida-Learning%20from%20Complementary%20Labels-2017-NeurIPS.pdf}
}
@article{PixelRNNOordICML2016, 
year = {2016}, 
title = {{Pixel Recurrent Neural Networks}}, 
author = {Oord, Aaron van den and Kalchbrenner, Nal and Kavukcuoglu, Koray}, 
journal = {ICML}, 
eprint = {1601.06759}, 
abstract = {{Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Oord-Pixel%20Recurrent%20Neural%20Networks-2016-ICML.pdf}
}
@article{DoRechtICML2019, 
year = {2019}, 
title = {{Do ImageNet Classifiers Generalize to ImageNet?}}, 
author = {Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal}, 
journal = {ICML}, 
eprint = {1902.10811}, 
abstract = {{We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy drops of 3\% - 15\% on CIFAR-10 and 11\% - 14\% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models' inability to generalize to slightly "harder" images than those found in the original test sets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Recht-Do%20ImageNet%20Classifiers%20Generalize%20to%20ImageNet--2019-ICML.pdf}
}
@article{ContextEncodersPathakCVPR2016, 
year = {2016}, 
title = {{Context Encoders: Feature Learning by Inpainting}}, 
author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A}, 
journal = {CVPR}, 
abstract = {{We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Pathak-Context%20Encoders-%20Feature%20Learning%20by%20Inpainting-2016-CVPR.pdf}
}
@article{DeepFaceRecognitionParkhiBMVC2015, 
year = {2015}, 
title = {{Deep Face Recognition}}, 
author = {Parkhi, Omkar M and Vedaldi, Andrea and Zisserman, Andrew}, 
journal = {BMVC}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Parkhi-Deep%20Face%20Recognition-2015-BMVC.pdf}
}
@article{ReformerKitaevICLR2020, 
year = {2020}, 
title = {{Reformer: The Efficient Transformer}}, 
author = {Kitaev, Nikita and Kaiser, Łukasz and Levskaya, Anselm}, 
journal = {ICLR}, 
eprint = {2001.04451}, 
abstract = {{Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(\$L\textasciicircum2\$) to O(\$L\textbackslashlog L\$), where \$L\$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of \$N\$ times, where \$N\$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kitaev-Reformer-%20The%20Efficient%20Transformer-2020-ICLR.pdf}
}
@article{TowardsPrivacy-PreservingWuECCV2018, 
year = {2018}, 
title = {{Towards Privacy-Preserving Visual Recognition via Adversarial Training: A Pilot Study}}, 
author = {Wu, Zhenyu and Wang, Zhangyang and Wang, Zhaowen and Jin, Hailin}, 
journal = {ECCV}, 
eprint = {1807.08379}, 
abstract = {{This paper aims to improve privacy-preserving visual recognition, an increasingly demanded feature in smart camera applications, by formulating a unique adversarial training framework. The proposed framework explicitly learns a degradation transform for the original video inputs, in order to optimize the trade-off between target task performance and the associated privacy budgets on the degraded video. A notable challenge is that the privacy budget, often defined and measured in task-driven contexts, cannot be reliably indicated using any single model performance, because a strong protection of privacy has to sustain against any possible model that tries to hack privacy information. Such an uncommon situation has motivated us to propose two strategies, i.e., budget model restarting and ensemble, to enhance the generalization of the learned degradation on protecting privacy against unseen hacker models. Novel training strategies, evaluation protocols, and result visualization methods have been designed accordingly. Two experiments on privacy-preserving action recognition, with privacy budgets defined in various ways, manifest the compelling effectiveness of the proposed framework in simultaneously maintaining high target task (action recognition) performance while suppressing the privacy breach risk.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wu-Towards%20Privacy-Preserving%20Visual%20Recognition%20via%20Adversarial%20Training-%20A%20Pilot%20Study-2018-ECCV.pdf}
}
@article{DeepInversionYinCVPR2020, 
year = {2020}, 
title = {{Dreaming to Distill: Data-free Knowledge Transfer via DeepInversion}}, 
author = {Yin, Hongxu and Molchanov, Pavlo and Li, Zhizhong and Alvarez, Jose M and Mallya, Arun and Hoiem, Derek and Jha, Niraj K and Kautz, Jan}, 
journal = {CVPR}, 
eprint = {1912.08795}, 
abstract = {{We introduce DeepInversion, a new method for synthesizing images from the image distribution used to train a deep neural network. We 'invert' a trained network (teacher) to synthesize class-conditional input images starting from random noise, without using any additional information about the training dataset. Keeping the teacher fixed, our method optimizes the input while regularizing the distribution of intermediate feature maps using information stored in the batch normalization layers of the teacher. Further, we improve the diversity of synthesized images using Adaptive DeepInversion, which maximizes the Jensen-Shannon divergence between the teacher and student network logits. The resulting synthesized images from networks trained on the CIFAR-10 and ImageNet datasets demonstrate high fidelity and degree of realism, and help enable a new breed of data-free applications - ones that do not require any real images or labeled data. We demonstrate the applicability of our proposed method to three tasks of immense practical importance -- (i) data-free network pruning, (ii) data-free knowledge transfer, and (iii) data-free continual learning. Code is available at https://github.com/NVlabs/DeepInversion}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yin-Dreaming%20to%20Distill-%20Data-free%20Knowledge%20Transfer%20via%20DeepInversion-2020-CVPR.pdf}
}
@article{CycleGANZhuICCV2017, 
year = {2017}, 
title = {{Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks}}, 
author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.}, 
journal = {ICCV}, 
abstract = {{Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G:X\textbackslashrightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F:Y\textbackslashrightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X))\textbackslashapprox X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhu-Unpaired%20Image-to-Image%20Translation%20Using%20Cycle-Consistent%20Adversarial%20Networks-2017-ICCV.pdf}
}
@article{RNSungCVPR2018, 
year = {2018}, 
title = {{Learning to Compare: Relation Network for Few-Shot Learning}}, 
author = {Sung, Flood and Yang, Yongxin and Zhang, Li and Xiang, Tao and Torr, Philip H S and Hospedales, Timothy M}, 
journal = {CVPR}, 
eprint = {1711.06025}, 
abstract = {{We present a conceptually simple, flexible, and general framework for few-shot learning, where a classifier must learn to recognise new classes given only few examples from each. Our method, called the Relation Network (RN), is trained end-to-end from scratch. During meta-learning, it learns to learn a deep distance metric to compare a small number of images within episodes, each of which is designed to simulate the few-shot setting. Once trained, a RN is able to classify images of new classes by computing relation scores between query images and the few examples of each new class without further updating the network. Besides providing improved performance on few-shot learning, our framework is easily extended to zero-shot learning. Extensive experiments on five benchmarks demonstrate that our simple approach provides a unified and effective approach for both of these two tasks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Sung-Learning%20to%20Compare-%20Relation%20Network%20for%20Few-Shot%20Learning-2018-CVPR.pdf}
}
@article{InstructGPTOuyangNeurIPS2022, 
year = {2022}, 
title = {{Training language models to follow instructions with human feedback}}, 
author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan}, 
journal = {NeurIPS}, 
eprint = {2203.02155}, 
abstract = {{Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ouyang-Training%20language%20models%20to%20follow%20instructions%20with%20human%20feedback-2022-NeurIPS.pdf}
}
@article{TopologicalInteractionGuptaECCV2022, 
year = {2022}, 
title = {{Learning Topological Interactions for Multi-Class Medical Image Segmentation}}, 
author = {Gupta, Saumya and Hu, Xiaoling and Kaan, James and Jin, Michael and Mpoy, Mutshipay and Chung, Katherine and Singh, Gagandeep and Saltz, Mary and Kurc, Tahsin and Saltz, Joel and Tassiopoulos, Apostolos and Prasanna, Prateek and Chen, Chao}, 
journal = {ECCV}, 
eprint = {2207.09654}, 
abstract = {{Deep learning methods have achieved impressive performance for multi-class medical image segmentation. However, they are limited in their ability to encode topological interactions among different classes (e.g., containment and exclusion). These constraints naturally arise in biomedical images and can be crucial in improving segmentation quality. In this paper, we introduce a novel topological interaction module to encode the topological interactions into a deep neural network. The implementation is completely convolution-based and thus can be very efficient. This empowers us to incorporate the constraints into end-to-end training and enrich the feature representation of neural networks. The efficacy of the proposed method is validated on different types of interactions. We also demonstrate the generalizability of the method on both proprietary and public challenge datasets, in both 2D and 3D settings, as well as across different modalities such as CT and Ultrasound. Code is available at: https://github.com/TopoXLab/TopoInteraction}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Gupta-Learning%20Topological%20Interactions%20for%20Multi-Class%20Medical%20Image%20Segmentation-2022-ECCV.pdf}
}
@article{OnlineKDHarutyunyanICLR2023, 
year = {2023}, 
title = {{Supervision Complexity and its Role in Knowledge Distillation}}, 
author = {Harutyunyan, Hrayr and Rawat, Ankit Singh and Menon, Aditya Krishna and Kim, Seungyeon and Kumar, Sanjiv}, 
journal = {ICLR}, 
eprint = {2301.12245}, 
abstract = {{Despite the popularity and efficacy of knowledge distillation, there is limited understanding of why it helps. In order to study the generalization behavior of a distilled student, we propose a new theoretical framework that leverages supervision complexity: a measure of alignment between teacher-provided supervision and the student's neural tangent kernel. The framework highlights a delicate interplay among the teacher's accuracy, the student's margin with respect to the teacher predictions, and the complexity of the teacher predictions. Specifically, it provides a rigorous justification for the utility of various techniques that are prevalent in the context of distillation, such as early stopping and temperature scaling. Our analysis further suggests the use of online distillation, where a student receives increasingly more complex supervision from teachers in different stages of their training. We demonstrate efficacy of online distillation and validate the theoretical findings on a range of image classification benchmarks and model architectures.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Harutyunyan-Supervision%20Complexity%20and%20its%20Role%20in%20Knowledge%20Distillation-2023-ICLR.pdf}
}
@article{PartialLabelsCourJMLR2011, 
year = {2011}, 
title = {{Learning from Partial Labels}}, 
author = {Cour, Timothee and Sapp, Benjamin and Taskar, Ben}, 
journal = {JMLR}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Cour-Learning%20from%20Partial%20Labels-2011-JMLR.pdf}
}
@article{DiceLossLiACL2020, 
year = {2020}, 
title = {{Dice Loss for Data-imbalanced NLP Tasks}}, 
author = {Li, Xiaoya and Sun, Xiaofei and Meng, Yuxian and Liang, Junjun and Wu, Fei and Li, Jiwei}, 
journal = {ACL}, 
eprint = {1911.02855}, 
abstract = {{Many NLP tasks such as tagging and machine reading comprehension are faced with the severe data imbalance issue: negative examples significantly outnumber positive examples, and the huge number of background examples (or easy-negative examples) overwhelms the training. The most commonly used cross entropy (CE) criteria is actually an accuracy-oriented objective, and thus creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples. In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks. Dice loss is based on the Sorensen-Dice coefficient or Tversky index, which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue. To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples.Theoretical analysis shows that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training. With the proposed training objective, we observe significant performance boost on a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task; SOTA results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Dice%20Loss%20for%20Data-imbalanced%20NLP%20Tasks-2020-ACL.pdf}
}
@article{ConvNeXtV2WooarXiv2023, 
year = {2023}, 
title = {{ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders}}, 
author = {Woo, Sanghyun and Debnath, Shoubhik and Hu, Ronghang and Chen, Xinlei and Liu, Zhuang and Kweon, In So and Xie, Saining}, 
journal = {arXiv}, 
eprint = {2301.00808}, 
abstract = {{Driven by improved architectures and better representation learning frameworks, the field of visual recognition has enjoyed rapid modernization and performance boost in the early 2020s. For example, modern ConvNets, represented by ConvNeXt, have demonstrated strong performance in various scenarios. While these models were originally designed for supervised learning with ImageNet labels, they can also potentially benefit from self-supervised learning techniques such as masked autoencoders (MAE). However, we found that simply combining these two approaches leads to subpar performance. In this paper, we propose a fully convolutional masked autoencoder framework and a new Global Response Normalization (GRN) layer that can be added to the ConvNeXt architecture to enhance inter-channel feature competition. This co-design of self-supervised learning techniques and architectural improvement results in a new model family called ConvNeXt V2, which significantly improves the performance of pure ConvNets on various recognition benchmarks, including ImageNet classification, COCO detection, and ADE20K segmentation. We also provide pre-trained ConvNeXt V2 models of various sizes, ranging from an efficient 3.7M-parameter Atto model with 76.7\% top-1 accuracy on ImageNet, to a 650M Huge model that achieves a state-of-the-art 88.9\% accuracy using only public training data.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Woo-ConvNeXt%20V2-%20Co-designing%20and%20Scaling%20ConvNets%20with%20Masked%20Autoencoders-2023-arXiv.pdf}
}
@article{COCO-StuffCaesarCVPR2018, 
year = {2018}, 
title = {{COCO-Stuff: Thing and Stuff Classes in Context}}, 
author = {Caesar, Holger and Uijlings, Jasper and Ferrari, Vittorio}, 
journal = {CVPR}, 
abstract = {{Semantic classes can be either things (objects with a well-defined shape, e.g. car, person) or stuff (amorphous background regions, e.g. grass, sky). While lots of classification and detection works focus on thing classes, less attention has been given to stuff classes. Nonetheless, stuff classes are important as they allow to explain important aspects of an image, including (1) scene type; (2) which thing classes are likely to be present and their location (through contextual reasoning); (3) physical attributes, material types and geometric properties of the scene. To understand stuff and things in context we introduce COCO-Stuff, which augments all 164K images of the COCO 2017 dataset with pixel-wise annotations for 91 stuff classes. We introduce an efficient stuff annotation protocol based on superpixels, which leverages the original thing annotations. We quantify the speed versus quality trade-off of our protocol and explore the relation between annotation time and boundary complexity. Furthermore, we use COCO-Stuff to analyze: (a) the importance of stuff and thing classes in terms of their surface cover and how frequently they are mentioned in image captions; (b) the spatial relations between stuff and things, highlighting the rich contextual relations that make our dataset unique; (c) the performance of a modern semantic segmentation method on stuff and thing classes, and whether stuff is easier to segment than things.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Caesar-COCO-Stuff-%20Thing%20and%20Stuff%20Classes%20in%20Context-2018-CVPR.pdf}
}
@article{MAEHeCVPR2022, 
year = {2022}, 
title = {{Masked Autoencoders Are Scalable Vision Learners}}, 
author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross}, 
journal = {CVPR}, 
eprint = {2111.06377}, 
abstract = {{This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/He-Masked%20Autoencoders%20Are%20Scalable%20Vision%20Learners-2022-CVPR.pdf}
}
@article{TLLMZhuNeurIPS2022, 
year = {2022}, 
title = {{Teach Less, Learn More: On the Undistillable Classes in Knowledge Distillation}}, 
author = {Zhu, Yichen and Liu, Ning and Xu, Zhiyuan and Liu, Xin and Meng, Weibing and Wang, Yi and Ou, Zhicai and Tang, Jian}, 
journal = {NeurIPS}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhu-Teach%20Less,%20Learn%20More-%20On%20the%20Undistillable%20Classes%20in%20Knowledge%20Distillation-2022-NeurIPS.pdf}
}
@article{XceptionCholletCVPR2017, 
year = {2017}, 
title = {{Xception: Deep Learning with Depthwise Separable Convolutions}}, 
author = {Chollet, François}, 
journal = {CVPR}, 
abstract = {{We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception \$V3\$ on the ImageNet dataset (which Inception \$V3\$ was designed for), and significantly outperforms Inception \$V3\$ on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception \$V3\$, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chollet-Xception-%20Deep%20Learning%20with%20Depthwise%20Separable%20Convolutions-2017-CVPR.pdf}
}
@article{CompressingGongICLR2015, 
year = {2015}, 
title = {{Compressing Deep Convolutional Networks using Vector Quantization}}, 
author = {Gong, Yunchao and Liu, Liu and Yang, Ming and Bourdev, Lubomir}, 
journal = {ICLR}, 
eprint = {1412.6115}, 
abstract = {{Deep convolutional neural networks (CNN) has become the most promising method for object recognition, repeatedly demonstrating record breaking results for image classification and object detection in recent years. However, a very deep CNN generally involves many layers with millions of parameters, making the storage of the network model to be extremely large. This prohibits the usage of deep CNNs on resource limited hardware, especially cell phones or other embedded devices. In this paper, we tackle this model storage issue by investigating information theoretical vector quantization methods for compressing the parameters of CNNs. In particular, we have found in terms of compressing the most storage demanding dense connected layers, vector quantization methods have a clear gain over existing matrix factorization methods. Simply applying k-means clustering to the weights or conducting product quantization can lead to a very good balance between model size and recognition accuracy. For the 1000-category classification task in the ImageNet challenge, we are able to achieve 16-24 times compression of the network with only 1\% loss of classification accuracy using the state-of-the-art CNN.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Gong-Compressing%20Deep%20Convolutional%20Networks%20using%20Vector%20Quantization-2015-ICLR.pdf}
}
@article{VANGuoarXiv2022, 
year = {2022}, 
title = {{Visual Attention Network}}, 
author = {Guo, Meng-Hao and Lu, Cheng-Ze and Liu, Zheng-Ning and Cheng, Ming-Ming and Hu, Shi-Min}, 
journal = {arXiv}, 
eprint = {2202.09741}, 
abstract = {{While originally designed for natural language processing tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings three challenges for applying self-attention in computer vision. (1) Treating images as 1D sequences neglects their 2D structures. (2) The quadratic complexity is too expensive for high-resolution images. (3) It only captures spatial adaptability but ignores channel adaptability. In this paper, we propose a novel linear attention named large kernel attention (LKA) to enable self-adaptive and long-range correlations in self-attention while avoiding its shortcomings. Furthermore, we present a neural network based on LKA, namely Visual Attention Network (VAN). While extremely simple, VAN surpasses similar size vision transformers(ViTs) and convolutional neural networks(CNNs) in various tasks, including image classification, object detection, semantic segmentation, panoptic segmentation, pose estimation, etc. For example, VAN-B6 achieves 87.8\% accuracy on ImageNet benchmark and set new state-of-the-art performance (58.2 PQ) for panoptic segmentation. Besides, VAN-B2 surpasses Swin-T 4\% mIoU (50.1 vs. 46.1) for semantic segmentation on ADE20K benchmark, 2.6\% AP (48.8 vs. 46.2) for object detection on COCO dataset. It provides a novel method and a simple yet strong baseline for the community. Code is available at https://github.com/Visual-Attention-Network.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Guo-Visual%20Attention%20Network-2022-arXiv.pdf}
}
@article{HamburgerGengICLR2021, 
year = {2021}, 
title = {{Is Attention Better Than Matrix Decomposition?}}, 
author = {Geng, Zhengyang and Guo, Meng-Hao and Chen, Hongxu and Li, Xia and Wei, Ke and Lin, Zhouchen}, 
journal = {ICLR}, 
eprint = {2109.04553}, 
abstract = {{As an essential ingredient of modern deep learning, attention mechanism, especially self-attention, plays a vital role in the global correlation discovery. However, is hand-crafted attention irreplaceable when modeling the global context? Our intriguing finding is that self-attention is not better than the matrix decomposition (MD) model developed 20 years ago regarding the performance and computational cost for encoding the long-distance dependencies. We model the global context issue as a low-rank recovery problem and show that its optimization algorithms can help design global information blocks. This paper then proposes a series of Hamburgers, in which we employ the optimization algorithms for solving MDs to factorize the input representations into sub-matrices and reconstruct a low-rank embedding. Hamburgers with different MDs can perform favorably against the popular global context module self-attention when carefully coping with gradients back-propagated through MDs. Comprehensive experiments are conducted in the vision tasks where it is crucial to learn the global context, including semantic segmentation and image generation, demonstrating significant improvements over self-attention and its variants.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Geng-Is%20Attention%20Better%20Than%20Matrix%20Decomposition--2021-ICLR.pdf}
}
@article{SegNeXtGuoNeurIPS2022, 
year = {2022}, 
title = {{SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation}}, 
author = {Guo, Meng-Hao and Lu, Cheng-Ze and Hou, Qibin and Liu, Zhengning and Cheng, Ming-Ming and Hu, Shi-Min}, 
journal = {NeurIPS}, 
eprint = {2209.08575}, 
abstract = {{We present SegNeXt, a simple convolutional network architecture for semantic segmentation. Recent transformer-based models have dominated the field of semantic segmentation due to the efficiency of self-attention in encoding spatial information. In this paper, we show that convolutional attention is a more efficient and effective way to encode contextual information than the self-attention mechanism in transformers. By re-examining the characteristics owned by successful segmentation models, we discover several key components leading to the performance improvement of segmentation models. This motivates us to design a novel convolutional attention network that uses cheap convolutional operations. Without bells and whistles, our SegNeXt significantly improves the performance of previous state-of-the-art methods on popular benchmarks, including ADE20K, Cityscapes, COCO-Stuff, Pascal VOC, Pascal Context, and iSAID. Notably, SegNeXt outperforms EfficientNet-L2 w/ NAS-FPN and achieves 90.6\% mIoU on the Pascal VOC 2012 test leaderboard using only 1/10 parameters of it. On average, SegNeXt achieves about 2.0\% mIoU improvements compared to the state-of-the-art methods on the ADE20K datasets with the same or fewer computations. Code is available at https://github.com/uyzhang/JSeg (Jittor) and https://github.com/Visual-Attention-Network/SegNeXt (Pytorch).}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Guo-SegNeXt-%20Rethinking%20Convolutional%20Attention%20Design%20for%20Semantic%20Segmentation-2022-NeurIPS.pdf}
}
@article{DoesStantonNeurIPS2021, 
year = {2021}, 
title = {{Does Knowledge Distillation Really Work?}}, 
author = {Stanton, Samuel and Izmailov, Pavel and Kirichenko, Polina and Alemi, Alexander A and Wilson, Andrew Gordon}, 
journal = {NeurIPS}, 
eprint = {2106.05945}, 
abstract = {{Knowledge distillation is a popular technique for training a small student network to emulate a larger teacher model, such as an ensemble of networks. We show that while knowledge distillation can improve student generalization, it does not typically work as it is commonly understood: there often remains a surprisingly large discrepancy between the predictive distributions of the teacher and the student, even in cases when the student has the capacity to perfectly match the teacher. We identify difficulties in optimization as a key reason for why the student is unable to match the teacher. We also show how the details of the dataset used for distillation play a role in how closely the student matches the teacher -- and that more closely matching the teacher paradoxically does not always lead to better student generalization.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Stanton-Does%20Knowledge%20Distillation%20Really%20Work--2021-NeurIPS.pdf}
}
@article{HDBaydinICLR2018, 
year = {2018}, 
title = {{Online Learning Rate Adaptation with Hypergradient Descent}}, 
author = {Baydin, Atilim Gunes and Cornish, Robert and Rubio, David Martinez and Schmidt, Mark and Wood, Frank}, 
journal = {ICLR}, 
eprint = {1703.04782}, 
abstract = {{We introduce a general method for improving the convergence rate of gradient-based optimizers that is easy to implement and works well in practice. We demonstrate the effectiveness of the method in a range of optimization problems by applying it to stochastic gradient descent, stochastic gradient descent with Nesterov momentum, and Adam, showing that it significantly reduces the need for the manual tuning of the initial learning rate for these commonly used algorithms. Our method works by dynamically updating the learning rate during optimization using the gradient with respect to the learning rate of the update rule itself. Computing this "hypergradient" needs little additional computation, requires only one extra copy of the original gradient to be stored in memory, and relies upon nothing more than what is provided by reverse-mode automatic differentiation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Baydin-Online%20Learning%20Rate%20Adaptation%20with%20Hypergradient%20Descent-2018-ICLR.pdf}
}
@article{ConViTAscoli, 
year = {2021}, 
title = {{ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases}}, 
author = {d'Ascoli, Stéphane and Touvron, Hugo and Leavitt, Matthew and Morcos, Ari and Biroli, Giulio and Sagun, Levent}, 
journal = {ICML}, 
eprint = {2103.10697}, 
abstract = {{Convolutional architectures have proven extremely successful for vision tasks. Their hard inductive biases enable sample-efficient learning, but come at the cost of a potentially lower performance ceiling. Vision Transformers (ViTs) rely on more flexible self-attention layers, and have recently outperformed CNNs for image classification. However, they require costly pre-training on large external datasets or distillation from pre-trained convolutional networks. In this paper, we ask the following question: is it possible to combine the strengths of these two architectures while avoiding their respective limitations? To this end, we introduce gated positional self-attention (GPSA), a form of positional self-attention which can be equipped with a ``soft" convolutional inductive bias. We initialise the GPSA layers to mimic the locality of convolutional layers, then give each attention head the freedom to escape locality by adjusting a gating parameter regulating the attention paid to position versus content information. The resulting convolutional-like ViT architecture, ConViT, outperforms the DeiT on ImageNet, while offering a much improved sample efficiency. We further investigate the role of locality in learning by first quantifying how it is encouraged in vanilla self-attention layers, then analysing how it is escaped in GPSA layers. We conclude by presenting various ablations to better understand the success of the ConViT. Our code and models are released publicly at https://github.com/facebookresearch/convit.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/d'Ascoli-ConViT-%20Improving%20Vision%20Transformers%20with%20Soft%20Convolutional%20Inductive%20Biases-2021-ICML.pdf}
}
@article{TwinsChuNeurIPS2021, 
year = {2021}, 
title = {{Twins: Revisiting the Design of Spatial Attention in Vision Transformers}}, 
author = {Chu, Xiangxiang and Tian, Zhi and Wang, Yuqing and Zhang, Bo and Ren, Haibing and Wei, Xiaolin and Xia, Huaxia and Shen, Chunhua}, 
journal = {NeurIPS}, 
eprint = {2104.13840}, 
abstract = {{Very recently, a variety of vision transformer architectures for dense prediction tasks have been proposed and they show that the design of spatial attention is critical to their success in these tasks. In this work, we revisit the design of the spatial attention and demonstrate that a carefully-devised yet simple spatial attention mechanism performs favourably against the state-of-the-art schemes. As a result, we propose two vision transformer architectures, namely, Twins-PCPVT and Twins-SVT. Our proposed architectures are highly-efficient and easy to implement, only involving matrix multiplications that are highly optimized in modern deep learning frameworks. More importantly, the proposed architectures achieve excellent performance on a wide range of visual tasks, including image level classification as well as dense detection and segmentation. The simplicity and strong performance suggest that our proposed architectures may serve as stronger backbones for many vision tasks. Our code is released at https://github.com/Meituan-AutoML/Twins .}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chu-Twins-%20Revisiting%20the%20Design%20of%20Spatial%20Attention%20in%20Vision%20Transformers-2021-NeurIPS.pdf}
}
@article{CPVTChuICLR2023, 
year = {2023}, 
title = {{Conditional Positional Encodings for Vision Transformers}}, 
author = {Chu, Xiangxiang and Tian, Zhi and Zhang, Bo and Wang, Xinlong and Wei, Xiaolin and Xia, Huaxia and Shen, Chunhua}, 
journal = {ICLR}, 
eprint = {2102.10882}, 
abstract = {{We propose a conditional positional encoding (CPE) scheme for vision Transformers. Unlike previous fixed or learnable positional encodings, which are pre-defined and independent of input tokens, CPE is dynamically generated and conditioned on the local neighborhood of the input tokens. As a result, CPE can easily generalize to the input sequences that are longer than what the model has ever seen during training. Besides, CPE can keep the desired translation-invariance in the image classification task, resulting in improved classification accuracy. CPE can be effortlessly implemented with a simple Position Encoding Generator (PEG), and it can be seamlessly incorporated into the current Transformer framework. Built on PEG, we present Conditional Position encoding Vision Transformer (CPVT). We demonstrate that CPVT has visually similar attention maps compared to those with learned positional encodings. Benefit from the conditional positional encoding scheme, we obtain state-of-the-art results on the ImageNet classification task compared with vision Transformers to date. Our code will be made available at https://github.com/Meituan-AutoML/CPVT .}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chu-Conditional%20Positional%20Encodings%20for%20Vision%20Transformers-2023-ICLR.pdf}
}
@article{PVTWangICCV2021, 
year = {2021}, 
title = {{Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions}}, 
author = {Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling}, 
journal = {ICCV}, 
abstract = {{Although convolutional neural networks (CNNs) have achieved great success in computer vision, this work investigates a simpler, convolution-free backbone network use-fid for many dense prediction tasks. Unlike the recently-proposed Vision Transformer (ViT) that was designed for image classification specifically, we introduce the Pyramid Vision Transformer (PVT), which overcomes the difficulties of porting Transformer to various dense prediction tasks. PVT has several merits compared to current state of the arts. (1) Different from ViT that typically yields low-resolution outputs and incurs high computational and memory costs, PVT not only can be trained on dense partitions of an image to achieve high output resolution, which is important for dense prediction, but also uses a progressive shrinking pyramid to reduce the computations of large feature maps. (2) PVT inherits the advantages of both CNN and Transformer, making it a unified backbone for various vision tasks without convolutions, where it can be used as a direct replacement for CNN backbones. (3) We validate PVT through extensive experiments, showing that it boosts the performance of many downstream tasks, including object detection, instance and semantic segmentation. For example, with a comparable number of parameters, PVT+RetinaNet achieves 40.4 AP on the COCO dataset, surpassing ResNet50+RetinNet (36.3 AP) by 4.1 absolute AP (see Figure 2). We hope that PVT could, serre as an alternative and useful backbone for pixel-level predictions and facilitate future research.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Pyramid%20Vision%20Transformer-%20A%20Versatile%20Backbone%20for%20Dense%20Prediction%20without%20Convolutions-2021-ICCV.pdf}
}
@article{PVTV2WangCVM2021, 
year = {2022}, 
title = {{PVT v2: Improved Baselines with Pyramid Vision Transformer}}, 
author = {Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling}, 
journal = {CVM}, 
eprint = {2106.13797}, 
abstract = {{Transformer recently has presented encouraging progress in computer vision. In this work, we present new baselines by improving the original Pyramid Vision Transformer (PVT v1) by adding three designs, including (1) linear complexity attention layer, (2) overlapping patch embedding, and (3) convolutional feed-forward network. With these modifications, PVT v2 reduces the computational complexity of PVT v1 to linear and achieves significant improvements on fundamental vision tasks such as classification, detection, and segmentation. Notably, the proposed PVT v2 achieves comparable or better performances than recent works such as Swin Transformer. We hope this work will facilitate state-of-the-art Transformer researches in computer vision. Code is available at https://github.com/whai362/PVT.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-PVT%20v2-%20Improved%20Baselines%20with%20Pyramid%20Vision%20Transformer-2022-CVM.pdf}
}
@article{EKGLeeECCV2022, 
year = {2022}, 
title = {{Ensemble Knowledge Guided Sub-network Search and Fine-tuning for Filter Pruning}}, 
author = {Lee, Seunghyun and Song, Byung Cheol}, 
journal = {ECCV}, 
eprint = {2203.02651}, 
abstract = {{Conventional NAS-based pruning algorithms aim to find the sub-network with the best validation performance. However, validation performance does not successfully represent test performance, i.e., potential performance. Also, although fine-tuning the pruned network to restore the performance drop is an inevitable process, few studies have handled this issue. This paper provides a novel Ensemble Knowledge Guidance (EKG) to solve both problems at once. First, we experimentally prove that the fluctuation of loss landscape can be an effective metric to evaluate the potential performance. In order to search a sub-network with the smoothest loss landscape at a low cost, we employ EKG as a search reward. EKG utilized for the following search iteration is composed of the ensemble knowledge of interim sub-networks, i.e., the by-products of the sub-network evaluation. Next, we reuse EKG to provide a gentle and informative guidance to the pruned network while fine-tuning the pruned network. Since EKG is implemented as a memory bank in both phases, it requires a negligible cost. For example, when pruning and training ResNet-50, just 315 GPU hours are required to remove around 45.04\% of FLOPS without any performance degradation, which can operate even on a low-spec workstation. the implemented code is available at https://github.com/sseung0703/EKG.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lee-Ensemble%20Knowledge%20Guided%20Sub-network%20Search%20and%20Fine-tuning%20for%20Filter%20Pruning-2022-ECCV.pdf}
}
@article{MLP-MixerTolstikhinNeurIPS2021, 
year = {2021}, 
title = {{MLP-Mixer: An all-MLP Architecture for Vision}}, 
author = {Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey}, 
journal = {NeurIPS}, 
eprint = {2105.01601}, 
abstract = {{Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. "mixing" the per-location features), and one with MLPs applied across patches (i.e. "mixing" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tolstikhin-MLP-Mixer-%20An%20all-MLP%20Architecture%20for%20Vision-2021-NeurIPS.pdf}
}
@article{MetaFormerYuCVPR2022, 
year = {2022}, 
title = {{MetaFormer is Actually What You Need for Vision}}, 
author = {Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng}, 
journal = {CVPR}, 
abstract = {{Transformers have shown great potential in computer vision tasks. A common belief is their attention-based token mixer module contributes most to their competence. However, recent works show the attention-based module in transformers can be replaced by spatial MLPs and the resulted models still perform quite well. Based on this observation, we hypothesize that the general architecture of the transformers, instead of the specific token mixer module, is more essential to the model's performance. To verify this, we deliberately replace the attention module in transformers with an embarrassingly simple spatial pooling operator to conduct only basic token mixing. Surprisingly, we observe that the derived model, termed as PoolFormer, achieves competitive performance on multiple computer vision tasks. For example, on ImageNet-1K, PoolFormer achieves 82.1 \% top-1 accuracy, surpassing well-tuned vision transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3\%/1.1\% accuracy with 35\%/52\% fewer parameters and 49\%/61\% fewer MACs. The effectiveness of Pool-Former verifies our hypothesis and urges us to initiate the concept of “MetaFormer”, a general architecture abstracted from transformers without specifying the token mixer. Based on the extensive experiments, we argue that MetaFormer is the key player in achieving superior results for recent transformer and MLP-like models on vision tasks. This work calls for more future research dedicated to improving MetaFormer instead of focusing on the token mixer modules. Additionally, our proposed PoolFormer could serve as a starting baseline for future MetaFormer architecture design.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yu-MetaFormer%20is%20Actually%20What%20You%20Need%20for%20Vision-2022-CVPR.pdf}
}
@article{ViTDetLiECCV2022, 
year = {2022}, 
title = {{Exploring Plain Vision Transformer Backbones for Object Detection}}, 
author = {Li, Yanghao and Mao, Hanzi and Girshick, Ross and He, Kaiming}, 
journal = {ECCV}, 
eprint = {2203.16527}, 
abstract = {{We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones pre-trained as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 AP\_box on the COCO dataset using only ImageNet-1K pre-training. We hope our study will draw attention to research on plain-backbone detectors. Code for ViTDet is available in Detectron2.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Exploring%20Plain%20Vision%20Transformer%20Backbones%20for%20Object%20Detection-2022-ECCV.pdf}
}
@article{MAESTFeichtenhoferNeurIPS2022, 
year = {2022}, 
title = {{Masked Autoencoders As Spatiotemporal Learners}}, 
author = {Feichtenhofer, Christoph and Fan, Haoqi and Li, Yanghao and He, Kaiming}, 
journal = {NeurIPS}, 
eprint = {2205.09113}, 
abstract = {{This paper studies a conceptually simple extension of Masked Autoencoders (MAE) to spatiotemporal representation learning from videos. We randomly mask out spacetime patches in videos and learn an autoencoder to reconstruct them in pixels. Interestingly, we show that our MAE method can learn strong representations with almost no inductive bias on spacetime (only except for patch and positional embeddings), and spacetime-agnostic random masking performs the best. We observe that the optimal masking ratio is as high as 90\% (vs. 75\% on images), supporting the hypothesis that this ratio is related to information redundancy of the data. A high masking ratio leads to a large speedup, e.g., > 4x in wall-clock time or even more. We report competitive results on several challenging video datasets using vanilla Vision Transformers. We observe that MAE can outperform supervised pre-training by large margins. We further report encouraging results of training on real-world, uncurated Instagram data. Our study suggests that the general framework of masked autoencoding (BERT, MAE, etc.) can be a unified methodology for representation learning with minimal domain knowledge.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Feichtenhofer-Masked%20Autoencoders%20As%20Spatiotemporal%20Learners-2022-NeurIPS.pdf}
}
@article{End-to-endStewartCVPR2016, 
year = {2016}, 
title = {{End-to-End People Detection in Crowded Scenes}}, 
author = {Stewart, Russell and Andriluka, Mykhaylo and Ng, Andrew Y.}, 
journal = {CVPR}, 
abstract = {{Current people detectors operate either by scanning an image in a sliding window fashion or by classifying a discrete set of proposals. We propose a model that is based on decoding an image into a set of people detections. Our system takes an image as input and directly outputs a set of distinct detection hypotheses. Because we generate predictions jointly, common post-processing steps such as non-maximum suppression are unnecessary. We use a recurrent LSTM layer for sequence generation and train our model end-to-end with a new loss function that operates on sets of detections. We demonstrate the effectiveness of our approach on the challenging task of detecting people in crowded scenes1 1The implementation is publicly available at https://github.com/Russel191/ReInspect. The implementation is publicly available at https://github.com/Russel191/ReInspect.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Stewart-End-to-End%20People%20Detection%20in%20Crowded%20Scenes-2016-CVPR.pdf}
}
@article{DETRCarionECCV2020, 
year = {2020}, 
title = {{End-to-End Object Detection with Transformers}}, 
author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey}, 
journal = {ECCV}, 
eprint = {2005.12872}, 
abstract = {{We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Carion-End-to-End%20Object%20Detection%20with%20Transformers-2020-ECCV.pdf}
}
@article{DeformableDETRZhuICLR2021, 
year = {2021}, 
title = {{Deformable DETR: Deformable Transformers for End-to-End Object Detection}}, 
author = {Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng}, 
journal = {ICLR}, 
eprint = {2010.04159}, 
abstract = {{DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhu-Deformable%20DETR-%20Deformable%20Transformers%20for%20End-to-End%20Object%20Detection-2021-ICLR.pdf}
}
@article{DINOZhangICLR2023, 
year = {2023}, 
title = {{DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection}}, 
author = {Zhang, Hao and Li, Feng and Liu, Shilong and Zhang, Lei and Su, Hang and Zhu, Jun and Ni, Lionel M and Shum, Heung-Yeung}, 
journal = {ICLR}, 
eprint = {2203.03605}, 
abstract = {{We present DINO (\textbackslashtextbf\{D\}ETR with \textbackslashtextbf\{I\}mproved de\textbackslashtextbf\{N\}oising anch\textbackslashtextbf\{O\}r boxes), a state-of-the-art end-to-end object detector. \% in this paper. DINO improves over previous DETR-like models in performance and efficiency by using a contrastive way for denoising training, a mixed query selection method for anchor initialization, and a look forward twice scheme for box prediction. DINO achieves \$49.4\$AP in \$12\$ epochs and \$51.3\$AP in \$24\$ epochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a significant improvement of \$\textbackslashtextbf\{+6.0\}\$\textbackslashtextbf\{AP\} and \$\textbackslashtextbf\{+2.7\}\$\textbackslashtextbf\{AP\}, respectively, compared to DN-DETR, the previous best DETR-like model. DINO scales well in both model size and data size. Without bells and whistles, after pre-training on the Objects365 dataset with a SwinL backbone, DINO obtains the best results on both COCO \textbackslashtexttt\{val2017\} (\$\textbackslashtextbf\{63.2\}\$\textbackslashtextbf\{AP\}) and \textbackslashtexttt\{test-dev\} (\textbackslashtextbf\{\$\textbackslashtextbf\{63.3\}\$AP\}). Compared to other models on the leaderboard, DINO significantly reduces its model size and pre-training data size while achieving better results. Our code will be available at \textbackslashurl\{https://github.com/IDEACVR/DINO\}.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-DINO-%20DETR%20with%20Improved%20DeNoising%20Anchor%20Boxes%20for%20End-to-End%20Object%20Detection-2023-ICLR.pdf}
}
@article{MaskDINOLiCVPR2023, 
year = {2023}, 
title = {{Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation}}, 
author = {Li, Feng and Zhang, Hao and xu, Huaizhe and Liu, Shilong and Zhang, Lei and Ni, Lionel M and Shum, Heung-Yeung}, 
journal = {CVPR}, 
eprint = {2206.02777}, 
abstract = {{In this paper we present Mask DINO, a unified object detection and segmentation framework. Mask DINO extends DINO (DETR with Improved Denoising Anchor Boxes) by adding a mask prediction branch which supports all image segmentation tasks (instance, panoptic, and semantic). It makes use of the query embeddings from DINO to dot-product a high-resolution pixel embedding map to predict a set of binary masks. Some key components in DINO are extended for segmentation through a shared architecture and training process. Mask DINO is simple, efficient, scalable, and benefits from joint large-scale detection and segmentation datasets. Our experiments show that Mask DINO significantly outperforms all existing specialized segmentation methods, both on a ResNet-50 backbone and a pre-trained model with SwinL backbone. Notably, Mask DINO establishes the best results to date on instance segmentation (54.5 AP on COCO), panoptic segmentation (59.4 PQ on COCO), and semantic segmentation (60.8 mIoU on ADE20K). Code will be avaliable at \textbackslashurl\{https://github.com/IDEACVR/MaskDINO\}.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Mask%20DINO-%20Towards%20A%20Unified%20Transformer-based%20Framework%20for%20Object%20Detection%20and%20Segmentation-2022-arXiv.pdf}
}
@article{SuperTicketsYouECCV2022, 
year = {2022}, 
title = {{SuperTickets: Drawing Task-Agnostic Lottery Tickets from Supernets via Jointly Architecture Searching and Parameter Pruning}}, 
author = {You, Haoran and Li, Baopu and Sun, Zhanyi and Ouyang, Xu and Lin, Yingyan}, 
journal = {ECCV}, 
eprint = {2207.03677}, 
abstract = {{Neural architecture search (NAS) has demonstrated amazing success in searching for efficient deep neural networks (DNNs) from a given supernet. In parallel, the lottery ticket hypothesis has shown that DNNs contain small subnetworks that can be trained from scratch to achieve a comparable or higher accuracy than original DNNs. As such, it is currently a common practice to develop efficient DNNs via a pipeline of first search and then prune. Nevertheless, doing so often requires a search-train-prune-retrain process and thus prohibitive computational cost. In this paper, we discover for the first time that both efficient DNNs and their lottery subnetworks (i.e., lottery tickets) can be directly identified from a supernet, which we term as SuperTickets, via a two-in-one training scheme with jointly architecture searching and parameter pruning. Moreover, we develop a progressive and unified SuperTickets identification strategy that allows the connectivity of subnetworks to change during supernet training, achieving better accuracy and efficiency trade-offs than conventional sparse training. Finally, we evaluate whether such identified SuperTickets drawn from one task can transfer well to other tasks, validating their potential of handling multiple tasks simultaneously. Extensive experiments and ablation studies on three tasks and four benchmark datasets validate that our proposed SuperTickets achieve boosted accuracy and efficiency trade-offs than both typical NAS and pruning pipelines, regardless of having retraining or not. Codes and pretrained models are available at https://github.com/RICE-EIC/SuperTickets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/You-SuperTickets-%20Drawing%20Task-Agnostic%20Lottery%20Tickets%20from%20Supernets%20via%20Jointly%20Architecture%20Searching%20and%20Parameter%20Pruning-2022-ECCV.pdf}
}
@article{SwitOKDQianECCV2022, 
year = {2022}, 
title = {{Switchable Online Knowledge Distillation}}, 
author = {Qian, Biao and Wang, Yang and Yin, Hongzhi and Hong, Richang and Wang, Meng}, 
journal = {ECCV}, 
eprint = {2209.04996}, 
abstract = {{Online Knowledge Distillation (OKD) improves the involved models by reciprocally exploiting the difference between teacher and student. Several crucial bottlenecks over the gap between them -- e.g., Why and when does a large gap harm the performance, especially for student? How to quantify the gap between teacher and student? -- have received limited formal study. In this paper, we propose Switchable Online Knowledge Distillation (SwitOKD), to answer these questions. Instead of focusing on the accuracy gap at test phase by the existing arts, the core idea of SwitOKD is to adaptively calibrate the gap at training phase, namely distillation gap, via a switching strategy between two modes -- expert mode (pause the teacher while keep the student learning) and learning mode (restart the teacher). To possess an appropriate distillation gap, we further devise an adaptive switching threshold, which provides a formal criterion as to when to switch to learning mode or expert mode, and thus improves the student's performance. Meanwhile, the teacher benefits from our adaptive switching threshold and keeps basically on a par with other online arts. We further extend SwitOKD to multiple networks with two basis topologies. Finally, extensive experiments and analysis validate the merits of SwitOKD for classification over the state-of-the-arts. Our code is available at https://github.com/hfutqian/SwitOKD.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Qian-Switchable%20Online%20Knowledge%20Distillation-2022-ECCV.pdf}
}
@article{ATSLiNeurIPS2022, 
year = {2022}, 
title = {{Asymmetric Temperature Scaling Makes Larger Networks Teach Well Again}}, 
author = {Li, Xin-Chun and Fan, Wen-Shu and Song, Shaoming and Li, Yinchuan and Li, Bingshuai and Shao, Yunfeng and Zhan, De-Chuan}, 
journal = {NeurIPS}, 
eprint = {2210.04427}, 
abstract = {{Knowledge Distillation (KD) aims at transferring the knowledge of a well-performed neural network (the \{\textbackslashit teacher\}) to a weaker one (the \{\textbackslashit student\}). A peculiar phenomenon is that a more accurate model doesn't necessarily teach better, and temperature adjustment can neither alleviate the mismatched capacity. To explain this, we decompose the efficacy of KD into three parts: \{\textbackslashit correct guidance\}, \{\textbackslashit smooth regularization\}, and \{\textbackslashit class discriminability\}. The last term describes the distinctness of \{\textbackslashit wrong class probabilities\} that the teacher provides in KD. Complex teachers tend to be over-confident and traditional temperature scaling limits the efficacy of \{\textbackslashit class discriminability\}, resulting in less discriminative wrong class probabilities. Therefore, we propose \{\textbackslashit Asymmetric Temperature Scaling (ATS)\}, which separately applies a higher/lower temperature to the correct/wrong class. ATS enlarges the variance of wrong class probabilities in the teacher's label and makes the students grasp the absolute affinities of wrong classes to the target class as discriminative as possible. Both theoretical analysis and extensive experimental results demonstrate the effectiveness of ATS. The demo developed in Mindspore is available at https://gitee.com/lxcnju/ats-mindspore and will be available at https://gitee.com/mindspore/models/tree/master/research/cv/ats.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Asymmetric%20Temperature%20Scaling%20Makes%20Larger%20Networks%20Teach%20Well%20Again-2022-NeurIPS.pdf}
}
@article{HDQinNeurIPS2022, 
year = {2022}, 
title = {{Hilbert Distillation for Cross-Dimensionality Networks}}, 
author = {Qin, Dian and Wang, Haishuai and Liu, Zhe and Xu, Hongjia and Zhou, Sheng and Bu, Jiajun}, 
journal = {NeurIPS}, 
eprint = {2211.04031}, 
abstract = {{3D convolutional neural networks have revealed superior performance in processing volumetric data such as video and medical imaging. However, the competitive performance by leveraging 3D networks results in huge computational costs, which are far beyond that of 2D networks. In this paper, we propose a novel Hilbert curve-based cross-dimensionality distillation approach that facilitates the knowledge of 3D networks to improve the performance of 2D networks. The proposed Hilbert Distillation (HD) method preserves the structural information via the Hilbert curve, which maps high-dimensional (>=2) representations to one-dimensional continuous space-filling curves. Since the distilled 2D networks are supervised by the curves converted from dimensionally heterogeneous 3D features, the 2D networks are given an informative view in terms of learning structural information embedded in well-trained high-dimensional representations. We further propose a Variable-length Hilbert Distillation (VHD) method to dynamically shorten the walking stride of the Hilbert curve in activation feature areas and lengthen the stride in context feature areas, forcing the 2D networks to pay more attention to learning from activation features. The proposed algorithm outperforms the current state-of-the-art distillation techniques adapted to cross-dimensionality distillation on two classification tasks. Moreover, the distilled 2D networks by the proposed method achieve competitive performance with the original 3D networks, indicating the lightweight distilled 2D networks could potentially be the substitution of cumbersome 3D networks in the real-world scenario.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Qin-Hilbert%20Distillation%20for%20Cross-Dimensionality%20Networks-2022-NeurIPS.pdf}
}
@article{EVAFangCVPR2023, 
year = {2023}, 
title = {{EVA: Exploring the Limits of Masked Visual Representation Learning at Scale}}, 
author = {Fang, Yuxin and Wang, Wen and Xie, Binhui and Sun, Quan and Wu, Ledell and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue}, 
journal = {CVPR}, 
eprint = {2211.07636}, 
abstract = {{We launch EVA, a vision-centric foundation model to explore the limits of visual representation at scale using only publicly accessible data. EVA is a vanilla ViT pre-trained to reconstruct the masked out image-text aligned vision features conditioned on visible image patches. Via this pretext task, we can efficiently scale up EVA to one billion parameters, and sets new records on a broad range of representative vision downstream tasks, such as image recognition, video action recognition, object detection, instance segmentation and semantic segmentation without heavy supervised training. Moreover, we observe quantitative changes in scaling EVA result in qualitative changes in transfer learning performance that are not present in other models. For instance, EVA takes a great leap in the challenging large vocabulary instance segmentation task: our model achieves almost the same state-of-the-art performance on LVISv1.0 dataset with over a thousand categories and COCO dataset with only eighty categories. Beyond a pure vision encoder, EVA can also serve as a vision-centric, multi-modal pivot to connect images and text. We find initializing the vision tower of a giant CLIP from EVA can greatly stabilize the training and outperform the training from scratch counterpart with much fewer samples and less compute, providing a new direction for scaling up and accelerating the costly training of multi-modal foundation models. To facilitate future research, we release all the code and models at https://github.com/baaivision/EVA.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Fang-EVA-%20Exploring%20the%20Limits%20of%20Masked%20Visual%20Representation%20Learning%20at%20Scale-2022-arXiv.pdf}
}
@article{SeaFormerWanICLR2023, 
year = {2023}, 
title = {{SeaFormer: Squeeze-enhanced Axial Transformer for Mobile Semantic Segmentation}}, 
author = {Wan, Qiang and Huang, Zilong and Lu, Jiachen and Yu, Gang and Zhang, Li}, 
journal = {ICLR}, 
eprint = {2301.13156}, 
abstract = {{Since the introduction of Vision Transformers, the landscape of many computer vision tasks (e.g., semantic segmentation), which has been overwhelmingly dominated by CNNs, recently has significantly revolutionized. However, the computational cost and memory requirement render these methods unsuitable on the mobile device, especially for the high-resolution per-pixel semantic segmentation task. In this paper, we introduce a new method squeeze-enhanced Axial TransFormer (SeaFormer) for mobile semantic segmentation. Specifically, we design a generic attention block characterized by the formulation of squeeze Axial and detail enhancement. It can be further used to create a family of backbone architectures with superior cost-effectiveness. Coupled with a light segmentation head, we achieve the best trade-off between segmentation accuracy and latency on the ARM-based mobile devices on the ADE20K and Cityscapes datasets. Critically, we beat both the mobile-friendly rivals and Transformer-based counterparts with better performance and lower latency without bells and whistles. Beyond semantic segmentation, we further apply the proposed SeaFormer architecture to image classification problem, demonstrating the potentials of serving as a versatile mobile-friendly backbone.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wan-SeaFormer-%20Squeeze-enhanced%20Axial%20Transformer%20for%20Mobile%20Semantic%20Segmentation-2023-ICLR.pdf}
}
@article{AFastKwonNeurIPS2022, 
year = {2022}, 
title = {{A Fast Post-Training Pruning Framework for Transformers}}, 
author = {Kwon, Woosuk and Kim, Sehoon and Mahoney, Michael W and Hassoun, Joseph and Keutzer, Kurt and Gholami, Amir}, 
journal = {NeurIPS}, 
eprint = {2204.09656}, 
abstract = {{Pruning is an effective way to reduce the huge inference cost of large Transformer models. However, prior work on model pruning requires retraining the model. This can add high cost and complexity to model deployment, making it difficult to use in many practical situations. To address this, we propose a fast post-training pruning framework for Transformers that does not require any retraining. Given a resource constraint and a sample dataset, our framework automatically prunes the Transformer model using structured sparsity methods. To retain high accuracy without retraining, we introduce three novel techniques: (i) a lightweight mask search algorithm that finds which heads and filters to prune based on the Fisher information; (ii) mask rearrangement that complements the search algorithm; and (iii) mask tuning that reconstructs the output activations for each layer. We apply our method to BERT-BASE and DistilBERT, and we evaluate its effectiveness on GLUE and SQuAD benchmarks. Our framework achieves up to 2.0x reduction in FLOPs and 1.56x speedup in inference latency, while maintaining < 1\% loss in accuracy. Importantly, our framework prunes Transformers in less than 3 minutes on a single GPU, which is over two orders of magnitude faster than existing pruning approaches that retrain. Our code is publicly available.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kwon-A%20Fast%20Post-Training%20Pruning%20Framework%20for%20Transformers-2022-NeurIPS.pdf}
}
@article{FEDPensoNeurIPS2022, 
year = {2022}, 
title = {{Functional Ensemble Distillation}}, 
author = {Penso, Coby and Achituve, Idan and Fetaya, Ethan}, 
journal = {NeurIPS}, 
eprint = {2206.02183}, 
abstract = {{Bayesian models have many desirable properties, most notable is their ability to generalize from limited data and to properly estimate the uncertainty in their predictions. However, these benefits come at a steep computational cost as Bayesian inference, in most cases, is computationally intractable. One popular approach to alleviate this problem is using a Monte-Carlo estimation with an ensemble of models sampled from the posterior. However, this approach still comes at a significant computational cost, as one needs to store and run multiple models at test time. In this work, we investigate how to best distill an ensemble's predictions using an efficient model. First, we argue that current approaches that simply return distribution over predictions cannot compute important properties, such as the covariance between predictions, which can be valuable for further processing. Second, in many limited data settings, all ensemble members achieve nearly zero training loss, namely, they produce near-identical predictions on the training set which results in sub-optimal distilled models. To address both problems, we propose a novel and general distillation approach, named Functional Ensemble Distillation (FED), and we investigate how to best distill an ensemble in this setting. We find that learning the distilled model via a simple augmentation scheme in the form of mixup augmentation significantly boosts the performance. We evaluated our method on several tasks and showed that it achieves superior results in both accuracy and uncertainty estimation compared to current approaches.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Penso-Functional%20Ensemble%20Distillation-2022-NeurIPS.pdf}
}
@article{SHAKELiNeurIPS2022, 
year = {2022}, 
title = {{Shadow Knowledge Distillation: Bridging Offline and Online Knowledge Transfer}}, 
author = {Li, Lujun and Zhe, Jin}, 
journal = {NeurIPS}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Shadow%20Knowledge%20Distillation-%20Bridging%20Offline%20and%20Online%20Knowledge%20Transfer-2022-NeurIPS.pdf}
}
@article{OnNordstromNeurIPS2022, 
year = {2022}, 
title = {{On Image Segmentation With Noisy Labels: Characterization and Volume Properties of the Optimal Solutions to Accuracy and Dice}}, 
author = {Nordström, Marcus and Hult, Henrik and Söderberg, Jonas and Löfman, Fredrik}, 
journal = {NeurIPS}, 
eprint = {2206.06484}, 
abstract = {{We study two of the most popular performance metrics in medical image segmentation, Accuracy and Dice, when the target labels are noisy. For both metrics, several statements related to characterization and volume properties of the set of optimal segmentations are proved, and associated experiments are provided. Our main insights are: (i) the volume of the solutions to both metrics may deviate significantly from the expected volume of the target, (ii) the volume of a solution to Accuracy is always less than or equal to the volume of a solution to Dice and (iii) the optimal solutions to both of these metrics coincide when the set of feasible segmentations is constrained to the set of segmentations with the volume equal to the expected volume of the target.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Nordström-On%20Image%20Segmentation%20With%20Noisy%20Labels-%20Characterization%20and%20Volume%20Properties%20of%20the%20Optimal%20Solutions%20to%20Accuracy%20and%20Dice-2022-NeurIPS.pdf}
}
@article{UsingSilvaMICCAIWorkshop2021, 
year = {2021}, 
title = {{Using Soft Labels to Model Uncertainty in Medical Image Segmentation}}, 
author = {Silva, João Lourenço and Oliveira, Arlindo L}, 
journal = {MICCAI Workshop}, 
eprint = {2109.12622}, 
abstract = {{Medical image segmentation is inherently uncertain. For a given image, there may be multiple plausible segmentation hypotheses, and physicians will often disagree on lesion and organ boundaries. To be suited to real-world application, automatic segmentation systems must be able to capture this uncertainty and variability. Thus far, this has been addressed by building deep learning models that, through dropout, multiple heads, or variational inference, can produce a set - infinite, in some cases - of plausible segmentation hypotheses for any given image. However, in clinical practice, it may not be practical to browse all hypotheses. Furthermore, recent work shows that segmentation variability plateaus after a certain number of independent annotations, suggesting that a large enough group of physicians may be able to represent the whole space of possible segmentations. Inspired by this, we propose a simple method to obtain soft labels from the annotations of multiple physicians and train models that, for each image, produce a single well-calibrated output that can be thresholded at multiple confidence levels, according to each application's precision-recall requirements. We evaluated our method on the MICCAI 2021 QUBIQ challenge, showing that it performs well across multiple medical image segmentation tasks, produces well-calibrated predictions, and, on average, performs better at matching physicians' predictions than other physicians.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Silva-Using%20Soft%20Labels%20to%20Model%20Uncertainty%20in%20Medical%20Image%20Segmentation-2021-MICCAI%20Workshop.pdf}
}
@article{SoftSegGrosMIA2021, 
year = {2021}, 
title = {{SoftSeg: Advantages of soft versus binary training for image segmentation}}, 
author = {Gros, Charley and Lemay, Andreanne and Cohen-Adad, Julien}, 
journal = {MIA}, 
abstract = {{Most image segmentation algorithms are trained on binary masks formulated as a classification task per pixel. However, in applications such as medical imaging, this “black-and-white” approach is too constraining because the contrast between two tissues is often ill-defined, i.e., the voxels located on objects’ edges contain a mixture of tissues (a partial volume effect). Consequently, assigning a single “hard” label can result in a detrimental approximation. Instead, a soft prediction containing non-binary values would overcome that limitation. In this study, we introduce SoftSeg, a deep learning training approach that takes advantage of soft ground truth labels, and is not bound to binary predictions. SoftSeg aims at solving a regression instead of a classification problem. This is achieved by using (i) no binarization after preprocessing and data augmentation, (ii) a normalized ReLU final activation layer (instead of sigmoid), and (iii) a regression loss function (instead of the traditional Dice loss). We assess the impact of these three features on three open-source MRI segmentation datasets from the spinal cord gray matter, the multiple sclerosis brain lesion, and the multimodal brain tumor segmentation challenges. Across multiple random dataset splittings, SoftSeg outperformed the conventional approach, leading to an increase in Dice score of 2.0\% on the gray matter dataset (p=0.001), 3.3\% for the brain lesions, and 6.5\% for the brain tumors. SoftSeg produces consistent soft predictions at tissues’ interfaces and shows an increased sensitivity for small objects (e.g., multiple sclerosis lesions). The richness of soft labels could represent the inter-expert variability, the partial volume effect, and complement the model uncertainty estimation, which is typically unclear with binary predictions. The developed training pipeline can easily be incorporated into most of the existing deep learning architectures. SoftSeg is implemented in the freely-available deep learning toolbox ivadomed (https://ivadomed.org).}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Gros-SoftSeg-%20Advantages%20of%20soft%20versus%20binary%20training%20for%20image%20segmentation-2021-MIA.pdf}
}
@article{LabelLemayMELBA2023, 
year = {2023}, 
title = {{Label fusion and training methods for reliable representation of inter-rater uncertainty}}, 
author = {Lemay, Andreanne and Gros, Charley and Karthik, Enamundram Naga and Cohen-Adad, Julien}, 
journal = {MELBA}, 
eprint = {2202.07550}, 
abstract = {{Medical tasks are prone to inter-rater variability due to multiple factors such as image quality, professional experience and training, or guideline clarity. Training deep learning networks with annotations from multiple raters is a common practice that mitigates the model's bias towards a single expert. Reliable models generating calibrated outputs and reflecting the inter-rater disagreement are key to the integration of artificial intelligence in clinical practice. Various methods exist to take into account different expert labels. We focus on comparing three label fusion methods: STAPLE, average of the rater's segmentation, and random sampling of each rater's segmentation during training. Each label fusion method is studied using both the conventional training framework and the recently published SoftSeg framework that limits information loss by treating the segmentation task as a regression. Our results, across 10 data splittings on two public datasets, indicate that SoftSeg models, regardless of the ground truth fusion method, had better calibration and preservation of the inter-rater rater variability compared with their conventional counterparts without impacting the segmentation performance. Conventional models, i.e., trained with a Dice loss, with binary inputs, and sigmoid/softmax final activate, were overconfident and underestimated the uncertainty associated with inter-rater variability. Conversely, fusing labels by averaging with the SoftSeg framework led to underconfident outputs and overestimation of the rater disagreement. In terms of segmentation performance, the best label fusion method was different for the two datasets studied, indicating this parameter might be task-dependent. However, SoftSeg had segmentation performance systematically superior or equal to the conventionally trained models and had the best calibration and preservation of the inter-rater variability.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lemay-Label%20fusion%20and%20training%20methods%20for%20reliable%20representation%20of%20inter-rater%20uncertainty-2023-JMLBI.pdf}
}
@misc{QUBIQMenze2020, 
year = {2020}, 
title = {{Quantification of Uncertainties in Biomedical Image Quantification Challenge at MICCAI}}, 
author = {Menze, Bjoern and Joskowicz, Leo and Bakas, Spyridon and Jakab, Andras and Konukoglu, Ender and Becker, Anton}, 
url = {https://qubiq.grand-challenge.org}
}
@misc{timmWightman2019, 
year = {2019}, 
title = {{Pytorch Image Models}}, 
author = {Wightman, Ross}, 
url = {https://github.com/rwightman/pytorch-image-models}
}
@article{SKDRijkNeurIPS2022, 
year = {2022}, 
title = {{Structural Knowledge Distillation for Object Detection}}, 
author = {Rijk, Philip de and Schneider, Lukas and Cordts, Marius and Gavrila, Dariu M}, 
journal = {NeurIPS}, 
eprint = {2211.13133}, 
abstract = {{Knowledge Distillation (KD) is a well-known training paradigm in deep neural networks where knowledge acquired by a large teacher model is transferred to a small student. KD has proven to be an effective technique to significantly improve the student's performance for various tasks including object detection. As such, KD techniques mostly rely on guidance at the intermediate feature level, which is typically implemented by minimizing an lp-norm distance between teacher and student activations during training. In this paper, we propose a replacement for the pixel-wise independent lp-norm based on the structural similarity (SSIM). By taking into account additional contrast and structural cues, feature importance, correlation and spatial dependence in the feature space are considered in the loss formulation. Extensive experiments on MSCOCO demonstrate the effectiveness of our method across different training schemes and architectures. Our method adds only little computational overhead, is straightforward to implement and at the same time it significantly outperforms the standard lp-norms. Moreover, more complex state-of-the-art KD methods using attention-based sampling mechanisms are outperformed, including a +3.5 AP gain using a Faster R-CNN R-50 compared to a vanilla model.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Rijk-Structural%20Knowledge%20Distillation%20for%20Object%20Detection-2022-NeurIPS.pdf}
}
@article{BIWDNGuanAAAI2018, 
year = {2018}, 
title = {{Who Said What: Modeling Individual Labelers Improves Classification}}, 
author = {Guan, Melody Y and Gulshan, Varun and Dai, Andrew M and Hinton, Geoffrey E}, 
journal = {AAAI}, 
eprint = {1703.08774}, 
abstract = {{Data are often labeled by many different experts with each expert only labeling a small fraction of the data and each data point being labeled by several experts. This reduces the workload on individual experts and also gives a better estimate of the unobserved ground truth. When experts disagree, the standard approaches are to treat the majority opinion as the correct label or to model the correct label as a distribution. These approaches, however, do not make any use of potentially valuable information about which expert produced which label. To make use of this extra information, we propose modeling the experts individually and then learning averaging weights for combining them, possibly in sample-specific ways. This allows us to give more weight to more reliable experts and take advantage of the unique strengths of individual experts at classifying certain types of data. Here we show that our approach leads to improvements in computer-aided diagnosis of diabetic retinopathy. We also show that our method performs better than competing algorithms by Welinder and Perona (2010), and by Mnih and Hinton (2012). Our work offers an innovative approach for dealing with the myriad real-world settings that use expert opinions to define labels for training.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Guan-Who%20Said%20What-%20Modeling%20Individual%20Labelers%20Improves%20Classification-2018-AAAI.pdf}
}
@article{HGNewellECCV2016, 
year = {2016}, 
title = {{Stacked Hourglass Networks for Human Pose Estimation}}, 
author = {Newell, Alejandro and Yang, Kaiyu and Deng, Jia}, 
journal = {ECCV}, 
eprint = {1603.06937}, 
abstract = {{This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a "stacked hourglass" network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Newell-Stacked%20Hourglass%20Networks%20for%20Human%20Pose%20Estimation-2016-ECCV.pdf}
}
@article{AdaptiveWingLossWangICCV2019, 
year = {2019}, 
title = {{Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression}}, 
author = {Wang, Xinyao and Bo, Liefeng and Fuxin, Li}, 
journal = {ICCV}, 
eprint = {1904.07399}, 
abstract = {{Heatmap regression with a deep network has become one of the mainstream approaches to localize facial landmarks. However, the loss function for heatmap regression is rarely studied. In this paper, we analyze the ideal loss function properties for heatmap regression in face alignment problems. Then we propose a novel loss function, named Adaptive Wing loss, that is able to adapt its shape to different types of ground truth heatmap pixels. This adaptability penalizes loss more on foreground pixels while less on background pixels. To address the imbalance between foreground and background pixels, we also propose Weighted Loss Map, which assigns high weights on foreground and difficult background pixels to help training process focus more on pixels that are crucial to landmark localization. To further improve face alignment accuracy, we introduce boundary prediction and CoordConv with boundary coordinates. Extensive experiments on different benchmarks, including COFW, 300W and WFLW, show our approach outperforms the state-of-the-art by a significant margin on various evaluation metrics. Besides, the Adaptive Wing loss also helps other heatmap regression tasks. Code will be made publicly available at https://github.com/protossw512/AdaptiveWingLoss.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Adaptive%20Wing%20Loss%20for%20Robust%20Face%20Alignment%20via%20Heatmap%20Regression-2019-ICCV.pdf}
}
@article{WingLossFengCVPR2018, 
year = {2018}, 
title = {{Wing Loss for Robust Facial Landmark Localisation with Convolutional Neural Networks}}, 
author = {Feng, Zhen-Hua and Kittler, Josef and Awais, Muhammad and Huber, Patrik and Wu, Xiao-Jun}, 
journal = {CVPR}, 
eprint = {1711.06753}, 
abstract = {{We present a new loss function, namely Wing loss, for robust facial landmark localisation with Convolutional Neural Networks (CNNs). We first compare and analyse different loss functions including L2, L1 and smooth L1. The analysis of these loss functions suggests that, for the training of a CNN-based localisation model, more attention should be paid to small and medium range errors. To this end, we design a piece-wise loss function. The new loss amplifies the impact of errors from the interval (-w, w) by switching from L1 loss to a modified logarithm function. To address the problem of under-representation of samples with large out-of-plane head rotations in the training set, we propose a simple but effective boosting strategy, referred to as pose-based data balancing. In particular, we deal with the data imbalance problem by duplicating the minority training samples and perturbing them by injecting random image rotation, bounding box translation and other data augmentation approaches. Last, the proposed approach is extended to create a two-stage framework for robust facial landmark localisation. The experimental results obtained on AFLW and 300W demonstrate the merits of the Wing loss function, and prove the superiority of the proposed method over the state-of-the-art approaches.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Feng-Wing%20Loss%20for%20Robust%20Facial%20Landmark%20Localisation%20with%20Convolutional%20Neural%20Networks-2018-CVPR.pdf}
}
@article{ImprovingJensenMICCAI2019, 
year = {2019}, 
title = {{Improving Uncertainty Estimation in Convolutional Neural Networks Using Inter-rater Agreement}}, 
author = {Jensen, Martin Holm and Jørgensen, Dan Richter and Jalaboi, Raluca and Hansen, Mads Eiler and Olsen, Martin Aastrup}, 
journal = {MICCAI}, 
abstract = {{Modern neural networks are pushing the boundaries of medical image classification. For some tasks in dermatology, state of the art models are able to beat human experts in terms of accuracy and type I/II errors. However, in the domain of medical applications, models should also be evaluated on how well they capture uncertainty in samples and labels. This aspect is key to building trust in computer-assisted systems, otherwise largely considered to be black boxes by their users. A common practice in supervised learning is to collect multiple evaluations per sample, which is particularly useful when inter-rater agreement is expected to be low. At the same time, model training traditionally uses label fusion, such as majority voting, to produce a single label for each sample. In this paper, we experimentally show that models trained to predict skin conditions become overconfident when this approach is used; i.e. the probability estimates of the model exceeds the true correctness likelihood. Additionally, we show that a better calibrated model is obtained when training with a label sampling scheme that takes advantage of inter-rater variability during training. The calibration improvements come at no cost in terms of model accuracy. Our approach is combined and contrasted with other recent techniques in uncertainty estimation. All experiments are evaluated on a proprietary dataset consisting of 31017 images of skin, where up to 12 experts have diagnosed each image.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Jensen-Improving%20Uncertainty%20Estimation%20in%20Convolutional%20Neural%20Networks%20Using%20Inter-rater%20Agreement-2019-MICCAI.pdf}
}
@article{DLRFCHeECCV2022, 
year = {2022}, 
title = {{Filter Pruning via Feature Discrimination in Deep Neural Networks}}, 
author = {He, Zhiqiang and Qian, Yaguan and Wang, Yuqi and Wang, Bin and Guan, Xiaohui and Gu, Zhaoquan and Ling, Xiang and Zeng, Shaoning and Wang, Haijiang and Zhou, Wujie}, 
journal = {ECCV}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/He-Filter%20Pruning%20via%20Feature%20Discrimination%20in%20Deep%20Neural%20Networks-2022-ECCV.pdf}
}
@misc{SMPIakubovskii2019, 
year = {2019}, 
title = {{Segmentation models pytorch}}, 
author = {Iakubovskii, Pavel}, 
url = {https://github.com/qubvel/segmentation\_models.pytorch}
}
@misc{MMSegmentation2020, 
year = {2020}, 
title = {{MMSegmentation: OpenMMLab Semantic Segmentation Toolbox and Benchmark}}, 
author = {Contributors, MMSegmentation}, 
url = {https://github.com/open-mmlab/mmsegmentation}
}
@article{Spatial-BCEWuECCV2022, 
year = {2022}, 
title = {{Adaptive Spatial-BCE Loss for Weakly Supervised Semantic Segmentation}}, 
author = {Wu, Tong and Gao, Guangyu and Huang, Junshi and Wei, Xiaolin and Wei, Xiaoming and Liu, Chi Harold}, 
journal = {ECCV}, 
abstract = {{For Weakly-Supervised Semantic Segmentation (WSSS) with image-level annotation, mostly relies on the classification network to generate initial segmentation pseudo-labels. However, the optimization target of classification networks usually neglects the discrimination between different pixels, like insignificant foreground and background regions. In this paper, we propose an adaptive Spatial Binary Cross-Entropy (Spatial-BCE) Loss for WSSS, which aims to enhance the discrimination between pixels. In Spatial-BCE Loss, we calculate the loss independently for each pixel, and heuristically assign the optimization directions for foreground and background pixels separately. An auxiliary self-supervised task is also proposed to guarantee the Spatial-BCE Loss working as envisaged. Meanwhile, to enhance the network’s generalization for different data distributions, we design an alternate training strategy to adaptively generate thresholds to divide the foreground and background. Benefiting from high-quality initial pseudo-labels by Spatial-BCE Loss, our method also reduce the reliance on post-processing, thereby simplifying the pipeline of WSSS. Our method is validated on the PASCAL VOC 2012 and COCO 2014 datasets, and achieves the new state-of-the-arts. Code is available at https://github.com/allenwu97/Spatial-BCE.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wu-Adaptive%20Spatial-BCE%20Loss%20for%20Weakly%20Supervised%20Semantic%20Segmentation-2022-ECCV.pdf}
}
@article{MetricsMaier-HeinarXiv2023, 
year = {2023}, 
title = {{Metrics Reloaded: Recommendations for image analysis validation}}, 
author = {Maier-Hein, Lena and others}, 
journal = {arXiv}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Maier-Hein-Metrics%20Reloaded-%20Recommendations%20for%20image%20analysis%20validation-2023-arXiv.pdf}
}
@article{CARHuangECCV2022, 
year = {2022}, 
title = {{CAR: Class-aware Regularizations for Semantic Segmentation}}, 
author = {Huang, Ye and Kang, Di and Chen, Liang and Zhe, Xuefei and Jia, Wenjing and He, Xiangjian and Bao, Linchao}, 
journal = {ECCV}, 
eprint = {2203.07160}, 
abstract = {{Recent segmentation methods, such as OCR and CPNet, utilizing "class level" information in addition to pixel features, have achieved notable success for boosting the accuracy of existing network modules. However, the extracted class-level information was simply concatenated to pixel features, without explicitly being exploited for better pixel representation learning. Moreover, these approaches learn soft class centers based on coarse mask prediction, which is prone to error accumulation. In this paper, aiming to use class level information more effectively, we propose a universal Class-Aware Regularization (CAR) approach to optimize the intra-class variance and inter-class distance during feature learning, motivated by the fact that humans can recognize an object by itself no matter which other objects it appears with. Three novel loss functions are proposed. The first loss function encourages more compact class representations within each class, the second directly maximizes the distance between different class centers, and the third further pushes the distance between inter-class centers and pixels. Furthermore, the class center in our approach is directly generated from ground truth instead of from the error-prone coarse prediction. Our method can be easily applied to most existing segmentation models during training, including OCR and CPNet, and can largely improve their accuracy at no additional inference overhead. Extensive experiments and ablation studies conducted on multiple benchmark datasets demonstrate that the proposed CAR can boost the accuracy of all baseline models by up to 2.23\% mIOU with superior generalization ability. The complete code is available at https://github.com/edwardyehuang/CAR.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Huang-CAR-%20Class-aware%20Regularizations%20for%20Semantic%20Segmentation-2022-ECCV.pdf}
}
@article{ReLabelYunCVPR2021, 
year = {2021}, 
title = {{Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels}}, 
author = {Yun, Sangdoo and Oh, Seong Joon and Heo, Byeongho and Han, Dongyoon and Choe, Junsuk and Chun, Sanghyuk}, 
journal = {CVPR}, 
eprint = {2101.05022}, 
abstract = {{ImageNet has been arguably the most popular image classification benchmark, but it is also the one with a significant level of label noise. Recent studies have shown that many samples contain multiple classes, despite being assumed to be a single-label benchmark. They have thus proposed to turn ImageNet evaluation into a multi-label task, with exhaustive multi-label annotations per image. However, they have not fixed the training set, presumably because of a formidable annotation cost. We argue that the mismatch between single-label annotations and effectively multi-label images is equally, if not more, problematic in the training setup, where random crops are applied. With the single-label annotations, a random crop of an image may contain an entirely different object from the ground truth, introducing noisy or even incorrect supervision during training. We thus re-label the ImageNet training set with multi-labels. We address the annotation cost barrier by letting a strong image classifier, trained on an extra source of data, generate the multi-labels. We utilize the pixel-wise multi-label predictions before the final pooling layer, in order to exploit the additional location-specific supervision signals. Training on the re-labeled samples results in improved model performances across the board. ResNet-50 attains the top-1 classification accuracy of 78.9\% on ImageNet with our localized multi-labels, which can be further boosted to 80.2\% with the CutMix regularization. We show that the models trained with localized multi-labels also outperforms the baselines on transfer learning to object detection and instance segmentation tasks, and various robustness benchmarks. The re-labeled ImageNet training set, pre-trained weights, and the source code are available at \{https://github.com/naver-ai/relabel\_imagenet\}.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yun-Re-labeling%20ImageNet-%20from%20Single%20to%20Multi-Labels,%20from%20Global%20to%20Localized%20Labels-2021-CVPR.pdf}
}
@article{FKDShenECCV2022, 
year = {2022}, 
title = {{A Fast Knowledge Distillation Framework for Visual Recognition}}, 
author = {Shen, Zhiqiang and Xing, Eric}, 
journal = {ECCV}, 
eprint = {2112.01528}, 
abstract = {{While Knowledge Distillation (KD) has been recognized as a useful tool in many visual tasks, such as supervised classification and self-supervised representation learning, the main drawback of a vanilla KD framework is its mechanism, which consumes the majority of the computational overhead on forwarding through the giant teacher networks, making the entire learning procedure inefficient and costly. ReLabel, a recently proposed solution, suggests creating a label map for the entire image. During training, it receives the cropped region-level label by RoI aligning on a pre-generated entire label map, allowing for efficient supervision generation without having to pass through the teachers many times. However, as the KD teachers are from conventional multi-crop training, there are various mismatches between the global label-map and region-level label in this technique, resulting in performance deterioration. In this study, we present a Fast Knowledge Distillation (FKD) framework that replicates the distillation training phase and generates soft labels using the multi-crop KD approach, while training faster than ReLabel since no post-processes such as RoI align and softmax operations are used. When conducting multi-crop in the same image for data loading, our FKD is even more efficient than the traditional image classification framework. On ImageNet-1K, we obtain 79.8\% with ResNet-50, outperforming ReLabel by \textbackslashtextasciitilde1.0\% while being faster. On the self-supervised learning task, we also show that FKD has an efficiency advantage. Our project page: http://zhiqiangshen.com/projects/FKD/index.html, source code and models are available at: https://github.com/szq0214/FKD.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Shen-A%20Fast%20Knowledge%20Distillation%20Framework%20for%20Visual%20Recognition-2022-ECCV.pdf}
}
@article{SMDLiuECCV2022, 
year = {2021}, 
title = {{DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning}}, 
author = {Gao, Yuting and Zhuang, Jia-Xin and Lin, Shaohui and Cheng, Hao and Sun, Xing and Li, Ke and Shen, Chunhua}, 
journal = {ECCV}, 
eprint = {2104.09124}, 
abstract = {{While self-supervised representation learning (SSL) has received widespread attention from the community, recent research argue that its performance will suffer a cliff fall when the model size decreases. The current method mainly relies on contrastive learning to train the network and in this work, we propose a simple yet effective Distilled Contrastive Learning (DisCo) to ease the issue by a large margin. Specifically, we find the final embedding obtained by the mainstream SSL methods contains the most fruitful information, and propose to distill the final embedding to maximally transmit a teacher's knowledge to a lightweight model by constraining the last embedding of the student to be consistent with that of the teacher. In addition, in the experiment, we find that there exists a phenomenon termed Distilling BottleNeck and present to enlarge the embedding dimension to alleviate this problem. Our method does not introduce any extra parameter to lightweight models during deployment. Experimental results demonstrate that our method achieves the state-of-the-art on all lightweight models. Particularly, when ResNet-101/ResNet-50 is used as teacher to teach EfficientNet-B0, the linear result of EfficientNet-B0 on ImageNet is very close to ResNet-101/ResNet-50, but the number of parameters of EfficientNet-B0 is only 9.4\textbackslash\%/16.3\textbackslash\% of ResNet-101/ResNet-50. Code is available at https://github. com/Yuting-Gao/DisCo-pytorch.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Gao-DisCo-%20Remedy%20Self-supervised%20Learning%20on%20Lightweight%20Models%20with%20Distilled%20Contrastive%20Learning-2021-arXiv.pdf}
}
@article{TaylorPruningMolchanovCVPR2019, 
year = {2019}, 
title = {{Importance Estimation for Neural Network Pruning}}, 
author = {Molchanov, Pavlo and Mallya, Arun and Tyree, Stephen and Frosio, Iuri and Kautz, Jan}, 
journal = {CVPR}, 
eprint = {1906.10771}, 
abstract = {{Structural pruning of neural network parameters reduces computation, energy, and memory transfer costs during inference. We propose a novel method that estimates the contribution of a neuron (filter) to the final loss and iteratively removes those with smaller scores. We describe two variations of our method using the first and second-order Taylor expansions to approximate a filter's contribution. Both methods scale consistently across any network layer without requiring per-layer sensitivity analysis and can be applied to any kind of layer, including skip connections. For modern networks trained on ImageNet, we measured experimentally a high (>93\%) correlation between the contribution computed by our methods and a reliable estimate of the true importance. Pruning with the proposed methods leads to an improvement over state-of-the-art in terms of accuracy, FLOPs, and parameter reduction. On ResNet-101, we achieve a 40\% FLOPS reduction by removing 30\% of the parameters, with a loss of 0.02\% in the top-1 accuracy on ImageNet. Code is available at https://github.com/NVlabs/Taylor\_pruning.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Molchanov-Importance%20Estimation%20for%20Neural%20Network%20Pruning-2019-CVPR.pdf}
}
@article{PatchSlimmingTangCVPR2022, 
year = {2022}, 
title = {{Patch Slimming for Efficient Vision Transformers}}, 
author = {Tang, Yehui and Han, Kai and Wang, Yunhe and Xu, Chang and Guo, Jianyuan and Xu, Chao and Tao, Dacheng}, 
journal = {CVPR}, 
eprint = {2106.02852}, 
abstract = {{This paper studies the efficiency problem for visual transformers by excavating redundant calculation in given networks. The recent transformer architecture has demonstrated its effectiveness for achieving excellent performance on a series of computer vision tasks. However, similar to that of convolutional neural networks, the huge computational cost of vision transformers is still a severe issue. Considering that the attention mechanism aggregates different patches layer-by-layer, we present a novel patch slimming approach that discards useless patches in a top-down paradigm. We first identify the effective patches in the last layer and then use them to guide the patch selection process of previous layers. For each layer, the impact of a patch on the final output feature is approximated and patches with less impact will be removed. Experimental results on benchmark datasets demonstrate that the proposed method can significantly reduce the computational costs of vision transformers without affecting their performances. For example, over 45\% FLOPs of the ViT-Ti model can be reduced with only 0.2\% top-1 accuracy drop on the ImageNet dataset.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tang-Patch%20Slimming%20for%20Efficient%20Vision%20Transformers-2022-CVPR.pdf}
}
@article{SETRZhengCVPR2021, 
year = {2021}, 
title = {{Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers}}, 
author = {Zheng, Sixiao and Lu, Jiachen and Zhao, Hengshuang and Zhu, Xiatian and Luo, Zekun and Wang, Yabiao and Fu, Yanwei and Feng, Jianfeng and Xiang, Tao and Torr, Philip H S and Zhang, Li}, 
journal = {CVPR}, 
eprint = {2012.15840}, 
abstract = {{Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (ie, without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28\% mIoU), Pascal Context (55.83\% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zheng-Rethinking%20Semantic%20Segmentation%20from%20a%20Sequence-to-Sequence%20Perspective%20with%20Transformers-2021-CVPR_1.pdf}
}
@article{DMLsWangMICCAI2023, 
year = {2023}, 
title = {{Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels}}, 
author = {Wang, Zifu and Popordanoska, Teodora and Bertels, Jeroen and Lemmens, Robin and Blaschko, Matthew}, 
journal = {MICCAI}
}
@article{BoxSupDaiICCV2015, 
year = {2015}, 
title = {{BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation}}, 
author = {Dai, Jifeng and He, Kaiming and Sun, Jian}, 
journal = {ICCV}, 
eprint = {1503.01640}, 
abstract = {{Recent leading approaches to semantic segmentation rely on deep convolutional networks trained with human-annotated, pixel-level segmentation masks. Such pixel-accurate supervision demands expensive labeling effort and limits the performance of deep networks that usually benefit from more training data. In this paper, we propose a method that achieves competitive accuracy but only requires easily obtained bounding box annotations. The basic idea is to iterate between automatically generating region proposals and training convolutional networks. These two steps gradually recover segmentation masks for improving the networks, and vise versa. Our method, called BoxSup, produces competitive results supervised by boxes only, on par with strong baselines fully supervised by masks under the same setting. By leveraging a large amount of bounding boxes, BoxSup further unleashes the power of deep convolutional networks and yields state-of-the-art results on PASCAL VOC 2012 and PASCAL-CONTEXT.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Dai-BoxSup-%20Exploiting%20Bounding%20Boxes%20to%20Supervise%20Convolutional%20Networks%20for%20Semantic%20Segmentation-2015-ICCV.pdf}
}
@article{ScribbleSupLinCVPR2016, 
year = {2016}, 
title = {{ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation}}, 
author = {Lin, Di and Dai, Jifeng and Jia, Jiaya and He, Kaiming and Sun, Jian}, 
journal = {CVPR}, 
eprint = {1604.05144}, 
abstract = {{Large-scale data is of crucial importance for learning semantic segmentation models, but annotating per-pixel masks is a tedious and inefficient procedure. We note that for the topic of interactive image segmentation, scribbles are very widely used in academic research and commercial software, and are recognized as one of the most user-friendly ways of interacting. In this paper, we propose to use scribbles to annotate images, and develop an algorithm to train convolutional networks for semantic segmentation supervised by scribbles. Our algorithm is based on a graphical model that jointly propagates information from scribbles to unmarked pixels and learns network parameters. We present competitive object semantic segmentation results on the PASCAL VOC dataset by using scribbles as annotations. Scribbles are also favored for annotating stuff (e.g., water, sky, grass) that has no well-defined shape, and our method shows excellent results on the PASCAL-CONTEXT dataset thanks to extra inexpensive scribble annotations. Our scribble annotations on PASCAL VOC are available at http://research.microsoft.com/en-us/um/people/jifdai/downloads/scribble\_sup}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lin-ScribbleSup-%20Scribble-Supervised%20Convolutional%20Networks%20for%20Semantic%20Segmentation-2016-CVPR.pdf}
}
@article{LearningLouizosICLR2018, 
year = {2018}, 
title = {{Learning Sparse Neural Networks through \$L\_0\$ Regularization}}, 
author = {Louizos, Christos and Welling, Max and Kingma, Diederik P}, 
journal = {ICLR}, 
eprint = {1712.01312}, 
abstract = {{We propose a practical method for \$L\_0\$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of \$L\_0\$ regularization. However, since the \$L\_0\$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected \$L\_0\$ norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the \textbackslashemph\{hard concrete\} distribution for the gates, which is obtained by "stretching" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Louizos-Learning%20Sparse%20Neural%20Networks%20through%20$L_0$%20Regularization-2018-ICLR.pdf}
}
@book{EncyclopediaDeza2009, 
year = {2009}, 
title = {{Encyclopedia of Distances}}, 
author = {Deza, Michel Marie and Deza, Elena}, 
publisher = {Springer}
}
@article{TinyBERTJiaoEMNLPFindings, 
year = {2020}, 
title = {{TinyBERT: Distilling BERT for Natural Language Understanding}}, 
author = {Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun}, 
journal = {EMNLP Findings}, 
eprint = {1909.10351}, 
abstract = {{Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large teacher BERT can be effectively transferred to a small student Tiny-BERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pretraining and task-specific learning stages. This framework ensures that TinyBERT can capture he general-domain as well as the task-specific knowledge in BERT. TinyBERT with 4 layers is empirically effective and achieves more than 96.8\% the performance of its teacher BERTBASE on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT with 4 layers is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only about 28\% parameters and about 31\% inference time of them. Moreover, TinyBERT with 6 layers performs on-par with its teacher BERTBASE.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Jiao-TinyBERT-%20Distilling%20BERT%20for%20Natural%20Language%20Understanding-2020-EMNLP%20Findings.pdf}
}
@article{CornerNetLawECCV2018, 
year = {2018}, 
title = {{CornerNet: Detecting Objects as Paired Keypoints}}, 
author = {Law, Hei and Deng, Jia}, 
journal = {ECCV}, 
eprint = {1808.01244}, 
abstract = {{We propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network. By detecting objects as paired keypoints, we eliminate the need for designing a set of anchor boxes commonly used in prior single-stage detectors. In addition to our novel formulation, we introduce corner pooling, a new type of pooling layer that helps the network better localize corners. Experiments show that CornerNet achieves a 42.2\% AP on MS COCO, outperforming all existing one-stage detectors.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Law-CornerNet-%20Detecting%20Objects%20as%20Paired%20Keypoints-2018-ECCV.pdf}
}
@article{CenterNetDuanICCV2019, 
year = {2019}, 
title = {{CenterNet: Keypoint Triplets for Object Detection}}, 
author = {Duan, Kaiwen and Bai, Song and Xie, Lingxi and Qi, Honggang and Huang, Qingming and Tian, Qi}, 
journal = {ICCV}, 
eprint = {1904.08189}, 
abstract = {{In object detection, keypoint-based approaches often suffer a large number of incorrect object bounding boxes, arguably due to the lack of an additional look into the cropped regions. This paper presents an efficient solution which explores the visual patterns within each cropped region with minimal costs. We build our framework upon a representative one-stage keypoint-based detector named CornerNet. Our approach, named CenterNet, detects each object as a triplet, rather than a pair, of keypoints, which improves both precision and recall. Accordingly, we design two customized modules named cascade corner pooling and center pooling, which play the roles of enriching information collected by both top-left and bottom-right corners and providing more recognizable information at the central regions, respectively. On the MS-COCO dataset, CenterNet achieves an AP of 47.0\%, which outperforms all existing one-stage detectors by at least 4.9\%. Meanwhile, with a faster inference speed, CenterNet demonstrates quite comparable performance to the top-ranked two-stage detectors. Code is available at https://github.com/Duankaiwen/CenterNet.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Duan-CenterNet-%20Keypoint%20Triplets%20for%20Object%20Detection-2019-ICCV.pdf}
}
@misc{PASCALVOC2007Everingham, 
year = {2007}, 
title = {{The PASCAL Visual Object Classes Challenge 2007}}, 
author = {Everingham, Mark and Gool, Luc van and Williams, Chris and Winn, John and Zisserman, Andrew}, 
url = {http://host.robots.ox.ac.uk/pascal/VOC/voc2007}
}
@misc{PASCALVOC2008Everingham, 
year = {2008}, 
title = {{The PASCAL Visual Object Classes Challenge 2008}}, 
author = {Everingham, Mark and Gool, Luc van and Williams, Chris and Winn, John and Zisserman, Andrew}, 
url = {http://host.robots.ox.ac.uk/pascal/VOC/voc2008}
}
@article{MoNuSAC2020VermaTMI2021, 
year = {2021}, 
title = {{MoNuSAC2020: A Multi-Organ Nuclei Segmentation and Classification Challenge}}, 
author = {Verma, Ruchika and others}, 
journal = {TMI}, 
abstract = {{Detecting various types of cells in and around the tumor matrix holds a special significance in characterizing the tumor micro-environment for cancer prognostication and research. Automating the tasks of detecting, segmenting, and classifying nuclei can free up the pathologists' time for higher value tasks and reduce errors due to fatigue and subjectivity. To encourage the computer vision research community to develop and test algorithms for these tasks, we prepared a large and diverse dataset of nucleus boundary annotations and class labels. The dataset has over 46,000 nuclei from 37 hospitals, 71 patients, four organs, and four nucleus types. We also organized a challenge around this dataset as a satellite event at the International Symposium on Biomedical Imaging (ISBI) in April 2020. The challenge saw a wide participation from across the world, and the top methods were able to match inter-human concordance for the challenge metric. In this paper, we summarize the dataset and the key findings of the challenge, including the commonalities and differences between the methods developed by various participants. We have released the MoNuSAC2020 dataset to the public.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Verma-MoNuSAC2020-%20A%20Multi-Organ%20Nuclei%20Segmentation%20and%20Classification%20Challenge-2021-TMI.pdf}
}
@article{SemanticKITTIBehleyICCV2019, 
year = {2019}, 
title = {{SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences}}, 
author = {Behley, Jens and Garbade, Martin and Milioto, Andres and Quenzel, Jan and Behnke, Sven and Stachniss, Cyrill and Gall, Juergen}, 
journal = {ICCV}, 
abstract = {{Semantic scene understanding is important for various applications. In particular, self-driving cars need a fine-grained understanding of the surfaces and objects in their vicinity. Light detection and ranging (LiDAR) provides precise geometric information about the environment and is thus a part of the sensor suites of almost all self-driving cars. Despite the relevance of semantic scene understanding for this application, there is a lack of a large dataset for this task which is based on an automotive LiDAR. In this paper, we introduce a large dataset to propel research on laser-based semantic segmentation. We annotated all sequences of the KITTI Vision Odometry Benchmark and provide dense point-wise annotations for the complete 360°, field-of-view of the employed automotive LiDAR. We propose three benchmark tasks based on this dataset: (i) semantic segmentation of point clouds using a single scan, (ii) semantic segmentation using multiple past scans, and (iii) semantic scene completion, which requires to anticipate the semantic scene in the future. We provide baseline experiments and show that there is a need for more sophisticated models to efficiently tackle these tasks. Our dataset opens the door for the development of more advanced methods, but also provides plentiful data to investigate new research directions.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Behley-SemanticKITTI-%20A%20Dataset%20for%20Semantic%20Scene%20Understanding%20of%20LiDAR%20Sequences-2019-ICCV.pdf}
}
@article{CamVidBrostowECCV2008, 
year = {2008}, 
title = {{Segmentation and Recognition Using Structure from Motion Point Clouds}}, 
author = {Brostow, Gabriel J and Shotton, Jamie and Fauqueur, Julien and Cipolla, Roberto}, 
journal = {ECCV}, 
abstract = {{We propose an algorithm for semantic segmentation based on 3D point clouds derived from ego-motion. We motivate five simple cues designed to model specific patterns of motion and 3D world structure that vary with object category. We introduce features that project the 3D cues back to the 2D image plane while modeling spatial layout and context. A randomized decision forest combines many such features to achieve a coherent 2D segmentation and recognize the object categories present. Our main contribution is to show how semantic segmentation is possible based solely on motion-derived 3D world structure. Our method works well on sparse, noisy point clouds, and unlike existing approaches, does not need appearance-based descriptors. Experiments were performed on a challenging new video database containing sequences filmed from a moving car in daylight and at dusk. The results confirm that indeed, accurate segmentation and recognition are possible using only motion and 3D world structure. Further, we show that the motion-derived information complements an existing state-of-the-art appearance-based method, improving both qualitative and quantitative performance.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Brostow-Segmentation%20and%20Recognition%20Using%20Structure%20from%20Motion%20Point%20Clouds-2008-ECCV.pdf}
}
@article{AreBeyerarXiv2020, 
year = {2020}, 
title = {{Are we done with ImageNet?}}, 
author = {Beyer, Lucas and Hénaff, Olivier J and Kolesnikov, Alexander and Zhai, Xiaohua and Oord, Aäron van den}, 
journal = {arXiv}, 
eprint = {2006.07159}, 
abstract = {{Yes, and no. We ask whether recent progress on the ImageNet classification benchmark continues to represent meaningful generalization, or whether the community has started to overfit to the idiosyncrasies of its labeling procedure. We therefore develop a significantly more robust procedure for collecting human annotations of the ImageNet validation set. Using these new labels, we reassess the accuracy of recently proposed ImageNet classifiers, and find their gains to be substantially smaller than those reported on the original labels. Furthermore, we find the original ImageNet labels to no longer be the best predictors of this independently-collected set, indicating that their usefulness in evaluating vision models may be nearing an end. Nevertheless, we find our annotation procedure to have largely remedied the errors in the original labels, reinforcing ImageNet as a powerful benchmark for future research in visual recognition.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Beyer-Are%20we%20done%20with%20ImageNet--2020-arXiv.pdf}
}
@article{APLMaICML2020, 
year = {2020}, 
title = {{Normalized Loss Functions for Deep Learning with Noisy Labels}}, 
author = {Ma, Xingjun and Huang, Hanxun and Wang, Yisen and Romano, Simone and Erfani, Sarah and Bailey, James}, 
journal = {ICML}, 
eprint = {2006.13554}, 
abstract = {{Robust loss functions are essential for training accurate deep neural networks (DNNs) in the presence of noisy (incorrect) labels. It has been shown that the commonly used Cross Entropy (CE) loss is not robust to noisy labels. Whilst new loss functions have been designed, they are only partially robust. In this paper, we theoretically show by applying a simple normalization that: any loss can be made robust to noisy labels. However, in practice, simply being robust is not sufficient for a loss function to train accurate DNNs. By investigating several robust loss functions, we find that they suffer from a problem of underfitting. To address this, we propose a framework to build robust loss functions called Active Passive Loss (APL). APL combines two robust loss functions that mutually boost each other. Experiments on benchmark datasets demonstrate that the family of new loss functions created by our APL framework can consistently outperform state-of-the-art methods by large margins, especially under large noise rates such as 60\% or 80\% incorrect labels.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ma-Normalized%20Loss%20Functions%20for%20Deep%20Learning%20with%20Noisy%20Labels-2020-ICML.pdf}
}
@article{WhatWangNeurIPS2022, 
year = {2022}, 
title = {{What Makes a "Good" Data Augmentation in Knowledge Distillation -- A Statistical Perspective}}, 
author = {Wang, Huan and Lohit, Suhas and Jones, Mike and Fu, Yun}, 
journal = {NeurIPS}, 
eprint = {2012.02909}, 
abstract = {{Knowledge distillation (KD) is a general neural network training approach that uses a teacher to guide a student. Existing works mainly study KD from the network output side (e.g., trying to design a better KD loss function), while few have attempted to understand it from the input side. Especially, its interplay with data augmentation (DA) has not been well understood. In this paper, we ask: Why do some DA schemes (e.g., CutMix) inherently perform much better than others in KD? What makes a "good" DA in KD? Our investigation from a statistical perspective suggests that a good DA scheme should reduce the variance of the teacher's mean probability, which will eventually lead to a lower generalization gap for the student. Besides the theoretical understanding, we also introduce a new entropy-based data-mixing DA scheme to enhance CutMix. Extensive empirical studies support our claims and demonstrate how we can harvest considerable performance gains simply by using a better DA scheme in knowledge distillation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-What%20Makes%20a%20-Good-%20Data%20Augmentation%20in%20Knowledge%20Distillation%20--%20A%20Statistical%20Perspective-2022-NeurIPS.pdf}
}
@article{K-NetZhangNeurIPS2021, 
year = {2021}, 
title = {{K-Net: Towards Unified Image Segmentation}}, 
author = {Zhang, Wenwei and Pang, Jiangmiao and Chen, Kai and Loy, Chen Change}, 
journal = {NeurIPS}, 
eprint = {2106.14855}, 
abstract = {{Semantic, instance, and panoptic segmentations have been addressed using different and specialized frameworks despite their underlying connections. This paper presents a unified, simple, and effective framework for these essentially similar tasks. The framework, named K-Net, segments both instances and semantic categories consistently by a group of learnable kernels, where each kernel is responsible for generating a mask for either a potential instance or a stuff class. To remedy the difficulties of distinguishing various instances, we propose a kernel update strategy that enables each kernel dynamic and conditional on its meaningful group in the input image. K-Net can be trained in an end-to-end manner with bipartite matching, and its training and inference are naturally NMS-free and box-free. Without bells and whistles, K-Net surpasses all previous published state-of-the-art single-model results of panoptic segmentation on MS COCO test-dev split and semantic segmentation on ADE20K val split with 55.2\% PQ and 54.3\% mIoU, respectively. Its instance segmentation performance is also on par with Cascade Mask R-CNN on MS COCO with 60\%-90\% faster inference speeds. Code and models will be released at https://github.com/ZwwWayne/K-Net/.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-K-Net-%20Towards%20Unified%20Image%20Segmentation-2021-NeurIPS.pdf}
}
@article{ContrastiveWeiarXiv2022, 
year = {2022}, 
title = {{Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation}}, 
author = {Wei, Yixuan and Hu, Han and Xie, Zhenda and Zhang, Zheng and Cao, Yue and Bao, Jianmin and Chen, Dong and Guo, Baining}, 
journal = {arXiv}, 
eprint = {2205.14141}, 
abstract = {{Masked image modeling (MIM) learns representations with remarkably good fine-tuning performances, overshadowing previous prevalent pre-training approaches such as image classification, instance contrastive learning, and image-text alignment. In this paper, we show that the inferior fine-tuning performance of these pre-training approaches can be significantly improved by a simple post-processing in the form of feature distillation (FD). The feature distillation converts the old representations to new representations that have a few desirable properties just like those representations produced by MIM. These properties, which we aggregately refer to as optimization friendliness, are identified and analyzed by a set of attention- and optimization-related diagnosis tools. With these properties, the new representations show strong fine-tuning performance. Specifically, the contrastive self-supervised learning methods are made as competitive in fine-tuning as the state-of-the-art masked image modeling (MIM) algorithms. The CLIP models' fine-tuning performance is also significantly improved, with a CLIP ViT-L model reaching 89.0\% top-1 accuracy on ImageNet-1K classification. On the 3-billion-parameter SwinV2-G model, the fine-tuning accuracy is improved by +1.5 mIoU / +1.1 mAP to 61.4 mIoU / 64.2 mAP on ADE20K semantic segmentation and COCO object detection, respectively, creating new records on both benchmarks. More importantly, our work provides a way for the future research to focus more effort on the generality and scalability of the learnt representations without being pre-occupied with optimization friendliness since it can be enhanced rather easily. The code will be available at https://github.com/SwinTransformer/Feature-Distillation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wei-Contrastive%20Learning%20Rivals%20Masked%20Image%20Modeling%20in%20Fine-tuning%20via%20Feature%20Distillation-2022-arXiv.pdf}
}
@article{NeRNAshkenaziICLR2023, 
year = {2023}, 
title = {{NeRN -- Learning Neural Representations for Neural Networks}}, 
author = {Ashkenazi, Maor and Rimon, Zohar and Vainshtein, Ron and Levi, Shir and Richardson, Elad and Mintz, Pinchas and Treister, Eran}, 
journal = {ICLR}, 
eprint = {2212.13554}, 
abstract = {{Neural Representations have recently been shown to effectively reconstruct a wide range of signals from 3D meshes and shapes to images and videos. We show that, when adapted correctly, neural representations can be used to directly represent the weights of a pre-trained convolutional neural network, resulting in a Neural Representation for Neural Networks (NeRN). Inspired by coordinate inputs of previous neural representation methods, we assign a coordinate to each convolutional kernel in our network based on its position in the architecture, and optimize a predictor network to map coordinates to their corresponding weights. Similarly to the spatial smoothness of visual scenes, we show that incorporating a smoothness constraint over the original network's weights aids NeRN towards a better reconstruction. In addition, since slight perturbations in pre-trained model weights can result in a considerable accuracy loss, we employ techniques from the field of knowledge distillation to stabilize the learning process. We demonstrate the effectiveness of NeRN in reconstructing widely used architectures on CIFAR-10, CIFAR-100, and ImageNet. Finally, we present two applications using NeRN, demonstrating the capabilities of the learned representations.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ashkenazi-NeRN%20--%20Learning%20Neural%20Representations%20for%20Neural%20Networks-2023-ICLR.pdf}
}
@article{WhatGalilICLR2023, 
year = {2023}, 
title = {{What Can We Learn From The Selective Prediction And Uncertainty Estimation Performance Of 523 Imagenet Classifiers}}, 
author = {Galil, Ido and Dabbah, Mohammed and El-Yaniv, Ran}, 
journal = {ICLR}, 
eprint = {2302.11874}, 
abstract = {{When deployed for risk-sensitive tasks, deep neural networks must include an uncertainty estimation mechanism. Here we examine the relationship between deep architectures and their respective training regimes, with their corresponding selective prediction and uncertainty estimation performance. We consider some of the most popular estimation performance metrics previously proposed including AUROC, ECE, AURC as well as coverage for selective accuracy constraint. We present a novel and comprehensive study of selective prediction and the uncertainty estimation performance of 523 existing pretrained deep ImageNet classifiers that are available in popular repositories. We identify numerous and previously unknown factors that affect uncertainty estimation and examine the relationships between the different metrics. We find that distillation-based training regimes consistently yield better uncertainty estimations than other training schemes such as vanilla training, pretraining on a larger dataset and adversarial training. Moreover, we find a subset of ViT models that outperform any other models in terms of uncertainty estimation performance. For example, we discovered an unprecedented 99\% top-1 selective accuracy on ImageNet at 47\% coverage (and 95\% top-1 accuracy at 80\%) for a ViT model, whereas a competing EfficientNet-V2-XL cannot obtain these accuracy constraints at any level of coverage. Our companion paper, also published in ICLR 2023 (A framework for benchmarking class-out-of-distribution detection and its application to ImageNet), examines the performance of these classifiers in a class-out-of-distribution setting.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Galil-What%20Can%20We%20Learn%20From%20The%20Selective%20Prediction%20And%20Uncertainty%20Estimation%20Performance%20Of%20523%20Imagenet%20Classifiers-2023-ICLR.pdf}
}
@article{LATSChenICLR2023, 
year = {2023}, 
title = {{A Unified Framework for Soft Threshold Pruning}}, 
author = {Chen, Yanqi and Ma, Zhengyu and Fang, Wei and Zheng, Xiawu and Yu, Zhaofei and Tian, Yonghong}, 
journal = {ICLR}, 
eprint = {2302.13019}, 
abstract = {{Soft threshold pruning is among the cutting-edge pruning methods with state-of-the-art performance. However, previous methods either perform aimless searching on the threshold scheduler or simply set the threshold trainable, lacking theoretical explanation from a unified perspective. In this work, we reformulate soft threshold pruning as an implicit optimization problem solved using the Iterative Shrinkage-Thresholding Algorithm (ISTA), a classic method from the fields of sparse recovery and compressed sensing. Under this theoretical framework, all threshold tuning strategies proposed in previous studies of soft threshold pruning are concluded as different styles of tuning \$L\_1\$-regularization term. We further derive an optimal threshold scheduler through an in-depth study of threshold scheduling based on our framework. This scheduler keeps \$L\_1\$-regularization coefficient stable, implying a time-invariant objective function from the perspective of optimization. In principle, the derived pruning algorithm could sparsify any mathematical model trained via SGD. We conduct extensive experiments and verify its state-of-the-art performance on both Artificial Neural Networks (ResNet-50 and MobileNet-V1) and Spiking Neural Networks (SEW ResNet-18) on ImageNet datasets. On the basis of this framework, we derive a family of pruning methods, including sparsify-during-training, early pruning, and pruning at initialization. The code is available at https://github.com/Yanqi-Chen/LATS.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-A%20Unified%20Framework%20for%20Soft%20Threshold%20Pruning-2023-ICLR.pdf}
}
@article{WhyEisenmannCVPR2023, 
year = {2023}, 
title = {{Why is the winner the best?}}, 
author = {Eisenmann, Matthias and Reinke, Annika and Weru, Vivienn and Tizabi, Minu Dietlinde and Isensee, Fabian and Adler, Tim J and Ali, Sharib and Andrearczyk, Vincent and Aubreville, Marc and Baid, Ujjwal and Bakas, Spyridon and Balu, Niranjan and Bano, Sophia and Bernal, Jorge and Bodenstedt, Sebastian and Casella, Alessandro and Cheplygina, Veronika and Daum, Marie and Bruijne, Marleen de and Depeursinge, Adrien and Dorent, Reuben and Egger, Jan and Ellis, David G and Engelhardt, Sandy and Ganz, Melanie and Ghatwary, Noha and Girard, Gabriel and Godau, Patrick and Gupta, Anubha and Hansen, Lasse and Harada, Kanako and Heinrich, Mattias and Heller, Nicholas and Hering, Alessa and Huaulmé, Arnaud and Jannin, Pierre and Kavur, Ali Emre and Kodym, Oldřich and Kozubek, Michal and Li, Jianning and Li, Hongwei and Ma, Jun and Martín-Isla, Carlos and Menze, Bjoern and Noble, Alison and Oreiller, Valentin and Padoy, Nicolas and Pati, Sarthak and Payette, Kelly and Rädsch, Tim and Rafael-Patiño, Jonathan and Bawa, Vivek Singh and Speidel, Stefanie and Sudre, Carole H and Wijnen, Kimberlin van and Wagner, Martin and Wei, Donglai and Yamlahi, Amine and Yap, Moi Hoon and Yuan, Chun and Zenk, Maximilian and Zia, Aneeq and Zimmerer, David and Aydogan, Dogu Baran and Bhattarai, Binod and Bloch, Louise and Brüngel, Raphael and Cho, Jihoon and Choi, Chanyeol and Dou, Qi and Ezhov, Ivan and Friedrich, Christoph M and Fuller, Clifton and Gaire, Rebati Raman and Galdran, Adrian and Faura, Álvaro García and Grammatikopoulou, Maria and Hong, SeulGi and Jahanifar, Mostafa and Jang, Ikbeom and Kadkhodamohammadi, Abdolrahim and Kang, Inha and Kofler, Florian and Kondo, Satoshi and Kuijf, Hugo and Li, Mingxing and Luu, Minh Huan and Martinčič, Tomaž and Morais, Pedro and Naser, Mohamed A and Oliveira, Bruno and Owen, David and Pang, Subeen and Park, Jinah and Park, Sung-Hong and Płotka, Szymon and Puybareau, Elodie and Rajpoot, Nasir and Ryu, Kanghyun and Saeed, Numan and Shephard, Adam and Shi, Pengcheng and Štepec, Dejan and Subedi, Ronast and Tochon, Guillaume and Torres, Helena R and Urien, Helene and Vilaça, João L and Wahid, Kareem Abdul and Wang, Haojie and Wang, Jiacheng and Wang, Liansheng and Wang, Xiyue and Wiestler, Benedikt and Wodzinski, Marek and Xia, Fangfang and Xie, Juanying and Xiong, Zhiwei and Yang, Sen and Yang, Yanwu and Zhao, Zixuan and Maier-Hein, Klaus and Jäger, Paul F and Kopp-Schneider, Annette and Maier-Hein, Lena}, 
journal = {CVPR}, 
eprint = {2303.17719}, 
abstract = {{International benchmarking competitions have become fundamental for the comparative performance assessment of image analysis methods. However, little attention has been given to investigating what can be learnt from these competitions. Do they really generate scientific progress? What are common and successful participation strategies? What makes a solution superior to a competing method? To address this gap in the literature, we performed a multi-center study with all 80 competitions that were conducted in the scope of IEEE ISBI 2021 and MICCAI 2021. Statistical analyses performed based on comprehensive descriptions of the submitted algorithms linked to their rank as well as the underlying participation strategies revealed common characteristics of winning solutions. These typically include the use of multi-task learning (63\%) and/or multi-stage pipelines (61\%), and a focus on augmentation (100\%), image preprocessing (97\%), data curation (79\%), and postprocessing (66\%). The "typical" lead of a winning team is a computer scientist with a doctoral degree, five years of experience in biomedical image analysis, and four years of experience in deep learning. Two core general development strategies stood out for highly-ranked teams: the reflection of the metrics in the method design and the focus on analyzing and handling failure cases. According to the organizers, 43\% of the winning algorithms exceeded the state of the art but only 11\% completely solved the respective domain problem. The insights of our study could help researchers (1) improve algorithm development strategies when approaching new problems, and (2) focus on open research questions revealed by this work.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Eisenmann-Why%20is%20the%20winner%20the%20best--2023-CVPR.pdf}
}
@article{MaskFormerChengNeurIPS2021, 
year = {2021}, 
title = {{Per-Pixel Classification is Not All You Need for Semantic Segmentation}}, 
author = {Cheng, Bowen and Schwing, Alexander G and Kirillov, Alexander}, 
journal = {NeurIPS}, 
eprint = {2107.06278}, 
abstract = {{Modern approaches typically formulate semantic segmentation as a per-pixel classification task, while instance-level segmentation is handled with an alternative mask classification. Our key insight: mask classification is sufficiently general to solve both semantic- and instance-level segmentation tasks in a unified manner using the exact same model, loss, and training procedure. Following this observation, we propose MaskFormer, a simple mask classification model which predicts a set of binary masks, each associated with a single global class label prediction. Overall, the proposed mask classification-based method simplifies the landscape of effective approaches to semantic and panoptic segmentation tasks and shows excellent empirical results. In particular, we observe that MaskFormer outperforms per-pixel classification baselines when the number of classes is large. Our mask classification-based method outperforms both current state-of-the-art semantic (55.6 mIoU on ADE20K) and panoptic segmentation (52.7 PQ on COCO) models.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Cheng-Per-Pixel%20Classification%20is%20Not%20All%20You%20Need%20for%20Semantic%20Segmentation-2021-NeurIPS.pdf}
}
@article{InteractiveLiCVPR2018, 
year = {2018}, 
title = {{Interactive Image Segmentation with Latent Diversity}}, 
author = {Li, Zhuwen and Chen, Qifeng and Koltun, Vladlen}, 
journal = {CVPR}, 
abstract = {{Interactive image segmentation is characterized by multimodality. When the user clicks on a door, do they intend to select the door or the whole house? We present an end-to-end learning approach to interactive image segmentation that tackles this ambiguity. Our architecture couples two convolutional networks. The first is trained to synthesize a diverse set of plausible segmentations that conform to the user's input. The second is trained to select among these. By selecting a single solution, our approach retains compatibility with existing interactive segmentation interfaces. By synthesizing multiple diverse solutions before selecting one, the architecture is given the representational power to explore the multimodal solution space. We show that the proposed approach outperforms existing methods for interactive image segmentation, including prior work that applied convolutional networks to this problem, while being much faster.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Interactive%20Image%20Segmentation%20with%20Latent%20Diversity-2018-CVPR.pdf}
}
@article{DeepXuCVPR2016, 
year = {2016}, 
title = {{Deep Interactive Object Selection}}, 
author = {Xu, Ning and Price, Brian and Cohen, Scott and Yang, Jimei and Huang, Thomas}, 
journal = {CVPR}, 
abstract = {{Interactive object selection is a very important research problem and has many applications. Previous algorithms require substantial user interactions to estimate the foreground and background distributions. In this paper, we present a novel deep-learning-based algorithm which has much better understanding of objectness and can reduce user interactions to just a few clicks. Our algorithm transforms user-provided positive and negative clicks into two Euclidean distance maps which are then concatenated with the RGB channels of images to compose (image, user interactions) pairs. We generate many of such pairs by combining several random sampling strategies to model users' click patterns and use them to finetune deep Fully Convolutional Networks (FCNs). Finally the output probability maps of our FCN-8s model is integrated with graph cut optimization to refine the boundary segments. Our model is trained on the PASCAL segmentation dataset and evaluated on other datasets with different object classes. Experimental results on both seen and unseen objects demonstrate that our algorithm has a good generalization ability and is superior to all existing interactive object selection approaches.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xu-Deep%20Interactive%20Object%20Selection-2016-CVPR.pdf}
}
@article{SAMKirillovICCV2023, 
year = {2023}, 
title = {{Segment Anything}}, 
author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollar, Piotr and Girshick, Ross}, 
journal = {ICCV}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kirillov-Segment%20Anything-2023-arXiv.pdf}
}
@article{NighttimeDaiITSC2018, 
year = {2018}, 
title = {{Dark Model Adaptation: Semantic Image Segmentation from Daytime to Nighttime}}, 
author = {Dai, Dengxin and Gool, Luc Van}, 
journal = {ITSC}, 
eprint = {1810.02575}, 
abstract = {{This work addresses the problem of semantic image segmentation of nighttime scenes. Although considerable progress has been made in semantic image segmentation, it is mainly related to daytime scenarios. This paper proposes a novel method to progressive adapt the semantic models trained on daytime scenes, along with large-scale annotations therein, to nighttime scenes via the bridge of twilight time -- the time between dawn and sunrise, or between sunset and dusk. The goal of the method is to alleviate the cost of human annotation for nighttime images by transferring knowledge from standard daytime conditions. In addition to the method, a new dataset of road scenes is compiled; it consists of 35,000 images ranging from daytime to twilight time and to nighttime. Also, a subset of the nighttime images are densely annotated for method evaluation. Our experiments show that our method is effective for model adaptation from daytime scenes to nighttime scenes, without using extra human annotation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Dai-Dark%20Model%20Adaptation-%20Semantic%20Image%20Segmentation%20from%20Daytime%20to%20Nighttime-2018-ITSC.pdf}
}
@article{DarkZurichSakaridisICCV2019, 
year = {2019}, 
title = {{Guided Curriculum Model Adaptation and Uncertainty-Aware Evaluation for Semantic Nighttime Image Segmentation}}, 
author = {Sakaridis, Christos and Dai, Dengxin and Gool, Luc Van}, 
journal = {ICCV}, 
eprint = {1901.05946}, 
abstract = {{Most progress in semantic segmentation reports on daytime images taken under favorable illumination conditions. We instead address the problem of semantic segmentation of nighttime images and improve the state-of-the-art, by adapting daytime models to nighttime without using nighttime annotations. Moreover, we design a new evaluation framework to address the substantial uncertainty of semantics in nighttime images. Our central contributions are: 1) a curriculum framework to gradually adapt semantic segmentation models from day to night via labeled synthetic images and unlabeled real images, both for progressively darker times of day, which exploits cross-time-of-day correspondences for the real images to guide the inference of their labels; 2) a novel uncertainty-aware annotation and evaluation framework and metric for semantic segmentation, designed for adverse conditions and including image regions beyond human recognition capability in the evaluation in a principled fashion; 3) the Dark Zurich dataset, which comprises 2416 unlabeled nighttime and 2920 unlabeled twilight images with correspondences to their daytime counterparts plus a set of 151 nighttime images with fine pixel-level annotations created with our protocol, which serves as a first benchmark to perform our novel evaluation. Experiments show that our guided curriculum adaptation significantly outperforms state-of-the-art methods on real nighttime sets both for standard metrics and our uncertainty-aware metric. Furthermore, our uncertainty-aware evaluation reveals that selective invalidation of predictions can lead to better results on data with ambiguous content such as our nighttime benchmark and profit safety-oriented applications which involve invalid inputs.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Sakaridis-Guided%20Curriculum%20Model%20Adaptation%20and%20Uncertainty-Aware%20Evaluation%20for%20Semantic%20Nighttime%20Image%20Segmentation-2019-ICCV.pdf}
}
@article{GPCISZhouCVPR2023, 
year = {2023}, 
title = {{Interactive Segmentation as Gaussian Process Classification}}, 
author = {Zhou, Minghao and Wang, Hong and Zhao, Qian and Li, Yuexiang and Huang, Yawen and Meng, Deyu and Zheng, Yefeng}, 
journal = {CVPR}, 
eprint = {2302.14578}, 
abstract = {{Click-based interactive segmentation (IS) aims to extract the target objects under user interaction. For this task, most of the current deep learning (DL)-based methods mainly follow the general pipelines of semantic segmentation. Albeit achieving promising performance, they do not fully and explicitly utilize and propagate the click information, inevitably leading to unsatisfactory segmentation results, even at clicked points. Against this issue, in this paper, we propose to formulate the IS task as a Gaussian process (GP)-based pixel-wise binary classification model on each image. To solve this model, we utilize amortized variational inference to approximate the intractable GP posterior in a data-driven manner and then decouple the approximated GP posterior into double space forms for efficient sampling with linear complexity. Then, we correspondingly construct a GP classification framework, named GPCIS, which is integrated with the deep kernel learning mechanism for more flexibility. The main specificities of the proposed GPCIS lie in: 1) Under the explicit guidance of the derived GP posterior, the information contained in clicks can be finely propagated to the entire image and then boost the segmentation; 2) The accuracy of predictions at clicks has good theoretical support. These merits of GPCIS as well as its good generality and high efficiency are substantiated by comprehensive experiments on several benchmarks, as compared with representative methods both quantitatively and qualitatively.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhou-Interactive%20Segmentation%20as%20Gaussian%20Process%20Classification-2023-CVPR.pdf}
}
@article{G2SDHuangCVPR2023, 
year = {2023}, 
title = {{Generic-to-Specific Distillation of Masked Autoencoders}}, 
author = {Huang, Wei and Peng, Zhiliang and Dong, Li and Wei, Furu and Jiao, Jianbin and Ye, Qixiang}, 
journal = {CVPR}, 
eprint = {2302.14771}, 
abstract = {{Large vision Transformers (ViTs) driven by self-supervised pre-training mechanisms achieved unprecedented progress. Lightweight ViT models limited by the model capacity, however, benefit little from those pre-training mechanisms. Knowledge distillation defines a paradigm to transfer representations from large (teacher) models to small (student) ones. However, the conventional single-stage distillation easily gets stuck on task-specific transfer, failing to retain the task-agnostic knowledge crucial for model generalization. In this study, we propose generic-to-specific distillation (G2SD), to tap the potential of small ViT models under the supervision of large models pre-trained by masked autoencoders. In generic distillation, decoder of the small model is encouraged to align feature predictions with hidden representations of the large model, so that task-agnostic knowledge can be transferred. In specific distillation, predictions of the small model are constrained to be consistent with those of the large model, to transfer task-specific features which guarantee task performance. With G2SD, the vanilla ViT-Small model respectively achieves 98.7\%, 98.1\% and 99.3\% the performance of its teacher (ViT-Base) for image classification, object detection, and semantic segmentation, setting a solid baseline for two-stage vision distillation. Code will be available at https://github.com/pengzhiliang/G2SD.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Huang-Generic-to-Specific%20Distillation%20of%20Masked%20Autoencoders-2023-CVPR.pdf}
}
@article{ToCoRuCVPR2023, 
year = {2023}, 
title = {{Token Contrast for Weakly-Supervised Semantic Segmentation}}, 
author = {Ru, Lixiang and Zheng, Heliang and Zhan, Yibing and Du, Bo}, 
journal = {CVPR}, 
eprint = {2303.01267}, 
abstract = {{Weakly-Supervised Semantic Segmentation (WSSS) using image-level labels typically utilizes Class Activation Map (CAM) to generate the pseudo labels. Limited by the local structure perception of CNN, CAM usually cannot identify the integral object regions. Though the recent Vision Transformer (ViT) can remedy this flaw, we observe it also brings the over-smoothing issue, \textbackslashie, the final patch tokens incline to be uniform. In this work, we propose Token Contrast (ToCo) to address this issue and further explore the virtue of ViT for WSSS. Firstly, motivated by the observation that intermediate layers in ViT can still retain semantic diversity, we designed a Patch Token Contrast module (PTC). PTC supervises the final patch tokens with the pseudo token relations derived from intermediate layers, allowing them to align the semantic regions and thus yield more accurate CAM. Secondly, to further differentiate the low-confidence regions in CAM, we devised a Class Token Contrast module (CTC) inspired by the fact that class tokens in ViT can capture high-level semantics. CTC facilitates the representation consistency between uncertain local regions and global objects by contrasting their class tokens. Experiments on the PASCAL VOC and MS COCO datasets show the proposed ToCo can remarkably surpass other single-stage competitors and achieve comparable performance with state-of-the-art multi-stage methods. Code is available at https://github.com/rulixiang/ToCo.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ru-Token%20Contrast%20for%20Weakly-Supervised%20Semantic%20Segmentation-2023-CVPR.pdf}
}
@article{CCVCWangCVPR2023, 
year = {2023}, 
title = {{Conflict-Based Cross-View Consistency for Semi-Supervised Semantic Segmentation}}, 
author = {Wang, Zicheng and Zhao, Zhen and Xing, Xiaoxia and Xu, Dong and Kong, Xiangyu and Zhou, Luping}, 
journal = {CVPR}, 
eprint = {2303.01276}, 
abstract = {{Semi-supervised semantic segmentation (SSS) has recently gained increasing research interest as it can reduce the requirement for large-scale fully-annotated training data. The current methods often suffer from the confirmation bias from the pseudo-labelling process, which can be alleviated by the co-training framework. The current co-training-based SSS methods rely on hand-crafted perturbations to prevent the different sub-nets from collapsing into each other, but these artificial perturbations cannot lead to the optimal solution. In this work, we propose a new conflict-based cross-view consistency (CCVC) method based on a two-branch co-training framework which aims at enforcing the two sub-nets to learn informative features from irrelevant views. In particular, we first propose a new cross-view consistency (CVC) strategy that encourages the two sub-nets to learn distinct features from the same input by introducing a feature discrepancy loss, while these distinct features are expected to generate consistent prediction scores of the input. The CVC strategy helps to prevent the two sub-nets from stepping into the collapse. In addition, we further propose a conflict-based pseudo-labelling (CPL) method to guarantee the model will learn more useful information from conflicting predictions, which will lead to a stable training process. We validate our new CCVC approach on the SSS benchmark datasets where our method achieves new state-of-the-art performance. Our code is available at https://github.com/xiaoyao3302/CCVC.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Conflict-Based%20Cross-View%20Consistency%20for%20Semi-Supervised%20Semantic%20Segmentation-2023-CVPR.pdf}
}
@article{LossMax-PoolingBuloCVPR2017, 
year = {2017}, 
title = {{Loss Max-Pooling for Semantic Image Segmentation}}, 
author = {Bulò, Samuel Rota and Neuhold, Gerhard and Kontschieder, Peter}, 
journal = {CVPR}, 
eprint = {1704.02966}, 
abstract = {{We introduce a novel loss max-pooling concept for handling imbalanced training data distributions, applicable as alternative loss layer in the context of deep neural networks for semantic image segmentation. Most real-world semantic segmentation datasets exhibit long tail distributions with few object categories comprising the majority of data and consequently biasing the classifiers towards them. Our method adaptively re-weights the contributions of each pixel based on their observed losses, targeting under-performing classification results as often encountered for under-represented object classes. Our approach goes beyond conventional cost-sensitive learning attempts through adaptive considerations that allow us to indirectly address both, inter- and intra-class imbalances. We provide a theoretical justification of our approach, complementary to experimental analyses on benchmark datasets. In our experiments on the Cityscapes and Pascal VOC 2012 segmentation datasets we find consistently improved results, demonstrating the efficacy of our approach.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bulò-Loss%20Max-Pooling%20for%20Semantic%20Image%20Segmentation-2017-CVPR.pdf}
}
@article{Over-trainingLiuICLR2023, 
year = {2023}, 
title = {{Over-training with Mixup May Hurt Generalization}}, 
author = {Liu, Zixuan and Wang, Ziqiao and Guo, Hongyu and Mao, Yongyi}, 
journal = {ICLR}, 
eprint = {2303.01475}, 
abstract = {{Mixup, which creates synthetic training instances by linearly interpolating random sample pairs, is a simple and yet effective regularization technique to boost the performance of deep models trained with SGD. In this work, we report a previously unobserved phenomenon in Mixup training: on a number of standard datasets, the performance of Mixup-trained models starts to decay after training for a large number of epochs, giving rise to a U-shaped generalization curve. This behavior is further aggravated when the size of original dataset is reduced. To help understand such a behavior of Mixup, we show theoretically that Mixup training may introduce undesired data-dependent label noises to the synthesized data. Via analyzing a least-square regression problem with a random feature model, we explain why noisy labels may cause the U-shaped curve to occur: Mixup improves generalization through fitting the clean patterns at the early training stage, but as training progresses, Mixup becomes over-fitting to the noise in the synthetic data. Extensive experiments are performed on a variety of benchmark datasets, validating this explanation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-Over-training%20with%20Mixup%20May%20Hurt%20Generalization-2023-ICLR.pdf}
}
@article{CMNEXTZhangCVPR2023, 
year = {2023}, 
title = {{Delivering Arbitrary-Modal Semantic Segmentation}}, 
author = {Zhang, Jiaming and Liu, Ruiping and Shi, Hao and Yang, Kailun and Reiß, Simon and Peng, Kunyu and Fu, Haodong and Wang, Kaiwei and Stiefelhagen, Rainer}, 
journal = {CVPR}, 
eprint = {2303.01480}, 
abstract = {{Multimodal fusion can make semantic segmentation more robust. However, fusing an arbitrary number of modalities remains underexplored. To delve into this problem, we create the DeLiVER arbitrary-modal segmentation benchmark, covering Depth, LiDAR, multiple Views, Events, and RGB. Aside from this, we provide this dataset in four severe weather conditions as well as five sensor failure cases to exploit modal complementarity and resolve partial outages. To make this possible, we present the arbitrary cross-modal segmentation model CMNeXt. It encompasses a Self-Query Hub (SQ-Hub) designed to extract effective information from any modality for subsequent fusion with the RGB representation and adds only negligible amounts of parameters (\textbackslashtextasciitilde0.01M) per additional modality. On top, to efficiently and flexibly harvest discriminative cues from the auxiliary modalities, we introduce the simple Parallel Pooling Mixer (PPX). With extensive experiments on a total of six benchmarks, our CMNeXt achieves state-of-the-art performance on the DeLiVER, KITTI-360, MFNet, NYU Depth V2, UrbanLF, and MCubeS datasets, allowing to scale from 1 to 81 modalities. On the freshly collected DeLiVER, the quad-modal CMNeXt reaches up to 66.30\% in mIoU with a +9.10\% gain as compared to the mono-modal baseline. The DeLiVER dataset and our code are at: https://jamycheung.github.io/DELIVER.html.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Delivering%20Arbitrary-Modal%20Semantic%20Segmentation-2023-CVPR.pdf}
}
@article{ContextClustersMaICLR2023, 
year = {2023}, 
title = {{Image as Set of Points}}, 
author = {Ma, Xu and Zhou, Yuqian and Wang, Huan and Qin, Can and Sun, Bin and Liu, Chang and Fu, Yun}, 
journal = {ICLR}, 
eprint = {2303.01494}, 
abstract = {{What is an image and how to extract latent features? Convolutional Networks (ConvNets) consider an image as organized pixels in a rectangular shape and extract features via convolutional operation in local region; Vision Transformers (ViTs) treat an image as a sequence of patches and extract features via attention mechanism in a global range. In this work, we introduce a straightforward and promising paradigm for visual representation, which is called Context Clusters. Context clusters (CoCs) view an image as a set of unorganized points and extract features via simplified clustering algorithm. In detail, each point includes the raw feature (e.g., color) and positional information (e.g., coordinates), and a simplified clustering algorithm is employed to group and extract deep features hierarchically. Our CoCs are convolution- and attention-free, and only rely on clustering algorithm for spatial interaction. Owing to the simple design, we show CoCs endow gratifying interpretability via the visualization of clustering process. Our CoCs aim at providing a new perspective on image and visual representation, which may enjoy broad applications in different domains and exhibit profound insights. Even though we are not targeting SOTA performance, COCs still achieve comparable or even better results than ConvNets or ViTs on several benchmarks. Codes are available at: https://github.com/ma-xu/Context-Cluster.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ma-Image%20as%20Set%20of%20Points-2023-ICLR.pdf}
}
@article{DropoutLiuarXiv2023, 
year = {2023}, 
title = {{Dropout Reduces Underfitting}}, 
author = {Liu, Zhuang and Xu, Zhiqiu and Jin, Joseph and Shen, Zhiqiang and Darrell, Trevor}, 
journal = {arXiv}, 
eprint = {2303.01500}, 
abstract = {{Introduced by Hinton et al. in 2012, dropout has stood the test of time as a regularizer for preventing overfitting in neural networks. In this study, we demonstrate that dropout can also mitigate underfitting when used at the start of training. During the early phase, we find dropout reduces the directional variance of gradients across mini-batches and helps align the mini-batch gradients with the entire dataset's gradient. This helps counteract the stochasticity of SGD and limit the influence of individual batches on model training. Our findings lead us to a solution for improving performance in underfitting models - early dropout: dropout is applied only during the initial phases of training, and turned off afterwards. Models equipped with early dropout achieve lower final training loss compared to their counterparts without dropout. Additionally, we explore a symmetric technique for regularizing overfitting models - late dropout, where dropout is not used in the early iterations and is only activated later in training. Experiments on ImageNet and various vision tasks demonstrate that our methods consistently improve generalization accuracy. Our results encourage more research on understanding regularization in deep learning and our methods can be useful tools for future neural network training, especially in the era of large data. Code is available at https://github.com/facebookresearch/dropout .}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-Dropout%20Reduces%20Underfitting-2023-ICLR.pdf}
}
@article{SGPAChenICLR2023, 
year = {2023}, 
title = {{Calibrating Transformers via Sparse Gaussian Processes}}, 
author = {Chen, Wenlong and Li, Yingzhen}, 
journal = {ICLR}, 
eprint = {2303.02444}, 
abstract = {{Transformer models have achieved profound success in prediction tasks in a wide range of applications in natural language processing, speech recognition and computer vision. Extending Transformer's success to safety-critical domains requires calibrated uncertainty estimation which remains under-explored. To address this, we propose Sparse Gaussian Process attention (SGPA), which performs Bayesian inference directly in the output space of multi-head attention blocks (MHAs) in transformer to calibrate its uncertainty. It replaces the scaled dot-product operation with a valid symmetric kernel and uses sparse Gaussian processes (SGP) techniques to approximate the posterior processes of MHA outputs. Empirically, on a suite of prediction tasks on text, images and graphs, SGPA-based Transformers achieve competitive predictive accuracy, while noticeably improving both in-distribution calibration and out-of-distribution robustness and detection.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Calibrating%20Transformers%20via%20Sparse%20Gaussian%20Processes-2023-ICLR.pdf}
}
@article{UnderstandingQuistICLR2023, 
year = {2023}, 
title = {{Understanding weight-magnitude hyperparameters in training binary networks}}, 
author = {Quist, Joris and Li, Yunqiang and Gemert, Jan van}, 
journal = {ICLR}, 
eprint = {2303.02452}, 
abstract = {{Binary Neural Networks (BNNs) are compact and efficient by using binary weights instead of real-valued weights. Current BNNs use latent real-valued weights during training, where several training hyper-parameters are inherited from real-valued networks. The interpretation of several of these hyperparameters is based on the magnitude of the real-valued weights. For BNNs, however, the magnitude of binary weights is not meaningful, and thus it is unclear what these hyperparameters actually do. One example is weight-decay, which aims to keep the magnitude of real-valued weights small. Other examples are latent weight initialization, the learning rate, and learning rate decay, which influence the magnitude of the real-valued weights. The magnitude is interpretable for real-valued weights, but loses its meaning for binary weights. In this paper we offer a new interpretation of these magnitude-based hyperparameters based on higher-order gradient filtering during network optimization. Our analysis makes it possible to understand how magnitude-based hyperparameters influence the training of binary networks which allows for new optimization filters specifically designed for binary neural networks that are independent of their real-valued interpretation. Moreover, our improved understanding reduces the number of hyperparameters, which in turn eases the hyperparameter tuning effort which may lead to better hyperparameter values for improved accuracy. Code is available at https://github.com/jorisquist/Understanding-WM-HP-in-BNNs}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Quist-Understanding%20weight-magnitude%20hyperparameters%20in%20training%20binary%20networks-2023-ICLR.pdf}
}
@article{ESDYoonICLR2023, 
year = {2023}, 
title = {{ESD: Expected Squared Difference as a Tuning-Free Trainable Calibration Measure}}, 
author = {Yoon, Hee Suk and Tee, Joshua Tian Jin and Yoon, Eunseop and Yoon, Sunjae and Kim, Gwangsu and Li, Yingzhen and Yoo, Chang D}, 
journal = {ICLR}, 
eprint = {2303.02472}, 
abstract = {{Studies have shown that modern neural networks tend to be poorly calibrated due to over-confident predictions. Traditionally, post-processing methods have been used to calibrate the model after training. In recent years, various trainable calibration measures have been proposed to incorporate them directly into the training process. However, these methods all incorporate internal hyperparameters, and the performance of these calibration objectives relies on tuning these hyperparameters, incurring more computational costs as the size of neural networks and datasets become larger. As such, we present Expected Squared Difference (ESD), a tuning-free (i.e., hyperparameter-free) trainable calibration objective loss, where we view the calibration error from the perspective of the squared difference between the two expectations. With extensive experiments on several architectures (CNNs, Transformers) and datasets, we demonstrate that (1) incorporating ESD into the training improves model calibration in various batch size settings without the need for internal hyperparameter tuning, (2) ESD yields the best-calibrated results compared with previous approaches, and (3) ESD drastically improves the computational costs required for calibration during training due to the absence of internal hyperparameter. The code is publicly accessible at https://github.com/hee-suk-yoon/ESD.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yoon-ESD-%20Expected%20Squared%20Difference%20as%20a%20Tuning-Free%20Trainable%20Calibration%20Measure-2023-ICLR.pdf}
}
@article{SGSDetkovICLR2023, 
year = {2023}, 
title = {{Reparameterization through Spatial Gradient Scaling}}, 
author = {Detkov, Alexander and Salameh, Mohammad and Qharabagh, Muhammad Fetrat and Zhang, Jialin and Lui, Wei and Jui, Shangling and Niu, Di}, 
journal = {ICLR}, 
eprint = {2303.02733}, 
abstract = {{Reparameterization aims to improve the generalization of deep neural networks by transforming convolutional layers into equivalent multi-branched structures during training. However, there exists a gap in understanding how reparameterization may change and benefit the learning process of neural networks. In this paper, we present a novel spatial gradient scaling method to redistribute learning focus among weights in convolutional networks. We prove that spatial gradient scaling achieves the same learning dynamics as a branched reparameterization yet without introducing structural changes into the network. We further propose an analytical approach that dynamically learns scalings for each convolutional layer based on the spatial characteristics of its input feature map gauged by mutual information. Experiments on CIFAR-10, CIFAR-100, and ImageNet show that without searching for reparameterized structures, our proposed scaling method outperforms the state-of-the-art reparameterization strategies at a lower computational cost.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Detkov-Reparameterization%20through%20Spatial%20Gradient%20Scaling-2023-ICLR.pdf}
}
@article{null, 
year = {2022}, 
title = {{Rethinking Confidence Calibration for Failure Prediction}}, 
author = {Zhu, Fei and Cheng, Zhen and Zhang, Xu-Yao and Liu, Cheng-Lin}, 
journal = {ECCV}, 
eprint = {2303.02970}, 
abstract = {{Reliable confidence estimation for the predictions is important in many safety-critical applications. However, modern deep neural networks are often overconfident for their incorrect predictions. Recently, many calibration methods have been proposed to alleviate the overconfidence problem. With calibrated confidence, a primary and practical purpose is to detect misclassification errors by filtering out low-confidence predictions (known as failure prediction). In this paper, we find a general, widely-existed but actually-neglected phenomenon that most confidence calibration methods are useless or harmful for failure prediction. We investigate this problem and reveal that popular confidence calibration methods often lead to worse confidence separation between correct and incorrect samples, making it more difficult to decide whether to trust a prediction or not. Finally, inspired by the natural connection between flat minima and confidence separation, we propose a simple hypothesis: flat minima is beneficial for failure prediction. We verify this hypothesis via extensive experiments and further boost the performance by combining two different flat minima techniques. Our code is available at https://github.com/Impression2805/FMFP}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhu-Rethinking%20Confidence%20Calibration%20for%20Failure%20Prediction-2022-ECCV.pdf}
}
@article{LARSYouarXiv2017, 
year = {2017}, 
title = {{LARGE BATCH TRAINING OF CONVOLUTIONAL NETWORKS}}, 
author = {You, Yang and Gitman, Igor and Ginsburg, Boris}, 
journal = {arXiv}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/You-LARGE%20BATCH%20TRAINING%20OF%20CONVOLUTIONAL%20NETWORKS-2017-arXiv.pdf}
}
@article{LAMBYouICLR2020, 
year = {2020}, 
title = {{Large Batch Optimization for Deep Learning: Training BERT in 76 minutes}}, 
author = {You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui}, 
journal = {ICLR}, 
eprint = {1904.00962}, 
abstract = {{Training large deep neural networks on massive datasets is computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance. By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes (Table 1). The LAMB implementation is available at https://github.com/tensorflow/addons/blob/master/tensorflow\_addons/optimizers/lamb.py}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/You-Large%20Batch%20Optimization%20for%20Deep%20Learning-%20Training%20BERT%20in%2076%20minutes-2020-ICLR.pdf}
}
@article{OnKeskarICLR2017, 
year = {2017}, 
title = {{On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima}}, 
author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter}, 
journal = {ICLR}, 
eprint = {1609.04836}, 
abstract = {{The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say \$32\$-\$512\$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Keskar-On%20Large-Batch%20Training%20for%20Deep%20Learning-%20Generalization%20Gap%20and%20Sharp%20Minima-2017-ICLR.pdf}
}
@article{MobileViTMehtaICLR2022, 
year = {2022}, 
title = {{MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer}}, 
author = {Mehta, Sachin and Rastegari, Mohammad}, 
journal = {ICLR}, 
eprint = {2110.02178}, 
abstract = {{Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4\% with about 6 million parameters, which is 3.2\% and 6.2\% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7\% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: https://github.com/apple/ml-cvnets}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Mehta-MobileViT-%20Light-weight,%20General-purpose,%20and%20Mobile-friendly%20Vision%20Transformer-2022-ICLR.pdf}
}
@article{MobileViTV2MehtaTMLR2023, 
year = {2023}, 
title = {{Separable Self-attention for Mobile Vision Transformers}}, 
author = {Mehta, Sachin and Rastegari, Mohammad}, 
journal = {TMLR}, 
eprint = {2206.02680}, 
abstract = {{Mobile vision transformers (MobileViT) can achieve state-of-the-art performance across several mobile vision tasks, including classification and detection. Though these models have fewer parameters, they have high latency as compared to convolutional neural network-based models. The main efficiency bottleneck in MobileViT is the multi-headed self-attention (MHA) in transformers, which requires \$O(k\textasciicircum2)\$ time complexity with respect to the number of tokens (or patches) \$k\$. Moreover, MHA requires costly operations (e.g., batch-wise matrix multiplication) for computing self-attention, impacting latency on resource-constrained devices. This paper introduces a separable self-attention method with linear complexity, i.e. \$O(k)\$. A simple yet effective characteristic of the proposed method is that it uses element-wise operations for computing self-attention, making it a good choice for resource-constrained devices. The improved model, MobileViTv2, is state-of-the-art on several mobile vision tasks, including ImageNet object classification and MS-COCO object detection. With about three million parameters, MobileViTv2 achieves a top-1 accuracy of 75.6\% on the ImageNet dataset, outperforming MobileViT by about 1\% while running \$3.2\textbackslashtimes\$ faster on a mobile device. Our source code is available at: \textbackslashurl\{https://github.com/apple/ml-cvnets\}}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Mehta-Separable%20Self-attention%20for%20Mobile%20Vision%20Transformers-2023-TMLR.pdf}
}
@article{MobileViTV3WadekararXiv2022, 
year = {2022}, 
title = {{MobileViTv3: Mobile-Friendly Vision Transformer with Simple and Effective Fusion of Local, Global and Input Features}}, 
author = {Wadekar, Shakti N and Chaurasia, Abhishek}, 
journal = {arXiv}, 
eprint = {2209.15159}, 
abstract = {{MobileViT (MobileViTv1) combines convolutional neural networks (CNNs) and vision transformers (ViTs) to create light-weight models for mobile vision tasks. Though the main MobileViTv1-block helps to achieve competitive state-of-the-art results, the fusion block inside MobileViTv1-block, creates scaling challenges and has a complex learning task. We propose changes to the fusion block that are simple and effective to create MobileViTv3-block, which addresses the scaling and simplifies the learning task. Our proposed MobileViTv3-block used to create MobileViTv3-XXS, XS and S models outperform MobileViTv1 on ImageNet-1k, ADE20K, COCO and PascalVOC2012 datasets. On ImageNet-1K, MobileViTv3-XXS and MobileViTv3-XS surpasses MobileViTv1-XXS and MobileViTv1-XS by 2\% and 1.9\% respectively. Recently published MobileViTv2 architecture removes fusion block and uses linear complexity transformers to perform better than MobileViTv1. We add our proposed fusion block to MobileViTv2 to create MobileViTv3-0.5, 0.75 and 1.0 models. These new models give better accuracy numbers on ImageNet-1k, ADE20K, COCO and PascalVOC2012 datasets as compared to MobileViTv2. MobileViTv3-0.5 and MobileViTv3-0.75 outperforms MobileViTv2-0.5 and MobileViTv2-0.75 by 2.1\% and 1.0\% respectively on ImageNet-1K dataset. For segmentation task, MobileViTv3-1.0 achieves 2.07\% and 1.1\% better mIOU compared to MobileViTv2-1.0 on ADE20K dataset and PascalVOC2012 dataset respectively. Our code and the trained models are available at: https://github.com/micronDLA/MobileViTv3}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wadekar-MobileViTv3-%20Mobile-Friendly%20Vision%20Transformer%20with%20Simple%20and%20Effective%20Fusion%20of%20Local,%20Global%20and%20Input%20Features-2022-arXiv.pdf}
}
@article{FasterNetChenCVPR2023, 
year = {2023}, 
title = {{Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks}}, 
author = {Chen, Jierun and Kao, Shiu-hong and He, Hao and Zhuo, Weipeng and Wen, Song and Lee, Chul-Ho and Chan, S -H Gary}, 
journal = {CVPR}, 
eprint = {2303.03667}, 
abstract = {{To design fast neural networks, many works have been focusing on reducing the number of floating-point operations (FLOPs). We observe that such reduction in FLOPs, however, does not necessarily lead to a similar level of reduction in latency. This mainly stems from inefficiently low floating-point operations per second (FLOPS). To achieve faster networks, we revisit popular operators and demonstrate that such low FLOPS is mainly due to frequent memory access of the operators, especially the depthwise convolution. We hence propose a novel partial convolution (PConv) that extracts spatial features more efficiently, by cutting down redundant computation and memory access simultaneously. Building upon our PConv, we further propose FasterNet, a new family of neural networks, which attains substantially higher running speed than others on a wide range of devices, without compromising on accuracy for various vision tasks. For example, on ImageNet-1k, our tiny FasterNet-T0 is \$2.8\textbackslashtimes\$, \$3.3\textbackslashtimes\$, and \$2.4\textbackslashtimes\$ faster than MobileViT-XXS on GPU, CPU, and ARM processors, respectively, while being \$2.9\textbackslash\%\$ more accurate. Our large FasterNet-L achieves impressive \$83.5\textbackslash\%\$ top-1 accuracy, on par with the emerging Swin-B, while having \$36\textbackslash\%\$ higher inference throughput on GPU, as well as saving \$37\textbackslash\%\$ compute time on CPU. Code is available at \textbackslashurl\{https://github.com/JierunChen/FasterNet\}.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Run,%20Don't%20Walk-%20Chasing%20Higher%20FLOPS%20for%20Faster%20Neural%20Networks-2023-CVPR.pdf}
}
@article{MimeLossKervadecarXiv2023, 
year = {2023}, 
title = {{On the dice loss gradient and the ways to mimic it}}, 
author = {Kervadec, Hoel and Bruijne, Marleen de}, 
journal = {arXiv}, 
eprint = {2304.04319}, 
abstract = {{In the past few years, in the context of fully-supervised semantic segmentation, several losses -- such as cross-entropy and dice -- have emerged as de facto standards to supervise neural networks. The Dice loss is an interesting case, as it comes from the relaxation of the popular Dice coefficient; one of the main evaluation metric in medical imaging applications. In this paper, we first study theoretically the gradient of the dice loss, showing that concretely it is a weighted negative of the ground truth, with a very small dynamic range. This enables us, in the second part of this paper, to mimic the supervision of the dice loss, through a simple element-wise multiplication of the network output with a negative of the ground truth. This rather surprising result sheds light on the practical supervision performed by the dice loss during gradient descent. This can help the practitioner to understand and interpret results while guiding researchers when designing new losses.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kervadec-On%20the%20dice%20loss%20gradient%20and%20the%20ways%20to%20mimic%20it-2023-arXiv.pdf}
}
@article{FixMatchSohnNeurIPS2020, 
year = {2020}, 
title = {{FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence}}, 
author = {Sohn, Kihyuk and Berthelot, David and Li, Chun-Liang and Zhang, Zizhao and Carlini, Nicholas and Cubuk, Ekin D and Kurakin, Alex and Zhang, Han and Raffel, Colin}, 
journal = {NeurIPS}, 
eprint = {2001.07685}, 
abstract = {{Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model's performance. In this paper, we demonstrate the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling. Our algorithm, FixMatch, first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93\% accuracy on CIFAR-10 with 250 labels and 88.61\% accuracy with 40 -- just 4 labels per class. Since FixMatch bears many similarities to existing SSL methods that achieve worse performance, we carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch's success. We make our code available at https://github.com/google-research/fixmatch.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Sohn-FixMatch-%20Simplifying%20Semi-Supervised%20Learning%20with%20Consistency%20and%20Confidence-2020-NeurIPS.pdf}
}
@article{ReMixMatchBerthelotICLR2020, 
year = {2020}, 
title = {{ReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation Anchoring}}, 
author = {Berthelot, David and Carlini, Nicholas and Cubuk, Ekin D and Kurakin, Alex and Sohn, Kihyuk and Zhang, Han and Raffel, Colin}, 
journal = {ICLR}, 
eprint = {1911.09785}, 
abstract = {{We improve the recently-proposed "MixMatch" semi-supervised learning algorithm by introducing two new techniques: distribution alignment and augmentation anchoring. Distribution alignment encourages the marginal distribution of predictions on unlabeled data to be close to the marginal distribution of ground-truth labels. Augmentation anchoring feeds multiple strongly augmented versions of an input into the model and encourages each output to be close to the prediction for a weakly-augmented version of the same input. To produce strong augmentations, we propose a variant of AutoAugment which learns the augmentation policy while the model is being trained. Our new algorithm, dubbed ReMixMatch, is significantly more data-efficient than prior work, requiring between \$5\textbackslashtimes\$ and \$16\textbackslashtimes\$ less data to reach the same accuracy. For example, on CIFAR-10 with 250 labeled examples we reach \$93.73\textbackslash\%\$ accuracy (compared to MixMatch's accuracy of \$93.58\textbackslash\%\$ with \$4\{,\}000\$ examples) and a median accuracy of \$84.92\textbackslash\%\$ with just four labels per class. We make our code and data open-source at https://github.com/google-research/remixmatch.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Berthelot-ReMixMatch-%20Semi-Supervised%20Learning%20with%20Distribution%20Alignment%20and%20Augmentation%20Anchoring-2020-ICLR.pdf}
}
@article{UniMatchYangCVPR2023, 
year = {2023}, 
title = {{Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation}}, 
author = {Yang, Lihe and Qi, Lei and Feng, Litong and Zhang, Wayne and Shi, Yinghuan}, 
journal = {CVPR}, 
eprint = {2208.09910}, 
abstract = {{In this work, we revisit the weak-to-strong consistency framework, popularized by FixMatch from semi-supervised classification, where the prediction of a weakly perturbed image serves as supervision for its strongly perturbed version. Intriguingly, we observe that such a simple pipeline already achieves competitive results against recent advanced works, when transferred to our segmentation scenario. Its success heavily relies on the manual design of strong data augmentations, however, which may be limited and inadequate to explore a broader perturbation space. Motivated by this, we propose an auxiliary feature perturbation stream as a supplement, leading to an expanded perturbation space. On the other, to sufficiently probe original image-level augmentations, we present a dual-stream perturbation technique, enabling two strong views to be simultaneously guided by a common weak view. Consequently, our overall Unified Dual-Stream Perturbations approach (UniMatch) surpasses all existing methods significantly across all evaluation protocols on the Pascal, Cityscapes, and COCO benchmarks. We also demonstrate the superiority of our method in remote sensing interpretation and medical image analysis. Code is available at https://github.com/LiheYoung/UniMatch.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-Revisiting%20Weak-to-Strong%20Consistency%20in%20Semi-Supervised%20Semantic%20Segmentation-2023-CVPR.pdf}
}
@article{FocalClickChenCVPR2022, 
year = {2022}, 
title = {{FocalClick: Towards Practical Interactive Image Segmentation}}, 
author = {Chen, Xi and Zhao, Zhiyan and Zhang, Yilei and Duan, Manni and Qi, Donglian and Zhao, Hengshuang}, 
journal = {CVPR}, 
abstract = {{Interactive segmentation allows users to extract target masks by making positive/negative clicks. Although explored by many previous works, there is still a gap between academic approaches and industrial needs: first, existing models are not efficient enough to work on low-power devices; second, they perform poorly when used to refine preexisting masks as they could not avoid destroying the correct part. FocalClick solves both issues at once by predicting and updating the mask in localized areas. For higher efficiency, we decompose the slow prediction on the entire image into two fast inferences on small crops: a coarse segmentation on the Target Crop, and a local refinement on the Focus Crop. To make the model work with preexisting masks, we formulate a sub-task termed Inter-active Mask Correction, and propose Progressive Merge as the solution. Progressive Merge exploits morphological information to decide where to preserve and where to update, enabling users to refine any preexisting mask effectively. FocalClick achieves competitive results against SOTA methods with significantly smaller FLOPs. It also shows significant superiority when making corrections on preexisting masks. Code and data will be released at github.com/XavierCHEN34/ClickSEG}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-FocalClick-%20Towards%20Practical%20Interactive%20Image%20Segmentation-2022-CVPR.pdf}
}
@article{SCPNetXiaCVPR2023, 
year = {2023}, 
title = {{SCPNet: Semantic Scene Completion on Point Cloud}}, 
author = {Xia, Zhaoyang and Liu, Youquan and Li, Xin and Zhu, Xinge and Ma, Yuexin and Li, Yikang and Hou, Yuenan and Qiao, Yu}, 
journal = {CVPR}, 
eprint = {2303.06884}, 
abstract = {{Training deep models for semantic scene completion (SSC) is challenging due to the sparse and incomplete input, a large quantity of objects of diverse scales as well as the inherent label noise for moving objects. To address the above-mentioned problems, we propose the following three solutions: 1) Redesigning the completion sub-network. We design a novel completion sub-network, which consists of several Multi-Path Blocks (MPBs) to aggregate multi-scale features and is free from the lossy downsampling operations. 2) Distilling rich knowledge from the multi-frame model. We design a novel knowledge distillation objective, dubbed Dense-to-Sparse Knowledge Distillation (DSKD). It transfers the dense, relation-based semantic knowledge from the multi-frame teacher to the single-frame student, significantly improving the representation learning of the single-frame model. 3) Completion label rectification. We propose a simple yet effective label rectification strategy, which uses off-the-shelf panoptic segmentation labels to remove the traces of dynamic objects in completion labels, greatly improving the performance of deep models especially for those moving objects. Extensive experiments are conducted in two public SSC benchmarks, i.e., SemanticKITTI and SemanticPOSS. Our SCPNet ranks 1st on SemanticKITTI semantic scene completion challenge and surpasses the competitive S3CNet by 7.2 mIoU. SCPNet also outperforms previous completion algorithms on the SemanticPOSS dataset. Besides, our method also achieves competitive results on SemanticKITTI semantic segmentation tasks, showing that knowledge learned in the scene completion is beneficial to the segmentation task.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xia-SCPNet-%20Semantic%20Scene%20Completion%20on%20Point%20Cloud-2023-CVPR.pdf}
}
@article{HRFormerYuanNeurIPS2021, 
year = {2021}, 
title = {{HRFormer: High-Resolution Transformer for Dense Prediction}}, 
author = {Yuan, Yuhui and Fu, Rao and Huang, Lang and Lin, Weihong and Zhang, Chao and Chen, Xilin and Wang, Jingdong}, 
journal = {NeurIPS}, 
eprint = {2110.09408}, 
abstract = {{We present a High-Resolution Transformer (HRFormer) that learns high-resolution representations for dense prediction tasks, in contrast to the original Vision Transformer that produces low-resolution representations and has high memory and computational cost. We take advantage of the multi-resolution parallel design introduced in high-resolution convolutional networks (HRNet), along with local-window self-attention that performs self-attention over small non-overlapping image windows, for improving the memory and computation efficiency. In addition, we introduce a convolution into the FFN to exchange information across the disconnected image windows. We demonstrate the effectiveness of the High-Resolution Transformer on both human pose estimation and semantic segmentation tasks, e.g., HRFormer outperforms Swin transformer by \$1.3\$ AP on COCO pose estimation with \$50\textbackslash\%\$ fewer parameters and \$30\textbackslash\%\$ fewer FLOPs. Code is available at: https://github.com/HRNet/HRFormer.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yuan-HRFormer-%20High-Resolution%20Transformer%20for%20Dense%20Prediction-2021-NeurIPS.pdf}
}
@article{OCRYuanECCV2020, 
year = {2020}, 
title = {{Object-Contextual Representations for Semantic Segmentation}}, 
author = {Yuan, Yuhui and Chen, Xiaokang and Chen, Xilin and Wang, Jingdong}, 
journal = {ECCV}, 
eprint = {1909.11065}, 
abstract = {{In this paper, we address the semantic segmentation problem with a focus on the context aggregation strategy. Our motivation is that the label of a pixel is the category of the object that the pixel belongs to. We present a simple yet effective approach, object-contextual representations, characterizing a pixel by exploiting the representation of the corresponding object class. First, we learn object regions under the supervision of ground-truth segmentation. Second, we compute the object region representation by aggregating the representations of the pixels lying in the object region. Last, \% the representation similarity we compute the relation between each pixel and each object region and augment the representation of each pixel with the object-contextual representation which is a weighted aggregation of all the object region representations according to their relations with the pixel. We empirically demonstrate that the proposed approach achieves competitive performance on various challenging semantic segmentation benchmarks: Cityscapes, ADE20K, LIP, PASCAL-Context, and COCO-Stuff. Cityscapes, ADE20K, LIP, PASCAL-Context, and COCO-Stuff. Our submission "HRNet + OCR + SegFix" achieves 1-st place on the Cityscapes leaderboard by the time of submission. Code is available at: https://git.io/openseg and https://git.io/HRNet.OCR. We rephrase the object-contextual representation scheme using the Transformer encoder-decoder framework. The details are presented in\textbackslashtextasciitildeSection3.3.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yuan-Object-Contextual%20Representations%20for%20Semantic%20Segmentation-2020-ECCV.pdf}
}
@article{OVSegLiangCVPR2023, 
year = {2023}, 
title = {{Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP}}, 
author = {Liang, Feng and Wu, Bichen and Dai, Xiaoliang and Li, Kunpeng and Zhao, Yinan and Zhang, Hang and Zhang, Peizhao and Vajda, Peter and Marculescu, Diana}, 
journal = {CVPR}, 
eprint = {2210.04150}, 
abstract = {{Open-vocabulary semantic segmentation aims to segment an image into semantic regions according to text descriptions, which may not have been seen during training. Recent two-stage methods first generate class-agnostic mask proposals and then leverage pre-trained vision-language models, e.g., CLIP, to classify masked regions. We identify the performance bottleneck of this paradigm to be the pre-trained CLIP model, since it does not perform well on masked images. To address this, we propose to finetune CLIP on a collection of masked image regions and their corresponding text descriptions. We collect training data by mining an existing image-caption dataset (e.g., COCO Captions), using CLIP to match masked image regions to nouns in the image captions. Compared with the more precise and manually annotated segmentation labels with fixed classes (e.g., COCO-Stuff), we find our noisy but diverse dataset can better retain CLIP's generalization ability. Along with finetuning the entire model, we utilize the "blank" areas in masked images using a method we dub mask prompt tuning. Experiments demonstrate mask prompt tuning brings significant improvement without modifying any weights of CLIP, and it can further improve a fully finetuned model. In particular, when trained on COCO and evaluated on ADE20K-150, our best model achieves 29.6\% mIoU, which is +8.5\% higher than the previous state-of-the-art. For the first time, open-vocabulary generalist models match the performance of supervised specialist models in 2017 without dataset-specific adaptations.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liang-Open-Vocabulary%20Semantic%20Segmentation%20with%20Mask-adapted%20CLIP-2023-CVPR.pdf}
}
@article{VPTJiaECCV2022, 
year = {2022}, 
title = {{Visual Prompt Tuning}}, 
author = {Jia, Menglin and Tang, Luming and Chen, Bor-Chun and Cardie, Claire and Belongie, Serge and Hariharan, Bharath and Lim, Ser-Nam}, 
journal = {ECCV}, 
eprint = {2203.12119}, 
abstract = {{The current modus operandi in adapting pre-trained models involves updating all the backbone parameters, ie, full fine-tuning. This paper introduces Visual Prompt Tuning (VPT) as an efficient and effective alternative to full fine-tuning for large-scale Transformer models in vision. Taking inspiration from recent advances in efficiently tuning large language models, VPT introduces only a small amount (less than 1\% of model parameters) of trainable parameters in the input space while keeping the model backbone frozen. Via extensive experiments on a wide variety of downstream recognition tasks, we show that VPT achieves significant performance gains compared to other parameter efficient tuning protocols. Most importantly, VPT even outperforms full fine-tuning in many cases across model capacities and training data scales, while reducing per-task storage cost.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Jia-Visual%20Prompt%20Tuning-2022-ECCV.pdf}
}
@article{Point-NNZhangCVPR2023, 
year = {2023}, 
title = {{Parameter is Not All You Need: Starting from Non-Parametric Networks for 3D Point Cloud Analysis}}, 
author = {Zhang, Renrui and Wang, Liuhui and Wang, Yali and Gao, Peng and Li, Hongsheng and Shi, Jianbo}, 
journal = {CVPR}, 
eprint = {2303.08134}, 
abstract = {{We present a Non-parametric Network for 3D point cloud analysis, Point-NN, which consists of purely non-learnable components: farthest point sampling (FPS), k-nearest neighbors (k-NN), and pooling operations, with trigonometric functions. Surprisingly, it performs well on various 3D tasks, requiring no parameters or training, and even surpasses existing fully trained models. Starting from this basic non-parametric model, we propose two extensions. First, Point-NN can serve as a base architectural framework to construct Parametric Networks by simply inserting linear layers on top. Given the superior non-parametric foundation, the derived Point-PN exhibits a high performance-efficiency trade-off with only a few learnable parameters. Second, Point-NN can be regarded as a plug-and-play module for the already trained 3D models during inference. Point-NN captures the complementary geometric knowledge and enhances existing methods for different 3D benchmarks without re-training. We hope our work may cast a light on the community for understanding 3D point clouds with non-parametric methods. Code is available at https://github.com/ZrrSkywalker/Point-NN.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Parameter%20is%20Not%20All%20You%20Need-%20Starting%20from%20Non-Parametric%20Networks%20for%203D%20Point%20Cloud%20Analysis-2023-CVPR.pdf}
}
@article{MViTFanICCV2021, 
year = {2021}, 
title = {{Multiscale Vision Transformers}}, 
author = {Fan, Haoqi and Xiong, Bo and Mangalam, Karttikeya and Li, Yanghao and Yan, Zhicheng and Malik, Jitendra and Feichtenhofer, Christoph}, 
journal = {ICCV}, 
eprint = {2104.11227}, 
abstract = {{We present Multiscale Vision Transformers (MViT) for video and image recognition, by connecting the seminal idea of multiscale feature hierarchies with transformer models. Multiscale Transformers have several channel-resolution scale stages. Starting from the input resolution and a small channel dimension, the stages hierarchically expand the channel capacity while reducing the spatial resolution. This creates a multiscale pyramid of features with early layers operating at high spatial resolution to model simple low-level visual information, and deeper layers at spatially coarse, but complex, high-dimensional features. We evaluate this fundamental architectural prior for modeling the dense nature of visual signals for a variety of video recognition tasks where it outperforms concurrent vision transformers that rely on large scale external pre-training and are 5-10x more costly in computation and parameters. We further remove the temporal dimension and apply our model for image classification where it outperforms prior work on vision transformers. Code is available at: https://github.com/facebookresearch/SlowFast}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Fan-Multiscale%20Vision%20Transformers-2021-ICCV.pdf}
}
@article{DropKeyLiCVPR2023, 
year = {2023}, 
title = {{DropKey}}, 
author = {Li, Bonan and Hu, Yinhan and Nie, Xuecheng and Han, Congying and Jiang, Xiangjian and Guo, Tiande and Liu, Luoqi}, 
journal = {CVPR}, 
eprint = {2208.02646}, 
abstract = {{In this paper, we focus on analyzing and improving the dropout technique for self-attention layers of Vision Transformer, which is important while surprisingly ignored by prior works. In particular, we conduct researches on three core questions: First, what to drop in self-attention layers? Different from dropping attention weights in literature, we propose to move dropout operations forward ahead of attention matrix calculation and set the Key as the dropout unit, yielding a novel dropout-before-softmax scheme. We theoretically verify that this scheme helps keep both regularization and probability features of attention weights, alleviating the overfittings problem to specific patterns and enhancing the model to globally capture vital information; Second, how to schedule the drop ratio in consecutive layers? In contrast to exploit a constant drop ratio for all layers, we present a new decreasing schedule that gradually decreases the drop ratio along the stack of self-attention layers. We experimentally validate the proposed schedule can avoid overfittings in low-level features and missing in high-level semantics, thus improving the robustness and stableness of model training; Third, whether need to perform structured dropout operation as CNN? We attempt patch-based block-version of dropout operation and find that this useful trick for CNN is not essential for ViT. Given exploration on the above three questions, we present the novel DropKey method that regards Key as the drop unit and exploits decreasing schedule for drop ratio, improving ViTs in a general way. Comprehensive experiments demonstrate the effectiveness of DropKey for various ViT architectures, e.g. T2T and VOLO, as well as for various vision tasks, e.g., image classification, object detection, human-object interaction detection and human body shape recovery.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-DropKey-2023-CVPR.pdf}
}
@article{CCNetHuangICCV2019, 
year = {2019}, 
title = {{CCNet: Criss-Cross Attention for Semantic Segmentation}}, 
author = {Huang, Zilong and Wang, Xinggang and Wei, Yunchao and Huang, Lichao and Shi, Humphrey and Liu, Wenyu and Huang, Thomas S}, 
journal = {ICCV}, 
eprint = {1811.11721}, 
abstract = {{Contextual information is vital in visual understanding problems, such as semantic segmentation and object detection. We propose a Criss-Cross Network (CCNet) for obtaining full-image contextual information in a very effective and efficient way. Concretely, for each pixel, a novel criss-cross attention module harvests the contextual information of all the pixels on its criss-cross path. By taking a further recurrent operation, each pixel can finally capture the full-image dependencies. Besides, a category consistent loss is proposed to enforce the criss-cross attention module to produce more discriminative features. Overall, CCNet is with the following merits: 1) GPU memory friendly. Compared with the non-local block, the proposed recurrent criss-cross attention module requires 11x less GPU memory usage. 2) High computational efficiency. The recurrent criss-cross attention significantly reduces FLOPs by about 85\% of the non-local block. 3) The state-of-the-art performance. We conduct extensive experiments on semantic segmentation benchmarks including Cityscapes, ADE20K, human parsing benchmark LIP, instance segmentation benchmark COCO, video segmentation benchmark CamVid. In particular, our CCNet achieves the mIoU scores of 81.9\%, 45.76\% and 55.47\% on the Cityscapes test set, the ADE20K validation set and the LIP validation set respectively, which are the new state-of-the-art results. The source codes are available at \textbackslashurl\{https://github.com/speedinghzl/CCNet\}.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Huang-CCNet-%20Criss-Cross%20Attention%20for%20Semantic%20Segmentation-2019-ICCV.pdf}
}
@article{PESBaiNeurIPS2021, 
year = {2021}, 
title = {{Understanding and Improving Early Stopping for Learning with Noisy Labels}}, 
author = {Bai, Yingbin and Yang, Erkun and Han, Bo and Yang, Yanhua and Li, Jiatong and Mao, Yinian and Niu, Gang and Liu, Tongliang}, 
journal = {NeurIPS}, 
eprint = {2106.15853}, 
abstract = {{The memorization effect of deep neural network (DNN) plays a pivotal role in many state-of-the-art label-noise learning methods. To exploit this property, the early stopping trick, which stops the optimization at the early stage of training, is usually adopted. Current methods generally decide the early stopping point by considering a DNN as a whole. However, a DNN can be considered as a composition of a series of layers, and we find that the latter layers in a DNN are much more sensitive to label noise, while their former counterparts are quite robust. Therefore, selecting a stopping point for the whole network may make different DNN layers antagonistically affected each other, thus degrading the final performance. In this paper, we propose to separate a DNN into different parts and progressively train them to address this problem. Instead of the early stopping, which trains a whole DNN all at once, we initially train former DNN layers by optimizing the DNN with a relatively large number of epochs. During training, we progressively train the latter DNN layers by using a smaller number of epochs with the preceding layers fixed to counteract the impact of noisy labels. We term the proposed method as progressive early stopping (PES). Despite its simplicity, compared with the early stopping, PES can help to obtain more promising and stable results. Furthermore, by combining PES with existing approaches on noisy label training, we achieve state-of-the-art performance on image classification benchmarks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bai-Understanding%20and%20Improving%20Early%20Stopping%20for%20Learning%20with%20Noisy%20Labels-2021-NeurIPS.pdf}
}
@article{DistilBERTSanhNeurIPSWorkshop2019, 
year = {2019}, 
title = {{DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}}, 
author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas}, 
journal = {NeurIPS Workshop}, 
eprint = {1910.01108}, 
abstract = {{As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Sanh-DistilBERT,%20a%20distilled%20version%20of%20BERT-%20smaller,%20faster,%20cheaper%20and%20lighter-2019-NeurIPS%20Workshop.pdf}
}
@article{SLWangICCV2019, 
year = {2019}, 
title = {{Symmetric Cross Entropy for Robust Learning With Noisy Labels}}, 
author = {Wang, Yisen and Ma, Xingjun and Chen, Zaiyi and Luo, Yuan and Yi, Jinfeng and Bailey, James}, 
journal = {ICCV}, 
abstract = {{Training accurate deep neural networks (DNNs) in the presence of noisy labels is an important and challenging task. Though a number of approaches have been proposed for learning with noisy labels, many open issues remain. In this paper, we show that DNN learning with Cross Entropy (CE) exhibits overfitting to noisy labels on some classes (“easy” classes), but more surprisingly, it also suffers from significant under learning on some other classes (“hard” classes). Intuitively, CE requires an extra term to facilitate learning of hard classes, and more importantly, this term should be noise tolerant, so as to avoid overfitting to noisy labels. Inspired by the symmetric KL-divergence, we propose the approach of Symmetric cross entropy Learning (SL), boosting CE symmetrically with a noise robust counterpart Reverse Cross Entropy (RCE). Our proposed SL approach simultaneously addresses both the under learning and overfitting problem of CE in the presence of noisy labels. We provide a theoretical analysis of SL and also empirically show, on a range of benchmark and real-world datasets, that SL outperforms state-of-the-art methods. We also show that SL can be easily incorporated into existing methods in order to further enhance their performance.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Symmetric%20Cross%20Entropy%20for%20Robust%20Learning%20With%20Noisy%20Labels-2019-ICCV.pdf}
}
@article{GroupViTXuCVPR2022, 
year = {2022}, 
title = {{GroupViT: Semantic Segmentation Emerges from Text Supervision}}, 
author = {Xu, Jiarui and Mello, Shalini De and Liu, Sifei and Byeon, Wonmin and Breuel, Thomas and Kautz, Jan and Wang, Xiaolong}, 
journal = {CVPR}, 
abstract = {{Grouping and recognition are important components of visual scene understanding, e.g., for object detection and semantic segmentation. With end-to-end deep learning systems, grouping of image regions usually happens implicitly via top-down supervision from pixel-level recognition labels. Instead, in this paper, we propose to bring back the grouping mechanism into deep networks, which allows semantic segments to emerge automatically with only text supervision. We propose a hierarchical Grouping Vision Transformer (GroupViT), which goes beyond the regular grid structure representation and learns to group image regions into progressively larger arbitrary-shaped segments. We train GroupViT jointly with a text encoder on a large-scale image-text dataset via contrastive losses. With only text supervision and without any pixel-level annotations, GroupViT learns to group together semantic regions and successfully transfers to the task of semantic segmentation in a zero-shot manner, i.e., without any further fine-tuning. It achieves a zero-shot accuracy of 52.3\% mIoU on the PASCAL VOC 2012 and 22.4\% mIoU on PASCAL Context datasets, and performs competitively to state-of-the-art transfer-learning methods requiring greater levels of supervision. We open-source our code at https://github.com/NVlabs/GroupViT.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xu-GroupViT-%20Semantic%20Segmentation%20Emerges%20from%20Text%20Supervision-2022-CVPR.pdf}
}
@article{SegFormerXieNeurIPS2021, 
year = {2021}, 
title = {{SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers}}, 
author = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Luo, Ping}, 
journal = {NeurIPS}, 
eprint = {2105.15203}, 
abstract = {{We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3\% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2\% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0\% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code will be released at: github.com/NVlabs/SegFormer.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xie-SegFormer-%20Simple%20and%20Efficient%20Design%20for%20Semantic%20Segmentation%20with%20Transformers-2021-NeurIPS.pdf}
}
@article{UNeXtValanarasuMICCAI2022, 
year = {2022}, 
title = {{UNeXt: MLP-based Rapid Medical Image Segmentation Network}}, 
author = {Valanarasu, Jeya Maria Jose and Patel, Vishal M}, 
journal = {MICCAI}, 
eprint = {2203.04967}, 
abstract = {{UNet and its latest extensions like TransUNet have been the leading medical image segmentation methods in recent years. However, these networks cannot be effectively adopted for rapid image segmentation in point-of-care applications as they are parameter-heavy, computationally complex and slow to use. To this end, we propose UNeXt which is a Convolutional multilayer perceptron (MLP) based network for image segmentation. We design UNeXt in an effective way with an early convolutional stage and a MLP stage in the latent stage. We propose a tokenized MLP block where we efficiently tokenize and project the convolutional features and use MLPs to model the representation. To further boost the performance, we propose shifting the channels of the inputs while feeding in to MLPs so as to focus on learning local dependencies. Using tokenized MLPs in latent space reduces the number of parameters and computational complexity while being able to result in a better representation to help segmentation. The network also consists of skip connections between various levels of encoder and decoder. We test UNeXt on multiple medical image segmentation datasets and show that we reduce the number of parameters by 72x, decrease the computational complexity by 68x, and improve the inference speed by 10x while also obtaining better segmentation performance over the state-of-the-art medical image segmentation architectures. Code is available at https://github.com/jeya-maria-jose/UNeXt-pytorch}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Valanarasu-UNeXt-%20MLP-based%20Rapid%20Medical%20Image%20Segmentation%20Network-2022-MICCAI.pdf}
}
@article{FLIPLiCVPR2023, 
year = {2023}, 
title = {{Scaling Language-Image Pre-training via Masking}}, 
author = {Li, Yanghao and Fan, Haoqi and Hu, Ronghang and Feichtenhofer, Christoph and He, Kaiming}, 
journal = {CVPR}, 
eprint = {2212.00794}, 
abstract = {{We present Fast Language-Image Pre-training (FLIP), a simple and more efficient method for training CLIP. Our method randomly masks out and removes a large portion of image patches during training. Masking allows us to learn from more image-text pairs given the same wall-clock time and contrast more samples per iteration with similar memory footprint. It leads to a favorable trade-off between accuracy and training time. In our experiments on 400 million image-text pairs, FLIP improves both accuracy and speed over the no-masking baseline. On a large diversity of downstream tasks, FLIP dominantly outperforms the CLIP counterparts trained on the same data. Facilitated by the speedup, we explore the scaling behavior of increasing the model size, data size, or training length, and report encouraging results and comparisons. We hope that our work will foster future research on scaling vision-language learning.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Scaling%20Language-Image%20Pre-training%20via%20Masking-2023-CVPR.pdf}
}
@article{GCViTHatamizadehICML2023, 
year = {2023}, 
title = {{Global Context Vision Transformers}}, 
author = {Hatamizadeh, Ali and Yin, Hongxu and Kautz, Jan and Molchanov, Pavlo}, 
journal = {ICML}, 
eprint = {2206.09959}, 
abstract = {{We propose global context vision transformer (GC ViT), a novel architecture that enhances parameter and compute utilization for computer vision tasks. The core of the novel model are global context self-attention modules, joint with standard local self-attention, to effectively yet efficiently model both long and short-range spatial interactions, as an alternative to complex operations such as an attention masks or local windows shifting. While the local self-attention modules are responsible for modeling short-range information, the global query tokens are shared across all global self-attention modules to interact with local key and values. In addition, we address the lack of inductive bias in ViTs and improve the modeling of inter-channel dependencies by proposing a novel downsampler which leverages a parameter-efficient fused inverted residual block. The proposed GC ViT achieves new state-of-the-art performance across image classification, object detection and semantic segmentation tasks. On ImageNet-1K dataset for classification, GC ViT models with 51M, 90M and 201M parameters achieve 84.3\%, 84.9\% and 85.6\% Top-1 accuracy, respectively, surpassing comparably-sized prior art such as CNN-based ConvNeXt and ViT-based Swin Transformer. Pre-trained GC ViT backbones in downstream tasks of object detection, instance segmentation, and semantic segmentation on MS COCO and ADE20K datasets outperform prior work consistently, sometimes by large margins.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hatamizadeh-Global%20Context%20Vision%20Transformers-2023-ICML.pdf}
}
@article{ReliabilityJorgeCVPR2023, 
year = {2023}, 
title = {{Reliability in Semantic Segmentation: Are We on the Right Track?}}, 
author = {Jorge, Pau de and Volpi, Riccardo and Torr, Philip and Rogez, Gregory}, 
journal = {CVPR}, 
eprint = {2303.11298}, 
abstract = {{Motivated by the increasing popularity of transformers in computer vision, in recent times there has been a rapid development of novel architectures. While in-domain performance follows a constant, upward trend, properties like robustness or uncertainty estimation are less explored -leaving doubts about advances in model reliability. Studies along these axes exist, but they are mainly limited to classification models. In contrast, we carry out a study on semantic segmentation, a relevant task for many real-world applications where model reliability is paramount. We analyze a broad variety of models, spanning from older ResNet-based architectures to novel transformers and assess their reliability based on four metrics: robustness, calibration, misclassification detection and out-of-distribution (OOD) detection. We find that while recent models are significantly more robust, they are not overall more reliable in terms of uncertainty estimation. We further explore methods that can come to the rescue and show that improving calibration can also help with other uncertainty metrics such as misclassification or OOD detection. This is the first study on modern segmentation models focused on both robustness and uncertainty estimation and we hope it will help practitioners and researchers interested in this fundamental vision task. Code available at https://github.com/naver/relis.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Jorge-Reliability%20in%20Semantic%20Segmentation-%20Are%20We%20on%20the%20Right%20Track--2023-CVPR.pdf}
}
@article{Point-ENicholarXiv2022, 
year = {2022}, 
title = {{Point-E: A System for Generating 3D Point Clouds from Complex Prompts}}, 
author = {Nichol, Alex and Jun, Heewoo and Dhariwal, Prafulla and Mishkin, Pamela and Chen, Mark}, 
journal = {arXiv}, 
eprint = {2212.08751}, 
abstract = {{While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at https://github.com/openai/point-e.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Nichol-Point-E-%20A%20System%20for%20Generating%203D%20Point%20Clouds%20from%20Complex%20Prompts-2022-arXiv.pdf}
}
@article{GLIDENicholICML2022, 
year = {2022}, 
title = {{GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models}}, 
author = {Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark}, 
journal = {ICML}, 
eprint = {2112.10741}, 
abstract = {{Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Nichol-GLIDE-%20Towards%20Photorealistic%20Image%20Generation%20and%20Editing%20with%20Text-Guided%20Diffusion%20Models-2022-ICML.pdf}
}
@article{DALL-E2RamesharXiv2022, 
year = {2022}, 
title = {{Hierarchical Text-Conditional Image Generation with CLIP Latents}}, 
author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark}, 
journal = {arXiv}, 
eprint = {2204.06125}, 
abstract = {{Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ramesh-Hierarchical%20Text-Conditional%20Image%20Generation%20with%20CLIP%20Latents-2022-arXiv.pdf}
}
@article{TGTZaheerICLR2023, 
year = {2023}, 
title = {{Teacher Guided Training: An Efficient Framework for Knowledge Transfer}}, 
author = {Zaheer, Manzil and Rawat, Ankit Singh and Kim, Seungyeon and You, Chong and Jain, Himanshu and Veit, Andreas and Fergus, Rob and Kumar, Sanjiv}, 
journal = {ICLR}, 
eprint = {2208.06825}, 
abstract = {{The remarkable performance gains realized by large pretrained models, e.g., GPT-3, hinge on the massive amounts of data they are exposed to during training. Analogously, distilling such large models to compact models for efficient deployment also necessitates a large amount of (labeled or unlabeled) training data. In this paper, we propose the teacher-guided training (TGT) framework for training a high-quality compact model that leverages the knowledge acquired by pretrained generative models, while obviating the need to go through a large volume of data. TGT exploits the fact that the teacher has acquired a good representation of the underlying data domain, which typically corresponds to a much lower dimensional manifold than the input space. Furthermore, we can use the teacher to explore input space more efficiently through sampling or gradient-based methods; thus, making TGT especially attractive for limited data or long-tail settings. We formally capture this benefit of proposed data-domain exploration in our generalization bounds. We find that TGT can improve accuracy on several image classification benchmarks as well as a range of text classification and retrieval tasks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zaheer-Teacher%20Guided%20Training-%20An%20Efficient%20Framework%20for%20Knowledge%20Transfer-2023-ICLR.pdf}
}
@article{DeepSetsZaheerNeurIPS2017, 
year = {2017}, 
title = {{Deep Sets}}, 
author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Ruslan and Smola, Alexander}, 
journal = {NeurIPS}, 
eprint = {1703.06114}, 
abstract = {{We study the problem of designing models for machine learning tasks defined on \textbackslashemph\{sets\}. In contrast to traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets that are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics \textbackslashcite\{poczos13aistats\}, to anomaly detection in piezometer data of embankment dams \textbackslashcite\{Jung15Exploration\}, to cosmology \textbackslashcite\{Ntampaka16Dynamical,Ravanbakhsh16ICML1\}. Our main theorem characterizes the permutation invariant functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We also derive the necessary and sufficient conditions for permutation equivariance in deep models. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zaheer-Deep%20Sets-2017-NeurIPS.pdf}
}
@article{OnTheDualityGarridoICLR2023, 
year = {2023}, 
title = {{On the duality between contrastive and non-contrastive self-supervised learning}}, 
author = {Garrido, Quentin and Chen, Yubei and Bardes, Adrien and Najman, Laurent and Lecun, Yann}, 
journal = {ICLR}, 
eprint = {2206.02574}, 
abstract = {{Recent approaches in self-supervised learning of image representations can be categorized into different families of methods and, in particular, can be divided into contrastive and non-contrastive approaches. While differences between the two families have been thoroughly discussed to motivate new approaches, we focus more on the theoretical similarities between them. By designing contrastive and non-contrastive criteria that can be related algebraically and shown to be equivalent under limited assumptions, we show how close those families can be. We further study popular methods and introduce variations of them, allowing us to relate this theoretical result to current practices and show how design choices in the criterion can influence the optimization process and downstream performance. We also challenge the popular assumptions that contrastive and non-contrastive methods, respectively, need large batch sizes and output dimensions. Our theoretical and quantitative results suggest that the numerical gaps between contrastive and noncontrastive methods in certain regimes can be significantly reduced given better network design choice and hyperparameter tuning.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Garrido-On%20the%20duality%20between%20contrastive%20and%20non-contrastive%20self-supervised%20learning-2023-ICLR.pdf}
}
@article{DINOV2OquabarXiv2023, 
year = {2023}, 
title = {{DINOv2: Learning Robust Visual Features without Supervision}}, 
author = {Oquab, Maxime and Darcet, Timothée and Moutakanni, Théo and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Hervé and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr}, 
journal = {arXiv}, 
eprint = {2304.07193}, 
abstract = {{The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Oquab-DINOv2-%20Learning%20Robust%20Visual%20Features%20without%20Supervision-2023-arXiv.pdf}
}
@article{DCGANRadfordICLR2016, 
year = {2016}, 
title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}}, 
author = {Radford, Alec and Metz, Luke and Chintala, Soumith}, 
journal = {ICLR}, 
eprint = {1511.06434}, 
abstract = {{In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Radford-Unsupervised%20Representation%20Learning%20with%20Deep%20Convolutional%20Generative%20Adversarial%20Networks-2016-ICLR.pdf}
}
@article{Multi-taskLearningKendallCVPR2018, 
year = {2018}, 
title = {{Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics}}, 
author = {Kendall, Alex and Gal, Yarin and Cipolla, Roberto}, 
journal = {CVPR}, 
eprint = {1705.07115}, 
abstract = {{Numerous deep learning applications benefit from multi-task learning with multiple regression and classification objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task's loss. Tuning these weights by hand is a difficult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kendall-Multi-Task%20Learning%20Using%20Uncertainty%20to%20Weigh%20Losses%20for%20Scene%20Geometry%20and%20Semantics-2018-CVPR_1.pdf}
}
@article{TaskPrompterYeICLR2023, 
year = {2023}, 
title = {{TaskPrompter: Spatial-Channel Multi-Task Prompting for Dense Scene Understanding}}, 
author = {Ye, Hanrong and Xu, Dan}, 
journal = {ICLR}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ye-TaskPrompter-%20Spatial-Channel%20Multi-Task%20Prompting%20for%20Dense%20Scene%20Understanding-2023-ICLR.pdf}
}
@article{ViTAdapterChenICLR2023, 
year = {2023}, 
title = {{Vision Transformer Adapter for Dense Predictions}}, 
author = {Chen, Zhe and Duan, Yuchen and Wang, Wenhai and He, Junjun and Lu, Tong and Dai, Jifeng and Qiao, Yu}, 
journal = {ICLR}, 
eprint = {2205.08534}, 
abstract = {{This work investigates a simple yet powerful dense prediction task adapter for Vision Transformer (ViT). Unlike recently advanced variants that incorporate vision-specific inductive biases into their architectures, the plain ViT suffers inferior performance on dense predictions due to weak prior assumptions. To address this issue, we propose the ViT-Adapter, which allows plain ViT to achieve comparable performance to vision-specific transformers. Specifically, the backbone in our framework is a plain ViT that can learn powerful representations from large-scale multi-modal data. When transferring to downstream tasks, a pre-training-free adapter is used to introduce the image-related inductive biases into the model, making it suitable for these tasks. We verify ViT-Adapter on multiple dense prediction tasks, including object detection, instance segmentation, and semantic segmentation. Notably, without using extra detection data, our ViT-Adapter-L yields state-of-the-art 60.9 box AP and 53.0 mask AP on COCO test-dev. We hope that the ViT-Adapter could serve as an alternative for vision-specific transformers and facilitate future research. The code and models will be released at https://github.com/czczup/ViT-Adapter.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Vision%20Transformer%20Adapter%20for%20Dense%20Predictions-2023-ICLR.pdf}
}
@article{KFIoULossYangICLR2023, 
year = {2023}, 
title = {{The KFIoU Loss for Rotated Object Detection}}, 
author = {Yang, Xue and Zhou, Yue and Zhang, Gefan and Yang, Jirui and Wang, Wentao and Yan, Junchi and Zhang, Xiaopeng and Tian, Qi}, 
journal = {ICLR}, 
eprint = {2201.12558}, 
abstract = {{Differing from the well-developed horizontal object detection area whereby the computing-friendly IoU based loss is readily adopted and well fits with the detection metrics. In contrast, rotation detectors often involve a more complicated loss based on SkewIoU which is unfriendly to gradient-based training. In this paper, we propose an effective approximate SkewIoU loss based on Gaussian modeling and Gaussian product, which mainly consists of two items. The first term is a scale-insensitive center point loss, which is used to quickly narrow the distance between the center points of the two bounding boxes. In the distance-independent second term, the product of the Gaussian distributions is adopted to inherently mimic the mechanism of SkewIoU by its definition, and show its alignment with the SkewIoU loss at trend-level within a certain distance (i.e. within 9 pixels). This is in contrast to recent Gaussian modeling based rotation detectors e.g. GWD loss and KLD loss that involve a human-specified distribution distance metric which require additional hyperparameter tuning that vary across datasets and detectors. The resulting new loss called KFIoU loss is easier to implement and works better compared with exact SkewIoU loss, thanks to its full differentiability and ability to handle the non-overlapping cases. We further extend our technique to the 3-D case which also suffers from the same issues as 2-D. Extensive results on various public datasets (2-D/3-D, aerial/text/face images) with different base detectors show the effectiveness of our approach.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-The%20KFIoU%20Loss%20for%20Rotated%20Object%20Detection-2023-ICLR.pdf}
}
@article{UnitBoxMMYu2016, 
year = {2016}, 
title = {{UnitBox: An Advanced Object Detection Network}}, 
author = {Yu, Jiahui and Jiang, Yuning and Wang, Zhangyang and Cao, Zhimin and Huang, Thomas}, 
journal = {MM}, 
eprint = {1608.01471}, 
abstract = {{In present object detection systems, the deep convolutional neural networks (CNNs) are utilized to predict bounding boxes of object candidates, and have gained performance advantages over the traditional region proposal methods. However, existing deep CNN methods assume the object bounds to be four independent variables, which could be regressed by the \$\textbackslashell\_2\$ loss separately. Such an oversimplified assumption is contrary to the well-received observation, that those variables are correlated, resulting to less accurate localization. To address the issue, we firstly introduce a novel Intersection over Union (\$IoU\$) loss function for bounding box prediction, which regresses the four bounds of a predicted box as a whole unit. By taking the advantages of \$IoU\$ loss and deep fully convolutional networks, the UnitBox is introduced, which performs accurate and efficient localization, shows robust to objects of varied shapes and scales, and converges fast. We apply UnitBox on face detection task and achieve the best performance among all published methods on the FDDB benchmark.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yu-UnitBox-%20An%20Advanced%20Object%20Detection%20Network-2016-MM.pdf}
}
@article{PIoULossChenECCV2020, 
year = {2020}, 
title = {{PIoU Loss: Towards Accurate Oriented Object Detection in Complex Environments}}, 
author = {Chen, Zhiming and Chen, Kean and Lin, Weiyao and See, John and Yu, Hui and Ke, Yan and Yang, Cong}, 
journal = {ECCV}, 
eprint = {2007.09584}, 
abstract = {{Object detection using an oriented bounding box (OBB) can better target rotated objects by reducing the overlap with background areas. Existing OBB approaches are mostly built on horizontal bounding box detectors by introducing an additional angle dimension optimized by a distance loss. However, as the distance loss only minimizes the angle error of the OBB and that it loosely correlates to the IoU, it is insensitive to objects with high aspect ratios. Therefore, a novel loss, Pixels-IoU (PIoU) Loss, is formulated to exploit both the angle and IoU for accurate OBB regression. The PIoU loss is derived from IoU metric with a pixel-wise form, which is simple and suitable for both horizontal and oriented bounding box. To demonstrate its effectiveness, we evaluate the PIoU loss on both anchor-based and anchor-free frameworks. The experimental results show that PIoU loss can dramatically improve the performance of OBB detectors, particularly on objects with high aspect ratios and complex backgrounds. Besides, previous evaluation datasets did not include scenarios where the objects have high aspect ratios, hence a new dataset, Retail50K, is introduced to encourage the community to adapt OBB detectors for more complex environments.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-PIoU%20Loss-%20Towards%20Accurate%20Oriented%20Object%20Detection%20in%20Complex%20Environments-2020-ECCV.pdf}
}
@article{LearningChenNeurIPS2017, 
year = {2017}, 
title = {{Learning Efficient Object Detection Models with Knowledge Distillation}}, 
author = {Chen, Guobin and Choi, Wongun and Yu, Xiang and Han, Tony and Chandraker, Manmohan}, 
journal = {NeurIPS}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Learning%20Efficient%20Object%20Detection%20Models%20with%20Knowledge%20Distillation-2017-NeurIPS.pdf}
}
@article{Alpha-IoULossesHeNeurIPS2021, 
year = {2021}, 
title = {{Alpha-IoU: A Family of Power Intersection over Union Losses for Bounding Box Regression}}, 
author = {He, Jiabo and Erfani, Sarah and Ma, Xingjun and Bailey, James and Chi, Ying and Hua, Xian-Sheng}, 
journal = {NeurIPS}, 
eprint = {2110.13675}, 
abstract = {{Bounding box (bbox) regression is a fundamental task in computer vision. So far, the most commonly used loss functions for bbox regression are the Intersection over Union (IoU) loss and its variants. In this paper, we generalize existing IoU-based losses to a new family of power IoU losses that have a power IoU term and an additional power regularization term with a single power parameter \$\textbackslashalpha\$. We call this new family of losses the \$\textbackslashalpha\$-IoU losses and analyze properties such as order preservingness and loss/gradient reweighting. Experiments on multiple object detection benchmarks and models demonstrate that \$\textbackslashalpha\$-IoU losses, 1) can surpass existing IoU-based losses by a noticeable performance margin; 2) offer detectors more flexibility in achieving different levels of bbox regression accuracy by modulating \$\textbackslashalpha\$; and 3) are more robust to small datasets and noisy bboxes.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/He-Alpha-IoU-%20A%20Family%20of%20Power%20Intersection%20over%20Union%20Losses%20for%20Bounding%20Box%20Regression-2021-NeurIPS.pdf}
}
@article{Focal-EIoULossZhangNeurocomputing2022, 
year = {2022}, 
title = {{Focal and Efficient IOU Loss for Accurate Bounding Box Regression}}, 
author = {Zhang, Yi-Fan and Ren, Weiqiang and Zhang, Zhang and Jia, Zhen and Wang, Liang and Tan, Tieniu}, 
journal = {Neurocomputing}, 
eprint = {2101.08158}, 
abstract = {{In object detection, bounding box regression (BBR) is a crucial step that determines the object localization performance. However, we find that most previous loss functions for BBR have two main drawbacks: (i) Both \$\textbackslashell\_n\$-norm and IOU-based loss functions are inefficient to depict the objective of BBR, which leads to slow convergence and inaccurate regression results. (ii) Most of the loss functions ignore the imbalance problem in BBR that the large number of anchor boxes which have small overlaps with the target boxes contribute most to the optimization of BBR. To mitigate the adverse effects caused thereby, we perform thorough studies to exploit the potential of BBR losses in this paper. Firstly, an Efficient Intersection over Union (EIOU) loss is proposed, which explicitly measures the discrepancies of three geometric factors in BBR, i.e., the overlap area, the central point and the side length. After that, we state the Effective Example Mining (EEM) problem and propose a regression version of focal loss to make the regression process focus on high-quality anchor boxes. Finally, the above two parts are combined to obtain a new loss function, namely Focal-EIOU loss. Extensive experiments on both synthetic and real datasets are performed. Notable superiorities on both the convergence speed and the localization accuracy can be achieved over other BBR losses.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Focal%20and%20Efficient%20IOU%20Loss%20for%20Accurate%20Bounding%20Box%20Regression-2022-Neurocomputing.pdf}
}
@article{GWDYangICML2021, 
year = {2021}, 
title = {{Rethinking Rotated Object Detection with Gaussian Wasserstein Distance Loss}}, 
author = {Yang, Xue and Yan, Junchi and Ming, Qi and Wang, Wentao and Zhang, Xiaopeng and Tian, Qi}, 
journal = {ICML}, 
eprint = {2101.11952}, 
abstract = {{Boundary discontinuity and its inconsistency to the final detection metric have been the bottleneck for rotating detection regression loss design. In this paper, we propose a novel regression loss based on Gaussian Wasserstein distance as a fundamental approach to solve the problem. Specifically, the rotated bounding box is converted to a 2-D Gaussian distribution, which enables to approximate the indifferentiable rotational IoU induced loss by the Gaussian Wasserstein distance (GWD) which can be learned efficiently by gradient back-propagation. GWD can still be informative for learning even there is no overlapping between two rotating bounding boxes which is often the case for small object detection. Thanks to its three unique properties, GWD can also elegantly solve the boundary discontinuity and square-like problem regardless how the bounding box is defined. Experiments on five datasets using different detectors show the effectiveness of our approach. Codes are available at https://github.com/yangxue0827/RotationDetection and https://github.com/open-mmlab/mmrotate.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-Rethinking%20Rotated%20Object%20Detection%20with%20Gaussian%20Wasserstein%20Distance%20Loss-2021-ICML.pdf}
}
@article{T2T-ViTYuanICCV2021, 
year = {2021}, 
title = {{Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet}}, 
author = {Yuan, Li and Chen, Yunpeng and Wang, Tao and Yu, Weihao and Shi, Yujun and Jiang, Zihang and Tay, Francis EH and Feng, Jiashi and Yan, Shuicheng}, 
journal = {ICCV}, 
eprint = {2101.11986}, 
abstract = {{Transformers, which are popular for language modeling, have been explored for solving vision tasks recently, e.g., the Vision Transformer (ViT) for image classification. The ViT model splits each image into a sequence of tokens with fixed length and then applies multiple Transformer layers to model their global relation for classification. However, ViT achieves inferior performance to CNNs when trained from scratch on a midsize dataset like ImageNet. We find it is because: 1) the simple tokenization of input images fails to model the important local structure such as edges and lines among neighboring pixels, leading to low training sample efficiency; 2) the redundant attention backbone design of ViT leads to limited feature richness for fixed computation budgets and limited training samples. To overcome such limitations, we propose a new Tokens-To-Token Vision Transformer (T2T-ViT), which incorporates 1) a layer-wise Tokens-to-Token (T2T) transformation to progressively structurize the image to tokens by recursively aggregating neighboring Tokens into one Token (Tokens-to-Token), such that local structure represented by surrounding tokens can be modeled and tokens length can be reduced; 2) an efficient backbone with a deep-narrow structure for vision transformer motivated by CNN architecture design after empirical study. Notably, T2T-ViT reduces the parameter count and MACs of vanilla ViT by half, while achieving more than 3.0\textbackslash\% improvement when trained from scratch on ImageNet. It also outperforms ResNets and achieves comparable performance with MobileNets by directly training on ImageNet. For example, T2T-ViT with comparable size to ResNet50 (21.5M parameters) can achieve 83.3\textbackslash\% top1 accuracy in image resolution 384\$\textbackslashtimes\$384 on ImageNet. (Code: https://github.com/yitu-opensource/T2T-ViT)}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yuan-Tokens-to-Token%20ViT-%20Training%20Vision%20Transformers%20from%20Scratch%20on%20ImageNet-2021-ICCV.pdf}
}
@article{RobustLossGhoshAAAI2017, 
year = {2017}, 
title = {{Robust Loss Functions under Label Noise for Deep Neural Networks}}, 
author = {Ghosh, Aritra and Kumar, Himanshu and Sastry, P S}, 
journal = {AAAI}, 
eprint = {1712.09482}, 
abstract = {{In many applications of classifier learning, training data suffers from label noise. Deep networks are learned using huge training data where the problem of noisy labels is particularly relevant. The current techniques proposed for learning deep networks under label noise focus on modifying the network architecture and on algorithms for estimating true labels from noisy labels. An alternate approach would be to look for loss functions that are inherently noise-tolerant. For binary classification there exist theoretical results on loss functions that are robust to label noise. In this paper, we provide some sufficient conditions on a loss function so that risk minimization under that loss function would be inherently tolerant to label noise for multiclass classification problems. These results generalize the existing results on noise-tolerant loss functions for binary classification. We study some of the widely used loss functions in deep networks and show that the loss function based on mean absolute value of error is inherently robust to label noise. Thus standard back propagation is enough to learn the true classifier even under label noise. Through experiments, we illustrate the robustness of risk minimization with such loss functions for learning neural networks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ghosh-Robust%20Loss%20Functions%20under%20Label%20Noise%20for%20Deep%20Neural%20Networks-2017-AAAI.pdf}
}
@article{NoisyNordstromarXiv2023, 
year = {2023}, 
title = {{Noisy Image Segmentation With Soft-Dice}}, 
author = {Nordström, Marcus and Hult, Henrik and Maki, Atsuto and Löfman, Fredrik}, 
journal = {arXiv}, 
eprint = {2304.00801}, 
abstract = {{This paper presents a study on the soft-Dice loss, one of the most popular loss functions in medical image segmentation, for situations where noise is present in target labels. In particular, the set of optimal solutions are characterized and sharp bounds on the volume bias of these solutions are provided. It is further shown that a sequence of soft segmentations converging to optimal soft-Dice also converges to optimal Dice when converted to hard segmentations using thresholding. This is an important result because soft-Dice is often used as a proxy for maximizing the Dice metric. Finally, experiments confirming the theoretical results are provided.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Nordström-Noisy%20Image%20Segmentation%20With%20Soft-Dice-2023-arXiv.pdf}
}
@article{3DSegmenterWuICLR2023, 
year = {2023}, 
title = {{3D Segmenter: 3D Transformer based Semantic Segmentation via 2D Panoramic Distillation}}, 
author = {Wu, Zhennan and Li, Yang and Huang, Yifei and Gu, Lin and Harada, Tatsuya and Sato, Hiroyuki}, 
journal = {ICLR}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wu-3D%20Segmenter-%203D%20Transformer%20based%20Semantic%20Segmentation%20via%202D%20Panoramic%20Distillation-2023-ICLR.pdf}
}
@article{Semantic-NeRFZhiICCV2021, 
year = {2021}, 
title = {{In-Place Scene Labelling and Understanding with Implicit Scene Representation}}, 
author = {Zhi, Shuaifeng and Laidlow, Tristan and Leutenegger, Stefan and Davison, Andrew J}, 
journal = {ICCV}, 
eprint = {2103.15875}, 
abstract = {{Semantic labelling is highly correlated with geometry and radiance reconstruction, as scene entities with similar shape and appearance are more likely to come from similar classes. Recent implicit neural reconstruction techniques are appealing as they do not require prior training data, but the same fully self-supervised approach is not possible for semantics because labels are human-defined properties. We extend neural radiance fields (NeRF) to jointly encode semantics with appearance and geometry, so that complete and accurate 2D semantic labels can be achieved using a small amount of in-place annotations specific to the scene. The intrinsic multi-view consistency and smoothness of NeRF benefit semantics by enabling sparse labels to efficiently propagate. We show the benefit of this approach when labels are either sparse or very noisy in room-scale scenes. We demonstrate its advantageous properties in various interesting applications such as an efficient scene labelling tool, novel semantic view synthesis, label denoising, super-resolution, label interpolation and multi-view semantic label fusion in visual semantic mapping systems.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhi-In-Place%20Scene%20Labelling%20and%20Understanding%20with%20Implicit%20Scene%20Representation-2021-ICCV.pdf}
}
@article{Mask2FormerChengCVPR2022, 
year = {2022}, 
title = {{Masked-attention Mask Transformer for Universal Image Segmentation}}, 
author = {Cheng, Bowen and Misra, Ishan and Schwing, Alexander G. and Kirillov, Alexander and Girdhar, Rohit}, 
journal = {CVPR}, 
abstract = {{Image segmentation groups pixels with different semantics, e.g., category or instance membership. Each choice of semantics defines a task. While only the semantics of each task differ, current research focuses on designing spe-cialized architectures for each task. We present Masked- attention Mask Transformer (Mask2Former), a new archi-tecture capable of addressing any image segmentation task (panoptic, instance or semantic). Its key components in-clude masked attention, which extracts localized features by constraining cross-attention within predicted mask regions. In addition to reducing the research effort by at least three times, it outperforms the best specialized architectures by a significant margin on four popular datasets. Most no-tably, Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on COCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7 mIoU onADE20K).}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Cheng-Masked-attention%20Mask%20Transformer%20for%20Universal%20Image%20Segmentation-2022-CVPR.pdf}
}
@article{FreeSOLOWangCVPR2022, 
year = {2022}, 
title = {{FreeSOLO: Learning to Segment Objects without Annotations}}, 
author = {Wang, Xinlong and Yu, Zhiding and Mello, Shalini De and Kautz, Jan and Anandkumar, Anima and Shen, Chunhua and Alvarez, Jose M}, 
journal = {CVPR}, 
abstract = {{Instance segmentation is a fundamental vision task that aims to recognize and segment each object in an image. However, it requires costly annotations such as bounding boxes and segmentation masks for learning. In this work, we propose a fully unsupervised learning method that learns class-agnostic instance segmentation without any annotations. We present FreeSOLO, a self-supervised instance segmentation framework built on top of the simple instance segmentation method SOLO. Our method also presents a novel localization-aware pre-training framework, where objects can be discovered from complicated scenes in an unsupervised manner. FreeSOLO achieves 9.8\%\$AP\_\{50\}\$on the challenging COCO dataset, which even outperforms several segmentation proposal methods that use manual annotations. For the first time, we demonstrate unsupervised class-agnostic instance segmen-tation successfully. FreeSOLO's box localization significantly outperforms state-of-the-art unsupervised object de-tection/discovery methods, with about 100\% relative improvements in COCO AP. FreeSOLO further demonstrates superiority as a strong pre-training method, outperforming state-of-the-art self-supervised pre-training methods by\$+9.8\textbackslash\%\$AP when fine-tuning instance segmentation with only 5\% COCO masks. Code is available at: github.com/NVlabs/FreeSOLO}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-FreeSOLO-%20Learning%20to%20Segment%20Objects%20without%20Annotations-2022-CVPR.pdf}
}
@article{PIDNetXuCVPR2023, 
year = {2023}, 
title = {{PIDNet: A Real-time Semantic Segmentation Network Inspired from PID Controller}}, 
author = {Xu, Jiacong and Xiong, Zixiang and Bhattacharyya, Shankar P}, 
journal = {CVPR}, 
eprint = {2206.02066}, 
abstract = {{Two-branch network architecture has shown its efficiency and effectiveness for real-time semantic segmentation tasks. However, direct fusion of low-level details and high-level semantics will lead to a phenomenon that the detailed features are easily overwhelmed by surrounding contextual information, namely overshoot in this paper, which limits the improvement of the accuracy of existed two-branch models. In this paper, we bridge a connection between Convolutional Neural Network (CNN) and Proportional-Integral-Derivative (PID) controller and reveal that the two-branch network is nothing but a Proportional-Integral (PI) controller, which inherently suffers from the similar overshoot issue. To alleviate this issue, we propose a novel three-branch network architecture: PIDNet, which possesses three branches to parse the detailed, context and boundary information (derivative of semantics), respectively, and employs boundary attention to guide the fusion of detailed and context branches in final stage. The family of PIDNets achieve the best trade-off between inference speed and accuracy and their test accuracy surpasses all the existed models with similar inference speed on Cityscapes, CamVid and COCO-Stuff datasets. Especially, PIDNet-S achieves 78.6\% mIOU with inference speed of 93.2 FPS on Cityscapes test set and 80.1\% mIOU with speed of 153.7 FPS on CamVid test set.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xu-PIDNet-%20A%20Real-time%20Semantic%20Segmentation%20Network%20Inspired%20from%20PID%20Controller-2023-CVPR.pdf}
}
@article{MOATYangICLR2023, 
year = {2023}, 
title = {{MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models}}, 
author = {Yang, Chenglin and Qiao, Siyuan and Yu, Qihang and Yuan, Xiaoding and Zhu, Yukun and Yuille, Alan and Adam, Hartwig and Chen, Liang-Chieh}, 
journal = {ICLR}, 
eprint = {2210.01820}, 
abstract = {{This paper presents MOAT, a family of neural networks that build on top of MObile convolution (i.e., inverted residual blocks) and ATtention. Unlike the current works that stack separate mobile convolution and transformer blocks, we effectively merge them into a MOAT block. Starting with a standard Transformer block, we replace its multi-layer perceptron with a mobile convolution block, and further reorder it before the self-attention operation. The mobile convolution block not only enhances the network representation capacity, but also produces better downsampled features. Our conceptually simple MOAT networks are surprisingly effective, achieving 89.1\% top-1 accuracy on ImageNet-1K with ImageNet-22K pretraining. Additionally, MOAT can be seamlessly applied to downstream tasks that require large resolution inputs by simply converting the global attention to window attention. Thanks to the mobile convolution that effectively exchanges local information between pixels (and thus cross-windows), MOAT does not need the extra window-shifting mechanism. As a result, on COCO object detection, MOAT achieves 59.2\% box AP with 227M model parameters (single-scale inference, and hard NMS), and on ADE20K semantic segmentation, MOAT attains 57.6\% mIoU with 496M model parameters (single-scale inference). Finally, the tiny-MOAT family, obtained by simply reducing the channel sizes, also surprisingly outperforms several mobile-specific transformer-based models on ImageNet. We hope our simple yet effective MOAT will inspire more seamless integration of convolution and self-attention. Code is made publicly available.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-MOAT-%20Alternating%20Mobile%20Convolution%20and%20Attention%20Brings%20Strong%20Vision%20Models-2023-ICLR.pdf}
}
@article{MaX-DeepLabWangCVPR2021, 
year = {2021}, 
title = {{MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers}}, 
author = {Wang, Huiyu and Zhu, Yukun and Adam, Hartwig and Yuille, Alan and Chen, Liang-Chieh}, 
journal = {CVPR}, 
abstract = {{We present MaX-DeepLab, the first end-to-end model for panoptic segmentation. Our approach simplifies the current pipeline that depends heavily on surrogate sub-tasks and hand-designed components, such as box detection, non-maximum suppression, thing-stuff merging, etc. Although these sub-tasks are tackled by area experts, they fail to comprehensively solve the target task. By contrast, our MaX-DeepLab directly predicts class-labeled masks with a mask transformer, and is trained with a panoptic quality inspired loss via bipartite matching. Our mask transformer employs a dual-path architecture that introduces a global memory path in addition to a CNN path, allowing direct communication with any CNN layers. As a result, MaX-DeepLab shows a significant 7.1\% PQ gain in the box-free regime on the challenging COCO dataset, closing the gap between box-based and box-free methods for the first time. A small variant of MaX-DeepLab improves 3.0\% PQ over DETR with similar parameters and M-Adds. Furthermore, MaX-DeepLab, without test time augmentation, achieves new state-of-the-art 51.3\% PQ on COCO test-dev set.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-MaX-DeepLab-%20End-to-End%20Panoptic%20Segmentation%20with%20Mask%20Transformers-2021-CVPR.pdf}
}
@article{kMaX-DeepLabYuECCV2022, 
year = {2022}, 
title = {{k-means Mask Transformer}}, 
author = {Yu, Qihang and Wang, Huiyu and Qiao, Siyuan and Collins, Maxwell and Zhu, Yukun and Adam, Hatwig and Yuille, Alan and Chen, Liang-Chieh}, 
journal = {ECCV}, 
eprint = {2207.04044}, 
abstract = {{The rise of transformers in vision tasks not only advances network backbone designs, but also starts a brand-new page to achieve end-to-end image recognition (e.g., object detection and panoptic segmentation). Originated from Natural Language Processing (NLP), transformer architectures, consisting of self-attention and cross-attention, effectively learn long-range interactions between elements in a sequence. However, we observe that most existing transformer-based vision models simply borrow the idea from NLP, neglecting the crucial difference between languages and images, particularly the extremely large sequence length of spatially flattened pixel features. This subsequently impedes the learning in cross-attention between pixel features and object queries. In this paper, we rethink the relationship between pixels and object queries and propose to reformulate the cross-attention learning as a clustering process. Inspired by the traditional k-means clustering algorithm, we develop a k-means Mask Xformer (kMaX-DeepLab) for segmentation tasks, which not only improves the state-of-the-art, but also enjoys a simple and elegant design. As a result, our kMaX-DeepLab achieves a new state-of-the-art performance on COCO val set with 58.0\% PQ, and Cityscapes val set with 68.4\% PQ, 44.0\% AP, and 83.5\% mIoU without test-time augmentation or external dataset. We hope our work can shed some light on designing transformers tailored for vision tasks. Code and models are available at https://github.com/google-research/deeplab2}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yu-k-means%20Mask%20Transformer-2022-ECCV.pdf}
}
@article{Panoptic-DeepLabChengCVPR2020, 
year = {2020}, 
title = {{Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation}}, 
author = {Cheng, Bowen and Collins, Maxwell D. and Zhu, Yukun and Liu, Ting and Huang, Thomas S. and Adam, Hartwig and Chen, Liang-Chieh}, 
journal = {CVPR}, 
abstract = {{In this work, we introduce Panoptic-DeepLab, a simple, strong, and fast system for panoptic segmentation, aiming to establish a solid baseline for bottom-up methods that can achieve comparable performance of two-stage methods while yielding fast inference speed. In particular, Panoptic-DeepLab adopts the dual-ASPP and dual-decoder structures specific to semantic, and instance segmentation, respectively. The semantic segmentation branch is the same as the typical design of any semantic segmentation model (e.g., DeepLab), while the instance segmentation branch is class-agnostic, involving a simple instance center regression. As a result, our single Panoptic-DeepLab simultaneously ranks first at all three Cityscapes benchmarks, setting the new state-of-art of 84.2\% mIoU, 39.0\% Ap, and 65.5\% PQ on test set. Additionally, equipped with MobileNetV3, Panoptic-DeepLab runs nearly in real-time with a single 1025 ×2049 image (15.8 frames per second), while achieving a competitive performance on Cityscapes (54.1 PQ\% on test set). On Mapillary Vistas test set, our ensemble of six models attains 42.7\% PQ, outperforming the challenge winner in 2018 by a healthy margin of 1.5\%. Finally, our Panoptic-DeepLab also performs on par with several top-down approaches on the challenging COCO dataset. For the first time, we demonstrate a bottom-up approach could deliver state-of-the-art results on panoptic segmentation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Cheng-Panoptic-DeepLab-%20A%20Simple,%20Strong,%20and%20Fast%20Baseline%20for%20Bottom-Up%20Panoptic%20Segmentation-2020-CVPR.pdf}
}
@article{CMT-DeepLabYuCVPR2022, 
year = {2022}, 
title = {{CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation}}, 
author = {Yu, Qihang and Wang, Huiyu and Kim, Dahun and Qiao, Siyuan and Collins, Maxwell and Zhu, Yukun and Adam, Hartwig and Yuille, Alan and Chen, Liang-Chieh}, 
journal = {CVPR}, 
abstract = {{We propose Clustering Mask Transformer (CMT-DeepLab), a transformer-based framework for panoptic segmentation designed around clustering. It rethinks the existing transformer architectures used in segmentation and detection; CMT-DeepLab considers the object queries as cluster centers, which fill the role of grouping the pixels when applied to segmentation. The clustering is computed with an alternating procedure, by first assigning pixels to the clusters by their feature affinity, and then updating the cluster centers and pixel features. Together, these operations comprise the Clustering Mask Transformer (CMT) layer, which produces cross-attention that is denser and more consistent with the final segmentation task. CMT-DeepLab improves the performance over prior art significantly by 4.4\% PQ, achieving a new state-of-the-art of 55.7\% PQ on the COCO test-dev set.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yu-CMT-DeepLab-%20Clustering%20Mask%20Transformers%20for%20Panoptic%20Segmentation-2022-CVPR.pdf}
}
@article{Axial-DeepLabWangECCV2020, 
year = {2020}, 
title = {{Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation}}, 
author = {Wang, Huiyu and Zhu, Yukun and Green, Bradley and Adam, Hartwig and Yuille, Alan and Chen, Liang-Chieh}, 
journal = {ECCV}, 
eprint = {2003.07853}, 
abstract = {{Convolution exploits locality for efficiency at a cost of missing long range context. Self-attention has been adopted to augment CNNs with non-local interactions. Recent works prove it possible to stack self-attention layers to obtain a fully attentional network by restricting the attention to a local region. In this paper, we attempt to remove this constraint by factorizing 2D self-attention into two 1D self-attentions. This reduces computation complexity and allows performing attention within a larger or even global region. In companion, we also propose a position-sensitive self-attention design. Combining both yields our position-sensitive axial-attention layer, a novel building block that one could stack to form axial-attention models for image classification and dense prediction. We demonstrate the effectiveness of our model on four large-scale datasets. In particular, our model outperforms all existing stand-alone self-attention models on ImageNet. Our Axial-DeepLab improves 2.8\% PQ over bottom-up state-of-the-art on COCO test-dev. This previous state-of-the-art is attained by our small variant that is 3.8x parameter-efficient and 27x computation-efficient. Axial-DeepLab also achieves state-of-the-art results on Mapillary Vistas and Cityscapes.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Axial-DeepLab-%20Stand-Alone%20Axial-Attention%20for%20Panoptic%20Segmentation-2020-ECCV.pdf}
}
@article{PLADingCVPR2023, 
year = {2023}, 
title = {{PLA: Language-Driven Open-Vocabulary 3D Scene Understanding}}, 
author = {Ding, Runyu and Yang, Jihan and Xue, Chuhui and Zhang, Wenqing and Bai, Song and Qi, Xiaojuan}, 
journal = {CVPR}, 
eprint = {2211.16312}, 
abstract = {{Open-vocabulary scene understanding aims to localize and recognize unseen categories beyond the annotated label space. The recent breakthrough of 2D open-vocabulary perception is largely driven by Internet-scale paired image-text data with rich vocabulary concepts. However, this success cannot be directly transferred to 3D scenarios due to the inaccessibility of large-scale 3D-text pairs. To this end, we propose to distill knowledge encoded in pre-trained vision-language (VL) foundation models through captioning multi-view images from 3D, which allows explicitly associating 3D and semantic-rich captions. Further, to foster coarse-to-fine visual-semantic representation learning from captions, we design hierarchical 3D-caption pairs, leveraging geometric constraints between 3D scenes and multi-view images. Finally, by employing contrastive learning, the model learns language-aware embeddings that connect 3D and text for open-vocabulary tasks. Our method not only remarkably outperforms baseline methods by 25.8\% \$\textbackslashsim\$ 44.7\% hIoU and 14.5\% \$\textbackslashsim\$ 50.4\% hAP\$\_\{50\}\$ in open-vocabulary semantic and instance segmentation, but also shows robust transferability on challenging zero-shot domain transfer tasks. See the project website at https://dingry.github.io/projects/PLA.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ding-PLA-%20Language-Driven%20Open-Vocabulary%203D%20Scene%20Understanding-2023-CVPR.pdf}
}
@article{TuningPintoarXiv2023, 
year = {2023}, 
title = {{Tuning computer vision models with task rewards}}, 
author = {Pinto, André Susano and Kolesnikov, Alexander and Shi, Yuge and Beyer, Lucas and Zhai, Xiaohua}, 
journal = {arXiv}, 
eprint = {2302.08242}, 
abstract = {{Misalignment between model predictions and intended usage can be detrimental for the deployment of computer vision models. The issue is exacerbated when the task involves complex structured outputs, as it becomes harder to design procedures which address this misalignment. In natural language processing, this is often addressed using reinforcement learning techniques that align models with a task reward. We adopt this approach and show its surprising effectiveness across multiple computer vision tasks, such as object detection, panoptic segmentation, colorization and image captioning. We believe this approach has the potential to be widely useful for better aligning models with a diverse range of computer vision tasks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Pinto-Tuning%20computer%20vision%20models%20with%20task%20rewards-2023-arXiv.pdf}
}
@article{IoU-NetJiangECCV2018, 
year = {2018}, 
title = {{Acquisition of Localization Confidence for Accurate Object Detection}}, 
author = {Jiang, Borui and Luo, Ruixuan and Mao, Jiayuan and Xiao, Tete and Jiang, Yuning}, 
journal = {ECCV}, 
eprint = {1807.11590}, 
abstract = {{Modern CNN-based object detectors rely on bounding box regression and non-maximum suppression to localize objects. While the probabilities for class labels naturally reflect classification confidence, localization confidence is absent. This makes properly localized bounding boxes degenerate during iterative regression or even suppressed during NMS. In the paper we propose IoU-Net learning to predict the IoU between each detected bounding box and the matched ground-truth. The network acquires this confidence of localization, which improves the NMS procedure by preserving accurately localized bounding boxes. Furthermore, an optimization-based bounding box refinement method is proposed, where the predicted IoU is formulated as the objective. Extensive experiments on the MS-COCO dataset show the effectiveness of IoU-Net, as well as its compatibility with and adaptivity to several state-of-the-art object detectors.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Jiang-Acquisition%20of%20Localization%20Confidence%20for%20Accurate%20Object%20Detection-2018-ECCV.pdf}
}
@article{GFLLiNeurIPS2020, 
year = {2020}, 
title = {{Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection}}, 
author = {Li, Xiang and Wang, Wenhai and Wu, Lijun and Chen, Shuo and Hu, Xiaolin and Li, Jun and Tang, Jinhui and Yang, Jian}, 
journal = {NeurIPS}, 
eprint = {2006.04388}, 
abstract = {{One-stage detector basically formulates object detection as dense classification and localization. The classification is usually optimized by Focal Loss and the box location is commonly learned under Dirac delta distribution. A recent trend for one-stage detectors is to introduce an individual prediction branch to estimate the quality of localization, where the predicted quality facilitates the classification to improve detection performance. This paper delves into the representations of the above three fundamental elements: quality estimation, classification and localization. Two problems are discovered in existing practices, including (1) the inconsistent usage of the quality estimation and classification between training and inference and (2) the inflexible Dirac delta distribution for localization when there is ambiguity and uncertainty in complex scenes. To address the problems, we design new representations for these elements. Specifically, we merge the quality estimation into the class prediction vector to form a joint representation of localization quality and classification, and use a vector to represent arbitrary distribution of box locations. The improved representations eliminate the inconsistency risk and accurately depict the flexible distribution in real data, but contain continuous labels, which is beyond the scope of Focal Loss. We then propose Generalized Focal Loss (GFL) that generalizes Focal Loss from its discrete form to the continuous version for successful optimization. On COCO test-dev, GFL achieves 45.0\textbackslash\% AP using ResNet-101 backbone, surpassing state-of-the-art SAPD (43.5\textbackslash\%) and ATSS (43.6\textbackslash\%) with higher or comparable inference speed, under the same backbone and training settings. Notably, our best model can achieve a single-model single-scale AP of 48.2\textbackslash\%, at 10 FPS on a single 2080Ti GPU. Code and models are available at https://github.com/implus/GFocal.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Generalized%20Focal%20Loss-%20Learning%20Qualified%20and%20Distributed%20Bounding%20Boxes%20for%20Dense%20Object%20Detection-2020-NeurIPS.pdf}
}
@article{SeesawLossWangCVPR2021, 
year = {2021}, 
title = {{Seesaw Loss for Long-Tailed Instance Segmentation}}, 
author = {Wang, Jiaqi and Zhang, Wenwei and Zang, Yuhang and Cao, Yuhang and Pang, Jiangmiao and Gong, Tao and Chen, Kai and Liu, Ziwei and Loy, Chen Change and Lin, Dahua}, 
journal = {CVPR}, 
eprint = {2008.10032}, 
abstract = {{Instance segmentation has witnessed a remarkable progress on class-balanced benchmarks. However, they fail to perform as accurately in real-world scenarios, where the category distribution of objects naturally comes with a long tail. Instances of head classes dominate a long-tailed dataset and they serve as negative samples of tail categories. The overwhelming gradients of negative samples on tail classes lead to a biased learning process for classifiers. Consequently, objects of tail categories are more likely to be misclassified as backgrounds or head categories. To tackle this problem, we propose Seesaw Loss to dynamically re-balance gradients of positive and negative samples for each category, with two complementary factors, i.e., mitigation factor and compensation factor. The mitigation factor reduces punishments to tail categories w.r.t. the ratio of cumulative training instances between different categories. Meanwhile, the compensation factor increases the penalty of misclassified instances to avoid false positives of tail categories. We conduct extensive experiments on Seesaw Loss with mainstream frameworks and different data sampling strategies. With a simple end-to-end training pipeline, Seesaw Loss obtains significant gains over Cross-Entropy Loss, and achieves state-of-the-art performance on LVIS dataset without bells and whistles. Code is available at https://github.com/open-mmlab/mmdetection.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Seesaw%20Loss%20for%20Long-Tailed%20Instance%20Segmentation-2021-CVPR.pdf}
}
@article{PainterWangCVPR2023, 
year = {2023}, 
title = {{Images Speak in Images: A Generalist Painter for In-Context Visual Learning}}, 
author = {Wang, Xinlong and Wang, Wen and Cao, Yue and Shen, Chunhua and Huang, Tiejun}, 
journal = {CVPR}, 
eprint = {2212.02499}, 
abstract = {{In-context learning, as a new paradigm in NLP, allows the model to rapidly adapt to various tasks with only a handful of prompts and examples. But in computer vision, the difficulties for in-context learning lie in that tasks vary significantly in the output representations, thus it is unclear how to define the general-purpose task prompts that the vision model can understand and transfer to out-of-domain tasks. In this work, we present Painter, a generalist model which addresses these obstacles with an "image"-centric solution, that is, to redefine the output of core vision tasks as images, and specify task prompts as also images. With this idea, our training process is extremely simple, which performs standard masked image modeling on the stitch of input and output image pairs. This makes the model capable of performing tasks conditioned on visible image patches. Thus, during inference, we can adopt a pair of input and output images from the same task as the input condition, to indicate which task to perform. Without bells and whistles, our generalist Painter can achieve competitive performance compared to well-established task-specific models, on seven representative vision tasks ranging from high-level visual understanding to low-level image processing. Painter significantly outperforms recent generalist models on several challenging tasks. Surprisingly, our model shows capabilities of completing out-of-domain tasks, which do not exist in the training data, such as open-category keypoint detection and object segmentation, validating the powerful task transferability of in-context learning.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Images%20Speak%20in%20Images-%20A%20Generalist%20Painter%20for%20In-Context%20Visual%20Learning-2023-CVPR.pdf}
}
@article{MedNeXtRoyarXiv2023, 
year = {2023}, 
title = {{MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image Segmentation}}, 
author = {Roy, Saikat and Koehler, Gregor and Ulrich, Constantin and Baumgartner, Michael and Petersen, Jens and Isensee, Fabian and Jaeger, Paul F and Maier-Hein, Klaus}, 
journal = {arXiv}, 
eprint = {2303.09975}, 
abstract = {{There has been exploding interest in embracing Transformer-based architectures for medical image segmentation. However, the lack of large-scale annotated medical datasets make achieving performances equivalent to those in natural images challenging. Convolutional networks, in contrast, have higher inductive biases and consequently, are easily trainable to high performance. Recently, the ConvNeXt architecture attempted to modernize the standard ConvNet by mirroring Transformer blocks. In this work, we improve upon this to design a modernized and scalable convolutional architecture customized to challenges of data-scarce medical settings. We introduce MedNeXt, a Transformer-inspired large kernel segmentation network which introduces - 1) A fully ConvNeXt 3D Encoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up and downsampling blocks to preserve semantic richness across scales, 3) A novel technique to iteratively increase kernel sizes by upsampling small kernel networks, to prevent performance saturation on limited medical data, 4) Compound scaling at multiple levels (depth, width, kernel size) of MedNeXt. This leads to state-of-the-art performance on 4 tasks on CT and MRI modalities and varying dataset sizes, representing a modernized deep architecture for medical image segmentation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Roy-MedNeXt-%20Transformer-driven%20Scaling%20of%20ConvNets%20for%20Medical%20Image%20Segmentation-2023-arXiv.pdf}
}
@article{LiMLiCVPR2023, 
year = {2023}, 
title = {{Less is More: Reducing Task and Model Complexity for 3D Point Cloud Semantic Segmentation}}, 
author = {Li, Li and Shum, Hubert P H and Breckon, Toby P}, 
journal = {CVPR}, 
eprint = {2303.11203}, 
abstract = {{Whilst the availability of 3D LiDAR point cloud data has significantly grown in recent years, annotation remains expensive and time-consuming, leading to a demand for semi-supervised semantic segmentation methods with application domains such as autonomous driving. Existing work very often employs relatively large segmentation backbone networks to improve segmentation accuracy, at the expense of computational costs. In addition, many use uniform sampling to reduce ground truth data requirements for learning needed, often resulting in sub-optimal performance. To address these issues, we propose a new pipeline that employs a smaller architecture, requiring fewer ground-truth annotations to achieve superior segmentation accuracy compared to contemporary approaches. This is facilitated via a novel Sparse Depthwise Separable Convolution module that significantly reduces the network parameter count while retaining overall task performance. To effectively sub-sample our training data, we propose a new Spatio-Temporal Redundant Frame Downsampling (ST-RFD) method that leverages knowledge of sensor motion within the environment to extract a more diverse subset of training data frame samples. To leverage the use of limited annotated data samples, we further propose a soft pseudo-label method informed by LiDAR reflectivity. Our method outperforms contemporary semi-supervised work in terms of mIoU, using less labeled data, on the SemanticKITTI (59.5@5\%) and ScribbleKITTI (58.1@5\%) benchmark datasets, based on a 2.3x reduction in model parameters and 641x fewer multiply-add operations whilst also demonstrating significant performance improvement on limited training data (i.e., Less is More).}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Less%20is%20More-%20Reducing%20Task%20and%20Model%20Complexity%20for%203D%20Point%20Cloud%20Semantic%20Segmentation-2023-CVPR.pdf}
}
@article{GSSChenCVPR2023, 
year = {2023}, 
title = {{Generative Semantic Segmentation}}, 
author = {Chen, Jiaqi and Lu, Jiachen and Zhu, Xiatian and Zhang, Li}, 
journal = {CVPR}, 
eprint = {2303.11316}, 
abstract = {{We present Generative Semantic Segmentation (GSS), a generative learning approach for semantic segmentation. Uniquely, we cast semantic segmentation as an image-conditioned mask generation problem. This is achieved by replacing the conventional per-pixel discriminative learning with a latent prior learning process. Specifically, we model the variational posterior distribution of latent variables given the segmentation mask. To that end, the segmentation mask is expressed with a special type of image (dubbed as maskige). This posterior distribution allows to generate segmentation masks unconditionally. To achieve semantic segmentation on a given image, we further introduce a conditioning network. It is optimized by minimizing the divergence between the posterior distribution of maskige (i.e., segmentation masks) and the latent prior distribution of input training images. Extensive experiments on standard benchmarks show that our GSS can perform competitively to prior art alternatives in the standard semantic segmentation setting, whilst achieving a new state of the art in the more challenging cross-domain setting.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Generative%20Semantic%20Segmentation-2023-CVPR.pdf}
}
@article{AugSegZhaoCVPR2023, 
year = {2023}, 
title = {{Augmentation Matters: A Simple-yet-Effective Approach to Semi-supervised Semantic Segmentation}}, 
author = {Zhao, Zhen and Yang, Lihe and Long, Sifan and Pi, Jimin and Zhou, Luping and Wang, Jingdong}, 
journal = {CVPR}, 
eprint = {2212.04976}, 
abstract = {{Recent studies on semi-supervised semantic segmentation (SSS) have seen fast progress. Despite their promising performance, current state-of-the-art methods tend to increasingly complex designs at the cost of introducing more network components and additional training procedures. Differently, in this work, we follow a standard teacher-student framework and propose AugSeg, a simple and clean approach that focuses mainly on data perturbations to boost the SSS performance. We argue that various data augmentations should be adjusted to better adapt to the semi-supervised scenarios instead of directly applying these techniques from supervised learning. Specifically, we adopt a simplified intensity-based augmentation that selects a random number of data transformations with uniformly sampling distortion strengths from a continuous space. Based on the estimated confidence of the model on different unlabeled samples, we also randomly inject labelled information to augment the unlabeled samples in an adaptive manner. Without bells and whistles, our simple AugSeg can readily achieve new state-of-the-art performance on SSS benchmarks under different partition protocols.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhao-Augmentation%20Matters-%20A%20Simple-yet-Effective%20Approach%20to%20Semi-supervised%20Semantic%20Segmentation-2023-CVPR.pdf}
}
@article{iMASZhaoCVPR2023, 
year = {2023}, 
title = {{Instance-specific and Model-adaptive Supervision for Semi-supervised Semantic Segmentation}}, 
author = {Zhao, Zhen and Long, Sifan and Pi, Jimin and Wang, Jingdong and Zhou, Luping}, 
journal = {CVPR}, 
eprint = {2211.11335}, 
abstract = {{Recently, semi-supervised semantic segmentation has achieved promising performance with a small fraction of labeled data. However, most existing studies treat all unlabeled data equally and barely consider the differences and training difficulties among unlabeled instances. Differentiating unlabeled instances can promote instance-specific supervision to adapt to the model's evolution dynamically. In this paper, we emphasize the cruciality of instance differences and propose an instance-specific and model-adaptive supervision for semi-supervised semantic segmentation, named iMAS. Relying on the model's performance, iMAS employs a class-weighted symmetric intersection-over-union to evaluate quantitative hardness of each unlabeled instance and supervises the training on unlabeled data in a model-adaptive manner. Specifically, iMAS learns from unlabeled instances progressively by weighing their corresponding consistency losses based on the evaluated hardness. Besides, iMAS dynamically adjusts the augmentation for each instance such that the distortion degree of augmented instances is adapted to the model's generalization capability across the training course. Not integrating additional losses and training procedures, iMAS can obtain remarkable performance gains against current state-of-the-art approaches on segmentation benchmarks under different semi-supervised partition protocols.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhao-Instance-specific%20and%20Model-adaptive%20Supervision%20for%20Semi-supervised%20Semantic%20Segmentation-2023-CVPR.pdf}
}
@article{Copy-PasteGhiasiCVPR2021, 
year = {2021}, 
title = {{Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation}}, 
author = {Ghiasi, Golnaz and Cui, Yin and Srinivas, Aravind and Qian, Rui and Lin, Tsung-Yi and Cubuk, Ekin D. and Le, Quoc V. and Zoph, Barret}, 
journal = {CVPR}, 
abstract = {{Building instance segmentation models that are data-efficient and can handle rare object categories is an important challenge in computer vision. Leveraging data augmentations is a promising direction towards addressing this challenge. Here, we perform a systematic study of the Copy-Paste augmentation (e.g., [13], [12]) for instance segmentation where we randomly paste objects onto an image. Prior studies on Copy-Paste relied on modeling the surrounding visual context for pasting the objects. However, we find that the simple mechanism of pasting objects randomly is good enough and can provide solid gains on top of strong baselines. Furthermore, we show Copy-Paste is additive with semi-supervised methods that leverage extra data through pseudo labeling (e.g. self-training). On COCO instance segmentation, we achieve 49.1 mask AP and 57.3 box AP, an improvement of +0.6 mask AP and +1.5 box AP over the previous state-of-the-art. We further demonstrate that Copy-Paste can lead to significant improvements on the LVIS benchmark. Our baseline model outperforms the LVIS 2020 Challenge winning entry by +3.6 mask AP on rare categories.1}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ghiasi-Simple%20Copy-Paste%20is%20a%20Strong%20Data%20Augmentation%20Method%20for%20Instance%20Segmentation-2021-CVPR.pdf}
}
@article{BiBenchQinICML2023, 
year = {2023}, 
title = {{BiBench: Benchmarking and Analyzing Network Binarization}}, 
author = {Qin, Haotong and Zhang, Mingyuan and Ding, Yifu and Li, Aoyu and Cai, Zhongang and Liu, Ziwei and Yu, Fisher and Liu, Xianglong}, 
journal = {ICML}, 
eprint = {2301.11233}, 
abstract = {{Network binarization emerges as one of the most promising compression approaches offering extraordinary computation and memory savings by minimizing the bit-width. However, recent research has shown that applying existing binarization algorithms to diverse tasks, architectures, and hardware in realistic scenarios is still not straightforward. Common challenges of binarization, such as accuracy degradation and efficiency limitation, suggest that its attributes are not fully understood. To close this gap, we present BiBench, a rigorously designed benchmark with in-depth analysis for network binarization. We first carefully scrutinize the requirements of binarization in the actual production and define evaluation tracks and metrics for a comprehensive and fair investigation. Then, we evaluate and analyze a series of milestone binarization algorithms that function at the operator level and with extensive influence. Our benchmark reveals that 1) the binarized operator has a crucial impact on the performance and deployability of binarized networks; 2) the accuracy of binarization varies significantly across different learning tasks and neural architectures; 3) binarization has demonstrated promising efficiency potential on edge devices despite the limited hardware support. The results and analysis also lead to a promising paradigm for accurate and efficient binarization. We believe that BiBench will contribute to the broader adoption of binarization and serve as a foundation for future research. The code for our BiBench is released https://github.com/htqin/BiBench .}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Qin-BiBench-%20Benchmarking%20and%20Analyzing%20Network%20Binarization-2023-ICML.pdf}
}
@article{PTLossZhangarXiv2023, 
year = {2023}, 
title = {{Do Not Blindly Imitate the Teacher: Using Perturbed Loss for Knowledge Distillation}}, 
author = {Zhang, Rongzhi and Shen, Jiaming and Liu, Tianqi and Liu, Jialu and Bendersky, Michael and Najork, Marc and Zhang, Chao}, 
journal = {arXiv}, 
eprint = {2305.05010}, 
abstract = {{Knowledge distillation is a popular technique to transfer knowledge from large teacher models to a small student model. Typically, the student learns to imitate the teacher by minimizing the KL divergence of its output distribution with the teacher's output distribution. In this work, we argue that such a learning objective is sub-optimal because there exists a discrepancy between the teacher's output distribution and the ground truth label distribution. Therefore, forcing the student to blindly imitate the unreliable teacher output distribution leads to inferior performance. To this end, we propose a novel knowledge distillation objective PTLoss by first representing the vanilla KL-based distillation loss function via a Maclaurin series and then perturbing the leading-order terms in this series. This perturbed loss implicitly transforms the original teacher into a proxy teacher with a distribution closer to the ground truth distribution. We establish the theoretical connection between this "distribution closeness" and the student model generalizability, which enables us to select the PTLoss's perturbation coefficients in a principled way. Extensive experiments on five datasets demonstrate PTLoss can significantly improve the distillation effectiveness for teachers of various scales.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Do%20Not%20Blindly%20Imitate%20the%20Teacher-%20Using%20Perturbed%20Loss%20for%20Knowledge%20Distillation-2023-arXiv.pdf}
}
@article{NCDRizCVPR2023, 
year = {2023}, 
title = {{Novel Class Discovery for 3D Point Cloud Semantic Segmentation}}, 
author = {Riz, Luigi and Saltori, Cristiano and Ricci, Elisa and Poiesi, Fabio}, 
journal = {CVPR}, 
eprint = {2303.11610}, 
abstract = {{Novel class discovery (NCD) for semantic segmentation is the task of learning a model that can segment unlabelled (novel) classes using only the supervision from labelled (base) classes. This problem has recently been pioneered for 2D image data, but no work exists for 3D point cloud data. In fact, the assumptions made for 2D are loosely applicable to 3D in this case. This paper is presented to advance the state of the art on point cloud data analysis in four directions. Firstly, we address the new problem of NCD for point cloud semantic segmentation. Secondly, we show that the transposition of the only existing NCD method for 2D semantic segmentation to 3D data is suboptimal. Thirdly, we present a new method for NCD based on online clustering that exploits uncertainty quantification to produce prototypes for pseudo-labelling the points of the novel classes. Lastly, we introduce a new evaluation protocol to assess the performance of NCD for point cloud semantic segmentation. We thoroughly evaluate our method on SemanticKITTI and SemanticPOSS datasets, showing that it can significantly outperform the baseline. Project page at this link: https://github.com/LuigiRiz/NOPS.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Riz-Novel%20Class%20Discovery%20for%203D%20Point%20Cloud%20Semantic%20Segmentation-2023-CVPR.pdf}
}
@article{FCFIWeiCVPR2023, 
year = {2023}, 
title = {{Focused and Collaborative Feedback Integration for Interactive Image Segmentation}}, 
author = {Wei, Qiaoqiao and Zhang, Hui and Yong, Jun-Hai}, 
journal = {CVPR}, 
eprint = {2303.11880}, 
abstract = {{Interactive image segmentation aims at obtaining a segmentation mask for an image using simple user annotations. During each round of interaction, the segmentation result from the previous round serves as feedback to guide the user's annotation and provides dense prior information for the segmentation model, effectively acting as a bridge between interactions. Existing methods overlook the importance of feedback or simply concatenate it with the original input, leading to underutilization of feedback and an increase in the number of required annotations. To address this, we propose an approach called Focused and Collaborative Feedback Integration (FCFI) to fully exploit the feedback for click-based interactive image segmentation. FCFI first focuses on a local area around the new click and corrects the feedback based on the similarities of high-level features. It then alternately and collaboratively updates the feedback and deep features to integrate the feedback into the features. The efficacy and efficiency of FCFI were validated on four benchmarks, namely GrabCut, Berkeley, SBD, and DAVIS. Experimental results show that FCFI achieved new state-of-the-art performance with less computational overhead than previous methods. The source code is available at https://github.com/veizgyauzgyauz/FCFI.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wei-Focused%20and%20Collaborative%20Feedback%20Integration%20for%20Interactive%20Image%20Segmentation-2023-CVPR.pdf}
}
@article{DNEHuCVPR2023, 
year = {2023}, 
title = {{Dense Network Expansion for Class Incremental Learning}}, 
author = {Hu, Zhiyuan and Li, Yunsheng and Lyu, Jiancheng and Gao, Dashan and Vasconcelos, Nuno}, 
journal = {CVPR}, 
eprint = {2303.12696}, 
abstract = {{The problem of class incremental learning (CIL) is considered. State-of-the-art approaches use a dynamic architecture based on network expansion (NE), in which a task expert is added per task. While effective from a computational standpoint, these methods lead to models that grow quickly with the number of tasks. A new NE method, dense network expansion (DNE), is proposed to achieve a better trade-off between accuracy and model complexity. This is accomplished by the introduction of dense connections between the intermediate layers of the task expert networks, that enable the transfer of knowledge from old to new tasks via feature sharing and reusing. This sharing is implemented with a cross-task attention mechanism, based on a new task attention block (TAB), that fuses information across tasks. Unlike traditional attention mechanisms, TAB operates at the level of the feature mixing and is decoupled with spatial attentions. This is shown more effective than a joint spatial-and-task attention for CIL. The proposed DNE approach can strictly maintain the feature space of old classes while growing the network and feature scale at a much slower rate than previous methods. In result, it outperforms the previous SOTA methods by a margin of 4\textbackslash\% in terms of accuracy, with similar or even smaller model scale.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hu-Dense%20Network%20Expansion%20for%20Class%20Incremental%20Learning-2023-CVPR.pdf}
}
@article{MaskConFengCVPR2023, 
year = {2023}, 
title = {{MaskCon: Masked Contrastive Learning for Coarse-Labelled Dataset}}, 
author = {Feng, Chen and Patras, Ioannis}, 
journal = {CVPR}, 
eprint = {2303.12756}, 
abstract = {{Deep learning has achieved great success in recent years with the aid of advanced neural network structures and large-scale human-annotated datasets. However, it is often costly and difficult to accurately and efficiently annotate large-scale datasets, especially for some specialized domains where fine-grained labels are required. In this setting, coarse labels are much easier to acquire as they do not require expert knowledge. In this work, we propose a contrastive learning method, called \$\textbackslashtextbf\{Mask\}\$ed \$\textbackslashtextbf\{Con\}\$trastive learning\textbackslashtextasciitilde(\$\textbackslashtextbf\{MaskCon\}\$) to address the under-explored problem setting, where we learn with a coarse-labelled dataset in order to address a finer labelling problem. More specifically, within the contrastive learning framework, for each sample our method generates soft-labels with the aid of coarse labels against other samples and another augmented view of the sample in question. By contrast to self-supervised contrastive learning where only the sample's augmentations are considered hard positives, and in supervised contrastive learning where only samples with the same coarse labels are considered hard positives, we propose soft labels based on sample distances, that are masked by the coarse labels. This allows us to utilize both inter-sample relations and coarse labels. We demonstrate that our method can obtain as special cases many existing state-of-the-art works and that it provides tighter bounds on the generalization error. Experimentally, our method achieves significant improvement over the current state-of-the-art in various datasets, including CIFAR10, CIFAR100, ImageNet-1K, Standford Online Products and Stanford Cars196 datasets. Code and annotations are available at https://github.com/MrChenFeng/MaskCon\_CVPR2023.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Feng-MaskCon-%20Masked%20Contrastive%20Learning%20for%20Coarse-Labelled%20Dataset-2023-CVPR.pdf}
}
@article{CrOCStegmullerCVPR2023, 
year = {2023}, 
title = {{CrOC: Cross-View Online Clustering for Dense Visual Representation Learning}}, 
author = {Stegmüller, Thomas and Lebailly, Tim and Bozorgtabar, Behzad and Tuytelaars, Tinne and Thiran, Jean-Philippe}, 
journal = {CVPR}, 
eprint = {2303.13245}, 
abstract = {{Learning dense visual representations without labels is an arduous task and more so from scene-centric data. We propose to tackle this challenging problem by proposing a Cross-view consistency objective with an Online Clustering mechanism (CrOC) to discover and segment the semantics of the views. In the absence of hand-crafted priors, the resulting method is more generalizable and does not require a cumbersome pre-processing step. More importantly, the clustering algorithm conjointly operates on the features of both views, thereby elegantly bypassing the issue of content not represented in both views and the ambiguous matching of objects from one crop to the other. We demonstrate excellent performance on linear and unsupervised segmentation transfer tasks on various datasets and similarly for video object segmentation. Our code and pre-trained models are publicly available at https://github.com/stegmuel/CrOC.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Stegmüller-CrOC-%20Cross-View%20Online%20Clustering%20for%20Dense%20Visual%20Representation%20Learning-2023-CVPR.pdf}
}
@article{PrinciplesKalbCVPR2023, 
year = {2023}, 
title = {{Principles of Forgetting in Domain-Incremental Semantic Segmentation in Adverse Weather Conditions}}, 
author = {Kalb, Tobias and Beyerer, Jürgen}, 
journal = {CVPR}, 
eprint = {2303.14115}, 
abstract = {{Deep neural networks for scene perception in automated vehicles achieve excellent results for the domains they were trained on. However, in real-world conditions, the domain of operation and its underlying data distribution are subject to change. Adverse weather conditions, in particular, can significantly decrease model performance when such data are not available during training.Additionally, when a model is incrementally adapted to a new domain, it suffers from catastrophic forgetting, causing a significant drop in performance on previously observed domains. Despite recent progress in reducing catastrophic forgetting, its causes and effects remain obscure. Therefore, we study how the representations of semantic segmentation models are affected during domain-incremental learning in adverse weather conditions. Our experiments and representational analyses indicate that catastrophic forgetting is primarily caused by changes to low-level features in domain-incremental learning and that learning more general features on the source domain using pre-training and image augmentations leads to efficient feature reuse in subsequent tasks, which drastically reduces catastrophic forgetting. These findings highlight the importance of methods that facilitate generalized features for effective continual learning algorithms.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kalb-Principles%20of%20Forgetting%20in%20Domain-Incremental%20Semantic%20Segmentation%20in%20Adverse%20Weather%20Conditions-2023-CVPR.pdf}
}
@article{FastViTVasuarXiv2023, 
year = {2023}, 
title = {{FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization}}, 
author = {Vasu, Pavan Kumar Anasosalu and Gabriel, James and Zhu, Jeff and Tuzel, Oncel and Ranjan, Anurag}, 
journal = {arXiv}, 
eprint = {2303.14189}, 
abstract = {{The recent amalgamation of transformer and convolutional designs has led to steady improvements in accuracy and efficiency of the models. In this work, we introduce FastViT, a hybrid vision transformer architecture that obtains the state-of-the-art latency-accuracy trade-off. To this end, we introduce a novel token mixing operator, RepMixer, a building block of FastViT, that uses structural reparameterization to lower the memory access cost by removing skip-connections in the network. We further apply train-time overparametrization and large kernel convolutions to boost accuracy and empirically show that these choices have minimal effect on latency. We show that - our model is 3.5x faster than CMT, a recent state-of-the-art hybrid transformer architecture, 4.9x faster than EfficientNet, and 1.9x faster than ConvNeXt on a mobile device for the same accuracy on the ImageNet dataset. At similar latency, our model obtains 4.2\% better Top-1 accuracy on ImageNet than MobileOne. Our model consistently outperforms competing architectures across several tasks -- image classification, detection, segmentation and 3D mesh regression with significant improvement in latency on both a mobile device and a desktop GPU. Furthermore, our model is highly robust to out-of-distribution samples and corruptions, improving over competing robust models.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Vasu-FastViT-%20A%20Fast%20Hybrid%20Vision%20Transformer%20using%20Structural%20Reparameterization-2023-arXiv.pdf}
}
@article{SparksWangCVPR2023, 
year = {2023}, 
title = {{Compacting Binary Neural Networks by Sparse Kernel Selection}}, 
author = {Wang, Yikai and Huang, Wenbing and Dong, Yinpeng and Sun, Fuchun and Yao, Anbang}, 
journal = {CVPR}, 
eprint = {2303.14470}, 
abstract = {{Binary Neural Network (BNN) represents convolution weights with 1-bit values, which enhances the efficiency of storage and computation. This paper is motivated by a previously revealed phenomenon that the binary kernels in successful BNNs are nearly power-law distributed: their values are mostly clustered into a small number of codewords. This phenomenon encourages us to compact typical BNNs and obtain further close performance through learning non-repetitive kernels within a binary kernel subspace. Specifically, we regard the binarization process as kernel grouping in terms of a binary codebook, and our task lies in learning to select a smaller subset of codewords from the full codebook. We then leverage the Gumbel-Sinkhorn technique to approximate the codeword selection process, and develop the Permutation Straight-Through Estimator (PSTE) that is able to not only optimize the selection process end-to-end but also maintain the non-repetitive occupancy of selected codewords. Experiments verify that our method reduces both the model size and bit-wise computational costs, and achieves accuracy improvements compared with state-of-the-art BNNs under comparable budgets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Compacting%20Binary%20Neural%20Networks%20by%20Sparse%20Kernel%20Selection-2023-CVPR.pdf}
}
@article{YOSOHuCVPR2023, 
year = {2023}, 
title = {{You Only Segment Once: Towards Real-Time Panoptic Segmentation}}, 
author = {Hu, Jie and Huang, Linyan and Ren, Tianhe and Zhang, Shengchuan and Ji, Rongrong and Cao, Liujuan}, 
journal = {CVPR}, 
eprint = {2303.14651}, 
abstract = {{In this paper, we propose YOSO, a real-time panoptic segmentation framework. YOSO predicts masks via dynamic convolutions between panoptic kernels and image feature maps, in which you only need to segment once for both instance and semantic segmentation tasks. To reduce the computational overhead, we design a feature pyramid aggregator for the feature map extraction, and a separable dynamic decoder for the panoptic kernel generation. The aggregator re-parameterizes interpolation-first modules in a convolution-first way, which significantly speeds up the pipeline without any additional costs. The decoder performs multi-head cross-attention via separable dynamic convolution for better efficiency and accuracy. To the best of our knowledge, YOSO is the first real-time panoptic segmentation framework that delivers competitive performance compared to state-of-the-art models. Specifically, YOSO achieves 46.4 PQ, 45.6 FPS on COCO; 52.5 PQ, 22.6 FPS on Cityscapes; 38.0 PQ, 35.4 FPS on ADE20K; and 34.1 PQ, 7.1 FPS on Mapillary Vistas. Code is available at https://github.com/hujiecpp/YOSO.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hu-You%20Only%20Segment%20Once-%20Towards%20Real-Time%20Panoptic%20Segmentation-2023-CVPR.pdf}
}
@article{WinCLIPJeongCVPR2023, 
year = {2023}, 
title = {{WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation}}, 
author = {Jeong, Jongheon and Zou, Yang and Kim, Taewan and Zhang, Dongqing and Ravichandran, Avinash and Dabeer, Onkar}, 
journal = {CVPR}, 
eprint = {2303.14814}, 
abstract = {{Visual anomaly classification and segmentation are vital for automating industrial quality inspection. The focus of prior research in the field has been on training custom models for each quality inspection task, which requires task-specific images and annotation. In this paper we move away from this regime, addressing zero-shot and few-normal-shot anomaly classification and segmentation. Recently CLIP, a vision-language model, has shown revolutionary generality with competitive zero-/few-shot performance in comparison to full-supervision. But CLIP falls short on anomaly classification and segmentation tasks. Hence, we propose window-based CLIP (WinCLIP) with (1) a compositional ensemble on state words and prompt templates and (2) efficient extraction and aggregation of window/patch/image-level features aligned with text. We also propose its few-normal-shot extension WinCLIP+, which uses complementary information from normal images. In MVTec-AD (and VisA), without further tuning, WinCLIP achieves 91.8\%/85.1\% (78.1\%/79.6\%) AUROC in zero-shot anomaly classification and segmentation while WinCLIP+ does 93.1\%/95.2\% (83.8\%/96.4\%) in 1-normal-shot, surpassing state-of-the-art by large margins.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Jeong-WinCLIP-%20Zero--Few-Shot%20Anomaly%20Classification%20and%20Segmentation-2023-CVPR.pdf}
}
@article{Label-FreeHuCVPR2023, 
year = {2023}, 
title = {{Label-Free Liver Tumor Segmentation}}, 
author = {Hu, Qixin and Chen, Yixiong and Xiao, Junfei and Sun, Shuwen and Chen, Jieneng and Yuille, Alan and Zhou, Zongwei}, 
journal = {CVPR}, 
eprint = {2303.14869}, 
abstract = {{We demonstrate that AI models can accurately segment liver tumors without the need for manual annotation by using synthetic tumors in CT scans. Our synthetic tumors have two intriguing advantages: (I) realistic in shape and texture, which even medical professionals can confuse with real tumors; (II) effective for training AI models, which can perform liver tumor segmentation similarly to the model trained on real tumors -- this result is exciting because no existing work, using synthetic tumors only, has thus far reached a similar or even close performance to real tumors. This result also implies that manual efforts for annotating tumors voxel by voxel (which took years to create) can be significantly reduced in the future. Moreover, our synthetic tumors can automatically generate many examples of small (or even tiny) synthetic tumors and have the potential to improve the success rate of detecting small liver tumors, which is critical for detecting the early stages of cancer. In addition to enriching the training data, our synthesizing strategy also enables us to rigorously assess the AI robustness.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hu-Label-Free%20Liver%20Tumor%20Segmentation-2023-CVPR.pdf}
}
@article{DREKhaziICLR2023, 
year = {2023}, 
title = {{Deep Ranking Ensembles for Hyperparameter Optimization}}, 
author = {Khazi, Abdus Salam and Arango, Sebastian Pineda and Grabocka, Josif}, 
journal = {ICLR}, 
eprint = {2303.15212}, 
abstract = {{Automatically optimizing the hyperparameters of Machine Learning algorithms is one of the primary open questions in AI. Existing work in Hyperparameter Optimization (HPO) trains surrogate models for approximating the response surface of hyperparameters as a regression task. In contrast, we hypothesize that the optimal strategy for training surrogates is to preserve the ranks of the performances of hyperparameter configurations as a Learning to Rank problem. As a result, we present a novel method that meta-learns neural network surrogates optimized for ranking the configurations' performances while modeling their uncertainty via ensembling. In a large-scale experimental protocol comprising 12 baselines, 16 HPO search spaces and 86 datasets/tasks, we demonstrate that our method achieves new state-of-the-art results in HPO.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Khazi-Deep%20Ranking%20Ensembles%20for%20Hyperparameter%20Optimization-2023-ICLR.pdf}
}
@article{DisWOTDongCVPR2023, 
year = {2023}, 
title = {{DisWOT: Student Architecture Search for Distillation WithOut Training}}, 
author = {Dong, Peijie and Li, Lujun and Wei, Zimian}, 
journal = {CVPR}, 
eprint = {2303.15678}, 
abstract = {{Knowledge distillation (KD) is an effective training strategy to improve the lightweight student models under the guidance of cumbersome teachers. However, the large architecture difference across the teacher-student pairs limits the distillation gains. In contrast to previous adaptive distillation methods to reduce the teacher-student gap, we explore a novel training-free framework to search for the best student architectures for a given teacher. Our work first empirically show that the optimal model under vanilla training cannot be the winner in distillation. Secondly, we find that the similarity of feature semantics and sample relations between random-initialized teacher-student networks have good correlations with final distillation performances. Thus, we efficiently measure similarity matrixs conditioned on the semantic activation maps to select the optimal student via an evolutionary algorithm without any training. In this way, our student architecture search for Distillation WithOut Training (DisWOT) significantly improves the performance of the model in the distillation stage with at least 180\$\textbackslashtimes\$ training acceleration. Additionally, we extend similarity metrics in DisWOT as new distillers and KD-based zero-proxies. Our experiments on CIFAR, ImageNet and NAS-Bench-201 demonstrate that our technique achieves state-of-the-art results on different search spaces. Our project and code are available at https://lilujunai.github.io/DisWOT-CVPR2023/.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Dong-DisWOT-%20Student%20Architecture%20Search%20for%20Distillation%20WithOut%20Training-2023-CVPR.pdf}
}
@article{TransUNetChenICMLWorkshop2021, 
year = {2021}, 
title = {{TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation}}, 
author = {Chen, Jieneng and Lu, Yongyi and Yu, Qihang and Luo, Xiangde and Adeli, Ehsan and Wang, Yan and Lu, Le and Yuille, Alan L and Zhou, Yuyin}, 
journal = {ICML Workshop}, 
eprint = {2102.04306}, 
abstract = {{Medical image segmentation is an essential prerequisite for developing healthcare systems, especially for disease diagnosis and treatment planning. On various medical image segmentation tasks, the u-shaped architecture, also known as U-Net, has become the de-facto standard and achieved tremendous success. However, due to the intrinsic locality of convolution operations, U-Net generally demonstrates limitations in explicitly modeling long-range dependency. Transformers, designed for sequence-to-sequence prediction, have emerged as alternative architectures with innate global self-attention mechanisms, but can result in limited localization abilities due to insufficient low-level details. In this paper, we propose TransUNet, which merits both Transformers and U-Net, as a strong alternative for medical image segmentation. On one hand, the Transformer encodes tokenized image patches from a convolution neural network (CNN) feature map as the input sequence for extracting global contexts. On the other hand, the decoder upsamples the encoded features which are then combined with the high-resolution CNN feature maps to enable precise localization. We argue that Transformers can serve as strong encoders for medical image segmentation tasks, with the combination of U-Net to enhance finer details by recovering localized spatial information. TransUNet achieves superior performances to various competing methods on different medical applications including multi-organ segmentation and cardiac segmentation. Code and models are available at https://github.com/Beckschen/TransUNet.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-TransUNet-%20Transformers%20Make%20Strong%20Encoders%20for%20Medical%20Image%20Segmentation-2021-ICML%20Workshop.pdf}
}
@article{DconnNetYangCVPR2023, 
year = {2023}, 
title = {{Directional Connectivity-based Segmentation of Medical Images}}, 
author = {Yang, Ziyun and Farsiu, Sina}, 
journal = {CVPR}, 
eprint = {2304.00145}, 
abstract = {{Anatomical consistency in biomarker segmentation is crucial for many medical image analysis tasks. A promising paradigm for achieving anatomically consistent segmentation via deep networks is incorporating pixel connectivity, a basic concept in digital topology, to model inter-pixel relationships. However, previous works on connectivity modeling have ignored the rich channel-wise directional information in the latent space. In this work, we demonstrate that effective disentanglement of directional sub-space from the shared latent space can significantly enhance the feature representation in the connectivity-based network. To this end, we propose a directional connectivity modeling scheme for segmentation that decouples, tracks, and utilizes the directional information across the network. Experiments on various public medical image segmentation benchmarks show the effectiveness of our model as compared to the state-of-the-art methods. Code is available at https://github.com/Zyun-Y/DconnNet.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-Directional%20Connectivity-based%20Segmentation%20of%20Medical%20Images-2023-CVPR.pdf}
}
@article{t-SNEMaatenJMLR2008, 
year = {2008}, 
title = {{Visualizing Data using t-SNE}}, 
author = {Maaten, Laurens van der and Hinton, Geoffrey}, 
journal = {JMLR}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Maaten-Visualizing%20Data%20using%20t-SNE-2008-JMLR.pdf}
}
@article{PPLKwonCVPR2023, 
year = {2023}, 
title = {{Probabilistic Prompt Learning for Dense Prediction}}, 
author = {Kwon, Hyeongjun and Song, Taeyong and Jeong, Somi and Kim, Jin and Jang, Jinhyun and Sohn, Kwanghoon}, 
journal = {CVPR}, 
eprint = {2304.00779}, 
abstract = {{Recent progress in deterministic prompt learning has become a promising alternative to various downstream vision tasks, enabling models to learn powerful visual representations with the help of pre-trained vision-language models. However, this approach results in limited performance for dense prediction tasks that require handling more complex and diverse objects, since a single and deterministic description cannot sufficiently represent the entire image. In this paper, we present a novel probabilistic prompt learning to fully exploit the vision-language knowledge in dense prediction tasks. First, we introduce learnable class-agnostic attribute prompts to describe universal attributes across the object class. The attributes are combined with class information and visual-context knowledge to define the class-specific textual distribution. Text representations are sampled and used to guide the dense prediction task using the probabilistic pixel-text matching loss, enhancing the stability and generalization capability of the proposed method. Extensive experiments on different dense prediction tasks and ablation studies demonstrate the effectiveness of our proposed method.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kwon-Probabilistic%20Prompt%20Learning%20for%20Dense%20Prediction-2023-CVPR.pdf}
}
@article{TunableConvolutionsMaggioniCVPR2023, 
year = {2023}, 
title = {{Tunable Convolutions with Parametric Multi-Loss Optimization}}, 
author = {Maggioni, Matteo and Tanay, Thomas and Babiloni, Francesca and McDonagh, Steven and Leonardis, Aleš}, 
journal = {CVPR}, 
eprint = {2304.00898}, 
abstract = {{Behavior of neural networks is irremediably determined by the specific loss and data used during training. However it is often desirable to tune the model at inference time based on external factors such as preferences of the user or dynamic characteristics of the data. This is especially important to balance the perception-distortion trade-off of ill-posed image-to-image translation tasks. In this work, we propose to optimize a parametric tunable convolutional layer, which includes a number of different kernels, using a parametric multi-loss, which includes an equal number of objectives. Our key insight is to use a shared set of parameters to dynamically interpolate both the objectives and the kernels. During training, these parameters are sampled at random to explicitly optimize all possible combinations of objectives and consequently disentangle their effect into the corresponding kernels. During inference, these parameters become interactive inputs of the model hence enabling reliable and consistent control over the model behavior. Extensive experimental results demonstrate that our tunable convolutions effectively work as a drop-in replacement for traditional convolutions in existing neural networks at virtually no extra computational cost, outperforming state-of-the-art control strategies in a wide range of applications; including image denoising, deblurring, super-resolution, and style transfer.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Maggioni-Tunable%20Convolutions%20with%20Parametric%20Multi-Loss%20Optimization-2023-CVPR.pdf}
}
@article{CoOpZhouIJCV2022, 
year = {2022}, 
title = {{Learning to Prompt for Vision-Language Models}}, 
author = {Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei}, 
journal = {IJCV}, 
eprint = {2109.01134}, 
abstract = {{Large pre-trained vision-language models like CLIP have shown great potential in learning representations that are transferable across a wide range of downstream tasks. Different from the traditional representation learning that is based mostly on discretized labels, vision-language pre-training aligns images and texts in a common feature space, which allows zero-shot transfer to a downstream task via prompting, i.e., classification weights are synthesized from natural language describing classes of interest. In this work, we show that a major challenge for deploying such models in practice is prompt engineering, which requires domain expertise and is extremely time-consuming—one needs to spend a significant amount of time on words tuning since a slight change in wording could have a huge impact on performance. Inspired by recent advances in prompt learning research in natural language processing (NLP), we propose Context Optimization (CoOp), a simple approach specifically for adapting CLIP-like vision-language models for downstream image recognition. Concretely, CoOp models a prompt’s context words with learnable vectors while the entire pre-trained parameters are kept fixed. To handle different image recognition tasks, we provide two implementations of CoOp: unified context and class-specific context. Through extensive experiments on 11 datasets, we demonstrate that CoOp requires as few as one or two shots to beat hand-crafted prompts with a decent margin and is able to gain significant improvements over prompt engineering with more shots, e.g., with 16 shots the average gain is around 15\% (with the highest reaching over 45\%). Despite being a learning-based approach, CoOp achieves superb domain generalization performance compared with the zero-shot model using hand-crafted prompts.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhou-Learning%20to%20Prompt%20for%20Vision-Language%20Models-2022-IJCV.pdf}
}
@article{ProDALuCVPR2022, 
year = {2022}, 
title = {{Prompt Distribution Learning}}, 
author = {Lu, Yuning and Liu, Jianzhuang and Zhang, Yonggang and Liu, Yajing and Tian, Xinmei}, 
journal = {CVPR}, 
eprint = {2205.03340}, 
abstract = {{We present prompt distribution learning for effectively adapting a pre-trained vision-language model to address downstream recognition tasks. Our method not only learns low-bias prompts from a few samples but also captures the distribution of diverse prompts to handle the varying visual representations. In this way, we provide high-quality task-related content for facilitating recognition. This prompt distribution learning is realized by an efficient approach that learns the output embeddings of prompts instead of the input embeddings. Thus, we can employ a Gaussian distribution to model them effectively and derive a surrogate loss for efficient training. Extensive experiments on 12 datasets demonstrate that our method consistently and significantly outperforms existing methods. For example, with 1 sample per category, it relatively improves the average result by 9.1\% compared to human-crafted prompts.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lu-Prompt%20Distribution%20Learning-2022-CVPR.pdf}
}
@article{RethinkingTianECCV2020, 
year = {2020}, 
title = {{Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?}}, 
author = {Tian, Yonglong and Wang, Yue and Krishnan, Dilip and Tenenbaum, Joshua B and Isola, Phillip}, 
journal = {ECCV}, 
eprint = {2003.11539}, 
abstract = {{The focus of recent meta-learning research has been on the development of learning algorithms that can quickly adapt to test time tasks with limited data and low computational cost. Few-shot learning is widely used as one of the standard benchmarks in meta-learning. In this work, we show that a simple baseline: learning a supervised or self-supervised representation on the meta-training set, followed by training a linear classifier on top of this representation, outperforms state-of-the-art few-shot learning methods. An additional boost can be achieved through the use of self-distillation. This demonstrates that using a good learned embedding model can be more effective than sophisticated meta-learning algorithms. We believe that our findings motivate a rethinking of few-shot image classification benchmarks and the associated role of meta-learning algorithms. Code is available at: http://github.com/WangYueFt/rfs/.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tian-Rethinking%20Few-Shot%20Image%20Classification-%20a%20Good%20Embedding%20Is%20All%20You%20Need--2020-ECCV.pdf}
}
@article{DenseCLIPRaoCVPR2022, 
year = {2022}, 
title = {{DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting}}, 
author = {Rao, Yongming and Zhao, Wenliang and Chen, Guangyi and Tang, Yansong and Zhu, Zheng and Huang, Guan and Zhou, Jie and Lu, Jiwen}, 
journal = {CVPR}, 
abstract = {{Recent progress has shown that large-scale pre-training using contrastive image-text pairs can be a promising alternative for high-quality visual representation learning from natural language supervision. Benefiting from a broader source of supervision, this new paradigm exhibits impressive transferability to downstream classification tasks and datasets. However, the problem of transferring the knowledge learned from image-text pairs to more complex dense prediction tasks has barely been visited. In this work, we present a new framework for dense prediction by implicitly and explicitly leveraging the pre-trained knowledge from CLIP. Specifically, we convert the original image-text matching problem in CLIP to a pixel-text matching problem and use the pixel-text score maps to guide the learning of dense prediction models. By further using the contextual information from the image to prompt the language model, we are able to facilitate our model to better exploit the pretrained knowledge. Our method is model-agnostic, which can be applied to arbitrary dense prediction systems and various pre-trained visual backbones including both CLIP models and ImageNet pre-trained models. Extensive experiments demonstrate the superior performance of our methods on semantic segmentation, object detection, and instance segmentation tasks. Code is available at https://github.com/raoyongming/DenseCLIP.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Rao-DenseCLIP-%20Language-Guided%20Dense%20Prediction%20with%20Context-Aware%20Prompting-2022-CVPR.pdf}
}
@article{RecallLossTianICRA2022, 
year = {2022}, 
title = {{Striking the Right Balance: Recall Loss for Semantic Segmentation}}, 
author = {Tian, Junjiao and Mithun, Niluthpol and Seymour, Zach and Chiu, Han-Pang and Kira, Zsolt}, 
journal = {ICRA}, 
eprint = {2106.14917}, 
abstract = {{Class imbalance is a fundamental problem in computer vision applications such as semantic segmentation. Specifically, uneven class distributions in a training dataset often result in unsatisfactory performance on under-represented classes. Many works have proposed to weight the standard cross entropy loss function with pre-computed weights based on class statistics, such as the number of samples and class margins. There are two major drawbacks to these methods: 1) constantly up-weighting minority classes can introduce excessive false positives in semantic segmentation; 2) a minority class is not necessarily a hard class. The consequence is low precision due to excessive false positives. In this regard, we propose a hard-class mining loss by reshaping the vanilla cross entropy loss such that it weights the loss for each class dynamically based on instantaneous recall performance. We show that the novel recall loss changes gradually between the standard cross entropy loss and the inverse frequency weighted loss. Recall loss also leads to improved mean accuracy while offering competitive mean Intersection over Union (IoU) performance. On Synthia dataset, recall loss achieves \$9\textbackslash\%\$ relative improvement on mean accuracy with competitive mean IoU using DeepLab-ResNet18 compared to the cross entropy loss. Code available at https://github.com/PotatoTian/recall-semseg.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tian-Striking%20the%20Right%20Balance-%20Recall%20Loss%20for%20Semantic%20Segmentation-2022-ICRA.pdf}
}
@article{AdaptiveChenTPAMI2023, 
year = {2023}, 
title = {{Adaptive Region-Specific Loss for Improved Medical Image Segmentation}}, 
author = {Chen, Yizheng and Yu, Lequan and Wang, Jen-Yeu and Panjwani, Neil and Obeid, Jean-Pierre and Liu, Wu and Liu, Lianli and Kovalchuk, Nataliya and Gensheimer, Michael Francis and Vitzthum, Lucas Kas and Beadle, Beth M. and Chang, Daniel T. and Le, Quynh-Thu and Han, Bin and Xing, Lei}, 
journal = {TPAMI}, 
abstract = {{Defining the loss function is an important part of neural network design and critically determines the success of deep learning modeling. A significant shortcoming of the conventional loss functions is that they weight all regions in the input image volume equally, despite the fact that the system is known to be heterogeneous (i.e., some regions can achieve high prediction performance more easily than others). Here, we introduce a region-specific loss to lift the implicit assumption of homogeneous weighting for better learning. We divide the entire volume into multiple sub-regions, each with an individualized loss constructed for optimal local performance. Effectively, this scheme imposes higher weightings on the sub-regions that are more difficult to segment, and vice versa. Furthermore, the regional false positive and false negative errors are computed for each input image during a training step and the regional penalty is adjusted accordingly to enhance the overall accuracy of the prediction. Using different public and in-house medical image datasets, we demonstrate that the proposed regionally adaptive loss paradigm outperforms conventional methods in the multi-organ segmentations, without any modification to the neural network architecture or additional data preparation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Adaptive%20Region-Specific%20Loss%20for%20Improved%20Medical%20Image%20Segmentation-2023-IEEE%20Transactions%20on%20Pattern%20Analysis%20and%20Machine%20Intelligence.pdf}
}
@article{LLaMATouvronarXiv2023, 
year = {2023}, 
title = {{LLaMA: Open and Efficient Foundation Language Models}}, 
author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume}, 
journal = {arXiv}, 
eprint = {2302.13971}, 
abstract = {{We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Touvron-LLaMA-%20Open%20and%20Efficient%20Foundation%20Language%20Models-2023-arXiv.pdf}
}
@article{SELF-INSTRUCTWangACL2023, 
year = {2023}, 
title = {{Self-Instruct: Aligning Language Model with Self Generated Instructions}}, 
author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh}, 
journal = {ACL}, 
eprint = {2212.10560}, 
abstract = {{Large "instruction-tuned" language models (finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off its own generations. Our pipeline generates instruction, input, and output samples from a language model, then prunes them before using them to finetune the original model. Applying our method to vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT\_001, which is trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT\_001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Self-Instruct-%20Aligning%20Language%20Model%20with%20Self%20Generated%20Instructions-2023-ACL.pdf}
}
@article{PaLMChowdheryarXiv2022, 
year = {2022}, 
title = {{PaLM: Scaling Language Modeling with Pathways}}, 
author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah}, 
journal = {arXiv}, 
eprint = {2204.02311}, 
abstract = {{Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chowdhery-PaLM-%20Scaling%20Language%20Modeling%20with%20Pathways-2022-arXiv.pdf}
}
@article{PaLM2AnilarXiv2023, 
year = {2023}, 
title = {{PaLM 2 Technical Report}}, 
author = {Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and Chu, Eric and Clark, Jonathan H and Shafey, Laurent El and Huang, Yanping and Meier-Hellstern, Kathy and Mishra, Gaurav and Moreira, Erica and Omernick, Mark and Robinson, Kevin and Ruder, Sebastian and Tay, Yi and Xiao, Kefan and Xu, Yuanzhong and Zhang, Yujing and Abrego, Gustavo Hernandez and Ahn, Junwhan and Austin, Jacob and Barham, Paul and Botha, Jan and Bradbury, James and Brahma, Siddhartha and Brooks, Kevin and Catasta, Michele and Cheng, Yong and Cherry, Colin and Choquette-Choo, Christopher A and Chowdhery, Aakanksha and Crepy, Clément and Dave, Shachi and Dehghani, Mostafa and Dev, Sunipa and Devlin, Jacob and Díaz, Mark and Du, Nan and Dyer, Ethan and Feinberg, Vlad and Feng, Fangxiaoyu and Fienber, Vlad and Freitag, Markus and Garcia, Xavier and Gehrmann, Sebastian and Gonzalez, Lucas and Gur-Ari, Guy and Hand, Steven and Hashemi, Hadi and Hou, Le and Howland, Joshua and Hu, Andrea and Hui, Jeffrey and Hurwitz, Jeremy and Isard, Michael and Ittycheriah, Abe and Jagielski, Matthew and Jia, Wenhao and Kenealy, Kathleen and Krikun, Maxim and Kudugunta, Sneha and Lan, Chang and Lee, Katherine and Lee, Benjamin and Li, Eric and Li, Music and Li, Wei and Li, YaGuang and Li, Jian and Lim, Hyeontaek and Lin, Hanzhao and Liu, Zhongtao and Liu, Frederick and Maggioni, Marcello and Mahendru, Aroma and Maynez, Joshua and Misra, Vedant and Moussalem, Maysam and Nado, Zachary and Nham, John and Ni, Eric and Nystrom, Andrew and Parrish, Alicia and Pellat, Marie and Polacek, Martin and Polozov, Alex and Pope, Reiner and Qiao, Siyuan and Reif, Emily and Richter, Bryan and Riley, Parker and Ros, Alex Castro and Roy, Aurko and Saeta, Brennan and Samuel, Rajkumar and Shelby, Renee and Slone, Ambrose and Smilkov, Daniel and So, David R and Sohn, Daniel and Tokumine, Simon and Valter, Dasha and Vasudevan, Vijay and Vodrahalli, Kiran and Wang, Xuezhi and Wang, Pidong and Wang, Zirui and Wang, Tao and Wieting, John and Wu, Yuhuai and Xu, Kelvin and Xu, Yunhan and Xue, Linting and Yin, Pengcheng and Yu, Jiahui and Zhang, Qiao and Zheng, Steven and Zheng, Ce and Zhou, Weikang and Zhou, Denny and Petrov, Slav and Wu, Yonghui}, 
journal = {arXiv}, 
eprint = {2305.10403}, 
abstract = {{We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives. Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities. When discussing the PaLM 2 family, it is important to distinguish between pre-trained models (of various sizes), fine-tuned variants of these models, and the user-facing products that use these models. In particular, user-facing products typically include additional pre- and post-processing steps. Additionally, the underlying models may evolve over time. Therefore, one should not expect the performance of user-facing products to exactly match the results reported in this report.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Anil-PaLM%202%20Technical%20Report-2023-arXiv.pdf}
}
@article{Med-PaLMSinghalNature2023, 
year = {2023}, 
title = {{Large language models encode clinical knowledge}}, 
author = {Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S. Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and Payne, Perry and Seneviratne, Martin and Gamble, Paul and Kelly, Chris and Babiker, Abubakr and Schärli, Nathanael and Chowdhery, Aakanksha and Mansfield, Philip and Demner-Fushman, Dina and Arcas, Blaise Agüera y and Webster, Dale and Corrado, Greg S. and Matias, Yossi and Chou, Katherine and Gottweis, Juraj and Tomasev, Nenad and Liu, Yun and Rajkomar, Alvin and Barral, Joelle and Semturs, Christopher and Karthikesalingam, Alan and Natarajan, Vivek}, 
journal = {Nature}, 
abstract = {{Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model1 (PaLM, a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM2 on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA3, MedMCQA4, PubMedQA5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics6), including 67.6\% accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17\%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today’s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications. Med-PaLM, a state-of-the-art large language model for medicine, is introduced and evaluated across several medical question answering tasks, demonstrating the promise of these models in this domain.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Singhal-Large%20language%20models%20encode%20clinical%20knowledge-2023-Nature.pdf}
}
@article{Med-PaLM2SinghalarXiv2023, 
year = {2023}, 
title = {{Towards Expert-Level Medical Question Answering with Large Language Models}}, 
author = {Singhal, Karan and Tu, Tao and Gottweis, Juraj and Sayres, Rory and Wulczyn, Ellery and Hou, Le and Clark, Kevin and Pfohl, Stephen and Cole-Lewis, Heather and Neal, Darlene and Schaekermann, Mike and Wang, Amy and Amin, Mohamed and Lachgar, Sami and Mansfield, Philip and Prakash, Sushant and Green, Bradley and Dominowska, Ewa and Arcas, Blaise Aguera y and Tomasev, Nenad and Liu, Yun and Wong, Renee and Semturs, Christopher and Mahdavi, S Sara and Barral, Joelle and Webster, Dale and Corrado, Greg S and Matias, Yossi and Azizi, Shekoofeh and Karthikesalingam, Alan and Natarajan, Vivek}, 
journal = {arXiv}, 
eprint = {2305.09617}, 
abstract = {{Recent artificial intelligence (AI) systems have reached milestones in "grand challenges" ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge. Large language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a "passing" score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2\% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach. Med-PaLM 2 scored up to 86.5\% on the MedQA dataset, improving upon Med-PaLM by over 19\% and setting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU clinical topics datasets. We performed detailed human evaluations on long-form questions along multiple axes relevant to clinical applications. In pairwise comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility (p < 0.001). We also observed significant improvements compared to Med-PaLM on every evaluation axis (p < 0.001) on newly introduced datasets of 240 long-form "adversarial" questions to probe LLM limitations. While further studies are necessary to validate the efficacy of these models in real-world settings, these results highlight rapid progress towards physician-level performance in medical question answering.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Singhal-Towards%20Expert-Level%20Medical%20Question%20Answering%20with%20Large%20Language%20Models-2023-arXiv.pdf}
}
@article{ScalingKaplanarXiv2020, 
year = {2020}, 
title = {{Scaling Laws for Neural Language Models}}, 
author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario}, 
journal = {arXiv}, 
eprint = {2001.08361}, 
abstract = {{We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kaplan-Scaling%20Laws%20for%20Neural%20Language%20Models-2020-arXiv.pdf}
}
@article{ChinchillaHoffmannNeurIPS2022, 
year = {2022}, 
title = {{Training Compute-Optimal Large Language Models}}, 
author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W and Vinyals, Oriol and Sifre, Laurent}, 
journal = {NeurIPS}, 
eprint = {2203.15556}, 
abstract = {{We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\$\textbackslashtimes\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hoffmann-Training%20Compute-Optimal%20Large%20Language%20Models-2022-NeurIPS.pdf}
}
@article{GPT-4OpenAIarXiv2023, 
year = {2023}, 
title = {{GPT-4 Technical Report}}, 
author = {OpenAI}, 
journal = {arXiv}, 
eprint = {2303.08774}, 
abstract = {{We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/OpenAI-GPT-4%20Technical%20Report-2023-arXiv.pdf}
}
@article{SparksBubeckarXiv2023, 
year = {2023}, 
title = {{Sparks of Artificial General Intelligence: Early experiments with GPT-4}}, 
author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi}, 
journal = {arXiv}, 
eprint = {2303.12712}, 
abstract = {{Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bubeck-Sparks%20of%20Artificial%20General%20Intelligence-%20Early%20experiments%20with%20GPT-4-2023-arXiv.pdf}
}
@article{LLAMA2TouvronarXiv2023, 
year = {2023}, 
title = {{LLAMA 2: Open Foundation and Fine-Tuned Chat Models}}, 
author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas}, 
journal = {arXiv}, 
eprint = {2307.09288}, 
abstract = {{In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Touvron-LLAMA%202-%20Open%20Foundation%20and%20Fine-Tuned%20Chat%20Models-2023-arXiv.pdf}
}
@article{NTK-SAPWangICLR2023, 
year = {2023}, 
title = {{NTK-SAP: Improving neural network pruning by aligning training dynamics}}, 
author = {Wang, Yite and Li, Dawei and Sun, Ruoyu}, 
journal = {ICLR}, 
eprint = {2304.02840}, 
abstract = {{Pruning neural networks before training has received increasing interest due to its potential to reduce training time and memory. One popular method is to prune the connections based on a certain metric, but it is not entirely clear what metric is the best choice. Recent advances in neural tangent kernel (NTK) theory suggest that the training dynamics of large enough neural networks is closely related to the spectrum of the NTK. Motivated by this finding, we propose to prune the connections that have the least influence on the spectrum of the NTK. This method can help maintain the NTK spectrum, which may help align the training dynamics to that of its dense counterpart. However, one possible issue is that the fixed-weight-NTK corresponding to a given initial point can be very different from the NTK corresponding to later iterates during the training phase. We further propose to sample multiple realizations of random weights to estimate the NTK spectrum. Note that our approach is weight-agnostic, which is different from most existing methods that are weight-dependent. In addition, we use random inputs to compute the fixed-weight-NTK, making our method data-agnostic as well. We name our foresight pruning algorithm Neural Tangent Kernel Spectrum-Aware Pruning (NTK-SAP). Empirically, our method achieves better performance than all baselines on multiple datasets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-NTK-SAP-%20Improving%20neural%20network%20pruning%20by%20aligning%20training%20dynamics-2023-ICLR.pdf}
}
@article{SegGPTWangICCV2023, 
year = {2023}, 
title = {{SegGPT: Segmenting Everything In Context}}, 
author = {Wang, Xinlong and Zhang, Xiaosong and Cao, Yue and Wang, Wen and Shen, Chunhua and Huang, Tiejun}, 
journal = {ICCV}, 
eprint = {2304.03284}, 
abstract = {{We present SegGPT, a generalist model for segmenting everything in context. We unify various segmentation tasks into a generalist in-context learning framework that accommodates different kinds of segmentation data by transforming them into the same format of images. The training of SegGPT is formulated as an in-context coloring problem with random color mapping for each data sample. The objective is to accomplish diverse tasks according to the context, rather than relying on specific colors. After training, SegGPT can perform arbitrary segmentation tasks in images or videos via in-context inference, such as object instance, stuff, part, contour, and text. SegGPT is evaluated on a broad range of tasks, including few-shot semantic segmentation, video object segmentation, semantic segmentation, and panoptic segmentation. Our results show strong capabilities in segmenting in-domain and out-of-domain targets, either qualitatively or quantitatively.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-SegGPT-%20Segmenting%20Everything%20In%20Context-2023-ICCV.pdf}
}
@article{HPMWangCVPR2023, 
year = {2023}, 
title = {{Hard Patches Mining for Masked Image Modeling}}, 
author = {Wang, Haochen and Song, Kaiyou and Fan, Junsong and Wang, Yuxi and Xie, Jin and Zhang, Zhaoxiang}, 
journal = {CVPR}, 
eprint = {2304.05919}, 
abstract = {{Masked image modeling (MIM) has attracted much research attention due to its promising potential for learning scalable visual representations. In typical approaches, models usually focus on predicting specific contents of masked patches, and their performances are highly related to pre-defined mask strategies. Intuitively, this procedure can be considered as training a student (the model) on solving given problems (predict masked patches). However, we argue that the model should not only focus on solving given problems, but also stand in the shoes of a teacher to produce a more challenging problem by itself. To this end, we propose Hard Patches Mining (HPM), a brand-new framework for MIM pre-training. We observe that the reconstruction loss can naturally be the metric of the difficulty of the pre-training task. Therefore, we introduce an auxiliary loss predictor, predicting patch-wise losses first and deciding where to mask next. It adopts a relative relationship learning strategy to prevent overfitting to exact reconstruction loss values. Experiments under various settings demonstrate the effectiveness of HPM in constructing masked images. Furthermore, we empirically find that solely introducing the loss prediction objective leads to powerful representations, verifying the efficacy of the ability to be aware of where is hard to reconstruct.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Hard%20Patches%20Mining%20for%20Masked%20Image%20Modeling-2023-CVPR.pdf}
}
@article{SEEMZouarXiv2023, 
year = {2023}, 
title = {{Segment Everything Everywhere All at Once}}, 
author = {Zou, Xueyan and Yang, Jianwei and Zhang, Hao and Li, Feng and Li, Linjie and Wang, Jianfeng and Wang, Lijuan and Gao, Jianfeng and Lee, Yong Jae}, 
journal = {arXiv}, 
eprint = {2304.06718}, 
abstract = {{Despite the growing demand for interactive AI systems, there have been few comprehensive studies on human-AI interaction in visual understanding e.g. segmentation. Inspired by the development of prompt-based universal interfaces for LLMs, this paper presents SEEM, a promptable, interactive model for Segmenting Everything Everywhere all at once in an image. SEEM has four desiderata: i) Versatility: by introducing a versatile prompting engine for different types of prompts, including points, boxes, scribbles, masks, texts, and referred regions of another image; ii) Compositionality: by learning a joint visual-semantic space for visual and textual prompts to compose queries on the fly for inference as shown in Fig 1; iii)Interactivity: by incorporating learnable memory prompts to retain dialog history information via mask-guided cross-attention; and iv) Semantic-awareness: by using a text encoder to encode text queries and mask labels for open-vocabulary segmentation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zou-Segment%20Everything%20Everywhere%20All%20at%20Once-2023-arXiv.pdf}
}
@article{AutoFocusFormerChenCVPR2023, 
year = {2023}, 
title = {{AutoFocusFormer: Image Segmentation off the Grid}}, 
author = {Ziwen, Chen and Patnaik, Kaushik and Zhai, Shuangfei and Wan, Alvin and Ren, Zhile and Schwing, Alex and Colburn, Alex and Fuxin, Li}, 
journal = {CVPR}, 
eprint = {2304.12406}, 
abstract = {{Real world images often have highly imbalanced content density. Some areas are very uniform, e.g., large patches of blue sky, while other areas are scattered with many small objects. Yet, the commonly used successive grid downsampling strategy in convolutional deep networks treats all areas equally. Hence, small objects are represented in very few spatial locations, leading to worse results in tasks such as segmentation. Intuitively, retaining more pixels representing small objects during downsampling helps to preserve important information. To achieve this, we propose AutoFocusFormer (AFF), a local-attention transformer image recognition backbone, which performs adaptive downsampling by learning to retain the most important pixels for the task. Since adaptive downsampling generates a set of pixels irregularly distributed on the image plane, we abandon the classic grid structure. Instead, we develop a novel point-based local attention block, facilitated by a balanced clustering module and a learnable neighborhood merging module, which yields representations for our point-based versions of state-of-the-art segmentation heads. Experiments show that our AutoFocusFormer (AFF) improves significantly over baseline models of similar sizes.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ziwen-AutoFocusFormer-%20Image%20Segmentation%20off%20the%20Grid-2023-CVPR.pdf}
}
@article{BiasIofinovaCVPR2023, 
year = {2023}, 
title = {{Bias in Pruned Vision Models: In-Depth Analysis and Countermeasures}}, 
author = {Iofinova, Eugenia and Peste, Alexandra and Alistarh, Dan}, 
journal = {CVPR}, 
eprint = {2304.12622}, 
abstract = {{Pruning - that is, setting a significant subset of the parameters of a neural network to zero - is one of the most popular methods of model compression. Yet, several recent works have raised the issue that pruning may induce or exacerbate bias in the output of the compressed model. Despite existing evidence for this phenomenon, the relationship between neural network pruning and induced bias is not well-understood. In this work, we systematically investigate and characterize this phenomenon in Convolutional Neural Networks for computer vision. First, we show that it is in fact possible to obtain highly-sparse models, e.g. with less than 10\% remaining weights, which do not decrease in accuracy nor substantially increase in bias when compared to dense models. At the same time, we also find that, at higher sparsities, pruned models exhibit higher uncertainty in their outputs, as well as increased correlations, which we directly link to increased bias. We propose easy-to-use criteria which, based only on the uncompressed model, establish whether bias will increase with pruning, and identify the samples most susceptible to biased predictions post-compression.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Iofinova-Bias%20in%20Pruned%20Vision%20Models-%20In-Depth%20Analysis%20and%20Countermeasures-2023-CVPR.pdf}
}
@article{CAT-KDGuoCVPR2023, 
year = {2023}, 
title = {{Class Attention Transfer Based Knowledge Distillation}}, 
author = {Guo, Ziyao and Yan, Haonan and Li, Hui and Lin, Xiaodong}, 
journal = {CVPR}, 
eprint = {2304.12777}, 
abstract = {{Previous knowledge distillation methods have shown their impressive performance on model compression tasks, however, it is hard to explain how the knowledge they transferred helps to improve the performance of the student network. In this work, we focus on proposing a knowledge distillation method that has both high interpretability and competitive performance. We first revisit the structure of mainstream CNN models and reveal that possessing the capacity of identifying class discriminative regions of input is critical for CNN to perform classification. Furthermore, we demonstrate that this capacity can be obtained and enhanced by transferring class activation maps. Based on our findings, we propose class attention transfer based knowledge distillation (CAT-KD). Different from previous KD methods, we explore and present several properties of the knowledge transferred by our method, which not only improve the interpretability of CAT-KD but also contribute to a better understanding of CNN. While having high interpretability, CAT-KD achieves state-of-the-art performance on multiple benchmarks. Code is available at: https://github.com/GzyAftermath/CAT-KD.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Guo-Class%20Attention%20Transfer%20Based%20Knowledge%20Distillation-2023-CVPR.pdf}
}
@article{PITuningWuICML2023, 
year = {2023}, 
title = {{\$\textbackslashpi\$-Tuning: Transferring Multimodal Foundation Models with Optimal Multi-task Interpolation}}, 
author = {Wu, Chengyue and Wang, Teng and Ge, Yixiao and Lu, Zeyu and Zhou, Ruisong and Shan, Ying and Luo, Ping}, 
journal = {ICML}, 
eprint = {2304.14381}, 
abstract = {{Foundation models have achieved great advances in multi-task learning with a unified interface of unimodal and multimodal tasks. However, the potential of such multi-task learners has not been exploited during transfer learning. In this work, we present a universal parameter-efficient transfer learning method, termed Predict-Interpolate Tuning (\$\textbackslashpi\$-Tuning), for vision, language, and vision-language tasks. It aggregates the parameters of lightweight task-specific experts learned from similar tasks to aid the target downstream task. The task similarities are predicted in a unified modality-independent space, yielding a scalable graph to demonstrate task relationships. \$\textbackslashpi\$-Tuning has several appealing benefits. First, it flexibly explores both intra- and inter-modal transferability between similar tasks to improve the accuracy and robustness of transfer learning, especially in data-scarce scenarios. Second, it offers a systematical solution for transfer learning with multi-task prediction-and-then-interpolation, compatible with diverse types of parameter-efficient experts, such as prompt and adapter. Third, an extensive study of task-level mutual benefits on 14 unimodal and 6 multimodal datasets shows that \$\textbackslashpi\$-Tuning surpasses fine-tuning and other parameter-efficient transfer learning methods both in full-shot and low-shot regimes. The task graph also enables an in-depth interpretable analysis of task transferability across modalities. The code will be available at https://github.com/TencentARC/pi-Tuning.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wu-$-pi$-Tuning-%20Transferring%20Multimodal%20Foundation%20Models%20with%20Optimal%20Multi-task%20Interpolation-2023-ICML.pdf}
}
@article{ResiDualXiearXiv2023, 
year = {2023}, 
title = {{ResiDual: Transformer with Dual Residual Connections}}, 
author = {Xie, Shufang and Zhang, Huishuai and Guo, Junliang and Tan, Xu and Bian, Jiang and Awadalla, Hany Hassan and Menezes, Arul and Qin, Tao and Yan, Rui}, 
journal = {arXiv}, 
eprint = {2304.14802}, 
abstract = {{Transformer networks have become the preferred architecture for many tasks due to their state-of-the-art performance. However, the optimal way to implement residual connections in Transformer, which are essential for effective training, is still debated. Two widely used variants are the Post-Layer-Normalization (Post-LN) and Pre-Layer-Normalization (Pre-LN) Transformers, which apply layer normalization after each residual block's output or before each residual block's input, respectively. While both variants enjoy their advantages, they also suffer from severe limitations: Post-LN causes gradient vanishing issue that hinders training deep Transformers, and Pre-LN causes representation collapse issue that limits model capacity. In this paper, we propose ResiDual, a novel Transformer architecture with Pre-Post-LN (PPLN), which fuses the connections in Post-LN and Pre-LN together and inherits their advantages while avoids their limitations. We conduct both theoretical analyses and empirical experiments to verify the effectiveness of ResiDual. Theoretically, we prove that ResiDual has a lower bound on the gradient to avoid the vanishing issue due to the residual connection from Pre-LN. Moreover, ResiDual also has diverse model representations to avoid the collapse issue due to the residual connection from Post-LN. Empirically, ResiDual outperforms both Post-LN and Pre-LN on several machine translation benchmarks across different network depths and data sizes. Thanks to the good theoretical and empirical performance, ResiDual Transformer can serve as a foundation architecture for different AI models (e.g., large language models). Our code is available at https://github.com/microsoft/ResiDual.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xie-ResiDual-%20Transformer%20with%20Dual%20Residual%20Connections-2023-arXiv.pdf}
}
@article{AGoodStudentZhuICCV2023, 
year = {2023}, 
title = {{A Good Student is Cooperative and Reliable: CNN-Transformer Collaborative Learning for Semantic Segmentation}}, 
author = {Zhu, Jinjing and Luo, Yunhao and Zheng, Xu and Wang, Hao and Wang, Lin}, 
journal = {ICCV}, 
eprint = {2307.12574}, 
abstract = {{In this paper, we strive to answer the question "how to collaboratively learn convolutional neural network (CNN)-based and vision transformer (ViT)-based models by selecting and exchanging the reliable knowledge between them for semantic segmentation?" Accordingly, we propose an online knowledge distillation (KD) framework that can simultaneously learn compact yet effective CNN-based and ViT-based models with two key technical breakthroughs to take full advantage of CNNs and ViT while compensating their limitations. Firstly, we propose heterogeneous feature distillation (HFD) to improve students' consistency in low-layer feature space by mimicking heterogeneous features between CNNs and ViT. Secondly, to facilitate the two students to learn reliable knowledge from each other, we propose bidirectional selective distillation (BSD) that can dynamically transfer selective knowledge. This is achieved by 1) region-wise BSD determining the directions of knowledge transferred between the corresponding regions in the feature space and 2) pixel-wise BSD discerning which of the prediction knowledge to be transferred in the logit space. Extensive experiments on three benchmark datasets demonstrate that our proposed framework outperforms the state-of-the-art online distillation methods by a large margin, and shows its efficacy in learning collaboratively between ViT-based and CNN-based models.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhu-A%20Good%20Student%20is%20Cooperative%20and%20Reliable-%20CNN-Transformer%20Collaborative%20Learning%20for%20Semantic%20Segmentation-2023-ICCV.pdf}
}
@article{SLaKLiuICLR2023, 
year = {2023}, 
title = {{More ConvNets in the 2020s: Scaling up Kernels Beyond 51x51 using Sparsity}}, 
author = {Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Chen, Xuxi and Xiao, Qiao and Wu, Boqian and Kärkkäinen, Tommi and Pechenizkiy, Mykola and Mocanu, Decebal and Wang, Zhangyang}, 
journal = {ICLR}, 
eprint = {2207.03620}, 
abstract = {{Transformers have quickly shined in the computer vision world since the emergence of Vision Transformers (ViTs). The dominant role of convolutional neural networks (CNNs) seems to be challenged by increasingly effective transformer-based models. Very recently, a couple of advanced convolutional models strike back with large kernels motivated by the local-window attention mechanism, showing appealing performance and efficiency. While one of them, i.e. RepLKNet, impressively manages to scale the kernel size to 31x31 with improved performance, the performance starts to saturate as the kernel size continues growing, compared to the scaling trend of advanced ViTs such as Swin Transformer. In this paper, we explore the possibility of training extreme convolutions larger than 31x31 and test whether the performance gap can be eliminated by strategically enlarging convolutions. This study ends up with a recipe for applying extremely large kernels from the perspective of sparsity, which can smoothly scale up kernels to 61x61 with better performance. Built on this recipe, we propose Sparse Large Kernel Network (SLaK), a pure CNN architecture equipped with sparse factorized 51x51 kernels that can perform on par with or better than state-of-the-art hierarchical Transformers and modern ConvNet architectures like ConvNeXt and RepLKNet, on ImageNet classification as well as a wide range of downstream tasks including semantic segmentation on ADE20K, object detection on PASCAL VOC 2007, and object detection/segmentation on MS COCO.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-More%20ConvNets%20in%20the%202020s-%20Scaling%20up%20Kernels%20Beyond%2051x51%20using%20Sparsity-2023-ICLR.pdf}
}
@article{SCYaoICLR2023, 
year = {2021}, 
title = {{Adaptive Early-Learning Correction for Segmentation from Noisy Annotations}}, 
author = {Liu, Sheng and Liu, Kangning and Zhu, Weicheng and Shen, Yiqiu and Fernandez-Granda, Carlos}, 
journal = {arXiv}, 
doi = {10.48550/arxiv.2110.03740}, 
eprint = {2110.03740}, 
abstract = {{Deep learning in the presence of noisy annotations has been studied extensively in classification, but much less in segmentation tasks. In this work, we study the learning dynamics of deep segmentation networks trained on inaccurately-annotated data. We discover a phenomenon that has been previously reported in the context of classification: the networks tend to first fit the clean pixel-level labels during an "early-learning" phase, before eventually memorizing the false annotations. However, in contrast to classification, memorization in segmentation does not arise simultaneously for all semantic categories. Inspired by these findings, we propose a new method for segmentation from noisy annotations with two key elements. First, we detect the beginning of the memorization phase separately for each category during training. This allows us to adaptively correct the noisy annotations in order to exploit early learning. Second, we incorporate a regularization term that enforces consistency across scales to boost robustness against annotation noise. Our method outperforms standard approaches on a medical-imaging segmentation task where noises are synthesized to mimic human annotation errors. It also provides robustness to realistic noisy annotations present in weakly-supervised semantic segmentation, achieving state-of-the-art results on PASCAL VOC 2012. Code is available at https://github.com/Kangningthu/ADELE}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-Adaptive%20Early-Learning%20Correction%20for%20Segmentation%20from%20Noisy%20Annotations-2021-arXiv.pdf}
}
@article{ConfidenceLiICLR2023, 
year = {2023}, 
title = {{Confidence Estimation Using Unlabeled Data}}, 
author = {Li, Chen and Hu, Xiaoling and Chen, Chao}, 
journal = {ICLR}, 
eprint = {2307.10440}, 
abstract = {{Overconfidence is a common issue for deep neural networks, limiting their deployment in real-world applications. To better estimate confidence, existing methods mostly focus on fully-supervised scenarios and rely on training labels. In this paper, we propose the first confidence estimation method for a semi-supervised setting, when most training labels are unavailable. We stipulate that even with limited training labels, we can still reasonably approximate the confidence of model on unlabeled samples by inspecting the prediction consistency through the training process. We use training consistency as a surrogate function and propose a consistency ranking loss for confidence estimation. On both image classification and segmentation tasks, our method achieves state-of-the-art performances in confidence estimation. Furthermore, we show the benefit of the proposed method through a downstream active learning task. The code is available at https://github.com/TopoXLab/consistency-ranking-loss}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Confidence%20Estimation%20Using%20Unlabeled%20Data-2023-ICLR.pdf}
}
@article{3DUX-NetLeeICLR2023, 
year = {2023}, 
title = {{3D UX-Net: A Large Kernel Volumetric ConvNet Modernizing Hierarchical Transformer for Medical Image Segmentation}}, 
author = {Lee, Ho Hin and Bao, Shunxing and Huo, Yuankai and Landman, Bennett A}, 
journal = {ICLR}, 
eprint = {2209.15076}, 
abstract = {{Vision transformers (ViTs) have quickly superseded convolutional networks (ConvNets) as the current state-of-the-art (SOTA) models for medical image segmentation. Hierarchical transformers (e.g., Swin Transformers) reintroduced several ConvNet priors and further enhanced the practical viability of adapting volumetric segmentation in 3D medical datasets. The effectiveness of hybrid approaches is largely credited to the large receptive field for non-local self-attention and the large number of model parameters. In this work, we propose a lightweight volumetric ConvNet, termed 3D UX-Net, which adapts the hierarchical transformer using ConvNet modules for robust volumetric segmentation. Specifically, we revisit volumetric depth-wise convolutions with large kernel size (e.g. starting from \$7\textbackslashtimes7\textbackslashtimes7\$) to enable the larger global receptive fields, inspired by Swin Transformer. We further substitute the multi-layer perceptron (MLP) in Swin Transformer blocks with pointwise depth convolutions and enhance model performances with fewer normalization and activation layers, thus reducing the number of model parameters. 3D UX-Net competes favorably with current SOTA transformers (e.g. SwinUNETR) using three challenging public datasets on volumetric brain and abdominal imaging: 1) MICCAI Challenge 2021 FLARE, 2) MICCAI Challenge 2021 FeTA, and 3) MICCAI Challenge 2022 AMOS. 3D UX-Net consistently outperforms SwinUNETR with improvement from 0.929 to 0.938 Dice (FLARE2021) and 0.867 to 0.874 Dice (Feta2021). We further evaluate the transfer learning capability of 3D UX-Net with AMOS2022 and demonstrates another improvement of \$2.27\textbackslash\%\$ Dice (from 0.880 to 0.900). The source code with our proposed model are available at https://github.com/MASILab/3DUX-Net.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lee-3D%20UX-Net-%20A%20Large%20Kernel%20Volumetric%20ConvNet%20Modernizing%20Hierarchical%20Transformer%20for%20Medical%20Image%20Segmentation-2023-ICLR.pdf}
}
@article{COMUSZadaianchukICLR2023, 
year = {2023}, 
title = {{Unsupervised Semantic Segmentation with Self-supervised Object-centric Representations}}, 
author = {Zadaianchuk, Andrii and Kleindessner, Matthaeus and Zhu, Yi and Locatello, Francesco and Brox, Thomas}, 
journal = {ICLR}, 
eprint = {2207.05027}, 
abstract = {{In this paper, we show that recent advances in self-supervised feature learning enable unsupervised object discovery and semantic segmentation with a performance that matches the state of the field on supervised semantic segmentation 10 years ago. We propose a methodology based on unsupervised saliency masks and self-supervised feature clustering to kickstart object discovery followed by training a semantic segmentation network on pseudo-labels to bootstrap the system on images with multiple objects. We present results on PASCAL VOC that go far beyond the current state of the art (50.0 mIoU), and we report for the first time results on MS COCO for the whole set of 81 classes: our method discovers 34 categories with more than \$20\textbackslash\%\$ IoU, while obtaining an average IoU of 19.6 for all 81 categories.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zadaianchuk-Unsupervised%20Semantic%20Segmentation%20with%20Self-supervised%20Object-centric%20Representations-2023-ICLR.pdf}
}
@article{DPKQiuICLR2023, 
year = {2023}, 
title = {{Better Teacher Better Student: Dynamic Prior Knowledge for Knowledge Distillation}}, 
author = {Qiu, Zengyu and Ma, Xinzhu and Yang, Kunlin and Liu, Chunya and Hou, Jun and Yi, Shuai and Ouyang, Wanli}, 
journal = {ICLR}, 
eprint = {2206.06067}, 
abstract = {{Knowledge distillation (KD) has shown very promising capabilities in transferring learning representations from large models (teachers) to small models (students). However, as the capacity gap between students and teachers becomes larger, existing KD methods fail to achieve better results. Our work shows that the `prior knowledge' is vital to KD, especially when applying large teachers. Particularly, we propose the dynamic prior knowledge (DPK), which integrates part of teacher's features as the prior knowledge before the feature distillation. This means that our method also takes the teacher's feature as `input', not just `target'. Besides, we dynamically adjust the ratio of the prior knowledge during the training phase according to the feature gap, thus guiding the student in an appropriate difficulty. To evaluate the proposed method, we conduct extensive experiments on two image classification benchmarks (i.e. CIFAR100 and ImageNet) and an object detection benchmark (i.e. MS COCO. The results demonstrate the superiority of our method in performance under varying settings. Besides, our DPK makes the performance of the student model positively correlated with that of the teacher model, which means that we can further boost the accuracy of students by applying larger teachers. More importantly, DPK provides a fast solution in teacher model selection for any given model.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Qiu-Better%20Teacher%20Better%20Student-%20Dynamic%20Prior%20Knowledge%20for%20Knowledge%20Distillation-2023-ICLR.pdf}
}
@article{ThatZepfICLR2023, 
year = {2023}, 
title = {{That Label's Got Style: Handling Label Style Bias for Uncertain Image Segmentation}}, 
author = {Zepf, Kilian and Petersen, Eike and Frellsen, Jes and Feragen, Aasa}, 
journal = {ICLR}, 
eprint = {2303.15850}, 
abstract = {{Segmentation uncertainty models predict a distribution over plausible segmentations for a given input, which they learn from the annotator variation in the training set. However, in practice these annotations can differ systematically in the way they are generated, for example through the use of different labeling tools. This results in datasets that contain both data variability and differing label styles. In this paper, we demonstrate that applying state-of-the-art segmentation uncertainty models on such datasets can lead to model bias caused by the different label styles. We present an updated modelling objective conditioning on labeling style for aleatoric uncertainty estimation, and modify two state-of-the-art-architectures for segmentation uncertainty accordingly. We show with extensive experiments that this method reduces label style bias, while improving segmentation performance, increasing the applicability of segmentation uncertainty models in the wild. We curate two datasets, with annotations in different label styles, which we will make publicly available along with our code upon publication.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zepf-That%20Label's%20Got%20Style-%20Handling%20Label%20Style%20Bias%20for%20Uncertain%20Image%20Segmentation-2023-ICLR.pdf}
}
@article{DiffusionModelsSohl-DicksteinICML2015, 
year = {2015}, 
title = {{Deep Unsupervised Learning using Nonequilibrium Thermodynamics}}, 
author = {Sohl-Dickstein, Jascha and Weiss, Eric A and Maheswaranathan, Niru and Ganguli, Surya}, 
journal = {ICML}, 
eprint = {1503.03585}, 
abstract = {{A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Sohl-Dickstein-Deep%20Unsupervised%20Learning%20using%20Nonequilibrium%20Thermodynamics-2015-ICML.pdf}
}
@article{VTMKimICLR2023, 
year = {2023}, 
title = {{Universal Few-shot Learning of Dense Prediction Tasks with Visual Token Matching}}, 
author = {Kim, Donggyun and Kim, Jinwoo and Cho, Seongwoong and Luo, Chong and Hong, Seunghoon}, 
journal = {ICLR}, 
eprint = {2303.14969}, 
abstract = {{Dense prediction tasks are a fundamental class of problems in computer vision. As supervised methods suffer from high pixel-wise labeling cost, a few-shot learning solution that can learn any dense task from a few labeled images is desired. Yet, current few-shot learning methods target a restricted set of tasks such as semantic segmentation, presumably due to challenges in designing a general and unified model that is able to flexibly and efficiently adapt to arbitrary tasks of unseen semantics. We propose Visual Token Matching (VTM), a universal few-shot learner for arbitrary dense prediction tasks. It employs non-parametric matching on patch-level embedded tokens of images and labels that encapsulates all tasks. Also, VTM flexibly adapts to any task with a tiny amount of task-specific parameters that modulate the matching algorithm. We implement VTM as a powerful hierarchical encoder-decoder architecture involving ViT backbones where token matching is performed at multiple feature hierarchies. We experiment VTM on a challenging variant of Taskonomy dataset and observe that it robustly few-shot learns various unseen dense prediction tasks. Surprisingly, it is competitive with fully supervised baselines using only 10 labeled examples of novel tasks (0.004\% of full supervision) and sometimes outperforms using 0.1\% of full supervision. Codes are available at https://github.com/GitGyun/visual\_token\_matching.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kim-Universal%20Few-shot%20Learning%20of%20Dense%20Prediction%20Tasks%20with%20Visual%20Token%20Matching-2023-ICLR.pdf}
}
@article{SuperpixelFCNYangCVPR2020, 
year = {2020}, 
title = {{Superpixel Segmentation With Fully Convolutional Networks}}, 
author = {Yang, Fengting and Sun, Qian and Jin, Hailin and Zhou, Zihan}, 
journal = {CVPR}, 
abstract = {{In computer vision, superpixels have been widely used as an effective way to reduce the number of image primitives for subsequent processing. But only a few attempts have been made to incorporate them into deep neural networks. One main reason is that the standard convolution operation is defined on regular grids and becomes inefficient when applied to superpixels. Inspired by an initialization strategy commonly adopted by traditional superpixel algorithms, we present a novel method that employs a simple fully convolutional network to predict superpixels on a regular image grid. Experimental results on benchmark datasets show that our method achieves state-of-the-art superpixel segmentation performance while running at about 50fps. Based on the predicted superpixels, we further develop a downsam-pling/upsampling scheme for deep networks with the goal of generating high-resolution outputs for dense prediction tasks. Specifically, we modify a popular network architecture for stereo matching to simultaneously predict super-pixels and disparities. We show that improved disparity estimation accuracy can be obtained on public datasets.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-Superpixel%20Segmentation%20With%20Fully%20Convolutional%20Networks-2020-CVPR.pdf}
}
@article{CLIPPOTschannenCVPR2023, 
year = {2023}, 
title = {{CLIPPO: Image-and-Language Understanding from Pixels Only}}, 
author = {Tschannen, Michael and Mustafa, Basil and Houlsby, Neil}, 
journal = {CVPR}, 
eprint = {2212.08045}, 
abstract = {{Multimodal models are becoming increasingly effective, in part due to unified components, such as the Transformer architecture. However, multimodal models still often consist of many task- and modality-specific pieces and training procedures. For example, CLIP (Radford et al., 2021) trains independent text and image towers via a contrastive loss. We explore an additional unification: the use of a pure pixel-based model to perform image, text, and multimodal tasks. Our model is trained with contrastive loss alone, so we call it CLIP-Pixels Only (CLIPPO). CLIPPO uses a single encoder that processes both regular images and text rendered as images. CLIPPO performs image-based tasks such as retrieval and zero-shot image classification almost as well as CLIP-style models, with half the number of parameters and no text-specific tower or embedding. When trained jointly via image-text contrastive learning and next-sentence contrastive learning, CLIPPO can perform well on natural language understanding tasks, without any word-level loss (language modelling or masked language modelling), outperforming pixel-based prior work. Surprisingly, CLIPPO can obtain good accuracy in visual question answering, simply by rendering the question and image together. Finally, we exploit the fact that CLIPPO does not require a tokenizer to show that it can achieve strong performance on multilingual multimodal retrieval without modifications.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tschannen-CLIPPO-%20Image-and-Language%20Understanding%20from%20Pixels%20Only-2023-CVPR.pdf}
}
@article{TowardsZhuICLR2023, 
year = {2023}, 
title = {{Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning}}, 
author = {Allen-Zhu, Zeyuan and Li, Yuanzhi}, 
journal = {ICLR}, 
eprint = {2012.09816}, 
abstract = {{We formally study how ensemble of deep learning models can improve test accuracy, and how the superior performance of ensemble can be distilled into a single model using knowledge distillation. We consider the challenging case where the ensemble is simply an average of the outputs of a few independently trained neural networks with the SAME architecture, trained using the SAME algorithm on the SAME data set, and they only differ by the random seeds used in the initialization. We empirically show that ensemble/knowledge distillation in deep learning works very differently from traditional learning theory, especially differently from ensemble of random feature mappings or the neural-tangent-kernel feature mappings, and is potentially out of the scope of existing theorems. Thus, to properly understand ensemble and knowledge distillation in deep learning, we develop a theory showing that when data has a structure we refer to as "multi-view", then ensemble of independently trained neural networks can provably improve test accuracy, and such superior test accuracy can also be provably distilled into a single model by training a single model to match the output of the ensemble instead of the true label. Our result sheds light on how ensemble works in deep learning in a way that is completely different from traditional theorems, and how the "dark knowledge" is hidden in the outputs of the ensemble -- that can be used in knowledge distillation -- comparing to the true data labels. In the end, we prove that self-distillation can also be viewed as implicitly combining ensemble and knowledge distillation to improve test accuracy.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Allen-Zhu-Towards%20Understanding%20Ensemble,%20Knowledge%20Distillation%20and%20Self-Distillation%20in%20Deep%20Learning-2023-ICLR.pdf}
}
@article{DSANingECCV2020, 
year = {2020}, 
title = {{DSA: More Efficient Budgeted Pruning via Differentiable Sparsity Allocation}}, 
author = {Ning, Xuefei and Zhao, Tianchen and Li, Wenshuo and Lei, Peng and Wang, Yu and Yang, Huazhong}, 
journal = {ECCV}, 
eprint = {2004.02164}, 
abstract = {{Budgeted pruning is the problem of pruning under resource constraints. In budgeted pruning, how to distribute the resources across layers (i.e., sparsity allocation) is the key problem. Traditional methods solve it by discretely searching for the layer-wise pruning ratios, which lacks efficiency. In this paper, we propose Differentiable Sparsity Allocation (DSA), an efficient end-to-end budgeted pruning flow. Utilizing a novel differentiable pruning process, DSA finds the layer-wise pruning ratios with gradient-based optimization. It allocates sparsity in continuous space, which is more efficient than methods based on discrete evaluation and search. Furthermore, DSA could work in a pruning-from-scratch manner, whereas traditional budgeted pruning methods are applied to pre-trained models. Experimental results on CIFAR-10 and ImageNet show that DSA could achieve superior performance than current iterative budgeted pruning methods, and shorten the time cost of the overall pruning process by at least 1.5x in the meantime.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ning-DSA-%20More%20Efficient%20Budgeted%20Pruning%20via%20Differentiable%20Sparsity%20Allocation-2020-ECCV.pdf}
}
@article{AC-MTXuMIA2023, 
year = {2023}, 
title = {{Ambiguity-selective consistency regularization for mean-teacher semi-supervised medical image segmentation}}, 
author = {Xu, Zhe and Wang, Yixin and Lu, Donghuan and Luo, Xiangde and Yan, Jiangpeng and Zheng, Yefeng and Tong, Raymond Kai-yu}, 
journal = {MIA}, 
abstract = {{Semi-supervised learning has greatly advanced medical image segmentation since it effectively alleviates the need of acquiring abundant annotations from experts, wherein the mean-teacher model, known as a milestone of perturbed consistency learning, commonly serves as a standard and simple baseline. Inherently, learning from consistency can be regarded as learning from stability under perturbations. Recent improvement leans toward more complex consistency learning frameworks, yet, little attention is paid to the consistency target selection. Considering that the ambiguous regions from unlabeled data contain more informative complementary clues, in this paper, we improve the mean-teacher model to a novel ambiguity-consensus mean-teacher (AC-MT) model. Particularly, we comprehensively introduce and benchmark a family of plug-and-play strategies for ambiguous target selection from the perspectives of entropy, model uncertainty and label noise self-identification, respectively. Then, the estimated ambiguity map is incorporated into the consistency loss to encourage consensus between the two models’ predictions in these informative regions. In essence, our AC-MT aims to find out the most worthwhile voxel-wise targets from the unlabeled data, and the model especially learns from the perturbed stability of these informative regions. The proposed methods are extensively evaluated on left atrium segmentation and brain tumor segmentation. Encouragingly, our strategies bring substantial improvement over recent state-of-the-art methods. The ablation study further demonstrates our hypothesis and shows impressive results under various extreme annotation conditions.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xu-Ambiguity-selective%20consistency%20regularization%20for%20mean-teacher%20semi-supervised%20medical%20image%20segmentation-2023-Medical%20Image%20Analysis.pdf}
}
@article{RobustDistillationWangUAI2023, 
year = {2023}, 
title = {{Robust Distillation for Worst-class Performance: On the Interplay Between Teacher and Student Objectives}}, 
author = {Wang, Serena and Narasimhan, Harikrishna and Zhou, Yichen and Hooker, Sara and Lukasik, Michal and Menon, Aditya Krishna}, 
journal = {UAI}, 
eprint = {2206.06479}, 
abstract = {{Knowledge distillation has proven to be an effective technique in improving the performance a student model using predictions from a teacher model. However, recent work has shown that gains in average efficiency are not uniform across subgroups in the data, and in particular can often come at the cost of accuracy on rare subgroups and classes. To preserve strong performance across classes that may follow a long-tailed distribution, we develop distillation techniques that are tailored to improve the student's worst-class performance. Specifically, we introduce robust optimization objectives in different combinations for the teacher and student, and further allow for training with any tradeoff between the overall accuracy and the robust worst-class objective. We show empirically that our robust distillation techniques not only achieve better worst-class performance, but also lead to Pareto improvement in the tradeoff between overall performance and worst-class performance compared to other baseline methods. Theoretically, we provide insights into what makes a good teacher when the goal is to train a robust student.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Robust%20Distillation%20for%20Worst-class%20Performance-%20On%20the%20Interplay%20Between%20Teacher%20and%20Student%20Objectives-2023-UAI.pdf}
}
@article{ConvMixerTrockmanTMLR2023, 
year = {2023}, 
title = {{Patches Are All You Need?}}, 
author = {Trockman, Asher and Kolter, J Zico}, 
journal = {TMLR}, 
eprint = {2201.09792}, 
abstract = {{Although convolutional networks have been the dominant architecture for vision tasks for many years, recent experiments have shown that Transformer-based models, most notably the Vision Transformer (ViT), may exceed their performance in some settings. However, due to the quadratic runtime of the self-attention layers in Transformers, ViTs require the use of patch embeddings, which group together small regions of the image into single input features, in order to be applied to larger image sizes. This raises a question: Is the performance of ViTs due to the inherently-more-powerful Transformer architecture, or is it at least partly due to using patches as the input representation? In this paper, we present some evidence for the latter: specifically, we propose the ConvMixer, an extremely simple model that is similar in spirit to the ViT and the even-more-basic MLP-Mixer in that it operates directly on patches as input, separates the mixing of spatial and channel dimensions, and maintains equal size and resolution throughout the network. In contrast, however, the ConvMixer uses only standard convolutions to achieve the mixing steps. Despite its simplicity, we show that the ConvMixer outperforms the ViT, MLP-Mixer, and some of their variants for similar parameter counts and data set sizes, in addition to outperforming classical vision models such as the ResNet. Our code is available at https://github.com/locuslab/convmixer.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Trockman-Patches%20Are%20All%20You%20Need--2023-TMLR.pdf}
}
@article{SwishRamachandranarXiv2017, 
year = {2017}, 
title = {{Searching for Activation Functions}}, 
author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V}, 
journal = {arXiv}, 
eprint = {1710.05941}, 
abstract = {{The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, \$f(x) = x \textbackslashcdot \textbackslashtext\{sigmoid\}(\textbackslashbeta x)\$, which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9\textbackslash\% for Mobile NASNet-A and 0.6\textbackslash\% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ramachandran-Searching%20for%20Activation%20Functions-2017-arXiv.pdf}
}
@article{GLUVariantsShazeerarXiv2020, 
year = {2020}, 
title = {{GLU Variants Improve Transformer}}, 
author = {Shazeer, Noam}, 
journal = {arXiv}, 
eprint = {2002.05202}, 
abstract = {{Gated Linear Units (arXiv:1612.08083) consist of the component-wise product of two linear projections, one of which is first passed through a sigmoid function. Variations on GLU are possible, using different nonlinear (or even linear) functions in place of sigmoid. We test these variants in the feed-forward sublayers of the Transformer (arXiv:1706.03762) sequence-to-sequence model, and find that some of them yield quality improvements over the typically-used ReLU or GELU activations.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Shazeer-GLU%20Variants%20Improve%20Transformer-2020-arXiv.pdf}
}
@article{Leaf-DiceLossFidonMICCAI2021, 
year = {2021}, 
title = {{Label-set Loss Functions for Partial Supervision: Application to Fetal Brain 3D MRI Parcellation}}, 
author = {Fidon, Lucas and Aertsen, Michael and Emam, Doaa and Mufti, Nada and Guffens, Frédéric and Deprest, Thomas and Demaerel, Philippe and David, Anna L and Melbourne, Andrew and Ourselin, Sébastien and Deprest, Jan and Vercauteren, Tom}, 
journal = {MICCAI}, 
eprint = {2107.03846}, 
abstract = {{Deep neural networks have increased the accuracy of automatic segmentation, however, their accuracy depends on the availability of a large number of fully segmented images. Methods to train deep neural networks using images for which some, but not all, regions of interest are segmented are necessary to make better use of partially annotated datasets. In this paper, we propose the first axiomatic definition of label-set loss functions that are the loss functions that can handle partially segmented images. We prove that there is one and only one method to convert a classical loss function for fully segmented images into a proper label-set loss function. Our theory also allows us to define the leaf-Dice loss, a label-set generalization of the Dice loss particularly suited for partial supervision with only missing labels. Using the leaf-Dice loss, we set a new state of the art in partially supervised learning for fetal brain 3D MRI segmentation. We achieve a deep neural network able to segment white matter, ventricles, cerebellum, extra-ventricular CSF, cortical gray matter, deep gray matter, brainstem, and corpus callosum based on fetal brain 3D MRI of anatomically normal fetuses or with open spina bifida. Our implementation of the proposed label-set loss functions is available at https://github.com/LucasFidon/label-set-loss-functions}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Fidon-Label-set%20Loss%20Functions%20for%20Partial%20Supervision-%20Application%20to%20Fetal%20Brain%203D%20MRI%20Parcellation-2021-MICCAI.pdf}
}
@article{RepOptimizerDingICLR2023, 
year = {2023}, 
title = {{Re-parameterizing Your Optimizers rather than Architectures}}, 
author = {Ding, Xiaohan and Chen, Honghao and Zhang, Xiangyu and Huang, Kaiqi and Han, Jungong and Ding, Guiguang}, 
journal = {ICLR}, 
eprint = {2205.15242}, 
abstract = {{The well-designed structures in neural networks reflect the prior knowledge incorporated into the models. However, though different models have various priors, we are used to training them with model-agnostic optimizers such as SGD. In this paper, we propose to incorporate model-specific prior knowledge into optimizers by modifying the gradients according to a set of model-specific hyper-parameters. Such a methodology is referred to as Gradient Re-parameterization, and the optimizers are named RepOptimizers. For the extreme simplicity of model structure, we focus on a VGG-style plain model and showcase that such a simple model trained with a RepOptimizer, which is referred to as RepOpt-VGG, performs on par with or better than the recent well-designed models. From a practical perspective, RepOpt-VGG is a favorable base model because of its simple structure, high inference speed and training efficiency. Compared to Structural Re-parameterization, which adds priors into models via constructing extra training-time structures, RepOptimizers require no extra forward/backward computations and solve the problem of quantization. We hope to spark further research beyond the realms of model structure design. Code and models \textbackslashurl\{https://github.com/DingXiaoH/RepOptimizers\}.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ding-Re-parameterizing%20Your%20Optimizers%20rather%20than%20Architectures-2023-ICLR.pdf}
}
@article{HSSNLiCVPR2022, 
year = {2022}, 
title = {{Deep Hierarchical Semantic Segmentation}}, 
author = {Li, Liulei and Zhou, Tianfei and Wang, Wenguan and Li, Jianwu and Yang, Yi}, 
journal = {CVPR}, 
eprint = {2203.14335}, 
abstract = {{Humans are able to recognize structured relations in observation, allowing us to decompose complex scenes into simpler parts and abstract the visual world in multiple levels. However, such hierarchical reasoning ability of human perception remains largely unexplored in current literature of semantic segmentation. Existing work is often aware of flatten labels and predicts target classes exclusively for each pixel. In this paper, we instead address hierarchical semantic segmentation (HSS), which aims at structured, pixel-wise description of visual observation in terms of a class hierarchy. We devise HSSN, a general HSS framework that tackles two critical issues in this task: i) how to efficiently adapt existing hierarchy-agnostic segmentation networks to the HSS setting, and ii) how to leverage the hierarchy information to regularize HSS network learning. To address i), HSSN directly casts HSS as a pixel-wise multi-label classification task, only bringing minimal architecture change to current segmentation models. To solve ii), HSSN first explores inherent properties of the hierarchy as a training objective, which enforces segmentation predictions to obey the hierarchy structure. Further, with hierarchy-induced margin constraints, HSSN reshapes the pixel embedding space, so as to generate well-structured pixel representations and improve segmentation eventually. We conduct experiments on four semantic segmentation datasets (i.e., Mapillary Vistas 2.0, Cityscapes, LIP, and PASCAL-Person-Part), with different class hierarchies, segmentation network architectures and backbones, showing the generalization and superiority of HSSN.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Deep%20Hierarchical%20Semantic%20Segmentation-2022-CVPR.pdf}
}
@article{TaskResYuCVPR2023, 
year = {2023}, 
title = {{Task Residual for Tuning Vision-Language Models}}, 
author = {Yu, Tao and Lu, Zhihe and Jin, Xin and Chen, Zhibo and Wang, Xinchao}, 
journal = {CVPR}, 
eprint = {2211.10277}, 
abstract = {{Large-scale vision-language models (VLMs) pre-trained on billion-level data have learned general visual representations and broad visual concepts. In principle, the well-learned knowledge structure of the VLMs should be inherited appropriately when being transferred to downstream tasks with limited data. However, most existing efficient transfer learning (ETL) approaches for VLMs either damage or are excessively biased towards the prior knowledge, e.g., prompt tuning (PT) discards the pre-trained text-based classifier and builds a new one while adapter-style tuning (AT) fully relies on the pre-trained features. To address this, we propose a new efficient tuning approach for VLMs named Task Residual Tuning (TaskRes), which performs directly on the text-based classifier and explicitly decouples the prior knowledge of the pre-trained models and new knowledge regarding a target task. Specifically, TaskRes keeps the original classifier weights from the VLMs frozen and obtains a new classifier for the target task by tuning a set of prior-independent parameters as a residual to the original one, which enables reliable prior knowledge preservation and flexible task-specific knowledge exploration. The proposed TaskRes is simple yet effective, which significantly outperforms previous ETL methods (e.g., PT and AT) on 11 benchmark datasets while requiring minimal effort for the implementation. Our code will be available at https://github.com/geekyutao/TaskRes.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yu-Task%20Residual%20for%20Tuning%20Vision-Language%20Models-2023-CVPR.pdf}
}
@article{LiVTXuCVPR2023, 
year = {2023}, 
title = {{Learning Imbalanced Data with Vision Transformers}}, 
author = {Xu, Zhengzhuo and Liu, Ruikang and Yang, Shuo and Chai, Zenghao and Yuan, Chun}, 
journal = {CVPR}, 
eprint = {2212.02015}, 
abstract = {{The real-world data tends to be heavily imbalanced and severely skew the data-driven deep neural networks, which makes Long-Tailed Recognition (LTR) a massive challenging task. Existing LTR methods seldom train Vision Transformers (ViTs) with Long-Tailed (LT) data, while the off-the-shelf pretrain weight of ViTs always leads to unfair comparisons. In this paper, we systematically investigate the ViTs' performance in LTR and propose LiVT to train ViTs from scratch only with LT data. With the observation that ViTs suffer more severe LTR problems, we conduct Masked Generative Pretraining (MGP) to learn generalized features. With ample and solid evidence, we show that MGP is more robust than supervised manners. In addition, Binary Cross Entropy (BCE) loss, which shows conspicuous performance with ViTs, encounters predicaments in LTR. We further propose the balanced BCE to ameliorate it with strong theoretical groundings. Specially, we derive the unbiased extension of Sigmoid and compensate extra logit margins to deploy it. Our Bal-BCE contributes to the quick convergence of ViTs in just a few epochs. Extensive experiments demonstrate that with MGP and Bal-BCE, LiVT successfully trains ViTs well without any additional data and outperforms comparable state-of-the-art methods significantly, e.g., our ViT-B achieves 81.0\% Top-1 accuracy in iNaturalist 2018 without bells and whistles. Code is available at https://github.com/XuZhengzhuo/LiVT.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xu-Learning%20Imbalanced%20Data%20with%20Vision%20Transformers-2023-CVPR.pdf}
}
@article{GAMZhangCVPR2023, 
year = {2023}, 
title = {{Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization}}, 
author = {Zhang, Xingxuan and Xu, Renzhe and Yu, Han and Zou, Hao and Cui, Peng}, 
journal = {CVPR}, 
eprint = {2303.03108}, 
abstract = {{Recently, flat minima are proven to be effective for improving generalization and sharpness-aware minimization (SAM) achieves state-of-the-art performance. Yet the current definition of flatness discussed in SAM and its follow-ups are limited to the zeroth-order flatness (i.e., the worst-case loss within a perturbation radius). We show that the zeroth-order flatness can be insufficient to discriminate minima with low generalization error from those with high generalization error both when there is a single minimum or multiple minima within the given perturbation radius. Thus we present first-order flatness, a stronger measure of flatness focusing on the maximal gradient norm within a perturbation radius which bounds both the maximal eigenvalue of Hessian at local minima and the regularization function of SAM. We also present a novel training procedure named Gradient norm Aware Minimization (GAM) to seek minima with uniformly small curvature across all directions. Experimental results show that GAM improves the generalization of models trained with current optimizers such as SGD and AdamW on various datasets and networks. Furthermore, we show that GAM can help SAM find flatter minima and achieve better generalization.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Gradient%20Norm%20Aware%20Minimization%20Seeks%20First-Order%20Flatness%20and%20Improves%20Generalization-2023-CVPR.pdf}
}
@article{Meta-mapperNajdenkoskaICLR2023, 
year = {2023}, 
title = {{Meta Learning to Bridge Vision and Language Models for Multimodal Few-Shot Learning}}, 
author = {Najdenkoska, Ivona and Zhen, Xiantong and Worring, Marcel}, 
journal = {ICLR}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Najdenkoska-Meta%20Learning%20to%20Bridge%20Vision%20and%20Language%20Models%20for%20Multimodal%20Few-Shot%20Learning-2023-ICLR.pdf}
}
@article{CoTWeiNeurIPS2022, 
year = {2022}, 
title = {{Chain of Thought Prompting Elicits Reasoning in Large Language Models}}, 
author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny}, 
journal = {NeurIPS}, 
eprint = {2201.11903}, 
abstract = {{We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wei-Chain%20of%20Thought%20Prompting%20Elicits%20Reasoning%20in%20Large%20Language%20Models-2022-NeurIPS.pdf}
}
@article{ExploringWangIJCV2023, 
year = {2023}, 
title = {{Exploring Vision-Language Models for Imbalanced Learning}}, 
author = {Wang, Yidong and Yu, Zhuohao and Wang, Jindong and Heng, Qiang and Chen, Hao and Ye, Wei and Xie, Rui and Xie, Xing and Zhang, Shikun}, 
journal = {IJCV}, 
eprint = {2304.01457}, 
abstract = {{Vision-Language models (VLMs) that use contrastive language-image pre-training have shown promising zero-shot classification performance. However, their performance on imbalanced dataset is relatively poor, where the distribution of classes in the training dataset is skewed, leading to poor performance in predicting minority classes. For instance, CLIP achieved only 5\% accuracy on the iNaturalist18 dataset. We propose to add a lightweight decoder to VLMs to avoid OOM (out of memory) problem caused by large number of classes and capture nuanced features for tail classes. Then, we explore improvements of VLMs using prompt tuning, fine-tuning, and incorporating imbalanced algorithms such as Focal Loss, Balanced SoftMax and Distribution Alignment. Experiments demonstrate that the performance of VLMs can be further boosted when used with decoder and imbalanced methods. Specifically, our improved VLMs significantly outperforms zero-shot classification by an average accuracy of 6.58\%, 69.82\%, and 6.17\%, on ImageNet-LT, iNaturalist18, and Places-LT, respectively. We further analyze the influence of pre-training data size, backbones, and training cost. Our study highlights the significance of imbalanced learning algorithms in face of VLMs pre-trained by huge data. We release our code at https://github.com/Imbalance-VLM/Imbalance-VLM.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Exploring%20Vision-Language%20Models%20for%20Imbalanced%20Learning-2023-IJCV.pdf}
}
@article{Zero-shotCoTKojimaNeurIPS2022, 
year = {2022}, 
title = {{Large Language Models are Zero-Shot Reasoners}}, 
author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke}, 
journal = {NeurIPS}, 
eprint = {2205.11916}, 
abstract = {{Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kojima-Large%20Language%20Models%20are%20Zero-Shot%20Reasoners-2022-NeurIPS.pdf}
}
@article{SoTNingarXiv2023, 
year = {2023}, 
title = {{Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding}}, 
author = {Ning, Xuefei and Lin, Zinan and Zhou, Zixuan and Yang, Huazhong and Wang, Yu}, 
journal = {arXiv}, 
eprint = {2307.15337}, 
abstract = {{This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of humans, we propose "Skeleton-of-Thought" (SoT), which guides LLMs to first generate the skeleton of the answer, and then conducts parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. Not only does SoT provide considerable speed-up (up to 2.39x across 11 different LLMs), but it can also potentially improve the answer quality on several question categories in terms of diversity and relevance. SoT is an initial attempt at data-centric optimization for efficiency, and reveal the potential of pushing LLMs to think more like a human for answer quality.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ning-Skeleton-of-Thought-%20Large%20Language%20Models%20Can%20Do%20Parallel%20Decoding-2023-arXiv.pdf}
}
@article{ToTYaoarXiv2023, 
year = {2023}, 
title = {{Tree of Thoughts: Deliberate Problem Solving with Large Language Models}}, 
author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L and Cao, Yuan and Narasimhan, Karthik}, 
journal = {arXiv}, 
eprint = {2305.10601}, 
abstract = {{Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/ysymyth/tree-of-thought-llm.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yao-Tree%20of%20Thoughts-%20Deliberate%20Problem%20Solving%20with%20Large%20Language%20Models-2023-arXiv.pdf}
}
@article{LoRAHuICLR2022, 
year = {2022}, 
title = {{LoRA: Low-Rank Adaptation of Large Language Models}}, 
author = {Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu}, 
journal = {ICLR}, 
eprint = {2106.09685}, 
abstract = {{An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hu-LoRA-%20Low-Rank%20Adaptation%20of%20Large%20Language%20Models-2022-ICLR.pdf}
}
@article{FLANWeiICLR2022, 
year = {2022}, 
title = {{Finetuned Language Models Are Zero-Shot Learners}}, 
author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V}, 
journal = {ICLR}, 
eprint = {2109.01652}, 
abstract = {{This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wei-Finetuned%20Language%20Models%20Are%20Zero-Shot%20Learners-2022-ICLR.pdf}
}
@article{LIMAZhouarXiv2023, 
year = {2023}, 
title = {{LIMA: Less Is More for Alignment}}, 
author = {Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and Zhang, Susan and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke and Levy, Omer}, 
journal = {arXiv}, 
eprint = {2305.11206}, 
abstract = {{Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43\% of cases; this statistic is as high as 58\% when compared to Bard and 65\% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhou-LIMA-%20Less%20Is%20More%20for%20Alignment-2023-arXiv.pdf}
}
@article{ScalingChungarXiv2022, 
year = {2022}, 
title = {{Scaling Instruction-Finetuned Language Models}}, 
author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V and Wei, Jason}, 
journal = {arXiv}, 
eprint = {2210.11416}, 
abstract = {{Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4\% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2\% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chung-Scaling%20Instruction-Finetuned%20Language%20Models-2022-arXiv.pdf}
}
@article{ScalingBulatovarXiv2023, 
year = {2023}, 
title = {{Scaling Transformer to 1M tokens and beyond with RMT}}, 
author = {Bulatov, Aydar and Kuratov, Yuri and Burtsev, Mikhail S}, 
journal = {arXiv}, 
eprint = {2304.11062}, 
abstract = {{This technical report presents the application of a recurrent memory to extend the context length of BERT, one of the most effective Transformer-based models in natural language processing. By leveraging the Recurrent Memory Transformer architecture, we have successfully increased the model's effective context length to an unprecedented two million tokens, while maintaining high memory retrieval accuracy. Our method allows for the storage and processing of both local and global information and enables information flow between segments of the input sequence through the use of recurrence. Our experiments demonstrate the effectiveness of our approach, which holds significant potential to enhance long-term dependency handling in natural language understanding and generation tasks as well as enable large-scale context processing for memory-intensive applications.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bulatov-Scaling%20Transformer%20to%201M%20tokens%20and%20beyond%20with%20RMT-2023-arXiv.pdf}
}
@article{OPSGuptaICML2023, 
year = {2023}, 
title = {{Online Platt Scaling with Calibeating}}, 
author = {Gupta, Chirag and Ramdas, Aaditya}, 
journal = {ICML}, 
eprint = {2305.00070}, 
abstract = {{We present an online post-hoc calibration method, called Online Platt Scaling (OPS), which combines the Platt scaling technique with online logistic regression. We demonstrate that OPS smoothly adapts between i.i.d. and non-i.i.d. settings with distribution drift. Further, in scenarios where the best Platt scaling model is itself miscalibrated, we enhance OPS by incorporating a recently developed technique called calibeating to make it more robust. Theoretically, our resulting OPS+calibeating method is guaranteed to be calibrated for adversarial outcome sequences. Empirically, it is effective on a range of synthetic and real-world datasets, with and without distribution drifts, achieving superior performance without hyperparameter tuning. Finally, we extend all OPS ideas to the beta scaling method.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Gupta-Online%20Platt%20Scaling%20with%20Calibeating-2023-ICML.pdf}
}
@article{BCPBaiCVPR2023, 
year = {2023}, 
title = {{Bidirectional Copy-Paste for Semi-Supervised Medical Image Segmentation}}, 
author = {Bai, Yunhao and Chen, Duowen and Li, Qingli and Shen, Wei and Wang, Yan}, 
journal = {CVPR}, 
eprint = {2305.00673}, 
abstract = {{In semi-supervised medical image segmentation, there exist empirical mismatch problems between labeled and unlabeled data distribution. The knowledge learned from the labeled data may be largely discarded if treating labeled and unlabeled data separately or in an inconsistent manner. We propose a straightforward method for alleviating the problem - copy-pasting labeled and unlabeled data bidirectionally, in a simple Mean Teacher architecture. The method encourages unlabeled data to learn comprehensive common semantics from the labeled data in both inward and outward directions. More importantly, the consistent learning procedure for labeled and unlabeled data can largely reduce the empirical distribution gap. In detail, we copy-paste a random crop from a labeled image (foreground) onto an unlabeled image (background) and an unlabeled image (foreground) onto a labeled image (background), respectively. The two mixed images are fed into a Student network and supervised by the mixed supervisory signals of pseudo-labels and ground-truth. We reveal that the simple mechanism of copy-pasting bidirectionally between labeled and unlabeled data is good enough and the experiments show solid gains (e.g., over 21\% Dice improvement on ACDC dataset with 5\% labeled data) compared with other state-of-the-arts on various semi-supervised medical image segmentation datasets. Code is available at https://github.com/DeepMed-Lab-ECNU/BCP\}.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bai-Bidirectional%20Copy-Paste%20for%20Semi-Supervised%20Medical%20Image%20Segmentation-2023-arXiv.pdf}
}
@article{CTOLinIPMI2023, 
year = {2023}, 
title = {{Rethinking Boundary Detection in Deep Learning Models for Medical Image Segmentation}}, 
author = {Lin, Yi and Zhang, Dong and Fang, Xiao and Chen, Yufan and Cheng, Kwang-Ting and Chen, Hao}, 
journal = {IPMI}, 
eprint = {2305.00678}, 
abstract = {{Medical image segmentation is a fundamental task in the community of medical image analysis. In this paper, a novel network architecture, referred to as Convolution, Transformer, and Operator (CTO), is proposed. CTO employs a combination of Convolutional Neural Networks (CNNs), Vision Transformer (ViT), and an explicit boundary detection operator to achieve high recognition accuracy while maintaining an optimal balance between accuracy and efficiency. The proposed CTO follows the standard encoder-decoder segmentation paradigm, where the encoder network incorporates a popular CNN backbone for capturing local semantic information, and a lightweight ViT assistant for integrating long-range dependencies. To enhance the learning capacity on boundary, a boundary-guided decoder network is proposed that uses a boundary mask obtained from a dedicated boundary detection operator as explicit supervision to guide the decoding learning process. The performance of the proposed method is evaluated on six challenging medical image segmentation datasets, demonstrating that CTO achieves state-of-the-art accuracy with a competitive model complexity.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lin-Rethinking%20Boundary%20Detection%20in%20Deep%20Learning%20Models%20for%20Medical%20Image%20Segmentation-2023-arXiv.pdf}
}
@article{WhatParkICLR2023, 
year = {2023}, 
title = {{What Do Self-Supervised Vision Transformers Learn?}}, 
author = {Park, Namuk and Kim, Wonjae and Heo, Byeongho and Kim, Taekyung and Yun, Sangdoo}, 
journal = {ICLR}, 
eprint = {2305.00729}, 
abstract = {{We present a comparative study on how and why contrastive learning (CL) and masked image modeling (MIM) differ in their representations and in their performance of downstream tasks. In particular, we demonstrate that self-supervised Vision Transformers (ViTs) have the following properties: (1) CL trains self-attentions to capture longer-range global patterns than MIM, such as the shape of an object, especially in the later layers of the ViT architecture. This CL property helps ViTs linearly separate images in their representation spaces. However, it also makes the self-attentions collapse into homogeneity for all query tokens and heads. Such homogeneity of self-attention reduces the diversity of representations, worsening scalability and dense prediction performance. (2) CL utilizes the low-frequency signals of the representations, but MIM utilizes high-frequencies. Since low- and high-frequency information respectively represent shapes and textures, CL is more shape-oriented and MIM more texture-oriented. (3) CL plays a crucial role in the later layers, while MIM mainly focuses on the early layers. Upon these analyses, we find that CL and MIM can complement each other and observe that even the simplest harmonization can help leverage the advantages of both methods. The code is available at https://github.com/naver-ai/cl-vs-mim.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Park-What%20Do%20Self-Supervised%20Vision%20Transformers%20Learn--2023-ICLR.pdf}
}
@article{FLANV2LongprearXiv2023, 
year = {2023}, 
title = {{The Flan Collection: Designing Data and Methods for Effective Instruction Tuning}}, 
author = {Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and Roberts, Adam}, 
journal = {arXiv}, 
eprint = {2301.13688}, 
abstract = {{We study the design decisions of publicly available instruction tuning methods, and break down the development of Flan 2022 (Chung et al., 2022). Through careful ablation studies on the Flan Collection of tasks and methods, we tease apart the effect of design decisions which enable Flan-T5 to outperform prior work by 3-17\%+ across evaluation settings. We find task balancing and enrichment techniques are overlooked but critical to effective instruction tuning, and in particular, training with mixed prompt settings (zero-shot, few-shot, and chain-of-thought) actually yields stronger (2\%+) performance in all settings. In further experiments, we show Flan-T5 requires less finetuning to converge higher and faster than T5 on single downstream tasks, motivating instruction-tuned models as more computationally-efficient starting checkpoints for new tasks. Finally, to accelerate research on instruction tuning, we make the Flan 2022 collection of datasets, templates, and methods publicly available at https://github.com/google-research/FLAN/tree/main/flan/v2.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Longpre-The%20Flan%20Collection-%20Designing%20Data%20and%20Methods%20for%20Effective%20Instruction%20Tuning-2023-arXiv.pdf}
}
@article{UnnaturalInstructionsHonovichACL2023, 
year = {2023}, 
title = {{Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor}}, 
author = {Honovich, Or and Scialom, Thomas and Levy, Omer and Schick, Timo}, 
journal = {ACL}, 
eprint = {2212.09689}, 
abstract = {{Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions: a large dataset of creative and diverse instructions, collected with virtually no human labor. We collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open-source manually-curated datasets, surpassing the performance of models such as T0++ and Tk-Instruct across various benchmarks. These results demonstrate the potential of model-generated data as a cost-effective alternative to crowdsourcing for dataset expansion and diversification.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Honovich-Unnatural%20Instructions-%20Tuning%20Language%20Models%20with%20(Almost)%20No%20Human%20Labor-2023-ACL.pdf}
}
@article{MinervaLewkowyczNeurIPS2022, 
year = {2022}, 
title = {{Solving Quantitative Reasoning Problems with Language Models}}, 
author = {Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and Wu, Yuhuai and Neyshabur, Behnam and Gur-Ari, Guy and Misra, Vedant}, 
journal = {NeurIPS}, 
eprint = {2206.14858}, 
abstract = {{Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lewkowycz-Solving%20Quantitative%20Reasoning%20Problems%20with%20Language%20Models-2022-NeurIPS.pdf}
}
@article{STaRZelikmanNeurIPS2022, 
year = {2022}, 
title = {{STaR: Bootstrapping Reasoning With Reasoning}}, 
author = {Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah D}, 
journal = {NeurIPS}, 
eprint = {2203.14465}, 
abstract = {{Generating step-by-step "chain-of-thought" rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning. This technique, the "Self-Taught Reasoner" (STaR), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to fine-tuning a 30\$\textbackslashtimes\$ larger state-of-the-art language model on CommensenseQA. Thus, STaR lets a model improve itself by learning from its own generated reasoning.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zelikman-STaR-%20Bootstrapping%20Reasoning%20With%20Reasoning-2022-NeurIPS.pdf}
}
@article{Fine-tune-CoTHoACL2023, 
year = {2023}, 
title = {{Large Language Models Are Reasoning Teachers}}, 
author = {Ho, Namgyu and Schmid, Laura and Yun, Se-Young}, 
journal = {ACL}, 
eprint = {2212.10071}, 
abstract = {{Language models (LMs) have demonstrated remarkable performance on downstream tasks, using in-context exemplars or human instructions. Recent works have shown that chain-of-thought (CoT) prompting can elicit models to solve complex reasoning tasks, step-by-step. However, the efficacy of prompt-based CoT methods is restricted to very large LMs such as GPT-3 (175B), thus limiting deployability. In this paper, we revisit the fine-tuning approach to enable complex reasoning in smaller LMs, optimized to efficiently perform a specific task. We propose Fine-tune-CoT, a method that leverages the capabilities of very large LMs to generate reasoning samples and teach smaller models via fine-tuning. We evaluate our method on publicly available LMs across a wide range of complex tasks and model sizes. We find that Fine-tune-CoT enables substantial reasoning capability in small models, whereas previous prompt-based baselines exhibit near-random performance. Student models can even outperform the teacher in some tasks while reducing model size requirements by several orders of magnitude. We conduct extensive ablations and sample studies to understand the reasoning capabilities of student models. We also identify several important nuances that have been overlooked in concurrent fine-tuning works on CoT and address them in our analysis.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ho-Large%20Language%20Models%20Are%20Reasoning%20Teachers-2023-ACL.pdf}
}
@article{FalsePromiseGudibandearXiv2023, 
year = {2023}, 
title = {{The False Promise of Imitating Proprietary LLMs}}, 
author = {Gudibande, Arnav and Wallace, Eric and Snell, Charlie and Geng, Xinyang and Liu, Hao and Abbeel, Pieter and Levine, Sergey and Song, Dawn}, 
journal = {arXiv}, 
eprint = {2305.15717}, 
abstract = {{An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply imitate the proprietary model's capabilities using a weaker open-source model. In this work, we critically analyze this approach. We first finetune a series of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the models using crowd raters and canonical NLP benchmarks. Initially, we were surprised by the output quality of our imitation models -- they appear far better at following instructions, and crowd workers rate their outputs as competitive with ChatGPT. However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data. We show that these performance discrepancies may slip past human raters because imitation models are adept at mimicking ChatGPT's style but not its factuality. Overall, we conclude that model imitation is a false promise: there exists a substantial capabilities gap between open and closed LMs that, with current methods, can only be bridged using an unwieldy amount of imitation data or by using more capable base LMs. In turn, we argue that the highest leverage action for improving open-source models is to tackle the difficult challenge of developing better base LMs, rather than taking the shortcut of imitating proprietary systems.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Gudibande-The%20False%20Promise%20of%20Imitating%20Proprietary%20LLMs-2023-arXiv.pdf}
}
@article{SyntheticPromptingShaoICML2023, 
year = {2023}, 
title = {{Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models}}, 
author = {Shao, Zhihong and Gong, Yeyun and Shen, Yelong and Huang, Minlie and Duan, Nan and Chen, Weizhu}, 
journal = {ICML}, 
eprint = {2302.00618}, 
abstract = {{Large language models can perform various reasoning tasks by using chain-of-thought prompting, which guides them to find answers through step-by-step demonstrations. However, the quality of the prompts depends on the demonstrations given to the models, and creating many of them by hand is costly. We introduce Synthetic prompting, a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning. Our method alternates between a backward and forward process to generate new examples. The backward process generates a question that match a sampled reasoning chain, so that the question is solvable and clear. The forward process produces a more detailed reasoning chain for the question, improving the quality of the example. We evaluate our method on numerical, symbolic, and algorithmic reasoning tasks, and show that it outperforms existing prompting techniques.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Shao-Synthetic%20Prompting-%20Generating%20Chain-of-Thought%20Demonstrations%20for%20Large%20Language%20Models-2023-ICML.pdf}
}
@article{LMSIHuangarXiv2022, 
year = {2022}, 
title = {{Large Language Models Can Self-Improve}}, 
author = {Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei}, 
journal = {arXiv}, 
eprint = {2210.11610}, 
abstract = {{Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate "high-confidence" rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4\%->82.1\% on GSM8K, 78.2\%->83.0\% on DROP, 90.0\%->94.4\% on OpenBookQA, and 63.4\%->67.9\% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Huang-Large%20Language%20Models%20Can%20Self-Improve-2022-arXiv.pdf}
}
@article{Self-consistencyWangICLR2023, 
year = {2023}, 
title = {{Self-Consistency Improves Chain of Thought Reasoning in Language Models}}, 
author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny}, 
journal = {ICLR}, 
eprint = {2203.11171}, 
abstract = {{Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Self-Consistency%20Improves%20Chain%20of%20Thought%20Reasoning%20in%20Language%20Models-2023-ICLR.pdf}
}
@article{Complexity-basedPromptingFuICLR2023, 
year = {2023}, 
title = {{Complexity-Based Prompting for Multi-Step Reasoning}}, 
author = {Fu, Yao and Peng, Hao and Sabharwal, Ashish and Clark, Peter and Khot, Tushar}, 
journal = {ICLR}, 
eprint = {2210.00720}, 
abstract = {{We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on math word reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3, our approach substantially improves multi-step reasoning accuracy, with an 8.6\% absolute improvement on GSM8K, and 6.4\% on MathQA. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of our methods under format perturbation and distribution shift.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Fu-Complexity-Based%20Prompting%20for%20Multi-Step%20Reasoning-2023-ICLR.pdf}
}
@article{ReproducibilitySimkoMIDL2023, 
year = {2023}, 
title = {{Reproducibility of the Methods in Medical Imaging with Deep Learning}}, 
author = {Simko, Attila and Garpebring, Anders and Jonsson, Joakim and Nyholm, Tufve and Löfstedt, Tommy}, 
journal = {MIDL}, 
eprint = {2210.11146}, 
abstract = {{Concerns about the reproducibility of deep learning research are more prominent than ever, with no clear solution in sight. The relevance of machine learning research can only be improved if we also employ empirical rigor that incorporates reproducibility guidelines, especially so in the medical imaging field. The Medical Imaging with Deep Learning (MIDL) conference has made advancements in this direction by advocating open access, and recently also recommending authors to make their code public - both aspects being adopted by the majority of the conference submissions. This helps the reproducibility of the methods, however, there is currently little or no support for further evaluation of these supplementary material, making them vulnerable to poor quality, which affects the impact of the entire submission. We have evaluated all accepted full paper submissions to MIDL between 2018 and 2022 using established, but slightly adjusted guidelines on reproducibility and the quality of the public repositories. The evaluations show that publishing repositories and using public datasets are becoming more popular, which helps traceability, but the quality of the repositories has not improved over the years, leaving room for improvement in every aspect of designing repositories. Merely 22\% of all submissions contain a repository that were deemed repeatable using our evaluations. From the commonly encountered issues during the evaluations, we propose a set of guidelines for machine learning-related research for medical imaging applications, adjusted specifically for future submissions to MIDL.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Simko-Reproducibility%20of%20the%20Methods%20in%20Medical%20Imaging%20with%20Deep%20Learning-2023-MIDL.pdf}
}
@article{GeoLSVasudevaMIDL2023, 
year = {2023}, 
title = {{GeoLS: Geodesic Label Smoothing for Image Segmentation}}, 
author = {Vasudeva, Sukesh Adiga and Dolz, Jose and Lombaert, Herve}, 
journal = {MIDL}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Vasudeva-GeoLS-%20Geodesic%20Label%20Smoothing%20for%20Image%20Segmentation-2023-MIDL.pdf}
}
@article{L-SoftmaxLiuICML2016, 
year = {2016}, 
title = {{Large-Margin Softmax Loss for Convolutional Neural Networks}}, 
author = {Liu, Weiyang and Wen, Yandong and Yu, Zhiding and Yang, Meng}, 
journal = {ICML}, 
eprint = {1612.02295}, 
abstract = {{Cross-entropy loss together with softmax is arguably one of the most common used supervision components in convolutional neural networks (CNNs). Despite its simplicity, popularity and excellent performance, the component does not explicitly encourage discriminative learning of features. In this paper, we propose a generalized large-margin softmax (L-Softmax) loss which explicitly encourages intra-class compactness and inter-class separability between learned features. Moreover, L-Softmax not only can adjust the desired margin but also can avoid overfitting. We also show that the L-Softmax loss can be optimized by typical stochastic gradient descent. Extensive experiments on four benchmark datasets demonstrate that the deeply-learned features with L-softmax loss become more discriminative, hence significantly boosting the performance on a variety of visual classification and verification tasks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-Large-Margin%20Softmax%20Loss%20for%20Convolutional%20Neural%20Networks-2016-ICML.pdf}
}
@article{HowDijkICCV2019, 
year = {2019}, 
title = {{How Do Neural Networks See Depth in Single Images?}}, 
author = {Dijk, Tom van and Croon, Guido de}, 
journal = {ICCV}, 
abstract = {{Deep neural networks have lead to a breakthrough in depth estimation from single images. Recent work shows that the quality of these estimations is rapidly increasing. It is clear that neural networks can see depth in single images. However, to the best of our knowledge, no work currently exists that analyzes what these networks have learned. In this work we take four previously published networks and investigate what depth cues they exploit. We find that all networks ignore the apparent size of known obstacles in favor of their vertical position in the image. The use of the vertical position requires the camera pose to be known; however, we find that these networks only partially recognize changes in camera pitch and roll angles. Small changes in camera pitch are shown to disturb the estimated distance towards obstacles. The use of the vertical image position allows the networks to estimate depth towards arbitrary obstacles– even those not appearing in the training set– but may depend on features that are not universally present.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Dijk-How%20Do%20Neural%20Networks%20See%20Depth%20in%20Single%20Images--2019-ICCV.pdf}
}
@article{HumpbackLiarXiv2023, 
year = {2023}, 
title = {{Self-Alignment with Instruction Backtranslation}}, 
author = {Li, Xian and Yu, Ping and Zhou, Chunting and Schick, Timo and Zettlemoyer, Luke and Levy, Omer and Weston, Jason and Lewis, Mike}, 
journal = {arXiv}, 
eprint = {2308.06259}, 
abstract = {{We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Self-Alignment%20with%20Instruction%20Backtranslation-2023-arXiv.pdf}
}
@article{LongFormKoksalarXiv2023, 
year = {2023}, 
title = {{LongForm: Optimizing Instruction Tuning for Long Text Generation with Corpus Extraction}}, 
author = {Köksal, Abdullatif and Schick, Timo and Korhonen, Anna and Schütze, Hinrich}, 
journal = {arXiv}, 
eprint = {2304.08460}, 
abstract = {{Instruction tuning enables language models to generalize more effectively and better follow user intent. However, obtaining instruction data can be costly and challenging. Prior works employ methods such as expensive human annotation, crowd-sourced datasets with alignment issues, or generating noisy examples via LLMs. We introduce the LongForm dataset, which is created by leveraging English corpus examples with augmented instructions. We select a diverse set of human-written documents from existing corpora such as C4 and Wikipedia and generate instructions for the given documents via LLMs. This approach provides a cheaper and cleaner instruction-tuning dataset and one suitable for long text generation. We finetune T5, OPT, and LLaMA models on our dataset and show that even smaller LongForm models have good generalization capabilities for text generation. Our models outperform 10x larger language models without instruction tuning on various tasks such as story/recipe generation and long-form question answering. Moreover, LongForm models outperform prior instruction-tuned models such as FLAN-T5 and Alpaca by a large margin. Finally, our models can effectively follow and answer multilingual instructions; we demonstrate this for news generation. We publicly release our data and models: https://github.com/akoksal/LongForm.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Köksal-LongForm-%20Optimizing%20Instruction%20Tuning%20for%20Long%20Text%20Generation%20with%20Corpus%20Extraction-2023-arXiv.pdf}
}
@article{RM-LossChaoMICCAI2022, 
year = {2022}, 
title = {{Regression Metric Loss: Learning a Semantic Representation Space for Medical Images}}, 
author = {Chao, Hanqing and Zhang, Jiajin and Yan, Pingkun}, 
journal = {MICCAI}, 
eprint = {2207.05231}, 
abstract = {{Regression plays an essential role in many medical imaging applications for estimating various clinical risk or measurement scores. While training strategies and loss functions have been studied for the deep neural networks in medical image classification tasks, options for regression tasks are very limited. One of the key challenges is that the high-dimensional feature representation learned by existing popular loss functions like Mean Squared Error or L1 loss is hard to interpret. In this paper, we propose a novel Regression Metric Loss (RM-Loss), which endows the representation space with the semantic meaning of the label space by finding a representation manifold that is isometric to the label space. Experiments on two regression tasks, i.e. coronary artery calcium score estimation and bone age assessment, show that RM-Loss is superior to the existing popular regression losses on both performance and interpretability. Code is available at https://github.com/DIAL-RPI/Regression-Metric-Loss.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chao-Regression%20Metric%20Loss-%20Learning%20a%20Semantic%20Representation%20Space%20for%20Medical%20Images-2022-MICCAI.pdf}
}
@article{BoundaryDoULossSunMICCAI2023, 
year = {2023}, 
title = {{Boundary Difference Over Union Loss For Medical Image Segmentation}}, 
author = {Sun, Fan and Luo, Zhiming and Li, Shaozi}, 
journal = {MICCAI}, 
eprint = {2308.00220}, 
abstract = {{Medical image segmentation is crucial for clinical diagnosis. However, current losses for medical image segmentation mainly focus on overall segmentation results, with fewer losses proposed to guide boundary segmentation. Those that do exist often need to be used in combination with other losses and produce ineffective results. To address this issue, we have developed a simple and effective loss called the Boundary Difference over Union Loss (Boundary DoU Loss) to guide boundary region segmentation. It is obtained by calculating the ratio of the difference set of prediction and ground truth to the union of the difference set and the partial intersection set. Our loss only relies on region calculation, making it easy to implement and training stable without needing any additional losses. Additionally, we use the target size to adaptively adjust attention applied to the boundary regions. Experimental results using UNet, TransUNet, and Swin-UNet on two datasets (ACDC and Synapse) demonstrate the effectiveness of our proposed loss function. Code is available at https://github.com/sunfan-bvb/BoundaryDoULoss.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Sun-Boundary%20Difference%20Over%20Union%20Loss%20For%20Medical%20Image%20Segmentation-2023-MICCAI.pdf}
}
@article{RandomErasingZhongAAAI2020, 
year = {2020}, 
title = {{Random Erasing Data Augmentation}}, 
author = {Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi}, 
journal = {AAAI}, 
eprint = {1708.04896}, 
abstract = {{In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: https://github.com/zhunzhong07/Random-Erasing.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhong-Random%20Erasing%20Data%20Augmentation-2020-AAAI.pdf}
}
@article{OPFGaoACL2023, 
year = {2023}, 
title = {{Small Pre-trained Language Models Can be Fine-tuned as Large Models via Over-Parameterization}}, 
author = {Gao, Ze-Feng and Zhou, Kun and Liu, Peiyu and Zhao, Wayne Xin and Wen, Ji-Rong}, 
journal = {ACL}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Gao-Small%20Pre-trained%20Language%20Models%20Can%20be%20Fine-tuned%20as%20Large%20Models%20via%20Over-Parameterization-2023-ACL.pdf}
}
@article{WINODICTEisenschlosEACL2023, 
year = {2023}, 
title = {{WinoDict: Probing language models for in-context word acquisition}}, 
author = {Eisenschlos, Julian Martin and Cole, Jeremy R and Liu, Fangyu and Cohen, William W}, 
journal = {EACL}, 
eprint = {2209.12153}, 
abstract = {{We introduce a new in-context learning paradigm to measure Large Language Models' (LLMs) ability to learn novel words during inference. In particular, we rewrite Winograd-style co-reference resolution problems by replacing the key concept word with a synthetic but plausible word that the model must understand to complete the task. Solving this task requires the model to make use of the dictionary definition of the new word given in the prompt. This benchmark addresses word acquisition, one important aspect of the diachronic degradation known to afflict LLMs. As LLMs are frozen in time at the moment they are trained, they are normally unable to reflect the way language changes over time. We show that the accuracy of LLMs compared to the original Winograd tasks decreases radically in our benchmark, thus identifying a limitation of current models and providing a benchmark to measure future improvements in LLMs ability to do in-context learning.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Eisenschlos-WinoDict-%20Probing%20language%20models%20for%20in-context%20word%20acquisition-2023-EACL.pdf}
}
@article{InstructionBiasParmarEACL2023, 
year = {2023}, 
title = {{Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions}}, 
author = {Parmar, Mihir and Mishra, Swaroop and Geva, Mor and Baral, Chitta}, 
journal = {EACL}, 
eprint = {2205.00415}, 
abstract = {{In recent years, progress in NLU has been driven by benchmarks. These benchmarks are typically collected by crowdsourcing, where annotators write examples based on annotation instructions crafted by dataset creators. In this work, we hypothesize that annotators pick up on patterns in the crowdsourcing instructions, which bias them to write many similar examples that are then over-represented in the collected data. We study this form of bias, termed instruction bias, in 14 recent NLU benchmarks, showing that instruction examples often exhibit concrete patterns, which are propagated by crowdworkers to the collected data. This extends previous work (Geva et al., 2019) and raises a new concern of whether we are modeling the dataset creator's instructions, rather than the task. Through a series of experiments, we show that, indeed, instruction bias can lead to overestimation of model performance, and that models struggle to generalize beyond biases originating in the crowdsourcing instructions. We further analyze the influence of instruction bias in terms of pattern frequency and model size, and derive concrete recommendations for creating future NLU benchmarks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Parmar-Don't%20Blame%20the%20Annotator-%20Bias%20Already%20Starts%20in%20the%20Annotation%20Instructions-2023-EACL.pdf}
}
@article{CircleLossSunCVPR2020, 
year = {2020}, 
title = {{Circle Loss: A Unified Perspective of Pair Similarity Optimization}}, 
author = {Sun, Yifan and Cheng, Changmao and Zhang, Yuhan and Zhang, Chi and Zheng, Liang and Wang, Zhongdao and Wei, Yichen}, 
journal = {CVPR}, 
abstract = {{This paper provides a pair similarity optimization viewpoint on deep feature learning, aiming to maximize the within-class similarity \$s\_\{p\}\$ and minimize the between-class similarity \$s\_\{n\}\$. We find a majority of loss functions, including the triplet loss and the softmax cross-entropy loss, embed \$s\_\{n\}\$ and \$s\_\{p\}\$ into similarity pairs and seek to reduce \$(s\_\{n\}-s\_\{p\})\$. Such an optimization manner is inflexible, because the penalty strength on every single similarity score is restricted to be equal. Our intuition is that if a similarity score deviates far from the optimum, it should be emphasized. To this end, we simply re-weight each similarity to highlight the less-optimized similarity scores. It results in a Circle loss, which is named due to its circular decision boundary. The Circle loss has a unified formula for two elemental deep feature learning paradigms, i.e., learning with class-level labels and pair-wise labels. Analytically, we show that the Circle loss offers a more flexible optimization approach towards a more definite convergence target, compared with the loss functions optimizing \$(s\_\{n\}-s\_\{p\})\$. Experimentally, we demonstrate the superiority of the Circle loss on a variety of deep feature learning tasks. On face recognition, person re-identification, as well as several fine-grained image retrieval datasets, the achieved performance is on par with the state of the art.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Sun-Circle%20Loss-%20A%20Unified%20Perspective%20of%20Pair%20Similarity%20Optimization-2020-CVPR.pdf}
}
@article{EfficientArtetxeEMNLP2022, 
year = {2022}, 
title = {{Efficient Large Scale Language Modeling with Mixtures of Experts}}, 
author = {Artetxe, Mikel and Bhosale, Shruti and Goyal, Naman and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Lin, Xi Victoria and Du, Jingfei and Iyer, Srinivasan and Pasunuru, Ramakanth and Anantharaman, Giri and Li, Xian and Chen, Shuohui and Akin, Halil and Baines, Mandeep and Martin, Louis and Zhou, Xing and Koura, Punit Singh and O'Horo, Brian and Wang, Jeff and Zettlemoyer, Luke and Diab, Mona and Kozareva, Zornitsa and Stoyanov, Ves}, 
journal = {EMNLP}, 
eprint = {2112.10684}, 
abstract = {{Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language modeling, zero- and few-shot priming, and full-shot fine-tuning. With the exception of fine-tuning, we find MoEs to be substantially more compute efficient. At more modest training budgets, MoEs can match the performance of dense models using \$\textbackslashsim\$4 times less compute. This gap narrows at scale, but our largest MoE model (1.1T parameters) consistently outperforms a compute-equivalent dense model (6.7B parameters). Overall, this performance gap varies greatly across tasks and domains, suggesting that MoE and dense models generalize differently in ways that are worthy of future study. We make our code and models publicly available for research use.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Artetxe-Efficient%20Large%20Scale%20Language%20Modeling%20with%20Mixtures%20of%20Experts-2022-EMNLP.pdf}
}
@article{FLAN-MoEShenarXiv2023, 
year = {2023}, 
title = {{Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models}}, 
author = {Shen, Sheng and Hou, Le and Zhou, Yanqi and Du, Nan and Longpre, Shayne and Wei, Jason and Chung, Hyung Won and Zoph, Barret and Fedus, William and Chen, Xinyun and Vu, Tu and Wu, Yuexin and Chen, Wuyang and Webson, Albert and Li, Yunxuan and Zhao, Vincent and Yu, Hongkun and Keutzer, Kurt and Darrell, Trevor and Zhou, Denny}, 
journal = {arXiv}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Shen-Mixture-of-Experts%20Meets%20Instruction%20Tuning-%20A%20Winning%20Combination%20for%20Large%20Language%20Models-2023-arXiv.pdf}
}
@article{GShardLepikhinICLR2021, 
year = {2021}, 
title = {{GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding}}, 
author = {Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng}, 
journal = {ICLR}, 
eprint = {2006.16668}, 
abstract = {{Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lepikhin-GShard-%20Scaling%20Giant%20Models%20with%20Conditional%20Computation%20and%20Automatic%20Sharding-2021-ICLR.pdf}
}
@article{MoEShazeerICLR2017, 
year = {2017}, 
title = {{Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}}, 
author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff}, 
journal = {ICLR}, 
eprint = {1701.06538}, 
abstract = {{The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Shazeer-Outrageously%20Large%20Neural%20Networks-%20The%20Sparsely-Gated%20Mixture-of-Experts%20Layer-2017-ICLR.pdf}
}
@article{PrefixTuningLiACL2021, 
year = {2021}, 
title = {{Prefix-Tuning: Optimizing Continuous Prompts for Generation}}, 
author = {Li, Xiang Lisa and Liang, Percy}, 
journal = {ACL}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Prefix-Tuning-%20Optimizing%20Continuous%20Prompts%20for%20Generation-2021-ACL.pdf}
}
@article{GoTBestaarXiv2023, 
year = {2023}, 
title = {{Graph of Thoughts: Solving Elaborate Problems with Large Language Models}}, 
author = {Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Podstawski, Michal and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten}, 
journal = {arXiv}, 
eprint = {2308.09687}, 
abstract = {{We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-ofThought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62\% over ToT, while simultaneously reducing costs by >31\%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Besta-Graph%20of%20Thoughts-%20Solving%20Elaborate%20Problems%20with%20Large%20Language%20Models-2023-arXiv.pdf}
}
@article{LISALaiarXiv2023, 
year = {2023}, 
title = {{LISA: Reasoning Segmentation via Large Language Model}}, 
author = {Lai, Xin and Tian, Zhuotao and Chen, Yukang and Li, Yanwei and Yuan, Yuhui and Liu, Shu and Jia, Jiaya}, 
journal = {arXiv}, 
eprint = {2308.00692}, 
abstract = {{Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction to identify the target objects or categories before executing visual recognition tasks. Such systems lack the ability to actively reason and comprehend implicit user intentions. In this work, we propose a new segmentation task -- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction pairs, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of the multi-modal Large Language Model (LLM) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a <SEG> token and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving: 1) complex reasoning; 2) world knowledge; 3) explanatory answers; 4) multi-turn conversation. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation image-instruction pairs results in further performance enhancement. Experiments show our method not only unlocks new reasoning segmentation capabilities but also proves effective in both complex reasoning segmentation and standard referring segmentation tasks. Code, models, and demo are at https://github.com/dvlab-research/LISA.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lai-LISA-%20Reasoning%20Segmentation%20via%20Large%20Language%20Model-2023-arXiv.pdf}
}
@article{DreamBoothRuizCVPR2023, 
year = {2023}, 
title = {{DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation}}, 
author = {Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir}, 
journal = {CVPR}, 
eprint = {2208.12242}, 
abstract = {{Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for "personalization" of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: https://dreambooth.github.io/}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ruiz-DreamBooth-%20Fine%20Tuning%20Text-to-Image%20Diffusion%20Models%20for%20Subject-Driven%20Generation-2023-CVPR.pdf}
}
@article{LMDLianarXiv2023, 
year = {2023}, 
title = {{LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models}}, 
author = {Lian, Long and Li, Boyi and Yala, Adam and Darrell, Trevor}, 
journal = {arXiv}, 
eprint = {2305.13655}, 
abstract = {{Recent advancements in text-to-image generation with diffusion models have yielded remarkable results synthesizing highly realistic and diverse images. However, these models still encounter difficulties when generating images from prompts that demand spatial or common sense reasoning. We propose to equip diffusion models with enhanced reasoning capabilities by using off-the-shelf pretrained large language models (LLMs) in a novel two-stage generation process. First, we adapt an LLM to be a text-guided layout generator through in-context learning. When provided with an image prompt, an LLM outputs a scene layout in the form of bounding boxes along with corresponding individual descriptions. Second, we steer a diffusion model with a novel controller to generate images conditioned on the layout. Both stages utilize frozen pretrained models without any LLM or diffusion model parameter optimization. We validate the superiority of our design by demonstrating its ability to outperform the base diffusion model in accurately generating images according to prompts that necessitate both language and spatial reasoning. Additionally, our method naturally allows dialog-based scene specification and is able to handle prompts in a language that is not well-supported by the underlying diffusion model.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lian-LLM-grounded%20Diffusion-%20Enhancing%20Prompt%20Understanding%20of%20Text-to-Image%20Diffusion%20Models%20with%20Large%20Language%20Models-2023-arXiv.pdf}
}
@article{OMS-DPMLiuICML2023, 
year = {2023}, 
title = {{OMS-DPM: Optimizing the Model Schedule for Diffusion Probabilistic Models}}, 
author = {Liu, Enshu and Ning, Xuefei and Lin, Zinan and Yang, Huazhong and Wang, Yu}, 
journal = {ICML}, 
eprint = {2306.08860}, 
abstract = {{Diffusion probabilistic models (DPMs) are a new class of generative models that have achieved state-of-the-art generation quality in various domains. Despite the promise, one major drawback of DPMs is the slow generation speed due to the large number of neural network evaluations required in the generation process. In this paper, we reveal an overlooked dimension -- model schedule -- for optimizing the trade-off between generation quality and speed. More specifically, we observe that small models, though having worse generation quality when used alone, could outperform large models in certain generation steps. Therefore, unlike the traditional way of using a single model, using different models in different generation steps in a carefully designed \textbackslashemph\{model schedule\} could potentially improve generation quality and speed \textbackslashemph\{simultaneously\}. We design OMS-DPM, a predictor-based search algorithm, to optimize the model schedule given an arbitrary generation time budget and a set of pre-trained models. We demonstrate that OMS-DPM can find model schedules that improve generation quality and speed than prior state-of-the-art methods across CIFAR-10, CelebA, ImageNet, and LSUN datasets. When applied to the public checkpoints of the Stable Diffusion model, we are able to accelerate the sampling by 2\$\textbackslashtimes\$ while maintaining the generation quality.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-OMS-DPM-%20Optimizing%20the%20Model%20Schedule%20for%20Diffusion%20Probabilistic%20Models-2023-ICML.pdf}
}
@article{JudgingZhengarXiv2023, 
year = {2023}, 
title = {{Judging LLM-as-a-judge with MT-Bench and Chatbot Arena}}, 
author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P and Zhang, Hao and Gonzalez, Joseph E and Stoica, Ion}, 
journal = {arXiv}, 
eprint = {2306.05685}, 
abstract = {{Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\textbackslash\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. We will publicly release MT-bench questions, 3K expert votes, and 30K conversations with human preferences from Chatbot Arena.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zheng-Judging%20LLM-as-a-judge%20with%20MT-Bench%20and%20Chatbot%20Arena-2023-arXiv.pdf}
}
@article{AoTSelarXiv2023, 
year = {2023}, 
title = {{Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models}}, 
author = {Sel, Bilgehan and Al-Tawaha, Ahmad and Khattar, Vanshaj and Wang, Lu and Jia, Ruoxi and Jin, Ming}, 
journal = {arXiv}, 
eprint = {2308.10379}, 
abstract = {{Current literature, aiming to surpass the "Chain-of-Thought" approach, often resorts to an external modus operandi involving halting, modifying, and then resuming the generation process to boost Large Language Models' (LLMs) reasoning capacities. This mode escalates the number of query requests, leading to increased costs, memory, and computational overheads. Addressing this, we propose the Algorithm of Thoughts -- a novel strategy that propels LLMs through algorithmic reasoning pathways, pioneering a new mode of in-context learning. By employing algorithmic examples, we exploit the innate recurrence dynamics of LLMs, expanding their idea exploration with merely one or a few queries. Our technique outperforms earlier single-query methods and stands on par with a recent multi-query strategy that employs an extensive tree search algorithm. Intriguingly, our results suggest that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself, hinting at LLM's inherent ability to weave its intuition into optimized searches. We probe into the underpinnings of our method's efficacy and its nuances in application.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Sel-Algorithm%20of%20Thoughts-%20Enhancing%20Exploration%20of%20Ideas%20in%20Large%20Language%20Models-2023-arXiv.pdf}
}
@article{ToTLongarXiv2023, 
year = {2023}, 
title = {{Large Language Model Guided Tree-of-Thought}}, 
author = {Long, Jieyi}, 
journal = {arXiv}, 
eprint = {2305.08291}, 
abstract = {{In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implemented a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving. Our implementation of the ToT-based Sudoku solver is available on GitHub: \textbackslashurl\{https://github.com/jieyilong/tree-of-thought-puzzle-solver\}.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Long-Large%20Language%20Model%20Guided%20Tree-of-Thought-2023-arXiv.pdf}
}
@article{DistillingStep-by-stepHsiehACLFinding2023, 
year = {2023}, 
title = {{Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes}}, 
author = {Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-Kuan and Nakhost, Hootan and Fujii, Yasuhisa and Ratner, Alexander and Krishna, Ranjay and Lee, Chen-Yu and Pfister, Tomas}, 
journal = {ACL Finding}, 
eprint = {2305.02301}, 
abstract = {{Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for training small models within a multi-task framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to few-shot prompted LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our finetuned 770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80\% of available data on a benchmark, whereas standard finetuning the same T5 model struggles to match even by using 100\% of the dataset. We release the code at: https://github.com/google-research/distilling-step-by-step .}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Hsieh-Distilling%20Step-by-Step!%20Outperforming%20Larger%20Language%20Models%20with%20Less%20Training%20Data%20and%20Smaller%20Model%20Sizes-2023-ACL%20Finding.pdf}
}
@article{DecompositionXiearXiv2023, 
year = {2023}, 
title = {{Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding}}, 
author = {Xie, Yuxi and Kawaguchi, Kenji and Zhao, Yiran and Zhao, Xu and Kan, Min-Yen and He, Junxian and Xie, Qizhe}, 
journal = {arXiv}, 
eprint = {2305.00633}, 
abstract = {{We endow Large Language Models (LLMs) with fine-grained self-evaluation to refine multi-step reasoning inference. We propose an effective prompting approach that integrates self-evaluation guidance through stochastic beam search. Our approach explores the reasoning search space using a well-calibrated automatic criterion. This enables an efficient search to produce higher-quality final predictions. With the self-evaluation guided stochastic beam search, we also balance the quality-diversity trade-off in the generation of reasoning chains. This allows our approach to adapt well with majority voting and surpass the corresponding Codex-backboned baselines by \$6.34\textbackslash\%\$, \$9.56\textbackslash\%\$, and \$5.46\textbackslash\%\$ on the GSM8K, AQuA, and StrategyQA benchmarks, respectively, in few-shot accuracy. Analysis of our decompositional reasoning finds it pinpoints logic failures and leads to higher consistency and robustness. Our code is publicly available at https://github.com/YuxiXie/SelfEval-Guided-Decoding.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xie-Decomposition%20Enhances%20Reasoning%20via%20Self-Evaluation%20Guided%20Decoding-2023-arXiv.pdf}
}
@article{BLIPLiICML2022, 
year = {2022}, 
title = {{BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation}}, 
author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven}, 
journal = {ICML}, 
eprint = {2201.12086}, 
abstract = {{Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7\% in average recall@1), image captioning (+2.8\% in CIDEr), and VQA (+1.6\% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at https://github.com/salesforce/BLIP.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-BLIP-%20Bootstrapping%20Language-Image%20Pre-training%20for%20Unified%20Vision-Language%20Understanding%20and%20Generation-2022-ICML.pdf}
}
@article{BLIP-2LiICML2023, 
year = {2023}, 
title = {{BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models}}, 
author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven}, 
journal = {ICML}, 
eprint = {2301.12597}, 
abstract = {{The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7\% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-BLIP-2-%20Bootstrapping%20Language-Image%20Pre-training%20with%20Frozen%20Image%20Encoders%20and%20Large%20Language%20Models-2023-ICML.pdf}
}
@article{X-IQEChenarXiv2023, 
year = {2023}, 
title = {{X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models}}, 
author = {Chen, Yixiong and Liu, Li and Ding, Chris}, 
journal = {arXiv}, 
eprint = {2305.10843}, 
abstract = {{This paper introduces a novel explainable image quality evaluation approach called X-IQE, which leverages visual large language models (LLMs) to evaluate text-to-image generation methods by generating textual explanations. X-IQE utilizes a hierarchical Chain of Thought (CoT) to enable MiniGPT-4 to produce self-consistent, unbiased texts that are highly correlated with human evaluation. It offers several advantages, including the ability to distinguish between real and generated images, evaluate text-image alignment, and assess image aesthetics without requiring model training or fine-tuning. X-IQE is more cost-effective and efficient compared to human evaluation, while significantly enhancing the transparency and explainability of deep image quality evaluation models. We validate the effectiveness of our method as a benchmark using images generated by prevalent diffusion models. X-IQE demonstrates similar performance to state-of-the-art (SOTA) evaluation methods on COCO Caption, while overcoming the limitations of previous evaluation models on DrawBench, particularly in handling ambiguous generation prompts and text recognition in generated images. Project website: https://github.com/Schuture/Benchmarking-Awesome-Diffusion-Models}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-X-IQE-%20eXplainable%20Image%20Quality%20Evaluation%20for%20Text-to-Image%20Generation%20with%20Visual%20Large%20Language%20Models-2023-arXiv.pdf}
}
@article{MiniGPT-4ZhuarXiv2023, 
year = {2023}, 
title = {{MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models}}, 
author = {Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed}, 
journal = {arXiv}, 
eprint = {2304.10592}, 
abstract = {{The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. We believe the primary reason for GPT-4's advanced multi-modal generation capabilities lies in the utilization of a more advanced large language model (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen LLM, Vicuna, using just one projection layer. Our findings reveal that MiniGPT-4 possesses many capabilities similar to those exhibited by GPT-4 like detailed image description generation and website creation from hand-written drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, providing solutions to problems shown in images, teaching users how to cook based on food photos, etc. In our experiment, we found that only performing the pretraining on raw image-text pairs could produce unnatural language outputs that lack coherency including repetition and fragmented sentences. To address this problem, we curate a high-quality, well-aligned dataset in the second stage to finetune our model using a conversational template. This step proved crucial for augmenting the model's generation reliability and overall usability. Notably, our model is highly computationally efficient, as we only train a projection layer utilizing approximately 5 million aligned image-text pairs. Our code, pre-trained model, and collected dataset are available at https://minigpt-4.github.io/.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhu-MiniGPT-4-%20Enhancing%20Vision-Language%20Understanding%20with%20Advanced%20Large%20Language%20Models-2023-arXiv_1.pdf}
}
@article{FLASHATTENTIONDaoNeurIPS2022, 
year = {2022}, 
title = {{FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}}, 
author = {Dao, Tri and Fu, Daniel Y and Ermon, Stefano and Rudra, Atri and Ré, Christopher}, 
journal = {NeurIPS}, 
eprint = {2205.14135}, 
abstract = {{Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15\% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3\$\textbackslashtimes\$ speedup on GPT-2 (seq. length 1K), and 2.4\$\textbackslashtimes\$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4\% accuracy) and Path-256 (seq. length 64K, 63.1\% accuracy).}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Dao-FlashAttention-%20Fast%20and%20Memory-Efficient%20Exact%20Attention%20with%20IO-Awareness-2022-NeurIPS.pdf}
}
@article{BoMDChenICCV2023, 
year = {2023}, 
title = {{BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification}}, 
author = {Chen, Yuanhong and Liu, Fengbei and Wang, Hu and Wang, Chong and Tian, Yu and Liu, Yuyuan and Carneiro, Gustavo}, 
journal = {ICCV}, 
eprint = {2203.01937}, 
abstract = {{Deep learning methods have shown outstanding classification accuracy in medical imaging problems, which is largely attributed to the availability of large-scale datasets manually annotated with clean labels. However, given the high cost of such manual annotation, new medical imaging classification problems may need to rely on machine-generated noisy labels extracted from radiology reports. Indeed, many Chest X-ray (CXR) classifiers have already been modelled from datasets with noisy labels, but their training procedure is in general not robust to noisy-label samples, leading to sub-optimal models. Furthermore, CXR datasets are mostly multi-label, so current noisy-label learning methods designed for multi-class problems cannot be easily adapted. In this paper, we propose a new method designed for the noisy multi-label CXR learning, which detects and smoothly re-labels samples from the dataset, which is then used to train common multi-label classifiers. The proposed method optimises a bag of multi-label descriptors (BoMD) to promote their similarity with the semantic descriptors produced by BERT models from the multi-label image annotation. Our experiments on diverse noisy multi-label training sets and clean testing sets show that our model has state-of-the-art accuracy and robustness in many CXR multi-label classification benchmarks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-BoMD-%20Bag%20of%20Multi-label%20Descriptors%20for%20Noisy%20Chest%20X-ray%20Classification-2023-ICCV.pdf}
}
@article{GRIPSPrasadEACL2023, 
year = {2023}, 
title = {{GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models}}, 
author = {Prasad, Archiki and Hase, Peter and Zhou, Xiang and Bansal, Mohit}, 
journal = {EACL}, 
eprint = {2203.07281}, 
abstract = {{Providing natural language instructions in prompts is a useful new paradigm for improving task performance of large language models in a zero-shot setting. Recent work has aimed to improve such prompts via manual rewriting or gradient-based tuning. However, manual rewriting is time-consuming and requires subjective interpretation, while gradient-based tuning can be extremely computationally demanding for large models and requires full access to model weights, which may not be available for API-based models. In this work, we introduce Gradient-free Instructional Prompt Search (GrIPS), a gradient-free, edit-based search approach for improving task instructions for large language models. GrIPS takes in instructions designed for humans and automatically returns an improved, edited prompt, while allowing for API-based tuning. The instructions in our search are iteratively edited using four operations (delete, add, swap, paraphrase) on text at the phrase-level. With InstructGPT models, GrIPS improves the average task performance by up to 4.30 percentage points on eight classification tasks from the Natural-Instructions dataset. We see improvements for both instruction-only prompts and for k-shot example+instruction prompts. Notably, GrIPS outperforms manual rewriting following the guidelines in Mishra et al. (2022) and also outperforms purely example-based prompts while controlling for the available compute and data budget. Lastly, we provide qualitative analysis of the edited instructions across several scales of GPT models. Our code is available at: https://github.com/archiki/GrIPS}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Prasad-GrIPS-%20Gradient-free,%20Edit-based%20Instruction%20Search%20for%20Prompting%20Large%20Language%20Models-2023-EACL.pdf}
}
@article{MaskCLIPDingICML2023, 
year = {2023}, 
title = {{Open-Vocabulary Universal Image Segmentation with MaskCLIP}}, 
author = {Ding, Zheng and Wang, Jieke and Tu, Zhuowen}, 
journal = {ICML}, 
eprint = {2208.08984}, 
abstract = {{In this paper, we tackle an emerging computer vision task, open-vocabulary universal image segmentation, that aims to perform semantic/instance/panoptic segmentation (background semantic labeling + foreground instance segmentation) for arbitrary categories of text-based descriptions in inference time. We first build a baseline method by directly adopting pre-trained CLIP models without finetuning or distillation. We then develop MaskCLIP, a Transformer-based approach with a MaskCLIP Visual Encoder, which is an encoder-only module that seamlessly integrates mask tokens with a pre-trained ViT CLIP model for semantic/instance segmentation and class prediction. MaskCLIP learns to efficiently and effectively utilize pre-trained partial/dense CLIP features within the MaskCLIP Visual Encoder that avoids the time-consuming student-teacher training process. MaskCLIP outperforms previous methods for semantic/instance/panoptic segmentation on ADE20K and PASCAL datasets. We show qualitative illustrations for MaskCLIP with online custom categories. Project website: https://maskclip.github.io.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ding-Open-Vocabulary%20Universal%20Image%20Segmentation%20with%20MaskCLIP-2023-ICML.pdf}
}
@article{BLOOMScaoarXiv2022, 
year = {2022}, 
title = {{BLOOM: A 176B-Parameter Open-Access Multilingual Language Model}}, 
author = {Workshop, BigScience and : and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ilić, Suzana and Hesslow, Daniel and Castagné, Roman and Luccioni, Alexandra Sasha and Yvon, François and Gallé, Matthias and Tow, Jonathan and Rush, Alexander M and Biderman, Stella and Webson, Albert and Ammanamanchi, Pawan Sasanka and Wang, Thomas and Sagot, Benoît and Muennighoff, Niklas and Moral, Albert Villanova del and Ruwase, Olatunji and Bawden, Rachel and Bekman, Stas and McMillan-Major, Angelina and Beltagy, Iz and Nguyen, Huu and Saulnier, Lucile and Tan, Samson and Suarez, Pedro Ortiz and Sanh, Victor and Laurençon, Hugo and Jernite, Yacine and Launay, Julien and Mitchell, Margaret and Raffel, Colin and Gokaslan, Aaron and Simhi, Adi and Soroa, Aitor and Aji, Alham Fikri and Alfassy, Amit and Rogers, Anna and Nitzav, Ariel Kreisberg and Xu, Canwen and Mou, Chenghao and Emezue, Chris and Klamm, Christopher and Leong, Colin and Strien, Daniel van and Adelani, David Ifeoluwa and Radev, Dragomir and Ponferrada, Eduardo González and Levkovizh, Efrat and Kim, Ethan and Natan, Eyal Bar and Toni, Francesco De and Dupont, Gérard and Kruszewski, Germán and Pistilli, Giada and Elsahar, Hady and Benyamina, Hamza and Tran, Hieu and Yu, Ian and Abdulmumin, Idris and Johnson, Isaac and Gonzalez-Dios, Itziar and Rosa, Javier de la and Chim, Jenny and Dodge, Jesse and Zhu, Jian and Chang, Jonathan and Frohberg, Jörg and Tobing, Joseph and Bhattacharjee, Joydeep and Almubarak, Khalid and Chen, Kimbo and Lo, Kyle and Werra, Leandro Von and Weber, Leon and Phan, Long and allal, Loubna Ben and Tanguy, Ludovic and Dey, Manan and Muñoz, Manuel Romero and Masoud, Maraim and Grandury, María and Šaško, Mario and Huang, Max and Coavoux, Maximin and Singh, Mayank and Jiang, Mike Tian-Jian and Vu, Minh Chien and Jauhar, Mohammad A and Ghaleb, Mustafa and Subramani, Nishant and Kassner, Nora and Khamis, Nurulaqilla and Nguyen, Olivier and Espejel, Omar and Gibert, Ona de and Villegas, Paulo and Henderson, Peter and Colombo, Pierre and Amuok, Priscilla and Lhoest, Quentin and Harliman, Rheza and Bommasani, Rishi and López, Roberto Luis and Ribeiro, Rui and Osei, Salomey and Pyysalo, Sampo and Nagel, Sebastian and Bose, Shamik and Muhammad, Shamsuddeen Hassan and Sharma, Shanya and Longpre, Shayne and Nikpoor, Somaieh and Silberberg, Stanislav and Pai, Suhas and Zink, Sydney and Torrent, Tiago Timponi and Schick, Timo and Thrush, Tristan and Danchev, Valentin and Nikoulina, Vassilina and Laippala, Veronika and Lepercq, Violette and Prabhu, Vrinda and Alyafeai, Zaid and Talat, Zeerak and Raja, Arun and Heinzerling, Benjamin and Si, Chenglei and Taşar, Davut Emre and Salesky, Elizabeth and Mielke, Sabrina J and Lee, Wilson Y and Sharma, Abheesht and Santilli, Andrea and Chaffin, Antoine and Stiegler, Arnaud and Datta, Debajyoti and Szczechla, Eliza and Chhablani, Gunjan and Wang, Han and Pandey, Harshit and Strobelt, Hendrik and Fries, Jason Alan and Rozen, Jos and Gao, Leo and Sutawika, Lintang and Bari, M Saiful and Al-shaibani, Maged S and Manica, Matteo and Nayak, Nihal and Teehan, Ryan and Albanie, Samuel and Shen, Sheng and Ben-David, Srulik and Bach, Stephen H and Kim, Taewoon and Bers, Tali and Fevry, Thibault and Neeraj, Trishala and Thakker, Urmish and Raunak, Vikas and Tang, Xiangru and Yong, Zheng-Xin and Sun, Zhiqing and Brody, Shaked and Uri, Yallow and Tojarieh, Hadar and Roberts, Adam and Chung, Hyung Won and Tae, Jaesung and Phang, Jason and Press, Ofir and Li, Conglong and Narayanan, Deepak and Bourfoune, Hatim and Casper, Jared and Rasley, Jeff and Ryabinin, Max and Mishra, Mayank and Zhang, Minjia and Shoeybi, Mohammad and Peyrounette, Myriam and Patry, Nicolas and Tazi, Nouamane and Sanseviero, Omar and Platen, Patrick von and Cornette, Pierre and Lavallée, Pierre François and Lacroix, Rémi and Rajbhandari, Samyam and Gandhi, Sanchit and Smith, Shaden and Requena, Stéphane and Patil, Suraj and Dettmers, Tim and Baruwa, Ahmed and Singh, Amanpreet and Cheveleva, Anastasia and Ligozat, Anne-Laure and Subramonian, Arjun and Névéol, Aurélie and Lovering, Charles and Garrette, Dan and Tunuguntla, Deepak and Reiter, Ehud and Taktasheva, Ekaterina and Voloshina, Ekaterina and Bogdanov, Eli and Winata, Genta Indra and Schoelkopf, Hailey and Kalo, Jan-Christoph and Novikova, Jekaterina and Forde, Jessica Zosa and Clive, Jordan and Kasai, Jungo and Kawamura, Ken and Hazan, Liam and Carpuat, Marine and Clinciu, Miruna and Kim, Najoung and Cheng, Newton and Serikov, Oleg and Antverg, Omer and Wal, Oskar van der and Zhang, Rui and Zhang, Ruochen and Gehrmann, Sebastian and Mirkin, Shachar and Pais, Shani and Shavrina, Tatiana and Scialom, Thomas and Yun, Tian and Limisiewicz, Tomasz and Rieser, Verena and Protasov, Vitaly and Mikhailov, Vladislav and Pruksachatkun, Yada and Belinkov, Yonatan and Bamberger, Zachary and Kasner, Zdeněk and Rueda, Alice and Pestana, Amanda and Feizpour, Amir and Khan, Ammar and Faranak, Amy and Santos, Ana and Hevia, Anthony and Unldreaj, Antigona and Aghagol, Arash and Abdollahi, Arezoo and Tammour, Aycha and HajiHosseini, Azadeh and Behroozi, Bahareh and Ajibade, Benjamin and Saxena, Bharat and Ferrandis, Carlos Muñoz and McDuff, Daniel and Contractor, Danish and Lansky, David and David, Davis and Kiela, Douwe and Nguyen, Duong A and Tan, Edward and Baylor, Emi and Ozoani, Ezinwanne and Mirza, Fatima and Ononiwu, Frankline and Rezanejad, Habib and Jones, Hessie and Bhattacharya, Indrani and Solaiman, Irene and Sedenko, Irina and Nejadgholi, Isar and Passmore, Jesse and Seltzer, Josh and Sanz, Julio Bonis and Dutra, Livia and Samagaio, Mairon and Elbadri, Maraim and Mieskes, Margot and Gerchick, Marissa and Akinlolu, Martha and McKenna, Michael and Qiu, Mike and Ghauri, Muhammed and Burynok, Mykola and Abrar, Nafis and Rajani, Nazneen and Elkott, Nour and Fahmy, Nour and Samuel, Olanrewaju and An, Ran and Kromann, Rasmus and Hao, Ryan and Alizadeh, Samira and Shubber, Sarmad and Wang, Silas and Roy, Sourav and Viguier, Sylvain and Le, Thanh and Oyebade, Tobi and Le, Trieu and Yang, Yoyo and Nguyen, Zach and Kashyap, Abhinav Ramesh and Palasciano, Alfredo and Callahan, Alison and Shukla, Anima and Miranda-Escalada, Antonio and Singh, Ayush and Beilharz, Benjamin and Wang, Bo and Brito, Caio and Zhou, Chenxi and Jain, Chirag and Xu, Chuxin and Fourrier, Clémentine and Periñán, Daniel León and Molano, Daniel and Yu, Dian and Manjavacas, Enrique and Barth, Fabio and Fuhrimann, Florian and Altay, Gabriel and Bayrak, Giyaseddin and Burns, Gully and Vrabec, Helena U and Bello, Imane and Dash, Ishani and Kang, Jihyun and Giorgi, John and Golde, Jonas and Posada, Jose David and Sivaraman, Karthik Rangasai and Bulchandani, Lokesh and Liu, Lu and Shinzato, Luisa and Bykhovetz, Madeleine Hahn de and Takeuchi, Maiko and Pàmies, Marc and Castillo, Maria A and Nezhurina, Marianna and Sänger, Mario and Samwald, Matthias and Cullan, Michael and Weinberg, Michael and Wolf, Michiel De and Mihaljcic, Mina and Liu, Minna and Freidank, Moritz and Kang, Myungsun and Seelam, Natasha and Dahlberg, Nathan and Broad, Nicholas Michio and Muellner, Nikolaus and Fung, Pascale and Haller, Patrick and Chandrasekhar, Ramya and Eisenberg, Renata and Martin, Robert and Canalli, Rodrigo and Su, Rosaline and Su, Ruisi and Cahyawijaya, Samuel and Garda, Samuele and Deshmukh, Shlok S and Mishra, Shubhanshu and Kiblawi, Sid and Ott, Simon and Sang-aroonsiri, Sinee and Kumar, Srishti and Schweter, Stefan and Bharati, Sushil and Laud, Tanmay and Gigant, Théo and Kainuma, Tomoya and Kusa, Wojciech and Labrak, Yanis and Bajaj, Yash Shailesh and Venkatraman, Yash and Xu, Yifan and Xu, Yingxin and Xu, Yu and Tan, Zhe and Xie, Zhongli and Ye, Zifan and Bras, Mathilde and Belkada, Younes and Wolf, Thomas}, 
journal = {arXiv}, 
eprint = {2211.05100}, 
abstract = {{Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Workshop-BLOOM-%20A%20176B-Parameter%20Open-Access%20Multilingual%20Language%20Model-2022-arXiv.pdf}
}
@article{GPTQFrantarICLR2023, 
year = {2023}, 
title = {{GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers}}, 
author = {Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan}, 
journal = {ICLR}, 
eprint = {2210.17323}, 
abstract = {{Generative Pre-trained Transformer (GPT) models set themselves apart through breakthrough performance across complex language modelling tasks, but also by their extremely high computational and storage costs. Specifically, due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs to execute, which limits the usability of such models. While there is emerging work on relieving this pressure via model compression, the applicability and performance of existing compression techniques is limited by the scale and complexity of GPT models. In this paper, we address this challenge, and propose GPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3 or 4 bits per weight, with negligible accuracy degradation relative to the uncompressed baseline. Our method more than doubles the compression gains relative to previously-proposed one-shot quantization methods, preserving accuracy, allowing us for the first time to execute an 175 billion-parameter model inside a single GPU. We show experimentally that these improvements can be leveraged for end-to-end inference speedups over FP16, of around 2x when using high-end GPUs (NVIDIA A100) and 4x when using more cost-effective ones (NVIDIA A6000). The implementation is available at https://github.com/IST-DASLab/gptq.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Frantar-GPTQ-%20Accurate%20Post-Training%20Quantization%20for%20Generative%20Pre-trained%20Transformers-2023-ICLR.pdf}
}
@article{LargeKandpalICML2023, 
year = {2023}, 
title = {{Large Language Models Struggle to Learn Long-Tail Knowledge}}, 
author = {Kandpal, Nikhil and Deng, Haikang and Roberts, Adam and Wallace, Eric and Raffel, Colin}, 
journal = {ICML}, 
eprint = {2211.08411}, 
abstract = {{The internet contains a wealth of knowledge -- from the birthdays of historical figures to tutorials on how to code -- all of which may be learned by language models. However, there is a huge variability in the number of times a given piece of information appears on the web. In this paper, we study the relationship between the knowledge memorized by large language models and the information in their pre-training datasets. In particular, we show that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model sizes (e.g., 176B parameters). Moreover, we find that while larger models are better at learning long-tail knowledge, we estimate that today's models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data. Finally, we show that retrieval-augmentation can reduce the dependence on relevant document count, presenting a promising approach for capturing the long-tail.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kandpal-Large%20Language%20Models%20Struggle%20to%20Learn%20Long-Tail%20Knowledge-2023-ICML.pdf}
}
@article{LLM.int8DettmersNeurIPS2022, 
year = {2022}, 
title = {{LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale}}, 
author = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke}, 
journal = {NeurIPS}, 
eprint = {2208.07339}, 
abstract = {{Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9\% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our software.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Dettmers-LLM.int8()-%208-bit%20Matrix%20Multiplication%20for%20Transformers%20at%20Scale-2022-NeurIPS.pdf}
}
@article{SmoothQuantXiaoICML2023, 
year = {2023}, 
title = {{SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models}}, 
author = {Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song}, 
journal = {ICML}, 
eprint = {2211.10438}, 
abstract = {{Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, for LLMs beyond 100 billion parameters, existing methods cannot maintain accuracy or do not run efficiently on hardware. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs that can be implemented efficiently. We observe that systematic outliers appear at fixed activation channels. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the GEMMs in LLMs, including OPT-175B, BLOOM-176B, and GLM-130B. SmoothQuant has better hardware efficiency than existing techniques using mixed-precision activation quantization or weight-only quantization. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. Thanks to the hardware-friendly design, we integrate SmoothQuant into FasterTransformer, a state-of-the-art LLM serving framework, and achieve faster inference speed with half the number of GPUs compared to FP16. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs. Code is available at: https://github.com/mit-han-lab/smoothquant.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xiao-SmoothQuant-%20Accurate%20and%20Efficient%20Post-Training%20Quantization%20for%20Large%20Language%20Models-2023-ICML.pdf}
}
@article{HyperTuningPhangICML2023, 
year = {2023}, 
title = {{HyperTuning: Toward Adapting Large Language Models without Back-propagation}}, 
author = {Phang, Jason and Mao, Yi and He, Pengcheng and Chen, Weizhu}, 
journal = {ICML}, 
eprint = {2211.12485}, 
abstract = {{Fine-tuning large language models for different tasks can be costly and inefficient, and even methods that reduce the number of tuned parameters still require full gradient-based optimization. We propose HyperTuning, a novel approach to model adaptation that uses a hypermodel to generate task-specific parameters for a fixed downstream model. We demonstrate a simple setup for hypertuning with HyperT5, a T5-based hypermodel that produces soft prefixes or LoRA parameters for a frozen T5 model from few-shot examples. We train HyperT5 in two stages: first, hyperpretraining with a modified conditional language modeling objective that trains a hypermodel to generate parameters; second, multi-task fine-tuning (MTF) on a large number of diverse language tasks. We evaluate HyperT5 on P3, MetaICL and Super-NaturalInstructions datasets, and show that it can effectively generate parameters for unseen tasks. Moreover, we show that using hypermodel-generated parameters as initializations for further parameter-efficient fine-tuning improves performance. HyperTuning can thus be a flexible and efficient way to leverage large language models for diverse downstream applications.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Phang-HyperTuning-%20Toward%20Adapting%20Large%20Language%20Models%20without%20Back-propagation-2023-ICML.pdf}
}
@article{DISCOChenACL2023, 
year = {2023}, 
title = {{DISCO: Distilling Counterfactuals with Large Language Models}}, 
author = {Chen, Zeming and Gao, Qiyue and Bosselut, Antoine and Sabharwal, Ashish and Richardson, Kyle}, 
journal = {ACL}, 
eprint = {2212.10534}, 
abstract = {{Models trained with counterfactually augmented data learn representations of the causal structure of tasks, enabling robust generalization. However, high-quality counterfactual data is scarce for most tasks and not easily generated at scale. When crowdsourced, such data is typically limited in scale and diversity; when generated using supervised methods, it is computationally expensive to extend to new counterfactual dimensions. In this work, we introduce DISCO (DIStilled COunterfactual Data), a new method for automatically generating high quality counterfactual data at scale. DISCO engineers prompts to generate phrasal perturbations with a large general language model. Then, a task-specific teacher model filters these generations to distill high-quality counterfactual data. While task-agnostic, we apply our pipeline to the task of natural language inference (NLI) and find that on challenging evaluations such as the NLI stress test, comparatively smaller student models trained with DISCO generated counterfactuals are more robust (6\% absolute) and generalize better across distributions (2\%) compared to models trained without data augmentation. Furthermore, DISCO augmented models are 10\% more consistent between counterfactual pairs on three evaluation sets, demonstrating that DISCO augmentation enables models to more reliably learn causal representations. Our repository is available at: https://github.com/eric11eca/disco}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-DISCO-%20Distilling%20Counterfactuals%20with%20Large%20Language%20Models-2023-ACL.pdf}
}
@article{MULTIINSTRUCTXuACL2023, 
year = {2023}, 
title = {{MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning}}, 
author = {Xu, Zhiyang and Shen, Ying and Huang, Lifu}, 
journal = {ACL}, 
eprint = {2212.10773}, 
abstract = {{Instruction tuning, a new learning paradigm that fine-tunes pre-trained language models on tasks specified through instructions, has shown promising zero-shot performance on various natural language processing tasks. However, it's still not explored for vision and multimodal tasks. In this work, we introduce MultiInstruct, the first multimodal instruction tuning benchmark dataset that consists of 47 diverse multimodal tasks covering 11 broad categories. Each task is designed at least with 5,000 instances (input-out pairs) from existing open-source datasets and 5 expert-written instructions. We take OFA as the base pre-trained model for multimodal instruction tuning, and to improve its performance, we explore multiple transfer learning strategies to leverage the large-scale Natural Instructions dataset. Experimental results demonstrate its strong zero-shot performance on various unseen multimodal tasks and the benefit of transfer learning from text-only instructions. We also design a new evaluation metric: Sensitivity, to evaluate how sensitive the model is to the variety of instructions. Our results indicate that the model is less sensitive to the varying instructions after finetuning on a diverse set of tasks and instructions for each task.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xu-MultiInstruct-%20Improving%20Multi-Modal%20Zero-Shot%20Learning%20via%20Instruction%20Tuning-2023-ACL.pdf}
}
@article{WatermarkKirchenbauerICML2023, 
year = {2023}, 
title = {{A Watermark for Large Language Models}}, 
author = {Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom}, 
journal = {ICML}, 
eprint = {2301.10226}, 
abstract = {{Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of "green" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kirchenbauer-A%20Watermark%20for%20Large%20Language%20Models-2023-ICML.pdf}
}
@article{GSM-ICShiICML2023, 
year = {2023}, 
title = {{Large Language Models Can Be Easily Distracted by Irrelevant Context}}, 
author = {Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed and Schärli, Nathanael and Zhou, Denny}, 
journal = {ICML}, 
eprint = {2302.00093}, 
abstract = {{Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Shi-Large%20Language%20Models%20Can%20Be%20Easily%20Distracted%20by%20Irrelevant%20Context-2023-ICML.pdf}
}
@article{LearningKaushikICLR2020, 
year = {2020}, 
title = {{Learning the Difference that Makes a Difference with Counterfactually-Augmented Data}}, 
author = {Kaushik, Divyansh and Hovy, Eduard and Lipton, Zachary C}, 
journal = {ICLR}, 
eprint = {1909.12434}, 
abstract = {{Despite alarm over the reliance of machine learning systems on so-called spurious patterns, the term lacks coherent meaning in standard statistical frameworks. However, the language of causality offers clarity: spurious associations are due to confounding (e.g., a common cause), but not direct or indirect causal effects. In this paper, we focus on natural language processing, introducing methods and resources for training models less sensitive to spurious patterns. Given documents and their initial labels, we task humans with revising each document so that it (i) accords with a counterfactual target label; (ii) retains internal coherence; and (iii) avoids unnecessary changes. Interestingly, on sentiment analysis and natural language inference tasks, classifiers trained on original data fail on their counterfactually-revised counterparts and vice versa. Classifiers trained on combined datasets perform remarkably well, just shy of those specialized to either domain. While classifiers trained on either original or manipulated data alone are sensitive to spurious features (e.g., mentions of genre), models trained on the combined data are less sensitive to this signal. Both datasets are publicly available.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kaushik-Learning%20the%20Difference%20that%20Makes%20a%20Difference%20with%20Counterfactually-Augmented%20Data-2020-ICLR.pdf}
}
@article{FilterNormalizationLiNeurIPS2018, 
year = {2018}, 
title = {{Visualizing the Loss Landscape of Neural Nets}}, 
author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom}, 
journal = {NeurIPS}, 
eprint = {1712.09913}, 
abstract = {{Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Visualizing%20the%20Loss%20Landscape%20of%20Neural%20Nets-2018-NeurIPS.pdf}
}
@article{PROXYPROXWoodworthICML2023, 
year = {2023}, 
title = {{Two Losses Are Better Than One: Faster Optimization Using a Cheaper Proxy}}, 
author = {Woodworth, Blake and Mishchenko, Konstantin and Bach, Francis}, 
journal = {ICML}, 
eprint = {2302.03542}, 
abstract = {{We present an algorithm for minimizing an objective with hard-to-compute gradients by using a related, easier-to-access function as a proxy. Our algorithm is based on approximate proximal point iterations on the proxy combined with relatively few stochastic gradients from the objective. When the difference between the objective and the proxy is \$\textbackslashdelta\$-smooth, our algorithm guarantees convergence at a rate matching stochastic gradient descent on a \$\textbackslashdelta\$-smooth objective, which can lead to substantially better sample efficiency. Our algorithm has many potential applications in machine learning, and provides a principled means of leveraging synthetic data, physics simulators, mixed public and private data, and more.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Woodworth-Two%20Losses%20Are%20Better%20Than%20One-%20Faster%20Optimization%20Using%20a%20Cheaper%20Proxy-2023-ICML.pdf}
}
@article{FlexGenShengICML2023, 
year = {2023}, 
title = {{FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU}}, 
author = {Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Fu, Daniel Y and Xie, Zhiqiang and Chen, Beidi and Barrett, Clark and Gonzalez, Joseph E and Liang, Percy and Ré, Christopher and Stoica, Ion and Zhang, Ce}, 
journal = {ICML}, 
eprint = {2303.06865}, 
abstract = {{The high computational and memory requirements of large language model (LLM) inference make it feasible only with multiple high-end accelerators. Motivated by the emerging demand for latency-insensitive tasks with batched processing, this paper initiates the study of high-throughput LLM inference using limited resources, such as a single commodity GPU. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. By solving a linear programming problem, it searches for efficient patterns to store and access tensors. FlexGen further compresses the weights and the attention cache to 4 bits with negligible accuracy loss. These techniques enable FlexGen to have a larger space of batch size choices and thus significantly increase maximum throughput. As a result, when running OPT-175B on a single 16GB GPU, FlexGen achieves significantly higher throughput compared to state-of-the-art offloading systems, reaching a generation throughput of 1 token/s for the first time with an effective batch size of 144. On the HELM benchmark, FlexGen can benchmark a 30B model with a 16GB GPU on 7 representative sub-scenarios in 21 hours. The code is available at https://github.com/FMInference/FlexGen}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Sheng-FlexGen-%20High-Throughput%20Generative%20Inference%20of%20Large%20Language%20Models%20with%20a%20Single%20GPU-2023-ICML.pdf}
}
@article{EMP-SSLTongarXiv2023, 
year = {2023}, 
title = {{EMP-SSL: Towards Self-Supervised Learning in One Training Epoch}}, 
author = {Tong, Shengbang and Chen, Yubei and Ma, Yi and Lecun, Yann}, 
journal = {arXiv}, 
eprint = {2304.03977}, 
abstract = {{Recently, self-supervised learning (SSL) has achieved tremendous success in learning image representation. Despite the empirical success, most self-supervised learning methods are rather "inefficient" learners, typically taking hundreds of training epochs to fully converge. In this work, we show that the key towards efficient self-supervised learning is to increase the number of crops from each image instance. Leveraging one of the state-of-the-art SSL method, we introduce a simplistic form of self-supervised learning method called Extreme-Multi-Patch Self-Supervised-Learning (EMP-SSL) that does not rely on many heuristic techniques for SSL such as weight sharing between the branches, feature-wise normalization, output quantization, and stop gradient, etc, and reduces the training epochs by two orders of magnitude. We show that the proposed method is able to converge to 85.1\% on CIFAR-10, 58.5\% on CIFAR-100, 38.1\% on Tiny ImageNet and 58.5\% on ImageNet-100 in just one epoch. Furthermore, the proposed method achieves 91.5\% on CIFAR-10, 70.1\% on CIFAR-100, 51.5\% on Tiny ImageNet and 78.9\% on ImageNet-100 with linear probing in less than ten training epochs. In addition, we show that EMP-SSL shows significantly better transferability to out-of-domain datasets compared to baseline SSL methods. We will release the code in https://github.com/tsb0601/EMP-SSL.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Tong-EMP-SSL-%20Towards%20Self-Supervised%20Learning%20in%20One%20Training%20Epoch-2023-arXiv.pdf}
}
@article{Cross-EntropyLossMaoICML2023, 
year = {2023}, 
title = {{Cross-Entropy Loss Functions: Theoretical Analysis and Applications}}, 
author = {Mao, Anqi and Mohri, Mehryar and Zhong, Yutao}, 
journal = {ICML}, 
eprint = {2304.07288}, 
abstract = {{Cross-entropy is a widely used loss function in applications. It coincides with the logistic loss applied to the outputs of a neural network, when the softmax is used. But, what guarantees can we rely on when using cross-entropy as a surrogate loss? We present a theoretical analysis of a broad family of loss functions, comp-sum losses, that includes cross-entropy (or logistic loss), generalized cross-entropy, the mean absolute error and other cross-entropy-like loss functions. We give the first \$H\$-consistency bounds for these loss functions. These are non-asymptotic guarantees that upper bound the zero-one loss estimation error in terms of the estimation error of a surrogate loss, for the specific hypothesis set \$H\$ used. We further show that our bounds are tight. These bounds depend on quantities called minimizability gaps. To make them more explicit, we give a specific analysis of these gaps for comp-sum losses. We also introduce a new family of loss functions, smooth adversarial comp-sum losses, that are derived from their comp-sum counterparts by adding in a related smooth term. We show that these loss functions are beneficial in the adversarial setting by proving that they admit \$H\$-consistency bounds. This leads to new adversarial robustness algorithms that consist of minimizing a regularized smooth adversarial comp-sum loss. While our main purpose is a theoretical analysis, we also present an extensive empirical analysis comparing comp-sum losses. We further report the results of a series of experiments demonstrating that our adversarial robustness algorithms outperform the current state-of-the-art, while also achieving a superior non-adversarial accuracy.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Mao-Cross-Entropy%20Loss%20Functions-%20Theoretical%20Analysis%20and%20Applications-2023-ICML.pdf}
}
@article{SCOTTWangACL2023, 
year = {2023}, 
title = {{SCOTT: Self-Consistent Chain-of-Thought Distillation}}, 
author = {Wang, Peifeng and Wang, Zhengyang and Li, Zheng and Gao, Yifan and Yin, Bing and Ren, Xiang}, 
journal = {ACL}, 
eprint = {2305.01879}, 
abstract = {{Large language models (LMs) beyond a certain scale, demonstrate the emergent capability of generating free-text rationales for their predictions via chain-of-thought (CoT) prompting. While CoT can yield dramatically improved performance, such gains are only observed for sufficiently large LMs. Even more concerning, there is little guarantee that the generated rationales are consistent with LM's predictions or faithfully justify the decisions. In this work, we propose a faithful knowledge distillation method to learn a small, self-consistent CoT model from a teacher model that is orders of magnitude larger. To form better supervision, we elicit rationales supporting the gold answers from a large LM (teacher) by contrastive decoding, which encourages the teacher to generate tokens that become more plausible only when the answer is considered. To ensure faithful distillation, we use the teacher-generated rationales to learn a student LM with a counterfactual reasoning objective, which prevents the student from ignoring the rationales to make inconsistent predictions. Experiments show that, while yielding comparable end-task performance, our method can generate CoT rationales that are more faithful than baselines do. Further analysis suggests that such a model respects the rationales more when making decisions; thus, we can improve its performance more by refining its rationales.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-SCOTT-%20Self-Consistent%20Chain-of-Thought%20Distillation-2023-ACL.pdf}
}
@article{CanLLMsChiangACL2023, 
year = {2023}, 
title = {{Can Large Language Models Be an Alternative to Human Evaluations?}}, 
author = {Chiang, Cheng-Han and Lee, Hung-yi}, 
journal = {ACL}, 
eprint = {2305.01937}, 
abstract = {{Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms. Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided. In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation. We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation. We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: open-ended story generation and adversarial attacks. We show that the result of LLM evaluation is consistent with the results obtained by expert human evaluation: the texts rated higher by human experts are also rated higher by the LLMs. We also find that the results of LLM evaluation are stable over different formatting of the task instructions and the sampling algorithm used to generate the answer. We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chiang-Can%20Large%20Language%20Models%20Be%20an%20Alternative%20to%20Human%20Evaluations--2023-ACL.pdf}
}
@article{PSWangACL2023, 
year = {2023}, 
title = {{Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models}}, 
author = {Wang, Lei and Xu, Wanyu and Lan, Yihuai and Hu, Zhiqiang and Lan, Yunshi and Lee, Roy Ka-Wei and Lim, Ee-Peng}, 
journal = {ACL}, 
eprint = {2305.04091}, 
abstract = {{Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual effort, Zero-shot-CoT concatenates the target problem statement with "Let's think step by step" as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Plan-and-Solve%20Prompting-%20Improving%20Zero-Shot%20Chain-of-Thought%20Reasoning%20by%20Large%20Language%20Models-2023-ACL.pdf}
}
@article{CoScriptYuanACL2023, 
year = {2023}, 
title = {{Distilling Script Knowledge from Large Language Models for Constrained Language Planning}}, 
author = {Yuan, Siyu and Chen, Jiangjie and Fu, Ziquan and Ge, Xuyang and Shah, Soham and Jankowski, Charles Robert and Xiao, Yanghua and Yang, Deqing}, 
journal = {ACL}, 
eprint = {2305.05252}, 
abstract = {{In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., "make a cake"), but leaves more specific goals with multi-facet constraints understudied (e.g., "make a cake for diabetics"). In this paper, we define the task of constrained language planning for the first time. We propose an overgenerate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, CoScript, which consists of 55,000 scripts. Empirical results demonstrate that our method significantly improves the constrained language planning ability of LLMs, especially on constraint faithfulness. Furthermore, CoScript is demonstrated to be quite effective in endowing smaller LMs with constrained language planning ability.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yuan-Distilling%20Script%20Knowledge%20from%20Large%20Language%20Models%20for%20Constrained%20Language%20Planning-2023-ACL.pdf}
}
@article{FromPretrainingDataFengACL2023, 
year = {2023}, 
title = {{From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models}}, 
author = {Feng, Shangbin and Park, Chan Young and Liu, Yuhan and Tsvetkov, Yulia}, 
journal = {ACL}, 
eprint = {2305.08283}, 
abstract = {{Language models (LMs) are pretrained on diverse data sources, including news, discussion forums, books, and online encyclopedias. A significant portion of this data includes opinions and perspectives which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. Our work develops new methods to (1) measure political biases in LMs trained on such corpora, along social and economic axes, and (2) measure the fairness of downstream NLP models trained on top of politically biased LMs. We focus on hate speech and misinformation detection, aiming to empirically quantify the effects of political (social, economic) biases in pretraining data on the fairness of high-stakes social-oriented tasks. Our findings reveal that pretrained LMs do have political leanings that reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and misinformation detectors. We discuss the implications of our findings for NLP research and propose future directions to mitigate unfairness.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Feng-From%20Pretraining%20Data%20to%20Language%20Models%20to%20Downstream%20Tasks-%20Tracking%20the%20Trails%20of%20Political%20Biases%20Leading%20to%20Unfair%20NLP%20Models-2023-ACL.pdf}
}
@article{SumCoTWangACL2023, 
year = {2023}, 
title = {{Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method}}, 
author = {Wang, Yiming and Zhang, Zhuosheng and Wang, Rui}, 
journal = {ACL}, 
eprint = {2305.13412}, 
abstract = {{Automatic summarization generates concise summaries that contain key ideas of source documents. As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the "Lasswell Communication Model" proposed by Lasswell (1948), allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs' zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available at https://github.com/Alsace08/SumCoT.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Element-aware%20Summarization%20with%20Large%20Language%20Models-%20Expert-aligned%20Evaluation%20and%20Chain-of-Thought%20Method-2023-ACL.pdf}
}
@article{AWQLinarXiv2023, 
year = {2023}, 
title = {{AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration}}, 
author = {Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song}, 
journal = {arXiv}, 
eprint = {2306.00978}, 
abstract = {{Large language models (LLMs) have shown excellent performance on various tasks, but the astronomical model size raises the hardware barrier for serving (memory size) and slows down token generation (memory bandwidth). In this paper, we propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only quantization. Our method is based on the observation that weights are not equally important: protecting only 1\% of salient weights can greatly reduce quantization error. We then propose to search for the optimal per-channel scaling that protects the salient weights by observing the activation, not weights. AWQ does not rely on any backpropagation or reconstruction, so it can well preserve LLMs' generalization ability on different domains and modalities, without overfitting to the calibration set; it also does not rely on any data layout reordering, maintaining the hardware efficiency. AWQ outperforms existing work on various language modeling, common sense QA, and domain-specific benchmarks. Thanks to better generalization, it achieves excellent quantization performance for instruction-tuned LMs and, for the first time, multi-modal LMs. We also implement efficient tensor core kernels with reorder-free online dequantization to accelerate AWQ, achieving a 1.45x speedup over GPTQ and is 1.85x faster than the cuBLAS FP16 implementation. Our method provides a turn-key solution to compress LLMs to 3/4 bits for efficient deployment.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Lin-AWQ-%20Activation-aware%20Weight%20Quantization%20for%20LLM%20Compression%20and%20Acceleration-2023-arXiv.pdf}
}
@article{MatchingPairsFoleyACL2023, 
year = {2023}, 
title = {{Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models}}, 
author = {Foley, Myles and Rawat, Ambrish and Lee, Taesung and Hou, Yufang and Picco, Gabriele and Zizzo, Giulio}, 
journal = {ACL}, 
eprint = {2306.09308}, 
abstract = {{The wide applicability and adaptability of generative large language models (LLMs) has enabled their rapid adoption. While the pre-trained models can perform many tasks, such models are often fine-tuned to improve their performance on various downstream applications. However, this leads to issues over violation of model licenses, model theft, and copyright infringement. Moreover, recent advances show that generative technology is capable of producing harmful content which exacerbates the problems of accountability within model supply chains. Thus, we need a method to investigate how a model was trained or a piece of text was generated and what their pre-trained base model was. In this paper we take the first step to address this open problem by tracing back the origin of a given fine-tuned LLM to its corresponding pre-trained base model. We consider different knowledge levels and attribution strategies, and find that we can correctly trace back 8 out of the 10 fine tuned models with our best method.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Foley-Matching%20Pairs-%20Attributing%20Fine-Tuned%20Models%20to%20their%20Pre-Trained%20Large%20Language%20Models-2023-ACL.pdf}
}
@article{FastSAMZhaoarXiv2023, 
year = {2023}, 
title = {{Fast Segment Anything}}, 
author = {Zhao, Xu and Ding, Wenchao and An, Yongqi and Du, Yinglong and Yu, Tao and Li, Min and Tang, Ming and Wang, Jinqiao}, 
journal = {arXiv}, 
eprint = {2306.12156}, 
abstract = {{The recently proposed segment anything model (SAM) has made a significant influence in many computer vision tasks. It is becoming a foundation step for many high-level tasks, like image segmentation, image caption, and image editing. However, its huge computation costs prevent it from wider applications in industry scenarios. The computation mainly comes from the Transformer architecture at high-resolution inputs. In this paper, we propose a speed-up alternative method for this fundamental task with comparable performance. By reformulating the task as segments-generation and prompting, we find that a regular CNN detector with an instance segmentation branch can also accomplish this task well. Specifically, we convert this task to the well-studied instance segmentation task and directly train the existing instance segmentation method using only 1/50 of the SA-1B dataset published by SAM authors. With our method, we achieve a comparable performance with the SAM method at 50 times higher run-time speed. We give sufficient experimental results to demonstrate its effectiveness. The codes and demos will be released at https://github.com/CASIA-IVA-Lab/FastSAM.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhao-Fast%20Segment%20Anything-2023-arXiv.pdf}
}
@article{SRe2LYinarXiv2023, 
year = {2023}, 
title = {{Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective}}, 
author = {Yin, Zeyuan and Xing, Eric and Shen, Zhiqiang}, 
journal = {arXiv}, 
eprint = {2306.13092}, 
abstract = {{We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe\$\textasciicircum2\$L) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for effective dataset condensation. The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution training, and the ability to scale up to arbitrary evaluation network architectures. Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets. Under 50 IPC, our approach achieves the highest 42.5\% and 60.8\% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5\% and 32.9\%, respectively. Our approach also outperforms MTT by approximately 52\$\textbackslashtimes\$ (ConvNet-4) and 16\$\textbackslashtimes\$ (ResNet-18) faster in speed with less memory consumption of 11.6\$\textbackslashtimes\$ and 6.4\$\textbackslashtimes\$ during data synthesis. Our code and condensed datasets of 50, 200 IPC with 4K recovery budget are available at https://zeyuanyin.github.io/projects/SRe2L/.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yin-Squeeze,%20Recover%20and%20Relabel-%20Dataset%20Condensation%20at%20ImageNet%20Scale%20From%20A%20New%20Perspective-2023-arXiv.pdf}
}
@article{MobileSAMZhangarXiv2023, 
year = {2023}, 
title = {{Faster Segment Anything: Towards Lightweight SAM for Mobile Applications}}, 
author = {Zhang, Chaoning and Han, Dongshen and Qiao, Yu and Kim, Jung Uk and Bae, Sung-Ho and Lee, Seungkyu and Hong, Choong Seon}, 
journal = {arXiv}, 
eprint = {2306.14289}, 
abstract = {{Segment Anything Model (SAM) has attracted significant attention due to its impressive zero-shot transfer performance and high versatility for numerous vision applications (like image editing with fine-grained control). Many of such applications need to be run on resource-constraint edge devices, like mobile phones. In this work, we aim to make SAM mobile-friendly by replacing the heavyweight image encoder with a lightweight one. A naive way to train such a new SAM as in the original SAM paper leads to unsatisfactory performance, especially when limited training sources are available. We find that this is mainly caused by the coupled optimization of the image encoder and mask decoder, motivated by which we propose decoupled distillation. Concretely, we distill the knowledge from the heavy image encoder (ViT-H in the original SAM) to a lightweight image encoder, which can be automatically compatible with the mask decoder in the original SAM. The training can be completed on a single GPU within less than one day, and the resulting lightweight SAM is termed MobileSAM which is more than 60 times smaller yet performs on par with the original SAM. For inference speed, With a single GPU, MobileSAM runs around 10ms per image: 8ms on the image encoder and 4ms on the mask decoder. With superior performance, our MobileSAM is around 5 times faster than the concurrent FastSAM and 7 times smaller, making it more suitable for mobile applications. Moreover, we show that MobileSAM can run relatively smoothly on CPU. The code for our project is provided at \textbackslashhref\{https://github.com/ChaoningZhang/MobileSAM\}\{\textbackslashtextcolor\{red\}\{MobileSAM\}\}), with a demo showing that MobileSAM can run relatively smoothly on CPU.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Faster%20Segment%20Anything-%20Towards%20Lightweight%20SAM%20for%20Mobile%20Applications-2023-arXiv.pdf}
}
@article{VQ-VAEOordNeurIPS2017, 
year = {2017}, 
title = {{Neural Discrete Representation Learning}}, 
author = {Oord, Aaron van den and Vinyals, Oriol and Kavukcuoglu, Koray}, 
journal = {NeurIPS}, 
eprint = {1711.00937}, 
abstract = {{Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Oord-Neural%20Discrete%20Representation%20Learning-2017-NeurIPS.pdf}
}
@article{ChatLawCuiarXiv2023, 
year = {2023}, 
title = {{ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases}}, 
author = {Cui, Jiaxi and Li, Zongjian and Yan, Yang and Chen, Bohua and Yuan, Li}, 
journal = {arXiv}, 
eprint = {2306.16092}, 
abstract = {{Large Language Models (LLMs) have shown the potential to revolutionize natural language processing tasks in various domains, sparking great interest in vertical-specific large models. However, unlike proprietary models such as BloombergGPT and FinGPT, which have leveraged their unique data accumulations to make strides in the finance domain, there hasn't not many similar large language models in the Chinese legal domain to facilitate its digital transformation. In this paper, we propose an open-source legal large language model named ChatLaw. Due to the importance of data quality, we carefully designed a legal domain fine-tuning dataset. Additionally, to overcome the problem of model hallucinations in legal data screening during reference data retrieval, we introduce a method that combines vector database retrieval with keyword retrieval to effectively reduce the inaccuracy of relying solely on vector database retrieval. Furthermore, we propose a self-attention method to enhance the ability of large models to overcome errors present in reference data, further optimizing the issue of model hallucinations at the model level and improving the problem-solving capabilities of large models. We also open-sourced our model and part of the data at https://github.com/PKU-YuanGroup/ChatLaw.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Cui-ChatLaw-%20Open-Source%20Legal%20Large%20Language%20Model%20with%20Integrated%20External%20Knowledge%20Bases-2023-arXiv.pdf}
}
@article{MIS-FMWangarXiv2023, 
year = {2023}, 
title = {{MIS-FM: 3D Medical Image Segmentation using Foundation Models Pretrained on a Large-Scale Unannotated Dataset}}, 
author = {Wang, Guotai and Wu, Jianghao and Luo, Xiangde and Liu, Xinglong and Li, Kang and Zhang, Shaoting}, 
journal = {arXiv}, 
eprint = {2306.16925}, 
abstract = {{Pretraining with large-scale 3D volumes has a potential for improving the segmentation performance on a target medical image dataset where the training images and annotations are limited. Due to the high cost of acquiring pixel-level segmentation annotations on the large-scale pretraining dataset, pretraining with unannotated images is highly desirable. In this work, we propose a novel self-supervised learning strategy named Volume Fusion (VF) for pretraining 3D segmentation models. It fuses several random patches from a foreground sub-volume to a background sub-volume based on a predefined set of discrete fusion coefficients, and forces the model to predict the fusion coefficient of each voxel, which is formulated as a self-supervised segmentation task without manual annotations. Additionally, we propose a novel network architecture based on parallel convolution and transformer blocks that is suitable to be transferred to different downstream segmentation tasks with various scales of organs and lesions. The proposed model was pretrained with 110k unannotated 3D CT volumes, and experiments with different downstream segmentation targets including head and neck organs, thoracic/abdominal organs showed that our pretrained model largely outperformed training from scratch and several state-of-the-art self-supervised training methods and segmentation models. The code and pretrained model are available at https://github.com/openmedlab/MIS-FM.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-MIS-FM-%203D%20Medical%20Image%20Segmentation%20using%20Foundation%20Models%20Pretrained%20on%20a%20Large-Scale%20Unannotated%20Dataset-2023-arXiv.pdf}
}
@article{DreamDiffusionBaiarXiv2023, 
year = {2023}, 
title = {{DreamDiffusion: Generating High-Quality Images from Brain EEG Signals}}, 
author = {Bai, Yunpeng and Wang, Xintao and Cao, Yan-pei and Ge, Yixiao and Yuan, Chun and Shan, Ying}, 
journal = {arXiv}, 
eprint = {2306.16934}, 
abstract = {{This paper introduces DreamDiffusion, a novel method for generating high-quality images directly from brain electroencephalogram (EEG) signals, without the need to translate thoughts into text. DreamDiffusion leverages pre-trained text-to-image models and employs temporal masked signal modeling to pre-train the EEG encoder for effective and robust EEG representations. Additionally, the method further leverages the CLIP image encoder to provide extra supervision to better align EEG, text, and image embeddings with limited EEG-image pairs. Overall, the proposed method overcomes the challenges of using EEG signals for image generation, such as noise, limited information, and individual differences, and achieves promising results. Quantitative and qualitative results demonstrate the effectiveness of the proposed method as a significant step towards portable and low-cost ``thoughts-to-image'', with potential applications in neuroscience and computer vision. The code is available here \textbackslashurl\{https://github.com/bbaaii/DreamDiffusion\}.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bai-DreamDiffusion-%20Generating%20High-Quality%20Images%20from%20Brain%20EEG%20Signals-2023-arXiv.pdf}
}
@article{SPAEYuarXiv2023, 
year = {2023}, 
title = {{SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs}}, 
author = {Yu, Lijun and Cheng, Yong and Wang, Zhiruo and Kumar, Vivek and Macherey, Wolfgang and Huang, Yanping and Ross, David A and Essa, Irfan and Bisk, Yonatan and Yang, Ming-Hsuan and Murphy, Kevin and Hauptmann, Alexander G and Jiang, Lu}, 
journal = {arXiv}, 
eprint = {2306.17842}, 
abstract = {{In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25\%.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yu-SPAE-%20Semantic%20Pyramid%20AutoEncoder%20for%20Multimodal%20Generation%20with%20Frozen%20LLMs-2023-arXiv.pdf}
}
@article{KD-CoTWangarXiv2023, 
year = {2023}, 
title = {{Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering}}, 
author = {Wang, Keheng and Duan, Feiyu and Wang, Sirui and Li, Peiguang and Xian, Yunsen and Yin, Chuantao and Rong, Wenge and Xiong, Zhang}, 
journal = {arXiv}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Knowledge-Driven%20CoT-%20Exploring%20Faithful%20Reasoning%20in%20LLMs%20for%20Knowledge-intensive%20Question%20Answering-2023-arXiv.pdf}
}
@article{LDIFSMukhotiarXiv2023, 
year = {2023}, 
title = {{Fine-tuning can cripple your foundation model; preserving features may be the solution}}, 
author = {Mukhoti, Jishnu and Gal, Yarin and Torr, Philip H. S. and Dokania, Puneet K.}, 
journal = {arXiv}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Mukhoti-Fine-tuning%20can%20cripple%20your%20foundation%20model;%20preserving%20features%20may%20be%20the%20solution-2023-arXiv.pdf}
}
@article{SegPromptZhuICCV2023, 
year = {2023}, 
title = {{SegPrompt: Boosting Open-world Segmentation via Category-level Prompt Learning}}, 
author = {Zhu, Muzhi and Li, Hengtao and Chen, Hao and Fan, Chengxiang and Mao, Weian and Jing, Chenchen and Liu, Yifan and Shen, Chunhua}, 
journal = {ICCV}, 
eprint = {2308.06531}, 
abstract = {{Current closed-set instance segmentation models rely on pre-defined class labels for each mask during training and evaluation, largely limiting their ability to detect novel objects. Open-world instance segmentation (OWIS) models address this challenge by detecting unknown objects in a class-agnostic manner. However, previous OWIS approaches completely erase category information during training to keep the model's ability to generalize to unknown objects. In this work, we propose a novel training mechanism termed SegPrompt that uses category information to improve the model's class-agnostic segmentation ability for both known and unknown categories. In addition, the previous OWIS training setting exposes the unknown classes to the training set and brings information leakage, which is unreasonable in the real world. Therefore, we provide a new open-world benchmark closer to a real-world scenario by dividing the dataset classes into known-seen-unseen parts. For the first time, we focus on the model's ability to discover objects that never appear in the training set images. Experiments show that SegPrompt can improve the overall and unseen detection performance by 5.6\% and 6.1\% in AR on our new benchmark without affecting the inference efficiency. We further demonstrate the effectiveness of our method on existing cross-dataset transfer and strongly supervised settings, leading to 5.5\% and 12.3\% relative improvement.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhu-SegPrompt-%20Boosting%20Open-world%20Segmentation%20via%20Category-level%20Prompt%20Learning-2023-ICCV.pdf}
}
@article{LONGNETDingarXiv2023, 
year = {2023}, 
title = {{LongNet: Scaling Transformers to 1,000,000,000 Tokens}}, 
author = {Ding, Jiayu and Ma, Shuming and Dong, Li and Zhang, Xingxing and Huang, Shaohan and Wang, Wenhui and Zheng, Nanning and Wei, Furu}, 
journal = {arXiv}, 
eprint = {2307.02486}, 
abstract = {{Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. To address this issue, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between any two tokens in a sequence; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LongNet yields strong performance on both long-sequence modeling and general language tasks. Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ding-LongNet-%20Scaling%20Transformers%20to%201,000,000,000%20Tokens-2023-arXiv.pdf}
}
@article{SignaTR6KGholamianICCV2023, 
year = {2023}, 
title = {{Handwritten and Printed Text Segmentation: A Signature Case Study}}, 
author = {Gholamian, Sina and Vahdat, Ali}, 
journal = {ICCV}, 
eprint = {2307.07887}, 
abstract = {{While analyzing scanned documents, handwritten text can overlay printed text. This causes difficulties during the optical character recognition (OCR) and digitization process of documents, and subsequently, hurts downstream NLP tasks. Prior research either focuses only on the binary classification of handwritten text, or performs a three-class segmentation of the document, i.e., recognition of handwritten, printed, and background pixels. This results in the assignment of the handwritten and printed overlapping pixels to only one of the classes, and thus, they are not accounted for in the other class. Thus, in this research, we develop novel approaches for addressing the challenges of handwritten and printed text segmentation with the goal of recovering text in different classes in whole, especially improving the segmentation performance on the overlapping parts. As such, to facilitate with this task, we introduce a new dataset, SignaTR6K, collected from real legal documents, as well as a new model architecture for handwritten and printed text segmentation task. Our best configuration outperforms the prior work on two different datasets by 17.9\% and 7.3\% on IoU scores.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Gholamian-Handwritten%20and%20Printed%20Text%20Segmentation-%20A%20Signature%20Case%20Study-2023-ICCV.pdf}
}
@article{SAM-Med2DChengarXiv2023, 
year = {2023}, 
title = {{SAM-Med2D}}, 
author = {Cheng, Junlong and Ye, Jin and Deng, Zhongying and Chen, Jianpin and Li, Tianbin and Wang, Haoyu and Su, Yanzhou and Huang, Ziyan and Chen, Jilong and Jiang, Lei and Sun, Hui and He, Junjun and Zhang, Shaoting and Zhu, Min and Qiao, Yu}, 
journal = {arXiv}, 
eprint = {2308.16184}, 
abstract = {{The Segment Anything Model (SAM) represents a state-of-the-art research advancement in natural image segmentation, achieving impressive results with input prompts such as points and bounding boxes. However, our evaluation and recent research indicate that directly applying the pretrained SAM to medical image segmentation does not yield satisfactory performance. This limitation primarily arises from significant domain gap between natural images and medical images. To bridge this gap, we introduce SAM-Med2D, the most comprehensive studies on applying SAM to medical 2D images. Specifically, we first collect and curate approximately 4.6M images and 19.7M masks from public and private datasets, constructing a large-scale medical image segmentation dataset encompassing various modalities and objects. Then, we comprehensively fine-tune SAM on this dataset and turn it into SAM-Med2D. Unlike previous methods that only adopt bounding box or point prompts as interactive segmentation approach, we adapt SAM to medical image segmentation through more comprehensive prompts involving bounding boxes, points, and masks. We additionally fine-tune the encoder and decoder of the original SAM to obtain a well-performed SAM-Med2D, leading to the most comprehensive fine-tuning strategies to date. Finally, we conducted a comprehensive evaluation and analysis to investigate the performance of SAM-Med2D in medical image segmentation across various modalities, anatomical structures, and organs. Concurrently, we validated the generalization capability of SAM-Med2D on 9 datasets from MICCAI 2023 challenge. Overall, our approach demonstrated significantly superior performance and generalization capability compared to SAM.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Cheng-SAM-Med2D-2023-arXiv.pdf}
}
@article{ShatterandGatherKimICCV2023, 
year = {2023}, 
title = {{Shatter and Gather: Learning Referring Image Segmentation with Text Supervision}}, 
author = {Kim, Dongwon and Kim, Namyup and Lan, Cuiling and Kwak, Suha}, 
journal = {ICCV}, 
eprint = {2308.15512}, 
abstract = {{Referring image segmentation, the task of segmenting any arbitrary entities described in free-form texts, opens up a variety of vision applications. However, manual labeling of training data for this task is prohibitively costly, leading to lack of labeled data for training. We address this issue by a weakly supervised learning approach using text descriptions of training images as the only source of supervision. To this end, we first present a new model that discovers semantic entities in input image and then combines such entities relevant to text query to predict the mask of the referent. We also present a new loss function that allows the model to be trained without any further supervision. Our method was evaluated on four public benchmarks for referring image segmentation, where it clearly outperformed the existing method for the same task and recent open-vocabulary segmentation models on all the benchmarks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kim-Shatter%20and%20Gather-%20Learning%20Referring%20Image%20Segmentation%20with%20Text%20Supervision-2023-ICCV.pdf}
}
@article{BWCRKaraniMICCAI2023, 
year = {2023}, 
title = {{Boundary-weighted logit consistency improves calibration of segmentation networks}}, 
author = {Karani, Neerav and Dey, Neel and Golland, Polina}, 
journal = {MICCAI}, 
eprint = {2307.08163}, 
abstract = {{Neural network prediction probabilities and accuracy are often only weakly-correlated. Inherent label ambiguity in training data for image segmentation aggravates such miscalibration. We show that logit consistency across stochastic transformations acts as a spatially varying regularizer that prevents overconfident predictions at pixels with ambiguous labels. Our boundary-weighted extension of this regularizer provides state-of-the-art calibration for prostate and heart MRI segmentation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Karani-Boundary-weighted%20logit%20consistency%20improves%20calibration%20of%20segmentation%20networks-2023-MICCAI.pdf}
}
@article{DOTZhaoICCV2023, 
year = {2023}, 
title = {{DOT: A Distillation-Oriented Trainer}}, 
author = {Zhao, Borui and Cui, Quan and Song, Renjie and Liang, Jiajun}, 
journal = {ICCV}, 
eprint = {2307.08436}, 
abstract = {{Knowledge distillation transfers knowledge from a large model to a small one via task and distillation losses. In this paper, we observe a trade-off between task and distillation losses, i.e., introducing distillation loss limits the convergence of task loss. We believe that the trade-off results from the insufficient optimization of distillation loss. The reason is: The teacher has a lower task loss than the student, and a lower distillation loss drives the student more similar to the teacher, then a better-converged task loss could be obtained. To break the trade-off, we propose the Distillation-Oriented Trainer (DOT). DOT separately considers gradients of task and distillation losses, then applies a larger momentum to distillation loss to accelerate its optimization. We empirically prove that DOT breaks the trade-off, i.e., both losses are sufficiently optimized. Extensive experiments validate the superiority of DOT. Notably, DOT achieves a +2.59\% accuracy improvement on ImageNet-1k for the ResNet50-MobileNetV1 pair. Conclusively, DOT greatly benefits the student's optimization properties in terms of loss convergence and model generalization. Code will be made publicly available.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhao-DOT-%20A%20Distillation-Oriented%20Trainer-2023-ICCV.pdf}
}
@article{DTPLiICCV2023, 
year = {2023}, 
title = {{Differentiable Transportation Pruning}}, 
author = {Li, Yunqiang and Gemert, Jan C van and Hoefler, Torsten and Moons, Bert and Eleftheriou, Evangelos and Verhoef, Bram-Ernst}, 
journal = {ICCV}, 
eprint = {2307.08483}, 
abstract = {{Deep learning algorithms are increasingly employed at the edge. However, edge devices are resource constrained and thus require efficient deployment of deep neural networks. Pruning methods are a key tool for edge deployment as they can improve storage, compute, memory bandwidth, and energy usage. In this paper we propose a novel accurate pruning technique that allows precise control over the output network size. Our method uses an efficient optimal transportation scheme which we make end-to-end differentiable and which automatically tunes the exploration-exploitation behavior of the algorithm to find accurate sparse sub-networks. We show that our method achieves state-of-the-art performance compared to previous pruning methods on 3 different datasets, using 5 different models, across a wide range of pruning ratios, and with two types of sparsity budgets and pruning granularities.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-Differentiable%20Transportation%20Pruning-2023-ICCV.pdf}
}
@article{CSKDZhaoICCV2023, 
year = {2023}, 
title = {{Cumulative Spatial Knowledge Distillation for Vision Transformers}}, 
author = {Zhao, Borui and Song, Renjie and Liang, Jiajun}, 
journal = {ICCV}, 
eprint = {2307.08500}, 
abstract = {{Distilling knowledge from convolutional neural networks (CNNs) is a double-edged sword for vision transformers (ViTs). It boosts the performance since the image-friendly local-inductive bias of CNN helps ViT learn faster and better, but leading to two problems: (1) Network designs of CNN and ViT are completely different, which leads to different semantic levels of intermediate features, making spatial-wise knowledge transfer methods (e.g., feature mimicking) inefficient. (2) Distilling knowledge from CNN limits the network convergence in the later training period since ViT's capability of integrating global information is suppressed by CNN's local-inductive-bias supervision. To this end, we present Cumulative Spatial Knowledge Distillation (CSKD). CSKD distills spatial-wise knowledge to all patch tokens of ViT from the corresponding spatial responses of CNN, without introducing intermediate features. Furthermore, CSKD exploits a Cumulative Knowledge Fusion (CKF) module, which introduces the global response of CNN and increasingly emphasizes its importance during the training. Applying CKF leverages CNN's local inductive bias in the early training period and gives full play to ViT's global capability in the later one. Extensive experiments and analysis on ImageNet-1k and downstream datasets demonstrate the superiority of our CSKD. Code will be publicly available.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhao-Cumulative%20Spatial%20Knowledge%20Distillation%20for%20Vision%20Transformers-2023-ICCV.pdf}
}
@article{RetNetSunarXiv2023, 
year = {2023}, 
title = {{Retentive Network: A Successor to Transformer for Large Language Models}}, 
author = {Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu}, 
journal = {arXiv}, 
eprint = {2307.08621}, 
abstract = {{In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost \$O(1)\$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Sun-Retentive%20Network-%20A%20Successor%20to%20Transformer%20for%20Large%20Language%20Models-2023-arXiv.pdf}
}
@article{DTPWeiICCV2023, 
year = {2023}, 
title = {{Disentangle then Parse:Night-time Semantic Segmentation with Illumination Disentanglement}}, 
author = {Wei, Zhixiang and Chen, Lin and Tu, Tao and Chen, Huaian and Ling, Pengyang and Jin, Yi}, 
journal = {ICCV}, 
eprint = {2307.09362}, 
abstract = {{Most prior semantic segmentation methods have been developed for day-time scenes, while typically underperforming in night-time scenes due to insufficient and complicated lighting conditions. In this work, we tackle this challenge by proposing a novel night-time semantic segmentation paradigm, i.e., disentangle then parse (DTP). DTP explicitly disentangles night-time images into light-invariant reflectance and light-specific illumination components and then recognizes semantics based on their adaptive fusion. Concretely, the proposed DTP comprises two key components: 1) Instead of processing lighting-entangled features as in prior works, our Semantic-Oriented Disentanglement (SOD) framework enables the extraction of reflectance component without being impeded by lighting, allowing the network to consistently recognize the semantics under cover of varying and complicated lighting conditions. 2) Based on the observation that the illumination component can serve as a cue for some semantically confused regions, we further introduce an Illumination-Aware Parser (IAParser) to explicitly learn the correlation between semantics and lighting, and aggregate the illumination features to yield more precise predictions. Extensive experiments on the night-time segmentation task with various settings demonstrate that DTP significantly outperforms state-of-the-art methods. Furthermore, with negligible additional parameters, DTP can be directly used to benefit existing day-time methods for night-time segmentation.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wei-Disentangle%20then%20Parse-Night-time%20Semantic%20Segmentation%20with%20Illumination%20Disentanglement-2023-ICCV.pdf}
}
@article{TinyMIMRenCVPR2023, 
year = {2023}, 
title = {{TinyMIM: An Empirical Study of Distilling MIM Pre-trained Models}}, 
author = {Ren, Sucheng and Wei, Fangyun and Zhang, Zheng and Hu, Han}, 
journal = {CVPR}, 
eprint = {2301.01296}, 
abstract = {{Masked image modeling (MIM) performs strongly in pre-training large vision Transformers (ViTs). However, small models that are critical for real-world applications cannot or only marginally benefit from this pre-training approach. In this paper, we explore distillation techniques to transfer the success of large MIM-based pre-trained models to smaller ones. We systematically study different options in the distillation framework, including distilling targets, losses, input, network regularization, sequential distillation, etc, revealing that: 1) Distilling token relations is more effective than CLS token- and feature-based distillation; 2) An intermediate layer of the teacher network as target perform better than that using the last layer when the depth of the student mismatches that of the teacher; 3) Weak regularization is preferred; etc. With these findings, we achieve significant fine-tuning accuracy improvements over the scratch MIM pre-training on ImageNet-1K classification, using all the ViT-Tiny, ViT-Small, and ViT-base models, with +4.2\%/+2.4\%/+1.4\% gains, respectively. Our TinyMIM model of base size achieves 52.2 mIoU in AE20K semantic segmentation, which is +4.1 higher than the MAE baseline. Our TinyMIM model of tiny size achieves 79.6\% top-1 accuracy on ImageNet-1K image classification, which sets a new record for small vision models of the same size and computation budget. This strong performance suggests an alternative way for developing small vision Transformer models, that is, by exploring better training methods rather than introducing inductive biases into architectures as in most previous works. Code is available at https://github.com/OliverRensu/TinyMIM.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Ren-TinyMIM-%20An%20Empirical%20Study%20of%20Distilling%20MIM%20Pre-trained%20Models-2023-CVPR.pdf}
}
@article{SpeculativeDecodingLeviathanICML2023, 
year = {2023}, 
title = {{Fast Inference from Transformers via Speculative Decoding}}, 
author = {Leviathan, Yaniv and Kalman, Matan and Matias, Yossi}, 
journal = {ICML}, 
eprint = {2211.17192}, 
abstract = {{Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. In this work we introduce speculative decoding - an algorithm to sample from autoregressive models faster without any changes to the outputs, by computing several tokens in parallel. At the heart of our approach lie the observations that (1) hard language-modeling tasks often include easier subtasks that can be approximated well by more efficient models, and (2) using speculative execution and a novel sampling method, we can make exact decoding from the large models faster, by running them in parallel on the outputs of the approximation models, potentially generating several tokens concurrently, and without changing the distribution. Our method supports existing off-the-shelf models without retraining or architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration compared to the standard T5X implementation, with identical outputs.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Leviathan-Fast%20Inference%20from%20Transformers%20via%20Speculative%20Decoding-2023-ICML.pdf}
}
@article{SpeculativeSamplingChenarXiv2023, 
year = {2023}, 
title = {{Accelerating Large Language Model Decoding with Speculative Sampling}}, 
author = {Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper, John}, 
journal = {arXiv}, 
eprint = {2302.01318}, 
abstract = {{We present speculative sampling, an algorithm for accelerating transformer decoding by enabling the generation of multiple tokens from each transformer call. Our algorithm relies on the observation that the latency of parallel scoring of short continuations, generated by a faster but less powerful draft model, is comparable to that of sampling a single token from the larger target model. This is combined with a novel modified rejection sampling scheme which preserves the distribution of the target model within hardware numerics. We benchmark speculative sampling with Chinchilla, a 70 billion parameter language model, achieving a 2-2.5x decoding speedup in a distributed setup, without compromising the sample quality or making modifications to the model itself.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Accelerating%20Large%20Language%20Model%20Decoding%20with%20Speculative%20Sampling-2023-arXiv.pdf}
}
@article{CCDGuanICCV2023, 
year = {2023}, 
title = {{Self-supervised Character-to-Character Distillation for Text Recognition}}, 
author = {Guan, Tongkun and Shen, Wei and Yang, Xue and Feng, Qi and Jiang, Zekun and Yang, Xiaokang}, 
journal = {ICCV}, 
eprint = {2211.00288}, 
abstract = {{When handling complicated text images (e.g., irregular structures, low resolution, heavy occlusion, and uneven illumination), existing supervised text recognition methods are data-hungry. Although these methods employ large-scale synthetic text images to reduce the dependence on annotated real images, the domain gap still limits the recognition performance. Therefore, exploring the robust text feature representations on unlabeled real images by self-supervised learning is a good solution. However, existing self-supervised text recognition methods conduct sequence-to-sequence representation learning by roughly splitting the visual features along the horizontal axis, which limits the flexibility of the augmentations, as large geometric-based augmentations may lead to sequence-to-sequence feature inconsistency. Motivated by this, we propose a novel self-supervised Character-to-Character Distillation method, CCD, which enables versatile augmentations to facilitate general text representation learning. Specifically, we delineate the character structures of unlabeled real images by designing a self-supervised character segmentation module. Following this, CCD easily enriches the diversity of local characters while keeping their pairwise alignment under flexible augmentations, using the transformation matrix between two augmented views from images. Experiments demonstrate that CCD achieves state-of-the-art results, with average performance gains of 1.38\% in text recognition, 1.7\% in text segmentation, 0.24 dB (PSNR) and 0.0321 (SSIM) in text super-resolution. Code is available at https://github.com/TongkunGuan/CCD.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Guan-Self-supervised%20Character-to-Character%20Distillation%20for%20Text%20Recognition-2023-ICCV.pdf}
}
@article{U-CELandgrafarXiv2023, 
year = {2023}, 
title = {{U-CE: Uncertainty-aware Cross-Entropy for Semantic Segmentation}}, 
author = {Landgraf, Steven and Hillemann, Markus and Wursthorn, Kira and Ulrich, Markus}, 
journal = {arXiv}, 
eprint = {2307.09947}, 
abstract = {{Deep neural networks have shown exceptional performance in various tasks, but their lack of robustness, reliability, and tendency to be overconfident pose challenges for their deployment in safety-critical applications like autonomous driving. In this regard, quantifying the uncertainty inherent to a model's prediction is a promising endeavour to address these shortcomings. In this work, we present a novel Uncertainty-aware Cross-Entropy loss (U-CE) that incorporates dynamic predictive uncertainties into the training process by pixel-wise weighting of the well-known cross-entropy loss (CE). Through extensive experimentation, we demonstrate the superiority of U-CE over regular CE training on two benchmark datasets, Cityscapes and ACDC, using two common backbone architectures, ResNet-18 and ResNet-101. With U-CE, we manage to train models that not only improve their segmentation performance but also provide meaningful uncertainties after training. Consequently, we contribute to the development of more robust and reliable segmentation models, ultimately advancing the state-of-the-art in safety-critical applications and beyond.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Landgraf-U-CE-%20Uncertainty-aware%20Cross-Entropy%20for%20Semantic%20Segmentation-2023-arXiv.pdf}
}
@article{Meta-TransformerZhangarXiv2023, 
year = {2023}, 
title = {{Meta-Transformer: A Unified Framework for Multimodal Learning}}, 
author = {Zhang, Yiyuan and Gong, Kaixiong and Zhang, Kaipeng and Li, Hongsheng and Qiao, Yu and Ouyang, Wanli and Yue, Xiangyu}, 
journal = {arXiv}, 
eprint = {2307.10802}, 
abstract = {{Multimodal learning aims to build models that can process and relate information from multiple modalities. Despite years of development in this field, it still remains challenging to design a unified network for processing various modalities (\$\textbackslashtextit\{e.g.\}\$ natural language, 2D images, 3D point clouds, audio, video, time series, tabular data) due to the inherent gaps among them. In this work, we propose a framework, named Meta-Transformer, that leverages a \$\textbackslashtextbf\{frozen\}\$ encoder to perform multimodal perception without any paired multimodal training data. In Meta-Transformer, the raw input data from various modalities are mapped into a shared token space, allowing a subsequent encoder with frozen parameters to extract high-level semantic features of the input data. Composed of three main components: a unified data tokenizer, a modality-shared encoder, and task-specific heads for downstream tasks, Meta-Transformer is the first framework to perform unified learning across 12 modalities with unpaired data. Experiments on different benchmarks reveal that Meta-Transformer can handle a wide range of tasks including fundamental perception (text, image, point cloud, audio, video), practical application (X-Ray, infrared, hyperspectral, and IMU), and data mining (graph, tabular, and time-series). Meta-Transformer indicates a promising future for developing unified multimodal intelligence with transformers. Code will be available at https://github.com/invictus717/MetaTransformer}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Zhang-Meta-Transformer-%20A%20Unified%20Framework%20for%20Multimodal%20Learning-2023-arXiv.pdf}
}
@article{RoBERTaLiuarXiv2019, 
year = {2019}, 
title = {{RoBERTa: A Robustly Optimized BERT Pretraining Approach}}, 
author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin}, 
journal = {arXiv}, 
eprint = {1907.11692}, 
abstract = {{Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-RoBERTa-%20A%20Robustly%20Optimized%20BERT%20Pretraining%20Approach-2019-arXiv.pdf}
}
@article{AlignDetLiICCV2023, 
year = {2023}, 
title = {{AlignDet: Aligning Pre-training and Fine-tuning in Object Detection}}, 
author = {Li, Ming and Wu, Jie and Wang, Xionghui and Chen, Chen and Qin, Jie and Xiao, Xuefeng and Wang, Rui and Zheng, Min and Pan, Xin}, 
journal = {ICCV}, 
eprint = {2307.11077}, 
abstract = {{The paradigm of large-scale pre-training followed by downstream fine-tuning has been widely employed in various object detection algorithms. In this paper, we reveal discrepancies in data, model, and task between the pre-training and fine-tuning procedure in existing practices, which implicitly limit the detector's performance, generalization ability, and convergence speed. To this end, we propose AlignDet, a unified pre-training framework that can be adapted to various existing detectors to alleviate the discrepancies. AlignDet decouples the pre-training process into two stages, i.e., image-domain and box-domain pre-training. The image-domain pre-training optimizes the detection backbone to capture holistic visual abstraction, and box-domain pre-training learns instance-level semantics and task-aware concepts to initialize the parts out of the backbone. By incorporating the self-supervised pre-trained backbones, we can pre-train all modules for various detectors in an unsupervised paradigm. As depicted in Figure 1, extensive experiments demonstrate that AlignDet can achieve significant improvements across diverse protocols, such as detection algorithm, model backbone, data setting, and training schedule. For example, AlignDet improves FCOS by 5.3 mAP, RetinaNet by 2.1 mAP, Faster R-CNN by 3.3 mAP, and DETR by 2.3 mAP under fewer epochs.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-AlignDet-%20Aligning%20Pre-training%20and%20Fine-tuning%20in%20Object%20Detection-2023-ICCV.pdf}
}
@article{DPM-OTLiICCV2023, 
year = {2023}, 
title = {{DPM-OT: A New Diffusion Probabilistic Model Based on Optimal Transport}}, 
author = {Li, Zezeng and Li, ShengHao and Wang, Zhanpeng and Lei, Na and Luo, Zhongxuan and Gu, Xianfeng}, 
journal = {ICCV}, 
eprint = {2307.11308}, 
abstract = {{Sampling from diffusion probabilistic models (DPMs) can be viewed as a piecewise distribution transformation, which generally requires hundreds or thousands of steps of the inverse diffusion trajectory to get a high-quality image. Recent progress in designing fast samplers for DPMs achieves a trade-off between sampling speed and sample quality by knowledge distillation or adjusting the variance schedule or the denoising equation. However, it can't be optimal in both aspects and often suffer from mode mixture in short steps. To tackle this problem, we innovatively regard inverse diffusion as an optimal transport (OT) problem between latents at different stages and propose the DPM-OT, a unified learning framework for fast DPMs with a direct expressway represented by OT map, which can generate high-quality samples within around 10 function evaluations. By calculating the semi-discrete optimal transport map between the data latents and the white noise, we obtain an expressway from the prior distribution to the data distribution, while significantly alleviating the problem of mode mixture. In addition, we give the error bound of the proposed method, which theoretically guarantees the stability of the algorithm. Extensive experiments validate the effectiveness and advantages of DPM-OT in terms of speed and quality (FID and mode mixture), thus representing an efficient solution for generative modeling. Source codes are available at https://github.com/cognaclee/DPM-OT}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-DPM-OT-%20A%20New%20Diffusion%20Probabilistic%20Model%20Based%20on%20Optimal%20Transport-2023-ICCV.pdf}
}
@article{MPGaoICCV2023, 
year = {2023}, 
title = {{Tuning Pre-trained Model via Moment Probing}}, 
author = {Gao, Mingze and Wang, Qilong and Lin, Zhenyi and Zhu, Pengfei and Hu, Qinghua and Zhou, Jingbo}, 
journal = {ICCV}, 
eprint = {2307.11342}, 
abstract = {{Recently, efficient fine-tuning of large-scale pre-trained models has attracted increasing research interests, where linear probing (LP) as a fundamental module is involved in exploiting the final representations for task-dependent classification. However, most of the existing methods focus on how to effectively introduce a few of learnable parameters, and little work pays attention to the commonly used LP module. In this paper, we propose a novel Moment Probing (MP) method to further explore the potential of LP. Distinguished from LP which builds a linear classification head based on the mean of final features (e.g., word tokens for ViT) or classification tokens, our MP performs a linear classifier on feature distribution, which provides the stronger representation ability by exploiting richer statistical information inherent in features. Specifically, we represent feature distribution by its characteristic function, which is efficiently approximated by using first- and second-order moments of features. Furthermore, we propose a multi-head convolutional cross-covariance (MHC\$\textasciicircum3\$) to compute second-order moments in an efficient and effective manner. By considering that MP could affect feature learning, we introduce a partially shared module to learn two recalibrating parameters (PSRP) for backbones based on MP, namely MP\$\_\{+\}\$. Extensive experiments on ten benchmarks using various models show that our MP significantly outperforms LP and is competitive with counterparts at less training cost, while our MP\$\_\{+\}\$ achieves state-of-the-art performance.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Gao-Tuning%20Pre-trained%20Model%20via%20Moment%20Probing-2023-ICCV.pdf}
}
@article{ETRISXuICCV2023, 
year = {2023}, 
title = {{Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation}}, 
author = {Xu, Zunnan and Chen, Zhihong and Zhang, Yong and Song, Yibing and Wan, Xiang and Li, Guanbin}, 
journal = {ICCV}, 
eprint = {2307.11545}, 
abstract = {{Parameter Efficient Tuning (PET) has gained attention for reducing the number of parameters while maintaining performance and providing better hardware resource savings, but few studies investigate dense prediction tasks and interaction between modalities. In this paper, we do an investigation of efficient tuning problems on referring image segmentation. We propose a novel adapter called Bridger to facilitate cross-modal information exchange and inject task-specific information into the pre-trained model. We also design a lightweight decoder for image segmentation. Our approach achieves comparable or superior performance with only 1.61\textbackslash\% to 3.38\textbackslash\% backbone parameter updates, evaluated on challenging benchmarks. The code is available at \textbackslashurl\{https://github.com/kkakkkka/ETRIS\}.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xu-Bridging%20Vision%20and%20Language%20Encoders-%20Parameter-Efficient%20Tuning%20for%20Referring%20Image%20Segmentation-2023-ICCV.pdf}
}
@article{WhyWuICCV2023, 
year = {2023}, 
title = {{Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?}}, 
author = {Wu, Cheng-En and Tian, Yu and Yu, Haichao and Wang, Heng and Morgado, Pedro and Hu, Yu Hen and Yang, Linjie}, 
journal = {ICCV}, 
eprint = {2307.11978}, 
abstract = {{Vision-language models such as CLIP learn a generic text-image embedding from large-scale training data. A vision-language model can be adapted to a new classification task through few-shot prompt tuning. We find that such a prompt tuning process is highly robust to label noises. This intrigues us to study the key reasons contributing to the robustness of the prompt tuning paradigm. We conducted extensive experiments to explore this property and find the key factors are: 1) the fixed classname tokens provide a strong regularization to the optimization of the model, reducing gradients induced by the noisy samples; 2) the powerful pre-trained image-text embedding that is learned from diverse and generic web data provides strong prior knowledge for image classification. Further, we demonstrate that noisy zero-shot predictions from CLIP can be used to tune its own prompt, significantly enhancing prediction accuracy in the unsupervised setting. The code is available at https://github.com/CEWu/PTNL.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wu-Why%20Is%20Prompt%20Tuning%20for%20Vision-Language%20Models%20Robust%20to%20Noisy%20Labels--2023-ICCV.pdf}
}
@article{OpenAssistantKopfarXiv2023, 
year = {2023}, 
title = {{OpenAssistant Conversations -- Democratizing Large Language Model Alignment}}, 
author = {Köpf, Andreas and Kilcher, Yannic and Rütte, Dimitri von and Anagnostidis, Sotiris and Tam, Zhi-Rui and Stevens, Keith and Barhoum, Abdullah and Duc, Nguyen Minh and Stanley, Oliver and Nagyfi, Richárd and ES, Shahul and Suri, Sameer and Glushkov, David and Dantuluri, Arnav and Maguire, Andrew and Schuhmann, Christoph and Nguyen, Huu and Mattick, Alexander}, 
journal = {arXiv}, 
eprint = {2304.07327}, 
abstract = {{Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages distributed across 66,497 conversation trees, in 35 different languages, annotated with 461,292 quality ratings. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers. To demonstrate the OpenAssistant Conversations dataset's effectiveness, we present OpenAssistant, the first fully open-source large-scale instruction-tuned model to be trained on human data. A preference study revealed that OpenAssistant replies are comparably preferred to GPT-3.5-turbo (ChatGPT) with a relative winrate of 48.3\% vs. 51.7\% respectively. We release our code and data under fully permissive licenses.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Köpf-OpenAssistant%20Conversations%20--%20Democratizing%20Large%20Language%20Model%20Alignment-2023-arXiv.pdf}
}
@article{CNNKimEMNLP2014, 
year = {2014}, 
title = {{Convolutional Neural Networks for Sentence Classification}}, 
author = {Kim, Yoon}, 
journal = {EMNLP}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Kim-Convolutional%20Neural%20Networks%20for%20Sentence%20Classification-2014-EMNLP.pdf}
}
@article{EDAWeiEMNLP2019, 
year = {2019}, 
title = {{EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks}}, 
author = {Wei, Jason and Zou, Kai}, 
journal = {EMNLP}, 
eprint = {1901.11196}, 
abstract = {{We present EDA: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50\% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wei-EDA-%20Easy%20Data%20Augmentation%20Techniques%20for%20Boosting%20Performance%20on%20Text%20Classification%20Tasks-2019-EMNLP.pdf}
}
@article{EmergentWeiTMLR2022, 
year = {2022}, 
title = {{Emergent Abilities of Large Language Models}}, 
author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William}, 
journal = {TMLR}, 
eprint = {2206.07682}, 
abstract = {{Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wei-Emergent%20Abilities%20of%20Large%20Language%20Models-2022-TMLR.pdf}
}
@article{OnBommasaniarXiv2021, 
year = {2021}, 
title = {{On the Opportunities and Risks of Foundation Models}}, 
author = {Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and Arx, Sydney von and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W and Tramèr, Florian and Wang, Rose E and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy}, 
journal = {arXiv}, 
eprint = {2108.07258}, 
abstract = {{AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Bommasani-On%20the%20Opportunities%20and%20Risks%20of%20Foundation%20Models-2021-arXiv.pdf}
}
@article{AFTWangarXiv2023, 
year = {2023}, 
title = {{Making Large Language Models Better Reasoners with Alignment}}, 
author = {Wang, Peiyi and Li, Lei and Chen, Liang and Song, Feifan and Lin, Binghuai and Cao, Yunbo and Liu, Tianyu and Sui, Zhifang}, 
journal = {arXiv}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wang-Making%20Large%20Language%20Models%20Better%20Reasoners%20with%20Alignment-2023-arXiv.pdf}
}
@article{PerceptualLossesJohnsonECCV2016, 
year = {2016}, 
title = {{Perceptual Losses for Real-Time Style Transfer and Super-Resolution}}, 
author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li}, 
journal = {ECCV}, 
eprint = {1603.08155}, 
abstract = {{We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a \textbackslashemph\{per-pixel\} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing \textbackslashemph\{perceptual\} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Johnson-Perceptual%20Losses%20for%20Real-Time%20Style%20Transfer%20and%20Super-Resolution-2016-ECCV.pdf}
}
@article{RegMixupPintoNeurIPS2022, 
year = {2022}, 
title = {{RegMixup: Mixup as a Regularizer Can Surprisingly Improve Accuracy and Out Distribution Robustness}}, 
author = {Pinto, Francesco and Yang, Harry and Lim, Ser-Nam and Torr, Philip H S and Dokania, Puneet K}, 
journal = {NeurIPS}, 
eprint = {2206.14502}, 
abstract = {{We show that the effectiveness of the well celebrated Mixup [Zhang et al., 2018] can be further improved if instead of using it as the sole learning objective, it is utilized as an additional regularizer to the standard cross-entropy loss. This simple change not only provides much improved accuracy but also significantly improves the quality of the predictive uncertainty estimation of Mixup in most cases under various forms of covariate shifts and out-of-distribution detection experiments. In fact, we observe that Mixup yields much degraded performance on detecting out-of-distribution samples possibly, as we show empirically, because of its tendency to learn models that exhibit high-entropy throughout; making it difficult to differentiate in-distribution samples from out-distribution ones. To show the efficacy of our approach (RegMixup), we provide thorough analyses and experiments on vision datasets (ImageNet \& CIFAR-10/100) and compare it with a suite of recent approaches for reliable uncertainty estimation.}}
}
@article{SGConvLiICLR2023, 
year = {2023}, 
title = {{What Makes Convolutional Models Great on Long Sequence Modeling?}}, 
author = {Li, Yuhong and Cai, Tianle and Zhang, Yi and Chen, Deming and Dey, Debadeepta}, 
journal = {ICLR}, 
eprint = {2210.09298}, 
abstract = {{Convolutional models have been widely used in multiple domains. However, most existing models only use local convolution, making the model unable to handle long-range dependency efficiently. Attention overcomes this problem by aggregating global information but also makes the computational complexity quadratic to the sequence length. Recently, Gu et al. [2021] proposed a model called S4 inspired by the state space model. S4 can be efficiently implemented as a global convolutional model whose kernel size equals the input sequence length. S4 can model much longer sequences than Transformers and achieve significant gains over SoTA on several long-range tasks. Despite its empirical success, S4 is involved. It requires sophisticated parameterization and initialization schemes. As a result, S4 is less intuitive and hard to use. Here we aim to demystify S4 and extract basic principles that contribute to the success of S4 as a global convolutional model. We focus on the structure of the convolution kernel and identify two critical but intuitive principles enjoyed by S4 that are sufficient to make up an effective global convolutional model: 1) The parameterization of the convolutional kernel needs to be efficient in the sense that the number of parameters should scale sub-linearly with sequence length. 2) The kernel needs to satisfy a decaying structure that the weights for convolving with closer neighbors are larger than the more distant ones. Based on the two principles, we propose a simple yet effective convolutional model called Structured Global Convolution (SGConv). SGConv exhibits strong empirical performance over several tasks: 1) With faster speed, SGConv surpasses S4 on Long Range Arena and Speech Command datasets. 2) When plugging SGConv into standard language and vision models, it shows the potential to improve both efficiency and performance.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-What%20Makes%20Convolutional%20Models%20Great%20on%20Long%20Sequence%20Modeling--2023-ICLR.pdf}
}
@article{LLaVALiuarXiv2023, 
year = {2023}, 
title = {{Visual Instruction Tuning}}, 
author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae}, 
journal = {arXiv}, 
eprint = {2304.08485}, 
abstract = {{Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53\%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Liu-Visual%20Instruction%20Tuning-2023-arXiv.pdf}
}
@article{WizardLMXuarXiv2023, 
year = {2023}, 
title = {{WizardLM: Empowering Large Language Models to Follow Complex Instructions}}, 
author = {Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin}, 
journal = {arXiv}, 
eprint = {2304.12244}, 
abstract = {{Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\textbackslash\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Xu-WizardLM-%20Empowering%20Large%20Language%20Models%20to%20Follow%20Complex%20Instructions-2023-arXiv.pdf}
}
@article{LATMCaiarXiv2023, 
year = {2023}, 
title = {{Large Language Models as Tool Makers}}, 
author = {Cai, Tianle and Wang, Xuezhi and Ma, Tengyu and Chen, Xinyun and Zhou, Denny}, 
journal = {arXiv}, 
eprint = {2305.17126}, 
abstract = {{Recent research shows the potential of enhancing the problem-solving ability of large language models (LLMs) through the use of external tools. However, prior work along this line depends on the availability of existing tools. In this work, we take an initial step towards removing this dependency by proposing a closed-loop framework, referred to as LLMs As Tool Makers (LATM), where LLMs create their own reusable tools for problem-solving. Our approach consists of two key phases: 1) tool making: an LLM acts as the tool maker that crafts tools for given tasks, where a tool is implemented as a Python utility function. 2) tool using: an LLM acts as the tool user, which applies the tool built by the tool maker for problem-solving. The tool user can be either the same or a different LLM from the tool maker. Tool-making enables an LLM to continually generate tools that can be applied to different requests so that future requests can call the corresponding APIs when beneficial for solving the tasks. Furthermore, the division of labor among LLMs for tool-making and tool-using phases introduces the opportunity to achieve cost effectiveness without degrading the quality of generated tools and problem solutions. For example, recognizing that tool-making demands more sophisticated capabilities than tool-using, we can apply a powerful yet resource-intensive model as the tool maker, and a lightweight while cost-effective model as the tool user. We validate the effectiveness of our approach across a variety of complex reasoning tasks, including Big-Bench tasks. With GPT-4 as the tool maker and GPT-3.5 as the tool user, LATM can achieve performance that is on par with using GPT-4 for both tool making and tool using, while the inference cost is significantly reduced.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Cai-Large%20Language%20Models%20as%20Tool%20Makers-2023-arXiv.pdf}
}
@article{MTSZhuICCV2023, 
year = {2023}, 
title = {{Rethinking Data Distillation: Do Not Overlook Calibration}}, 
author = {Zhu, Dongyao and Lei, Bowen and Zhang, Jie and Fang, Yanbo and Zhang, Ruqi and Xie, Yiqun and Xu, Dongkuan}, 
journal = {ICCV}, 
eprint = {2307.12463}, 
abstract = {{Neural networks trained on distilled data often produce over-confident output and require correction by calibration methods. Existing calibration methods such as temperature scaling and mixup work well for networks trained on original large-scale data. However, we find that these methods fail to calibrate networks trained on data distilled from large source datasets. In this paper, we show that distilled data lead to networks that are not calibratable due to (i) a more concentrated distribution of the maximum logits and (ii) the loss of information that is semantically meaningful but unrelated to classification tasks. To address this problem, we propose Masked Temperature Scaling (MTS) and Masked Distillation Training (MDT) which mitigate the limitations of distilled data and achieve better calibration results while maintaining the efficiency of dataset distillation.}}
}
@article{AFFNetHuangICCV2023, 
year = {2023}, 
title = {{Adaptive Frequency Filters As Efficient Global Token Mixers}}, 
author = {Huang, Zhipeng and Zhang, Zhizheng and Lan, Cuiling and Zha, Zheng-Jun and Lu, Yan and Guo, Baining}, 
journal = {ICCV}, 
eprint = {2307.14008}, 
abstract = {{Recent vision transformers, large-kernel CNNs and MLPs have attained remarkable successes in broad vision tasks thanks to their effective information fusion in the global scope. However, their efficient deployments, especially on mobile devices, still suffer from noteworthy challenges due to the heavy computational costs of self-attention mechanisms, large kernels, or fully connected layers. In this work, we apply conventional convolution theorem to deep learning for addressing this and reveal that adaptive frequency filters can serve as efficient global token mixers. With this insight, we propose Adaptive Frequency Filtering (AFF) token mixer. This neural operator transfers a latent representation to the frequency domain via a Fourier transform and performs semantic-adaptive frequency filtering via an elementwise multiplication, which mathematically equals to a token mixing operation in the original latent space with a dynamic convolution kernel as large as the spatial resolution of this latent representation. We take AFF token mixers as primary neural operators to build a lightweight neural network, dubbed AFFNet. Extensive experiments demonstrate the effectiveness of our proposed AFF token mixer and show that AFFNet achieve superior accuracy and efficiency trade-offs compared to other lightweight network designs on broad visual tasks, including visual recognition and dense prediction tasks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Huang-Adaptive%20Frequency%20Filters%20As%20Efficient%20Global%20Token%20Mixers-2023-ICCV.pdf}
}
@article{VPIYanarXiv2023, 
year = {2023}, 
title = {{Virtual Prompt Injection for Instruction-Tuned Large Language Models}}, 
author = {Yan, Jun and Yadav, Vikas and Li, Shiyang and Chen, Lichang and Tang, Zheng and Wang, Hai and Srinivasan, Vijay and Ren, Xiang and Jin, Hongxia}, 
journal = {arXiv}, 
eprint = {2307.16888}, 
abstract = {{We present Virtual Prompt Injection (VPI) for instruction-tuned Large Language Models (LLMs). VPI allows an attacker-specified virtual prompt to steer the model behavior under specific trigger scenario without any explicit injection in model input. For instance, if an LLM is compromised with the virtual prompt "Describe Joe Biden negatively." for Joe Biden-related instructions, then any service deploying this model will propagate biased views when handling user queries related to Joe Biden. VPI is especially harmful for two primary reasons. Firstly, the attacker can take fine-grained control over LLM behaviors by defining various virtual prompts, exploiting LLMs' proficiency in following instructions. Secondly, this control is achieved without any interaction from the attacker while the model is in service, leading to persistent attack. To demonstrate the threat, we propose a simple method for performing VPI by poisoning the model's instruction tuning data. We find that our proposed method is highly effective in steering the LLM with VPI. For example, by injecting only 52 poisoned examples (0.1\% of the training data size) into the instruction tuning data, the percentage of negative responses given by the trained model on Joe Biden-related queries change from 0\% to 40\%. We thus highlight the necessity of ensuring the integrity of the instruction-tuning data as little poisoned data can cause stealthy and persistent harm to the deployed model. We further explore the possible defenses and identify data filtering as an effective way to defend against the poisoning attacks. Our project page is available at https://poison-llm.github.io.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yan-Virtual%20Prompt%20Injection%20for%20Instruction-Tuned%20Large%20Language%20Models-2023-arXiv.pdf}
}
@article{DAPTChoICCV2023, 
year = {2023}, 
title = {{Distribution-Aware Prompt Tuning for Vision-Language Models}}, 
author = {Cho, Eulrang and Kim, Jooyeon and Kim, Hyunwoo J}, 
journal = {ICCV}, 
eprint = {2309.03406}, 
abstract = {{Pre-trained vision-language models (VLMs) have shown impressive performance on various downstream tasks by utilizing knowledge learned from large data. In general, the performance of VLMs on target tasks can be further improved by prompt tuning, which adds context to the input image or text. By leveraging data from target tasks, various prompt-tuning methods have been studied in the literature. A key to prompt tuning is the feature space alignment between two modalities via learnable vectors with model parameters fixed. We observed that the alignment becomes more effective when embeddings of each modality are `well-arranged' in the latent space. Inspired by this observation, we proposed distribution-aware prompt tuning (DAPT) for vision-language models, which is simple yet effective. Specifically, the prompts are learned by maximizing inter-dispersion, the distance between classes, as well as minimizing the intra-dispersion measured by the distance between embeddings from the same class. Our extensive experiments on 11 benchmark datasets demonstrate that our method significantly improves generalizability. The code is available at https://github.com/mlvlab/DAPT.}}
}
@article{OPROYangarXiv2023, 
year = {2023}, 
title = {{Large Language Models as Optimizers}}, 
author = {Yang, Chengrun and Wang, Xuezhi and Lu, Yifeng and Liu, Hanxiao and Le, Quoc V and Zhou, Denny and Chen, Xinyun}, 
journal = {arXiv}, 
eprint = {2309.03409}, 
abstract = {{Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8\% on GSM8K, and by up to 50\% on Big-Bench Hard tasks.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yang-Large%20Language%20Models%20as%20Optimizers-2023-arXiv.pdf}
}
@article{FLM-101BLiarXiv2023, 
year = {2023}, 
title = {{FLM-101B: An Open LLM and How to Train It with \$100K Budget}}, 
author = {Li, Xiang and Yao, Yiqun and Jiang, Xin and Fang, Xuezhi and Meng, Xuying and Fan, Siqi and Han, Peng and Li, Jing and Du, Li and Qin, Bowen and Zhang, Zheng and Sun, Aixin and Wang, Yequan}, 
journal = {arXiv}, 
eprint = {2309.03852}, 
abstract = {{Large language models (LLMs) have achieved remarkable success in NLP and multimodal tasks. Despite these successes, their development faces two main challenges: (i) high computational cost; and (ii) difficulty in conducting fair and objective evaluations. LLMs are prohibitively expensive, making it feasible for only a few major players to undertake their training, thereby constraining both research and application opportunities. This underscores the importance of cost-effective LLM training. In this paper, we utilize a growth strategy to significantly reduce LLM training cost. We demonstrate that an LLM with 101B parameters and 0.31TB tokens can be trained on a \$100K budget. We also adopt a systematic evaluation paradigm for the IQ evaluation of LLMs, in complement to existing evaluations that focus more on knowledge-oriented abilities. We introduce our benchmark including evaluations on important aspects of intelligence including symbolic mapping, itrule understanding, pattern mining, and anti-interference. Such evaluations minimize the potential impact of memorization. Experimental results show that our model FLM-101B, trained with a budget of \$100K, achieves comparable performance to powerful and well-known models, eg GPT-3 and GLM-130B, especially in the IQ benchmark evaluations with contexts unseen in training data. The checkpoint of FLM-101B will be open-sourced at https://huggingface.co/CofeAI/FLM-101B.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Li-FLM-101B-%20An%20Open%20LLM%20and%20How%20to%20Train%20It%20with%20$100K%20Budget-2023-arXiv.pdf}
}
@article{CUREChenarXiv2023, 
year = {2023}, 
title = {{Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models}}, 
author = {Chen, Yangyi and Sikka, Karan and Cogswell, Michael and Ji, Heng and Divakaran, Ajay}, 
journal = {arXiv}, 
eprint = {2309.04461}, 
abstract = {{Vision-language models (VLMs) have recently demonstrated strong efficacy as visual assistants that can parse natural queries about the visual content and generate human-like outputs. In this work, we explore the ability of these models to demonstrate human-like reasoning based on the perceived information. To address a crucial concern regarding the extent to which their reasoning capabilities are fully consistent and grounded, we also measure the reasoning consistency of these models. We achieve this by proposing a chain-of-thought (CoT) based consistency measure. However, such an evaluation requires a benchmark that encompasses both high-level inference and detailed reasoning chains, which is costly. We tackle this challenge by proposing a LLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously ensuring the generation of a high-quality dataset. Based on this pipeline and the existing coarse-grained annotated dataset, we build the CURE benchmark to measure both the zero-shot reasoning performance and consistency of VLMs. We evaluate existing state-of-the-art VLMs, and find that even the best-performing model is unable to demonstrate strong visual reasoning capabilities and consistency, indicating that substantial efforts are required to enable VLMs to perform visual reasoning as systematically and consistently as humans. As an early step, we propose a two-stage training framework aimed at improving both the reasoning performance and consistency of VLMs. The first stage involves employing supervised fine-tuning of VLMs using step-by-step reasoning samples automatically generated by LLMs. In the second stage, we further augment the training process by incorporating feedback provided by LLMs to produce reasoning chains that are highly consistent and grounded. We empirically highlight the effectiveness of our framework in both reasoning performance and consistency.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Chen-Measuring%20and%20Improving%20Chain-of-Thought%20Reasoning%20in%20Vision-Language%20Models-2023-arXiv.pdf}
}
@article{LanguageYuarXiv2023, 
year = {2023}, 
title = {{Language Models as Black-Box Optimizers for Vision-Language Models}}, 
author = {Yu, Samuel and Liu, Shihong and Lin, Zhiqiu and Pathak, Deepak and Ramanan, Deva}, 
journal = {arXiv}, 
eprint = {2309.05950}, 
abstract = {{Vision-language models (VLMs) pre-trained on web-scale datasets have demonstrated remarkable capabilities across a variety of vision and multimodal tasks. Currently, fine-tuning methods for VLMs mainly operate in a white-box setting, requiring access to model parameters for backpropagation. However, many VLMs rely on proprietary data and are not open-source, which restricts the use of white-box approaches for fine-tuning. Given that popular private large language models (LLMs) like ChatGPT still offer a language-based user interface, we aim to develop a novel fine-tuning approach for VLMs through natural language prompts, thereby avoiding the need to access model parameters, feature embeddings, or output logits. In this setup, we propose employing chat-based LLMs as black-box optimizers to search for the best text prompt on the illustrative task of few-shot image classification using CLIP. Specifically, we adopt an automatic "hill-climbing" procedure that converges on an effective prompt by evaluating the accuracy of current prompts and asking LLMs to refine them based on textual feedback, all within a conversational process without human-in-the-loop. In a challenging 1-shot learning setup, our simple approach surpasses the white-box continuous prompting method CoOp by an average of 1.5\% across 11 datasets including ImageNet. Our approach also outperforms OpenAI's manually crafted prompts and is more efficient than other black-box methods like iterative APE. Additionally, we highlight the advantage of conversational feedback incorporating both positive and negative prompts, suggesting that LLMs can utilize the implicit "gradient" direction in textual feedback for a more efficient search. Lastly, we find that the text prompts generated through our strategy are not only more interpretable but also transfer well across different CLIP architectures in a black-box manner.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Yu-Language%20Models%20as%20Black-Box%20Optimizers%20for%20Vision-Language%20Models-2023-arXiv.pdf}
}
@article{HAMLETColomerICCV2023, 
year = {2023}, 
title = {{To Adapt or Not to Adapt? Real-Time Adaptation for Semantic Segmentation}}, 
author = {Colomer, Marc Botet and Dovesi, Pier Luigi and Panagiotakopoulos, Theodoros and Carvalho, Joao Frederico and Härenstam-Nielsen, Linus and Azizpour, Hossein and Kjellström, Hedvig and Cremers, Daniel and Poggi, Matteo}, 
journal = {ICCV}, 
eprint = {2307.15063}, 
abstract = {{The goal of Online Domain Adaptation for semantic segmentation is to handle unforeseeable domain changes that occur during deployment, like sudden weather events. However, the high computational costs associated with brute-force adaptation make this paradigm unfeasible for real-world applications. In this paper we propose HAMLET, a Hardware-Aware Modular Least Expensive Training framework for real-time domain adaptation. Our approach includes a hardware-aware back-propagation orchestration agent (HAMT) and a dedicated domain-shift detector that enables active control over when and how the model is adapted (LT). Thanks to these advancements, our approach is capable of performing semantic segmentation while simultaneously adapting at more than 29FPS on a single consumer-grade GPU. Our framework's encouraging accuracy and speed trade-off is demonstrated on OnDA and SHIFT benchmarks through experimental results.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Colomer-To%20Adapt%20or%20Not%20to%20Adapt-%20Real-Time%20Adaptation%20for%20Semantic%20Segmentation-2023-ICCV.pdf}
}
@article{HowQinarXiv2023, 
year = {2023}, 
title = {{How Good is Google Bard's Visual Understanding? An Empirical Study on Open Challenges}}, 
author = {Qin, Haotong and Ji, Ge-Peng and Khan, Salman and Fan, Deng-Ping and Khan, Fahad Shahbaz and Gool, Luc Van}, 
journal = {arXiv}, 
eprint = {2307.15016}, 
abstract = {{Google's Bard has emerged as a formidable competitor to OpenAI's ChatGPT in the field of conversational AI. Notably, Bard has recently been updated to handle visual inputs alongside text prompts during conversations. Given Bard's impressive track record in handling textual inputs, we explore its capabilities in understanding and interpreting visual data (images) conditioned by text questions. This exploration holds the potential to unveil new insights and challenges for Bard and other forthcoming multi-modal Generative models, especially in addressing complex computer vision problems that demand accurate visual and language understanding. Specifically, in this study, we focus on 15 diverse task scenarios encompassing regular, camouflaged, medical, under-water and remote sensing data to comprehensively evaluate Bard's performance. Our primary finding indicates that Bard still struggles in these vision scenarios, highlighting the significant gap in vision-based understanding that needs to be bridged in future developments. We expect that this empirical study will prove valuable in advancing future models, leading to enhanced capabilities in comprehending and interpreting fine-grained visual data. Our project is released on https://github.com/htqin/GoogleBard-VisUnderstand}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Qin-How%20Good%20is%20Google%20Bard's%20Visual%20Understanding-%20An%20Empirical%20Study%20on%20Open%20Challenges-2023-arXiv.pdf}
}
@article{f-DISTILLWenACL2023, 
year = {2023}, 
title = {{f-Divergence Minimization for Sequence-Level Knowledge Distillation}}, 
author = {Wen, Yuqiao and Li, Zichao and Du, Wenyu and Mou, Lili}, 
journal = {ACL}, 
eprint = {2307.15190}, 
abstract = {{Knowledge distillation (KD) is the process of transferring knowledge from a large model to a small one. It has gained increasing attention in the natural language processing community, driven by the demands of compressing ever-growing language models. In this work, we propose an f-DISTILL framework, which formulates sequence-level knowledge distillation as minimizing a generalized f-divergence function. We propose four distilling variants under our framework and show that existing SeqKD and ENGINE approaches are approximations of our f-DISTILL methods. We further derive step-wise decomposition for our f-DISTILL, reducing intractable sequence-level divergence to word-level losses that can be computed in a tractable manner. Experiments across four datasets show that our methods outperform existing KD approaches, and that our symmetric distilling losses can better force the student to learn from the teacher distribution.}}, 
local-url = {file://localhost/Users/zifu/Documents/Papers%20Library/Wen-f-Divergence%20Minimization%20for%20Sequence-Level%20Knowledge%20Distillation-2023-ACL.pdf}
}