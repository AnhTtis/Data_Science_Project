% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage[english]{babel}
\usepackage{multirow,boldline}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{float}
\usepackage{dsfont}
\usepackage{placeins}
\usepackage{stfloats}
\usepackage{derivative}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
%\usepackage{amsthm}

\usepackage{todonotes}

% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%
\begin{document}
%
\title{Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Zifu Wang\inst{1} \and
Teodora Popordanoska\inst{1} \and
Jeroen Bertels\inst{1} \and
Robin Lemmens\inst{2,3} \and
Matthew B. Blaschko\inst{1}}

\authorrunning{Wang et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{ESAT-PSI, KU Leuven, Leuven, Belgium \and
Department of Neurosciences, KU Leuven, Leuven, Belgium \and
Department of Neurology, UZ Leuven, Leuven, Belgium \\
\email{firstname.lastname@kuleuven.be}}

\maketitle              % typeset the header of the contribution
%
\begin{abstract}
The soft Dice loss (SDL) has taken a pivotal role in many automated segmentation pipelines in the medical imaging community. Over the last years, some reasons behind its superior functioning have been uncovered and further optimizations have been explored. However, there is currently no implementation that supports its direct use in settings with soft labels. Hence, a synergy between the use of SDL and research leveraging the use of soft labels, also in the context of model calibration, is still missing. In this work, we introduce Dice semimetric losses (DMLs), which (i) are by design identical to SDL in a standard setting with hard labels, but (ii) can be used in settings with soft labels. Our experiments on the public QUBIQ, LiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels (e.g. averaging, label smoothing, and knowledge distillation) over hard labels (e.g. majority voting and random selection). As a result, we obtain superior Dice scores and model calibration, which supports the wider adoption of DMLs in practice. Code is available at \href{https://github.com/zifuwanggg/JDTLosses}{https://github.com/zifuwanggg/JDTLosses}.
\keywords{Dice Score \and Dice Loss \and Soft Labels \and Model Calibration}
\end{abstract}

\section{Introduction}\label{sec:introduction}
Image segmentation is a fundamental task in medical image analysis. One of the key design choices in many segmentation pipelines that are based on neural networks is the loss function. In fact, the choice of loss function goes hand in hand with the metrics chosen to quantify the quality of the predicted segmentation~\cite{NatureVapnik1995}. The intersection-over-union (IoU) and the Dice score are commonly used metrics because they reflect both size and localization agreement, and they are more in line with perceptual quality compared to, e.g., pixel-wise accuracy~\cite{OptimizationTMI2020,MetricsMaier-HeinarXiv2023}. Hence, directly optimizing the IoU or the Dice score using differentiable surrogates as (part of) the loss function has become standard practice in semantic segmentation dealing with natural or medical images, respectively~\cite{Lovasz-softmaxLossBermanCVPR2018,nnU-NetIsenseeNatureMethods2021,OptimizationTMI2020,JMLsWangarXiv2023}. In medical imaging in particular, the Dice score and the soft Dice loss (SDL)~\cite{SoftDiceLossSudreMICCAIWorkshop2017} have become the standard practice, and some reasons behind its superior functioning have been uncovered and further optimizations have been explored~\cite{OptimizationTMI2020,TheoreticalBertelsMIA2021,TheDiceLossTilborghsMICCAI2022}.

Another mechanism to further improve the predicted segmentation that has gained much attention in recent years, is the use of soft labels during training. Soft labels can be the result of data augmentation tricks such as label smoothing (LS)~\cite{InceptionV2V3SzegedyCVPR2016,SVLSIslamIPMI2021} and are involved in regularization techniques such as knowledge distillation (KD)~\cite{KDHintonNeurIPSWorkshop2015,EMKDQinTMI2021}. Their role is to provide additional regularization to make the model less prone to overfitting \cite{InceptionV2V3SzegedyCVPR2016,KDHintonNeurIPSWorkshop2015} and to combat overconfidence~\cite{CalibrationGuoICML2017}, e.g., providing superior model calibration~\cite{WhenMullerNeurIPS2019}. In medical imaging, soft labels not only arise due to LS or KD, but are also present inherently due to significant intra- and inter-rater variability. For example, multiple annotators often disagree on organ and lesion boundaries, and one can average their annotations to obtain soft label maps~\cite{SoftSegGrosMIA2021,MRNetJiCVPR2021,UsingSilvaMICCAIWorkshop2021,LabelLemayMELBA2023}.

This work investigates how the medical imaging community can combine the use of SDL with soft labels to reach a state of synergy. While the original SDL surrogate was posed as a relaxed form of the Dice score, naively inputting soft labels to SDL is possible (e.g.\ in open-source segmentation libraries~\cite{SMPIakubovskii2019,MMSegmentation2020,nnU-NetIsenseeNatureMethods2021,MedISegZhangarXiv2022}) but will push the predictions towards 0-1 outputs rather than make them similar to the soft labels~\cite{TheoreticalBertelsMIA2021,JMLsWangarXiv2023}. Consequently, the use of SDL when dealing with soft labels might not align with a user's expectations, with potential adverse effects on the Dice score, model calibration and volume estimation~\cite{TheoreticalBertelsMIA2021}. More recently~\cite{JMLsWangarXiv2023}, this incompatibility to soft labels has been assigned to many regional losses, e.g.~\cite{OptimalNowozinCVPR2014,SoftJaccardRahmanISVC2016,Lovasz-softmaxLossBermanCVPR2018,LovaszHingeYuTPAMI2018,PixIoUYuICML2021}. Motivated by this observation, firstly (Sect.~\ref{sec:methods}), we propose two probabilistic extensions of SDL, i.e., Dice semimetric losses (DMLs), which satisfy the conditions of a semimetric and are fully compatible with both hard and soft labels. In a setting with hard labels, DMLs are identical to SDL and can safely replace SDL in existing implementations. Secondly (Sect.~\ref{sec:experiments}), we perform extensive experiments on the public QUBIQ, LiTS and KiTS benchmarks to empirically confirm the potential synergy of DMLs with soft labels (e.g.\ averaging, LS, KD) over hard labels (e.g.\ majority voting, random selection).

\section{Methods}\label{sec:methods}
We adopt the notation of \cite{JMLsWangarXiv2023}. In particular, we denote the predicted segmentation as $\dot x \in \{1,...,C\}^p$ and the ground-truth segmentation as $\dot y \in \{1,...,C\}^p$, where $C$ is the number of classes and $p$ the number of pixels. For a class $c$, we define the set of predictions as $x^c=\{\dot x=c\}$, the set of ground-truth as $y^c=\{\dot y=c\}$, the union as $u^c=x^c\cup y^c$, the intersection as $v^c=x^c\cap y^c$, the symmetric difference (i.e., the set of mispredictions) as $m^c=(x^c \setminus y^c)\cup (y^c\setminus x^c)$, the Jaccard index as $\text{IoU}^c = \frac{|v^c|}{|u^c|}$, and the Dice score as $\text{Dice}^c=\frac{2\text{IoU}^c}{1+\text{IoU}^c}=\frac{2|v^c|}{|x^c|+|y^c|}$. In what follows, we will represent sets as binary vectors $x^c,y^c,u^c,v^c,m^c \in \{0,1\}^p$ and denote $|x^c|=\sum_{i=1}^p x^c_i$ the cardinality of the relevant set. Moreover, when the context is clear, we will drop the superscript $c$.

\subsection{Existing Extensions}
If we want to optimize the Dice score, hence, minimize the Dice loss $\Delta_{\text{Dice}} = 1-\text{Dice}$ in a continuous setting, we need to extend $\Delta_{\text{Dice}}$ with $\overline{\Delta}_{\text{Dice}}$ such that it can take any predicted segmentation $\tilde{x}\in[0,1]^p$ as input. Hereinafter, when there is no ambiguity, we will use $x$ and $\tilde{x}$ interchangeably. 

The soft Dice loss (SDL) \cite{SoftDiceLossSudreMICCAIWorkshop2017} extends $\Delta_{\text{Dice}}$ by realizing that when $x,y \in \{0,1\}^p$, $|v|=\langle x, y\rangle$, $|x|=\|x\|_1$ and $|y|=\|y\|_1$. Therefore, SDL replaces the set notation with vector functions:
\begin{equation}
   \overline{\Delta}_{\text{SDL}}: x\in[0,1]^p, y\in\{0,1\}^p \mapsto 1 - \frac{2\langle x, y\rangle}{\|x\|_1+\|y\|_1}.
\end{equation}
The soft Jaccard loss (SJL) \cite{OptimalNowozinCVPR2014,SoftJaccardRahmanISVC2016} can be defined in a similar way:
\begin{equation}
   \overline{\Delta}_{\text{SJL}}: x\in[0,1]^p, y\in\{0,1\}^p \mapsto 1 - \frac{\langle x, y\rangle}{\|x\|_1+\|y\|_1-\langle x, y\rangle}.
\end{equation}

A major limitation of loss functions based on $L^1$ relaxations, such as SDL, SJL, the soft Tversky loss \cite{SoftTverskyLossSalehiMICCAIWorkshop2017} and the focal Tversky loss \cite{FocalTverskyLossAbrahamISBI2019}, and loss functions that rely on the Lovasz extension, such as the Lovasz hinge loss \cite{LovaszHingeYuTPAMI2018}, the Lovasz-Softmax loss \cite{Lovasz-softmaxLossBermanCVPR2018} and the PixIoU loss \cite{PixIoUYuICML2021}, is that they cannot handle soft labels \cite{JMLsWangarXiv2023}. That is, when $y$ is also in $[0,1]^p$. In particular, both SDL and SJL are not minimized at $x=y$, but rather pushing $x$ towards the vertices $\{0,1\}^p$ \cite{TheoreticalBertelsMIA2021,JMLsWangarXiv2023}. For example, consider $y=0.5$, it is easy to verify that SDL is minimized at $x=1$, which is clearly erroneous.  

Therefore, Wang and Blaschko \cite{JMLsWangarXiv2023} proposed two variants of SJL that they call Jaccard metric losses (JMLs). $\overline{\Delta}_{\text{JML,1}}$ and $\overline{\Delta}_{\text{JML,2}}: [0,1]^p\times[0,1]^p \rightarrow [0,1]$ are defined as
\begin{align}\label{eq:jml}
    \overline{\Delta}_{\text{JML,1}} = 1 - \frac{\|x+y\|_1-\|x-y\|_1}{\|x+y\|_1+\|x-y\|_1}, \quad
    \overline{\Delta}_{\text{JML,2}} = 1 - \frac{\|x\odot y\|_1}{\|x\odot y\|_1+\|x-y\|_1}.
\end{align}
JMLs are shown to be a metric in $[0,1]^p$, according to the definition below.
\begin{definition}[Metric \cite{EncyclopediaDeza2009}]
A mapping $f: [0,1]^p \times [0,1]^p \rightarrow \mathbb{R}$ is called a metric if it satisfies the following conditions for all $a,b,c \in [0,1]^p$:
\begin{enumerate}[label=(\roman*)]
    \item (Reflexivity). $f(a, a) = 0$.
    \item (Positivity). If $a\neq b$, then $f(a,b)>0$.
    \item (Symmetry). $f(a,b) = f(b,a)$.
    \item (Triangle inequality). $f(a,c) \leq f(a,b) + f(b,c)$.
\end{enumerate}
\end{definition}
Note that reflexivity and positivity jointly imply $x=y \Leftrightarrow f(x,y) = 0$, hence, a loss function that satisfies these conditions will be compatible with soft labels. 

\subsection{Dice Semimetric Losses}
We focus here on the Dice loss. For a derivation of the Tversky loss, please refer to Appendix \ref{app:tversky}. 

Since $\text{Dice}=\frac{2\text{IoU}}{1+\text{IoU}}\Rightarrow 1-\text{Dice}=\frac{1-\text{IoU}}{2-(1-\text{IoU})}$, we have $\overline{\Delta}_{\text{Dice}}=\frac{\overline{\Delta}_{\text{IoU}}}{2- \overline{\Delta}_{\text{IoU}}}$. There exist several alternatives to define $\overline{\Delta}_{\text{IoU}}$, but not all of them are feasible, e.g., SJL. Generally, it is easy to verify the following proposition:
\begin{proposition}\label{thm:DiceIoUcompatibility}
$\overline{\Delta}_{\text{Dice}}$ satisfies reflexivity and positivity if and only if $\overline{\Delta}_{\text{IoU}}$ does.
\end{proposition}

Among the definitions of $\overline{\Delta}_{\text{IoU}}$, Wang and Blaschko \cite{JMLsWangarXiv2023} found only two candidates as defined in Eq. (\ref{eq:jml}) that satisfy reflexivity and positivity. Following Proposition~\ref{thm:DiceIoUcompatibility}, we transform these two IoU losses and define Dice semimetric losses (DMLs) $\overline{\Delta}_{\text{DML,1}}, \overline{\Delta}_{\text{DML,2}}: [0,1]^p\times[0,1]^p \rightarrow [0,1]$ as  
\begin{align}
    \overline{\Delta}_{\text{DML,1}} = 1-\frac{\|x+y\|_1-\|x-y\|_1}{\|x+y\|_1}, \quad
    \overline{\Delta}_{\text{DML,2}} = 1-\frac{2\|xy\|_1}{2\|xy\|_1+\|x-y\|_1}.
\end{align}

$\Delta_{\text{Dice}}$ that is defined over integers does not satisfy the triangle inequality \cite{RelaxedGrageraTCS2018}, which is shown to be helpful in KD \cite{JMLsWangarXiv2023}. Nonetheless, we can consider a weaker form of the triangle inequality:
\begin{equation}
    f(a,c) \leq \rho (f(a,b) + f(b,c)).
\end{equation}
Functions that satisfy the relaxed triangle inequality for some fixed scalar $\rho$ and conditions (i)-(iii) of a metric are called semimetrics. $\Delta_{\text{Dice}}$ is a semimetric in $\{0,1\}^p$ \cite{RelaxedGrageraTCS2018}. We show DMLs are semimetrics in $[0,1]^p$ in the following theorem:
\begin{theorem} \label{thm:rhodice}
$\overline{\Delta}_{\text{DML,1}}$ and $\overline{\Delta}_{\text{DML,2}}$ are semimetrics in $[0,1]^p$.
\end{theorem}
The proof can be found in Appendix \ref{app:thm_semi}. Moreover, DMLs have properties that are similar to JMLs and they are presented as follows:
\begin{theorem} \label{thm:eqneq}
$\forall x\in[0,1]^p,\  y\in \{0,1\}^p$ and $x\in \{0,1\}^p,\  y\in [0,1]^p$, $\overline{\Delta}_{\text{SDL}}=\overline{\Delta}_{\text{DML,1}}=\overline{\Delta}_{\text{DML,2}}$. $\exists x, y \in [0,1]^p, \overline{\Delta}_{\text{SDL}}\neq\overline{\Delta}_{\text{DML,1}}\neq\overline{\Delta}_{\text{DML,2}}$.
\end{theorem}
\begin{theorem} \label{thm:smaller}
$\forall x, y\in[0,1]^p$, $\overline{\Delta}_{\text{DML,1}}\leq \overline{\Delta}_{\text{DML,2}}$.
\end{theorem}
 The proofs are similar to those given in \cite{JMLsWangarXiv2023}. Importantly, Theorem~\ref{thm:eqneq} indicates that we can safely replace the existing implementation of SDL with DMLs and no change will be incurred since they are identical when only hard labels are presented.
 
\section{Experiments}\label{sec:experiments}
In this section, we demonstrate how models can benefit from soft labels. In particular, using QUBIQ \cite{QUBIQMenze2020}, which contains multi-rater information, we show that models trained with averaged annotation maps can significantly surpass those trained with majority votes and random selections. Leveraging LiTS \cite{LiTSBilicMIA2023} and KiTS \cite{KiTSHellerMIA2021}, we illustrate the synergy with LS and KD.

\subsection{Datasets}
\subsubsection{QUBIQ} is a recent challenge held at MICCAI 2020 and 2021 specifically for the evaluation of inter-rater variability. Following \cite{MRNetJiCVPR2021,UsingSilvaMICCAIWorkshop2021}, we use QUBIQ 2020, which contains 7 segmentation tasks in 4 different CT and MR datasets, including prostate (55 cases, 2 tasks, 6 raters), brain growth (39 cases, 1 task, 7 raters), brain tumor (32 cases, 3 tasks, 3 raters) and kidney (24 cases, 1 task, 3 raters). For each dataset, we calculate the averaged Dice score between each rater and majority votes in Table \ref{tb:ratio} (Appendix \ref{app:tables}). We can see that for certain datasets such as brain tumor task 2, the inter-rater disagreement can be very high. Following \cite{MRNetJiCVPR2021}, we resize all images to $256\times 256$. 

\subsubsection{LiTS} contains 201 high-quality CT scans of liver tumor. 131 cases are used for training and 70 for testing. As ground-truth labels of the test set are not publicly available, we only use the training set. Following \cite{EMKDQinTMI2021}, we resize all images to $512\times 512$ and window the HU values of CT images to [-60, 140].

\subsubsection{KiTS} includes 210 annotated patientsâ€™ CT scans of kidney tumor. Following \cite{EMKDQinTMI2021}, we resize all images to $512\times 512$ and window the HU values of CT images to [-200, 300].

\subsection{Implementation Details}
We adopt various encoders including ResNet50/18 \cite{ResNetHeCVPR2016}, EfficientNetB0 \cite{EfficientNetTanICML2019} and MobileNetV2 \cite{MobileNetV2SandlerCVPR2018}. All encoders are pretrained on ImageNet \cite{ImageNetDengCVPR2009} provided by timm library \cite{timmWightman2019}. We consider both UNet \cite{U-NetRonnebergerMICCAI2015} and DeepLabV3+ \cite{DeepLabV3+ChenECCV2018} as the decoder.

The model is trained with SGD with an initial learning rate of 0.01, momentum of 0.9, weight decay of 0.0005, and the learning rate is decayed with a poly policy. The batch size is set to 8 and the number of epochs is 150 for QUBIQ, 60 for LiTS and KiTS. We leverage a mixture of CE and DMLs weighted by 0.25 and 0.75, respectively. In this paper, we focus on how models can benefit from soft labels. Since the superiority of SDL over CE has been widely proven in the medical imaging community \cite{OptimizationTMI2020,nnU-NetIsenseeNatureMethods2021} and also in our preliminary experiments as shown in Table \ref{tb:ceDMLs} (Appendix \ref{app:tables}), we no longer compare our methods with CE in the experimental sections. 

\subsection{Evaluation Metrics} \label{sec:bdice}
We report both the Dice score and the expected calibration error (ECE) \cite{CalibrationGuoICML2017}. For QUBIQ experiments, we additionally present binarized Dice score (BDice), which is the official evaluation metrics used in QUBIQ challenge. To compute BDice, both predictions and soft labels are thresholded at different probability levels (0.1, 0.2, ..., 0.8, 0.9). Then we compute the Dice score at each level and average these scores with all thresholds. For all experiments, we conduct 5-fold cross validation, making sure that each case is presented in exactly one validation set and report the mean values in the aggregated validation set. 

\subsection{Results on QUBIQ}
In Table \ref{tb:qubiq}, we compare different training methods on QUBIQ using UNet-ResNet50, including hard labels obtained through (i) majority votes \cite{LabelLemayMELBA2023} (ii) random sampling each rater's annotation \cite{ImprovingJensenMICCAI2019}; and soft labels derived from (i) averaging across all annotations \cite{SoftSegGrosMIA2021,UsingSilvaMICCAIWorkshop2021,LabelLemayMELBA2023} (ii) label smoothing \cite{InceptionV2V3SzegedyCVPR2016}. In the literature \cite{SoftSegGrosMIA2021,UsingSilvaMICCAIWorkshop2021,LabelLemayMELBA2023}, annotations are usually averaged with uniform weights. We additionally consider weighting each rater's annotation by its Dice score with respect to majority votes, so that a rater who deviates far from majority votes receives a low weight. Note that for all methods, the Dice score and ECE are computed with respect to majority votes while BDice is calculated as illustrated in Section \ref{sec:bdice}.

Generally, models trained with soft labels not only become more accurate, but also more calibrated. The rich multi-rater information can be captured by simply averaging their annotations. In particular, averaging annotations with uniform weights obtains the highest BDice, while weighted average achieves the highest Dice score. Overall, we find weighted average outperforms other methods, except on Brain tumor T2 where there exists a high degree of disagreement among raters (see also Table \ref{tb:ratio} in Appendix \ref{app:tables}). We hypothesize that improved results could be obtained with more sophisticated weighting schemes such as learned weights \cite{BIWDNGuanAAAI2018}, which is beyond the scope of this paper.

\begin{table}[h]
\centering
\caption{Results on QUBIQ with UNet-ResNet50.} \label{tb:qubiq}
\begin{tabular}{cccccccccc}
\hlineB{2}
Dataset & Metric & Majority & Random & Uniform & Weighted & LS \\ 
\hline
\multicolumn{1}{c}{\multirow{3}{*}{Prostate T1}}
& Dice (\%) & 95.65 & 95.80 & 95.74 & \textbf{95.99} & 95.71 \\
& BDice (\%) & 94.72 & 95.15 & 95.19 & \textbf{95.37} & 94.91 \\
& ECE (\%) & 0.51 & 0.39 & 0.22 & \textbf{0.20} & 0.36 \\
\hline
\multicolumn{1}{c}{\multirow{3}{*}{Prostate T2}}
& Dice (\%) & 89.39 & 88.87 & 89.57 & 89.79 & \textbf{89.82} \\
& BDice (\%) & 88.31 & 88.23 & 89.35 & \textbf{89.66} & 88.85 \\
& ECE (\%) & 0.52 & 0.47 & 0.26 & \textbf{0.25} & 0.41 \\
\hline
\multicolumn{1}{c}{\multirow{3}{*}{Brain growth}}
& Dice (\%) & 91.09 & 90.65 & 90.94 & \textbf{91.46} & 91.23 \\
& BDice (\%) & 88.72 & 88.81 & 89.89 & \textbf{90.40} & 89.88 \\
& ECE (\%) & 1.07 & 0.85 & \textbf{0.27} & 0.34 & 0.41 \\
\hline
\multicolumn{1}{c}{\multirow{3}{*}{Brain tumor T1}}
& Dice (\%) & 86.46 & 87.24 & 87.74 & 87.78 & \textbf{87.84} \\
& BDice (\%) & 85.74 & 86.59 & 86.67 & \textbf{86.92} & 86.91 \\
& ECE (\%) & 0.62 & 0.55 & 0.38 & \textbf{0.36} & 0.37 \\
\hline
\multicolumn{1}{c}{\multirow{3}{*}{Brain tumor T2}}
& Dice (\%) & 58.58 & 48.86 & 52.42 & 61.01 & \textbf{61.23} \\
& BDice (\%) & 38.68 & 49.19 & \textbf{55.11} & 44.23 & 40.61 \\
& ECE (\%) & 0.25 & 0.81 & 0.74 & 0.26 & \textbf{0.22} \\
\hline
\multicolumn{1}{c}{\multirow{3}{*}{Brain tumor T3}}
& Dice (\%) & 53.54 & 54.64 & 53.45 & 56.75 & \textbf{57.01} \\
& BDice (\%) & 52.33 & 53.53 & 51.98 & 53.90 & \textbf{55.26} \\
& ECE (\%) & 0.17 & 0.17 & 0.14 & \textbf{0.09} & 0.11 \\
\hline
\multicolumn{1}{c}{\multirow{3}{*}{Kidney}}
& Dice (\%) & 62.96 & 68.10 & 71.33 & \textbf{76.18} & 71.21 \\
& BDice (\%) & 62.47 & 67.69 & 70.82 & \textbf{75.67} & 70.41 \\
& ECE (\%) & 0.88 & 0.78 & 0.67 & \textbf{0.53} & 0.62 \\
\hline
\multicolumn{1}{c}{\multirow{3}{*}{All}}
& Dice (\%) & 76.80 & 76.30 & 77.31 & \textbf{79.85} & 79.15 \\
& BDice (\%) & 72.99 & 75.59 & \textbf{77.00} & 76.59 & 75.26 \\
& ECE (\%) & 0.57 & 0.57 & 0.38 & \textbf{0.29} & 0.35 \\
\hlineB{2}
\end{tabular}
\end{table}

We compare our method with state-of-the-art (SOTA) methods in Table \ref{tb:qubiq_sota}. In our method, we average annotations with uniform weights for Brain tumor T2 and with each rater's Dice score for all other datasets. Our method that simply averages annotations to produce soft labels obtains superior results compared to methods that adopt complex architectures or training techniques. 

\begin{table}[!h]
\centering
\caption{Comparing with SOTA methods on QUBIQ using UNet-ResNet50. All results are BDice (\%).} \label{tb:qubiq_sota}
\begin{tabular}{ccccccc}
\hlineB{2}
Dataset & Dropout \cite{DropoutGalICML2016} & Multi-head \cite{BIWDNGuanAAAI2018} & MRNet \cite{MRNetJiCVPR2021} & SoftSeg \cite{SoftSegGrosMIA2021,LabelLemayMELBA2023} & Ours \\ 
\hline
Prostate T1 & 94.91 & 95.18 & 95.21 & 95.02 & \textbf{95.37} \\
Prostate T2 & 88.43 & 88.32 & 88.65 & 88.81 & \textbf{89.66} \\
Brain growth & 88.86 & 89.01 & 89.24 & 89.36 & \textbf{90.40} \\
Brain tumor T1 & 85.98 & 86.45 & 86.33 & 86.41 & \textbf{86.92} \\
Brain tumor T2 & 48.04 & 51.17 & 51.82 & 52.56 & \textbf{55.11} \\
Brain tumor T3 & 52.49 & 53.68 & \textbf{54.22} & 52.43 & 53.90 \\
Kidney & 66.53 & 68.00 & 68.56 & 69.83 & \textbf{75.67} \\
All & 75.03 & 75.97 & 76.18 & 76.34 & \textbf{78.14} \\
\hlineB{2}
\end{tabular}
\end{table}

\subsection{Results on LiTS and KiTS}
Wang and Blaschko \cite{JMLsWangarXiv2023} empirically found that a calibrated teacher can distill a more accurate student. In parallel, Menon et al. \cite{AStatisticalMenonICML2021} argued the reason why KD works is because the teacher provides an estimation of Bayes class-probabilities $p^*(y|x)$ and this can lower the variance of the student's empirical loss. In Appendix \ref{app:thm:ce}, we prove $|\mathbb{E}[p^*(y|x) - f(x)]| \leq \mathbb{E}[|\mathbb{E}[y|f(x)]-f(x)|]$. That is, the bias of the estimation is bounded above by the calibration error and this explains why calibration of the teacher would be important for the student. Inspired by this, we apply a recent kernel density estimator (KDE) \cite{KDE-XEPopordanoskaNeurIPS2022} that provides consistent estimation of $\mathbb{E}[y|f(x)]$. We then adopt it as a post-hoc calibration method to replace the temperature scaling to calibrate the teacher in order to improve the performance of the student. For more details of KDE, please refer to Appendix \ref{app:kde}.

In Table \ref{tb:lits_kits}, we compare models trained with hard labels, LS \cite{InceptionV2V3SzegedyCVPR2016} and KD \cite{KDHintonNeurIPSWorkshop2015} on LiTS and KiTS, respectively. For all KD experiments, we use UNet-ResNet50 as the teacher. Again, we obtain noticeable improvements in both the Dice score and ECE. Also note that for UNet-ResNet18 and UNet-EfficientNetB0 on LiTS, the student's Dice score exceeds that of the teacher.

\begin{table}[h]
\centering
\caption{Results on LiTS and KiTS.} \label{tb:lits_kits}
\begin{tabular}{cccccccc}
\hlineB{2}
\multirow{2}{*}{Model} & \multirow{2}{*}{Metrics} & \multicolumn{3}{c}{LiTS} & \multicolumn{3}{c}{KiTS} \\ \cline{3-8} & & Hard & LS & KD & Hard & LS & KD \\ \hline
\multirow{2}{*}{UNet-R50} 
& Dice (\%) & 59.79 & \textbf{60.59} & - & 72.66 & \textbf{73.92} & - \\
& ECE (\%) & 0.51 & \textbf{0.49} & - & 0.39 & \textbf{0.33} & - \\ \hline  
\multirow{2}{*}{UNet-R18} 
& Dice (\%) & 57.92 & 58.60 & \textbf{60.30} & 67.96 & 69.09 & \textbf{71.34} \\
& ECE (\%) & 0.52 & \textbf{0.48} & 0.50 & 0.44 & \textbf{0.38} & 0.44 \\ \hline   
\multirow{2}{*}{UNet-EB0} 
& Dice (\%) & 56.90 & 57.66 & \textbf{60.11} & 70.31 & 71.12 & \textbf{71.73} \\
& ECE (\%) & 0.56 & \textbf{0.47} & 0.52 & 0.39 & \textbf{0.35} & 0.39 \\ \hline   
\multirow{2}{*}{UNet-MB2} 
& Dice (\%) & 56.16 & 57.20 & \textbf{58.92} & 67.46 & 68.19 & \textbf{68.85} \\
& ECE (\%) & 0.54 & \textbf{0.48} & 0.50 & 0.42 & \textbf{0.38} & 0.41 \\ \hline   
\multirow{2}{*}{DL3+-R18} 
& Dice (\%) & 56.10 & 57.07 & \textbf{59.12} & 69.95 & 70.61 & \textbf{70.80} \\
& ECE (\%) & 0.53 & \textbf{0.50} & 0.52 & 0.40 & \textbf{0.38} & 0.40 \\ 
\hlineB{2}
\end{tabular}
\end{table}

\subsection{Ablation Studies}
In Table \ref{tb:DMLs}, we compare SDL with DMLs. For QUBIQ, we train UNet-ResNet50 with soft labels obtained from weighted average and report BDice. For LiTS and KiTS, we train UNet-ResNet18 with KD and present the Dice score. For a fair comparison, we disable KDE in all KD experiments. We find models trained with SDL can still benefit from soft labels to a certain extent, this is because (i) models are trained with a mixture of CE and SDL, and CE is compatible with soft labels (ii) although SDL pushes predictions towards vertices, in a binary segmentation setting, this can still add some regularization effects. However, SDL is significantly outperformed by DMLs. As for DMLs, we find $\overline{\Delta}_{\text{DML,1}}$ is superior to $\overline{\Delta}_{\text{DML,2}}$, and therefore we suggest the use of $\overline{\Delta}_{\text{DML,1}}$ in practice.

In Table \ref{tb:kdloss}, we ablate the contribution of each KD term on LiTS and KiTS with a UNet-ResNet18 student. CE and DML represents adding the CE and DML term between the teacher and the student, respectively. Results shown in the table verify the effectiveness of the proposed loss and KDE method. In Table \ref{tb:bandwidth} (Appendix \ref{app:tables}), we illustrate the effect of bandwidth that controls the smoothness of the kernel density estimation.

\begin{table}
\centering
\caption{Comparing SDL with DMLs.} \label{tb:DMLs}
\begin{tabular}{ccccc}
\hlineB{2}
Dataset & Hard & $\overline{\Delta}_{\text{SDL}}$ & $\overline{\Delta}_{\text{DML,1}}$ & $\overline{\Delta}_{\text{DML,2}}$ \\ 
\hline
QUBIQ & 72.99 & 73.79 & \textbf{76.59} & 76.42 \\
LiTS & 57.92 & 58.12 & \textbf{59.31} & 59.12 \\
KiTS & 67.96 & 68.26 & \textbf{69.29} & 69.07 \\
\hlineB{2}
\end{tabular}
\end{table}
 
\begin{table}
\centering
\caption{Evaluating each KD term on LiTS and KiTS with a UNet-ResNet18 student. All results are the Dice score (\%).} \label{tb:kdloss}
\begin{tabular}{ccccc}
\hlineB{2}
Dataset & Hard & CE & DML & KDE \\ 
\hline
LiTS & 57.92 & 58.23 & 59.31 & \textbf{60.30} \\
KiTS & 67.96 & 68.14 & 69.29 & \textbf{71.34} \\
\hlineB{2}
\end{tabular}
\end{table}

\section{Conclusion}\label{sec:conclusion}
In this work, we introduce the Dice semimetrics losses (DMLs), which are identical to the soft Dice loss (SDL) in a standard setting with hard labels, but are fully compatible with soft labels. Our extensive experiments on the public QUBIQ, LiTS and KiTS benchmarks support that incorporating soft labels leads to higher Dice score and lower calibration error, indicating that these losses can find wide application in diverse medical image segmentation problems. Hence, we suggest to replace the existing implementation of SDL with DMLs. 

\section*{Acknowledgements}
We acknowledge support from the Research Foundation - Flanders (FWO) through project numbers G0A1319N and S001421N, and funding from the Flemish Government under the Onderzoeksprogramma Artifici\"{e}le Intelligentie (AI) Vlaanderen programme. The resources and services used in this work were provided by the VSC (Flemish Supercomputer Center), funded by the Research Foundation - Flanders (FWO) and the Flemish Government.

%\clearpage

\bibliographystyle{splncs04}
\bibliography{miccai2023}

\clearpage

\appendix
\section{Theorem \ref{thm:rhodice}} \label{app:thm_semi}
\begin{proof}
We omit the subscript since the proof for $\overline{\Delta}_{\text{DML,1}}$ and $\overline{\Delta}_{\text{DML,2}}$ are identical. Note that $0\leq \overline{\Delta}_{\text{DML}}=\frac{\overline{\Delta}_{\text{JML}}}{2- \overline{\Delta}_{\text{JML}}}\leq \overline{\Delta}_{\text{JML}}\leq 1$ and $\overline{\Delta}_{\text{JML}}$ satisfies the triangle inequality.
    \begin{align}
        & \overline{\Delta}_{\text{DML}}(a,c) \leq \rho(\overline{\Delta}_{\text{DML}}(a,b)+\overline{\Delta}_{\text{DML}}(b,c)) \\
        \Rightarrow & \frac{\overline{\Delta}_{\text{JML}}(a,c)}{2- \overline{\Delta}_{\text{JML}}(a,c)} \leq \frac{\rho\overline{\Delta}_{\text{JML}}(a,b)}{2- \overline{\Delta}_{\text{JML}}(a,b)} + \frac{\rho\overline{\Delta}_{\text{JML}}(b,c)}{2- \overline{\Delta}_{\text{JML}}(b,c)} \\
        \Rightarrow & \frac{\overline{\Delta}_{\text{JML}}(a,b) + \overline{\Delta}_{\text{JML}}(b,c)}{2 - \overline{\Delta}_{\text{JML}}(a,b) - \overline{\Delta}_{\text{JML}}(b,c)} \leq \frac{\rho\overline{\Delta}_{\text{JML}}(a,b)}{2- \overline{\Delta}_{\text{JML}}(a,b)} + \frac{\rho\overline{\Delta}_{\text{JML}}(b,c)}{2- \overline{\Delta}_{\text{JML}}(b,c)} \\
        \Rightarrow & \overline{\Delta}_{\text{JML}}(a,b) \Big(\frac{1}{2 - \overline{\Delta}_{\text{JML}}(a,b) - \overline{\Delta}_{\text{JML}}(b,c)} - \frac{\rho}{2-\overline{\Delta}_{\text{JML}}(a,b)}\Big) \leq \nonumber \\
        & \overline{\Delta}_{\text{JML}}(b,c) \Big(\frac{\rho}{2-\overline{\Delta}_{\text{JML}}(b,c)} - \frac{1}{2-\overline{\Delta}_{\text{JML}}(a,b)-\overline{\Delta}_{\text{JML}}(b,c)} \Big) \label{eq:DMLleftright}.
    \end{align}
    Assume that $\overline{\Delta}_{\text{DML}}(a,b) + \overline{\Delta}_{\text{DML}}(b,c) \leq \frac{1}{\rho}$, otherwise $\rho$-relaxed triangle inequality trivially holds. Now we show that the left-hand side of Eq. (\ref{eq:DMLleftright}) is no greater than 0.
    \begin{align}
    & \frac{\rho}{2-\overline{\Delta}_{\text{JML}}(a,b)} \geq \frac{1}{2 - \overline{\Delta}_{\text{JML}}(a,b) - \overline{\Delta}_{\text{JML}}(b,c)} \\
    \Rightarrow & \frac{\rho+\overline{\Delta}_{\text{DML}}(a,b)}{1+\overline{\Delta}_{\text{DML}}(a,b)} \geq \frac{1+(1+\rho)\overline{\Delta}_{\text{DML}}(b,c)}{1+\overline{\Delta}_{\text{DML}}(b,c)} \label{eq:DMLright1} \\
    \Rightarrow & \frac{\rho+\overline{\Delta}_{\text{DML}}(a,b)}{1+\overline{\Delta}_{\text{DML}}(a,b)} \geq \frac{1+(1+\rho)(\frac{1}{\rho}-\overline{\Delta}_{\text{DML}}(a,b))}{1+(\frac{1}{\rho}-\overline{\Delta}_{\text{DML}}(a,b))} \label{eq:DMLright2} \\
    \Rightarrow & \rho^2 -\rho - 1 + \rho^2\overline{\Delta}_{\text{DML}}^2(a,b) \geq 0 \\
    \Rightarrow & \rho \geq \frac{1+\sqrt{5}}{2} \approx 1.62.
    \end{align}
    where in Eq. (\ref{eq:DMLright2}), we use the fact the right hand side of Eq. (\ref{eq:DMLright1}) is an increasing function of $\overline{\Delta}_{\text{DML}}(b,c)$. Similarly, we can show that the right-hand side of Eq. (\ref{eq:DMLleftright}) is no less than 0 when $\rho \geq \frac{1+\sqrt{5}}{2}$. Note that for $a=[0,1], b=[1,1], c=[1,0]$, $\overline{\Delta}_{\text{DML}}(a,c)=1, \overline{\Delta}_{\text{DML}}(a,b)=\overline{\Delta}_{\text{DML}}(b,c)=\frac{1}{3}$, so $\rho$ should at least be $\frac{3}{2}$. $\hfill\square$
    \end{proof}

\section{Theorem \ref{thm:ce}} \label{app:thm:ce}
\begin{theorem} \label{thm:ce}
 $|\mathbb{E}[p^*(y|x) - f(x)]| \leq \mathbb{E}[|\mathbb{E}[y|f(x)]-f(x)|]$.
\end{theorem}
\begin{proof}
Subscripts are omitted for simplicity and all expectations are over $x, y \sim \mathbb{P}_{x,y}$, the (unknown) data generating distribution. Note that $\mathbb{E}[|\mathbb{E}[y|f(x)]-f(x)|]$ is the definition of the calibration error. The proof is similar to \cite{OnPopordanoskaMICCAI2021}. 
\begin{align}
    & |\mathbb{E}[p^*(y|x) - f(x)]| \\
    = & |\mathbb{E}[p^*(y|x) - \mathbb{E}[y|f(x)] + \mathbb{E}[y|f(x)] - f(x)]| \\
    = & |\mathbb{E}[p^*(y|x)] - \mathbb{E}[\mathbb{E}[y|f(x)]] + \mathbb{E}[\mathbb{E}[y|f(x)] - f(x)]| \\
    = & |\underbrace{\mathbb{E}[p^*(y|x) - y]}_{=0} + \mathbb{E}[\mathbb{E}[y|f(x)] - f(x)]| \\
    \leq & \mathbb{E}[|\mathbb{E}[y|f(x)] - f(x)|]. \label{eq:jensen} \\
\end{align}
In Eq. (\ref{eq:jensen}), we apply Jensen's inequality due to the convexity of the absolute value. $\hfill\square$
\end{proof}

\section{Tables} \label{app:tables}
\begin{table}[h] 
\centering
\caption{The averaged Dice score between each rater and majority votes. D1: Prostate T1, D2: Prostate T2, D3: Brain growth T1, D4: Brain tumor T1, D5: Brain tumor T2, D6: Brain tumor T3, D7: Kidney T1.} \label{tb:ratio}
\begin{tabular}{cccccccc} \hlineB{2}
Dataset & D1 & D2 & D3 & D4 & D5 & D6 & D7 \\ 
\hline
\# Raters & 6 & 6 & 7 & 3 & 3 & 3 & 3 \\
Dice (\%) & 96.49 & 92.17 & 91.20 & 95.44 & 68.73 & 92.71 & 97.41 \\
\hlineB{2}
\end{tabular}
\end{table}

\begin{table}[h] 
\centering
\caption{Comparing CE with DMLs on QUBIQ, LiTS and KiTS using UNet-ResNet50. All results are the Dice score (\%).} \label{tb:ceDMLs}
\begin{tabular}{cccc} \hlineB{2}
Dataset & QUBIQ & LiTS & KiTS \\ \hline
CE & 74.97  & 57.76 & 68.52 \\
DML & \textbf{76.80} & \textbf{59.79} & \textbf{72.66} \\
\hlineB{2}
\end{tabular}
\end{table}

\begin{table}[h] 
\centering
\caption{Comparing different bandwidths on LiTS and KiTS with a UNet-ResNet18 student. All results are the Dice score (\%).} \label{tb:bandwidth}
\begin{tabular}{cccccccc} \hlineB{2}
Bandwidth & 0 & 5e-5 & 1e-4 & 5e-4 & 1e-3 & 5e-3 & 1e-2 \\ \hline
LiTS & 59.31 & 59.05 & 59.07 & 59.97 & \textbf{60.30} & 59.56 & 59.62 \\
KiTS & 69.29 & 69.05 & 69.80 & 70.41 & \textbf{71.34} & 68.75 & 69.18 \\
\hlineB{2}
\end{tabular}
\end{table}

\section{Calibrated Knowledge Distillation} \label{app:kde}
Wang and Blaschko \cite{JMLsWangarXiv2023} empirically found that a calibrated teacher can distill a more accurate student. In parallel, Menon et al. \cite{AStatisticalMenonICML2021} argued the reason why KD works is because the teacher provides an estimation of Bayes class-probabilities $p^*(y|x)$ and this can lower the variance of the student's empirical loss. Theorem \ref{thm:ce} implies the bias of the estimation is bounded above by the calibration error. Thus, if the teacher's calibration error is reduced, it can provide a closer estimation of Bayes class-probabilities. Together with the argument in \cite{AStatisticalMenonICML2021}, this shows why the teacher's calibration is important for the student and hence answers many interesting observations in KD:
\begin{itemize}
    \item Why is smoothing a teacher's predictions with temperature scaling useful \cite{KDHintonNeurIPSWorkshop2015}? Since modern neural networks are often over-confident \cite{CalibrationGuoICML2017}, temperature scaling is an effective post-hoc calibration method \cite{CalibrationGuoICML2017,LTSDingICCV2021,Meta-CalMaICML2021,OnPopordanoskaMICCAI2021,PostRousseauISBI2021} to decrease a teacher's calibration error. 
    \item Why does a stronger teacher not necessarily distill a better student \cite{DISTHuangNeurIPS2022}? Because a stronger teacher, such as a deeper network, although might be more accurate, could be less calibrated \cite{CalibrationGuoICML2017}.
    \item Why can label smoothing (LS) increase a model's calibration but a teacher trained with LS could hurt the performance of the student \cite{WhenMullerNeurIPS2019}? We believe after the teacher is trained with LS, the optimal temperature in KD should generally be lower in order to achieve a low calibration error. If one still uses a high temperature, the teacher might be overly-smoothed and become under-confident.
\end{itemize}

Inspired by the fact that we can improve KD by decreasing the teacher's calibration error, we propose to use $\mathbb{E}[y|f(x)]$ as a distillation signal to supervise the student. Note that by the definition of the calibration error, $\mathbb{E}[y|f(x)]$ is the optimal recalibration mapping of $f(x)$ that will give 0 calibration error. Moreover, given only access to $f(x)$, predicting $\mathbb{E}[y|f(x)]$ is the Bayesian optimal classifier. 

Nevertheless, $\mathbb{E}[y|f(x)]$ cannot be computed exactly because it depends on the unknown data generating distribution. Hence, we adopt a kernel density estimator (KDE) that is proven to be consistent \cite{KDE-XEPopordanoskaNeurIPS2022}:
\begin{equation} \label{eq:kde}
     \mathbb{E}[y|f(x)] \approx \widehat{\mathbb{E}[y|f(x)]} = \frac{\sum_{i=1}^{n}k_{\text{kernel}}(f(x), f(x_i))y_i}{\sum_{i=1}^{n}k_{\text{kernel}}(f(x), f(x_i))},
\end{equation}
For binary classification, $k_{kernel}$ is a Beta kernel:
\begin{equation}
    k_{Beta}(f(x_j),f(x_i)) = f(x_j)^{\alpha_i-1} (1-f(x_j))^{\beta_i-1} \frac{\Gamma(\alpha_i + \beta_i)}{\Gamma(\alpha_i)\Gamma(\beta_i)},
\end{equation}
and for the multiclass setting, $k_{kernel}$ is a Dirichlet kernel:
\begin{equation}
    k_{Dir}(f(x_j),f(x_i)) = \frac{\Gamma(\sum_{k=1}^K \alpha_{ik})}{\prod_{k=1}^K \Gamma(\alpha_{ik})} \prod_{k=1}^K f(x_j)_{k}^{\alpha_{ik}-1}
\end{equation}
with $\Gamma(\cdot)$ the gamma function, $\alpha_{i} = \frac{f(x_i)}{h} + 1$ and $\beta_i=\frac{1-f(x_i)}{h} + 1$, where $h \in \mathbb{R}_{>0}$ is a bandwidth parameter that controls smoothness of the kernel density estimation. Like the temperature in temperature scaling, the estimation becomes smoother when we increase $h$.

The complexity of KDE is $O(n)$ for a single pixel, leading to $O(n^2)$ overall complexity. With gradient descent training, KDE is estimated using a mini-batch. Nonetheless, in semantic segmentation, $n=B\times H \times W$ and it can be a large number, where $B$ is the batch size, $H$ and $W$ are height and width, respectively. To decrease the amount of computation, for each training data batch, we randomly sample $n_{\text{key}}$ key points such that $n_{\text{key}} \ll n$. Consequently, for each pixel $x$, we only perform the kernel computation with respect to these key points. Hence, the complexity is considerably reduced from $O(n^2)$ to $O(n\times n_{\text{key}})$, resulting a negligible extra cost. 

Another consideration in semantic segmentation is that labels are usually highly unbalanced. When sampling $n_{key}$ key points from a $n_{batchsize}\times H \times W$ data batch, it is quite likely that some classes are omitted. Consequently, during kernel computation, all predictions with respect to these classes will become 0. To overcome this, if the number of unique classes within a training data batch is $n_{unique}$, we sample each unique class with $\lceil \frac{n_{key}}{n_{unique}} \rceil$. Besides, since medical images are dominated by background pixels, we empirically find that it suffices to only apply KDE to misclassified pixels as well as pixels that are near the boundary.

Through the kernel computation, we broadcast the information contained in these key points to all pixels. We utilize both labels and predictions of these key points to adjust each pixel's predictions. In Table \ref{tb:kdloss}, we ablate the role of KDE. In Table \ref{tb:bandwidth}, we show the effect of bandwidth on LiTS and KiTS with a UNet-ResNet18 student. We notice the performance of KDE is sensitive to the bandwidth.

\section{The Compatible Tversky Loss and the Compatible Focal Tversky Loss} \label{app:tversky}
The Tversky index is defined as 
\begin{equation}
    \frac{|v|}{|v| + \alpha (|x| - |v|) + \beta (|y| - |v|)}
\end{equation}
where $\alpha, \beta \geq 0$ controls the magnitude of penalties for false-positives and false-negatives, respectively. With $\alpha = \beta = 0.5$, the Tversky index becomes the Dice score; with $\alpha = \beta = 1$, it simplifies to the Tanimoto coefficient.

Adopting the same idea as SDL and SJL, the soft Tversky loss (STL) \cite{SoftTverskyLossSalehiMICCAIWorkshop2017} is written as 
\begin{equation}
     \overline{\Delta}_{\text{STL}}: x\in[0,1]^p, y\in\{0,1\}^p \mapsto 1 - \frac{\langle x, y\rangle}{\alpha\|x\|_1+\beta\|y\|_1+(1-\alpha-\beta)\langle x, y\rangle}.
\end{equation}
Since SDL as a special case of STL is incompatible with soft labels, so is STL. The key is to write $|v|$ as $\frac{1}{2}(\|x+y\|_1-\|x-y\|_1)$, so we can define the compatible Tversky loss (CTL) as
\begin{align}
     \overline{\Delta}_{\text{CTL}}&: x\in[0,1]^p, y\in [0,1]^p \\
     & \mapsto 1 - \frac{\|x+y\|_1-\|x-y\|_1}{2\alpha\|x\|_1+2\beta\|y\|_1+(1-\alpha-\beta)(\|x+y\|_1-\|x-y\|_1)}.
\end{align}
Due to the asymmetric nature of the Tversky index, $\overline{\Delta}_{\text{CTL}}$ does not satisfy symmetry, thus cannot be a semimetric. However, it is compatible with soft labels.
\begin{theorem}
$\overline{\Delta}_{\text{CTL}}$ satisfies reflexivity and positivity.
\end{theorem}
\begin{proof}
Let $S_1=\{i:x_i\geq y_i\}$ and $S_2=\{i:x_i<y_i\}$.
\begin{align}
& \overline{\Delta}_{\text{CTL}}(x,y) = 0 \\
\Rightarrow & (\alpha-\beta)\|x\|_1+(-\alpha+\beta)\|y\|_1+(\alpha+\beta)\|x-y\|_1=0 \\
\Rightarrow & (\alpha-\beta)\sum_{i\in S_1}x_i + (\alpha-\beta)\sum_{i\in S_2}x_i + (-\alpha+\beta)\sum_{i\in S_1}y_i + (-\alpha+\beta)\sum_{i\in S_2}y_i \\
& (\alpha+\beta)\sum_{i\in S_1}x_i - (\alpha+\beta)\sum_{i\in S_2}x_i - (\alpha+\beta)\sum_{i\in S_1}y_i + (\alpha+\beta)\sum_{i\in S_2}y_i = 0 \\
\Rightarrow & \alpha \sum_{i\in S_1}(x_i-y_i) + \beta \sum_{i\in S_2}(y_i-x_i) = 0
\end{align}
where the last equality holds if and only if $x=y$. $\hfill\square$
\end{proof}

Again, following a similar proof as given in \cite{JMLsWangarXiv2023}, we can show that CTL is identical to STL in a standard setting with hard labels.
\begin{theorem}
$\forall x\in[0,1]^p,\  y\in \{0,1\}^p$ and $x\in \{0,1\}^p,\  y\in [0,1]^p$, $\overline{\Delta}_{\text{STL}}=\overline{\Delta}_{\text{CTL}}$. $\exists x, y \in [0,1]^p, \overline{\Delta}_{\text{STL}}\neq\overline{\Delta}_{\text{CTL}}$.
\end{theorem}

We can add a focal term as in the focal Tversky loss \cite{FocalLossLinTPAMI2018,FocalTverskyLossAbrahamISBI2019} and we call it the compatible focal Tversky loss (CFTL):
\begin{equation}
    \overline{\Delta}_{\text{CFTL}}: x\in[0,1]^p, y\in [0,1]^p \mapsto \overline{\Delta}_{\text{CTL}}^{\gamma}
\end{equation}
where $\gamma$ is the focal term. With $\gamma > 1$, CFTL focuses more on less accurate predictions that have been misclassified. In Figure \ref{fig:cftl}, we plot the loss value of CFTL with $\alpha=0.7, \beta=0.3$ when the soft label is 0.8. With increased $\gamma$, the loss value flattens when the prediction is near the ground-truth and increases more rapidly when the prediction is far away from the soft label. Note that we can also add the focal term to JMLs and DMLs.

Together with \cite{JMLsWangarXiv2023}, variants of commonly used regional losses, e.g. the soft Jaccard loss, the soft Dice loss, the soft Tversky loss and the focal Tversky loss, are proposed. These new variants are identical to the original versions in a standard setting with hard labels, but are fully compatible with soft labels. Given that soft labels are pervasive in machine learning, we recommend to replace the existing implementation of these losses with our new variants.

\begin{figure}
\centering
    \includegraphics[width=0.9\textwidth]{figures/tversky.pdf}
    \caption{The loss value of CFTL with $\alpha=0.7, \beta=0.3$ when the soft label is 0.8.}
    \label{fig:cftl}
\end{figure}

\end{document}