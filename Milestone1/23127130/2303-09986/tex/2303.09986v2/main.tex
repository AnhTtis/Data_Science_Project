%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper
\usepackage{cite}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
% \usepackage{amsthm}
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

% Title suggestion
\title{\LARGE \bf
Towards AI-controlled FES-restoration of movements: Learning cycling stimulation pattern with reinforcement learning}  

\author{Nat Wannawas$^1$,
        A. Aldo Faisal$^{1,2}$
\thanks{$^{1}$Brain \& Behaviour Lab, Imperial College London, United Kingdom, SW7 2AZ. (nat.wannawas18@imperial.ac.uk). $^2$Chair of Digital Health, Universit√§t Bayreuth, Germany 95445. (aldo.faisal@imperial.ac.uk).}
}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Functional electrical stimulation (FES) has been increasingly integrated with other rehabilitation devices, including robots. FES cycling is one of the common FES applications in rehabilitation, which is performed by stimulating leg muscles in a certain pattern. The appropriate pattern varies across individuals and requires manual tuning which can be time-consuming and challenging for the individual user. Here, we present an AI-based method for finding the patterns, which requires no extra hardware or sensors. Our method has two phases, starting with finding model-based patterns using reinforcement learning and detailed musculoskeletal models. The models, built using open-source software, can be customised through our automated script and can be therefore used by non-technical individuals without extra cost. Next, our method fine-tunes the pattern using real cycling data. We test our both in simulation and experimentally on a stationary tricycle. In the simulation test, our method can robustly deliver model-based patterns for different cycling configurations. The experimental evaluation shows that our method can find a model-based pattern that induces higher cycling speed than an EMG-based pattern. By using just 100 seconds of cycling data, our method can deliver a fine-tuned pattern that gives better cycling performance. Beyond FES cycling, this work is a showcase, displaying the feasibility and potential of human-in-the-loop AI in real-world rehabilitation.
\end{abstract}
Index Terms--Electrical Stimulation, FES, Reinforcement Learning, FES cycling, stimulation pattern

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
% Functional Electrical Stimulation (FES) induces muscle contraction by stimulating the muscle with low-energy electrical signals, allowing individuals with paralyses to perform physical exercises. 
Functional Electrical Stimulation induced cycling (FES cycling) in paralysed individuals is one of the most widely performed exercises that can help prevent adverse health effects such as muscle atrophy and improve cardiovascular fitness \cite{Ferrante2008, Bo2017}. FES cycling is also integrated with robotic exoskeleton to provide gait rehabilitation training \cite{Mazzoleni2017}. FES cycling is achieved by repetitively stimulating leg muscles in a certain pattern. This stimulation pattern plays a major role in delivering smooth and fast cycling.

Different individuals require different stimulation pattern. The process of finding an appropriate pattern is usually performed during the system setup. In many practical situations, finding the pattern is still a manual process based on trial-and-error and clinicians' experiences \cite{Hunt2004,Kim2008,Mcdaniel2017}. This manual process can be time-consuming and challenging, especially for a lone user setting up the system at home. Several methods for finding the stimulation patterns have been proposed. One of them is to mimic the natural order of muscle activation which can be observed through electromyogram (EMG) of healthy subjects during voluntary cycling \cite{Hunt2004,Petrofsky2003,Wakeling2009,Lopes2014,Metani2016}. This EMG-based method is not easy to implement as it requires an experimental setup and a healthy subject with similar body size to the target individual. In addition, the obtained pattern may not be optimal for the target individual \cite{Schmoll2022}. Another method is to use a biomechanical model to compute the angles where certain muscles produce positive torque. Several model-based methods based on different cycling models and cost functions were developed. Chen et al \cite{Chen1997} used a five-bar linkage cycling model and determined the pattern from the flexion and extension of hip and knee joints. Idso \cite{Idso2004} used similar linkage model but determined the pattern based on a metabolic cost. Li et al. \cite{Li2009} added muscle routing details into the linkage model and used multilayer perceptrons to approximate the angles where the muscles induce positive torque on the crank. These model-based studies, however, rely heavily on the models' accuracy, and the experimental evaluations are not reported.

Several methods have reported successes in real-world experiments. Wiesener et Schauer \cite{Wiesener2017} proposed a method that transforms thigh and knee angles into a fixed range on which the seat-position-independent pattern can be determined. Several other methods utilise torque feedback measured through force sensors attached to the pedals. Ambrosini et al. \cite{Ambrosini2014} used both EMG and torque feedback and determined the pattern based on the regions where EMG and positive torque overlap. Maneski et al. \cite{Maneski2017} and Schmoll et al. \cite{Schmoll2022} proposed similar methods in which the pedal force was recorded from the passive (motorised or human-assisted) and active (stimulated) sessions. The angles where the stimulated muscles produce positive torque are then revealed by removing the passive force from the total force. These methods are effective but require experimental sessions and force sensors. These requirements can be challenging for at-home practices.

Here, we present a method for finding the stimulation pattern that requires neither force sensor nor motorised ergometer. Our method utilises reinforcement learning (RL), a machine learning algorithm that learns to do tasks by interacting with environments. The applications of RL to FES controls have been studied in \cite{Wannawas2021,Wannawas2022,Fischer2021,Abreu2022,Wannawas2023a}. A closely-related study is our previous work \cite{Wannawas2021} which focuses on controlling the cycling speed. That RL setup, however, has difficulties in delivering clean and balanced patterns that behave well on conventional FES cycling systems. Additionally, it requires too large amount of data to gather in the real world.

This work further improves the previous setup to learn well-behave patterns and provides a strategy to learn further in the real world with a handful amount of data. The our method's processes of learning the patterns have two phases. The first phase is to govern model-based patterns in which an open-source software is used to build detailed musculoskeletal models. The second phase is to collect real cycling data which are used to fine-tune the patterns. Our method is compatible with conventional FES cycling systems and aimed to be able to use by the cyclists themselves. We evaluate our method in a real-world setting in which the patterns governed by our and EMG-based methods as well as their cycling performances compared.

\section{Method}
The objective of our method is to find a stimulation pattern which is crank angle intervals in which the stimulated muscles are in \emph{ON} states. Our method has two phases: a model-based and a fine-tuning phases (Fig.\ref{fig:Framework}a). The process starts in the model-based phase with the building of a customised musculoskeletal model corresponding to the real cyclist and the cycling setup. After that, reinforcement learning (RL) is applied to find a model-based pattern. The process then proceeds into the fine-tuning phase which alternates between cycling data collection and pattern update. At the beginning, the cycling data corresponding to the model-based pattern are collected and used to update the pattern. The updated pattern is then applied to collect the data in the next round. These process occurs repeatedly for a fixed number of times or until the pattern converges.

\begin{figure*}[htbp!]
    \begin{center}
    \includegraphics[width=1.95\columnwidth]{figures/fig1.png}
    \caption{(a) The diagram showing the pipeline of 2-phase our method. (b) Our OpenSim cycling model in a generic configuration. (c) The experimental setup using BerkelBike placed on a cycling trainer. The stimulation is applied to \emph{quadriceps} and \emph{hamstrings} via self-adhesive surface electrodes.
    \label{fig:Framework}}
    \vspace{-5mm}
    \end{center}
\end{figure*}

\subsubsection{Musculoskeletal Model}
As the quality of a model-based pattern depends heavily on the model, we opt to build our cycling models by using OpenSim \cite{Seth2011}, an open-source biomechanical simulation software that can produce high quality models. In addition, Opensim has a Python application programming interface (API) that facilitates the integration with machine learning libraries (such as our own RL software). The API also enables a customised model to be conveniently created through a single command, thereby obviating the need for users' OpenSim skill.

Fig.\ref{fig:Framework}b shows our OpenSim cycling model in a generic configuration. The model has a cycling crankset placed on a cycling trainer that exerts rolling resistance through friction at the contact point. The pedals of the crankset are attached to the feet of a lower-limb musculoskeletal model, an OpenSim built-in model, sitting on a seat. The lower-limb model has 18 Hill-type muscles; 6 of them which are quadriceps, hamstrings, and gluteus maximus on both legs are stimulated. The muscles' activation delay is set to 100 ms \cite{Kralj1973}. Note that we re-route quadriceps muscles and modify the knee joint of the original built-in model so that the legs' lengths can be easily adjusted to match a real person. The lower-limb model has 4 movable joints: hip and knee joints of both legs. The ankle joints are fixed at $90^\circ$. The lumbar joint, which is equal to the seat angle, is adjustable. This cycling model can be customised to match the real cyclist and cycling setup. The customisation parameters are the vertical and horizontal distances between the crank centre and the hip, the crank arm's and legs' lengths, and the seat angle.

\subsubsection{Reinforcment Learning (RL)}
Here, we use RL, a machine learning algorithm with a learning agent that learns to control an environment by interacting with it, to learn the stimulation pattern. The interactions occur in a discrete time fashion, described as follows. At the beginning of each timestep, the agent observes an environment's state $\textbf{s}$ and selects an action $\textbf{a}$ based on its policy $\pi$. The action is applied to the environment, causing its to be in a new state $\textbf{s'}$. The agent then receives an immediate reward $r$ and observes the new state. This interaction experience is collected as a tuple of $\textbf{s}, \textbf{a}, r, \textbf{s'}$ which is stored in a replay buffer $\mathcal{D}$. This experience tuple is used to learn an optimal policy $\pi^*$ that maximises a return $R$--the sum of discounted rewards.

The learning mechanisms are different across different RL algorithms. Here, we choose an algorithm called Soft Actor-Critic (SAC) \cite{Haarnoja2019_1}, one of the state-of-the-art RL algorithms with successes in real-world control tasks. SAC has two components: an actor and a critic. In simple terms, the critic learns to estimate the expected return of a state-action pair, known as Q value ($Q(\boldsymbol{s},\boldsymbol{a})$). The Q value is used to adjust the actor's policy $\pi$ by increasing the its probability of choosing an action with high Q value. Both actor and critic are usually parameterised by neural networks with parameters $\theta_\pi$ and $\theta_Q$, respectively. In SAC, the parameters $\theta_Q$ and $\theta_\pi$ are optimise with gradient descend to minimise cost functions
\begin{equation}
    \label{eq:q}
    \begin{split}
        J_Q =\;&\mathbb{E}_{s,a,r,s'\sim\mathcal{D}}[Q_{\theta_Q}(s,a) - (r(s,a)\\&+\gamma Q_{\theta_Q}(s,\pi(s')))-log(\pi_{\theta_\phi}(a'|s'))]^2,
    \end{split}
\end{equation}
where $\gamma\in[0,1)$ is a discount factor, and
\begin{equation}
    \label{eq:p}
    \begin{split}
        J_\pi =\;&\mathbb{E}_{s\sim\mathcal{D}}[\mathbb{E}_{a\sim\pi_{\theta_\pi}}[log(\pi_{\theta_\phi}(s,a))-Q_{\theta_Q}(s,a)]],
    \end{split}
\end{equation}
respectively.

\subsubsection{RL problem formulation}
Here, we present the formulation of RL setup to learn the model-based stimulation pattern. The process of learning the model-based pattern involves the interaction with the OpenSim cycling model whose state vector $s$ comprises the crank angle $\theta_c$ in $rad$ and the cadence $\dot{\theta}_c$ in $rad/s$. The model is controlled by apply the stimulation on leg muscles: \emph{quadriceps}, \emph{hamstrings}, and \emph{gluteus maximus}. The control vector, which comprises the normalised stimulation intensities, is $\boldsymbol{u}\in\mathbb{R}_{[0,1]}^6$. Note that in a two-muscle cycling case, \emph{gluteus maximus} muscles are excluded, and $\boldsymbol{u}$ becomes a 4-element vector.

The setup on the RL agent side is as follows. To reduce the action search space, we assume that the left and right legs are symmetric. This assumption allows us to learn the pattern of only one leg which tremendously reduces the required amount of interactions and computing time. The action vector at time step $t$ becomes $\boldsymbol{a}_t\in\mathbb{R}_{[0,1]}^3$ for the three-muscle case and $\boldsymbol{a}\in\mathbb{R}_{[0,1]}^2$ for the two-muscle case. The observed state vector $\boldsymbol{s}_t$ is $[sin(\theta_{c,t}), cos(\theta_{c,t}), \dot{\theta}_{c,t}, \boldsymbol{a}_{t-1}]^T$. Note that we represent the crank angle in the form of its sine and cosine because we want the states at $\lim_{\theta_c\to2\pi+}\theta_c$ and $\lim_{\theta_c\to2\pi-}\theta_c$ to be close in the state space and smoothly continuing after each full revolution. The reward function is
\begin{equation}
r_t=f_r(\dot{\theta}_{c,t+1}, \boldsymbol{a}_t) = \dot{\theta}_{c,t+1} - \beta\sum_{i=1}^n a_{i,t}^2,
\label{eq:reward}
\end{equation}
where $\dot{\theta}_{c,t+1}$ is the cadence at the next time step after $a_t$ was applied; $a_{i,t}$ is the stimulation on muscle $i$ at time $t$; $\beta$ is an action penalty weight which is set to $1.0$; and $n$ is the number of stimulated muscles. The intuition of the reward function is that we want to find the stimulation pattern that produces forward torque on the crank. Given a constant rolling resistance, high cadence ($\dot{\theta}_{c,t+1}$) should be observed if the forward torque is effectively induced in each revolution. The action penalty term encourages the efficient uses of muscles to minimise the fatigue.

The interaction between the RL agent and the model is described as follows. Even though we assume the symmetry between both legs and learn only the right leg's pattern, we have to control the stimulation on them simultaneously, requiring us to determine the left's leg stimulation based one the right leg's pattern. As the left crank is equivalent to the right crank rotated by $180^\circ$, the left leg's stimulation $\boldsymbol{a}_{l,t}$ is equivalent to the right leg's stimulation corresponding to a state vector $[sin(\theta_{c,t}+\pi), cos(\theta_{c,t}+\pi), \dot{\theta}_{c,t}, \boldsymbol{a}_{l,t-1}]^T$. To formalise this, the control vector $\boldsymbol{u_t}$ for each timestep is obtained by concatenating $\boldsymbol{a_t}$ and $\boldsymbol{a}_{l,t}$:
\begin{equation}
    \begin{split}
        a_t &= \pi_{\theta_\pi}(\boldsymbol{s}_t)\\
        a_{l,t} &= \pi_{\theta_\pi}(\boldsymbol{s}_{l,t})\\
        u_t &= [a_t;a_{l,t}],
    \end{split}
\end{equation}
where
\begin{equation}
    \begin{split}
        \boldsymbol{s}_t &= [sin(\theta_{c,t}),cos(\theta_{c,t}),\dot{\theta}_{c,t},\boldsymbol{a}_{t-1}]^T\\
        \boldsymbol{s}_{l,t} &= [-sin(\theta_{c,t}),-cos(\theta_{c,t}),\dot{\theta}_{c,t},\boldsymbol{a}_{l,t-1}]^T.
    \end{split}
\end{equation}
Following this, we can obtain two immediate rewards, $r_t=f_r(\dot{\theta}_{c,t+1},a_t)$ and $r_{l,t}=f_r(\dot{\theta}_{c,t+1},a_{l,t})$, and two experience tuples, $(s_t,a_t,r_t,s_{t+1})$ and $(s_{l,t},a_{l,t},r_{l,t},s_{l,t+1})$, from an interaction in one time step.

The RL agent is trained in an episodic fashion. Each episode starts a random crank angle with zero initial cadence. Each episode has 100 timesteps with the size of 50 ms. The agent's neural network parameters $\boldsymbol{\theta}_\phi$ and $\boldsymbol{\theta}_Q$ are updated at the end of each timestep. The performance test episode, which is an episode without the random actions for exploration, was carried out every 5 episodes. The training stops when the performance reaches a plateau.

\subsubsection{Offline learning on the real data}
The RL problem formulation described earlier has an RL agent interacting directly with the environment, the OpenSim cycling model. In real world settings, however, the direct interaction may have the following issues. Firstly, the direct interaction requires an FES stimulator with low-latency interface with a computer. This may not be the case for some commercially available stimulators with built-in control units designed to run specific, usually crank-angle-based, programs. Secondly, the direct interaction may have a safety issue because the RL agent can apply any forms of the stimulation at any situations to explore their outcomes. This can make, for example, the cycling to stop suddenly and cause injury.

To avoid these issues, we collect the real cycling data from cycling sessions controlled by conventional pattern with a single \emph{ON} interval. The collected data are then converted into experience tuples in the same way as described earlier. This learning setting, in RL context, is called offline learning, a learning setting where an RL agent learns from a fixed set of experiences that are not collected by the agent itself. Offline learning poses challenges in learning an optimal policy as, for example, the agent may think that an action that is never applied during the data collection is good.

To successfully perform offline learning, we adopt one of the state-of-the-art offline RL algorithm called conservative Q learning (CQL) \cite{Kumar2020}. CQL provides the modification of RL algorithms to successfully perform offline learning at minimal extra computational cost. CQL, in brief, learns conservative Q values by allowing the values of only state-action pairs that were applied during the data collection to be high. This is done by adding a regularisation term to the Q objective function (Eq.\ref{eq:q}) as
\begin{equation}
    \label{eq:cql}
    \begin{split}
        J_{CQL} =\;&\mathbb{E}_{s\sim\mathcal{D}}[log\sum_a exp(Q_{\theta_Q}(s,a))\\
        &-\mathbb{E}_{a\sim\pi_\beta(a|s)}[Q_{\theta_Q}(s,a)]]+\frac{1}{2}J_Q,
    \end{split}
\end{equation}
where $\pi_\beta$ is the policy that collected the data, which is the model-based pattern in our case.

\subsubsection{RL Architecture}
In this work, the actor and critic are parameterised by neural networks with two hidden layers. The hidden layers have 64 units with ReLU activation function. Note that the number of hidden units were determined empirically, starting the empirical search at 250 units \cite{Wannawas2021} and reducing the number until the agent fails to learn the pattern. This is to avoid over-fitting and improve the learning speed. Sigmoid activation function is applied at the policy network's output layer to squash the output between $[0,1]$, making it compatible OpenSim's muscle activation.

\subsection{Experiment}
The experiments for evaluating our methods are divided into two sets of experiments: model-based experiments, and real-world experiments. The model-based experiments are for evaluating the robustness of the RL-based stimulation pattern finding. The real-world experiments are for evaluating the performances of patterns governed by our method and comparing them to those of an EMG-based method.

\subsubsection{Model-based experiments}
The model-based experiments are designed to evaluate the robustness of our RL-based method in finding the model-based stimulation patterns. Note that the learning of the RL agent is a stochastic process where the success may depend on the initialisation or happen by chance. In these experiment, we apply our method to 5 different cycling configurations (different seat positions and legs' length) in both two-muscle and three-muscle settings, comprising 10 test cases in total; 10 runs were performed on each case. 

\subsubsection{Real-world experiment}
We test the full pipeline of our stimulation pattern finding method, together with our crank angle measurement method, in a real-world experiment. The experiment was carried out on a healthy subject which allows us to collect an exact corresponding EMG-based pattern for performance comparisons. The experiment had the subject performing two-muscle cycling on a tricycle (BerkelBike, BerkelBike BV, Sint-Michielsgestel, The Netherlands), placed on a cycling trainer (Fig.\ref{fig:Framework}c). FES pulses were generated by RehaStim1 (HASOMED GmbH, Magdeburg, Germany) and applied on Quadriceps and Hamstrings of the subject via self-adhesive electrodes. At the beginning of the experiment, the right leg's length of the subject and the distance between the seat and the crank centre are measured to build the customised model.

After the model-based training finished, the real FES cycling sessions with the model-based pattern were performed. Each session was 10-second long. Note that simple pattern adjustments such as shrinking and extending the stimulation intervals were done between each sessions to increase the variety of the stimulation. Ten sessions were carried out in total, yielding 100 seconds of cycling data, which was equivalent to 4,000 experience tuples. The RL agent was then trained on these tuples in the offline mode which yielded a fine-tuned pattern thereafter. The performances of the model-based, fine-tuned, and EMG-based patterns were tested on 30-second cycling sessions with 30-minute break between each session to minimise the effects of muscle fatigue.

The EMG-based pattern was collected by recording the EMG from the right Quadriceps and Hamstrings of the subject while performing voluntary cycling. The EMG data ware collected through Olimex EKG-EMG shield (Olimex Ltd, Bulgaria). The raw data were processed into RMS values, which show the voluntary activation of the muscles. The EMG pattern was then obtained by averaging the crank angle intervals where the muscles were active.

\section{Results}
\subsection{Model-based experiments}
We tested the model-based phase of our method on 5 different cycling configurations in both two-muscle and three-muscle settings, totalling 10 test cases; 10 runs were performed on each case. The model-based training progresses were monitored by episodic returns--the discounted reward that the RL agents obtained in each training episode.

Fig.\ref{fig:returns}a and b show the normalised episodic returns of 2- and 3-muscle cases, respectively. In the early period of the training, the returns are in a near-zero region in both cases because the agents were unable to induce full-revolution cycling. After that, the returns rise quickly between the $5^{th}$ and $15^{th}$ episode as their abilities to induce the cycling motion improve. The rise is sharper in 2-muscle case as the 2-muscle patterns are easier to learn. The returns reach plateaus at around the $20^{th}$ episode, indicating the discoveries of optimal model-based patterns. Note that low return episodes, which result to large standard deviations, could occur even after the optimal patterns were discovered if the initial crank angles were close to the dead angles. The occurrences are more often in 2-muscle cases. The red bars in Fig.\ref{fig:pattern} is an example model-based pattern. This pattern was applied to initiate the data collection in the real-world experiments.
\begin{figure}[!ht]
    \begin{center}
    \includegraphics[width=0.98\columnwidth]{figures/returns.png}
    \vspace{-1mm}
    \caption{The model-based learning curves on 2-muscle (left) and 3-muscle (right) cycling. The solid curves and shades are the means and standard deviations, respectively.
    \label{fig:returns}}
    \vspace{-4mm}
    \end{center}
\end{figure}
\vspace{-1mm}

\subsection{Real-world experiments}
The fine-tuned pattern was obtained based on 100 seconds of cycling data collected from ten 10-second cycling sessions. Fig.\ref{fig:pattern} compares the model-based, EMG-based, and fine-tuned patterns. The Hamstrings \emph{Off} angles are similar across different patterns. The fine-tuned pattern a has larger Hamstrings active range than the others. For Quadriceps, the model-based pattern has the largest active range. The Quadriceps \emph{Off} angles of EMG- and model-based patterns are very close, while the model-based and fine-tuned are similar in the \emph{On} angle. Noticeably, the EMG-based \emph{On} angles are behind those of the others. This reflects the muscles' activation delay for which the pattern has to compensate by starting the stimulation before the crank reaches the angles where the muscles produce positive torque.

Fig.\ref{fig:speed}a-c compares the cycling performances of the three patterns in terms of the crank's speed. The out-of-the-box model-based pattern (Fig.\ref{fig:speed}b) successfully induced cycling motion on the real cycling system. The model-based pattern produced the average speed of 49.83 RPM which is slightly higher than that produced by the EMG-based pattern (Fig.\ref{fig:speed}a). The speed difference is mainly due to the differences in the \emph{On} angles where the earlier \emph{On} generates faster speed. The fine-tuned pattern produced the highest average speed at 52.37 RPM (Fig.\ref{fig:speed}c).

\iffalse
\begin{figure}[hb!]
    \begin{center}
    \includegraphics[width=0.98\columnwidth]{figures/results.png}
    \vspace{-2mm}
    \caption{(a) The (green) EMG-based, (orange) model-based, and (blue) fine-tuned stimulation pattern. Cycling crank speed induced by (b) EMG-based, (c) model-based, and (d) fine-tuned patterns over 30-second sessions.
    \label{fig:results}}
    \end{center}
\end{figure}
\fi

\begin{figure}[hb!]
    \begin{center}
    \includegraphics[width=0.80\columnwidth]{figures/Pattern.png}
    \vspace{-2mm}
    \caption{(a) The (green) EMG-based, (red) model-based, and (blue) fine-tuned stimulation patterns.
    \label{fig:pattern}}
    \end{center}
    \vspace{-4mm}
\end{figure}

\begin{figure}[hb!]
    \begin{center}
    \includegraphics[width=0.98\columnwidth]{figures/Speed.png}
    \vspace{-2mm}
    \caption{Cycling crank speed induced by (a) EMG-based, (b) model-based, and (c) fine-tuned patterns over 30-second sessions.
    \label{fig:speed}}
    \end{center}
    \vspace{-4mm}
\end{figure}

\section{Discussion \& Conclusion}
We present an RL-based method for finding the stimulation pattern that requires neither force nor EMG sensors. Our method begins with creating customised OpenSim cycling models which are automated by our Python script. RL agents then interact with the customised models to learn model-based patterns. After that, the model-based patterns are applied to the real cycling from which the cycling data are collected and used to govern fine-tuned patterns through offline reinforcement learning.

Regarding the evaluations, the simulation tests show that our method can robustly find model-based patterns for both 2- and 3-muscle cycling with different cycling postures. The experimental evaluations show that the out-of-the-box model-based pattern can induce cycling motion with similar performance to that of the EMG-based pattern. The model-based pattern is improved by using the cycling data and offline RL. The improvement yields the fine-tuned pattern that has better performance than the others.

One limitation of the proposed method is that muscular fatigue can affect the cadence and therefore influence the fine-tuned pattern result. The fatigue effect can be minimised by inserting a break between trials or applying weights on the cadence data collected from different trials to compensate for the fatigue. Alternatively, we have recently introduced a machine-learning-based method \cite{Wannawas2023b} that could be integrated into the system to automatically learn the influences of muscular fatigue and distinguish the low cycling speed caused by bad stimulation and that caused by the fatigue. 

Several future developments could be explored to push this method towards practical uses outside the laboratory. One of them is a graphic user interface (GUI) that helps the user manage the processes which involve several sub-processes and files. Regarding the algorithm itself, this method does not exploit the potential of using the data to optimise the OpenSim model which can lead to a better model-based pattern and shorter fine-tuning phase. It is also worth mentioning that the optimality itself is with respect to the objective or the reward function (Eq.\ref{eq:reward}). The setup presented here is meant for achieving high-speed cycling. This may not always be the best, for example, for performing long cycling. Finding the pattern for other cycling purposes can be done by modifying the reward function such as increasing $\beta$ to obtain more stimulation-efficient patterns for long cycling.
% It is worth to note that the offline RL is constrained by the actions that are tried in the real world. Given the fact the data collection begins with trying out the model-based pattern, the actions that are tried may not be much different from the original model-based pattern which can be a local optima. Deploying Online RL on the optimised model where a variety of actions can be tried could yields the patterns that are more optimal.

Crank angle measuring device also plays an important role in enabling the public use of our method. Although there are several ways to interface our method with the existing cycling systems' devices, building the interface can be a technical challenge for the users. Developing a low-cost, dedicated device that can be easily installed on the existing systems will tremendously benefit the users. In this regard, IMU-based devices strapped on the legs \cite{Wiesener2017,Schmoll2022,Maneski2017} are attractive because they can be built using inexpensive electronic components and do not require any modification on the existing cycling equipment.

% Toward the big goal of increasing FES cycling's accessibility to the extent of common at-home usage, it is important that the users can use the systems with minimal technical intervention. 
In a broader view, we believe that artificial intelligence and electrical stimulation will have an important role in rehabilitation robots of the future. This proof-of-concept work is a particular showcase of human-in-the-loop AI in rehabilitation, displaying its feasibility and potential. Specifically, this real-world success of reinforcement learning in FES control is a step towards the development on AI-based intelligent controls that can power rehabilitation robots for the restoration of general movements \cite{Shafti2019,Stewart2019,Wannawas2023a}.

\addtolength{\textheight}{-10cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{ACKNOWLEDGMENT}
NW acknowledges his support by the Royal Thai Government Scholarship. AAF acknowledges his support by UKRI Turing AI Fellowship (EP/V025449/1).

\bibliography{bib}
\bibliographystyle{IEEEtran}

\end{document}