@book{hennig_probabilistic_2022,
	title = {Probabilistic Numerics},
	publisher = {Cambridge University Press},
	author = {Hennig, P and Osborne, M A},
	year = {2022},
}

@inproceedings{hendrycks_benchmarking_2019,
	title = {Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},
	abstract = {In this paper we establish rigorous benchmarks for image classiﬁer robustness. Our ﬁrst benchmark, {IMAGENET}-C, standardizes and expands the corruption robustness topic, while showing which classiﬁers are preferable in safety-critical applications. Then we propose a new dataset called {IMAGENET}-P which enables researchers to benchmark a classiﬁer’s robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We ﬁnd that there are negligible changes in relative corruption robustness from {AlexNet} classiﬁers to {ResNet} classiﬁers. Afterward we discover ways to enhance corruption and perturbation robustness. We even ﬁnd that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.},
	booktitle = {Proceedings of the International Conference on Learning Representations},
	publisher = {{arXiv}},
	author = {Hendrycks, Dan and Dietterich, Thomas},
	urldate = {2023-01-11},
	year = {2019},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1903.12261 [cs, stat]}
}

@inproceedings{yu_slimmable_2019,
	title = {Slimmable Neural Networks},
	abstract = {We present a simple and general method to train a single neural network executable at different widths1, permitting instant and adaptive accuracy-efﬁciency trade-offs at runtime. Instead of training individual networks with different width conﬁgurations, we train a shared network with switchable batch normalization. At runtime, the network can adjust its width on the ﬂy according to on-device benchmarks and resource constraints, rather than downloading and ofﬂoading different models. Our trained networks, named slimmable neural networks, achieve similar (and in many cases better) {ImageNet} classiﬁcation accuracy than individually trained models of {MobileNet} v1, {MobileNet} v2, {ShufﬂeNet} and {ResNet}-50 at different widths respectively. We also demonstrate better performance of slimmable models compared with individual ones across a wide range of applications including {COCO} bounding-box object detection, instance segmentation and person keypoint detection without tuning hyper-parameters. Lastly we visualize and discuss the learned features of slimmable networks. Code and models are available at: https://github.com/{JiahuiYu}/slimmable\_networks.},
	booktitle = {International Conference on Learning Representations},
	publisher = {{arXiv}},
	author = {Yu, Jiahui and Yang, Linjie and Xu, Ning and Yang, Jianchao and Huang, Thomas},
	urldate = {2023-01-02},
	year = {2019},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1812.08928 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{chai_improving_2019,
	title = {Improving Quadrature for Constrained Integrands},
	abstract = {Quadrature is the problem of estimating intractable integrals, a problem that arises in many Bayesian machine learning settings. We present an improved Bayesian framework for estimating intractable integrals of speciﬁc kinds of constrained integrands. We derive the necessary approximation scheme for a speciﬁc and especially useful instantiation of this framework: the use of a log transformation to model non-negative integrands. We also propose a novel method for optimizing the hyperparameters associated with this framework; we optimize the hyperparameters in the original space of the integrand as opposed to in the transformed space, resulting in a model that better explains the actual data. Experiments on both synthetic and real-world data demonstrate that the proposed framework achieves moreaccurate estimates using less wall-clock time than previously preposed Bayesian quadrature procedures for non-negative integrands.},
	eventtitle = {22nd International Conference on Artificial Intelligence and Statistics},
	booktitle = {Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics},
	author = {Chai, Henry and Garnett, Roman},
	year = {2019},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1802.04782},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{gunter_sampling_2014,
	title = {Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature},
	abstract = {We propose a novel sampling framework for inference in probabilistic models: an active learning approach that converges more quickly (in wall-clock time) than Markov chain Monte Carlo ({MCMC}) benchmarks. The central challenge in probabilistic inference is numerical integration, to average over ensembles of models or unknown (hyper-)parameters (for example to compute the marginal likelihood or a partition function). {MCMC} has provided approaches to numerical integration that deliver state-of-the-art inference, but can suffer from sample inefﬁciency and poor convergence diagnostics. Bayesian quadrature techniques offer a model-based solution to such problems, but their uptake has been hindered by prohibitive computation costs. We introduce a warped model for probabilistic integrands (likelihoods) that are known to be non-negative, permitting a cheap active learning scheme to optimally select sample locations. Our algorithm is demonstrated to offer faster convergence (in seconds) relative to simple Monte Carlo and annealed importance sampling on both synthetic and real-world examples.},
	eventtitle = {28th Annual Conference on Neural Information Processing Systems},
	pages = {2789--2797},
	booktitle = {Proceedings of the 28th Annual Conference on Neural Information Processing Systems},
	author = {Gunter, Tom and Osborne, Michael A. and Garnett, Roman and Hennig, Philipp and Roberts, Stephen J.},
	year = {2014},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1411.0439},
	keywords = {Statistics - Machine Learning},
}

@article{ohagan_bayeshermite_1991,
	title = {Bayes–Hermite quadrature},
	volume = {29},
	abstract = {Bayesian quadrature treats the problem of numerical integration as one of statistical inference. A prior Gaussian process distribution is assumed for the integrand, observations arise from evaluating the integrand at selected points, and a posterior distribution is derived for the integrand and the integral. Methods are developed for quadrature in {IRP}. A particular application is integrating the posterior density arising from some other Bayesian analysis. Simulation results are presented, to show that the resulting Bayes-Hermite quadrature rules may perform better than the conventional Gauss-Hermite rules for this application. A key result is derived for product designs, which makes Bayesian quadrature practically useful for integrating in several dimensions. Although the method does not at present provide a solution to the more difficult problem of quadrature in high dimensions, it does seem to offer real improvements over existing methods in relatively low dimensions.},
	pages = {245--260},
	number = {3},
	journal = {Journal of Statistical Planning and Inference},
	author = {O'Hagan, A.},
	urldate = {2018-12-17},
	year = {1991},
	langid = {english},
}

@inproceedings{osborne_active_2012,
	title = {Active Learning of Model Evidence Using Bayesian Quadrature},
	abstract = {Numerical integration is a key component of many problems in scientiﬁc computing, statistical modelling, and machine learning. Bayesian Quadrature is a modelbased method for numerical integration which, relative to standard Monte Carlo methods, offers increased sample efﬁciency and a more robust estimate of the uncertainty in the estimated integral. We propose a novel Bayesian Quadrature approach for numerical integration when the integrand is non-negative, such as the case of computing the marginal likelihood, predictive distribution, or normalising constant of a probabilistic model. Our approach approximately marginalises the quadrature model’s hyperparameters in closed form, and introduces an active learning scheme to optimally select function evaluations, as opposed to using Monte Carlo samples. We demonstrate our method on both a number of synthetic benchmarks and a real scientiﬁc problem from astronomy.},
	pages = {46--54},
	booktitle = {Advances in Neural Information Processing Systems 26},
	author = {Osborne, Michael A and Duvenaud, David and Garnett, Roman and Rasmussen, Carl E and Roberts, Stephen J and Ghahramani, Zoubin},
	year = {2012},
	langid = {english},
}

@article{minka_deriving_2000,
	title = {Deriving quadrature rules from Gaussian processes},
	pages = {1--21},
	institution = {Statistics Department, Carnegie Mellon University},
	type = {Technical},
	author = {Minka, Thomas},
	year = {2000},
}

@book{rasmussen_gaussian_2006,
	title = {Gaussian Processes for Machine Learning},
	publisher = {{MIT} Press},
	author = {Rasmussen, Carl and Williams, Christopher},
	year = {2006},
}

@article{elsken_neural_2019,
	title = {Neural Architecture Search: A Survey},
	volume = {20},
	shorttitle = {Neural Architecture Search},
	abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and errorprone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this ﬁeld of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
	pages = {1--21},
	number = {55},
	journal = {Journal of Machine Learning Research},
	author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
	year = {2019},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1808.05377},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@book{garnett_bayesian_2021,
	title = {Bayesian Optimization},
	publisher = {Cambridge University Press},
	author = {Garnett, Roman},
	year = {2021},
	langid = {english},
}

@inproceedings{ru_interpretable_2021,
	title = {Interpretable Neural Architecture Search via Bayesian Optimisation with Weisfeiler-Lehman Kernels},
	abstract = {Current neural architecture search ({NAS}) strategies focus only on ﬁnding a single, good, architecture. They offer little insight into why a speciﬁc network is performing well, or how we should modify the architecture if we want further improvements. We propose a Bayesian optimisation ({BO}) approach for {NAS} that combines the Weisfeiler-Lehman graph kernel with a Gaussian process surrogate. Our method optimises the architecture in a highly data-efﬁcient manner: it is capable of capturing the topological structures of the architectures and is scalable to large graphs, thus making the high-dimensional and graph-like search spaces amenable to {BO}. More importantly, our method affords interpretability by discovering useful network features and their corresponding impact on the network performance. Indeed, we demonstrate empirically that our surrogate model is capable of identifying useful motifs which can guide the generation of new architectures. We ﬁnally show that our method outperforms existing {NAS} approaches to achieve the state of the art on both closed- and open-domain search spaces.},
	booktitle = {Proceedings of the 9th International Conference on Learning Representations},
	author = {Ru, Binxin and Wan, Xingchen and Dong, Xiaowen and Osborne, Michael},
	year = {2021},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2006.07556},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{shervashidze_weisfeiler-lehman_2011,
	title = {Weisfeiler-Lehman Graph Kernels},
	volume = {12},
	abstract = {In this article, we propose a family of efﬁcient kernels for large graphs with discrete node labels. Key to our method is a rapid feature extraction scheme based on the Weisfeiler-Lehman test of isomorphism on graphs. It maps the original graph to a sequence of graphs, whose node attributes capture topological and label information. A family of kernels can be deﬁned based on this Weisfeiler-Lehman sequence of graphs, including a highly efﬁcient kernel comparing subtree-like patterns. Its runtime scales only linearly in the number of edges of the graphs and the length of the Weisfeiler-Lehman graph sequence. In our experimental evaluation, our kernels outperform state-of-the-art graph kernels on several graph classiﬁcation benchmark data sets in terms of accuracy and runtime. Our kernels open the door to large-scale applications of graph kernels in various disciplines such as computational biology and social network analysis.},
	pages = {2539--2561},
	journal = {Journal of Machine Learning Research},
	author = {Shervashidze, Nino},
	year = {2011},
	langid = {english},
}

@article{dong_nats-bench_2021,
	title = {{NATS}-Bench: Benchmarking {NAS} Algorithms for Architecture Topology and Size},
	issn = {0162-8828, 2160-9292, 1939-3539},
	doi = {10.1109/TPAMI.2021.3054824},
	shorttitle = {{NATS}-Bench},
	abstract = {Neural architecture search ({NAS}) has attracted a lot of attention and has been illustrated to bring tangible beneﬁts in a large number of applications in the past few years. Architecture topology and architecture size have been regarded as two of the most important aspects for the performance of deep learning models and the community has spawned lots of searching algorithms for both of those aspects of the neural architectures. However, the performance gain from these searching algorithms is achieved under different search spaces and training setups. This makes the overall performance of the algorithms incomparable and the improvement from a sub-module of the searching model unclear. In this paper, we propose {NATS}-Bench, a uniﬁed benchmark on searching for both topology and size, for (almost) any up-to-date {NAS} algorithm. {NATS}-Bench includes the search space of 15,625 neural cell candidates for architecture topology and 32,768 for architecture size on three datasets. We analyze the validity of our benchmark in terms of various criteria and performance comparison of all candidates in the search space. We also show the versatility of {NATS}-Bench by benchmarking 13 recent state-of-the-art {NAS} algorithms on it. All logs and diagnostic information trained using the same setup for each candidate are provided. This facilitates a much larger community of researchers to focus on developing better {NAS} algorithms in a more comparable and computationally effective environment. All codes are publicly available at: https://xuanyidong.com/assets/projects/{NATS}-Bench.},
	pages = {1--1},
	journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Dong, Xuanyi and Liu, Lu and Musial, Katarzyna and Gabrys, Bogdan},
	year = {2021},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2009.00437},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{hayakawa_positively_2022,
	title = {Positively Weighted Kernel Quadrature via Subsampling},
	abstract = {We study kernel quadrature rules with convex weights. Our approach combines the spectral properties of the kernel with recombination results about point measures. This results in effective algorithms that construct convex quadrature rules using only access to i.i.d. samples from the underlying measure and evaluation of the kernel and that result in a small worst-case error. In addition to our theoretical results and the beneﬁts resulting from convex weights, our experiments indicate that this construction can compete with the optimal bounds in well-known examples.},
	publisher = {{arXiv: 2107.09597}},
	author = {Hayakawa, Satoshi and Oberhauser, Harald and Lyons, Terry},
	year = {2022},
	langid = {english},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning},
}

@inproceedings{shu_neural_2022,
	title = {Neural Ensemble Search via Bayesian Sampling},
	abstract = {Recently, neural architecture search ({NAS}) has been applied to automate the design of neural networks in real-world applications. A large number of algorithms have been developed to improve the search cost or the performance of the final selected architectures in {NAS}. Unfortunately, these {NAS} algorithms aim to select only one single wellperforming architecture from their search spaces and thus have overlooked the capability of neural network ensemble (i.e., an ensemble of neural networks with diverse architectures) in achieving improved performance over a single final selected architecture. To this end, we introduce a novel neural ensemble search algorithm, called neural ensemble search via Bayesian sampling ({NESBS}), to effectively and efficiently select well-performing neural network ensembles from a {NAS} search space. In our extensive experiments, {NESBS} algorithm is shown to be able to achieve improved performance over state-of-the-art {NAS} algorithms while incurring a comparable search cost, thus indicating the superior performance of our {NESBS} algorithm over these {NAS} algorithms in practice.},
	author = {Shu, Yao and Chen, Yizhou and Dai, Zhongxiang and Low, Bryan Kian Hsiang},
    booktitle = {Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence},
	year = {2022},
	langid = {english},
}

@misc{tchernychova_caratheodory_2015,
	title = {Caratheodory cubature measures},
	institution = {University of Oxford},
	type = {Doctor of Philosophy},
	author = {Tchernychova, Maria},
	year = {2015},
	langid = {english},
}

@inproceedings{zaidi_neural_2022,
	title = {Neural Ensemble Search for Uncertainty Estimation and Dataset Shift},
	abstract = {Ensembles of neural networks achieve superior performance compared to stand-alone networks in terms of accuracy, uncertainty calibration and robustness to dataset shift. {\textbackslash}emph\{Deep ensembles\}, a state-of-the-art method for uncertainty estimation, only ensemble random initializations of a {\textbackslash}emph\{fixed\} architecture. Instead, we propose two methods for automatically constructing ensembles with {\textbackslash}emph\{varying\} architectures, which implicitly trade-off individual architectures' strengths against the ensemble's diversity and exploit architectural variation as a source of diversity. On a variety of classification tasks and modern architecture search spaces, we show that the resulting ensembles outperform deep ensembles not only in terms of accuracy but also uncertainty calibration and robustness to dataset shift. Our further analysis and ablation studies provide evidence of higher ensemble diversity due to architectural variation, resulting in ensembles that can outperform deep ensembles, even when having weaker average base learners. To foster reproducibility, our code is available: {\textbackslash}url\{https://github.com/automl/nes\}},
	author = {Zaidi, Sheheryar and Zela, Arber and Elsken, Thomas and Holmes, Chris and Hutter, Frank and Teh, Yee Whye},
    booktitle = {Advances in Neural Information Processing Systems},
    volume = 36,
	year = {2022},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2006.08573 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{liu_darts_2019,
	title = {{DARTS}: Differentiable Architecture Search},
	shorttitle = {{DARTS}},
	abstract = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efﬁcient search of the architecture using gradient descent. Extensive experiments on {CIFAR}-10, {ImageNet}, Penn Treebank and {WikiText}-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classiﬁcation and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efﬁcient architecture search algorithms.},
    booktitle = {Proceedings of the International Conference on Learning Representations},
	author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
	year = {2019},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1806.09055 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{guo_calibration_2017,
	title = {On Calibration of Modern Neural Networks},
	abstract = {Conﬁdence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classiﬁcation models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors inﬂuencing calibration. We evaluate the performance of various post-processing calibration methods on state-ofthe-art architectures with image and document classiﬁcation datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a singleparameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.},
	author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
    booktitle = {Proceedings of the 34th International Conference of Machine Learning},
	year = {2017},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1706.04599 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{xu_pc-darts_2020,
	title = {{PC}-{DARTS}: Partial Channel Connections for Memory-Efficient Architecture Search},
	shorttitle = {{PC}-{DARTS}},
	abstract = {Differentiable architecture search ({DARTS}) provided a fast solution in ﬁnding effective network architectures, but suffered from large memory and computing overheads in jointly training a super-network and searching for an optimal architecture. In this paper, we present a novel approach, namely, Partially-Connected {DARTS}, by sampling a small part of super-network to reduce the redundancy in exploring the network space, thereby performing a more efﬁcient search without comprising the performance. In particular, we perform operation search in a subset of channels while bypassing the held out part in a shortcut. This strategy may suffer from an undesired inconsistency on selecting the edges of super-net caused by sampling different channels. We alleviate it using edge normalization, which adds a new set of edge-level parameters to reduce uncertainty in search. Thanks to the reduced memory cost, {PC}-{DARTS} can be trained with a larger batch size and, consequently, enjoys both faster speed and higher training stability. Experimental results demonstrate the effectiveness of the proposed method. Speciﬁcally, we achieve an error rate of 2.57\% on {CIFAR}10 with merely 0.1 {GPU}-days for architecture search, and a state-of-the-art top-1 error rate of 24.2\% on {ImageNet} (under the mobile setting) using 3.8 {GPU}-days for search. Our code has been made available at https://github.com/yuhuixu1993/{PC}-{DARTS}.},
	author = {Xu, Yuhui and Xie, Lingxi and Zhang, Xiaopeng and Chen, Xin and Qi, Guo-Jun and Tian, Qi and Xiong, Hongkai},
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    volume = 43,
    pages = {2953--2970},
	year = {2020},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1907.05737 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{chen_progressive_2019,
	title = {Progressive Differentiable Architecture Search: Bridging the Depth Gap between Search and Evaluation},
	shorttitle = {Progressive Differentiable Architecture Search},
	abstract = {Recently, differentiable search methods have made major progress in reducing the computational costs of neural architecture search. However, these approaches often report lower accuracy in evaluating the searched architecture or transferring it to another dataset. This is arguably due to the large gap between the architecture depths in search and evaluation scenarios. In this paper, we present an efﬁcient algorithm which allows the depth of searched architectures to grow gradually during the training procedure. This brings two issues, namely, heavier computational overheads and weaker search stability, which we solve using search space approximation and regularization, respectively. With a signiﬁcantly reduced search time (∼7 hours on a single {GPU}), our approach achieves state-of-the-art performance on both the proxy dataset ({CIFAR}10 or {CIFAR}100) and the target dataset ({ImageNet}). Code is available at https://github.com/ chenxin061/pdarts.},
	author = {Chen, Xin and Xie, Lingxi and Wu, Jun and Tian, Qi},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
	year = {2019},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1904.12760 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{white_bananas_2020,
	title = {{BANANAS}: Bayesian Optimization with Neural Architectures for Neural Architecture Search},
	shorttitle = {{BANANAS}},
	abstract = {Over the past half-decade, many methods have been considered for neural architecture search ({NAS}). Bayesian optimization ({BO}), which has long had success in hyperparameter optimization, has recently emerged as a very promising strategy for {NAS} when it is coupled with a neural predictor. Recent work has proposed diﬀerent instantiations of this framework, for example, using Bayesian neural networks or graph convolutional networks as the predictive model within {BO}. However, the analyses in these papers often focus on the full-ﬂedged {NAS} algorithm, so it is diﬃcult to tell which individual components of the framework lead to the best performance.},
	author = {White, Colin and Neiswanger, Willie and Savani, Yash},
    journal = {Advances in Neural Information Processing Systems},
    volume = 36,
	year = {2020},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1910.11858 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{lakshminarayanan_simple_2017,
	title = {Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles},
	abstract = {Deep neural networks ({NNs}) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in {NNs} is a challenging and yet unsolved problem. Bayesian {NNs}, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require signiﬁcant modiﬁcations to the training procedure and are computationally expensive compared to standard (non-Bayesian) {NNs}. We propose an alternative to Bayesian {NNs} that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classiﬁcation and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian {NNs}. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on {ImageNet}.},
	author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
    journal = {Advances in Neural Information Processing Systems},
    volume = 30,
	year = {2017},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1612.01474 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kandasamy_neural_2019,
	title = {Neural Architecture Search with Bayesian Optimisation and Optimal Transport},
	abstract = {Bayesian Optimisation ({BO}) refers to a class of methods for global optimisation of a function f which is only accessible via point evaluations. It is typically used in settings where f is expensive to evaluate. A common use case for {BO} in machine learning is model selection, where it is not possible to analytically model the generalisation performance of a statistical model, and we resort to noisy and expensive training and validation procedures to choose the best model. Conventional {BO} methods have focused on Euclidean and categorical domains, which, in the context of model selection, only permits tuning scalar hyper-parameters of machine learning algorithms. However, with the surge of interest in deep learning, there is an increasing demand to tune neural network architectures. In this work, we develop {NASBOT}, a Gaussian process based {BO} framework for neural architecture search. To accomplish this, we develop a distance metric in the space of neural network architectures which can be computed efﬁciently via an optimal transport program. This distance might be of independent interest to the deep learning community as it may ﬁnd applications outside of {BO}. We demonstrate that {NASBOT} outperforms other alternatives for architecture search in several cross validation based model selection tasks on multi-layer perceptrons and convolutional neural networks.},
	author = {Kandasamy, Kirthevasan and Neiswanger, Willie and Schneider, Jeff and Poczos, Barnabas and Xing, Eric},
    journal = {Advances in Neural Information Processing Systems},
    volume = 31,
	year = {2019},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1802.07191 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}


@InProceedings{Wan_2022_WACV,
    author    = {Wan, Xingchen and Ru, Binxin and Esparan\c{c}a, Pedro M. and Carlucci, Fabio Maria},
    title     = {Approximate Neural Architecture Search via Operation Distribution Learning},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {January},
    year      = {2022},
    pages     = {2377-2386}
}

@article{zoph2016neural,
  title={Neural architecture search with reinforcement learning},
  author={Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1611.01578},
  year={2016}
}

@article{elsken2019neural,
  title={Neural architecture search: A survey},
  author={Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  journal={The Journal of Machine Learning Research},
  volume={20},
  number={1},
  pages={1997--2017},
  year={2019},
  publisher={JMLR. org}
}

@inproceedings{real2017large,
  title={Large-scale evolution of image classifiers},
  author={Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc V and Kurakin, Alexey},
  booktitle={International Conference on Machine Learning},
  pages={2902--2911},
  year={2017},
  organization={PMLR}
}

@inproceedings{zoph2018learning,
  title={Learning transferable architectures for scalable image recognition},
  author={Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={8697--8710},
  year={2018}
}

@inproceedings{liu2019auto,
  title={Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation},
  author={Liu, Chenxi and Chen, Liang-Chieh and Schroff, Florian and Adam, Hartwig and Hua, Wei and Yuille, Alan L and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={82--92},
  year={2019}
}

@article{he2021automl,
  title={AutoML: A survey of the state-of-the-art},
  author={He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
  journal={Knowledge-Based Systems},
  volume={212},
  pages={106622},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{pham2018efficient,
  title={Efficient neural architecture search via parameters sharing},
  author={Pham, Hieu and Guan, Melody and Zoph, Barret and Le, Quoc and Dean, Jeff},
  booktitle={International conference on machine learning},
  pages={4095--4104},
  year={2018},
  organization={PMLR}
}

@inproceedings{sandler2018mobilenetv2,
  title={Mobilenetv2: Inverted residuals and linear bottlenecks},
  author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4510--4520},
  year={2018}
}

@inproceedings{guo2020single,
  title={Single path one-shot neural architecture search with uniform sampling},
  author={Guo, Zichao and Zhang, Xiangyu and Mu, Haoyuan and Heng, Wen and Liu, Zechun and Wei, Yichen and Sun, Jian},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XVI 16},
  pages={544--560},
  year={2020},
  organization={Springer}
}

@inproceedings{bender2018understanding,
  title={Understanding and simplifying one-shot architecture search},
  author={Bender, Gabriel and Kindermans, Pieter-Jan and Zoph, Barret and Vasudevan, Vijay and Le, Quoc},
  booktitle={International conference on machine learning},
  pages={550--559},
  year={2018},
  organization={PMLR}
}

@inproceedings{yu2020bignas,
  title={Bignas: Scaling up neural architecture search with big single-stage models},
  author={Yu, Jiahui and Jin, Pengchong and Liu, Hanxiao and Bender, Gabriel and Kindermans, Pieter-Jan and Tan, Mingxing and Huang, Thomas and Song, Xiaodan and Pang, Ruoming and Le, Quoc},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part VII 16},
  pages={702--717},
  year={2020},
  organization={Springer}
}

@inproceedings{yu2019universally,
  title={Universally slimmable networks and improved training techniques},
  author={Yu, Jiahui and Huang, Thomas S},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1803--1811},
  year={2019}
}

@article{liu2021survey,
  title={A survey on evolutionary neural architecture search},
  author={Liu, Yuqiao and Sun, Yanan and Xue, Bing and Zhang, Mengjie and Yen, Gary G and Tan, Kay Chen},
  journal={IEEE transactions on neural networks and learning systems},
  year={2021},
  publisher={IEEE}
}

@inproceedings{liang2018evolutionary,
  title={Evolutionary architecture search for deep multitask networks},
  author={Liang, Jason and Meyerson, Elliot and Miikkulainen, Risto},
  booktitle={Proceedings of the genetic and evolutionary computation conference},
  pages={466--473},
  year={2018}
}

@article{shi2020bridging,
  title={Bridging the gap between sample-based and one-shot neural architecture search with bonas},
  author={Shi, Han and Pi, Renjie and Xu, Hang and Li, Zhenguo and Kwok, James and Zhang, Tong},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1808--1819},
  year={2020}
}

@article{zhou2023autopeft,
  title={AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning},
  author={Zhou, Han and Wan, Xingchen and Vuli{\'c}, Ivan and Korhonen, Anna},
  journal={arXiv preprint arXiv:2301.12132},
  year={2023}
}