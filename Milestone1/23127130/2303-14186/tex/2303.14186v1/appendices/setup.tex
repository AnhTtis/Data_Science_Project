\subsection{Datasets and models}

\paragraph{\cifar.}
We construct the \cifartwo dataset as the subset of \cifarten \citep{krizhevsky2009learning} consisting
of only the ``cat'' and ``dog'' classes. We initially used \cifartwo as the main test bed when designing \trak, as it is a binary classification task and also smaller in size.
On both \cifartwo and \cifarten, we train a ResNet-9 architecture.\footnote{\url{https://github.com/wbaek/torchskeleton/blob/master/bin/dawnbench/cifar10.py}}
For \cifartwo, we use (max) learning rate $0.4$, momentum $0.9$, weight decay $5\text{e-}4$, and train for $100$ epochs using a cyclic learning rate schedule with a single peak at epoch 5.
For \cifarten, we replace the learning rate with $0.5$ and train for $24$ epochs.

Our code release includes a notebook\footnote{\url{https://github.com/MadryLab/trak/blob/main/examples/cifar2_correlation.ipynb}} that can reproduce the \cifartwo results end-to-end.
\paragraph{ImageNet.}
We use the full 1000-class ImageNet dataset and train a modified ResNet-18 architecture.
Models are trained from scratch for 15 epochs,
cyclic learning rate with peak at epoch 2 and initial learning rate $5.2$,
momentum $0.8$, weight decay $4\text{e-}5$, and label smoothing $0.05$.


\paragraph{\qnli.}
We finetune a pre-trained \bert model (\texttt{bert-base-cased}\footnote{\url{https://huggingface.co/bert-base-cased}}) on the \qnli (Question-answering Natural Language Inference) task from the \glue benchmark.
We use the default training script\footnote{\url{https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py}} from HuggingFace with a few modifications: we use SGD (20 epochs, learning rate starting at $1\text{e-}3$) instead of AdamW, and we remove the last \texttt{tanh} non-linearity before the classification layer. Removing the last non-linearity prevents the model outputs in saturating, resulting in higher LDS. (That said, we find that \trak scores can be still computed on the models with non-linearity; this was only for improving evaluation.)
We restrict the training set to 50,000 examples, approximately half of the full training set.

\paragraph{\clip on \mscoco.}
We use an open-source implementation\footnote{\url{https://github.com/mlfoundations/open_clip}} of \clip.
The model uses a ResNet-50 for the image encoder and a Transformer for the text encoder (for captions).
We train for $100$ epochs using the
Adam optimizer with batch size $600$, a cosine learning rate schedule with starting learning rate $0.001$, weight decay $0.1$, and momentum $0.9$. All images are
resized to a resolution of $224\times 224$. We use random resize crop, random
horizontal flip, and Gaussian blur as data augmentations.


In the counterfactual evaluation, we consider a normalized
notion of cosine similarity, $\bar{r} = r/(r_\text{95} - r_\text{5})$, where $r$ is the raw correlation between image and caption embeddings and $r_{\alpha}$ is the $\alpha$-percentile of image-caption similarities across the entire dataset. Results remain similar with other choices of metric.

\paragraph{Fact tracing \mtfive on \ftracetrex.}
We follow the setup exactly as in \citet{akyurek2022towards} as we describe in \Cref{subsec:fact_trace}, other than using a smaller architecture (\texttt{mt5-small}).


\paragraph{\textsc{ModelDiff} on \textsc{Living17}.}
The \textsc{Living17} dataset~\cite{santurkar2021breeds} is an image classification dataset derived from the ImageNet dataset and consists of 17 classes, each comprised of four original ImageNet classes.

We train the standard ResNet-18 architecture on the above dataset, either using standard data augmentation (random resized cropping and random horizontal flips) or with no data augmentation (only center cropping, same as used on when evaluating). The goal of the case study from \citet{shah2022modeldiff} is to distinguish the above two learning algorithms in terms of the feature priors of the resulting trained models.
To run \textsc{ModelDiff}, follow the setup in \citet{shah2022modeldiff} exactly; we refer to the work for more details of the case study and implementation details.




\subsection{\trak hyperparameters}
\trak only has two hyperparmeters: the projection dimension $k$ and the number of models $M$.
The following hyperparameters were used unless specified otherwise:
\begin{table}[!htbp]
    \centering
    \begin{tabular}{llrr}
        \toprule
        Dataset & Model & Number of models ($M$) & Projection dimension ($k$) \\
        \midrule
        \cifartwo & ResNet-9 & - & 4,000 \\
        \cifarten & ResNet-9 & - & 20,000 \\
        \qnli & \bertbase & - & 4,000 \\
        ImageNet & ResNet-18 & - &  15,000 \\
        \mscoco & ResNet-50 (\clip) & 100 & 20,000 \\
        \ftracetrex & \texttt{mt5-small} & 10 & 4,000 \\
        \textsc{Living-17} & ResNet-18 & 100 & 1,000 \\
        \bottomrule
    \end{tabular}
    \caption{\trak hyperparameters used for different experiments. Blank indicates that different numbers were used depending on the experiment.}
    \label{tab:proj_dim}
\end{table}

\subsection{Baselines}
\label{app:baselines}

We provide details on baselines used in our evaluation in \Cref{sec:eval}.
Though most of the existing approximation-based methods only use a single model checkpoint in their original formulation, we average the methods over multiple independent checkpoints to help increase its performance.

\paragraph{Influence functions.}
The standard Hessian-based influence functions yield the attribution scores
\[ \tau(z_j)_i = \nabla L(z_j;\theta) \; H_\theta^{-1} \; \nabla L(z_i; \theta), \]
where $H_\theta$ is the empirical Hessian w.r.t. the training set.
We use an existing PyTorch implementation\footnote{\url{https://github.com/alstonlo/torch-influence}} that uses the stochastic approximation of inverse-Hessian-vector products using the \textsc{LiSSA} \citet{agarwal2017second} algorithm as done in \citet{koh2017understanding}.
As in the original work, we compute the gradients only with respect to the last linear layer; using additional layers caused the inversion algorithm to either diverge or to run out of memory.
For hyperparameters, we use similar values as done in prior work; we use $r=1$, $d=5000$, and damping factor of 0.01. We find that additional repeats ($r$, the number of independent trials to average each iHvp estimate) does not help, while increasing the depth ($d$, the number of iterations used by \textsc{LiSSA}) helps significantly.

\paragraph{Influence functions based on the Arnoldi iteration.}
This variant of influence functions from \citet{schioppa2022scaling} is based on approximating the top eigenspace of the Hessian using the Arnoldi iteration \cite{arnoldi1951principle}. We use the original implementation in \textsc{JAX}.\footnote{\url{https://github.com/google-research/jax-influence}}  We normalize the gradients as recommended in the original paper.
While much faster than the original formulation in \citet{koh2017understanding}, we find that the attribution scores not very predictive (according to the LDS).

\paragraph{TracIn.}
We use the \tracincp estimator from \citep{pruthi2020estimating}, defined as
\[ \tau(z_j)_i = \sum\limits_{t=1}^T \eta_t \cdot \nabla L(z_j;\theta_t) \cdot  \nabla L(z_i; \theta_t), \]
where $\theta_t$ is the checkpoint from the epoch $t$ and $\eta_t$ is the corresponding learning rate $\eta_t$.
We also average over trajectories of multiple independently trained models, which increases its performance. We approximate the dot products using random projections of dimensions 500-1000 as we do for \trak, as the estimator is intractable otherwise.
We found that increasing the number of samples (epochs) from the training trajectory does not lead to much improvement.

\paragraph{Gradient Aggregated Similarity (\gas).}
This is a ``renormalized'' version of the TracInCP \cite{hammoudeh2022training} based on using the cosine similarity instead of raw dot products.
In general, its performance is indistinguishable from that of \tracin.

\paragraph{Representation similarity.}
We use the {\em signed} $\ell_2$ dot product in representation space (feature embeddings of the penultimate layer), where the sign indicates whether the labels match.
We also experimented with cosine similarity but the resulting performance was similar.

\paragraph{Empirical influences.}
We use the subsampling-based approximation to leave-one-out influences as used by \cite{feldman2020neural}, which is a difference-in-means estimator given by
\[ \tau(z_j)_i = \mathbb{E}_{S \ni z_i} \modeleval{z_j}{\theta}
- \mathbb{E}_{S \not\ni z_i} \modeleval{z_j}{\theta} \]
where the first (second) expectation is over training subsets that include (exclude) example $z_i$.

\paragraph{Datamodels.}
We use the $\ell_1$-regularized regression-based estimators from \citet{ilyas2022datamodels}, using up to 60,000 models for \cifartwo and 300,000 models for \cifarten (trained on different random 50\% subsets of the full training set).


\subsection{Hardware and wall-time measurements}
\label{app:wall_time}
For all of our experiments, we use NVIDIA A100 GPUs each with 40GB of memory and 12 CPU cores.
We evaluate the computational cost of attribution methods using two metrics, {\em total wall-time} and the {\em total number of trained models used}; see \Cref{sec:eval} for motivation behind these metrics.
For most attribution methods, one or more of the following components dominate their total runtime:
\begin{itemize}
    \item \texttt{TRAIN\_TIME}: the time to train one model (from scratch)
    \item \texttt{GRAD\_TIME}: the time to compute gradients of one model (including computing random projections) for the entire dataset under consideration (both train and test sets). This time may vary depending on size of the projection dimension, but our fast implementation (\Cref{app:impl}) can handle dimensions of up to 80,000 without much increase in runtime.
\end{itemize}

\noindent The total compute time for each method was approximated as follows, where $M$ is the number of models used:
\begin{itemize}
\item {\bf \trak:} $M \times (\texttt{TRAIN\_TIME} + \texttt{GRAD\_TIME})$, as we have to compute gradients for each of the trained models.
\item {\bf Datamodel \cite{ilyas2022datamodels} and Empirical Influence \cite{feldman2020neural}:} $M \times \texttt{TRAIN\_TIME}$. The additional cost of estimating datamodels or influences from the trained models (which simply involves solving a linear system) is negligible compared to the cost of training.
\item {\bf \textsc{LiSSA} based influence functions \cite{koh2017understanding}:} These approaches are costly because they use thousands of Hessian-vector product iterations to approximate a single inverse-Hessian-vector product (which is needed for each target example).
Hence, we computed these attribution scores for a much smaller sample of validation set (50 to 100). We measured the empirical runtime on this small sample and extrapolated to the size of the entire (test) dataset.
\item {\bf Influence function based on the Arnoldi iteration \cite{schioppa2022scaling}:} We ran the authors' original code\footnote{\url{https://github.com/google-research/jax-influence}} on \cifar models of the same architecture (after translating them to \texttt{JAX}) and measured the runtime.
\item {\bf \tracin \cite{pruthi2020estimating} and \gas \cite{hammoudeh2022identifying}:} $M \times (\texttt{TRAIN\_TIME} + \texttt{GRAD\_TIME} \times T)$, where $T$ is the number of checkpoints used per model.
\end{itemize}

