\subsection{The one-step Newton approximation for leave-one-out influence}
\label{app:theory_newton}
The key formula we use in \trak is the estimate for the leave-one-out (LOO) influence in logistic regression (\Cref{lem:formal}).
Here, we reproduce the derivation of this estimate from \citet{pregibon1981logistic} then extend it to incorporate example-dependent bias terms. %

\paragraph{Convergence condition for logistic regression.}
Assume that we optimized the logistic regression instance via Newton-Raphson, i.e., the iterates are updated as
\begin{equation}
    \thetahat_{t+1} \leftarrow \thetahat_{t} + H_{\thetahat_t}^{-1} \nabla_\theta \mathcal{L}(\thetahat_t) \label{eqn:newton_general}
\end{equation}
where $H_{\thetahat}$ is the Hessian and $\nabla_\theta \mathcal{L}(\thetahat)$ the gradient associated with the total training loss $\mathcal{L}(\thetahat) = \sum\limits_{z_i \in S} L(z_i;\theta)$.
In the case of logistic regression, the above update is given by
\begin{equation}
    \thetahat_{t+1} \leftarrow \thetahat_t + (X^\top R X)^{-1} X^\top \hat{\mathbf{q}} \label{eqn:newton_logistic}
\end{equation}
where $\hat{\mathbf{q}} = \vec{1} - \hat{\mathbf{p}}$ is the vector of the probabilities for the {\em incorrect} class evaluated at $\thetahat_t$ and $R = \text{\normalfont diag}(p^\star (1 - p^\star))$ is the corresponding matrix.
Upon convergence, the final parameters $\thetastar$ satisfy the following:
\begin{equation}
    (X^\top R X)^{-1} X^\top \mathbf{q}^\star = 0 \label{eqn:opt}
\end{equation}
where $\mathbf{q}^\star$ is the inverse probability vector corresponding to $\thetastar$.

\paragraph{The one-step Newton approximation.}
We estimate the counterfactual parameters $\thetastar_{-i}$ that would have resulted from training on the same training set excluding example $i$ by simply taking a single Newton step starting from the same global optimum $\thetastar$:
\begin{equation}
\thetastar_{-i} = \thetastar + (X_{-i}^\top R_{-i} X_{-i})^{-1} X_{-i}^\top \mathbf{q}_{-i}^\star,
\end{equation}
where the subscript $-i$ denotes the corresponding matrices and vectors without the $i$-th training example.
Rearranging and using (\ref{eqn:opt}),
\begin{align*}
    \thetastar - \thetastar_{-i} &= -  (X_{-i}^\top R_{-i} X_{-i})^{-1} X_{-i}^\top \mathbf{q}_{-i}^\star\\
    \thetastar - \thetastar_{-i} &= (X^\top R X)^{-1} X^\top \mathbf{q}^\star -  (X_{-i}^\top R_{-i} X_{-i})^{-1} X_{-i}^\top \mathbf{q}_{-i}^\star
\end{align*}
\noindent Using the Shermanâ€“Morrison formula to simplify above,\footnote{This is used also, for instance, to derive the LOO formulas for standard linear regression.} we have
\begin{equation}
    \thetastar - \thetastar_{-i} = \frac{(X^\top R X)^{-1} x_i}{1 - x_i^\top (X^\top R X)^{-1} x_i \cdot p_i^\star (1 - p_i^\star)} \mathbf{q}^\star_i = \frac{(X^\top R X)^{-1} x_i}{1 - x_i^\top (X^\top R X)^{-1} x_i \cdot p_i^\star (1 - p_i^\star)} (1 - \mathbf{p}^\star_i)
\end{equation}
The above formula estimates the change in the parameter vector itself. To estimate the change in prediction at a given example $x$, we multiply above with vector $x$ on the left to get the formula in \Cref{lem:formal}.

The approximation here is assuming the solution converges in one step. Prior works \cite{koh2019accuracy} quantify the fidelity of such approximation under some assumptions.
The effectiveness of \trak across a variety of settings suggests that the approximation is accurate in regimes that arise in practice.

\paragraph{Incorporating bias terms.}
The above derivation is standard for the case of the usual logistic regression, but it also directly extends to the case where the individual predictions incorporate example-dependent bias terms $b_i$ that are independent of $\theta$. In particular, note that the probability function after linearization in Step 1 is given by
\begin{equation}
    p(z_i; \theta) = \sigma(-y_i \cdot (\nabla \modeleval{z}{\thetastar} \cdot \theta + b_i))
\end{equation}
where $\sigma (\cdot)$ is again the sigmoid function.
Because the Hessian and the gradients of the training loss only depend on $\theta$ through $p(z_i; \theta)$, and because the $b_i$'s are independent of $\theta$, the computation going from
\cref{eqn:newton_general} to \cref{eqn:newton_logistic} is not affected.
The rest of the derivation also remains identical as the bias terms are already incorporated into $\mathbf{q}^\star$.

\subsection{Random projections preserve gradient flow}
\label{app:theory_jl}

In step 2 of \trak, we use random projections to reduce the dimension of the gradient vectors. Here, we justify this approximation when our model is trained via gradient descent. Similar analysis has been used prior, e.g., by \citet{malladi2022kernel}.

In the limit of small learning rate, the time-evolution of model output $f(z;\theta)$ under gradient descent (or the gradient flow) is captured by the following differential equation \citep{jacot2018neural}:
\begin{align}
    \frac{df(z;\theta)}{dt} = \sum\limits_i \frac{\partial \loss{z_i}}{\partial f(z_i; \theta)} \cdot (\nabla f(z_i;\theta) \cdot \nabla f(z;\theta)) \approx  \sum\limits_i \frac{\partial \loss{z_i}}{\partial f(z_i;\theta)} \cdot (\phi_i \cdot \phi) \label{eqn:ode}
\end{align}
where the approximation assumes that the gradients remain invariant.

If we treat the outputs $\{\hat{f}(z_i;\theta)\}_i$ as time-varying variables, then their time evolution is entirely described by the above system of differential equations (one for each $i$, replacing $z$ with $z_i$ above).
Importantly, the above equations only depend on the gradients through their inner products. Hence, as long as we preserve the inner products to sufficient accuracy, the resulting system has approximately the same evolution as the original one.




\subsection{Subsampling the training set}
\label{app:theory_alpha}

In step 4 of our algorithm, we ensemble the estimate over multiple models.  As
we discuss in \Cref{app:more_ablation}, this helps \trak capture more predictive
signal. An important design choice is training each model on a different random
subset of the training set.

This is motivated by the following connection to empirical influences.
Recall that we designed \trak to optimize the linear datamodeling score.
As we discuss in \Cref{sec:prelim}, datamodels can be viewed as an ``oracle'' for optimizing the same metric.
Further, as \citet{ilyas2022datamodels} observes, datamodels can be viewed as a regularized version of empirical influences \cite{feldman2020neural}, which are defined as a conditional expectation,
\begin{align}
    \tau(z_j)_i &= \mathbb{E}_{S' \sim \mathcal{D}}[f(z_j;\thetastar(S'))|
    z_i \in S']
    - \mathbb{E}_{S' \sim \mathcal{D}}[f(z_j;\thetastar(S'))|
    z_i \not\in S']
\end{align}
where $\mathcal{D}$ is a distribution over $\alpha$-fraction subsets of training set $S$. Assuming the expectation over $\alpha$-fraction subsets is identical to that over subsets of one additional element, we can rearrange this as
\begin{align}
    \tau(z_j)_i &= \mathbb{E}_{S' \sim \mathcal{D}}[f(z_j;\thetastar(S' \cup \{z_i\})) - f(z_j;\thetastar(S'))].
\end{align}
The above expression is simply the expectation of leave-one-out influence over different random subsets. As the estimate from step 3 of our algorithm is specific to a single training set, we need to average over different subsets in order to approximate the above quantity.

In principle, the estimates computed from $\thetastar(S')$ only apply to the training examples included in the subset $S'$, since the underlying formula (\Cref{lem:formal}) concerns examples that were included for the original converged parameter $\thetastar$. Hence, when averaging over the models, each model should only update the \trak scores corresponding to examples in $S'$. However, we found that the estimates are marginally better when we update the estimates for the entire training set $S$ (i.e., even those that were not trained on).

\paragraph{Generalization across different $\alpha$'s.}
A possible concern is that we overfit to a particular regime of $\alpha$ used in evaluating with the LDS. In \Cref{fig:jointplot}, we evaluate \trak scores (computing using $\alpha=0.5$) in other regimes and find that they continue to be highly predictive (though with some degradation in correlation).
More generally, our various counterfactual evaluations using the full training set (CIFAR-10 brittleness result, the CLIP counterfactuals) indicate that \trak scores provide valuable information on the full training set.

\subsection{Linearity and model output function}
\paragraph{Latent assumption of linearity.}
Our evaluation of data attributions cast them as linear predictors (or datamodels).
While not always immediate, linearity is a latent assumption behind most of the prior methods that we evaluate in this paper. The approach based on influence functions \cite{koh2017understanding,koh2019accuracy} typically uses the sum of LOO influences to estimate influences for groups of examples. Shapley values satisfy additivity by construction \cite{ghorbani2019data,jia2019towards}.
For empirical (or subsampled) influences \cite{feldman2020neural}, its definition does not make it obvious, but we can show that it indeed corresponds to a first-order Taylor approximation of the model output function, which is a linear function of included training examples. The \tracin estimator also implicitly assumes linearity \citep{pruthi2020estimating}

That said, some earlier works also incorporate additional corrections beyond the first order linear terms \cite{basu2019second} and find that they improve correlation with the true influences.

\paragraph{Choice of model output function $f$.}
In our experiments, we choose the model output function suitable for the task at hand: for classification and language modeling, we used a notion of margin that is equivalent to the logit function, while for CLIP, we used a similar one based on the CLIP loss.

Our particular choice of the logit function ($\log p/(1-p)$) in the multi-class classification case was motivated by theoretical \cite{saunshi2023understanding} and empirical \cite{ilyas2022datamodels} observations from prior work.
In particular, this choice of model output function is well approximated by linear datamodels, both in practice \citep{ilyas2022datamodels} and in theory \citep{saunshi2023understanding}.
The notion of margin used in \citet{ilyas2022datamodels}---where the margin is computed as the logit for the correct class minus the second highest class---can also be viewed as an approximation to the one used here.
More generally, choosing a good $f$ boils down to linearizing (w.r.t. $\theta$) as much of the model as possible, but not too much. On one extreme, choosing $f(z) = z$ (i.e., linearizing nothing, as there is no dependence on $\theta$) leaves us with all of the complex non-linearity coming from dependence of $L$ on $f$, so we have to use (at least) second-order approximations; this is essentially the same approximation used by the Hessian-based influence function.  On the other extreme, if we let $f = L$, we linearize too much, which does not work well as $L$ in general is non-linear.





