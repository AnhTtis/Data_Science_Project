We begin with a focus on the supervised learning regime.
We will denote by $S = \{z_1, \ldots, z_n\}$ an
ordered training set of examples, where each $z_i = (x_i, y_i) \in \mathcal{Z}$ is an input-label pair.
We represent machine learning models (implicitly) using a {\em model output function}
$\modeleval{z}{\theta}$, which maps an example of interest $z$ and model parameters
$\theta$
to a real number.
There are a variety of model output functions that one can employ---for example,
the loss $\loss{z}$ of the model on the example $z$ is a natural choice.
Ultimately, though, the appropriate model output function to use
will depend on the setting that we are studying.

Throughout this work, we also assume that models are trained
to minimize the empirical training loss, i.e., that the parameters
of these models are given by
\begin{equation}
  \label{eq:erm}
  \thetastar(S) \coloneqq \arg\min_{\theta} \sum_{z_i \in S} \loss{z_i},
\end{equation}
where, again, $\loss{z_i}$ is the model training loss on example $z_i$.
We write $\thetastar$ as a function of $S$ as we will later consider varying $S$---but when $S$ is clear from the context, we omit it and just write $\thetastar$.

In this paper, our overarching goal is to trace model predictions back to the composition of training data.
This goal---which we refer to as {\em data attribution}---is not new.
Prior work has approached it using methods such as
influence functions and their many variants
\citep{hampel2011robust,koh2017understanding,feldman2020neural,hammoudeh2022identifying};
sampling-based estimators such as Shapley values \citep{lundberg2017unified},
empirical influences \citep{feldman2020neural},
and datamodels \citep{ilyas2022datamodels}; as well as
various other approaches
\citep{yeh2018representer,pruthi2020estimating,hammoudeh2022training}.  Each of
these methods implements a similar interface: given a model
and an output of interest (e.g., loss for a given prediction),
a data attribution method computes a {\em score} for each
training input indicating its importance to the output of interest.
\cref{def:attribution} below makes
this interface precise:

\begin{definition}[\em Data attribution]
  \label{def:attribution}
  Consider an ordered training set of examples $S = \{z_1, \ldots, z_n\}$ and a
  model output function $\modeleval{z}{\theta}$.
  A {\em data attribution method} $\tau(z, S)$
  is a function $\tau: \mathcal{Z} \times \mathcal{Z}^n \to \mathbb{R}^n$ that,
  for any example $z \in \mathcal{Z}$ and a training set $S$,
  assigns a (real-valued) score
  to each training input $z_i \in S$ indicating its importance\footnote{We make the notion of ``importance'' more
  precise later (in Definition \ref{def:attr_output}).}
  to the model output $\modeleval{z}{\thetastar(S)}$.
  When the second argument $S$ is clear from the context, we will omit the second argument and
  simply write $\tau(z)$.
\end{definition}
\begin{example}[\em Influence functions as an attribution method]
  \label{ex:infl_attr}
  An example of a data attribution method is the {\em influence function} approach,
  a concept from robust statistics.
  For a specific model output function $\modeleval{z}{\theta}$ on an example of interest $z$, an influence function assigns a score to
  each training example $z_i$ that approximates the effect on the output $\modeleval{z}{\theta}$ of infinitesimally up-weighting
  that training example. %
  A classic
  result from \citep{cook1982residuals} shows that this score can be computed as
  \[
    \tau_{\text{IF}}(z)_i = \nabla_\theta \modeleval{z}{\thetastar}^\top\cdot H_\theta^{-1}\cdot \nabla_\theta \lossstar{z_i},
  \]
  where, again, $\thetastar$ are the parameters that minimize the empirical risk, $\lossstar{z_i}$ is the training loss of example $z_i$, and $H_\theta$ is the Hessian
  $\nabla_\theta^2 \frac{1}{n} \sum_{z_i\in S} \lossstar{z_i}$ of the total training loss.
\end{example}

\paragraph{Evaluating attribution methods.}
Given the variety of existing
data attribution methods, we need a method %
to evaluate them in a consistent way.
One popular approach is to simply manually inspect the
training examples that the method identifies as most important
for a given prediction or set of predictions.
Such manual inspection can be a
useful sanity check, %
but
is
also often subjective and unreliable.
For example, in computer vision,
visual similarity between two images
does not
fully capture
the influence of one on the other in terms of model behavior~\citep{ilyas2022datamodels}.

A more objective alternative is to treat the
scores from a data attribution method as estimates of some
ground-truth parameters---such as leave-one-out influences
\citep{koh2017understanding,basu2021influence,koh2019accuracy}
or Shapley values \citep{lundberg2017unified}---and
then measure the accuracy of these estimates.
This approach to evaluation is not only more quantitative than visual inspection
but also inherits all favorable properties of the ground-truth parameter
being considered (e.g., additivity of Shapley values \citep{shapley1951notes}).
However, getting access to these ground-truth parameters can be prohibitively expensive in large-scale settings.



Finally, yet another possibility is to measure the
utility of data attribution scores for
an auxiliary task such as identifying mislabeled data
\citep{koh2017understanding,hammoudeh2022identifying} or active learning
\citep{jia2021scalability}.
This approach can indeed be a
useful proxy for evaluating data attribution methods, but the resulting
metrics may be too sensitive to the particulars of the
auxiliary task and thus make comparisons across different problems and settings difficult.


\subsection{The linear datamodeling score (LDS)}
\label{sub:lds}
Motivated by the above shortcomings of existing methodologies,
we propose a new metric for evaluating data attribution methods.
At the heart of our metric is the perspective that an effective data attribution
method should be able to make accurate {\em counterfactual predictions} about model outputs.
In other words, if a method can accurately quantify the importance of individual training examples to model outputs,
it should also be
able to predict how model outputs change when the training set is modified
in a particular way.

Inspired by \citet{ilyas2022datamodels},
we cast this counterfactual estimation task as that
of predicting the model output function
$\modeleval{z}{\thetastar(S')}$ given different subsets of the training set $S'$.
More precisely, consider---for a fixed example of interest $z \in \mathcal{Z}$---the model output
$\modeleval{z}{\thetastar(S')}$ arising from training on a subset $S'
\subset S$ of the training set $S$ (see \eqref{eq:erm}).\footnote{In many settings,
the non-determinism of training makes this model output function a random variable,
but we treat it as deterministic to simplify our notation.
We handle non-determinism explicitly in \cref{sec:estimator_algo}.}
Since $z$ is fixed and the learning algorithm $\thetastar(\cdot)$ is fixed,
we can view
this model output as a function of $S'$ alone.
A good data attribution method should help us {predict} the former
from the latter.


To operationalize this idea, we first need a way of converting a given data attribution
method $\tau(\cdot)$ into a counterfactual predictor.
We observe that the vast majority of data attribution methods are {\em additive}---that is,
they define the importance of a group of training examples to
be the sum of the importances of the examples in the group.\footnote{
  Note that this additivity assumption can be {explicit} or {implicit}.
  Shapley values \citep{shapley1951notes} and datamodels \citep{ilyas2022datamodels},
  for example, take additivity as an axiom.
  Meanwhile, attribution methods based on influence functions
  \citep{hampel2011robust,koh2017understanding,koh2019accuracy}
  implicitly use a first-order Taylor approximation of the loss function with respect
  to the vector of training example loss weights, which is precisely equivalent to
  an additivity assumption.
}
Motivated by this observation, we define an attribution method's
{\em prediction} of the model output for a subset $S' \subset S$
as the sum of the corresponding scores:
\begin{definition}[\em Attribution-based output predictions]
  \label{def:attr_output}
  Consider a training set $S$, a model output function $\modeleval{z}{\theta}$,
  and a corresponding data attribution method $\tau$
  (see Definition \ref{def:attribution}).
  The {\em attribution-based output prediction} of the model output
  $\modeleval{z}{\thetastar(S')}$ is defined as
  \begin{equation}
    \label{eq:data_attr_agg}
    g_\tau(z, S'; S) \coloneqq \sum_{i\ :\ z_i \in S'} \tau(z, S)_i = \tau(z, S) \cdot \bm{1}_{S'},
  \end{equation}
  where $\bm{1}_{S'}$ is the {\em indicator vector} of the subset $S'$ of $S$
  (i.e., $(\bm{1}_{S'})_i = \bm{1}\{z_i \in S'\}$).
\end{definition}
Intuitively, \cref{def:attr_output} turns any data attribution
method into a counterfactual predictor.
Specifically, for a given counterfactual training set
$S' \subset S$, the attribution method's prediction is simply the sum of the
scores of the examples contained in $S'$.

Now that we have defined how to derive predictions from an attribution method,
we can evaluate these predictions using the {\em linear datamodeling score},
defined as follows:
\begin{definition}[\em Linear datamodeling score]
  \label{def:datamodeling}
  Consider a training set $S$, a model output function $\modeleval{z}{\theta}$,
  and a corresponding data attribution method $\tau$
  (see Definition \ref{def:attribution}).
  Let $\{S_1, \ldots, S_m: S_i \subset S\}$ be $m$
  randomly sampled subsets of the training set $S$,
  each of size $\alpha \cdot n$
  for some $\alpha \in (0, 1)$.
  The {\em linear datamodeling score} (LDS) of a data attribution $\tau$
  for a specific example $z \in \mathcal{Z}$ is given by
  \[
    LDS(\tau, z) \coloneqq \bm{\rho}(\{\modeleval{z}{\thetastar(S_j)}: j \in [m]\},
    \{g_\tau(z, S_j;S): j \in [m]\}),
  \]
  where $\bm{\rho}$ denotes Spearman rank correlation \citep{spearman1904proof}.
  The attribution method's LDS for an entire test set is then simply the average
  per-example score.
\end{definition}




\noindent Note that the linear datamodeling score defined above
is quantitative,
simple to compute,\footnote{
  In practice, we can estimate the LDS with 100-500 models, as the average rank correlation (over sufficiently number of test examples) converges fairly quickly with sample size.
}
and not tied to a specific task or modality.






\subsection{An oracle for data attribution}
\label{subsec:oracle}
\cref{def:datamodeling} immediately suggests an ``optimal''
approach to data attribution (at least, in terms of optimizing LDS).
This approach simply samples random subsets $\{S_1,\ldots S_m\}$ of the training set;
trains a model on each subset (yielding $\{\thetastar(S_1), \ldots, \thetastar(S_m)\}$);
evaluates each corresponding model output function $\modeleval{z}{\thetastar(S_j)}$;
and then {\em fits} scores $\tau(z)$ that predict $\modeleval{z}{\thetastar(S_i)}$
from the indicator vector $\bm{1}_{S_i}$ using (regularized) empirical risk minimization.
Indeed, \citet{ilyas2022datamodels} take exactly this approach---the resulting
datamodel-based attribution for an example $z$ is then given by
\begin{align}
  \label{eq:testset_datamodels}
  \tau_{\textsc{dm}}(z) \coloneqq \min_{\beta \in \mathbb{R}^n}
  \frac{1}{m} \sum_{i=1}^m \lr{{\beta}^\top \mask{S_i} - \modeleval{z}{\thetastar(S_i)}}^2 + \lambda \|\beta\|_1.
\end{align}
The attributions $\tau_{\textsc{dm}}(z)$ turn out to indeed perform well according
to \cref{def:datamodeling}---that is, they yield counterfactual
predictions that are highly correlated with true model outputs (see \cref{fig:headline}).
Unfortunately, however, estimating accurate linear predictors \eqref{eq:testset_datamodels}
may require tens (or even hundreds) of thousands of samples
$(S_j, \modeleval{z}{\thetastar(S_j)})$.
Since each one of these samples involves training a model from scratch,
this direct estimator can be expensive to compute in large-scale settings.
More generally, this limitation applies to all sampling-based attribution methods, such as empirical influences \citep{feldman2020neural,carlini2022quantifying} and Shapley values \citep{ghorbani2019data,jia2019towards}.

In light of the above, we can view the approach of \citet{ilyas2022datamodels} as an ``oracle''
of sorts---it makes accurate counterfactual predictions
(and as a result has found downstream utility \citep{ilyas2022datamodels,shah2022modeldiff,chang2022careful}),
but is (often prohibitively) costly to compute.



\subsection{Data attribution methods beyond sampling}
How might we be able to circumvent the estimation cost of sampling-based attributions?
Let us start by examining the existing data attribution methods---specifically, the ones that
use only one (or a few) trained models---and evaluate them on our LDS benchmark.

\paragraph{Simulating re-training with influence functions.}
The bottleneck of the ``oracle'' datamodels attribution method \eqref{eq:testset_datamodels}
\citep{ilyas2022datamodels} is
that obtaining each sample $(S_j, \modeleval{z}{S_j})$ requires re-training our
model of interest from scratch on each subset $S_j$.
An alternative approach could be to {\em simulate} the effect of this re-training by
making some structural assumptions about the model being studied---e.g., that
its loss is locally well-approximated by a quadratic.
This idea has inspired a long line of work around {\em influence function estimation}
\citep{koh2017understanding,pruthi2020estimating,schioppa2022scaling}.
The resulting {\em influence function attributions}
(\cref{ex:infl_attr})
accurately approximate linear models and other
simple models, but
can
perform poorly in non-convex settings (e.g., in the context of deep neural networks) \citep{basu2021influence,ilyas2022datamodels,bae2022if}.
Indeed, as we can see in Figure~\ref{fig:headline}
(and as we later study in \cref{sec:eval}), estimators based on
influence functions \citep{koh2017understanding,schioppa2022scaling,hammoudeh2022identifying}
significantly underperform on our LDS benchmark (\cref{def:datamodeling}) when evaluated on neural networks on standard vision and natural language tasks.






\paragraph{Heuristic measures of example importance.}
There are also approaches that use more heuristic measures of
training example importance for data attribution.
These include methods based on, e.g.,
representation space similarity
\citep{zhang2018unreasonable,hanawa2021evaluation}
or gradient agreement
\citep{hammoudeh2022identifying}.
While such methods often yield qualitatively compelling
results, our experiments (again, see Figure~\ref{fig:headline})
indicate that, similarly to influence-based estimators,
they are unable to make meaningful counterfactual predictions
about model outputs in the large-scale, non-convex
settings we evaluate them on.