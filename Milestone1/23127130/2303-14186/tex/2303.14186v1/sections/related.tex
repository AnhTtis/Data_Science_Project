In this section, we highlight and discuss how \trak connects to prior works on training data attribution, the neural tangent kernel, and kernel approximation.

\paragraph{Training data attribution.}
There is a sizable body of work on data attribution methods.
Here we discuss approaches most similar to ours, but we refer the reader back to \Cref{sec:prelim} for an overview of prior work on data attribution methods and to \citet{hammoudeh2022training} for an even more
extensive survey.

In the setting of generalized linear models, \citet{wojnowicz2016influence} speed up classical influence estimation
(\Cref{lem:formal})
by leveraging random projections. Also, \citet{khanna2019interpreting} employ a similar estimator based on the Fisher
matrix for data attribution and subset selection. Their experiments are limited though
to small neural networks and linear models.
 Most similarly to our approach, \citet{achille2021lqf} leverage the linearized model for approximating influence functions
(among other applications). However,
their approach introduces several changes to the model of interest (such as modifying activations, loss, and
regularization) and focuses on finetuning in smaller-scale settings, whereas \trak can be applied directly to the original model (and at scale).

Similarly to us, prior works also investigate the tradeoffs between scalability and efficacy of
data attribution methods. For instance, \citet{jia2021scalability} study these tradeoffs by proposing new metrics and comparing according to them leave-one-out methods (e.g., influence functions) and Shapley values.
They put forth, in particular, a new estimator for Shapley values that is based on approximating the original model with a $k$-nearest neighbors model over the pre-trained embeddings---this can be viewed as an alternative to working with the linearized model.

As discussed in \Cref{sec:prelim}, a major line of work uses {\em Hessian-based influence functions} for data attribution \cite{koh2017understanding,koh2019accuracy,basu2021influence}.
In particular, the influence function effectively---up to an error that can be bounded---computes the one-step Newton approximation  with respect to the full model parameters \cite{koh2019accuracy}.
Recall that \trak also leverages the one-step Newton approximation in order to estimate  leave-one-out influences for logistic regression (Step 3 of our algorithm).
However, in contrast to the influence function approach, the Hessian matrix we leverage (the matrix $X^\top R X$ in \Cref{lem:formal}) is positive (semi)definite as it is computed with respect to the {\em linearized model} rather than the original model.
As a result, computing \trak does not require the use of additional regularization (beyond the one implicitly induced by our use of random projections), which is practically necessary in the influence function approach.

\paragraph{Neural tangent kernel.}
The neural tangent kernel (NTK) \cite{jacot2018neural} and its generalizations \cite{yang2021tensor} are
widely studied as a tool for theoretically analyzing generalization
\citep{arora2019fine}, optimization \citep{wei2019regularization}, and
robustness \citep{gao2019convergence} of (overparameterized) neural networks.  While these works focus on neural networks in the their
large or infinite-width limit, a line of recent works
\citep{mu2020gradients,achille2021lqf,long2021properties,atanasov2022neural,wei2022more,malladi2022kernel,atanasov2023onset,ma2022behind}
studies instead the
finite-width {\em empirical NTK} (eNTK).
Our \trak estimator is partly motivated by the observation from this line of work that kernel regression with the eNTK provides a good approximation to the original model.

While we leverage the eNTK approximation for data attribution, prior works leveraged the NTK and eNTK for
various other applications, such as studying generalization
\citep{bachmann2022generalization}, sample selection for active
learning \citep{holzmuller2022framework}, model selection
\citep{deshpande2021linearized}, federated learning
\citep{yu2022tct}, and fast domain adaptation \citep{maddox2021fast}.  Our
reduction to the linear case (Step 1 in \Cref{sec:estimator_algo}) is analogous to the approach of
\citet{bachmann2022generalization} that leverages formulas for the leave-one-out error of kernel methods coupled with the NTK approximation to
estimate the generalization error.
Another related work is that of \citet{zhang2022rethinking}, who theoretically characterize the accuracy of the Hessian-based influence function
in the NTK regime (i.e., large-width limit).


Finally, although the work on NTK popularized the idea of leveraging gradients as features,
similar ideas can be traced back to works on the Fisher kernel and related ideas
\cite{zinkevich2017holographic}.

\paragraph{Kernel methods and random projections.}
Our application of random projections to improve computational efficiency of kernel approximation is a widely used idea in
kernel methods
\citep{blum2006random,rahimi2007random}. Aside from computational advantages, this technique can also provide insight into empirical phenomena. For example, \citet{malladi2022kernel} use the
kernel view along with random projections as a lens to explain the efficacy of subspace-based finetuning methods.
