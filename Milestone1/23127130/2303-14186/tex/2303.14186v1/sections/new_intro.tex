Training data is a key driver of model behavior in modern machine learning systems.
Indeed, model errors, biases, and capabilities can all stem from the training data \citep{ilyas2019adversarial,gu2017badnets,geirhos2019imagenet}.
Furthermore, improving the quality of training data generally improves the performance
of the resulting models \citep{huh2016makes,lee2022deduplicating}.
The importance of training data to model behavior has motivated extensive work
on {\em data attribution}, i.e., the task of tracing model predictions back to the
training examples that informed these predictions.
Recent work
demonstrates, in particular, the utility of data attribution methods in applications such as
explaining predictions \citep{koh2017understanding,ilyas2022datamodels},
debugging model behavior \citep{kong2022resolving,shah2022modeldiff},
assigning data valuations \citep{ghorbani2019data,jia2019towards},
detecting poisoned or mislabeled data \citep{lin2022measuring,hammoudeh2022identifying},
and curating data \citep{khanna2019interpreting,liu2021influence,jia2021scalability}.


However, a recurring tradeoff in the space of data attribution methods is that of
{\em computational demand} versus {\em efficacy}.
On the one hand, methods such as
influence approximation \citep{koh2017understanding, schioppa2022scaling}
or gradient agreement scoring \citep{pruthi2020estimating}
are computationally attractive
but can be unreliable
in non-convex settings
\citep{basu2021influence,ilyas2022datamodels,akyurek2022towards}.
On the other hand, sampling-based methods such as empirical influence
functions \citep{feldman2020neural}, Shapley value estimators \citep{ghorbani2019data,jia2019towards} or datamodels \citep{ilyas2022datamodels} are
more successful at accurately attributing predictions to training data
but require training thousands (or tens of thousands) of models
to be effective.
We thus ask:
\begin{center}
    {\em Are there data attribution methods that are both scalable and effective in large-scale non-convex settings?}
\end{center}

\begin{figure}[!htb]
    \centering
    \input{figures/main_figure.tex}
\caption{Our data attribution method \trak achieves state-of-the-art tradeoffs between
speed and efficacy.
Here, we benchmark its performance relative to prior methods on
\cifarten-trained ResNet-9 models and \qnli-trained \bertbase models. The $x$-axis indicates
the time (in minutes) it takes to run each method on a single A100 GPU
(see \Cref{app:wall_time} for details).
The $y$-axis indicates the method's efficacy
as measured by its ability to make accurate counterfactual predictions
(see \Cref{def:attr_output} for the precise metric);
error bars indicate 95\% bootstrap confidence intervals.
}
\label{fig:headline}
\end{figure}


To properly answer this question,
we first need a unifying metric for evaluating data attribution methods.
To this end, we adopt the view that a data attribution method is useful
insofar as it can make accurate {\em counterfactual predictions}, i.e.,
answer questions of the form
``what would happen if I trained the model on a given subset $S'$ of my training set?''
This perspective motivates a benchmark---inspired by the datamodeling framework
\citep{ilyas2022datamodels}---that
measures the correlation between true model outputs
and attribution-derived predictions for those outputs.


With this benchmark in hand, in \cref{sec:method} we consider our motivating question and introduce \trak
(\spelledout),
a new data attribution method for parametric, differentiable models.
The key idea behind \trak is to first approximate models with a kernel machine
(e.g., through the empirical neural tangent kernel \citep{jacot2018neural})
and then to leverage our understanding of the resulting kernel domain to derive data attribution scores.

We demonstrate that %
\trak retains the efficacy of sampling-based
attribution methods while being several orders of magnitude cheaper computationally.
For example (Figure \ref{fig:headline}), on \textsc{CIFAR-10} (image classification)
and \textsc{QNLI} (natural language inference),
 \trak can be as
effective as datamodels \citep{ilyas2022datamodels} while being
100-1000x faster to compute.
Furthermore, \trak is as fast as existing gradient-based methods such as
TracIn \citep{pruthi2020estimating} or variations of influence functions \citep{koh2017understanding,schioppa2022scaling},
while being significantly more predictive of model behavior.

As a result, \trak enables us to study the connection between model predictions
and training data in large-scale settings.
For example,
we use \trak to study predictions of ImageNet classifiers (\cref{sec:eval});
to understand the shared image-text embedding space of \clip models \citep{radford2021learning} trained
on \mscoco \citep{lin2014microsoft} (\cref{sec:CLIP});
and to fact-trace language models
(a 300M-parameter \texttt{mT5-small} model \cite{raffel2020exploring,xue2021mt5})
finetuned on \ftracetrex (\cref{subsec:fact_trace}).


