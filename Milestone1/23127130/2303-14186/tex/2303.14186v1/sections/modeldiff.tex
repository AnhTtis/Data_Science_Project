\subsection{Accelerating datamodel applications}
\label{subsec:datamodel_apps}
Our evaluation thus far has demonstrated that data attribution scores computed with
\trak  can {\em predict} how a given model's output changes as a function of the composition of the corresponding model's training set.
While the capability to make such predictions is useful in its own right, prior
work has shown that this primitive also enables many downstream applications
\cite{koh2017understanding,jia2019towards,alaa2020discriminative}.  For example,
prior works leverage datamodel scores to identify brittle predictions
\cite{ilyas2022datamodels} and to compare different learning algorithms
\cite{shah2022modeldiff}. We now show that using \trak in place of datamodel
scores can significantly speed up these downstream applications too.

\subsubsection{Estimating prediction brittleness}
\citet{ilyas2022datamodels} use datamodel scores to
provide {\em lower bounds} on the {\em brittleness} of a given example---that
is, given an example of interest $z$, they identify a subset of the training set
whose removal from the training data causes the resulting re-trained model to
misclassify $z$.
The brittleness estimation algorithm that \citet{ilyas2022datamodels} leverage
hinges on the fact that the datamodel attribution function $\tau_\text{DM}(z)$
can accurately predict model outputs, i.e., achieve high LDS. Motivated by
\trak's good performance on the linear datamodeling task (see, e.g.,
\cref{fig:headline_full}), we examine estimating the brittleness of \cifarten examples
using \trak scores in place of datamodel ones (but otherwise following the
procedure of \citet{ilyas2022datamodels}). Our results (see
\Cref{fig:brittle}) indicate that \trak scores computed from an ensemble of just
100 models are about as effective at estimating brittleness as datamodel scores
computed from 50,000 models. Thus, \trak scores can be a viable (and orders of
magnitude faster) alternative to datamodels for estimating
prediction brittleness.

\begin{figure*}[t]
    \centering
    \input{figures/brittleness.tex}
    \caption{{\em Using \trak scores to identify brittle model predictions.}
    Following the methodology of \citet{ilyas2022datamodels}, we apply
    different data attribution methods to estimate the brittleness of model predictions on
    examples from the \cifarten validation set. The number of models used by each attribution method is specified in parentheses, e.g., \trak
    (100) indicates that \trak scores were computed using an ensemble of 100 trained
    models.}
\label{fig:brittle}
\end{figure*}

\subsubsection{Learning algorithm comparisons}
A useful way to leverage datamodels is to view them as {\em data representations}.
More specifically, following \citet{ilyas2022datamodels}, for an example of interest $z$, one can view the datamodel
attribution $\tau_\text{DM}(z)$ as an embedding of $z$ into $\mathbb{R}^{n}$,
where $n$ is the size of the training dataset.  Analyzing examples in such
induced {\em datamodel representation spaces} turns out to enable uncovering
dataset biases and model-specific subpopulations
\cite{ilyas2022datamodels}.
Furthermore, this representation space is not specific to a particular model
instance or architecture---it is {\em globally aligned} in the sense that for
the same example $z$, the attribution score $\tau_\text{DM}(z)_i$ of a given
train example $i$ has a consistent interpretation across {\em different}
learning pipelines.
\citet{shah2022modeldiff} leverage the properties of the datamodel
representation space to perform model-agnostic {\em learning algorithm comparison}
(called \modeldiff): given two learning algorithms, they show how to
use datamodels to identify {\em distinguishing features}, i.e., features that
are used by one learning algorithm but not the other.



Once again, motivated by \trak's good performance on the LDS metric,
we investigate whether \trak scores can substitute for datamodel scores in this
context. To this end, we revisit one of the case studies from
\citet{shah2022modeldiff}---the one that compares image classifiers trained {with}
and {without} data augmentation, and identifies features that distinguish these
two classes of models.
When applied to this case study, \modeldiff computed with \trak scores recovers similar distinguishing features
to the ones originally found by \citet{shah2022modeldiff} (using datamodel scores)---see \Cref{fig:modeldiff} for more details.
Also, employing \trak scores in place of datamodel scores reduces the total
computational cost by a factor of 100, showing, once again, that \trak can dramatically
accelerate downstream tasks that rely on accurate attribution scores.





