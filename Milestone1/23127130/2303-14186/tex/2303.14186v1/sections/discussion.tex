In our work, we formalize the problem of data attribution and introduce a new
method, \trak, that is effective and efficiently scalable. We then demonstrate the usefulness of \trak in a variety of
large-scale settings: image classifiers trained on \cifar and ImageNet, language models (\bert
and \mtfive), and image-text models (\clip).

Still, \trak is not without limitations: in particular, it requires the model to be differentiable, and its effectiveness also depends
on the suitability of the linear approximation.
That said, the success of the applying the NTK on language modeling tasks \cite{malladi2022kernel} as well as our own experiments both suggest that this approximation is likely to continue to work for larger models.
In this case, \trak presents a unique opportunity to reap the benefits of data attribution methods in previously untenable domains, such as large generative models.
In \cref{app:future_work}, we further discuss possible avenues for future work.

