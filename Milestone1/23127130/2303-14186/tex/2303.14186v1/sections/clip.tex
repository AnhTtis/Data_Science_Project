\subsection{Attributing CLIP models}
\label{sec:CLIP}
Recent works have found that one can leverage natural language supervision to
help models learn a rich joint image-text embedding space. In particular, CLIP (Contrastive Language-Image Pre-training)
\cite{radford2021learning} representations have become a versatile primitive
bridging visual and language domains and is used, for example, for zero-shot
classification \cite{radford2021learning} and as text encoders for latent
diffusion models \cite{rombach2022high}. While the
quality of these representations---as measured by aggregate metrics such as
downstream zero-shot accuracy---appears to be driven largely by the properties and scale of
the training datasets \cite{fang2022data,santurkar2022caption,cherti2022reproducible},
we lack a fine-grained understanding of how the composition of the training data contributes to learning well-aligned representations.
To that end, we use \trak to investigate how training data influences the resulting
CLIP embeddings at a {\em local} level. That is, we want to be able to pin-point
training examples that cause a model to learn a given {\em specific}
image-caption pair association.


\subsubsection{Computing \trak for CLIP}
Similarly to the classification setting we were considering so far, we need to
first choose an appropriate model output function (see, e.g.,
\cref{eq:modelout_mc})
to compute attribution scores with \trak.
This choice will be motivated by the CLIP training loss (which we review below)
and will reduce our setting back to the classification case.

\paragraph{The CLIP loss.}
A CLIP model with parameters $\theta$ takes in an image-caption pair $(x,y)$ and
outputs an image embedding $\imagemb{x}$ and a text embedding $\textemb{y}$.
Given a (random) batch of training examples $B = \{(x_1,y_1),...,(x_n,y_n)\}$,
the CLIP training loss computes all $n \times n$  pairwise cosine similarities
between the image and text embeddings
\[
    S_{ij} \coloneqq \imagemb{x_i} \cdot \textemb{y_j},
\]
and aims to maximize the cosine similarities $S_{ii}$ of correct pairs while
minimizing the cosine similarities $S_{ij}$, for $i\ne j$, of incorrect pairs.
More specifically, the training loss of example $(x_i,y_i) \in B$ is defined as
the following symmetric cross entropy over the similarity scores $S_{ij}$:
    \begin{equation}
        \loss{x_i,y_i} =  - \log \frac{\exp(S_{ii})}{\sum\limits_{1\le j \le n} \exp(S_{ij})} - \log \frac{\exp(S_{ii})}{\sum\limits_{1\le j \le n} \exp(S_{ji})}, \label{eq:clip_loss}
    \end{equation}
where the first term corresponds to matching each image $x_i$ to its correct
caption $y_i$, and the second term corresponds to matching each caption to its
correct image. In effect, we are solving two classification problems: one where
the images are inputs and captions (from the same batch) are labels, and vice
versa.

\paragraph{Reducing to classification.}
Recall that in the classification setting we trained the model with the cross
entropy loss (i.e., $-\log p(z;\theta)$, where $p(z;\theta)$ is the
correct-class probability), and used the model output function
$\modeleval{z}{\theta} = \log p(z;\theta)/(1-p(z;\theta))$
(\Cref{eq:modelout_mc}), i.e., the logit transform of the correct-class
probability to compute \trak scores.

To take advantage of the same formula in the CLIP setting, note that our loss
(\ref{eq:clip_loss}) can be viewed as having the form
\begin{align*}
    \loss{x_i,y_i} = -\log{p_1(x_i,y_i; \theta)} - \log{p_2(x_i,y_i; \theta)},
\end{align*}
where $p_1(x_i,y_i; \theta)$ corresponds to the probability of matching an image
to its corresponding caption based on the cosine similarity, and likewise for
$p_2(x_i,y_i;\theta)$. A natural choice of model output function in this case,
then, is using the sum of the model output functions corresponding to the
two classification problems:
    \begin{align*}
        \modeleval{x_i,y_i}{\theta} &\coloneqq \log\left(\frac{p_1(x_i,y_i; \theta)}{1 - p_1(x_i,y_i; \theta)}\right) + \log\left(\frac{p_2(x_i,y_i; \theta)}{1 - p_2(x_i,y_i; \theta)}\right) \\
         &= -\log\sum\limits_{1 \le j \le n}\exp(S_{ij} - S_{ii}) - \log\sum\limits_{1 \le j \le n}\exp(S_{ji}-S_{ii}).
    \end{align*}
Indeed, this choice allows us once again (see \Cref{ssec:multiclass}) to reduce
our problem to an instance of logistic regression and apply the same formula for
influence approximation (\Cref{lem:formal}) as before.
We can then also compute \trak scores following the same approach
(i.e., using \Cref{alg:estimator_pseudo} in \cref{sec:pseudocode}).

\begin{figure*}[t]
    \includegraphics*[width=\textwidth]{figures/CLIP_topinfls.pdf}
    \includegraphics*[width=\textwidth]{figures/CLIP_topinfls2.pdf}
    \caption{
        {\em Attributing CLIP trained on \mscoco.} The first column shows two
        target image-caption pairs from the validation set of \mscoco. The
        second two columns display the nearest neighbors to the target in CLIP
        embedding space (using the average of image and text cosine
        similarities). The next two columns show the train set samples that,
        according to \trak, are most helpful for aligning the image embedding to
        the caption embedding. Similarly, the last two columns display the train
        samples that are the most detracting from aligning the image and caption
        embeddings. In \Cref{app:more_examples}, we display more examples and
        also compare to \tracin.
    }
    \label{fig:CLIP_nearest_neighbors}
\end{figure*}

\subsubsection{Results}
We train image-text models (with a ResNet-50 \cite{he2015deep} as the image
encoder and a Transformer \cite{vaswani2017attention} as the text encoder) using
the \clip objective on \mscoco \citep{lin2014microsoft}. To evaluate the
effectiveness of \trak applied to such \clip models, we perform a qualitative
(visual) analysis; and a quantitative (counterfactual) evaluation. In both cases, we
compare \trak with \tracin and \clip similarity distance\footnote{We use the average of cosine similarities between the image
embeddings and between the text embeddings.} baselines.

\paragraph{Visual analysis.}
\cref{fig:CLIP_nearest_neighbors} displays two target examples of interest along
with the corresponding training examples having the highest attribution scores
(according to \trak and \clip similarity distance---see \Cref{app:more_examples} for the analysis corresponding to \tracin). For the first example, the
nearest neighbor in the \clip space (the polar bear) turns out to have a {\em
negative} attribution score according to \trak. For the second example, the most
helpful \trak examples are the ones for which the captions contain the
phrase ``a couple of animals'' but where the images do not necessarily feature
giraffes (possibly because the target caption does not mention ``giraffe'' either). On
the other hand, the most helpful examples according to \clip
similarity distance all feature giraffes. These differences suggest that \trak
attribution scores may capture significantly different traits from \clip
similarity distance.

\paragraph{Counterfactual evaluation.}
We next investigate to what extent training examples identified by each
attribution method affect the \clip model's ability to learn a given
image-caption association.
Specifically, we say that a \clip model has {\em learned} a given association between
an image and a caption whenever their corresponding image and caption embeddings
have high cosine similarity relative to other image-caption pairs. To
evaluate each attribution method (i.e., \trak, \tracin, and \clip similarity
distance), for a given target image-caption pair, we remove from the training
set the $k$ examples with the most positive attribution scores a given
attribution method produces, and then re-train a model from scratch (averaging
over ten training runs to reduce stochasticity). Finally, we examine the decrease in
cosine similarity between the embeddings of target image and caption pair, and average this
result over different target pairs.

Our results (\Cref{fig:CLIP_barplot_counterfactuals}) indicate that removing
training inputs identified by \trak can significantly degrade the model's
ability to learn the target image-caption pair. Indeed, removing just $k=400$
target-specific training puts (i.e., less than $0.5\%$ of the train set)
decreases the (average) \clip similarity distance between the target image and
caption embeddings by 0.36. In contrast, removing the same number of nearest
neighbors in \clip space results in a much smaller effect size (a 0.11 decrease),
while removing training examples identified by \tracin has no
significant effect.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/CLIP_barplot_counterfactuals.pdf}
    \caption{
    {\em Which training inputs can we remove from the training set so as the
    resulting \clip model no longer associates a target image with its caption?}
    We measure how the cosine similarity between target image and caption
    embeddings is affected when we re-train a \clip model after removing the most
    influential training examples---as identified by \trak, \tracin, and \clip
    similarity distance. We report the {\em decrease} in cosine similarity, averaged over
    $100$ randomly selected image-caption pairs from the validation set. Error bars
    represent $95\%$ confidence intervals.
    }
    \label{fig:CLIP_barplot_counterfactuals}
\end{figure}