In \Cref{sec:eval}, we evaluated our data attribution method \trak on standard
image classification and NLP tasks and compared its performance to existing
attribution methods. We now illustrate the usefulness of \trak through three
additional applications:

\paragraph{Attributing \clip models.} In \cref{sec:CLIP}, we
use \trak to study image-text embeddings of models
trained with the \clip contrastive loss \citep{radford2021learning}. In
particular, we show how leveraging \trak allows us to identify small subsets of
the training set that, when removed, cause the resulting \clip embeddings to fail
to capture a given image-caption pair association.

\paragraph{Fact tracing language models.}
Next, in \cref{subsec:fact_trace}, we use \trak to provide data attribution for
language models \cite{vaswani2017attention}. In particular, we apply \trak to
{\em fact tracing}: the problem of tracing a language model's factual assertion
back to the corresponding training examples.
On the \ftracetrex  fact tracing benchmark, \trak significantly outperforms
the best gradient-based baseline (\tracin) used in prior work. Furthermore,
while \trak performs worse than an information retrieval baseline (BM25
\citep{robertson1995okapi}), we demonstrate that this is likely a shortcoming of the
benchmark rather than of \trak.
In particular, removing training examples traced by \trak (and re-training the
model) reduces that model's accuracy on the corresponding facts {\em more} than
removing training examples traced by BM25---and, in fact, more than
removing the {\em ground-truth} training examples as indicated by \ftracetrex.

\paragraph{Accelerating datamodel applications.} Finally, in
\cref{subsec:datamodel_apps}, we use \trak to accelerate two downstream
applications that leverage datamodel scores. That is, first, we look at the
problem of estimating {\em prediction brittleness} using datamodel scores
\cite{ilyas2022datamodels}. Then, we revisit the \modeldiff algorithm
\cite{shah2022modeldiff}, which leverages datamodel scores for {\em learning algorithm
comparison}, i.e., the task of distinguishing two learning algorithms based on
feature priors they instill. For both applications, using \trak scores in
place of datamodel scores reduces the total computational cost by at least a
factor of 100 while retaining the same effectiveness.

