\subsection{The one-step Newton approximation for leave-one-out influence}
\label{app:theory_newton}
The key formula we use in \trak is the estimate for the leave-one-out (LOO) influence in logistic regression (\Cref{lem:formal}).
Here, we reproduce the derivation of this estimate from \citet{pregibon1981logistic} then extend it to incorporate example-dependent bias terms. %

\paragraph{Convergence condition for logistic regression.}
Assume that we optimized the logistic regression instance via Newton-Raphson, i.e., the parameters are iteratively updated as
\begin{equation}
    \thetahat_{t+1} \leftarrow \thetahat_{t} + H_{\thetahat_t}^{-1} \nabla_\theta {L}(\thetahat_t) \label{eqn:newton_general}
\end{equation}
where $H_{\thetahat}$ is the Hessian and $\nabla_\theta {L}(\thetahat)$ is the gradient associated with the total training loss ${L}(\thetahat) = \sum_{z_i \in S} L(z_i;\theta)$.
In the case of logistic regression, the above update is given by
\begin{equation}
    \thetahat_{t+1} \leftarrow \thetahat_t + (X^\top R X)^{-1} X^\top \hat{{q}} \label{eqn:newton_logistic}
\end{equation}
where $\hat{{q}} = \vec{1} - \hat{{p}}$ is the vector of the probabilities for the {\em incorrect} class evaluated at $\thetahat_t$ and $R = \text{\normalfont diag}(\hat{p} (1 -\hat{p})$ is the corresponding matrix.
Upon convergence, the final parameters $\thetastar$ satisfy the following:
\begin{equation}
    (X^\top R X)^{-1} X^\top {q}^\star = 0 \label{eqn:opt}
\end{equation}
where ${q}^\star$ is the incorrect-class probability vector corresponding to $\thetastar$.

\paragraph{The one-step Newton approximation.}
We estimate the counterfactual parameters $\thetastar_{-i}$ that would have resulted from training on the same training set excluding example $i$ by simply taking a single Newton step starting from the same global optimum $\thetastar$:
\begin{equation}
\thetastar_{-i} = \thetastar + (X_{-i}^\top R_{-i} X_{-i})^{-1} X_{-i}^\top {q}_{-i}^\star,
\end{equation}
where the subscript $-i$ denotes the corresponding matrices and vectors without the $i$-th training example.
Rearranging and using (\ref{eqn:opt}),
\begin{align*}
    \thetastar - \thetastar_{-i} &= -  (X_{-i}^\top R_{-i} X_{-i})^{-1} X_{-i}^\top {q}_{-i}^\star\\
    \thetastar - \thetastar_{-i} &= (X^\top R X)^{-1} X^\top {q}^\star -  (X_{-i}^\top R_{-i} X_{-i})^{-1} X_{-i}^\top {q}_{-i}^\star
\end{align*}
\noindent Using the Shermanâ€“Morrison formula to simplify above,\footnote{This is used also, for instance, to derive the LOO formulas for standard linear regression.} we have
\begin{equation}
    \thetastar - \thetastar_{-i} = \frac{(X^\top R X)^{-1} x_i}{1 - x_i^\top (X^\top R X)^{-1} x_i \cdot p_i^\star (1 - p_i^\star)} {q}^\star_i = \frac{(X^\top R X)^{-1} x_i}{1 - x_i^\top (X^\top R X)^{-1} x_i \cdot p_i^\star (1 - p_i^\star)} (1 - {p}^\star_i)
\end{equation}
The above formula estimates the change in the parameter vector itself. To estimate the change in prediction at a given example $x$, we take the inner product of the above expression with vector $x$ to get the formula in \Cref{lem:formal}.

The approximation here is in assuming the updates converge in one step. Prior works \cite{koh2019accuracy} quantify the fidelity of such approximation under some assumptions.
The effectiveness of \trak across a variety of settings suggests that the approximation is accurate in regimes that arise in practice.

\paragraph{Incorporating bias terms.}
The above derivation is commonly done for the case of standard logistic regression, but it also directly extends to the case where the individual predictions incorporate example-dependent bias terms $b_i$ that are independent of $\theta$. In particular, note that the likelihood function after linearization in Step 1 is given by
\begin{equation}
    p(z_i; \theta) = \sigma(-y_i \cdot (\nabla_\theta \modeleval{z_i}{\thetastar} \cdot \theta + b_i))
\end{equation}
where $\sigma (\cdot)$ is the sigmoid function.
Because the Hessian and the gradients of the training loss only depend on $\theta$ through $p(z_i; \theta)$, and because $b_i$'s are independent of $\theta$, the computation going from
\cref{eqn:newton_general} to \cref{eqn:newton_logistic} is not affected.
The rest of the derivation also remains identical as the bias terms are already incorporated into $p^\star$ and ${q}^\star$.

\paragraph{Generalization to other settings.}
\label{app:general_model_output}
While our derivations in this paper focus on the case of logistic regression, more generally, \trak can be easily adapted to any choice of model output function as long as the training loss $L$ is a convex function of the model output $f$.
The corresponding entries in the $\mathbf{Q}=\text{diag}(1-p^\star_i)$ matrix in \Cref{lem:formal} is then replaced by $\partial L / \partial f (z_i)$.
The $R$ matrix and the leverage scores also change accordingly, though we do not include them in our estimator (that said, including them may improve the estimator in settings beyond classification).

However, in general one needs care in choosing an appropriate model output function in order to maximize the performance on the linear datamodeling prediction task. If the chosen model output is not well approximated by a linear function of training examples, then that puts an upper bound on the predictive performance of {\em any} attribution method in our framework. We discuss appropriate choices of model output functions further in \Cref{app:linear}.

\subsection{Random projections preserve gradient flow}
\label{app:theory_jl}

In Step 2 of \trak, we use random projections to reduce the dimension of the gradient vectors. Here, we justify this approximation when our model is trained via gradient descent. Similar analysis has been used prior, e.g., by \citet{malladi2022kernel}.

In the limit of small learning rate, the time-evolution of model output $f(z;\theta)$ under gradient descent (or gradient flow) is captured by the following differential equation \citep{jacot2018neural}:
\begin{align}
    \frac{df(z;\theta)}{dt} = \sum\limits_i \frac{\partial \loss{z_i}}{\partial f(z_i; \theta)} \cdot (\nabla f(z_i;\theta) \cdot \nabla f(z;\theta)) \approx  \sum\limits_i \frac{\partial \loss{z_i}}{\partial f(z_i;\theta)} \cdot (g_i \cdot g(z)) \label{eqn:ode}
\end{align}
where $g_i$ and $g(z)$ are the gradients of the final model corresponding to examples $z_i$ and $z$ as before. The approximation is due to assuming that the gradients do not change over time.

If we treat the outputs $\{\hat{f}(z_i;\theta)\}_i$ as time-varying variables, then their time evolution is entirely described by the above system of differential equations (one for each $i$, replacing $z$ with $z_i$ above).
Importantly, the above equations only depend on the gradients through their inner products. Hence, as long as we preserve the inner products to sufficient accuracy, the resulting system has approximately the same evolution as the original one. This justifies replacing the gradient features with their random projections.




\subsection{Subsampling the training set}
\label{app:theory_alpha}

In Step 4 of our algorithm, we ensemble the attribution scores over multiple models.  As
we investigate in \Cref{app:ablation_num_models}, this significantly improves \trak's performance. An important design choice is training each model on a different random
subset of the training set.

This choice is motivated by the following connection between \trak scores and empirical influences \cite{feldman2020neural}.
Recall that we designed \trak to optimize the linear datamodeling score.
As we discuss in \Cref{sec:prelim}, datamodels can be viewed as an ``oracle'' for optimizing the same metric.
Further, as \citet{ilyas2022datamodels} observes, datamodels can be viewed as a regularized version of empirical influences \cite{feldman2020neural}, which are defined as a difference-in-means estimator,
\begin{align}
    \tau(z_j)_i &= \mathbb{E}_{S' \sim \mathcal{D}}[f(z_j;\thetastar(S'))|
    z_i \in S']
    - \mathbb{E}_{S' \sim \mathcal{D}}[f(z_j;\thetastar(S'))|
    z_i \not\in S']
\end{align}
where $\mathcal{D}$ is the uniform distribution over $\alpha$-fraction subsets of training set $S$. Assuming the expectation over $\alpha$-fraction subsets is identical to that over subsets of one additional element, we can rearrange the above expression as
\begin{align}
    \tau(z_j)_i &= \mathbb{E}_{S' \sim \mathcal{D}}[f(z_j;\thetastar(S' \cup \{z_i\})) - f(z_j;\thetastar(S'))].
\end{align}
The above expression is simply the expectation of leave-one-out influence over different random subsets. As the estimate from step 3 of our algorithm is specific to a single training set, we need to average over different subsets in order to approximate the above quantity.

In principle, the estimates computed from $\thetastar(S')$ only apply to the training examples included in the subset $S'$, since the underlying formula (\Cref{lem:formal}) concerns examples that were included for the original converged parameter $\thetastar$. Hence, when averaging over the models, each model should only update the \trak scores corresponding to examples in $S'$. However, we found that the estimates are marginally better when we update the estimates for the entire training set $S$ (i.e., even those that were not trained on).

\paragraph{Generalization across different $\alpha$'s.}
A possible concern is that we overfit to a particular regime of $\alpha$ used in evaluating with the LDS. In \Cref{fig:jointplot}, we evaluate \trak scores (computing using $\alpha=0.5$) in other regimes and find that they continue to be highly predictive (though with some degradation in correlation).
More generally, our various counterfactual evaluations using the full training set (CIFAR-10 brittleness estimates in \Cref{fig:brittle}, the CLIP counterfactuals in \Cref{fig:CLIP_barplot_counterfactuals}) indicate that \trak scores remain predictive near the $\alpha=1$ regime.

\subsection{Linearity and model output function}
\label{app:linear}

We study linear predictors derived from attribution scores, as linearity is a latent assumption for many popular attribution methods.
Linearity also motivates our choices of model output functions.

\paragraph{Latent assumption of linearity.}
Our evaluation of data attribution methods cast them as linear predictors.
While not always immediate, linearity is a latent assumption behind most of the prior methods that we evaluate in this paper.
Datamodels and Shapley values satisfy additivity by construction \cite{ghorbani2019data,jia2019towards}.
The approach based on influence functions \cite{koh2017understanding,koh2019accuracy} typically uses the sum of LOO influences to estimate influences for groups of examples.
Similarly, empirical (or subsampled) influences \cite{feldman2020neural} also correspond to a first-order Taylor approximation of the model output function. The \tracin estimator also implicitly assumes linearity \citep{pruthi2020estimating}.

That said, others works also incorporate additional corrections beyond the first order linear terms \cite{basu2019second} and find the resulting predictions better approximate the true influences.

\paragraph{Choice of model output function $f$.}
In our experiments, we choose the model output function suitable for the task at hand: for classification and language modeling, we used a notion of margin that is equivalent to the logit function, while for CLIP, we used a similar one based on the CLIP loss.

Our particular choice of the logit function ($\log p/(1-p)$) in the multi-class classification case was motivated by theoretical \cite{saunshi2023understanding} and empirical \cite{ilyas2022datamodels} observations from prior works.
In particular, this choice of model output function is well approximated by {\em linear} datamodels, both in practice and in theory.
A slightly different definition of margin used in \citet{ilyas2022datamodels}---where the margin is computed as the logit for the correct class minus the second highest class---can also be viewed as an approximation to the one used here.

More generally, choosing a good $f$ boils down to linearizing (w.r.t. $\theta$) as much of the model output as possible, but not too much. On one extreme, choosing $f(z) = z$ (i.e., linearizing nothing, as there is no dependence on $\theta$)
means that the one-step Newton approximation has to capture all of the non-linearity in both the model and the dependence of $L$ on $f$; this is essentially the same approximation used by the Hessian-based influence function.  On the other extreme, if we choose $f = L$, we linearize too much, which does not work well as $L$ in general is highly non-linear as a function of $f$.





