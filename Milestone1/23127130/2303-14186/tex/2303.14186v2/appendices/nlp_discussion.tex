\subsection{The \ftracetrex Dataset}
\label{app:fact_tracing:dataset}
The training set of \ftracetrex is sourced from the \trex dataset \citep{elsahar2018t},
with each training example excerpted from a DBPedia abstract \citep{hellmann2013integrating} and
annotated with a list of facts it expresses.\footnote{See \citep{akyurek2022towards}
for more details on the annotation methodology.}
The test set of \ftracetrex is sourced from the \texttt{LAMA} dataset \citep{petroni2019language},
and each test example is a sentence that expresses a single fact---every training
example that expresses the same fact is called a ``proponent'' of this test example.
Now, given a test example expressing some fact,
the goal of fact tracing (as defined by the \ftracetrex benchmark)
is to correctly identify the corresponding
proponents from the training set.

More precisely, \citet{akyurek2022towards} propose the following evaluation methodology,
which we follow exactly
(with the exception that, due to computational constraints,
we use a smaller 300M-parameter \texttt{mt5-small} model
instead of the 580M-parameter \texttt{mt5-base}).
We first finetune the pretrained language model \cite{raffel2020exploring}
on the training set of \ftracetrex.
Then, we iterate through the \ftracetrex test set and find the examples
on which the pre-trained model is incorrect
and the finetuned model is correct,\footnote{
    To decide whether a model is ``correct'' on a given test example,
    we use MT5 as a conditional generation model. That is, we
    feed in a masked version of the query, e.g.,
    ``\textunderscore\textunderscore\ is the capital of France,''
    and mark the model as ``correct'' if the conditional generation
    matches the masked word.
} which \citet{akyurek2022towards} refer to as the ``novel facts'' learned by the model after finetuning.
For each novel fact identified,
we collect a set of candidate training examples,
comprising all proponents as well as
300 ``distractors'' from the training set.
\citet{akyurek2022towards} propose to evaluate different attribution methods
based on how well they identify the ground-truth proponents among each candidate
set.

Concretely, given an attribution method $\tau(\cdot)$,
we compute attribution scores $\tau(z)$ for each of the
novel facts in the test set.
For each novel fact, we sort the corresponding candidate examples by their
score $\tau(z)_i$. Finally, we compute the mean reciprocal rank (MRR), a standard
information retrieval metric, of ground-truth proponents
across the set of novel facts, defined as
\[
    \text{MRR} = \sum_{z \in \parbox{1cm}{\tiny novel \\ facts}}\frac{1}{\min\limits_{i\, \in\, \text{proponents}(z)} \text{rank}(\tau(z), i)}.
\]


\subsection{Fine-tuning details}
\label{app:fact_tracing:loss}
We finetune the pre-trained language model using the masked language modeling
objective \citep{devlin2019bert}.
In particular, for each training example $z_i \in [K]^L$
(where $K$ is the vocabulary size and $L$ is the maximum passage length),
we mask out a subject or object within the passage.
(E.g., a training example ``Paris is the capital of France'' might become an
input-label pair [``\textunderscore\textunderscore\ is the capital of France'',
``Paris'']).
We then treat the language modeling problem as multiple separate
$K$-way classification tasks.
Each task corresponds to predicting a single token of the masked-out text,
given (as input) the entire passage minus the token being predicted.
The loss function is the average cross-entropy loss on this sequence of
classification tasks.

\subsection{Computing \trak for masked language modeling}
\label{app:fact_tracing:trak}
The model output function we use, more precisely, is given by:
\[
    \modeleval{z}{\theta} = \!\!\!\!\!\!\sum\limits_{\ \ \ j\ \in\ \parbox{0.7cm}{\centering\tiny masked \\ tokens}}
    \log \left(\frac{p(z^{j}|z^{-j};\theta)}{1-p(z^{j}|z^{-j};\theta)}\right).
\]
In particular, to compute this model output function,
we compute the model output function \eqref{eq:modelout_mc}
for each one of the $V$-way classification problems separately, then define
our model output function as the sum of these computed outputs.

\subsection{Counterfactual experiment setup}
\label{app:fact_tracing:cfx}
To understand the possible roots of \trak's underperformance
relative to BM25 on \ftracetrex,
we carry out a counterfactual analysis.
Specifically, for a subset of the \ftracetrex test set, we
create three corresponding {\em counterfactual training sets}.
Each training set corresponds to \underline{removing} one of three collections of examples
from the \ftracetrex training set:
\begin{enumerate}
    \item[(a)]
    the union (across all 50 selected novel facts) of the 500 most important
    training examples for each novel fact, as identified by \trak
    (this corresponds to removing $17,914$ total training examples, leaving $1,542,539$ remaining);
    \item[(b)]
    the union of the 500 most important
    training examples for each novel fact, as identified by BM25
    ($18,146$ total examples removed, and $1,542,307$ remaining);
    \item[(c)]
    the union of the proponents---as defined by \ftracetrex---for
    each novel fact ($10,780$ examples removed, and $1,549,673$ remaining)
\end{enumerate}
Then, starting from a pre-trained \texttt{mt5-small} model (the same model that we
finetuned in (B) above to identify novel facts),
we finetune several models on each counterfactual training set, and compute
their average accuracy on the selected subset of 50 novel facts.
Note that, by construction, we know that on this subset
(i) the pre-trained model has an
accuracy of 0\%; and (ii) finetuning on the
entire \ftracetrex training set (i.e., with no examples removed)
yields models with 100\% accuracy.\footnote{
    In particular, recall that in order for a test example to be categorized as a
    ``novel fact,'' it must be both (a) incorrectly handled by the pre-trained
    \texttt{mt5-small} model and (b) correctly handled by a finetuned model.
}
As for the counterfactual training sets, one should note that:
\begin{itemize}
    \item Counterfactual training set (c) is missing all of the
    proponents for our subset of 50 novel facts---we would thus expect the
    corresponding finetuned model to have very low accuracy.
    In particular, there is ostensibly no direct evidence for {\em any} of the novel facts
    of interest anywhere in this counterfactual training set.
    \item Being constructed with BM25, counterfactual training set (b)
    has high lexical overlap with the novel facts of interest.
    Since BM25 performs well on the \ftracetrex benchmark,
    we would also expect the resulting models to have low accuracy.
\end{itemize}
In \cref{fig:nlp_counterfactual}, we report the resulting models' average performance on
the set of 50 selected novel facts.
What we find is that, counter to the above intuition,
{\em only the} \trak-{\em based counterfactual training set is able to
significantly change model behavior}.
That is, the counterfactual effect of removing the most important images
as identified by \trak on the selected subset of novel facts
is significantly higher than both
(a) that of removing the most important images according to BM25; and
(b) that of removing the {\em ground-truth proponents} of the facts as indicated
by the \ftracetrex benchmark.



