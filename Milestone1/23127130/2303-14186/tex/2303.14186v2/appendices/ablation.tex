We perform a number of ablation studies to understand how different
components of \trak affect its performance.
Specifically, we study the following:
\begin{itemize}
    \item The dimension of the random projection, $k$. %
    \cref{sec:estimator_algo}).
    \item The number of models ensembled, $M$. %
    \cref{sec:estimator_algo}).
    \item Proxies for ensembles to further improve \trak's computational
    efficiency.
    \item The role of different terms in the influence estimation formula (\cref{eq:trak}).
    \item Alternative choice of the kernel (using last layer representations).
    \item Alternative methods of ensembling over models.
\end{itemize}

\noindent As in \Cref{sec:eval}, we evaluate the linear datamodeling score (LDS) on models trained on the \cifartwo, \cifarten,
and \qnli datasets. Note that the LDS is in some cases lower than the counterparts in \Cref{fig:headline_full} as we use a smaller projected dimension ($k$) and do not use soft-thresholding in these experiments.

\subsection{Dimension of the random projection}

Recall that when we compute \trak we reduce the dimensionality of the gradient
features using random projections (Step 2 of \cref{sec:estimator_algo}).
Intuitively, as the resulting dimension $k$ increases, the corresponding
projection better preserves inner products, but is also more expensive to
compute. We now study how the choice of the projection dimension $k$ affects
\trak's attribution performance.


\Cref{fig:jl_dim} (Left) shows that as we increase the dimension, the
LDS initially increases as expected; random projections to a higher dimension
preserve the inner product more accurately, providing a better approximation of
the gradient features. However, beyond a certain point, increasing projection
dimension {\em decreases} the LDS. We hypothesize that using random projections
to a lower dimension has a regularizing effect that competes with the increase
in approximation error.\footnote{Indeed, we can view our approach of first
projecting features to a lower dimension and then performing linear regression
in the compressed feature space, as an instance of {\em compressed linear
regression} \cite{maillard2009compressed} and also related to principal
components regression \cite{thanei2017random}. These approaches are known to
have a regularizing effect, so \trak may also benefit from that effect.} Finally,
the dimension at which LDS peaks {\em increases} as we increase the number of
models $M$ used to compute \trak.

\begin{figure}[!htbp]
    \centering
    \input{figures/jl_plot.tex}
\caption{
\textbf{Left:}
  {\em The impact of the dimension of random projection on \trak's
  performance on \cifartwo.} Each line corresponds to a different value of $M \in \{10,20,...,100\}$ (the number of models \trak is averaged over); darker lines correspond to higher $M$. As we increase the projected dimension, the LDS initially increases. However, beyond a certain dimension, the LDS
  begins to decrease. The ``optimal'' dimension (i.e., the peak in the above
  graph) increases with higher $M$.
\textbf{Right:}
  {\em The impact of ensembling more models on \trak's performance on \cifartwo.} The
  performance of \trak  as a function of the number of models used in the
  ensembling step. \trak scores are computed with
  random projections of dimension $k=4000$.
}
\label{fig:jl_dim}
\end{figure}

\subsection{Number of models used in the ensemble}
\label{app:ablation_num_models}
An important component of computing \trak is ensembling over multiple independently trained models (Step 4 in \cref{sec:estimator_algo}).  In our experiments, we average
\trak's attribution scores over ensembles of size ranging from $1$ to $100$.
Here, we quantify the importance of this procedure on \trak's performance.



\Cref{fig:jl_dim} (Right) shows that
\trak enjoys a significantly
better data attribution performance with more models.
That said,
even without ensembling (i.e., using a single model), \trak still performs
better (e.g., LDS of 0.096 on \cifartwo) than all prior gradient-based methods
that we evaluate.

\subsection{Proxies for model ensembles in compute-constrained settings}
\label{app:proxies}
In \cref{app:ablation_num_models} we saw that ensembling leads to significantly
higher efficacy (in terms of LDS).
In many settings, however, it is computationally expensive to
train several independent models to make an ensemble.
Hence, we study whether there is a cheaper alternative to
training multiple independent models that does not significantly sacrifice
efficacy.
To this end, we explore two avenues of approximating the full ensembling step while
dramatically reducing the time required for model training.
In particular, we investigate:
\begin{enumerate}
    \item using multiple checkpoints from each training trajectory;
    \item using checkpoints from early training, long before the model has
    converged.
\end{enumerate}

\paragraph{Multiple checkpoints from each training trajectory.}
We compute \trak scores
using a {\em fixed} number of checkpoints, but while varying the number of
independently-trained models.
For example, for 100 checkpoints, we can use the
final checkpoints from $100$ independently-trained models, the last two
checkpoints from $50$ independently-trained models, etc. We observe (see
\cref{tab:num_independent_runs_app}) that \trak achieves comparable LDS when we use
last $T$ checkpoints along the trajectory of the same models as a proxy for
independently-trained models in the ensembling step.


\paragraph{Using checkpoints from early training.}
We explore whether each of the models in the ensemble has to be fully trained to
convergence. In particular, we study the effect of using checkpoints from early
epochs on the LDS. While \trak benefits from using later-epoch gradient features, it
maintains its efficacy even when we use gradient features from training runs
long before reaching convergence (see \Cref{tab:ablation_epoch_used_app}). Leveraing this can further improve the computational efficiency of \trak.


\input{figures/efficiency_table}



\subsection{Role of different terms.}
The \trak estimator (\Cref{eq:trak}) has a number of different components. We label each component (of the single model estimator) as follows:
\[
    \tau(z)_i = \frac{\phi(z)^\top \overbrace{(\Phi^\top R \Phi)^{-1}}^\text{reweighting} \phi(z_i) \cdot \overbrace{\frac{1}{1+e^{f(z_i)}}}^\text{loss gradient}}{1 - \underbrace{h_i}_\text{leverage score}}
\]

\noindent We ablate each of the terms above and re-evaluate the resulting variant of \trak on \cifartwo.
Our results in \Cref{tab:terms_ablate} indicate the following:
\begin{itemize}
\item {\bf Reweighting:} Experiment 6 shows that this matrix is a critical part of \trak's performance. Conceptually, this matrix distinguishes our estimator from prior gradient based similarity metrics such as \tracin.
\item {\bf Diagonal term $R$:} The full reweighting matrix includes a diagonal term $R$. Although it is theoretically motivated by \Cref{lem:formal}, including this term results in lower LDS, so we do not include it (Experiments 2,4).
\item {\bf Loss gradient}: This term corresponds to the $\mathbf{Q}$ matrix (\Cref{eq:q_mat}) and encodes the probability of the incorrect class, $1-p_i$; the name is based on the derivation in \Cref{app:theory_newton}, where this term corresponds to scalar associated with the gradient of the loss. Intuitively, this term helps reweight training examples based on on models' confidence on them.
 Experiment 5 shows that this term improves the performance substantially.
\item {\bf Leverage score:} This term does not impact the LDS meaningfully, so we do not include it (Experiments 1,2).
\item {\bf Averaging ``out'' vs ``in'':} Averaging the estimator and the loss gradient term separately, then re-scaling by the average loss gradient results in higher LDS (Experiment 3).
\end{itemize}

\begin{table}[!bht]
  \begin{tabular}{llllllr}
      \toprule
      Experiment & Reweighting & Loss &   Diagonal $R$ & Leverage & Averaging &  Correlation \\
      \midrule
      0 &         \checkmark &  \checkmark &   \xmark    &      \xmark    &     out &        0.499 \\
      1 &         \checkmark &  \checkmark &   \xmark    &      \checkmark &     out &        0.499 \\
      2 &         \checkmark &  \checkmark &  \checkmark &      \checkmark &     out &        0.430 \\
      3 &         \checkmark &  \checkmark &   \xmark    &       \xmark    &      in &        0.416 \\
      4 &         \checkmark &  \checkmark &  \checkmark &       \xmark    &     out &        0.403 \\
      5 &         \checkmark &   \xmark    &   \xmark    &       \xmark    &     out &        0.391 \\
      6 &          \xmark    &  \checkmark &   \xmark    &       \xmark    &     out &        0.056 \\
      \bottomrule
      \end{tabular}
  \centering
  \caption{{\em Ablating the contribution of each term in the \trak estimator.} For these experiments, we use random projections of dimenseion $k=2000$.}
  \label{tab:terms_ablate}
\end{table}

\subsection{Choice of the kernel}
To understand how the choice of the kernel impacts the performance of \trak,
we also compute a version of \trak using feature representations of the penultimate layer in place of the projected gradients.
This choice is equivalent to restricting the gradient features to those of the last linear layer.
As \Cref{tab:kernel_choice} shows, this method significantly improves on all existing baselines based on gradient approximations,\footnote{Note that as with the eNTK, the use of multiple models here is crucial: only using a single model gives a correlation of 0.006.} but still underperforms significantly relative to \trak. This gap suggests that the eNTK is capturing additional  information that is not captured by penultimate layer representations.
Moreover, the larger gap on \cifarten compared to \cifartwo and \qnli (both of which are binary classificaiton tasks) hints that the gap will only widen on more complex tasks.

We note that \trak applied only to the last layer is almost equivalent to the influence function approximation. Indeed, they perform similarly (e.g., the influence function approximation also achieves a LDS of 0.19 on \qnli).

\begin{table}[h]
    \centering
    \begin{tabular}{lrr}
        \toprule
              Dataset &  Kernel representation &  Linear Datamodeling Score (LDS) \\
        \midrule
          \cifartwo &  eNTK  &     {\bf 0.516} \\
          \cifartwo &  penultimate layer   &     0.198 \\
        \midrule
          \cifarten & eNTK &     {\bf 0.413} \\
          \cifarten & penultimate layer   &     0.120 \\
        \midrule
          \qnli & eNTK & {\bf 0.589} \\
          \qnli & penultimate layer & 0.195  \\
        \bottomrule
    \end{tabular}
    \caption{{\em Choice of the kernel in \trak}. We compare \trak computed using the eNTK (i.e., using features derived from full gradients) with \trak computed using the kernel derived from last layer feature representations. The attribution scores are ensembled over $M=100$ models.}
    \label{tab:kernel_choice}
    \end{table}


\subsection{Ensembling vs. Averaging the eNTK}
There are different ways to ensemble a kernel method given multiple kernels $\{K_i\}_i$: (i) we can average the Gram matrices corresponding to each kernel first and then predict using the averaged kernel (i.e., work with $\overline{K} = \frac{1}{n} \sum K_i$),  (ii) we can average their induced features (with respect to some fixed basis of functions) and use the corresponding kernel, or (iii) we can average the predictions derived from each kernel \citep{atanasov2023onset}.
\trak's algorithm follows the third approach (Step 4).

Here we ensemble using the first approach instead (i.e., using the averaged eNTK). We do this by first averaging the Gram matrices corresponding to each models' eNTK, using the Cholesky decomposition to extract features from the averaged Gram matrix ($G=LL^\top$), then using resulting features $L$ into the same influence formula (Step 3).
We find that computing \trak with this average eNTK gives a significantly underperforming estimator  (LDS of 0.120 on \cifartwo) than averaging {\em after} computing the estimator from each eNTK (LDS of 0.499).
This gap suggests that the underlying model is better approximated as an ensemble of kernel predictors rather than a predictor based on a single kernel.






\subsection{Summary}
To summarize the results of our ablation, \trak performs best when averaging over a
sufficient number of models (though computationally cheaper alternatives also work); gradients computed at later epochs; and random
projections to sufficiently high---but not too high---dimension. Using the reweighting matrix in \cref{eq:trak}, as well as deriving
the features from the full model gradient are also both critical to \trak's
predictive performance.