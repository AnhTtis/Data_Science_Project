\subsection{Further applications of \trak}
Prior works have demonstrated the potential of leveraging data attribution for a
variety of downstream applications, ranging from explaining predictions
\cite{koh2017understanding,kong2022resolving}, cleaning datasets
\cite{jia2019towards}, removing poisoned examples \cite{lin2022measuring} to quantifying uncertainty \cite{alaa2020discriminative}.
Given the effectiveness of \trak, we expect that using it in place of existing attribution
methods will improve the performance in many of these downstream applications.
Moreover, given its computational efficiency, \trak can expand the settings in
which these prior data attribution methods are feasible. Indeed, we already saw some examples in \Cref{subsec:datamodel_apps}. We highlight a few promising
directions in particular:

\paragraph{Fact tracing and attribution for generative models.}
Fact tracing, which we studied in \Cref{subsec:fact_trace}, is a problem of increasing relevancy as large language models are widely deployed. Leveraging \trak for fact tracing, or attribution more broadly, may
help understand the capabilities or improve the trustworthiness of recent models such as GPT-3
\cite{brown2020language} and ChatGPT,\footnote{\url{https://chat.openai.com/}}
by tracing their outputs back to sources in a way that is faithful to the actual model.
More broadly, attribution for generative models (e.g., stable diffusion \cite{ho2020denoising,rombach2022high}) is an interesting direction for future work.

\paragraph{Optimizing datasets.}
\trak scores allow one to quantify the impact of individual training examples on model predictions on a given target example.
By aggregating this information, we can optimize what data we train the models on,
for instance, to choose {\em coresets} or to select new data for {\em active learning}.
Given the trend of training models on ever increasing size of datasets \cite{hoffmann2022training}, filtering data based on their \trak scores can also help models achieve with the benefits of scale without the computational cost.

Another advantage of \trak is that it is fully differentiable in the input (note that the associated gradients are different from the gradients with respect to model parameters that we use when computing \trak).
One potential direction is to leverage this differentiability for {\em dataset distillation}. Given the effectiveness of the NTK for this problem \cite{nguyen2021dataset}, there is potential in leveraging \trak---which uses the eNTK---in this setting.

\subsection{Understanding and improving the \trak estimator}

\paragraph{Empirical NTK.}
\trak leverages the empirical NTK to approximate the original model. Better understanding of when this approximation is accurate may give insights into improving \trak's efficacy.
For example, incorporating higher order approximations \cite{huang2020dynamics,bai2020beyond} beyond the linear approximation used in \trak is a possible direction.

\paragraph{Training dynamics and optimization.}
Prior works \citep{leclerc2020two,lewkowycz2020large} suggest that neural network training can exhibit two stages or regimes: in the first stage, the features learned by the network evolve rapidly; in the second stage, the features remain approximately invariant and the overall optimization trajectory is more akin a convex setting. We can view our use of the final eNTK as modeling this second stage. %
Understanding the extent to which the first stage (which \trak does not model) accounts for the remaining gap between true model outputs and \trak's predictions may help us understand the limits of our method as well as improve its efficacy.
Another direction is to study whether properly accounting for other optimization components used during training, such as mini-batches, momentum, or weight decay, can improve our estimator.

\paragraph{Ensembles.} As we saw in \Cref{app:ablation_num_models}, computing \trak
over an ensemble of models significantly improves its efficacy.  In particular, our results suggest that the eNTK's derived from
independently trained models capture non-overlapping information. %
Better understanding of the role of ensembling here may
us better understand the mechanisms underlying ensembles in other contexts
and can also provide practical insights for improving \trak's efficiency. For
instance, understanding when model checkpoints from a single trajectory can
approximate the full ensemble (\Cref{app:proxies}) can be valuable in settings where it is expensive to
even finetune several models.
