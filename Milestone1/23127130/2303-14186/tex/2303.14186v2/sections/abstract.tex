The goal of {\em data attribution} is to trace model predictions back to training data.
Despite a long line of work towards this goal,
existing approaches to data attribution
tend to force users to choose between
computational tractability and efficacy.
That is, computationally tractable methods can struggle with accurately attributing
model predictions in non-convex settings (e.g., in the context of deep neural networks),
while methods that are effective in such regimes require training thousands of
models, which makes them impractical for large models or datasets.

In this work, we introduce \trak (\spelledout),
a data attribution method that is both effective {\em and}
computationally tractable for large-scale, differentiable models.
In particular, by leveraging only a handful of trained models,
\trak can match the performance of attribution methods that require training thousands of
models.
We demonstrate the utility of \trak across various modalities and scales:  image
classifiers trained on ImageNet, vision-language models (\clip), and
language models (\bert and \mtfive).
We provide code for using \trak
(and reproducing our work) at
\url{https://github.com/MadryLab/trak}.






