We now evaluate \trak (see \cref{eq:trak} and \cref{alg:estimator_pseudo} in
\cref{sec:pseudocode}) in a variety of vision and natural language settings. To
this end, we compare \trak with existing data attribution methods and show that
it achieves significantly better tradeoffs between efficacy and computational
efficiency.

\subsection{Experimental setup}
We evaluate and study \trak with the following experimental setup.

\vspace*{-1em}
\paragraph{Datasets, models, and baselines.}
We use ResNet-9 classifiers trained on
the \cifar dataset (\cifarten, and a two-class subset called \cifartwo);
ResNet-18 \citep{he2015deep} classifiers trained on the 1000-class
ImageNet \citep{russakovsky2015imagenet} dataset,
and pre-trained \bert \citep{devlin2019bert} models
finetuned on the \qnli (Question-answering Natural
Language Inference) classification task from the \glue benchmark \citep{wang2018glue}.
We provide further details on these choices of dataset and task in \Cref{app:datasets_models}.

To put \trak's performance into context, we also evaluate a variety of existing attribution methods,
including influence functions \citep{koh2017understanding};
a variant based on the Arnoldi iteration \citep{schioppa2022scaling};
\tracin \citep{pruthi2020estimating};
gradient aggregated similarity (\gas) \citep{hammoudeh2022training};
representation similarity \citep{hanawa2021evaluation};
empirical influences \citep{feldman2020neural};
and datamodels \citep{ilyas2022datamodels}.
(See \Cref{app:baselines} for more details.)


\paragraph{Evaluation with linear datamodeling scores.}
For each method and each dataset we consider,
we compute its linear datamodeling score (LDS) as described in \Cref{def:datamodeling}.
Specifically, let $\tau$ be a given data attribution method
(as framed in \cref{def:attribution}),
and let $g_\tau(z, S'; S)$ be its corresponding attribution-derived prediction function
(see \cref{def:attr_output}).
Then, to evaluate $\tau$:

\begin{enumerate}
\item We sample $100$ different random subsets $\{S_j \subset S: j \in [100]\}$
of the training set $S$,
and train five models on each one of these subsets.
Each subset $S_j$ is sampled to be 50\% of the size of $S$,
but we also consider
other subsampling ratios in \Cref{app:more_results}.

\item For each example of interest $z$ (i.e., for each example in the test set
of the dataset we are studying),
we approximate the expectation of the model output
$\mathbb{E}[\modeleval{z}{\thetastar_i(S_j)}]$ for each training subset $S_j$
(where the expectation is taken over the learning algorithm's randomness)
by averaging across the corresponding five models $\{\thetastar_i(S_j)\}_{i=1}^5$.

\item We then compute the linear datamodeling score for each example of interest
$z$ as the Spearman rank correlation
\citep{spearman1904proof} between the averaged model outputs computed in the previous step
and the attribution-derived predictions $g_\tau(z, S_j; S)$ of model outputs.
That is, we compute:
    \[
        \text{Spearman-}\rho\bigg(
            \underbrace{\left\{\frac{1}{5} \sum_{i=1}^5 \modeleval{z}{\thetastar_i(S_j)}: j \in [100] \right\}}_\text{averaged model outputs}, \underbrace{\{g_\tau(z, S_j; S): j \in [100]\}}_{\substack{\text{attributed-derived predictions}\\\text{of model outputs}}}
        \bigg)
    \]


\item Finally, we average the LDS (\cref{def:datamodeling}) across 2,000 examples of interest,
sampled uniformly at random from the validation set,
and report this score along with the $95\%$ bootstrap confidence intervals corresponding to the random
re-sampling from the subsets $S_j$.
\end{enumerate}

\paragraph{Computational cost.}
We quantify the computational cost of each attribution method using two metrics.
The first one is the {\em total wall-time} of computing attribution scores on a single A100 GPU.
This metric is intuitive and useful, but depends on implementation details and hardware.
We thus also study a second metric, namely,
the {\em total number of trained models used}.
This metric is hardware and implementation-agnostic; it is motivated by an
observation that for large models,
the time it takes to compute attribution scores will be dominated by
the time it takes to {train} the models needed for
attribution.\footnote{For many data attribution methods, such as influence function-based
methods or \trak, there is an extra step of computing per-example gradients
through the model of interest.
However, this step is generally fully parallelizable,
and usually bounded by the time it takes to train a model from scratch.}
We find that for both metrics, our results lead to similar conclusions.




\subsection{Results}
Across all models and datasets that we consider, \trak attains a significantly
better tradeoff between efficacy (as measured by the LDS) and computational
efficiency than all the other attribution methods that we examine (see
\Cref{fig:headline,fig:headline_full} and \Cref{tab:all_best}). Indeed, \trak attains
efficacy comparable to datamodels (which achieves the best performance among existing methods when unconstrained)
with a computational footprint that is
(on average) over 100x smaller.

\begin{figure}[!htb]
    \centering
    \input{figures/main_figure_full.tex}
\caption{{\em \trak achieves state-of-the-art tradeoffs between
    attribution efficacy and efficiency.} We use \trak to attribute ResNet-9
    classifiers trained on \cifartwo and \cifarten; ResNet-18 classifiers trained
    on ImageNet; and BERT-base models finetuned on \qnli.  The $x$-axis indicates
    the computational cost measured as the number of trained models that a given
    method uses to compute attribution scores. The $y$-axis indicates the
    method's efficacy as measured by the linear datamodeling score (LDS). Error
    bars indicate 95\% bootstrap confidence intervals.}
\label{fig:headline_full}
\end{figure}

\paragraph{Inspecting \trak-identified examples.}
In \Cref{fig:imagenet_nns} we also display, for two randomly chosen test examples
from \qnli, \cifarten, and ImageNet datasets, the training examples corresponding
to the most positive and negative \trak scores.








\begin{figure}[!hb]
    \centering
    \input{figures/qnli_rand_table.tex}
    \vspace{0.5em}
    \includegraphics[width=\linewidth,trim={0 2.2cm 0 0},clip]{figures/cifar10_nns.pdf}
    \vspace{0.5em}
    \includegraphics[width=\linewidth,trim={0 2.4cm 0 0},clip]{figures/imagenet_nns.pdf}
\caption{
    We present two randomly selected test examples and their corresponding
    most helpful (highest-scoring) and most detracting (lowest-scoring)
    training examples as identified by \trak,
    for \bertbase classifiers trained on \qnli (top);
    ResNet-9 classifiers trained on \cifarten (middle);
    and ResNet-18 classifiers trained on ImageNet (bottom).
    We observe that \trak-identified training examples are
    semantically similar to the corresponding target examples,
    and that the vast majority of helpful (detracting) examples are of
    the same (different) class as the target.
    We present more such examples in \Cref{app:more_examples}
    and at \url{trak.csail.mit.edu}.
}
\label{fig:imagenet_nns}
\end{figure}





\paragraph{Comparing \trak and datamodel scores.}
Recall from \cref{subsec:oracle} that one can view datamodels \citep{ilyas2022datamodels}
as an ``oracle'' of sorts for the linear datamodeling score (LDS) objective.
It turns out, as we show in \cref{tab:corr_between}, that \trak scores
correlate with datamodel scores, while scores of other attribution methods do not.
(We define correlation here as the Spearman rank correlation between the
vectors $\tau_\trak(z)$ and $\tau_\text{DM}(z)$, averaged over multiple examples
of interest $z$.)



\begin{table}[h]
    \centering
    \begin{tabular}{ccccccc}
        \toprule
        Method &   $\trak_{100}$ & $\trak_{20}$ & \tracin \citep{pruthi2020estimating} &  IF \citep{koh2017understanding}  &  GAS \citep{hammoudeh2022identifying} &  random \\
        \midrule
        $\rho(\tau, \tau_{\text{DM}})$  &  0.26 & 0.19 & 0.00 &  0.03 &  0.03 &  -0.03 \\
        \bottomrule
    \end{tabular}
    \caption{{\em Correlation with datamodel scores.} We measure the
    correlation between the attribution scores computed by different methods
    $\tau$ and those given by datamodels $\tau_{\text{DM}}$
    \citep{ilyas2022datamodels} on the \cifarten dataset. Specifically, for each
    test example of interest $z$, we compute the Spearman rank correlation
    $(\rho)$ between $\tau(z)_i$ and $\tau_{\text{DM}}(z)_i$ over training
    examples $i$ that have  nonzero datamodel weight $\tau_{\text{DM}}(z)_i$ and
    then average the resulting correlation over $1000$ randomly chosen examples of interest.
    $\trak_{N}$ indicates a version of $\trak$ that uses $N$ trained models in
    its estimator.}
    \label{tab:corr_between}
\end{table}

\paragraph{Understanding the roots of \trak's performance.}
In \cref{app:more_ablation}, we study the roots of \trak's performance through
an extensive ablation study. We vary, for example, how we linearize the model of
interest (Step 1 in \cref{sec:estimator_algo}), the dimension $k$ of the random
projection we use (Step 2 in \cref{sec:estimator_algo}), how we apply the Newton
step attribution from \cref{lem:formal} (Step 3 in \cref{sec:estimator_algo}),
and how we aggregate information from independently trained models (Step 4 in
\cref{sec:estimator_algo}).

As a byproduct of this investigation, we find two ways of computing \trak at
even lower cost: (a) leveraging models that have not been trained to
convergence, and (b) taking advantage of multiple checkpoints from the same
model, rather than multiple models from independent training runs.  We find (see
\cref{tab:ablation_epoch_used,tab:num_independent_runs},
explained further and reproduced in \cref{app:more_ablation})
that both of these
optimizations can dramatically reduce \trak's computational cost without
significantly degrading its performance.

\begin{table}[htbp]
    \centering
    \begin{minipage}{.46\textwidth}
      \centering
      \begin{tabular}{cc}
      \toprule
      \# training epochs & LDS \\
      \midrule
      1 & 0.100 \\
      5 & 0.204 \\
      10 & 0.265 \\
      15 & 0.293 \\
      25 & 0.308 \\
      \bottomrule
      \end{tabular}
      \caption{The performance of \trak on \cifarten as a function of the epoch at which we
      terminate model training. In all cases, \trak scores are computed with projection dimension $k = 1000$ and $M=100$ independently trained models.}
      \label{tab:ablation_epoch_used}
    \end{minipage}\hfill
    \begin{minipage}{.50\textwidth}
      \centering
      \begin{tabular}{cc}
      \toprule
      \# independent models & LDS \\
      \midrule
      5 & 0.329 \\
      6 & 0.340 \\
      10 & 0.350 \\
      100 & 0.355 \\
      \bottomrule
      \end{tabular}
      \caption{{\trak maintains its efficacy when we use multiple checkpoints from different epochs of
      the same training run instead of checkpoints from
      independently-trained models (\cifarten).} In all cases, $M=100$ checkpoints and projection dimension $k = 4000$ are used to compute \trak scores. }
      \label{tab:num_independent_runs}
    \end{minipage}
\end{table}


