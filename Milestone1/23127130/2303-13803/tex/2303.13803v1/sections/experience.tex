\section{Lessons and future direction}
\label{sec:experience}


\parabf{Safety protection.}
How to guarantee the safety of shared workloads is one of the most important problems of deploying GPU sharing.
Thus, most GPU sharing solutions~\cite{cgpu,gu2018gaiagpu} in production do not consider MPS due to its poor isolation ability.
In contrast, to our best knowledge, we are the first to thoroughly analyze all unsafe cases encountered in our production cluster and propose corresponding solutions for these cases.
Almost all unsafe cases in our deployment can be handled by the graceful exit mechanism of \sysname.
Besides, we have worked with NVIDIA to improve MPS.
Some bugs and features we reported have been verified and fixed by NVIDIA.
For example, we observed that sharing workloads compiled by different GCC versions with MPS can cause the MPS server hangs, and this problem has been verified by NVIDIA and fixed in NVIDIA GPU Driver 470.

However, things become complicated when considering malicious behaviors, e.g., intriguing sticky CUDA error by dividing zero to influence the shared workload.
To avoid malicious behaviors, \sysname only accepts trustworthy offline workloads and we employ a fault detector with manually-summarized rules to monitor collected device metrics and alert when abnormal situations are found.
\revise{
For now, \sysname is only used in our internal clusters and for internal users. 
These protection approaches seem safe enough according to our deployment experience.
Yet if considering external users in a cloud setting,
we need more general and automated approaches for safety protection and malicious behaviour detection.
One opportunity is to leverage DL to discover malicious behaviours automatically~\cite{saufi2019challenges}.
Besides, enabling the scheduler to identify fail-prone workloads and avoid sharing them with other workloads is another possible approach.
}

\parabf{Slowdown of online workloads.}
In this paper, we get the latency of online workloads increases less than $20\%$, i.e., $10ms$.
The degradation is affordable and acceptable in our internal cluster, because most latency demands are more than 100ms for production online workloads.
Note that the degradation threshold is a tradeoff between the online service quality and resource utilization, and it can be changed in \sysname by two mechanisms.
First, the parameters of GPU load~\ref{equ:gpu_load}$\&$\ref{equ:clock_factor} in \bytecuda affect how the offline workload is executed and then how the online workload is influenced.
Second, we can adjust the SM percentage assigned to offline workloads to change the slowdown of online workloads.
How to select a proper degradation threshold for each cluster or even each online workload is left as an open problem.

\parabf{The number of shared workloads.}
In \sysname, we share at most one offline workload with each online workload because one offline workload is usually enough to fill SMs up.
Sharing multiple offline workloads with multiple online workloads may bring more benefits, especially for light-weighted offline workloads.
However, there are four challenges to sharing multiple workloads.
First, we need to guarantee the performance of all online workloads.
Second, we need to limit the total SM percentage used by multiple offline workloads which cannot be simply limited by MPS parameters.
Third, \bytecuda needs to monitor kernel launches of all offline workloads and decide which kernel to delay or launch according to their priority.
Fourth, the scheduling algorithm to choose sharing pairs with more than three workloads becomes an NP-hard problem~\cite{zhao2022multi}.
\revise{
How to solve these challenges to utilize GPU better is an interesting future direction.
}