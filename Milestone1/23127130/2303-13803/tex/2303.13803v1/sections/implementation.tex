\section{Implementation}
\label{sec:impl}

At \company, we have deployed \sysname in our internal clusters to serve daily DL workloads.
The internal clusters consist of heterogeneous GPUs, including NVIDIA T4 GPU and NVIDIA A10 GPU.
Integrated with Kubernetes~\cite{k8s}, \sysname manages thousands of GPUs in each cluster and more than 20,000 GPUs in all.

\parabf{Service manager.}
For online workloads, we use the existing service manager at \company which deploys containers, discovers service, and autoscales horizontal pods.

\parabf{Global manager.}
We modify the Kubernetes scheduler to schedule offline workloads.
The workload profiler takes several dedicated GPUs, whose number is negligible to the total number of GPUs.
When a new offline workload comes, the workload profiler performs a few dry runs of the workload and utilizes the NVIDIA Data Center GPU Manager (DCGM) tools~\cite{dcgm} and NVIDIA Management Library (NVML)~\cite{nvml} libraries to collect GPU metrics.
We collect about 2,000 data for each GPU type to train the speed predictor.
The MLPs of the speed predictor have four layers with hidden size $64\times 64$.
The MLPs are trained with momentum SGD optimizer~\cite{ruder2016overview} in PyTorch v1.8.0~\cite{paszke2019pytorch} until they converge.
\sysname invokes the scheduler periodically to schedule all offline workloads.
When moving workloads, we record checkpoints of offline workloads and restart the workloads after transmitting the models and checkpoints.
As the datasets are usually colossal, we store the datasets in a remote file system and fetch data during the execution.
We implement the scheduler as a third-party plugin to the Kubernetes scheduler.


\parabf{Local executor.}
Each local executor executes online workloads according to the service manager and offline workloads according to the global manager.
DL workloads are executed in Docker containers with our customized components.
We add Best-Effort GPU DevicePlugin in Kubernetes and relevant control paths with Kubelet and \sysprobe for offline workloads.
To control SM percentage, we leverage the environment variable $CUDA\_MPS\_ACTIVE\_THREAD\_PERCENTAGE$ provided by MPS.
The GPU monitor collects resource metrics through DCGM~\cite{dcgm} and NVML~\cite{nvml} for NVIDIA GPU.
The \sysprobe updates the state machine with the collected resource metrics and empirically-set thresholds.
When the state is unhealthy, the \sysprobe will ask the NodeManager in Kubernetes to evict offline workloads.
\bytecuda intercepts nearly 800 CUDA driver APIs for GPU memory allocation and kernel launch.
The GPU memory quota of offline workloads is fixed to $40\%$ as Figure~\ref{fig:motiv_gpu_resource} reports that most online workloads use less than $60\%$ GPU memory.
We adopt the cpuset of Cgroup for CPU isolation.
For memory, \sysname will evict offline workloads if memory usage is higher than a threshold or the kernel swap daemon is busy for a long time.
The parameters to calculate GPU load in Equation~\ref{equ:gpu_load}$\&$\ref{equ:clock_factor} are empirically selected through trial-and-error.
