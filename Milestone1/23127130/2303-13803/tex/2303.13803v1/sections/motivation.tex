\section{Motivation}
\label{sec:motivation}

In this section, we begin with introducing DL workloads and critical terminologies.
Then we describe the observations from the production cluster for online workloads to motivate the design of \sysname.
We end by discussing opportunities to share the GPUs between different DL workloads.

\subsection{DL workloads}
DL workloads use the deep neural network (DNN) to perform inference or training.
DL workloads are usually classified into two categories, i.e., online workload and offline workload, according to the latency demand.
\emph{Online workload} refers to latency-critical inference, such as real-time recommendation~\cite{covington2016deep,gao2021learning} and machine translation~\cite{vaswani2017attention,gehring2017convolutional}. 
Online workloads have strict latency demands because longer end-to-end latency may hurt users' experience.
Additionally, the requests for online workloads are usually submitted periodically at different frequencies.
\emph{Offline workload} does not have strong latency demand, such as DL training,  batch inference, scientific computing~\cite{senior2020improved}, and automatic neural architecture search~\cite{liu2018progressive,tan2019mnasnet}.
These workloads usually take hours or even days to finish.
The offline workloads do not have hard time requirements and can usually highly utilize the computing units of GPU, making them suitable to fill the idle GPU resource.

\subsection{Production cluster for online workloads}
Production clusters exploit GPUs to accelerate DL workloads~\cite{crankshaw2017clipper,gujarati2020serving,han2022microsecond}.
GPUs are usually assigned to online workloads exclusively to guarantee the latency demand.
We study GPU resource utilization in production clusters from two aspects: memory and computing power.

\begin{figure}[t]
        \centerline{\includegraphics[width=\linewidth,trim=0 0 0 20,clip]{figures/Motiv_GPU_resource.pdf}}
        \vspace{-0.2in}
        \caption{GPU resource statistic in a production cluster for online workloads.}
        \vspace{-0.1in}
        \label{fig:motiv_gpu_resource}
\end{figure}
\parabf{Low GPU resource utilization.}
We collect one week's statistics of GPU computation utilization and memory usage in the inference cluster of \company, as shown in Figure~\ref{fig:motiv_gpu_resource}.
\revise{
The inference workloads include various popular DL models, such as CNN, GNN, LLM, and recommendation models.
}
As for the GPU computing utilization, we use two metrics: GPU utilization and SM activity~\cite{dcgm}.
GPU utilization and SM utilization represent how busy the GPU is in time and in space, respectively.
GPU memory usage is the ratio of used memory to memory capacity.
Figure~\ref{fig:motiv_gpu_resource} illustrates that both GPU utilization and SM utilization are lower than $60\%$ for more than $99\%$ GPUs.
In addition, GPU memory usage is less than $60\%$ for about $90\%$ GPUs.
These numbers show that GPUs are underutilized in both memory and computing power, indicating a great waste of valuable GPUs.

\parabf{Fluctuating and predictable GPU utilization.}
\begin{figure}[t]
        \centerline{\includegraphics[width=\linewidth,trim=0 0 0 20,clip]{figures/Motiv_host.pdf}}
        \vspace{-0.15in}
        \caption{Resource usage of one typical online workload. GPU util., SM act., and GPU mem. are short for GPU utilization, SM activity, and GPU memory usage, respectively.}
        \vspace{-0.1in}
        \label{fig:motiv_host}
\end{figure}
We take one typical online workload in the production cluster of \company as an example and show its GPU computing utilization and memory usage in Figure~\ref{fig:motiv_host}.
Both the GPU utilization and SM activity fluctuate greatly in one day, because the number of online requests varies from time to time.
For example, more users use entertainment applications in the evening and send more online requests to related services, while during the day, fewer requests are sent.
The GPU memory usage is stable because the DL framework, e.g., PyTorch~\cite{paszke2019pytorch}, caches the intermediate GPU memory for efficiency.
Besides, we observe that the curves of the GPU usage metrics are smooth in minutes and periodical in days.
Thus, we can predict the GPU usage metrics by the past values.


\begin{figure}[t]
        \centerline{\includegraphics[width=0.9\linewidth]{figures/Motiv_GPUsharing.pdf}}
        \vspace{-0.15in}
        \caption{An example of different GPU sharing approaches for NVIDIA GPUs.}
        \vspace{-0.1in}
        \label{fig:motiv_gpusharing}
\end{figure}

\subsection{Opportunities in GPU sharing}
Some recent work~\cite{cgpu,mig, multi-streams, mps} has exploited GPU sharing approaches to improve GPU resource utilization. 
There are two aspects of GPU sharing, i.e., time-sharing and space-sharing.
We compare GPU sharing approaches for widely-deployed NVIDIA GPUs with an example as shown in Figure~\ref{fig:motiv_gpusharing}.

\parabf{Time-sharing is not efficient to improve GPU resource utilization.}
In time-sharing, shared workloads use different time slices.
To protect the performance of online workloads, priority-based time-sharing~\cite{xiao2020antman,cgpu} (Figure~\ref{fig:motiv_gpusharing}(a)) assigns more time slices to high-priority workloads.
However, a single online workload usually cannot fill all SMs of one GPU completely~\cite{han2022microsecond, ma2020rammer}, leading to a waste of GPU computing power.

\parabf{Opportunity: space-sharing of GPU.}
When a workload cannot fully utilize the GPU computing units, i.e., SMs, it can share the idle SMs with other workloads.
The SMs of one GPU can be divided into multiple parts, and used by different workloads simultaneously, i.e., space-sharing.
We summarize three space-sharing approaches to share widely-deployed NVIDIA GPUs in Figure~\ref{fig:motiv_gpusharing}.
NVIDIA proposes \textit{multi-instance GPU (MIG)}~\cite{mig} which can partition one GPU into multiple instances, as shown in Figure~\ref{fig:motiv_gpusharing}(b).
However, the partition cannot be dynamically adjusted during workload execution, and thus, we have to allocate maximum resources for online workloads which leads to a waste of GPU.
\revise{
Additionally, MIG only works for specific new-generation GPU types, e.g., A100 and H100, which are not widely used in production clusters.
}
CUDA provides \textit{multiple streams}~\cite{multi-streams} (Figure~\ref{fig:motiv_gpusharing}(c)) to execute kernels from multiple workloads, whereas the concurrent workloads can significantly degrade the performance of online workloads.
Besides, NVIDIA stream can only share with other streams in one process, which needs to merge multiple workloads and is hard to manage in production clusters.
We find that \textit{NVIDIA MPS}~\cite{mps} (Figure~\ref{fig:motiv_gpusharing}(d)) is the best trade-off between GPU resource utilization and online performance.
\revise{
MPS is supported by Kepler and newer NVIDIA GPUs which are the majority of the GPUs used in production clusters.
}
MPS enables NVIDIA GPU to execute multiple workloads at the same time by assigning different sets of SMs to the shared workloads.
Besides, MPS provides environment variables to roughly control the SM percentage used by each workload, which enables performance protection of online workloads.

\begin{figure}[t]
	\subfigure[]{
        \begin{minipage}{0.47\linewidth}
        \centerline{\includegraphics[width=\linewidth,trim=0 0 0 0,clip]{figures/Motiv_mps.pdf}}
        % \vspace{-0.1in}
        \label{fig:motiv_mps}
        \end{minipage}
        }
	\subfigure[]{
        \begin{minipage}{0.47\linewidth}
        \centerline{\includegraphics[width=\linewidth,trim=0 0 0 0,clip]{figures/Motiv_mps_cmatp.pdf}}
        % \vspace{-0.1in}
        \label{fig:motiv_cmatp}
        \end{minipage}
    }
     \vspace{-0.1in}
     \caption{(a) GPU sharing with MPS and adjusted SM percentage (A-B represents sharing one online workload A with one offline workload B. V, D are short for VGG16 and DenseNet201, respectively), and (b) Impact of the SM percentage for offline workloads (DenseNet201 as the online workload and VGG16 as the offline workload).}
     \vspace{-0.1in}
    \label{fig:motiv_mps_all}
\end{figure}

To show the effect of MPS, we choose two DL models, VGG16~\cite{simonyan2014very} and DenseNet201~\cite{huang2017densely} as workloads.
We use the inference of these DL models as online workloads and the training as offline workloads.
These workloads are tested on NVIDIA T4 GPU.
Figure~\ref{fig:motiv_mps} reports the normalized performance when we share one online workload with one offline workload.
The normalized performance is the average iteration duration when running alone divided by the average iteration duration when shared with other workloads.
To protect the performance of online workloads, we adjust the SM percentage of offline workloads.
Figure~\ref{fig:motiv_mps} demonstrates that one GPU can provide up to $62\%$ more computing power while slowing online workloads less than $20\%$.
The results indicate the potential of sharing GPU in space with MPS.

\parabf{Challenges for space-sharing.}
There are some technical challenges to deploy space-sharing in large-scale DL clusters.
First, the primary goal of production clusters is to guarantee the performance of online workloads.
MPS enables us to roughly change the SM percentage used by each workload.
However, it cannot guarantee the performance of online workloads.
For example, the online requests may suddenly burst due to a special activity, but the SM percentage for offline workloads cannot be reduced timely.
Thus, we need to control the execution process of the shared workloads to protect online workloads.
Second, MPS is notorious for its serious error propagation problems.
Specifically, when one workload encounters an error, the shared workload will also be influenced.
These safety problems are intractable and critical in production clusters.
Third, different SM percentages assigned to offline workloads can greatly influence the efficiency of both shared workloads.
We change the SM percentage assigned to offline workloads from $10\%$ to $100\%$ as shown in Figure~\ref{fig:motiv_cmatp}.
The normalized performance of both online workload and offline workload varies more than $5\times$.
Thus, choosing a proper SM percentage is important to provide efficient GPU sharing.
Fourth, different shared pairs of online and offline workloads show different impacts on the shared workloads in Figure~\ref{fig:motiv_mps}.
The normalized performance of offline workloads varies up to $50\%$ in Figure~\ref{fig:motiv_mps}.
Additionally, the number of possible sharing plans is factorial to the number of workloads, which is enormous for a production cluster.
We need to efficiently decide how to share workloads to maximize offline efficiency, while maintaining the performance of online workloads.
