\section{Related work}
\label{sec:related}

\paraf{DL workload scheduling.}
Existing DL schedulers are mainly designed for online or offline workloads, but not both, to ensure the performance of online workloads and avoid interference.
The primary goals of online workload schedulers~\cite{crankshaw2017clipper,shen2019nexus,gujarati2020serving,romero2021infaas} are meeting the latency demand and improving overall throughput.
However, these schedulers let one workload monopolize GPUs and thus, cannot fully exploit the GPU resource.
Most prior offline workload schedulers~\cite{gu2019tiresias,hwang2021elastic,qiao2020pollux,mohan2022synergy} also allocate GPUs exclusively.
However, existing offline workload schedulers cannot be directly applied to GPU-sharing clusters because they cannot ensure the performance of high-priority online workloads.
Differently, \sysname leverages space-sharing to improve GPU resource utilization and applies a two-level protection mechanism to guarantee the performance of online workloads.

\parabf{Resource sharing for big data workloads.}
Prior work has studied resource sharing for big data workloads and CPU clusters.
DRF~\cite{ghodsi2011dominant} extends max-min fairness to achieve resource sharing fairness.
Tetris~\cite{grandl2014multi}, Graphene~\cite{grandl2016graphene}, and Carbyne~\cite{grandl2016altruistic} propose heuristic algorithms to solve multi-resource scheduling problems and improve resource utilization.
MonoSpark~\cite{ousterhout2017monotasks} improves performance clarity by splitting data analytics tasks into monotasks.
Many large enterprises deploy resource-sharing clusters.
Apollo system~\cite{boutin2014apollo} improves resource utilization by opportunistic tasks in Microsoft.
Google's Borg~\cite{verma2015large,tirmazi2020borg} adopts machine sharing with performance isolation to achieve high utilization.
In comparison, DL workloads use GPUs to speed up, and GPUs lack efficient and safe sharing mechanisms.
Thus, it is more challenging to deploy GPU sharing in production clusters.

\parabf{GPU sharing for DL workloads.}
Recently, GPU sharing has been studied for DL workloads.
The techniques to share GPUs mainly fall into two categories.
Prior time-sharing approaches~\cite{xiao2018gandiva,wang2021wavelet,lim2021zico,zhao2022multi} may impact the efficiency of shared workloads and cannot fully utilize GPU computing power in every time slice.
\revise{
Salus~\cite{yu2019salus} and PipeSwitch~\cite{bai2020pipeswitch} propose fast job switching and memory management techniques to speed up time-sharing.
But they cannot avoid the intrinsic drawbacks of time-sharing.
}
Some work~\cite{gu2018gaiagpu,xiao2020antman,weng2022pai,cgpu} assigns time slices according to priority to guarantee the performance of online workloads.
However, these approaches still cannot improve resource utilization for each time slice.
\sysname employs space-sharing and performance protection mechanisms for efficient and safe GPU sharing.

Space-sharing is the other direction to share GPU.
NVIDIA's MPS~\cite{mps} is a general method to multiplex jobs on NVIDIA GPUs.
Gavel~\cite{narayanan2020heterogeneity} leverages MPS directly but it cannot guarantee the performance of online workloads.
\revise{
GSLICE~\cite{dhakal2020gslice} advances MPS to support dynamic and fair resource allocation but it does not consider cluster-level scheduling.
}
Retiarii~\cite{zhang2020retiarii} merges multiple similar models to improve GPU utilization which is infeasible for production clusters running diverse workloads.
\revise{
DeepPool~\cite{park2022efficient} and Reef~\cite{han2022microsecond} leverage priority-based multi-stream approaches.
However, multi-stream approaches are unfit for production deployment mainly due to two reasons.
First, it needs to manually merge different workloads into one process to leverage NVIDIA GPU streams, which is not friendly to existing infrastructure and users.
Second, multiple streams may introduce the overhead of locks and CPU kernel launches.
}
In contrast, \sysname is a practical space-sharing cluster system that has been deployed at \company.

\revise{
Some work predicts performance interference among shared workloads for time-sharing~\cite{chen2016baymax,chen2017prophet} and space-sharing~\cite{zhao2019themis}.
These approaches play the similar role as the DL-based speed predictor in \sysname, and are orthogonal to other system designs.
}

