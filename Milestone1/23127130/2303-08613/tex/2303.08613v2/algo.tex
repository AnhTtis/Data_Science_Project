\section{The OSRL-UCB Algorithm}
In this section, we introduce the algorithm for Online Scoring Rule Learning with Upper Confidence Bound (OSRL-UCB). We begin with an overview of the algorithm in \S\ref{sec:algo overview} and introduce an action-informed oracle that is necessary for the algorithm in \S\ref{sec:oracle}. In \S\ref{sec:UCB}, we give a detailed description of the OSRL-UCB algorithm. To simplify notation, we let $k$ to denote $b_k$ in the sequel.

\subsection{Algorithm Overview}\label{sec:algo overview}
Define $\cV_k=\cbr{S\in\cS\given g(k, S)\ge g(k', S), \forall k'\in[K]}$ as the $k$-th section in which the agent takes action $b_k$ as her best response. The principal's optimization problem \eqref{eq:stackelberg-2} can be reformulated as
\begin{align}\label{eq:bandit problem}
    \max_{k\in [K]} h^{k,*}\defeq\sup_{S\in\cV_k} \EE_{\sigma\sim q_k}\sbr{u(\sigma)-S(\sigma)}, 
\end{align}
where $h^{k, *}$ is the principal's optimal profit when the agent's best response is $k$.
Let $S^{k, *}$ be the optimal solution to the inner problem of \eqref{eq:bandit problem}.
If the principal knows the best scoring rule $S^{k, *}$ that achieves $h^{k,*}$, the problem immediately reduces to a multi-arm bandit where $k\in[K]$ is the $K$ arms and $h^{k, *}$ is the reward for arm $k$. Such a problem can thus be handled by the standard UCB algorithm \citep{auer2002finite}. 
However, there are two obstacles: (i) the action region $\cV_k$ is unknown; (ii) the belief distribution $q_k$ is unknown.
Recall the definition of $\cV_k$:
\begin{align*}
    \cV_k=\cbr{S\in\cS\given \inp[]{q_k-q_{k'}}{S}_\Sigma\ge c_k-c_{k'}, \forall k'\in[K]}.
\end{align*}
To identify $\cV_k$, we just need to estimate the belief distribution $q_k$ and the pairwise cost difference defined as $C(i,j)=c_i-c_j$. Specifically, estimator $\hat q=(\hat q_k)_{k\in[K]}$ can be updated by the empirical distribution of $\sigma_t$ such that $k_t=k$ while estimator $\hat C=(\hat C(i,j))_{i,j\in[K]}$ can be updated using the following identity
\begin{align}\label{eq:C difference}
    C(i,j)=\inp[]{q_i-q_j}{S}_{\Sigma}, \quad\forall S\in\cV_i\cap\cV_j, 
\end{align}
where we plug in estimator $\hat q$.
In addition, we must identify a scoring rule $S\in\cV_i\cap\cV_j$ to successfully employ \eqref{eq:C difference}. 
To this end, we employ a binary search method on the convex combination of $S_1,S_2$ such that $k^*(S_1)=i$ and $k^*(S_2)=j$. The details of the binary search are given in \Cref{alg:BS}.
To inform the principal of the $K$ actions and also to guarantee that the principal can find a scoring rule $S$ such that $k^*(S)=i$ for the sake of the binary search, we introduce with examples an action-informed oracle in \S\ref{sec:oracle}, which provides the principal with foreknown scoring rules $\tilde S_1, \tilde S_1,\dots,\tilde S_K$ such that $k^*(\tilde S_i)=i$.

Now that the estimation problem of $q_k$ and $C(i,j)$ is addressed, the inner problem of \eqref{eq:bandit problem} can be solved by the following constrained linear program,
\begin{align}\label{eq:cV LP} 
    \LP_k:\quad &h^{k, *}=\max_{S\in\cS}\quad \inp[]{ q_k}{u-S}_{\Sigma},\\
    &\mathrm{s.t.}\quad\inp[]{q_k- q_{k'}}{S}_\Sigma\ge C(k, k'), \quad\forall k'\in[K]. \notag 
\end{align}
If \eqref{eq:cV LP} is solved, we can reduce the outer problem of \eqref{eq:bandit problem} to a bandit by viewing $[K]$ as the set of arms and the $h^{k, *}$ as the reward for each arm $k\in[K]$. 
To illustrate the method for solving $\LP_k$,
let $\tilde \cQ=\{(\tilde q_k)_{k\in[K]}\}$ and $\tilde \cC=\{(\tilde C_{ij})_{i,j\in[K]}\}$ be the confidence sets for $\hat q$ and $\hat C$ that capture the real $q$ and $C$ with high probability.
% For any $\tilde q=(\tilde q_k)_{k\in[K]}\in\tilde \cQ$ and any $\tilde C=(\tilde C_{ij})_{i,j\in[K]}\in\tilde \cC$, let the optimal value of $\LP_k$ when plugging in $\tilde q, \tilde C$ be $h^k_{\tilde q, \tilde C}$ and the optimal solution be $S^k_{\tilde q, \tilde C}$.
To encourage exploration, we follow the principle of optimism and obtain an upper confidence bound for $h^{k, *}$ by solving $\LP_k$ with plugged-in $(\tilde q, \tilde C)$ that maximizes the optimization goal (principal's profit) over the confidence sets $\tilde \cQ\times\tilde \cC$.
This optimistic version of $\LP_k$ is given by 
$\OptLP_k$ in \eqref{eq:LP_k}
, in which we do not explicitly construct these confidence sets, but instead exploit the confidence intervals for estimating the utilities, the payments, and the cost differences using $\hat q$ and $\hat C$. 
With the optimistic reward for each action given by $\OptLP_k$, the algorithm simply chooses action $k_t^*$ that maximizes this reward, and obtain the scoring rule solution $S_t^*$ corresponding to $k_t^*$.

However, there is one remaining issue if we want to deploy $S_t^*$ to incentive the agent to choose action $k_t^*$. The fact that the constraints of \eqref{eq:cV LP} are relaxed in $\OptLP_k$ by using optimism may cause $S_t^*$ to go beyond $\cV_{k_t^*}$.
To address such a problem, we propose to deploy a conservatively adjusted scoring rule $S_t = (1-\alpha_t) S_t^* + \alpha_t \tilde S_{k_t^*}$, which guarantees the agent to choose $k_t^*$ with high probability. By letting $\alpha_t$ to decay with $t$, the conservatively adjusted scoring rule also guarantees optimism.
The algorithm is summarized in Algorithm \ref{alg:UCB} and details of the algorithm is available in \S\ref{sec:UCB}.

% \iffalse 
% \begin{algorithm}
% \caption{Online Scoring Rule Learning with Upper Confidence Bound (OSRL-UCB)}\label{alg:UCB}
% \begin{algorithmic}[1]
% \STATE {\bfseries Input:} {$\cS$, $(\tilde S_{1}, \cdots, \tilde S_{K})$.}
% \WHILE{$t\le T$}
% \STATE Estimate the belief distributions $\hat q_k^t$ by \eqref{eq:hat q} and the corresponding confidence intervals $I_q^t(k)$ by \eqref{eq:I_q};
% % \begin{align*}
% %   \hat q_k^t(\sigma) & \leftarrow \frac{1}{n_k^{t}}\sum_{\tau=1}^{t-1}\ind(\sigma_\tau=\sigma, k_\tau=k), \\
% % I_{q}^t(k) &\leftarrow  \sqrt{\frac{2\log(K\cdot 2^M t)}{n_{k}^t}} .
% % \end{align*}
% \STATE Estimate the pairwise cost differences $\hat C^t(i,j)$ and the corresponding confidence intervals $I_c^t(i,j)$ by \eqref{eq:hat C-I_c};
% % \begin{align*}
% % l_{ij} &\leftarrow\text{shortest path}(\cG, i,j), \\
% % \hat C^t(i, j) &\leftarrow \sum_{(i', j')\in l_{ij}} \theta^t(i', j'), \\
% % I_{c}^{t}(i, j) &\leftarrow \sum_{(i', j')\in l_{ij}} \varphi^t(i', j')
% % \end{align*}
% \STATE Solve $\OptLP_k$ in \eqref{eq:LP_k} and obtain the optimal value $h_\LP^t(k)$ and the optimal solution $S_{\LP, k}^t$ for $k\in[K]$;
% \STATE Choose the best arm $k_t^* \leftarrow \argmax_{k\in[K]} h_\LP^t(k)$ and let  $S_t^*\leftarrow S_{\LP, k_t^*}^t$;
% \STATE Pay with the conservative scoring rule $S_t \leftarrow \alpha_t\tilde S_{k_t^*} + \rbr{1-\alpha_t} S_t^*$, and collect the agent's responding action $k_t$;
% \IF{$k_t\neq k_t^*$}
% %     \STATE $t\leftarrow t+1$
% % \ELSE
%     \STATE Conduct binary search $\text{BR}(S_t, \tilde S_{k_t^*}, k_t^*, t)$ as specified in \Cref{alg:BS}.
% \ENDIF
% \ENDWHILE
% \end{algorithmic}
% \end{algorithm}
% \fi


% \begin{algorithm}
% \caption{Online Scoring Rule Learning with Upper Confidence Bound (OSRL-UCB)}\label{alg:UCB}
% \begin{algorithmic}[1]
% \STATE {\bfseries Input:} {$\cS$, $(\tilde S_{1}, \cdots, \tilde S_{K})$.}
% \WHILE{$t\le T$}
% \STATE Estimate the belief distributions $\hat q_k^t$ by \eqref{eq:hat q} and the corresponding confidence intervals $I_q^t(k)$ by \eqref{eq:I_q};
% \STATE Estimate the cost differences $\hat C^t(i,j)$ and the corresponding confidence intervals $I_c^t(i,j)$ by \eqref{eq:hat C-I_c};
% \STATE Solve $\OptLP_k$ in \eqref{eq:LP_k} with the optimal value $h_\LP^t(k)$ and the optimal solution $S_{\LP, k}^t$ for $k\in[K]$;
% \STATE Choose the best arm $k_t^* \leftarrow \argmax_{k\in[K]} h_\LP^t(k)$ and let  $S_t^*\leftarrow S_{\LP, k_t^*}^t$;
% \STATE Announce conservative scoring rule $S_t = \alpha_t\tilde S_{k_t^*} + \rbr{1-\alpha_t} S_t^*$, and observe the agent's response $k_t$;
% \IF{$k_t\neq k_t^*$}
%     \STATE Conduct binary search $\text{BR}(S_t, \tilde S_{k_t^*}, k_t^*, t)$ as specified in \Cref{alg:BS};
% \ENDIF
% \ENDWHILE
% \end{algorithmic}
% \end{algorithm}

\begin{figure}
\makebox[\linewidth]{%
\begin{minipage}{.95\linewidth}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\STATE {\bfseries Input:} {$\cS$, $(\tilde S_{1}, \cdots, \tilde S_{K})$, $T$.}
\WHILE{$t\le T$}
\STATE Estimate the belief distributions $\hat q_k^t$ by \eqref{eq:hat q} with confidence intervals $I_q^t(k)$ by \eqref{eq:I_q};
\STATE Estimate the cost differences $\hat C^t(i,j)$ with confidence intervals $I_c^t(i,j)$ by \eqref{eq:hat C-I_c};
\STATE Solve $\OptLP_k$ in \eqref{eq:LP_k} with the optimal value $h_\LP^t(k)$ and solution $S_{\LP, k}^t$ for $k\in[K]$;
\STATE Choose the best arm $k_t^* \leftarrow \argmax_{k\in[K]} h_\LP^t(k)$ and let  $S_t^*\leftarrow S_{\LP, k_t^*}^t$;
\STATE Announce scoring rule $S_t = \alpha_t\tilde S_{k_t^*} + \rbr{1-\alpha_t} S_t^*$, and get the agent's response $k_t$;
\IF{$k_t\neq k_t^*$}
    \STATE Conduct binary search $\text{BS}(S_t, \tilde S_{k_t^*}, k_t^*, t)$ as specified in \Cref{alg:BS};
\ENDIF
\STATE Move on to a new round $t\leftarrow t+1$;
\ENDWHILE
\end{algorithmic}
\caption{Online Scoring Rule Learning with Upper Confidence Bound (OSRL-UCB)}\label{alg:UCB}
\end{algorithm}
\end{minipage}}
\end{figure}


% \begin{algorithm}
% \caption{UCB for online scoring rule learning (OSRL-UCB) algorithm}\label{alg:UCB}
% \KwData{$\cS$, $(\tilde S_{0}, \cdots, \tilde S_{K})$, $t=0$.}
% \While{$t\le T$}{
% Estimate the belief distributions $\hat q_k^t$ with confidence intervals $I_q^t(k)$:
% \vspace{-10pt}
% $$\hat q_k^t(\sigma) \leftarrow \frac{1}{n_k^{t}}\sum_{\tau=1}^{t-1}\ind(\sigma_\tau=\sigma, k_\tau=k),\quad I_{q}^t(k) \leftarrow  \sqrt{\frac{2\log(K\cdot 2^M t)}{n_{k}^t}};$$\\
% \vspace{-10pt}
% Estimate pairwise cost differences $\hat C^t(i,j)$ with confidence intervals $I_c^t(i,j)$:
% \vspace{-10pt}
% $$l_{ij}\leftarrow\text{shortest path}(\cG, i,j), \quad \hat C^t(i, j) \leftarrow \sum_{(i', j')\in l_{ij}} \theta^t(i', j'), \quad I_{c}^{t}(i, j) \leftarrow \sum_{(i', j')\in l_{ij}} \varphi^t(i', j');$$\\
% \vspace{-10pt}
% Solve $\OptLP_k$ in \eqref{eq:LP_k}  with optimal value $h_\LP^t(k)$ and solution $S_{\LP, k}^t$ for $k\in[K]$
% \;
% Choose the best arm $k_t^* \leftarrow \argmax_{k\in[K]} h_\LP^t(k)$ and let  $S_t^*\leftarrow S_{\LP, k_t^*}^t$\;
% Announce conservative scoring rule $S_t \leftarrow \alpha_t\tilde S_{k_t^*} + \rbr{1-\alpha_t} S_t^*$, make decision $a^*(\hat\sigma_t)$, and collect the agent's responding action $k_t$\;
% \eIf{$k_t=k_t^*$}{
%     $t\leftarrow t+1$\;
% }
% {
%     Do binary search (Algorithm \ref{alg:BS}): $t\leftarrow t +\text{BR}(S_t, \tilde S_{k_t^*}, k_t^*, t)$\;
% }
% }
% \end{algorithm}


\subsection{Oracle for Action Awareness}\label{sec:oracle}
We assume that there is an oracle that provides the principal with a foreknown set of scoring rules that induce the agent to pick all her actions. This is a strictly weaker assumption than many existing online learning models in strategic environments, which assume that the principal can predict the best response of the agent, or know some parameters of the agent's utility function.

\begin{assumption}[Action-informed oracle]\label{asp:oracle}
We assume that there is an oracle which comes up with $K$ scoring rules $(\tilde S_1, \cdots, \tilde S_K)$ such that under $\tilde S_k$, the agent's best response is action $k$ for $k\in[K]$. Moreover, for the agent's profit $g(k, S)=\EE_{\sigma\sim q_k} S(\sigma)-c_k$, we assume that there exists $\varepsilon>0$ such that for any $k'\neq k$, we have $g(k, \tilde S_k)-g(k', \tilde S_k) > \varepsilon$.
\end{assumption}
With the oracle in Assumption \ref{asp:oracle}, the principal is initially aware of the $K$ actions that the agent might respond, which can be easily done by trying $(\tilde S_1, \cdots, \tilde S_K)$ one by one. In addition, we assume that $\tilde S_k$ lies within the interior of the $k$-th section $\cV_k=\cbr{S\in\cS\given g(k, S)\ge g(k', S), \forall k'\in[K]}$
and keeps some distance from the boundary of $\cV_k$. We remark that having $\tilde S_k$ away from the boundary is essential to induce desired behavior in the agent with high probability when applying an approximating scoring rule based on $\tilde S_k$ but with some errors. 
Notably, the assumption that the agent has marginal gain $\varepsilon$ by choosing $k$ under $\tilde S_k$ also ensures a minimum radius of section $\cV_k$, which corresponds to the non-degenerate assumption in strategic Stackelberg games \citep{letchford2009learning}.
We first present an impossible result for online learning the optimal proper scoring rule without the oracle.
\begin{lemma}[Impossible result]\label{lem:impossible}
    Suppose number of actions $K\ge 3$ and the number of possible beliefs $M\ge 3$. Without the action-informed oracle, for any online algorithm, there exists an instance such that $\text{Reg}(T) = \Omega(T)$.
\end{lemma}
See \S\ref{prof:impossible} for a construction of the hard instance.
Without the action-informed oracle, any online algorithm inevitably suffers from a linear regret. The intuition behind the hard instance is that without the oracle, any algorithm can fail to locate a scoring rule that  induces the desirable action from the agent.
This happens because the feasible region, i.e., $\cV_k$ in the space of bounded proper scoring rules for this desirable action $k$, can be arbitrarily small. 

We give two examples where an action-informed oracle is achievable. The first example is through random sampling, adapted from \citep{letchford2009learning}. We sample from the class of strongly proper scoring rules $\cS_\beta$, with definition deferred to \Cref{def:strongly proper}.

\begin{example}[Action awareness via random sampling]\label{exp:random sampling}
Let  $d_1=\min_{1\le i< j\le M}\nbr{\sigma_i-\sigma_j}_\infty$ and $d_2=\min_{1\le k<k'\le K}\max_{i\in[M]}\sbr{q_k(i)-q_{k'}(i)}$.  
Since the proper scoring rule class $\cS$ has bounded norm, the $\beta$-strongly proper scoring class also has bounded volume $\vol(\cS_\beta)<\infty$. 
Set $\kappa={d_1^2\beta}/{2}$ and
let $\tilde\cV_k=\{S\in\cS_\beta\given g(k, S)\ge g(k', S)+\kappa, \forall k'\neq k, k'\in[K]\}$. We  suppose $\vol(\tilde\cV_k)\ge \eta \Vol(\cS_\beta)$ for $k\in[K]$. 
Initiate $\cM=\emptyset$ as the candidate set.
Each time, we randomly sample a $\beta$-strongly proper scoring rule $S\in\cS_\beta$ and obtain the agent's best response $k^*(S)$ with respect to $S$. 
Let $e_i(\sigma, \omega)=\ind(\sigma=\sigma_i)$ for $i\in[M]$. By setting $\kappa=d_1^2\cdot\beta/2$, we deploy $\cbr{S-\kappa e_i}_{i\in[M]}$ and see if the agent still responds $k^*(S)$. If so, Let $\sS=\sS\cup \cbr{S}$; if not, reject $S$. 
    After $\cO(M\eta^{-1} K\log K)$ rounds, with high probability, $\sS$  serves as a valid action-informed oracle  with parameter $\epsilon=d_2\cdot \kappa$.
% \begin{example}[Action awareness via random sampling]\label{exp:random sampling}
% Suppose that $\cS$ has bounded volume $\vol(\cS)<\infty$.
% Let $\tilde\cV_k=\cbr{S\in\cS\given g(k, S)\ge g(k', S)+\varepsilon, \forall k'\neq k}$ and suppose $\vol(\tilde\cV_k)\ge \eta \Vol(\cS)$ for $k=1,\cdots, K$. 
% Initiate $\sS=\emptyset$.
% Each time, we randomly sample a scoring rule $S\in\cS$ and obtain the agent's best response $k^*(S)$ under $S$. Let $e_i(\sigma, \omega)=\ind(\sigma=\sigma_i)$ for $i\in[M]$. Deploy $\cbr{S+\varepsilon e_i}_{i\in[M]}$ and see if the agent still responds $k^*(S)$. If so, Let $\sS=\sS\cup \cbr{S}$; if not, reject $S$. After $\cO(M\eta^{-1} K\log K)$ rounds, $\sS$ serves as a valid action-informed oracle with high probability. 
\end{example}
See \S\ref{sec:random sampling proof} for more details.
In Example \ref{exp:random sampling}, we ensure a set of scoring rules that successfully induces each action by randomly sampling in $\cS$ for up to $\cO(MK\log K)$ times.
We next show another example via use of linear contract where the information structure satisfies some special properties. 

\begin{example}[Action awareness via linear scoring rule]\label{exp:linear contract}
Suppose that these $K$ actions are sorted in an increasing order with respect to the cost. Define $u_k = \EE_{\omega\sim\sigma, \sigma\sim q_k}\sbr{u(a^*(\sigma), \omega)}$ and suppose $u_k>0$. We assume the marginal information gain is strictly decaying, i.e., there exists a $\epsilon>0$ such that 
    \begin{gather*}
        \frac{u_{K}-u_{K-1}}{c_{K}-c_{K-1}} >\epsilon, \quad \text{and}
        \quad
        \frac{u_k-u_{k-1}}{c_k-c_{k-1}} - \frac{u_{K}-u_k}{c_{K}-c_k} >\epsilon, \quad \forall k=2,\cdots, K-1.
    \end{gather*}
    Moreover, we assume that $(u_2-u_1)/(c_2-c_1)\le b$. The principal sets the scoring rule as $S(\sigma, \omega)=\lambda u(a^*(\sigma), \omega)$ and conducts binary search on $\lambda\in[0, 2/\epsilon]$. Specifically, the binary searches are iteratively conducted on all the segments $(\lambda_1, \lambda_2)$ with $k^*(\lambda_1 u) \neq k^*(\lambda_2 u)$, where $\lambda_1, \lambda_2$ are neighboring points on $[0, 1/\epsilon]$ that are previously searched. With the maximal searching depth $m= \ceil{\log_2(2b(b-\epsilon)/\epsilon^2)}$, we can identify all the agent's actions. Suppose that $(\lambda_1^k, \lambda_2^k)$ is the largest segment with $\lambda_1^k$ and $\lambda_2^k$ searched before and $k^*(\lambda_1^k u)=k^*(\lambda_1^k u)=k$. By letting $\tilde S_k=(\lambda_1^k + \lambda_2^k)u/2$, we obtain an oracle with $\varepsilon=\epsilon u_1/4b^2$. The procedure takes $\cO(K\log_2(\varepsilon^{-1}))$ rounds.
\end{example}
See \S\ref{sec:linear contract proof} for more details.
In this example, we exploit the power of linear contract \citep{dutting2019simple} to identify all the agent's actions and produce a set of scoring rules based on the principal's utility. Specifically, the assumption of decaying marginal information gain is commonly seen in real world practice, and it further guarantees that all the actions are inducible using linear contracts. To obtain such an oracle, we just need at most $\cO(K\log_2(\varepsilon^{-1}))$ trials, which is more efficient than random sampling.
We remark that these foreknown scoring rules obtained by the oracle do not need to be optimal in each section $\cV_k$.
They can even be obtained through random sampling from $\beta$-strongly proper scoring rules  (See \Cref{exp:random sampling}) for general setting, or discovered in a linear scoring rule class (See \Cref{exp:linear contract}) if the marginal information gain is strictly decaying.

\subsection{OSRL-UCB Algorithm}\label{sec:UCB}
% Given the action-informed oracle, the principal can take the agent's actions as the bandit arms, though the best scoring rule corresponds to each \say{arm}, i.e., 
% the inner problem of \eqref{eq:bandit problem}, still remains unknown.
% We propose a UCB algorithm that optimistically chooses the agent's responding action $k_t^*$ by solving for the possibly best scoring rule $S_t^*$ that induces the agent to choose this action through linear programming (LP). 
% To solve this LP, the principal has to acquire knowledge about the information structure. 
% Specifically, the distribution of the agent's belief $q_k$ for $k\in[K]$, and the pairwise cost difference $c_i-c_j$ for $(i, j)\in([K])^2$ are required. 
% Given that the principal learns the information structure up to some errors, $S_t^*$ might fail to induce the desired agent's action. 
% To deal with the problem, we combine $S_t^*$ with $\tilde S_{k_t^*}$ to guarantee high probability of agent responding $k_t^*$ though at some cost of suboptimality. 
% The algorithm is summarized in Algorithm \ref{alg:UCB}.
The OSRL-UCB algorithm contains the following four parts: (i) learning the belief distributions; (ii) learning the pairwise cost differences; (iii) solving for the optimal payment/scoring rule in each $\cV_k$ via optimistic linear programming $\OptLP_k$ and choosing the best \say{arm} via UCB; (iv) applying a conservative scoring rule and conduct binary search if $k_t\neq k_t^*$ to refine our estimation on $\cV_k$.
In this subsection, we describe the OSRL-UCB algorithm in detail. 


\paragraph{Learning the Belief Distributions.}
Let $n_{k}^t$ denote the total number of times the agent responds action $k$ before round $t$. Then, $q_k$ can be learned empirically as 
\begin{align}\label{eq:hat q}
    \hat q_k^t(\sigma) = \frac{1}{n_{k}^t} \sum_{\tau=1}^{t-1}\ind(\sigma_\tau=\sigma, k_\tau=k), \forall \sigma\in\Sigma.
\end{align}
Following from a standard concentration result in \citet{mardia2020concentration},  we define the confidence interval for $\hat q_k^t$ as
\begin{align}\label{eq:I_q}
    I_{q}^t(k) =  \sqrt{\frac{2\log((K)\cdot 2^M t)}{n_{k}^t}}
\end{align}
We state the concentration result in \Cref{lem:confidence-interval}.
Under $\hat q_k^t$, the estimated payment of scoring rule $S$ if the agent responds action $k$ is $
    \hat v_{S}^t(k) = \inp[] {S(\cdot)} {\hat q_k^t(\cdot)}_\Sigma$.

\paragraph{Learning the Pairwise Cost Differences.}
We next illustrate how to learn the pairwise cost difference. 
For each $\tau<t$ such that $k_\tau = i$ and $j\neq i$, define 
\begin{gather*}
    C_+^t(i, j) = \min_{\tau<t:k_\tau=i}\hat v_{S_\tau}(i) - \hat v_{S_\tau}(j) + B_S \rbr{I_{q}^t(i) + I_{q}^t(j)},\nend
    C_-^t(i,j)=\max_{\tau<t:k_\tau= j} \hat v_{S_\tau}(i) - \hat v_{S_\tau}(j) - B_S \rbr{I_{q}^t(i) + I_{q}^t(j)}.
\end{gather*}
For each pair $(i, j)$, we also define 
\begin{align*}
    \theta^t(i, j) &= \frac{C_+^t(i, j) + C_-^t(i, j)}{2}, 
    % \nend
    \quad
    \varphi^t(i, j) = \sbr{\frac{C_+^t(i, j) - C_-^t(i, j)}{2}}_+.
\end{align*}
Directly using $\theta^t(i, j)$ as the estimation of pairwise cost difference is not the best option for two reasons: (i) For $\theta^t(i,j)$ to be accurate, we need to detect $S_\tau$ such that $S_\tau$ lies on the boundary $\cV_i\cap\cV_j$. However, it can happen that $\cV_i\cap\cV_j=\emptyset$, thus producing a constant error. (ii) Even if $\cV_i\cap\cV_j\neq \emptyset$, finding $S_\tau \in \cV_i\cap\cV_j$ for all $(i,j)$ pairs can be costly and potentially increase the algorithm complexity. 
Instead, we observe that the number of unknown parameters in the cost is at most $K$, thus it suffices to search in a \say{tree} structure. 
Specifically, let $\cG=(\cB, \cE)$ denote the graph where the node set $\cB$ corresponds to the $K$ agent actions and the edge set $\cE$ corresponds to the pairwise cost differences $C(i,j)=c_i-c_j$. 
In addition, we let $\varphi^t(i,j)$ be the \say{length} assigned to edge $C(i,j)$, which corresponds to the uncertainty of using $\theta^t$ to estimate the cost difference. Therefore, the problem of estimating $C(i, j)$ with minimal error becomes finding the \textit{shortest path} between $b_i$ and $b_j$ on the graph $\cG$, which can be efficiently handled by Dijkstra's algorithm or the Bellman-Ford algorithm \citep{ahuja1990faster}. In summary, the cost difference is estimated by
\begin{gather}
    l_{ij}=\text{shortest path}(\cG, i,j), \quad \hat C^t(i, j) = \sum_{(i', j')\in l_{ij}} \theta^t(i', j'), 
    % \nend
    \quad
    I_{c}^{t}(i, j) = \sum_{(i', j')\in l_{ij}} \varphi^t(i', j'), \label{eq:hat C-I_c}
\end{gather}
where $I_c^t(i,j)$ is the confidence interval for the pairwise cost estimator $\hat C^t(i,j)$. 
Moreover, we can easily check that $\hat C^t(i, j)=-\hat C^t(j, i)$ since $l_{ij}$ is the same as $l_{ji}$ and $\theta^t(i, j)=-\theta^t(j, i)$ by definition. The validity of $I_c^t(i,j)$ serving as a confidence interval for $\hat C^t$ is proved in \Cref{lem:C interval}.

\paragraph{Solving for $\OptLP$ and Choosing the Best Arm.}
Note that we have the estimator $\hat q_k^t$ for the belief distributions and the estimator $\hat C^t$ for the pairwise cost differences. We are now able to solve the linear program (LP) given in \eqref{eq:cV LP} for the best scoring rule corresponding to agent action $b_k$. To incorporate the optimism principle, we relax the constraint of \eqref{eq:cV LP} using the confidence interval $I_q^t$, $I_c^t$ obtained before. 
Specifically, for agent action $b_k$, we consider the following optimistic linear program $\OptLP_k$, 
\begin{align}\label{eq:LP_k}
    &\OptLP_k:\quad \max_{S\in\cS} \quad \inp[]{u}{\hat q_k^t}_\Sigma + B_u I_q^t(k) -v, \notag \\
    &\qquad\qquad\quad \mathrm{s.t.}\quad \abr{v-\hat v_{S}^t(k)} \le B_S \cdot I_{q}^t(k), \\
    &v - \hat v_{S}^t(i) \ge \hat C^t(k, i) - \rbr{I_{c}^{t}(k, i) + B_S\cdot I_{q}^t(i)}, \forall i\neq k. \notag 
\end{align}
Here, the optimization goal is to maximize the principal's profit under the agent's best response $b_k$, where we add $B_u I_q^t(k)$ to upper bound the true value with high probability. 
The first constraint actually constructs a confidence interval $B_S I_q^t(k)$ for the payment $v$, while the second constraint relax the initial boundary condition in \eqref{eq:cV LP} using $I_c^t$ and $I_q^t$.
The relaxations in the constraints guarantee that $\OptLP_k$ is optimistic with high probability, as is verified in \Cref{lem:linear-program-bound}. 
Suppose that the optimal value and solution for $\OptLP_k$ are $h_\LP^t(k)$ and $S_{\LP, k}^t$, respectively. 
If $\OptLP_k$ is infeasible, we just let $h_\LP^t(k) = \inp[]{u}{\hat q_k^t}_\Sigma -\EE_{\sigma\sim\hat q_k} \tilde S_k(\sigma) + (B_S + B_u)I_{q}^t(k)$ and $S_k^t=\tilde S_k$.
Next, by viewing the problem as a $K$-arm bandit, we choose the best \say{arm} that maximizes the principal's optimistic expected profit,
$k_t^* = \argmax_{k\in[K]} h_\LP^t(k).  $
For simplicity, we let $S_t^* := S_{\LP, k_t^*}^t$.

% \paragraph{Conservative scoring rule and binary searching.}
\paragraph{Constructing Conservative Scoring Rules.}
However, it happens that we can sometimes be ``overoptimistic'' about the agent's best response. That is, as we relax the boundary constraint in $\OptLP_k$, the agent might not respond action $k_t^*$ under scoring rule $S_t^*$.
This fact suggests that we ought to be \emph{conservative} to a certain degree in the design of scoring rule. In particular, we consider the conservative scoring rule as,
$ S_t = (1-\alpha_t) S_t^* + \alpha_t \tilde S_{k_t^*}.$
The intuition is that since the agent strictly prefers the action $k_t^*$ under the scoring rule $\tilde S_{k_t^*}$, combining $S_t^*$ with $\tilde S_{k_t^*}$ increases the agent's relative preference of choosing action $k_t^*$. 
% if $S_t^*$ goes beyond $\cV_{k_t^*}$.
% This is because by choosing action $k_t^*$, the agent has marginal profit over other actions under $\tilde S_{k_t^*}$ and this marginal profit enters $S_t$ by convex combination. 
In \Cref{lem:mistake}, we show that with a choice of $\alpha_t=\cO(t^{-1/3})$, we can guarantee with high probability that the agent would response with action $k_t^*$ under the conservative scoring rule $S_t$. This also means the optimism (reflected by the agent's choice of action $k_t^*$) is guaranteed.

\paragraph{Refining Parameter Estimations.} Once $S_t$ is deployed, we consider two types of outcomes. If the agent responds with action $k_t=k_t^*$, our estimates of agent's decision boundary $\cV_{k_t^*}$ is good enough, and we can simply proceed to the next round. Otherwise, the agent responds with another action $k_t\neq k_t^*$, and we need to improve our estimations on $\cV_{k_t^*}$ by updating $\hat q^t, \hat C^t$ in order to successfully induce the desired action $k_t^*$ in the future. Specifically, due to the special conservative construction of $S_t$, it suffices to update the boundary of $\cV_{k_t^*}$ that lies between $S_t$ and $\tilde S_{k_t^*}$.
To achieve this, we conduct binary search on the segment connecting $\tilde S_{k_t^*}$ and $S_t$ and locate the first switch point where the agent's best response changes from action $k_t^*$ to another action. Details of the binary search algorithm are available in \Cref{alg:BS}.
% The binary search algorithm is summarized in Algorithm \ref{alg:BS}.
% \begin{algorithm}
% \caption{Binary Search {BR}($S_0, S_1, k_1$, $t$) }\label{alg:BS}
% \begin{algorithmic}[1]
% \STATE {\bfseries Input:}$S_0, S_1, k_1, t$
% \STATE {\bfseries Output:}$n$
% \STATE $\lambda_{\min}\leftarrow 0,\quad \lambda_{\max}\leftarrow 1, \quad
% n\leftarrow 0$
% \WHILE{$\lambda_{\max}-\lambda_{\min} \ge I_q^t(k)\land I_q^t(k_1)$}
% \STATE $\lambda \leftarrow \rbr{\lambda_{\min}+\lambda_{\max}}/2$
% \STATE Interact with $S_{t+n}\leftarrow (1-\lambda) S_0 + \lambda S_1$
% \IF{$k^*(S_{t+n})=k_1$}
%     \STATE $\lambda_{\max} \leftarrow \lambda$
% \ELSE
%     \STATE $\lambda_{\min}\leftarrow \lambda$
% \ENDIF
% \STATE $n\leftarrow n+1$
% \ENDWHILE
% \end{algorithmic}
% \end{algorithm}


% \begin{algorithm}
% \caption{Binary Search {BR}($S_0, S_1, k_1$, $t$) }\label{alg:BS}
% \KwData{$S_0, S_1, k_1, t$}
% \KwResult{$n$}
% $\lambda_{\min}\leftarrow 0,\quad \lambda_{\max}\leftarrow 1, \quad
% n\leftarrow 0$\;
% \While{$\lambda_{\max}-\lambda_{\min} \ge I_q^t(k)\land I_q^t(k_1)$}{
% $\lambda \leftarrow \rbr{\lambda_{\min}+\lambda_{\max}}/2$\;
% Interact with $S_{t+n}\leftarrow (1-\lambda) S_0 + \lambda S_1$\;
% \eIf{$k^*(S_{t+n})=k_1$}{
%     $\lambda_{\max} \leftarrow \lambda$;
% }
% {
%     $\lambda_{\min}\leftarrow \lambda$;
% }
% $n\leftarrow n+1$\;
% }
% \end{algorithm}


To be specific, in the process of searching the switching point, there are two useful information that we can utilize: Firstly, for the boundary of $\cV_{k_t^*}$ that lies on the segment $(\tilde S_{k_t^*},S_t)$, we induce the two actions $(i,j)$ that this boundary separates at least once, thus their belief distribution estimator $\hat q_i^t$ and $\hat q_j^t$ will be updated.
Secondly, and more importantly, this switching point $S$ lies near the boundary, we can refine the cost difference by $ \hat C(i,j)\approx\inp[]{\hat q_i^t-\hat q_j^t}{S}_{\Sigma}$ in \eqref{eq:C difference}.
Thus, this binary search can refine both the belief estimator and the cost difference estimator, which leads to a better estimation of $\cV_{k_t^*}$. Notably, such  a binary search procedure achieves sufficient accuracy within logarithmic searching time.



% Therefore, the maximal binary searching depth is
% \begin{align*}
%     m \le \max\cbr{1, - \log_2\rbr{I_{q, k}^t \land I_{q, k'}^t}}.
% \end{align*}
% After the binary search ends, we go over step \RNum{2} again. 
% The intuition behind the binary search is to obtain a good estimation of the underlying information structure, specifically, the pairwise cost difference.



