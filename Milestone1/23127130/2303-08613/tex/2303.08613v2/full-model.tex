\section{Contract Design as a Special Case of Scoring Rule Design}\label{sec:related to contract}
In this section, we compare the contract design framework to the scoring rule design framework and reduces the standard contract designing problem to a special case of the scoring rule designing problem.
In a contract designing problem, we refer to $\Omega$ as the outcome space. 
Consider the following contract designing problem:
\begin{mdframed}[style=box]
\textbf{Contract designing problem in the principal-agent framework}

\vspace{5pt}
\noindent
At the $t$-th round, the principal and the agent play as the following:
\begin{itemize}[noitemsep, topsep=3pt]
% [leftmargin=*,topsep=0pt]
% \setlength{\itemsep}{0pt}
    \item[1.] The principal announces a contract $C_t:\Omega\rightarrow \RR_+$ to the agent.
    \item[2.] Based on $C_t$, the agent chooses an action $b_{k_t}\in\cB$ indexed by $k_t$ and bears a cost $c_{k_t}\ge 0$. The action $b_{k_t}$ can be observed by the principal, but the cost $c_{k_t}$ is private to the agent.
    \item[3.] The stochastic environment then selects an outcome $\omega_t\in\Omega$ according to $p(\omega_t\given b_{k_t})$. The outcome $\omega_t$ is revealed as observation, but the generating process $p(\omega_t\given b_{k_t})$ is private to the agent.
    % \item[3.] 
    % % The agent computes her own belief $\sigma_t\in\Delta(\Omega)$ on the hidden state according to \eqref{eq:sigma}, 
    % The agent gives her report $\hat\omega_t\in\Omega$ on her observation.
    \item[4.] The principal makes a decision $a_t\in\cA$ based on $\omega_t$.
    \item[5.] In the end, the principal obtains her utility $u(a_t, \omega_t)$ and pays the agent by $C_t( \omega_t)$.
\end{itemize}
\end{mdframed}
The difference between this contract designing problem and the scoring rule designing problem is that $\omega_t$ is revealing, and the agent's action influences the principal's utility only through her action choice without giving any report. 
We remark that we can replace $u(a_t, \omega_t)$ by $u(\omega_t)=u(a^*(\omega_t), \omega_t)$ if the principal knows about the utility function and always takes the best action. In this contract design problem, the agent has an action policy $\pi:\cC\rightarrow [K]$, where $\cC$ is the contract space.
The principal targets at designing the optimal contract that maximizes her profit, i.e., utility minus payment, subject to the agent's best response given by maximizing the agent's profit, i.e., payment minus cost. The Stackelberg game for this contract designing problem can be formulated as
\begin{equation}
\begin{aligned}\label{eq:stackelberg-contract}
&\max_{C\in\cC}\quad \EE_{\omega\sim p(\cdot\given b_{\pi^*(C)})} \sbr{u(\omega) - C(\omega)},\\
&\mathrm{s.t.}\quad \pi^*(C)\in\argmax_{k\in[K]} \EE_{\omega\sim p(\cdot\given b_k)} C\rbr{\omega} - c_{k},
\end{aligned}
\end{equation}


% For any contract design problem with action space $[K]$ and outcome space $\Omega$, a contract is a mapping $C:\Omega\to \RR$ from the outcome space to a payment. After the principal posts a menu of contracts $\{C_i\}_i$, the agent selects a contract and best responds with an action. The principal receives a reward $u:\Omega\to \RR$ which is a function of the outcome. The principal designs optimal contract to maximize revenue, the expected reward minus the payment of the contract.

In the sequel, we aim to show in the scoring rule designing problem:
(i) If the hidden state is perfectly revealing, i.e., $o_t=\omega_t$ as the agent's observation after taking her action, there exists a class of scoring rules such that the above contract designing problem is equivalent to the scoring rule designing problem. 
(ii) Using proper scoring rules, the principal's optimal profit under the scoring rule framework is no less than the optimal profit under the contract framework.


To show (i), consider the scoring rule class 
$$
\sS^C=\cbr{S\in\sS\given S(\hat\sigma, \omega)=\ind(\hat\sigma=e_\omega)\cdot C(\omega), \forall C\in\cC}, 
$$
where $e_{\omega'}(\omega)=\ind(\omega=\omega')\in\Delta(\Omega)$.
Even though $S\in\sS^C$ might not be a proper scoring rule, the agent will always be truth-telling, i.e., $\hat\sigma=e_\omega$, since only by telling the truth can she gains nonzero payment.
Therefore, this hidden state $\omega_t$ is also revealed to the principal through the agent's report.
The Stackelberg game in \eqref{eq:stackelberg-2} in the scoring rule problem can therefore be written as,
\begin{equation}
\begin{aligned}\label{eq:stackelberg-scoring}
&\max_{S\in\sS^C}\quad \EE_{\omega\sim p(\omega\given b_{k^*(S)})} \sbr{u(\omega) - S(e_\omega, \omega)},\\
&\mathrm{s.t.}\quad k^*(S)\in\argmax_{k\in[K]} \EE_{\omega\sim p(\cdot\given b_{k})} S\rbr{e_\omega, \omega} - c_{k},
\end{aligned}
\end{equation}
By noting that $S(e_\omega, \omega)=C(\omega)$, we have that \eqref{eq:stackelberg-scoring} and \eqref{eq:stackelberg-contract} are actually the same problem.
We thus conclude that the contract designing problem is perfectly reduced to this scoring rule designing problem.

We also remark that even if the hidden state is perfectly revealing, the principal need not be aware in advance. By sticking to a proper scoring rule, the agent always tells the truth. Moreover, using the revelation principle stated in \Cref{lem:revelation}, for any $S\in\sS^C$, there always exists a proper scoring rule $\tilde S\in\cS$ that generates the same expected payment $\EE_{\omega\sim p(\cdot\given b_k)} \tilde S(e_\omega, \omega)=\EE_{\omega\sim p(\cdot\given b_k)} C(\omega)$ for the agent, even though $\tilde S(e_\omega, \omega)$ might not be equal to $C(\omega)$ pointwise. Therefore, statement (ii) is also justified and we conclude that the (proper) scoring rule framework has more power than the contract framework by asking one more question about the agent's belief.



\section{More Details on the Revelation Principle}\label{sec:proper scoring rule}
% In this section, we first formulate the problem of optimally acquiring information under the principal-agent framework, where the principal aims to find a scoring rule to pay the agent for giving reports and this scoring rule should maximize the principal's profit under the agent's best response. Then, we show that restricting to the class of proper scoring rule is without any loss of generality for the principal's purpose. Given that the information structure is unknown to the principal, we propose an online learning framework for the principal to optimally acquire information. 

% \subsection{Acquiring Information under the Principal-agent Framework}
% To formulate the problem of optimally acquiring information under the principal-agent framework, we consider a stochastic environment with a principal and an almighty agent. 
% At the $t$-th round, there is a hidden state $\omega_t\in\Omega$ that will affect the principal's utility, but unknown to both the agent and the principal until the end of this round. To elicit refined information from the agent,
% the principal moves first and offers a scoring rule to the agent, under which the agent receives a payment according to the quality of her report on the posterior of the hidden state. 
% The agent may choose from $K$ actions with some cost, obtain an observation related to the hidden state, and report to the principal her belief on the hidden state.
% After receiving the agent's report, the principal then makes a decision $a_t\in\cA$ and pays the agent according to the scoring rule. The information acquisition proceeds as the following:
% \begin{mdframed}[style=box]
% \textbf{Information acquisition via scoring rule}\\
% At the $t$-th round, the principal, and the agent play as the following:
% \begin{itemize}[leftmargin=*,topsep=0pt]
%     \item[1.] The principal announces a scoring rule $S_t:\Delta(\Omega)\times\Omega\rightarrow\RR_+$ to the agent.
%     \item[2.] The agent chooses an action from $\cB$ indexed by $k_t$ and bears a cost $c_{k_t}\ge 0$.
%     \item[3.] The stochastic environment selects the hidden state $\omega_t\in\Omega$ and emits an observation $o_t\in\cO$ to the agent.
%     \item[3.] The agent chooses a distribution $\hat\sigma_t\in\Delta(\Omega)$ and reports it to the principal as her estimation on the posterior of the hidden state.
%     \item[4.] Based on the report $\hat\sigma_t$ and the agent's action choice, the principal makes a decision $a_t\in\cA$.
%     \item[5.] When the hidden state $\omega_t$ is revealed, the principal obtains her utility $u(a_t, \omega_t)$, and pays the agent by $S_t(\hat\sigma_t, \omega_t)$.
% \end{itemize}
% \end{mdframed}
% \addtocounter{footnote}{-2} %3=n
% \stepcounter{footnote}
% \footnotetext{}

% Here, the scoring rule $\cS_t$ is a payment rule based on the agent's report $\hat\sigma$ and the real state $\omega_t$ with bounded norm $\nbr{S}_\infty\le B_S$. We let $B_S$ large enough such that all the agent's actions are inducible. The reward function $u(a, \omega)$ also has bounded norm $\nbr{u}_\infty\le B_u$. 
% We consider the agent's action set $\cB$ and the observation set $\cO$ to be finite.
% Specifically, $\cB=\{b_0, b_1,\dots,b_K\}$ has $K+1$ actions, with action $b_0$ being the null action and leading to a zero cost. We remark that $b_0$ captures the situation when the agent has no incentive to participate and just receives a null observation. The remaining $K$ actions are referred to as the effective actions in the sequel.
% Notably, we allow the hidden state to be influenced by the agent's action choice.
% Recall the previously discussed producer-consultant example, where the producer is the principal, the consultant is the agent, and the value of the customer group is the hidden state.
% In this example, the consultant may conduct some market investigations and collect relevant information for refining the customer group's value. Such actions may actually cause inference to the marketing and affect the customer's value. Therefore, the hidden state $\omega_t$ is endogenous in our setting.

% \paragraph{Information structure.} 
% In the remaining part of this subsection, we ignore the subscript $t$ for a while.
% In this information acquisition process, we consider the agent to be almighty who has full knowledge of the \emph{information structure}, i.e., each action's cost $c_k$ and the generating process $p(\omega, o\given b_k)$ for the hidden state and the observation under action $b_k$.
% Therefore, after obtaining the observation $o$, the agent is able to refine her belief on the hidden state as $\sigma(\omega)=p(\omega\given b_{k}, o)$. If $k=0$, the belief simply corresponds to the distribution $\sigma(\omega)=p(\omega)$.
% Note that $\sigma$ can also be viewed as a random variable mapping from the sample space to the probability space over the hidden state.
% In addition, $\sigma$ has support $\Sigma$ with cardinality $M\defeq \abr{\Sigma}\le K\times\abr{\cO}+1$.
% Let $q_k(\sigma)\in\Delta(\Sigma)$ be the distribution of $\sigma$ under the agent's action $k\in[K]^+$ where $[K]^+=\{0, 1,\dots,K\}$.
% Since $\sigma$ already captures all the information about the hidden state from the observation, we also refer to the costs $\{c_k\}_{k\in[K]^+}$ and the distributions of the belief $\{q_k\}_{k\in[K]^+}$ as the information structure.
% The information structure is private to the agent.
% We consider the situation where the principal only has knowledge of her utility function $u$ 
% \footnote{Since the utility and the hidden state are both known to the principal at the end of each round, the principal is able to obtain good estimation of the utility function. Therefore, we consider $u$ to be the principal's private information in Table \ref{table:info}.} and is able to observe the agent's action $b_{k}$
% \footnote{In cases where the principal cannot observe the agent's action, there are still ways to distinguish different actions. For instance, when $q_k$ has different support for different $k$ and the agent is truth-telling about her belief under proper scoring rules (see \S\ref{sec:proper scoring rule}), the principal is able to learn the support for a particular agent's action by repeating the same scoring rule multiple times. The next time the agent chooses the same action, the principal will be aware.}. The summary of different information types in this information acquisition process is available in Table \ref{table:info}.
% \begin{table}[h!]
% \centering
% \begin{tabular}{|c|c|c|c|}
%     \hline
%     Public Info & Agent's Private Info & Principal's Private Info & Delayed Info\\
%     \hline
%     {$S_t, k_t, \hat\sigma_t$} & $\{c_k\}_{k\in[K]^+}, \{q_k\}_{k\in[K]^+}, \sigma$ & $u$ & $w_t$\\
%     \hline
% \end{tabular}
% \caption{Table for the information types.}
% \label{table:info}
% \end{table}

% \paragraph{Information acquisition as a Stackelberg game.} 
% In this information acquisition process, the agent always aims to find her own action selection policy $k=\mu(S)$ and a reporting scheme $\hat\sigma=\nu(S, \sigma, k)$ that maximizes her own expected profit $g^{\mu,\nu}(S)$.
% Here, the reporting scheme does not depend on $o$ since $\sigma$ captures all the information about the hidden state.
% On the other hand, the principal aims to find the scoring rule $S$ and the best decision policy $a=\iota(S, \hat\sigma, k)$ that maximizes her own expected profit $h^\iota(S)$ under the agent's best response $(\mu^*,\nu^*)\in\argmax_{\mu,\nu} g^{\mu,\nu}(S)$. 
% The problem can therefore be formulated as a Stackelberg game,
% % \begin{mini!}|l|[3]
% %     {S\in\cS, \atop \pi:\cS\times\Delta(\Omega)\times[K]^+\rightarrow \cA}{h^\pi(S) 
% %     \defeq \EE_{\omega\sim\sigma \sigma\sim q_{\mu^*(S)}} \sbr{u(\pi(S, \nu^*(S, \sigma, k), k), \omega) - S(\nu^*(S, \sigma, k), \omega)}}
% % \end{mini!}
% \begin{equation}
% \begin{aligned}\label{eq:stackelberg-1}
% &\max_{\iota, S:\nbr{S}_\infty\le B_S}\quad h^\iota(S) \defeq \EE_{\omega\sim\sigma, \sigma\sim q_{\mu^*(S)}} \sbr{u\rbr{\iota\big(S, \nu^*(S, \sigma, \mu^*(S)), \mu^*(S)\big), \omega} - S(\nu^*(S, \sigma, \mu^*(S)), \omega)},\\
% &\qquad\mathrm{s.t.}\quad(\mu^*, \nu^*)\in\argmax_{\mu, \nu}  g^{\mu,\nu}(S)\defeq\EE_{\omega\sim\sigma, \sigma\sim q_{\mu(S)}} S\rbr{\nu(S, \sigma, \mu(S)), \omega} - c_{\mu(S)}.
% \end{aligned}
% \end{equation}
% Here, by taking the $(\mu^*, \nu^*)$ that maximizes the principal's profit within the agent's best response, we are actually assuming that the agent is tie-breaking in favor of the principal.

% \subsection{Eliciting Information via Proper Scoring Rules}
% Acquiring information with a proper scoring rule is a generalization of incentivizing information acquisition with contracts. In contract design, the agent selects a contract before taking an action; while in scoring rule design, the agent reports his signal after taking an action, which is equivalent to selecting a contract after taking the action. Hence, acquiring information with a proper scoring rule has more power than contract design. The following example shows that a contract design problem can be reduced to a scoring rule design problem in our paper.



%The scoring rule can be viewed as a special case of contracts in information acquisition \citep{10.1145/3490486.3538261}. 
In this section, we provide a formal argument on the revelation principle in our model. That is, it is without loss of generality to only design the proper scoring rules under which the agent is encouraged to be truth-telling. 
% In the original model, the agent is not guaranteed to report her actual belief under a general scoring rule. However, there is a class of proper scoring rules under which the agent is encouraged to be truth-telling \citep{gneiting2007strictly, dawid2007geometry}. 
\begin{definition}[Proper scoring rule]
    A scoring rule $S:\Delta(\Omega)\times\Omega\rightarrow \RR_+$ is proper if, for any belief $\sigma\in\Delta(\Omega)$ and any reported posterior $\hat\sigma\in\Delta(\Omega)$,  we have $\EE_{\omega\sim \sigma}S(\hat\sigma, \omega)\le \EE_{\omega\sim \sigma}S(\sigma, \omega)$. In addition, if the inequality holds strictly for any $\hat\sigma\neq \sigma$, the scoring rule $S$ is strictly proper. 
\end{definition}
Let $S$ be a proper scoring rule and fix the agent's policy $\mu(\cdot)$ for action selection. For a reporting scheme $\nu$ and any true belief $\sigma$, it follows from definition \ref{def:PSR} that
\begin{align*}
    g^{\mu,\nu}(S)= \EE_{\omega\sim\sigma} S(\nu(S, \sigma, \mu(S)), \omega) - c_{\mu(S)}\le \EE_{\omega\sim\sigma} S(\sigma, \omega) - c_{\mu(S)}.
\end{align*}
Therefore, the agent's expected payment is maximized by always being truth-telling about her belief under the class of proper scoring rule.
In the following, we let $S(\hat\sigma, \sigma)=\EE_{\omega\sim\sigma}S(\hat\sigma, \omega)$. 
To give an example of proper scoring rules, let us consider the binary hidden state space $\Omega=\{0, 1\}$ where the class of proper scoring rules admits the Schervish representation \citep{gneiting2007strictly}, i.e., $S(p, 1)=G(p) + (1-p) G'(p)$ and $S(p, 0)=G(p)-p G'(p)$ where $p\in[0, 1]$ and $G:[0, 1]\rightarrow\RR_+$ is a convex function.
Intuitively, the expected payment of a proper scoring rule $S$ given belief $\sigma$ and report $p$ is $S(p, \sigma)=G(p)+(\sigma-p)G'(p)$, which corresponds to the supporting line of $G$ at $p$.
In this example, the convexity of $G$ guarantees that $S(p, \sigma)=G(p)+(\sigma-p)G'(p)\le G(\sigma)=S(\sigma, \sigma)$.
Moreover, the next observation in Lemma \ref{lem:revelation} suggests that restricting to the class of proper scoring rules does not incur any loss of generality for the principal's purpose.
\begin{lemma}[Restatement of the revelation principle]
    There exists a proper scoring rule $S^*$ that is an optimal solution to \eqref{eq:stackelberg-1-concise} if the agent is truth-telling under any proper scoring rule.
    \begin{proof}
        We first prove that for any scoring rule $S$ such that $\nbr{S}_\infty\le B_S$, there always exists a \emph{proper} scoring rule $S'(\hat\sigma, \omega)=S(\nu^*(S, \hat\sigma, k), \omega)$ such that they make the same payment to the agent for any $\sigma\in\Delta(\Omega)$ and any agent's action choice. To prove that $S'$ is a proper scoring rule with norm bounded by $B_S$, we have $B_S\ge S(\nu^*(S, \hat\sigma, k), \omega)=S'(\hat\sigma, \omega)\ge 0$ and $$S'(\hat\sigma, \sigma)= S(\nu^*(S, \hat\sigma, k), \sigma)\le S(\nu^*(S, \sigma, k), \sigma)=S'(\sigma, \sigma).$$
        The fact that $S'$ makes the same payment can be verified by plugging in $\hat\sigma=\sigma$ in the definition of $S'$ since the agent is truth-telling under proper scoring rules, and taking expectation with respect to $\omega\sim\sigma$, i.e., $S'(\sigma, \sigma)=S(\nu^*(S, \sigma, k), \sigma)$, which proves the first part.
        
        Secondly, we prove that encouraging the agent to report the real belief $\sigma$ makes the principal's revenue nondecreasing. Note that
        \begin{align*}
            \max_{\iota} \EE_{\omega\sim\sigma, \sigma\sim q_k}\sbr{u(\iota(S, \nu^*(S, \sigma, k), k), \omega)} \le  \max_{\iota} \EE_{\omega\sim\sigma, \sigma\sim q_k}\sbr{u(\iota(S, \sigma, k), \omega)}=\EE_{\omega\sim\sigma, \sigma\sim q_k}\sbr{u(a^*(\sigma), \omega)},
        \end{align*}
        where $a^*(\sigma)=\argmax_{a\in\cA}\EE_{\omega\sim\sigma} u(a, \omega)$.
        Here, the inequality holds by noting that $\nu^*(S, \sigma, k)$ is a function of $\sigma$, and the equality holds by noting that $\omega\indep (S,k)\given \sigma$.
        To conclude, by choosing $S'$ instead of $S$, the payment is exactly the same while the principal's revenue is nondecreasing. Thus, the principal's profit is nondecreasing by choosing $S'$ and there must exist a proper scoring rule that is also an optimal scoring rule. 
    \end{proof}
\end{lemma}

% The power of a proper scoring rule is thus obvious -- not only paying the agent the exact amount \emph{any} scoring rule is capable of, but also encouraging truth-telling from the agent. 
Following Lemma \ref{lem:revelation}, the principal's optimal scoring rule lies within the class of proper scoring rules $\cS$ with bounded norm $\nbr{S}_\infty\le B_S$.
One concern about the use of proper scoring rules is that being truth-telling might not be the unique maximizer to the agent's utility. However, we note that the class of proper scoring rules is a convex hull with strictly proper scoring rules as the interior. Thus, adding an infinitesimal portion of a \emph{strictly proper} scoring rule to any \emph{proper} scoring rule always yields a \emph{strictly proper} scoring rule. In this sense, we can safely make the assumption that the agent always reports the true posterior under a proper scoring rule.
The following table summarizes all the different information types in our model.


% \newpage
\section{More Details on the Binary Search Algorithm}
In this section, we give a summary of the binary seach algorithm as follows, 
\begin{figure}
\makebox[\linewidth]{%
\begin{minipage}{\linewidth}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\STATE {\bfseries Input:} $S_0, S_1, k^*(S_1), t, k_t, \{I_q^t(k)\}_{k\in[K]}$;
\STATE {\bfseries Output:} $t$;
\IF{$k_t = k^*(S_1)$}
    \STATE Break the binary search algorithm;
\ENDIF
\STATE Initiate $\lambda_{\min}\leftarrow 0, \lambda_{\max}\leftarrow 1, t_0\leftarrow t$;
\WHILE{$\lambda_{\max}-\lambda_{\min} \ge I_q^{t_0}(k_{t})\land I_q^{t_0}(k^*(S_1))$}
\STATE Start a new round $t\leftarrow t+1$;
\STATE Pick $\lambda \leftarrow \rbr{\lambda_{\min}+\lambda_{\max}}/2$ as the middle point; 
\STATE The principal announces scoring  rule $S_{t} = (1-\lambda) S_0 + \lambda S_1$ and obtain the agent's response $k_{t}$;
\IF{$k_{t}=k^*(S_1)$}
    \STATE Update $\lambda_{\max} \leftarrow \lambda$;
\ELSE
    \STATE Update $\lambda_{\min}\leftarrow \lambda$;
\ENDIF
\ENDWHILE
\end{algorithmic}
\caption{Binary Search {BS}($S_0, S_1, k^*(S_1)$, $t$) }\label{alg:BS}
\end{algorithm}
\end{minipage}}
\end{figure}
The binary searching algorithm at step $t$ works on the segment connecting two scoring rules $S_0$ and $S_1$, where the agent's best response under $S_1$ is $k^*(S_1)$. The goal of this binary search is to find the first switching point of the agent's best response from $k_1$ to another action on this segment. 
The binary searching algorithm keeps updating on $\lambda_{\max}$ and $\lambda_{\min}$ as the candidate interval that contains the first switching point. Note that any $\lambda\in[0, 1]$ corresponds to a scoring rule on the segment connecting $S_0, S_1$. Each time, the algorithm deploy a scoring rule corresponding to $\rbr{\lambda_{\min}+\lambda_{\max}}/2$ and the candidate interval is thus cut by half.
In addition, we consider binary searching to a finite depth $m$ such that the searching error respects the minimal uncertainty of $\hat q_{k'}^t, \hat q_{k_t^*}^t$, i.e., 
$2^{-m}\le I_{q}^{t}(k') \land I_{q}^{t}(k_t^*)$ in Algorithm \ref{alg:BS}. Thus, the binary search achieves sufficient accuracy with logarithmic searching time.

% To simplify our discussion, we introduce the concept of equivalent proper scoring rules.
% \begin{definition}[equivalence of proper scoring rules]
% we say that $S, S'\in\cS$ are equivalent proper scoring rules if $\EE_{\omega\sim\sigma}S(\sigma, \omega)=\EE_{\omega\sim\sigma}S'(\sigma, \omega)$ for any $\sigma\in\Delta(\Omega)$.
% \end{definition}
% As it turns out that even within the class of proper scoring rules, there might be more than one scoring rule resulting in the same payment for any $\sigma\in\Sigma$. 

 
% If $G$ is not smooth at $p_0$, then we have a class of proper scoring rules corresponding to multiple sub-gradient of $G$ but having the same payment $G(p)$. With a little abuse of notation, we use $S(\sigma)=\EE_{\omega\sim\sigma}S(\sigma, \omega)$ to represent the proper scoring rule class that is equivalent to $S(\sigma, \omega)$ and $\cS=\cbr{S(\sigma)}$ to represent all the equivalent proper scoring rules.

% \subsection{Online Learning for the Optimal Scoring Rule}
% When $S\in\cS$, the agent's best report scheme is $v^*(S, \sigma)=\sigma$ and the principal's best decision policy $\iota^*$ can be simplified to $a^*(\sigma)$ since $\omega\indep (S,k)\given \sigma$. 
% Using the notations $u(\sigma)=\EE_{\omega\sim\sigma}u(a^*(\sigma), \omega)$, $S(\sigma)=\EE_{\omega\sim\sigma}S(\sigma, \omega)$,  and $k^*(S)=\mu^*(S)$, we rewrite \eqref{eq:stackelberg-1} under the class of proper scoring rules $\cS$ as
% \begin{equation}
% \begin{aligned}\label{eq:stackelberg-2}
% &\max_{S\in\cS}\quad h(S) \defeq \EE_{\sigma\sim q_{k^*(S)}} \sbr{u(\sigma) - S(\sigma)},\\
% &\mathrm{s.t.}\quad k^*(S)\in\argmax_{k\in[K]^+} g(k,S)\defeq \EE_{\sigma\sim q_{k}} S\rbr{\sigma} - c_{k}.
% \end{aligned}
% \end{equation}
% The difficulty in solving \eqref{eq:stackelberg-2} is that the information structure, i.e., $q_k$ and $c_k$, is unknown to the principal. 
% In this work, we study the problem of online learning the optimal scoring rule for information acquisition. 
% Given $H_{t-1}=\cbr{(S_\tau, k_\tau, \sigma_\tau, \omega_\tau)}_{\tau\in[t-1]}\in\cH_{t-1}$ as the history observed by the principal before round $t$, the principal is able to deploy a policy for the next round's scoring rule $\pi_{t}:\cH_{t-1}\rightarrow \cS$.
% Hence, the data generating process is described as the following,
% \begin{align}\label{eq:data generating}
%     p^\pi(S_t, k_t, \sigma_t, \omega_t\given H_{t-1}) = \ind\rbr{S_t=\pi_t(H_{t-1}), k_t=k^*(S_t)} q_{k_t}(\sigma_t) \sigma_t(\omega_t), 
% \end{align}
% and the regret for this online policy $\pi=\cbr{\pi_t}_{t\in[T]}$ is defined as
% \begin{align*}
%     \Reg^\pi(T)\defeq T\cdot \max_{S\in\cS} h(S) - \EE_\pi\sbr{\sum_{t=1}^T u(a^*(\sigma_t), \omega_t) - S_t(\sigma_t, \omega_t)}, 
% \end{align*}
% where the expectation is taken with respect to the data generating process. We aim to develop an online policy $\pi_t$ that learns the optimal scoring rule with small regret.



% we remark that requiring the agent to report her action is without loss of generality when the agent is telling the truth. In cases where the agent is not required to report her action, the principal can always repeat the same scoring rules multiple rounds and distinguish the agent's different  actions via comparing the reports. Specifically, if $q_k$ and $q_{k'}$ are different in the sense $\nbr{q_k - q_{k'}}_1\ge \eta$ for any $k\neq k'$, it is sufficient to tell two different actions with high probability after $\cO(M/\eta^2)$ repetitions.

% Now we specify the agent's best response. 
% Based on our previous discussion,
% the best response of the agent under proper scoring rule $S\in\cS$ can be denoted by tuple $\BR{S}=\rbr{k^*(S), \hat\sigma^*}$ where $\hat\sigma^*=\sigma$ under proper scoring rules.
% To study the  agent's best action $k^*(S)$, 
% note that we consider the action set $\cB$ and the observation set $\cO$ to be finite. As a consequence, the set of possible posteriors $\sigma$ is also finite, and we denote it as $\Sigma\subset\Delta(\Omega)$ with $\abr{\Sigma}=M \le K \cdot \abr{O} + 1$.
% Consider $q_k\in\Delta(\Sigma)$ as the distribution of posterior $\sigma$ if the agent chooses action $b_k$.
% The agent's distribution belief when choosing $b_k$ can then be written as,
% \begin{align*}
%     p(\omega\given b_k) = \sum_{\sigma\in\Sigma}\sigma(\omega) q_k(\sigma).
% \end{align*}
% We let $g(k, S)$ denote the agent's expected profit by choosing action $b_k$ under scoring rule $S$.  Following the fact $\hat\sigma^*=\sigma$, we can characterize the agent's best response as
% \begin{align}
%     k^*(S)= \argmax_{k\in[K]\cup \cbr{0}}  g(k, S), \where g(k, S) \defeq \EE_{\sigma\sim q_k} S(\sigma) - c_k, \label{def:k^*}
% \end{align}
% where we recall $S(\sigma)=\EE_{\omega\sim\sigma}S(\sigma, \omega)$.
% When we talk about \say{best response}, we typically refer to the agent's best action $k^*(S)$ in the sequel.

% Upon receiving the agent's report, the principal will adopt the reported belief and chooses the best action $a^*(\sigma) = \argmax_{a\in\cA} \EE_{\omega\sim\sigma}u(a, \omega)$.
% In the sequel, we simply use $u(\sigma)\defeq\max_{a\in\cA} \EE_{\omega\sim\sigma}u(a, \omega)$ for the maximal utility achievable for the principal based on the report $\sigma$.
% and $S(\sigma)\defeq\EE_{\omega\sim\sigma}S(\sigma, \omega)$ for the expected payment received by the agent.
% After the principal carries out her action, the hidden state $\omega$ is revealed. The principal receives utility $u(a^*(\sigma), \omega)$ and pays the agent a total amount of $S(\sigma, \omega)$.
% Let $h(S)$ denote the principal's expected profit.
% Under the best response of the agent, the principal aims to find the best proper scoring rule $S^*$ that maximizes her expected profit, 
% \begin{align}
%     S^*\in \argmax_{S\in\cS} h(S), \where h(S)=\EE_{\sigma\sim q_{k^*(S)}} \sbr{u(\sigma) - S(\sigma)}.\label{def:S^*}
% \end{align}
