
\section{Proofs on Action-informed Oracle in \Cref{sec:oracle}}\label{sec:prof oracle}
In this section, we prove the claims in examples in \Cref{sec:oracle}.

\subsection{Proof of \Cref{lem:impossible}}\label{prof:impossible}
Without loss of generality, we just need to construct a hard instance by considering the case $K=3$ and $M=3$. 
To simplify the discussion, we consider two hidden states $\Omega=\{\omega_1, \omega_2\}$ and consider $B_S=1$ as the boxing condition for the scoring rules.
The idea of constructing the hard instance is as follows:
\begin{itemize}
\item[(i)] Make sure that action $a_2$ is the optimal agent's response for the optimal scoring rule while any scoring rule not inducing the agent to chose $a_2$ yields a constant regret at least $1$. 
\item[(ii)] Let $\mathcal{V}_2$ be a "single point" on the boundary of $\mathcal{S}$ and show that with a great chance, any algorithm can fail to find the correct scoring rule located in $\mathcal{V}_2$ without the oracle.
\end{itemize}
Since $\Omega=\{\omega_1, \omega_2\}$, we can equally  represent $\sigma_1, \sigma_2, \sigma_3$ by the their mass assigned to $\omega_1$. With a little abuse of notation, we let $\sigma_i$ denote $\sigma_i(\omega_1)$ and $1-\sigma_i$ denote $\sigma_i(\omega_2)$. 
We consider the case $\Sigma=\{\sigma_1=(0, 1), \sigma_2=(.5, .5), \sigma_3=(1,0)\}$ for $M=3$. 
For simplicity, we let $S_i = \mathbb{E}_\omega S(\sigma_i, \omega)$ for $i=1, 2, 3$.
Hence, the set of proper scoring rules is specified by conditions:
\begin{equation}\label{cond:proper-1}
0\le S_2\le \frac{S_1+S_3}{2}, \quad B_S\ge S_1\ge 0, \quad B_S \ge S_3\ge 0.
\end{equation}
We remark that the first condition guarantees that the scoring rule is proper by requiring the curve of $G(\sigma) = \EE_\omega[S(\sigma, \omega)]$ to be convex \citep{gneiting2007strictly}, and the rest are just box conditions.
In fact, one can easily construct a proper scoring rule $S(\sigma, \omega)$ for any $S=(S_1, S_2, S_3)$ satisfying the above constraints using the Schervish representation \citep{gneiting2007strictly} by fitting a convex function $G$ defined on $[0, 1]$ passing through $(0, S_1), (.5, S_2), (1, S_3)$,  and consider the supporting hyperplanes of $G$ at these three points. 


To ensure that $\mathcal{V}_2$ is a single point on the boundary in (ii), we let $q_2- q_1$ and $q_2-q_3$ be
$$q_2-q_1 = \beta \cdot (0, -1, 1)^\top, \quad q_2-q_3=\beta \cdot (1, 1, -2)^\top, $$
where $\beta$ should be considered a fixed constant and $q_1, q_2, a_3$ are what we want to design. These conditions imply the following expression for $\mathcal{V}_2$,
$$
\cV_2 = \{S\in \mathcal{S}\,|\,(0, -1, 1) S \ge \beta^{-1}(c_2-c_1), \quad (1, 1, -2)S\ge \beta^{-1}(c_2-c_3)\}. 
$$
We further let $e_1 = \beta^{-1}(c_2-c_1)$ and $e_2 = \beta^{-1}(c_2-c_3)$. One can easily verify that as long as $e_1+e_2 = 1$ and $-1/2\le e_1\le 0$,
$\mathcal{V}_2$ shrinks to a single point given by $S^*=(1, -e_1, 0)$ and the conditions in Equation \eqref{cond:proper-1} are always satisfied. Hence, we can safely restrict ourselves to the instances specified by parameter $e_1$ and $c_1, c_2, c_3$ can be adjusted accordingly. 
Hence, we are actually considering a single parameter linear system.

To ensure (i), i.e., the optimality of $a_2$, we first need to ensure that $a_2$ beats $a_1$ and $a_2$ at $S^*$ by some constant $\gamma\ge 1$:
\begin{equation}\label{cond:optimality-2}\begin{aligned} 
\langle q_2 - q_1, u - S^*\rangle&\ge \gamma,\\ 
\langle q_2 - q_3, u - S^*\rangle&\ge \gamma, 
\end{aligned}\end{equation}

where $S^*\in \mathcal{V}_2$. Note that since $q_2 - q_1$ does not lie on the same line with $q_2-q_3$, we can always choose $u$ such that \eqref{cond:optimality-2} holds regardless of what $S^*$ is. For any $S\in\mathcal{V}_1$, we can verify that for the principal's profit, 
$$
\langle q_2, u-S^*\rangle - \langle q_1, u-S\rangle = \langle q_2 -q_1, u-S^*\rangle + \langle q_1, S- S^*\rangle \ge \gamma - 1, 
$$
where the last inequality is a direct result of \eqref{cond:optimality-2} and the fact that each entry of $S$ should be within $[0, 1]$. 
The same holds for $S\in\mathcal{V}_3$. 
Hence, it suffices to find an instance satisfying \eqref{cond:optimality-2} with $\gamma=2$. 

Construction of the hard instance:
Following the above discussion, we construct the instance by letting $\gamma=2$, $\beta = 1/16$, and 
\begin{equation}\begin{aligned}
q_1 &= (3/4, 3/16, 1/16), \nonumber\\
q_2 &=(3/4, 1/8, 1/8), \nonumber\\
q_3 &=(11/16, 1/16, 1/4) \nonumber\\
\nonumber u& = (96, 0, 32) + (1, -e_1, 0).\\
\end{aligned}\end{equation}
Here, the only thing unknown is the cost $c_1, c_2, c_3$, where the effect of the unknown cost is encoded in $e_1\in[-1/2, 0]$, which only affects the position of $S^*$.

Since we are considering a one parameter linear system with the second coordinate of $S^*$ undetermined, it is without loss of generality to consider finding the optimal scoring rule on the line $l=(1, \lambda, 0)$ where $0\le \lambda\le 1/2$. Suppose we assign scoring rule $S^t=(1, b^t, 0)$ at step $t$ and the agent responses $a^t\in\mathcal{A}=\{a_1, a_2,a_3\}$. In fact, for any $b^t< e_1$, the agent will response $a_1$ and for any $b^t> e_1$, the agent will response $a_3$. 
Then, we assert that any online strategy of the principal can result in linear regret since no algorithm can decide the exact position of $e_1$ under the feedback of the agent's actions only (Perhaps the best one can do is binary searching for $e_1$, which does not help either). 
In addition, any $b^t\neq e_1$ yields a suboptimality at least 1 at each round. 
Moreover, we remark that letting $\cV_2$ shrink to a point still makes $S^*$ the optimal scoring rule by the tie-breaking rule which is in favor of the principal.
Nevertheless, one can always make $\cV_2$ an interval around $e_1$ as small as possible.
Hence, we conclude that for any algorithm, there always exists a hard instance that yield a linear regret $\text{Reg}\ge \Omega(T)$.
We finish the proof of \Cref{lem:impossible}.

\subsection{Proof of \Cref{exp:random sampling}}\label{sec:random sampling proof}

To ensure the scoring rules we sample are $\epsilon$ better on the induced action, we sample from the strongly proper scoring rules.

\begin{definition}[Strongly proper scoring rules]\label{def:strongly proper}
    A scoring rule $S:\Delta(\Omega)\times\Omega\to\RR $ is $\beta$-strongly proper if for all  $p, q\in \Delta(\Omega)$, $\EE_{\omega\sim q}[S(q, \omega)]-\EE_{\omega\sim q}[S(p, \omega)]\geq \frac{\beta}{2}\nbr{q-p}^2_1$.
\end{definition}

\begin{example}[Restatement of \Cref{exp:random sampling}]
Let  $d_1=\min_{1\le i< j\le M}\nbr{\sigma_i-\sigma_j}_\infty$ and $ d_2=\min_{1\le k<k'\le K}\allowbreak\max_{i\in[M]}\sbr{q_k(i)-q_{k'}(i)}$.  
Since the proper scoring rule class $\cS$ has bounded norm, the $\beta$-strongly proper scoring class also has bounded volume $\vol(\cS_\beta)<\infty$. 
Set $\kappa={d_1^2\beta}/{2}$ and
let $\tilde\cV_k=\{S\in\cS_\beta\given g(k, S)\ge g(k', S)+\kappa, \forall k'\neq k, k'\in[K]\}$. We  suppose $\vol(\tilde\cV_k)\ge \eta \Vol(\cS_\beta)$ for $k\in[K]$. 
Initiate $\cM=\emptyset$ as the candidate set.
Each time, we randomly sample a $\beta$-strongly proper scoring rule $S\in\cS_\beta$ and obtain the agent's best response $k^*(S)$ with respect to $S$. 
Let $e_i(\sigma, \omega)=\ind(\sigma=\sigma_i)$ for $i\in[M]$. By setting $\kappa=d_1^2\cdot\beta/2$, we deploy $\cbr{S-\kappa e_i}_{i\in[M]}$ and see if the agent still responds $k^*(S)$. If so, Let $\sS=\sS\cup \cbr{S}$; if not, reject $S$. 
    After $\cO(M\eta^{-1} K\log K)$ rounds, with high probability, $\sS$  serves as a valid action-informed oracle  with parameter $\epsilon=d_2\cdot \kappa$.
\end{example}

%Note that the two constraints in the definition of $\tilde\cV_k$: 1) $g(k, S)\ge g(k', S)+\varepsilon$; 2) $S \text{ is $\beta$-strongly proper}$. Increasing $\beta$ in the first constraint reduces the volumn of $\tilde\cV_k$


Note that the volumn of $\tilde\cV_k$ is continuous,  decreasing in $\beta$, and goes to $\vol(\tilde\cV_k)$ as $\beta\to 0$. Hence, if $\cV_k$ has a constant fraction of total volumn, we can find $\beta$ such that $\tilde \cV_k$ has a constant fraction of total volumn.



\begin{proof}

First, if $\kappa=d_1^2\cdot\beta/2$, $\cbr{S-\epsilon e_i}_{i\in[M]}$ is also proper since $S$ is $\beta$-strongly proper.

Next, let $d_2=\min_{k'}\max_{i}\sbr{q_k(i)-q_{k'}(i)}$. we show that any each time the probability of hitting a scoring rule in $\tilde\cV_k$ is at least $\eta$. Suppose $S$ is not rejected if for all $i\in [M]$, the agent prefers action $k$ to $k'$: $g(k, S-\kappa e_i)-g(k', S-\kappa e_i)\geq 0$. By properness of $S-\kappa e_i$, 
\begin{align*}
    g(k, S-\kappa e_i)-g(k', S-\kappa e_i)=\langle S-\kappa e_i, q_k-q_{k'}\rangle-(c_k-c_{k'})\\
    =\langle S, q_k-q_{k'}\rangle - \kappa (q_k(i)-q_{k'}(i))-(c_k-c_{k'})\geq 0.
\end{align*}
If this holds for any $i\in [M]$ and $k'\in[K]$, it means $\langle S, q_k-q_{k'}\rangle-(c_k-c_{k'})\geq  \max_{i\in[M]}\kappa (q_k(i)-q_{k'}(i))\geq\kappa d_2$. One the other hand, if $S$ is in set $\tilde \cV_k$, it never gets rejected. 
% We show this by comparing 
% \begin{align*}
%     g(k, S-\varepsilon e_i)-g(k', S-\varepsilon e_i)=\langle S-\varepsilon e_i, q_k-q_{k'}\rangle-(c_k-c_{k'})\\
%     =\langle S, q_k-q_{k'}\rangle - \varepsilon (q_k(i)-q_{k'}(i))-(c_k-c_{k'})\geq 0.
% \end{align*}%there exists some $i, k'$ such that $\langle S, q_k-q_{k'}\rangle\leq \varepsilon (\Pr_k[\sigma_i]-\Pr_{k'}[\sigma_i])\leq \varepsilon $. %Hence, any scoring rule $S\in \tilde\cV_k$ satisfying $\langle S, q_k-q_{k'}\rangle > \varepsilon $ for all $k'\in [K]$ cannot get rejected.

  After $\frac{c}{\eta}K\log K$ rounds, the probability that one action is not induced is $(1-\eta)^{\frac{c}{\eta}K\log K}=(\frac{1}{K})^{cK}$. Taking a union bound, the probability that any action is not induced is at most $(\frac{1}{K})^{cK-1}$. Setting $c$ large enough, this probability will be sufficiently small. To conclude, after $\cO(M\eta^{-1} K\log K)$ rounds, we get an oracle with parameter $\epsilon=d_2\cdot \kappa$.

    
\end{proof}



\subsection{Proof of \Cref{exp:linear contract}}
\label{sec:linear contract proof}

\begin{example}[Restatement of \Cref{exp:linear contract}]
Suppose that these $K$ signals are sorted in the increasing order of the cost. Define $u_k = \EE_{\omega\sim\sigma, \sigma\sim q_k}\sbr{u(a^*(\sigma), \omega)}$ and suppose $u_k>0$. We assume the marginal information gain is strictly decaying, i.e., there exists a $\epsilon>0$ such that 
    \begin{gather*}
        \frac{u_{K}-u_{K-1}}{c_{K}-c_{K-1}} >\epsilon, \quad \text{and}\\
        \quad
        \frac{u_k-u_{k-1}}{c_k-c_{k-1}} - \frac{u_{k+1}-u_k}{c_{k+1}-c_k} >\epsilon, \quad \forall k=2,\cdots, K-1.
    \end{gather*}
    Moreover, we assume that $(u_2-u_1)/(c_2-c_1)\le b$. The principal sets the scoring rule as $S(\sigma, \omega)=\lambda u(a^*(\sigma), \omega)$ and conducts binary search on $\lambda\in[0, 2/\epsilon]$. Specifically, the binary searches are iteratively conducted on all the segments $(\lambda_1, \lambda_2)$ with $k^*(\lambda_1 u) \neq k^*(\lambda_2 u)$, where $\lambda_1, \lambda_2$ are neighboring points on $[0, 1/\epsilon]$ that are previously searched. With the maximal searching depth $m= \ceil{\log_2(2b(b-\epsilon)/\epsilon^2)}$, we can identify all the agent's actions. Suppose that $(\lambda_1^k, \lambda_2^k)$ is the largest segment with $\lambda_1^k$ and $\lambda_2^k$ searched before and $k^*(\lambda_1^k u)=k^*(\lambda_2^k u)=k$. By letting $\tilde S_k=(\lambda_1^k + \lambda_2^k)u/2$, we obtain an oracle with $\varepsilon=\epsilon u_1/4b^2$. The procedure takes $\cO(K\log_2(\varepsilon^{-1}))$ rounds.
\end{example}

\begin{proof}
    
First, we show that by setting $\lambda=\lambda_i=\rbr{c_{i}-c_{i-1}}/\rbr{u_i-u_{i-1}}$ for any $i=2,\dots,K$, the agent is indifferent between taking actions $i$ and $i-1$.
In addition, we let $\lambda_1=0$ and $\lambda_{K+1}=2/\epsilon$.
% Let $\lambda_0=0$.
For any $\lambda\in (\lambda_i, \lambda_{i+1})$, the agent best responds with action $b_i$. In addition, $\lambda_2=(c_2-c_1)/(u_2-u_1)\ge 1/b$ and $\lambda_K=(c_K-c_{K-1})/(u_K-u_{K-1})\le 1/\epsilon$. Thus, by searching in $\lambda\in[0, 2/\epsilon]$, all the actions are guaranteed to be induced by this linear contract.

Since the sequence $\rbr{u_i-u_{i-1}}/\rbr{c_i-c_{i-1}}$ is strictly decaying, $\lambda_i$ is strictly increasing and $\lambda_i-\lambda_{i-1}$ is lower bounded by ${1}/\rbr{b-\epsilon}-{1}/{b}={\epsilon}/{b(b-\epsilon)}$, where the lower bound is reached by $\lambda_2=1/b$ and $\lambda_3=1/(b-\epsilon)$. Combined with the fact that search is conducted on $[0, 2/\epsilon]$, we get the maximal searching depth $m= \ceil{\log_2(2b(b-\epsilon)/\epsilon^2)}$ such that the neighboring searching points has a gap no more than $\epsilon/2b(b-\epsilon)$. Therefore, using the fact that $\lambda_i-\lambda_{i-1}\ge \epsilon/b(b-\epsilon)$, we can guarantee that all the actions are induced by this binary search.

For the chosen $\lambda_1^k$ and $\lambda_2^k$ such that $k^*(\lambda_1^k u)=k^*(\lambda_2^k u)=k$, we can guarantee that $\lambda_{k}\le\lambda_1^k\le \lambda_{k} +\epsilon/2b(b-\epsilon)$ and $\lambda_{k+1}-\epsilon/2b(b-\epsilon)\le\lambda_2^k\le \lambda_{k+1}$. Hence, $\lambda_k+\epsilon/4b(b-\epsilon)\le (\lambda_1^k+\lambda_2^2)/2\le \lambda_{k+1}-\epsilon/4b(b-\epsilon)$.
Therefore, by setting $\lambda=(\lambda_1^k + \lambda_2^k)/2$, the marginal profit for choosing $b_k$ over other actions is lower bounded by $\epsilon/4b(b-\epsilon)\cdot u_k\ge \epsilon/4b(b-\epsilon)\cdot u_1\ge \epsilon u_1/4b^2$.
Hence, we obtain the oracle with $\varepsilon=\epsilon u_1/4b^2$.


\end{proof}




\section{Proof of \Cref{thm:regret}}\label{prof: main theorem}
\begin{proof}[Proof of \Cref{thm:regret}]
Here we give a high-level analysis of the $\tilde{\cO}(K^2 T^{-2/3})$ regret for Algorithm~\ref{alg:UCB}. We consider two types of events in $T$ rounds, $\ind(k_t=k_t^*)$ or $\ind(k_t\neq k_t^*)$. In the first type of events, by the construction of $S_t$, the suboptimality gap at each round can be decoupled into components contributed by $(1-\alpha_t)S_t^*$ or $\alpha_t \tilde S_{k^*_t}$. The cumulative regret from $(1-\alpha_t)S_t^*$ follows the optimism principle of UCB and is on the order of $T^{1/2}$. The cumulative regret from $\alpha_t \tilde S_{k^*_t}$ is on the order of $T^{2/3}$, since $\alpha_t = t^{-1/3}$ and the suboptimality gap of $S_{k^*_t}$ is constant. Meanwhile, for the second type of events, we can bound the total number of rounds such event occurs in the worst case as $\tilde{\cO}(K^2 T^{2/3})$. This requires some involved arguments, though it is intuitively the sample complexity to learn $\norm{\hat{q}_k - q_k}\leq t^{-1/3}$ for every $k\in[K]$. This corresponds to the worst case where these signals are competitive and $n_{k, t}$ grows at the same rate for any $k\in[K]$, which means that we have to precisely learn the belief distributions and costs for all the actions.
Therefore, based on the dominating term, the total regret is $\tilde{\cO}(K T^{2/3})$.

We analyze the regret by the following two cases: (i) rounds $t$ such that the agent responds with action $k_t=k_t^*$; (ii) rounds $t$ such that the agent responds with action $k_t\neq k_t^*$. Let $\subopt(k, S_t)=\max_{S\in\cS} h(S)-\EE_{\omega\sim\sigma, \sigma\sim q_k}[u(a^*(\sigma_t), \omega_t) - S_t(\sigma_t, \omega_t)]$ be the regret of implementing scoring rule $S_t$ with the agent selecting action $b_k$ in a single round. We have for the online regret that,
\begin{align}
    \Reg^\pi(T)&=\EE_\pi\sbr{\sum_{t=1}^T \rbr{\max_{S\in\cS} h(S) - \EE_{\omega_t\sim\sigma_t, \sigma_t\sim q_{k_t}}\sbr{u(a^*(\sigma_t), \omega_t) - S_t(\sigma_t, \omega_t)}}} \nonumber\\
&=\sum_{t=1}^T\underbrace{\EE\sbr{\ind(k_t=k_t^*)\subopt(k_t, S_t)}}_{\displaystyle{A_t}}+\sum_{t=1}^T\underbrace{\EE\sbr{\ind(k_t\neq k_t^*)\subopt(k_t, S_t)}}_{\displaystyle{B_t}}. \nonumber
\end{align}
Here and in the sequel, we always use $\EE$ to denote the expectation taken with respect to the data generating process described in \eqref{eq:data generating}.

\paragraph{Bounding regret $A_t$.}
Since $S_t=\alpha_t \Tilde{S}_{k_t^*}+(1-\alpha_t)S_t^*$, it follows from the linearity of $\subopt(k_t, S_t)$ that
\begin{align}
    \subopt(k_t, S_t)=\alpha_t\subopt(k_t, \Tilde{S}_{k_t^*})+(1-\alpha_t)\subopt(k_t, S_t^*).\label{eq:subopt decomp}
\end{align}
Here, the first term $\subopt(k_t, \Tilde{S}_{k_t^*})$ is bounded by constant $C_1=2(B_S+B_u)$, and the second term $\subopt(k_t, S_t^*)$ is bounded by $2\left(\nbr{S_t^*}_\infty+B_u\right) I_{q, k_t^*}^t$ with probability at least $1-1/t$ by the following lemma.
\begin{lemma}\label{lem:linear-program-bound}
Define event $\cE_t$ as $
    \cE_t=\cbr{\nbr{q_k-\hat q_k^t}_1\le I_q^t(k), \forall k\in[K]^+}.
$
Then event $\cE_t$ holds with probability at least $1-1/t$ and it holds on event $\cE_t$ that
\begin{align*}
    u_{k_t^*} - v_{S_t^*}(k_t^*) + 2\left(B_S+B_u\right) I_{q}^t(k_t^*)\ge \max_{S\in\cS} h(S),
\end{align*}
where $h(S)=\EE_{\omega\sim \sigma, \sigma\sim q_{k^*(S)}}\sbr{u(a^*(\sigma), \omega) - S(\sigma, \omega)}$ is the optimal principal's profit, $k_t^* = \argmax_{k\in[K]} h_\LP^t(k)$,  and
$S_t^* := S_{\LP, k_t^*}^t$.
\begin{proof}
    See \S\ref{prof:linear-program-bound} for a detailed proof.
\end{proof}
\end{lemma}
The first term on the right-hand side of \eqref{eq:subopt decomp} can be directly bounded by $\alpha_t C_1$. For the second term on event $\cE_t$, it can be bounded by $C_1 I_q^t(k_t^*)$ using \Cref{lem:linear-program-bound}.
If event $\cE_t$ does not hold, the second term is still bounded by $C_1$ with probability at most $1/t$. Thus, we have for $\alpha_t=K t^{-1/3}\land 1$ that 
\begin{align}
    \sum_{t=1}^T A_t&\leq \sum_{t=1}^T C_1 \rbr{\alpha_t+\frac 1 t+I_{q}^t(k_t^*)}\nonumber\\
    &\leq C_1 \rbr{ K \rbr{\frac 3 2T^{2/3}+1}+(\log T+1)+\sum_{k\in[K]^+}\sum_{t:k_t^*=k} \sqrt{\frac{2\log(K\cdot 2^M t)}{n_{k}^t}}}\nonumber\\
    &\leq C_1\rbr{K \rbr{\frac 3 2T^{2/3}+1}+\log T+1}+C_1\sqrt{2\log(K\cdot 2^M T)}\cdot \sum_{k\in[K]^+}\rbr{2\sqrt{n_k^T}+1} \nonumber\\
    &\leq \cO(T^{2/3})+\cO(\sqrt{|M|+\log(KT)}\cdot\sqrt{KT}), 
\end{align}
where the first equality follows from our previous discussions on \eqref{eq:subopt decomp}, the second inequality holds from the definition of $I_q^t$, and the last inequality holds by the Jensen's inequality that $\sum_{k\in[K]^+}\sqrt{n_k^T}/K\le \sqrt{\sum_{k\in[K]^+}n_k^T/K}$.

\paragraph{Bounding regret $B_t$.} 
We characterize regret $\sum_{t=1}^T B_t$ by bounding the expected number of rounds that $k_t\neq k_t^*$.
To do so, we invoke the following lemma.
\begin{lemma}\label{lem:mistake}
For any fixed $i\neq k_t^*$, under the condition that
\begin{align*}
    \alpha_t\ge 2\varepsilon^{-1} \rbr{I_{c}^{t}(k_t^*, i)+B_S \rbr{I_{q}^t(i)+I_{q}^t(k_t^*)}}:=\Delta_t(k_t^*,i), 
\end{align*}
the agent must respond $k_t\neq i$ under the scoring rule $S_t=\alpha_t \Tilde{S}_{k_t^*}+(1-\alpha_t)S_t^*$ on event $\cE_t$.
\begin{proof}
    See \S\ref{prof:mistake} for a detailed proof.
\end{proof}
\end{lemma}
Under the condition $k_t\neq k_t^*$, the algorithm must be doing binary search.
Now, we introduce the notations that will be used in bounding $B_t$.
Compare to the definition in \Cref{alg:BS}, we instead define $\BS(S_0, S_1, \tilde \lambda_{\min}, \tilde\lambda_{\max}, \tau_0, \tau_1, k_0, k_1, t_0, t_1)$ for a binary search (BS), where $(S_0, S_1)$ is the initial segment that this BS is conducted on, $(\tilde \lambda_{\min}, \tilde\lambda_{\max})$ corresponds to the value of $(\lambda_{\min}, \lambda_{\max})$ that this BS ends with,  $\tau_0, \tau_1$ correspond to the rounds in which $(1-\tilde\lambda_{\min}) S_0 + \tilde\lambda_{\min} S_1$ and $(1-\tilde\lambda_{\max}) S_0 + \tilde\lambda_{\max} S_1$ are played, respectively, $k_0, k_1$ are the best response under $(1-\tilde\lambda_{\min}) S_0 + \tilde\lambda_{\min} S_1$ and $(1-\tilde\lambda_{\max}) S_0 + \tilde\lambda_{\max} S_1$, and $t_0, t_1$ are the starting round and the ending round of this BS.
Notably, $k_0$ is not necessarily the best response under $S_0$ but $k_1$ is guaranteed to be the best response under both $S_1$ and $(1-\tilde\lambda_{\max}) S_0 + \tilde\lambda_{\max} S_1$.


For the BS that lasts until round $t$, we let $t_0(t)$ be the first round of the BS (if round $t$ is not doing BS, we just let $t_0(t)=t$), $(k_0(t), k_1(t))$ be the best response that this BS ends with, $(S_0(t), S_1(t))$ be the segment that this BS searches on.
We consider the following two situations for the case $k_t\neq k_t^*$: (i) $\Delta_{t_0 (t)}(k_0(t), k_1(t))\le \alpha_{t_0(t)}$;
(ii) $\Delta_{t_0 (t)}(k_0(t), k_1(t))> \alpha_{t_0(t)}$.
Following such an idea, we have
% \begin{align*}
%     \sum_{t=1}^T B_t&= \sum_{t=1}^T \EE\sbr{\ind(k_t\neq k_t^*)\subopt(k_t, S_t)}\\
%     &\le C_1 \sum_{t=1}^T \EE\sbr{\ind(\BS=1, )}
% \end{align*}

    \begin{align*}
        \sum_{t=1}^T B_t&= \sum_{t=1}^T \EE\sbr{\ind(k_t\neq k_t^*)\subopt(k_t, S_t)}\\
        &\le C_1\left[\sum_{t=1}^T\EE\sbr{\ind(\BS=1, \Delta_{t_0 (t)}(k_0(t), k_1(t))\le  \alpha_{t_0(t)})}+\sum_{t=1}^T\EE\sbr{\ind(\BS=1, \Delta_{t_0 (t)}(k_0(t), k_1(t))> \alpha_{t_0(t)})}\right]\\
        &\leq C_1\bigg(\underbrace{\sum_{t=1}^T\frac{1}{t_0(t)}}_{\displaystyle{(B.1)}}+\underbrace{\sum_{t=1}^T\EE\sbr{\ind(\BS=1, \Delta_{t_0 (t)}(k_0(t), k_1(t))> \alpha_{t_0(t)})}}_{\displaystyle{(B.2)}} \bigg),
    \end{align*}
where the first inequality holds by $\subopt(k_t, S_t)\le C_1$, the second inequality holds by \Cref{lem:mistake}.
Specifically, when doing a binary search, the deployed scoring rule $S_t$ must lie on the segment $(S_0(t), S_1(t))$, where $S_0(t)=\alpha_{t_0(t)}\tilde S_{k^*_{t_0(t)}} + (1-\alpha_{t_0(t)}) S_{t_0(t)}^*$ and $S_1(t)=\tilde S_{k^*_{t_0(t)}}$. Hence, $S_t$ can be equivalently expressed as 
\begin{align*}
    S_0(t)=\alpha_{t}'\tilde S_{k^*_{t_0(t)}} + (1-\alpha_{t}') S_{t_0(t)}^*, 
\end{align*}
where $\alpha_{t}'\ge \alpha_{t_0(t)} $.
Therefore, under the condition that $\Delta_{t_0 (t)}(k_0(t), k_1(t))\le \alpha_{t_0(t)}$ and that the agent responds $k_0(t)\neq k_1(t)$ in this binary search, we can use \Cref{lem:mistake} to bound the probability by $1/t_0(t)$. 

To bound $(B.2)$, we define a concept called \emph{essential binary search}.
\begin{definition}[Essential binary search]
We call a binary search $\BS(S_0, S_1, \tilde \lambda_{\min}, \tilde\lambda_{\max}, \tau_0, \tau_1, k_0, k_1, t_0, t_1)$  an \textit{essential binary search} (essential BS) if $\alpha_{t_0}< \Delta_{t_0}(k_0, k_1)$ where $\alpha_{t}=K t^{-1/3}\land 1$.
\end{definition}
The following lemma bounds the total rounds of essential BSs.
    
    
    
    \begin{lemma}\label{lem:essentialBS}
    Consider binary search $\BS(S_0, S_1, \tilde \lambda_{\min}, \tilde\lambda_{\max}, \tau_0, \tau_1, k_0, k_1, t_0, t_1)$. Suppose that the total number of essential binary searches in $T$ rounds is $N$. Then, we have
    \begin{equation}
        N\leq K\cdot \left(12 \varepsilon^{-1}B_S\right)^{2} 2 \log \left(K 2^{M} T\right) T^{2 / 3}.
    \end{equation}
    \begin{proof}
        See \S\ref{prof:essentialBS} for a detailed proof.
    \end{proof}
    \end{lemma}
Let $m$ be the maximal rounds in a binary search, i.e., $t_0(t)\ge t- m$. 
Using the terminal criteria for binary search, we have
\begin{align*}
    m\le \max_{t, k}-\log_2\rbr{I_q^t(k)}=\cO(\log(T)).
\end{align*}
The term $(B.1)$ is directly bounded by
\begin{align*}
    (B.1)\le \log(T) + m=\cO(\log(T)).
\end{align*}
The term $(B.2)$ is bounded using \Cref{lem:essentialBS}, 
\begin{align*}
    (B.2)\le m\cdot K\cdot \left(12 \varepsilon^{-1}B_S\right)^{2} 2 \log \left(K 2^{M} T\right) T^{2 / 3} = \cO(\varepsilon^{-2}B_S^2(M +\log(KT))mKT^{2/3}).
\end{align*}
% Using the fact that $m\le $, we give the bound on $ \sum_{t=1}^T B_t$:
% \begin{equation}
%      \sum_{t=1}^T B_t\leq C_1\left[\log(T)+1+\left(12 \varepsilon^{-1}B_S\right)^{2} 2 \log \left(K 2^{M} T\right)(K-1) T^{2 / 3}\right],
% \end{equation}
which finishes the proof.


\end{proof}

\subsection{Proof of \Cref{lem:linear-program-bound}}\label{prof:linear-program-bound}
\begin{lemma}[Restatement of \Cref{lem:linear-program-bound}]
Define event $\cE_t$ as $
    \cE_t=\cbr{\nbr{q_k-\hat q_k^t}_1\le I_q^t(k), \forall k\in[K]^+}.
$
Then event $\cE_t$ holds with probability at least $1-1/t$ and it holds on event $\cE_t$ that
\begin{align*}
    \inp[]{q_{k_t^*}}{u-S_t^*}_\Sigma + 2\left(B_S+B_u\right) I_{q}^t(k_t^*)\ge \max_{S\in\cS} h(S),
\end{align*}
where $h(S)=\EE_{\omega\sim \sigma, \sigma\sim q_{k^*(S)}}\sbr{u(a^*(\sigma), \omega) - S(\sigma, \omega)}$, $v_S(k)=\inp[]{q_k}{S}_\Sigma$ and $k^*(S)$ is the agent's best response.
\end{lemma}
\begin{proof}[Proof of \Cref{lem:linear-program-bound}]
We first state the concentration result from \citet{mardia2020concentration}.
\begin{lemma}[\citet{mardia2020concentration}, Lemma 1, Concentration for empirical distribution]\label{lem:confidence-interval}
Let $p$ be a probability distribution supported in a finite set with cardinality at most $M$ and $p_n$ be the empirical distribution of $n$ i.i.d. samples from $p$. Then, for all sample size $n\in\NN_+$ and $0<\delta<1$, 
\begin{align*}
    \PP\rbr{\nbr{p-p_n}_1\ge \sqrt{\frac{2\log\rbr{2^M/\delta}}{n}}}\le \delta.
\end{align*}
\end{lemma}
By \Cref{lem:confidence-interval} and taking a union bound, with probability at least $1-1/t$, we have $\nbr{\hat q_k^t-q_k}_1\le I_q^t(k)$ for all $k\in[K]^+$, which gives event $\cE_t$.
In the sequel, we discuss under the condition that event $\cE_t$ holds.
We can verify for $v_S(k)=\inp[]{q_k}{S}_\Sigma$ that
\begin{equation}\label{eq:v_S error}
    \abr{\hat v_S^t(k)- v_S(k)}=\abr{\inp[]{\hat q_k^t-q_k}{S}_\Sigma }\le B_S I_q^t(k).
\end{equation}
To study the confidence interval for $\hat C^t(i,j)$, we invoke the following lemma. 
\begin{lemma}\label{lem:C interval}
    Under event $
    \cE_t=\cbr{\nbr{q_k-\hat q_k^t}_1\le I_q^t(k), \forall k\in[K]^+}
$, we have
\begin{align*}
    \abr{\hat C^t(i, j)- C(i,j)} \le I_{c}^t(i, j), \quad \forall (i, j)\in[K].
\end{align*}
\begin{proof}
    A direct corollary of \eqref{eq:v_S error} is
\begin{equation}
\begin{aligned}\label{eq:C error-1}
    C_+^t(i, j) \ge \min_{\tau<t:k_\tau=i} v_{S_\tau}(i) - v_{S_\tau}(j) \ge C(i, j), \\
    C_-^t(i, j) \le \max_{\tau<t:k_\tau=j} v_{S_\tau}(i) - v_{S_\tau}(j) \le C(i, j).
\end{aligned}
\end{equation}
Using the fact in \eqref{eq:C error-1}, we have
\begin{align}\label{eq:C error-2}
    \abr{\hat C^t(i, j)- C(i,j)} 
    &= \abr{\sum_{(i', j')\in l_{ij}} \frac{C_+^t(i', j') + C_-^t(i', j')}{2}-C(i', j')}\nend
    &\le \frac 1 2 \sum_{(i', j')\in l_{ij}} \rbr{\abr{C_+^t(i', j')- C(i', j')} + \abr{C_-^t(i', j')- C(i', j')}}\nend
    &= \frac 1 2 \sum_{(i', j')\in l_{ij}} \abr{C_+^t(i', j')-C_-^t(i', j')} = I_c^t(i, j), 
\end{align}
where the inequality holds by triangular inequality, the second equality holds by \eqref{eq:C error-1}, and the last equality holds by definition of $I_C^t$.
\end{proof}
\end{lemma}

let $k^*$ and $S^*$ be the best response and the optimal scoring rule that achieves the optimal objective $\sup_{S\in\cS} h(S)$ in \eqref{eq:stackelberg-2}, respectively.
Recall the linear programming $\OptLP_k$, 
\begin{equation}
\begin{aligned}
    \OptLP_k:\qquad &\max_{S\in\cS} \quad \inp[]{u}{\hat q_k^t}_\Sigma + B_u I_q^t(k) -v, \nend
    &\mathrm{s.t.}\quad \abr{v-\hat v_{S}^t(k)} \le B_S \cdot I_{q}^t(k), \\
    &\qquad \quad v - \hat v_{S}^t(i) \ge \hat C^t(k, i) - \rbr{I_{c}^{t}(k, i) + B_S\cdot I_{q}^t(i)}, \quad \forall i\neq k.
\end{aligned}
\end{equation}
It is easy to verify that $S^*$ satisfies all the constraint in $\OptLP_{k^*}$ using \eqref{eq:v_S error} and \eqref{eq:C error-2} with $v=v_{S^*}(k^*)$ and $k=k^*$.
Also, $\inp[]{u}{\hat q_k^t}_\Sigma + B_u I_q^t(k)\ge \inp[]{u}{q_k}_\Sigma$ for $\forall k\in[K]^+$.
Therefore, by noting that $k_t^* = \argmax_{k\in[K]^+} h_\LP^t(k)$, we conclude that $h_\LP^t(k_t^*)\ge h_\LP^t(k^*)\ge h(S^*)$.
% \begin{align*}
%     \max_{S\in \cS}h(S^*)&=\EE_{\omega\sim \sigma, \sigma\sim q_{k^*}}\sbr{u(a^*(\sigma), \omega) - S^*(\sigma, \omega)}\\
%     &\leq \EE_{\omega\sim \sigma, \sigma\sim \hat{q}_{k^*}}\sbr{u(a^*(\sigma), \omega) }+B_u I^t_q(k^*)-v^{t}_{\LP}(k^*), 
% \end{align*}
Also, let $v_\LP^*$ be the solution to $\OptLP_{k_t^*}$ and recall that $S_t^*=S_{\LP, k_t^*}^{t}$ we have
\begin{align*}
    h_\LP^t(k_t^*)&=\inp[]{u}{\hat q_{k_t^*}^t}_\Sigma + B_u I_q^t(k_t^*) -v_\LP^*\nend
    &\le (B_u+B_S) I^t_q(k^*)+\EE_{\omega\sim \sigma, \sigma\sim \hat{q}_{k^*_t}}\sbr{u(a^*(\sigma), \omega) - S^*_t(\sigma, \omega)}\\
    &\le 2(B_u +B_S )I^t_q(k^*) +\EE_{\omega\sim \sigma, \sigma\sim q_{k^*_t}}\sbr{u(a^*(\sigma), \omega)-S_t^*(\sigma, \omega)}, 
\end{align*}
where the first inequality holds by noting that $\abr{v_\LP^*-\hat v_{S_t^*}(k_t^*)}\le B_S I_{q}^t(k_t^*)$, and the second inequality holds by definition of event $\cE_t$.
Thus, we finish the proof. 
\end{proof}

\subsection{Proof of \Cref{lem:mistake}}\label{prof:mistake}
\begin{lemma}[Restatement of \Cref{lem:mistake}]
For $\forall i\neq k_t^*$, under the condition that
\begin{align*}
    \alpha_t\ge 2\varepsilon^{-1} \rbr{I_{c}^{t}(k_t^*, i)+B_S \rbr{I_{q}^t(i)+I_{q}^t(k_t^*)}}:=\Delta_t(k_t^*,i), 
\end{align*}
the agent will not respond $k_t=i$ under the scoring rule $S_t=\alpha_t \Tilde{S}_{k_t^*}+(1-\alpha_t)S_t^*$ on event $\cE_t$.
\end{lemma}
\begin{proof}[Proof of \Cref{lem:mistake}]
On event $\cE_t$,  for any $i\neq k_t^*$, if $\alpha_t\ge \Delta_t(k_t^*,i)$, we aim to show the agent prefers action $b_{k_t^*}$ to action $b_i$. Recall $S_t=\alpha_t \Tilde{S}_{k_t^*}+(1-\alpha_t)S_t^*$. The expected profit for the agent generated by responding action $b_i$ is
\begin{equation*}
    v_{S_t}(i)-c_i%&\leq\EE_{\sigma\sim \hat{q}_{i}}\sbr{S_t(\sigma)}-c_i+B_SI_q^t(i)\\
    \leq\alpha_tv_{\Tilde{S}_{k_t^*}}(i)+(1-\alpha_t)\hat{v}^t_{S_t^*}(i)-c_i+(1-\alpha_t)B_SI_q^t(i),%\\
    %&\leq \alpha_t (v_{\Tilde{S}_{k_t^*}}-c_k-\frac{\epsilon\varepsilon_c}{2(1+\epsilon)})+(1-\alpha_t)
\end{equation*}
and the expected utility generated by responding action $b_{k_t^*}$ is
\begin{equation*}
     v_{S_t}(k_t^*)-c_{k_t^*}%&\leq\EE_{\sigma\sim \hat{q}_{i}}\sbr{S_t(\sigma)}-c_i+B_SI_q^t(i)\\
    \geq\alpha_tv_{\Tilde{S}_{k_t^*}}(k_t^*)+(1-\alpha_t)\hat{v}^t_{S_t^*}(k_t^*)-c_{k_t^*}-(1-\alpha_t)B_SI_q^t(k_t^*).
\end{equation*}
By \Cref{asp:oracle}, we already have $(v_{\Tilde{S}_{k_t^*}}(k_t^*)-c_{k_t^*})-(v_{\Tilde{S}_{k_t^*}}(i)-c_{i})>\varepsilon$. 
Since $S_{t}^*$ is the solution to $\OptLP_{k_t^*}$, following the constraints of $\OptLP_{k_t^*}$ we have $$\hat{v}^t_{S_t^*}(k_t^*)-\hat{v}^t_{S_t^*}(i)\geq \hat{C}^t(k_t^*, i)-I^t_c(k_t^*, i)-B_S (I^t_q(i)+I^t_q(k_t^*)).$$ 
Combining the above inequalities, we get
\begin{align*}
     \sbr{v_{S_t}(k_t^*)-c_{k_t^*}}-\sbr{ v_{S_t}(i)-c_i}
     &\ge \alpha_t\cdot \varepsilon +(1-\alpha_t)\rbr{\hat{C}^t(k_t^*, i)-C(k_t^*, i)-I^t_c(k_t^*, i)-B_S (I^t_q(i)+I^t_q(k_t^*))}\nend
     &\qquad - (1-\alpha_t) B_S\rbr{I_q^t(k_t^*)+I_q^t(i)}\nend
     &\geq \alpha_t\cdot\varepsilon+2(1-\alpha_t)\sbr{-I^t_c(k_t^*, i)-B_S (I^t_q(i)+I^t_q(k_t^*))}\\
     &>\alpha_t \cdot \varepsilon -2\sbr{I^t_c(k_t^*, i)+B_S (I^t_q(i)+I^t_q(k_t^*))}, 
\end{align*}
where the second inequality holds by recalling that we have proved $C(k_t^*,i)\leq \hat{C}^t(k_t^*, i)+I^t_c(k_t^*, i)$ in \eqref{eq:C error-2} under $\cE_t$, and the last inequality holds by noting that $0<\alpha_t\le 1$.
When $\alpha_{t} \geq 2\varepsilon^{-1}\left(I_{c}^{t}\left(k_{t}^{*}, i\right)+B_S\left(I_{q}^{t}(i)+I_{q}^{t}(k_{t}^{*})\right)\right)$, the above $\sbr{v_{S_t}(k_t^*)-c_{k_t^*}}-\sbr{ v_{S_t}(i)-c_i}>0$, which means the agent prefers action $k_t^*$ to $i$.

    \end{proof}


\subsection{Proof of \Cref{lem:essentialBS}}\label{prof:essentialBS}
In the sequel, 
\Cref{lem:confidence-after-binary-search} shows essential BS reduces confidence interval, while \Cref{lem:no-more-search} states the stopping criteria of essential BS. Combining \Cref{lem:confidence-after-binary-search} and \Cref{lem:no-more-search}, we can bound the total rounds in essential BS.


\begin{proposition}
\label{lem:confidence-after-binary-search}
    For a binary search $\BS(S_0, S_1, \tilde \lambda_{\min}, \tilde\lambda_{\max}, \tau_0, \tau_1, k_0, k_1, t_0, t_1)$, we have
    \begin{equation}
        I_c^t(k_0, k_1)\leq 2\sqrt{2\log(K\cdot 2^M T)}B_S\rbr{\frac{1}{\sqrt{n_{k_0}^{{t_1}}}}+\frac{1}{\sqrt{n_{k_1}^{{t_1}}}}}.
    \end{equation}
\end{proposition}

\begin{proof}
[Proof of \Cref{lem:confidence-after-binary-search}]
By definition of $I_c^t(k_0, k_1)$, for any $t> {t_1}$,
\begin{align*}
    I_c^t(k_0, k_1) = \sum_{(i', j')\in l_{k_0k_1}} \varphi^t(i', j')\le \varphi^t(k_0, k_1),
\end{align*}
where the inequality holds by noting that $l_{k_0k_1}$ is the \say{shortest path} with $\varphi^t$ being the \say{length} of each edge. Using the definition of $\varphi^t$, we conclude that
\begin{align*}
    I_c^t(k_0, k_1)&\le \frac{1}{2}\sbr{C_+^t(k_0, k_1)-C_-^t(k_0, k_1)}_+\\
    &=\frac{1}{2}\sbr{\min_{\tau'<t:k_{\tau'}=k_0}\rbr{\hat{v}^t_{S_{\tau'}}(k_0)-\hat{v}^t_{S_{\tau'}}(k_1)}-\max_{\tau'<t:k_{\tau'}=k_0}\rbr{\hat{v}^t_{S_{\tau'}}(k_0)-\hat{v}^t_{S_{\tau'}}(k_1)}}_+\\
    &\qquad\qquad+B_S\left(I_{q}^{t}(k_0)+I_{q}^{t}(k_1)\right)\\
    &\leq \frac{1}{2}\sbr{\hat{v}^t_{S_{\tau_0}}(k_0)-\hat{v}^t_{S_{\tau_0}}(k_1)-\hat{v}^t_{S_{\tau_1}}(k_0)+\hat{v}^t_{S_{\tau_1}}(k_1)}_+ 
    +B_S\left(I_{q}^{t}(k_0)+I_{q}^{t}(k_1)\right), 
\end{align*}
where the last inequality holds by noting that $t>{t_1}\ge (\tau_0, \tau_1)$. Consequently, we have by the definition of $\hat v_S^t$ that,
\begin{align*}
    I_c^t(k_0, k_1)&\le\frac{1}{2}\abr{\langle S_{\tau_0}- S_{\tau_1}, \hat{q}^t_{k_0}-\hat{q}^t_{k_1}\rangle_\Sigma}+B_S\left(I_{q}^{t}(k_0)+I_{q}^{t}(k_1)\right)\\
    &=\frac{(\tilde\lambda_{\max}-\tilde\lambda_{\min})}{2}\abr{\langle  S_1-S_0, \hat{q}^t_{k_0}-\hat{q}^t_{k_1}\rangle_\Sigma}+B_S\left(I_{q}^{t}(k_0)+I_{q}^{t}(k_1)\right)\\
    &\leq \frac{\min\rbr{I^{t_1}_q(k_0), I^{t_1}_q(k_1)}}{2} \rbr{\abr{\inp[]{S_1-S_0}{\hat q_{k_0}^t}_\Sigma}+\abr{\inp[]{S_1-S_0}{\hat q_{k_1}^t}_\Sigma}}+B_S\left(I_{q}^{t}(k_0)+I_{q}^{t}(k_1)\right), 
\end{align*}
where $\tilde\lambda_{\max}$ and $\tilde\lambda_{\min}$ are the values of $\lambda_{\max}$ and $\lambda_{\min}$ when this BS terminates, respectively. Here, the last inequality holds by the terminal criteria of the BS. Furthermore, we can upper bound the right-hand side by
\begin{align*}
    I_c^t(k_0, k_1)
    &\le \min\rbr{I^{t_1}_q(k_0), I^{t_1}_q(k_1)} B_S+B_S\left(I_{q}^{t}(k_0)+I_{q}^{t}(k_1)\right)\nend
    &\leq \sbr{\sqrt{2\log(K\cdot 2^M T)}\cdot \rbr{\frac{1}{\sqrt{n_{k_0}^{t_1}}}+\frac{1}{\sqrt{n_{k_1}^{t_1}}}+\frac{1}{\sqrt{\max\cbr{n_{k_0}^{t_1}, n_{k_1}^{t_1}}}}}}B_S\\
    &\leq 2B_S\sbr{\sqrt{2\log(K\cdot 2^M T)}\cdot \rbr{\frac{1}{\sqrt{n_{k_0}^{t_1}}}+\frac{1}{\sqrt{n_{k_1}^{t_1}}}}}, 
\end{align*}
where the first inequality holds by noting that $\hat q_{k}^t$ is an empirical probability and the fact that $\nbr{S_1-S_0}_\infty\le B_S$ since $S$ is nonnegative, and the second inequality holds by noting that $n_k^t$ is nondecreasing and ${t_1}<t$.
    
\end{proof}

\begin{proposition}
    \label{lem:no-more-search}
    For a binary search $\BS(S_0, S_1, \tilde \lambda_{\min}, \tilde\lambda_{\max}, \tau_0, \tau_1, k_0, k_1, t_0, t_1)$, if
    \begin{equation}
        \min\{n_{k_0}^{{t_1}}, n_{k_1}^{{t_1}}\}\geq \left(12 \varepsilon^{-1}B_S\right)^{2} 2 \log \left(K 2^{M} T\right)T^{2 / 3}\eqdef N^*,
    \end{equation}
    then we have $t^{-1/3}\ge \Delta_t(k_0, k_1)$ for any $t_1<t\le T$ and 
    no more essential {BS} ending with the same $(k_0, k_1)$ is possible.
\end{proposition}
\begin{proof}
    [Proof of \Cref{lem:no-more-search}]
    For any ${t_1}<t\leq T$, we have
    \begin{align*}
        t^{-1/3}\ge T^{-1/3}&=\left(12 \varepsilon^{-1}B_S\right)\sqrt{\frac{ 2 \log \left(K 2^{M} T\right)}{N^*}}\\
        &\geq 2\left(\varepsilon^{-1}B_S\right)\sqrt{ 2 \log \left(K 2^{M} T\right)}\rbr{2\frac{1}{\sqrt{n^{t_1}_{k_0}}}+2\frac{1}{\sqrt{n^{t_1}_{k_1}}}+\frac{1}{\sqrt{n_{k_0}^t}}+\frac{1}{\sqrt{n_{k_1}^t}}}\\
        &\geq 2\varepsilon^{-1}\rbr{B_S\rbr{I_q^t(k_0)+I_q^t(k_1)} +I_c^t(k_0, k_1)}= \Delta_t(k_0, k_1), 
    \end{align*}
    where the equality holds by definition of $N^*$, the second inequality holds by our condition $\min\{n_{k_0}^{{t_1}}, n_{k_1}^{{t_1}}\}\geq N^*$, and the last inequality holds by using \Cref{lem:confidence-after-binary-search}.
    Therefore, we conclude that by letting $\alpha_t=K t^{-1/3}\land 1\ge t^{-1/3}$, the condition for an essential BS, i.e.,  $\alpha_t< \Delta_t(k_0, k_1)$ no longer holds for any $t$ such that ${t_1}<t\le T$.
\end{proof}
Now, we are ready to prove \Cref{lem:essentialBS}.
\begin{lemma}[Restatement of \Cref{lem:essentialBS}]
    Consider binary search $\BS(S_0, S_1, \tilde \lambda_{\min}, \tilde\lambda_{\max}, \tau_0, \tau_1, k_0, k_1, t_0, t_1)$. Suppose that the total number of essential binary searches in $T$ rounds is $N$. Then, we have
    \begin{equation}
        N\leq \left(12 \varepsilon^{-1}B_S\right)^{2} 2 \log \left(K 2^{M} T\right)K T^{2 / 3}=KN^*.
    \end{equation}
    \end{lemma}
\begin{proof}[Proof of \Cref{lem:essentialBS}]
    % By \Cref{lem:no-more-search}, only if $\min\{n_{k_0}^{{t_1}}, n_{k_1}^{{t_1}}\}<N^* $ where ${t_1}$ is the ending round of a BS, such a BS can be an essential BS ending with $(k_0, k_1)$. Consider the following problem: given a complete graph $G=(V, E)$ where $V=[K]^+$ are the $K+1$ agent's actions, and $E=\{(i, j)|i, j\in V\}$ are the essential BSs on $(k_0, k_1)$. Each node has a weight $w_{i}$, indicating the number of rounds in essential BSs searching with node $i$.The maximum total weight that can be placed on edges, satisfying $\min\rbr{w_i, w_j}<N^*$, bounds the total number of rounds in essential BS. The total weight is $(K-1)N^*=\left(12 \varepsilon^{-1}B_S\right)^{2} 2 \log \left(K 2^{M} T\right)(K-1) T^{2 / 3}$.
    To prove \Cref{lem:essentialBS}, we first introduce another concept called \emph{critical binary search} (critical BS).
    \begin{definition}[Critical binary search]
        For an \emph{essential} binary search $\BS(S_0, S_1, \tilde \lambda_{\min}, \tilde\lambda_{\max}, \tau_0, \tau_1, k_0, k_1, t_0, t_1)$, we consider it to be critical if $\min\{n_{k_0}^{t_1}, n_{k_1}^{t_1}\}\ge N^*$.
    \end{definition}
    \paragraph{Critical BSs.}
    We claim that the number of critical BS is no more than $K$.
    Suppose the opposite holds, i.e., number of critical BS no less than $K+1$. 
    Consider a graph $\cG=(V, E)$ where $V=[K]^*$ and $E$ is formed by the set the critical BSs ending with $(k_0, k_1)\in V$.
    We claim that one of the following two cases must hold
    \begin{itemize}
        \item[(i)] There exists $(i,j)\in[K]^+$ such that there are at least two critical BSs ending with $(k_0, k_1)=(i, j)$;
        \item[(ii)] There is a loop in $\cG$ formed by the critical BSs.
    \end{itemize}
    We prove that both (i) and (ii) must not happen. 
    For (i), following from the definition of critical BS that $\min\{n_{k_0}^{{t_1}}, n_{k_1}^{{t_1}}\}\geq N^*$ where $t_1$ is the ending round of a critical BS and using \Cref{lem:no-more-search} that there should be no more essential BSs ending with $(k_0, k_1)$ after $t_1$, we directly have (i) impossible. For (ii), suppose that $\BS(S_0, S_1, \tilde \lambda_{\min}, \tilde\lambda_{\max}, \tau_0, \tau_1, k_0, k_1, t_0, t_1)$ is the first that creates a loop. Then before $t_0$, there must be a path $\ell'_{k_0 k_1}$ of critical BSs. For any edge $(k_0', k_1')$ on $\ell'_{k_0k_1}$, there must be a critical Binary search $\BS(S_0', S_1', \tilde \lambda_{\min}', \tilde\lambda_{\max}', \tau_0', \tau_1', k_0', k_1', t_0', t_1')$ with $t_1'< t_0$. Using \Cref{lem:no-more-search} again, we have
    \begin{align*}
        t_0^{-1/3} \ge \Delta_{t_0}(k_0', k_1'), \quad \forall (k_0', k_1')\in \ell'_{k_0, k_1}.
    \end{align*}
    Since a path in $\cG$ has length at most $K$, we conclude that
    \begin{align*}
        \alpha_{t_0}= K t_0^{-1/3} &\ge \sum_{(k_0', k_1')\in\ell'_{k_0 k_1}} \Delta_{t_0}(k_0', k_1')\\
        &=\sum_{(k_0', k_1')\in\ell'_{k_0 k_1}} 2\varepsilon^{-1}\rbr{B_S\rbr{I_q^{t_0}(k_0')+I_q^{t_0}(k_1')} +I_c^{t_0}(k_0', k_1')}\\
        &\ge 2\varepsilon^{-1}\rbr{B_S\rbr{I_q^{t_0}(k_0)+I_q^{t_0}(k_1)} +I_c^{t_0}(k_0, k_1)} = \Delta_{t_0}(k_0, k_1), 
    \end{align*}
    where the last inequality holds by noting that $I_c^{t_0}(k_0', k_1')\le \varphi^{t_0}(k_0', k_1')$ and $I_c^{t_0}$ is obtained by the shortest path algorithm with edge length $\varphi^{t_0}$. Such a fact again suggests that there cannot be any essential BR ending with $(k_0, k_1)$ that starts at $t_0$. Thus, a loop also cannot be formed, and we prove our claim that the total number of critical BSs is no more than $K$.
    
    \paragraph{Essential but noncritical BSs.}
    Next, we count the number of BSs that are essential but not critical. Recall the graph $\cG=(V, E)$ where $V=[K]^*$ and $E$ is formed by the set the critical BSs ending with $(k_0, k_1)\in V$. 
    Let $w^t(i,j)$ be the total number of essential BSs ending with $(k_0, k_1)=(i,j)$ before round $t$ and let $w(i,j)=w^T(i,j)$. 
    Let $d_k^t=\sum_{k'\neq k} w(k, k')$ be the total weights for node $k\in V$. For any pair $(k_0, k_1)\in V$, if $d^t_{k_1}\land d^t_{k_2}\ge N^*$, there should be no more essential BSs after $t$ by \Cref{lem:no-more-search}. On the other hand, any BS ending with $(k_0, k_1)$ guarantees that $d_{k_0}^t$ and $d_{k_1}^t$ increase at least by $1$. 
    Hence, the problem of counting the number of essential but noncritical BSs can be described as the following weight-placing game, 
    \begin{itemize}
        \item[(i)] initiate $d_k=0$ for $\forall k\in V$;
        \item[(ii)] at each round, select an edge $(k_0,k_1)\in V$ such that $d_{k_0}\land d_{k_1}< N^*$ and add $1$ to both $d_{k_0}$ and $d_{k_1}$;
        \item[(iii)] the game ends if no more edge can be selected.
    \end{itemize}
    The total rounds of this weight-placing game upper bound the total number of essential but noncritical BSs.
    We have the following proposition that gives the maximal number of rounds in this weight-placing game.
    \begin{proposition}[Weight-placing game]
        For the previously described weight-placing game, the total number of rounds should not exceed $(|V|-1)(N^*-1)$.
        \begin{proof}
            We prove this proposition by induction on $|V|$. The case $|V|=1$ is trivial. For $|V|=2$ the result is obvious since there is only one edge to choose. Suppose that the result holds for $|V|\le v-1$. For $|V|=v$, suppose that $(v-1)(N^*-1)+1$ rounds are played. We study the quantity $d^-=\min_{k\in V} d_k$.
\begin{itemize}
    \item If there exists $k\in V$ such that $d_k\le N^*-2$, then for the subgraph $\cG^{k}=\cG\backslash\{k\}$, we have at least $(v-2)(N^*-1)+2$ rounds played on $\cG^k$. However, since $\cG^k$ only has $v-1$ nodes, we have from the induction that there should be no more than $(v-2)(N^*-1)$ rounds played on $\cG^k$, which causes a contradictory. Thus, we have $d^-\ge N^*-1$ if $(v-1)(N^*-1)+1$ rounds are played.
    \item If $d^-\ge N^*$, then the last round in this play should be mistakenly played following rules (ii) and (iii). 
\end{itemize}
Therefore, we conclude that $d^-=N^*-1$ and we are able to choose a node $k\in V$ such that $d_k=N^*-1$. On the subgraph $\cG^k=\cG\backslash\{k\}$, we have by induction that at most $(v-2)(N^*-1)$ rounds can be played. Therefore, the total rounds played on $\cG$ is no more than
\begin{align*}
    (v-2)(N^*-1)+d_k = (v-1)(N^*-1), 
\end{align*}
which contradicts our assumption that  $(v-1)(N^*-1)+1$ rounds are played. 
Thus, for $|V|=v$, no more than $(v-1)(N^*-1)$ rounds can be played.
By induction, we conclude that the proposition holds for any $|V|$.
        \end{proof}
    \end{proposition}
    Hence, the total number of essential but noncritical BSs is bounded by $K(N^*-1)$. Given that the number of critical BSs bounded by $K$, the total number of essential BSs is thus bounded by $KN^*$.
\end{proof}
