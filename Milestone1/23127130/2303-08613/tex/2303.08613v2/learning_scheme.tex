\section{Theoretical Results}
% \subsection{Model Assumptions}
% \begin{assumption}[Regularity of the information structure and the scoring rule]\label{asp:reg}
%     We assume that,
% \begin{itemize}
%     \item[(i)] (convexity) $\sS$ is a star-shaped convex function class, meaning that if $S_1, S_2\in\sS$, we have $\alpha S_1 + (1-\alpha) S_2 \in\sS$ for any $0\le \alpha\le 1$ and $\lambda S\in\sS$ for any $\lambda\in[0, 1)$ if $S\in\sS$. Moreover, we assume that $S(\sigma, \omega)=u(a^*(\sigma), \omega)\in\sS$.
%     % \item[(ii)] (compactness) Let $v_S = \sbr{\EE S(\Sigma_i, \omega)}_{i\in[K]}$ be the vector of expected scoring under each signal and define $\cV=\cbr{v_S\given \forall S\in\sS}$. We assume that $\cV$ is a compact subset of $\RR^K$.
%     % \item[(iii)] (realizibility) Let $\cB_\epsilon(c)=\cbr{z\in\RR^K\given \nbr{z-c}_\infty\le \epsilon}$ denotes the $\epsilon$-ball centered at $c$ in $\RR^K$. We assume that $\cB_\epsilon(c)\in\Int\cV$, where $c$ is the cost vector. 
%     \item[(ii)] (decaying marginal information gain) Suppose that this $K$ signal are sorted in the increasing order of the cost. Define $u_k = \EE_{\omega\sim\sigma, \sigma\sim q_k}\sbr{u(a^*(\sigma), \omega)}$. We assume the marginal information gain is strictly decaying, i.e., there exists a $\epsilon>0$
%     \begin{gather*}
%         \frac{u_{K}-u_{K-1}}{c_{K}-c_{K-1}} >\epsilon, \quad \text{and}\quad
%         \log\frac{u_k-u_{k-1}}{c_k-c_{k-1}} - \log{\frac{u_{k+1}-u_k}{c_{k+1}-c_k}} >\log(1+\epsilon), \quad \forall k=1,\cdots, K-1.
%     \end{gather*}
%     Moreover, we assume that $c_k - c_{k-1} >\delta_c$ for $c=1, \cdots, K$.
% \end{itemize}
% \end{assumption}
% The convexity of $\sS$ enables us to deploy the \textit{linear scoring rule}.

% The decaying marginal information gain property indicates that all the signals are \textit{linearly-implementable}.
% \begin{lemma}[Linear implementable]
% Under Assumption \ref{asp:reg}, all the signals are linearly-implementable \citep{dutting2019simple}. 
% \end{lemma}
% Linear implementable means that using the linear scoring rule $S(\sigma, \omega)=\lambda u(a^*(\sigma), \omega)$ where $\lambda\in[0, 1/\epsilon]$, all the signals can be induced. 
% We remark that the compactness assumption is satisfied if and only if there exists $S_1, \cdots, S_K\in\sS$ such that $(v_{S_i})_{i\in[K]}$ are linear independent.
% Such an assumption characterizes the richness of the scoring rule class as well as the nondegeneration of the signal structure since it implies that the prior over the posteriors of each signal should also be linear independent.
% Such a property is essential for stimulating all the possible actions from the agent using the scoring rule. 
% For brevity, we define 
% \begin{align*}
%     S(\sigma) = \EE_{\omega\sim\sigma}\sbr{S(\sigma, \omega)}, \quad
%     v_S^{(i)} = \EE\sbr{S(\Sigma_i, \omega)}, \quad
%     u^{(i)} = \EE\sbr{u(a(\Sigma_i), \omega)},
% \end{align*}
% where $a^*(\sigma)=\argmax_{a\in\cA} \EE_{\omega\sim\sigma}[u(a, \omega)]$.
% In the sequel, we consider $\supp(\sigma)$ to be a finite set with cardinality $M$.
% The following results may be useful.

% We assume $u$ function is globally bounded, and the scoring rule class $\sS$ is also globally bounded.

% \subsection{Algorithm Outline}
% We propose a two-step online algorithm for solving the scoring rule learning problem. 
% \paragraph{Step \RNum{1}:}(Induce the $K$ signals) In the beginning, we deploy linear contract $S(\sigma, \omega)=\lambda u(a^*(\sigma), \omega)$ with a binary search on $\lambda\in [0, 1/\epsilon]$.
% The target is to find $\tilde S_0, \cdots, \tilde S_K$ such that when deploying $\tilde S_k$, the agent is guaranteed to respond with $\Sigma_k$.
% \begin{lemma}\label{lem:linear-binary-search}
% This binary search needs at most $2K\log_2(1/4\epsilon)$ interventions with the agent. Moreover, we can find $\tilde S_0, \cdots, \tilde S_K$ such that $(v_{\tilde S_k, k} - c_k) - (v_{\tilde S_k, i}-c_i) > \epsilon\delta_c/2(1+\epsilon)$ for any $i\neq k$.
% \end{lemma}
% We remark that if we have some prior knowledge that enables us to induce the $K$ signals, we can drop the decaying marginal information gain assumption and go directly to the next step.







% % However, the problem is that the agent might not respond with signal $\Sigma_{k_t^*}$ as we have expected. We next address the problem by adopting a interpolation scoring rule.

% % \subparagraph{Interacting in the $t$-th loop.} We deploy scoring rule 
% \begin{align*}
%     S_t = \alpha_t\tilde S_{k_t^*} + \rbr{1-\alpha_t} S_t^*
% \end{align*}
% in the $t$-th loop. The following lemma explains why we should use a interpolation of these two scoring rule. 

% The above lemma characterizes the probability that the agent fails to respond $\Sigma_{k_t^*}$ as we have expected. Since $S_t^*$ may be over-optimistic about the fact that the agent should respond with $\Sigma_{k_t^*}$, we move the scoring rule towards the original $\tilde S_{k_t^*}$ (under this scoring rule, the agent is known to respond with $\Sigma_{k_t^*}$) in order to ensure a high probability that the agent will respond with signal $\Sigma_{k_t^*}$.
% In the sequel, we let $\alpha_t$ to be a fixed decaying sequence, e.g., $\alpha_t=(K-1) t^{-1/3}\land 1$.



In this subsection, we provide the learning result for the OSRL-UCB algorithm.
% \subsection{Learning Result}
\begin{theorem}[Regret for OSRL-UCB]\label{thm:regret}
    Under Assumption \ref{asp:oracle} on the action-informed oracle, with $\alpha_t = \min\{K t^{-1/3}, 1\}$, the   OSRL-UCB algorithm \ref{alg:UCB} has regret
\begin{align*}
    \Reg(T) = \tilde\cO\bigl ( (B_S+B_u)B_S^2\varepsilon^{-2} \cdot K^2 C_\cO\cdot T^{2/3}\big) , 
\end{align*}
where $B_S$ and $B_u$ bound the magnitudes of the scoring rule and the utility function, respectively, $\varepsilon$ is the marginal profit gain given by the action informed oracle, $K$ is the number of the agent's actions, and $C_\cO$ is the cardinality of the observation set.
\vspace{-10pt}
\begin{proof}
We defer the detailed proof to \S\ref{prof: main theorem}. 
% At a high level, it hinges on the following regret decomposition,
% \begin{align*}
%  \Reg^\pi(T) & = \sum_{t=1}^T\underbrace{\EE\sbr{\ind(k_t=k_t^*)\subopt(k_t, S_t)}}_{\displaystyle{A_t}}\\ 
%  & + \sum_{t=1}^T\underbrace{\EE\sbr{\ind(k_t\neq k_t^*)\subopt(k_t, S_t)}}_{\displaystyle{B_t}}.    
% \end{align*}
\end{proof}
\end{theorem}

Here, we use $\tilde \cO$ to omit logarithmic factors.
The regret depends quadratically on the agent's action number and linearly on the cardinality of the observation set.
Notably, the regret is independent of the size of the hidden state $|\Omega|$. 
In addition, we achieve a $\cO(T^{2/3})$ sublinear rate of regret in terms of the principal's accumulative profit for eliciting information under the scoring rule framework. Such a result is achieved with a mild assumption of an action-informed oracle that provides a set of scoring rules with marginal profit gain for the agent that induce all the agent's actions. 
We do not assume the learner to have sufficient knowledge about the other strategic player(s) in contrast to many existing works \citep{balcan2015commitment,guo2022no,wu2022sequential}. 
In addition, we only assume the principal to have knowledge of her utility and can observe the agent's action choice. For discussion of the these two assumptions, we refer the readers to the footnote in \S\ref{sec:model}.

For the action-informed oracle with a set of foreknown scoring rules, these foreknown scoring rules do not need to be optimal in each section $\cV_k$.
They can even be obtained through random sampling from $\beta$-strongly proper scoring rules  (See \Cref{exp:random sampling}) for general setting, or discovered in a linear scoring rule class (See \Cref{exp:linear contract}) if the marginal information gain is strictly decaying.
We also give the following corollaries that characterize the regret combined with the effort to find an action-informed oracle.
\begin{corollary}[Regret with oracle in \Cref{exp:random sampling}]\label{cor:random sampling}
Let $\tilde\cV_k=\{S\in\cS_\beta\given g(k, S)\ge g(k', S)+\kappa, \forall k'\neq k, k'\in[K]\}$ where $\cS_\beta\in\cS$ is the class of $\beta$-strongly proper scoring rules, and suppose $\vol(\tilde\cV_k)\ge \eta \Vol(\cS_\beta)$ for $k\in[K]$.
Running the oracle acquisition process in Example \ref{exp:random sampling} for $T^\gamma$ rounds before deploying the OSRL-UCB algorithm for $T-T^\gamma$ rounds, the online regret is bounded by
% \vspace{-2mm}
\begin{align*}
    \Reg(T) &= \tilde\cO\big( (d_2 d_1^2 \beta)^{-2} \cdot KM\cdot T^{2/3}\big) 
    % \nend&\qquad
    +\cO(KT\exp(-T^\gamma\eta/M) + T^\gamma),
\end{align*}
where $d_1=\min_{i\neq j, \forall (i,j)\in[M]}\nbr{\sigma_i-\sigma_j}_\infty$, $d_2=\min_{k'\neq k, (k, k')\in[K]}\max_{i\in[M]}\sbr{q_k(i)-q_{k'}(i)}$, and $M$ is the cardinality of $\Sigma$.
\end{corollary}
% \vspace{-2mm}
And also we characterize the regret for the action-informed oracle obtained by linear scoring rule.
\begin{corollary}[Regret with oracle in \Cref{exp:linear contract}]\label{cor:linear contract}
Suppose the model assumption that the marginal information
gain is strictly decaying in \Cref{exp:linear contract} holds.
By running the oracle acquisition process in \Cref{exp:linear contract} for $\cO(K\log_2(\varepsilon^{-1}))$ rounds and the OSRL-UCB algorithm for the remaining rounds, the online regret is bounded by
\begin{align*}
    \Reg(T) &= \tilde\cO\big(\varepsilon^{-2} \cdot K^2 C_\cO\cdot T^{2/3}\big)+ \cO(K\log_2(\varepsilon^{-1})), 
\end{align*}
where $\varepsilon=\epsilon u_1/4b^2$, and $\epsilon, u_1, b$ are constants defined in \Cref{exp:linear contract}.
\end{corollary}

Corollaries \Cref{cor:random sampling} and \Cref{cor:linear contract} both provide regret bound without any using of oracle. Specifically, \Cref{cor:random sampling} considers a more general framework under the assumption of lower bounded action section volume while \Cref{cor:linear contract} assumes marginal information decay, which is commonly seen in real world practice. Specifically, \Cref{cor:random sampling} shows that by random sampling for $T^\gamma$ rounds where $0<\gamma<2/3$, it suffices for the principal to have $\tilde \cO(T^{2/3})$ regret. In addition, $\gamma$ can be significantly small since the second term diminishes exponentially on $T^\gamma$. 
In addition, \Cref{cor:linear contract} shows that running constant number of additional rounds in the oracle acquisition process does not deteriorate the regret bound.

Following the discussion in \citet{jin2018q}, our algorithm also has PAC guarantee as the following.
\begin{corollary}[PAC guarantee]
For every $\zeta>0$, the OSRL-UCB algorithm with action informed oracle finds a $\zeta$-optimal scoring rule using $\tilde\cO(\varepsilon^{-6} K^6C_\cO^3\zeta^{-3})$ samples.
\end{corollary}

%In general, we remark that the regret bound in \Cref{thm:regret} improves over the $\cO(T^{3/4})$ regret of \citet{camara2020mechanisms}, though their model for a stage game covers ours as special case. 
%Meanwhile, 
\vspace{-1mm}
\citet{zhu2022sample} provides an $\cO(T^{2/3})$ regret lower bound for the online learning problem towards the optimal contract. Despite that standard contract design is a special case of our model, their setting is different from ours in that the agent may have possibly infinitely many actions. So the regret lower bound of our problem remains an open question. To close this gap, we believe the key question remains to be answered is whether the decision boundary of $\cV_{k^*}$ can be determined efficiently. That is, even if the best action $k^*$ is known, the learner is still unable to solve the optimal scoring rule from $\OptLP_{k^*}$ without enough knowledge about $\cV_{k^*}$. For now, we are able to construct a class of instances where such boundary of $\cV_{k^*}$ can determine with binary search and thus avoid the costly learning of $q_k$ for every $k\in[K]$.  However, it still remains unclear if these efficient search techniques can possibly be generalized to arbitrary instances --- a definitive answer should close up the regret lower and upper bound of this problem. We leave this intriguing direction to  future work.
% --- Is it necessary to learn $q_k$ for every $k\in[K]$ to determine the $\cV_{k^*}$ and solve $\OptLP_{k^*}$?  
%  According to Lemma \ref{lem:mistake}, the total rounds it takes for the agent to align with the principal's expected response is $n_{k, t}=\cO(t^{2/3})$ for all $k\in[K]$. Therefore, the regret from agent misalignment is  $\cO(K T^{2/3})$. On the other hand, since we shrink $S_t^*$ to $\tilde S_{k_t^*}$ by $\alpha_t$, the regret from suboptimality adds up to $\cO(\sum_{t=1}^T\alpha_t) = \cO(T^{2/3})$. This suggests that our regret analysis of Algorithm \ref{alg:UCB} is tight. 
% Combining these two parts of regret gives the result.



% We will be keeping the current scoring rule $S$ and a target scoring rule $\hat S$ generated by Algorithm \ref{alg:Opt} during the update. The logic under Algorithm \ref{alg:pseudo} is that with the current knowledge of the signal structure, we try to find the target scoring rule $\hat S$ that makes the payment just compensate for the cost of each known signal. Now, we move the current scoring rule to the target scoring rule by means of binary search on the segment connecting $S$ and $\hat S$. If this goes on well, we are just done. Otherwise, there must be a signal that we haven't identified yet and the agent will switch to that signal at some step of the binary search. Thus, we can expand our observed signal set and do this all again. 

% The guarantee is that if the agent actions just the same as what we have expected under $\hat S$ (choosing a signal from the current observed signal set), such a target scoring rule $\hat S$ provides a upper bound of the principal's best earnings. Since we assume that there are at most $K$ available signals, the expansion of the observed signal set will occur only for a finite number of times.
% So finally the optimal is reached. 

% \todo{Current algorithm is simplified by assuming that the estimation of the belief in Algorithm \ref{alg:game} is exact and ignoring a $2^{-N}$ error of $\lambda$ in Algorithm \ref{alg:BS}.}

% \SetKwComment{Comment}{/* }{ */}
% \begin{algorithm}
% \caption{(Play the game) $\hat q = \mathrm{Game}(S, T)$\;
% Description: Input the scoring rule and the total number of rounds. The algorithm plays the game for $T$ rounds and collects the report $\sigma^t$ from the agent and output the estimated prior of $\sigma^t$, which we assume to be within some belief class $\sQ\subseteq\Delta(\Delta(\Omega)).$ 
% }\label{alg:game}
% \KwData{$S, T, \sQ$\Comment*[r]{$\sQ$ is a given model class}}
% \KwResult{$\hat q$}
% Play with scoring rule $S$ for $T$ rounds and collect $[\sigma^t]_{t\in[T]}$\;
% % $\hat u=\sum_{t\in[T]}\EE_{\omega\sim\sigma^t }\sbr{u(a(\sigma^t), \omega)}/T$\;
% % $\hat v = \sum_{t\in[T]}\EE_{\omega\sim\sigma^t }\sbr{S(\sigma^t, \omega)}/T$\;
% $\hat q = \min_{q\in \sQ} \kl\rbr{\sum_{t\in T}\ind(\sigma = \sigma^t)/T\,\|\, q}$\;
% \end{algorithm}

% \begin{algorithm}
% \caption{(Binary Search) $(\lambda, q_\lambda) = \mathrm{BS}(S_0, S_1, N, T, \delta_q, Q)$\;
% Description: Input initial scoring rule $S_0$, target scoring rule $S_1$, the maximal times of binary searches $N$, the total round number $T$ played during each search, a threshold $\delta_q$ for categorification within the available belief set $Q$.
% The algorithm does binary search along the path connecting two scoring rules and tries to identify the largest $\lambda$ where the agent first switches its policy. If the agent doesn't switch its policy, then the algorithm will not terminate (meaning that the algorithm has found the optimal scoring rule. )}\label{alg:BS}
% \KwData{$S_0, S_1, N, T, \delta_q, Q$}
% \KwResult{$\lambda, q_\lambda$}
% $\lambda_{\min}\leftarrow 0,\quad \lambda_{\max}\leftarrow 1$\;
% $n\leftarrow 0$\;
% \While{$n<N$}{
% $\lambda = \rbr{\lambda_{\min}+\lambda_{\max}}/2$\;
% $S=\lambda S_0 + (1-\lambda) S_1$\;
% $q_\lambda\leftarrow \mathrm{Game}(S, T)$\;
% \eIf{$\min_{q\in Q}\kl\rbr{q_\lambda \,\|\, q}>\delta_q$}{
%     $\lambda_{\min} \leftarrow \lambda$\Comment*[r]{Unknown behavior}
% }
% {
%     $\lambda_{\max}\leftarrow \lambda$\Comment*[r]{Known behavior}
% }
% $n\leftarrow n+1$\;
% }
% \end{algorithm}

% \begin{algorithm}
% \caption{(Scoring Rule Optimization) $\hat S = \mathrm{Opt}(C, Q, \beta_c)$\;
% Description: Input the relative cost set $C$, the belief set $Q$, and also a hyper-parameter $\beta_c$. 
% The algorithm tries to minimize the principal's payment by designing a scoring rule that minimizes $b_S^{(1)}$ (since we can show that the first signal found by the algorithm always has the largest gain, i.e., $g^{(1)}=u^{(1)}-c^{(1)}$ is the largest. Therefore, when the payment equals the cost for all known signals, the first signal is favourable by the principal). 
% % Moreover, the objective is subject to a penalty for $v_S$ deviating from the relative cost $C$, which means we tempts to make the scoring rule aligned with the cost. 
% }\label{alg:Opt}
% \KwData{$C, Q, \beta_c, \sS$}
% \KwResult{$\hat S$}
% \vspace{-20pt}
% \begin{align*}
%     \hat S = \argmin_{S\in\sS} \cbr{v_S^{(1)} + \beta_c\min_{\eta\in\RR}\nbr{v_S-C-\eta \ind}_\infty},\qquad \text{where}\quad
%     v_S \leftarrow \sbr{\EE_{\sigma\sim q}S(\sigma) }_{q\in Q};
% \end{align*}
% \end{algorithm}

% \begin{algorithm}
% \caption{Pseudo Algorithm for Learning the $\epsilon$-Optimal Scoring Rule
% }\label{alg:pseudo}
% \KwData{$u, M, T, \beta_c, \delta_q, \sS$}
% \KwResult{$\hat S$}
% $C\leftarrow \cbr{0}, \quad Q\leftarrow \emptyset$\Comment*[r]{Initialize the relative cost set}
% $S=u(a(\sigma), \omega)$\;
% $(u, v, q)\leftarrow\mathrm{Game}(S, T)$\Comment*[r]{Run the game with scoring $u$}
% $Q\leftarrow \cbr{q}$\Comment*[r]{Initialize the signal belief set}
% $m\leftarrow 0$\;
% \While{$m < M$}{
% $\hat S \leftarrow \mathrm{Opt}(C, Q, \beta_c)$\Comment*[r]{Optimize $\hat S$ under current $C, Q$}
% $(\lambda, q) = \mathrm{BS}(S, \hat S, N, T, \delta_q, Q)$\Comment*[r]{Binary search for the first switch}
% $S\leftarrow \lambda S + (1-\lambda)\hat S$\;
% $Q\leftarrow Q\cup\cbr{q}$\Comment*[r]{Add a new signal's belief}
% $C\leftarrow C\cup\cbr{\EE_{\sigma\sim q}\sbr{ S(\sigma)} - \frac{1}{\abr{Q}}\sum_{q'\in Q}\EE_{\sigma\sim q'}\sbr{S(\sigma)} + \frac{1}{\abr{C}}\sum_{c\in C} c}$\;
% $m\leftarrow m+1$\;
% }
% \end{algorithm}


% \section{Future Work}
% In this section, we would like to point out several directions for the future development of this class project. 

% \subsection{Finer-grained Analysis of Regret }
% Despite an improvement over prior work \cite{camara2020mechanisms}, the current $O(T^{2/3})$ regret bound is still worse than $O(T^{1/2})$ regret bound in the standard bandit learning problems. We conjecture that there exists a large class of lower bound instances that require $\Omega(T^{2/3})$ regret. That is, in order to have $o(T^{2/3})$ regret, the suboptimal signals shall be induced with $o(T^{2/3})$ times. This however means that the suboptimal signals are estimated with error at least $\omega(T^{-1/3})$. Then, it is difficult to construct a $T^{-1/2}$-optimal scoring rule to induce the optimal signal after the exploration phase. 
% Meanwhile, we believe there are certain interesting special cases where $O(T^{1/2})$ regret bound is attainable. For example, when the signal structures are known, but only the utility function is unknown. 


% \subsection{Extension to Markovian Environment}
% We would also like to study the information acquisition problem under the Markovian environment, where both the agent and the principal may have long term objectives. For example, the agent may have incentives to share less information at the beginning to gain more manipulating power to extract more surplus in later steps of the process.  
\vspace{-2mm}
\section{Conclusion}
\vspace{-1mm}
We study the problem of incentivizing information acquisition through proper scoring rules under the principal-agent framework with information asymmetry. We propose the OSRL-UCB algorithm and show that with a mild oracle assumption, it  achieves  a $\cO(K^2 C_\cO T^{2/3})$ sublinear regret. Future direction includes establishing regret lower bound  and extensions to the contextual and dynamic settings.
