\subsection{Related Work}\label{sec:related-work-full}

\paragraph{Optimal Scoring Rules.} This paper builds on a set of literature on optimizing scoring rules. Several papers consider the model with multiple levels of effort. \citet{neyman2021binary} design  outcome-optimal scoring rule under a binary-state model, and for integral levels of effort, where effort levels represent the number of samples drawn and are informationally ordered. \citet{HSLW-22} design  effort-optimal scoring rule, under a multi-dimensional binary-state model, and each dimension of the effort corresponds to one of the independent multi-dimensional state. 
In contrast, our paper considers a general state space, with multiple levels of effort not necessarily ordered or independent. 
\citet{li2022optimization,10.1145/3490486.3538261, CY-21} design optimal scoring rule for a binary effort model, which is different to our multiple-effort-level model.
\citet{oesterheld2020minimum} design regret-optimal scoring rule for multiple agents in a single round when the information structure is unknown to the principal, while our model only has one agent, and our learning algorithm achieves diminishing  regret over multiple rounds. 
Also, all the papers mentioned above model the state as exogenously given, while in our paper, the prior of the state can potentially be affected by agent's endogenous action. 
%The optimization goal of \citet{li2022optimization, CY-21} is to maximize the information gain by exerting effort, while that of  \citet{10.1145/3490486.3538261} is to maximize revenue from acquiring information (i.e.\ the utility over outcome minus the payment). 
%Our work optimize for the revenue of the principal. 
%Our problem of information elicitation for decision-making under the principal-agent framework has been studied by \citet{10.1145/3490486.3538261,oesterheld2020minimum}. While they have explored the computational aspects of this problem as well as its extension to multiple agents, we primarily focus on the learning aspects, when the principal has limited knowledge of the game parameters.

\paragraph{The Principal-Agent Problem.} Our model of information acquisition can be viewed as a class of principal-agent problem, which has been established as a crucial branch of economics known as the contract theory \citep{grossman1992analysis, smith2004contract, laffont2009theory}. 
Driven by an accelerating trend of contract-based markets deployed to Internet-based applications, the principal-agent problem recently started to receive a surging interest especially from the computer science community \citep{dutting2019simple, dutting2021complexity, guruganesh2021contracts, alon2021contracts, castiglioni2021bayesian, castiglioni2022designing}.
As pointed out by \citet{alon2021contracts}, this includes online markets for crowdsourcing, sponsored content creation, affiliate marketing, freelancing and etc. The economic value of these markets is substantial and the role of data and computation is pivotal. Different from the classic contract design problems, we focus on the design of contracts (i.e., scoring rules) that optimally elicit the information acquired by the agents at some cost. 

\paragraph{Online Learning in Strategic Environment.}  More broadly, our work add to the literature on   online learning  in strategic environments, which has gained popularity in recent years. %\citet{braverman2018selling, deng2019strategizing} consider a dynamic pricing problem (or the Stackelberg games of complete information) against no-regret agents.
In particular, the online learner's utility at each round is determined by both her own action and the strategic response(s) of other player(s) in certain repeated game, and the typical goal of this learner is to find her optimal strategy under some equilibrium
These repeated games are adopted from the influential economic models including, but not limited to, the Stackelberg (security) game~\citep{marecki2012playing, balcan2015commitment, haghtalab2022learning},
 %\citep{marecki2012playing, balcan2015commitment,bai2021sample, zhong2021can, haghtalab2022learning}, 
 auction design~\cite{amin2013learning, feng2018learning, golrezaei2019dynamic, guo2022no}, 
 matching~\citep{jagadeesan2021learning},
 %\cite{jagadeesan2021learning, min2022learn}, 
 contract design~\cite{zhu2022sample}, 
 Bayesian persuasion~\citep{castiglioni2020online, 10.1145/3465456.3467593, wu2022sequential}. Our model can be viewed as a generalized information elicitation problem in an online learning setup. To our best knowledge, there is no previous work that considered any similar learning problem. In addition, we remark that, in many of these existing works, e.g.,~\citet{balcan2015commitment,guo2022no,wu2022sequential}, the learner is assumed to have sufficient knowledge about the other strategic player(s), and her uncertainty is regarding the environment or her own utility. Assumptions of such kind can significantly simplify the problem into the standard online learning problems, once the learner can almost predict the best response of other player(s). Our work, however, does not make any of such assumption and the most challenging part of our learning algorithm design is indeed to ensure the desired agent response under uncertainty.
 \citet{camara2020mechanisms} considers the online mechanism design problem, where both the principal and the agent may learn over time from the state history. However, our paper assumes the agent is myopic in each round. 
% consider the online learning problem in the Stackelberg security game with adversarially chosen follower types. \citet{haghtalab2022learning} study the online learning problem in Stackelberg game with a non-myopic agent. 
%Our model can also be understood as a meaningful special case of  \citet{camara2020mechanisms}, as they consider the online mechanism design problem, where both the principal and the agent may learn over time from the state history. However, they do not achieve diminishing regret when the agent is more informed than the principal. We are able to utilize more problem structures of this special case and obtain sublinear regret.
%; the principal seek to minimize the counterfactual internal regret. 
% In , the agents are myopic in each round, which can be understood as . We are able to obtain diminishing regret.
 
% Meanwhile, consider the Bayesian persuasion in an online learning setup where the agent is assumed to be myopic and perfectly knows its best response, whereas our model can be viewed as an information elicitation problem in an online learning setup. The Bayesian persuasion and information elicitation problems are both important models in information economics that require different design insights. 


% It remain open here in our model on the effective online learning approach to design optimal scoring rule to acquire information. 