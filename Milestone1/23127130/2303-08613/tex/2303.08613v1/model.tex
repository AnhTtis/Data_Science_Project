\section{The Information Acquisition Model}\label{sec:model}
In this section, we introduce the problem of optimally acquiring information under the principal-agent framework and formulate the problem as a Stackelberg game. 
Following the revelation principle \citep{myerson1979incentive}, we simplify the problem by restricting to the class of proper scoring rules, which enables efficient online learning.
% , where the principal aims to find a scoring rule to pay the agent for giving reports and this scoring rule should maximize the principal's profit under the agent's best response. Then, we show that restricting to the class of proper scoring rule is without any loss of generality for the principal's purpose. Given that the information structure is unknown to the principal, we propose an online learning framework for the principal to optimally acquire information. 
% In this section, we formulate the online learning problem of incentivizing information acquisition. 

\subsection{Basics of Information Acquisition}

To formulate the problem of optimally acquiring information under the principal-agent framework, we consider a stochastic environment with a principal and an almighty agent. 
At the $t$-th round, there is a hidden state $\omega_t\in\Omega$ that will affect the principal's utility, but is unknown to both the agent and the principal until the end of this round.
To elicit refined \emph{information}, i.e., the agent's belief of the hidden state,
the principal moves first and offers a scoring rule to the agent, 
based on  which the agent receives a payment according to the quality of her reported belief. 
The agent is allowed to choose an action from her finite action space $\cB$ with some cost, obtain an observation related to the hidden state, and generate a report on her belief, which puts the principal in a better position to make a decision.
%by picking an action in $\cA$.
In the end, the hidden state is revealed, and a utility is generated for the principal, who then pays the agent based on the scoring rule. 
For any $t\geq 0$, in the $t$-th round, the interactions between the principal and the agent are as follows. 

\begin{mdframed}[style=box]
\textbf{Information acquisition via scoring rule}

\vspace{5pt}
\noindent
At the $t$-th round, the principal and the agent play as the following:
\begin{itemize}[noitemsep, topsep=3pt]
% \setlength{\itemsep}
% [leftmargin=*,topsep=0pt]
    \item[1.] The principal commits to a scoring rule~$S_t:\Delta(\Omega)\times\Omega\rightarrow\RR_+$, where $\Delta(\Omega)$ is the space of distributions over $\Omega$.
    \item[2.] Based on $S_t$, the agent chooses an action $b_{k_t}\in\cB$ indexed by $k_t$ and bears a cost $c_{k_t}\ge 0$. The action $b_{k_t}$ can be observed by the principal.
    % The action choice $k_t$ is observed by the principal, but the cost $c_{k_t}$ is private to the agent.
    \item[3.] The stochastic environment then selects a hidden state $\omega_t\in\Omega$ and emits an observation $o_t\in\cO$ only for the agent according to $p(\omega_t, o_t\given b_{k_t})$. The hidden state $\omega_t$ is unknown to both the agent and the principal at this moment.
    \item[4.] 
    % The agent computes her own belief $\sigma_t\in\Delta(\Omega)$ on the hidden state according to \eqref{eq:sigma}, 
    The agent reports a belief $\hat\sigma_t\in\Delta(\Omega)$ about the hidden state  to the principal.
    \item[5.] The principal makes a decision $a_t\in\cA$ based on $(\hat\sigma_t, k_t, S_t)$.
    \item[6.] In the end, the hidden state $\omega_t$ is revealed. The principal obtains her utility $u(a_t, \omega_t)$ and pays the agent by $S_t(\hat\sigma_t, \omega_t)$.
\end{itemize}
\end{mdframed}
\iffalse 
\begin{mdframed}[style=box]
\textbf{Information acquisition via scoring rule}

At the $t$-th round, the principal and the agent play as the following:
\begin{itemize}[leftmargin=*,topsep=0pt]
    \item[1.] The principal announces a scoring rule $S_t:\Delta(\Omega)\times\Omega\rightarrow \RR_+$ to the agent.
    \item[2.] Based on $S_t$, the agent chooses an action $b_{k_t}\in\cB$ indexed by $k_t$ and bears a cost $c_{k_t}\ge 0$. The action $b_{k_t}$ can be observed by the principal.
    % The action choice $k_t$ is observed by the principal, but the cost $c_{k_t}$ is private to the agent.
    \item[3.] The stochastic environment then selects a hidden state $\omega_t\in\Omega$ and emits an observation $o_t\in\cO$ only for the agent according to $p(\omega_t, o_t\given b_{k_t})$. The hidden state $\omega_t$ is unknown to both the agent and the principal at this moment.
    \item[3.] 
    % The agent computes her own belief $\sigma_t\in\Delta(\Omega)$ on the hidden state according to \eqref{eq:sigma}, 
    The agent gives her report $\hat\sigma_t\in\Delta(\Omega)$ on her belief of the hidden state.
    \item[4.] The principal makes a decision $a_t\in\cA$ based on $(\hat\sigma_t, k_t, S_t)$.
    \item[5.] In the end, the hidden state $\omega_t$ is revealed. The principal obtains her utility $u(a_t, \omega_t)$ and pays the agent by $S_t(\hat\sigma_t, \omega_t)$.
\end{itemize}
\end{mdframed}
\fi 
% \addtocounter{footnote}{-2} %3=n
% \stepcounter{footnote}
% \footnotetext{}
Here, the scoring rule $\cS_t$ is a payment rule that depends on the agent's report $\hat\sigma_t$ and the true state $\omega_t$.
Let $\sS$ be the class of scoring rules with bounded norm $\nbr{S}_\infty\le B_S$.
In the sequel, we assume the principal picks $\cS_t$ from $\sS$.
We assume that the reward function $u:\cA\times\Omega\rightarrow \RR$ also has a bounded norm $\nbr{u}_\infty\le B_u$.
We consider the agent's action set $\cB$ and the observation set $\cO$ to be finite.
Specifically, the agent's action space $\cB=\{b_1,\dots,b_K\}$ has $K$ actions.
% , with action $b_0$ being the null action and leading to a zero cost. We remark that $b_0$ captures the situation when the agent has no incentive to participate and just receives a null observation. 
In the sequel, we also use the action index $k_t$ to represent the agent's action.
The agent's policy for choosing her action $k_t$, her report $\hat\sigma_t$, and the principal's policy for choosing her action $a_t$ will be introduced shortly after.

Notably, our modeling  captures the endogenous effect that the agent's action choice may influence the environment state. 
This is more general than assuming the state is exogenous, and captures the real-world situations that the act of information acquisition, e.g., market investigation,  affects the stochastic environment. Consider the example of a portfolio manager and financial analyst introduced in \S\ref{sec:intro}. The report written by the analyst about a particular stock, when released to the public, may generate considerable impact and affect the stock price \citep{lui2012equity}. 
 
%Recall the previously discussed producer-consultant example, where the producer is the principal, the consultant is the agent, and the value of the customer group is the hidden state.
%In this example, the consultant may conduct some market investigations and collect relevant information for refining the customer group's value. Such actions may actually cause inference to the marketing and affect the customer's value. Therefore, we models such  such that the agent's action choice may influences the environment state.

\paragraph{Information Structure.} 
In the remaining part of this subsection, we ignore the subscript $t$ for a while.
In this information acquisition process, we assume the agent is almighty that has full knowledge of the \emph{information structure}, i.e., each action's cost $c_k$ and the generating process $p(\omega, o\given b_k)$ for the hidden state and the observation under action $b_k$.
Therefore, after obtaining the observation $o$, the agent is able to refine her belief $\sigma\in\Delta(\Omega)$ of  the hidden state via the Bayesian rule 
$
\sigma(\omega)\defeq p(\omega\given o, b_k)=p(\omega, o\given b_{k})/p(o\given b_{k}).
$
% If $k=0$, the agent receives a null observation and her belief simply corresponds to $\sigma(\omega)=p(\omega\given b_0)$.
Note that $\sigma$ is a random measure mapping from the observation space to the probability space $\Delta(\Omega)$ and captures the randomness in $o$. 
Let $\Sigma\subset \Delta(\Omega)$ be the support of $\sigma$. Define $M$ as the cardinality of $\Sigma$ and it follows from the discreteness of $\cB$ and $\cO$ that $M\le K\times C_{\cO}$, where $C_{\cO}$ is the cardinality of $\cO$.
Let $q_k(\sigma)\in\Delta(\Sigma)$ be the distribution of $\sigma$ under the agent's action $k\in[K]$.
Since $\sigma$ already captures all the information about the hidden state from the observation, we ignore the observation $o$ and refer to the costs $\{c_k\}_{k\in[K]}$ and the distributions of the belief $\{q_k\}_{k\in[K]}$ as the information structure, which are private to the agent. We summarize all types of information in \Cref{table:info}.

\begin{table*}[h!]
\centering
\captionsetup{width=.9\linewidth}
\begin{tabular}{|c|c|c|c|}
    \hline
    Public Info & Agent's Private Info & Principal's Private Info & Delayed Info\\
    \hline
    {$S_t, k_t, \hat\sigma_t$} & $\{c_k\}_{k\in[K]}, \{q_k\}_{k\in[K]}, o_t, \sigma_t$ & $u$ & $w_t$\\
    \hline
\end{tabular}
\caption{
Table for the information types. Here, \emph{Public Info} refers to the information that is observable to both the principal and the agent at each round, \emph{Agent's Private Info} refers to the information that is private to the agent, \emph{Principal's Private Info} refers to the information that is private to the principal, and \emph{Delayed Info} only contains the hidden state $\omega_t$, which is unobservable when round $t$ proceeds but revealed when round $t$ terminates.} 
\label{table:info}
\end{table*}

% \paragraph{Information Acquisition as A Stackelberg Game.} 
\subsection{Information Acquisition via Proper Scoring Rules} 
We start with the observation that the information acquisition process can be formulated as a general Stackelberg game.
Let $\mu:\sS\rightarrow [K]$ and $\nu:\sS\times\Sigma\times[K]\rightarrow \Delta(\Omega)$  be the agent's policy for action selection  and  belief reporting, respectively.
Here, the reporting policy $\nu$ depends on $\sigma$ instead of $o$ since $\sigma$ captures all the information about the hidden state.
Given any scoring rule $S$, the agent (as the follower of this game) finds her own action by $k=\mu(S)$,  and reports $\hat\sigma=\nu(S, \sigma, k)$ that maximizes her own expected profit, i.e., 
$ g(\mu,\nu; S) \defeq\EE [ S\rbr{\nu(S, \sigma, \mu(S)), \omega} - c_{\mu(S)}],$ where the expectation is taken over the randomness of $\omega, \sigma$ with respect to $q_{\mu(S)}$. 
Let $\iota:\sS\times\Delta(\Omega)\times[K]\rightarrow \cA$ be the principal's decision policy. 
Hence, the principal (as the leader of this game) is to find the  optimal scoring rule $S$ and the best decision policy $\iota$ that maximize  her own expected profit given the agent's best response $(\mu^*,\nu^*)$, i.e., 
$
h(\mu^*, \nu^*; S, \iota) \defeq \EE [ u ( \iota\big(S, \nu^*(S, \sigma, \mu^*(S)), \mu^*(S)\big), \omega) - S(\nu^*(S, \sigma, \mu^*(S)), \omega) ],
$
where the expectation is taken over the randomness of $\omega, \sigma$ with respect to $q_{\mu^*(S)}$.
The optimal leader strategies are known as the strong Stackelberg equilibria that can be formulated as solutions of a bilevel  optimization problem~\cite{conitzer2016stackelberg}, i.e.,
\begin{equation}\label{eq:stackelberg-1-concise}
\max_{\iota, S\in\sS} h(\mu^*, \nu^*; S, \iota),\quad \ \mathrm{s.t.}\ (\mu^*, \nu^*)\in\argmax_{\mu, \nu}  g(\mu,\nu; S).
\end{equation}
However, the bilevel optimization in  \eqref{eq:stackelberg-1-concise}  is  computationally intractable, since the agent's action space (particularly, the space of reporting scheme $\nu$) is intractable.  

% \begin{equation}
% \begin{aligned}\label{eq:stackelberg-1}
% &\max_{\iota, S:\nbr{S}_\infty\le B_S}\quad h^\iota(S; \mu^*, \nu^*) ,\\
% &\qquad\mathrm{s.t.}\quad(\mu^*, \nu^*)\in\argmax_{\mu, \nu}  g^{\mu,\nu}(S).
% \end{aligned}
% \end{equation}

% The problem can therefore be formulated as a Stackelberg game,
% \begin{mini!}|l|[3]
%     {S\in\cS, \atop \pi:\cS\times\Delta(\Omega)\times[K]\rightarrow \cA}{h^\pi(S) 
%     \defeq \EE_{\omega\sim\sigma \sigma\sim q_{\mu^*(S)}} \sbr{u(\pi(S, \nu^*(S, \sigma, k), k), \omega) - S(\nu^*(S, \sigma, k), \omega)}}
% \end{mini!}
% \begin{equation}
% \begin{aligned}\label{eq:stackelberg-1}
% &\max_{\iota, S:\nbr{S}_\infty\le B_S}\quad h^\iota(S) \defeq \EE_{\omega\sim\sigma, \sigma\sim q_{\mu^*(S)}} \sbr{u\rbr{\iota\big(S, \nu^*(S, \sigma, \mu^*(S)), \mu^*(S)\big), \omega} - S(\nu^*(S, \sigma, \mu^*(S)), \omega)},\\
% &\qquad\mathrm{s.t.}\quad(\mu^*, \nu^*)\in\argmax_{\mu, \nu}  g^{\mu,\nu}(S)\defeq\EE_{\omega\sim\sigma, \sigma\sim q_{\mu(S)}} S\rbr{\nu(S, \sigma, \mu(S)), \omega} - c_{\mu(S)}.
% \end{aligned}
% \end{equation}
% Here, by taking the $(\mu^*, \nu^*)$ that maximizes the principal's profit within the agent's best response, we are actually assuming that the agent is in favor of the principal.

% \subsection{Eliciting Information via Proper Scoring Rules}
% \label{sec:proper scoring rule}
To resolve such an issue,
in the following, we establish a revelation principle result that guarantees that, without loss of generality,  it suffices to only focus on the case where the agent is truthful.  
%we are able to drop the reporting scheme from the agent's action space by showing that the agent's truthful report can be without loss of generality.
% while the agent is not guaranteed to report her true belief from her investigation, 
Specifically, there is a well-known class of scoring rules (in Definition \ref{def:PSR}) that characterizes all scoring rules, under which   truthfully reporting is in the agent's best interest~\cite{savage1971elicitation}. 
\begin{definition}[Proper scoring rule]\label{def:PSR}
    A scoring rule $S:\Delta(\Omega)\times\Omega\rightarrow \RR_+$ is proper if, for any belief $\sigma\in\Delta(\Omega)$ and any reported posterior $\hat\sigma\in\Delta(\Omega)$,  we have $\EE_{\omega\sim \sigma}S(\hat\sigma, \omega)\le \EE_{\omega\sim \sigma}S(\sigma, \omega)$. In addition, if the inequality holds strictly for any $\hat\sigma\neq \sigma$, the scoring rule $S$ is strictly proper. 
\end{definition}
By definition, reporting the true belief maximizes the payment for the agent. 
Following the revelation principle~\cite{myerson1979incentive}, we can argue that any equilibrium with possibly untruthful report of belief can be implemented by an equilibrium with truthful report of belief. This means that the principal's optimal scoring rules can be assumed  proper without loss of generality, and we can thereby restrict the reporting scheme $\nu$ to the truthful ones --- we state the revelation principle for the information acquisition model in the following lemma and defer its proof to \S\ref{sec:proper scoring rule}. 
\begin{lemma}[Revelation principle]\label{lem:revelation}
    There exists a proper scoring rule $S^*$ that is an optimal solution to \eqref{eq:stackelberg-1-concise}.
\end{lemma}

In the sequel, we let $\cS$ denote the class of proper scoring rules with bounded norm $\nbr{S}_\infty\le B_S$. When $S\in\cS$, the agent's best report scheme is $v^*(S, \sigma)=\sigma$ since being truth-telling maximizes the payment, and the principal's best decision policy $\iota^*$ can be simplified to $a^*(\sigma):=\iota^*(S, \sigma, k)$ since $S$ and $k$ adds no information to the hidden state given $\sigma$. 
Using the notations $u(\sigma)=\EE_{\omega\sim\sigma}u(a^*(\sigma), \omega)$, $S(\sigma)=\EE_{\omega\sim\sigma}S(\sigma, \omega)$,  and $k^*(S)=\mu^*(S)$, we can transform the optimization program in \eqref{eq:stackelberg-1-concise} into
\begin{align}\label{eq:stackelberg-2}
&\max_{S\in\cS}\quad \EE_{\sigma\sim q_{k^*(S)}} \sbr{u(\sigma) - S(\sigma)},\\
&\mathrm{s.t.}\quad k^*(S)\in\argmax_{k\in[K]} \EE_{\sigma\sim q_{k}} S\rbr{\sigma} - c_{k}, \notag 
\end{align}
where we will denote the agent's utility function as $g(k,S)\defeq \EE_{\sigma\sim q_{k}} S\rbr{\sigma} - c_{k}$ and the principal's utility function under the agent's best response $k^*(S)$ as $h(S) \defeq \EE_{\sigma\sim q_{k^*(S)}} \sbr{u(\sigma) - S(\sigma)}$. This optimization program can be solved efficiently, e.g., by solving for  the optimal proper scoring rule that induces each of the agent's actions as the best response.  
And we will revisit \eqref{eq:stackelberg-2} in the online learning process where the information structure, i.e., $q_k$ and $c_k$, is unknown to the principal. 

\paragraph{Contract Design as a Special Case of Scoring Rule Design.}
We also remark that our model is fully capable of modeling the standard contract designing problem.
If the information structure is full-revealing (e.g., $\sigma$ is a point belief), our model with the endogenous states degenerates to a standard contract design problem, where the scoring rule $S$ becomes a contract as a mapping from (truthfully reported) outcome to payment. We defer the details to \S\Cref{sec:related to contract}.

% The scoring rule can be viewed as a special case of contracts in information acquisition \citep{papireddygari2022contracts}. 
% The agent is not guaranteed to report her actual belief under a general scoring rule. However, there is a class of proper scoring rules under which the agent is encouraged to be truth-telling \citep{gneiting2007strictly, dawid2007geometry}. 

% Let $S$ be a proper scoring rule and fix the agent's policy $\mu(\cdot)$ for action selection. For a reporting scheme $\nu$ and any true belief $\sigma$, it follows from definition \ref{def:PSR} that
% \begin{align*}
%     g^{\mu,\nu}(S)= \EE_{\omega\sim\sigma} S(\nu(S, \sigma, \mu(S)), \omega) - c_{\mu(S)}\le \EE_{\omega\sim\sigma} S(\sigma, \omega) - c_{\mu(S)}.
% \end{align*}
% Therefore, the agent's expected payment is maximized by always being truth-telling about her belief under the class of proper scoring rule.
% In the following, we let $S(\hat\sigma, \sigma)=\EE_{\omega\sim\sigma}S(\hat\sigma, \omega)$. 
% To give an example of proper scoring rules, let us consider the binary hidden state space $\Omega=\{0, 1\}$ where the class of proper scoring rules admits the Schervish representation \citep{gneiting2007strictly}, i.e., $S(p, 1)=G(p) + (1-p) G'(p)$ and $S(p, 0)=G(p)-p G'(p)$ where $p\in[0, 1]$ and $G:[0, 1]\rightarrow\RR_+$ is a convex function.
% Intuitively, the expected payment of a proper scoring rule $S$ given belief $\sigma$ and report $p$ is $S(p, \sigma)=G(p)+(\sigma-p)G'(p)$, which corresponds to the supporting line of $G$ at $p$.
% In this example, the convexity of $G$ guarantees that $S(p, \sigma)=G(p)+(\sigma-p)G'(p)\le G(\sigma)=S(\sigma, \sigma)$.
% Moreover, the next observation in Lemma \ref{lem:revelation} suggests that restricting to the class of proper scoring rules does not incur any loss of generality for the principal's purpose.
% \begin{lemma}[Revelation principle]\label{lem:revelation}
%     There exists a proper scoring rule $S^*$ that is an optimal solution to \eqref{eq:stackelberg-1} if the agent is truth-telling under any proper scoring rule.
% \end{lemma}

% % The power of a proper scoring rule is thus obvious -- not only paying the agent the exact amount \emph{any} scoring rule is capable of, but also encouraging truth-telling from the agent. 
% Following Lemma \ref{lem:revelation}, the principal's optimal scoring rule lies within the class of proper scoring rules $\cS$ with bounded norm $\nbr{S}_\infty\le B_S$.
% One concern about the use of proper scoring rules is that being truth-telling might not be the unique maximizer to the agent's utility. However, we note that the class of proper scoring rules is a convex hull with strictly proper scoring rules as the interior. Thus, adding an infinitesimal portion of a \emph{strictly proper} scoring rule to any \emph{proper} scoring rule always yields a \emph{strictly proper} scoring rule. In this sense, we can safely make the assumption that the agent always reports the true posterior under a proper scoring rule.
% To simplify our discussion, we introduce the concept of equivalent proper scoring rules.
% \begin{definition}[equivalence of proper scoring rules]
% we say that $S, S'\in\cS$ are equivalent proper scoring rules if $\EE_{\omega\sim\sigma}S(\sigma, \omega)=\EE_{\omega\sim\sigma}S'(\sigma, \omega)$ for any $\sigma\in\Delta(\Omega)$.
% \end{definition}
% As it turns out that even within the class of proper scoring rules, there might be more than one scoring rule resulting in the same payment for any $\sigma\in\Sigma$. 

 
% If $G$ is not smooth at $p_0$, then we have a class of proper scoring rules corresponding to multiple sub-gradient of $G$ but having the same payment $G(p)$. With a little abuse of notation, we use $S(\sigma)=\EE_{\omega\sim\sigma}S(\sigma, \omega)$ to represent the proper scoring rule class that is equivalent to $S(\sigma, \omega)$ and $\cS=\cbr{S(\sigma)}$ to represent all the equivalent proper scoring rules.

\subsection{Online Learning to Acquire Information}
We now formalize the online learning problem of solving the optimal scoring rule for information acquisition. We consider the situation where the principal only has knowledge of her utility function $u$ 
\footnote{Since the utility and the hidden state are both known to the principal at the end of each round, if the agent is truth-telling about her belief, the utility function can be efficiently learned. To simplify our discussion, we consider $u$ to be known by the principal.} and is able to observe the agent's action $b_{k}$
\footnote{In cases where the principal cannot observe the agent's action, there are still ways to distinguish different actions. For instance, when $q_k$ has different support for different $k$ and the agent is truth-telling about her belief under proper scoring rules, the principal is able to learn the support for a particular agent's action by repeating the same scoring rule multiple times. The next time the agent chooses the same action, the principal will be aware. But in general, learning the optimal scoring rule without observing the agent's action still remains a hard problem.}. In Table \ref{table:info}, we summarize all the information types discussed above together with their availability. 
\iffalse 
\begin{table*}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
    \hline
    Public Info & Agent's Private Info & Principal's Private Info & Delayed Info\\
    \hline
    {$S_t, b_{k_t}, \hat\sigma_t$} & $\{c_k\}_{k\in[K]}, \{q_k\}_{k\in[K]}, o_t, \sigma_t$ & $u$ & $w_t$\\
    \hline
\end{tabular}
\caption{Table for the information types. Here, \say{Public Info} refers to the information that is observable to both the principal and the agent at each round, \say{Agent's Private Info} refers to the information that is private to the agent, \say{Principal's Private Info} refers to the information that is private to the principal, and \say{Delayed Info} only contains the hidden state $\omega_t$, which is unobservable when round $t$ proceeds but revealed when round $t$ terminates.}
\label{table:info}
\end{table*}
\fi 
Given $H_{t-1}=\cbr{(S_\tau, k_\tau, \sigma_\tau, \omega_\tau)}_{\tau\in[t-1]}\in\cH_{t-1}$ as the history observed by the principal before round $t$, the principal is able to deploy a policy for the next round's scoring rule $\pi_{t}:\cH_{t-1}\rightarrow \cS$.
Hence, the data generating process is described as the following,
\begin{align}\label{eq:data generating}
    p^\pi(S_t, k_t, \sigma_t, \omega_t\given H_{t-1}) &= \ind\rbr{S_t=\pi_t(H_{t-1}), k_t=k^*(S_t)}\cdot q_{k_t}(\sigma_t) \cdot \sigma_t(\omega_t).
\end{align}
The regret for the online policy $\pi=\cbr{\pi_t}_{t\in[T]}$ is defined as,
\begin{align*}
    &\Reg^\pi(T)
    % \nend&\quad
    \defeq T\cdot \max_{S\in\cS} h(S) - \EE_\pi\sbr{\sum_{t=1}^T u(a^*(\sigma_t), \omega_t) - S_t(\sigma_t, \omega_t)}, 
\end{align*}
where the expectation is taken with respect to the data generating process. We aim to develop an online policy $\pi_t$ that learns the optimal scoring rule with small regret. 
% we remark that requiring the agent to report her action is without loss of generality when the agent is telling the truth. In cases where the agent is not required to report her action, the principal can always repeat the same scoring rules multiple rounds and distinguish the agent's different  actions via comparing the reports. Specifically, if $q_k$ and $q_{k'}$ are different in the sense $\nbr{q_k - q_{k'}}_1\ge \eta$ for any $k\neq k'$, it is sufficient to tell two different actions with high probability after $\cO(M/\eta^2)$ repetitions.

% Now we specify the agent's best response. 
% Based on our previous discussion,
% the best response of the agent under proper scoring rule $S\in\cS$ can be denoted by tuple $\BR{S}=\rbr{k^*(S), \hat\sigma^*}$ where $\hat\sigma^*=\sigma$ under proper scoring rules.
% To study the  agent's best action $k^*(S)$, 
% note that we consider the action set $\cB$ and the observation set $\cO$ to be finite. As a consequence, the set of possible posteriors $\sigma$ is also finite, and we denote it as $\Sigma\subset\Delta(\Omega)$ with $\abr{\Sigma}=M \le K \cdot \abr{O} + 1$.
% Consider $q_k\in\Delta(\Sigma)$ as the distribution of posterior $\sigma$ if the agent chooses action $b_k$.
% The agent's prior belief when choosing $b_k$ can then be written as,
% \begin{align*}
%     p(\omega\given b_k) = \sum_{\sigma\in\Sigma}\sigma(\omega) q_k(\sigma).
% \end{align*}
% We let $g(k, S)$ denote the agent's expected profit by choosing action $b_k$ under scoring rule $S$.  Following the fact $\hat\sigma^*=\sigma$, we can characterize the agent's best response as
% \begin{align}
%     k^*(S)= \argmax_{k\in[K]\cup \cbr{0}}  g(k, S), \where g(k, S) \defeq \EE_{\sigma\sim q_k} S(\sigma) - c_k, \label{def:k^*}
% \end{align}
% where we recall $S(\sigma)=\EE_{\omega\sim\sigma}S(\sigma, \omega)$.
% When we talk about \say{best response}, we typically refer to the agent's best action $k^*(S)$ in the sequel.

% Upon receiving the agent's report, the principal will adopt the reported belief and chooses the best action $a^*(\sigma) = \argmax_{a\in\cA} \EE_{\omega\sim\sigma}u(a, \omega)$.
% In the sequel, we simply use $u(\sigma)\defeq\max_{a\in\cA} \EE_{\omega\sim\sigma}u(a, \omega)$ for the maximal utility achievable for the principal based on the report $\sigma$.
% and $S(\sigma)\defeq\EE_{\omega\sim\sigma}S(\sigma, \omega)$ for the expected payment received by the agent.
% After the principal carries out her action, the hidden state $\omega$ is revealed. The principal receives utility $u(a^*(\sigma), \omega)$ and pays the agent a total amount of $S(\sigma, \omega)$.
% Let $h(S)$ denote the principal's expected profit.
% Under the best response of the agent, the principal aims to find the best proper scoring rule $S^*$ that maximizes her expected profit, 
% \begin{align}
%     S^*\in \argmax_{S\in\cS} h(S), \where h(S)=\EE_{\sigma\sim q_{k^*(S)}} \sbr{u(\sigma) - S(\sigma)}.\label{def:S^*}
% \end{align}
