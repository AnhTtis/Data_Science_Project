%!TEX root = icml-main.tex

\section{Introduction} \label{sec:intro} 

Delegated information acquisition is a widely popular situation where one party (known as the principal) wants to acquire some information that assists decision-making, and thus hires another party (known as the agent) to gather information on its behalf.
Consider a portfolio manager who aims to learn the potential of a company in order to decide whether to invest in its stock. The manager hires an analyst, who spends some effort to conduct the research, hands in a report to the manager, and gets payment according to the report she writes.
Based on the report, the manager makes the investment decision and earns profit if the stock rises.
The level of effort the analyst puts into the research affects the quality of the information gathered, i.e., the report, and her own cost incurred in conducting the research.
As a result, a rational analyst would choose an effort level that maximizes her own profit -- the difference between the payment and the cost.
Whereas the manager also wants to maximize its own profit in expectation, which is given by the expected gain from the investment, subtracted by the payment.
Knowing that the analyst is rational, the goal of the manager is to design a payment rule that incentivizes the analyst to spend a proper amount of effort, such that the acquired information leads to the investment decision that maximizes the portfolio manager's profit.


Mathematically, such a problem can be modeled under the principal-agent model \citep{laffont2009theory}, where the principal wants to know the state $\omega $  of a stochastic environment,
 and the information acquired by the agent is a distribution $\hat \sigma $ over the state space $\Omega$, also known as the reported belief. 
The game between the principal and the agent is as follows. 
At the beginning, the state is not realized and unknown to both parties. 
The principal 
chooses a scoring rule $S$ \citep{savage1971elicitation, gneiting2007strictly} as the payment rule and  presents it to the agent. 
The agent chooses among $K$  effort levels by selecting an action $b_k\in \{b_1,\ldots, b_K\} $   at a cost $c_k$. Then $b_k$  determines
the joint distribution $p(\omega, o \given b_k)$ of the state $\omega$ and an observation $o$.  
Based on such a conditional distribution  and the realized value of $o$, the agent reports $\hat \sigma$ to the principal. 
Based on such acquired information, the principal chooses an action $a \in \mathcal{A}$. 
Finally, the state $\omega$ is revealed, the principal receives utility $u(a, \omega)$ and pays the agent $S(\hat \sigma, \omega)$. 
Here, the payment to the agent is determined by the  scoring rule $S$, which quantifies the value of the   reported belief by comparing it  with the realized state. 

More generally, our model is a general Stackelberg game with information asymmetry \citep{von1952theory, mas1995micro}, where the agent knows the distribution of the state while the principal does not. 
The principal first  announces a scoring rule $S$. Then the principal chooses an effort level $b_k$ which maximizes her own profit and reports a  belief $\hat \sigma$. That is, $b_k$ is the best response   of the agent to $S$. 
The expected profits of both the principal and the agents are functions of $S$, $b_k$, and $\hat \sigma$. 
From such a perspective, designing the  scoring rule that optimally elicits information is equivalent to finding the strong Stackelberg equilibrium of such a Stackelberg game. 


In this work, we focus on the online setting of such a principal-agent model.
In particular, we aim to answer: 
%In particular, we aim to answer the following question: 
\begin{center}
     \emph{From the perspective of the principal, 
     how to learn the optimal scoring rule \\ by interacting with a strategic agent?} 
\end{center} 


The online setting comes with a few challenges. 
First, as the agent is strategic, the reported belief $\hat \sigma$ might be untruthful, or even arbitrary. 
Second, similar to other online learning problems such as bandits \citep{lattimore2020bandit}, we need to explore the stochastic  environment.
More importantly, in our problem, the distribution of the state is determined by the action $b_k$ of the agent, which is beyond the control of the principal. Thus, any successful learning algorithm needs to execute scoring rules that incentivize the agent to explore her action space. 
Third, both the cost $c_k$ and the distribution $p(\omega, o \given  b_k) $ are unknown to the principal. 
In other words, the profit functions of both the principal and the agent are unknown and needs to be estimated from the online data. In particular, this implies that the best response of the agent, as a function of the principal's scoring rule, is unknown. To find principal's optimal scoring rule, we need to know how to incentivize the agent to choose the most favorable $b_k$ for principal, which requires learning the best response function. 

 

We tackle these challenges by introducing a novel algorithm, OSRL-UCB, which leverages  proper scoring rules \citep{gneiting2007strictly}, the principle of  optimism in the face of uncertainty \citep{auer2002finite, lattimore2020bandit}, and the particular constrained optimization formulation of our principal-agent model. 
In particular, to elicit truthful information, we prove a revelation principle~\cite{myerson1979incentive} that shows that it suffices to only focus on the class of proper scoring rules (Lemma \ref{lem:revelation}). 
Then we show that the principal's profit maximization problem can be written as a $K$-armed bandit problem, where the reward of each arm $h^{k,*}$ is the optimal profit  of the principal when the best response of the agent is fixed to $b_k$ (Equation \eqref{eq:bandit problem}). The value of $h^{k, *}$ is determined by the optimal objective of a constrained optimization problem --- finding a proper scoring rule $S^k$ that maximizes the principal's profit, subject to the constraint that the best response to $S^k$ is $b_k$.
Furthermore, we show that  $h^{k, *}$  can be equivalently written as the optimal value of a 
 constrained optimization linear program (LP) involves the unknown pairwise cost  differences and the distribution of the truthful belief induced by $b_k$ (Equation \eqref{eq:cV LP}). 
Following the optimism principle, we aim to construct upper confidence bounds (UCB) of each $h^{k,*}$ and incentive the agent to pick the action that maximizes the UCB. 
To this end, we construct confidence sets  for the pairwise cost differences and belief distributions  based on the online data, and obtain a UCB of $h^{k, *}$ by solving an optimistic variant of the LP by replacing the unknown quantities by elements in the corresponding confidence sets (Equation \eqref{eq:LP_k}). Furthermore, the optimal solution to such an optimistic LP, which is a scoring rule, might violate the condition that its  best response is $b_k$. To remedy this, we devise a conservative modification scheme which simultaneously guarantees desired best response and  optimism with high probability. 
Finally, we prove that the proposed algorithm achieves a $\tilde \cO  (K^2   \mathcal{C}_{\mathcal{O}}  \cdot T ^{2/3})$ sublinear   regret upper bound  after $T$ rounds of interactions. 
Here $K$ is the number of effort levels of the agent and $\mathcal{C}_{\mathcal{O}}   $ is the number of all possible observations, and $  \tilde \cO$ omits logarithmic terms.
A key feature of our regret bound is that it is independent of the number of states of the environment.  
%Such a regret improves upon the $\mathcal{O}(T^{3/4}) $ regret in \citet{camara2020mechanisms}, although their model is more general than ours. 
%{\color{red}Furthermore, our regret bound can be translated into a $\mathcal{O}(K^3/\epsilon^3)$ sample complexity bound for learning an $\epsilon$-optimal scoring rule.
%{\color{red}
%Finally, as we will show in \ref{}, our principal-agent model for delegated  information elicitation  includes contract design as a special case. 
%As a result, our OSRL-UCB algorithm can be readily applied to contract design with a $\mathcal{O}(T^{2/3})$ regret. How to compare?}


\iffalse 
\begin{itemize}
    \item background -- example -- delegated information elicitation. 
    \item introduce the formulation -- a adaptation of principal agent model. What are the observations, actions, and learning objectives of the principal and agent. Stackelberg game. Exogenous effect can be a feature of our model. 
    \item introduce our problem. (1) the population problem is rewrite as a simpler optimization problem thanks to the revelation principle. (2) consider the online setting where the reward functions are unknown, regret minimization
    \item challenges: (a) exploration, (b) unknown cost and cannot control the agent's action (the agent's optimization problem is unknown), (c) the information structure is unknown $(q_k, k \in [K])$ unknown
    \item How we solve the problem: (a) reformulate as an optimization problem $\max_{k} h^k$, where   $h^{k}$ is optimal profit of the principal when the best response of the agent is $b_k$. Finding the value of each $h^k$ involves another optimization problem, where the feasible set is determined by an unknown best response function and the objective function involves an expectation with respect to the unknown distribution $q_k$. Suppose we know each $h^k$, the problem is reduced to a multi-armed bandit problem. Thus, we propose an algorithm that applies the UCB algorithm on estimates of $h^k$, whose construction is tailored to the problem structure. In specific, to estimate $h^k$, we estimate $q_k$ and the feasible region separately. Here $q_k$ is estimated  based on the empirical distribution of the beliefs. The estimation of the feasible region is more delicate. In particular, we propose to learn the decision boundary of the best response function via binary search with estimated cost differences. Furthermore, to overcome the estimation error of the decision boundaries, we employ a conservative estimate of the feasible set, which is a subset of the true feasible region with high probability. 
    \item summarize what we have talked about: 
    The logic of the algorithm is quite simple:
    
    1. we reformulate the problem as $max_k h^k$ -- so the problem is reduced to a multi-armed bandit problem where the \say{reward function} of arm $k$ is $h^k$. 
    
    2. Here $h^k$ is defined by a constrained optimization problem where the objective function involves an expectation with respect to the unknown distribution $q_k$ and the constraint set involves $q_k$ and the unknown cost $c_k$.
    
    3. Assume the model is known ($q_k$ and $c_k$ known), $h^k$ can be solved by an LP involving $q_k$ and cost difference $C(k, k')$.
    
    4. Following the UCB / principle of optimism in the face of uncertainty, to encourage exploration, we want to construct an upper confidence bound of $h^k$.
    
    5. To this end, we need to first construct confidence sets of $q_k$ and $C(k, k')$ and then solve an optimistic variant of LP by maximizing $\tilde q, \tilde C $ in Confidence Set, $LP(\tilde q, \tilde C)$.
    
    6. Solving this optimistic LP and then maximize over $k$ gives $k_t^*, S_t^*$, where $k_t^*$ is the desired action of the agent in the $t$-th round and $S_t^*$ is a learned scoring rule.
    
    7. However, the solution of the optimistic LP (which is a scoring rule) need to satisfy the condition that its best response is $k$. That is, $S_t^*$ should induce $k_t^*$. However, this might not be true. To remedy this, we propose a **conservative modification** by averaging  $S_t^*$ with $\tilde S_{k^*_t}$ (which is a scoring rule that induces $k_t^*$). And we use binary search to learn the weights of averaging.
    8. The conservative modification enjoys the property that it induces $k_t^*$ with high probability (As shown in Lemma X).
    \item Theoretical results: $T^{2/3}$ regret bound. 
\end{itemize}
\fi 

\iffalse 
We would like to study the problem of optimally acquiring information. For example, a producer would like to employ a consultant to conduct the market survey of the buyer's demand for a better pricing of its product. The producer faces a decision problem with payoff relevant to the state of the demand, i.e., the value distribution. While the producer may know the general value distribution of its customer, it would like to conduct first-order price discrimination using more fine-grained knowledge, such as the value of each customer groups. Hence, the produce would like to pay for these extra information, a prediction on the value distribution for a specific group, in order to design a profit maximizing price for its product. 
On the today's Internet platform, such game could be repeating for thousands of times every day --- the consultant may be a search engine or social media that are tracking the user's interest in real time, while the producer may be an advertiser who would like to acquire valuable information on the user groups and decide which specific group of users to target. 
% This game may be dynamically repeated. In each round, the producer offers a price of the information to the consultant, sets the price on the product, and receives profit. The producer may make decisions based on historical observations. 

The only way to incentivize high-quality or truthful prediction from the consultant is to pay him based on the comparison between the reported distribution and the realized state. By revelation principle, it is without loss to reward the consultant with a proper scoring rule, which incentivizes the consultant to report his true information at hand. 
However,  incentivizing the consultant to report truthfully with a scoring rule is not enough when the information is costly. The consultant can always  report the prior distribution without paying any cost on learning. The work on the design of scoring rules \citet{li2022optimization,neyman2021binary,10.1145/3490486.3538261} designs optimal scoring rules that can incentivize a strategic agent to pay a learning cost and report more precise information.  These papers assume the information structure is known to the principal. In real life, the principal often faces a situation where she does not have knowledge about the information structure and information cost of the agent. In our project, we try to solve this question by designing a learning algorithm for the principal, which learns the optimal scoring rule. 

\paragraph{endogenous effect}
The principal would like to design scoring rules to induce the agent to take favorable actions and acquire relevant information for her decision. This corresponds to the application scenarios where the consultant's market survey could influence on the consumers' value proposition.

\fi 

%\subsection{Our Results and Contributions}


% \input{related-work-brief.tex}
\input{relatedwork.tex}
