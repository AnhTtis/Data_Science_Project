
Deep neural networks currently constitute the core of several AI systems that support decisions in many socially important tasks like the hiring process, healthcare diagnosis, and law enforcement. Despite their efficacy in aggregate, it has become apparent that they 
can learn to encode subtle biases that disproportionately disadvantage particular sub-populations (\eg based on age, gender, ethnicity, etc.) \cite{buolamwini2018gender,berk2021fairness,seyyed2021underdiagnosis,obermeyer2019dissecting}. The causes of this unfairness
are many, from amplifying bias that already exists in the training data \cite{wang2020towards}, to learning spurious correlations \cite{geirhos2020shortcut}. However, the end result is the same: AI systems may exacerbate rather than alleviate social problems of inequality and discrimination. This issue has motivated a growing body of research in fairness interventions \cite{wang2020towards,zietlow2022leveling,park2022fair} â€” algorithms that are particularly designed to optimize some notion of fairness simultaneously with conventional learning objectives. Overall, the research in this area is still in its infancy and several factors have been overlooked till now. 

\begin{figure}[t]
\centering
%\includegraphics[width=0.95\linewidth]{teaser_new.pdf}
%\includegraphics[width=0.95\linewidth]{teaser_new.png}
\includegraphics[width=0.95\linewidth]{teaser_new.pdf}
\caption{
We start from a model that exhibits some degree of unfairness as evidenced by the Difference in Accuracy between protected groups (left). We exploit Cross-Domain (CD) learning to reduce the visual domain shift among groups and improve the generalization ability of the model, obtaining an unfairness mitigation effect (right).
}
\label{fig:teaser} \vspace{-5mm}
\end{figure}

One is the natural alignment of the fair learning problem with the more widely studied cross-domain (CD) learning challenge in computer vision. In the latter area, the goal is to produce models agnostic to the specific details of visual domains (\eg camera pose, lighting, image style)
to get generalization across them. 
By mapping visual domains to protected subgroups, we can see that the wealth of existing algorithms for promoting domain invariance could potentially benefit fairness (see Figure \ref{fig:teaser}). Thus, \textbf{our first contribution is to introduce a new fairness benchmark for computer vision}. It spans both face 
and medical images for classification and landmark detection tasks  
and compares 14 CD learning approaches alongside three state-of-the-art (SOTA) fairness algorithms.
%
We remark that current unfairness mitigation strategies in computer vision are restricted to classification problems. To overcome this limitation we include in our benchmark the task of landmark detection on face images of different demographic groups, as the bias related to sensitive attributes can affect the precision with which critical keypoints are located.




Another aspect on which there is still a lot of confusion and open debate is about how systems should be evaluated. There are multiple competing notions of fairness and ways to quantify it \cite{verma2018fairnessExplained}. 
Previous studies measure group fairness by accuracy difference between advantaged and disadvantaged subgroups \cite{hardt2016equality}. However, this goal has been criticized in philosophy and ethics literature \cite{mason2001egalitarianism}. Purely minimizing the gap between subgroup performance, may lead to choosing a model with worse accuracy for all subgroups, which is Pareto inefficient \cite{zietlow2022leveling} and violates the ethical principles of beneficence and non-maleficence \cite{beauchamp2003methods}. 
\textbf{As our second contribution, we analyze existing group fairness criteria and propose a novel evaluation metric named \emph{Harmonic Fairness} that properly aggregates overall performance and fairness level to assess the quality of a model.} After having explained its design process, we use it as the basis for all the evaluations of our benchmark. 

The results of our extensive experimental analysis confirm the effectiveness of CD methods and the relevance of the proposed metric. It highlights how less popular approaches in the CD literature provide a significant advantage for unfairness mitigation on different tasks, systematically outperforming the tailored SOTA approaches. Moreover, it  shows that CD models trained to overcome the bias due to one sensitive attribute can be beneficial also to prevent unfairness with respect to a different one. This transfer ability provides insights into the robustness of CD approaches for fairness applications.
Overall, our work paves the way for a more systematic analysis of fairness problems in computer vision and the related unfairness mitigation methods, providing reliable tools for future evaluations. 

