\noindent\textbf{Mitigating Unfairness.} 
The concept of fairness is very broad and has been largely discussed in the machine learning literature 
to support social, economic, and law choices  \cite{brennan2013emergence,van2019hiring,berk2019accuracy,jain2017weapons}. 
For our work, we focus on group fairness whose aim is to develop decision techniques 
that are invariant to differences across non-overlapping subsets of data defined by human-sensitive attributes like gender and ethnicity. 
%
Several studies have been conducted on face and medical image collections
to demonstrate how their biases lead to poor performing recognition models on some minority groups, progressively attracting the attention of the computer vision community \cite{kleinberg2018discrimination,zong2023medfair}. 
The existing strategies developed to mitigate unfairness have tackled the problem at three main levels depending on when they are applied within the learning process. 
As data unbalancing is among the main sources of unfairness, some methods act \emph{before training} by
collecting ad hoc datasets \cite{Kark2021fairface}, introducing strategic sampling  \cite{wang2020towards} or developing generative models that mitigate the imbalance through image synthesis \cite{zietlow2022leveling,kortylewski2019analyzing, wang2020towards,sattigeri2019fairness}. 
%
Other techniques have been designed to prevent models from capturing spurious data correlations \emph{during training}, by improving the 
representation learning procedure. Some approaches quantify these correlations and minimize them by aligning the representations of different demographic groups \cite{jung2021fair,wang2019racial}. 
Disentanglement-based approaches force orthogonality between target classes and sensitive attributes in order to disregard the latter during task learning \cite{dwork2018decoupled,lee2021learning,tartaglione2021end,sarhan2020fairness}. A similar goal is obtained by adversarial approaches that include dedicated modules to reduce the discriminability of semantic attributes \cite{dhar2021pass,kehrenberg2020null,diana2021minimax,gong2020jointly,wang2020towards}. 
%
Other approaches leverage feature distillation \cite{jung2021fair}, reinforcement \cite{wang2020mitigating} and contrastive learning \cite{park2022fair}. Very recently, a different family of methods proposed to identify and remove the critical parts of the models causing unfairness \cite{zhang2022recover,Savani2020intra}.
%
Finally, \emph{post-processing} techniques modify output predictions on the basis of fairness criteria \cite{Kim2019multiaccuracy}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{1mm}\noindent\textbf{Cross-Domain Learning.} 
In real-world conditions training and test data often belong to different domains. Cross-domain models are trained to provide good performance on any unseen target domain at test time (\emph{Domain Generalization}), or to adapt the training source knowledge to a specific, different but related target (\emph{Domain Adaptation}). 

% SSDG
The techniques proposed to tackle the challenging \textit{Single-Source Domain Generalization} (SSDG) setting extend regularization strategies usually applied in empirical risk minimization to prevent overfitting (\eg label smoothing \cite{szegedy2016rethinking}), and reshape them to face large source-target domain shifts. 
These include strategic dropout based on gradient observation \cite{huang2020self}, tailored model selection \cite{cha2021swad,izmailov2018averaging} or data-augmentation  to increase data variability \cite{wang2021learning}. 
%
% MSDG
When training samples are drawn from multiple domains,
robust models can be obtained via data-augmentation techniques 
\cite{xu2020adversarial}, or style-transfer-based approaches \cite{borlino2021rethinking}.
Other popular \textit{Multi-Source Domain Generalization} (MSDG) strategies 
align the source domain representations through Maximum-Mean Discrepancy (MMD) minimization \cite{li2018domain} or adversarial learning \cite{li2018deep,kim2021selfreg}. 
A similar aim is also pursued by 
multi-task models that combine supervised and self-supervised learning \cite{carlucci2019domain,bucci2021self}. 
Meta-learning solutions get prepared for the source-target discrepancy experienced at test time by emulating the same condition with data drawn from the different sources during training \cite{li2018learning, li2019episodic}. 

% USDA
In the \textit{Unsupervised Domain Adaptation} (UDA) setting the target data is available at training time but it is unlabeled. Possible strategies to close the domain gap are based on adversarial learning \cite{ganin2016domain} and feature alignment via MMD \cite{long2015learning} or via feature norms matching \cite{xu2019larger}. Pixel-wise adaptation can also be performed with
GAN-based techniques
\cite{CycleGAN2017, isola2017image}.
Clearly, MSDG and UDA share several solutions with slight differences  
due to the availability of multiple sources in one case, and source and target in the other. 
%
Finally, when the target is at least partially labeled, the setting is named \textit{Supervised Domain Adaptation} (SDA) 
and inherits most of the techniques developed for the more challenging UDA, SSDA and MSDA. Further constraints are eventually added to prevent overfitting in case of a very limited amount of labeled target data \cite{saito2019semi,li2021learning, yang2021deep, kim2020attract}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As it is clear from the overview provided above, \emph{mitigating unfairness} and \emph{cross-domain learning} already share some solutions, which is possible by interpreting sensitive attributes as domains. The literature on fairness has been largely dedicated to evaluating tabular data with low-capacity models, while related works in computer vision focus only on  classification tasks. On the other hand, previous works on cross-domain learning broadly cover object classification and detection, as well as semantic segmentation, re-identification and retrieval problems \cite{csurka_book}. Still, the task of regression has been significantly less studied \cite{DAR_ICML_21,wu2022distributioninformed} and only a few works proposed robust methods for keypoint localization across domains \cite{2d_keypoints_ricci, jiang2021regda,keypoint_eccv2022,kim2022unified}. 

\vspace{1mm}\noindent\textbf{Landmark Detection}. 
Locating specific points in an image is crucial for applications like face recognition \cite{juhong_2017_facerecog, anghelone_2022_tfld}, object tracking \cite{huang_2022_stenttracking} and pose estimation \cite{xu2022vitpose}, with practical use in fields such as medicine, sports, and robotics. Keypoints as object corners and edges or facial features like the eyes, nose, and mouth are indicated as landmarks. 
%
Older landmark detection methods treat the task as a regression problem, where the goal is to predict 
continuous pixel coordinates 
for each landmark \cite{feng2018wing, wayne2018lab}. Recent methods have obtained significant gains in accuracy and robustness by modeling the landmark locations 
through a spatial probability distribution and providing high-resolution 2D heatmaps as output \cite{mccouat2022contour, jiang2021regda}. We consider these heatmap-based strategies in studying the problem of fair landmark detection.

