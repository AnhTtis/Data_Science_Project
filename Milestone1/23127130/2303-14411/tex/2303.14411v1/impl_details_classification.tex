
\section{Implementation Details}

\subsection{Classification}
For all the experiments we follow \cite{zietlow2022leveling} in terms of base architecture, training details, and validation protocol. In particular, all the methods are built upon the ImageNet pre-trained ResNet-50 \cite{he2016deep} backbone optimized with Adam ($lr=10^{-4}$, batch size 64). 
As data augmentation, we use a center crop to 128x128 and RandAugment with $N=3$ and $M=15$. The validation is done every 500 iterations and the best model is selected based on the best $mGA$ computed on the validation set. Note that for g-SMOTE \cite{zietlow2022leveling} we used the GAN inversion model provided in  \cite{dinh2021hyperinverter}, pre-trained on CelebA: the official GAN code and weights used in \cite{zietlow2022leveling} have not been released by the authors.
Although there may be some debate around the use of generative approaches that are not tailored specifically to the medical task at hand, we decided to incorporate g-SMOTE into both the experiments on the COVID-19 Chest X-Ray and Fitzpatrick17k datasets for completeness.



We perform an extensive hyper-parameters search to find the best models for every approach considered in our benchmark. In particular, we apply the Random Search \cite{bergstra2012random} algorithm followed by a refinement stage in the following hyper-parameter intervals:

\smallskip\noindent \textbf{LSR} \cite{szegedy2016rethinking}: $\varepsilon$ is the coefficient used to smooth the ground truth labels such that $y_k^{LS}=y_k(1-\varepsilon)+\varepsilon/K$, where $K$ indicates the number of classes. $ \left \{ \varepsilon \in [0.1, 0.5] \right \} $ ;

\smallskip\noindent  \textbf{SWAD} \cite{cha2021swad}: $r$ is the tolerance rate used on the validation loss function when searching the interval in which the model's parameters have to be sampled and averaged. We didn't tune the optimum patience ($N_e$) and the overfit patience ($N_s$) since the overfitting behavior could be observed already after the very first validation. $ \left \{ r \in [0.1, 1.3] \right \}$ ;
 
\smallskip\noindent \textbf{RSC} \cite{huang2020self}: $f$ indicates the dropping percentage to mute the spatial feature maps, $b$ indicates the percentage of the batch on which RSC is applied. $ \left \{  f \in [10, 80] ,  b \in [10, 80] \right \}$ ;

\smallskip\noindent\textbf{L2D} \cite{lee2021learning}: $\alpha_1$ weights the contribution of the supervised contrastive loss function and $\alpha_2 $ weights the negative log-likelihood between the latent vectors of the source image $x$ and the generated one $x^+$ in the final objective function. $ \left \{  \alpha_1 \in [0.1, 3.0], \alpha_2 \in [0.1, 3.0] \right \} $ ;

\smallskip\noindent \textbf{DANN} \cite{ganin2016domain}, \textbf{CDANN} \cite{li2018deep}: $\lambda$ is the hyper-parameter that weights the reverse gradient during the backpropagation step, $\gamma$ controls the penalty assigned to the norm computed on the gradients of the domain discriminator.  $ \left \{ \lambda \in [0.01, 1.00], \gamma \in [0.01, 0.50] \right \} $ ;
 
\smallskip\noindent\textbf{SagNets} \cite{nam2021reducing}: $\lambda$ weights the adversarial loss function. $  \left \{ \lambda \in [0, 2] \right \} $ ;

\smallskip\noindent \textbf{AFN} \cite{xu2019larger}:  $\lambda$ trades off the feature-norm penalty and the supervised cross-entropy loss, $R$ is the value at which the norms of the extracted features are forced to converge to. $ \left \{ \lambda \in [0.01, 0.10], R \in [5, 100] \right \}$ ;
 
\smallskip\noindent \textbf{MMD} \cite{li2018domain}: $\gamma$ weights the MMD loss term in the final objective. $ \left \{ \gamma \in [0.1, 1.0] \right \} $ ;
 
\smallskip\noindent \textbf{Fish} \cite{shi2021gradient}: $\eta$ weights the gradient inner product.  $ \left \{  \eta \in [0.01, 0.10] \right \}$ ;

\smallskip\noindent \textbf{RelRot},  \textbf{RelRotAlign} \cite{bucci2020effectiveness}: $\alpha$ weights the importance of the self-supervised loss function in the total objective. $ \left \{  \alpha \in [0.1, 1.0] \right \} $ ;

\smallskip\noindent \textbf{SelfReg} \cite{kim2021selfreg}: $\lambda_{feature}$ and $\lambda_{logit}$ control, respectively, the in-batch dissimilarity losses applied to the intermediate features and the logits from the classifier. $ \left \{  \lambda_{feature} \in [0.1, 1.0], \lambda_{logit} \in [0.1, 1.0] \right \} $ ;
 
\smallskip\noindent \textbf{GroupDRO} \cite{sagawa2019distributionally}: $C$ is a model capacity, $\eta$ is the step size to update the weights in order to balance worst and best performing groups. $ \left \{  \eta \in [0.001, 0.05],  C \in [1, 10] \right \} $ ;

\smallskip\noindent \textbf{g-SMOTE} \cite{zietlow2022leveling}: $m$ in the number of nearest neighbors considered, $k$ is the number of random points chosen among the $m$ and $\lambda$ is the probability of selecting a batch from the original dataset during training. $ \left \{ m \in [2, 10], k \in [2, m], \lambda \in [0.1, 1.0] \right \}$ .