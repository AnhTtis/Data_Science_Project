
\subsection{Landmark Detection}

Throughout our experiments, we adopt the architecture and training procedures outlined in \cite{jiang2021regda}. To ensure consistency, we also use the validation protocol proposed in \cite{zietlow2022leveling}. Our approach employs an ImageNet pre-trained ResNet-18 \cite{he2016deep} backbone, followed by an upsampling head consisting of three 2D transposed convolutions with a dimension of 200 and a kernel size of 4. This head performs heatmap regression to determine the position of each landmark, resulting in an output tensor $\mathbf{\hat{Y}} \in \mathbb{R}^{200 \times 200 \times 68}$.
Our network is optimized using stochastic gradient descent (SGD) with a learning rate of 0.1, momentum of 0.9, weight decay of 1e-4, and a batch size of 32 for 35000 iterations. We incorporate a multi-step learning rate decay with a decay factor of 0.1, using iteration 22500 and 30000 as milestones.
To apply several augmentation sequentially we use the TorchLM library\footnote{\url{https://github.com/DefTruth/torchlm}}.
The augmentations are: random rotation (with angles ranging from -180 to 180 degrees), random horizontal flip (with a probability of 0.5), random shear (with x and y rescale factors of 0.6 and 1.3, respectively), color jitter (with brightness, contrast, and saturation set to 0.24, 0.25, and 0.25, respectively) and Gaussian blur (with a kernel size of 5 and $\sigma=(0.1, 0.8)$). We validate every 500 iterations and select the best model based on the highest $mGS$ score on the validation set.


We conduct an exhaustive search for optimal hyperparameters for all the approaches included in our benchmark. Specifically, we employ the Random Search algorithm \cite{bergstra2012random}, followed by a refinement stage, within the hyperparameter intervals as specified below:

\smallskip\noindent \textbf{AFN} \cite{xu2019larger}:  $\lambda$ trades off the feature-norm penalty and the supervised cross-entropy loss, $R$ is the value at which the norms of the extracted features are forced to converge to. $ \left \{ \lambda \in [1e-6, 0.10], R \in [5, 100] \right \}$ ;

\smallskip\noindent \textbf{DANN} \cite{ganin2016domain}: $\lambda$ is the hyper-parameter that weights the reverse gradient during the backpropagation step, $\gamma$ controls the penalty assigned to the norm computed on the gradients of the domain discriminator.  $ \left \{ \lambda \in [1e-6, 1.00], \gamma \in [0.01, 0.50] \right \} $;

\smallskip\noindent \textbf{RegDA} \cite{jiang2021regda}: \emph{margin} trades off between the KL divergence loss and the Regression Disparity loss. $t$ is a modifier of the magnitude of the Regression Disparity loss. $ \left \{ margin \in [1.0, 10.0], t \in [0.01, 1.0] \right \} $.