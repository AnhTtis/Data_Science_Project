

\section{Harmonic Fairness}
\label{sec:HF2}

To better deal with the peculiarities of the space defined by $mGA$ and $MGA$, we \emph{formalize relative distances} for each method with respect to its biased reference and introduce \emph{our new Harmonic Fairness} metric. 

\vspace{1mm}\noindent\textbf{Classification.} 
We focus on $MGA$ and $DA=MGA-mGA$, using the subscripts $b$ and $m$ to refer respectively to the baseline model and its unfairness-mitigated version. The relative differences are: \vspace{-1mm}
\begin{align}\small
\Delta DA  & = DA_b - DA_m\\
\Delta MGA & = MGA_m - MGA_b
\end{align}
with $\Delta DA, \Delta MGA \in \{-100,100\}$. Both these values will be high for an accurate and fair model. Thus, we combine them in the \emph{Harmonic Fairness} metric defined as:
\begin{equation}\small
HF = \frac{\Delta DA' \times \Delta MGA'}{\Delta DA' + \Delta MGA'}~,
\label{eq:HF}
\end{equation}
where we added an additional shift to the component values to avoid degenerate cases (dividing by 0): $\Delta DA'=\Delta DA+100$ and $\Delta MGA'=\Delta MGA+100$.
The minimal value $HF=0$ corresponds to having either $\Delta DA = -100$ or $\Delta MGA = -100$, which can be obtained with a very poorly defined model that reduces the performance (increasing $DA$ or decreasing $MGA$) rather than improving over the baseline. An unfairness mitigation model that maintains the same $DA$ and $MGA$ of the original baseline gets $HF=50$. Finally, every increase over this value corresponds to models able to symmetrically improve accuracy and fairness. 
 
Getting back to the points $\medstar$ and $\square$ analyzed before and always considering the $\triangle$ as a baseline, we obtain the meaningful ranking $HF_{\medstar}=54.62 > HF_{\square}=51.77$ 
which matches the expectations given the advantage of the former over the latter. 
%
We remark that $HF$ takes into proper account the model starting baseline and encourages a decrease in $DA$ and an increase in $MGA$ with different strengths depending on the baseline position, consequently shaping the space in various ways as shown by the isolines of $HF$ in the right part of Figure \ref{fig:fairness_explained}. Of course, the right way to benchmark multiple methods is by setting a fixed baseline model considered as a shared reference for all of them.
%


\vspace{1mm}\noindent\textbf{Landmark Detection.}
When dealing with landmark detection every data sample can be defined as $(\bx,a,\bY)$, where $\bY\in \mathbb{R}^{K\times 2}$ is a set of $\by_{1,\ldots,K}$ landmark coordinates. The reference metric for this task is the Normalized Mean Error ($NME$) calculated as:\vspace{-1mm}
\begin{equation}\small
NME(\bY,\hat{\bY}) = \frac{1}{K}\sum_{i=1}^K\frac{\|\by_i-\hat{\by_i}\|_2}{D}~,
\end{equation}
where $D$ is a normalization factor, usually chosen as the interocular distance for face images. 
We indicate with $SDR$ the Success Detection Rate calculated as the percentage of images whose NMEs is less than a given threshold. Symmetrically to what was done for classification, we define \emph{Max Group Success} ($MGS$) and \emph{Min Group Success} ($mGS$), respectively as the success rate of the best and worst performing protected groups. We consider also the difference between groups $DS = MGS - mGS$, and to assess the effectiveness of an unfairness mitigation model $m$ over the reference baseline $b$ we calculate: \vspace{-1mm}
\begin{align}\small 
\Delta DS & = DS_b - DS_m \\
\Delta MGS & = MGS_m - MGS_b \vspace{-1mm}
\end{align}
with $\Delta DS, \Delta MGS \in \{-100,100\}$. We then combine these values to get the $HF$ metric for landmark detection consistent with what defined for classification in equation (\ref{eq:HF}).

\vspace{1mm}\noindent\textbf{Rescaling.}
To better investigates fine-grained differences among the results of various unfairness mitigation methods we adopt a simple sigmoid rescaling: $\sigma(HF) = \frac{1}{1+\exp^{-HF+50}}$, with $\sigma(HF) \in \{0,1\}$. Hence, $\sigma(HF)>0.5$ will indicate a gain over the reference baseline.  

