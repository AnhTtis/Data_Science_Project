
Evaluating the group fairness of a classification model means assessing its performance on different population subgroups and comparing them.
Many criteria have been proposed for this \cite{Verma2018fairness,Watcher2021bias}.
In the following, we review the most used metrics in computer vision. 
%
We start from the basic definitions of True Positive Rate $TPR=TP/(TP+FN)$, False Positive Rate
$FPR=FP/(FP+TN)$ and Accuracy $Acc = (TP+TN)/(TP+TN+FP+FN)$.
In terms of conditional probabilities for data 
with two different attributes, it holds 
\begin{align}
TPR_{a=0} & = P(\hat{y}=1|y=1, a=0) \\
FPR_{a=0} & = P(\hat{y}=1|y=0, a=0) 
\end{align}
and their analogues for $a=1$.
%
The \emph{Difference in Equal Opportunity (DEO)}  measures fairness by 
\begin{equation} 
|P(\hat{y}=1|y=1, a=0) - P(\hat{y}=1|y=1, a=1)|~,
\end{equation}
so the maximum fairness is obtained for $DEO=0$ when $TPR_{a=0}=TPR_{a=1}$. 
%
The \emph{Difference in Equalized Odds (DEOdds)}  measures fairness by 
\begin{equation} %\small
\sum_{t\in\{0,1\}}|P(\hat{y}=1|y=t, a=0) - P(\hat{y}=1|y=t, a=1)|~,
\end{equation}
thus maximum fairness is obtained for $DEOdds=0$ when both $DEO=0$ and $FPR_{a=0}=FPR_{a=1}$. In other words, the decision of the classifier should be conditionally independent of the attribute, given the ground truth ($\hat{y}\perp a | y$).
%
Another basic way to consider the variation of the model's output over the subgroups identified by the attributes is via the \emph{Difference in Accuracy (DA)}: 
\begin{equation}
|P(\hat{y}=y|a=0)-P(\hat{y}=y|a=1)|~.
\end{equation} 
%
All these metrics evaluate the relative behavior of the classifier on data subgroups defined by different attributes but lose track of its absolute performance. This is a critical issue as shown by the practical example in the left part of Figure \ref{fig:hist_fair}. Although the performance of the two classifiers is different, with $C1$ better than $C2$, they have the same value of $DEO$.
Moreover, both $DEO$ and $DEOdds$ are minimized by a trivial classifier that predicts always $\hat{y}=1$. In that case, for all the attributes it holds $FN=TN=0$, so $TPR=FPR=1$ and $DEO=DEOdds=0$. Since the accuracy reduces to the Positive Predictive Value ($PPV=TP/(TP+FP)$), also $DA$ becomes uninformative.

\begin{figure}[tb]
\centering
\includegraphics[width=0.93\linewidth]{fairness_mGA-MGA_.pdf}
\caption{Visualization of the $[mGA,MGA]$ space with exemplar points. The bottom triangular part of the space is unfeasible as by definition $mGA$ is lower than $MGA$. The three plots on the right show the HF isolines when starting from different baseline methods indicated by the $\triangle$, $\lozenge$ and $\times$ points. }
\label{fig:fairness_explained}
\vspace{-4.5mm}
\end{figure}

Recent works have introduced the \emph{Minimum Group Accuracy (mGA)} as fairness criterion: rather than evaluating differences in statistics across groups, it considers the classification accuracy of the worst performing group \cite{zietlow2022leveling,diana2021minimax,martinez2020minimax}. 
The rationale of this metric is that by increasing $mGA$ we are certainly improving the overall accuracy. Hence we avoid the suboptimal condition of unnecessarily harming all groups to get a trade-off improvement in fairness measured by $DEO$ and $DEOdds$.
%
Still, when the goal is to evaluate whether a certain unfairness mitigation method was able to improve over the reference classifier, $mGA$ is not sufficiently informative as exemplified by the right part of Figure \ref{fig:hist_fair}. Here $a=0$ is the privileged attribute, thus the one that identifies the best group with the associated \emph{Maximum Group Accuracy (MGA)}. When moving from $C1$ to $C2$, $mGA$ increases and so does $MGA$. Although globally the classifier has improved, the disadvantaged group suffers even more for unfair treatment with respect to the privileged one as indicated by the increased $DA$.  


With these premises, we can state that fairness can be meaningfully assessed only together with prediction accuracy. Both their performance can be considered by looking at several bar plots jointly or at bi-dimensional plots as done in \cite{zietlow2022leveling}. However, interpreting them and making sense of multiple pieces of information at once is difficult, and defining a single score would facilitate rigorous quantitative evaluations. 
To this purpose, we can start from the space defined by $mGA$ and $MGA$. As shown in Figure \ref{fig:fairness_explained}, the bottom right triangular part of the space is an unfeasible region where  $mGA>MGA$. In the top right corner, the point with $[mGA,MGA]=[100,100]$ indicates the optimal utopia condition. The results of various methods can be collected in this space and ranked on the basis of the $L2$ Distance To the Optimum ($DTO$, \cite{zong2023medfair}) which sounds like a reasonable  metric for the score.
 
Let's focus for instance on the marked points in the figure and consider the biased reference classifier represented by $\triangle=[50,100]$. We expect a good unfairness mitigation method to keep the top $MGA=100$ result and improve $mGA$ to reduce the discrepancy among groups, thus moving horizontally towards the ideal point. The point $\medstar=[70.38,100]$ is a possible result for such an approach.
Differently, a method that trades off accuracy for fairness would decrease $MGA$ while improving $mGA$ to reduce $DA$ to zero. This behavior is exemplified by the 
point $\square=[79.06,79.06]$. It can be noticed that both $\square$ and $\triangle$ 
share the same Pareto efficiency level 
approximated by the circumference centered in $[0,0]$, as done in \cite{zietlow2022leveling}. Instead, $\medstar$ shows an advantage in efficiency, which is feasible as discussed in \cite{liu2023pushing}.
Despite their clear difference, the points $\medstar$ and $\square$ are equivalent according to $DTO$.
Thus, although $DTO$ keeps track of both $mGA$ and $MGA$ it might not be sufficiently informative to benchmark different unfairness mitigation approaches. The presented analysis also highlights the importance of taking as reference the performance of the
baseline to fully understand model comparisons.

