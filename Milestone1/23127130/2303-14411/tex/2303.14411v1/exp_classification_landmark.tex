\section{CelebA - 13 Attributes Experiment}
\input{tab_celeba_13}
\input{figure_landmark_curves_suppl}
\input{tab_classification_yo2mf}
\input{tab_landmark_transfer}
We present here for completeness the results of our analysis for the classification problem on the set of 13 attributes already used in \cite{ramaswamy2021fair,zietlow2022leveling}. According to \cite{ramaswamy2021fair}, these attributes are the most reliable out of the whole set of 40 CelebA attributes as they can be labeled objectively, without being ambiguous for a human.

Table \ref{tab:celeba13_5} shows the results on {CelebA} for the binary facial attribute prediction task. Specifically, each number corresponds to the average over 13 experiments done considering every binary attribute alone (\ie 13 different binary classifiers). 
The cross-domain models are presented in the different horizontal sections, sorted by family: Regularization, Adversarial, Feature Alignment, and Self-Supervised-based techniques. In all the cases we managed the gender sensitive attributes as domain. 
The bottom part of the table contains the SOTA fairness approaches. 

Made exceptions for SWAD, Fish, and FSCL, all the approaches exceed the baseline (\ie ${\sigma(HF)} > 0.500$).  The best approach is AFN which is able to increase both $mGA$ and $MGA$, decreasing $DA$. The second best is DANN confirming the effectiveness of adversarial techniques to deal with the fairness problem \cite{wang2020towards,wang2022fairness}. Considering the high baseline accuracy, the improvements of the different methods appear relatively small but they are consistent with the results presented in \cite{zietlow2022leveling} (Supplementary Table S1). 


As a final note, we share our initial surprise in collecting very low results for FSCL. This approach is based on contrastive learning and is designed to avoid encoding sensitive information in the learned embedding space, thus it is very much in line with the cross-domain logic. However as any contrastive learning approach, its limitation is in the amount of data needed to fully train the model which makes it ineffective in our experimental setting. 




\subsection{Landmark Detection - Further Analysis}

In the interest of comprehensiveness, we integrate the curves already presented in Figure 4 of the main submission by including here in Figure \ref{fig:NMEth_app} the results on both \emph{Skin Tone} and \emph{Age}. For the former the behavior of the studied methods is consistent in showing an advantage over the baseline regardless of the used metric.
This is not the case for the latter, as already discussed in the main paper. We also include the curves of ${\sigma(HF)}$ that show the same trend of $HF$ in terms of model ranking. 

Although no previous publication proposed an unfairness mitigation approach for landmark detection, GroupDRO might sound general enough to be applied also in this setting. This approach dynamically adjusts loss weights during optimization to prioritize the poorest-performing protected group. However, our investigation revealed that, even after a comprehensive hyperparameter search, the loss of the worst group decreases extremely slowly in landmark detection, and the method keeps focusing on it which ultimately makes it unable to obtain an improvement neither on the best group nor overall. The result is a high  Normalized Mean Error (NME) achieved by  GroupDRO during training and a consequent 0\% Success Detection Rate (SDR) on the test set. Notably, applying looser thresholds did not improve the situation, suggesting that the logic employed by GroupDRO may not be well-suited for landmark detection tasks.

\subsection{Model Transferability - Further Analysis}
We present here further results of the experiments performed to study the transferability of unfairness mitigation models. Specifically for the classification task, we consider \emph{age} as the initial sensitive attribute and we test the obtained model to evaluate whether it helps in mitigating unfairness with respect to \emph{gender}. From the results we observe that the advantage obtained with the gender-robust model on different age groups is not symmetric: the results in the right part of Table \ref{tab:transfer} show that none of the methods improve over the baseline. 
The transferability results look instead always very promising on landmark detection where a model trained to be fair on \emph{skin tone} is effective also in reducing the performance gap among different \emph{age} groups and vice-versa as shown in Table \ref{tab:transfer_landmark}.
Overall the possibility to reuse fair models on different sensitive attributes connects with the ability of the models to capture knowledge shared across them and generalize at deployment time to new social conditions with different ethical constraints. We find it an interesting aspect that gives rise to new research questions and deserves more attention in the future.

