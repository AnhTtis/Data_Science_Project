\documentclass[12pt]{article}
%\usepackage{amsmath}
\usepackage{graphicx}
%\usepackage{enumerate}
\usepackage{natbib} %comment out if you do not have the package
\usepackage{url} % not crucial - just used below for the URL 
\usepackage{bbm}

\usepackage{enumerate}
\usepackage{color}
\usepackage{float}
\usepackage{amsmath,amsthm,amssymb,amsfonts}
%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

\newtheorem{thm}{Theorem}
\newtheorem{remark}{Remark}
\newtheorem{lemma}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{prop}{Proposition}
\newtheorem{conj}{Conjecture}
\newtheorem{claim}{Claim}


\newcommand{\note}[1]{{\bf\textcolor{red}{[note: #1]}}}

\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf  An Effective Multivariate Normality Test via  Hessians of Empirical Cumulant Generating Functions}
 \author{Kwun Chuen Gary Chan
	%  	\thanks{
		%    The authors gratefully acknowledge the following grants \textit{DMS1711952, RGPIN/03124-2021, HKGRF-14300319, HKGRF-14301321    	
			%}}\hspace{.2cm}
	\\
	Department of Biostatistics, University of Washington,\\
	\\
	Hok Kan Ling\\
	Department of Mathematics and Statistics, Queen's University\\
	\\
	Chuan-Fa Tang\\
	Mathematical Sciences, The University of Texas at Dallas\\
	\\
	Sheung Chi Phillip Yam\\
	Department of Statistics, The Chinese University of Hong Kong
}
\date{}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf An Effective Multivariate Normality Test via  Hessians of Empirical Cumulant Generating Functions}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
%The text of your abstract.  100 or fewer words. (template said)
% submission website said <= 200 words
In this article, we propose a new class of consistent tests for $p$-variate normality. These tests are based on the characterization of the standard multivariate normal distribution, that the Hessian of the corresponding cumulant generating function is identical to the $p\times p$ identity matrix and the idea of decomposing the information from the joint distribution into the dependence copula and all marginal distributions. Under the null hypothesis of multivariate normality, our proposed test statistic is independent of the unknown mean vector and covariance matrix so that the distribution-free critical value of the test can be obtained by Monte Carlo simulation. We also derive the asymptotic null distribution of proposed test statistic and establish the consistency of the test against different fixed alternatives. Last but not least, a comprehensive and extensive Monte Carlo study also illustrates that our test is a superb yet computationally convenient competitor to many well-known existing test statistics. 
\end{abstract}

\noindent%
{\it Keywords:}  
moment generating function; consistency; goodness-of-fit test; $L^2$-statistic; empirical cumulant generating function
%3 to 6 keywords, that do not appear in the title

\vfill

\newpage
\spacingset{2} % DON'T change the spacing!
\section{Introduction}\label{sec:intro}
The normal distribution is  certainly one of the most important distributions in statistics that underlie many statistical procedures. Therefore, validation of the normality assumption behind the data is of fundamental importance and interest; to this end, there are a significant number of tests for both univariate and multivariate normality in the literature. For instance, \cite{mecklin2004appraisal} surveyed dozens of common multivariate normality tests and divided most of the tests into four categories: (i) procedures based on graphical plots and correlation coefficients; (ii) goodness-of-fit tests; (iii) test based on measures of skewness and kurtosis; (iv) and tests based on the empirical characteristic functions. Furthermore, there is still an ongoing research devoted in developing new normality tests. For example, \cite{ebner2020tests} reviewed the recent developments in tests for multivariate normality with an emphasis on asymptotic properties of several classes of weighted $L^2$-statistics, where these statistics are mostly based on empirical moment generating functions or empirical characteristic functions. 

%\cite{baringhaus1988consistent} and \cite{henze1990class} considered the statistic
%\begin{equation*}
%	\text{BHEP}_{n,\beta} = n\int |\Psi_n(t) - \Psi_0(t)|^2 w_\beta(t)dt,
%\end{equation*}
%where $\Psi_n(t) := n^{-1}\sum^n_{j=1}\exp(i t^\top Z_{n,j})$, $\Psi_0(t) = \exp(-\|t\|^2/2)$ and $w_\beta(t) = (2\pi \beta^2)^{-p/2}\exp(-\|t\|^2/(2\beta^2))$. \cite{henze2019new} considered a moment generating function analogue to the BEHP-test statistic. There are also tests based on characterization of normal distribution involving the moment generating function and the characteristic function; see \cite{henze2019characterizations}, \cite{henze2020testing}, \cite{dorr2021new}. For a more comprehensive review of these tests, see 
%


%The distribution of the test statistic is independent of the mean and variance matrix under the null hypothesis and the test is consistent against general alternatives. We provide extensive simulation studies to illustrate the finite sample performance. For the alternative distributions,



%\begin{remark}
%	\note{Change} This is in contrast to the test considered in \cite{henze2020testing}, where they proposed the following test statistic
%	\begin{equation*}
%		\int_{\mathbb{R}^p}\|\triangledown M^{(n)}(t) - t M^{(n)}(t)\|^2 w_\gamma(t) dt.
%	\end{equation*}
%	When $w_\gamma(t) = \exp(-\gamma\|t\|^2)$, a closed-form expression can be obtained. 
%\end{remark}
In this article, we introduce a novel test for multivariate normality based on a system of second order partial derivatives of the cumulant generating function and decomposing a joint distribution into the dependence copula and all marginal distributions. 
Let $X$ be a random vector on $\mathbb{R}^p$. Denote $\mathcal{N}_p$ to be the class of all non-degenerate $p$-variate normal distributions. Suppose that we observe a random sample of $X_1,\ldots,X_n$ having the same joint distribution as that of $X$. We consider the traditional and conventional problem of testing the null hypothesis:
\begin{equation*}
	H_0: \text{The law of $X$ belongs to } \mathcal{N}_p,
\end{equation*}
against the general alternative hypothesis:
\begin{equation*}
	H_1: \text{The law of $X$ does not belong to } \mathcal{N}_p.
\end{equation*}
Let $\overline{X}_n := n^{-1}\sum^n_{i=1}X_i$ be the sample mean and $S_n := n^{-1}\sum^n_{i=1}(X_i- \overline{X}_n)(X_i-\overline{X}_n)^\top$ be the (biased) sample covariance matrix. Let $S^{-1/2}_n$ denote the unique symmetric square root of $S^{-1}_n$. For the rest of this article, we assume that $n \geq p + 1$. 
Due to the absolute continuity of the multivariate normal distribution, $S^{-1}_n$ exists with probability one under $H_0$, see for example \cite{eaton1973non}. Therefore, under the alternative hypothesis, if $S_n$ is not invertible, we can simply reject the null hypothesis. Thus, from now on we assume $S^{-1}_n$ exists.
It is known that, under the null hypothesis, the distribution of the so-called scaled residuals $Z_{n,i} := S^{-1/2}_n(X_i - \overline{X}_n)$, $i=1,\ldots,n$, does not depend on the mean vector $\mu$ and covariance matrix $\Sigma$ of $X$; also see \cite{szkutnik2021comment} for details. In other words, the distribution of the scaled residuals when  $X$ is from any non-degenerate multivariate normal distribution is the same as that when $X \sim N_p(0, \textbf{I}_p)$. Thus, for test statistics that only involve the scaled residuals $Z_{n,i}$'s, under the null hypothesis, it suffices to consider the case $X\sim N_p(0, \textbf{I}_p)$, where $\textbf{I}_p$ denotes the $p\times p$ identity matrix.
Denote $M(t) := \mathbb{E}(e^{t^\top X})$ to be the moment generating function of $X$. The cumulant generating function of $X$ is also defined by $\Lambda(t) := \log M(t)$, for $t$ in the proximity of origin. Our proposed test relies on the following key observation: if $X \sim N_p(0, \textbf{I}_p)$, then
\begin{equation}\label{eq:normal_cgf_Hess}
	H_\Lambda (t) = \textbf{I}_p,\quad t \in \mathbb{R}^p,
\end{equation}
where $H_f$ denotes the Hessian matrix of a function $f$; clearly,  the converse is also true, namely if the Hessian of the cumulant generating function of a random vector is identically equal to $\textbf{I}_p$, then the random vector has a standard multivariate normal distribution.

In general, a random vector can deviate from the multivariate normality from two causes: (i) at least one marginal distribution of a component random variable is non-normal; or (ii) the copula structure of the random vector is not the Gaussian one; see for example \cite{nelsen2007introduction}.
 In fact, practitioners often look at univariate normal Q-Q plots, P-P plots (\cite{gan1990goodness}) and bivariate plots of the marginals taken any two at a time, by performing univariate tests on each of the marginal distribution as well as performing tests based on dimension reduction (e.g., a test of the squared radii $Z_{n,i}^T Z_{n,i}$'s); see Chapter 9 in \cite{thode2002testing}. These create multiple testing issues, and using Bonferroni rule to combine them as an adjustment can be conservative.
Motivated by the fact that a multivariate distribution can be determined by specifying its corresponding marginal distributions and dependence structure, for any  $t = (t_1,\ldots,t_p)^\top \in \mathbb{R}^p$, define the function $M^*(\cdot)$ via the decomposition
\begin{equation*}
	M(t) \equiv \left( \prod^p_{j=1} M_j(t_j) \right) M^*(t),
\end{equation*}
where $M_{j}(\cdot)$ is the moment generating function of the $j$th component of $X$. 
%That is, we define $M^*$ by
%\begin{equation*}
%	M^*(t)  := \frac{M_X(t)}{\prod^p_{j=1} M_{X_j}(t_j)}.
%\end{equation*}
%Then
%\begin{equation*}
%	\log M^*(t) = \log M_X(t) - \sum^p_{j=1} \log M_{X_j}(t_j).
%\end{equation*}
Further, if $M$ is twice differentiable, then
\begin{equation}\label{eq:H_log_M_star}
	H_{\log M^*}(t) \equiv H_{\log M}(t) - \sum^p_{j=1} H_{\log M_j}(t_j).
\end{equation}
Denote $\Lambda_j := \log M_j$ and $\Lambda^* := \log M^*$. We rewrite (\ref{eq:H_log_M_star}) as 
\begin{equation}\label{eq:H_star_lambda_D}
	H_{\Lambda^*}(t) \equiv H_{\Lambda}(t) - D(t),
\end{equation}
where $D(t)$ is a $p \times p $ diagonal matrix with elements $H_{\Lambda_1}(t_1),\ldots,H_{\Lambda_p}(t_p)$ in order.
%\begin{equation*}
%	D(t) = \left(
%	\begin{array}{cccc}
	%		H_{\Lambda_1}(t_1) & 0 & \cdots & 0\\
	%		\vdots & \vdots & \vdots & \vdots \\
	%		0 & 0 & \cdots & H_{\Lambda_p}(t_p) \\
	%	\end{array}
%	\right).
%\end{equation*}
Under the null hypothesis $H_0$, by (\ref{eq:normal_cgf_Hess}),
\begin{equation}\label{eq:decomposition}
	H_{\Lambda^*}(t) = \textbf{0}_{p\times p} \text{ and } D(t) = \textbf{I}_p, \text{ for all } t \in \mathbb{R}^p,
\end{equation}
where $\textbf{0}_{p\times p}$ is the $p \times p$ zero matrix. Our proposed test is based on an empirical version of the two identities of (\ref{eq:decomposition}).

%Note that
%\begin{equation}\label{eq:general_cgf_Hess}
%	H_\Lambda(t) =\frac{M(t) H_M(t) - \triangledown M(t)( \triangledown M(t))^\top}{M(t)^2}, \quad t\in \mathbb{R}^p.
%\end{equation}
Recall that under $H_0$, the scaled residuals $Z_{n,i}$'s are independent of the unknown $\mu$ and $\Sigma$, and they will resemble a random sample from $N_p(0, \textbf{I}_p)$. Let $M^{(n)}(t) := \frac{1}{n}\sum^n_{i=1} e^{t^\top Z_{n,i}}$, $	\triangledown M^{(n)}(t) := \frac{1}{n}\sum^n_{i=1} Z_i e^{t^\top Z_{n,i}}$ and $	H_{M^{(n)}}(t) := \frac{1}{n}\sum^n_{i=1} Z_{n,i} Z_{n,i}^\top e^{t^\top Z_{n,i}}$ be the empirical version of the unknown theoretical moment generating function, its gradient and Hessian based on the scaled residuals, respectively.
An empirical version $H_{\Lambda^{(n)}}$ of $H_{\Lambda}$ using the scaled residuals is then given by
\begin{align}\label{eq:H_Lambda_n}
	H_{\Lambda^{(n)}}(t) := \frac{M^{(n)}(t) H_{M^{(n)}}(t) - \triangledown M^{(n)}(t)(\triangledown M^{(n)}(t))^\top}{M^{(n)}(t)^2},
\end{align}
where $\Lambda^{(n)} := \log M^{(n)}$. The corresponding empirical version of $H_{\Lambda^*_n}$ is 
\begin{equation*}
	H_{(\Lambda^{(n)})^*}(t) = H_{\Lambda^{(n)}}(t) - D^{(n)}(t),
\end{equation*}
where $D^{(n)}(t)$ is the $p \times p$ diagonal matrix with elements $H_{\Lambda^{(n)},11}(s_1),\ldots, H_{\Lambda^{(n)},pp}(s_p)$. Here, $s_j = (0,\ldots,0,t_j,0,\ldots,0)^\top$, where $t_j$ is in the $j$th position of this $p$-vector. 
In view of (\ref{eq:decomposition}), a large value of the $L^2$-statistics $n\int \cdots \int_{\mathbb{R}^p} \sum_{i=1}^p \sum^p_{j > i} H^2_{(\Lambda^{(n)})^*, ij}(t) dt$ or $n\int \cdots \int_{\mathbb{R}^p} \sum^p_{i=1} (D^{(n)}_{ii}(t) -1)^2 dt$ is an evidence against $H_0$.
Since these two integrals do not generally admit a closed form expression because of the denominator $M^{(n)}(t)$ in (\ref{eq:H_Lambda_n}), we consider their discretized approximations and define the following statistics:
\begin{align}\label{eq:Hn_Dn_t}
	H^{(n)}_{N} = n\sum^N_{l=1} \sum^p_{i=1}\sum^p_{j>i} H^2_{(\Lambda^{(n)})^*, ij}(t_l) \quad \text{ and } \quad 
	D^{(n)}_{N} = n\sum^N_{l=1}\sum^p_{i=1} (D^{(n)}_{ii}(t_l)-1)^2,
\end{align}
where $\{t_1,\ldots,t_N\}$ is a collection of vectors in $\mathbb{R}^p$. For example, we may choose them randomly in the ball $\{t \in \mathbb{R}^p : \|t \| \leq R \}$ for some $R > 0$ and $\|\cdot\|$ is the usual Euclidean norm. Our simulation studies show that $R = 3$ will work well in most scenarios. To see why a moderate value (like $3$) can be a good choice, we can write (\ref{eq:H_Lambda_n}) as
\begin{equation}\label{eq:alternative_weight}
	H_{\Lambda^{(n)}}(t) = \sum^n_{i=1} Z_{n,i} Z_{n,i}^\top W_{n,i}(t) - \left(\sum^n_{i=1}Z_{n,i} W_{n,i}\right) \left(\sum^n_{i=1}Z_{n,i} W_{n,i}\right)^\top,
\end{equation}
where $W_{n,i}(t) := \frac{e^{t^\top Z_{n,i}}}{\sum^n_{j=1} e^{t^\top Z_{n,j}}}$ and $H_{\Lambda^{(n)}}(t)$ can be interpreted as a weighted estimate of the covariance matrix. Under $H_0$,
 $Z_{n,i}$ behaves like a random vector from $N(0,\textbf{I}_p)$ under $H_0$ so that all the components of $Z_{n,i}$ will be around $-3$ to $3$ most of the time. A larger value of $\|t\|$ tend to put more weights to extreme values of $Z_{n,i}$'s in (\ref{eq:alternative_weight}). Thus, a moderate value of $R$ can avoid $H^{(n)}_N$ and $D^{(n)}_N$ depend heavily on only a few extreme values of $Z_{n,i}$'s. On the other hand, if $R$ is  small, then the weights are more even and less information in the empirical cumulant generating function is used, which may result in a less powerful test (recall that the population version identities in (\ref{eq:decomposition}) hold for all values of $t \in \mathbb{R}^p$ under $H_0$).
 
We can interpret $H^{(n)}_{N}$ as capturing the overall deviation from multivariate normality dependence structure while $D^{(n)}_{N}$ focuses on the deviation from marginal univariate normality. Note that $D^{(n)}_N$ is still a function of the scaled residuals, so that it does not depend on the mean and covariance matrix of the normal distribution under $H_0$.
The computation of $H^{(n)}_N$ and $D^{(n)}_N$ is straightforward as we essentially only have to compute the scaled residuals and the empirical Hessian of the cumulant generating function given in (\ref{eq:H_Lambda_n}) at different values of $t$.
Since the magnitudes of $H^{(n)}_N$ and $D^{(n)}_N$ are different, to combine evaluative assessment  $H^{(n)}_N$ and $D^{(n)}_N$, we define our proposed test statistic to be
\begin{equation}
T^{(n)}_N = \max\bigg\{ \frac{H^{(n)}_{N} - \mathbb{E}^S( H^{(n)}_{N})}{SD^S( H^{(n)}_{N}) },   
	\frac{D^{(n)}_{N} - \mathbb{E}^S( D^{(n)}_{N})}{SD^S( D^{(n)}_{N}) }\bigg\},
\end{equation}
such that we reject $H_0$ for large values of $T^{(n)}_N$. Here, the superscript ``$S$" refers to the calculations based on $S$ simulations: $\mathbb{E}^S(H^{(n)}_N) := \frac{1}{S}\sum^S_{s=1} H^{(n)}_{N,s}$, $SD^S(H^{(n)}_N) := \sqrt{ \frac{1}{S-1}(\sum^S_{s=1}H^{(n)}_{N,s} -  \mathbb{E}^S(H^{(n)}_N))^2}$, and $H^{(n)}_{N,s}$ is computed as in $H^{(n)}_N$ but using the scaled residuals from the $s$-th random sample of $X_{1,s},\ldots,X_{n,s} \stackrel{i.i.d.}{\sim} N(0, \textbf{I}_p)$; that is, $\mathbb{E}^S(H^{(n)}_N)$ and $SD^S(H^{(n)}_N)$ are the estimated mean and standard deviation of $H^{(n)}_N$  from $S$ independent Monte Carlo simulations. Similar definitions and interpretation apply to $\mathbb{E}^S(D^{(n)}_N)$ and $SD^S(D^{(n)}_N)$. Since we shall find the critical values of the test statistic using simulation, these estimates can be obtained as a byproduct at the same time. 




For testing univariate normality, we base on the fact that $\Lambda''(t) = 1$ for all $t \in \mathbb{R}$ if $X \sim N(0, 1)$, and define its empirical and discrete approximate:
\begin{equation}\label{eq:univariate_TS}
	U^{(n)}_N := \sum^N_{l=1}((\Lambda^{(n)})''(t_l) - 1)^2,
\end{equation}
where for some $t_1,\ldots,t_N \in \mathbb{R}$,
\begin{equation*}
	(\Lambda^{(n)})''(t) := \frac{M^{(n)}(t)(M^{(n)})''(t) - (M^{(n)})'(t))^2}{(M^{(n)})^2(t)},
\end{equation*}
such that we reject $H_0$ for large values of $U^{(n)}_N$.
Under the null hypothesis, $T^{(n)}_N$ is independent of the unknown mean vector $\mu$ and covariance matrix $\Sigma$ for the multivariate case, and $U^{(n)}_N$ is independent of the unknown mean and variance for the univariate case. 
In Section \ref{sect:consistency}, we shall further show that our test is consistent especially when the moment generating function exists and is twice differentiable. In general, one cannot find a test that is uniformly powerful than all the other tests against all alternatives. In Section \ref{sect:simulation}, through an extensive simulation study, we see that our proposed test often has higher powers over various alternative distributions compared with some prevalent common tests as well as some recently formulated tests. In particular, for the univariate case, our test performs the best in various short-tailed symmetric and asymmetric distributions compared with other common normality test, including the well-known and powerful Shapiro-Wilk test (\cite{shapiro1965analysis}).  For the multivariate case, our proposed test outperforms other common tests in many of the short-tailed distributions and some of the long-tailed distributions. For other distributions, the proposed test tends to have powers in between those of different tests.

The organization of the paper is as follows. In Section \ref{sect:asy_null_dist}, we derive the asymptotic distribution of the test statistic $T^{(n)}_N$ under the null hypothesis $H_0$. Consistency of the newly proposed test statistic will be established in Section \ref{sect:consistency}. We provide a comprehensive Monte Carlo simulation study in Section \ref{sect:simulation}. Discussion and conclusion are given in Section  \ref{sect:conclusion}.
Technical proofs and additional simulation results are appended in the section of supplementary materials.


\section{Asymptotic Null Distribution}\label{sect:asy_null_dist}
In this section, we derive the asymptotic distribution of the test statistic $T^{(n)}_N$ under the null hypothesis that each sample $X_i$ has a nondegenerate multivariate normal distribution. Recall that our proposed test statistic is independent of the mean and covariance matrix of $X$. To derive the asymptotic null distribution, it therefore suffices to consider the case when $\mathbb{E}(X) = 0$ and $Var(X) = \textbf{I}_p$.

Let $M^{(n)}_0(t) := \frac{1}{n}\sum^n_{i=1} e^{t^\top X_i}$, $\triangledown M^{(n)}_0(t) := \frac{1}{n}\sum^n_{i=1} X_i e^{t^\top X_i}$ and $H_{M^{(n)}_0}(t) := \frac{1}{n}\sum^n_{i=1}X_i X_i^\top e^{t^\top X_i}$ be the empirical moment generating function, its gradient and Hessian based on $X_i$'s (not the scaled residuals $Z_{n,i}$'s), respectively. The corresponding population versions are $M_0(t) = e^{\|t\|^2/2}$, $\triangledown M_0(t) = e^{\|t\|^2/2} t$ and $H_{M_0}(t) = e^{\|t\|^2/2}(tt^\top +  \textbf{I}_p)$. Define $f_0:\mathbb{R}^p \times \mathbb{R}^p \rightarrow \mathbb{R}$, $f_1: \mathbb{R}^p \times \mathbb{R}^p \rightarrow \mathbb{R}^p$ and $f_2:\mathbb{R}^p \times \mathbb{R}^p \rightarrow \mathbb{R}^{p \times p}$ one by one as follows:
\begin{align*}
	f_0(x;t) &:= e^{\|t\|^2/2} \left\{-t^\top x - \frac{(t^\top x)^2}{2} + \frac{\|t\|^2}{2}\right\}; \\
	f_1(x;t) &:= e^{\|t\|^2/2} \left\{- (\textbf{I}_p + tt^\top)x -  \frac{1}{2} (2 \textbf{I}_p + tt^\top)(x x^\top - \textbf{I}_p)t \right\}; \\
	f_2(x;t) &:= e^{\|t\|^2/2} \bigg\{ - \frac{1}{2} (xx^\top - \textbf{I}_p)(\textbf{I}_p + tt^\top)- x t^\top 
	- \frac{1}{2} (\textbf{I}_p + tt^\top) (xx^\top - \textbf{I}_p)- t x^\top 
	\\
	&\quad  - \frac{1}{2} \sum^p_{h=1}\sum^p_{k=1}  \left( \frac{\partial tt^\top}{\partial t_h} + tt^\top t_h + t_h \textbf{I}_p \right) t_k (x_{h} x_{k}- \mathbbm{1}(h=k)) - (\textbf{I}_p+tt^\top)t^\top x  \bigg\},
\end{align*}
where $\mathbbm{1}(\cdot)$ is the indicator function. Since $\mathbb{E}(X) = 0, \mathbb{E}(XX^\top) = \textbf{I}_p$ and $\mathbb{E}((t^\top X)^2) = t^\top \mathbb{E}(XX^\top)t = \|t\|^2$, straightforward calculation shows that $\mathbb{E}(f_j(X_i;t))=0$ for $j=0,1,2$ and any $t \in \mathbb{R}^p$, where the same notation $0$ is adopted as the zero element in the corresponding high dimensional space if there is no cause of ambiguity. Also, let $Q^h:=\frac{\partial tt^\top}{\partial t_h}$. $Q^h$ is a matrix of $0$'s except for the $h$th row and $h$th column, where the $(h,j)$ element, $Q^h_{hj} = t_j$ for $j=1,\ldots,h-1,h+1,\ldots,p$; while the $(i,h)$ element, $Q^h_{ih}=t_i$ for $i=1,\ldots,h-1,h+1,\ldots,p$, and $Q^h_{hh} = 2 t_h$.
%
The following lemma first gives the approximation errors due to the use of scaled residuals in the empirical version of the moment generating function and its gradient and Hessian. 
\begin{lemma}\label{lemma:Mn_Mn0}
	Under $H_0$, for any $t \in \mathbb{R}^p$, we have
	\begin{enumerate}[(a)]
		\item $M^{(n)}(t) - M^{(n)}_0(t) = n^{-1}\sum^n_{i=1}f_0(X_i;t) + o_p(n^{-1/2})$;
		
		\item $\triangledown M^{(n)}(t) - \triangledown M^{(n)}_0(t) = n^{-1} \sum^n_{i=1}f_1(X_i;t) + o_p(n^{-1/2})$;
		
		\item  $H_{M^{(n)}}(t) - H_{M^{(n)}_0}(t) = n^{-1}\sum^n_{i=1}f_2(X_i;t) + o_p(n^{-1/2})$.
		
	\end{enumerate}
\end{lemma}


%\begin{lemma}\label{lemma:Mn'_Mn0'}
%		Under $H_0$,
%	\begin{align*}
	%&		\sqrt{n}\{\triangledown M^{(n)}(t) - \triangledown M_{n,0}(t)\} \\
	%&=
	%	- \frac{e^{\|t\|^2/2}}{\sqrt{n}}\sum^n_{i=1} \left( X_i X_i^\top t - t + X_i + tt^\top X_i + \frac{1}{2} (I+ tt^\top)(X_i X_i^\top - I)t \right) + o_p(1).
	%	\end{align*}
%\end{lemma}

%\begin{proof}[Proof of Lemma \ref{lemma:Mn'_Mn0'}]
%
%\end{proof}

%\begin{lemma}\label{lemma:Mn''_Mn0''}
%	Under $H_0$,
%	\begin{align*}
	%			& \sqrt{n}\{ H_{M_0}(t) - H_{M_{n,0}}(t)\} \\
	%			&= -\frac{e^{\|t\|^2/2}}{2\sqrt{n}}\sum^n_{i=1}(X_i X_i^\top - I) (I+ tt^\top) - 	\frac{e^{\|t\|^2/2}}{\sqrt{n}}\sum^n_{i=1} X_i t^\top\\
	%			& \quad - \frac{e^{\|t\|^2/2}}{2\sqrt{n}} (I+tt^\top) \sum^n_{i=1}(X_i X_i^\top - I) - \frac{e^{\|t\|^2/2}}{\sqrt{n}} \sum^n_{i=1} t X_i^\top\\
	%			& \quad + \mathbb{E}(e^{t^\top X} XX^\top X^\top) \left( - \frac{1}{2\sqrt{n}} \sum^n_{j=1}(X_j X_j^\top - I) \right) t  - \frac{e^{\|t\|^2/2}}{\sqrt{n}} (I+ tt^\top) \sum^n_{i=1} t^\top X_i + o_p(1).
	%	\end{align*}
%\end{lemma}

%\begin{proof}[Proof of Lemma \ref{lemma:Mn''_Mn0''}]
%
%
%\end{proof}
%
%
%From Lemma \ref{lemma:Mn_Mn0}, \ref{lemma:Mn'_Mn0'}, \ref{lemma:Mn''_Mn0''}, we know
%\begin{align*}
%	M^{(n)}(t) &= M_{n,0}(t)  + \frac{1}{n}\sum^n_{i=1}f_0(X_i;t) + o_p(n^{-1/2}), \\
%	M'_n(t) &= M'_{n,0}(t)  + \frac{1}{n}\sum^n_{i=1}f_1(X_i;t) + o_p(n^{-1/2}), \\
%	M''_n(t) &= M''_{n,0}(t)  + \frac{1}{n}\sum^n_{i=1}f_2(X_i;t) + o_p(n^{-1/2}), 
%\end{align*}
%where $\mathbb{E}(f_j(X_i;t)) = 0$ for $j=0,1,2$.

%
%\begin{align*}
%	\sum_i e^{t^\top Z_{ni}} = \sum_i e^{t^\top X_i} e^{t^\top \Delta_{ni}}.
%\end{align*}
%\begin{align*}
%	\sum_i Z_{ni} e^{t^\top Z_{ni}} = \sum_i X_ie^{t^\top X_i} e^{t^\top \Delta_i} + \sum_i \Delta_{ni} e^{t^\top X_i } e^{t^\top \Delta_{ni}}.
%\end{align*}
%
%\begin{align*}
%	\sum_i Z_{ni} Z_{ni}^\top e^{t^\top Z_{ni}} &= \sum_i X_i X_i^\top e^{t^\top X_i} e^{t^\top \Delta_{ni}} + \sum_i \Delta_{ni} X_i^\top e^{t^\top X_i} e^{t^\top \Delta_{ni}}  \\
%	&\quad \sum_i X_i \Delta^\top_{ni} e^{t^\top X_i} e^{t^\top \Delta_{ni}} + \sum_i \Delta_{ni} \Delta_{ni}^\top e^{t^\top X_i} e^{t^\top \Delta_{ni}}.
%\end{align*}
Define $g_1, g_2:\mathbb{R}^p \times \mathbb{R}^p \rightarrow \mathbb{R}^{p \times p}$ by
\begin{align*}
	g_1(x;t) &:=	e^{\|t\|^2/2} \left\{
	e^{t^\top x}(x x^\top + tt^\top - t x^\top - xt^\top - \textbf{I}_p) 	\right\},	\\
	g_2(x;t) &:= e^{\|t\|^2/2} \left\{ f_2(x;t) + (tt^\top + \textbf{I}_p)f_0(x;t) - t f^\top_1(x;t) - f_1(x;t) t^\top - 2 f_0(x;t) \textbf{I}_p \right\}.
\end{align*}
As $\mathbb{E}(e^{t^\top X} XX^\top) = e^{\|t\|/2}(tt^\top + \textbf{I}_p)$ and $\mathbb{E}(e^{t^\top X} X) = t e^{\|t\|^2/2}$, we have $\mathbb{E}(g_1(X_i;t)) = 0$. Also, $\mathbb{E}(g_2(X_i;t)) = 0$ because $\mathbb{E}(f_j(X_i;t))=0$ for $j=0,1,2$.
 To derive the asymptotic distribution of the test statistic $T^{(n)}_N$, we first derive that of $\sqrt{n}(H_{\Lambda^{(n)}}(t) - \textbf{I}_p )$. Since
\begin{equation*}
H_{\Lambda^{(n)}}(t) - \textbf{I}_p = (M^{(n)})^{-2}(t) \cdot \{M^{(n)}(t) H_{M^{(n)}}(t) - \triangledown M^{(n)}(t) (\triangledown M^{(n)}(t))^\top - (M^{(n)})^2(t) \textbf{I}_p \},
\end{equation*}
we establish the following lemma, which write the second term of the right side of the above equation into an average of mean zero terms of independent variable and asymptotic negligible reminder.
\begin{lemma}\label{lemma:Mn_I}
	Under $H_0$, for any $t \in \mathbb{R}^p$, we have
	\begin{enumerate}[(a)]
		\item $M^{(n)}_0(t) H_{M^{(n)}_0}(t) - \triangledown M^{(n)}_0(t) (\triangledown M^{(n)}_0(t))^\top - (M^{(n)}_0)^2(t) \textbf{I}_p = n^{-1}\sum^n_{i=1} g_1(X_i;t) + o_p(n^{-1/2})$;
		
		\item $	M^{(n)}(t) H_{M^{(n)}}(t) - \triangledown M^{(n)}(t) (\triangledown M^{(n)}(t))^\top - (M^{(n)})^2(t) \textbf{I}_p = n^{-1}\sum^n_{i=1} (g_1(X_i;t) +g_2(X_i;t)) + o_p(n^{-1/2})$.
		
		
	\end{enumerate}
	
	
\end{lemma}

With Lemmas \ref{lemma:Mn_Mn0} and \ref{lemma:Mn_I}, we have the following theorem.
\begin{thm}\label{thm:H_I}
	Under $H_0$, for any $t \in \mathbb{R}^p$, we have
	\begin{equation*}
		\sqrt{n}(H_{\Lambda^{(n)}}(t) - \textbf{I}_p) = \frac{1}{\sqrt{n}}\sum^n_{i=1} h(X_i;t) + o_p(1),
	\end{equation*}
	where $h(x;t) := e^{-\|t\|} (g_1(x;t) + g_2(x;t))$.
\end{thm}




Consider $t_1,\ldots,t_N \in \mathbb{R}^p$ in (\ref{eq:Hn_Dn_t}). For a $p\times p$ matrix $A = (a_{ij})$, we denote $A^v = (a_{11},\ldots,a_{1p},a_{22},\ldots,a_{2p},\ldots,a_{pp})$ to be the vectorization of the upper triangular part of $A$. 
%By regarding the upper triangular part of the random matrices $\sqrt{n}(H_{\Lambda^{(n)}}(t) - \textbf{I}_p )$ and $\frac{1}{\sqrt{n}}\sum^n_{i=1}h(X_i;t)$ as $p(p+1)/2$-dimensional random vectors, we have, 
By Theorem \ref{thm:H_I}, the multivariate central limit theorem and Slutsky's theorem, we have
\begin{equation*}
	(	\sqrt{n}(H^v_{\Lambda^{(n)}}(t_1) - \textbf{I}^v_p),\ldots,	\sqrt{n}(H^v_{\Lambda^{(n)}}(t_N) - \textbf{I}^v_p))^\top \stackrel{d}{\rightarrow} N_{p(p+1)N/2}(0, \Sigma_N),
\end{equation*}
where the $(i,j)$-th element of $\Sigma_{N}$ is $\text{Cov}(h^v_{i'}(X;t_l), h^v_{j'}(X;t_{l'}))$, for $i= (l-1) p(p+1)/2  + i', j= (l'-1) p(p+1)/2 + j', l,l' = 1,\ldots,N, i',j' = 1,\ldots,p(p+1)/2$.  By the continuous mapping theorem, we can immediately obtain the limiting distributions of $H^{(n)}_N$, $D^{(n)}_N$, and $T^{(n)}_N$ respectively of, denoted by, $H_N, D_N$ and $T_N$, where
		\begin{equation*}
	T_N := \max  \left\{ 
	\frac{H_N - \mathbb{E}^S(H_N)}{SD^S(H_{N})},
	\frac{D_N - \mathbb{E}^S(D_N)}{SD^S(D_N)}
	\right\},
\end{equation*}
with $\mathbb{E}^S(H_N) := \frac{1}{S}\sum^S_{s=1}H_{N, s}$, $SD^S(H_N) := \sqrt{\frac{1}{S-1}\sum^S_{s=1}(H_{N,s} - \mathbb{E}^S(H_N))}$, and $H_{N,1},\ldots,H_{N,S}$ is a random sample having the same distribution as $H_N$. Similar definition applies to $\mathbb{E}^S(D_N)$ and $SD^S(D_N)$. We do not try to simplify the asymptotic null distribution $T_N$ further because the critical value of the test statistic at any finite sample size can be approximated by simulation.
%
%\begin{cor}\label{corollary:null_dist}
%	Under $H_0$, we have
%	\begin{enumerate}[(a)]
%		\item $H_{n,M} \stackrel{d}{\rightarrow} H_M := $
%		\item $D_{n,M} \stackrel{d}{\rightarrow} D_M := $
%		\item $T^{(n)}_N \stackrel{d}{\rightarrow} T_{M}$, where
%
%	\end{enumerate}
%\end{cor}

\section{Consistency}\label{sect:consistency}
While the scaled residuals $Z_{n,i}$ of a multivariate normal with a general covariance matrix are equal in distribution as the scaled residuals of a standard multivariate normal distribution, such a nice property does not hold with $X$ is in the alternative hypothesis.  To study the convergence of $H_N^{(n)}$ and $D_N^{(n)}$ for a general $X$ with finite second moments, we can assume $E(X)=0$ without loss of generality, and $Var(X)=\Sigma$. Denote $\tilde{X} := \Sigma^{-1/2} X$, $\tilde{M}(t) = \mathbb{E}(e^{t^\top \tilde{X}})$ and $\tilde{\Lambda}(t) := \log \tilde{M}(t)$.
The following theorem establishes the strong limits of $U^{(n)}_N$ for the univariate case, and that of $H^{(n)}_N$ and $D^{(n)}_N$ for the multivariate case, altogether imply the consistency of our test.
\begin{thm}\label{thm:consistency_limit}
	Suppose that the moment generating function of $X$ exists and is twice differentiable. 
	\begin{enumerate}[(a)]
		\item If $X$ is univariate, with probability $1$,
		\begin{equation*}
			\lim_{n \rightarrow \infty} \frac{U^{(n)}_N}{n} = \sum^N_{l=1}(\tilde{\Lambda}''(t_l) - 1)^2.
		\end{equation*}
	\item If $X$ is multivariate, with probability $1$,
	\begin{align*}
		\lim_{n \rightarrow \infty} \frac{H^{(n)}_N}{n} &= \sum^N_{l=1} \sum^p_{i=1}\sum^p_{j > i} (H_{\tilde{\Lambda}^*,ij}(t_l))^2 ,\\
		\lim_{n \rightarrow \infty} \frac{D^{(n)}_N}{n} &= \sum^N_{l=1} \sum^p_{i=1} ( \tilde{D}_{ii}(t_{li}) - 1)^2,
		%		\lim_{n \rightarrow \infty} MS_{n, M}&= \max 
		%		\left\{
		%		\frac{\sum^M_{l=1} \|H_\Lambda(t_l) - \textbf{I}\|^2_F - \mathbb{E}(\sum^M_{l=1} \|H_\Lambda(t_l) - \textbf{I}\|^2_F)}{SD(\sum^M_{l=1} \|H_\Lambda(t_l) - \textbf{I}\|^2_F) },
		%		\right\}
	\end{align*}
where $H_{\tilde{\Lambda}^*}$ and $\tilde{D}_{ii}$ are defined similarly as in (\ref{eq:H_star_lambda_D}).
		\end{enumerate}
\end{thm}
Suppose  that the moment generating function of $X$ exists and is twice differentiable but $X$ is not from $\mathcal{N}_p$. Then, $\tilde{X}$ is not distributed as $N_p(0, \textbf{I}_p)$ and there must exist a point $t^*$ in the neighbourhood of $0$ such that $H_{\tilde{\Lambda}}(t^*) \neq \textbf{I}_p$. Since $H_{\tilde{\Lambda}}(\cdot)$ is continuous, there exists a neighbourhood $\mathcal{O}$ of $t^*$ such that $H_{\tilde{\Lambda}}(t) \neq \textbf{I}_p$ for all $t \in \mathcal{O}$. This implies that  $H_{ \tilde{\Lambda}^*}(t) \neq 0$ or $\tilde{D}(t) \neq \textbf{I}_p$ for all $t \in \mathcal{O}$. As a result, if $\{t_l\}^N_{l=1}$ is chosen using a space-filling design and $N$ is large enough, some $t_l$ will be in $\mathcal{O}$.  Hence, almost surely, $\lim_{n \rightarrow \infty} H^{(n)}_N = \infty$ or $\lim_{n \rightarrow \infty} D^{(n)}_N = \infty$ so that $\lim_{n \rightarrow \infty} T^{(n)}_N = \infty$. In practice, one can also perform the test with a large enough $N$ and see if the $p$-value is stable when $N$ increases.


\section{Simulation Studies}\label{sect:simulation}
%\subsection{Univariate}
We carried out extensive Monte Carlo study to evaluate the finite-sample sizes and powers of our proposed test statistics and compare with several tests in the literature. 
For the univariate case, we compare with 
\begin{enumerate}[(a)]
	\item the Cram\'er-von Mises (CvM) test (\cite{cramer1928composition}, \cite{mises1931wahrscheinlichkeitsrechnung}, and \cite{smirnov1936sui});
	\item the Anderson-Darling (AD) test (\cite{anderson1954test});
	\item the Shapiro-Wilk (SW) test (\cite{shapiro1965analysis});
	\item the Jarque-Bera (JB) test (\cite{jarque1987test});
	\item the Henze-Visagie (HV) test (\cite{henze2020testing}).
\end{enumerate}
The first four tests are well-known and will not be reviewed; see \cite{yap2011comparisons} for a review of these tests. 
For the implementation of CvM and AD, the functions \verb|cvm.test| and \verb|ad.test| in the R package \verb|nortest| are used, respectively. The SW test can be carried out using \verb|Shapiro.test| in the \verb|stats| R package. The JB test is carried out using \verb|jarque.bera.test| from the R package \verb|tseries|.
The HV test is developed based on a system of first-order partial differential equations that characterize the moment generating function of the $p$-variate standard normal distribution. The test statistic is
\begin{equation}\label{eq:HV}
	HV_{n,\gamma} := n \int_{\mathbb{R}^p} \|\triangledown M^{(n)}(t) - tM^{(n)}(t)\|^2 \exp (-\gamma \|t\|^2) dt,
\end{equation}
which can be computed in a closed-form formula; see equation (9) in (\cite{henze2020testing}). $H_0$ is rejected for large values of $HV_{n,\gamma}$. \cite{henze2020testing} recommended $\gamma = 5$ to be used when performing the test based on their numerical results  and we follow this suggestion in our numerical study. The R package \verb|mnt| contains the function \verb|HV| to compute this test statistic. For our proposed test statistic and the HV test statistic, $100,000$ independent replications were used to determine the critical value of the tests. Each size or power estimate is based on $10,000$ replications. We largely follow the choices of alternative distributions considered in \cite{yap2011comparisons}, where various shapes of distributions were considered. The alternative distributions considered here can be classified into symmetric short-tailed distributions, symmetric long-tailed distributions, asymmetric distributions, and mixture of normal distributions. 

We first define additional notation for some of these distributions. Denote GLD$(\lambda_1,\lambda_2,\lambda_3,\lambda_4)$ to be the genearlized lambda distribution proposed by \cite{ramberg1974approximate}, which is a four-parameter generalization of the two-parameter Tukey's Lambda family of distribution (\cite{hastings1947low}). The percentile function of GLD$(\lambda_1,\lambda_2,\lambda_3,\lambda_4)$ is given as
\begin{equation*}
	Q(y) = \lambda_1 + \frac{y^{\lambda_3} - (1-y)^{\lambda_4}}{\lambda_2}, \quad \text{where } 0 \leq y \leq 1,
\end{equation*}
where $\lambda_1$ is the location parameter, $\lambda_2$ is the scale parameter and $\lambda_3$ and $\lambda_4$ are the shape parameters. The density function of GLD$(\lambda_1,\lambda_2,\lambda_3,\lambda_4)$ at $x = Q(y)$ is 
\begin{equation*}
	f(x) = \frac{\lambda_2}{\lambda_3 y^{\lambda_3 - 1} - \lambda_4(1-y)^{\lambda_4-1}}.
\end{equation*}
The truncated normal distribution of a normal distribution with mean $\mu$ and standard deviation $\sigma$ truncated to the interval $(a,b)$ is denoted by Trunc$(a, b,\mu,\sigma)$. The scale-contaminated normal distribution, denote by ScConN$(p, b)$, is a mixture of two normal distributions with probability $p$ from a normal distribution $N(0, b^2)$ and probability $1-p$ from $N(0, 1)$. LoConN$(p, a)$ denotes the distribution of a mixture of two normal distributions with probability $p$ from a normal distribution with mean $a$ and variance $1$ and with probability $1-p$ from a standard normal distribution.

The symmetric short-tailed distributions include $U(0, 1)$, Beta$(0.5,0.5)$, Beta$(2,2)$, $\allowbreak \text{GLD}(0,1,0.25,0.25)$, GLD$(0,1,0.5,0.5)$, GLD$(0,1,0.75,0.75)$, GLD$(0,1,1.25,1.25)$, $\allowbreak \text{Trunc}(-2,2,0,1)$, $\allowbreak \text{Trunc}(-3,3,0,2)$, and $\text{Trunc}(-2,2,0,2)$. The symmetric long-tailed distributions include Laplace, logistic, GLD$(0, 1,-0.1,-0.1)$, GLD$(0,1,-0.15,-0.15)$, $t(5)$, $t(10)$, and $t(15)$. The asymmetric distributions include exp$(1)$, lognormal$(0, 0.5)$, Gamma$(4, 5)$, Beta$(2, 1)$, Beta$(3, 2)$, Weibull$(3, 1)$, Pareto$(1, 3)$, $\chi^2(4)$, $\chi^2(10)$, and $\chi^2(20)$. The normal mixtures considered are ScConN$(0.2, 5)$, ScConN$(0.05, 5)$, LoConN$(0.5, 3)$, and LoConN$(0.5, 2)$.


In the simulation study, the set of points $\{t_1,\ldots, t_N\}$ is chosen randomly from $\{t \in \mathbb{R}^p: \|t\| \leq R\}$. The upper panel of Figure \ref{figure:uni_B_M_dependence} shows the empirical reject proportions of our test in the univariate case for the $32$ distributions in the alternative hypothesis with different values $R$ when $N$ is fixed at $500$ when the sample size is $50$. It can be seen that the results are similar for $R$ increasing from $3$ to $10$. The lower panel of Figure \ref{figure:uni_B_M_dependence} fixed $R = 3$ with different values of $N$. We can see that different values of $N$ result in similar reject proportions. Similar results were obtained with different sets of random points.

Figures \ref{figure:uni_result1} and \ref{figure:uni_result2} compare our proposed tests with other normality tests at different sample sizes. 
 It can be seen that our test outperformed other tests, including the Shapiro-Wilk test, which is often considered as the best univariate normality test, when the true distribution has a bounded support and tend to have performance in between those of other tests in other distributions. Under $H_0$ (results not shown in the figures), the sizes of our tests are close to $0.05$, as our test statistic is distribution-free under $H_0$ and the critical value is determined using simulation.

For the multivariate case, we compare our proposed test statistic with the following tests: the energy test of \cite{szekely2005new}, the Henze-Visagie (HV) test (defined in (\ref{eq:HV})), the Henze–Jim\'enez-Gamero (HJ) test (\cite{henze2019new}, the Henze-Zirkler (HZ) test (\cite{henze1990class}) and the Mardia's test (\cite{mardia1970measures}) based on skewness (MS) and kurtosis (MK). A brief description of these tests is as follows. \cite{szekely2005new} proposed the test statistic
\begin{equation*}
	\mathcal{E}_n := n \left( \frac{2}{n}\sum^n_{i=1}\mathbb{E}\|Z_{n,i}-X\| - \frac{2 \Gamma((p+1)/2)}{\Gamma(p/2)} - \frac{1}{n^2}\sum^n_{i,j=1}\|Z_{n,i} - Z_{n,j}\| \right),
\end{equation*}
where the first expectation is taking with respect to $X$, which follows $N(0, \textbf{I}_p)$, and
\begin{equation*}
	\mathbb{E}(\|a-X\|) = \frac{\sqrt{2} \Gamma(\frac{p+1}{2})}{\Gamma(\frac{p}{2})} +
	\sqrt{ \frac{2}{\pi}} \sum^\infty_{k=0}\frac{(-1)^k}{k!2^k} \frac{\|a\|^{2k+2}}{(2k+1)(2k+2)} \frac{ \Gamma( \frac{p+1}{2}) \Gamma(k + \frac{3}{2})}{\Gamma(k+\frac{p}{2}+1)}.
\end{equation*}
The test using $\mathcal{E}_n$ is known as the energy test and we make use of the function \verb|mvnorm.etest| in the R package \verb|engergy| to compute the test statistic and its $p$-value. \cite{szekely2005new} concluded that the energy test is a powerful omnibus test having relatively good power against general alternatives compared with other tests. 
%The test from \cite{henze2020testing} is a recent normality test based on a partial differential equation involving the moment generating function. Their test statistic is
%\begin{equation*}
%	HV_{n, \gamma} := n \int_{\mathbb{R}^d} \| \triangledown M^{(n)}(t) - t M^{(n)}(t)\|^2 \exp(-\gamma \|t\|^2) dt
%\end{equation*}
%and reject $H_0$ for large values of $HV_{n, \gamma}$. \cite{henze2020testing} recommended that $\gamma = 5$ when performing the normality test and we follow this suggestion in our numerical study.
The Henze–Jim\'enez-Gamero test is based on the test statistic
\begin{equation*}
	HJ_{n, \beta} := n \int_{\mathbb{R}^p} (M^{(n)}(t) - M(t))^2 \exp (- \beta\| t\|^2) dt,
\end{equation*} 
where the test rejects $H_0$ for large values of $HJ_{n,\beta}$. We include HV and HJ tests for comparison because our test is also based on characterization of normality using a system of partial differential equations involving the moment generating function. It will be of interest to compare the performance of these tests. The Henze-Zirkler test is based on empirical characteristic function of the scaled residuals:
\begin{equation*}
	HZ_{n,\gamma} := (2\pi\gamma^2)^{-p/2} \int_{\mathbb{R}^d} \left| \frac{1}{n}\sum^n_{j=1}\exp(i t^\top Z_{n,j} ) - \exp \left( - \frac{\|t\|^2}{2} \right) \right|^2 \exp \left( - \frac{\|t\|^2}{2\gamma^2} \right)dt.
\end{equation*}
We compute the test statistics for HV, HJ and HZ tests using the functions \verb|HV|, \verb|HJ|, and \verb|HZ| in the package \verb|mnt|, respectively.
Mardia's test for multinormality based on sample skewness rejects $H_0$ for large values of $b_{1,p}$, where
\begin{equation*}
	b_{1,p} := \frac{1}{n^2}\sum^n_{i,j=1}(Z^\top_{n,i} Z_{n,j})^3.
\end{equation*}
The sample kurtosis is given by
\begin{equation*}
	b_{2, p} := \frac{1}{n}\sum^n_{i=1}\|Z_{n,i}\|^4.
\end{equation*}
See \cite{mardia1970measures} for their limiting null distributions. To perform the tests based on the sample skewness and kurtosis, we use the function \verb|mult.norm| from the R package \verb|QuantPsyc|. All the critical values for these multivariate tests are determined from $100,000$ independent replications under the null hypothesis.

For the distributions in the alternative hypothesis, we consider multivariate distributions with independent components  where the marginal distributions are the $32$ alternative distributions considered in the univariate case, except for the normal mixture. In addition, we consider dependent multivariate distributions with normal marginals, where the dependence structure is generated from some common copulas, including the Clayton, Gumbel, Frank, Ali–Mikhail–Haq (AMH), and $t$ copulas (see \cite{nelsen2007introduction}). Except for the $t$ copula, the other copulas are parameterized by one parameter. For $t$ copula, we use the notation tCopula($\rho$, df), where $\rho$ and df denote the parameter in the exchangeable dispersion structure and degrees of freedom, respectively. We also consider the setting where the dependence structure is generated from the Gaussian copula but the marginal distributions are non-normal.

Figure \ref{figure:multi_B_M_dependence} shows the performance of the proposed test with different values of $R$ and $N$ when $n = 50$. The results are similar and the test with $R = 3$ performed the best over the distributions considered. For the values of $N$, as long as it is not too small, increasing $N$ will not increase the power of the test. Figures \ref{figure:multi_result1} and \ref{figure:multi_result2} compares the empirical reject proportions of the proposed test to other tests with different dimensions and when $n = 50$, $R = 3$ and $N = 500$. We see that our test performs the best in many different multivariate distributions. Under $H_0$, all the tests considered have around $0.05$ sizes as the critical values can be determined by simulation.

More details of the simulation results for both the univariate and multivariate cases are given in the Appendix.

\begin{figure}[H]
	\centering
	\includegraphics[width = 14cm]{figure/univariate_H1_B_M.pdf}
	\caption{Univariate test under alternatives when $n = 50$. The upper panel shows the empirical reject proportions under the $32$ alternative distributions when $N = 500$ with different values of $R$. The lower panel shows  the empirical reject proportions under the $32$ alternative distributions when $R = 3$ with different values of $N$.}
	\label{figure:uni_B_M_dependence}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[width = 16cm]{figure/univariate_H1_1.pdf}
	\caption{Univariate test under different alternative distributions. Here $R = 3$ and $N = 500$ for our test statistic.}
	\label{figure:uni_result1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width = 16cm]{figure/univariate_H1_2.pdf}
	\caption{Univariate test under different alternative distributions (continued). Here $R = 3$ and $N = 500$ for our test statistic.}
	\label{figure:uni_result2}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width = 14cm]{figure/multi_H1_R_N.pdf}
	\caption{Multivariate test under alternatives when $n = 50$. The upper panel shows the empirical reject proportions under the $48$ alternative distributions when $N = 500$ and $p = 3$ with different values of $R$. 
		The middle panel shows  the empirical reject proportions when $R = 3$ and $p = 3$ with different values of $N$.
		The lower panel shows  the empirical reject proportions when $R = 3$ and $p = 10$ with different values of $N$.}
\label{figure:multi_B_M_dependence}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[width = 16cm]{figure/multivariate_H1_1.pdf}
	\caption{Multivariate test under different alternative distributions and different dimensions $p$ when $n = 50$. Here $R = 3$ and $N = 500$ for our test statistic.}
\label{figure:multi_result1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width = 16cm]{figure/multivariate_H1_2.pdf}
	\caption{Multivariate test under different alternative distributions and different dimensions $p$ when $n = 50$. Here $R = 3$ and $N = 500$ for our test statistic. $S$ is a matrix with all $0.5$'s in the off-diagonal entries and $1$'s in the diagonal entries.}
	\label{figure:multi_result2}
\end{figure}

\section{Conclusion}\label{sect:conclusion}
In this article, a novel class of tests for multivariate normality is proposed. Our extensive Monte Carlo study suggested that our test is more powerful in many alternatives compared with existing common tests. 
We suggest performing our test with $R = 3$ and $N = 500$, where the set of points can be simulated uniformly from the ball centred at $0$ with radius $R$.

Another possible class of tests can be obtained replacing the moment generating function with the characteristic function in the definition of the cumulant generating function. The finite-sample performance of such a test is undergoing investigation. The idea of combining the dependence structure and marginal information can also be applied to other goodness-of-fit problems.


\section*{Acknowledgement}
Chan and Tang were partially funded by the US National Institutes of Health grant R01HL122212. Hok Kan Ling acknowledges the support by NSERC
Grant RGPIN/03124-2021.
The fourth author acknowledges financial support from Hong Kong General Research Fund Grants HKGRF-14300319 “Shape-Constrained Inference: Testing for Monotonicity” and HKGRF-14301321“General Theory for Infinite Dimensional Stochastic Control: Mean Field and Some Classical Problems.
% anonymous submission, so no acknowledgement here

\section{Appendix} % will move to the supplementary material
To facilitate the proofs, we first define additional notations and describe some of their properties. Let
\begin{equation}\label{eq:Delta_ni}
	\Delta_{n,i} := Z_{n,i} - X_i = (S^{-1/2}_n - \textbf{I}_p)X_i - S^{-1/2}_n\overline{X}_n.	
\end{equation}
By Taylor's theorem, for some $|\theta_{n,i}(t)| \leq 1$, we have
\begin{equation}\label{eq:Taylor_etDelta}
	e^{t^\top \Delta_{n,i}} = 1 + t^\top \Delta_{n,i} + \frac{1}{2} (t^\top \Delta_{n,i})^2 e^{\theta_{n,i}(t) t^\top \Delta_{ni}}.
\end{equation}
Under $H_0$, by (2.13) of \cite{henze1997new}, we have
\begin{equation}\label{eq:S_squareroot_I}
	\sqrt{n}(S^{-1/2}_n - \textbf{I}_p) = - \frac{1}{2\sqrt{n}} \sum^n_{i=1} (X_i X_i^\top -  \textbf{I}_p) + O_p(n^{-1/2}),
\end{equation}
which is $O_p(1)$ by the central limit theorem because $\mathbb{E}(X_i X_i^\top - \textbf{I}_p) = 0$. Define $\|\cdot\|_2$ to be the spectral norm of a matrix. From (\ref{eq:Delta_ni}), we have
\begin{equation}\label{eq:Delta_bound}
	\|\Delta_{n,i}\| \leq  \|(S^{-1/2}_n - \textbf{I}_p)\|_2 \cdot \|X_i\| + \|S^{-1/2}_n\|_2 \cdot  \|\overline{X}_n\|.
\end{equation}
This inequality (\ref{eq:Delta_bound}) together with (\ref{eq:S_squareroot_I}) and the fact that  $\max_{i=1,\ldots,n}\|X_i\| = O_p(\log n)$ (see Proposition A.1 in \cite{henze2019characterizations}) imply that
\begin{equation}\label{eq:t_Delta_ni2}
	\max_{i=1,\ldots,n} \| \Delta_{n,i}\| = O_p(n^{-1/2}\log n).
\end{equation}
%
Recall that $M_0(t)$ denotes the moment generating function of $X \sim N_p(0, \textbf{I}_p)$. By the strong law of large numbers, for each $t \in \mathbb{R}^p$, $M_{n,0}(t) \stackrel{a.s.}{\rightarrow}  M(t) = e^{\|t\|^2/2}$, $\triangledown M_{n,0}(t) \stackrel{a.s.}{\rightarrow} \triangledown M_0(t) = t e^{\|t\|^2/2}$, and $H_{M_{n,0}}(t) \stackrel{a.s.}{\rightarrow} H_{M_0}(t) = (tt^\top + \textbf{I}_p) e^{\|t\|^2/2}$. Finally, $\frac{1}{n}\sum^n_{i=1} e^{t^\top X_i} X_i X_i^\top X_{ih} \stackrel{a.s.}{\rightarrow} \mathbb{E}(e^{t^\top X} X X^\top X_h)$, where $X_h$ is the $h$-th component of $X$ and
\begin{align}
	\mathbb{E}(e^{t^\top X} X X^\top X_h) &	 = \frac{\partial}{\partial t_h} \mathbb{E}(e^{t^\top X} XX^\top) 
	= \frac{\partial}{\partial t_h}\left\{ (tt^\top + \textbf{I}_p) e^{\|t\|^2/2}\right\} \nonumber  \\
	&= \frac{\partial tt^\top}{\partial t_h} e^{\|t\|^2/2} + tt^\top t_h e^{\|t\|^2/2} + t_h e^{\|t\|^2/2}\textbf{I}_p \nonumber \\
	&=e^{\|t\|^2/2} \left( \frac{\partial tt^\top}{\partial t_h} + tt^\top t_h + t_h \textbf{I}_p \right). \label{eq:EXXXh}
\end{align}
\subsection{Proofs for Section \ref{sect:asy_null_dist}}


\begin{proof}[Proof of Lemma \ref{lemma:Mn_Mn0}]
	\begin{enumerate}[(a)]
		\item 
		By  (\ref{eq:Delta_ni}) and (\ref{eq:Taylor_etDelta}),
		\begin{align}
			\sqrt{n}(M^{(n)}(t) - M^{(n)}_0(t)) 
			&= \frac{1}{\sqrt{n}}\sum^n_{i=1} e^{t^\top X_i} (e^{t^\top \Delta_{n,i}} - 1)  \nonumber \\
			& = \frac{1}{\sqrt{n}}\sum^n_{i=1} e^{t^\top X_i} \left\{ t^\top \Delta_{n,i} + \frac{1}{2}(t^\top \Delta_{n,i})^2 e^{\theta_{n,i}(t) t^\top \Delta_{n,i}} \right\} \nonumber \\
			&=: A_{1n} + A_{2n} + A_{3n} + A_{4n}, \label{eq:Lemma_1_a_decompose}
		\end{align}
		where
		\begin{align*}
			A_{1n} &:= \frac{1}{\sqrt{n}}\sum^n_{i=1} e^{t^\top X_i} t^\top (S^{-1/2}_n - \textbf{I}_p)X_i;\\
			A_{2n} &:= -\frac{1}{\sqrt{n}}\sum^n_{i=1} e^{t^\top X_i} t^\top (S^{-1/2}_n -  \textbf{I}_p) \overline{X}_n; \\
			A_{3n} &:= -\frac{1}{\sqrt{n}}\sum^n_{i=1} e^{t^\top X_i} t^\top \overline{X}_n;\\
			A_{4n} &:= \frac{1}{\sqrt{n}} \sum^n_{i=1} e^{t^\top X_i}\frac{1}{2}(t^\top \Delta_{n,i})^2 e^{\theta_{n,i}(t) t^\top \Delta_{n,i}}.
		\end{align*}
		For $A_{1n}$, we have	
		\begin{align}
			A_{1n}	&= \frac{1}{n}\sum^n_{i=1} e^{t^\top X_i} X_i^\top  \sqrt{n}(S^{-1/2}_n -\textbf{I}_p)t \nonumber \\
			&= \left( \frac{1}{n}\sum^n_{i=1} e^{t^\top X_i} X_i^\top \right) \left\{ - \frac{1}{2\sqrt{n}} \sum^n_{j=1} (X_j X_j^\top - \textbf{I}_p) + O_p(n^{-1/2}) \right\} t \nonumber  \\
			&= \left( \frac{1}{n}\sum^n_{i=1} e^{t^\top X_i} X_i^\top \right) \left\{ - \frac{1}{2\sqrt{n}} \sum^n_{j=1} (X_j X_j^\top - \textbf{I}_p)  \right\} t  + o_p(1) \nonumber  \\
			& = e^{\|t\|^2/2} t^\top \left\{ - \frac{1}{2\sqrt{n}} \sum^n_{j=1} (X_j X_j^\top - \textbf{I}_p)  \right\}  t  + o_p(1) \nonumber  \\
			&= - \frac{e^{\|t\|^2/2}}{2\sqrt{n}} \sum^n_{i=1}\left\{ (t^\top X_i)^2 - \|t\|^2 \right\}  + o_p(1), \label{eq:Lemma_1_a_decompose_A1n}
		\end{align}
		where the second equality follows from (\ref{eq:S_squareroot_I}) and the second last equality follows from the weak law of large numbers.
		For $A_{2n}$, by (\ref{eq:S_squareroot_I}), 
		\begin{align}
			A_{2n}				&= \bigg(- \frac{1}{n} \sum^n_{i=1} e^{t^\top X_i} t^\top \bigg)\sqrt{n}(S^{-1/2}_n - \textbf{I}_p) \overline{X}_n \nonumber  \\
			&= O_p(1) O_p(1) O_p(n^{-1/2}) = o_p(1). \label{eq:Lemma_1_a_decompose_A2n}
		\end{align}		
		For $A_{3n}$, by the weak law of large numbers,
		\begin{align}
			A_{3n} 	&= \bigg( - e^{\|t\|^2/2}t^\top + o_p(1) \bigg) \sqrt{n} \cdot \overline{X}_n = -e^{\|t\|^2/2} \frac{1}{\sqrt{n}} \sum^n_{i=1} t^\top X_i + o_p(1). \label{eq:Lemma_1_a_decompose_A3n}
		\end{align}
		By (\ref{eq:t_Delta_ni2}), we have
		\begin{align}		
			A_{4n}	&\leq \frac{\sqrt{n}}{2} \|t\|^2 \max_{i=1,\ldots,n} \|\Delta_{n,i} \|^2 e^{\|t\| \max_{i=1,\ldots,n}\|\Delta_{n,i}\|} \left( \frac{1}{n} \sum^n_{i=1} e^{t^\top X_i} \right) \nonumber \\
			&= \sqrt{n} O_p(n^{-1}(\log n)^2 ) O_p(1) O_p(1)= o_p(1). \label{eq:Lemma_1_a_decompose_A4n}
		\end{align}
	The result then follows from (\ref{eq:Lemma_1_a_decompose})-(\ref{eq:Lemma_1_a_decompose_A4n}).
		
		\item 		By  (\ref{eq:Delta_ni}) and (\ref{eq:Taylor_etDelta}),
		\begin{align}
			&	\sqrt{n}(\triangledown M^{(n)}(t) - \triangledown M^{(n)}_{0}(t)) \nonumber \\
			&= \frac{1}{\sqrt{n}}\sum^n_{i=1}(Z_{n,i} - X_i) e^{t^\top Z_{n,i}} + \frac{1}{\sqrt{n}}\sum^n_{i=1} X_i \left( e^{t^\top Z_{n,i}} - e^{t^\top X_i} \right)\nonumber \\
			&= \frac{1}{\sqrt{n}}\sum^n_{i=1} \Delta_{n,i} e^{t^\top X_i} e^{t^\top \Delta_{n,i}} + \frac{1}{\sqrt{n}}\sum^n_{i=1} X_i e^{t^\top X_i} (e^{t^\top \Delta_{n,i}} - 1) \nonumber \\
			&= B_{1n} + B_{2n} + B_{3n} + B_{4n},\label{eq:B1n_B2n_B3n_B4n}
		\end{align}
		where
		\begin{align*}
			B_{1n} &:= \frac{1}{\sqrt{n}}\sum^n_{i=1} (S^{-1/2}_n - \textbf{I}_p) X_i e^{t^\top X_i} e^{t^\top \Delta_{n,i}};\\
			B_{2n} &:= - \frac{1}{\sqrt{n}}\sum^n_{i=1} (S^{-1/2}_n -  \textbf{I}_p) \overline{X}_n e^{t^\top X_i} e^{t^\top \Delta_{n,i}};\\
			B_{3n} &:= - \frac{1}{\sqrt{n}}\sum^n_{i=1} \overline{X}_n e^{t^\top X_i} e^{t^\top \Delta_{n,i}};\\
			B_{4n} &:= \frac{1}{\sqrt{n}}\sum^n_{i=1} X_i e^{t^\top X_i} \left\{ t^\top \Delta_{n,i} + \frac{1}{2}(t^\top \Delta_{n,i})^2 e^{\theta_{n,i}(t) t^\top\Delta_{n,i}} \right\}.
		\end{align*}
		By (\ref{eq:S_squareroot_I}) and the weak law of large numbers,
		\begin{align}
			B_{1n} &= \frac{1}{n} \sum^n_{i=1} \left\{ - \frac{1}{2\sqrt{n}} \sum^n_{j=1}(X_j X_j^\top - \textbf{I}_p) \right\} X_i e^{t^\top X_i} + o_p(1)  \nonumber \\
			&= - \frac{1}{2\sqrt{n}} \sum^n_{i=1} (X_i X_i^\top - \textbf{I}_p) \left( t e^{\|t\|^2/2} + o_p(1) \right) + o_p(1)\nonumber  \\
			&=- \frac{e^{\|t\|^2/2}}{2\sqrt{n}} \sum^n_{i=1} (X_i X_i^\top t - t)    + o_p(1). \label{eq:B1n}
		\end{align}
		By (\ref{eq:S_squareroot_I}),
		\begin{align}
			\|B_{2n}\| & \leq \left \|\frac{1}{n}\sum^n_{i=1}e^{t^\top X_i}\right\| \|S^{-1/2}_n - \textbf{I}_p\|_2 \|\sqrt{n} \cdot \overline{X}_n\| e^{ \|t\| \max_{i=1,\ldots,n} \|\Delta_{n,i}\| } \nonumber  \\
			&= O_p(1) O_p(n^{-1/2}) O_p(1) O_p(1) = o_p(1). \label{eq:B2n}
		\end{align}	
		By the weak law of large numbers,
		\begin{align}\label{eq:B3n}
			B_{3n} &= - \frac{1}{n}\sum^n_{i=1} e^{t^\top X_i} \sqrt{n} \cdot \overline{X}_n + o_p(1) 
			= - \frac{e^{\|t\|^2/2} }{\sqrt{n}}\sum^n_{i=1}X_i + o_p(1).
		\end{align}
		Finally, by  (\ref{eq:Delta_ni}) 
		\begin{align}
			B_{4n} &= \frac{1}{\sqrt{n}}\sum^n_{i=1} X_i e^{t^\top X_i} t^\top \Delta_{n,i}  + o_p(1) \nonumber \\
			&= \frac{1}{\sqrt{n}}\sum^n_{i=1} X_i e^{t^\top X_i} t^\top (S^{-1/2}_n - \textbf{I}_p) X_i - \frac{1}{\sqrt{n}}\sum^n_{i=1} X_i e^{t^\top X_i} t^\top (S^{-1/2}_n - \textbf{I}_p) \overline{X}_n \nonumber \\
			& \quad - \frac{1}{\sqrt{n}}\sum^n_{i=1} X_i e^{t^\top X_i} t^\top \overline{X}_n +o_p(1)\nonumber \\
			&=\frac{1}{\sqrt{n}}\sum^n_{i=1} e^{t^\top X_i} X_i  X^\top_i (S^{-1/2}_n - \textbf{I}_p) t - t e^{\|t\|^2/2} \frac{1}{\sqrt{n}} \sum^n_{i=1} t^\top X_i + o_p(1)\nonumber \\
			&= - \frac{e^{\|t\|^2/2}}{2\sqrt{n}} (\textbf{I}_p + tt^\top) \sum^n_{i=1} (X_i X_i^\top - \textbf{I}_p) t - \frac{e^{\|t\|^2/2}}{\sqrt{n}} \sum^n_{i=1} t t^\top X_i +o_p(1), \label{eq:B4n}
		\end{align}
	where the last equality follows from  (\ref{eq:S_squareroot_I}) and the weak law of large numbers.
		Combining (\ref{eq:B1n_B2n_B3n_B4n}), (\ref{eq:B1n}), (\ref{eq:B2n}), (\ref{eq:B3n}), and (\ref{eq:B4n}), we have
			\begin{align*}
			&	\sqrt{n}(\triangledown M^{(n)}(t) - \triangledown M^{(0)}_{0}(t)) \\
			&= \frac{e^{\|t\|^2/2}}{\sqrt{n}} \sum^n_{i=1} \left\{ - \frac{1}{2} (X_iX_i^\top t - t) - X_i -\frac{1}{2}(\textbf{I}_p + tt^\top) (X_i X_i^\top - \textbf{I}_p)t - tt^\top X_i \right\} + o_p(1)\\
			&= \frac{e^{\|t\|^2/2}}{\sqrt{n}} \sum^n_{i=1} \left\{- (\textbf{I}_p +tt^\top)X_i - \frac{1}{2} (2 \textbf{I}_p + tt^\top) (X_iX_i^\top - \textbf{I}_p)t \right\} + o_p(1).
		\end{align*}
			
		\item 	Note that
		\begin{align*}
			Z_{n,i}Z_{n,i}^\top - X_i X_i^\top &= 	(\Delta_{n,i}+X_i)(\Delta_{n,i}+ X_i)^\top - X_i X_i^\top\\
			&=\Delta_{n,i} \Delta^\top_{n,i} + \Delta_{n,i} X^\top_i + X_i \Delta^\top_{n,i}.
		\end{align*}
		Thus,
		\begin{align*}
			&	\sqrt{n}( H_{M^{(n)}}(t) - H_{M^{(n)}_{0}}(t))\\
			%		&= \frac{1}{\sqrt{n}} \sum^n_{i=1} Z_{n,i}Z_{n,i}^\top e^{t^\top Z_{n,i}} - \frac{1}{\sqrt{n}} \sum^n_{i=1} X_i X_i^\top e^{t^\top X_i}\\
			&= \frac{1}{\sqrt{n}}\sum^n_{i=1} (Z_{n,i} Z_{n,i}^\top - X_i X_i^\top) e^{t^\top X_i} e^{t^\top \Delta_{n,i}} + \frac{1}{\sqrt{n}}\sum^n_{i=1} X_i X_i^\top e^{t^\top X_i} (e^{t^\top \Delta_{n,i}} - 1)\\
			&=C_{1n} + C_{2n} + C_{3n} +C_{4n},
		\end{align*}
		where 
		\begin{align*}
			C_{1n} &:= \frac{1}{\sqrt{n}}\sum^n_{i=1} \Delta_{n,i}\Delta_{n,i}^\top e^{t^\top X_i} e^{t^\top \Delta_{n,i}};\\
			C_{2n} &:= \frac{1}{\sqrt{n}}\sum^n_{i=1} \Delta_{n,i}X^\top_i e^{t^\top X_i} e^{t^\top \Delta_{n,i}};\\
			C_{3n} &:= \frac{1}{\sqrt{n}}\sum^n_{i=1} X_i\Delta_{n,i}^\top e^{t^\top X_i} e^{t^\top \Delta_{n,i}};\\
			C_{4n} &:= \frac{1}{\sqrt{n}} \sum^n_{i=1} X_i X_i^\top e^{t^\top X_i} \left\{ t^\top \Delta_{n,i} + \frac{1}{2}(t^\top \Delta_{n,i})^2e^{\theta_{n,i}(t) t^\top \Delta_{n,i}} \right\}.
		\end{align*}
		For $C_{1n}$, by (\ref{eq:t_Delta_ni2}),
		\begin{align*}
			\|C_{1n}\|_2 &\leq \sqrt{n} \left| \frac{1}{n}\sum^n_{i=1}e^{t^\top X_i} \right| \max_{i=1,\ldots,n} \|\Delta_{n,i}\|^2 e^{\| t\| \max_{i=1,\ldots,n} \|\Delta_{n,i}\|} \\
			&= \sqrt{n} O_p(1) O_p(n^{-1} (\log n)^2)O_p(1) = o_p(1).
		\end{align*}
		For $C_{2n}$, by (\ref{eq:S_squareroot_I}),
		\begin{align*}
			C_{2n} &=\frac{1}{\sqrt{n}}\sum^n_{i=1} (S^{-1/2}_n - \textbf{I}_p) X_i X_i^\top e^{t^\top X_i} - \frac{1}{\sqrt{n}}\sum^n_{i=1} (S^{-1/2}_n - \textbf{I}_p) \overline{X}_n X_i^\top e^{t^\top X_i} \\
			& \quad - \frac{1}{\sqrt{n}} \sum^n_{i=1} \overline{X}_n X_i^\top e^{t^\top X_i} + o_p(1)\\
			&= \frac{1}{n}\sum^n_{i=1} \left\{- \frac{1}{2\sqrt{n}} \sum^n_{j=1}(X_j X_j^\top - \textbf{I}_p) + O_p(n^{-1/2}) \right\} X_i X_i^\top e^{t^\top X_i} + O_p(n^{-1/2})\\
			& \quad - \frac{1}{\sqrt{n}} \sum^n_{i=1} X_i \left( t^\top e^{\|t\|^2/2} + o_p(1) \right) + o_p(1)\\
			&=- \frac{1}{2\sqrt{n}}\sum^n_{i=1} (X_i X_i^\top - \textbf{I}_p) e^{\|t\|^2/2} (\textbf{I}_p+ tt^\top + o_p(1)) - \frac{e^{\|t\|^2/2}}{\sqrt{n}}\sum^n_{i=1} X_i t^\top + o_p(1) \\
			&= -\frac{e^{\|t\|^2/2}}{2\sqrt{n}}\sum^n_{i=1}(X_i X_i^\top - \textbf{I}_p) (\textbf{I}_p + tt^\top) -
			\frac{e^{\|t\|^2/2}}{\sqrt{n}}\sum^n_{i=1} X_i t^\top + o_p(1).
		\end{align*}
		Since $C_{3n} = C_{2n}^\top$, we have
		\begin{equation*}
			C_{3n} = -\frac{e^{\|t\|^2/2}}{2\sqrt{n}}\sum^n_{i=1}  (\textbf{I}_p + tt^\top)(X_i X_i^\top - \textbf{I}_p) -
			\frac{e^{\|t\|^2/2}}{\sqrt{n}}\sum^n_{i=1} t X^\top_i  + o_p(1).
		\end{equation*}
		For $C_{4n}$, 		
		\begin{align*}
			C_{4n} &= \frac{1}{\sqrt{n}} \sum^n_{i=1} X_i X_i^\top e^{t^\top X_i} t^\top (S^{-1/2}_n - \textbf{I}_p) X_i - \frac{1}{\sqrt{n}}\sum^n_{i=1} X_i X_i^\top e^{t^\top X_i} t^\top (S^{-1/2}_n - \textbf{I}_p)\overline{X}_n \\
			&\quad - \frac{1}{\sqrt{n}} \sum^n_{i=1} X_i X_i^\top e^{t^\top X_i} t^\top \overline{X}_n + o_p(1)\\
			&=\frac{1}{n}\sum^n_{i=1} e^{t^\top X_i} X_i X_i^\top X^\top_i \left\{ - \frac{1}{2\sqrt{n}} \sum^n_{j=1}(X_j X_j^\top - \textbf{I}_p) +O_p(n^{-1/2}) \right\} t + O_p(n^{-1/2})\\
			& \quad - \frac{e^{\|t\|^2/2}}{\sqrt{n}} (\textbf{I}_p + tt^\top) \sum^n_{i=1} t^\top X_i + o_p(1)\\
			&=\frac{1}{n}\sum^n_{i=1} e^{t^\top X_i} X_i X_i^\top X^\top_i \left\{ - \frac{1}{2\sqrt{n}} \sum^n_{j=1}(X_j X_j^\top - \textbf{I}_p) \right\} t  - \frac{e^{\|t\|^2/2}}{\sqrt{n}} (\textbf{I}_p + tt^\top) \sum^n_{i=1} t^\top X_i \\
			& \quad + o_p(1).
		\end{align*}
		Denote $\tilde{C}_n:= - \frac{1}{2\sqrt{n}} \sum^n_{j=1}(X_j X_j^\top - \textbf{I}_p)$.
		Note that
		\begin{align*}
			&	\frac{1}{n}\sum^n_{i=1} e^{t^\top X_i} X_i X_i^\top X^\top_i \tilde{C}_n t\\
			&= \frac{1}{n}\sum^n_{i=1} e^{t^\top X_i} X_i X_i^\top \sum^p_{h=1}\sum^p_{k=1} X_{ih} \tilde{C}_{n,hk}t_k\\
			&= \sum^p_{h=1}\left\{ \left( \frac{1}{n}\sum^n_{i=1} e^{t^\top X_i} X_i X_i^\top X_{ih} \right) \sum^p_{k=1} t_k \tilde{C}_{n,hk}\right\}\\
			&= \sum^p_{h=1}\left\{ \mathbb{E}(e^{t^\top X} XX^\top X_h) \sum^p_{k=1} t_k \tilde{C}_{n,hk}\right\} +o_p(1)\\
			&= - \frac{1}{2\sqrt{n}}\sum^n_{i=1} \left\{ \sum^p_{h=1} \sum^p_{k=1} \mathbb{E}(e^{t^\top X} XX^\top X_h) t_k (X_{ih} X_{ik} - \mathbbm{1}(h=k)) \right\} + o_p(1).
		\end{align*}
		Thus, by (\ref{eq:EXXXh}), 
		\begin{align*}
			C_{4n} 
			&= - \frac{e^{\|t\|^2/2}}{2\sqrt{n}}\sum^n_{i=1} \left\{ \sum^p_{h=1} \sum^p_{k=1} \left( \frac{\partial tt^\top}{\partial t_h} + tt^\top t_h + t_h \textbf{I}_p \right) t_k (X_{ih} X_{ik} - \mathbbm{1}(h=k)) \right\} \\
			& \quad - \frac{e^{\|t\|^2/2}}{\sqrt{n}} (\textbf{I}_p + tt^\top) \sum^n_{i=1} t^\top X_i + o_p(1).
		\end{align*}
	\end{enumerate}
	
\end{proof}



\begin{proof}[Proof of Lemma \ref{lemma:Mn_I}]
	\begin{enumerate}[(a)]
		\item We can write
		\begin{equation*}
			M^{(n)}_{0}(t) H_{M^{(n)}_{0}}(t) - \triangledown M^{(n)}_{0}(t) (\triangledown M^{(n)}_{0}(t))^\top - (M^{(n)}_{0}(t))^2 \textbf{I}_p = E_{1n} + E_{2n} + E_{3n},
		\end{equation*}
		where
		\begin{align*}
			E_{1n} &:= M_0(t)H_{M^{(n)}_{0}}(t) - \triangledown M_0(t) (\triangledown M^{(n)}_{0}(t))^\top - M_0(t) M^{(n)}_{0}(t) \textbf{I}_p; \\
			E_{2n} &:= \{M^{(n)}_{0}(t) - M_0(t)\} H_{M_0}(t) - \{\triangledown M^{(n)}_{0}(t) - \triangledown M_0(t)\} (\triangledown M_{0}(t))^\top \\
			& \quad - \{M^{(n)}_{0}(t) - M_0(t)\} M_{0}(t) \textbf{I}_p; \\
			E_{3n} &:= \{M^{(n)}_{0}(t) - M_0(t)\} \{H_{M^{(n)}_{0}}(t) - H_{M_{0}}(t)\} \\
			& \quad - \{\triangledown M^{(n)}_{0}(t) -  \triangledown M_0(t)\} \{ \triangledown  M^{(n)}_{0}(t)  - \triangledown  M_0(t) \}^\top\\
			&\quad  - \{M^{(n)}_{0}(t) - M_0(t)\} \{M^{(n)}_{0}(t) - M_0(t)\} \textbf{I}_p.
		\end{align*}
		For $E_{1n}$, we have
		\begin{align*}
			E_{1n} &= \frac{1}{n}\sum^n_{i=1} \bigg( e^{\|t\|^2/2}X_iX_i^\top e^{t^\top X_i} - e^{\|t\|^2/2}t X_i^\top e^{t^\top X_i} - e^{\|t\|^2/2}e^{t^\top X_i} \textbf{I}_p \bigg)\\
			&= \frac{e^{\|t\|^2/2}}{n} \sum^n_{i=1} e^{t^\top X_i} (X_i X_i^\top - t X_i^\top - \textbf{I}_p).
		\end{align*}
		For $E_{2n}$, we have
		\begin{align*}
			E_{2n}&:= \frac{1}{n}\sum^n_{i=1}(e^{t^\top X_i} - e^{\|t\|^2/2})\{ H_{M_0}(t) - M_0(t)\textbf{I}_p\} \\
			& \quad -  \frac{1}{n}\sum^n_{i=1}(X_i e^{t^\top 
				X_i} - t e^{\|t\|^2/2}) (\triangledown M_0(t))^\top	\\
			&= \frac{e^{\|t\|^2/2}}{n} \sum^n_{i=1} \left\{ (e^{t^\top X_i} - e^{\|t\|^2/2}) tt^\top  - (X_it^\top e^{t^\top X_i} - tt^\top e^{\|t\|^2/2}) \right\} \\
			&= \frac{e^{\|t\|^2/2}}{n} \sum^n_{i=1} \left\{ e^{t^\top X_i} (tt^\top - X_it^\top)  \right\}.
		\end{align*}
		By Lemma \ref{lemma:Mn_Mn0},  $E_{3n} = O_p(n^{-1})$. Thus, the result follows.
		
		\item 
		Denote $F_{1n} := M^{(n)}(t) H_{M^{(n)}}(t)$, $F_{2n} := \triangledown M^{(n)}(t) (\triangledown M^{(n)}(t))^\top$ and $F_{3n} :=  (M^{(n)}(t))^2 \textbf{I}_p$. Then,
		\begin{equation*}
			M^{(n)}(t) H_{M^{(n)}}(t) - \triangledown M^{(n)}(t) (\triangledown M^{(n)}(t))^\top - (M^{(n)}(t))^2 \textbf{I}_p = F_{1n} - F_{2n} - F_{3n}.
		\end{equation*}
		Recall that $\mathbb{E}(f_j(X_i;t)) = 0$, by the central limit theorem $n^{-1}\sum^n_{i=1}f_j(X_i;t) = O_p(n^{-1/2})$ for $j=0,1,2$. By Lemma \ref{lemma:Mn_Mn0}, 
		\begin{align*}
			F_{1n} &= \left( M^{(n)}_{0}(t) + \frac{1}{n}\sum^n_{i=1}f_0(X_i;t) + o_p(n^{-1/2})\right)
			\left(H_{M^{(n)}_{0}}(t) + \frac{1}{n}\sum^n_{i=1}f_2(X_i;t) + o_p(n^{-1/2})\right) \\
			&= M^{(n)}_{0}(t) H_{M^{(n)}_{0}}(t)+ M^{(n)}_{0}(t) \left(\frac{1}{n}\sum^n_{i=1} f_2(X_i;t)\right) + \left(\frac{1}{n}\sum^n_{i=1} f_0(X_i;t)\right) H_{M^{(n)}_{0}}(t) + o_p(n^{-1/2}) \\
			&= M^{(n)}_{0}(t) H_{M^{(n)}_{0}}(t)+ M_{0}(t) \left(\frac{1}{n}\sum^n_{i=1} f_2(X_i;t)\right) + \left(\frac{1}{n}\sum^n_{i=1} f_0(X_i;t)\right) H_{M_{0}}(t) + o_p(n^{-1/2}),
		\end{align*}
		where the last equality holds as $(M^{(n)}_{0}(t) - M_0(t))\left( n^{-1}\sum^n_{i=1}f_2(X_i;t) \right)= O_p(n^{-1/2}) O_p(n^{-1/2}) = O_p(n^{-1})$ and $\left( n^{-1}\sum^n_{i=1}f_0(X_i;t) \right)(H_{M^{(n)}_{0}} (t) - H_{M_0}(t)) =  O_p(n^{-1/2}) O_p(n^{-1/2}) = O_p(n^{-1})$, by the central limit theorem. Thus,
		\begin{equation}\label{eq:lemma_Bn1}
			F_{1n} = M^{(n)}_{0}(t) H_{M^{(n)}_{0}}(t) + \frac{e^{\|t\|^2/2}}{n}\sum^n_{i=1} \left\{
			f_2(X_i;t) + (tt^\top + \textbf{I}_p) f_0(X_i;t)	\right\} + o_p(n^{-1/2}).
		\end{equation}
		Similarly, by  Lemma \ref{lemma:Mn_Mn0}, 
		\begin{align*}
F_{2n} 	&= \left(\triangledown M^{(n)}_{0}(t) + \frac{1}{n}\sum^n_{i=1}f_1(X_i;t) + o_p(n^{-1/2})\right)\\
				&\quad \quad 
				\left( \triangledown M^{(n)}_{0}(t) + \frac{1}{n}\sum^n_{i=1}f_1(X_i;t) + o_p(n^{-1/2})\right)^\top \\
			&= \triangledown M^{(n)}_{0}(t) (\triangledown M^{(n)}_{0}(t))^\top + \triangledown M^{(n)}_{0}(t) \left(\frac{1}{n}\sum^n_{i=1}f_1(X_i;t)^\top \right) \\
			& \quad + \left( \frac{1}{n}\sum^n_{i=1}f_1(X_i;t)\right) (\triangledown M^{(n)}_{0}(t))^\top + o_p(n^{-1/2}) \\
			&=\triangledown M^{(n)}_{0}(t) (\triangledown M^{(n)}_{0}(t))^\top + \triangledown M_{0}(t) \left( \frac{1}{n}\sum^n_{i=1}f_1(X_i;t)^\top \right)\\
			& \quad + \left( \frac{1}{n}\sum^n_{i=1}f_1(X_i;t)\right) (\triangledown M_{0}(t))^\top   + o_p(n^{-1/2}).
		\end{align*}
		Thus,
		\begin{equation}\label{eq:lemma_Bn2}
			F_{n2} = \triangledown M^{(n)}_{0}(t) (\triangledown M^{(n)}_{0}(t))^\top + \frac{e^{\|t\|^2/2}}{n}\sum^n_{i=1} \left(
			t f^\top_1(X_i;t) + f_1(X_i;t)t^\top 
			\right) + o_p(n^{-1/2}).
		\end{equation}
		Using the same argument, we have
		\begin{align}
			F_{3n} &=
			\left( M^{(n)}_{0}(t) + \frac{1}{n}\sum^n_{i=1}f_0(X_i;t) + o_p(n^{-1/2}) \right)^2\textbf{I}_p \nonumber \\
			&=\left( \left(M^{(n)}_{0}(t)\right)^2 + \frac{2 M^{(n)}_{0}(t)}{n} \sum^n_{i=1}f_0(X_i;t) + o_p(n^{-1/2})\right) \textbf{I}_p  \nonumber\\
			&=	\left( M^{(n)}_{0}(t)\right)^2  \textbf{I}_p + \frac{e^{\|t\|^2/2}}{n}\sum^n_{i=1}(2 f_0(X_i;t) \textbf{I}_p ) + o_p(n^{-1/2}) \label{eq:lemma_Bn3}.
		\end{align}
		The result follows by combining (\ref{eq:lemma_Bn1}), (\ref{eq:lemma_Bn2}), and (\ref{eq:lemma_Bn3}) and part (a) of this lemma.
		
	\end{enumerate}
	
\end{proof}


\begin{proof}[Proof of Theorem \ref{thm:H_I}]
	By Lemma \ref{lemma:Mn_Mn0} (a) and Lemma \ref{lemma:Mn_I} (b), we have
	\begin{align*}
		&	\sqrt{n}(H_{\Lambda_n}(t) - \textbf{I}_p)\\
		&= (M^{(n)}(t))^{-2} \sqrt{n} \{ M^{(n)}(t)H_{M^{(n)}}(t) - \triangledown M^{(n)}(t) (\triangledown  M^{(n)}(t))^\top - (M^{(n)})^2(t) \textbf{I}_p\}\\
		&= M^{-2}_0(t) \sqrt{n} \{ M^{(n)}(t)H_{M^{(n)}}(t) - \triangledown M^{(n)}(t) (\triangledown  M^{(n)}(t))^\top - (M^{(n)})^2(t) \textbf{I}_p\} \\
		&\quad + \{(M^{(n)}(t))^{-2}-M^{-2}_0(t)\} \sqrt{n} \{ M^{(n)}(t)H_{M^{(n)}}(t) - \triangledown  M^{(n)}(t) (\triangledown  M^{(n)}(t))^\top - (M^{(n)})^2(t) \textbf{I}_p\}\\
		&= \frac{1}{\sqrt{n}}\sum^n_{i=1} h(X_i;t) +o_p(1) +O_p(n^{-1/2})O_p(1).
	\end{align*}
	Thus, the results follow.
\end{proof}


	
	
	\subsection{Proofs for Section \ref{sect:consistency}}
The following lemma is a generalization of Proposition 5 in  \cite{henze2020testing}, where that proposition corresponds to the case when $A = \textbf{I}_p$.
	\begin{lemma}\label{lemma:matrix_sqrt_conv}
		Let $A_n$ be a sequence of $p \times p$ symmetric positive definite matrices, $A$ be a $p \times p$ symmetric positive definite matrix, and $b_n$ be an increasing sequence of positive real numbers satisfying $\lim_{n\to\infty}b_n = \infty$. Suppose that $			\lim_{n\to\infty}b_n\|A_n - A\|_2 = 0$, then
		\begin{equation*}
			\lim_{n\to\infty}b_n\|A_n^{-1/2} - A^{-1/2}\|_2 = 0. 		
		\end{equation*}
		
	\end{lemma}

	\begin{proof}[Proof of Lemma \ref{lemma:matrix_sqrt_conv}]
 Note that 
		\begin{align*}
			A_n - A =&~ (A_n^{1/2} + A^{1/2})(A_n^{1/2} - A^{1/2})\\
			=&~ (A_n^{1/2} + A^{1/2})A_n^{1/2}(A^{-1/2} - A_n^{-1/2})A^{1/2}.
		\end{align*}
		Thus, we have
		\begin{equation}\label{eq:matrix_key_inq}
			b_n\|A^{-1/2} - A_n^{-1/2}\|_2 
			\leq \|A_n^{-1/2}\|_2 \cdot
			\|(A_n^{1/2} + A^{1/2})^{-1}\|_2 \cdot
			\|A^{-1/2}\|_2 \cdot 	 (b_n \|A_n - A\|_2).
		\end{equation}
		Since $\lim_{n \rightarrow \infty}b_n\|A_n - A\|_2 = 0$, it remains to show that the other terms on the RHS of  (\ref{eq:matrix_key_inq}) are bounded. Clearly, $\|A^{-1/2}\|_2 < \infty$.
		Denote $\lambda_{\min}(B)$ to be the smallest eigenvalue of a matrix $B$. Let $\lambda_1 := \lambda_{\min}(A) > 0$ since $A$ is positive definite. Since $\|\cdot\|_2$ is the spectral norm, by Weyl's inequality, we have
		\begin{align}\label{eq:40} 
			\|(A_n^{1/2} + A^{1/2})^{-1}\|_2
			= \frac{1}{\lambda_{\min}\left(A^{1/2}_n + A^{1/2}\right)} 
			\leq \frac{1}{\lambda_{\min}(A^{1/2}_n) + \lambda_1^{1/2}} \leq \frac{1}{\lambda^{1/2}_1} < \infty. 
		\end{align}
		Let $F_n := A - A_n$. 	Since $\lim_{n \rightarrow \infty} b_n\|A_n - A\|_2 = 0$, there exists $n_0 \in \mathbb{N}$ such that for all $n \geq n_0$, $\|F_n\|_2 = \|A_n - A\|_2 \leq \lambda_1/2$. 
		For all $n \geq n_0$, by Weyl's inequality again,
		\begin{align*}
			\|A_n^{-1}\|_2
			=&~ \frac{1}{\lambda_{\min}(A_n)} =  \frac{1}{\lambda_{\min}(A - F_n)} \leq \frac{1}{\lambda_1 + \lambda_{\min}(-F_n)} \\
			\leq&~ \frac{1}{\lambda_1 - \|F_n\|_2 } \leq  \frac{1}{\lambda_1 - \lambda_1/2} =  \frac{2}{\lambda_1} <\infty.
		\end{align*}
		This implies that $\|A_n^{-1/2}\|_2\leq \sqrt{2/\lambda_1} < \infty$ for all $n \geq n_0$.
	\end{proof}
	
Let $\tilde{X}_i := \Sigma^{-1/2}X_i$ for $i=1,\ldots,n$. Denote $\tilde{M}^{(n)}(t) := \frac{1}{n}\sum^n_{i=1} e^{t^\top \tilde{X}_i}$, $\triangledown \tilde{M}^{(n)}(t) := \frac{1}{n}\sum^n_{i=1} \tilde{X}_i e^{t^\top \tilde{X}_i}$ and $H_{\tilde{M}^{(n)}}(t) := \frac{1}{n}\sum^n_{i=1} \tilde{X}_i \tilde{X} _i e^{t^\top \tilde{X}_i}$.
	\begin{lemma}\label{lemma:alternative_M}
		Suppose that the moment generating function of $X$ exists and is twice differentiable. 	We have 
		\begin{enumerate}[(a)]
			\item $M^{(n)}(t) - \tilde{M}^{(n)}(t) \stackrel{a.s.}{\rightarrow} 0$;
			
			\item $\| \triangledown M^{(n)}(t) - \triangledown  \tilde{M}^{(n)}(t) \| \stackrel{a.s.}{\rightarrow} 0$;
			
			\item $\|H_{M^{(n)}}(t) - H_{\tilde{M}^{(n)}}(t)\|_2 \stackrel{a.s.}{\rightarrow} 0$.
			
		\end{enumerate}
	\end{lemma}
	
	\begin{proof}[Proof of Lemma \ref{lemma:alternative_M}]
		Define
		\begin{equation*}
			\tilde{\Delta}_{n,i} := Z_{n,i} - \tilde{X}_i = (S^{-1/2}_n - \Sigma^{-1/2})X_i - S^{-1/2}_n \overline{X}_n.
		\end{equation*}
		Let $\xi_n := \max_{i=1,\ldots,n}\| \tilde{\Delta}_{n,i}\|$. Then,
		\begin{equation}\label{eq:xi_n_ineq}
			\xi_n \leq n^{1/4} \|S^{-1/2}_n - \Sigma^{-1/2}\|_2 \cdot n^{-1/4} \max_{i=1,\ldots,n}\|X_j\| + \|S^{-1/2}_n\|_2 \|\overline{X}_n\|.
		\end{equation}
		Since the existence of the moment generating function implies that $\mathbb{E}\|X\|^4 < \infty$, Theorem 5.2 of \cite{barndorff1963limit} gives $n^{-1/4}\max_{i=1,\ldots,n}\|X_i\| \rightarrow 0$ almost surely.
		As $S_n - \Sigma = n^{-1}\sum^n_{i=1}(X_i X_i^\top - \textbf{I}_p) - \overline{X}_n \overline{X}^\top_n$, Kolmogorov's variance criterion for averages (see \cite{kallenberg2021foundations} p.113) implies that $n^{1/2-\varepsilon}\|S_n - \Sigma\|_2 \rightarrow 0$ almost surely for any $\varepsilon > 0$. Lemma \ref{lemma:matrix_sqrt_conv} then yields $n^{1/2-\varepsilon}\|S^{-1/2}_n - \Sigma^{-1/2}\|_2 \rightarrow 0$ almost surely. From the proof of Lemma \ref{lemma:matrix_sqrt_conv}, we also know that $\sup_{n\geq 1}\|S^{-1/2}_n\|_2 < \infty$ almost surely. By the strong law of large numbers, $\|\overline{X}_n\| \rightarrow 0$ almost surely. In view of (\ref{eq:xi_n_ineq}),  with probability one,
		\begin{equation*}
			\xi_n \leq o(n^{-1/4+\varepsilon}) o(1) + o(1) = o(1)
		\end{equation*}
	for $\varepsilon < 1/4$. Thus,
		\begin{equation}\label{eq:xi_n_limit}
			\lim_{n \rightarrow \infty} \xi_n  = 0, \quad \text{a.s.}
		\end{equation}
		By Taylor's theorem, for some $|\theta_{n,i}(t)|\leq 1$, we have
		\begin{equation}\label{eq:Taylor_Deltao}
			e^{t^\top \tilde{\Delta}_{n,i}}  = 1 + t^\top \tilde{\Delta}_{n,i} + \frac{1}{2} (t^\top \tilde{\Delta}_{n,i})^2 e^{\theta_{n,i}(t) t^\top \tilde{\Delta}_{n,i}}.
		\end{equation}
		 By (\ref{eq:Taylor_Deltao}),
			\begin{align*}
				M^{(n)}(t) - \tilde{M}^{(n)}(t) 	&=\frac{1}{n}\sum^n_{i=1} e^{t^\top \tilde{X}_i}( e^{t^\top \tilde{\Delta}_{n,i}} - 1)\\
				&=\frac{1}{n} \sum^n_{i=1} e^{t^\top \tilde{X}_i} \left\{ t^\top \tilde{\Delta}_{n,i} + \frac{1}{2}(t^\top \tilde{\Delta}_{n,i})^2 e^{\theta_{n,i}(t) t^\top \tilde{\Delta}_{n,i}} \right\}.
			\end{align*}
			Thus, as $\xi_n = \max_{i=1,\ldots,n} \|\tilde{\Delta}_{n,i}\|$,
			\begin{align}
				|M^{(n)}(t)  - \tilde{M}^{(n)}(t)| & \leq \left| \tilde{M}^{(n)}(t) \right| \max_{i=1,\ldots,n} \left| 
				t^\top \tilde{\Delta}_{n,i} + \frac{1}{2}(t^\top \tilde{\Delta}_{n,i})^2 e^{\theta_{n,i}(t) t^\top \tilde{\Delta}_{n,i}} \right| \nonumber \\
				& \leq \left|\tilde{M}^{(n)}(t)\right| \max_{i=1,\ldots,n}  \bigg(\|t\| \cdot \|\tilde{\Delta}_{n,i}\|
				+ \frac{1}{2} \|t\|^2 \cdot \| \tilde{\Delta}_{n,i}\|^2 e^{\theta_{n,i}(t) \|t\| \cdot \| \tilde{\Delta}_{n,i}\|}\bigg) \nonumber  \\
				& \leq \left|\tilde{M}^{(n)}(t) \right| \bigg(\|t\| \xi_n
				+ \frac{1}{2} \|t\|^2 \xi_n^2 e^{ \|t\| \xi_n}\bigg). \label{eq:consistency_lemma_M}
			\end{align}
Recall that $\tilde{M}(t) := \mathbb{E}(e^{t^\top \tilde{X}})$.
			By the strong law of large numbers, $\tilde{M}^{(n)}(t) \stackrel{a.s.}{\rightarrow} \tilde{M}(t)$. Combining (\ref{eq:consistency_lemma_M}) and (\ref{eq:xi_n_limit}), we deduce that
			\begin{equation}
				M^{(n)}(t) - \tilde{M}^{(n)}(t) \stackrel{a.s.}{\rightarrow}  0.
			\end{equation}
Similarly, using (\ref{eq:Taylor_Deltao}), we have
			\begin{align*}
				& \triangledown M^{(n)}(t) - \triangledown \tilde{M}^{(n)}(t) \\
				&= \frac{1}{n}\sum^n_{i=1}(Z_{n,i} - \tilde{X}_i) e^{t^\top Z_{n,i}}+ \frac{1}{n}\sum^n_{i=1}\tilde{X}_i (e^{t^\top Z_{n,i}} - e^{t^\top \tilde{X}_i} ) \\
				&= \frac{1}{n}\sum^n_{i=1} e^{t^\top \tilde{X}_i} \tilde{\Delta}_{n,i}\cdot e^{t^\top \tilde{\Delta}_{n,i}} + 
				\frac{1}{n}\sum^n_{i=1} \tilde{X}_i e^{t^\top \tilde{X}_i} \left\{ t^\top \tilde{\Delta}_{n,i} + \frac{1}{2} (t^\top \tilde{\Delta}_{n,i})^2 e^{\theta_{n,i}(t) t^\top \tilde{\Delta}_{n,i}}\right\}.
			\end{align*}
By the facts that $\tilde{M}^{(n)}(t) \stackrel{a.s.}{\rightarrow} \tilde{M}(t)$, $\|\triangledown \tilde{M}^{(n)}(t)\| \stackrel{a.s.}{\rightarrow} \| \triangledown \tilde{M}(t)\|$, and (\ref{eq:xi_n_limit}), we obtain that
			\begin{align*}
				&\left\| \triangledown M^{(n)}(t) - \triangledown \tilde{M}^{(n)}(t) \right\| \\
				& \leq |\tilde{M}^{(n)}(t)| \max_{i=1,\ldots,n} \|\tilde{\Delta}_{n,i} e^{t^\top \tilde{\Delta}_{n,i}}\| + \|\triangledown \tilde{M}_{n}(t)\| \max_{i=1,\ldots,n} \left| t^\top \tilde{\Delta}_{n,i} + \frac{1}{2} (t^\top \tilde{\Delta}_{n,i})^2 e^{\theta_{n,i}(t) t^\top \tilde{\Delta}_{n,i}}\right|\\
				& \leq |\tilde{M}^{(n)}(t)| \xi_n e^{\|t\| \xi_n} + \|\triangledown \tilde{M}_{n}(t)\| \left(\|t\| \xi_n + \frac{1}{2} \|t\|^2\xi_n^2 e^{\|t\| \xi_n} \right) \stackrel{a.s}{\rightarrow} 0.
			\end{align*}
			Similar to the proof of Lemma \ref{lemma:Mn_Mn0},  we expand
			\begin{align*}
				H_{M^{(n)}}(t) - H_{\tilde{M}^{(n)}}(t) &=: J_{1n} + J_{2n} + J_{3n} + J_{4n},
			\end{align*}
			where
			\begin{align*}
				J_{1n} &:= \frac{1}{n}\sum^n_{i=1} \tilde{\Delta}_{n,i}\tilde{\Delta}_{n,i}^\top e^{t^\top \tilde{X}_i} e^{t^\top \tilde{\Delta}_{n,i}};\\
				%
				J_{2n} &:= \frac{1}{n}\sum^n_{i=1} \tilde{\Delta}_{n,i}\tilde{X}_i^\top e^{t^\top \tilde{X}_i} e^{t^\top \tilde{\Delta}_{n,i}};\\
%
				J_{3n} &:= \frac{1}{n}\sum^n_{i=1} \tilde{X}_i \tilde{\Delta}_{n,i}^\top e^{t^\top \tilde{X}_i} e^{t^\top \tilde{\Delta}_{n,i}};\\
				%
				J_{4n} &:= \frac{1}{n} \sum^n_{i=1} \tilde{X}_i \tilde{X}_i^\top e^{t^\top \tilde{X}_i} \left\{ t^\top \tilde{\Delta}_{n,i} + \frac{1}{2}(t^\top \tilde{\Delta}_{n,i})^2e^{\theta_{n,i}(t) t^\top \tilde{\Delta}_{n,i}} \right\}.
			\end{align*}
We  shall show that the spectral norms of each of the above terms all converge	to $0$ almost surely.		For $J_{1,n}$, we see that
			\begin{equation*}
				\|J_{1n}\|_2  \leq  \xi^2_n |\tilde{M}^{(n)}(t)| e^{\|t\|\xi_n}\stackrel{a.s.}{\rightarrow} 0.
			\end{equation*}
			For $J_{2n}$, we have
			\begin{equation*}
				\|J_{2n}\|_2 \leq \frac{1}{n}\sum^n_{i=1}\|\tilde{\Delta}_{n,i}\| \cdot \|\tilde{X}_i\| e^{t^\top \tilde{X}_i} e^{\|t\| \xi_n} \leq \xi_n e^{\|t\| \xi_n} \left( \frac{1}{n}\sum^n_{i=1}\|\tilde{X}_i\|e^{t^\top \tilde{X}_i}\right) \stackrel{a.s.}{\rightarrow} 0,
			\end{equation*}
		as $n^{-1}\sum^n_{i=1}\|\tilde{X}_i\| e^{t^\top \tilde{X}_i} \stackrel{a.s.}{\rightarrow} \mathbb{E}(\|\tilde{X}\|e^{t^\top \tilde{X}})$.
			As $J_{3n} = J_{2n}^\top$, we have $\|J_{3n}\|_2 \stackrel{a.s.}{\rightarrow} 0$. Finally, by the fact that $H_{\tilde{M}^{(n)}}(t) \stackrel{a.s.}{\rightarrow} H_{\tilde{M}}(t)$ and (\ref{eq:xi_n_limit}),
			\begin{equation*}
				\|J_{4n}\|_2 \leq \|H_{\tilde{M}^{(n)}}(t)\|_2 \left(\|t\| \xi_n + \frac{1}{2} \|t\|^2\xi_n^2 e^{\|t\| \xi_n} \right) \stackrel{a.s.}{\rightarrow} 0.
			\end{equation*}

	\end{proof}
	
	
	\begin{proof}[Proof of Theorem \ref{thm:consistency_limit}]
We only prove the multivariate case as the univariate case follows by changing the notation.	Simple algebra shows that
		\begin{align*}
		H_{\Lambda^{(n)}}(t) - \textbf{I}_p =	\frac{M^{(n)}(t) H_{M^{(n)}}(t) - \triangledown M^{(n)}(t) (\triangledown M^{(n)}(t))^\top}{(M^{(n)}(t))^2} - \textbf{I}_p =: Q_{1n}(t) + Q_{2n}(t) + Q_{3n}(t),
		\end{align*}
		where
		\begin{align*}
			Q_{1n}(t) &:= \frac{\tilde{M}^{(n)}(t) H_{\tilde{M}^{(n)}}(t) - \triangledown \tilde{M}^{(n)}(t) (\triangledown \tilde{M}^{(n)}(t))^\top}{(\tilde{M}^{(n)}(t))^2} - \textbf{I}_p;\\
			%
			Q_{2n}(t) &:= \left\{M^{(n)}(t) H_{M^{(n)}}(t) - \triangledown M^{(n)}(t) (\triangledown M^{(n)}(t))^\top \right\}  \left\{ \frac{ (\tilde{M}^{(n)}(t))^2 - (M^{(n)}(t))^2 }{ (M^{(n)}(t) \tilde{M}^{(n)}(t))^2 } \right\}; \\
			%
			Q_{3n}(t) &:=\frac{1}{(\tilde{M}^{(n)}(t))^2} 	\bigg\{M^{(n)}(t) H_{M^{(n)}}(t) - \triangledown M^{(n)}(t) (\triangledown M^{(n)}(t))^\top  \\
			& \quad \quad\quad- \tilde{M}^{(n)}(t) H_{\tilde{M}^{(n)}}(t) + \triangledown \tilde{M}^{(n)}(t) (\triangledown \tilde{M}^{(n)}(t))^\top \bigg\}.
		\end{align*}
		By the strong law of large numbers, 
		\begin{equation*}
			Q_{1n}(t) \stackrel{a.s.}{\rightarrow} \frac{\tilde{M}(t)H_{\tilde{M}}(t) - \triangledown \tilde{M}(t) (\triangledown \tilde{M}(t))^\top}{(\tilde{M}(t))^2} - \textbf{I}_p = H_{\tilde{\Lambda}}(t) - \textbf{I}_p.
		\end{equation*}
	According to Lemma \ref{lemma:alternative_M}, it is straightforward to show that $Q_{2n}(t) \stackrel{a.s.}{\rightarrow} 0$ and $Q_{3n}(t) \stackrel{a.s.}{\rightarrow} 0$. The claims in the theorem then follow from the continuous mapping theorem.
	\end{proof}

\include{tables}


%
%\bigskip
%\begin{center}
%{\large\bf SUPPLEMENTARY MATERIAL}
%\end{center}
%
%\begin{description}
%
%\item[Title:] Supplementary material for Multivariate Normality Tests Based on Hessian of Cumulant Generating Function (pdf). This file contains the proofs of Theorem...., and additional simulation results.
%
%\item[R code:] The zip file contains R code that implement the proposed multivariate normality test and reproduce Tables.... 
%
%
%
%\end{description}


\bibliographystyle{Chicago}
\bibliography{normal_test_reference}


\end{document}
