% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage[colorlinks=true,linkcolor=black,citecolor=blue,urlcolor=blue,]{hyperref}
\usepackage[doipre={doi:~}]{uri}
\renewcommand\UrlFont{\color{blue}\rmfamily}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage[table,xcdraw]{xcolor}
\usepackage{booktabs}
\usepackage[misc]{ifsym}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
% \title{Iteratively Coupled Patch Embedding and Bag Classification for Multiple Instance Learning on Whole Slide Images}
\title{Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification}
%
\titlerunning{Iteratively Coupled Multiple Instance Learning}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Hongyi Wang\inst{1} \and
% index{Wang, Hongyi} 
Luyang Luo\inst{2} \and
% index{Luo, Luyang} 
Fang Wang\inst{3} \and
% index{Wang, Fang} 
Ruofeng Tong\inst{1,4} \\
% index{Tong, Ruofeng} 
Yen-Wei Chen\inst{5,1} \and
% index{Chen, Yen-Wei} 
Hongjie Hu\inst{3} \and
% index{Hu, Hongjie} 
Lanfen Lin\inst{1}$^{(\scriptsize\textrm{\Letter})}$ \and
% index{Lin, Lanfen} 
Hao Chen\inst{2,6}$^{(\scriptsize\textrm{\Letter})}$
% index{Chen, Hao} 
}
%
\authorrunning{Wang et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
% \institute{Princeton University, Princeton NJ 08544, USA \and
% Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
% \email{lncs@springer.com}\\
% \url{http://www.springer.com/gp/computer-science/lncs} \and
% ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
% \email{\{abc,lncs\}@uni-heidelberg.de}}
\institute{College of Computer Science and Technology, Zhejiang University, Hangzhou, China \and
Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China \and
Department of Radiology, Sir Run Run Shaw Hospital, Hangzhou, China \and
Research Center for Healthcare Data Science, Zhejiang Lab, Hangzhou, China \and
College of Information Science and Engineering, Ritsumeikan University, Kusatsu, Japan \and
Department of Chemical and Biological Engineering, The Hong Kong University of Science and Technology, Hong Kong, China \\
\email{llf@zju.edu.cn}
\email{jhc@cse.ust.hk}
}

%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
% Whole slide image (WSI) classification is a challenging task due to their very high resolution and the lack of fine-grained labels. Currently, classification for WSIs is usually formatted as a Multiple Instance Learning (MIL) problem when only slide-level labels are available. 
% % To reduce the computational cost, existing MIL methods usually embed the patches (i.e., the instances) in a WSI (i.e., the bag) into feature vectors through a pre-trained feature extractor. Without patch-level labels, feature extractor cannot be fine-tuned and thus may generate imperfect representations for the instances.  
% MIL methods generally consists of a patch embedding process and a bag-level classification process, and they cannot be trained end-to-end due to high computational cost. Such training scheme blocks patch embedder's access to the slide-level labels, and brings inconsistency into the entire MIL pipeline. To address this problem, we propose to bridge the loss back-propagation process from the bag level classifier to the patch embedder with a novel framework called Iteratively Coupled Multiple Instance Learning (ICMIL). In ICMIL, we propose to use the category information in the bag-level classifier to guide the patch-level fine-tuning of the patch feature extractor. Then, the refined extractor can be used to generate better instance representations for achieving a better bag-level classifier in return. In this way, patch embedder and bag classifier can be coupled together at a low cost, and the entire MIL classification model can benefit from the information exchange between the two processes. We have tested our framework on two datasets with three different backbones. The experimental results show that our framework can consistently improve the performance of different MIL methods. 

% By ChatGPT:
Whole Slide Image (WSI) classification remains a challenge due to their extremely high resolution and the absence of fine-grained labels. Presently, WSI classification is usually regarded as a Multiple Instance Learning (MIL) problem when only slide-level labels are available. MIL methods involve a patch embedding module and a bag-level classification module, but they are prohibitively expensive to be trained in an end-to-end manner. Therefore, existing methods usually train them separately, or directly skip the training of the embedder. Such schemes hinder the patch embedder's access to slide-level semantic labels, resulting in inconsistency within the entire MIL pipeline. To overcome this issue, we propose a novel framework called Iteratively Coupled MIL (ICMIL), which bridges the loss back-propagation process from the bag-level classifier to the patch embedder.
In ICMIL, we use category information in the bag-level classifier to guide the patch-level fine-tuning of the patch feature extractor. The refined embedder then generates better instance representations for achieving a more accurate bag-level classifier. By coupling the patch embedder and bag classifier at a low cost, our proposed framework enables information exchange between the two modules, benefiting the entire MIL classification model. We tested our framework on two datasets using three different backbones, and our experimental results demonstrate consistent performance improvements over state-of-the-art MIL methods. The code is available at: https://github.com/Dootmaan/ICMIL.

\keywords{Multiple Instance Learning \and Whole Slide Image \and Deep Learning.}
\end{abstract}
%
%
%
\section{Introduction}
% \subsection{A Subsection Sample}

% Nowadays, whole slide scanning is more and more widely used to visualize the tissue samples in disease diagnosis and pathological researches. Comapred with traditional microscope-based observation, whole slide scanning can convert glass slides into gigapixel digital images for convenient storage and further analysis. However, the high resolution of WSIs also makes the automated classification of them become very challenging. A common solution to this problem is patch-based classification \cite{cvpr2016em,chen2022deep}. Patch-based classification predicts the slide-level label by firstly predicting the labels of tiled small patches in a WSI. In this way, most existing image classification models can be directly applied, but at the cost of extra patch-level labeling. Unfortunately, patch-level labeling by histopathology experts is very expensive and time-consuming. Therefore, many weak-supervised \cite{cvpr2016em,zhang2018whole} and semi-supervised \cite{cheng2022deep,chen2022deep} methods have been proposed to generate patch-level pseudo labels at a low cost. However, the lack of reliable supervision directly hinders the performance of these methods, and there also can be serious class-imbalance problem since the tumor patches usually only account for a small portion of the entire WSI.

% By ChatGPT
Whole slide scanning is increasingly used in disease diagnosis and pathological research to visualize tissue samples. Compared to traditional microscope-based observation, whole slide scanning converts glass slides into gigapixel digital images that can be conveniently stored and analyzed. However, the high resolution of WSIs also makes their automated classification challenging \cite{lu2021ai}.
Patch-based classification is a common solution to this problem \cite{cvpr2016em,zhang2018whole,chen2022deep}. It predicts the slide-level label by first predicting the labels of small, tiled patches in a WSI. This approach allows for the direct application of existing image classification models, but requires additional patch-level labeling. Unfortunately, patch-level labeling by histopathology experts is expensive and time-consuming. Therefore, many weakly-supervised \cite{cvpr2016em,zhang2018whole} and semi-supervised \cite{cheng2022deep,chen2022deep} methods have been proposed to generate patch-level pseudo labels at a lower cost.
However, the lack of reliable supervision directly hinders the performance of these methods, and serious class-imbalance problems could arise, as tumor patches may only account for a small portion of the entire WSI \cite{dsmil}.

\begin{figure}[t]
\includegraphics[width=\textwidth]{MIL2.pdf}
\caption{The typical pipeline of traditional MIL methods on WSIs. } 
% \vspace{-3mm}
\label{MIL}
\end{figure}

% In contrast, MIL-based methods are becoming more preferred nowadays due to their only demand for slide-level labels. The typical pipeline of MIL methods is presented in Fig.~\ref{MIL}. As shown, MIL methods view a WSI as a bag, and consider the tiled patches in it as instances. The goal is to predict whether there is a positive instance (i.e., tumor patch) in a bag, and if there is, the bag is considered as positive too. However, in practical use, image patch instances are too large for the limited GPU memory, so firstly they will be sent into a fixed ImageNet pre-trained feature extractor $g(\cdot)$ to be converted into feature maps. The instance features will then be aggregated by $a(\cdot)$ into a slide-level feature vector to be sent into a bag-level classifier $f(\cdot)$ for MIL training. End-to-end training of the feature extractor and bag classifier is prohibitively expensive especially for high resolution WSIs, since they can be very large bags each containing over 50000 instances. Therefore, many methods tends to merely focus on improving $a(\cdot)$ and $f(\cdot)$, leaving $g(\cdot)$ untrained on the WSI dataset (as shown in Fig.~\ref{Difference}(b)). Recently, there are also methods proposing to fine-tune the $g(\cdot)$ with self-supervised \cite{srinidhi2022self,dsmil,hipt} or semi-supervised techniques\cite{liu2022multiple}, but since the two processes are still trained separately with different supervision signals, it will still result in inconsistency and lack of information exchange within the entire MIL processes (as shown in Fig.~\ref{Difference}(c)). 

% By ChatGPT
In contrast, MIL-based methods have become increasingly preferred due to their only demand for slide-level labels \cite{maron1997framework}. The typical pipeline of MIL methods is shown in Fig.~\ref{MIL}, where WSIs are treated as bags, and tiled patches are considered as instances. The aim is to predict whether there are positive instances, such as tumor patches, in a bag, and if so, the bag is considered positive as well. In practice, a fixed ImageNet pre-trained feature extractor $g(\cdot)$ is usually used to convert the tiled patches in a WSI into feature maps due to limited GPU memory. These instance features are then aggregated by $a(\cdot)$ into a slide-level feature vector to be sent to the bag-level classifier $f(\cdot)$ for MIL training.
Due to the high computational cost, end-to-end training of the feature extractor and bag classifier is prohibitive, especially for high-resolution WSIs. As a result, many methods focus solely on improving $a(\cdot)$ or $f(\cdot)$, leaving $g(\cdot)$ untrained on the WSI dataset (as shown in Fig.\ref{Difference}(b)). However, the domain shift between WSI and natural images may lead to sub-optimal representations, so recently there have been methods proposed to fine-tune $g(\cdot)$ using self-supervised techniques \cite{srinidhi2022self,dsmil,hipt} or weakly-supervised techniques \cite{liu2022multiple,wang2020ud,jin2023label} (as shown in Fig.\ref{Difference}(c)). Nevertheless, since these two processes are still trained separately with different supervision signals, they lack joint optimization and may still leads to inconsistency within the entire MIL pipeline.

% Existing MIL methods fine-tune the $g(\cdot)$ by self-supervised pre-training \cite{srinidhi2022self} or only updating the weights with a few high score patches \cite{dsmil}, so bag classifier $f(\cdot)$ is considered as a completely

%================================================================
% Existing patch embedding methods cannot utilize the slide-level labels (But some methods that use attention scores to generate pseudo labels actually make some use of the slide-level label) but ours can (by freezing f(\cdot) and back-propagate pseudo labels back to g(\cdot)).


%================================================================

\begin{figure}[t]
\center
\includegraphics[width=0.97\textwidth]{ICMIL_difference_new.pdf}
\caption{Comparison between ICMIL and existing methods. (a) Ordinary end-to-end classification pipeline. (b) MIL methods that use fixed pre-trained ResNet50 as $g(\cdot)$. (c) MIL methods that introduce extra self-supervised fine-tuning of $g(\cdot)$. (d) Our proposed ICMIL which can bridge the loss back-propagation process from $f(\cdot)$ to $g(\cdot)$ by iteratively coupling them during training. }
% \vspace{-4mm}
\label{Difference}
\end{figure}

% Hence, to address these problems, we propose a noval MIL framework called ICMIL, which can iteratively couple the patch feature embedding process and the bag-level classification process together for better MIL training (as shown in Fig.~\ref{Difference}(d)). Unlike most previous works that mainly focus on designing fancy instance aggregators $a(\cdot)$ and bag classifiers $f(\cdot)$ \cite{abmil,clam,dtfdmil}, we focus on bridging the loss back-propagation process from $f(\cdot)$ to $g(\cdot)$ to help $g(\cdot)$ perceive the slide-level labels. Specifically, after achieving a bag-level classifier with the traditional MIL pipeline, we propose to use this bag-level classifier to initialize a instance-level classifier. Therefore, the bag-level classifier can use the category knowledge learned from the bag-level features to help decide each instance's category. For this part, we further propose a teacher-student alike method for efficiently generating pseudo labels and fine-tuning $g(\cdot)$ at the same time. After fine-tuning, $g(\cdot)$ can generate better patch representations for training bag-level classifier $f(\cdot)$ again, who can then be used for the next round of iteration. 
% By ChatGPT
To address the challenges mentioned above, we propose a novel MIL framework called ICMIL, which can iteratively couple the patch feature embedding process with the bag-level classification process to enhance the effectiveness of MIL training (as illustrated in Fig.~\ref{Difference}(d)). Unlike previous works that mainly focused on designing sophisticated instance aggregators $a(\cdot)$ \cite{dsmil,shao2021transmil,lu2021smile} or bag classifiers $f(\cdot)$ \cite{abmil,clam,dtfdmil}, we aim to bridge the loss back-propagation process from $f(\cdot)$ to $g(\cdot)$ to improve $g(\cdot)$'s ability to perceive slide-level labels. Specifically, we propose to use the bag-level classifier $f(\cdot)$ to initialize an instance-level classifier $f'(\cdot)$, enabling $f(\cdot)$ to use the category knowledge learned from bag-level features to determine each instance's category.
In this regard, we further propose a teacher-student \cite{hinton2015distilling} approach to effectively generate pseudo labels and simultaneously fine-tune $g(\cdot)$. After fine-tuning, the domain shift problem is alleviated in $g(\cdot)$, leading to better patch representations. The new representations can be used to train a better bag-level classifier in return for the next round of iteration.

% However, directly applying the pseudo labels generated by bag-level classifier on patch-level feature extractor fine-tuning only brings limited extra information, and may introduces some false supervision signals. To address this problem, we design a teacher-student alike method for high robustness feature extractor training. The teacher and student branches receive two different augmentations of a patch, and the student model will learn a better and robuster representation based on the soft label generated by the teacher.

% In summary, our contributions are three-folded. (1) We propose an iteratively coupled MIL framework that bridges the loss propagation from bag classifier to patch embedder. It can fine-tune the patch embeddings based on the bag-level classifier, and the new embeddings can in return help training a more accurate bag-level classifier. (2) We propose a teacher-student alike method for effective and robust knowledge learning from bag-level classifier $f(\cdot)$ to instance-level representation embedder $g(\cdot)$. (3) We conduct thorough experiments on two datasets with three different backbones and verify the effectiveness of our framework.
%By ChatGPT
In summary, our contributions are: (1) We propose ICMIL which bridges the loss propagation from the bag classifier to the patch embedder by iteratively coupling them during training. This framework fine-tunes the patch embedder based on the bag-level classifier, and the refined embeddings, in turn, help train a more accurate bag-level classifier. (2) We propose a teacher-student approach to achieve effective and robust knowledge transfer from the bag-level classifier $f(\cdot)$ to the instance-level representation embedder $g(\cdot)$. (3) We conduct extensive experiments on two datasets using three different backbones and demonstrate the effectiveness of our proposed framework.

% 
% Subsequent paragraphs, however, are indented.

% \subsubsection{Sample Heading (Third Level)} Only two levels of
% headings should be numbered. Lower level headings remain unnumbered;
% they are formatted as run-in headings.

% \paragraph{Sample Heading (Fourth Level)}
% The contribution should contain no more than four levels of
% headings. Table~\ref{tab1} gives a summary of all heading levels.

% \begin{table}
% \caption{Table captions should be placed above the
% tables.}\label{tab1}
% \begin{tabular}{|l|l|l|}
% \hline
% Heading level &  Example & Font size and style\\
% \hline
% Title (centered) &  {\Large\bfseries Lecture Notes} & 14 point, bold\\
% 1st-level heading &  {\large\bfseries 1 Introduction} & 12 point, bold\\
% 2nd-level heading & {\bfseries 2.1 Printing Area} & 10 point, bold\\
% 3rd-level heading & {\bfseries Run-in Heading in Bold.} Text follows & 10 point, bold\\
% 4th-level heading & {\itshape Lowest Level Heading.} Text follows & 10 point, italic\\
% \hline
% \end{tabular}
% \end{table}


% \noindent Displayed equations are centered and set on a separate
% line.
% \begin{equation}
% x + y = z
% \end{equation}
% Please try to avoid rasterized images for line-art diagrams and
% schemas. Whenever possible, use vector graphics instead (see
% Fig.~\ref{fig1}).

% \begin{figure}
% \includegraphics[width=\textwidth]{fig1.eps}
% \caption{A figure caption is always placed below the illustration.
% Please note that short captions are centered, while long ones are
% justified by the macro package automatically.} \label{fig1}
% \end{figure}

% \begin{theorem}
% This is a sample theorem. The run-in heading is set in bold, while
% the following text appears in italics. Definitions, lemmas,
% propositions, and corollaries are styled the same way.
% \end{theorem}
% %
% % the environments 'definition', 'lemma', 'proposition', 'corollary',
% % 'remark', and 'example' are defined in the LLNCS documentclass as well.
% %
% \begin{proof}
% Proofs, examples, and remarks have the initial word in italics,
% while the following text appears in normal font.
% \end{proof}
% For citations of references, we prefer the use of square brackets
% and consecutive numbers. Citations using labels or the author/year
% convention are also acceptable. The following bibliography provides
% a sample reference list with entries for journal
% articles~\cite{ref_article1}, an LNCS chapter~\cite{ref_lncs1}, a
% book~\cite{ref_book1}, proceedings without editors~\cite{ref_proc1},
% and a homepage~\cite{ref_url1}. Multiple citations are grouped
% \cite{ref_article1,ref_lncs1,ref_book1},
% \cite{ref_article1,ref_book1,ref_proc1,ref_url1}.

% \vspace{-1mm}
\section{Methodology}
% ICMIL is a MIL framework that aims to improve the patch feature embedding performance with the help of bag-level classifier. Despite the novel iterative optimization method in this framework, the main pipeline of ICMIL remains the same as ordinary MIL methods. This characteristic makes it able to bring about extra performance gain without modifying the existing model structures.
% \vspace{-1mm}
\subsection{Iterative Coupling of Embedder and Bag Classifier in ICMIL}
% \vspace{-1mm}
The general idea of ICMIL is shown in Fig.~\ref{general_idea}, which is inspired by the Expectation-Maximization (EM) algorithm. EM has been used with MIL in some previous works \cite{luo2020weakly,liu2022multiple,wang2017instance}, but it was only treated as an assisting tool for aiding the training of either $g(\cdot)$ or $f(\cdot)$ in the traditional MIL pipelines. In contrast, we are the first to consider the optimization of the entire MIL pipeline as an EM alike problem, utilizing EM for coupling $g(\cdot)$ and $f(\cdot)$ together iteratively.
% Unlike previous MIL works that conduct $g(\cdot)$ and $f(\cdot)$ separately, we manage to let them cooperate with each other in ICMIL. 
% For a given dataset, we first train a bag-level classifier in the traditional way with the patch embeddings generated by a fixed ResNet50 pre-trained on ImageNet (step \textcircled{1} in Fig.~\ref{general_idea}(b)). Then, we can use this bag-level classifier to generate pseudo labels for each instance-level representation. Such operation is possible since the bag-level representations are essentially linear combinations of the instance-level representations, so they exist in the same hidden space; additionally, as shown in Fig.~\ref{general_idea}(a), the instance-level representations by max pooling on the bags proves that the instance- and bag-level decision boundaries can be very similar, proving the feasibility of using $f(\cdot)$ for guiding each instance's pseudo label. Next, we freeze the weights of $f(\cdot)$ and fine-tune $g(\cdot)$ with these pseudo labels (step \textcircled{2} in Fig.~\ref{general_idea}(b)). To realize a robuster fine-tuning, we design a teacher-student alike method to introduce extra information to the supervisions and realize robuster knowledge distillation from $f(\cdot)$ to $g(\cdot)$. After that, $g(\cdot)$ has been fine-tuned for the specific WSI dataset, and therefore can generate better representations for each instance, which can in return help in boosting the performance of $f(\cdot)$. Furthermore, after having a better bag-level classifier $f(\cdot)$ with these newly-generated instance representations, we can use it again for the next round of iterative coupling, and so on.
% ChatGPT
To begin with, we first employ a traditional approach to train a bag-level classifier $f(\cdot)$ on a given dataset, with patch embeddings generated by a fixed ResNet50 \cite{resnet} pre-trained on ImageNet \cite{imagenet} (step \textcircled{1} in Fig.\ref{general_idea}). Subsequently, this $f(\cdot)$ is considered as the initialization of a hidden instance classifer $f'(\cdot)$, generating pseudo-labels for each instance-level representation. This operation is feasible when the bag-level representations aggregated by $a(\cdot)$ are in the same hidden space as the instance representations, and most aggregation methods (e.g., max pooling, attention-based) satisfy this condition since they essentially make linear combinations of instance-level representations. 
% Additionally, the instance- and bag-level decision boundaries are very similar after $a(\cdot)$, making it possible for using $f(\cdot)$ to guide each instance's pseudo label.

\begin{figure}[t]
\center
\includegraphics[width=0.83\textwidth]{ICMIL_brief_idea.pdf}
\caption{The core idea of ICMIL: iteratively, \textcircled{1} fix the embedder $g(\cdot)$ and train the bag classifier $f(\cdot)$, \textcircled{2} fix the classifier $f(\cdot)$ and fine-tune the instance embedder $g(\cdot)$.}
% \vspace{-4mm}
\label{general_idea}
\end{figure}

Next, we freeze the weights of $f(\cdot)$ and fine-tune $g(\cdot)$ with the generated pseudo-labels (step \textcircled{2} in Fig.~\ref{general_idea}), of which the detailed implementation is presented in Section~\ref{tsfinetuning}.
% To ensure robust fine-tuning, we propose a teacher-student \cite{hinton2015distilling} approach to provide additional information to the supervisions and realize effective knowledge distillation from $f(\cdot)$ to $g(\cdot)$.
After this, $g(\cdot)$ is fine-tuned for the specific WSI dataset, which allows it to generate improved representations for each instance, thereby enhancing the performance of $f(\cdot)$. Moreover, with a better $f(\cdot)$, we can use the iterative coupling technique again, resulting in further performance gains and mitigation to the distribution inconsistencies between instance- and bag-level embeddings.


% In ICMIL, a bag-level classifier can only be used for instance-level classification if the initial instance representations from the ImageNet pre-trained ResNet50 can already project the positive instances and negative instances to different areas in the hidden space tentatively. Luckily, performance of the ImageNet pre-trained ResNet50 has been proved in many previous max pooling based MIL methods. These methods choose to use one stand-out instance to represent the entire bag, and achieved acceptable results. This indicates that learning a bag classifier can also equal to learning a instance classifier, which is just the fundamental assumption of ICMIL. 
% \vspace{-1mm}
\subsection{Instance Aggregation Method in ICMIL}
% \vspace{-1mm}
% Most instance aggregators are compatible with ICMIL, as long as they project the bag representations to the same hidden space as the instance representations'. However, a simpler one is usually more preferred since it may leads to smaller difference between the decision boundaries of bag-level classifer $f(\cdot)$ and instance-level classifier $f'(\cdot)$, which can accelerate the convergence of ICMIL. 
Although most instance aggregators are compatible with ICMIL, they still have an impact on the efficiency and effectiveness of ICMIL. In addition to that $a(\cdot)$ has to project the bag representations to the same hidden space as the instance representations, it also should avoid being over-complicated. Otherwise, $a(\cdot)$ may lead to larger difference between the decision boundaries of bag-level classifer $f(\cdot)$ and instance-level classifier $f'(\cdot)$, which may cause ICMIL taking more time to converge.
% Luckily, most of the existing $a(\cdot)$ are succinct enough and would not bring about such effects.
% Different from $g(\cdot)$ and $f(\cdot)$ only accepting one single instance- or slide-level representation at a time, instance aggregation process accepts several instance representations at a time, and output a slide-level representation. This difference makes $a(\cdot)$ not suitable for iterative optimization with other MIL processes. Therefore, in ICMIL, $a(\cdot)$ is optimized each time along with $f(\cdot)$, but is not involved in the generation of instance-level pseudo labels. 

% The universal goal of most existing aggregating methods is to emphasis the positive instances and weakening the negative instances, and from another angle they can be viewed as converting a bag-level classification back into instance-level classification. Hence, it still all comes down to a good instance representation if we are trying to have a better bag-level classifier, and that is exactly what ICMIL is designed for.
Therefore, in our experiments, we choose to use the attention-based instance aggregation method \cite{abmil} which has been widely used in many of the existing MIL frameworks \cite{abmil,clam,dtfdmil}. For a bag that contains $K$ instances, attention-based aggregation method firstly learns an attention score for each instance. Then, the aggregated bag-level representation $H$ is defined as:

\begin{equation}
    H=\sum_{k=1}^Ka_kh_k,
\end{equation}
\begin{equation}
    a_k=\frac{{\rm exp}\{\omega^T(tanh(V_1h_k)\odot sigm(V_2h_k))\}}{\sum_{j=1}^K{\rm exp}\{ \omega^T(tanh(V_1h_j)\odot sigm(V_2h_j))\}},
\end{equation}

\noindent where $a_k$ is the attention score for the $k$-th instance $h_k$ in the bag. Obviously, $H$ and $h_k$ remains in the same hidden space, satisfying the prerequisite of ICMIL. 
% Specifically, the attention score $a_k$ for the $k$-th instance $h_k$ is defined as:

% \begin{equation}
%     a_k=\frac{{\rm exp}\{\omega^T(tanh(V_1h_k)\odot sigm(V_2h_k))\}}{\sum_{j=1}^K{\rm exp}\{ \omega^T(tanh(V_1h_j)\odot sigm(V_2h_j))\}},
% \end{equation}

% \noindent where $\omega$, $V_1$ and $V_2$ are learnable matrices, $\odot$ is element-wise wise multiplication, and $tanh()$ and $sigm()$ are different activation functions. 
% Nevertheless, it should be noted that $\omega$ is often used as a dimension reduction operator in practical implementation, and this may result in bag-level representations being projected to a different hidden space from the instance-level features. So, when using $f(\cdot)$ to generate pseudo labels for the instances in ICMIL, $\omega$ should also be applied for each instance to ensure consistency. 
% Then, the bag-level representation $H$ is defined as the weighted summation of the instance representations, which is:

% \begin{equation}
%     H=\sum_{k=1}^Ka_kh_k
% \end{equation}

% In general, our framework can provide an accuracy improvement for most existing methods without modification of their structures. 

\begin{figure}[t]
\center
\includegraphics[width=\textwidth]{ICMIL_teacher_student_OLD.pdf}
\caption{A schematic view of the proposed teacher-student alike model for label propagation from $f(\cdot)$ to $g(\cdot)$ (mainly in step \textcircled{2}), and its position in ICMIL pipeline.} 
% \vspace{-4mm}
\label{knowledge_distillation}
\end{figure}

% \vspace{-1mm}
\subsection{Label Propagation from Bag Classifier to Embedder}
\label{tsfinetuning}
% \vspace{-1mm}
% We propose a teacher-student alike model for more accurate and robust label propagation from $f(\cdot)$ to $g(\cdot)$. The structure of this model is presented in Fig.~\ref{knowledge_distillation}. Compared with the na誰ve method of directly generating all the pseudo labels and retraining $g(\cdot)$ all over again, the proposed method can train a new $g(\cdot)$ along with the pseudo label generation process, which is more flexible. Besides, the augmented inputs can also bring in more extra information to the training process by making better use of the supervision signals, resulting in a robuster $g(\cdot)$. In addition, a learnable $f'(\cdot)$ for student branch is also used to allow flexiblity to the instance-level fine-tuning. 

% Specifically, we freeze the weights of $g(\cdot)$ and $f(\cdot)$ to set them as the teacher branch, and then try to train a student patch embedding network $g'(\cdot)$ to learn the category knowledge from the teacher. For a given patch input, the teacher is responsible for generating the corresponding pseudo label, while the student receives the augmented image and try to generate a similar prediction as the teacher's. During training, the gradient is back-propagated to $g'(\cdot)$ through a learnable $f'(\cdot)$, which is an instance-level classifier whose initial weights are the same of $f(\cdot)$. We use this learnable $f'(\cdot)$ instead of a fixed $f(\cdot)$ mainly because there may be slight difference between the instance- and bag-level classification boundary. However, this difference should be minor. So as to prevent $f'(\cdot)$ being too different from $f(\cdot)$, there is also a weight similarity loss $L_w$ to constrain them. In this way, the patch embeddings from $g'(\cdot)$ can still suits the bag-level classification task, avoiding being too tailored for the instance-level classifier $f'(\cdot)$. In addition, a consistency loss $L_c$ is added to make sure that student branch can generate similar soft labels as the teacher does. 
%ChatGPT
We propose a novel teacher-student model for accurate and robust label propagation from $f(\cdot)$ to $g(\cdot)$. The model's architecture is depicted in Fig.~\ref{knowledge_distillation}. In contrast to the conventional approach of generating all pseudo labels and retraining $g(\cdot)$ from scratch, our proposed method can simultaneously process the pseudo label generation and $g(\cdot)$ fine-tuning tasks, making it more flexible. Moreover, incorporating augmented inputs in the training process allows for the better utilization of supervision signals, resulting in a more robust $g(\cdot)$. We also introduce a learnable $f'(\cdot)$ to self-adaptively modifying the instance-level decision boundary for more effective fine-tuning of the embedder. 

Specifically, we freeze the weights of $g(\cdot)$ and $f(\cdot)$ and set them as the teacher. We then train a student patch embedding network, $g'(\cdot)$, to learn category knowledge from the teacher. For a given patch input $x$, the teacher generates the corresponding pseudo label, while the student receives an augmented image $x'$ and attempts to generate a similar prediction to that of the teacher through a consistency loss $L_c$. This loss function is defined as:

\begin{equation}
    L_c=\sum_{c=1}^{C} \left[f(x)_c log\left( \frac{f(x)_c}{f'(x')_c}\right)\right],
\end{equation}

\noindent where $f(\cdot)$ and $f'(\cdot)$ are teacher classifer and student classifier respectively, $f(\cdot)_c$ indicates the c-th channel of $f(\cdot)$, and $C$ is the total number of channels. 

Additionally, during training, a learnable instance-level classifier is used on the student to back-propagate the gradients to $g'(\cdot)$. The initial weights of $f'(\cdot)$ are the same as those of $f(\cdot)$, as the differences in the instance- and bag-level classification boundaries is expected to be minor. To make $f'(\cdot)$ not so different from $f(\cdot)$ during training, a weight similarity loss, $L_w$, is further imposed to constrain it by drawing closer their each layer's outputs under the same input. By applying $L_w$, the patch embeddings from $g'(\cdot)$ can still suit the bag-level classification task well, rather than being tailored solely for the instance-level classifier $f'(\cdot)$. $L_w$ is defined as:
% Moreover, we introduce a , to ensure that the student branch can generate soft labels similar to those of the teacher.
% The two loss functions can be defined as follows:

\begin{equation}
    L_w=\sum_{l=1}^{L} \sum_{c=1}^{C} \left[f(x)_c^l log\left( \frac{f(x)_c^l}{f'(x)_c^l}\right)\right],
\end{equation}

\noindent where $f(\cdot)_c^l$ indicates the c-th channel of l-th layer's output in $f(\cdot)$. The overall loss function for this step is $L_c+\alpha L_w$, with $\alpha$ set to 0.5 in our experiments.

% \vspace{-1mm}
\section{Experiments}
% \vspace{-1mm}
\subsection{Datasets}
% \vspace{-1mm}
Our experiments utilized two datasets, with the first being the publicly available breast cancer dataset, Camelyon16 \cite{camelyon16}. This dataset consists of a total of 399 WSIs, with 159 normal and 111 metastasis WSIs for the training set, and the remaining 129 for test. Although patch-level labels are officially provided in Camelyon16, they were not used in our experiments. 
% This is because our aim was to design a reliable Multiple Instance Learning (MIL) model that only requires slide-level labels for training. It is worth noting that typically only 10\% of instances in a tumor bag are positive in Camelyon16.

% KF-PRO-400-HI
The second dataset is a private hepatocellular carcinoma (HCC) dataset collected from Sir Run Run Shaw Hospital, Hangzhou, China. This dataset comprises a total of 1140 valid tumor WSIs scanned at 40$\times$ magnification, and the objective is to identify the severity of each case based on the Edmondson-Steiner (ES) grading. The ground truth labels are binary classes of low risk and high risk, which were provided by experienced pathologists. 
% WSIs with ES grading I-II were classified as cases with less severity (i.e., negative), while those with ES grading III-IV were considered as cases with high severity (i.e., positive). 
% Different from Camelyon16, distinguishing the low and high severity of tumor is harder than simply tell tumor patches from normal ones since the positive embeddings and negative embeddings may be more mixed up in the hidden space. Under such circumstances, a powerful instance feature projection with cleaner decision boundary can help boost the bag-level classification greatly.
% \vspace{-1mm}
\subsection{Implementation Details}
% \vspace{-1mm}
For Camelyon16, we tiled the WSIs into 256$\times$256 patches on 20$\times$ magnification using the official code of \cite{dtfdmil}, while for the HCC dataset the patches are 384$\times$384 on 40$\times$ magnification following the pathologists' advice. For both datasets, we used an ImageNet pre-trained ResNet50 \iffalse \footnote{https://download.pytorch.org/models/resnet50-11ad3fa6.pth}\fi to initialize $g(\cdot)$. The instance embedding process was the same of \cite{clam}, which means for each patch, it would be firstly embedded into a 1024-dimension vector, and then be projected to a 512-dimension hidden space for further bag-level training. For the training of bag classifier $f(\cdot)$, we used an initial learning rate of 2e-4 with Adam \cite{adam} optimizer for 200 epochs with batch size being 1. Camelyon16 results are reported on the official test split, while the HCC dataset used a 7:1:2 split for training, validation and test. For the training of patch embedder $g(\cdot)$, we used an initial learning rate of 1e-5 with Adam \cite{adam} optimizer with the batch size being 100. Three metrics were used for evaluation. Namely, area under curve (AUC), F1 score, and slide-level accuracy (Acc). Experiments were all conducted on a Nvidia Tesla M40 (12GB). 


\begin{table}[t]
\renewcommand\tabcolsep{3pt}
\renewcommand\arraystretch{0.93}
\caption{Results of ablation studies on Camelyon16 with AB-MIL.}
\center
\label{ablation_study}
\subtable[Ablation study on the ICMIL iteration times]{
\begin{tabular}{l
>{\columncolor[HTML]{F4F9FF}}c 
>{\columncolor[HTML]{ECF4FF}}c 
>{\columncolor[HTML]{ECF4FF}}c 
>{\columncolor[HTML]{DAE8FC}}c 
>{\columncolor[HTML]{DAE8FC}}c 
>{\columncolor[HTML]{CDE1FF}}c 
>{\columncolor[HTML]{CDE1FF}}c }
\hline
ICMIL Iterations & 0     & 0.5 & 1     & 1.5  & 2     & 2.5  & 3    \\ \hline
AUC              & 85.4 & 88.8    & 90.0 & 89.7 & 90.5 & 90.4 & 90.0 \\
F1               & 78.0 & 79.4    & 80.5 & 80.1 & 82.0 & 80.7 & 81.7 \\
Acc              & 84.5 & 85.0    & 86.6 & 86.0 & 85.8 & 86.9 & 86.6    \\ \hline
\end{tabular}
}
\subtable[Loss Propagation]{
\begin{tabular}{ccc}
\hline
Method & Na誰ve & Ours \\ \hline
AUC    & 88.5     & 90.0      \\
F1     & 78.8     & 80.5      \\
Acc    & 83.9     & 86.6      \\ \hline
\end{tabular}
}
\end{table}

% \subsection{Ablation Study}
% The results of ablation studies are presented in Table~\ref{ablation_study}. Please note that one complete ICMIL fine-tuning includes 1,000,000 randomly picked instances, and only after one complete iteration will the newly trained $f(\cdot)$ be used as guidance for next iteration.

% From Table~\ref{ablation_study}(a), we can learn that as the number of ICMIL iteration increases, the performance will also go up until reaching a stable point. Since the number of instances is very large in WSI datasets, we finally choose to run ICMIL 1 iteration for fine-tuning $g(\cdot)$ one time to achieve a balance between performance gain and time consumption. From Table~\ref{ablation_study}(b), it is shown that our teacher-student based method outperforms the na誰ve "pseudo label generation" method for fine-tuning $g(\cdot)$, which proves the effectiveness of introducing the learnable instance-level classifier $f'(\cdot)$.
% Following results are all reported under this experimental setting.

\begin{table}[t]
\renewcommand\tabcolsep{1.8pt}
\renewcommand\arraystretch{0.94}
\caption{Comparison with other methods on Camelyon16 and HCC datasets, where $^{\dagger}$ indicates the corresponding Camelyon16 results are cited from \cite{dtfdmil}. Best results are in bold, while the second best ones are underlined.}
\center
\label{experimental_results}
\begin{tabular}{cccccccccc}
\hline
\multirow{2}{*}{Method} & \multicolumn{3}{c}{Loss Propagation}                         & \multicolumn{3}{c}{Camelyon16} & \multicolumn{3}{c}{HCC} \\ \cmidrule(l){2-4} \cmidrule(l){5-7}  \cmidrule(l){8-10} 
                        & $g(\cdot)$                      & $f(\cdot)$ & $f$$\rightarrow$$g$                 & AUC(\%)      & F1(\%)       & Acc(\%)      & AUC(\%)    & F1(\%)     & Acc(\%)   \\ \hline
Mean Pooling            &                           & \checkmark     &  & 60.3    & 44.1    & 70.1    & 76.4  & 83.1  & 73.7 \\
Max Pooling             &                           & \checkmark     &  & 79.5    & 70.6    & 80.3    &  80.1      &  84.3      & 76.8      \\
RNN-MIL$^{\dagger}$ \cite{rnnmil}                 &                           &   \checkmark   &  & 87.5    & 79.8    & 84.4    & 79.4       &  84.1      & 75.5      \\
AB-MIL$^{\dagger}$ \cite{abmil}                 &                           &  \checkmark    &  & 85.4    & 78.0    & 84.5    & 81.2  & 86.0  & 78.1 \\
DS-MIL$^{\dagger}$ \cite{dsmil}                 &   \checkmark    &  \checkmark    &  & 89.9    & 81.5    & 85.6    & 86.1       & 86.6       & 81.4      \\
CLAM-SB$^{\dagger}$  \cite{clam}                &                           & \checkmark     &  & 87.1    & 77.5    & 83.7    &  82.1      & 84.3       & 77.1      \\
CLAM-MB$^{\dagger}$ \cite{clam}                &                           & \checkmark      & & 87.8    & 77.4    & 82.3    &  81.7      & 83.7       & 76.3      \\
TransMIL$^{\dagger}$ \cite{shao2021transmil}               &                           & \checkmark     &  & 90.6    & 79.7    & 85.8    & 81.2       & 84.4       & 76.7      \\
DTFD-MIL \cite{dtfdmil}               &                           &   \checkmark   &  & \underline{93.2}    & \underline{84.9}    & \underline{89.0}    & 83.0  & 85.5  & 78.1 \\ \hline
\vspace{-0.5mm}$\rm \mathop{Ours}\limits_{(w/\ Max\ Pooling)}$ & \checkmark &  \checkmark    & \checkmark &   $\mathop{85.2}\limits_{(+5.7)}$      & $\mathop{74.7}\limits_{(+4.1)}$         & $\mathop{81.9}\limits_{(+1.6)}$    & $\mathop{86.6}\limits_{(+6.5)}$  & $\mathop{87.3}\limits_{(+3.0)}$     & $\mathop{82.0}\limits_{(+5.2)}$  \\
\vspace{-0.5mm}$\rm \mathop{Ours}\limits_{(w/\ AB-MIL)}$    & \checkmark & \checkmark     & \checkmark & $\mathop{90.0}\limits_{(+4.6)}$   & $\mathop{80.5}\limits_{(+2.5)}$    & $\mathop{86.6}\limits_{(+2.1)}$  & $\mathop{87.1}\limits_{\underline{(+5.9)}}$        &  $\mathop{88.3}\limits_{\underline{(+2.3)}}$       & $\mathop{83.3}\limits_{\underline{(+5.2)}}$       \\
\vspace{-0.5mm}$\rm \mathop{Ours}\limits_{(w/\ DTFD-MIL)}$   & \checkmark &  \checkmark    & \checkmark 
&  $\boldsymbol{\mathop{93.7}\limits_{(+0.5)}}$      & $\boldsymbol{\mathop{87.0}\limits_{(+2.1)}}$   & $\boldsymbol{\mathop{90.6}\limits_{(+1.6)}}$  & $\boldsymbol{\mathop{87.7}\limits_{(+4.7)}}$     &  $\boldsymbol{\mathop{89.1}\limits_{(+3.6)}}$      &  $\boldsymbol{\mathop{83.5}\limits_{(+5.4)}}$     \\ \hline
\end{tabular}
\end{table}


\begin{figure}[t]
\includegraphics[width=\textwidth]{visualization.pdf}
\caption{Visualization of the instance- and bag-level representations before and after ICMIL training. We sample one instance from one bag w/ Max Pooling. Only one iteration of ICMIL is used to achieve the right figure.} 
% \vspace{-4mm}
\label{visualization}
\end{figure}

% \vspace{-1mm}
\subsection{Experimental Results}
% \vspace{-1mm}
\subsubsection{Ablation Study.}
The results of ablation studies are presented in Table~\ref{ablation_study}. \iffalse Please note that one complete ICMIL fine-tuning includes 1,000,000 randomly picked instances, and only after one complete iteration will the newly trained $f(\cdot)$ be used as guidance for next iteration.\fi From Table~\ref{ablation_study}(a), we can learn that as the number of ICMIL iteration increases, the performance will also go up until reaching a stable point. Since the number of instances is very large in WSI datasets, we empirically recommend to choose to run ICMIL one iteration for fine-tuning $g(\cdot)$ to achieve the balance between performance gain and time consumption. From Table~\ref{ablation_study}(b), it is shown that our teacher-student-based method outperforms the na誰ve ``pseudo label generation" method for fine-tuning $g(\cdot)$, which demontrates the effectiveness of introducing the learnable instance-level classifier $f'(\cdot)$.

% \vspace{-3mm}

\subsubsection{Comparison with Other Methods.}
Experimental results are presented in Table~\ref{experimental_results}. As shown, our ICMIL framework consistently improves the performance of three different MIL baselines (i.e., Max Pool, AB-MIL, and DTFD-MIL), demonstrating the effectiveness of bridging the loss back-propagation from bag calssifier to embedder. It proves that a more suitable patch embedding can greatly enhance the overall MIL classification framework. When used with the state-of-the-art MIL method DTFD-MIL, ICMIL further increases its performance on Camelyon16 by 0.5\% AUC, 2.1\% F1, and 1.6\% Acc.
% This proves that ICMIL successfully enables the information exchange within the entire MIL framework, resulting in a better cooperation between instance embedding and bag classification. Among all the MIL baselines, Max Pooling enjoys the highest improvement from ICMIL, which is a 5.7\% increase on AUC. One reason is that a lower baseline leads to higher potential of improving, while another reason is that Max Pooling based method explicitly model the bag-level classification into an instance-level classification problem, therefore benefiting the most from our motivation of increasing the instance-level representations' quality. 

% Results on the HCC dataset are a little different. On this dataset, Mean Pooling performs better than usual. This is because the WSIs in this dataset usually contain a large area of tumor. Therefore, average pooling on the instances become less harmful, since it would not offset the difference between postive and negative instances too much. In addition, it is shown that the performance difference between different vanilla MIL methods tends to be smaller, as the max difference of AUC is only 9.7\% (DS-MIL to Mean Pooling) compared to Camelyon16's 32.9\% (DTFD-MIL to Mean Pooling). This is because the difference between low severity instances and high severity instances is much smaller than the difference between tumor and non-tumor instances, making the aggregated bag-level instances also difficult to be classified. This is also why DS-MIL can outperform DTFD-MIL on this dataset, for it uses self-supervised pretraining to initialize $g(\cdot)$, leading to better instance representations. In comparison, ICMIL also involves instance-level fine-tuning, and can improve the separability of positive and negative cases with the help of bag classifier. After applying ICMIL on DTFD-MIL, it still outperform other methods, indicating the superiority of its bag classifier design.
% short version by ChatGPT
Results on the HCC dataset also proves the effectiveness of ICMIL, despite the minor difference on the relative performance of baseline methods. Mean Pooling performs better on this dataset due to the large area of tumor in the WSIs (about 60\% patches are tumor patches), which mitigates the impact of average pooling on instances. Also, the performance differences among different vanilla MIL methods tends to be smaller on this dataset since risk grading is a harder task than Camelyon16. In this situation, the quality of instance representations plays a crucial role in generating more separable bag-level representations. 
As a result, after applying ICMIL on the MIL baselines, these methods all gain great performance boost on the HCC dataset.

% \subsection{Visualization Analysis}

% To further verify the effectiveness of our proposed method, we present the instance- and bag-level representations of Camelyon16 before and after ICMIL in Fig.~\ref{visualization} (with AB-MIL backbone). As shown in the figure, after only one iteration of $g(\cdot)$ fine-tuning in ICMIL, the instance-level representations can already distinguish from each other better, which naturally leads to a better aggregated bag-level representation. Moreover, the bag-level representations are also better aligned with the instance representations, indicating that the instance- and bag-level decision boundaries indeed come closer to each other. 
% ChatGPT make this short
Furthermore, Fig.~\ref{visualization} displays the instance-level and bag-level representations of Camelyon16 dataset before and after applying ICMIL on AB-MIL backbone. The results indicate that one iteration of $g(\cdot)$ fine-tuning in ICMIL significantly improves the instance-level representations, leading to a better aggregated bag-level representation naturally. Besides, the bag-level representations are also more closely aligned with the instance representations, proving that ICMIL can reduce the inconsistencies between $g(\cdot)$ and $f(\cdot)$ by coupling them together for training, resulting in a better separability.


% \subsection{Visualization Analysis}
% \vspace{-1.5mm}
\section{Conclusion}
% \vspace{-1.5mm}
In this work, we propose ICMIL, a novel framework that iteratively couples the feature extraction and bag classification stages to improve the accuracy of MIL models. ICMIL leverages the category knowledge in the bag classifier as pseudo supervision for embedder fine-tuning, bridging the loss propagation from classifier to embedder. We also design a two-stream model to efficiently facilitate such knowledge transfer in ICMIL. The fine-tuned patch embedder can provide more accurate instance embeddings, in return benefiting the bag classifier. The experimental results show that our method brings consistent improvement to existing MIL backbones.

% \vspace{-1.5mm}
\subsubsection{Acknowledgements} This work was supported by the National Key Research and Development Project (No. 2022YFC2504605), National Natural Science Foundation of China (No. 62202403) and Hong Kong Innovation and Technology Fund (No. PRP/034/22FX). It was also supported in part by the Grant in Aid for Scientific Research from the Japanese Ministry for Education, Science, Culture and Sports (MEXT) under the Grant No. 20KK0234, 21H03470.
%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{paper322}
%
% \begin{thebibliography}{8}
% \bibitem{ref_article1}
% Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)

% \bibitem{ref_lncs1}
% Author, F., Author, S.: Title of a proceedings paper. In: Editor,
% F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
% Springer, Heidelberg (2016). \doi{10.10007/1234567890}

% \bibitem{ref_book1}
% Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
% Location (1999)

% \bibitem{ref_proc1}
% Author, A.-B.: Contribution title. In: 9th International Proceedings
% on Proceedings, pp. 1--2. Publisher, Location (2010)

% \bibitem{ref_url1}
% LNCS Homepage, \url{http://www.springer.com/lncs}. Last accessed 4
% Oct 2017
% \end{thebibliography}
\end{document}
