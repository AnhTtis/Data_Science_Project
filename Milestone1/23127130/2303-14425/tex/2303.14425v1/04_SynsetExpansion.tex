In this section, we propose an extension method towards adding redundancy to the synsets even though the mined synsets are enough for doing synonym-aware pretraining.
Since there has much redundant information in synonymous expressions, understanding synonymous expression is more difficult than synonyms.
Adding redundant information to the synsets will help models identify the irrelevant information of the expressions, which will improve the robustness of the models.


% We aim to improve both the ability of PLM in understanding the core semantics of two synonymous expressions, and in differentiating the redundant information of two synonymous expressions.
% % We find that one purpose of synonym-awaring is to boost the understanding of the PLMs to different words with the same semantics, and another purpose is to understand some redundant information in expressions.
% Adding redundant information to the synsets will help the models to effectively identify irrelevant information and thus become more robust.
% Therefore, although the mined synsets are already available for doing synonym-aware pretraining, but we still hope to do the further expansion.
% In this section, we propose our synsets expansion method designed from the perspective of increasing redundancy to the mined synsets.


By adding redundancy, we aim to find the expression pattern, which is composed of important and unimportant parts to deliver core semantic or auxiliary semantics.
We propose to use frequency $f$, Point-wise Mutual Information (PMI)~\cite{church-hanks-1990pmi}, and left-right neighborhood characters richness of every word-piece to find its core part.

PMI is a measurement for the association of two given characters, referring to the probability of a combination of characters occurring as a whole word.
Left and right neighborhood characters' richness is also a metric to detect whether the given sequence is a complete word or not, and it is often calculated by the entropy of their left and right neighboring characters.
The higher the entropy of the left and right neighboring characters is, the richer the left and right neighborhood characters' richness is, which means the word-piece is more likely to be used as a complete word.

We propose the following formula to calculate the Probability of Core Semantic (PCS) of every word-pieces of an expression:
\begin{equation}
\small
    PCS(x,y) = f_{xy} * PMI(x,y) * lrEnt(xy).
\end{equation}
$f_{xy}$ denote the importance of word-piece $xy$, which is concatenate by word-piece $x$ and $y$.
$PMI$ and $lrEnt$ denote PMI metric and left-right entropy.
% , they are combined to calculate the probability that there is no redundancy in piece $xy$.

We choose word-pieces with higher $PCS$ scores to do the replacement with each other within the same property just like the example in the middle of Figure~\ref{fig:my_label}.
In this way, we will get expanded synsets.

% and is calculated as follows:
% \begin{equation}
% \small
%     PMI(x,y)=p(x,y)log_2\frac{p(x,y)}{p(x)p(y)}.
% \end{equation}

% The best way to calculate the left and right neighborhood characters' richness is to calculate the entropy of their left and right neighboring characters as follows:
% \begin{equation}
% \small
%     lrEnt(w)=-\sum_{w_n\in W_{Neighbor}}P(w_n|w)log_2P(w_n|w).
% \end{equation}

