% \subsection{Knowledge Graph Synsets Mining}
% Synsets Mining in Knowledge Graph is similar to Knowledge Graph Canonization.
% Knowledge Graphs Canonization is mainly focusing on entity canonization and relations canonization, and the essence of such a process is how to get a good embedding for all entities and relations in Knowledge Graph to do the clustering efficiently.

% GHMS models~\cite{galarraga2014ghms,wu2018FAC} propose to use the source URL information and frequency information of triple information involved to jointly perform semantic partitioning and representation learning, and finally do the clustering to obtain similar representations.
% CESI~\cite{vashishth2018cesi} utilizes side information of triples to further improve the embedding representation.
% And CUVA~\cite{vashishth2018cesi} uses the joint representation of entities and relations and a self-encoder to do the clustering.


% \subsection{Text Data Augmentation}

% Jason Wei et al.\cite{wei2019eda} proposed an EDA textual data augmentation method that uses a very simple four transformation operation that introduces reasonable operations enabling more robust model training in general, but Shayne Longpre et al.\cite{ longpre2020traditionalDAisBad} et al. found that under the current Transfomer\cite{vaswani2017attention} based LM, the classical data augmentation approach of EDA instead led to degraded results of the model in downstream applications.
% Afterwards, the perspective of textual data augmentation shifted to deep models, mainly deep generative models \cite{anaby2020seq2seqDA,yang2020g-daug,yoo2021gpt3mixDA}, using a prompts-based approach \cite{wang2022promptda}, and adversarial network \cite{staliunaite2021ganDA} which are the three main branches.

% \subsection{Synonym Mining}
% Synsets Mining in Knowledge Graph is similar to Knowledge Graph Canonization.
% Knowledge Graphs Canonization is mainly focusing on entity canonization and relations canonization, and the essence of such a process is how to get a good embedding for all entities and relations in Knowledge Graph to do the clustering efficiently.

% GHMS models~\cite{galarraga2014ghms,wu2018FAC} propose to use the source URL information and frequency information of triple information involved to jointly perform semantic partitioning and representation learning, and finally do the clustering to obtain similar representations.
% CESI~\cite{vashishth2018cesi} utilizes side information of triples to further improve the embedding representation.
% And CUVA~\cite{vashishth2018cesi} uses the joint representation of entities and relations and a self-encoder to do the clustering.

% Synsets Mining in Knowledge Graph is similar to Knowledge Graph Canonization.
% Knowledge Graphs Canonization is mainly focusing on entity canonization and relations canonization, and the essence of such a process is how to get a good embedding for all entities and relations in Knowledge Graph to do the clustering efficiently.

% GHMS models~\cite{galarraga2014ghms,wu2018FAC} propose to use the source URL information and frequency information of triple information involved to jointly perform semantic partitioning and representation learning, and finally do the clustering to obtain similar representations.
% CESI~\cite{vashishth2018cesi} utilizes side information of triples to further improve the embedding representation.
% And CUVA~\cite{vashishth2018cesi} uses the joint representation of entities and relations and a self-encoder to do the clustering.

% \subsection{Knowledge Augmented Language Models }
% Jason Wei et al.\cite{wei2019eda} proposed an EDA textual data augmentation method that uses a very simple four transformation operation that introduces reasonable operations enabling more robust model training in general, but Shayne Longpre et al.\cite{ longpre2020traditionalDAisBad} et al. found that under the current Transfomer\cite{vaswani2017attention} based LM, the classical data augmentation approach of EDA instead led to degraded results of the model in downstream applications.
% Afterwards, the perspective of textual data augmentation shifted to deep models, mainly deep generative models \cite{anaby2020seq2seqDA,yang2020g-daug,yoo2021gpt3mixDA}, using a prompts-based approach \cite{wang2022promptda}, and adversarial network \cite{staliunaite2021ganDA} which are the three main branches.


% Jason Wei et al.\cite{wei2019eda} proposed an EDA textual data augmentation method that uses a very simple four transformation operation that introduces reasonable operations enabling more robust model training in general, but Shayne Longpre et al.\cite{ longpre2020traditionalDAisBad} et al. found that under the current Transfomer\cite{vaswani2017attention} based LM, the classical data augmentation approach of EDA instead led to degraded results of the model in downstream applications.
% Afterwards, the perspective of textual data augmentation shifted to deep models, mainly deep generative models \cite{anaby2020seq2seqDA,yang2020g-daug,yoo2021gpt3mixDA}, using a prompts-based approach \cite{wang2022promptda}, and adversarial network \cite{staliunaite2021ganDA} which are the three main branches.


% can leverage the semantic class information detected from set expansion to enhance the synonym set discovery process.
% When giving a corpus and a term list, one can leverage surface string~\cite{wang2019surfcon}, co-occurrence statistics~\cite{baroni2004using}, textual pattern~\cite{yahya2014renoun}, distributional similarity~\cite{wang2015solving}, or their combinations~\cite{qu2017automatic,fei2019hierarchical} to extract synonyms. 
% These methods mostly find synonymous term pairs or a rank list of query entityâ€™s synonym, instead of entity synonym sets. 
% Recent years, studies focus on doing entities set expansion and synonym expansion simultaneously since these two works are considered to be tightly coupled to each other.

% Some studies propose to further cut-off the rank list into a set output~\cite{ren2015synonym} or to build a synonym graph and then apply graph clustering techniques to derive synonym sets~\cite{gonccalo2014eco,ustalov2017watset}. 
% However, they all operate directly on the entire input vocabulary which can be too extensive and noisy. Comparing to them, our approach can leverage the semantic class information detected from set expansion to enhance the synonym set discovery process.

\subsection{Synonym Detection}
Early efforts on synonym detection focus on finding entity synonyms from a semi-supervised situation such as query logs~\cite{ren2015synonym}, web tables~\cite{he2016automatic}, and synonymy dictionaries~\cite{ustalov2017watset}. 
In comparison, our work aims at developing a framework to cluster all chosen vocabulary terms into synsets without any supervised signals.
Modern synonym detection combines entity set expansion task and synonym detection task which are considered tightly coupled to each other~\cite{shen2020synsetexpan}, and the syntactic dependencies are more developed by using deep model to find synonymous meaning~\cite{yang2022synonym}.
There are other works who tries to find synonyms in an Open-KG~\cite{qu2017automatic} especially when there is a need to canonize an Open-KG, but most them utilize side information to compensate for the lack of semantic~\cite{vashishth2018cesi,lin2019canonicalization,shen2022multi}, others detecting synonyms by jointly conducting entity linking~\cite{liu2021joint}.
% However, they all ignore the fact that not all of the terms in Open-KG are suitable to do synonym detection.
However, they ignore that not all the content in KG is suitable for synonym detection, so their methods are too time-extensive and results are noisy. 
Our methods first detect which terms are suitable to do the synonym detection, and utilize three heterogeneous features to extract the semantics of the terms and to cluster them into synsets.

\subsection{Synonym Augmentation}
There are many efforts have been made to use synonyms to enhance the effect of downstream applications.
LIBERT~\cite{lauscher2019specializing} trains Bert from scratch with an auxiliary task that binary classifies whether entity pairs are synonymous pairs.
SAPBERT pre-trains BERT with synsets from Unified Medical Language System, which is a comprehensive collection of biomedical terms.
Some choose to use a synonym to construct sentences by simply using templates and conducting Mask-Word-Predication task~\cite{yuan2022generative}. 
And PICSO uses an adapter to store the synonym knowledge to prevent the undermining of the original PLMs~\cite{li2022embracing}.
We aim to develop specific pre-training tasks for injecting synonym knowledge from explicit synsets into LMs. 
The tasks we developed not only ensures the enhancement of PLM's ability to understand synonyms, but also ensures that no semantic drift will be introduced.
% There are few works point out that PLMs are vulnerable to the synonym substitution based attacks~\cite{chiang2022far}, there are works who use synonym encoding~\cite{wang2021natural}, contrastive learning~\cite{shi2022ascl} or neiborhood ensemble~\cite{zhou2021defense} to do the defense.
% In comparison, our work propose to fine-tune or pre-train the model, which fundamentally enhance the robustness against synonym substitution attacks.


