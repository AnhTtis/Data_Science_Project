Synonymous expressions are words, morphemes, or phrases that mean exactly or nearly the same to each other.
In the field of Natural Language Process~(NLP), synonymous expressions play an important role in various applications.
For example, many downstream tasks have suffered from synonym substitution attack~\cite{chiang2022far}, the attack brings in imperceptible perturbations,
while not changing human predictions, will make a well-trained NLP model behave much worse than usual.
And synonym knowledge facilitates models to capture fine-grained semantic relations in some similarity-oriented tasks in the field of Information Retrieval~(IR)~\cite{li2022embracing}.
For the tasks like entity linking~\cite{hoffart2011robust} and knowledge graph canonization~\cite{galarraga2014canonicalizing}, the core challenge lies in modeling the semantic similarity of entities in complex contexts, where understanding the synonymous relationship between phrases is essential.



\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{introduction.png}
    \caption{An example of how Sem4SALM works for mining synsets in Open-KG and improving model's understanding to synonymous expressions.
    }
    \label{fig:intro}
    \vspace{-5mm}
\end{figure*}
However, most prevailing language models~(LMs) have limited ability to synonymous expressions understanding.
Statistical language models~\cite{cavnar1994ngram} is based on a probabilistic approach where they assign probabilities to each word in a sentence given the context. 
{Without explicit synsets to do the augmentation, statistical language models will take synonymous expressions only as common words with no similar semantics. }
Pretrained Deep Language Models~(PLMs)~\cite{peters2018deep} play an important role in NLP nowadays, however, they also have limitations in synonymous expressions understanding.
This is because most PLMs are mainly trained by mask prediction task~\cite{devlin2018bert}, which primarily concern with the probability of a certain sequence of words, but do not concern with the nuances of the word's semantics. 
As a result, PLMs tend to pick a word that fits the context best but is unable to understand the synonymous relation between words.


Given the limitations of the existing models, we propose to improve them from the following two points:
\begin{enumerate}
    \item \textbf{A large explicit synset is needed.} It is difficult for the language model itself to capture implicit synonymous relations directly from context, let alone enhance the model's understanding of synonymous expressions. \red{It is still necessary to start by using explicit synsets.}
    \item \textbf{Synonym-aware pre-training \red{methods} are needed.}
    \red{Since the prevailing mask-word-predication task has demonstrated its limitation in synonymous expressions understanding.}
    Therefore, novel pre-training methods which enhance PLM's synonymous expressions understanding are needed to be developed.
\end{enumerate}




Using Open Knowledge Graph~(Open-KG)~\cite{lehmann2015dbpedia} to compensate for the \red{limitation of the absence of explicit synsets is a good choice}.
The Open-KG is constructed from the online encyclopedia, which is contributed by a large number of netizens' crowdsourcing.
From the table listed in Figure~\ref{fig:intro}, it is intuitive to find that there \red{must} have many synonymous expressions in Open-KG.
For example, people often use a few different words to describe the gender of others,
but there have 233 different expressions of gender in Wiki-data~\cite{vrandevcic2014wikidata}, which is one of the most famous Open-KG in the world,
and even 534 different expressions of gender in CN-DBpedia~\cite{xu2017cn}, which is the largest Chinese Open-KG.

The synonymous expressions in Open-KG will alleviate \red{the problem of synsets}, but a novel PLM pre-training method that inject the synonym knowledge into the existing models are still needed.
Since the synonymous expressions have the same meaning in many aspects, the model's output of two sentences with only synonymous differences should be roughly similar.
And the distance between representations of two sentences with only synonymous differences should be close but still keep the ambiguity of synonymous expressions in some specific aspects.

So, We propose Sem4SALM to enhance the understanding of the synonymous expression to the PLMs in this paper.
As shown in Figure~\ref{fig:intro}, Sem4SALM is a framework that first mine synonymous expressions in the Open-KG and then do synonym-aware pretraining based on the mined synsets to LMs.
Sem4SALM leverages the frequency information to capture the core semantic of different expressions, then uses both string and distributed representation features of the core semantic to better calculate the similarity between each expression.
And Sem4SALM also consists of two complementary pretraining tasks, which inject synonymous expression knowledge and synonymous context knowledge into models.


In summary, we conclude that this paper makes the following contributions:
\begin{enumerate}
    \item We are the first to propose a combined work between synonymous expressions mining and synonym knowledge injecting, which we think are two \red{naturally consecutive} works.
    \item We propose a novel synonymous expressions mining method that \red{can get synsets in Open-KG}, and we also propose two novel pretraining methods to inject the synonym knowledge into LMs.
    \item Extensive experiments have been carried out to verify the effectiveness of Sem2SALM. And the comparison shows that Sem2SALM is the most effective among all the other baselines in improving the synonymous expressions understanding to models.
\end{enumerate}



