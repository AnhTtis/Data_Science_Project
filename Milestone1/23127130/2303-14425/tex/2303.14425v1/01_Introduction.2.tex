Synonymous expressions are words, morphemes, or phrases that mean exactly or nearly the same to each other.
In the field of Natural Language Process~(NLP), synonymous expressions play an important role in various applications.
% Synonymous expressions play an important role in various applications, In the field of Natural Language Process~(NLP)
For example, many downstream tasks have suffered from synonym substitution attack~\cite{chiang2022far}, the attack brings in imperceptible perturbations,
while not changing human predictions, will make a well-trained NLP model behave much worse than usual.
And synonym knowledge facilitates models to capture fine-grained semantic relations in some similarity-oriented tasks in the field of Information Retrieval~(IR)~\cite{li2022embracing}.
For the tasks like entity linking~\cite{hoffart2011robust} and knowledge graph canonization~\cite{galarraga2014canonicalizing}, the core challenge lies in modeling the semantic similarity of entities in complex contexts, where understanding the synonymous relationship between phrases is essential.



\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{introduction.png}
    \caption{An example of how SemSAP works for mining synsets in Open-KG and improving model's understanding to synonymous expressions.
    }
    \label{fig:intro}
    \vspace{-5mm}
\end{figure*}
However, most prevailing language models~(LMs) have limited ability to synonymous expressions understanding.
Statistical language models~\cite{cavnar1994ngram} is based on a probabilistic approach where they assign probabilities to each word in a sentence given the context. 
{Without explicit synsets to do the augmentation, statistical language models will take synonymous expressions only as common words with no similar semantics. }
Pretrained Deep Language Models~(PLMs)~\cite{peters2018deep} play an important role in NLP nowadays, however, they also have limitations in synonymous expressions understanding.
This is because most PLMs are mainly trained by mask prediction task~\cite{devlin2018bert}, which primarily concern with the probability of a certain sequence of words, but do not concern with the nuances of the word's semantics. 
As a result, PLMs tend to pick a word that fits the context best but is unable to understand the synonymous relation between words.


Based on the above limitations of the existing models, we propose to improve them from the following two points:
\begin{enumerate}
    \item \textbf{A large-scale explicit synset is needed.} It is difficult for the language model itself to capture implicit synonymous relations directly from context, let alone enhance the model's understanding of synonymous expressions.
    Starting from using explicit synsets is a more feasible way at present.
    % \red{It is still necessary to start by using explicit synsets.}
    \item \textbf{Synonym-aware pretraining method is needed.}
    The prevailing pretraining task prevent models from awaring the phenomenon of synonymous expressions.
    Therefore, novel pretraining methods which directly inject synonym knowledge into LMs are needed to be developed.
\end{enumerate}


Although there has some explicit synsets, such as Wordnet~\cite{miller1995wordnet} or BigCilin~\cite{liu2022bigcilin}, it is diffcult to directly apply them to facilitate downstream tasks for their homogeneous expression and limited number of synonymous relation.
We find it is feasible to use Open Knowledge Graph~(Open-KG)~\cite{lehmann2015dbpedia} to improve the diversity of the expressions and to enlarge the number of synonymous relation of existing explicit synsets. 
The Open-KG is constructed from the online encyclopedia, which is contributed by a large number of netizens' crowdsourcing.
From the table listed in Figure~\ref{fig:intro}, it is intuitive to find that there must have many diverse synonymous expressions in Open-KG.
For example, people often use a few different words to describe the gender of others,
but there have 233 different expressions of gender in Wiki-data~\cite{vrandevcic2014wikidata}, which is one of the most famous Open-KG in the world,
and even 534 different expressions of gender in CN-DBpedia~\cite{xu2017cn}, which is the largest Chinese Open-KG.

The existing pretraining methods are also insufficient to allow language models to understand synonyms, so a novel pretraining method that inject the synonym knowledge into the existing models is needed.
Since the synonymous expressions have the same meaning in many aspects, the model's output of two sentences with only synonymous differences should be similar.
And the distance between the representations of two sentences with only synonymous differences should be close but still keep the ambiguity of synonymous expressions in some specific aspects.




In this paper, we propose \textbf{Sem4SAP}, a framework of \textbf{S}ynonymous \textbf{E}xpressions \textbf{M}ining in Open-KG \textbf{for} \textbf{S}ynonym-\textbf{A}ware \textbf{P}retraining as shown in Figure~\ref{fig:intro}.
% As shown in Figure~\ref{fig:intro}, SemSAP is a framework that first mine synonymous expressions in the Open-KG and then conduct synonym-aware pretraining based on the mined synsets to LMs.
SemSAP leverages the frequency information of each part of the expressions to detect the core semantic, then uses both textual and distributed representational features of the core semantic to better calculate the similarity between each expressions.
SemSAP also consists of two complementary pretraining tasks, which inject synonymous expression knowledge and synonymous context knowledge into models.

As the result of our synonymous expression mining method, we propose \textbf{LS3}, a database of \textbf{L}arge-\textbf{S}cale Diver\textbf{S}e \textbf{S}ynsets.
To the best of our knowledge, LS3 is at least one magnitude larger than previous synonym databases.
It contains two millions synonymous expressions and ten millions synonymous relation mined from Wiki-Data and CN-DBpedia, two largest Open-KG in the world.
It is also kept updated to the latest version of these two Open-KGs every month.

In summary, we conclude that this paper makes the following contributions:
\begin{enumerate}
    % \item We propose that synsets mining is Sem4SAP is a, which combine synsets in Open-KG and do synonym-awaresynonymous expressions mining method towards Open-KG, and we also propose a novel pretraining method to inject the synonym knowledge into LMs.
    \item We propose a framework called Sem4SAP, which consist of a novel synonymous expression mining method and a novel synonym-aware pretraining method.
    \item We construct a new large-scale dataset of synsets called LS3 to support fair comparison across different baselines and facilitate future research related to synonymous expressions understanding.
    % To the best of our knowledge, DAS-Synset is the first million-scale synsets covering two different languages.
    % It is also the first synsets collection covers a great number of diverse synonymous expressions, and keep updated to the latest version of Wiki-data and CN-DBpedia.
    \item Extensive experiments have been carried out to verify the effectiveness of Sem2SALM. And the comparison shows that Sem2SALM is the most effective methods among all the other baselines in improving the synonymous expressions understanding of models.
\end{enumerate}



