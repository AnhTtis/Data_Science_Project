
In order to make better use of the mined synsets, we design a pretraining method for synonym-aware LMs.
The pretraining method consists of two pretraining tasks to inject synonym knowledge into models, and two extra tricks to prevent overfitting and semantic drifting of the models.

% \input{000_fig_ablation.tex}
\textbf{Token-level Synonym Boosting:} 
We first design a token-level synonym boosting task for injecting vocabulary synonym knowledge into LMs.
This task allows the LM to consider two synonymous expressions as having the same semantics.
So the objective of this task is to pull in the output of two expressions if they are synonymous to each other, otherwise, remain unchanged.

\textbf{Sentence-level Synonym Boosting:}
Then we design a sentence-level synonym boosting task for injecting contextual synonym knowledge into LMs.
This task allows the LM to consider two sentences as having the same semantics if there is only a synonym substitution difference between them.
And the objective of this task is just like the previous that pull in the output of two sentences if they are synonymous.

\textbf{Two extra tricks:}
We design two extra tricks to prevent overfitting and semantic drifting, especially for continual pretraining based on other PLMs.

The first trick is to fix the output of the most common expressions and pull in the outputs of the others in the token-level synonym pretraining task.
Since the most common expressions are trained most often, their representations are more expressive than the other expressions.
In this way, the semantics of these most expressive expressions are delivered to the other synonymous expressions.

The second trick is to stop pulling in when the distance between the outputs of synonymous expressions is much shorter than the initial distance in both synonym boosting pretraining tasks.
It ensures that the model's comprehension of similar texts will not become identical, which remains the ambiguity of different expressions in some aspects.





