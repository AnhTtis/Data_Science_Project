In this section, we evaluate the effectiveness of Sem4SAP.
The setup of our experiments is detailed in Sec.~\ref{061}.
The overall experiments of Sem4SAP and other baselines are detailed in Sec.~\ref{062}.
The ablation study is shown in Sec.~\ref{063}.
And the case study is in Sec.~\ref{064}.

\begin{table}
    \small
    \centering
    \resizebox{0.8\columnwidth}{!}{
    \begin{tabular}{c|c|c|c}
        \hline
        \textbf{Word}& \textbf{ENG Synset} & \textbf{CHN Synset} & \textbf{WordNet 3.0}\\
        \hline
        Female & 11 & 16 & 3 \\
        U.S. & 7 & 8 & 11 \\
        Ph.D. & 10 & 12 & 2 \\
        ♂ & 14 & 22 & 0 \\
        \hline
        \emph{\#Words} & 34,329 & 65,010 & 147,306 \\
        \emph{\#Expended} & 272,745 & 2,719,864 & - \\
        \hline
    \end{tabular}
    }
    \vspace{-2mm}
    \caption{Results and examples of synsets mined by Sem4SAP. Second row and third row denote the number of synonymous expressions to the word in first row in our mined synsets, and fourth row denote the number in Wordnet.}
    \vspace{-5mm}
    \label{tab:synsets}
\end{table}

\subsection{Experiments Setup}
\label{061}

\subsubsection{Datasets}
We conduct synonymous expressions mining in English Open-KG Dbpedia~\cite{lehmann2015dbpedia} and Chinese Open-KG CN-DBpedia~\cite{xu2017cn}, the result and some examples of our mined synsets have listed in Table~\ref{tab:synsets}.
Since these two Open-KGs are composed of content in the general area, so we use General Language Understanding Evaluation benchmark GLUE~\cite{wang2018glue}, and Chinese general Language Understanding Evaluation benchmark CLUE~\cite{xu2020clue} to verify the effectiveness of Sem4SAP.
These benchmarks consist of many datasets, and we choose ten of them that we think best suit our evaluation, the detailed description, and statistics will be listed in the Appendix.
Besides, since we aim at improving the model's understanding of synonymous expressions, so we add back translated data to the test set of these datasets.
Specifically, we use Youdao Translation API
% \footnote{https://fanyi.youdao.com/} 
to generate new data by translating the data in the test set into English or Chinese and back-translating them, and adding new data to the test set.
Back translation will bring in many synonymous expressions, which is able to verify whether the models will be affected by synonymous substitution attacks.






\subsubsection{Baselines}
We choose several works that can also enhance PLM's understanding of synonymous expressions as baselines.
We also choose Bert and Roberta as the backbone PLMs.
The abbreviation and description of these works are as follows:

\textbf{Bert}~\cite{devlin2018bert} is one of the most famous PLMs which shows its great ability in natural language understanding.
\textbf{Roberta}~\cite{liu2019roberta} has made progress in the pretraining technique which performs better than Bert in natural language understanding.
\textbf{-BT} denotes the performance of the original dataset.
\textbf{EDA}~\cite{wei2019eda} is one of the most famous text data augmentation methods in using four text transformations, which are synonym substitution, random insertion, random swapping, and random deletion, to enhance the performance of the model. 
\textbf{LIBERT}~\cite{lauscher2019specializing} trains Bert from scratch with an auxiliary task that binary classifies whether entity pairs are synonymous pairs.
\textbf{PICSO}~\cite{li2022embracing} use an adapter to store the synonymous knowledge without undermining the raw knowledge in PLM.
And \textbf{Sem4SAP} is our proposed method.


\subsubsection{Parameter Setting}
% All experiments are implemented by Python 3.9 on a 12-Core Xeon(R) Gold 5218R 2.10GHz CPU, 128GB RAM, and NVIDIA GeForce RTX 3090 machine. 
% We use CUDA with version number 11.6 and we run our deep models on the GPU.

In synonymous expressions mining phrase, we choose 5000 properties in Chinese and English Open-KGs respectively with the lowest $PCP$ score, and clear 40\% edges with the lowest weight during the clustering.
In Synonym-Aware Pretraining, we take the expressions that match exactly the value objects of the highest frequency in Open-KG as the most common expressions.
We also choose to stop pulling in the output of the synonymous expressions or sentences when their distance has already been pulled in at least 60 \% shorter than the initial distance.

% In the PLM synonym-aware pretraining and fine-tuning process, we use AdamW
% % \footnote{https://pytorch.org/docs/stable/generated/torch.optim.\\AdamW.html} 
% optimizer with initial learning rate 1e-5, batch size at 16 and weight decay at 0.01.
% We also use ReduceLROnPlateau
% % \footnote{https://pytorch.org/docs/stable/generated/torch.optim.\\lr\_scheduler.ReduceLROnPlateau.html} 
% scheduler with ten patience epochs. 


\subsubsection{Fairness of the Experiments}
To keep the fairness of the experiments, we first acquire the text from the training data in all datasets listed in Table~\ref{tab:overallExperiemnt}.
Then, we conduct the synonym substitution to the acquired text by replacing the words or phrases in the text which match exactly to the expressions in mined synsets.
Finally, we have at least pretrain all baselines in the way how backbone PLMs are trained.
So, the pretraining corpus for all backbone PLMs is the same, the only difference is the pretraining method.


\subsection{Overall Experiments}
\label{062}
We compare baselines on both GLUE and CLUE dataset, and the experimental results are listed in Table~\ref{tab:overallExperiemnt}.
The \emph{yellow} cell denotes the result of the original dataset without doing back translation, and most of these results are cited from the paper of Robert, and Bert and from the leaderboard of GLUE and CLUE.
The \emph{green} cell denotes the best result, and the \emph{lime} cell denotes the second-best result.
The $\uparrow$ denotes whether the performance is higher than the raw PLM's.
The \underline{\underline{double underline}} denotes whether the performance on the back translation dataset is even higher than the performance on the original dataset.

\textbf{Robustness of PLMs:} From the result, we can see that the performances of the PLMs on original datasets are mostly better than the performances on the dataset added back to translation data, which represents that the existing PLMs are all suffering from substitution attacks.

\textbf{Performance of Baselines:} The EDA and LIBERT mostly lower the performance of the raw PLM, this is because they introduce synonym knowledge improperly, which impairs the existing knowledge in PLMs.
PICSO perform well on QQP, TNEWS, and CSL datasets, which correspond to the results in their paper.

\textbf{Performance of Sem4SAP: }Sem4SAP out-perform most other baselines in most tasks.
Sem4SAP is also a stable method that achieves better performance in all the tasks than raw PLM except for some Natural Language Inference tasks (MNLI-mm, CMNLI).

\textbf{Performance of Large-scale PLM: }For models with larger parameters (Roberta-large), most baselines only inject harmful information to the model while ours further improve the performance.
Moreover, for large models, Sem4SAP still makes the performance better than some of the original datasets, which means using Sem4SAP to inject synonym knowledge is also an effective way of text data augmentation to the large-scale PLMs.





% \input{000 fig_ablation.tex}
\subsection{Ablation Study}
\label{063}
The result of ablation studies is presented in Figure~\ref{fig:ablation}.
The left figure shows the performances on different downstream applications by removing seven components in Sem4SAP respectively.
The middle and right figures show the quality of mined synsets in Chinese and English language by removing four components of our synonym mining method one by one, the results are annotated by three graduate students.
The label in the Figure~\ref{fig:ablation} are listed as follows:
``\textbf{PS: random}'' denote we randomly choose the properties in Property Selection phrase.
``\textbf{SC: w/o f}'' denote we don't use the frequency information in Synset Clustering phrase.
``\textbf{SC: w/o ts}'' denote we remove the textual similarity in Synset Clustering phrase.
``\textbf{SC: w/o drs}'' denote we remove the distributed representational similarity in Synset Clustering phrase.
``\textbf{SE: w/o SE}'' denote we don't do the Synset Expansion.
``\textbf{SAP: w/o TSB}'' denote we remove the Token-level Synonym Boosting method in Synonym-Aware Pretraining phrase.
``\textbf{SAP: w/o SSB}'' denote we remove the Sentence-level Synonym Boosting method in Synonym-Aware Pretraining phrase.

% \red{
We use Bert-base model in all three figures.
In the left figure, we use accuracy metric to evaluate the result of ablation study.
In the middle and right figure, we use five additional metrics to present the result of the clustering:
% We mainly calcuate the accuracy of every task in left figure, and use Bert as the backbone PLM in three figures.
% And to better illustrate the clustering results, there five additional indicators we have introduced in the middle and right figures.
% }
``\textbf{N\_S}'' represents the number of Synsets mined by Sem4SAP.
``\textbf{N\_sv}'' represents the number of different value objects mined by Sem4SAP.
``\textbf{N\_esv}'' represents the number of synonymous expressions obtained by Sem4SAP after doing synset expansion.
``\textbf{RI}'' indicate the Rand Index~\cite{rand1971objective}, which is a metric to denote the clustering purification。
``\textbf{w f}'' or ``\textbf{w/o f}'' denote calculating RI with frequency of the value objects or not.


From Figure~\ref{fig:ablation}, we can find that the method of Property Selection and using frequency to do the clustering are the two most important designs of our synonymous expression mining method.
Few synonyms and many synsets are clustered by randomly selecting properties, which means only a few synonyms in each synset, and the clustering purity is really low, resulting in the smallest covering area in the left figure.
When frequency information is not used, the mined synsets will contain a large number of synonyms, but the clustering purity is very low, resulting in the second smallest covering in the left figure.
And Sem4SAP will be better than bert-base when we use at least the above two methods.
Moreover, we found that removing the token-level or sentence-level synonym boosting method will reduce the performance of the model a lot, so we conclude that these two methods are complementary. 
The token-level synonym boosting method enhances the ability of the model to understand specific synonyms, while the sentence-level synonym boosting method enhances the ability of the model to understand synonyms in context.

\subsection{Case Study}
\label{064}
Here we give some cases of Sem4SAP in Table~\ref{tab:case}.
The first table is the cases of our mined synsets.
Although there have some symbols in Open-KG, ``♂'' for example, Sem4SAP still understand its meaning by mining the co-occurrence of other words, like ``男性'' (male) $\rightarrow$ ``男♂'' (male ♂) $\rightarrow$ ``♂的'' (gender of ♂).
Sem4SAP cluster ``美国 (U.S.A)'' and ``鹰国 (Country of Eagle)'' in the same synset, which shows Sem4SAP has a good understanding of slangs.
We think this is because ``美国 (U.S.A)'' and ``老鹰 (Eagle)'' may have some co-occurrence in pre-training data of PLMs, and Sem4SAP can well integrate the existing knowledge in PLMs and also capture the important parts of the words to better understanding them.

The second table gives some cases of how performance changes by using Sem4SAP.
But it still suffers from synonym substitution attacks.
When the Chinese phrase ``找私房钱'' (look for own money) changes into ``找钱'' (look for money, give change), the model will not identify it as a sentence of amusement anymore.
If the English phrase ``end raunchy'' changes into ``end crass'', the model may identify that ``crass'' is a much more negative word than ``raunchy'', then identify the sentence as ``Negative Emotion''.
And Sem4SAP can make models understand that these are synonyms and that they should have similar semantics under the given context, which will improve the performance in downstream tasks.
\input{000_tab_case.tex}
