Synonymous expressions are words, morphemes, or phrases that mean exactly or nearly the same to each other~\cite{stanojevic2009cognitive}.
In the field of Natural Language Process~(NLP), synonymous expressions play an important role in various applications.
For example, many downstream tasks have suffered from synonym substitution attack~\cite{chiang2022far}, the attack brings in imperceptible perturbations,
while not changing human predictions, will make a well-trained NLP model behave much worse than usual.
And synonym knowledge facilitates models to capture fine-grained semantic relations in some similarity-oriented tasks in the field of Information Retrieval~(IR)~\cite{li2022embracing}.
For the tasks like entity linking~\cite{hoffart2011robust} and knowledge graph canonization~\cite{galarraga2014canonicalizing}, the core challenge lies in modeling the semantic similarity of entities in complex contexts, where understanding the synonymous relationship between phrases is essential.



\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.8\linewidth]{introduction.png}
    \vspace{-2mm}
    \caption{An example of how Sem4SAP works for synonymous expression mining in Open-KG and improving model's understanding of synonymous expressions.
    }
    \label{fig:intro}
    \vspace{-5mm}
\end{figure*}
However, most prevailing language models~(LMs) have limited ability to understand synonymous expressions.
Statistical language models~\cite{cavnar1994ngram} is based on a probabilistic approach where they assign probabilities to each word in a sentence given the context. 
{Without directly using synonymous expressions to do the augmentation, statistical language models will take synonymous expressions only as common words with no similar semantics. }
Pretrained Deep Language Models~(PLMs)~\cite{peters2018deep} play an important role in NLP nowadays, however, they are mainly trained by mask prediction task~\cite{devlin2018bert}, which primarily concern with the probability of a certain sequence of words, but do not concern with the nuances of the word's semantics. 
As a result, PLMs, even large-scale PLMs, tend only to pick a word that fits the context best but not understand the synonymous relation between words.


Based on the above limitations of the existing models, we propose to improve machine's understanding to synonymous expressions from the following two aspects:
\begin{enumerate}
    \item \textbf{Construct large-scale explicit synsets.} It is difficult for the models themself to capture implicit synonymous relations directly from context, let alone enhance the model's understanding of synonymous expressions.
    Starting from explicit synset, which is the set of synonymous expressions, is more feasible at present.
    \item \textbf{Develop synonym-aware pretraining tasks.}
    The prevailing pretraining tasks prevent PLMs from being aware of the synonymous expressions.
    Therefore, novel pretraining tasks which directly inject synonym knowledge into LMs need to be developed.
\end{enumerate}


Although there have some explicit synsets, such as Wordnet~\cite{miller1995wordnet} or BigCilin~\cite{liu2022bigcilin}, it is difficult to directly apply them to facilitate downstream tasks for their homogeneous expression and the limited number of synonymous relation.
We find it feasible to use Open Knowledge Graph~(Open-KG)~\cite{lehmann2015dbpedia} to improve the diversity of the expressions and to enlarge the number of synonymous relations of existing explicit synsets. 
The Open-KG is constructed from the online encyclopedia, which is contributed by a large number of netizens' crowdsourcing.
From the table listed in Figure~\ref{fig:intro}, it is intuitive to find that there are many diverse synonymous expressions in Open-KG.
For example, gender is generally be divided into two categories,
but there have 233 different expressions of gender in Wiki-data~\cite{vrandevcic2014wikidata}, which is one of the most famous Open-KG in the world,
and even 534 different expressions of gender in CN-DBpedia~\cite{xu2017cn}, which is the largest Chinese Open-KG.


% The existing pretraining methods are also insufficient to allow language models to understand synonyms, so a novel pretraining method that injects the synonym knowledge into the existing models is needed.
The limitation of the existing pretraining tasks lies in lacking of the objectives for learning to represent the relation among synonymous expressions. 
The challenge is that although synonymous expressions have the same semantics most of time, they also have some specific meaning in some sepcific aspects.
So, the representations of two sentences with only synonymous differences should be similar, but the ambiguity should also be kept.
% Since the synonymous expressions have the same meaning in many aspects, the distance between the representations of two sentences with only synonymous differences should be close but still keep the ambiguity of synonymous expressions in some specific aspects.
% Since the synonymous expressions have the same meaning in many aspects, the model's output of two sentences with only synonymous differences should be similar.

In this paper, we propose \textbf{Sem4SAP}.
As shown in Figure~\ref{fig:intro}, Sem4SAP is a framework of \textbf{S}ynonymous \textbf{E}xpressions \textbf{M}ining in Open-KG \textbf{for} \textbf{S}ynonym-\textbf{A}ware \textbf{P}retraining.
In Synonymous Expression Mining, Sem4SAP leverages the frequency information in Open-KG to detect the core semantic of a expression and uses both textual and distributed representational features of the core semantic to calculate the similarity between each expression for better clustering expressions into synsets.
Besides, by keeping track of the updated content in Open-KG, synsets will always contain the newly emerged expressions, which ensures the output of Sem4SAP is always closely related to the up-to-date Internet. 
In Synonym-Aware Pretraining, Sem4SAP consists of two complementary pretraining tasks, which inject synonymous expression knowledge and synonymous context knowledge into models, and two tricks to keep the specific ambiguity of the synonymous expressions.


In summary, we conclude that this paper makes the following contributions:
\begin{enumerate}
    \item We propose to acquire the up-to-date synsets with greatly diverse synonymous expressions by using the crowd-sourcing content in continually updating Open-KG. 
    \item We propose a novel synonymous expressions mining method to cluster the content in Open-KG into synsets, and we also propose two novel pretraining tasks to inject the synonym knowledge into LMs.
    \item Extensive experiments have been carried out to verify that Sem4SAP outperforms all the other baselines in improving the synonymous expressions understanding to models, and every component in Sem4SAP is effective.
\end{enumerate}



