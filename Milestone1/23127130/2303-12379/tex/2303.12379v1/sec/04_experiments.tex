\section{Experiments}
% In this section, we first describe the datasets in Section~\ref{Dataset}.
% To verify the effectiveness of the proposed method \frameworkname{}, we conduct experiments on MSVD and a widely-used benchmark dataset: Youtube-8M~\cite{abu2016youtube}. The implementation details are shown in Section~\ref{imple} and the introduction of the evaluation metric we use in our experiment is in Section~\ref{metric}. In Section~\ref{eva_MSVD}, we compare the proposed \frameworkname{} method with state-of-the-art techniques to evaluate its performance on existing and new music. Assessing them on another cross-modal dataset, Youtube-8M~\cite{abu2016youtube}, shown in Section~\ref{eva_yt8m}. We provide our result on quantitative and qualitative in the following.

% \wc{
In this section, we describe the datasets including \ourdataset{} and Youtube-8M~\cite{abu2016youtube} in Sec.~\ref{Dataset}. The implementation details and evaluation metric are presented in Sec.~\ref{imple}. Moreover, we provide quantitative and qualitative results for the comparison between our approach and state-of-the-art techniques in Sec.~\ref{eva_MSVD}. The ablation study is in Sec.~\ref{ablation}.
% }

\subsection{Dataset}\label{Dataset}
% Table~\ref{tab:table_msvd} shows details of the datasets employed in our experiments.

% \wc{
We summarize the details about the datasets in Table~\ref{tab:table_msvd} and further illustrate them as follows.
% }
\paragraph{\ourdataset{}.}
Matching short videos with background music requires a suitable dataset, but there is no publicly available one~\footnote{
The dataset used in CMVAE is not publicly available.
% For the CMVAE, they don't release dataset.
}. The Youtube-8M~\cite{abu2016youtube} dataset contains video features and music features, but most \music{} are the soundtrack of videos. Additionally, most videos are longer than one minute. To overcome these limitations, we collect videos with background music from a well-known multimedia platform. We filter out videos whose music (1) is the soundtrack of the video or (2) upload by the video uploaders. Furthermore, we exclude those videos longer than 20 seconds. We download corresponding videos randomly for each \music{}. 
The video resolution is down-sampled by one-fourth to reduce data size. Our Music for Short Video Dataset (\textbf{\ourdataset{}}) comprises about 150,000 video-music samples, each 8 seconds long and one frame per second. We used the uploader's choice of background \music{} as the ground truth for each sample. In summary, the dataset includes 390 \music{}, split into a \closesetname{} set (265 clips) and an unseen music set (125 clips). The \closesetname{} set was randomly divided into training, validation, and testing sets, with a ratio of 20:1:4 for 500 corresponding videos. The \opensetname{} set contains 140 videos for each \music{}, with no music present in the training music set.
%Unofficial music is often made by users and is at risk of not being generalized well by other videos.

\paragraph{Youtube-8M~\cite{abu2016youtube}.}
Youtube-8M~\cite{abu2016youtube} is not ideal for our proposed use case since the majority of the music in video-music pairs is the natural sound from the video. 
We follow CEVAR~\cite{suris2018cross} to conduct experiments on a random subset of 6000 clips. We use the pre-computed video-level features in the dataset: a single vector for audio information and a single vector for visual information in a video. All experiments use these fixed pre-computed features.
Youtube-8M includes video genre classification labels that indicate the topic of the video clip, and CEVAR utilized the labels as additional training signals.
We ignore the video genre classification labels to make the experiment setting the same as our MSVD dataset.

\input{tab/compare_method.tex}
\input{tab/compare_m_backbone.tex}
\subsection{Implementation Details}\label{imple}
\fuen{
We implement our proposed framework using three NVIDIA RTX 3090 with PyTorch~\cite{paszke2019pytorch}. All of the network parameters, including video encoder, music encoder, and shared prototype, are jointly optimized by Equation~\eqref{eq:L-final}. 
Adam~\cite{kingma2014adam} optimizer is adopted with learning rate 1e-5, and both the margin values $\mu$ and $\tau$ are set to $0.2$. The weight decay is set to $0.002$, and the batch size is set to $128$ for all models. The embedding size $l$ of video and music features is set to 256. Hyper-parameters are selected based on the evaluation metric on the validation set at recall@10. $\alpha$ and $\beta$ in Equation~\eqref{eq:LL} and \eqref{eq:L-final} are $0.38$ and $2$, respectively.}

\paragraph{Evaluation Metric.}\label{metric}
The evaluation metric Recall@K is denoted as:
\begin{equation}
    \text{Recall}@K=\sum_{v\in \mathcal{V}^{te}}\dot\#Hits_v@K
\end{equation}
where $\mathcal{V}^{te}$ denotes the set of testing video.
The Recall@K indicates the percentage of queries for which the model returns the correct item in its top K result.

% \subsection{Comparison of methods}
\subsection{Experimental Results}
\paragraph{Baselines.}
\fuen{To demonstrate the effectiveness of the proposed framework, we compare our \frameworkname~with the state-of-the-art video-music matching approaches. 1) CEVAR~\cite{suris2018cross}: the approach adopting softmax loss for video-music matching. 2) CMVAE~\cite{yi2021cross}: the approach adopting a variational auto-encoder for video-music matching.}


%which are based on supervised and variational auto-encoder methods, respectively. For CEVAR, the shared head is a fully connected layer and uses softmax loss for classification. For CMVAE, the loss for video and music is calculated using a cross-modal variational auto-encoder. We also provide the random guess result as a reference, which is $k/n$, where $k$ represents the top $k$ and $n$ represents the number of videos in the testing set.

\paragraph{Comparison on \ourdataset.}
\label{eva_MSVD}
\fuen{The quantitative results on our proposed \ourdataset~ dataset is shown in Table~\ref{tab:method}. In general, our proposed method outperforms the other baselines in all metrics. Compared to CMVAE, we found that learning the distribution of videos and music with a variational auto-encoder might not be a satisfying solution for video-music matching task, and our method improved Recall@10 by $2.4\%$ on \closesetname~and $46.7\%$ on \opensetname~. On the other hand, although CEVAR also learns a shared prototype with a softmax loss function, we improve Recall@10 by $5.3\%$ on \closesetname~and $15.3\%$ on \opensetname. We found that lifting video and music features to a hyper-sphere by our \faceloss~ is necessary for improving the decision boundary between classes.}

%our method improved Recall@10 by $5.3\%$ on \closesetname{} and $15.3\%$ on \opensetname{}. Softmax loss emphasizes the correct classification while ignoring the discernment between other features except for the correct class. However, the \faceloss{} emphasizes the discriminative from features. Compared to CMVAE, a variational-based generative, our method improved Recall@10 by $2.4\%$ on \closesetname{} and $46.7\%$ on \opensetname{}. 

\fuen{In addition to the comparison with SOTA approaches, we also compare the performance difference between CosFace and ArcFace loss functions. We empirically found that CosFace leads to a better training convergence, and thus we adopt CosFace to lift our video and music features to a shared hyper-sphere. Moreover, we conduct experiments for selecting the best margin value $\mu$ in Equation~\eqref{eq:LL} as shown in Fig.~\ref{fig:margin}. We evaluate the recall@20 performance on the \closesetname~set of MSVD by increasing $\mu$ from $0.01$ to $0.2$. The results indicate that CosFace achieves better performance and training stability than ArcFace does. As $\mu$ increases from $0.1$ to $0.2$, the variation of the ArcFace performance is approximately 0.1, while the variation of CosFace performance is smaller than 0.01. These findings suggest us to adopt CosFace in our \faceloss.}

%We also compare the CosFace loss and ArcFace loss and find that the performance of \frameworkname{} with CosFace loss was higher. The margin parameter $\mu$ plays a key role in the margin-based loss, so we conducted an experiment to examine the effect of $\mu$ on the \faceloss{} we used. We evaluated the recall@20 performance on the \closesetname{} set of MSVD by varying $\mu$ from $0.01$ to $0.2$, as shown in Fig.~\ref{fig:margin}. The results indicate that the CosFace-based loss achieved better performance with higher stability than the ArcFace-based loss. As $\mu$ increased from $0.1$ to $0.2$, the variation of the ArcFace-based loss performance was approximately 0.1, while the variation of the CosFace-based loss performance was less than 0.01. These findings demonstrate the superiority of the CosFace-based loss.

\fuen{Furthermore, as shown in Table~\ref{tab:mbk}, we compare the performance of two different backbones adopted in our music encoder. 1) Vggish: the backbone adopted by CMVAE. 2) ResNet-18, a more advanced residual network~\cite{he2016deep}. The performance of ResNet-18 outperforms Vggish at all metrics on both \closesetname~and \opensetname~despite the fact that Vggish is pretrained on Audio Set~\cite{gemmeke2017audio}. However, the results of adopting Vggish in our \frameworkname~framework still outperform other SOTA approaches on \closesetname~set in Table~\ref{tab:method}.} 

%However, the results of adopting Vggish in our \frameworkname~framework still outperform other SOTA approaches, as shown in Table~\ref{tab:method}.

%outperformed Vggish at all metrics on both \closesetname{} and \opensetname{} as shown in Table~\ref{tab:mbk}.
%Note that Vggish is pre-trained on the audio dataset. Nevertheless, using Vggish as the music encoder in the \frameworkname{} architecture still outperforms other state-of-the-art methods on \closesetname{} 
%according to the performance in Table~\ref{tab:method}.
\input{fig/fig_margin.tex}

\paragraph{Comparison on Youtube-8M~\cite{abu2016youtube}.}\label{eva_yt8m}
% \ys{
\fuen{We also evaluate our method on the Youtube-8M, a cross-modal video and music retrieval benchmark dataset, and compare it with SOTA approaches. Our evaluation is performed on the 1024 video-music pairs given by the repository of CEVAR. Since the features provided by Youtube-8M are one-dimensional, we use the original CEVAR backbone instead of R(2+1)D and 2D ResNet-18. The backbone consists of a set of fully connected layers that transform the original features into embeddings, with each hidden layer using ReLU as the activation function. Our results, presented in Table~\ref{tab:table_yt8m}, demonstrate the effectiveness of \frameworkname{} for cross-modal video and music matching on both \closesetname{} and \opensetname{}. Compared to CEVAR, our approach improves Recall@10 by a factor of $2$ on \opensetname{}. These results convincingly demonstrate better applicability of our proposed method than other methods.}
% }

\input{tab/yt8m_compare_method.tex}
\paragraph{Qualitative Comparison.}
\label{qua}
% We visualize the top-5 similarity score of matching music for the video in the \opensetname{} set to further compare \frameworkname{} and CEVAR, as shown in Fig.~\ref{fig:quialitative}. 

\fuen{To further address the performance difference between our \frameworkname~and CEVAR, we visualize the predicted top-5 matched music in \opensetname{} for video in Fig.~\ref{fig:quialitative} on \ourdataset~dataset.
The matched music is sorted based on similarity scores in descending order, with the ground truth one highlighted by red boxes. From the first example, we can observe that the \frameworkname{} method predicts the ground truth music with the highest similarity score, while CEVAR cannot even find the ground truth one in the top-5 matched list. 
Also, by visualizing the similarity of the prediction shown in Fig.~\ref{fig:quialitative} (c), we found that the similarity scores of CEVAR for each music only vary in a small range, while \frameworkname~produces higher variances between different music.}


%we find that the CEVAR doesn't spread out the similarity scores for each music.
%In contrast, \frameworkname{} learns to discriminate the features better so that the similarity scores become distinct and meaningful.

% From part (c) Fig. of the second example, we find that the CEVAR doesn't spread out the similarity scores for each music, which means \frameworkname{} learns to discriminate the features better.
\input{fig/fig_qualitative}

\subsection{Ablation Study}
\label{ablation}
\paragraph{Low-Level Feature.}
\fuen{We conduct an ablation study for the effectiveness of low-level music features extracted from openSMILE feature extraction toolkit, including CHROMA, loudness, and pitch. We compared the video-music matching performance with and without low-level features on both \closesetname{} and \opensetname{}. The results are presented in Table.~\ref{tab:table_llf}. The results suggest that low-level features are crucial for video-music matching tasks. Particularly on \opensetname{}, the performance of Recall@5 is improved by 1.8 times due to the fact that the pitch of the music provides information about the tempo and speed, which are directly associated with the alignment between actions in videos with the drum beats in the music. Additionally, the changes in music volume with time are also an important cue for matching video and music, which further improves the matching performance.}
\input{tab/ablation_llf.tex}
% \subsection{Ablation - The effectiveness of similarity loss}\label{esl}


\paragraph{Similarity Loss.}
% We investigate the impact of incorporating the similarity loss, which measures the distance between the output of the video encoder and that of the music encoder. In particular, we consider an anchor video, along with a positive pair of music and a negative pair of music. In addition to learning to distinguish between prototype samples, it is also crucial to learn the differences between sample-to-sample features.
% Furthermore, using multiple layers to learn these differences can enhance performance. We present the results of our experiments in Table~\ref{tab:table_emb}. Our results show that incorporating the similarity loss improves performance on both \closesetname{} and \opensetname{}, indicating the effectiveness of this technique.
\fuen{The effect of \simloss~is shown in Table~\ref{tab:table_emb}. The similarity loss aims at discriminating the features between a positive and a negative pairs of video and music inputs, and consider modality-to-modality distances for further improving matching performance. Hence, our results show that incorporating the similarity loss improves performance on both \closesetname{} and \opensetname{}, indicating that video-music matching task can benefit from this technique.}


%We investigate the effect of incorporating the similarity loss, which is measured in the output of the video encoder and music encoder. The similarity loss learns to discriminate the features from a positive pair and a negative pair of an anchor, which is sample-to-sample and is different from sample-to-prototype such as the face-loss. Furthermore, adding more connections to learn these discriminations can enhance performance. We present the results of our experiments in Table~\ref{tab:table_emb}. Our results show that incorporating the similarity loss improves performance on both \closesetname{} and \opensetname{}, indicating the effectiveness of this technique.
\input{tab/ablation_emb.tex}