\section{Conclusion}

    In this work, we develop a cross-modal framework \frameworkname{}, which addresses the challenges in music recommendation for new users or new music give short-form videos. \frameworkname{} constructs a shared embedding space between video and music representations effectively by adopting CosFace loss based on margin-based cosine similarity loss. Also, we collect a large-scale dataset (\textbf{\ourdataset{}}) which contains 390 individual music clips and the corresponding matched 150,000 videos, and we will release the dataset after the acceptance. We demonstrate that our approach achieves state-of-the-art performance for matching video and music on \textbf{Youtube-8M} and our \textbf{\ourdataset{}} datasets.



    % We developed a cross-modal framework \frameworkname{} on content-based system for matching video and background music, which addresses the challenges in music recommendation for new users or new music give short-form videos. The \frameworkname{} finds a shared embedding space between video and audio representations effectively by leveraging cosface loss based on margin-based cosine similarity loss.
    % Furthermore, we establish a large-scale dataset called \textbf{\ourdataset{}}. It provides 390 individual music clips and the corresponding matched 150,000 videos. The experiments on \textbf{Youtube-8M} and our \textbf{\ourdataset{}} datasets demonstrate the effectiveness of our proposed framework and achieve state-of-the-art video and music matching performance. %In future work, we would like to ~~~.