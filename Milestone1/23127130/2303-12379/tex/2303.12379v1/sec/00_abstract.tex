\begin{abstract}
    We propose a content-based system for matching video and background \music. The system aims to address the challenges in music recommendation for new users or new music give short-form videos. 
    %is the measurement of the similarity between video and music for new users and/or new musics.
    %since the instability from cross-modal inputs usually leads to a sub-optimal training results. 
    To this end, we propose a cross-modal framework \frameworkname{} that finds a shared embedding space between video and music representations. To ensure the embedding space can be effectively shared by both representations, we leverage CosFace loss based on margin-based cosine similarity loss.
    %
    Furthermore, we establish a large-scale dataset called \textbf{\ourdataset{}}, in which we provide 390 individual \music{} and the corresponding matched 150,000 videos.
    %short video background music dataset(SVBMD)\fuen{WTF}
    %
    % Furthermore, we establish a large-scale content-based video background music recommendation dataset, datasetofname, composed of approximately 390 different music clips associated with 150,000 videos.  QAQ
    We conduct extensive experiments on \textbf{Youtube-8M} and our \textbf{\ourdataset{}} datasets. Our quantitative and qualitative results demonstrate the effectiveness of our proposed framework and achieve state-of-the-art video and music matching performance.
    %We provide both quantitative and qualitative results on the established short video background music dataset(SVBMD) to show the effectiveness of the proposed approach.
    % Experiments on the established datasetofname demonstrate the effectiveness of the proposed method. A qualitative recommendation result is also included.
\end{abstract}