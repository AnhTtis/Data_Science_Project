\section{Approach}

% In this section, we first define our video music pair in Section~\ref{pro_def}. 
In this section, we introduce the problem formulation of the video-music matching task in Sec.~\ref{pro_def}. Then, we detail our loss functions in Sec.~\ref{objective} for lifting cross-modality features to a shared space.
Finally, we introduce our proposed \frameworkname~framework for video-music matching in Sec.~\ref{v_e}.

\subsection{Problem Definition}\label{pro_def}
Given a music set $\mathcal{M}=\{m\}_{i=1}^{N_m}$ with $N_m$ music and a video set $\mathcal{V}=\{v\}_{i=1}^{N_v}$ with $N_v$ videos from training dataset, where $(m,v)$ denote \music~and video samples. Our video-music matching task is formulated as two transformation functions ${\rm f}(m) \rightarrow y_m$ and ${\rm g}(v) \rightarrow y_v$, and $(y_m,y_v) \in [1,N_m]$ denote the predicted matching \music{} indexes in the training dataset. For matching video and \music{} as a metric learning problem, we further adopt the  shared weight $\textbf{W}$ to lift video and music features to a shared embedding space: 
\begin{align}
    \begin{split}
        % &{\rm h}({\rm f}(m)) \rightarrow y_m~,\\
        &\mathbf{W}\cdot{\rm f}(m) \rightarrow y_m~,\\
        &\mathbf{W} \cdot g(v) \rightarrow y_v~.
    \end{split}
    \label{eq:f-g}
\end{align}
During the testing stage, video and \music{} can be matched by estimating the cosine similarity between their features, i.e., ${\rm cos}({\rm f}(m),{\rm g}(v))$. For \music{} already included in the training dataset, we can directly use $y_v$ as the matched one. In this paper, the transformation functions $\rm f$ and $\rm g$ are respectively referred to as a video and a music encoder of which output embedding dimensions are both chosen as $l$.


\subsection{Cross-Modality Training Objectives}\label{objective}
Softmax loss is commonly used in the classification problem for minimizing intra-class and maximizing inter-class distances, which is formulated as:
\begin{equation}
    \begin{aligned}
        L_S(x_i, \mathbf{W}) = -\log \frac{e^{\mathbf{W}_{y_i} \cdot x_i}}{\sum_{j=1}^N e^{\mathbf{W}_j \cdot x_i}}~,
    \end{aligned}
    \label{eq:softmax-loss}
\end{equation}
where $\mathbf{W}$ is the prototype of each class, i.e., the weight of the last layer in a network, $N$ is the total number of classes, $x_i$ is the feature, and $y_i$ is the ground truth class index of $x_i$. For further improving the decision boundary between different classes, CosFace~\cite{cosface} proposed to lift the features and prototypes to a hyper-sphere by introducing a scaling term $s$ and a margin $\mu$:

\begin{equation}
\begin{aligned}
    &\cos(\theta_{k, i}) = \frac{\mathbf{W}_{k} \cdot x_i}{\norm{\mathbf{W}_{k}} \cdot \norm{x_i}}~, \\
    L_{C}(x_i, \mathbf{W})=&-\log \frac{e^{s \cdot [\cos(\theta_{y_i,i})-\mu]}}{e^{s \cdot [\cos(\theta_{y_i,i})-\mu]} + \sum_{j \neq i}^N e^{s \cdot \cos(\theta_{j,i})}}~.
    \label{eq:cosface}
\end{aligned}
\end{equation}
Since Equation~\eqref{eq:cosface} is based on the angles between intra and inter classes (i.e., $\theta_{y_i,i}$ and $\theta_{j,i}$) in a normalized feature space, the features $x_i$ are thus optimized in a hyper-sphere.

\paragraph{Cross-Modality Lifting Loss.}
\label{loss_class}
\fuen{
To solve the video-music matching task as a metric learning problem, we aim at lifting video and music features to the same hyper-sphere. In this way, we can match the videos to their most appropriate music by calculating the cosine similarity between them. Hence, we propose ``Cross-Modality Lifting Loss'' by adopting a shared prototype $\mathbf{W}$ for both video and music features and considering modality-to-prototype distances:
\begin{equation}
\begin{aligned}
    L_{LL} ({\rm g}(v), {\rm f}(m), \mathbf{W}) = L_C({\rm g}(v), \mathbf{W}) + \alpha L_C({\rm f}(m), \mathbf{W})~,
    \label{eq:LL}
\end{aligned}
\end{equation}
where $(v,m)$ are input music and video, $(\rm g,f)$ are the transformation functions as described in Equation~\eqref{eq:f-g}, and $\alpha$ is a hyper-parameter. In practice, the transformation functions are implemented as two independent feature encoders for video and music.}


\paragraph{Cross-Modality Similarity Loss.}
\label{loss_sim}
\input{tab/MSVD.tex}
\fuen{
Although our proposed \faceloss~can effectively minimize the intra and maximize the inter class distances, we found that only considering such a modality-to-prototype distance still leads to a sub-optimal performance since videos and music are eventually matched based on their features instead of their prototype during the testing stage. To this end, we follow \cite{suris2018cross} to adopt ``Cross-Modality Similarity Loss'' aiming at addressing the video-to-music feature distances for improving our downstream video-music matching performance:
\begin{equation}
\begin{aligned}
    \cos(x_i, x_j) &= \frac{x_i \cdot x_j}{\norm{x_i} \cdot \norm{x_j}}~, \\
    L_{SL} ({\rm g}(v), {\rm f}(m), {\rm f}(m^\prime)) &= max[\tau, \cos({\rm g}(v), {\rm f}(m^\prime))] \\
    &- \cos({\rm g}(v), {\rm f}(m))~,
    \label{eq:SL}
\end{aligned}
\end{equation}
where $(v, m)$ indicates a positive video-music pair queried according to the ground truth music index of $v$, $(v, m^\prime)$ indicates a negative one randomly sampled from the dataset, and $\tau$ is a selected margin value. With \simloss, we can apply direct constraints in-between the predicted video and music features, and consider modality-to-modality distances, which provides consistent video-music matching schemes under both training and testing stages.
}


\subsection{\frameworkname~Framework}
\paragraph{Video Encoder.}
\label{v_e}
\fuen{To extract video features, we adopt a R(2+1)D ResNet-18~\cite{tran2018closer}, pretrained on Kinetics-400~\cite{kay2017kinetics} and followed by a fully-connected layer to infer the video features with embedding size $l$, as our video encoder ($\rm g$ in Equation~\eqref{eq:f-g}).}

\paragraph{Music Encoder.}
\label{m_e}
\fuen{we firstly calculate the Mel spectrograms of the input \music{} and adopt a ResNet-18~\cite{he2016deep} pretrained on ImageNet~\cite{deng2009imagenet} to extract the high-level music features. In addition, inspired by \cite{yi2021cross}, we use openSMILE~\cite{eyben2010opensmile} to extract the low-level music features, including MFCC, voice intensity, pitch, etc. The low-level and high-level features are then fused by concatenation and we infer the final music features by passing the fused features into an additional fully-connected layer with embedding size $l$. We refer this music encoder as $\rm f$ in Equation~\eqref{eq:f-g}.}

\paragraph{Training and Testing.}
\label{matching}
\fuen{During each training iteration, we use a pair of video and music for calculating $L_{LL}$ (Equation~\eqref{eq:LL}). For $L_{SL}$ (Equation~\eqref{eq:SL}), we randomly sample a negative music for calculation. The final training objective is established as:
\begin{equation}
\begin{aligned}
L(v, m, m^\prime) = L_{LL}({\rm g}(v), f(m), \mathbf{W})& + \beta L_{SL}({\rm g}(v), {\rm f}(m),\\
{\rm f}(m^\prime))
% &+ L_{SL}({\rm g}(v), {\rm f}(m^\prime))]~,
\label{eq:L-final}
\end{aligned}
\end{equation}
where $m^\prime$ indicates a randomly-sampled negative music, and $\beta$ is a hyper-parameter.}

\fuen{During the testing stage, the \closesetname~and \opensetname~sets are evaluated with different schemes, as illustrated in the right part of Fig.~\ref{fig:framework}.
For \closesetname~set, the video features are inferred from our video encoder $\rm g$, while the music features are directly pulled from the trained prototype since the trained prototype can represent the feature center of each training music, which can significantly reduce the inference time of \opensetname~features. 
For \opensetname~set, the music features are extracted from our music encoder $\rm f$.}

\fuen{Finally, we match the videos and music by calculating the cosine similarity between their features and select the top 20 music clips with the highest similarities as the matched music list.}