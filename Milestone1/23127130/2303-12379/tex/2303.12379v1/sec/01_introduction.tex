\section{Introduction}
In recent years, short-form videos have rapidly entered our daily lives. People record their life by uploading short videos to various platforms such as TikTok, Instagram Reels, and Youtube Shorts. 
The daily usage time of TikTok by young generation users (ages 
10-25) in 2022 has grown by 2.38 times from 2017 to 2022, with 44 minutes into 105 minutes of daily TikTok. After the opening of Instagram Reels in April 2022, the number of short-form videos has increased significantly with an increase of 971 percent in August compared to April.
These platforms typically provide context-based recommendation systems to help users attach background \music{} to their uploaded videos. These \music{} are mostly selected according to users' previous selection or current trends, which eventually bias towards a few existing trendy background music.
The diversity of background music is important for these platforms since new trendy music can be more engaging and the uniqueness of trendy music can attract more new users to the platform.
However, the contents of background music and videos are typically not considered.
%
\input{fig/fig_teaser.tex}

The content-based music matching system, as shown in Figure~\ref{fig:teaser}, is crucial for 
(1) recommending new music as they haven't been selected and (2) recommending existing music to new users as they lack of previously selected music.  
There are few pioneers who work on cross-modal matching for video and audio content.
Hong~\etal~\cite{hong2018cbvmr} proposed a content-based retrieval model that combines the inter-modal ranking loss and soft intra-modal structure loss to construct a shared embedding space.
Sur{\'\i}s~\etal~\cite{suris2018cross} (referred to as CEVAR) proposed a joint embedding model with the classification loss along with a similarity loss, which incorporates the video labels provided by the YouTube-8M~\cite{abu2016youtube} dataset.
Yi~\etal~\cite{yi2021cross} (referred to as CMVAE) proposed a hierarchical Bayesian generative model using variational auto-encoder and matching relevant background music to videos by the corresponding latent embeddings.
However, all these methods suffer from a big performance gap between matching existing music to new videos v.s. matching new music to new videos. We use seen or existing interchangeably and unseen or new interchangeably.

%The content-based music matching system is crucial for attracting new multimedia platform users and increasing the traffic of new musics. 
%The previous behavior information of new users is insufficient, and new music lacks user usage data. 
%There are two major challenges for video-music matching systems. 1) The previous behavior information of new users is unknown and insufficient. 2) The new musics are lack of enough usage data for being precisely utilized into matching systems.
%To solve the above challenges, 
We treat this cross-modal matching task as a metric-learning problem where most \music{} matches to a few videos as they are non-trendy. Moreover, we want our method improves on both recommending existing music, as well as new music.
%We design a cross-modal content-based system ``\textbf{XXXX}''. As illustrated in Figure~\ref{fig:teaser}, our proposed framework can be generalized to both trained and unseen videos and musics. For unseen musics, the video-music matching task can be formulated into a metric-learning problem by calculating the feature similarity between videos and musics For trained musics, we consider the task as a classification problem since the music prototypes are included in the final classification layer of the proposed network.
%\ys{In testing, matching music can be evaluated under trained music set and unseen music set. For the trained music set, all testing musics are in the training set. 
%Therefore, the videos in the trained music set can be solved as a classification problem. For the unseen music set, the testing musics are out of the training set, making music matching video more challenging. It would be overhead if we retrained whenever we had new music. 
%We hope that when there is new music, the previously trained model can be directly applied. Since it is incredible to classify videos by the musics in training set, we need to calculate the similarity between the video representation and music representation.
%Furthermore, \ys{Most non-trending music is pair less with videos, and the ground truth number of the background music matching for video is only one. 
%The ground truth comes from the music chosen by the uploader.}
It is not obvious at first glance, but face recognition~\cite{turk1991face,sun2014deep,liu2017sphereface,kemelmacher2016megaface} shares the same challenges to our task. A face recognition model not only needs to recognize existing faces in training. It also needs to enroll new faces and identify them without retraining the model.
In addition, the model should only require a person to enroll a few faces for ease of usage.
Inspired by these observations, we adopt the CosFace loss~\cite{cosface} and the ArcFace loss~\cite{deng2019arcface}, widely used for face recognition to our cross-modal matching task. We refer to these losses as lifting loss. To find a shared embedding space between video and music, there is a shared head that makes both video and music features more aligned. Furthermore, we calculate the similarity loss between video and music features. More specifically, we combine \faceloss{} and \simloss{} for cross-modal matching. 

There is no publicly available dataset for matching video and background music. 
% for us to \ys{conduct} experiments. 
% It is challenging to construct such a dataset since the pairs of video and music on these platforms are unorganized and constantly changing. In order to make sure the quality, we discard most of them and only leave the videos with official background music. 
We established a dataset, Music for Short Video Dataset \textbf{\ourdataset{}}, containing 150k videos and 390 corresponding background \music{} pairs. The differences between Youtube-8M~\cite{abu2016youtube} and MSVD are (1) the ground truth of music and video in the former is a one-to-one mapping, (2) most of the music in the former is the soundtrack of the video and (3) most of the videos in the former are longer than 1 minute.
We divide the MSVD into \closesetname{} set and \opensetname{} set corresponding to real-world scenarios.  
% We extract video features and music features using generic networks pre-trained on large datasets.  
With the \textbf{\ourdataset{}} dataset established, we conduct experiments to evaluate the performance of \frameworkname{} and compare it with previous methods.


Our main contributions are summarized as follows:
\begin{itemize}
    \item[(1)] We propose a novel cross-modal framework \frameworkname{} with \faceloss{} and \simloss{} for content-based background music matching, which can be applied to both seen and unseen video-music samples.
    
    \item[(2)] We collect a short video and background music matching dataset called ``Music for Short Video Dataset~(\ourdataset{})'', the first publicly available dataset for the video-music matching task which provides 390 \music{} and their 150,000 corresponding videos.

    \item[(3)] \frameworkname{} achieve the state-of-the-art on \textbf{\ourdataset{}} and Youtube-8M~\cite{abu2016youtube}.
\end{itemize}