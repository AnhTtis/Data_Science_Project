\section{Approach}

% In this section, we first define our video music pair in Section~\ref{pro_def}. 
In this section, we present our problem formulation in Sec.~\ref{pro_def}
% And 
Then, we introduce our architecture for retrieving video and music features in Sec.~\ref{v_e} and Sec.~\ref{m_e}.
Finally, we show the training objectives in Sec.~\ref{loss_class} and Sec.~\ref{loss_sim}. The detail of testing the video and music matching is shown in Sec.~\ref{matching}


\subsection{Problem Definition}\label{pro_def}
% \ys{Suppose $\mathcal{M}$ be a set of music and $\mathcal{V}$ be a set of videos. Each pair $(m, v)$ where $m\in \mathcal{M}$ and $v\in \mathcal{V}$ is associated with a mapping $f:\mathcal{V}\times \mathcal{M}\rightarrow {0,1,2,...,n}$, where $n$ is the number of music. This mapping generates a set of triplets ${(v,m,y)}$, where $y=f(v,m)$ is an indicator of the match between the video and the background music $m$, which is the ground truth selected by the uploader. The task is to predict the most suitable music clip $m$ from a set of candidate clips for a given video $v$, based on their content features.}

%Suppose $\mathcal{M}$ and $\mathcal{V}$ are the sets of musics and videos, respectively. Each pair $(m, v)$ where $m\in \mathcal{M}$ and $v\in \mathcal{V}$ is associated with a mapping $f:\mathcal{V}\times \mathcal{M}\rightarrow y$, where $y\in [1, n]$ is an indicator of the match between the video $v$ and the background music $m$, i.e., the matched music index, and $n$ is the total number of seen musics.
%The target of video-music matching task is to select the most appropriate music clip $m$ for a given video $v$ from a set of candidate clips $\mathcal{M}$ based on their content features.

Given a music set $\mathcal{M}=\{m\}_{i=1}^{N_m}$ and a video set $\mathcal{V}=\{v\}_{i=1}^{N_v}$ from the training dataset, where $(m,v)$ denote music and video clips and $(N_m,N_v)$ are the total numbers of them, our video-music matching task is formulated as two mapping functions ${\rm f}(m) \rightarrow y_m$ and ${\rm g}(v) \rightarrow y_v$, where ${\rm f}$ and ${\rm g}$ represent music encoder and video encoder, and $(y_m,y_v) \in [1,N_m]$ denote the predicted matched music indexes in the training dataset. For matching video and music clips as a metric learning problem, we further adopt a mapping function $\rm h$ with the shared weight $\textbf{W}$ to lift video and music features to a shared embedding space: 
\begin{align}
    \begin{split}
        &{\rm h}({\rm f}(m)) \rightarrow y_m~,\\
        &{\rm h(g}(v)) \rightarrow y_v~.
    \end{split}
    \label{eq:f-g}
\end{align}
During the testing stage, video and music clips can be matched by estimating the cosine similarity between their features, i.e., $\text{cos}({\rm f}(m),{\rm g}(v))$. For music clips already included in the training dataset, we can directly use $y_v$ as the matched one. In this paper, the mapping functions $f$ and $g$ are respectively referred to as a video and a music encoder of which output embedding dimensions are both chosen as $l$.














\subsection{Video and Music Encoders}
% {
%To extract video features, we adopt a R(2+1)D ResNet-18~\cite{tran2018closer} pretrained on Kinetics-400~\cite{kay2017kinetics} as our video encoder ($g$ in Equation~\eqref{eq:f-g}).

%In order to acquire a shared embedding space between the video and music representations, their feature embedding sizes need to be the same. We ensure this by setting the dimension of both feature embeddings as $l$. 

\paragraph{Video Encoder.}
\label{v_e}
\fuen{To extract video features, we adopt a R(2+1)D ResNet-18~\cite{tran2018closer}, pretrained on Kinetics-400~\cite{kay2017kinetics} and followed by a fully-connected layer to infer the video feature with embedding size $l$, as our video encoder ($g$ in Equation~\eqref{eq:f-g}).}


%To improve the quality of the video representation and adapt it from the pre-trained domain, we further update the video encoder with the objectives we described in Sec.~\ref{loss_class} and Sec.~\ref{loss_sim}.

% To improve the quality of the video representation and the domain adaptation from the pre-trained dataset, we enable the loss function to make all the weights of the video encoder trainable.
% }

\paragraph{Music Encoder.}
\label{m_e}
% \ys{
% We exploit openSMILE~\cite{eyben2010opensmile} to extract the low-level music features, which consist of MFCC, voice intensity, pitch, etc. Additionally, we transform music into a Mel spectrogram, which can be treated as an image. Consequently, we apply the 2D ResNet18~\cite{he2016deep} pre-trained on the ImageNet dataset~\cite{deng2009imagenet} as the music encoder to extract music features. We connect the last layer of 2D ResNet18~\cite{tran2018closer} to a fully connected layer in order to convert the embedding size of output to $l$ after concatenating with the low-level music features. To enhance the quality of music representation and domain adaptation from the pre-trained dataset, all weights of the 2D ResNet18~\cite{tran2018closer} part music encoder are trainable by the loss function.
% }

%As for music feature representation, we combine both low-level and high-level music features. 

\fuen{Inspired by \cite{yi2021cross}, we firstly use openSMILE~\cite{eyben2010opensmile} to extract the low-level music features, including MFCC, voice intensity, pitch, etc. In addition, we calculate the Mel spectrograms of the input music clips and adopt a ResNet-18~\cite{he2016deep} pretrained on ImageNet~\cite{deng2009imagenet} to extract the high-level music features. The low-level and high-level features are then fused by a concatenation and we infer the final music features by passing the fused features into an additional fully-connected layer with embedding size $l$. We refer this music encoder as $f$ in Equation~\eqref{eq:f-g}.}

%We leverage openSMILE~\cite{eyben2010opensmile} to extract the low-level music features, which consist of MFCC, voice intensity, pitch, etc. Additionally, we transform music into a Mel spectrogram, which can be treated as an image, and apply the ResNet18~\cite{he2016deep} pre-trained on the ImageNet dataset~\cite{deng2009imagenet} to obtain high-level music features. These two features are concatenated and converted with a fully-connected layer to the shared embedding space with dimension $l$. Again, the music encoder $\rm f$ (refer to equation~\ref{eq:f-g}) is updated with the objectives we described in Sec.~\ref{loss_class} and Sec.~\ref{loss_sim}.

\paragraph{Video and Music Matching.}
\label{matching}
\fuen{In order to acquire a shared embedding space between the video and music representations, their feature embedding sizes need to be the same. We ensure this by setting the dimension of both feature embeddings as $l$. Need to revise}

During testing stage, we evaluate under \closesetname{} set and \opensetname{} set, as illustrated in the right part of Fig~\ref{fig:framework}.

% \ys{For \closesetname{} set $(\mathcal{V}^{seen}, \mathcal{M}^{seen})$, we obtain video features from the video encoder. For each $v_i\in \mathcal{V}^{seen}$, we compute the probabilities of matching each music clips in training set using the output of the shared head, resulting in the probabilities $P_{v_i}=\{p_{v_{i,j}}\}_{j=1}^{N_m}$. Higher probabilities indicate a stronger match between the video clips and music clips.}
For \closesetname{} set, we obtain video features from the video encoder and compute the probabilities of matching each music in the training set using the output of the shared head. Higher probabilities indicate a stronger match between the video clips and music clips.

% \ys{For \opensetname{} set $(\mathcal{V}^{unseen}, \mathcal{M}^{unseen})$, we obtain the video features using the video encoder, and similarly, we extract the music features with the music encoder. For each $v_i\in \mathcal{V}^{unseen}$, we calculate the cosine similarity between the $m_j \in \mathcal{M}^{unseen}$ by the equation~\ref{eqn:cos_sim}. Higher consine similarity values denote a stronger match between the video clips and the music clips.}

For \opensetname{} set, we obtain the video features using the video encoder, and similarly, we extract the music features with the music encoder. For the features of each video, we calculate the cosine similarity between the features of music in \opensetname{} set by the equation~\ref{eqn:cos_sim}. Higher consine similarity values denote a stronger match between the video clips and the music clips.

\subsection{Cross-Modality Lifting Loss}\label{loss_class}
In the classification problem, the softmax loss is commonly used, which maximizes the posterior probability of the ground-truth identities. In our task, since the video features and music features share the weight $\textbf{W}$ at the shared head, we simplify the equation by defining both video features and music features as shared features $x$ and shared ground truth $y$. The softmax loss can be formulated as:\\
\begin{equation}
    \begin{aligned}
        L_S(x) = -\log \frac{e^{\textbf{W}_{y_i} \cdot x_i}}{\sum_{j=1}^N e^{\textbf{W}_j \cdot x_i}}~,
    \end{aligned}
    \label{eq:softmax-loss}
\end{equation}
$N$ represents the batch size. $\textbf{W}$ indicates the weight of the last fully connected (FC) layer. 

However, the softmax loss is limited by its inability to produce sufficiently discriminative features, as it primarily focuses on correct classification. To address this issue, we introduce the angular edge-based supervised \faceloss{} and add a scaling term $s$, such as CosFace~\cite{cosface} or ArcFace~\cite{deng2019arcface}. 
The CosFace loss function is formulated as follows:\\
\begin{equation}
\begin{split}
    L_{C}(x)=
    -\frac{1}{N}\sum_{i=1}^N\log(\frac{e^{s(\cos(\theta_{y_i,i})-\mu)}}{e^{s(\cos(\theta_{y_i,i})-\mu)}+\sum\limits_{j\neq i}^ne^{s\cos(\theta_{j,i})}}),\\
\end{split}
\end{equation}
subject to
\begin{equation}
    \begin{split}
        \text{cos}(\theta_{j,i})=\textbf{W}_j^Tx_i
        =\norm{\textbf{W}_j}\norm{x_i}\cos\theta_j,
    \end{split}
\end{equation}
where $\mu$ is the angular margin and $\theta_j$ is the angle between $\textbf{W}_j$ and $x_i$.\\ 
Similarly, the ArcFace loss function can be expressed as:\\
\begin{equation}
\begin{split}
    L_{A}=
    -\frac{1}{N}\sum_{i=1}^N\log(\frac{e^{s\cos(\theta_{y_i,i}+\mu)}}{e^{s\cos(\theta_{y_i,i}+\mu)}+\sum\limits_{j\neq i}^ne^{s\cos(\theta_{j,i})}})
\end{split}
\end{equation}

We define the \faceloss{} as $L_C$ or $L_A$. To combine the two \faceloss{} of video branch $L_{face_v}$ and music branch $L_{face_m}$, we use the following formula:
\begin{equation}
L_{face}=L_{face_v}+\alpha L_{face_m},
\end{equation}
where $\alpha$ balances the impact of the video branch \faceloss{} and music branch \faceloss{}.

\subsection{Cross-Modality Similarity Loss}\label{loss_sim}
\input{tab/MSVD.tex}
To further improve the effectiveness of distinguishing different music idex features in the representation space, we add a similarity loss to strengthen the connection between the last layer of two branches. Moreover, we aim to get the embedding distance of sample-to-sample closer when they are positive pairs and farther when they are negative pairs.

The cosine similarity of video features and music features is computed as: 
\begin{equation}
\label{eqn:cos_sim}
sim =  {\rm cos}({\rm g}(v_i), {\rm f}(m_i)) =  \frac{{\rm g}(v_i) \cdot {\rm f}(m_i)}{\norm{{\rm g}({v_i})}\norm{{\rm f}(m_i)}}
\end{equation}
for any pair of vectors $v_i$ and $m_i$.

We determine whether the video and music are a positive pair or a negative pair based on whether 
$y_v=y_m$. For positive pairs, we maximize their similarity, while for negative pairs, we minimize their similarity. We add a margin $\mu$ to the negative pairs to ensure their similarity is small enough to be considered zero.

In training, both positive and negative pairs are used. 
For negative pairs of the video $v_i$, we randomly sample music from the set of $\mathcal{M}$ whose $y_m\neq y_{v_i}$. The similarity loss is denoted as:\\
\begin{equation}
\begin{split}
L_{cos}(v_i, m_j, y)=\left\{\begin{array}{l}1-{\rm cos}(v_i,m_j), y=1\\{\rm max}(0, {\rm cos}(v_i,m_j)-\mu), y=-1\end{array}\right.\\
\end{split}
\end{equation}
where $y=1$ represents positive pair sampling ($y_{v_i}=y_{m_j}$) and $y=-1$ represents negative pair sampling ($y_{v_i}\neq y_{m_j}$).

Finally, our loss function is the combination of \faceloss{} and similarity loss, and can be optimized as:\\
\begin{equation}
L = L_{face}+\beta L_{cos}
\end{equation}
where $\beta$ balances the \faceloss{} and similarity loss. By minimizing the loss function, we aim to train a modal that can effectively distinguish different music indexes in the representation space.

% \paragraph{Video and Music Matching.}~\label{matching}
\iffalse
\subsection{Video and Music Matching}~\label{matching}

\fuen{In order to acquire a shared embedding space between the video and music representations, their feature embedding sizes need to be the same. We ensure this by setting the dimension of both feature embeddings as $l$. Need to revise}

During testing stage, we evaluate under \closesetname{} set and \opensetname{} set, as illustrated in the right part of Fig~\ref{fig:framework}.

% \ys{For \closesetname{} set $(\mathcal{V}^{seen}, \mathcal{M}^{seen})$, we obtain video features from the video encoder. For each $v_i\in \mathcal{V}^{seen}$, we compute the probabilities of matching each music clips in training set using the output of the shared head, resulting in the probabilities $P_{v_i}=\{p_{v_{i,j}}\}_{j=1}^{N_m}$. Higher probabilities indicate a stronger match between the video clips and music clips.}
For \closesetname{} set, we obtain video features from the video encoder and compute the probabilities of matching each music in the training set using the output of the shared head. Higher probabilities indicate a stronger match between the video clips and music clips.

% \ys{For \opensetname{} set $(\mathcal{V}^{unseen}, \mathcal{M}^{unseen})$, we obtain the video features using the video encoder, and similarly, we extract the music features with the music encoder. For each $v_i\in \mathcal{V}^{unseen}$, we calculate the cosine similarity between the $m_j \in \mathcal{M}^{unseen}$ by the equation~\ref{eqn:cos_sim}. Higher consine similarity values denote a stronger match between the video clips and the music clips.}

For \opensetname{} set, we obtain the video features using the video encoder, and similarly, we extract the music features with the music encoder. For the features of each video, we calculate the cosine similarity between the features of music in \opensetname{} set by the equation~\ref{eqn:cos_sim}. Higher consine similarity values denote a stronger match between the video clips and the music clips.
\fi