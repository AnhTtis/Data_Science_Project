\section{Related Works}
We introduce the previous cross-modal matching methods in Sec.~\ref{cross_modal_matching}, and present the metric learning in Sec.~\ref{metric_learning}. 
\subsection{Cross-Modal Matching}\label{cross_modal_matching}
Most existing cross-modal matching methods typically focus on textual and visual modalities, using open-sourced datasets such as MSCOCO~\cite{lin2014microsoft}, Flickr30k~\cite{plummer2015flickr30k}, ActivityNet-captions~\cite{krishna2017dense} and MSR-VTT~\cite{xu2016msr}.
Zhen~\etal~\cite{zhen2019deep} proposed a deep supervised cross-modal retrieval method that simultaneously minimizes discrimination loss in the label space and invariance loss.
Wei~\etal~\cite{wei2020universal} introduced a universal weighting metric learning framework and a new polynomial loss under it. The universal weighting framework provides a powerful tool to analyze various metric-learning-based weighting and loss, which have been widely used in cross-modal matching.
However, there has been little effort devoted to matching video and music content. Hong~\etal~\cite{hong2018cbvmr} presented a content-based retrieval model that only uses content features between music and videos, constraining the relative distance relationship of samples within each modality. While Suris~\etal~\cite{suris2018cross} uses the visual features and audio features provided by Youtube-8M~\cite{abu2016youtube} to minimize the distance of visual embedding and audio embedding of the same video at representation space to predict the corresponding video label.
Yi~\etal~\cite{yi2021cross} proposed a hierarchical Bayesian generation model using the variational auto-encoder to match relevant background music to video through their latent embedding.
Nonetheless, these methods face a significant performance gap when matching existing music to new videos compared to matching new music to new videos. This work introduces face recognition-inspired loss to mitigate the gap.

\input{fig/fig_framework.tex}

\subsection{Metric learning}\label{metric_learning}
The goal of metric learning is to learn the similarity between features to enable accurate feature matching and verification. To improve the quality of feature embedding, contrastive loss~\cite{chopra2005learning,deng2009imagenet} and triplet loss~\cite{wang2014learning,hoffer2015deep} are commonly employed techniques that help increase the Euclidean margin. In addition, there are other variations of metric learning methods such as center loss~\cite{wen2016discriminative} and angular loss~\cite{cosface} that have been proposed specifically for face recognition tasks. Center loss aims to minimize the distance between each sample and its corresponding class center. In contrast, angular loss minimizes the distance between the feature embedding and its corresponding class boundary. Overall, metric learning has played a crucial role in enabling accurate verification systems for learning the similarity between features.
