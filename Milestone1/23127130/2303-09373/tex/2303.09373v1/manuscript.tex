% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage[colorlinks]{hyperref}
\usepackage[nameinlink,noabbrev]{cleveref}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{booktabs}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{3D Masked Autoencoding and Pseudo-labeling for Domain Adaptive Segmentation of Heterogeneous Infant Brain MRI}
%
\titlerunning{MAPSeg for Domain Adaptive Segmentation of Infant Brain MRI}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{
Xuzhe Zhang\inst{1}\orcidID{0000-0002-8378-6082}, 
Yuhao Wu\inst{2},
Jia Guo\inst{1}\orcidID{0000-0003-4803-0279}, 
Jerod M. Rasmussen\inst{3}\orcidID{0000-0002-9400-7750}, 
Thomas G. O’Connor\inst{4}, 
Hyagriv N. Simhan\inst{5}, 
Sonja Entringer\inst{3,6}\orcidID{0000-0002-9926-7076}, 
Pathik D. Wadhwa\inst{3}, 
Claudia Buss\inst{3,6}, 
Cristiane S. Duarte\inst{1}, 
Andrea Jackowski\inst{7}\orcidID{0000-0001-8842-5406}, 
Hai Li\inst{2}\orcidID{0000-0003-3228-6544}, 
Jonathan Posner\inst{2}, 
Andrew F. Laine\inst{1}\orcidID{0000-0003-3797-0628}, 
Yun Wang\inst{2}\orcidID{0000-0003-2426-2876}
}
%
\authorrunning{Xuzhe Zhang et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Columbia University, New York NY 10027, USA \and
Duke University, Durham NC 27708, USA \and
University of California Irvine, Irvine CA 92697, USA\and
University of Rochester Medical Center, Rochester NY 14642, USA \and
University of Pittsburgh, Pittsburgh PA 15260, USA\and
Charité – Universitätsmedizin Berlin, Charitépl. 1, 10117 Berlin, Germany\and
Federal University of São Paulo, São Paulo - SP, 04021-001, Brazil
}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Robust segmentation of infant brain MRI across multiple ages, modalities, and sites remains challenging due to the intrinsic heterogeneity caused by different MRI scanners, vendors, or acquisition sequences, as well as varying stages of neurodevelopment. To address this challenge, previous studies have explored domain adaptation (DA) algorithms from various perspectives, including feature alignment, entropy minimization, contrast synthesis (style transfer), and pseudo-labeling. This paper introduces a novel framework called MAPSeg (Masked Autoencoding and Pseudo-labelling Segmentation) to address the challenges of cross-age, cross-modality, and cross-site segmentation of subcortical regions in infant brain MRI. Utilizing 3D masked autoencoding as well as masked pseudo-labeling, the model is able to jointly learn from labeled source domain data and unlabeled target domain data. We evaluated our framework on expert-annotated datasets acquired from different ages and sites. MAPSeg consistently outperformed other methods, including previous state-of-the-art supervised baselines, domain generalization, and domain adaptation frameworks in segmenting subcortical regions regardless of age, modality, or acquisition site. The code and pretrained encoder will be publicly available at \url{https://github.com/XuzheZ/MAPSeg}.

\keywords{Domain adaptation \and Self-supervised learning \and Subcortical segmentation \and Masked autoencoder \and Pseudo-labeling \and Infant neuroimaging}
\end{abstract}
%
%
%
\section{Introduction}
The human brain undergoes significant growth and expansion during the early postnatal years, with both cortical and subcortical regions undergoing dynamic development \cite{gilmore2018imaging}. Subcortical regions play a crucial role in regulating basic functions such as movement and sensory processing, and their development during this period is closely linked to the development of cortical regions. The connections between subcortical and cortical regions are critical for integrating sensory and motor information and regulating behavior \cite{cruz2023cortical}. Disruptions in subcortical development during this period can have long-lasting effects on brain function and behavior and are implicated in neurodevelopmental disorders such as autism \cite{shen2022subcortical} and ADHD\cite{baribeau2019structural}. Therefore, understanding subcortical regions is essential for uncovering the mechanisms underlying brain disorders that affect cognitive, emotional, and motor function and for developing effective treatments.

MRI is a non-invasive method to study early brain development, providing crucial information about the volume, shape and developmental trajectories of these subcortical regions. However, MRI bears some intrinsic heterogeneity caused by different acquisition scanners and sequences. In addition, the rapid development of infant's brain also leads to contrast change during first two years of life. These factors contribute to domain shifts (Fig.\ref{fig:1}), which can degrade the segmentation accuracy when applied to new domains.

In this study, we introduce a novel domain adaptive segmentation framework named Masked Autoencoding and Pseudo-labeling Segmentation (\textbf{MAPSeg}), by taking advantage of large-scale pretraining and self-learning through masked pseudo-labeling. The main contributions of this study are summarized as follows: 
\begin{enumerate}
  \item Designed a novel domain adaptive segmentation framework from the perspective of self-supervised learning.
  \item Evaluated the proposed framework extensively on cross-modality, cross-age, and cross-site challenges in segmenting infant subcortical regions.
  \item Investigated the effect of each component in the proposed framework.
\end{enumerate}

\begin{figure}[t!]
\centering
\includegraphics[width=275pt]{fig1-01.eps}
\caption{Illustrations of heterogeneity in infant brain MRI caused by different acquisition vendors/sequences, different modalities, and different acquisition ages.}\label{fig:1}
\end{figure}

\section{Methods}
The overview of the proposed framework is depicted in Fig.\ref{fig2}. MAPSeg consists of three components: a 3D masked multi-scale autoencoder for self-supervised pretraining, a 3D masked pseudo-labeling for domain adaptation, and a global-local collaborative network to fuse global and local contexts for segmentation. 

\textbf{3D Multi-scale Masked Autoencoder.} Masked autoencoder (MAE) is a simple yet powerful self-supervised learning (SSL) technique that has demonstrated the potential to train an encoder capable of generalizing well and outperforming supervised baseline (i.e., w/o MAE pretraining) in downstream tasks \cite{he2022masked}. In this study, we propose a 3D variant of MAE (Fig.\ref{fig2}.a). During training, a local sub-volume $x$ is randomly sampled from the global volume, and the global volume $X$ is downsampled to match the size of local sub-volume. Both $x$ and $X$ are randomly masked some patches and are denoted by $x^M$ and $X^M$. We train a universal encoder and a universal decoder to reconstruct $x/X$ based on $x^M/X^M$ using mean squared error as the objective function. Following the previous study \cite{he2022masked}, we implement an asymmetric design with a lightweight decoder compared to the encoder. The detailed configuration can be found in Section \ref{section:2.1}. 

\textbf{Self-supervised Domain Adaptation via 3D Pseudo-labeling.} Pseudo-labeling self-training is a prevalent technique in domain adaptative segmentation and the quality of the pseudo-label is the key to success. Several previous studies have focused on pseudo-label denoising and developing reliable uncertainty measures to weigh the pseudo-label \cite{chen2021source,zhang2021prototypical}. We hypothesize that masked encoding is an ideal pathway to pseudo-label self-training and the similar idea was brought up in a recent study \cite{hoyer2022mic}. 

\begin{figure}[t!]
\centering
\includegraphics[width=285pt]{fig2-02.eps}
\caption{An overview of our proposed framework. Note that the pretrained encoder in (a) are subsequently used as encoder in segmentation model, and $f_\phi$ and $f_\theta$ in (b) are equivalent to the Encoder+Segmentation Decoder in (c). } \label{fig2}
\end{figure}

However, using the masked pseudo-labeling alone may still lead to poor adaptation results if the discrepancy between source and target domains is significant (e.g., cross-modality MRI segmentation). We argue that using the pre-trained encoder from MAE may lead to high-quality pseudo-label and will help stablize training. Therefore, before pseudo-label self-training, we first train the encoder through MAE and then append a segmentation decoder after it (Fig.\ref{fig2}.b). 

Given image $x_s$ and label $y_s$ from source domain and image $x_t$ from target domain, the segmentation model is then trained through a teacher-student framework where teacher model $f_\theta$ takes the target image $x_t$ as input and generates the pseudo labels $f_\theta(x_t)$, with gradient detached. The student model $f_\phi$ takes the masked target image $x_t^M$ as input and output $f_\phi(x_t^M)$. Student model is also jointly trained by supervised component in source domain (using $y_s$ with both unmasked $x_s$ and masked $x_s^M$). The loss function of masked pseudo-labeling can be formulated as:  
\begin{equation}
\mathcal{L}_{pse}=\frac{1}{2}\mathcal{L}_{seg}(f_\phi(x_s), y_s)+\frac{1}{2}\mathcal{L}_{seg}(f_\phi(x_s^M), y_s)+\lambda \eta \mathcal{L}_{seg}(f_\phi(x_t^M), f_\theta(x_t)),
\end{equation}
where $\lambda$ is the weight of pseudo-labeling loss. It is set as 0 for the first 10 epochs and then rises from 0.1 to 1 in the following 10 epochs as a warm-up. $\eta$ is the quality estimate of the pseudo label. We follow previous studies \cite{hoyer2022mic,tranheden2021dacs} and set $\eta$ as the ratio of pixels that have a maximum softmax probability exceeding a threshold $\tau$ which is set as 0.95 empirically. The segmentation loss $\mathcal{L}_{seg}$ is hybrid loss consists of cross-entropy loss and soft dice loss \cite{isensee2021nnu}. 
 \begin{equation}
\mathcal{L}_{seg}(\hat{y}, y)=-\sum_{i=1}^{H}\sum_{j=1}^{W}\sum_{k=1}^{D}\sum_{c=1}^{C}y_{ijkc} \log\hat{y}_{ijkc} - \frac{2\sum\hat{y}y}{\sum \hat{y}+\sum y},
\end{equation}
where $\hat{y}$ and $y$ stand for the predicted and ground truth probability distributions for a pixel belonging to a class. 

During the pseudo-labeling training, teacher model $f_\theta$ is a copy of student model $f_\phi$ in the beginning and its parameters $\theta$ are then updated via exponential moving average (EMA) based on parameters of $f_\phi$ \cite{tarvainen2017mean}.

 \begin{equation}
\theta_{t+1} \gets \alpha \theta_{t} + (1-\alpha)\phi_t, 
\end{equation}
where $t$ and $t+1$ indicate training steps and $\alpha$ is the step size of EMA, which is set as 0.999 in this study. The teacher-student framework is utilized to provide a more stable pseudo label during training and is a standard strategy in semi-/self-supervised learning \cite{grill2020bootstrap,tarvainen2017mean}. 

\textbf{Global-local Collaborative Segmentation Network.} The segmentation backbone model consists of the pretrained MAE encoder and a decoder that was adapted from DeepLabV3 \cite{chen2017rethinking}. In the decoding path, we took advantage of the Atrous Spatial Pyramid Pooling (ASPP), which employs dilated convolution at multiple scales and provides access to larger fields of view (FOV) to better extract context information instead of local structural details. The DeepLabV3 was adapted by substituting all 2D operations with 3D operations. Detailed configurations can be found in Section \ref{section:2.1}. As the encoder can extract features at both local and global scales, we designed a Global-local Collaborative Segmentation Network (Fig.\ref{fig2}.c) inspired by a previous study \cite{chen2019collaborative}. To segment each local sub-volume, the local features $\chi_{loc}$ and global features $\chi_{glo}$ are concatenated along the channel dimension and fed into the decoder. To help encoder to extract meaningful semantic segmentation at the global scale, the model is also trained to segment the downsampled scans via
\begin{equation}
\mathcal{L}_{G}=\frac{1}{2}\mathcal{L}_{seg}(f_\phi(X_s), Y_s)+\frac{1}{2}\mathcal{L}_{seg}(f_\phi(X_s^M), Y_s)+\lambda \eta \mathcal{L}_{seg}(f_\phi(X_t^M), f_\theta(X_t)),
\end{equation}
where $X$ and $Y$ denote downsampled scans and labels. During this training, there is no crop or resize of features and we simply duplicate the encoded global features to match the number of channels.

The rationale behind the global-local collaboration is to take advantage of a strong prior within medical images, which is the global anatomical distribution. We hypothesize that when performing UDA to large domain shift (i.e., cross-modality), the local contrast may vary a lot which will lead to corrupted pseudo labels, while making predictions from global-local features fusion will improve pseudo-labeling training. Following the previous study \cite{chen2019collaborative}, we add a regularization between local and cropped global features to prevent model from making prediction solely relying on local features as local features have more fine-grained structural details. Instead of $\mathcal{L}_2$ regularization used in \cite{chen2019collaborative}, we maximize the cosine similarity between the local and cropped global features:
\begin{equation}
\mathcal{L}_{cos} = 1 - \frac{\chi_{loc}\cdot\chi_{glo}}{\max(\| \chi_{loc} \|_2, \| \chi_{glo} \|_2, \epsilon)},
\end{equation}
where $\chi_{loc}$ is the features extracted by encoder from local sub-volume and $\chi_{glo}$ is the cropped and upsampled features extracted from downsampled global scan, and $\epsilon$ is used to prevent zero-division. Therefore, the overall loss function for global-local collaboration can be written as 
\begin{equation}
\mathcal{L}_{global}= \mathcal{L}_{G} + \frac{1}{4} (\mathcal{L}_{cos}+\mathcal{L}_{cos}^M),
\end{equation}
where $\mathcal{L}_{cos}^M$ is calculated based on masked input. The overall objective function can be formulated as 
\begin{equation}
\mathcal{L} = \mathcal{L}_{pse}+0.1\cdot \mathcal{L}_{global},
\end{equation}

\subsection{Implementation Details} \label{section:2.1}
\textbf{3D Masked Multi-scale Autoencoder.} We implemented the 3D MAE using 3D-ResNetBlocks instead of Vision Transformer, different from the previous study \cite{he2022masked}, due to the constraint of GPU memory. The encoder consists of eight 3D-ResNetBlocks \cite{he2016deep,wolny2020accurate}, and each one includes 3 convolutional blocks with Group Normalization, 3D Conv layer, and ReLU activation. The input channel is 1, and the encoding dimension is 512. The spatial resolution is reduced by a factor of 4 in the first Conv layer and remains unchanged afterward. A lightweight decoder is appended to the encoder, which consists of a 3D TransConv layer that upsamples the input by a factor of 4 and reduces the embedding dimension from 512 to 32. The output is then fed into a single 3D-ResNetBlock same as the ones in the encoder, followed by a projection layer to generate the single channel output. The size of $x$ and $X$ is set as $96^3$ and batch size is set as 4 pairs of $x$ and $X$. The block size used for random maskout is set as 8 for $x$ and 4 for $X$, because global scan has a larger FOV than local patch. The mask ratio is set as 50\% during pretraining. Following the previous study \cite{he2022masked}, MSE calculated on masked regions is used as loss function. During DA training, the mask ratio for source domain is drawn from a unifrom distribution of [50\%, 75\%] while the mask ratio for target domain remains 50\%.

\textbf{Global-local collaborative network.} After MAE pretraining, the pretrained encoder serves as an encoder in the segmentation network. After encoding, the Global-local collaboration (Fig.\ref{fig2}.c) fuses the local and global features and forms a latent representation with a dimension of 1024. The concatenated feature is fed into a 3D ASPP layer \cite{chen2017rethinking} with an embedding dimension of 512. Afterward, the features go through a 3D TransConv layer that upsamples the spatial dimension by a factor of 4 and reduces the dimension from 512 to 64. Two subsequent 3D convolutional blocks are appended, and the last one reduces the dimension from 64 to the number of classes, providing the final prediction. The batch size during segmentation is set a 1 (in total of 4 patches, 2 from the source domain and 2 from the target domain). During training, each local sub-volume with size of $96^3$ is randomly sampled from global scan. During inference, the final output is formed by sliding window across entire volumetric scan. The details of training, computing infrastructure, visualization, and failure analysis can be found in the supplemental materials. 

\section{Experiments and Results}
\subsection{Datasets}
We included 2,524 structural brain MRI scans in this study. Among them, 2,409 are unannotated scans and are only involved in the 3D Masked Multi-scale Autoencoder pretraining. The data involved were acquired worldwide, and detailed descriptions can be found below. 

\textbf{ECHO}: The Environmental Influences on Child Health Outcomes (ECHO) Project recruited pregnant women receiving antenatal care at various sites. We included 138 T1w and 125 T2w brain MRI acquired at the University of Pittsburgh Medical Center (ECHO-Pittsburgh), 80 T1w and 53 T2w brain MRI acquired at the University of Rochester Medical Center (ECHO-Rochester), and 40 T2w brain MRI acquired at the New York State Psychiatric Institute (ECHO-NYSPI). All scans were acquired shortly after the birth. All three sites were approved by their Institutional Review Board and parents provided written informed consent forms. 
\textbf{Healthy Minds}: “Maternal Adversity, Inflammation, and Neurodevelopment: How Intergenerational Processes Perpetuate Disadvantage in a Low-Resource Setting” project ('Healthy Minds'), is a prospective cohort study at Hospital São Paulo - the Federal University of São Paulo (UNIFESP), Brazil. 
We included about 103 T2w MRI scans which were acquired shortly after birth. The Institutional Review Board of the UNIFESP approved all study procedures, and all parents provided written informed consent forms.

\textbf{Public Datasets:} We also included three publicly available infant brain MRI datasets. We included 426 T1w and 557 T2w MRI scans from the Developing Human Connectome Project (dHCP) V1.0.2 data release \cite{edwards2022developing}, 569 T1w and 423 T2w MRI scans from Baby Connectome Project (BCP) \cite{howell2019unc}, and 10 T2w MRI scans from Melbourne Children's Regional Infant Brain (M-CRIB) project \cite{alexander2017new}. All scans were preprocessed by skull stripping \cite{hoopes2022synthstrip} and bias-field correction \cite{tustison2010n4itk}.

\textbf{Annotations:} We obtained expert annotations of subcortical regions for 50 independent and longitudinal subjects, whose T1w and T2w MRI were co-registered. Seven subcortical regions including caudate, putamen, pallidum, thalamus, amygdala, hippocampus and accumbens, were manually annotated by Neuromorphometrics, Inc., Somerville, MA. These 50 subjects come from the BCP project with two subjects per month from 0 to 24 months. Similarly, 5 neonatal subjects from ECHO-NYSPI were annotated by the same rater. Additionally, the M-CRIB project provided 10 scans and expert annotations, which include the same seven subcortical regions. Using these three sets of data and annotations, we are able to validate our framework in three scenarios (i.e., cross-modality, cross-site, and cross-age). The detailed experiments and results can be found in the following subsections.  

\begin{table*}[b!]
    \caption{Performance comparisons among different methods. Average Dice coefficients across N subjects are reported.
    }
    \centering
    \begin{center}
        \resizebox{0.95\textwidth}{!}{%
            \begin{tabular}{c|cccccccc}
                \toprule
                \multicolumn{9}{c}{BCP T1w MRI (training\&validation, N=25) $\rightarrow$ BCP T2w MRI (test, N=25)}\\	
                \hline
                \multirow{2}{*}{Method} &\multicolumn{8}{c}{Dice(\%) $\uparrow$} \\
                \cline{2-9}
                &Hippocampus &Amygdala &Caudate &Putamen &Pallidum&Thalamus &Accumbens &Average \\
                
                \hline
                
                nn-UNet (supervised) \cite{isensee2021nnu}&11.05&10.09&41.63&7.61&0.64&10.36&0.23&11.66 \\
                
                Swin UNETR(supervised) \cite{hatamizadeh2022swin}&0&0.05&1.35&0.13&0&1.04&0.51&0.44\\
                
                \hline
                SFDA-DPL\cite{chen2021source} &35.19&29.38&36.16&21.19&0&56.18&0&25.44\\
                
                SynthSeg-V1~\cite{billot_synthseg_2023} &67.28&54.41&73.31&78.48&\textbf{72.08}&80.91&47.83&67.61\\		
                
                SynthSeg-robust~\cite{billot_robust_2023} &68.71&54.11&69.23&\textbf{78.88}&70.87&76.28&43.91&66.00\\
                
                MAPSeg (Ours) &\textbf{73.39}&\textbf{69.19}&\textbf{77.99}&78.39&69.72&\textbf{85.77}&\textbf{64.77}&\textbf{73.89}\\
                \bottomrule

                \multicolumn{9}{c}{BCP T2w MRI (training\&validation, N=50) $\rightarrow$ ECHO-NYSPI T2w MRI (test, N=5)}\\	
                \hline
                \multirow{2}{*}{Method} &\multicolumn{8}{c}{Dice(\%) $\uparrow$} \\
                \cline{2-9}
                &Hippocampus &Amygdala &Caudate &Putamen &Pallidum&Thalamus &Accumbens &Average \\
                
                \hline
                
                nn-UNet (supervised) \cite{isensee2021nnu}&38.32&8.21&81.69&67.39&29.49&86.88&12.96&46.42 \\
                
                Swin UNETR(supervised) \cite{hatamizadeh2022swin}&16.33&5.00&73.44&44.61&12.06&84.53&31.28&38.18\\
                
                \hline
                SFDA-DPL\cite{chen2021source} &0&0&52.71&24.17&0&66.86&0&20.53\\
                
                SynthSeg-V1~\cite{billot_synthseg_2023} &66.38&52.32&44.73&74.79&48.25&85.68&37.19&58.48\\		
                
                SynthSeg-robust~\cite{billot_robust_2023} &56.83&51.97&55.62&75.72&39.20&65.53&41.14&55.15\\
                
                MAPSeg (Ours) &\textbf{68.36}&\textbf{72.27}&\textbf{85.42}&\textbf{86.89}&\textbf{65.06}&\textbf{90.60}&\textbf{68.04}&\textbf{76.66}\\
                \bottomrule

        \end{tabular}}
    \end{center}
    \label{tab:crossmodality}
\end{table*}

\subsection{Results}
\textbf{Cross-modality Segmentation of Subcortical Regions. }We first evaluated MAPSeg in the cross-modality scenario, in which the model was first trained on T1w MRI (source domain) and then tested on T2w MRI (target domain). Cross-modality segmentation is one of the most challenging tasks in domain adaptive segmentation for neuroimaging. Using MRI scans from 50 BCP subjects, we randomly split them into two non-overlapping subsets of 25 subject. The model was trained on T1w MRI of one group (18 scans for training and 7 for validation). The best validation model was then applied to the test data (25 T2w scans of another group). The detailed results can be found in Table \ref{tab:crossmodality}.

\textbf{Cross-site Segmentation of Subcortical Regions.} Domain shift caused by cross-site scans is a common issue in neuroimaging studies, as large cohorts often recruit subjects from several sites, where scans may be acquired by different vendors, scanners, and sequences. We conducted experiments using three distinct cohorts with manual annotation (BCP, M-CRIB, and ECHO-NYSPI) to simulate a real-world cross-site scenario. In this scenario, a model is trained on a single site (BCP) and is applied to different sites (M-CRIB and ECHO-NYSPI). Utilizing 50 T2w MRI scans from BCP as the source domain, we randomly selected 40 scans for training and 10 for validation. We then tested the best validation model on two target domains, ECHO-NYSPI and M-CRIB. The results of ECHO-NYSPI can be found in Table \ref{tab:crossmodality}. The results of M-CRIB can be found in the supplemental materials.

\textbf{Cross-age Segmentation of Subcortical Regions.} We also conducted experiments in cross-age segmentation using longitudinal scans. The detailed results can be found in the supplemental materials. 

\textbf{Ablation studies.} We conducted an ablation study to investigate each component of our framework. In Table \ref{tab:ablation}, the results suggest that pretraining on large-scale, heterogeneous datasets is the cornerstone of the framework as it provides a more generalized encoder and therefore facilitates the pseudo-labeling training. And global-local collaboration is also beneficial to the performance as it brings in global distribution prior, and combining all three components will yield the optimal performance.

	\begin{table*}[t!]
		\caption{Ablation studies on cross-modality segmentation. 
		}
		\centering
		\begin{center}
			\resizebox{0.5\textwidth}{!}{%
				\begin{tabular}{cccccc|c}
					\toprule
					\hline
					\multicolumn{6}{c|}{Components}&\multicolumn{1}{c}{Performance}\\
                        \hline
					\cline{1-7}
					\multicolumn{2}{c|}{Pretraining} &\multicolumn{2}{c|}{Pseudo-label} &\multicolumn{2}{c|}{Global-local} &Dice(\%) $\uparrow$\\
					
					\hline
					\multicolumn{2}{c|}{} &\multicolumn{2}{c|}{} &\multicolumn{2}{c|}{} &24.98  \\
     \multicolumn{2}{c|}{\checkmark} &\multicolumn{2}{c|}{} &\multicolumn{2}{c|}{} &66.78 \\
     \multicolumn{2}{c|}{} &\multicolumn{2}{c|}{\checkmark} &\multicolumn{2}{c|}{} &38.14  \\
     \multicolumn{2}{c|}{\checkmark} &\multicolumn{2}{c|}{\checkmark} &\multicolumn{2}{c|}{} &69.90  \\
     \multicolumn{2}{c|}{\checkmark} &\multicolumn{2}{c|}{\checkmark} &\multicolumn{2}{c|}{\checkmark} &\textbf{73.89}  \\
					
					\bottomrule
			\end{tabular}}
		\end{center}
		\label{tab:ablation}
	\end{table*}
 
\section{Conclusions}
In this paper, we introduced a novel framework called \textbf{MAPSeg}, an unsupervised domain adaptive segmentation framework utilizing 3D masked autoencoding and pseudo-labeling. With extensive experiments on three challenging DA segmentation tasks for infant structural brain MRI, we have demonstrated that MAPSeg outperforms previous pseudo-label-based DA framework (i.e., SFDA-DPL) and domain generalization frameworks (i.e., SynthSeg). Our framework performed consistently well across all three domain shift tasks, which may suggest that, as in other fields (e.g., NLP and natural image vision), self-supervised learning can be a pathway to achieve generalizable artificial intelligence in medical imaging. There are several directions that could be explored in the future. One potential direction is to investigate different segmentation backbones in the MAPSeg framework, such as UNet, as it may be able to better preserve structural details than our DeepLabV3-like model. In addition, how to address class imbalance in the case of self-supervised learning should also be explored. 
%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{reference}
%
\pagebreak
\centering
\textbf{Supplemental Materials}
\begin{figure}
\centering
\includegraphics[width=200pt]{suppl_fig1-03.eps}
\caption{Architectures of different components within MAPSeg } \label{suppl.fig.1}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=0.67\textwidth]{vis-05.eps}
\caption{Visualization of outputs from different algorithms applied to cross-modality task.} \label{suppl.fig.2}
\end{figure}

\begin{figure}[t!]
\centering
\includegraphics[width=0.55\textwidth]{failure-06.eps}
\caption{Illustration of potential segmentation failures after proposed unsupervised domain adaptation. MAPSeg oversegments the pallidum region and undersegment the caudate region.} \label{suppl.fig.3}
\end{figure}






\begin{table*}
    \caption{Training details of 3D Masked Multi-scale Autoencoding and Pseudo-labeling
    }
    \centering
    \begin{center}
        \resizebox{\textwidth}{!}{%
            \begin{tabular}{c|c|c|c}
            \toprule
            \bottomrule
            \multirow{2}{*}{Parameters} &
            \multirow{2}{*}{3D Masked Multi-scale Autoencoding}&\multicolumn{2}{c}{3D Masked Pseudo-labeling} \\
            \cline{3-4}
            &&Source Domain&Target Domain\\
            \hline
            LR&$2e^{-4}$&\multicolumn{2}{c}{$1e^{-4}$}\\
            \hline
            LR Scheduler&CosineAnnealing&\multicolumn{2}{c}{CosineAnnealingWarmRestarts}\\
            \hline
            Epoch(steps)&300 (600000)&\multicolumn{2}{c}{100 (10000)}\\
            \hline
            Early Stop&N/A&\multicolumn{2}{c}{25 epochs}\\
            \hline
            \multirow{2}{*}{Data Augmentation}&\multirow{2}{*}{Scaling [0.75, 1.5], Rotation [-40,40]}&Scaling [0.7,1.4], Rotation [-30,30]&\multirow{2}{*}{Scaling [0.7,1.3], Rotation [-30,30]}\\
            &&Bias Field, Gamma Transform&\\
            \hline
            Augmentation Prob.&0.5&\multicolumn{2}{c}{0.35}\\
            \hline
            Package&\multicolumn{3}{c}{PyTorch 1.13}\\

            \hline
            GPU & \multicolumn{3}{c}{NVIDIA RTX A5000, 24GB VRAM}
        \end{tabular}}
    \end{center}
    \label{tab:Training}
\end{table*}

\begin{table*}[b!]
    \caption{Performance comparisons among different methods. Average Dice coefficients across N subjects are reported.
    }
    \centering
    \begin{center}
        \resizebox{\textwidth}{!}{%
            \begin{tabular}{c|cccccccc}
                \toprule
                \multicolumn{9}{c}{BCP T2w MRI 13-24 months (training\&validation, N=24) $\rightarrow$ BCP T2w MRI 0-6 months (test, N=14)}\\	
                \hline
                \multirow{2}{*}{Method} &\multicolumn{8}{c}{Dice(\%) $\uparrow$} \\
                \cline{2-9}
                &Hippocampus &Amygdala &Caudate &Putamen &Pallidum&Thalamus &Accumbens &Average \\
                
                \hline
                
                nn-UNet (supervised) &55.76&74.69&74.70&66.15&42.06&85.47&66.37&66.74 \\
                
                Swin UNETR(supervised) &48.64&73.24&73.81&59.81&43.31&88.82&56.68&63.47\\
                
                \hline
                SFDA-DPL &30.76&45.97&42.23&43.65&20.52&68.26&0&35.91\\
                
                SynthSeg-V1~ &63.98&55.70&48.75&72.41&51.23&71.10&44.36&58.22\\		
                
                SynthSeg-robust~ &63.45&53.89&47.86&\textbf{75.50}&51.21&65.10&33.30&55.76\\
                
                MAPSeg (Ours) &\textbf{71.95}&\textbf{76.58}&\textbf{80.56}&69.95&\textbf{59.44}&\textbf{91.24}&\textbf{67.89}&\textbf{73.94}\\
                \bottomrule

                \multicolumn{9}{c}{BCP T2w MRI (training\&validation, N=50) $\rightarrow$ M-CRIB T2w MRI (test, N=10)}\\	
                \hline
                \multirow{2}{*}{Method} &\multicolumn{8}{c}{Dice(\%) $\uparrow$} \\
                \cline{2-9}
                &Hippocampus &Amygdala &Caudate &Putamen &Pallidum&Thalamus &Accumbens &Average \\
                
                \hline
                
                nn-UNet (supervised) &59.32&5.41&80.44&81.77&44.73&83.46&57.84&59.00 \\
                
                Swin UNETR(supervised) &9.24&0.19&61.42&62.39&10.40&81.40&29.65&36.38\\
                
                \hline
                SFDA-DPL &2.52&0&64.39&57.02&29.75&79.45&0&33.30\\
                
                SynthSeg-V1~ &59.44&\textbf{53.39}&44.53&74.77&44.85&77.86&46.08&52.27\\		
                
                SynthSeg-robust&47.16&49.30&31.87&72.77&32.98&32.74&31.49&42.61\\
                
                MAPSeg (Ours) &\textbf{70.51}&48.95&\textbf{85.74}&\textbf{86.42}&\textbf{73.70}&\textbf{86.46}&\textbf{54.41}&\textbf{72.31}\\
                \bottomrule

        \end{tabular}}
    \end{center}
    \label{tab:Suppl.results}
\end{table*}


\end{document}
