@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@InProceedings{Du_2019_ICCV,
author = {Du, Liang and Tan, Jingang and Yang, Hongye and Feng, Jianfeng and Xue, Xiangyang and Zheng, Qibao and Ye, Xiaoqing and Zhang, Xiaolin},
title = {SSF-DAN: Separated Semantic Feature Based Domain Adaptation Network for Semantic Segmentation},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@inproceedings{NEURIPS2019_db9ad56c,
 author = {Zhao, Sicheng and Li, Bo and Yue, Xiangyu and Gu, Yang and Xu, Pengfei and Hu, Runbo and Chai, Hua and Keutzer, Kurt},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Multi-source Domain Adaptation for Semantic Segmentation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/db9ad56c71619aeed9723314d1456037-Paper.pdf},
 volume = {32},
 year = {2019}
}

@InProceedings{10.1007/978-3-030-58555-6_42,
author="Huang, Jiaxing
and Lu, Shijian
and Guan, Dayan
and Zhang, Xiaobing",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="Contextual-Relation Consistent Domain Adaptation for Semantic Segmentation",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="705--722",
abstract="Recent advances in unsupervised domain adaptation for semantic segmentation have shown great potentials to relieve the demand of expensive per-pixel annotations. However, most existing works address the domain discrepancy by aligning the data distributions of two domains at a global image level whereas the local consistencies are largely neglected. This paper presents an innovative local contextual-relation consistent domain adaptation (CrCDA) technique that aims to achieve local-level consistencies during the global-level alignment. The idea is to take a closer look at region-wise feature representations and align them for local-level consistencies. Specifically, CrCDA learns and enforces the prototypical local contextual-relations explicitly in the feature space of a labelled source domain while transferring them to an unlabelled target domain via backpropagation-based adversarial learning. An adaptive entropy max-min adversarial learning scheme is designed to optimally align these hundreds of local contextual-relations across domain without requiring discriminator or extra computation overhead. The proposed CrCDA has been evaluated extensively over two challenging domain adaptive segmentation tasks (e.g., GTA5 {\$}{\$}{\backslash}rightarrow {\$}{\$}{\textrightarrow}Cityscapes and SYNTHIA {\$}{\$}{\backslash}rightarrow {\$}{\$}{\textrightarrow}Cityscapes), and experiments demonstrate its superior segmentation performance as compared with state-of-the-art methods.",
isbn="978-3-030-58555-6"
}

@InProceedings{Pathak_2016_CVPR,
author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A.},
title = {Context Encoders: Feature Learning by Inpainting},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}
@InProceedings{pmlr-v119-henaff20a,
  title = 	 {Data-Efficient Image Recognition with Contrastive Predictive Coding},
  author =       {Henaff, Olivier},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {4182--4192},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/henaff20a/henaff20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/henaff20a.html},
  abstract = 	 {Human observers can learn to recognize new categories of images from a handful of examples, yet doing so with artificial ones remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable. We therefore revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations. This new implementation produces features which support state-of-the-art linear classification accuracy on the ImageNet dataset. When used as input for non-linear classification with deep neural networks, this representation allows us to use 2-5x less labels than classifiers trained directly on image pixels. Finally, this unsupervised representation substantially improves transfer learning to object detection on the PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet classifiers.}
}

@InProceedings{pmlr-v119-chen20s,
  title = 	 {Generative Pretraining From Pixels},
  author =       {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {1691--1703},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/chen20s/chen20s.pdf},
  url = 	 {https://proceedings.mlr.press/v119/chen20s.html},
  abstract = 	 {Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full fine-tuning, matching the top supervised pre-trained models. We are also competitive with self-supervised benchmarks on ImageNet when substituting pixels for a VQVAE encoding, achieving 69.0% top-1 accuracy on a linear probe of our features.}
}


@InProceedings{Doersch_2015_ICCV,
author = {Doersch, Carl and Gupta, Abhinav and Efros, Alexei A.},
title = {Unsupervised Visual Representation Learning by Context Prediction},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {December},
year = {2015}
}
@InProceedings{Kong_2023_CVPR,
    author    = {Kong, Xiangwen and Zhang, Xiangyu},
    title     = {Understanding Masked Image Modeling via Learning Occlusion Invariant Feature},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {6241-6251}
}
@InProceedings{Xie_2023_CVPRa,
    author    = {Xie, Zhenda and Geng, Zigang and Hu, Jingcheng and Zhang, Zheng and Hu, Han and Cao, Yue},
    title     = {Revealing the Dark Secrets of Masked Image Modeling},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {14475-14485}
}
@InProceedings{Xie_2023_CVPRb,
    author    = {Xie, Zhenda and Zhang, Zheng and Cao, Yue and Lin, Yutong and Wei, Yixuan and Dai, Qi and Hu, Han},
    title     = {On Data Scaling in Masked Image Modeling},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {10365-10374}
}
@InProceedings{10.1007/978-3-031-20056-4_18,
author="Kakogeorgiou, Ioannis
and Gidaris, Spyros
and Psomas, Bill
and Avrithis, Yannis
and Bursuc, Andrei
and Karantzalos, Konstantinos
and Komodakis, Nikos",
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="What to Hide from Your Students: Attention-Guided Masked Image Modeling",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="300--318",
abstract="Transformers and masked language modeling are quickly being adopted and explored in computer vision as vision transformers and masked image modeling (MIM). In this work, we argue that image token masking differs from token masking in text, due to the amount and correlation of tokens in an image. In particular, to generate a challenging pretext task for MIM, we advocate a shift from random masking to informed masking. We develop and exhibit this idea in the context of distillation-based MIM, where a teacher transformer encoder generates an attention map, which we use to guide masking for the student.",
isbn="978-3-031-20056-4"
}

@InProceedings{Xie_2022_CVPR,
    author    = {Xie, Zhenda and Zhang, Zheng and Cao, Yue and Lin, Yutong and Bao, Jianmin and Yao, Zhuliang and Dai, Qi and Hu, Han},
    title     = {SimMIM: A Simple Framework for Masked Image Modeling},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {9653-9663}
}
@InProceedings{He_2022_CVPR,
    author    = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll\'ar, Piotr and Girshick, Ross},
    title     = {Masked Autoencoders Are Scalable Vision Learners},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {16000-16009}
}
@InProceedings{Tang_2022_CVPRb,
    author    = {Tang, Yucheng and Yang, Dong and Li, Wenqi and Roth, Holger R. and Landman, Bennett and Xu, Daguang and Nath, Vishwesh and Hatamizadeh, Ali},
    title     = {Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image Analysis},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {20730-20740}
}

@inproceedings{lee2013pseudo,
  title={Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks},
  author={Lee, Dong-Hyun and others},
  booktitle={Workshop on challenges in representation learning, ICML},
  volume={3},
  number={2},
  pages={896},
  year={2013},
  organization={Atlanta}
}
@article{laine2016temporal,
  title={Temporal ensembling for semi-supervised learning},
  author={Laine, Samuli and Aila, Timo},
  journal={arXiv preprint arXiv:1610.02242},
  year={2016}
}
@inproceedings{NIPS2016_30ef30b6,
 author = {Sajjadi, Mehdi and Javanmardi, Mehran and Tasdizen, Tolga},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/30ef30b64204a3088a26bc2e6ecf7602-Paper.pdf},
 volume = {29},
 year = {2016}
}

@InProceedings{Hu_2021_CVPR,
    author    = {Hu, Zijian and Yang, Zhengyu and Hu, Xuefeng and Nevatia, Ram},
    title     = {SimPLE: Similar Pseudo Label Exploitation for Semi-Supervised Classification},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {15099-15108}
}
@InProceedings{Petrovai_2022_CVPR,
    author    = {Petrovai, Andra and Nedevschi, Sergiu},
    title     = {Exploiting Pseudo Labels in a Self-Supervised Learning Framework for Improved Monocular Depth Estimation},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {1578-1588}
}
@InProceedings{Hoyer_2023_CVPR,
    author    = {Hoyer, Lukas and Dai, Dengxin and Wang, Haoran and Van Gool, Luc},
    title     = {MIC: Masked Image Consistency for Context-Enhanced Domain Adaptation},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {11721-11732}
}

@InProceedings{Zhang_2021_CVPR,
    author    = {Zhang, Pan and Zhang, Bo and Zhang, Ting and Chen, Dong and Wang, Yong and Wen, Fang},
    title     = {Prototypical Pseudo Label Denoising and Target Structure Learning for Domain Adaptive Semantic Segmentation},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {12414-12424}
}

@InProceedings{pmlr-v37-long15,
  title = 	 {Learning Transferable Features with Deep Adaptation Networks},
  author = 	 {Long, Mingsheng and Cao, Yue and Wang, Jianmin and Jordan, Michael},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {97--105},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/long15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/long15.html},
  abstract = 	 {Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks.}
}

@inproceedings{NIPS2016_ac627ab1,
 author = {Long, Mingsheng and Zhu, Han and Wang, Jianmin and Jordan, Michael I},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Unsupervised Domain Adaptation with Residual Transfer Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/ac627ab1ccbdb62ec96e702f07f6425b-Paper.pdf},
 volume = {29},
 year = {2016}
}

@inproceedings{NIPS2004_96f2b50b,
 author = {Grandvalet, Yves and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {L. Saul and Y. Weiss and L. Bottou},
 pages = {},
 publisher = {MIT Press},
 title = {Semi-supervised Learning by Entropy Minimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2004/file/96f2b50b5d3613adf9c27049b2a888c7-Paper.pdf},
 volume = {17},
 year = {2004}
}

@inproceedings{NIPS2017_68053af2,
 author = {Tarvainen, Antti and Valpola, Harri},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/68053af2923e00204c3ca7c6a3150cf7-Paper.pdf},
 volume = {30},
 year = {2017}
}


@article{grill2020bootstrap,
  title={Bootstrap your own latent-a new approach to self-supervised learning},
  author={Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21271--21284},
  year={2020}
}

@article{chen2017rethinking,
  title={Rethinking atrous convolution for semantic image segmentation},
  author={Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  journal={arXiv preprint arXiv:1706.05587},
  year={2017}
}

@InProceedings{Chen_2019_CVPR,
author = {Chen, Wuyang and Jiang, Ziyu and Wang, Zhangyang and Cui, Kexin and Qian, Xiaoning},
title = {Collaborative Global-Local Networks for Memory-Efficient Segmentation of Ultra-High Resolution Images},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}
@article{ZHUANG201677,
title = {Multi-scale patch and multi-modality atlases for whole heart segmentation of MRI},
journal = {Medical Image Analysis},
volume = {31},
pages = {77-87},
year = {2016},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2016.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S1361841516000219},
author = {Xiahai Zhuang and Juan Shen},
keywords = {Multi-scale patch, Whole heart segmentation, Multi-modality atlas, Local atlas ranking},
abstract = {A whole heart segmentation (WHS) method is presented for cardiac MRI. This segmentation method employs multi-modality atlases from MRI and CT and adopts a new label fusion algorithm which is based on the proposed multi-scale patch (MSP) strategy and a new global atlas ranking scheme. MSP, developed from the scale-space theory, uses the information of multi-scale images and provides different levels of the structural information of images for multi-level local atlas ranking. Both the local and global atlas ranking steps use the information theoretic measures to compute the similarity between the target image and the atlases from multiple modalities. The proposed segmentation scheme was evaluated on a set of data involving 20 cardiac MRI and 20 CT images. Our proposed algorithm demonstrated a promising performance, yielding a mean WHS Dice score of 0.899 ± 0.0340, Jaccard index of 0.818 ± 0.0549, and surface distance error of 1.09 ± 1.11 mm for the 20 MRI data. The average runtime for the proposed label fusion was 12.58 min.}
}
@ARTICLE{8458220,
  author={Zhuang, Xiahai},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Multivariate Mixture Model for Myocardial Segmentation Combining Multi-Source Images}, 
  year={2019},
  volume={41},
  number={12},
  pages={2933-2946},
  doi={10.1109/TPAMI.2018.2869576}}

@ARTICLE{9965747,
  author={Luo, Xinzhe and Zhuang, Xiahai},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={$\mathcal {X}$-Metric: An N-Dimensional Information-Theoretic Framework for Groupwise Registration and Deep Combined Computing}, 
  year={2023},
  volume={45},
  number={7},
  pages={9206-9224},
  doi={10.1109/TPAMI.2022.3225418}}

@article{gilmore2018imaging,
  title={Imaging structural and functional brain development in early childhood},
  author={Gilmore, John H and Knickmeyer, Rebecca C and Gao, Wei},
  journal={Nature Reviews Neuroscience},
  volume={19},
  number={3},
  pages={123--137},
  year={2018},
  publisher={Nature Publishing Group}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{hatamizadeh2022swin,
  title={Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images},
  author={Hatamizadeh, Ali and Nath, Vishwesh and Tang, Yucheng and Yang, Dong and Roth, Holger R and Xu, Daguang},
  booktitle={Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 7th International Workshop, BrainLes 2021, Held in Conjunction with MICCAI 2021, Virtual Event, September 27, 2021, Revised Selected Papers, Part I},
  pages={272--284},
  year={2022},
  organization={Springer}
}

@article{howell2019unc,
  title={The UNC/UMN Baby Connectome Project (BCP): An overview of the study design and protocol development},
  author={Howell, Brittany R and Styner, Martin A and Gao, Wei and Yap, Pew-Thian and Wang, Li and Baluyot, Kristine and Yacoub, Essa and Chen, Geng and Potts, Taylor and Salzwedel, Andrew and others},
  journal={NeuroImage},
  volume={185},
  pages={891--905},
  year={2019},
  publisher={Elsevier}
}

@article{edwards2022developing,
  title={The developing human connectome project neonatal data release},
  author={Edwards, A David and Rueckert, Daniel and Smith, Stephen M and Seada, Samy Abo and Alansary, Amir and Almalbis, Jennifer and Allsop, Joanna and Andersson, Jesper and Arichi, Tomoki and Arulkumaran, Sophie and others},
  journal={Frontiers in neuroscience},
  volume={16},
  year={2022},
  publisher={Frontiers Media SA}
}
@ARTICLE{10261458,
  author={Liu, Shaolei and Yin, Siqi and Qu, Linhao and Wang, Manning and Song, Zhijian},
  journal={IEEE Transactions on Medical Imaging}, 
  title={A Structure-aware Framework of Unsupervised Cross-Modality Domain Adaptation via Frequency and Spatial Knowledge Distillation}, 
  year={2023},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TMI.2023.3318006}}

@article{alexander2017new,
  title={A new neonatal cortical and subcortical brain atlas: the Melbourne Children's Regional Infant Brain (M-CRIB) atlas},
  author={Alexander, Bonnie and Murray, Andrea L and Loh, Wai Yen and Matthews, Lillian G and Adamson, Chris and Beare, Richard and Chen, Jian and Kelly, Claire E and Rees, Sandra and Warfield, Simon K and others},
  journal={Neuroimage},
  volume={147},
  pages={841--851},
  year={2017},
  publisher={Elsevier}
}

@article{hoopes2022synthstrip,
  title={SynthStrip: skull-stripping for any brain image},
  author={Hoopes, Andrew and Mora, Jocelyn S and Dalca, Adrian V and Fischl, Bruce and Hoffmann, Malte},
  journal={NeuroImage},
  volume={260},
  pages={119474},
  year={2022},
  publisher={Elsevier}
}

@article{tustison2010n4itk,
  title={N4ITK: improved N3 bias correction},
  author={Tustison, Nicholas J and Avants, Brian B and Cook, Philip A and Zheng, Yuanjie and Egan, Alexander and Yushkevich, Paul A and Gee, James C},
  journal={IEEE transactions on medical imaging},
  volume={29},
  number={6},
  pages={1310--1320},
  year={2010},
  publisher={IEEE}
}

@article{cruz2023cortical,
  title={Cortical-subcortical interactions in goal-directed behavior},
  author={Cruz, K Guadalupe and Leow, Yi Ning and Le, Nhat Minh and Adam, Elie and Huda, Rafiq and Sur, Mriganka},
  journal={Physiological reviews},
  volume={103},
  number={1},
  pages={347--389},
  year={2023},
  publisher={American Physiological Society Rockville, MD}
}

@article{shen2022subcortical,
  title={Subcortical brain development in autism and fragile X syndrome: evidence for dynamic, age-and disorder-specific trajectories in infancy},
  author={Shen, Mark D and Swanson, Meghan R and Wolff, Jason J and Elison, Jed T and Girault, Jessica B and Kim, Sun Hyung and Smith, Rachel G and Graves, Michael M and Weisenfeld, Leigh Anne H and Flake, Lisa and others},
  journal={American Journal of Psychiatry},
  volume={179},
  number={8},
  pages={562--572},
  year={2022},
  publisher={Am Psychiatric Assoc}
}

@article{hazlett2017early,
  title={Early brain development in infants at high risk for autism spectrum disorder},
  author={Hazlett, Heather Cody and Gu, Hongbin and Munsell, Brent C and Kim, Sun Hyung and Styner, Martin and Wolff, Jason J and Elison, Jed T and Swanson, Meghan R and Zhu, Hongtu and Botteron, Kelly N and others},
  journal={Nature},
  volume={542},
  number={7641},
  pages={348--351},
  year={2017},
  publisher={Nature Publishing Group UK London}
}

@article{baribeau2019structural,
  title={Structural neuroimaging correlates of social deficits are similar in autism spectrum disorder and attention-deficit/hyperactivity disorder: analysis from the POND Network},
  author={Baribeau, Danielle A and Dupuis, Annie and Paton, Tara A and Hammill, Christopher and Scherer, Stephen W and Schachar, Russell J and Arnold, Paul D and Szatmari, Peter and Nicolson, Rob and Georgiades, Stelios and others},
  journal={Translational psychiatry},
  volume={9},
  number={1},
  pages={72},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@article{wolny2020accurate,
  title={Accurate and versatile 3D segmentation of plant tissues at cellular resolution},
  author={Wolny, Adrian and Cerrone, Lorenzo and Vijayan, Athul and Tofanelli, Rachele and Barro, Amaya Vilches and Louveaux, Marion and Wenzl, Christian and Strauss, S{\"o}ren and Wilson-S{\'a}nchez, David and Lymbouridou, Rena and others},
  journal={Elife},
  volume={9},
  pages={e57613},
  year={2020},
  publisher={eLife Sciences Publications, Ltd}
}

@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@InProceedings{Tang_2022_CVPR,
    author    = {Tang, Minxue and Ning, Xuefei and Wang, Yitu and Sun, Jingwei and Wang, Yu and Li, Hai and Chen, Yiran},
    title     = {FedCor: Correlation-Based Active Client Selection Strategy for Heterogeneous Federated Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {10102-10111}
}

@inproceedings{fedmask,
author = {Li, Ang and Sun, Jingwei and Zeng, Xiao and Zhang, Mi and Li, Hai and Chen, Yiran},
title = {FedMask: Joint Computation and Communication-Efficient Personalized Federated Learning via Heterogeneous Masking},
year = {2021},
isbn = {9781450390972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485730.3485929},
doi = {10.1145/3485730.3485929},
abstract = {Recent advancements in deep neural networks (DNN) enabled various mobile deep learning applications. However, it is technically challenging to locally train a DNN model due to limited data on devices like mobile phones. Federated learning (FL) is a distributed machine learning paradigm which allows for model training on decentralized data residing on devices without breaching data privacy. Hence, FL becomes a natural choice for deploying on-device deep learning applications. However, the data residing across devices is intrinsically statistically heterogeneous (i.e., non-IID data distribution) and mobile devices usually have limited communication bandwidth to transfer local updates. Such statistical heterogeneity and communication bandwidth limit are two major bottlenecks that hinder applying FL in practice. In addition, considering mobile devices usually have limited computational resources, improving computation efficiency of training and running DNNs is critical to developing on-device deep learning applications. In this paper, we present FedMask - a communication and computation efficient FL framework. By applying FedMask, each device can learn a personalized and structured sparse DNN, which can run efficiently on devices. To achieve this, each device learns a sparse binary mask (i.e., 1 bit per network parameter) while keeping the parameters of each local model unchanged; only these binary masks will be communicated between the server and the devices. Instead of learning a shared global model in classic FL, each device obtains a personalized and structured sparse model that is composed by applying the learned binary mask to the fixed parameters of the local model. Our experiments show that compared with status quo approaches, FedMask improves the inference accuracy by 28.47\% and reduces the communication cost and the computation cost by 34.48X and 2.44X. FedMask also achieves 1.56X inference speedup and reduces the energy consumption by 1.78X.},
booktitle = {Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems},
pages = {42–55},
numpages = {14},
keywords = {Personalization, On-device AI, Efficient federated learning systems, Data heterogeneity},
location = {Coimbra, Portugal},
series = {SenSys '21}
}

@inproceedings{
Peng2020Federated,
title={Federated Adversarial Domain Adaptation},
author={Xingchao Peng and Zijun Huang and Yizhe Zhu and Kate Saenko},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HJezF3VYPB}
}

@article{MAL-083,
url = {http://dx.doi.org/10.1561/2200000083},
year = {2021},
volume = {14},
journal = {Foundations and Trends® in Machine Learning},
title = {Advances and Open Problems in Federated Learning},
doi = {10.1561/2200000083},
issn = {1935-8237},
number = {1–2},
pages = {1-210},
author = {Peter Kairouz and H. Brendan McMahan and Brendan Avent and Aurélien Bellet and Mehdi Bennis and Arjun Nitin Bhagoji and Kallista Bonawitz and Zachary Charles and Graham Cormode and Rachel Cummings and Rafael G. L. D’Oliveira and Hubert Eichner and Salim El Rouayheb and David Evans and Josh Gardner and Zachary Garrett and Adrià Gascón and Badih Ghazi and Phillip B. Gibbons and Marco Gruteser and Zaid Harchaoui and Chaoyang He and Lie He and Zhouyuan Huo and Ben Hutchinson and Justin Hsu and Martin Jaggi and Tara Javidi and Gauri Joshi and Mikhail Khodak and Jakub Konecný and Aleksandra Korolova and Farinaz Koushanfar and Sanmi Koyejo and Tancrède Lepoint and Yang Liu and Prateek Mittal and Mehryar Mohri and Richard Nock and Ayfer Özgür and Rasmus Pagh and Hang Qi and Daniel Ramage and Ramesh Raskar and Mariana Raykova and Dawn Song and Weikang Song and Sebastian U. Stich and Ziteng Sun and Ananda Theertha Suresh and Florian Tramèr and Praneeth Vepakomma and Jianyu Wang and Li Xiong and Zheng Xu and Qiang Yang and Felix X. Yu and Han Yu and Sen Zhao}
}

@article{insight_from_GDPR,
  author       = {Nguyen Binh Truong and
                  Kai Sun and
                  Siyao Wang and
                  Florian Guitton and
                  Yike Guo},
  title        = {Privacy Preservation in Federated Learning: Insights from the {GDPR}
                  Perspective},
  journal      = {CoRR},
  volume       = {abs/2011.05411},
  year         = {2020},
  url          = {https://arxiv.org/abs/2011.05411},
  eprinttype    = {arXiv},
  eprint       = {2011.05411},
  timestamp    = {Mon, 28 Dec 2020 18:21:18 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2011-05411.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{guan2023federated,
  author       = {Hao Guan and
                  Mingxia Liu},
  title        = {Federated Learning for Medical Image Analysis: {A} Survey},
  journal      = {CoRR},
  volume       = {abs/2306.05980},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2306.05980},
  doi          = {10.48550/ARXIV.2306.05980},
  eprinttype    = {arXiv},
  eprint       = {2306.05980},
  timestamp    = {Wed, 14 Jun 2023 13:17:00 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2306-05980.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{9741336,
  author={Yao, Kai and Su, Zixian and Huang, Kaizhu and Yang, Xi and Sun, Jie and Hussain, Amir and Coenen, Frans},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={A Novel 3D Unsupervised Domain Adaptation Framework for Cross-Modality Medical Image Segmentation}, 
  year={2022},
  volume={26},
  number={10},
  pages={4976-4986},
  doi={10.1109/JBHI.2022.3162118}}

@INPROCEEDINGS {8954439,
author = {T. Vu and H. Jain and M. Bucher and M. Cord and P. Perez},
booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation},
year = {2019},
volume = {},
issn = {},
pages = {2512-2521},
abstract = {Semantic segmentation is a key problem for many computer vision tasks. While approaches based on convolutional neural networks constantly break new records on different benchmarks, generalizing well to diverse testing environments remains a major challenge. In numerous real-world applications, there is indeed a large gap between data distributions in train and test domains, which results in severe performance loss at run-time. In this work, we address the task of unsupervised domain adaptation in semantic segmentation with losses based on the entropy of the pixel-wise predictions. To this end, we propose two novel, complementary methods using (i) entropy loss and (ii) adversarial loss respectively. We demonstrate state-of-the-art performance in semantic segmentation on two challenging “synthetic-2-real” set-ups and show that the approach can also be used for detection.},
keywords = {computer vision;adaptation models;computational modeling;semantics;object detection;benchmark testing;minimization},
doi = {10.1109/CVPR.2019.00262},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2019.00262},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@ARTICLE{8764342,
  author={Dou, Qi and Ouyang, Cheng and Chen, Cheng and Chen, Hao and Glocker, Ben and Zhuang, Xiahai and Heng, Pheng-Ann},
  journal={IEEE Access}, 
  title={PnP-AdaNet: Plug-and-Play Adversarial Domain Adaptation Network at Unpaired Cross-Modality Cardiac Segmentation}, 
  year={2019},
  volume={7},
  number={},
  pages={99065-99076},
  doi={10.1109/ACCESS.2019.2929258}}

@InProceedings{Liu_2021_CVPR,
    author    = {Liu, Yuang and Zhang, Wei and Wang, Jun},
    title     = {Source-Free Domain Adaptation for Semantic Segmentation},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {1215-1224}
}

@inproceedings{10.1145/2939672.2939716,
author = {Chidlovskii, Boris and Clinchant, Stephane and Csurka, Gabriela},
title = {Domain Adaptation in the Absence of Source Domain Data},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939716},
doi = {10.1145/2939672.2939716},
abstract = {The overwhelming majority of existing domain adaptation methods makes an assumption of freely available source domain data. An equal access to both source and target data makes it possible to measure the discrepancy between their distributions and to build representations common to both target and source domains. In reality, such a simplifying assumption rarely holds, since source data are routinely a subject of legal and contractual constraints between data owners and data customers. When source domain data can not be accessed, decision making procedures are often available for adaptation nevertheless. These procedures are often presented in the form of classification, identification, ranking etc. rules trained on source data and made ready for a direct deployment and later reuse. In other cases, the owner of a source data is allowed to share a few representative examples such as class means. In this paper we address the domain adaptation problem in real world applications, where the reuse of source domain data is limited to classification rules or a few representative examples. We extend the recent techniques of feature corruption and their marginalization, both in supervised and unsupervised settings. We test and compare them on private and publicly available source datasets and show that significant performance gains can be achieved despite the absence of source data and shortage of labeled target data.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {451–460},
numpages = {10},
keywords = {marginalization, domain adaptation, machine learning, emerging applications, classification},
location = {San Francisco, California, USA},
series = {KDD '16}
}


@InProceedings{pmlr-v80-hoffman18a,
  title = 	 {{C}y{CADA}: Cycle-Consistent Adversarial Domain Adaptation},
  author =       {Hoffman, Judy and Tzeng, Eric and Park, Taesung and Zhu, Jun-Yan and Isola, Phillip and Saenko, Kate and Efros, Alexei and Darrell, Trevor},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1989--1998},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/hoffman18a/hoffman18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/hoffman18a.html},
  abstract = 	 {Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models have shown tremendous progress towards adapting to new environments by focusing either on discovering domain invariant representations or by mapping between unpaired image domains. While feature space methods are difficult to interpret and sometimes fail to capture pixel-level and low-level domain shifts, image space methods sometimes fail to incorporate high level semantic knowledge relevant for the end task. We propose a model which adapts between domains using both generative image space alignment and latent representation space alignment. Our approach, Cycle-Consistent Adversarial Domain Adaptation (CyCADA), guides transfer between domains according to a specific discriminatively trained task and avoids divergence by enforcing consistency of the relevant semantics before and after adaptation. We evaluate our method on a variety of visual recognition and prediction settings, including digit classification and semantic segmentation of road scenes, advancing state-of-the-art performance for unsupervised adaptation from synthetic to real world driving domains.}
}

@InProceedings{Zou_2019_ICCV,
author = {Zou, Yang and Yu, Zhiding and Liu, Xiaofeng and Kumar, B.V.K. Vijaya and Wang, Jinsong},
title = {Confidence Regularized Self-Training},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@article{Chen_Dou_Chen_Qin_Heng_2019, title={Synergistic Image and Feature Adaptation: Towards Cross-Modality Domain Adaptation for Medical Image Segmentation}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/3874}, DOI={10.1609/aaai.v33i01.3301865}, abstractNote={&lt;p&gt;This paper presents a novel unsupervised domain adaptation framework, called &lt;em&gt;Synergistic Image and Feature Adaptation (SIFA)&lt;/em&gt;, to effectively tackle the problem of domain shift. Domain adaptation has become an important and hot topic in recent studies on deep learning, aiming to recover performance degradation when applying the neural networks to new testing domains. Our proposed SIFA is an elegant learning diagram which presents synergistic fusion of adaptations from both image and feature perspectives. In particular, we simultaneously transform the appearance of images across domains and enhance domain-invariance of the extracted features towards the segmentation task. The feature encoder layers are shared by both perspectives to grasp their mutual benefits during the end-to-end learning procedure. Without using any annotation from the target domain, the learning of our unified model is guided by adversarial losses, with multiple discriminators employed from various aspects. We have extensively validated our method with a challenging application of crossmodality medical image segmentation of cardiac structures. Experimental results demonstrate that our SIFA model recovers the degraded performance from 17.2% to 73.0%, and outperforms the state-of-the-art methods by a significant margin.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Chen, Cheng and Dou, Qi and Chen, Hao and Qin, Jing and Heng, Pheng-Ann}, year={2019}, month={Jul.}, pages={865-872} }
@article{zheng2021rectifying,
  title={Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation},
  author={Zheng, Zhedong and Yang, Yi},
  journal={International Journal of Computer Vision},
  volume={129},
  number={4},
  pages={1106--1120},
  year={2021},
  publisher={Springer}
}
@ARTICLE{8988158,
  author={Chen, Cheng and Dou, Qi and Chen, Hao and Qin, Jing and Heng, Pheng Ann},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Unsupervised Bidirectional Cross-Modality Adaptation via Deeply Synergistic Image and Feature Alignment for Medical Image Segmentation}, 
  year={2020},
  volume={39},
  number={7},
  pages={2494-2505},
  doi={10.1109/TMI.2020.2972701}}

@InProceedings{daformer,
    author    = {Hoyer, Lukas and Dai, Dengxin and Van Gool, Luc},
    title     = {DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {9924-9935}
}

@InProceedings{hrda,
author="Hoyer, Lukas
and Dai, Dengxin
and Van Gool, Luc",
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="372--391",
abstract="Unsupervised domain adaptation (UDA) aims to adapt a model trained on the source domain (e.g. synthetic data) to the target domain (e.g. real-world data) without requiring further annotations on the target domain. This work focuses on UDA for semantic segmentation as real-world pixel-wise annotations are particularly expensive to acquire. As UDA methods for semantic segmentation are usually GPU memory intensive, most previous methods operate only on downscaled images. We question this design as low-resolution predictions often fail to preserve fine details. The alternative of training with random crops of high-resolution images alleviates this problem but falls short in capturing long-range, domain-robust context information. Therefore, we propose HRDA, a multi-resolution training approach for UDA, that combines the strengths of small high-resolution crops to preserve fine segmentation details and large low-resolution crops to capture long-range context dependencies with a learned scale attention, while maintaining a manageable GPU memory footprint. HRDA enables adapting small objects and preserving fine segmentation details. It significantly improves the state-of-the-art performance by 5.5 mIoU for GTA{\$}{\$}{\backslash}rightarrow {\$}{\$}{\textrightarrow}Cityscapes and 4.9 mIoU for Synthia{\$}{\$}{\backslash}rightarrow {\$}{\$}{\textrightarrow}Cityscapes, resulting in unprecedented 73.8 and 65.8 mIoU, respectively. The implementation is available at github.com/lhoyer/HRDA.",
isbn="978-3-031-20056-4"
}

@ARTICLE{10273225,
  author={Ji, Wen and Chung, Albert C. S.},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Unsupervised Domain Adaptation for Medical Image Segmentation Using Transformer With Meta Attention}, 
  year={2023},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TMI.2023.3322581}}

@article{Feng_Ju_Wang_Song_Zhao_Ge_2023, title={Unsupervised Domain Adaptation for Medical Image Segmentation by Selective Entropy Constraints and Adaptive Semantic Alignment}, volume={37}, url={https://ojs.aaai.org/index.php/AAAI/article/view/25138}, DOI={10.1609/aaai.v37i1.25138}, abstractNote={Generalizing a deep learning model to new domains is crucial for computer-aided medical diagnosis systems. Most existing unsupervised domain adaptation methods have made significant progress in reducing the domain distribution gap through adversarial training. However, these methods may still produce overconfident but erroneous results on unseen target images. This paper proposes a new unsupervised domain adaptation framework for cross-modality medical image segmentation. Specifically, We first introduce two data augmentation approaches to generate two sets of semantics-preserving augmented images. Based on the model’s predictive consistency on these two sets of augmented images, we identify reliable and unreliable pixels. We then perform a selective entropy constraint: we minimize the entropy of reliable pixels to increase their confidence while maximizing the entropy of unreliable pixels to reduce their confidence. Based on the identified reliable and unreliable pixels, we further propose an adaptive semantic alignment module which performs class-level distribution adaptation by minimizing the distance between same class prototypes between domains, where unreliable pixels are removed to derive more accurate prototypes. We have conducted extensive experiments on the cross-modality cardiac structure segmentation task. The experimental results show that the proposed method significantly outperforms the state-of-the-art comparison algorithms. Our code and data are available at https://github.com/fengweie/SE_ASA.}, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Feng, Wei and Ju, Lie and Wang, Lin and Song, Kaimin and Zhao, Xin and Ge, Zongyuan}, year={2023}, month={Jun.}, pages={623-631} }

@inproceedings{DBLP:conf/isbi/MushtaqBDA23,
  author       = {Erum Mushtaq and
                  Yavuz Faruk Bakman and
                  Jie Ding and
                  Salman Avestimehr},
  title        = {Federated Alternate Training (Fat): Leveraging Unannotated Data Silos
                  in Federated Segmentation for Medical Imaging},
  booktitle    = {20th {IEEE} International Symposium on Biomedical Imaging, {ISBI}
                  2023, Cartagena, Colombia, April 18-21, 2023},
  pages        = {1--5},
  publisher    = {{IEEE}},
  year         = {2023},
  url          = {https://doi.org/10.1109/ISBI53787.2023.10230533},
  doi          = {10.1109/ISBI53787.2023.10230533},
  timestamp    = {Wed, 04 Oct 2023 17:01:25 +0200},
  biburl       = {https://dblp.org/rec/conf/isbi/MushtaqBDA23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{fmtda,
  author={Yao, Chun-Han and Gong, Boqing and Qi, Hang and Cui, Yin and Zhu, Yukun and Yang, Ming-Hsuan},
  booktitle={2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, 
  title={Federated Multi-Target Domain Adaptation}, 
  year={2022},
  volume={},
  number={},
  pages={1081-1090},
  doi={10.1109/WACV51458.2022.00115}
}

@inproceedings{peng2019moment,
  title={Moment matching for multi-source domain adaptation},
  author={Peng, Xingchao and Bai, Qinxun and Xia, Xide and Huang, Zijun and Saenko, Kate and Wang, Bo},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1406--1415},
  year={2019}
}

@INPROCEEDINGS {cross_city,
author = {Y. Chen and W. Chen and Y. Chen and B. Tsai and Y. Wang and M. Sun},
booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
title = {No More Discrimination: Cross City Adaptation of Road Scene Segmenters},
year = {2017},
volume = {},
issn = {2380-7504},
pages = {2011-2020},
abstract = {Despite the recent success of deep-learning based semantic segmentation, deploying a pre-trained road scene segmenter to a city whose images are not presented in the training set would not achieve satisfactory performance due to dataset biases. Instead of collecting a large number of annotated images of each city of interest to train or refine the segmenter, we propose an unsupervised learning approach to adapt road scene segmenters across different cities. By utilizing Google Street View and its time-machine feature, we can collect unannotated images for each road scene at different times, so that the associated static-object priors can be extracted accordingly. By advancing a joint global and class-specific domain adversarial learning framework, adaptation of pre-trained segmenters to that city can be achieved without the need of any user annotation or interaction. We show that our method improves the performance of semantic segmentation in multiple cities across continents, while it performs favorably against state-of-the-art approaches requiring annotated training data.},
keywords = {urban areas;image segmentation;roads;semantics;feature extraction;training;google},
doi = {10.1109/ICCV.2017.220},
url = {https://doi.ieeecomputersociety.org/10.1109/ICCV.2017.220},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {oct}
}

@article{Liu_Yin_Qu_Wang_2023, title={Reducing Domain Gap in Frequency and Spatial Domain for Cross-Modality Domain Adaptation on Medical Image Segmentation}, volume={37}, url={https://ojs.aaai.org/index.php/AAAI/article/view/25260}, DOI={10.1609/aaai.v37i2.25260}, abstractNote={Unsupervised domain adaptation (UDA) aims to learn a model trained on source domain and performs well on unlabeled target domain. In medical image segmentation field, most existing UDA methods depend on adversarial learning to address the domain gap between different image modalities, which is ineffective due to its complicated training process. In this paper, we propose a simple yet effective UDA method based on frequency and spatial domain transfer under multi-teacher distillation framework. In the frequency domain, we first introduce non-subsampled contourlet transform for identifying domain-invariant and domain-variant frequency components (DIFs and DVFs), and then keep the DIFs unchanged while replacing the DVFs of the source domain images with that of the target domain images to narrow the domain gap. In the spatial domain, we propose a batch momentum update-based histogram matching strategy to reduce the domain-variant image style bias. Experiments on two commonly used cross-modality medical image segmentation datasets show that our proposed method achieves superior performance compared to state-of-the-art methods.}, number={2}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Liu, Shaolei and Yin, Siqi and Qu, Linhao and Wang, Manning}, year={2023}, month={Jun.}, pages={1719-1727} }

@InProceedings{sheller,
author="Sheller, Micah J.
and Reina, G. Anthony
and Edwards, Brandon
and Martin, Jason
and Bakas, Spyridon",
editor="Crimi, Alessandro
and Bakas, Spyridon
and Kuijf, Hugo
and Keyvan, Farahani
and Reyes, Mauricio
and van Walsum, Theo",
title="Multi-institutional Deep Learning Modeling Without Sharing Patient Data: A Feasibility Study on Brain Tumor Segmentation",
booktitle="Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="92--104",
abstract="Deep learning models for semantic segmentation of images require large amounts of data. In the medical imaging domain, acquiring sufficient data is a significant challenge. Labeling medical image data requires expert knowledge. Collaboration between institutions could address this challenge, but sharing medical data to a centralized location faces various legal, privacy, technical, and data-ownership challenges, especially among international institutions. In this study, we introduce the first use of federated learning for multi-institutional collaboration, enabling deep learning modeling without sharing patient data. Our quantitative results demonstrate that the performance of federated semantic segmentation models (Dice = 0.852) on multimodal brain scans is similar to that of models trained by sharing data (Dice = 0.862). We compare federated learning with two alternative collaborative learning methods and find that they fail to match the performance of federated learning.",
isbn="978-3-030-11723-8"
}

@article{YANG2021101992,
title = {Federated semi-supervised learning for COVID region segmentation in chest CT using multi-national data from China, Italy, Japan},
journal = {Medical Image Analysis},
volume = {70},
pages = {101992},
year = {2021},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2021.101992},
url = {https://www.sciencedirect.com/science/article/pii/S1361841521000384},
author = {Dong Yang and Ziyue Xu and Wenqi Li and Andriy Myronenko and Holger R. Roth and Stephanie Harmon and Sheng Xu and Baris Turkbey and Evrim Turkbey and Xiaosong Wang and Wentao Zhu and Gianpaolo Carrafiello and Francesca Patella and Maurizio Cariati and Hirofumi Obinata and Hitoshi Mori and Kaku Tamura and Peng An and Bradford J. Wood and Daguang Xu},
keywords = {COVID-19, Chest CT, Federated learning, Semi-supervision},
abstract = {The recent outbreak of Coronavirus Disease 2019 (COVID-19) has led to urgent needs for reliable diagnosis and management of SARS-CoV-2 infection. The current guideline is using RT-PCR for testing. As a complimentary tool with diagnostic imaging, chest Computed Tomography (CT) has been shown to be able to reveal visual patterns characteristic for COVID-19, which has definite value at several stages during the disease course. To facilitate CT analysis, recent efforts have focused on computer-aided characterization and diagnosis with chest CT scan, which has shown promising results. However, domain shift of data across clinical data centers poses a serious challenge when deploying learning-based models. A common way to alleviate this issue is to fine-tune the model locally with the target domains local data and annotations. Unfortunately, the availability and quality of local annotations usually varies due to heterogeneity in equipment and distribution of medical resources across the globe. This impact may be pronounced in the detection of COVID-19, since the relevant patterns vary in size, shape, and texture. In this work, we attempt to find a solution for this challenge via federated and semi-supervised learning. A multi-national database consisting of 1704 scans from three countries is adopted to study the performance gap, when training a model with one dataset and applying it to another. Expert radiologists manually delineated 945 scans for COVID-19 findings. In handling the variability in both the data and annotations, a novel federated semi-supervised learning technique is proposed to fully utilize all available data (with or without annotations). Federated learning avoids the need for sensitive data-sharing, which makes it favorable for institutions and nations with strict regulatory policy on data privacy. Moreover, semi-supervision potentially reduces the annotation burden under a distributed setting. The proposed framework is shown to be effective compared to fully supervised scenarios with conventional data sharing instead of model weight sharing.}
}

@ARTICLE{9672690,  
author={Liu, Zhizhe and Zhu, Zhenfeng and Zheng, Shuai and Liu, Yang and Zhou, Jiayu and Zhao, Yao},  
journal={IEEE Journal of Biomedical and Health Informatics},   title={Margin Preserving Self-Paced Contrastive Learning Towards Domain Adaptation for Medical Image Segmentation},   
year={2022},  
volume={26},  
number={2},  
pages={638-647},  
doi={10.1109/JBHI.2022.3140853}}

@ARTICLE{10021602,
  author={Dong, Shunjie and Pan, Zixuan and Fu, Yu and Xu, Dongwei and Shi, Kuangyu and Yang, Qianqian and Shi, Yiyu and Zhuo, Cheng},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Partial Unbalanced Feature Transport for Cross-Modality Cardiac Image Segmentation}, 
  year={2023},
  volume={42},
  number={6},
  pages={1758-1773},
  doi={10.1109/TMI.2023.3238067}}

@ARTICLE{FedMix,
  author={Wicaksana, Jeffry and Yan, Zengqiang and Zhang, Dong and Huang, Xijie and Wu, Huimin and Yang, Xin and Cheng, Kwang-Ting},
  journal={IEEE Transactions on Medical Imaging}, 
  title={FedMix: Mixed Supervised Federated Learning for Medical Image Segmentation}, 
  year={2023},
  volume={42},
  number={7},
  pages={1955-1968},
  doi={10.1109/TMI.2022.3233405}}

@inproceedings{Mixup,
title={mixup: Beyond Empirical Risk Minimization},
author={Hongyi Zhang and Moustapha Cisse and Yann N. Dauphin and David Lopez-Paz},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=r1Ddp1-Rb},
}

@ARTICLE{Cui_structure_driven,
  author={Cui, Zhiming and Li, Changjian and Du, Zhixu and Chen, Nenglun and Wei, Guodong and Chen, Runnan and Yang, Lei and Shen, Dinggang and Wang, Wenping},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Structure-Driven Unsupervised Domain Adaptation for Cross-Modality Cardiac Segmentation}, 
  year={2021},
  volume={40},
  number={12},
  pages={3604-3616},
  doi={10.1109/TMI.2021.3090432}}

@ARTICLE{iseg,
  author={Sun, Yue and Gao, Kun and Wu, Zhengwang and Li, Guannan and Zong, Xiaopeng and Lei, Zhihao and Wei, Ying and Ma, Jun and Yang, Xiaoping and Feng, Xue and Zhao, Li and Le Phan, Trung and Shin, Jitae and Zhong, Tao and Zhang, Yu and Yu, Lequan and Li, Caizi and Basnet, Ramesh and Ahmad, M. Omair and Swamy, M. N. S. and Ma, Wenao and Dou, Qi and Bui, Toan Duc and Noguera, Camilo Bermudez and Landman, Bennett and Gotlib, Ian H. and Humphreys, Kathryn L. and Shultz, Sarah and Li, Longchuan and Niu, Sijie and Lin, Weili and Jewells, Valerie and Shen, Dinggang and Li, Gang and Wang, Li},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Multi-Site Infant Brain Segmentation Algorithms: The iSeg-2019 Challenge}, 
  year={2021},
  volume={40},
  number={5},
  pages={1363-1376},
  doi={10.1109/TMI.2021.3055428}}

@article{mcrib,
  title={A new neonatal cortical and subcortical brain atlas: the Melbourne Children's Regional Infant Brain (M-CRIB) atlas},
  author={Alexander, Bonnie and Murray, Andrea L and Loh, Wai Yen and Matthews, Lillian G and Adamson, Chris and Beare, Richard and Chen, Jian and Kelly, Claire E and Rees, Sandra and Warfield, Simon K and others},
  journal={Neuroimage},
  volume={147},
  pages={841--851},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{
loshchilov2017sgdr,
title={{SGDR}: Stochastic Gradient Descent with Warm Restarts},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=Skq89Scxx}
}

@inproceedings{
loshchilov2018decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@article{PEREZGARCIA2021106236,
title = {TorchIO: A Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning},
journal = {Computer Methods and Programs in Biomedicine},
volume = {208},
pages = {106236},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106236},
url = {https://www.sciencedirect.com/science/article/pii/S0169260721003102},
author = {Fernando Pérez-García and Rachel Sparks and Sébastien Ourselin},
keywords = {Medical image computing, Deep learning, Data augmentation, Preprocessing},
abstract = {Background and objective
Processing of medical images such as MRI or CT presents different challenges compared to RGB images typically used in computer vision. These include a lack of labels for large datasets, high computational costs, and the need of metadata to describe the physical properties of voxels. Data augmentation is used to artificially increase the size of the training datasets. Training with image subvolumes or patches decreases the need for computational power. Spatial metadata needs to be carefully taken into account in order to ensure a correct alignment and orientation of volumes.
Methods
We present TorchIO, an open-source Python library to enable efficient loading, preprocessing, augmentation and patch-based sampling of medical images for deep learning. TorchIO follows the style of PyTorch and integrates standard medical image processing libraries to efficiently process images during training of neural networks. TorchIO transforms can be easily composed, reproduced, traced and extended. Most transforms can be inverted, making the library suitable for test-time augmentation and estimation of aleatoric uncertainty in the context of segmentation. We provide multiple generic preprocessing and augmentation operations as well as simulation of MRI-specific artifacts.
Results
Source code, comprehensive tutorials and extensive documentation for TorchIO can be found at http://torchio.rtfd.io/. The package can be installed from the Python Package Index (PyPI) running pip install torchio. It includes a command-line interface which allows users to apply transforms to image files without using Python. Additionally, we provide a graphical user interface within a TorchIO extension in 3D Slicer to visualize the effects of transforms.
Conclusion
TorchIO was developed to help researchers standardize medical image processing pipelines and allow them to focus on the deep learning experiments. It encourages good open-science practices, as it supports experiment reproducibility and is version-controlled so that the software can be cited precisely. Due to its modularity, the library is compatible with other frameworks for deep learning with medical images.}
}
@ARTICLE{811270,
  author={Van Leemput, K. and Maes, F. and Vandermeulen, D. and Suetens, P.},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Automated model-based tissue classification of MR images of the brain}, 
  year={1999},
  volume={18},
  number={10},
  pages={897-908},
  doi={10.1109/42.811270}}

@article{SUDRE201750,
title = {Longitudinal segmentation of age-related white matter hyperintensities},
journal = {Medical Image Analysis},
volume = {38},
pages = {50-64},
year = {2017},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2017.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1361841517300257},
author = {Carole H. Sudre and M. Jorge Cardoso and Sebastien Ourselin},
keywords = {White matter hyperintensities, Segmentation, Longitudinal},
abstract = {Although white matter hyperintensities evolve in the course of ageing, few solutions exist to consider the lesion segmentation problem longitudinally. Based on an existing automatic lesion segmentation algorithm, a longitudinal extension is proposed. For evaluation purposes, a longitudinal lesion simulator is created allowing for the comparison between the longitudinal and the cross-sectional version in various situations of lesion load progression. Finally, applied to clinical data, the proposed framework demonstrates an increased robustness compared to available cross-sectional methods and findings are aligned with previously reported clinical patterns.}
}

@inproceedings{chen2021source,
  title={Source-free domain adaptive fundus image segmentation with denoised pseudo-labeling},
  author={Chen, Cheng and Liu, Quande and Jin, Yueming and Dou, Qi and Heng, Pheng-Ann},
  booktitle={Medical Image Computing and Computer Assisted Intervention--MICCAI 2021: 24th International Conference, Strasbourg, France, September 27--October 1, 2021, Proceedings, Part V 24},
  pages={225--235},
  year={2021},
  organization={Springer}
}

@inproceedings{NEURIPS2022_bcdec1c2,
 author = {Gandelsman, Yossi and Sun, Yu and Chen, Xinlei and Efros, Alexei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {29374--29385},
 publisher = {Curran Associates, Inc.},
 title = {Test-Time Training with Masked Autoencoders},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/bcdec1c2d60f94a93b6e36f937aa0530-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@article{karani2021test,
  title={Test-time adaptable neural networks for robust medical image segmentation},
  author={Karani, Neerav and Erdil, Ertunc and Chaitanya, Krishna and Konukoglu, Ender},
  journal={Medical Image Analysis},
  volume={68},
  pages={101907},
  year={2021},
  publisher={Elsevier}
}

@article{he2021autoencoder,
  title={Autoencoder based self-supervised test-time adaptation for medical image analysis},
  author={He, Yufan and Carass, Aaron and Zuo, Lianrui and Dewey, Blake E and Prince, Jerry L},
  journal={Medical image analysis},
  volume={72},
  pages={102136},
  year={2021},
  publisher={Elsevier}
}