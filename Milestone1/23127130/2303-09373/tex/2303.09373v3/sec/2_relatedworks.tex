\section{Related Work}
\label{sec:works}
\subsection{Masked Image Modeling}
Masked image modeling (MIM) represents a category of methods that learn representations from corrupted or incomplete images~\cite{Doersch_2015_ICCV, pmlr-v119-chen20s, pmlr-v119-henaff20a, Pathak_2016_CVPR}, and can naturally serve as a pretext task for self-supervised learning. For example, masked autoencoder (MAE) trains an encoder by reconstructing missing regions from a masked image input and has demonstrated improved generalization and performance in downstream tasks~\cite{He_2022_CVPR, Xie_2022_CVPR, 10.1007/978-3-031-20056-4_18, Xie_2023_CVPRa, Xie_2023_CVPRb,Kong_2023_CVPR, Tang_2022_CVPRb}. MAPSeg heavily relies on MIM, leveraging MAE and masked pseudo-labeling (MPL), to achieve versatile UDA.

\subsection{Pseudo-Labeling}
Pseudo-labeling facilitates learning from limited or imperfect data and is prevalent in semi- and self-supervised learning~\cite{lee2013pseudo,Hu_2021_CVPR, Petrovai_2022_CVPR}. Consistency regularization is widely used in pseudo-label learning~\cite{NIPS2016_30ef30b6, laine2016temporal}, which is a scheme that forces the model to output consistent prediction for inputs with different degrees of perturbation (\eg, weakly- and strongly-augmented images). Mean Teacher~\cite{NIPS2017_68053af2}, a teacher-student framework that generates pseudo-labels from the teacher model (which is a temporal ensembling of the student model), is also a common strategy. In this work, we utilize the teacher model to generate pseudo labels based on complete images and guide the learning of student model on masked images. 

\subsection{Unsupervised Domain Adaptation}
Discrepancy minimization, adversarial learning, and pseudo-labeling are the three main directions explored in UDA. Previous studies have explored minimizing the discrepancy between source and target domains within different spaces, such as input~\cite{Chen_Dou_Chen_Qin_Heng_2019, 9741336, pmlr-v80-hoffman18a}, feature~\cite{he2021autoencoder, 10021602, 8764342, Du_2019_ICCV,NIPS2016_ac627ab1, Feng_Ju_Wang_Song_Zhao_Ge_2023}, and output spaces~\cite{10.1007/978-3-030-58555-6_42, 8954439}, and they sometimes overlap with approaches base on adversarial learning as the supervisory signal to align two distributions may come from statistical distance metrics~\cite{pmlr-v37-long15,NIPS2004_96f2b50b} or a discriminator model~\cite{8764342, 8954439, pmlr-v80-hoffman18a}. Meanwhile, self-training with pseudo-label is also a prevalent technique~\cite{Zhang_2021_CVPR, zheng2021rectifying, Zou_2019_ICCV} and has shown significant improvement on natural image segmentation~\cite{hrda, daformer,Hoyer_2023_CVPR}. Hoyer \etal~\cite{Hoyer_2023_CVPR} proposed masked image consistency as a plug-in to improve previous UDA baselines. In contrast, MAPSeg leverages the synergy between MAE and MPL, and employs MPL as a standalone component for various scenarios.

In this study, we exploit the vanilla pseudo-labeling with three straightforward yet crucial measures to stabilize the training. We hypothesize that random masking is an ideal strong perturbation for consistency regularization in pseudo-labeling, and the model pretrained via MAE can be efficiently adapted to infer semantics of missing regions from visible patches. This hypothesis is justified in Sec. \hyperref[tab:ablation]{4.3}. In addition, we leverage the anatomical distribution prior in medical images and make predictions jointly based on local and global contexts, which also help mitigate the pseudo-label drifts. We demonstrate the superior performance and versatility of MAPSeg in different UDA scenarios in the following sections.

\subsection{Federated Learning}
Federated Learning (FL) is a distributed learning paradigm that aims to train models on decentralized data~\cite{mcmahan2017communication}. FL has attracted great attention in the research community in the last few years and numerous works have focused on the key challenges raised by FL such as data/system heterogeneity~\cite{Tang_2022_CVPR} and communication/computation efficiency~\cite{fedmask}. By virtue of keeping privacy-sensitive medical data local, FL has been adopted for various medical image analysis tasks~\cite{guan2023federated}. Sheller \etal~\cite{sheller} pioneered FL for brain tumor segmentation on multimodal brain scans in a multi-institutional collaboration and showed its promising performance compared to centralized training. Yang \etal~\cite{YANG2021101992} proposed a federated semi-supervised learning framework for COVID-19 detection that relaxed the requirement for all clients to have access to ground truth annotations. FedMix~\cite{FedMix} further alleviated the necessity for all clients to possess dense pixel-level labels, allowing users with weak bounding-box labels or even image-level class labels to collaboratively train a segmentation model. In contrast, MAPSeg assumes all clients have completely unlabeled data when extended to federated UDA scenario. Mushtaq \etal~\cite{DBLP:conf/isbi/MushtaqBDA23} proposed a Federated Alternate Training (\textit{FAT}) scheme that leverages both labeled and unlabeled data silos. It employs mixup~\cite{Mixup} and pseudo-labeling to enable self-supervised learning on the unlabeled participants. MAPSeg, on the other hand, adopts masked pseudo-labeling and global-local feature collaboration for adapting to unlabeled target domains. Yao \etal~\cite{fmtda} introduced the federated multi-target domain adaptation problem and a solution termed \textit{DualAdapt}. It decouples the local-classifier adaptation with client-side self-supervised learning from the feature alignment via server-side mixup and adversarial training. MAPSeg addresses the same federated multi-target UDA problem, and we compare our results to those of \textit{FAT} and \textit{DualAdapt} in Sec. \hyperref[exp:fl]{4.3}.

\subsection{Test-Time UDA}
While federated UDA eases the constraint of centralized data, its learning paradigm still requires synchronous learning across server and clients. Test-time UDA~\cite{chen2021source, NEURIPS2022_bcdec1c2, he2021autoencoder, karani2021test,10.1145/2939672.2939716, Liu_2021_CVPR} assumes the unavailability of source-domain data when adapting to target domains. This assumption significantly limits the applicability of methods based on image translation, adversarial learning, and feature distribution alignment which require simultaneous access to both source and target data. Gandelsman \etal~\cite{NEURIPS2022_bcdec1c2} explored using MAE retraining during test-time to improve classification without employing pseudo labeling. Chen \etal~\cite{chen2021source} proposed using prototype and uncertainty estimation for denoised pseudo labeling of 2D fundus images. Karani \etal~\cite{karani2021test} designed a 2D denoising autoencoder to refine pseudo labels. He \etal~\cite{he2021autoencoder} employed AE during test-time to align source and target feature distributions by minimizing AE reconstruction loss. We demonstrate that, with slight performance drop on source domain, MAPSeg can be extended to test-time UDA with comparable performance to that of centralized UDA on target domain (\cref{sec.ttdaresults}).