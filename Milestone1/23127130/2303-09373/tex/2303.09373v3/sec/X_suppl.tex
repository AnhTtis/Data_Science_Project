\renewcommand{\figurename}{Suppl.Fig.}
\renewcommand{\tablename}{Suppl.Tab.}
\setcounter{table}{0}
\setcounter{figure}{0}
\setcounter{section}{0}

\clearpage
\setcounter{page}{1}
\maketitlesupplementary


\section{Appendix}
\label{sec:appendix}

\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}

\subsection{Model Architecture}
\label{sec:archite}
\newcommand{\blocka}[2]{\multirow{3}{*}{\(\left[\begin{array}{c}\text{3$\times$3, #1}\\[-.1em] \text{3$\times$3, #1} \end{array}\right]\)$\times$#2}
}
\newcommand{\block}[3]{\multirow{3}{*}{\(\left[\begin{array}{c}\text{4$\times$4$\times$4, #1}\\[-.1em] \text{3$\times$3$\times$3, #2}\\[-.1em] \text{3$\times$3$\times$3, #2}\end{array}\right]\)$\times$#3}
}
\newcommand{\blockb}[3]{\multirow{3}{*}{\(\left[\begin{array}{c}\text{3$\times$3$\times$3, #2}\\[-.1em] \text{3$\times$3$\times$3, #2}\\[-.1em] \text{3$\times$3$\times$3, #1}\end{array}\right]\)$\times$#3}
}
\newcommand{\seghead}[1]{\multirow{2}{*}{\(\left[\begin{array}{c}\text{3$\times$3$\times$3, 64}\\[-.1em] \text{1$\times$1$\times$1, cls\_num}\\[-.1em] \end{array}\right]\)$\times$#1}
}
\begin{figure}[b]
  \centering
     \includegraphics[width=1\linewidth]{./figs/arch-12.pdf}
  \caption{Illustrations of 3D ResNet Block and 3D Atrous Spatial Pyramid Pooling (ASPP) layer.}
  \label{fig:arch}
\end{figure}

MAPSeg is implemented using PyTorch. Detailed configurations of model and training can be found below. 

\noindent\textbf{ 3D Multi-Scale Masked Autoencoder (MAE).} We implement the 3D MAE using 3D ResNet Blocks \cite{he2016deep,wolny2020accurate} instead of Vision Transformers, different from the previous study \cite{He_2022_CVPR}, due to the constraint of GPU memory. The encoder consists of eight 3D ResNet Blocks. The 3D ResNet Block is depicted in \hyperref[fig:arch]{Suppl.Fig.1a}. Following the previous study \cite{He_2022_CVPR}, we adopt an asymmetric design by employing a lightweight decoder (\hyperref[tab:arch]{Suppl.Tab.1}). 

\renewcommand\arraystretch{1.1}
\setlength{\tabcolsep}{3pt}
\begin{table}[t]
\label{tab:arch}
\begin{center}
\resizebox{1\linewidth}{!}{
\begin{tabular}{c|c|c|c}
\hline
\toprule
\multicolumn{4}{c}{\textbf{Encoder}}\\	
\hline
Layer Name & Input Size & Output Size & Architecture \\
\hline
\multirow{3}{*}{enc\_res1} & \multirow{3}{*}{(1,96,96,96)} & \multirow{3}{*}{(512,24,24,24)} &  \block{512}{512}{1} \\
&  &  &  \\
&  &  &  \\
\hline
\multirow{3}{*}{enc\_res2.x} & \multirow{3}{*}{(512,24,24,24)} & \multirow{3}{*}{(512,24,24,24)} &  \blockb{512}{512}{7} \\
&  &  &  \\
&  &  &  \\
\toprule
\multicolumn{4}{c}{\textbf{MAE Decoder}}\\	
\hline
Layer name & Input size & Output size & Architecture \\
\hline
trans\_conv1 & (512,24,24,24) & (32,96,96,96) &  4$\times$4$\times$4, 32, stride 4\\

\hline
\multirow{3}{*}{dec\_res1} & \multirow{3}{*}{(32,96,96,96)} & \multirow{3}{*}{(16,96,96,96)} &  \blockb{16}{16}{1} \\
&  &  &  \\
&  &  &  \\
\hline
final\_recon & (16,96,96,96) & (1,96,96,96) &  3$\times$3$\times$3, 1, stride 1\\
\toprule
\multicolumn{4}{c}{\textbf{Segmentation Decoder}}\\	
\hline
Layer name & Input size & Output size & Architecture \\
\hline
ASPP & (1024,24,24,24) & (512,24,24,24) &  \hyperref[fig:arch]{Suppl. Fig.1b}\\
\hline


trans\_conv2 & (512,24,24,24) & (64,96,96,96) &  4$\times$4$\times$4, 64, stride 4\\
\hline
\multirow{2}{*}{seg\_head} & \multirow{2}{*}{(64,96,96,96)} & \multirow{2}{*}{(cls\_num,96,96,96)} &  \seghead{1} \\
&  &  &  \\
\toprule
\end{tabular}
}
\end{center}
\vspace{-.5em}
\caption{Architectures of different components of MAPSeg. Building blocks ([kernal size, output channels]) are shown in brackets, with the number of blocks stacked. Downsampling is performed by the first block of enc\_res1 with a stride of 4.
}
\vspace{-.5em}
\end{table}

\noindent\textbf{3D Global-Local Collaboration (GLC).} The segmentation backbone (\hyperref[tab:arch]{Suppl.Tab.1}) consists of the pretrained encoder and a segmentation decoder that is adapted from DeepLabV3 \cite{chen2017rethinking}. In the decoding path, we take advantage of the Atrous Spatial Pyramid Pooling (ASPP), which employs dilated convolution at multiple scales and provides access to larger FOV (\hyperref[fig:arch]{Suppl.Fig.1b}). After feature extraction, the GLC module fuses the local and global features and forms a latent representation with a dimension of 1024, which is then fed into the ASPP layer. During training, each local sub-volume with size of $96\times96\times96$ is randomly sampled from global scan. During inference, the final output is formed by sliding window with stride of 80 across entire volumetric scan. 

\subsection{Training Recipe}
\label{sec:recipe}
\textbf{MAE Pretraining.}
For the MAE Pretraining, we follow the training configurations listed in \hyperref[tab:mae_pretrain]{Suppl.Tab.2}. Each mini-batch contains a pair of randomly sampled local patch $x$ and downsampled global scan $X$. The masking patch in \hyperref[tab:mae_pretrain]{Suppl.Tab.2} only applies to $x$ and is always half-sized for $X$ because of the larger FOV. For example, in the ablation study of masking patch size, a masking patch of 16 to $x$ indicates a masking patch of 8 to $X$. We implement the augmentation using TorchIO \cite{PEREZGARCIA2021106236}. During the MAE stage, we employ random 3D affine transformation, with isotropic scaling 75-150\% and rotation [-40\textdegree, 40\textdegree].

\begin{table}[]
\tablestyle{6pt}{1.02}
\scriptsize
\begin{tabular}{c|c}
config & value \\
\shline
masking patch & 8$\times$8$\times$8\\
masking ratio & 70\%\\
optimizer & AdamW \cite{loshchilov2018decoupled}\\
learning rate & $2e^{-4}$\\
weight decay &0.05\\
optim. momentum & $\beta_1, \beta_2=0.9,0.95$\\
\multirow{2}{*}{lr scheduler} & cosine annealing \cite{loshchilov2017sgdr}\\
& $T_{max}$=20, min\_lr=$1e^{-6}$\\
total epochs & 300\\
annealing epochs & last 100\\
batch size & 4\\
iters/epoch & 500\\
aug. prob. & 0.35\\
\multirow{1}{*}{augmentation} & random affine \\
\end{tabular}
\vspace{-.5em}
\caption{MAE Pretraining Configurations}
\label{tab:mae_pretrain} \vspace{-.5em}
\end{table}

\noindent\textbf{Centralized UDA.}
For the centralized UDA on brain MRI segmentation tasks, detailed training configuration can be found in \hyperref[tab:cent_mpl]{Suppl.Tab.3}. Similarly, each mini-batch contains a pair of $x$ and $X$ from the source domain and another pair from the target domain (four 96$\times$96$\times$96 patches). During warmup epochs, the model is only trained on source domain. We utilize $\mathit{Score}$ to select the best model and the patience is set as 50 epochs. For the target domain, we design a similar random 3D affine transformation, with scaling 70-130\% and rotation [-30\textdegree, 30\textdegree]. A stronger augmentation strategy is applied to the source domain, consisting of random affine (scaling 70-140\% and rotation [-30\textdegree, 30\textdegree]), random bias field \cite{811270,SUDRE201750}, and random gamma transformation ($\gamma \in [e^{-0.4}, e^{0.4}]$). For the centralized UDA on public cardiac CT$\rightarrow$MRI segmentation, we use the same configuration except for training epochs of 150 and warmup epochs of 50. For MRI$\rightarrow$CT cardiac segmentation, we use a less aggressive augmentation strategy because MRI is noisier than CT. We set the scaling ratio to 85-115\% and rotation to [-15\textdegree, 15\textdegree] for both source and target domains, and exclude random bias field and gamma transformation. The warmup epoch is set as 70. 

\begin{table}[]
\tablestyle{6pt}{1.02}
\scriptsize
\begin{tabular}{c|c}
config & value \\
\shline
masking patch & 8$\times$8$\times$8\\
masking ratio & 70\%\\
optimizer & AdamW \cite{loshchilov2018decoupled}\\
learning rate & $1e^{-4}$\\
weight decay &0.01\\
optim. momentum & $\beta_1, \beta_2=0.9,0.999$\\
\multirow{2}{*}{lr scheduler} & cosine annealing warm restart \cite{loshchilov2017sgdr}\\
& $T_{0}$=10, $T_{mult}$=2, min\_lr=$1e^{-8}$\\
total epochs & 100\\
warmup epochs & first 10 \\
annealing epochs & all\\
early stop & 50\\
batch size & 1\\
iters/epoch & 100\\
aug. prob. & 0.35\\
\multirow{3}{*}{source aug.} & random affine \\
& random bias field \\
& random gamma trans. \\
\multirow{1}{*}{target aug.} & random affine \\

\end{tabular}
\vspace{-.5em}
\caption{Centralized UDA configurations for brain MRI segmentation.}
\label{tab:cent_mpl} \vspace{-.5em}
\end{table}

\noindent\textbf{Federated UDA.}
For the federated UDA tasks, we follow the procedure detailed in \cref{alg:fed_mapseg}. We initialize the encoder of the global model $f_\phi$ with the encoder pretrained on the large-scale data mentioned in Sec.4.3. We set the global FL round $R = 100$. We set both the server and client update steps to 1 epoch with batch size of 1. Training configuration inherits mostly from that of the centralized UDA, except a global cosine annealing learning rate schedule is adopted to decay the learning rate from $1e^{-4}$ to $1e^{-6}$ over the course of the FL rounds.

\noindent\textbf{Test-Time UDA.}
For the test-time UDA tasks, we follow the same configuration as listed in \hyperref[tab:cent_mpl]{Suppl.Tab.3}. The difference is that the model can only access source domain data (image and label) during warmup epochs and can only access target domain data (image only) after that, while centralized UDA has synchronous access to both source and target domain data throughout the whole training process.
\begin{algorithm}[tb]
\caption{Federated MAPSeg (Fed-MAPSeg)}
\label{alg:fed_mapseg}
\begin{algorithmic}[1]
\REQUIRE Source domain dataset $D_S = \{(x_s, y_s)\}$ and target domain datasets $D_T^k = \{(x_t^k)\}$ for each client $k$, pretrained global model $f_\phi$, number of FL round $R$, number of server update steps $T_s$, number of client update steps $T_t$
\FOR{$r=1,2,\cdots, R$}
    \STATE Initialize server EMA teacher model: $\theta \gets \phi$
    \FOR{$t=1,2,\cdot,T_s$}
        \STATE Sample patches $(x_s, y_s)$ from $D_S$ and generate downsampled global volume and masked inputs $X_s$, $X_s^M$, $x_s^M$
        \STATE Update $f_\phi$ on server by minimizing $\mathcal{L}_s$ (Eq.9)
        \STATE Update server EMA teacher model parameter $\theta$ with (Eq.3)
    \ENDFOR
    \STATE Server broadcast $\theta$ to clients
    \FOR{each client $k$ in parallel}
        \STATE $\phi_k \gets \theta$, $\theta_k \gets \theta$
        \FOR{$t=1,2,\cdots,T_t$}
            \STATE Sample patches $x_t^k$ from $D_T^k$ and generate downsampled global volume and masked inputs $X_t^k$, $(X_t^k)^M$, $(x_s^k)^M$
            \STATE Generate pseudolabels for unmasked inputs $x_t^k$ and $X_t^k$ using the teacher model $f_{\theta_k}$: $f_{\theta_k}(x_t^k)$ and $f_{\theta_k}(X_t^k)$
            \STATE Update $f_{\phi_k}$ by minimizing $\mathcal{L}_u$ (Eq.10)
            \STATE Update client EMA teacher model parameter with (Eq.3)
        \ENDFOR
        \STATE Upload $\theta_k$ to server
    \ENDFOR
\STATE The server aggregates $\theta_k$ from clients:
\[\Bar{\theta} \gets \sum_k \frac{|D_T^k|}{\sum_k |D_T^k|}\theta_k\]
\STATE Update server model parameters $\phi \gets \Bar{\theta}$
\ENDFOR
\end{algorithmic}
\end{algorithm}
\subsection{Implementation of Comparing Methods}
\label{sec:baseline}
For other comparing methods in centralized UDA, we adapt their official implementations. For DAFormer, HRDA, and MIC, we modify the ground truth labels to make them denser, as we observe that the original sparse annotations cause trouble for those methods. Specifically, we crop the scans to include only brain regions. In addition to having foreground classes of 7 subcortical regions (which account for approximately 2\% of overall voxels), we assign another foreground class to the remaining brain regions. Therefore, there are 9 classes for DAFormer, HRDA, and MIC, 8 foreground and 1 background classes. This modification significantly improves the results. For the FL baselines FAT~\cite{DBLP:conf/isbi/MushtaqBDA23} and DualAdapt~\cite{fmtda}, since there is no public official implementation available, we implement both methods following the description in the original papers and finetune thoroughly. We use the same network backbone initialized with the same pretrained encoder and training configuration (FL rounds, global learning rate schedule, local update steps, batch size, etc.) as Fed-MAPSeg whenever possible.


\subsection{Dataset Description}
\label{sec:datadetail}
We include a diverse collection of 2,421 brain MRI scans from several international projects, each with its unique focus on infant brain development. From the Developing Human Connectome Project (dHCP) V1.0.2 data release\footnote{\url{https://www.developingconnectome.org/data-release/data-release-user-guide/}} \cite{edwards2022developing} in the UK, we incorporate 983 scans (426 T1-weighted, T1w), acquired shortly after birth. The Baby Connectome Project (BCP) \cite{howell2019unc} in the USA contributes 892 scans (519 T1w), featuring longitudinal data. Additionally, from the Environmental Influences on Child Health Outcomes (ECHO) project, also in the USA, we have 433 scans (218 T1w) from newborn infants. The ‘Maternal Adversity, Inflammation, and Neurodevelopment’ (Healthy Minds) project from Brazil, conducted at Hospital São Paulo - Federal University of São Paulo (UNIFESP), adds 103 T2-weighted (T2w) MRI scans, acquired shortly after birth and available in the National Institute of Mental Health Data Archive (collection ID 3811). Lastly, the Melbourne Children’s Regional Infant Brain (M-CRIB) project \cite{mcrib} from Australia provides 10 additional T2w scans. All studies involved have received Institutional Review Board (IRB) approvals. MAPSeg takes normalized scans as inputs. During training, the intensity of each volumetric scan is clipped at a percentile randomly drawn from a uniform distribution $\mathcal{U}(99,100)$, then normalized to 0-1. During inference, the intensity clip is fixed at 99.5\%. The top 0.5\% intensity is clipped as 1 to cope with outlier pixels (hyperintensities) that are usual in MRI. 

\subsection{Results of MRI $\rightarrow$ CT cardiac segmentation}
\label{sec:mri2ctresult}
The performance of MAPSeg on the public cardiac MRI$\rightarrow$CT segmentation is reported in \hyperref[tab:mri2ct]{Suppl.Tab.4}. Similarly, we use the same dataset partition as previous studies. MAPSeg consistently outperforms other baseline methods, although the performance gap is smaller than CT$\rightarrow$MRI.
\begin{table}
  \caption{Results of cardiact MRI$\rightarrow$CT segmentation.}
  \vspace{-1em}
  \centering
  \begin{center}
      \resizebox{0.78\linewidth}{!}{%
          \begin{tabular}{c|cccccccc}
              \toprule
              \multicolumn{6}{c}{Cardiac CT $\rightarrow$ MRI segmentation}\\	
              \hline
              \multirow{2}{*}{Method} &\multicolumn{5}{c}{Dice(\%) $\uparrow$} \\
              \cline{2-6}
              &AA &LAC &LVC &MYO &Avg \\
              \hline
              PnP-AdaNet\cite{8764342} &74.0&68.9&61.9&50.8&63.9 \\
              SIFA-V1\cite{Chen_Dou_Chen_Qin_Heng_2019} &81.1&76.4&75.7&58.7&73.0 \\
              SIFA-V2\cite{8988158} &81.3&79.5&73.8&61.6&74.1 \\
              DAFormer\cite{daformer} &85.5&88.2&74.5&60.2&77.1 \\
              MPSCL\cite{9672690} & 90.3&87.1&86.5&72.5&84.1\\
              MA-UDA\cite{10273225} &90.8&88.7&77.6&67.4&81.1 \\
              SE-ASA\cite{Feng_Ju_Wang_Song_Zhao_Ge_2023} &83.8&85.2&82.9&71.7&80.9 \\
              FSUDA-V1\cite{Liu_Yin_Qu_Wang_2023} &86.4&86.9&84.8&81.8&85.0\\
              PUFT\cite{10021602} &88.1&88.5&87.5&74.1&84.6\\
              SDUDA\cite{Cui_structure_driven}&87.9&88.1&88.4&78.7&85.8\\
              FSUDA-V2\cite{10261458} &88.2&\textbf{88.9}&85.2&\textbf{82.2}&86.1\\
              
              \hline
              \textbf{MAPSeg (Ours)}&\underline{\textbf{93.3}}&\underline{87.3}&\underline{\textbf{89.1}}&\underline{78.9}&\underline{\textbf{87.1}}\\

              \bottomrule

      \end{tabular}}
  \end{center}
  \label{tab:mri2ct}
  \vspace{-1.5em}
\end{table}

\subsection{Additional Analysis}
\label{sec:additionres}
\noindent\textbf{Influence of MAE Pretraining on UDA Results.} We conduct an additional analysis to investigate the relationship between MAE training steps and downstream UDA performance. The experiments are conducted on cross-sequence brain MRI segmentation (\hyperref[fig:maesteps]{Suppl.Fig.2}). We observe significant improvement in UDA performance at the first 75,000 MAE training steps, which then gradually saturates. We choose 150,000 MAE training steps as the benefits of further training diminish. 
\begin{figure}
  \centering
     \includegraphics[width=1\linewidth]{./figs/maesteps-13.pdf}
  \caption{Downstream cross-sequence centralized UDA performance vs. MAE pretraining iterations.}
  \label{fig:maesteps}
\end{figure}

\noindent\textbf{Sensitivity to hyperparameters.} We conduct additional experiments on cross-sequence brain MRI segmentation to investigate the sensitivity of MAPSeg to hyperparameters (\hyperref[tab:hyperparams]{Suppl.Tab.5}). Specifically, we investigate the step size ($\alpha$) of EMA update as well as weights of loss terms ($\gamma$ and $\delta$). When one parameter is varying, other parameters remain unchanged. We notice that the performance is relatively stable across a wide range of hyperparameters. Since we did not tune the hyperparameters extensively during development, the default parameters may not represent the optimal setting.

\begin{table}[h]
    \caption{Influence of hyperparameters on results, bold indicates used parameters.}
    \centering
    \label{tab:hyperparams}
    \begin{center}
        \resizebox{0.77\linewidth}{!}{%
            \begin{tabular}{c|ccccc}
                \hline

                $\alpha$ &\textbf{0.999/0.9999} &0.99/0.999 &0.99 &0.999 &0.9999 \\
                \hline
                Dice (\%) &77.73&74.00&74.26&74.74&78.06 \\
                \hline

                $\gamma$ &\textbf{0.05} & 0.5 & 0.1 & 0.01 & 0.005  \\
                \hline
                Dice (\%) &77.73 & 77.22 & 77.97 & 77.98 & 77.99 \\
                \hline

                $\delta$ &\textbf{0.025} &0.25 &0.1 &0.01 &0.0025 \\
                \hline
                Dice (\%) &77.73 & 76.74 & 78.08 & 77.82 & 78.57 \\
                \hline
        \end{tabular}}
    \end{center}
\end{table}


\subsection{Visualization}
\label{sec:appendixvis}
\textbf{MAE.}
Some visualizations of MAE results (axial slices) are provided in \hyperref[fig:maeres]{Suppl.Fig.3}. 
\begin{figure}
  \centering
     \includegraphics[width=1\linewidth]{./figs/mae-res-14.pdf}
  \caption{A randomly sampled T2w scan in cross-sequence task. MAE parameters is same as in \hyperref[tab:mae_pretrain]{Suppl.Tab.2}}
  \label{fig:maeres}
\end{figure}




\noindent\textbf{UDA Results.}
We provide qualitative comparisons of different methods on cross-sequence (X-Seq), cross-site (X-Site), and cross-age (X-Age) brain MRI segmentation tasks in \hyperref[fig:visres]{Suppl.Fig.4}. MAPSeg consistently provides accurate segmentation in different UDA settings. It is worth noting that, despite the second best performance in cross-sequence, DAR-UNet tends to oversegment on cross-site and cross-age tasks, partially because of translation errors. On cross-site and cross-age tasks, despite DAFormer, HRDA, and MIC generate reasonably good segmentation inside the subcortical regions, they exhibit extensive false positives outside the subcortical regions, leading to suboptimal overall Dice score.

\begin{figure*}
  \centering
     \includegraphics[width=1\linewidth]{./figs/visres-15.pdf}
  \caption{Qualitative comparisons. Three rows (top to bottom) of each task represent axial plane, coronal plane, and sagittal plane, respectively.}
  \label{fig:visres}
\end{figure*}




