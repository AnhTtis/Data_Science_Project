\section{Related Work}
\label{sec:works}
\subsection{Masked Image Modeling}
Masked image modeling (MIM) represents a category of methods that learn representations from corrupted or incomplete images~\cite{Doersch_2015_ICCV, pmlr-v119-chen20s, pmlr-v119-henaff20a, Pathak_2016_CVPR}, and can naturally serve as a pretext task for self-supervised learning. For example, masked autoencoder (MAE) trains an encoder by reconstructing missing regions from a masked image input and has demonstrated improved generalization and performance in downstream tasks~\cite{He_2022_CVPR, Xie_2022_CVPR, 10.1007/978-3-031-20056-4_18, Xie_2023_CVPRa, Xie_2023_CVPRb,Kong_2023_CVPR, Tang_2022_CVPRb}. 

\subsection{Pseudo-Labeling}
Pseudo-labeling facilitates learning from limited or imperfect data and is prevalent in semi- and self-supervised learning~\cite{lee2013pseudo,Hu_2021_CVPR, Petrovai_2022_CVPR}. Consistency regularization is widely used in pseudo-label learning~\cite{NIPS2016_30ef30b6, laine2016temporal}, which is a scheme that forces the model to output consistent prediction for inputs with different degrees of perturbation (\eg, weakly- and strongly-augmented images). Mean Teacher~\cite{NIPS2017_68053af2}, a teacher-student framework that generates pseudo-labels from the teacher model (which is a temporal ensembling of the student model), is also a common strategy. In this study, we utilize the teacher model to generate pseudo labels based on complete images and guide the learning of student model on masked images. 

\subsection{Unsupervised Domain Adaptation}
Discrepancy minimization, adversarial learning, and pseudo-labeling are the three main directions explored in UDA. Previous studies have explored minimizing the discrepancy between source and target domains within different spaces, such as input~\cite{Chen_Dou_Chen_Qin_Heng_2019, 9741336, pmlr-v80-hoffman18a}, feature~\cite{10021602, 8764342, Du_2019_ICCV,NIPS2016_ac627ab1, Feng_Ju_Wang_Song_Zhao_Ge_2023}, and output spaces~\cite{10.1007/978-3-030-58555-6_42, 8954439}, and they sometimes overlap with approaches base on adversarial learning as the supervisory signal to align two distributions may come from statistical distance metrics~\cite{pmlr-v37-long15,NIPS2004_96f2b50b} or a discriminator model~\cite{8764342, 8954439, pmlr-v80-hoffman18a}. Meanwhile, self-training with pseudo-label is also a prevalent technique~\cite{Zhang_2021_CVPR, zheng2021rectifying, Zou_2019_ICCV} and has shown significant improvement on natural image segmentation~\cite{hrda, daformer,Hoyer_2023_CVPR}.

In this study, we exploit the vanilla pseudo-labeling with three straightforward yet crucial measures to stabilize the training. We hypothesize that random masking is an ideal strong perturbation for consistency regularization in pseudo-labeling, and the model pretrained via MAE can be efficiently adapted to infer semantics of missing regions from visible patches. This hypothesis is justified in \cref{sec:ablation}. In addition, we leverage the anatomical distribution prior in medical images and make predictions jointly based on local and global contexts, which also help mitigate the pseudo-label drifts. We compare MAPSeg with previous SOTA frameworks and demonstrate its versatility in different UDA scenarios in the following sections.

\subsection{Federated Learning}
Federated Learning (FL) is a distributed learning paradigm that aims to train models on decentralized data~\cite{mcmahan2017communication}. FL has attracted great attention in the research community in the last few years and numerous works have focused on the key challenges raised by FL such as data/system heterogeneity~\cite{Tang_2022_CVPR} and communication/computation efficiency~\cite{fedmask}. Since medical imaging data are privacy-sensitive and regulations such as Health Insurance Portability and Accountability Act (HIPAA) and EU General Data Protection Regulation (GDPR)~\cite{MAL-083, insight_from_GDPR} restrict the sharing of patient data collected from different medical institutions, FL has been adopted for various medical image analysis tasks~\cite{guan2023federated}. Sheller \etal~\cite{sheller} pioneered FL for brain tumor segmentation on multimodal brain scans in a multi-institutional collaboration and showed its promising performance compared to centralized training. Yang \etal~\cite{YANG2021101992} proposed a federated semi-supervised learning framework for COVID-19 detection that relaxed the requirement for all clients to have access to ground truth annotations. FedMix~\cite{FedMix} further alleviated the necessity for all clients to possess dense pixel-level labels, allowing users with weak bounding-box labels or even image-level class labels to collaboratively train a segmentation model. In contrast, MAPSeg assumes all clients have completely unlabeled data when extended to FL scenario. Mushtaq \etal~\cite{DBLP:conf/isbi/MushtaqBDA23} proposed a Federated Alternate Training (\textit{FAT}) scheme that leverages both labeled and unlabeled data silos. It employs mixup~\cite{Mixup} and pseudo-labeling to enable self-supervised learning on the unlabeled participants. MAPSeg, on the other hand, adopts masked pseudo-labeling and global-local feature collaboration for adapting to unlabeled target domains. Yao \etal~\cite{fmtda} introduced the federated multi-target domain adaptation problem and an approach called \textit{DualAdapt} to solve it. It decouples the local-classifier adaptation with client-side self-supervised learning from the feature alignment with server-side mixup and adversarial training. MAPSeg addresses the same federated UDA problem, and we compare our results to those of \textit{FAT} and \textit{DualAdapt} in \cref{sec:results_FLUDA}.

\subsection{Test-Time UDA}
While federated UDA eases the constraint of centralized data, its learning paradigm still requires synchronous learning across server and clients. Test-time UDA~\cite{10.1145/2939672.2939716, Liu_2021_CVPR} assumes the unavailability of source-domain data when adapting to target domains. This assumption significantly limits the applicability of methods based on image translation, adversarial learning, and feature distribution alignment which require simultaneous access to both source and target data. We demonstrate that, beyond federated UDA, MAPSeg can be extended to test-time UDA with comparable performance on target domain similar to that of centralized UDA, while exhibiting slightly more performance drop on source domain (\cref{sec.ttdaresults}). 