{
    "arxiv_id": "2303.16604",
    "paper_title": "Bi-directional Training for Composed Image Retrieval via Text Prompt Learning",
    "authors": [
        "Zheyuan Liu",
        "Weixuan Sun",
        "Yicong Hong",
        "Damien Teney",
        "Stephen Gould"
    ],
    "submission_date": "2023-03-29",
    "revised_dates": [
        "2023-03-30"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "cs.IR",
        "cs.LG"
    ],
    "abstract": "Composed image retrieval searches for a target image based on a multi-modal user query comprised of a reference image and modification text describing the desired changes. Existing approaches to solving this challenging task learn a mapping from the (reference image, modification text)-pair to an image embedding that is then matched against a large image corpus. One area that has not yet been explored is the reverse direction, which asks the question, what reference image when modified as describe by the text would produce the given target image? In this work we propose a bi-directional training scheme that leverages such reversed queries and can be applied to existing composed image retrieval architectures. To encode the bi-directional query we prepend a learnable token to the modification text that designates the direction of the query and then finetune the parameters of the text embedding module. We make no other changes to the network architecture. Experiments on two standard datasets show that our novel approach achieves improved performance over a baseline BLIP-based model that itself already achieves state-of-the-art performance.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.16604v1"
    ],
    "publication_venue": "12 pages, 5 figures"
}