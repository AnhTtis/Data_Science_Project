\section*{Supplementary Material}
\section{Balancing the Forward and Reversed Loss Terms}\label{sec:balancing-losses}

We investigate the effect of varying the hyperparameter $\alpha$ in the bi-directional loss (\eqnref{eq:loss-total}).
As a general rule of thumb, we discover that large $\alpha$ values close to, or beyond 1.0 adversely harm the performance, which corroborates with our hypothesis on the effect of false negatives in the reversed direction in~\secref{sec:false-neg}. We, therefore, seek to balance the forward and reversed loss terms by reducing $\alpha$. We also note that the second-stage combiner training is more sensitive to tunings in $\alpha$ compared to the first stage. We suspect the reason to be related to the model capacity, as the first-stage finetuning is relatively light in architecture, while the second-stage combiner module is of much higher complexity (\figref{fig:model-0} right). To this end, the combiner could more easily, and quickly, overfit to the noise brought by the false negatives.

Our choices of $\alpha$ for each training stage on both datasets for results reported in Tables~\ref{tab:baseline_1} and~\ref{tab:baseline_0} are detailed as follows.
For Fashion-IQ~\cite{fashioniq}, in both stages, we discover that an $\alpha$ of around 0.5 is optimal. We note that for the first stage, further decreasing it to 0.4 yields a slightly better result. On CIRR~\cite{Liu:CIRR}, we find that the training consistently benefits from a relatively small $\alpha$, we set it to 0.1 in both stages.

In \figref{fig:alpha_ablation} we illustrate the effect of varying $\alpha$ on performance in the second-stage combiner training. We notice that as long as $\alpha$ sits within a certain range that is smaller than 1.0, the results are fairly robust. 

\begin{figure}[th]
  \begin{center}
    \includegraphics[trim={0pt 0pt 0pt 0pt},clip, width=0.49\linewidth]{figs_arxiv/fiq}
    \includegraphics[trim={0pt 0pt 0pt 0pt},clip, width=0.48\linewidth]{figs_arxiv/cir}
  \end{center}
  \caption[Performance \textit{vs.} $\alpha$ in the second stage bi-directional training]{Performance \textit{vs.} $\alpha$ in the second stage bi-directional training. \textbf{(Left)} Fashion-IQ validation set. \textbf{(Right)} CIRR test set. We select a few $\alpha$ values to examine the trend surrounding optimality. Note the relatively small scale in performance (y-axis), suggesting the performance is fairly robust against varying $\alpha$ within a certain range. Compare the results with Tables~\ref{tab:baseline_1} and~\ref{tab:baseline_0}.
  }\label{fig:alpha_ablation}
  \vspace{-7pt}
\end{figure}

\section{Inference on Reversed Queries}\label{sec:inference-on-reversed-queries}

\secref{sec:false-neg} details the impact of false-negatives. In~\tabref{tab:val-reversed} we demonstrate that validating on the reversed queries yields subpar results, which collaborates with our observation of a higher loss in the reversed path. This leads to our inference strategy that only takes into account the forward queries.

\begin{table*}[tp]
  \centering \scalebox{0.7}{
  \begin{tabular}{p{0.03\linewidth}p{0.2\linewidth}rrrrrrr} 
  \toprule
   &    & \multicolumn{3}{c}{\textbf{Fashion-IQ}}               & \multicolumn{4}{c}{\textbf{CIRR}} \\
  \cmidrule(lr){3-5}
  \cmidrule(lr){6-9}
  \multicolumn{1}{l}{} & \textbf{BLIP4CIR+Bi} & R@10  & R@50  & Average & R@1 &  R@5   & R$_\text{Subset}$@1  & Average  \\ 
  \midrule
  \textbf{1}  &  on forward queries  & 43.49 & 67.31 & 55.40 & 42.36 & 75.46  & 72.90 & 74.18 \\ 
  \textbf{2}  &  on reversed queries  & 23.08 & 45.05 & 34.07 & 18.08 & 49.25  & 44.51 & 46.88 \\ 
  \bottomrule
  \end{tabular}}
  \caption{Comparison of performance when validating on the forward and reversed queries. Results obtained on validation sets after the second-stage combiner training, directly comparable to results in~\tabref{tab:ablate_0}.
  }\label{tab:val-reversed}
\end{table*}

\begin{table*}[tp]
  \centering
  \scalebox{0.7}{
  \begin{tabular}{p{0.03\linewidth}lrrrrrrrrr} 
  \toprule
  \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{2}{c}{\textbf{Dress}} & \multicolumn{2}{c}{\textbf{Shirt}} &\multicolumn{2}{c}{\textbf{Toptee}} &\multicolumn{2}{c}{\textbf{Average}} & \textbf{Avg.} \\
  \cmidrule(lr){3-4}
  \cmidrule(lr){5-6}
  \cmidrule(lr){7-8}
  \cmidrule(lr){9-10}
  \multicolumn{1}{l}{} & \multicolumn{1}{l}{\textbf{Methods}} & R@10 & R@50 & R@10 & R@50 & R@10 & R@50 & R@10 & R@50 & \textbf{Metric}  \\ 
  \midrule
  \textbf{1} & BLIP4CIR~(first-stage) & 4.81 & 15.42 & 8.10 & 16.63  & 7.75 & 17.64  & 6.89 & 16.57 & 11.72   \\ 
  \textbf{2} & BLIP4CIR+Bi~(first-stage) & 22.91 & 45.96 & 23.80 & 41.22  & 27.03 & 45.44  & 24.58 & 44.20 & 34.39  \\ 
  \bottomrule
  \end{tabular}}
  \caption[Performance comparison on the reversed query retrieval with or without bi-directional training on Fashion-IQ validation set]{Performance comparison on the \textit{reversed query} retrieval with or without bi-directional training, Fashion-IQ validation set. We report the average Recall@10 and 50 of all three categories. Note that the comparison is on the first-stage text encoder finetuning (\figref{fig:model-0} left).}
  \label{tab:main1-fiq_reversed_val}
\end{table*}


% \shadowoffset{2pt}
\setlength{\fboxsep}{0.75pt}
\begin{figure*}[tp]
  \centering\scriptsize
  \begin{minipage}{0.99\linewidth}
    \centering
    \setlength{\tabcolsep}{1.5pt}
    \begin{tabular}{p{0.17\linewidth}ccccc}
      \textbf{(a)}& \multicolumn{5}{l}{\texttt{[BACKWARD]} Add one more deer and add some sunlight.}
      \\[.8ex]
      \frame{\includegraphics[height=12.5ex]{figs_arxiv/quali/dev-985-0-img0}}& 
      \textcolor{ForestGreen}{\fboxrule=2pt\fbox{\includegraphics[trim={0 0 0 0}, clip, width=14ex, height=12.5ex]{figs_arxiv/quali/dev-1010-3-img1}}}&
      \frame{\includegraphics[width=14ex, height=12.5ex]{figs_arxiv/quali/dev-1038-1-img0}}&
      \frame{\includegraphics[width=14ex, height=12.5ex]{figs_arxiv/quali/dev-499-1-img0}}& 
      \frame{\includegraphics[width=8ex, height=12.5ex]{figs_arxiv/quali/dev-720-2-img0}}&
      \frame{\includegraphics[width=14ex, height=12.5ex]{figs_arxiv/quali/dev-878-0-img1}}
      \\[2ex]
      
      \textbf{(b)}& \multicolumn{5}{l}{\texttt{[BACKWARD]} Put the fries in a white plate with white background, clean.}
      \\[.8ex]
      \frame{\includegraphics[height=12.5ex]{figs_arxiv/quali/dev-104-1-img0}}& 
      \textcolor{ForestGreen}{\fboxrule=2pt\fbox{\includegraphics[width=15ex, height=12.5ex]{figs_arxiv/quali/dev-104-3-img1}}}&
      \frame{\includegraphics[width=16ex, height=12.5ex]{figs_arxiv/quali/dev-187-2-img0}}& 
      \frame{\includegraphics[height=12.5ex]{figs_arxiv/quali/dev-155-3-img1}}&
      \frame{\includegraphics[width=15ex, height=12.5ex]{figs_arxiv/quali/dev-212-2-img0}}&
      \frame{\includegraphics[width=16ex, height=12.5ex]{figs_arxiv/quali/dev-642-1-img1}}
      \\[2ex]
      
      \textbf{(c)}& \multicolumn{5}{l}{\texttt{[BACKWARD]} Change the plate to rectangular.}
      \\[.8ex]
      \frame{\includegraphics[trim={0 0 0 0}, clip, height=12.5ex]{figs_arxiv/quali/dev-607-3-img0}}& 
      \frame{\includegraphics[height=12.5ex]{figs_arxiv/quali/dev-332-1-img0}}& 
      \textcolor{ForestGreen}{\fboxrule=2pt\fbox{\includegraphics[width=16ex, height=12.5ex]{figs_arxiv/quali/dev-187-2-img0}}}&
      \frame{\includegraphics[trim={0 0 0 0}, clip, height=12.5ex]{figs_arxiv/quali/dev-607-1-img0}}&
      \frame{\includegraphics[trim={0 0 0 0}, clip, width=16ex,  height=12.5ex]{figs_arxiv/quali/dev-187-0-img0}}&
      \frame{\includegraphics[width=16ex, height=12.5ex]{figs_arxiv/quali/dev-607-0-img0}}
      \\[2ex]
      
      \textbf{(d)}& \multicolumn{5}{l}{\texttt{[BACKWARD]} Fewer paper towels per pack.}
      \\[.8ex]
      \frame{\includegraphics[width=12ex, height=12.5ex]{figs_arxiv/quali/dev-430-3-img0}}& 
      \textcolor{ForestGreen}{\fboxrule=2pt\fbox{\includegraphics[width=14ex, height=12.5ex]{figs_arxiv/quali/dev-63-0-img1}}}&
      \frame{\includegraphics[width=16ex, height=12.5ex]{figs_arxiv/quali/dev-153-2-img1}}&
      \frame{\includegraphics[width=9ex, height=12.5ex]{figs_arxiv/quali/dev-1035-0-img1}}&
      \frame{\includegraphics[width=16ex, height=12.5ex]{figs_arxiv/quali/dev-1028-1-img1}}&
      \frame{\includegraphics[width=16ex, height=12.5ex]{figs_arxiv/quali/dev-430-3-img1}}
      \\[2ex]
      
    \end{tabular}
    \end{minipage}\\[5pt]

    \caption[Qualitative examples of reversed query retrieval on CIRR]{Qualitative examples of \textit{reversed query} retrieval on the first-stage text encoder finetuning (CIRR). 
    In each example, leftmost is target image, green box denotes the ground truth (reference image), the reversed modification text is provided above the images. We show the top-5 candidates in ranking.
    Note that the reference image and target image exchange roles here and that the modification text shall be interpreted in its reversed semantic --- for this, we specifically show the prepended text token.
  }\label{fig:main1-qualitative-reversed-cirr}
\end{figure*}

\section{Analysis on the Learned Reversed Semantics}\label{sec:analysis-on-not-tokens}
We perform both quantitative and qualitative analyses to examine if our bi-directional training is encouraging the learning of the reversed semantics.
Specifically, \tabref{tab:main1-fiq_reversed_val} (rows 2 \textit{vs.} 1) compares the retrieval performance on the Fashion-IQ reversed queries with or without bi-directional training.
We examine the model after the first-stage text encoder finetuning, as in~\figref{fig:model-0} (left).
The result suggests that a model specifically trained with bi-directional queries is better equipped at reasoning over reversed semantics, which substantiates our claim.
However, note that the performance on said queries is generally much lower than on the (standard) forward ones due to the larger number of potential false negatives, which has been discussed in~\secref{sec:false-neg}.

We additionally present four qualitative examples of CIRR retrieved on the reversed queries.
In~\figref{fig:main1-qualitative-reversed-cirr} (a) and (d) where the reversed text is unambiguous (\ie ``add'' is negated to ``remove'', ``fewer'' is negated to ``more''), we show the model is capable of reasoning over such reversed semantics. 
We demonstrate a more complicated case in (b), where one might not definitively predict the ground truth content by examining the query. Still, among the top-5 ranked candidates, we argue that the model produces a plausible result, with the ground truth ranked the highest.

We especially illustrate the existence of false negatives among candidates in~\figref{fig:main1-qualitative-reversed-cirr} (c) --- though the issue is present in multiple examples. 
Here, in particular, ``change to rectangular'' shall be reversed to ``change \textit{from} rectangular'', which points to a range of possible shapes. Indeed, the top-5 ranked candidates all contain non-rectangular plates --- though only one of them is labelled positive. 
Here, we note that not all such reversed examples with false negatives can be successfully retrieved.  Evidence can be seen when comparing the performance on the reversed queries (\tabref{tab:main1-fiq_reversed_val} row 2) to the performance on the forward ones (\tabref{tab:baseline_1} row 19), where the former is much lower than the latter.
This further validates our decisions to not perform inference on the reversed queries (\secref{sec:false-neg}) and to downscale the reversed contrastive loss (\secref{sec:balancing-losses}).

