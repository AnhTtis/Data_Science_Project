\section*{Supplementary Material}
\section{Implementation Details}

\paragraph{Balancing the Reversed Loss.}
Here we provide details on determining the hyperparameter $\alpha$ in the loss mentioned in~\eqnref{eq:loss-total}.
As discussed in~\secref{sec:false-neg}, due to the false negative issue in the reversed path, the magnitude of the reversed loss term $\mathcal{L}_\text{B}$ is generally higher, and one shall pay special attention to balancing it with the forward loss term $\mathcal{L}_\text{F}$ such that the reversed queries do not inadvertently harm the training. 
We also note that the second-stage combiner training is more sensitive to tunings in $\alpha$ compared to the first stage. We suspect the reason to be related to the model capacity, as the first-stage finetuning is relatively light in architecture, while the second-stage combiner module is of much higher complexity (\figref{fig:model-0}). To this end, the combiner could more easily, and quickly, overfit the noise brought by the false negatives.

\tabref{tab:hyperparameter-alpha} lists the $\alpha$ values used in our training on both datasets. We conduct a grid search to identify the optimal $\alpha$.
For Fashion-IQ, in both stages, we discover that an $\alpha$ of around 0.5 is optimal, while for the first stage, further decreasing it to 0.4 yields slightly better results. On CIRR, we find that the training consistently benefits from a relatively small $\alpha$.

\begin{table}[h!]
  \centering \scalebox{0.85}{
  \begin{tabularx}{\columnwidth}{YYYYY} 
  \toprule
           & \multicolumn{2}{c}{\textbf{Fashion-IQ}}               & \multicolumn{2}{c}{\textbf{CIRR}} \\
  \cmidrule(lr){2-3}
  \cmidrule(lr){4-5}
   &  Stage-I & Stage-II  &  Stage-I & Stage-II \\ 
  \midrule
  \textbf{$\alpha$} & 0.4 & 0.5  & 0.1 & 0.1 \\
  \bottomrule
  \end{tabularx}}
  \caption{$\alpha$ values used in our experiments.
  }\label{tab:hyperparameter-alpha}
\end{table}


\section{Inference on Reversed Queries}

\secref{sec:false-neg} details the impact of false-negatives. In~\tabref{tab:val-reversed} we demonstrate that validating on the reversed queries yields subpar results, which collaborates with our observation of a higher loss in the reversed path. This leads to our inference strategy that only takes into account the forward queries.

\begin{table*}[h!]
  \centering \scalebox{0.80}{
  \begin{tabular}{llrrrrrrr} 
  \toprule
   &    & \multicolumn{3}{c}{\textbf{Fashion-IQ}}               & \multicolumn{4}{c}{\textbf{CIRR}} \\
  \cmidrule(lr){3-5}
  \cmidrule(lr){6-9}
  \multicolumn{1}{l}{} & \textbf{BLIP4CIR+Bi} & R@10  & R@50  & Average & R@1 &  R@5   & R$_\text{Subset}$@1  & Average  \\ 
  \midrule
  \textbf{1}  &  on forward queries  & 43.49 & 67.31 & 55.40 & 42.36 & 75.46  & 72.90 & 74.18 \\ 
  \textbf{2}  &  on reversed queries  & 23.08 & 45.05 & 34.07 & 18.08 & 49.25  & 44.51 & 46.88 \\ 
  \bottomrule
  \end{tabular}}
  \caption{Comparison of performance when validating on the forward and reversed queries. Results obtained on validation sets, as in Table 3. 
  }\label{tab:val-reversed}
\end{table*}

\begin{figure}[t!]
  \begin{center}
    \includegraphics[trim={0pt 0pt 10pt 32pt},clip, width=0.48\linewidth]{figs/plt_val_recall_subset1.pdf}
    \includegraphics[trim={0pt 0pt 10pt 32pt},clip, width=0.48\linewidth]{figs/plt_val_recall_5.pdf}
  \end{center}
  \caption{Validation performance on \textbf{(left)} Recall$_\text{Subset}$@1 and \textbf{(right)} Recall@5 throughout training (200 epochs). Results obtained on the CIRR validation set. \textbf{BLIP4CIR:} baseline model that uses combiner architecture. \textbf{BLIP4CIR+Bi:} our method that involves the bi-directional training scheme.
  }\label{fig:val-recall-subset-1}
\end{figure}

\section{Recall$_\text{Subset}$ on CIRR}

As mentioned in~\secref{sec:exp-cirr}, we notice that the state-of-the-art architecture (\ie combiner~\cite{Baldrati_2022_CVPR_clip4cir1}) learns poorly on the more challenging Recall$_\text{Subset}$@$K$ metric.
Here, we illustrate its performance on Recall$_\text{Subset}$@1 throughout training.
As shown in~\figref{fig:val-recall-subset-1} left (blue), the combiner effectively fails in learning on said metric, with the performance gradually decreasing. We also note the high fluctuations observed among epochs, which is partially due to the high granularity of the metric (as discussed in~\secref{sec:exp-cirr}).
To compare, we overlay the validation curve obtained using our bi-directional training scheme, as in~\figref{fig:val-recall-subset-1} left (orange). We note that globally, our performance in Recall$_\text{Subset}$@1 is on par with the baseline. However, since we follow previous work~\cite{Liu:CIRR} to select the best-performing epoch via an average score of Recall@5 and Recall$_\text{Subset}$@1, the reported performance in~\tabref{tab:baseline_0} and~\tabref{tab:ablate_0} may not necessarily be optimal on each individual metric.

We stress that our bi-directional training scheme does \textit{not} aim at improving the existing model architecture. Hence, it is unsurprising that our method inherits the issues on Recall$_\text{Subset}$@$K$ from the combiner baseline. 
Given the characteristics of the validation curves, we argue that the performance comparison between the baseline and our method on Recall$_\text{Subset}$@$K$ (\tabref{tab:baseline_0}) shows little insight. We instead point to other metrics, such as Recall@5 or 10 for comparison, as discussed in~\secref{sec:exp-cirr}. As an example, we showcase the validation curve on Recall@5 through training (\figref{fig:val-recall-subset-1} right).