\section{Experiments}
\label{sec:exp}
In this section, we describe the corpus, experimental setup and results.

\subsection{Corpus}
The corpus used in this work comes from the Alzheimerâ€™s Dementia Recognition Through Spontaneous Speech (ADReSS) Challenge 2020 corpus~\cite{luz2020alzheimer}. This challenge selects a sub-task of Pitt Corpus in the DementiaBank database~\cite{becker1994natural}, which requires all the participants to describe the Cookie Theft picture as shown in Fig.~\ref{fig:cookie}. The ADReSS corpus consists of 156 different English speakers' audio samples with corresponding transcripts. Among them, 78 of the speakers are healthy control (35 male, 43 female) while the rest are with AD (35 male, 43 female). The corpus is divided into a standard train (108 speakers, about 2 hours) and test (48 speakers, about 1 hour) sets with balanced distributions of age, gender and disease conditions.


\subsection{Experimental Setup}

\subsubsection{Data Preprocessing}
% Firstly, to better extract the audio features, we used the FullSubNet toolkit~\cite{hao2021fullsubnet} to enhance the audio. Then, all the audio samples are sliced into 30-second segments with a hop ratio of 0.25 and averaged the predicted detection probabilities for the same speaker. Finally, we evaluated the model performance by classification accuracy and F1 (macro) scores on the unseen ADReSS test data.
% pre-processing
At the beginning of the experiments, we preprocess the speech with enhancement~\cite{hao2021fullsubnet} and normalization methods for internal consistency of the data. In addition, we use data augmentation to enrich the data and improve the robustness by slightly changing the acoustic characteristics with minor distortions. Specifically, we used three strategies, pitch-shifting, speed-perturbation, and dithering for each input waveform during the training stage~\cite{cariani1996neural,colosi1998efficient,schuchman1964dither}.
The shifted ranges of pitch and speed are [-100, 100] semitones and [-0.05, +0.05] rates, respectively.

% long form data
Notably, the data on AD investigated in this work are in long-form, e.g., several minutes of spontaneous speech associated with transcripts from the picture description task. However, the inputs of the high-level pretrained models are usually within 30 seconds for audio or 512 tokens for text. To deal with this issue, we slice the experimental audio into 30-second segments with a hop ratio of 0.25, and obtain the aligned transcripts that are less than 512 tokens. We then aggregate the extracted segment-level features for each speaker.

\subsubsection{Model \& Training Details}
The variants of the Whisper and BERT models adopted in this work are respectively the small and base-uncased versions pretrained on English corpora , both of which output 768-dimensional embeddings.
The projector is composed of two stacked 8-dimensional linear layers with layer normalization, and the classifier is a fully-connected linear classifier. We insert a dropout layer with a rate of 0.25 between the projector and temporal pooling layer.

The training loss of this work is set to be the binary cross-entropy loss. We use AdamW~\cite{loshchilov2017decoupled} as our optimizer with a learning rate of $1e-4$ and a weight decay of $1e-5$. The models are trained with a batch size of 16 for 50 epochs.


\subsubsection{Evaluation Protocols}

We evaluate the model performance on the unseen ADReSS test data, with the metrics of classification accuracy and macro F1 scores, the mean of class-wise F1-scores, that averaged on five random runs.
To better compare our proposed methods with previous literature, we choose multiple baselines, including Luz \textit{et al.} which used ComParE and Linguistic feature sets~\cite{luz2020alzheimer}, 
Koo~\cite{koo2020exploiting} and Syed \textit{et al.}~\cite{syed2021automated} which used acoustic pretrained models, and Yuan~\cite{yuan2020disfluencies}, Matej~\cite{martinc2021temporal} and Yi \textit{et al.}~\cite{wang2022exploring} which used linguistic pretrained models.
% To extract the wave features, we utilize Wav2vec 2.0, Hubert, and WavLM, three commonly used SSL speech models. We take the "large" variants of three models in our work, which contain 1024-dimensional embedding output. In addition, for the Whisper model, we apply the variant of  ``small.en'' with the outputs of 768-dimensional embeddings. After extracting the hidden representations from the pretrained model, we placed two stacked 8-dimensional linear layers with layer normalization each as the projector before a fully-connected linear classifier. Also, dropout layers with a rate of 0.25 are inserted before the projector and the classifier, respectively, for regularization. Finally, we explored two ways of aggregating information from the layers of pretrained models. One is called ``weighted sum (WS)'', which add up all element with a certain weight. Another is ``maximum single (MS)'', which selects one single layer with the best performance.
% Also, we analyze the difference of using the attentive temporal pooling and mean values across time steps(denoted as ``Mean'' ) to refine our experiment.
% Our proposed methods are compared with previous work in the literature on the same ADReSS dataset, including models based on conventional acoustic features \cite{luz2020alzheimer}, VGGish embeddings \cite{koo2020exploiting} and fusion of OpenL3 embeddings \cite{syed2021automated}.


% \subsubsection{Training hyperparameters}
% The training loss of this work is set to be the cross-entropy loss. We use the AdamW~\cite{loshchilov2017decoupled} as our optimizer with a weight decay of $1e-5$. During the training, the pretrained models are frozen, and the subsequent modules will be fine-tuned with a learning rate of $1e-4$ and a batch size of 16 for 50 epochs.


