\section{Introduction}
\label{sec:into}

% background -> importance of multimodal -> related works -> research gap (motivation) -> proposed work
Alzheimer’s disease (AD) is a progressive neurodegenerative disease that causes gradual, irreversible deterioration in cognitive domains (memory, communication, etc.). Since there is no effective treatment for AD currently, early detection of this disease is particularly crucial for timely intervention and better disease control~\cite{mueller2005ways,rasmussen2019alzheimer}. Previous studies~\cite{appell1982study,cummings1988alzheimer,croot2000phonological,gayraud2011syntactic} have shown that symptoms of AD may be observable in spoken language at a very early stage, such as temporal disfluency, and difficulties in word finding and retrieval. These studies lay the theoretical foundation for using acoustic and linguistic information to screen for AD, which is attracting increasing interest from international research community.

% Compared to conventional diagnosis methods, e.g., Magnetic Resonance Imaging and Fluid Analysis~\cite{gainotti2014neuropsychological}, spoken language-based diagnosis methods are cheaper, more convenient, and have high potential for large-scale screening.
%Tremendous research efforts have been devoted to investigating AD detection using speech (the acoustic signal) and language (words and sentences) features as biomarkers \cite{fraser2016linguistic,weiner2019speech,pulido2020alzheimer,luz2020alzheimer,luz2021detecting,frankenberg2021verbal,meghanani2021exploration,qiao2021alzheimer}.
% For example, Weiner and Frankenberg \textit{et al.} \cite{weiner2019speech,frankenberg2021verbal} explored a lot of traditional acoustic and linguistic features with nested forward feature selection method, and found that the features on parts-of-speech, word categories and pauses are highly related to the Alzheimer’s disease.

The use of spoken language to screen for AD offers the advantages of affordability, accessibility and hence scalability, compared to conventional methods such as brain scans, blood tests and face-to-face neuropsychological assessments~\cite{gainotti2014neuropsychological}. Active research efforts are being devoted to finding spoken language features (both audio and linguistic features) as biomarkers of AD~\cite{fraser2016linguistic,weiner2019speech,pulido2020alzheimer,frankenberg2021verbal}.
For example, Weiner and Frankenberg \textit{et al.} \cite{weiner2019speech,frankenberg2021verbal} explored many traditional acoustic and linguistic features with a nested forward feature selection method, and found that the features on parts-of-speech, word categories and pauses are highly related to AD. Hence, possible approaches to design advanced algorithms to extract powerful acoustic and linguistic features to diagnose Alzheimer’s disease have become an emerging topic.

% \textcolor{red}{related works using speech and language} Some previous work successfully prove that the pretrained representations can be pivotal to audio-based AD detection \cite{koo2020exploiting,balagopalan2021comparingAcoustic,syed2021automated}.
% For example, Koo \textit{et al.} \cite{koo2020exploiting} obtain a better performance than traditional acoustic features by using the averaged features trained for audio classification, which is called VGGish~\cite{hershey2017cnn}.
%  Also, Balagopalan \textit{et al.} \cite{balagopalan2021comparingAcoustic} organically integrate the convetional MFCC features with the pretrained Wav2Vec 2.0.
% Syed \textit{et al.} \cite{syed2021automated} used an ensemble method by fusing the embeddings of OpenL3-Music and Environment variants \cite{cramer2019look}.
%pretrained acoustic embeddings \cite{koo2020exploiting,balagopalan2021comparingAcoustic}

Inspired by the success of pretrained models, especially in speech (e.g., VGGish~\cite{koo2020exploiting}, Wav2Vec 2.0~\cite{balagopalan2021comparingAcoustic}, OpenL3~\cite{syed2021automated}) and text (e.g., BERT~\cite{balagopalan2020bert}, ERNIE~\cite{yuan2020disfluencies}, Glove~\cite{martinc2021temporal}), the development of AD detection is shifting from low-level features to higher-level representations in pretrained models. Although the BERT-like models have achieved promising performance on the AD detection task~\cite{yuan2020disfluencies,balagopalan2020bert,li2021comparative,wang2022exploring}, we note that these works mainly used higher-level representations of pretrained models, while features from intermediate layers may not have been devoted sufficient attention.
It is shown that intermediate layers encode rich hierarchical information for various features, e.g., surface features at the bottom, syntactic features in the middle and semantic features at the top~\cite{jawahar2019bert}.
Therefore, exploring and leveraging different levels of pretrained representations for better AD detection task is worthy of investigation.

% Jawahar \textit{et al.}~\cite{jawahar2019bert} pointed out that top layers mainly capture features related to semantics, while middle and bottom layers extract syntactic and surface features for linguistic pretrained models. Similarly, Pasad \textit{et al.}~\cite{pasad2021layer} pointed out the top layers extract more semantic information than the bottom layers for the acoustic pretrained models.
% Therefore, how to combine features from different layers of pretrained models to better capture AD-related features for screening is worthy of investigation.

%With the recent advancements in pretrained representations, the AD detection are shifting from aforementioned low-level features to the high-level pretrained representations, such as acoustic VGGish~\cite{koo2020exploiting}, Wav2Vec 2.0~\cite{balagopalan2021comparingAcoustic}, OpenL3~\cite{syed2021automated}, and linguistic BERT~\cite{balagopalan2020bert}, ERNIE~\cite{yuan2020disfluencies}.
%Although the BERT-like models achieve promising performance on the AD detection task \cite{yuan2020disfluencies,balagopalan2020bert,li2021comparative,wang2022exploring}, the investigation of the high-level pretrained representations are still underlooked.
%For example, different layers from the pretrained models contains different information, but most works usually selected from the last layer; the AD cues may be obvious at some certain time, but most works usually averaged along the time dimension.
%This motivates us to investigate better AD-related information from different layers and time-steps in the pretrained representations.

% These high-level representations may be also fallacious and sensitive due to the limitation of the target data size and the large mismatch between the source and the target domains \cite{novikova2021robustness}.

% \textcolor{red}{Importance of multimodal (motivation)}

% Moreover, it is also noted that the detection of AD is an inherently multimodal task, e.g., the clinical cognitive assessments usually contain tasks from memory, language, and visual aspects~\cite{sachdev2014classifying}.
% Information from different modalities may offer complementary cues and enhanced robustness for AD detection, and thus motivates investigations in multimodal fusion.
% For example, Martinc \textit{et al.}~\cite{martinc2021temporal} combined the acoustic temporal information with the structural textual embeddings as bag-of-words, Zhu \textit{et al.}~\cite{zhu2021wavbert} investigated the Automatic Speech Recognition model and the language model with a conversion network. However, the performance of pretrained acoustic models are usually inferior to pretrained textual models for the AD detection task, which opens up room for investigation.
% However, due to the limitation and large variance of pretrained acoustic models, the combination of pretrained acoustic and linguistic models may not be necessary or beneficial and opens up room for investigation.

The acoustic and linguistic features are typically used to distinguish the people with Alzheimer's disease from healthy controls, modeling the richness or disorder of participants' utterances to some extent. In addition to these features, it is also important to model the correctness and pertinence of participants' utterances for the cognitive tasks.
For example, in the widely used Cookie Theft Picture Description task, where people are asked to describe everything happening in a picture, Laura \textit{et al.} proposed information coverage measured by the statistics of the text and predefined referent~\cite{hernandez2018computer}.
This motivates us to propose features for modeling both the characteristics of disorder and the pertinence to the cognitive tasks.

% Furthermore, it is also noted that the visual prompt from cognitive tests could help examine the participants' cognitive status. For example, in the widely used Cookie Theft Picture Description Task, the participants are asked to look at a complex picture and describe what they see. The clinical test is usually scored based on the utterances that are generated in the description, and by measuring the number of keywords, simple/complex and complete/incomplete sentences \cite{giles1996performance}.
% This motivates us to explore the use of embeddings to model the pertinence and richness between the key components in the picture and the description in the participant's spoken utterances.


% \textcolor{red}{Proposed work, results, contribution, paper organisation}
% 1. improve audio performance a lot
% 2. effective combination of audio and text (visualize attn weights before and after fusion)
% 3. add information from visual prompt

In this work, we propose several efficient strategies to extract AD-related cues from embeddings of pretrained models, including aggregation along the layer and time dimension. We also use these extracted embeddings to measure task-related pertinence by correlation operation.
% We also investigate embeddings from different modalities and the pertinence between picture-based keywords and spoken utterances.
To validate the effectiveness of our proposed method, we conduct experiments on the Alzheimer’s Dementia Recognition through Spontaneous Speech (ADReSS) dataset \cite{luz2020alzheimer} with a binary classification setup. The proposed models are evaluated with accuracy and F1 scores, and obtain comparable results over other state-of-the-art methods in recent literature.
% Specifically, the unimodal methods obtain accuracy scores of 88.79\%, 90.09\% and 85.94\% for acoustic, linguistic and task-pertinence methods, respectively. And a combination of these features achieves an accuracy of 91.41\%.
It's also exciting to find that the acoustic representations are comparable to the linguistic features, which could be more robust and generalizable in multilingual tasks and more helpful for fully automatic AD detection tasks.

The rest of the paper is organized as follows. Section \ref{sec:method} introduces the proposed methods for modeling with acoustic, linguistic and visual embeddings. Then, Section \ref{sec:exp} describes the experimental setups and results, as well as further analysis of the proposed methods and comparison with recent literature. Finally, Section \ref{sec:conclusion} concludes the paper and presents possible future research directions.
