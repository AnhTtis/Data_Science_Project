\section{Methodology}
\label{sec:method}
This section presents our methodology. Firstly, we describe the high-level embeddings for the audio and the text modalities. Then we introduce the measurement of task-related pertinence using these embeddings. And finally, we illustrate the feature extractor and classifier for the AD detection task.


\subsection{Acoustic \& Linguistic Embeddings}
To obtain rich characteristics for the AD detection, we adopt different pretrained models for speech and text to extract acoustic and linguistic features respectively. 

For the acoustic features, we compared several self-supervised (SSL) pretrained models (e.g., Wav2Vec 2.0~\cite{baevski2020wav2vec}, HuBERT~\cite{hsu2021hubert}, WavLM~\cite{chen2022wavlm}), and a weakly-supervised Whisper model~\cite{radford2022robust}.
The Wav2vec 2.0 is a model that jointly learns contextualized speech representations and an inventory of discretized speech units.
The HuBERT introduces a prior lexicon based on offline clustering to provide pseudo labels for speech units. The model is trained to predict the cluster assignments from the input speech units, which encourages the model to learn a combined acoustic and language model.
In order to solve full-stack downstream speech tasks, WavLM jointly learns masked speech prediction and denoising, by using some simulated noisy or overlapped
speech data \cite{chen2022wavlm}. The gated relative position bias is utilized to capture the sequence ordering of input speech better. With these improvements, WavLM is effective for not only the ASR task, but also several downstream tasks~\cite{horiguchi2020end}.

However, the speech of individuals with AD may differ from that of a general speaker, which may affect the performance of the self-supervised pretrained models on AD detection. A recent work, named Whisper~\cite{radford2022robust}, has achieved state-of-the-art performance on many ASR tasks, which is trained on a web-scale 680,000 hours in a weakly supervised multilingual multitask fashion. Therefore, compared with previous models, Whisper is more advantageous and robust for application to different downstream tasks. Considering these merits of Whisper in processing speech tasks, we adopt it as the default acoustic feature extractor. For linguistic features, we choose BERT~\cite{devlin2018bert}, a Transformer-based~\cite{vaswani2017attention} pretrained model, as the basic backbone network, which adopts masked language modeling and next sentence prediction as the pre-training objective. 

Whisper calculates the logarithm Mel-Spectrum and then connects with two Convolution layers, followed by the Transformer layers. We feed audio into Whisper and use its produced features as acoustic representation. So for each second of audio, we can obtain features from different Transformer layers, which have a dimension of $50 \times H_a \times L_a$ (dimensions of time, feature and layer respectively). And for the text modality, we feed the transcribed text from the audios of patients into BERT model to obtain the features as $1 \times H_t \times L_t$ of each token.
% \cite{radford2022robust}
%The Whisper is a robust ASR model trained on a web-scale 680,000 hours in a weakly supervised multilingual multitask fashion.. It is competitive with other fully supervised ASR systems on several common benchmarks and especially obtains stable performances in zero-shot settings without any fine-tuning, which demonstrates its strong robustness and generalization.

%Whisper~\cite{radford2022robust} and BERT~\cite{devlin2018bert} as the acoustic and linguistic features extractors respectively.

%For acoustic features extraction, we adopt Whisper~\cite{radford2022robust}. The Whisper is a robust ASR model trained on a web-scale 680,000 hours in a weakly supervised multilingual multitask fashion. It is competitive with other fully supervised ASR systems on several common benchmarks and especially obtains stable performances in zero-shot settings without any fine-tuning, which demonstrates its strong robustness and generalization. BERT is a Transformer-based pretrained model, which adopts masked language modeling and next sentence prediction as the pre-training objective. BERT is a milestone in NLP and has achieved remarkable progress in natural language understanding tasks. 

% Whisper & BERT

%While BERT is a ubiquitous language model that pretrained on language modeling and next sentence prediction tasks. It is also competitive on a number of natural language understanding tasks.

% The variants of the Whisper and BERT models we adopted in this work are the ``small.en'' and ``bert-base-uncased'' versions respectively.
%Both of the Whisper and BERT models are based on the Transformer encoder and decoder blocks~\cite{vaswani2017attention}. Here we only use the intermediate inputs or outputs from the Transformer encoder layers as the acoustic or linguistic embeddings.
%For the audio modality, the Whisper model calculates the logarithm Mel-Spectrum and then convoluted with two Convolution layers, followed by the Transformer encoders. So for each second of audio, we can obtain embeddings in a dimension of $50 \times H_a \times L_a$ (dimensions of time, feature and layer respectively).
%While for the text modality, the dimension is $1 \times H_t \times L_t$ for each token.

\subsection{Task-related Information}
\input{figs/cookie}

For the Cookie Theft Picture Description Task, we aim to integrate information from the picture using a series of pre-defined keywords, as shown in Fig.~\ref{fig:cookie}. The keywords include the named entities (nouns) and actions (verbs) happening in the picture.
We calculate the correlation between the linguistic embeddings of the spoken utterances (describing the picture) and the pre-defined keywords for the picture.
Formally, let the extracted embeddings of spoken utterances and task-related keywords be $z_u$ and $z_k$, then the task-related correlation is $Corr. = z_u \times z_k$ by element-wise production.


\subsection{Feature Extractor}

\input{figs/feature_extractor}

As mentioned earlier, the raw embeddings extracted from the Whisper or BERT for the two modalities are large and redundant. To further condense the information which should be helpful for the AD detection task, we propose several subsequent modules as illustrated in Fig.~\ref{fig:feat}.
First, we perform layer aggregation to calibrate layer-wise feature responses. Here, we adopt two strategies: the first is ``weighted sum (WS)'', which sums up all elements with learnable weights; the second is ``maximum single (MS)'', which selects one single layer with the best performance.
Second, we project the calibrated features into a lower dimensionality to reduce feature redundancy while retaining intra-class variability. The projector is an MLP with Layer-Normalization, which can also map audio and text modalities into the same dimensional space for fusion.
Third, we use an attentive temporal pooling layer~\cite{santos2016attentive} to compress the sequence with variant time lengths into a fixed-length vector, and capture richer statistics of the temporal features for the AD detection task.
Finally, we average the features from different segments for each speaker.


\subsection{Classifier}

The extracted features are either fed into a classifier directly or combined with others with concatenation operation. The classifier used in this work is a fully-connected layer that produces the probabilities of binary classification between an individual having AD or being a Healthy Control (HC).
