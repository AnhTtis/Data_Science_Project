\documentclass[10pt,twocolumn]{article}

% \usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algpseudocode}
\usepackage{algorithm}
% \usepackage{pdfpages}


\newcommand{\gnote}[1]{{\color{red}{[Guy: #1]}}}
\newcommand{\ynote}[1]{{\color{blue}{[Yossi: #1]}}}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,,colorlinks,bookmarks=false]{hyperref}

% \iccvfinalcopy % *** Uncomment this line for the final submission

% \def\iccvPaperID{10925} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{EPiC: Ensemble of Partial Point Clouds for Robust Classification}

\author{Meir Yossef Levi, Guy Gilboa\\
Viterbi Faculty of Electrical and Computer Engineering \\
Technion - Israel Institute of Technology, Haifa, Israel\\
{\tt\small me.levi@campus.technion.ac.il ; guy.gilboa@ee.technion.ac.il}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Guy Gilboa\\
% Viterbi Faculty of Electrical and Computer Engineering\\
% Technion - Israel Institute of Technology, Haifa, Israel\\
% {\tt\small guy.gilboa@ee.technion.ac.il}
}

\maketitle
% Remove page # from the first page of camera-ready.
% \ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
    Robust point cloud classification is crucial for real-world applications, %nevertheless, there is lack of literature in robust classification, relative to classic classification.
    as consumer-type 3D sensors often yield partial and noisy data, degraded by various artifacts.
    %it is imperative to develop highly robust processing algorithms. 
    In this work we propose a general ensemble framework, based on partial point cloud sampling. Each ensemble member is exposed to only partial input data. 
    %It is based on classification of partial point clouds, which are differently sampled at inference.
    Three sampling strategies are used jointly, two local ones, based on patches and curves, and a global one of random sampling.
    
    We demonstrate the robustness of our method to various local and global degradations.
    We show that our framework significantly improves the robustness of top classification netowrks by a large margin.
    %as basic models, all of them robustified with a large margin, and most of them improved performance on the clean set accuracy. 
    Our experimental setting uses the recently introduced ModelNet-C database by Ren et al.\cite{modelnetc}, where we reach SOTA both on unaugmented and on augmented data. Our unaugmented  mean Corruption Error (mCE) is 0.64 (current SOTA is 0.86) and 0.50 for augmented data 
    (current SOTA is 0.57).
    %version (with WolfMix\cite{pointwolf, rsmix}) of RPC\cite{modelnetc} we reach . 
    We analyze and explain these remarkable results through diversity analysis. Our code is availabe at: \url{https://github.com/yossilevii100/EPiC}
    %conducted extensive analysis showing the diversity of our sub-sampling ensemble mechanism comparing to current ensemble methods. Moreover, our ensemble is efficient in terms of hardware consumption since we uses only three-headed trained networks. In order to reduce memory consumption even more we showed that shallower version of DGCNN\cite{dgcnn} yield similar results.
\end{abstract}

\begin{figure}[t!]
    \centering
    \captionsetup[subfigure]{justification=centering}
    \begin{subfigure}[t]{0.1\textwidth} %{0.25\textwidth}
\includegraphics[width=1\textwidth]{figures/random_bottle.pdf}
\caption{Random}
        \label{subfig:random_sub_sampling}
    \end{subfigure}
    \begin{subfigure}[t]{0.1\textwidth} %{0.25\textwidth}
\includegraphics[width=1\textwidth]{figures/patch_bottle.pdf}
\caption{Patch}    
        \label{subfig:patch_sub_sampling}
    \end{subfigure}
    \begin{subfigure}[t]{0.1\textwidth} %{0.25\textwidth}
\includegraphics[width=1\textwidth]{figures/curve_bottle2.pdf}
\caption{Curve}
        \label{subfig:curve_sub_sampling}
    \end{subfigure}
    % \caption*{Numeric generation of a minimal set and the corresponding flowbox coordinates}
    \caption{{\bf EPiC concept.} Three sampling mechanism are used in our ensemble: \textbf{Random} captures global information, \textbf{Patch} holds full local resolution, and  \textbf{Curve} is more exploratory in nature. {\bf Blue} - anchor point, {\bf Red} - sampled points. We show and explain why such ensembles are highly robust to various corruptions.
    }
    \label{fig:sampling}
\end{figure}
%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

A major obstacle in data-driven algorithms is the strong relation between performance and precise knowledge of input statistics. A way to test network robustness is to create corrupted test sets, where training is unaware of the specific corruptions. In recent years this has been investigated for images, with the creation of corrupted benchmarks e.g.: ImageNet-C, CIFAR10-C, CIFAR100-C \cite{imagenet_c} and MNIST-C \cite{mnist_c}.
In \cite{modelnetc} the idea is extended to point cloud classification, with the introduction of ModelNet-C. 
In our work we present a generic framework, based on sampling, which is light, flexible and robust to Out-Of-Distribution (OOD) samples.

% \begin{figure}
%   \centering
% %   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    \includegraphics[width=1\linewidth, height=1\linewidth]{figures/overall.pdf}

%    \caption{{\bf Approach Overview}. Our ensemble consists of partial point clouds, obtained using three sampling methods: Random, Curves and Patches. Three specialized networks provide class predictions which are aggregated to a unified prediction. The partial information available for each ensemble member yields high diversity and robust OOD performance. %classification in OOD calculated based on EASAM output  and per sub-sampling predictions.
%    }
%    \label{fig:overview}
% \end{figure}

%-------------------------------------------------------------------------
%{\bf Ensemble of Partial Point Clouds (EPiC).}
Ensemble learning is a long-standing concept in robust machine learning \cite{adaboost, neural_network_ensembles, ensemble_methods}. We would like to obtain an ensemble of learners, which are loosely correlated, that generalize well also to OOD input.
There are two main questions: 1) How to form these learners?
   2) How to combine their results?

One classical way to form ensembles in deep learning (see for instance \cite{lakshminarayanan2017simple}), is to rely on the partial-randomness of the training process. In this setting, the ensemble consists of networks of the same architecture  trained on the same training-set. Variations of networks' output stem from the different random parameter initialization and the stochastic gradient descent process. This induces \emph{Stochastic diversity}.
Another major approach to generate ensembles (both in classical machine learning and in deep learning \cite{adaboost,lakshminarayanan2017simple,zefrehi2020imbalance}) is to change the sampling distribution of the training set for each learner. A mixture-of-experts approach is to 
train different types of classifiers, in neural networks this is accomplished by different network architectures. This induces \emph{Architecture diversity}. 
These approaches, however, may not yield sufficient OOD robustness (our experiments show their diversity is limited).
Deep ensembles are being investigated with new ways to calibrate and to estimate uncertainty through increasing the ensemble diversity \cite{zaidi2021neural, uncertainty3d}.

Our approach to form ensembles is by exposing each ensemble member to limited input data. It is performed by generating different samples of each point cloud at both training and testing,
see Fig. \ref{fig:sampling}.
%or \emph{Test-Time Sampling Ensembles}. 
The ensemble becomes highly diverse, since each classifier has access to different parts of the data. Moreover, each sampling method has unique  characteristics. We observe that many types of corruption do not corrupt the data uniformly (see examples in Fig. \ref{fig:corruptions}). Some partial point clouds may thus be less corrupted, achieving accuracy closer to the case of clean data. Highly corrupted partial point clouds yield almost random network response, and can be modeled as adding noise to the classification decision. In this case, applying mean over the outputs of the ensemble significantly diminishes noise and improves accuracy . 
%And since different sampling mechanism generates sub-samples with different characteristics. At the cost of obtaining weaker classifiers, 
Four major advantages are gained by our proposed scheme:
1) The framework is completely generic and can essentially work with any point cloud classification network (we experimented with five, gaining improvement in all);
2) Many diverse classifiers can be generated;
3) Partial data is robust to local corruptions  and to outliers, performing well on OOD samples;
4) The required networks to be trained is the number of sampling methods (in our case three), and not the ensemble size.
%We suggest using three methods for obtaining partial point clouds: Patches, Curves and Random sampling.


\begin{figure}[ptbh!]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width = 1.1\linewidth, height =0.8 
   \linewidth]{figures/overcoming_corruptions2.pdf}
   \caption{{\bf Overcoming corruptions by sampling}.
   Sampled partial point clouds (red points) have instances in which they are mostly not exposed to corruptions. The ensemble becomes highly robust to various unknown degradations. This is mostly apparent for non-uniform corruptions.
   %{\bf Add-Global} is best handled by Curves since they less likely to sample low conductant regions. {\bf Drop-Global.} is best handled by random sampling since the existing points may be selected also during training. {\bf Drop and Add local.} 
   %is handled by local sampling since they may not see the corruption during sampling.
   }
   \label{fig:corruptions}
\end{figure}

%\section{Our Proposed Method}
%\label{sec:method}

%We then turn to the problem of aggregating the  classification results of the ensemble members into a single final decision. We examine three options: majority voting (as done for instance in \cite{pointguard,point_set_voting}), a simple mean of the softmax results and designing a special aggregation module, we refer to as \emph{Ensemble Aggregation Self-Attention based Module} (EASAM) (illustrated in Fig. \ref{fig:easam}). 


% \begin{figure}
%   \centering
%    \includegraphics[width=\linewidth, height=0.7\linewidth]{figures/basic_model5.pdf}

%    \caption{{\bf PiC-net architecture}. Shallow version of CurveNet \cite{curvenet} as the featurizer, followed by a double head module consist of a combination of self and cross attention layers. The input can be any kind of sub-sample, in any given amount of points.}
%    \label{fig:basic_model}
% \end{figure}

%table \ref{table:shallowness}

\begin{table*}
  \centering
  % \caption{Un-Augmented Comparison}
  \begin{tabular}{p{3.8cm} || p{1.0cm} p{1.0cm} p{1.0cm} p{1.0cm} p{1.1cm} p{1.1cm} p{1.0cm} p{1.0cm} p{0.8cm}}
    \hline
    Network (\#Ensemble size)& OA ↑ & mCE ↓ & Scale & Jitter & Drop-G & Drop-L & Add-G & Add-L & Rotate \\
    \hline
    DGCNN \cite{dgcnn} & 92.6\% & 1.000 & 1.000 &  1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000\\
    \hspace*{0.2cm} EPiC(\#12) (Ours) & 93.0\% & \underline{0.669} & 1.000 &  0.680 & 0.331 & 0.498 & 0.349 & \underline{0.807} & 1.019\\
    \hline
    GDANet\cite{gdanet} & 93.4\% & 0.892 & \textbf{0.830} & 0.839 & 0.794 &  0.894 & 0.871 & 1.036 &  0.981\\
    \hspace*{0.2cm} EPiC(\#12) (Ours) & 93.6\% & 0.704 & 0.936 &  0.864 & \underline{0.315} & 0.478 & 0.295 & 0.862 & 1.177\\
    \hline
    CurveNet\cite{curvenet} & \underline{93.8}\% & 0.927 & 0.872 & 0.725 & 0.710 &  1.024 & 1.346 & 1.000 &  \textbf{0.809}\\
    \hspace*{0.2cm} EPiC(\#12) (Ours)  & 92.1\% & 0.742 & 1.245 &  \textbf{0.617} & 0.363 & 0.585 & 0.495 & 1.029 & \underline{0.860}\\
    \hline
    PCT\cite{pct} & 93.0\% & 0.925 & 0.872 & 0.870 & 0.528 &  1.000 & 0.780 & 1.385 &  1.042\\
    \hspace*{0.2cm} EPiC(\#12) (Ours)  & 93.4\% & \textbf{0.646} & 0.894 &  0.851 & \textbf{0.306} & \textbf{0.435} & \underline{0.285} & \textbf{0.735} & 1.019\\
    \hline
    RPC\cite{modelnetc} & 93.0\% & 0.863 & \underline{0.840} & 0.892 & 0.492 &  0.797 & 0.929 & 1.011 &  1.079\\
    \hspace*{0.2cm} EPiC(\#12) (Ours)  & 93.6\% & 0.750 & 0.915 & 1.057 & 0.323 & \underline{0.440} & \textbf{0.281} & 0.902 & 1.330\\
    \hline
    PointNet\cite{pointnet} & 90.7\% & 1.422 & 1.266 &  \underline{0.642} & 0.500 & 1.072 & 2.980 & 1.593 & 1.902\\
    PointNet++\cite{pointnet++} & 93.0\% & 1.072 & 0.872 &  1.177 & 0.641 & 1.802 & 0.614 & 0.993 & 1.405\\
    RSCNN\cite{rscnn} & 92.3\% & 1.130 & 1.074 & 1.171 & 0.806 &  1.517 & 0.712 & 1.153 &  1.479\\
    SimpleView\cite{simple_view} & \textbf{93.9}\% & 1.047 & 0.872 & 0.715 & 1.242 &  1.357 & 0.983 & 0.844 &  1.316\\
    PAConv\cite{paconv} & 93.6\% & 1.104 & 0.904 & 1.465 & 1.000 &  1.005 & 1.085 & 1.298 &  0.967\\
    % PointGuard\cite{pointguard} (\#10,000) & 82.5\% & 1.249 & 3.000 & 0.661 & 0.694 &  0.976 & 0.603 & 0.840 &  1.972\\
    % \hline
    % Classic Ensemble-Mean (\#12) & 93.0\% & 1.014 & 1.000 &  0.677 & 0.810 &  0.865 & 1.814 & 1.004 &  \underline{0.93}\\
    % Classic Ensemble-Majority (\#12) & 93.3\% & 1.022 & 1.032 &  0.671 & 0.823 &  0.894 & 1.800 & 1.004 &  \underline{0.93}\\
    % \hline
    % EPiC-Net-Mean (\#12) (Ours) & 92.6\% & {\bf0.694} & 1.128 &  \bf{0.633} & {\bf0.355} &  {\bf0.493} & {\bf0.376} & {\bf0.753} &  1.121\\
    % EPiC-Net-EASAM (\#12) (Ours) & 93.2\% & \underline{0.719} & 1.128 &  \underline{0.639} & \underline{0.367} &  \underline{0.512} & \underline{0.427} & \underline{0.811} &  1.121\\
    % option2 EPiC-net (EASAM \#96) & 93.2\% & \underline{0.701} & 1.085 &  {\bf0.592} & \underline{0.367} &  0.512 & 0.4 & \underline{0.782} &  1.172\\
    % \hline
    % GDANet+W.M & 93.4\% & {\bf0.571} & 0.904 &  0.883 & 0.532 &  0.551 & 0.305 & 0.415 &  0.409\\
    % EPiC-Net+W.M(\#12) (Ours) & 91.9\% & 0.570 & 1.191 &  0.465 & 0.391 & 0.483 & 0.451 & 0.418 &  0.591\\
    \hline
  \end{tabular}
  \caption{{\bf Main Experimental Result. ModelNet-C Unaugmented classification comparison.} {\bf Bold} best, \underline{underline} second best. Our framework dramatically improves robustness of all examined networks, as indicated by the mCE measure. Experiments using EPiC were conducted on the five most robust networks ($mCE \le 1$).  %2) Our method improves Overall Accuracy on almost all networks.  %% Less important
  }
  \label{table:comparison_unaugmented}
\end{table*}

%Our main contributions can be summarized as follows:
%1) To reach high OOD performance in point cloud classification we propose an ensemble which is highly diversified in the measurement space (unlike diversification in architecture \cite{networks_ensemble}, representation \cite{representations_ensemble} or network parameter space \cite{lakshminarayanan2017simple}). This methodology is simple and requires far less networks to be trained. As far as we know, in the context of point cloud classification, we are the first to use ensembles of several sub-sampling mechanisms.
%2) We obtain lowly correlated learners, by training on the clean training set alone. %We generate ensemble members which are more diverse than existing ensemble methods, and analyze the diversification.  %% Repeating..

We reach robustness to various local and global degradations, yielding state-of-the-art results on corrupted ModelNet-C \cite{modelnetc} ($mCE = 0.646$ using PCT\cite{pct} and $mCE = 0.501$ using augmented with WolfMix\cite{pointwolf, rsmix} version of RPC \cite{modelnetc}), even with a small ensemble of size 12. %Our method significantly surpasses all other methods on Drop-Global, Drop-Local and Add-Global corruptions.
% May raise again the augmentation claims %
%3) We propose a general methodological approach that can be applied on any given network.

%4) We examine various ways to aggregate the predictions of ensemble members, including developing a new module (EASAM). Our conclusions are that simple mean achieves best OOD robustness at the cost of some degradation in accuracy of the clean test data. Our proposed EASAM method reaches a  good compromise for both the clean and degraded data. Majority voting is inferior to both methods.

%The rest of the paper is organized as follows: we summarize the main related work in Section 
%\ref{sec:related}, describe the proposed method and its motivation in Section \ref{sec:method}, show experimental results and provide implementation details in Section \ref{sec:exp} and conclude in Section \ref{sec:conc}.
%\gnote{Sections 4 and 6 not mentioned, not sure we need this paragraph.}\ynote{which paragraph is redundant? if you think some parts are not partitioned well to sections then change it according to yours}


\section{Related Work}
\label{sec:related}
{\bf Point Cloud classification.} It is customary to categorize point cloud classification networks to three mechanisms: multi-view, voxelizing, and point-wise networks. One of the challenges in point cloud processing is that, unlike images, 3D points are irregular in space. Multi-view methods project the point cloud into different viewpoints, generate 2D images, and apply CNN based networks \cite{multi_view_and_voxel, simple_view}. The projection forces the data to be regular. These approaches are slower because of the rendering phase, and might lose useful geometric information.
In the voxelizing mechanism, the solution for the irregularity problem is to  partition the 3D space into a voxel grid \cite{multi_view_and_voxel, modelnet40, voxel2}. This approach suffers heavily from sensitivity to the choice of grid regularity.
In the context of point-wise networks, PointNet \cite{pointnet} presented a pioneering method of applying MLP on the raw 3D points.
%yielding very good classification results at the cost of discarding local information. As a following work, PointNet++\cite{pointnet++} is designed to  capture also local per-point structures, improving overall performance.
DGCNN\cite{dgcnn}, Dynamic Graph CNN, dynamically constructs a graph through the network. The graph is initially based on the raw 3D points, and progresses to more evolved  feature spaces with semantic connectivity. 
GDANet \cite{gdanet}, Geometry-Disentangled Attention Network, dynamically disentangles point clouds into contour and flat
components. Respective features are fused to provide distinct and complementary geometric information from each representation. Recently, PCT \cite{pct}, Point Cloud Transformer, adopted transformer architecture \cite{attention_is_all_you_need}
%, a highly successful architecture originating from the NLP field.  It divides the point cloud into patches, in an analogue manner to words in a sentence, 
to create per-patch embedding, leveraging self attention mechanisms. CurveNet\cite{curvenet} generates features for each point by guided-walk over the cloud followed by a curve grouping operator.
Ren et al.\cite{modelnetc} studied the impact of several architectural choices on the robustness of the network and combined the most robust components to create RPC, which is the SOTA network on ModelNet-C. RPC takes advantage of 3D representations, using KNN, frequency grouping and self-attention mechanism that turned out to be the most robust combination. Another related work is Point-BERT\cite{pointbert} which splits the point cloud into patches as well. It then tokenizes the patches and uses pre-trained transformers with Mask Point Modeling. Point-MLP\cite{point_mlp} is based on residual MLPs, achieving impressive results on the clean data.
%\gnote{Need to mention GDANet\cite{gdanet}. Basically, elaborate more on the 5 networks you use in Table 1, as those are the most robust.} \ynote{Done!}

%We chose an adaptation of CurveNet as our basic ensemble network as it is highly flexible and robust.

{\bf Robustness to shifts from the training set.} Traditionally, most of the attention in terms of robustness focused on basic deformations: jitter, scale and rotation.
%Along the history of robustness in ,
% of robust point cloud classification
There are different approaches in the literature focusing on some specific deformations, degradations or corruptions. For example, a long standing research topic is rotation invariance,
thoroughly addressed, e.g. by ClusterNet\cite{clusternet} and  LGR-Net\cite{lgr_net}.
%In this context,  ClusterNet\cite{clusternet} developed a point cloud representation that is rotation invariant, and designed an architecture which takes advantage of this representation. LGR-Net\cite{lgr_net} combined local geometry with global topology using an MLP-based attention module. 
PointCleanNet\cite{pointcleannet} and PointASNL\cite{point_asnl} addressed specifically robustness to outliers.
%To gain robustness to outliers, PointCleanNet\cite{pointcleannet} is a data-driven method for outlier removal. PointASNL\cite{point_asnl} introduced an adaptive sample module followed by a local-nonlocal module which enhances robustness to outliers. 
Studies investigating robustness to occlusions (common in 3D-sensors) use transformers \cite{IT_net} and voting mechanisms \cite{point_set_voting}.
Another aspect of robustness is related to 3D adversarial attacks, adressed by \cite{adversarial_pcs, extending_adversarial}. 
%
%In this setting an adversary intentionally aims to corrupt the input data in order to deceive the classifier and cause misprediction. 
%A common degradation in 3D point clouds is related to partial available data. That is, only partial point clouds are given as input. This is standard in range acquisition methods, 
%such as LiDARs. In addition, there are often occlusions and inaccuracies along the 3D pre-processing pipeline. IT-Net \cite{IT_net}, a transformer network, handles partial point clouds by obtaining a canonical pose using a series of 3D rigid transformations. Point Set Voting\cite{point_set_voting} suggested to separate the point cloud into patches and classify according to the combined set of patch features.
PointGuard\cite{pointguard} is also focused on guarding against adversarial attacks. $K$ random sampling procedures are performed for each point cloud, where the overall prediction is obtained by majority voting.
The theoretical analysis of \cite{pointguard} advocates the use of sampling very few points and using a huge ensemble ($K=10000$). 
Our experiments indicate that such ensembles do not perform as well on the clean data and are competitive only for certain types of OOD samples.

\begin{figure*}[ptbh!]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width = 
   0.75\linewidth, height =0.38 
   \linewidth]{figures/intuition_transpose2.pdf}
   \caption{{\bf Pointwise Importance}. Left column - corruptions (grey - added points, black - removed points). The rest of the columns show color-coded pointwise importance. 
   % 1) Curves are most influenced by the lampshade, patches mostly by the bulb. The ensemble is influenced by both, makes the network pay attention not only to prominent features, but also to delicate ones.
 Although Random is more influenced by Add-local, and Patches+Random by Drop-local (dashed orange circles), the aggregated result is robust to both (dashed grey circles).}
   \label{fig:intuition}
\end{figure*}

A major approach to increase robustness is by augmentation techniques which enrich the training set. RSmix\cite{rsmix} suggests mixing two samples from the database in a smooth manner, along with smoothing the labels of both samples, creating new virtual samples. This method inherently inserts jittering, scaling, translation and rotation. PointWolf\cite{pointwolf} applies non-rigid manipulations on selected anchor points. These include scaling, 3D rotation and translation with local jittering. Such complex manipulations are able, for example, to change a person's posture or the style of a chair. 
We note that augmentation introduces expected corruptions within the training set, thus violating, to some degree, the testing principles of OOD robustness.
To make our work complete, we examine the proposed framework also on augmented data (WolfMix\cite{modelnetc}, a combination of PointWolf and RSmix). We achieve SOTA in this case as well.
%Our experiment show that a combination of our method with state-of-the-art augmentation techniques achieves SOTA on ModelNet-C. 

\subsection{Benchmarks}
\label{sec:benchmarks}
% We perform our method on ModelNet40 and ModelNet-C\cite{modelnetc} databases.
ModelNet-40 \cite{modelnet40} is a widely used dataset consisting of CAD meshes from 40 classes, such as airplane, chair and sofa. Each mesh was uniformly sampled to form a 3D point cloud consisting of 1024 points. The dataset has 12,311 samples, divided to 9843 for training and 2468 for test. The dataset contains closely semantic classes like vase, plant and flower pot, or chair and stool, which makes this dataset highly challenging.
%
Recently, in \cite{modelnetc} Ren et al proposed a corrupted point cloud benchmark to assess OOD robustness. It is based on the well studied ModelNet-40 and is referred to as ModelNet-C. 
We evaluate the robustness for OOD corruptions by applying it on ModelNet-C \cite{modelnetc}. This dataset contains seven types of corruptions: jitter, scale, rotate, add-global, add-local, drop-global and drop-local. Each of these corruptions is rendered in five levels of difficulty. In \cite{modelnetc} a unified calculation mechanism was defined to measure robustness. This measure is termed mCE (mean Corruption Error). It basically evaluates the results in comparison to DGCNN, which serves as a reference algorithm (and hence, by definition, has $mCE \equiv 1$). Each specific corruption has a similar score, relative to DGCNN, averaged over all five levels of difficulty. Since the measure is of error, a lower score is better. Please refer to 
\cite{modelnetc} for more details.


%{\bf ModelNet-C.}
% Seven different types of corruptions are applied to the clean dataset: jitter, scale, rotation, add global, add local, drop global and drop local. The extent of corruption for each degradation is also given in several degrees of severity. A detailed explanation on ModelNet-C is given in Section \ref{sec:benchmarks}.
%We use this benchmark to demonstrate the validity of our approach.


%-------------------------------------------------------------------------

\section{Our Proposed Method}
\label{sec:method}

\subsection {Notations}
We denote by $N$ the number of cloud points at the input of a network (which, due to sampling, may vary). The number of features for each point in the cloud is $F$. We refer to $\tilde{K}$ as the ensemble size of each sub-sample mechanism and to $K$ as the combined ensemble size. In our case we suggest three sampling mechanisms of the same size, therefore $K = 3\tilde{K}$. $N_p$, $N_c$ and $N_r$ are the number of points in patches, curves and random sub-samples, respectively. In curve sampling there may be occasional repeating points, $N \le N_c$.
For curve extraction we have a hyper-parameter $M$ controlling the number of neighbors to choose from in a random-walk iteration. $C$ is the number of classes ($C=40$ in ModelNet-40), and $p \in \mathbb{R}^C$ is a single prediction vector. $P\in \mathbb{R}^{K \times C}$ is an ensemble of predictions. 
%$w$ is a single coefficient weight predicted by EASAM, and $W \in \mathbb{R}^K$ is a weight vector for the entire ensemble.

\subsection {Motivation of our approach}
The ability to classify in a robust manner is closely coupled with the ability to classify based on partial information. We would like a classification network to perform reasonably well also when some information is missing, noise is added or when outliers are introduced. Thus, it is desired to obtain a diverse set of features which are based on different locations in the shape. 
This could be demonstrated well in the experiment illustrated in Fig. \ref{fig:intuition}.

We visualize the internal classification importance of points for networks specializing in curves, patches and random. For the demonstration we focused on two degradations of adding and removing points locally. 
Commonly, a network gets a point cloud $X \in \mathbb{R}^{N\times 3}$ and encodes it into features $X_f \in \mathbb{R}^{N\times F}$ by a variety of sophisticated layers. The standard method to aggregate the points axis is by applying a symmetric function which is permutation invariant, such as max or mean.
In this example, in order to obtain features $X_f$, we use DGCNN\cite{dgcnn}.
We calculate the importance of each point $j$, denoted by $Imp(j)$, in the following manner,
\begin{equation}
    Imp(j) = \sum_{k=1}^F I(j==\arg\max_{n}(X_f(n,k))),
  \label{eq:importance}
\end{equation}
where  $I$ is an indicator function (1 when true and 0 otherwise).
The importance attempts to quantify the number of features of a specific point which are part of the global feature vector. This usually means that the feature's magnitude at that point is maximal, dominating the respective feature vector entries among all other points.
In this example, to cover the entire point cloud, we calculate the importance for all partial point clouds and present the average result in Fig. \ref{fig:intuition}.

When parts of the cloud are missing, or outliers are introduced, potentially corrupting features, we want the classification to be based on a variety of features, even though they may be less prominent in the clean set. Therefore, our aim is to ``spread'' the importance as evenly as possible. This insight motivates our approach for using partial point cloud ensembles to impose feature diversity. Additional analysis and insights appear hereafter.

\subsection{Proposed approach}
We use three types of sub-samples: two local ones, \emph{Curves} and \emph{Patches} and a global one consisting of \emph{Random-sampling}.
For the local methods, we first use farthest point sampling (FPS) algorithm \cite{pointnet++} to choose $\tilde{K}$ anchors. From each anchor we extract a patch and a curve. 

\textit {Patch extraction} is done by finding $N_p$ nearest neighbors. This sub-sample mechanism is more conservative, hence it preserves well the local information.

\textit {Curve extraction} is done by a random-walk process, beginning from the anchor point. $N_c$ random-walk iterations are performed, at each iteration one of $M$ nearest neighbors is chosen randomly. The choice is with replacement (hence the sampled partial point cloud may be smaller than $N_c$). This mechanism is more exploratory in nature and less structured.

\textit {Random extraction} is done by simply sub-sampling $N_r$ random points from the entire point cloud (without replacement, $N = N_r$).

The values of these parameters were determined once  and were used in the same manner in all our experiments for all classification networks (see details in Supp).

{\bf Generic framework.}
Our approach is generic and can be applied in conjunction with any point cloud classification network. 
The experiments (detailed in the experimental section) are conducted with five different architectures. 
These architectures are the most OOD-robust. Our method considerably improves the robustness of every one of them, as indicated by the mCE measure.
%These networks where significant improvement in robustness is achieved. 
%Therefore, we experiment the robustification by adopting several widely used off-the-shelf methods as basic networks that perform as well as DGCNN\cite{dgcnn} or better. 
We use three instances of the same architecture. Each instance is trained to classify point clouds obtained by a certain sub-sampling mechanism. A recap of our approach (inference) is given in Algorithm \ref{alg:inference}, where the training procedure is detailed in the Supp.


\begin{algorithm}
\caption{Classification using EPiC (inference)}\label{alg:inference}
\begin{algorithmic}
\Require{$X, params_{Patches}, params_{Curves}, params_{Random}$} 
% \Ensure $y = x^n$
\State $model_{Patches} \gets params_{Patches}$
\State $model_{Curves} \gets params_{Curves}$
\State $model_{Random} \gets params_{Random}$
\State $anchors \gets FarthestPointSampling(X,\tilde{K})$

\For{$k \in \tilde{K}$} 
\State $Patch \gets FetchPatch(X, anchors(k))$\Comment{Local}
\State $Curve \gets FetchCurve(X, anchors(k))$\Comment{Local}
\State $Random \gets FetchRandom(X)$\Comment{Global}

\State $P_{Patch}^{k}\gets model_{Patches}(Patch)$
\State $P_{Curve}^{k} \gets model_{Curves}(Curve)$
\State $P_{Random}^{k} \gets model_{Random}(Random)$
\EndFor
\State $P_{ensemble} \gets Concatenate(P_{Patch}^{1:\tilde{K}}, P_{Curve}^{1:\tilde{K}}, P_{Random}^{1:\tilde{K}})$
\State $P \gets Mean(P_{ensemble})$; $Class=\arg\max(P).$

\end{algorithmic}
\end{algorithm}

%A major robustification improvement is achieved for all examined networks using our approach together with accuracy improvement on the clean set as summarized in Table \ref{table:comparison_unaugmented}.  %% Should be stated later.

%\subsection{PiC-net}
%Two main reasons led us to design a 
%Since the classifiers are loosely correlated due to diversity in sampling schemes, we can gain good combined results using more compact basic network. For that end we designed a light-weight architecture dubbed \textit {PiC-net}, a shallow version of CurveNet\cite{curvenet}. We decided to use CurveNet as the main featurizer for two main reasons: 1) This network performs well under corruptions and very well on overall accuracy. 2) CurveNet is robust for the geometry, size and sampling mechanism, in contrast to some other methods.

%We use only a shallow version in order to reduce the memory consumption. This way, after combining three instances, the network size is still comparable to other non-ensemble networks. A  comparison of model size is shown in Table \ref{table:model_size}. Moreover, ensembles perform well when the learning capacity of each member is not very strong.

%Per-point features are then fed to an attention layer. This attention layer consists of dual Offset-Attention Modules (OAM), as in  PCT\cite{pct}. Our experience suggests that OAM allows to better extract features deeper in the network. The output of OAM's is processed by dual cross-attention modules. The enhanced feature vector consists of a concatenation of intermediate results. This exposes features from a variety of depths. The same common decoder is used. The full architecture is depicted in Fig. \ref{fig:basic_model}.

%\subsection{Ensemble Aggregation Self-Attention based Module (EASAM).}

%The goal of EASAM is to predict a weight $w_i$ for each prediction $p_i$ of an ensemble member. The overall prediction is: $p_{ensemble} = \sum_{i=1}^{K} {w_{i}p_{i}}$. 
%Mean is, naturally, a special case of this technique, in which $w_i = \frac{1}{K}$ for all $i$.
%The architecture of EASAM is very simple:
%\begin{equation}
 %   W = SoftMax\scalebox{2}{$($}Max\scalebox{1.66}{$($}SA\scalebox{1.33}{$($}SA(P)\scalebox{1.33}{$)$}\scalebox{1.66}{$)$}\scalebox{2}{$)$} \in  \mathbb{R}^K,
 % \label{eq:easam}
%\end{equation}
%where $SA$ denotes a self-attention layer.
%Predictions, $P$, are fed to two consecutive self-attention layers. Then we apply maximum on the features dimension and normalize using SoftMax. The full architecture is depicted in Fig. \ref{fig:easam}.
%We compare aggregation results of EASAM, Mean and Majority Voting. EASAM achieves better overall accuracy (clean data), while performing very well under corruptions (slightly worse than Mean). Please see Table \ref{table:aggregations}. For more technical choices considerations please refer to the Supp.


\begin{table*}
  \centering
  % \caption{Augmented Comparison}
  \begin{tabular}{p{3.9cm} || p{1.0cm} p{1.0cm} p{1.0cm} p{1.0cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.0cm} p{0.8cm}}
    \hline
    Networks (\#Ensemble size)& OA ↑ & mCE ↓ & Scale & Jitter & Drop-G & Drop-L & Add-G & Add-L & Rotate \\
    \hline
    DGCNN\cite{dgcnn}+W.M & 93.2\% & 0.590 & 0.989 &  0.715 & 0.698 & 0.575 & 0.285 & 0.415 & \underline{0.451}\\
    \hspace*{0.2cm} EPiC(\#12)+W.M (Ours) & 92.1\% & 0.529 & 1.021 &  \textbf{0.541} & 0.355 & 0.488 & 0.288 & 0.407 & 0.600\\
    \hline
    GDANet\cite{gdanet}+W.M & \textbf{93.4}\% & 0.571 & \textbf{0.904} &  0.883 & 0.532 &  0.551 & 0.305 & 0.415 & \textbf{0.409}\\
    \hspace*{0.2cm} EPiC(\#12)+W.M (Ours) & 92.5\% & 0.530 & 0.968 & \underline{0.639} & 0.343 & 0.473 & 0.275 & 0.433 & 0.577\\
    \hline
    PCT\cite{pct}+W.M & \textbf{93.4}\% & 0.574 & 1.000 &  0.854 & 0.379 &  0.493 & 0.298 & 0.505 &  0.488\\
    \hspace*{0.2cm} EPiC(\#12)+W.M (Ours) & 92.7\% & \underline{0.510} & \underline{0.915} &  0.699 & \underline{0.323} & \underline{0.425} & \underline{0.268} & \underline{0.404} & 0.535\\
    \hline
    RPC\cite{modelnetc}+W.M & \underline{93.3}\% & 0.601 & 1.011 &  0.968 & 0.423 &  0.512 & 0.332 & 0.480 & 0.479\\
    \hspace*{0.2cm} EPiC(\#12)+W.M (Ours) & 92.7\% & \textbf{0.501} & \underline{0.915} &  0.680 & \textbf{0.315} & \textbf{0.420} & \textbf{0.251} & \textbf{0.382} & 0.544\\
    \hline
  \end{tabular}
  \caption{{\bf ModelNet-C Augmented 
  (WolfMix) classification Comparison.} {\bf Bold} best, \underline{underline} second best. 1) dramatically robustification gained by using EPiC on conventional methods. 2) Our method improves mCE and almost all OOD corruptions.}
  \label{table:comparison_augmented}
\end{table*}


\begin{table*}
  \centering
  % \caption{Augmented Comparison}
  \begin{tabular}{p{4.0cm} || p{0.85cm} p{0.95cm} p{0.75cm} p{0.8cm} p{1.1cm} p{1.1cm} p{1.0cm} p{1.0cm} p{0.75cm}}
    \hline
    Ensembles & OA↑ & mCE↓ & Scale & Jitter & Drop-G & Drop-L & Add-G & Add-L & Rotate \\
    \hline
    NS-1A \cite{dgcnn} & \underline{93.5}\% & 1.006 & \textbf{0.862} &  0.709 & 0.762 & 0.889 & 2.064 & 1.025 & \textbf{0.730}\\
    % Homo Input Hetro Arch (\cite{dgcnn}, \cite{modelnetc},\cite{pct}) & 93.4\% & 0.934 & 0.894 &  0.636 & 0.617 & 0.783 & 1.793 & 1.007 & 0.805\\
    NS-3A \cite{dgcnn, pct, gdanet} & 93.4\% & 0.856 & \textbf{0.862} &  \underline{0.642} & 0.657 & 0.797 & 1.275 & 0.982 & \underline{0.777}\\
    \hline
    S-1A \cite{dgcnn}, Ours & 93.0\% & \textbf{0.669} & 1.000 &  0.680 & \underline{0.331} & \underline{0.498} & \underline{0.349} & \underline{0.807} & 1.019\\
    S-3A \cite{dgcnn, pct, gdanet}, Ours & \textbf{93.8}\% & \underline{0.671} &\underline{0.915} &  0.813 & \textbf{0.315} & \textbf{0.469} & \textbf{0.302} & 0.811 & 1.074\\
    % Hetro Input Hetro Arch (\cite{dgcnn}, \cite{pct},\cite{modelnetc}) & 93.4\% & 0.689 & 0.894 &  0.877 & 0.319 & 0.454 & 0.295 & 0.858 & 1.126\\
    \hline
    Point-Guard (\#1,000) \cite{pointguard} & 89.6\% & 0.949 & 1.947 &  \textbf{0.617} & 0.427 &  0.599 & 0.376 & \textbf{0.702} &  1.977\\
    \hline
  \end{tabular}
  \caption{{\bf ModelNet-C Ensemble methods classification Comparison.} {\bf Bold} best, \underline{underline} second best. Robustness improved by our suggested method.}
  \label{table:comparison_ensemble}
\end{table*}


% \begin{table*}
%   \centering
%   % \caption{Augmented Comparison}
%   \begin{tabular}{p{6.0cm} || p{0.8cm} || p{0.7cm} p{0.7cm} p{0.7cm} p{1.1cm} p{1.1cm} p{1.0cm} p{1.0cm} p{0.8cm}}
%     \hline
%     Ensembles Diversity & Mean↓&Clean&Scale&Jitter&Drop-G&Drop-L&Add-G&Add-L&Rotate\\
%     \hline
%     Homo Samp Homo Arch \cite{dgcnn} & 0.880 & 0.97 & 0.96 & 0.91 &\underline{0.87} & 0.87 & \underline{0.75} & 0.89 & 0.91\\
%     Homo Samp Hetro Arch \cite{dgcnn, pct, gdanet} & 0.858 & 0.97 & 0.96 & 0.89 & \textbf{0.84} & \underline{0.85} & \textbf{0.72} & \underline{0.85} & 0.90\\
%     \hline
%     Hetro Samp Homo Arch \cite{dgcnn}, Ours & \textbf{0.798} & \textbf{0.91} &\textbf{0.87} & \underline{0.74} & 0.90 & \textbf{0.84} & 0.79 & \textbf{0.64} & \textbf{0.79}\\
%     Hetro Samp Hetro Arch \cite{dgcnn, pct, gdanet}, Ours & \underline{0.804} & \underline{0.92} & \underline{0.89} & \textbf{0.73} & 0.92 & 0.86 & 0.80 & \textbf{0.64} & \underline{0.80}\\
%     \hline
%   \end{tabular}
%   \caption{{\bf Ensemble methods diversity Comparison.} {\bf Bold} best, \underline{underline} second best.}
%   \label{table:comparison_diversity}
% \end{table*}

%\begin{figure}
%  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width =\linewidth]{figures/easam7.pdf}

 %  \caption{{\bf Ensemble Aggregation Self-Attention based Module (EASAM)}. This module receives predictions from basic models and predicts coefficient per-prediction for a weighted sum.}
 %  \label{fig:easam}
%\end{figure}

\begin{figure}[ptbh!]
    \centering
    \captionsetup[subfigure]{justification=centering}
    \begin{subfigure}[t]{0.2\textwidth} %{0.25\textwidth}
\includegraphics[width=1\textwidth]{figures/clean_diversity_homo_in_homo_arc.pdf}
\caption{{\bf NS-1A}, $c=0.948$}
        \label{subfig:clean_homo_homo}
    \end{subfigure}
    \begin{subfigure}[t]{0.2\textwidth} %{0.25\textwidth}
\includegraphics[width=1\textwidth]{figures/clean_diversity_homo_in_hetro_arc.pdf}
\caption{{\bf NS-3A}, $c=0.940$}    
        \label{subfig:clean_homo_hetro}
    \end{subfigure}
    \begin{subfigure}[t]{0.2\textwidth} %{0.25\textwidth}
\includegraphics[width=1\textwidth]{figures/clean_diversity_hetro_in_homo_arc.pdf}
\caption{{\bf S-1A}, $c=0.825$}
        \label{subfig:clean_hetro_homo}
    \end{subfigure}
    \begin{subfigure}[t]{0.2\textwidth} %{0.25\textwidth}
\includegraphics[width=1\textwidth]{figures/clean_diversity_hetro_in_hetro_arc.pdf}
\caption{{\bf S-3A}, $c=0.847$}
        \label{subfig:clean_hetro_hetro}
    \end{subfigure}
    % \caption*{Numeric generation of a minimal set and the corresponding flowbox coordinates}
    \caption{\textbf{Correlation between ensemble members.} Correlation output of ensemble members on full non-sampled clean test-set (NS), top, with a single and three architectures, compared to sampling (S) by our approach, bottom, see setting details in Section \ref{sec:analysis}. $c$ is defined in Eq. \eqref{eq:C} (lower means higher diversity). Sampling affords higher diversity, compared to stochastic and architecture sources of diversity. 
    }
    \label{fig:corr}
\end{figure}

\section{Diversity Analysis}
\label{sec:analysis}
In order to leverage the advantage afforded by ensembles, a high level of diversity of ensemble members is required. %Diverse ensembles generally perform better than highly correlated ones. 
This motivates us to quantitatively study the diversity of several types of ensembles. We investigate the following sources of diversity:
\begin{enumerate}
    \item {\bf Stochastic.} Diversity stemming from the stochasticity of the training process (initialization and SGD).
    \item {\bf Architecture.} Diversity caused by using different network architectures.
    \item {\bf Sampling.} Diversity due to different sampling methods and randomness of the sampling process.
\end{enumerate}
Four types of ensembles were examined (each consisting of 12 members, correlation results are shown in Fig. \ref{fig:corr}):
\begin{enumerate}
    \item {\bf No sampling, single architecture (NS-1A).} \emph{Stochastic diversity.} The ensemble consists of   12 instances of DGCNN\cite{dgcnn}. The entire point-cloud is used as input.
    \item {\bf No sampling, three architectures (NS-3A).} \emph{Architecture + stochastic diversity.}
    The ensemble consists of three architectures (each of 4 instances, in this order): PCT\cite{pct}, GDANet\cite{gdanet} and DGCNN\cite{dgcnn}. The entire point-cloud is used as input.
    \item {\bf Sampling (ours), single architecture (S-1A).} 
    \emph{Sampling diversity.}
    An ensemble consisting of three instances of DGCNN\cite{dgcnn}. Each instance is trained to specialize in a different sampling mechanism. The sampling methods (in this order) are: Patches, Curves and Random. Each instance uses 4 different sub-sample inputs.
    \item {\bf Sampling (ours), three architecture (S-3A).} 
    \emph{Sampling + architecture diversity.}
    Similar setting to the sampling diversity experiment. Here the following architectures, GDANet\cite{gdanet}, PCT\cite{pct} and DGCNN\cite{dgcnn}, were used on sampled inputs of Patches, Curves and Random, respectively. 
\end{enumerate}





%\textbf{Homo Sampling Hetro Architecture.} Here, we still predict using the entire point-cloud, but based on three different architectures. The architectures selected (by this order) are: PCT\cite{pct}, GDANet\cite{gdanet} and DGCNN\cite{dgcnn}.  Therefore, this ensemble diversification is derived by the architectural differences.

%\textbf{Hetro Sampling Homo Architecture.} This ensemble is consist of three instances of DGCNN\cite{dgcnn}. Each instance is trained to specialize in different sub-sampling mechanism. The sampling methods (by this order) are: Patches, Curves and Random. Each of the instances predicts 4 sub-samples fetched during test-time to sum up to ensemble of size 12. This ensemble is diversified by the sampling procedure.

%\textbf{Hetro Architecture Hetro Sampling.} In this context, we combined architectural and sampling diversity by applying GDANet\cite{gdanet}, PCT\cite{pct} and DGCNN\cite{dgcnn} on Patches, Curves and Random correspondly. This ensemble is diversified by a combination of sampling procedure and architectural selections.

In addition, PointGuard \cite{pointguard} was examined, as a different ensemble reference, with an ensemble of size $1000$.
%\ynote{they suggested 10,000 so I comment the rest of the sentence}.  %%OK
% as suggested by the authors. 
%This method can be interpreted as Homo Architecture Homo Sampling, but with homogeneous sampling consist of random subsets of the sample and not the entire set. 

\textbf{Measuring diversity through correlation.} 
%In our framework we divide each point-cloud to an ensemble of 12 classifiers. 
% Repeating
We introduce a measure which quantifies (inverse) diversification by 
\begin{equation}
    c = \frac{1} {S}\sum_{i=1}^{S}{\frac{||C_i-I||^2} {K^{2}-K}},
  \label{eq:C}
\end{equation}
where $S$ is the dataset size, $K$ is the ensemble size, $I_{K\times K}$ is a unit matrix, $||\cdot||$ is the Frobenius norm and C is the Pearson correlation matrix of the ensemble predictions.
%\gnote{we may want to have a formula here (or in the supp) for $C$.} \ynote{A bit not sure about the notation. Would be happy to write it down together. I just used built-in function name CorrCoeff}
%\gnote{problematic, this mean should appear in the equation or the def of $C$ should include it.} \ynote{Added, Please verify my notation}
The final measure $c \in [0,1]$ is a scalar quantifying the diversity of the ensemble (in an inverse manner) for a given dataset.
For $c=0$ the members' response is completely uncorrelated (most diverse). For $c=1$ the members are fully correlated (zero diversity) and an ensemble is not necessary (would produce identical results as a single member). Note that since each member is quite accurate (a ``strong learner''), with an accuracy of around $90\%$, on binary problems we expect a diverse ensemble to be with $c \approx 0.9^2$ (in the multiclass case analysis requires additional assumptions, but should be in similar ranges).   
%to be much not contribute means that all of the classifiers predictions are the same. $c$ can be interpreted as the normalized mean correlation between each of the ensemble members. The lower $c$ is the more diverse the set is. 
% We calculate $c$ for the clean set and under each of the corruptions to all 4 ensembles introduced above and summarized in table \ref{table:comparison_diversity}. 
As can be seen in Fig. \ref{fig:corr}, ensembles based on sampling are considerably more diverse than those based on stochastic or architecture diversity. Moreover, curves and patches are lowly correlated, although the same anchor points are used. Diversity stems mostly from the sampling scheme, in addition to locality.
%
In Table \ref{table:comparison_ensemble} the improved robustness gained by the four ensemble methods is shown. %The experiments are on ModelNet-C. 
Our partial-point-cloud strategy improves robustness in most criteria, excelling in mCE, with the three architecture configuration (S-3A) having superior results also in overall accuracy (clean set).

%% Currently not enough space, removing
%\textbf{Qualitative analysis} To demonstrate the correlation between ensemble members we computed $C$ on the clean set for each of the ensembles. The matrices are given in Fig \ref{fig:corr}. Our main insights are:

%1) Using Heterogeneous sampling (c),(d) comparing to (a),(b) is diversifying the ensemble while using Heterogeneous architecture (b), (d) is not changing much the diversification of the results.

%2) Random is a widely used mechanism for ensemble (like for example in PointGuard \cite{pointguard}), but as can be seen random generating samples that are very self correlated, in comparison to local sampling schemes as curve or patch extraction.

%3) Although patches and curves are extracted from the same anchor points, thus are spanned in close spatial locations of the shape, it can be seen that there is relatively low correlation between them, emphasizing the impact of the sub-sampling mechanism on the diversity.



\section{Experiments}
\label{sec:exp}
We present our results for point cloud classification on ModelNet-C dataset, training on the clean dataset only and measuring performance on both clean and corrupted sets.
{\bf Implementation details.}
 We train the basic models independently on partial point clouds. 
 %in which each partial point cloud label is the label of the entire sample. 
 The predictions are aggregated using mean. For the unaugmented version we use only basic, standard augmentation procedures (detailed below) in order not to violate the OOD principle. For WolfMix augmented version we first augment the entire sample, then we generate the different sub-samples.
%\subsubsection{Training PiC-nets}
We eliminate the randomness with a fixed seed. All three models are trained simultaneously 300 epochs with learning rate of $5e^{-4}$, with cosine annealing scheduler \cite{cosine_annealing} to zero. We use a batch size of 256. For the unaugmented version we followed DGCNN \cite{dgcnn} protocol for augmentation: 1) random anisotropic scaling in the range $[2/3, 3/2]$; 2) random translation in the range $[-0.2, +0.2]$. The implementation uses Pytorch library \cite{pytorch}. Cross-Entropy loss is minimized.
In the training phase we split each sample to 4 farthest point samples, and independently predict each partial point cloud class. The inference procedure is detailed in Algorithm \ref{alg:inference}.
%\subsubsection{Training EASAM}
%At this phase we set the parameters of all basic models to be fixed while minimizing the Cross-Entropy loss only with regard to the EASAM parameters. We split the sampling to 8 farthest point samples. The hyper-parameters are the same as the training of basic models. Learning rate is $0.05$, running for 50 epochs.
\begin{table*}[ht]
  \centering
    \begin{tabular}{p{4cm} || p{1.0cm} p{1.0cm} p{1.0cm} p{1.0cm} p{1.1cm} p{1.1cm} p{1.0cm} p{1.0cm} p{0.8cm}}
    \hline
    Sub-samples & OA ↑ & mCE ↓ & Scale & Jitter & Drop-G & Drop-L & Add-G & Add-L & Rotate \\
    \hline
    DGCNN-Curves (\#4) & 90.7\% & 1.069 & 1.628 & 1.297 & 0.431 & 0.729 & \underline{0.363} & 1.618 & 1.414\\
    DGCNN-Patches (\#4) & 92.8\% & 0.793 & \textbf{0.989} & 1.165 & 0.577 & 0.536 & 0.505 & 0.851 & \textbf{0.930}\\
    DGCNN-Random (\#4) & 91.5\% & 0.766 & 1.234 & \textbf{0.399} & \underline{0.351} & 0.812 & 0.580 & \textbf{0.793} & 1.195\\
    \hline
    DGCNN-Mean (\#12) & \textbf{93.0}\% & \textbf{0.669} & \underline{1.000} &  \underline{0.680} & \textbf{0.331} &  \textbf{0.498} & \textbf{0.349} & 0.807 &  \underline{1.019}\\
    DGCNN-Maj. Voting (\#12) & \underline{92.6}\% & \underline{0.706} & 1.043 &  0.794 & 0.359 &  \underline{0.517} & 0.380 & \underline{0.800} &  1.051\\
    
    \hline
  \end{tabular}
  \caption{\textbf{Sub-Samples vs. Aggregated.} \textbf{Bold} best among aggregations, \underline{underline} best among sub-samples. Aggregations are almost
always superior for any corruption. Thus, it can be inferred that the partial classifiers are low-correlated.}
  \label{table:aggregations}
\end{table*}


\subsection{Main evaluation results}
The EPiC framework is implemented using several widely used point cloud classification networks.
Table \ref{table:comparison_unaugmented} shows the  Unaugmented results and Table \ref{table:comparison_augmented} shows results following augmentation by WolfMix \cite{pointwolf, rsmix}.

\textbf{Unaugmented.} A major advantage is achieved in terms of mCE for all networks. Using PCT\cite{pct} with EPiC reaches mCE=0.646, which outperform current SOTA of RPC\cite{modelnetc} (mCE=0.863) by a large margin. With respect to accuracy on the clean data set (OA), we see improvement in four out of five cases, with only CurveNet\cite{curvenet} degrading, where RPC\cite{modelnetc} and GDANet\cite{gdanet} reach 93.6\%. 


\textbf{Augmented (WolfMix).} We further examined wheather our method can improve augmentation procedures (which to some extent violate some OOD assumptions). EPiC on augmented data consistently improves robustness. Using RPC \cite{modelnetc} EPiC achieves mCE=0.501 surpassing current augmented SOTA (mCE=0.571). OOD robustness comes at a cost of minor accuracy drops, consistently for all networks. The trade-off between accuracy and robustness is well demonstrated in \cite{modelnetc} for point-cloud classification, and in \cite{robustness} for general classification settings.

\subsection{When our method performs best?} 
%\gnote{I suggest to term it "Uniform degradation" - degrades all points in a uniform manner, and "Nonuniform degradation". First define these terms. Then sort the degradations of ModelNet-C to Uniform and Nonuniform. We are good in Nonuniform. We need to explain more about Jitter, which is uniform but can be robustified..} \ynote{Gave it a shot. Now, let your Shakespeare abilities shine!!}

Let $X \in \mathbb{R}^{N\times 3}$ be the set of coordinates of a clean point cloud. Following a corruption transformation $T_c$ acting on $X$ we get a corrupted point cloud $X_c \in \mathbb{R}^{M\times 3}$, where $X_c=T_c(X)$. Let the intersection of these sets be define by $X_{\cap}:= X \cap X_c$ of size $|X_{\cap}|$ points. We define the \emph{uniformity} of the corruption transformation by
\begin{equation}
    \label{eq:u}
    u(X,T_c) := 1 - \frac{|X_{\cap}|}{max(N,M)}.
\end{equation}
For a fully uniform corruption $T_c$, all points of the original point cloud change, hence $X_{\cap}=\emptyset$ and $|X_{\cap}|=0$, yielding $u=1$. When the transformation is highly selective, affecting only a few points, $X_{\cap} \approx X$ and  $u \to 0$. We refer to the latter as a highly \emph{nonuniform corruption}. This measure is very general and can quantify various diverse corruptions. 

We can roughly divide the corruptions of ModelNet-C 
to \begin{enumerate}
    \item \emph{Uniform:} Scale, Rotation.
    \item \emph{Nonuniform:}  Drop-Global, Drop-Local, Add-Global, Add-Local.
\end{enumerate}
In the case of Jitter one can slightly extend the definition of uniformity by a ball of size $\epsilon$ around each point, when calculating $X_\cap$. Then uniformity grows with the standard deviation of the Jitter. 
%\ynote{This explanation is put jitter inside non-uniform group, but do not give our insight for why random is affectively imitate jitter. don't you think this extra explanation is neccesary?}
%\gnote{We can explain more, I have a rough idea how this can be understood. We need to see if we have enough space (or put this in the supp).}

\emph{Our method performs best on nonuniform corruptions.}
In these cases some partial point clouds in the ensemble are mostly not exposed to the corruption, as shown in Fig. \ref{fig:corruptions}.
In these cases the classification is with high accuracy. Partial point clouds which are highly exposed to the corruption have high chances of yielding missclassification. However, our experiments indicate these missclassifications are quite random and can be approximately modeled as noise. As we perform a \emph{mean} operation over the ensemble outputs, this noise is mostly averaged out, where the correct (mostly unexposed) members of the ensemble dominate the decision. This phenomenon can be seen in the experiment shown and explained in Fig. \ref{fig:noise}. A plot of mCE as a function of $u$ is shown in Fig. \ref{fig:add_global_plot}, illustrating the above rationale.

\emph{The case of Jitter.}
Let us assume the points in the cloud are sampled approximately evenly, with a mean distance between each point of $\ell$. Let the Jitter corruption be of standard deviation $\sigma$.
If $\sigma \ll \ell$ essentially the point cloud is similar to the original clean one and most classifiers would perform well. In terms of the relaxed definition of uniformity, we can set $\epsilon = \ell$ and for $\sigma \ll \epsilon$ we get $u \to 0$ (the corruption is ''invisible'' to the classification network).
Random sampling copes very well with Jitter. We can view random sampling as reducing the point cloud resolution. Basically, we now have a larger distance $L > \ell$ between the points. Since the classifier is trained on the low resolution input, as long as $\sigma \ll L$ we get $u \to 0$. Hence, we are more robust to Jitter with a larger standard deviation. See \cite{pointguard} for additional perspectives and insights on this topic.


\begin{figure}[!ptbh]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width = 0.65
   \linewidth, height =0.48
   \linewidth]{figures/add_global_plot2.pdf}
   \caption{\textbf{mCE versus uniformity.} mCE is monotonically increasing with $u$ (average result on test set of ModelNet-C, Add-Global corruption, 5 degrees of severity).}
   \label{fig:add_global_plot}
\end{figure}

%One can partition the degradations into two groups of corruptions. First, a corruption which uniformly manipulate all the points in the sample, denoted as ''unofirm degradation''. This degradation do not leave any point unchanged. Second, a corruption which manipulate only sub-set of the points, denoted as ''Nonuniform degradation'' which manipulate only subset of the points or even do not change any input point (e.g. Add either global or local) . In the context of ModelNet-C corruptions, uniform degradation group consist of Scale, Rotate and Jitter. Nonuniform degradation group consist of Drop-Global, Drop-Local, Add-Global and Add-Local. Our Method is perform well under Nonuniform degradations, when some of the points remains the same as they appeared in the clean set. Another perspective for the same phenomenon is that by looking at the entire point-cloud one can say that any of the corruptions is shifting the sample to Out-Of-Distribution area, since non of the corrupted examples is seen during training. This leads the network to predict as an extrapolation. But by looking only on in a narrow local manner, there is a co-existence of subsets of the points in the train and test set, let the network stay in interpolation regime, leads to much better performance.
%Our method is performs very well under Nonuniform degradations, therefore, under Add and Drop degradations we are able to well predict the sample, while Scale and Rotate are much more challenging. But if Jitter is belongs to the unofirm degradations, how come that we perform very well on this corruption? This can be explained by the way of random impacts the sampling. While Jitter is seems to appear differently in training and test phases, it is not the case. 
%Imagine a subset of $N_r$ points that were sampled from the point-cloud. Now, we can remove a point from the sub-set and choose one of it's neighbors instead. This new sub-set is also a valid random subset of the point-cloud that in fact imitate a jitter on the region were the point selected. We can repeat this procedure on any of the points and with any of their neighbors. In this way we affectively achieve robustness in training, although none of the points were jittered during training.

%So, our experience suggests that our method can have a generalization for unknown corruptions as long as there is subsets of points co-exist in training and test phase. An evidence for this generalization can be seen from our performance on Addition corruptions. Although any addition of points is done in training phase, our method performs extremely well on Add-Global.



\subsection{Sampling, aggregation and network size} 
In Table \ref{table:aggregations} we show experimental results for each sub-sampling method with DGCNN\cite{dgcnn} as the basic model (additional models are shown in the Supp.). Ensembles of size four are tested, aggregated using mean. In addition, the results of all three methods (forming an ensemble of 12 members) are aggregated using either mean or majority-voting. 
Each sampling method has its strengths and weaknesses: Patches are much better in terms of accuracy on clean point clouds. In this case features are well preserved and patches have access to the full resolution within a region. 
Since patches have compact support, they perform better also on Drop-L and are somewhat immune to global corruptions (but less so, compared to the two other sampling methods). Curves perform best on Add-G. Intuitively, curves are most likely to sample regions with high conductance, thus global outliers are less likely to be selected. %But they influenced the most by Scale and Rotation. 
Random is excellent in the case of Jitter and generally has the best mCE among sampling methods.
%We note the large diversity and low correlation of these classifiers, leading to the fact that 
The aggregated ensemble results, either using mean or majority-voting, are both better than any specific sampling method. Mean yields the best mCE, 
while majority voting, a popular ensemble aggregation method, turns out to be slightly worse (this trend is consistent when using other networks as well).



% {\bf Generalization to unknown corruptions.}
% % As opposed to augmentation techniques such as PointWolf\cite{pointwolf} or RSMix\cite{rsmix} which inherently violate the OOD assumption with insertion of diverse degradations, our method keep the training set clean from corruptions. 
% In any of the training phases we did not add points to the given input samples. Thus, a good results on Add-Global and Add-Local degradations can inferred as an evidence to the generalization capabilities of our approach. As opposed to augmentation techniques which must be familiar with each of the degradations apriori.

% {\bf Overall accuracy performance.}
% Ren et al.\cite{modelnetc} has demonstrated the tradeoff between overfitting and robustness to domain shift. Thus, it is acceptable that improving robustness is accompanied with slight overall accuracy degradation and a fair comparison in terms of accuracy would be against robust classification methods. These are relatively new field of research, due to the fact that ModelNet-C\cite{modelnetc} dataset was introduced only lately. Two robust approaches are examined, PointGuard\cite{pointguard} and RPC\cite{modelnetc}. Both have lower accuracy than other conventional methods. In comparison to them our approach yield higher clean accuracy and better MCE score.
% Because the training phase is pure from corruptions pollution. Our experimental analysis suggest that the robustification is sourced by the sub-sampling schemes and the accuracy improved by ensemble aggregation. Therefore, EASAM is able to improve accuracy, fortunately only with a slight degradation in robustness.

% {\bf EPiC Vs WolfMix.}
% To clean the table regarding the efficiency of our approach comparing to augmentation techniques, we augment the input samples using WolfMix\cite{pointwolf, rsmix, modelnetc}, then apply our method on the augmented samples. The results comparing to GDANet\cite{gdanet} with WolfMix (current SOTA results of augmented network \cite{modelnetc}) are presented in Table \ref{table:comparison_augmented}. % talk more about the results. 
% Note that our results on Jitter, Drop-Global and Drop-Local without any extra augmentation is better than SOTA GDANet+WolfMix on these degradations.
% Extra explanations on WolfMix evaluation are in the Supp.

\begin{figure}[ptbh!]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width = 
   \linewidth, height =0.68 
   \linewidth]{figures/corruptions5.pdf}
   \caption{{\bf Exposure to corruption modeled as noise}. This illustration is based on the following experiment: All instances of class Table were corrupted by Add-Local. Curve is used for sampling. The sampled instances were divided into two groups: {\bf Unexposed} (left) are curves containing less than 10 corrupted points. {\bf Exposed} (right) are curves containing more than 50 corrupted points. Soft-Max predictions of each group were averaged (bottom row). The prediction of exposed curves is almost random (highly noisy) and can be well handled by averaging over the ensemble outputs. Unexposed instances are classified well.}
   \label{fig:noise}
\end{figure}

%\gnote{Removed ablation studies section.}

%%%%%%%%%% ================================

%\section{Ablation Studies}
%We conducted a comprehensive ablative study in order to better understand the impact of each hyper-parameter under corruptions and in relation to the overall accuracy. The studied hyper-parameters are (by the following order): Number of nearest neighbors for curve extraction, \textit{$M$}; Number of Curve points, \textit{$N_c$}; Number of patch points, \textit{$N_p$}; Number of random points, \textit{$N_r$} and sub-sample size, \textit{$\tilde{K}$}.
%Our parameter search was performed in the following manner: The base parameters were chosen to be $M=40$, $N_c=576$, $N_p=512$, $N_r=128$ and $\tilde{K}=32$. At each stage we  scanned the values of a specific parameter, while fixing all other parameters. 
%We take the optimal parameter  value after each stage. The optimized value was chosen manually by considering both overall accuracy and robustness to corruptions. In order to save training phases, we first optimize using mean as an aggregating scheme.
%The optimized hyper-parameters are: $M=40$, $N_c=512$, $N_p=512$, $N_r=128$ and $\tilde{K}=4$.
%We observe that local sub-samples should be large enough to allow both accuracy and robustness. A global sub-sampling, such as random, requires far less points. Surprisingly, the ensemble size may be very small. Good results are obtained for $\tilde{K}=4$, allowing very reasonable inference time. 
%\gnote{See more details in the supplementary.}
%See Figs. \ref{fig:ablation_m}, \ref{fig:ablation_only_nc},\ref{fig:ablation_only_np}, \ref{fig:ablation_only_nr}, and \ref{fig:ablation_only_k_tilde}.
%\gnote{Move all figs to supp.}
%%%%%%%%%% ================================


%\subsection{Model Size}
%In classical ensemble learning, which relies on process randomness, as explained in the introduction, the number of networks is the ensemble size $K$. Since we use homogeneous networks and heterogeneous inputs, the required number of networks is equal to  the number of sub-sampling mechanisms (3 in our case). This makes our approach much lighter in terms of hardware consumption, which is comparable to other (non-ensembled) existing methods.
%The model size of our approach in comparison to several other networks is shown in Table \ref{table:model_size}.
%\begin{table}[ht]
% \centering
%  \begin{tabular}{c | c}
%    \hline
 %   Model & \# params (M)\\
%    \hline
%    CurveNet\cite{curvenet} & $\sim$ 2.1\\
%    DGCNN\cite{dgcnn} & $\sim$ 1.8\\
%    PCT\cite{pct} & $\sim$ 2.8\\
 %   PointNet\cite{pointnet} & $\sim$ 3.4\\
 %   PointNet++\cite{pointnet++} & $\sim$ 1.7\\
%    RPC\cite{modelnetc} & $\sim$ 2.7\\
 %   \hline
 %   EPiC-Net+EASAM (\#12) & $\sim$ $3*1.1+0.03= 3.33$,\\
%    Classic ensemble (\#12) & $\sim$ $12*1.1=13.2$\\
 %   
%    \hline
 % \end{tabular}
 % \caption{{\bf Model Size.} Our total network size is comparable to other methods and significantly better then classical ensemble learning with the same ensemble size.}
%  \label{table:model_size}
%\end{table}


\begin{table}[!ptbh]
  \centering
    \begin{tabular}{p{2.8cm} || p{1.0cm} p{1.0cm} p{1.5cm}}
    \hline
    Network & OA ↑ & mCE ↓ & \#parameters\\
    \hline
    \bf{Classic DGCNN} & \bf{92.6\%} & \bf{1.000} & \bf{1.8M}\\
    \hline
    \bf{EPiC based on} &  &  & \\
    \hline
$\quad$ DGCNN-v1 & 88.7\% & 1.041 & 159K\\
 $\quad$   \bf{DGCNN-v2} & \bf{92.2\%} & \bf{0.773} & \bf{636K}\\ 
 $\quad$   DGCNN-v3 & 92.3\% & 0.720 & 2.47M\\ 
$\quad$    DGCNN-v4 & 93.0\% & 0.669 & 5.4M\\
$\quad$    DGCNN-v5 & 92.7\% & 0.684 & 39M\\  %    \bf{DGCNN-v2} & \bf{92.2\%} & \bf{0.773} & \bf{636K}\\ 
%EPiC(DGCNN)$^{12}$-v1 & 88.7\% & 1.041 & 159K\\
%   \bf{DGCNN-v2} & \bf{92.2\%} & \bf{0.773} & \bf{636K}\\     
%    EPiC$_{DGCNN}^{12}$-v3 & 92.3\% & 0.720 & 2.47M\\ 
%    EPiC(DGCNN-v4)$^{12}$ & 93.0\% & 0.669 & 5.4M\\
%    DGCNN-v5(\#12) & 92.7\% & 0.684 & 39M\\  
    \hline
  \end{tabular}
  \caption{{\bf Network size vs Performance. 
  } EPiC with DGCNN-v2 is an excellent compromise of a lean architecture, with a small number of parameters, which is  much more robust (and almost as accurate), compared to the original full DGCNN network.}
  \label{table:shallowness}
\end{table}

{\bf Affect of network size.}
Since each ensemble member has access only to partial data , we check weather a full size model is required. Five  versions of DGCNN are examined, the most shallow is $v1$ and the deepest is $v5$, where $v4$ is the original network (architectures details appear in the Supp.). The results are shown in Table \ref{table:shallowness}. The number of parameters required for the entire ensemble (3 instances) is shown on the right column. Note that $v2$ yields a robust version with about third of the parameters of the classical network (top row), with just a slight degradation in overall accuracy.

%Several motivations led us to analyze the impact of networks' shallowness on accuracy and robustness:
%1) Since only partial data inserted to each of the networks, using same amount of parameters may be redundant and in extreme case may even cause overfit.
%2) Boosting theory advocate that as long as the classifiers diverse enough, even weak learners can be sufficient to create strong combined classifier.
%3) Lower the computational cost for memory demanding applications. Worth to mention that point cloud classification networks have relatively low amount of learnable parameters, therefore such parameters reducing may intercept as a negligible. But our analyze shed light on a general idea that can adopt in other fields as well.

%We analyze the trade-off between shallowness and performance of our method by constructing 5 levels of shallowness of DGCNN. 

%DGCNN is constructed from 4 levels of depth. On each level, the graph is reconstructed from the previous stage outcome so the features from the current stage are recalculated from the graph constructed from features of previous stage. Therefore, in order to examine the shallowness affect on the results we used different versions of DGCNN, on which each version is constructed out of different amount of graph reconstructions. For example, DGCNN-v1 is built only from one graph construction based on the spatial distance, therefore it is extracting features only from the spatial information. DGCNN-v2 is built from 2 stages of graph constructions, thus, it can extract more semantic contextual information than the shallower version. DGCNN-v1 is up to 64 embedding dimensions, DGCNN-v2 up to 128, such that each version double the number of embedding dimensions that previous version has, therefore the number of parameter is growing exponentially.
%Comparing to the classical DGCNN, It can be seen that combining our method with DGCNN-v2, is yielding much robust network (MCE=0.773 compare to 1.000) with much less learnable parameters (636K compare to 1.8M) at the cost of slight accuracy drop (92.2\% compare to 92.6\%). The results, together with the number of parameters are organized in Table \ref{table:shallowness}



\section{Conclusion}
\label{sec:conc}
In this work, we demonstrated the OOD robustness of ensembles based on partial information input for the task of point cloud classification. The approach relies on obtaining lowly-correlated input samples for each member of the ensemble. We integrate three types of sampling schemes: Curves, Patches and Random. In terms of training - only a single network for each sampling scheme is trained, which  saves training time. The ensemble is created in real time. This naturally increases inference time, but is reasonable for small ensembles (we show a size of $K=12$). For highly demanding real time applications, the networks can be duplicated and processed in parallel. For highly demanding memory consumption applications, significantly smaller networks can be used, which are still highly robust.
%We designed an aggregation module denoted by EASAM that yields a good compromise between overall accuracy and robustness to OOD corruptions. However, a simple mean operation was found to be slightly more robust, with majority-voting performing worse. Our sampling strategy was able to obtain weak learners which are highly diverse. 
Note that ensembles can be distilled back to a single network, for better hardware and time efficiency, as suggested for instance in \cite{shen2019meal}.
%
Since our proposed approach is purely abstract it can be extended to additional problems. We plan to further investigate how such mechanisms can improve robustness in other fields as well.
%Moreover, since our approach is methodological and do not rely on any specific network, we encourage future researchers to apply it on novel networks to leverage robustness to domain shift corruptions.


%\begin{figure}
%  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width=1.0\linewidth, height=0.33\linewidth]{figures/ablation_m.pdf}
%   \caption{{\bf Neighbors in random walk, curve extraction}.}
%   \label{fig:ablation_m}
%\end{figure}
%\begin{figure}
%  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width=1.0\linewidth, height=0.33\linewidth]{figures/ablation_only_nc.pdf}
%   \caption{{\bf Curve sub-sample size}.}
%   \label{fig:ablation_only_nc}
%\end{figure}
%\begin{figure}
%  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width=1.0\linewidth, height=0.33\linewidth]{figures/ablation_only_np.pdf}
%   \caption{{\bf Patch sub-sample size}.}
%   \label{fig:ablation_only_np}
%\end{figure}
%\begin{figure}
%  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width=1.0\linewidth, height=0.33\linewidth]{figures/ablation_only_nr.pdf}
%   \caption{{\bf Random sub-sample size}.}
%   \label{fig:ablation_only_nr}
%\end{figure}
%\begin{figure}
%  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width=1.0\linewidth, height=0.33\linewidth]{figures/ablation_only_k_tilde.pdf}
%   \caption{{\bf Ensemble size per sub-sample.
%   \gnote{Move to Supp.}
%  }}
%   \label{fig:ablation_only_k_tilde}
%\end{figure}

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egpaper_for_review}
}
% \include{supp.tex}
\pagebreak
\section{Ablation Studies}
We conducted comprehensive ablative studies in order to better understand the impact of each hyper-parameter. The hyper-parameters are: Number of nearest neighbors for curve extraction, \textit{$M$}; Number of points in Curve sampling, \textit{$N_c$}; Number of points in Patch sampling, \textit{$N_p$}; Number of points in Random sampling, \textit{$N_r$} and number of partial point clouds for each sampling method, \textit{$\tilde{K}$} (we retained the restriction that $\tilde{K}$ is the same for all sampling methods).
Our parameter search was performed in the following manner: The base parameters were chosen to be $M=40$, $N_c=576$, $N_p=512$, $N_r=128$ and $\tilde{K}=32$. At each stage we  scanned the values of a specific parameter, while fixing all other parameters (the order is as listed above). 
We fixed the parameter to the optimal value after each stage. The optimized value was chosen manually by considering both overall accuracy and robustness to corruptions. Mean is used as an aggregating scheme.
The chosen values of hyper-parameters are: $M=40$, $N_c=512$, $N_p=512$, $N_r=128$ and $\tilde{K}=4$.
We observe that local sub-samples should be large enough to allow both accuracy and robustness. A global sub-sampling, such as random, requires far less points. Surprisingly, the ensemble size may be very small. Good results are obtained for $\tilde{K}=4$, allowing very reasonable inference time. 
See Figs \ref{fig:ablation_m}, \ref{fig:ablation_only_nc}, \ref{fig:ablation_only_np}, \ref{fig:ablation_only_nr} and \ref{fig:ablation_only_k_tilde}.

\textbf{Extended Ablation Studies}
We demonstrate the influence of each hyper-parameter under any of the corruptions in Figs \ref{fig:ablation_nc_m_np} and \ref{fig:ablation_nr_k_tilde}. The most meaningful hyper-parameter is $\tilde{K}$. Ensemble size of 12 yielding very good trade-off between running time, overall accuracy and robustness to OOD degradations. All corruptions are improved when enlarging the ensemble size, where Add-L is affected the most.

% \section{A remark on Test Time Ensemble vs. Voting} The terms 
% \gnote{Not sure this section is needed (opens new fronts "of attack" by revs.) In any case if we keep it - it should be at the end of the supp.}

% \textit{Rotation Vote} or \textit{Scaling Vote} are typically used to describe an ensemble method implemented at test-time that is used to boost an existing network performance. PointNet and PointNet++ are applying \textit{Rotation Vote} by multiply rotating and shuffling the point cloud. An average of the predictions is then taken. RSCNN and DensePoint are performing a similar procedure with random scale, referred to as \textit{Scaling Vote}. To evaluate correctly OOD robust classification (against unknown corruptions, for example of rotation or scale), these techniques should naturally not be used. In principle they model and take into account a specific corruption and thus violate the OOD principle. The reasoning is similar to training with augmentations that take into account certain corruptions. Our approach, on the other hand, is inherently different, for the reasons detailed below. Thus, the comparison to other stand-alone networks is ''clean'' from OOD pollution. The differences are that  

% 1. Whereas the above mentioned techniques inherently violate the OOD assumption by insertion of a specific degradation, our ensemble consists of sub-sampling procedures which are completely general and are not directly related to any specific corruption.

% 2. our generalization capability for unknown possible corruptions is larger as demonstrated at the main paper.

\section{Training procedure} We train all three partial point-cloud networks simultaneously and derivate each sub-sampling network's loss with regard to the entire point-cloud label as detailed in Algorithm \ref{alg:training}


\section{Sub-Samples analysis of more models}
\textbf{GDANet.} For training, We lower the batch size to 50 and changed the GDM Module to accept point clouds with less than 256 elements by lower M value to 128 in Geometry-Disentangle Module (GDM).

All other networks did not require any extra tweaks, the analysis results of all networks is detailed in Tables
\ref{table:gdanet_unaugmented_sub_samples},
\ref{table:gdanet_augmented_sub_samples},
\ref{table:dgcnn_unaugmented_sub_samples}, 
\ref{table:dgcnn_augmented_sub_samples}, 
\ref{table:pct_unaugmented_sub_samples},
\ref{table:pct_augmented_sub_samples}, 
\ref{table:rpc_unaugmented_sub_samples} and
\ref{table:rpc_augmented_sub_samples}. 

\section{Network size (shallowness) analysis}

DGCNN is constructed from 4 levels of depth. Features of current level are recalculated based on features of previous level. In order to examine the required depth for our case we used different versions of DGCNN. Each version is constructed using a different number of graph reconstructions. For example, DGCNN-v1 is built only from one graph based on the spatial distance, therefore features are directly derived from  spatial information. As depth increases (DGCNN-v2 through DGCNN-v5) the network acquires additional, more complex,  semantic contextual information. DGCNN-v1 has up to 64 embedding dimensions, DGCNN-v2 up to 128, and so on. Each version doubles the embedding dimensions  (which grows exponentially with depth).
Comparing to the classical DGCNN (v4), we observe  that EPiC with DGCNN-v2 yields a highly robust network (MCE=0.773 compare to 1.000) with much less learnable parameters (636K compare to 1.8M). This comes at the cost of a slight accuracy drop (92.2\% compared to 92.6\%). Thus, the EPiC framework can be very lean and economic from a production perspective.


\section{Corruptions and sub-sampling}
% \gnote{These are good examples in the Figs. It should be in the right order, why we refer to Figs 9-12 here??}

Here we give some of our insights on how the different sub-sampling methods react to different corruptions. Examples are shown in Figs. \ref{fig:supp_add_g}, \ref{fig:supp_drop_g}, \ref{fig:supp_add_l} and \ref{fig:supp_drop_l}.

{\bf Add global.} Curves are highly insensitive to relatively far points (which often appear in Add-G). Therefore, they perform best under this corruption. Patches and Random are not influenced by the conductance, thus the added points may be considered as well, corrupting the sample. 

{\bf Add local.} Here, the corruption is smoother in space and may be interpreted as part of the shape. Patches and Curves are more likely to be affected by it. 

{\bf Drop Global.} Drop Global can be viewed as globally changing the density of the points of the shape. Therefore, it makes sense that there is a direct proportion between sub-sampling performance and typical density (Patches are dense while Random is sparse, Curves are somewhere in between).
Moreover, we take into account a variable number of points, e.g. for Patches $min(N,N_p)$. Thus, when the number of points is lower, as in the case of Drop-Global, Patches cover more of the shape.

{\bf Drop Local.} This corruption may cause separation between different shape parts (creating disconnected regions). Curves may thus be ``stuck'' in a disconnected region. Patches are less affected by this corruption and perform better. 

% \section{Training procedure}
% Our framework was trained in a dual stage process. First, we trained basic networks. Then, we set their parameters to be fixed and trained only with regard to EASAM's parameters. We examine this procedure choice on PicNet by training it with EASAM in the same training process. This means that during training, an ensemble member has access (through the SGD training) to the results of other members, fact that we think that may cause lazy classifiers. We trained with ensemble size of 12. All other hyper-parameters are the same as in the original process (beside batch size now set to 32 and for 200 epochs). We observe that having two separate training phases is crucial both in terms of robustness and of overall accuracy. The results are summarized in Table \ref{table:all_in_once}.
% \begin{table*}[h]
%   \centering
%     \begin{tabular}{p{3cm} | p{2cm} p{3cm}}
%     \hline
%     Training Procedure & OA ↑ & mCE ↓ \\
%     \hline
%     Dual stage & \bf{93.2}\% & \bf{0.719} \\
%     \hline
%     Single stage & 91.0\% & 0.844 \\
%     \hline
%   \end{tabular}
%   \caption{\bf{Training Procedure.}}
%   \label{table:all_in_once}
% \end{table*}

% \section{{\bf Diversity matters}}
% To emphasize the significance of the ensemble diversity, we conducted a comparison in which each sub-sampling ensemble consists of 12 members. This is the same size as the total aggregated. The performance of each sub-sampling method is improved. Nevertheless, aggregations are still much better than any of the sub-sampling methods. Please see Table \ref{table:diversity_matters}

% \begin{table*}[h]
%   \centering
%     \begin{tabular}{p{5cm} | p{1cm}  p{1cm} p{1cm} p{1cm} p{1cm} p{1cm} p{1cm} p{1cm} p{1cm}}
%     \hline
%     Sub-samples (\#Ensemble size) & OA ↑ & mCE ↓ & Scale & Jitter & Drop-G & Drop-L & Add-G & Add-L & Rotate \\
%     \hline
%     Curves (\#12) & 90.0\% & 0.946 & 1.574 &  0.946 & 0.484 & 0.657 & 0.386 & 1.200 & 1.372\\
%     Patches (\#12) & 92.2\% & 0.909 & \bf{1.117} &  1.234 & 0.629 & 0.531 & 0.668 & 1.069 & \bf{1.116}\\
%     Random (\#12) & 91.0\% & 0.794 & 1.436 &  \bf{0.424} & 0.363 & 0.739 & 0.525 & 0.76 & 1.312\\
%     \hline
%     Majority Voting (\#12) & 92.5\% & 0.730 & 1.191 & 0.687 & 0.383 &  0.502 & 0.447 & {\bf0.749} &  1.149\\
%     Mean (\#12) & 92.6\% & {\bf0.694} & 1.128 &  0.633 & {\bf0.355} &  {\bf0.493} & {\bf0.376} & 0.753 &  1.121\\
%     EASAM (\#12) & {\bf93.2}\% & 0.719 & 1.128 &  0.639 & 0.367 & 0.512 & 0.427 & 0.811 &  1.121\\
%     % EASAM (ours) & {\bf93.2}\% & {\bf0.701} & {\bf1.085} & {\bf0.592}  & 0.367 &  0.512 & 0.400 & {\bf0.782} &  {\bf1.172}\\
    
%     \hline
%   \end{tabular}
%   \caption{{\bf Diversity matters.} {\bf Bold} best.}
%   \label{table:diversity_matters}
% \end{table*}

% \section{{\bf Technical Choices Considerations}}

% {\bf{EASAM.}} After predicting classification vector for each sub-sample scheme by the basic network, we can interpret each such vector as a global sub-sample representation, and leverage self-attention mechanism in order to tokenize each partial point-cloud. Then, learn a global context of the entire shape by the attention  between the partial samples. This mechanism is done for example in PointBert or PCT. Since the number of parameters of EASAM is negligible in terms of machine learning networks (please refer to section \textit{Model Size} in the main paper), we did not conducted ablative study regarding the contribution of each component of EASAM.

% {\bf{PiC-net.}} Our main motivation of architectural design is to limit the amount of memory consumption and running time of the network. CurveNet is a good starting point, since it can better handle unstructured samples like curves. Moreover, it has good MCE score, and generally perform well under corruptions. CurveNet stacked 8 Curve Intervention Convolution (CIC) blocks together to construct a ResNet style network. We cut the number of CICs to 2 instances only, and receive deeper learning by combining it with self and cross attention layers. In order to learn from various depths we concatenated intermediate features along the network. We test PiC-net with 12 different randomly initialized networks and achieved 92.0\%-92.7\% on the clean test set. Therefore, we do lead to degradation in the clean set comparing to the original CurveNet, but as a result we came up with much lighter network. Note that we do not expect the best clean result from each of the classifiers, but rather expect the combination between them to yield good results.

% \section{Exposure to corruption modeled as noise - More examples}
% We repeat the experiment for more classes as demonstrated for airplane in Fig. \ref{fig:airplane_noise}, for person in Fig. \ref{fig:person_noise} and for bottle in Fig. \ref{fig:person_noise}. Airplane is a well populated class, therefore also exposed curves in add local corruption are well predicted. In the other hand, exposed curves on bottles are biased to predict plant. There exist examples in between, like the person class, on which some exposed curves are still predicting a person but some others are predicting piano.

% \begin{figure}[h]
%  \centering
  
%   \includegraphics[width=0.8\linewidth, height=0.5\linewidth]{figures/airplane_noise.pdf}
%   \caption{}
%   \label{fig:airplane_noise}
% \end{figure}

% \begin{figure}
%  \centering
  
%   \includegraphics[width=0.8\linewidth, height=0.5\linewidth]{figures/person_noise.pdf}
%   \caption{}
%   \label{fig:person_noise}
% \end{figure}


% \begin{figure}
%  \centering
  
%   \includegraphics[width=0.8\linewidth, height=0.5\linewidth]{figures/bottle_noise.pdf}
%   \caption{}
%   \label{fig:bottle_noise}
% \end{figure}

\subsection{Networks and augmentation evaluation}
Evaluation of most augmented and un-augmented networks was reproduced by the code supplied by Ren et al. Additionally, in the ensemble section we show results on PointGuard. These experiments are detailed below. 

% \textbf{Classical Ensemble.} We trained 12 identical instances of PiC-net for 300 epochs with the same learning parameters provided above. Each ensemble member receives the entire point cloud as input. Due to  randomness of initialization and training we obtain some diversity in the ensemble. The overall accuracy was in the range $[92.0, 92.7]$. We examined both mean and majority voting as aggregation methods. These ensembles  achieve good robustness for Jitter but performed worse than most networks on Add-Global. 

\textbf{Point Guard.} This method obtains highly sparse random sub-samples (they suggest $N=16$), using a large ensemble of size $K=1,000$. Its theoretical analysis and experimental setting do not assume a specific architecture for the classifier. Thus,  DGCNN is used as the basic classifier. As suggested in PointGuard we trained our classifier for 7500 epochs to work on the sub-samples, obtaining an overall accuracy of $80.5\%$ (estimated on randomly 4 sub-samples per each sample in the test set). Predictions are aggregated using majority voting. 
Note that to obtain provable robustness they use very small $N$ and try to compensate by extremely large $K$. However this tradeoff turns out to be significantly inferior in terms of overall accuracy and less competitive in $mCE$.

\textbf{WolfMix.} We follow ModelNet-C evaluation metrics.
we use the default hyper-parameters in PointWOLF (Kim et al., 2021). We set the number of
anchors to 4, sampling method to farthest point sampling, kernel bandwidth to 0.5, maximum local rotation range to 10 degrees, maximum local scaling to 3, and maximum local translation to 0.25. AugTune proposed along with PointWOLF is not used in training. For the mixing step, we use the default hyper-parameters in RSMix (Lee et al., 2021). We set RSMix probability to 0.5, $\beta$ to 1.0, and the maximum number of point modifications to 512. For training, the number of neighbors in k-NN is reduced to 20, the number of epochs is increased to 500.

\section{On Batch Normalization and OOD Robustness}
Using fixed learnt batch normalization parameters at test time can be problematic, since it assumes that the samples of the train and test are both approximately of similar  distributions. However, different types of corruptions can yield different statistics.
Obviously, learning the statistics at test time may violate the OOD principle. Nevertheless, for some ``offline'' applications, which can work in batches, we wanted to examine whether test time batch normalization is helpful  
%to let the network learn the normalization parameters during test time in terms of 
and increases robustness. To demonstrate this idea, normalization parameters were computed during evaluation (with a batch size of 256). We observe that $mCE$ significantly improves, as can be seen in Table \ref{table:batchnorm}.

% \section{More uniform and non-uniform degradations}
% \begin{table*}[h]
%   \centering
%     \begin{tabular}{p{4cm} || p{1.4cm} p{1.4cm} p{1.4cm} p{1.4cm} p{1.4cm} p{1.4cm}}
%     \hline
%     Method & AbsX ↑ & AbsY ↑  & AbsZ↑ & Shift(1)↑ & double\\
%     \hline
%     DGCNN-Classic  & 81.5\% & 68.7\% & 69.3\% & 80.2\%& 85.9\%\\
%     DGCNN-EPiC (\#12) & 85.4\% & 69.8\% & 78.8\% & 82.7\% & 88.9\% \\
    
%     \hline
%   \end{tabular}
%   \caption{{\bf Un-Augmented DGCNN sub-samples analysis.} {\bf Bold} best.}
%   \label{table:dgcnn_unaugmented_sub_samples}
% \end{table*}





\begin{table*}
  \centering
    \begin{tabular}{p{4cm} || p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.2cm} p{1.2cm} p{1.1cm} p{1.1cm} p{1.1cm}}
    \hline
    Sub-samples & OA ↑ & mCE ↓ & Scale & Jitter & Drop-G & Drop-L & Add-G & Add-L & Rotate \\
    \hline
    GDANet-Curves (\#4) & 91.3\% & 1.047 & 1.298 & 1.551 & 0.423 & 0.599 & 0.336 & 1.549 & 1.572\\
    GDANet-Patches (\#4) & \underline{93.2\%} & 0.861 & \underline{0.957} & 1.358 & 0.504 & 0.551 & 0.481 & 1.044 & \textbf{1.130}\\
    GDANet-Random (\#4) & 90.9\% & 0.819 & 1.287 & \textbf{0.462} & 0.359 & 0.836 & 0.512 & 0.898 & 1.381\\
    \hline
    GDANet-Mean (\#12) & \textbf{93.6\%} & \textbf{0.704} & \textbf{0.936} & \underline{0.864} & \textbf{0.315} &  \textbf{0.478} & \textbf{0.295} & \textbf{0.862} &  \underline{1.177}\\
    GDANet-Maj. Vot.(\#12) & \underline{93.2\%} & \underline{0.749} & 0.968 & 1.028 & \underline{0.343} &  \underline{0.498} & \underline{0.325} & \underline{0.876} &  1.205\\
    
    \hline
  \end{tabular}
  \caption{{\bf Un-Augmented GDANet sub-samples analysis.} {\bf Bold} best. \underline{Underline} second best.}
  \label{table:gdanet_unaugmented_sub_samples}
\end{table*}

\begin{table*}
  \centering
    \begin{tabular}{p{4cm} || p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.2cm} p{1.2cm} p{1.1cm} p{1.1cm} p{1.1cm}}
    \hline
    Sub-samples & OA ↑ & mCE ↓ & Scale & Jitter & Drop-G & Drop-L & Add-G & Add-L & Rotate \\
    \hline
    GDANet-Curves (\#4) & 90.8\% & 0.740 & 1.266 & 0.984 & 0.411 & 0.614 & 0.332 & 0.818 & 0.758\\
    GDANet-Patches (\#4) & 92.1\% & 0.667 & \underline{0.979} & 1.275 & 0.464 & 0.493 & 0.353 & 0.531 & \textbf{0.572}\\
    GDANet-Random (\#4) & 90.8\% & 0.646 & 1.234 & \textbf{0.462} & 0.383 & 0.758 & 0.359 & 0.571 & 0.758\\
    \hline
    GDANet-Mean (\#12) & \textbf{92.5\%} & \textbf{0.530} & \textbf{0.968} & \underline{0.639} & \textbf{0.343} &  \textbf{0.473} & \textbf{0.275} & \textbf{0.433} &  \underline{0.577}\\
    GDANet-Maj. Vot.(\#12) & \underline{92.2}\% & \underline{0.558} & 1.000 & 0.725 & \underline{0.355} &  \underline{0.478} & \underline{0.292} & \underline{0.462} &  0.591\\
    
    \hline
  \end{tabular}
  \caption{{\bf WolfMix Augmented GDANet sub-samples analysis.} {\bf Bold} best. \underline{Underline} second best.}
  \label{table:gdanet_augmented_sub_samples}
\end{table*}

\begin{table*}
  \centering
    \begin{tabular}{p{4cm} || p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.2cm} p{1.2cm} p{1.1cm} p{1.1cm} p{1.1cm}}
    \hline
    Sub-samples & OA ↑ & mCE ↓ & Scale & Jitter & Drop-G & Drop-L & Add-G & Add-L & Rotate \\
    \hline
    DGCNN-Curves (\#4) & 90.7\% & 1.069 & 1.628 & 1.297 & 0.431 & 0.729 & \underline{0.363} & 1.618 & 1.414\\
    DGCNN-Patches (\#4) & \underline{92.8\%} & 0.793 & \textbf{0.989} & 1.165 & 0.577 & 0.536 & 0.505 & 0.851 & \textbf{0.930}\\
    DGCNN-Random (\#4) & 91.5\% & 0.766 & 1.234 & \textbf{0.399} & \underline{0.351} & 0.812 & 0.580 & \textbf{0.793} & 1.195\\
    \hline
    DGCNN-Mean(\#12) & \textbf{93.0\%} & \textbf{0.669} & \underline{1.000} &  \underline{0.680} & \textbf{0.331} &  \textbf{0.498} & \textbf{0.349} & 0.807 &  \underline{1.019}\\
    DGCNN-Maj. Vot.(\#12) & 92.6\% & \underline{0.706} & 1.043 &  0.794 & 0.359 &  \underline{0.517} & 0.380 & \underline{0.800} &  1.051\\
    
    \hline
  \end{tabular}
  \caption{{\bf Un-Augmented DGCNN sub-samples analysis.} {\bf Bold} best. \underline{Underline} second best.}
  \label{table:dgcnn_unaugmented_sub_samples}
\end{table*}

\begin{table*}
  \centering
    \begin{tabular}{p{4cm} || p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.2cm} p{1.2cm} p{1.1cm} p{1.1cm} p{1.1cm}}
    \hline
    Sub-samples & OA ↑ & mCE ↓ & Scale & Jitter & Drop-G & Drop-L & Add-G & Add-L & Rotate \\
    \hline
    DGCNN-Curves (\#4) & 89.5\% & 0.802 & 1.426 & 0.896 & 0.468 & 0.676 & 0.386 & 0.873 & 0.888\\
    DGCNN-Patches (\#4) & 91.7\% & 0.618 & \underline{1.053} & 0.823 & 0.536 & 0.512 & 0.380 & 0.433 & \textbf{0.591}\\
    DGCNN-Random (\#4) & 91.2\% & 0.639 & 1.298 & \textbf{0.405} & \underline{0.379} & 0.768 & 0.369 & 0.527 & 0.730\\
    \hline
    DGCNN-Mean (\#12) & \underline{92.1\%} & \textbf{0.529} & \textbf{1.021} &  \underline{0.541} & \textbf{0.355} &  \textbf{0.488} & \textbf{0.288} & \textbf{0.407} &  \underline{0.600}\\
    DGCNN-Maj. Vot.(\#12) & \textbf{92.3\%} & \underline{0.552} & \underline{1.053} &  0.592 & \underline{0.379} &  \underline{0.498} & \underline{0.308} & \underline{0.418} &  0.614\\
    
    \hline
  \end{tabular}
  \caption{{\bf WolfMix Augmented DGCNN sub-samples analysis.} {\bf Bold} best. \underline{Underline} second best.}
  \label{table:dgcnn_augmented_sub_samples}
\end{table*}

\begin{table*}
  \centering
    \begin{tabular}{p{4cm} || p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.2cm} p{1.2cm} p{1.1cm} p{1.1cm} p{1.1cm}}
    \hline
    Sub-samples & OA ↑ & mCE ↓ & Scale & Jitter & Drop-G & Drop-L & Add-G & Add-L & Rotate \\
    \hline
    PCT-Curves (\#4) & 91.1\% & 0.934 & 1.234 & 1.320 & 0.391 & 0.551 & 0.322 & 1.393 & 1.330\\
    PCT-Patches (\#4) & 92.5\% & 0.915 & 0.989 & 1.557 & 0.556 & 0.541 & 0.549 & 1.080 & 1.135\\
    PCT-Random (\#4) & 91.9\% & 0.741 & 1.245 & \textbf{0.427} & 0.335 & 0.744 & 0.502 & 0.785 & 1.149\\
    \hline
    PCT-Mean (\#12) & \textbf{93.4\%} & \textbf{0.646} & \textbf{0.894} & \underline{0.851} & \textbf{0.306} &  \textbf{0.435} & \textbf{0.285} & \textbf{0.735} &  \textbf{1.019}\\
    PCT-Maj. Vot.(\#12) & \underline{93.1\%} & \underline{0.693} & \underline{0.957} & 0.994 & \underline{0.331} &  \underline{0.454} & \underline{0.315} & \underline{0.760} &  \underline{1.042}\\
    
    \hline
  \end{tabular}
  \caption{{\bf Un-Augmented PCT sub-samples analysis.} {\bf Bold} best. \underline{Underline} second best.}
  \label{table:pct_unaugmented_sub_samples}
\end{table*}

\begin{table*}
  \centering
    \begin{tabular}{p{4cm} || p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.2cm} p{1.2cm} p{1.1cm} p{1.1cm} p{1.1cm}}
    \hline
    Sub-samples & OA ↑ & mCE ↓ & Scale & Jitter & Drop-G & Drop-L & Add-G & Add-L & Rotate \\
    \hline
    PCT-Curves (\#4) & 90.4\% & 0.699 & 1.255 & 0.927 & 0.427 & 0.570 & 0.332 & 0.698 & 0.684\\
    PCT-Patches (\#4) & \textbf{92.7\%} & 0.633 & \textbf{0.904} & 1.339 & 0.391 & \textbf{0.420} & 0.312 & 0.505 & \underline{0.558}\\
    PCT-Random (\#4) & 91.0\% & 0.636 & 1.245 & \textbf{0.554} & 0.375 & 0.700 & 0.342 & 0.567 & 0.670\\
    \hline
    PCT-Mean (\#12) & \textbf{92.7\%} & \textbf{0.510} & \underline{0.915} & \underline{0.699} & \textbf{0.323} &  \underline{0.425} & \textbf{0.268} & \textbf{0.404} &  \textbf{0.535}\\
    PCT-Maj. Vot.(\#12) & \underline{92.6\%} & \underline{0.532} & 0.947 & 0.756 & \underline{0.343} &  0.430 & \underline{0.271} & \underline{0.422} &  \underline{0.558}\\
    
    \hline
  \end{tabular}
  \caption{{\bf WolfMix Augmented PCT sub-samples analysis.} {\bf Bold} best. \underline{Underline} second best.}
  \label{table:pct_augmented_sub_samples}
\end{table*}

\begin{table*}
  \centering
    \begin{tabular}{p{4cm} || p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.2cm} p{1.2cm} p{1.1cm} p{1.1cm} p{1.1cm}}
    \hline
    Sub-samples & OA ↑ & mCE ↓ & Scale & Jitter & Drop-G & Drop-L & Add-G & Add-L & Rotate \\
    \hline
    RPC-Curves (\#4) & 91.6\% & 1.068 & 1.287 & 1.680 & 0.399 & 0.594& 0.322 & 1.495 & 1.702\\
    RPC-Patches (\#4) & 92.4\% & 0.934 & 0.979 & 1.532 & 0.488 & 0.536 & 0.498 & 1.233 & \textbf{1.274}\\
    RPC-Random (\#4) & 91.5\% & 0.804 & 1.181 & \textbf{0.491} & 0.355 & 0.739 & 0.498 & \textbf{0.855} & 1.512\\
    \hline
    RPC-Mean (\#12) & \textbf{93.6\%} & \textbf{0.750} & \textbf{0.915} & \underline{1.057} & \textbf{0.323} &  \textbf{0.440} & \textbf{0.281} & \underline{0.902} &  \underline{1.330}\\
    RPC-Maj. Vot.(\#12) & \underline{93.0\%} & \underline{0.791} & \underline{0.957} & 1.168 & \underline{0.343} &  \underline{0.473} & \underline{0.319} & 0.913 &  1.367\\
    
    \hline
  \end{tabular}
  \caption{{\bf Un-Augmented RPC sub-samples analysis.} {\bf Bold} best. \underline{Underline} second best.}
  \label{table:rpc_unaugmented_sub_samples}
\end{table*}

\begin{table*}
  \centering
    \begin{tabular}{p{4cm} || p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.2cm} p{1.2cm} p{1.1cm} p{1.1cm} p{1.1cm}}
    \hline
    Sub-samples & OA ↑ & mCE ↓ & Scale & Jitter & Drop-G & Drop-L & Add-G & Add-L & Rotate \\
    \hline
    RPC-Curves (\#4) & 91.7\% & 0.686 & 1.106 & 1.038 & 0.375 & 0.551 & 0.315 & 0.698 & 0.716\\
    RPC-Patches (\#4) & \underline{92.2\%} & 0.603 & 0.957 & 1.190 & 0.399 & \underline{0.430} & 0.298 & 0.415 & \textbf{0.535}\\
    RPC-Random (\#4) & 91.2\% & 0.609 & 1.170 & \textbf{0.459} & 0.359 & 0.662 & 0.342 & 0.542 & 0.730\\
    \hline
    RPC-Mean (\#12) & \textbf{92.7\%} & \textbf{0.501} & \textbf{0.915} & \underline{0.680} & \textbf{0.315} &  \textbf{0.420} & \textbf{0.251} & \textbf{0.382} &  \underline{0.544}\\
    RPC-Maj. Vot.(\#12) &\textbf{92.7\%} & \underline{0.526} & \underline{0.947} & 0.766 & \underline{0.335} &  \textbf{0.420} & \underline{0.268} & \underline{0.400} &  0.549\\
    
    \hline
  \end{tabular}
  \caption{{\bf WolfMix Augmented RPC sub-samples analysis.} {\bf Bold} best. \underline{Underline} second best.}
  \label{table:rpc_augmented_sub_samples}
\end{table*}

\algrenewcomment[1]{\(\triangleright\) #1}
\algnewcommand{\LineComment}[1]{\State \(\triangleright\) #1}

\begin{algorithm*}[h]
\caption{Classification using EPiC (Training)}\label{alg:training}
\begin{algorithmic}

\For{$epoch \in epochs$} 
\For{$X, label \in TrainingDataSet$} 

\State $anchors \gets RandomlySelect([0:1023])$

\For{$anchor \in anchors$} 
\LineComment{Fetch partial point clouds}
\State $Patch \gets FetchPatch(X, anchors(k))$
\State $Curve \gets FetchCurve(X, anchors(k))$
\State $Random \gets FetchRandom(X)$

\LineComment{Apply models}
\State $P_{Patch}\gets model_{Patches}(Patch)$
\State $P_{Curve} \gets model_{Curves}(Curve)$
\State $P_{Random} \gets model_{Random}(Random)$

\LineComment{Derivate with regard to the entire point-cloud label}
\State $Loss_{Patch} \gets backward(P_{Patch}, label)$ 
\State $Loss_{Curve} \gets backward(P_{Curve}, label)$
\State $Loss_{Random} \gets backward(P_{Random}, label)$

\EndFor

\EndFor

\EndFor

\end{algorithmic}
\end{algorithm*}

\begin{table*}
  \centering
  \begin{tabular}{c | c | c}
    \hline
    Method & OA ↑ & mCE \\
    \hline
    DGCNN & 93.0\% & 0.669\\
    DGCNN + BatchNorm & 92.7\% & 0.527\\
    \hline
    DGCNN (W.M) & 93.2\% & 0.590\\
    DGCNN (W.M) + BatchNorm & 92.0\% & 0.512\\
    \hline
  \end{tabular}
  \caption{{\bf BatchNorm at test time.} Whereas overall accuracy is slightly degraded, OOD robustness is significantly increased, yielding lower $mCE$. This violates standard OOD assumptions but may be useful in some scenarios.}
  \label{table:batchnorm}
\end{table*}

\begin{figure*}
 \centering
  
  \includegraphics[width=0.5\linewidth, height=0.2\linewidth]{figures/ablation_m.pdf}
  \caption{{\bf Neighbors in random walk, curve extraction}.}
  \label{fig:ablation_m}
\end{figure*}
\begin{figure*}
 \centering
  \includegraphics[width=0.5\linewidth, height=0.2\linewidth]{figures/ablation_only_nc.pdf}
  \caption{{\bf Curve sub-sample size}.}
  \label{fig:ablation_only_nc}
\end{figure*}
\begin{figure*}
 \centering
  \includegraphics[width=0.5\linewidth, height=0.2\linewidth]{figures/ablation_only_np.pdf}
  \caption{{\bf Patch sub-sample size}.}
  \label{fig:ablation_only_np}
\end{figure*}
\begin{figure*}
 \centering
  \includegraphics[width=0.5\linewidth, height=0.2\linewidth]{figures/ablation_only_nr.pdf}
  \caption{{\bf Random sub-sample size}.}
  \label{fig:ablation_only_nr}
\end{figure*}
\begin{figure*}
 \centering
  \includegraphics[width=0.5\linewidth, height=0.2\linewidth]{figures/ablation_only_k_tilde.pdf}
  \caption{{\bf Ensemble size per sub-sample.
  % \gnote{Move to Supp.}
 }}
  \label{fig:ablation_only_k_tilde}
\end{figure*}


\begin{figure*}
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.6\linewidth, height=0.7\linewidth]{figures/ablation_nc_m_np.pdf}

   \caption{\bf{$N_c$, $M$ and $N_p$}} 
   \label{fig:ablation_nc_m_np}
\end{figure*}
\begin{figure*}
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.6\linewidth, height=0.5\linewidth]{figures/ablation_nr_k_tilde_small.pdf}

   \caption{\bf{$N_r$ and $\tilde{K}$.}} 
   \label{fig:ablation_nr_k_tilde}
\end{figure*}

% \textbf{CurveNet.} Something on CurveNet
% \begin{table*}[h]
%   \centering
%     \begin{tabular}{p{4cm} || p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.2cm} p{1.2cm} p{1.1cm} p{1.1cm} p{1.1cm}}
%     \hline
%     Sub-samples & OA ↑ & mCE ↓ & Scale & Jitter & Drop-G & Drop-L & Add-G & Add-L & Rotate \\
%     \hline
%     CurveNet-Curves (\#4) & 91.1\% & 0.934 & 1.234 & 1.320 & 0.391 & 0.551 & 0.322 & 1.393 & 1.330\\
%     CurveNet-Patches (\#4) & 92.5\% & 0.915 & 0.989 & 1.557 & 0.556 & 0.541 & 0.549 & 1.080 & 1.135\\
%     CurveNet-Random (\#4) & 91.9\% & 0.741 & 1.245 & 0.427 & 0.335 & 0.744 & 0.502 & 0.785 & 1.149\\
%     \hline
%     CurveNet-Mean (\#12) & 91.9\% & 0.746 & 1.277 & 0.608 & 0.363 &  0.570 & 0.495 & 1.036 &  0.874\\
    
%     \hline
%   \end{tabular}
%   \caption{{\bf Un-Augmented CurveNet sub-samples analysis.} {\bf Bold} best.}
%   \label{table:dgcnn_sub_samples}
% \end{table*}

\begin{figure*}[h]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1\linewidth, height=0.75\linewidth]{figures/supp_add_g.pdf}

   \caption{\bf{Add Global}} 
   \label{fig:supp_add_g}
\end{figure*}


\begin{figure*}[h]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1\linewidth, height=0.75\linewidth]{figures/supp_drop_g.pdf}

   \caption{\bf{Drop Global}} 
   \label{fig:supp_drop_g}
\end{figure*}

\begin{figure*}[h]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1\linewidth, height=0.75\linewidth]{figures/supp_add_l.pdf}

   \caption{{\bf Add Local.}
   }
   \label{fig:supp_add_l}
\end{figure*}

\begin{figure*}[h]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1\linewidth, height=0.75\linewidth]{figures/supp_drop_l.pdf}

   \caption{{\bf Drop Local.}
   }
   \label{fig:supp_drop_l}
\end{figure*}


\end{document}