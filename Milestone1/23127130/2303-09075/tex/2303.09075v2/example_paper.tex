%%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}

\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{svg}
\usepackage{amssymb}
\usepackage{CJKutf8}
\usepackage{url}
\usepackage{caption}
\usepackage{graphicx} 
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage[english]{babel}
\usepackage[normalem]{ulem}

\usepackage[symbol]{footmisc}

% \renewcommand{\thefootnote}{\fnsymbol{footnote}}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{}

\begin{document}

\twocolumn[
\icmltitle{Self-Consistent Learning: \\Cooperation between Generators and Discriminators}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Tong Wu}{equal,yyy}
\icmlauthor{Hao Wang}{equal,comp}
\icmlauthor{Zhongshen Zeng}{comp}
\icmlauthor{Wei Wang}{yyy}
\icmlauthor{Hai-Tao Zheng}{yyy,pc}
\icmlauthor{Jiaxing Zhang}{comp}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Shezhen International Graduate School, Tsinghua Universiy}
\icmlaffiliation{comp}{International Digital Economy Academy}
\icmlaffiliation{pc}{Pengcheng Laboratory}

\icmlcorrespondingauthor{Hai-Tao Zheng}{zheng.haitao@sz.tsinghua.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\footnotetext[3]{The work was done while Tong Wu was an intern at IDEA}
%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Using generated data to improve the performance of downstream discriminative models has recently gained popularity due to the great development of pre-trained language models. In most previous studies, generative models and discriminative models are trained separately and thus could not adapt to any changes in each other. As a result, the generated samples can easily deviate from the real data distribution, while the improvement of the discriminative model quickly reaches saturation. Generative adversarial networks (GANs) train generative models via an adversarial process with discriminative models to achieve joint training. However, the training of standard GANs is notoriously unstable and often falls short of convergence. In this paper, to address these issues, we propose a \textit{self-consistent learning} framework, in which a discriminator and a generator are cooperatively trained in a closed-loop form. The discriminator and the generator enhance each other during multiple rounds of alternating training until a scoring consensus is reached. This framework proves to be easy to train and free from instabilities such as mode collapse and non-convergence. Extensive experiments on sentence semantic matching demonstrate the effectiveness of the proposed framework: the discriminator achieves 10+ AP of improvement on the zero-shot setting and new state-of-the-art performance on the full-data setting. 
\end{abstract}
\section{Introduction}

The advance of Pre-trained Language Models (PLMs) \citep{DBLP:conf/nips/BrownMRSKDNSSAA20,DBLP:journals/corr/abs-2204-02311} has substantially improved the performance of deep neural networks across a variety of Natural Language Processing (NLP) tasks \citep{DBLP:journals/csur/DongLGCLSY23}. Various language models, based on the Transformer \citep{vaswani2017attention} architecture,  have been proposed, leading to state-of-the-art (SOTA) performance on the fundamental discrimination tasks. These models are first trained with self-supervised training objectives (e.g., predicting masked tokens according to surrounding tokens) on massive unlabeled text data, then fine-tuned on annotated data to adapt to downstream tasks of interest.  However, annotated data is usually limited to a wide range of downstream tasks, which results in overfitting and a lack of generalization to unseen data.

One straightforward way to deal with this data scarcity problem is data augmentation \citep{xie2020unsupervised}, and incorporating generative models to perform data augmentation has been widely adopted recently \citep{carlini2021extracting, gangal2022nareor}. Despite its popularity, the generated text can easily deviate from the real data distribution without exploiting any of the signals passed back from the discrimination task. In previous studies, generative data augmentation and discrimination have been well studied as separate problems, but it is less clear how these two can be leveraged in one framework and how their performances can be improved simultaneously. \looseness=-1

Generative Adversarial Networks (GANs) \citep{https://doi.org/10.48550/arxiv.1406.2661,gulrajani2017improved} are good attempts to couple generative and discriminative models in an adversarial manner, where a two-player minimax game between learners is carefully crafted. GANs have achieved tremendous success in domains such as image generation \citep{denton2015deep}, and related studies have also shown their effectiveness in semi-supervised learning \citep{salimans2016improved,kumar2017semi}. However, GANs are notoriously difficult to train, most training objectives work well for only one model, either the discriminator or the generator, so rarely both learners can be optimal at the same time \citep{arjovsky2017towards,wiatrak2019stabilizing}.  This essentially arises from the adversarial nature of GANs, that during the process, optimizing one learner can easily destroy the learning ability of the other, making GANs fail to converge. 

% \begin{figure*}[ht] %H为当前位置，!htb为忽略美学标准，htbp为浮动图形
% \vskip 0.2in
% \begin{center}
% \includegraphics[width=0.8\textwidth]{sim-gen.png}
% \end{center}
% \caption{Overview of the flow chart for the SCL framework.} 
% \label{framework}
% \vskip -0.2in
% \end{figure*}

Another limitation of simultaneously optimizing the generator and the discriminator comes from the discrete nature of text in NLP, as no gradient propagation can be done from discriminators to generators. One theoretically sound attempt is to use reinforcement learning (RL), but the sparsity and the high variance of the rewards in NLP make the training particularly unstable \citep{caccia2019language}. 

To address these shortcomings, we novelly introduce a self-consistent learning framework based on one generator and one discriminator: the generator and the discriminator are alternately trained by way of cooperation instead of competition, and the samples are used as the medium to pass the feedback signal from the discriminator. Specifically, in each round of training, the samples generated by the generator are synthetically labeled by the discriminator, and then only part of them would be selected based on dynamic thresholds and used for the training of the discriminator and the generator in the next round. Several benefits can be discovered from this cooperative training process. First, a closed-loop form of cooperation can be established so that we can get the optimal generator and discriminator at the same time. Second, this framework helps improve the generation quality while ensuring the domain specificity of generator, which in turn contributes to training. Third, a steady stream of diverse synthetic samples can be added to the training in each round and lead to continuous improvement of the performance of all learners. Finally, we can start the training with only domain-related corpus and obtain strong results, while these data can be easily sampled with little cost or supervision. Also, the performance on labeled datasets can be further boosted based on the SOTA level. As an example to demonstrate the effectiveness of our framework, we examine it on the task of sentence semantic matching. The experiments show that our method significantly improves over standalone state-of-the-art discriminative models on zero-shot and full-data settings.

Our contributions are summarized as follows,

$\bullet$ We propose a self-consistent learning framework that incorporates the generator and the discriminator, in which both achieve remarkable performance gains simultaneously.

$\bullet$ We propose a dynamic selection mechanism such that cooperation between the generator and the discriminator drives the convergence to reach their scoring consensus.

$\bullet$ Experimental results show that our proposed framework significantly outperforms the state-of-the-art methods for the task of sentence semantic matching. 


\section{Related Works}
To alleviate the lack of annotated data in supervised learning in NLP,  semi-supervised learning (SSL) has been a popular line of research \citep{van2020survey}. The sources of the unlabeled data required by SSL are either collected from the domains or generated by generative language models. Then NLU models can learn from the unlabeled data by pseudo-labeling \citep{arazo2020pseudo,DBLP:conf/bmvc/Banitalebi-Dehkordi21} and consistent regularization \citep{jeong2019consistency,sohn2020fixmatch}. However,  collecting unlabeled data comes at a cost(though smaller than labeling data), and the total amount is limited. Even with generative models, there is no guarantee of the quality of the generated samples, because the model cannot tune the generating results based on the performance of the downstream tasks. In contrast, our method usually includes a continuously updated generative model, which dynamically adjusts its generation according to the performance of downstream tasks.

In GANs, the generator is adversarially trained with the discriminator. Unlike conventional GANs in continuous domains, language GANs usually employ Gumbel-Softmax differentiation \citep{jang2017categorical,yin2020meta}, Reinforcement Learning (RL) \citep{yu2017seqgan,wu2021textgail}, or modified training objectives \citep{montahaei2021dgsan} to update the generator, to use the non-differential signals from the discriminator. However, language GANs are often criticized for underperforming Maximum likelihood estimation (MLE) and are very difficult to train, even the single optimality of either the generator or the discriminator cannot be guaranteed \citep{alvarez2022gans}. In comparison,  our proposed framework allows us to cooperatively couple the generator and the discriminator, leading to continuous improvement for both learners. \looseness=-1

\section{Methodology}
\subsection{cooperative or adversarial}
Following the principle of self-consistency outlined in \cite{ma2022principles}, a closed-loop training needs to be built between the generator and the discriminator, either cooperatively or adversarially. GANs are typical examples of adversarial learning, but training GANs remains quite unstable. Let us consider an extreme case to show the possible instability: the discriminator can perfectly distinguish real data and fake data generated by the generator, and the generator can fully reproduce the real data distribution. Then the discriminator has only a 50\% probability of selecting all samples that are generated by the generator. Therefore, any further updates to the generator parameters based on the feedback from the discriminator deviate the generator from the optimum. Neither the generator nor the discriminator can likely be optimal \citep{arjovsky2017towards, lamprier2022generative}. In practice, a very delicate balance needs to be maintained between the discriminator and the generator to keep the training stable. In terms of cooperatively closed-loop learning, as discussed below, it does not suffer from instability: the generator and the discriminator usually enhance each other.  \looseness=-1

\begin{figure*}[ht] %H为当前位置，!htb为忽略美学标准，htbp为浮动图形
\vskip 0.2in
\begin{center}
\includegraphics[width=0.8\textwidth]{sim-gen.png}
\end{center}
\caption{Overview of the flow chart for the SCL framework.} 
\label{framework}
\vskip -0.2in
\end{figure*}

\subsection{Self-consistent Learning Framework}
In this section, we introduce our self-consistent learning (\textbf{SCL}) framework.

As shown in Figure~\ref{framework}, our framework, similar to the GANs, consists of a generator and a discriminator model. However, contrasting to the GANs, these two parts in our framework work cooperatively to enhance each other. Specifically, for any given class $k$, the generator $\mathcal{G}$ now become a conditional generator that takes in an input sentence $s^a_k$ and generate an output sentence $s^b_k$. The discriminator $\mathcal{D}$ is then responsible for discriminating the sentence using a dynamic threshold $\epsilon_\mathcal{D}$. The discriminated sentence is used as positive or negative data for that specific class to continue the training process. Once the new discriminator is trained, the sentence is discriminated again by the new discriminator with a different dynamic threshold $\epsilon_\mathcal{G}$. This time only the positive data is passed to the generator as the training data for the new round. In this way, a closed loop of cooperation is formed.

In the above closed-loop training, we propose a \textbf{selection mechanism} that uses dynamic thresholds to filter samples. This mechanism is empirically shown to play a critical role in closing the gap between the generator and the discriminator, and thus makes this cooperation loop a virtuous circle. Specifically, as shown in Equation~\ref{mlp}, the output probability $p_{\mathcal{D}}(y=k|s^b_k)$ that the sentence $\{s^b_k\}$ belongs to class $k$ is calculated from the embedding representation $\mathbf{H}$\footnote{{We follow \cite{reimers-gurevych-2019-sentence} and use the embedding representation of $CLS$-token as the sentence representation $\mathbf{H}$ .}} of $\{s^b_k\}$,

\begin{equation}
\label{mlp} 
    p_{\mathcal{D}}(y=k|s^b_k) = \text{softmax}( \text{MLP}(\mathbf{H})) 
\end{equation}

where $y$ represents the class label.  Then, through the filtering function ${\texttt{filter}^{(t)}_k(\cdot)}$ in round $t$ for the $k$-th class 
 in Equation~\ref{thre}, we keep samples whose output probability is not less than threshold $\epsilon_{t,k}$, while other generated samples whose confidence is lower than threshold $\epsilon_{t,k}$ are discarded.

\begin{equation}
\label{thre}
    \texttt{filter}^{(t)}_k(s^b_k) \triangleq {p_{\mathcal{D}}(k|s^b_k) \geq \epsilon_{t,k}} 
\end{equation}

where $\epsilon_{t,k}$ represents the dynamic threshold for accepting $\{s^b_k\}$ as negative or positive samples in the $t$-th round. The generalized threshold function for $\epsilon_{t,k}$ is defined as,

\begin{equation}
\label{dyn_thre}
    \epsilon_{t,k} = f(t, \mathcal{L}_{t-1,k}, \epsilon_{t-1,k}) 
\end{equation}

where $\mathcal{L}_{t-1,k}$ and $\epsilon_{t-1,k}$ represent the discriminator loss and threshold for round $t-1$, respectively. $\mathcal{L}_{0,k}$ is set as 0 and $\epsilon_{0,k} = \lambda$, where $\lambda$ represents a hyperparameter. \looseness=-1

\begin{theorem}
\label{theorem_kl}
At round $t$, given the previous round discriminator $\mathcal{D}^{t-1}_\phi$, the aim of optimizing the generator $\mathcal{G}^{t}_\theta$ boils down to:
\begin{equation}
\nonumber
    \min_{\theta}  \mathbb{D}_{KL}(p_{\mathcal{D}^{t-1}_\phi}^k(\cdot), p_{\mathcal{G}^{t}_\theta}^k(\cdot)) 
\end{equation}

where $\mathbb{D}_{KL}$ is the standard KL divergence, $p_{\mathcal{G}^{t}_\theta}^k(\cdot)$ refers to the degree of confidence that the sentences generated by the generator belongs to a given class $k$ (we can either train the generator to express its confidence in the generated sentences\citep{lin2022teaching} or use a fixed third-party model to score them\citep{gao2021simcse}), and $p_{\mathcal{D}^{t-1}_\phi}^k(\cdot)$ the probability of being classified into class $k$ given by the discriminator. \looseness=-1
\end{theorem}

Theorem \ref{theorem_kl} shows that the generator at round $t$ is encouraged to approximate the probability distribution given by the previous round discriminator. The proof is given in Appendix~\ref{proof}. In particular, on the basis of a well-pretrained discriminator, the generated distribution of the generator can be guaranteed to be faithful to the real data distribution. 

\textbf{Why Cooperative, Not Adversarial?} (1) the generator is no longer a challenger to the discriminator that only provides negative data points to fool it,  but now serves as a data augmenter to provide both positive and negative data points to enhance the discriminator; (2) the generator no longer updates its parameters through the policy gradients guided by the signals from the discriminator, but rather by utilizing the filtered data points to further improve its conditional generation quality. Note that by deliberately choosing the conditional generation paradigm along with the selection mechanism, we not only make the training more stable due to the different training goals, but also bypass the mode collapse problem of GANs (see Section~\ref{ablation exp} and Appendix \ref{adv_exp} for further discussions). Besides, by iterating through the loops, our framework achieves self-consistency by honing the domain specificity of the generator and increasing the domain data exposure of the discriminator.

\subsection{Sentence Semantic Matching}
We leverage the sentence semantic matching task ($i.e.$ $k=2$) as an example to demonstrate the effectiveness of our method. At this time, corresponding to Equation~\ref{thre}, $k=1/0$ represents the positive/negative class, and $\texttt{filter}^{(t)}_{1/0}$ represents the filter function in round $t$ for the positive/negative class respectively. First, let us introduce the formal definition of this task. Given two sentences ${s}^a = \{{w}_1^a, {w}_2^a, ..., {w}_{\ell_a}^a\}$ and ${s}^b = \{{w}_1^b, {w}_2^b, ..., {w}_{\ell_b}^b\}$, where ${w}_i^a$ and ${w}_j^b$ represent the $i$-th and $j$-th tokens in the sentences, and $\ell_a$ and $\ell_b$ indicate the  length of ${s}^a$ and ${s}^b$. The goal of this task is to learn a discriminator $\mathcal{D}$ to precisely predict the label $y=\mathcal{D}({s}^a, {s}^b)$, where $y \in \mathcal{Y}=\{0, 1\}$ indicates whether the two sentences are similar.

In our task, $\mathcal{G}$ is trained to generate a similar sentence ${s}^b$ from any given sentence ${s}^a$ and $\mathcal{D}$ is trained to predict label $y$ from any given sentence pair $\{{s}^a, {s}^b\}$. As demonstrated in Figure~\ref{framework}, there are mainly two training processes in the entire framework: fix $\mathcal{G}$ to train $\mathcal{D}$ and fix $\mathcal{D}$ to train $\mathcal{G}$. We introduce the two training procedures in detail with the $t$-th round training.

\textbf{Training $\mathcal{D}$}: We first randomly sample $s^a_t$ from domain-related corpus $C$, and then input $s^a_t$ to $\mathcal{G}^t$ to generate $s^b_t$. Next, we feed sentence pair $\{{s}^a_t, {s}^b_t\}$ into $\mathcal{D}^{t-1}$ to predict the label $y_{t-1}$, and filter $\{{s}^a_t, {s}^b_t, y_{t-1}\}$ using threshold $\epsilon_{\mathcal{D}}^{t-1}$. Finally, we train $\mathcal{D}^{t-1}$ on the selected data and pre-training data $P$ to get an improved discriminator $\mathcal{D}^{t}$. Note that the filtered data have both positive and negative samples. The update process of $\mathcal{D}$ seeks to minimize the cross-entropy loss over all instances:

\begin{equation}
\label{train_dis}
\begin{split}
	\mathcal{L}_\mathcal{D}(\boldsymbol{s}, \boldsymbol{y}) = \frac{1}{|\boldsymbol{s}|} \sum_{i=1}^{|\boldsymbol{s}|}-[y_i\cdot \log p_{\mathcal{D}}(y_i=1|s_i^a, s_i^b)\\+(1-y_i)\cdot \log (1-p_{\mathcal{D}}(y_i=1|s_i^a, s_i^b))] 
\end{split}
\end{equation}

\textbf{Training $\mathcal{G}$}: We feed the generated sentence pairs $\{{s}^a_t, {s}^b_t\}$ into $\mathcal{D}^{t}$ to predict new labels $y_{t}$, and then filter $\{{s}^a_t, {s}^b_t, y_{t}\}$ using threshold $\epsilon_\mathcal{G}^t$ and additional rules \footnote{The additional rules are used to exclude sentences which are too long, too short, or too similar according to the longest common substring algorithm.}. Note that the filtered data has only positive samples. For the filtered data, we supplement it with the pre-training data $P$ to update $\mathcal{G}^{t}$ to $\mathcal{G}^{t+1}$ \footnote{Note that the pre-training data $P$ is used to warm up $\mathcal{G}$ and $\mathcal{D}$. Although pre-training data is not mandatory in subsequent training, we empirically found that including it when training $\mathcal{G}$ can prevent language degeneration and improve downstream performances.} We also take out ${s}^b_t$ from the filtered data and add them to the domain-related corpus. The expanded domain corpus are used to sample conditional sentences in the next round of generation. The update procedure of $\mathcal{G}$ employs the negative log-likelihood function over all instances:

\begin{equation}
\nonumber
	\mathcal{L}_\mathcal{G}(\boldsymbol{s^a}, \boldsymbol{s^b}) = -\frac{1}{|\boldsymbol{s^b}|} \sum_{t=1}^{|\boldsymbol{s^b}|}\log p_{\mathcal{G}}(s^b_t|s^b_{<t}, \boldsymbol{s^a})
\end{equation}

For the selection mechanism, we adopt the form $\epsilon^t=m*t+\lambda$ after comparing the effects of different threshold functions through experiments according to Equation~\ref{dyn_thre} (See Appendix \ref{exp_filter} for details), where $m$ is the increment of the threshold for each round, $\lambda$ is the initial threshold, and $\epsilon_t$ is the threshold for rounds $t$.

In the process of training $\mathcal{G}$, since the sentences generated in each round are added to the domain-related corpus, the source of domain-specific data is thus monotonically expanding by iterating the self-consistent learning loop. The formalized process is shown in Algorithm~\ref{scl}.

\begin{algorithm}
\caption{Self-consistent Learning (\textbf{SCL})} 
\label{scl}
\begin{algorithmic}[1]
\REQUIRE Generator $\mathcal{G}$; Discriminator $\mathcal{D}$; Domain-Related Corpus $C$; Pre-training Data $P$.
\STATE Initialize $\mathcal{G}^0$ and $\mathcal{D}^0$ with pre-trained language models;
\STATE Warm-up $\mathcal{G}^0$ and $\mathcal{D}^0$ with pre-training data $P$ to get $\mathcal{G}^1$ and $\mathcal{D}^1$;
\FOR {each round $i \in [1, n]$}
\IF {Two consecutive rounds of discriminator still improve}
\STATE Generate similar sentences $s^b\sim p_{\mathcal{G}^i}(\cdot|s^a)$ from sampled sentences $s^a$ from $C$;
\STATE Predict pseudo-labels $y^i \sim p_{\mathcal{D}^i}(\cdot|s^a, s^b)$;
\STATE Use threshold $\epsilon_\mathcal{D}^i$ to select data on $\{s^a, s^b, y^i\}$ to train $\mathcal{D}^{i+1}$;
\STATE Predict pseudo-labels $y^{i+1} \sim p_{\mathcal{D}^{i+1}}(\cdot|s^a, s^b)$;
\STATE Use threshold $\epsilon_\mathcal{G}^i$ and additional rules to select data on $\{s^a, s^b, y^{i+1}\}$ to train $\mathcal{G}^{i+1}$;
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\section{Experiments}
\subsection{Tasks Design}
\label{task design}
In our experiments, the pre-training datasets are used to warm up the discriminator and generator, and the domain-related corpus is a set of independent sentences. To avoid label leakage, none of the training datasets participate in the pre-training of the generator and discriminator. In other words, the datasets in pre-training and self-consistent training are two non-overlapped datasets.

\textbf{Zero-Shot Baseline}: We utilize the best-performing Chinese model RoBERTa-wwm-ext-large \citep{cui-etal-2020-revisiting, 9599397}  and English model ALBERT-xxlarge-v2 \citep{Lan2020ALBERT:} as the base discriminators in our self-consistent learning framework. \looseness=-1

\textbf{Fine-Tune Baseline}: We compare our model with several state-of-the-art semantic matching models, including the following. Chinese models MacBERT \citep{cui-etal-2020-revisiting}, StructBERT \citep{Wang2020StructBERT:}, RoFormer \citep{2021arXiv210409864S,roformerv2}, XLNet \citep{xu-etal-2020-clue}, ELECTRA \citep{cui-etal-2020-revisiting}, ALBERT \citep{Lan2020ALBERT:}, RoBERTa \citep{cui-etal-2020-revisiting,9599397} and English models BERT \citep{kenton2019bert}, XLM-RoBERTa (XLM-R) \citep{conneau2020unsupervised}, XLNet \citep{yang2019xlnet}, ELECTRA \citep{DBLP:conf/iclr/ClarkLLM20}, ALBERT \citep{Lan2020ALBERT:}, RoBERTa \citep{liu2019roberta}. For a fair comparison, we use models that are as close in size as possible. \looseness=-1

\subsection{Experiments Setup}
\subsubsection{Datasets}
We conduct experiments on three Chinese semantic matching datasets AFQMC \citep{xu-etal-2020-clue}, CHIP-STS \citep{zhang-etal-2022-cblue}, Chinese-QQP \citep{wang-etal-2018-glue} and an English semantic matching dataset MRPC \citep{wang-etal-2018-glue}.  More details about the datasets are given in Appendix~\ref{dataset setting}.

\subsubsection{Model pre-training}
We adopt the well-established Transformer-XL \citep{dai2019transformer}/OPT \citep{zhang2022opt} architectures as the Chinese/English generator. To enable the generator to generate similar sentences with better linguistic quality, we pre-train a Transformer-XL model with 5.0 billion parameters and incrementally pre-train an OPT model with 2.7 billion parameters on the corpus consisting of plain texts and similar sentence pairs. Cleaned large-scale Chinese corpus WuDaoCorpora \citep{YUAN202165} and English corpus WikiText \citep{DBLP:conf/iclr/MerityX0S17} are used as plain texts. Similar sentence pairs that do not overlap with downstream datasets are used in the pre-training, and the designed prompts are employed to guide the generation of similar sentences. More details regarding model pre-training can be found in Appendix~\ref{model training}.

\subsection{Zero-Shot Results}
\label{zeroshot res}
Table~\ref{zero-shot} shows how the F1 score of the discriminator varies with the number of self-consistent learning rounds on different datasets in the zero-shot task. According to Algorithm~\ref{scl}, the training is stopped when the discriminator no longer improves for two consecutive rounds. In addition, these four datasets are collected from different domains to further reflect the generality of our method in different domains. Specific training settings are recorded in  Appendix~\ref{parameters}.

\begin{table}[t]
\caption{Zero-Shot Performance of the Discriminator. (F1 score (\%))}
\label{zero-shot}

\vskip 0.15in
\small
\centering
\begin{tabular}{ccccc}
\toprule
Round & \begin{tabular}[c]{@{}c@{}}AFQMC\\ (\textit{Financial})\end{tabular} &  \begin{tabular}[c]{@{}c@{}}CHIP-STS\\ (\textit{Medical})\end{tabular} &  \begin{tabular}[c]{@{}c@{}}QQP\\ (\textit{Common})\end{tabular} & \begin{tabular}[c]{@{}c@{}}MRPC\\ (\textit{News})\end{tabular} \\ \midrule
baseline & \uline{38.25} & \uline{58.82} & \uline{57.88} & \uline{68.54} \\ 
1 & 39.61 & 62.89 & 60.08 & 75.47 \\
2 & 44.98 & 67.24 & 58.57 & 76.63 \\
3 & 45.99 & 71.38 & 60.30 & 83.00 \\ 
4 & 45.71 & 71.45 & 61.31 & 83.90 \\ 
5 & 48.01 & 74.06 & 64.47 & 84.24 \\ 
6 & 50.41 & 74.08 & 66.44 & 84.50 \\ 
7 & 50.68 & 76.66 & 63.88 & 84.32 \\
8 & \textbf{51.36} & 76.30 & 65.46 & \textbf{84.61} \\ 
9 & - & 76.67 & 68.08 & - \\ 
10 & - & \textbf{77.42} & \textbf{70.51} & - \\ \midrule
 & \textbf{+13.11} & \textbf{+18.60} & \textbf{+12.63} & \textbf{+16.07}\\
\bottomrule
\end{tabular}
\end{table}

The scores in the last line of Table~\ref{zero-shot} give the improvement of our discriminator in the last round relative to the first round. We can see that the F1 score gradually increases after each training round, eventually reaching a 10+ absolute percentage (AP) improvement. We believe what drives the improvement of the discriminator is the self-consistency, which it acquires with the generator step by step during the loop.  \looseness=-1

To verify that the generator also improves after self-consistent training, we adopt Perplexity and Bertscore \citep{DBLP:conf/iclr/ZhangKWWA20} to measure the language fluency and the semantic similarity (i.e. domain specificity) respectively. For different generators in different rounds, we first select $s^a$ in similar sentence pairs from the same test set as the original sentences input, and generate similar sentences $s^b$ with greedy search. The reason for not using other sampling methods is to ensure reproducibility. Given the generated sentences, we introduce an additional GPT2 \footnote{Wenzhong-GPT2-110M \citep{wang2022fengshenbang} for Chinese data, and GPT2-base \citep{radford2019language} for English data.} model to calculate the perplexity of generated similar sentences, and use a third-party library \footnote{\href{https://pypi.org/project/bert-score/}{https://pypi.org/project/bert-score/}} to calculate the bertscore between the original and generated similar sentences. The results are shown in Table~\ref{gen_res}. 

\begin{table}[t]
\caption{Zero-Shot Performance of the Generator.}
\label{gen_res}

\vskip 0.15in
\small
\centering
\begin{tabular}{ccccc}
\toprule
 &  AFQMC & CHIP-STS & \begin{tabular}[c]{@{}c@{}}QQP \end{tabular} & MRPC \\ \midrule
\begin{tabular}[c]{@{}r@{}}Perplexity ↓\\\textit{-first round}\end{tabular} & 10.13 & 6.86 & 12.94 & 28.71 \\
\begin{tabular}[c]{@{}r@{}}Perplexity ↓\\\textit{-last round}\end{tabular} & 8.43 & 5.97 & 12.27 & 17.56 \\ \midrule
\begin{tabular}[c]{@{}r@{}}Bertscore ↑\\\textit{-first round}\end{tabular} & 0.79 & 0.84 & 0.87 & 0.94 \\ 
\begin{tabular}[c]{@{}r@{}}Bertscore ↑\\\textit{-last round}\end{tabular} & 0.80 & 0.85 & 0.89 & 0.97 \\ \bottomrule
\end{tabular}
\end{table}

\begin{table*}[t]
\caption{F1 Score(\%) of Different Discriminators on the Test Datasets.}
\label{compare}

\vskip 0.15in
\small
\centering
\sc
\begin{tabular}{ccccccc}
\toprule
 & \#Param(zh/en) & AFQMC & CHIP-STS & Chinese-QQP & MRPC & AVG \\ \midrule
BERT\tiny{\textit{large}} & -/335M & - & - & - & 82.51 & - \\ 
XLM-R\tiny{\textit{base}} & -/278M & - & - & - & 84.27 & - \\  \midrule
MacBERT\tiny{\textit{large}} & 326M/- & 61.11 & 85.94 & 72.94 & - & 73.33 \\
StructBERT\tiny{\textit{large}} & 326M/- & 60.56 & 85.17 & 76.33 & - & 74.02 \\
RoFormer\tiny{\textit{large}} & 316M/- & \cellcolor[HTML]{C0C0C0}{64.19} & 84.16 & \cellcolor[HTML]{C0C0C0}76.56 & - & 74.97 \\  \midrule
XLNet\tiny{\textit{large}} & 360M/360M & 50.31 & 82.97 & 64.96 & 79.51 & 69.44 \\
ELECTRA\tiny{\textit{large}} & 324M/334M & 54.59 & 84.97 & 71.81 & 89.64 & 75.25 \\ 
ALBERT\tiny{\textit{large}} & 221M/223M & 56.87 & 86.32 & 70.52 & \cellcolor[HTML]{C0C0C0}91.21 & 76.23 \\ 
RoBERTa\tiny{\textit{large}} & 326M/355M & 57.29 & \cellcolor[HTML]{C0C0C0}86.93 & 74.58 & 90.24 & \cellcolor[HTML]{C0C0C0}77.26 \\ 
\midrule
Our Method & - & \textbf{66.59} & \textbf{88.39} & \textbf{78.43} & \textbf{92.78} & \textbf{81.55} \\ \bottomrule
\end{tabular}
\end{table*}

We can see that the perplexity/bertscore of the last round in Table~\ref{gen_res} has decreased/improved compared to the first round. Note that a lower perplexity indicates a more fluent sentence, while a higher bertscore indicates a more similar sentence. It suggests that after self-consistent training,  the generator is gradually improved in language fluency and semantic similarity (i.e. domain specificity). The reason why the improvement of the generator is not as obvious as that of the discriminator is that the size of the generator is several times that of the discriminator, and the total number of training samples is limited. In Appendix~\ref{generate samples}, the generated samples of the generator in different rounds are given to show the changes in the generation.

\subsection{Fine-Tune Results}
Our method not only works well in the zero-shot case, but also achieves good results in the full-data case. For the sake of a fair comparison, we reproduce several state-of-the-art semantic matching models on the four training sets, and their performances on the test sets are shown in Table~\ref{compare}.

% \begin{table}[htbp]
% \small
% \centering
% \caption{F1 Score(\%) of Different Discriminators on the Test Datasets.}
% \label{compare}
% \begin{tabular}{ccccccc}
% \toprule
%  & AFQMC & CHIP-STS & Chinese-QQP & MRPC & AVG \\ \midrule
% BERT\tiny{\textit{large}} & - & - & - & 82.51 & - \\ 
% XLM-R\tiny{\textit{base}} & - & - & - & 84.27 & - \\  \midrule
% MacBERT\tiny{\textit{large}} & 61.11 & 85.94 & 72.94 & - & 73.33 \\
% StructBERT\tiny{\textit{large}} & 60.56 & 85.17 & 76.33 & - & 74.02 \\
% RoFormer\tiny{\textit{large}} & \cellcolor[HTML]{C0C0C0}{64.19} & 84.16 & \cellcolor[HTML]{C0C0C0}76.56 & - & 74.97 \\  \midrule
% XLNet\tiny{\textit{large}} & 50.31 & 82.97 & 64.96 & 79.51 & 69.44 \\
% ELECTRA\tiny{\textit{large}} & 54.59 & 84.97 & 71.81 & 89.64 & 75.25 \\ 
% ALBERT\tiny{\textit{large}} & 56.87 & 86.32 & 70.52 & \cellcolor[HTML]{C0C0C0}91.21 & 76.23 \\ 
% RoBERTa\tiny{\textit{large}} & 57.29 & \cellcolor[HTML]{C0C0C0}86.93 & 74.58 & 90.24 & \cellcolor[HTML]{C0C0C0}77.26 \\ 
% \midrule
% Our Method & - & \textbf{66.59} & \textbf{88.39} & \textbf{78.43} & \textbf{92.78} & \textbf{81.55} \\ \bottomrule
% \end{tabular}
% \end{table}

Our approach uses the best-performing model on a single test set as the base discriminator for self-consistent learning. The bold scores in the last line of Table~\ref{compare} show that our method outperforms the SOTA results (shaded in gray) by 1 to 2 AP on all four test datasets, indicating the potential of self-consistent learning to further improve the model performance and establish new SOTA. 

\subsection{Evaluating Self-consistency}
In this section, we evaluate the consistency between the generator and the discriminator as the learning loop unfolds. We follow the same method used in Section~\ref{zeroshot res} and use greedy search to generate similar sentences on the same test set. Then we take the confidence of the discriminator $R_\mathcal{D}$ as the score of the discriminator, which is calculated for the original sentences $s^a$ and the generated similar sentences $s^b$ according to Equation~\ref{r_d}. 

\begin{equation}
	R_\mathcal{D} = p_\mathcal{D}(y^+|s^a, s^b) \label{r_d} 
\end{equation}

where $y^+$ represents a positive label. 

\begin{figure*}[ht] %H为当前位置，!htb为忽略美学标准，htbp为浮动图形
\vskip 0.2in
\begin{center}
\includegraphics[width=1.0\textwidth]{all1.png}
\end{center}
\caption{Results of ablation experiments on pre-training data and selection mechanism. Results of the proposed method, results without pre-training data, and results without the selection mechanism are given in red, green, and blue, respectively.} 
\label{ablation}
\vskip -0.2in
\end{figure*}

However, for the generator, to the best of our knowledge, there is no reliable way to measure how similar $s^a$ and $s^b$ are by using the generator itself. Therefore, to quantify this similarity, we introduce a third-party static model SimCSE \footnote{We use SimCSE-BERT-base to calculate scores on Chinese datasets and sup-SimCSE-BERT-base-uncased on English datasets. \citep{gao2021simcse}} to get the embedding representation $\mathbf{A},\mathbf{B}$ of sentences $s^a,s^b$. The cosine similarity $R_\mathcal{G}$ between $\mathbf{A}$ and $\mathbf{B}$ is then calculated according to Equation~\ref{r_g} to approximate the score of the generator.

\begin{gather}
    \mathbf{A}, \mathbf{B} = \text{Encoder}(s^a), \text{Encoder}(s^b) \nonumber \\ 
	R_\mathcal{G} = \frac{\mathbf{A} \cdot \mathbf{B}}{\left\|\mathbf{A}\right\|_2*\left\|\mathbf{B}\right\|_2} 
\label{r_g}
\end{gather}

where $\mathbf{A}$ and $\mathbf{B}$ both represent the embedding representation at the $[CLS]$ position. Note that the original sentence $s^a$ remains unchanged in each round, while the generated sentence $s^b$ changes.

\begin{table}[t]
\captionsetup{width=0.7\textwidth}
\captionof{table}{The KL divergence in the first and last rounds.}
\label{consistent}

\vskip 0.15in
\small
\centering
\sc
\begin{tabular}{ccc}
\toprule
 & \begin{tabular}[l]{@{}r@{}}KL Divergence\\ \textit{-first round}\end{tabular} & \begin{tabular}[l]{@{}r@{}}KL Divergence\\ \textit{-last round}\end{tabular} \\ \midrule
AFQMC & 0.29 & 0.26 \\
CHIP-STS & 0.16 & 0.08 \\
Chinese-QQP & 0.18 & 0.06 \\
MRPC &0.22 & 0.02 \\ \bottomrule
\end{tabular}
\end{table}


\begin{figure}[ht] 
\vskip 0.2in
\begin{center}
\includegraphics[width=0.35\textwidth]{kl.png}
\captionsetup{width=0.5\textwidth}
\caption{The KL Divergence between the score distributions of the Discriminator and the Generator.} 
\label{distance}
\end{center}
\vskip -0.2in
\end{figure}

Finally, for the trained discriminator and generator in each round $t$, we can obtain two score distributions $\mathbf{R_\mathcal{D}^t}$ and $\mathbf{R_\mathcal{G}^t}$ correspondingly. According to Theorem~\ref{theorem_kl}, we draw the curves of KL divergence between $\mathbf{R_\mathcal{D}^t}$ and $\mathbf{R_\mathcal{G}^t}$ in each round for the four datasets: AFQMC, CHIP-STS, Chinese-QQP, and MRPC. As illustrated in Figure~\ref{distance}, all the curves show a clear downward trend, indicating that the distance between the two score distributions decreases with the increase in the number of training rounds until a score consensus is reached. Table~\ref{consistent} shows the values of KL divergence in the first and last rounds. Numerically, it is more evident that the distances are significantly reduced on the four datasets. \looseness=-1

% \begin{figure*}[htbp] 
% \begin{minipage}[b]{0.5\textwidth} 
% \centering 
% \includegraphics[width=0.75\textwidth]{kl.png}
% \captionsetup{width=0.75\textwidth}
% \caption{The KL Divergence between the score distributions of the Discriminator and the Generator.} 
% \label{distance}
% \end{minipage}
% \begin{minipage}[b]{0.5\textwidth} 
% \small
% \centering
% \begin{tabular}{ccc}
% \toprule
%  & \begin{tabular}[l]{@{}r@{}}KL Divergence\\ \textit{-first round}\end{tabular} & \begin{tabular}[l]{@{}r@{}}KL Divergence\\ \textit{-last round}\end{tabular} \\ \midrule
% AFQMC & 0.29 & 0.26 \\
% CHIP-STS & 0.16 & 0.08 \\
% Chinese-QQP & 0.18 & 0.06 \\
% MRPC &0.22 & 0.02 \\ \bottomrule
% \end{tabular}
% \captionsetup{width=0.7\textwidth}
% \captionof{table}{The KL divergence in the first and last rounds.}
% \label{consistent}
% \end{minipage} 
% \end{figure*}

\subsection{Effect of Pre-training Data and Selection Mechanism}
\label{ablation exp}
We perform ablation experiments on the pre-training data and the selection mechanism in the zero-shot case. As described in Section~\ref{task design}, the pre-training data is used to pre-train the generator and discriminator, completely independent of the experimental datasets in self-consistent training.

To explore the influence of pre-training data on self-consistent training, we no longer add it in each round when training the discriminator, and only the generated data is used. But when the generator is trained, pre-training data is still retained to prevent language degeneration and lack of expressive diversity of the generation. The result of removing pre-training data is shown as the green curves in Figure~\ref{ablation}. With all other training parameters being the same, after the same number of training rounds, the discriminator is slightly worse compared to the original method (red curves in Figure~\ref{ablation}). However, the green curves maintain an upward trend and are very close to the red curves in all datasets except CHIP-STS. This shows that the generated data plays a key role in continuously improving the discriminator, while the pre-training data has a limited role. 

In order to explore the effect of the selection mechanism on training the discriminator, we remove the selection mechanism when training the discriminator, while the training of the generator remains unchanged. The blue curves in Figure~\ref{ablation} depict the performance of the discriminator in each round after removing the selection mechanism. Compared to the original method (red curves), the discriminator only improves in the first round after removing the selection mechanism, which demonstrates the importance of the selection mechanism on the discriminator for the convergence of the self-consistent learning framework.

\section{Conclusion}
In this paper, we propose a self-consistent learning framework to enable cooperative training of the generator and the discriminator.  During the training process, the generator and the discriminator continuously enhance each other until reaching a score consensus. This framework can utilize both limited labeled data and large-scale unlabeled domain-related corpus. Experimental results on four Chinese/English sentence semantic matching datasets demonstrate that as a form of closed-loop training, our proposed framework can achieve new state-of-the-art results with continuously improved generators and discriminators. 

For future work, we will explore the effectiveness of our self-consistent learning framework on more NLP tasks, since the framework is straightforward and has no additional requirements on generators and discriminators.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2023}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn


\section{Proofs}
\label{proof}
\subsection{Proof of Theorem 1}
 \textbf{Theorem 1} \textit{At round $t$,given the previous round discriminator $\mathcal{D}^{t-1}_\phi$, the aim of the optimization of the generator $\mathcal{G}^{t}_\theta$,  boils down to,}
\begin{equation}
\nonumber
    \min_{\theta}  \mathbb{D}_{KL}(p_{\mathcal{D}^{t-1}_\phi}^k(\cdot), p_{\mathcal{G}^{t}_\theta}^k(\cdot))
\end{equation}

\textit{where $\mathbb{D}_{KL}$ is the standard KL divergence, $p_{\mathcal{G}^{t}_\theta}^k(\cdot)$ refers to the degree of confidence that the sentences generated by the generator belong to a given class $k$ (we can either train the generator to express its confidence in the generated sentences\citep{lin2022teaching} or use a fixed third-party model to score them\citep{gao2021simcse}), and $p_{\mathcal{D}^{t-1}_\phi}^k(\cdot)$ the probability of being classified into class $k$ given by the discriminator. \looseness=-1}

\textbf{Proof.} We use the previous round  generator $\mathcal{G}^{t-1}_\theta$ to generate samples, and filter them using the previous round discriminator $\mathcal{D}^{t-1}_\phi$ with a threshold $\epsilon_{t-1}$, then these samples are used for training of the current round generator $\mathcal{G}^{t}_\theta$. Therefore, the optimization of $\mathcal{G}^{t}_\theta$ will tend to  maximize the probability that the generated samples pass the discrimination for the fixed $\mathcal{D}^{t-1}_\phi$. For a given class $k$, we have \looseness=-1
\begin{equation}
\nonumber
\small
 \max_{\theta} \mathbb{E}_{x\sim  p_{\mathcal{G}^{t-1}_\theta}^k} p_{\mathcal{G}^{t}_\theta}^k(x) \quad \texttt{s.t.} \quad \texttt{filter}^{(t-1)}_k(x) = 1
\end{equation}
where the definition of function $\texttt{filter}^{(t-1)}_k(\cdot)$ has been given in Equation~\ref{thre}.


The above objective is equivalent to sampling from the generator being optimized in round $t$ and making these samples pass the discrimination in round $t-1$ as much as possible, which gives
\begin{equation}
\nonumber
 \max_{\theta} \mathbb{E}_{x\sim  p_{\mathcal{G}^{t}_\theta}^k}p_{\mathcal{D}^{t-1}_{\phi}}^k(x)
\end{equation}
where $p_{\mathcal{D}^{t-1}_{\phi}}^k(x)$ is fixed.

A further transformation of the formula shows that
\begin{align*}
& \max_{\theta} \mathbb{E}_{x\sim  p_{\mathcal{G}^{t}_\theta}^k}p_{\mathcal{D}^{t-1}_{\phi}}^k(x) \\
\stackrel{(\romannumeral 1)} \Rightarrow & \max_\theta \int \mathrm{d}\pmb{\theta} \nabla_{\pmb{\theta}} \mathbb{E}_{x\sim  p_{\mathcal{G}^{t}_\theta}^k}p_{\mathcal{D}^{t-1}_\phi}^k(x) \\
\stackrel{(\romannumeral 2)} \Rightarrow
& \max_{\theta} \int \mathrm{d}\pmb{\theta} \mathbb{E}_{x\sim  p_{\mathcal{G}^{t}_\theta}^k} \nabla_{\pmb{\theta}} \log p_{\mathcal{G}^{t}_\theta}^k(x)p_{\mathcal{D}^{t-1}_\phi}^k(x) \\
\stackrel{(\romannumeral 3)} \Rightarrow
& \max_{\theta} \int \mathrm{d}\pmb{\theta}  \nabla_{\pmb{\theta}} \frac{1}{N}\sum_{i=1}^{N} \{\log p_{\mathcal{G}^{t}_\theta}^k(x_i)p_{\mathcal{D}^{t-1}_\phi}^k(x_i) - \log p_{\mathcal{D}^{t-1}_\phi}^k(x_i)p_{\mathcal{D}^{t-1}_\phi}^k(x_i)\} \\
\stackrel{(\romannumeral 4)} \Rightarrow
& \min_{\theta} \mathbb{D}_{KL}(p_{\mathcal{D}^{t-1}_\phi}^k(\cdot), p_{\mathcal{G}^{t}_\theta}^k(\cdot))
\end{align*}

where $(\romannumeral 1)$ uses the integral property that integrating the derivative of a function gives the original function along with a constant, $(\romannumeral 2)$ takes advantage of the derivative property of the logarithmic function, $(\romannumeral 3)$ approximates the expectation of the probability distribution $p_{\mathcal{G}^{t}_\theta}^k(\cdot)$  by using averaging on $N$ samples sampling from $p_{\mathcal{G}^{t}_\theta}^k(\cdot)$, and adding a constant term with respect to $\theta$ under the summation would not change its derivative, and $(\romannumeral 4)$ cancels out the integral and the derivative and uses the definition of KL divergence. 
The above concludes our proof.

\section{Experiments on Different Threshold Functions}
\label{exp_filter}
To compare the effect of different threshold functions on the final result, we use four type of functions, including oscillatory function (cosine), constant function and monotonically increasing functions (quadratic and linear). For the fairness of comparison, we keep the maxima and minima the same for all functions(except for the constant threshold), and the values are given in Appendix~\ref{parameters}. In addition, the number of training rounds for different functions on the same dataset remains the same. 

In the results below, the best results and the second-best results are bold and underlined, respectively.

\begin{table}[htbp]
\caption{Performance of Different Threshold Functions in Zero-Shot Setting. (F1 Score(\%))}
\label{zero_shot_func}

\vskip 0.15in
\small
\centering
\begin{tabular}{ccccccc}
\toprule
            & \scriptsize AFQMC          & \scriptsize CHIP-STS       & \scriptsize QQP    & \scriptsize MRPC           & \scriptsize AVG            \\ \midrule
Baseline    & 38.25          & 58.82          & 57.88          & 68.54          & 55.87          \\
Cosine & 47.38          & \uline{74.26}    & 64.39          & 83.48    & 67.38          \\
Constant       & 47.06 & 74.15          & 68.67 & \uline{84.11}    & 68.50          \\
Quadratic      & \textbf{51.75} & 73.09          & \textbf{70.85} & 83.48          & 69.79          \\ \midrule
Linear  & \uline{51.36}    & \textbf{77.42} & \uline{70.51}    & \textbf{84.61} & \textbf{70.98} \\ \bottomrule
\end{tabular}
\end{table}

As can be seen from the Table~\ref{zero_shot_func}, in the zero-shot setting, the chosen linear function outperforms the other functions, and all the threshold functions show an averaging 10+ AP improvement relative to the baseline. Therefore, the self-consistent learning framework makes it easy to choose a certain threshold function and perform well, and the results are not so sensitive to the choice of the functions.

\begin{figure*}[ht] %htbp
\vskip 0.2in
\begin{center}
\includegraphics[width=1.0\textwidth]{zero_shot.png}
\end{center}
\caption{Results of contrast experiments on Cosine(green), Constant(orange), Quadratic(blue) and Linear(red) function.} 
\label{filter_fig}
\vskip -0.2in
\end{figure*}

Figure~\ref{filter_fig} dipicts the comparison results in each round. The linear function (red line) is significantly better than the other functions on both CHIP-STS and MRPC datasets. In the AFQMC and Chinese-QQP datasets, the quadratic function (blue line) is slightly more effective than the linear function. In general, we can intuitively see that all functions show a significant increase relative to the starting point.

\begin{table}[htbp]
\caption{Performance of Different Threshold Functions in Fine-Tune Setting. (F1 Score(\%))}
\label{fine_tune_func}

\vskip 0.15in
\small
\centering
\begin{tabular}{ccccccc}
\toprule
            & \scriptsize AFQMC          & \scriptsize CHIP-STS       & \scriptsize QQP    & \scriptsize MRPC           & \scriptsize AVG            \\ \midrule
Baseline    & 64.19          & 86.93          & 76.56          & 91.21          & 79.72          \\
Cosine & 66.43          & 88.01          & 77.33          & 92.63          & 81.10          \\
Constant  & \uline{66.57}    & \uline{88.15}    & 78.45          & 92.51        & 81.42          \\
Quadratic      & 66.37          & 87.76          & \textbf{79.26} & \uline{92.75}    & 81.54          \\ \midrule
Linear  & \textbf{66.59} & \textbf{88.39} & \uline{78.43}    & \textbf{92.78} & \textbf{81.55} \\ \bottomrule
\end{tabular}
\end{table}

Table~\ref{fine_tune_func} shows the effects of different threshold functions in the fine-tune experiment. It can be seen that all functions have a $1\sim 2$ AP increase relative to the baseline, and the chosen linear function achieves the best performance on all datasets except Chinese-QQP.

\section{Contrastive Experiments with Adversarial Training}
\label{adv_exp}
In this section, We further demonstrate the superiority of the cooperative approach by comparing the results with adversarial experiments. All experimental settings independent of the training method remain the same in the adversarial training. 

During the experiments, the generator is no longer trained using the samples filtered by the discriminator, but the rewards passed by the discriminator assist the training. All generated samples are treated as negative samples when training the discriminator.

Specifically, $\mathcal{G}$ takes the prompt ' "$s^a$" is similar to " ' and the first $M$ tokens of $s^b$ as input to get $M$ sentence pairs $<s^a, s^b_m>$, where $m$ is from 1 to $M$. Note that we repeat the process of generating sentences $N$ times to reduce the negative impact caused by the large variance of the rewards.\footnote{In practice, we take $M=5,N=5$ for ease of calculation.} The sentence pair is formalized as

\begin{equation}
\nonumber
<s^a, s^b_m>=\mathcal{G}_\theta(s^b_m|s^b_{<m}, \boldsymbol{s^a} ; N)
\end{equation}

Once the $M*N$ sentence pairs are generated, they are passed as input to the $\mathcal{D}$ to obtain the probability score $Q_m^n$ for each of them. We take the average of $Q_m^n$ over $N$ as the reward $\bar Q_m$ corresponding to the $m$-th token. If the sentence length of $s^b$ is greater than $M$, the rewards of the remaining tokens are all the same as those of the $M$-th token. Taking the $m$-th token as an example, the rewards $\bar Q_m$ can be formalized as

\begin{equation}
\nonumber
\begin{array}{l}
\bar Q_{\mathcal{D}_\phi}^{\mathcal{G}_\theta}(m) =\left\{\begin{array}{cc}
\frac{1}{N} \sum_{n=1}^{N} \mathcal{D}_\phi(g_m^n) & m \leq M \\
\bar Q(M) & m>M
\end{array}\right.
\end{array}
\end{equation}

where $g_m^n$ is the $n$-th sentence pair with length $m$.\looseness=-1

Therefore, the objective function for training the generator $\mathcal{G}$ is,

\begin{equation}
\nonumber
	\mathcal{L}_\mathcal{G}(\boldsymbol{s^a}, \boldsymbol{s^b}) = -\frac{1}{|\boldsymbol{s^b}|} \sum_{t=1}^{|\boldsymbol{s^b}|}\log(p_{\mathcal{G}}(s^b_t|s^b_{<t}, \boldsymbol{s^a}) * \bar Q_t)
\end{equation}

The loss function of training the discriminator remains the same as Equation \ref{train_dis}, but differing from cooperative training, the generated samples are regarded as negative samples to the discriminator, and the training target for the discriminator can be given by

\begin{equation}
\nonumber
	\min_{\phi} -\mathbb{E}_{X \sim p_{\text {data}}}\left[\log \mathcal{D}_\phi(X)\right] -\mathbb{E}_{X \sim p_{\mathcal{G}_\theta}}\left[\log \left(1-\mathcal{D}_\phi(X)\right)\right]
\end{equation}

The results of zero-shot and fine-tune on the four datasets are shown in Tables~\ref{zero_shot_adv} and \ref{fine_tune_adv}.

\begin{table}[htbp]
\caption{Zero-Shot Performance of Adversarially Trained Discriminator. (F1 score (\%))}
\label{zero_shot_adv}

\vskip 0.15in
\small
\centering
\begin{tabular}{ccccc}
\toprule
Round &  \begin{tabular}[c]{@{}c@{}}AFQMC\\ (\textit{Financial})\end{tabular} &  \begin{tabular}[c]{@{}c@{}}CHIP-STS\\ (\textit{Medical})\end{tabular} &  \begin{tabular}[c]{@{}c@{}}QQP\\ (\textit{Common})\end{tabular} &  \begin{tabular}[c]{@{}c@{}}MRPC\\ (\textit{News})\end{tabular} \\ \midrule
baseline & \uline{38.25} & \uline{58.82} & \uline{57.88} & \uline{68.54} \\ 
1 & 0.0 & 8.73 & 21.71 & 4.19 \\
2 & 0.02 & 7.13 & 49.30 & 7.06 \\
3 & 0.0 & 0.29 & 42.94 & 5.32 \\ 
4 & 0.0 & 1.09 & 41.13 & 0.0 \\ 
5 & 0.0 & 0.10 & 43.10 & 1.72 \\ 
6 & 0.0 & 0.39 & 34.30 & 67.38 \\ 
7 & 0.0 & 0.20 & 42.62 & 48.31 \\
8 & 0.0 & 0.20 & 34.95 & 37.97 \\ 
9 & - & 0.20 & 41.81 & - \\ 
10 & - & 0.20 & 40.00 & - \\ \bottomrule
\end{tabular}
\end{table}

As can be seen from Table~\ref{zero_shot_adv}, in the zero-shot setting, training in an adversarial manner does not give any improvement over the baseline. Because the initial discriminator in the zero-shot setting is very weak in distinguishing positive and negative samples, it is reasonable to believe that if all generated samples are considered negative samples from the very beginning, it is difficult for the discriminator to know how to distinguish positive samples. As a result, the F1 scores on both AFQMC and CHIP-STS datasets end up being 0,  while the scores on the Chinese-QQP and MRPC datasets fluctuate intensively with the number of rounds,  which further validates the instability of the adversarial training in the zero-shot setting.

\begin{table}[htbp]
\caption{Fine-Tune Performance of the Discriminator. (F1 score (\%))}
\label{fine_tune_adv}

\vskip 0.15in
\small
\centering
\begin{tabular}{ccccccc}
\toprule
            & AFQMC          & CHIP-STS       & QQP    & MRPC           & AVG            \\ \midrule
 Baseline    & 64.19          & 86.93          & 76.56          & 91.21          & 79.72          \\
 Adversarial & 58.37          & 80.46          & 77.93          & 92.18          & 77.24          \\
 \begin{tabular}[c]{@{}c@{}}Cooperative\\ \tiny{(Our Method)}\end{tabular}  & \textbf{66.59} & \textbf{88.39} & \textbf{78.43}    & \textbf{92.78} & \textbf{81.55} \\ \bottomrule
\end{tabular}
\end{table}

For the fine-tune experiments, Table~\ref{fine_tune_adv} shows that training in an adversarial manner can slightly improve the performance on the Chinese-QQP and MRPC datasets, but is still worse than the cooperative training. On the AFQMC and CHIP-STS dataset, adversarial training makes it even worse relative to the baseline. It is worth noting that the whole process of adversarial training is so unstable and it is easy to collapse after a few training rounds.

\color{black}
\section{Model Details}
\label{model training}
The 5.0B Transformer-XL is pre-trained on 32 A100s with 40G memory for 45 days, the batch size is set to 32*8=256. After running 445k steps, the final validation loss reduces to about 2.4. The 2.7B OPT is incrementally trained on the basis of the open-source model.

\begin{table*}[htbp]
\caption{Statistics of Experimental Datasets}
\label{dataset}

\vskip 0.15in
\small
\centering
\begin{tabular}{cccccc}
\toprule
 &  & AFQMC & CHIP-STS & Chinese-QQP & MRPC(en) \\ \midrule
\multirow{2}{*}{Zero-Shot} & \begin{tabular}[c]{@{}c@{}}domain-related\\ corpus\end{tabular} & 68668 & 32000 & 10800 & 8,152 \\
 & test dataset & 4316 & 4000 & 3600 & 1725 \\ \midrule
\multirow{2}{*}{Fine-Tune} & training dataset & 34334 & 16000 & 5400 & 4076 \\ 
 & test dataset & 4316 & 4000 & 3600 & 1725 \\ \bottomrule
\end{tabular}
\end{table*}
 
During the pre-training of the generator model, we utilize the memory-cache mechanism of Transformer-XL and design a special attention mask to concatenate the multiple input sentences into one sample, to reduce the number of the padding token in a batch and therefore increase the number of effective tokens. To make the generation more robust, we add noise to the original sentences by randomly replacing or discarding tokens with a 5\% probability. In addition, the prompts that we use for Chinese generation and English generation are as follows,

\begin{itemize}
\item Chinese prompt: \begin{CJK}{UTF8}{gbsn} “$s^a$”的相似句是“$s^b$” \end{CJK} (en: A similar sentence to ``$s^a$" is ``$s^b$".)
\item English prompt: ``$s^a$" is similar to ``$s^b$"
\end{itemize}

When training the discriminator, following the usage of special tokens in BERT \citep{DBLP:conf/naacl/DevlinCLT19}, we use $[SEP]$ to concatenate two sentences and take the embedding at the $[CLS]$ position to represent the whole sentence to predict the label. Moreover, we utilize the mask method in BERT to randomly mask 15\% of the input tokens.

\section{Dataset Details}
\label{dataset setting}
The statistics of the experimental datasets are reported in Table~\ref{dataset}.

Other Chinese datasets (LCQMC \citep{liu-etal-2018-lcqmc}, OPPO, PAWS-X-zh \citep{yang-etal-2019-paws}, BQ \citep{chen-etal-2018-bq}, CCKS, Chinese-STS-B \citep{wang-etal-2018-glue}) and English datasets (QQP \citep{wang-etal-2018-glue}, STS-B \citep{wang-etal-2018-glue}, PAWS-X-en \citep{yang-etal-2019-paws}) are collected and used as the corpus of similar sentence pairs for pre-training the generator. 

The Chinese-QQP dataset contains 9000 pieces of data randomly selected and translated from the English QQP dataset, which is then divided into training set and test set in a ratio of 3:2.

\section{Parameter Settings}
\label{parameters}
The training parameters of zero-shot are shown in Table~\ref{zero-shot-parm}. The three thresholds are used to select positive and negative examples for training the discriminator and positive examples for training the generator, respectively. We adopt cosine annealing learning rate decay strategy during training.

\begin{table*}[htbp]
\caption{Parameter Settings of Zero-Shot.}
\label{zero-shot-parm}

\vskip 0.15in
\small
\centering
\begin{tabular}{ccccc}
\toprule
 & AFQMC & CHIP-STS & Chinese-QQP & MRPC \\ \midrule
\begin{tabular}[l]{@{}r@{}}Maximum Threshold\\ \textit{-negative}\end{tabular} & 0.8 & 0.9 & 0.95 & 0.95 \\ 
\begin{tabular}[l]{@{}r@{}}Minimum Threshold\\ \textit{-negative}\end{tabular} & 0.6 & 0.7 & 0.8 & 0.8 \\ 
\begin{tabular}[l]{@{}r@{}}Maximum Threshold\\ \textit{-positive}\end{tabular} & 0.8 & 0.9 & 0.95 & 0.95 \\ 
\begin{tabular}[l]{@{}r@{}}Minimum Threshold\\ \textit{-positive}\end{tabular} & 0.6 & 0.7 & 0.8 & 0.8 \\ 
\begin{tabular}[l]{@{}r@{}}Maximum Threshold\\ \textit{-generator}\end{tabular} & 0.6 & 0.9 & 0.95 & 0.95 \\ 
\begin{tabular}[l]{@{}r@{}}Minimum Threshold\\ \textit{-generator}\end{tabular} & 0.6 & 0.7 & 0.8 & 0.8 \\ 
Threshold Increase & 0.07 & 0.1 & 0.05 & 0.05 \\
Sentence Num & 6000 & 6000 & 3000 & 4000 \\ 
Learning Rate & \multicolumn{4}{c}{2e-5} \\ 
Warm Up Steps & \multicolumn{4}{c}{40} \\ 
Early Stopping Patience & \multicolumn{4}{c}{1} \\ 
\begin{tabular}[l]{@{}r@{}}Generator Batch Size\\ \textit{-training}\end{tabular} & \multicolumn{3}{c}{2(concatenate 30 samples)} & 24 \\ 
\begin{tabular}[l]{@{}r@{}}Generator Batch Size\\ \textit{-predicting}\end{tabular} & \multicolumn{3}{c}{512} & 100 \\ 
\begin{tabular}[l]{@{}r@{}}Discriminator Batch Size\\ \textit{-training}\end{tabular} & \multicolumn{3}{c}{64} & 32 \\ 
\begin{tabular}[l]{@{}r@{}}Discriminator Batch Size\\ \textit{-predicting}\end{tabular} & \multicolumn{3}{c}{384} & 96 \\ \bottomrule
\end{tabular}
\end{table*}

The training parameters of fine-tuning are shown in Table~\ref{fine-tune-parm}.

\begin{table*}[htbp]
\caption{Parameter Settings of Fine-Tune.}
\label{fine-tune-parm}

\vskip 0.15in
\small
\centering
\begin{tabular}{ccccc}
\toprule
 & AFQMC & CHIP-STS & Chinese-QQP & MRPC \\\midrule
\begin{tabular}[l]{@{}r@{}}Maximum Threshold\\ \textit{-negative}\end{tabular} & 0.98 & 0.98 & 0.84 & 0.8 \\ 
\begin{tabular}[l]{@{}r@{}}Minimum Threshold\\ \textit{-negative}\end{tabular} & 0.9 & 0.7 & 0.6 & 0.6 \\ 
\begin{tabular}[l]{@{}r@{}}Maximum Threshold\\ \textit{-positive}\end{tabular} & 0.98 & 0.98 & 0.98 & 0.8 \\
\begin{tabular}[l]{@{}r@{}}Minimum Threshold\\ \textit{-positive}\end{tabular} & 0.9 & 0.7 & 0.9 & 0.6 \\ 
\begin{tabular}[l]{@{}r@{}}Maximum Threshold\\ \textit{-generator}\end{tabular} & 0.98 & 0.98 & 0.98 & 0.8 \\ 
\begin{tabular}[l]{@{}r@{}}Minimum Threshold\\ \textit{-generator}\end{tabular} & 0.9 & 0.7 & 0.9 & 0.6 \\ 
Threshold Increase & 0.07 & 0.07 & 0.07 & 0.2 \\
Sentence Num & 6000 & 6000 & 3000 & 3000 \\
Learning Rate & \multicolumn{4}{c}{5e-6} \\
Warm Up Steps & \multicolumn{4}{c}{40} \\
Early Stopping Patience & \multicolumn{4}{c}{1} \\
\begin{tabular}[c]{@{}r@{}}Generator Batch Size\\\textit{ -training}\end{tabular} & \multicolumn{3}{c}{2(concatenate 30 samples)} & 24 \\
\begin{tabular}[l]{@{}r@{}}Generator Batch Size\\\textit{ -predicting}\end{tabular} & \multicolumn{3}{c}{512} & 100 \\
\begin{tabular}[l]{@{}r@{}}Discriminator Batch Size\\ \textit{-training}\end{tabular} & 32 & 64 & 32 & 32 \\ 
\begin{tabular}[l]{@{}r@{}}Discriminator Batch Size\\ \textit{-predicting}\end{tabular} & 256 & 384 & 256 & 96 \\ \bottomrule
\end{tabular}
\end{table*}

\section{Generate Samples}
\label{generate samples}
The generators use nucleus sampling \citep{Holtzman2020The} to generate similar sentences.
Generated examples in English are shown in Table~\ref{samples_en} and in Chinese in Table~\ref{samples_zh}.

\begin{table*}[htbp]
\caption{Generated Samples in English}
\label{samples_en}

\vskip 0.15in
\centering
\begin{tabular}{c}
\hline
MRPC(News) \\ \hline
\begin{tabular}[c]{@{}c@{}}\textit{origin:} Hong Kong was flat, Australia, Singapore and South Korea lost 0.2-0.4 percent.\\ \textit{first round:} the market is flat, the US is flat, Europe is flat, Japan is flat, \\ ... Malaysia is flat, Philippines is flat, Vietnam is flat, Singapore is\\ \textit{last round:} Hong Kong was unchanged, Australia, Singapore and South Korea \\ were down 0.2-0.4 percent.\end{tabular} \\ \hline
\begin{tabular}[c]{@{}c@{}}\textit{origin:} Ballmer has been vocal in the past warning that Linux is a threat to Microsoft.\\ \textit{first round:} Microsoft is a threat to Linux.\\ \textit{last round:} Ballmer has been vocal in the past about the threat Linux poses to Microsoft.\end{tabular} \\ \hline
\begin{tabular}[c]{@{}c@{}}\textit{origin:} Last year, Congress passed similar, though less expensive, buyout\\ legislation for peanut farmers, ending that Depression-era program.\\ \textit{first round:} The government is going to buy out your farm and give you a check\\ for \$1,000,000.00. You can't refuse.\\ \textit{last round:} Congress passed a similar bill last year to end a Depression-era\\ program for peanut farmers, ending that program.\end{tabular} \\ \hline
\begin{tabular}[c]{@{}c@{}}\textit{origin:} Licensing revenue slid 21 percent, however, to \$107.6 million.\\ \textit{first round:} The company's net income fell to \$1.1 million, or 2 cents per share,\\ in the quarter ended Sept. 30, from \$1.3 million, or 3 cents per share, a year earlier.\\ \textit{last round:} Licensing revenue fell 21 percent to \$107.6 million \\ in the third quarter.\end{tabular} \\ \hline
\begin{tabular}[c]{@{}c@{}}\textit{origin:} SCO says the pricing terms for a license will not \\ be announced for weeks.\\ \textit{first round:} we're not going to tell you how much it costs to buy a car, but\\ we're going to tell you how much it costs to rent one.\\ \textit{last round:} The pricing terms for a license for the SCO software will not \\ be announced for several weeks.\end{tabular} \\ \hline
\end{tabular}
\end{table*}



\begin{table*}[htbp]
\caption{Generated Samples in Chinese.}
\label{samples_zh}

\vskip 0.15in
\small
\centering
\begin{CJK}{UTF8}{gbsn}
\begin{tabular}{cc}
\toprule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}AFQMC\\ (Financial)\end{tabular}} & \begin{tabular}[c]{@{}c@{}}\textit{origin：}可以用自己的支付宝为别人的花呗还款吗\\(Can I use my Alipay to repay someone else's Huabei)\\ \textit{first round：}花呗的钱可以让其他人代还吗\\(Can someone else pay for the money from Huabei)\\ \textit{last round：}花呗能不能帮别人还\\(Can my Huabei help others repay?)\end{tabular} \\ \cmidrule{2-2} 
 & \begin{tabular}[c]{@{}c@{}}\textit{origin：}花呗怎么还欠费\\(How does Huabei pay off the arrears)\\ \textit{first round：}花呗怎么还有电费\\(Why does Huabei still have electricity bills)\\ \textit{last round：}花呗还款怎么还\\(How to pay off Huabei)\end{tabular} \\ \cmidrule{2-2} 
 & \begin{tabular}[c]{@{}c@{}}\textit{origin：}我的花呗收款二维码\\(QR code for my Huabei payment)\\ \textit{first round：}商家花呗的二维码\\(Merchant Huabei's QR code)\\ \textit{last round：}花呗收款二维码在哪里\\(Where is the QR code for Huabei payment)\end{tabular} \\ \midrule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}CHIP-STS\\ (Medical)\end{tabular}} & \begin{tabular}[c]{@{}c@{}}\textit{origin：}艾滋病的病因是什么\\(What is the cause of AIDS)\\ \textit{first round：}艾滋病毒是什么?\\(What is HIV?)\\ \textit{last round：}艾滋病是什么原因引起的?\\(What causes AIDS?)\end{tabular} \\ \cmidrule{2-2} 
 & \begin{tabular}[c]{@{}c@{}}\textit{origin：}高血压总是流口水是怎么回事\\(High blood pressure is always drooling what is going on)\\ \textit{first round：}高血压怎么回事\\(What about high blood pressure)\\ \textit{last round：}高血压为什么会流口水?\\(Why does high blood pressure cause drooling?)\end{tabular} \\ \cmidrule{2-2} 
 & \begin{tabular}[c]{@{}c@{}}\textit{origin：}得了糖尿病,现在越来越瘦了怎么回事\\(Why am I getting thinner and thinner now that I have diabetes)\\ \textit{first round：}糖尿病现在怎么回事?\\(What's going on with diabetes now?)\\ \textit{last round：}糖尿病患者为什么会瘦?\\(Why do people with diabetes lose weight?)\end{tabular} \\ \midrule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Chinese-QQP\\ (Common)\end{tabular}} & \begin{tabular}[c]{@{}c@{}}\textit{origin：}如何从此网站删除我的帐户?\\(How do I delete my account from this site?)\\ \textit{first round：}怎么删除网站\\(How to delete a website)\\ \textit{last round：}如何才能删除我的帐户?\\(How can I delete my account?)\end{tabular} \\ \cmidrule{2-2} 
 & \begin{tabular}[c]{@{}c@{}}\textit{origin：}关于电子产品的一些好书是什么?\\(What are some good books on electronics?)\\ \textit{first round：}有什么好的电子产品推荐\\(Any good electronics recommendations)\\ \textit{last round：}有哪些关于电子产品的好书?\\(What are some good books about electronics?)\end{tabular} \\ \cmidrule{2-2} 
 & \begin{tabular}[c]{@{}c@{}}\textit{origin：}为什么没有人看到无尽和无限之间的区别?\\(Why does no one see the difference between endless and infinite?)\\ \textit{first round：}为什么宇宙中没有极限的存在\\(Why is there no limit in the universe)\\ \textit{last round：}为什么没有人知道无限和有限之间的区别?\\(Why does no one know the difference between infinite and finite?)\end{tabular} \\ \bottomrule
\end{tabular}
\end{CJK}
\end{table*}



\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
