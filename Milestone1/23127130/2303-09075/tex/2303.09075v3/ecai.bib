@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{DBLP:journals/corr/abs-2302-13971,
  author    = {Hugo Touvron and
               Thibaut Lavril and
               Gautier Izacard and
               Xavier Martinet and
               Marie{-}Anne Lachaux and
               Timoth{\'{e}}e Lacroix and
               Baptiste Rozi{\`{e}}re and
               Naman Goyal and
               Eric Hambro and
               Faisal Azhar and
               Aur{\'{e}}lien Rodriguez and
               Armand Joulin and
               Edouard Grave and
               Guillaume Lample},
  title     = {LLaMA: Open and Efficient Foundation Language Models},
  journal   = {CoRR},
  volume    = {abs/2302.13971},
  year      = {2023},
  url       = {https://doi.org/10.48550/arXiv.2302.13971},
  doi       = {10.48550/arXiv.2302.13971},
  eprinttype = {arXiv},
  eprint    = {2302.13971}
}

@inproceedings{vaswani2017attention,
 author = {Ashish Vaswani and
Noam Shazeer and
Niki Parmar and
Jakob Uszkoreit and
Llion Jones and
Aidan N. Gomez and
Lukasz Kaiser and
Illia Polosukhin},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
 booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, {USA}},
 editor = {Isabelle Guyon and
Ulrike von Luxburg and
Samy Bengio and
Hanna M. Wallach and
Rob Fergus and
S. V. N. Vishwanathan and
Roman Garnett},
 pages = {5998--6008},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
 year = {2017}
}

@misc{https://doi.org/10.48550/arxiv.1406.2661,
 author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 copyright = {arXiv.org perpetual, non-exclusive license},
 doi = {10.48550/ARXIV.1406.2661},
 keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
 publisher = {arXiv},
 title = {Generative Adversarial Networks},
 url = {https://arxiv.org/abs/1406.2661},
 year = {2014}
}

@inproceedings{caccia2019language,
 author = {Massimo Caccia and
Lucas Caccia and
William Fedus and
Hugo Larochelle and
Joelle Pineau and
Laurent Charlin},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/CacciaCFLPC20.bib},
 booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
Addis Ababa, Ethiopia, April 26-30, 2020},
 publisher = {OpenReview.net},
 timestamp = {Thu, 07 May 2020 01:00:00 +0200},
 title = {Language GANs Falling Short},
 url = {https://openreview.net/forum?id=BJgza6VtPB},
 year = {2020}
}

@inproceedings{DBLP:conf/bmvc/Banitalebi-Dehkordi21,
 author = {Amin Banitalebi{-}Dehkordi and
Yong Zhang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/bmvc/Banitalebi-Dehkordi21.bib},
 booktitle = {32nd British Machine Vision Conference 2021, {BMVC} 2021, Online,
November 22-25, 2021},
 pages = {122},
 publisher = {{BMVA} Press},
 timestamp = {Wed, 22 Jun 2022 16:52:45 +0200},
 title = {Repaint: Improving the Generalization of Down-Stream Visual Tasks
by Generating Multiple Instances of Training Examples},
 url = {https://www.bmvc2021-virtualconference.com/assets/papers/0068.pdf},
 year = {2021}
}

@article{sohn2020fixmatch,
 author = {Sohn, Kihyuk and Berthelot, David and Carlini, Nicholas and Zhang, Zizhao and Zhang, Han and Raffel, Colin A and Cubuk, Ekin Dogus and Kurakin, Alexey and Li, Chun-Liang},
 journal = {Advances in neural information processing systems},
 pages = {596--608},
 title = {Fixmatch: Simplifying semi-supervised learning with consistency and confidence},
 volume = {33},
 year = {2020}
}

@inproceedings{wu2021textgail,
 author = {Wu, Qingyang and Li, Lei and Yu, Zhou},
 booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
 number = {16},
 pages = {14067--14075},
 title = {Textgail: Generative adversarial imitation learning for text generation},
 volume = {35},
 year = {2021}
}

@InProceedings{pmlr-v97-lu19d,
  title = 	 {{C}o{T}: Cooperative Training for Generative Modeling of Discrete Data},
  author =       {Lu, Sidi and Yu, Lantao and Feng, Siyuan and Zhu, Yaoming and Zhang, Weinan},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4164--4172},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/lu19d/lu19d.pdf},
  url = 	 {https://proceedings.mlr.press/v97/lu19d.html},
  abstract = 	 {In this paper, we study the generative models of sequential discrete data. To tackle the exposure bias problem inherent in maximum likelihood estimation (MLE), generative adversarial networks (GANs) are introduced to penalize the unrealistic generated samples. To exploit the supervision signal from the discriminator, most previous models leverage REINFORCE to address the non-differentiable problem of sequential discrete data. However, because of the unstable property of the training signal during the dynamic process of adversarial training, the effectiveness of REINFORCE, in this case, is hardly guaranteed. To deal with such a problem, we propose a novel approach called Cooperative Training (CoT) to improve the training of sequence generative models. CoT transforms the min-max game of GANs into a joint maximization framework and manages to explicitly estimate and optimize Jensen-Shannon divergence. Moreover, CoT works without the necessity of pre-training via MLE, which is crucial to the success of previous methods. In the experiments, compared to existing state-of-the-art methods, CoT shows superior or at least competitive performance on sample quality, diversity, as well as training stability.}
}

@ARTICLE{9296209,
  author={Kim, Yanghoon and Won, Seungpil and Yoon, Seunghyun and Jung, Kyomin},
  journal={IEEE Access}, 
  title={Collaborative Training of Gans in Continuous and Discrete Spaces for Text Generation}, 
  year={2020},
  volume={8},
  number={},
  pages={226515-226523},
  doi={10.1109/ACCESS.2020.3045166}}

@InProceedings{pmlr-v162-lamprier22a,
  title = 	 {Generative Cooperative Networks for Natural Language Generation},
  author =       {Lamprier, Sylvain and Scialom, Thomas and Chaffin, Antoine and Claveau, Vincent and Kijak, Ewa and Staiano, Jacopo and Piwowarski, Benjamin},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {11891--11905},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/lamprier22a/lamprier22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/lamprier22a.html},
  abstract = 	 {Generative Adversarial Networks (GANs) have known a tremendous success for many continuous generation tasks, especially in the field of image generation. However, for discrete outputs such as language, optimizing GANs remains an open problem with many instabilities, as no gradient can be properly back-propagated from the discriminator output to the generator parameters. An alternative is to learn the generator network via reinforcement learning, using the discriminator signal as a reward, but such a technique suffers from moving rewards and vanishing gradient problems. Finally, it often falls short compared to direct maximum-likelihood approaches. In this paper, we introduce Generative Cooperative Networks, in which the discriminator architecture is cooperatively used along with the generation policy to output samples of realistic texts for the task at hand. We give theoretical guarantees of convergence for our approach, and study various efficient decoding schemes to empirically achieve state-of-the-art results in two main NLG tasks.}
}

@article{ma2022principles,
 author = {Ma, Yi and Tsao, Doris and Shum, Heung-Yeung},
 journal = {Frontiers of Information Technology \& Electronic Engineering},
 pages = {1--26},
 publisher = {Springer},
 title = {On the principles of Parsimony and Self-consistency for the emergence of intelligence},
 year = {2022}
}

@inproceedings{arjovsky2017towards,
 author = {Mart{\'{\i}}n Arjovsky and
L{\'{e}}on Bottou},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/ArjovskyB17.bib},
 booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
Toulon, France, April 24-26, 2017, Conference Track Proceedings},
 publisher = {OpenReview.net},
 timestamp = {Thu, 04 Apr 2019 01:00:00 +0200},
 title = {Towards Principled Methods for Training Generative Adversarial Networks},
 url = {https://openreview.net/forum?id=Hk4\_qw5xe},
 year = {2017}
}

@inproceedings{reimers-gurevych-2019-sentence,
    title = "Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks",
    author = "Reimers, Nils  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1410",
    doi = "10.18653/v1/D19-1410",
    pages = "3982--3992",
    abstract = "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
}

@inproceedings{cui-etal-2020-revisiting,
 address = {Online},
 author = {Cui, Yiming  and
Che, Wanxiang  and
Liu, Ting  and
Qin, Bing  and
Wang, Shijin  and
Hu, Guoping},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
 doi = {10.18653/v1/2020.findings-emnlp.58},
 pages = {657--668},
 publisher = {Association for Computational Linguistics},
 title = {Revisiting Pre-Trained Models for {C}hinese Natural Language Processing},
 url = {https://aclanthology.org/2020.findings-emnlp.58},
 year = {2020}
}

@inproceedings{Lan2020ALBERT:,
 author = {Zhenzhong Lan and
Mingda Chen and
Sebastian Goodman and
Kevin Gimpel and
Piyush Sharma and
Radu Soricut},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/LanCGGSS20.bib},
 booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
Addis Ababa, Ethiopia, April 26-30, 2020},
 publisher = {OpenReview.net},
 timestamp = {Thu, 07 May 2020 01:00:00 +0200},
 title = {{ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language
Representations},
 url = {https://openreview.net/forum?id=H1eA7AEtvS},
 year = {2020}
}

@inproceedings{xu-etal-2020-clue,
 address = {Barcelona, Spain (Online)},
 author = {Xu, Liang  and
Hu, Hai  and
Zhang, Xuanwei  and
Li, Lu  and
Cao, Chenjie  and
Li, Yudong  and
Xu, Yechen  and
Sun, Kai  and
Yu, Dian  and
Yu, Cong  and
Tian, Yin  and
Dong, Qianqian  and
Liu, Weitang  and
Shi, Bo  and
Cui, Yiming  and
Li, Junyi  and
Zeng, Jun  and
Wang, Rongzhao  and
Xie, Weijian  and
Li, Yanting  and
Patterson, Yina  and
Tian, Zuoyu  and
Zhang, Yiwen  and
Zhou, He  and
Liu, Shaoweihua  and
Zhao, Zhe  and
Zhao, Qipeng  and
Yue, Cong  and
Zhang, Xinrui  and
Yang, Zhengliang  and
Richardson, Kyle  and
Lan, Zhenzhong},
 booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
 doi = {10.18653/v1/2020.coling-main.419},
 pages = {4762--4772},
 publisher = {International Committee on Computational Linguistics},
 title = {{CLUE}: A {C}hinese Language Understanding Evaluation Benchmark},
 url = {https://aclanthology.org/2020.coling-main.419},
 year = {2020}
}

@inproceedings{zhang-etal-2022-cblue,
 abstract = {Artificial Intelligence (AI), along with the recent progress in biomedical language understanding, is gradually offering great promise for medical practice. With the development of biomedical language understanding benchmarks, AI applications are widely used in the medical field. However, most benchmarks are limited to English, which makes it challenging to replicate many of the successes in English for other languages. To facilitate research in this direction, we collect real-world biomedical data and present the first Chinese Biomedical Language Understanding Evaluation (CBLUE) benchmark: a collection of natural language understanding tasks including named entity recognition, information extraction, clinical diagnosis normalization, single-sentence/sentence-pair classification, and an associated online platform for model evaluation, comparison, and analysis. To establish evaluation on these tasks, we report empirical results with the current 11 pre-trained Chinese models, and experimental results show that state-of-the-art neural models perform by far worse than the human ceiling.},
 address = {Dublin, Ireland},
 author = {Zhang, Ningyu  and
Chen, Mosha  and
Bi, Zhen  and
Liang, Xiaozhuan  and
Li, Lei  and
Shang, Xin  and
Yin, Kangping  and
Tan, Chuanqi  and
Xu, Jian  and
Huang, Fei  and
Si, Luo  and
Ni, Yuan  and
Xie, Guotong  and
Sui, Zhifang  and
Chang, Baobao  and
Zong, Hui  and
Yuan, Zheng  and
Li, Linfeng  and
Yan, Jun  and
Zan, Hongying  and
Zhang, Kunli  and
Tang, Buzhou  and
Chen, Qingcai},
 booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2022.acl-long.544},
 pages = {7888--7915},
 publisher = {Association for Computational Linguistics},
 title = {{CBLUE}: A {C}hinese Biomedical Language Understanding Evaluation Benchmark},
 url = {https://aclanthology.org/2022.acl-long.544},
 year = {2022}
}

@inproceedings{wang-etal-2018-glue,
 author = {Alex Wang and
Amanpreet Singh and
Julian Michael and
Felix Hill and
Omer Levy and
Samuel R. Bowman},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/WangSMHLB19.bib},
 booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
New Orleans, LA, USA, May 6-9, 2019},
 publisher = {OpenReview.net},
 timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
 title = {{GLUE:} {A} Multi-Task Benchmark and Analysis Platform for Natural
Language Understanding},
 url = {https://openreview.net/forum?id=rJ4km2R5t7},
 year = {2019}
}
