\documentclass{ecai}
\usepackage{graphicx}
\usepackage{latexsym}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MATH
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TABLE & FIGURE
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{svg}
\usepackage{subcaption}
\usepackage[normalem]{ulem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{CJKutf8}
\usepackage{color}
% \usepackage[sort]{natbib}
\usepackage[square,sort,comma,numbers]{natbib}
% \newcommand{\tong}[1]{\textcolor{red}{#1}}

\ecaisubmission   % inserts page numbers. Use only for submission of paper.
                  % Do NOT use for camera-ready version of paper.

\begin{document}

\begin{frontmatter}

\title{Enhancing Text Generation with Cooperative Training}


\author[A]{Tong Wu\thanks{The work was done as an intern at IDEA.}$^{**;}$}\orcid{0009-0003-3154-1213}
\author[B]{Hao Wang\thanks{These authors contributed equally to this work.}}
\author[B]{Zhongshen Zeng}
\author[A]{Wei Wang}
\author[A,C]{Hai-Tao Zheng\thanks{Corresponding Authors. Email: zheng.haitao@sz.tsinghua.edu.cn, zhangjiaxing@idea.edu.cn}}
\author[B]{Jiaxing Zhang$^{***;}$}


\address[A]{Shezhen International Graduate School, Tsinghua Universiy}
\address[B]{International Digital Economy Academy}
\address[C]{Pengcheng Laboratory}


% \author[A]{\fnms{First}~\snm{Author}\orcid{....-....-....-....}\thanks{Corresponding Author. Email: somename@university.edu.}}
% \author[B]{\fnms{Second}~\snm{Author}\orcid{....-....-....-....}}
% \author[B]{\fnms{Third}~\snm{Author}\orcid{....-....-....-....}} % use of \orcid{} is optional


% \address[A]{Short Affiliation of First Author}
% \address[B]{Short Affiliation of Second Author and Third Author}

\begin{abstract}
Recently, there has been a surge in the use of generated data to enhance the performance of downstream models, largely due to the advancements in pre-trained language models. However, most prevailing methods trained generative and discriminative models in isolation, which left them unable to adapt to changes in each other. These approaches lead to generative models that are prone to deviating from the true data distribution and providing limited benefits to discriminative models. While some works have proposed jointly training generative and discriminative language models, their methods remain challenging due to the non-differentiable nature of discrete data. To overcome these issues, we introduce a \textit{self-consistent learning} framework in the text field that involves training a discriminator and  generator cooperatively in a closed-loop manner until a scoring consensus is reached. By learning directly from selected samples, our framework are able to mitigate training instabilities such as mode collapse and non-convergence. Extensive experiments on four downstream benchmarks, including AFQMC, CHIP-STS, QQP, and MRPC, demonstrate the efficacy of the proposed framework.
\end{abstract}

\end{frontmatter}

\begin{figure*}[ht]
\begin{center}
\includegraphics[width=0.8\textwidth]{Figure/sim-gen.pdf}
\end{center}
\caption{Overview of the flow chart for the SCL framework.} 
\label{framework}
\end{figure*}

\section{Introduction}
\input{Section/1-Introduction}


\section{Related Works}
\input{Section/2-RelatedWorks}


\section{Methodology}
\input{Section/3-Methodology}


\section{Experiments}
\input{Section/4-Experiments}


\section{Conclusion}
\input{Section/5-Conclusion}

\section{Acknowledgements}
This research is supported by National Natural Science Foundation of China (Grant No.62276154), Research Center for Computer Network (Shenzhen) Ministry of Education, Beijing Academy of Artificial Intelligence (BAAI), the Natural Science Foundation of Guangdong Province (Grant No. 2023A1515012914), Basic Research Fund of Shenzhen City (Grant No. JCYJ20210324120012033 and JSGG20210802154402007), the Major Key Project of PCL for Experiments and Applications (PCL2021A06), and Overseas Cooperation Research Fund of Tsinghua Shenzhen International Graduate School (HW2021008).

\bibliography{ecai}
\bibliographystyle{unsrt}

% \newpage
% \appendix
% \input{Section/6-Appendix}

\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
