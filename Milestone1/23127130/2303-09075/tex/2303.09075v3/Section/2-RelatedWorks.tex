To alleviate the lack of annotated data in supervised learning in NLP,  semi-supervised learning (SSL) has been a popular line of research . The sources of the unlabeled data required by SSL are either collected from the domains or generated by generative language models. Then NLU models can learn from the unlabeled data by pseudo-labeling \cite{DBLP:conf/bmvc/Banitalebi-Dehkordi21} and consistent regularization \cite{sohn2020fixmatch}. However,  collecting unlabeled data comes at a cost(though smaller than labeling data), and the total amount is limited. Even with generative models, there is no guarantee of the quality of the generated samples, because the model cannot tune the generating results based on the performance of the downstream tasks. In contrast, our method usually includes a continuously updated generative model, which dynamically adjusts its generation according to the performance of downstream tasks.

GANs can be used as data enhancer to complement the lack of data for downstream tasks. Unlike conventional GANs in continuous domains, sequential GANs for discrete outputs are usually trained with reinforcement learning methods \cite{wu2021textgail}. But they usually suffer from high variance, partly due to the non-stationarity nature of their reward distribution. Whereas work based on cooperative training has opened the way for more efficient methods. CoT \cite{pmlr-v97-lu19d} explicitly estimates and optimizes JS divergence through a joint maximization framework, ConcreteGAN \cite{9296209} employs an autoencoder to learn implicit data manifold thus providing learning objective for adversarial training in a continuous space. However, their approach is still to back propagate through the gradient signal. More similar to our work is RML-GAN \cite{pmlr-v162-lamprier22a}, which uses a discriminator combined with a generative strategy to output real text samples for the task at hand. But they require complex and time-consuming Monte Carlo tree search, whereas we utilize a dynamic selection mechanism, and the training objective of the discriminator is exactly the same as that of the downstream task.
