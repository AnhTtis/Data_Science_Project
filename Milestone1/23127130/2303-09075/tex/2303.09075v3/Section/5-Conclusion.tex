In this paper, we propose a self-consistent learning framework in the text field to enable cooperative training of the generator and the discriminator.  During the training process, the generator and the discriminator continuously enhance each other until reaching a score consensus. This framework can utilize both limited labeled data and large-scale unlabeled domain-related corpus. Experimental results on four Chinese / English datasets demonstrate that as a form of closed-loop training, our proposed framework can outperforms the strong baselines with continuously improved generators and discriminators. 

% For future work, we will explore the effectiveness of our self-consistent learning framework on more NLP tasks, since the framework is straightforward and has no additional requirements on generators and discriminators.
