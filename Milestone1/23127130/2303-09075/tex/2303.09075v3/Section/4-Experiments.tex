\subsection{Tasks Design}
\label{task design}
In our experiments, the pre-training datasets are used to warm up the discriminator and generator, and the domain-related corpus is a set of independent sentences. To avoid label leakage, none of the training datasets participate in the pre-training of the generator and discriminator. In other words, the datasets in pre-training and self-consistent training are two non-overlapped datasets.

\textbf{Zero-Shot Baseline}: In the zero-shot setting, we utilize the warm-up generator and employ the constructed prompts to directly generate samples without any specific learning towards the prediction targets. These samples are then filtered by the discriminator and used as training data for the next round of the generator.We utilize the best-performing Chinese model RoBERTa-wwm-ext-large \cite{cui-etal-2020-revisiting}  and English model ALBERT-xxlarge-v2 \cite{Lan2020ALBERT:} as the base discriminators in our self-consistent learning framework. 

\textbf{Fine-Tune Baseline}: In the fine-tuning setting, similar sentence pairs like $<s_a, s_b>$ are used as training data for the generator in the form of "$s_a / s_b$ is similar to $s_b / s_a$". Here, "$s_a / s_b$ is similar to" serves as the prompt, and $s_b / s_a$ is the target that the generator will learn to predict. We compare our model with several strong baselines Chinese models MacBERT , StructBERT , RoFormer , XLNet, ELECTRA, ALBERT, RoBERTa and English models BERT, XLM-RoBERTa (XLM-R), XLNet, ELECTRA, ALBERT, RoBERTa.

\subsection{Experiments Setup}
\subsubsection{Datasets}
We conduct experiments on three Chinese datasets AFQMC (Financial) \cite{xu-etal-2020-clue}, CHIP-STS (Medical) \cite{zhang-etal-2022-cblue}, QQP-ZH (Common) \cite{wang-etal-2018-glue} and an English dataset MRPC (News) \cite{wang-etal-2018-glue}.  More details about the datasets are given in supplementary material.

% \subsubsection{Model pre-training}
% We adopt the well-established Transformer-XL \cite{dai2019transformer} / OPT \cite{zhang2022opt} architectures as the Chinese / English generator. To enable the generator to generate similar sentences with better linguistic quality, we pre-train a Transformer-XL model with 5.0 billion parameters and incrementally pre-train an OPT model with 2.7 billion parameters on the corpus consisting of plain texts and similar sentence pairs. Cleaned large-scale Chinese corpus WuDaoCorpora \cite{YUAN202165} and English corpus WikiText \cite{DBLP:conf/iclr/MerityX0S17} are used as plain texts. Similar sentence pairs that do not overlap with downstream datasets are used in the pre-training, and the designed prompts are employed to guide the generation of similar sentences. More details regarding model pre-training can be found in supplementary material.

\subsection{Zero-Shot Results}
\label{zeroshot res}
Table~\ref{dis_results}(a) shows how the F1 score of the discriminator varies with the number of self-consistent learning rounds on different datasets in the zero-shot task. According to Algorithm~\ref{scl}, the training is stopped when the discriminator no longer improves for two consecutive rounds. In addition, these four datasets are collected from different domains to further reflect the generality of our method in different domains.

\input{Table/coo_dis_results}

The scores in the last line of Table~\ref{dis_results}(a) give the improvement of our discriminator in the last round relative to the first round. We can see that the F1 score gradually increases after each training round, eventually reaching a 10+ absolute percentage (AP) improvement. We believe what drives the improvement of the discriminator is the self-consistency, which it acquires with the generator step by step during the loop. 

To verify that the generator also improves after self-consistent training, we adopt Perplexity and Bertscore to measure the language fluency and the semantic similarity (i.e. domain specificity) respectively. For different generators in different rounds, we first select $s^a$ in similar sentence pairs from the same test set as the original sentences input, and generate similar sentences $s^b$ with greedy search. The reason for not using other sampling methods is to ensure reproducibility. Given the generated sentences, we introduce an additional GPT2 \footnote{Wenzhong-GPT2-110M for Chinese data, and GPT2-base for English data.} model to calculate the perplexity of generated similar sentences, and use a third-party library \footnote{\href{https://pypi.org/project/bert-score/}{https://pypi.org/project/bert-score/}} to calculate the bertscore between the original and generated similar sentences. The results are shown in Table~\ref{gen_res}. 

\input{Table/coo_gen_zeroshot}

We can see that the perplexity / bertscore of the last round in Table~\ref{gen_res} has decreased / improved compared to the first round. Note that a lower perplexity indicates a more fluent sentence, while a higher bertscore indicates a more similar sentence. It suggests that after self-consistent training,  the generator is gradually improved in language fluency and semantic similarity (i.e. domain specificity). The reason why the improvement of the generator is not as obvious as that of the discriminator is that the size of the generator is several times that of the discriminator, and the total number of training samples is limited. In supplementary material, the generated samples of the generator in different rounds are given to show the changes in the generation.

\subsection{Fine-Tune Results}
Our method not only works well in the zero-shot case, but also achieves good results in the full-data case. For the sake of a fair comparison, we reproduce several strong baselines on the four training sets, and their performances on the test sets are shown in Table~\ref{dis_results}(b).

Our approach uses the best-performing model on a single test set as the base discriminator for self-consistent learning. The bold scores in the last line of Table~\ref{dis_results}(b) show that our method outperforms the strong baselines (shaded in gray) by 1 to 2 AP on all four test datasets, indicating the potential of self-consistent learning to further improve the model performance. 

\subsection{Evaluating Self-consistency}
In this section, we evaluate the consistency between the generator and the discriminator as the learning loop unfolds. We follow the same method used in Section~\ref{zeroshot res} and use greedy search to generate similar sentences on the same test set. Then we take the confidence of the discriminator $R_\mathcal{D}$ as the score of the discriminator, which is calculated for the original sentences $s^a$ and the generated similar sentences $s^b$ according to Equation~\ref{r_d}. 

\begin{equation}
	R_\mathcal{D} = p_\mathcal{D}(y^+|s^a, s^b) \label{r_d} 
\end{equation}

where $y^+$ represents a positive label. 

\begin{figure*}[ht] %H为当前位置，!htb为忽略美学标准，htbp为浮动图形
\begin{center}
\includegraphics[width=0.8\textwidth]{Figure/all1.pdf}
\end{center}
\captionsetup{width=0.9\textwidth}
\caption{Results of ablation experiments on pre-training data and selection mechanism of Zero-Shot. Results of the proposed method, without pre-training data, and without the selection mechanism are given in red, green, and blue, respectively.} 
\label{ablation}
\end{figure*}

For the generator, using its own perplexity as a criterion for determining the similarity between sentences $s^a$ and $s^b$ is not always effective. Perplexity primarily reflects the generator's ability to fit similar data pairs, but it falls short in mitigating the impact of noise pairs. Therefore, to quantify this similarity, we introduce a third-party static model SimCSE \footnote{We use SimCSE-BERT-base to calculate scores on Chinese datasets and sup-SimCSE-BERT-base-uncased on English datasets. } to get the embedding representation $\mathbf{a},\mathbf{b}$ of sentences $s^a,s^b$. The cosine similarity $R_\mathcal{G}$ between $\mathbf{a}$ and $\mathbf{b}$ is then calculated according to Equation~\ref{r_g} to approximate the score of the generator.

\begin{gather}
    \mathbf{a}, \mathbf{b} = \text{Encoder}(s^a), \text{Encoder}(s^b) \nonumber \\ 
	R_\mathcal{G} = \frac{\mathbf{a} \cdot \mathbf{b}}{\left\|\mathbf{a}\right\|_2*\left\|\mathbf{b}\right\|_2} 
\label{r_g}
\end{gather}

where $\mathbf{a}$ and $\mathbf{b}$ both represent the embedding representation at the $[CLS]$ position. Note that the original sentence $s^a$ remains unchanged in each round, while the generated sentence $s^b$ changes.

\begin{figure}[ht] 
\begin{center}
\includegraphics[width=0.3\textwidth]{Figure/kl.pdf}
\captionsetup{width=0.4\textwidth}
\caption{The KL Divergence between the score distributions of Discriminator and Generator in Zero-Shot.} 
\label{distance}
\end{center}
\end{figure}

Finally, for the trained discriminator and generator in each round $t$, we can obtain two score distributions $\mathbf{R_\mathcal{D}^t}$ and $\mathbf{R_\mathcal{G}^t}$ correspondingly. According to Theorem~\ref{theorem_kl}, we draw the curves of KL divergence between $\mathbf{R_\mathcal{D}^t}$ and $\mathbf{R_\mathcal{G}^t}$ in each round for the four datasets: AFQMC, CHIP-STS, QQP-ZH, and MRPC. As illustrated in Figure~\ref{distance}, all the curves show a clear downward trend, indicating that the distance between the two score distributions decreases with the increase in the number of training rounds until a score consensus is reached.

\subsection{Effect of Pre-training Data and Selection Mechanism}
\label{ablation exp}
We perform ablation experiments on the pre-training data and the selection mechanism in the zero-shot case. As described in Section~\ref{task design}, the pre-training data is used to pre-train the generator and discriminator, completely independent of the experimental datasets in self-consistent training.

To explore the influence of pre-training data on self-consistent training, we no longer add it in each round when training the discriminator, and only the generated data is used. But when the generator is trained, pre-training data is still retained to prevent language degeneration and lack of expressive diversity of the generation. The result of removing pre-training data is shown as the green curves in Figure~\ref{ablation}. With all other training parameters being the same, after the same number of training rounds, the discriminator is slightly worse compared to the original method (red curves in Figure~\ref{ablation}). However, the green curves maintain an upward trend and are very close to the red curves in all datasets except CHIP-STS. This shows that the generated data plays a key role in continuously improving the discriminator, while the pre-training data has a limited role. 

In order to explore the effect of the selection mechanism on training the discriminator, we remove the selection mechanism when training the discriminator, while the training of the generator remains unchanged. The blue curves in Figure~\ref{ablation} depict the performance of the discriminator in each round after removing the selection mechanism. Compared to the original method (red curves), the discriminator only improves in the first round after removing the selection mechanism, which demonstrates the importance of the selection mechanism on the discriminator for the convergence of the self-consistent learning framework. 

\subsection{Experiments on Different Threshold Functions}
To compare the effect of different threshold functions on the final result, we use four type of functions, including oscillatory function (cosine), constant function and monotonically increasing functions (quadratic and linear). For the fairness of comparison, we keep the maxima and minima the same for all functions(except for the constant threshold), and the values are given in supplementary material.

The best results and the second-best results are \textbf{bold} and \underline{underlined}, respectively. As can be seen from the Table~\ref{zero_shot_func}, in the zero-shot setting, the chosen linear function outperforms the other functions, and all the threshold functions show an averaging 10+ AP improvement relative to the baseline. Therefore, the self-consistent learning framework makes it easy to choose a certain threshold function and perform well, and the results are not so sensitive to the choice of the functions. A more detailed figure of the effect of different threshold functions on the results is shown in supplementary material. 

\input{Table/thre_zeroshot}

\input{Table/thre_finetune}

Table~\ref{fine_tune_func} shows the effects of different threshold functions in the fine-tune experiment. It can be seen that all functions have a $1\sim 2$ AP increase relative to the baseline, and the chosen linear function achieves the best performance on all datasets except QQP-ZH.

\subsection{Contrast Experiments with Adversarial Training}

We further demonstrate the superiority of the cooperative approach by comparing the results with adversarial experiments. All experimental settings independent of the training method remain the same in the adversarial training. 

During the experiments, the generator is no longer trained using the samples filtered by the discriminator, but the rewards passed by the discriminator assist the training. All generated samples are treated as negative samples when training the discriminator.

Specifically, $\mathcal{G}$ takes the prompt ' "$s^a$" is similar to " ' and the first $M$ tokens of $s^b$ as input to get $M$ sentence pairs $<s^a, s^b_m>$, where $m$ is from 1 to $M$. Note that we repeat the process of generating sentences $N$ times to reduce the negative impact caused by the large variance of the rewards.\footnote{In practice, we take $M=5,N=5$ for ease of calculation.} The sentence pair is formalized as

\begin{equation}
\nonumber
<s^a, s^b_m>=\mathcal{G}_\theta(s^b_m|s^b_{<m}, \boldsymbol{s^a} ; N)
\end{equation}

Once the $M*N$ sentence pairs are generated, they are passed as input to the $\mathcal{D}$ to obtain the probability score $Q_m^n$ for each of them. We take the average of $Q_m^n$ over $N$ as the reward $\bar Q_m$ corresponding to the $m$-th token. If the sentence length of $s^b$ is greater than $M$, the rewards of the remaining tokens are all the same as those of the $M$-th token. Taking the $m$-th token as an example, the rewards $\bar Q_m$ can be formalized as

\begin{equation}
\nonumber
\begin{array}{l}
\bar Q_{\mathcal{D}_\phi}^{\mathcal{G}_\theta}(m) =\left\{\begin{array}{cc}
\frac{1}{N} \sum_{n=1}^{N} \mathcal{D}_\phi(g_m^n) & m \leq M \\
\bar Q(M) & m>M
\end{array}\right.
\end{array}
\end{equation}

where $g_m^n$ is the $n$-th sentence pair with length $m$.\looseness=-1

Therefore, the objective function for training the generator $\mathcal{G}$ is,

\begin{equation}
\nonumber
	\mathcal{L}_\mathcal{G}(\boldsymbol{s^a}, \boldsymbol{s^b}) = -\frac{1}{|\boldsymbol{s^b}|} \sum_{t=1}^{|\boldsymbol{s^b}|}\log(p_{\mathcal{G}}(s^b_t|s^b_{<t}, \boldsymbol{s^a}) * \bar Q_t)
\end{equation}

The loss function of training the discriminator remains the same as Equation \ref{train_dis}, but differing from cooperative training, the generated samples are regarded as negative samples to the discriminator, and the training target for the discriminator can be given by

\begin{equation}
\nonumber
	\min_{\phi} -\mathbb{E}_{X \sim p_{\text {data}}}\left[\log \mathcal{D}_\phi(X)\right] -\mathbb{E}_{X \sim p_{\mathcal{G}_\theta}}\left[\log \left(1-\mathcal{D}_\phi(X)\right)\right]
\end{equation}

The results of zero-shot and fine-tune on the four datasets are shown in Tables~\ref{zero_shot_adv} and \ref{fine_tune_adv}.

\input{Table/adv_dis_zeroshot}

As can be seen from Table~\ref{zero_shot_adv}, in the zero-shot setting, training in an adversarial manner does not give any improvement over the baseline. Because the initial discriminator in the zero-shot setting is very weak in distinguishing positive and negative samples, it is reasonable to believe that if all generated samples are considered negative samples from the very beginning, it is difficult for the discriminator to know how to distinguish positive samples. As a result, the F1 scores on both AFQMC and CHIP-STS datasets end up being 0,  while the scores on the QQP-ZH and MRPC datasets fluctuate intensively with the number of rounds,  which further validates the instability of the adversarial training in the zero-shot setting.

\input{Table/adv_dis_finetune}

For the fine-tune experiments, Table~\ref{fine_tune_adv} shows that training in an adversarial manner can slightly improve the performance on the QQP-ZH and MRPC datasets, but is still worse than the cooperative training. On the AFQMC and CHIP-STS dataset, adversarial training makes it even worse relative to the baseline. It is worth noting that the whole process of adversarial training is so unstable and it is easy to collapse after a few training rounds.