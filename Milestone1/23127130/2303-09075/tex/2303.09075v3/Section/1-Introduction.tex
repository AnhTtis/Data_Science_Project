The advance of Pre-trained Language Models (PLMs) like GPT-3 \cite{brown2020language} and LLaMA \cite{DBLP:journals/corr/abs-2302-13971} has substantially improved the performance of deep neural networks across a variety of Natural Language Processing (NLP) tasks. Various language models, based on the Transformer \cite{vaswani2017attention} architecture,  have been proposed, leading to state-of-the-art (SOTA) performance on the fundamental discrimination tasks. These models are first trained with self-supervised training objectives (e.g., predicting masked tokens according to surrounding tokens) on massive unlabeled text data, then fine-tuned on annotated data to adapt to downstream tasks of interest.  However, annotated data is usually limited to a wide range of downstream tasks, which results in overfitting and a lack of generalization to unseen data.

One straightforward way to deal with this data scarcity problem is data augmentation , and incorporating generative models to perform data augmentation has been widely adopted recently . Despite its popularity, the generated text can easily deviate from the real data distribution without exploiting any of the signals passed back from the discrimination task. In previous studies, generative data augmentation and discrimination have been well studied as separate problems, but it is less clear how these two can be leveraged in one framework and how their performances can be improved simultaneously. \looseness=-1

Generative Adversarial Networks (GANs) \cite{https://doi.org/10.48550/arxiv.1406.2661} are good attempts to couple generative and discriminative models in an adversarial manner, where a two-player minimax game between learners is carefully crafted. GANs have achieved tremendous success in domains such as image generation , and related studies have also shown their effectiveness in semi-supervised learning. However,  in the text field, GANs are difficult to train, most training objectives work well for only one model, either the discriminator or the generator, so rarely both learners can be optimal at the same time. This essentially arises from the adversarial nature of GANs, that during the process, optimizing one learner can easily destroy the learning ability of the other, making GANs fail to converge.

Another limitation of simultaneously optimizing the generator and the discriminator comes from the discrete nature of text in NLP, as no gradient propagation can be done from discriminators to generators. One theoretically sound attempt is to use reinforcement learning (RL), but the sparsity and the high variance of the rewards in NLP make the training particularly unstable \cite{caccia2019language}. 

To address these shortcomings, we novelly introduce a self-consistent learning framework based on one generator and one discriminator: the generator and the discriminator are alternately trained by way of cooperation instead of competition, and the selected samples are used as the medium to pass the feedback signal from the discriminator. Specifically, in each round of training, the samples generated by the generator are synthetically labeled by the discriminator, and then only part of them would be selected based on dynamic thresholds and used for the training of the discriminator and the generator in the next round. Several benefits can be discovered from this cooperative training process. First, a closed-loop form of cooperation can be established so that we can get the optimal generator and discriminator at the same time. Second, this framework helps improve the generation quality while ensuring the domain specificity of generator, which in turn contributes to training. Third, a steady stream of diverse synthetic samples can be added to the training in each round and lead to continuous improvement of the performance of all learners. Finally, we can start the training with only domain-related corpus and obtain strong results, while these data can be easily sampled with little cost or supervision. Also, the performance on labeled datasets can be further boosted based on the strong baselines. As an example to demonstrate the effectiveness of our framework in the text field, we examine it on four downstream text generation benchmarks, including AFQMC, CHIP-STS, QQP, and MRPC. The experiments show that our method significantly improves over standalone state-of-the-art discriminative models on zero-shot and full-data settings.

Our contributions are summarized as follows,

$\bullet$ We propose a self-consistent learning framework in the text field that incorporates the generator and the discriminator, in which both achieve remarkable performance gains simultaneously.

$\bullet$ We propose a dynamic selection mechanism such that cooperation between the generator and the discriminator drives the convergence to reach their scoring consensus.

$\bullet$ Experimental results show that the generator in our framework can continuously adjust its generation samples based on the performance of downstream tasks, while the discriminator can outperform the strong baselines.

