\subsection{cooperative or adversarial}
Following the principle of self-consistency outlined in \cite{ma2022principles}, a closed-loop training needs to be built between the generator and the discriminator, either cooperatively or adversarially. GANs are typical examples of adversarial learning, but training  GANs remains quite unstable. Let us consider an extreme case to show the possible instability: the discriminator can perfectly distinguish real data and fake data generated by the generator, and the generator can fully reproduce the real data distribution. Then the discriminator has only a 50\% probability of selecting all samples that are generated by the generator. Therefore, any further updates to the generator parameters based on the feedback from the discriminator deviate the generator from the optimum. Neither the generator nor the discriminator can likely be optimal \cite{arjovsky2017towards}. In practice, a very delicate balance needs to be maintained between the discriminator and the generator to keep the training stable. In terms of cooperatively closed-loop learning, as discussed below, it does not suffer from instability: the generator and the discriminator usually enhance each other. 

\subsection{Self-consistent Learning Framework}
In this section, we introduce our self-consistent learning (\textbf{SCL}) framework. 

As shown in Figure~\ref{framework}, our framework, similar to the GANs, consists of a generator and a discriminator model. However, contrasting to the GANs, these two parts in our framework work cooperatively to enhance each other. Specifically, for any given class $k$, the generator $\mathcal{G}$ now become a conditional generator that takes in an input sentence $s^a_k$ and generate an output sentence $s^b_k$. The discriminator $\mathcal{D}$ is then responsible for discriminating the sentence using a dynamic threshold $\epsilon_\mathcal{D}$. The discriminated sentence is used as positive or negative data for that specific class to continue the training process. Once the new discriminator is trained, the sentence is discriminated again by the new discriminator with a different dynamic threshold $\epsilon_\mathcal{G}$. This time only the positive data is passed to the generator as the training data for the new round. In this way, a closed loop of cooperation is formed. \looseness=-1

In the above closed-loop training, we propose a \textbf{selection mechanism} that uses dynamic thresholds to filter samples. This mechanism is empirically shown to play a critical role in closing the gap between the generator and the discriminator, and thus makes this cooperation loop a virtuous circle. Specifically, as shown in Equation~\ref{mlp}, the output probability $p_{\mathcal{D}}(y=k|s^b_k)$ that the sentence $\{s^b_k\}$ belongs to class $k$ is calculated from the embedding representation $\mathbf{h}$\footnote{We follow \cite{reimers-gurevych-2019-sentence} and use the embedding representation of $CLS$-token as the sentence representation $\mathbf{h}$ .} of $\{s^b_k\}$,

\begin{equation}
\label{mlp} 
p_{\mathcal{D}}(y=k|s^b_k) = \text{softmax}( \text{MLP}(\mathbf{h}))
\end{equation}

where $y$ represents the class label.  Then, through the filtering function ${\texttt{filter}^{(t)}_k(\cdot)}$ in round $t$ for the $k$-th class 
 in Equation~\ref{thre}, we keep samples whose output probability is not less than threshold $\epsilon_{t,k}$, while other generated samples whose confidence is lower than threshold $\epsilon_{t,k}$ are discarded.

\begin{equation}
\label{thre}
    \texttt{filter}^{(t)}_k(s^b_k) \triangleq {p_{\mathcal{D}}(k|s^b_k) \geq \epsilon_k^t} 
\end{equation}

where $\epsilon_{t,k}$ represents the dynamic threshold for accepting $\{s^b_k\}$ as negative or positive samples in the $t$-th round. The generalized threshold function for $\epsilon_k^t$ is defined as,

\begin{equation}
\label{dyn_thre}
    \epsilon_k^t = f(t, \mathcal{L}_{t-1,k}, \epsilon_k^{t-1}) 
\end{equation}

where $\mathcal{L}_{t-1,k}$ and $\epsilon_k^{t-1}$ represent the discriminator loss and threshold for round $t-1$, respectively. $\mathcal{L}_{0,k}$ is set as 0 and $\epsilon_k^0 = \lambda$, where $\lambda$ represents a hyperparameter. \looseness=-1

\begin{theorem}
\label{theorem_kl}
At round $t$, given the previous round discriminator $\mathcal{D}^{t-1}_\phi$, the aim of the optimization of the generator $\mathcal{G}^{t}_\theta$,  boils down to, \looseness=-1
\begin{equation}
\nonumber
    \min_{\theta}  \mathbb{D}_{KL}(p_{\mathcal{D}^{t-1}_\phi}^k(\cdot), p_{\mathcal{G}^{t}_\theta}^k(\cdot)) 
\end{equation}

where $\mathbb{D}_{KL}$ is the standard KL divergence, $p_{\mathcal{G}^{t}_\theta}^k(\cdot)$ refers to the degree of confidence that the sentences generated by the generator belong to a given class $k$ (we can either train the generator to express its confidence in the generated sentences or use a fixed third-party model to score them), and $p_{\mathcal{D}^{t-1}_\phi}^k(\cdot)$ the probability of being classified into class $k$ given by the discriminator.
\end{theorem}

Theorem \ref{theorem_kl} shows that the generator at round $t$ is encouraged to approximate the probability distribution given by the previous round discriminator. In particular, on the basis of a well-pretrained discriminator, the generated distribution of the generator can be guaranteed to be faithful to the real data distribution. 

\textbf{Proof.} We use the previous round  generator $\mathcal{G}^{t-1}_\theta$ to generate samples, and filter them using the previous round discriminator $\mathcal{D}^{t-1}_\phi$ with a threshold $\epsilon^{t-1}$, then these samples are used for the training of the current round generator $\mathcal{G}^{t}_\theta$. Therefore, the optimization of $\mathcal{G}^{t}_\theta$ will tend to  maximize the probability that the generated samples pass the discrimination for the fixed $\mathcal{D}^{t-1}_\phi$. For a given class $k$, we have \looseness=-1
\begin{equation}
\nonumber
\small
 \max_{\theta} \mathbb{E}_{x\sim  p_{\mathcal{G}^{t-1}_\theta}^k} p_{\mathcal{G}^{t}_\theta}^k(x) \quad \texttt{s.t.} \quad \texttt{filter}^{(t-1)}_k(x) = 1
\end{equation}
where the definition of function $\texttt{filter}^{(t-1)}_k(\cdot)$ has been given in Equation~\ref{thre}. 


The above objective is equivalent to sampling from the generator being optimized in round $t$ and making these samples pass the discrimination in round $t-1$ as much as possible, which gives
\begin{equation}
\nonumber
 \max_{\theta} \mathbb{E}_{x\sim  p_{\mathcal{G}^{t}_\theta}^k}p_{\mathcal{D}^{t-1}_{\phi}}^k(x)
\end{equation}
where $p_{\mathcal{D}^{t-1}_{\phi}}^k(x)$ is fixed.

A further transformation of the formula shows that
\begin{align*}
& \max_{\theta} \mathbb{E}_{x\sim  p_{\mathcal{G}^{t}_\theta}^k}p_{\mathcal{D}^{t-1}_{\phi}}^k(x) \\
\stackrel{(\romannumeral 1)} \Rightarrow & \max_\theta \int \mathrm{d}\pmb{\theta} \nabla_{\pmb{\theta}} \mathbb{E}_{x\sim  p_{\mathcal{G}^{t}_\theta}^k}p_{\mathcal{D}^{t-1}_\phi}^k(x) \\
\stackrel{(\romannumeral 2)} \Rightarrow
& \max_{\theta} \int \mathrm{d}\pmb{\theta} \mathbb{E}_{x\sim  p_{\mathcal{G}^{t}_\theta}^k} \nabla_{\pmb{\theta}} \log p_{\mathcal{G}^{t}_\theta}^k(x)p_{\mathcal{D}^{t-1}_\phi}^k(x) \\
\stackrel{(\romannumeral 3)} \Rightarrow
& \max_{\theta} \int \mathrm{d}\pmb{\theta}  \nabla_{\pmb{\theta}} \frac{1}{N}\sum_{i=1}^{N} \{\log p_{\mathcal{G}^{t}_\theta}^k(x_i)p_{\mathcal{D}^{t-1}_\phi}^k(x_i) \\&- \log p_{\mathcal{D}^{t-1}_\phi}^k(x_i)p_{\mathcal{D}^{t-1}_\phi}^k(x_i)\} \\
\stackrel{(\romannumeral 4)} \Rightarrow
& \min_{\theta} \mathbb{D}_{KL}(p_{\mathcal{D}^{t-1}_\phi}^k(\cdot), p_{\mathcal{G}^{t}_\theta}^k(\cdot))
\end{align*}

where $(\romannumeral 1)$ uses the integral property that integrating the derivative of a function gives the original function along with a constant, $(\romannumeral 2)$ takes advantage of the derivative property of the logarithmic function, $(\romannumeral 3)$ approximates the expectation of the probability distribution $p_{\mathcal{G}^{t}_\theta}^k(\cdot)$  by using averaging on $N$ samples sampling from $p_{\mathcal{G}^{t}_\theta}^k(\cdot)$, and adding a constant term $-\log p_{\mathcal{D}^{t-1}_\phi}^k(\cdot)p_{\mathcal{D}^{t-1}_\phi}^k(\cdot)$ with respect to $\theta$ under the summation would not change its derivative, and $(\romannumeral 4)$ cancels out the integral and the derivative and uses the definition of KL divergence. 
The above concludes our proof.

\textbf{Why Cooperative, Not Adversarial?} (1) the generator is no longer a challenger to the discriminator that only provides negative data points to fool it,  but now serves as a data augmenter to provide both positive and negative data points to enhance the discriminator; (2) the generator no longer updates its parameters through the policy gradients guided by the signals from the discriminator, but rather by utilizing the filtered data points to further improve its conditional generation quality. Note that by deliberately choosing the conditional generation paradigm along with the selection mechanism, we not only make the training more stable due to the different training goals, but also mitigate the mode collapse problem of GANs. Besides, by iterating through the loops, our framework achieves self-consistency by honing the domain specificity of the generator and increasing the domain data exposure of the discriminator.

\subsection{Text Generation}
We leverage the four text generation tasks ($i.e.$ $k=2$) as an example to demonstrate the effectiveness of our method. At this time, corresponding to Equation~\ref{thre}, $k=1/0$ represents the positive/negative class, and $\texttt{filter}^{(t)}_{1/0}$ represents the filter function in round $t$ for the positive/negative class respectively. First, let us introduce the formal definition of this task. Given two sentences ${s}^a = \{{w}_1^a, {w}_2^a, ..., {w}_{\ell_a}^a\}$ and ${s}^b = \{{w}_1^b, {w}_2^b, ..., {w}_{\ell_b}^b\}$, where ${w}_i^a$ and ${w}_j^b$ represent the $i$-th and $j$-th tokens in the sentences, and $\ell_a$ and $\ell_b$ indicate the  length of ${s}^a$ and ${s}^b$. The goal of this task is to learn a discriminator $\mathcal{D}$ to precisely predict the label $y=\mathcal{D}({s}^a, {s}^b)$, where $y \in \mathcal{Y}=\{0, 1\}$ indicates whether the two sentences are similar.

In our task, $\mathcal{G}$ is trained to generate a similar sentence ${s}^b$ from any given sentence ${s}^a$ and $\mathcal{D}$ is trained to predict label $y$ from any given sentence pair $\{{s}^a, {s}^b\}$. As demonstrated in Figure~\ref{framework}, there are mainly two training processes in the entire framework: fix $\mathcal{G}$ to train $\mathcal{D}$ and fix $\mathcal{D}$ to train $\mathcal{G}$. We introduce the two training procedures in detail with the $t$-th round training.

\textbf{Training $\mathcal{D}$}: We first randomly sample $s^a_t$ from domain-related corpus $C$, and then input $s^a_t$ to $\mathcal{G}^t$ to generate $s^b_t$. Next, we feed sentence pair $\{{s}^a_t, {s}^b_t\}$ into $\mathcal{D}^{t-1}$ to predict the label $y_{t-1}$, and filter $\{{s}^a_t, {s}^b_t, y_{t-1}\}$ using threshold $\epsilon_{\mathcal{D}}^{t-1}$. Finally, we train $\mathcal{D}^{t-1}$ on the selected data and pre-training data $P$ to get an improved discriminator $\mathcal{D}^{t}$. Note that the filtered data have both positive and negative samples. The update process of $\mathcal{D}$ seeks to minimize the cross-entropy loss over all instances:

\begin{equation}
\label{train_dis}
\begin{split}
	\mathcal{L}_\mathcal{D}(\boldsymbol{s}, \boldsymbol{y}) = \frac{1}{|\boldsymbol{s}|} \sum_{i=1}^{|\boldsymbol{s}|}-[y_i\cdot \log p_{\mathcal{D}}(y_i=1|s_i^a, s_i^b)\\+(1-y_i)\cdot \log (1-p_{\mathcal{D}}(y_i=1|s_i^a, s_i^b))] 
\end{split}
\end{equation}

\textbf{Training $\mathcal{G}$}: We feed the generated sentence pairs $\{{s}^a_t, {s}^b_t\}$ into $\mathcal{D}^{t}$ to predict new labels $y_{t}$, and then filter $\{{s}^a_t, {s}^b_t, y_{t}\}$ using threshold $\epsilon_\mathcal{G}^t$ and additional rules \footnote{The additional rules are used to exclude sentences which are too long, too short, or too similar according to the longest common substring algorithm.}. Note that the filtered data has only positive samples. For the filtered data, we supplement it with the pre-training data $P$ to update $\mathcal{G}^{t}$ to $\mathcal{G}^{t+1}$ \footnote{Note that the pre-training data $P$ is used to warm up $\mathcal{G}$ and $\mathcal{D}$. Although pre-training data is not mandatory in subsequent training, we empirically found that including it when training $\mathcal{G}$ can prevent language degeneration and improve downstream performances.} We also take out ${s}^b_t$ from the filtered data and add them to the domain-related corpus. The expanded domain corpus are used to sample conditional sentences in the next round of generation. The update procedure of $\mathcal{G}$ employs the negative log-likelihood function over all instances:

\begin{equation}
\nonumber
	\mathcal{L}_\mathcal{G}(\boldsymbol{s^a}, \boldsymbol{s^b}) = -\frac{1}{|\boldsymbol{s^b}|} \sum_{t=1}^{|\boldsymbol{s^b}|}\log p_{\mathcal{G}}(s^b_t|s^b_{<t}, \boldsymbol{s^a})
\end{equation}

For the selection mechanism, we adopt the form $\epsilon^t=m*t+\lambda$ after comparing the effects of different threshold functions through experiments according to Equation~\ref{dyn_thre}, where $m$ is the increment of the threshold for each round, $\lambda$ is the initial threshold, and $\epsilon^t$ is the threshold for rounds $t$.

In the process of training $\mathcal{G}$, since the sentences generated in each round are added to the domain-related corpus, the source of domain-specific data is thus monotonically expanding by iterating the self-consistent learning loop. The formalized process is shown in Algorithm~\ref{scl}. 

% To better understand the first two steps. Specifically, in the first step, we start by selecting pre-trained language models as the initial states for both the generator (G0G^0) and the discriminator (D0D^0). These models have been trained on extensive corpora, such as WuDaoCorpora for Chinese and WikiText for English. However, at this stage, the generator and the discriminator may not excel at generating similar sentences and discerning similarity, because the pre-trained data does not include examples of these specific task format. In the second step, to tackle the aforementioned issue, we preform further pre-training using domain-specific data containing similar sentence pairs. This "warm-up" process endows the initialized PLMs with the preliminary capability to generate similar sentences and discern their similarity.

\begin{algorithm}
\caption{Self-consistent Learning (\textbf{SCL})} 
\label{scl}
\begin{algorithmic}[1]
\REQUIRE Generator $\mathcal{G}$; Discriminator $\mathcal{D}$; Domain-Related Corpus $C$; Pre-training Data $P$.
\STATE Initialize $\mathcal{G}^0$ and $\mathcal{D}^0$ with pre-trained language models;
\STATE Warm-up $\mathcal{G}^0$ and $\mathcal{D}^0$ with pre-training data $P$ to get $\mathcal{G}^1$ and $\mathcal{D}^1$;
\FOR {each round $i \in [1, n]$}
\IF {Two consecutive rounds of discriminator still improve}
\STATE Generate similar sentences $s^b\sim p_{\mathcal{G}^i}(\cdot|s^a)$ from sampled sentences $s^a$ from $C$;
\STATE Predict pseudo-labels $y^i \sim p_{\mathcal{D}^i}(\cdot|s^a, s^b)$;
\STATE Use threshold $\epsilon_\mathcal{D}^i$ to select data on $\{s^a, s^b, y^i\}$ to train $\mathcal{D}^{i+1}$;
\STATE Predict pseudo-labels $y^{i+1} \sim p_{\mathcal{D}^{i+1}}(\cdot|s^a, s^b)$;
\STATE Use threshold $\epsilon_\mathcal{G}^i$ and additional rules to select data on $\{s^a, s^b, y^{i+1}\}$ to train $\mathcal{G}^{i+1}$;
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}