@inproceedings{van2021clinical,
	title        = {Clinical Outcome Prediction from Admission Notes using Self-Supervised Knowledge Integration},
	author       = {Betty van Aken and Jens{-}Michalis Papaioannou and Manuel Mayrdorfer and Klemens Budde and Felix A. Gers and Alexander L{\"{o}}ser},
	year         = 2021,
	booktitle    = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, {EACL} 2021, Online, April 19 - 23, 2021},
	publisher    = {Association for Computational Linguistics},
	pages        = {881--893},
	doi          = {10.18653/v1/2021.eacl-main.75},
	url          = {https://doi.org/10.18653/v1/2021.eacl-main.75},
	editor       = {Paola Merlo and J{\"{o}}rg Tiedemann and Reut Tsarfaty},
	timestamp    = {Thu, 20 Jan 2022 10:02:49 +0100},
	biburl       = {https://dblp.org/rec/conf/eacl/AkenPMBGL21.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{ma2018kame,
	title        = {{KAME:} Knowledge-based Attention Model for Diagnosis Prediction in Healthcare},
	author       = {Fenglong Ma and Quanzeng You and Houping Xiao and Radha Chitta and Jing Zhou and Jing Gao},
	year         = 2018,
	booktitle    = {Proceedings of the 27th {ACM} International Conference on Information and Knowledge Management, {CIKM} 2018, Torino, Italy, October 22-26, 2018},
	publisher    = {{ACM}},
	pages        = {743--752},
	doi          = {10.1145/3269206.3271701},
	url          = {https://doi.org/10.1145/3269206.3271701},
	editor       = {Alfredo Cuzzocrea and James Allan and Norman W. Paton and Divesh Srivastava and Rakesh Agrawal and Andrei Z. Broder and Mohammed J. Zaki and K. Sel{\c{c}}uk Candan and Alexandros Labrinidis and Assaf Schuster and Haixun Wang},
	timestamp    = {Tue, 25 Jun 2019 10:38:05 +0200},
	biburl       = {https://dblp.org/rec/conf/cikm/MaYXCZG18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{qiao2019MNN,
	title        = {{MNN:} Multimodal Attentional Neural Networks for Diagnosis Prediction},
	author       = {Zhi Qiao and Xian Wu and Shen Ge and Wei Fan},
	year         = 2019,
	booktitle    = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, {IJCAI} 2019, Macao, China, August 10-16, 2019},
	publisher    = {ijcai.org},
	pages        = {5937--5943},
	doi          = {10.24963/ijcai.2019/823},
	url          = {https://doi.org/10.24963/ijcai.2019/823},
	editor       = {Sarit Kraus},
	timestamp    = {Tue, 20 Aug 2019 16:18:33 +0200},
	biburl       = {https://dblp.org/rec/conf/ijcai/QiaoWG019.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:conf/emnlp/Perez-MayosBW21,
	title        = {How much pretraining data do language models need to learn syntax?},
	author       = {Laura P{\'{e}}rez{-}Mayos and Miguel Ballesteros and Leo Wanner},
	year         = 2021,
	booktitle    = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021},
	publisher    = {Association for Computational Linguistics},
	pages        = {1571--1582},
	doi          = {10.18653/v1/2021.emnlp-main.118},
	url          = {https://doi.org/10.18653/v1/2021.emnlp-main.118},
	editor       = {Marie{-}Francine Moens and Xuanjing Huang and Lucia Specia and Scott Wen{-}tau Yih},
	timestamp    = {Thu, 20 Jan 2022 10:02:37 +0100},
	biburl       = {https://dblp.org/rec/conf/emnlp/Perez-MayosBW21.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{rust2019howgoodtokenizer,
	title        = {How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models},
	author       = {Phillip Rust and Jonas Pfeiffer and Ivan Vulic and Sebastian Ruder and Iryna Gurevych},
	year         = 2021,
	booktitle    = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, {ACL/IJCNLP} 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021},
	publisher    = {Association for Computational Linguistics},
	pages        = {3118--3135},
	doi          = {10.18653/v1/2021.acl-long.243},
	url          = {https://doi.org/10.18653/v1/2021.acl-long.243},
	editor       = {Chengqing Zong and Fei Xia and Wenjie Li and Roberto Navigli},
	timestamp    = {Mon, 09 Aug 2021 16:25:37 +0200},
	biburl       = {https://dblp.org/rec/conf/acl/RustPVRG20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{denil2018hyperbolicattention,
	title        = {Hyperbolic Attention Networks},
	author       = {{\c{C}}aglar G{\"{u}}l{\c{c}}ehre and Misha Denil and Mateusz Malinowski and Ali Razavi and Razvan Pascanu and Karl Moritz Hermann and Peter W. Battaglia and Victor Bapst and David Raposo and Adam Santoro and Nando de Freitas},
	year         = 2019,
	booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=rJxHsjRqFQ},
	timestamp    = {Thu, 25 Jul 2019 14:25:52 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/GulcehreDMRPHBB19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{vaswani2017attention,
	title        = {Attention is All you Need},
	author       = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
	year         = 2017,
	booktitle    = {Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, {USA}},
	pages        = {5998--6008},
	url          = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	editor       = {Isabelle Guyon and Ulrike von Luxburg and Samy Bengio and Hanna M. Wallach and Rob Fergus and S. V. N. Vishwanathan and Roman Garnett},
	timestamp    = {Thu, 21 Jan 2021 15:15:21 +0100},
	biburl       = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{devlin2019bert,
	title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language Understanding},
	author       = {Jacob Devlin and Ming{-}Wei Chang and Kenton Lee and Kristina Toutanova},
	year         = 2019,
	booktitle    = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)},
	publisher    = {Association for Computational Linguistics},
	pages        = {4171--4186},
	doi          = {10.18653/v1/n19-1423},
	url          = {https://doi.org/10.18653/v1/n19-1423},
	editor       = {Jill Burstein and Christy Doran and Thamar Solorio},
	timestamp    = {Wed, 16 Mar 2022 23:55:36 +0100},
	biburl       = {https://dblp.org/rec/conf/naacl/DevlinCLT19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{adam2015,
	title        = {Adam: {A} Method for Stochastic Optimization},
	author       = {Diederik P. Kingma and Jimmy Ba},
	year         = 2015,
	booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
	url          = {http://arxiv.org/abs/1412.6980},
	editor       = {Yoshua Bengio and Yann LeCun},
	timestamp    = {Thu, 25 Jul 2019 14:25:37 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{gu2021pubmedbert,
	title        = {Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing},
	author       = {Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
	year         = 2021,
	month        = {oct},
	journal      = {ACM Trans. Comput. Healthcare},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3458754},
	issn         = {2691-1957},
	url          = {https://doi.org/10.1145/3458754},
	issue_date   = {January 2022},
	abstract     = {Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this article, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition. To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding &amp; Reasoning Benchmark) at .},
	articleno    = 2,
	numpages     = 23,
	keywords     = {Biomedical, NLP, domain-specific pretraining}
}
@inproceedings{lipton16learning,
	title        = {Learning to Diagnose with {LSTM} Recurrent Neural Networks},
	author       = {Zachary Chase Lipton and David C. Kale and Charles Elkan and Randall C. Wetzel},
	year         = 2016,
	booktitle    = {4th International Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
	url          = {http://arxiv.org/abs/1511.03677},
	editor       = {Yoshua Bengio and Yann LeCun},
	timestamp    = {Thu, 25 Jul 2019 14:25:38 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/LiptonKEW15.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{choi16retain,
	title        = {{RETAIN:} An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism},
	author       = {Edward Choi and Mohammad Taha Bahadori and Jimeng Sun and Joshua Kulas and Andy Schuetz and Walter F. Stewart},
	year         = 2016,
	booktitle    = {Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain},
	pages        = {3504--3512},
	url          = {https://proceedings.neurips.cc/paper/2016/hash/231141b34c82aa95e48810a9d1b33a79-Abstract.html},
	editor       = {Daniel D. Lee and Masashi Sugiyama and Ulrike von Luxburg and Isabelle Guyon and Roman Garnett},
	timestamp    = {Thu, 21 Jan 2021 15:15:22 +0100},
	biburl       = {https://dblp.org/rec/conf/nips/ChoiBSKSS16.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{ma17dipole,
	title        = {Dipole: Diagnosis Prediction in Healthcare via Attention-based Bidirectional Recurrent Neural Networks},
	author       = {Fenglong Ma and Radha Chitta and Jing Zhou and Quanzeng You and Tong Sun and Jing Gao},
	year         = 2017,
	booktitle    = {Proceedings of the 23rd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining, Halifax, NS, Canada, August 13 - 17, 2017},
	publisher    = {{ACM}},
	pages        = {1903--1911},
	doi          = {10.1145/3097983.3098088},
	url          = {https://doi.org/10.1145/3097983.3098088},
	timestamp    = {Fri, 25 Dec 2020 01:14:16 +0100},
	biburl       = {https://dblp.org/rec/conf/kdd/MaCZYSG17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{boag2018s,
	title        = {What’s in a note? unpacking predictive value in clinical note representations},
	author       = {Boag, Willie and Doss, Dustin and Naumann, Tristan and Szolovits, Peter},
	year         = 2018,
	journal      = {AMIA Summits on Translational Science Proceedings},
	publisher    = {American Medical Informatics Association},
	volume       = 2018,
	pages        = 26
}
@inproceedings{liu2018deep,
	title        = {Deep {EHR:} Chronic Disease Prediction Using Medical Notes},
	author       = {Jingshu Liu and Zachariah Zhang and Narges Razavian},
	year         = 2018,
	booktitle    = {Proceedings of the Machine Learning for Healthcare Conference, {MLHC} 2018, 17-18 August 2018, Palo Alto, California},
	publisher    = {{PMLR}},
	series       = {Proceedings of Machine Learning Research},
	volume       = 85,
	pages        = {440--464},
	url          = {http://proceedings.mlr.press/v85/liu18b.html},
	editor       = {Finale Doshi{-}Velez and Jim Fackler and Ken Jung and David C. Kale and Rajesh Ranganath and Byron C. Wallace and Jenna Wiens},
	timestamp    = {Wed, 03 Apr 2019 18:17:31 +0200},
	biburl       = {https://dblp.org/rec/conf/mlhc/LiuZR18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{peng2020self,
	title        = {Self-attention Enhanced Patient Journey Understanding in Healthcare System},
	author       = {Xueping Peng and Guodong Long and Tao Shen and Sen Wang and Jing Jiang},
	year         = 2020,
	booktitle    = {Machine Learning and Knowledge Discovery in Databases - European Conference, {ECML} {PKDD} 2020, Ghent, Belgium, September 14-18, 2020, Proceedings, Part {III}},
	publisher    = {Springer},
	series       = {Lecture Notes in Computer Science},
	volume       = 12459,
	pages        = {719--735},
	doi          = {10.1007/978-3-030-67664-3\_43},
	url          = {https://doi.org/10.1007/978-3-030-67664-3\_43},
	editor       = {Frank Hutter and Kristian Kersting and Jefrey Lijffijt and Isabel Valera},
	timestamp    = {Sun, 25 Jul 2021 11:47:49 +0200},
	biburl       = {https://dblp.org/rec/conf/pkdd/PengLS0020.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{sushil2018patient,
	title        = {Patient representation learning and interpretable evaluation using clinical notes},
	author       = {Sushil, Madhumita and {\v{S}}uster, Simon and Luyckx, Kim and Daelemans, Walter},
	year         = 2018,
	journal      = {Journal of biomedical informatics},
	publisher    = {Elsevier},
	volume       = 84,
	pages        = {103--113}
}
@article{johnson2016mimic,
	title        = {MIMIC-III, a freely accessible critical care database},
	author       = {Johnson, Alistair EW and Pollard, Tom J and Shen, Lu and Lehman, Li-wei H and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Anthony Celi, Leo and Mark, Roger G},
	year         = 2016,
	journal      = {Scientific data},
	publisher    = {Nature Publishing Group},
	volume       = 3,
	number       = 1,
	pages        = {1--9}
}
@article{bodenreider2004unified,
	title        = {The unified medical language system (UMLS): integrating biomedical terminology},
	author       = {Bodenreider, Olivier},
	year         = 2004,
	journal      = {Nucleic acids research},
	publisher    = {Oxford University Press},
	volume       = 32,
	number       = {suppl\_1},
	pages        = {D267--D270}
}
@article{choi2017generating,
	title        = {Generating Multi-label Discrete Electronic Health Records using Generative Adversarial Networks},
	author       = {Edward Choi and Siddharth Biswal and Bradley A. Malin and Jon Duke and Walter F. Stewart and Jimeng Sun},
	year         = 2017,
	journal      = {CoRR},
	volume       = {abs/1703.06490},
	url          = {http://arxiv.org/abs/1703.06490},
	eprinttype   = {arXiv},
	eprint       = {1703.06490},
	timestamp    = {Fri, 04 Oct 2019 15:59:46 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/ChoiBMDSS17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{miotto2016deep,
	title        = {Deep patient: an unsupervised representation to predict the future of patients from the electronic health records},
	author       = {Miotto, Riccardo and Li, Li and Kidd, Brian A and Dudley, Joel T},
	year         = 2016,
	journal      = {Scientific reports},
	publisher    = {Nature Publishing Group},
	volume       = 6,
	number       = 1,
	pages        = {1--10}
}
@article{Topol2019HighperformanceMT,
	title        = {High-performance medicine: the convergence of human and artificial intelligence},
	author       = {Eric J. Topol},
	year         = 2019,
	journal      = {Nature Medicine},
	volume       = 25,
	pages        = {44--56}
}
@article{Esteva2021DeepLM,
	title        = {Deep learning-enabled medical computer vision},
	author       = {Andre Esteva and Katherine Chou and Serena Yeung and Nikhil Naik and Ali Madani and Ali Mottaghi and Yun Liu and Eric J. Topol and Jeff Dean and Richard Socher},
	year         = 2021,
	journal      = {NPJ Digital Medicine},
	volume       = 4
}
@inproceedings{Yang2021HowTL,
	title        = {How to leverage the multimodal {EHR} data for better medical prediction?},
	author       = {Bo Yang and Lijun Wu},
	year         = 2021,
	booktitle    = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021},
	publisher    = {Association for Computational Linguistics},
	pages        = {4029--4038},
	doi          = {10.18653/v1/2021.emnlp-main.329},
	url          = {https://doi.org/10.18653/v1/2021.emnlp-main.329},
	editor       = {Marie{-}Francine Moens and Xuanjing Huang and Lucia Specia and Scott Wen{-}tau Yih},
	timestamp    = {Thu, 20 Jan 2022 10:02:06 +0100},
	biburl       = {https://dblp.org/rec/conf/emnlp/YangW21.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Alsentzer2019PubliclyAC,
	title        = {Publicly Available Clinical {BERT} Embeddings},
	author       = {Emily Alsentzer and John R. Murphy and Willie Boag and Wei{-}Hung Weng and Di Jin and Tristan Naumann and Matthew B. A. McDermott},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1904.03323},
	url          = {http://arxiv.org/abs/1904.03323},
	eprinttype   = {arXiv},
	eprint       = {1904.03323},
	timestamp    = {Wed, 24 Apr 2019 12:21:25 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1904-03323.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{hashir-mortality,
	title        = {{Towards unstructured mortality prediction with free-text clinical notes}},
	author       = {Mohammad Hashir and Rapinder Sawhney},
	year         = 2020,
	journal      = {Journal of Biomedical Informatics},
	volume       = 108,
	pages        = 103489,
	timestamp    = {Fri, 02 Oct 2020 16:25:26 +0200},
	biburl       = {https://dblp.org/rec/journals/jbi/HashirS20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{han-medical,
	title        = {Patient representation learning and interpretable evaluation using clinical notes},
	author       = {Madhumita Sushil and Simon Suster and Kim Luyckx and Walter Daelemans},
	year         = 2018,
	journal      = {J. Biomed. Informatics},
	volume       = 84,
	pages        = {103--113},
	doi          = {10.1016/j.jbi.2018.06.016},
	url          = {https://doi.org/10.1016/j.jbi.2018.06.016},
	timestamp    = {Thu, 14 Oct 2021 08:52:38 +0200},
	biburl       = {https://dblp.org/rec/journals/jbi/SushilSLD18.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{winter-EtAl:2022:LREC,
	title        = {{KIMERA}: Injecting Domain Knowledge into Vacant Transformer Heads},
	author       = {Winter, Benjamin  and  Rosero, Alexei Figueroa  and  LÃ¶ser, Alexander  and  Gers, Felix Alexander  and  Siu, Amy},
	year         = 2022,
	month        = {June},
	booktitle    = {Proceedings of the Language Resources and Evaluation Conference},
	publisher    = {European Language Resources Association},
	address      = {Marseille, France},
	pages        = {363--373},
	url          = {https://aclanthology.org/2022.lrec-1.38},
	abstract     = {Training transformer language models requires vast amounts of text and computational resources. This drastically limits the usage of these models in niche domains for which they are not optimized, or where domain-specific training data is scarce. We focus here on the clinical domain because of its limited access to training data in common tasks, while structured ontological data is often readily available. Recent observations in model compression of transformer models show optimization potential in improving the representation capacity of attention heads. We propose KIMERA (Knowledge Injection via Mask Enforced Retraining of Attention) for detecting, retraining and instilling attention heads with complementary structured domain knowledge. Our novel multi-task training scheme effectively identifies and targets individual attention heads that are least useful for a given downstream task and optimizes their representation with information from structured data. KIMERA generalizes well, thereby building the basis for an efficient fine-tuning. KIMERA achieves significant performance boosts on seven datasets in the medical domain in Information Retrieval and Clinical Outcome Prediction settings. We apply KIMERA to BERT-base to evaluate the extent of the domain transfer and also improve on the already strong results of BioBERT in the clinical domain.}
}
@inproceedings{papaioannou-EtAl:2022:LREC,
	title        = {Cross-Lingual Knowledge Transfer for Clinical Phenotyping},
	author       = {Papaioannou, Jens-Michalis  and  Grundmann, Paul  and  van Aken, Betty  and  Samaras, Athanasios  and  Kyparissidis, Ilias  and  Giannakoulas, George  and  Gers, Felix  and  Loeser, Alexander},
	year         = 2022,
	month        = {June},
	booktitle    = {Proceedings of the Language Resources and Evaluation Conference},
	publisher    = {European Language Resources Association},
	address      = {Marseille, France},
	pages        = {900--909},
	url          = {https://aclanthology.org/2022.lrec-1.95},
	abstract     = {Clinical phenotyping enables the automatic extraction of clinical conditions from patient records, which can be beneficial to doctors and clinics worldwide. However, current state-of-the-art models are mostly applicable to clinical notes written in English. We therefore investigate cross-lingual knowledge transfer strategies to execute this task for clinics that do not use the English language and have a small amount of in-domain data available. Our results reveal two strategies that outperform the state-of-the-art: Translation-based methods in combination with domain-specific encoders and cross-lingual encoders plus adapters. We find that these strategies perform especially well for classifying rare phenotypes and we advise on which method to prefer in which situation. Our results show that using multilingual data overall improves clinical phenotyping models and can compensate for data sparseness.}
}
@article{cai2020hybrid,
	title        = {A Hybrid {BERT} Model That Incorporates Label Semantics via Adjustive Attention for Multi-Label Text Classification},
	author       = {Linkun Cai and Yu Song and Tao Liu and Kunli Zhang},
	year         = 2020,
	journal      = {{IEEE} Access},
	volume       = 8,
	pages        = {152183--152192},
	doi          = {10.1109/ACCESS.2020.3017382},
	url          = {https://doi.org/10.1109/ACCESS.2020.3017382},
	timestamp    = {Sat, 19 Sep 2020 13:18:30 +0200},
	biburl       = {https://dblp.org/rec/journals/access/CaiSLZ20.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{kim2017structured-attention,
	title        = {Structured Attention Networks},
	author       = {Yoon Kim and Carl Denton and Luong Hoang and Alexander M. Rush},
	year         = 2017,
	booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
	publisher    = {OpenReview.net},
	url          = {https://openreview.net/forum?id=HkE0Nvqlg},
	timestamp    = {Sat, 09 Apr 2022 12:37:56 +0200},
	biburl       = {https://dblp.org/rec/conf/iclr/KimDHR17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{bahdanau2015neuraltranslation,
	title        = {Neural Machine Translation by Jointly Learning to Align and Translate},
	author       = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
	year         = 2015,
	booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
	url          = {http://arxiv.org/abs/1409.0473},
	editor       = {Yoshua Bengio and Yann LeCun},
	timestamp    = {Wed, 17 Jul 2019 10:40:54 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{lapuschkin2019unmasking,
	title        = {Unmasking Clever Hans predictors and assessing what machines really learn},
	author       = {Lapuschkin, Sebastian and W{\"a}ldchen, Stephan and Binder, Alexander and Montavon, Gr{\'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
	year         = 2019,
	journal      = {Nature communications},
	publisher    = {Nature Publishing Group},
	volume       = 10,
	number       = 1,
	pages        = {1--8}
}
@inproceedings{liu2020kbert,
	title        = {{K-BERT:} Enabling Language Representation with Knowledge Graph},
	author       = {Weijie Liu and Peng Zhou and Zhe Zhao and Zhiruo Wang and Qi Ju and Haotang Deng and Ping Wang},
	year         = 2020,
	booktitle    = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA, February 7-12, 2020},
	publisher    = {{AAAI} Press},
	pages        = {2901--2908},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/5681},
	timestamp    = {Mon, 07 Mar 2022 16:58:18 +0100},
	biburl       = {https://dblp.org/rec/conf/aaai/LiuZ0WJD020.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{scheible2020gottbert,
	title        = {{GottBERT}: a pure {German} language model},
	author       = {Scheible, Raphael and Thomczyk, Fabian and Tippmann, Patric and Jaravine, Victor and Boeker, Martin},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2012.02110}
}
@article{martin2019camembert,
	title        = {{CamemBERT}: a tasty French language model},
	author       = {Martin, Louis and Muller, Benjamin and Su{\'a}rez, Pedro Javier Ortiz and Dupont, Yoann and Romary, Laurent and de La Clergerie, {\'E}ric Villemonte and Seddah, Djam{\'e} and Sagot, Beno{\^\i}t},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1911.03894}
}
@article{canete2020spanish,
	title        = {Spanish pre-trained bert model and evaluation data},
	author       = {Canete, Jos{\'e} and Chaperon, Gabriel and Fuentes, Rodrigo and Ho, Jou-Hui and Kang, Hojin and P{\'e}rez, Jorge},
	year         = 2020,
	journal      = {Pml4dc at iclr},
	volume       = 2020,
	pages        = {1--10}
}
@article{gu2021domain,
	title        = {Domain-specific language model pretraining for biomedical natural language processing},
	author       = {Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
	year         = 2021,
	journal      = {ACM Transactions on Computing for Healthcare (HEALTH)},
	publisher    = {ACM New York, NY},
	volume       = 3,
	number       = 1,
	pages        = {1--23}
}
@article{lee2020biobert,
	title        = {{BioBERT}: a pre-trained biomedical language representation model for biomedical text mining},
	author       = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
	year         = 2020,
	journal      = {Bioinformatics},
	publisher    = {Oxford University Press},
	volume       = 36,
	number       = 4,
	pages        = {1234--1240}
}
@article{rasmy2021med,
	title        = {{Med-BERT}: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction},
	author       = {Rasmy, Laila and Xiang, Yang and Xie, Ziqian and Tao, Cui and Zhi, Degui},
	year         = 2021,
	journal      = {NPJ digital medicine},
	publisher    = {Nature Publishing Group},
	volume       = 4,
	number       = 1,
	pages        = {1--13}
}
@article{lentzen2022critical,
	title        = {{Critical assessment of transformer-based AI models for German clinical notes}},
	author       = {Lentzen, Manuel and Madan, Sumit and Lage-Rupprecht, Vanessa and Kühnel, Lisa and Fluck, Juliane and Jacobs, Marc and Mittermaier, Mirja and Witzenrath, Martin and Brunecker, Peter and Hofmann-Apitius, Martin and Weber, Joachim and Fröhlich, Holger},
	year         = 2022,
	month        = 11,
	journal      = {JAMIA Open},
	volume       = 5,
	number       = 4,
	doi          = {10.1093/jamiaopen/ooac087},
	issn         = {2574-2531},
	url          = {https://doi.org/10.1093/jamiaopen/ooac087},
	note         = {ooac087},
	abstract     = {{Healthcare data such as clinical notes are primarily recorded in an unstructured manner. If adequately translated into structured data, they can be utilized for health economics and set the groundwork for better individualized patient care. To structure clinical notes, deep-learning methods, particularly transformer-based models like Bidirectional Encoder Representations from Transformers (BERT), have recently received much attention. Currently, biomedical applications are primarily focused on the English language. While general-purpose German-language models such as GermanBERT and GottBERT have been published, adaptations for biomedical data are unavailable. This study evaluated the suitability of existing and novel transformer-based models for the German biomedical and clinical domain.We used 8 transformer-based models and pre-trained 3 new models on a newly generated biomedical corpus, and systematically compared them with each other. We annotated a new dataset of clinical notes and used it with 4 other corpora (BRONCO150, CLEF eHealth 2019 Task 1, GGPONC, and JSynCC) to perform named entity recognition (NER) and document classification tasks.General-purpose language models can be used effectively for biomedical and clinical natural language processing (NLP) tasks, still, our newly trained BioGottBERT model outperformed GottBERT on both clinical NER tasks. However, training new biomedical models from scratch proved ineffective.The domain-adaptation strategy’s potential is currently limited due to a lack of pre-training data. Since general-purpose language models are only marginally inferior to domain-specific models, both options are suitable for developing German-language biomedical applications.General-purpose language models perform remarkably well on biomedical and clinical NLP tasks. If larger corpora become available in the future, domain-adapting these models may improve performances.In 2022, the majority of clinical documents are still written as free text. Assuming that these records are consistently and correctly transformed into structured data, they present an opportunity for optimized health-economic purposes as well as personalized patient care. Deep-learning methods, particularly transformer-based models, have recently received much attention as they excel in a variety of fields; however, the majority of applications are currently only available in English. Although there are general-language models in German, none have been developed specifically for biomedical or clinical documents. In this context, this study systematically compared 8 previously published general-language models and 3 newly trained biomedical domain models in information extraction and document classification tasks. Our findings show that while training entirely new models with currently available data has proven ineffective, adapting existing models for biomedical language holds a lot of promise. Furthermore, we found out that even models that have not been specifically developed for biomedical applications can achieve excellent results in the specified fields.}},
	eprint       = {https://academic.oup.com/jamiaopen/article-pdf/5/4/ooac087/47042093/ooac087.pdf}
}

@article{bressem2020highly,
	title        = {Highly accurate classification of chest radiographic reports using a deep learning natural language model pre-trained on 3.8 million text reports},
	author       = {Bressem, Keno K and Adams, Lisa C and Gaudin, Robert A and Tr{\"o}ltzsch, Daniel and Hamm, Bernd and Makowski, Marcus R and Sch{\"u}le, Chan-Yong and Vahldiek, Janis L and Niehues, Stefan M},
	year         = 2020,
	journal      = {Bioinformatics},
	publisher    = {Oxford University Press},
	volume       = 36,
	number       = 21,
	pages        = {5255--5261}
}
@article{frei2022gernermed,
	title        = {{GERNERMED}: An open {German} medical NER model},
	author       = {Frei, Johann and Kramer, Frank},
	year         = 2022,
	journal      = {Software Impacts},
	publisher    = {Elsevier},
	volume       = 11,
	pages        = 100212
}
@inproceedings{borchert2022ggponc,
	title        = {{GGPONC} 2.0-the {German} clinical guideline corpus for oncology: Curation workflow, annotation policy, baseline NER taggers},
	author       = {Borchert, Florian and Lohr, Christina and Modersohn, Luise and Witt, Jonas and Langer, Thomas and Follmann, Markus and Gietzelt, Matthias and Arnrich, Bert and Hahn, Udo and Schapranow, Matthieu-P},
	year         = 2022,
	booktitle    = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},
	pages        = {3650--3660}
}
@inproceedings{borchert-etal-2020-ggponc,
	title        = {{GGPONC}: A Corpus of {G}erman Medical Text with Rich Metadata Based on Clinical Practice Guidelines},
	author       = {Borchert, Florian  and Lohr, Christina  and Modersohn, Luise  and Langer, Thomas  and Follmann, Markus  and Sachs, Jan Philipp  and Hahn, Udo  and Schapranow, Matthieu-P.},
	year         = 2020,
	month        = nov,
	booktitle    = {Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {38--48},
	doi          = {10.18653/v1/2020.louhi-1.5},
	url          = {https://aclanthology.org/2020.louhi-1.5}
}
@mastersthesis{Shrestha2021,
	title        = {Development of a Language Model for Medical Domain},
	author       = {Manjil Shrestha},
	year         = 2021,
	pages        = 141,
	type         = {masterthesis},
	school       = {Hochschule Rhein-Waal}
}
@article{lee2021deduplicating,
	title        = {Deduplicating training data makes language models better},
	author       = {Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2107.06499}
}
@article{you2019lamb,
	title        = {Large batch optimization for deep learning: Training bert in 76 minutes},
	author       = {You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1904.00962}
}
@article{devlin2018pretraining,
	title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language Understanding},
	author       = {Jacob Devlin and Ming{-}Wei Chang and Kenton Lee and Kristina Toutanova},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1810.04805},
	url          = {http://arxiv.org/abs/1810.04805},
	archiveprefix = {arXiv},
	eprint       = {1810.04805},
	timestamp    = {Tue, 30 Oct 2018 20:39:56 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{liu2019roberta,
	title        = {Roberta: A robustly optimized bert pretraining approach},
	author       = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1907.11692}
}
@inproceedings{germeval18,
	title        = {Fine-Grained Classification of Offensive Language},
	author       = {Risch, Julian and Krebs, Eva and Löser, Alexander and Riese, Alexander and Krestel, Ralf},
	year         = 2018,
	month        = {September 21st},
	booktitle    = {Proceedings of GermEval 2018 (co-located with KONVENS)},
	pages        = {38--44}
}
@article{henry20202018,
	title        = {2018 n2c2 shared task on adverse drug events and medication extraction in electronic health records},
	author       = {Henry, Sam and Buchan, Kevin and Filannino, Michele and Stubbs, Amber and Uzuner, Ozlem},
	year         = 2020,
	journal      = {Journal of the American Medical Informatics Association},
	publisher    = {Oxford University Press},
	volume       = 27,
	number       = 1,
	pages        = {3--12}
}

@inproceedings{deepset2019,
	title = "{G}erman{'}s Next Language Model",
    author = {Chan, Branden  and
      Schweter, Stefan  and
      M{\"o}ller, Timo},
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.598",
    doi = "10.18653/v1/2020.coling-main.598",
    pages = "6788--6796",
    abstract = "In this work we present the experiments which lead to the creation of our BERT and ELECTRA based German language models, GBERT and GELECTRA. By varying the input training data, model size, and the presence of Whole Word Masking (WWM) we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size. We adopt an evaluation driven approach in training these models and our results indicate that both adding more data and utilizing WWM improve model performance. By benchmarking against existing German models, we show that these models are the best German models to date. All trained models will be made publicly available to the research community.",
}

}
@misc{grassco2022,
	title        = {{GraSCCo}},
	author       = {Stefan Schulz},
	year         = 2022,
	month        = may,
	publisher    = {Zenodo},
	doi          = {10.5281/zenodo.6539131},
	url          = {https://doi.org/10.5281/zenodo.6539131},
	version      = {v1}
}
@article{modersohn2022grascco,
	title        = {{GRASCCO}-The First Publicly Shareable, Multiply-Alienated {German} Clinical Text Corpus},
	author       = {Modersohn, Luise and Schulz, Stefan and Lohr, Christina and Hahn, Udo},
	year         = 2022,
	journal      = {Studies in health technology and informatics},
	volume       = 296,
	pages        = {66--72}
}
@inproceedings{akbik2018coling,
	title        = {Contextual String Embeddings for Sequence Labeling},
	author       = {Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},
	year         = 2018,
	booktitle    = {{COLING} 2018, 27th International Conference on Computational Linguistics},
	pages        = {1638--1649}
}
@inproceedings{dedup,
	title        = {Deduplicating Training Data Makes Language Models Better},
	author       = {Katherine Lee and Daphne Ippolito and Andrew Nystrom and Chiyuan Zhang and Douglas Eck and Chris Callison{-}Burch and Nicholas Carlini},
	year         = 2022,
	booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland, May 22-27, 2022},
	publisher    = {Association for Computational Linguistics},
	pages        = {8424--8445},
	doi          = {10.18653/v1/2022.acl-long.577},
	url          = {https://doi.org/10.18653/v1/2022.acl-long.577},
	editor       = {Smaranda Muresan and Preslav Nakov and Aline Villavicencio},
	timestamp    = {Mon, 01 Aug 2022 16:27:46 +0200},
	biburl       = {https://dblp.org/rec/conf/acl/LeeINZECC22.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{schmidt2021tbase,
  title={{TBase}-an integrated electronic health record and research database for kidney transplant recipients},
  author={Schmidt, Danilo and Osmanodja, Bilgin and Pfefferkorn, Matthias and Graf, Verena and Raschke, Dirk and Duettmann, Wiebke and Naik, Marcel G and Gethmann, Carolin J and Mayrdorfer, Manuel and Halleck, Fabian and others},
  journal={JoVE (Journal of Visualized Experiments)},
  number={1},
  pages={e61971},
  year={2021},
  volume={170}
}
