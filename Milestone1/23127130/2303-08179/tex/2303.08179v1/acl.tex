%
% File coling2020.tex
%
% Contact: feiliu@cs.ucf.edu & liang.huang.sh@gmail.com
%% Based on the style files for COLING-2018, which were, in turn,
%% Based on the style files for COLING-2016, which were, in turn,
%% Based on the style files for COLING-2014, which were, in turn,
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{arxiv}
\usepackage{url}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{todonotes}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{array}
\usepackage{hyperref}
\usepackage{makecell}
\usepackage{rotating}
\usepackage{longtable}
%\usepackage{natbib}
\usepackage{authblk}
\usepackage{doi}


% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.


% Remove the "review" option to generate the final version.
%\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\title{medBERT.de: A Comprehensive German BERT Model for the Medical Domain}


\author[1,2,3,9,*]{Keno K. Bressem}
\author[4,*]{Jens-Michalis Papaioannou}
\author[4,*]{Paul Grundmann}
\author[5]{Florian Borchert}
\author[6,7]{Lisa C. Adams}
\author[4]{Leonhard Liu}
\author[2]{Felix Busch}
\author[2]{Lina Xu}
\author[2]{Jan P. Loyen}
\author[2]{Stefan M. Niehues}
\author[8]{Moritz Augustin}
\author[8]{Lennart Grosser}
\author[7]{Marcus R. Makowski}
\author[1,9,10]{Hugo JWL. Aerts}
\author[4]{Alexander Löser}


\affil[1]{Artificial Intelligence in Medicine (AIM) Program, Mass General Brigham, Harvard Medical School, Boston, MA, USA,  \authorcr Email: \tt \{kbressem, HAerts\}@bwh.harvard.edu \vspace{0.3cm}} 
\affil[2]{Charité – Universitätsmedizin Berlin, corporate member of Freie Universität Berlin and Humboldt-Universität zu Berlin, Institute for Radiology, Berlin, Germany  \vspace{0.3cm}}
\affil[3]{Berlin Institute of Health at Charité – Universitätsmedizin Berlin, Germany \vspace{0.3cm}}
\affil[4]{Berliner Hochschule für Technik (BHT), Berlin, Germany \authorcr Email:  \tt \{michalis.papaioannou, pgrundmann, leonhard.liu, aloeser\}@bht-berlin.de \vspace{0.3cm}}
\affil[5]{Digital Health Center, Hasso Plattner Institute, University of Potsdam, Germany, \authorcr Email: \tt \{florian.borchert\}@hpi.de \vspace{0.3cm}}
\affil[6]{Department of Radiology, Stanford University School of Medicine, Stanford, CA, USA \authorcr Email: \tt \{lcadams\}@stanford.edu \vspace{0.3cm}}
\affil[7]{Department of Diagnostic and Interventional Radiology, School of Medicine and Klinikum Rechts der Isar, Technical University of Munich, Munich, Germany, \authorcr Email: \tt \{marcus.makowski\}@tum.de  \vspace{0.3cm}}
\affil[8]{Tiplu GmbH, Hamburg, Germany \authorcr Email: \tt \{m.augustin, l.grosser\}@tiplu.de \vspace{0.6cm}}
\affil[9]{Departments of Radiation Oncology and Radiology, Dana-Farber Cancer Institute and Brigham and Women’s Hospital, Boston, MA, USA \vspace{0.3cm}}
\affil[10]{Radiology and Nuclear Medicine, CARIM \& GROW, Maastricht University, Maastricht, the Netherlands \vspace{0.3cm}}
\affil[*]{Contributed equally}


\begin{document}

\maketitle

\begin{abstract}
This paper presents medBERT.de, a pre-trained German BERT model specifically designed for the German medical domain. The model has been trained on a large corpus of 4.7 Million German medical documents and has been shown to achieve new state-of-the-art performance on eight different medical benchmarks covering a wide range of disciplines and medical document types. In addition to evaluating the overall performance of the model, this paper also conducts a more in-depth analysis of its capabilities. We investigate the impact of data deduplication on the model's performance, as well as the potential benefits of using more efficient tokenization methods. Our results indicate that domain-specific models such as medBERT.de are particularly useful for longer texts, and that deduplication of training data does not necessarily lead to improved performance. Furthermore, we found that efficient tokenization plays only a minor role in improving model performance, and attribute most of the improved performance to the large amount of training data. To encourage further research, the pre-trained model weights and new benchmarks based on radiological data are made publicly available for use by the scientific community.

\end{abstract}

\section{Introduction} % (400 words)

Self-supervised pre-training has become a popular approach in natural language processing (NLP) because it allows the creation of high-performance language models. By training a model on a large corpus of text, the model can learn useful representations of the language. However, the effectiveness of these representations is closely related to the type of data used for pre-training. 
When applied to a different type of text, such as a different language, the model may not perform as well, requiring the development of specialized models. Consequently, language-specific models, such as those for German \cite{scheible2020gottbert}, French \cite{martin2019camembert}, or Spanish \cite{canete2020spanish}, have been developed and have been successful in improving performance for these languages. However, even within a language, technical languages can be very different from spoken language, necessitating the development of domain-specific models \cite{gu2021domain}. Medical terminology is a prime example. 

Medical language models that are specifically trained to process and structure medical text have the potential to greatly improve the efficiency and accuracy of medical document analysis. 
However, the challenge in training such models lies in the limited availability of relevant text data, especially for languages other than English. In addition, the sensitive nature of medical information often limits the generation of large medical text corpora. 
Despite these challenges, the development of medical language models is highly desirable, as they could help process and structure the vast amounts of text generated daily in hospitals.

For English, specialized models for biomedical language processing have already been developed \cite{gu2021domain,lee2020biobert}. \cite{lee2020biobert} proposed BioBERT, a version of the BERT model trained on English biomedical abstracts, which outperforms previous models on biomedical benchmarks while maintaining the performance of the original BERT model \cite{devlin2019bert}. Med-BERT, proposed by\cite{rasmy2021med}, is the first model to be fully trained using hospital data, specifically semi-structured EHRs, resulting in improved performance for downstream prediction models. For non-English languages, the development of specialized 
models is more difficult due to less available data. Nevertheless, previous work on German medical models has demonstrated the potential of using specialized models for biomedical language processing \cite{lentzen2022critical}, \cite{frei2022gernermed}, \cite{bressem2020highly}. For example, BioGottBERT, \cite{lentzen2022critical}, which is trained on open medical German texts from Wikipedia and scientific abstracts, could outperform its generalized counterpart \cite{scheible2020gottbert} on medical tasks. However, these models often suffer from limitations such as limited training data, narrow focus (i.e., focused on only one medical subspecialty), or unrepresentative benchmarks, which limit their comparability.

To address these challenges, our goal is to build a comprehensive German clinical language model - \emph{medBERT.de} - that is trained on a diverse set of medical texts, including scientific texts, medical books, and hospital data from various medical domains. Furthermore, by providing openly available benchmarks using realistic hospital data, we aim to enable the reproducibility of our results and facilitate comparison with other models.

\section{Material and Methods} % (800 words, max. 2 Seiten)

This study was approved by the local ethics committees of \emph{Charité - Universitätsmedizin Berlin} (EA2/078/22). In accordance with local laws and regulations, written informed consent was not required due to the retrospective design. A total of 4.7 million documents from 11 different sources were included, representing 10 GB of raw data. For details, see table \ref{table:datasources} or appendix 
\ref{Appendix:Data Sources}. 

\input{data}

\subsection{Data Annotation and Benchmarking}

\subsubsection{Radiology Benchmarks}
Three medical benchmarks, each based on 2000 radiology reports, were created from radiology reports. Patients that undergo radiological examinations often have a variety of different conditions. For this reason, radiology reports capture a greater diversity of information than, e.g., electronic health records that stem from a single medical field. In this study, we focus on three tasks. The first task is a classification task based on short text reports from chest X-rays. The second task, also a classification task, is based on longer reports of CT examinations. The third task is a named entity recognition task (NER), that is based on medium-sized reports of CT/X-ray examinations of the wrist. All of these reports were obtained from a large, level 1 hospital in Germany and cover a wide range of bone, lung, heart, and vascular diseases. 

For the first medical benchmark, a multi-label text classification task, three board-certified radiologists (KKB, LCA, SMN) manually labeled 2000 reports for the global presence/absence of four pathologies and four different types of therapy devices. The second benchmark, a text classification task, contained 2000 reports of computed tomography (CT) scans. All reports had to include the chest but could include additional body parts (head, neck, abdomen).  All studies were labeled by a final-year medical student (LX) for the presence/absence of 23 different chest pathologies and were then reviewed by two board-certified radiologists (LCA, KKB). 
For the third benchmark, a named entity recognition task, 2000 reports were labeled by a final-year medical student (JPL) for the presence/absence of 42 labels. The labels were then verified by two board-certified radiologists (LCA, KKB). Details of each label are provided in the appendix. \ref{appendix:labels}
We make these benchmarks openly available at \url{https://doi.org/10.5281/zenodo.7574287}

\subsubsection{Private Medical Benchmarks}
A significant proportion of radiology reports in the training corpus can lead to overfitting, hindering the model's ability to generalize to new clinical scenarios. Therefore, the benchmarks based on radiological texts alone may not be sufficient to assess how well the models would perform on new types of clinical data. To mitigate this, we developed benchmarks using new, unseen clinical records, namely surgical reports and discharge summaries. Both types of clinical records were not previously included in the training data. The benchmarks were multi-label classification tasks, where the model had to predict multiple diagnoses (ICD-10) or procedure (OPS) codes from the text, allowing a thorough evaluation of its performance in real-world situations. 
Discharge notes and operative reports are more challenging benchmarks than radiology reports because they tend to be longer and more complex. In addition, the information contained in discharge summaries and operative reports differs from that found in radiology reports. While radiology reports focus primarily on describing anatomy, pathological findings, and formulating a diagnosis, discharge notes and operative reports provide a broader context and include information about the patient's condition, treatment, medications, and follow-up plans.

Discharge summaries, surgery reports, as well as all ICD-10 and OPS codes were extracted from the hospital information system of the same level I hospital as before. For these tasks, no manual labeling was performed. Instead, we assigned to the surgery reports as labels all OPS codes of the same patient that matched the date of the text document. Furthermore, we restricted the codes to the surgery chapter of the OPS 
 system \footnote{\url{https://www.dimdi.de/static/de/klassifikationen/ops/kode-suche/opshtml2022/chapter-5.htm}}. For the discharge summaries, we assigned all codes (in one task diagnoses as ICD-10, in another task procedures as OPS) of the patient as labels. For each of these tasks, we included as labels the most frequent codes, such that the test set consisted of at least 10 examples for each label. 
Since all three tasks involve automatic and, particularly for the discharge summaries tasks, only approximately accurate labels (e.g., codes are present but the information could be described elsewhere in the respective EHR), performance even for perfect pattern recognition is not expected to be very high - by construction. Thus, the results of these tasks should not be interpreted in an absolute manner but rather relatively by comparing the performance between models. 
The OPS code 5-984 ("Microsurgical technique") is the most common label used for surgery reports, 8-930 ("Monitoring of respiration, heart, and circulation without measurement of pulmonary artery pressure and central venous pressure") for OPS-labeled discharge summaries, and the ICD code Z11 ("Special procedures for testing for infectious and parasitic diseases") for ICD-10-labeled discharge summaries.

\subsubsection{Open Medical Benchmarks}
As an additional openly available benchmark task, we consider the prediction of named entities in \textsc{GGPOnc} 2.0, a corpus of German clinical practice guidelines in oncology and the largest freely distributable data set of semantically annotated German medical texts \cite{borchert2022ggponc}. Seven medical students (all passed their first medical exam) annotated more than 200K mentions of clinically named entities. For the benchmark, we consider the most challenging setting with 8 fine-grained semantic classes and long entity spans. We use the same training/development/test splits as in the original baseline NER experiments. We also consider \textsc{GraSCCo}, a synthetic corpus of 62 clinical case reports, as a benchmark \cite{modersohn2022grascco}. Since the initial release of \textsc{GraSCCo} does not include semantic annotations, a single annotator from the \textsc{GGPOnc} annotation team created named entity annotations using the original \textsc{GGPOnc} annotation scheme and instructions. This resulted in 5.8K entity annotations in the long/fine setting.

\subsection{Data Anonymization}
Radiology reports extracted from our hospital database typically do not contain identifiable information such as name or date of birth. However, in rare cases, this information may have been added by the radiologist. Therefore, we used a named entity recognition model for the German language to identify all patient names in the \cite{akbik2018coling} data set. Identified names were then manually verified by two radiologists (KKB, LCA) and subsequently removed from the text. In addition, dates were removed from the text and replaced with wildcards. For benchmarks, each document was reviewed at least three times by the authors (KKB, LCA, JMP, PG) to ensure that no identifiable information was present. 

\subsection{Architectures / Models}

To evaluate the performance of our proposed model, we compare six different models based on the BERT or RoBERTa architecture. 
Two general German language models, two German medical models, and two versions of our pre-trained BERT model:
\begin{itemize}
	\item{GottBERT, which is the current state-of-the-art RoBERTa-based model for German text \cite{scheible2020gottbert, liu2019roberta} }
	\item{A multilingual BERT-based model \cite{devlin2018pretraining}.}
	\item{BioGottBERT, a version of GottBERT fine-tuned to medical texts and German Wikipedia \cite{lentzen2022critical}}
	\item{German-MedBERT, a version of the German BERT \cite{deepset2019} fine-tuned on a crawl of German medical websites.}
    \item{\emph{medBERT.de}, our model pre-trained on 4.7 Million German medical texts}
    \item{\emph{medBERT.de\textsubscript{dedup}}, a variant of \emph{medBERT.de} trained on a slightly smaller corpus where duplicated radiology reports had been removed}
\end{itemize}

\subsection{Deduplication}
Radiology reports are often written in a semi-structured form with very similar sentences. Because of this repetition, the information content of many documents is lower in terms of semantic concepts than other data sources used.
Language models tend to quickly overfit due to these data-inherent properties.
A common strategy to counteract this behavior is to deduplicate the pre-training data \cite{dedup}. Therefore, we measure the cosine distance between all reports by encoding them as bag of word representations. We only keep documents for which there is no other document with a similarity greater than 0.75. Due to computational restriction, we had to limit this approach to short reports only. Still, using this approach, we reduce the number of radiology reports for pre-training from 4,504,167 to 3,657,801 reports. To evaluate the impact of deduplication, 
we report the performance of the two BERT models trained with and without deduplicated data. 

\subsection{Hyperparameters/Pretraining Details}
We pre-train the model using the Lamb optimizer \cite{you2019lamb}. As usual for BERT-based models, we train the model in two phases, the first with a maximum sequence length of 128 and the second with a sequence length of 512. To pre-train our model from scratch, we use the hyperparameters given by \cite{you2019lamb}. In the first phase, we use a learning rate of $6e^{-3}$ and a batch size of 65,536 with 2,000 warm-up steps and a polynomial decaying learning rate for a total of 7,038 steps. In the second phase, we train with a batch size of 32,768 and a maximum learning rate of $4e^{-e}$, 200 warmup steps and a total of 1,563 steps.  
We remove very rare Unicode characters that appear less than three times from our pre-training data. This allows the tokenizer vocabulary to contain more specific sub-words and removes unnecessary tokens from the vocabulary that have an impact on the memory footprint. In addition, we set the number of occurrences required for a word to be included in the vocabulary to 20 to avoid including patient names in the vocabulary that may have been missed during anonymization. 

\subsection{Experimental Design}
We perform a hyperparameter optimization on all downstream tasks with median pruning on either the area under the receiver operating characteristic curve (AUROC, classification tasks) or token F1 (NER tasks). For each model-task combination, we perform 100 runs and tune the learning rate, the batch size, and the number of warm-up steps. Finally, we evaluate the best performing models for which the hyperparaemters are reported in Appendix \ref{appendix:parameters}.

\subsection{Data Availability}
Training data is not available due to privacy concerns. Weights for \emph{medBERT.de} and \emph{medBERT.de\textsubscript{dedup}} can be accessed at \url{https://huggingface.co/GerMedBERT/medbert-512} and \url{https://huggingface.co/GerMedBERT/medbert-512-no-duplicates} respectively. 

\section{Results}

\input{Results}
\subsection{Radiology Benchmarks} 
In the chest x-ray task, we found that the two best performing models were our own pre-trained BERT models. Our model trained on the corpus with duplicates removed (medBERT.de\textsubscript{dedup}) achieves a slightly better performance with an average AUROC of 83.65 compared to 83.42 of the model trained on the whole corpus (medBERT.de). The third-best performance is achieved by GottBERT with a mean AUROC of 83.48, which also outperformed other medical models. 
In the chest CT task, our model trained on the whole corpus performed best with an AUROC of 96.69, closely followed by  \emph{medBERT.de\textsubscript{dedup}.de}  (AUROC 96.39). Other models show a significantly lower performance of 19\%. GottBERT and BioGottBERT both perform similary with mostly sub-percentage differences in all metrics except precision. 
For the NER task, emph{medBERT.de\textsubscript{dedup}.de} showed the performance in all metrics except global Recall. However, the scores of all models are in a similar range, with mostly 1-3\% differences between the best and worst-performing models. 
The results suggest that the advantage of domain-specific models is more pronounced for longer texts. On the X-ray (98 ± 27 words) and NER (108 ± 41 words) tasks, which consist of short reports of only a few sentences, the difference between the models is not as pronounced as on the CT reports, which are considerably longer (258 ± 100 words). 

Tables \ref{table:clftasks} and \ref{table:nertasks} provide an overview of all tasks and metrics. Detailed metrics for each class on the radiology benchmarks can be found in the Appendix \ref{appendix:detailed_metrics}.

\subsection{Open Medical Benchmarks}
The \textsc{GGPOnc} benchmark \cite{borchert2022ggponc}, a German corpus based on clinical practice guidelines for oncology, and the \textsc{GraSCCo} benchmark \cite{grassco2022}, a corpus consisting of artificially generated electronic health records for various diseases, were used for this comparison.
On \textsc{GGPOnc}, our models achieved higher AUROC, precision (global and token-level) and token-level recall than the other models, while BioGottBERT achieved the highest macro F1 score and recall. Deduplication of training data did not seem to have a positive impact on model performance, as \emph{medBERT.de} consistently achieved higher metrics than \emph{medBERT.de}\textsubscript{dedup}.
On \textsc{GraSCCo}, GottBERT and \emph{medBERT.de} showed the best performance. \emph{medBERT.de} had the highest AUROC (85.14), AUROC\textsubscript{tok} (75.17), and Recall\textsubscript{tok} (75.17), while GottBERT had the best performance on all other metrics. However, the overall difference between the models is small. 

\subsection{Private Medical Benchmarks}
Three medical benchmarks were constructed using discharge letters and surgical reports, which are not publicly available for privacy reasons. Each benchmark consisted of 2000 texts, which were stratified into a training set of 1000 texts, a validation set of 500 texts, and a test set of 500 texts.
The first task is to predict 65 different ICD-10 codes from discharge summaries. Both of our models show superior performance on this task, outperforming all other models. The best model is \emph{medBERT.de}, although the difference to \emph{medBERT.de\textsubscript{dedup}} is in the sub-percentage range. 
The second task is to predict 49 OPS codes from discharge summaries. Again, \emph{medBERT.de} and \emph{medBERT.de\textsubscript{dedup}.de}  performs better than all other models. However, the overall performance of all models is below average, with particularly low scores for the F1 measure and recall.
In the third task, the classification of 10 OPS codes from surgical reports, our domain-specific models again show the best performance. Compared to the other two tasks, the overall scores are also higher. We attribute this to the fact that this task is less complex since surgical reports are generally shorter and have less variability than discharge summaries. In addition, the number of labels is reduced, which further contributed to the reduced difficulty.

\subsection{General Domain Benchmarks}
In addition to the medical and radiological benchmark tasks, we also evaluated performance on the GermEval18 \cite{germeval18}. We found that GottBERT outperformed all domain-adapted models in all evaluated metrics.
We also observed that the \emph{medBERT.de} models were outperformed by the BioGottBERT model. This can be attributed to the fact that the pre-training corpus for GottBERT contains a greater amount of general domain knowledge and language, giving the model an advantage in general domain tasks. In addition, BioGottBERT, which is based on GottBERT, may have the same advantage but may be affected by catastrophic forgetting because it has only been trained on corpora from the medical domain.

\subsection{Tokenizer Fertility}
Ruse et al. suggests that a tokenizer that produces fewer sub-words per word may improve performance due to better-developed embeddings \cite{rust2019howgoodtokenizer}. Therefore, we measure tokenizer fertility, which measures the average number of sub-word per tokenized word for all of our evaluated models. For the evaluation, we use the text data from our chest CT, chest x-ray classification, and wrist NER tasks. 
As expected, we measure the lowest fertility for the tokenizer of the \emph{medBERT.de} model, which is trained on data following a similar distribution.
We find that tokenizer fertility does not necessarily correlate with improved performance. For example, our \emph{medBERT.de} models both have the lowest fertility score of 1.18 (see table \ref{tab:fertility}) and are the best-performing models on the chest CT and x-ray as well as the wrist NER task. However, it is closely followed by GottBERT, which has a much higher fertility score of 1.75, but performs similarly well. Furthermore, we observe that deduplication of the training data has almost no effect on the fertility of the \emph{medBERT.de} model.

\input{fertility_table.tex}

\section{Discussion}
In this study, we trained a domain-specific German BERT model on a large data set of German medical texts, including articles, papers, and electronic medical records. We then fine-tuned the model on various medical benchmarks and found that it outperformed both general domain language models and other medical domain models, demonstrating the model's ability to capture the unique characteristics and terminology of German medical language more effectively than general German models. Our results highlight the advantages of using domain-specific language models for the German medical language but also the importance of a large training corpus. In line with previous research \cite{DBLP:conf/emnlp/Perez-MayosBW21}, we attribute the superior performance of our model on medical benchmarks to the larger amount of data used in training compared to German-MedBERT or BioGottBERT. However, our results also suggest that data for pre-training or fine-tuning domain-specific models should not consist solely of specialized language, as this may negatively affect the model's performance on general tasks. This is demonstrated by the Germeval18 task, where GottBERT outperformed BioGottBERT, even though the two models have identical architectures and BioGottBERT was initialized with GottBERT's weights. This performance difference can be attributed to the fact that BioGottBERT was trained on the entire German Wikipedia, which contains general domain language. 
In addition, we observe a drastically improved performance on the OPS and ICD code classification tasks compared to all other domain-specific and general domain models. This indicates that our models are able to generalize to different subdomains that are not included in the pre-training data. The results, therefore, suggest, that the models are able to capture relevant clinical and medical concepts, thus demonstrating their ability to adapt to the different clinical language used in discharge notes.
In contrast to \cite{rust2019howgoodtokenizer}, we did not observe a direct correlation between the fertility of the tokenizer and the performance of the model on downstream tasks. This suggests that fertility alone is not a predictive measure of a model's performance on specialized downstream tasks. Nevertheless, it is likely that the fertility of the tokenizer played a role in the model's performance on tasks involving longer texts, particularly on clinical benchmarks based on discharge notes and surgical reports. Since the texts for these benchmarks were truncated to fit into 512 tokens, some information may have been lost in the process. A more efficient tokenizer may be able to encode more information, potentially improving the model's performance on these tasks.
In our study, we found a mixed impact of data deduplication. While earlier research suggested benefits from deduplication \cite{dedup}, we did not see a consistent improvement in performance with our model (\emph{medBERT.de}) compared to the deduplicated version (\emph{medBERT.de\textsubscript{dedup}}). While on certain benchmarks, our \emph{medBERT.de} performed better than the deduplicated version, on others it performed worse. This discrepancy could be due to the fact that our deduplication process was not as extensive, as it was only applied to short reports. Furthermore, we have not yet applied deduplication to non-radiological text, which might contain duplicates.


\subsection{Limitations} % optional, but recommended 
A limitation of our study is that about 40\% of the data consists of radiology reports, which may differ in style from other types of electronic health records. In addition, certain medical specialties, such as ophthalmology and pathology, are underrepresented in our sample due to their limited use of radiological imaging. On the other hand, other specialties, such as psychiatry, may be underrepresented because the conditions they treat are not typically seen on imaging. It is also worth noting that the texts were collected from a single university hospital, and it is possible that the performance of our model on new data may be affected by differences in reporting styles between institutions. We suspect that our training data does not sufficiently capture the semantic information needed to improve performance compared to a language model trained on a general domain corpus. This can be partly explained by the repetitive nature of radiology reports.

\subsection{Conclusion and Outlook} % optional
In conclusion, this study has shown the benefits of using a domain-specific German BERT model, trained on a large data set of German medical texts, for tasks related to the German medical language. The model achieved superior performance compared to the general domain and other medical domain models, underlining the value of using domain-specific models. However, to further improve performance, a future German clinical language model should be trained on on a more diverse data set, e.g., including discharge summaries from a broad range of medical specialties. Nevertheless, this model represents a new state-of-the-art for German clinical language, outperforming GottBERT. 


\section{Acknowledgements}
We would like to thank DocCheck AG and Thieme Medical Publishers for their assistance with data collection. We would also like to thank Manjil Shrestha and Rolf Becker for providing additional training data from their German MedBERT model. We also thank the Scientific Computing of the IT Division at the Charité - Universitätsmedizin Berlin for providing computational resources that contributed to the research results reported in this paper.
KKB is grateful for his participation in the BIH Charité Digital Clinician Scientist Program funded by Charité-Universitätsmedizin Berlin and the Berlin Institute of Health. 

\bibliographystyle{abbrv}

\bibliography{bibliography}


\section{Appendix}
\subsection{Data Sources Details} \label{Appendix:Data Sources}


\paragraph{DocCheck Flexikon:} 
The DocCheck Flexikon (\url{https://flexikon.doccheck.com/}) is an open wiki dealing with medical topics. It contains overview articles about diseases, diagnostic procedures, or treatments in all areas of medicine. This study includes all articles of the Flexikon that have been published until January 1\textsuperscript{st}, 2022. In addition, entries from the \textit{DocCheck} forum and product descriptions from the medical store were included. This resulted in 63,884 documents (92 MB of raw text).  

\paragraph{\textsc{GGPOnc}:} 
\textsc{GGPOnc} is a freely available German language corpus based on clinical practice guidelines for oncology with expert annotations. It is available at \cite{borchert-etal-2020-ggponc, borchert2022ggponc}

\paragraph{Webcrawl:} 
A webcrawl of several German medical forums was performed, as described in \cite{Shrestha2021}. The webcrawl consisted of 11,322 documents (65 MB of raw text). 

\paragraph{Pubmed Abstracts:} 
We crawled PubMed (\url{https://pubmed.ncbi.nlm.nih.gov/}) for German publications with openly available German abstracts published by September 1\textsuperscript{st} 2022. This resulted in 12,139 documents, representing 16 MB of raw text.

\paragraph{Radiology Reports:} 
All radiology reports created between January 1\textsuperscript{st}, 2009, and December 31\textsuperscript{st}, 2021, were extracted from the Radiology Information System of \textit{Charité - Universitätsmedizin Berlin}. After removing texts with less than 100 characters, we performed a similarity analysis to exclude duplicate reports. All remaining documents were then anonymized. This resulted in 3.66 million radiology reports that were included in the training corpus (4,195 MB of raw text). 

\paragraph{Springer Nature Corpus:} 
Using the \textit{Springer Nature API}, we identified German-language, open-access publications from the Springer Nature group. Abstracts and full text of the publications were extracted and added to the training corpus. In total, this involved 257,999 documents and 1,986 MB of data. 

\paragraph{Thieme Publishing Group Corpus:} 
With the permission of the \textit{Thieme Publishing Group}, medical textbooks, licensed by the Charité, and journals for continuing medical education (including the Up2Date series, available at \url{https://www.thieme.de/de/aerzte-in-weiterbildung/up2date-fachzeitschriften-20280.htm}) were included in the training corpus. After cleaning the texts by removing figures and tables, this resulted in 330,994 documents and 2,898 MB of raw text. 

\paragraph{Electronic Health Records:} 
We included 373,421 electronic health records (EHR) from the TBase database form the Department of Nephrology and the Center for Kidney Transplantation at \textit{Charité - Universitätsmedizin Berlin} \cite{schmidt2021tbase}. These included physician letters, microbiology reports, pathology reports, and diagnostic procedure reports (440 MB of raw text). 

\paragraph{PhD Theses:} 
In this study, we used a data set of 7,481 open-access German medical dissertations and postdoctoral theses from the \textit{Charité - Universitätsmedizin Berlin} available at \url{https://refubium.fu-berlin.de/handle/fub188/13}. We cleaned the data by removing sentences that did not contain German stop words and excluded theses with a length of fewer than 15 pages. This process ensured that only relevant, high-quality information was included in our analysis. In total, 648 MB of data was added to the training corpus. 

\paragraph{Wikipedia:} 
Entries from the German Wikipedia dealing with medical topics were extracted and added to the training corpus. There were 3,639 texts, corresponding to 22 MB. 

\subsection{Detailed metrics per class for benchmarks based on radiology reports} \label{appendix:detailed_metrics}

In evaluating the performance of our models on radiology report benchmarks, we analyzed detailed metrics per class to provide a deeper understanding of the model's performance on specific classes within the data set. The metrics included precision, recall, F1 score, and AUROC, which were calculated for each class individually, allowing for a more nuanced evaluation of the model's performance and potential shortcomings. 

\input{xray_detailed_results.tex}
\input{ct_detailed_results.tex}
\input{wrist_detailed_results.tex}

\subsection{Distribution of classes for benchmarks based on radiology reports} \label{appendix:labels}

For the benchmarks based on radiology reports, the distribution of classes is an important factor to consider when evaluating the performance of our models. As some diagnoses are rarer than others, the class distribution is highly skewed, especially for the CT and NER tasks. 

\input{benchmarks}

\newpage

\subsection{Model Selection for Radiology tasks} \label{appendix:parameters}

For the benchmarks based on radiology reports, the hyperparameter optimization resulted in the following parameters: 


\input{parameters_ct.tex}
\input{parameters_xray.tex}
\input{parameters_wrist.tex}






\end{document}