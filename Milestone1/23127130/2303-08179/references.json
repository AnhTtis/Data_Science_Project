{
  "2308-02976": {
    "title": "Spanish Pre-trained BERT Model and Evaluation Data",
    "authors": [
      "J. Cañete",
      "Gabriel Chaperon",
      "Rodrigo Fuentes",
      "Jou-Hui Ho",
      "Hojin Kang",
      "Jorge P'erez"
    ],
    "submission_date": "2023-08-06",
    "revised_dates": [],
    "doi": "10.48550/arXiv.2308.02976",
    "arxiv_id": "2308.02976",
    "venue": "arXiv.org",
    "year": 2023
  },
  "2109-03160": {
    "title": "How much pretraining data do language models need to learn syntax?",
    "authors": [
      "Laura Pérez-Mayos",
      "Miguel Ballesteros",
      "L. Wanner"
    ],
    "submission_date": "2021-09-07",
    "revised_dates": [],
    "doi": "10.18653/v1/2021.emnlp-main.118",
    "arxiv_id": "2109.03160",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021
  },
  "2107-06499": {
    "title": "Deduplicating Training Data Makes Language Models Better",
    "authors": [
      "Katherine Lee",
      "Daphne Ippolito",
      "A. Nystrom",
      "Chiyuan Zhang",
      "D. Eck",
      "Chris Callison-Burch",
      "Nicholas Carlini"
    ],
    "submission_date": "2021-07-14",
    "revised_dates": [],
    "doi": "10.18653/v1/2022.acl-long.577",
    "arxiv_id": "2107.06499",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2021
  },
  "2012-15613": {
    "title": "How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models",
    "authors": [
      "Phillip Rust",
      "Jonas Pfeiffer",
      "Ivan Vulic",
      "Sebastian Ruder",
      "Iryna Gurevych"
    ],
    "submission_date": "2020-12-31",
    "revised_dates": [],
    "doi": "10.18653/v1/2021.acl-long.243",
    "arxiv_id": "2012.15613",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020
  },
  "2012-02110": {
    "title": "GottBERT: a pure German Language Model",
    "authors": [
      "Raphael Scheible",
      "Fabian Thomczyk",
      "P. Tippmann",
      "V. Jaravine",
      "M. Boeker"
    ],
    "submission_date": "2020-12-03",
    "revised_dates": [],
    "doi": "10.18653/v1/2024.emnlp-main.1183",
    "arxiv_id": "2012.02110",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020
  },
  "2010-10906": {
    "title": "German’s Next Language Model",
    "authors": [
      "Branden Chan",
      "Stefan Schweter",
      "Timo Möller"
    ],
    "submission_date": "2020-10-21",
    "revised_dates": [],
    "doi": "10.18653/V1/2020.COLING-MAIN.598",
    "arxiv_id": "2010.10906",
    "venue": "International Conference on Computational Linguistics",
    "year": 2020
  },
  "2007-15779": {
    "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
    "authors": [
      "Yu Gu",
      "Robert Tinn",
      "Hao Cheng",
      "Michael R. Lucas",
      "N. Usuyama",
      "Xiaodong Liu",
      "Tristan Naumann",
      "Jianfeng Gao",
      "Hoifung Poon"
    ],
    "submission_date": "2020-07-31",
    "revised_dates": [],
    "doi": "10.1145/3458754",
    "arxiv_id": "2007.15779",
    "venue": "ACM Trans. Comput. Heal.",
    "year": 2020
  },
  "2007-06400": {
    "title": "GGPONC: A Corpus of German Medical Text with Rich Metadata Based on Clinical Practice Guidelines",
    "authors": [
      "Florian Borchert",
      "Christina Lohr",
      "Luise Modersohn",
      "T. Langer",
      "M. Follmann",
      "J. Sachs",
      "U. Hahn",
      "M. Schapranow"
    ],
    "submission_date": "2020-07-13",
    "revised_dates": [],
    "doi": "10.18653/v1/2020.louhi-1.5",
    "arxiv_id": "2007.06400",
    "venue": "International Workshop on Health Text Mining and Information Analysis",
    "year": 2020
  },
  "2006-06202": {
    "title": "A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages",
    "authors": [
      "Pedro Ortiz Suarez",
      "L. Romary",
      "Benoît Sagot"
    ],
    "submission_date": "2020-06-11",
    "revised_dates": [],
    "doi": "10.18653/v1/2020.acl-main.156",
    "arxiv_id": "2006.06202",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020
  },
  "2005-12833": {
    "title": "Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction",
    "authors": [
      "L. Rasmy",
      "Yang Xiang",
      "Z. Xie",
      "Cui Tao",
      "Degui Zhi"
    ],
    "submission_date": "2020-05-22",
    "revised_dates": [],
    "doi": "10.1038/s41746-021-00455-y",
    "arxiv_id": "2005.12833",
    "venue": "npj Digital Medicine",
    "year": 2020
  },
  "1911-03894": {
    "title": "CamemBERT: a Tasty French Language Model",
    "authors": [
      "Louis Martin",
      "Benjamin Muller",
      "Pedro Ortiz Suarez",
      "Yoann Dupont",
      "L. Romary",
      "Eric Villemonte de la Clergerie",
      "Djamé Seddah",
      "Benoît Sagot"
    ],
    "submission_date": "2019-11-10",
    "revised_dates": [],
    "doi": "10.18653/V1/2020.ACL-MAIN.645",
    "arxiv_id": "1911.03894",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019
  },
  "1907-11692": {
    "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
    "authors": [
      "Yinhan Liu",
      "Myle Ott",
      "Naman Goyal",
      "Jingfei Du",
      "Mandar Joshi",
      "Danqi Chen",
      "Omer Levy",
      "M. Lewis",
      "Luke Zettlemoyer",
      "Veselin Stoyanov"
    ],
    "submission_date": "2019-07-26",
    "revised_dates": [],
    "arxiv_id": "1907.11692",
    "venue": "arXiv.org",
    "year": 2019
  },
  "1904-00962": {
    "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes",
    "authors": [
      "Yang You",
      "Jing Li",
      "Sashank J. Reddi",
      "Jonathan Hseu",
      "Sanjiv Kumar",
      "Srinadh Bhojanapalli",
      "Xiaodan Song",
      "J. Demmel",
      "K. Keutzer",
      "Cho-Jui Hsieh"
    ],
    "submission_date": "2019-04-01",
    "revised_dates": [],
    "arxiv_id": "1904.00962",
    "venue": "International Conference on Learning Representations",
    "year": 2019
  },
  "1901-08746": {
    "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
    "authors": [
      "Jinhyuk Lee",
      "WonJin Yoon",
      "Sungdong Kim",
      "Donghyeon Kim",
      "Sunkyu Kim",
      "Chan Ho So",
      "Jaewoo Kang"
    ],
    "submission_date": "2019-01-25",
    "revised_dates": [],
    "doi": "10.1093/bioinformatics/btz682",
    "arxiv_id": "1901.08746",
    "venue": "Bioinform.",
    "year": 2019
  },
  "1810-04805": {
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "authors": [
      "Jacob Devlin",
      "Ming-Wei Chang",
      "Kenton Lee",
      "Kristina Toutanova"
    ],
    "submission_date": "2019-01-01",
    "revised_dates": [],
    "doi": "10.18653/v1/N19-1423",
    "arxiv_id": "1810.04805",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2019
  }
}