\documentclass[10pt,twocolumn,letterpaper]{article}
\pdfoutput=1
\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{paralist}
% Include other packages here, before hyperref.
\usepackage{subfig}
\usepackage{xcolor}

\definecolor{GG}{RGB}{48,144,96}
\usepackage{algorithm} %format of the algorithm 
\usepackage{algorithmic} %format of the algorithm 
\newcommand{\INDSTATE}[1][1]{\STATE\hspace{#1\algorithmicindent}}
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{3147} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{AUTO: Adaptive Outlier Optimization for Online Test-Time OOD Detection}

\author{Puning Yang$^{1,2}$, Jian Liang$^{1}$, Jie Cao$^{1}$, and Ran He$^{1,2}$\\
$^{1}$ CRIPAC \& MAIS, Institute of Automation, Chinese Academy of Sciences\\
$^{2}$ School of Artificial Intelligence, University of Chinese Academy of Sciences\\
% {\tt\small \{puning.yang, jie.cao\}@cripac.ia.ac.cn, liangjian92@gmail.com, rhe@nlpr.ia.ac.cn}
{\tt\small puning.yang@cripac.ia.ac.cn, liangjian92@gmail.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Second Author\\
%Institution2\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
Out-of-distribution (OOD) detection is a crucial aspect of deploying machine learning models in open-world applications. 
Empirical evidence suggests that training with auxiliary outliers substantially improves OOD detection. 
However, such outliers typically exhibit a distribution gap compared to the test OOD data and do not cover all possible test OOD scenarios. 
Additionally, incorporating these outliers introduces additional training burdens.
In this paper, we introduce a novel paradigm called test-time OOD detection, which utilizes unlabeled online data directly at test time to improve OOD detection performance.
While this paradigm is efficient, it also presents challenges such as catastrophic forgetting.
To address these challenges, we propose adaptive outlier optimization (AUTO), which consists of an in-out-aware filter, an ID memory bank, and a semantically-consistent objective. 
AUTO adaptively mines pseudo-ID and pseudo-OOD samples from test data, utilizing them to optimize networks in real time during inference.
Extensive results on CIFAR-10, CIFAR-100, and ImageNet benchmarks demonstrate that AUTO significantly enhances OOD detection performance.
The code will be available at \href{https://github.com/Puning97/AUTO-for-OOD-detection}{https://github.com/Puning97/AUTO-for-OOD-detection}.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Deep neural networks often exhibit overconfidence when predicting out-of-distribution (OOD) samples, which undermines their reliability in the open-world deployment~\cite{nguyen2015deep,bendale2016towards}.
Therefore, discerning the OOD data from in-distribution (ID) data is critical and motivates recent studies in OOD detection~\cite{hendrycks2016baseline}.
Existing solutions focus on either designing new scoring functions \cite{hendrycks2016baseline,liang2018enhancing,lee2018simple,liu2020energy,sun2021react,wei2022mitigating} or training models with auxiliary outliers~\cite{hendrycks2018deep,liu2020energy,du2022vos,ming2022posterior,tao2023nonparametric,wang2023outofdistribution}.
The latter paradigm, known as \emph{Outlier Exposure} based (OE-based) methods, has shown to be effective due to their ability to leverage the knowledge of outliers.

\begin{figure}
    \flushleft
    \setlength{\abovecaptionskip}{10pt}
    \setlength{\belowcaptionskip}{-5pt}
    \includegraphics[width=\linewidth]{Figure_1.png}
    \caption{Comparsion between MSP~\cite{hendrycks2016baseline}, OE~\cite{hendrycks2018deep}, WOODS\cite{katzsamuels2022training}, and our AUTO.
    Models are trained on CIFAR-100~\cite{krizhevsky2009learning} with ResNet-34~\cite{he2016identity}.}
    \label{fig:first}
\end{figure}

Despite the promising results, the distribution gap between the training outliers and test data can mislead the model, thus impairing the performance of OOD detection.
In addition, a limited set of training outliers can hardly cover various test OOD scenarios, resulting in unstable performances.
Addressing both issues is essential but challenging for the OE-based methods.
Notably, WOODS~\cite{katzsamuels2022training} directly samples the outlier set from unlabeled test data, thus narrowing the distribution gap between training outliers and test OOD data.
Results in Figure \ref{fig:first} reveal the benefits of this scheme.
However, it still incurs additional computational costs during training.
Moreover, the finite test OOD data sampled by WOODS during the training phase can not represent all potential OOD samples in the current deployment, leading to sub-optimal performance.

To overcome the above limitations, we first formulate a practical and challenging setting called \emph{test-time} OOD detection.
We suggest a simple yet powerful way to access test OOD data -- directly modifying models at test time.
Our setting does not impose any burden during the training phase but modifies the model at test time, offering strong efficiency, flexibility, and generality.
To learn from unlabeled online data, we propose to update the model once the incoming test sample is identified as a pseudo-OOD sample.
However, optimizing the model with only pseudo-OOD samples will cause catastrophic forgetting~\cite{mccloskey1989catastrophic}, which underperforms both ID and OOD tasks.

Accordingly, we propose \textbf{A}daptive o\textbf{UT}lier \textbf{O}ptimiza-tion (AUTO), which consists of an in-out-aware filter, a dynamic memory bank, and a semantically-consistent objective.
Specifically, the in-out-aware filter is initialized by training ID data to identify pseudo-OOD and pseudo-ID samples and expose them to models at test time.
The dynamic memory bank contains one ID sample per category, from which the model learns alongside the identified outliers during testing.
Besides, we observe potential bias in the test-time optimization, which leads to degraded ID performance.
Thus, we design a simple objective to keep the prediction of the latest model consistent with that of the original model. That further helps the model maintain ID accuracy.
Extensive evaluations suggest that AUTO substantially enhances OOD detection performance and outperforms the competing methods by a large margin.



Our contributions are summarized as follows:
%
\begin{compactitem}
    \item We explore a practical and challenging problem, \emph{test-time out-of-distribution detection}, which directly optimizes networks with unlabeled test data at test time.%consists of the same ID data and variable OOD data.
    \item 
    We introduce AUTO. Except for selecting and learning from pseudo-OOD samples, it also employs an ID memory bank and a semantically-consistent objective to overcome the forgetting issue.
    \item Extensive results demonstrate that AUTO achieves significant improvements in all OOD detection tasks while preserving ID classification performance.
\end{compactitem}

\section{Related Work}
\noindent{\textbf{OOD Detection with ID data only.}}
\label{OOD discuss}
A lot of studies have explored the OOD detection task in the past few years.
The OpenMax score~\cite{bendale2015towards} is the first method to detect OOD samples by utilizing the Extreme Value Theory.
Hendrycks \emph{et al.} \cite{hendrycks2016baseline} present a baseline using the Maximum Softmax Probability (MSP) but may not be suitable for OOD detection \cite{morteza2022provable}.
Later on, researchers improve the OOD detection performance in two ways:
(1) advanced score functions are proposed, including ODIN score \cite{liang2018enhancing}, Mahalanobis distance-based score \cite{lee2018simple}, Energy score \cite{liu2020energy,wang2021canmulti,lin2021mood}, GradNorm score \cite{huang2021importance}, and MaxLogit score \cite{hendrycks2022scaling}.
(2) novel techniques are proposed to modify the logit space, including SSD \cite{sehwag2020ssd}, ReAct \cite{sun2021react}, GEM \cite{morteza2022provable}, KNN \cite{sun2022knn}, LogitNorm \cite{wei2022mitigating}, DICE \cite{sun2022dice}, and CIDER \cite{ming2023cider}.

\noindent{\textbf{OOD Detection with auxiliary outliers.}}
Another promising direction toward OOD detection involves the auxiliary outliers for model regularization.
The pioneering work, Outlier Exposure (OE) \cite{hendrycks2018deep}, trains models with auxiliary outliers, inspiring a new line of work, such as CCU \cite{meinke2019towards}, SOFL \cite{mohseni2020self}, Energy \cite{liu2020energy}, ATOM \cite{chen2021robustifying}, and POEM \cite{ming2022posterior}.
Considering the scarcity of training outliers, recent works synthesize pseudo-OOD samples using ID samples (VOS \cite{du2022vos}, NPOS~\cite{tao2023nonparametric}, and DOE~\cite{wang2023outofdistribution}) or sample test data directly \cite{katzsamuels2022training}. 
It is noteworthy that WOODS \cite{katzsamuels2022training} has tried to leverage test data to improve detection performance.
However, WOODS requires access to test data at training time, which is not feasible in some security-sensitive scenarios (\eg federated learning).
In contrast, AUTO directly utilizes data at test time, which is more general and flexible.

\noindent{\textbf{Test-time optimization.}}
To address the distribution shift without the leakage of training data, a recent paradigm called source-free domain adaptation \cite{liang2020we,liang2021source,kundu2020universal} adapts the pre-trained source model to the whole target domain.
However, this paradigm is not suitable for streaming data where the test data can be seen only once.
A pioneering work~\cite{sun2020test} designs a customized multi-head model and provides two modes (\ie, vanilla and online) for adapting the model to a single instance at test time.
While the vanilla mode \cite{zhang2021memo} individually adapts the pre-trained model to each test sample, the online mode can reuse past knowledge to help recognize the incoming sample better.
A number of follow-up methods \cite{wang2020tent,iwasawa2021test} adopt the online test-time optimization manner but do not require a customized pre-trained model, making the test-time adaptation more attractive.
However, all these methods aim to improve the generalization ability of the pre-trained model, which can not handle the unknown classes in the OOD detection task.
Notably, ETLT \cite{fan2022simple} proposes calibrating the OOD scores by establishing the correlations between the sample's OOD score and its input feature at test time.


\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.\textwidth]{framework.png}
    \caption{Illustration of the Adaptive oUTlier Optimization (AUTO) framework. The key components include a dynamic ID memory bank, an in-out-aware filter, and a semantically-consistency loss $L_{SC}$. 
    Different color means different operations at test time: Each sample is given the MSP score $S^t(x^t)$ and judged by the filter.
    Then, according to the judgment, the sample will activate different operations.
    For instance, if it is recognized as a pseudo-ID sample, blue lines are activated:
    this sample will be utilized to replace the sample with the same label in the ID memory bank.
    }
    \label{fig:framework}
\end{figure*}

\section{Online Test-Time OOD Detection}
\subsection{Preliminary: OOD Detection}
Let $\mathcal{X} \subseteq \mathbb{R}^{n} $ be the input space, $\mathcal{Y}=\left \{ 1,...,C \right \}$ be the label space, and ${h} = \rho \circ \varphi : \mathcal{X} \to \mathbb{R}^{C}$ be the model, where $\rho$ is the classifier and $\varphi$ is the feature extractor.
The supervised methods aim to learn the joint data distribution from the training set $\mathcal{D}_{{id}}$.
Let $\mathcal{P}_{{ID}}$ and $\mathcal{P}_{{OOD}}$ denote the marginal ID and OOD distribution on $\mathcal{X}$, separately.
The OOD detector $f_{\lambda}(\cdot)$ aims to find a proper model ${h}$ and score function $S(\cdot)$ to detect OOD data well:
\begin{equation}
    f_{\lambda}({x})=\begin{cases}
 {ID} & S({x}) \ge \beta \\
  {OOD}& S({x})<\beta
\end{cases},
\end{equation}
where $\beta$ is a given threshold.
OE \cite{hendrycks2018deep} aims to get a better ${h}$ by regularizing the target model to produce low-confidence predictions for test OOD data.
Thus OE introduces auxiliary outliers at training time. Let $\mathcal{D}_{aux}$ be the auxiliary OOD dataset on $\mathcal{X}$, which is also disjoint with the test OOD dataset.
The learning objective can be written as:
\begin{equation}
    \mathbb{E}_{( {x},{y})\sim \mathcal{D}_{{id}}}
[\ell_{\mathrm{CE}}({h}({x}),{y})]
+\lambda \mathbb{E}_{( {x}) \sim \mathcal{D}_{aux}}
[\ell_{\mathrm{OE}}({h}({x}))],
    \label{l_oe}
\end{equation}
where $\lambda$ is a trade-off hyperparameter and is set to 0.5 for vision tasks, $\ell_{\mathrm{CE}}$ is the cross-entropy loss, and $\ell_{\mathrm{OE}}$ is defined by Kullback-Leibler divergence of softmax predictions to the uniform distribution.
For phraseology consistency, in this paper, we describe training-time examples as \emph{auxiliary outliers} and exclusively use \emph{OOD data} to describe test-time unknown inputs.

\subsection{Challenges of Test-Time OOD Detection}
\label{3.2}
This paper formalizes the problem of enhancing OOD detection in an online unlabeled data stream at test time.
With this new setting, concrete questions arise:
(1) \emph{How to roughly distinguish test data as ID or OOD?} Tackling this question will help select pseudo-OOD and pseudo-ID samples that AUTO learns from during testing.
(2) Assuming the continual learning at the image-by-image level is effective, \emph{will constant changes lead to catastrophic forgetting?}
If it does, \emph{how to overcome it?}
(3) Different from training-phase methods, the update efficiency for test-time methods needs to be considered, especially for time-critical tasks.
Thus, \emph{How to efficiently update the model?}

\section{Adaptive Outlier Optimization}
In this section, we first describe the proposed Adaptive oUTlier Optimization (AUTO) framework. As illustrated in Figure~\ref{fig:framework}, AUTO comprises three key components: an in-out-aware filter to tackle the selection of training samples (Section \ref{4.1}), a dynamic ID memory bank, and a semantically-consistent objective to tackle the forgetting issue (Section \ref{4.2}), respectively.
Then, we elaborate on the parameter updating strategy for efficient model optimization (Section \ref{4.3}).
The full algorithm is provided with the above components, which systematically work as a whole and reciprocate each other (Section \ref{4.4}).


\subsection{Adaptive In-out-aware Filter}
\label{4.1}
We first model the distribution of open world data $\mathcal{P}_{OPEN}$ with the Huber contamination model \cite{huber1992robust}:
\begin{equation}
    \mathcal{P}_{OPEN} = \kappa\mathcal{P}_{ID} + (1-\kappa)\mathcal{P}_{OOD},
    \label{open_equ}
\end{equation}
where $\kappa  \in [0,1)$.
The mixture of unlabeled data from $\mathcal{P}_{OPEN}$ posits a unique obstacle for differentiated optimization based on data distribution.
In our method, pseudo-OOD samples are roughly distinguished from test ID data by our adaptive in-out-aware filter, which is initialized by the statistics from the training ID data.
For instance, given ID examples ${x}_{id}^{i} \sim \mathcal{P}_{ID}~,i \in [1,N]$, we compute the MSP \cite{hendrycks2016baseline} score $S^0({x}_{id}^{i})$ of each sample and then estimate the mean $\mu_{in}$ and standard deviation $\sigma_{in}$ of ID data:
\begin{equation}
    \mu_{in}=\frac{\sum_{i=1}^{N}S^0({x}_{id}^{i})}{N}, \sigma_{in}=\sqrt{\frac{\sum_{i=1}^{N}(S^0({x}_{id}^{i})-\mu_{in})}{N}}. \\
\end{equation}
Then, the outlier-aware and inner-aware margins are initialized as follows:
\begin{equation}
m_{in}^0=\mu_{in} + k_1 \times \sigma_{in},~~~m_{out}^0=\mu_{in} - k_2 \times \sigma_{in},
\end{equation}
where $k_1$ and $k_2$ are hyperparameters.
On the one hand, we regard a sample with a score higher than $m_{in}$ as a pseudo-ID sample.
We keep $m_{in}$ fixed during training, and this setting works well in the experiments.
On the other hand, the confidence scores of all samples are decreasing as we update models with the outliers~\cite{hendrycks2018deep,liu2020energy}.
Taking this phenomenon into consideration, we update $m_{out}$ with a greedy strategy.
A sample ${x}^t$ with a score lower than $m_{out}$ is recognized as a pseudo-OOD sample and rewritten as $\hat{{x}}^t_{ood}$.
Then, we use $\hat{{x}}^t_{ood}$ to optimize the model as follows:
\begin{equation}
    \mathcal{L}_{\textrm{ood}}=\ell_{\mathrm{OE}}({h}^{t}(\hat{{x}}^t_{ood})),
\end{equation}
where ${h}^{t}$ represents the latest updated model.

Meanwhile, We record the mean of historical OOD score values of the pseudo-OOD samples.
Then, we use the mean value to update $m_{out}$ as follows:
Assuming that we have recorded the mean score of $M$ pseudo-OOD samples when the t-th sample inputs:
\begin{equation}
    m_{out}^{t+1} =\begin{cases}
\frac{M \cdot m_{out}^{t} + S^{t}({x}^t)}{M+1}  & \text{ if } S^{t}({x}^t)<m_{out}^t ,\\
m_{out}^{t}  & \text{else.}
\end{cases}
    \label{outlier_margin}
\end{equation}

\subsection{Anti-forgetting Components}
\label{4.2}
\textbf{Dynamic ID memory bank.}
We introduce a dynamic memory bank $\mathcal{M}_{id}$ into the ID classification loss formulated in Eq. \ref{l_oe}.
The memory bank store one sample per category and is initialized with samples randomly selected from training data.
We update the samples in the memory bank with the test-time ID data in the same category.
Concretely, given a test-time sample $\hat{{x}}_{id}$ whose score is higher than the inner-aware margin $m_{in}$ and its pseudo label $\hat{{y}}_{id}$, we utilize it to update the memory bank as follows:
\begin{equation}
    \hat{{x}}_{id} \to {x}_{\mathcal{M}},\quad \text{if } \hat{{y}}_{id}= {y}_{\mathcal{M}}.
\end{equation}

Empirically, we notice that the training with only $\mathcal{M}_{id}$ does not help improve OOD detection, thus we do not modify our model until encountering a $\hat{{x}}_{ood}$. The test-time ID objective $\mathcal{L}_{id}$ is formulated as follows:
\begin{equation}
    \mathcal{L}_{id}=\mathbb{E}_{( {x},{y})\sim \mathcal{M}_{id}}
[\ell_{\mathrm{CE}}({h}({x}),{y})]
    %\mathbb{E}_{(x,y)\sim\mathcal{M}_{id}}[-\log f_y(x)].
\end{equation}

\textbf{Semantically-Consistent Objective.}
We find that at the beginning of test-time optimization, $m_{out}^0$ may misclassify some ID samples as OOD.
Such misclassifications subsequently confuse the model during optimization. 
To address this problem, we propose to maintain the consistency between the predictions of the original model and the updated model.
Specifically, at the beginning of the testing stage, we make a duplicate of the model ${h}^0$ and freeze its parameters.
The prediction of the duplicated model is denoted as ${y}^0$.
Intuitively, if the results predicted by the model remain consistent with ${y}^0$, the performance on the source task will not degrade.
Let $p^t_{{y}}$ denote the softmax probability that the t-th sample belongs to the class ${y}$.
Our objective is:
\begin{equation}
    \mathcal{L}_{SC}=\begin{cases}
0,  & \text{ if } y^0=y^t \\
p^t_{y^t} - p^t_{y^0} + \phi, & \text{ if } y^0 \neq y^t
\end{cases},
\end{equation}
which enforces $p^t_{y^t}$ close to $p^t_{y^0}$ with a margin $\phi$. That means the prediction of $h^t$ is supposed to be higher than that of $h^0$ at least by $\phi$. In this work, we empirically set $\phi$ as a pre-defined value.
It is worth noting that $\phi$ should not be too large that it may influence the optimization between the pseudo-OOD sample and the uniform distribution.

\subsection{Modulation Parameters}
\label{4.3}
Let $\theta$ denote all the parameters of the model, 
updating $\theta$ is a natural choice, but it is sub-optimal for test-time OOD detection.
Therefore, we consider optimizing part of the network parameters.
While this operation is common in many tasks, there is still a lack of research on identifying which part should be updated to improve OOD detection performance efficiently.
Following the partial updating principle~\cite{wang2020tent}, we explore the influence of optimizing different combinations of parameters, \eg, the last feature block $\theta_{last}$, all batch normalization layers $\theta_{bn}$, and the classifier $\theta_{fc}$.
Table~\ref{tab:opti} displays the results that optimize the above combinations.
We finally optimize $\theta_{last}$ while keeping the remaining parameters fixed during testing.

\subsection{Overall Objective}
\label{4.4}
The overall test-time optimization objective is formulated as follows:
\begin{equation}\mathcal{L}_{total}=\mathcal{L}_{id}+\lambda_1\mathcal{L}_{ood}+\lambda_2\mathcal{L}_{SC},
\label{totalloss}
\end{equation}
where $\lambda_1,\lambda_2$ are set to 1 and 0.1, respectively.
To adequately leverage the information in the outlier and the memory bank,
we repeat the optimization process on each outlier iteratively for a given number of iterations, $T$.
While seemingly separated from each other, the three components of AUTO are working collaboratively. 
First, the in-out-aware filter selects high-quality ID and OOD samples from the unlabeled test data, which facilitates the positive update of models.
Second, the anti-forgetting components help model enlarges the margin between ID and OOD data, which paybacks to the filter and helps it select samples more accurately.
The entire training process converges when the three components perform satisfactorily.
The pseudo-code is illustrated in \textbf{Appendix}.

\setlength{\abovecaptionskip}{10pt}
\setlength{\belowcaptionskip}{-5pt}

\setlength{\tabcolsep}{2.5pt}
\begin{table*}[t]
    \resizebox{\textwidth}{!}{
    \centering
    \begin{tabular}{lrrrrrrrrrrrr}
    \toprule
    \makebox[0.1\textwidth][l]{\multirow{3}{*}{\textbf{Method}}} &
    \multicolumn{6}{c}{\textbf{CIFAR-10}} & \multicolumn{6}{c}{\textbf{CIFAR-100}}\\
    %\cmidrule(lr){2-7}\cmidrule(lr){8-13}
    &\multicolumn{3}{c}{\textbf{ResNet-34}}&\multicolumn{3}{c}{\textbf{WideResNet-40-2}}&\multicolumn{3}{c}{\textbf{ResNet-34}}&\multicolumn{3}{c}{\textbf{WideResNet-40-2}}\\
    \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}\cmidrule(lr){11-13}
    &\textbf{FPR95}$\downarrow$&\textbf{AUROC}$\uparrow$&\textbf{ID\_Acc}$\uparrow$&\textbf{FPR95}$\downarrow$&\textbf{AUROC}$\uparrow$&\textbf{ID\_Acc}$\uparrow$&\textbf{FPR95}$\downarrow$&\textbf{AUROC}$\uparrow$&\textbf{ID\_Acc}$\uparrow$&\textbf{FPR95}$\downarrow$&\textbf{AUROC}$\uparrow$&\textbf{ID\_Acc}$\uparrow$\\
    \midrule
    MSP \cite{hendrycks2016baseline}&46.49&92.53&94.87&52.00&90.57&94.53&83.53&74.34&77.51& 79.15&76.44&\textbf{75.84}\\
    ODIN\cite{liang2018enhancing}   &30.00&93.94&94.87 &34.32&91.38&94.53 &82.76&75.27&77.51 &69.75&81.29&\textbf{75.84}\\
    Mahalanobis \cite{lee2018simple}&44.31&93.31&94.87 &25.61&95.19&94.53 &75.56&80.82&77.51 &71.14&79.71&\textbf{75.84}\\
    Energy \cite{liu2020energy}     &28.77&94.07&94.87 &33.41&91.53&94.53 &82.65&75.33&77.51 &69.65&81.30&\textbf{75.84}\\
    ReAct \cite{sun2021react}       &32.57&93.16&94.85 &58.67&82.85&93.41 &74.76&82.01&77.09 &92.01&64.53&64.77\\
    Logit Norm \cite{wei2022mitigating} 
    &18.14&96.61&94.68 &21.03&95.86&94.42 &76.08&76.83&76.40 &54.90&87.60&76.02\\
    KNN \cite{sun2022knn}&36.71&94.15&94.87&36.63&93.31&94.53&71.33&82.44&77.51&59.92&84.36&\textbf{75.84}\\
    \midrule
    OE~\cite{hendrycks2018deep}&6.02&98.58&95.08&7.08&98.51&94.44&58.52&87.30&76.84&54.04&85.82&75.59\\
    Energy with $\mathcal{D}_{aux}$~\cite{liu2020energy}&\textbf{2.93}&\textbf{98.71}&\textbf{95.49}&\textbf{2.91}&\textbf{98.97}&\textbf{94.91}&53.02&90.05&77.19&44.43&90.47&75.75\\
    POEM~\cite{ming2022posterior}&11.25&97.62&89.57 &7.17&98.37&90.62 &19.78&95.94&69.49&24.30&\textbf{95.96}&69.37\\
    WOODS~\cite{katzsamuels2022training}&10.10&97.75&94.79&12.14&97.58&94.72&34.90&91.21&77.84&22.65&94.54&75.74\\
    DOE~\cite{wang2023outofdistribution}&8.93&97.84&94.74&5.00&98.75&94.43&32.43&93.65&76.95&26.09&94.43&74.98\\
    AUTO&6.47&98.64&94.98&9.45&97.94&93.33&\textbf{11.85}&\textbf{97.36}&\textbf{77.90} &\textbf{17.55}&95.55&73.71\\
    \bottomrule
    \end{tabular}
    }
    \caption{Comparison with competitive OOD detection methods on CIFAR benchmarks. $\uparrow$ indicates larger values are better and vice versa.
    All values are percentages and are averaged over six OOD test datasets described in Section \ref{setup}.
    Bold numbers indicate superior results. $\kappa$ in Eq. (\ref{open_equ}) is set to 0.5 in CIFAR benchmarks.}
    \label{tab:cifar}
\end{table*}
\section{Experiments}
In this section, we evaluate AUTO for test-time OOD detection on CIFAR10/100 and ImageNet benchmarks.
We compare AUTO with previous OOD detection methods, both OOD performance and ID performance (Section \ref{exp_res}).
Besides, we present extensive ablation experiments to validate the robustness of AUTO (Section \ref{abla}).

\subsection{Setup}
\label{setup}
\noindent \textbf{Benchmarks} 
Following the common benchmarks in OOD detection literatures~\cite{liu2020energy,du2022vos}, we evaluate our method on CIFAR-10/100 \cite{krizhevsky2009learning} and ImageNet \cite{deng2009imagenet}.
For CIFAR benchmarks, we consider six common OOD datasets: SVHN \cite{netzer2011reading}, Textures \cite{cimpoi2014describing}, LSUN-Crop \cite{yu2015lsun}, LSUN-Resize \cite{yu2015lsun}, iSUN \cite{xu2015turkergaze}, and Places365 \cite{zhou2017places}.
Images in CIFAR benchmarks are resized to $32 \times 32$.
For the ImageNet benchmark, we use subsets of four datasets from SUN \cite{xiao2010sun}, Textures \cite{cimpoi2014describing}, Place50 \cite{zhou2017places}, and iNaturalist \cite{van2018inaturalist}.
Images in the ImageNet benchmark are resized to $224 \times 224$.
We provide more details of the ID and OOD datasets and categories in \textbf{Appendix}.
The batch size is set to 1 during testing.

\noindent \textbf{Evaluation metrics.} 
We evaluate our framework and baseline methods using the following metrics: 1) The false positive rate of OOD samples when the true positive rate of in-distribution samples is at 95\% (FPR95); 2) The area under the receiver operating characteristic curve (AUROC); and 3) The ID classification accuracy (ID\_Acc).

\noindent \textbf{Training details.}
For CIFAR benchmarks, we train two backbones from scratch: ResNet-34 \cite{he2016identity} and the Wide ResNet \cite{zagoruyko2016wide} architecture with 40 layers and widen factor of 2.
For the ImageNet benchmark, we use a pre-trained ResNet-50 model \cite{he2016identity} from the PyTorch \cite{paszke2019pytorch} and a pre-trained Vision Transformer \cite{dosovitskiy2020image} from the Timm library \cite{rw2019timm}.

For modifying during testing, we use stochastic gradient descent with the learning rate set to that of the last epoch during training, which is 0.001 in all our experiments.
We set weight decay and momentum to zero during test-time OOD detection, inspired by practice in \cite{liu2018rethinking,he2019rethinking}.
We do not tune our hyperparameters for each $\mathcal{D}_{open}^{test}$ distribution, so that $\mathcal{D}_{open}^{test}$ is kept unknown like with open-world scenarios.
We use the cross-validation strategy to select $\lambda_1,\lambda_2,\phi,k_1,k_2$, which is shown in \textbf{Appendix}.

\textbf{Due to space constraints, additional experiments and ablation studies are included in Appendix.}
\subsection{Main Results}

\label{exp_res}

\textbf{AUTO outperforms baselines trained without auxiliary outliers.}
We compare AUTO with \emph{post hoc} OOD detection methods, which include MSP \cite{hendrycks2016baseline}, ODIN \cite{liang2018enhancing}, Mahalanobis \cite{lee2018simple}, Energy \cite{liu2020energy}, Logitnorm \cite{wei2022mitigating}, ReAct \cite{sun2021react}, DICE \cite{sun2022dice}, and KNN \cite{sun2022knn}.
Like \emph{post hoc} OOD detection methods, AUTO does not require additional modifications at training time.
As shown in Table \ref{tab:cifar}, AUTO outperforms all \emph{post hoc} methods by a large margin.
Compared with the best baseline in each data-net pair, AUTO reduces the FPR95 by \textbf{30.24}\% (ResNet-34) and \textbf{27.18}\% (WRN) on CIFAR-10 and by \textbf{59.48}\%(ResNet-34) and \textbf{42.37}\% (WRN) on CIFAR-100.
Besides, as shown in Table \ref{tab:mix_model}, Auto can further enhance OOD performance based on other \emph{post hoc} methods.
Accessing test OOD data greatly benefits AUTO with an acceptable inference time increase (as shown in Table \ref{tab:opti}).
The superior performance demonstrates that AUTO could be a complementary method for all well-trained models to efficiently excavate their intrinsic OOD discriminative capability with lightweight modifications during testing.

\begin{table}[t]
    \centering
    \begin{tabular}{lrrr}
    \toprule
        \multicolumn{1}{l}{\textbf{Methods}} & \textbf{FPR95 $\downarrow$} &\textbf{AUROC $\uparrow$}&\textbf{ID\_Acc $\uparrow$}\\
    \midrule
        ReAct~\cite{sun2021react}&32.57&93.16&94.85\\
        ReAct+AUTO &\textbf{6.13}&\textbf{98.69}&\textbf{94.93}\\
        \midrule
        LogitNorm~\cite{wei2022mitigating}&18.14&96.61&\textbf{94.68}\\
        LogitNorm+AUTO&\textbf{12.60}&\textbf{97.31}&93.49\\
    \bottomrule
    \end{tabular}
    \caption{Results on well-trained models from \emph{post hoc} methods. Model is trained on CIFAR-10 using ResNet-34.}
    \label{tab:mix_model}
\end{table}

\setlength{\tabcolsep}{3.0pt}
\begin{table*}[t]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{clrrrrrrrrrrr}
    \toprule
    \multicolumn{1}{c}{\multirow{3}{*}{\textbf{Backbone}}} &\multicolumn{1}{c}{\multirow{3}{*}{\textbf{Methods}}} & \multicolumn{8}{c}{\textbf{OOD Datasets}} & \multicolumn{2}{c}{\multirow{2}{*}{\textbf{Average}}}&\multirow{3}{*}{\textbf{ID\_Acc}$\uparrow$}\\ 
    \cmidrule(lr){3-10}
    \multicolumn{2}{c}{ }   & \multicolumn{2}{c}{\textbf{SUN}}   & \multicolumn{2}{c}{\textbf{Textures}}  & \multicolumn{2}{c}{\textbf{iNaturalist}} & \multicolumn{2}{c}{\textbf{Places}}  \\ 
    \cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}\cmidrule(lr){9-10}\cmidrule(lr){11-12}
    \multicolumn{2}{c}{}&\multicolumn{1}{c}{FPR95$\downarrow$} &AUROC$\uparrow$  & FPR95$\downarrow$ &AUROC$\uparrow$  & FPR95$\downarrow$ &AUROC$\uparrow$  & FPR95$\downarrow$ & \multicolumn{1}{c}{AUROC$\uparrow$}  & \multicolumn{1}{c}{FPR95$\downarrow$ }&AUROC$\uparrow$ \\ 
     \midrule
    \multicolumn{1}{l}{\multirow{12}{*}{\textbf{ResNet-50}}}&
    MSP\cite{hendrycks2016baseline}  & 68.53 & 81.75 & 66.15 & 80.46 & 52.69 & 88.42 & 71.59 & 80.63 & 64.74 & 82.82 & \textbf{76.12} \\ 
    &ODIN\cite{liang2018enhancing}& 54.04 & 86.89 & 45.50 & 87.57 & 41.50 & 91.38 & 62.12 & 84.45 & 50.79 & 87.57 & \textbf{76.12} \\ 
    &Mahalanobis\cite{lee2018simple}&98.35&42.10&54.78&85.02&96.95&52.60&98.47&42.01&87.14&55.43&76.12\\
    &Energy\cite{liu2020energy} & 58.25 & 86.73 & 52.30 & 86.73 & 53.94 & 90.60 & 65.40 & 84.12 & 57.47 & 87.05 & \textbf{76.12} \\ 
    &ReAct\cite{sun2021react}  & 23.69 & 94.44 & 46.33 & 90.30 & 19.71 & 96.37 & 33.30 & 91.96 & 30.76 & 93.27 & 74.82 \\ 
    &DICE+ReAct\cite{sun2022dice}&  26.49 & 93.83 & 29.36 & 92.65 & 20.07 & 96.11 & 38.35 & 90.61 & 28.57 & 93.30 & 67.01 \\ 
    &KNN\cite{sun2022knn}&70.50&80.46&11.26&97.41&60.30&86.09&78.81&74.66&55.22&84.66&\textbf{76.12}\\
    \cmidrule(lr){2-13}
    &OE*\cite{hendrycks2018deep}& 80.10&76.55&66.38&82.04&78.31&75.23&70.41&81.78&73.80&78.90&75.51\\
    &MixOE*\cite{zhang2023mixture}&74.62&79.81&58.00&85.83&80.51&74.30&84.33&69.20&74.36&77.28&74.62\\
    &VOS*\cite{du2022vos}&98.72&38.50&70.20&83.62&94.83&57.69&87.75&65.65&87.87&61.36&74.43\\
    &DOE*\cite{wang2023outofdistribution}&80.94&76.26&34.67&88.90&55.87&85.98&67.84&83.05&59.83&83.54&75.50\\
    &AUTO &\textbf{9.84} & \textbf{97.49} & \textbf{19.11} & \textbf{95.56} & \textbf{2.48} & \textbf{99.33} & \textbf{22.44} & \textbf{94.10} & \textbf{13.47} & \textbf{96.62} & 73.38 \\
    \midrule
    \multicolumn{1}{l}{\multirow{9}{*}{\textbf{ViT-B-16}}}& MSP\cite{hendrycks2016baseline}  & 73.80 & 79.49 & 63.07 & 81.50 & 39.40 & 92.41 & 74.09 & 79.56 & 62.59 & 83.24 & 78.01 \\ 
    &ODIN\cite{liang2018enhancing}  & 62.81 & 83.20 & 51.45 & 86.31 & 30.28 & 92.65 & 66.21 & 81.51 & 52.69 & 85.92 & 78.01 \\ &Mahalanobis\cite{lee2018simple}&79.88&81.82&72.10&80.33&18.22&95.37&84.05&73.70&63.57&82.81&78.01\\
    &Energy\cite{liu2020energy}  & 69.29 & 84.52 & 51.97 & 88.30 & 37.84 & 94.46 & 72.03 & 82.74 & 57.78 & 87.51 & 78.01 \\ 
    &ReAct\cite{sun2021react}    & 72.19 & 84.12&  53.17 & 88.12& 29.54 & 95.19 & 74.15 & 82.22 & 57.26 & 87.41 & 78.01 \\
    &KNN \cite{sun2022knn}& 51.01&89.46&41.12&90.55&7.32&98.50&54.08&88.31&38.38&91.71&78.01\\
    \cmidrule(lr){2-13}
    &VOS$^{\dagger}$\cite{du2022vos}&43.03&91.92&56.67&87.64&31.65&94.53&41.62&90.23&43.24&90.86&\textbf{79.64}\\
    &NPOS$^{\dagger}$\cite{tao2023nonparametric}&28.96&94.63&57.39&85.91&27.63&94.75&35.45&91.63&37.36&91.73&79.55\\
    &AUTO &\textbf{15.60} & \textbf{96.32} & \textbf{27.71} & \textbf{94.05} & \textbf{0.87} & \textbf{99.77} & \textbf{33.97} & \textbf{92.97} & \textbf{19.54} & \textbf{95.78} & 79.45\\
    \bottomrule
    \end{tabular}
    }
    \caption{Comparison with competitive OOD detection methods on the ImageNet benchmark. $\uparrow$ indicates larger values are better and vice versa. 
    We refer part of results directly from DOE~\cite{wang2023outofdistribution} (*) and NPOS \cite{tao2023nonparametric} ($^{\dagger}$).}
    \label{imagenet}
\end{table*}

\textbf{AUTO achieves competitive results compared with OE-based counterparts.}
In Table \ref{tab:cifar}, we contrast AUTO with OE-based methods, which include OE~\cite{hendrycks2018deep}, Energy~\cite{liu2020energy}, POEM~\cite{ming2022posterior}, WOODS~\cite{katzsamuels2022training}, and DOE~\cite{wang2023outofdistribution}.
All the baseline methods are fine-tuned using the same pre-trained model.
Despite not being optimal on CIFAR-10, AUTO's improvement is still remarkable, with \textbf{7.93}\% (ResNet-34) and \textbf{3.71}\% (WRN) better results on CIFAR-100.
Note that the tiny-ImageNet dataset is adopted as the auxiliary outlier, which is mainly different from the considered test OOD datasets.
Thus, we highlight that AUTO reduces the distribution gap between auxiliary outliers and test OOD data, resulting in superior performance.




\textbf{AUTO scales effectively to large datasets.}
Compared with CIFAR benchmarks, the ImageNet benchmark is more challenging due to the larger feature space (512 $\rightarrow$ 2048) and the larger label space (10/100 $\rightarrow$ 1000).
In Table~\ref{imagenet}, we evaluate AUTO on ImageNet using a ResNet backbone and a Vision Transformer.
For post hoc counterparts, we still perform better with both backbones.
For OE-based methods, ImageNet-21K-P is adopted as the auxiliary outlier.
Since OE-based methods on Imagenet have been published recently, and their codes have yet to be released, we refer to the results reported in original papers~\cite{wang2023outofdistribution,tao2023nonparametric} for now. 
Surprisingly, AUTO still significantly outperforms these latest works.
The above results suggest that adjusting models at test time is obviously more efficient than predicting the unknowns during training in these complicated scenarios.

\textbf{AUTO gets impressive OOD performance while maintaining comparable ID classification accuracy.}
Deployed models should maintain source task performance while detecting OOD examples in the open world.
We compare the ID classification accuracy of AUTO in Table~\ref{tab:cifar} and Table~\ref{imagenet} with \emph{post hoc} OOD detection methods.
AUTO improves classification accuracy on ResNet-34 and Vision Transformer.
Compared with the original models, it only underperforms on ResNet-50 and WideResNet-40-2
The gap is just 1.2\% on CIFAR-10, 1.13\% on CIFAR-100, and 2.74\% on ImageNet.
Such a loss is acceptable and will not affect the safe deployment of the model.

\begin{table}[t]
    \centering   
    \resizebox{\linewidth}{!}{
    \begin{tabular}{clrrr}
    \toprule
        \textbf{OOD data}& \textbf{Methods}&\textbf{FPR95 $\downarrow$} &\textbf{AUROC $\uparrow$}&\textbf{ID$\_$Acc $\uparrow$}\\
    \midrule
        &MSP~\cite{hendrycks2016baseline}&82.69&75.07&\textbf{78.05}\\
      Places365  &OE~\cite{hendrycks2018deep}&74.35&80.19&76.37\\
      SVHN  &WOODS~\cite{katzsamuels2022training}&73.32&80.35&\textbf{77.10}\\
        &AUTO&\textbf{42.16}&\textbf{87.59}&77.94\\
    \bottomrule
    \end{tabular}
    }
    \caption{Results on the mixed OOD scenario. Model is trained on CIFAR-100 using ResNet-34.}
    \label{tab:mix_data}
\end{table}

\textbf{AUTO can handle mixed OOD scenarios.}
Previous testing scenarios only include one OOD data and one ID data.
In this paper, we introduce mixed OOD scenarios.
Models will encounter at least two kinds of OOD data at test time.
Results in Table \ref{tab:mix_data} suggest that the models' performance in this new scenario differs from an arithmetic average of performances in single-OOD scenarios.
The intricate composition of data presents challenges for all methods. 
Nevertheless, AUTO continues to demonstrate exceptional performance, exhibiting a greater performance advantage over OE and WOODS.
This underscores AUTO's superior capability to handle mixed OOD scenarios.


\textbf{AUTO is flexible in time-series OOD scenarios.}
Except for the mixed OOD scenario, our paper also explores time-series OOD scenarios, where the environment may change over time.
This means that simply sampling and learning from the target scenario before deployment may not be effective, as the OOD data may change.
We evaluate this challenge and demonstrate that AUTO overcomes this problem by continuously adjusting models. 
As shown in Table \ref{tab:dynamic}, a model trained on CIFAR-100 encounters the LSUN-Resize and Places365 datasets sequentially. 
\emph{Post hoc} methods are not affected by this change, but WOODS performs well only on LSUN-Resize and underperforms significantly on Places365. 
In contrast, AUTO constantly modifies the model, enabling it to adapt to the new CIFAR-100/Places365 scenario.

\begin{table}[h]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{llrrr}
    \toprule
    \textbf{OOD}&\textbf{Method} & \textbf{FPR95} $\downarrow$&\textbf{AUROC} $\uparrow$&\textbf{ID$\_$Acc} $\uparrow$ \\
    \midrule
   \multirow{4}{*}{LSUN\_R} & MSP~\cite{hendrycks2016baseline}&84.87&80.81&77.51\\
    &OE~\cite{hendrycks2018deep}& 75.91&82.53&77.02\\
    &WOODS~\cite{katzsamuels2022training}& 24.34&94.74&77.80\\
    &AUTO& \textbf{0.74}&\textbf{99.82}&\textbf{78.02}\\
    \midrule
    \multirow{4}{*}{Places365} & MSP~\cite{hendrycks2016baseline}&80.81&76.05&77.51\\
    &OE~\cite{hendrycks2018deep}& 63.36&86.23&76.32\\
    &WOODS~\cite{katzsamuels2022training}& 75.87&81.74&\textbf{78.00}\\
    &AUTO& \textbf{51.75}&\textbf{88.63}&77.42\\
    \bottomrule
    \end{tabular}
    }
    \caption{Results on the test-seires OOD scenario. ResNet-34 trained on CIFAR-100 will encounter LSUN-Resize and Place365 sequentially.}
    \label{tab:dynamic}
\end{table}


\textbf{AUTO further enhances the OOD detection performance of models trained with auxiliary outliers.}
The aforementioned findings suggest that the incorporation of AUTO can effectively enhance the ability of models trained solely on ID data to detect OOD samples.
To further investigate the potential of AUTO in improving OOD detection performance for models trained with auxiliary outliers, we conducted additional experiments.
As presented in Table \ref{tab:oe_model}, with the help of AUTO, models trained with auxiliary outliers improve their OOD detection performance further.
Moreover, results from Tables \ref{tab:dynamic} and \ref{tab:oe_model} also reveal that the model trained on LSUN-Resize before being deployed to Places365 achieves better performance than the model directly deployed to Places365.
These findings demonstrate the versatility of AUTO.

\begin{table}[h]
    \centering
    \begin{tabular}{lrrr}
    \toprule
        \textbf{Model} & \textbf{FPR95} $\downarrow$ &\textbf{AUROC} $\uparrow$ & \textbf{ID\_Acc} $\uparrow$\\
         \midrule
        AUTO&52.35&87.57&76.24\\
        \midrule
        OE~\cite{hendrycks2018deep} & 63.36&86.23&\textbf{76.84}\\
        OE+AUTO&\textbf{51.02}&\textbf{88.32}&75.67\\
        \midrule
        WOODS~\cite{katzsamuels2022training} &62.50&81.99&\textbf{77.75}\\
        WOODS+AUTO&\textbf{46.98}&\textbf{88.99}&75.23\\
         \bottomrule
    \end{tabular}
    \caption{Results on CIFAR-100/Places365 with ResNet-34, AUTO enhances OE-based methods.}
    \label{tab:oe_model}
\end{table}


\subsection{Ablation Study}
\label{abla}
\textbf{Modulation parameters.}
We evaluate the impact of different optimization objects, as presented in Table~\ref{tab:opti}.
On the one hand, taking both ID and OOD tasks into account, the performance of optimizing the last feature block is superior.
On the other hand, models in open-world scenarios, particularly those engaged in online stream applications, need to notice the optimization efficiency.
We note that the inference time per sample is approximately 5ms.
AUTO necessitates only a modest 2.4x increase in processing time. 
Such an increment in time cost is absolutely tolerable.
Thus, we conclude that the utilization of the last feature block as the optimization objective is a more efficient strategy.

\begin{table}[h]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lrrrr}
        \toprule
        \multicolumn{1}{c}{\textbf{Modu. Para.}}& \textbf{FPR95} $\downarrow$&\textbf{AUROC} $\uparrow$&\textbf{ID\_Acc} $\uparrow$&\textbf{Time} \\
        \midrule
        %Energy(w. $\matchcal{D}_{aux}$) & 8.3min\\
        No Para.& 83.53&74.34&77.51& 1x \\
        Block 1 & 77.72&78.77&\textbf{78.27}&1.8x\\
        Block 2 & 79.59&77.44&77.47&2.3x\\
        Block 3 & 35.18&90.23&75.38&2.9x\\
        Block 4 & \textbf{11.85}&\textbf{97.36}&77.90&3.4x\\
        BN      & 20.21&95.19&77.32&3.1x\\
        FC      & 80.01&77.92&78.02&1.5x\\
        All Para.& 35.46&88.16&64.57& 11.2x\\
        \bottomrule
    \end{tabular}
    }
    \caption{Ablation study on different modulation parameters. Model is trained on CIFAR-100 with ResNet-34.}
    \label{tab:opti}
\end{table}

\begin{figure*}[t]
    %\centering
    \flushleft
    \setlength{\abovecaptionskip}{10pt}
    \setlength{\belowcaptionskip}{-5pt}
    \subfloat[FPR95 on different $\lambda_{2}$]{
        \label{fig:subfig:a}
        \includegraphics[width=0.245\textwidth]{lambda2_fpr.png}}
    \subfloat[ID\_Acc on different $\lambda_{2}$]{
        \label{fig:subfig:b}
        \includegraphics[width=0.245\textwidth]{lambda2_acc.png}}
    \subfloat[FPR95 on different $\phi$]{
        \label{fig:subfig:c}
        \includegraphics[width=0.245\textwidth]{phi_fpr.png}}
    \subfloat[ID\_Acc on different $\phi$]{
        \label{fig:subfig:d}
        \includegraphics[width=0.245\textwidth]{phi_acc.png}}
    \caption{Performance of AUTO with varying $ \{\lambda_2,\phi\}$ on ResNet. Average FPR95 and ID\_Acc are reported.}
    \label{fig:anti}
\end{figure*}

\textbf{Components in AUTO.}
Results presented in Table~\ref{obj_com} evaluate the efficacy of various components.
Our results demonstrate that training the model solely with an ID memory bank leads to similar performance as the method without optimization, indicating that optimizing on ID data alone does not effectively enhance OOD detection. Furthermore, while training on outliers alone improves OOD detection, it results in catastrophic forgetting, as evidenced by the decline in ID classification accuracy.
With the help of the ID memory bank, the model jointly updated by both ID and OOD samples already exhibits progress in both OOD detection and ID classification.
However, we want to achieve a higher level of ID performance.
Our results show that the incorporation of a semantically-consistent objective makes this goal come true.

\begin{table}[h]
    \centering
    \begin{tabular}{cccrrr}
    \toprule
    $\mathcal{L}_{\textrm{id}}$&$\mathcal{L}_{\textrm{ood}}$&
    \multicolumn{1}{c}{$\mathcal{L}_{\textrm{SC}}$} &\textbf{FPR95 $\downarrow$} &\textbf{AUROC $\uparrow$}&\textbf{ID\_Acc $\uparrow$}  \\
    \midrule
    $\checkmark$&~&~&79.89&76.34&77.50 \\
    &$\checkmark$&&60.26&79.36&65.92\\
    $\checkmark$&$\checkmark$&&12.54&97.12&77.55\\
    $\checkmark$&$\checkmark$&$\checkmark$&\textbf{11.85}&\textbf{97.36}&\textbf{77.90}\\
    \bottomrule
    \end{tabular}
    \caption{Ablation study on different combinations of objectives. Model is trained on CIFAR-100 with ResNet-34.}
    \label{obj_com}
\end{table}



\textbf{Parameter analysis of $\mathcal{L}_{SC}$.}
We conduct a parameter analysis of $\mathcal{L}_{SC}$ to evaluate the impact of $\lambda_2$ and $\phi$ on the model.
Firstly, we examine the effect of different values of $\lambda_2$ in Figure~\ref{fig:subfig:a} and \ref{fig:subfig:b}, with $\phi=0.2$.
Empirical results demonstrate that a larger $\lambda_2$ prioritizes the ID classification task over OOD detection.
The larger $\lambda_2$ also weakens the ID information learned from dynamic ID memory, harming both ID and OOD tasks.
Furthermore, we investigate how the parameter $\phi$ affects OOD detection and ID classification in Figure~\ref{fig:subfig:c} and \ref{fig:subfig:d}, with $\lambda_2=0.1$.
Our findings confirm that $\phi$ should not be excessively large to preserve the optimization between OOD examples and the uniform distribution.
While the impact of $\phi$ on ID performance is minimal, as shown in Table \ref{obj_com}, it does supplement part of the remaining ID deficiencies from dynamic ID memory.

\begin{table}[h]
    \centering
    \begin{tabular}{lrrrrrr}
    \toprule
       $T=2$   & $\kappa=$0.9& 0.7& 0.5& 0.3& 0.1\\
       \midrule
        Texture & 44.86&23.05&19.11&\textbf{16.49}&23.50\\
        Places365&77.80&52.87&47.54&46.34&\textbf{42.12}\\
        \midrule
        $\kappa=0.5$&  $T=$ 0& 1& 2& 3& 5\\
       \midrule
        Texture & 78.55&22.52&\textbf{19.11}&19.65&19.22\\
        Places365&81.50&51.34&47.53&46.16&\textbf{45.82}\\
        \bottomrule
    \end{tabular}
    \caption{Ablation study on $\kappa$ and $T$, ResNet-34 is trained on CIFAR-100, FPR95 is reported. Lower is better.}
    \label{tab:fraction}
\end{table}

\textbf{Sensitivity analysis on $\kappa$ and $T$.}
We conduct a sensitivity analysis on the parameters $\kappa$ and $T$, as summarized in Table~\ref{tab:fraction}.
Increasing the percentage of OOD data (decreasing $\kappa$) in the test set generally improves the performance of AUTO, as it allows the model to access more OOD samples and make more iterations.
The model's performance sometimes suffers from an increase in OOD data, as it can contaminate the ID memory, more details can be found in \textbf{Appendix}.
Subsequently, we examined the effect of $T$, the number of iterations when each OOD sample is encountered.
Multiple iterations can expedite the model's convergence and facilitate the detection of OOD data that may have been missed during the convergence process, thereby improving model performance.
However, we found that the incremental benefits diminish as $T$ increases beyond two iterations.
Considering both performance and speed, we recommend setting $T=2$ as the standard iteration frequency.

\begin{table}[h]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cccrrr}
    \toprule
    \textbf{Samples}&\textbf{$\mu_{in}$}&\textbf{$\sigma_{in}$}&\textbf{FPR95} $\downarrow$&\textbf{AUROC $\uparrow$}&\textbf{ID$\_$Acc} $\uparrow$\\
    \midrule
      100   &0.9985&0.0024&47.03&89.51&77.59\\
      1000  &0.9976&0.0145&46.66&89.59&\textbf{77.70}\\
      10000 &0.9976&0.0115&\textbf{45.94}&\textbf{89.72}&77.65\\
      50000 (All)&0.9977&0.0111&46.27&89.67&77.53\\
      \bottomrule
    \end{tabular}
}
    \caption{Results on the Places365 \cite{zhou2017places} dataset, model is trained on CIFAR-100 using ResNet-34. Utilizing fewer ID samples to initialize AUTO is more efficient.}
    \label{tab:samples}
\end{table}

\textbf{Approximate estimation of the mean and standard deviation on training ID data.}
In order to initialize the in-out-aware filter, the statistics of the training ID data $\mu_{in}$ and $\sigma_{in}$ are estimated using all available training samples.
This method introduces high computation costs before deployments. 
Actually, the statistics can be approximated using a small subset of the training data.
Table \ref{tab:samples} presents these approximate statistics and corresponding performances.
Our findings suggest that approximate statistics facilitate the initialization of AUTO, significantly reducing computation costs and streamlining model deployment.


\section{Conclusion}
This paper introduces a novel setting called \emph{test-time out-of-distribution detection}, whereby models directly optimize with the unlabeled online test data.
We present AUTO, which consists of three key components: an in-out-aware filter, a dynamic ID memory bank, and a semantically-consistent objective.
AUTO excavates a well-trained model's intrinsic OOD discriminative capability from a unique test-time online optimization perspective.
Extensive results demonstrate that AUTO can improve OOD detection performance substantially while maintaining the accuracy of ID classification.
We hope our work could serve as a springboard for future works, provide new insights for revisiting the model development in OOD detection, and draw more attention toward the testing phase.
  

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
{\Large{\noindent\textbf{Appendix}}}
\section{Pesudo-code}
We present our pesudo-code in Alg. \ref{alg1}.
We calculate the MSP \cite{hendrycks2016baseline} score for the current test sample and identify the sample with the proposed in-out-aware filter.
Our system will be updated once the sample is identified as a pseudo-ID or pseudo-OOD sample.
\begin{algorithm}[h]
    \algsetup{linenosize=\small} %\scriptsize
    \caption{Pseudocode of AUTO in a PyTorch-like style.}
    \label{alg1}
    \begin{algorithmic}
    {\small
    \STATE  {\color{GG}\#M\_id: ID memory bank with one sample each class}
    \STATE  {\color{GG}\#m\_i,m\_o: inner-aware margin and outlier-aware margin for selecting test samples.}
    \STATE {\color{GG}\#f\_t: the updated model when the t-th sample inputs}
    \STATE For x in loader: {\color{GG}\# load one sample each time}
    \INDSTATE sm=softmax(f\_t(x)) {\color{GG}\# softmax: 1xC}
    \INDSTATE msp=max(sm) {\color{GG}\# msp score}
    \INDSTATE if msp$>$m\_i: {\color{GG}\# recognized as an ID sample}
    \INDSTATE[2] pred=argmax(sm) {\color{GG}\# prediction of x}
    \INDSTATE[2] {\color{GG}\# search sample with same label in M\_id}
    \INDSTATE[2] for i in range len (M\_id): 
    \INDSTATE[3]    if pred==y\_i: {\color{GG}\# y\_i: labels in M\_id.}
    \INDSTATE[4]        x\_i=x {\color{GG}\# replace sample}
    \INDSTATE if msp$<$m\_o: {\color{GG}\#recognized as an OOD sample.}
    \INDSTATE[2]{\color{GG}\# training T iterations on each sample}
    \INDSTATE[2] ind=0
    \INDSTATE[2] while ind$<$T:
    %\INDSTATE[3]    {\color{GG}\# the overall loss, Eqn. (11)}
    %\INDSTATE[3]    loss=L\_total(x,M\_id)
    \INDSTATE[3]    sm\_t=softmax(f\_t(x))
    \INDSTATE[3]    {\color{GG}\# CELoss: CrossEntropyLoss}
    \INDSTATE[3]    {\color{GG}\# u:uniform distribution}
    \INDSTATE[3]    loss=CELoss(sm\_t, u) 
    \INDSTATE[3]    {\color{GG}\# optimize with ID memory bank}
    \INDSTATE[3]    for i in range len (M\_id):
%    \INDSTATE[4]        {\color{GG}\# CELoss: CrossEntropyLoss}
    \INDSTATE[4]        loss+=CELoss(f\_t(x\_i),y\_i)
    \INDSTATE[3]    {\color{GG}\# semantic consistency}
    \INDSTATE[3]    sm\_0=softmax(f\_0(x))
    \INDSTATE[3]    pred\_t=argmax(sm\_t)
    \INDSTATE[3]    pred\_0=argmax(sm\_0)
    \INDSTATE[3]    if not (pred\_t == pred\_0):
    \INDSTATE[4]        loss+=sm\_t[pred\_t]-sm\_t[pred\_0]+phi
    \INDSTATE[3]    {\color{GG}\#update model}
    \INDSTATE[3]    loss.backward()
    \INDSTATE[3]    update(f\_t.params)
}    
    \end{algorithmic}
\end{algorithm}



\section{Experiments}
\subsection{Details of Datasets}
Firstly, we summarize the ID configurations in Table \ref{tab:id_data}.
For CIFAR benchmarks, we use the standard split with 50,000 training and 10,000 test images.
For the ImageNet benchmark, we use the standard validation split with 50,000 images as ID samples at test time.
\begin{table}[h]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cccc}
    \toprule
        Dataset & Training Set ($\mathcal{D}_{id}^{train}$)& Test Set ($\mathcal{D}_{id}^{test}$)\\
    \midrule
        CIFAR-10/100 &50000&10000\\
        ImageNet &1281167&50000\\
    \bottomrule
    \end{tabular}
    }
    \caption{ID data configurations.}
    \label{tab:id_data}
\end{table}

Then, we summarize the OOD configurations for CIFAR and ImageNet benchmarks.

For CIFAR benchmarks, we follow the common OOD settings in \cite{liu2020energy}.
Details are shown in Table \ref{cifar:ood_data}.
\begin{table}[h]
    \centering
    \begin{tabular}{lccc}
    \toprule
        \multirow{2}{*}{Dataset} & \multirow{2}{*}{Test OOD ($\mathcal{D}_{ood}^{test}$)} &\multicolumn{2}{c}{$\mathcal{D}_{aux}^{wood}$}\\
        \cmidrule(lr){3-4}
        & & $\mathcal{D}_{ood}^{test}$ &$\mathcal{D}_{id}^{test}$\\
    \midrule
        SVHN&10k&5k&5k\\
        Textures&5640&2820&2820\\
        LSUN\_C&10k&5k&5k\\
        LSUN\_R&10k&5k&5k\\
        Places365&10k&5k&5k\\
        iSUN&8925&4462&4463\\
    \bottomrule
    \end{tabular}
    \caption{OOD data for CIFAR benchmarks.}
    \label{cifar:ood_data}
\end{table}

We use 80 Million Tiny Images \cite{torralba200880} dataset as $\mathcal{D}_{aux}$ for OE-based methods.
The 80 Million Tiny Images is a large-scale, diverse dataset of 3232 natural images scrapped from the Internet.
Following the OE setting, we remove all examples of 80 Million Tiny Images which appear in the CIFAR datasets.
In particular, we change the auxiliary data set $\mathcal{D}_{aux}^{wood}$ for the WOODS method according to the number of OOD samples (Table \ref{cifar:ood_data}). 
For $\mathcal{D}_{id}^{train}$, 45000 images are utilized for training the model, and the other 5000 images are utilized for validation.

For the ImageNet benchmark, we follow the common setting from \cite{huang2021mos}.
Details are shown in Table \ref{cifar:ood_data}.

\begin{table}[h]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|cccccc}
    \toprule
        Dataset & iNaturalist & Textures& Places50& SUN\\
        Test OOD ($\mathcal{D}_{ood}^{test}$)&10k&5640&10k&10k\\
    \bottomrule
    \end{tabular}
    }
    \caption{OOD data for ImageNet benchmarks.}
    \label{imagenet:ood_data}
\end{table}

We directly use the pre-trained model in the ImageNet benchmark, the training set $\mathcal{D}_{id}^{train}$ is utilized to sample the ID memory bank.

For both CIFAR and ImageNet benchmarks, the main results reported in our paper are tested on the test set, which is denoted as:
\begin{equation}
    \mathcal{D}_{test} = \mathcal{D}_{ood}^{test} + \mathcal{D}_{id}^{test}.
\end{equation}

\subsection{Details of Models}
For training the classification models on CIFAR-10/100 data, we use the ResNet-34 \cite{he2016identity} and the Wide ResNet \cite{zagoruyko2016wide} architecture with 40 layers and widen factor of 2.
These models are optimized by stochastic gradient descent with Nesterov momentum \cite{duchi2011adaptive}.
We set the momentum to 0.9 and the weight decay coefficient to 0.0005.
Models are trained for 200 epochs.
The start learning rate is 0.1 and decays by a factor of 10 at epochs 80 and 140.
We use a batch size of 128 and a dropout rate of 0.3.

For training OOD detectors with auxiliary data, we follow the OE\cite {hendrycks2018deep} and energy \cite{liu2020energy} setting, we use the default: $\lambda=0.5$ in OE; $m_{in}=-7$,  $m_{out}=-25$ and $\beta=0.1$ in energy. Models are trained from scratch for 200 epochs.

\subsection{Details of Baselines}
To evaluate the baselines, we follow the original definition in MSP \cite{hendrycks2016baseline}, ODIN score \cite{liang2018enhancing}, Mahalanobis score \cite{lee2018simple}, Energy score \cite{liu2020energy}, ReAct \cite{sun2021react}, LogitNorm \cite{wei2022mitigating}, DICE \cite{sun2022dice}, and KNN distance \cite{sun2022knn}.

For ODIN, we set the temperature $T$ as 1000.

For Energy, the temperature is set to be $T = 1$.

For ReAct, the rectification threshold $p$ is set to 1.
More discussions are shown in Table \ref{tab:react}.

For LogitNorm, we set the temperature hyperparameter $\tau$ as 0.04 for CIFAR-10 and 0.0085 for CIFAR-100. More discussions are shown in Table \ref{tab:logit}.

For DICE, the sparsity ratio is set to $90\%$.

For POEM and DOE, models are trained from scratch.

\section{Results and Discussions.}
\subsection{Discussion on Existing Methods}
Before the proposition of AUTO, we systemically study the existing methods and find some experiment results interesting.
These methods substantially improve OOD detection performance with a fixed setting, but sometimes they will unintentionally make a negative optimization.

\textbf{Effectiveness of ReAct \cite{sun2021react}.}
ReAct \cite{sun2021react} is a successful method to improve OOD detection performance in most of scenarios, the central hyperparameter -rectification threshold- always set to a small value (\eg 1,2).
However, we find that ReAct is negative when the rectification threshold $p$ is set small for the model trained on ImageNet \cite{deng2009imagenet} using Vision Transformer \cite{dosovitskiy2020image}.
Results is shown in Table \ref{tab:react}.
We believe that if $p$ is dynamically adjusted during testing, the influence of ReAct would be positive.
\begin{table}[h]
    \centering
    \begin{tabular}{lrrr}
        \toprule
        $p$ & \textbf{FPR95} $\downarrow$& \textbf{AUROC} $\uparrow$ &\textbf{ID $\_$Acc} $\uparrow$\\
        \midrule
        1 &92.20&76.51&77.19\\
        10&61.08&86.82&77.84\\
        15&58.03&87.25&\textbf{78.06}\\
        20&\textbf{57.26}&87.41&78.01\\
        25&57.33&\textbf{87.47}&78.02\\
        50&\textbf{57.26}&87.05&78.01\\
        $+\infty$&\textbf{57.26}&87.05&78.01\\
        \bottomrule
    \end{tabular}
    \caption{Effect of rectification threshold $p$ for inference. Model is trained on ImageNet using Vision Transformer. Results are averaged on four OOD datasets.}
    \label{tab:react}
\end{table}

\textbf{Effectiveness of LogitNorm \cite{wei2022mitigating}.}
LogitNorm \cite{wei2022mitigating} substantially mitigate the overconfident problem of neural networks.
It can be easily adopted in practical settings without changing the loss and training theme.
However, the performance of this method is heavily influenced by its temperature parameter $\tau$, which modulates the magnitude of the logits.
Authors of Logitnorm state that $\tau$ should decrease as the number of categories increases.
For instance, it is set to 0.04 on CIFAR-10 and 0.01 on CIFAR-100 \cite{krizhevsky2009learning}.
However, we discover that this conclusion is empirical.
Effect on the $\tau$ is shown in Table \ref{tab:logit}.
It suggests that we need to dynamically search for the best value of $\tau$ at a finer granularity level during testing.
Because the best $\tau$ may be different in various test scenarios.
\begin{table}[h]
    \centering
    \begin{tabular}{lrrr}
    \toprule
        $\tau$ &  \textbf{FPR95} $\downarrow$& \textbf{AUROC} $\uparrow$ &\textbf{ID $\_$Acc} $\uparrow$\\
        \midrule
        0.01&76.08&76.83&76.40\\
        0.02&71.43&78.32&76.86\\
        0.04&\textbf{69.89}&\textbf{79.67}&76.82\\
        0.05&70.36&76.24&\textbf{77.71}\\
        \bottomrule
    \end{tabular}
    \caption{Effect of a temperature parameter $\tau$ for inference. Model is trained on CIFAR-100 using ResNet-34. Results are averaged on six OOD datasets.}
    \label{tab:logit}
\end{table}

\subsection{More Results}
\textbf{The influence of hyperparameters. }
Hyperparameters in our work are set as follows:
$\lambda_1=1, \lambda_2=0.1, \phi=0.2, T=2$, these parameters have been discussed in the main text.
The initialization of the inner-aware and outlier-aware margins are validated in the following experiments.
We evaluate the value of $k_1,k_2$ with the cross-validation strategy.
For instance, we utilize the Texture \cite{cimpoi2014describing} dataset to choose $k_1,k_2$ when the model is deployed in the scenario which is consists of CIFAR-100 and SVHN \cite{netzer2011reading}.
Considering variations of the ID data and OOD data, we choose a set of parameters for each model.
Results are shown in Table \ref{tab:k} and Table \ref{tab:k_cifar}.
\begin{table}[h]
    \centering
    \begin{tabular}{lrrr}
    \toprule
       $k_1,k_2$  &  \textbf{FPR95} $\downarrow$& \textbf{AUROC} $\uparrow$ &\textbf{ID $\_$Acc} $\uparrow$\\
    \midrule
       0,0&\textbf{27.71}&\textbf{94.05}&\textbf{79.12}\\
       0,1&48.68&90.84&78.27\\
       0,2&50.96&89.90&78.61\\
       0,3&53.14&88.04&78.15\\
    \bottomrule
    \end{tabular}
    \caption{Ablation study on the initialization of in-out-aware margins $k_1,k_2$, model is trained on ImageNet using Vision Transformer.
    AUTO is tested on the Texture dataset.}
    \label{tab:k}
\end{table}
\begin{table}[h]
    \centering
    \begin{tabular}{lrrr}
    \toprule
       $k_1,k_2$  &  \textbf{FPR95} $\downarrow$& \textbf{AUROC} $\uparrow$ &\textbf{ID $\_$Acc} $\uparrow$\\
    \midrule
       0,0&\textbf{18.00}&\textbf{96.17}&77.78\\
       0,1&18.02&96.13&77.76\\
       0,2&18.16&96.10&\textbf{78.02}\\
       0,3&18.07&96.15&77.78\\
    \bottomrule
    \end{tabular}
    \caption{Ablation study on the initialization of in-out-aware margins $k_1,k_2$, model is trained on CIFAR-100 using ResNet-34 \cite{he2016identity}.
    AUTO is tested on the Texture dataset.}
    \label{tab:k_cifar}
\end{table}

We find that $k_1,k_2$ are insensitive on CIFAR-10/CIFAR-100 benchmarks.
Although the performance is similar, a bigger interval means more samples are utilized to modify models.
Thus, the increased computational cost due to the initialization is noteworthy.
In summary, we set $k_1=0,k_2=3$ for models trained on all ID datasets using ResNet/WideResNet \cite{zagoruyko2016wide} architectures.
For models trained on the ImageNet using the Vision Transformer, we set $k_1=0,k_2=0$.

\textbf{A class prototype memory bank enhances both ID and OOD performance.}
When sufficient ID data is available, it is possible to build a fixed class prototype memory bank. 
Table \ref{tab:memory} shows that incorporating such a memory bank can facilitate further improvements in both OOD and ID performance. 
However, it may not be practical to construct such a memory bank, particularly if ID samples are scarce. Additionally, while the class prototype memory bank slightly outperforms dynamically updating the memory bank using test data, the latter approach is more convenient and flexible. 
Therefore, we have opted for the latter approach in AUTO.

\begin{table}[h]
    \centering
    %\resizebox{\linewidth}{!}{
    \begin{tabular}{llrrr}
    \toprule
         $\kappa$&\textbf{Memory}& \textbf{FPR95} $\downarrow$& \textbf{AUROC} $\uparrow$ &\textbf{ID $\_$Acc} $\uparrow$\\
     \midrule
        \multirow{2}{*}{0.5}&Proto.&\textbf{13.21}&\textbf{96.97}&\textbf{78.27} \\
         &Update&13.29&96.91&77.16\\
         \cmidrule(lr){2-5}
         \multirow{2}{*}{0.9}&Proto.& \textbf{11.84}&\textbf{97.42}&78.31\\
         &Update&13.14&96.92&\textbf{78.39}\\
         \bottomrule
    \end{tabular}
    %}
    \caption{AUTO (MSP) results with different memory bank. The Proto. represents the class prototype memory bank. Models are trained on CIFAR-100 with ResNet-34 and tested on six OOD datasets. Average results are reported.}
    \label{tab:memory}
\end{table}



\textbf{A gradually reducing weighting factor for $\mathcal{L}_{SC}$.}
As we mentioned in the main text, setting a large $\lambda_2$ for $\mathcal{L}_{SC}$ can lead to underperformance on both ID and OOD tasks. 
To address this issue, we propose a gradually decreasing weighting factor $\beta$, which decreases as the number of iterations increases. 
Table \ref{tab:reduce} demonstrates that this factor effectively prevents the degradation of OOD performance, but it also reduces the gain of $\mathcal{L}_{SC}$ on ID performance. 
Since $\mathcal{L}_{SC}$ is initially proposed to enhance ID performance, we decide not to incorporate this factor into our framework.

\begin{table}[h]
    \centering
    \begin{tabular}{lrrr}
    \toprule
       $\mathcal{L}_{SC}$  & \textbf{FPR95} $\downarrow$& \textbf{AUROC} $\uparrow$ &\textbf{ID $\_$Acc} $\uparrow$\\
    \midrule
        with $\beta$ & \textbf{10.92}&\textbf{97.38}&77.83\\
        w/o $\beta$& 11.85&97.36&\textbf{77.90}\\
    \bottomrule
    \end{tabular}
    \caption{A gradually reducing weighting factor for $\mathcal{L}_{SC}$ can enhance OOD detection but reduces the gain of $\mathcal{L}_{SC}$ on ID performance. Models are trained on CIFAR-100 with ResNet-34 and tested on six OOD datasets. Average results are reported.}
    \label{tab:reduce}
\end{table}


\textbf{Results on test-time methods.}
Existing test-time optimization methods have primarily focused on adaptation tasks.
Although these methods are not developed with OOD detection in mind, as a paradigm for accessing data in the testing phase, it is still necessary to provide a comparison and explanation.
In Table \ref{tab:TENT}, we investigate two settings where models are trained on CIFAR-100 during training and tested on (1) CIFAR-100/Places365 scenario or (2) CIFAR-100-C/Places365 scenario.
We can summarize that (1) OOD data proves to be detrimental to Tent, which attempts to fit the OOD data into the known data distribution. This not only fails to detect OOD data, but also harms ID performance.
(2) Introducing covariate shift to ID data during testing is a new scenario that cannot be covered by OOD detection methods. Additionally, this new scenario is not aligned with the OOD detection paradigm.
\setlength{\tabcolsep}{2.5pt}
\begin{table}[h]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{llrrr}
    \toprule
        \textbf{Test Data} &\textbf{Method}& \textbf{FPR95} $\downarrow$& \textbf{AUROC} $\uparrow$ &\textbf{ID $\_$Acc} $\uparrow$\\
    \midrule
         \multirow{3}{*}{\shortstack{CIFAR-100 \\Places365}}&MSP~\cite{hendrycks2016baseline}&83.53&74.34&\textbf{77.51}\\
         &Tent~\cite{wang2020tent}&86.32&68.29&67.01\\
         &AUTO&\textbf{13.29}&\textbf{96.91}&77.16\\
    \midrule
    \multirow{3}{*}{\shortstack{CIFAR-100-C \\Places365}}&MSP~\cite{hendrycks2016baseline}&92.75&54.25&20.49\\
         &Tent~\cite{wang2020tent}&\textbf{90.05}&\textbf{61.15}&\textbf{59.45}\\
         &AUTO&100.00&3.20&19.91\\
    \bottomrule
    \end{tabular}
    }
    \caption{ID data are changed at test time. Models are trained on CIFAR-100 with ResNet-34 during training.}
    \label{tab:TENT}
\end{table}

The results presented in Table \ref{tab:TENT} demonstrate that OOD detection methods fail to perform in the CIFAR-100-C scenario. 
To further investigate this phenomenon, we conduct additional experiments as shown in Table \ref{tab:100-c}, where we train our model on CIFAR-100 and evaluated its performance on a CIFAR-100/CIFAR-100-C scenario. Our findings reveal that OOD detection methods treat CIFAR-100-C data as OOD data, as evidenced by the MSP method successfully detecting CIFAR-100-C, similar to its detection of Places365.
Meanwhile, by utilizing AUTO, the pre-trained model is able to accurately distinguish between CIFAR-100-C and CIFAR-100 during testing.

\setlength{\tabcolsep}{2.5pt}
\begin{table}[h]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{llrrr}
    \toprule
        \textbf{Test Data} &\textbf{Method}& \textbf{FPR95} $\downarrow$& \textbf{AUROC} $\uparrow$ &\textbf{ID $\_$Acc} $\uparrow$\\
    \midrule
        \multirow{2}{*}{\shortstack{CIFAR-100 \\CIFAR-100-C}}&MSP &85.37&73.96&77.51\\
        &AUTO & \textbf{5.61}&\textbf{98.69}&\textbf{78.06}\\
    \bottomrule
    \end{tabular}
    }
    \caption{AUTO identify CIFAR-100-C (covariance shift) data as OOD samples.}
    \label{tab:100-c}
\end{table}

Based on the aforementioned experimental results, it is evident that attempting to force Tent to fit OOD data or OOD detection methods to fit ID data with covariance shift is unreasonable. 
It is clear that novel experimental settings (CIFAR-100 $\rightarrow$ CIFAR-100-C/Places365) necessitate the development of innovative methodologies.

\textbf{AUTO outperforms the test-time self-supervised OOD detection method.}
As mentioned in Related Works, ELET \cite{fan2022simple} is an OOD detection method that learns a detector from scratch at test time.
We compare AUTO and ELET in Table \ref{tab:elet}.
Results show that AUTO is better than ELET.
\begin{table}[h]
    \centering
    \begin{tabular}{lrr}
    \toprule
        \multirow{2}{*}{\textbf{Method}} &  \multicolumn{2}{c}{\textbf{FPR95 $\downarrow$ /AUROC $\uparrow$}}\\
        \cmidrule(lr){2-3}
         & CIFAR-10&CIFAR-100\\
    \midrule
    ELET~\cite{fan2022simple}&18.57/94.89&62.00/80.91\\
    AUTO &\textbf{9.45/97.94}&\textbf{17.55/95.55}\\
    \bottomrule
    \end{tabular}
    \caption{AUTO outperforms ELET. Models are tested on six OOD datasets with WideResNet-40-2.}
    \label{tab:elet}
\end{table}



\textbf{AUTO with different score functions.}
To further verify the generality and the effectiveness of AUTO, we test AUTO with three representative OOD scoring functions, namely, MSP \cite{hendrycks2016baseline}, Free energy \cite{liu2020energy}, and MaxLogit \cite{hendrycks2022scaling}. Regarding all the cases with different scoring functions, our AUTO always achieves good performance (Table \ref{tab:scorefunction}), demonstrating that our proposal can genuinely make the target model learn from OOD knowledge for OOD detection.

\setlength{\tabcolsep}{2.0pt}
\begin{table*}[h]
    \resizebox{\textwidth}{!}{
    \centering
    \begin{tabular}{lrrrrrrrrrrrrrrr}
    \toprule
 \makebox[0.12\textwidth][l]{\multirow{2}{*}{\textbf{Method}}} & \multicolumn{2}{c}{\textbf{SVHN}} & \multicolumn{2}{c}{\textbf{Texture}} & \multicolumn{2}{c}{\textbf{LSUN\_Crop}} & \multicolumn{2}{c}{\textbf{LSUN\_Resize}} & \multicolumn{2}{c}{\textbf{Places365}} &\multicolumn{2}{c}{\textbf{iSUN}}&\multicolumn{3}{c}{\textbf{Average}}\\
 \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}\cmidrule(lr){12-13}\cmidrule(lr){14-16}
    &\textbf{FPR95} $\downarrow$&\textbf{AUROC} $\uparrow$&\textbf{FPR95} $\downarrow$&\textbf{AUROC} $\uparrow$&\textbf{FPR95} $\downarrow$&\textbf{AUROC} $\uparrow$&\textbf{FPR95} $\downarrow$&\textbf{AUROC} $\uparrow$&\textbf{FPR95} $\downarrow$&\textbf{AUROC} $\uparrow$&\textbf{FPR95} $\downarrow$&\textbf{AUROC} $\uparrow$&\textbf{FPR95} $\downarrow$&\textbf{AUROC} $\uparrow$&\textbf{IN\_Acc} $\uparrow$\\ 
    \midrule
    MSP \cite{hendrycks2016baseline} &1.11&99.77&20.46&95.57&3.91&99.05&\textbf{0.70}&\textbf{99.82}&52.35&87.57&\textbf{1.21}&\textbf{99.70}&13.29&96.91&77.16\\
        Energy \cite{liu2020energy}      &\textbf{0.96}&\textbf{99.79}&\textbf{18.07}&\textbf{96.15}&\textbf{3.54}&\textbf{99.16}&0.74&\textbf{99.82}&46.27&89.67&1.52&99.59&\textbf{11.85}&\textbf{97.36}&\textbf{77.90}\\
    MaxLogit \cite{hendrycks2022scaling} &1.37&99.72&21.61&95.55&4.24&99.02&0.98&99.80&\textbf{43.95}&\textbf{89.68}&1.95&95.55&12.35&97.22&77.87\\
    \bottomrule
    \end{tabular}
    }
    \caption{AUTO's results with different score functions, ResNet-34 is trained on CIFAR-100. $\uparrow$ indicates larger values are better and vice versa. Bold numbers indicate superior results.}
    \label{tab:scorefunction}
\end{table*}

\textbf{Results on Individual Datasets.}
For models trained on CIFAR-10 (Table \ref{tab:cifar10}) and CIFAR-100 (Table \ref{tab:cifar100}), we report their specific results on six OOD datasets respectively.
\setlength{\tabcolsep}{2.0pt}
\begin{table*}[t]
    \resizebox{\textwidth}{!}{
    \centering
    \begin{tabular}{llrrrrrrrrrrrrrrr}
    \toprule
    \makebox[0.08\textwidth][l]{\multirow{2}{*}{\textbf{Backbone}}} & \makebox[0.1\textwidth][l]{\multirow{2}{*}{\textbf{Method}}} & \multicolumn{2}{c}{\textbf{SVHN}} & \multicolumn{2}{c}{\textbf{Texture}} & \multicolumn{2}{c}{\textbf{LSUN\_Crop}} & \multicolumn{2}{c}{\textbf{LSUN\_Resize}} & \multicolumn{2}{c}{\textbf{Places365}} &\multicolumn{2}{c}{\textbf{iSUN}}&\multicolumn{3}{c}{\textbf{Average}}\\
    \cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}\cmidrule(lr){9-10}\cmidrule(lr){11-12}\cmidrule(lr){13-14}\cmidrule(lr){15-17}
    &&\textbf{FPR95} $\downarrow$&\textbf{AUROC} $\uparrow$&\textbf{FPR95} $\downarrow$&\textbf{AUROC} $\uparrow$&\textbf{FPR95} $\downarrow$&\textbf{AUROC} $\uparrow$&\textbf{FPR95} $\downarrow$&\textbf{AUROC} $\uparrow$&\textbf{FPR95} $\downarrow$&\textbf{AUROC} $\uparrow$&\textbf{FPR95} $\downarrow$&\textbf{AUROC} $\uparrow$&\textbf{FPR95} $\downarrow$&\textbf{AUROC} $\uparrow$&\textbf{IN\_Acc} $\uparrow$\\ 
    \midrule
    \multirow{13}{*}{\textbf{RN-34}}
    &MSP \cite{hendrycks2016baseline}&35.51&95.05&51.84&91.34&43.34&93.93&45.75&93.54&56.55&88.11&45.98&93.24&46.49&92.53&94.87\\
    &ODIN\cite{liang2018enhancing}   &19.10&96.57&38.82&91.80&21.21&96.20&26.78&95.32&45.28&88.65&28.82&95.11&30.00&93.94&94.87\\
    &Mahalanobis \cite{lee2018simple}&22.22&96.47&35.47&94.38&60.92&92.59&44.89&93.59&56.30&89.58&46.08&93.25&44.31&93.31&94.87\\
    &Energy \cite{liu2020energy}     &18.18&96.67&37.53&92.09&19.34&96.50&25.30&95.47&44.44&88.50&27.81&95.19&28.77&94.07&94.87\\
    &ReAct \cite{sun2021react}       &23.15&95.42&41.51&90.61&25.35&94.92&28.69&94.79&46.51&88.79&30.19&94.42&32.57&93.16&94.85\\
    &Logit Norm \cite{wei2022mitigating}&12.71&97.82&29.87&94.21&\underline{0.90}&99.77&18.39&96.77&26.73&94.67&20.24&96.43&18.14&96.81&94.68\\
    &KNN~\cite{sun2022knn}              &32.49&95.34&40.82&93.36&28.15&95.71&33.69&95.10&48.25&90.80&36.86&94.59&36.71&94.15&94.87\\
    \cmidrule(lr){2-17}
    &OE \cite{hendrycks2018deep}     &3.48&98.98&9.27&98.24&1.24&99.68&3.21&99.09&15.88&96.36&3.05&99.10&\underline{6.02}&98.58&\underline{95.08}\\
    &Energy (w. $\mathcal{D}_{aux}$)\cite{liu2020energy} &\underline{1.16}&99.53&\underline{4.34}&\underline{98.64}&\textbf{0.84}&99.01&0.98&99.08&\textbf{8.87}&\underline{97.26}&1.41&99.06&\textbf{2.93}&\textbf{98.71}&\textbf{95.49}\\
    &POEM~\cite{ming2022posterior}&1.17&99.30&\textbf{0.96}&\textbf{99.50}&54.46&89.54&\textbf{0.00}&\textbf{100.00}&\underline{10.32}&\textbf{97.49}&\textbf{0.00}&\textbf{100.00}&11.25&97.64&89.57\\
    &WOODS~\cite{katzsamuels2022training}&2.26&99.55&22.75&94.71&3.13&99.47&1.86&99.65&26.62&93.76&4.01&99.36&10.10&97.75&94.79\\
    &DOE~\cite{wang2023outofdistribution}&\textbf{1.15}&\textbf{99.80}&9.60&97.94&1.05&\underline{99.79}&4.20&99.15&34.65&91.00&2.95&99.37&8.93&97.84&94.74\\
    &AUTO&1.35&\underline{99.73}&12.64&97.69&0.99&\textbf{99.81}&\underline{0.84}&\underline{99.81}&21.97&95.03&\underline{1.03}&\underline{99.77}&6.47&\underline{98.64}&94.98\\
    \midrule
    \multirow{13}{*}{\textbf{WRN-40-2}}
    &MSP \cite{hendrycks2016baseline}&51.21&92.36&62.49&86.75&30.24&95.73&51.53&91.39&59.78&87.52&56.77&89.66&52.00&90.57&94.53\\
    &ODIN\cite{liang2018enhancing}   &33.34&92.29&55.02&83.96&6.88 &98.51&29.46&93.59&44.58&87.80&36.66&92.12&34.32&91.38&94.53\\
    &Mahalanobis \cite{lee2018simple}&6.89 &98.69&11.65&97.99&16.63&97.11&27.27&95.52&61.84&86.73&29.40&95.11&25.61&95.19&94.53\\
    &Energy \cite{liu2020energy}     &31.85&92.37&55.63&83.93& 6.44&98.66&28.29&93.75&43.03&88.13&35.20&92.32&33.41&91.53&94.53\\
    &ReAct \cite{sun2021react}       &88.11&73.53&77.57&73.82&52.40&84.02&35.53&91.40&53.88&85.32&44.51&89.00&58.67&82.85&93.41\\
    &Logit Norm \cite{wei2022mitigating}&10.64&97.46&44.35&91.84&1.47&98.98&17.85&96.86&35.68&92.98&16.21&97.04&21.03&95.86&94.42\\
    &KNN~\cite{sun2022knn}&45.16&92.92&43.63&91.52&27.16&95.44&26.82&95.40&43.97&90.77&33.01&93.83&36.63&93.31&94.53\\
    \cmidrule(lr){2-17}
    &OE \cite{hendrycks2018deep}     &3.24&99.17&10.82&98.01&1.30&99.67&4.20&99.09&17.31&96.23&5.63 &98.9 &7.08 &98.51&94.44\\
    &Energy (w. $\mathcal{D}_{aux}$)\cite{liu2020energy} &\textbf{0.72}&\underline{99.58}&\underline{4.36}&\underline{98.82}&0.58&99.42&1.25&99.23&\underline{8.81}&\underline{97.57}&1.72&99.2&\textbf{2.91}&\textbf{98.97}&\textbf{94.91}\\
    &POEM~\cite{ming2022posterior}&2.61&99.43&\textbf{0.48}&\textbf{99.75}&34.63&92.46&\textbf{0.00}&\textbf{100.00}&\textbf{5.30}&\textbf{98.55}&\textbf{0.00}&\textbf{100.00}&7.17&98.37&90.62\\
    &WOODS~\cite{katzsamuels2022training}&2.62&99.50&36.35&93.02&1.75&99.62&\underline{1.03}&\underline{99.75}&29.25&93.96&1.85&99.61&12.14&97.58&\underline{94.72}\\
    &DOE~\cite{wang2023outofdistribution}&3.05&99.25&8.70&97.97&\textbf{0.10}&\textbf{99.84}&1.55&99.39&14.85&96.64&1.75&99.39&\underline{5.00}&\underline{98.74}&94.43\\
    &AUTO&\underline{1.03}&\textbf{99.77}&17.7&96.2&\underline{0.49}&\underline{99.74}&1.34&99.55&35.26&92.76&\underline{0.87}&\underline{99.63}&9.45&97.94&93.33\\
    \bottomrule
    \end{tabular}
    }
    \caption{Detailed results on six common OOD benchmark datasets, models are trained on CIFAR-10. $\uparrow$ indicates larger values are better and vice versa. The bold and underlined numbers respectively indicate the first and second best results.}
    \label{tab:cifar10}
\end{table*}

\setlength{\tabcolsep}{2.0pt}
\begin{table*}[t]
    \resizebox{\textwidth}{!}{
    \centering
    \begin{tabular}{llrrrrrrrrrrrrrrr}
    \toprule
    \makebox[0.08\textwidth][l]{\multirow{2}{*}{\textbf{Backbone}}} & \makebox[0.1\textwidth][l]{\multirow{2}{*}{\textbf{Method}}} & \multicolumn{2}{c}{\textbf{SVHN}} & \multicolumn{2}{c}{\textbf{Texture}} & \multicolumn{2}{c}{\textbf{LSUN\_Crop}} & \multicolumn{2}{c}{\textbf{LSUN\_Resize}} & \multicolumn{2}{c}{\textbf{Places365}} &\multicolumn{2}{c}{\textbf{iSUN}}&\multicolumn{3}{c}{\textbf{Average}}\\
    \cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}\cmidrule(lr){9-10}\cmidrule(lr){11-12}\cmidrule(lr){13-14}\cmidrule(lr){15-17}
    &&\textbf{FPR95} $\downarrow$&\textbf{AUROC} $\uparrow$&\textbf{FPR95} $\downarrow$&\textbf{AUROC} $\uparrow$&\textbf{FPR95} $\downarrow$&\textbf{AUROC} $\uparrow$&\textbf{FPR95} $\downarrow$&\textbf{AUROC} $\uparrow$&\textbf{FPR95} $\downarrow$&\textbf{AUROC} $\uparrow$&\textbf{FPR95} $\downarrow$&\textbf{AUROC} $\uparrow$&\textbf{FPR95} $\downarrow$&\textbf{AUROC} $\uparrow$&\textbf{IN\_Acc} $\uparrow$\\ 
    \midrule
    \multirow{13}{*}{\textbf{RN-34}}
    &MSP \cite{hendrycks2016baseline}   &84.77&72.71&80.15&77.85&86.00&75.27&84.87&80.81&80.81&76.05&84.58&72.60&83.53&74.34&77.51\\
    &ODIN\cite{liang2018enhancing}      &84.93&72.73&79.06&78.29&87.71&73.90&81.37&75.34&81.33&75.36&82.17&75.98&82.76&75.27&77.51\\
    &Mahalanobis \cite{lee2018simple}   &70.20&85.59&64.33&85.45&81.43&78.09&80.46&78.47&78.97&77.52&78.00&79.81&75.56&80.82&77.51\\
    &Energy \cite{liu2020energy}        &85.89&72.77&79.47&77.93&89.37&73.75&78.96&76.09&82.33&75.12&79.90&76.33&82.65&75.33&77.51\\
    &ReAct \cite{sun2021react}          &75.49&82.97&68.86&83.73&83.19&82.31&71.19&81.43&77.25&79.86&72.55&81.73&74.76&82.01&77.09\\
    &Logit Norm \cite{wei2022mitigating}&51.12&91.51&86.99&72.96&46.30&90.94&96.11&64.72&80.22&77.81&95.75&63.03&76.08&76.83&76.40\\
    &KNN~\cite{sun2022knn}              &68.02&84.92&70.46&83.02&71.26&82.30&70.08&83.54&77.43&78.23&70.75&82.64&71.33&82.44&77.51\\
    \cmidrule(lr){2-17}
    &OE \cite{hendrycks2018deep}         &44.58&98.98&64.91&86.07&24.52&95.59&75.91&82.53&63.36&86.23&77.81&81.32&58.52&87.3&76.84\\
    &Energy (w. $\mathcal{D}_{aux}$)\cite{liu2020energy} &34.24&94.34&65.71&87.81&16.51&97.19&71.60&86.35&55.59&88.95&74.44&85.65&53.02&90.05&77.19\\
    &POEM~\cite{ming2022posterior}       &28.79&95.84& \textbf{3.07}&\textbf{98.86}&69.85&86.32& \textbf{0.00}&\textbf{100.00}&\textbf{16.95}&\textbf{96.61}&\textbf{0.00}&\textbf{100.00}&\underline{19.78}&96.27&69.49\\
    &WOODS~\cite{katzsamuels2022training}&17.25&96.64&46.11&87.30&27.95&93.52&24.34&94.74&62.50&81.99&31.25&93.05&34.90&91.21&77.84\\
    &DOE~\cite{wang2023outofdistribution}&36.25&93.80&38.15&91.86&\underline{13.60}&\underline{97.69}&32.60&94.85&\underline{44.10}&88.87&29.90&94.83&32.43&\underline{97.22}&\underline{77.87}\\
    &AUTO& \textbf{0.96}&\textbf{99.79}&\underline{18.07}&\underline{96.15}&\textbf{3.54}&\textbf{99.16}&\underline{0.74}&\underline{99.82}&46.27&\underline{89.67}&\underline{1.52}&\underline{99.59}&\textbf{11.85}&\textbf{97.36}&\textbf{77.90}\\
    \midrule
    \multirow{13}{*}{\textbf{WRN-40-2}}
    &MSP \cite{hendrycks2016baseline}   &78.32&80.15&83.26&74.18&66.08&84.32&81.24&73.17&82.50 &74.93&83.52&71.89&79.15&76.44&75.84\\
    &ODIN\cite{liang2018enhancing}      &74.01&83.78&79.69&77.29&33.37&94.24&72.43&79.86&81.48&75.22&77.49&77.35&69.75&81.29&75.84\\
    &Mahalanobis \cite{lee2018simple}   &58.99&87.93&38.45&91.64&98.83&63.55&72.44&81.56&87.00   &71.58&71.15&82.02&71.14&79.71&75.84\\
    &Energy \cite{liu2020energy}        &76.04&83.71&80.99&76.90&29.66&94.92&72.24&79.93&81.94&74.69&77.03&77.61&69.65&81.30&75.84\\
    &ReAct \cite{sun2021react}          &98.77&55.95&91.19&59.71&91.25&59.71&86.92&72.35&93.79&62.09&90.17&68.69&92.01&64.53&64.77\\
    &Logit Norm \cite{wei2022mitigating}&52.54&91.13&72.26&79.90&9.62 &98.38&58.48&88.18&75.23&80.04&61.27&87.98&54.90&87.60&\textbf{76.02}\\
    &KNN~\cite{sun2022knn}&45.11&90.47&51.17&87.36&64.49&82.22&56.86&87.22&83.23&73.12&58.68&85.77&59.92&84.36&75.84\\
    \cmidrule(lr){2-17}
    &OE \cite{hendrycks2018deep}        &53.71&90.13&60.98&84.59&16.74&96.81&65.99&79.71&57.03&86.30&69.80&77.37&54.04&85.82&75.59\\
    &Energy (w. $\mathcal{D}_{aux}$)\cite{liu2020energy} &34.13&94.84&56.04&87.97&9.80&98.26&56.39&86.63&\underline{50.49}&\underline{89.83}&59.71&85.27&44.43&90.47&75.75\\
    &POEM~\cite{ming2022posterior}&48.96&92.67&\textbf{3.81}&\textbf{98.41}&71.59&88.96&\textbf{0.00}&\textbf{100.00}&\textbf{21.41}&\textbf{95.74}&\textbf{0.00}&\textbf{100.00}&24.30&\textbf{95.96}&69.37\\
    &WOODS~\cite{katzsamuels2022training}&\underline{7.99}&\underline{98.50}&42.17&89.62&7.73&98.59&7.74&98.65&54.71&84.48&15.53&97.42&\underline{22.65}&94.54&75.74\\
    &DOE~\cite{wang2023outofdistribution}&33.35&94.59&\underline{32.55}&\underline{92.75}&\textbf{3.70}&\textbf{98.99}&14.45&97.43&56.55&85.77&15.95&97.02&26.09&94.43&74.98\\
    &AUTO
    &\textbf{1.74}&\textbf{99.53}&34.41&91.57&\underline{4.82}&\underline{98.63}&\underline{2.06}&\underline{99.34}&59.86&84.88&\underline{2.38}&\underline{99.32}&\textbf{17.55}&\underline{95.55}&73.71\\
    \bottomrule
    \end{tabular}
    }
    \caption{Detailed results on six common OOD benchmark datasets, models are trained on CIFAR-100. $\uparrow$ indicates larger values are better and vice versa.  The bold and underlined numbers respectively indicate the first and second best results.}
    \label{tab:cifar100}
\end{table*}

\section{Experiment Platform}
All our experiments are run on the NVIDIA RTX3090 GPU.
The Pytorch version is 1.9.0 with Python 3.8.
The operating system is Ubuntu Linux 16.04.

\end{document}