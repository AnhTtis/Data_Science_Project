\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{CJKutf8}
\graphicspath{ {./images/} }
\usepackage{amsmath,amsthm,amssymb,amsfonts,amscd,keyval}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{dsfont}
\usepackage{float}
\usepackage{xspace}
\usepackage{subfig}
\usepackage{bbding}
\usepackage{makecell}
 \usepackage{threeparttable}
 \usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\title{Transformer and Snowball Graph Convolution Learning for Biomedical Graph Classification}


\author{%
Jinlong Hu$^1$ \quad Yangmin Huang$^1$ \quad Shoubin Dong$^1$ \quad\\
$^1$Guangdong Key Lab of Communication and Computer Network\\
 School of Computer Science and Engineering\\
 South China University of Technology\\
 Guangzhou, China\\
\texttt{jlhu@scut.edu.cn} \quad \texttt{csymhuang@mail.scut.edu.cn} \quad \texttt{sbdong.scut.edu.cn}
}
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\

\begin{document}
\begin{CJK}{UTF8}{gbsn}
\maketitle
\begin{abstract}
Graph or network has been widely used for describing and modeling complex systems in biomedicine. Deep learning methods, especially graph neural networks (GNNs), have been developed to learn and predict with such structured data. In this paper, we proposed a novel transformer and snowball encoding networks (TSEN) for biomedical graph classification, which introduced transformer architecture with graph snowball connection into GNNs for learning whole-graph representation. TSEN combined graph snowball connection with graph transformer by snowball encoding layers, which enhanced the power to capture multi-scale information and global patterns to learn the whole-graph features. On the other hand, TSEN also used snowball graph convolution as position embedding in transformer structure, which was a simple yet effective method for capturing local patterns naturally. Results of experiments using four graph classification datasets demonstrated that TSEN outperformed the state-of-the-art typical GNN models and the graph-transformer based GNN models. 
\end{abstract}


% keywords can be removed
\keywords{Graph neural networks \and Transformer \and Snowball graph convolutional networks}


\section{Introduction}
Graph or network is pervasive in describing and modeling complex systems in biomedicine, such as molecular graphs, multi-omics graphs, and pharmaceutical graphs. Recently, a growing number of deep learning methods, especially graph neural networks (GNNs) have been developed for learning and predicting on biomedical graph data, such as predicting the function of proteins in an interaction network, and predicting the toxicity or biological activity profile of a candidate drug~\cite{li2022graph,yi2022graph,RN1}.

Graph classification is one type of the most prominent learning tasks of GNNs, which typically learns a graph-level representation for graph classification in an end-to-end fashion. To learn an effective representation, GNNs were expected to capture the global patterns as well as the local patterns and their interaction~\cite{RN2}. There have been various attempts to enhance the expressive power of GNNs, for example, enlarging the receptive field~\cite{RN4}, using effective graph pooling operations~\cite{RN6,RN7}, and relieving the over-smoothing caused by increased network depth~\cite{RN8,RN18}. Compared with data from other fields, biomedical graphs are usually complex and noisy, and the data size is usually limited, since collecting biomedical graph data usually requires time consuming and laborious wet experiments~\cite{yi2022graph}. In the view of biomedical graph data, it is difficult to develop targeted graph-level representation learning approaches to produce compact vector representations which can be optimized for the classification task.

Recently, transformer~\cite{RN9} has been shown excel performance in nature language processing (NLP), such as Bert~\cite{RN10} and GPT~\cite{RN11}, and this architecture was also introduced for graph structure data in biomedicine~\cite{RN14}. To date, transformer has been shown to be good at capturing the long-range dependency and global information in graph~\cite{RN15}. And, most researchers tried to properly incorporate structural information of graphs into GNN models with transformer, such as using graph structure encoding in Graphormer~\cite{RN16}, and subgraph representation in structure-aware transformer (SAT)~\cite{RN17}. However, it is still a challenge to make transformer architecture work in whole-graph representation learning to extract rich relational information for classifying biomedical graphs.

In this paper, we proposed a transformer and snowball encoding networks (TSEN) for biomedical graph classification, which introduced transformer architecture with snowball connection into GNNs for learning whole-graph representations from given graphs. TSEN learns multi-scale information of nodes with snowball encoding, and read-out the node information with global attention to obtain graph level representation for graph classification. A snowball encoding function was proposed to capture short-range and long-range information by combining graph convolution with dense connection structure and graph transformer.
The main contributions are summarized as follows: 

(1) We proposed a novel graph transformer architecture with snowball connection for graph neural networks, which enhanced the power to capture multi-scale information and global patterns of graphs.  

(2) We used snowball graph convolution as position embedding in transformer structure, which was a simple yet effective method to capture structure information naturally in transformer structure for graph-structure data.

The proposed method was evaluated using four real-world biomedical networks, including two molecular networks and two human brain functional networks. The results showed that our proposed method achieved the best performance compared with the state-of-the-art typical GNN models and the graph-transformer based GNN models. Furthermore, we validated the effectiveness of TSEN with ablation experiments, and further analyzed the representation capacity of TSEN and the influence of hyper-parameters on the model.

The rest of this paper is organized as follows. Section~\ref{sec:headings} reviews the related literature. Section~\ref{sec:notation} describes the notation. Section~\ref{sec:method} presents the proposed TSEN. Section~\ref{sec:experiments} reports the experiments and results. Section~\ref{sec:analysis} reports the analysis of TSEN model. Finally, Section~\ref{sec:conclusion} concludes this paper.



\section{Related Work}
\label{sec:headings}
\paragraph{Graph Classification with GNNs.}
Given a biomedical graph, graph classification with GNNs typically learns a graph-level representation for graph classification. Graph-level representation learning with GNNs mainly includes graph pooling and coarsening method. The graph pooling methods usually use global pooling methods to read-out the graphs such as SAGPool~\cite{RN6}. The coarsening methods apply grouping nodes into similarity clusters to extract global representations, such as Diff-Pool~\cite{RN7} and Graclus multi-layer clustering~\cite{RN19}.

Recent works showed that more sophisticated layers for aggregating or read-out the graphs, such as clustering layers, may not bring any benefits on many real-world graph data~\cite{RN20}. To capture better structure information for graph classification, GraphSAGE~\cite{RN44} used random edge sampling methods, Graph Isomorphism Networks (GIN)~\cite{RN34} and subgraph isomorphism counting method~\cite{RN23} used graph isomorphism-based methods to achieve the ability as the Weisfeiler-Lehman test.

Residual and dense connections~\cite{RN24,RN25} have demonstrated their great improvement in traditional neural networks such as convolutional neural networks (CNNs), and the structures of residual and dense connection were also applied to GNN domain~\cite{RN8,RN18,RN26,RN27}. Residual connections could mitigate the problem of over-smoothing and improve the performance of GNNs with deeper structure~\cite{RN8,RN26}. Dense connections use the structure of snowball connections and consistently up to more convolutional layers, and the snowball structure could concatenate multi-scale features incrementally~\cite{RN18}. The multi-scale information of graphs could be helpful for enhancing the expressiveness of GNNs on graph classification~\cite{RN28,RN29,RN45}.
\paragraph{Transformer on graph data}
Transformer~\cite{RN9} has been successfully used in sequence modeling such as natural language modeling, since its special design of multi-head attention could capture the long-range dependency and relationship. In the transformer structure for GNNs, positional embedding methods were used to improve the power of capturing local patterns and structure on graphs. For example, Graph-Bert~\cite{RN30} used three positional embedding methods, Weisfeiler-Lehman absolute role embedding, intimacy-based relative positional embedding, and hop-based relative distance embedding, to extract substructure information. Generalization of transformer (GT)~\cite{RN31} proposed a positional embedding method, Laplacian eigenvector, to improve its performance. Graphormer~\cite{RN16} was recently proposed to raise positional encoding methods with edge, spatial and centrality, designed for solving large-scale datasets of molecules.

Besides using positional embedding for graphs in transformer, standard GNN module could be combined with transformer to extract structure information, and use a “readout” mechanism similar to “<cls>” in NLP to obtain a global graph embedding~\cite{RN15}. Furthermore, Chen et al.~\cite{RN17} suggested that using GNN-based methods was effective for generating subgraph representation and introducing structure information naturally to transformer structure. In this paper, we used graph convolution as the position embedding method and introduced the snowball structure into transformer, to learn whole-graph representations for biomedical graph classification.

\section{Notation}
\label{sec:notation}
\paragraph{Graph.}
Let ${\rm{\;G}} = \left( {V,E,{\bf{X}}} \right)$ be a graph, where $V = \left\{ {{v_0}, \ldots ,{v_{{\rm{n}} - 1}}} \right\}$ denotes the set of nodes of the graph, $E \subseteq V \times V$ is the set of edges, and ${\bf{X}} \in {{\mathbb R}^{{\rm{n}} \times {\rm{s}}}}$ is a multi-variate signal on the graph nodes, with ${\rm{s}}$ represents the dimension of node features.
\paragraph{Graph convolution operating.}
The graph convolutional operator $f\left(  \cdot  \right)$  is defined as follows:
\begin{equation}
{\bf{H}} = f\left( {\bf{X}} \right) = activation\left( {{\bf{LXW}}} \right)
\label{eq:gcn}
\end{equation}
Where ${\bf{H}} \in {{\mathbb R}^{{\rm{n}} \times {\rm{s'}}}}$ denotes the convoluted node features, ${\bf{W}} \in {{\mathbb R}^{{\rm{s}} \times {\rm{s'}}}}$ represents the weighted matrix, with ${\rm{s'}}$ represents the dimension of convoluted node features, $activation\left(  \cdot  \right)$ is the activation function; ${\rm{\;}}{\bf{L}}$ is the normalized graph Laplacian which is defined by: 
\begin{equation}
{\bf{L}} = {\bf{I}} - {{\bf{D}}^{ - \left( {\frac{1}{2}} \right)}}{\bf{A}}{{\bf{D}}^{ - \left( {\frac{1}{2}} \right)}}
\label{eq:laplacian}
\end{equation}
Where ${\bf{A}} \in {{\mathbb R}^{{\rm{n}} \times {\rm{n}}}}$ is the adjacency matrix of the graph, with elements ${a_{{\rm{ij}}}} = 1$ means there exists the edge $\left( {{v_i},{v_j}} \right) \in E$; ${\bf{D}} \in {{\mathbb R}^{{\rm{n}} \times {\rm{n}}}}$ is the diagonal degree matrix for ${\bf{A}}$, where ${d_{{\rm{ii}}}} = \sum\nolimits_i {{a_{{\rm{ij}}}}}$, and ${\bf{I}}$ is the identity matrix.
\paragraph{Transformer operating.}
The standard structure of transformer has two key parts, self-attention part and feed-forward network (FFN) part, with $mh\_Attention\left(  \cdot  \right)$ and $FFN\left(  \cdot  \right)$ denote above operations, as defined as follows.
\begin{equation}
mh\_Attention\left( x \right) = Concatenate\left( {Attention\left( x \right)} \right)
\label{eq:attention}
\end{equation}
\begin{equation}
FFN\left( x \right) = max\left( {0,x{W_1} + {b_1}} \right){W_2} + b
\label{eq:ffn}
\end{equation}
\begin{equation}
Attention\left( x \right) = softmax\left( {\frac{{Qx{K^T}x}}{{\sqrt {{d_k}} }}} \right)Vx
\label{eq:mh}
\end{equation}
Where $Q$, $K$ and $V$ denote the weighted matrices, $softmax\left(  \cdot  \right)$ is the softmax operations, ${d_k}$ is the dimension of input $x$, ${W_1}$ and ${W_2}$ are weighted matrices for linear projection, and ${b_1}$ and $b$ denotes biases.
\section{Proposed Method}
\label{sec:method}
In this paper, we proposed a novel snowball and encoding networks, namely TSEN, with the mixture structure of snowball graph convolution and transformer. In TSEN, the snowball graph convolution could be regarded as the substitute of position embedding in typical transformer structure, and the transformer could be regarded as an encoding layer in snowball structure on the other hand. The TSEN consists of graph snowball encoding part, graph representation part, and graph classification part, as shown in figure ~\ref{fig:model}.
\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{model.pdf}
    \caption{Architecture of TSEN. with three parts, graph snowball encoding part, graph representation part and graph classification part. (a) Graph snowball encoding part has multiple graph snowball encoding layers, and each layer uses graph convolution operation to capture local patterns, and concatenates features from previous layers to obtain multi-scale information, and then uses transformer to capture global information. (b) Graph representation part uses global attention to read-out the node features from the graph snowball encoding layers, and then concatenates all the attention information. (c) Graph classification part uses a MLP to obtain the probability of classes of the graphs.}
    \label{fig:model}
\end{figure*}
\subsection{Graph snowball encoding part}
The graph snowball encoding part consists of n layers, and each layer includes a snowball convolution sub-module and a transformer encoder sub-module. The snowball convolution sub-module firstly concatenates the node features from all previous layers, and uses graph convolution operation to aggregate the concatenated features, and then generates node embedding with local structure information. Afterwards, the transformer encoder sub-module encodes the node embedding with the transformer to capture global information, and forwards the encoded output into the next snowball convolution sub-module.

In the snowball convolution sub-module, a snowball graph convolutional operator was used to obtain the convolutional node embeddings, as defined as follows.
\begin{equation}
{{\bf{H}}^{\left( 0 \right)}} = {\bf{X}}\;
\label{eq:snowballh0}
\end{equation}
\begin{equation}
{{\bf{S}}^{\left( i \right)}} = f\left( {\left[ {{{\bf{H}}^{\left( 0 \right)}},{{\bf{H}}^{\left( 1 \right)}},...,{{\bf{H}}^{\left( i \right)}}} \right]{{\bf{W}}^{\left( i \right)}}} \right)
\label{eq:snowball}
\end{equation}
Where $f\left(  \cdot  \right)$ is the graph convolution operation which was defined in (~\ref{eq:gcn}), ${{\bf{W}}^{\left( i \right)}}$ is the weighted matrix, and $\left[  \cdot  \right]$ denotes the concatenating operation.
The standard structure of transformer was used in the encoder sub-module. The transformer encoding operation was defined as follows.
\begin{equation}
{{\bf{H}}^{'\left( i \right)}} = mh\_Attention\left( {LayerNorm\left( {{{\bf{S}}^{\left( i \right)}}} \right)} \right) + {{\bf{S}}^{\left( i \right)}}
\label{eq:encode1}
\end{equation}
\begin{equation}
{{\bf{H}}^{\left( {i + 1} \right)}} = FFN\left( {LayerNorm\left( {{{\bf{H}}^{'\left( i \right)}}} \right)} \right) + {{\bf{H}}^{'\left( i \right)}}
\label{eq:encode2}
\end{equation}
Where $mh\_Attention\left(  \cdot  \right)$ and $FFN\left(  \cdot  \right)$ of the transformer were defined in (~\ref{eq:attention}) and (~\ref{eq:ffn}), $LayerNorm( \cdot )$ are layer normalization operations, the convoluted node features ${{\bf{S}}^{\left( i \right)}}$ are the inputs.
Layer normalization was used before multi-head self-attention and the FFN blocks to improve the stability of the transformer part~\cite{RN32}, and dropout and residual operations were adapted in this part.
\subsection{Graph representation part}
In the graph representation part, we used a global attention layer to obtain a graph representation vector for each layer of the graph snowball encoding part. The global attention mechanism of this layer introduced an extra global node connecting to all nodes in the graph, which was similar to the “<cls>” token to gather global information in NLP tasks~\cite{RN15}, and was an efficient way to represent the graph from node embedding~\cite{RN33}. Then we concatenated all the graph representation vectors as the whole-graph representation~\cite{RN34}, which was used as the input for the downstream classification tasks.
In the global attention layers, multi-layer perception (MLP) was used to map the node features to a score, and the weight of the node was calculated with softmax operation. The global attention operating was defined as a weighted summation with the score and the weight from all the nodes, and the operating of Layer $t$ was defined as follows.
\begin{equation}
{{\bf{h}}_t} = \mathop \sum \limits_{i = 1}^n softmax\left( {{h_{gate}}\left( {{H_i}} \right)} \right) \odot {H_i}
\label{eq:globalattention}
\end{equation}
Where $n$ denotes the number of nodes, ${h_{gate}}\left( \cdot \right)$ is a MLP network, mapping the node features to a score to represent the importance of the node in Layer $t$.
Finally, the whole-graph representation was defined by concatenating the global attention vectors from all the layers, as defined as the follows.
\begin{equation}
{{\bf{h}}_{\cal G}} = \left[ {{{\bf{h}}_1},{{\bf{h}}_2},...{{\bf{h}}_t}} \right],t = 0,1,2,...,n
\label{eq:concat}
\end{equation}
\subsection{Graph classification part}
In the graph classification part, the whole-graph representation was used as the input for MLP and softmax to generate classification result.
\section{Experiments}
\label{sec:experiments}
To evaluate the performance of our proposed TSEN model, we experimentally compared with the recent state-of-the-art models in four real-world datasets, and also performed ablation studies. Furthermore, we analyzed the model by evaluating the representation power and the influence of the hyper-parameters.

\subsection{Datasets}
In this paper, we used four datasets as shown in Table 1. 

ABIDE I~\cite{RN35} contains the resting-state functional Magnetic Resonance Imaging (fMRI) data from Autistic Spectrum Disorder (ASD) patients and health controls (HC). By eliminating the low quality of imaging~\cite{RN36}, we selected 974 subjects, including 467 ASD and 507 HC, to construct brain functional networks for classification experiments. 

REST-meta-MDD~\cite{RN37} includes brain network data generated from the resting-state fMRI data from major depressive disorders (MDD) patients and HC. We selected 2027 subjects, including 1041 MDD and 986 HC, to construct brain functional networks for classification experiments. In the datasets of ABIDE I and REST-meta-MDD, we considered a brain functional network of a subject as a graph, where the node represents the region of interest (ROI) of the brain~\cite{craddock}, and the edge between nodes represents the brain functional connection of ROIs ~\cite{RN39}. The edge was defined with the binarization of Pearson correlation coefficient between ROIs. The threshold of the binarization of edges was defined as 0.5 for ABIDE I, and 0.4 for REST-meta-MDD.

COX2 dataset~\cite{RN40} is a set of 467 cyclooxygenase-2 (COX-2) inhibitors with in vitro activity against human recombinant enzymes, and they could be classified into two categories of active and inactive compounds. 

NCI1 dataset~\cite{RN41} is a set of 4110 graphs representing of a chemical compound relative to anti-cancer screens, where the chemical compound was assessed as positive or negative for cell lung cancer. 
\begin{table}[h]
\centering
\caption{The Characteristic of Graph Datasets}
\label{tab:dataset}
\begin{threeparttable} %添加此处
\begin{tabular}{cccccc}
\hline
Dataset       & Classes & Graphs & Avg. Nodes & Avg. Edges & Attr. \\ \hline
COX2          & 2       & 467    & 41.22      & 43.45      & 35    \\
ABIDE I       & 2       & 974    & 200.00     & 2362.25    & 200   \\
REST-meta-MDD & 2       & 2027   & 200.00     & 39692.55   & 200   \\
NCI1          & 2       & 4110   & 29.87      & 32.30      & 37    \\ \hline
\end{tabular}
\begin{tablenotes} %添加此处
\item “Attr.” means the dimension of the node features. %添加此处
\end{tablenotes} %添加此处
\end{threeparttable} %添加此处
\end{table}

\subsection{Baseline}
The baselines are of three kinds, including typical GNN models, GNN models with special design for graph classification, and graph-transformer based GNN models.
\begin{itemize}
		\item[$\bullet$] GCN~\cite{RN42}, GAT~\cite{RN43}, GraphSAGE~\cite{RN44}, and snowball GCN (SBGCN)~\cite{RN18}: There are typical GNN models that were not specifically designed for graph classification. GCN is based on a variant of CNNs which operate directly on graphs. GAT is a GNN architecture using masked self-attentional layers. GraphSAGE is based on randomly edges sampling to enhance the ability of capturing global structure. SBGCN is a GCN architecture using a snowball structure to capture multi-scale information.
		\item[$\bullet$] DiffPool~\cite{RN7}, GIN~\cite{RN34}, PGCN~\cite{RN28}, and MRGNN~\cite{RN45}: There are graph neural networks which have a special design for graph classification. GIN is a well-known typical GNN that was expected to achieve the ability as the Weisfeiler-Lehman graph isomorphism test, but a graph-level readout function of GIN was specially designed to produce the embedding of the entire graph for graph classification tasks [20]. DiffPool is a differentiable graph pooling module that could generate hierarchical representations for graph classification. PGCN introduced the polynomial graph convolution layer to independently exploit neighbouring nodes at different topological distances, and used subsequent readout layers to generate graph representation. MRGNN used a shallow readout function to generate an unsupervised graph representation.
		\item[$\bullet$] GraphTrans~\cite{RN15}, Graphormer~\cite{RN16}, and Spectral Attention Network (SAN)~\cite{RN46}: They are three well-known graph-transformer based models. GraphTrans applied graph convolution operation to introduce local patterns and structure information of graphs. Graphormer specially designed three positional encoding methods for graph classification. SAN used learned positional encoding of Laplacian spectrum to introduce position information of nodes in graphs. 
\end{itemize}
\subsection{Experiment setting}
For the implementation, we used pytorch-geometric~\cite{RN47} as the backend for all typical GNN models, and used pytorch as the backend of transformer structure for all graph-transformer methods. GELU~\cite{RN48} was selected as the activation function of transformer encoder to further improve the unlinearity expressive power compared to ReLU. The batch normalization and dropout operations were added for every fully connected layer in MLP module, and AdamW~\cite{RN49} served as the optimizer. For loss function, we selected cross-entropy for the brain network datasets and NCI1 datasets experiments, and focal loss~\cite{RN50} as the loss function for COX2 dataset since its samples are unbalanced. During training, linear increasing warm-up and inverse square root decay strategy were selected as the scheduler for the learning rate~\cite{RN51} during the procedure of training.

For all experiments, the learning rate, weight decay, and batch size were set as 1, 10-4, and 16, respectively. The dropout rate for fully connected layers in MLP module was set as 0.5, and dropout rate for transformer was set as 0.1. We trained for 500 epochs during each fold experiment for COX2 and NCI1, and 300 epochs for the two brain network datasets. 

To evaluate the performance of the models, we used five times 10-fold cross validation strategy, and used F1 score as the metric for the classification models. For the implementation details, we made our source codes publicly available on our project website \href{https://github.com/largeapp/TSEN}{https://github.com/largeapp/TSEN}.
\subsection{Experiment Results}
The results of classification experiments are shown as the Table 2. The results showed that the proposed TSEN achieved the best performance compared with the other methods. The GNN models with special designs for graph classification, DiffPool and MRGNN, also achieved excel performance. The compared graph-transformer models did not achieve good performance in this experimental setting. That indicated there was room for improvement for graph-transformer based models for wide graph classification applications. That also indicated the structure of TSEN had advantages in capturing the whole-graph representation for graph classification. 
\begin{table}[h]
\label{tab:performance}
\centering
\caption{F1 Score ($\%$) of Graph Classification (±std)}
\begin{threeparttable} %添加此处
\begin{tabular}{ccccc}
\hline
Models     & COX2                      & ABIDE I                   & REST-meta-MDD             & NCI1                      \\ \hline
GCN        & 24.33±22.67               & 71.44±6.14                & 63.14±2.99                & 77.22±2.27                \\
GAT        & 38.89±22.53               & 70.86±4.42                & 63.95±3.54                & 66.77±7.21                \\
GraphSAGE  & 64.50±14.05               & 69.90±4.66                & 65.02±4.33                & 75.46±2.98                \\
SBGCN      & 65.18±11.49               & 71.64±3.94                & 65.57±3.30                & 80.50±3.10                \\
GIN        & 64.72±10.64               & 67.94±6.01                & 65.34±5.31                & 75.49±2.77                \\
DiffPool   & 67.70±8.21                & 73.31±3.55                & {\ul 67.03±1.97}          & {\ul 84.57±2.31}          \\
PGCN       & 65.11±24.19               & 69.67±4.66                & 62.86±4.24                & 83.49±2.27                \\
MRGNN      & {\ul 67.94±13.18}         & {\ul 73.91±3.98}          & 65.95±3.15                & 79.17±2.29                \\
SAN        & 51.53±19.37               & 65.21±3.07                & 63.49±1.65                & 74.81±7.14                \\
Graphormer & 26.47±19.13               & 61.24±9.85                & 56.48±7.09                & 72.64±20.05               \\
GraphTrans & 56.02±28.47               & 66.90±8.07                & 60.84±3.00                & 82.07±0.68                \\
ours       & {\ul \textbf{73.77±6.46}} & {\ul \textbf{74.06±3.87}} & {\ul \textbf{67.39±2.43}} & {\ul \textbf{85.79±2.66}} \\ \hline
\end{tabular}
\begin{tablenotes} %添加此处
\item In each row, the highest F1 score is highlighted in bold and the top two are underlined. %添加此处
\end{tablenotes} %添加此处
\end{threeparttable} %添加此处
\end{table}

We performed ablation study to evaluate the influence of different parts of TSEN model by comparing TSEN with five models, including GCN, SBGCN, SBGCN with FFN part (SBGCN\_FFN), SBGCN with self-attention part (SBGCN\_SA), and GCN with transformer (GCN\_Trans). GCN\_Trans are also TSEN without snowball structure. The classification performance of the ablation experiments is shown in Table 3. 
\begin{table}[h]
\label{tab:ablation}
\centering
\caption{F1 Score ($\%$) of Ablation Study (±std)}
\begin{threeparttable} %添加此处
\begin{tabular}{ccccc}
\hline
Models     & COX2                      & ABIDE I                   & REST-meta-MDD       & NCI1                      \\ \hline
GCN        & 24.33±22.67               & 71.44±6.14                & 63.14±2.99          & 77.22±2.27                \\
SBGCN      & 65.18±11.49               & 71.64±3.94                & 65.57±3.30          & 80.50±3.10                \\
SBGCN\_FFN & 54.88±10.20               & 70.34±6.06                & 63.61±5.22          & 82.79±3.80                \\
SBGCN\_SA  & {\ul 67.54±10.20}         & 71.49±3.78                & \textbf{67.95±3.63} & 80.98±3.42                \\
GCN\_Trans & 65.27±24.52               & {\ul 72.07±4.21}          & 65.45±4.38          & {\ul 82.96±3.82}          \\
ours       & {\ul \textbf{73.77±6.46}} & {\ul \textbf{74.06±3.87}} & {\ul 67.39±2.43}    & {\ul \textbf{85.79±2.66}} \\ \hline
\end{tabular}
\begin{tablenotes} %添加此处
\item In each row, the highest F1 score is highlighted in bold and the top two are underlined. %添加此处
\end{tablenotes} %添加此处
\end{threeparttable} %添加此处
\end{table}

Compared with the other five models, the proposed TSEN achieved the best performance. The models with self-attention (e.g., SBGCN\_SA and TSEN) had better performance than the models without self-attention, and GCN with transformer achieved better performance than GCN or Snowball GCN without transformer. That indicated the potential of transformer structure for graph classification. 

And GCN with transformer but without snowball connection was still not better than TSEN which combined transformer and snowball convolution connection. That indicated the power of the combination for graph classification.
\section{Model Analysis}
\label{sec:analysis}
In this section, we analyzed the representation power of TSEN by T-distributed Stochastic Neighbor Embedding (T-SNE) visualization~\cite{RN52}, and centered kernel alignment (CKA) similarity~\cite{RN53}. We also analyzed the influence of hyper-meters of transformer in TSEN.
\subsection{The representation power of TSEN}
We compared the representation of four models, including GCN, snowball GCN, GCN with transformer, and TSEN, using T-SNE visualization with NCI1 dataset. The graph representation was obtained after concatenating all read-out outputs and before inputting into MLP module for classification. The T-SNE visualization of these models are shown in Figure ~\ref{fig:tsne}. 
\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{fig2.pdf}
    \caption{The T-SNE visualization of graph representation. Each point represents a graph, and the colors indicate different classes of graphs.}
    \label{fig:tsne}
\end{figure*}

From Figure ~\ref{fig:tsne}(a), ~\ref{fig:tsne}(b), and ~\ref{fig:tsne}(c), the representations generated by GCN, Snowball GCN, GCN with transformer were ambiguous and hard to discriminate. From Figure 2(d), compared with other models, the representation of TSEN was easier to distinguish, and that indicated the power of representation of TSEN. 

Based on CKA RBF kernel, we calculated the CKA similarity value of representation between different models on the ABIDE I dataset. As shown in Table 4, we compared CKA similarity among models in the first layer and the second layer respectively. The results showed that the CKA similarity between TSEN with other models was relatively smaller than the others. The smaller similarity value indicated that the representation of TSEN was more different than other models, or more powerful representation of the TSEN model.
\begin{table}[h]
\label{tab:ablation}
\centering
\caption{CKA Similarity Between Models}
\begin{threeparttable} %添加此处
\begin{tabular}{ccc}
\hline
Between Models           & First Layer & Second Layer \\ \hline
SBGCN and SBGCN\_SA      & 0.1251      & 0.0458       \\
SBGCN and SBGCN\_FFN     & 0.0595      & 0.0426       \\
SBGCN\_SA and SBGCN\_FFN & 0.0218      & 0.0356       \\
TSEN and SBGCN           & 0.0867      & 0.0234       \\
TSEN and SBGCN\_SA       & 0.0387      & 0.0267       \\
TSEN and SBGCN\_FFN      & 0.0188      & 0.0135       \\ \hline
\end{tabular}
\end{threeparttable} %添加此处
\end{table}

\subsection{The influence of hyper-parameters of TSEN}
We evaluated the influence of hyper-parameters of TSEN with the number of attention heads and the shape of FFN of the transformer.
\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{fig3.pdf}
    \caption{The impact of hyper-parameters of transformer of TSEN. (a) The number of attention heads is 2, 4, 8, 10, 20, 40, 50, (b) the shape of FFN is 256, 512, 1024, 2048, 3072, 4096. The green line represents F1 score, the blue line represents accuracy, and the orange line represents AUC of classification performance on ABIDE I dataset.}
    \label{fig:param}
\end{figure*}

As shown in Figure ~\ref{fig:param}, we evaluated the influence based on the metric of accuracy, AUC, and F1 score on ABIDE I dataset. The results showed that the number of heads of self-attention and the shape of FFN, had significant influence on the performance of TSEN. From Figure ~\ref{fig:param}(a), the results showed the performance of TSEN did not increase linearly with more heads, and showed decreased performance with more than 20 heads. That indicated excessive heads were redundant and might not contribute to improve the performance~\cite{RN54}. From figure ~\ref{fig:param}(b), the results also showed the performance of TSEN did not increase linearly with more shape of FFN. That might be because the higher dimension of FFN led to the model more difficult to be trained.

\section{Conclusion}
\label{sec:conclusion}
To learn an effective representation for biomedical graph classification, we proposed TSEN, a novel graph transformer model to capture whole-graph representation with multi-scale information for classification. The results of experiments demonstrated the combination of graph snowball structure and graph transformer could enhance the power to capture global patterns. That also demonstrated using graph convolution as position embedding in transformer structure was a simple yet effective method to capture graph patterns of graphs naturally.

For future work, we would investigate the connections between the graph properties and the graph transformer architectures, which can help better understanding the strong potential of graph transformer models. Besides, we would  further evaluate TSEN on more biomedical graph data from molecular level to healthcare systems level.
\section{Acknowledgements}
\label{sec:ack}
This work was supported by the Natural Science Foundation of Guangdong Province of China [2021A1515011942], and the Innovation Fund of Introduced High-end Scientific Research Institutions of Zhongshan [2019AG031].

\bibliographystyle{unsrt}  
\bibliography{references}  %%% Remove comment to use the external .bib file (using bibtex).
%%% and comment out the ``thebibliography'' section.


%%% Comment out this section when you \bibliography{references} is enabled.


\end{CJK}
\end{document}
