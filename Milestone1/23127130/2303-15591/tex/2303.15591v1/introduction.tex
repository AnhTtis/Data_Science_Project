\section{Introduction}
\label{sec:intro}
\noindent Scaling up of neural nets in the past few years has steadily improved performance on wide variety of downstream visual tasks. However, model adaptation is often necessary to achieve the best performance in downstream tasks like fine-grained recognition \cite{WelinderEtal2010}, semantic segmentation \cite{DBLP:journals/corr/ChenPK0Y16} or object recognition \cite{8825470}. While traditional techniques like full-model finetuning have become the de-facto approach to  adaptation, they are not well suited for many  scenarios. For example, finetuning is susceptible to catastrophic forgetting \cite{kirkpatrick2017overcoming} as it modifies model parameters without the knowledge of future domains, and potentially losing prior knowledge of current adaptation. Moreover, finetuning all of the model parameters of a large vision model with just a few training examples can lead to poor generalization. This is in contrast to human intelligence that is capable of solving wide variety of downstream tasks with extremely few exemplars. 

\noindent Motivated by the need for better adaptation, parameter efficient techniques like partial-finetuning or adapters\cite{rebuffi2018efficient, zhang2020side} have been developed to constructively adapt large models without significant parameter overhead. While serving as  effective alternatives to finetuning, most parameter efficient techniques have been designed with convolutional architectures in mind. In light of recent works \cite{dosovitskiy2020vit} that demonstrate that Vision Transformers are more suitable for scaling up than CNNs, designing adaptation techniques that exploit the Transformer architecture can be extremely useful. To that end, visual prompt tuning (VPT) \cite{jia2022vpt} has been proposed as a way to constructively adapt transformers by introducing learnable tokens at every layer that interact with the patch and class tokens and are optimized together with a classifier head. While being effective in practice, VPT allows only partial interactions between prompts and the remaining tokens, thus, leveraging only a part of the prompt capacity.
Moreover, it often requires a large number of inserted prompts to achieve optimal performance but that significantly increases the computation costs due to the quadratic computational complexity of the self-attention layer.


\noindent In this work, we explore an alternate design to prompting motivated by the potential for greater prompt capacity. We propose ExPRes, an \textbf{ex}pressive \textbf{p}rompt tuning method with \textbf{res}idual tokens that inherits the strengths of parameter efficient adaptation while significantly improving downstream performance. Our prompt design is inspired by the two key observations - propagation of prompts by multilayered interaction with other tokens is crucial for strong capacity and learnable residual tokens can modulate the propagated prompts to favour task-specific relations (unlike in \cite{jia2022vpt}). We first propagate \textit{shallow} prompts through the encoder that are average pooled at the last layer to yield semantic image-level representations. Shallow prompts by themselves have limited capacity since they cannot specifically modulate token-token relations at higher layers. Therefore to harness the prompts, we add residual tokens to propagated prompts at various layerwise computations of the Transformer encoder including LayerNorm, self-attention and multi-head projection to facilitate layerwise modulation without increasing the number of prompts per layer.  This results in enhanced prompt capacity at almost no additional computational cost.

\noindent We empirically validate the effectiveness of our method on a variety of downstream 
% generalization
tasks including fine-grained recognition and semantic segmentation. Our use of additional learnable parameters in the form of residual and shallow prompts allows the retention of prior knowledge in the form of frozen encoder weights while being extremely parameter efficient (prompts are $\leq 1 \%$ of the total parameters). Thus, our method is highly suited for real world adaptation that requires information retention at low memory and computational overheads. Additionally, we show that in most cases we require fewer prompts than VPT to achieve the same or better performance, making it more suitable for limited data settings.
Our main contributions can be summarized as follows:
\begin{itemize}
\item We propose a novel prompting technique: EXPRES, that uses a combination of shallow and deep residual prompts to facilitate constructive adaptation to downstream tasks with limited labelled datasets.
\item Our method significantly outperforms full-finetuning based adaptation by $4.6\%$
on VTAB-1k. Moreover, our method outperforms state-of-the-art prompting approach \cite{jia2022vpt} on the same benchmarks with significantly fewer prompts, suggesting that prompt design is crucial to extracting more capacity at a given parameter/computational budget.
\item To the best of our knowledge, we are the first to demonstrate the effectiveness of prompting for diverse applications such as few-shot semantic segmentation. Our method outperforms strong adaptation baselines by $25\%$ and achieves competitive performance with respect to language-assisted segmentation \cite{li2022languagedriven} despite training on significantly less data with dense annotations.
\end{itemize}


