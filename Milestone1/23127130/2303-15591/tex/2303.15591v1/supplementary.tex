\appendix
%%%%%%%%%%%%%%%%
\iffalse
\section{\rdnote{Plan for supplementary}}
\begin{itemize}
\item FGVC (Timm checkpoint does better)
\item  21k and standard checkpoints are different, but weights are very similar! (weights are within 0.01)
\item Hyperparams: Mention the range (segmentation done, need to mention classification)
\item Yoni to get 1.0 train 
\item Description of experiments
\item Segmentation visualization
\end{itemize}
\fi
%%%%%%%%%%%%%%%%%

\section{Hyperparamter Details for Main Results}
\begin{table}[h]
    \centering
    \begin{tabular}{cc}
    \toprule
    Hyperparameter &Range\\
    \midrule
         optimizer& ADAMW\\
         learning-rate, ($lr$)& $\{0.005, 0.001, 0.0005, 0.0001\}$ \\
         weight-decay,  ($wd$)& $\{0.0001,0.001\}$ \\
         epochs & 100 \\
         warmup epochs& 10 \\
         \bottomrule
    \end{tabular}
    \caption{\textbf{Hyperparameter Range}}
    \label{tab:hypam_summ}
\end{table}

\begin{table*}[h]
    \centering
    \resizebox{1
    \textwidth}{!}{
\begin{tabular}{ccccccccc|ccccc|ccccccccc}
\toprule
 &  \multicolumn{8}{c}{Natural}  &     \multicolumn{5}{c}{Specialized}    &\multicolumn{9}{c}{Structured}   \\
\midrule
        & \rotatebox{90}{\bf{CIFAR-100}}
  &\rotatebox{90}{\bf{Caltech101} }
  &\rotatebox{90}{\bf{DTD} }
  &\rotatebox{90}{\bf{Flowers102} }
  &\rotatebox{90}{\bf{Pets} }
  &\rotatebox{90}{\bf{SVHN} }
  &\rotatebox{90}{\bf{Sun397} }
  &\rotatebox{90}{\bf{Mean}}
  &\rotatebox{90}{\bf{Patch Camelyon} }
  &\rotatebox{90}{\bf{EuroSAT} }
  &\rotatebox{90}{\bf{Resisc45} }
  &\rotatebox{90}{\bf{Retinopathy} }
  &\rotatebox{90}{\bf{Mean}}
  &\rotatebox{90}{\bf{Clevr/count} }
  &\rotatebox{90}{\bf{Clevr/distance} }
  &\rotatebox{90}{\bf{DMLab}}
  &\rotatebox{90}{\bf{KITTI/distance} }
  &\rotatebox{90}{\bf{dSprites/location} }
  &\rotatebox{90}{\bf{dSprites/orientation} }
  &\rotatebox{90}{\bf{SmallNORB/azimuth} }
  &\rotatebox{90}{\bf{SmallNORB/elevation} }
  &\rotatebox{90}{\bf{Mean}}  \\
\midrule
VPT-shallow &77.7 &86.9 &62.6 &97.5 &87.3 &74.5 &51.2 &76.81  &78.2 &92.0 &75.6 &72.9 &79.66  &50.5 &58.6 &40.5 &67.1 &68.7 &36.1 &20.2 &34.1 &46.98 \\
num. prompts ($M$)&100 &5 &1 &200 &50 &200 &1 &79.4 &5 &50 &50 &10 &28.7 &100 &200 &100 &100 &100 &100 &200 &200 &137.5\\
 \midrule
 VPT-deep &\bfseries78.8 &\bfseries90.8 &65.8 &98.0 &88.3 &78.1 &49.6 &78.48  &81.8 &96.1 &83.4 &68.4 &82.43   &\bfseries68.5 &60.0 &\bfseries46.5 &72.8 &73.6 &47.9 &\bfseries32.9 &\bfseries37.8 &54.98 \\
 num. prompts ($M$)&10 &10 &10 &1 &1 &50 &5 &12.4 &100 &100 &10 &1 &52.8 &50 &200 &100 &50 &10 &50 &200 &200 &107.5
\\
 \midrule
 EXPRES (\textit{ours})&78.0&89.6&\bfseries68.8&\bfseries98.7&\bfseries88.9&81.9&\bfseries51.9&\bfseries79.7&\bfseries84.8&\bfseries96.2&80.9&\bfseries74.2&\bfseries84.0&66.5&\bfseries60.4&\bfseries46.5&\bfseries77.6&\bfseries78.0&\bfseries49.5&26.1&35.3&\bfseries55.0\\
 num. prompts ($M$)&30&10&15&10&5&10&10&12.9&30&10&10&10&15.0&100&10&10&10&30&30&10&30&28.75\\
 \bottomrule
\end{tabular}    }
    \caption{\textbf{Extended \vtab{} results:} Comparing \expres{} with VPT-shallow and VPT-deep (VPT) with optimal number of prompts per \vtab{} task. The highest accuracies are highlighted per task.}
    \label{tab:vtab_hypam}
\end{table*}

\noindent The hyperparameters and their ranges used in our main experiments are tabulated in Table \ref{tab:hypam_summ}. In Table \ref{tab:vtab_hypam}, we report the original results of Table \ref{tab:vtab} with optimal number of prompts per task.When comparing to other prompting techniques like VPT, we use the official implementation\footnote{\url{https://github.com/KMnP/vpt}}.  We observe that in many tasks (like SVHN, Patch-Cam., Clevr/distance, Kitti/dist., dsprites/orient.), \expres{} outperforms VPT-deep with significantly fewer prompts. 

\section{Effect of Hyperparameters}
\label{sec:hypam_suppl}



\begin{figure}[h]
    \centering

\includegraphics[width=0.9\columnwidth]{figures/hypam.png}
\caption{\textbf{Effect of hyperparameters for Five-Shot Semantic Segmentation on \pascal{0}}}
    \label{fig:hypam_voc}
\end{figure}

 \begin{figure}[h]
     \centering
     \includegraphics[width=0.9\columnwidth]{figures/vis.png}
     \caption{Five-Shot Predictions on \pascal{i}}
     \label{fig:viseg}
 \end{figure}

\noindent We study the effect of hyperparameters on various baselines and \expres{} used for five-shot segmentation. To create the validation set for the $i^\text{th}$ fold, we sample $100$ random tasks from the corresponding train-split that consists of fold exclusive categories, $\{5^j|j\neq i\}$ and evaluate the average accuracy over these tasks. For each baseline method, we test the following learning rates: $\{0.0001, 0.0005, 0.001, 0.005\}$. For prompt based methods, we fix the weight decay to $0.0001$ and test following number of prompts: $\{5,10,20\}$. For rest of the methods (non-prompt-based), we vary the weight decay in the set, $\{0.001,0.0001\}$. In Figure \ref{fig:hypam_voc}, ADAMW is chosen as the optimizer and fold-$5^0$ as the fold for analysis. For other hyperparameters like number of epochs and warmup epochs, fixed values of $100$ and $10$ respectively worked well. We use the above hyperparameters validation to pick the optimal values for final evaluation. In Figure \ref{fig:viseg}, we visualize the predictions for \expres{} and compare it to various baselines. \expres{} tends to produce more complete segmentation masks than others across different folds. 


\section{Additional Ablations on \vtab{}}
\begin{figure}[h]
    \centering
    \scriptsize
\includegraphics[width=0.9\columnwidth]{figures/propagation_type_vtab.png}
    \caption{\textbf{Prompt propagation (additional ablations):} Effect of propagating prompts with modulation upto a layer, $l=\{2, \ldots, 12\}$ of the ViT-B/16 encoder with total $12$ layers. The datasets are sampled from the \vtab{} benchmark.}
    \label{fig:propag_suppl}
\end{figure}


\begin{figure*}[t]
    \centering

\includegraphics[width=0.9\linewidth]{figures/prompt_type_vtab.png}
    \caption{\textbf{Residual Prompt Types (additional ablations):} Evaluating the importance of different type of residual prompts on \vtab{} datasets.}
    \label{fig:ab_ptype_suppl}
\end{figure*}

\noindent We demonstrate the importance of \textbf{prompt propagation} and \textbf{ residual prompts} on additional tasks (\textit{DTD} and \textit{Clevr-Count}) from the \vtab{} benchmark. In all ablations, we use optimal learning rates and weight decays with $M=15$ for \textit{DTD} task and $M=10$ for \textit{Clevr-Count} task.   In Figure \ref{fig:propag_suppl}, we repeat the ablation of propagating shallow prompts without residual prompting for additional datasets. We observe similar trends as  in Figure \ref{fig:propag}- downstream performance depends directly on the extent of prompt 
propagation with modulation \ie, interaction with other tokens via self-attention. We also observe that for \textit{Clevr-Count}, the performance quickly rises upto layer $4$, then plateaus. Thus, the exact performance trend with increasing propagation through the layers varies slightly with the downstream task. 


\noindent In Figure \ref{fig:ab_ptype_suppl}, we repeat the ablation that delineates the importance of each type of residual prompt for additional datasets. We observe that trends are similar to  Figure \ref{fig:ab_ptype}. Within each block (\textit{Att} and \textit{MLP}), layer norm residual prompts yield notable improvements in most tasks. Within MSA block (\textit{Att}), the \textit{QKV} prompts are dominant over \textit{Proj} prompts for \textit{natural} tasks (DTD, svhn) of \vtab{} while the trend reverses for \textit{structured} tasks (Clevr-Dist, Clevr-Count). Most importantly, when comparing blockwise performance, prompting the MSA block (\textit{Att}) consistently outperforms prompting the MLP block (\textit{MLP}), reinforcing the conclusions in \S \ref{subsec:ablate}.


\section{Effect of Prompt Initiation Layer}

 \begin{figure}[t]
       \includegraphics[width=1.1\columnwidth]{figures/propagation_startfrom_vtab.png}
        \caption{\textbf{Starting \expres{} prompting at specific layers: } $l_1 \to l_2$ means prompting starts at $l_1$ (closer to input) and ends at $l_2$.  number of prompts is fixed at $10$.}
     \label{fig:prompt_layer}
 \end{figure}
\noindent One of the key design decisions of \expres{} prompting is where to insert the prompts. In Fig. \ref{fig:prompt_layer} we show that initiating \expres{} prompting at early layers is generally a good strategy. Combined with the observations in \ref{fig:propag}, we recommend using prompts at every layer to ensure best performance.

\section{Effect of Architectural Choice}

\begin{table}[h]
\setlength{\tabcolsep}{2pt}
    \centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{cccccccc}
\toprule 
&Linear&MLP-3&Partial-1&Biastune&\vpt{}&\vptsh{}&\expres{}\\

\midrule 
VTAB-\textit{Specialized} &80.8&75.2&81.7&80.1&84.5&82.5&\bfseries84.6\\
	 \bottomrule
\end{tabular}  
}
    \caption{\textbf{Comparing various methods for adapting Swin-Base model pretrained on ImageNet-21k}}
    \label{tab:swin}
\end{table}


\noindent\expres{} was designed as a generic prompting technique for Transformer architectures. To demonstrate it's generality, we compare our method with other adaptation techniques with Swin \cite{liu2021swin} transformers as the backbone. In particular, we incorporate \expres{} prompts in Swin-Base architecture, where fixed number (10) of shallow prompts are propagated through all four stages without any modification during patch merging with residual prompts being added at each layer of the SwinTF-blocks. Evaluations on a subset of the \vtab{} dataset in Table \ref{tab:swin} shows that overall our method outperforms various baselines and state-of-the-art methods even when applied to a different variant of transformer architecure.


\section{FGVC results}
\begin{table}
\scriptsize
\begin{center}
\begin{tabular}{
crrrrr
}
\toprule
  &Cub&Flower&Dogs&Cars\\
  \\
\midrule
\fullft{} &87.3 &98.8 &89.4 &\bfseries84.5
\\
\linear{} &85.3 &97.9 &86.2 &51.3
\\
\partialft{}-1 &85.6 &98.2 &85.5 &66.2
\\
\mlp{}-3 &85.1 &97.9 &84.9 &53.8
\\
\sidetune{} &84.7 &96.9 &85.8 &48.6
\\
\bias{} &88.4 &98.8 &\bfseries91.2 &79.4
\\
\vptsh{} &86.7 &98.4 &90.7 &68.7
\\
\vpt{} &\bfseries88.5 &\bfseries99.0 &90.2 &83.6
\\
\expres{} (\textit{ours})&88.3&\bfseries99.0&90.0&80.5
\\
\bottomrule
\end{tabular}
\end{center}
\caption{\textbf{FGVC benchmark:} Per task adaptation results with ViT-B/16 model.}
    \label{tab:fgvc}
\end{table}
\noindent In Table \ref{tab:fgvc}, we compare the performance of our approach to adaptation baselines  and other prompting techniques on the FGVC benchmark. On four datasets our method outperforms most baselines and performs competitively with other prompting techniques.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Additional Discussion on Performance-Cost Tradeoff}
\begin{figure}[h]
\centering
\includegraphics[width=0.9\columnwidth]{figures/ClevrDist_suppl.png}
    \caption{\textbf{Additional Computational Efficiency of EXPRES} Comparing Accuracy vs Number of Prompts for VPT and \expres{}}
    \label{fig:compef_supp}
\end{figure}

\noindent In Figure \ref{fig:compef_supp}, we compare the computational efficiency of our \expres{} with VPT on additional \vtab{} datasets. We fix the learning rate and weight decay for EXPRES at $lr=0.01, wd=0.0001$ and for VPT at the optimal values from \cite{jia2022vpt}. Rest all hyperparameters like batch size, epochs and warmup epochs are left unchanged. We observe trends similar to Figure \ref{fig:compef}, where for a given accuracy specification, the difference between number of prompts required for \expres{} and VPT can be upto an order (\eg, clevr-count in Figure \ref{fig:compef_supp}).  


\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{c|cc}
         & Tuned Params ($\%$) & GMACs \\
         \midrule
         FT-all & 100.0 &17.47\\
         Linear & 0.090 &17.47\\
         \midrule
         \vptsh{} (M=1)& 0.091&17.56\\
         % \vptsh{} (M=10)& 0.099& 18.39\\
         \vptsh{} (M=100)& 0.179& 26.87\\
         \midrule                  
         \vpt{} (M=1)& 0.100&17.50\\
         % \vpt{} (M=10)& 0.197&17.72\\ 
         \vpt{} (M=100)& 1.166&19.96\\
         \midrule
         \expres{} (M=1)& 0.144 &17.56\\
         % \expres{} (M=10)& 0.637 & 18.39\\
         \expres{} (M=100)& 5.560 & 26.87\\
         \midrule
    \end{tabular}
    \caption{\textbf{Memory and computational cost analysis using a ViT-B/16 pre-trained on supervised ImageNet-21k}. We consider input resolution of $224\times224\times3$ and a $100$-way classification task. Tuned parameters are reported as a percentage of the parameters in the backbone model (including classifier head). Here, M is number of prompts per layer.}
    \label{tab:cost}
\end{table}

\noindent In Table \ref{tab:cost}, we compare the computational as well as memory cost of various adaptation techniques. The computational cost is reported in terms of GMACs and memory cost in terms of tuned parameters relative to full model parameters. We observe that with $100$ prompts, our \expres{} requires memory ($4.7$M params.) that is comparable to \vpt{} ($1$M params.) and  orders of magnitude less than finetuning  ($\sim 86$M params.). While the computational cost of our method ($26.9$ GMACs) is slightly more than Linear ($17.5$ GMACs) and \vpt{} ($19.9$ GMACs) when using large number ($100$) of prompts, our method greatly outperforms Linear (Table \ref{tab:vtab}) and achieves better optimal performance than \vpt{} with far fewer prompts (Figure \ref{fig:compef} and Figure \ref{fig:compef_supp}), providing good performance-cost tradeoffs.


\section{Ablations for Semantic Segmentation}
\begin{table}[h]
    \centering
    \scriptsize
    \begin{tabular}{cccc}
    \toprule
    &Q&K&MLP\\
    \midrule
    Accuracies &44.39&\bfseries51.61& 38.25\\
    \bottomrule
    \end{tabular}
\caption{\textbf{Ablation:} Effect of various representations for five-shot semantic segmentation on \pascal{0}}
\label{tab:segout}
\end{table}
\noindent We evaluate various ways of the extracting dense representations from transformer backbone for five-shot semantic segmentation. In Table \ref{tab:segout}, we compare last-layer keys ($K$), queries ($Q$) and MLP-block($MLP$) outputs. We use a learning rate of $0.005$, weight decay of $0.0001$, $5$ prompts and average the accuracies over $100$ tasks randomly sampled from the train-split of \pascal{0}.
We observe that keys (K) are the most effective representations for semantic segmentation, so we use them in all our segmentation experiments.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Interpretability of Learnt  Prompts}
\begin{figure}[h]
       \includegraphics[width=1\columnwidth]{figures/apple_prompts_linear_view.png}
        \caption{\textbf{\expres{} prompt attention at different layers:} We display input image (top-row), the attention maps of a trained \expres{} prompt (middle-row), and the attention map of the same prompt without residuals (bottom-row) evaluated at different layers. Attention maps for this prompt from early (close to input) to later (close to output) layers are arranged from left to right in each row.}
        % , the applied residuals result in expressive prompts.}
     \label{fig:modulate}
 \end{figure}
 
\noindent In Figure \ref{fig:modulate}, we provide visualizations to demonstrate that residual prompts learn semantic information and facilitate fine-grained layerwise modulation of the attention.
Here, an arbitrarily chosen but fixed prompt location is used for visualization purposes. We observe that residual prompts learn spatially fine-grained details that are diverse across layers and that removing them reduces the diversity of the attention maps across layers, confirming our hypothesis.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
