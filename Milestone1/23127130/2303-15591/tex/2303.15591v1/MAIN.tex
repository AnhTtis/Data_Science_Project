% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

\makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{subfiles} 
\usepackage{tikz}
% \usepackage[dvipsnames]{xcolor}
\usepackage{multirow}
\DeclareMathOperator*{\argmin}{arg\,min}
\definecolor{yd}{rgb}{0.0, 0.5, 0.0}
% \newcommand{\ydnote}[1]{}%{{\color{yd}#1}}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\newcommand{\ydnote}[1]{\textcolor{red}{[{\bf YD:} #1]}}

\newcommand{\ar}[1]{\textcolor{red}{[{\bf AR:} #1]}}
\newcommand{\rdnote}[1]{\textcolor{blue}{[{\bf RD:} #1]}}
\newcommand{\run}[1]{\textcolor{green}{[{\bf Active}#1]}}

% methods
\newcommand{\onlykey}[1]{\textcolor{black}{{\textit{only-key}}}}
\newcommand{\noatt}[1]{\textcolor{black}{{\textit{no-attn}}}}
\newcommand{\expres}[1]{\textcolor{black}{{EXPRES}}}
\newcommand{\vtab}[1]{\textcolor{black}{{VTAB-1k}}}
\newcommand{\vpt}[1]{\textcolor{black}{{VPT}}}
\newcommand{\vptsh}[1]{\textcolor{black}{{VPT-shallow}}}

\newcommand{\partialft}[0]{\textsc{Partial}}
\newcommand{\linear}[0]{\textsc{Linear}}
\newcommand{\fullft}[0]{\textsc{Full}}
\newcommand{\sidetune}[0]{\textsc{Sidetune}}
\newcommand{\mlp}[0]{\textsc{Mlp}}
\newcommand{\bias}[0]{\textsc{Bias}}
\newcommand{\adapter}[0]{\textsc{Adapter}}
\newcommand{\promptbias}[0]{\textsc{VPT+Bias}}

% datasets
\newcommand{\pascal}[1]{\textcolor{black}{{$\mathbf{\text{PASCAL}-5^#1}$}}}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{12148} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
% \title{Expressive Prompting With Residuals}
\title{Learning Expressive Prompting With Residuals for Vision Transformers}


% \title{Expressive Prompting With Residuals for Image Recognition}
% \title{Learning Expressive Prompting With Residuals for vision transformers}


\author{Rajshekhar Das$^{1,2*}$, Yonatan Dukler$^{2}$, Avinash Ravichandran$^{2**}$, Ashwin Swaminathan$^{2}$ \\
Carnegie Mellon University$^{1}$ \hspace{1cm} AWS AI Labs$^{2}$ \\
{\tt\small rajshekd@andrew.cmu.edu,~dukler@amazon.com,~swashwin@amazon.com}
}
% {\tt\small rajshekd@andrew.cmu.edu}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Rajshekhar Das, Yonatan Dukler$^{\dagger}$, Avinash Ravichandran$^{**}$, Ashwin Swaminathan \\
% AWS AI Labs\\
% {\tt\small \{dukler, ravinash, swashwin\}@amazon.com}
% }
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Prompt learning is an  efficient approach to adapt transformers by inserting learnable set of parameters into the input and intermediate representations of a pre-trained model.
In this work, we present Expressive Prompts with Residuals (\expres{}) which modifies the prompt learning paradigm specifically for effective adaptation of vision transformers (ViT).
Our method constructs downstream representations via learnable ``output'' tokens (\textit{shallow} prompts), that are akin to the learned class tokens of the ViT. 
Further for better steering of the downstream representation processed by the frozen transformer, we introduce residual learnable tokens that are added to the output of various computations. 
We apply \expres{} for image classification and few-shot semantic segmentation, and show our method is capable of achieving state of the art prompt tuning on 3/3 categories of the VTAB benchmark.
In addition to strong performance, we observe that our approach is an order of magnitude more prompt efficient than existing visual prompting baselines.
We analytically show the computational benefits of our approach over weight space adaptation  techniques like finetuning.
Lastly we systematically corroborate the architectural design of our method via a series of ablation experiments.

\let\thefootnote\relax\footnotetext{$^*$Work conducted while interning at AWS AI Labs.}
\let\thefootnote\relax\footnotetext{$^{**}$Work conducted while at AWS AI Labs.}

\end{abstract}

%%%%%%%%% BODY TEXT
% \subfile{introduction}
\iffalse
\ydnote{
Narrative
\begin{itemize}
    \item Adapting vision transformers
    \item Fine tuning is expensive
    \item Recently prompt tuning came about as a viable solution
    \item We present \expres{} marries shallow and deep.
    \item We show EXPRES is capable of beating state of the art while being more parameter efficient
    \item On VTAB dataset EXPRES beats VPT, this is despite only using the standard pre training
    \item We also apply EXPRES in few shot settings and findâ€¦
    \item Lastly we show EXPRES can extend past classification in the settings of semantic segmentation (if we have results)
    \item Action item, why is it okay to only consider latent space adaptations (i.e. adaptations that don't modify the ViT parameters)
    \item Potentially have ablation of the pre-training of ViT we use to drive hypothesis that pre-training quality leads to better prompts.
\end{itemize}
}
\rdnote{
Main Claims and corresponding experiments:
\item Performance: VTAB, FGVC
\item Generality: Segmentation
\item Efficiency: Prompts vs Accuracy 
\item Continual adaptation without high overhead: Parameter Efficiency 
}
\fi

%%%%%%%%%%%%%%%%%% INTRODUCTION %%%%%%%%%%%%%%%

\begin{figure}[t]
    \centering
    \includegraphics[width=.9\linewidth]{figures/teaser_basic.png}
    \caption{
    In this work we propose EXPRES, a novel adaptation technique for large vision models with the goal of diverse downstream adaptation.
    }
    \label{fig:teaser}
\end{figure}
 \subfile{introduction}
%%%%%%%%%%%%%%%%%% RELATED WORKS %%%%%%%%%%%%%%%%

\begin{figure*}[t]
    \centering
    % \includegraphics[width=.7\linewidth]{figures/opt4.png}
    \includegraphics[width=.8\textwidth]{figures/Express_Img.png}
    \caption{\textbf{\expres{} Architecture in detail:} \expres{} optimizes two types of prompts, shallow prompts (\eg, $p_i$) and residual prompts (\eg, $\nabla_{LN,i}$), to construct task specific representation without updating the pretrained encoder weights. Each residual prompt is a learnable vector that is added to the output of various computations such as Layer Norm, Query-Key-Value projections, and linear projection after the MSA operation.
    % \rdnote{compare VPT, memory  transformers and ours}
    % \ar{Can we reduce this figure?}
    }
    \label{fig:main}
\end{figure*}
\subfile{relatedworks}
%%%%%%%%%%%%%%%%%% APPROACH %%%%%%%%%%%%%%%%%% 
\subfile{methods} 
%%%%%%%%%%%%%%%%%% Experiments %%%%%%%%%%%%%%%
\subfile{experiments} 
%%%%%%%%%%%%%%%%%% Conclusion %%%%%%%%%%%%%%%
\section{Conclusion}
In this work we propose a novel prompting technique for adapting large vision models. Our method demonstrates strong performance across variety of downstream tasks with varying dataset sizes. Further, our method 
outperforms commonly used finetuning approach as well as the recently proposed VPT method on standard benchmarks. We also demonstrate diverse adaptation ability of our method from classification to semantic segmentation tasks in the few-shot setting. Lastly, our method is more parameter efficient that existing weight-space and prompt based adaptation techniques. In the future, we plan to extend our method to additional settings including vision-language learning.
%%%%%%%%% REFERENCES

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

 % %%%%%%% APPENDIX
 \newpage
 \subfile{supplementary}

\end{document}

