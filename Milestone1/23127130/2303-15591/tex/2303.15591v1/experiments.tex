\section{Experiments}


\begin{table*}[t]
    \centering
    \scriptsize
    \begin{tabular}{cccc|cccc}
\toprule
Method & Im-Pre & Dense-Pre & Backbone &  $5^0$ & $5^1$ & $5^2$  & $5^3$\\
\midrule
 SPNET\cite{8953827}&In-1k&\checkmark&ResNet-101&23.8	&17.0&14.1&18.3\\
 ZS3Net\cite{NEURIPS2019_0266e33d}&In-1k&\checkmark&ResNet-101&40.8&39.4&39.3&33.6\\
 LSEG\cite{li2022languagedriven}&In-1k&\checkmark&ViT-L/16&\textbf{61.3}&\textbf{63.6}&43.1&41.0\\
\midrule
Linear&In-21k&-&ViT-B/16&5.1&43.9&28.1&29.1\\
FT-all&In-21k&-&ViT-B/16&18.3&31.9&27.9&23.2\\
Biastune&In-21k&-&ViT-B/16&2.4&39.2&13.9&20.1\\
\midrule

VPT-deep \cite{jia2022vpt}&In-21k&-&ViT-B/16 &33.4&56.2&49.8&47.7\\
\expres{} (\textit{\textbf{Ours}})&In-21k&-&ViT-B/16&41.8&60.2&\bfseries52.4&\bfseries51.4\\
\bottomrule
\end{tabular}
\caption{\textbf{Five-Shot Semantic Segmentation on \pascal{i}:} Per fold adaptation results with ViT-B/16 model pretrained on ImageNet-21k.}
    \label{tab:fss_voc}
\end{table*}

We validate the effectiveness of \expres{} on a variety of benchmarks consisting of wide variety of tasks and dataset sizes. We also analyse the importance of various model components and design decisions. 

\noindent\textbf{Datasets: }To evaluate \expres{}, we use two different benchmarks, \vtab{} \cite{zhai2019largescale} and FGVC\cite{jia2022vpt}.  The \textbf{\vtab{}} benchmark consists of $19$ different visual classification tasks categorized under three groups: \textit{Natural} - tasks with natural images captured with standard cameras; \textit{Specialized} - tasks with images captured under specialized settings (medical and satellite imagery); and \textit{Structured} - tasks that requires understanding scene geometry, like object distance. Each task-specific dataset contains $1000$ training examples with varying number of samples per class, depending on the number of classes. For validation purposes and hyper-parameter selection, we use a $800-200$ split of the training set and then train on all $1000$ examples for final results, which are based on evaluation on the entire test set. The \textbf{FGVC} benchmark consists of the finegrained datasets including CUB \cite{WelinderEtal2010}, Oxford Flowers \cite{4756141}, Stanford Dogs \cite{KhoslaYaoJayadevaprakashFeiFei_FGVC2011} and Stanford Cars \cite{10.5555/3298023.3298221}. In conjunction with \vtab{}, we use the FGVC datasets to conduct key ablation studies for \expres{}. A random $90-10$  split of each dataset is used for hyperparameter selection. For few shot segmentation, we use the standard  \textbf{\pascal{i}} \cite{OneShot} benchmark that was created from PASCAL VOC 2012 \cite{Everingham2014ThePV} with extra mask annotations for 20 object classes, evenly divided into $4$ folds: $\{5^i: i \in \{0, 1, 2, 3\}\}$. Following prior works for evaluation scheme, we randomly sample $1000$ episodes per fold and report the average performance over all episodes of the corresponding fold. However, unlike prior works, we tune our \expres{} model on few-shot training examples and cross-validate the hyperparameters (\eg number of prompts, $M$) on a validation set with fold-exclusive categories. In all our experiments,  we use a reasonably small budget for hyperparameters (see supplementary) following recent studies \cite{oliver2018realistic} that highlight the likelihood of overoptimistic results in limited-labelled data settings due to  excessive hyperparameter tuning on large validation sets. \\
\textbf{Implementation Details: } In all our experiments, we use a fixed encoder, ViT-B/16 pretrained on ImageNet-22K \cite{ILSVRC15}. This model is effective on wide variety of tasks and allows direct comparison with prior works. For each downstream task, we train for a total $100$ epochs with an initial warmup of $10$ epochs. We use AdamW as our default optimizer with a suitable learning rate and fixed weight decay of $1e-4$. For \vtab{} and FGVC experiments, we use a fixed batch size of $64$. For segmentation experiments, since overall training set sizes are extremely small ($\leq 10$ images in total, we use the entire training set per batch. We use input image resolution of $224\times 224$ for classification tasks and $384\times 384$ for semantic segmentation tasks as dense prediction usually benefits from larger resolution. Since, ViT-B/16 is pretrained on standard  $224\times 224$ resolution images, we use interpolated positional embeddings to accommodate larger resolutions in the case of segmentation. Finally, we use standard data augmentations for classification benchmarks \ie, \texttt{Resize} $\to$ \texttt{Random-Crop} $\to$ \texttt{Horizontal-Flip} during training and \texttt{Resize} $\to$ \texttt{Center-Crop} during evaluation while for segmentation we only use \texttt{Resize}.

\noindent\textbf{Evaluation Metrics} For classification experiments, we use accuracy as our performance metric. For evaluating segmentation masks, we use mean intersection over union (mIoU) that averages over the intersection over union curves per class.

\subsection{Main Results}

\noindent We compare \expres{} with a number of commonly used adaptation techniques on \vtab{}. The adaptation methods can be categorized as \textit{head-oriented}, \textit{backbone-oriented} and \textit{prompt-based}. Under the first category,   \textit{Linear} only optimizes a linear classifier for the downstream task while \textit{MLP-k} uses a $k$-layer multilayer perceptron (MLP) as the classifier head. As an example of the second category, \textit{Sidetune} \cite{zhang2020side} uses features that are linearly interpolated between pretrained features and features from a ``side'' network trained on downstream data.  While \textit{Biastune} \cite{cai2020tinytl} adapts only the bias terms of an otherwise frozen backbone. \textit{Adapter-d} \cite{houlsby2019parameter,pfeiffer2020adapterfusion,pfeiffer2020AdapterHub} introduces lightweight MLP modules inserted between Transformer layers. \textit{Partial-k} finetunes the last \textit{k} layers while keeping the rest of the backbone frozen, finally \textit{FT-all} finetunes all the layers. 
% Under the third category, \textit{VisPrompt} \cite{bahng2022visual} optimizes learnable prompts that are introduced at the input layer.
\textit{VPT-shallow} \cite{jia2022vpt} additionally optimizes a linear classifier with the learnable input prompts propagated through the encoder. \textit{VPT-deep} \cite{jia2022vpt} introduces more capacity by replacing the propagated prompts with a new set of learnable prompts at each layer. 

\noindent\textbf{\vtab{} Results (Table \ref{tab:vtab}):} Across all three splits of \vtab{}, our method significantly outperforms the best \textit{head-oriented} techniques: $~+11\%$ (\textit{natural}), $~+7\%$ (\textit{specialized}) and $~+23\%$ (\textit{structured}). Similar trends hold even when comparing with the more powerful class of \textit{backbone-oriented} techniques. While \textit{FT-all} has been widely adopted as an effective technique for most adaptation scenarios, our method consistently outperforms it by a significant margin of $~+4\%$ (\textit{natural}), $~+1\%$ (\textit{specialized}) and $~+7\%$ (\textit{structured}). Most interestingly our method even outperforms other powerful prompting techniques like \textit{VPT-deep} on $12$ (out of $19$) datasets by a margin of about  $~+1\%$ (\textit{natural}), $~+2\%$ (\textit{specialized}) and $~+0.02\%$ (\textit{structured}). The performance gains are particularly impressive when considered from the perspective of computation vs performance tradeoff. Compared to \textit{VPT-deep} that uses $53$ prompts for \textit{natural} and $108$ prompts for structures, our method only requires $10$ and $29$ prompts for the respective splits. A more detailed summary is provided in the supplementary. 

\begin{figure}[ht]
    \centering
\includegraphics[width=0.8\columnwidth]{figures/propagation_type.png}
    \caption{\textbf{Prompt Propagation:} Effect of propagating prompts with modulation upto a layer, $l=\{2, \ldots, 12\}$ of the ViT-B/16 encoder with total $12$ layers. The datasets are sampled from the FGVC benchmark.}
    \label{fig:propag}
\end{figure}
\begin{figure*}[ht]
    \centering
\includegraphics[width=0.8\linewidth]{figures/prompt_type.png}
    \caption{\textbf{Residual Prompt Types:} Evaluating the importance of different type of residual prompts on FGVC datasets.}
    \label{fig:ab_ptype}
\end{figure*}

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\columnwidth]{figures/ClevrDist.png}
    \caption{\textbf{Computational Efficency Plots:} Comparing accuracy and number of prompts for VPT and \expres{}}
    \label{fig:compef}
\end{figure}


\subsection{Few-Shot Semantic Segmentation Without Dense Pretraining}
\label{sec:fss}

\noindent We test the efficacy of \expres{} on a novel adaptation setup where the backbone, pretrained on classification tasks, is directly adapted for few-shot semantic segmentation task without additional training on large densely-annotated datasets. This is in contrast with most works that follow a two-stage pretraining procedure \ie, training on ImageNet-1k with image level labels followed by meta-learning on densely annotated datasets constructed from the train folds of \pascal{i}. In contrast, we only perform the first stage pretraining on a sufficiently diverse dataset (ImageNet-21k in our case) with no meta-learning stage. During evaluation, each few-shot episode randomly samples a target image with a segmentation mask that assigns a label $1$ to the pixels corresponding to one of the object categories in that image and a label $0$ to remaining pixels (background). From the same object category, a set of five images are randomly sampled to form the training set where the mask of each image is annotated in the same way as the target. Our method as well as all baselines that perform meta-learning are optimized given a context set to yield a binary classifier for the target image. For dense representations, we use the last layer \textit{keys} of the MSA-block as we found them to be more accurate than the typical MLP-block output. We observe that our method consistently outperforms the baselines (by $25\%$) as well as other prompt techniques such as VPT-deep (by $5\%$). Surprisingly, our method even outperforms \cite{li2022languagedriven} that leverages densely annotated datasets (training data per fold) with large language models for a second stage of pretraining. Specifically, on 2/4 folds, \expres{} outperforms \cite{li2022languagedriven} by $10\%$ despite training on a significantly smaller densely-annotated dataset (five images). These results are particularly significant as they suggest that highly competitive results can be achieved with models pretrained on image classification. Consequently, results may be further improved by scaling up the image-level annotated datasets which are cheaper to scale than the densely annotated datasets. In the supplementary, we provide visualizations of the segmentation mask predictions by our method as well as Linear, FT-all and VPT-deep methods. 



\subsection{Ablation Studies}
\label{subsec:ablate}
We conduct extensive ablations to evaluate key design decisions used to develop \expres{} such as feature construction, residual prompting, number of prompts \emph{etc.} For all ablations, we use the same ViT-B/16 backbone. When using FGVC datasets, we use only $10\%$ of each dataset with official splits provided in \cite{jia2022vpt}. 

\noindent\textbf{Prompt Propagation:} We evaluate the importance of propagating prompts through the ViT-B/16 encoder with layerwise modulation. Specifically,  upto a layer $l$, we allow all tokens (patch, class and prompt) to attend to each other at every layer. Beyond $l$,  prompts are not allowed to interact with other tokens at all; they are simply projected by the value heads of MSA-block followed by MLP processing at every layer. In our experiments, we fix the number of prompts to $10$ irrespective of the dataset. From Figure \ref{fig:propag}, we observe that downstream performance depends directly on the extent of prompt propagation with modulation: accuracy improves as more layers allow prompts to interact with other tokens. These results motivate our method, \expres{}, that facilitates finegrained layerwise modulation via residual tokens.

\noindent \textbf{Residual Prompt Type:} In Figure \ref{fig:ab_ptype} we evaluate the importance of different residual prompt types (one-at-a-time) including attention, LayerNorm (\textit{LN}), query-key-value projections (\textit{QKV}) and linear multi-head projection (\textit{Proj}). We also evaluate residual prompting in the MLP block: after LayerNorm (\textit{LN,mlp}), first linear projection (\textit{L1,mlp}) and second linear projection (\textit{L2,mlp}). We evaluate the composite effect of multiple prompt types within a computational block \ie, \textit{Att} (MSA block) and \textit{MLP} (MLP block)
As a baseline, we provide the per-dataset results for shallow prompting, referred to as \textit{None} in the figure. The number of prompts at each layer are fixed at $10$. We observe that, within the MSA block, adding residual prompts to LayerNorm  and  query-key-value projections yields the most improvements ($3.5\%$ and $2.8\%$ on an average respectively) over \textit{None}. Moreover, LayerNorm prompts in the MLP block are also more effective than prompting the two linear layers. Comparing blockwise performance, prompting the MSA block (\textit{Att}) yields significantly better performance (by $1.6\%$) than  prompting the MLP block (\textit{MLP}).
The performance gap between \textit{Att} and \textit{MLP} highlights the importance of directly modulating layerwise interaction between tokens for better adaptation. Consequently, we use \textit{Att} as our default setting for all experiments. 

\noindent \textbf{Computational Efficiency of Prompting}: In prompting techniques, the primary computational overhead arises from the quadratic complexity (in number of tokens)  of the transformer encoder. So to evaluate computation efficiency of prompting, we investigate the rate at which performance improves with number of prompts and provide comparisons between our method and VPT in Figure \ref{fig:compef}. We evaluate on two different datasets sampled from different categories of \vtab{}. For a given accuracy, \eg $58.5\%$ on Clevr-Distance, \expres{} requires an order less prompts than VPT, resulting in 2 orders less computations. Overall \expres{} achieves higher optimal performance with far fewer prompts than VPT.

\iffalse
\begin{itemize}
\item Computational Efficiency
\item Visualizing clusters of attention weights after prompt tuning
\item Shared Prompts per type per layer
\end{itemize}
\fi
