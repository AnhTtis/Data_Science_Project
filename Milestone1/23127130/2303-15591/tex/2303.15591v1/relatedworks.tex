\section{Related work}
\noindent \textbf{Large Vision Models:} With the advent of Transformer models~\cite{vaswani2017attention} and adoption  to various computer vision tasks, including image classification~\cite{dosovitskiy2020vit,liu2021swin}, object detection~\cite{carion2020end,li2021benchmarking}, semantic and panoptic segmentation~\cite{strudel2021segmenter,zheng2020rethinking,wang2021max}, video understanding~\cite{girdhar2019video,wang2022bevt,feichtenhofer2022masked} and few-shot learning~\cite{doersch2020crosstransformers}, the scale of vision models have increased by orders of magnitude. Typically trained using large labelled data, either unimodal like ImageNet-21K \cite{ILSVRC15} or multimodal, these models demonstrate superior performance on wide variety of visual tasks. Given their superior performance and much larger scale compared to ConvNets, the question of adaptating such models efficiently becomes crucial. Motivated by the need, our work is primarily focussed on adaptation of Vision Transformers.

\noindent \textbf{Transfer Learning} has been extensively studied for vision tasks in the context of ConvNets~\cite{zhuang2020comprehensive} and many techniques have been introduced including side tuning~\cite{zhang2020side}, residual adapter~\cite{rebuffi2017learning}, bias tuning~\cite{cai2020tinytl}, \etc. However, Transformer specific adaptation for visual tasks has received relatively less attention. At the same time, in the NLP domain, the dominance of large-scale pre-trained Transformer-based Large Language Models (LLM)~\cite{devlin-etal-2019-bert,2020t5,brown2020gpt3}, has paved way for many approaches~\cite{he2022towards,guo2020parameter,hu2021lora} that efficiently fine-tune LLMs for different downstream NLP tasks~\cite{wang2018glue,wang2019superglue}. In this work we compare with the most representative methods for fair benchmarking. For example, Adapters~\cite{houlsby2019parameter} insert extra lightweight modules inside each Transformer layer. One adapter module generally consists of a linear down-projection, followed by a nonlinear activation function, and a linear up-projection, together with a residual connection~\cite{pfeiffer2020adapterfusion,pfeiffer2020AdapterHub}. Instead of inserting new modules, \cite{cai2020tinytl} proposed to update the bias term and freeze the rest of backbone parameters when fine-tuning ConvNets. BitFit~\cite{bao2021beit} applied this technique to Transformers and verified its effectiveness on LLM tuning. Through our experiments that we demonstrate that our method, EXPRES provides a more effective way of adapting Transformers  compared to prior approaches.

\noindent \textbf{Prompting:} An  alternative to traditional adaptation methods is prompting~\cite{liu2021pre} - originally proposed as a way of prepending language instruction to the input text so that a pre-trained LLM can ``understand'' the task. Through trial and error selection of appropriate prompts, GPT-3 shows strong generalization to downstream transfer learning tasks even in the few-shot or zero-shot settings~\cite{brown2020gpt3}. This was followed up by other works on better prompt construction~\cite{shin2020autoprompt,jiang2020can}.  Recent works ~\cite{li-liang-2021-prefix,lester-etal-2021-power,liu2021p} propose to treat the prompts as task-specific continuous vectors and directly optimize them via gradients during fine-tuning. Such approaches, named ``Prompt Tuning'' achieve performance comparable to finetuning but with 1000$\times$ less parameters in some cases. Following the success in LLMs, prompt have also been adopted for vision-language models ~\cite{radford2021learning,zhou2021learning,ju2021prompting,yao2021cpt,ge2022domain}. Nonetheless, all the above methods prompt the text encoders and hence are tied to language as input. However, many realistic visual tasks such as dense prediction may not be well aligned with the language modality. Thus, it becomes imperative to develop prompting approaches that can work in the visual modality. 
To that end, recent work on visual prompting \cite{jia2022vpt,conder2022efficient,sandler2022fine, wang2022learning, bahng2022visual} provides encouraging results. In particular, \cite{jia2022vpt} demonstrate that even in the visual domain, adaptation based on continuous prompting can outperform finetuning, especially when the training datasets are small. Our work however shows that current prompting methods do not fully exploit the capacity of prompting for a vision transformer. Through a principled approach to prompting, we derive a more effective prompting technique that achieves state-of-the-art performance at much smaller computational overhead.


\noindent \textbf{Few-Shot Classification:} Few-shot classification has received a lot of attention in recent years. While a variety of approaches \cite{song2022comprehensive} have been proposed, the most successful ones seek to transfer \textit{positive knowledge} either by finetuning~\cite{tian2020rethink, Dhillon2020A, afrasiyabi2020associative} or meta-learning~\cite{Snell2017PrototypicalNF, finn2017model, NIPS2016_6385,sung2018learning, Ravi2017OptimizationAA, Gidaris2018DynamicFV,garcia2017few,fei2021melr,kwon2021repurposing}. Finetuning based few-shot learners can be viewed as specialists that perform well on the target domain   \cite{chen2019closer,tian2020rethink,guo2020broader}, but suffer from catastrophic forgetting \cite{shi2021overcoming} on the base domain. Meta-learning approaches, on the other hand, can be seen as generalists that enjoy complete immunity against forgetting but at the cost of somewhat lower performance in the target domain. In this work, we propose EXPRES as a constructive adaptation technique that is immune to forgetting but at the same time benefits from task specific parameter tuning, thus, leveraging the best of both worlds. One of our key contribution is to demonstrate the transferability of Transformers from classification to dense prediction tasks (semantic segmentation) when adapted with EXPRES.

\noindent \textbf{Few-Shot Semantic Segmentation:} This task was originally proposed in \cite{shaban2017one}. Most works after that follow the metric learning paradigm \cite{dong2018few} with various novelties from improved support-query matching \cite{siam2020weakly,liu2020dynamic,yang2020brinet} to better optimization  \cite{zhu2020self,liu2021learning}, memory modules~\cite{wu2021learning,xie2021few}, graph neural networks~\cite{xie2021scale,wang2020few,zhang2019pyramid}, and more \cite{tian2020differentiable,lu2021simpler, he2021progressive,zhuge2021deep,zhang2021self,liu2021anti,min2021hypercorrelation,li2020fss}.  Some methods generate representative support prototypes with attention mechanism~\cite{zhang2019canet,gairola2020simpropnet}, adaptive prototype learning~\cite{siam2019amp,li2021adaptive,ouyang2020self}, or various prototype generation techniques~\cite{nguyen2019feature,yang2020prototype, wang2019panet,liu2020crnet}. In contrast, our approach directly adapts a model pretrained on image classification to few-shot semantic segmentation without intermediate pretraining on densely-annotated datasets.

