\section{Search Space Design and Training Analysis}
%As shown in many studies~\cite{autoformerv2,nse,attentivenas}, a well-designed search space is extremely important for neural architecture search, because it determines both the accuracy and efficiency bounds. Unlike CNN search spaces have been well-studied on optimizing the latency for mobile devices~\cite{ofa,proxylessnas,hurricane}, little attention is paid on designing a latency-friendly ViT search space for both weak and strong mobile devices. In this section, we first design a very large and latency-friendly search space to cover diverse mobile devices, then we discuss the challenges of training a very large search space and analyze the root causes of its poor performance.

%In this section, we first design a very large search space to support diverse mobile devices, then we  analyze the reasons for the poor performance of supernet training on this search space.
	\vspace{-1ex}
\subsection{Search Space Design}
	\vspace{-1ex}
%Inspired by recent  works~\cite{levit,nasvit} which achieve competitive accuracy under small-regime FLOPs, we design a hybrid CNN-ViT search space as shown in Table~\ref{tbl:searchspace}. In CNN stage, we adopt  MobileNetv2~\cite{mobilenetv2} (abbr. MBv2) and MobileNetv3~\cite{mobilenetv3} (abbr. MBv3)  building blocks.  In ViT stage, we redesign the attention layer based on NASViT and LeViT attentions to improve mobile efficiency.  We remove the latency-expensive talking head layers~\cite{shazeer2020talking} and replace Gelu activation with Hswish. We did not use shifted window attention in Swin~\cite{swin}, since the efficiency advances become marginally in a hybrid model when the input size is small. We measure the latency on real-world mobile phones. Our optimized attention can accelerate the inference latency by more than 2$\times$, significantly improving the efficiency of the transformer block.

 

%Common search spaces in NAS works~\cite{bignas,ofa} can be decoupled as: (1) macro-architecture that defines the choices of depths, widths, kernel sizes per layer;  and (2) building block choices for each layer. In supernet-based NAS, each layer is fixed to a specific building block (e.g., MobileNetV3 block) to utilize weight entanglement sharing technique. Based on this, we first design a latency-friendly building block for ViT search space, then we define the marco-architecture to cover the huge diversity. 

\noindent\textbf{Mobile-friendly ViTs}. While many works aim to design ViT models with high accuracy and small FLOPs, we observe that models with small FLOPs may have high latency on mobile devices. For example, NASViT-A0~\cite{nasvit} has fewer FLOPs than MobileNetV3, but it runs 2$\times$ slower on a Pixel4 phone.  MobileFormer~\cite{mobileformer} has only 52M FLOPs but runs 5.5$\times$ slower. This is because these models incorporate effective but \textit{mobile-unfriendly operations} that reduce FLOPs.

Our goal is to design accurate ViTs that can achieve low-latency on mobile devices. To achieve this, we draw inspiration from recent works~\cite{levit,nasvit} and construct our search space based on mobile-friendly CNN-ViT architecture as shown in  Table~\ref{tbl:searchspace}.
 In CNN stage, we use MobileNetv2~\cite{mobilenetv2} (MBv2) and MobileNetv3~\cite{mobilenetv3} (MBv3)  blocks. In ViT stage, we make key modifications  based on NASViT  attentions for better efficiency. We remove the slow talking heads~\cite{shazeer2020talking} and use Hswish instead of Gelu. We opt not to employ shifted window attention in Swin~\cite{swin}, because it does not help much when the input size is small. We measure the latency on real devices. Our attention can speed up latency by  $>$2$\times$, making the transformer block more efficient.

\vspace{2pt}
\noindent\textbf{A very large search space}.  
%To ensure optimal performance and efficiency for NAS, the design of block configurations, such as the choices for widths and depths, is crucial once the building blocks have been selected. 
Unlike previous ViT NAS works~\cite{autoformer,su2021vitas} that focused primarily on large models exceeding 1G FLOPs, our search space must accommodate a wide range of ViT configurations, from tiny to large, to meet the demands of diverse mobile devices. For instance, a high-end device like the Pixel 6 can handle a large ViT model with 500M FLOPs to meet latency constraint of $\sim$30ms, whereas a less powerful device such as the Pixel 1 requires a tiny model with $<$100M FLOPs to meet the same constraint.

%Once the building blocks are decided, the design of block configurations, such as widths and depths, plays a crucial role in determining both the performance and efficiency for NAS. In contrast to previous work which has primarily focused on large models, our search space must accommodate an wide range of ViTs, including both tiny and large configurations, to accommodate the huge diversity of mobile devices. For example, our measurements show that when targeting a latency of $\sim$30ms, we can deploy a large ViT model with 500M FLOPs on a high-end Pixel 6 device, but a weaker device like the Pixel 1 requires a smaller model with less than 150M FLOPs to meet this constraint.


 
 Table ~\ref{tbl:searchspace} presents the final search space. We add many small choices for each block dimensions to include tiny ViTs. We also make several key designs to cover  potentially good subnets based on previous works. Specifically, we follow~\cite{park2022how} and increase the maximun width choices and decrease the depth choices for ViT stages. We follow LeViT~\cite{levit} and allow $V$ matrix to have larger expansion ratio of \{2, 3, 4\} (i.e., $V$ scale). We also allow larger expansion ratios of \{2, 3, 4, 5\} for MLP layers, because they are not redundant when using a typical ratio of 2. 
 

 
 
% Table ~\ref{tbl:searchspace} presents the final search space. We add many small choices for each block dimensions to include tiny ViTs. We also make several key designs to cover  potentially good subnets based on previous works. Specifically,  inspired by the work~\cite{park2022how}, we enlarge the maximum choice of widths  and add smaller depths choices for ViT stages. Inspired by LeViT~\cite{levit}, we allow the dimension of $V$ matrix to search larger expansion ratio of  \{2, 3, 4\} (i.e., $V$ scale). Additionally, since previous studies have shown that ViT models often have little redundancy in MLP layers~\cite{vitstudy}, we allow for larger expansion ratios of \{2, 3, 4, 5\} in the MLP layers. As a result, our search space includes a wide range of ViT subnets with greatly varying sizes, ranging from 37M to 3191M FLOPs. 



In total, our search space covers a wide range of ViT subnets with varying sizes, from 37 to 3191 MFLOPs. It contains an enormous 1.09$\times10^{17}$ subnets, which is a significant increase of \textbf{10$^7$$\times$ larger} than a typical search space in two-stage NAS (refer to Appendix. C for details). This presents new challenges in training a high-quality supernet over such a large search space.





 
 %To this end, our search space covers a wide range of ViT subnets with different sizes, from 37M to 3191M FLOPs. In total, our search space has  1.09$\times10^{17}$ subnets to cover the huge diversity of mobile devices, which is 10$^6\times$ larger than a normal search space as shown in Table ~\ref{tbl:quantize}, posing new challenges to train a high-quality supernet. 

% However, \textit{existing ViT search spaces cannot be applied on the diverse mobile devices}, as the current design does not consider  weak mobile devices and is not optimized for latency.  For instance, even the int8 quantized version of smallest model in NASViT search space (190M FLOPs) runs  63 ms  on Pixel 1, which exceeds the real-time latency constraint, showing a big gap between the search space design  and deployment on weak devices. 


\begin{table}[t]
	%	\begin{center}
			\centering
		\fontsize{8.5}{8.5} \selectfont
			\vspace{-2.5ex}
		\caption{Our very large search space to support both weak and strong mobile devices. It's 10$^{7}\times$ larger than that in  two-stage NAS. Tuples of three values represent the lowest value, highest, and steps.}%: since we set attention head size to 16, the channel numbers in ViT stages use 16 as the step size. }
		\label{tbl:searchspace}
		\begin{tabular}		{@{\hskip2pt}c@{\hskip2pt}c@{\hskip3pt}c@{\hskip3pt}c@{\hskip5pt}c@{\hskip5pt}c@{\hskip2pt}}
	\hline
			Stage&Depths&\makecell{Channels}&\makecell{Kernel size \\(V scale)}& \makecell{Expansion\\ ratio} & Stride\\
			\hline
			Conv 3$\times$3 & 1 & (16, 24, 8)& 3 & -&2\\
			MBv2 block& 1-2& (16, 24, 8) & 3, 5& 1 &  1\\
			MBv2 block& 2-5 & (16, 32, 8) & 3, 5& 3, 4, 5, 6&2\\
			MBv3 block & 2-6&(16, 48, 8) & 3, 5& 3, 4, 5, 6&2\\
			Transformer & 1-5&(48, 96, 16)&2, 3, 4&2, 3, 4, 5&2\\
			Transformer & 1-6&(80, 160, 16)&2, 3, 4&2, 3, 4, 5&1\\
			Transformer & 1-6&(144, 288, 16)&2, 3, 4&2, 3, 4, 5&2\\
			Transformer & 1-6&(160, 320, 16)&2, 3, 4&2, 3, 4, 5&2\\
			MBPool& -&1984&-&6&-\\
			\hline
			Input resolution & \multicolumn{5}{c}{128, 160, 176, 192, 224, 256}\\
		\hline
		\end{tabular}  
\end{table}



\subsection{Analysis of Training a Very Large Supernet}
\label{sec:analysis}
\vspace{-1ex}
\begin{figure*}[t]
	\centering
	\includegraphics[width=0.9\textwidth]{gradient.png}	
\vspace{-2ex}
	\caption{(a) Model accuracy achieved by training from scratch vs. inheriting supernet weights; (b) Gradient cosine similarity of models with different FLOPs; (c) Under the same FLOPs level, good models share more similar gradients than random sampled models.}
	\label{fig:gradient}
\end{figure*}
Supernet training differs from standard single network training in that all the subnets share weights for their common parts. The shared weights may receive conflicting gradients that lead to different optimization directions, which lowers the final accuracy. Several techniques have been proposed to mitigate this issue~\cite{bignas,alphanet,nasvit}, have demonstrated success in training high-quality supernet over typical search spaces. Among them, sandwich  rule is essential to ensure performance lower and upper bounds. Specifically, it samples  a min, max and two random subnets per iteration.
%Several techniques have been proposed to mitigate this issue, such as sandwich rule sampling with inplace distillation~\cite{bignas}, $\alpha$-divergence-based KD~\cite{alphanet}, gradients projection and less data augmentation~\cite{nasvit}. 
%In particular, sandwich  rule is widely used to ensure performance lower and upper bounds. %With these techniques, we can train a high-quality supernet over a normal size search space.


However, when training a ViT supernet over our vast  search space in Table~\ref{tbl:searchspace}, we observe significant accuracy drop using previous best practices~\cite{alphanet,nasvit}. Specifically, we use sandwich rule and follow the same training receipts in NASViT~\cite{nasvit}.  Fig.~\ref{fig:gradient}(a)  compares the accuracy of 20 random subnets achieved by inheriting supernet weights to retraining on ImageNet. Compared to retraining, models derived from the supernet experience an accuracy drop with up to 8\%.


%We randomly sample 20 subnets  for evaluation, and compare the accuracy achieved by inheriting supernet weights with retraining. Fig.~\ref{fig:gradient}(a) shows that models from supernet suffer a large $\sim$8\% accuracy drop on ImageNet compared to retraining.







%Compared to the standard single network training,  a major difference of supernet training is that all the subnets share weights for their common parts in each layer. The shared weights may receive different gradients that result in different optimization directions, which causes  final accuracy drops. Many techniques are proposed to alleviate this issue,  including the popular sandwich rule sampling with inplace distillation~\cite{bignas}, $\alpha$-divergence-based KD~\cite{alphanet},  gradients projection and less data augmentation~\cite{nasvit}. Specifically, sandwich sampling rule is widely applied to  guarantee performance lower and upper bounds. By these techniques, we can train a high-quality supernet over a normal size search space.  Subnets can directly inherit  weights from  supernet for  deployment with comparable accuracy to those retrained from scratch, which eliminates the huge cost of training or fine-tuning of each subnets invidually.

%Though promising, we observe a significant accuracy degradation when training a very large supernet over our search space, which is $10^6\times$ larger than a normal size search space. We adopt sandwich rule sampling and  closely follow the previous best practices~\cite{alphanet,nasvit} for 360 epochs training on ImageNet. After the training, we  randomly sample 20 subnets with different FLOPs for evaluation. We compare the accuracy achieved by inheriting weights from supernet to those retrained from scratch. Fig.~\ref{fig:gradient}(a) shows the results. Compared to retraining, models from supernet drop a significant $\sim$8\% accuracy. 

\vspace{3pt}
\noindent\textbf{Analysis.} 
Compared to a typical ViT supernet, our supernet includes many tiny subnets and the sizes between subnets can differ greatly. %Therefore, we evaluate the gradient changes  among subnets with different sizes. 
We randomly sample subnets with MFLOPs ranging from 50 to 1200 from our supernet, and compute the cosine similarity of shared weights' gradient between each pair under the same batch of training data. A lower similarity indicates larger differences in gradients and thus more difficulty in training the supernet well. Fig.~\ref{fig:gradient}(b) shows that  \textit{gradient similarity of shared weights between two subnets is negatively correlated with their FLOPs difference} (\textbf{Observation\#1}). Subnets with similar FLOPs achieve the highest gradient similarity, while the similarity is close to 0 if there is a large FLOPs difference between two subnets.

Besides FLOPs difference, subnet quality may also affect gradient similarity of shared weights. If a poor subnet is sampled and trained, it would disturb the weights of good subnets. To verify this hypothesis, we randomly sample 50 subnets with same level FLOPs and compute the shared weights gradient cosine similarity between top subnets and randomly-sampled subnets. Fig.~\ref{fig:gradient}(c) suggests that \textit{gradient similarity between same-size subnets can be greatly improved if they are good subnets} (\textbf{Observation\#2}).



 %Compared the a normal size supernet, our  supernet contains a large number of tiny subnets ($<100$M FLOPs) and  the sizes of two subnets can vary significantly.  Therefore, we start by studying the gradient changes shared among subnets with different sizes. We randomly sample subnets under the FLOPs of 50M, 100M, 200M, 300M, 600M, 900M and 1200M from our supernet, and compute the cosine similarity of shared weights' gradient between each other under the same batch of training data.  A lower similarity indicates larger differences in gradients and thus more difficult to well train the supernet.   As shown in  Fig.~\ref{fig:gradient}(b), we surprisingly observe that \textbf{ 1) gradient similarity of shared weights between two subnets is negatively correlated with the FLOPs difference between them.} Subnets with same-level FLOPs achieve the highest gradient similarity, while the similarity is close to 0 if there is a big FLOPs difference between two subnets.



%Besides FLOPs difference, subnet quality may also affect gradient similarity of shared weights. If a weak subnet is sampled and gets trained, it would disturb the weights of good subnets. To verify this hypothesis, we randomly sample 50 subnets with same level FLOPs and compute the shared weights gradient cosine similarity betwen top subnets and randomly-sampled subnets. Fig.~\ref{fig:gradient}(c) suggests that \textbf{2) gradient similarity between same-size subnets can be greatly improved  if they are good subnets.}

%Compared the a normal size supernet, our  supernet contains a large number of tiny subnets ($<100$M FLOPs) and  the sizes of two subnets can vary significantly.  Therefore, we start by studying the gradient changes shared among subnets with different sizes. We randomly sample subnets under the FLOPs of 50M, 100M, 200M, 300M, 600M, 900M and 1200M from our supernet, and compute the cosine similarity of shared weights' gradient between each other under the same batch of training data.  A lower similarity indicates larger differences in gradients and thus more difficult to well train the supernet.   As shown in  Fig.~\ref{fig:gradient}(b), we surprisingly observe that \textbf{ 1) gradient similarity of shared weights between two subnets is negatively correlated with the FLOPs difference between them.} Subnets with same-level FLOPs achieve the highest gradient similarity, while the similarity is close to 0 if there is a big FLOPs difference between two subnets.



%Besides FLOPs difference, subnet quality may also affect gradient similarity of shared weights. If a weak subnet is sampled and gets trained, it would disturb the weights of good subnets. To verify this hypothesis, we randomly sample 50 subnets with same level FLOPs and compute the shared weights gradient cosine similarity betwen top subnets and randomly-sampled subnets. Fig.~\ref{fig:gradient}(c) suggests that \textbf{2) gradient similarity between same-size subnets can be greatly improved  if they are good subnets.}


%We now rethink the effectiveness of the sandwich rule in training a very large supernet. In this rule, a max, min, and 2 random subnets are trained at each step, which may have different FLOPs. In a large search space that includes tiny and large ViTs,  randomly sampling two subnets can easily lead to a FLOPs difference of more than 700M (refer the supplementary materials), where the gradient similarity is often close to 0. Furthermore, the quality of 2 random subnets is not guaranteed. Therefore, the sandwich rule  can easily cause gradient conflicts in a very large supernet.%, necessitating new methods to enhance training quality.
 




