\section{Methodology}
Inspired by the observations in \cref{sec:preference_rule},  we propose two methods to address the gradient conflict issue. (i) We introduce  complexity-aware sampling
to restrict the FLOPs difference between adjacent training steps, as large difference incurs gradient conflict problem (\cref{sec:adjacent_sampling}). (ii) Since ``good subnets'' can enhance each other, which further reduce gradient conflicts, we propose performance-aware sampling to ensure the training process can sample good subnets as much as possible (\cref{sec:preference_rule}). 

\subsection{Preliminary}
For a search space $\mathcal{A}$, two-stage NAS aims to train a weight-sharing network (supernet) over $\mathcal{A}$ and jointly optimize the parameters of all subnets. 
Since it is infeasible to train all subnets in $\mathcal{A}$, this is often achieved by the widely-used \textit{sandwich rule}, which samples two types of subnets at each training step: \textbf{(i)} an expectation term approximated by a sampled subnet set through a prior distribution $\Gamma$ over the search space \{$s_m \vert s_m \sim \Gamma (\mathcal{A})$\}$_{m=1}^M$ and
\textbf{(ii)} a fixed subnet set $s_n \in \mathcal{S}$.  Formally, the supernet training  can be framed as the following optimization problem:
\begin{equation} 
	\small
	\begin{aligned}
		& \underset {w} { \operatorname {arg\,min} } \, \left[
		\mathbb{E}_{s_m \sim \Gamma(\mathcal{A})} \mathcal{L}_{D} \left( f \left( w_{s_m}\right) \right) + \sum_{s_n \in \mathcal{S}} \mathcal{L}_{D} (f(w_{s_n})) \right] \\
	%	& \approx \underset {w} { \operatorname {arg\,min} } \, \left[ \sum^M_{s_m \sim U(\mathcal{A})} \mathcal{L}_\mathcal{T} (f(w_{s_m})) + \sum_{s_n \in \mathcal{S}} \mathcal{L}_\mathcal{T} (f(w_{s_n})) \right], 
	\end{aligned} 
	% \sum^M_{s_m \sim \Gamma(\mathcal{A})} \mathcal{L} (f({\bf{x}}, w_{s_m}); {\bf{y}}) \right], 
	\label{eq:sandwich_rule}
\end{equation} 
where $w$ is the shared weights for all subnets, $f(\cdot)$ denotes the neural network, $\mathcal{L}_\mathcal{T}$ is the loss on the training set $D$ and $w_s$ is the exclusive weights of subnet $s$. 


Without loss of generality, there are two types of subnets in any space --  \emph{the random subnets that each dimensions are sampled between maximum and minimum  settings}; and \emph{the smallest and largest subnets that each dimensions	are the minimum and maximum settings, respectively}.
In recent works \cite{bignas, attentivenas, nasvit}, sandwich rule often approximates the first term in Eq. \ref{eq:sandwich_rule} by randomly sampling $M$=2 subnets from a uniform distribution $\Gamma$. In the second term, $\mathcal{S}$ typically consists of the largest subnet $s_l$ and the smallest subnet $s_s$.

We now revisit the effectiveness of applying sandwich rule on training our mobile-specialized supernet in Table~\ref{tbl:searchspace}. Obviously, it can easily cause gradient conflicts due to two reasons. First, it always sample the smallest (37M FLOPs), biggest (3191 MFLOPs) and 2 random subnets, which results in a significant FLOPs difference and often causes the gradient similarity to be close to 0 (Fig.~\ref{fig:gradient}(b)). Second, the size and quality of the 2 randomly sampled subnets cannot be guaranteed, which 
exacerbates the issue.



%We now rethink the effectiveness of the sandwich rule in training a very large supernet. In this rule, a max, min, and 2 random subnets are trained at each step, which may have different FLOPs. In a large search space that includes tiny and large ViTs,  randomly sampling two subnets can easily lead to a FLOPs difference of more than 700M (refer the supplementary materials), where the gradient similarity is often close to 0. Furthermore, the quality of 2 random subnets is not guaranteed. Therefore, the sandwich rule  can easily cause gradient conflicts in a very large supernet.


\vspace{-1ex}
\subsection{Complexity-aware Sampling}
\label{sec:adjacent_sampling} 
\vspace{-1ex}
%The first term in Eq. \ref{eq:sandwich_rule} is crucial for optimizing numerous subnets during training. To further understand the impact of \emph{a very large space} to it, Fig. \ref{fig:space_comparison} visualizes the behavior of uniform sampling over two different search spaces. One can see when the search space $\mathcal{A}$ is sufficiently large, great complexity gaps occur at any time by using a uniform distribution. For a large search space, this apparently violates \textbf{Observation\#1} that the complexity difference between the sampled subnets should be as small as possible. Moreover, it also reveals that the original training scheme in Eq. \ref{eq:sandwich_rule} for a large search space can result in gradient conflicts for the sampled subnets and cause the subnets to converge to sub-optimal eventually. Therefore, the preoccupation is to relieve the conflict by eliminating the difference between subnets as much as possible. Accordingly, we handle each of these subnets in the following way. 

In this section, we introduce complexity-aware\footnote{We use \rm{FLOPs} to represent the complexity metric, which can trivially be generalized to other metrics (e.g., latency).} sampling to mitigate the gradient conflicts by large FLOPs difference, while ensuring that different-sized subnets can be trained.



\noindent\textbf{Adjacent Step Sampling For Uniform Subnets.} 
Due to the extremely large space, randomly sampling $M$ subnets can result in significant FLOPs differences (\emph{see Appendix.C for detailed illustrations}) both within the same training step and across different training steps. 
% Hence, we propose to constrain the FLOPs of $M$ random subnets for each training step and across different training steps. 
Therefore, we propose to constrain the FLOPs of $M$ random subnets for each training step. 
As shown in Fig.~\ref{fig:alg}, we simply constrain the subnets within the same training step to have the same level of FLOPs.

However, it is non-trivial to effectively constrain the FLOPs differences between different training steps, as it requires ensuring that subnets of varying sizes can be trained to support diverse resource constraints. We propose  adjacent step sampling to gradually change the FLOPs level. 

%Following this, we first define a \emph{complexity distance function $g(s; \mathcal{F})=|{\rm{FLOPs}}(s)-\mathcal{F}|$} to measure the complexity difference between a sampled subnet $s$ and a certain $\rm{FLOPs}$ constant $\mathcal{F}$ (e.g., $\mathcal{F}$ = 600 $\rm{MFLOPs}$). 

%At step $t$, to ensure the near complexity to last step $t-1$, for a sampled subnet, whether it is trained or not should satisfy $g\left(s^{(t)}; \mathcal{F} = {\rm FLOPs}(s^{(t-1)})\right) \leq Z$, where $Z$ is the complexity threshold hyper-parameter, otherwise a new subnet should be re-sampled. 
%This limits the complexity differences between two adjacent training steps to relieve the gradient conflict problem. 

Specifically, we define a set of gradually increased complexity levels ${C_1, ..., C_K}$ (e.g., {100, 200, ..., 800} {\rm MFLOPs}), which cover a range of ViT models from tiny to large. Suppose step $t$-1 samples the $i^{th}$ complexity level $C^{(t-1)}_i$, and step $t$ samples $M$ subnets $s^{(t)}$ under the $j^{th}$ complexity level of $C^{(t)}_j$. To satisfy the adjacent step sampling, the FLOPs distance between these two steps must satisfy the following:
\begin{equation}
	\small
	g\left(s^{(t)};  C^{(t-1)}_i\right) =  |C_j^{(t)} - C^{(t-1)}_i|=0
\end{equation}
To satisfy the equation, we offer $C^{(t)}_j$ three options: ${C_{i-1}^{(t-1)}, C_i^{(t-1)}, C_{i+1}^{(t-1)}}$, representing the choices of \emph{decreasing the FLOPs level by one}, \emph{maintaining the current FLOPs level}, or \emph{increasing the FLOPs level by one}, respectively.

\vspace{2pt}
\noindent\textbf{Remove the Biggest Subnet.} Since the biggest subnet has 3191 M\rm{FLOPs}  in our search space, it naturally introduces a large FLOPs difference with our considered complexity levels, which can cause gradient conflict at each step. 
Empirically, we find that removing the largest subnet stabilizes the training process and improves overall performance. 
% That ulteriorly demonstrate the above hypothesis that the complexity

\vspace{2pt}
\noindent\textbf{Use Multiple Hierarchical Smallest Subnets (HSS).} 
Since the smallest subnet has only 37 MFLOPs, sampling at the large complexity range ($C_i \geq $ 500 MFLOPs) can also introduce large FLOPs difference at each step. Unlike the biggest subnet, the smallest subnet decides the performance lower bound of the whole space \cite{tang2022arbitrary}, which cannot be removed simply. Therefore, instead of sampling the min subnet with only 37 MFLOPs at each step, we sample a nearest min subnet from the \emph{hierarchical smallest subnets (HSS) set} $\hat{\mathbf{S}} = \{s_n\}_{n=1}^N$. 
HSS set includes $N$=3 pre-defined subnets with discrepant complexity. 
At step $t$, when sampling around a complexity level $C^{(t)}$, we select a subnet $s_n \in \hat{\mathbf{S}}$ whose complexity is closest to it as the smallest subnet, as in Eq. \ref{eq:hss_equation}. 

In our experiments, we conduct empirical analysis and select the $N$=3 smallest subnets as the HSS set $\hat{\mathbf{S}}$. As shown in Fig.~\ref{fig:alg}, these subnets include the original 37 MFLOPs subnet (min$_1$), as well as a 160 MFLOPs subnet (min$_2$) and a 280 MFLOPs subnet (min$_3$), which are sampled and added as the second and third smallest subnets, respectively. 



\emph{Discussion for HSS set.} The multiple ``smallest'' subnets in HSS set logically partition the whole search space into $N$ hierarchical sub-spaces (Fig.~\ref{fig:alg}). This is differs from previous methods\cite{autoformer,autoformerv2}, which manually divide the space into separate sub-spaces and train them individually. The HSS set offers two advantages: \textit{(i)}: it enables unified weight-sharing across the entire space, allowing small subnets to benefit from large subnets. Moreover, it has been proven that incorporating small models into larger ones can significantly enhance small subnets' performance\cite{cai2021network}. \textit{(ii)}: It does not rely on any heuristic space re-design or strong relationship assumption between dimensions (\emph{e.g.,} linear correlation in \cite{autoformerv2}), which enhances the universality of our method. 



% A previous study \cite{tang2022arbitrary} has demonstrated the importance of the smallest model, which is the lower bound for the whole space. 
\vspace{2pt}
\noindent\textbf{Optimization Objective.} With applying complexity-aware sampling, we reformulate Eq. \ref{eq:sandwich_rule}  as the follows:
%At training step $t$ for the complexity level $C^{(t)}_j \in \{C_{i-1}^{(t-1)}, C_i^{(t-1)}, C_{i+1}^{(t-1)}\}$, the complexity-aware training is to rewrite Eq. \ref{eq:sandwich_rule} solve into the following optimization objective, 
\begin{small}
	\begin{equation}
		\begin{aligned}
			\underset {w} { \operatorname {arg\,min} } \, \left[\sum^M_{s_m^{(t)} \in \mathbf{U}} \mathcal{L}_{D} \left(f(w_{s_m^{(t)}})\right) + \sum_{s_n \in \hat{\mathbf{S}}} \sigma (s_n, C^{(t)}_j) \mathcal{L}_{D} \left( f(w_{s_n}) \right) \right], 
		\end{aligned}
	\end{equation} 
\end{small}
where $t$ denotes the current training step, 
 $\mathbf{U}$ is the stochastic subnet set containing $M$=3 uniform subnets, in which  each subnet has the FLOPs level of $C^{(t)}_j$ for step $t$: 
\begin{small}
\begin{equation}
	\mathbf{U} = \left\{s_m^{(t)} \vert s_m^{(t)} \sim \Gamma (\mathcal{A}) \: \& \: g'\left(s^{(t)}_m; C^{(t)}_j \right) == 0 \right\}_{m=1}^M, 
	\label{eq:subnetwork_set_sampling}
\end{equation}
\end{small}
and $\sigma(\cdot)$ selects the nearest smallest subnet from HSS: 
\begin{small}
\begin{equation}
	\sigma(s_n, C^{(t)}_j)= 
	\begin{cases}
		1&\mbox{if $s_n$ is the nearest min that is larger than $C^{(t)}_j$}\\
		0&\mbox{otherwise, }
	\end{cases}
	\label{eq:hss_equation}
\end{equation}
\end{small}
% \scriptsize
Note that we still use the notion $\Gamma$ since the prior distribution will be determined in \cref{sec:preference_rule}. 


\subsection{Performance-aware Sampling}
\label{sec:performance_aware_sampling}
% memory bank and path preference rule
In ~\cref{sec:analysis}, we observe that top-performing subnets can further alleviate the gradient conflict issue. Inspired by this, we introduce a performance-aware sampling  to sample subnets with potentially higher accuracy.

Specifically, for the $M=3$ same-sized subnets in $\mathbf{U}$ (as defined in Eq.~\ref{eq:subnetwork_set_sampling}), we aim to sample from a new distribution that favors good subnets, rather than a uniform distribution with random performance. To achieve this, we propose an exploration and exploitation policy that constructs the new distribution based on quality-aware memory bank and path preference rule. The quality-aware memory bank is used to exploit historical good subnets with a probability of $q$, while preference rule  explores subnets with wider width and shallower depth with a probability of $1-q$.

\vspace{2pt}
\noindent\textbf{Quality-aware Memory Bank.}
% \subsubsection{Quality-aware memory bank}
\label{sec:memory_bank}
%Eq. \ref{eq:sandwich_rule} is ``memoryless'' to the sampled subnets, which means that a good model may be influenced by a bad model during training and thus lose its superiority. 
As shown in Fig.~\ref{fig:alg}, \emph{memory bank} stores the historical up-to-date good subnets for each FLOPs level.  Specifically, the good subnets are identified through comparing cross-entropy loss on the current min-batch. Suppose  step $t$ sample the $j^{th}$ FLOPs level of $C_j$, 
 ${\rm B}_j$ denotes the good subnets  for FLOPs of $C_j$ at step $t$, 
the prior distribution $\Gamma$ can be as a mixture distribution of the dynamic memory bank and other unexplored subnets.
From this perspective,  the expectation of subnet $s^{(t)}_m$ is
\begin{gather}
	\mathbb{E}_{s_m^{(t)} \sim \Gamma (\mathcal{A})} = q \cdot U({\rm B}_j) + (1-q) \cdot U(\hat{\mathcal{A}}_{C_j}), \label{eq:quality_aware_sampling} \\ 
	\text{where \;} {\rm B}_j \cup \hat{\mathcal{A}}_{C_j} = \mathcal{A}_{C_j} \nonumber
	% \mathbb{E}_{s_m^{(t)} \sim \Gamma (\mathcal{A})} [s^{(t)}_i] = q \cdot P(B_i) + (1-q) \cdot U(C_i), B_i \cap C_i = \emptyset
	% s_t^i \sim q \cdot P(B_i) + (1-q) \cdot U(C_i), B_i \cap C_i = \emptyset
\end{gather}
where $\mathcal{A}_{C_j}$ denotes all the subnets with FLOPs level of $C_j$ in search space $\mathcal{A}$.   $U(\hat{\mathcal{A}}_{C_j})$ is the uniform distribution of unexplored subnets, which will be further regulated by the path preference rule in next section. 


At the early training, a relatively small value (i.e., 0.2) of $q$ is applied so that uniform sampling dominates the training to exploring promising subnets. 
As training proceeds, the memory banks are gradually filled, at which $q$ is also gradually increased to exploit these memorized good subnets. 

\emph{Memory bank replacement strategy.} 
We adopt the \emph{Worst-Performing} strategy to replace the subnet in the memory bank. When the current subnet outperforms the worst-performing subnet in the memory bank, the worst-performing subnet is replaced by the current subnet.

\vspace{2pt}
\noindent\textbf{Path Preference Rule.}
% \subsubsection{Path Preference Rule}
\label{sec:preference_rule}
In Eq.~\ref{eq:quality_aware_sampling}, when exploring unvisited subnets from $\hat{\mathcal{A}_{C_j}}$, we propose to sample ViT-preferred architectures that are more likely to achieve higher accuracy. Our approach is inspired by recent studies~\cite{park2022how, raghu2021vision}, which found that in pure ViT models, the later Transformer stages tend to prefer wider channels over more layers compared to CNN models. We empirically verify that this conclusion holds true in the hybrid CNN-Transformer NAS space (see Appendix), which motivates us to incorporate this preference into the super-network training to filter out inferior subnets.



Specifically, when a subnet $s^{(t)} = \{o_{width}^{(t)}, {o_{depth}^{(t)}\}}$ is sampled  (\emph{i.e.,} through uniform distribution of second term in Eq. \ref{eq:quality_aware_sampling}), we expect it to have \emph{wider widths and shallower depths in Transformer stages}, where $o_{width}^{(t)}$ and $o_{depth}^{(t)}$ are the width and depth dimension (\emph{i.e.,} the dimension of interests \footnote{For simplicity, we omit the remaining dimensions (e.g., kernel size, expansion ratio, etc.) since they are not the dimensions of interest.}), respectively. 
% Specifically, for a sampled model, more channels and less depth should be allocated for its Transformer stages. 
% However, these two independent dimensions cannot be directly compared for ``more'' and ``less'' quantitatively. 
% However, the two independent dimensions, width and depth, cannot be compared directly without a proper metric. 
%\vspace{-0.15cm}
%\begin{definition}[Path preference rule]
%	\textit{
		% A fibration is a mapping bwtween two topological spaces that has the homotopy lifting property for every space \(X\).
%		For a sampled subnet $s^{(t)} = \{o_{width}^{(t)}, {o_{depth}^{(t)}\}}, s^{(t)} \in \hat{\mathcal{A}}_{C_i}$ at step $t$. 
		% If $x_1 \ge x_2$, then train the subnet $s^{(t)}$, otherwise re-sample a new subnet through uniform distribution and repeat the steps above. 
%		If we can find a proper metric $\Phi$ that can quantify a subnet's preference in depth and width dimensions and meets $\Phi_a (o_{width}^{(t)}) \ge \Phi_b (o_{depth}^{(t)}$), then subnet $s^{(t)}$ is appropriate for training. 
%	}
%\end{definition}
%\vspace{-0.15cm}

However, it's non-trivial to determine whether a ViT is a deep and narrow model or a shallow and wide model. Our  approach involves quantifying a subnet's FLOPs distribution in terms of the depth and width dimensions by comparing it with an anchor model. This enables us to determine the subnet's preference in terms of depths and widths. Specifically, suppose the anchor model is $s^{ac} = \{o_{width}^{ac}, {o_{depth}^{ac}\}}$ and the sampled subnet is 
$s^{(t)} = \{o_{width}^{(t)}, {o_{depth}^{(t)}\}}$, our first step is to generate a new subnet $\overline{s^{(t)}} = \{{\bf o_{width}^{ac}}, o_{depth}^{(t)}\}$ by aligning all dimensions to anchor model except the depth dimension, and another new subnet $\widehat{s^{(t)}} = \{o_{width}^{(t)}, {\bf o_{depth}^{ac}}\}$ by aligning all dimensions to anchor model except widths.  Then, we compute the FLOPs differences between two new subnets and the anchor model:
\begin{gather}
	\Phi_a (o_{width}^{(t)}) = {\rm FLOPs}(\overline{s^{(t)}}) - {\rm FLOPs}(s^{ac}), \\
	\Phi_b (o_{depth}^{(t)}) = {\rm FLOPs}(\widehat{s^{(t)}}) - {\rm FLOPs}(s^{ac}). 
\end{gather}
 If $\Phi_a\ge\Phi_b$,   subnet $s^{(t)}$ is considered to have wider widths and shallower depths for transformer stages, which adheres to path preference rule, and we will train it. Otherwise, we resample a new subnet and repeat the above steps. 

The quality of the anchor model $s^{ac}$ can impact the validity of these comparisons. We conjecture that the memory bank captures such preferences and thus select the subnet with minimal loss in the memory bank as the anchor model.
 


%for a upcoming subnet $s^{(t)}$, we will execute the following steps: 
%\begin{enumerate}[0]
%	\item[1)] Swap the width and depth dimensions of $s^{(t)}$ and $s^{ac}$ respectively.  
%	Then, obtain two new subnets $\overline{s^{(t)}} = \{{\bf o_{width}^{ac}}, o_{depth}^{(t)}\}$ and $\widehat{s^{(t)}} = \{o_{width}^{(t)}, {\bf o_{depth}^{ac}}\}$. 
%	\item[2)] Compute the FLOPs differences between two new subnets and the anchor: 
%	\vspace{-0.15cm}
%	\begin{gather}
%		\Phi_a (o_{width}^{(t)}) = {\rm FLOPs}(\overline{s^{(t)}}) - {\rm FLOPs}(s^{ac}), \\
%		\Phi_b (o_{depth}^{(t)}) = {\rm FLOPs}(\widehat{s^{(t)}}) - {\rm FLOPs}(s^{ac}). 
%	\end{gather}
%	\item[3)] If $\Phi_a \ge \Phi_b$, subnet $s^{(t)}$ has wider widths and shallower depths for transformer stages and is trained. Otherwise, we re-sample a new subnet through uniform distribution and repeat the steps above. 
%\end{enumerate}

%The above three steps  aim to map the width and depth dimensions of $s^{(t)}$ to the anchor, allowing for a comparison of the relative FLOPs differences between the two new subnets. Apparently, the quality of the anchor model $s^{ac}$ is a critical factor in determining the validity of these comparisons. We conjecture that the memory bank captures such preferences and thus select the subnet with minimal loss in the memory bank as the anchor model.

% \subsection{Conflict-aware Rule} 
% The gradient conflict problem becomes more severe as the search space size grows. 
% However, the overhead of designing the search space is quite expensive, as it requires to fine-tune several dimensions in the search space (e.g., depth, width, MLP ratio, etc.) manually and we do not have such prior-knowledge for CNN-transformer hybrid space. 
% To solve this intractable issue, we propose the conflict-aware rule to avoid the space-redesign by leveraging the above key observations and analysis. 

% \textbf{Adjacent sampling} 
% According to observation\#1 that large complexity difference between two subnets is more likely to cause gradient conflict, the preoccupation is to relieve the conflict by eliminating the the difference between subnets as much as possible. 
% We first define a \emph{complexity distance function $g(s_x, s_y)=|\text{\rm{FLOPs}}(s_x)-\text{\rm{FLOPs}}(s_y)|$} to measure the model-wise difference between two sampled subnets $s_x$ and $s_y$. 
% For the upcoming sampled model at step $t$, whether it is trained or not should satisfy $g(s_t, s_{t-1}) \leq K$, where $K$ is the complexity threshold, otherwise a new model should be re-sampled. 
% Accordingly, it ensures the complexity of the model at step $t$ is adjacent to step $t-1$.  
% Therefore, we relieve the gradient conflict caused by excessive complexity differences between the two fully random-sampling subnets in original sandwich rule \cite{bignas,attentivenas}. 
% For simplicity, we use $K$ discrete complexity levels $C_i \in \{C_1, ..., C_K\}$ (e.g., \{100, 200, ..., 800\} M\rm{FLOPs}) to approximate the continuous distance function, where $C_i$ represents the $i$-th complexity level. 
\begin{figure}[t]
	\centering
	\includegraphics[width=1\columnwidth]{alg.png}	
	\vspace{-4.5ex}
	\caption{The overview of our proposed confict-aware supernet training. At step $t$, we first sample a target FLOPs that is close to that at step $t$-1, then we sample 3 subnets with same level of FLOPs and find a nearest min subnet to update supernet weights.}
	%\caption{(a): models exhibit significantly different latency on diverse mobile devices; (b): latency of current efficient ViTs  on Pixel4; hybrid models surpass pure ViTs in small flops regime.}
	\label{fig:alg}
\end{figure}
\subsection{Overall Supernet Training Process}
	\vspace{-1ex}
	Fig.~\ref{fig:alg} shows the supernet training process that uses both complexity-aware and performance-aware sampling techniques. At step $t$, we choose a FLOPs level $C^{(t)}_j$ from $\{C^{(t-1)}_{i-1}, C^{(t-1)}_{i}, C^{(t-1)}_{i+1}\}$ that is close to $C^{(t-1)}_i$  at step $t-1$. Then, we sample 1 smallest subnet and $M$=3 stochastic subnets for training. The smallest subnet is the nearest one from the HSS set with FLOPs closest to $C^{(t)}_j$. The $M$=3 subnets have FLOPs equal to $C^{(t)}_j$. They are either from the memory bank ${\rm B}_j$ with probability $q$ or based on the path preference rule with probability 1-$q$. We compute and accumulate the gradients for above 4 subnets. Then, we use the accumulated gradients to update supernet parameters. 
	
	
%Fig.~\ref{fig:alg} depicts the overall supernet training process, which incorporates both complexity-aware sampling and performance-aware sampling techniques. At step $t$, we select a FLOPs level $C^{(t)}_j$ from the set $\{C^{(t-1)}_{i-1}, C^{(t-1)}_{i}, C^{(t-1)}_{i+1}\}$ that is close to  $C^{(t-1)}_i$ at step $t$-1. Then, we sample 1 smallest subnet and $M$=3 stochastic subnest for training. Specifically, we sample a nearest smallest subnet from HSS set whose FLOPs is closest to $C^{(t)}_j$. We sample $M$=3 stochastic subnets that match the FLOPs level $C^{(t)}_j$. These subnets are selected either from the memory bank ${\rm B}_j$ with a probability of $q$ or through the path preference rule with a probability of 1-$q$. We compute and accumulate the gradients for above 4 subnets. After that, we use the accumulated gradients to update the parameters of the supernet. 
 

% We only adopt heavy regularization (e.g., weight-decay, dropout) for Transformer stages \cite{deit,levit}. 