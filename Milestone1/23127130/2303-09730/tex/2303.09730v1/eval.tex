\section{Evaluation}
	\vspace{-1ex}
\noindent\textbf{Setup}. We apply our proposed techniques to train our very large ViT supernet for 600 epochs. The complexity levels used in the training are set to \{100, 200, 300, 400, 500, 700, 900, 1200\} MFLOPs, which are suitable for mobile-regime ViTs. The other training setting and  hyper-parameters follows the existing best practices~\cite{nasvit,alphanet}. We list the detailed numbers in supplementary materials.

Then, we evaluate the effectiveness of our trained ViT supernet on four mobile phones with varying resource levels: weak (Pixel1), neutral (Pixel4, Xiaomi11), and strong (Pixel6). For each device, we set a range of latency constraints and use nn-Meter~\cite{nnmeter} to build a latency predictor for efficient search. We use the evolutionary search method in OFA~\cite{ofa} to search 5k subnets for each latency constraint. 





\subsection{Main Results on ImageNet}
	\vspace{-1ex}
\begin{table}[t]
%\small
	\fontsize{8}{8} \selectfont
\caption{{\sysname} performance on ImageNet-1K~\cite{imagenet} with comparison to state-of-the-art efficient CNN and ViT models. We group models based on the hardware they are suited  according to FLOPs.   $^*$: latency is measured on each group's corresponding hardware.    }
\vspace{-3ex}
%\setlength{\tabcolsep}{1.25mm}{
\label{tbl:overallresults}
\begin{tabular}
	{@{\hskip1pt}ccc@{\hskip2pt}c@{\hskip2pt}c@{\hskip1pt}}
\toprule
\multicolumn{5}{c}{\textbf{Tiny models:$<$100  MFLOPs for weak Pixel1 phone}}                                                                   \\ \hline
Model                        & MFLOPs        & Top-1 Acc                  &Latency$^*$       & Type                \\ \hline
ShuffleNet-V2 x0.5 \cite{ma2018shufflenet}            & 41            & 60.3              &        8.7 ms         & CNN                 \\
\textbf{ElasticViT-T0}                & \textbf{37}            & \textbf{61.1}                    &  \textbf{8.5 ms}        & ViT NAS          \\
MnasNet-x0.35 \cite{tan2019mnasnet}               & 63            & 64.1              &   13.5 ms             & CNN NAS             \\
%MobileNetV3Small&56&67.4&11.0 ms &CNN NAS\\
MobileFormer~\cite{mobileformer}                 & 52            & 68.7               &       176.6 ms         & ViT              \\
\textbf{ElasticViT-T1}               & 62           &       67.2            &   \textbf{13.1 ms}        & ViT NAS          \\ \midrule
\multicolumn{5}{c}{\textbf{Tiny models: 100$\sim$150 MFLOPs for weak Pixel1 phone}  }                                                      \\ \hline
ShuffleNet-V2 1$\times$ \cite{ma2018shufflenet} &146&69.4& 24.3 ms &CNN\\
Cream~\cite{Cream}                        & 114           & 72.8                    &       24.3 ms    & CNN NAS             \\
FBNet-v2 \cite{wan2020fbnetv2}                      & 126           & 73.2                   &     22.5 ms       & CNN NAS             \\
MobileNet-V3 x0.75 \cite{mobilenetv3}            & 155           & 73.3              &     26.4 ms            & CNN NAS             \\
MobileFormer~\cite{mobileformer}                   & 96          & 72.8                    &        216.3 ms   & ViT              \\
\textbf{ElasticViT-T2}                & \textbf{119}          & \textbf{73.8}          &        \textbf{22.4 ms}       & ViT NAS          \\ 
\textbf{ElasticViT-T3}                & 160         & \textbf{75.2}                &     \textbf{29.2 ms}    & ViT NAS          \\ 
\midrule
\multicolumn{5}{c}{\textbf{Small models: 200$\sim$350 MFLOPs for neutral  Pixel4 phone}}                                                        \\ \hline
MobileNet-v3 x1.0 \cite{mobilenetv3}            & 219           & 75.2          &       24.2 ms              & CNN NAS             \\
FBNet-v2 \cite{wan2020fbnetv2}                    & 238           & 76.0              &           22.1 ms      & CNN NAS             \\
% OFA                           & 230           & 76.0                               & CNN NAS             \\
OFA \#25 \cite{ofa}                    & 230           & 76.4                  &      25.2 ms       & CNN NAS             \\
% OFA \#75                     & 230           & 76.9                               & CNN NAS             \\
BigNAS-S \cite{bignas}                      & 242           & 76.5                   &      32.6 ms      & CNN NAS             \\
%RegNAS                       & 298           & 77.1                               & CNN NAS             \\
MobileFormer~\cite{mobileformer}                 & 214           & 76.7               &       415.1 ms         & ViT              \\
HR-NAS-A~\cite{ding2021hr}                   & 267           & 76.6                       &      -  & ViT NAS             \\
\textbf{ElasticViT-S1}       & \textbf{218} & \textbf{77.2}           &    \textbf{21.0 ms}       & ViT NAS \\
GreedyNAS-v2~\cite{greedynas}                 & 324           & 77.5               &          65.4 ms     & CNN NAS             \\

LeViT-128S~\cite{levit}                   & 305           & 76.6                 &       30.5 ms       & ViT             \\

HR-NAS-B~\cite{ding2021hr}                   & 325           & 77.3                  &    -         & ViT NAS             \\

\textbf{ElasticViT-S2} & \textbf{318} & \textbf{78.6}            &     \textbf{29.6 ms}    & \textbf{ViT NAS} \\ 
\midrule
\multicolumn{5}{c}{\textbf{Medium models: 350$\sim$500 MFLOPs for neutral Pixel4 phone}}                                                         \\ \hline

EfficientNet-B0 \cite{tan2019efficientnet}            & 390           & 77.1                &          55.1 ms     & CNN NAS             \\
%FP-NAS                       & 399           & 78.0                      &         & CNN NAS             \\
BigNAS-M \cite{bignas}                  & 418           & 78.9                    &     64.3 ms      & CNN NAS             \\
MobileViT-XXS~\cite{mobilevit}& 364& 69.0 &84.1 ms&ViT\\
% LeViT-128s  &  305 & 76.6 & Hybrid\\
LeViT-128~\cite{levit}                    & 406           & 78.6                  &          40.2 ms   & ViT              \\

MobileViTv3-0.5~\cite{mobilevitv3}& 481& 72.3&96.7 ms&ViT\\
%BossNAS-M2                      & 403           & 77.4                               & Hybrid NAS             \\
\textbf{ElasticViT-M} & \textbf{415} & \textbf{79.1}             &   \textbf{37.4 ms}     & ViT NAS \\ 
\midrule
\multicolumn{5}{c}{\textbf{Large models: $\geq$ 500 MFLOPs for  strong Pixel6 phone}}                                                         \\ \hline
MAGIC-AT~\cite{xu2022analyzing}                        & 598           & 76.8                &           37.9 ms    & CNN NAS             \\
BigNAS-L \cite{bignas}                  & 586           & 79.5                      &     45.7 ms    & CNN NAS             \\
MobileViTv2-0.5~\cite{mobilevitv2}                  & 500           & 70.2               &      56.5 ms          & ViT              \\
%MobileFormer                 & 508           & 79.3                     &          & ViT            \\
UniNet-B0~\cite{uninet}                      & 560           & 79.1                     &    53.9 ms      & ViT NAS              \\
\textbf{ElasticViT-L1} & \textbf{516} & \textbf{79.4}            &     \textbf{33.1 ms}    & ViT NAS \\

%OFA \#75                     & 595           & 80.0                               & CNN NAS             \\
EfficientNet-B1 \cite{tan2019efficientnet}        & 700           & 79.1                    & 49.9 ms         & CNN NAS             \\
EdgeViT-XXS~\cite{edgevit}                     & 600           & 60.0                   &   69.6 ms         & ViT            \\
%LeViT-192~\cite{levit}                    & 658           & 80.0                        &      35.1 ms & ViT            \\
MobileViT-XS~\cite{mobilevit}&986&74.8&84.4 ms&ViT\\
Autoformer-Tiny~\cite{autoformer} & 1300 & 74.7&71.1 ms& ViT NAS\\
ViTAS-Twins-T~\cite{su2022vitas}& 1400&79.4& -&ViT NAS\\
\textbf{ElasticViT-L2} & \textbf{704} & \textbf{79.8}                 & \textbf{43.8 ms}   & ViT NAS \\ 
\textbf{ElasticViT-L3} & \textbf{806} & \textbf{80.0}                 &  \textbf{50.5 ms}  & ViT NAS \\ 
 \hline

\end{tabular}
%}
\end{table}

% \begin{table*}[t]
% 	\begin{center}
% 		%	\small
% 		\fontsize{8.8}{8.8} \selectfont
% 		\caption{Comparison with SOTA models.}
% 		\label{tbl:end-to-end}
% 		\begin{tabular}
% 		{ccccc}
% 			\midrule
% 			Group & Model & FLOPs& Accuracy& Type \\
% 			\midrule 
			
% 			$<$200M& ShuffleNetV2 0.5$\times$&41M&60.3&\\
% 			$<$200M&Mobile-Former-52M&52M&68.7&\\
% 			$<$200M& MobileNetV3Small&56M&67.4&\\
% 				$<$200M&Mobile-Former-96M&96M&72.8&\\
% 			$<$200M& ShuffleNetV2 1$\times$&146M&69.4&\\
% 				$<$200M&Mobile-Former&151M&75.2&\\
				
% 				% $<$200M&Mobile-Former-151M&151M&75.2&\\
% 				$<$200M&\textbf{Ours-A0}&197M&76.0&\\
% 			\midrule
		
% 			200-300&MobileViTv3-XXS& 289M & 70.98& \\
% 			200-300&MobileNetV3-Large& 219M & 75.2& \\
% 			200-300&Mobile-Former&214M& 76.7&\\
% 			200-300&OFA (\#25)& 230M & 76.4& \\
% 			200-300&BigNAS-S   & 242M & 76.5& \\
% % 			200-300&AttentiveNAS-A0& 203M & 77.3& \\
% % 			200-300&NASViT-A0& 208M & 78.2&\\
% % 			200-300&AttentiveNAS-A1& 279M & 78.4& \\
% 			200-300&\textbf{Ours-A1}& 288M& 77.2&\\
% 				\midrule
% 			300-400&MobileNetV2& 300& 72&\\
% 			300-400&MobileViTv1-XXS& 364& 69&\\
% 			300-400&EfficientNetB0& 390& 77.3&\\
% 			300-400&NASViT-A1&309& 79.7&\\
% 			300-400&LeViT-128S & 305& 76.6&\\
% 			300-400&AttentiveNAS-A2& 317 & 78.8& \\
% 			300-400&FBNetV3-A& 357& 79.1&\\
% 			300-400&\textbf{Ours-A2}& 379& 78.2&\\
% 				\midrule
% 			400-500&MobileViTv3-0.5& 481& 72.33&\\
			
% 			400-500&LeViT-128& 406& 78.6&\\
% 			400-500&BigNAS-M& 418& 78.9&\\
% 			400-500&NASViT-A2& 421& 80.5&\\
% 			400-500&BN-NAS&470&75.7&80.5&\\
% 			400-500&\textbf{Ours-A3}& 474& 78.7&\\
% 				\midrule
% % 			500-600& NASViT-A3& 528& 81&\\
% % 			500-600&NASViT-A4& 591& 81.4&\\
% % 			500-600&FBNetV3-C& 557& 80.5&\\
%             500-600&MobileFormer& 508 &79.3& Hybrid\\
% 			500-600&MAGIC& 598 &76.8& CNN-NAS\\
% 				\midrule
% 			600-1000&MobileViTv3-XS& 927& 76.7&\\
% 			600-1000& NASViT-A5& 757& 81.8&\\
% 			600-1000&LeViT-192& 658& 80.0&\\
% 				600-1000&FBNetV3-E & 762& 81.3& \\
% 					\midrule
				
			
			
			
			
			
% 			\midrule
% 		\end{tabular}
		
% 	\end{center}
% 	%\vspace{-1ex}
% \end{table*}

\noindent\textbf{Comparison with efficient CNN and ViT models.} Table~\ref{tbl:overallresults} reports the comparison with state-of-the-art models including both strong CNNs and recent efficient ViTs.  Remarkably, for the first time, we utilize two-stage NAS to deploy lightweight and low-latency ViT models ranging from 37 to 800 MFLOPs, enabling us to bring accurate ViTs to a wide range of mobile devices. Without retraining or finetuning, our discovered subnets {\sysname} models significantly outperform all evaluated ViT and  CNN baselines.

First, \textit{our models significantly outperform prior ViTs that are designed for mobile devices.} 
 For tiny ViTs, our {\sysname}-T3 achieves 75.2\% accuracy under only 160 MFLOPs, which is 2.9\% better than MobileNetV3x0.75 in terms of similar FLOPs. For medium-sized ViTs, {\sysname}-M achieves 79.1\% accuracy under 415 MFLOPs, significantly outperforming existing mobile ViTs with 0.5\% and 4.8\% higher accuracy than LeViT and MobileViTv3~\cite{mobilevitv3}, respectively.  For large ViT models where ImageNet classification accuracy saturates, {\sysname}-L3 still has 0.6\% and 5.3\% accuracy improvement compared with Autoformer~\cite{autoformer} and ViTAS~\cite{su2022vitas} with 1.7$\times$ and 1.6$\times$ fewer FLOPs. Furthermore, our ViT models not only achieve high accuracy but also demonstrate fast real inference latency, making them practical for deployment on resource-constrained mobile phones. This sets them apart from other mobile ViT approaches that solely focus on reducing FLOPs. For instance, with only 214 MFLOP, MobileFormer~\cite{mobileformer} has a slow latency of 415.1 ms on Pixel4, which is 17.2$\times$ slower than MobileNetV3 and 19.7$\times$ slower than our {\sysname}-S1.%, which showcases the effectiveness of our approach in achieving both efficiency and speed.
 
 %Furthermore, our ViT models also  achieve fast real inference latency, demonstrating their practicality on resource-constrained mobile phones. This is in contrast to some mobile ViT approaches that only focus on reducing FLOPs. For instance, with only 214 MFLOPs, MobileFormer~\cite{mobileformer} runs a slow latency of 415.1 ms on Pixel4, which is 17.2$\times$ slower than MobileNetV3, and 19.7$\times$ slower than our {\sysname}-S1.
 
 

Second, \textit{our models also surpass lightweight CNNs with higher accuracy and lower latency.} For instance, {\sysname}-T0 achieves 61.1\% accuracy with only 37 MFLOPs, which outperforms ShuffleNetv2x0.5 with 0.8\% higher accuracy. {\sysname}-T3 achieves
the same accuracy of 75.2\% as MobileNetV3 with only 160 MFLOPs, while be 1.2$\times$ faster. This is the first time that ViT outperforms CNN with a faster speed on mobile devices within the 200 MFLOPs range.





\begin{figure*}[t]
	\centering
	\includegraphics[width=1\columnwidth]{mobile_latency.png}	
	\vspace{-4ex}
	\caption{Under the same latency constraint, the discovered ViTs by {\sysname} surpass state-of-the-art mobile CNNs on diverse mobile devices. From left to right: old and weak devices to latest and strong devices. }
	%\caption{(a): models exhibit significantly different latency on diverse mobile devices; (b): latency of current efficient ViTs  on Pixel4; hybrid models surpass pure ViTs in small flops regime.}
	\label{fig:mobilelatency}
\end{figure*}
%Moreover, our discovered ViT models show fast inference latency on real-world mobile devices.


%Please note that we are the very first work to achieve $\leq$ 50MFLOPs ViT inference without any bell and whistles operators.


%\textbf{{\sysname} under different constraints}. Pixel 4 latency, flops. Compare with NASViT, and AttentiveNAS. 

\noindent\textbf{Deploying efficient ViTs for diverse mobile phones}. Now we apply our ViT supernet to get different specialized subnets for diverse mobile devices. As a baseline for comparison, we choose OFA~\cite{ofa}, which represents the state-of-the-art hardware-aware NAS for delivering efficient mobile-regime CNNs. 
 Instead of relying solely on FLOPs to measure on-device efficiency, we use real inference latency for comparison. % since a ViT with a small number of FLOPs can still have slow latency due to hardware-unfriendly operators like the MobileFormer ( in Table~\ref{tbl:overallresults}). 
To ensure a fair comparison, we adopt the same approach as OFA and build a latency predictor for each of our test devices. We conduct an evolutionary search to find the optimal CNN subnets for each device. Note that we didn't compare with ViT NAS baselines, as existing works focus on large ViTs, whose supernets are out of range for searching low-latency models for mobile devices. 
 
Fig.~\ref{fig:mobilelatency} presents the latency-accuracy curve of our ViTs compared to OFA on diverse mobile devices. Our discovered ViT models consistently outperform OFA under various latency constraints, on both weak and strong mobile devices. Notably, our models are the first ViT models to achieve real-time inference latency on mobile devices without compromising accuracy. These results demonstrate the effectiveness of our approach and highlight the potential of transformers for efficient, high-performance models on mobile devices.


\subsection{Ablation Study}

%\begin{table*}[t]
%	\begin{center}
		%\small
%		\fontsize{8}{8} \selectfont

%		\begin{tabular}	
%			{cccccccccccc}
%			\hline
%			\multirow{2}{*}{Method} & \multicolumn{11}{c}{MFLOPs}\\
%		   &min&min2&min3& 100& 200 & 300 & 400& 500& 600 & 700 &800\\
%		   \hline
%		   \sysname &61.2&75.2 & 78.2& 73.8&77.2 &78.6 &79.1&79.4& 79.6&79.9 &80.0\\
		 % ${}-$ Prefer rule &60.9 &74.9& 77.97&73.2 &76.9 &78.4&78.9 &79.3&79.3&79.6\\
%		  ${}-$ performance-aware sampling& 61.2&75.0& 78.0&72.8 &76.7 &78.4 &79.0&79.3&79.4&79.6&79.7\\
%		    ${}-$ Prefer rule${}-$ Pool ${}-$ multi min&66.7 &73.1&75.6 &73.2 &75.6 &76.7 &77.5&78.2&78.3&78.7&\\
%		   \hline 
%		   Sandwich rule+AlphaNet &64.7& 71.5&74.4 &69.9 & 74.2& 76.5& 77.4&77.8&78.3&78.7&79.0 \\
%			\hline
%		\end{tabular}   
		% \vspace{-2.5ex}
%		\caption{Ablation study}
%		\label{tbl:ablation_study}
%	\end{center}
%\end{table*}
We now conduct ablation studies to evaluate 1)   how each of our techniques can improve supernet training; 2) how our techniques mitigate the gradient conflict issues; and  3) the performance of  our searched model compared to retraining.


\begin{table}[t]
	\begin{center}
		%\small
		\fontsize{8.5}{8.5} \selectfont
		\vspace{-3ex}
		\caption{Ablation study results on ImageNet. We show the top1 accuracy of best searched models for each case. Note that \textit{Multiple min} is applied on top of \textit{Adjacent sampling}; \textit{Perf-aware sampling} is applied on top of both \textit{Adjacent sampling} and \textit{Multiple min}.}
		\label{tbl:ablation_study}
		\begin{tabular}	
			{@{\hskip1pt}c@{\hskip3pt}c@{\hskip5pt}c@{\hskip5pt}c@{\hskip5pt}c@{\hskip5pt}c@{\hskip5pt}c@{\hskip5pt}c@{\hskip5pt}c@{\hskip1pt}}
			\midrule
			\multirow{2}{*}{Method} & \multicolumn{8}{c}{MFLOPs}\\ &100& 200 & 300 & 400& 500& 600 & 700 &800\\
			\midrule
			Baseline (Sandwich rule) &69.9 & 74.2& 76.5& 77.4&77.8&78.3&78.7&79.0 \\
			\midrule
			Adjacent step sampling &73.2 &75.6 &76.7 &77.5&78.2&78.3&78.7&79.0\\
			+Multiple min (HSS)&72.8 &76.7 &78.4 &79.0&79.3&79.4&79.6&79.7\\
			++Perf-aware sampling & \textbf{73.8}&\textbf{77.2} &\textbf{78.6} &\textbf{79.1}&\textbf{79.4}& \textbf{79.6}&\textbf{79.8} &\textbf{80.0}\\
			\midrule
		\end{tabular}   
		% \vspace{-2.5ex}
	\end{center}
\end{table}

\vspace{2pt}
\noindent\textbf{Ablation study on each technique.} Table~\ref{tbl:ablation_study} shows the accuracy of the best-searched models under different supernet training techniques. We start with the baseline supernet that is trained using the original sandwich rule. Then we apply our techniques one by one to enhance the supernet training. We keep all the other training settings and search process consistent for a fair comparison. 

Table~\ref{tbl:ablation_study} illustrates the effectiveness of our proposed techniques in enhancing supernet training over a vast ViT search space. Both adjacent step sampling and multiple min strategy effectively controls the FLOPs differences among trained subnets,  resulting in substantial top-1 accuracy gains of up to 3.3\%. Furthermore, by further using performance-aware sampling to train good subnets, we are able to further improve the best-searched ViTs by up to 1\% accuracy.



\vspace{2pt}
\noindent\textbf{The effectiveness of mitigating gradient conflicts.} Our accuracy improvements primarily stem from the effective mitigation of gradient conflicts. To validate this, we conduct experiments on two supernets trained with different approaches: one using our proposed techniques and one using the sandwich rule as a baseline. We freeze the weights for both supernets and study the gradients of different subnets under the same batch of training data. We randomly sample 50 subnets under three levels of FLOPs: 50M, 200M, and 600M. We compute the cosine similarity of shared weights' gradient between each pair of subnets under the same FLOPs. A higher cosine similarity indicates less gradient conflict.

Table~\ref{tbl:ablation_study2} shows the average gradient similarity between subnets on both supernets. Compared to vanilla sandwich rule, we can significantly improve the gradient similarity for both random  and good subnets, suggesting that our method can efficiently mitigate the gradient conflicts.



\begin{table}[t]
	\begin{center}
		\small
		\fontsize{8.5}{8.5} \selectfont
		\caption{Gradient cosine similarity between subnets in supernet trained with different methods. The "good" subnets refer to the top10 subnets chosen from the randomly sampled 50 subnets.}
		\vspace{-3ex}
		\begin{tabular}	%{@{\hskip1pt}c|@{\hskip4pt}c@{\hskip4pt}c@{\hskip4pt}c@{\hskip4pt}c@{\hskip4pt}c@{\hskip4pt}c@{\hskip1pt}}
			{@{\hskip2pt}c@{\hskip2pt}|c@{\hskip5pt}c@{\hskip5pt}|c@{\hskip5pt}c@{\hskip5pt}|c@{\hskip5pt}c@{\hskip1pt}}
			\hline
			\multirow{3}{*}{Method}&\multicolumn{6}{c}{MFLOPs}\\
			\cline{2-7}
			&\multicolumn{2}{c|}{50}&\multicolumn{2}{c|}{200}&\multicolumn{2}{c}{600}\\
			&Random&Good&Random&Good&Random&Good\\
			\hline
			Sandwich rule &0.37 & 0.47&0.32 &0.41 &0.31 & 0.46\\
			\textbf{Ours} &\textbf{0.50} & \textbf{0.56}& \textbf{0.51}&\textbf{0.67} &\textbf{0.51} &\textbf{0.67} \\
			\hline
		\end{tabular}   
		% \vspace{-2.5ex}
		\label{tbl:ablation_study2}
	\end{center}
\end{table}

\noindent\textbf{Comparison with training from scratch}. High-quality supernet training can ensure that subnets achieve comparable accuracy as those trained from scratch. We retrain each subnet with a batch size of 512 on 8 Nvidia V100 GPUs, following the same training settings as LeViT~\cite{levit}. 
 Table~\ref{tbl:ablation_study1} compares the accuracy on ImageNet. These selected subnets can achieve even higher accuracy by directly inheriting weights from our supernet, with up to 2\% improvement. Interestingly, we notice that larger ViT models achieve comparable accuracy to retraining, while  tiny and small ViTs ($<$500 MFLOPs) can benefit more from supernet training.
 
 

\begin{table}[t]
	\begin{center}
		\small
		\fontsize{8.5}{8.5} \selectfont
			\caption{Best-searched ViT top-1 accuracy of inheriting supernet's weights vs.  retraining from scratch.}
			\vspace{-3ex}
		\begin{tabular}%	{ccccccccccc}
		%	{@{\hskip1pt}c@{\hskip4pt}c@{\hskip4pt}c@{\hskip4pt}c@{\hskip4pt}c@{\hskip4pt}c@{\hskip4pt}c@{\hskip4pt}c@{\hskip4pt}c@{\hskip4pt}c@{\hskip4pt}c@{\hskip4pt}c@{\hskip1pt}}
	%		\hline
	%		\multirow{2}{*}{Method} & \multicolumn{10}{c}{MFLOPs}\\
	%		&min& min2&min3&100& 200 & 300 & 400& 500& 600 & 700 \\
%			\hline
%			\sysname && & & & & & & &&\\
%		 Train from scratch &60.0& 74.5&76.1&73.4 &75.3 & 76.6& 77.7& &79.5&79.9& \\
		 {@{\hskip1pt}c@{\hskip7pt}c@{\hskip7pt}c@{\hskip7pt}c@{\hskip7pt}c@{\hskip7pt}c@{\hskip7pt}c@{\hskip7pt}c@{\hskip7pt}c@{\hskip7pt}c@{\hskip1pt}}
		 \hline		 \multirow{2}{*}{Method} & \multicolumn{8}{c}{MFLOPs}\\
		 &100& 200 & 300 & 400& 500& 600 & 700 & 800\\
		 \hline
		 Train from scratch &73.4 &75.3 & 76.6& 77.7& 79.0&79.5&\textbf{79.9}& 80.0\\
		  \textbf{\sysname}  &\textbf{73.8}& \textbf{77.2}&\textbf{78.6} &\textbf{79.1} &\textbf{79.4} &\textbf{79.6} &79.8&80.0\\
			\hline
		\end{tabular}   
		% \vspace{-2.5ex}
		\label{tbl:ablation_study1}
	\end{center}
\end{table}

\vspace{-1ex}
\subsection{Transfer Learning}
\vspace{-1ex}
We transfer {\sysname} to  a list of commonly 
used transfer learning datasets: 1) CIFAR10 and CIFAR100~\cite{cifar10}; 2) fine-grained classification: Food-101~\cite{food101}, Oxford Flowers~\cite{flower} and Pets~\cite{pets}. We take the pretrained checkpoints on ImageNet and fine-tune on new datasets. We closely follow the hyper-parameter settings in GPipe~\cite{gpipe}.  The results are summarized in 
Table~\ref{tbl:finegrainedcls}. Compared to existing efficient CNNs and ViTs, our {\sysname} models achieves significantly better accuracy with fast inference speed on Pixel 4.



\begin{table}[t]
	\begin{center}
		%\small
		\fontsize{8.2}{8.2} \selectfont
		\caption{Transfer learning results on downstream image classification datasets. We measure the latency on Pixel 4. }
		\vspace{-2.5ex}
		\begin{tabular}	
			{@{\hskip1pt}c@{\hskip4pt}c@{\hskip4pt}c@{\hskip4pt}c@{\hskip3pt}c@{\hskip3pt}c@{\hskip4pt}c@{\hskip4pt}c@{\hskip1pt}}
			\hline
		{Model}  &Latency& CIFAR10&CIFAR100&Food-101&Flowers&Pets\\
			%MobileNetV2 & 300&&&&&&\\
			\hline
			MobileNetV3&24.2 ms&97.1&83.3&86.8&94.3&87.7\\
		%	{\sysname}-E0&100&&97.0&84.8&84.3&93.0&90.6\\
			\textbf{{\sysname}-S1}&\textbf{21.0 ms}&\textbf{97.5}&\textbf{86.1}&\textbf{87.2}&\textbf{94.3}&\textbf{92.1}\\
			\hline
			LeViT-128S&30.5 ms&96.8&85.0&73.6&86.2&90.1\\
			\textbf{{\sysname}-S2}&\textbf{29.6 ms}&\textbf{97.5}&\textbf{86.9}&\textbf{88.3}&\textbf{95.2}&\textbf{92.9}\\
			\hline
			EfficientNet-B0&55.1 ms&97.9&86.9&89.1&92.4&92.2 \\
			LeViT-128&40.2 ms&97.8&86.6&80.8&86.2&92.2\\
			\textbf{{\sysname}-M}&\textbf{37.5 ms}&\textbf{97.9}&\textbf{87.0}&88.8&\textbf{95.6}&\textbf{93.3}\\
			\hline
		\end{tabular}   
		% \vspace{-2.5ex
		\label{tbl:finegrainedcls}
	\end{center}
\end{table}


%\begin{table}[t]
%	\begin{center}
		%\small
	%	\fontsize{8}{8} \selectfont
		
%		\begin{tabular}	
%			{@{\hskip2pt}c@{\hskip2pt}|c@{\hskip5pt}c@{\hskip5pt}cc@{\hskip5pt}c@{\hskip5pt}c@{\hskip5pt}|c@{\hskip3pt}c@{\hskip2pt}}
%			\hline
%			\multirow{2}{*}{Model} & \multicolumn{6}{c|}{RetinaNet 1$\times$} & FLOPs&Latency\\
%			\cline{2-7}
%			& AP & AP$_{50}$& AP$_{75}$& AP$_{s}$& AP$_{m}$& AP$_{l}$&(M) & (ms)\\
%			MobileNetV3 & 29.9& 49.3& 30.8& 14.9& 33.3& 41.1& 217&\\
%			{\sysname}-E1&& & & & &&200&\\
%			EfficientNet-B0& 35.9& 55.1&38.3 &19.3 &39.2 &49.8&390&\\
%			LeViT-128S& & & & & &&305&\\
%			LeViT-128& & & & & &&406&\\
	%		{\sysname}-E2& & & & & &&300&\\
%			{\sysname}-E2& & & & & &&406&\\
%			\hline
%		\end{tabular}   
		% \vspace{-2.5ex}
%		\caption{Transfer learning performance on COCO
%			val2017 with comparisons to other mobile-regime models.}
%		\label{tbl:coco}
%	\end{center}
%\end{table}


%\begin{table*}[t]
%	\begin{center}
		%	\small
%		\fontsize{8.8}{8.8} \selectfont
%		\caption{Transfer learning results(\%) on fine-grained classification tasks.}
%		\label{tbl:transfer-learning-fg-cls}
%		\begin{tabular}
%		{cccccccccc}
%			\midrule
  %              Model & MFLOPs & CIFAR-10 & CIFAR-100 & Flowers & Pets & Food-101 & Cars & FGVC aircraft \\
  %              \midrule 
 %               AttentiveNAS-A0 & 203 & 97.71 & 86.23 & 95.20 & 92.10 & 87.62 & 90.36 & %83.83 \\
%                AlphaNet-A0 & 203 & 97.80 & 86.52 & 95.43 & 92.83 & 87.52 & 90.06 & 82.99 \\
 %               AttentiveNAS-A1 & 279 & 97.75 & 86.21 & 95.45 & 93.00 & 88.63 & 91.01 & 85.54 \\
%                AlphaNet-A1 & 279 & 97.58 & 86.61 & 95.54 & 93.40 & 88.62 & 91.16 & 85.06 \\
   %             AttentiveNAS-A2 & 317 & 97.88 & 86.20 & 95.71 & 92.80 & 88.75 & 91.16 & 86.14 \\
 %               AlphaNet-A2 & 317 & 97.70 & 86.58 & 95.50 & 93.08 & 88.66 & 91.43 & 85.09 \\
  %              AttentiveNAS-A3 & 357 & 97.78 & 86.61 & 95.64 & 93.32 & 88.76 & 91.63 & 86.41 \\
  %              AlphaNet-A3 & 357 & 98.00 & 87.24 & 95.53 & 93.59 & 88.67 & 91.87 & 86.11 \\
   %             EfficientNet-B0 & 390 & 97.97 & 86.94 & 95.17 & 92.40 & 89.09 & 92.23 & 86.35 \\
    %            AttentiveNAS-A4 & 444 & 98.00 & 86.95 & 95.72 & 93.27 & 89.09 & 92.21 & 87.73 \\
    %            AlphaNet-A4 & 444 & 97.87 & 86.85 & 95.90 & 93.65 & 89.17 & 92.17 & 87.34 \\
   %             AttentiveNAS-A5 & 491 & 98.01 & 87.16 & 95.95 & 93.24 & 89.56 & 92.49 & 88.27 \\
   %             AlphaNet-A5 (small) & 491 & 98.07 & 87.10 & 96.08 & 93.95 & 89.39 & 92.17 & 87.79 \\
   %             AlphaNet-A5 (base) & 596 & 97.84 & 86.98 & 96.10 & 93.92 & 90.25 & 92.56 & 88.51 \\
  %              AttentiveNAS-A6 & 709 & 97.80 & 86.94 & 95.64 & 93.79 & 89.60 & 92.86 & 88.03 \\
   %             AlphaNet-A6 & 709 & 98.00 & 87.35 & 96.21 & 94.09 & 89.85 & 92.85 & 88.21 \\
  %          \midrule
%		\end{tabular}
%	\end{center}
	%\vspace{-1ex}
%\end{table*}





