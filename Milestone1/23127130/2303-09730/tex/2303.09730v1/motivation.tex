\section{Search Space Design and Training Analysis}
As shown in many studies~\cite{autoformerv2,nse,attentivenas}, a well-designed search space is extremely important for neural architecture search, because it determines both the accuracy and efficiency bounds. Unlike CNN search spaces have been well-studied on optimizing the latency for mobile devices~\cite{ofa,proxylessnas,hurricane}, little attention is paid on designing a latency-friendly ViT search space for both weak and strong mobile devices. In this section, we first design a very large and latency-friendly search space to cover diverse mobile devices, then we discuss the challenges of training a very large search space and analyze the root causes of its poor performance.



%As shown in many studies~\cite{autoformerv2,nse}, a well-defined search space is extremely important for neural architecture search, because it determines both the accuracy and efficiency bounds. Unfortunately, unlike CNN search spaces have been well-studied for optimizing the latency on mobile devices~\cite{ofa,proxylessnas,hurricane}, little attention is paid on designing a mobile-friendly ViT search space. In this section, we rethink the search space design and discuss the practical challenges. 


%Inspired by recent works~\cite{} demonstrate that hybrid CNN-ViT architectures can achieve competitive accuracy under a small FLOPs , we consider to design a search space based on hybrid CNN-ViT architecture.

%\subsection{ViT Latency Study on Mobile Devices}


%\textbf{Diverse mobile devices}. Since latency plays a hard constraint in many real-world mobile applications, we conduct a comprehensive study to investigate the latency performance of state-of-the-art ViT models on mobile devices.  We first study how the diverse mobile devices impact model latency. As shown in Fig.~\ref{fig:modellatency}(a), the real-world mobile devices exhibit a huge diversity. For instance, the inference latency of NASViT-a0 on a powerful Pixel 6 is 1.5$\times$ and 3.0$\times$ faster than Pixel 4 and Pixel 1, respectively. As a result, the optimal model under a given latency constraint varies significantly on diverse mobile devices. Moreover, we observe that the weak phones are extremely resource hungry, and require tiny models with $<$200M FLOPs for real-time inference. 

%\textbf{Hybrid CNN-ViTs are preferred}. We collect a set of efficient neural architectures including CNN, ViT and hybrid models, and benchmark their inference latency. Fig.~\ref{fig:modellatency}(b) summarizes the FLOPs and latency on a Pixel 4 phone. We can observe that hybrid CNN-ViTs outperform the pure ViTs in small FLOPs regime. However, the existing smallest hybrid model still cannot serve on weak devices, which further motivates our work.



\subsection{Efficient Search Space Design}

Common search spaces in NAS works~\cite{bignas,ofa} can be decoupled as: (1) macro-architecture that defines the choices of depths, widths, kernel sizes per layer;  and (2) building block choices for each layer. In supernet-based NAS, each layer is fixed to a specific building block (e.g., MobileNetV3 block) to utilize weight entanglement sharing technique. Based on this, we first design a latency-friendly building block for ViT search space, then we define the marco-architecture to cover the huge diversity. 




\subsubsection{Efficient Building Blocks}
\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\columnwidth]{figs/attention.png}	
	\vspace{-1.5ex}
	\caption{Mobile-friendly transformer block in our search space. Left: multi-head self-attention; for downsampling layer, we add a MobileNetv3 block with kernel size of 3$\times$3 before LayerNorm;  Right: MLP layer. During the supernet training, the input channel number $C$, $H$, $W$, V scale ratio $k$ and MLP ratio $e$ are elastic. }
	\label{fig:transformer}
\end{figure}

\begin{table}[t]
%	\begin{center}
		\centering
	%	\small
		\fontsize{8.5}{8.5} \selectfont
		\begin{tabular}	{@{\hskip5pt}c@{\hskip5pt}|c@{\hskip5pt}c|c@{\hskip5pt}c@{\hskip5pt}}
	%	{c|ccc|ccc}
			\hline
			\multirow{3}{*}{Channel}& \multicolumn{2}{c|}{Pixel 6} & \multicolumn{2}{c}{Pixel 4}\\
		& NASViT & Our  Attention& NASViT & Our Attention\\
			&(ms)&(ms) $\downarrow$ &(ms)&(ms) $\downarrow$ \\
		\hline
		 64&3.66&\textbf{1.75} (-52.2\%)&5.89&\textbf{2.89} (-50.9\%)\\
		 96& 6.52 &\textbf{2.98} (-54.3\%)&10.76&\textbf{5.18} (-51.8\%)\\
		 112& 8.14 &\textbf{3.58} (-56.1\%)& 13.42 &\textbf{6.22} (-53.7\%)\\
			\hline
		\end{tabular}   
		\vspace{-1.5ex}
		\caption{Latency comparison of two Attention structures.  Our optimized attention can achieves $>2\times$ latency reduction than NASViT attention. $HW$=14$\times$14, V scale=4, QK dimension=8.}
		\label{tbl:attentioncompare}
%	\end{center}
\end{table}

%\begin{table}[t]
%	\begin{center}
		%\small
%		\fontsize{8}{8} \selectfont
%		\begin{tabular}		{ccc|ccc}
%			\hline
%			\multirow{2}{*}{Model}&	\multirow{2}{*}{FLOPs} &\multirow{2}{*}{FLOPs}  &\multicolumn{3}{c}{Layer-level latency breakdown}\\
%			& & & Conv Stage& Attention& MLP  \\
	%		\hline 
%			NASViT-a0&208M&36.1 ms & 43\% & 44.9\%&7.7\% \\
%			NASViT-a1&309M&58.2 ms & 37.3\%&52.6\%&7\%\\
%			\hline
%		\end{tabular}   
%		\caption{NASViT model latency and layer-level latency breakdown on Pixel 4 phone.}
	%	\label{tbl:attentioncompare}
%	\end{center}

%\end{table}

\begin{table}[t]
%	\begin{center}
	%	\small
	\centering
		\fontsize{8}{8} \selectfont
		\begin{tabular}		{@{\hskip5pt}c@{\hskip5pt}c@{\hskip5pt}c@{\hskip5pt}c@{\hskip5pt}c@{\hskip5pt}c@{\hskip5pt}}
			\hline
			Stage&Depths&\makecell{Channels}&\makecell{Kernel size \\(V scale)}& \makecell{Expansion\\ ratio} & Stride\\
			\hline 
			Conv 3$\times$3 & 1 & (16, 24, 8)& 3 & -&2\\
			MBv2 block& 1-2& (16, 24, 8) & 3, 5& 1 &  1\\
			MBv2 block& 2-5 & (16, 32, 8) & 3, 5& 3, 4, 5, 6&2\\
			MBv3 block & 2-6&(16, 48, 8) & 3, 5& 3, 4, 5, 6&2\\
			Transformer & 1-5&(48, 96, 16)&2, 3, 4&2, 3, 4, 5&2\\
			Transformer & 1-6&(80, 160, 16)&2, 3, 4&2, 3, 4, 5&2\\
			Transformer & 1-6&(144, 288, 16)&2, 3, 4&2, 3, 4, 5&1\\
			Transformer & 1-6&(160, 320, 16)&2, 3, 4&2, 3, 4, 5&2\\
			MBPool& -&1984&-&6&-\\
			\hline 
			Input resolution & \multicolumn{5}{c}{128, 160, 176, 192, 224, 256}\\
				\hline
		\end{tabular}   
		\caption{Our search space that covers both weak and strong mobile devices. Tuples of three values represent the lowest value, highest, and steps: since we set attention head size to 16, the channel numbers in ViT stages use 16 as the step size.}
		\label{tbl:searchspace}
%	\end{center}
\end{table}

%Motivated by Fig.~\ref{fig:modellatency}(b), we consider  hybrid CNN-ViT based search space. Our strategy is to apply existing CNN search space design wisdoms on the CNN stage, and then redesign  both efficient and effective architectures for ViT stages.

As shown in Table~\ref{tbl:searchspace}, our search space is comprised of a STEM conv layer, 3 CNN stages in the shallow layers, 4 transformer stages in the deep layers and classification layer. For CNN stage, we adopt the effective MobileNetv2 (abbr. MBv2) and MobileNetv3 (abbr. MBv3) building blocks, which have been widely used in mobile-regime CNN search spaces~\cite{ofa,proxylessnas,attentivenas}. For efficiency, we replace Swish to Hswish activation for MBv3 block. 



% the first 3 shallow stages are CNN blocks and the deep 4 stages are transformer blocks. 
%Following the hybrid strategy in LeViT and NASViT, we adopt CNN blocks in the shallow layers and transformer blocks in the deep stages. 


For ViT stage, we redesign the attention and optimize the MLP layers for efficient inference on mobile devices. 

\noindent\textbf{Efficient Self-Attention Mechanism.} 
Attention serves as the key component in transformers. To improve attention effectiveness, a line of work~\cite{mobilevitv2,edgevit,swin,nextvit,mobileformer} propose several new structures. 
Swin~\cite{swin} introduced the shifted window attention to reduce the computation complexity with respect to the number of tokens. 
However, the efficiency advances of window attention becomes marginally when the input $H$ and $W$ are in small size (i.e., short input tokens). 
Unfortunately, the input $H$ and $W$ are downsampled to a small size of 14$\times$14 for transformer stages in a typical hybrid model. 

\tc{
On the other hand, LeViT~\cite{levit}, MobileFormer \cite{mobileformer} and NASViT~\cite{nasvit} give several insights to design hybrid models. 
Specifically, LeViT proposed to use a larger value matrix (V) dimension than the query (Q) and key (K) matrix. 
To reduce the latency, LeViT also replace normalization layers and GeLU activation function to batch normalization and Hswish, respectively. 
To futher enhance the learning capacity, NASViT incorporates two talking head~\cite{shazeer2020talking} layers and depth-wise convolution layer in the self-attention module. 
To capture the local and global information simultaneously, MobileFormer proposes a parallel CNN-Trnasoformer structure and exchange these information through a bidirectional bridge. 
% The resulted NASViT outperforms LeViT with smaller FLOPs and higher accuracy. 
As shown in Tab. \ref{tbl:baseline_latency}, while these neoteric mechanisms achieve promising performance-FLOPs trade-off, they fall into the low efficiency for real on-device inference as the mobile-unfriendly talking head, activation function and bidirectional bridge. 
}

\begin{table}[h]
\small
\caption{TODO}
\begin{tabular}{c|ccc}
\hline
Model        & FLOPs (M) & Latency (ms) & Top-1 Acc. (\%) \\ \hline
LeViT-128    & 406       & 25.5         & 78.6            \\
MobileFormer & 214       & 142.8        & 76.7            \\
NASViT       & 208       & 23.6         & 78.2            \\ \hline
\label{tbl:baseline_latency}
\end{tabular}
\end{table}
% However, some new attention structure is , thus causes 

% However, our on-device profiling suggests that NASViT attention structure is inefficient in terms of latency, due to the memory-expensive talking head and swish activation. Surprisingly, $\sim$ 30\% of the model latency are caused by
% talking head and swish. Moreover, talking head becomes more expensive as the number of Atterntion heads increase. 

To develop an attention layer that is not only mobile-friendly (latency efficient) but also effective, we redesign the attention layer based on LeViT and NASViT attentions. Fig.~\ref{fig:transformer} illustrates the architecture of our proposed mobile-friendly attention blocks. We remove the latency-expensive talking head, and increase the default QKV dimension to 16. We use latency-friendly Hswish as the activation function. Table~\ref{tbl:attentioncompare} compares the latency on real-world mobile phones. Our optimized attention can accelerate the inference latency by $>2\times$, which greatly improves the transformer block efficiency. Moreover, the optimized attention achieves more latency reduction under  large channel setting, which will be proposed as a key feature for hybrid model in later section. 


\vspace{3px}
\noindent\textbf{MLP Layer} is built by replacing the standard Gelu activation in a 2-layer MLP by a cheap Hswish activation.  In LeViT and NASViT, the expansion factor is reduced from 4 to 2 for latency reduction. Since ViT models have been observed with little redudancy in MLP layers~\cite{vitstudy}, we allow MLP to use larger expansion ratio of \{3, 4, 5\}. 


%\vspace{3px}
%\noindent\textbf{Conv Stage}. For CNN stage, we directly apply the effective MobileNetv2 (abbr. MBv2) and MobileNetv3 (abbr. MBv3) building blocks, which have been widely used in mobile-regime CNN search spaces~\cite{ofa,proxylessnas,attentivenas}. For efficiency, we replace Swish to Hswish activation for MBv3 block. 





\subsubsection{Macro-architectures for Diverse Mobile Devices}
\begin{figure}[t]
	\centering
	\includegraphics[width=1\columnwidth]{figs/data/preference/preference.png}	
	\caption{Boxplot visualization of  best-searched 50 models from NASViT search space. The y-axis lists out the available choices for the corresponding search dimension. For small FLOPs regime, the top models choose the minimal depth of 3 in Stage 3, 4 and 5; top models choose the maximum width of 184 in Stage 5. }
	\label{fig:preference}
\end{figure}

Once the building blocks are fixed, the design of macro-architecture determines both the performance and efficiency bounds. In recent years, many works~\cite{attentivenas,proxylessnas} observe that mobile-regime CNNs prefer deeper depths and narrower channels, and manually set small channel numbers but large depths in the search space. NASViT~\cite{nasvit} also applies this CNN heuristic and inherit the macro-architecture of AttentiveNAS~\cite{attentivenas} search space.  However, a recent ViT study~\cite{park2022how} suggests that \textit{Conv and ViT exihibit opposite behaviours, and using larger channel numbers in late stages can improve the accuracy}.

To verify whether this design rule is validated in hybrid model, we compare the macro-architectures of best models in NASViT and reveal the following insight:

\noindent\textbf{Insight\#1}: \textit{ViT stage prefers wider widths and shallower depths.} Fig.~\ref{fig:preference} visualizes the  depths and channels of top 50 models. Specifically, we perform evolutionary search to explore 5k models and select top 50 with the highest accuracy for analysis. Surprisingly, even allowed to set a deeper depths, our results suggest that top models at 300M FLOPs choose the minimal depth choice of 3 in the 3 ViT stages. We also observe an interesting phenomenon in Stage 5. Both 300M and 500M top models choose the largest width choice in Stage 5. These observations suggest that the existing search space design for ViT stages are suboptimal. 

 Consequently, new search space that is optimized by ViT preferred characteristics is required, however, the design is challenging for diverse mobile devices. In the past, CNN search space design requires domain expertise about existing models, which are the results of years. Given different mobile devices which are equipped with diverse computation and memory resources, we need to carefully customize the widths and depths settings to meet both the deployment scenarios and ViT prefered characteristics. However, designing customized macro-architectures for every scenario is computationally expensive and laborious. 
 
 In our work, insteading of customizing search space for diverse mobile devices, we design a very large search sapce covering both tiny and large ViTs. Table~\ref{tbl:searchspace} summarizes the detailed search space. In particular, we enlarge the maximum choice of widths  and add smaller depths choices for ViT stages. To support weak devices, we also add many small choices for each search dimensions. As a result, our search space contains a huge number of 4.1$\times$10$^{18}$ candidate models, which is 10$^7$$\times$ of NASViT and AttentiveNAS.

\lz{Dilemma: customize  vs a very large search space}
 
\subsection{Analysis of Training a Huge Supernet}


\begin{figure*}[th]
	\centering
	\includegraphics[width=0.9\textwidth]{figs/data/gradient/gradient.png}	
	\caption{(a) Model accuracy achieved by training from scratch vs. inheriting supernet weights; (b) Gradient cosine similarity of models with different FLOPs; (c) Under the same FLOPs budget, good models share more similar gradients than random sampled models.}
	\label{fig:gradient}
\end{figure*}

%Supernet-based NAS assembles all candidate architectures into a weight sharing network (supernet) with each architecture corresponding to one subnet. 
Compared to the standard single network training,  a major difference of supernet training is that all the subnets share weights for their common parts in each layer. The shared weights may receive different gradients that result in different optimization directions, which causes  final accuracy drops. Many techniques are proposed to alleviate this issue,  including the popular sandwich rule sampling with inplace distillation~\cite{bignas}, $\alpha$-divergence-based KD~\cite{alphanet},  gradients projection and less data augmentation~\cite{nasvit}. Specifically, sandwich sampling rule is widely applied to  guarantee performance lower and upper bounds. 
By these techniques, we can train a high-quality supernet over a normal size search space.  Subnets can directly inherit  weights from  supernet for  deployment with comparable accuracy to those retrained from scratch, which eliminates the huge cost of training or fine-tuning of each subnets invidually.
 
 
%At each step, a sandwich sampling rule is applied with inplace distillation to train 4 subnets (max, 2 random and min). After supernet training finishes, different subnets can directly inherit the weights from the supernet for evaluation and deployment with comparable accuracy to those retrained from scratch, which eliminates the huge cost of training or fine-tuning. 


%In supernet-based NAS, a supernet assembles all candidate architectures into a weight sharing network with each architecture corresponding to one subnet. At each step, a sandwich sampling rule is applied with inplace distillation to train 4 subnets (max, 2 random and min). After supernet training finishes, different subnets can directly inherit the weights from the supernet for evaluation and deployment with comparable accuracy to those retrained from scratch, which eliminates the huge cost of training or fine-tuning. 

Though promising, we observe a significant accuracy degradation when training a very large supernet over our search space, which is $10^7\times$ larger than a normal size search space. We closely follow the previous best practices in NASViT for 360 epochs supernet training on ImageNet and randomly sample 20 subnets with different FLOPs for evaluation. We compare the accuracy achieved by inheriting weights from supernet to those retrained from scratch. 
Fig.~\ref{fig:gradient}(a) shows the results. Compared to retraining, models from supernet drop a significant $\sim$8\% accuracy. 

\noindent\textbf{Analysis.}  Compared the a normal size supernet (e.g., NASViT), our proposed supernet contains a large amount of tiny subnets ($<100$M FLOPs). Therefore, we start by studying the gradient changes shared among subnets with different sizes. We randomly sample subnets under the FLOPs of 50M, 100M, 200M, 300M, 600M, 900M and 1200M from our supernet, and compute the cosine similarity of shared weights' gradient between each other under the same batch of training data.  A lower similarity indicates larger differences in gradients and thus more difficult to well train the supernet.   As shown in  Fig.~\ref{fig:gradient}(b), we surprisingly observe that \textbf{ 1) gradient similarity of shared weights between two subnets is negatively correlated with the FLOPs difference between them.} Subnets with same-level FLOPs achieve the highest gradient similarity, while the similarity is close to 0 if there is a big FLOPs difference between two subnets.

%Compared to the standard single network training, a major difference of supernet training is that all the subnets share weights for their common parts in each layer. During supernet training, the shared weights may receive different gradients that result in different optimization directions, which causes gradient conflict and final  accuracy drops. This phenomenon is also noticed by ~\cite{nasvit,xu2022analyzing}. NASViT proposes three techniques to mitigate the gradient conflict issue, but it still cannot train a high-quality supernet over a very large search space.

%To understand the root cause of the poor performance, we first study the gradient changes shared among subnets with different sizes, as our propose supernet contains a large amount of tiny subnets ($<100$M FLOPs) comparing to a normal size supernet (e.g., NASViT). We randomly sample subnets under the FLOPs of 50M, 100M, 200M, 300M, 600M, 900M and 1200M, and compute the cosine similarity of shared weights' gradient between each other under the same batch of training data.  A lower similarity indicates larger differences in gradients and thus more difficult to well train the supernet.   As shown in  Fig.~\ref{fig:gradient}(b), we surprisingly observe that \textbf{ 1) gradient similarity of shared weights between two subnets is negatively correlated with the FLOPs difference between them. } Subnets with similar FLOPs achieve the highest gradient similarity, while the similarities are close to zero if there are a big FLOPs difference between two subnets.

Besides FLOPs difference, subnet quality may also affect gradient similarity of shared weights. If a weak subnet is sampled and gets trained, it would disturb the weights of good subnets. To verify this hypothesis, we randomly sample 50 subnets with same level FLOPs and compute the shared weights gradient cosine similarity betwen top subnets and randomly-sampled subnets. Fig.~\ref{fig:gradient} suggests that \textbf{2) gradient similarity between same-size subnets can be greatly improved  if they are good subnets.}

We now rethink how effective the sandwich sampling rule is in training a very large supernet. When applying sandwich rule, each training step trains a max, min and 2 random subnets, which are with different FLOPs. The differences can be more severe in a large search space that covers both tiny and large ViTs. Moreover, the quality of 2 random subnets has no guarantee in sandwich rule. 







%\subsection{vision transformers on diverse mobile CPU}
%levit: transformers are faster than CNN on high-performance hardware. Most hardware accelerators (GPUs, TPUs) are optimized to perform large matrix multiplication. In transformers, attention and MLP blocks rely
%mainly on these operations. Convolutions, in contrast, require complex data access patterns, so their operation is often IO-bound. These considerations are important for our
%exploration of the speed/accuracy tradeoff.







