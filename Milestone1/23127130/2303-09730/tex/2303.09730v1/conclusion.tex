	\vspace{-1ex}
\section{Conclusion}
	\vspace{-1ex}
In this paper, we propose {\sysname}, a two-stage NAS approach that trains a high-quality supernet for deploying accurate and low-latency vision transformers on diverse mobile devices. Our approach introduces two key techniques to address the gradient conflicts issue by constraining FLOPs differences among sampled subnets and sampling potentially good subnets, greatly improving supernet training quality.  Our discovered ViT models outperfom prior-art efficient CNNs and ViTs on the ImageNet dataset, establishing new SOTA accuracy under various of latency constraints. 