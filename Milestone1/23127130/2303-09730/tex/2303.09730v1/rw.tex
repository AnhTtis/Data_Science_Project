\vspace{-3ex}
\section{Related works}
\vspace{-1ex}
\noindent\textbf{Efficient Vision transformers}. %Transformers are initially proposed in the natural lanaguage processing tasks~\cite{transformer,bert}, and is notable for its use of self-attention to model long-range dependencies in the data. Today, ViT~\cite{vit} and its follow-ups~\cite{deit,chu2021conditional,han2021transformer,swin,t2t,wang2021pyramid} surpasses the current state-of-the-art CNNs on multiple vision tasks. However, the success of ViTs come at the cost of expensive computation resources, making them impractical for real-world applications on resource-constrained mobile devices.
 %To accelerate ViT, many approaches have been introduced with different methodologies, such as inventing new architectures or modules~\cite{chen2021crossvit,hassani2021escaping,li2022sepvit}, optimizing attention operation~\cite{mobilevitv2,edgevit,swin,nextvit} and introducing hybrid architectures of CNN and transformer~\cite{wu2021cvt,cmt,mobilevit,mobilevitv3,levit,mobileformer,efficientformer}. Specifically, hybrid architectures of CNN and ViT typically allow smaller models to perform well. To futher enhance the learning capacity, NASViT~\cite{nasvit} incorporates two talking head~\cite{shazeer2020talking} layers and depth-wise convolution layer in the self-attention module.   MobileFormer~\cite{mobileformer} proposes a parallel CNN-Transformer structure and exchange these information through a bidirectional bridge. However, these architectures fail to achieve high inference speed on real-world mobile devices because of the memory-expensive operations such as talking head, activation function and bidirectional bridge. 
 Many methods have been proposed to design efficient ViTs. They use different ways, such as new architectures or modules~\cite{chen2021crossvit,hassani2021escaping,li2022sepvit}, better attention operation~\cite{mobilevitv2,edgevit,swin,nextvit}  and hybrid CNN-transformer~\cite{wu2021cvt,cmt,mobilevit,mobilevitv3,levit,mobileformer,efficientformer}. Hybrid models usually perform well with small sizes by introducing special operations. For instance,  MobileFormer~\cite{mobileformer} uses a parallel CNN-transformer structure with bidirectional bridge. 
 However, although the FLOPs are reduced, these ViTs still have high latency because of  mobile-unfriendly operations such as the  bidirectional bridge.
  
%  Instead of developing complex transformer operations, we design a large search space that is optimized for mobile devices and can automatically search  effective  ViTs under low resource contraints.
   
 
 %allow smaller models to perform well. LeViT~\cite{levit} is the first to achieve 76.6\% top-1 ImageNet accuracy with only 305M FLOPs. The recently proposed NASViT~\cite{nasvit}  achieves 78.2\% accuracy with 200M FLOPs. Nevertheless, the majority of vision transformers are heavy-weight, little efforts has been made to design small models especically tiny ViTs for efficient inference on weak mobile devices.
 
 
 
% Though these models can achieve competitive performance to mobile-regime CNNs, the majority are heavy-weight, and the design of  tiny ViTs for weak mobile devices is less explored. 
 

\noindent\textbf{Neural Architecture Search}. NAS has
achieved an amazing success in automating the design of efficient CNN architectures~\cite{pham2018efficient,guo2020single,proxylessnas,bignas,ofa}. Recently, several works apply NAS to find improved ViTs, such as Autoformer~\cite{autoformer}, S3~\cite{autoformerv2}, ViTAS~\cite{su2021vitas} and ViT-ResNAS~\cite{liao2021searching}. These methods focus on searching models exceeding 1G FLOPs. For small ViTs, HR-NAS~\cite{ding2021hr}, UniNet~\cite{uninet} and NASViT~\cite{nasvit} search for hybrid CNN-ViTs and achieve promising results under small FLOPs. However, these NAS works mainly optimize for FLOPs without considering the efficiency on diverse mobile devices, which leads to suboptimal performance.



%However, these approaches are not designed for diverse mobile devices that have varying levels of resources and require both small and large ViTs for deployment. As a result, the searched architectures are sub-optimal

%However, there has been little attention paid to using NAS to design efficient ViTs for diverse mobile devices that have varying levels of resources and require both small and large ViTs for deployment. To the best of our knowledge, our work is the first NAS approach that can deliver superior ViTs with a range of 37 to 800 MFLOPs.

%NASViT~\cite{nasvit} adds talking head layers~\cite{shazeer2020talking} and depth-wise convolution layer in self-attention. 

%These methods focus on searching models exceeding 1G FLOPs. For small ViTs, HR-NAS~\cite{ding2021hrnas}, UniNet~\cite{uninet} and NASViT~\cite{nasvit} search for hybrid CNN-ViTs and achieve promising results under small FLOPs. However, there has been little attention paid to using NAS to design efficient ViTs for diverse mobile devices that have varying levels of resources and require both small and large ViTs for deployment. To the best of our knowledge, our work is the first NAS approach that can deliver superior ViTs with a range of 37 to 800 MFLOPs.
\noindent\textbf{Supernet Training}. Early NAS methods~\cite{zoph2016neural,zoph2018learning,real2019regularized,real2017large} are very costly because they need to train and evaluate many architectures from scratch. More recent one-shot NAS methods~\cite{guo2020single,proxylessnas,liu2018darts,chu2021fairnas} use weight-sharing to save time. But they still need to retrain the best architecture from scratch for higher accuracy, which is expensive when targeting multiple constraints.
To solve this problem, two-stage NAS, such as OFA~\cite{ofa}, BigNAS~\cite{bignas} and AutoFormer~\cite{autoformer} decouples the training and search. They train a supernet where the good subnets can be directly deployed without retraining. However, they employ uniform sampling to sample subnets,  which can lead to subnets with significantly different sizes being sampled in a much larger search space, resulting in gradient conflicts and inferior performance.
 %which can easily sample subnets with vastly different sizes in a much larger search space, leading to gradient conflicts 
 %One key technique is to use the sandwich rule to sample a min, max and two random subnets per iteration. However, we find that sandwich rule causes gradient conflicts between subnets in a large ViT supernet, resulting in inferior performance. 
Our work proposes conflict-aware supernet training to address this issue.



%Early NAS approaches can be prohibitively expensive as hundreds of candidate architectures need to be trained from scratch and evaluated~\cite{zoph2016neural,zoph2018learning,real2019regularized,real2017large}. More recent one-shot NAS approaches have adopted weight-sharing to improve the search efficiency. Each architecture (subnet) uses only a subset of the operations in the supernet; and these subnets can be  ranked by using the shared weights to estimate their relative accuracies~\cite{guo2020single,proxylessnas,liu2018darts,chu2021fairnas}. Despite the widespread use, a number of studies~\cite{yu2019evaluating,laube2021exploring, xie2021weight,zhang2020does} have noticed that the shared weights can cause interference and fail to reflect the true ranking of subnets. To alleviate this issue, multiple works have been proposed~\cite{pourchot2020share,NEURIPS2021_65d90fc6,xu2022analyzing,hu2020angle,9525258}. However, they all require a post process of retraining from scratch for the best searched architecture to obtain a final model with higher accuracy, which are expensive when targeting multiple deployment constraints.

%To alleviate these issues, OFA~\cite{ofa}, BigNAS~\cite{bignas} and slimmable networks~\cite{yu2019universally,frankle2018lottery} make a significant progress to train a high-quality single-stage supernet where all subnets reach good performance as those retrained from scratch; then one can apply typical search algorithms to find subnets under various deployment requirements. One key technique is to use the sandwich rule to sample 4 subnets (min, max and two random), where small subnets are nested in large subnets. However,  the effectiveness of sandwich rule  is rarely analyzed. In our work, we found that the sandwich rule can cause gradient conflicts between subnets in a very large supernet and proposed conflict-aware training to address this issue.



%\textbf{NAS}. NAS has shown its effectiveness in automating CNN architecture design. Early NAS works use either reinforcement learning~\cite{zoph2016neural,zoph2018learning} or evolutionary algorithms~\cite{real2019regularized,real2017large}, which were prohibitively expensive on large-scale datasets. More NAS efforts have made the search more efficient through weight-sharing~\cite{pham2018efficient,guo2020single} and gradient-based search~\cite{liu2018darts,wu2019fbnet}.  This helps alleviate the expensive cost of training all candidate architectures from scratch and the search process is significantly accelerated.  However, such methods require two-stage training: once the best architectures have been identified, they have to be retrained from scratch to obtain high-quality accuracy for model deployment. Recently, OFA~\cite{ofa} and BigNAS~\cite{bignas} train a once-for-all supernet without retraining candidate sub-networks. BigNAS adopts sandwich rule and inplace distillation, which are simple and efficient, and has been widely applied in many follow-ups~\cite{attentivenas,alphanet,nasvit}. 
 
%For vision transformers, there are few works applying NAS to discover better architectures~\cite{su2021vitas,li2021bossnas,autoformer,nasvit,liao2021searching}. Autoformer~\cite{autoformer}, S3~\cite{autoformerv2} and ViTAS~\cite{su2021vitas} search pure transformers for large-FLOPs regime with more than 1G FLOPs. For smaller ViT models, NASViT~\cite{nasvit} is the first work that searches a family of ViT ranging from 200M to 800M FLOPs over a hybrid CNN-ViT search space. However, none of them can search both tiny to large ViTs. Through our proposed \lz{xxx}, {\sysname} is able to well train a much larger supernet than the existing works, delivering adaptive-sized ViTs ranging from 15M to 800M FLOPs. 

%\textbf{Supernet Training}. Weight-sharing between sub-networks in a supernet is a cricual factor to NAS success, as it greatly improve search efficiency. Prior efficient NAS approaches widely adopt the single-path strategy~\cite{guo2020single} to train the supernet. In each training step, the sampled sub-network reuses the latest weights on the supernet, which is updated by previous sampled sub-networks. Despite the widespread use of single-path strategy, a number of studies~\cite{yu2019evaluating,laube2021exploring, xie2021weight,zhang2020does} have noticed that the resulting supernet weights might fail to reflect the true ranking of sub-networks, limiting NAS to find better architectures. To alleviate this issue, multiple works have been proposd~\cite{zhang2020does,pourchot2020share,NEURIPS2021_65d90fc6,xu2022analyzing,hu2020angle,9525258}. 

%Recent works, OFA~\cite{ofa}, BigNAS~\cite{bignas} and slimmable networks~\cite{yu2019universally,frankle2018lottery} make a significant progress to improve the weight-sharing NAS effectiveness by weight entanglement sharing strategy. The weight entanglement strategy enforces different candidate blocks in the same layer to share as many weights as possible.  Unlike the single-path strategy, the effect of weight entanglement to NAS is rarely studied. NASViT~\cite{nasvit} noticed that there existing gradient conflicts in hybrid CNN-ViT supernet. In our work, we further reason the confict phenomenon and provide a simple but effective strategy to alleviate the issue. \lz{to improve}



