\begin{abstract}
%Designing  lightweight vision transformers (ViT) that meets varying latency requirements across different mobile devices is a crucial yet challenging task. Our work aims to leverage two-stage neural architecture search (NAS) to first train a high-quality supernet and then search an optimal sub-network (subnet) for direct deployment without the need for retraining. However, supporting both weak and powerful mobile devices simultaneously requires a huge size of search space comprising tiny to large ViT subnets. Prior approaches that use the Sandwich rule to sample a minimum, maximum, and two uniform subnets can result in a low-quality supernet. This is due to the issue of \textit{gradient conflict}: the sampled subnets often have significant differences in model size (e.g., 50M vs. 1G FLOPs), leading to different optimization directions and inferior convergence. 

%Designing  lightweight vision transformers (ViT) that meets varying latency constraints across different mobile devices is a critical and challenging task. Our work aims to leverage two-stage neural architecture search (NAS) to search an sub-network (subnet)  from a well-trained supernet for direct deployment. Our work aims to leverage two-stage neural architecture search (NAS) that  trains a high-quality supernet and then search an optimal sub-network (subnet) for direct deployment.  To support a wide range of mobile devices, we design a very large search space that covers tiny to large ViT subnets.  However, prior approaches that use the Sandwich rule to sample a minimum, maximum, and two uniform subnets can result in a low-quality supernet. This is due to the gradient conflict issue: the sampled subnets often have vastly model size differences (e.g., 50M vs. 1G FLOPs), leading to different optimization directions and inferior convergence. 
 
% In this work, we propose {\sysname} to train a high-quality supernet that supports low-latency ViTs for diverse mobile devices. To address the issue of gradient conflict, we introduce two key techniques. First, we propose complexity-aware sampling to limit the size of the sampled subnets to be close to the previous step. Second, we propose performance-aware sampling to select good subnets that can further reduce gradient conflicts. Our discovered models, {\sysname} models, achieve top-1 accuracy from 67.2\% to 80.0\% on ImageNet from 60M to 800M FLOPs without extra retraining, outperforming all prior CNNs and ViTs in terms of accuracy and mobile efficiency. Our tiny and small models are also the first ViT models that surpass state-of-the-art CNNs under real-time latency constraint on mobile devices.
Neural Architecture Search (NAS) has shown promising performance in the automatic design of vision transformers (ViT) exceeding 1G FLOPs. However, designing lightweight and low-latency ViT models for diverse mobile devices remains a big challenge. In this work, we propose {\sysname}, a two-stage NAS approach that trains a high-quality ViT supernet over a very large search space that supports a wide range of mobile devices, and then searches an optimal sub-network (subnet) for direct deployment. However, prior supernet training methods that rely on uniform sampling suffer from the gradient conflict issue: the sampled subnets can have vastly different model sizes (e.g., 50M vs. 2G FLOPs), leading to different optimization directions and inferior performance. To address this challenge, we propose two novel sampling techniques: complexity-aware sampling and performance-aware sampling. Complexity-aware sampling limits the FLOPs difference among the subnets sampled across adjacent training steps, while covering different-sized subnets in the search space. Performance-aware sampling further selects subnets that have good accuracy, which can reduce gradient conflicts and improve supernet quality. Our discovered models, {\sysname} models, achieve top-1 accuracy from 67.2\% to 80.0\% on ImageNet from 60M to 800M FLOPs without extra retraining, outperforming all prior CNNs and ViTs in terms of accuracy and latency. Our tiny and small models are also the first ViT models that surpass state-of-the-art CNNs with significantly lower latency on mobile devices. For instance, {\sysname}-S1 runs 2.62$\times$ faster than EfficientNet-B0 with 0.1\% higher accuracy. 


 
 

\end{abstract}