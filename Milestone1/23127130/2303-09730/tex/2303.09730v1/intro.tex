\vspace{-3ex}
\section{Introduction}
\vspace{-1ex}
Vision Transformers (ViTs) have achieved remarkable success in various computer vision tasks~\cite{vit,swin,deit,detr,deformabledetr}.
%have emerged as a popular  architecture choice in computer vision due to their superior performance in various tasks~\cite{vit,swin,deit,detr,deformabledetr}. 
However, the success comes at a significant cost - ViTs are heavy-weight and have high inference latency costs, posing a great challenge to bring ViTs to  resource-limited mobile devices~\cite{vitstudy}. Designing accurate and low-latency ViTs  becomes an important but challenging problem. 

 %Consequently, convolutional neural networks (CNNs) remain the dominant choice in real-world mobile applications~\cite{appstudy}, as they have significantly lower latency than ViTs while achieving comparable accuracy levels~\cite{vitstudy}.
 
 
 %However, this improved performance comes at the cost of large model size and expensive inference cost, making them extremely difficult for the deployment on resource-constrained mobile devices. 
 %Consequently, convolutional neural networks (CNNs) remain the dominant choice in real-world mobile applications~\cite{appstudy}, as they have significantly lower latency than ViTs while achieving comparable accuracy levels~\cite{vitstudy}.
 
 

 
% When  constrained to  small or medium FLOPs ranges, ViTs are inferior to  convolutional neural networks (CNNs), making CNNs the dominant choice in mobile applications~\cite{appstudy}. As a result, CNNs remain the dominate choice in real-world  mobile applications~\cite{appstudy}, since ViTs have a much higher latency compared to mobile-regime CNNs when acheving same level accuracy~\cite{vitstudy}.
%the high inference latency of ViTs are unacceptable~\cite{vitstudy}. % A recent study~\cite{vitstudy} suggests that ViTs run 3.4$\times$ - 40$\times$ slower than MobileNetV2~\cite{mobilenetv2} on a Pixel 4 phone, making them impractical for real-world applications on resource-constrained mobile devices.

%To design more efficient ViTs, recent research has focused on  optimizing the self-attention operation~\cite{edgevit,swin,nextvit} or designing hybrid CNN-ViT models~\cite{cmt,mobilevit,mobilevitv3,levit,mobileformer,efficientformer}. 
%There has been several works~\cite{edgevit,swin,nextvit,cmt,mobilevit,mobilevitv3,levit,mobileformer,efficientformer}  on developing efficient ViTs, but they offen require a post-process of  significant human effort to customize models for  specific deployment requirements. 
%Although many works~\cite{edgevit,swin,nextvit,cmt,mobilevit,mobilevitv3,levit,mobileformer,efficientformer} are proposed to design efficient ViTs, doing this by hand  takes substantial human efforts and computation resources  for various deployment requirements.
Neural Architecture Search (NAS) provides a powerful tool for automating efficient DNN design. Recently, two-stage NAS such as BigNAS~\cite{bignas} and AutoFormer~\cite{autoformer}, decouple training and searching process and achieves remarkable search efficiency and accuracy. The first stage  trains a weight-shared supernet assembling all candidate architectures in the search space, and the second stage uses typical search algorithms to find best sub-networks (subnets) under various resource constraints. The searched subnets can directly inherit supernet weights for deployment, achieving comparable accuracy to those retrained from scratch. Such two-stage NAS can eliminate  the prohibitively expensive cost for traditional NAS to  retrain each subnet, making it a practical approach for efficient deployment.

\begin{figure}[t]
	\centering
	\includegraphics[width=1\columnwidth]{teaser.png}	
	\vspace{-4.5ex}
	\caption{We train a high-quality ViT supernet for a wide range of mobile devices. Our discovered ViTs outperform SOTA CNNs and ViTs with higher accuracy, fewer FLOPs and faster speed. }
	\label{fig:teaser}
\end{figure}

%Neural Architecture Search (NAS)  is a powerful tool for automatically designing efficient models. Two-stage NAS methods such as OFA~\cite{ofa} and BigNAS~\cite{bignas} have notably achieved state-of-the-art performance in designing efficient CNNs for diverse mobile devices.  It first trains a weight-shared supernet over the search space, and then uses typical search algorithms to find best sub-networks (subnets) within a specific constraint from the supernet. During supernet training, a crucial technique is the sandwich rule~\cite{bignas,yu2019universally}, which trains the largest, smallest, and two randomly sampled subnets at each step. This enables the subnets to directly inherit weights from the supernet for deployment, achieving comparable accuracy to those retrained from scratch. As a result, two-stage NAS can eliminate  the cost for traditional NAS to  retrain each subnet, making it a practical and effcient approach for deployment under various constraints.



% To reduce this cost, two-stage Neural architecture search (NAS) methods such as OFA~\cite{ofa} and BigNAS~\cite{bignas} have achieved state-of-the-art performance for CNN deployment. It first trains a high-quality supernet over the search space, and then uses typical search algorithms to find best subnets within constraint from the supernet. The subnets can directly inherit weights from the supernet for deployment, achieving comparable accuracy to those retrained from scratch. This eliminates the need to retrain each subnet individually, making it a cost-effective approach for deployment under any given constraints.
%Another line of works~\cite{autoformer,nasvit} leverage Neural archiecture search (NAS) to automate the design of ViTs. 

The success of two-stage NAS heavily relies on the quality of the supernet training in the first stage. However, it's extremely challenging to train a high-quality ViT supernet
for mobile devices, due to the  \textbf{vast mobile diversity}: mobile applications must support a wide range of mobile phones with varying computation capabilities, from the latest high-end devices to older ones with much slower CPUs. For instance, Google Pixel 1 runs $~$4$\times$ slower than Pixel 6. As a result, the supernet must cover ViTs that range from  tiny size ($<$ 100M FLOPs) for weak devices to large size for strong ones.
 However, including both tiny and large ViTs results in an overwhelmingly larger  search space compared to typical search spaces in two-stage NAS~\cite{ofa,bignas,nasvit}. Training a supernet over such a search space has been known to suffer from performance degradation due to optimization interference caused by subnets with vastly different sizes~\cite{nse,Yu2020Evaluating,zhang2020you}. 
While existing works~\cite{lin2020mcunet,hurricane,autoformer,su2022vitas} circumvent this issue by manually designing multiple separate normal-sized search spaces, the multi-space  approach can be costly. Moreover, there has been limited discussion regarding the root causes of this problem and how to effectively address it.
 
% the process of search space design requires significant human expertise, which is time-consuming and expensive.

%The success of two-stage NAS heavily relies on the supernet training in the first stage. However, training a ViT supernet that can effectively cover the \textbf{vast diversity} of mobile devices is extremely challenging. Mobile applications must support a wide range of mobile phones with varying computation capabilities, from the latest high-end devices to older ones with much slower CPUs.  This requires the subnets in a ViT supernet to range from tiny ViTs ($<$ 100M FLOPs) for weak mobile devices to large ViTs for strong ones. However, including both tiny and large ViTs in the search space results in an extremely larger search space compared to typical search spaces. Training a supernet over such a vast search space has been known to suffer from performance degradation due to optimization interference and convergence of subnets with vastly different sizes~\cite{nse,Yu2020Evaluating,zhang2020you}. While existing works~\cite{lin2020mcunet,hurricane,autoformer,su2022vitas} solve this issue by manually designing multiple separate small search spaces with normal sizes, the process of search space design requires significant human expertise, which is time-consuming and expensive.


%partitioning the search space into multiple smaller su   b-spaces, it requires significant human expertise for sub search space design. 

% however, the manual partition is expensive and requires lots of human expertise. Little has been discussed about the underlying reasons and how to solve it effectively.  little has been discussed about the underlying reasons and how to solve it effectively. 





%In the past, many techniques~\cite{ofa,bignas,yu2019universally,attentivenas,alphanet} have been proposed to improve the training of CNN supernet. One crucial technique is the sandwich rule~\cite{bignas,yu2019universally}, which trains a biggest, smallest, and two randomly sampled subnets at each step. 

% we have found it to be extremely challenging to train a very large ViT supernet that can effectively cover the \textbf{vast diversity} of mobile devices:  a mobile application has to support a wide range of mobile phones, from the latest high-end devices to older ones with much slower CPUs. Therefore, unlike previous ViT NAS works~\cite{su2021vitas,li2021bossnas,autoformer,nasvit,liao2021searching,ding2021hr} which focus primarily on large models, the supernet training must handle both \textit{tiny ViTs} ($<$100M FLOPs) for weak mobile devices and \textit{large ViTs} for strong ones. This is challenging because covering both tiny and large ViTs results in a very large supernet, which has been observed to suffer from performance degradation due to optimization interference and convergence of subnets with vastly different sizes~\cite{nse,Yu2020Evaluating,zhang2020you}. While existing works~\cite{lin2020mcunet,hurricane,autoformer} circumvent this issue by partitioning the whole search space into multiple smaller sub-spaces, little has been discussed about the underlying reasons and how to solve it effectively. 

%Desigining multiple hardware customized search spaces for different devices can be an alternative~\cite{hurricane,	lin2020mcunet}. However, search space customization for diverse mobile devices requires prior knowledge with both ViT design and hardware expertise, which is prohibitively expensive and impractical. 
%Therefore, we aim to seek other solution and ask the following question: \textit{can we simply construct a huge search space which is a super-set covering both tiny and large hybrid CNN-ViTs for the diverse mobile devices and obtain superior results from it? } If the answer is YES, this approach would solve the hardware-aware search space customization challenge. Unfortunately, training a very large search space has been demonstrated with performance degradation issue~\cite{nse,Yu2020Evaluating,zhang2020you}. 
%Our experiment results show that the a subnet from a very large supernet drops a significant 8\%  top1 ImageNet accuracy compared to a normal size supernet.

%\begin{figure}[t]
 %	\centering
% 	\includegraphics[width=1\columnwidth]{figs/overview.png}	
 %	\caption{We propose conflict-aware sampling to alleviate the gradient conflict caused by complexitity differences between sampled subnets at each step. At step $t$, we first sample a target complexitity $C_t$ that is close to that at step $t-1$, then we sample three subnets with same level of complexitity of $C_t$ and find a nearest min subnet to update supernet weights.}
 	%\caption{(a): models exhibit significantly different latency on diverse mobile devices; (b): latency of current efficient ViTs  on Pixel4; hybrid models surpass pure ViTs in small flops regime.}
 %	\label{fig:overview}
 %\end{figure}

In this work, we introduce {\sysname}, a novel approach for training a high-quality vision transformer supernet that can efficiently serve both strong and weak mobile devices. Our approach is built upon  a single very large search space optimized for mobile devices, containing  a wide range of vision transformers with sizes ranging from 37M to 3G FLOPs. This search space is 10$^7\times$ larger than typical two-stage NAS search spaces, allowing us to accommodate a broad range of mobile devices with various resource constraints.


 %Compared to typical search spaces in two-stage NAS, our search space is 10$^8\times$ larger, allowing us to accommodate various constraints of a broad range of mobile devices.%For a given latency requirement on a target mobile device, we perform evolutionary search with a latency predictor~\cite{nnmeter} to derive an optimal subnet from the supernet. It can be directly deployed without additional training or finetuning cost. 



%In this work, we introduce {\sysname}, a novel approach that trains a high-quality vision transformer supernet over a single, remarkably large search space to cater to both strong and weak mobile devices. The search space includes a wide range of vision transformers with diverse sizes, from less than 37M to 3G FLOPs, making it 10$^8$$\times$ larger than typical search spaces in BigNAS and OFA. For a given latency requirement on a target mobile device, we perform evolutionary search with a latency predictor~\cite{nnmeter} to derive an optimal subnet from the supernet. It can be directly deployed without additional training or finetuning cost. 
%To tackle the challenge of training a high-quality ViT supernet across our extensive search space, we  examine the supernet training procedure and investigate the root causes of its poor performance. 
We start by investigating the root causes of poor performance when training a ViT supernet over our excessively large search space. 
We found that the main reason  is that prior supernet training methods rely on uniform sampling~\cite{autoformer,nasvit,bignas,ofa}, which can easily sample subnets with vastly different model sizes (e.g., 50M vs. 1G FLOPs) from our search space. This leads to conflicts between subnets' gradients and creates optimization challenges. We make two key observations:\textit{(i) the gradient conflict between two subnets increases with the FLOPs difference between them;} and \textit{(ii) gradient conflict between same-sized subnets can be significantly reduced if they are good subnets.}






%However, directly applying sandwich rule in BigNAS results in poor performance on our extremely large ViT supernet. To explore the underlying causes, we examine the supernet training procedure and identify that the main reason is that sandwich rule often samples subnets with large variations in model size from a very large supernet. This leads to gradient conflicts between subnets and creates optimization challenges. We make two key observations: \textit{(i) the gradient conflict between two subnets increases with the FLOPs difference between them;} and \textit{(ii) gradient conflict between same-sized subnets can be significantly reduced if they are good subnets.}

Inspired by the above observations, we propose two key techniques to address the gradient conflict issue.  First, we propose  \textit{complexity-aware sampling} to limit the difference in FLOPs between sampled subnets across adjacent training steps, while ensuring that different-sized subnets within the search space are sampled. We achieve this by constraining the FLOPs level of the sampled subnets to be close to that of the previous step. Furthermore, we employ a multiple-min strategy to sample the nearest smallest subnet based on the FLOPs sampled at each step, thus ensuring performance bounds without introducing a large FLOPs difference with other subnets.
 Second, we introduce \textit{performance-aware sampling} that further reduces the gradient conflicts among subnets with similar FLOPs. Our method samples subnets with higher potential accuracy at each step from a prior distribution that is dynamically updated based on an exploration and exploitation policy. The policy leverages a memory bank and a ViT architecture preference rule. The preference rule guides the exploration of subnets with wider width and shallower depth, which are empirically preferred by ViT architectures. The memory bank stores historical  good subnets for each FLOPs level using prediction loss as a criterion.

%Inspired by the above observations, we propose two key techniques to address the gradient conflict issue.  First, we propose \textit{complexity-aware sampling} to limit the FLOPs difference between sampled subnets, instead of using the sandwich rule that can produce subnets with large FLOPs difference. At each training step, we constrain the FLOPs level of the sampled subnets to be close to that of the previous step. Furthermore, since sandwich rule always samples the min and max subnet, which can cause a large FLOPs difference within a step, we skip sampling the max subnet and use a nearest min subnet strategy instead of sampling the original min subnet. Second, we introduce \textit{performance-aware sampling} that further reduces the gradient conflicts among subnets with similar FLOPs. Our method samples subnets with higher potential accuracy at each step from a prior distribution that is dynamically updated based on an exploration and exploitation policy. The policy leverages a memory bank and a ViT architecture preference rule. The preference rule guides the exploration of subnets with wider width and shallower depth, which are empirically preferred by ViT architectures. The memory bank stores historical  good subnets for each FLOPs level using prediction loss as a criterion.

%We perform evolutionary search with a latency predictor~\cite{nnmeter} over our high-quality supernet to  derive an optimal subnet under a given latency requirement. The searched subnet can be directly deployed without additional training or finetuning cost. 
Our contributions are summarized as follows:
\begin{itemize}
	\vspace{-1ex}
	\item We propose {\sysname} to automate the design of accurate and low-latency ViTs for diverse mobile devices. For the first time we are able to train a high-quality ViT supernet over a vast and mobile-regime search space.
		\vspace{-1ex}
	\item We conduct thorough analysis on the poor-quality supernet trained by existing approaches, and find that uniform sampling results in subnets of vastly different sizes, leading to gradient conflicts. 
	\vspace{-1ex}
	\item Inspired by our analysis, we propose two methods, complexity-aware sampling and performance-aware sampling, to effectively address the gradient issues by sampling good subnets and limiting their FLOPs differences across adjacent training steps.
	\vspace{-1ex}
	\item Extensive experiments on ImageNet~\cite{imagenet}
	and four mobile devices demonstrate that our discovered models achieve significant improvements over SOTA efficient CNN and ViT models in terms of both inference speed and accuracy. For example,  {\sysname}-T3 achieves
	 the same accuracy of 75.2\% as MobileNetV3 with only 160 MFLOPs, while be 1.2$\times$ faster. This is the first time that ViT outperforms CNN with a faster speed on mobile devices within the 200 MFLOPs range, to the best of our knowledge. {\sysname}-L achieves 80.0\% accuracy with 806 MFLOPs, which is 5.3\% higher than Autoformer-Tiny~\cite{autoformer} while using 1.61$\times$ fewer FLOPs. We also prove that {\sysname} substantially enhance the quality of supernet training, resulting in a noteworthy 3.9\% accuracy improvements for best-searched models. 
\end{itemize}



 

%To address the gradient conflict issue, we propose two key techniques. Firstly, instead of using the original sandwich rule to sample four subnets with potentially large FLOPs differences in a very large supernet, we propose adjacent step sampling as shown in Fig.~\ref{fig:overview}. \tc{How about using ``At each step, we constrain the complexity level of the sampling subnet to close to the previous step. And we further adopt a nearest min subnet to update the supernet weights. '' ?} At each step, we sample a target complexitity which is close to the previous step and then sample three subnets with the same level of FLOPs size and find a nearest min subnet to update the supernet weights. \tc{``Secondly, ... .'' needs to be aligned with Sec 4.2. \\example: To further alleviate the gradient conflicts, we devise a performance-aware sampling scheme to select the potentially good subnets to train, by xxx (reshaping the sampling prior distribution dynamically). } Secondly, to further reduce the gradient conflict between same-sized subnets, we adopt an exploration and exploitation policy to find good subnets using a preference rule and a quality-aware memory bank. \tc{Moreover, to ensure the sampling quality, we define a preference rule to filter the unpromising sub-networks...}The preference rule explores potentially good subnets with wider width and shallower depth based on our empirical study of ViT architecture preferences, while the memory bank stores historical good subnets for each complexity using prediction loss as a metric. For a given mobile device, we perform evolutionary search with a latency predictor~\cite{nnmeter} to derive an optimal subnet within target requirement. It can be directly deployed without additional training or finetuning cost. 


% In particular, hybrid CNN-ViTs proposed by LeViT~\cite{levit} and NASViT~\cite{nasvit} have shown its superiority in small FLOPs regime. As shown in Fig.~\ref{fig:modellatency}(b), hybrid models significantly outperform pure ViTs with up to 4.2$\times$ smaller latency on a Pixel 4 phone. However,  existing hybrid models still face the efficient deployment challenges due to the \textbf{huge diversity} of mobile devices: one mobile application has to support a diverse range of mobile phones, from a latest high-end Pixel 6  to a 6-year-old Pixel 1 with a much slower CPU, which are equipped with different computation and memory resources. Consequently, the optimal model under a given latency constraint varies significantly across devices. Moreover,  little attention is paid on designing tiny ViTs for weak devices. When targeting $<40ms$, our measurements  suggest that we can deploy a large LeViT-192 (658M FLOPs) on Pixel 6 and a small NASViT-A0 (208M FLOPs) on Pixel 4 (Fig.~\ref{fig:modellatency}(a)),  but none of existing ViTs can meet this constraint on Pixel 1. 


%To design more efficient ViTs, recent research has focused on  optimizing the self-attention operation~\cite{edgevit,swin,nextvit} or designing hybrid CNN-ViT models~\cite{cmt,mobilevit,mobilevitv3,levit,mobileformer,efficientformer}. Another line of works~\cite{autoformer,nasvit} leverage Neural archiecture search (NAS) to automate the design of ViTs under multiple constraints. In particular, hybrid CNN-ViTs proposed by LeViT~\cite{levit} and NASViT~\cite{nasvit} have shown its superiority in small FLOPs regime. As shown in Fig.~\ref{fig:modellatency}(b), hybrid models significantly outperform pure ViTs with up to 4.2$\times$ smaller latency on a Pixel 4 phone. However,  existing hybrid models still face the efficient deployment challenges due to the \textbf{huge diversity} of mobile devices: one mobile application has to support a diverse range of mobile phones, from a latest high-end Pixel 6  to a 6-year-old Pixel 1 with a much slower CPU, which are equipped with different computation and memory resources. Consequently, the optimal model under a given latency constraint varies significantly across devices. Moreover,  little attention is paid on designing tiny ViTs for weak devices. When targeting $<40ms$, our measurements  suggest that we can deploy a large LeViT-192 (658M FLOPs) on Pixel 6 and a small NASViT-A0 (208M FLOPs) on Pixel 4 (Fig.~\ref{fig:modellatency}(a)),  but none of existing ViTs can meet this constraint on Pixel 1. 


%To handle with the huge diversity on real-world mobile devices, our work leverages once-for-all NAS  (e.g., OFA~\cite{ofa} and BigNAS~\cite{bignas}) to train a big supernet (search space) containing a variety of hybrid CNN-ViTs with elastic sizes and latency.
% For a given latency constraint, we perform  evolutionary search to derive an optimal subnet from the supernet. It can be directly deployed on target device without additional training cost.  
% However, \textit{existing ViT search spaces cannot be applied on the diverse mobile devices}, as the current design does not consider  weak mobile devices and is not optimized for latency.  For instance, even the int8 quantized version of smallest model in NASViT search space (190M FLOPs) runs  63 ms  on Pixel 1, which exceeds the real-time latency constraint, showing a big gap between the search space design  and deployment on weak devices. 
 
 

%Desigining multiple hardware customized search spaces for different devices can be an alternative~\cite{hurricane,	lin2020mcunet}. However, search space customization for diverse mobile devices requires prior knowledge with both ViT design and hardware expertise, doing this by hand takes substantial human efforts and computational resources.
%which is prohibitively expensive and impractical. 
%Therefore, we aim to seek other solution and ask the following question: \textit{can we simply construct a huge search space which is a super-set covering both tiny and large hybrid CNN-ViTs for the diverse mobile devices and obtain superior results from it? } If the answer is YES, this approach would solve the hardware-aware search space customization challenge. Unfortunately, training a very large search space has been demonstrated with performance degradation issue~\cite{nse,Yu2020Evaluating,zhang2020you}. 
%Our experiment results show that the a subnet from a very large supernet drops a significant 8\%  top1 ImageNet accuracy compared to a normal size supernet.



 





