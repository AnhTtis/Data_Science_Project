\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
\usepackage[preprint]{neurips_2024}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{wasysym}
\usepackage{cancel}
\usepackage[dvipsnames]{xcolor}
% \usepackage[ruled,vlined]{algorithm2e}
\usepackage{multirow}
\usepackage{diagbox}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       %professional-quality tables

\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{units}
\usepackage[normalem]{ulem}

\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref.
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclarePairedDelimiter{\norm}{\lVert}{\rVert} 

\usepackage{enumitem}
\setlist{leftmargin=*,itemsep=0pt}

\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\argsup}{arg\,sup}
\newcommand{\xmark}{\ding{55}}

\newcommand{\KL}[2]{\text{D}_{\text{KL}}\left(#1\Vert #2\right)}
\newcommand{\UKL}[2]{\text{D}_{\text{KL}}\left(#1\Vert #2\right)}
\newcommand{\Df}[3]{D_{#1}\left(#2\Vert #3\right)}
\newcommand*{\defeq}{\stackrel{\text{def}}{=}}

\allowdisplaybreaks
\everypar{\looseness=-1}


\title{Light Unbalanced Optimal Transport}

\author{
  Milena Gazdieva\\
  Skolkovo Institute of Science and Technology\\
  Moscow, Russia \\
  \texttt{milena.gazdieva@skoltech.ru} \\
  \And
  Arip Asadulaev\\
  ITMO University\\
  Artificial Intelligence Research Institute\\
  Moscow, Russia \\
  \texttt{asadulaev@airi.net} \\
  \AND
  Evgeny Burnaev \\
  Skolkovo Institute of Science and Technology\\
  Artificial Intelligence Research Institute\\
  Moscow, Russia\\
  \texttt{e.burnaev@skoltech.ru}
  \And
  Alexander Korotin\\
  Skolkovo Institute of Science and Technology\\
  Artificial Intelligence Research Institute\\
  Moscow, Russia \\
  \texttt{a.korotin@skoltech.ru} \\
}

\begin{document}

\maketitle

\begin{abstract} 
While the continuous Entropic Optimal Transport (EOT) field has been actively developing in recent years, it became evident that the classic EOT problem is prone to different issues like the sensitivity to outliers and imbalance of classes in the source and target measures. This fact inspired the development of solvers that deal with the \textit{unbalanced} EOT (UEOT) problem $-$ the generalization of EOT allowing for mitigating the mentioned issues by relaxing the marginal constraints. Surprisingly, it turns out that the existing solvers are either based on heuristic principles or heavy-weighted with complex optimization objectives involving several neural networks. We address this challenge and propose a novel theoretically-justified, lightweight, unbalanced EOT solver. Our advancement consists of developing a novel view on the optimization of the UEOT problem yielding tractable and a non-minimax optimization objective. We show that combined with a light parametrization recently proposed in the field our objective leads to a fast, simple, and effective solver which allows solving the continuous UEOT problem in minutes on CPU. We prove that our solver provides a universal approximation of UEOT solutions and obtain its generalization bounds. We give illustrative examples of the solver's performance.
\end{abstract}

\vspace{-1mm}
\section{Introduction}
\label{sec-introduction}

\vspace{-1mm}
The computational \textit{optimal transport} (OT) has proven to be a powerful tool for solving various popular tasks, e.g., image-to-image translation \citep{xie2019scalable,fan2020scalable,mokrov2024energy,gushchin2023entropic}, image generation \citep{wang2021deep,de2021diffusion,chen2021likelihood} and biological data transfer \citep{bunne2023learning,koshizuka2022neural,vargas2021solving}. Historically, the majority of early works in the field were built upon solving the OT problem between discrete probability measures \citep{cuturi2013sinkhorn,peyre2019computational}. Only recently the advances in the field of generative models have led to explosive interest from the ML community in developing the \textbf{continuous} OT solvers, see \citep{korotin2021continuous} for a survey. The setup of this problem assumes that the learner needs to estimate the OT plan between continuous measures given only empirical samples of data from them. Due to convexity-related issues of OT problem \citep{korotin2023kernel}, many works consider the EOT problem, i.e., use \textit{entropy} regularizers which guarantee, e.g., the uniqueness of learned plans.



Meanwhile, researches attract attention to other shortcomings of the classic OT problem. It enforces hard constraints on the marginal measures and, thus, does not allow for mass variations. As a result, OT shows high sensitivity to an imbalance of classes and outliers in the source and target measures \citep{balaji2020robust} which are almost inevitable for large-scale datasets. To overcome these issues, it is common to consider extensions of the OT problem, e.g., unbalanced OT/EOT (UOT/UEOT) \citep{chizat2017unbalanced, liero2018optimal}. The unbalanced OT/EOT formulations allow for variation of total mass by relaxing the marginal constraints through the use of divergences.

The scope of our paper is the continuous UEOT problem. It seems that in this field, a solver that is fast, light, and theoretically justified has not yet been developed. Indeed, many of the existing solvers follow a kind of heuristical principles and are based on the solutions of discrete OT. For example, \citep{lubeck2022neural} uses a regression to interpolate the discrete solutions, and \citep{eyring2023unbalancedness, klein2023generative} build a flow matching upon them. Almost all of the other solvers \citep{choi2023generative, yang2018scalable} employ several neural networks with many hyper-parameters and require time-consuming optimization procedures. We solve the aforementioned shortcomings by introducing a novel lightweight solver that can play the role of a simple baseline for unbalanced EOT.


\vspace{-1mm}
\textbf{Contributions.} We develop a novel \textit{lightweight} solver to estimate continuous \textbf{unbalanced} EOT couplings between probability measures (\wasyparagraph\ref{sec-method}). Our solver has a non-minimax optimization objective and employs the Gaussian mixture parametrization for the UEOT plans. We provide the generalization bounds for our solver (\wasyparagraph\ref{sec-generalization-bounds}) and experimentally test it on several tasks (\wasyparagraph \ref{sec-gaussian-exp}, \wasyparagraph\ref{sec-alae-exp}).

\vspace{-1mm}
\textbf{Notations.} We work in the Euclidian space $(\mathbb{R}^d, \|\cdot\|)$. We use $\mathcal{P}_{2,ac}(\mathbb{R}^d)$ (or $\mathcal{M}_{2,+}(\mathbb{R}^d)$) to denote the set of absolutely continuous Borel probability (or non-negative) measures on $\mathbb{R}^d$ with finite second moment and differential entropy. We use $\mathcal{C}_{2}(\mathbb{R}^d)$ to denote the space of all \textit{continuous} functions $\zeta:\mathbb{R}^d\rightarrow\mathbb{R}$ for which $\exists a=a(\xi), b=b(\xi)$ such that $\forall x\in\mathbb{R}^d:$ $|\zeta(x)|\leq a +b\|x\|^2$. Its subspace of functions which are additionally \textit{bounded from above} is denoted as $\mathcal{C}_{2,b}(\mathbb{R}^d)$.
For any $p\in \mathcal{P}_{2,ac}(\mathbb{R}^d)$ (or $\mathcal{M}_{2,+}(\mathbb{R}^d)$), we use $p(x)$ to denote its density at a point $x\in\mathbb{R}^d$.
For a given measure $\gamma\in \mathcal{M}_{2,+}(\mathbb{R}^d \times \mathbb{R}^d)$, we denote its total mass by $\|\gamma\|_1\!\defeq\!\int_{\mathbb{R}^d\times \mathbb{R}^d}\gamma(x, y) dx dy$. We use $\gamma_x(x)$, $\gamma_y(y)$ to denote the marginals of $\gamma(x,y)$. They statisfy the equality $\|\gamma_x\|_1\!=\!\|\gamma_y\|_1\!=\!\|\gamma\|_1$. We write $\gamma(y|x)$ to denote the conditional \textit{probability} measure. Each such measure has a unit total mass. We use $\overline{f}$ to denote the Fenchel conjugate of a function $f$: $\overline{f}(t)=\sup_{u\in \mathbb{R}^d} \{ut-f(u)\}$. We use $\mathbb{I}_{A}$ to denote the convex indicator of a set $A$, i.e., $\mathbb{I}_{A}(x)=0$ if $x\in A$; $\mathbb{I}_{A}(x)=+\infty$ if $x\notin A$.

\vspace{-1mm}
\section{Background}
\label{sec-background}
Here we give an overview of the relevant entropic optimal transport (EOT) concepts. For additional details on balanced EOT, we refer to \citep{cuturi2013sinkhorn, genevay2019entropy, peyre2019computational}, unbalanced EOT - to \citep{chizat2017unbalanced, liero2018optimal}.

\vspace{-1mm}
\textbf{$f$-divergences for positive measures}. For \textit{positive} measures $\mu_1,\mu_{2}\in \mathcal{M}_{2,+}\!(\mathbb{R}^{d'})$ and a lower semi-continuous function $f:\mathbb{R}\rightarrow \mathbb{R}\cup \{\infty\}$, \textit{$f$-divergence} between $\mu_{1},\mu_{2}$ is defined by:
$$\mathcal{D}_f(\mu_1\|\mu_2)\defeq \int_{\mathbb{R}^{d'}} f\bigg(\frac{\mu_{1}(x)}{\mu_{2}(x)}\bigg)\mu_{2}(x)dx.$$
We consider $f(t)$ which are convex, non-negative and attain zero uniquely when $t=1$. In this case, $D_{f}$ is a valid measure of dissimilarity between two positive measures (see Appendix \ref{app-ablation} for \underline{details}). This means that $D_{f}(\mu_1\|\mu_2)\!\geq\! 0$ and $D_{f}(\mu_1\|\mu_2)\!=\! 0$ if and only if $\mu_{1}=\mu_{2}$.

Kullback-Leibler divergence $\text{D}_{\text{KL}}$ \citep{chizat2017unbalanced, sejourne2022unbalanced}, is a particular case of such $f$-divergence for positive measures. It has a generator function $f_{\text{KL}}(t)\!\defeq\! t\log t\! -\! t\! +\!1$. Its convex conjugate $\overline{f_{\text{KL}}}(t)\!=\!\exp(t)-1$. Another example is the $\chi^{2}$-divergence $\text{D}_{\chi^2}$ which is generated by the $f_{\chi^2}(t)=\mathbb{I}_{t<0}\cdot \infty + \mathbb{I}_{t\geq 0}\cdot (t-1)^2$. The convex conjugate of this function is $\overline{f_{\chi^2}}(t)=\mathbb{I}_{t<-2}\cdot (-1) + \mathbb{I}_{t\geq -2}\cdot (\frac{1}{4} t^2 + t)$.

\vspace{-1mm}
\textbf{Entropy for positive measures.} For $\mu\in \mathcal{M}_{2,+}(\mathbb{R}^{d'})$, its entropy \citep{chizat2017unbalanced} is given by
\begin{equation}
    H(\mu)\!\defeq\! -\int_{\mathbb{R}^{d'}}\mu(x)\! \log \mu(x) dx+\int_{\mathbb{R}^{d'}}\mu(x)dx.
    \label{entropy}
\end{equation}
When $\mu\in\mathcal{P}_{2,ac}(\mathbb{R}^{d'})$, i.e., $\int_{\mathbb{R}^{d'}}\mu(x)dx=1$, equation \eqref{entropy} is the usual differential entropy minus 1.

\vspace{-1mm}
\textbf{Classic EOT formulation (with the quadratic cost).}
Consider two probability measures $p\!\in\! \mathcal{P}_{2,ac}(\mathbb{R}^d)$, $q\!\in\!\mathcal{P}_{2,ac}(\mathbb{R}^d)$. For $\varepsilon>0$, the EOT problem between $p$ and $q$ is

\vspace{-5mm}\begin{eqnarray}
\min_{\pi\in\Pi(p, q)} \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \frac{\|x-y\|^2}{2} \pi(x, y) dx dy - \varepsilon H(\pi),
\label{eot-classic}
\end{eqnarray}
where $\Pi(p,q)$ is the set of probability measures $\pi\!\in\!\mathcal{P}_{2,ac}(\mathbb{R}^d\!\times\! \mathbb{R}^d)$ with marginals $p$, $q$ (transport plans). Plan $\pi^*$ attaining the minimum exists, it is unique and called the \textit{EOT plan.}

\vspace{-1mm}
Classic EOT imposes hard constraints on the marginals which leads to several issues, e.g., sensitivity to outliers \citep{balaji2020robust}, inability to handle potential measure shifts such as class imbalances in the measures $p, q$. The UEOT problem \citep{yang2018scalable, choi2023generative} overcomes these issues by relaxing the marginal constraints \citep{sejourne2022unbalanced}.

\textbf{Unbalanced EOT formulation (with the quadratic cost).} 
Let $D_{f_1}$ and $D_{f_2}$ be two $f$-divergences over $\mathbb{R}^d$.
For two probability measures $p\in \mathcal{P}_{2,ac}(\mathbb{R}^d)$, $q\in\mathcal{P}_{2,ac}(\mathbb{R}^d)$ and $\varepsilon>0$, the unbalanced EOT problem between $p$ and $q$ consists of finding a minimizer of
\begin{eqnarray}
    \inf_{\gamma\in\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d)} \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \frac{\|x-y\|^2}{2} \gamma(x, y) dx dy -
     \varepsilon H(\gamma)\!+\Df{f_1}{\gamma_x}{p}   + \Df{f_2}{\gamma_y}{q}.
     \label{unbalanced-eot-primal}
\end{eqnarray}

Here the minimum is attained for a unique $\gamma^*$ which is called the \textit{unbalanced optimal entropic} (UEOT) plan. Typical choices of $f_i$ ($i\in[1,2]$) are $f_i(a)=\tau_i f_{\text{KL}}(a)$ or $f_i(x)=\tau_i f_{\chi^2}(a)$ ($\tau_i>0$) yielding the scaled $\text{D}_{\text{KL}}$ and $\text{D}_{\chi^{2}}$, respectively. In this case, the bigger $\tau_1$ ($\tau_2$) is, the more $\gamma_x$ ($\gamma_y$) is penalized for not matching the corresponding marginal distribution $p$ ($q$).

\vspace{-1mm}
\textit{Remark.} The balanced EOT problem \eqref{eot-classic} is a special case of \eqref{unbalanced-eot-primal}. Indeed, let $f_1$ and $f_2$ be the convex indicators of $\{1\}$, i.e., $f_1(x)=f_2(x)=\mathbb{I}_{x=1} \cdot 0 + \mathbb{I}_{x\neq 1}\cdot \infty$. Then the $f$-divergences $\Df{f_1}{\gamma_x}{p}$ and $\Df{f_2}{\gamma_y}{q}$ become infinity if $p\neq\gamma_x$ or $q\neq\gamma_y$, and become zeros otherwise.


\textbf{Dual form of unbalanced EOT problem \eqref{unbalanced-eot-primal}} is formulated as follows
\begin{eqnarray}
    \!\!\!\sup_{(\phi, \psi)} \!\Bigg\lbrace\!\!\! -\!\!\varepsilon \!\!\int_{\mathbb{R}^d}\!\! \int_{\mathbb{R}^d}\! \!\!\exp\{\frac{1}{\varepsilon} \!( \phi(x) \!+\! \psi(y)\!-\! \frac{\|x-y\|^2}{2}) \} dx dy \!- \!\!
    \int_{\mathbb{R}^d} \!\!\overline{f}_1 (-\phi(x))p(x) dx \! - \!\!\!\int_{\mathbb{R}^d}\!\! \overline{f}_2 (-\psi(y))q(y) dy
    \Bigg\rbrace.\!
    \label{unbalanced-eot-dual}
\end{eqnarray}
Potentials $\phi^*$, $\psi^*$ delivering maximum to \eqref{unbalanced-eot-dual} have the following connection with the solution of the primal unbalanced problem \eqref{unbalanced-eot-primal}:
\begin{eqnarray}
\label{gamma-potentials}
    \gamma^*(x, y) \!=\! \exp\{\frac{\phi^*(x)}{\varepsilon}\} \!\exp\{-\frac{\|x-y\|^2}{2\varepsilon}\} \!\exp\{\frac{\psi^*(y)}{\varepsilon}\}.
\end{eqnarray}
For our further purposes, we need to consider potentials $(\phi, \psi)$ in problem \eqref{unbalanced-eot-dual} which belong to the space $C_{2,b}(\mathbb{R}^d)\times C_{2,b}(\mathbb{R}^d)$. Since established variants of dual forms \citep{chizat2017unbalanced} typically correspond to other functional spaces, we derive and theoretically justify the \underline{\textit{dual problem}} \eqref{unbalanced-eot-dual} in Appendix \ref{appendix-dual-form}.

\textbf{Computational UEOT setup.} Analytical solution for the \textit{unbalanced} EOT problem is, in general, not known.\footnote{Analytical solutions are known only for some specific cases. For example, \citep{janati2020entropic} consider the case of Gaussian measures and UEOT problem with $\text{D}_{\text{KL}}$ divergence instead of differential entropy. This setup differs from ours.}
Moreover, in real-world setups where unbalanced EOT is applicable, the measures $p$, $q$ are typically not available explicitly but only through their empirical samples (datasets). 

We assume that data measures $p, q\in \mathcal{P}_{2,ac}(\mathbb{R}^d)$ are unknown and accessible only by a limited number of i.i.d. empirical samples $\{x_0, ..., x_N\}\!\sim\! p$, $\{y_0, ..., y_M\}\!\sim\! q$. We aim to approximate the optimal UEOT plans solving \eqref{unbalanced-eot-primal} between the entire measures $p, q$. The recovered plans should allow the out-of-sample estimation, i.e., generation of samples from $\gamma^*(\cdot|x^{\text{new}})$ where $x^{\text{new}}$ is a new test point (not necessarily present in the train data). Optionally, one may require the ability to sample from $\gamma^*_x$. 


The described setup is typically called the \textit{continuous EOT} and should not be twisted up with the \textit{discrete EOT} \citep{peyre2019computational, cuturi2013sinkhorn}. There the aim is to recover the (unbalanced) EOT plan between the empirical measures $\hat{p}\!=\!\!\frac{1}{N}\!\sum_{i=1}^{N} \!\delta_{x_i}$, $\hat{q}\!=\!\!\frac{1}{M}\!\sum_{j=1}^{M} \!\delta_{y_j}$ and the out-of-sample estimations are typically not needed.


\section{Related Work}
\vspace{-1mm}
\label{sec-related-work}
\begin{table*}[!t]
\vspace{-1mm}
\tiny
\hspace{-10mm}\begin{tabular}{ c|c|c|c|c }
\hline
\shortstack{\textbf{Solver}} & \shortstack{\textbf{Problem}} & \shortstack{\textbf{Principles}} & \shortstack{\textbf{What recovers?}} & \shortstack{\textbf{Limitations}}\\ 
\hline
\citep{yang2018scalable} & UOT &  \makecell{Solves $c$-transform based semi-dual\\ max-min reformulation of UOT using neural nets} & \makecell{Scaling factor $\nicefrac{\gamma^*(x)}{p(x)}$ and \\ stochastic OT map $T^*(x,z)$} & \makecell{Complex max-min objective;\\3 neural networks}\\
\hline
\citep{lubeck2022neural} & \makecell{Custom \\UOT} & \makecell{Regression on top of discrete EOT\\ between re-balanced measures\\ combined with ICNN-based solver \citep{makkuva2020optimal}} & \makecell{Scaling factors and \\ OT maps between re-scaled measures} & \makecell{Heuristically uses \\ minibatch OT approximations}\\
\hline
\citep{choi2023generative} & UOT &  \makecell{Solves semi-dual max-min \\reformulation of UOT using neural nets} & Stochastic UOT map $T^*(x,z)$ & \makecell{Complex max-min objective;\\2 neural networks}\\
\hline
\citep{eyring2023unbalancedness} & UEOT & \makecell{Flow Matching on top of discrete UEOT\\using neural nets} & \makecell{Parametrized vector field \\$(v_{t,\theta})_{t\in[0,1]}$ to transport the mass} & \makecell{Heuristically uses \\ minibatch OT approximations}\\
\hline
\citep{klein2023generative} & UEOT & \makecell{Conditional Flow Matching on top of discrete EOT\\between re-balanced measures using neural nets} & \makecell{Scaling factors and parametrized conditional\\ vector field $(v_{t,\theta})_{t\in[0,1]}$ to transport \\the mass between re-scaled measures} & \makecell{Heuristically uses \\ minibatch OT approximations}\\
\hline
 \makecell{U-LightOT \\(\textbf{ours})} &  UEOT & \makecell{Solves non-minimax reformulation \\of dual UEOT using Gaussian Mixtures} & \makecell{Density of UEOT plan $\gamma^*$ together with light\\ procedure to sample $x\sim\gamma^*_x(\cdot)$ and $y\sim\gamma^*_y(\cdot|x)$}& \makecell{Restricted to Gaussian\\ Mixture parametrization}
 \\
\hline
\end{tabular}
\vspace{-1mm}
\captionsetup{justification=centering}
 \caption{Comparison of the principles of existing UOT/UEOT solvers and \textbf{our} proposed light solver.}
 \label{table-comparison}
\vspace{-5mm}
\end{table*}

\vspace{-1mm}
Nowadays, the sphere of continuous OT/EOT solvers is actively developing. Some of the early works related to this topic utilize OT cost as the loss function \citep{gulrajani2017improved, genevay2018learning, arjovsky2017wasserstein}. These approaches are not relevant to us as they do not learn OT/EOT maps (or plans).
We refer to \citep{korotin2022kantorovich} for a detailed review.

\vspace{-1mm}At the same time, there exist a large amount of works within the discrete OT/EOT setup \citep{cuturi2013sinkhorn,dvurechenskii2018decentralize,xie2022accelerated, nguyen2022unbalanced}, see \citep{peyre2019computational} for a survey. We again emphasize that solvers of this type are not relevant to us as they construct discrete matching between the given (train) samples and typically do not provide a generalization to the new unseen (test) data. Only recently ML community started developing out-of-sample estimation procedures based on discrete/batched OT. For example, \citep{fatras2020learning,pooladian2021entropic,hutter2021minimax,deb2021rates,manole2021plugin,rigollet2022sample} mostly develop such estimators using the barycentric projections of the discrete EOT plans. Though these procedures have nice theoretical properties, their scalability remains unclear.

\vspace{-1mm}
\textbf{Balanced OT/EOT solvers.} 
There exists a vast amount of neural solvers for continuous OT problem.
Most of them learn the OT maps (or plans) via solving saddle point optimization problems \citep{asadulaev2024neural, fan2023neural, korotin2021neural, gazdieva2022unpaired, rout2022generative, mokrov2024energy}. 
Though the recent works \citep{gushchin2023entropic, seguy2018large, daniels2021score,korotin2024light,gushchin2024light} tackle the EOT problem \eqref{eot-classic}, they consider its balanced version. Hence they are not relevant to us. 
Among these works, only \citep{korotin2024light,gushchin2024light} evade non-trivial training/inference procedures and are ideologically the closest to ours. The difference between them consists of the particular loss function used.
In fact, \textbf{our paper} proposes the solver which subsumes these solvers and generalizes them for the unbalanced case. The derivation of our solver is non-trivial and requires solid mathematical apparatus, see \wasyparagraph\ref{sec-method}. 

\vspace{-1mm}
\textbf{Unbalanced OT/EOT solvers.} A vast majority of early works in this field tackle the discrete UOT/UEOT setup \citep{chapel2021unbalanced, fatras2021unbalanced,pham2020unbalanced} but the principles under their construction are not easy to generalize to the continuous setting. Thus, many of the recent papers that tackle the continuous unbalanced OT/EOT setup employ discrete solutions in the construction of their solvers. For example, \citep{lubeck2022neural} regress neural network on top of scaling factors obtained using the discrete UEOT while simultaneously learning the continuous OT maps using an ICNN method \citep{makkuva2020optimal}. In \citep{eyring2023unbalancedness} and \citep{klein2023generative}, the authors implement Flow Matching \citep[FM]{lipman2022flow} and conditional FM on top of the discrete UEOT plans, respectively. 
The algorithm of the latter consists of regressing neural networks on top of scaling factors and simultaneously learning a conditional vector field to transport the mass between re-balanced measures. Despite the promising practical performance of these solvers, it is still unclear to what extent such approximations of UEOT plans are theoretically justified.


The recent papers \citep{yang2018scalable, choi2023generative} are more related to our study as they do not rely on discrete OT approximations of the transport plan. However, they have non-trivial minimax optimization objectives solved using \textit{complex} GAN-style procedures. Thus, these GANs often lean on heavy neural parametrization, may incur instabilities during training, and require careful hyperparameter selection \citep{lucic2018gans}. 

For completeness, we also mention other papers which are only slightly relevant to us. For example, \citep{gazdieva2023extremal} considers incomplete OT which relaxes only one of the OT marginal constraints and is less general than the unbalanced OT. Other works \citep{dao2023robust, balaji2020robust} incorporate unbalanced OT into the training objective of GANs aimed at generating samples from noise. 

In contrast to the listed works, our paper proposes a \textit{theoretically justified and lightweight} solver to the UEOT problem, see Table \ref{table-comparison} for the detailed comparison of solvers.




\vspace{-1mm}
\section{Light Unbalanced OT Solver}
\label{sec-method}

\vspace{-2mm}
In this section, we derive the optimization objective (\wasyparagraph \ref{sec-optimization}) of our U-LightOT solver, present practical aspects of training and inference procedures (\wasyparagraph \ref{sec-algorithm}) and derive the generalization bounds for our solver (\wasyparagraph\ref{sec-generalization-bounds}). We provide the \textit{\underline{proofs of all theoretical results}} in Appendix \ref{sec-proofs}.

\vspace{-2mm}
\subsection{Derivation of the Optimization Objective}
\label{sec-optimization}
\vspace{-2mm}
Following the learning setup described above, we aim to get a parametric approximation $\gamma_{\theta, w}$ of the UEOT plan $\gamma^*$. Here $\theta,\omega$ are the model parameters to learn and it will be clear later why we split them into two groups. To recover $\gamma_{\theta,\omega}\approx \gamma^{*}$, our aim is to learn $\theta,\omega$ by directly minimizing the $\text{D}_{\text{KL}}$ divergence between $\gamma_{\theta,w}$ and $\gamma^*$: 
\begin{equation}
    \KL{\gamma^*}{\gamma_{\theta,w}}\rightarrow \min_{(\theta,w)}.
    \label{main-objective}
\end{equation}\vspace{-4mm}


The main difficulty of this optimizing objective \eqref{main-objective} is obvious: the UEOT plan $\gamma^*$ is \textit{unknown}. Fortunately, below we show that one still can optimize \eqref{main-objective} without knowing $\gamma^{*}$. 

Recall that the optimal UEOT plan $\gamma^*$ has the form \eqref{gamma-potentials}. We first make some changes of the variables. Specifically, we define  
$v^*(y)\defeq\exp\{\frac{2\psi^*(y)-\|y\|^2}{2\varepsilon}\}$.
Formula \eqref{gamma-potentials} now reads as
\begin{eqnarray}
    \gamma^*(x, y) = \exp\{\frac{2\phi^*(x)-\|x\|^2}{2\varepsilon}\} \!
    \exp\{\frac{\langle x, y\rangle}{\varepsilon}\} v^*(y)\! \Longrightarrow \label{gamma-mid-potentials} %\nonumber \\
    \!\gamma^*(y|x) \propto \exp\{\frac{\langle x, y\rangle}{\varepsilon}\} v^*(y).
    \label{v-unnormalized}
\end{eqnarray}
Since the conditional plan has the unit mass, we may write
\vspace{-1mm}\begin{eqnarray}
    \gamma^*(y|x)=\exp\{\frac{\langle x, y\rangle}{\varepsilon}\} \frac{v^*(y)}{c_{v^*}(x)}
    \label{conditional-distrib}
\end{eqnarray}\vspace{-1mm}
where $c_{v^*}(x)\!\!\defeq\!\!\int_{\mathbb{R}^d} \exp\{\frac{\langle x, y\rangle}{\varepsilon}\} v^*(y) dy$ is the normalization costant ensuring that $\int_{\mathbb{R}^d} \gamma^* (y|x) dy = 1$. 

Consider the decomposition $\gamma^*(x, y) = \gamma_x^*(x) \gamma^*(y|x)$. It shows that to obtain parametrization for the entire plan $\gamma^*(x)$, it is sufficient to consider parametrizations for its left marginal $\gamma^*_x$ and the conditional measure $\gamma^*(y|x)$. 
Meanwhile, equation \eqref{conditional-distrib} shows that \textit{conditional measures $\gamma^*(\cdot| x)$ are entirely described by the variable $v^*$}. We use these observations to parametrize $\gamma_{\theta,w}$. We set
\vspace{-1mm}\begin{eqnarray}
    \gamma_{\theta, w}(x,y)\defeq u_w(x)\gamma_{\theta}(y|x)\!=\!
    u_w(x) \!\frac{\exp\{\nicefrac{\langle x, y\rangle}{\varepsilon}\}\! v_{\theta}(y)}{c_{\theta}(x)},\!
    \label{parametrization}
\end{eqnarray}\vspace{-1mm}
where $u_w$ and $v_{\theta}$ parametrize marginal measure $\gamma_x^*$ and the variable $v^*$, respectively. In turn, the constant $c_{\theta}(x)\defeq\int_{\mathbb{R}^d} \exp\{\frac{\langle x, y\rangle}{\varepsilon}\} v_{\theta}(y) dy$ is the parametrization of $c_{v^*}(x)$. Next, we demonstrate our \textbf{main result} which shows that the optimization of \eqref{main-objective} can be done \textit{without} the access to $\gamma^*$.

\begin{theorem}[Tractable form of $\text{D}_{\text{KL}}$ minimization] 
    Assume that $\gamma^*$ is parametrized using \eqref{parametrization}. Then the following bound holds: $\varepsilon\KL{\gamma^*}{\gamma_{\theta,w}}\!\leq\! \mathcal{L}(\theta,w)-\mathcal{L}^*,$  where
    \vspace{-1mm}\begin{eqnarray}
        \mathcal{L}(\theta,w)\!\defeq\!\! 
    \int_{\mathbb{R}^d}\!\! \overline{f}_1\! (- \varepsilon \log \frac{u_w(x)}{c_{\theta}(x)}\!-\!\frac{\|x\|^2}{2})p(x) dx \!+\!\! 
    \int_{\mathbb{R}^d}\! \overline{f}_2 (-\varepsilon \log v_{\theta}(y)\! -\!\frac{\|y\|^2}{2})q(y) dy \!+\! \varepsilon \|u_w\|_1,
    \label{tractable-objective}
    \end{eqnarray}%\vspace{-1mm}
    and constant $(-\mathcal{L}^*$) is the optimal value of the dual form \eqref{unbalanced-eot-dual}. The bound is \textbf{tight} in the sence that it turns to $0=0$ when $v_{\theta}(y)\!=\!\!\exp\{\nicefrac{2\psi^*(y)-\|y\|^2}{2\varepsilon}\}$ and $u_{\omega}(x)\!=\!\!\exp\{\nicefrac{2\phi^*(x)-\|x\|^2}{2\varepsilon}\}$.
    \label{thm-kl}
\end{theorem}\vspace{-2mm}
In fact, \eqref{tractable-objective} is the dual form \eqref{unbalanced-eot-dual} but with potentials $(\phi,\psi)$ expressed through $u_{\omega}, v_{\theta}$ (and $c_{\theta}$):
\begin{eqnarray*}
\phi(x)\!\leftarrow\!\phi_{\theta,\omega}(x)\!=\!\varepsilon\! \log\! \frac{u_w(x)}{c_{\theta}(x)}\!+\!\frac{\|x\|^2}{2},\;\;\;\psi(y)\!\leftarrow\! \psi_{\theta}(y)\!=\!\varepsilon \log v_{\theta}(y)\! +\!\frac{\|y\|^2}{2}.\!
\end{eqnarray*}
Our result can be interpreted as the bound on the quality of approximate solution $\gamma_{\theta,\omega}$ \eqref{unbalanced-eot-primal} recovered from the approximate solution to the dual problem \eqref{unbalanced-eot-dual}. It can be directly proved using the original $(\phi,\psi)$ notation of the dual problem, but we use $(u_{\omega},v_{\theta})$ instead as with this change of variables the form of the recovered plan $\gamma_{\theta,\omega}$ is more interpretable ($u_{w}$ defines the first marginal, $v_{\theta}$ -- conditionals).

 Instead of optimizing \eqref{main-objective} to get $\gamma_{\theta,\omega}$, we may optimize the upper bound \eqref{tractable-objective} which is more tractable. Indeed, \eqref{tractable-objective} is a sum of the expectations w.r.t. the probability measures $p,q$. We can obtain Monte-Carlo estimation of \eqref{tractable-objective} from random samples and optimize it with stochastic gradient descent procedure w.r.t. $(\theta, \omega)$. The main \textbf{challenge} here is the computation of the variable $c_{\theta}$ and term $\|u_{\omega}\|_1$. Below we propose a smart parametrization by which both variables can be derived analytically.


\vspace{-1mm}\subsection{Parameterization and the Optimization Procedure}

\vspace{-1mm}
\textbf{Paramaterization.} Recall that $u_w$ parametrizes the density of the marginal $\gamma^*_x$  which is unnormalized. Setting $x=0$ in equation \eqref{v-unnormalized}, we get $\gamma^*(y|x=0) \propto v^*(y)$ which means that $v^*$ also corresponds to unnormalized density of the measure.  These motivate us to use the unnormalized Gaussian mixture parametrization for the potential $v_{\theta}(y)$ and measure $u_w(x)$: 
\vspace{-2mm}
\begin{eqnarray}
    \label{gauss-parametrization}
    v_{\theta}(y)\!\!=\!\!\!\sum_{k=1}^{K}\!\! \alpha_k \mathcal{N}(y|r_k, \!\varepsilon S_k); \;\;
    u_{\omega}(x)\!\!=\!\!\!\sum_{l=1}^{L} \!\beta_l \mathcal{N}(x|\mu_l, \!\varepsilon \Sigma_l).\!
    \label{marginal-parametrization}
\end{eqnarray}\vspace{-4mm}

Here $\theta\defeq\{\alpha_k, r_k, S_k\}_{k=1}^{K}$, $w\defeq\{\beta_l, \mu_l, \Sigma_l\}_{l=1}^{L}$ with $\alpha_k,\beta_l\geq0$, $r_k, \mu_l\in\mathbb{R}^d$ 
and $0\prec S_k,\Sigma_l\in \mathbb{R}^{d\times d}$. The covariance matrices are scaled by $\varepsilon$ just for convenience.

For this type of parametrization, it holds that $\|u_w\|_1=\sum_{l=1}^L \beta_l.$ Moreover, there exist closed-from expressions for the normalization constant $c_{\theta}(x)$ and conditional plan $\gamma_{\theta}(y|x)$, see \citep[Proposition 3.2]{korotin2024light}. Specifically, define $r_k(x)\defeq r_k+S_k x$ and $\widetilde{\alpha}_{k}(x)\defeq \alpha_k \exp\{\frac{x^T S_k x +2 r_k^T x}{2 \varepsilon}\}$. It holds that
\vspace{-2mm}\begin{eqnarray}
    c_{\theta}(x) = \sum_{k=1}^K \widetilde{\alpha}_{k}(x);\;\;
    \gamma_{\theta}(y|x)=\frac{1}{c_{\theta}(x)} \sum_{k=1}^K \widetilde{\alpha}_{k}(x) \mathcal{N}(y|r_k(x), \varepsilon S_k).
    \label{gamma-gauss-parametrization}
\end{eqnarray}\vspace{-2mm}
Using this result and \eqref{gauss-parametrization}, we get the expression for $\gamma_{\theta, w}$:
\begin{eqnarray}
    \gamma_{\theta, w}(x,y)=u_{\omega}(x)\cdot \gamma_{\theta}(y|x)=
    \underbrace{\sum_{l=1}^{L} \!\beta_l \mathcal{N}(x|\mu_l, \!\varepsilon \Sigma_l)}_{u_{\omega}(x)} \cdot
    \underbrace{\frac{ \sum_{k=1}^K \widetilde{\alpha}_{k}(x) \mathcal{N}(y|r_k(x), \varepsilon S_k)}{\sum_{k=1}^K \widetilde{\alpha}_{k}(x)}}_{\gamma_{\theta}(y|x)}.
    \vspace{-2mm}
\end{eqnarray}


\label{sec-algorithm}
\textbf{Training.} We recall that the measures $p, q$ are accessible only by a number of empirical samples (see the learning setup in \wasyparagraph \ref{sec-background}). Thus, given samples $\{x_1,...,x_N\}$ and $\{y_1,...,y_M\}$, we optimize the empirical analogue of \eqref{tractable-objective}:
\begin{eqnarray}
   \widehat{\mathcal{L}}(\theta, w)\!\defeq\!
      \frac{1}{N}\sum_{i=1}^N \overline{f}_1 (- \varepsilon \log \frac{u_w(x_i)}{c_{\theta}(x_{i})}\!-\!\frac{\|x_i\|^2}{2}) \!+\!
    \frac{1}{M}\sum_{j=1}^M  \overline{f}_2 (-\varepsilon \log v_{\theta}(y_j) \!-\!\frac{\|y_j\|^2}{2})\! \!+\! \varepsilon \|u_w\|_1
    \label{empirical-objective}
\end{eqnarray}
using minibatch gradient descent procedure w.r.t. parameters $(\theta, w)$. In the parametrization of $v_{\theta}$ and $u_w$ \eqref{gauss-parametrization}, we utilize the diagonal matrices $S_k$, $\Sigma_l$. This allows for decreasing the number of learnable parameters in $\theta$ and speeding up the computation of $S_k^{-1},\;\Sigma_l^{-1}$. %For the parametrization of $u_w$, we also consider the diagonal matrices. 

\vspace{-1mm}\textbf{Inference.} Sampling from the conditional and marginal measures $\gamma_{\theta, w}(y|x)\!\approx\! \gamma^*(y|x)$, $u_w\!\approx\! \gamma_{x}^{*}$ is easy and lightweight since they are explicitly parametrized as Gaussian mixtures, see \eqref{gamma-gauss-parametrization}, \eqref{marginal-parametrization}.



\subsection{Connection to Related Prior Works}
\label{sec-prior-work}
The idea of using Gaussian Mixture parametrization for dual potentials in EOT-related tasks first appeared in the EOT/SB benchmark \citep{gushchin2023building}. There it was used to obtain the benchmark pairs of probability measures with the known EOT solution between them. In \citep{korotin2024light}, the authors utilized this type of parametrization to obtain a light solver \textbf{(LightSB)} for the \textbf{balanced} EOT.


Our solver for \textbf{unbalanced} EOT
\eqref{tractable-objective} subsumes their solver for balanced EOT as well as one problem subsumes the other for the special case of divergences, see the remark in \wasyparagraph\ref{sec-background}.  Let $f_1$, $f_2$ be convex indicators of $\{1\}$. Then $\overline{f_1}(t)=t$ and $\overline{f_2}(t)=t$ and objective \eqref{tractable-objective} becomes
\vspace{-2mm}\begin{eqnarray}
    \mathcal{L}(\theta,w)\!=\! 
    \int_{\mathbb{R}^d} \!(- \varepsilon \log \frac{u_w(x)}{c_{\theta}(x)}\!-\!\frac{\|x\|^2}{2})p(x) dx + 
    \int_{\mathbb{R}^d} (-\varepsilon \log v_{\theta}(y) \!-\!\frac{\|y\|^2}{2})q(y) dy \!+\! \varepsilon \|u_w\|_1=
    \nonumber\\
    - \varepsilon\Big(\!\int_{\mathbb{R}^d}\!\!\!\log \frac{u_w(x)}{c_{\theta}(x)}p(x)dx \!+\!\!\!
    \int_{\mathbb{R}^d} \!\!\log v_{\theta}(y)q(y)dy\!-\!
    \|u_w\|_1\!\Big)-\!\underbrace{\int_{\mathbb{R}^d}\!\!\frac{\|x\|^2}{2}p(x) dx \!-\!\! \int_{\mathbb{R}^d}\!\! \frac{\|y\|^2}{2}q(y) dy}_{\defeq C(p,q)}\!=\!
    \nonumber\\
    \varepsilon\big(\!\!\int_{\mathbb{R}^d}\log c_{\theta}(x)p(x)dx\big) \!-\!\!
    \int_{\mathbb{R}^d} \log v_{\theta}(y)q(y)dy\big)- 
    \label{lightsb-objective}
    \\
    \varepsilon \int_{\mathbb{R}^d} \log u_w(x) p(x)dx + \varepsilon \|u_w\|_1 - C(p,q).
    \label{marginal-objective}
\end{eqnarray}\vspace{-2mm}

Here line \eqref{lightsb-objective} depends exclusively on $\theta$ and exactly coincides with the LightSB's objective, see \citep[Proposition 8]{korotin2024light}. At the same time, line \eqref{marginal-objective} depends only on $\omega$, and its minimum is attained when $u_w=p$. Thus, this part is not actually needed in the balanced case, see \citep[Appendix C]{korotin2024light}.


\vspace{-1mm}
\subsection{Generalization Bounds and Universal Approximation Property}
\vspace{-1mm}

\label{sec-generalization-bounds}

Theoretically, to recover the UEOT plan $\gamma^*$, one needs to solve the problem $\mathcal{L}(\theta,\omega)\rightarrow \min_{\theta,\omega}$ as stated in our Theorem \ref{thm-kl}. In practice, the measures $p$ and $q$ are accessible via empirical samples $\{x_1,...,x_N\}$ and $\{y_1,...,y_M\}$, thus, one needs to optimize the empirical counterpart $\widehat{\mathcal{L}}(\theta,\omega)$ of $\mathcal{L}(\theta,\omega)$, see \eqref{empirical-objective}. Besides, the available potentials $u_{\omega}$, $v_{\theta}$ over which one optimizes the objective come from the restricted class of functions. Specifically, we consider unnormalized Gaussian mixtures $u_{\omega}$, $v_{\theta}$ with $K$ and $L$ components respectively. 
Thus, one may naturally wonder: \textbf{how close is the recovered plan $\gamma_{\widehat{\theta}, \widehat{\omega}}$ to the UEOT plan $\gamma^*$} given that $(\widehat{\theta},\widehat{\omega})=\arg\min_{\theta,\omega}\widehat{\mathcal{L}}(\theta,\omega)$?
To address this question, we study the \textit{generalization error} $\mathbb{E}\KL{\gamma^*}{\gamma_{\widehat{\theta},\widehat{\omega}}}$, i.e., the expectation of $\text{D}_{\text{KL}}$ between $\gamma^*$ and $\gamma_{\widehat{\theta},\widehat{\omega}}$ 
taken w.r.t. the random realization of the train datasets $X\sim p$, $Y\sim q$. 

Let $(\overline{\theta}, \overline{\omega})\!=\!\arg\min_{(\theta,\omega)\in \Theta\times \Omega} \mathcal{L}(\theta,\omega)$ denote the best approximators of $\mathcal{L}(\theta,\omega)$ in the admissible class. From Theorem \ref{thm-kl} it follows that that the generalization error $\mathbb{E}\KL{\gamma^*}{\gamma_{\widehat{\theta},\widehat{\omega}}}$
can be bounded by ${\mathbb{E}(\mathcal{L}(\widehat{\theta},\widehat{\omega}) - \mathcal{L}^*)}$. The latter can be decomposed into the estimation and approximation errors:
\begin{eqnarray}
    \mathbb{E}(\mathcal{L}(\widehat{\theta},\widehat{\omega}) \!-\!\! \mathcal{L}^*)\!=\!
    \mathbb{E}[\mathcal{L}(\widehat{\theta},\widehat{\omega}) \!-\!\! \mathcal{L}(\overline{\theta},\overline{\omega})] \!+\! \mathbb{E}[\mathcal{L}(\overline{\theta},\overline{\omega}) \!-\!\! \mathcal{L}^*]\!=\!
    % \nonumber\\
    \underbrace{\mathbb{E}[\mathcal{L}(\widehat{\theta},\widehat{\omega}) \!-\!\! \mathcal{L}(\overline{\theta},\overline{\omega})]}_{\text{Estimation error}} \!+\! \underbrace{[\mathcal{L}(\overline{\theta},\overline{\omega})\!-\!\! \mathcal{L}^*]}_{{\text{Approximation error}}}.
\end{eqnarray}

We establish the quantitative bound for the estimation error in the proposition below.

 \begin{proposition}[Bound for the estimation error] Let $p,q$ be compactly supported and assume that $\overline{f}_1,\;\overline{f}_2$ are Lipshitz. Assume that the considered parametric classes $\Theta$, $\Omega$ $(\ni (\theta, \omega))$ consist of unnormalized Gaussian mixtures with $K$ and $L$ components respectively with bounded means $\|r_{k}\|,\|\mu_l\|\leq R$ (for some $R>0$), covariances $sI\preceq S_{k},\Sigma_l\preceq SI$ (for some $0<s\leq S$) and weights $a\leq \alpha_{k},\beta_l\leq A$ (for some $0<a\leq A$). Then the following holds:
$$\mathbb{E}\big[\mathcal{L}(\widehat{\theta}, \widehat{\omega})-\mathcal{L}(\overline{\theta}, \overline{\omega})\big]\leq O(\frac{1}{\sqrt{N}})+O(\frac{1}{\sqrt{M}}),$$
where $O(\cdot)$ hides the constants depending only on $K,L,R,s,S,a,A,p,q, \varepsilon$ but not on sizes $M,N$.
\label{prop-estimation}
\end{proposition}

This proposition allows us to conclude that the estimation error converges to zero (when $N$ and $M$ tend to infinity) at the usual Monte-Carlo rate. It remains to clarify the question: \textit{can we make the approximation error arbitrarily small}? We answer this question positively in our Theorem below.

\begin{theorem}[Gaussian mixture parameterization for the variables provides the universal approximation of UEOT plans] Let $p$ and $q$ be compactly supported. Then for all $\delta>0$ there exist Gaussian mixtures $v_{\theta}$, $u_{\omega}$ \eqref{gauss-parametrization} with \textbf{scalar} covariances $S_{k}=\lambda_{k}I_{d}\succ 0$, $\Sigma_{l}=\zeta_{l}I_{d}\succ 0$ of their components that satisfy the inequality $\KL{\gamma^{*}}{\gamma_{\theta,\omega}}\leq \varepsilon^{-1}(\mathcal{L(\theta,\omega})-\mathcal{L}^*)< \delta.$
\label{thm-universal-approximation}
\end{theorem}


\textbf{In summary}, results of this section show that one can make the generalization error \textit{arbitrarily small} given a sufficiently large amount of samples and components in the Gaussian parametrization. It means that theoretically our solver can recover the UEOT plan arbitrarily well.

\vspace{-2mm}
\section{Experiments}
\vspace{-2mm}
\label{sec-experiments}
\begin{figure*}[t!]
\begin{subfigure}[t]{\linewidth}
\begin{center}
    \vspace{-2mm}
    \centering
    \includegraphics[width=0.5\textwidth]{pics/toy_legend_blank.png}
\end{center}
\end{subfigure}
\centering
\begin{subfigure}[b]{0.235\linewidth}
    \centering
    \includegraphics[width=0.995\linewidth]{pics/toy_input.png}
    \caption{Input, target measures}
    \label{fig:gauss-input}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.235\linewidth}
    \centering
    \includegraphics[width=0.995\linewidth]{pics/toy_100.png}
    \caption{U-LightOT, $\tau=10^2$}
    \label{fig:gauss-lightsb}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.235\linewidth}
    \centering
    \includegraphics[width=0.995\linewidth]{pics/toy_10.png}
    \caption{U-LightOT, $\tau=10^1$}
    \label{fig:gauss-mix}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.235\linewidth}
    \centering
    \includegraphics[width=0.995\linewidth]{pics/toy_unb.png}
    \caption{U-LightOT, $\tau=10^0$}
    \label{fig:gauss-ulightsb}
\end{subfigure}
\vspace{-1mm}\caption{\centering Conditional plans $\gamma_{\theta,\omega}(y|x)$ learned by our solver in \textit{Gaussians Mixture} experiment with unbalancedness parameter $\tau\in[10^0,10^1,10^2]$. Here $p_{\omega}$ denotes the normalized first marginal $u_{w}$, i.e., $p_{\omega}=u_{\omega}/\|u_{\omega}\|_1$. }
\label{fig:gauss-kl}
\vspace{-3mm}
\end{figure*}


In this section, we test our U-LightOT solver on several setups from the related works.
The code is written using \texttt{PyTorch} framework and will be made public. The experiments are issued in the convenient form of \texttt{*.ipynb} notebooks. Each experiment requires several minutes of training on CPU with 4 cores. Technical \textit{\underline{training details}} are given in Appendix \ref{sec:exp-details}. 

\vspace{-1mm}
\subsection{Example with the Mixture of Gaussians}
\vspace{-1mm}
\label{sec-gaussian-exp}


 We provide an illustrative \textit{'Gaussians Mixture'} example in 2D to demonstrate the ability of our solver to deal with the imbalance of classes in the source and target measures. We follow the experimental setup proposed in \citep[Figure 2]{eyring2023unbalancedness} and define the probability measures $p$, $q$ as follows (Fig. \ref{fig:gauss-input}:
$$p(x)\!=\!\frac{1}{4} \mathcal{N}(x|(-2, 3), 0.1\cdot I_2)\!+\!\frac{3}{4} \mathcal{N}(x|(1, 3), 0.1 \cdot I_2),$$
$$q(y)\!=\!\frac{3}{4} \mathcal{N}(y|(-2,0), 0.1 \cdot I_2)\!+\!\frac{1}{4} \mathcal{N}(y|(1,0), 0.1 \cdot I_2).$$
 
We test our U-LightOT solver with \textit{scaled} $\text{D}_{\text{KL}}$ divergences, i.e., $f_1(t), f_2(t)$ are defined by $\tau \cdot f_{\text{KL}}(t)= \tau(t\log(t) - t + 1)$ where $\tau>0$. We provide the learned plans for $\tau\in[1,\;10^2,\;10^3]$. The results show that parameter $\tau$ can be used to control the level of unbalancedness of the learned plans. For $\tau=1$, our U-LightOT solver truly learns the UEOT plans, see Fig. \ref{fig:gauss-ulightsb}. When $\tau$ increases, the solver fails to transport the mass from the input Gaussians to the closest target ones. Actually, for $\tau=10^3$, our solutions are similar to the solutions of the balanced solver  \citep[LightSB]{korotin2024light} which approximates EOT plans between the measures. Hereafter, we treat $\tau$ as an \textit{unbalancedness} parameter. In Appendix \ref{app-ablation}, we test the performance of our solver with \underline{$\text{D}_{\chi^{2}}$ divergence.}

\textit{Remark.} We conduct all experiments here with the entropy regularization parameter $\varepsilon=0.05$. The parameter $\varepsilon$ is responsible for the stochasticity of the learned transport $\gamma_{\theta}(\cdot|x)$. Since we are mostly interested in the correct transport of the mass (controlled by $f_1,f_2$) rather than the stochasticity, we do not pay much attention to $\varepsilon$ throughout the paper.


\subsection{Unpaired Image-to-Image Translation}
\label{sec-alae-exp}

\begin{table}[t!]
    \centering
    % \small
    \footnotesize
    % \begin{sc}
    \begin{tabular}{ c|c c c c c c c c} 
    \hline
    \textbf{Experiment} &  \makecell{OT-FM\\\cite{eyring2023unbalancedness}} & \makecell{\citep[LightSB]{korotin2024light} \textit{or}\\\citep[LightSBM]{gushchin2024light}} & \citep[UOT-FM]{eyring2023unbalancedness} &  \makecell{UOT-SD \\\citep{choi2023generative}} & \makecell{UOT-GAN\\\cite{yang2018scalable}} &\makecell{U-LightOT \\(\textbf{ours})}  \\
    \hline
     \textit{Young}$\rightarrow$\textit{Adult}  & 60.71    &87.11   &\underline{87.72} &46.02  &85.95  & \textbf{94.63}   \\  
     \textit{Adult}$\rightarrow$\textit{Young}  & 56.75    &76.68   &81.34 &54.27  &\underline{88.37} &\textbf{94.36}    \\ \hline
     \textit{Man}$\rightarrow$\textit{Woman}  & 73.82    &81.33   &\underline{87.35}  &72.53  &83.79 &\textbf{92.85}   \\ 
     \textit{Woman}$\rightarrow$\textit{Man}  &77.56    &83.29   &88.20  &75.03 &\underline{89.32} &\textbf{95.13}    \\  \hline
    \end{tabular}
     % \end{sc}
     \vspace{1mm}
     \caption{
     \centering Comparison of accuracies of keeping the attributes of the source images. \linebreak The best results are in \textbf{bold}, second best are \underline{underlined}.}
     \label{table-alae-accuracy-keep}
     \vspace{-3mm}
\end{table}


\vspace{-1mm}
Another popular testbed which is usually considered in OT/EOT papers is the unpaired image-to-image translation \citep{zhu2017toward} task. Since our solver uses the parametrization based on Gaussian mixture, it may be hard to apply U-LightOT for learning translation directly in the image space. Fortunately, nowadays it is common to use autoencoders \citep{rombach2022high} for more efficient translation. %in the latent spaces of generative models. 
% \textbf{Experimental setup.} 
We follow the setup of \citep[Section 5.4]{korotin2024light} and use pre-trained ALAE autoencoder \citep{pidhorskyi2020adversarial} for $1024\times1024$ FFHQ dataset \citep{karras2019style} of human faces. We consider different subsets of FFHQ dataset (\textit{Adult}, \textit{Young}, \textit{Man}, \textit{Woman}) and all variants of translations between them: \textit{Adult}$\leftrightarrow$\textit{Young} and \textit{Woman}$\leftrightarrow$\textit{Man}.
% We split the data on the train and test part following \citep{korotin2024light} and use their $512$-dimensional latent codes extracted for images in each domain.

\begin{wraptable}{r}{4.5cm}
    \vspace{-4mm}
    \centering
    \footnotesize
    \begin{tabular}{ c|c c  } 
    \hline
    \textbf{Class} & \textit{Man} & \textit{Woman}  \\
    \hline
     \textit{Young}  & $15K$    &$23K$     \\  \hline
     \textit{Adult}  &$7K$    &$3.5K$     \\ \hline
    \end{tabular}
    \captionsetup{justification=centering}
    \caption{Number of \textit{train} FFHQ images for each subset.}
     \label{table-classes}
     \vspace{-4mm}
\end{wraptable}

The main challenge of the described translations is the \textbf{imbalance} of classes in the images from source and target subsets, see Table \ref{table-classes}. Let us consider in more detail \textit{Adult}$\rightarrow$\textit{Young} translation. In the FFHQ dataset, the amount of \textit{adult women} significantly outnumbers the \textit{adult men}, while the amount of \textit{young women} is smaller than that of \textit{young men}. Thus, balanced OT/EOT solvers are expected to translate some of the \textit{young woman} representatives to the \textit{adult man} ones. At the same time, solvers based on unbalanced transport are supposed to alleviate this issue.

% \vspace{-1mm}
\textbf{Baselines.} We perform a comparison with the recent procedure \citep[UOT-FM]{eyring2023unbalancedness} which considers roughly the same setup and demonstrates good performance. This method interpolates the results of unbalanced discrete OT to the continuous setup using the flow matching~\cite{lipman2022flow}. For completeness, we include the comparison with LightSB and balanced optimal transport-based flow matching (OT-FM)~\cite{lipman2022flow} to demonstrate the issues of the balanced solvers. We also considered neural network based solvers such as UOT-SD\citep{choi2023generative} and UOT-GAN\cite{yang2018scalable}. 

% \vspace{-1mm}
\textbf{Metrics.} We aim to assess the ability of the solvers to perform the translation of latent codes keeping the class of the input images unchanged, e.g., keeping the gender in \textit{Adult}$\rightarrow$\textit{Young} translation. Thus, we train a $99\%$ MLP classifier on the latent codes of images of different classes and compute the accuracy between the classes of the source images and those of the generated ones.

\textbf{Results.} Our evaluation shows that U-LightOT solver can effectively solve translation tasks in high dimensions ($d=512$) and outperforms its alternatives in dealing with class imbalance issues. In Table \ref{table-alae-accuracy-keep}, we quantitatively demonstrate this effect. Namely, we show that our solver achieves the best accuracy of keeping the class of source images. Qualitative examples are given in Fig. \ref{fig:alae}. 
In Appendix \ref{app-add-results}, we give \textit{\underline{additional qualitative results}}. Additionally, in Appendix \ref{app-ablation}, we perform the \textit{\underline{ablation study}} of our solver focusing on the selection of parameters $\tau$, $\varepsilon$.

% \newpage

\begin{figure*}[ht!]
\begin{center}
\begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=0.995\linewidth]{pics/Fig_YOUNG_TO_ADULT_main.png}
    \caption{\textit{Young} $\rightarrow$  \textit{Adult}}
    \label{fig:alae-y2a}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=0.995\linewidth]{pics/Fig_ADULT_TO_YOUNG_main.png}
    \caption{\textit{Adult} $\rightarrow$  \textit{Young}}
    \label{fig:alae-a2y}
\end{subfigure}
\hfill
\vspace{3mm}\newline 
\begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=0.995\linewidth]{pics/Fig_MAN_TO_WOMAN_main.png}
    \caption{\textit{Man} $\rightarrow$  \textit{Woman}}
    \label{fig:alae-m2w}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=0.995\linewidth]{pics/Fig_WOMAN_TO_MAN_main.png}
    \caption{\textit{Woman} $\rightarrow$  \textit{Man}}
    \label{fig:alae-w2m}
\end{subfigure}
\vspace{-1mm}\caption{Unpaired translation with LightSB, OT-FM, UOT-FM and our U-LightOT solver applied in the latent space of ALAE for FFHQ images (1024$\times$1024).}
\label{fig:alae}
\end{center}
\vspace{-7mm}
\end{figure*}

\section{Discussion}

\vspace{-2mm}
\textbf{Potential impact.} Our light and unbalanced solver has a lot of advantages in comparison with the other existing UEOT solvers. First, it does not require complex max-min optimization. Second, it provides the closed form of the conditional measures $\gamma_{\theta,\omega}(y|x)\approx \gamma^*(y|x)$ of the UEOT plan. Moreover, it allows for sampling both from the conditional measure $\gamma_{\theta,\omega}(y|x)$ and marginal measure $u_w(x)\approx \gamma_x^*(x)$. Besides, the decisive superiority of our lightweight and unbalanced solver is its simplicity and convenience of use. Indeed, it has a straightforward and non-minimax optimization objective and avoids heavy neural parametrization. As a result, our lightweight and unbalanced solver converges in minutes on CPU. We expect that these advantages could boost the usage of our solver as a standard and easy baseline for UEOT task with applications in different spheres.

The \underline{limitations and broader impact} of our solver are discussed in Appendix \ref{app-limitations}.

\bibliography{bibliography}
\bibliographystyle{abbrvnat}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Proofs}
\label{sec-proofs}

\subsection{Proof of Theorem \ref{thm-kl}}
\begin{proof}[Proof] 
To begin with, we derive the useful property of the optimal UEOT plans.
We recall that $\phi^*$ and $\psi^*$ are the minimizers of dual form \eqref{unbalanced-eot-dual}, i.e., $(\phi^*,\psi^*)\in\argmin_{(\phi,\psi)} \mathcal{J}(\phi,\psi)$ where 
\begin{eqnarray}
\mathcal{J}(\phi,\psi)\!\defeq \! \nonumber\\
\varepsilon \int_{\mathbb{R}^d} \!\!\int_{\mathbb{R}^d}\! \!\!\exp\{\frac{1}{\varepsilon} \!( \phi(x) + \psi(y)- \frac{\|x-y\|^2}{2}) \} dx dy + \!\int_{\mathbb{R}^d} \!\!\overline{f}_1 (-\phi(x))p(x) dx  + \!\int_{\mathbb{R}^d} \!\!\overline{f}_2 (-\psi(y))q(y) dy.
\label{def-alternative-obj}
\end{eqnarray}
Then, by the first order optimality condition $\frac{d}{d\phi}\mathcal{J}(\phi,\psi)|_{\phi=\phi^*}=0$ and $\frac{d}{d\psi}\mathcal{J}(\phi,\psi)|_{\psi=\psi^*}\!=\!0$. It means that
\begin{eqnarray}
    \frac{d}{d\phi}\mathcal{J}(\phi,\psi)|_{\phi=\phi^*}(x)= \cancel{\varepsilon \cdot\varepsilon^{-1}} \int_{\mathbb{R}^d} \exp\{\varepsilon^{-1}(\phi^*(x)+\psi^*(y)-\nicefrac{\|x-y\|^2}{2})\} dy -\nabla \overline{f}_1 (-\phi^*(x)) p(x)=0 \Longrightarrow \nonumber
    \\
    \nabla \overline{f}_1 (-\phi^*(x))=\frac{\gamma^*_x(x)}{p(x)}
\end{eqnarray}
holds for all $x$, s.t. $p(x)>0$. Analogously, we get that $\nabla \overline{f}_2 (-\psi^*(y))=\frac{\gamma^*_y(y)}{q(y)}$ for all $y$, s.t. $q(y)>0$.

For convenience of the further derivations, we consider the change of variables to simplify the expression for $\mathcal{L}(\theta,\omega)$. We define $\phi_{\theta,\omega}(x)\defeq \varepsilon \log \frac{u_{\omega}(x)}{c_{\theta}(x)} {\color{black}+} \frac{\|x\|^2}{2}$ and $\psi_{\theta}(y)\defeq \varepsilon \log v_{\theta}(y){\color{black}+} \frac{\|y\|^2}{2}$. Then
\begin{eqnarray*}
    \mathcal{L}(\theta,\omega) = \int_{\mathbb{R}^d} \overline{f}_1 (-\phi_{\theta,\omega}(x))p(x) dx +
    \int_{\mathbb{R}^d} \overline{f}_2 (-\psi_{\theta}(y))q(y) dy + \varepsilon \|u_w\|_1=\mathcal{J}(\phi_{\theta,\omega},\psi_{\theta}).
    \label{kl-functional-before}
\end{eqnarray*}

Next, we simplify our main objective:
\begin{eqnarray}
    \KL{\gamma^{*}}{\gamma_{\theta,\omega}}\!=\!\|\gamma^*\|_1\bigg[\KL{\frac{\gamma^*}{\|\gamma^*\|_1}}{\frac{\gamma_{\theta,\omega}}{\|\gamma_{\theta,\omega}|_1}}+\frac{\|\gamma_{\theta,\omega}\|_1}{\|\gamma^*\|_1}\!-\!1\!-\!\log\frac{\|\gamma_{\theta,\omega}\|_1}{\|\gamma^*\|}\bigg]\!=\! 
    \nonumber
    \\
    \!\!\cancel{\|\gamma^*\|_1\! \cdot \!\|\gamma^*\|_1^{-1}}  \Bigg(\!\!\int_{\mathbb{R}^d\times\mathbb{R}^d}\!\!\! \gamma^*(x,y)\log \gamma^*(x,y) dx dy\! -
    \nonumber
    \\
    \!\!\!\int_{\mathbb{R}^d\times\mathbb{R}^d} \!\!\gamma^*(x,y) \log \|\gamma^*\|_1 dx dy \!-\!\!\int_{\mathbb{R}^d\times\mathbb{R}^d} \!\!\!\gamma^*(x,y) \log \gamma_{\theta, \omega}(x,y) dx dy\! + 
    \nonumber
    \\
    \int_{\mathbb{R}^d\times\mathbb{R}^d} \gamma^*(x,y) \log \|\gamma_{\theta,\omega}\|_1 dx dy + \|\gamma_{\theta,\omega}\|_1 - \|\gamma^*\|_1 -
    \nonumber 
    \\
    \|\gamma^*\|_1 \log\frac{\|\gamma_{\theta,\omega}\|_1}{\|\gamma^*\|_1}\Bigg)= \int_{\mathbb{R}^d\times\mathbb{R}^d} \gamma^*(x,y)\log \gamma^*(x,y) dx dy -\! 
    \nonumber
    \\
    \cancel{\|\gamma^*\|_1 \log \|\gamma^*\|_1} \! - \int_{\mathbb{R}^d\times\mathbb{R}^d} \!\!\!\gamma^*(x,y) \log \gamma_{\theta, \omega}(x,y) dx dy + \cancel{\|\gamma^*\|_1 \log \|\gamma_{\theta,\omega}\|_1}\! +
    \nonumber
    \\ \!\|\gamma_{\theta,\omega}\|_1 - \|\gamma^*\|_1 - \cancel{\|\gamma^*\|_1\log\|\gamma_{\theta,\omega}\|_1} + 
    \nonumber
    \\
    \cancel{\|\gamma^*\|_1\log\|\gamma^*\|_1}=\int_{\mathbb{R}^d\times\mathbb{R}^d} \gamma^*(x,y)\log \gamma^*(x,y) dx dy - 
    \nonumber
    \\
    \int_{\mathbb{R}^d\times\mathbb{R}^d} \!\!\!\gamma^*(x,y) \log \gamma_{\theta, \omega}(x,y) dx dy + \!\|\gamma_{\theta,\omega}\|_1 - \|\gamma^*\|_1.
    \label{kl-derivation-bgt}
\end{eqnarray}
Now we recall that the ground-truth UEOT plan $\gamma^*(x,y)$ and the optimal dual variables $\phi^*(x)$, $\psi^*(y)$ are connected via equation \eqref{gamma-potentials}. Similarly, our parametrized plan $\gamma_{\theta,\omega}(x,y)$ can be expressed using $\phi_{\theta,\omega}(x)$, $\psi_{\theta}(y)$ as $\gamma_{\theta,\omega}(x,y) = \exp\{\varepsilon^{-1}(\phi_{\theta,\omega}(x)+\psi_{\theta}(y)-\nicefrac{\|x-y\|^2}{2})\}$.
Then
\begin{eqnarray}
    \!\!\!\eqref{kl-derivation-bgt}=\varepsilon^{-1}\!\int_{\mathbb{R}^d\times\mathbb{R}^d}\!\! \gamma^*(x,y)\big(\phi^*(x)\!+\!\psi^*(y)\!-\!\nicefrac{\|x-y\|^2}{2}\big) dx dy \!-
    \nonumber
    \\
    \varepsilon^{-1}\!\int_{\mathbb{R}^d\times\mathbb{R}^d} \!\!\gamma^*(x,y) \big(\phi_{\theta,\omega}(x)\!+\!\psi_{\theta}(y)\!-\!\nicefrac{\|x-y\|^2}{2}\big)dxdy\!+
    \|\gamma_{\theta,\omega}\|_1 - \|\gamma^*\|_1 = 
    \nonumber
    \\
    \varepsilon^{-1} \int_{\mathbb{R}^d\times\mathbb{R}^d} \gamma^*(x,y) \big(\phi^*(x)+\psi^*(y) - \phi_{\theta,\omega}(x)-\psi_{\theta}(y) \big)dxdy  + \|\gamma_{\theta,\omega}\|_1 - \|\gamma^*\|_1=
    \nonumber\\
    \varepsilon^{-1} \int_{\mathbb{R}^d} \gamma^*_x(x) \big(\phi^*(x) - \phi_{\theta,\omega}(x) \big)dx  + \varepsilon^{-1} \int_{\mathbb{R}^d} \gamma^*_y(y) \big(\psi^*(y) - \psi_{\theta}(y)\big)dy + \|\gamma_{\theta,\omega}\|_1- \|\gamma^*\|_1.
    \label{kl-derivation-bc}
\end{eqnarray}
 
Finally, we derive
\begin{eqnarray}
    \!\!\eqref{kl-derivation-bc}=\varepsilon^{-1} \!\int_{\mathbb{R}^d} \!\!\!\!\!\!\underbrace{\frac{\gamma^*_x(x)}{p(x)}}_{\nabla \overline{f}_1 (-\phi^*(x))}\!\!\!\!\! \big(\phi^*(x) \!-\! \phi_{\theta,\omega}(x) \big)p(x)dx  \!+
    \nonumber
    \\
    \! \varepsilon^{-1} \!\!\int_{\mathbb{R}^d} \!\!\!\underbrace{\frac{\gamma^*_y(y)}{q(y)}}_{\nabla \overline{f}_2 (-\psi^*(y))}\!\!\! \big(\psi^*(y) \!-\! \psi_{\theta}(y)\big)q(y)dy \!+\! \|\gamma_{\theta,\omega}\|_1\!-\! \|\gamma^*\|_1\!=
    \nonumber\\
    \varepsilon^{-1}\!\!\int_{\mathbb{R}^d} \!\!\nabla \overline{f}_1 (-\phi^*(x)) \big(\phi^*(x) \!-\! \phi_{\theta,\omega}(x) \big)p(x)dx  \!+
    \nonumber
    \\
    \! \varepsilon^{-1} \int_{\mathbb{R}^d} \!\!\nabla \overline{f}_2 (-\psi^*(y)) \big(\psi^*(y) \!-\! \psi_{\theta}(y)\big)q(y)dy \!+\! \|\gamma_{\theta,\omega}\|_1\!-\! \|\gamma^*\|_1\!\leq
    \label{proof-inequality}
    \\
    \varepsilon^{-1} \!\!\int_{\mathbb{R}^d} \!\!( \overline{f}_1(-\phi_{\theta,\omega}{(x)}) \!-\! \overline{f}_1(-\phi^*{(x)})) p(x)dx  \!+
    \nonumber
    \\
    \! \varepsilon^{-1}\!\! \int_{\mathbb{R}^d}\! ( \overline{f}_2(-\psi_{\theta}{(y)}) \!-\! \overline{f}_2(-\psi^*{(y)}))q(y)dy \!+\! \|\gamma_{\theta,\omega}\|_1\!-\! \|\gamma^*\|_1\!=\! 
    \nonumber
    \\
    \varepsilon^{-1}\! \cdot \Big(\!\int_{\mathbb{R}^d} \!\!\! \overline{f}_1(-\phi_{\theta,\omega}{(x)}) p(x)dx \!\!+\!\!\!  \int_{\mathbb{R}^d} \! \!\overline{f}_2(-\psi_{{\theta}}{(y)}) q(y)dy \!+\! \varepsilon \|\gamma_{\theta,\omega}\|_1\!\!-
    \nonumber
    \\
    \! 
    \underbrace{(\!\int_{\mathbb{R}^d}\!\!\overline{f}_1(\!-\!\phi^*{(x)}) p(x)dx \!  +\!\!\! \int_{\mathbb{R}^d}\!\!\overline{f}_2(\!-\!\psi^*{(y)})q(y)dy \!+\! \varepsilon \|\gamma^*\|_1)\!}_{\mathcal{L}^*\defeq}\!\!\Big) \!\!=
    \nonumber\\
    \varepsilon^{-1} (\mathcal{J}(\phi_{\theta,\omega},\psi_{\omega})-\mathcal{L}^*)= \varepsilon^{-1} (\mathcal{L}(\theta,\omega)-\mathcal{L}^*).
    \nonumber
\end{eqnarray}
The inequality \eqref{proof-inequality} follows from the convexity of the functions $\overline{f_1}$ and $\overline{f_2}$.
\end{proof}

\subsection{Proof of Proposition \ref{prop-estimation}}
We begin with proving the auxiliary theoretical results (Propositions \ref{prop-rademacher-bound}, \ref{prop-rademacher-complexity}) which are needed to prove the main proposition of this section.

\begin{proposition}[Rademacher bound on the estimation error]
\label{prop-rademacher-bound}
It holds that
    $$\mathbb{E}\big[\mathcal{L}(\widehat{\theta})-\mathcal{L}(\overline{\theta})\big]\leq 4\mathcal{R}_{N}(\mathcal{F}_1, p)+4\mathcal{R}_{M}(\mathcal{F}_2, q),$$
    where $\mathcal{F}_1=\{\overline{f}_1 (- \phi_{\theta,\omega}) |(\theta,\omega)\in\Theta\times\Omega\}$, $\mathcal{F}_2=\{\overline{f}_2 (- \psi_{\theta}) |\theta\in\Theta\}$ for $\phi_{\theta,\omega}(x)=\varepsilon \log \frac{u_w(x)}{c_{\theta}(x)}+\frac{\|x\|^2}{2}$, $\psi_{\theta}(y)=\varepsilon \log v_{\theta}(y)+\frac{\|y\|^2}{2}$,
    and $\mathcal{R}_{N}(\mathcal{F}_1,p)$, $\mathcal{R}_{M}(\mathcal{F}_2,q)$ denote the Rademacher complexity \citep[\wasyparagraph 26]{shalev2014understanding} of the functional classes $\mathcal{U},\;\mathcal{V}$ w.r.t. to the sample sizes $N$, $M$ of distributions $p$, $q$.
    \label{estimation-through-rademacher}
\end{proposition}
\begin{proof}[Proof of Proposition \ref{estimation-through-rademacher}]
    The derivation of this fact is absolutely analogous to \citep[Proposition H.2]{korotin2024light}, \citep[Theorem 4]{mokrov2024energy} or \citep[Theorem 3.4]{taghvaei20192}.
\end{proof}

\begin{proposition}[Bound on the Rademacher complexity of the considered classes]
\label{prop-rademacher-complexity}
    Let $0<a\leq A$, let $0<u\leq U$, let $0<w\leq W$ and $V>0$. Consider the class of functions 
    \begin{gather}\mathcal{F}_1=\big\{x\mapsto \overline{f}_1 (- \varepsilon \log u_w(x)+ \varepsilon \log c_{\theta}(x) -\frac{\|x\|^2}{2})\big\},\nonumber\\ 
    \mathcal{F}_2=\big\{y\mapsto \overline{f}_2 (- \varepsilon \log v_{\theta}(y) -\frac{\|y\|^2}{2})\big\} \text{ where}\nonumber\\
    u_{\omega}(x),\; v_{\theta}(y),\; c_{\theta}(x) \text{ belong to the class} \nonumber\\
    \mathcal{V}=\big\{x\mapsto  \sum_{k=1}^{K}\alpha_{k}\exp\big(x^{T}U_{k}x+v_{k}^{T}x+w_{k})\text{ with }\\ uI\preceq U_{k}=U_{k}^{T}\preceq UI; \|v_{k}\|\leq V; w\leq w_{k}\leq W; a\leq \alpha_{k}\leq A \big\}.\nonumber
    \end{gather}
     % are the log-sum-exp quadratic functions satisfying the conditions of 
    Following \citep[Proposition H.3]{korotin2024light}, we call the functions of the class $\mathcal{V}$ as constrained log-sum-exp quadratic functions. We assume that $\overline{f}_1,\;\overline{f}_2$ are Lipshitz functions and measures $p$, $q$ are compactly supported with the supports lying in a zero-centered ball of a radius $R>0$. Then
    $$\mathcal{R}_{N}(\mathcal{F}_1,p)\leq \frac{C_0}{\sqrt{N}},\;\mathcal{R}_{M}(\mathcal{F}_2,q)\leq \frac{C_1}{\sqrt{M}}$$
    where the constants $C_0$, $C_1$ \textbf{do not depend} on sizes $N$, $M$ of the empirical samples from $p,\;q$.
    \label{proposition-rademacher-complexity}
\end{proposition}

\begin{proof}[Proof of Proposition \ref{proposition-rademacher-complexity}]
    Thanks to \citep[Proposition H.3]{korotin2024light}, the Rademacher complexities of constrained log-sum-exp quadratic functions $x\mapsto \log u_{\omega}(x)$, $x\mapsto \log  c_{\theta}(x)$ and $y\mapsto  \log v_{\theta}(y)$ are known to be bounded by $O(\frac{1}{\sqrt{N}})$ or $O(\frac{1}{\sqrt{M}})$ respectively. According to the definition of Rademacher complexity, for single quadratic functions $x\mapsto \frac{x^T x}{2}$ ($y\mapsto \frac{y^T y}{2}$) it is just equal to zero. %also bounded by $O(\frac{1}{\sqrt{N}})$ ($O(\frac{1}{\sqrt{M}})$) which, e.g., %follows from their representation using the Reproducing Kernel Hilbert spaces (RKHS), see \citep[Lemma 5 \& Eq. 24]{latorre2021effect}. 
    Then, using the well-known scaling and additivity properties of the Rademacher complexity \citep{shalev2014understanding}, we get that $x\mapsto - \varepsilon \log u_w(x)+ \varepsilon \log c_{\theta}(x) -\frac{\|x\|^2}{2}$ and $y\mapsto - \varepsilon \log v_{\theta}(y) -\frac{\|y\|^2}{2}$ are bounded by $O(\frac{1}{\sqrt{N}})$ and $O(\frac{1}{\sqrt{M}})$ respectively. The remaining step is to
    recall that $\overline{f}_1(x)$ and $\overline{f}_2(y)$ are Lipschitz. Therefore, according to Talagrand's contraction principle \citep{mohri2018foundations}, the Rademaher complexities of $\mathcal{F}_{1}$ and $\mathcal{F}_{2}$ are also bounded by $O(\frac{1}{\sqrt{N}})$ and $O(\frac{1}{\sqrt{M}})$, respectively.
\end{proof}

\begin{proof}[Proof of Proposition \ref{prop-estimation}]
    The proof of this proposition directly follows from Propositions \ref{estimation-through-rademacher} and \ref{proposition-rademacher-complexity}.
\end{proof}

\subsection{Proof of Theorem \ref{thm-universal-approximation}}

\label{appendix-dual-form}
To begin with, we provide a quick reminder about the Fenchel-Rockafellar theorem which is needed to derive the dual form of problem \eqref{unbalanced-eot-primal}.

\begin{theorem}[Fenchel-Rockafellar \citep{rockafellar1967duality}]
    Let $(E, E')$ and $(F, F')$ be two couples of topologically paired spaces. Let $A : E \mapsto F$ be a continuous linear operator and $\overline{A} : F' \mapsto E'$ be its adjoint. Let $f$ and $g$ be lower semicontinuous and proper convex functions defined on $E$ and $F$ respectively. If there exists $x\in \text{dom} f$ s.t. $g$ is continuous at $Ax$, then
    $$\sup_{x\in E} -f(-x) -g (Ax) = \min_{\overline{y}\in F'} \overline{f}(\overline{A} \; \overline{y}) + \overline{g}(\overline{y})
    $$
    and the min is attained. Moreover, if there exists a maximizer $x\in E$ then there exists $\overline{y} \in \overline{F}$ satisfying $Ax \in \partial \overline{g}(\overline{y})$ and $\overline{A} \overline{y} \in \partial f(-x)$.
\end{theorem}

We note that in the below theorem $\mathcal{C}_2(\mathbb{R}^d)$ \textbf{does not} denote the space of twice differentiable continuous functions. The exact definition of this space and $\mathcal{C}_{2,b}(\mathbb{R}^d)$ is given in \textbf{notations} part.


\begin{theorem}[Dual form of problem \eqref{unbalanced-eot-primal}]
The primal UEOT problem \eqref{unbalanced-eot-primal} has the dual counterpart \eqref{unbalanced-eot-dual} where the potentials $(\phi,\psi)$ belong to the space $\mathcal{C}_{2,b}(\mathbb{R}^d)\times \mathcal{C}_{2,b}(\mathbb{R}^d)$. The minimum of \eqref{unbalanced-eot-primal} is attained for a unique $\gamma^*\in \mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d)$. In turn, $\phi^*$ and $\psi^*$ maximize \eqref{unbalanced-eot-dual} if and only if \eqref{gamma-potentials} holds true.
\end{theorem}
\begin{proof}
    We recall that in the primal form, the minimization is performed over functions $\gamma$ belonging to $\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d)$. In this proof, we suppose that this space is endowed with a coarsest topology $\sigma(\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d))$ which makes continuous the linear functionals $\gamma \mapsto \int \zeta d \gamma, \; \forall \zeta\in \mathcal{C}_{2}(\mathbb{R}^d\times\mathbb{R}^d)$. Then the topological space $\big(\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d), \sigma(\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d))\big)$ has a topological dual $\big(\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d), \sigma(\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d))\big)'$  which, actually, is (linear) isomorphic to the space $\mathcal{C}_2(\mathbb{R}^d\times\mathbb{R}^d)$, see \citep[Lemma 9.9]{gozlan2017kantorovich}. This fact opens an opportunity to apply the well-celebrated Fenchel-Rockafellar theorem.  For this purpose, we will consider the following spaces: $E\defeq C_{2}(\mathbb{R}^d)\times C_{2}(\mathbb{R}^d)$, $F\defeq C_{2}(\mathbb{R}^d\times \mathbb{R}^d)$ and their duals $E'\defeq \mathcal{M}_{2,+}(\mathbb{R}^d)\times \mathcal{M}_{2,+}(\mathbb{R}^d)$ and $F'\defeq \mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d)$.

    {\underline{\textit{Step 1.}}} Recall that the convex conjugate of any function $g:\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d) \rightarrow \mathbb{R}\cup\{+\infty\}$ is defined for each $\zeta\in (\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d))'\cong \mathcal{C}_2(\mathbb{R}^d\times\mathbb{R}^d)$ as $\overline{g}(\zeta)=\sup_{\gamma\in\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d)} \lbrace\langle \gamma, \zeta \rangle - g(\gamma)\rbrace$. For the convenience of further derivations, we introduce additional functionals corresponding to the summands in the primal UEOT problem \eqref{unbalanced-eot-primal}:
    \begin{eqnarray}
        P(\gamma)\defeq \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \frac{\|x-y\|^2}{2} \gamma(x, y) dx dy - 
     \varepsilon H(\gamma);\;\;\;
     % \nonumber\\
     F_1(\gamma_x)\defeq \Df{f_1}{\gamma_x}{p};\;\;\;
     F_2(\gamma_y)\defeq \Df{f_2}{\gamma_y}{q}.
    \end{eqnarray}

    For our purposes, we need to calculate the convex conjugates of these functionals. Fortunately, convex conjugates of $f$-divergences $F_1(\gamma_x)$ 
    and $F_2(\gamma_y)$ 
    are well-known, see \citep[Proposition 23]{agrawal2021optimal}, and equal to
    \begin{eqnarray}
        \overline{F_1}(\phi) \defeq \int_{\mathbb{R}^d} \overline{f_1} (\phi(x)) p(x) dx, \;\;\;
        \overline{F_2}(\psi) \defeq \int_{\mathbb{R}^d} \overline{f_2} (\psi(y)) q(y) dy.
        \nonumber
    \end{eqnarray}
    To proceed, we calculate the convex conjugate of $P(\gamma)$:
    \begin{eqnarray}
        \overline{P}(\zeta)=\overline{\int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \frac{\|x-y\|^2}{2} \gamma(x, y) dx dy - 
     \varepsilon H(\gamma)\!}=
        \nonumber\\
        =\sup_{\gamma\in\mathcal{M}_{2,+}  (\mathbb{R}^d\times\mathbb{R}^d)} \Big\lbrace 
        \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \zeta(x, y) \gamma(x, y) dx dy - \big(\int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \frac{\|x-y\|^2}{2} \gamma(x, y) dx dy - 
      \varepsilon H(\gamma)\! \big)
      \Big\rbrace  
     = \nonumber\\
     \sup_{\gamma\in\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d)}
     \Big\lbrace 
     \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \big(\zeta(x,y)- \frac{\|x-y\|^2}{2}\big) \gamma(x,y) dx dy +
     \nonumber\\
     \varepsilon \underbrace{\Big(\int_{\mathbb{R}^d}\int_{\mathbb{R}^d} -\gamma(x,y)\log \gamma(x, y) dx dy - 1 +\|\gamma\|_1\!\Big)}_{=H(\gamma)}
     \Big\rbrace
     =\nonumber\\
     \sup_{\gamma\in\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d)} \varepsilon \cdot 
     \Big\lbrace 
     \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \gamma(x,y) \Big(\frac{\zeta(x,y)- \frac{\|x-y\|^2}{2}}{\varepsilon} - \log \gamma(x, y)\Big) dx dy -1 + \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \gamma(x,y)dx dy\! 
     \Big\rbrace
     =\nonumber\\
      \sup_{\gamma\in\mathcal{M}_{2,+} (\mathbb{R}^d\times\mathbb{R}^d)} (-
     \varepsilon) \cdot 
     \Big\lbrace \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \gamma(x,y) \big(\log \gamma(x, y)-\frac{\zeta(x,y)- \frac{\|x-y\|^2}{2}}{\varepsilon} \big) dx dy + 1- \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \gamma(x,y)dx dy\!\Big\rbrace
     =\nonumber\\
      \sup_{\gamma\in\mathcal{M}_{2,+} (\mathbb{R}^d\times\mathbb{R}^d)} (-
     \varepsilon) \cdot 
     \Big\lbrace \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \gamma(x,y) \big(\log \gamma(x, y)-\frac{\zeta(x,y)- \frac{\|x-y\|^2}{2}}{\varepsilon} \big) dx dy + 1- \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \gamma(x,y)dx dy\! +
     \nonumber
    \\
    \underbrace{\int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \exp\lbrace\frac{\zeta(x,y)- \frac{\|x-y\|^2}{2}}{\varepsilon}\rbrace dx dy - \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \exp\lbrace\frac{\zeta(x,y)- \frac{\|x-y\|^2}{2}}{\varepsilon}\rbrace dx dy}_{=0}
     \Big\rbrace =
     \label{conj-to-kl-before}
     \\
     \sup_{\gamma\in\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d)} (-\varepsilon) \cdot \Bigg\lbrace\KL{\gamma}{\exp\lbrace\frac{\zeta(x,y)- \frac{\|x-y\|^2}{2}}{\varepsilon}\rbrace} + 1-  \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \exp\lbrace\frac{\zeta(x,y)- \frac{\|x-y\|^2}{2}}{\varepsilon}\rbrace dx dy\Bigg\rbrace.
     \label{conjugate-first-summand}
    \end{eqnarray}
    Here in the transition from \eqref{conj-to-kl-before} to \eqref{conjugate-first-summand}, we keep in mind our prior calculations of  $\text{D}_{\text{KL}}$  in \eqref{kl-derivation-bgt}. 
    
    Recall that  $\text{D}_{\text{KL}}$  is non-negative and attains zero at the unique point 
    \begin{eqnarray}
        \gamma^*(x,y)=\exp\{\frac{\zeta(x,y)- \frac{\|x-y\|^2}{2}}{\varepsilon}\}.
        \label{optimal-gamma}
    \end{eqnarray}
    Thus, we get 
    \begin{eqnarray}
        \overline{P}(\zeta)= 
        \varepsilon \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \exp\{\frac{\zeta(x,y)- \frac{\|x-y\|^2}{2}}{\varepsilon}\}dx dy -\varepsilon.
    \end{eqnarray}


    {\underline{\textit{Step 2.}}} Now we are ready to apply the Fenchel-Rockafellar theorem in our case. To begin with, we show that this theorem is applicable to problem \eqref{unbalanced-eot-primal}, i.e., that the functions under consideration satisfy the necessary conditions.
    Indeed, it is known that the convex conjugate of any functional (e.g., $\overline{F_1}(\cdot)$, $\overline{F_2}(\cdot)$, $\overline{P}(\cdot)$) is \textbf{lower semi-continuous} and \textbf{convex}. Besides, the listed functionals are \textbf{proper convex}\footnote{\textit{Proper convex} function is a real-valued convex function which has a non-empty domain, never attains the value $(-\infty)$ and is not identically equal to $(+\infty)$. This property ensures that the minimization problem for this function has non-trivial solutions.}. Indeed, the properness of $\overline{F_1}(\cdot)$ and $\overline{F_1}(\cdot)$ follows from the fact that $f$-divergences are known to be lower-semicontinuous and proper themselves, while properness of $\overline{P}(\cdot)$ is evident from \eqref{conjugate-first-summand}. 
    
    Now we consider the linear operator $A: C_2(\mathbb{R}^d)\times C_2(\mathbb{R}^d ) \mapsto C_2(\mathbb{R}^d\times \mathbb{R}^d)$ which is defined as $A(\phi,\psi): (x,y) \mapsto \phi(x) + \psi(y)$. It is continuous, and its adjoint is defined on $\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d)$ as $\overline{A}(\gamma)=(\gamma_x, \gamma_y)$. Indeed, $\langle \overline{A}(\gamma), (u,v)\rangle=\langle \gamma, A(u,v)\rangle=\int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \gamma(x, y) (u(x)+v(y)) dx dy = \int_{\mathbb{R}^d} \gamma_x(x) u(x) dx + \int_{\mathbb{R}^d} \gamma_y(y) v(y) dy$.
    
    Thus, the strong duality and the existence of minimizer for \eqref{unbalanced-eot-primal} follows from the Fenchel-Rockafellar theorem which states that problems
    \begin{eqnarray}
        \sup_{(\phi,\psi)\in C_2(\mathbb{R}^d)\times C_2(\mathbb{R}^d)}
        \{ -\overline{P}(A(\phi,\psi))
        - \overline{F_1}(-\phi)
        - \overline{F_2}(-\psi)
        \}
        \label{duality-fenchel}
    \end{eqnarray}
    and 
    \begin{eqnarray}
        \min_{\gamma\in \mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d)}
        \{\varepsilon P(\gamma) + F_1(\gamma_x)+ F_2(\gamma_y)
        \}
    \end{eqnarray}
    are equal. Uniqueness of the minimizer for \eqref{unbalanced-eot-primal} comes from the strict convexity of $P(\cdot)$ (which holds thanks to the entropy term). Note that the conjugate of the sum of $F_1$ and $F_2$ is equal to the sum of their conjugates since they are defined for separate non-intersecting groups of parameters.
    
    Next we prove that the supremum can be restricted to $\mathcal{C}_{2, b}(\mathbb{R}^d\times\mathbb{R}^d)$. Here we use $\vee$ to denote the operation of taking maximum between the function $f:\mathbb{R}^d\rightarrow\mathbb{R}$ and real value $k$: $f\vee k= \lbrace \max(f(x),k)\; \vert\; x\in \mathbb{R}^d\rbrace$.
    We observe that
    \begin{eqnarray}
        % \overline{P}(A(\phi,\psi)) = 
        \sup_{(\phi,\psi)\in C_2(\mathbb{R}^2)\times C_2(\mathbb{R}^2)} \lbrace -\varepsilon \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \exp\{ \frac{\phi(x) + \psi(y) - \frac{\|x-y\|^2}{2}}{\varepsilon}\}dx dy -\overline{F_1}(-\phi) - \overline{F_2}(-\psi)\rbrace
        = 
        \nonumber\\
        \lim_{k_1\rightarrow -\infty}\lim_{k_2\rightarrow  -\infty}\sup_{(\phi,\psi)\in C_2(\mathbb{R}^2)\times C_2(\mathbb{R}^2)} \lbrace-\varepsilon \int_{\mathbb{R}^d } \int_{\mathbb{R}^d } \exp\{ \frac{\phi(x) \vee k_1 + \psi(y) \vee k_2 - \frac{\|x-y\|^2}{2}}{\varepsilon}\}dx dy - 
        \nonumber\\
        \int_{\mathbb{R}^d} \overline{f_1} (\psi(x)\vee k_1) p(x) dx -\int_{\mathbb{R}^d} \overline{f_2} (\psi(x)\vee k_2) q(y) dy
        \rbrace=
        \nonumber\\
        \lim_{k_1\rightarrow -\infty}\lim_{k_2\rightarrow -\infty} \lbrace -\overline{P}(A(\phi \vee k_1,\psi \vee k_2))  -\overline{F_1}(-\phi\vee k_1) - \overline{F_2}(-\psi \vee k_2) \rbrace.
    \end{eqnarray}

    Thus, we get

    \begin{eqnarray}
        \sup_{(\phi,\psi)\in C_2(\mathbb{R}^2)\times C_2(\mathbb{R}^2)} \lbrace
      -\overline{P}(A(\phi,\psi)) -\overline{F_1}(-\phi) - \overline{F_2}(-\psi) \rbrace =
      \nonumber\\
      \sup_{(\phi,\psi)\in C_2(\mathbb{R}^2)\times C_2(\mathbb{R}^2)} \lim_{k_1\rightarrow -\infty}\lim_{k_2\rightarrow -\infty} \lbrace -\overline{P}(A(\phi \vee k_1,\psi \vee k_2)) -\overline{F_1}(-\phi\vee k_1) - \overline{F_2}(-\psi \vee k_2) \rbrace
      \leq 
      \nonumber\\
      \sup_{(\phi,\psi)\in C_{2,b}(\mathbb{R}^2)\times C_{2,b}(\mathbb{R}^2)} 
      \lbrace
      -\overline{P}(A(\phi,\psi)) -\overline{F_1}(-\phi) - \overline{F_2}(-\psi) \rbrace.
    \end{eqnarray}

    Since the other inequality is obvious, the two quantities are equal which completes the proof.
    
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm-universal-approximation}] Our aim is to prove that for all $\delta >0$ there exist unnormalized Gaussian mixtures $u_{\omega}$ and $v_{\theta}$ s.t. $\mathcal{L}(\theta,\omega)-\mathcal{L}^*<\delta\varepsilon$. Following \eqref{def-alternative-obj}, we define
$$ \mathcal{J}(\phi,\psi)\defeq \varepsilon \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \!\exp\{\frac{1}{\varepsilon} \!( \phi(x) + \psi(y)\!-\! \frac{\|x-y\|^2}{2}) \} dx dy \!+ \!
    \int_{\mathbb{R}^d} \overline{f}_1 (-\phi(x))p(x) dx \! + \!\int_{\mathbb{R}^d} \overline{f}_2 (-\psi(y))q(y) dy.$$
Then from \eqref{unbalanced-eot-dual}, it follows that $\mathcal{L}^*=\inf_{(\phi,\psi)\in \mathcal{C}_{2,b}(\mathbb{R}^d)\times \mathcal{C}_{2,b}(\mathbb{R}^d)} \mathcal{J}(\phi, \psi)$. Finally, using the definition of the infimum, we get that for all $\delta'>0$ there exist some functions $(\widehat{\phi}, \; \widehat{\psi}) \in \mathcal{C}_{2,b}(\mathbb{R}^d)\times \mathcal{C}_{2,b}(\mathbb{R}^d)$ such that $\mathcal{J}(\widehat{\phi},\widehat{\psi}) < \mathcal{L}^* + \delta'$. For further derivations, we set $\delta'\defeq\frac{\delta\varepsilon}{2}$ and pick the corresponding $(\widehat{\phi}, \; \widehat{\psi})$.

\underline{\textit{Step 1.}} {\color{black}We start with the derivation of some inequalities useful for future steps.} Since ${(\phi,\psi)\in \mathcal{C}_{2,b}(\mathbb{R}^d)\!\times\! \mathcal{C}_{2,b}(\mathbb{R}^d)}$, they have upper bounds $\widehat{a}$ and $\widehat{b}$ such that for all $x,y\in \mathbb{R}^d$: $\widehat{\phi}(x)\leq a$ and $\widehat{\psi}(y)\leq b$ respectively. We recall that by the assumption of the theorem, measures $p$ and $q$ are compactly supported. Thus, there exist balls centered at $x=0$ and $y=0$ and having some radius $R>0$ which contain the supports of $p$ and $q$ respectively. Then we define
\begin{eqnarray*}
    \widetilde{\phi}(x) \defeq \widehat{\phi}(x) - \max\{0, {\color{black}\max\{\|x\|^2-R^2, \|x\|^4-R^4\}\}}\leq \widehat{\phi}(x)\leq \widehat{a};\nonumber\\
    \widetilde{\psi}(y) \defeq \widehat{\psi}(y) - \max\{0, \|y\|^2-R^2, \}\leq \widehat{\psi}(y)\leq \widehat{b}.\nonumber
\end{eqnarray*}
We get that
\begin{eqnarray}
    \widetilde{\phi}(x) \leq \widehat{\phi}(x), \widetilde{\psi}(y) \leq \widehat{\psi}(y)
 \Longrightarrow \nonumber\\
 \varepsilon \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \!\exp\{\frac{1}{\varepsilon} \!( \widetilde{\phi}(x) + \widetilde{\psi}(y)\!-\! \frac{\|x-y\|^2}{2}) \} dx dy \leq \varepsilon \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \!\exp\{\frac{1}{\varepsilon} \!( \widehat{\phi}(x) + \widehat{\psi}(y)\!-\! \frac{\|x-y\|^2}{2}) \} dx dy
 \label{derivation-1st}
\end{eqnarray}
Importantly, for all $x$ and $y$ within the supports of $p$ and $q$ it holds that $\widetilde{\phi}(x)=\widehat{\phi}(x)$ and $\widetilde{\psi}(y)=\widehat{\psi}(y)$, respectively. Then
\begin{eqnarray}
    \int_{\mathbb{R}^d} \overline{f}_1 (-\widehat{\phi}(x))p(x) dx = \int_{\mathbb{R}^d} \overline{f}_1 (-\widetilde{\phi}(x))p(x) dx, \;\;\;\;
    \!\int_{\mathbb{R}^d} \overline{f}_2 (-\widehat{\psi}(y))q(y) dy = \int_{\mathbb{R}^d} \overline{f}_2 (-\widetilde{\psi}(y))q(y) dy.
    \label{derivation-2nd}
\end{eqnarray}
Combining \eqref{derivation-1st} and \eqref{derivation-2nd}, we get that $\mathcal{J}(\widetilde{\phi}, \widetilde{\psi}) \leq \mathcal{J}(\widehat{\phi}, \widehat{\psi})<\mathcal{L}^*+\delta$.

Before moving on, we note that functions $\exp\{\nicefrac{\widetilde{\phi}(x)}{\varepsilon}\}$ and $\exp\{\nicefrac{\widetilde{\psi}(y)}{\varepsilon}\}$ {\color{black}are continuous and non-negative}. Therefore, since measures $p$ and $q$ are compactly supported, there exist some constants $e_{\min}, \; h_{\min}>0$ such that $\exp\{\nicefrac{\widetilde{\phi}(x)}{\varepsilon}\}>e_{\min}$ and $\exp\{\nicefrac{\widetilde{\psi}(y)}{\varepsilon}\}>h_{\min}$ for all $x$ and $y$ from the supports of measures $p$ and $q$ respectively. We keep these constants for future steps.

\underline{\textit{Step 2.}} This step of our proof is similar to \citep[Theorem 3.4]{korotin2024light}. We get that
\begin{eqnarray}
    \exp\big(\nicefrac{\widetilde{\phi}(x)}{\varepsilon}\big)\leq \exp\bigg(\frac{\widehat{a}-\max \{0, \|x\|^{2}-R^2\}}{\varepsilon}\bigg)\leq \exp\big(\frac{\widehat{a}+R^{2}}{\varepsilon}\big)\cdot\exp(-\nicefrac{\|x\|^{2}}{\varepsilon}),
    \label{bound-phi}
    \\
    \exp\big(\nicefrac{\widetilde{\psi}(y)}{\varepsilon}\big)\leq \exp\bigg(\frac{\widehat{b}-\max \{0, \|y\|^{2}-R^2\}}{\varepsilon}\bigg)\leq \exp\big(\frac{\widehat{b}+R^{2}}{\varepsilon}\big)\cdot\exp(-\nicefrac{\|y\|^{2}}{\varepsilon}).
    \nonumber
\end{eqnarray}
From this we can deduce that $y\mapsto \exp(\nicefrac{\widetilde{\psi}(y)}{\varepsilon})$ is a normalizable density since it is bounded by the unnormalized Gaussian density. Moreover, we see that it vanishes at the infinity. Thus, using the result \citep[Theorem 5a]{nguyen2020approximation}, we get that for all $\delta''>0$ there exists an unnormalized Gaussian mixture $v_{\widetilde{\theta}}=v_{\widetilde{\theta}}(\delta'')$ such that

\begin{eqnarray}\|v_{\widetilde{\theta}}-\exp(\nicefrac{\widetilde{\psi}}{\varepsilon})\|_{\infty}=\sup_{y\in\mathbb{R}^{D}}|v_{\widetilde{\theta}}(y)-\exp(\nicefrac{\widetilde \psi(y)}{\varepsilon})|<\delta''.
\label{diff-to-exp}
\end{eqnarray}

Following the mentioned theorem, we can set all the covariances in $v_{\widetilde{\theta}}$ to be scalar
, i.e., define $v_{\widetilde{\theta}}(x_1)=\sum_{k=1}^{K}{\color{black}\widetilde{\alpha}_{k}\mathcal{N}(x_{1}|\widetilde{r}_{k},\varepsilon \widetilde{\lambda}_{k}I).}$ for some $K$ and {\color{black}$\widetilde{\alpha}_k\in \mathbb{R}_{+}$, $\widetilde{r}_k\in\mathbb{R}^{D}$, $\widetilde{\lambda}_{k}\in \mathbb{R}_{+}$ ($k\in\{1,\dots,K\}$)}. For our future needs, we set $$\delta''=\frac{\delta\varepsilon}{2}\cdot \Bigg[ L_1 \cdot \frac{\varepsilon}{e_{\min}} +  L_2 \cdot \frac{\varepsilon}{h_{\min}} + \varepsilon (2\pi\varepsilon)^{\frac{d}{2}}\Big( 1 + (\pi\varepsilon)^{\frac{d}{2}} \exp\big\{\frac{\widehat{a}+R^{2}}{\varepsilon}\big\}\Big)\Bigg]^{-1}.$$

For simplicity, we consider the other mixture $v_{\theta}(y)\defeq v_{\widetilde{\theta}}(y)\exp(-\frac{\|y\|^{2}}{2\varepsilon})$ which is again unnormalized and has scalar covariances, see the proof of \citep[Theorem 3.4]{korotin2024light} for explanation. {\color{black}We denote the weights, means, and covariances of this mixture by $\alpha_k\in \mathbb{R}_+$, $r_k\in\mathbb{R}^D$ and $\lambda_k\in\mathbb{R}_+$, respectively.}

We derive that 
\begin{eqnarray}
    \varepsilon \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \!\exp\big\{\frac{1}{\varepsilon} \!( \widetilde{\phi}(x) + \widetilde{\psi}(y)\!-\! \frac{\|x-y\|^2}{2}) \big\} dx dy = \varepsilon \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \!\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\} \exp\{-\frac{\|x-y\|^2}{2\varepsilon}\}  \exp\{\frac{\widetilde{\psi}(y)}{\varepsilon}\} dx dy > \nonumber\\
    \varepsilon \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \!\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\} \exp\{-\frac{\|x-y\|^2}{2\varepsilon}\}  (v_{\widetilde{\theta}}(y) - \delta'') dx dy = 
    \nonumber\\
    \varepsilon \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \!\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\} \exp\{-\frac{\|x-y\|^2}{2\varepsilon}\}  v_{\widetilde{\theta}}(y) dx dy - 
    \delta'' \varepsilon \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \!\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\} \exp\{-\frac{\|x-y\|^2}{2\varepsilon}\} dx dy = 
    \nonumber\\
    \!\!\varepsilon \int_{\mathbb{R}^d} \!\!\int_{\mathbb{R}^d} \!\!\!\!\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\} \!\exp\{-\frac{\|x\|^2}{2\varepsilon}\} \!\exp\{\frac{\langle x,y\rangle}{\varepsilon}\} \!\underbrace{\exp\{-\frac{\|y\|^2}{2\varepsilon}\} \!v_{\widetilde{\theta}}(y)}_{=v_{\theta}(y)} dx dy \!-\! 
    \delta'' \!\varepsilon \!\int_{\mathbb{R}^d} \!\!\int_{\mathbb{R}^d} \!\!\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\} \!\exp\{-\frac{\|x-y\|^2}{2\varepsilon}\} dx dy \!= 
    \nonumber\\
    \varepsilon \int_{\mathbb{R}^d} \!\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\} \exp\{-\frac{\|x^2\|}{2\varepsilon}\}\Big(\underbrace{\int_{\mathbb{R}^d} \exp\{\frac{\langle x,y\rangle}{\varepsilon}\}  v_{\theta}(y) dy}_{=c_{\theta}(x)}\Big) dx
    - \delta'' \varepsilon  \int_{\mathbb{R}^d} \!\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\} \Big(\underbrace{\int_{\mathbb{R}^d} \exp\{-\frac{\|x-y\|^2}{2\varepsilon}\} dy}_{=(2\pi \varepsilon)^{\nicefrac{d}{2}}}\Big) dx =
    \nonumber\\
    \varepsilon \int_{\mathbb{R}^d} \!\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\} \exp\{-\frac{\|x^2\|}{2\varepsilon}\} c_{\theta}(x) dx  - \delta'' \varepsilon  (2\pi \varepsilon)^{\nicefrac{d}{2}} \int_{\mathbb{R}^d} \!\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\} dx \stackrel{\eqref{bound-phi}}{>}
    \nonumber\\
    \varepsilon \int_{\mathbb{R}^d} \!\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\} \exp\{-\frac{\|x^2\|}{2\varepsilon}\} c_{\theta}(x) dx - \delta'' \varepsilon  (2\pi \varepsilon)^{\nicefrac{d}{2}} \exp\big\{\frac{\widehat{a}+R^{2}}{\varepsilon}\big\} \int_{\mathbb{R}^d} \underbrace{\exp\{-\nicefrac{\|x\|^{2}}{\varepsilon}\} dx}_{=(\pi\varepsilon)^{\nicefrac{d}{2}}}=
    \nonumber\\
    \varepsilon \int_{\mathbb{R}^d} \!\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\} \exp\{-\frac{\|x^2\|}{2\varepsilon}\} c_{\theta}(x) dx - \delta''  2^{\nicefrac{d}{2}} \pi^d  \varepsilon^{(d+1)} \exp\big\{\frac{\widehat{a}+R^{2}}{\varepsilon}\big\}.
    \label{first-integral-buw}
\end{eqnarray}

\underline{\textit{Step 3.}} At this point, we will show that for every $\delta''>0$, there exists an unnormalized Gaussian mixture $u_{\widetilde{\omega}}$ which is $\delta''$-close to $\exp\{\nicefrac{\widetilde{\phi}(x)}{\varepsilon}\}c_{\theta}(x)$. Using the closed-form expression for $c_{\theta(x)}$ from \citep[Proposition 3.2]{korotin2024light}, we get that
%Let us rewrite the formula for $c_{\theta}(x)$:

\begin{eqnarray*}
    c_{\theta}(x)=
     \sum_{k=1}^K \alpha_k  \exp\{-\frac{\|r_k\|^2}{2\varepsilon\lambda_k}\} \exp\{\frac{\|r_k + x\lambda_k\|^2}{2\varepsilon\lambda_k}\}.
\end{eqnarray*}

Then 
\begin{eqnarray*}
    \exp\big(\nicefrac{\widetilde{\phi}(x)}{\varepsilon}\big)c_{\theta}(x)\leq \exp\bigg(\frac{\widehat{a}-\max \{0, \|x\|^{4}-R^4\}}{\varepsilon}\bigg)c_{\theta}(x) \leq \exp\big(\frac{\widehat{a}+R^{4}}{\varepsilon}\big)\cdot\exp(-\nicefrac{\|x\|^{4}}{\varepsilon}) c_{\theta}(x)=
    \\
     \sum_{k=1}^K \alpha_k  \exp\big(\frac{\widehat{a}+R^{4}}{\varepsilon}\big)  \exp\{-\frac{\|r_k\|^2}{2\varepsilon\lambda_k}\} \cdot\exp(-\frac{\|x\|^{4}}{\varepsilon})  \exp\{\frac{\|r_k + x\lambda_k\|^2}{2\varepsilon\lambda_k}\}=
    \\
    \sum_{k=1}^K \alpha_k  \exp\big(\frac{\widehat{a}+R^{4}}{\varepsilon}\big)  \exp\{-\frac{\|r_k\|^2}{2\varepsilon\lambda_k}\} \cdot \exp\{\frac{\|r_k + x\lambda_k\|^2 - 2\lambda_k \|x\|^4}{2\varepsilon\lambda_k}\}
\end{eqnarray*}

From this, we see that $\exp\big(\nicefrac{\widetilde{\phi}(x)}{\varepsilon}\big){\color{black}c_{\theta}(x)}$ tends to zero while $x$ approaches infinity. It means that $x\mapsto \exp\big(\nicefrac{\widetilde{\phi}(x)}{\varepsilon}\big)c_{\theta}(x)$ corresponds to the normalizable density. Using \citep[Theorem 5a]{nguyen2020approximation}, we get that for all $\delta''>0$ there exists an unnormalized Gaussian mixture $u_{\widetilde{\omega}}$ such that
\begin{equation}
\|u_{\widetilde{\omega}}-\exp(\nicefrac{\widetilde{\phi}}{\varepsilon})c_{\theta}\|_{\infty}=\sup_{x\in\mathbb{R}^{D}}|u_{\widetilde{\omega}}(x)-\exp(\nicefrac{\widetilde{\phi}(x)}{\varepsilon})c_{\theta}(x))|<\delta''.
\label{diff-to-exp-2}
\end{equation}

Analogously with $v_{\widetilde{\theta}}$, we can set all the covariances in $u_{\widetilde{\omega}}$ to be scalar, i.e., define $u_{\widetilde{\omega}}={\color{black}\sum_{l=1}^L \widetilde{\beta}_l \mathcal{N}(x|\widetilde{\mu}_l, \varepsilon \widetilde{\zeta}_l I)}$ for some $L$, {\color{black}$\widetilde{\mu}_l\in \mathbb{R}^d$, $\widetilde{\zeta}_l\in\mathbb{R}_+\;(l\in\{1,...,L\})$}. Moreover, we consider $u_{\omega}(x) =  u_{\widetilde{\omega}}(x) \exp\{-\frac{\|x\|^2}{2\varepsilon}\}$ which is again an unnormalized density with scalar covariances. {\color{black}We denote the weights, means, covariances of this mixture by $\beta_l\in \mathbb{R}_+$, $\mu_l\in\mathbb{R}^D$, $\zeta_l\in\mathbb{R}_+$ respectively.}

Next we recall the equation \eqref{first-integral-buw} and get that
\begin{eqnarray}
    \eqref{first-integral-buw} > \varepsilon \int_{\mathbb{R}^d} \exp\{-\frac{\|x\|^2}{2\varepsilon}\} (u_{\widetilde{\omega}}(x) - \delta'') dx - \delta''  2^{\nicefrac{d}{2}} \pi^d  \varepsilon^{(d+1)} \exp\big\{\frac{\widehat{a}+R^{2}}{\varepsilon}\big\}=
    \nonumber\\
    \varepsilon \int_{\mathbb{R}^d} \underbrace{\exp\{-\frac{\|x\|^2}{2\varepsilon}\} u_{\widetilde{\omega}}(x)}_{=u_{\omega}(x)} dx - \varepsilon \delta'' \underbrace{\int_{\mathbb{R}^d} \exp\{-\frac{\|x\|^2}{2\varepsilon}\} dx}_{=(2\pi\varepsilon)^{\frac{d}{2}}} - \delta''  2^{\nicefrac{d}{2}} \pi^d  \varepsilon^{(d+1)} \exp\big\{\frac{\widehat{a}+R^{2}}{\varepsilon}\big\}=
    \nonumber\\
    \varepsilon \|u_{\omega}\|_1 -\varepsilon \delta'' (2\pi\varepsilon)^{\frac{d}{2}}\Big( 1 + (\pi\varepsilon)^{\frac{d}{2}} \exp\big\{\frac{\widehat{a}+R^{2}}{\varepsilon}\big\}\Big).
    \label{estimation-norm}
\end{eqnarray}

\underline{\textit{Step 4.}} Now we turn to other expressions. Using the property that a function $t\mapsto \log t$ is $\frac{1}{t_{\min}}$-Lipshitz on interval $[t_{\min}, +\infty)$ we get
\begin{eqnarray}
     \log (\exp\{\frac{\widetilde{\psi}(y)}{\varepsilon}\}) - \log (\exp\{\frac{\widetilde{\psi}(y)}{\varepsilon}\} - \delta'') \leq \frac{\delta''}{h_{\min}} \Longrightarrow \log (\exp\{\frac{\widetilde{\psi}(y)}{\varepsilon}\} - \delta'') \geq \frac{\widetilde{\psi}(y)}{\varepsilon} - \frac{\delta''}{h_{\min}}.
     \label{log-lipshitz-v}
\end{eqnarray}

Similarly, we get that
\begin{eqnarray}
     \log (\exp\{\frac{\widetilde{\phi}(y)}{\varepsilon}\}c_{\theta}(x) - \delta'') \geq \frac{\widetilde{\phi}(y)}{\varepsilon} + \log c_{\theta}(x) - \frac{\delta''}{e_{\min}}.
     \label{log-lipshitz-u}
\end{eqnarray}

We use this inequality, monotonicity of logarithm function, and \eqref{bound-phi}, to derive 
\begin{eqnarray}
    v_{\widetilde{\theta}}(y) \stackrel{\eqref{bound-phi}}{>} \exp\{\frac{\widetilde{\psi}(y)}{\varepsilon}\} - \delta''\Longrightarrow \log v_{\widetilde{\theta}}(y) > \log (\exp\{\frac{\widetilde{\psi}(y)}{\varepsilon}\} - \delta'') \stackrel{\eqref{log-lipshitz-v}}{\geq} \frac{\widetilde{\psi}(y)}{\varepsilon} - \frac{\delta''}{h_{\min}} \Longrightarrow 
    \nonumber\\
    - \widetilde{\psi}(y) > -\varepsilon \log v_{\widetilde{\theta}}(y) - \frac{\varepsilon 
    \delta''}{h_{\min}};
    \\
    u_{\widetilde{\omega}}(x) \stackrel{\eqref{diff-to-exp-2}}{>} \exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\}c_{\theta}(x) - \delta'' \Longrightarrow \log u_{\widetilde{\omega}}(x) > \log (\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\}c_{\theta}(x) - \delta'') \stackrel{\eqref{log-lipshitz-u}}{\geq}
    \nonumber\\
    \frac{\widetilde{\phi}(y)}{\varepsilon} + \log c_{\theta}(x) - \frac{\delta''}{e_{\min}} 
    \Longrightarrow 
    % \log \frac{u_{\widetilde{\omega}}(x)}{c_{\theta}(x)} > \frac{\widetilde{\phi}(y)}{\varepsilon} - \frac{\delta''}{e_{\min}}.
    - \widetilde{\phi}(y) > - \varepsilon \log \frac{u_{\widetilde{\omega}}(x)}{c_{\theta}(x)} - \frac{\varepsilon\delta''}{e_{\min}}.
\end{eqnarray}

Recall that  $\overline{f_1}$ and $\overline{f_2}$ are non-decreasing functions. Moreover, they are Lipshitz with the constants $L_1$, $L_2$ respectively. Thus, we get

\begin{eqnarray}
    \overline{f_2}(- \widetilde{\psi}(y)) > \overline{f_2}(-\varepsilon \log v_{\widetilde{\theta}}(y) - \frac{\varepsilon \delta''}{h_{\min}}) = \overline{f_2}(-\varepsilon (\log v_{\theta}(y) + \frac{\|y\|^2}{2\varepsilon}) - \frac{\varepsilon \delta''}{h_{\min}}) =
    \nonumber\\
    \overline{f_2}(-\varepsilon \log v_{\theta}(y) - \frac{\|y\|^2}{2} - \frac{\varepsilon \delta''}{h_{\min}}) \geq \overline{f_2}(-\varepsilon \log v_{\theta}(y) - \frac{\|y\|^2}{2}) - L_2 \cdot \frac{\varepsilon \delta''}{h_{\min}};
    \\
    \overline{f_1}(- \widetilde{\phi}(y)) > \overline{f_1}(- \varepsilon \log \frac{u_{\widetilde{\omega}}(x)}{c_{\theta}(x)} - \frac{\varepsilon\delta''}{e_{\min}}) = \overline{f_1}(- \varepsilon \log u_{\widetilde{\omega}}(x) + \varepsilon\log c_{\theta}(x) - \frac{\varepsilon\delta''}{e_{\min}}) = 
    \nonumber\\
    \overline{f_1}(- \varepsilon \log u_{\omega}(x) - \frac{\|x\|^2}{2} + \varepsilon\log c_{\theta}(x) - \frac{\varepsilon\delta''}{e_{\min}}) = \overline{f_1}(- \varepsilon \log \frac{u_{\omega}(x)}{c_{\theta}(x)} - \frac{\|x\|^2}{2} - \frac{\varepsilon\delta''}{e_{\min}}) \geq
    \nonumber\\
    \overline{f_1}(- \varepsilon \log \frac{u_{\omega}(x)}{c_{\theta}(x)} - \frac{\|x\|^2}{2}) - L_1 \cdot \frac{\varepsilon\delta''}{e_{\min}}.
\end{eqnarray}
Integrating these inequalities over all $x$ and $y$ in supports of $p$ and $q$ respectively, we get

\begin{eqnarray}
    \int_{\mathbb{R}^d} \overline{f}_2 (-\psi(y))q(y) dy \geq \int_{\mathbb{R}^d}  \overline{f_2}(-\varepsilon \log v_{\theta}(y) - \frac{\|y\|^2}{2})q(y)dy - L_2 \cdot \frac{\varepsilon \delta''}{h_{\min}};
    \\
    \int_{\mathbb{R}^d} \overline{f}_1 (-\phi(x))p(x) dx \geq \int_{\mathbb{R}^d}  \overline{f_1}(- \varepsilon \log \frac{u_{\omega}(x)}{c_{\theta}(x)} - \frac{\|x\|^2}{2})p(x)dx - L_1 \cdot \frac{\varepsilon \delta''}{e_{\min}}.
    \label{estimation-divergences}
\end{eqnarray}

Finally, combining \eqref{estimation-norm} and \eqref{estimation-divergences}, we get
\begin{eqnarray*}
    \mathcal{L}(\theta,\omega)=\mathcal{J}(\phi_{\theta,\omega},\psi_{\theta}) = \varepsilon \|u_{\omega}\|_1 + \overline{f_1}(- \varepsilon \log \frac{u_{\omega}(x)}{c_{\theta}(x)} - \frac{\|x\|^2}{2})p(x)dx + \overline{f_2}(-\varepsilon \log v_{\theta}(y) - \frac{\|y\|^2}{2})q(y)dy < 
    \nonumber\\
    \varepsilon \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \!\exp\{\frac{1}{\varepsilon} \!( \widetilde{\phi}(x) + \widetilde{\psi}(y)\!-\! \frac{\|x-y\|^2}{2}) \} dx dy \!+ \!
    \int_{\mathbb{R}^d} \overline{f}_1 (-\widetilde{\phi}(x))p(x) dx \! + \!\int_{\mathbb{R}^d} \overline{f}_2 (-\widetilde{\psi}(y))q(y) dy + 
    \nonumber\\
    L_1 \cdot \frac{\varepsilon \delta''}{e_{\min}} + L_2 \cdot \frac{\varepsilon \delta''}{h_{\min}} + \varepsilon \delta'' (2\pi\varepsilon)^{\frac{d}{2}}\Big( 1 + (\pi\varepsilon)^{\frac{d}{2}} \exp\big\{\frac{\widehat{a}+R^{2}}{\varepsilon}\big\}\Big) 
    <
    \\
    \mathcal{L}^* + \delta' + \delta'' \Bigg[ L_1 \cdot \frac{\varepsilon}{e_{\min}} +  L_2 \cdot \frac{\varepsilon}{h_{\min}} + \varepsilon (2\pi\varepsilon)^{\frac{d}{2}}\Big( 1 + (\pi\varepsilon)^{\frac{d}{2}} \exp\big\{\frac{\widehat{a}+R^{2}}{\varepsilon}\big\}\Big)\Bigg]
    \leq \mathcal{L}^* + \frac{\delta\varepsilon}{2} + \frac{\delta\varepsilon}{2}
    \Longrightarrow
    \nonumber\\
    \KL{\gamma_{\theta,\omega}}{\gamma^*} \leq \varepsilon^{-1}(\mathcal{L}(\theta,\omega) - \mathcal{L}^*) < \delta
\end{eqnarray*}
which completes the proof.
\end{proof}

\section{Experiments Details}
\label{sec:exp-details}
\subsection{General Details}
For minimization of the objective \eqref{empirical-objective}, we parametrize $\alpha_k,r_k,S_k$ and $\beta_l,\mu_l,\Sigma_l$ of $v_{\theta}$ and $u_{\omega}$ \eqref{gauss-parametrization} respectively. For simplicity, we parametrize $\alpha_k,\;\beta_l$ as their logarithms $\log\alpha_k,\;\log\beta_l$, variables $r_k,\;\mu_l$ are parametrized directly. We consider diagonal matrices $S_k,\;\Sigma_l$ and parametrize them via the values $\log(S_k)_{i,i}$, $\log(\Sigma_l)_{i,i}$ respectively. We initialize the parameters following the scheme in \citep{korotin2024light}.
In all experiments, we use Adam optimizer and set $K=L$.

\subsection{Details on Experiment with Gaussian Mixtures}
For our solver we use $K=L=5$, $\varepsilon=0.05$, $lr=3e-4$ and batchsize $128$. We do $2\cdot 10^4$ gradient steps.

\textbf{Baselines.} For the OT-FM and UOT-FM methods, we parameterize the vector field $(v_{t,\theta})_{t\in[0,1]}$ for mass transport using a 1-layer feed-forward network with 64 neurons and ReLU activation. These methods are built on the solutions (plans $\pi^{*}(x,y)$) of discrete OT problems, to obtain them we use the POT~\cite{flamary2021pot} package. Especially for the UOT-FM, we use the \texttt{ot.unbalanced.sinkhorn} with the regularization equal to $0.05$. We set the number of training and inference time steps equal to $100$. For the LightSB algorithm, we use the parameters presented by the authors in the official repository.

\subsection{Details on Image Translation Experiment}
We use the code and decoder model from
\begin{center}
    \url{https://github.com/podgorskiy/ALAE}
\end{center} 
We download the data and neural network extracted attributes for the FFHQ dataset from 
\begin{center}
\url{https://github.com/ngushchin/LightSB/}
\end{center}
In the \textit{Adult} class we include the images with the attribute \textit{Age} $\!\geq 44$; in the \textit{Young} class - with the \textit{Age}$\in[16, 44]$. We excluded the images with faces of children to increase the accuracy of classification per \textit{gender} attribute.
For the experiments with our solver, we use weighted $\text{D}_{\text{KL}}$ divergence with $\tau=100$ and set $K=L=50$, $\varepsilon=0.1$, $lr=1$, and batch size $128$. We do $5\cdot10^3$ gradient steps. 

\textbf{Classifier.} We trained an MLP classifier to distinguish images of the classes ($Adult$, $Young$, $Woman$, $Man$) to measure the accuracy of keeping the class of the input images by different solvers.

\textbf{Baselines.} For the OT-FM and UOT-FM methods, we parameterize the vector field $(v_{t,\theta})_{t\in[0,1]}$ for mass transport using a 2-layer feed-forward network with 512 hidden neurons and ReLU activation. An additional sinusoidal embedding\cite{tong2020trajectorynet} was applied for the parameter $t$. Other parameters were set similarly to the \textit{Gaussian Mixture} experiment. While both UOT-SB and UOT-GAN methods were not previously applied to the FFHQ dataset, we set up a grid search for the parameters and followed the instructions provided by the authors for parameter settings. For UOT-SB and UOT-GAN, we used a 3-layer neural network with 512 hidden neurons, and ReLU activation was used for the generator networks and the potential and discriminator, respectively. A learning rate of \(1 \times 10^{-5}\) with the Adam optimizer was used to train the networks in UOT-SB, and a learning rate equal to \(1 \times 10^{-4}\) was used for UOT-GAN. The methods were trained for 10k iterations with a batch size equal to 128, and the best results achieved during the evaluation steps were reported. Divergence used for UOT-SB is KL. All other parameters were set to the default values provided by the authors for CIFAR-10 generation tasks.



\section{Ablation Studies}
\label{app-ablation}

\textbf{Details about $f$-divergences between positive measures.} In the classic form, $f$-divergences are defined as measures of dissimilarity between two \textit{probability} measures. This definition should be revised when dealing with measures of arbitrary masses. Below we show that if the function $f$ is convex, non-negative, and attains zero uniquely at point $\{1\}$ then $\Df{f}{\mu_1}{\mu_2}$ is a valid measure of dissimilarity between two positive measures.

Let $\mu_1,\mu_2\in \mathcal{M}_{2,+}(\mathbb{R}^{d'})$ be two positive measures. The $f$-divergence $\Df{f}{\mu_1}{\mu_2}\geq 0$ which obvious from the non-negativity of $f$. From the definition of $\Df{f}{\mu_1}{\mu_2}$ and the fact that function $f$ attains zero uniquely  at a point $\{1\}$, we obtain that $\Df{f}{\mu_1}{\mu_2}=0$ if and only if $\mu_1=\mu_2$. Then from the definition of $\Df{f}{\mu_1}{\mu_2}$, its non-negativity and properties of function $f$ (it attains zero uniquely  at a point $\{1\}$) we get that $\Df{f}{\mu_1}{\mu_2}=0$ if and only if $\mu_1=\mu_2$. It completes the proof.

We assessed the performance of our solver with scaled $\text{D}_{\text{KL}}$ divergences in the main text, see \wasyparagraph\ref{sec-gaussian-exp}, \wasyparagraph\ref{sec-alae-exp}. For completeness, below we evaluate our solver with $\text{D}_{\chi^{2}}$ divergence in \textit{Gaussian Mixture} experiment. We use the same experimental setup as in \wasyparagraph\ref{sec-gaussian-exp}. Interestingly, the solver's results differ from those which we obtain for $\text{D}_{\text{KL}}$ divergence. For $\text{D}_{\chi^2}$ divergence, supports of learned plans' marginals constitute only parts of source and target measures' supports when $\tau=1$. The issue disappears with a slight increase of $\tau$, i.e., for $\tau=2$. At the same time, a further increase of $\tau$ is useless, since the learned plans fail to deal with class imbalance issue. Thus, parameter $\tau$ should be adjusted heuristically. In the case of $\text{D}_{\text{KL}}$ divergence, supports coincide for all $\tau$, see Fig. \ref{fig:gauss-kl}. This motivates us to use $\text{D}_{\text{KL}}$ divergences in our main experiments.

\begin{figure*}[t!]
\vspace{-3mm}
\centering
\begin{subfigure}[b]{0.235\linewidth}
    \centering
    \includegraphics[width=0.995\linewidth]{pics/toy_chi_10.png}
    \caption{U-LightOT, $\tau=10$}
    \label{fig:gauss-input}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.235\linewidth}
    \centering
    \includegraphics[width=0.995\linewidth]{pics/toy_chi_5.png}
    \caption{U-LightOT, $\tau=5$}
    \label{fig:gauss-lightsb}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.235\linewidth}
    \centering
    \includegraphics[width=0.995\linewidth]{pics/toy_chi_2.png}
    \caption{U-LightOT, $\tau=2$}
    \label{fig:gauss-mix}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.235\linewidth}
    \centering
    \includegraphics[width=0.995\linewidth]{pics/toy_chi_1.png}
    \caption{U-LightOT, $\tau=1$}
    \label{fig:gauss-ulightsb}
\end{subfigure}
\vspace{-1mm}\caption{\centering Conditional plans $\gamma_{\theta,\omega}(y|x)$ learned by our solver with scaled $\text{D}_{\chi^2}$ divergences in \textit{Gaussians Mixture} experiment ($\tau\in[1,2,5,10]$).}
\end{figure*}

\textit{Remark 1.} By definition, convex conjugates of $\text{D}_{\text{KL}}$ and $\text{D}_{\chi^2}$ divergences are proper, non-negative and non-decreasing. These properties are used in some of our theoretical results. 

\textit{Remark 2.} For \textit{scaled} divergences $\tau \mathcal{D}_f$ the convex conjugates of generator functions $\tau \cdot f$ equal to $\tau\cdot \overline{f}(\frac{t}{\tau})$ (it is a well-known property of $f$-divergences). Notably, in the case of scaled $\text{D}_{\chi^{2}}$ divergence, the changes are more tricky, i.e., $\overline{\tau \cdot f_{\chi^2}(t)}=\mathbb{I}_{t<-2\tau}\cdot (-\tau) + \mathbb{I}_{t\geq -2\tau}\cdot (0.25 t^2 + t)$.

\textbf{Parameters $\tau$, $\varepsilon$.} The effect of entropy regularization parameter $\varepsilon$ is well studied, see, e.g., \citep{gushchin2023entropic, korotin2024light}. Namely, increasing the parameter $\varepsilon$ stimulates the conditional distributions $\gamma_{\theta}(y|x)$ to become more dispersed. Still, below we provide an additional quantitative analysis of its influence on the learned translation. An ablation study on unbalancedness parameter $\tau$ in \textit{Gaussian Mixture} experiment is conducted in \wasyparagraph\ref{sec-gaussian-exp} and above in this section. However, one may naturally wonder \textit{how this parameter will influence the performance of our solver in image translation experiments?} To address this question, we learn the translations \textit{Young}$\rightarrow$ \textit{Adult}, \textit{Man}$\rightarrow$ \textit{Woman} on FFHQ dataset varying the parameters $\tau$, $\varepsilon$, see \wasyparagraph\ref{sec-alae-exp} for the experimental setup details. We test our solver with scaled $\text{D}_{\text{KL}}$ divergence training it for 3K iterations. Other hyperparameters are in Appendix \ref{sec:exp-details}.
In Tables \ref{table:ablation-acc-Y2A}, \ref{table:ablation-acc-M2W}, we report the accuracy of keeping the attributes of the source images (e.g., gender in \textit{Young}$\rightarrow$\textit{Adult} translation). In Tables \ref{table:ablation-fd-Y2A}, \ref{table:ablation-fd-M2W}, we report FD metrics which is defined as \textit{Frechet distance} between means and covariances of the learned and the target measures.

\begin{table}[!h]
\centering
\scriptsize
\begin{tabular}{ c|c c c c c c c} 
    \hline
    \diagbox{$\varepsilon$}{$\tau$} & $10$ & $20$ & $50$ & \underline{$\mathbf{10^2}$} & $10^3$ & $10^6$  \\
    \hline
	$0.01$ & $96.44 \pm 0.08$ & $96.09 \pm 0.07$ & $93.83 \pm 0.12$ & $90.30 \pm 0.95$ & $81.98 \pm 0.20$ & $80.05\pm 0.8$\\
	$0.1$& $94.90 \pm 0.34$ & $94.23 \pm 0.03$ & $91.85 \pm 0.47$ & $88.77 \pm 0.57$ & $80.85 \pm 0.77$ & $77.80\pm 0.28$\\
	$0.5$& 	$89.15 \pm 0.52$ & $87.42 \pm 0.40$ & $84.39 \pm 0.91$ & $80.32 \pm 0.40$ & $71.77 \pm 0.68$ & $71.21\pm0.57$\\
	$1.0$& $-$ & $81.20 \pm 0.44$ & $77.92 \pm 0.45$ & $75.09 \pm 0.21$ & $67.08 \pm 0.64$ & $66.12\pm 0.42$
 \\\hline
    \end{tabular}
     % \end{sc}
     \vspace{1mm}
     \caption{Test accuracy ($\uparrow$) of keeping the class in \textit{Young} $\rightarrow$ \textit{Adult} translation.}
    \label{table:ablation-acc-Y2A}
    \vspace{-4mm}
\end{table}

\begin{table}[!h]
\centering
\scriptsize
\begin{tabular}{ c|c c c c c c} 
    \hline
    \diagbox{$\varepsilon$}{$\tau$} & $10$ & $20$ & $50$ & \underline{$\mathbf{10^2}$} & $10^3$ & $10^6$  \\
    \hline
     	
	$0.01$ & $35.58 \pm 0.61$ & $28.48 \pm 1.20$ & $19.33 \pm 3.09$ & $15.12 \pm 1.92$ & $12.31 \pm 1.01$ & $10.95\pm 0.03$\\
	$0.1$	& $46.90 \pm 0.94$ & $40.76 \pm 1.68$ & $45.75 \pm 18.63$ & $28.01 \pm 0.77$ & $27.96 \pm 3.75$ & $27.68\pm 1.35$\\
	$0.5$	& $114.93 \pm 1.27$ & $105.48 \pm 0.66$ & $99.00 \pm 5.39$ & $92.65 \pm 0.79$ & $89.71 \pm 1.35$ & $89.28\pm 0.53$\\
	$1.0$	& $-$ & $168.18 \pm 3.75$& $150.95 \pm 0.55$& $143.08 \pm 0.69$& $136.61 \pm 0.97$& $137.26\pm 2.18$\\
 \hline
    \end{tabular}
     % \end{sc}
     \vspace{1mm}
     \caption{Test FD ($\downarrow$) of generated latent codes in \textit{Young} $\rightarrow$ \textit{Adult} translation.}
    \label{table:ablation-fd-Y2A}
    \vspace{-4mm}
\end{table}

\begin{table}[!h]
\centering
\scriptsize
\begin{tabular}{ c|c c c c c c c c} 
    \hline
    \diagbox{$\varepsilon$}{$\tau$} & $10$ & $20$ & $50$ & $10^2$ & \underline{$\mathbf{5\cdot 10^2}$}&$10^3$ & $10^6$  \\
    \hline
	$0.01$ & $96.33 \pm 0.35$ & $96.08 \pm 0.11$ & $93.28 \pm 0.65$ & $89.36 \pm 0.54$ & $83.85 \pm 0.84$& $81.57 \pm 1.36$ & $79.87 \pm 0.21$\\
	$0.1$& $94.79 \pm 0.13$ & $93.88 \pm 0.47$ & $91.60 \pm 0.25$ & $88.59 \pm 0.40$ & $81.14 \pm 1.56$& $78.70 \pm 0.84$ & $78.92 \pm 1.20$\\
	$0.5$& 	$88.75 \pm 0.72$ & $88.11 \pm 0.14$ & $86.03 \pm 0.72$ & $81.34 \pm 0.74$ &$75.09 \pm 1.02$& $73.71 \pm 1.16$ & $71.53 \pm 0.58$\\
	$1.0$& $-$ & $81.28 \pm 1.02$ & $78.72 \pm 0.89$ & $75.15 \pm 1.26$ & $70.99 \pm 0.68$& $68.26 \pm 1.75$ & $66.95 \pm 1.26$
 \\\hline
    \end{tabular}
     % \end{sc}
     \vspace{1mm}
     \caption{Test accuracy ($\uparrow$) of keeping the class in \textit{Man} $\rightarrow$ \textit{Woman} translation.}
    \label{table:ablation-acc-M2W}
    \vspace{-4mm}
\end{table}

\begin{table}[!h]
\centering
\scriptsize
\hspace{-15mm}
\begin{tabular}{ c|c c c c c c c} 
    \hline
    \diagbox{$\varepsilon$}{$\tau$} & $10$ & $20$ & $50$ & $10^2$ & \underline{$\mathbf{5\cdot 10^2}$} & $10^3$ & $10^6$  \\
    \hline
     	
	$0.01$ & \!\!\!$79.86 \pm 6.94$ \!\!\!& $68.80 \pm 3.17$ & \!\!\!$62.00 \pm 12.65$\!\!\! & $69.13 \pm 21.26$ & $40.30 \pm 7.22$& $38.20 \pm 3.63$ & $32.47 \pm 0.62$\\
	$0.1$	& \!\!\!$89.42 \pm 0.10$\!\!\! & $91.44 \pm 14.59$ & $67.07 \pm 2.53$ & $89.87 \pm 39.27$ & $50.36 \pm 0.89$& $67.79 \pm 13.59$ & $48.46 \pm 0.25$\\
	$0.5$	& \!\!\!$161.30 \pm 2.23$\!\!\! & \!\!\!$148.66 \pm 1.45$\!\!\! & $137.93 \pm 5.64$ & $125.50 \pm 0.38$ & $117.95 \pm 0.09$& $117.25 \pm 0.80$ & $130.64 \pm 21.15$\\
	$1.0$	& $-$ & \!\!\!$212.39 \pm 3.33$\!\!\!& \!\!\!$198.20 \pm 9.68$\!\!\!& $187.03 \pm 12.06$ & $186.33 \pm 15.09$& $174.32 \pm 13.00$& $163.90 \pm 0.28$\\
 \hline
    \end{tabular}
     % \end{sc}
     \vspace{1mm}
     \caption{Test FD ($\downarrow$) of generated latent codes in \textit{Man} $\rightarrow$ \textit{Woman} translation.}
    \label{table:ablation-fd-M2W}
    \vspace{-4mm}
\end{table}



\textit{Results} show that increase of $\varepsilon$ negatively influences both accuracy and FD of generated latent codes which is caused by an increased dispersity of $\gamma_{\theta}(y|x)$. At the same time, when $\tau$ \textit{increases}, the learned plans provide \textit{worse accuracy} for keeping the input latents' class but \textit{better FD} of generated latent codes. It is an expected behavior since for bigger $\tau$, the constraints on the marginals of the learned plans become more strict. That is, we enforce the marginals of the learned plans to be closer to source and target measures which allows learning more accurate mappings to target measure but does not allow keeping the source classes in the case of imbalance issues. Interestingly, in \textit{Young}$\rightarrow$\textit{Adult} translation, FD of learned latents do not change much for $\tau\geq10^2$ while accuracy exhibits a significant drop between $\tau=10^2$ and $\tau=10^3$. Thus, we can treat $\tau=10^2$ as optimal since it provides the best tradeoff between the quality of learned translations and their ability to keep the features of input latents. In the case of \textit{Man}$\rightarrow$\textit{Woman} translation, the values of accuracy and FD exhibit significant differences for considered $\tau$. It shows the necessity to consider a more detailed scale where we can choose $\tau=5\cdot 10^2$ as the optimal one. 

\textit{Remark.} FD should be treated as a \textit{relative} measure of similarity between learned and target measures. The results obtained by balanced solver \citep[LightSB]{korotin2024light} (equivalent to ours for big $\tau$) are considered as a gold standard. 



\section{Limitations and Broader Impact}
\label{app-limitations}
\textbf{Limitations.} One limitation of our solver is the usage of the Gaussian Mixture parametrization which might restrict the scalability of our solver. This points to the necessity for developing ways to optimize objective \eqref{unbalanced-eot-dual} with more general parametrization, e.g., neural networks. This is a promising avenue for future work. 

\textbf{Broader impact.} Our work aims to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.


\section{Additional Experimental Results}
\label{app-add-results}
We give additional illustrations for our U-LightOT solver and its alternatives applied in the latent space of ALAE autoencoder in Figures \ref{fig:alae-a2y},\ref{fig:alae-y2a}, \ref{fig:alae-m2w}, \ref{fig:alae-w2m}.

\begin{figure*}[h!]
\begin{center}
    \includegraphics[width=0.995\linewidth]{Appendix/Fig_YOUNG_TO_ADULT_10.png}
\end{center}
\vspace{-3mm}\caption{Comparison of solvers, \textit{Young} $\rightarrow$  \textit{Adult} translation.}
\label{fig:alae-y2a}
\end{figure*}

\begin{figure*}[h!]
\begin{center}
    \includegraphics[width=0.995\linewidth]{Appendix/Fig_ADULT_TO_YOUNG_10.png}
\end{center}
\vspace{-3mm}\caption{Comparison of solvers, \textit{Adult}$\rightarrow$\textit{Young} translation.}
\label{fig:alae-a2y}
\end{figure*}

\begin{figure*}[h!]
\begin{center}
    \includegraphics[width=0.995\linewidth]{Appendix/Fig_MAN_TO_WOMAN_10.png}
\end{center}
\vspace{-3mm}\caption{Comparison of solvers, \textit{Man}$\rightarrow$\textit{Woman} translation.}
\label{fig:alae-m2w}
\end{figure*}

\begin{figure*}[h!]
\begin{center}
    \includegraphics[width=0.995\linewidth]{Appendix/Fig_WOMAN_TO_MAN_10.png}
\end{center}
\vspace{-3mm}\caption{Comparison of solvers, \textit{Woman}$\rightarrow$\textit{Man} translation.}
\label{fig:alae-w2m}
\end{figure*}

\end{document}
