\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
\usepackage[preprint]{neurips_2022}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
    % \usepackage[final]{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[dvipsnames]{xcolor}         % colors
\usepackage[nointegrals]{wasysym}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{subcaption}

\usepackage[ruled,vlined]{algorithm2e}

\usepackage{amsmath,amssymb,amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\argsup}{arg\,sup}
\newcommand{\xmark}{\ding{55}}%

\usepackage{enumitem}

\everypar{\looseness=-1}


\title{Partial Neural Optimal Transport}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{
  Milena Gazdieva\\
  Skolkovo Institute of Science and Technology\\
  Moscow, Russia\\
  \texttt{milena.gazdieva@skoltech.ru} \\
   \And
  Alexander Korotin\\
  Skolkovo Institute of Science and Technology\\
  Artificial Intelligence Research Institute\\
  Moscow, Russia\\
  \texttt{a.korotin@skoltech.ru} \\
   \And
   Evgeny Burnaev \\
  Skolkovo Institute of Science and Technology\\
  Artificial Intelligence Research Institute\\
  Moscow, Russia\\
  \texttt{e.burnaev@skoltech.ru} \\
}


\begin{document}

\maketitle

\begin{abstract}
\looseness=-1
We propose a novel neural method to compute \textit{partial} optimal transport (OT) maps, i.e., OT maps between parts of measures of the specified masses. We test our partial neural optimal transport algorithm on synthetic examples.
\end{abstract}

\section{Introduction}
\vspace{-1mm}
Computing \textit{partial} optimal transport maps between measures is an important problem which arises in different tasks, e.g., in single-cell biology \citep{lubeck2022neural}. However, existing approaches tackle discrete setting \cite{bonneel2019spot} and, thus, do not provide out-of-train sample estimation. 
Due to this, recent methods for computing \textit{continuous} optimal transport maps using neural networks \citep{korotin2021neural, korotin2022kernel, gazdieva2022unpaired, fan2021scalable, rout2021generative, gazdieva2023extremal} are gaining increasing interest in the ML community. Among them, \citep{gazdieva2023extremal} solves the particular case of partial OT problem. Particularly, it proposes a method to learn a mapping between input measure and the closest part of the output measure w.r.t. to the given cost function $c(x,y)$. However, the problem of computing partial neural optimal transport maps has not not been yet covered.

We fill this gap by proposing a novel algorithm to compute \textit{partial} optimal transport maps between measures using neural networks, see \wasyparagraph\ref{sec:dual-form}, \wasyparagraph\ref{sec:algorithm}. The proposed method generalizes previously proposed one \cite{gazdieva2023extremal}. The approach is evaluated on toy 2D examples, see \wasyparagraph\ref{sec:toy-exp}.

\textbf{Notation.} We work in compact Polish spaces $\mathcal{X}$, $\mathcal{Y}$ and use $\mathcal{P(X)}$, $\mathcal{P(Y)}$ to denote the corresponding sets of probability measures on them. We denote the set of non-negative measures on $\mathcal{X}\times\mathcal{Y}$ as $\mathcal{M_+(X\times Y)}$. 
For a given measure $\gamma\in \mathcal{M_+(X \times Y)}$, $\gamma_x$ and $\gamma_y$ denote its projections on $\mathcal{X}$ and $\mathcal{Y}$ respectively. For a measurable map $T:\mathcal{X}\rightarrow\mathcal{Y}$, we denote the corresponding push-forward operator as $T_{\sharp}:\mathcal{P(X)}\rightarrow\mathcal{P(Y)}$.

\vspace{-1mm}
\section{Background}
\vspace{-2mm}
In this section, we provide an overview of the \textit{optimal transport} (OT) theory \cite{villani2003topics} concepts related to our paper.

\textbf{Conventional OT formulation.}
Let $\mathbb{P}\in \mathcal{P(X)}$, $\mathbb{Q}\in\mathcal{P(Y)}$ and $c(x,y):\mathcal{X}\times\mathcal{Y}\rightarrow \mathbb{R}$ be a continuous cost function. Then Monge's primal formulation of the OT cost is given by

\vspace{-4mm}
\begin{equation}
\text{Cost}(\mathbb{P},\mathbb{Q})\stackrel{def}{=}\inf_{T\sharp\mathbb{P}=\mathbb{Q}}\int_{\mathcal{X}}c\big(x,T(x)\big)d\mathbb{P}(x),
\label{ot-monge}
\end{equation}
where the infimum is taken over measurable $T:\mathcal{X}\rightarrow\mathcal{Y}$ mapping $\mathbb{P}$ to $\mathbb{Q}$ (see Fig. \ref{fig:ot-map}). Maps $T^*$ attaining the minimum are called \textit{optimal transport maps.}

\begin{figure}[h!]
\vspace{-2mm}
\begin{subfigure}[b]{0.45\linewidth}
    \begin{center}
         \includegraphics[width=0.95\linewidth]{pics/OT_map_def_pink.png}
    \end{center}
\caption{Monge's formulation.}
\label{fig:ot-map}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.45\linewidth}
    \begin{center}
         \includegraphics[width=0.95\linewidth]{pics/OT_plan_def_pink.png}
    \end{center}
\caption{Kantorovich's formulation.}
\label{fig:ot-plan}
\end{subfigure}
\vspace{-2mm}
\caption{\centering Formulations of OT problem.}
\end{figure}

\vspace{-2mm}
Monge's formulation does not allow mass splitting, i.e., for some $\mathbb{P}$, $\mathbb{Q}$ minimizer $T^*$ of \eqref{ot-monge} does not exist or even there may be no map satisfying $T_{\sharp}\mathbb{P}=\mathbb{Q}$. Thus, Kantorovich's relaxation \cite{kantorovitch1958translocation} is commonly considered (see Fig. \ref{fig:ot-plan}):
\begin{eqnarray}
\text{Cost}(\mathbb{P},\mathbb{Q})\stackrel{def}{=}\inf_{\pi\in\Pi(\mathbb{P},\mathbb{Q})}\int_{\mathcal{X}\times\mathcal{Y}}c(x,y)d\pi(x,y).
\label{ot-kantorovich}
\end{eqnarray}
Here the infimum is taken over \textit{transport plans} $\Pi(\mathbb{P}, \mathbb{Q})$, i.e., measures $\pi\in\mathcal{P(X\times Y)}$ satisfying $\pi_x=\mathbb{P}$, $\pi_y=\mathbb{Q}$. There always exists a plan $\pi^*$ attaining the minimum in \eqref{ot-kantorovich} which is called an \textit{optimal transport plan}.

\begin{wrapfigure}{r}{0.45\textwidth}
    \vspace{-8mm}
    \begin{center}
         \includegraphics[width=0.95\linewidth]{pics/OT_plan_def_partial_pink.png}
    \end{center}
\vspace{-2mm}
\caption{Partial Kantorovich's formulation.}
\label{fig:ot-partial}
\vspace{-13mm}
\end{wrapfigure}
\textbf{Partial OT formulation.}
Consider a problem of transpoting mass $m$ between measures $w_0 \mathbb{P}$ and $w_1 \mathbb{Q}$ for $w_0, w_1 \geq m \geq 0$. Then the partial OT problem \cite{figalli2010optimal, caffarelli2010free} is defined by (see Fig. \ref{fig:ot-partial})
\begin{eqnarray}
\text{Cost}_{m}(w_0\mathbb{P},w_1\mathbb{Q}) \stackrel{def}{=} \nonumber\\ \inf_{\gamma \in \Pi_m(w_0\mathbb{P}, w_1\mathbb{Q})}\int_{\mathcal{X}\times\mathcal{Y}}c(x,y)d\gamma(x,y)
\label{not-partial-ot-kantorovich}
\end{eqnarray}
where $\Pi_m(w_0\mathbb{P}, w_1\mathbb{Q})$ is the set of non-negative measures $\gamma\in\mathcal{M_+}(\mathcal{X}\times\mathcal{Y})$ such that $\gamma_x\leq w_0 \mathbb{P}$, $\gamma_y\leq w_1\mathbb{Q}$ and $\int 1 d\gamma(x, y) = m$, i.e., the mass of $\gamma$ is $m$.

\section{A Neural Algorithm to Compute Partial Optimal Transport}
In \wasyparagraph\ref{sec:dual-form}, we derive the dual problem which can be used to recover the partial optimal transport map. In \wasyparagraph\ref{sec:algorithm}, we propose a computational algorithm to solve this problem using neural networks.

\vspace{-1mm}
\subsection{Dual Form}
\label{sec:dual-form}

\begin{algorithm}[t!]
\SetInd{0.5em}{0.3em}
    {
        \SetAlgorithmName{Algorithm}{empty}{Empty}
        \SetKwInOut{Input}{Input}
        \SetKwInOut{Output}{Output}
        \Input{measures
        $\mathbb{P},\mathbb{Q}$ accessible by samples;\\ transport map $T_{\theta}\!:\!\mathcal{X}\!\rightarrow\!\mathcal{Y}$; potential $f_{\psi}:\!\mathcal{X}\!\rightarrow\!\mathbb{R}_{-}$; \\scaling factor $\xi_{w}\!:\!\mathcal{X}\!\rightarrow\![0, w_0]$; multiplier $\lambda\!:\!\mathbb{R}\!\rightarrow\!\mathbb{R}$;\\
        transport cost $c:\mathcal{X}\times\mathcal{Y}\rightarrow\mathbb{R}$; mass $m\geq 0$; weights $w_0, w_1\geq m\geq 0$;\\
        number $K_{T}$ of inner iterations;\\
        }
        \Output{approximate partial OT map  $T_{\theta}$; scaling factor $\xi_w$;}
        }
        
        \Repeat{not converged}{
            Sample batches $X\sim\mathbb{P}$, $Y\!\sim\! \mathbb{Q}$\;
            $\mathcal{L}_{f}\leftarrow  w_1 \cdot \frac{1}{|Y|}\sum\limits_{y\in Y}f_{\psi}(y) - \frac{1}{|X|}\sum\limits_{x\in X} \big[\xi_w(x) \cdot f_{\psi}\big(T_{\theta}(x)\big)+ \lambda \cdot(\xi_w(x)-m) \big]$\;
            Update $\psi$, $\lambda$ by using $\frac{\partial \mathcal{L}_{f}}{\partial \psi}$, $\frac{\partial \mathcal{L}_{f}}{\partial \lambda}$ respectively \textit{to maximize} $\mathcal{L}_{f}$\;
            \For{$k_{T} = 1,2, \dots, K_{T}$}{
                Sample batch $X\sim \mathbb{P}$\;
                ${\mathcal{L}_{T}\leftarrow\frac{1}{|X|}\sum\limits_{x\in X}\big[c
                \big(x, T_{\theta}(x)\big)- \xi_w(x) \cdot f_{\psi}\big(T_{\theta}(x)\big) + \lambda \cdot(\xi_w(x)-m)\big]}$\;
            Update $\theta$, $w$ by using $\frac{\partial \mathcal{L}_{T}}{\partial \theta}$, $\frac{\partial \mathcal{L}_{T}}{\partial w}$ respectively \textit{to minimize} $\mathcal{L}_{T}$\;
            }
        }
        \caption{Procedure to compute the neural partial OT map between $\mathbb{P}$ and $\mathbb{Q}$ for transport cost $c(x,y)$, weights $w_0$, $w_1$ and mass $m$.}
        \label{algorithm-pot}
\end{algorithm}


\begin{theorem}[Dual form of partial OT] 
It holds
\begin{eqnarray}
\label{dual-form}
\vspace{-1mm}
    \text{\normalfont Cost}_{m}(w_0\mathbb{P},w_1\mathbb{Q}) =
    \max_{\substack{f\leq 0\\\lambda\in\mathbb{R}}} \min_{\substack{\gamma_x \leq w_0 \mathbb{P}  \\ 
    \gamma \geq 0}}  \int_{\mathcal{X}\times\mathcal{Y}} c(x, y) d \gamma (x, y) - \int_{\mathcal{Y}} f(y) d \gamma_y(y) + \\w_1 \int_{\mathcal{Y}} f(y) d \mathbb{Q} (y) + \lambda\big(\int_{\mathcal{X}} 1 d\gamma_x(x)- m\big)\stackrel{def}{=} \max_{\substack{f\leq 0\\\lambda\in\mathbb{R}}} \min_{\substack{\gamma_x \leq w_0 \mathbb{P}  \\ 
    \gamma \geq 0}}\widetilde{\mathcal{L}}(\{f, \lambda\}, \gamma),\nonumber
\end{eqnarray}
where the maximization is performed over non-positive continuous $f:\mathcal{Y}\rightarrow \mathbb{R}_{-}$ and scalar $\lambda\in\mathbb{R}$, and minimization $-$ over non-negative measures $\gamma$ whose projection on $\mathcal{X}$ is bounded by $w_0\mathbb{P}$.
\end{theorem}
 For convenience, we denote the functional under the $\max\min$ by $\widetilde{\mathcal{L}}(\{f, \lambda\}, \gamma)$.

\begin{lemma}[Partial OT plans are contained in solutions of the saddle-point problem] Let $\{f^*, \lambda^*\}$ be any optimal pair $\{f^*, \lambda^*\}\in\arg\max_{f,\lambda} [\min_{\gamma}\widetilde{\mathcal{L}}(\{f, \lambda\}, \gamma)]$ in \eqref{dual-form}. Let $\gamma^*$ be any partial OT plan between $w_0\mathbb{P}$ and $w_1\mathbb{Q}$. Then we have
\begin{equation}
    \gamma^*\in \argmin_{\substack{\gamma_x \leq w_0 \mathbb{P}  \\ 
    \gamma \geq 0}} \widetilde{\mathcal{L}}(\{f^*, \lambda^*\}, \gamma).
\end{equation}
\label{partial-ot-in-saddle}
\end{lemma}

Our Lemma \ref{partial-ot-in-saddle} shows that for \textit{some} optimal saddle points $(\{f^{*}, \lambda^*\}, \gamma^{*})$ of \eqref{dual-form} it holds that $\gamma^{*}$ is the partial OT plan between $w_0\mathbb{P}$ and $w_1\mathbb{Q}$. In general, the $\argmin_{\gamma}$ set for an optimal $\{f^{*}, \lambda^*\}$ might contain not only partial OT plans $\gamma^{*}$, but other functions as well (\textit{fake solutions}~\citep{korotin2022kernel}).

\vspace{-3mm}
\subsection{Computational Algorithm}
\label{sec:algorithm}
\vspace{-1mm}
Thanks to the condition $\gamma_x \leq w_0 \mathbb{P}$, the measure $\gamma_x$ is absolutely continuous w.r.t. the measure $\mathbb{P}$. Thus, $\gamma_x$ can be presented as $d\gamma_x(x)=\xi(x)d\mathbb{P}(x)$ where $\xi(x)$ is a measurable scalar-valued function $\xi:\mathcal{X}\rightarrow[0,w_0]$. This approach of the scaling factor parametrization is inspired by \citep{yang2018scalable}.
Then we get the following saddle-point optimization problem:

\vspace{-4mm}
\begin{eqnarray}
    \text{\normalfont Cost}_{m}(w_0\mathbb{P},w_1\mathbb{Q}) =
    \max_{\substack{f\leq 0\\\lambda\in\mathbb{R}}} \min_{\substack{T:\mathcal{X}\rightarrow\mathcal{Y} \\ \xi(x) \in [0, w_0]}} \int_{\mathcal{X}} c(x, T(x)) \xi(x)d \mathbb{P}(x) - \int_{\mathcal{X}} f(T(x))\xi(x)d \mathbb{P}(x) + \nonumber\\ 
    w_1 \int_{\mathcal{Y}} f(y) d \mathbb{Q} (y) + \lambda\big(\int_{\mathcal{X}} \xi(x) d\mathbb{P}(x)- m\big) \stackrel{def}{=}\max_{\substack{f\leq 0\\\lambda\in\mathbb{R}}} \min_{\substack{T:\mathcal{X}\rightarrow\mathcal{Y} \\ \xi(x) \in [0, w_0]}}\mathcal{L}(\{f, \lambda\}, \{T, \xi\}).
    \label{main-objective}
\end{eqnarray}
\vspace{-4mm}


\begin{corollary}
\label{map-in-saddle-points}
Let $\{f^*, \lambda^*\}$ be any maximizer in \eqref{main-objective}. Assume that there exists a deterministic plan $\gamma^*$ solving problem \eqref{dual-form}. It means that $\exists T^*:\mathcal{X}\rightarrow\mathcal{Y}$ and $\exists \xi^*:\mathcal{X}\rightarrow[0,w_0]$ such that $d\gamma^*(x,y)=\xi^*(x)d\mathbb{P}(x) d\delta_{T^*(x)}$. Then
\vspace{-1mm}
\begin{equation}
    \{T^*, \xi^*\} \in \argmin_{\substack{T:\mathcal{X}\rightarrow\mathcal{Y} \\ \xi(x) \in [0, w_0]}} \mathcal{L}\big(\{f^*, \lambda^*\}, \{T, \xi\}\big)
\end{equation}
\vspace{-6mm}
\end{corollary}

Our Proposition \ref{map-in-saddle-points} states that partial OT maps $\{T^*, \xi^*\}$ belong to some optimal solutions $(\{f*, \lambda^*\}, \{T^*, \xi^*\})$ of the saddle point problem \eqref{main-objective}. But the set of optimal solutions might also contain other \textit{fake solutions}.

To solve the optimization problem \eqref{main-objective}, we approximate the map $T$, potential $f$, scaling factor $\xi$ with networks $T_{\theta}$, $f_{\psi}$, $\xi_w$ respectively. The nets are trained using stochastic gradient ascent-descent and random batches from $\mathbb{P},\mathbb{Q}$. The optimization precedure is detailed in our Algorithm \ref{algorithm-pot}.

\vspace{-3mm}
\section{Evaluation.} 
\vspace{-2mm}
In \wasyparagraph\ref{sec:toy-exp}, we provide toy 2D examples. We write the code using \texttt{PyTorch} framework.% and will be made publicly available. 

\textbf{Transport costs.} We experiment with the quadratic cost $c(x,y)=\ell^{2}(x,y)$.

\textbf{Neural networks.} We use fully connected networks both for the mapping $T_{\theta}$, potential $f_{\psi}$ and scaling factor $\xi_w$. To make outputs of $f_{\psi}$ non-positive, we initialize its final layer as $x\mapsto -|x|$. We make $\xi_w$ bounded by $w_0$ by using x $\rightarrow w_0 \frac{Tanh(x)+1}{2}$ as the last layer.
We set multiplier $\lambda$ to be a learnable scalar.

\textbf{Optimization.} We use Adam \citep{kingma2014adam} optimizer both for $T_{\theta}$, $f_{\psi}$, $\xi_w$ and $\lambda$. We set learning rate as $lr=10^{-4}$ and use the default betas. We set $K_T = 10$.

\vspace{-2mm}
\subsection{Toy 2D examples}
\vspace{-1mm}
\label{sec:toy-exp}
In this section, we provide 2D examples: \textit{Gaussians}, \textit{Symmetric Wi-Fi}. %, \textit{Strong Accept}. 
We show that setting the mass parameter $m$ and weights $w_0$, $w_1$, we obtain mappings between corresponding parts of the input and output measures, i.e., $\frac{m}{w_0}$ mass of $\mathbb{P}$ is mapped to $\frac{m}{w_1}$ mass of $\mathbb{Q}$.

In \textit{'Gaussians'} experiment, we provide learned partial neural OT map for the parameters $w_0=2, w_1=2, m=1$. We visualize the results in Fig. \ref{fig:gaussian_res}.

In \textit{'Symmetric Wi-Fi'} experiment (Fig. \ref{fig:part_wifi_res}) we define input and output measures as 3 archs. We set the partial neural OT parameters as $w_0=3, w_1=3, m=2$. We see from Fig. \ref{fig:part_wifi_res} that partial neural OT method learns a mapping between 2 archs of $\mathbb{P}$ and 2 archs of $\mathbb{Q}$.

For completeness, in Fig. \ref{fig:gauss-dpot}, \ref{fig:wifi-dpot}, we provide the pairings obtained by discrete partial OT method on empirical samples. In 2D, it can be considered as a fine approximation of ground truth.

\begin{figure}[!t]
\begin{subfigure}{0.265\textwidth}
  \begin{center}
    \includegraphics[width=\linewidth]{pics/gauss_res_0.png}
  \end{center}
\vspace{1mm}
  \caption{Input, output measures.}
  \label{fig:gauss-gt}
\end{subfigure}
\hfill
\begin{subfigure}{0.236\textwidth}
  \begin{center}
    \includegraphics[width=\linewidth]{pics/gauss_res_1.png}
  \end{center}
\vspace{-2.5mm}
  \caption{\centering Rejected samples and \protect\linebreak learned scaling factor.}
  \label{fig:gauss-sf}
\end{subfigure}
\hfill
\begin{subfigure}{0.236\textwidth}
  \begin{center}
    \includegraphics[width=\linewidth]{pics/gauss_res_2.png}
  \end{center}
\vspace{1mm}
  \caption{Partial neural OT.}
  \label{fig:gauss-npot}
\end{subfigure}
\hfill
\begin{subfigure}{0.236\textwidth}
  \begin{center}
    \includegraphics[width=\linewidth]{pics/gauss_res_3.png}
  \end{center}
\vspace{1mm}
  \caption{\centering Discrete partial OT.}
  \label{fig:gauss-dpot}
\end{subfigure}
\vspace{-1mm}
\caption{\centering Partial OT maps learned by our Algorithm \ref{algorithm-pot} in \textit{'Gaussians'} experiment.}
\label{fig:gaussian_res}
\end{figure}


 \begin{figure}[!t]
 \vspace{-2mm}
\begin{subfigure}{0.265\textwidth}
  \begin{center}
    \includegraphics[width=\linewidth]{pics/symm_wifi_res_0.png}
  \end{center}
\vspace{1mm}
  \caption{Input, output measures.}
  \label{fig:wifi-gt}
\end{subfigure}
\hfill
\begin{subfigure}{0.236\textwidth}
  \begin{center}
    \includegraphics[width=\linewidth]{pics/symm_wifi_res_1.png}
  \end{center}
\vspace{-2.5mm}
  \caption{\centering Rejected samples and \protect\linebreak learned scaling factor.}
  \label{fig:wifi-sf}
\end{subfigure}
\hfill
\begin{subfigure}{0.236\textwidth}
  \begin{center}
    \includegraphics[width=\linewidth]{pics/symm_wifi_res_2.png}
  \end{center}
\vspace{1mm}
  \caption{Partial neural OT.}
  \label{fig:wifi-npot}
\end{subfigure}
\hfill
\begin{subfigure}{0.236\textwidth}
  \begin{center}
    \includegraphics[width=\linewidth]{pics/symm_wifi_res_3.png}
  \end{center}
\vspace{1mm}
  \caption{\centering Discrete partial OT.}
  \label{fig:wifi-dpot}
\end{subfigure}
\vspace{-1mm}
\caption{\centering Partial OT maps learned by our Algorithm \ref{algorithm-pot} in \textit{'Symmetric Wi-Fi'} experiment.}
\label{fig:part_wifi_res}
\vspace{-6mm}
\end{figure}

\vspace{-3mm}
\section{Related Work}
\vspace{-3mm}
In recent years, the neural optimal transport methods \citep{korotin2019wasserstein, rout2021generative, gazdieva2022unpaired, korotin2021neural, fan2021scalable, asadulaev2022neural, gushchin2022entropic, gazdieva2023extremal} have been actively developed. Among these methods, ours is mostly related to \citep{gazdieva2023extremal}. This paper proposes an algorithm to learn a mapping between \textit{whole} input distribution and the closest (w.r.t. chosen cost function $c(x,y)$) \textit{part} of the target distribution. It is a particular case of our method for the parameters $\xi(x)\equiv1,w_0=1, m=1$.

\vspace{-3mm}
\section{Future Work}
\vspace{-2mm}
The problem of learning a mapping between measures of unequal mass arises in important biological task of predicting cells' responses to drugs during cancer treatment \cite{lubeck2022neural}. We plan to consider application of the proposed partial neural optimal method to this task in our future work.

\bibliography{references}
\bibliographystyle{plain}

\end{document}