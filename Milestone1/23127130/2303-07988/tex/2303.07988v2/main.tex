\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{wasysym}
\usepackage{cancel}
\usepackage[dvipsnames]{xcolor}
\usepackage{multirow}
\usepackage{diagbox}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       %professional-quality tables

\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{units}
\usepackage[normalem]{ulem}


\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclarePairedDelimiter{\norm}{\lVert}{\rVert} 

\usepackage{enumitem}
\setlist{leftmargin=*,itemsep=0pt}

\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\argsup}{arg\,sup}
\newcommand{\xmark}{\ding{55}}

\newcommand{\alex}[1]{{\scriptsize\textbf{\color{red} AK: #1}}}
\newcommand{\milena}[1]{{\scriptsize\textbf{\color{blue} MG: #1}}}
\newcommand{\arip}[1]{{\scriptsize\textbf{\color{orange} AA: #1}}}
\newcommand{\KL}[2]{\text{KL}\left(#1\Vert #2\right)}
\newcommand{\UKL}[2]{\text{UKL}\left(#1\Vert #2\right)}
\newcommand{\Df}[3]{D_{#1}\left(#2\Vert #3\right)}
\newcommand*{\defeq}{\stackrel{\text{def}}{=}}

\allowdisplaybreaks
\everypar{\looseness=-1}


\icmltitlerunning{Unbalanced and Light Optimal Transport}

\begin{document}

\twocolumn[
\icmltitle{Unbalanced and Light Optimal Transport}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Milena Gazdieva}{sk}
\icmlauthor{Arip Asadulaev}{airi,itmo}
\icmlauthor{Evgeny Burnaev}{sk,airi}
\icmlauthor{Alexander Korotin}{sk,airi}
\end{icmlauthorlist}

\icmlaffiliation{sk}{Skolkovo Institute of Science and Technology, Moscow, Russian Federation}
\icmlaffiliation{airi}{Artificial Intelligence Research Institute, Moscow, Russian Federation}
\icmlaffiliation{itmo}{ITMO University, Saint Petersburg, Russian Federation}

\icmlcorrespondingauthor{Milena Gazdieva}{milena.gazdieva@skoltech.ru}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution

\begin{abstract} 
While the field of continuous Entropic Optimal Transport (EOT) has been actively developing in recent years, it became evident that the classic EOT problem is prone to different issues like the sensitivity to outliers and imbalance of classes in the source and target measures. This fact inspired the development of solvers which deal with the \textit{unbalanced} EOT (UEOT) problem $-$ the generalization of EOT allowing for mitigating the mentioned issues by relaxing the marginal constraints. Surprisingly, it turns out that the existing solvers are either based on heuristic principles or heavy-weighted with complex optimization objectives involving several neural networks. We address this challenge and propose a novel theoretically-justified and lightweight unbalanced EOT solver. Our advancement consists in developing a novel view on the optimization of the UEOT problem yielding tractable and non-minimax optimization objective. We show that combined with a light parametrization recently proposed in the field our objective leads to fast, simple and effective solver. It allows solving the continuous UEOT problem in minutes on CPU. We provide illustrative examples of the performance of our solver.
\end{abstract}

\section{Introduction}
\label{sec-introduction}
During the recent years, the field of entropic optimal transport \citep[EOT]{cuturi2013sinkhorn} has witnessed a lot of transformations. The majority of early works in the field are built upon solving the EOT problem between discrete probability measures \citep{peyre2019computational}. Subsequently, the advances in the field of generative models have led to explosive interest from the ML community in developing the \textbf{continuous} EOT solvers, see \citep{gushchin2023building} for a survey. The setup of this problem assumes that the learner needs to estimate the EOT plan between continuous measures given only empirical samples of data from them. Such solvers found extensive applications in various areas, including image-to-image translation \citep{mokrov2024energy,gushchin2023entropic}, image generation \citep{wang2021deep,de2021diffusion,chen2021likelihood} and biological data transfer \citep{koshizuka2022neural,vargas2021solving}.

At the same time, various researches attract attention to the shortcomings of the classic EOT problem. In this formulation, EOT enforces hard constraints on the marginal measures, thus, does not allow for mass variations and sets equal weights for all samples. As a result, EOT shows high sensitivity to outliers and imbalance of classes in the source and target measures \citep{balaji2020robust} which are almost inevitable for large-scale datasets. In order to overcome these issues, it is common to consider extensions of EOT problem, e.g., unbalanced EOT (UEOT) \citep{chizat2017unbalanced, liero2018optimal}. The UEOT formulation allows for variation of total mass and individual samples' weights by relaxing the marginal constraints through the use of divergences.

The scope of our paper is the continuous UEOT problem. It seems that in this field, a solver that is fast, light, and theoretically justified has not yet been developed. Indeed, many of the existing solvers follow a kind of heuristical principles and are based on the solutions of discrete OT. For example, \citep{lubeck2022neural} use a regression to interpolate the discrete solutions, and \citep{eyring2023unbalancedness} build a flow matching upon them. Almost all of the other solvers \citep{choi2023generative, yang2018scalable} employ several neural networks with many hyper-parameters and require time-consuming optimization procedures. We solve the aforementioned shortcomings by introducing a novel lightweight solver which can play a role of simple baseline for unbalanced EOT.

\textbf{Contributions.} We develop a novel \textit{lightweight} solver to estimate continuous \textbf{unbalanced} EOT couplings between probability measures (\wasyparagraph\ref{sec-method}). Our solver has a non-minimax optimization objective and employs the Gaussian mixture parametrization for the EOT plans. As a result, for measures of moderate dimensions.
We experiment with our solver on illustrative synthetic tasks (\wasyparagraph \ref{sec-gaussian-exp}) and the unpaired image-to-image translation (\wasyparagraph\ref{sec-alae-exp}).

\textbf{Notations.} We work in the Euclidian space $(\mathbb{R}^d, \|\cdot\|)$. We use $\mathcal{P}_{2,ac}(\mathbb{R}^d)$ (or $\mathcal{M}_{2,+}(\mathbb{R}^d)$) to denote the set of absolutely continuous Borel probability (or non-negative) measures on $\mathbb{R}^d$ with finite second moment and differential entropy. For any $p\in \mathcal{P}_{2,ac}(\mathbb{R}^d)$ (or $\mathcal{M}_{2,+}(\mathbb{R}^d)$), we use $p(x)$ to denote its density at a point $x\in\mathbb{R}^d$.
For a given measure $\gamma\in \mathcal{M}_{2,+}(\mathbb{R}^d \times \mathbb{R}^d)$, we denote its total mass by $\|\gamma\|_1\defeq\int_{\mathbb{R}^d}\gamma(x, y) dx dy$. We use $\gamma_x(x)$ and $\gamma_y(y)$ to denote the marginals of $\gamma(x,y)$. They statisfy the equality $\|\gamma_x\|_1=\|\gamma_y\|_1=\|\gamma\|_1$. We write $\gamma(y|x)$ to denote the conditional probability measure. Each such measure has a unit total mass.
For two measures $p\in \mathcal{M}_{2,+}(\mathbb{R}^d)$ and $q\in \mathcal{M}_{2,+}(\mathbb{R}^d)$, the $f$-divergence  between them is defined as $\Df{f}{p}{q} \defeq \int_{\mathbb{R}^d} f(\nicefrac{p(x)}{q(x)})q(x) dx$. We use $\overline{f}$ to denote the Fenchel conjugate of a convex function $f$: $\overline{f}(t)=\sup_{u\in \mathbb{R}^d} \{ut-f(u)\}$. For two probability measures $p,q\in\mathcal{P}_{2,ac}(\mathbb{R}^{D})$, we use $\KL{p}{q}$ to denote the Kullback-Leibler divergence between them. In turn, we use $H(p)$ to denote the differential entropy of $p$. To avoid any confusion, we apply KL or $H$ exclusively to \textit{probability} measures.

\section{Background}
\label{sec-background}
Here we give an overview of the relevant entropic optimal transport (EOT) concepts. For additional details on balanced EOT, we refer to \citep{cuturi2013sinkhorn, genevay2019entropy, peyre2019computational}, unbalanced EOT - \citep{chizat2017unbalanced, liero2018optimal}.

\textbf{Unbalanced KL divergence and differential entropy}. We start with recalling an auxiliary concept - the \textit{Unbalanced Kullback-Leibler divergence} (UKL). For two positive measures $\gamma_1,\gamma_2\in\mathcal{M}_{2,+}(\mathbb{R}^d\times \mathbb{R}^d)$, UKL between them is
\begin{eqnarray*}
    \UKL{\gamma_1}{\gamma_2}\!\defeq \!\|\gamma_1\|_1\!\cdot\! \bigg[\!\KL{\frac{\gamma_1}{\|\gamma_1\|_1}}{\frac{\gamma_2}{\|\gamma_2\|_1}}\!+\!h\big(\!\frac{\|\gamma_{2}\|_1}{\|\gamma_{1}\|_1}\!\big)\!\bigg],
\end{eqnarray*}
where $h(a)\defeq a-1-\log a$. The latter is a convex and non-negative function attaining zero uniquely at $a=1$. KL term here is the usual Kullback-Leibler divergence computed between normalized measures, i.e., it is always non-negative and equals to zero only when $\gamma_{1}/\|\gamma_{1}\|_{2}=\gamma_{2}/\|\gamma_{2}\|_{2}$. 
Therefore, it follows that $\UKL{\gamma_1}{\gamma_2}$ is always non-negative. Furthermore, it equals to zero only when $\gamma_1=\gamma_2$. These facts make UKL a valid measure of dissimilarity to compare two \textit{positive} measures. In particular, when $\gamma_{1},\gamma_{2}$ are probability measures (i.e., $\|\gamma_{1}\|_{1}=\|\gamma_{2}\|_{1}$), UKL just coincides with the usual notion of KL.

For $\gamma\in\mathcal{M}_{2,+}(\mathbb{R}^{D})$ we define its unbalanced entropy:
\begin{equation}
    H_u(\gamma)\defeq \|\gamma\|_{1}\big[H(\frac{\gamma}{\|\gamma\|_1})-h(\|\gamma\|_{1}^{-1})\big],
    \nonumber
\end{equation}
which boils to the usual differential entropy when $\|\gamma\|_{1}=1$. 

\textbf{Classic EOT formulation (with the quadratic cost).}
Consider two probability measures $p\!\in\! \mathcal{P}_{2,ac}(\mathbb{R}^d)$, $q\!\in\!\mathcal{P}_{2,ac}(\mathbb{R}^d)$. For $\varepsilon>0$, the EOT problem between $p$ and $q$ consists in finding a minimizer of

\vspace{-5mm}\begin{eqnarray}
% \text{EOT}_{\varepsilon}(p, q) = 
\min_{\pi\in\Pi(p, q)} \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \frac{\|x-y\|^2}{2} \pi(x, y) dx dy - \varepsilon H(\pi),
\label{eot-classic}
\end{eqnarray}
where $\Pi(p,q)$ is the set of probability measures $\pi\in\mathcal{P}_{2,ac}(\mathbb{R}^d\times \mathbb{R}^d)$ with marginals $p$ and $q$ (transport plans), $H(\pi)$ is the differential entropy. Plan $\pi^*$ attaining the minimum exists, it is unique and called the \textit{EOT plan.} 

The EOT problem may be sensitive to outliers \citep{balaji2020robust} which are highly probable in real-world datasets. Moreover, it might not handle potential measure shifts such as class imbalances. Therefore, it is common to consider the unbalanced EOT problem \citep{yang2018scalable, choi2023generative} which overcomes these issues by relaxing the marginal constraints \citep{sejourne2022unbalanced}.



\textbf{Unbalanced EOT formulation (with the quadratic cost).} 
Let $D_{f_1}$ and $D_{f_2}$ be two $f$-divergences over $\mathbb{R}^d$.
For two probability measures $p\in \mathcal{P}_{2,ac}(\mathbb{R}^d)$, $q\in\mathcal{P}_{2,ac}(\mathbb{R}^d)$ and $\varepsilon>0$, the unbalanced EOT problem between $p$ and $q$ consists of finding a minimizer of
\begin{eqnarray}
    \inf_{\gamma\in\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d)} \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \frac{\|x-y\|^2}{2} \gamma(x, y) dx dy - \nonumber\\
     \varepsilon H_u(\gamma)\!+\mathcal{D}_{f_1}(\gamma_x, p)  + \mathcal{D}_{f_2}(\gamma_y, q).
     \label{unbalanced-eot-primal}
\end{eqnarray}

Here the minimum is attained for a unique $\gamma^*$ which is called the \textit{unbalanced optimal entropic} (UEOT) plan. Typical choices of $f_i$ are $f_i(x)=\lambda_i x\log x$ or $f_i(x)=\lambda_i (x-1)^2$ ($\lambda_i>0$) yielding the scaled KL and $\chi^2$ divergences, respectively. 

\textit{Remark.} The balanced EOT problem \eqref{eot-classic} is a special case of \eqref{unbalanced-eot-primal}. Indeed, let $f_1$ and $f_2$ be the convex indicators of $\{1\}$, i.e., $f_1(x)=f_2(x)=\mathbb{I}_{x=1} \cdot 0 + \mathbb{I}_{x\neq 1}\cdot \infty$. Then the $f$-divergences $\Df{f_1}{\gamma_x}{p}$ and $\Df{f_2}{\gamma_y}{q}$ become infinity if $p\neq\gamma_x$ or $q\neq\gamma_y$, and become zeros otherwise.

\textbf{Dual form of unbalanced EOT problem \eqref{unbalanced-eot-primal}} is
\begin{eqnarray}
    \!\sup_{(\phi, \psi)} \!\Bigg\lbrace\!\!\! -\!\!\varepsilon \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \!\exp\{\frac{1}{\varepsilon} \!( \phi(x) + \psi(y)\!-\! \frac{\|x-y\|^2}{2}) \} dx dy \!- \!\nonumber\\
    \int_{\mathbb{R}^d} \overline{f}_1 (-\phi(x))p(x) dx \! - \!\int_{\mathbb{R}^d} \overline{f}_2 (-\psi(y))q(y) dy
    \Bigg\rbrace\defeq\mathcal{L}^{*}.\!
    \label{unbalanced-eot-dual}
\end{eqnarray}
There exists a pair of \textit{optimal} potentials $(\phi^*, \psi^*)$ delivering maximum to this problem.
Optimal potentials have the following connection with the solution of the problem \eqref{unbalanced-eot-primal}:
\begin{eqnarray}
\label{gamma-potentials}
    \gamma^*(x, y) \!=\! \exp\{\frac{\phi^*(x)}{\varepsilon}\} \!\exp\{-\frac{\|x-y\|^2}{2\varepsilon}\} \!\exp\{\frac{\psi^*(y)}{\varepsilon}\}.
\end{eqnarray}


\textbf{Computational UEOT setup.} Analytical solution for the \textit{unbalanced} EOT problem is, in general, not known.\footnote{Analytical solutions are known only for some specific cases. For example, \citep{janati2020entropic} consider the case of Gaussian measures and unbalanced EOT problem with KL divergence instead of differential entropy. This setup is different from ours.}
Moreover, in real-world setups where unbalanced EOT is applicable, the measures $p$, $q$ are typically not available explicitly but only through their empirical samples (datasets). Below we summarize the learning setup which we consider.

We assume that data measures $p, q\in \mathcal{P}_{2,ac}(\mathbb{R}^d)$ are unknown and accessible only by a limited number of i.i.d. empirical samples $\{x_0, ..., x_N\}\sim p$, $\{y_0, ..., y_M\}\sim q$. Our aim is to approximate the optimal UEOT plans solving \eqref{unbalanced-eot-primal} between the entire measures $p, q$. The recovered plans should allow the out-of-sample estimation, i.e., allow generating samples from $\gamma^*(\cdot|x^{\text{new}})$ where $x^{\text{new}}$ is a new test point (not necessarily present in the train data). Optionally, one may require the ability to sample from $\gamma^*_x$.

The described setup is typically called the \textit{continuous OT}. It should not be twisted up with \textit{discrete OT} setup \citep{peyre2019computational, cuturi2013sinkhorn} where the aim is to recover the (unbalanced) EOT plan between the empirical measures $\hat{p}=\frac{1}{N}\sum_{i=1}^{N} \delta_{x_i}$, $\hat{q}=\frac{1}{M}\sum_{j=1}^{M} \delta_{y_j}$. There the out-of-sample estimations are typically not needed.

\section{Related Work}
\label{sec-related-work}
\begin{table*}[!ht]
\vspace{-1mm}
\tiny
\hspace{-2mm}\begin{tabular}{ c|c|c|c|c }
\hline
\backslashbox{\textbf{Solver}}{\textbf{Property}} & \shortstack{\textbf{Problem}} & \shortstack{\textbf{Principles}} & \shortstack{\textbf{What recovers?}} & \shortstack{\textbf{Limitations}}\\ 
\hline
\citep{yang2018scalable} & UOT &  \makecell{Solves $c$-trasform based semi-dual\\ max-min reformulation of UOT using neural nets} & \makecell{Scaling factor $\nicefrac{\gamma^*(x)}{p(x)}$ and \\ stochastic OT map $T^*(x,z)$} & \makecell{Complex max-min objective;\\3 neural networks}\\
\hline
\citep{lubeck2022neural} & Custom UOT & \makecell{Regression on top of discrete EOT between re-balanced measures\\ combined with ICNN-based solver \citep{makkuva2020optimal}} & \makecell{Scaling factors and \\ OT maps between re-scaled measures} & \makecell{Heuristically uses \\ minibatch OT approximations}\\
\hline
\citep{choi2023generative} & UOT &  \makecell{Solves semi-dual max-min \\reformulation of UOT using neural nets} & Stochastic UOT map $T^*(x,z)$ & \makecell{Complex max-min objective;\\2 neural networks}\\
\hline
\citep{eyring2023unbalancedness} & UEOT & \makecell{Flow Matching on top of discrete UEOT\\using neural nets} & \makecell{Parametrized vector field \\$(v_{t,\theta})_{t\in[0,1]}$ to transport the mass} & \makecell{Heuristically uses \\ minibatch OT approximations}\\
\hline
 U-LightOT (\textbf{ours}) &  UEOT & \makecell{Solves non-minimax reformulation \\of dual UEOT using Gaussian Mixtures} & \makecell{Density of UEOT plan $\gamma^*$ together with light\\ procedure to sample $x\sim\gamma^*_x(\cdot)$ and $y\sim\gamma^*_y(\cdot|x)$}& \makecell{Restricted to \\Gaussian Mixture parametrization}
 \\
\hline
\end{tabular}
\vspace{-3mm}
\captionsetup{justification=centering}
 \caption{Comparison of the principles of existing unbalanced OT/EOT solvers and \textbf{our} proposed light solver.}
  \label{table-comparison}
\vspace{-3mm}
\end{table*}

Nowadays, the sphere of continuous OT/EOT solvers is actively developing. Some of the early works related to this topic utilize OT cost as the loss function \citep{gulrajani2017improved, genevay2018learning, arjovsky2017wasserstein}. These approaches are not relevant to us as they do not learn OT/EOT maps (or plans).
We refer to \citep{korotin2022kantorovich} for a detailed review and bechmark of many of them.

At the same time, there exist a large amount of works within the discrete OT/EOT setup \citep{cuturi2013sinkhorn,dvurechenskii2018decentralize,xie2022accelerated, nguyen2022unbalanced}, see \citep{peyre2019computational} for a survey. We again emphasize that solvers of this type are not relevant to us as they construct discrete matching between the given (train) samples and typically do not provide generalization to the new unseen (test) data. Only recently ML community has started developing out-of-sample estimation procedures based on discrete/batched OT. For example, \citep{fatras2020learning,pooladian2021entropic,hutter2021minimax,deb2021rates,manole2021plugin,rigollet2022sample} mostly develop such estimators using the barycentric projections of the discrete EOT plans. Though these procedures have nice theoretical properties, their scalability remains unclear.

\textbf{Balanced OT/EOT solvers.} 
There exist a vast amount of neural solvers for continuous OT problem.
Most of them learn the OT maps (or plans) via solving saddle point optimization problems \citep{asadulaev2024neural, fan2023neural, korotin2021neural, gazdieva2022unpaired, rout2022generative, mokrov2024energy}. Though the recent works \citep{gushchin2023entropic, seguy2018large, daniels2021score,korotin2024light} tackle the EOT problem \eqref{eot-classic}, they consider its balanced version. Hence they are not relevant to us. 

Among these works, only \citep{korotin2024light} evades non-trivial training/inference procedures and is ideologically the closest to ours. 
In fact, \textbf{our paper} proposes the solver which subsumes their solver and generalizes it for the unbalanced case. The derivation of our solver is non-trivial and requires solid mathematical apparatus, see \wasyparagraph\ref{sec-method}.

\textbf{Unbalanced OT/EOT solvers.} A vast majority of early works in this field tackle the discrete UOT/UEOT setup \citep{chapel2021unbalanced, fatras2021unbalanced,pham2020unbalanced} but the principles under their construction are not easy to generalize to the continuous setting. Thus, many of the recent papers which tackle the continuous unbalanced OT/EOT setup employ the discrete solutions in the construction of their solvers. For example, \citep{lubeck2022neural} regress neural network on top of scaling factors obtained using the discrete UEOT while simultaneously learning the continuous OT maps using an ICNN method \citep{makkuva2020optimal}. In \citep{eyring2023unbalancedness}, the authors implement Flow Matching \citep[FM]{lipman2022flow} on top of the discrete UEOT plan. 
Despite the promising practical performance of these solvers, it is still unclear to what extent such approximations of UEOT plans are theoretically justified.

The recent papers \citep{yang2018scalable, choi2023generative} are more related to our study as they do not rely on discrete OT approximations of the transport plan. However, they have non-trivial minimax optimization objectives solved using \textit{complex} GAN-style procedures. Thus, these GANs often lean on the heavy neural parametrization, may incur instabilities during training and require careful hyperparameter selection \citep{lucic2018gans}. 

For completeness, we also mention other papers which are only slightly relevant to us. For example, \citep{gazdieva2023extremal} consider incomplete OT which relaxes only one of the OT marginal constraints and is less general than the unbalanced OT. Other works \citep{dao2023robust, balaji2020robust} incorporate unbalanced OT into the training objective of GANs aimed at generating samples from noise. 

In contrast to the listed works, our paper proposes a \textit{theoretically justified and lightweight} solver to the UEOT problem, see Table \ref{table-comparison} for the detailed comparison of solvers.


\section{Unbalanced and Light OT Solver}
\label{sec-method}
In this section, we derive the optimization objective (\wasyparagraph \ref{sec-optimization}) of our U-LightOT solver and present practical aspects of training and inference procedures (\wasyparagraph \ref{sec-algorithm}). The proof for our  theorem is given in Appendix \ref{sec-proofs}.

\subsection{Derivation of the Optimization Objective}
\label{sec-optimization}
Following the learning setup described above, we aim to get a parametric approximation $\gamma_{\theta, w}$ of the UEOT plan $\gamma^*$. Here $\theta,\omega$ are the model parameters to learn and it will be clear later why we split them in two groups.

To recover $\gamma_{\theta,\omega}\approx \gamma^{*}$, our aim is to learn $\theta,\omega$ by directly minimizing the UKL divergence between $\gamma_{\theta,w}$ and $\gamma^*$: 
\begin{equation}
    \UKL{\gamma^*}{\gamma_{\theta,\omega}}\rightarrow \min_{(\theta,\omega)}.
    \label{main-objective}
\end{equation}\vspace{-4mm}

The main difficulty of this optimizing objective \eqref{main-objective} is obvious: the UEOT plan $\gamma^*$ is \textit{unknown}. Surprisingly, below we show that one still can optimize \eqref{main-objective} without knowing $\gamma^{*}$. 

Recall that the optimal UEOT plan $\gamma^*$ has the form \eqref{gamma-potentials}. We first do some changes of variables. Specifically, we define $v^*(y)\defeq\exp\{\frac{2\psi^*(y)-\|y\|^2}{2\varepsilon}\}$.
Formula \eqref{gamma-potentials} now reads as
\begin{eqnarray}
    \gamma^*(x, y) = \exp\{\frac{2\phi^*(x)-\|x\|^2}{2\varepsilon}\} \!\!
    \exp\{\frac{\langle x, y\rangle}{\varepsilon}\} v^*(y)\! \Longrightarrow \label{gamma-mid-potentials} \nonumber \\
    \!\!\!\gamma^*(y|x) \propto \exp\{\frac{\langle x, y\rangle}{\varepsilon}\} v^*(y).
    \label{v-unnormalized}
\end{eqnarray}
Since the conditional plan has the unit mass, we may write
\begin{eqnarray}
    \gamma^*(y|x)=\exp\{\frac{\langle x, y\rangle}{\varepsilon}\} \frac{v^*(y)}{c_{v^*}(x)}
    \label{conditional-distrib}
\end{eqnarray}
where $c_{v^*}(x)\defeq\int_{\mathbb{R}^d} \exp\{\frac{\langle x, y\rangle}{\varepsilon}\} v^*(y) dy$ is the normalization costant which ensures that $\int_{\mathbb{R}^d} \gamma^* (y|x) dy = 1$. 

Consider the decomposition $\gamma^*(x, y) = \gamma_x^*(x) \gamma^*(y|x)$. It shows that to obtain parametrization for the entire plan $\gamma^*(x)$, it is sufficient to consider parametrizations for its left marginal $\gamma^*_x$ and the conditional measure $\gamma^*(y|x)$. Meanwhile, equation \eqref{conditional-distrib} shows that \textit{conditional measures $\gamma^*(\cdot| x)$ are entirely described by the variable $v^*$}. We use these observations to parametrize $\gamma_{\theta,w}$. We set
\begin{eqnarray}
    \gamma_{\theta, w}(x,y)\defeq u_w(x)\gamma_{\theta}(y|x)\!=\!
    u_w(x) \!\frac{\exp\{\nicefrac{\langle x, y\rangle}{\varepsilon}\}\! v_{\theta}(y)}{c_{\theta}(x)},\!
    \label{parametrization}
\end{eqnarray}
where $u_w$ and $v_{\theta}$ parametrize marginal measure $\gamma_x^*$ and the variable $v^*$, respectively. In turn, the constant $c_{\theta}(x)\defeq\int_{\mathbb{R}^d} \exp\{\frac{\langle x, y\rangle}{\varepsilon}\} v_{\theta}(y) dy$ is the parametrization of $c_{v^*}(x)$. 

Next we demonstrate our \textbf{main result} which shows that the optimization of \eqref{main-objective} can be done \textit{without} the access to $\gamma^*$.

\begin{theorem}[Tractable form of the UKL minimization] 
    Assume that $\gamma^*$ is parametrized using \eqref{parametrization}. Then the following bound holds: $\varepsilon\UKL{\gamma^*}{\gamma_{\theta,w}}\!\leq\! \mathcal{L}(\theta,w)-\mathcal{L}^*,$  where
    \vspace{-3mm}\begin{eqnarray}
        \mathcal{L}(\theta,w)\defeq 
    \int_{\mathbb{R}^d} \overline{f}_1 (- \varepsilon \log \frac{u_w(x)}{c_{\theta}(x)}-\frac{\|x\|^2}{2})p(x) dx + \nonumber\\
    \int_{\mathbb{R}^d} \overline{f}_2 (-\varepsilon \log v_{\theta}(y) -\frac{\|y\|^2}{2})q(y) dy + \varepsilon \|u_w\|_1
    \label{tractable-objective}
    \end{eqnarray}\vspace{-1mm}
    and constant $\mathcal{L}^*$ is the optimal value of the dual form \eqref{unbalanced-eot-dual}. 
    
    The bound is \textbf{tight} in the sence that it turns to $0=0$ when $v_{\theta}(y)\!=\!\!\exp\{\frac{2\psi^*(y)-\|y\|^2}{2\varepsilon}\}$ and $u_{\omega}(x)\!=\!\!\exp\{\frac{2\phi^*(x)-\|x\|^2}{2\varepsilon}\}$.
    \label{thm-ukl}
\end{theorem}\vspace{-3mm}
In fact, equation \eqref{tractable-objective} is the dual form \eqref{unbalanced-eot-dual} but with potentials $(\phi,\psi)$ expressed through $u_{\omega}, v_{\theta}$ (and $c_{\theta}$).
\vspace{-2mm}$$\phi(x)\leftarrow\phi_{\theta,\omega}(x)=\varepsilon \log \frac{u_w(x)}{c_{\theta}(x)}+\frac{\|x\|^2}{2},$$
\vspace{-2mm}$$\psi(y)\leftarrow \psi_{\theta}(y)=\varepsilon \log v_{\theta}(y) +\frac{\|y\|^2}{2}$$
Our result can be interpreted as the bound on the quality of approximate solution $\gamma_{\theta,\omega}$ \eqref{unbalanced-eot-primal} recovered from the approximate solution to the dual problem \eqref{unbalanced-eot-dual}. It can be directly proved using the original $(\phi,\psi)$ notation of the dual problem, but we use $(u_{\omega},v_{\theta})$ instead as with this change of variables the form of the recovered plan $\gamma_{\theta,\omega}$ is more interpretable ($u_{w}$ defines the first marginal, $v_{\theta}$ -- conditionals).

 Instead of optimizing \eqref{main-objective} to get $\gamma_{\theta,\omega}$, we may optimize the obtained upper bound \eqref{tractable-objective} which is more tractable. Indeed, \eqref{tractable-objective} is a sum of the expectations w.r.t. the probability measures $p,q$. It means that we can use random samples to estimate \eqref{tractable-objective} using Monte-Carlo estimation and optimize it with stochastic gradient descent procedure w.r.t. $(\theta, \omega)$. The main \textbf{challenge} here is the computation of the variable $c_{\theta}$ and term $\|u_{\omega}\|_1$. Below we show the smart parametrization by which both variables can be derived analytically.

\subsection{Parameterization and the Optimization Procedure}

\textbf{Paramaterization.} Recall that $u_w$ parametrizes the density of the marginal $\gamma^*_x$  which is unnormalized. Setting $x=0$ in equation \eqref{v-unnormalized}, we get $\gamma^*(y|x=0) \propto v^*(y)$ which means that $v^*$ also corresponds to unnormalized density of the measure.  These motivate us to use the unnormalized Gaussian mixture parametrization for the potential $v_{\theta}(y)$ and measure $u_w(x)$: 
\vspace{-4mm}
\begin{eqnarray}
    \label{gauss-parametrization}
    v_{\theta}(y)\!\!=\!\!\!\sum_{k=1}^{K}\!\! \alpha_k \mathcal{N}(y|r_k, \!\varepsilon S_k); \nonumber\\
    u_{\omega}(x)\!\!=\!\!\!\sum_{l=1}^{L} \!\beta_l \mathcal{N}(x|\mu_l, \!\varepsilon \Sigma_l).\!
\end{eqnarray}\vspace{-4mm}

Here $\theta\defeq\{\alpha_k, r_k, S_k\}_{k=1}^{K}$, $w\defeq\{\beta_l, \mu_l, \Sigma_l\}_{l=1}^{L}$ with $\alpha_k,\beta_l\geq0$, $r_k, \mu_l\in\mathbb{R}^d$ 
and $0\prec S_k,\Sigma_l\in \mathbb{R}^{d\times d}$. Here covariance matrices are scaled by the $\varepsilon$ for convenience.

For this type of the parametrization, it obviously holds that $\|u_w\|_1=\sum_{l=1}^L \beta_l.$ Moreover, there exist closed-from expressions for the normalization constant $c_{\theta}(x)$ and conditional plan $\gamma_{\theta}(y|x)$, see \citep[Proposition 3.2]{korotin2024light}. Specifically, define $r_k(x)\defeq r_k+S_k x$ and $\widetilde{\alpha}_{k}(x)\defeq \alpha_k \exp\{\frac{x^T S_k x +2 r_k^T x}{2 \varepsilon}\}$. It holds that
\begin{eqnarray}
    c_{\theta}(x) = \sum_{k=1}^K \widetilde{\alpha}_{k}(x);\nonumber\\
    \gamma_{\theta}(y|x)=\frac{1}{c_{\theta}(x)} \sum_{k=1}^K \widetilde{\alpha}_{k}(x) \mathcal{N}(y|r_k(x), \varepsilon S_k).
    \label{gamma-gauss-parametrization}
\end{eqnarray}
Using this result and \eqref{gauss-parametrization}, we get the expression for $\gamma_{\theta, w}$:
\begin{eqnarray}
    \gamma_{\theta, w}(x,y)=u_{\omega}(x)\cdot \gamma_{\theta}(y|x)=
    \nonumber\\
    \underbrace{\sum_{l=1}^{L} \!\beta_l \mathcal{N}(x|\mu_l, \!\varepsilon \Sigma_l)}_{u_{\omega}(x)} \cdot
    \underbrace{\frac{ \sum_{k=1}^K \widetilde{\alpha}_{k}(x) \mathcal{N}(y|r_k(x), \varepsilon S_k)}{\sum_{k=1}^K \widetilde{\alpha}_{k}(x)}}_{\gamma_{\theta}(y|x)}.
\end{eqnarray}

\label{sec-algorithm}
\textbf{Training.} We recall that the measures $p, q$ are accesible only by a number of empirical samples (see the learning setup in \wasyparagraph \ref{sec-background}). Thus, given samples $\{x_1,...,x_N\}$ and $\{y_1,...,y_M\}$, we optimize the empirical analogue of \eqref{tractable-objective}
\begin{eqnarray}
   \widehat{\mathcal{L}}(\theta, w)\defeq
      \frac{1}{N}\sum_{i=1}^N \overline{f}_1 (- \varepsilon \log \frac{u_w(x_i)}{c_{\theta}(x)}-\frac{\|x_i\|^2}{2}) + \nonumber\\
    \frac{1}{M}\sum_{j=1}^M  \overline{f}_2 (-\varepsilon \log v_{\theta}(y_j) -\frac{\|y_j\|^2}{2})\! + \!\varepsilon \|u_w\|_1
    \label{empirical-objective}
\end{eqnarray}
using minibatch gradient descent procedure w.r.t. parameters $(\theta, w)$. In the parametrization of $v_{\theta}$ and $u_w$ \eqref{gauss-parametrization}, we utilize the diagonal matrices $S_k$, $\Sigma_k$. This allows for decreasing the number of learnable parameters in $\theta$ and speeding up the computation of $S_k^{-1},\;\Sigma_l^{-1}$. %For the parametrization of $u_w$, we also consider the diagonal matrices. 

\textbf{Inference.} Our solver provides easy and lightweight sampling both from the marginal measure $u_w\approx \gamma_{x}^{*}$ and the conditional measure $\gamma_{\theta, w}(y|x)\approx \gamma^*(y|x)$ whose parameters are explicitly stated in equation \eqref{gamma-gauss-parametrization}.

\subsection{Connection to Related Prior Works}
\label{sec-prior-work}
The idea of using Gaussian Mixture parametrization for dual potentials in EOT-related tasks firstly appeared in the EOT/SB benchmark \citep{gushchin2023building}. There it was used to obtain the benchmark pairs of probability measures with the known EOT solution between them. In \citep{korotin2024light}, the authors utilized this type of parametrization to obtain a light solver \textbf{(LightSB)} the \textbf{balanced} EOT.

Our approach for \textbf{unbalanced} EOT \eqref{tractable-objective} subsumes their approach for balanced EOT. Specifically, our solver can be reduced to theirs by considering $\overline{f_1}(t)=t$ and $\overline{f_2}(t)=t$ in the equation \eqref{tractable-objective}. Namely, objective \eqref{tractable-objective} becomes
\vspace{-2mm}\begin{eqnarray}
    \mathcal{L}(\theta,w)= 
    \int_{\mathbb{R}^d} (- \varepsilon \log \frac{u_w(x)}{c_{\theta}(x)}-\frac{\|x\|^2}{2})p(x) dx + 
    \nonumber\\
    \int_{\mathbb{R}^d} (-\varepsilon \log v_{\theta}(y) -\frac{\|y\|^2}{2})q(y) dy + \varepsilon \|u_w\|_1=
    \nonumber\\
    - \varepsilon\big(\int_{\mathbb{R}^d}\log \frac{u_w(x)}{c_{\theta}(x)})p(x)dx +
    \int_{\mathbb{R}^d} \log v_{\theta}(y)q(y)dy\big)-
    \nonumber\\
    \|u_w\|_1+\underbrace{\int_{\mathbb{R}^d}\frac{\|x\|^2}{2}p(x) dx - \int_{\mathbb{R}^d} \frac{\|y\|^2}{2}q(y) dy}_{\defeq C(p,q)}=
    \nonumber\\
    \varepsilon\big(\int_{\mathbb{R}^d}\log c_{\theta}(x)p(x)dx -
    \int_{\mathbb{R}^d} \log v_{\theta}(y)q(y)dy\big)- 
    \label{lightsb-objective}
    \\
    \varepsilon \int_{\mathbb{R}^d} \log u_w(x) p(x)dx + \|u_w\|_1 + C(p,q).
    \label{marginal-objective}
\end{eqnarray}\vspace{-4mm}

Note that here line \eqref{lightsb-objective} depends exclusively on $\theta$ and line \eqref{marginal-objective} on $\omega$. Furthermore, line \eqref{lightsb-objective} exactly coincides with the LightSB's objective, see \citep[Proposition 8]{korotin2024light}. Also note that the minimum of line \eqref{marginal-objective} is attained when $u_w=p$ and this part is not actually needed in the balanced case, see \citep[Appendix C]{korotin2024light}.

\section{Experimental Illustrations}
\begin{figure*}[t!]
\begin{center}
    \vspace{-2mm}
    \includegraphics[width=0.5\textwidth]{toy/legend_blank.png}
\end{center}
\end{figure*}

\begin{figure*}[t!]
\vspace{-3mm}
\centering
\begin{subfigure}[b]{0.235\linewidth}
    \centering
    \includegraphics[width=0.995\linewidth]{toy/Input.png}
    \caption{Input, target measures}
    \label{fig:gauss-input}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.235\linewidth}
    \centering
    \includegraphics[width=0.995\linewidth]{toy/LightSB.png}
    \caption{LightSB}
    \label{fig:gauss-lightsb}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.235\linewidth}
    \centering
    \includegraphics[width=0.995\linewidth]{toy/Inter.png}
    \caption{Interpolation}
    \label{fig:gauss-mix}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.235\linewidth}
    \centering
    \includegraphics[width=0.995\linewidth]{toy/UOT.png}
    \caption{U-LightOT (ours)}
    \label{fig:gauss-ulightsb}
\end{subfigure}
\vspace{-2mm}\caption{\centering Conditional plans $\gamma_{\theta,\omega}(y|x)$ learned by our solver in \textit{Gaussians} experiment. Here $p_{\omega}$ denotes the normalized first marginal $u_{w}$, i.e., $p_{\omega}=u_{\omega}/\|u_{\omega}\|_1$.}
\end{figure*}

In this section, we test our solver on the synthetic task (\wasyparagraph\ref{sec-gaussian-exp}) and unpaired image translation (\wasyparagraph\ref{sec-alae-exp}). The code is written using \texttt{PyTorch} framework and will be made public. The experiments are issued in the form of \texttt{*.ipynb} notebooks. Each experiment requires several minutes of training on CPU with 4 cores.

\vspace{-2mm}\subsection{Example with the Mixture of Gaussians}
\label{sec-gaussian-exp}

In this section, we provide \textit{'Gaussians'} example in 2D to demonstrate the ability of our solver to deal with the imbalance of classes in the source and target measures. We follow the idea of the experimental setup proposed in \citep[Figure 2]{eyring2023unbalancedness}. Both probability measures $p$ and $q$ are the mixtures of Gaussians, see Figure \ref{fig:gauss-input}. The difference is in the masses of the individual Gaussians, i.e., 
$$p(x)\!=\!\frac{1}{4} \mathcal{N}(x|(-2, 3), 0.1\cdot I_2)\!+\!\frac{3}{4} \mathcal{N}(x|(1, 3), 0.1 \cdot I_2)$$
$$q(y)\!=\!\frac{3}{4} \mathcal{N}(y|(-2,0), 0.1 \cdot I_2)\!+\!\frac{1}{4} \mathcal{N}(y|(1,0), 0.1 \cdot I_2)$$

\textbf{Baselines.} We consider \citep[LightSB]{korotin2024light} as the main baseline which computes balanced EOT maps between the measures. In Figure \ref{fig:gauss-input}, we show that it fails to transport the mass from the input Gaussians to the closest target ones due to the imbalance of their masses. In fact, as we wrote in Section \ref{sec-prior-work}, LightSB is a special \textit{balanced} case of our solver given the $f$-divergences $\overline{f_1}(t)=\overline{f_2}(t)=t$.

In Figure \ref{fig:gauss-ulightsb}, we demonstrate the results of our U-LightOT solver $\overline{f_1}, \overline{f_2}$ defined as the $\text{SoftPlus}(t)=\log(1+\exp^{t})$ function. In this case, our solver successfully overcomes the imbalance issue and transports the mass correctly. 

For completeness, we also present a kind of interpolation between the balanced and SoftPlus cases. We consider $\overline{f_1}(t)=\overline{f_2}(t)=\frac{1}{2} t + \frac{1}{2}\text{SoftPlus}(t)$. In Figure \ref{fig:gauss-mix}, we see that for the mass is not transported correctly. However, the amount of incorrecly mapped points is less than in the balanced (LightSB) case (Figure \ref{fig:gauss-ulightsb}).

\textit{Remark.} We conduct all experiments here with the entropy regularization parameter $\varepsilon=0.05$. The parameter $\varepsilon$ is resposible for the stochasticity of the learned transport $\gamma_{\theta}(\cdot|x)$. Since we are mostly interested in the correct transport of the mass (controlled by $f_1,f_2$) rather than the stochasticity, we do not pay much attention to $\epsilon$ throughout the paper.


\subsection{Unpaired Image-to-Image Translation}
\label{sec-alae-exp}

\begin{figure*}[t!]
\begin{center}
\begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=0.995\linewidth]{pics/Fig_YOUNG_TO_ADULT_7.png}
    \caption{\textit{Young} $\rightarrow$  \textit{Adult}}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=0.995\linewidth]{pics/Fig_ADULT_TO_YOUNG_7.png}
    \caption{\textit{Adult} $\rightarrow$  \textit{Young}}
\end{subfigure}
\hfill
\vspace{3mm}\newline 
\begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=0.995\linewidth]{pics/Fig_MAN_TO_WOMAN_7_new.png}
    \caption{\textit{Man} $\rightarrow$  \textit{Woman}}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=0.995\linewidth]{pics/Fig_WOMAN_TO_MAN_7_new.png}
    \caption{\textit{Woman} $\rightarrow$  \textit{Man}}
\end{subfigure}
\vspace{-3mm}\caption{Unpaired translation with LightSB, OT-FM, UOT-FM and our U-LightOT solver applied in the latent space of ALAE for FFHQ images (1024$\times$1024).}
\label{fig:alae}
\end{center}
\end{figure*}


In this section, we consider the image-to-image translation task \citep{zhu2017toward}. We follow the experimental setup of \citep[Section 5.4]{korotin2024light} and use pre-trained ALAE autoencoder \citep{pidhorskyi2020adversarial} for $1024\times1024$ FFHQ dataset \citep{karras2019style} of human faces. We consider different variants of translation: \textit{adult}$\leftrightarrow$\textit{young} and \textit{woman}$\leftrightarrow$\textit{man}. We split the data on train and test part following \citep{korotin2024light} and use their $512$-dimensional latent codes extracted for images in each domain.

\begin{table}[t!]
    \vspace{-5mm}
    \centering
    \small
    \begin{tabular}{ c|c c  } 
    \hline
    Class & Man & Woman  \\
    \hline
     Young  & $15K$    &$23K$     \\  \hline
     Adult  &$7K$    &$3.5K$     \\ \hline
    \end{tabular}
    \captionsetup{justification=centering}
    \caption{Number of \textit{train} FFHQ images for each class.}
     \label{table-classes}
     \vspace{-7mm}
\end{table}

The challenge of the described translations lies in the \textbf{imbalance} of classes in the measures of source and target images, see Table \ref{table-classes}. Indeed, let us consider in more details \textit{adult}$\rightarrow$\textit{young} translation. In FFHQ dataset, the amount of adult input images labeled as \textit{woman} significantly outnumbers adults labeled as \textit{man}. The situation is opposite for the target (young) data. Thus, solvers based on the balanced transport are expected to translate some of the \textit{young woman} faces to the \textit{adult man} ones. At the same time, solvers based on unbalanced transport are expected to solve this issue.

\textbf{Baselines.} We perform comparison with the recent procedure \citep[UOT-FM]{eyring2023unbalancedness} which considered roughly the same setup and demonstrated good performance. This method interpolates the results of unbalanced discrete OT to the continuous setup using the flow matching~\cite{lipman2022flow}. For completeness, we include the comparison with LightSB and balanced optimal transport-based flow matching (OT-FM)~\cite{lipman2022flow} to demonstrate the issues of the balanced solvers. Qualitative results for OT-FM, LightSB and our U-LightOT solver are given in Figure \ref{fig:alae}. Additional qualitative results are given in Appendix \ref{app-add-results}.

\textbf{Metrics.} Our aim is to assess the ability of the solvers to perform the translation of latent codes keeping the class of the input images unchanged, e.g., in \textit{adult}$\rightarrow$\textit{young} translation, the gender should not be changed. We train a $99\%$ MLP classifier on the images of different classes and compute the accuracy between the actual classes of the input images and those of the generated ones. The results are given in Table \ref{table-alae} and show that our solver outperforms its alternatives in dealing with class imbalance issue.


\begin{table}[t!]
    \centering
    \small
    \begin{tabular}{ c|c c c c c } 
    \hline
    Translation & OT-FM	& LightSB & UOT-FM	& \makecell{U-LightOT \\(ours)}  \\
    \hline
     Young$\rightarrow$Adult  &60.71    &87.11   &87.72   & \textbf{94.92}   \\  
     Adult$\rightarrow$Young  &56.75    &76.68   &81.34   &\textbf{95.39}    \\ \hline
     Man$\rightarrow$Woman  &73.82    &81.33   &87.35   &\textbf{92.80}   \\ 
     Woman$\rightarrow$Man  &77.56    &83.29   &88.20   &\textbf{95.17}    \\  \hline
    \end{tabular}
     \caption{Accuracy of keeping the class of the input image.}
     \label{table-alae}
     \vspace{-5mm}
\end{table}

\section{Discussion}
\textbf{Potential impact.}
Our light and unbalanced solver has a lot of advantages in comparison with the other existing UEOT solvers. First, it does not require complex max-min optimization. Second, it provides the closed form of the conditional measures $\gamma_{\theta,\omega}(y|x)\approx \gamma^*(y|x)$ of the UEOT plan. Moreover, it allows for sampling both from the conditional measure $\gamma_{\theta,\omega}(y|x)$ and marginal measure $u_w(x)\approx \gamma_x(x)$. Besides, the decisive superiority of our lightweight and unbalanced solver is its simplicity and convenience of using. Indeed, it has a straightforward and non-minimax optimization objective and avoids heavy neural parametrization. As a result, our lighweight and unbalanced solver converges in minutes on CPU. We expect that these advantages could boost the usage of our solver as a standard and easy baseline for UEOT task with applications in different spheres. 

\textbf{Limitations.} One limitation of our solver is the usage of the Gaussian Mixture parametrization which might restrict the scalability of our solver. This points to the necessity for developing ways to optimize objective \eqref{unbalanced-eot-dual} with more general parametrization, e.g., neural networks. This is a promising avenue for future work.

\section{Broader impact}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.

\bibliography{bibliography}
\bibliographystyle{icml2024}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Proofs}
\label{sec-proofs}

\begin{proof}[Proof of Theorem \ref{thm-ukl}] 
To begin with, we derive the useful property of the optimal UEOT plans.
We recall that $\phi^*$ and $\psi^*$ are the minimizers of dual form \eqref{unbalanced-eot-dual}, i.e., $(\phi^*,\psi^*)\in\argmin_{(\phi,\psi)} \mathcal{J}(\phi,\psi)$ where 
$$
\mathcal{J}(\phi,\psi)\defeq \varepsilon \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \!\exp\{\frac{1}{\varepsilon} \!( \phi(x) + \psi(y)\!-\! \nicefrac{\|x-y\|^2}{2}) \} dx dy \!+ \int_{\mathbb{R}^d} \overline{f}_1 (-\phi(x))p(x) dx \! + \!\int_{\mathbb{R}^d} \overline{f}_2 (-\psi(y))q(y) dy.
$$
Then, by the first order optimality condition $\frac{d}{d\phi}\mathcal{J}(\phi,\psi)|_{\phi=\phi^*}=0$ and $\frac{d}{d\psi}\mathcal{J}(\phi,\psi)|_{\psi=\psi^*}\!=\!0$. It means that
\begin{eqnarray}
    \frac{d}{d\phi}\mathcal{J}(\phi,\psi)|_{\phi=\phi^*}(x)= \cancel{\varepsilon \cdot\varepsilon^{-1}} \int_{\mathbb{R}^d} \exp\{\varepsilon^{-1}(\phi^*(x)+\psi^*(y)-\nicefrac{\|x-y\|^2}{2})\} dy -\nabla \overline{f}_1 (-\phi^*(x)) p(x)=0 \Longrightarrow \nonumber
    \\
    \nabla \overline{f}_1 (-\phi^*(x))=\frac{\gamma^*_x(x)}{p(x)}
\end{eqnarray}
holds for all $x$, s.t. $p(x)>0$. Analogously, we get that $\nabla \overline{f}_2 (-\psi^*(y))=\frac{\gamma^*_y(y)}{q(y)}$ for all $y$, s.t. $q(y)>0$.

For convenience of the further derivations, we consider the change of variables to simplify the expression for $\mathcal{L}(\theta,\omega)$. We define $\phi_{\theta,\omega}(x)\defeq \varepsilon \log \frac{u_{\omega}(x)}{c_{\theta}(x)} - \frac{\|x\|^2}{2}$ and $\psi_{\theta}(y)\defeq \varepsilon \log v_{\theta}(y)- \frac{\|y\|^2}{2}$. Then
\begin{eqnarray*}
    \mathcal{L}(\theta,\omega) = \int_{\mathbb{R}^d} \overline{f}_1 (-\phi_{\theta,\omega}(x))p(x) dx +
    \int_{\mathbb{R}^d} \overline{f}_2 (-\psi_{\theta}(y))q(y) dy + \varepsilon \|u_w\|_1=\mathcal{J}(\phi_{\theta,\omega},\psi_{\omega}).
    \label{ukl-functional-before}
\end{eqnarray*}
% For completeness, we note that $\mathcal{J}(\phi_{\theta,\omega},\psi_{\theta})=\mathcal{L}(\theta,\omega)$.

Next, we simplify our main objective:
\begin{eqnarray}
    \UKL{\gamma^{*}}{\gamma_{\theta,\omega}}\!=\!\|\gamma^*\|_1\bigg[\KL{\frac{\gamma^*}{\|\gamma^*\|_1}}{\frac{\gamma_{\theta,\omega}}{\|\gamma_{\theta,\omega}|_1}}+\frac{\|\gamma_{\theta,\omega}\|_1}{\|\gamma^*\|_1}\!-\!1\!-\!\log\frac{\|\gamma_{\theta,\omega}\|_1}{\|\gamma^*\|}\bigg]\!=\! 
    \nonumber
    \\
    \cancel{\|\gamma^*\|_1\! \cdot \!\|\gamma^*\|_1^{-1}}  \Bigg(\int_{\mathbb{R}^d\times\mathbb{R}^d}\!\!\! \gamma^*(x,y)\log \gamma^*(x,y) dx dy -\int_{\mathbb{R}^d\times\mathbb{R}^d} \gamma^*(x,y) \log \|\gamma^*\|_1 dx dy -\int_{\mathbb{R}^d\times\mathbb{R}^d} \!\!\!\gamma^*(x,y) \log \gamma_{\theta, \omega}(x,y) dx dy + 
    \nonumber
    \\
    \int_{\mathbb{R}^d\times\mathbb{R}^d} \gamma^*(x,y) \log \|\gamma_{\theta,\omega}\|_1 dx dy + \|\gamma_{\theta,\omega}\|_1 - \|\gamma^*\|_1 - \|\gamma^*\|_1 \log\frac{\|\gamma_{\theta,\omega}\|_1}{\|\gamma^*\|_1}\Bigg)= \int_{\mathbb{R}^d\times\mathbb{R}^d} \gamma^*(x,y)\log \gamma^*(x,y) dx dy -\! 
    \nonumber
    \\
    \cancel{\|\gamma^*\|_1 \log \|\gamma^*\|_1} \! - \int_{\mathbb{R}^d\times\mathbb{R}^d} \!\!\!\gamma^*(x,y) \log \gamma_{\theta, \omega}(x,y) dx dy + \cancel{\|\gamma^*\|_1 \log \|\gamma_{\theta,\omega}\|_1}\! + \!\|\gamma_{\theta,\omega}\|_1 - \|\gamma^*\|_1 - \cancel{\|\gamma^*\|_1\log\|\gamma_{\theta,\omega}\|_1} + 
    \nonumber
    \\
    \cancel{\|\gamma^*\|_1\log\|\gamma^*\|_1}=\int_{\mathbb{R}^d\times\mathbb{R}^d} \gamma^*(x,y)\log \gamma^*(x,y) dx dy - \int_{\mathbb{R}^d\times\mathbb{R}^d} \!\!\!\gamma^*(x,y) \log \gamma_{\theta, \omega}(x,y) dx dy + \!\|\gamma_{\theta,\omega}\|_1 - \|\gamma^*\|_1.
    \label{ukl-derivation-bgt}
\end{eqnarray}
Now we recall that the ground-truth UEOT plan $\gamma^*(x,y)$ and the optimal dual variables $\phi^*(x)$, $\psi^*(y)$ are connected via equation \eqref{gamma-potentials}. Similarly, our parametrized plan $\gamma_{\theta,\omega}(x,y)$ can be expressed using $\phi_{\theta,\omega}(x)$, $\psi_{\theta}(y)$ as $\gamma_{\theta,\omega}(x,y) = \exp\{\varepsilon^{-1}(\phi_{\theta,\omega}(x)+\psi_{\theta}(y)-\nicefrac{\|x-y\|^2}{2})\}$.
Then
\begin{eqnarray}
    \eqref{ukl-derivation-bgt}=\varepsilon^{-1}\int_{\mathbb{R}^d\times\mathbb{R}^d} \gamma^*(x,y)\big(\phi^*(x)+\psi^*(y)-\nicefrac{\|x-y\|^2}{2}\big) dx dy - \varepsilon^{-1}\int_{\mathbb{R}^d\times\mathbb{R}^d} \gamma^*(x,y) \big(\phi_{\theta,\omega}(x)+\psi_{\theta}(y)-\nicefrac{\|x-y\|^2}{2}\big)dxdy+
    \nonumber\\
    \|\gamma_{\theta,\omega}\|_1 - \|\gamma^*\|_1 = \varepsilon^{-1} \int_{\mathbb{R}^d\times\mathbb{R}^d} \gamma^*(x,y) \big(\phi^*(x)+\psi^*(y) - \phi_{\theta,\omega}(x)-\psi_{\theta}(y) \big)dxdy  + \|\gamma_{\theta,\omega}\|_1 - \|\gamma^*\|_1=
    \nonumber\\
    \varepsilon^{-1} \int_{\mathbb{R}^d} \gamma^*_x(x) \big(\phi^*(x) - \phi_{\theta,\omega}(x) \big)dx  + \varepsilon^{-1} \int_{\mathbb{R}^d} \gamma^*_y(y) \big(\psi^*(y) - \psi_{\theta}(y)\big)dy + \|\gamma_{\theta,\omega}\|_1- \|\gamma^*\|_1.
    \label{ukl-derivation-bc}
\end{eqnarray}

Finally, we derive
\begin{eqnarray}
    \eqref{ukl-derivation-bc}=\varepsilon^{-1} \int_{\mathbb{R}^d} \!\!\!\!\underbrace{\frac{\gamma^*_x(x)}{p(x)}}_{\nabla \overline{f}_1 (-\phi^*(x))}\!\!\!\!\! \big(\phi^*(x) - \phi_{\theta,\omega}(x) \big)p(x)dx  + \varepsilon^{-1} \int_{\mathbb{R}^d} \underbrace{\frac{\gamma^*_y(y)}{q(y)}}_{\nabla \overline{f}_2 (-\psi^*(y))}\!\!\! \big(\psi^*(y) - \psi_{\theta}(y)\big)q(y)dy + \|\gamma_{\theta,\omega}\|_1- \|\gamma^*\|_1=
    \nonumber\\
    \varepsilon^{-1}\int_{\mathbb{R}^d} \nabla \overline{f}_1 (-\phi^*(x)) \big(\phi^*(x) - \phi_{\theta,\omega}(x) \big)p(x)dx  + \varepsilon^{-1} \int_{\mathbb{R}^d} \nabla \overline{f}_2 (-\psi^*(y)) \big(\psi^*(y) - \psi_{\theta}(y)\big)q(y)dy + \|\gamma_{\theta,\omega}\|_1- \|\gamma^*\|_1\leq
    \label{proof-inequality}
    \\
    \varepsilon^{-1} \int_{\mathbb{R}^d} ( \overline{f}_1(-\phi_{\theta,\omega}) - \overline{f}_1(-\phi^*)) p(x)dx  + \varepsilon^{-1} \int_{\mathbb{R}^d} ( \overline{f}_2(-\psi_{\theta,\omega}) - \overline{f}_2(-\psi^*))q(y)dy + \|\gamma_{\theta,\omega}\|_1- \|\gamma^*\|_1=
    \nonumber
    \\
    \varepsilon^{-1} \Big(\int_{\mathbb{R}^d}  \overline{f}_1(-\phi_{\theta,\omega}) p(x)dx +  \int_{\mathbb{R}^d}  \overline{f}_2(-\psi_{\theta,\omega}) q(y)dy + \varepsilon \|\gamma_{\theta,\omega}\|_1- \underbrace{(\int_{\mathbb{R}^d}\overline{f}_1(-\phi^*) p(x)dx   + \int_{\mathbb{R}^d}\overline{f}_2(-\psi^*)q(y)dy + \varepsilon \|\gamma^*\|_1)}_{\mathcal{L}^*\defeq}\Big) =
    \nonumber\\
    \varepsilon^{-1} (\mathcal{J}(\phi_{\theta,\omega},\psi_{\omega})-\mathcal{L}^*)= \varepsilon^{-1} (\mathcal{L}(\theta,\omega)-\mathcal{L}^*).
    \nonumber
\end{eqnarray}
The inequality \eqref{proof-inequality} follows from the convexity of the functions $\overline{f_1}$ and $\overline{f_2}$.
\end{proof}

\section{Experiments Details}
\subsection{General Details}
For minimization of the objective \eqref{empirical-objective}, we parametrize $\alpha_k,r_k,S_k$ and $\beta_l,\mu_l,\Sigma_l$ of $v_{\theta}$ and $u_{\omega}$ \eqref{gauss-parametrization} respectively. For simplicity, we parametrize $\alpha_k,\;\beta_l$ as their logarithms $\log\alpha_k,\;\log\beta_l$, variables $r_k,\;\mu_l$ are parametrized directly. We consider diagonal matrices $S_k,\;\Sigma_l$ and parametrize them via the values $\log(S_k)_{i,i}$, $\log(\Sigma_l)_{i,i}$ respectively. We initialize the parameters following the scheme in \citep{korotin2024light}.
In all experiments, we use Adam optimizer and set $K=L$.

\subsection{Details on Experiment with Gaussian Mixtures}
For our solver we use $K=L=5$, $\varepsilon=0.05$, $lr=3e-4$ and batchsize $128$. We do $2\cdot 10^4$ gradient steps.

\textbf{Baselines.} For the OT-FM and UOT-FM methods, we parameterize the vector field $(v_{t,\theta})_{t\in[0,1]}$ for mass transport using a 1-layer feed-forward network with 64 neurons and ReLU activation. These methods are built on the solutions (plans $\pi^{*}(x,y)$) of discrete OT problems, to obtain them we use the POT~\cite{flamary2021pot} package. Especially for the UOT-FM, we use the \texttt{ot.unbalanced.sinkhorn} with the regularization equal to $0.05$. We set the number of training and inference time steps equal to $100$. For LightSB algorithm, we use the parameters presented by the authors in the official repository.

\subsection{Details on Image Translation Experiment}
We use the code and decoder model from
\begin{center}
    \url{https://github.com/podgorskiy/ALAE}
\end{center} 
We download the data and neural network extracted attributes for the FFHQ dataset from 
\begin{center}
\url{https://github.com/ngushchin/LightSB/}
\end{center}
In the \textit{Adult} class we include the images with the attribute \textit{Age} $\!\geq 44$; in the \textit{Young} class - with the \textit{Age}$\in[16, 44]$. We excluded the images with faces of children to increase the accuracy of classification per \textit{gender} attribute.
For the experiments with our solver we use $K=L=50$, $\varepsilon=0.1$, $lr=1$ and batchsize $128$. We do $5\cdot10^3$ gradient steps.

\textbf{Classifier.} We trained an MLP classifier to distinguish images of the classes ($Adult$, $Young$, $Woman$, $Man$) to measure the accuracy of keeping the class of the input images by different solvers.

\textbf{Baselines.} For the OT-FM and UOT-FM methods, we parameterize the vector field $(v_{t,\theta})_{t\in[0,1]}$ for mass transport using a 2-layer feed-forward network with 512 hidden neurons and ReLU activation. An additional sinusoidal embedding\cite{tong2020trajectorynet} was applied for the parameter $t$. Other parameters were set similarly to the Gaussian Mixtures experiment.

\section{Additional Experimental Results}
\label{app-add-results}
We give additional illustrations for our U-LightOT solver and its alternatives applied in the latent space of ALAE autoencoder in Figures \ref{fig:alae-add-1}, \ref{fig:alae-add-2}.

\begin{figure*}[t!]
\begin{center}
\begin{subfigure}[b]{0.95\linewidth}
    \includegraphics[width=0.995\linewidth]{Appendix/Fig_YOUNG_TO_ADULT_8.png}
    \caption{Young $\rightarrow$  Adult}
    \label{fig:alae-y2a}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.95\linewidth}
    \includegraphics[width=0.995\linewidth]{Appendix/Fig_ADULT_TO_YOUNG_8.png}
    \caption{Adult $\rightarrow$  Young}
    \label{fig:alae-a2y}
\end{subfigure}
\vspace{-3mm}\caption{Comparison of solvers, \textit{Adult}$\leftrightarrow$\textit{Young} translation.}
\label{fig:alae-add-1}
\end{center}
\end{figure*}

\newpage
\begin{figure*}[t!]
\begin{center}
\vspace{3mm}\newpage 
\begin{subfigure}[b]{0.95\linewidth}
    \includegraphics[width=0.995\linewidth]{Appendix/Fig_MAN_TO_WOMAN_8.png}
    \caption{Man $\rightarrow$  Woman}
    \label{fig:alae-m2w}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.95\linewidth}
    \includegraphics[width=0.995\linewidth]{Appendix/Fig_WOMAN_TO_MAN_8.png}
    \caption{Woman $\rightarrow$  Man}
    \label{fig:alae-w2m}
\end{subfigure}
\vspace{-3mm}\caption{Comparison of solvers, \textit{Woman}$\leftrightarrow$\textit{Man} translation.}
\label{fig:alae-add-2}
\end{center}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}