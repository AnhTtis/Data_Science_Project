\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}

\usepackage[final]{neurips_2024}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
% \usepackage{xcolor}         % colors

\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{wasysym}
\usepackage{cancel}
\usepackage[dvipsnames]{xcolor}
\usepackage{multirow}
\usepackage{diagbox}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       %professional-quality tables

\usepackage{caption}
\usepackage{subcaption}
% \usepackage{subfigure}
\usepackage{wrapfig}
\usepackage{units}
\usepackage[normalem]{ulem}

\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref.
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclarePairedDelimiter{\norm}{\lVert}{\rVert} 

\usepackage{enumitem}
\setlist{leftmargin=*,itemsep=0pt}

\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\argsup}{arg\,sup}
\newcommand{\xmark}{\ding{55}}

\newcommand{\alex}[1]{{\scriptsize\textbf{\color{red} AK: #1}}}
\newcommand{\milena}[1]{{\scriptsize\textbf{\color{purple} MG: #1}}}
\newcommand{\arip}[1]{{\scriptsize\textbf{\color{orange} AA: #1}}}
\newcommand{\KL}[2]{\text{D}_{\text{KL}}\left(#1\Vert #2\right)}
\newcommand{\UKL}[2]{\text{D}_{\text{KL}}\left(#1\Vert #2\right)}
\newcommand{\Df}[3]{D_{#1}\left(#2\Vert #3\right)}
\newcommand*{\defeq}{\stackrel{\text{def}}{=}}

\allowdisplaybreaks
\everypar{\looseness=-1}


\title{Light Unbalanced Optimal Transport}

\author{
  Milena Gazdieva\\
  Skolkovo Institute of Science and Technology\\
  Artificial Intelligence Research Institute\\
  Moscow, Russia \\
  \texttt{milena.gazdieva@skoltech.ru} \\
  \And
  Arip Asadulaev\\
  ITMO University\\
  Artificial Intelligence Research Institute\\
  Moscow, Russia \\
  \texttt{asadulaev@airi.net} \\
  \AND
  Evgeny Burnaev \\
  Skolkovo Institute of Science and Technology\\
  Artificial Intelligence Research Institute\\
  Moscow, Russia\\
  \texttt{e.burnaev@skoltech.ru}
  \And
  Alexander Korotin\\
  Skolkovo Institute of Science and Technology\\
  Artificial Intelligence Research Institute\\
  Moscow, Russia \\
  \texttt{a.korotin@skoltech.ru} \\
}

\begin{document}

\maketitle

\begin{abstract}\vspace{-1mm} 
While the continuous Entropic Optimal Transport (EOT) field has been actively developing in recent years, it became evident that the classic EOT problem is prone to different issues like the sensitivity to outliers and imbalance of classes in the source and target measures. This fact inspired the development of solvers that deal with the \textit{unbalanced} EOT (UEOT) problem $-$ the generalization of EOT allowing for mitigating the mentioned issues by relaxing the marginal constraints. Surprisingly, it turns out that the existing solvers are either based on heuristic principles or heavy-weighted with complex optimization objectives involving several neural networks. We address this challenge and propose a novel theoretically-justified, lightweight, unbalanced EOT solver. Our advancement consists of developing a novel view on the optimization of the UEOT problem yielding tractable and a non-minimax optimization objective. We show that combined with a light parametrization recently proposed in the field our objective leads to a fast, simple, and effective solver which allows solving the continuous UEOT problem in minutes on CPU. We prove that our solver provides a universal approximation of UEOT solutions and obtain its generalization bounds. We give illustrative examples of the solver's performance. The code is publicly available at 
\begin{center}\vspace{-1mm}\small\url{https://github.com/milenagazdieva/LightUnbalancedOptimalTransport}
\end{center}
\end{abstract}

\vspace*{-2mm}
\section{Introduction}
\label{sec-introduction}

\vspace{-1.5mm}
The computational \textit{optimal transport} (OT) has proven to be a powerful tool for solving various popular tasks, e.g., image-to-image translation \citep{xie2019scalable,fan2020scalable,mokrov2024energy,gushchin2023entropic}, image generation \citep{wang2021deep,de2021diffusion,chen2021likelihood} and biological data transfer \citep{bunne2023learning,koshizuka2022neural,vargas2021solving}. Historically, the majority of early works in the field were built upon solving the OT problem between discrete probability measures \citep{cuturi2013sinkhorn,peyre2019computational}. Only recently the advances in the field of generative models have led to explosive interest from the ML community in developing the \textbf{continuous} OT solvers, see \citep{korotin2021continuous} for a survey. The setup of this problem assumes that the learner needs to estimate the OT plan between continuous measures given only empirical samples of data from them. Due to convexity-related issues of OT problem \citep{korotin2023kernel}, many works consider the EOT problem, i.e., use \textit{entropy} regularizers which guarantee, e.g., the uniqueness of learned plans.

\vspace{-0.5mm}Meanwhile, researches attract attention to other shortcomings of the classic OT problem. It enforces hard constraints on the marginal measures and, thus, does not allow for mass variations. As a result, OT shows high sensitivity to an imbalance of classes and outliers in the source and target measures \citep{balaji2020robust} which are almost inevitable for large-scale datasets. To overcome these issues, it is common to consider extensions of the OT problem, e.g., unbalanced OT/EOT (UOT/UEOT) \citep{chizat2017unbalanced, liero2018optimal}. The unbalanced OT/EOT formulations allow for variation of total mass by relaxing the marginal constraints through the use of divergences.

The scope of our paper is the continuous UEOT problem. It seems that in this field, a solver that is fast, light, and theoretically justified has not yet been developed. Indeed, many of the existing solvers follow a kind of heuristical principles and are based on the solutions of discrete OT. For example, \citep{lubeck2022neural} uses a regression to interpolate the discrete solutions, and \citep{eyring2023unbalancedness, klein2023generative} build a flow matching upon them. Almost all of the other solvers \citep{choi2024generative, yang2018scalable} employ several neural networks with many hyper-parameters and require time-consuming optimization procedures. We solve the aforementioned shortcomings by introducing a novel lightweight solver that can play the role of a simple baseline for unbalanced EOT.

\textbf{Contributions.} We develop a novel \textit{lightweight} solver to estimate continuous \textbf{unbalanced} EOT couplings between probability measures (\wasyparagraph\ref{sec-method}). Our solver has a non-minimax optimization objective and employs the Gaussian mixture parametrization for the UEOT plans. We provide the generalization bounds for our solver (\wasyparagraph\ref{sec-generalization-bounds}) and experimentally test it on several tasks (\wasyparagraph \ref{sec-gaussian-exp}, \wasyparagraph\ref{sec-alae-exp}).

\textbf{Notations.} We work in the Euclidian space $(\mathbb{R}^d, \|\cdot\|)$. We use $\mathcal{P}_{2,ac}(\mathbb{R}^d)$ 
to denote the set of absolutely continuous (a.c.) Borel probability measures on $\mathbb{R}^d$ with finite second moment and differential entropy. The set of nonnegative measures on $\mathbb{R}^d$ with finite second moment is denoted as $\mathcal{M}_{2,+}(\mathbb{R}^d)$.
We use $\mathcal{C}_{2}(\mathbb{R}^d)$ to denote the space of all \textit{continuous} functions $\zeta:\mathbb{R}^d\rightarrow\mathbb{R}$ for which $\exists a=a(\zeta), b=b(\zeta)$ such that $\forall x\in\mathbb{R}^d:$ $|\zeta(x)|\leq a +b\|x\|^2$. Its subspace of functions which are additionally \textit{bounded from above} is denoted as $\mathcal{C}_{2,b}(\mathbb{R}^d)$.
For a.c. measure $p\in \mathcal{P}_{2,ac}(\mathbb{R}^d)$ (or $\mathcal{M}_{2,+}(\mathbb{R}^d)$), we use $p(x)$ to denote its density at a point $x\in\mathbb{R}^d$.
For a given a.c. measure $\gamma\in \mathcal{M}_{2,+}(\mathbb{R}^d \times \mathbb{R}^d)$, we denote its total mass by $\|\gamma\|_1\!\defeq\!\int_{\mathbb{R}^d\times \mathbb{R}^d}\gamma(x, y) dx dy$. We use $\gamma_x(x)$, $\gamma_y(y)$ to denote the marginals of $\gamma(x,y)$. They satisfy the equality $\|\gamma_x\|_1\!=\!\|\gamma_y\|_1\!=\!\|\gamma\|_1$. We write $\gamma(\cdot|x)$ to denote the conditional \textit{probability} measure. Each such measure has a unit total mass.
We use $\overline{f}$ to denote the Fenchel conjugate of a function $f$: $\overline{f}(t)\defeq\sup_{u\in \mathbb{R}} \{ut-f(u)\}$. We use $\mathbb{I}_{A}$ to denote the convex indicator of a set $A$, i.e., $\mathbb{I}_{A}(x)=0$ if $x\in A$; $\mathbb{I}_{A}(x)=+\infty$ if $x\notin A$.

\section{Background}%\vspace{-1.5mm}
\label{sec-background}
Here we give an overview of the relevant entropic optimal transport (EOT) concepts. For additional details on balanced EOT, we refer to \citep{cuturi2013sinkhorn, genevay2019entropy, peyre2019computational}, unbalanced EOT - to \citep{chizat2017unbalanced, liero2018optimal}.

\vspace{-1mm}
\textbf{$f$-divergences for positive measures}.
For \textit{positive} measures $\mu_1,\mu_{2}\in \mathcal{M}_{2,+}\!(\mathbb{R}^{d'})$ and a lower semi-continuous function $f:\mathbb{R}\rightarrow \mathbb{R}\cup \{\infty\}$, the \textit{$f$-divergence} between $\mu_{1},\mu_{2}$ is defined by
\vspace{-1mm}$$\Df{f}{\mu_1}{\mu_2}\defeq \int_{\mathbb{R}^{d'}} f\bigg(\frac{\mu_{1}(x)}{\mu_{2}(x)}\bigg)\mu_{2}(x)dx\text{ if }\mu_{1}\ll \mu_2\text{ and }+\infty\text{ otherwise}.\vspace{-1mm}$$
We consider $f(t)$ which are convex, non-negative and attain zero uniquely when $t=1$. In this case, $D_{f}$ is a valid measure of dissimilarity between two positive measures (see Appendix \ref{app-ablation} for \underline{details}). This means that $D_{f}(\mu_1\|\mu_2)\!\geq\! 0$ and $D_{f}(\mu_1\|\mu_2)\!=\! 0$ if and only if $\mu_{1}=\mu_{2}$. In our paper, we also assume that all $f$ that we consider satisfy the property that $\overline{f}$ is a \textit{non-decreasing} function.

Kullback-Leibler divergence $\text{D}_{\text{KL}}$ \citep{chizat2017unbalanced, sejourne2022unbalanced}, is a particular case of such $f$-divergence for positive measures.
It has a generator function $f_{\text{KL}}(t)\!\defeq\! t\log t\! -\! t\! +\!1$. Its convex conjugate $\overline{f_{\text{KL}}}(t)\!=\!\exp(t)-1$. Another example is the $\chi^{2}$-divergence $\text{D}_{\chi^2}$ which is generated by $f_{\chi^2}(t)\defeq(t-1)^2$ when $t\geq0$ and $\infty$ otherwise. The convex conjugate of this function is $\overline{f_{\chi^2}}(t)=-1$ if $t\!<\!-2$ and $\frac{1}{4} t^2 \!+\! t$ when $t\!\geq\! 2$.

\textit{Remark.} By the definition, convex conjugates of $f_{\text{KL}}$ and $f_{\chi^2}$ divergences are proper, non-negative and non-decreasing. These properties are used in some of our theoretical results. %\milena{say smth about NaNs??}%In practice, $\text{D}_{\text{KL}}$ divergence

\vspace{-1mm}
\textbf{Entropy for positive measures.} For $\mu\in \mathcal{M}_{2,+}(\mathbb{R}^{d'})$, its entropy \citep{chizat2017unbalanced} is given by
\vspace{-1mm}\begin{equation}
    H(\mu)\!\defeq\! -\int_{\mathbb{R}^{d'}}\mu(x)\! \log \mu(x) dx+\|\mu\|_1\text{, if }\mu\text{ is a.c. and }-\!\infty\text{ otherwise}.
    \label{entropy}\vspace{-1mm}
\end{equation}
When $\mu\in\mathcal{P}_{2,ac}(\mathbb{R}^{d'})$, i.e., $\|\mu\|_1\!=\!1$, equation \eqref{entropy} is the usual differential entropy minus 1.


\textbf{Classic EOT formulation (with the quadratic cost).}
Consider two probability measures $p\!\in\! \mathcal{P}_{2,ac}(\mathbb{R}^d)$, $q\!\in\!\mathcal{P}_{2,ac}(\mathbb{R}^d)$. For $\varepsilon>0$, the EOT problem between $p$ and $q$ is

\vspace{-5mm}\begin{eqnarray}
\min_{\pi\in\Pi(p, q)} \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \frac{\|x-y\|^2}{2} \pi(x, y) dx dy - \varepsilon H(\pi),
\label{eot-classic}
\end{eqnarray}
where $\Pi(p,q)$ is the set of probability measures $\pi\!\in\!\mathcal{P}_{2,ac}(\mathbb{R}^d\!\times\! \mathbb{R}^d)$ with marginals $p$, $q$ (transport plans). %, $H(\pi)$ is the differential entropy. 
Plan $\pi^*$ attaining the minimum exists, it is unique and called the \textit{EOT plan.}

Classic EOT imposes hard constraints on the marginals which leads to several issues, e.g., sensitivity to outliers \citep{balaji2020robust}, inability to handle potential measure shifts such as class imbalances in the measures $p, q$. The UEOT problem \citep{yang2018scalable, choi2024generative} overcomes these issues by relaxing the marginal constraints \citep{sejourne2022unbalanced}.


\textbf{Unbalanced EOT formulation (with the quadratic cost).} 
Let $D_{f_1}$ and $D_{f_2}$ be two $f$-divergences over $\mathbb{R}^d$.
For two probability measures $p\in \mathcal{P}_{2,ac}(\mathbb{R}^d)$, $q\in\mathcal{P}_{2,ac}(\mathbb{R}^d)$ and $\varepsilon>0$, the unbalanced EOT problem between $p$ and $q$ consists of finding a minimizer of
\begin{eqnarray}    \inf_{\gamma\in\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d)} \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \frac{\|x-y\|^2}{2} \gamma(x, y) dx dy -
     \varepsilon H(\gamma)\!+\Df{f_1}{\gamma_x}{p}   + \Df{f_2}{\gamma_y}{q},
     \label{unbalanced-eot-primal}
\end{eqnarray}\vspace*{-6.5mm}
\begin{wrapfigure}{r}{0.43\textwidth}
  \vspace{-2mm}\begin{center}
    \includegraphics[width=0.97\linewidth]{pics/UEOT_plan_weak_def_v3.png}
  \end{center}
  \vspace{-2.0mm}
  \caption{\centering Unbalanced EOT problem.}
\label{fig:ueot-def}
\vspace{-5mm}
\end{wrapfigure}

see Fig. \ref{fig:ueot-def}. Here the minimum is attained for a unique $\gamma^*$ which is called the \textit{unbalanced optimal entropic} (UEOT) plan. Typical choices of $f_i$ ($i\in[1,2]$) are $f_i(t)=\tau_i f_{\text{KL}}(t)$ or $f_i(t)=\tau_i f_{\chi^2}(t)$ ($\tau_i>0$) yielding the scaled $\text{D}_{\text{KL}}$ and $\text{D}_{\chi^{2}}$, respectively. In this case, the bigger $\tau_1$ ($\tau_2$) is, the more $\gamma_x$ ($\gamma_y$) is penalized for not matching the corresponding marginal distribution $p$ ($q$).

\vspace{-0.5mm}\textit{Remark 1.} While the set $\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d)$ contains not only a.c. measures, infimum in problem \eqref{unbalanced-eot-primal} is attained for a.c. measure $\gamma^*$ since $-\varepsilon H(\gamma^*)$ term turns to $+\infty$ otherwise. Thus, almost everywhere in the paper we are actually interested in the subset of a.c. measures in $\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d)$.

\vspace{-0.5mm}\textit{Remark 2.} The balanced EOT problem \eqref{eot-classic} is a special case of \eqref{unbalanced-eot-primal}. Indeed, let $f_1$ and $f_2$ be the convex indicators of $\{1\}$, i.e., $f_1=f_2=\mathbb{I}_{x=1}$. Then the $f$-divergences $\Df{f_1}{\gamma_x}{p}$ and $\Df{f_2}{\gamma_y}{q}$ become infinity if $p\neq\gamma_x$ or $q\neq\gamma_y$, and become zeros otherwise.


\textbf{Dual form of unbalanced EOT problem \eqref{unbalanced-eot-primal}} is formulated as follows
% \vspace{-1mm}
\begin{eqnarray}
    \!\!\!\sup_{(\phi, \psi)} \!\Bigg\lbrace\!\!\! -\!\!\varepsilon \!\!\int_{\mathbb{R}^d}\!\! \int_{\mathbb{R}^d}\! \!\!\exp\{\frac{1}{\varepsilon} \!( \phi(x) \!+\! \psi(y)\!-\! \frac{\|x-y\|^2}{2}) \} dx dy \!- \!\!
    \int_{\mathbb{R}^d} \!\!\overline{f}_1 (-\phi(x))p(x) dx \! - \!\!\!\int_{\mathbb{R}^d}\!\! \overline{f}_2 (-\psi(y))q(y) dy
    \Bigg\rbrace.\!
    \label{unbalanced-eot-dual}
\end{eqnarray}
It is known that there exist two measurable functions $\phi^*$, $\psi^*$ delivering maximum to \eqref{unbalanced-eot-dual}. They have the following connection with the solution of the primal unbalanced problem \eqref{unbalanced-eot-primal}:
\begin{eqnarray}
\label{gamma-potentials}
    \gamma^*(x, y) \!=\! \exp\{\frac{\phi^*(x)}{\varepsilon}\} \!\exp\{-\frac{\|x-y\|^2}{2\varepsilon}\} \!\exp\{\frac{\psi^*(y)}{\varepsilon}\}.
\end{eqnarray}
\vspace{-1mm}\textit{Remark.} For some of our results, we will need to restrict potentials $(\phi, \psi)$ in problem \eqref{unbalanced-eot-dual} to the space $\mathcal{C}_{2,b}(\mathbb{R}^d)\times \mathcal{C}_{2,b}(\mathbb{R}^d)$. Since established variants of dual forms \citep{chizat2017unbalanced} typically correspond to other functional spaces, we derive and theoretically justify a variant of the \underline{\textit{dual problem}} \eqref{unbalanced-eot-dual} in Appendix \ref{appendix-dual-form}. Note that it may hold that optimal potentials $\psi^{*},\phi^{*}\notin \mathcal{C}_{2,b}(\mathbb{R}^{D})$, i.e., the supremum is not achieved within our considered spaces. This is not principle for our subsequent derivations. 

\vspace{-0.5mm}\textbf{Computational UEOT setup.} Analytical solution for the \textit{unbalanced} EOT problem is, in general, not known.\footnote{Analytical solutions are known only for some specific cases. For example, \citep{janati2020entropic} consider Gaussian measures and UEOT problem with $\text{D}_{\text{KL}}$ divergence instead of the differential entropy. This setup differs from ours.}
Moreover, in real-world setups where unbalanced EOT is applicable, the measures $p$, $q$ are typically not available explicitly but only through their empirical samples (datasets). 

\vspace{-0.5mm}We assume that data measures $p, q\in \mathcal{P}_{2,ac}(\mathbb{R}^d)$ are unknown and accessible only by a limited number of i.i.d.\ empirical samples $\{x_0, ..., x_N\}\!\sim\! p$, $\{y_0, ..., y_M\}\!\sim\! q$. We aim to approximate the optimal UEOT plan $\gamma^*$ solving \eqref{unbalanced-eot-primal} between the entire measures $p, q$. The recovered plans should allow the out-of-sample estimation, i.e., generation of samples from $\gamma^*(\cdot|x^{\text{new}})$ where $x^{\text{new}}$ is a new test point (not necessarily present in the train data). Optionally, one may require the ability to sample from $\gamma^*_x$. 


\vspace{-0.5mm}The described setup is typically called the \textit{continuous EOT} and should not be twisted up with the \textit{discrete EOT} \citep{peyre2019computational, cuturi2013sinkhorn}. There the aim is to recover the (unbalanced) EOT plan between the empirical measures $\hat{p}\!=\!\!\frac{1}{N}\!\sum_{i=1}^{N} \!\delta_{x_i}$, $\hat{q}\!=\!\!\frac{1}{M}\!\sum_{j=1}^{M} \!\delta_{y_j}$ and out-of-sample estimations are typically not needed.


\vspace{-1mm}\section{Related Work}\vspace{-2mm}
\label{sec-related-work}

\begin{table*}[!t]
\vspace{-2mm}
\tiny
\hspace{-10mm}\begin{tabular}{ c|c|c|c|c }
\hline
\shortstack{\textbf{Solver}} & \shortstack{\textbf{Problem}} & \shortstack{\textbf{Principles}} & \shortstack{\textbf{What recovers?}} & \shortstack{\textbf{Limitations}}\\ 
\hline
\citep{yang2018scalable} & UOT &  \makecell{Solves $c$-transform based semi-dual\\ max-min reformulation of UOT using neural nets} & \makecell{Scaling factor $\nicefrac{\gamma^*(x)}{p(x)}$ and \\ stochastic OT map $T^*(x,z)$} & \makecell{Complex max-min objective;\\3 neural networks}\\
\hline
\citep{lubeck2022neural} & \makecell{Custom \\UOT} & \makecell{Regression on top of discrete EOT\\ between re-balanced measures\\ combined with ICNN-based solver \citep{makkuva2020optimal}} & \makecell{Scaling factors and \\ OT maps between re-scaled measures} & \makecell{Heuristically uses \\ minibatch OT approximations}\\
\hline
\citep{choi2024generative} & UOT &  \makecell{Solves semi-dual max-min \\reformulation of UOT using neural nets} & Stochastic UOT map $T^*(x,z)$ & \makecell{Complex max-min objective;\\2 neural networks}\\
\hline
\citep{eyring2023unbalancedness} & UEOT & \makecell{Flow Matching on top of discrete UEOT\\using neural nets} & \makecell{Parametrized vector field \\$(v_{t,\theta})_{t\in[0,1]}$ to transport the mass} & \makecell{Heuristically uses \\ minibatch OT approximations}\\
\hline
\citep{klein2023generative} & UEOT & \makecell{Conditional Flow Matching on top of discrete EOT\\between re-balanced measures using neural nets} & \makecell{Scaling factors and parametrized conditional\\ vector field $(v_{t,\theta})_{t\in[0,1]}$ to transport \\the mass between re-scaled measures} & \makecell{Heuristically uses \\ minibatch OT approximations}\\
\hline
 \makecell{U-LightOT \\(\textbf{ours})} &  UEOT & \makecell{Solves non-minimax reformulation \\of dual UEOT using Gaussian Mixtures} & \makecell{Density of UEOT plan $\gamma^*$ together with light\\ procedure to sample $x\sim\gamma^*_x(\cdot)$ and $y\sim\gamma^*_y(\cdot|x)$}& \makecell{Restricted to Gaussian\\ Mixture parametrization}
 \\
\hline
\end{tabular}
\vspace{-1mm}
\captionsetup{justification=centering}
 \caption{Comparison of the principles of existing UOT/UEOT solvers and \textbf{our} proposed light solver.}
 \label{table-comparison}
\vspace{-5mm}
\end{table*}

Nowadays, the sphere of continuous OT/EOT solvers is actively developing. Some of the early works related to this topic utilize OT cost as the loss function \citep{gulrajani2017improved, genevay2018learning, arjovsky2017wasserstein}. These approaches are not relevant to us as they do not learn OT/EOT maps (or plans).
We refer to \citep{korotin2022kantorovich} for a detailed review.


\vspace{-1mm}At the same time, there exist a large amount of works within the discrete OT/EOT setup \citep{cuturi2013sinkhorn,dvurechenskii2018decentralize,xie2022accelerated, nguyen2022unbalanced}, see \citep{peyre2019computational} for a survey. We again emphasize that solvers of this type are not relevant to us as they construct discrete matching between the given (train) samples and typically do not provide a generalization to the new unseen (test) data. Only recently ML community started developing out-of-sample estimation procedures based on discrete/batched OT. For example, \citep{fatras2020learning,pooladian2021entropic,hutter2021minimax,deb2021rates,manole2021plugin,rigollet2022sample} mostly develop such estimators using the barycentric projections of the discrete EOT plans. Though these procedures have nice theoretical properties, their scalability remains unclear.

\textbf{Balanced OT/EOT solvers.} 
There exists a vast amount of neural solvers for continuous OT problem.
Most of them learn the OT maps (or plans) via solving saddle point optimization problems \citep{asadulaev2024neural, fan2023neural, korotin2021neural, gazdieva2022unpaired, rout2022generative, mokrov2024energy}. 
Though the recent works \citep{gushchin2023entropic, seguy2018large, daniels2021score,korotin2024light,gushchin2024light} tackle the EOT problem \eqref{eot-classic}, they consider its balanced version. Hence they are not relevant to us. 
Among these works, only \citep{korotin2024light,gushchin2024light} evade non-trivial training/inference procedures and are ideologically the closest to ours. The difference between them consists of the particular loss function used.
In fact, \textbf{our paper} proposes the solver which subsumes these solvers and generalizes them for the unbalanced case. The derivation of our solver is non-trivial and requires solid mathematical apparatus, see \wasyparagraph\ref{sec-method}. 


\textbf{Unbalanced OT/EOT solvers.} A vast majority of early works in this field tackle the discrete UOT/UEOT setup \citep{chapel2021unbalanced, fatras2021unbalanced,pham2020unbalanced} but the principles behind their construction are not easy to generalize to the continuous setting. Thus, many of the recent papers that tackle the continuous unbalanced OT/EOT setup employ discrete solutions in the construction of their solvers. For example, \citep{lubeck2022neural} regress neural network on top of scaling factors obtained using the discrete UEOT while simultaneously learning the continuous OT maps using an ICNN method \citep{makkuva2020optimal}. In \citep{eyring2023unbalancedness} and \citep{klein2023generative}, the authors implement Flow Matching \citep[FM]{lipman2022flow} and conditional FM on top of the discrete UEOT plans, respectively. 
The algorithm of the latter consists of regressing neural networks on top of scaling factors and simultaneously learning a conditional vector field to transport the mass between re-balanced measures.
Despite the promising practical performance of these solvers, it is still unclear to what extent such approximations of UEOT plans are theoretically justified.


The recent papers \citep{yang2018scalable, choi2024generative} are more related to our study as they do not rely on discrete OT approximations of the transport plan. However, they have non-trivial minimax optimization objectives solved using \textit{complex} GAN-style procedures. Thus, these GANs often lean on heavy neural parametrization, may incur instabilities during training, and require careful hyperparameter selection \citep{lucic2018gans}. 

For completeness, we also mention other papers which are only slightly relevant to us. For example, \citep{gazdieva2023extremal} considers incomplete OT which relaxes only one of the OT marginal constraints and is less general than the unbalanced OT. Other works \citep{dao2023robust, balaji2020robust} incorporate unbalanced OT into the training objective of GANs aimed at generating samples from noise. 

In contrast to the listed works, our paper proposes a \textit{theoretically justified and lightweight} solver to the UEOT problem, see Table \ref{table-comparison} for the detailed comparison of solvers.

\vspace{-3.5mm}
\section{Light Unbalanced OT Solver}
\vspace{-2.5mm}
\label{sec-method}
In this section, we derive the optimization objective (\wasyparagraph \ref{sec-optimization}) of our U-LightOT solver, present practical aspects of training and inference procedures (\wasyparagraph \ref{sec-algorithm}) and derive the generalization bounds for our solver (\wasyparagraph\ref{sec-generalization-bounds}). We provide the \textit{\underline{proofs of all theoretical results}} in Appendix \ref{sec-proofs}.


\subsection{Derivation of the Optimization Objective}
\label{sec-optimization}
Following the learning setup described above, we aim to get a parametric approximation $\gamma_{\theta, w}$ of the UEOT plan $\gamma^*$. Here $\theta,\omega$ are the model parameters to learn, and it will be clear later why we split them into two groups. To recover $\gamma_{\theta,\omega}\approx \gamma^{*}$, our aim is to learn $\theta,\omega$ by directly minimizing the $\text{D}_{\text{KL}}$ divergence between $\gamma_{\theta,w}$ and $\gamma^*$: 
\vspace{-2mm}\begin{equation}
    \KL{\gamma^*}{\gamma_{\theta,w}}\rightarrow \min_{(\theta,w)}.
    \label{main-objective}
\end{equation}\vspace{-4mm}


The main difficulty of this optimizing objective \eqref{main-objective} is obvious: the UEOT plan $\gamma^*$ is \textit{unknown}. Fortunately, below we show that one still can optimize \eqref{main-objective} without knowing $\gamma^{*}$. 

Recall that the optimal UEOT plan $\gamma^*$ has the form \eqref{gamma-potentials}. We first make some changes of the variables. Specifically, we define 
$v^*(y)\defeq\exp\{\frac{2\psi^*(y)-\|y\|^2}{2\varepsilon}\}$.
Formula \eqref{gamma-potentials} now reads as
\begin{eqnarray}
    \gamma^*(x, y) = \exp\{\frac{2\phi^*(x)-\|x\|^2}{2\varepsilon}\} \!
    \exp\{\frac{\langle x, y\rangle}{\varepsilon}\} v^*(y)\! \Longrightarrow \label{gamma-mid-potentials} %\nonumber \\
    \!\gamma^*(y|x) \propto \exp\{\frac{\langle x, y\rangle}{\varepsilon}\} v^*(y).
    \label{v-unnormalized}
\end{eqnarray}
Since the conditional plan has the unit mass, we may write
\vspace{-1mm}\begin{eqnarray}
    \gamma^*(y|x)=\exp\{\frac{\langle x, y\rangle}{\varepsilon}\} \frac{v^*(y)}{c_{v^*}(x)}
    \label{conditional-distrib}
\end{eqnarray}\vspace{-5mm}

where $c_{v^*}(x)\!\!\defeq\!\!\int_{\mathbb{R}^d} \exp\{\frac{\langle x, y\rangle}{\varepsilon}\} v^*(y) dy$ is the normalization costant ensuring that $\int_{\mathbb{R}^d} \gamma^* (y|x) dy = 1$. 

Consider the decomposition $\gamma^*(x, y) = \gamma_x^*(x) \gamma^*(y|x)$. It shows that to obtain parametrization for the entire plan $\gamma^*(x,y)$, it is sufficient to consider parametrizations for its left marginal $\gamma^*_x$ and the conditional measure $\gamma^*(y|x)$. 
Meanwhile, equation \eqref{conditional-distrib} shows that \textit{conditional measures $\gamma^*(\cdot| x)$ are entirely described by the variable $v^*$}. We use these observations to parametrize $\gamma_{\theta,w}$. We set
\vspace{-2mm}\begin{eqnarray}
    \gamma_{\theta, w}(x,y)\defeq u_{\omega}(x)\gamma_{\theta}(y|x)\!=\!
    u_{\omega}(x) \!\frac{\exp\{\nicefrac{\langle x, y\rangle}{\varepsilon}\}\! v_{\theta}(y)}{c_{\theta}(x)},\!
    \label{parametrization}\vspace{-2mm}
\end{eqnarray}
where $u_{\omega}$ and $v_{\theta}$ parametrize marginal measure $\gamma_x^*$ and the variable $v^*$, respectively. In turn, the constant $c_{\theta}(x)\defeq\int_{\mathbb{R}^d} \exp\{\frac{\langle x, y\rangle}{\varepsilon}\} v_{\theta}(y) dy$ is the parametrization of $c_{v^*}(x)$. Next, we demonstrate our \underline{\textbf{main result}} which shows that the optimization of \eqref{main-objective} can be done \textit{without} the access to $\gamma^*$.

\begin{theorem}[Tractable form of $\text{D}_{\text{KL}}$ minimization] 
    Assume that $\gamma^*$ is parametrized using \eqref{parametrization}. Then the following bound holds: $\varepsilon\KL{\gamma^*}{\gamma_{\theta,w}}\!\leq\! \mathcal{L}(\theta,w)-\mathcal{L}^*,$  where
    \vspace{-1mm}\begin{eqnarray}
        \mathcal{L}(\theta,w)\!\defeq\!\! 
    \int_{\mathbb{R}^d}\!\! \overline{f}_1\! (- \varepsilon \log \frac{u_{\omega}(x)}{c_{\theta}(x)}\!-\!\frac{\|x\|^2}{2})p(x) dx \!+\!\! 
    \int_{\mathbb{R}^d}\! \overline{f}_2 (-\varepsilon \log v_{\theta}(y)\! -\!\frac{\|y\|^2}{2})q(y) dy \!+\! \varepsilon \|u_{\omega}\|_1,
    \label{tractable-objective}
    \end{eqnarray}%\vspace{-1mm}
    and constant $(-\mathcal{L}^*$) is the optimal value of the dual form \eqref{unbalanced-eot-dual}. The bound is \textbf{tight} in the sence that it turns to $0=0$ when $v_{\theta}(y)\!=\!\!\exp\{\nicefrac{2\psi^*(y)-\|y\|^2}{2\varepsilon}\}$ and $u_{\omega}(x)\!=\!\!c_{\theta}(x)\exp\{\nicefrac{2\phi^*(x)-\|x\|^2}{2\varepsilon}\}$.
    \label{thm-kl}
\end{theorem}\vspace{-2mm}
In fact, \eqref{tractable-objective} is the dual form \eqref{unbalanced-eot-dual} but with potentials $(\phi,\psi)$ expressed through $u_{\omega}, v_{\theta}$ (and $c_{\theta}$):
\vspace{-1mm}
\begin{eqnarray*}
\phi(x)\!\leftarrow\!\phi_{\theta,\omega}(x)\!=\!\varepsilon\! \log\! \frac{u_{\omega}(x)}{c_{\theta}(x)}\!+\!\frac{\|x\|^2}{2},\;\;\;\psi(y)\!\leftarrow\! \psi_{\theta}(y)\!=\!\varepsilon \log v_{\theta}(y)\! +\!\frac{\|y\|^2}{2}.\!\vspace{-2mm}
\end{eqnarray*}
Our result can be interpreted as the bound on the quality of approximate solution $\gamma_{\theta,\omega}$ \eqref{unbalanced-eot-primal} recovered from the approximate solution to the dual problem \eqref{unbalanced-eot-dual}. It can be directly proved using the original $(\phi,\psi)$ notation of the dual problem, but we use $(u_{\omega},v_{\theta})$ instead as with this change of variables the form of the recovered plan $\gamma_{\theta,\omega}$ is more interpretable ($u_{w}$ defines the first marginal, $v_{\theta}$ -- conditionals).

 Instead of optimizing \eqref{main-objective} to get $\gamma_{\theta,\omega}$, we may optimize the upper bound \eqref{tractable-objective} which is more tractable. Indeed, \eqref{tractable-objective} is a sum of the expectations w.r.t. the probability measures $p,q$. We can obtain Monte-Carlo estimation of \eqref{tractable-objective} from random samples and optimize it with stochastic gradient descent procedure w.r.t. $(\theta, \omega)$. The main \textbf{challenge} here is the computation of the variable $c_{\theta}$ and term $\|u_{\omega}\|_1$. Below we propose a smart parametrization by which both variables can be derived analytically.


\subsection{Parametrization and the Optimization Procedure}

\textbf{Parametrization.} Recall that $u_{\omega}$ parametrizes the density of the marginal $\gamma^*_x$  which is unnormalized. Setting $x=0$ in equation \eqref{v-unnormalized}, we get $\gamma^*(y|x=0) \propto v^*(y)$ which means that $v^*$ also corresponds to an unnormalized density of a measure.  These motivate us to use the unnormalized Gaussian mixture parametrization for the potential $v_{\theta}(y)$ and measure $u_{\omega}(x)$: 
\vspace{-0.5mm}\begin{eqnarray}
    \label{gauss-parametrization}
    v_{\theta}(y)\!=\!\sum_{k=1}^{K}\!\! \alpha_k \mathcal{N}(y|r_k, \!\varepsilon S_k); \;\;
    u_{\omega}(x)\!\!=\!\!\!\sum_{l=1}^{L} \!\beta_l \mathcal{N}(x|\mu_l, \!\varepsilon \Sigma_l).\!
    \label{marginal-parametrization}
\end{eqnarray}\vspace{-5mm}

Here $\theta\defeq\{\alpha_k, r_k, S_k\}_{k=1}^{K}$, $w\defeq\{\beta_l, \mu_l, \Sigma_l\}_{l=1}^{L}$ with $\alpha_k,\beta_l\geq0$, $r_k, \mu_l\in\mathbb{R}^d$ 
and $0\prec S_k,\Sigma_l\in \mathbb{R}^{d\times d}$. The covariance matrices are scaled by $\varepsilon$ just for convenience.

For this type of parametrization, it holds that $\|u_{\omega}\|_1=\sum_{l=1}^L \beta_l.$ Moreover, there exist closed-from expressions for the normalization constant $c_{\theta}(x)$ and conditional plan $\gamma_{\theta}(y|x)$, see \citep[Proposition 3.2]{korotin2024light}. Specifically, define $r_k(x)\defeq r_k+S_k x$ and $\widetilde{\alpha}_{k}(x)\defeq \alpha_k \exp\{\frac{x^T S_k x +2 r_k^T x}{2 \varepsilon}\}$. It holds that
\vspace{-1.5mm}\begin{eqnarray}
    c_{\theta}(x) = \sum_{k=1}^K \widetilde{\alpha}_{k}(x);\;\;
    \gamma_{\theta}(y|x)=\frac{1}{c_{\theta}(x)} \sum_{k=1}^K \widetilde{\alpha}_{k}(x) \mathcal{N}(y|r_k(x), \varepsilon S_k).
    \label{gamma-gauss-parametrization}
\end{eqnarray}
\vspace{-2mm}
Using this result and \eqref{gauss-parametrization}, we get the expression for $\gamma_{\theta, w}$:
\begin{eqnarray}
    \gamma_{\theta, w}(x,y)=u_{\omega}(x)\cdot \gamma_{\theta}(y|x)=
    \underbrace{\sum_{l=1}^{L} \!\beta_l \mathcal{N}(x|\mu_l, \!\varepsilon \Sigma_l)}_{u_{\omega}(x)} \cdot
    \underbrace{\frac{ \sum_{k=1}^K \widetilde{\alpha}_{k}(x) \mathcal{N}(y|r_k(x), \varepsilon S_k)}{\sum_{k=1}^K \widetilde{\alpha}_{k}(x)}}_{\gamma_{\theta}(y|x)}.
\end{eqnarray}
\vspace{-3mm}

\label{sec-algorithm}
\textbf{Training.} We recall that the measures $p, q$ are accessible only by a number of empirical samples (see the learning setup in \wasyparagraph \ref{sec-background}). Thus, given samples $\{x_1,...,x_N\}$ and $\{y_1,...,y_M\}$, we optimize the empirical analog of \eqref{tractable-objective}:
\vspace{-1mm}\begin{eqnarray}
   \widehat{\mathcal{L}}(\theta, w)\!\defeq\!
      \frac{1}{N}\sum_{i=1}^N \overline{f}_1 (- \varepsilon \log \frac{u_{\omega}(x_i)}{c_{\theta}(x_{i})}\!-\!\frac{\|x_i\|^2}{2}) \!+\!
    \frac{1}{M}\sum_{j=1}^M  \overline{f}_2 (-\varepsilon \log v_{\theta}(y_j) \!-\!\frac{\|y_j\|^2}{2})\! \!+\! \varepsilon \|u_{\omega}\|_1
    \label{empirical-objective}
\end{eqnarray}\vspace*{-4mm}

using minibatch gradient descent procedure w.r.t. parameters $(\theta, w)$. In the parametrization of $v_{\theta}$ and $u_{\omega}$ \eqref{gauss-parametrization}, we utilize the diagonal matrices $S_k$, $\Sigma_l$. This allows decreasing the number of learnable parameters in $\theta$ and speeding up the computation of inverse matrices $S_k^{-1},\;\Sigma_l^{-1}$. 

\vspace{-1mm}\textbf{Inference.} Sampling from the conditional and marginal measures $\gamma_{\theta}(y|x)\!\approx\! \gamma^*(y|x)$, $u_{\omega}\!\approx\! \gamma_{x}^{*}$ is easy and lightweight since they are explicitly parametrized as Gaussian mixtures, see \eqref{gamma-gauss-parametrization}, \eqref{marginal-parametrization}.

\vspace{-1mm}\subsection{Connection to Related Prior Works}\vspace{-1mm}
\label{sec-prior-work}
The idea of using Gaussian Mixture parametrization for dual potentials in EOT-related tasks first appeared in the EOT/SB benchmark \citep{gushchin2023building}. There it was used to obtain the benchmark pairs of probability measures with the known EOT solution between them. In \citep{korotin2024light}, the authors utilized this type of parametrization to obtain a light solver \textbf{(LightSB)} for the \textbf{balanced} EOT.

Our solver for \textbf{unbalanced} EOT
\eqref{tractable-objective} subsumes their solver for balanced EOT as well as one problem subsumes the other for the special case of divergences, see the remark in \wasyparagraph\ref{sec-background}.  Let $f_1$, $f_2$ be convex indicators of $\{1\}$. Then $\overline{f_1}(t)=t$ and $\overline{f_2}(t)=t$ and objective \eqref{tractable-objective} becomes
\vspace{-0.1mm}\begin{eqnarray}
    \mathcal{L}(\theta,w)\!=\! 
    \int_{\mathbb{R}^d} \!(- \varepsilon \log \frac{u_{\omega}(x)}{c_{\theta}(x)}\!-\!\frac{\|x\|^2}{2})p(x) dx + 
    \int_{\mathbb{R}^d} (-\varepsilon \log v_{\theta}(y) \!-\!\frac{\|y\|^2}{2})q(y) dy \!+\! \varepsilon \|u_{\omega}\|_1=
    \nonumber\\
    - \varepsilon\Big(\!\int_{\mathbb{R}^d}\!\!\!\log \frac{u_{\omega}(x)}{c_{\theta}(x)}p(x)dx \!+\!\!\!
    \int_{\mathbb{R}^d} \!\!\log v_{\theta}(y)q(y)dy\!-\!
    \|u_{\omega}\|_1\!\Big)-\!\underbrace{\int_{\mathbb{R}^d}\!\!\frac{\|x\|^2}{2}p(x) dx \!-\!\! \int_{\mathbb{R}^d}\!\! \frac{\|y\|^2}{2}q(y) dy}_{\defeq \text{Const}(p,q)}\!=\!
    \nonumber\\
    \varepsilon\big(\!\!\int_{\mathbb{R}^d}\log c_{\theta}(x)p(x)dx\big) \!-\!\!
    \int_{\mathbb{R}^d} \log v_{\theta}(y)q(y)dy\big)- 
    \label{lightsb-objective}
    \\
    \varepsilon \int_{\mathbb{R}^d} \log u_{\omega}(x) p(x)dx + \varepsilon \|u_{\omega}\|_1 - \text{Const}(p,q).
    \label{marginal-objective}
    % \nonumber\\
    % \int_{\mathbb{R}^d}\frac{\|x\|^2}{2}p(x) dx - \int_{\mathbb{R}^d} \frac{\|y\|^2}{2}q(y) dy\nonumber
\end{eqnarray}\vspace{-2mm}

Here line \eqref{lightsb-objective} depends exclusively on $\theta$ and exactly coincides with the LightSB's objective, see \citep[Proposition 8]{korotin2024light}. At the same time, line \eqref{marginal-objective} depends only on $\omega$, and its minimum is attained when $u_{\omega}=p$. Thus, this part is not actually needed in the balanced case, see \citep[Appendix C]{korotin2024light}.


\vspace{-1.5mm}\subsection{Generalization Bounds and Universal Approximation Property}\vspace{-1mm}
\label{sec-generalization-bounds}

Theoretically, to recover the UEOT plan $\gamma^*$, one needs to solve the problem $\mathcal{L}(\theta,\omega)\rightarrow \min_{\theta,\omega}$ as stated in our Theorem \ref{thm-kl}. In practice, the measures $p$ and $q$ are accessible via empirical samples $X\defeq\{x_1,...,x_N\}$ and $Y\defeq\{y_1,...,y_M\}$, thus, one needs to optimize the empirical counterpart $\widehat{\mathcal{L}}(\theta,\omega)$ of $\mathcal{L}(\theta,\omega)$, see \eqref{empirical-objective}. Besides, the available potentials $u_{\omega}$, $v_{\theta}$ over which one optimizes the objective come from the restricted class of functions. Specifically, we consider unnormalized Gaussian mixtures $u_{\omega}$, $v_{\theta}$ with $K$ and $L$ components respectively. 
Thus, one may naturally wonder: \textbf{how close is the recovered plan $\gamma_{\widehat{\theta}, \widehat{\omega}}$ to the UEOT plan $\gamma^*$} given that $(\widehat{\theta},\widehat{\omega})=\arg\min_{\theta,\omega}\widehat{\mathcal{L}}(\theta,\omega)$?
To address this question, we study the \textit{generalization error} $\mathbb{E}\KL{\gamma^*}{\gamma_{\widehat{\theta},\widehat{\omega}}}$, i.e., the expectation of $\text{D}_{\text{KL}}$ between $\gamma^*$ and $\gamma_{\widehat{\theta},\widehat{\omega}}$ 
taken w.r.t. the random realization of the train datasets $X\sim p$, $Y\sim q$. 

Let $(\overline{\theta}, \overline{\omega})\!=\!\arg\min_{(\theta,\omega)\in \Theta\times \Omega} \mathcal{L}(\theta,\omega)$ denote the best approximators of $\mathcal{L}(\theta,\omega)$ in the admissible class. From Theorem \ref{thm-kl} it follows that that $\mathbb{E}\KL{\gamma^*}{\gamma_{\widehat{\theta},\widehat{\omega}}}$
can be upper bounded using ${\mathbb{E}(\mathcal{L}(\widehat{\theta},\widehat{\omega}) - \mathcal{L}^*)}$. The latter can be decomposed into the estimation and approximation errors:
\begin{eqnarray}
    % \mathbb{E}\KL{\gamma^*}{\gamma_{\widehat{\theta},\widehat{\omega}}}\leq 
    \mathbb{E}(\mathcal{L}(\widehat{\theta},\widehat{\omega}) \!-\!\! \mathcal{L}^*)\!=\!
    \mathbb{E}[\mathcal{L}(\widehat{\theta},\widehat{\omega}) \!-\!\! \mathcal{L}(\overline{\theta},\overline{\omega})] \!+\! \mathbb{E}[\mathcal{L}(\overline{\theta},\overline{\omega}) \!-\!\! \mathcal{L}^*]\!=\!
    % \nonumber\\
    \underbrace{\mathbb{E}[\mathcal{L}(\widehat{\theta},\widehat{\omega}) \!-\!\! \mathcal{L}(\overline{\theta},\overline{\omega})]}_{\text{Estimation error}} \!+\! \underbrace{[\mathcal{L}(\overline{\theta},\overline{\omega})\!-\!\! \mathcal{L}^*]}_{{\text{Approximation error}}}.
\end{eqnarray}

\vspace{-4mm}
We establish the quantitative bound for the estimation error in the proposition below.

 \begin{proposition}[Bound for the estimation error] Let $p,q$ be compactly supported and assume that $\overline{f}_1,\;\overline{f}_2$ are Lipshitz. Assume that the considered parametric classes $\Theta$, $\Omega$ $(\ni (\theta, \omega))$ consist of unnormalized Gaussian mixtures with $K$ and $L$ components respectively with bounded means $\|r_{k}\|,\|\mu_l\|\leq R$ (for some $R>0$), covariances $sI\preceq S_{k},\Sigma_l\preceq SI$ (for some $0<s\leq S$) and weights $a\leq \alpha_{k},\beta_l\leq A$ (for some $0<a\leq A$). Then the following holds:
\vspace{-1mm}$$\mathbb{E}\big[\mathcal{L}(\widehat{\theta}, \widehat{\omega})-\mathcal{L}(\overline{\theta}, \overline{\omega})\big]\leq O(\frac{1}{\sqrt{N}})+O(\frac{1}{\sqrt{M}}),$$\vspace{-4.5mm}

where $O(\cdot)$ hides the constants depending only on $K,L,R,s,S,a,A,p,q, \varepsilon$ but not on sizes $M,N$.
\label{prop-estimation}
\end{proposition}\vspace{-1mm}

This proposition allows us to conclude that the estimation error converges to zero when $N$ and $M$ tend to infinity at the usual parametric rate. It remains to clarify the question: \textit{can we make the approximation error arbitrarily small}? We answer this question positively in our Theorem below.

\begin{theorem}[Gaussian mixture parameterization for the variables provides the universal approximation of UEOT plans] Let $p$ and $q$ be compactly supported and assume that $\overline{f}_1,\;\overline{f}_2$ are Lipshitz. Then for all $\delta>0$ there exist Gaussian mixtures $v_{\theta}$, $u_{\omega}$ \eqref{gauss-parametrization} with \textbf{scalar} covariances $S_{k}=\lambda_{k}I_{d}\succ 0$, $\Sigma_{l}=\zeta_{l}I_{d}\succ 0$ of their components that satisfy $\KL{\gamma^{*}}{\gamma_{\theta,\omega}}\leq \varepsilon^{-1}(\mathcal{L(\theta,\omega})-\mathcal{L}^*)< \delta.$
\label{thm-universal-approximation}
\end{theorem}
\vspace{-0.9mm}\textbf{In summary}, results of this section show that one can make the generalization error \textit{arbitrarily small} given a sufficiently large amount of samples and components in the Gaussian parametrization. It means that theoretically our solver can recover the UEOT plan arbitrarily well.

\vspace{-3mm}
\section{Experiments}\vspace{-2mm}
\label{sec-experiments}
\begin{figure*}[t!]
\begin{subfigure}[t]{\linewidth}
\begin{center}
    \vspace{-2mm}
    \centering
    \includegraphics[width=0.5\textwidth]{pics/toy_legend_blank.png}
\end{center}
\end{subfigure}
\centering
\begin{subfigure}[b]{0.235\linewidth}
    \centering
    \includegraphics[width=0.995\linewidth]{pics/toy_input.png}
    \caption{Input, target measures}
    \label{fig:gauss-input}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.235\linewidth}
    \centering
    \includegraphics[width=0.995\linewidth]{pics/toy_100.png}
    \caption{U-LightOT, $\tau=10^2$}
    \label{fig:gauss-lightsb}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.235\linewidth}
    \centering
    \includegraphics[width=0.995\linewidth]{pics/toy_10.png}
    \caption{U-LightOT, $\tau=10^1$}
    \label{fig:gauss-mix}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.235\linewidth}
    \centering
    \includegraphics[width=0.995\linewidth]{pics/toy_unb.png}
    \caption{U-LightOT, $\tau=10^0$}
    \label{fig:gauss-ulightsb}
\end{subfigure}
\vspace{-1mm}\caption{\centering Conditional plans $\gamma_{\theta,\omega}(y|x)$ learned by our solver in \textit{Gaussians Mixture} experiment with unbalancedness parameter $\tau\in[10^0,10^1,10^2]$. Here $p_{\omega}$ denotes the normalized first marginal $u_{w}$, i.e., $p_{\omega}=u_{\omega}/\|u_{\omega}\|_1$. }
\label{fig:gauss-kl}
\end{figure*}

In this section, we test our U-LightOT solver on several setups from the related works. 
The code is written using \texttt{PyTorch} framework and is publicly available at
\vspace{-1mm}\begin{center}
    \url{https://github.com/milenagazdieva/LightUnbalancedOptimalTransport}.
\end{center}\vspace{-1mm}
The experiments are issued in the convenient form of \texttt{*.ipynb} notebooks. Each experiment requires several minutes of training on CPU with 4 cores. Technical \textit{\underline{training details}} are given in Appendix \ref{sec:exp-details}. 

\vspace{-2mm}
\subsection{Example with the Mixture of Gaussians}\vspace{-2mm}
\label{sec-gaussian-exp}

 We provide an illustrative \textit{'Gaussians Mixture'} example in 2D to demonstrate the ability of our solver to deal with the imbalance of classes in the source and target measures. We follow the experimental setup proposed in \citep[Figure 2]{eyring2023unbalancedness} and define the probability measures $p$, $q$ as follows (Fig. \ref{fig:gauss-input}):
$$p(x)\!=\!\frac{1}{4} \mathcal{N}(x|(-3, 3), 0.1\cdot I_2)\!+\!\frac{3}{4} \mathcal{N}(x|(1, 3), 0.1 \cdot I_2),$$
$$q(y)\!=\!\frac{3}{4} \mathcal{N}(y|(-3,0), 0.1 \cdot I_2)\!+\!\frac{1}{4} \mathcal{N}(y|(1,0), 0.1 \cdot I_2).$$

We test our U-LightOT solver with \textit{scaled} $\text{D}_{\text{KL}}$ divergences, i.e., $f_1(t), f_2(t)$ are defined by $\tau \cdot f_{\text{KL}}(t)= \tau(t\log(t) - t + 1)$ where $\tau>0$. We provide the learned plans for $\tau\in[1,\;10^1,\;10^2]$. The results in Fig. \ref{fig:gauss-kl} show that parameter $\tau$ can be used to control the level of unbalancedness of the learned plans. For $\tau=1$, our U-LightOT solver truly learns the UEOT plans, see Fig. \ref{fig:gauss-ulightsb}. When $\tau$ increases, the solver fails to transport the mass from the input Gaussians to the closest target ones.
Actually, for $\tau=10^2$, our solutions are similar to the solutions of \citep[LightSB]{korotin2024light} solver which approximates balanced EOT plans between the measures. Hereafter, we treat $\tau$ as the \textit{unbalancedness} parameter. In Appendix \ref{app-ablation}, we test the performance of our solver with \underline{$\text{D}_{\chi^{2}}$ divergence.}

\textit{Remark.} Here we conduct all experiments with the entropy regularization parameter $\varepsilon=0.05$. The parameter $\varepsilon$ is responsible for the stochasticity of the learned transport $\gamma_{\theta}(\cdot|x)$. Since we are mostly interested in the correct transport of the mass (controlled by $f_1,f_2$) rather than the stochasticity, we do not pay much attention to $\varepsilon$ throughout the paper. 


\subsection{Unpaired Image-to-Image Translation}
\label{sec-alae-exp}


\vspace{-1mm}
Another popular testbed which is usually considered in OT/EOT papers is the unpaired image-to-image translation \citep{zhu2017toward} task. Since our solver uses the parametrization based on Gaussian mixture, it may be hard to apply U-LightOT for learning translation directly in the image space. Fortunately, nowadays it is common to use autoencoders \citep{rombach2022high} for more efficient translation. 
We follow the setup of \citep[Section 5.4]{korotin2024light} and use pre-trained ALAE autoencoder \citep{pidhorskyi2020adversarial} for $1024\times1024$ FFHQ dataset \citep{karras2019style} of human faces. We consider different subsets of FFHQ dataset (\textit{Adult}, \textit{Young}, \textit{Man}, \textit{Woman}) and all variants of translations between them: \textit{Adult}$\leftrightarrow$\textit{Young} and \textit{Woman}$\leftrightarrow$\textit{Man}.

\begin{figure*}[ht!]
\includegraphics[width=\linewidth]{pics/alae_comp.pdf}
\caption{\centering Visualization of pairs of accuracies (\textit{keep}-\textit{target}) 
for our U-LightOT solver and other OT/EOT methods in the image translation experiment. The values of unbalancedness parameters for our U-LightOT solver ($\tau$) and \citep[UOT-FM]{eyring2023unbalancedness} ($\lambda=reg\_m$) are specified directly on the plots.
}
\label{fig:alae-comp}
\end{figure*}


\begin{wraptable}{r}{4.5cm}
    \vspace{-4mm}
    \centering
    \footnotesize
    \begin{tabular}{ c|c c  } 
    \hline
    \textbf{Class} & \textit{Man} & \textit{Woman}  \\
    \hline
     \textit{Young}  & $15K$    &$23K$     \\  \hline
     \textit{Adult}  &$7K$    &$3.5K$     \\ \hline
    \end{tabular}
    \captionsetup{justification=centering}
    \caption{Number of \textit{train} FFHQ images for each subset.}
     \label{table-classes}
     \vspace{-4mm}
\end{wraptable}The main challenge of the described translations is the \textbf{imbalance} of classes in the images from source and target subsets, see Table \ref{table-classes}. Let us consider in more detail \textit{Adult}$\rightarrow$\textit{Young} translation. In the FFHQ dataset, the amount of \textit{adult men} significantly outnumbers the \textit{adult women}, while the amount of \textit{young men} is smaller than that of \textit{young women}. Thus, balanced OT/EOT solvers are expected to translate some of \textit{adult man} representatives to \textit{young woman} ones. At the same time, solvers based on unbalanced transport are supposed to alleviate this issue.

\textbf{Baselines.} We perform a comparison with the recent procedure \citep[UOT-FM]{eyring2023unbalancedness} which considers roughly the same setup and demonstrates good performance. This method interpolates the results of unbalanced discrete OT to the continuous setup using the flow matching~\cite{lipman2022flow}. For completeness, we include the comparison with LightSB and balanced optimal transport-based flow matching (OT-FM)~\cite{lipman2022flow} to demonstrate the issues of the balanced solvers. We also consider neural network based solvers relying on the adversarial learning such as UOT-SD \citep{choi2024generative} and UOT-GAN\cite{yang2018scalable}. 

\textbf{Metrics.} We aim to assess the ability of the solvers to perform the translation of latent codes keeping the class of the input images unchanged, e.g., keeping the gender in \textit{Adult}$\rightarrow$\textit{Young} translation. Thus, we train a $99\%$ MLP classifier for gender using the latent codes of images. We use it to compute the accuracy of preserving the gender during the translation. We denote this number by \textit{accuracy (keep)}. 

Meanwhile, it is also important to ensure that the generated images belong to the distribution of the target images rather than the source ones, e.g., belong to the distribution of \textit{young} people in our example. To monitor this property, we use another 99\% MLP classifier to identify whether each of the generated images belong to the target domain or to the source one. Then we calculate the fraction of the generated images belonging to the target domain which we denote as \textit{accuracy (target).
}


\textbf{Results.} 
Unbalanced OT/EOT approaches are equipped with some kind of unbalancedness parameter (like $\tau$ in our solver) which influences the methods' results: with the increase of unbalancedness, accuracy of keeping the class increases but the accuracy of mapping to the target decreases. The latter is because of the relaxation of the marginal constraints in UEOT. For a fair comparison, we aim to compute the trade-offs between the keep/target accuracies for different values of the unbalancedness. We do this for our solver and UOT-FM. We found that GAN-based unbalanced approaches show unstable behaviour, so we report UOT-SD and UOT-GAN's results only for one parameter value. 

For convenience, we visualize the accuracies' pairs for our solver and its competitors in Fig. \ref{fig:alae-comp}. The evaluation shows that our U-LightOT method can effectively solve translation tasks in high dimensions ($d=512$) and outperforms its alternatives in dealing with class imbalance issues. Namely, we see that our solver allows for achieving the best accuracy of keeping the attributes of the input images than other methods while it provides good accuracy of mapping to the target class. While balanced methods and some of the unbalanced ones provide really high accuracy of mapping to the target, their corresponding accuracies of keeping the attributes are worse than ours meaning that we are better in dealing with class imbalance issues. As expected, for large parameter $\tau$, the results of our U-LightOT solver coincide with those for LightSB which is a balanced solver. Meanwhile, our solver has the lowest wall-clock running time among the existing unbalanced solvers, see Table \ref{table-wall-clock-time} for comparison. We demonstrate qualitative results of our solver and baselines in Fig. \ref{fig:alae}. The choice of unbalancedness parameter for visualization of our method and UOT-FM is detailed in Appendix \ref{sec:exp-details}.

We present the results of the \textit{\underline{quantitative comparison}} in the form of tables in Appendix \ref{app-alae-comp}. 
 In Appendix \ref{app-ablation}, we perform the \textit{\underline{ablation study}} of our solver focusing on the selection of parameters $\tau$, $\varepsilon$ and number of Gaussian mixtures' components.

\begin{figure*}[t!]
\begin{center}
\begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=0.999\linewidth]{pics/Fig_YOUNG_TO_ADULT_7.png}
    \caption{\textit{Young} $\rightarrow$  \textit{Adult}}
    \label{fig:alae-y2a}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=0.999\linewidth]{pics/Fig_ADULT_TO_YOUNG_7.png}
    \caption{\textit{Adult} $\rightarrow$ \textit{Young}}
    \label{fig:alae-a2y}
\end{subfigure}
\hfill
% \vspace{3mm}
\newline 
\begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=0.999\linewidth]{pics/Fig_MAN_TO_WOMAN_7.png}
    \caption{\textit{Man} $\rightarrow$  \textit{Woman}}
    \label{fig:alae-m2w}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=0.999\linewidth]{pics/Fig_WOMAN_TO_MAN_7.png}
    \caption{\textit{Woman} $\rightarrow$  \textit{Man}}
    \label{fig:alae-w2m}
\end{subfigure}
\vspace{-1mm}\caption{\centering Unpaired translation with LightSB, OT-FM, UOT-FM and our U-LightOT solvers applied in the latent space of ALAE \cite{pidhorskyi2020adversarial} for FFHQ images \cite{karras2019style} (1024$\times$1024).}
\label{fig:alae}
\end{center}
\vspace*{-5mm}
\end{figure*}

\begin{table}[t!]
    \centering
    % \small
    \footnotesize
    % \begin{sc}
    \begin{tabular}{ c| c c c c c c} 
    \hline
    \textbf{Experiment} & \citep[UOT-FM]{eyring2023unbalancedness} & \citep[UOT-SD]{choi2024generative} & \citep[UOT-GAN]{yang2018scalable} &U-LightOT (\textbf{ours})  \\
    \hline
     \textit{Time (sec)}   &\underline{03:21} &18:11  &16:30  & \textbf{02:38}   \\
     \hline
    \end{tabular}
     % \end{sc}
     \vspace{1mm}
     \caption{\centering 
      Comparison of wall-clock running times of unbalanced OT/EOT solvers in \textit{Young}$\rightarrow$\textit{Adult} translation.  The best results are in \textbf{bold}, second best are \underline{underlined}.}
     \label{table-wall-clock-time}
     \vspace*{-7mm}
\end{table}

\section{Discussion}
\label{sec-discussion}

\vspace{-2mm}
\textbf{Potential impact.} Our light and unbalanced solver has a lot of advantages in comparison with the other existing UEOT solvers. First, it does not require complex max-min optimization. Second, it provides the closed form of the conditional measures $\gamma_{\theta}(y|x)\approx \gamma^*(y|x)$ of the UEOT plan. Moreover, it allows for sampling both from the conditional measure $\gamma_{\theta}(y|x)$ and marginal measure $u_{\omega}(x)\approx \gamma_x^*(x)$. Besides, the decisive superiority of our lightweight and unbalanced solver is its simplicity and convenience of use. Indeed, it has a straightforward and non-minimax optimization objective and avoids heavy neural parametrization. As a result, our lightweight and unbalanced solver converges in minutes on CPU. We expect that these advantages could boost the usage of our solver as a standard and easy baseline for UEOT task with applications in different spheres.

The \underline{limitations and broader impact} of our solver are discussed in Appendix \ref{sec-limitations}.


\textsc{ACKNOWLEDGEMENTS.} The work of Skoltech was supported by the Analytical center under the RF Government (subsidy agreement 000000D730321P5Q0002, Grant No. 70-2021-00145 02.11.2021). We thank Kirill Sokolov, Mikhail Persiianov and Petr Mokrov for providing valuable feedback and suggestions for improving the proofs and clarity of our paper.

\bibliography{bibliography}
\bibliographystyle{abbrvnat}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Proofs}
\label{sec-proofs}

\subsection{Proof of Theorem \ref{thm-kl}}
\begin{proof}[Proof]
For convenience, we split this proof into several steps.

\textit{\underline{Step 1.}} To begin with, we recall that the primal UEOT problem \eqref{unbalanced-eot-primal} attains a minimizer $\gamma^*$. Then by substituting $\gamma^*$ directly in \eqref{unbalanced-eot-primal}, we get
\begin{eqnarray}
     \eqref{unbalanced-eot-primal}=\int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \frac{\|x-y\|^2}{2} \gamma^*(x, y) dx dy -
     \varepsilon H(\gamma^*)\!+\Df{f_1}{\gamma_x^*}{p}   + \Df{f_2}{\gamma_y^*}{q}=\nonumber\\
     \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \frac{\|x-y\|^2}{2} \gamma^*(x, y) dx dy + \varepsilon\int_{\mathbb{R}^{d}}\int_{\mathbb{R}^d}\gamma^*(x,y)\! \log \gamma^*(x,y) dxdy-\varepsilon\|\gamma^*\|_1 + 
     % \label{line-conn-before}
     \nonumber\\
     \int_{\mathbb{R}^{d}} f_1\bigg(\frac{\gamma_x^*(x)}{p(x)}\bigg)p(x)dx+\int_{\mathbb{R}^{d}} f_2\bigg(\frac{\gamma_y^*(y)}{q(y)}\bigg)q(y)dy=-\varepsilon\|\gamma^*\|_1+\nonumber\\
     \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \cancel{\frac{\|x-y\|^2}{2} \gamma^*(x, y)} dx dy + \varepsilon\int_{\mathbb{R}^{d}}\gamma^*(x,y)\! \cdot \underbrace{\varepsilon^{-1}\big(\phi^*(x)\!+\!\psi^*(y)\!-\!\cancel{\frac{\|x-y\|^2}{2}}\big)}_{\log \gamma^*(x,y)=} dxdy\label{line-conn}+\\
       \int_{\mathbb{R}^{d}} f_1\bigg(\frac{\gamma_x^*(x)}{p(x)}\bigg)p(x)dx+\int_{\mathbb{R}^{d}} f_2\bigg(\frac{\gamma_y^*(y)}{q(y)}\bigg)q(y)dy =-\varepsilon\|\gamma^*\|_1+\nonumber\\
     \int_{\mathbb{R}^{d}}\!\gamma^*_x(x) \phi^*(x)dx \!+\!\! \int_{\mathbb{R}^{d}}\gamma^*_y(y) \psi^*(y)dy \!+\!\! \int_{\mathbb{R}^{d}} f_1\bigg(\frac{\gamma_x^*(x)}{p(x)}\bigg)p(x)dx\!+\!\!\int_{\mathbb{R}^{d}} f_2\bigg(\frac{\gamma_y^*(y)}{q(y)}\bigg)q(y)dy\!=\!\nonumber\\
     -\varepsilon\|\gamma^*\|_1 \!+\!\int_{\mathbb{R}^{d}}\!\!\Bigg(\!\!f_1\bigg(\frac{\gamma_x^*(x)}{p(x)}\!\!\bigg)\!+\!\phi^*(x) \frac{\gamma^*_x(x)}{p(x)}\!\!\Bigg)p(x)dx\!+\!\int_{\mathbb{R}^{d}}\!\!\Bigg(\!f_2\bigg(\frac{\gamma_y^*(y)}{q(y)}\bigg)\!+\!\psi^*(y) \frac{\gamma^*_y(y)}{q(y)}\!\!\Bigg)q(y)dy.
     \label{primal-optimal}
\end{eqnarray}
Here in line \eqref{line-conn}, we use equation \eqref{gamma-potentials} which establishes the connection between $\gamma^*$ and the optimal potentials $\phi^*$, $\psi^*$ delivering maximum to the dual UEOT problem \eqref{unbalanced-eot-dual}. Substituting these optimal potentials to the dual problem, we get
\begin{eqnarray}
    \eqref{unbalanced-eot-dual}=
    -\varepsilon \underbrace{\!\!\int_{\mathbb{R}^d}\!\! \int_{\mathbb{R}^d}\! \!\!\exp\{\frac{1}{\varepsilon} \!( \phi^*(x) \!+\! \psi^*(y)\!-\! \frac{\|x-y\|^2}{2}) \} dx dy}_{=\|\gamma^*\|_1} \!- \!\!
    \int_{\mathbb{R}^d} \!\!\overline{f}_1 (-\phi^*(x))p(x) dx \! - \nonumber\\
    \!\!\!\int_{\mathbb{R}^d}\!\! \overline{f}_2 (-\psi^*(y))q(y) dy=%\nonumber\\
    -\varepsilon \|\gamma^*\|_1 \!- \!\!
    \int_{\mathbb{R}^d} \!\!\overline{f}_1 (-\phi^*(x))p(x) dx \! - \!\!\!\int_{\mathbb{R}^d}\!\! \overline{f}_2 (-\psi^*(y))q(y) dy.
    \label{dual-optimal}
\end{eqnarray}
Note that \eqref{primal-optimal} equals to \eqref{dual-optimal} due to the equivalence between primal and dual UEOT problems, see \wasyparagraph\ref{sec-background}. We will use this fact in further derivations.

For every function $f$, its convex conjugate \begin{eqnarray}
    \overline{f}(t)=\sup_{u\in\mathbb{R}}\{ut-f(u)\}\Longrightarrow -\overline{f}(-t)=-\sup_{u\in\mathbb{R}}\{-ut-f(u)\}=\inf_{u\in\mathbb{R}}\{ut+f(u)\}\Longrightarrow\nonumber\\
    -\overline{f}(-t) \leq ut+f(u) \;\;\forall u\in\mathbb{R}.
    \label{conj-prop}
\end{eqnarray} 
Then for arbitrary $x\in\mathbb{R}^d$, taking $f=f_1$, $u=\frac{\gamma_x^*(x)}{p(x)}$, $t=\phi^*(x)$, we get
\begin{eqnarray}
    -\overline{f_1}(-\phi^*(x)) \leq \frac{\gamma_x^*(x)}{p(x)}\phi^*(x)+f_1\Big(\frac{\gamma_x^*(x)}{p(x)}\Big)\!\!\Longrightarrow\!\label{ineq-0}\\ -\!\!\int_{\mathbb{R}^d}\overline{f_1}(\phi^*(x)) p(x) dx \leq \int_{\mathbb{R}^d} \Bigg(\frac{\gamma_x^*(x)}{p(x)}\phi^*(x)+f_1\Big(\frac{\gamma_x^*(x)}{p(x)}\Big) \Bigg) p(x) dx. \label{ineq-1}
\end{eqnarray}
Similarly, for arbitrary $y\in\mathbb{R}^d$
\begin{eqnarray}
    -\!\!\int_{\mathbb{R}^d}\overline{f_2}(\psi^*(y)) q(y) dy \leq \int_{\mathbb{R}^d} \Bigg(\frac{\gamma_y^*(y)}{q(y)}\psi^*(y)+f_2\Big(\frac{\gamma_y^*(y)}{q(y)}\Big) \Bigg) q(y) dy. \label{ineq-2}
\end{eqnarray}
Since \eqref{primal-optimal}$=$\eqref{dual-optimal}, inequalities \eqref{ineq-0}, \eqref{ineq-1}, \eqref{ineq-2} actually turn to equalities.
 

\textit{\underline{Step 2.}} Since we consider convex, lower semi-continuous functions $f$, it holds that ${\overline{\overline{f}}=f}$. Then, substituting $\overline{f}$ in formula \eqref{conj-prop}, we derive
\begin{eqnarray}
    -f(-t)=-\overline{\overline{f}}(-t)=\inf_{u\in\mathbb{R}}\{ut+\overline{f}(u)\}\Longrightarrow
    -f(-t) \leq ut+\overline{f}(u) \;\;\forall u\in\mathbb{R}.
    \label{conj-prop2}
\end{eqnarray}
Thus, for arbitrary $x\in\mathbb{R}^d$, taking $f=f_1$, $t=-\frac{\gamma_x^*(x)}{p(x)}$, we get that 
$-f_1\Big(\frac{\gamma_x^*(x)}{p(x)}\Big) \leq u\frac{\gamma_x^*(x)}{p(x)}+\overline{f_1}(-u) \;\forall u\in\mathbb{R}.$ At the same time, from \textit{step 1}, we know that $-f_1\Big(\frac{\gamma_x^*(x)}{p(x)}\Big)= \frac{\gamma_x^*(x)}{p(x)} \phi^*(x) + \overline{f_1}(-\phi^*(x))$, see \eqref{ineq-0}. Then 
\begin{eqnarray}
     \frac{\gamma_x^*(x)}{p(x)} \phi^*(x) + \overline{f_1}(-\phi^*(x)) \leq u\frac{\gamma_x^*(x)}{p(x)}+\overline{f_1}(-u) \;\forall u\in\mathbb{R} \Longrightarrow \nonumber\\
     \frac{\gamma_x^*(x)}{p(x)} (\phi^*(x) - u) \leq (\overline{f_1}(-u) - \overline{f_1}(-\phi^*(x))\;\forall u\in\mathbb{R}.
     \label{conj-prop3-1}
\end{eqnarray}
Similarly, for arbitrary $y\in\mathbb{R}^d$
\begin{eqnarray}
     \frac{\gamma_y^*(y)}{q(y)} (\psi^*(y) - u) \leq (\overline{f_2}(-u) - \overline{f_2}(-\psi^*(y))\;\forall u\in\mathbb{R}.
     \label{conj-prop3-2}
\end{eqnarray}

\textit{\underline{Step 3.}} Now we are ready to prove the main result. We note that:
\begin{eqnarray}
    \KL{\gamma^{*}}{\gamma_{\theta,\omega}}\!=
    \int_{\mathbb{R}^d\times\mathbb{R}^d} \gamma^*(x,y)\log \gamma^*(x,y) dx dy - 
    \nonumber
    \\
    \int_{\mathbb{R}^d\times\mathbb{R}^d} \!\!\!\gamma^*(x,y) \log \gamma_{\theta, \omega}(x,y) dx dy + \!\|\gamma_{\theta,\omega}\|_1 - \|\gamma^*\|_1.
    \label{kl-derivation-bgt}
\end{eqnarray}
While the ground-truth UEOT plan $\gamma^*(x,y)$ and the optimal dual variables $\phi^*(x)$, $\psi^*(y)$ are connected via equation \eqref{gamma-potentials}, our parametrized plan $\gamma_{\theta,\omega}(x,y)$ can be expressed using $\phi_{\theta,\omega}(x)$, $\psi_{\theta}(y)$ as $\gamma_{\theta,\omega}(x,y) = \exp\{\varepsilon^{-1}(\phi_{\theta,\omega}(x)+\psi_{\theta}(y)-\nicefrac{\|x-y\|^2}{2})\}$, see \eqref{parametrization}.
Then
\begin{eqnarray}
    \!\!\!\eqref{kl-derivation-bgt}=\varepsilon^{-1}\!\int_{\mathbb{R}^d\times\mathbb{R}^d}\!\! \gamma^*(x,y)\big(\phi^*(x)\!+\!\psi^*(y)\!-\!\nicefrac{\|x-y\|^2}{2}\big) dx dy \!-
    \nonumber
    \\
    \varepsilon^{-1}\!\int_{\mathbb{R}^d\times\mathbb{R}^d} \!\!\gamma^*(x,y) \big(\phi_{\theta,\omega}(x)\!+\!\psi_{\theta}(y)\!-\!\nicefrac{\|x-y\|^2}{2}\big)dxdy\!+
    \|\gamma_{\theta,\omega}\|_1 - \|\gamma^*\|_1 = 
    \nonumber
    \\
    \varepsilon^{-1} \int_{\mathbb{R}^d\times\mathbb{R}^d} \gamma^*(x,y) \big(\phi^*(x)+\psi^*(y) - \phi_{\theta,\omega}(x)-\psi_{\theta}(y) \big)dxdy  + \|\gamma_{\theta,\omega}\|_1 - \|\gamma^*\|_1=
    \nonumber\\
    \varepsilon^{-1} \int_{\mathbb{R}^d} \frac{\gamma^*_x(x)}{p(x)} \big(\phi^*(x) - \phi_{\theta,\omega}(x) \big)p(x)dx  +\nonumber\\ \varepsilon^{-1} \int_{\mathbb{R}^d} \frac{\gamma^*_y(y)}{q(y)} \big(\psi^*(y) - \psi_{\theta}(y)\big)q(y)dy + \|\gamma_{\theta,\omega}\|_1- \|\gamma^*\|_1\stackrel{\eqref{conj-prop3-1}-\eqref{conj-prop3-2}}{\leq} \nonumber\\
    \varepsilon^{-1} \!\!\int_{\mathbb{R}^d} \!\!( \overline{f}_1(-\phi_{\theta,\omega}{(x)}) \!-\! \overline{f}_1(-\phi^*{(x)})) p(x)dx  \!+
    \nonumber
    \\
    \! \varepsilon^{-1}\!\! \int_{\mathbb{R}^d}\! ( \overline{f}_2(-\psi_{\theta}{(y)}) \!-\! \overline{f}_2(-\psi^*{(y)}))q(y)dy \!+\! \|\gamma_{\theta,\omega}\|_1\!-\! \|\gamma^*\|_1\!=\! 
    \nonumber
    \\
    \varepsilon^{-1}\! \cdot \Big(\!\int_{\mathbb{R}^d} \!\!\! \overline{f}_1(-\phi_{\theta,\omega}{(x)}) p(x)dx \!\!+\!\!\!  \int_{\mathbb{R}^d} \! \!\overline{f}_2(-\psi_{{\theta}}{(y)}) q(y)dy \!+\! \varepsilon \|\gamma_{\theta,\omega}\|_1\!\!-
    \nonumber
    \\
    \! 
    \underbrace{(\!\int_{\mathbb{R}^d}\!\!\overline{f}_1(\!-\!\phi^*{(x)}) p(x)dx \!  +\!\!\! \int_{\mathbb{R}^d}\!\!\overline{f}_2(\!-\!\psi^*{(y)})q(y)dy \!+\! \varepsilon \|\gamma^*\|_1)\!}_{\mathcal{L}^*\defeq}\!\!\Big) \!\!=
    \nonumber\\
     \varepsilon^{-1} (\mathcal{L}(\theta,\omega)-\mathcal{L}^*).
    \nonumber
    % \label{kl-derivation-bc}
\end{eqnarray}
\end{proof}

\subsection{Proof of Proposition \ref{prop-estimation}}
We begin with proving the auxiliary theoretical results (Propositions \ref{prop-rademacher-bound}, \ref{prop-rademacher-complexity}) which are needed to prove the main proposition of this section.

\begin{proposition}[Rademacher bound on the estimation error]
\label{prop-rademacher-bound}
It holds that
    $$\mathbb{E}\big[\mathcal{L}(\widehat{\theta})-\mathcal{L}(\overline{\theta})\big]\leq 4\mathcal{R}_{N}(\mathcal{F}_1, p)+4\mathcal{R}_{M}(\mathcal{F}_2, q),$$
    where $\mathcal{F}_1=\{\overline{f}_1 (- \phi_{\theta,\omega}) |(\theta,\omega)\in\Theta\times\Omega\}$, $\mathcal{F}_2=\{\overline{f}_2 (- \psi_{\theta}) |\theta\in\Theta\}$ for $\phi_{\theta,\omega}(x)=\varepsilon \log \frac{u_{\omega}(x)}{c_{\theta}(x)}+\frac{\|x\|^2}{2}$, $\psi_{\theta}(y)=\varepsilon \log v_{\theta}(y)+\frac{\|y\|^2}{2}$,
    and $\mathcal{R}_{N}(\mathcal{F}_1,p)$, $\mathcal{R}_{M}(\mathcal{F}_2,q)$ denote the Rademacher complexity \citep[\wasyparagraph 26]{shalev2014understanding} of the functional classes $\mathcal{U},\;\mathcal{V}$ w.r.t. to the sample sizes $N$, $M$ of distributions $p$, $q$.
    \label{estimation-through-rademacher}
\end{proposition}
\begin{proof}[Proof of Proposition \ref{estimation-through-rademacher}]
    The derivation of this fact is absolutely analogous to \citep[Proposition B.1]{korotin2024light}, \citep[Theorem 4]{mokrov2024energy} or \citep[Theorem 3.4]{taghvaei20192}.
\end{proof}

\begin{proposition}[Bound on the Rademacher complexity of the considered classes]
\label{prop-rademacher-complexity}
    Let $0<a\leq A$, let $0<u\leq U$, let $0<w\leq W$ and $V>0$. Consider the class of functions 
    \begin{gather}\mathcal{F}_1=\big\{x\mapsto \overline{f}_1 (- \varepsilon \log u_{\omega}(x)+ \varepsilon \log c_{\theta}(x) -\frac{\|x\|^2}{2})\big\},\nonumber\\ 
    \mathcal{F}_2=\big\{y\mapsto \overline{f}_2 (- \varepsilon \log v_{\theta}(y) -\frac{\|y\|^2}{2})\big\} \text{ where}\nonumber\\
    u_{\omega}(x),\; v_{\theta}(y),\; c_{\theta}(x) \text{ belong to the class} \nonumber\\
    \mathcal{V}=\big\{x\mapsto  \sum_{k=1}^{K}\alpha_{k}\exp\big(x^{T}U_{k}x+v_{k}^{T}x+w_{k})\text{ with }\\ uI\preceq U_{k}=U_{k}^{T}\preceq UI; \|v_{k}\|\leq V; w\leq w_{k}\leq W; a\leq \alpha_{k}\leq A \big\}.\nonumber
    \end{gather}
    Following \citep[Proposition B.2]{korotin2024light}, we call the functions of the class $\mathcal{V}$ as constrained log-sum-exp quadratic functions. We assume that $\overline{f}_1,\;\overline{f}_2$ are Lipshitz functions and measures $p$, $q$ are compactly supported with the supports lying in a zero-centered ball of a radius $R>0$. Then
    $$\mathcal{R}_{N}(\mathcal{F}_1,p)\leq \frac{C_0}{\sqrt{N}},\;\mathcal{R}_{M}(\mathcal{F}_2,q)\leq \frac{C_1}{\sqrt{M}}$$
    where the constants $C_0$, $C_1$ \textbf{do not depend} on sizes $N$, $M$ of the empirical samples from $p,\;q$.
    \label{proposition-rademacher-complexity}
\end{proposition}

\begin{proof}[Proof of Proposition \ref{proposition-rademacher-complexity}]
    Thanks to \citep[Proposition B.2]{korotin2024light}, the Rademacher complexities of constrained log-sum-exp quadratic functions $x\mapsto \log u_{\omega}(x)$, $x\mapsto \log  c_{\theta}(x)$ and $y\mapsto  \log v_{\theta}(y)$ are known to be bounded by $O(\frac{1}{\sqrt{N}})$ or $O(\frac{1}{\sqrt{M}})$ respectively. According to the definition of Rademacher complexity, for single quadratic functions $x\mapsto \frac{x^T x}{2}$ ($y\mapsto \frac{y^T y}{2}$) it is just equal to zero. 
    Then, using the well-known scaling and additivity properties of the Rademacher complexity \citep{shalev2014understanding}, we get that $x\mapsto - \varepsilon \log u_{\omega}(x)+ \varepsilon \log c_{\theta}(x) -\frac{\|x\|^2}{2}$ and $y\mapsto - \varepsilon \log v_{\theta}(y) -\frac{\|y\|^2}{2}$ are bounded by $O(\frac{1}{\sqrt{N}})$ and $O(\frac{1}{\sqrt{M}})$ respectively. The remaining step is to
    recall that $\overline{f}_1(x)$ and $\overline{f}_2(y)$ are Lipschitz. Therefore, according to Talagrand's contraction principle \citep{mohri2018foundations}, the Rademaher complexities of $\mathcal{F}_{1}$ and $\mathcal{F}_{2}$ are also bounded by $O(\frac{1}{\sqrt{N}})$ and $O(\frac{1}{\sqrt{M}})$, respectively.
\end{proof}

\begin{proof}[Proof of Proposition \ref{prop-estimation}]
    The proof directly follows from Propositions \ref{estimation-through-rademacher} and \ref{proposition-rademacher-complexity}.
\end{proof}

\subsection{Proof of Theorem \ref{thm-universal-approximation}}

\label{appendix-dual-form}
To begin with, we provide a quick reminder about the Fenchel-Rockafellar theorem which is needed to derive the dual form of problem \eqref{unbalanced-eot-primal}.

\begin{theorem}[Fenchel-Rockafellar \citep{rockafellar1967duality}]
    Let $(E, E')$ and $(F, F')$ be two couples of topologically paired spaces. Let $A : E \mapsto F$ be a continuous linear operator and $\overline{A} : F' \mapsto E'$ be its adjoint. Let $f$ and $g$ be lower semi-continuous and proper convex functions defined on $E$ and $F$ respectively. If there exists $x\in \text{dom} f$ s.t. $g$ is continuous at $Ax$, then
    $$\sup_{x\in E} -f(-x) -g (Ax) = \min_{\overline{y}\in F'} \overline{f}(\overline{A} \; \overline{y}) + \overline{g}(\overline{y})
    $$
    and the min is attained. Moreover, if there exists a maximizer $x\in E$ then there exists $\overline{y} \in F'$ satisfying $Ax \in \partial \overline{g}(\overline{y})$ and $\overline{A} \overline{y} \in \partial f(-x)$.
\end{theorem}

We note that in the below theorem $\mathcal{C}_2(\mathbb{R}^d)$ \textbf{does not} denote the space of twice differentiable continuous functions. The exact definition of this space and $\mathcal{C}_{2,b}(\mathbb{R}^d)$ is given in \textbf{notations} part.


\begin{theorem}[Dual form of problem \eqref{unbalanced-eot-primal}]\label{aux-theorem-dual}
The primal UEOT problem \eqref{unbalanced-eot-primal} has the dual counterpart \eqref{unbalanced-eot-dual} where the potentials $(\phi,\psi)$ belong to the space $\mathcal{C}_{2,b}(\mathbb{R}^d)\times \mathcal{C}_{2,b}(\mathbb{R}^d)$. The minimum of \eqref{unbalanced-eot-primal} is attained for a unique $\gamma^*\in \mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d)$.
\end{theorem}
\begin{proof}
    We recall that in the primal form, the minimization is performed over functions $\gamma$ belonging to $\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d)$. In this proof, we suppose that this space is endowed with a coarsest topology $\sigma(\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d))$ which makes continuous the linear functionals $\gamma \mapsto \int \zeta d \gamma, \; \forall \zeta\in \mathcal{C}_{2}(\mathbb{R}^d\times\mathbb{R}^d)$. 
    % Here the space of functions $\mathcal{C}_{2}(\mathbb{R}^d)$ is the same as $\mathcal{C}_{2,b}(\mathbb{R}^d)$
    Then the topological space $\big(\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d), \sigma(\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d))\big)$ has a topological dual $\big(\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d), \sigma(\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d))\big)'$  which, actually, is (linear) isomorphic to the space $\mathcal{C}_2(\mathbb{R}^d\times\mathbb{R}^d)$, see \citep[Lemma 9.9]{gozlan2017kantorovich}. This fact opens an opportunity to apply the well-celebrated Fenchel-Rockafellar theorem.  For this purpose, we will consider the following spaces: $E\defeq \mathcal{C}_2(\mathbb{R}^d)\times \mathcal{C}_2(\mathbb{R}^d)$, $F\defeq \mathcal{C}_2(\mathbb{R}^d\times \mathbb{R}^d)$ and their duals $E'\defeq \mathcal{M}_{2,+}(\mathbb{R}^d)\times \mathcal{M}_{2,+}(\mathbb{R}^d)$ and $F'\defeq \mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d)$.
 
    {\underline{\textit{Step 1.}}} Recall that the convex conjugate of any function $g:\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d) \rightarrow \mathbb{R}\cup\{+\infty\}$ is defined for each $\zeta\in (\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d))'\cong \mathcal{C}_2(\mathbb{R}^d\times\mathbb{R}^d)$ as $\overline{g}(\zeta)=\sup_{\gamma\in\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d)} \lbrace\langle \gamma, \zeta \rangle - g(\gamma)\rbrace$. For the convenience of further derivations, we introduce additional functionals corresponding to the summands in the primal UEOT problem \eqref{unbalanced-eot-primal}:
    \begin{eqnarray}
        P(\gamma)\defeq \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \frac{\|x-y\|^2}{2} \gamma(x, y) dx dy - 
     \varepsilon H(\gamma);\;\;\;
     \nonumber\\
     F_1(\gamma_x)\defeq \Df{f_1}{\gamma_x}{p};\;\;\;
     F_2(\gamma_y)\defeq \Df{f_2}{\gamma_y}{q}.
    \end{eqnarray}

    For our purposes, we need to calculate the convex conjugates of these functionals. Fortunately, convex conjugates of $f$-divergences $F_1(\gamma_x)$ 
    % ($\Df{f_1}{\gamma_x}{p}$) 
    and $F_2(\gamma_y)$ %($= \Df{f_2}{\gamma_y}{q}$)
    are well-known, see \citep[Proposition 23]{agrawal2021optimal}, and equal to
    \begin{eqnarray}
        \overline{F_1}(\phi) \defeq \int_{\mathbb{R}^d} \overline{f_1} (\phi(x)) p(x) dx, \;\;\;
        \overline{F_2}(\psi) \defeq \int_{\mathbb{R}^d} \overline{f_2} (\psi(y)) q(y) dy.
        \nonumber
    \end{eqnarray}
    To proceed, we calculate the convex conjugate of $P(\gamma)$:
    \begin{eqnarray}
        % \overline{\eqref{unbalanced-eot-primal}} = 
        % \nonumber\\
        \overline{P}(\zeta)=\overline{\int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \frac{\|x-y\|^2}{2} \gamma(x, y) dx dy - 
     \varepsilon H(\gamma)\!}=
        \nonumber\\
        =\sup_{\gamma\in\mathcal{M}_{2,+}  (\mathbb{R}^d\times\mathbb{R}^d)} \Big\lbrace 
        \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \zeta(x, y) \gamma(x, y) dx dy -
        \big(\int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \frac{\|x-y\|^2}{2} \gamma(x, y) dx dy - \nonumber\\ 
      \varepsilon H(\gamma)\! \big)
      \Big\rbrace  
     = \nonumber\\
     \sup_{\gamma\in\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d)}
     \Big\lbrace 
     \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \big(\zeta(x,y)- \frac{\|x-y\|^2}{2}\big) \gamma(x,y) dx dy +
     \nonumber\\
     \varepsilon \underbrace{\Big(\int_{\mathbb{R}^d}\int_{\mathbb{R}^d} -\gamma(x,y)\log \gamma(x, y) dx dy {\color{black}} +\|\gamma\|_1\!\Big)}_{=H(\gamma)}
     \Big\rbrace
     =\nonumber\\
     \sup_{\gamma\in\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d)} \varepsilon \cdot 
     \Big\lbrace 
     \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \gamma(x,y) \Big(\frac{\zeta(x,y)- \frac{\|x-y\|^2}{2}}{\varepsilon} - \log \gamma(x, y)\Big) dx dy {\color{black}} +\|\gamma\|_1\!\Big\rbrace = \nonumber\\
      \sup_{\gamma\in\mathcal{M}_{2,+} (\mathbb{R}^d\times\mathbb{R}^d)} (-
     \varepsilon) \cdot 
     \Big\lbrace \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \gamma(x,y) \big(\log \gamma(x, y)-\frac{\zeta(x,y)- \frac{\|x-y\|^2}{2}}{\varepsilon} \big) dx dy {\color{black}} - \|\gamma\|_1\!\Big\rbrace = \nonumber\\  \sup_{\gamma\in\mathcal{M}_{2,+} (\mathbb{R}^d\times\mathbb{R}^d)} (-
     \varepsilon) \cdot 
     \Big\lbrace \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \gamma(x,y) \big(\log \gamma(x, y)-\frac{\zeta(x,y)- \frac{\|x-y\|^2}{2}}{\varepsilon} \big) dx dy {\color{black}}- \|\gamma\|_1 + \nonumber\\
     \underbrace{\int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \exp\lbrace\frac{\zeta(x,y)- \frac{\|x-y\|^2}{2}}{\varepsilon}\rbrace dx dy - \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \exp\lbrace\frac{\zeta(x,y)- \frac{\|x-y\|^2}{2}}{\varepsilon}\rbrace dx dy}_{=0}
     \Big\rbrace =
     \label{conj-to-kl-before}
     \\
     \sup_{\gamma\in\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d)} (-\varepsilon) \cdot \Bigg\lbrace\KL{\gamma}{\exp\lbrace\frac{\zeta(x,y)- \frac{\|x-y\|^2}{2}}{\varepsilon}\rbrace} {\color{black}} - \nonumber\\
     \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \exp\lbrace\frac{\zeta(x,y)- \frac{\|x-y\|^2}{2}}{\varepsilon}\rbrace dx dy\Bigg\rbrace.
     \label{conjugate-first-summand}
    \end{eqnarray}
    Here in the transition from \eqref{conj-to-kl-before} to \eqref{conjugate-first-summand}, we keep in mind our prior calculations of  $\text{D}_{\text{KL}}$  in \eqref{kl-derivation-bgt}. Recall that  $\text{D}_{\text{KL}}$  is non-negative and attains zero at the unique point 
    \begin{eqnarray}
        \gamma(x,y)=\exp\{\frac{\zeta(x,y)- \frac{\|x-y\|^2}{2}}{\varepsilon}\}.
        \label{optimal-gamma}
    \end{eqnarray}
    Thus, we get
    \begin{eqnarray}
        \overline{P}(\zeta)= 
        \varepsilon \int_{\mathbb{R}^d}\int_{\mathbb{R}^d} \exp\{\frac{\zeta(x,y)- \frac{\|x-y\|^2}{2}}{\varepsilon}\}dx dy {\color{black}}.
    \end{eqnarray}


    {\underline{\textit{Step 2.}}} Now we are ready to apply the Fenchel-Rockafellar theorem in our case. To begin with, we show that this theorem is applicable to problem \eqref{unbalanced-eot-primal}, i.e., that the functions under consideration satisfy the necessary conditions.
    Indeed, it is known that the convex conjugate of any functional (e.g., $\overline{F_1}(\cdot)$, $\overline{F_2}(\cdot)$, $\overline{P}(\cdot)$) is \textbf{lower semi-continuous} and \textbf{convex}. Besides, the listed functionals are \textbf{proper convex}\footnote{\textit{Proper convex} function is a real-valued convex function which has a non-empty domain, never attains the value $(-\infty)$ and is not identically equal to $(+\infty)$. This property ensures that the minimization problem for this function has non-trivial solutions.}. Indeed, the properness of $\overline{F_1}(\cdot)$ and $\overline{F_2}(\cdot)$ follows from the fact that $f$-divergences are known to be lower-semicontinuous and proper themselves, while properness of $\overline{P}(\cdot)$ is evident from \eqref{conjugate-first-summand}. 
    
    Now we consider the linear operator $A: \mathcal{C}_2(\mathbb{R}^d)\times \mathcal{C}_2(\mathbb{R}^d ) \mapsto \mathcal{C}_2(\mathbb{R}^d\times \mathbb{R}^d)$ which is defined as $A(\phi,\psi): (x,y) \mapsto \phi(x) + \psi(y)$. It is continuous, and its adjoint is defined on $\mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d)$ as $\overline{A}(\gamma)=(\gamma_x, \gamma_y)$. Indeed, $\langle \overline{A}(\gamma), (u,v)\rangle=\langle \gamma, A(u,v)\rangle=\int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \gamma(x, y) (u(x)+v(y)) dx dy = \int_{\mathbb{R}^d} \gamma_x(x) u(x) dx + \int_{\mathbb{R}^d} \gamma_y(y) v(y) dy$.  
    Thus, the strong duality and the existence of minimizer for \eqref{unbalanced-eot-primal} follows from the Fenchel-Rockafellar theorem which states that problems
    \begin{eqnarray}
        \sup_{(\phi,\psi)\in \mathcal{C}_2(\mathbb{R}^d)\times \mathcal{C}_2(\mathbb{R}^d)}
        \{ -\overline{P}(A(\phi,\psi))
        - \overline{F_1}(-\phi)
        - \overline{F_2}(-\psi)
        \}
        \label{duality-fenchel}
    \end{eqnarray}
    and 
    \begin{eqnarray}
        \min_{\gamma\in \mathcal{M}_{2,+}(\mathbb{R}^d\times\mathbb{R}^d)}
        \{P(\gamma) + F_1(\gamma_x)+ F_2(\gamma_y)
        \}
    \end{eqnarray}
    are equal. The uniqueness of the minimizer for \eqref{unbalanced-eot-primal} comes from the strict convexity of $P(\cdot)$ (which holds thanks to the entropy term). Note that the conjugate of the sum of $F_1$ and $F_2$ is equal to the sum of their conjugates since they are defined for separate non-intersecting groups of parameters.
    
    Next we prove that the supremum can be restricted to $\mathcal{C}_{2, b}(\mathbb{R}^d\times\mathbb{R}^d)$. Here we use ``$\wedge$'' to denote the operation of taking minimum between the function $f:\mathbb{R}^d\rightarrow\mathbb{R}$ and real value $k$: $(f\wedge k)(x)\defeq \min(f(x),k)$.

    
    Then analogously to \citep[Theorem 9.6]{gozlan2017kantorovich}, we get:
    \begin{eqnarray}
        \sup_{(\phi,\psi)\in \mathcal{C}_2(\mathbb{R}^{d})\times \mathcal{C}_2(\mathbb{R}^{d})} \lbrace
      -\overline{P}(A(\phi,\psi)) -\overline{F_1}(-\phi) - \overline{F_2}(-\psi) \rbrace =
      \nonumber\\
      \sup_{(\phi,\psi)\in \mathcal{C}_2(\mathbb{R}^{d})\times \mathcal{C}_2(\mathbb{R}^{d})} \lim_{k_1, k_2\rightarrow \infty} 
      \lbrace -\overline{P}(A(\phi \wedge k_1,\psi \wedge k_2)) -\overline{F_1}(-(\phi\wedge k_1)) - \overline{F_2}(-(\psi \wedge k_2)) \rbrace
      \leq 
      \nonumber\\
      \sup_{(\phi,\psi)\in \mathcal{C}_{2,b}(\mathbb{R}^{d})\times \mathcal{C}_{2,b}(\mathbb{R}^{d})} 
      \lbrace
      -\overline{P}(A(\phi,\psi)) -\overline{F_1}(-\phi) - \overline{F_2}(-\psi) \rbrace.
    \end{eqnarray}

    Since the another inequality is obvious, the two quantities are equal which completes the proof.
    
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm-universal-approximation}] Our aim is to prove that for all $\delta >0$ there exist unnormalized Gaussian mixtures $u_{\omega}$ and $v_{\theta}$ s.t. $\mathcal{L}(\theta,\omega)-\mathcal{L}^*<\delta\varepsilon$. We define
\begin{eqnarray}\mathcal{J}(\phi,\psi)\defeq 
\nonumber\\
\varepsilon \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \!\exp\{\frac{1}{\varepsilon} \!( \phi(x) + \psi(y)\!-\! \frac{\|x-y\|^2}{2}) \} dx dy \!+ \!
    \int_{\mathbb{R}^d} \overline{f}_1 (-\phi(x))p(x) dx \! + \!\int_{\mathbb{R}^d} \overline{f}_2 (-\psi(y))q(y) dy.
    \nonumber
\end{eqnarray}
Then from \eqref{unbalanced-eot-dual} and Theorem \eqref{aux-theorem-dual}, it follows that $\mathcal{L}^*=\inf_{(\phi,\psi)\in \mathcal{C}_{2,b}(\mathbb{R}^d)\times \mathcal{C}_{2,b}(\mathbb{R}^d)} \mathcal{J}(\phi, \psi)$. Finally, using the definition of the infimum, we get that for all $\delta'>0$ there exist some functions $(\widehat{\phi}, \; \widehat{\psi}) \in \mathcal{C}_{2,b}(\mathbb{R}^d)\times \mathcal{C}_{2,b}(\mathbb{R}^d)$ such that $\mathcal{J}(\widehat{\phi},\widehat{\psi}) \!\!<\!\! \mathcal{L}^* \!\!+\!\! \delta'$. For further derivations, we set $\delta'\defeq\frac{\delta\varepsilon}{2}$ and pick the corresponding $(\widehat{\phi}, \; \widehat{\psi})$.
% \milena{here we need to define $\delta$}

\underline{\textit{Step 1.}} {\color{black}We start with the derivation of some inequalities useful for future steps.} Since ${(\phi,\psi)\in \mathcal{C}_{2,b}(\mathbb{R}^d)\!\times\! \mathcal{C}_{2,b}(\mathbb{R}^d)}$, they have upper bounds $\widehat{a}$ and $\widehat{b}$ such that for all $x,y\in \mathbb{R}^d$: $\widehat{\phi}(x)\leq \widehat{a}$ and $\widehat{\psi}(y)\leq \widehat{b}$ respectively. We recall that by the assumption of the theorem, measures $p$ and $q$ are compactly supported. Thus, there exist balls centered at $x=0$ and $y=0$ and having some radius $R>0$ which contain the supports of $p$ and $q$ respectively. Then we define
\begin{eqnarray*}
    \widetilde{\phi}(x) \defeq \widehat{\phi}(x) - \max\{0, {\color{black}\max\{\|x\|^2-R^2, \|x\|^4-R^4\}\}}\leq \widehat{\phi}(x)\leq \widehat{a};\nonumber\\
    \widetilde{\psi}(y) \defeq \widehat{\psi}(y) - \max\{0, \|y\|^2-R^2{\color{black}} \}\leq \widehat{\psi}(y)\leq \widehat{b}.\nonumber
\end{eqnarray*}
We get that
\begin{eqnarray}
    \widetilde{\phi}(x) \leq \widehat{\phi}(x), \widetilde{\psi}(y) \leq \widehat{\psi}(y)
 \Longrightarrow \nonumber\\
 \varepsilon \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \!\exp\{\frac{1}{\varepsilon} \!( \widetilde{\phi}(x) + \widetilde{\psi}(y)\!-\! \frac{\|x-y\|^2}{2}) \} dx dy \leq
 \nonumber
 \\\varepsilon \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \!\exp\{\frac{1}{\varepsilon} \!( \widehat{\phi}(x) + \widehat{\psi}(y)\!-\! \frac{\|x-y\|^2}{2}) \} dx dy
 \label{derivation-1st}
\end{eqnarray}
Importantly, for all $x$ and $y$ within the supports of $p$ and $q$ it holds that $\widetilde{\phi}(x)=\widehat{\phi}(x)$ and $\widetilde{\psi}(y)=\widehat{\psi}(y)$, respectively. Then
\begin{eqnarray}
    \int_{\mathbb{R}^d} \overline{f}_1 (-\widehat{\phi}(x))p(x) dx = \int_{\mathbb{R}^d} \overline{f}_1 (-\widetilde{\phi}(x))p(x) dx, \nonumber \\
    \!\int_{\mathbb{R}^d} \overline{f}_2 (-\widehat{\psi}(y))q(y) dy = \int_{\mathbb{R}^d} \overline{f}_2 (-\widetilde{\psi}(y))q(y) dy.
    \label{derivation-2nd}
\end{eqnarray}
Combining \eqref{derivation-1st} and \eqref{derivation-2nd}, we get that $\mathcal{J}(\widetilde{\phi}, \widetilde{\psi}) \leq \mathcal{J}(\widehat{\phi}, \widehat{\psi})<\mathcal{L}^*+\delta$.

Before moving on, we note that functions $\exp\{\nicefrac{\widetilde{\phi}(x)}{\varepsilon}\}$ and $\exp\{\nicefrac{\widetilde{\psi}(y)}{\varepsilon}\}$ {\color{black}are continuous and non-negative}. Therefore, since measures $p$ and $q$ are compactly supported, there exist some constants $e_{\min}, \; h_{\min}>0$ such that $\exp\{\nicefrac{\widetilde{\phi}(x)}{\varepsilon}\}>e_{\min}$ and $\exp\{\nicefrac{\widetilde{\psi}(y)}{\varepsilon}\}>h_{\min}$ for all $x$ and $y$ from the supports of measures $p$ and $q$ respectively. We keep these constants for future steps.

\underline{\textit{Step 2.}} This step of our proof is similar to \citep[Theorem 3.4]{korotin2024light}. We get that
\begin{eqnarray}
    \exp\big(\nicefrac{\widetilde{\phi}(x)}{\varepsilon}\big)\leq \exp\bigg(\frac{\widehat{a}-\max \{0, \|x\|^{2}-R^2\}}{\varepsilon}\bigg)\leq \exp\big(\frac{\widehat{a}+R^{2}}{\varepsilon}\big)\cdot\exp(-\nicefrac{\|x\|^{2}}{\varepsilon}),
    \label{bound-phi}
    \\
    \exp\big(\nicefrac{\widetilde{\psi}(y)}{\varepsilon}\big)\leq \exp\bigg(\frac{\widehat{b}-\max \{0, \|y\|^{2}-R^2\}}{\varepsilon}\bigg)\leq \exp\big(\frac{\widehat{b}+R^{2}}{\varepsilon}\big)\cdot\exp(-\nicefrac{\|y\|^{2}}{\varepsilon}).
    \nonumber
\end{eqnarray}
From this we can deduce that $y\mapsto \exp(\nicefrac{\widetilde{\psi}(y)}{\varepsilon})$ is a normalizable density since it is bounded by the unnormalized Gaussian density. Moreover, we see that it vanishes at the infinity. Thus, using the result \citep[Theorem 5a]{nguyen2020approximation}, we get that for all $\delta''>0$ there exists an unnormalized Gaussian mixture $v_{\widetilde{\theta}}=v_{\widetilde{\theta}}({\color{black}y})$ such that

\begin{eqnarray}\|v_{\widetilde{\theta}}-\exp(\nicefrac{\widetilde{\psi}}{\varepsilon})\|_{\infty}=\sup_{y\in\mathbb{R}^{D}}|v_{\widetilde{\theta}}(y)-\exp(\nicefrac{\widetilde \psi(y)}{\varepsilon})|<\delta''.
\label{diff-to-exp}
\end{eqnarray}

Following the mentioned theorem, we can set all the covariances in $v_{\widetilde{\theta}}$ to be scalar, i.e., define $v_{\widetilde{\theta}}({\color{black}y})=\sum_{k=1}^{K}{\color{black}\widetilde{\alpha}_{k}\mathcal{N}({\color{black}y}|\widetilde{r}_{k},\varepsilon \widetilde{\lambda}_{k}I_{\color{black}d}){\color{black}}}$ for some $K$ and {\color{black}$\widetilde{\alpha}_k\in \mathbb{R}_{+}$, $\widetilde{r}_k\in\mathbb{R}^{{\color{black}d}}$, $\widetilde{\lambda}_{k}\in \mathbb{R}_{+}$ ($k\in\{1,\dots,K\}$)}. For our future needs, we set $$\delta''=\frac{\delta\varepsilon}{2}\cdot \Bigg[ L_1 \cdot \frac{\varepsilon}{e_{\min}} +  L_2 \cdot \frac{\varepsilon}{h_{\min}} + \varepsilon (2\pi\varepsilon)^{\frac{d}{2}}\Big( 1 + (\pi\varepsilon)^{\frac{d}{2}} \exp\big\{\frac{\widehat{a}+R^{2}}{\varepsilon}\big\}\Big)\Bigg]^{-1}.$$

For simplicity, we consider the other mixture $v_{\theta}(y)\defeq v_{\widetilde{\theta}}(y)\exp(-\frac{\|y\|^{2}}{2\varepsilon})$ which is again unnormalized and has scalar covariances, see the proof of \citep[Theorem 3.4]{korotin2024light} for explanation. {\color{black}We denote the weights, means, and covariances of this mixture by $\alpha_k\in \mathbb{R}_+$, $r_k\in\mathbb{R}^{\color{black}d}$ and $\lambda_k\in\mathbb{R}_+$, respectively.}

We derive that 

\begin{eqnarray}
    \varepsilon \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \!\exp\big\{\frac{1}{\varepsilon} \!( \widetilde{\phi}(x) + \widetilde{\psi}(y)\!-\! \frac{\|x-y\|^2}{2}) \big\} dx dy = 
    \nonumber
    \\
    \varepsilon \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \!\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\} \exp\{-\frac{\|x-y\|^2}{2\varepsilon}\}  \exp\{\frac{\widetilde{\psi}(y)}{\varepsilon}\} dx dy > \nonumber\\
    \varepsilon \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \!\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\} \exp\{-\frac{\|x-y\|^2}{2\varepsilon}\}  (v_{\widetilde{\theta}}(y) - \delta'') dx dy = 
    \nonumber\\
    \varepsilon \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \!\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\} \exp\{-\frac{\|x-y\|^2}{2\varepsilon}\}  v_{\widetilde{\theta}}(y) dx dy - 
    \nonumber
    \\
    \delta'' \varepsilon \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \!\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\} \exp\{-\frac{\|x-y\|^2}{2\varepsilon}\} dx dy = 
    \nonumber\\
    \!\!\varepsilon \int_{\mathbb{R}^d} \!\!\int_{\mathbb{R}^d} \!\!\!\!\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\} \!\exp\{-\frac{\|x\|^2}{2\varepsilon}\} \!\exp\{\frac{\langle x,y\rangle}{\varepsilon}\} \!\underbrace{\exp\{-\frac{\|y\|^2}{2\varepsilon}\} \!v_{\widetilde{\theta}}(y)}_{=v_{\theta}(y)} dx dy \!-
    \nonumber
    \\
    \delta'' \!\varepsilon \!\int_{\mathbb{R}^d} \!\!\int_{\mathbb{R}^d} \!\!\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\} \!\exp\{-\frac{\|x-y\|^2}{2\varepsilon}\} dx dy \!= 
    \nonumber\\
    \varepsilon \int_{\mathbb{R}^d} \!\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\} \exp\{-\frac{\|x^2\|}{2\varepsilon}\}\Big(\underbrace{\int_{\mathbb{R}^d} \exp\{\frac{\langle x,y\rangle}{\varepsilon}\}  v_{\theta}(y) dy}_{=c_{\theta}(x)}\Big) dx
    -
    \nonumber
    \\
    \delta'' \varepsilon  \int_{\mathbb{R}^d} \!\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\} \Big(\underbrace{\int_{\mathbb{R}^d} \exp\{-\frac{\|x-y\|^2}{2\varepsilon}\} dy}_{=(2\pi \varepsilon)^{\nicefrac{d}{2}}}\Big) dx =
    \nonumber\\
    \varepsilon \int_{\mathbb{R}^d} \!\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\} \exp\{-\frac{\|x^2\|}{2\varepsilon}\} c_{\theta}(x) dx  - \delta'' \varepsilon  (2\pi \varepsilon)^{\nicefrac{d}{2}} \int_{\mathbb{R}^d} \!\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\} dx \stackrel{\eqref{bound-phi}}{>}
    \nonumber\\
    \varepsilon \int_{\mathbb{R}^d} \!\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\} \exp\{-\frac{\|x^2\|}{2\varepsilon}\} c_{\theta}(x) dx - 
    \nonumber
    \\
    \delta'' \varepsilon  (2\pi \varepsilon)^{\nicefrac{d}{2}} \exp\big\{\frac{\widehat{a}+R^{2}}{\varepsilon}\big\} \int_{\mathbb{R}^d} \underbrace{\exp\{-\nicefrac{\|x\|^{2}}{\varepsilon}\} dx}_{=(\pi\varepsilon)^{\nicefrac{d}{2}}}=
    \nonumber\\
    \varepsilon \int_{\mathbb{R}^d} \!\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\} \exp\{-\frac{\|x^2\|}{2\varepsilon}\} c_{\theta}(x) dx - \delta''  2^{\nicefrac{d}{2}} \pi^d  \varepsilon^{(d+1)} \exp\big\{\frac{\widehat{a}+R^{2}}{\varepsilon}\big\}.
    \label{first-integral-buw}
\end{eqnarray}

\underline{\textit{Step 3.}} At this point, we will show that for every $\delta''>0$, there exists an unnormalized Gaussian mixture $u_{\widetilde{\omega}}$ which is $\delta''$-close to $\exp\{\nicefrac{\widetilde{\phi}(x)}{\varepsilon}\}c_{\theta}(x)$. Using the closed-form expression for $c_{\theta(x)}$ from \citep[Proposition 3.2]{korotin2024light}, we get that
%Let us rewrite the formula for $c_{\theta}(x)$:

\begin{eqnarray*}
    c_{\theta}(x)=
     \sum_{k=1}^K \alpha_k  \exp\{-\frac{\|r_k\|^2}{2\varepsilon\lambda_k}\} \exp\{\frac{\|r_k + x\lambda_k\|^2}{2\varepsilon\lambda_k}\}.
\end{eqnarray*}

Then 
\begin{eqnarray*}
    \exp\big(\nicefrac{\widetilde{\phi}(x)}{\varepsilon}\big)c_{\theta}(x)\leq 
    \nonumber
    \\
    \exp\bigg(\frac{\widehat{a}-\max \{0, \|x\|^{4}-R^4\}}{\varepsilon}\bigg)c_{\theta}(x) \leq \exp\big(\frac{\widehat{a}+R^{4}}{\varepsilon}\big)\cdot\exp(-\nicefrac{\|x\|^{4}}{\varepsilon}) c_{\theta}(x)=
    \\
     \sum_{k=1}^K \alpha_k  \exp\big(\frac{\widehat{a}+R^{4}}{\varepsilon}\big)  \exp\{-\frac{\|r_k\|^2}{2\varepsilon\lambda_k}\} \cdot\exp(-\frac{\|x\|^{4}}{\varepsilon})  \exp\{\frac{\|r_k + x\lambda_k\|^2}{2\varepsilon\lambda_k}\}=
    \\
    \sum_{k=1}^K \alpha_k  \exp\big(\frac{\widehat{a}+R^{4}}{\varepsilon}\big)  \exp\{-\frac{\|r_k\|^2}{2\varepsilon\lambda_k}\} \cdot \exp\{\frac{\|r_k + x\lambda_k\|^2 - 2\lambda_k \|x\|^4}{2\varepsilon\lambda_k}\}
\end{eqnarray*}

From this, we see that $\exp\big(\nicefrac{\widetilde{\phi}(x)}{\varepsilon}\big){\color{black}c_{\theta}(x)}$ tends to zero while $x$ approaches infinity. It means that $x\mapsto \exp\big(\nicefrac{\widetilde{\phi}(x)}{\varepsilon}\big)c_{\theta}(x)$ corresponds to the normalizable density. Using \citep[Theorem 5a]{nguyen2020approximation}, we get that for all $\delta''>0$ there exists an unnormalized Gaussian mixture $u_{\widetilde{\omega}}$ such that
\begin{equation}
\|u_{\widetilde{\omega}}-\exp(\nicefrac{\widetilde{\phi}}{\varepsilon})c_{\theta}\|_{\infty}=\sup_{x\in\mathbb{R}^{D}}|u_{\widetilde{\omega}}(x)-\exp(\nicefrac{\widetilde{\phi}(x)}{\varepsilon})c_{\theta}(x))|<\delta''.
\label{diff-to-exp-2}
\end{equation}

Analogously with $v_{\widetilde{\theta}}$, we can set all the covariances in $u_{\widetilde{\omega}}$ to be scalar, i.e., define $u_{\widetilde{\omega}}={\color{black}\sum_{l=1}^L \widetilde{\beta}_l \mathcal{N}(x|\widetilde{\mu}_l, \varepsilon \widetilde{\zeta}_l I_{\color{black}d})}$ for some $L$, {\color{black}$\widetilde{\mu}_l\in \mathbb{R}^d$, $\widetilde{\zeta}_l\in\mathbb{R}_+\;(l\in\{1,...,L\})$}. Moreover, we consider $u_{\omega}(x) =  u_{\widetilde{\omega}}(x) \exp\{-\frac{\|x\|^2}{2\varepsilon}\}$ which is again an unnormalized density with scalar covariances. {\color{black}We denote the weights, means, covariances of this mixture by $\beta_l\in \mathbb{R}_+$, $\mu_l\in\mathbb{R}^{\color{black}d}$, $\zeta_l\in\mathbb{R}_+$ respectively.}

Next we recall the equation \eqref{first-integral-buw} and get that
\begin{eqnarray}
    \eqref{first-integral-buw} > \varepsilon \int_{\mathbb{R}^d} \exp\{-\frac{\|x\|^2}{2\varepsilon}\} (u_{\widetilde{\omega}}(x) - \delta'') dx - \delta''  2^{\nicefrac{d}{2}} \pi^d  \varepsilon^{(d+1)} \exp\big\{\frac{\widehat{a}+R^{2}}{\varepsilon}\big\}=
    \nonumber\\
    \varepsilon \int_{\mathbb{R}^d} \underbrace{\exp\{-\frac{\|x\|^2}{2\varepsilon}\} u_{\widetilde{\omega}}(x)}_{=u_{\omega}(x)} dx - \varepsilon \delta'' \underbrace{\int_{\mathbb{R}^d} \exp\{-\frac{\|x\|^2}{2\varepsilon}\} dx}_{=(2\pi\varepsilon)^{\frac{d}{2}}} - \delta''  2^{\nicefrac{d}{2}} \pi^d  \varepsilon^{(d+1)} \exp\big\{\frac{\widehat{a}+R^{2}}{\varepsilon}\big\}=
    \nonumber\\
    \varepsilon \|u_{\omega}\|_1 -\varepsilon \delta'' (2\pi\varepsilon)^{\frac{d}{2}}\Big( 1 + (\pi\varepsilon)^{\frac{d}{2}} \exp\big\{\frac{\widehat{a}+R^{2}}{\varepsilon}\big\}\Big).
    \label{estimation-norm}
\end{eqnarray}

\underline{\textit{Step 4.}} Now we turn to other expressions. Using the property that a function $t\mapsto \log t$ is $\frac{1}{t_{\min}}$-Lipshitz on interval $[t_{\min}, +\infty)$ we get
\begin{eqnarray}
     \log (\exp\{\frac{\widetilde{\psi}(y)}{\varepsilon}\}) - \log (\exp\{\frac{\widetilde{\psi}(y)}{\varepsilon}\} - \delta'') \leq \frac{\delta''}{h_{\min}} \Longrightarrow \nonumber\\
     \log (\exp\{\frac{\widetilde{\psi}(y)}{\varepsilon}\} - \delta'') \geq \frac{\widetilde{\psi}(y)}{\varepsilon} - \frac{\delta''}{h_{\min}}.
     \label{log-lipshitz-v}
\end{eqnarray}

Similarly, we get that
\begin{eqnarray}
     \log (\exp\{\frac{\widetilde{\phi}(y)}{\varepsilon}\}c_{\theta}(x) - \delta'') \geq \frac{\widetilde{\phi}(y)}{\varepsilon} + \log c_{\theta}(x) - \frac{\delta''}{e_{\min}}.
     \label{log-lipshitz-u}
\end{eqnarray}

We use this inequality, monotonicity of logarithm function, and \eqref{diff-to-exp}, to derive 
\begin{eqnarray}
    v_{\widetilde{\theta}}(y) \stackrel{\eqref{diff-to-exp}}{>} \exp\{\frac{\widetilde{\psi}(y)}{\varepsilon}\} - \delta''\Longrightarrow \log v_{\widetilde{\theta}}(y) > \log (\exp\{\frac{\widetilde{\psi}(y)}{\varepsilon}\} - \delta'') \stackrel{\eqref{log-lipshitz-v}}{\geq} \frac{\widetilde{\psi}(y)}{\varepsilon} - \frac{\delta''}{h_{\min}} \Longrightarrow 
    \nonumber\\
    - \widetilde{\psi}(y) > -\varepsilon \log v_{\widetilde{\theta}}(y) - \frac{\varepsilon 
    \delta''}{h_{\min}};
    \\
    u_{\widetilde{\omega}}(x) \stackrel{\eqref{diff-to-exp-2}}{>} \exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\}c_{\theta}(x) - \delta'' \Longrightarrow \log u_{\widetilde{\omega}}(x) > \log (\exp\{\frac{\widetilde{\phi}(x)}{\varepsilon}\}c_{\theta}(x) - \delta'') \stackrel{\eqref{log-lipshitz-u}}{\geq}
    \nonumber\\
    \frac{\widetilde{\phi}(y)}{\varepsilon} + \log c_{\theta}(x) - \frac{\delta''}{e_{\min}} 
    \Longrightarrow 
    - \widetilde{\phi}(y) > - \varepsilon \log \frac{u_{\widetilde{\omega}}(x)}{c_{\theta}(x)} - \frac{\varepsilon\delta''}{e_{\min}}.
\end{eqnarray}


Recall that  $\overline{f_1}$ and $\overline{f_2}$ are non-decreasing functions. Moreover, they are Lipshitz with the constants $L_1$, $L_2$ respectively. Thus, we get

\begin{eqnarray}
    \overline{f_2}(- \widetilde{\psi}(y)) \geq \overline{f_2}(-\varepsilon \log v_{\widetilde{\theta}}(y) - \frac{\varepsilon \delta''}{h_{\min}}) = \overline{f_2}(-\varepsilon (\log v_{\theta}(y) + \frac{\|y\|^2}{2\varepsilon}) - \frac{\varepsilon \delta''}{h_{\min}}) =
    \nonumber\\
    \overline{f_2}(-\varepsilon \log v_{\theta}(y) - \frac{\|y\|^2}{2} - \frac{\varepsilon \delta''}{h_{\min}}) \geq \overline{f_2}(-\varepsilon \log v_{\theta}(y) - \frac{\|y\|^2}{2}) - L_2 \cdot \frac{\varepsilon \delta''}{h_{\min}};
    \\
    \overline{f_1}(- \widetilde{\phi}(y)) \geq \overline{f_1}(- \varepsilon \log \frac{u_{\widetilde{\omega}}(x)}{c_{\theta}(x)} - \frac{\varepsilon\delta''}{e_{\min}}) = \overline{f_1}(- \varepsilon \log u_{\widetilde{\omega}}(x) + \varepsilon\log c_{\theta}(x) - \frac{\varepsilon\delta''}{e_{\min}}) = 
    \nonumber\\
    \overline{f_1}(- \varepsilon \log u_{\omega}(x) - \frac{\|x\|^2}{2} + \varepsilon\log c_{\theta}(x) - \frac{\varepsilon\delta''}{e_{\min}}) = \overline{f_1}(- \varepsilon \log \frac{u_{\omega}(x)}{c_{\theta}(x)} - \frac{\|x\|^2}{2} - \frac{\varepsilon\delta''}{e_{\min}}) \geq
    \nonumber\\
    \overline{f_1}(- \varepsilon \log \frac{u_{\omega}(x)}{c_{\theta}(x)} - \frac{\|x\|^2}{2}) - L_1 \cdot \frac{\varepsilon\delta''}{e_{\min}}.
\end{eqnarray}
Integrating these inequalities over all $x$ and $y$ in supports of $p$ and $q$ respectively, we get

\begin{eqnarray}
    \int_{\mathbb{R}^d} \overline{f}_2 (-\widetilde{\psi}(y))q(y) dy \geq \int_{\mathbb{R}^d}  \overline{f_2}(-\varepsilon \log v_{\theta}(y) - \frac{\|y\|^2}{2})q(y)dy - L_2 \cdot \frac{\varepsilon \delta''}{h_{\min}};
    \\
    \int_{\mathbb{R}^d} \overline{f}_1 (-\widetilde{\phi}(x))p(x) dx \geq \int_{\mathbb{R}^d}  \overline{f_1}(- \varepsilon \log \frac{u_{\omega}(x)}{c_{\theta}(x)} - \frac{\|x\|^2}{2})p(x)dx - L_1 \cdot \frac{\varepsilon \delta''}{e_{\min}}.
    \label{estimation-divergences}
\end{eqnarray}

Finally, combining \eqref{estimation-norm} and \eqref{estimation-divergences}, we get
\begin{eqnarray*}
    \mathcal{L}(\theta,\omega)= \mathcal{J}\Big( \underbrace{\varepsilon \log \frac{u_\omega(x)}{c_\theta(x)} + \frac{\Vert x \Vert^2}{2}}_{\phi_{\theta, \omega} \defeq}, \underbrace{\varepsilon \log v_\theta(y) + \frac{\Vert y \Vert^2}{2}}_{\psi_\theta \defeq}\Big) = 
    \nonumber
    \\
    \varepsilon \|u_{\omega}\|_1 + \int_{\mathbb{R}^d} \overline{f_1}(- \varepsilon \log \frac{u_{\omega}(x)}{c_{\theta}(x)} - \frac{\|x\|^2}{2})p(x)dx + \int_{\mathbb{R}^d}\overline{f_2}(-\varepsilon \log v_{\theta}(y) - \frac{\|y\|^2}{2})q(y)dy < 
    \nonumber\\
    \varepsilon \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \!\exp\{\frac{1}{\varepsilon} \!( \widetilde{\phi}(x) + \widetilde{\psi}(y)\!-\! \frac{\|x-y\|^2}{2}) \} dx dy \!+ 
    \nonumber
    \\
    \!
    \int_{\mathbb{R}^d} \overline{f}_1 (-\widetilde{\phi}(x))p(x) dx \! + \!\int_{\mathbb{R}^d} \overline{f}_2 (-\widetilde{\psi}(y))q(y) dy + 
    \nonumber\\
    L_1 \cdot \frac{\varepsilon \delta''}{e_{\min}} + L_2 \cdot \frac{\varepsilon \delta''}{h_{\min}} + \varepsilon \delta'' (2\pi\varepsilon)^{\frac{d}{2}}\Big( 1 + (\pi\varepsilon)^{\frac{d}{2}} \exp\big\{\frac{\widehat{a}+R^{2}}{\varepsilon}\big\}\Big) 
    <
    \\
    \mathcal{L}^* + \delta' + \delta'' \Bigg[ L_1 \cdot \frac{\varepsilon}{e_{\min}} +  L_2 \cdot \frac{\varepsilon}{h_{\min}} + \varepsilon (2\pi\varepsilon)^{\frac{d}{2}}\Big( 1 + (\pi\varepsilon)^{\frac{d}{2}} \exp\big\{\frac{\widehat{a}+R^{2}}{\varepsilon}\big\}\Big)\Bigg]
    \leq
    \nonumber
    \\
    \mathcal{L}^* + \frac{\delta\varepsilon}{2} + \frac{\delta\varepsilon}{2}
    \Longrightarrow
    \nonumber\\
    \KL{\gamma_{\theta,\omega}}{\gamma^*} \leq \varepsilon^{-1}(\mathcal{L}(\theta,\omega) - \mathcal{L}^*) < \delta
\end{eqnarray*}
which completes the proof.
\end{proof}

\textit{Remark.} In fact, the assumption of the Lishitzness of $\overline{f_1}(x)$, $\overline{f_2}(x)$ can be omitted. Indeed, under the "\textit{everything is compact}" assumptions of Proposition \ref{prop-estimation} and Theorem \ref{thm-universal-approximation}, inputs to $\overline{f_1}(\cdot)$, $\overline{f_2}(\cdot)$ also always belong to certain compact sets. The convex functions are known to be Lipshitz on compact subsets of $\mathbb{R}$, see \citep[Chapter 3, \wasyparagraph 18]{hardy1952inequalities}, and we actually do not need the Lipschitzness on the entire $\mathbb{R}$.

\section{Experiments Details}
\label{sec:exp-details}
\subsection{General Details}
To minimize our objective \eqref{empirical-objective}, we parametrize $\alpha_k,r_k,S_k$ of $v_{\theta}$ and $\beta_l,\mu_l,\Sigma_l$ of $u_{\omega}$ in \eqref{gauss-parametrization}. Here we follow the standard practices in deep learning and parametrize logarithms $\log\alpha_k,\;\log\beta_l$ instead of directly parameterizing $\alpha_{k},\beta_l$. In turn, variables $r_k,\;\mu_l$ are parametrized directly as multi-dimensional vectors. We consider diagonal matrices $S_k,\;\Sigma_l$ and parametrize them via their diagonal values $\log(S_k)_{i,i}$ and $\log(\Sigma_l)_{i,i}$ respectively. We initialize the parameters following the scheme in \citep{korotin2024light}.
In all our experiments, we use the Adam optimizer.

\subsection{Details of the Experiment with Gaussian Mixtures}
We use $K=L=5$, $\varepsilon=0.05$, $lr=3e-4$ and batchsize $128$. We do $2\cdot 10^4$ gradient steps. For the LightSB algorithm, we use the parameters presented by the authors in the official repository.

\subsection{Details of the Image Translation Experiment}
We use the code and decoder model from
\begin{center}
    \url{https://github.com/podgorskiy/ALAE}
\end{center} 
We download the data and neural network extracted attributes for the FFHQ dataset from 
\begin{center}
\url{https://github.com/ngushchin/LightSB/}
\end{center}
In the \textit{Adult} class we include the images with the attribute \textit{Age} $\!\geq 44$; in the \textit{Young} class - with the \textit{Age}$\in[16, 44]$. We excluded the images with faces of children to increase the accuracy of classification per \textit{gender} attribute.
For the experiments with our solver, we use weighted $\text{D}_{\text{KL}}$ divergence with parameters $\tau$ specified in Appendix \ref{app-ablation}, and set $K=L=10$, $\varepsilon=0.05$, $lr=1$, and batch size to $128$. We do $5\cdot10^3$ gradient steps using Adam optimizer \citep{kingma2014adam} and \texttt{MultiStepLR} scheduler with parameter $\gamma=0.1$ and milestones$=[500,1000]$. For testing \citep[LightSB]{korotin2024light} solver, we use the official code (see the link above) and instructions provided by the authors.

\textbf{Baselines.} For the OT-FM and UOT-FM methods, we parameterize the vector field $(v_{t,\theta})_{t\in[0,1]}$ for mass transport using a 2-layer feed-forward network with 512 hidden neurons and ReLU activation.
An additional sinusoidal embedding\cite{tong2020trajectorynet} was applied for the parameter $t$. The learning rate for the Adam optimizer was set to 1e-4. To obtain an optimal transport plan $\pi^{*}(x,y)$ discrete OT solvers from the POT~\cite{flamary2021pot} package were used. 
 These methods are built on the solutions (plans $\pi^{*}(x,y)$) of discrete OT problems, to obtain them we use the POT~\cite{flamary2021pot} package. Especially for the UOT-FM, we use the \texttt{ot.unbalanced.sinkhorn} with the regularization equal to $0.05$. We set the number of training and inference time steps equal to $100$. To obtain results of UOT-FM for Fig. \ref{fig:alae-comp}, we run this method for 3K epochs with parameter $reg\_m\in[5e-4,5e-3,5e-2,0.5,1,10,10^2,10^3,10^4,10^5,10^6]$ and reported the mean values of final metrics for 3 independent launches with different seeds. In Tables \ref{table-alae-accuracy-keep}, \ref{table-alae-accuracy-target}, \ref{table-alae-fd}, for each translation we report the results for one chosen parameter specified in Appendix \ref{app-alae-comp}. We use the corresponding checkpoints of UOT-FM to visualize its performance in Fig. \ref{fig:alae}. For \citep[UOT-SD]{choi2024generative}, \citep[UOT-GAN]{yang2018scalable} we use the official code provided by the authors, see the links:
\begin{center}
    \url{https://github.com/Jae-Moo/Unbalanced-Optimal-Transport-Generative-Model}
\end{center} 

\begin{center}
\url{https://github.com/uhlerlab/unbalanced_ot}
\vspace{-2mm}
\end{center} 
While both UOT-SB, UOT-GAN methods were not previously applied to the FFHQ dataset, we set up a grid search for the parameters and followed the instructions provided by the authors for parameter settings. Both for UOT-SB, UOT-GAN, we used a 3-layer neural network with 512 hidden neurons, and ReLU activation was used for the generator networks and the potential and discriminator, respectively. Adam optimizer \citep{kingma2014adam} with $lr=10^{-5}$ and $lr=10^{-4}$ was used to train the networks in UOT-SB and UOT-GAN, respectively. We train the methods for 10K iterations and set a batch size to 128. For UOT-SD, we used $\text{D}_{\text{KL}}$ divergence with their unbalancedness parameter $\tau\!=\!0.002$. For other parameters, we used the default values provided by the authors for CIFAR-10 generation tasks. For all baseline models which use entropy regularization, we set $\varepsilon=0.05$.
\vspace{-2mm}


\section{Additional Discussion \& Experiments with $f$-divergences}
\label{app-ablation}

\textbf{Details about $f$-divergences between positive measures.} In the classic form, $f$-divergences are defined as measures of dissimilarity between two \textit{probability} measures. This definition should be revised when dealing with measures of arbitrary masses. In the paragraph below we show that if the function $f$ is convex, non-negative, and attains zero uniquely at point $\{1\}$ then $\Df{f}{\mu_1}{\mu_2}$ is a valid measure of dissimilarity between two positive measures.

Let $\mu_1,\mu_2\in \mathcal{M}_{2,+}(\mathbb{R}^{d'})$ be two positive measures. The $f$-divergence satisfies $\Df{f}{\mu_1}{\mu_2}\geq 0$ which is obvious from the non-negativity of $f$. From the definition of $\Df{f}{\mu_1}{\mu_2}$ and the fact that function $f$ attains zero uniquely  at a point $\{1\}$, we obtain that $\Df{f}{\mu_1}{\mu_2}=0$ if and only if $\mu_1(x)=\mu_2(x)$ holds $\mu_{2}$-everywhere. Actually, $\mu_1(x)=\mu_2(x)$ should hold for all $x$ as $\mu_{1}$ must be absolutely continuous w.r.t. $\mu_{2}$ (otherwise $\Df{f}{\mu_1}{\mu_2}$ is assumed to be equal $+\infty$).

\textbf{The usage of $D_{\chi^2}$ divergence.} We tested the performance of our solver with scaled $\text{D}_{\text{KL}}$ divergences in the main text, see \wasyparagraph\ref{sec-gaussian-exp}, \wasyparagraph\ref{sec-alae-exp}. For completeness, here we evaluate our solver with $\text{D}_{\chi^{2}}$ divergence in \textit{Gaussian Mixture} experiment. We use the same experimental setup as in \wasyparagraph\ref{sec-gaussian-exp} and present the qualitative results in Fig. \ref{fig:gauss-chi2}. Interestingly, the solver's results differ from those which we obtain for $\text{D}_{\text{KL}}$ divergence. For $\text{D}_{\chi^2}$ divergence, supports of learned plans' marginals constitute only parts of source and target measures' supports when $\tau=1$. The issue disappears with a slight increase of $\tau$, i.e., for $\tau=2$. At the same time, a further increase of $\tau$ is useless, since the learned plans fail to deal with class imbalance issue. Thus, parameter $\tau$ should be adjusted heuristically. In the case of $\text{D}_{\text{KL}}$ divergence, supports coincide for all $\tau$, see Fig. \ref{fig:gauss-kl}. This motivates us to use $\text{D}_{\text{KL}}$ divergences in our main experiments.

\begin{figure*}[t!]
\vspace{-3mm}
\centering
\begin{subfigure}[b]{0.235\linewidth}
    \centering
    \includegraphics[width=0.995\linewidth]{pics/toy_chi_10.png}
    \caption{U-LightOT, $\tau=10$}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.235\linewidth}
    \centering
    \includegraphics[width=0.995\linewidth]{pics/toy_chi_5.png}
    \caption{U-LightOT, $\tau=5$}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.235\linewidth}
    \centering
    \includegraphics[width=0.995\linewidth]{pics/toy_chi_2.png}
    \caption{U-LightOT, $\tau=2$}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.235\linewidth}
    \centering
    \includegraphics[width=0.995\linewidth]{pics/toy_chi_1.png}
    \caption{U-LightOT, $\tau=1$}
\end{subfigure}
\vspace{-1mm}\caption{\centering Conditional plans $\gamma_{\theta,\omega}(y|x)$ learned by our solver with scaled $\text{D}_{\chi^2}$ divergences in \textit{Gaussians Mixture} experiment ($\tau\in[1,2,5,10]$).}
\label{fig:gauss-chi2}
\end{figure*}

\textbf{Parameter $\tau$ in \textit{Gaussian Mixture} experiment.} An ablation study on unbalancedness parameter $\tau$ in \textit{Gaussian Mixture} experiment is conducted in \wasyparagraph\ref{sec-gaussian-exp} and above in this section. For completeness, we also perform the quantitative assessment of our solver with $\text{D}_{\text{KL}}$ divergence for different unbalancedness parameters $\tau$. We compute the normalized OT cost ($\mathbb{E}_{x\sim p} \mathbb{E}_{y\sim \gamma(y|x)} \frac{(x-y)^2}{2}$) between the source and generated distributions, and the Wasserstein distance between the generated and target distributions (computed by a discrete OT solver). For completeness, we additionally calculate the metrics for the \textit{balanced} \citep[LightSB]{korotin2024light} approach. 
The results are presented in Table \ref{table:ablation-tau-gauss}. 

\begin{table}[!ht]
    \centering\scriptsize
    \begin{tabular}{l|l l l l l}
    \hline
         \textbf{Method}& U-LightOT ($\tau=1$) & U-LightOT ($\tau=10$) & U-LightOT ($\tau=50$) & U-LightOT ($\tau=100$) & LightSB  \\ \hline
         OT cost ($\downarrow$) & $2.023$& $2.913$ & $3.874$ & $3.931$ & $3.952$\\\hline
         $\mathbb{W}_2$-distance ($\downarrow$) &$2.044$ & $1.107$ & $0.138$& $0.091$& $0.088$\\
         \hline
    \end{tabular}
    \vspace{1mm}
    \caption{Normalized OT cost between the input and learned distributions, and Wasserstein distance between the learned and target distributions in \textit{Gaussian mixture} experiment.}
    \vspace*{-3mm}
    \label{table:ablation-tau-gauss}
\end{table}

\textit{Results.} 
Recall that the unbalanced nature of our solver leads to two important properties. Firstly, our solver better preserves the properties of the input objects than the balanced approaches $-$ indeed, it allows for preserving object attributes (classes) even in the case of class imbalance. Secondly, due to the relaxed boundary condition for the target distribution, the distribution generated by our solver is naturally less similar to the target distribution than for balanced methods.

The above intuitive reasoning is confirmed by the metrics we obtained. Indeed, as the $\tau$ parameter increases, when our method becomes more and more similar to balanced approaches, the normalized OT cost between the source and generated distributions increases, and the Wasserstein distance between learned and target distributions decreases. LightSB [1] baseline, which is a purely balanced approach, shows the best quality in terms of Wasserstein distance and the worst in terms of OT cost.

\textbf{Parameters $\tau$, $\varepsilon$ in image experiments.} The effect of entropy regularization parameter $\varepsilon$ is well studied, see, e.g., \citep{gushchin2023entropic, korotin2024light}. Namely, increasing the parameter $\varepsilon$ stimulates the conditional distributions $\gamma_{\theta}(y|x)$ to become more dispersed. Still, below we provide an additional quantitative analysis of its influence on the learned translation. 
Besides, we address the question \textit{how does the parameter $\tau$ influence the performance of our solver in image translation experiments?}
To address this question, we learn the translations \textit{Young}$\rightarrow$ \textit{Adult}, \textit{Man}$\rightarrow$ \textit{Woman} on FFHQ dataset varying the parameters $\tau$, $\varepsilon$, see \wasyparagraph\ref{sec-alae-exp} for the experimental setup details. We test our solver with scaled $\text{D}_{\text{KL}}$ divergence training it for {\color{black}5K} iterations. Other hyperparameters are in Appendix \ref{sec:exp-details}.
In Tables \ref{table:ablation-acc-Y2A}, \ref{table:ablation-acc-A2Y}, \ref{table:ablation-acc-M2W}, \ref{table:ablation-acc-W2M}, we report the accuracy of keeping the attributes of the source images (e.g., gender in \textit{Young}$\rightarrow$\textit{Adult} translation). In Tables \ref{table:ablation-target-acc-Y2A}, \ref{table:ablation-target-acc-A2Y}, \ref{table:ablation-target-acc-M2W}, \ref{table:ablation-target-acc-W2M}, we report the accuracy of mapping to the correct target class (e.g., \textit{adult} people in \textit{Young}$\rightarrow$\textit{Adult} translation). In Tables \ref{table:ablation-fd-Y2A}, \ref{table:ablation-fd-A2Y}, \ref{table:ablation-fd-M2W}, \ref{table:ablation-fd-W2M}, we report FD metrics which is defined as \textit{Frechet distance} between means and covariances of the learned and the target measures. For convenience, we additionally illustrate the results of ablation studies on Fig. \ref{fig:alae-abl-eps}.

\begin{table}[!ht]
    \centering\scriptsize
    \begin{tabular}{l|l l l l l l l}
    \hline
        \diagbox{$\varepsilon$}{$\tau$} & $10$ & $20$ & $50$ & $10^2$ & \underline{$\mathbf{250}$} &$10^3$ & $10^6$ \\ \hline
        $0.01$ & $96.37 \!\pm\! 0.07$ & $96.12 \!\pm\! 0.25$ & $93.44 \!\pm\! 0.22$ & $90.26 \!\pm\! 0.18$ & $85.15 \!\pm\! 0.82$ & $81.79 \!\pm\! 0.98$ & $79.27 \!\pm\! 1.11$ \\ 
        $0.05$ & $96.03 \!\pm\! 0.08$ & $95.55 \!\pm\! 0.13$ & $93.19 \!\pm\! 0.55$ & $88.93 \!\pm\! 0.94$ & $84.49 \!\pm\! 1.57$& $80.59 \!\pm\! 0.55$ & $78.42 \!\pm\! 0.73$ \\ 
        $0.1$ & $95.20 \!\pm\! 0.21$ & $94.92 \!\pm\! 0.24$ & $92.30 \!\pm\! 0.25$ & $87.99 \!\pm\! 0.83$ & $82.91 \!\pm\! 1.01$& $78.76 \!\pm\! 0.50$ & $78.25 \!\pm\! 0.87$ \\ 
        $0.5$ & $88.53 \!\pm\! 0.26$ & $87.46 \!\pm\! 0.41$ & $83.41 \!\pm\! 0.80$ & $80.14 \!\pm\! 0.56$ & $76.00 \!\pm\! 0.84$& $73.36 \!\pm\! 0.23$ & $71.84 \!\pm\! 0.27$ \\ 
        $1.0$ & $-$ & $81.26 \!\pm\! 1.09$ & $77.84 \!\pm\! 0.33$ & $74.33 \!\pm\! 0.54$ & $71.01 \!\pm\! 1.13$& $67.43 \!\pm\! 0.15$ & $66.71 \!\pm\! 0.13$ \\ \hline
    \end{tabular}
    \caption{{\color{black}Test accuracy ($\uparrow$) of keeping the class in \textit{Young} $\rightarrow$ \textit{Adult} translation.}}
    \label{table:ablation-acc-Y2A}
\end{table}

% \vspace*{-3mm}
\begin{table}[!ht]
    \centering\scriptsize
    \begin{tabular}{l|l l l l l l l}
    \hline
        \diagbox{$\varepsilon$}{$\tau$} & $10$ & $20$ & $50$ & $10^2$ & \underline{$\mathbf{250}$} &$10^3$ & $10^6$ \\ \hline
        $0.01$ & $38.94 \!\pm\! 0.91$ & $51.92 \!\pm\! 0.80$ & $67.68 \!\pm\! 1.06$ & $75.49 \!\pm\! 0.30$ & $81.34 \!\pm\! 1.06$ &$83.67 \!\pm\! 0.74$ & $85.27 \!\pm\! 1.35$ \\ 
        $0.05$ & $40.09 \!\pm\! 0.16$ & $53.19 \!\pm\! 0.49$ & $69.01 \!\pm\! 0.74$ & $77.41 \!\pm\! 0.67$ & $81.78 \!\pm\! 0.33$& $84.80 \!\pm\! 0.90$ & $85.63 \!\pm\! 0.76$ \\ 
        $0.1$ & $44.18 \!\pm\! 1.50$ & $56.74 \!\pm\! 0.58$ & $71.77 \!\pm\! 0.57$ & $78.34 \!\pm\! 0.91$ & $83.70 \!\pm\! 0.54$& $87.07 \!\pm\! 0.37$ & $88.21 \!\pm\! 0.23$ \\ 
        $0.5$ & $50.51 \!\pm\! 0.34$ & $65.61 \!\pm\! 2.04$ & $81.50 \!\pm\! 1.17$ & $87.48 \!\pm\! 0.06$ & $92.21 \!\pm\! 0.29$ & $92.93 \!\pm\! 0.14$ & $93.80 \!\pm\! 0.55$ \\  
        $1.0$ & $-$ & $70.81 \!\pm\! 2.69$ & $83.82 \!\pm\! 2.45$ & $89.56 \!\pm\! 0.50$ & $93.78 \!\pm\! 0.35$& $95.04 \!\pm\! 0.20$ & $95.50 \!\pm\! 0.41$ \\ \hline
    \end{tabular}
    \caption{{\color{black}Test accuracy ($\uparrow$) of mapping to the target in \textit{Young} $\rightarrow$ \textit{Adult} translation.}}
    \label{table:ablation-target-acc-Y2A}
\end{table}
\vspace*{-3mm}\begin{table}[!ht]
    \centering\scriptsize
    \begin{tabular}{l|l l l l l l l}
    \hline
        \diagbox{$\varepsilon$}{$\tau$} & $10$ & $20$ & $50$ & $10^2$ & \underline{$\mathbf{250}$} & $10^3$ & $10^6$ \\ \hline
        $0.01$ & $35.55 \!\pm\! 1.24$ & $26.77 \!\pm\! 1.59$ & $17.39 \!\pm\! 0.19$ & $16.42 \!\pm\! 1.96$ & $12.28 \!\pm\! 1.16$& $11.46 \!\pm\! 0.70$ & $10.85 \!\pm\! 0.12$ \\  
        $0.05$ & $42.05 \!\pm\! 3.04$ & $31.47 \!\pm\! 0.03$ & $23.90 \!\pm\! 1.49$ & $18.31 \!\pm\! 0.16$ & $17.15 \!\pm\! 0.48$& $16.27 \!\pm\! 0.62$ & $19.89 \!\pm\! 5.51$ \\  
        $0.1$ & $50.05 \!\pm\! 2.48$ & $40.67 \!\pm\! 0.73$ & $31.15 \!\pm\! 0.32$ & $27.49 \!\pm\! 0.56$ & $25.86 \!\pm\! 0.88$& $24.57 \!\pm\! 0.06$ & $26.30 \!\pm\! 0.15$ \\  
        $0.5$ & $114.49 \!\pm\! 1.06$ & $104.42 \!\pm\! 0.33$ & $98.21 \!\pm\! 4.21$ & $92.48 \!\pm\! 0.98$ & $89.42 \!\pm\! 0.10$& $88.74 \!\pm\! 0.12$ & $89.55 \!\pm\! 1.18$ \\ 
        $1.0$ & $-$ & $165.39 \!\pm\! 1.50$ & $149.84 \!\pm\! 0.86$ & $142.39 \!\pm\! 0.18$ & $137.40 \!\pm\! 0.21$& $135.85 \!\pm\! 0.17$ & $135.25 \!\pm\! 0.17$ \\ \hline
    \end{tabular}
    \caption{{\color{black}Test FD ($\downarrow$) of generated latent codes in \textit{Young} $\rightarrow$ \textit{Adult} translation.} }
    \label{table:ablation-fd-Y2A}
\end{table}

\begin{table}[!ht]
    \centering\scriptsize
    \begin{tabular}{l|l l l l l l}
    \hline
        \diagbox{$\varepsilon$}{$\tau$} & $10$ & $20$ & $50$ & \underline{$\mathbf{10^2}$} & $10^3$ & $10^6$ \\ \hline
        $0.01$ & $96.14 \!\pm\! 0.08$ & $95.96 \!\pm\! 0.03$ & $93.60 \!\pm\! 0.72$ & $89.88 \!\pm\! 0.74$ & $81.49 \!\pm\! 0.89$ & $80.19 \!\pm\! 2.17$ \\ 
        $0.05$ & $95.36 \!\pm\! 0.41$ & $95.26 \!\pm\! 0.24$ & $92.77 \!\pm\! 0.48$ & $89.48 \!\pm\! 0.60$ & $81.34 \!\pm\! 0.35$ & $79.99 \!\pm\! 0.42$ \\  
        $0.1$ & $94.73 \!\pm\! 0.22$ & $94.33 \!\pm\! 0.21$ & $92.65 \!\pm\! 0.52$ & $89.12 \!\pm\! 0.40$ & $81.06 \!\pm\! 0.82$ & $77.73 \!\pm\! 1.21$ \\  
        $0.5$ & $88.21 \!\pm\! 0.82$ & $88.43 \!\pm\! 0.15$ & $86.13 \!\pm\! 0.94$ & $83.89 \!\pm\! 0.67$ & $74.92 \!\pm\! 0.53$ & $70.32 \!\pm\! 1.46$ \\  
        $1.0$ & $-$ & $81.67 \!\pm\! 0.94$ & $79.71 \!\pm\! 1.28$ & $77.15 \!\pm\! 1.65$ & $67.80 \!\pm\! 0.30$ & $65.31 \!\pm\! 0.90$ \\ \hline
    \end{tabular}
    \caption{{\color{black}Test accuracy ($\uparrow$) of keeping the class in \textit{Adult} $\rightarrow$ \textit{Young} translation.}}
    \label{table:ablation-acc-A2Y}
\end{table}

\begin{table}[!ht]
    \centering\scriptsize
    \begin{tabular}{l| l l l l l l}
    \hline
        \diagbox{$\varepsilon$}{$\tau$} & $10$ & $20$ & $50$ & \underline{$\mathbf{10^2}$} & $10^3$ & $10^6$ \\ \hline
        0.01 & $67.45 \!\pm\! 0.51$ & $76.38 \!\pm\! 0.36$ & $84.86 \!\pm\! 0.23$ & $87.87 \!\pm\! 0.37$ & $91.56 \!\pm\! 0.40$ & $92.29 \!\pm\! 0.37$ \\  
        $0.05$ & $67.21 \!\pm\! 0.10$ & $76.66 \!\pm\! 1.22$ & $84.43 \!\pm\! 0.40$ & $87.79 \!\pm\! 0.38$ & $91.98 \!\pm\! 0.50$ & $92.29 \!\pm\! 0.71$ \\  
        $0.1$ & $65.29 \!\pm\! 1.54$ & $76.58 \!\pm\! 1.10$ & $84.29 \!\pm\! 0.73$ & $88.65 \!\pm\! 0.64$ & $92.33 \!\pm\! 0.49$ & $92.87 \!\pm\! 0.11$ \\  
        $0.5$ & $69.13 \!\pm\! 3.43$ & $76.74 \!\pm\! 1.39$ & $82.33 \!\pm\! 6.53$ & $86.86 \!\pm\! 2.89$ & $94.06 \!\pm\! 0.62$ & $94.91 \!\pm\! 0.45$ \\  
        $1.0$ & $-$ & $80.13 \!\pm\! 1.89$ & $85.87 \!\pm\! 0.53$ & $91.00 \!\pm\! 0.62$ & $95.66 \!\pm\! 0.41$ & $96.37 \!\pm\! 0.52$ \\ \hline
    \end{tabular}
    \caption{{\color{black}Test accuracy ($\uparrow$) of mapping to the target in \textit{Adult} $\rightarrow$ \textit{Young} translation.}}
    \label{table:ablation-target-acc-A2Y}
\end{table}

\begin{table}[!ht]
    \centering\scriptsize
    \begin{tabular}{l|l l l l l l}
    \hline
        \diagbox{$\varepsilon$}{$\tau$} & $10$ & $20$ & $50$ & \underline{$\mathbf{10^2}$} & $10^3$ & $10^6$ \\ \hline
        $0.01$ & $45.58 \!\pm\! 7.90$ & $31.24 \!\pm\! 0.62$ & $22.44 \!\pm\! 0.32$ & $18.23 \!\pm\! 0.24$ & $17.30 \!\pm\! 2.99$ & $24.29 \!\pm\! 12.53$ \\  
        $0.05$ & $60.09 \!\pm\! 13.23$ & $38.88 \!\pm\! 3.3$3 & $27.85 \!\pm\! 0.07$ & $30.79 \!\pm\! 8.58$ & $24.59 \!\pm\! 4.48$ & $26.44 \!\pm\! 7.23$ \\  
        $0.1$ & $54.45 \!\pm\! 1.07$ & $46.52 \!\pm\! 0.70$ & $41.98 \!\pm\! 5.22$ & $34.02 \!\pm\! 0.25$ & $44.25 \!\pm\! 9.33$ & $40.16 \!\pm\! 7.51$ \\ 
        $0.5$ & $128.91 \!\pm\! 1.09$ & $121.00 \!\pm\! 0.91$ & $116.78 \!\pm\! 10.57$ & $111.49 \!\pm\! 5.02$ & $102.51 \!\pm\! 0.04$ & $102.31 \!\pm\! 0.11$ \\  
        $1.0$ & $-$ & $187.33 \!\pm\! 1.64$ & $173.11 \!\pm\! 1.67$ & $171.32 \!\pm\! 13.12$ & $156.25 \!\pm\! 4.40$ & $152.73 \!\pm\! 0.38$ \\ \hline
    \end{tabular}
    \caption{{\color{black}Test FD ($\downarrow$) of generated latent codes in \textit{Adult} $\rightarrow$ \textit{Young} translation.} }
    \label{table:ablation-fd-A2Y}
\end{table}

\begin{table}[!ht]
    \centering\scriptsize
    \begin{tabular}{l|l l l l l l}
    \hline
        \diagbox{$\varepsilon$}{$\tau$} & $10$ & $20$ & $50$ & \underline{$\mathbf{10^2}$} & $10^3$ & $10^6$ \\ \hline
        $0.01$ & $93.75 \!\pm\! 0.10$ & $93.28 \!\pm\! 0.07$ & $92.01 \!\pm\! 0.16$ & $90.85 \!\pm\! 0.12$ & $88.70 \!\pm\! 0.36$ & $87.62 \!\pm\! 0.34$ \\  
        $0.05$ & $93.40 \!\pm\! 0.17$ & $93.20 \!\pm\! 0.14$ & $91.91 \!\pm\! 0.25$ & $90.30 \!\pm\! 0.39$ & $88.05 \!\pm\! 0.20$ & $87.77 \!\pm\! 0.23$ \\  
        $0.1$ & $92.99 \!\pm\! 0.18$ & $92.78 \!\pm\! 0.15$ & $91.24 \!\pm\! 0.46$ & $89.57 \!\pm\! 0.06$ & $87.45 \!\pm\! 0.62$ & $87.21 \!\pm\! 0.11$ \\  
        $0.5$ & $89.96 \!\pm\! 0.40$ & $88.34 \!\pm\! 0.37$ & $87.24 \!\pm\! 0.26$ & $86.75 \!\pm\! 1.06$ & $84.09 \!\pm\! 0.31$ & $83.54 \!\pm\! 0.41$ \\  
        $1.0$ & $-$ & $84.94 \!\pm\! 0.54$ & $83.54 \!\pm\! 0.41$ & $81.80 \!\pm\! 0.14$ & $80.72 \!\pm\! 0.20$ & $79.87 \!\pm\! 0.47$ \\ \hline
    \end{tabular}
    \caption{{\color{black}Test accuracy ($\uparrow$) of keeping the class in \textit{Man} $\rightarrow$ \textit{Woman} translation.}}
    \label{table:ablation-acc-M2W}
\end{table}

\begin{table}[!ht]
    \centering\scriptsize
    \begin{tabular}{l|l l l l l l}
    \hline
        \diagbox{$\varepsilon$}{$\tau$} & $10$ & $20$ & $50$ & \underline{$\mathbf{10^2}$} & $10^3$ & $10^6$ \\ \hline
        $0.01$ & $61.38 \!\pm\! 0.29$ & $74.57 \!\pm\! 0.78$ & $85.78 \!\pm\! 0.87$ & $90.32 \!\pm\! 0.70$ & $93.79 \!\pm\! 0.26$ & $94.19 \!\pm\! 0.40$ \\  
        $0.05$ & $60.97 \!\pm\! 1.14$ & $74.99 \!\pm\! 0.32$ & $85.01 \!\pm\! 0.64$ & $90.23 \!\pm\! 0.50$ & $93.87 \!\pm\! 0.25$ & $94.44 \!\pm\! 0.07$ \\ 
        $0.1$ & $61.36 \!\pm\! 0.60$ & $73.52 \!\pm\! 0.24$ & $86.38 \!\pm\! 0.40$ & $90.38 \!\pm\! 0.39$ & $94.19 \!\pm\! 0.28$ & $94.73 \!\pm\! 0.36$ \\  
        $0.5$ & $65.74 \!\pm\! 0.87$ & $73.61 \!\pm\! 0.58$ & $86.45 \!\pm\! 0.48$ & $89.80 \!\pm\! 0.66$ & $95.14 \!\pm\! 0.58$ & $95.59 \!\pm\! 0.61$ \\  
        $1.0$ & $-$ & $78.18 \!\pm\! 0.18$ & $86.85 \!\pm\! 0.74$ & $91.50 \!\pm\! 0.74$ & $95.63 \!\pm\! 0.14$ & $95.93 \!\pm\! 0.17$ \\ \hline
    \end{tabular}
    \caption{{\color{black}Test accuracy ($\uparrow$) of mapping to the target in \textit{Man} $\rightarrow$ \textit{Woman} translation.}}
    \label{table:ablation-target-acc-M2W}
\end{table}

\begin{table}[!ht]
    \centering\scriptsize
    \begin{tabular}{l|l l l l l l}
    \hline
        \diagbox{$\varepsilon$}{$\tau$} & $10$ & $20$ & $50$ & \underline{$\mathbf{10^2}$} & $10^3$ & $10^6$ \\ \hline
        $0.01$ & $54.84 \!\pm\! 4.12$ & $37.87 \!\pm\! 0.55$ & $27.27 \!\pm\! 3.17$ & $22.69 \!\pm\! 3.48$ & $27.73 \!\pm\! 4.28$ & $17.32 \!\pm\! 1.29$ \\  
        $0.05$ & $51.61 \!\pm\! 2.58$ & $51.25 \!\pm\! 11.78$ & $34.28 \!\pm\! 1.04$ & $27.29 \!\pm\! 2.60$ & $30.63 \!\pm\! 1.67$ & $25.77 \!\pm\! 2.22$ \\  
        $0.1$ & $59.45 \!\pm\! 3.60$ & $51.17 \!\pm\! 3.28$ & $43.05 \!\pm\! 3.68$ & $38.02 \!\pm\! 1.64$ & $34.42 \!\pm\! 0.91$ & $37.54 \!\pm\! 1.79$ \\  
        $0.5$ & $132.45 \!\pm\! 6.07$ & $119.82 \!\pm\! 1.07$ & $107.16 \!\pm\! 1.48$ & $107.23 \!\pm\! 6.76$ & $103.24 \!\pm\! 2.32$ & $100.71 \!\pm\! 1.66$ \\  
        $1.0$ & $-$ & $182.26 \!\pm\! 2.20$ & $164.51 \!\pm\! 1.50$ & $156.41 \!\pm\! 2.04$ & $146.73 \!\pm\! 0.28$ & $146.05 \!\pm\! 0.10$ \\ \hline
    \end{tabular}
    \caption{{\color{black}Test FD ($\downarrow$) of generated latent codes in \textit{Man} $\rightarrow$ \textit{Woman} translation.}}
    \label{table:ablation-fd-M2W}
\end{table}

\begin{table}[!ht]
    \centering\scriptsize
    \begin{tabular}{l|l l l l l l}
    \hline
        \diagbox{$\varepsilon$}{$\tau$} & $10$ & $20$ & $50$ & $10^2$ & \underline{$\mathbf{10^3}$} & $10^6$ \\ \hline
        $0.01$ & $95.70 \!\pm\! 0.05$ & $95.31 \!\pm\! 0.10$ & $93.68 \!\pm\! 0.09$ & $92.46 \!\pm\! 0.13$ & $89.08 \!\pm\! 0.32$ & $88.81 \!\pm\! 0.05$ \\  
        $0.05$ & $95.55 \!\pm\! 0.08$ & $95.05 \!\pm\! 0.09$ & $93.67 \!\pm\! 0.10$ & $92.32 \!\pm\! 0.27$ & $89.66 \!\pm\! 0.17$ & $88.39 \!\pm\! 0.42$ \\  
        $0.1$ & $95.21 \!\pm\! 0.26$ & $94.83 \!\pm\! 0.19$ & $93.14 \!\pm\! 0.30$ & $91.96 \!\pm\! 0.26$ & $89.09 \!\pm\! 0.53$ & $87.38 \!\pm\! 0.40$ \\  
        $0.5$ & $93.02 \!\pm\! 0.22$ & $92.53 \!\pm\! 0.29$ & $91.13 \!\pm\! 0.01$ & $89.73 \!\pm\! 0.44$ & $85.87 \!\pm\! 1.20$ & $85.24 \!\pm\! 0.41$ \\ 
        $1.0$ & $-$ & $90.00 \!\pm\! 0.78$ & $88.83 \!\pm\! 0.73$ & $87.46 \!\pm\! 0.30$ & $83.32 \!\pm\! 1.03$ & $83.16 \!\pm\! 0.55$ \\ \hline
    \end{tabular}
    \caption{{\color{black}Test accuracy ($\uparrow$) of keeping the class in \textit{Woman} $\rightarrow$ \textit{Man} translation.}}
    \label{table:ablation-acc-W2M}
\end{table}

\begin{table}[!ht]
    \centering\scriptsize
    \begin{tabular}{l|l l l l l l}
    \hline
        \diagbox{$\varepsilon$}{$\tau$} & $10$ & $20$ & $50$ & $10^2$ & \underline{$\mathbf{10^3}$} & $10^6$ \\ \hline
        $0.01$ & $51.54 \!\pm\! 0.48$ & $64.67 \!\pm\! 1.09$ & $77.59 \!\pm\! 0.39$ & $82.52 \!\pm\! 0.49$ & $88.61 \!\pm\! 0.48$ & $88.99 \!\pm\! 0.18$ \\  
        $0.05$ & $49.26 \!\pm\! 0.72$ & $64.09 \!\pm\! 1.08$ & $76.88 \!\pm\! 0.17$ & $83.47 \!\pm\! 0.52$ & $88.59 \!\pm\! 0.55$ & $89.14 \!\pm\! 0.55$ \\  
        $0.1$ & $49.74 \!\pm\! 0.78$ & $63.99 \!\pm\! 0.82$ & $76.96 \!\pm\! 0.54$ & $82.45 \!\pm\! 0.22$ & $89.29 \!\pm\! 0.55$ & $89.14 \!\pm\! 0.38$ \\ 
        $0.5$ & $48.64 \!\pm\! 2.31$ & $60.88 \!\pm\! 0.55$ & $77.58 \!\pm\! 0.28$ & $82.10 \!\pm\! 1.17$ & $89.82 \!\pm\! 0.99$ & $90.63 \!\pm\! 1.06$ \\ 
        $1.0$ & $-$ & $62.62 \!\pm\! 0.24$ & $74.95 \!\pm\! 1.17$ & $82.42 \!\pm\! 0.73$ & $90.20 \!\pm\! 0.34$ & $90.75 \!\pm\! 0.37$ \\ \hline
    \end{tabular}
    \caption{{\color{black}Test accuracy ($\uparrow$) of mapping to the target in \textit{Woman} $\rightarrow$ \textit{Man} translation.}}
    \vspace*{-5mm}
    \label{table:ablation-target-acc-W2M}
\end{table}

\begin{table}[!ht]
    \centering\scriptsize
    \begin{tabular}{l|l l l l l l}
    \hline
        \diagbox{$\varepsilon$}{$\tau$} & $10$ & $20$ & $50$ & $10^2$ & \underline{$\mathbf{10^3}$} & $10^6$ \\ \hline
        $0.01$ & $47.00 \!\pm\! 1.74$ & $33.65 \!\pm\! 0.67$ & $23.85 \!\pm\! 1.15$ & $20.83 \!\pm\! 1.27$ & $16.48 \!\pm\! 0.09$ & $18.78 \!\pm\! 3.44$ \\  
        $0.05$ & $52.30 \!\pm\! 1.29$ & $39.79 \!\pm\! 1.25$ & $29.66 \!\pm\! 1.81$ & $27.23 \!\pm\! 3.45$ & $24.68 \!\pm\! 2.80$ & $23.43 \!\pm\! 1.91$ \\  
        $0.1$ & $58.40 \!\pm\! 0.62$ & $48.66 \!\pm\! 0.73$ & $37.55 \!\pm\! 0.35$ & $37.74 \!\pm\! 2.39$ & $31.63 \!\pm\! 0.42$ & $32.83 \!\pm\! 2.02$ \\  
        $0.5$ & $131.17 \!\pm\! 0.78$ & $120.63 \!\pm\! 0.76$ & $108.44 \!\pm\! 0.60$ & $104.85 \!\pm\! 1.17$ & $101.29 \!\pm\! 0.11$ & $102.26 \!\pm\! 1.58$ \\  
        $1.0$ & $-$ & $186.46 \!\pm\! 0.92$ & $169.64 \!\pm\! 0.63$ & $160.52 \!\pm\! 0.42$ & $152.78 \!\pm\! 0.13$ & $152.37 \!\pm\! 0.09$ \\ \hline
    \end{tabular}
    \caption{{\color{black}Test FD ($\downarrow$) of generated latent codes in \textit{Woman} $\rightarrow$ \textit{Man} translation.}}
    \vspace*{-4mm}
    \label{table:ablation-fd-W2M}
\end{table}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{pics/alae_abl_eps.png}
    \caption{Visualization of ablation studies on parameters $\tau$, $\varepsilon$ in image translation experiment.}
    \label{fig:alae-abl-eps}
\end{figure}

\textit{Results} show that increase of $\varepsilon$ negatively influences both accuracy of keeping the attributes of the source images and FD of generated latent codes which is caused by an increased dispersity of $\gamma_{\theta}(y|x)$. Interestingly, accuracy of mapping to the correct target class does not have an evident dynamics w.r.t. $\varepsilon$. At the same time, when $\tau$ \textit{increases}, the learned plans provide \textit{worse accuracy} for keeping the input latents' class but \textit{better FD} of generated latent codes {\color{black}and accuracy of mapping to the target class}. It is an expected behavior since for bigger $\tau$, the constraints on the marginals of the learned plans become more strict. That is, we enforce the marginals of the learned plans to be closer to source and target measures which allows learning more accurate mappings to target measure but does not allow keeping the source classes in the case of imbalance issues. Interestingly, in \textit{Adult}$\rightarrow$\textit{Young} translation, FD of learned latents and accuracy of mapping to the target do not change much for $\tau\geq10^2$ while accuracy of keeping the attributes exhibits a significant drop between $\tau=10^2$ and $\tau=10^3$. Thus, we can treat $\tau=10^2$ as optimal since it provides the best trade-off between the quality of learned translations and their ability to keep the features of input latents. In the case of \textit{Young}$\rightarrow$\textit{Adult} translation, the values of accuracy and FD exhibit significant differences for considered $\tau$. Thus, we may consider a more detailed scale and choose $\tau=2.5\cdot 10^2$ as the optimal one.

It is important to note that our method offers a flexible way to select a domain translation configuration that allows for better preserving the properties of the original objects or generating a distribution closer to the target one. The final optimal configuration selection remains at the discretion of the user. The highlighted values in the Tables are used for comparison with other approaches in \wasyparagraph\ref{app-alae-comp}.

\textit{Remark.} FD should be treated as a \textit{relative} measure of similarity between learned and target measures. The results obtained by balanced solver \citep[LightSB]{korotin2024light} (equivalent to ours for big $\tau$) are considered as a gold standard.

\textbf{Number of Gaussian components in potentials.} For completeness, we perform an ablation study of our U-LightOT solver with different number on Gaussian components ($K$, $L$) in potentials $v_{\theta}$ and $u_{\omega}$, respectively. We run the solver in \textit{Young}$\rightarrow$\textit{Adult} translation with 5K steps, $\varepsilon=0.05$ and set $\tau=250$. The quantitative results (accuracy of keeping the class, accuracy of mapping to the target, FD of generated latents vs target latents) are presented in the Tables below.

\begin{table}[!ht]
    \centering\scriptsize
    \hspace*{-1cm}\begin{tabular}{l|l l l l l l l l}
    \hline
         \diagbox{$K$}{$L$} & $1$ & $2$ & $4$ & $8$ & $16$ & $32$ & $64$ & $128$ \\\hline  
        $1$ & $84.17 \!\pm\! 0.45$ & $84.73 \!\pm\! 1.27$ & $84.25 \!\pm\! 1.08$ & $85.08 \!\pm\! 0.78$ & $84.84 \!\pm\! 0.92$ & $85.82 \!\pm\! 0.70$ & $85.52 \!\pm\! 0.74$ & $83.70 \!\pm\! 0.48$ \\  
        $2$ & $84.99 \!\pm\! 0.59$ & $84.54 \!\pm\! 0.43$ & $84.56 \!\pm\! 0.39$ & $84.18 \!\pm\! 0.63$ & $83.87 \!\pm\! 2.03$ & $83.56 \!\pm\! 1.37$ & $84.66 \!\pm\! 0.44$ & $86.74 \!\pm\! 0.54$ \\ 
        $4$ & $84.50 \!\pm\! 0.86$ & $84.81 \!\pm\! 0.46$ & $84.60 \!\pm\! 0.37$ & $84.65 \!\pm\! 0.77$ & $83.75 \!\pm\! 1.01$ & $84.00 \!\pm\! 1.52$ & $83.51 \!\pm\! 0.90$ & $83.99 \!\pm\! 0.63$ \\ 
        $8$ & $83.88 \!\pm\! 1.01$ & $84.08 \!\pm\! 0.81$ & $83.71 \!\pm\! 0.60$ & $82.76 \!\pm\! 1.98$ & $84.69 \!\pm\! 0.38$ & $84.30 \!\pm\! 0.39$ & $85.05 \!\pm\! 1.59$ & $83.03 \!\pm\! 1.02$ \\ 
       $16$ & $84.65 \!\pm\! 0.33$ & $85.00 \!\pm\! 1.62$ & $84.28 \!\pm\! 0.78$ & $84.76 \!\pm\! 1.23$ & $83.66 \!\pm\! 0.33$ & $85.14 \!\pm\! 0.49$ & $83.77 \!\pm\! 1.16$ & $84.34 \!\pm\! 0.15$ \\  
        $32$ & $83.48 \!\pm\! 0.40$ & $86.02 \!\pm\! 1.22$ & $84.79 \!\pm\! 0.60$ & $84.44 \!\pm\! 0.42$ & $85.24 \!\pm\! 0.84$ & $84.06 \!\pm\! 1.00$ & $84.73 \!\pm\! 0.51$ & $84.54 \!\pm\! 0.27$ \\ 
        $64$ & $85.24 \!\pm\! 0.27$ & $85.23 \!\pm\! 0.59$ & $84.12 \!\pm\! 0.12$ & $84.64 \!\pm\! 0.64$ & $84.01 \!\pm\! 1.95$ & $84.10 \!\pm\! 1.26$ & $84.77 \!\pm\! 1.25$ & $83.76 \!\pm\! 1.44$ \\ 
        $128$ & $85.21 \!\pm\! 0.29$ & $85.16 \!\pm\! 0.07$ & $84.63 \!\pm\! 1.15$ & $84.64 \!\pm\! 0.49$ & $84.12 \!\pm\! 0.57$ & $84.11 \!\pm\! 0.76$ & $84.22 \!\pm\! 0.68$ & $84.64 \!\pm\! 0.93$ \\ \hline
    \end{tabular}
    \vspace{1mm}
    \caption{\centering Test accuracy ($\uparrow$) of keeping the attributes in \textit{Young}$\rightarrow$\textit{Adult} translation for our U-LightOT solver with different number of Gaussian components in potentials.}
    \vspace*{-5mm}
\end{table}

\begin{table}[!ht]
    \centering\scriptsize
    \hspace*{-1cm}\begin{tabular}{l|l l l l l l l l}
    \hline
        \diagbox{$K$}{$L$} & $1$ & $2$ & $4$ & $8$ & $16$ & $32$ & $64$ & $128$ \\ \hline
        $1$ & $83.35 \!\pm\! 0.85$ & $82.03 \!\pm\! 0.11$ & $82.69 \!\pm\! 0.82$ & $83.34 \!\pm\! 0.08$ & $82.95 \!\pm\! 0.46$ & $82.80 \!\pm\! 0.58$ & $82.49 \!\pm\! 0.64$ & $82.54 \!\pm\! 0.36$ \\ 
        $2$ & $83.26 \!\pm\! 0.98$ & $81.80 \!\pm\! 0.81$ & $81.82 \!\pm\! 0.66$ & $82.35 \!\pm\! 0.92$ & $82.77 \!\pm\! 0.45$ & $82.26 \!\pm\! 0.98$ & $82.36 \!\pm\! 0.37$ & $82.84 \!\pm\! 0.38$ \\ 
        $4$ & $82.76 \!\pm\! 0.17$ & $81.47 \!\pm\! 0.29$ & $82.32 \!\pm\! 0.13$ & $82.60 \!\pm\! 0.66$ & $82.70 \!\pm\! 0.64$ & $82.48 \!\pm\! 0.37$ & $83.23 \!\pm\! 1.24$ & $82.44 \!\pm\! 0.68$ \\
        $8$ & $82.35 \!\pm\! 0.39$ & $81.63 \!\pm\! 0.38$ & $82.23 \!\pm\! 0.52$ & $82.19 \!\pm\! 0.72$ & $82.64 \!\pm\! 0.58$ & $82.55 \!\pm\! 0.18$ & $82.85 \!\pm\! 0.28$ & $82.87 \!\pm\! 0.09$ \\
        $16$ & $82.43 \!\pm\! 0.61$ & $82.12 \!\pm\! 1.10$ & $81.30 \!\pm\! 0.79$ & $82.11 \!\pm\! 0.93$ & $82.56 \!\pm\! 0.26$ & $81.88 \!\pm\! 0.85$ & $83.34 \!\pm\! 0.57$ & $82.90 \!\pm\! 0.39$ \\ 
        $32$ & $81.57 \!\pm\! 0.47$ & $81.39 \!\pm\! 0.62$ & $81.76 \!\pm\! 0.27$ & $81.28 \!\pm\! 0.57$ & $82.47 \!\pm\! 1.30$ & $82.06 \!\pm\! 0.41$ & $82.35 \!\pm\! 0.55$ & $82.56 \!\pm\! 0.89$ \\ 
        $64$ & $82.01 \!\pm\! 0.79$ & $82.33 \!\pm\! 0.83$ & $81.97 \!\pm\! 0.61$ & $82.01 \!\pm\! 0.26$ & $82.44 \!\pm\! 0.74$ & $81.61 \!\pm\! 0.75$ & $82.06 \!\pm\! 0.51$ & $83.44 \!\pm\! 0.38$ \\ 
        $128$ & $82.39 \!\pm\! 0.92$ & $82.18 \!\pm\! 0.56$ & $81.74 \!\pm\! 0.19$ & $82.22 \!\pm\! 0.74$ & $81.58 \!\pm\! 0.23$ & $82.66 \!\pm\! 0.33$ & $83.07 \!\pm\! 0.30$ & $82.85 \!\pm\! 0.45$ \\ \hline
    \end{tabular}
    \vspace{1mm}
    \caption{\centering Test accuracy ($\uparrow$) of mapping to the target in \textit{Young}$\rightarrow$\textit{Adult} translation for our U-LightOT solver with different number of Gaussian components in potentials.}
    \vspace*{-5mm}
\end{table}

\begin{table}[!ht]
    \centering\scriptsize
    \hspace*{-1cm}\begin{tabular}{l|l l l l l l l l}
    \hline
        \diagbox{$K$}{$L$} & $1$ & $2$ & $4$ & $8$ & $16$ & $32$ & $64$ & $128$ \\ \hline
        $1$ & $17.41 \!\pm\! 0.28$ & $17.42 \!\pm\! 0.36$ & $18.10 \!\pm\! 1.41$ & $17.83 \!\pm\! 0.41$ & $17.97 \!\pm\! 0.69$ & $18.42 \!\pm\! 0.86$ & $17.69 \!\pm\! 0.37$ & $18.07 \!\pm\! 0.61$ \\ 
        $2$ & $19.07 \!\pm\! 0.81$ & $17.47 \!\pm\! 0.83$ & $16.87 \!\pm\! 0.25$ & $16.93 \!\pm\! 0.56$ & $21.05 \!\pm\! 1.86$ & $17.38 \!\pm\! 0.69$ & $17.22 \!\pm\! 0.76$ & $17.83 \!\pm\! 0.70$ \\ 
        $4$ & $17.07 \!\pm\! 0.53$ & $16.66 \!\pm\! 0.14$ & $17.02 \!\pm\! 0.84$ & $18.01 \!\pm\! 1.79$ & $16.71 \!\pm\! 0.06$ & $17.07 \!\pm\! 0.95$ & $16.59 \!\pm\! 0.41$ & $16.42 \!\pm\! 0.10$ \\ 
        $8$ & $16.55 \!\pm\! 0.22$ & $23.56 \!\pm\! 7.59$ & $16.37 \!\pm\! 0.23$ & $16.81 \!\pm\! 0.95$ & $17.21 \!\pm\! 1.01$ & $18.37 \!\pm\! 1.47$ & $17.13 \!\pm\! 0.77$ & $16.96 \!\pm\! 0.86$ \\
        $16$ & $17.58 \!\pm\! 1.58$ & $18.06 \!\pm\! 1.61$ & $18.25 \!\pm\! 2.46$ & $17.69 \!\pm\! 1.87$ & $18.00 \!\pm\! 0.34$ & $17.19 \!\pm\! 0.71$ & $17.62 \!\pm\! 0.93$ & $18.97 \!\pm\! 2.01$ \\ 
        $32$ & $17.85 \!\pm\! 0.10$ & $17.15 \!\pm\! 0.71$ & $16.50 \!\pm\! 0.10$ & $21.91 \!\pm\! 4.05$ & $20.04 \!\pm\! 2.78$ & $19.31 \!\pm\! 3.22$ & $18.79 \!\pm\! 2.03$ & $22.56 \!\pm\! 1.66$ \\
        $64$ & $21.39 \!\pm\! 1.32$ & $21.91 \!\pm\! 4.23$ & $21.00 \!\pm\! 4.47$ & $18.02 \!\pm\! 1.24$ & $21.63 \!\pm\! 3.14$ & $19.56 \!\pm\! 0.99$ & $17.53 \!\pm\! 0.47$ & $20.26 \!\pm\! 4.46$ \\ 
        $128$ & $22.09 \!\pm\! 4.91$ & $35.68 \!\pm\! 21.07$ & $33.80 \!\pm\! 21.67$ & $20.46 \!\pm\! 2.39$ & $24.80 \!\pm\! 5.33$ & $22.73 \!\pm\! 3.03$ & $22.09 \!\pm\! 1.25$ & $22.72 \!\pm\! 7.41$ \\ \hline
    \end{tabular}
    \vspace{1mm}
    \caption{\centering Test FD ($\downarrow$) of generated latent codes in \textit{Young}$\rightarrow$\textit{Adult} translation for our U-LightOT solver with different number of Gaussian components in potentials.}
\end{table}\vspace{-2mm}

The results show that in the considered task, our solver provides good performance even for small number of Gaussian components. This can be explained by the smoothness of the latent representations of data in ALAE autoencoder.


\section{Additional Experimental Results}
\subsection{Quantitative comparison with other methods in Image-to-Image translation experiment}\label{app-alae-comp}
In this section, we provide additional results of quantitative comparison of our U-LightOT solver and other unbalanced and balanced OT/EOT solvers in image translation experiment. Tables \ref{table-alae-accuracy-keep}, \ref{table-alae-accuracy-target}, \ref{table-alae-fd}, provide values used for plotting Fig. \ref{fig:alae-comp} in the main text. The unbalancedness parameters used for our U-LightOT solver and \citep[UOT-FM]{eyring2023unbalancedness} are specified in th Tables below. For obtaining the result of \citep[UOT-SD]{choi2024generative}, we use their unbalancedness parameter $\tau=0.002$. For other details on methods' parameters used to obtain the results below, see Appendix \ref{sec:exp-details}. Note that we do not include FID metric for assessing the quality of generated images since we found that it is not a representative metric for assessing the performance of models performing the translation of ALAE latent codes.

\begin{table}[h!]
    \centering
    % \small
    \scriptsize
    % \begin{sc}
    \begin{tabular}{ c|c c c c c c c c} 
    \hline
    \textbf{Experiment} &  \makecell{OT-FM\\\cite{eyring2023unbalancedness}} & \makecell{\citep[LightSB]{korotin2024light} \textit{or}\\\citep[LightSBM]{gushchin2024light}} & \citep[UOT-FM]{eyring2023unbalancedness} &  \makecell{UOT-SD \\\citep{choi2024generative}} & \makecell{UOT-GAN\\\cite{yang2018scalable}} &\makecell{U-LightOT \\(\textbf{ours})}  \\
    \hline
     \textit{Young}$\rightarrow$\textit{Adult}  &  $67.71$  &  $78.16 $  & $84.46$ ($reg\_m=0.005$) & $45.71$ & $73.85$ &  $84.49$ ($\tau=250$)  \\  
     \textit{Adult}$\rightarrow$\textit{Young}  &   $53.79$   &  $80.25$  & $76.05$ ($reg\_m=0.005$) & $49.30$  &$74.74$ &$89.48$ ($\tau=10^2$)    \\ \hline
     \textit{Man}$\rightarrow$\textit{Woman}  &   $76.05$   &  $87.82$  & $84.42$ ($reg\_m=0.005$) & $75.50$  & $84.04$ &$90.30$ ($\tau=10^2$)  \\ 
     \textit{Woman}$\rightarrow$\textit{Man}  &  $72.40$   &  $88.10$  & $86.10$ ($reg\_m=0.05$) & $72.03$ & $84.56$ &$89.66$ ($\tau=10^3$)   \\  \hline
    \end{tabular}
     % \end{sc}
     \vspace{1mm}
     \caption{
     \centering {\color{black}Comparison of accuracies of keeping the attributes of the source images. 
     }}
     \label{table-alae-accuracy-keep}
     \vspace{-3mm}
\end{table}

\begin{table}[h!]
    \centering
    % \small
    \scriptsize
    % \begin{sc}
    \begin{tabular}{ c|c c c c c c c c} 
    \hline
    \textbf{Experiment} &  \makecell{OT-FM\\\cite{eyring2023unbalancedness}} & \makecell{\citep[LightSB]{korotin2024light} \textit{or}\\\citep[LightSBM]{gushchin2024light}} & \citep[UOT-FM]{eyring2023unbalancedness} &  \makecell{UOT-SD \\\citep{choi2024generative}} & \makecell{UOT-GAN\\\cite{yang2018scalable}} &\makecell{U-LightOT \\(\textbf{ours})}  \\
    \hline
     \textit{Young}$\rightarrow$\textit{Adult}  &   $93.28$  & $85.97$  & $74.10$ ($reg\_m=0.005$) & $87.33$ & $84.25$ & $81.78$ ($\tau=250$)  \\  
     \textit{Adult}$\rightarrow$\textit{Young}  &  $96.12$    &  $93.10$  & $89.30$ ($reg\_m=0.005$)&  $97.39$ & $95.88$ &$87.79$ ($\tau=10^2$)   \\ \hline
     \textit{Man}$\rightarrow$\textit{Woman}  &   $93.33$   &  $94.37$  &$92.35$ ($reg\_m=0.005$)  & $98.16$  & $97.38$ &$90.23$ ($\tau=10^2$)  \\ 
     \textit{Woman}$\rightarrow$\textit{Man}  & $94.27$    &  $89.66$  &  $88.53$ ($reg\_m=0.05$) & $94.96$ & $92.91$ &$88.59$ ($\tau=10^3$)   \\  \hline
    \end{tabular}
     % \end{sc}
     \vspace{1mm}
     \caption{\centering{\color{black}
      Comparison of accuracies of mapping to the target.
      }}
     \label{table-alae-accuracy-target}
     \vspace{-3mm}
\end{table}

\begin{table}[h!]
    \centering
    % \small
    \scriptsize
    % \begin{sc}
    \begin{tabular}{ c|c c c c c c c c} 
    \hline
    \textbf{Experiment} &  \makecell{OT-FM\\\cite{eyring2023unbalancedness}} & \makecell{\citep[LightSB]{korotin2024light} \textit{or}\\\citep[LightSBM]{gushchin2024light}} & \citep[UOT-FM]{eyring2023unbalancedness} &  \makecell{UOT-SD \\\citep{choi2024generative}} & \makecell{UOT-GAN\\\cite{yang2018scalable}} &\makecell{U-LightOT \\(\textbf{ours})}  \\
    \hline
     \textit{Young}$\rightarrow$\textit{Adult}  &  $11.93$   & $15.50$  & $11.57$ ($reg\_m=0.005$) & $13.28$ & $11.23$ & $17.15$ ($\tau=250$) \\  
     \textit{Adult}$\rightarrow$\textit{Young}  &   $14.10$   & $21.41$  & $17.00$ ($reg\_m=0.005$) &  $18.44$ &$14.94$ & $30.79$  ($\tau=10^2$)  \\ \hline
     \textit{Man}$\rightarrow$\textit{Woman}  &  $16.20$    &  $20.91$  &$10.31$  ($reg\_m=0.005$)&  $16.13$ & $22.41$ & $27.29$ ($\tau=10^2$)  \\ 
     \textit{Woman}$\rightarrow$\textit{Man}  & $11.42$ &  $30.87$  & $6.99$ ($reg\_m=0.05$) & $13.23$ & $10.55$ & $24.68$ ($\tau=10^3$)   \\  \hline
    \end{tabular}
     % \end{sc}
     \vspace{1mm}
     \caption{\centering{\color{black}
      Comparison of FD between the generated and learned latents. 
      }}
     \label{table-alae-fd}
     \vspace{-3mm}
\end{table}

\subsection{Outlier robustness property of U-LightOT solver}


To show the outlier robustness property of our solver, we conduct the experiment on Gaussian Mixtures with added outliers and visualize the results in Fig. \ref{fig-outlier-robustness}. The setup of the experiment, in general, follows the \textit{Gaussian mixtures} experiment setup described in section 5.2 of our paper. The difference consists in outliers (small gaussians) added to the input and output measures. 

\begin{figure*}[h!]
\vspace{-3mm}
\centering
\begin{subfigure}[b]{0.19\linewidth}
    \centering
    \includegraphics[width=0.995\linewidth]{pics/toy_outliers_pq.png}
    \caption{  Measures $p,q$}
\end{subfigure}
\begin{subfigure}[b]{0.19\linewidth}
    \centering
    \includegraphics[width=0.995\linewidth]{pics/toy_outliers_lsb_unb.png}
    \caption{\centering LightSB}  
\end{subfigure}
\begin{subfigure}[b]{0.19\linewidth}
    \centering
    \includegraphics[width=0.995\linewidth]{pics/toy_outliers_res.png}
    \caption{\centering U-LightOT (ours)} 
\end{subfigure}
\caption{\centering Conditional plans $\gamma_{\theta,\omega}(y|x)$ learned by our solver ($\tau=1$) and \linebreak LightSB in \textit{Gaussian Mixtures with otliers} experiment. 
}
\label{fig-outlier-robustness}
\end{figure*}

\textit{The results} show that our U-LightOT solver successfully eliminates the outliers and manages to simultaneously handle the class imbalance issue. At the same time, the balanced LightSB \citep{korotin2024light} solver fails to deal with either of these problems.


\section{Limitations and Broader Impact}
\label{sec-limitations}
\textbf{Limitations.} One limitation of our solver is the usage of the Gaussian Mixture parametrization which might restrict the scalability of our solver. This points to the necessity for developing ways to optimize objective \eqref{unbalanced-eot-dual} with more general parametrization, e.g., neural networks. 

\textbf{Broader impact.} Our work aims to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.

\end{document}
