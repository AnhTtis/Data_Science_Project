\begin{figure}[t]
\includegraphics[width=\linewidth]{mano_arxiv.eps}
\caption{Left: The anatomical MANO model with exemplary movements of joint $i$ in the sagittal ($\theta^i_x$) and coronal plane ($\theta^i_y$). Middle: Dense correspondence encoding. Right: Segmentation sets $S^i_{3d}$ computed from correspondence space.}\label{Overview}
\label{fig:mano}
\end{figure}
\subsection{Dense Correspondence with Semantic Encodings}
\label{section:dense}
Our goal is to fit the MANO model such that it best describes the observations in an RGB-D image. To this end, we establish correspondences between a pixel $(x,y)$ and a vertex $\boldsymbol v \in \mathcal{V}$ through a novel, shared canonical correspondence space embedded in $[0,1]^3$. For this, we define the function $\boldsymbol c\colon\mathcal{V}\rightarrow [0,1]^3$, which maps $\boldsymbol v$ to its coordinate in the correspondence space. As depicted in \autoref{fig:mano}, the space is encoded into a Hue-Saturation-Value (HSV) color cylinder wrapped around the flat rest pose of the model, aligned such that the axes describe semantic features of the hand. The hue range is scaled to lie within the extent of the MANO model and describes the finger type. The saturation is computed on each finger separately and encodes the corresponding segment. To distinguish between the front and back of the hand, the value axis encodes the surface normal along the y-axis. In summary, the correspondence space encodes both, spatial and semantic hand features while being compact, continuous, and deterministic to compute. The semantic encoding enables us to define a function $d\colon[0,1]^3\rightarrow\{1,...,20\}$ that computes a discrete segmentation label out of the continuous space, which is later used in \autoref{section:optim}. \autoref{fig:mano} shows the corresponding segmented vertex sets $\mathcal{S}_{3d} = \{S^i_{3d}\}_{i=1}^{20}$ with $S^i_{3d} = \{\boldsymbol v \in \mathcal{V} \mid d(\boldsymbol c(\boldsymbol v)) = i \}$, of which 15 refer to the three segments of each finger, and the remaining divide the large area of the wrist into 5 per-finger regions. 

 \paragraph{\textbf{Correspondence Regression.}}
As depth-only datasets are limited in availability and generalization across depth images of different sensor types is challenging, we leverage a variety of RGB(-D) datasets~\cite{interhand,freihand,h2o,honnotate,rgb_hampali} to train our network only on RGB data in a fully supervised manner. We use a mixture of (semi-)automatically labeled ground-truth MANO parameters to transform the models to their position in the image and render the parts of the visible surface to obtain ground-truth correspondence images. The network is an extension of Yolact~\cite{yolact} with an additional branch for correspondence prediction, which is trained by minimizing the smooth L1 loss between the predicted and ground-truth correspondence value of each pixel within the ground-truth segmentation mask of the hand. At inference time, we multiply the correspondence values with the predicted mask $\boldsymbol M$ to acquire per-pixel correspondences only for the hand. 

\paragraph{\textbf{Correspondence Matching.}}
\label{sec:matching}
Correspondence pairs are established by comparing each predicted $\boldsymbol c_p = \boldsymbol F(x,y)$ at pixel $(x,y)$ with $\boldsymbol c_v = \boldsymbol c(\boldsymbol v)$ of every MANO vertex $\boldsymbol v$. However, to find a match, using a traditional distance thresholded nearest-neighbor~\cite{depth_mueller} algorithm is not the optimal strategy as the continuous nature of the correspondence space can lead to wrong assignments at positions in between fingers. Thus, we compute nearest-neighbor matches within each segmented vertex set $\mathcal{S}^i_{3d}$ and its corresponding pixel set $S^i_{2d} = \{ (x,y) \mid d(\boldsymbol F(x,y)) = i \}$ instead.  A match between $\boldsymbol c_p$ and $ \boldsymbol c_v$ is used to construct a pair $(\boldsymbol p,\boldsymbol v)$ of 3D correspondences between $\boldsymbol v$ and an image point $\boldsymbol p \in \mathbb{R}^3$, computed from the back-projection of the depth value at $\boldsymbol D(x,y)$. Since $\boldsymbol c_p$ is predicted in the view of the RGB camera, it is not exactly aligned with the pixel positions of $\boldsymbol D$. Particularly at the edges of the hand silhouette, the predictions can map to erroneous points of the background. Hence, we first discard pairs, in which $\boldsymbol D(x,y)$ deviates too far from the median depth of the hand, determined by a threshold $t_d$. Second, we filter out points at silhouette edges with degraded and noisy depth by inspecting whether the angle of the point-wise normal computed from $\boldsymbol D$ exceeds a given threshold $t_n$. Lastly, we discard all pairs $(\boldsymbol p,\boldsymbol v)$, of which the Euclidean norm of their difference exceeds the 3D distance threshold $t_{3d}$. The final 3D correspondence set is denoted by $\mathcal{C}_{3d}$.






