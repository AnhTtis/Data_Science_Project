

\begin{figure*}[]
\centering
\includegraphics[width=\textwidth]{images/eval.pdf}%
\caption{Top: Images and angle results from an abduction and adduction sequence (repeated $3\times$). Middle: Results of the rotating ball sequence. Unobserved (grey hand surface) or error-prone (red surface) poses are listed as disconnected grey dots in the plots.  Bottom: Comparison of ShaRPy with OpenPose~\cite{openpose_full,openpose_hand}.}%
\label{fig:eval}
\end{figure*}



\section{Results}
\label{section:eval}


Our network is implemented and trained in PyTorch. 
At inference time, it is embedded together with the rest of the pipeline into a shared C++ framework, which utilizes the libTorch library for automatic differentiation. During shape and pose estimation, we initialize the tracking by using the L-BFGS optimizer and then iteratively refine the energy with Adam. 

\paragraph{Experiments.}
The results are divided into two experiments. 
First, we show the clinical applicability of our setup (with V2) on a male, 61 years old PsA patient (Disease Activity in Psoriatic Arthritis score: 17.52) and demonstrate the reliability to discard invalid pose predictions through the detection of uncertainty. Second, we quantitatively and qualitatively compare the accuracy of our method with the state-of-the-art (SOTA). For the evaluation, we apply three different training procedures, denoted as \textit{V1}, \textit{V2}, and \textit{V3}. In V1, we exclusively train on the H2O~\cite{h2o} dataset. In V2, we train on all previously mentioned datasets~\cite{h2o,honnotate,rgb_hampali,interhand,freihand}. In V3, we exclude H2O and train on the remaining data~\cite{honnotate,rgb_hampali,interhand,freihand}. 

\paragraph{H2O.}  The H2O dataset is recorded from multi-view RGB-D images of two hands manipulating objects that are placed on a desk. It contains accurate 3D hand annotations of egocentric views, which we find most similar to a top-down view of a clinical setup for hand function assessments. On top of that, the manipulation of objects besides hand motion itself is interesting as an extension of hand function assessments. In the corresponding H2O dataset benchmark for hand pose estimation, the performance is measured with respect to the Mean End-point Error (MEPE) and the Percentage of Correct Keypoints (PCK).

\subsection{Clinical applicability} Similar to clinical practice, we recorded a sequence of the finger adduction and abduction together with the finger hyperextension and assess the hand function by measuring the angles of the fingers, using the middle finger as a reference. We achieve this by projecting the segments between the proximal interphalangeal joints (PIP) and MCP joints onto the wrist plane and computing the angle deviation from the PIP-MCP segment of the middle finger. The results are depicted in \autoref{fig:eval}. We are further able to visualize the finger hyperextension due to the depth information, which is not possible in RGB-only approaches. Next, we recorded the patient holding a ball and rotating the wrist around the camera. As we can assume that the fingers hardly move during this task, we expect corresponding results in the finger angles. We plot the angles of the pose $\theta$ around the MCP of the thumb and the index and filter out all measurements, in which one of the respective finger segments is marked as uncertain within three consecutive frames. We compare the results with unfiltered angle measurements and perceive a significant decrease in angle variance from $112.55^{\circ}$ to $18.16^{\circ}$ on the middle finger and from $125.29^{\circ}$ to $37.84^{\circ}$ on the thumb, which was less visible and mainly close to silhouette edges in the depth map.

\begin{table*}[] 
	\centering
	\begin{tabular}{ c||c|c||c|c||c|c } 
		& \multicolumn{2}{c||}{MEPE (mm)$\downarrow$}& \multicolumn{2}{c||}{3D PCK@15mm$\uparrow$} & \multicolumn{2}{c}{3D PCK@30mm$\uparrow$} \\ 
		\hline
		& left & right & left & right & left & right \\
		\hline
		\hline
		Hasson et al.~\cite{hasson_dense_optical_flow} & 39.56 & 41.87 & - & - & - & - \\
		Tekin et al.~\cite{tekin_keypoints_rgb} & 41.32 & 38.86 & - & - & - & - \\
		Kwon et al.~\cite{h2o} & 41.45 & 37.21 & - & - & - & - \\
		Aboukhandra et al.~\cite{aboukhadra_2023} & 36.80 & 36.50 & - & - & - & - \\
		Cho et al.~\cite{cho_2023} & 24.40 & 25.80 & - & - & - & - \\
		Wen et al.~\cite{workshop_wen}\textbf{*},~\cite{wen_2023} & 35.02 & 35.63 & 12.67 & 2.98 & 43.71 & 37.12 \\
		\hline
		Cho et al.~\cite{workshop_cho}\textbf{*} & 14.40 & 15.90 & 70.75 & 54.61 & 93.81 & 95.08 \\
		Luo et al.~\cite{workshop_luo}\textbf{*} & 20.80 & 24.70 & 40.77 & 32.29 & 80.36 & 73.56 \\
		\hline
		\textbf{Ours (V1)} & \textbf{20.47} & \textbf{19.07} &  \textbf{21.04} &  \textbf{27.81} & \textbf{92.81} & \textbf{94.73} \\
		\textbf{Ours (V3)} & 28.62 & 28.42 & 12.95 & 16.64 & 81.61 & 86.15 \\
	\end{tabular}
	\caption{Results on the H2O~\cite{h2o} hand pose challenge.  For each metric, we indicate whether higher results ($\uparrow$) or lower results ($\downarrow$) are better. The best results among accepted conference publications are highlighted in bold. For completeness, we also list workshop contributions, which are tailored towards the H2O challenge, and denote them with \textbf{*}.}.
	\label{table:quantitative}
\end{table*}

\subsection{Comparison with State-of-the-art} Since there is no established evaluation method for \textit{dense} pose and \textit{shape} estimation with \textit{uncertainty} estimation in \textit{clinical} applications, we compare our method with the SOTA on pose estimation. Therefore, we evaluate the accuracy of ShaRPy on the H2O~\cite{h2o} dataset, which contains hand motion sequences of healthy subjects most visually close to a clinical setting. In \autoref{fig:eval}, we compare the qualitative results of V2 with OpenPose~\cite{openpose_full,openpose_hand}. For a quantitative comparison, our results are submitted and objectively evaluated on a public leaderboard. %
The benchmark is tailored to RGB keypoint-based methods and evaluates the plausibility of poses in the presence of strong occlusions. \autoref{table:quantitative} summarizes the results with respect to the MEPE and the PCK. In summary, ShaRPy places first or third on the leaderboard, even though we did not design our system specifically for a keypoint-based pose estimation challenge, do not focus on plausibility, and, solve a more challenging problem of indirectly estimating the pose through shape along with the shape itself. On top of that, we show the generalization ability of our version V3, which outperforms most methods by placing third.

\section{Conclusion} In this work, we proposed the first markerless hand tracking approach, which calculates uncertainty in the pose estimates. Our approach combines a data-driven dense correspondence predictor with a flexible, generative energy minimization framework to estimate the optimal hand pose and shape that best explains the given observations.
Further, we detect uncertain poses through the detection of unobserved and error-prone surface segments. We demonstrate through quantitative and qualitative results that our approach provides outstanding pose estimation accuracy, on top of its generalization to both, unknown datasets of healthy individuals and patient data. Furthermore, we provide results of clinical hand function assessments and show that, compared to other markerless approaches, our approach has no limitation in terms of its applicability and, instead, includes more favorable properties such as additional shape estimation and the robust filtering of uncertain poses. We believe our approach can be used to drive further research in the context of markerless tracking in clinical applications. 
\newline



