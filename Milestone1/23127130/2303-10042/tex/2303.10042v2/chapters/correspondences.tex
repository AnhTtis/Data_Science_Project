
\section{Dense Correspondence with Semantic Encodings}
\label{section:dense}


Our goal is to fit the MANO model such that it best describes the observations in an RGB-D image. To this end, we establish correspondences between a pixel $(x,y)$ and a vertex $\boldsymbol v \in \mathcal{V}$ through a novel, shared canonical correspondence space embedded in $[0,1]^3$. For this, we define the function $\boldsymbol c\colon\mathcal{V}\rightarrow [0,1]^3$, which maps $\boldsymbol v$ to its coordinate in the correspondence space. As depicted in \autoref{fig:mano}, the space is encoded into a Hue-Saturation-Value (HSV) color cylinder wrapped around the flat rest pose of the model, 
aligned such that the axes describe semantic features of the hand. 
The hue describes the angle of a vertex in a circle within the coronal plane and encodes the finger type.
We scale the range of $[0^{\circ},360^{\circ})$ to lie within the extent of the MANO model to ensure space compactness.
This is important to distinguish between different fingers as small differences in values can lead to wrong assignments during the correspondence prediction and matching (see \autoref{sec:matching}).
The saturation is computed on each finger separately and encodes the corresponding finger segment on an axis between the origin of the hand wrist and the fingertip. To distinguish between the front and back of the hand, the value axis encodes the surface normal along the y-axis. In summary, our novel correspondence space encodes both, spatial and semantic hand features while being compact, continuous, and deterministic to compute. The semantic encoding enables us to define a function $d\colon[0,1]^3\rightarrow\{1,...,20\}$ that computes a discrete segmentation label out of the continuous space, which is later used in \autoref{sec:matching} and \autoref{sec:uncertainty}. \autoref{fig:mano} shows the corresponding segmented vertex sets $\mathcal{S}_{3d} = \{S^i_{3d}\}_{i=1}^{20}$ with $S^i_{3d} = \{\boldsymbol v \in \mathcal{V} \mid d(\boldsymbol c(\boldsymbol v)) = i \}$, of which 15 refer to the three segments of each finger, and the remaining divide the large area of the wrist into 5 per-finger regions. 



\subsection{Correspondence Regression}
\label{sec:regression}
As depth-only datasets are limited in availability and generalization across depth images of different sensor types is challenging, we leverage a variety of RGB(-D) datasets~\cite{interhand,freihand,h2o,honnotate,rgb_hampali} to train our correspondence regression network only on RGB data in a fully supervised manner, and leverage the additional depth component only at test-time during energy minimization. We use a mixture of automatically and semi-automatically labeled ground-truth MANO parameters to transform the models to their position in the image and render the parts of the visible surface to obtain ground-truth correspondence images $\boldsymbol F$. In order to detect inconsistent per-pixel predictions of $\boldsymbol F$ at inference time and relate them to certain regions of the hand, an additional segmentation map of the visible parts of the hand is required. As our novel correspondence space enables the direct conversion from unique coordinates to coarse hand segments, it is not necessary to predict an additional segmentation mask, which could potentially lead to inconsistent per-pixel predictions with $\boldsymbol F$ otherwise.
Instead, for each hand visible in an image $\boldsymbol I$, our framework only predicts dense correspondences, of which we compute a segmentation set $S^i_{2d} = \{ (x,y) \mid d(\boldsymbol F(x,y)) = i \}$ of pixels $(x,y)$ for each segmentation label $i$.

Our regression network is an extension of Yolact~\cite{yolact} to which we add an additional branch for correspondence prediction. It is trained by minimizing the smooth L1 loss between the predicted and ground-truth correspondence value of each pixel within the ground-truth segmentation mask of the hand. At inference time, we multiply the correspondence values with the predicted mask $\boldsymbol M$ to acquire per-pixel correspondences only for the hand. 

\subsection{Correspondence Matching}
\label{sec:matching}
Correspondence pairs are established by comparing each predicted $\boldsymbol c_p = \boldsymbol F(x,y)$ at pixel $(x,y)$ with $\boldsymbol c_v = \boldsymbol c(\boldsymbol v)$ of every MANO vertex $\boldsymbol v$. 
A common method to find a match is a traditional nearest-neighbor search~\cite{depth_mueller}.
In particular for hands, this method can result in wrong correspondence pairs at positions in between fingers. This is because, contrary to the continuous nature of the correspondence space, the assignment of a pixel to a vertex of a specific finger is a discrete problem.
We improve the quality of correspondence pairs by using both, the correspondence space and its discrete segmentation, and compute nearest-neighbor matches only within the sets of segmented vertices $\mathcal{S}^i_{3d}$ and segmented pixels $S^i_{2d}$ that share the same segmentation label. In other words, we first reject possible matches on the coarse segmentation level in case they do not share the same label and, subsequently, compute matches in the correspondence space. A match between $\boldsymbol c_p$ and $ \boldsymbol c_v$ is used to construct a pair $(\boldsymbol p,\boldsymbol v)$ of 3D correspondences between $\boldsymbol v$ and an image point $\boldsymbol p \in \mathbb{R}^3$, computed from the back-projection of the depth value at $\boldsymbol D(x,y)$. 
Since $\boldsymbol c_p$ is predicted in the view of the RGB camera, it is not exactly aligned with the pixel positions of $\boldsymbol D$. 
Particularly at the edges of the hand silhouette, the predictions can map to erroneous points of the background. Hence, we first discard pairs, in which $\boldsymbol D(x,y)$ deviates too far from the median depth of the hand, determined by a threshold $t_d$. 
Second, we filter out points at silhouette edges with degraded and noisy depth by inspecting whether the angle of the point-wise normal computed from $\boldsymbol D$ exceeds a given threshold $t_n$. 
Lastly, we discard all pairs $(\boldsymbol p,\boldsymbol v)$, of which the Euclidean norm of their difference exceeds the 3D distance threshold $t_{3d}$. The final 3D correspondence set is denoted by $\mathcal{C}_{3d}$.






