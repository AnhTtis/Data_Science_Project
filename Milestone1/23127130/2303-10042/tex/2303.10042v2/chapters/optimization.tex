
\section{Pose and Shape Tracking}
\label{section:optim}
In this stage, we solve an energy-minimization problem to obtain the optimal MANO parameter set $\Omega^k = (R^k,t^k,\theta^k,\beta^k)$ at timestep $k$:
\begin{equation*}
\arg \min_{\Omega^k} \left[   \omega_{3d} \lambda E_{3d}(\mathcal{C}_{3d}) + \omega_{2d} E_{2d}(\mathcal{C}_{2d}) + E_{reg}(\Omega^k, \Omega^{k-1}) \right]
\end{equation*}
We denote the respective weights of a term $E_*$ as $\omega_*$ and define $\lambda = \exp{(J+1)}$, where $J$ is the Jaccard index of the predicted mask $\boldsymbol M$ and the mask $\boldsymbol M_v$ of the rasterized MANO model. We generate $\boldsymbol M_v$ by using the differentiable rasterizer Nvdiffrast~\cite{nvdiffrast}. $E_{3d}$ and $E_{reg}$ are similar to Mueller et al.~\cite{depth_mueller}: The data term $E_{3d}$ consists of a point-to-point and point-to-plane error. The regularization term $E_{reg}$ enforces plausible poses and shapes, as well as temporal smoothness, and consists of $E_{shape}$, $E_{pose}$, and $E_{temp}$. In contrast to~\cite{depth_mueller}, we use the anatomically rephrased orientations of the MANO model such that $E_{pose}$ enforces poses within anatomical limits.
Furthermore, we introduce the term $E_{2d}$ defined on the set of valid pixels $\mathcal{C}_{2d}$ within $\boldsymbol M$ and $\boldsymbol M_v$. For each pixel $(x,y)\in C_{2d}$, the term penalizes the squared L2 norm between $\boldsymbol F(x,y)$ and $\boldsymbol F_v(x,y)$, where $F_v$ is the correspondence image of the rasterized hand. %
In other words, $E_{2d}$ enforces the MANO model to lie within the predicted hand silhouette and provides a more accurate estimation of $\boldsymbol \beta$ compared to $E_{3d}$. 
In our energy minimization framework, we distinguish between the \textit{Initialization} phase, which is only executed in the first frame or when the tracking is lost, and the \textit{Refinement} phase, in which we iteratively minimize $E$. During initialization, we first solve the orthogonal Procrustes problem to obtain the initial wrist parameters $\boldsymbol R$ and $\boldsymbol t$. Secondly, we make use of an implicit pose prior to initialize $\boldsymbol \theta$ with plausible parameters. 
For this purpose, we transform the anatomically rephrased $\boldsymbol \theta$ into a PCA space, which we pre-compute from annotated RGB(-D) datasets~\cite{interhand,freihand,h2o,honnotate,rgb_hampali}. 
Then, we solve the energy formulation with respect to the PCA pose parameters in order to obtain a plausible initialization of $\boldsymbol \theta$. As the PCA pose space is not expressive enough to capture the high variance of different hand poses, we refine $\boldsymbol \theta$ in the subsequent Refinement stage.
\newline
\newline
\section{Uncertainty Estimation} 
\label{sec:uncertainty}


\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{images/inconsistencies.pdf}%
\caption{Correspondence predictions (P) on images from H2O~\cite{h2o} compared with their ground-truth (GT). Our network was trained on HO3D~\cite{honnotate}, InterHand2.6M~\cite{interhand}, $\text{H}_2$O-3D~\cite{rgb_hampali} and FreiHAND~\cite{freihand}. Inconsistencies in the regressed coordinates are highlighted in red.}%
\label{fig:inconsistencies}
\end{figure}



As mentioned in \autoref{section:intro}, the generalization capability of data-driven pose and shape estimation approaches is limited with respect to inputs that do not lie within the learned data distribution, e.g., unseen hand poses or viewpoints. 
Our approach poses no exception to this general limitation and we observe correspondence mispredictions that exhibit inconsistencies in the anatomic structure of the hand, which is encoded by the correspondence space. These inconsistencies are not only noticeable visually (see \autoref{fig:inconsistencies}) but also during energy minimization. More specifically, we experience high residuals in regions, where it is not possible to optimize the parameters of the anatomically constrained MANO model such that its surface is optimal with respect to the position in the image given by the pixels of the correspondence pairs. Correspondence coordinates with a significant deviation from their actual position in the space are assigned to a wrong segmentation label through the discretization of $d(\cdot)$. Hence, hand segments can either be over-saturated with mispredicted correspondences or have no correspondences at all despite being visible in the input image, as depicted in \autoref{fig:inconsistencies}.
Based on these observations, we obtain an uncertainty value $u_i$ for each segment $i$ on the surface of the MANO model given by the segmentation sets $S^i_{3d}$, which are computed from the predicted correspondence image $\boldsymbol F$. We compute the uncertainty value such that:\begin{equation*}
     u_i =  \begin{cases}
       1 & \text{if segment $i$ unobserved or error-prone} \\
       0 & \text{else}
    \end{cases}
\end{equation*}
Since a segment relates to the set of vertices deformed by a particular joint, we can directly infer uncertainty with respect to its respective pose parameter.
We consider a segment $i$ as unobserved if:
\begin{equation*}
 \frac{\lvert \mathcal{V}^i_{vis} \rvert}{\lvert S^i_{3d} \rvert} < \tau_{v},\quad\text{with}\quad\mathcal{V}^i_{vis} = \{v \in \mathcal{S}^i_{3d} \mid (*,v) \in \mathcal{C}_{3d}\}
\end{equation*}
Further, we consider a segment $i$ as error-prone if:
\begin{equation*}
 \frac{\lvert \mathcal{P}_{2d} \rvert}{\lvert S^i_{2d} \rvert} > \tau_{2d}\quad\text{or}\quad\frac{\lvert \mathcal{P}_{3d} \rvert}{\lvert S^i_{3d} \rvert} > \tau_{3d} 
\end{equation*}
We define $\mathcal{P}^i_{2d} = \{ (x,y) \in \mathcal{S}^i_{2d} \mid (x,y) \in \mathcal{C}_{2d}\wedge E_{2d}(x,y) > \varepsilon_{2d}\}$ as the set of error-prone pixels and, analogously, $\mathcal{P}^i_{3d} = \{ \boldsymbol v \in S^i_{3d} \mid (*,\boldsymbol v) \in C_{3d}\wedge E_{z}(\boldsymbol v) > \varepsilon_{3d}\}$ as the set of error-prone vertices. The term $E_{z}(\boldsymbol v)$ is defined as the average L1 loss between the z-axis values of all pairs in $\mathcal{C}_{3d}$, in which $\boldsymbol v$ is included.







