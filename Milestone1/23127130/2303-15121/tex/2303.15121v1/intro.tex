%------------------
% Introduction
%------------------
\section{Introduction} \label{sec:intro}
We consider the problem of finite-time identification of a linear dynamical system (LDS) of the form
%
%
\begin{equation} \label{eq:lin_dyn_sys_mod}
    x_{t+1} = A^* x_t + \eta_{t+1} \quad \text{ for } t=0,1,\dots,T \quad \text{ and } x_0 = 0,
\end{equation}
%
where $A^* \in \matR^{n \times n}$ is the unknown system  matrix to be estimated, $x_t \in \matR^n$ is the observed state at time $t$, and $\eta_t \in \matR^n$ is the unobserved (random) process noise.  Such problems arise in many areas such as control theory, reinforcement learning and time-series analysis to name a few. An important line of research in recent years has focused on theoretically analyzing the performance of the ordinary least squares (OLS) estimator, by deriving non-asymptotic error bounds for the estimation of $A^*$ (e.g., \cite{FaraUnstable18,Simchowitz18a, Sarkar19, Jedra20}), holding with high probability provided $T$ is sufficiently large.
The analyses  depends crucially on the spectrum of $A^*$ -- in particular on the spectral radius of $A^*$, namely $\rho(A^*)$. 

The focus of this paper is the strictly stable setting where $\rho(A^*) < 1$. 
Denoting $\Gamma_s(A) = \sum_{k=0}^s A^k (A^k)^{\top}$ for $s \geq 0$ to be the controllability Grammian of the system, and $\lambda_{\min}(\cdot)$ to be the smallest eigenvalue of a symmetric matrix, it was shown recently \cite{Jedra20} that the OLS estimate $\est{A}$ satisfies with probability at least $1-\delta$ 
%
\begin{equation*}
\norm{\est{A} - A^*}_2 \lesssim \sqrt{\frac{\log(1/\delta) + n}{\lambda_{\min}(\sum_{s=0}^{T-1} \Gamma_s(A^*))}},
\end{equation*}
%
provided $\lambda_{\min}(\sum_{s=0}^{T-1} \Gamma_s(A^*)) \gtrsim J^2(A^*)(\log(1/\delta) + n)$. Here $\norm{\cdot}_2$ denotes the spectral norm and $(\eta_t)_{t \geq 1}$ are considered to be i.i.d subgaussian vectors -- see Section \ref{prob_setup} for a description of notations. The quantity $J(A^*)$ is defined in \eqref{eq:stab_param} and is finite when $\rho(A^*) < 1$; it is moreover bounded by a constant if $\norm{A^*}_2 < 1$ is a constant. Since $\lambda_{\min}(\sum_{s=0}^{T-1} \Gamma_s(A^*)) \geq T$, we can rewrite the above bound as  
%
\begin{equation} \label{eq:err_bd_jedra}
    \norm{\est{A} - A^*}_2 \lesssim \sqrt{\frac{\log(1/\delta) + n}{T}} \quad \text{ if } \quad T \gtrsim  J^2(A^*)(\log(1/\delta) + n).
\end{equation}
%
In other words,  a meaningful error bound is ensured provided the length of the trajectory is at least of the order of the dimension $n$. Furthermore, this bound is also optimal in terms of dependence on $\delta, n$ and $T$ \cite{Simchowitz18a}.  

It is natural to consider the scenario where additional structural information is available regarding $A^*$ -- in this case one would expect that incorporating such information in the estimation procedure should lead to an improved performance compared to the vanilla OLS estimator. 
In many cases of interest, $A^*$ actually has an intrinsically low dimensional structure and it is possible to capture this structural information of $A^*$ through a known convex set $\calK$ containing $A^*$. Computationally, the estimate $\est{A}$ is then obtained by the penalized least squares estimator \eqref{eq:Aest_convex}, which is also a convex program that can typically be solved efficiently in practice. From a statistical perspective, one would expect to be able to improve the error bounds in \eqref{eq:err_bd_jedra} in terms of the dependence on the (extrinsic) dimension $n$. Two examples of such $\calK$ -- which will also be used later for instantiating our more general result -- are outlined below.
%
\begin{enumerate}
\item ({\bf Example 1}) $\calK$ is a $d$-dimensional subspace of $\matR^{n \times n}$ for some $d \leq n^2$.

\item ({\bf Example 2}) If $A^*$ is $k$-sparse, i.e., has $k$ non-zero entries, then one can choose $\calK$ to be a suitably scaled $\ell_1$ ball such that $A^* \in \calK$. It is well known in the statistics and signal processing literature that the resulting estimator -- known as the LASSO -- promotes solutions which are sparse (see for e.g. \cite{candes09,neghabhan12,candes06}). 
\end{enumerate}
%
For the above examples, the intrinsic dimension of $A^*$ is essentially captured by the quantities $d$ or $k$, and we expect that the  error bounds in \eqref{eq:err_bd_jedra} should improve in terms of exhibiting a milder dependence on $n$. In particular, when $d, k \ll n^2$, we expect the estimation error for $A^*$ to be small for moderately large values of $T$.

\subsection{Our contributions} 
For the setting where $\rho(A^*) < 1$ and $A^* \in \calK$, we derive non-asymptotic bounds on the estimation error in the Frobenius norm $\norm{\est{A} - A^*}_F$ for the estimator \eqref{eq:Aest_convex}, holding with high probability; see Theorem \ref{thm:main_err_tangent_cone} for the full statement. Our bound depends on the local size of the tangent cone of $\calK$ at $A^*$, captured via Talagrand's $\gamma_1, \gamma_2$ functionals \cite{talagrand2014upper} (see Definitions \ref{def:gamma_fun} and \ref{def:tancone} in Section \ref{prob_setup}). Upon instantiating our bounds for the aforementioned choices of $\calK$, we obtain the following corollaries.
%
\begin{enumerate}
    \item ({\bf Example 1}) In this case, we have (see Corollary \ref{cor:subspace}) with probability at least $1-\delta$, 
    %
    \begin{equation} \label{eq:intro_subsp_bd_corr}
      \norm{\est{A} - A^*}_F \lesssim   J(A^*)\left(\frac{\log(1/\delta) + \sqrt{d}}{\sqrt{T}} \right) \quad \text{if} \quad T \gtrsim J^4(A^*) \max\set{d, \log^2(1/\delta)} .
    \end{equation}
   % 
   Suppose for simplicity that $\norm{A^*}_2 < 1$ so that $J(A^*)$ is a constant. If $d = n^2$, we obtain the rate $\frac{n}{\sqrt{T}}$ which matches that obtained from \eqref{eq:err_bd_jedra}  using the standard inequality $\norm{\est{A} - A^*}_F \leq \sqrt{n} \norm{\est{A} - A^*}_2$. Moreover, we would also need $T \gtrsim n^2$ in \eqref{eq:err_bd_jedra} in order to drive $\norm{\est{A} - A^*}_F$ below a specified threshold. For general $d$, however, we show the sample complexity of estimating $A^*$ to be of order $d$ which is relevant when $d \ll n^2$.

    \item ({\bf Example 2})  In this case we obtain Corollary \ref{cor:sparse_example} which is best interpreted for specific regimes of the sparsity level $k$. For instance, if $k$ is of the order $n$, we show that 
    %
      \begin{equation*}
       \norm{\est{A} - A^*}_F \lesssim J(A^*) \left(\frac{\log(1/\delta) + \sqrt{n \log n}}{\sqrt{T}} + \frac{(n \log n)^{3/2}}{T} \right) 
   \end{equation*}
   %    
   if $T \gtrsim J^4(A^*) \max\set{n \log n, \log^2(1/\delta)}$. Assuming $J(A^*)$ is constant, note that we actually need $T \gtrsim (n \log n)^{3/2}$ to drive $\norm{\est{A} - A^*}_F$ below a specified threshold, however, this is still much milder than what we need in general.
\end{enumerate}

%----------------
% Related work
%----------------
\subsection{Related work}
\paragraph{Learning unstructured LDS.} A line of recent work has focused on deriving non-asymptotic error bounds for learning linear systems of the form \eqref{eq:lin_dyn_sys_mod}, without any explicit structural assumption on $A^*$. The majority of these works analyze the OLS under different assumptions on $\rho(A^*)$, namely: strict stability ($\rho(A^*) < 1$) \cite{Jedra20,FaraUnstable18}; marginal stability ($\rho(A^*) \leq 1$) \cite{Simchowitz18a, Sarkar19}; purely explosive systems ($\rho(A^*) > 1$) \cite{FaraUnstable18,Sarkar19}. While $\est{A}$ is known in closed form, the main challenge in the analysis comes from handling the interaction between the matrix of covariates $x_t$, and that of noise terms $\eta_t$ due to their dependencies. Common techniques used in the analysis involve concentration results for self normalized processes \cite{selfnormbook, abbasi11}, and Mendelson's ``small-ball'' method \cite{pmlr-v35-mendelson14}, the latter of which was extended to dependent data in \cite{Simchowitz18a} leading to sharper error bounds. When $\rho(A^*) \leq 1$, the authors in \cite{Simchowitz18a} interpret the quantity $\lambda_{\min}(\Gamma_{T-1})$ as a measure of the signal-noise-ratio \cite{Simchowitz18a} -- larger values  lead to improved error bounds. As mentioned earlier, the results of \cite{Jedra20} depend on a similar quantity, namely 
$\lambda_{\min}(\sum_{s=0}^{T-1} \Gamma_{s})$, which plays a key role in their error bounds. These terms do not appear explicitly within our analysis and it is unclear (albeit interesting) how this can be done. The main tools that we employ involve concentration results for the suprema of second-order subgaussian chaos processes indexed by a set of matrices \cite{krahmer14, dirksen15}; see Section \ref{subsec:tools} for details. 

\paragraph{Learning structured LDS.} Relatively fewer works have considered the setup where $A^*$ possesses additional structure. In \cite{Fattahi2019}, a more general version of \eqref{eq:lin_dyn_sys_mod} was considered where $x_{t+1} = A^* x_t + B^* u_t + \eta_t$, with $B \in \matR^{n \times m}$ and $u_t \in \matR^{m}$ denoting the inputs. Assuming the unknown $A, B$ to be $k$-sparse, and $u_t = K_0 x_t + v_t$ where $v_t$ is random with a user specified distribution ($K_0$ is a feedback controller), a LASSO type estimator was analyzed. Assuming $x_0$ rests at its stationary distribution, uniform asymtotic stability of the closed-loop system, and certain technical assumptions involving $A^*, B^*$ and $K_0$, entry-wise error bounds were obtained for the estimation of $A^*, B^*$. It was shown that these bounds can sometimes be obtained with $T$ of the order $k^2 \log (n + m)$. If $k$ is of order $n$, this means that $T \gtrsim n^2 \log (n+m)$ samples are needed for recovering the \emph{support} of $A^*, B^*$. This is larger than our sample complexity bound for controlling the Frobenius norm error. In \cite{Pereira10}, the model \eqref{eq:lin_dyn_sys_mod} was considered with $A^*$ assumed to be $k$-sparse and strictly stable. Under certain assumptions on the problem parameters, it was shown for a LASSO-type estimator that the support of $A^*$ is recovered exactly provided $T \gtrsim \text{poly}(k) \cdot \log n$. The results in \cite{zheng2021finite} are applicable to model \eqref{eq:lin_dyn_sys_mod}, with additional linear information about $A^*$ assumed to be available. This can be reformulated as saying that for a known $d$-dimensional basis $\set{V_i}_{i=1}^d \subset \matR^{n \times n}$ and a known offset $\bar{V} \in \matR^{n \times n}$, we have $A^* - \bar{V} \in \text{span}\set{V_i}_{i=1}^d$. This is identical to Example $1$. If $\rho(A^*) < 1$ and $\norm{A^*}_2 \leq C$ for some constant $C > 0$, they show that $\norm{A^* - \est{A}}_F \lesssim \sqrt{\frac{d\log(d/\delta)}{T}}$ provided the smallest singular value of $A^*$ is sufficiently smaller than $1$.  This is similar to our bound in \eqref{eq:intro_subsp_bd_corr}. They also cover the setting $\rho(A^*) \leq 1$ where the analysis uses the small ball method \cite{pmlr-v35-mendelson14}. 

\paragraph{Learning structured signals from random linear measurements.} Consider the relatively easier setting of linear regression with independent covariates and noise, i.e., $y = X \beta^* + \eta$ where $X \in \matR^{m \times n}$ is the matrix of covariates, $\beta^* \in \matR^n$ is the unknown signal,  $\eta$ is noise, and the entries of $X$ and $\eta$ are assumed (for simplicity) to be centered, independent Gaussian's. The problem of recovering $\beta^*$ -- assuming it belongs to a convex set $\calK \subseteq \matR^n$ -- has received significant interest over the past decade from the statistical and signal processing communities. It is now known that the efficient recovery of $\beta^*$ is possible via convex programs (e.g., penalizing least squares with constraint $\calK$) with the sample complexity $m$ depending on the Gaussian width of the local tangent cone of $\calK$ at $\beta^*$; see for e.g., \cite{neykov19b, rudelson08, chandra12, Tropp2015, planlasso16} and also \cite{lotz14} who introduced a related notion of `statistical dimension'. For some sets $\calK$ (such as the $\ell_1$ ball), sharp estimates for the Gaussian width are available through tools such as Gordon's escape through the mesh theorem \cite{Gordon88}, that leads to tight sample complexity bounds. While our proof technique is similar in spirit to these papers (in particular \cite{neykov19b}, the model in \eqref{eq:lin_dyn_sys_mod} leads to additional technical difficulties. For instance, we cannot use Gordon's theorem anymore and require other concentration tools for the underlying second order subgaussian chaos. To our knowledge, existing works for finite time identification of \eqref{eq:lin_dyn_sys_mod} do not provide bounds for general convex bodies $\calK$; our main goal is to fill this gap (to an extent) by drawing ideas from the above literature.