%-------------------
% Problem setup
%-------------------
\section{Problem setup and results} \label{prob_setup}
%Define the problem setup: notation, model, goal etc. State main result(s).
\paragraph{Notation.} For any vector $x \in \matR^n$, $\norm{x}_p$ denotes the usual $\ell_p$ norm of $x$. For $X \in \matR^{n \times m}$, we denote $\norm{X}_2$, $\norm{X}_F$ to be respectively the spectral and Frobenius norms of $X$, while $\dotprod{X}{Y} = \Tr(X^\top Y)$ denotes the inner product between $X$ and $Y$. Also, $\vect(X) \in \matR^{nm}$ is formed by stacking the columns of $X$ and $\norm{X}_{1,1} = \norm{\vect(X)}_1$ denotes the entry-wise $\ell_1$ norm of $X$. The symbol $\otimes$ denotes the Kronecker product between matrices. Sets will be usually denoted by calligraphic letters, and their cardinalities by $\abs{\cdot}$. For $a,b > 0$, we say $a \lesssim b$ if there exists a constant $C > 0$ such that $a \leq C b$. If $a \lesssim b$ and $a \gtrsim b$, then we write $a \asymp b$. For $n \times n$ matrices, we denote the unit Frobenius sphere by $\frobsphere$, the unit Frobenius ball by $\frobball$, and the identity matrix by $I_n$. The values of symbols used for denoting constants (e.g., $c, C, c_1$ etc.) may change from line to line. Finally, recall that the subgaussian norm of a random variable $X$ is given by 
$\norm{X}_{\psi_2}:= \sup_{p \geq 1} p^{-1/2} (\expec \abs{X}^p)^{1/p}$, see for e.g. \cite{vershynin_2012}. We say $X$ is $L$-subgaussian if $\norm{X}_{\psi_2} \leq L$.

%-----------
% Setup
%-----------
\subsection{Setup}
Consider the autonomous linear dynamical system in \eqref{eq:lin_dyn_sys_mod} where $(\eta_t)_{t \geq 1}$ are assumed to be zero-mean, independent and identically distributed (i.i.d) random variables for $t=0,\dots,T$. Specifically, $\eta_t$ is assumed to have i.i.d $L$-subgaussian entries (for some constant $L$), each of unit variance. Given $(x_t)_{t=0}^{T+1}$, our goal is to estimate $A^*$ in a suitable norm under the constraint that $A^* \in \calK$ for a closed convex set $\calK \subseteq \matR^{n \times n}$. We focus on the penalized least squares estimator  
%
\begin{equation} \label{eq:Aest_convex}
 \est{A} \in \argmin{A \in \calK} \sum_{t=0}^{T} \norm{ x_{t+1} - A x_t}_2^2.
\end{equation}
%
Note that \eqref{eq:Aest_convex} is a convex program which can usually be efficiently solved by interior-point methods, and in many cases by more efficient methods (e.g., projected gradient descent) specialized to the structure of $\calK$. Also, if $\eta_t$ were i.i.d Gaussian's, then \eqref{eq:Aest_convex} would simply correspond to the maximum likelihood estimator (MLE) of $A^*$.

In our analysis, we will assume $A^*$ is strictly stable, i.e., its spectral radius $\rho(A^*) < 1$. The quantity $J(A^*)$ defined as 
%
\begin{equation} \label{eq:stab_param}
  J(A^*) := \sum_{i=0}^{\infty} \norm{(A^*)^i}_2   
\end{equation}
%
was introduced in \cite{Jedra20} for the analysis of the OLS estimator for strictly stable linear dynamical systems, and will also appear in our results. It is not difficult to verify that $J(A^*)$ is bounded if $\rho(A^*) < 1$, although it could grow with $n$. If $\norm{A^*}_2 < 1$ then $J(A^*) \leq \frac{\norm{A^*}_2}{1-\norm{A^*}_2}$.
%
%
%\hemant{(Actually, this needs to be checked in more detail -- how to bound $J(A^*)$ in terms of $\rho(A^*)$ if $A^*$ is stable?)} \denis{Using Jordan normal form theorem: there is a non-singular matrix $P\in\mathbb{R}^{n\times n}$ such that $A^*=P^{-1}J^* P$, where $J^*$ is a block-diagonal matrix having on the main diagonal real Jordan blocks. Hence, $(A^*)^i=P^{-1}(J^*)^i P$ and $\norm{(A^*)^i}_2\leq\norm{P^{-1}}_2\norm{(J^*)^i}_2 \norm{P}_2\leq\norm{P^{-1}}_2\norm{P}_2 \rho(A^*)^i$, which allows us to get a converging upper bound for $J(A^*)$.}
%\hemant{I think there are two problems with the above argument. The first is that $\norm{P^{-1}}_2$, $\norm{P}_2$ can grow with $n$, even as fast as polynomial with $n$, which makes the error bounds appearing later quite bad; note that the dependence on $n$ is crucial in what we say later. Also, I am not sure if $\norm{J^*}_2 \leq \rho(A^*)$ is true? See for e.g., this link: \url{https://math.stackexchange.com/questions/853143/2-norm-of-a-canonical-jordan-form-and-spectral-radius}} 

Before stating our results, we need to present some definitions which will be used later on. 

%
% Preliminaries
%
\subsection{Preliminaries}
We begin by recalling Talagrand's $\gamma_{\alpha}$ functionals \cite{talagrand2014upper} which can be thought of as a measure of the complexity of a (not necessarily convex) set.
%
%
\begin{definition}[\cite{talagrand2014upper}] \label{def:gamma_fun}
Let $(\calS,d)$ be a metric space. We say that a sequence of subsets of $\calS$, namely $(\calS_r)_{r \geq 0}$ is an admissible sequence if $\abs{\calS_0} = 1$ and $\abs{\calS_r} \leq 2^{2^{r}}$ for every $r \geq 1$. Then for any $0 < \alpha < \infty$, the $\gamma_{\alpha}$ functional of $(\calS, d)$ is defined as 
%
\begin{equation*}
    \gamma_{\alpha}(\calS, d) := \inf \sup_{s \in \calS} \sum_{r=0}^{\infty} 2^{r/\alpha} d(s, \calS_r)
\end{equation*}
%
with the infimum being taken over all admissible sequences of $\calS$. %\denis{Explain the notation $|\mathcal{S}_r|$?} \hemant{Added in notations paragraph.}
\end{definition}
%
It can be verified that for any two metrics $d_1, d_2$ such that $d_1 \leq a d_2$ for some $a > 0$,  it holds
%\footnote{See for instance \cite[Exercise 2.2.20]{talagrand2014upper} for $\alpha = 2$, the arguments however extend to $\alpha \geq 1$.} 
that $\gamma_{\alpha}(\calS, d_1) \leq a \gamma_{\alpha}(\calS, d_2)$. Furthermore for $\calS' \subset \calS$, we have\footnote{This holds for the $\gamma_{\alpha}$ functional defined in \cite[Definition 2.2.19]{talagrand2014upper} with $C_{\alpha} = 1$. However, $\gamma_{\alpha}$ as in Definition \ref{def:gamma_fun} is equivalent to that in \cite[Definition 2.2.19]{talagrand2014upper} up to a constant depending only on $\alpha$; see \cite[Section 2.3]{talagrand2014upper}.} that $\gamma_{\alpha}(\calS', d) \leq C_{\alpha} \gamma_{\alpha}(\calS, d)$ for $C_{\alpha} > 0$ depending only on $\alpha$. The $\gamma_{\alpha}$ functionals can be bounded in terms of the covering numbers of the set $\calS$. For any $\epsilon > 0$, denote $\calN(\calS, d, \epsilon)$ to be the minimum number of balls of radius $\epsilon$ (with centers in $\calS$) which are needed to cover $\calS$. Then, one can show\footnote{This can be deduced using  \cite[Corollary $2.3.2$]{talagrand2014upper}, and by replicating the arguments after the proof of \cite[Lemma 2.2.11]{talagrand2014upper} to general $\alpha \geq 1$.} 
that
%
\begin{equation} \label{eq:gamma_bound}
    \gamma_{\alpha}(\calS, d) \leq c_{\alpha} \int_{0}^{\diam(\calS)} \log^{1/\alpha} \calN(\calS, d, \epsilon) d\epsilon
\end{equation}
%
where $c_{\alpha} > 0$ depends only on $\alpha$, and $\diam(\calS)$ is the diameter of $\calS$.  
For $\alpha = 2$, the right-hand side (RHS) of \eqref{eq:gamma_bound} is the well-known Dudley entropy integral \cite{Dudley67}. In fact, by Talagrand's majorizing measure theorem \cite{talagrand2014upper}, $\gamma_2(\calS,d)$ characterizes the expected suprema of centered Gaussian processes $(X_s)_{s \in \calS}$ as
%
\begin{equation} \label{eq:talag_maj_meas_thm}
  c \gamma_2(\calS,d) \leq \expec \sup_{s \in \calS}  X_s \leq C \gamma_2(\calS,d)
\end{equation}
%
for some universal constants $c, C > 0$, with the canonical distance $d(s,s') := (\expec[X_s - X_{s'}]^2)^{1/2}$. For example, if $\calS \subset \matR^{n \times m}$, and $X_s = \dotprod{G}{s}$ for a $n \times m$ matrix $G$ with iid standard Gaussian entries, we have $d(s,s') = \norm{s - s'}_F^2$. Then \eqref{eq:talag_maj_meas_thm} implies $\expec \sup_{s \in \calS} \dotprod{G}{s} \asymp \gamma_2(\calS, \norm{\cdot}_F)$ where $\expec \sup_{s \in \calS} \dotprod{G}{s}$ is known as the \emph{Gaussian width} of the set $\calS$, denoted as $w(\calS)$. %\denis{You use the symbol $\asymp$ just to say that two quantities are of the same order, but defined it for sequences? Explain what does it mean also for not sequences?} \hemant{Yes you are right, I think its better to just define it for two positive numbers $a,b$. Corrected this in notations paragraph. Does it look ok now?}
%
%
%
\paragraph{Tangent cone.}
Our sample complexity bounds for estimating $A^*$ will depend on the local size of the \emph{tangent cone} of the set $\calK$ at $A^*$. 
%
\begin{definition}[Tangent cone] \label{def:tancone}
For a convex set $\calK$  and $A \in \calK$, the tangent cone at $A$ is defined as 
%
\begin{equation*}
    \calT_{\calK,A} := \cl\set{t(B - A): t \geq 0, \ B \in \calK}
\end{equation*}
%
where $\cl(\cdot)$ denotes the closure of a set.
\end{definition}
%
As we will see shortly, our results will involve the gamma functionals $\gamma_1(\calT_{\calK,A^*} \cap \frobsphere, \norm{\cdot})$ and $\gamma_2(\calT_{\calK,A^*} \cap \frobsphere, \norm{\cdot})$ with $\norm{\cdot}$ corresponding to either the spectral or Frobenius norm. Small values of these terms will translate to weaker requirements on the sample size $T$ for accurately estimating $A^*$. By virtue of the earlier discussion, note that $\gamma_2(\calT_{\calK,A^*} \cap \frobsphere, \norm{\cdot}_F) \asymp w(\calT_{\calK,A^*} \cap \frobsphere)$.

%----------------------
% Main results 
%----------------------
\subsection{Main results}
Our main result is the following theorem which bounds the estimation error $\norm{\est{A} - A^*}_F$.  
%
\begin{theorem} \label{thm:main_err_tangent_cone}
There exist constants $C_1,C_2, C_3 > 0$ depending only on $L$ such that for any $\delta \in (0,1)$ and $B\in\calK$, if
%
$$T \geq C_1 J^4(A^*) \max\set{\gamma_2^2(\tanconeB \cap \frobsphere, \norm{\cdot}_2), \log^2(C_2/\delta)},$$
%
then it holds with probability at least $1-\delta$ that 
%
\begin{equation*}
    \norm{\est{A} - A^*}_F \leq C_3  \left[J(A^*)\left(\frac{\log(C_2/\delta) + \gamma_2(\tanconeB \cap \frobsphere, \norm{\cdot}_F)}{\sqrt{T}} + \frac{\gamma_1(\tanconeB \cap \frobsphere, \norm{\cdot}_2)}{T} \right) + J^2(A^*) \norm{A^* - B}_F \right].
\end{equation*}
%
\end{theorem}
%
In the formulation of this theorem a generic matrix $B \in \calK$ is introduced, and a natural choice for it is $B=A^*$, which minimizes the last term in RHS above. However, the shape of $\tanconeB$, which becomes important in evaluation of $\gamma_{\alpha}(\tanconeB \cap \frobsphere, \norm{\cdot}_2)$ for $\alpha=1,2$, may be more suitable for calculation if $B\ne A^*$. 

We now recall the two examples from Section \ref{sec:intro} for which the bounds in Theorem \ref{thm:main_err_tangent_cone} can be made explicit in terms of $n$.

\paragraph{Example $1$ ($d$-dimensional subspace).} In this case, observe from the definition of $\tanconeB$ that $\tanconeB = \calK$ for any choice of $B \in \calK$. One can then use standard covering number bounds to bound the $\gamma_{\alpha}$-functionals in Theorem \ref{thm:main_err_tangent_cone}. This leads to the following corollary of Theorem \ref{thm:main_err_tangent_cone} where we take $B = A^*$.
%
%
\begin{corollary} \label{cor:subspace}
Let $\calK \subset \matR^{n \times n}$ be a $d$-dimensional subspace. If $T \gtrsim J^4(A^*) \max\set{d, \log^2(1/\delta)}$ for $\delta \in (0,1)$, then it holds with probability at least $1-\delta$ that
%
\begin{equation*}
    \norm{\est{A} - A^*}_F \lesssim   J(A^*)\left(\frac{\log(1/\delta) + \sqrt{d}}{\sqrt{T}} \right).
\end{equation*}
%
\end{corollary}
%
In the unconstrained case where $\calK = \matR^{n \times n}$, so that $d = n^2$, \eqref{eq:Aest_convex} is the OLS estimator. Then Corollary \ref{cor:subspace} states that if $T \gtrsim J^4(A^*) \max\set{n^2, \log^2(1/\delta)}$, we have  
%
\begin{equation} \label{eq:unconstr_bd}
    \norm{\est{A} - A^*}_F \lesssim J(A^*)\left(\frac{\log(1/\delta) + n}{\sqrt{T}} \right).
\end{equation}
%
Existing error bounds in the literature for the OLS estimator are typically in the spectral norm, but can of course be converted to the Frobenius norm with an extra factor of $\sqrt{n}$. Indeed, the result of \cite{Jedra20} in \eqref{eq:err_bd_jedra} implies
%Denoting $\Gamma_s(A) = \sum_{k=0}^s A^k (A^k)^{\top}$ for $s \geq 0$, and $\lambda_{\min}(\cdot)$ to be the smallest eigenvalue of a symmetric matrix, they show that w.p at least $1-\delta$, 
%
%\begin{equation*}
%\norm{\est{A} - A^*}_2 \lesssim %\sqrt{\frac{\log(1/\delta) + n}{\lambda_{\min}%(\sum_{s=0}^{T-1} \Gamma_s(A^*))}}
%\end{equation*}
%
%provided $\lambda_{\min}(\sum_{s=0}^{T-1} \Gamma_s(A^*)) \gtrsim J^2(A^*)(\log(1/\delta) + n)$. Moreover, this bound is also optimal up to the dependence on $J(A^*)$. Since $\lambda_{\min}(\sum_{s=0}^{T-1} \Gamma_s(A^*)) \geq T$, this implies
%
\begin{equation} \label{eq:jedra_frob_bd}
  \norm{\est{A} - A^*}_F \leq \sqrt{n} \norm{\est{A} - A^*}_2 \lesssim \sqrt{\frac{n\log(1/\delta) + n^2}{T}}
\end{equation}
%
provided $T \gtrsim J^2(A^*)(\log(1/\delta) + n)$. The bound in \eqref{eq:jedra_frob_bd} is of the same order as in \eqref{eq:unconstr_bd}, barring the extra $J(A^*)$ term in our bound. Moreover, when $J(A^*)$ is a constant, note that $T$ needs to be at least of the order $n^2$ -- in both \eqref{eq:unconstr_bd} and \eqref{eq:jedra_frob_bd} -- in order to drive the error below a specified threshold. Of course, in case $d \ll n^2$, then the requirement $T \gtrsim d$ in Corollary \ref{cor:subspace} is relatively mild, as one would expect, given that $\calK$ has an intrinsic dimension $d$. 

%-------------------------------------------------
% The sparse case, with \ell_1 ball constraint
%
\paragraph{Example $2$ ($\ell_1$ ball).} We now consider the setting where $A^*$ is $k$-sparse, i.e., has at most $k$ non-zero entries for some $1 \leq k \leq n^2$. A standard strategy for recovering sparse matrices (resp. vectors) is to take $\calK$ to be a suitably scaled ball in the $\norm{\cdot}_{1,1}$ (resp. $\ell_1$) norm. We take $\calK := \norm{A^*}_{1,1} \calB_{1,n}$ where
%
\begin{equation} \label{eq:l11_ball}
    \calB_{1,n} := \set{A \in \matR^{n \times n}: \norm{A}_{1,1} \leq 1}
\end{equation}
%
so that $A^*$ lies on the boundary of $\calK$. Then the tangent cone $\tanconeAstar$ has the form
%
\begin{equation*}
    \tanconeAstar := \cl\set{t U: \norm{A^* + U}_{1,1} \leq \norm{A^{\star}}_{1,1}, \ t \geq 0}.
\end{equation*}
%
One can bound $w(\tanconeAstar \cap \frobsphere)$ by using existing results in the literature \cite{rudelson08} (see also \cite{stojnic09,lotz14,chandra12,Tropp2015}) -- these results apply for vectors but can be directly invoked in our setting by treating $n \times n$ matrices as vectors in $R^{n^2}$. This leads to the same order-wise bound on $\gamma_2(\tanconeAstar \cap \frobsphere, \norm{\cdot}_F)$ due to \eqref{eq:talag_maj_meas_thm} with $\calS = \tanconeAstar \cap \frobsphere$. Furthermore, the $\gamma_1$ functional term can be bounded in terms of $w(\tanconeAstar \cap \frobsphere)$ using Sudakov's minoration inequality \cite[Theorem 7.4.1]{HDPbook}. These considerations lead to the following corollary of Theorem \ref{thm:main_err_tangent_cone}.
%
%
\begin{corollary} \label{cor:sparse_example}
    Suppose $A^*$ is $k$-sparse and $\calK = \norm{A^*}_{1,1} \calB_{1,n}$ where $\calB_{1,n}$ is the unit ball defined in \eqref{eq:l11_ball}, and denote $\beta(n,k) := \sqrt{k \log(n^2/k) + k}$. For any $\delta \in (0,1)$, if 
    %
    $$T \gtrsim J^4(A^*) \max\set{\beta^2(n,k), \log^2(1/\delta)}$$
    %
    then with probability at least $1-\delta$ it holds that 
    %
    \begin{equation*}
       \norm{\est{A} - A^*}_F \lesssim J(A^*) \left(\frac{\log(1/\delta) + \beta(n,k)}{\sqrt{T}} + \frac{n \beta(n,k) [1 + \log(\frac{n}{\beta(n,k)})]}{T}\right).
    \end{equation*}
    %
   \end{corollary}
   %
   As a sanity check, note that when $k = n^2$ then $\beta(n,k) = n$ and we recover the statement of Corollary \ref{cor:subspace} with $d = n^2$. This is expected since $A^*$ does not possess any additional structure, and hence the constraint $\calK$ -- the purpose of which is to promote sparse solutions -- does not provide any benefit. The non-trivial sparsity regime is when $k = o(n^2)$. Consider for instance the case where $k \asymp n$. Then, we have $\beta(n,k) \asymp \sqrt{n \log n}$ and Corollary \ref{cor:sparse_example} gives the error bound
   %
   \begin{equation} \label{eq:sparse_rec_bd_1}
       \norm{\est{A} - A^*}_F \lesssim J(A^*) \left(\frac{\log(1/\delta) + \sqrt{n \log n}}{\sqrt{T}} + \frac{(n \log n)^{3/2}}{T} \right)
   \end{equation}
   %
   provided $T \gtrsim J^4(A^*) \max\set{n \log n, \log^2(1/\delta)}$. Notice that while this condition on $T$ implies the error bound in \eqref{eq:sparse_rec_bd_1}, we actually need $T$ to be at least of the order $J(A^*) (n \log n)^{3/2}$ to drive the error in \eqref{eq:sparse_rec_bd_1} below a specified threshold. This ``gap'' is of course due to the term $(n \log n)^{3/2}/T$ in \eqref{eq:sparse_rec_bd_1} -- this term arises from the bound on $\gamma_1(\tanconeAstar \cap \frobsphere, \norm{\cdot}_F)$ within the proof of the corollary and is likely suboptimal. Nevertheless, the bound in \eqref{eq:sparse_rec_bd_1} clearly has a milder dependence on $n$ as compared to that obtained for the OLS in \eqref{eq:unconstr_bd}.