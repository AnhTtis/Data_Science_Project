%------------------
% Introduction
%------------------
\section{Introduction} \label{sec:intro}
We consider the problem of finite-time identification of a linear dynamical system (LDS) of the form
%
%
\begin{equation} \label{eq:lin_dyn_sys_mod}
    x_{t+1} = A^* x_t + \eta_{t+1} \quad \text{ for } t=0,1,\dots,T \quad \text{ and } x_0 = 0,
\end{equation}
%
where $A^* \in \matR^{n \times n}$ is the unknown system  matrix to be estimated, $x_t \in \matR^n$ is the observed state at time $t$, and $\eta_t \in \matR^n$ is the unobserved (random) process noise.  Such problems arise in many areas such as control theory, reinforcement learning and time-series analysis to name a few. An important line of research in recent years has focused on theoretically analyzing the performance of the ordinary least squares (OLS) estimator, by deriving non-asymptotic error bounds for the estimation of $A^*$ (e.g., \cite{FaraUnstable18,Simchowitz18a, Sarkar19, Jedra20}), holding with high probability provided $T$ is sufficiently large.
The analyses  depend crucially on the spectrum of $A^*$ -- in particular on the spectral radius of $A^*$, namely $\rho(A^*)$. 

The focus of this paper is the strictly stable setting where $\rho(A^*) < 1$. 
Denoting $\Gamma_s(A) = \sum_{k=0}^s A^k (A^k)^{\top}$ for $s \geq 0$ to be the controllability Grammian of the system, and $\lambda_{\min}(\cdot)$ to be the smallest eigenvalue of a symmetric matrix, it was shown recently \cite{Jedra20} that the OLS estimate $\est{A}$ satisfies with probability at least $1-\delta$ 
%
\begin{equation*}
\norm{\est{A} - A^*}_2 \lesssim \sqrt{\frac{\log(1/\delta) + n}{\lambda_{\min}(\sum_{s=0}^{T-1} \Gamma_s(A^*))}},
\end{equation*}
%
provided $\lambda_{\min}(\sum_{s=0}^{T-1} \Gamma_s(A^*)) \gtrsim J^2(A^*)(\log(1/\delta) + n)$. Here $\norm{\cdot}_2$ denotes the spectral norm and $(\eta_t)_{t \geq 1}$ are considered to be i.i.d subgaussian vectors -- see Section \ref{prob_setup} for a description of notations. The quantity $J(A^*)$ is defined in \eqref{eq:stab_param} and is finite when $\rho(A^*) < 1$; it is moreover bounded by a constant if $\norm{A^*}_2 < c < 1$ for some constant $c$. Since $\lambda_{\min}(\sum_{s=0}^{T-1} \Gamma_s(A^*)) \geq T$, we can rewrite the above bound as  
%
\begin{equation} \label{eq:err_bd_jedra}
    \norm{\est{A} - A^*}_2 \lesssim \sqrt{\frac{\log(1/\delta) + n}{T}} \quad \text{ if } \quad T \gtrsim  J^2(A^*)(\log(1/\delta) + n).
\end{equation}
%
In other words,  a meaningful error bound is ensured provided the length of the trajectory is at least of the order of the dimension $n$. Furthermore, this bound is also optimal in terms of dependence on $\delta, n$ and $T$ \cite{Simchowitz18a}.  

It is natural to consider the scenario where additional structural information is available regarding $A^*$ -- in this case one would expect that incorporating such information in the estimation procedure should lead to an improved performance compared to the vanilla OLS estimator. 
In many cases of interest, $A^*$ actually has an intrinsically low dimensional structure and it is possible to capture this structural information of $A^*$ through a known convex set $\calK$ containing $A^*$. Computationally, the estimate $\est{A}$ is then obtained by the constrained least squares estimator 
%
\begin{equation} \label{eq:Aest_convex}
 \est{A} \in \argmin{A \in \calK} \sum_{t=0}^{T} \norm{ x_{t+1} - A x_t}_2^2
\end{equation}
%
which is also a convex program that can typically be solved efficiently in practice. From a statistical perspective, one would expect to be able to improve the error bounds in \eqref{eq:err_bd_jedra} in terms of the dependence on the (extrinsic) dimension $n$. Some  examples of such $\calK$ -- which will also be used later for instantiating our more general results -- are outlined below.
%
\begin{enumerate}
\item ({\bf Example 1}) $\calK$ is a $d$-dimensional subspace of $\matR^{n \times n}$ for some $d \leq n^2$. This was considered recently in \cite{zheng2021finite} which cited applications of this setup in the time series analysis of spatio-temporal studies and social networks.

\item ({\bf Example 2}) If $A^*$ is $k$-sparse, i.e., has $k$ non-zero entries, then one can choose $\calK$ to be a suitably scaled $\ell_1$ ball such that $A^* \in \calK$. Such assumptions exist in the literature for model \eqref{eq:lin_dyn_sys_mod} as will be discussed in Section \ref{subsec:rel_work}. It is well known in the statistics and signal processing communities that the resulting estimator -- known as the LASSO -- promotes solutions which are sparse (see, e.g.,  \cite{candes09,neghabhan12,candes06}). 

\item ({\bf Example 3}) In this case, we consider $A^*$ to be formed by sampling an unknown convex function $f_0: [0,a]^2 \rightarrow \matR$ on a uniform grid. Then, the set $\calK$ can be written as
%
\begin{equation*}
  \calK = \set{A \in \matR^{n \times n}: \ A_{ij} = f\left(\frac{i-1}{n-1}, \frac{j-1}{n-1} \right) \text{ for some convex } f:[0,a]^2 \rightarrow \matR}.   
\end{equation*}
%
This problem is similar to the problem of convex-regression that has been studied extensively in the statistics community in the i.i.d setting, see,  e.g., \cite{hildreth54, Kur2020ConvexRI,Deme17} and references therein. From a computational perspective, it is useful to know that $\calK$ can be characterized as a convex cone \cite[Lemma 2.2]{Seijo11} which thus leads to a convex quadratic program that can be solved efficiently through standard solvers in practice; there also exist efficient specialized solvers for convex regression (e.g., \cite{chen2021multivariate}).
\end{enumerate}
%
In examples $1,2$, the intrinsic dimension of $A^*$ is essentially captured by the quantities $d$ or $k$, and we expect that the  error bounds in \eqref{eq:err_bd_jedra} should improve in terms of exhibiting a milder dependence on $n$. In particular, when $d, k \ll n^2$, we expect the estimation error for $A^*$ to be small for moderately large values of $T$. While this is less obvious for example $3$, we will see later that a similar conclusion can be drawn if $f_0$ is piecewise affine with a `simple structure'.

\subsection{Our contributions} 
For the setting where $\rho(A^*) < 1$ and $A^* \in \calK$, we derive non-asymptotic bounds on the estimation error in the Frobenius norm $\norm{\est{A} - A^*}_F$ for the estimator \eqref{eq:Aest_convex}, holding with high probability; see Theorems \ref{thm:main_err_tangent_cone} and \ref{thm:no_tancone_Struc} for the full statements. Our bounds depend on the `local size' of the set $\calK$ at $A^*$, captured via Talagrand's $\gamma_1, \gamma_2$ functionals \cite{talagrand2014upper} (see Definitions \ref{def:gamma_fun} and \ref{def:tancone} in Section \ref{prob_setup}). Upon instantiating our bounds for the aforementioned choices of $\calK$, we obtain the following corollaries.
%
%
\begin{enumerate}
    \item ({\bf Example 1}) In this case, Theorem \ref{thm:main_err_tangent_cone} states (see Corollary \ref{cor:subspace}) that with probability at least $1-\delta$, 
    %
    \begin{equation} \label{eq:intro_subsp_bd_corr}
      \norm{\est{A} - A^*}_F \lesssim   J(A^*)\left(\frac{\log(1/\delta) + \sqrt{d}}{\sqrt{T}} \right) \quad \text{if} \quad T \gtrsim J^4(A^*) \max\set{d, \log^2(1/\delta)} .
    \end{equation}
   % 
   Suppose for simplicity that $\norm{A^*}_2 < 1$ so that $J(A^*)$ is a constant. If $d = n^2$, we obtain the rate $\frac{n}{\sqrt{T}}$ which matches that obtained from \eqref{eq:err_bd_jedra}  using the standard inequality $\norm{\est{A} - A^*}_F \leq \sqrt{n} \norm{\est{A} - A^*}_2$. Moreover, we would then also need $T \gtrsim n^2$ in \eqref{eq:err_bd_jedra} in order to drive $\norm{\est{A} - A^*}_F$ below a specified threshold. For general $d$, however, the sample complexity of estimating $A^*$ is seen to be of order $d$ which is relevant when $d \ll n^2$.

    \item ({\bf Example 2})  In this case we obtain Corollary \ref{cor:sparse_example} of Theorem \ref{thm:main_err_tangent_cone} which is best interpreted for specific regimes of the sparsity level $k$. For instance, if $k$ is of the order $n$, we show that 
    %
      \begin{equation*}
       \norm{\est{A} - A^*}_F \lesssim J(A^*) \left(\frac{\log(1/\delta) + \sqrt{n \log n}}{\sqrt{T}} + \frac{(n \log n)^{3/2}}{T} \right) 
   \end{equation*}
   %    
   if $T \gtrsim J^4(A^*) \max\set{n \log n, \log^2(1/\delta)}$. Assuming $J(A^*)$ is constant, note that $T \gtrsim (n \log n)^{3/2}$ suffices to drive $\norm{\est{A} - A^*}_F$ below a specified threshold, however, this is still much milder than what we need in general.

   \item ({\bf Example 3}) Suppose $f_0$ is piecewise affine convex with $K$ pieces, and where the $i$th piece lies on a convex subdomain $\Omega_i$ with $\Omega_1,\dots,\Omega_K$ a partition of the domain $[0,a]^2$. Suppose each $\Omega_i$ is formed by the intersection of at most $s$ pairs of parallel halfspaces\footnote{We use the terminology of \cite{Kur2020ConvexRI} here. A pair of parallel halfspaces is the set $\set{v \in \matR^2: a \leq w^\top v \leq b}$ for scalars $a < b$ and a unit vector $w \in \matR^2$.}. Then Corollary \ref{cor:convex_reg_biv} of Theorem \ref{thm:no_tancone_Struc} implies that if $n$ is large enough and $T \gtrsim J^4(A^*) \max \set{c^s K \log^s n, \log^2\left(\frac{1}{\delta}\right)}$  then with probability at least $1-\delta$ 
     %
     %
     \begin{equation*}
      \norm{\est{A} - A^*}_F \lesssim J(A^*) \left(\frac{K c^s \log^{s+1} n}{T} + \frac{\log(1/\delta)}{\sqrt{T}}\right)   
     \end{equation*}
   %
  for some constant $c > 0$. So if $J(A^*), K$ and $s$ are constants then it suffices that $T \gtrsim \polylog n$ to drive the error below a specified threshold.  
\end{enumerate}

%----------------
% Related work
%----------------
\subsection{Related work} \label{subsec:rel_work}
\paragraph{Learning unstructured LDS.} A recent line of work has focused on deriving non-asymptotic error bounds for learning linear systems of the form \eqref{eq:lin_dyn_sys_mod}, without any explicit structural assumption on $A^*$. The majority of these works analyze the OLS under different assumptions on $\rho(A^*)$, namely: strict stability ($\rho(A^*) < 1$) \cite{Jedra20,FaraUnstable18}; marginal stability ($\rho(A^*) \leq 1$) \cite{Simchowitz18a, Sarkar19}; purely explosive systems ($\rho(A^*) > 1$) \cite{FaraUnstable18,Sarkar19}. While $\est{A}$ is known in closed form, the main challenge in the analysis comes from handling the interaction between the matrix of covariates $x_t$, and that of noise terms $\eta_t$ due to their dependencies. Common techniques used in the analysis involve concentration results for self normalized processes \cite{selfnormbook, abbasi11}, and Mendelson's ``small-ball'' method \cite{pmlr-v35-mendelson14}, the latter of which was extended to dependent data in \cite{Simchowitz18a} leading to sharper error bounds. When $\rho(A^*) \leq 1$, the authors in \cite{Simchowitz18a} interpret the quantity $\lambda_{\min}(\Gamma_{T-1})$ as a measure of the signal-noise-ratio \cite{Simchowitz18a} -- larger values  lead to improved error bounds. As mentioned earlier, the results of \cite{Jedra20} depend on a similar quantity, namely 
$\lambda_{\min}(\sum_{s=0}^{T-1} \Gamma_{s})$, which plays a key role in their error bounds. These terms do not appear explicitly within our analysis and it is unclear (albeit interesting) how this can be done. The main tools that we employ involve concentration results for the suprema of second-order subgaussian chaos processes indexed by a set of matrices \cite{krahmer14, dirksen15}; see Section \ref{subsec:tools} for details. 

\paragraph{Learning structured LDS.} Existing works for learning structured LDS are typically focused on specific structures such as sparsity, group sparsity, or low-rank models.
%
\begin{itemize}
\item In \cite{Fattahi2019}, a more general version of \eqref{eq:lin_dyn_sys_mod} was considered where $x_{t+1} = A^* x_t + B^* u_t + \eta_t$, with $B^* \in \matR^{n \times m}$ and $u_t \in \matR^{m}$ denoting the inputs. Assuming the unknown $A^*, B^*$ to be $k$-sparse, and $u_t = K_0 x_t + v_t$ where $v_t$ is random with a user specified distribution ($K_0$ is a feedback controller), a LASSO type estimator was analyzed. Assuming $x_0$ rests at its stationary distribution, uniform asymtotic stability of the closed-loop system, and certain technical assumptions involving $A^*, B^*$ and $K_0$, entry-wise error bounds were obtained for the estimation of $A^*, B^*$. It was shown that these bounds can sometimes be obtained with $T$ of the order $k^2 \log (n + m)$. If $k$ is of order $n$, this means that $T \gtrsim n^2 \log (n+m)$ samples are needed for recovering the \emph{support} of $A^*, B^*$. This is larger than our sample complexity bound for controlling the Frobenius norm error. 

\item In \cite{Pereira10}, the model \eqref{eq:lin_dyn_sys_mod} was considered with $A^*$ assumed to be $k$-sparse and strictly stable. Under certain assumptions on the problem parameters, it was shown for a LASSO-type estimator that the support of $A^*$ is recovered exactly provided $T \gtrsim \text{poly}(k) \cdot \log n$. 

\item The model \eqref{eq:lin_dyn_sys_mod} is a vector autoregressive (VAR) model of order $1$. In \cite{Melnyk16}, a more general order $d$-VAR model\footnote{Note that an order $d$-VAR can be written as an order-$1$ VAR in a higher dimensional space.} was considered involving matrices $A^*_1,\dots,A^*_d$. They analyzed a penalized least squares method where the penalty is imposed by a norm-based regularizer $R(\cdot)$; examples of $R$ including: $\ell_1$ norm, group LASSO, sparse group LASSO and OWL (ordered weighted $\ell_1$ norm). The recent work \cite{Wang23} considered an order $d$ VAR as well, but allowed for heavy-tail $\eta_t$'s and model-misspecification. They proposed an estimator based on the Yule-Walker equation that deploys a regularizer (norm-based) $R$, this leads to near minimax optimal results for estimating the model parameters under different structural settings: 
sparsity, low-rankness, and linear constraints. Other existing results primarily make sparsity-type assumptions on the parameters \cite{Loh12,Basu15,Kock2015,song2011large}. The recent work \cite{Basu15} considered the model \eqref{eq:lin_dyn_sys_mod} and assumed that $A^*$ has a low-rank plus sparse decomposition, they analyzed a penalized least squares approach for recovering the low-rank and sparse components of $A^*$.

\item The results in \cite{zheng2021finite} are applicable to model \eqref{eq:lin_dyn_sys_mod}, with additional linear information about $A^*$ assumed to be available. This can be reformulated as saying that for a known $d$-dimensional basis $\set{V_i}_{i=1}^d \subset \matR^{n \times n}$ and a known offset $\bar{V} \in \matR^{n \times n}$, we have $A^* - \bar{V} \in \text{span}\set{V_i}_{i=1}^d$. This is identical to Example $1$. If $\rho(A^*) < 1$ and $\norm{A^*}_2 \leq C$ for some constant $C > 0$, they show that $\norm{A^* - \est{A}}_F \lesssim \sqrt{\frac{d\log(d/\delta)}{T}}$ provided the smallest singular value of $A^*$ is sufficiently smaller than $1$.  This is similar to our bound in \eqref{eq:intro_subsp_bd_corr}. They also cover the setting $\rho(A^*) \leq 1$ where the analysis uses the small ball method \cite{pmlr-v35-mendelson14}. 

\end{itemize}

\paragraph{Learning structured signals from random linear measurements.} Consider the relatively easier setting of linear regression with independent covariates and noise, i.e., 
%
\begin{equation} \label{eq:struc_signal_rec_ind}
y = X \beta^* + \eta
\end{equation}
%
%
where $X \in \matR^{m \times n}$ is the matrix of covariates, $\beta^* \in \matR^n$ is the unknown signal,  $\eta$ is noise, and $X$ is \emph{independent} of $\eta$. Suppose for simplicity that the entries of $X$ and $\eta$ are i.i.d centered Gaussian's. The problem of recovering $\beta^*$ -- assuming it belongs to a convex set $\calK \subseteq \matR^n$ -- has received significant interest over the past decade from the statistical and signal processing communities. It is now known that the efficient recovery of $\beta^*$ is possible via convex programs (e.g., via solving least squares with constraint $\calK$, or by penalizing the objective) with the sample complexity $m$ depending on the Gaussian width of the `local size' of $\calK$ at $\beta^*$; see for e.g., \cite{neykov19b, rudelson08, chandra12, Tropp2015, planlasso16} and also \cite{lotz14} who introduced a related notion of `statistical dimension'. For some sets $\calK$ (such as the $\ell_1$ ball), sharp estimates for the Gaussian width are available through tools such as Gordon's escape through the mesh theorem \cite{Gordon88}, that leads to tight sample complexity bounds. While our proof technique is similar in spirit to these papers (in particular \cite{neykov19b,planlasso16}), the model in \eqref{eq:lin_dyn_sys_mod} leads to additional technical difficulties. For instance, we cannot use Gordon's theorem anymore and require other concentration tools for the underlying second order subgaussian chaos. To our knowledge, existing works for finite-time identification of \eqref{eq:lin_dyn_sys_mod} do not provide bounds for general closed convex sets $\calK$; our main goal is to fill this gap (to an extent) by drawing ideas from the above literature.  

\paragraph{Shape constrained regression.} There is a rich line of work for the Gaussian sequence model  
%
\begin{equation*}
  y_i = \beta_i^* + \eta_i; \quad i=1,\dots,n,
\end{equation*}
%
where $\beta^*_i = f(x_i)$ (with $x_i$'s the design points) for some unknown function $f \in \mathcal{F}$, with the function class $\mathcal{F}$ known. This is known as `shape constrained' nonparametric regression since the shape of $f$ (depending on $\mathcal{F}$) imparts structure to $\beta^*$ of the form $\beta^* \in \mathcal{K} \subset \mathbb{R}^n$ where 
%
$$\calK := \set{(f(x_1),\dots,f(x_n)) \in \matR^n: \ f \in \mathcal{F}}.$$
%
When $\eta_i$'s are i.i.d centered Gaussians, then the maximum likelihood estimator (MLE) of $\beta^*$ would be the orthogonal projection of $y$ on $\calK$. 
There exist many interesting examples of $\mathcal{F}$ which impart an intrinsically low-dimensional structure to $\mathcal{K}$, thus leading to better error bounds (in terms of $n$) for the MLE as compared to the unconstrained case. Some examples include: convex functions (e.g., \cite{Chatter15,Kur2020ConvexRI}), isotonic functions (e.g., \cite{Chatter15,Bellec18}) and Lipschitz functions \cite{neykov19b}; see \cite{Guntu18Survey} for a nice overview of this area and also the seminal work of Chatterjee \cite{Chatter14Persp}. In this respect, our work can be interpreted as shape constrained regression for the vector autoregressive model in \eqref{eq:lin_dyn_sys_mod}.