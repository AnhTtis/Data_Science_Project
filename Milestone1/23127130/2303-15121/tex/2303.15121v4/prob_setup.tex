%-------------------
% Problem setup
%-------------------
\section{Problem setup and results} \label{prob_setup}
%Define the problem setup: notation, model, goal etc. State main result(s).
\paragraph{Notation.} For any vector $x \in \matR^n$, $\norm{x}_p$ denotes the usual $\ell_p$ norm of $x$. For $X \in \matR^{n \times m}$, we denote $\norm{X}_2$, $\norm{X}_F$ to be respectively the spectral and Frobenius norms of $X$, while $\dotprod{X}{Y} = \Tr(X^\top Y)$ denotes the inner product between $X$ and $Y$. Also, $\vect(X) \in \matR^{nm}$ is formed by stacking the columns of $X$ and $\norm{X}_{1,1} = \norm{\vect(X)}_1$ denotes the entry-wise $\ell_1$ norm of $X$. The symbol $\otimes$ denotes the Kronecker product between matrices. Sets will be usually denoted by calligraphic letters, and their cardinalities by $\abs{\cdot}$. For any set $\calS \subset \matR^{n \times m}$ and $X \in \matR^{n \times m}$, we will denote 
%
$$\calS - X = \set{A-X: A \in \calS}.$$
%
For $a,b > 0$, we say $a \lesssim b$ if there exists a constant $C > 0$ such that $a \leq C b$. If $a \lesssim b$ and $a \gtrsim b$, then we write $a \asymp b$. For $n \times n$ matrices, we denote the unit Frobenius sphere by $\frobsphere$, the unit Frobenius ball by $\frobball$, and the identity matrix by $I_n$. The values of symbols used for denoting constants (e.g., $c, C, c_1$ etc.) may change from line to line. Finally, recall that the subgaussian norm of a random variable $X$ is given by 
$\norm{X}_{\psi_2}:= \sup_{p \geq 1} p^{-1/2} (\expec \abs{X}^p)^{1/p}$, see for e.g. \cite{vershynin_2012}. We say $X$ is $L$-subgaussian if $\norm{X}_{\psi_2} \leq L$.

%-----------
% Setup
%-----------
\subsection{Setup}
Consider the autonomous linear dynamical system in \eqref{eq:lin_dyn_sys_mod} where $(\eta_t)_{t \geq 1}$ are assumed to be zero-mean, independent and identically distributed (i.i.d) random variables for $t=0,\dots,T$. Specifically, $\eta_t$ is assumed to have i.i.d $L$-subgaussian entries (for some constant $L$), each of unit variance. Given $(x_t)_{t=0}^{T+1}$, our goal is to estimate $A^*$ under the constraint that $A^* \in \calK$ for a closed convex set $\calK \subseteq \matR^{n \times n}$. We focus on the estimator  \eqref{eq:Aest_convex} which is a convex program that can usually be efficiently solved by interior-point methods, and in many cases by more efficient methods (e.g., projected gradient descent) specialized to the structure of $\calK$. Also, if $\eta_t$ were i.i.d Gaussian's, then \eqref{eq:Aest_convex} would simply correspond to the maximum likelihood estimator (MLE) of $A^*$.

In our analysis, we will assume $A^*$ is strictly stable, i.e., its spectral radius $\rho(A^*) < 1$. The quantity $J(A^*)$ defined as 
%
\begin{equation} \label{eq:stab_param}
  J(A^*) := \sum_{i=0}^{\infty} \norm{(A^*)^i}_2  
\end{equation}
%
was introduced in \cite{Jedra20} for the analysis of the OLS estimator for strictly stable linear dynamical systems, and will also appear in our results. It is not difficult to verify that $J(A^*)$ is bounded if $\rho(A^*) < 1$, although it could grow with $n$. If $\norm{A^*}_2 < 1$ then $J(A^*) \leq \revo{\frac{1}{1-\norm{A^*}_2}}$.
%
%
%\hemant{(Actually, this needs to be checked in more detail -- how to bound $J(A^*)$ in terms of $\rho(A^*)$ if $A^*$ is stable?)} \denis{Using Jordan normal form theorem: there is a non-singular matrix $P\in\mathbb{R}^{n\times n}$ such that $A^*=P^{-1}J^* P$, where $J^*$ is a block-diagonal matrix having on the main diagonal real Jordan blocks. Hence, $(A^*)^i=P^{-1}(J^*)^i P$ and $\norm{(A^*)^i}_2\leq\norm{P^{-1}}_2\norm{(J^*)^i}_2 \norm{P}_2\leq\norm{P^{-1}}_2\norm{P}_2 \rho(A^*)^i$, which allows us to get a converging upper bound for $J(A^*)$.}
%\hemant{I think there are two problems with the above argument. The first is that $\norm{P^{-1}}_2$, $\norm{P}_2$ can grow with $n$, even as fast as polynomial with $n$, which makes the error bounds appearing later quite bad; note that the dependence on $n$ is crucial in what we say later. Also, I am not sure if $\norm{J^*}_2 \leq \rho(A^*)$ is true? See for e.g., this link: \url{https://math.stackexchange.com/questions/853143/2-norm-of-a-canonical-jordan-form-and-spectral-radius}} 

Before stating our results, we need to present some definitions which will be used later on. 

%
% Preliminaries
%
\subsection{Preliminaries} \label{subsec:prelim}
We begin by recalling Talagrand's $\gamma_{\alpha}$ functionals \cite{talagrand2014upper} which can be interpreted as a measure of the complexity of a (not necessarily convex) set.
%
%
\begin{definition}[\cite{talagrand2014upper}] \label{def:gamma_fun}
Let $(\calS,d)$ be a metric space. We say that a sequence of subsets of $\calS$, namely $(\calS_r)_{r \geq 0}$ is an admissible sequence if $\abs{\calS_0} = 1$ and $\abs{\calS_r} \leq 2^{2^{r}}$ for every $r \geq 1$. Then for any $0 < \alpha < \infty$, the $\gamma_{\alpha}$ functional of $(\calS, d)$ is defined as 
%
\begin{equation*}
    \gamma_{\alpha}(\calS, d) := \inf \sup_{s \in \calS} \sum_{r=0}^{\infty} 2^{r/\alpha} d(s, \calS_r)
\end{equation*}
%
with the infimum being taken over all admissible sequences of $\calS$. %\denis{Explain the notation $|\mathcal{S}_r|$?} \hemant{Added in notations paragraph.}
\end{definition}
%
The following properties of the $\gamma_{\alpha}$ functional are useful to note.
%
\begin{enumerate}
\item It can be verified that for any two metrics $d_1, d_2$ such that $d_1 \leq a d_2$ for some $a > 0$,  it holds
%\footnote{See for instance \cite[Exercise 2.2.20]{talagrand2014upper} for $\alpha = 2$, the arguments however extend to $\alpha \geq 1$.} 
that $\gamma_{\alpha}(\calS, d_1) \leq a \gamma_{\alpha}(\calS, d_2)$.

\item For $\calS' \subset \calS$, we have that $\gamma_{\alpha}(\calS', d) \leq C_{\alpha} \gamma_{\alpha}(\calS, d)$ for $C_{\alpha} > 0$ depending only on $\alpha$.

\item \revo{If $f: (\calT, d_1) \rightarrow (\calU, d_2)$ is onto and for some constant $M$ satisfies
%
\begin{equation*}
    d_2(f(x), f(y)) \leq M d_1(x,y) \quad \text{for all } x,y \in \calT,
\end{equation*}
%
then $\gamma_{\alpha}(\calU, d_2) \leq C_{\alpha} M \gamma_{\alpha}(\calT, d_1)$ for $C_{\alpha} > 0$ depending only on $\alpha$.}
\end{enumerate}
%
\revo{Properties $2$ and $3$ above are stated in \cite[Theorem 1.3.6]{tal05} (with $C_{\alpha} = 1$) for an alternative definition of the $\gamma_{\alpha}$ functional \cite[Definition 2.2.19]{talagrand2014upper} which is equivalent to Definition \ref{def:gamma_fun} up to a constant depending only on $\alpha$; see \cite[Section 2.3]{talagrand2014upper}.}
%
%

The $\gamma_{\alpha}$ functionals can be upper bounded in terms of the covering numbers of the set $\calS$. For any $\epsilon > 0$, denote $\calN(\calS, d, \epsilon)$ to be the minimum number of balls of radius $\epsilon$ (with centers in $\calS$) which are needed to cover $\calS$. Then, one can show\footnote{This can be deduced using  \cite[Corollary $2.3.2$]{talagrand2014upper}, and by replicating the arguments after the proof of \cite[Lemma 2.2.11]{talagrand2014upper} to general $\alpha \geq 1$.} 
that
%
\begin{equation} \label{eq:gamma_bound}
    \gamma_{\alpha}(\calS, d) \leq c_{\alpha} \int_{0}^{\diam(\calS)} \log^{1/\alpha} \calN(\calS, d, \epsilon) d\epsilon
\end{equation}
%
where $c_{\alpha} > 0$ depends only on $\alpha$, and $\diam(\calS)$ is the diameter of $\calS$.  
For $\alpha = 2$, the right-hand side (RHS) of \eqref{eq:gamma_bound} is the well-known Dudley entropy integral \cite{Dudley67}. In fact, by Talagrand's majorizing measure theorem \revo{\cite[Theorem 2.4.1]{talagrand2014upper}}, $\gamma_2(\calS,d)$ characterizes the expected suprema of centered Gaussian processes $(X_s)_{s \in \calS}$ as
%
\begin{equation} \label{eq:talag_maj_meas_thm}
  c \gamma_2(\calS,d) \leq \expec \sup_{s \in \calS}  X_s \leq C \gamma_2(\calS,d)
\end{equation}
%
for some universal constants $c, C > 0$, with the canonical distance $d(s,s') := (\expec[X_s - X_{s'}]^2)^{1/2}$. For example, if $\calS \subset \matR^{n \times m}$, and $X_s = \dotprod{G}{s}$ for a $n \times m$ matrix $G$ with iid standard Gaussian entries, we have $d(s,s') = \norm{s - s'}_F^2$. Then \eqref{eq:talag_maj_meas_thm} implies $\expec \sup_{s \in \calS} \dotprod{G}{s} \asymp \gamma_2(\calS, \norm{\cdot}_F)$ where $\expec \sup_{s \in \calS} \dotprod{G}{s}$ is known as the \emph{Gaussian width} of the set $\calS$, denoted as $w(\calS)$. %\denis{You use the symbol $\asymp$ just to say that two quantities are of the same order, but defined it for sequences? Explain what does it mean also for not sequences?} \hemant{Yes you are right, I think its better to just define it for two positive numbers $a,b$. Corrected this in notations paragraph. Does it look ok now?}
%
%
%
\paragraph{Tangent cone.}
One of our sample complexity bounds for estimating $A^*$ will depend on the local size of the \emph{tangent cone} of the set $\calK$ at $A^*$. 
%
\begin{definition}[Tangent cone] \label{def:tancone}
For a convex set $\calK$  and $A \in \calK$, the tangent cone at $A$ is defined as 
%
\begin{equation*}
    \calT_{\calK,A} := \cl\set{t(B - A): t \geq 0, \ B \in \calK}
\end{equation*}
%
where $\cl(\cdot)$ denotes the closure of a set.
\end{definition}
%
As we will see shortly, our first main result, namely Theorem \ref{thm:main_err_tangent_cone} below, will involve the gamma functionals $\gamma_1(\calT_{\calK,A^*} \cap \frobsphere, \norm{\cdot})$ and $\gamma_2(\calT_{\calK,A^*} \cap \frobsphere, \norm{\cdot})$ with $\norm{\cdot}$ corresponding to either the spectral or Frobenius norm. Small values of these terms will translate to weaker requirements on the sample size $T$ for accurately estimating $A^*$. By virtue of the earlier discussion, note that $\gamma_2(\calT_{\calK,A^*} \cap \frobsphere, \norm{\cdot}_F) \asymp w(\calT_{\calK,A^*} \cap \frobsphere)$.

%----------------------
% Main results 
%----------------------
\subsection{Main results}
Our first main result is the following theorem which bounds the estimation error $\norm{\est{A} - A^*}_F$. The proofs of our main results are detailed in Section \ref{sec:proof}.
%
\begin{theorem}[Tangent cone structure] \label{thm:main_err_tangent_cone}
There exist constants $C_1,C_2, C_3,C_4 > 0$ depending only on $L$ such that for any $\delta \in (0,1)$ and $B\in\calK$, if
%
$$T \geq C_1 J^4(A^*) \max\set{\gamma_2^2(\tanconeB \cap \frobsphere, \norm{\cdot}_2), \log^2(C_2/\delta)},$$
%
then it holds with probability at least $1-\delta$ that 
%
\begin{align*}
    \norm{\est{A} - A^*}_F 
    &\leq C_3 J(A^*)\left(\frac{\log(C_2/\delta) + \gamma_2(\tanconeB \cap \frobsphere, \norm{\cdot}_F)}{\sqrt{T}} + \frac{\gamma_1(\tanconeB \cap \frobsphere, \norm{\cdot}_2)}{T} \right) \\
    %
    %
    &+ C_4 J^2(A^*) \norm{A^* - B}_F .
\end{align*}
%
\end{theorem}
%
In the formulation of this theorem a generic matrix $B \in \calK$ is introduced, and a natural choice for it is $B=A^*$, which minimizes the last term in RHS above. However, the shape of $\tanconeB$, which becomes important in evaluation of $\gamma_{\alpha}(\tanconeB \cap \frobsphere, \norm{\cdot}_2)$ for $\alpha=1,2$, may be more suitable for calculation if $B\ne A^*$ (but sufficiently `close' to $A^*$). We will illustrate this on \rev{Examples $1,2$ and $4$} in the next section. \rev{In particular, we will take $B = A^*$ in Examples $1$ and $2$. For Example $4$, we will require a particular construction of $B \neq A^*$ such that $\gamma_{\alpha}(\tanconeB \cap \frobsphere, \norm{\cdot}_F)$ is suitably ``small''.}

Our second main result is useful in situations where $\calK$ does not have a small tangent cone structure at $A^*$, or at points in $\calK$ sufficiently close to $A^*$.
%
% without tangent cone structure
%
\begin{theorem}[without tangent cone structure]\label{thm:no_tancone_Struc} 
%
There exist constants $C_1,C_2, C_3, C_4 > 0$ depending only on $L$ such that the following is true. For any $\delta \in (0,1)$, $x > 0$ and $B \in \calK$, suppose that 
%
\begin{equation*}
T \geq C_1 J^4(A^*) \max \set{\frac{\gamma_2^2((\calK-B) \cap x\frobball,\norm{\cdot}_2)}{x^2}, \log^2(C_2/\delta)}. 
\end{equation*}
%
Then with probability at least $1-\delta$, the estimate $\est{A}$ satisfies
%
\begin{align*}
\norm{\est{A} - A^*}_F &\leq C_3 J(A^*) \left[\frac{\log(C_2/\delta)}{\sqrt{T}} + \frac{\gamma_1((\calK-B) \cap x\frobball, \norm{\cdot}_2)}{T x} + \frac{\gamma_2((\calK-B) \cap x\frobball, \norm{\cdot}_F)}{\sqrt{T} x} \right] \\
&+ C_4 J^2(A^*) \norm{A^*-B}_F + x.
\end{align*}
%
\end{theorem}
%
%
Suppose we take $B = A^*$ and say $A^*$ lies in the interior of $\calK$. Then $\calT_{\calK,A^*} = \matR^{n \times n}$ which renders Theorem \ref{thm:main_err_tangent_cone} to be unhelpful. In such a situation, Theorem \ref{thm:no_tancone_Struc} can be meaningful in terms of capturing the local size of $\calK$ at $A^*$ at a suitably small scale $x = o(1)$ (as $T \rightarrow \infty$). The optimal choice of $x$ will depend on how the terms involving the $\gamma_1$  and $\gamma_2$ functionals scale with $x$. We will apply Theorem \ref{thm:no_tancone_Struc} on Example $3$ in the next section. Interestingly, we will see that both the $\gamma_1$ and $\gamma_2$ terms are bounded by quantities depending \emph{linearly} on $x$. In this case, the limit $x = 0$ is the optimal choice.
%
%

\begin{remark} \label{rem:main_thms}
  Both Theorems \ref{thm:main_err_tangent_cone} and \ref{thm:no_tancone_Struc} have analogues in the literature for recovering (in the $\ell_2$ norm) a structured signal $\beta^*$ in the linear model \eqref{eq:struc_signal_rec_ind}, where the design matrix $X$ and the noise $\eta$ are typically independent; see for instance \cite{neykov19b, planlasso16}. Indeed, our proofs are structured along the same lines as that in \cite{neykov19b} (which in turn follows ideas from \cite{planlasso16}). However, our linear model in \eqref{eq:lin_dyn_sys_mod} is different, especially due to the temporal correlation between $x_t$ and $\eta_{t},\eta_{t-1},\dots...\eta_{1}$. This leads to additional technical difficulties in the analysis -- as opposed to \cite{neykov19b, planlasso16} we cannot use Gordon's mesh theorem \cite{Gordon88} and need to intead use a different set of concentration tools (see Section \ref{subsec:tools}) for controlling the terms in Lemma \ref{lem:ineq_first_ord_cond} (see Section \ref{subsec:proof_main_tancone}). One implication of this is the appearance of the $\gamma_1$ term in our bounds which can often be larger than the $\gamma_2$ term.
\end{remark}
%
%

\begin{remark} \label{rem:rel_work_comp}
    %\todo{Add comments on novelty of results: Theorem 2 is new, and Theorem 1 in its present form is also new (since $B \neq A^*$). Add discussion with PGD paper \cite{PGDstruct21}.}
\rev{
As discussed in Section \ref{subsec:rel_work}, existing works for learning structured LDS are typically focused on specific structural assumptions such as sparsity, low-rankness, group sparsity, or low-rank plus sparse matrices, and do not apply for general convex constraints $\calK$.} 

\rev{A work\footnote{We came across this work after the completion of the present paper.} closely related to ours is \cite{PGDstruct21}. For a convex regularizer $R(\cdot)$, it is assumed that $A^* \in \calK$ with $\calK := \set{A: R(A) \leq R(A^*)}$. The PGD method is shown to linearly converge (up to problem-dependent factors) to the `statistical error' $\frac{w(\calT_{\calK,A^*})}{\sqrt{T}}$ provided $T$ is at least of the order $w^2(\calT_{\calK,A^*})$ \cite[Theorem 1]{PGDstruct21}. There are some important points to be noted about this result.}
%
\begin{itemize}
    \item \rev{The proof makes use of an inequality  $\gamma_1(\calS,\norm{\cdot}_F) \leq \gamma_2^2(\calS,\norm{\cdot}_F)$, for a general set $\calS$, which was stated in \cite[Lemma 2.7]{Melnyk16}. We could not validate the proof of this inequality. Without using the inequality, one can verify from the proof of \cite[Theorem 1]{PGDstruct21} that the statistical error therein would then be of the order $$\frac{\gamma_2(\calT_{\calK,A^*} \cap \frobsphere, \norm{\cdot}_F)}{\sqrt{T}} + \frac{\gamma_1(\calT_{\calK,A^*} \cap \frobsphere, \norm{\cdot}_2)}{T}$$ provided $T = \Omega(\gamma_2^2(\calT_{\calK,A^*} \cap \frobsphere))$. This is the same as in Theorem \ref{thm:main_err_tangent_cone} for $B = A^*$. While there are differences in the technical details between our proof of Theorem \ref{thm:main_err_tangent_cone} and that of \cite[Theorem 1]{PGDstruct21}, an important aspect of Theorem \ref{thm:main_err_tangent_cone} is that the bounds are stated in terms of tangent cone structure at $B$ (not necessarily equal to $A^*$) which can be useful in certain situations (we show this in Corollary \ref{cor:lipschitz_reg_examp} for Example $4$).
    }

    \item \rev{Upon inspection of the proof of \cite[Theorem 1]{PGDstruct21}, it seems possible that the analysis applies for general convex sets $\calK$ without any change. However this would have to be verified in detail.}
\end{itemize}
%
\end{remark}

%--------------------------------------------------
% Instantiating our main results on some examples
%--------------------------------------------------
\subsection{Instantiating our results on examples}
%
We now recall the examples from Section \ref{sec:intro} for which the bounds in Theorems \ref{thm:main_err_tangent_cone} and \ref{thm:no_tancone_Struc} can be made explicit in terms of $n$. The proofs are detailed in Section \ref{sec:proof_corr}.

\paragraph{Example $1$ ($d$-dimensional subspace).} In this case, observe from the definition of $\tanconeB$ that $\tanconeB = \calK$ for any choice of $B \in \calK$. One can then use standard covering number bounds to bound the $\gamma_{\alpha}$-functionals in Theorem \ref{thm:main_err_tangent_cone}. This leads to the following corollary of Theorem \ref{thm:main_err_tangent_cone} where we take $B = A^*$.
%
%
\begin{corollary} \label{cor:subspace}
Let $\calK \subset \matR^{n \times n}$ be a $d$-dimensional subspace. If $T \gtrsim J^4(A^*) \max\set{d, \log^2(1/\delta)}$ for $\delta \in (0,1)$, then it holds with probability at least $1-\delta$ that
%
\begin{equation*}
    \norm{\est{A} - A^*}_F \lesssim   J(A^*)\left(\frac{\log(1/\delta) + \sqrt{d}}{\sqrt{T}} \right).
\end{equation*}
%
\end{corollary}
%
In the unconstrained case where $\calK = \matR^{n \times n}$, so that $d = n^2$, \eqref{eq:Aest_convex} is the OLS estimator. Then Corollary \ref{cor:subspace} states that if $T \gtrsim J^4(A^*) \max\set{n^2, \log^2(1/\delta)}$, we have  
%
\begin{equation} \label{eq:unconstr_bd}
    \norm{\est{A} - A^*}_F \lesssim J(A^*)\left(\frac{\log(1/\delta) + n}{\sqrt{T}} \right).
\end{equation}
%
Existing error bounds in the literature for the OLS estimator are typically in the spectral norm, but can of course be converted to the Frobenius norm with an extra factor of $\sqrt{n}$. Indeed, the result of \cite{Jedra20} in \eqref{eq:err_bd_jedra} implies
%Denoting $\Gamma_s(A) = \sum_{k=0}^s A^k (A^k)^{\top}$ for $s \geq 0$, and $\lambda_{\min}(\cdot)$ to be the smallest eigenvalue of a symmetric matrix, they show that w.p at least $1-\delta$, 
%
%\begin{equation*}
%\norm{\est{A} - A^*}_2 \lesssim %\sqrt{\frac{\log(1/\delta) + n}{\lambda_{\min}%(\sum_{s=0}^{T-1} \Gamma_s(A^*))}}
%\end{equation*}
%
%provided $\lambda_{\min}(\sum_{s=0}^{T-1} \Gamma_s(A^*)) \gtrsim J^2(A^*)(\log(1/\delta) + n)$. Moreover, this bound is also optimal up to the dependence on $J(A^*)$. Since $\lambda_{\min}(\sum_{s=0}^{T-1} \Gamma_s(A^*)) \geq T$, this implies
%
\begin{equation} \label{eq:jedra_frob_bd}
  \norm{\est{A} - A^*}_F \leq \sqrt{n} \norm{\est{A} - A^*}_2 \lesssim \sqrt{\frac{n\log(1/\delta) + n^2}{T}}
\end{equation}
%
provided $T \gtrsim J^2(A^*)(\log(1/\delta) + n)$. The bound in \eqref{eq:jedra_frob_bd} is of the same order as in \eqref{eq:unconstr_bd}, barring the extra $J(A^*)$ term in our bound. Moreover, when $J(A^*)$ is a constant, note that $T$ needs to be at least of the order $n^2$ -- in both \eqref{eq:unconstr_bd} and \eqref{eq:jedra_frob_bd} -- in order to drive the error below a specified threshold. Of course, in case $d \ll n^2$, then the requirement $T \gtrsim d$ in Corollary \ref{cor:subspace} is relatively mild, as one would expect, given that $\calK$ has an intrinsic dimension $d$. 

%-------------------------------------------------
% The sparse case, with \ell_1 ball constraint
%-------------------------------------------------
\paragraph{Example $2$ ($\ell_1$ ball).} We now consider the setting where $A^*$ is $k$-sparse, i.e., has at most $k$ non-zero entries for some $1 \leq k \leq n^2$. A standard strategy for recovering sparse matrices (resp. vectors) is to take $\calK$ to be a suitably scaled ball in the $\norm{\cdot}_{1,1}$ (resp. $\ell_1$) norm. We take $\calK := \norm{A^*}_{1,1} \calB_{1,n}$ where
%
\begin{equation} \label{eq:l11_ball}
    \calB_{1,n} := \set{A \in \matR^{n \times n}: \norm{A}_{1,1} \leq 1}
\end{equation}
%
so that $A^*$ lies on the boundary of $\calK$. Then the tangent cone $\tanconeAstar$ has the form
%
\begin{equation*}
    \tanconeAstar := \cl\set{t U: \norm{A^* + U}_{1,1} \leq \norm{A^{\star}}_{1,1}, \ t \geq 0}.
\end{equation*}
%
One can bound $w(\tanconeAstar \cap \frobsphere)$ by using existing results in the literature \cite{rudelson08} (see also \cite{stojnic09,lotz14,chandra12,Tropp2015}) -- these results apply for vectors but can be directly invoked in our setting by treating $n \times n$ matrices as vectors in $\matR^{n^2}$. This leads to the same order-wise bound on $\gamma_2(\tanconeAstar \cap \frobsphere, \norm{\cdot}_F)$ due to \eqref{eq:talag_maj_meas_thm} with $\calS = \tanconeAstar \cap \frobsphere$. Furthermore, the $\gamma_1$ functional term can be bounded in terms of $w(\tanconeAstar \cap \frobsphere)$ using Sudakov's minoration inequality \cite[Theorem 7.4.1]{HDPbook}. These considerations lead to the following corollary of Theorem \ref{thm:main_err_tangent_cone}.
%
%
\begin{corollary} \label{cor:sparse_example}
    Suppose $A^*$ is $k$-sparse and $\calK = \norm{A^*}_{1,1} \calB_{1,n}$ where $\calB_{1,n}$ is the unit ball defined in \eqref{eq:l11_ball}, and denote $\beta(n,k) := \sqrt{k \log(n^2/k) + k}$. For any $\delta \in (0,1)$, if 
    %
    $$T \gtrsim J^4(A^*) \max\set{\beta^2(n,k), \log^2(1/\delta)}$$
    %
    then with probability at least $1-\delta$ it holds that 
    %
    \begin{equation*}
       \norm{\est{A} - A^*}_F \lesssim J(A^*) \left(\frac{\log(1/\delta) + \beta(n,k)}{\sqrt{T}} + \frac{n \beta(n,k) [1 + \log(\frac{n}{\beta(n,k)})]}{T}\right).
    \end{equation*}
    %
   \end{corollary}
   %
   As a sanity check, note that when $k = n^2$ then $\beta(n,k) = n$ and we recover the statement of Corollary \ref{cor:subspace} with $d = n^2$. This is expected since $A^*$ does not possess any additional structure, and hence the constraint $\calK$ -- the purpose of which is to promote sparse solutions -- does not provide any benefit. The non-trivial sparsity regime is when $k = o(n^2)$. Consider for instance the case where $k \asymp n$. Then, we have $\beta(n,k) \asymp \sqrt{n \log n}$ and Corollary \ref{cor:sparse_example} gives the error bound
   %
   \begin{equation} \label{eq:sparse_rec_bd_1}
       \norm{\est{A} - A^*}_F \lesssim J(A^*) \left(\frac{\log(1/\delta) + \sqrt{n \log n}}{\sqrt{T}} + \frac{(n \log n)^{3/2}}{T} \right)
   \end{equation}
   %
   provided $T \gtrsim J^4(A^*) \max\set{n \log n, \log^2(1/\delta)}$. Notice that while this condition on $T$ implies the error bound in \eqref{eq:sparse_rec_bd_1}, we actually need $T$ to be at least of the order $J(A^*) (n \log n)^{3/2}$ to drive the error in \eqref{eq:sparse_rec_bd_1} below a specified threshold. This ``gap'' is of course due to the term $(n \log n)^{3/2}/T$ in \eqref{eq:sparse_rec_bd_1} -- this term arises from the bound on $\gamma_1(\tanconeAstar \cap \frobsphere, \norm{\cdot}_F)$ within the proof of the corollary and is likely suboptimal. Nevertheless, the bound in \eqref{eq:sparse_rec_bd_1} clearly has a milder dependence on $n$ as compared to that obtained for the OLS in \eqref{eq:unconstr_bd}.

   %-------------------------
   % Convex regression
   %-------------------------
   \paragraph{Example $3$ (convex regression).}  
   We will now illustrate the use of Theorem \ref{thm:no_tancone_Struc} in the context of convex regression.  Consider the case where $\calK$ is the collection of matrices formed by sampling bivariate convex functions on a square grid of $\Omega = [0,1/\sqrt{2}]^2$, i.e.,
   %
   \begin{align} \label{eq:K_bivariate_convex}
        \calK := \set{A \in \matR^{n \times n}: A_{ij} = f\left(\frac{i-1}{(n-1)\sqrt{2}}, \frac{j-1}{(n-1)\sqrt{2}} \right) \text{ for some } f:\Omega \rightarrow \matR \text{ convex }}.
   \end{align}
   %
   %
   Then the following corollary of Theorem \ref{thm:no_tancone_Struc} is obtained taking $B = A^*$ with $A^*$ formed by sampling a convex function with a simple (i.e., piecewise affine) structure. 
   %
   %
   \begin{corollary} \label{cor:convex_reg_biv}
     Let $\Omega_1,\dots,\Omega_K$ be convex subsets of $\Omega = [0,1/\sqrt{2}]^2$ which (a) form a partition of $\Omega$, and (b) are such that each $\Omega_i$ is an intersection of at most $s$ pairs of parallel halfspaces. Suppose $A^* \in \calK$ for some convex $f_0$ which is affine on each $\Omega_i$. Then there exists a constant $c > 0$ such that the following is true. For any $\delta \in (0,1)$, if  
     %
     \begin{equation*}
       \frac{n^2}{\log^s n} \geq K c^{s} \ \text{ and } \ T \gtrsim J^4(A^*) \max \set{c^s K \log^s n, \log^2\left(\frac{1}{\delta}\right)},
     \end{equation*}
     %
     then with probability at least $1-\delta$, 
     %
     %
     \begin{equation*}
      \norm{\est{A} - A^*}_F \lesssim J(A^*) \left(\frac{K c^s \log^s n \log(\frac{n^2}{c^s K \log^s n})}{T} + \frac{\log(1/\delta) + c^{s/2} K^{1/2} \log^{s/2} n}{\sqrt{T}}\right).   
     \end{equation*}
     %
     %
   \end{corollary}
   %
   The proof uses metric entropy estimates from \cite[Section 4]{Kur2020ConvexRI} for a collection of bivariate convex functions lying within a ball of radius $x$ around $f_0$. This then leads to bounds for the $\gamma_1$ and $\gamma_2$ functionals appearing in Theorem \ref{thm:no_tancone_Struc}. Interestingly, we find for the specified $A^*$ that  
   %
   %
\begin{align*}
 \gamma_1((\calK - A^*) \cap x\frobball, \norm{\cdot}_2)  &\lesssim  x c^s K \log^s n \log\left(\frac{n^2}{c^s K \log^s n} \right), \\
 %
 %
\gamma_2((\calK-A^*) \cap x\frobball, \norm{\cdot}_2) 
&\lesssim x c^{s/2} K^{1/2} \log^{s/2} n.
\end{align*}
% 
Hence the only dependence on $x$ in the error bound of Theorem \ref{thm:no_tancone_Struc} is through the last term, and since the bound holds for any $x > 0$, we can take the limit $x = 0$.
   
The scaling factor of $\sqrt{2}$ in the definition of $\Omega$ essentially ensures that $\Omega$ lies within the unit $\ell_2$ ball in $\matR^2$, which is needed in order to use the results of \cite{Kur2020ConvexRI}. Note that in order to drive the error below a threshold, it suffices to have 
   %
   \begin{equation*}
       T \gtrsim J^4(A^*) \max \set{c^s K \log^s n \log \left(\frac{n^2}{c^s K \log^s n} \right), \log^2\left(\frac{1}{\delta}\right)}.
   \end{equation*}
   %
   If, e.g., $s$ and $K$ are constants (i.e., $f_0$ has a simple structure), then 
   %
   $$T \gtrsim J^4(A^*) \max\set{\text{polylog}(n), \log^2\left(\frac{1}{\delta}\right)}$$ 
   %
   is sufficient, which is much milder then the requirement for OLS. 

   %-------------------------
   % Lipschitz regression
   %-------------------------
   \paragraph{\rev{Example $4$ (Lipschitz regression).}} 
   \rev{For this example, we illustrate the applicability of Theorem \ref{thm:main_err_tangent_cone} when $B \neq A^*$.  Let $f_1,\dots,f_n$ be unknown Lipschitz functions where $f_i:[0,1] \rightarrow \matR$ and 
   %
   \begin{equation*}
       \abs{f_i(x) - f_i(y)} \leq L_i \abs{x - y}; \ \quad \forall x,y \in [0,1], 
   \end{equation*}
   % 
   for some $L_i \geq 0$. We then assume that each row of $A^*$ is formed by uniformly sampling $f_i$ with a step size $1/T$, i.e., 
   %
   \begin{equation*}
     A^*_{i,j} = f_i(j/T); \quad j=1,\dots,n, 
   \end{equation*}  
   %
   with $T \geq n$. Hence we consider the constraint set $\calK$ to be 
   %
    \begin{equation} \label{eq:lips_reg_set}
       \calK := \set{A \in \matR^{n \times n}:  \abs{A_{i,j} - A_{i,j+1}} \leq \frac{L_i}{T}, \ i \in [n], \ j \in [n-1]}.
    \end{equation}
    %
    This example is essentially motivated by that proposed in \cite[Section 3.1]{neykov19b} which considered $\beta^* \in \matR^n$ to be formed by sampling an unknown Lipschitz function. The techniques we will subsequently employ in the proof of Corollary \ref{cor:lipschitz_reg_examp} below are essentially adaptations of those used in \cite[Corollary 3.4]{neykov19b}. The main idea is to show the existence of $B$ which is a good approximation of $A^*$, and each row of which is formed by sampling a piecewise linear function with few pieces. For ease of exposition, we will outline our final result using $L$ such that $L_i \leq L$ for each $i$.} 
%
\rev{
\begin{corollary}\label{cor:lipschitz_reg_examp}
    Suppose $A^* \in \calK$ with $\calK$ as in \eqref{eq:lips_reg_set} and let $L_i \leq L$ for each $i$. For any $\delta \in (0,1)$, if
    %
    \begin{equation} \label{eq:T_bd_lipreg_fin}
       T \gtrsim \max \set{J^4(A^*) \log^2(1/\delta), J^{20/7}(A^*) n^{10/7} L^{4/7} (\log n)^{2/7}}    
    \end{equation}
    %
    then with probability at least $1-\delta$, it holds that  
    %
    \begin{align*}
        \norm{\est{A} - A^*}_F &\lesssim J^2(A^*) \Bigg[ \frac{\log(1/\delta)}{\sqrt{T}} + \left(\max\set{\sqrt{\frac{n\log n}{T}},\frac{(n \log n)^{3/2}}{T} }\right) \\
        &+ \left( \max \set{1,\frac{(n \log n)^{4/5}}{T^{2/5}}} \right) \left( \frac{n^{6/5} L^{2/5}}{T^{4/5}} (\log n)^{2/5} \right) \Bigg].
    \end{align*}
\end{corollary}
%
Let us examine the requirement on $T$ which ensures that $\norm{\est{A} - A^*}_F \leq \epsilon$ for any $\epsilon \in (0,1)$. Apart from the condition on $T$ in \eqref{eq:T_bd_lipreg_fin}, one can verify that we additionally require
%
\begin{equation*}
  T \gtrsim \max \set{\frac{n \log n}{\epsilon^2}, \frac{(n \log n)^{3/2}}{\epsilon}, \frac{n^{3/2} L^{1/2} (\log n)^{1/2}}{\epsilon^{5/4}}, \frac{(n^{5/3} \log n) L^{1/3}}{\epsilon^{5/6}}}.
\end{equation*}
%
In particular, this means that accurate estimation of $A^*$ is possible with $T = o(n^2)$ samples. 
}