
\section{Evaluation}
\label{sec:evaluation}

\subsection{Ground-truth and Metrics}

\textbf{Ground-truth} is information about known properties of some entities based on observation and expert knowledge. In the classification problem, it provides the precise class of each instance (called \textbf{label}). Ground truth plays a vital role in machine learning because it is a  benchmark for evaluating performance and finding the best from models with various algorithms, parameters, and features. The data set \Cresci has three types of bot accounts, so we reconstruct them into three ground-truth sets shown in Table~\ref{tab:gt}. Each set consists of bot accounts and genuine users, and we define bots as positive instances and genuine ones as negative instances. One set by one, we gradually escalate the scope of bots for a more detailed performance evaluation in three situations.


\begin{table}[h]
\centering
\caption{Ground-truth}
\label{tab:gt}
\vspace{-6pt}
\renewcommand{\arraystretch}{1.1}
\small
\begin{tabular}{|c|c|c|p{1.5in}|}
\hline
{\bf Set}  & {\bf Positive} & {\bf Negative} & {\bf Descriptions}\\
\hline
BotSet 1 & 4912 & 1083 &social bots, genuine users \\
\hline
BotSet 2 & 5912 & 1083 & social bots, traditional bots, genuine users \\
\hline
BotSet 3 & 9263 & 1083 & social bots, traditional bots, fake followers, genuine users \\
\hline
\end{tabular}
\end{table}

In a standard machine learning classification pipeline, engineers randomly split the instances of a ground truth into two parts, including \emph{train set} occupying 70 percent and \emph{test set} occupying 30 percent. \BotShape uses instances of the train set to fit the functional relationship between features and actual labels. Then it outputs predicted labels for instances in the test set after inputting features. Due to the difference between actual labels and predicted labels in the test set, there are four cases for each instance: (i) True Positive (TP): the actual and the predicted label are all positive; (ii) False Positive(FP): the actual label is negative, and the predicted is positive; (iii) True Negative (TN): the actual and the predicted label are all negative; (iv) False Negative (FN): the actual label is positive, and the predicted is negative.


We adopt two metrics \textbf{accuracy} and \textbf{f1-score} based on those four statistics. \textbf{Accuracy} equals $\frac{TP+TN}{TP+FP+TN+FN}$. It reflects the correct rate of all predictions, regardless of whether the actual class is positive. Generally, the higher the accuracy, the more right prediction occurs in an online system. \textbf{F1-score} combines two important metric \emph{precision} and \emph{recall}, equaling $\frac{2 \cdot precision \cdot recall}{precision+reall}$. \emph{Precision} is the rate of correct prediction in predicted positive instances, equaling $\frac{TP}{TP+FP}$. High precision refers to a lower false alarm rate, meaning fewer wrongly punished genuine users. \emph{Recall} is the bot discovery rate in actual positive instances, and it equals $\frac{TP}{TP+FN}$. Higher \emph{recall} stands for a less count of bots that the detection system will miss. \textbf{F1-score} could comprehensively reflects \emph{precision} and \emph{recall} performance, by averaging them. In general, the higher \emph{f1-score is}, the higher \emph{precision} and \emph{recall} are.





\subsection{Assess Effectiveness of Behavioral Features}
\label{subsec:eval_features}

We evaluate the bot detection performance of \BotShape three ground-truth. We split feature data into three groups: \textbf{account}, \textbf{behavior}, and \textbf{time-series}. \textbf{Account features} refer to user properties profiling an account, including five indicators directly coming from data set \Cresci: the count of statuses, the count of followers, the count of friends, the count of favorites, and the count of lists. Also, for fair competition, we add one indicator into the feature set: the total count of tweets within one year after registration. \textbf{Sequence features} (mentioned in sub section~\ref{subsec:design_bs}) consists of 3 types of raw behavioral sequences, including daily tweet count in the first 30 days, weekly tweet count in the first 53 weeks, and monthly tweet count in the first 12 months after registration. \textbf{Pattern features} (mentioned in sub section~\ref{subsec:design_ts}) have two significant features compressed from \textbf{behavior} feature set, including \emph{seasonality} and \emph{shapelets} features. 


\begin{table*}[h]
  \centering
  \caption{Accuracy and F1-scores}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
    \toprule
    \multicolumn{2}{|c|}{\textbf{data set}} & \multicolumn{4}{c|}{\textbf{social bots}} & \multicolumn{4}{c|}{\textbf{social + traditional bots}} & \multicolumn{4}{c|}{\textbf{all bots + fake followers}} \\
    \midrule
    \multicolumn{2}{|c|}{\textbf{feature set}} & \textbf{account} & \textbf{sequence} & \textbf{pattern} & \textbf{gain} & \textbf{account} & \textbf{sequence} & \textbf{pattern} & \textbf{gain} & \textbf{account} & \textbf{sequence} & \textbf{pattern} & \textbf{gain} \\
    \midrule
    \multirow{2}[4]{*}{\textbf{SVM}} & \textbf{Accuracy} & 86.56\% & 96.82\% & \textbf{97.83\%} & \textcolor[rgb]{ .275,  .663,  .129}{\textbf{11.27\%}} & 80.94\% & 93.16\% & \textbf{95.71\%} & \textcolor[rgb]{ .275,  .663,  .129}{\textbf{14.77\%}} & 90.70\% & 96.32\% & \textbf{97.18\%} & \textcolor[rgb]{ .275,  .663,  .129}{\textbf{6.48\%}} \\
\cmidrule{2-14}          & \textbf{F1 Score} & 79.65\% & 94.45\% & \textbf{96.11\%} & \textcolor[rgb]{ .275,  .663,  .129}{\textbf{16.46\%}} & 74.44\% & 88.55\% & \textbf{91.90\%} & \textcolor[rgb]{ .275,  .663,  .129}{\textbf{17.46\%}} & 80.13\% & 90.63\% & \textbf{92.28\%} & \textcolor[rgb]{ .275,  .663,  .129}{\textbf{12.15\%}} \\
    \midrule
    \multirow{2}[4]{*}{\textbf{LR}} & \textbf{Accuracy} & 94.95\% & 86.46\% & \textbf{98.84\%} & \textcolor[rgb]{ .275,  .663,  .129}{\textbf{3.89\%}} & 94.41\% & 84.71\% & \textbf{97.27\%} & \textcolor[rgb]{ .275,  .663,  .129}{\textbf{2.86\%}} & 95.75\% & 89.54\% & \textbf{98.57\%} & \textcolor[rgb]{ .275,  .663,  .129}{\textbf{2.82\%}} \\
\cmidrule{2-14}          & \textbf{F1 Score} & 90.68\% & 64.90\% & \textbf{97.92\%} & \textcolor[rgb]{ .275,  .663,  .129}{\textbf{7.24\%}} & 88.13\% & 57.30\% & \textbf{94.90\%} & \textcolor[rgb]{ .275,  .663,  .129}{\textbf{6.77\%}} & 86.40\% & 58.77\% & \textbf{96.23\%} & \textcolor[rgb]{ .275,  .663,  .129}{\textbf{9.83\%}} \\
    \midrule
    \multirow{2}[4]{*}{\textbf{MLP}} & \textbf{Accuracy} & 82.97\% & 84.34\% & \textbf{98.94\%} & \textcolor[rgb]{ .275,  .663,  .129}{\textbf{15.97\%}} & 84.06\% & 84.06\% & \textbf{96.80\%} & \textcolor[rgb]{ .275,  .663,  .129}{\textbf{12.73\%}} & 89.54\% & 89.54\% & \textbf{98.93\%} & \textcolor[rgb]{ .275,  .663,  .129}{\textbf{9.39\%}} \\
\cmidrule{2-14}          & \textbf{F1 Score} & 45.35\% & 54.01\% & \textbf{98.11\%} & \textcolor[rgb]{ .275,  .663,  .129}{\textbf{52.76\%}} & 45.67\% & 45.67\% & \textbf{93.98\%} & \textcolor[rgb]{ .275,  .663,  .129}{\textbf{48.31\%}} & 47.24\% & 47.24\% & \textbf{97.17\%} & \textcolor[rgb]{ .275,  .663,  .129}{\textbf{49.93\%}} \\
    \midrule
    \multirow{2}[4]{*}{\textbf{RF}} & \textbf{Accuracy} & 97.22\% & 86.81\% & \textbf{98.69\%} & \textcolor[rgb]{ .275,  .663,  .129}{\textbf{1.47\%}} & 96.84\% & 97.62\% & \textbf{98.96\%} & \textcolor[rgb]{ .275,  .663,  .129}{\textbf{2.12\%}} & 98.19\% & 98.51\% & \textbf{99.17\%} & \textcolor[rgb]{ .275,  .663,  .129}{\textbf{0.98\%}} \\
\cmidrule{2-14}          & \textbf{F1 Score} & 94.97\% & 68.55\% & \textbf{97.68\%} & \textcolor[rgb]{ .275,  .663,  .129}{\textbf{2.71\%}} & 93.68\% & 95.44\% & \textbf{98.01\%} & \textcolor[rgb]{ .275,  .663,  .129}{\textbf{4.32\%}} & 94.96\% & 95.98\% & \textbf{97.73\%} & \textcolor[rgb]{ .275,  .663,  .129}{\textbf{2.76\%}} \\
    \bottomrule
    \end{tabular}%
  \label{tab:eval_clf}%
\end{table*}%

To prove that features constructed by \BotShape feature engineering approach (\textbf{behavior feature} and \textbf{time series feature}) have steady improvements on different classifiers, we select four widely used algorithms: Support Vector Machine \cite{hearst1998support} (SVM), Logistic Regression \cite{kleinbaum2002logistic} (LR), Multilayer Perceptron \cite{murtagh1991multilayer} (MLP) and Random Forest \cite{breiman2001random} (RF). We apply a famous machine learning python library scikit-learn \cite{pedregosa2011scikit} to implement all the classifiers.

Table~\ref{tab:eval_clf} shows the accuracy and f1-scores on all classifiers and feature groups. The result points out \textbf{pattern feature} performs best on two metrics across all classifiers, with all accuracy exceeding 97\% and a very high f1-score range from 92\% to 98\%. \textbf{Sequence feature} also presents high accuracy on SVM and RF, but it shows low f1-scores on classifier Logistic Regression and Multilayer Perceptron. We infer that irrelevant indicators in \textbf{behavior feature} become noise, making a simple classifier learn fake functional relationships between noisy features and labels. Random Forest and Support Vector Machine are more robust to noisy features. 


\subsection{Compare with Other Approaches}
As sub-section~\ref{subsec:related} mentioned, there are three detection approaches: \textbf{accout-based}, \textbf{content-based}, and \textbf{graph-based}. Under the condition of \Cresci data set, we could contrast \BotShape with \textbf{account-based features}. We could not experiment with the \textbf{Graph-based} method because there are no edges like the following relationship in the data set. We do not implement \textbf{content-based} method because our approach has already achieved very high accuracy, and its used computation resource is very saving compared with content models.

In table~\ref{tab:eval_clf}, we define a new metric called \emph{performance gain} to measure the improvement of \BotShape, which is the difference between \emph{pattern features} and \emph{account features}. The value of \emph{performance gain} is influenced by the actual accuracy of \BotShape and the initial accuracy of \emph{account features} because the largest accuracy value is 100\%. F1-score is also the same. \emph{Gain of accuracy} ranges from 0.98\% to 11.27\%, with an average value of 6.53\%. \emph{Gain of f1-score} ranges from 2.71\% to 52.77\% with an average value of 19.23\%. 
