
@article{aagm2022siam,
author = {Akimoto, Y. and Auger, A. and Glasmachers, T. and Morinaga, D.},
title = {Global Linear Convergence of Evolution Strategies on More than Smooth Strongly Convex Functions},
journal = {SIAM Journal on Optimization},
volume = {32},
number = {2},
pages = {1402-1429},
year = {2022},
doi = {10.1137/20M1373815},
}

@inproceedings{DBLP:conf/icml/JinNJ20,
  author    = {Chi Jin and
               Praneeth Netrapalli and
               Michael I. Jordan},
  title     = {What is Local Optimality in Nonconvex-Nonconcave Minimax Optimization?},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning,
               {ICML} 2020, 13-18 July 2020, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  pages     = {4880--4889},
  publisher = {{PMLR}},
  year      = {2020},
  url       = {http://proceedings.mlr.press/v119/jin20e.html},
  timestamp = {Tue, 15 Dec 2020 17:40:19 +0100},
  biburl    = {https://dblp.org/rec/conf/icml/JinNJ20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{surrogatetheory,
author = {Akimoto, Y.},
title = {Monotone Improvement of Information-Geometric Optimization Algorithms with a Surrogate Function},
year = {2022},
isbn = {9781450392372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512290.3528690},
doi = {10.1145/3512290.3528690},
abstract = {A surrogate function is often employed to reduce the number of objective function evaluations for optimization. However, the effect of using a surrogate model in evolutionary approaches has not been theoretically investigated. This paper theoretically analyzes the information-geometric optimization framework using a surrogate function. The value of the expected objective function under the candidate sampling distribution is used as the measure of progress of the algorithm. We assume that the surrogate function is maintained so that the population version of the Kendall's rank correlation coefficient between the surrogate function and the objective function under the candidate sampling distribution is greater than or equal to a predefined threshold. We prove that information-geometric optimization using such a surrogate function leads to a monotonic decrease in the expected objective function value if the threshold is sufficiently close to one. The acceptable threshold value is analyzed for the case of the information-geometric optimization instantiated with Gaussian distributions, i.e., the rank-μ update CMA-ES, on a convex quadratic objective function. As an alternative to the Kendall's rank correlation coefficient, we investigate the use of the Pearson correlation coefficient between the weights assigned to candidate solutions based on the objective function and the surrogate function.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1354–1362},
numpages = {9},
keywords = {Pearson's correlation coefficient, monotone improvement, covariance matrix adaptation evolution strategy, surrogate function, information-geometric optimization, Kendall's rank correlation coefficient},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@inproceedings{multifidelity,
author = {Akimoto, Y. and Shimizu, T. and Yamaguchi, T.},
title = {Adaptive Objective Selection for Multi-Fidelity Optimization},
year = {2019},
isbn = {9781450361118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321707.3321709},
doi = {10.1145/3321707.3321709},
abstract = {In simulation-based optimization we often have access to multiple simulators or surrogate models that approximate a computationally expensive or intractable objective function with different tradeoffs between the fidelity and computational time. Such a setting is called multi-fidelity optimization. In this paper, we propose a novel strategy to adaptively select which simulator to use during optimization of comparison-based evolutionary algorithms. Our adaptive switching strategy works as a wrapper of multiple simulators: optimization algorithms optimize the wrapper function and the adaptive switching strategy selects a simulator inside the wrapper, implying wide applicability of the proposed approach. We empirically investigate how efficiently the adaptive switching strategy manages simulator selection on test problems and theoretically investigate how it changes the fidelity level during optimization.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {880–888},
numpages = {9},
keywords = {adaptive objective switching, covariance matrix adaptation evolution strategy, multi-fidelity optimization},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@InProceedings{constrainedmultifidelity,
author="Akimoto, Y.
and Sakamoto, N.
and Ohtani, M.",
title="Multi-fidelity Optimization Approach Under Prior and Posterior Constraints and Its Application to Compliance Minimization",
booktitle="Parallel Problem Solving from Nature -- PPSN XVI",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="81--94",
abstract="In this paper, we consider a multi-fidelity optimization under two types of constraints: prior constraints and posterior constraints. The prior constraints are prerequisite to execution of the simulation that computes the objective function value and the posterior constraint violation values, and are evaluated independently from the simulation with significantly lower computational time than the simulation. We have several simulators that approximately simulate the objective and constraint violation values with different trade-offs between accuracy and computational time. We propose an approach to solve the described constrained optimization problem with as little computational time as possible by utilizing multiple simulators. Based on a covariance matrix adaptation evolution strategy, we combines three algorithmic components: prior constraint handling technique, posterior constraint handling technique, and adaptive simulator selection technique for multi-fidelity optimization. We apply the proposed approach to a compliance minimization problem and show a promising convergence behavior.",
isbn="978-3-030-58112-1",
doi={https://doi.org/10.1007/978-3-030-58112-1_6}
}

@inproceedings{lqcmaes,
author = {Hansen, N.},
title = {A Global Surrogate Assisted CMA-ES},
year = {2019},
isbn = {9781450361118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321707.3321842},
doi = {10.1145/3321707.3321842},
abstract = {We explore the arguably simplest way to build an effective surrogate fitness model in continuous search spaces. The model complexity is linear or diagonal-quadratic or full quadratic, depending on the number of available data. The model parameters are computed from the Moore-Penrose pseudoinverse. The model is used as a surrogate fitness for CMA-ES if the rank correlation between true fitness and surrogate value of recently sampled data points is high. Otherwise, further samples from the current population are successively added as data to the model. We empirically compare the IPOP scheme of the new model assisted lq-CMA-ES with a variety of previously proposed methods and with a simple portfolio algorithm using SLSQP and CMA-ES. We conclude that a global quadratic model and a simple portfolio algorithm are viable options to enhance CMA-ES. The model building code is available as part of the pycma Python module on Github and PyPI.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {664–672},
numpages = {9},
keywords = {CMA-ES, quadratic model, surrogate, evolution strategies, covariance matrix adaptation},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@inproceedings{evolutioncontrol,
author = {Pitra, Z. and Hanu\v{s}, M. and Koza, J. and Tumpach, J. and Hole\v{n}a, M.},
title = {Interaction between Model and Its Evolution Control in Surrogate-Assisted CMA Evolution Strategy},
year = {2021},
isbn = {9781450383509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449639.3459358},
doi = {10.1145/3449639.3459358},
abstract = {Surrogate regression models have been shown as a valuable technique in evolutionary optimization to save evaluations of expensive black-box objective functions. Each surrogate modelling method has two complementary components: the employed model and the control of when to evaluate the model and when the true objective function, aka evolution control. They are often tightly interconnected, which causes difficulties in understanding the impact of each component on the algorithm performance. To contribute to such understanding, we analyse what constitutes the evolution control of three surrogate-assisted versions of the state-of-the-art algorithm for continuous black-box optimization --- the Covariance Matrix Adaptation Evolution Strategy. We implement and empirically compare all possible combinations of the regression models employed in those methods with the three evolution controls encountered in them. An experimental investigation of all those combinations allowed us to asses the influence of the models and their evolution control separately. The experiments are performed on the noiseless and noisy benchmarks of the Comparing-Continuous-Optimisers platform and a real-world simulation benchmark, all in the expensive scenario, where only a small budget of evaluations is available.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {528–536},
numpages = {9},
keywords = {surrogate modelling, evolution control, evolutionary optimization, CMA-ES, black-box optimization},
location = {Lille, France},
series = {GECCO '21}
}

@inproceedings{minmax,
author = {Miyagi, A. and Akimoto, Y. and Yamamoto, H.},
title = {Well Placement Optimization under Geological Statistical Uncertainty},
year = {2019},
isbn = {9781450361118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321707.3321736},
doi = {10.1145/3321707.3321736},
abstract = {To control fluid flow in underground geologic formations, the placement of injection/production wells needs to be optimized taking geological characteristics of the reservoir into account. However, the optimum solution might not perform beneficially in the real world as the simulation result indicated because a reservoir model generally contains considerable geological uncertainty due to limited information of deep underground. Optimizing objective function integrating the response values from different models can be considered as an approach to this difficulty. In the previous study, such objective functions were proposed. However, their applicability has not been evaluated deeply. Therefore, their applicability was examined through well placement optimization for Carbon dioxide Capture and Storage (CCS) under geological statistical uncertainty as a case study. In the case study, we considered an optimization of the multiple wells for CO2 injection in a heterogeneous reservoir whose geological uncertainty was represented by 50 statistically independent reservoir models. As a result, the optimum solution with using all models showed the high applicability by comparing with the sensitivity analysis of nominal solutions, which is independently optimized for the single model, against the uncertainty. In addition, one of the proposed objective functions showed the superior result.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1284–1292},
numpages = {9},
keywords = {carbon dioxide capture and storage, robust optimization, geological statistical uncertainty, CMA-ES, well placement optimization},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@article{rios2013derivative,
  title={Derivative-free optimization: a review of algorithms and comparison of software implementations},
  author={Rios, L. M. and Sahinidis, N. V},
  journal={Journal of Global Optimization},
  volume={56},
  number={3},
  pages={1247--1293},
  year={2013},
  publisher={Springer}
}
@misc{nonconcavesmall,
  doi = {10.48550/ARXIV.2110.03950},
  url = {https://arxiv.org/abs/2110.03950},
  author = {Ostrovskii, D. M. and Barazandeh, B. and Razaviyayn, M.},
  title = {Nonconvex-Nonconcave Min-Max Optimization with a Small Maximization Domain},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{nonconcaveweakconcave,
author = {Liu, M. and Rafique, H. and Lin, Q. and Yang, T.},
title = {First-Order Convergence Theory for Weakly-Convex-Weakly-Concave Min-Max Problems},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {In this paper, we consider first-order convergence theory and algorithms for solving a class of non-convex non-concave min-max saddle-point problems, whose objective function is weakly convex in the variables of minimization and weakly concave in the variables of maximization. It has many important applications in machine learning including training Generative Adversarial Nets (GANs). We propose an algorithmic framework motivated by the inexact proximal point method, where the weakly monotone variational inequality (VI) corresponding to the original min-max problem is solved through approximately solving a sequence of strongly monotone VIs constructed by adding a strongly monotone mapping to the original gradient mapping. We prove first-order convergence to a nearly stationary solution of the original min-max problem of the generic algorithmic framework and establish different rates by employing different algorithms for solving each strongly monotone VI. Experiments verify the convergence theory and also demonstrate the effectiveness of the proposed methods on training GANs.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {169},
numpages = {34},
keywords = {variational inequality, min-max, weakly-convex-weakly-concave, first-order convergence, generative adversarial nets}
}

@InProceedings{nonconcaveweakmvi,
  title = 	 { Efficient Methods for Structured Nonconvex-Nonconcave Min-Max Optimization },
  author =       {Diakonikolas, J. and Daskalakis, C. and Jordan, M.},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2746--2754},
  year = 	 {2021},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/diakonikolas21a/diakonikolas21a.pdf},
  url = 	 {https://proceedings.mlr.press/v130/diakonikolas21a.html},
  abstract = 	 { The use of min-max optimization in the adversarial training of deep neural network classifiers, and the training of generative adversarial networks has motivated the study of nonconvex-nonconcave optimization objectives, which frequently arise in these applications. Unfortunately, recent results have established that even approximate first-order stationary points of such objectives are intractable, even under smoothness conditions, motivating the study of min-max objectives with additional structure. We introduce a new class of structured nonconvex-nonconcave min-max optimization problems, proposing a generalization of the extragradient algorithm which provably converges to a stationary point. The algorithm applies not only to Euclidean spaces, but also to general $\ell_p$-normed finite-dimensional real vector spaces. We also discuss its stability under stochastic oracles and provide bounds on its sample complexity. Our iteration complexity and sample complexity bounds either match or improve the best known bounds for the same or less general nonconvex-nonconcave settings, such as those that satisfy variational coherence or in which a weak solution to the associated variational inequality problem is assumed to exist. }
}


@inproceedings{nonconcavehidden,
 author = {Vlatakis-Gkaragkounis, E. V. and Flokas, L. and Piliouras, G.},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {2373--2386},
 publisher = {Curran Associates, Inc.},
 title = {Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},
 url={https://proceedings.neurips.cc/paper/2021/file/13bf4a96378f3854bcd9792d132eff9f-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{nonconvexpl,
 author = {Nouiehed, M. and Sanjabi, M. and Huang, T. and Lee, J. D. and Razaviyayn, M.},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Solving a Class of Non-Convex Min-Max Games Using Iterative First Order Methods},
 url = {https://proceedings.neurips.cc/paper/2019/file/25048eb6a33209cb5a815bff0cf6887c-Paper.pdf},
 volume = {32},
 year = {2019}
}



@inbook{minmaxcomplexity,
author = {Daskalakis, C. and Skoulakis, S. and Zampetakis, M.},
title = {The Complexity of Constrained Min-Max Optimization},
year = {2021},
isbn = {9781450380539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406325.3451125},
abstract = {Despite its important applications in Machine Learning, min-max optimization of objective functions that are nonconvex-nonconcave remains elusive. Not only are there no known first-order methods converging to even approximate local min-max equilibria (a.k.a. approximate saddle points), but the computational complexity of identifying them is also poorly understood. In this paper, we provide a characterization of the computational complexity as well as of the limitations of first-order methods in this problem. Specifically, we show that in linearly constrained min-max optimization problems with nonconvex-nonconcave objectives an approximate local min-max equilibrium of large enough approximation is guaranteed to exist, but computing such a point is PPAD-complete. The same is true of computing an approximate fixed point of the (Projected) Gradient Descent/Ascent update dynamics, which is computationally equivalent to computing approximate local min-max equilibria. An important byproduct of our proof is to establish an unconditional hardness result in the Nemirovsky-Yudin 1983 oracle optimization model, where we are given oracle access to the values of some function f : P → [−1, 1] and its gradient ∇ f, where P ⊆ [0, 1]d is a known convex polytope. We show that any algorithm that uses such first-order oracle access to f and finds an ε-approximate local min-max equilibrium needs to make a number of oracle queries that is exponential in at least one of 1/ε, L, G, or d, where L and G are respectively the smoothness and Lipschitzness of f. This comes in sharp contrast to minimization problems, where finding approximate local minima in the same setting can be done with Projected Gradient Descent using O(L/ε) many queries. Our result is the first to show an exponential separation between these two fundamental optimization problems in the oracle model.},
booktitle = {Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing},
pages = {1466–1478},
numpages = {13}
}


@misc{du2019,
      title={Linear Convergence of the Primal-Dual Gradient Method for Convex-Concave Saddle Point Problems without Strong Convexity}, 
      author={Simon S. D. and Wei H.},
      year={2019},
      eprint={1802.01504},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@INPROCEEDINGS{Adolphs2019,
	copyright = {In Copyright - Non-Commercial Use Permitted},
	year = {2019},
	booktitle = {Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS 2019)},
	volume = {89},
	editor = {Chaudhuri, Kamalika and Sugiyama, Masashi},
	author = {Adolphs, L. and Daneshmand, H. and Lucchi, A. and Hofmann, T.},
	journal = {Proceedings of Machine Learning Research},
	size = {10 p.},
	issn = {2640-3498},
	language = {en},
	address = {Cambridge, MA},
	publisher = {PMLR},
	DOI = {10.3929/ethz-b-000391340},
	title = {Local Saddle Point Optimization: A Curvature Exploitation Approach},
	PAGES = {486 - 495},
	Note = {22nd International Conference on Artificial Intelligence and Statistics (AISTATS 2019); Conference Location: Okinawa, Japan; Conference Date: April 16-18, 2019}
}

@inproceedings{Heusel@2017,
author = {Heusel, M. and Ramsauer, H. and Unterthiner, T. and Nessler, B. and Hochreiter, S.},
title = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6629–6640},
numpages = {12},
location = {Long Beach, California, USA},
series = {NIPS'17}
}


@inproceedings{Bogunovic2018,
author = {Bogunovic, I. and Scarlett, J. and Jegelka, S. and Cevher, V.},
title = {Adversarially Robust Optimization with Gaussian Processes},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {5765–5775},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@article{Conn2012,
author = { A.   R.   Conn  and  L.   N.   Vicente },
title = {Bilevel derivative-free optimization and its application to robust optimization},
journal = {Optimization Methods and Software},
volume = {27},
number = {3},
pages = {561-577},
year  = {2012},
publisher = {Taylor \& Francis},
doi = {10.1080/10556788.2010.547579},
URL = { 
        https://doi.org/10.1080/10556788.2010.547579    
},
eprint = { 
        https://doi.org/10.1080/10556788.2010.547579
}}



@InProceedings{Liu2020,
  title = 	 {Min-Max Optimization without Gradients: Convergence and Applications to Black-Box Evasion and Poisoning Attacks},
  author =       {Liu, S. and Lu, S. and Chen, X. and Feng, Y. and Xu, K. and Al-Dujaili, A. and Hong, M. and O'Reilly, U.},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {6282--6293},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/liu20j/liu20j.pdf},
  url = 	 {https://proceedings.mlr.press/v119/liu20j.html},
}

@INPROCEEDINGS{Barbosa1999,
  author={Barbosa, H. J. C.},
  booktitle={Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406)}, 
  title={A coevolutionary genetic algorithm for constrained optimization}, 
  year={1999},
  volume={3},
  number={},
  pages={1605-1611 Vol. 3},}

@INPROCEEDINGS{Herrmann1999,
  author={Herrmann, J. W.},
  booktitle={Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406)}, 
  title={A genetic algorithm for minimax optimization problems}, 
  year={1999},
  volume={2},
  number={},
  pages={1099-1103 Vol. 2},}

@ARTICLE{Tahk2000,
  author={ {Min-Jea Tahk} and  {Byung-Chan Sun}},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Coevolutionary augmented Lagrangian methods for constrained optimization}, 
  year={2000},
  volume={4},
  number={2},
  pages={114-124},}

@INPROCEEDINGS{Shi2002,
  author={ {Yuhui Shi} and R. A. {Krohling}},
  booktitle={Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No.02TH8600)}, 
  title={Co-evolutionary particle swarm optimization to solve min-max problems}, 
  year={2002},
  volume={2},
  number={},
  pages={1682-1687 vol.2},}

@INPROCEEDINGS{Krohling2004,
  author={R. A. {Krohling} and F. {Hoffmann} and L. S. {Coelho}},
  booktitle={Proceedings of the 2004 Congress on Evolutionary Computation (IEEE Cat. No.04TH8753)}, 
  title={Co-evolutionary particle swarm optimization for min-max problems using Gaussian distribution}, 
  year={2004},
  volume={1},
  number={},
  pages={959-964 Vol.1},}


@article{Hur2003,
author = { Jong   Hur  and  Hungu   Lee  and  Min-Jea   Tahk },
title = {Parameter robust control design using bimatrix co-evolution algorithms},
journal = {Engineering Optimization},
volume = {35},
number = {4},
pages = {417-426},
year  = {2003},
publisher = {Taylor \& Francis},
doi = {10.1080/0305215031000154659},
URL = { https://doi.org/10.1080/0305215031000154659},
eprint = { https://doi.org/10.1080/0305215031000154659}
}

@ARTICLE{Cramer2009,
  author={A. M. {Cramer} and S. D. {Sudhoff} and E. L. {Zivi}},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={Evolutionary Algorithms for Minimax Problems in Robust Design}, 
  year={2009},
  volume={13},
  number={2},
  pages={444-453},}


@ARTICLE{Qiu2018,
  author={X. {Qiu} and J. {Xu} and Y. {Xu} and K. C. {Tan}},
  journal={IEEE Transactions on Cybernetics}, 
  title={A New Differential Evolution Algorithm for Minimax Optimization in Robust Design}, 
  year={2018},
  volume={48},
  number={5},
  pages={1355-1368},}

@ARTICLE{Jensen2003,
author={Jensen M.T. },
year={2003}, 
title={A New Look at Solving Minimax Problems with Coevolutionary Genetic Algorithms}, 
booktitle={Metaheuristics: Computer Decision-Making. Applied Optimization},
volume={86},
publisher={Springer Boston},
doi={https://doi.org/10.1007/978-1-4757-4137-7_17},
}

@InProceedings{Branke2008,
author="B. J{\"u}rgen and R. Johanna",
title="New Approaches to Coevolutionary Worst-Case Optimization",
booktitle="Parallel Problem Solving from Nature -- PPSN X",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="144--153",
isbn="978-3-540-87700-4"
}

﻿@Article{Picheny2019,
author={Picheny, Victor
and Binois, Mickael
and Habbal, Abderrahmane},
title={A Bayesian optimization approach to find Nash equilibria},
journal={Journal of Global Optimization},
year={2019},
month={Jan},
day={01},
volume={73},
number={1},
pages={171-192},
issn={1573-2916},
doi={10.1007/s10898-018-0688-0},
url={https://doi.org/10.1007/s10898-018-0688-0}
}

@ARTICLE{Lu2020,
  author={Lu, Songtao and Tsaknakis, Ioannis and Hong, Mingyi and Chen, Yongxin},
  journal={IEEE Transactions on Signal Processing}, 
  title={Hybrid Block Successive Approximation for One-Sided Non-Convex Min-Max Problems: Algorithms and Applications}, 
  year={2020},
  volume={68},
  number={},
  pages={3676-3691},
  doi={10.1109/TSP.2020.2986363}}

@article{Al-Dujaili2019,
   title={On the application of Danskin’s theorem to derivative-free minimax problems},
   url={http://dx.doi.org/10.1063/1.5089993},
   DOI={10.1063/1.5089993},
   publisher={Author(s)},
   author={Al-Dujaili, A. and Srikant, S. and Hemberg, E. and O'Reilly, U.},
   year={2019}
}

@inproceedings{Srinivas2010,
author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham and Seeger, Matthias},
title = {Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {1015–1022},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}



@article{Pang2011,
author = {Pang, Jong-Shi and Scutari, Gesualdo},
title = {Nonconvex Games with Side Constraints},
journal = {SIAM Journal on Optimization},
volume = {21},
number = {4},
pages = {1491-1522},
year = {2011},
doi = {10.1137/100811787},
URL = { https://doi.org/10.1137/100811787},
eprint = { https://doi.org/10.1137/100811787}
}



@InProceedings{Adolphs2019,
  title = {Local Saddle Point Optimization: A Curvature Exploitation Approach},
  author = {Adolphs, Leonard and Daneshmand, Hadi and Lucchi, Aurelien and Hofmann, Thomas},
  booktitle =  {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {486--495},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/adolphs19a/adolphs19a.pdf},
  url = 	 {https://proceedings.mlr.press/v89/adolphs19a.html},
}


@misc{Dmitrii2021,
      title={Nonconvex-Nonconcave Min-Max Optimization with a Small Maximization Domain}, 
      author={D. M. Ostrovskii and B. Barazandeh and M. Razaviyayn},
      year={2021},
      eprint={2110.03950},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}


@misc{Maziar2018,
      title={Solving Non-Convex Non-Concave Min-Max Games Under Polyak-{\L}ojasiewicz Condition}, 
      author={Maziar Sanjabi and Meisam Razaviyayn and Jason D. Lee},
      year={2018},
      eprint={1812.02878},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}


@misc{Chi2020,
      title={What is Local Optimality in Nonconvex-Nonconcave Minimax Optimization?}, 
      author={Chi Jin and Praneeth Netrapalli and Michael I. Jordan},
      year={2020},
      eprint={1902.00618},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inbook{Thekumprampil2019,
author = {Thekumprampil, Kiran Koshy and Jain, Prateek and Netrapalli, Praneeth and Oh, Sewoong},
title = {Efficient Algorithms for Smooth Minimax Optimization},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {1136},
numpages = {12}
}


@misc{Hassan2021,
      title={Weakly-Convex Concave Min-Max Optimization: Provable Algorithms and Applications in Machine Learning}, 
      author={Hassan Rafique and Mingrui Liu and Qihang Lin and Tianbao Yang},
      year={2021},
      eprint={1810.02060},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}


@article{Mingrui2021,
  author  = {M. Liu and H. Rafique and Q. Lin and T. Yang},
  title   = {First-order Convergence Theory for Weakly-Convex-Weakly-Concave Min-max Problems},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {169},
  pages   = {1-34},
  url     = {http://jmlr.org/papers/v22/20-533.html}
}

@misc{Tianyi2021,
      title={On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems}, 
      author={Tianyi Lin and Chi Jin and Michael I. Jordan},
      year={2021},
      eprint={1906.00331},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@ARTICLE{Razaviyayn2020,
  author={Razaviyayn, M. and Huang, T. and Lu, S. and Nouiehed, M. and Sanjabi, M. and Hong, M.},
  journal={IEEE Signal Processing Magazine}, 
  title={Nonconvex Min-Max Optimization: Applications, Challenges, and Recent Theoretical Advances}, 
  year={2020},
  volume={37},
  number={5},
  pages={55-66},
  doi={10.1109/MSP.2020.3003851}}
  
@article{Maher2019,
title = "Solving a class of non-convex min-max games using iterative first order methods",
author = "Maher Nouiehed and Maziar Sanjabi and Tianjian Huang and Lee, {Jason D.} and Meisam Razaviyayn",
note = "Publisher Copyright: {\textcopyright} 2019 Neural information processing systems foundation. All rights reserved. Copyright: Copyright 2020 Elsevier B.V., All rights reserved.; 33rd Annual Conference on Neural Information Processing Systems, NeurIPS 2019 ; Conference date: 08-12-2019 Through 14-12-2019",
year = "2019",
language = "English (US)",
volume = "32",
journal = "Advances in Neural Information Processing Systems",
issn = "1049-5258",
}

@article{Bertsimas2010,
author = {Bertsimas, D. and Nohadani, O. and Teo, K. M.},
title = {Robust Optimization for Unconstrained Simulation-Based Problems},
journal = {Operations Research},
volume = {58},
number = {1},
pages = {161-178},
year = {2010},
doi = {10.1287/opre.1090.0715},
URL = {https://doi.org/10.1287/opre.1090.0715},
eprint = {https://doi.org/10.1287/opre.1090.0715}}


@article{Bertsimas2010b,
author = {Bertsimas, D. and Nohadani, O. and Teo, K. M.},
title = {Nonconvex Robust Optimization for Problems with Constraints},
journal = {INFORMS Journal on Computing},
volume = {22},
number = {1},
pages = {44-58},
year = {2010},
doi = {10.1287/ijoc.1090.0319},
URL = { https://doi.org/10.1287/ijoc.1090.0319},
eprint = {https://doi.org/10.1287/ijoc.1090.0319}
}

@InProceedings{Liang2019,
  title = 	 {Interaction Matters: A Note on Non-asymptotic Local Convergence of Generative Adversarial Networks},
  author =       {Liang, T. and Stokes, J.},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {907--915},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/liang19b/liang19b.pdf},
  url = 	 {https://proceedings.mlr.press/v89/liang19b.html},
  abstract = 	 {Motivated by the pursuit of a systematic computational and algorithmic understanding of Generative Adversarial Networks (GANs), we present a simple yet unified non-asymptotic local convergence theory for smooth two-player games, which subsumes several discrete-time gradient-based saddle point dynamics. The analysis reveals the surprising nature of the off-diagonal interaction term as both a blessing and a curse. On the one hand, this interaction term explains the origin of the slow-down effect in the convergence of Simultaneous Gradient Ascent (SGA) to stable Nash equilibria. On the other hand, for the unstable equilibria, exponential convergence can be proved thanks to the interaction term, for four modified dynamics proposed to stabilize GAN training: Optimistic Mirror Descent (OMD), Consensus Optimization (CO), Implicit Updates (IU) and Predictive Method (PM). The analysis uncovers the intimate connections among these stabilizing techniques, and provides detailed characterization on the choice of learning rate. As a by-product, we present a new analysis for OMD proposed in Daskalakis, Ilyas, Syrgkanis, and Zeng [2017] with improved rates.}
}


@article{akimoto2018,
  author    = {Youhei Akimoto and
               Anne Auger and
               Tobias Glasmachers},
  title     = {Drift Theory in Continuous Search Spaces: Expected Hitting Time of
               the {(1+1)-ES} with 1/5 Success Rule},
  journal   = {CoRR},
  volume    = {abs/1802.03209},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.03209},
  eprinttype = {arXiv},
  eprint    = {1802.03209},
  timestamp = {Mon, 13 Aug 2018 16:46:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1802-03209.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Inproceedings{Morinaga2021,
author = {Morinaga, D. and Fukuchi, K. and Sakuma, J. and Akimoto, Y.},
title = {Convergence Rate of the (1+1)-Evolution Strategy with Success-Based Step-Size Adaptation on Convex Quadratic Functions},
year = {2021},
isbn = {9781450383509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449639.3459289},
doi = {10.1145/3449639.3459289},
abstract = {The (1+1)-evolution strategy (ES) with success-based step-size adaptation is analyzed on a general convex quadratic function and its monotone transformation, that is, f(x) = g((x - x*)TH(x - x*)), where g: R → R is a strictly increasing function, H is a positive-definite symmetric matrix, and x* ∈ Rd is the optimal solution of f. The convergence rate, that is, the decrease rate of the distance from a search point mt to the optimal solution x*, is proven to be in O(exp(-L/Tr(H))), where L is the smallest eigenvalue of H and Tr(H) is the trace of H. This result generalizes the known rate of O(exp(-1/d)) for the case of H = Id (Id is the identity matrix of dimension d) and O(exp(-1/(d · ξ))) for the case of H = diag(ξ · Id/2, Id/2). To the best of our knowledge, this is the first study in which the convergence rate of the (1+1)-ES is derived explicitly and rigorously on a general convex quadratic function, which depicts the impact of the distribution of the eigenvalues in the Hessian H on the optimization and not only the impact of the condition number of H.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1169--1177},
numpages = {9},
keywords = {convex quadratic function, (1+1)-evolution strategy, convergence rate, convergence analysis},
location = {Lille, France},
series = {GECCO '21}
}


@article{akimoto2019,
    author = {Akimoto, Y. and Hansen, N.},
    title = "{Diagonal Acceleration for Covariance Matrix Adaptation Evolution Strategies}",
    journal = {Evolutionary Computation},
    volume = {28},
    number = {3},
    pages = {405-435},
    year = {2020},
    month = {09},
    abstract = "{We introduce an acceleration for covariance matrix adaptation evolution strategies (CMA-ES) by means of adaptive diagonal decoding (dd-CMA). This diagonal acceleration endows the default CMA-ES with the advantages of separable CMA-ES without inheriting its drawbacks. Technically, we introduce a diagonal matrix D that expresses coordinate-wise variances of the sampling distribution in DCD form. The diagonal matrix can learn a rescaling of the problem in the coordinates within a linear number of function evaluations. Diagonal decoding can also exploit separability of the problem, but, crucially, does not compromise the performance on nonseparable problems. The latter is accomplished by modulating the learning rate for the diagonal matrix based on the condition number of the underlying correlation matrix. dd-CMA-ES not only combines the advantages of default and separable CMA-ES, but may achieve overadditive speedup: it improves the performance, and even the scaling, of the better of default and separable CMA-ES on classes of nonseparable test functions that reflect, arguably, a landscape feature commonly observed in practice.The article makes two further secondary contributions: we introduce two different approaches to guarantee positive definiteness of the covariance matrix with active CMA, which is valuable in particular with large population size; we revise the default parameter setting in CMA-ES, proposing accelerated settings in particular for large dimension.All our contributions can be viewed as independent improvements of CMA-ES, yet they are also complementary and can be seamlessly combined. In numerical experiments with dd-CMA-ES up to dimension 5120, we observe remarkable improvements over the original covariance matrix adaptation on functions with coordinate-wise ill-conditioning. The improvement is observed also for large population sizes up to about dimension squared.}",
    issn = {1063-6560},
    doi = {10.1162/evco_a_00260},
    url = {https://doi.org/10.1162/evco\_a\_00260},
}



@inproceedings{sakamoto2019,
author = {Sakamoto, Naoki and Akimoto, Youhei},
title = {Adaptive Ranking Based Constraint Handling for Explicitly Constrained Black-Box Optimization},
year = {2019},
isbn = {9781450361118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321707.3321717},
doi = {10.1145/3321707.3321717},
pages = {700–708},
numpages = {9},
keywords = {CMA-ES, invariance, explicit constraint handling, black-box optimization},
location = {Prague, Czech Republic},
series = {GECCO '19}
}



@article{fujii2018,
author = {Fujii, G. and Akimoto, Y.  and Takahashi, M. },
title = {Exploring optimal topology of thermal cloaks by CMA-ES},
journal = {Applied Physics Letters},
volume = {112},
number = {6},
pages = {061108},
year = {2018},
doi = {10.1063/1.5016090},
URL = { https://doi.org/10.1063/1.5016090},
}

@article{marsden2004,
  title={Optimal aeroacoustic shape design using the surrogate management framework},
  author={Marsden, A. L. and Wang, M. and Dennis, J. E. and Moin, P.},
  journal={Optimization and Engineering},
  volume={5},
  number={2},
  pages={235--262},
  year={2004},
  publisher={Springer}
}

@inproceedings{urieli2011,
title={On Optimizing Interdependent Skills: A Case Study in Simulated 3D Humanoid Robot Soccer},
author={D. Urieli and P. MacAlpine and S. Kalyanakrishnan and Y. Bentor and P. Stone},
booktitle={Proc. of 10th Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS'11)},
month={May},
url="http://www.cs.utexas.edu/users/ai-lab?AAMAS11-urieli",
year={2011}
}


@article{maki2020,
  title={Application of optimal control theory based on the evolution strategy (CMA-ES) to automatic berthing},
  author={Maki, A. and Sakamoto, N. and Akimoto, Y. and Nishikawa, H. and Umeda, N.},
  journal={Journal of Marine Science and Technology},
  volume={25},
  number={1},
  pages={221--233},
  year={2020},
  publisher={Springer}
}


@article{Onwunalu2010,
author={Onwunalu, J. E. and Durlofsky, L. J.},
title={Application of a particle swarm optimization algorithm for determining optimum well location and type},
journal={Computational Geosciences},
year={2010},
volume={14},
number={1},
pages={183-198},
issn={1573-1499},
doi={10.1007/s10596-009-9142-1},
url={https://doi.org/10.1007/s10596-009-9142-1}
}

@inproceedings{miyagi@ghgt,
	author = {A. Miyagi and H. Yamamoto and Y. Akimoto and Z. Xue},
	title = {Parallel Workflow to Optimize Well Placement in Heterogeneous Reservoir using Covariance Matrix Adaptation Evolution Strategy},
	year = {2018},
	numpages = {10},
	location = {Melbourne},
	series = {GHGT-14}
}

@article{bouzarkouna2012,
  title={Well placement optimization with the covariance matrix adaptation evolution strategy and meta-models},
  author={Bouzarkouna, Z. and Ding, D. Y. and Auger, A.},
  journal={Computational Geosciences},
  volume={16},
  pages={75--92},
  year={2012},
  publisher={Springer},
  doi={https://doi.org/10.1007/s10596-011-9254-2}
}

@article{akimoto2022berthing,
author = {Akimoto, Y. and Miyauchi, Y. and Maki, A.},
title = {Saddle Point Optimization with Approximate Minimization Oracle and Its Application to Robust Berthing Control},
year = {2022},
doi = {10.1145/3510425},
volume = {2},
number = {1},
journal = {ACM Trans. Evol. Learn. Optim.},
}

@incollection{WANG20201,
title = {1 - Uncertainty quantification in materials modeling},
editor = {Y. Wang and D. L. McDowell},
booktitle = {Uncertainty Quantification in Multiscale Materials Modeling},
publisher = {Woodhead Publishing},
pages = {1-40},
year = {2020},
series = {Elsevier Series in Mechanics of Advanced Materials},
isbn = {978-0-08-102941-1},
doi = {https://doi.org/10.1016/B978-0-08-102941-1.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780081029411000018},
author = {Yan Wang and David L. McDowell},
keywords = {Integrated Computational Materials Engineering, Materials design, Multiscale simulation, Uncertainty quantification, Verification and validation},
}

@article{Freitas2002,
title = {The issue of numerical uncertainty},
journal = {Applied Mathematical Modelling},
volume = {26},
number = {2},
pages = {237-248},
year = {2002},
issn = {0307-904X},
doi = {https://doi.org/10.1016/S0307-904X(01)00058-0},
url = {https://www.sciencedirect.com/science/article/pii/S0307904X01000580},
author = {C. J. Freitas}
}

@inproceedings{Arnold2010,
author = {Arnold, D. V. and Hansen, N.},
title = {Active Covariance Matrix Adaptation for the (1+1)-CMA-ES},
year = {2010},
isbn = {9781450300728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1830483.1830556},
doi = {10.1145/1830483.1830556},
booktitle = {Proceedings of the 12th Annual Conference on Genetic and Evolutionary Computation},
pages = {385--392},
numpages = {8},
keywords = {variable metric algorithm, stochastic optimisation, covariance matrix adaptation, evolution strategy},
location = {Portland, Oregon, USA},
series = {GECCO '10}
}

@article{Miyauchi2022,
title = {Optimization on planning of trajectory and control of autonomous berthing and unberthing for the realistic port geometry},
journal = {Ocean Engineering},
volume = {245},
pages = {110390},
year = {2022},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.110390},
author = {Y. Miyauchi and R. Sawada and Y. Akimoto and N. Umeda and A. Maki},
}

@article{Oliveira2013,
author = {Oswaldo de Oliveira},
title = {{The Implicit and Inverse Function Theorems: Easy Proofs}},
volume = {39},
journal = {Real Analysis Exchange},
number = {1},
publisher = {Michigan State University Press},
pages = {207 -- 218},
keywords = {Calculus of Vector Functions, Differential Calculus, Functions of Several Variables, Implicit Function Theorems},
year = {2013},
doi = {rae/1404230147},
URL = {https://doi.org/}
}

@inproceedings{yamaguchi2018,
author = {Yamaguchi, T. and Akimoto, Y.},
title = {A Note on the CMA-ES for Functions with Periodic Variables},
year = {2018},
isbn = {9781450357647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205651.3205669},
doi = {10.1145/3205651.3205669},
abstract = {In this short paper, we reveal the issue of the covariance matrix adaptation evolution strategy when solving a function with periodic variables. We investigate the effect of a simple modification that the coordinate-wise standard deviation of the sampling distribution is restricts to the one-fourth of the period length. This is achieved by pre- and post-multiplying a diagonal matrix to the covariance matrix.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {227–228},
numpages = {2},
keywords = {mirroring, CMA-ES, periodic variables},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@Inbook{Hansen2014,
author={Hansen, N. and Auger, A.},
title={Principled Design of Continuous Stochastic Search: From Theory to Practice},
bookTitle={Theory and Principled Methods for the Design of Metaheuristics},
year={2014},
publisher={Springer Berlin Heidelberg},
address={Berlin, Heidelberg},
pages={145--180},
isbn={978-3-642-33206-7}}

@article{Hansen2001,
author = {Hansen, N. and Ostermeier, A.},
title = {Completely Derandomized Self-Adaptation in Evolution Strategies},
year = {2001},
volume = {9},
number = {2},
doi = {10.1162/106365601750190398},
journal = {Evol. Comput.},
pages = {159-195},
}

@article{invariance,
title = {Impacts of invariance in search: When CMA-ES and PSO face ill-conditioned and non-separable problems},
journal = {Applied Soft Computing},
volume = {11},
number = {8},
pages = {5755--5769},
year = {2011},
doi = {https://doi.org/10.1016/j.asoc.2011.03.001},
author = {N. Hansen and R. Ros and N. Mauny and M. Schoenauer and A. Auger},
}

@book{kendall,
  title={Rank Correlation Methods},
  edition={5th},
  author={M. Kendall and J. D. Gibbons},
  year={1990},
  publisher={Oxford University Press}
}

@inproceedings{hansen2019,
author = {Hansen, N.},
title = {A Global Surrogate Assisted CMA-ES},
year = {2019},
doi = {10.1145/3321707.3321842},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {664--672},
series = {GECCO '19}
}

@inproceedings{miyagi2021,
author = {Miyagi, A. and Fukuchi, K. and Sakuma, J. and Akimoto, Y.},
title = {Adaptive Scenario Subset Selection for Min--Max Black-Box Continuous Optimization},
year = {2021},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
series = {GECCO '21},
pages={697--705},
doi={10.1145/3449639.3459291},
}

@inproceedings{akimoto2019multi,
author = {Akimoto, Y. and Shimizu, T. and Yamaguchi, T.},
title = {Adaptive Objective Selection for Multi-Fidelity Optimization},
year = {2019},
doi = {10.1145/3321707.3321709},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {880--888},
series = {GECCO '19}
}


@inproceedings{Hansen2009,
author = {Hansen, N.},
title = {Benchmarking a BI-Population CMA-ES on the BBOB-2009 Function Testbed},
year = {2009},
isbn = {9781605585055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1570256.1570333},
doi = {10.1145/1570256.1570333},
booktitle = {Proceedings of the 11th Annual Conference Companion on Genetic and Evolutionary Computation Conference: Late Breaking Papers},
pages = {2389–2396},
numpages = {8},
keywords = {CMA-ES, benchmarking, black-box optimization, evolutionary computation},
location = {Montreal, Qu\'{e}bec, Canada},
series = {GECCO '09}
}

@inproceedings{Hansen2010,
author = {Hansen, N. and Auger, A. and Ros, R. and Finck, S. and Po\v{s}\'{\i}k, P.},
title = {Comparing Results of 31 Algorithms from the Black-Box Optimization Benchmarking BBOB-2009},
year = {2010},
isbn = {9781450300735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1830761.1830790},
doi = {10.1145/1830761.1830790},
booktitle = {Proceedings of the 12th Annual Conference Companion on Genetic and Evolutionary Computation},
pages = {1689–1696},
numpages = {8},
keywords = {benchmarking, black-box optimization},
location = {Portland, Oregon, USA},
series = {GECCO '10}
}

@inproceedings{tanabe2021,
author = {Tanabe, T. and Fukuchi, K. and Sakuma, J. and Akimoto, Y.},
title = {Level Generation for Angry Birds with Sequential VAE and Latent Variable Evolution},
year = {2021},
isbn = {9781450383509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449639.3459290},
doi = {10.1145/3449639.3459290},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1052–1060},
numpages = {9},
keywords = {sequential VAE, procedural content generation, AngryBirds, latent variable evolution},
location = {Lille, France},
series = {GECCO '21}
}

@inproceedings{Auger2018cec,
	title = {A Restart CMA Evolution Strategy With Increasing Population Size},
	year = 2005,
	author = {Auger, A. and Hansen, H.},
	booktitle = {Proceedings of 2005 IEEE Congress on Evolutionary Computation},
	series = {CEC '05},
	isbn = {0-7803-9363-5},
	location = {Edinburgh, Scotland},
	pages = {1769-1776},
	numpages = {8},
	doi = {10.1109/CEC.2005.1554902},
	publisher = {IEEE},
}

@article{Abdullah2019,
author = {Al-Dujaili,A. and Srikant, S.  and Hemberg, E.  and O'Reilly, U. },
title = {On the application of Danskin's theorem to derivative-free minimax problems},
journal = {AIP Conference Proceedings},
volume = {2070},
number = {1},
pages = {020026},
year = {2019},
doi = {10.1063/1.5089993},
URL = { https://aip.scitation.org/doi/abs/10.1063/1.5089993},
}

@techreport{kraft1988,
  title={A software package for sequential quadratic programming},
  author={Kraft, D.},
  year={1988},
  series={Technical Report},
  note={DFVLR-FB 88-28, DLR German Aerospace Center – Institute for Flight Mechanics, Koln, Germany}
}

@article{Oberkampf2002,
title = {Error and uncertainty in modeling and simulation},
journal = {Reliability Engineering \& System Safety},
volume = {75},
number = {3},
pages = {333-357},
year = {2002},
issn = {0951-8320},
doi = {https://doi.org/10.1016/S0951-8320(01)00120-X},
url = {https://www.sciencedirect.com/science/article/pii/S095183200100120X},
author = {W. L. Oberkampf and S. M. DeLand and B. M. Rutherford and K. V. Diegert and K. F. Alvin},
keywords = {Modeling, Simulation, Nondeterministic features, Epistemic uncertainty, Subjective uncertainty, Aleatory uncertainty, Stochastic uncertainty},
}

@article{walker2003,
author = { W.E. Walker and  P. Harremoës  and  J. Rotmans and J.P. van der Sluijs  and  M.B.A. van Asselt and  P. Janssen and M.P. Krayer von Krauss},
title = {Defining Uncertainty: A Conceptual Basis for Uncertainty Management in Model-Based Decision Support},
journal = {Integrated Assessment},
volume = {4},
number = {1},
pages = {5-17},
year  = {2003},
publisher = {Taylor & Francis},
doi = {10.1076/iaij.4.1.5.16466},
URL = { 
        https://doi.org/10.1076/iaij.4.1.5.16466
},
eprint = { 
        https://doi.org/10.1076/iaij.4.1.5.16466}
}

@phdthesis{bouzarkouna2012phd,
  TITLE = {{Well placement optimization}},
  AUTHOR = {Bouzarkouna, Z.},
  URL = {https://tel.archives-ouvertes.fr/tel-00690456},
  SCHOOL = {{Universit{\'e} Paris Sud - Paris XI}},
  YEAR = {2012},
  TYPE = {Theses},
  PDF = {https://tel.archives-ouvertes.fr/tel-00690456/file/VD2_BOUZARKOUNA_ZYED_03042012.pdf},
  HAL_ID = {tel-00690456},
  HAL_VERSION = {v1},
}

@article{chen2013,
author = {P. Chen and Quarteroni A. and Rozza G.},
title = {Simulation-based uncertainty quantification of human arterial network hemodynamics},
journal = {International Journal for Numerical Methods in Biomedical Engineering},
volume = {29},
number = {6},
pages = {698-721},
keywords = {uncertainty quantification, sensitivity analysis, stochastic collocation method, cardiovascular modeling, human arterial network, wave propagation, hemodynamics},
doi = {https://doi.org/10.1002/cnm.2554},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cnm.2554},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cnm.2554},
year = {2013}
}

@INPROCEEDINGS{Anna2018,
  author={A. P. G. Scheidegger and A. Banerjee and T. F. Pereira},
  booktitle={2018 Winter Simulation Conference (WSC)}, 
  title={UNCERTAINTY QUANTIFICATION IN SIMULATION MODELS: A PROPOSED FRAMEWORK AND APPLICATION THROUGH CASE STUDY}, 
  year={2018},
  volume={},
  number={},
  pages={1599-1610},
  doi={10.1109/WSC.2018.8632281}
}

@inproceedings{akimoto2022cmaes,
author = {Akimoto, Y. and Hansen, N.},
title = {CMA-ES and Advanced Adaptation Mechanisms},
year = {2022},
isbn = {9781450392686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520304.3533648},
doi = {10.1145/3520304.3533648},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1243–1268},
numpages = {26},
location = {Boston, Massachusetts},
series = {GECCO '22}
}
  

@article{miyagi2023,
title = {Adaptive scenario subset selection for worst-case optimization and its application to well placement optimization},
journal = {Applied Soft Computing},
volume = {133},
pages = {109842},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.109842},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622008912},
author = {A. Miyagi and K. Fukuchi and J. Sakuma and Y. Akimoto},
keywords = {Worst-case optimization, Simulation-based optimization, Support scenarios, Adaptive scenario subset selection, Covariance matrix adaptation evolution strategy (CMA-ES)},
}

@inproceedings{Aitokhuehi2004,
   author = "Durlofsky, L. J. and Aitokhuehi, I. and Artus, V. and Yeten, B. and Aziz, K.",
   title = "Optimization of Advanced Well Type and Performance", 
   year = "2004",
   doi = "https://doi.org/10.3997/2214-4609-pdb.9.B031",
   url = "https://www.earthdoc.org/content/papers/10.3997/2214-4609-pdb.9.B031",
   series = "9th European Conference on the Mathematics of Oil Recovery",
   issn = "2214-4609",
  }


@article{Artus2006comgeo,
author={Artus, V. and Durlofsky, L. J. and Onwunalu, J. and Aziz, K.},
title={Optimization of nonconventional wells under uncertainty using statistical proxies},
journal={Computational Geosciences},
year={2006},
volume={10},
number={4},
pages={389-404},
issn={1573-1499},
doi={10.1007/s10596-006-9031-9},
url={https://doi.org/10.1007/s10596-006-9031-9}
}


@article{Yeten2003spe,
author={Yeten, B. and Durlofsky, L. J. and Aziz, K.},
title={Optimization of Nonconventional Well Type, Location, and Trajectory},
journal={SPE Journal},
year={2003},
volume={8},
number={03},
pages={200-210},
issn={1086-055X},
doi={10.2118/86880-PA},
url={https://doi.org/10.2118/86880-PA}
}


@inproceedings{igel2006,
author = {Igel, C. and Suttorp, T. and Hansen, N.},
title = {A Computational Efficient Covariance Matrix Update and a (1+1)-CMA for Evolution Strategies},
year = {2006},
isbn = {1595931864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1143997.1144082},
doi = {10.1145/1143997.1144082},
booktitle = {Proceedings of the 8th Annual Conference on Genetic and Evolutionary Computation},
pages = {453–460},
numpages = {8},
keywords = {evolution strategy, Cholesky factors, covariance matrix adaptation, rank-one update},
location = {Seattle, Washington, USA},
series = {GECCO '06}
}

@INPROCEEDINGS{Jastrebski2006,
  author={Jastrebski, G.A. and Arnold, D.V.},
  booktitle={2006 IEEE International Conference on Evolutionary Computation}, 
  title={Improving Evolution Strategies through Active Covariance Matrix Adaptation}, 
  year={2006},
  volume={},
  number={},
  pages={2814-2821},
  doi={10.1109/CEC.2006.1688662}}
  