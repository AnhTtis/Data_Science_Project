% journal suggestion : IEEE Access or ACM TELO (invited)

% TODO
% 実験の箇所におけるコメントの対応
% Conclusion の Hyperparameter に関する記述（取り除くのも一つ）
% Abstract のrevise
% ACM Telo のスタイルに変更
% Reference の表記がおかしくないか，間違いがないか，確認
% ノーテーションが統一されているか確認
% Code公開（投稿後でも大丈夫）

%\documentclass[xelatex,ja=standard,jafont=noto]{bxjsarticle}
\documentclass{ieeeaccess}

% Graphics
\usepackage{graphicx}
\graphicspath{{./}} % ----- Enter the path to the directory of figures ----- %
% Tables and Figures
\usepackage{subcaption}
\usepackage{booktabs} % or \usepackage{tabu}?
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{makecell}
% Bibtex related packages
\usepackage{cite} % align the numbers of references automatically
%\usepackage{amsrefs} % gives nice reference style
%\bibliographystyle{amsalpha} % ams style citation
\bibliographystyle{plain}
\usepackage{comment}


%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++%
%+++ Math Packages and Commands 
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++%
% Packages
\usepackage{latexsym}
\usepackage{amsmath} % AMS
\usepackage{amssymb}
\usepackage{mathtools}       % many extension of amsmath
\usepackage{stmaryrd}        % \llbracket, \rrbracket
\usepackage{bm}              % bold font
% Mathematical commands that are recursively used
\providecommand{\argmax}{\operatornamewithlimits{argmax}} % argmax
\providecommand{\argmin}{\operatornamewithlimits{argmin}} % argmin
\providecommand{\limsup}{\operatornamewithlimits{limsup}} % limsup
\providecommand{\liminf}{\operatornamewithlimits{liminf}} % liminf
\DeclareMathOperator{\Tr}{Tr}     % trace
\DeclareMathOperator{\Var}{Var}   % variance
\DeclareMathOperator{\Cov}{Cov}   % covariance
\DeclareMathOperator{\Cond}{Cond} % condition number
\DeclareMathOperator{\diag}{diag} % diagonal
\DeclareMathOperator{\vect}{vec}  % vectorization
\DeclareMathOperator{\vech}{vech} % half vectorization
\providecommand{\R}{\mathbb{R}} % real field
\providecommand{\E}{\mathbb{E}} % expectation
\providecommand{\T}{\mathrm{T}} % transpose
\providecommand{\rmd}{\mathrm{d}} % d for infinitesimal
\providecommand{\ind}[1]{\mathbb{I}\{#1\}} % indicator function
\renewcommand{\geq}{\geqslant} % geq
\renewcommand{\leq}{\leqslant} % leq
% Paired Delimiter from mathtools
% E.g., \inner{}{}, \inner*{}{}, \inner[Big]{}{}
% Or,   \norm{}, \norm*{}, \norm[Big]{}
\DeclarePairedDelimiterX{\inner}[2]{\langle}{\rangle}{#1, #2}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
% AMS theorem
% Sometimes theorem environments and proof environments are already
% defined in a style file. In such a case the following should be
% commented out.
\usepackage{amsthm}
% Editing commands
\usepackage{xcolor}
\usepackage{ifthen}
% \usepackage{showlabels} % uncomment in the final version
% \usepackage{showkeys} % alternative
% Hyperref
\usepackage[colorlinks=false,pdfborder={0 0 0}]{hyperref}
\usepackage{cleveref}
% Must be placed after cleveref
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
% Times fonts
% Font related packages sometimes cause errors. In such a case,
% include these packages at the end of the preamble.
%\usepackage{newtxtext}
%\usepackage{newtxmath}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newcommand{\CC}{\mathbf{C}}
\DeclareMathOperator{\sign}{sign}

\providecommand{\wracma}{\texttt{WRA-CMA}}
\providecommand{\wraslsqp}{\texttt{WRA-AGA}}
\providecommand{\wracmaadv}{\texttt{WRA-CMA+ADV}}
\providecommand{\wraslsqpadv}{\texttt{WRA-AGA+ADV}}
\providecommand{\wra}{\texttt{WRA}}
\providecommand{\acma}{\texttt{ADV-CMA-ES}}
\providecommand{\zopgda}{\texttt{ZOPGDA}}
\providecommand{\cmaes}{\texttt{CMA-ES}}
\providecommand{\slsqp}{\texttt{SLSQP}}

\providecommand{\yw}{\hat{y}}% worst scenario
\providecommand{\ywset}{\hat{Y}}% worst scenario set

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
% \title{CMA-ES with Worst-Case Ranking Approximation for Min-Max Optimization}
% \author{}
% \date{}

\begin{document}
\history{Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.}
\doi{10.1109/ACCESS.2017.DOI}

\title{CMA-ES with Worst-Case Ranking Approximation for Min-Max Optimization and its Application to  Berthing Control Tasks}
\author{
% \author{\uppercase{Atsuhiro Miyagi}\authorrefmark{1}, 
% \IEEEmembership{Fellow, IEEE},
\uppercase{Atsuhiro Miyagi\authorrefmark{1,2}, Yoshiki Miyauchi\authorrefmark{3}, Atsuo Maki\authorrefmark{3}, Kazuto Fukuchi \authorrefmark{2,4}, Jun Sakuma\authorrefmark{2,4}, and Youhei Akimoto \authorrefmark{2,4}}}
% \IEEEmembership{Member, IEEE}}
\address[1]{Taisei corporation, Yokohama, Kanagawa, 245-0051 Japan}
\address[2]{University of Tsukuba, Tsukuba, Ibaraki 305-8573 Japan}
\address[3]{Department of Naval Architecture and Ocean Engineering, Graduate School of Engineering, Osaka University, 2-1 Yamadaoka, Suita, Osaka 565-0971, Japan}
\address[4]{RIKEN Center for Advanced Intelligence Project, Chuo-ku, Tokyo 103-0027 Japan}
\tfootnote{
% This paragraph of the first footnote will contain support information, including sponsor and financial support acknowledgment. For example, ``This work was supported in part by the U.S. Department of Commerce under Grant BS123456.''
This paper is partially supported by JSPS KAKENHI Grant Number 19H04179 and 22H01701.
}

\markboth
{Author \headeretal: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS}
{Author \headeretal: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS}

\corresp{Corresponding author: Atsuhiro Miyagi (e-mail: mygath00@pub.taisei.co.jp) and Youhei Akimoto (e-mail: akimoto@cs.tsukuba.ac.jp).}

\begin{abstract}

% 本研究では、目的関数がblack--boxであるmin--max continuous optimization problem $\min_{x \in \mathcal{X} \max_{y \in \mathcal{Y}}}f(x,y)$について検討した。この中で、ワーストケース関数$F(x)=\max_{y \in \mathcal{Y}}f(x,y)$における最適解$x^*$のワーストシナリオ集合$\ywset (x^*)$が以下のsituationとなる問題に着目した。
% situation (a) : $\abs{\ywset} (x^*) = 1$で設計変数$x$の変化に対してでワーストシナリオが連続でsensitiveである。
% situation (b) : $\abs{\ywset} (x^*) > 1$で設計変数$x$の変化に対してでワーストシナリオは不連続である。
% Situation (a)と(b)に対処するために新しいアプローチを提案した。提案アプローチでは、推定されたワーストケース関数値に基づく解候補のランキングを使用して最適化アルゴリズムcmaesにより最適解を探査する。ワーストケース関数値に基づく解候補のランキングを推定するためにwraメカニズムを提案した。situation (a)と(b)を含むテスト問題に我々の提案アプローチと既存アプローチを適用し数値実験を行った。その結果、既存アプローチは最適化に失敗し、提案アプローチのみが最適解に収束しsituation (a)と(b)に対処できることが示された。
% さらに、the robust berthing control problem に提案アプローチを適用した。その結果、風の不確実性に対してロバストな解を安定して求められる点で、提案アプローチは有用であることが示された。


In this study, we consider min--max continuous optimization problem $\min_{x \in \mathbb{X} \max_{y \in \mathbb{Y}}}f(x,y)$ whose objective function is black--box. To obtain the global min--max solution $x^*$ minimizing the worst-case function $F(x)=\max_{y \in \mathbb{Y}}f(x,y)$, a popular approach update $x$ and $y$ simultaneously and alternatively. However, the previous studies have reported the limitations of the existing approaches in the following situations.
Situation (a) : $x^*$ is forms a strict global min--max saddle point $(\tilde{x}, \tilde{y})$ such that $f(\tilde{x}, y) < f(\tilde{x}, \tilde{y}) < f(x, \tilde{y})$ for any $x \in \mathbb{X} \setminus \tilde{x}$ and $y \in \mathbb{Y} \setminus \tilde{y}$, and the worst scenario $\yw(x) = \argmax_{y \in \mathbb{Y}}f(x,y)$ is sensitive with changing $x$.
Situation (b) : $x^*$ does not form a strict global min--max saddle point of $f$. 
To address situations (a) and (b), we propose a new approach. The proposed approach aims to minimize the worst-case function directory using the covariance matrix adaptation strategy, in which the rankings of the solution candidates are approximated by our proposed Worst-case Ranking Approximation (\wra{}) mechanism.
Numerical experiments were conducted by applying our proposed approach and the existing approaches to test problems involving situations (a) and (b). The results showed that only the proposed approaches among the applied approaches could address the situations (a) and (b).
Furthermore, the effectiveness of the proposed approach was demonstrated for the robust berthing control problem in which wind condition is uncertain. 
As a result of $20$ independent trials, in comparison with the existing approaches, the proposed approaches could stably obtain the robust solution, while the existing approaches output the solution colliding with the berth in many trials. 

\end{abstract}


\begin{keywords}
% Enter key words or phrases in alphabetical order, separated by commas. For a list of suggested keywords, send a blank e-mail to keywords@ieee.org or visit \underline{http://www.ieee.org/organizations/pubs/ani\_prod/keywrd98.txt}
Black-Box Min–Max Continuous Optimization, Covariance Matrix Adaptation Evolution Strategy, Non-convex Non-concave function, Robust Berthing Control Problem, Worst-case Ranking Approximation 
\end{keywords}

\titlepgskip=-15pt

\maketitle

% DeepL （https://www.deepl.com/en/translator）はかなりいい仕事をしてくれます．
% 日本語が適切であれば，段落ごとに機械翻訳するだけでかなり良い質の英語になります．

% 全体に関するコメントはここにまとめます．

% \paragraph{ストーリー}
% Difficulty (I) や (II) に関するストーリーが良くありません．
% Related works の前半までは，問題の特徴(I)--(IV)に着目して関連研究が紹介されています．
% Difficulty (II) は特徴(III)--(IV)に関するものなので，いいかと思います．一方，Difficulty (I) は特徴(III)--(IV)とは無関係であり，関連研究も述べられていません．
% この論文で考える問題の仮定と沿ったものになっている必要があります．
% 一方で，例えば問題の特徴として，Difficulty (I) に関するもの，Difficulty (II)に関するもの（現在の特徴(III)と(IV)）のいずれかを有する問題を考える，とすることも，あまりよくありません．別々の論文にすべきと考えられるからです．

% うまくいくことが実験で確認されたすべての問題を網羅するような仮定を設けようとしているように見えますが，やるべきは逆です．実験で確認されたもので包含できるような仮定にした方がいいです．（逆に実験は網羅的であるべきだからです．）


% \paragraph{問題の仮定}
% 上と関連します．特徴(III)と特徴(IV)という切り口が，あまり良くない気がします．まず，いずれもほとんど何も言っていないように見えます．〜とは限らない，としか言っていないからです．例えばstrongly convex-concaveなら，WRAを採用する必要はなく，もっと効率的な方法があるのではないでしょうか？であるとすれば，〜とは限らないとするのではなく，WRAが他の手法よりも優れている状況を仮定として述べるべきです．

% 加えて，特徴(III)と特徴(IV)がほとんど包含関係にあるように思います．ワーストシナリオが不連続ならば，non-concaveだと思いますが，違いますか？だとすれば，特徴（III）を言えば特徴（IV）がそもそも成立しているような気がします．ですから，切り分けがあまり良くないように思います．

% Difficulty (I) や Difficulty (II) につながるような問題の仮定にすることが必要です．
% 例えば，特徴(IV)はなくし，特徴(III)を「設計変数の変化に対して，ワーストシナリオの変化が不連続もしくは連続であるが非常にsensitiveである」などとしてみてはどうでしょうか．こうすると，Difficulty (I) や Difficulty (II) につながるのではないでしょうか．



\section{Introduction} \label{sec:intro}


\paragraph{Background}

%近年、simulation-based optimizationが幅広い分野で注目されている。Simulation-based optimizationは、最適化アルゴリズムを使用して最適な設計案を探査する技術であり、設計変数$x \in \mathbb{X}$に対する目的関数値$h:\mathbb{X} \rightarrow \mathbb{R}$を数値シミュレーションで評価する。我々が対象とする数値シミュレーションとは、決定論的シミュレーションである。既往の研究では、berthing control\cite{maki2020, Miyauchi2022}, well placement\cite{miyagi@ghgt, bouzarkouna2012, Onwunalu2010}, topology design\cite{fujii2018, marsden2004}等、様々な問題へ適用された事例が報告されている。simulation-based optimizationを使用する前提として、数値シミュレーションで目的関数を高い精度で評価できることが重要である。例えば、数値シミュレーションとしてFinite Element Method (FEM) を用いる場合、支配方程式や構成則に関わる係数や外力項等の条件を適切に決定し、ある解について実世界で得られる目的関数値$h_{\mathrm{real}}(x)$を高い精度で評価できることが重要である。つまり、$h(x) \approx h_{\mathrm{real}}(x)$とできる数値シミュレータを用意することが重要である。一般的に、これらの数値シミュレーションの条件は事前調査をもとに専門技術者により決定されるが、不確実性が含まれることが多い。そのため、数値シミュレーションによって評価される目的関数にも不確実性が含まれる場合が多い\cite{Freitas2002, Wang20201}。simulation-based optimizationを実際の問題で使用する場合は、数値シミュレーションに含まれる不確実性に対してロバストな解が得られることが望ましい。

Simulation-based optimization becomes an attractive technique for various industrial fields. 
% The optimization explores optimal design by using  optimization algorithms, and evaluates the objective function value $h:\mathbb{X} \rightarrow \mathbb{R}$ for the design variable $x \in \mathbb{X}$ by numerical simulation.
Given a design vector $x \in \mathbb{X} \subseteq R^m$, the objective function $h_{\mathrm{sim}} : \mathbb{X} \to R$ is evaluated through numerical simulation. 
%Our numerical simulation is  a deterministic simulation. 
Simulation-based optimization have been applied to many real world applications, including berthing control \cite{maki2020, Miyauchi2022}, well placement \cite{miyagi@ghgt, bouzarkouna2012, Onwunalu2010}, and topology design \cite{fujii2018, marsden2004}. 
To perform simulation-based optimization, it is necessary to determine simulation condition in advance so that the objective function value in the real world, $h_{\mathrm{real}}(x)$, is computed accurately. 
% For example, when using the Finite Element Method (FEM) as a numerical simulation, it is important to appropriately determine the coefficients related to the governing equations and constitutive law, external force terms, and other conditions. 
In other words, a simulator such that $h_{\mathrm{sim}}(x) \approx h_{\mathrm{real}}(x)$ must be prepared. 
However, due to some uncertainties in the real world, these predetermined conditions often contain errors and hence $h_{\mathrm{sim}}(x)$ does not approximate $h_{\mathrm{real}}(x)$ well. 
In such situations, there is a risk that the optimal solution obtained in simulation based optimization, $x_{\mathrm{sim}} = \argmin_{x \in \mathbb{X}} h_{\mathrm{sim}}(x)$, does not perform well in the real world and results in $h_{\mathrm{real}}(x_\mathrm{sim}) \gg h_{\mathrm{sim}}(x_\mathrm{sim})$.
%Therefore, when simulation-based optimization is applied under uncertainties, it is important to obtain solutions that are robust to the uncertainties.% involved in numerical simulation.


%ロバストな解を求めるために、berthing control\cite{akimoto2022berthing}やelectromagnetic scattering design\cite{Bertsimas2010}を対象とした既往の研究では、以下のようなmin--max問題が検討された。
%\begin{align}%
% 	&\min_{x \in \mathbb{X}} \max_{y \in \mathbb{Y}} f(x, y) , \label{eq:minmax} 
% \end{align}
% ここで、$f(x,y)$は目的関数、$x \in \mathbb{X} \subseteq R^m$は設計変数で$y \in \mathbb{Y} \subseteq R^n$はシミュレーションの条件として専門技術者により決定されるパラメータで、最適化段階では不確実なパラメータである（本稿ではシナリオ変数と呼称する）。min--max問題では、ワーストケース関数$F(x):=\max_{y \in \mathbb{Y}} f(x, y)$を最小化できる最適解$x^*=\argmin_{x \in \mathbb{X}}F(x)$を求めることを目的としている。シミュレーション条件の不確実性を考慮しない場合は、専門技術者により決定されたシミュレーション条件の推奨値$y_\mathrm{est} \in \mathbb{Y}$を使用して、最適解を探査する。つまり、$h(x) = f(x, y_\mathrm{est}) $に対して最適解を探査する。しかし、この推定値$y_\mathrm{est}$が実際の環境$y_\mathrm{real}$と異なっている場合、実際の環境では解$x_{y_\mathrm{est}}=\argmin_{x \in \mathbb{X}} f(x,y_\mathrm{est})$の目的関数値は大きく、$f(x_{y_\mathrm{est}}, y_\mathrm{est}) \ll f(x_{y_\mathrm{est}}, y_\mathrm{real})$となることが懸念される。一方、\eqref{eq:minmax}では、実際の環境で少なくとも発揮される性能関数，すなわちワーストケース関数 $F(x)=\max_{y \in \mathbb{Y}}f(x,y) \ge f(x,y_\mathrm{real})$，について最適解を探査する。そのため、最適解$x^*$の目的関数値は、不確実性に左右されず、少なくとも$F(x^*)$となることが期待できる。適切な$y_\mathrm{est}$を選定できない場合、\eqref{eq:minmax}に示すmin--max問題を考えることは重要である。

One approach to find a robust solution is to formulate the problem as a min--max optimization problem 
\begin{align}
	&\min_{x \in \mathbb{X}} \max_{y \in \mathbb{Y}} f(x, y) , \label{eq:minmax} 
\end{align}
where $f(x,y)$ is the objective function and $y \in \mathbb{Y} \subseteq R^n$ is a parameter vector for simulation conditions and is uncertain at the optimization stage, which is referred to as the scenario vector in this study. 
%Previous studies demonstrated this approach on real-world problems such as berthing controller design \cite{akimoto2022berthing} and electromagnetic scattering design \cite{Bertsimas2010}.
This approach aims to find the global min--max solution $x^*=\argmin_{x \in \mathbb{X}} F(x)$ that minimizes the worst-case function $F(x):=\max_{y \in \mathbb{Y}} f(x, y)$. 
In this formulation, the simulator designed by an expert engineer, $h_{\mathrm{sim}}$, corresponds to $f(\cdot, y_{\mathrm{sim}})$ with a scenario vector $y_\mathrm{sim} \in \mathbb{Y}$, and the real world objective, $h_\mathrm{real}$, corresponds to $f(\cdot, y_\mathrm{real})$ with a scenario vector $y_\mathrm{real} \in \mathbb{Y}$ which is unknown and may change over time.
Minimizing the worst-case function $F$ leads to minimizing the upper bound of the objective function value in the real world, i.e., $F(x) \geq f(x,y_\mathrm{real})$ as long as $y_\mathrm{real} \in \mathbb{Y}$.
% The optimal solution to min--max problem \eqref{eq:minmax}, $x^*$, guarantees $F(x^*) \geq f(x^*,y_\mathrm{real})$ as long as $y_\mathrm{real} \in \mathbb{Y}$.
%, whereas the minimization of $h_{\mathrm{sim}}$ may not result in minimizing $h_\mathrm{real}$ and it may happen that $h_{\mathrm{real}}(x_\mathrm{sim}) \gg h_{\mathrm{sim}}(x_\mathrm{sim}) $.
% Therefore, the objective function value at the optimal solution $x^*$ obtained by \eqref{eq:minmax} is expected to be at least $F(x^*)$, independent of uncertainty of the simulation conditions. When an appropriate scenario cannot be estimated, considering the min--max problem \eqref{eq:minmax} is one of approach to obtain a robust solution.


% 本研究では、simulation-based optimizationにおいて、数値シミュレーションの不確実性に対してロバストな解が求められる手法の開発を目的とし、min--max問題を対象とした新しいアプローチを提案する。今回取り扱う問題では、以下の三点を仮定する。
% (I) : 目的関数$f$の$x$や$y$に対する勾配は使用できないこととする。
% (II) : 目的関数$f$とワーストケース関数$F$は明示的でない(Black-box関数)こととする。本稿では、このような問題を black-box min--max optimizationと呼称する。さらに、本研究では、各解$x$のワーストシナリオ集合$\ywset(x) = \{y|F(x)=f(x, y)\}$の変化について以下の点を有する問題に着目する。
% (III) : ワーストケース関数の局所解周辺における設計変数の変化に対して，ワーストシナリオの変化が不連続もしくは連続であるが非常にsensitiveである。多くの既存研究において，とりわけ理論解析において，目的関数がシナリオ変数に対してconcaveであることが仮定されている．しかし，実際の問題では、$y$に関する目的関数はnon-concaveなケースが報告されており\cite{Razaviyayn2020, Bertsimas2010}、このような問題ではワーストシナリオの変化は不連続となる場合がある。
% 上記3点を踏まえて、本研究では、black-box min--max  optimizationにおける (III) を考慮できるderivative-freeな最適化手法について検討する。


In this study, we propose a novel approach for simulation-based optimization to obtain a robust solution against uncertainties involved in the numerical simulations. We assume that
the gradient of the objective function $f$ with respect to $x$ and $y$ is unavailable, and that
the objective function $f$ and the worst-case function $F$ are non-explicit (black-box functions). 
In this paper, we refer to such a problem as a black-box min--max optimization. 
In particular, we focus on the following two types of problems, for which the existing approaches \cite{akimoto2022berthing,Liu2020} for the black-box min--max optimization have been reported to fail to converge or to exhibit slow convergence: 
(A) $f$ is smooth and strongly convex--concave around $x^*$, but a strong interaction between $x$ and $y$ exists; or
(B) $f$ is not smooth or strongly convex--concave around $x^*$. 
%The latter situation happens when $f$ is non-concave in $y$ \cite{DBLP:conf/icml/JinNJ20}, and 
It has been reported that the objective function with respect to $y$ in real-world applications is often non-concave \cite{Razaviyayn2020, Bertsimas2010}, i.e., falls into the latter situation.

% Furthermore, we focus on the change of the worst-case scenario set $\ywset(x) = \{y|F(x)=f(x, y)\}$ for each solution $x$.
% (III) : Around the global minimum solution of the worst-case function, the change of the worst-case scenario is discontinuous or continuous but very sensitive to the change of $x$. 
% In many existing studies, especially in theoretical analysis, it is assumed that the objective function is concave with respect to the scenario vector. However, it has been reported that the objective function with respect to $y$ in real-world applications is often non-concave \cite{Razaviyayn2020, Bertsimas2010}. In such problems, the worst-case scenario can easily be discontinuous. 
% Generally, min--max optimization of a function which is non-concave in $y$ is intractable \cite{minmaxcomplexity}. 
% Therefore, we additionally assume maximization problem $\max_{y \in \mathbb{Y}} f(x, y)$ can be efficiently solvable by existing solvers and minimization problem $\min_{x \in \mathbb{X}} F(x)$ can be (possibly locally) efficiently solvable.

\begin{figure*}[t]
    \centering
    \begin{subfigure}{0.5\hsize}%
    \includegraphics[width=\hsize]{trajectories/cma(no wind).pdf}%
    \caption{Controller optimized for no wind condition}%
    \label{fig:berthing:nowind}%
    \end{subfigure}%
    \begin{subfigure}{0.5\hsize}%
    \includegraphics[width=\hsize]{trajectories/adv-cma.pdf}%
    \caption{Controller optimized for the worst wind condition}%
    \label{fig:berthing:advcma}%
    \end{subfigure}%
    \caption{%
    Visualization of the trajectories obtained by the controllers for (a) no wind condition, and (b) for the worst wind condition with the maximum wind velocity of $0.5$ [m/s]. Center: the objective function values $f(x, y)$ under $y$ representing the wind velocity of $0.5$ [m/s] and varying wind direction presented by the polar axis. The objective function value smaller than $10$ implies that the ship is controlled without a collision to the berth.
    The others: trajectories observed under wind from $0, 45, \dots, 315$ [deg] with velocity $0.5$ [m/s]. The red points are the target position. These controllers were obtained in \cite{akimoto2022berthing}.
    }%
    \label{fig:berthing-trajectory-adv}
\end{figure*}

\paragraph{Robust berthing control problem}

% 本研究では、berthing control problem\cite{maki2020, Miyauchi2022}を実問題の例として取り扱う。船舶の着桟時は、桟橋に座礁しないように航行速度を小さくし、桟橋と船舶との距離を考慮した繊細な制御が必要なため、自動制御が困難となる。自動着桟問題の一例では、船舶の進行方向やプロペラの回転数等の船舶制御に関わる項目を設計変数とし、目標とする停留位置と船舶の距離等の目的関数値を数値シミュレーションで求め、最小化する。ここで、数値シミュレーションでは、船舶の運動方程式を解くことで船舶の移動距離を計算できる。しかし、その運動方程式中の係数等は模型実験等から決定されるため、数値シミュレーションから得られた船舶の移動距離には不確実性が含まれる。さらに、天候は不確実である。特定の天候状態で最適化された制御装置は、それとは異なる天候状態では座礁等の大事故を引き起こす可能性がある。

As an example real-world application, we consider the automatic berthing control problems \cite{maki2020, Miyauchi2022}. 
Its objective is to obtain a controller that realizes a fine control of a ship toward a target state with the least collision risk and minimum elapsed time. 
Given a controller parameterized by $x$, a trajectory of the ship motion is simulated by numerically solving a ship maneuvering model. 
The objective function value is evaluated based on the computed trajectory. 
However, this simulation contains uncertainties. 
% For instance, the coefficients in the maneuvering model are determined by empirical formulae, scaled model tests or some regression results. Furthermore, the environmental conditions, such as winds, currents, or, waves  in the real world is uncertain and changes over time. 
% The optimum control obtained by simulation based optimization under a particular weather condition and an estimated model parameters may contain modeling error. 
% Although the error is small, it may cause a serious accident in the real world, such as collision to the berth, because the ship need to maneuver at very close to the berth.


%既往の研究\cite{akimoto2022berthing}では、これら数値シミュレーションの不確実性を考慮するためにmin--max問題として定式化したRobust berthing control problemを紹介し、その重要性について示している。\Cref{fig:berthing-trajectory-adv} にその有用性の一端を示す．\Cref{fig:berthing:nowind}は風外乱がないものとして(1+1)-CMA-ESを用いて最適化された制御則，\Cref{fig:berthing:advcma}は風向（$[0, 360]$ [deg]）および風速（$[0, 0.5]$ [m/s]）をシナリオ変数としてAdversarial-CMA-ES~\cite{akimoto2022berthing}を用いて最適化された制御則である．無風環境で最適化された制御則を用いた場合，0.5 [m/s]の風環境で運用すると$50\%$以上の風向において座礁してしまう軌跡が確認される．一方，Adversarial-CMA-ESを用いて得られた制御則では，どのような風向に対しても$0.1$程度の目的関数値となるような軌跡が確認される．

Previous study \cite{akimoto2022berthing} formulated the problem of find a robust berthing controller as a min--max optimization problem. 
\Cref{fig:berthing-trajectory-adv} shows the importance of considering the uncertainties. \Cref{fig:berthing:nowind} and \Cref{fig:berthing:advcma} show the trajectories and the objective function values under different wind conditions when a controller optimized by the (1+1)-CMA-ES \cite{Arnold2010} under no wind disturbance, and a controller optimized by \acma{} \cite{akimoto2022berthing}, where the uncertainty of wind direction ($[0, 360]$ [deg]) and wind speed ($[0, 0.5]$ [m/s]) is taken into account, are used, respectively. 
When the controller optimized under no-wind assumption is used, we often observe the collision of the ship to the berth under wind disturbance of velocity $0.5$ [m/s]. On the other hand, the robust controller obtained by \acma{} successfully avoid the collision for all wind direction.


%Robust berthing control problem は，上述の特徴(I)--(III)を有すると考えられる．特徴(I)および(II)は，文献\cite{akimoto2022berthing}でも指摘されている通りである．特徴(III)については，\Cref{fig:berthing-trajectory-adv}にその痕跡が垣間見える．目的関数$f(x, y)$は，風向（$y$の一部）に対して多峰性を有していることが，中央図からわかる．そのため，non-concaveとなり，特徴(III)を自然に有すると考えられる．とりわけ，沖から桟橋への風$y_{\mathrm{sea}}$と桟橋から沖への風$y_{\mathrm{land}}$の間でワーストシナリオが切り替わる現象が予備実験により確認されている．これは，次のように説明される．沖から桟橋への風$y_{\mathrm{sea}}$のもとでは，衝突回避のために桟橋に近づきすぎないような制御が獲得される．しかし，そのような制御を桟橋から沖への風$y_{\mathrm{land}}$のもとで運用すると，桟橋付近の目標状態から離れた位置に停泊してしまい，高い目的関数値となってしまう．逆に，$y_{\mathrm{land}}$のもとで最適化された制御則を$y_{\mathrm{sea}}$のもとで運用すると，桟橋に衝突してしまう．そのため，ワーストケースを最小化するような制御則は，$y_{\mathrm{land}}$と$y_{\mathrm{sea}}$がワーストシナリオになる領域の境界上に存在し，いずれかのシナリオのもとでの最適解ではないことが予想される．


The robust berthing control problem is expected to fall into situation (B).
Its vestiges can be seen in  \Cref{fig:berthing-trajectory-adv}. The central figure shows that the objective function $f(x, y)$ is multimodal with respect to the wind direction. Therefore, $f(x, y)$ is non-concave in $y$. 
Moreover, we observed in our preliminary experiments that the worst scenario switches between offshore-to-berth wind $y_{\mathrm{sea}}$ and berth-to-offshore wind $y_{\mathrm{land}}$. 
This is explained as follows. 
In offshore-to-berth wind $y_{\mathrm{sea}}$, the optimum control avoids getting too close to the berth to avoid a collision. Under such control, berth-to-offshore wind $y_{\mathrm{land}}$ becomes the worst scenario because the vessel stops at a position far from the target position near the berth, resulting in a high objective function value. Conversely, if the optimum control for $y_{\mathrm{land}}$ is operated, the worst scenario is $y_{\mathrm{sea}}$ which causes the vessel to collide with the berth. 
Therefore, the control that minimizes the objective function at the worst scenario is expected to exist on the boundary of the regions where the worst-case scenario changes between $y_{\mathrm{land}}$ and $y_{\mathrm{sea}}$, and it is not the optimal solution under either scenario.


\paragraph{Related works}

% ここでは、min--max問題を対象とした既存の研究の中で、本研究で仮定した (I)--(III) に関連した問題を対象とした事例を紹介する。これまでの既存研究では、本研究で仮定した (I)--(III) のすべての項目を考慮した事例は見受けられなかったため、項目毎の事例について説明する。

%In this section, we present previous studies that consider min--max problem related to (I)--(III) assumed in this study. We present previous studies for each of the assumptions because we have not found any previous studies that considered all assumptions (I)--(III).


% まず、シナリオ$y$に対する目的関数がnon-concaveとなるmin-max問題を対象とした既存研究について紹介する。
% 既存研究\cite{Bertsimas2010, Bertsimas2010b}では、目的関数がsmoothであることを想定し、Karush-Kuhn-Tucker conditionsを使用して、ワーストケース関数$F$の勾配を近似して最適解を探査する手法が提案されている。
% しかし、対象としている問題設定が我々の問題設定と異なる。そのため、近似勾配を求める手法を導入してこれらの手法を改善したとしても、我々の問題設定に適用することは自明ではない。

Gradient-based min--max optimization has been actively studied in recent years.
However, the majority of existing studies investigates min-max problems of functions that are concave in $y$, despite that many real world problems are not necessarily concave in $y$ \cite{Razaviyayn2020}. 
At the same time it has been shown that a general non-convex non-concave min--max problem is theoretically intractable \cite{minmaxcomplexity}. 
Some research effort has been devoted to identify structures of non-concave min--max problems that makes it efficiently solvable \cite{nonconvexpl,nonconcaveweakmvi,nonconcaveweakconcave,nonconcavehidden} or to exploit a small domain for scenario vector \cite{nonconcavesmall}. 
These investigations do not cover situation (B) above. 
Previous studies \cite{Bertsimas2010, Bertsimas2010b} have developed gradient-based approaches for more general non-concave min-max problems, where both implementation error and parameter uncertainty are taken into account. 
This approach is however designed to exploit the existence of implementation error and it is not trivial to extend it to derivative-free situation via gradient approximation.

% First, we introduce previous studies \cite{Bertsimas2010, Bertsimas2010b} on min-max problem that $h(y)$ is nonconcave. In the studies, the smoothness of the objective function was assumed and the Karush-Kuhn-Tucker condition was used to approximate the gradient of the worst-case function $F$. However, the target problem setting is different from our problem setting. Therefore, even if we improve these approaches to explore the optimal solution using the approximated gradient, it is not obvious to apply them to our target problem.


% 次に、Min--max問題に対するderivative-free approach について紹介する。既存研究では、coevolutionary approach \cite{Barbosa1999, Herrmann1999,Qiu2018, Abdullah2019}、\acma{} \cite{akimoto2022berthing}、Zero-Order Projected Gradient Descent Ascent (\zopgda{} : \cite{Liu2020})、surrogate-model–based approaches \cite{Bogunovic2018}等が提案されている。これらの中で、coevolutionary approachの一手法であるminimax differential evolution \cite{Qiu2018}は本研究での仮定 (III) を考慮しているものの、\acma{}との比較実験でstrongly convex concaveな関数であっても最適解に収束できなかったことが報告されている\cite{akimoto2022berthing}。このため、仮定 (III)  に十分に対応てきないと考えられる。また、既存の研究における数値実験\cite{Liu2020}で、surrogate-model–based approachesの一つであるstableopt \cite{Bogunovic2018} はサロゲートモデルの学習に膨大な時間を要することが報告されている。そのため、多くの$f$-calls（本研究では$2 \times 10^6$程度まで）が消費される問題への適用は現実的ではないと考えられる。既存手法\zopgda{}と\acma{}は、目的関数がstrongly convex concaveでその勾配がLipschitz continuousな問題において最適解への収束が理論的に保証されており，上述の関連手法との比較実験においても優れたワーストケース関数値が得られることが報告されている．しかし，仮定 (III)を満たすような，ワーストシナリオが不連続に変化する問題では，非常に多くの$f$-callsを要するか，収束に失敗することがが報告されている\cite{akimoto2022berthing}．

Derivative-free approaches for min--max optimization includes coevolutionary approaches \cite{Barbosa1999, Herrmann1999,Qiu2018, Abdullah2019}, simultaneous descent ascent approaches \cite{akimoto2022berthing,Liu2020}, and surrogate-model-based approaches \cite{Bogunovic2018}. 
Among them, \acma{} \cite{akimoto2022berthing} and \zopgda{} \cite{Liang2019} are theoretically guaranteed to converge to the optimal solution and its neighborhood, respectively, on smooth strongly convex concave min--max problems. 
However, it has been reported that they fail to converge in situation (B) and exhibit slow convergence in situation (A) \cite{akimoto2022berthing}. 
Although some coevolutionary approaches, such as MMDE \cite{Qiu2018}, is intended to address the difficulty in situation (B), it has been reported that they fail to converge not on such problems, and even on strongly convex-concave problems \cite{akimoto2022berthing}. STABLEOPT \cite{Bogunovic2018}, a Bayesian optimization approach, is expected to address the difficulty in situation (B). However, because of the high computational time of Gaussian process, it is impractical for problems where relatively high number of $f$-calls are required to obtain a satisfactory solutions, as pointed out in \cite{Liu2020}. 

% Among them, although minimax differential evolution \cite{Qiu2018}, which is one of the coevolutionary approaches, considers assumption (III), it failed to converge to the optimal solution even in a strongly convex concave function in comparison experiments with \acma{} \cite{akimoto2022berthing}. Therefore, we believe that this approach cannot adequately addressed assumption (III). 
% In addition, numerical experiments in previous study \cite{Liu2020} have reported that stableopt \cite{Bogunovic2018}, which is one of the surrogate-model-based approaches, takes a huge time to learn a surrogate model. Therefore, application these approaches to our problem is impractical because the problem is anticipated to consume many $f$-calls (about $2 \times 10^6$ in this study). The existing approaches \zopgda{} and \acma{} are theoretically guaranteed to converge to the optimal solution for problems that the objective function is strongly convex concave and its gradient is Lipschitz continuous. Furthermore, in comparison experiments with minimax differential evolution \cite{Qiu2018} and stableopt \cite{Bogunovic2018}, they have obtained better  worst-case function value. 
% However, at problems that the worst-case scenario changes in discontinuously ( assumption (III)), it has been reported for \zopgda{} and \acma{} that either a large number of $f$-calls are required or convergence fails \cite{Liang2019, akimoto2022berthing}.


% 以上のように，本研究で想定する特徴(I)--(III)を持つmin--max最適化に対するアプローチは，十分に検討されていないのが実情である．

%As mentioned above, any previous studies have not fully investigated approaches to min--max problem with assumptions (I)--(III).
%As reviewed above, black-box min--max optimization problem with assumption (III) has not been well investigated to date, whereas we believe such a problem appears in real world applications such as robust berthing control problem.



\paragraph{Contribution}

% 本研究では、目的関数がblack boxなmin--max continuous optimization problemsについて検討し、先に述べた仮定(I)--(III)に対処できるアプローチを提案した。本研究の貢献を以下に示す。

The contributions of this study are as follows.

\emph{Approach (\Cref{sec:proposed})}.
    % \item 仮定(I)--(III)に対処するために新しいアプローチを提案した。提案アプローチは、近似されたワーストケース関数に対してthe Covariance Matrix Adaptation Evolution Strategy (\cmaes{}) により最適解を探査する。\cmaes{}がワーストケース関数に対して解を探査しているように振る舞えるように、生成された解候補のワーストケース関数でのランクを推定するWorst-case Ranking Approximation (\wra{}) を提案した。\wra{}では、min--max問題の内側の最大化問題$\max_{y \in \mathbb{Y}}f(x,y)$を近似的に解く。この最大化問題に対するソルバーとして\cmaes{}を実装したアルゴリズム\wracma{}と\slsqp{}を実装したアルゴリズム\wraslsqp{}を紹介した。
    Aiming at addressing the limitations of existing approaches observed in situations (A) and (B) above, a novel approach that directly search for the global min--max solution is proposed. 
    The proposed approach minimizes the worst-case objective function by the covariance matrix adaptation evolution strategy (CMA-ES) \cite{Hansen2001,Hansen2014,akimoto2019}, where the rankings of the solution candidates are approximated by the here-proposed worst-case ranking approximation mechanism (WRA). 
    The WRA mechanism approximately solves the maximization problem $\max_{y \in \mathbb{Y}}f(x,y)$ for each solution candidate $x$. 
    To save $f$-calls required to solve each maximization problem, we design a warm starting strategy and an early stopping strategy. 
    We propose two variants of WRA implementations using CMA-ES and Approximated Gradient Ascent (AGA) 
    % SLSQP \cite{kraft1988} 
    as the internal solver.
    To deal with non-convex real-world applications, we incorporate a restart strategy and a local search strategy. 

    % \item 数値実験では、仮定(III)の一部であるワーストシナリオの変化は連続であるが非常にsensitiveである状況を模擬したテスト問題に、既存手法である\zopgda{}と\acma{}と提案アプローチ\wracma{}と\wraslsqp{}を適用し、比較を行った。その結果、提案アプローチのみが最適解に収束できることが示された。
\emph{Evaluation (\Cref{sec:test})}.
We design $11$ test problems with different characteristics, including both situations (A) and (B). Numerical experiments on these test problems reveal the limitation of existing approaches and show that the proposed approaches can deal with both situations (A) and (B). 
We experimentally show that the scaling of the runtime on a smooth strongly convex--concave with respect to the interaction term (denoted by $b$ in \Cref{sec:test}) is significantly improved over existing approaches.
To understand when the proposed approach is effective in situation (B), we investigate the effect of each component of WRA mechanism on each of the following situations: (S) the global min--max solution $x^*$ is a strict min--max saddle point; (W) $x^*$ is a weak min--max saddle point; and (N) $x^*$ is not a min--max saddle point. 

% To verify that the proposed approach can address situation (A), we compared the proposed approaches, \wracma{} and \wraslsqp{}, and the existing approaches, \zopgda{} and \acma{}, on test problems. We empirically observed that the proposed approaches converged to the global min--max solution with less $f$-calls on .  
%     The results showed that the proposed approaches were superior to the existing approaches because the proposed approaches could converge to the optimal solution . 
%     % \item 数値実験では、仮定(III)の一部であるワーストシナリオの変化が不連続な状況を模擬したテスト問題にを模擬したテスト問題に、既存手法と提案アプローチを適用し、比較を行った。その結果、提案アプローチのみが最適解に収束できることが示された。
%     To verify that the proposed approaches can address the problem where the worst scenario changes discontinuously, which is part of assumption (iii), we conducted the numerical experiment on test problems. We compared the proposed approaches with the existing approaches. The results in the numerical experiments showed that the proposed approaches could converge to the optimal solution, whereas the existing approaches failed.
    
\emph{Application (\Cref{sec:berthing})}.
    % \item 提案アプローチを不確実パラメータが異なる3つのRobust berthing controlの問題に適用し、既存手法の結果との比較により、その適用効果を評価した。実験結果では、提案アプローチはほぼすべての試行でワーストケースでも座礁せずに安定して着桟できる解を求めることができ、この点で既存手法よりも優位であることが示された。
    The proposed approaches and the existing approaches are applied to the robust berthing control problems with three types of scenario vectors.
    In the cases that the wind direction is included in the scenario vector, we confirm that
    the proposed approaches often obtain controllers that can avoid collision to the berth in the worst situation, whereas the controllers obtained by the existing approaches tend to fail to avoid collision to the berth in the worst situation.
    In the case where the existing approach can often obtain collision-free controllers, we confirm that the existing approach achieves better worst case performance than the proposed ones.
    We also demonstrate the effect of the hybrid of the existing and the proposed approaches.
    
    % We demonstrated the effectiveness of the proposed approaches by comparing the results with those of existing approaches.
    % Experimental results showed that the proposed approaches were superior to the existing approaches in terms of that the proposed approaches could  stably find a robust solution to wind uncertainty.
    
\emph{Implementation}.
Our implementations are publicly available in \textcolor{red}{our GitHub repository\footnote{\url{link}}}.


\section{Preliminaries}\label{sec:formulation}

The objective of this study is to locate the global minimum solution to the worst-case objective function $F$ defined as
\begin{align}
	F(x) = \max_{y \in \mathbb{Y}} f(x, y) , \label{eq:Fx}
\end{align}
where $f:\mathbb{X} \times \mathbb{Y} \rightarrow R$ is the objective function,
$\mathbb{X} \subseteq R^m$ is the search domain for \emph{design vector} $x$,
and $\mathbb{Y} \subseteq R^n$ is the search domain for \emph{scenario vector} $y$.
We assume that $f$ and $F$ are black-box and their gradient information and higher order information are unavailable.
For each $x \in \mathbb{X}$, let $\ywset(x) = \{y \mid F(x)=f(x, y)\} = \argmax_{y \in \mathbb{Y}} f(x, y)$ be called the \emph{worst scenario set}. 
If $\ywset(x)$ is a singleton, i.e., $\abs{\ywset(x)} = 1$, then the unique element is called the \emph{worst scenario} and is denoted by $\yw(x)$.
The global minimum solution of $F$ is called the \emph{global min--max solution} of $f$ and denoted by $x^* = \argmin_{x \in \mathbb{X}} F(x)$.
% We assume that $\abs{\ywset(x)} = 1$ and $\yw(x)$ is continuous almost everywhere in $\mathbb{X}$, and around some minimum points of $F$ either that (a) $\abs{\ywset(x)} = 1$ but $\yw(x)$ is sensitive to the change of $x$, i.e., $\norm{\yw(x) - \yw(x')} / \norm{x - x'}$ is large, or that (b) $\abs{\ywset(x)} > 1$, where $\yw(x)$ is discontinuous.


The \emph{min--max saddle point} (\Cref{def:saddlep}) is an important concept that characterizes the difficulties in obtaining $x^*$.
In what follows, a neighborhood of design vector $x$ is a set $\mathcal{E}_x$ such that there exists an open ball $\mathbb{B}(x, r)=\{\tilde{x} \in \mathbb{R}^m \mid \norm{x-\tilde{x}}<r\}$ included in $\mathcal{E}_x$ as a subset.
We define a neighborhood $\mathcal{E}_y$ of scenario vector $y$ analogously.
A critical point $(x, y) \in \mathbb{X}\times\mathbb{Y}$ of objective function $f$ is such that $\nabla f (x,y) = (\nabla_x f (x,y), \nabla_y f (x,y)) = 0$. 
\begin{definition}[min--max saddle point]\label{def:saddlep}
A point $(\tilde{x}, \tilde{y}) \in \mathbb{X} \times \mathbb{Y}$ is a local min--max saddle point of a function $f:\mathbb{X} \times \mathbb{Y} \rightarrow \mathbb{R}$ if there exists a neighborhood $\mathcal{E}_x \times \mathcal{E}_y \subseteq \mathbb{X} \times \mathbb{Y}$ including $(\tilde{x}, \tilde{y})$ such that for any $(x,y) \in \mathcal{E}_x \times \mathcal{E}_y$, the condition $ f(\tilde{x}, y) \leq f(\tilde{x}, \tilde{y}) \leq f(x,\tilde{y})$ holds. If $\mathcal{E}_x = \mathbb{X}$ and $\mathcal{E}_y = \mathbb{Y}$, the point $(\tilde{x}, \tilde{y})$ is called the global min--max saddle point. If the equality holds only if $(x,y) = (\tilde{x}, \tilde{y})$, it is called a strict min--max saddle point. A saddle point that is not a strict min--max saddle point is called a weak min--max saddle point.
\end{definition}


We focus on whether $x^*$ is a strict min--max saddle point. 
If so, problem of locating $x^*$ turns into the problem of locating the global min--max saddle point. 
In such a situation, locating multiple local min--max saddle points, $\{(x_i, y_i)\}_{i=1}^{K}$, and selecting the best one, $\argmin_{1\leq i \leq K} \max_{1\leq j \leq K}f(x_i, y_j)$, can offer the optimal solution $x^*$ as long as the global min--max saddle point is included in $\{(x_i, y_i)\}_{i=1}^{K}$. 
Existing approaches \cite{akimoto2022berthing,Liu2020} for locating local min--max saddle points may be used for this purpose.
However, if $x^*$ is not a strict min--max saddle point, the above approach may not provide a reasonable solution and a different approach needs to be taken.


\begin{comment}
The following proposition provides a necessary and sufficient condition for $x^*$ to form a global min--max saddle point.
%
\begin{proposition}\label{prop:saddle}
Let $x^* \in \argmin_{x \in \mathbb{X}}$ be (one of) the global min--max solution of $f$.
(i) If there exists a $\bar{y} \in \ywset(x^*)$ such that $f(x, \bar{y}) \geq f(x^*, \bar{y})$ for all $x \in \mathbb{X}$, 
then $(x^*, \bar{y})$ is a global min--max saddle point.
(ii) Otherwise, (i.e., if for each $\bar{y} \in \ywset(x^*)$, there exists an $x \in \mathbb{X}$ such that $f(x, \bar{y}) < f(x^*, \bar{y})$) 
$(x^*, y')$ is not a global min--max saddle point for any $y' \in \mathbb{Y}$.
\end{proposition}
\begin{proof}
Statement~(i) follows that 
\begin{align}
    f(x, \bar{y}) \geq f(x^*, \bar{y}) = F(x^*) \geq f(x^*, y) 
\end{align}
for all $x \in \mathbb{X}$ and for all $y \in \mathbb{Y}$.
Statement~(ii) is proved by contradiction.
If we assume that $(x^*, y')$ is a global min--max saddle point for some $y' \in \mathbb{Y}$, 
then we have
\begin{align}
    f(x, y') \geq f(x^*, y') \geq f(x^*, y) 
\end{align}
for all $x \in \mathbb{X}$ and $y \in \mathbb{Y}$. The right-hand side inequality implies that $y' \in  \ywset(x^*)$.
However, the left-hand side inequality contradicts to the condition of the statement. 
This completes the proof.
\end{proof}




% We remark on \Cref{prop:saddle} from practical viewpoints.
% We can categorize the problems into the following four types:
% (Type 1) $\abs{\ywset(x^*)} = 1$ and $(x^*, \yw(x^*))$ is a global min--max saddle point; 
% (Type 2) $\abs{\ywset(x^*)} = 1$ and $(x^*, \yw(x^*))$ is not a global min--max saddle point; and
% (Type 3) $\abs{\ywset(x^*)} > 1$ and $(x^*, y)$ do not form a min--max saddle point for any $y \in \ywset(x^*)$;
% (Type 4) $\abs{\ywset(x^*)} > 1$ and $(x^*, y)$ forms a global min--max saddle point for any $y \in \ywset(x^*)$.
% \Cref{prop:saddle} implies that the global min--max solution $x^*$ can be a global min--max saddle point of $f$ even if $\abs{\ywset(x^*)} > 1$. 
% However, we believe that it is very unlikely in practice, because it requires that $f(x, y)$ for all $y \in \ywset(x^*)$ have the same value at the minimum point $x^*$ of $f(x, \bar{y})$ for some $\bar{y} \in \ywset(x^*)$. 
% Though \Cref{prop:saddle} only mentions about a global min--max saddle point, we believe from the analogous reason that it is also very unlikely in practice that $x^*$ forms a local min--max saddle point.
% Therefore, we conjecture that the global minimum solution $x^*$ of the worst-case function $F$ is a (global or local) min--max saddle point only if $\ywset(x^*)$ is a singleton in practice.
% % Moreover, if $\abs{\ywset(x^*)} > 1$, it is likely that the worst scenario $\yw(x)$ changes discontinuously around $x^*$ except for special cases. 
% % Therefore, situation (a) ($\abs{\ywset(x^*)} = 1$ but $\yw(x^*)$ is sensitive) implies that $x^*$ is located at a global min--max saddle point, and situation (b) ($\abs{\ywset(x)} > 1$) implies that $x^*$ is located at neither global nor local min--max saddle point.

We can categorize the problems into the following four types:
(Type 1) $\abs{\ywset(x^*)} = 1$ and $(x^*, \yw(x^*))$ is a global min--max saddle point; 
(Type 2) $\abs{\ywset(x^*)} = 1$ and $(x^*, \yw(x^*))$ is not a global min--max saddle point; and
(Type 3) $\abs{\ywset(x^*)} > 1$ and $(x^*, y)$ do not form a min--max saddle point for any $y \in \ywset(x^*)$;
(Type 4) $\abs{\ywset(x^*)} > 1$ and $(x^*, y)$ forms a global min--max saddle point for any $y \in \ywset(x^*)$.
A typical example of Type~1 is a strictly convex--concave function $f$, where $f$ is strictly convex in $x$ and strictly concave in $y$. 
Then, $\ywset(x)$ is a singleton for all $x \in \mathbb{X}$. 
There exists a unique global min--max saddle point $(x^*, \yw(x^*))$ and $x^*$ is the unique global min--max solution.
An example of Type~2 is $f(x, y) = -x^2 + 2xy - (1/4)y^2$ with $\mathbb{X} = [-1, 1]$. On this problem, the global min--max solution is $x^* = 0$ and the worst scenario is $\yw(x^*) = 0$, but $(0, 0)$ is not a min--max saddle point.
An example of Type~3 is $f(x,y)=(x+ \sin(\pi y))^2$ with $\mathbb{Y} = [-1, 1]$, as given in \cite{akimoto2022berthing}. 
For this function, the worst scenario is $\yw(x) = -\frac{1}{2}$ if $x < 0$, and $\yw(x) = \frac{1}{2}$ if $x > 0$. The global min--max solution is $x^* = 0$, where $\ywset(x^*) = \{\pm \frac{1}{2} \}$.
The min--max saddle points are $(-1, \frac{1}{2})$ and $(1, -\frac{1}{2})$. Therefore, $x^*$ is not a min--max saddle point.
We hypothesize that the robust berthing control task under wind condition uncertainty discussed in the introduction is similar to this  example.
An example of Type~4 is a bi-linear function $f(x, y) = x y$ with $\mathbb{Y} = [-1, 1]$.
The global min--max solution is $x^* = 0$ and the worst scenario set is $\ywset(x^*) = \mathbb{Y}$.
In this case, $(x^*, 0)$ is a weak global min--max saddle point. 


In the convergence analysis of the existing approaches for min--max optimization, it is often assumed that $f$ is twice continuously differentiable and $\mu$-strongly convex--concave, which falls into Type~1. On such a function, the Hessian matrix is $\begin{bmatrix}H_{x,x}(x, y) & H_{x,y}(x, y) \\ H_{y,x}(x, y) & H_{y,y}(x, y)\end{bmatrix}$, where $H_{x,x}(x, y)$, $H_{x,y}(x, y)$, $H_{y,x}$, and $H_{y,y}(x, y)$ are the matrices whose $(i,j)$ elements are $\frac{\partial^2 f}{\partial x_i\partial x_j}(x, y)$, $\frac{\partial^2 f}{\partial x_i\partial y_j}(x, y)$, $\frac{\partial^2 f}{\partial y_i\partial x_j}(x, y)$, and $\frac{\partial^2 f}{\partial y_i\partial y_j}(x, y)$, respectively. The $\mu$-strong convexity--concavity implies that both $H_{x,x}(x, y) - \mu I_m$ and $\mu I_n - H_{y,y}(x, y)$ are non-negative definite for some $\mu > 0$.
Its unique critical point $(x^*, y^*)$ is the unique global min--max saddle point. 
It is well-known (e.g., \cite[Proposition~2.5]{akimoto2022berthing}) that the worst scenario $\yw(x)$ uniquely exists for all $x \in \mathbb{X}$, $y^* = \yw(x^*)$, and its Jacobian is 
\begin{equation}
\nabla_x \yw(x) = - (H_{y,y}(x, \yw(x)))^{-1} H_{y,x}(x, \yw(x)) .    \label{eq:jacoby}
\end{equation}
We emphasize that the worst scenario is unique and continuous, but it can be very sensitive to the change of $x$ if the singular values of the Jacobian $\nabla_x \yw(x)$ are large.
In this case, the sensitivity of $\yw(x)$ is measured by the greatest singular value of $(H_{y,y}(x, \yw(x)))^{-1} H_{y,x}(x, \yw(x))$.

% $\abs{\ywset(x^*)} = 1$か$\abs{\ywset(x^*)} > 1$は、$F$の局所解がシナリオの境界上に存在するかそうでないかの違いがある。
% 解がmin max saddleかそうでないかの軸で、$\abs{\ywset(x^*)} = 1$か$\abs{\ywset(x^*)} > 1$となる問題の特徴について、
% 考えられる問題の候補は、
% 1, 解がmin max saddleで$\abs{\ywset(x^*)} = 1$
% 2, 解がmin max saddleで$\abs{\ywset(x^*)} > 1$　現実的にはおこらない

% 3, 解がmin max saddleではなくて$\abs{\ywset(x^*)} = 1$　理論上はおこらない．
%$f=-x^2 + 2xy - 1/4 y^2$の場合はどうでしょうか？situation (ii)に含まれ、ワーストシナリオは一つとなるのではないでしょうか？
% f = -(y - Bx)^2 + x^2
% yw = 4 x, F = - x^2 + 8 x^2 - 4 x^2 = 3 x^2, (0, 0)

% 4, 解がmin max saddleではなくて$\abs{\ywset(x^*)} > 1$

% robust berthing problemを想定して、つまり、$\abs{\ywset(x^*)} > 1$となることを想定して、解$x^*$の状況を考える。
% 解がmin max saddleで、$\abs{\ywset(x^*)} > 1$な場合を考える。この場合（ケース2の場合）、いくつかのシナリオ$y \in \ywset$の目的関数が$x^*$で極小値を持つ必要があり、レアケースである。そのため、ワーストシナリオは1つであると考えられる。

% 問題としてはType-1, Type-2, Type-3に分けて、各々の問題は、Difficluty (I)にType-2とType-3はDificulty (II)に分類される。
\end{comment}

\providecommand{\btx}{z}
\begin{table*}[]
    \centering%\scriptsize
    \caption{%
    Test problem definitions and their worst scenarios. 
    The search domains for $x$ and $y$ are $\mathbb{X} = [\ell_x, u_x]^m$ and $\mathbb{Y} = [-b_y, b_y]^n$, respectively. 
    The interaction between $x$ and $y$ is controlled by $m \times n$ matrix $B$.
    For $f_3$, we assume that $B$ is of full column rank, and let $B^\dagger = [b_1^\dagger, \dots, b_m^\dagger]$ be the Moore-Penrose inverse of $B$ and $\alpha = -\frac{\min(\abs{u_x}, \abs{\ell_x})}{ (30/7) \max_{i=1,\dots,m} \norm{b_i^\dagger}_1}$.
    For $f_9$, we set $n_* = \min\{n, 3\}$.
    For $f_{10}$, we assume $m = n$ and $B = \diag(1, \dots, 1)$.
    The optimal solutions for the worst case objective functions are $x^* = \bm{0}$ except for $f_1$, $f_3$, and $f_9$. The optimal solution is 
    $[B x^*]_{i} = 0$ for $f_1$, 
    $[B x^*]_{i} = \alpha$ for $f_3$, 
    and $[B^\T x^*]_{i} = - \sinh(1)$ for $i \leq n_*$ and $[B^\T x^*]_{i} = 0$ for $i > n_*$ for $f_9$.
    Here, $[x]_i$ denotes the $i$th coordinate of a vector $x$. 
%    $\norm{B}_F = \sqrt{\Tr(B B^\T)}$. 
    }
    \label{tab:testp}
    \begin{tabular}{ll}
    \toprule
    Definition & 
    % $x$ &
    % $y$ & 
    $[\yw(x)]_i$ (here $\btx = B^{\T} x$ for short)
    \\
    \midrule
    
    $f_1 = x^{\T} B y$ & 
    % linear & 
    % linear &
    $\begin{cases} b_y \sign([z]_i ) & [z]_i \neq 0 \\ \text{arbitrary} & [z]_i = 0 \end{cases}$
    \\

    $f_2 = \frac12 \norm{x}_2^2 + x^\T B y$ & 
    % st-cv & 
    % linear  & 
    $\begin{cases} b_y \sign([z]_i ) & [z]_i \neq 0 \\ \text{arbitrary} & [z]_i = 0 \end{cases}$
    \\

    $f_3 = \frac{1}{2} \norm{B^\T x - (\alpha - c b_y) \bm{1}_n}_2^2 + c x^\T B y$ & 
    % \makecell{cv \\st-cv if $B$ is \\of full rank} & 
    % linear & 
    $\begin{cases} b_y \sign([z]_i ) & [z]_i \neq 0 \\ \text{arbitrary} & [z]_i = 0 \end{cases}$
    \\        
        
    $f_4 = \frac{1}{2} \norm{x}_2^2 + x^\T B y + \frac{1}{2} \norm{y}_2^2$ & 
    % st-cv & 
    % non-cc  &   
    $\begin{cases} b_y \sign([z]_i ) & [z]_i \neq 0 \\ \pm b_y & [z]_i = 0 \end{cases}$    \\

    
    $f_5 = \frac{1}{2} \norm{x}_2^2 + x^\T B y - \frac{1}{2} \norm{y}_2^2$ & 
    % st-cv & 
    % st-cc  & 
    $\begin{cases}
    [z]_i & \abs{[z]_i} \leq b_y \\
    b_y \sign([z]_i) & \abs{[z]_i} > b_y
    \end{cases}$ 
    \\

    $\textstyle f_6 = \frac12 \norm{x}_2^2 + \norm{x}_1 + x^\T B y - \norm{y}_1 - \frac12 \norm{y}_2^2$ & 
    % non-sm st-cv & 
    % non-sm st-cc & 
    $\begin{cases}
    0 & \abs{[z]_i} \leq 1 \\
    [z]_i - \sign([z]_i)  & 1 < \abs{[z]_i} \leq b_y + 1 \\
    b_y \sign([z]_i) & b_y + 1 < \abs{[z]_i}
    \end{cases}$ 
    \\
    
    $f_7 = \frac14 \norm{x}_2^4 + x^\T B y - \frac14\norm{y}_2^4$ & 
    % cv & 
    % cc & 
    $\begin{cases}
    \frac{[z]_i}{\norm{z}_2^{2/3}}  & \frac{[z]_i}{\norm{z}_2^{2/3}} \leq b_y \\
    b_y \sign([z]_i) & \frac{[z]_i}{\norm{z}_2^{2/3}} > b_y
    \end{cases}$
    \\

    $f_8 = \norm{x}_1 + x^\T B y - \norm{y}_1$ & 
    % non-sm cv & 
    % non-sm cc &   
    $
    \begin{cases}
    0 & \abs{[z]_i} \leq 1 \\
    b_y \sign([z]_i) & \abs{[z]_i} > 1 
    \end{cases}
    $ 
    \\

    $f_9 = \sum_{i=1}^{n_*} \left([B^\T x]_i + \exp\left(\sign([y]_i)\right) \cdot \sin\left( \frac{ \pi [y]_i}{b_y}\right) \right)^2  + \sum_{i=n_*+1}^{n} \left([B^\T x]_i^2 - [y]_i^2 \right)$ & 
    % \makecell{cv \\st-cv if $B$ is \\of full rank} & 
    % non-cc &   
    $
    \begin{cases}
    (b_y / 2)  & [z]_i \geq - \sinh(1) \ \&\ i\leq n_*\\
    - (b_y / 2)  & [z]_i \leq - \sinh(1) \ \&\ i\leq n_*\\
    0 &  i > n_*
    \end{cases}
    $
    \\

    $f_{10} = \norm{x^\T B}_2^2 - 2 \norm{y - x^\T B}_2^{2} $ & 
    % non-cv & 
    % st-cc &   
    $
    \begin{cases}
    [z]_i & \abs{[z]_i} \leq b_y \\
    b_y \sign([z]_i) & \abs{[z]_i} > b_y
    \end{cases} 
    $
    \\

    $f_{11} = \sum_{i=1}^{n} \left(\frac{1}{2} [x]_i^2 + 10^{-3i/n} [x^\T B]_i [y]_i - \frac{10^{-6i/n}}{2} [y]_i^2 \right)$ & 
    % st-cv & 
    % st-cc, ill-conditioned &   
    $
    \begin{cases}
    10^{3i/n}[z]_i & \abs{10^{3i/n}[z]_i} \leq b_y \\
    b_y \sign([z]_i) & \abs{10^{3i/n}[z]_i} > b_y
    \end{cases} 
    $
    \\

    \bottomrule
    \end{tabular}
\end{table*}


\section{Test Problems}\label{sec:testp}
\Cref{tab:testp} lists the test problems that we use in the experiments of this paper. 
Although it is difficult to formally frame the target problems of this study, this list provides examples of problems in the scope of this study. We describe the characteristics of these problems in the following.

The functions $f_5$, $f_6$, $f_7$, $f_8$, and $f_{11}$ are strictly convex--concave.
On such problems, the worst scenario set $\ywset(x)$ is a singleton for each $x \in \mathbb{X}$.
The global min--max solution $x^*$ is the strict global min--max saddle point $(x^*, \yw(x^*))$. 
Among them, $f_5$, $f_6$, and $f_{11}$ are strongly convex--concave, and $f_5$, $f_7$, and $f_{11}$ are smooth.
The functions $f_5$ and $f_{11}$ are both smooth and strongly convex--concave, where the convergence of the existing approaches are investigated. 
Differently from $f_5$ and the other functions, $f_{11}$ is designed to be highly ill-conditioned in $y$ to demonstrate the impact of the ill-conditioning.
Although $x^*$ is a strict global min--max saddle point, we will see in our experiments that the existing approaches fail to converge if the objective function is not smooth ($f_{6}$ and $f_{8}$) or exhibit slow convergence if the objective function is not strongly convex--concave ($f_{7}$). 

The functions $f_1$, $f_2$, and $f_3$ are convex--linear. 
On such problems, the worst scenario is typically located at the border of the scenario domain $\mathbb{Y}$. 
On $f_1$ and $f_2$, where the former is a bi-linear function and the latter is strongly convex in $x$, the global min--max solution $x^*$ forms a weak min--max saddle point $(x^*, y)$ for any $y \in \mathbb{Y}$. 
The worst scenario set at $x^*$ is hence $\ywset(x^*) = \mathbb{Y}$, but the worst scenario $\yw(x)$ in a neighborhood of $x^*$ is one of the $2^n$ vertices $\bar{Y}$ of $\mathbb{Y}$. 
However, $x^*$ is not a global minimum point of $f(x, y)$ for any $y \in \bar{Y}$. 
In case of $f_3$, $\abs{\ywset(x^*)} = 1$ and $(x^*, \yw(x^*))$ is a strict global min--max saddle point.

The functions $f_4$, $f_9$, and $f_{10}$ are not convex--concave.
On these problems, the global min--max solution $x^*$ does not form a min--max saddle point. 
Similarly to $f_1$ and $f_2$, the worst scenarios of $f_4$ are located at the vertices $\bar{Y}$ of $\mathbb{Y}$.
However, differently from them, $\ywset(x^*) = \bar{Y}$ and $x^*$ does not form a min--max saddle point. 
In case of $f_9$, the worst scenarios are not at the vertices of $\mathbb{Y}$ but at the some specific points inside $\mathbb{Y}$, and $\abs{\ywset(x^*)} > 1$.
These two functions are multimodal in $y$ for each $x \in \mathbb{X}$ and the global maximum (i.e., the worst scenario) changes depending on $x$. 
Differently from $f_4$ and $f_9$, $f_{10}$ is concave in $y$, but also concave in $x$. 
Because of the concavity in $y$, we have $\abs{\ywset(x)} = 1$ for all $x \in \mathbb{X}$.
Moreover, $\yw(x)$ is continuous. 
The worst case objective function $F$ is convex around $x^*$. 
However, $x^*$ is not a min--max saddle point.

We focus on some characteristics related to the difficulty in approximating the local landscape of the worst case objective function $F$. 
One of the characteristics common to $f_1$--$f_4$ and $f_9$ is that the worst scenario changes discontinuously. 
Especially on $f_1$, $f_2$, $f_4$, and $f_9$, the worst scenarios spread over multiple distant points in a neighborhood of the global min--max solution $x^*$.
The landscape of $F$ can not be approximated well around such a discontinuous point if we only have a single candidate $\tilde{y}$ of the corresponding worst scenario.
We expect from \Cref{fig:berthing-trajectory-adv} that the robust berthing control problem discussed in \Cref{sec:intro} has the above difficulty.
The landscape of $F$ can not be approximated well with a single candidate $\tilde{y}$ on $f_{10}$ as well because of the concavity of $f_{10}$ in $x$.
The non-smoothness of $f_6$ and $f_8$ in $y$ can also cause a difficulty in approximating $F(x)$ in a neighborhood by $f(x, \tilde{y})$ with a single candidate $\tilde{y}$. 
We expect that the landscape of $F$ is relatively easier to approximate for smooth convex--concave functions such as $f_5$, $f_7$, and $f_{11}$. 
However, if the worst scenario $\yw: x \to \yw(x)$ is continuous, yet very sensitive, then approximating the landscape of $F$ with a single candidate $\tilde{y}$ will be unreasonable.
Such a sensitivity is controlled by $B$ in the test problem definition.
The greater the greatest singular value of $B$ is, the more sensitive the worst scenario is.
In these situations, approximating the landscape of $F(x)$ locally around some point $\bar{x}$ by $f(x, \tilde{y})$ with a single candidate $\tilde{y} \approx \yw(\bar{x})$ is inadequate. 

\section{Limitations of existing approaches} \label{sec:limit}


%\Cref{sec:intro}に述べたように，black-box min--max optimizationにおいて，\zopgda{}\cite{Liu2020}と\acma{}\cite{akimoto2022berthing}は有望なアプローチであると考えられる．しかし，特徴(III)を有する問題に対しては十分とは言えない．本章では，これら二手法の課題を述べる．

As mentioned in \Cref{sec:intro}, \zopgda{} \cite{Liu2020} and \acma{} \cite{akimoto2022berthing} are promising approaches for the black-box min--max optimization. 
%In this section, we explain these two approaches and discuss their limitations.
%
% 既存手法\zopgda{}と\acma{}はともに，local min--max saddle point $(x^*, y^*)$ への収束を期待して，設計されている．反復$t$での解候補を$(x^{t}, y^{t})$として，いずれの方法も以下のように更新を繰り返す．
% \begin{align}
% (x^{t+1}, y^{t+1}) = (x^t, y^t) + (\eta_x \cdot B_x, \eta_y \cdot B_y) , \label{eq:xyupdate}
% \end{align}
% ここで、$\eta_x$と$\eta_y$は学習率を表し、$B_x$と$B_y$は$x$と$y$の更新方向を表す．\zopgda{}の場合，$B_x$と$B_y$は目的関数の近似勾配用いて$(B_x, B_y) = (-\widehat{\nabla_{x} f}(x^t, y^t), \widehat{\nabla_{y} f}(x^t, y^t))$と表される．学習率は目的関数毎に調整が必要となる．\acma{}の場合，$\argmin_{x \in \mathbb{X}}f(x, y^t)$および$\argmax_{y \in \mathbb{Y}}f(x^t, y)$を(1+1)-CMA-ES \cite{Arnold2010} を用いて近似的に最適化することにより得られる$\bar{x}^t$および$\bar{y}^t$を用いて，$B_x = \bar{x}^t - x^t$および$B_y = \bar{y}^t - y^t$と表される．
% 学習率は，最適化過程において適応的に決定されるため，問題毎の調整が不要となっている．
%
Both of the existing approaches are designed to converge to a strict local min--max saddle point $(\tilde{x}, \tilde{y})$. Let $(x^{t}, y^{t})$ be a pair of the solution candidate and the scenario candidate at iteration $t$. These approaches update it as 
\begin{align}
(x^{t+1}, y^{t+1}) = (x^t, y^t) + (\eta_x \cdot B_x, \eta_y \cdot B_y) , \label{eq:xyupdate}
\end{align}
where $\eta_x$ and $\eta_y$ denote the learning rates, and $B_x$ and $B_y$ denote the update vectors for $x$ and $y$, respectively. 
In \zopgda{}, $(B_x, B_y)$ consists of approximate gradients of the objective function, $(-\widehat{\nabla_{x} f}(x^t, y^t), \widehat{\nabla_{y} f}(x^t, y^t))$. The learning rates need to be tuned for each problem.
In \acma{}, $(B_x, B_y)$ consists of $(\bar{x}^t - x^t, \bar{y}^t - y^t)$, where $\bar{x}^t$ and $\bar{y}^t$ are approximations of $\argmin_{x \in \mathbb{X}}f(x, y^t)$ and $\argmax_{y \in \mathbb{Y}}f(x^t, y)$, respectively, obtained by (1+1)-CMA-ES \cite{Arnold2010}.
The learning rates are adapted during the optimization to alleviate tedious parameter tuning.

% 上記二つの既存手法は、目的関数がtwice continuously differentiable で globally strongly convex-concaveであり，その勾配がLipschitz continuousである場合に， global min--max saddle pointおよびその近傍に収束することが理論的に保証されている\cite{akimoto2022berthing,Liu2020}．とりわけ，文献\cite{akimoto2022berthing}では，一次収束の十分条件が示されている．Globally strongly convex-concave関数のもとでは，ワーストケース関数の最適解が唯一のglobal min--max saddle point を成すため，これらの方法の最適性が保証されている．理論保証は無くとも，目的関数が最適解近傍でlocally strongly convex-concaveかつ勾配がlocally Lipschitz continuousである場合にも、\eqref{eq:xyupdate}を用いて$x$と$y$を交互に更新することは有効であると考えられる。これは，目的関数がlocally strongly convex-concave である場合，局所ワーストシナリオ$\tilde{y}_{\mathrm{worst}}(x)$は，local min--max saddle point 近傍において滑らかであり，$x^{t}$から$x^{t+1}$への変化が小さければ、ワーストシナリオ$\yw(x^{t})$から$\yw(x^{t+1})$への変化も小さいと考えられるためである。一度$y^t$が$\yw(x^{t})$を高い精度で推定できていれば、\eqref{eq:xyupdate}中の学習率$\eta_y$を十分に小さく設定することで、$x$の変化してもワーストシナリオを追従できると考えられる。


The above two existing approaches are theoretically guaranteed to converge to the global min--max saddle point \cite{akimoto2022berthing} or its neighborhood \cite{Liu2020} when the objective function is twice continuously differentiable and globally strongly convex-concave. 
Because the global min--max solution $x^*$ is the the global min--max saddle point of $f$ in such problems, it implies the convergence toward $x^*$ or its neighborhood.
In particular, the previous study \cite{akimoto2022berthing} showed sufficient conditions for linear convergence. 
% It may be intuitively understood as follows.
% As described in \Cref{sec:formulation}, the worst scenario $\yw(x)$ is continuous. 
% It implies that the change from $\yw(x^{t})$ to $\yw(x^{t+1})$ can be arbitrarily small by setting a sufficiently small $\eta_x$. 
% Therefore, if $y^t$ approximates $\yw(x^{t})$ with high accuracy, $\yw(x^{t+1})$ can be also approximated with high accuracy by $y^{t+1}$ as long as $\eta_y$ is sufficiently small as well. 
Although the global convergence is not theoretically guaranteed, updating $x$ and $y$ alternately as in \eqref{eq:xyupdate} is expected to converge toward a local min--max saddle point if the objective function is a locally smooth and strongly convex-concave around the local min--max saddle point.
%and the change of the worst-case scenario with changing the solution candidate from $x^{t}$ to $x^{t+1}$ is not sensitive.


% 一方で、既存手法について複数の課題が報告されている\cite{akimoto2022berthing}。これらの中で、本研究で対象している問題に既存手法を適用する際の2つのDifficultyを記す。下記では、設計変数の次元数を$m=1$でシナリオの次元数を$n=1$とした4つの例題$f_{ex1}$--$f_{ex4}$を使用して各Difficultyについて説明する。


On the other hand, the previous study \cite{akimoto2022berthing} has reported several limitations of the existing approaches. Among them, we focus on the following two limitations in this study. 
%In the following, each difficulty is explained using four example problems $f_{ex1}$--$f_{ex4}$ with $m=1$ for the dimension of the design vector and $n=1$ for the scenario vector.

\paragraph{Difficulty (I): slow convergence on smooth strongly convex--concave problems}

% First, we discuss the slow convergence issue in situation (a): $\abs{\ywset(x^*)} = 1$ but $\yw(x)$ is sensitive around the global minimum solution $x^*$.
% As mentioned in \Cref{sec:formulation}, $x^*$ is a global min--max saddle point in this situation.しかし，目的関数がglobally strongly convex concave な場合，$x$と$y$の相互作用が強くなればなるほど，min--max saddle pointに収束するために膨大な$f$-callsを要することが報告されている\cite{akimoto2022berthing}。例えば、$f_{ex1}(x,y)=(a/2)x^2+bxy-(c/2)y^2$の問題を考える．このとき，$\yw(x) = (b/c) x$であるため，$\abs{b} \gg c$の場合，$x$ の変化に対して$\yw(x)$の変化が非常に大きくなる．更新式\eqref{eq:xyupdate}に従って，$x$の変化に対する$y$の変化に追従するためには，$x$の変化が十分に小さいことが必要であると，直感的に理解できる．逆に，$y$の変化に対して最適な$x$の変化が小さいことも必要となる．実際，一次収束のためには，学習率を$O(ac / (ac + b^2))$に比例して小さくする必要があることが，理論的に導かれている\cite{akimoto2022berthing}。また，最適解からの$\epsilon$近傍に収束までに費やす$f$-callsが，$(1+b^2/ac)$に比例することが，数値実験にも示されている\cite{akimoto2022berthing}．First-order approach (目的関数の勾配に関する情報を用いて最適解を探査する手法) でも同様の課題が報告されている\cite{Liang2019}おり，\zopgda{}は近似された目的関数の勾配を使用する点以外はFirst-order approachと同じ手法であるため、\zopgda{}でも同様の問題があると考えられる．\acma{} では，学習率が適応的に選択されるため，問題毎に調節しなければならないという問題は回避されるものの，その結果として収束速度の低下は免れない．Convex-concave であるがstrongly convex-concaveでない場合には，状況は更に悪くなる．例えば、目的関数が$f_{ex2}(x,y)=1/4x^4 + bxy - 1/4y^4$である問題を考える。この目的関数は、$f_{ex1}$と類似しているが、$a=1/2x^2$で$c=1/2y^2$となっている状況であると考えられる。このとき，学習率は$O(ac / (ac + b^2))$に比例して小さくする必要があるため、最適解周辺での解探査では学習率を極端に小さくなる。これにより、min--max saddle pointへ一次収束するという既存手法の利点が損なわれ、最適解への収束が困難になる。実際，そのような問題が，数値実験にて報告されている\cite{akimoto2022berthing}．

First, we discuss the slow convergence issue on smooth strongly convex--concave problems pointed out in \cite{akimoto2022berthing}. 
For example, consider a convex--concave quadratic problem $f_{ex1}(x,y)=(a/2)x^2+bxy-(c/2)y^2$. 
The worst scenario is $\yw(x)= (b/c) x$ for each $x$ and the optimal solution is $\hat{x}(y) = - (b/a) y$ for each $y$. 
It is intuitively understood that both $\yw(x)$ and $\hat{x}(y)$ should not be too sensitive to follow their change by \eqref{eq:xyupdate}. 
In fact, it has been theoretically derived that for linear convergence, the learning rate must be set as $\eta_x, \eta_y \in O(ac / (ac + b^2)$ and the required number of iterations to find near-optimal solution is $\Omega(1 + b^2 / (ac))$. We refer to \cite{akimoto2022berthing} for details. 
A similar limitation has been reported for the simultaneous gradient descent ascent (SGDA) approach \cite{Liang2019}. 
The same limitation is expected to exist in \zopgda{} because it is regarded as the approximation of the SGDA approach. 
The adaptation of the learning rates in \acma{} can mitigate the difficulty in tuning learning rates.
However, it cannot avoid the issue of slow convergence. 

The situation is worse if the objective function is convex-concave but not strongly convex-concave. 
For example, consider $f_7$ with $n = m = 1$ and $B = b$. 
This objective function is similar to $f_{ex1}$, but the coefficients are regarded as $a=(1/2)x^2$ and $c=(1/2)y^2$, i.e., decreasing as the solution approaches the global min--max saddle point $(x^*, y^* = \yw(x^*))$.
In this problem, the learning rate must converge toward zero as the solution approaches $(x^*, y^*)$.
This jeopardises the advantage of the existing approaches, i.e., linear convergence to the min--max saddle point.
In fact, the previous study \cite{akimoto2022berthing} has reported such an issue empirically.

\paragraph{Difficulty (II): non-convergence to a min--max solution that is not a strict min--max saddle point}

% Next, we discuss the non-convergence issue in situation (b): $\abs{\ywset(x^*)} > 1$. In this situation, $x^*$ is unlikely to be min--max saddle point. Then, approaches trying to locate min--max saddle points fails to converge to $x^*$. これに関して、2つの例をあげる。
% まず一つ目は、目的関数が$f_{ex3}(x,y)=(x+ \sin(\pi y))^2$となる問題であり，これについては\Cref{sec:formulation}にて述べたとおりであり，\acma{}や\zopgda{}は収束に失敗することが予想される．
% ２つ目の例は，$[-1,1] \times [-1,1]$の探査範囲で$f_{ex4}(x,y)=xy$の最適解を探査する問題である．この問題も，$f_{ex3}$と同様，ワーストシナリオは$x^* = 0$を堺に$-1$と$1$とで不連続に切り替わる．ただし，$f_{ex4}$の場合，$x^*$は weak global min--max saddle point を成す．これは，$\ywset(x^*) = \mathcal{Y}$ となることに起因する．この問題の場合にも，\acma{}では数値実験において，$x^*$に収束失敗することが示されている．また，first-order approach においても，収束できないことが理論的に報告されており\cite{Liang2019}，その近似である\zopgda{}も収束できないことが予想される．他方，$f_{ex4}$の場合には更新方法を工夫することで，first-order approach でも最適解へ収束可能であることが報告されており\cite{Liang2019}，$f_{ex3}$とは難しさが異なると考えられる．

Next, we discuss the non-convergence issue on problems where $x^*$ is not a strict min--max saddle point. 
The existing approaches fail to converge to $x^*$. 
Such a situation happens when the objective function is not strictly convex--concave. 
The situations can be categorized into two:
(W) $x^*$ is a weak min--max saddle point; and (N): $x^*$ is not a min--max saddle point. 
Among the test problems in \Cref{tab:testp}, $f_1$ and $f_2$ fall into category (W), and $f_4$, $f_9$, and $f_{10}$ fall into category (N).
A numerical experiment in \cite{akimoto2022berthing} has shown that \acma{} fails to converge to $x^*$ on such problems. 
A theoretical investigation in \cite{Liang2019} have reported that the SGDA fails to converge as well. 
Therefore, \zopgda{} is also expected to fail. 
The previous study \cite{Liang2019} has also reported that with some modifications SGDA can converge to the weak global min--max saddle point on bi-linear functions.
The existing approaches may be able to tackle problems of category (W) by incorporating such a modifications. 
However, problems of category (N) can not be solved. 

In the experiments in this study, we also confirm that there exists a situation where the existing approaches fail to converge even if $x^*$ is a strict global min--max saddle point. Example functions are $f_6$ and $f_8$, which are strictly convex-concave but not smooth. 
The situation where $x^*$ is a strict global min--max saddle point but $f$ is non-smooth is denoted as category (S). 


% We provide two examples. The first example is $f_{ex3}(x,y)=(x+ \sin(\pi y))^2$ (Type-3) as mentioned in \Cref{sec:formulation}. Because $x^*$ is not a min-max saddle point, both \acma{} and \zopgda{} fail to converge to $x^*$. 
% The second example is $f_{ex4}(x,y)=xy$ (Type-4), where the search domain is $[-1,1] \times [-1,1]$. 
% In this case, the optimum solution to the worst-case function $F(x) = \abs{x}$ is $x^* = 0$ and the worst scenario set is $\ywset(x^*) = \mathbb{Y}$. Except for $x^*$, the worst scenario set is singleton, and $\yw(x) = -1$ if $x < 0$, and $\yw(x) = 1$ if $x > 0$. 
% Similarly to $f_{ex3}$, the worst scenario switches discontinuously between $-1$ and $1$. 
% However, differently from $f_{ex3}$, $x^*$ is at the weak global min--max saddle point.
% A numerical experiment in \cite{akimoto2022berthing} have shown that \acma{} fails to converge to $x^*$. 
% A theoretical investigation in \cite{Liang2019} have reported that the SGDA fails to converge as well. 
% Therefore, \zopgda{} is also expected to fail.\footnote{The previous study \cite{Liang2019} has also reported that with some modifications SGDA can converge to the weak global min--max saddle point on $f_{ex4}$. Therefore, the characteristics of $f_{ex3}$ and $f_{ex4}$ are considered to be different.}
% The last example is $f_{ex5}(x,y) = -x^2 + 2xy - (1/4)y^2$ (Type-2), where the worst scenario $\yw(x) = 4x$. 
% That is, $\abs{\ywset(x)} = 1$ and $\yw(x)$ is continuous everywhere on $\mathbb{X}$. 
% The global min--max solution $x^* = 0$ forms a critical point ($\nabla f(0, 0) = 0$), but it is not a min--max saddle point.


% 以上の議論から，本研究で対象としている問題に既存手法\zopgda{}と\acma{}を適用した場合、Difficulty (I)もしくは(II)がlimitationになると考えられる。上記2つのDifficultyは本研究の数値実験\Cref{sec:test}においても確認されている。

% From the above discussion, when the existing approaches \zopgda{} and\acma{} are applied to our target problems, they will suffer from the convergence due to Difficulty (I) or (II). The above two Difficulties are also confirmed in the numerical experiments in this study.


\paragraph{Direction to address Difficulties (I) and (II)}

%Difficulty (I)と(II)はいずれも、$x$と$y$を交互に探索することでmin-max saddle pointを探索する、という方法論に起因していると考えられる。我々が対象とす問題において上記Difficultyを回避する一つの方法として、ワーストケース関数を近似して、これを直接最適化する方法が考えられる。このアプローチがDifficulty (I)と(II)に有効と考えらる理由を，ワーストケース関数が厳密に得られるという仮定のもと，以下に説明する。

% Difficulty (I) and (II) are possibly caused by the methodology that searches for the min-max saddle point by alternately updating $x$ and $y$. 
% As discussed in \Cref{sec:testp}, the local landscape of the worst case objective function $F$ will not be approximated well by $f(x, y)$ with a single point $y$ on problems in categories (W), (N), and (S).
One approach to avoid Difficulty (II) is to approximate the worst-case function $F$ by solving $\max_{y \in \mathbb{Y}} f(x, y)$ numerically and optimize it directly.
If $F$ can be approximated well for each $x \in \mathbb{X}$, i.e., $\max_{y \in \mathbb{Y}} f(x, y)$ can be solved efficiently for each $x$, and $F$ can be globally optimized efficiently by a numerical solver, it does not matter whether $x^*$ is a min--max saddle point or not. Therefore, Difficulty (II) can be addressed naturally. 
%
% Here, we explain why this approach is considered effective for Difficulty (I) and (II). In the following, we assume that $F$ can be approximated well for each $x$.
% %The explanations are based on the assumption that the worst-case functions are obtained exactly.
%%
%
% まず、Difficulty (I)について説明する。任意の strongly convex-concave関数は，そのthe global min--max saddle point周辺において、a convex-concave quadratic関数によって近似できる。そこで、簡単のため、目的関数が a convex-concave quadratic関数$f(x, y) = \frac12 x^\T A x + x^\T B y - \frac12 y^\T C y$となる場合を考える。ここで、$A \in \R^{m\times m}$と$C \in \R^{n \times n}$は 正定値対称行列、 $B \in \R^{m \times n}$は任意の行列、である。ワーストシナリオは唯一で$\yw(x) = C^{-1} B^\T x$であり、ワーストケース関数は$F(x) = f(x, \yw(x)) = \frac12 x^\T \big(A + B C^{-1} B^\T \big) x$である。
% \Cref{eq:jacoby}に示したとおり，ワーストシナリオ$\yw(x)$のヤコビアンは$C^{-1} B^\T$となり，これが大きな特異値を持つ場合には問題となる．
% しかし，ワーストケース関数を直接最適化する場合，$F$は凸二次関数である．Covariance Matrix Adaptation Evolution Strategy (\cmaes{} : \cite{Hansen2001, Hansen2014, akimoto2019}) のようなsecond-order informationを使用する最適化手法では、ヘッセ行列に依存せずに同じ収束速度で一次収束することが実験的に示されている\cite{invariance}。
% このため、ワーストケース関数$F$を直接最適化するというアプローチは、収束のために消費される$f$-callsがワーストシナリオ$\yw(x)$のヤコビアンに依存しにくいと考えられる。目的関数がweakly convex-concaveな場合，上の議論は成立しない．しかし，例えば前述の$f_{ex2}$を考えると，$\yw(x) = (b x)^{1/3}$であり，$F(x) = (1/4) x^4 + (3/4)(b x)^{4/3}$となる．これは$x^*=0$においてsmoothでないものの，strongly convexであるため，最適解への収束は可能であると考えられる．

We also expect that it can be a solution to Difficulty (I).
Because any smooth strongly convex-concave function can be approximated by a quadratic convex-concave function around the global min--max saddle point, for simplicity, we focus on $f_{ex1}$.
Its worst-case function is $F(x) = \frac12 (a + b^2/c) x^2$. 
Because it is a convex quadratic function, a reasonable solver converges linearly toward its global minimum point $x^*$.
In case of $n > 1$ and $m > 1$, the worst case function can be ill-conditioned. 
However, if we employ a solver that uses second-order information, such as covariance matrix adaptation evolution strategy (CMA-ES) \cite{Hansen2001, Hansen2014, akimoto2019}, we expect that it can be solved efficiently. Therefore, the number of $f$-calls spent by the approach that directly optimizes $F$ is expected to be less sensitive to the interaction term.
If the objective function is smooth weakly convex-concave, this argument does not hold. However, considering the aforementioned example $f_{7}$, we have $F(x) = (1/4) x^4 + (3/4)(b x)^{4/3}$, which is not smooth at $x^*=0$ but strongly convex. Therefore, we expect that a comparison-based approach, which is invariant to any increasing transformation of the objective function, can solve it efficiently.

% 次に、Difficulty (II)について説明する。
% 前述の$f_{ex3}$の場合，ワーストケース関数は$F(x) = (\abs{x} + 1)^2$となる．
% これは，$f_{ex2}$の場合と同様，$x^*=0$においてsmoothでないものの，strongly convexな関数である．
% また，$f_{ex4}$の場合，ワーストケース関数は$F(x) = \abs{x}$となる．
% こちらは，$x^*=0$においてsmoothでないconvex関数である．
% CMA-ESのようなランキングベースの方法を用いる場合，$F$の最適化とこれを単調増加変換させた関数の最適化は等価であるから，$F(x) = \abs{x}$の最適化は$\abs{x}^2$の最適化，すなわち$x^*=0$においてsmoothでないものの，strongly convexな関数の最適化と相違ないことに注意されたい．
% したがって，いずれの場合にも最適解への収束は可能であると期待される．


% Because they are all convex, we expect that they can be efficiently solvable by comparison-based approaches such as \cmaes, for which the smoothness of the function itself is not necessary to be effective.
% Therefore, as long as the worst case objective function $F$ can be solvable, we expect that directly minimizing $F$ can address Difficulty (II).

% 以上より、ワーストケース関数$F$を直接最適化するというアプローチはDifficulty (I)および(II)のいずれに対しても有効であると期待される．
% 一方，$\abs{\ywset(x^*)} > 1$であること，もしくは$\abs{\ywset(x^*)} = 1$であるが$\yw(x)$が$x$の変化に対してsensitiveであることは，ワーストケース関数をnon-smoothにする一因であると予想される．このとき，収束は可能であっても，悪スケール性に起因して収束速度が低下する可能性が考えられる．この点は数値実験により確認する．

% From the above, the approach which directly optimizing the worst-case function $F$ is expected to be effective for both Difficulty (I) and (II).
% On the other hand, assumption (III) that $\abs{\ywset(x^*)} > 1$ or $\abs{\ywset(x^*)} = 1$ but $\yw(x)$ is sensitive to the change in $x$ is expected to be one of the factors that make the worst-case function non-smooth. In this case, even if convergence is possible, the convergence speed may be reduced due to the ill-scale. This point will be confirmed through numerical experiments.

\section{Proposed approach}\label{sec:proposed}

% 本研究では、black-box min--max optimization problem \eqref{eq:minmax} を対象とし、Worst-case Ranking Approximation (\wra{}) により近似されたワーストケース関数をCovariance Matrix Adaptation Evolution Strategy (\cmaes{} : \cite{Hansen2001, Hansen2014, akimoto2019})  により直接最小化するアプローチを提案する。
% 提案アプローチでは、近似された$F$について最適解を探査するため、\Cref{sec:limit}に通り、Difficulty (I)と(II)に有効であると期待できる。
% 提案アプローチ中の\wra{}では、\cmaes{}より生成された解候補$x$のワーストケース関数値$F(x)$を推定するために、\eqref{eq:minmax}の内側の最大化問題$\max_{y \in \mathbb{Y}}f(x,y)$を近似的に解く。また、最大化問題を近似的に解く際の$f$-callsを少なくするためにwarm starting strategy、Early stopping strategyとFuture preparing strategy を用いている。
% ここでは、提案アプローチと\wra{}の全体のフレームについて述べ、最大化問題を解くソルバーとして\cmaes{}を使用したアルゴリズム\wracma{}と\slsqp{}を使用したアルゴリズム\wraslsqp{}について紹介する。


We propose a new approach to address Difficulties (I) and (II). 
The main idea is to directly minimize the worst-case objective function $F$.
The bottleneck of directly minimizing $F$ in the black-box min-max optimization setting is the computational time for each $F(x)$ evaluation, which requires to solve maximization problem $\max_{y \in \mathbb{Y}} f(x, y)$ approximately. 
To tackle this bottleneck, we propose to employ CMA-ES to minimize $F$ (\Cref{sec:cmaes4F}), and propose worst-case ranking approximation (WRA) mechanism that approximates the ranking of $\{F(x_i)\}_{i=1}^{\lambda_x}$ for given solution candidates $\{x_i\}_{i=1}^{\lambda_x}$ (\Cref{sec:wra}).


% In \wra{}, the worst case function value $F(x)$ for each solution candidate $x$ generated from \cmaes{} is estimated by approximately optimizing the maximization problem $\max_{y \in \mathbb{Y}}f(x,y)$ inside \eqref{eq:minmax}. In addition, \wra{} uses warm starting strategy, Early stopping strategy and Post-process for next warm starting to reduce $f$-calls for the internal maximization problem.
% In this section, we describe the overall frame of the proposed approach and \wra{}, and introduce the proposed algorithm \wracma{} and \wraslsqp{} using respectively \cmaes{} and \slsqp{} as solvers for approximately solving the internal maximization problem.

For the proposed approach to work effectively, we suppose (a) $\abs{\ywset(x)} = 1$ and $\yw(x)$ is continuous almost everywhere in $\mathbb{X}$, (b) the solver for the inner-maximization problem can globally maximize $f(x, y)$ with respect to $y$ efficiently for each $x \in \mathbb{X}$, and (c) the solver for the outer-minimization problem, here CMA-ES, can minimize $F$ efficiently. 

\subsection{CMA-ES for outer minimization} \label{sec:cmaes4F}

% 提案アプローチでは、min--max問題\eqref{eq:minmax}の外側の最小化問題に対して、最適化手法CMA-ESを用いる。CMA-ESはblack-box continuous 関数$h(x)$に対するstate-of-the-art derivative-free approachであることが知られている\cite{hansen2009, Hansen2010}。また、CMA-ESはquasi-parameter-free approachである。適用する問題へのパラメータチューニングが不要な点で実用性が高いアプローチであり、多くの実問題で適用された事例が報告されている\cite{miyagi@ghgt, maki2020, urieli2011, fujii2018, tanabe2021}。

The proposed approach tries to solve the outer minimization problem of \eqref{eq:minmax} by CMA-ES.
CMA-ES is known as a state-of-the-art derivative-free optimization approach for black-box continuous optimization problems \cite{hansen2009, Hansen2010, rios2013derivative} and has been applied to many real-world applications \cite{miyagi@ghgt, maki2020, urieli2011, fujii2018, tanabe2021}. 
There are two important characteristics of CMA-ES that attracts practitioners.
One is that it is a quasi-parameter-free approach, i.e., one does not need any hyper-parameter tuning except for a population size $\lambda_x$, which is desired to be increased if the problem is multimodal or noisy, or if many CPU cores are available.
Because the worst-case function $F$ is black-box and it is hard to understand the characteristics of $F$ in advance, the parameter-free nature is important.
The other is that it is parallel implementation friendly. 
The objective function values ($F$ in our case) of multiple solution candidates generated at an iteration can be evaluated in parallel. 
It is desired when the computational cost of the objective function evaluation is high. 
Because each evaluation of $F$ is expensive as it requires solving maximization problem $\max_{y \in \mathbb{Y}} f(x, y)$ approximately, it is practically very important.


% Min--max問題\eqref{eq:minmax}の外側の最小化問題に対して、最適化手法CMA-ES使用する際、各反復$t$で以下の処理を順番に行うことで最適解を探査する。
% まず、ガウス分布$\mathcal{N}(m^t, \Sigma^t)$より$\lambda$個の解候補$\{x_1,...,x_{\lambda}\}$を生成する。次に、各解候補についてワーストケース関数値$\{F(x_{i})\}_{i=1}^\lambda$を評価する。
% このとき、ワーストケース関数値$\{F(x_{i})\}_{i=1}^\lambda$についてのランキング$\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda})$をつける。最後にこのランキング情報を用いてガウス分布のパラメータである平均ベクトル$m^t$と共分散行列$\Sigma^t$、進化パス等の動的なパラメータ$\theta$を更新する。これらの処理は、ガウス分布が収束したとみなされるまで繰り返される。

CMA-ES repeats the sampling, evaluation, and update steps until a termination condition is satisfied. 
Let $t \geq 0$ be the iteration counter.
First, $\lambda_x$ solution candidates $\{x_i\}_{i=1}^{\lambda_x}$ are generated independently from a Gaussian distribution $\mathcal{N}(m_x^t, \Sigma_x^t)$ with mean vector $m_x^t \in \mathbb{X}$ and covariance matrix $\Sigma_x^t \in R^{m \times m}$.
Next, the worst-case function values of $\lambda_x$ solution candidates, $\{F(x_{i})\}_{i=1}^{\lambda_x}$, are evaluated and there rankings $\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda_x})$ are computed, where $i$th ranked solution candidate has the $i$th smallest $F$ value. 
Finally, CMA-ES updates the distribution parameters, $m^t_x$ and $\Sigma^t_x$, and other dynamic parameters using solution candidates and their rankings. 
An important aspect of the update of CMA-ES is that it is comparison-based. 
That is, as long as the rankings of the solution candidates, $\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda_x})$, are computed, the worst-case function values, $\{F(x_i)\}_{i=1}^{\lambda_x}$, does not need to be accurately computed.


In this study, we implemented the version of CMA-ES  proposed in \cite{akimoto2019}, namely dd-CMA-ES%
\footnote{\url{https://gist.github.com/youheiakimoto/1180b67b5a0b1265c204cba991fa8518}}%
, as the default solver. 
The configuration of CMA-ES follows the default one proposed in \cite{akimoto2019}.
If the search domain has a box constraint, we employ the mirroring technique along with upper-bounding the coordinate-wise standard deviation $\sqrt{[\Sigma_x^t]_{l,l}}$ for $l = 1, \dots, m$ \cite{yamaguchi2018}.
The initial distribution parameters, $m_x^0$ and $\Sigma_x^0$, should be set problem-dependently. 
We terminate CMA-ES when $\max_l \sqrt{[\Sigma_x^t]_{l,l}} < V_{\min}^{x}$ is satisfied, where $V_{\min}^{x}$ is a problem-dependent threshold, or $\Cond(\Sigma_x) > \Cond_{\max}^{x}=10^{14}$ is satisfied, where $\Cond(\Sigma_x)$ is the condition number, i.e., the ratio of the greatest and smallest eigenvalues, of $\Sigma_x^t$.

\subsection{Worst-case Ranking Approximation}\label{sec:wra}

\providecommand{\ny}{N_\omega}

% ワーストケース関数$F$に対して直接最適解を探査するという提案アプローチを実現するためには、\Cref{sec:cmaes4F}で述べた通り、\cmaes{}より生成された各解候補$x$についてワーストケース関数値を評価する必要がある。しかし、目的関数$f$はblack-box関数なため、正確に解くことはできない。そこで、なんらかの最適化手法 (ソルバーと呼称する) を用いて各解候補について$\max_{y \in \mathbb{Y}}f(x,y)$を解き、ワーストケース関数値$F(x)$を近似することが考えられる。このとき、生成された各解候補$x_i$ ($i=1,...,\lambda$) について$\max_{y \in \mathbb{Y}}f(x_i,y)$を高い精度で解いた際のランクは$\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda})$と同等であると考えられる。しかし、各反復で、$\lambda$回$\max_{y \in \mathbb{Y}}f(x,y)$を解くため、min--max問題を解くために必要となる全体の$f$-callsが膨大となることが予想できる。
% そこで、少ない$f$-callsで$\max_{y \in \mathbb{Y}}f(x,y)$を近似的に解く\wra{} を提案する。\wra{}の疑似コードを\Cref{alg:wra}にまとめた。\wra{}は大きく分けて、warm starting strategy、Early stopping strategy、Future preparing strategyから構成されている。以下にこれらについて説明する。

The proposed WRA mechanism approximates the rankings of solution candidates by roughly solving maximization problems $\max_{y \in \mathbb{Y}} f(x_i, y)$ for each solution candidate $x_i$. 
%As mentioned above, the evaluation of $F$ values is the bottleneck in optimizing $F$ directly.
To save the internal $f$-calls to approximate the rankings $\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda_x})$, we incorporates a warm starting strategy, where we try to start each maximization $\max_{y \in \mathbb{Y}} f(x_i, y)$ with a good initial solution candidate and a good configuration of the internal solver (\Cref{sec:warmstart}), and an early stopping strategy, where we try to stop each maximization $\max_{y \in \mathbb{Y}} f(x_i, y)$ once $\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda_x})$ are considered well approximated (\Cref{sec:earlystop}).
The overall framework is summarized in \Cref{alg:wra}.

Hereunder, let $\mathcal{M}$ be a solver used to approximately solve $\max_{y \in \mathbb{Y}} f(x_i, y)$. 
Let $\omega$ represent configurations of the solver $\mathcal{M}$ that are inherited over the WRA calls, and $\theta$ represent the other configurations that are not inherited.
%For example, if \cmaes{} is used as a solver $\mathcal{M}$, configuration $\omega$ includes distribution parameters $m$ and $\Sigma$, other dynamic parameters denoted as $\theta$, and static parameters. Given a configuration $\omega$, we call $\mathcal{M}(\omega)$ an instance of solver $\mathcal{M}$. 

% Therefore, for each solution candidate, solving $\max_{y \in \mathbb{Y}}f(x,y)$ by some optimization approaches (referred as a solver in this study) is one approach.
% When $\max_{y \in \mathbb{Y}} f(x_i,y)$ is solved for each $i=1,\dots,\lambda$ with high accuracy, it is possible to obtain the ranking equivalent to $\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda})$. However, the overall $f$-calls to obtain the optimal solution $x^*$ is expected to be huge because $\lambda$ times the maximization problems $\max_{y \in \mathbb{Y}}f(x,y)$ is solved in each iteration.
% Therefore, we propose \wra{} which approximately solves the maximization problem $\max_{y \in \mathbb{Y}}f(x,y)$ with less $f$-calls. 
% The pseudo code of \wra{} is summarized in \Cref{alg:wra}. \wra{} is composed of three major parts : warm starting strategy, Early stopping strategy, and Post-process for next warm starting. These are explained below.


\begin{algorithm}[t]
	\caption{WRA: Worst-case Ranking Approximation}\label{alg:wra}
    \begin{algorithmic}[1]
    \Require $x_1, \dots, x_{\lambda_x}$ 
    \Require $\{({y}_k, \omega_k, p_k)\}_{k=1}^{\ny}$
    \Require $\tau_{\mathrm{threshold}}$, $p_{\mathrm{threshold}}$, $p_p$, $p_n$
    
    \State\label{line:wra-initstart} \texttt{// Warm starting}
    \For{$i = 1, \dots, \lambda_x$}
    \State evaluate $f(x_{i}, y_k)$ for all $k = 1,\dots,\ny$
    \State $k^\mathrm{worst}_{i} = \argmax_{k \in \{ 1, \dots,\ny\}} f(x_i, y_k)$
    \State $\hat{y}_i=y_{k^\mathrm{worst}_{i}}$, $\tilde{\omega}_i = \omega_{k^\mathrm{worst}_{i}}$, and $F^0_i = f(x_i, y_{k^\mathrm{worst}_{i}})$
    \EndFor\label{line:wra-initend}
    
    \State\label{line:wra-worststart} \texttt{// Early stopping}
    \State initialize $\tilde{\theta}_1, \dots, \tilde{\theta}_{\lambda_x}$
    \For{$\mathrm{rd} = 1,2,\dots$}
    \For{$i = 1, \dots, \lambda_x$}
    % \State Perform a round of $\mathcal{M}(\tilde\omega_{i})$ and obtain an approximate solution $\hat{y}_i$ and $F_{i}^{\mathrm{rd}} = f(x_i, \hat{y}_i)$
    \State $F_{i}^{\mathrm{rd}}, \hat{y}_i, \tilde{\omega}_i, \tilde{\theta}_i \gets \mathcal{M}(F_{i}^{\mathrm{rd}-1}, \hat{y}_i, \tilde{\omega}_i, \tilde{\theta}_i)$ 
    \EndFor
    \State $\tau = \text{Kendall}(\{F^{\mathrm{rd}-1}_{i}\}_{i=1}^{\lambda_x}, \{F^{\mathrm{rd}}_{i}\}_{i=1}^{\lambda_x})$
    \State \textbf{break if} $\tau > \tau_{\mathrm{threshold}}$
    \EndFor\label{line:wra-worstend}
    
    \State \texttt{// Post-processing}\label{line:wra-poststart}
    \State $S^\mathrm{worst} = \{{k^\mathrm{worst}_{i}} \text{ for } i = 1,\dots,\lambda_x\}$ %without duplication
    \For{$\tilde{k} \in S^\mathrm{worst}$}
    \State $\ell = \argmin_{i=1,\dots,\ny} \{F^{\mathrm{rd}}_i \mid k^\mathrm{worst}_i = \tilde{k}\}$ 
    \State $y_{\tilde{k}} = \hat{y}_\ell$, $\omega_{\tilde{k}} = \tilde{\omega}_\ell$  
    % \State Update $\omega_{\tilde{k}}$ based on $\tilde{\omega}_\ell$ 
    \State $p_{\tilde{k}} = \min(p_{\tilde{k}} + p_p, 1)$
    \EndFor
    \State $p_k = p_k -  p_n \ind{k \notin S^\mathrm{worst}}$ for all $k=1,...,\ny$
    \For{$k = 1,\dots,\ny$}
    \State refresh $(y_k, \omega_k, p_k)$ \textbf{if} $p_k < p_{\mathrm{threshold}}$
    \EndFor
    \label{line:wra-postend}
    \State \Return $\{F_i^{\mathrm{rd}}\}_{i=1}^{\lambda_x}$ and $\{({y}_k, \omega_k, p_k)\}_{k=1}^{\ny}$ for the next call
  \end{algorithmic}
\end{algorithm}


\subsubsection{Warm starting strategy}\label{sec:warmstart}

%% from post-process
Two key ideas behind the design of our warm starting strategy are as follows. 

% まず、$\max_{y \in \mathbb{Y}}f(x_i, y)$を解くために使用された後のソルバーのパラメータ $\tilde{\omega}_i$は$k_i^{\mathrm{worst}}$番目のパラメータ$\omega_{k_i^{\mathrm{worst}}}$として継承される。これは、\cmaes{}の以下の点を利用している。
% 最適化手法\cmaes{}のガウス分布$\mathcal{N}(m^t, \Sigma^t)$は1回の反復では大きく変化しない。つまり、反復$t$で生成された解候補と反復$t+1$で生成された解候補はほぼ同様に分布すると考えられる。このため、反復$t$で生成された解候補のワーストシナリオの分布は、反復$t+1$での解候補に対するワーストシナリオの分布と類似していると考えられる。
% これより、ある反復$t$の解候補$x$に対してワーストシナリオを探査したソルバーのパラメータは次の反復$t+1$でも有効であることが期待できる。
% もしも、$k_i^{\mathrm{worst}}$が重複したときは、$k_i^{\mathrm{worst}}$の重複なしのインデックス集合を$S^{\mathrm{worst}}$とし、解候補のインデックス$l=\argmin_{i=1,\dots,\lambda} \{F^z_i \mid k_i^{\mathrm{worst}} = S^{\mathrm{worst}}\}$のパラメータ$\tilde{\omega}_l$を$k_i^{\mathrm{worst}}$番目のパラメータとして継承する。

First, we carry over the worst scenario candidates and the configurations from the last WRA calls. 
The Gaussian distribution $\mathcal{N}(m^t, \Sigma^t)$ of CMA-ES for the outer minimization does not change significantly in one iteration. 
Then, the distribution of the worst scenarios for the solution candidates generated at iteration $t$ is considered to be similar to that at iteration $t+1$.
Therefore, we expect that using the solver configurations used at the last iteration will contribute to reduce the number of $f$-calls. 
This idea is expected to be effective for the problem where $\abs{\ywset(x)} = 1$ and $\yw(x)$ is continuous almost everywhere in $\mathbb{X}$.  

% 次に、$\ny$個のパラメータの多様性を確保する。これは、解候補の小さな更新でワーストシナリオが大きく変化する問題に備えるためである。例えば、目的関数がbi-linear関数$f(x,y)=xy$となるような問題である。この問題は\Cref{sec:limit}で説明した通り、最適解周辺でワーストシナリオが大きく変化する。このような問題に備えて、warm starting strategyで選択されなかったパラメータは定期的に新しくすることとする。新しくするパラメータと保存するパラメータを区別するためのconfidence vector$\{p_k\}_{k=1}^{\ny} \in (0,1)$を導入する。パラメータが更新されたインデックスのconfidenceは$p_p$により増加し、それ以外のインデックスのconfidenceは$p_n$により減少させる。そして、$p_k \leq p_{\mathrm{threshold}}$となった$k$番目のパラメータは新しくする。

Second, we maintain $\ny$ ($\geq 1$) configurations.
Consider situations (W) and (N) described in \Cref{sec:testp}.
The worst scenarios corresponding to solution candidates $\{x_i\}_{i=1}^{\lambda_x}$ generated in a single iteration may not be concentrated at one point, but may be distributed around $\abs{\ywset(x^*)}$ distinct points even if $\{x_i\}_{i=1}^{\lambda_x}$ are concentrated around $x^* = \argmin_{x \in \mathbb{X}} F(x)$. 
If we only maintain one configuration, it may be a good initial configuration only for a small portion of $\{x_i\}_{i=1}^{\lambda_x}$.
There is a high risk that $F$-values are accurately estimated only for these candidates and they are underestimated for the others due to insufficient maximization.
To tackle this difficulty, we maintain multiple configurations and try to keep them diverse. 

% warm starting strategyはある反復$t$で生成された各解候補$x_i$ ($i=1,...,\lambda$)についての最大化問題$\max_{y \in \mathbb{Y}}f(x_i,y)$を近似的に解く$\lambda$個のソルバーを適切に選択することで$f$-callsを少なくすることを目的としている。\Cref{alg:wra}中の\Cref{line:wra-initstart}--\Cref{line:wra-initend}がwarm starting strategyの箇所である。

% warm starting strategy aims to reduce $f$-calls by appropriately selecting $\lambda$ solvers for the maximization problem $\max_{y \in \mathbb{Y}}f(x_i,y)$ for each $i=1,\dots,\lambda$ at a certain iteration $t$.
These two ideas are implemented in the proposed warm starting strategy.
It consists of (1) selecting a good initial worst scenario candidate $\tilde{y}$ and configuration $\tilde{\omega}$ of solver $\mathcal{M}$ for each solution candidate $x_i$ among $\ny$ pairs $\{(y_k, \omega_k)\}_{k=1}^{\ny}$ (Lines~\ref{line:wra-initstart}--\ref{line:wra-initend} in \Cref{alg:wra}), and (2) preparing $\ny$ pairs $\{(y_k, \omega_k)\}_{k=1}^{\ny}$ for the next WRA call (Lines~\ref{line:wra-poststart}--\ref{line:wra-postend} in \Cref{alg:wra}).
%
For each $x_i$, we evaluate $f(x_i, y_k)$ for $k = 1, \dots, \ny$ and select the worst scenario candidate. 
Let $k_i^{\mathrm{worst}}=\argmax_k f(x_i, y_k)$ be the index of the worst scenario candidate among $\{y_k\}_{k=1}^{\ny}$. 
Then, we select the configuration $\omega_{k_i^{\mathrm{worst}}}$ of the solver that generated $y_{k_i^{\mathrm{worst}}}$ as the initial configuration $\tilde{\omega}_i$ to search for the worst scenario for $x_i$. 
% warm starting strategyでは、ソルバー用のパラメータ$\omega$を予め$\ny$個用意しておき、その中から実際に使用されるソルバーの初期値として使用されるパラメータ$\tilde{\omega}$を$\lambda$個選択する。ここで、$k$ ($k=1,...,\ny$) 番目のパラメータ$\omega_k$を初期値として使用したソルバーが生成したシナリオを$y_k$とし、パラメータの選択方法について説明する。まず、ある解候補$x_i$ ($i=1,...,\lambda$) に対して、各シナリオ$\{y_k\}_{k=1}^{y_\mathrm{member}}$の目的関数値$\{f(x_i, y_k)\}_{k=1}^{\ny}$を計算する。この$y_\mathrm{member}$個の目的関数値の中から、インデックス$k_i^{\mathrm{worst}}=\argmax_k f(x_i, y_k)$を求める。そして、$x_i$に対する最大化問題$\max_{y \in \mathbb{Y}}f(x_i, y)$を近似的に解くために、$k_i^{\mathrm{worst}}$番目のパラメータ$\omega_{k_i^{\mathrm{worst}}}$を初期値$\tilde{\omega}_i$として選択する。
% すべての解候補に対してまったく新しい初期値を使用するよりは、各解候補に対して悪いシナリオを生成する初期値を使用するほうが各ソルバーで消費される$f$-callsを少なくできると考えられる。なお、異なる複数の解候補が同一の初期値を選択することは許容する。
%
% warm starting strategy prepares $\ny$ parameter sets for the solver in advance, and selects $\lambda$ parameter sets to be used as initial parameters for the solver.
% Let $\omega$ be a parameter set for the solver, $\tilde{\omega}$ be the initial parameter set for the solver, and $y_k$ ($k=1,...,\ny$) be the scenario generated by the solver using the $k$-th parameter set $\omega_k$ as the initial parameter set $\tilde{\omega}$. Selection of the parameter set in warm starting strategy is described below. 
% First, for a given solution candidate $x_i$ ($i=1,.... ,\lambda$), the objective function value $\{f(x_i, y_k)\}_{k=1}^{\ny}$ for each scenario $\{y_k\}_{k=1}^{\ny}$ is calculated. From this $\ny$ of objective function values, the index $k_i^{\mathrm{worst}}=\argmax_k f(x_i, y_k)$ is obtained. 
% Next, the $k_i^{\mathrm{worst}}$-th parameter set $\omega_{k_i^{\mathrm{worst}}}$ is selected as the initial parameter set $\tilde{\omega}_i$ for the solver of the maximization problem $\max_{y \in \mathbb{Y}}f(x_i, y)$ for a given solution candidate $x_i$.
% Rather than using completely new initial parameter set for each maximization problems, using initial parameter set that generates worse scenario will reduce $f$-calls spent by each solver.
% Note that it is accepted for multiple different solution candidates to choose the same initial parameter set.
%
After approximating $\{F(x_i)\}_{i=1}^{\lambda_x}$, we update the set of configurations of the solver. 
Basically, we replace the selected configurations with the configurations obtained after the solver execution. 
If the same configuration is selected for different solution candidates, we replace the configuration with the one used for the solution candidate with the best approximated worst case value. 

Moreover, to avoid keeping configurations that have not been used, we refresh such configurations and try to have diverse configurations. For this purpose, we maintain a parameter $p_k\in (0,1]$ for $k = 1,\dots,\ny$. They are initialized as $1$. It is increased by $p_p$ if $k$th configuration is selected, and decreased by $p_n$ otherwise. Once we have $p_k \leq p_{\mathrm{threshold}}$, the $k$th configuration and the corresponding worst scenario candidate are refreshed in the same manner as their initialization, and $p_k$ is reset to $1$. 


\subsubsection{Early stopping strategy}\label{sec:earlystop}

% Early stopping strategyでは、$\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda})$が推定できた時点で$\lambda$個のソルバーを終了させることで、$f$-callsを節約することを目的としている。\Cref{alg:wra}中の\Cref{line:wra-worststart}--\Cref{line:wra-worstend}がEarly stopping strategyの箇所である。


Early stopping strategy aims to save $f$-calls by terminating $\lambda_x$ solvers once the rankings of the worst-case function values of given solution candidates, $\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda_x})$, are regarded as well-approximated. Early stopping strategy is described at Lines \ref{line:wra-worststart}--\ref{line:wra-worstend} in \Cref{alg:wra}.


% Early stopping strategyを設計するにあたり、\cmaes{} \cite{Hansen2001, Hansen2014, akimoto2019}の以下の特徴に着目した。
% \cmaes{}はcomparison based search algorithmであるため、ワーストケース関数を近似した関数$\bar{F}$でのランク$\mathrm{Rank}_{\bar{F}}(\{x_{i}\}_{i=1}^{\lambda})$が$\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda})$と同じ時、\cmaes{}はワーストケース関数での最適解探査と同様に振る舞う。つまり、$\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda})$が推定できれば、ワーストケース関数$F$を厳密に求める必要はなく、$f$-callsを節約できる。

The main idea is is as follows.
As mentioned earlier, CMA-ES is a comparison-based approach. 
Therefore, the worst-case function values are not needed to be accurately estimated as long as their rankings are computed. 
We further hypothesize that CMA-ES behaves similarly on the approximated rankings if the rankings of solution candidates are approximated with high correlation to the true rankings, in terms of Kendall \cite{kendall}. This hypothesis is often imposed in surrogate-assisted approaches and related approaches \cite{lqcmaes,multifidelity,constrainedmultifidelity,minmax,evolutioncontrol} and is validated partly in theory \cite{surrogatetheory}.
Because the true rankings of the worst-case function values are unavailable, instead of trying to check the rank correlation between the true rankings and the approximate rankings, we keep track of the change of the rankings and stop if the change is regarded as sufficiently small.

To compute the rankings of the worst-case function values, $\lambda_x$ solvers are run in parallel, and periodically compute the rankings of the solution candidates using the approximated worst-case function values, $\{F_i^{\mathrm{rd}} = f(x_i, \hat{y}_i)\}_{i=1}^{\lambda_x}$, where $\mathrm{rd} \geq 0$ is the number of ranking computations so far and is called the round. 
After each round, we compute the Kendall's rank correlation between the current and last approximations of the rankings, $\tau(\{F_i^{\mathrm{rd}-1}\}_{i=1}^{\lambda_x}, \{F_i^{\mathrm{rd}}\}_{i=1}^{\lambda_x})$. 
If it is greater than the pre-defined threshold $\tau_\mathrm{threshold} \geq 0$, we regard the rankings are well approximated and terminate the solvers. A reasonable definition of a round of a solver call depends on the choice of the solver. We discuss the choice of the solver and the definition of the round in the next section.

% which is the rank at the approximated objective function $\bar{F}$ and $\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda})$ are equivalent, \cmaes{} behaves like optimization for the worst-case function. 
% In other words, solvers do not need to approximate the worst-case function values $\bar{F}(x)$ more accurately than that required to approximate $\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda})$. 


% Early stopping strategyでは、ワーストケース関数値$\{F(x_i)\}_{i=1}^{\lambda}$と近似されたワーストケース関数値$\{\bar{F}(x_i)\}_{i=1}^{\lambda}$のケンドールの相関係数$\tau$\cite{kendall}が大きければ、$\lambda$個のソルバーによるワーストシナリオ探査を終了する。例えば、パラメータ$\tau_{\mathrm{threshold}} \in (0,1]$を用いて、$\tau \geq \tau_{\mathrm{threshold}}$となったら終了する。ケンドールの相関係数$\tau$が大きい\cmaes{}はお互い類似した振る舞いを示すことから、サロゲート関数の精度を評価するためによく用いられる \cite{hansen2019, akimoto2019multi, miyagi2021}。しかし、ワーストケース関数は得られないため、$\tau$は計算できない。そこで、$\tau$を推定する方法を考える必要がある。


% Early stopping strategy terminate $\lambda$ solvers for the worst-case scenario exploration when Kendall's correlation coefficient $\tau$ \cite{kendall} between the worst-case function value $\{F(x_i)\}_{i=1}^{\lambda}$ and the approximate worst-case function value $\{\bar{F}(x_i)\}_{i=1}^{\lambda}$ is high. 
% For example, using the parameter $\tau_{\mathrm{threshold}} \in (0,1]$, if $\tau \geq \tau_{\mathrm{threshold}}$, $\lambda$ solvers will be terminated. 
% Two \cmaes{} with high Kendall's $\tau$ at each iteration show similar behavior to each other, thus, Kendall's $\tau$ is often used to evaluate the quality of the surrogate function \cite{hansen2019, akimoto2019multi, miyagi2021}. However, $\tau$ cannot be calculated because the worst-case function is not available. Therefore, we need to estimate $\tau$ during optimization.


% ケンドールの相関係数$\tau$を推定するために、ソルバーにより推定された最新のワーストケース関数値と過去のワーストケース関数値を使用する。各解候補$x_i$に対して最初に推定されたワーストケース関数値を$F_i^0$と表記し、ソルバーによるワーストシナリオ探査を開始する。そして、シナリオが$c_{\max}$回改悪されればワーストシナリオ探査を中断する。この中断をroundと呼称し、$z \geq 1$によりカウントする。また、各解候補$x_i$の$z$ round後のワーストケース関数値の推定値$F_i^z$と表記する。そして、$\{F_i^z\}_{i=1}^{\lambda}$と$\{F_i^{z-1}\}_{i=1}^{\lambda}$を用いてケンドールの相関係数$\tau$を計算し、$\{F(x_i)\}_{i=1}^{\lambda}$と$\{F_i^{z-1}\}_{i=1}^{\lambda}$での$\tau$を推定する。このroundは$\tau \geq \tau_{\mathrm{threshold}}$となるまで繰り返される。

% To estimate Kendall's $\tau$, the latest worst-case function values estimated by the solver and the previous worst-case function values are used. 
% At each iteration, first worst-case function value estimated for each solution candidate $x_i$ is denoted as $F_i^0$ ($i=1,\dots,\lambda$), and $\lambda$ solvers start the worst scenario exploration. 
% In each solver, the worst scenario exploration is interrupted if the scenario is modified $c_{\max}$ times. This interruption is called "round" and is counted by $z \geq 1$. We also denote the estimated worst-case function value for each solution candidate $x_i$ after $z$ round as $F_i^z$. 
% After interrupting $\lambda$ solvers, we calculate Kendall's $\tau$ using $\{F_i^z\}_{i=1}^{\lambda}$ and $\{F_i^{z-1}\}_{i=1}^{\lambda}$ to estimate the $\tau$ between $\{F(x_i)\}_{i=1}^{\lambda}$ and $\{F_i^{z-1}\}_{i=1}^{\lambda}$. This round is repeated until $\tau \geq \tau_{\mathrm{threshold}}$.


% \subsubsection{Post-processing for next warm starting}

% % Post-processing for next warm startingでは、以後の反復で実行される\wra{}がより効率的になるように、ソルバーのパラメータ集合$\{\omega_k\}_{k=1}^{\ny}$を更新する。\Cref{alg:wra}中の\Cref{line:wra-poststart}--\Cref{line:wra-postend}がPostprocessing for next warm startingの箇所である。
% % ソルバーのパラメータ集合$\{\omega_k\}_{k=1}^{\ny}$を更新するために以下に示す2つの処理を行っている。

% In Post-processing for next warm starting, the parameter sets $\{\omega_k\}_{k=1}^{\ny}$ for the solver are updated so that \wra{} in subsequent calls will be more efficient. 
% In \Cref{alg:wra}, lines \Cref{line:wra-poststart}--\Cref{line:wra-postend} is the place for Post-processing for next warm starting.
% The parameter sets $\{\omega_k\}_{k=1}^{\ny}$ are updated by following two processes.

% % まず、$\max_{y \in \mathbb{Y}}f(x_i, y)$を解くために使用された後のソルバーのパラメータ $\tilde{\omega}_i$は$k_i^{\mathrm{worst}}$番目のパラメータ$\omega_{k_i^{\mathrm{worst}}}$として継承される。これは、\cmaes{}の以下の点を利用している。
% % 最適化手法\cmaes{}のガウス分布$\mathcal{N}(m^t, \Sigma^t)$は1回の反復では大きく変化しない。つまり、反復$t$で生成された解候補と反復$t+1$で生成された解候補はほぼ同様に分布すると考えられる。このため、反復$t$で生成された解候補のワーストシナリオの分布は、反復$t+1$での解候補に対するワーストシナリオの分布と類似していると考えられる。
% % これより、ある反復$t$の解候補$x$に対してワーストシナリオを探査したソルバーのパラメータは次の反復$t+1$でも有効であることが期待できる。
% % もしも、$k_i^{\mathrm{worst}}$が重複したときは、$k_i^{\mathrm{worst}}$の重複なしのインデックス集合を$S^{\mathrm{worst}}$とし、解候補のインデックス$l=\argmin_{i=1,\dots,\lambda} \{F^z_i \mid k_i^{\mathrm{worst}} = S^{\mathrm{worst}}\}$のパラメータ$\tilde{\omega}_l$を$k_i^{\mathrm{worst}}$番目のパラメータとして継承する。

% First, the solver parameter set $\tilde{\omega}_i$ which is used to solve $\max_{y \in \mathbb{Y}}f(x_i, y)$ is inherited as the $k_i^{\mathrm{worst}}$-th parameter set $\omega_{k_i^{\mathrm{ worst}}}$. This is from the following characteristic of \cmaes{}
% The Gaussian distribution $\mathcal{N}(m^t, \Sigma^t)$ of \cmaes{} does not change significantly in one iteration. In other words, the solution candidates generated at iteration $t$ and those generated at iteration $t+1$ are expected to be almost similarly distributed. 
% Therefore, the distribution of worst-case scenarios for the solution candidates at iteration $t$ is considered to be similar to the distribution of worst-case scenarios for the solution candidates at iteration $t+1$.
% From this, we expect that the used parameter set for the the worst scenario exploration at iteration $t$ are still valid at the next iteration $t+1$.
% If $k_i^{\mathrm{worst}}$ duplicates, the parameter the parameter set $\tilde{\omega}_l$ is inherited as $\omega_{k_i^{\mathrm{ worst}}}$, where $l$ is the index of the solution candidate $l=\argmin_{i=1,\dots,\lambda} \{F^z_i \mid k_i^{\mathrm{worst}} = S^{\mathrm{worst}}\}$ and $S^{\mathrm{worst}}$ is the index set of $k_i^{\mathrm{worst}}$ without duplication.


% % 次に、$\ny$個のパラメータの多様性を確保する。これは、解候補の小さな更新でワーストシナリオが大きく変化する問題に備えるためである。例えば、目的関数がbi-linear関数$f(x,y)=xy$となるような問題である。この問題は\Cref{sec:limit}で説明した通り、最適解周辺でワーストシナリオが大きく変化する。このような問題に備えて、warm starting strategyで選択されなかったパラメータは定期的に新しくすることとする。新しくするパラメータと保存するパラメータを区別するためのconfidence vector$\{p_k\}_{k=1}^{\ny} \in (0,1)$を導入する。パラメータが更新されたインデックスのconfidenceは$p_p$により増加し、それ以外のインデックスのconfidenceは$p_n$により減少させる。そして、$p_k \leq p_{\mathrm{threshold}}$となった$k$番目のパラメータは新しくする。

% Second, a diversity of $\ny$ parameter sets is maintained. This is to prepare for problems where the worst scenario changes significantly with small updates of the solution candidates, for example, on a bi-linear function $f(x,y)=xy$. 
% As explained in the section on \Cref{sec:limit}, the worst scenario on $f(x,y)=xy$ changes significantly around the optimal solution. To prepare for such a problem, the parameter set not selected in the warm starting strategy should be periodically renewed. 
% To distinguish the parameter set to be renewed or saved, we use a confidence vector$\{p_k\}_{k=1}^{\ny} \in (0,1)$. The confidence of the index whose parameter set are selected as $\tilde{\omega}_i$ for some $x_i$ is increased by $p_p$, and the confidence of the other indexes is decreased by $p_n$. If $p_k \leq p_{\mathrm{threshold}}$, such a $k$-th parameter set is renewed.


% \begin{algorithm}[t]
% 	\caption{Worst-case ranking approximation}\label{alg:wra}
    % \begin{algorithmic}[1]\small
    % \Require solution candidates to be ranked: $x_1, \dots, x_{\lambda_x}$ 
    % \Require stopping conditions: $\tau_{\mathrm{threshold}} \in (0, 1]$, $c_{\max} \geq 1$, $V_{\min} \geq 0$
    % \Require $p_{\mathrm{threshold}} \in (0,1)$, $p_p \in (0,1)$, $p_n \in (0,1)$, $S^{worst} = \emptyset$
    % \Require Solver parameters : $\omega_k$ for $k=1,\dots,\ny$
    
    % \State\label{line:wra-initstart} \texttt{// warm starting part}
    % \For{$i = 1, \dots, \lambda_x$}
    % \State Evaluate $f(x_{i}, y_k)$ for all $k = 1,\dots,\ny$
    % \State Select the worst index $k^\mathrm{worst}_{i} = \argmax_{k \in \{ 1, \dots,\ny\}} f(x_i, y_k)$ and let $F^0_i = f(x_i, y_{k^\mathrm{worst}_{i}})$, $\hat{y}_i=y_{k^\mathrm{worst}_{i}}$, $\tilde{\omega}_i = \omega_{k^\mathrm{worst}_{i}}$ 
    % \State $S^\mathrm{worst} = S^\mathrm{worst} \cup \{{k^\mathrm{worst}_{i}}\}$ if $k^\mathrm{worst}_{i} \notin S^\mathrm{worst}$
    % \EndFor \label{line:wra-initend}
    
    % \State\label{line:wra-worststart} \texttt{// Early stopping part}
    % \State $\tau = -1$, $z = 0$, $t_1, \dots, t_{\lambda_x} = 0$, $h_1, \dots, h_{\lambda_x} = 0$
    % \While{ $\tau \leq \tau_{\mathrm{threshold}}$}
    % \State $z = z + 1$
    % \For {$i = 1, \dots, \lambda_x$}
    % \State $c = 0$
    % \While{$c < c_{\max}$}
    % \State Obtain the worst candidate $\hat{y}'_i$ from solver using $\tilde{\omega}_i$
    % \If {$F^{z}_i < f(x^t_i,\hat{y}'_i)$}
    % \State $F^z_i = f(x^t_i,\hat{y}_i)$, $\hat{y}_i = \hat{y}'_i$, and $c = c + 1$
    % \EndIf
    % \EndWhile
    % \EndFor
%   \State $\tau = \text{Kendall}(\{F^{z-1}_{i}\}_{i=1}^{\lambda_x}, \{F^{z}_{i}\}_{i=1}^{\lambda_x})$
    % \EndWhile\label{line:wra-worstend}
    
    % \State \texttt{// Post-processing for next warm starting part}\label{line:wra-poststart}
    % \State $\tilde{\lambda} = \abs{S^\mathrm{worst}}$
    % \For{$j = 1,..,\tilde{\lambda}$}
    % \State $l = \argmin_{i=1,...,\lambda_x} \{F^z_i | k^\mathrm{worst}_i = S^\mathrm{worst}_j\}$ 
    % \State $\omega_{S^\mathrm{worst}_j} = \tilde{\omega}_l$ and $y_{S^\mathrm{worst}_j} = \hat{y}_l$
    % \State $p_{S^\mathrm{worst}_j} = p_{S^\mathrm{worst}_j} + p_p$
    % \EndFor
    % \State $p_k = p_k - \ind{k \notin S^\mathrm{worst}} p_n$ for all $k=1,...,\ny$
    % \State Refresh $k$th solver If $p_k \leq p_{\mathrm{threshold}}$
    % \label{line:wra-postend}
    % \State \Return the rankings of $F_1^z, \dots, F_{\lambda}^z$
%   \end{algorithmic}
% \end{algorithm}

\subsubsection{Hyper-parameters} \label{sec:wraparameters}

The hyperparameters for WRA are,
% the settings for outer \cmaes{} $(m^0, \Sigma^0, \theta^0, V_{\min}, \Cond_{\max})$ described in \Cref{sec:cmaes4F}, 
the threshold for Kendall's rank correlation $\tau_{\mathrm{threshold}}$, the number of configurations $\ny$, the threshold $p_{\mathrm{threshold}}$ and the parameters $p_p$ and $p_n$ for the refresh strategy. 
The initial configurations $\{\omega_k\}_{k=1}^{\ny}$ and the initial worst scenario candidates $\{y_k\}_{k=1}^{\ny}$ must be set problem-dependently. 
Here we describe the impact of these hyperparameters.

% Threshold $\tau_{\mathrm{threshold}}$ should be set to relatively high value to approximate $\tau(\{F_i^{\mathrm{rd}-1}\}_{i=1}^{\lambda_x}, \{F_i^{\mathrm{rd}}\}_{i=1}^{\lambda_x})$. 
Threshold $\tau_{\mathrm{threshold}}$ should be set to a relatively high value to approximate $\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda_x})$ with high accuracy.
However, setting a high value to $\tau_{\mathrm{threshold}}$ (e.g., $\tau_{\mathrm{threshold}} = 1$) has a risk of spending too many $f$-calls. 
Based on our parameter survey,
% in \Cref{sec:sens}
we found that the performance of the proposed approach was not very sensitive to the change of $\tau_{\mathrm{threshold}}$ on the test problems. 
Therefore, we set its default value as $\tau_{\mathrm{threshold}} = 0.7$ and we used this value throughout the experiments in this paper. 
% \textcolor{red}{Its sensitivity is investigated in \Cref{sec:sens}.}


% configurationの数は最適解でのワーストシナリオの数に依存すると考えられる。最適解のワーストシナリオ最適解のワーストシナリオが多い問題では、configurationの数を大きくすることで、ワーストシナリオに収束するためのconfigurationを保存しておくことが可能である。しかし、configurationの数を大きくしすぎると、warm startingで多くの$f$-callsを消費する。


% Considering that the solvers for each $\{x_i\}_{i=1}^{\lambda_x}$ have effective configurations, $\ny$ should be larger than $\lambda_x$. However, when a higher value is set to $\ny$, more $f$-calls will be required in the warm starting strategy. On the other hand, if $\ny$ is small, the warm starting strategy will be less effective. The number of configurations $\ny$ should be set for each problem.

The number of configurations, $\ny$, is desired to be set no smaller than the number $\abs{\ywset(x^*)}$ of worst scenarios around $x^*$ to keep good configurations and good initial solutions for each scenario in $\ywset(x^*)$. 
At the same time, because $\ny$ $f$-calls are required to select the initial configuration for each $x$, $\ny$ is desired to be as small as possible. 
However, $\abs{\ywset(x^*)}$ is unknown in advance and is problem-dependent.
We suggest to set $\ny$ to be a few time greater than $\lambda_x$ to give $\lambda_x$ solution candidates a chance to use $\lambda_x$ distinct worst scenario candidates. Its effect is discussed in \Cref{sec:test}.

Parameters $p_{\mathrm{threshold}}$, $p_p$, and $p_n$ affect the frequency of each configuration to be refreshed.
If the configurations are frequently refreshed, the warm starting strategy may be less effective. 
Based on the preliminary investigation, we set $p_{\mathrm{threshold}}=0.1$, $p_p=0.4$ and $p_n=0.05$ as the default values. 
In this case, the configurations $\{\tilde{\omega_i}\}_{i=1}^{\lambda_x}$ are kept for at least $6 = (p_p - p_{\mathrm{threshold}}) / p_n$ outer-loop iterations after the last use, or $18 = (1 - p_{\mathrm{threshold}}) / p_n$ iterations after the initialization and the last refresh.
% \textcolor{red}{The effect of the refresh strategy is discussed in \Cref{sec:test}.}



\subsection{Implementation of WRA}

% \wra{}中のソルバーとして\cmaes{}と\slsqp{}を使用したアルゴリズム\wracma{}と\wraslsqp{}を\Cref{alg:wra-imp}にまとめた。\Cref{alg:wra-imp}では$\lambda$個の解候補を入力とし、そのランキングを出力する。\Cref{alg:wra-imp}中で、\wra{}のフレームワークを黒文字で、\cmaes{}の箇所を赤文字で、\slsqp{}の箇所を青文字で各々示している。

% \Cref{alg:wra-imp}中、\Cref{line:imp-initstart}--\Cref{line:imp-initend}はwarm starting strategyで、\Cref{line:imp-worststart}--\Cref{line:imp-worstend}はEarly stopping strategyで、\Cref{line:imp-poststart}--\Cref{line:imp-postend}はPost-processing for next warm startingである。

We implement two variants of WRA with CMA-ES (\Cref{sec:wracma}) and Approximate Gradient Ascent (AGA, \Cref{sec:wraslsqp}) as a solver $\mathcal{M}$.
% % \footnote{\color{red}\url{insert the url of wracma }}
% and 
% \wraslsqp{}, 
% % \footnote{\color{red}\url{insert the url of slsqp }},
% where CMA-ES and Approximate Gradient Ascent (AGA), respectively, are implemented as a solver $\mathcal{M}$. 
% % \cmaes{} and \slsqp{} for \wra{} are presented in \Cref{alg:CMAESinwra} and \Cref{alg:SLSQPinwra}, and each solver is applied to line $12$ in \Cref{alg:wra}.

%　ケンドールのτを計算するタイミングを決めるためにc_{\max}を導入した。シナリオがc_{\max}回更新されれれば、シナリオ探査を中断し、ケンドールのτを計算することとした。シナリオがc_{\max}回更新される前に解候補$x_i$に対するソルバー$\mathcal{M}(\omega_i)$がシナリオ探査を終了した場合、その反復$t$においてソルバー$\mathcal{M}(\omega_i)$はシナリオを更新しないとする。


% In \Cref{alg:CMAESinwra} and \Cref{alg:SLSQPinwra}, we use $c_{\max}$ to decide the timing to compute the ranking correlation $\tau(\{F_i^{\mathrm{rd}-1}\}_{i=1}^{\lambda_x}, \{F_i^{\mathrm{rd}}\}_{i=1}^{\lambda_x})$. 
% When the scenario is improved $c_{\max}$ times at each solver, the solvers are terminated and the ranking correlation $\tau(\{F_i^{\mathrm{rd}-1}\}_{i=1}^{\lambda_x}, \{F_i^{\mathrm{rd}}\}_{i=1}^{\lambda_x})$ is computed.
% When the solver $\mathcal{M}(\omega_i)$ for solution candidate $x_i$ terminates the scenario improvement before the scenario is updated $c_{\max}$ times, the scenario $\hat{y}_i$ is not modified at that iteration $t$.


% The proposed algorithms, \wracma{} and \wraslsqp{}, take $\lambda$ solution candidates as input and outputs their ranking. In \Cref{alg:wra-imp}, the framework of \wra{} is shown in black letters, the part of \cmaes{} is shown in red letters, and the part of \slsqp{} is shown in blue letters respectively.

% In \Cref{alg:wra-imp}, lines~\ref{line:imp-initstart}--\ref{line:imp-initend} is the warm starting strategy, lines~\ref{line:imp-worststart}--\ref{line:imp-worstend} is the Early stopping strategy and lines~\ref{line:imp-poststart}--\ref{line:imp-postend} is Post-processing for the next warm starting.

\subsubsection{WRA using CMA-ES} \label{sec:wracma}

% ここでは、\Cref{alg:wra-imp}中で、ソルバーとして実装された\cmaes{}に関連する箇所について説明する。
% Line 4では、$k_i^{\mathrm{worst}}$番目のパラメータが選択されている。\cmaes{}には、ガウス分布に関わるパラメータ以外に、進化パス等の動的パラメータ$\theta$がある。
% Line 5では、これら動的パラメータ$\theta$は\wra{}が使用される毎に初期化する。\footnote{これは、他の解候補に対してワーストシナリオを探査した\cmaes{}の動的パラメータ$\theta$を継承して別の解候補のワーストシナリオを探査すると、sytematic biasが発生することが報告されている\cite{akimoto2022berthing}。これを防ぐために動的パラメータ$\theta$は\wra{}が使用される毎に初期化する。
% }
% Line 16では、\cmaes{}により1反復分だけワーストシナリオを探査する。
% Line 21では、$c_{\max}$の他に\cmaes{}を終了させる処理として、各軸の標準偏差$\sqrt{\Sigma_{l,l}}$を用いている。これは、標準偏差が十分小さくなるとシナリオの大幅な改悪が期待できないと考えられるためである。しかし、次の反復で\wra{}が使用されるときに、この終了条件をすぐに満足し、十分なシナリオ探査が行われる前に\cmaes{}が終了することが考えられる。これを防ぐために、\cmaes{}は少なくとも$T_{\min}$反復はシナリオ探査を行うとした。
% Line 31--33では、\cmaes{}のパラメータが継承されている。このとき、$\sqrt{\Sigma_{l,l}}$が$V_{\min}$以下にならないように処理している。これは、次に\wra{}が使用されるときに、$T_{\min}$反復後に\cmaes{}が即座に収束することを防ぐためである。
% Line 38では、$p_k \leq p_{\mathrm{threshold}}$となった$k$番目の\cmaes{}のパラメータを新しくしている。

% We explain \Cref{alg:wra} where \cmaes{} is implemented as a solver.
The first variant uses dd-CMA-ES \cite{akimoto2019} as a solver $\mathcal{M}$, summarized in \Cref{alg:CMAESinwra}. 
If the search domain has a box constraint, we employ the mirroring technique along with upper-bounding the coordinate-wise standard deviation \cite{yamaguchi2018}.
%\textcolor{red}{
The configuration $\tilde{\omega}$ includes the mean vector $\tilde{m}$ and the covariance matrix $\tilde{\Sigma}$, and $\tilde{\theta}$ includes other parameters such as evolution paths, iteration  counter $t' \geq 0$ (initialized as $t' = 0$), and termination flag $h$ (initialized as $h = \textsc{False}$).
%}
%other dynamic parameters such as evolutionary paths, static parameters such as population size $\lambda_y$, and the termination flag $h_\mathrm{end}$.
%Each solver call is summarized in \Cref{alg:CMAESinwra}.
% The parameters $\theta$ are initialized at the proposed value in \cite{akimoto2019} in each time \wra{} is called.
% \footnote{It has been reported that sytematic bias is caused when the parameters $\theta$ used in the worst scenario exploration for one solution candidate are used in that for another solution candidate \cite{akimoto2022berthing}. To prevent this, the parameters $\theta$ is initialized at every \wra{} calls.
% }

% Therefore, in Line 5, the initial distribution of the internal \cmaes{} for $x_i$ is set to $\mathcal{N} (m_{k_i^{\mathrm{worst}}}, \Sigma_{k_i^{\mathrm{worst}}})$ and $\theta$ is initialized.  
% In Line 12, the worst scenario is approximately explored by \Cref{alg:CMAESinwra}, which is explained later. 
% In Line 25, if $p_k \leq p_{\mathrm{threshold}}$, the parameters are refreshed as the following : the mean vector is sampled from the search domain $m_k \sim \mathcal{U}(\mathbb{X})$, the covariance matrix $\Sigma_k$ and the confidence $p_k$ is reset to initial settings, and the scenario $y_k$ is sampled from the refreshed distribution $\mathcal{N}(m_k, \Sigma_k)$. 


% Next, we explain where the implemented \cmaes{} as a solver in \wra{} differs from the regular \cmaes{} as explained in \Cref{sec:cmaes4F}.



% \Cref{alg:CMAESinwra} works to find the scenario that approximates worst-case function value at each solution candidate.
% In \Cref{alg:CMAESinwra}, the solution candidate $x$, the approximated worst-case function value for the solution candidate $F_y$, the candidate of the worst scenario $\hat{y}$, and the parameters $(\tilde{m}, \tilde{\Sigma}, \theta)$ for internal \cmaes{} are imported from \wra{}. 
% \Cref{alg:CMAESinwra} repeats the following steps until that a termination condition is satisfied.
% After sampling scenarios $\{\hat{y}'_k\}_{k=1}^{\lambda_y}$ from the distribution $\mathcal{N}(\tilde{m}, \tilde{\Sigma})$, the objective function values for every scenarios $\{f_k\}_{k=1}^{\lambda_y}$ are evaluated. 
% When the scenario search domain has a box constraint, The mirroring technique along with upper-bounding the coordinate-wise standard deviation \cite{yamaguchi2018} is used.
% The worst index $\tilde{k^{\mathrm{worst}}} = \argmax_{k=1,\dots,\lambda_y} f_k$ is selected. 
% When $F_y < f(x, \hat{y}_{\tilde{k}^\mathrm{worst}}^{'})$, we regards that the scenario is improved, and $(\hat{y}, F_y)$ is updated to $(\hat{y}_{\tilde{k}^\mathrm{worst}}^{'}, f(x, \hat{y}_{\tilde{k}^\mathrm{worst}}^{'}))$ and the number of the scenario improvement $c$ is counted.
% The parameters $(\tilde{m}, \tilde{\Sigma}, \theta)$ are updated as the same as the regular version of \cmaes{}.  

Because the proposed approach is a double-loop approach, setting the termination conditions for internal loop is important. 
\Cref{alg:CMAESinwra} runs CMA-ES until the worst scenario candidate is improved $c_{\max}$ times. 
If the worst scenario candidate has been improved for $c_{\max}$ times, we regard that it has been significantly improved.
Similarly to CMA-ES for outer minimization, we terminate the maximization process if all the coordinate-wise standard deviation, $\sqrt{[\tilde{\Sigma}]_{\ell,\ell}}$, becomes smaller than $V_{\min}^{y}$.
In this situation, we expect that the distribution is sufficiently concentrated and no more significant improvement will be obtained.
We stop CMA-ES if the condition number, $\Cond(\tilde{\Sigma})$, becomes greater than $\Cond_{\max}^{y}$.
%, and such a covariant matrix $\tilde{\Sigma}$ is set to the initial setting at the round $\tilde{\Sigma}_{init}$.
If one of the latter two conditions are satisfied, we set $h = \textsc{True}$ and CMA-ES will not be executed in the following rounds in the current WRA calls.

The distribution parameters are inherited over WRA calls. 
Once the condition $\max_{\ell} \sqrt{[\tilde{\Sigma}]_{\ell,\ell}} < V_{\min}^{y}$ is satisfied for some configurations, it is expected to be immediately satisfied in the next WRA call if these configurations are selected. 
However, because the objective function with respect to $y$, i.e., $f(x_i, y)$, are different as solution candidates, $x_i$, are different from previous WRA calls, there is a chance that the distribution will be enlarged due to the step-size adaptation mechanism of CMA-ES and a significant improvement will be produced. 
For this reason, we force the standard deviation to be no smaller than $V_{\min}^y$ (\Cref{l:sig_correct_1}--\Cref{l:sig_correct_2}) and CMA-ES to run at least $T_{\min}$ iterations for each WRA call.

% Termination criteria of \Cref{alg:CMAESinwra} is in the followings.
% When the scenario is improved $c_{\max}$ times, the solver is terminated to compute the ranking correlation $\tau(\{F_i^{\mathrm{rd}-1}\}_{i=1}^{\lambda_x}, \{F_i^{\mathrm{rd}}\}_{i=1}^{\lambda_x})$.
% We terminate \Cref{alg:CMAESinwra} when all the coordinate-wise standard deviation, $\sqrt{\tilde{\Sigma}_{\ell,\ell}}$, becomes smaller than $V_{\min}^{y}$.
% In this case, we assume the search distribution has converged and do not expect a significant modification of the scenario.
% However, it is conceivable in next \wra{} call, this termination condition will be satisfied immediately and \Cref{alg:CMAESinwra} will terminate without sufficient scenario modification. 
% To prevent this, we force \Cref{alg:CMAESinwra} to explore scenario at least $T_{\min}$ iterations. The number of iteration $t'$ in \Cref{alg:CMAESinwra} for $x_i$ is included in $\theta_i$ and inherited to next call.
% Furthermore, $\tilde{\Sigma}$ is updated so that all the coordinate-wise standard deviation $\sqrt{\tilde{\Sigma}_{\ell, \ell}}$ is more than $V_{\min}^{y}$ to prevent immediate termination after $T_{\min}$ iterations at next \wra{} call.
% When the condition number of the covariance matrix $\Cond(\tilde{\Sigma})$ becomes greater than $\Cond_{\max}^{y}=10^{14}$, we terminate \Cref{alg:CMAESinwra} and reset such covariant matrix $\tilde{\Sigma}$.
% When the internal \cmaes{} is terminated by satisfying $\max_{\ell=1,\dots,n}\sqrt{\tilde{\Sigma}_{\ell,\ell}} < V_{\min}^{y}$ or $\Cond(\tilde{\Sigma}) > \Cond_{\max}^{y}$, such \cmaes{} is not resumed at that iteration $t$.


%c_{\max}を大きく設定すれば、より多くの$f$-callsを要すると予想される。一方で、$f$-callsを小さくするためにc_{\max}を小さくすると、ワーストケース関数でのランキング$\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda_x})$を高い精度で推定する前にシナリオ探査が終了することが懸念される。
% 本研究では、$f$-callsを小さくするために、単純に、c_{\max}=1とした。

Its hyperparameters includes the initial configurations for internal CMA-ES $(\{m_k\}_{k=1}^{\ny}, \{\Sigma_k\}_{k=1}^{\ny}, \theta)$, the initial scenarios $\{y_k\}_{k=1}^{\ny}$, and the termination conditions for \Cref{alg:CMAESinwra}, $c_{\max}$, $V_{\min}^{y}$, and $T_{\min}$. 
% We describe the parameters for internal \cmaes{} : $(\{m_k\}_{k=1}^{\ny}, \{\Sigma_k\}_{k=1}^{\ny}, \theta, \{y_k\}_{k=1}^{\ny}, \tilde{V}_{\min}, \tilde{\Cond}_{\max})$.
The configuration and the initialization of $\theta$, including the initialization of evolution paths and the population size $\lambda_y$, follow the values proposed in \cite{akimoto2019}. 
The parameter $c_{\max}$ impacts to the approximation accuracy of the rankings on the worst-case function values $\mathrm{Rank}_{F}(\{x_i\}_{i=1}^{\lambda_x})$ and $f$-calls to approximate the rankings.  
If $c_{\max}$ is set to a greater value, WRA will require more $f$-calls. On the other hand, setting $c_{\max}$ to a smaller value has a risk to terminate the scenario improvement before the ranking on the worst-case function $\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda_x})$ is estimated with sufficient accuracy. 
% We see the effect of $c_{\max}$ in \Cref{sec:test} and \Cref{sec:sens}.
The parameter $T_{\max}$ can be set to a constant value as CMA-ES can increase the standard deviation rapidly if it is desired. 
We set $T_{\max} = 10$ as the default value.
% We assume Kendall's $\tau$ at small round does not becomes more than $\tau_{\mathrm{threshold}}$, therefore, small $c_{\max}$ may be acceptable. 
% Although $c_{\max}$ is problem-dependently, we simply set $c_{\max}=1$ in order to save $f$-calls in this study.
% The parameter $V_{\min}^{y}$ impacts to the approximation accuracy of the worst-case function values $F(x)$ for each  solution candidate $x$ and $f$-calls until the termination. 
% When the parameter $V_{\min}^{y}$ is set to a smaller value, the worst-case function value $F(x)$ for each solution candidate will be approximated with higher accuracy and will require more $f$-calls until the termination. On the other hand, \wracma{} with a higher $V_{\min}^{y}$ will terminate faster and will suffer from approximating the worst-case function value with higher accuracy. 
% The parameter $T_{\min}$ impacts $f$-calls when \Cref{alg:CMAESinwra} starts with the small search distribution. Although such \Cref{alg:CMAESinwra} with a smaller $T_{\min}$ will terminate faster, \Cref{alg:CMAESinwra} has less chance to improve the scenario. On the other hand, when the search distribution becomes small to improve the scenario, \Cref{alg:CMAESinwra} with a higher $T_{\min}$ will require many $f$-calls. 
The parameter $V_{\min}^{y}$ and the initial distributions $\{(m_k, \Sigma_k)\}_{k=1}^{\ny}$ must be set problem-dependently. 
The initial scenarios $\{y_k\}_{k=1}^{\ny}$ are drawn from the initial distributions, i.e., $y_k \sim \mathcal{N}(m_k, \Sigma_k)$.


\begin{algorithm}[t]
	\caption{CMA-ES in Worst-case ranking approximation}\label{alg:CMAESinwra}
    \begin{algorithmic}[1]\small
    \Require $x, \hat{y}, F_y, \tilde{m}, \tilde{\Sigma}, \tilde{\theta}$ 
    \Require $V_{\min}>0$, $c_{\max} \geq 1$, $\lambda_y = \lfloor 4 + 3\log(n) \rfloor$
    
    \State $\tilde{\Sigma}_{init}=\tilde{\Sigma}$, $c=0$
    \While{$c < c_{\max}$ \textbf{and} $h = \textsc{False}$}%or $h = 0$}
    
    \State Sample $ \{\hat{y}'\}_{k=1}^{\lambda_y} \sim \mathcal{N}(\tilde{m}, \tilde{\Sigma})$
    \State Evaluate $f_k = f(x, \hat{y}_{k}^{'})$ for all $k=1,\dots,\lambda_y$
    
    \State Select the worst index $\tilde{k}^\mathrm{worst}=\argmax_{k=1,\dots,\lambda_y} f_k$
    \If {$f(x, \hat{y}_{\tilde{k}^\mathrm{worst}}^{'}) > F_{y}$}
    \State $F_{y} = \max_{k=1,\dots,\lambda_y} f_k$, $\hat{y} = \hat{y}'_{\tilde{k}^\mathrm{worst}}$, and $c = c + 1$
    \EndIf
    
    \State Perform CMA-ES update using $\{\hat{y}_{k}^{'}, f_k\}_{k=1}^{\lambda_y}$
    
    % \State $h = 1$ \textbf{if} $\max_{\ell}\left\{\sqrt{[\tilde{\Sigma}]_{\ell,\ell}}\right\} < V_{\min}$ and $t' \geq T_{\min}$
    % \State $h = 1$ \textbf{if} $Cond(\Sigma) < \tilde{Cond}_{\max}$
    \If {$\max_{\ell}\left\{\sqrt{[\tilde{\Sigma}]_{\ell,\ell}}\right\} < V_{\min}^{y}$ \text{and} $t' \geq T_{\min}$} 
    \State\label{l:sig_correct_1} $D = \diag\left(\max\left(1, \frac{V_{\min}^y}{\sqrt{[\tilde{\Sigma}]_{1,1}}}\right), \dots, \max\left(1, \frac{V_{\min}^y}{\sqrt{[\tilde{\Sigma}]_{n,n}}}\right)\right)$
    \State\label{l:sig_correct_2} $\tilde{\Sigma} = D \tilde{\Sigma} D$
    \State $h = \textsc{True}$
    \EndIf
    \State $h = \textsc{True}$ and set $\tilde{\Sigma}=\tilde{\Sigma}_{init}$ \textbf{ if } $\Cond(\tilde{\Sigma}) > \Cond_{\max}^{y}$
    \State $t'=t'+1$
    \EndWhile
    \State \Return $\hat{y}$, $F_y$, $\tilde{m}$, $\tilde{\Sigma}$, $\tilde{\theta}$
  \end{algorithmic}
\end{algorithm}


\subsubsection{WRA using AGA} \label{sec:wraslsqp}


% \Cref{alg:wra-imp}中で、ソルバーとして実装された\slsqp{}に関わる箇所について説明する。
% Line 6では、$k_i^{\mathrm{worst}}$番目のパラメータが選択されている。
% Line 17では、\slsqp{}を使用して、ワーストシナリオの候補$\hat{y}'_i$を求めている。\slsqp{}については後述する。
% Line 34では、使用されたパラメータが継承されている。
% Line 39では、$p_k \leq p_{\mathrm{threshold}}$となった$k$番目のパラメータを新しくしている。

% We explain \Cref{alg:wra-imp} where \slsqp{} is implemented as a solver.
% In Line 6, $k_i^{\mathrm{worst}}$-th parameter set is selected as initial scenario and learning rate for \slsqp{}.
% Line 17 uses \slsqp{} to find the worst scenario candidate $\hat{y}'_i$. The implemented \slsqp{} is explained in next paragraph.
% In Line 22, \slsqp{} is terminated when \slsqp{} fail to modify the scenario. Optimization failure is also explained in next paragraph.
% In this case, we assume the search distribution has converged and do not expect a significant modification of the scenario. 
% In Line 34, the parameter set used for the worst scenario exploration is inherited.
% Line 39 renews $k$-th parameter set such that $p_k \leq p_{\mathrm{threshold}}$.


% \wraslsqp{}中に実装した\slsqp{}を\Cref{alg:SLSQPinwra}に示す。\Cref{alg:SLSQPinwra}は、ある解候補$x$に対して、シナリオ$\hat{y}'$よりも目的関数値が大きくなるシナリオを探査する。
% まず、シナリオ$\hat{y}'$での数値勾配$\bar{\nabla}_y f$をpythonのscipyモジュールのSLSQP関数より求める。ここで得られた数値勾配と学習率$\tilde{\eta}$を用いてシナリオを更新する。更新されたシナリオ$\hat{y}''$を用いることで目的関数値を増加できればパラメータ$\beta \in (0,1)$により学習率を大きくする。
% 反対に、更新されたシナリオ$\hat{y}''$を使用しても目的関数値を増加できない場合、目的関数値を増加できるまで、$\beta \in (0,1)$により学習率を小さくする。ただし、$\max(\abs{\tilde{\eta} \bar{\nabla_y} f}) \leq U_{\min}$となった場合、つまり、どの次元についても$y$の改悪幅が$U_{\min}$以下となった場合、\slsqp{}は終了することとした。これは、シナリオの大幅な改悪が期待できないと考えられるためである。これに伴い、\wra{}中において、そのような解におけるワーストシナリオ探査は終了することとした。


% The implemented \slsqp{} in \wraslsqp{} is shown in \Cref{alg:SLSQPinwra}. \Cref{alg:SLSQPinwra} increases the objective function value $F_y$ for a given solution candidate $x$ by modifying a given scenario $\hat{y}'$.
% First, the numerical gradient $\bar{\nabla}_y f$ at $\hat{y}'$ is obtained from the SLSQP function in scipy module in python. The scenario is updated using the numerical gradient obtained and the learning rate $\tilde{\eta}$. 
% If the objective function value can be increased by using the updated scenario $\hat{y}''$, the learning rate is increased by the parameter $\beta \in (0,1)$.
% Conversely, if the objective function value cannot be increased by using the updated scenario $\hat{y}''$, the learning rate is reduced by $\beta \in (0,1)$ until the objective function value is more than $F_y$. 
% However, if $\max(\abs{\tilde{\eta} \bar{\nabla_y} f}) \leq U_{\min}$, that is, if the update range for $y$ is less than $U_{\min}$ for any dimension, \slsqp{} is terminated. This is because no significant deterioration of the scenario is expected. Accordingly, the worst scenario exploration for such a solution is terminated at line 22 in \Cref{alg:wra-imp}.


% We explain \Cref{alg:wra} where \slsqp{} is implemented as a solver.
The second variant uses AGA as a solver $\mathcal{M}$, summarized in \Cref{alg:SLSQPinwra}. The solver uses the numerical gradient $\bar{\nabla}_y$ at the worst scenario candidate $\hat{y}$ obtained by \slsqp{} function in scipy module in python.
% \slsqp{} is used as a solver $\mathcal{M}$. 
If the search domain for the scenario vector has a box constraint, the idea of projected gradient is used to force the worst scenario candidate to be feasible.
The configuration $\omega$ for \Cref{alg:SLSQPinwra} includes the learning rate $\{\eta_k\}_{k=1}^{\ny}$, and the other parameters
% such as , iteration count $t'$, and termination flag $h$ 
are included in $\theta$. 
% In Line 5 at \Cref{alg:wra}, the learning rate of \slsqp{} for $x_i$ is set to $\eta_{k_i^{\mathrm{worst}}}$.  
% In Line 12, the worst scenario is approximately explored by \Cref{alg:SLSQPinwra}, which is explained later. 
% In Line 25, if $p_k \leq p_{\mathrm{threshold}}$, the learning rate $\eta_k$ is reset to the initial value, and the scenario is sampled from the scenario search domain $y_k \sim \mathcal{U}(\mathbb{Y})$. 

% Similarly to \Cref{alg:CMAESinwra}, \Cref{alg:SLSQPinwra} runs until the worst scenario candidate is updated $c_{\max}$ times. 
% The numerical gradient $\bar{\nabla}_y f$ is obtained using the SLSQP function in scipy module in python.
We use a simple adaptation mechanism for the learning rate $\tilde{\eta}$ in \Cref{alg:SLSQPinwra}, similarly to the backtracking line search.
The learning rate $\tilde{\eta}$ is decreased by $\beta \in (0,1)$ until the worst scenario candidate is improved.
If the worst scenario candidate is improved for the first trial, the learning rate is increased by $1/\beta$.
This is because the significant improvement of the worst scenario candidate is expected by large learning rate at next iteration. 



Termination criteria of \Cref{alg:SLSQPinwra} are described as follows.
\Cref{alg:SLSQPinwra} is terminated when the scenario is improved for $c_{\max}$ times.
If 
%$\max(\abs{\tilde{\eta} \bar{\nabla}_y f}) \leq U_{\min}$, 
the infinity norm of the update vector is smaller than $U_{\min}$, i.e.,
$\norm{\tilde{\eta} \bar{\nabla}_y f}_{\infty} \leq U_{\min}$, 
we consider that a significant increase of the objective function value is not expected and terminate the solver.
% This is because no significant deterioration of the scenario is expected.
% This is because the significant increase of the objective function value is not expected by the worst scenario candidate generated at such the situation. 
When \Cref{alg:SLSQPinwra} is terminated by the latter condition, we set $h=\text{TRUE}$ and $\mathcal{M}$ is not called with the current configuration in the current WRA call.


Its hyperparameters include the initial learning rate $\{\eta_k\}_{k=1}^{\ny}$, the parameter for updating the learning rate $\beta$, and the termination threshold $U_{\min}$ and the maximum number of improvement, $c_{\max}$. 
% We describe the parameters for \slsqp{} : $(\{\eta_k\}_{k=1}^{\ny}, \{y_k\}_{k=1}^{\ny}, U_{\min}, c_{\max})$.
% The parameter $c_{\max}$ is set at $1$ as in \wracma{}.
% The parameter $U_{\min}$ impact to $f$-calls until the termination and the accuracy of the approximated worst-case function value. 
% \wraslsqp{} with a smaller $U_{\min}$ is expected to approximate the worst-case function value $F(x)$ for each solution candidate with higher accuracy. However, too much small $U_{\min}$ will require many $f$-calls for the termination. 
% The parameter for updating the learning rate $\beta$ should be set to a high value for the problem where the effective learning rate drastically changes at each scenario. 
% The initial learning rate $\{\eta_k\}_{k=1}^{\ny}$, $\beta$, $U_{\min}$ should be set problem-dependently.
% , however, we set $\eta_k = 1$ for every $k=1,\dots,\ny$, $\beta=0.5$, $U_{\min}=10^{-5}$ as default values.
They should be set problem-dependently.
% The sensitivities of \wraslsqp{} on the choice of $c_{\max}$ and $\tau_{\mathrm{threshold}}$ were investigated in \Cref{sec:sens}. 

\begin{algorithm}[t]
	\caption{AGA in Worst-case ranking approximation}\label{alg:SLSQPinwra}
    \begin{algorithmic}[1]\small
    \Require $x, \hat{y}, F_y, \tilde{\eta}$
    \Require $U_{\min}>0$, $c_{\max} \geq 1$, $\beta \in (0,1)$
    % \Require Input : $ (x, \hat{y}, F_y, \tilde{\eta}) $
    \State $c=0$
    \While{$c < c_{\max}$ \textbf{and} $h = \textsc{False}$}%or $h_i = 0$}

    % \State Obtain the approximated gradient $\bar{\nabla_y} f$ at $\hat{y}'$ by using SLSQP in scipy module of python    
    \State Obtain approximated gradient $\bar{\nabla}_y f$ at $\hat{y}$    
    \State $\hat{y}' = \hat{y} + \tilde{\eta} \bar{\nabla}_y f $
    \If {$f(x,\hat{y}')>F_{y}$}
    \State $\tilde{\eta} = \tilde{\eta} / \beta$
    \Else
    \While{$f(x,\hat{y}') \leq F_y $}
    \State $\tilde{\eta} = \tilde{\eta} \times \beta$
    \State $\hat{y}' = \hat{y} + \tilde{\eta} \bar{\nabla}_y f$
    \State $h = \textsc{True}$ \textbf{ if } $\norm{\tilde{\eta} \bar{\nabla}_y f}_{\infty} \leq U_{\min}$
    \EndWhile
    \EndIf

    \If {$f(x, \hat{y}') > F_y $}
    \State $F_y = f(x, \hat{y}')$, $\hat{y} = \hat{y}'$, and $c = c + 1$
    \EndIf
    % \State \textbf{Terminate if} $F_y \geq f(x^t_i, \hat{y}'')$

    \EndWhile
    
    \State \Return $\hat{y}$, $F_y$, $\tilde{\eta}$
  \end{algorithmic}
\end{algorithm}

%\subsection{Restart Strategy}
\subsection{Restart and Local Search Strategy} \label{sec:restart}

% 実際の問題でよりよい解を見つけるための実用的な2つの方法を以下に記す。

We implement two devices for practical use to enhance exploration (by restart) and exploitation (by local search).

%提案手法を実際の問題で使用する際は、リスタート戦略を用いてより良い解を探査する。実際の問題では、ワーストケース関数がウィークな多峰性関数などのCMA-ESがそもそも収束できない関数であることが考えられる。このような問題では、リスタート戦略を用いることでよりよい解が得られると期待できる。提案アプローチでは、$x$を探査する\cmaes{}が収束したと判断されれば、リスタートすることとする。このとき、ハイパーパラメータは初期設定のときと同様の方法でリセットする。


% One is to use the restart strategy \cite{Auger2018cec} that will be effective to explore better solutions. In the real--world applications, it is possible that the worst-case function is a function that CMA-ES cannot converge to the global solution in one trial, such as the multimodal function with week structure. For such problems, some restarts with different initial settings will help to find a better solution. In the proposed approach, we will restart when \cmaes{} for the outer loop has converged. At this time, the hyperparameters are reset in the same way as in the initial settings.
A restart strategy is implemented to obtain better local optimal solutions when $F$ is multimodal. 
When a termination condition is satisfied before a $f$-call budget or a wall clock time budget is exhausted, the $\lambda_x$ solution candidates and the $\ny$ worst scenario candidates at the last iteration are stored in $\mathcal{X}^*$ and $\mathcal{Y}^*$, respectively. 
We restart the search without inheriting any information from previous restarts.
Once the budgets are exhausted, the last solution candidates and worst scenario candidates are stored as well.
The final output of the algorithm, i.e., the candidate of the global min--max solution, is $\argmin_{x \in \mathcal{X}^*} \max_{y \in \mathcal{Y}^*} f(x,y)$.
One can also include randomly sampled scenario vectors to $\mathcal{Y}^*$ when deciding the final output for a better estimate of $F(x)$.
The resulting algorithms using CMA-ES and AGA with this restart strategy are denoted as \wracma{} and \wraslsqp{}, respectively, in the following sections.


% The restart strategy \cite{Auger2018cec} as in the existing approach \acma{} is expected to help the proposed approaches to find a better solution. In the proposed approaches, once \cmaes{} was evaluated as convergence exploring for $x$ at iteration $t$, \cmaes{} is restarted with the new distribution $m^t \sim \mathbb{X}$ and $\Sigma^t = \diag(0.5,.... ,0.5)^2$, and the initialized dynamic parameters $\theta_x$ at the recommended parameters \cite{akimoto2019}. 
% When \cmaes{} for exploring $x$ is restarted, in solver \cmaes{} in \wracma{}, the mean vector is $\{ m_k \}_{k=1}^{\ny} \sim \mathcal{U}(-1, 1)^n$ and the covariance matrix is $\{ \Sigma_k \}_{k=1}^{\ny} = \diag(0.5,...,0.5) ,0.5)^2$, the other parameters $\theta$ are initialized. 
% In case of solver \slsqp{}, the scenario set is renewed $\{y_k\}_{k=1}^{\ny} \sim \mathcal{U}(-1, 1)^n$ and the learning rate set is initialized to $\{\eta_{k}\}_{k=1}^{\ny}=1$. Furthermore, the confidence vector is also initialized $\{p_k\}_{k=1}^{\ny} = 1$.



% 実際の問題でよりより解を見つけるもう一つの方法として、提案手法が収束したあとに、\acma{}を使用して高い精度の局所解が求められる方法が期待できる。
% 提案手法はワーストランキングの推定に多くの$f$-callsが消費されると考えれるため、局所探査では非効率になると考えれる。効率的に局所解の精度を向上させるために、$f$-callsの消費が小さい\acma{}を使用する。
% \acma{}には、悪いシナリオを保存しておく集合$\mathcal{Y}$が用意されており、$\mathcal{Y}$の中に局所解のワーストシナリオを入力できれば、少なくとも$f_{\mathcal{Y}}(x) := \max_{y \in \mathcal{Y}} f(x,y)$における局所解に収束できる。
% \wra{}の収束時のシナリオ$\{y_k^{\mathrm{converge}}\}_{k=1}^{\ny}$は凡そ局所解周辺のワーストシナリオ集合であると考えられ、$\mathcal{Y}=\{y_k^{\mathrm{converge}}\}_{k=1}^{\ny}$として、\acma{}を使用することで少ない$f$-callsでよりよい解が得られると期待している。
%\acma{}の初期値については、\wra{}収束時の反復$t$におけるガウス分布の情報を使用する。
% 設計変数$x$を探査する(1+1)-CMA-ESの初期ガウス分布$\mathcal{N}(x_{\mathrm{Adv}}, \Sigma_{\mathrm{Adv}}^x)$は、$x_{\mathrm{Adv}}=(x_l \mid l=\argmin_{i=1,\dots,\lambda}F^z_i)$、$\Sigma_{\mathrm{Adv}}^x=\Sigma^t$とする。シナリオ$y$を探査する(1+1)-CMA-ESの初期ガウス分布$\mathcal{N}(y_{\mathrm{Adv}}, \Sigma_{\mathrm{Adv}}^y)$は、インデックス$k_{\mathrm{Adv}} =  \argmax_{k=1,\dots,\ny}f(x_{\mathrm{Adv}}, y_k)$を使用して以下の通りに設定する。
% シナリオを探査する(1+1)-CMA-ESの初期値については\wra{}内で使用するソルバー毎に異なる。
% \wracma{}を使用する場合、$\mathcal{N}(y_{\mathrm{Adv}}, \Sigma_{\mathrm{Adv}}^y) = \mathcal{N}(m_{k_{\mathrm{Adv}}}, \Sigma_{k_{\mathrm{Adv}}})$とする。また、\wraslsqp{}を使用する場合、$\mathcal{N}(y_{\mathrm{Adv}}, \Sigma_{\mathrm{Adv}}^y) = \mathcal{N}(y_{k_{\mathrm{Adv}}}, 0.1 \times \Sigma_{\mathrm{Adv}}^x)$とする。\wra{}より継承されるパラメータ以外のパラメータは全て既存研究での推奨パラメータ\cite{akimoto2022berthing}とした。上記の手法を\wracmaadv{}及び\wraslsqpadv{}と表記する。

We implement an optional local search strategy using \acma{}.
If the problem is locally smooth and strongly convex--concave, \acma{} exhibits significantly faster convergence than \wra{}. 
Therefore, by stopping each run of \wra{} early and performing \acma{}, we expect that the solution candidate obtained by \wra{} will be locally more improved by \acma{} than by spending the same $f$-calls by \wra{}.
This is implemented as follows.
When a termination condition is satisfied, let $\mathcal{Y} = \{y_k\}_{k=1}^{\ny}$ be the set of $\ny$ worst scenario candidates, $i_\mathrm{Adv} = \argmin_{i=1, \dots,\lambda_x} \max_{y \in \mathcal{Y}} f(x_i, y)$ be the best solution candidate, and $k_{\mathrm{Adv}} =  \argmax_{k=1,\dots,\ny} f(x_{i_\mathrm{Adv}}, y_k)$ be the corresponding worst scenario index, obtained at the last iteration.
Then, \acma{} is applied to optimize $f_{\mathcal{Y}}(x, y) = \max_{\tilde{y} \in \{y\} \cup \mathcal{Y}}f(x, \tilde{y})$, with distributions initialized around $(x_{i_\mathrm{Adv}}, y_{k_{\mathrm{Adv}}})$ to exhibit local search.
The search distribution for $x$ in \acma{} is initialized by the distribution for $x$ in WRA at the last iteration. 
The distribution parameters for search in $y$ is initialized by those of $\omega_{k_\mathrm{Adv}}$ if \Cref{alg:CMAESinwra} is used, and the mean is initialized by $y_{k_\mathrm{Adv}}$ obtained in \Cref{alg:SLSQPinwra} and a relatively small initial covariance matrix is set if \Cref{alg:SLSQPinwra} is used.
The other parameters of \acma{} are set to the default values proposed in \cite{akimoto2022berthing}. 
Once \acma{} is terminated, we perform a restart as in \wracma{} and \wraslsqp{}. 
The proposed approaches using \Cref{alg:CMAESinwra} and \Cref{alg:SLSQPinwra} with the local search and the restart strategy are denoted as \wracmaadv{} and \wraslsqpadv{}, respectively.

% The other is to use \acma{} after the proposed approach has converged.
% The proposed approach will be inefficient in local exploration because many $f$-calls will be consumed to approximate the ranking on the worst-case function. 
% On the other hand, we expect that \acma{} can efficiently improve the accuracy of a local solution. 
% \acma{} has a set $\mathcal{Y}$ that stores bad scenarios, and if we can give the worst scenario around the local solution in $\mathcal{Y}$, \acma{} can converge to the local solution on  $f_{\mathcal{Y}}(x) := \max_{y \in {\mathcal{Y}}} f(x,y)$ at least. 
% The scenarios at the convergence of the proposed approach are considered to be the approximated worst scenario set around the local solution, and denoted as $\{y_k^{\mathrm{converge}}\}_{k=1}^{\ny}$. 
% We expect that \acma{} works effectively by setting $\mathcal{Y}=\{y_k^{\mathrm{converge}}\}_{k=1}^{\ny}$. When \acma{} has converged, the proposed approach is restarted.
% The parameters for \acma{} is set as the following. 
% When the proposed approach converged at iteration $t$, their parameters are inherited to the initial parameters of \acma{}.
% The initial Gaussian distribution $\mathcal{N}(x_{\mathrm{Adv}}, \Sigma_{\mathrm{Adv}}^x)$ of the (1+1)-CMA-ES exploring the design vector $x$ is $x_{\mathrm{Adv}}=(x_l \mid l=\argmin_{i=1 ,\dots,\lambda} F^z_i)$ and $\Sigma_{\mathrm{Adv}}^x=\Sigma^t$.
% The initial Gaussian distribution $\mathcal{N}(y_{\mathrm{Adv}}, \Sigma_{\mathrm{Adv}}^y)$ of the (1+1)-CMA-ES exploring scenario $y$ is set as follows, with using index $k_{\mathrm{Adv}} =  \argmax_{k=1,\dots,\ny} f(x_{\mathrm{Adv}}, y_k)$.
% When \wracma{} is used, $\mathcal{N}(m_{k_{\mathrm{Adv}}}, \Sigma_{k_{\mathrm{Adv }}})$ among $\ny$ distributions is selected for $\mathcal{N}(y_{\mathrm{Adv}}, \Sigma_{\mathrm{Adv}}^y)$. 
% When \wraslsqp{} is used, we set the initial distribution parameters as $y_{\mathrm{Adv}} = y_{k_{\mathrm{Adv}}}$ and $\Sigma_{\mathrm{Adv}}^y =  0.1 \times \Sigma_{\mathrm{Adv}}^0$, where $\Sigma_{\mathrm{Adv}}^0$ is the covariance matrix for exploring $y$ initialized in the same manner with \acma{}. We use the small search distribution because the objective of using \acma{} is the local search.
% Other initial parameters for \acma{} are the recommended parameters in the previous study \cite{akimoto2022berthing}. The above approaches are denoted as \wracmaadv{} and \wraslsqpadv{}.


% After the convergence of \wraslsqp{} and \wracma{}, we expect to find better solutions by using \acma{} as a local optimization solver.
% \acma{} has a scenario set $\mathcal{Y}$ for keeping the approximated worst scenarios around the local solution. The scenarios at the convergence of \wra{} are denoted as  $\{y_k^{\mathrm{converge}}\}_{k=1}^{\ny}$ which are  and are considered to be the approximated worst scenario set around the local solution. 
% We expect that \acma{} works effectively by setting $\mathcal{Y}=\{y_k^{\mathrm{converge}}\}_{k=1}^{\ny}$ because \acma{} can explore the local solution on $f _{\mathcal{Y}}(x) := \max_{y \in \mathcal{Y}} f(x,y)$ at least. 
% Furthermore, when \wra{} is regarded as convergence at iteration $t$, the parameters of \wra{} are inherited to the initial parameters of \acma{}.
% The initial Gaussian distribution $\mathcal{N}(x_{\mathrm{Adv}}, \Sigma_{\mathrm{Adv}}^x)$ of the (1+1)-CMA-ES exploring the design variable $x$ is $x_{\mathrm{Adv}}=(x_l \mid l=\argmin_{i=1 ,\dots,\lambda} F^z_i)$ and $\Sigma_{\mathrm{Adv}}^x=\Sigma^t$. 
% The initial Gaussian distribution $\mathcal{N}(y_{\mathrm{Adv}}, \Sigma_{\mathrm{Adv}}^y)$ of the (1+1)-CMA-ES exploring scenario $y$ is set as follows, with using index $k_{\mathrm{Adv}} =  \argmax_{k=1,\dots,y_{\mathrm{Adv}}}f(x_{\mathrm{Adv}}, y_k)$.
% When \wracma{} is used, $\mathcal{N}(m_{k_{\mathrm{Adv}}}, \Sigma_{k_{\mathrm{Adv }}})$ among $\ny$ distributions is selected for $\mathcal{N}(y_{\mathrm{Adv}}, \Sigma_{\mathrm{Adv}}^y)$. When \wraslsqp{} is used, $\mathcal{N}(y_{\mathrm{Adv}}, \Sigma_{\mathrm{Adv}}^y) = \mathcal{N}(y_{k_{\mathrm{Adv}}}, 0.1 \times \Sigma_{\mathrm{Adv}}^x)$. Other initial parameters for \acma{} are the recommended parameters in the previous study \cite{akimoto2022berthing}. The above approaches are denoted as \wracmaadv{} and \wraslsqpadv{}.




\section{Numerical experiments on test problems} \label{sec:test}

% 提案アプローチである\wracma{}及び\wraslsqp{}と、既存手法、\acma{}\footnote{https://gist.github.com/youheiakimoto/ab51e88c73baf68effd95b750100aad0}と\zopgda{}\footnote{https://github.com/KaidiXu/ZO-minmax}、を使用して数値実験を行い、以下の仮説を検証する。
% (A) $\abs{\ywset(x^*)} = 1$ で $\yw(x)$ が$F$の最適解$x^*$周辺で連続で sensitiveな問題では、提案アプローチのみが最適解に収束できる (\Cref{sec:ex1})。
% (B) $\abs{\ywset(x^*)} > 1$でワーストシナリオが不連続な問題では、提案アプローチのみが最適解に収束できる (\Cref{sec:ex2})。

We conducted numerical experiments to confirm that existing approaches \acma{}% 
\footnote{\url{https://gist.github.com/youheiakimoto/ab51e88c73baf68effd95b750100aad0}} 
and 
\zopgda{}% 
\footnote{\url{https://github.com/KaidiXu/ZO-minmax}}
face Difficulties (I) and (II), whereas the proposed approach can cope with them. 


% the following hypotheses:
% (A) For problems where $\abs{\ywset(x^*)} = 1$ but $\yw(x)$ is continuously sensitive around the optimum solution $x^*$ of the worst-case function $F$, only the proposed approach among the applied approaches can converge to the optimal solution (\Cref{sec:ex1}).
% (B) For problems where $\abs{\ywset(x^*)} > 1$ and the worst scenarios are discontinuous, only the proposed approach among the applied approaches can converge to the optimal solution (\Cref{sec:ex2}).

\subsection{Common settings} \label{sec:common}

% 上記の仮説を検証するためのテスト問題$f_1$--$f_{10}$を考えたので\Cref{tab:testp}に示す。テスト問題$f_1$--$f_{10}$は、最適解$x^*$周辺で目的関数が異なる性質 (smoothness, convexity and concavity) を有する問題である。
% 今回の数値実験では、共通して、探査範囲を$\mathbb{X}=[-3, 3]^m$で$\mathbb{Y}=[-3, 3]^n$とする。また、設計変数の次元数は$m=20$で、シナリオ変数の次元数は$n=20$とする。
% また、テスト問題中の係数行列は$B = \diag{(b,...,b)}$とし、係数$b$によりワーストシナリオの変化のsensitivityを制御する。
We used test problems listed in \Cref{tab:testp}.
Unless otherwise specified, the dimensions were $m = n = 20$, and the search domains were $\mathbb{X}=[-3, 3]^m$ and $\mathbb{Y}=[-3, 3]^n$.
The coefficient matrix $B$ was set to $B = \diag(b,.... ,b)$.
% The min--max solution $x^*$ is a strict global min--max saddle point on $f_5$--$f_8$ and $f_{11}$, a weak min--max saddle point on $f_1$--$f_4$, and not a min--max saddle point on  $f_9$ and $f_{10}$.

The proposed and existing approaches were configured as follows. 
The initial mean vector (\wracma{} and \wraslsqp{}) and initial solution candidate (\acma{} and \zopgda{}) for outer minimization were drawn from $\mathcal{U}(\mathbb{X})$.
The initial covariance matrices for the outer minimization were set to $\left(\frac{u_x - \ell_x}{4}\right)^2 I_m$ in \wracma, \wraslsqp, and \acma{}. 
The initial mean vectors (\wracma{}) and the initial worst scenario candidates (\wraslsqp{}, \acma{}, and \zopgda{}) were drawn independently from $\mathcal{U}(\mathbb{Y})$. 
The initial covariance matrices for the inner maximization were set to $\left(\frac{b_y}{2}\right)^2 I_n$ in \wracma{} and \acma{}.
%The initial worst scenario candidates in \wracma{} were drawn as $y_k \sim \mathcal{N}(m_k, \Sigma_k)$.
For \wracma{} and \wraslsqp{}, we set $V_{\min}^x=10^{-12}$ (no restart happened with this setting), $c_{\max}=1$, and $\ny=36 (=3 \times \lambda_x)$. For \wracma{}, we set $V_{\min}^y=10^{-4}$ and $T_{\min} =10$. For \wraslsqp{}, we set $U_{\min}=10^{-5}$, $\beta=0.5$, and the initial learning rate $\{\eta_k\}_{k=1}^{\ny} = 1$.  
% \textcolor{red}{$V_{\min}^y=10^{-4}$}.
For \zopgda{}, with reference to \cite{Liu2020}, the learning rates were set to $\eta_x = 0.02$ and $\eta_y =0.05$, the number of random direction vectors was set to $q=5$,  and the smoothing parameter for gradient estimation was set to $\mu=10^{-3}$.
For \acma, with reference to \cite{akimoto2022berthing}, threshold parameter for restart was set to $G_{\mathrm{tol}} = 10^{-6}$, the minimal learning rate was set to $\eta_{\min}=10^{-4}$, and the minimal standard deviation was set to $\bar{\sigma}_{\min} = 10^{-8}$. 
For simplicity of the analysis, the restart strategy of \wracma{} and \wraslsqp{} was not used in this experiments. \acma{} performed restart because it is implemented by default. 
We also turned off the diagonal acceleration mechanism both in CMA-ES for the outer minimization and the inner maximization in \wracma{}.



% Min--max問題の外側ループに対する\cmaes{}のパラメータは以下の通りに設定した。初期平均ベクトルは$m^0 \sim \mathcal{U}(-3, 3)^m$、初期共分散行列は$\Sigma^0 = \diag(1.5,...,1.5)^2$とした。進化パス等のその他のパラメータ$\theta_x$は推奨パラメータ\cite{akimoto2019}を用いた。

% The configuration of \cmaes{} for the outer loop minimization in the proposed approaches was set as follows. 
% % We used the version of \cmaes{} called DD-CMA-ES \cite{akimoto2019}%
% % \footnote{\url{insert the url of dd-cma-es}}.
% The initial mean vector was $m^0 \sim \mathcal{U}(\mathbb{X})$ and the initial covariance matrix was $\Sigma^0 = \diag(1.5,.... ,1.5)^2$. We set the parameters for the termination : $V_{\min}=10^{-8}$ and $Cond_{\max}=10^{-12}$.
% % The other dynamic parameters $\theta_x$ and the static parameters were set to their default values proposed in \cite{akimoto2019}.


% \wra{}のパラメータは以下の通りに設定した : $\ny= 3 \times \lambda_x$。\cmaes{}が各反復で生成する解候補の数は推奨パラメータ\cite{akimoto2019}の通り、$\lambda_x= 4 + \lfloor 3 \ln{(m)} \rfloor = 12$なため、$\ny=36$である。
% 次に、\wracma{}中のソルバー\cmaes{}では、初期平均ベクトルは$\{ m_k \}_{k=1}^{\ny} \sim \mathcal{U}(-3, 3)^n$で、初期共分散行列は$\{ \Sigma_k \}_{k=1}^{\ny} = \diag(1.5,...,1.5)^2$とした
% \footnote{
% テスト問題$f_4$において、ソルバー\cmaes{}の共分散行列の条件数$Cond(\Sigma)$が極端に大きくなることを確認した。そこで、テスト問題$f_4$においてのみ$Cond(\Sigma) > 10^{6}$となったソルバー\cmaes{}は、シナリオ探査を終了するとした。
% }。
% \wraslsqp{}中のソルバー\slsqp{}では、$\ny$個の初期シナリオは$\{y_k\}_{k=1}^{\ny} \sim \mathcal{U}(-3, 3)^n$、初期学習率は$\{\eta_{k}\}_{k=1}^{\ny}=1$。


% The hyperparameters for \wra{} were set as follows : $\tau_{\mathrm{threshold}}=0.7$, $T_{\min}=10$, $\gamma=1/n$ ($c_{\max}=\gamma \times n = 1$), $\ny = 3 \times \lambda_x$, $\{p_k\}_{k=1}^{\ny} = 1$, $p_p=0.4$, $p_n=0.05$, $p_{\mathrm{threshold}}=0.1$. 
% Because $\lambda_x= 4 + \lfloor 3 \ln{(m)} \rfloor = 12$ for $m = 20$, we have $\ny=36$ in this experiment.
% The configuration of \cmaes{} in \wracma{} was as follows.
% We used DD-CMA-ES with the default setting recommended in \cite{akimoto2019}. 
% The initial mean vector was $m_k \sim \mathcal{U}(\mathbb{Y})$ and the initial covariance matrix was $\Sigma_k = \diag(1.5,.... ,1.5)^2$ for all $k = 1,\dots,\ny$%
% \footnote{
% In test problem $f_4$, the condition number $Cond(\Sigma)$ of the covariance matrix of solver \cmaes{} becomes huge. Therefore, in only test problem $f_4$, we assumed that the solver \cmaes{} terminates the scenario exploration when $Cond(\Sigma) > 10^{6}$.
% }.
% The minimum standard deviation was set to $V_{\min} = 10^{-4}$.
% The configuration of \slsqp{} in \wraslsqp{} was as follows.
% We used \slsqp{} implemented in \texttt{scipy}. 
% The initial scenario was $y_k \sim \mathcal{U}(\mathbb{Y})$ and initial learning rate was $\eta_{k}=1$. The factor for the learning rate update is $\beta = 0.5$. The minimal parameter update for termination was set to $U_{\min} = 10^{-5}$.

%The sensitivities of \wracma{} and \wraslsqp{} on the choice of $\gamma$ and $\tau_{\mathrm{threshold}}$ were investigated in \Cref{sec:sens}. 
% The results of \wracma{} showed that $f$-calls could be reduced by setting $\gamma$ appropriately, and the sensitivity of $\tau_{\mathrm{threshold}}$ to $f$-calls was relatively small. The results for \wraslsqp{} showed that the sensitivity of $\gamma$ and $\tau_{\mathrm{threshold}}$ to $f$-calls was relatively small.

% The hyperparameters for the proposed approaches were set as follows.
% The number of configurations for \wracma{} and \wraslsqp{} was commonly set at $\ny = 3 \times \lambda_x$. Because $\lambda_x= 4 + \lfloor 3 \ln{(m)} \rfloor = 12$ for $m = 20$, we have $\ny=36$ in this experiment.
% The configuration of \cmaes{} in \wracma{} was as follows.
% The initial mean vector was $m_k \sim \mathcal{U}(\mathbb{Y})$ and the initial covariance matrix was $\Sigma_k = \diag(1.5,.... ,1.5)^2$ for all $k = 1,\dots,\ny$. The parameter for the termination was set to $\tilde{Cond}_{\max}=10^{-6}$.  
% % eigen valueのルートの大小の比が-6乗
% The configuration of \slsqp{} in \wraslsqp{} was as follows. The initial scenario was $y_k \sim \mathcal{U}(\mathbb{Y})$ and initial learning rate was $\eta_{k}=1$.
% Other hyperparameters were set to the default values described in \Cref{sec:wracma} for \wracma{} and \Cref{sec:wraslsqp} for \wraslsqp{}. 

% 既存手法\zopgda{}と\acma{}のパラメータは各々の既存研究 (各々\cite{Liu2020}、\cite{akimoto2022berthing}) を参照して設定した。
% \zopgda{}に対しては学習率を$\eta_x=0.02$及び$\eta_y=0.05$とした。また、\acma{}のリスタートに関わるパラメータ$G_{\mathrm{tol}}$は$10^{-6}$とした。


% The configurations of the existing approaches, \zopgda{} and \acma{}, were based on the previous studies \cite{Liu2020,akimoto2022berthing}. For both \zopgda{} and \acma{}, the initial the design vector and the scenario vector were generated from the search domain $x^0 \sim \mathcal{U}(\mathbb{X})$ and $y^0 \sim \mathcal{U}(\mathbb{Y})$. As specific configurations for \zopgda{}, the learning rates were set to $\eta_x = 0.02$ and $\eta_y =0.05$. 
% Specific configurations for \acma{} are in the followings.
% Two step sizes for exploring $x$ and $y$ were initialized to $\sigma^x=\sigma^y=1.5$. 
% The termination threshold $(G_{\mathrm{tol}}, \bar{\sigma}_{\min})$ for the restart strategy was set to $(10^{-6}, 10^{-8})$ respectively.


% 探査領域の矩形制約に対しては、\zopgda{}はデフォルトで設定されているprojected gradientを使用した。提案手法と\acma{}は、標準偏差に上限を設けたミラーリング\cite{yamaguchi2018}を使用した。

% The box constraint of the search domain is treated by the projected gradient in \zopgda{}, and was treated by the mirroring technique along with upper-bounding the coordinate-wise standard deviation \cite{yamaguchi2018} for the proposed approaches and \acma{}.


% 各々のアルゴリズムの性能は$20$回の独立試行により評価する。収束判定は以下の通り行う。$f$-callsの上限を$10^{7}$回とし、それまでに$\abs{F(m^t)-F(x^*)}$を満足すれば最適解に収束できたと判定する。\zopgda{}における$m^t$は、反復$t$の解候補$x^t$とする。最適解に収束する前に、$10^{7}$回の$f$-callsが消費された場合、最適解探査は失敗したと評価する。また、$\wra{}$において、$f$-callsが$10^{7}$回に到達する前に設計変数$x$を探査する\cmaes{}の探査範囲が小さくなった場合 (例えば、$\max_{j}(\sqrt{\Sigma_{j,j}}) < 10^{-8}$や共分散行列の条件数が$Cond(\Sigma) > 10^{8}$となった場合) 、最適解探査を中断し、解探査は失敗されたと評価する。


The performance of each algorithm is evaluated by $20$ independent trials. 
We regarded the trial was successful if $\abs{F(z)-F(x^*)} \leq 10^{-6}$ was satisfied for $z = m_x^t$ in case of \acma{} and WRA, and for $z = x^t$ in case of \zopgda, before $10^7$ $f$-calls were spent. If the maximum $f$-calls were spent or some internal termination conditions were satisfied, we regard the trial was failed.


% In \wra{}, if the search distribution of \cmaes{} exploring the design vector $x$ becomes too small before $f$-calls reaches $10^{7}$ (e.g., $\max_{j}(\sqrt{\Sigma_{j,j}}) < 10^{-8}$ or the condition number of covariance matrix $Cond(\Sigma) > 10^{8}$), such a run is terminated and is evaluated as failed.




\subsection{Experiment 1} \label{sec:ex1}

% 仮説 (A) を検証するために、$b \in \{1,3,10,30,100\}$としたテスト問題$f_5$--$f_{8}$と$f_{10}$に4つの最適化アルゴリズムを適用した。

% To confirm hypothesis (A), four approaches were applied to test problems $f_5$--$f_{8}$ and $f_{11}$ with $b \in \{1,3,10,30,100\}$. 

To confirm that the proposed approaches overcome Difficulty (I), four approaches were applied to smooth convex--concave problems $f_5$, $f_{7}$ and $f_{11}$ for varying $b$ with and without bounds for the search domains.% at $b \in \{1,3,10,30,100\}$.

\providecommand{\exss}{0.5}
\begin{figure}[t]
\centering%
\begin{subfigure}{\exss\hsize}%
\centering%
    \includegraphics[width=\hsize]{figures/multirun-unbound_plot_fn5_ntry20.pdf}%
    \caption{$f_{5}$ (unbounded)}%
\end{subfigure}%
\begin{subfigure}{\exss\hsize}%
\centering%
    \includegraphics[width=\hsize]{figures/multirun_plot_fn5_ntry20.pdf}%
    \caption{$f_{5}$}%
\end{subfigure}%
\\
\begin{subfigure}{\exss\hsize}%
    \includegraphics[width=\hsize]{./figures/multirun-unbound_plot_fn7_ntry20.pdf}%
  \caption{$f_7$ (unbounded)}%  
\end{subfigure}%
\begin{subfigure}{\exss\hsize}%
    \includegraphics[width=\hsize]{./figures/multirun_plot_fn7_ntry20.pdf}%
  \caption{$f_7$}%  
\end{subfigure}%
\\
\begin{subfigure}{\exss\hsize}%
\centering%
    \includegraphics[width=\hsize]{figures/multirun-unbound_plot_fn11_ntry20.pdf}%
    \caption{$f_{11}$ (unbounded)}%
\end{subfigure}%
\begin{subfigure}{\exss\hsize}%
\centering%
    \includegraphics[width=\hsize]{figures/multirun_plot_fn11_ntry20.pdf}%
    \caption{$f_{11}$}%
\end{subfigure}%
\caption{Average and standard deviation of the number of $f$-calls spent by \wracma{}, \wraslsqp{}, \zopgda{}, and \acma{} over successful trials on $f_5$, $f_{7}$ and $f_{11}$ with $b \in \{1, 3, 10, 30, 100 \}$. Left: unbounded search domains ($\mathbb{X} = R^m$ and $\mathbb{Y} = R^n$). Right: bounded search domains.}
\label{fig:ex1}
\end{figure}


\begin{figure}[t]
\centering%
\begin{subfigure}{\exss\hsize}%
\centering%
    \includegraphics[width=\hsize]{figures/multirun_results_20ntry_fn11.pdf}%
    \caption{$f_{11}$ (unbounded)}%
\end{subfigure}%
\begin{subfigure}{\exss\hsize}%
\centering%
    \includegraphics[width=\hsize]{figures/multirun_results-unbound_20ntry_fn11.pdf}%
    \caption{$f_{11}$}%
\end{subfigure}%
\caption{Gap $\abs{F(m^t) - F(x^*)}$ with the number of $f$-calls at $b=1$ on $f_{11}$.
Solid line: median (50 percentile) over 20 runs. Shaded area: interquartile range ($25$--$75$ percentile) over 20 runs.}
\label{fig:ex1_fcalls}
\end{figure}

\subsubsection{Results}

% 最適解探査に要した$f$-callsを\Cref{fig:ex1_sensb}に示す。また、各問題において$b=1$としたときのギャップ$F(m^t)-F(x^*)$の推移を\Cref{fig:ex1_fcalls}に示す。



\Cref{fig:ex1} shows that the proposed approaches, \wracma{} and \wraslsqp{}, could successfully optimize $f_5$ and $f_7$ with all $b \in \{1, 3, 10, 30, 100 \}$ in all trials, whereas the existing approaches, \acma{} and \zopgda{}, failed to optimize them except for $f_5$ with $b \leq 3$. 
\wracma{} was the only approach that successfully optimized $f_11$ with all $b$ values, whereas \acma{} could optimize $f_{11}$ with $b = 1$ and only when $b=3$ without bound for the search domain. 
From these results, we confirm that both \zopgda{} and \acma{} fails on problems where the min--max solution is a global min--max saddle point but it is not locally smooth and strongly convex--concave, and that the proposed approaches can solve such problems. 

When the search domain is unbounded on $f_5$, both \zopgda{} and \acma{} successfully locate near optimal solutions for $b \leq 3$ with smaller $f$-calls than the proposed approaches. However, for $b \geq 30$, both approaches failed to converge even though $f_5$ is a smooth and strongly convex--concave function.
%, where their convergence is guaranteed if the search domain is unbounded.
On $f_{11}$ with the unbounded search domain, \zopgda{} failed to converge at every trials, and \acma{} could not obtain successful convergence for $b \geq 10$ even though $f_{11}$ is also a smooth and strongly convex--concave function.
In case of \zopgda{}, an inadequate learning rate may be a possible reason. 
For convergence, it must be tuned problem-dependently.
However, even if an appropriate value is set, the slow convergence issue discussed in \Cref{sec:limit} appears.
In case of \acma{}, when $b \geq 30$ on $f_5$ and $f_{11}$ with the unbounded search domain, slow convergence issue is main reason, as the expected $f$-calls (blue dash line in \Cref{fig:ex1}) on $f_5$ exceeded $f$-calls budget. 
When the search domain is unbounded, we expect \acma{} obtain the successful convergence on $f_{11}$ until $b=10$ as the same as on $f_5$  because the required number of iterations to find near-optimal solution on both functions is the same at $\Omega (1+b^2)$. However, \acma{} failed to converge on $f_{11}$ with $b=10$. We observed that \acma{} suffered to approach $x^*$ because the learning rate reached to lower bound $\eta_{\min}$. From this point, to obtain successful convergence on $f_{11}$ with $b=10$, lower bound $\eta_{\min}$ for \acma{} need to be set in properly.
% It was possible that the learning rate was not always updated properly on $f_{11}$, where the objective function for $y$ is ill-scale.  
% 1つの試行を見ると、\eta_minが10^{-4}に到達していることを確認した。つまり、\etaの適応が必ずしもうまくいっていない可能性がある。
When there was bound for search domain, \acma{} failed to converge with $b=10$ on $f_{5}$ and $b=3$ on $f_{11}$, respectively.  
% One possible reason is the bounded domain, which is not considered in the theoretical analysis in \cite{akimoto2022berthing}. 

The difference between \wracma{} and \wraslsqp{} appears in the speed of convergence on $f_5$ and $f_7$, as well as on the performance on $f_{11}$. 
On $f_5$ and $f_7$, \wraslsqp{} converged faster than \wracma{}. 
On the other hand, \wraslsqp{} failed to optimize $f_{11}$ within a given budget of $f$-calls.
\Cref{fig:ex1_fcalls} shows gap $F(m^t) - F(x^*)$ on $f_{11}$ with $b=1$.
From \Cref{fig:ex1_fcalls}, we expect that \wraslsqp{} eventually converges but the convergence speed is very slow. 
% It is probably because \Cref{alg:SLSQPinwra} with small $c_{\max}$ approximates the rankings $Rank_F\{(x_i)_{i=1}^{\lambda_x})\}$ on $f_{11}$ that is ill-conditioned in $y$. 
% \Cref{alg:SLSQPinwra} is an extended approach of \slsqp{}, therefore, \Cref{alg:SLSQPinwra} will converge slowly on ill-conditioned function in which \slsqp{} shows slow convergence. 
Preliminary, we confirmed that \Cref{alg:SLSQPinwra} converge slowly on ill-conditioned function.  
Therefore, on $f_{11}$ which is ill-conditioned function in $y$, \Cref{alg:SLSQPinwra} with small $c_{\max}$ cannot significantly improve the worst scenario candidate $\hat{y}$ and early stopping strategy may terminate the inner maximization process before approximating the worst scenario in adequate accuracy. As a result of the underestimation of the  rankings on the worst case function $Rank_F\{(x_i)_{i=1}^{\lambda_x})\}$ by WRA, outer minimization failed to converge at the global min--max solution. This result indicates the importance of the choice of the internal solver for WRA mechanism. Because CMA-ES is a variable metric approach and the covariance matrices are inherited over WRA calls, \wracma{} could optimize $f_{11}$ efficiently.


% \Cref{fig:ex1_sensb}から、ワーストシナリオの変化が大きい ($b$が大きいと) 場合でも、提案アプローチ\wracma{}は最適解に収束できていることがわかる。
% これは、\wracma{}が適切に$F$を近似できたためと考えられる。例えば、$b=1$としたテスト問題$f_5$において、20試行の中の1試行の各反復$t$で得られた$\{F^z_i\}_{i=1}^{\lambda}$とワーストケース関数値$\{F(x_i)\}_{i=1}^{\lambda}$のKendall's $\tau$の推移を\Cref{fig:ex1_kendall}に示す。図が示す通り、高頻度で高い$\tau$が得られており、おおむね$\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda})$を推定できていると考えられる。\Cref{fig:ex1_fcalls}からも、$f$-callsの増加に伴って最適解に漸近できていることがわかる。
% \Cref{fig:ex1_sensb}が示す通り、今回の実験では、$b \geq 10$では、\wracma{}でしか解けていない。
% また、ワーストシナリオのsensitivityに関して$f$-callsの変動が鈍感である。特に、テスト問題$f_5$での結果に関しては、$f$-callsはおおよそ$\log{b}$に比例している。これについては\Cref{sec:dis}で議論する。


% \Cref{fig:ex1_sensb} shows that the proposed approach \wracma{} was able to converge to the optimal solution even when the worst scenario change was sensitive (when $b$ was large).
% This can be attributed to that \wracma{} was able to approximate $F$ appropriately. For example, in test problem $f_5$ with $b=1$, Kendall's $\tau$ between $\{F^z_i\}_{i=1}^{\lambda}$ and worst case function values $\{F(x_i)\}_{i=1}^{\lambda}$ obtained at each iteration $t$ resulted from a trial among 20 is shown in \Cref{fig:ex1_kendall}. 
% As the figure shows, high $\tau$ is obtained in frequently. Therefore, $\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda})$ was appropriately estimated. 
% \Cref{fig:ex1_fcalls} shows that \wracma{} could asymptotically approach the optimal solution as $f$-calls increases.
% As shown in \Cref{fig:ex1_sensb}, in this experiments, only \wracma{} could solve the problem with $b \geq 10$.
% In addition, the change of $f$-calls with respect to $b$ (the sensitivity of the worst scenario change) was relatively insensitive. In particular, in test problem $f_5$, $f$-calls were roughly proportional to $\log{b}$. This will be discussed in the \Cref{sec:dis}.

% \Cref{fig:ex1_sensb} shows that the proposed approaches, \wracma{} and \wraslsqp{}, could converge to the optimal solution even when the worst scenario change was sensitive (when $b$ was large) on test problems $f_5$--$f_8$. In this experiments, only the proposed approaches among the applied approaches could solve the problem on test problems $f_5$--$f_8$ with $b \geq 10$.  
% This experiment results are attributed to that the proposed approaches approximated $F$ appropriately. For example, in test problem $f_5$ with $b=1$, Kendall's $\tau$ between $\{F^z_i\}_{i=1}^{\lambda}$ and the worst-case function values $\{F(x_i)\}_{i=1}^{\lambda}$ obtained at each iteration $t$ resulted from a trial among 20 is shown in \Cref{fig:ex1_kendall}. 
% As the figure shows, the proposed approaches obtained obtained high $\tau$ in frequently. In addition, \Cref{fig:ex1_fcalls} shows that the proposed approaches could asymptotically approach the optimal solution as $f$-calls increases.
% %Therefore, $\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda})$ was appropriately estimated.
% Moreover, the change of $f$-calls with respect to $b$ (the sensitivity of the worst scenario change) was relatively insensitive. In particular, in test problem $f_5$, $f$-calls required by \wracma{} were roughly proportional to $\log{b}$. This will be discussed in the \Cref{sec:dis}.
% In this experiments, \wraslsqp{} converged with less $f$-calls than \wracma{}. This is possibly because the objective function for $y$ in test problem $f_5$--$f_8$ was concave, therefore, \slsqp{} could approximate  $\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda })$ with less $f$-calls than \cmaes{}.



% \Cref{fig:ex1_sensb}から、\wraslsqp{}も\wracma{}と同様に、ワーストシナリオの変化が大きい場合でも、最適解に収束できていることがわかる。
% テスト問題$f_5$--$f_8$では\wracma{}よりも少ない$f$-callsで収束てきた。これは、テスト問題$f_5$--$f_8$の$y$に対する目的関数がconcaveであったため、\cmaes{}よりも\slsqp{}のほうが少ない$f$-callsで$\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda})$を近似できたためと考えられる。\wracma{}と同様に、$b=1$としたテスト問題$f_5$における、20試行の中の1試行の$\{F^z_i\}_{i=1}^{\lambda}$とワーストケース関数値$\{F(x_i)\}_{i=1}^{\lambda}$のKendall's $\tau$の推移を\Cref{fig:ex1_kendall}に示す。図が示す通り、高頻度で高い$\tau$が得られている。
% 一方、テスト問題$f_{10}$では、どの試行においても、\wraslsqp{}は最適解に収束できなかった。これは、テスト問題$f_{10}$での$y$に対する目的関数$h(y)$がill scaleであり、\slsqp{}でワーストシナリオを適切に求めることができなかったためと考えられる。つまり、\wraslsqp{}は$\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda})$を近似できず、ワーストケース関数$F$と大きくことなった関数について最適解を探査したため、$x^*$に収束できなかったと考えられる。


% From the \Cref{fig:ex1_sensb}, similar to \wracma{}, \wraslsqp{} could converge to the optimal solution even when the worst scenario change was sensitive.
% In test problems $f_5$--$f_8$, \wraslsqp{} converged with less $f$-calls than \wracma{}. This is possibly because the objective function for $y$ in test problem $f_5$--$f_8$ was concave, therefore, \slsqp{} could approximate  $\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda })$ with less $f$-calls than \cmaes{}. 
% Similar to \wracma{}, \Cref{fig:ex1_kendall} shows the history of Kendall's $\tau$ between $\{F^z_i\}_{i=1}^{\lambda}$ and worst case function value $\{F(x_i)\}_{i=1}^{\lambda}$ for one of the 20 trials in test problem $F_5$ with $B=1$. As the figure shows, high $\tau$ are obtained at high frequencies.
% On the other hand, for test problem $f_{10}$, \wraslsqp{} failed to converge to the optimal solution in any trial. This may be because the objective function for $y$ was ill scale, and the worst scenario could not be properly obtained by \slsqp{}. 
% In other words, \wraslsqp{} could not approximate $\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda})$. As a result, \wraslsqp{} could not converge to $x^*$ because it explored the optimal solution for a function that is significantly different from the worst-case function $F$.


% On test problem $f_{11}$, although \wracma{} converged at the optimal solution, \wraslsqp{} failed at any trials. This may be because the objective function for $y$ on $f_{11}$ was ill scale, and the worst scenario was not properly approximated by \slsqp{}. This leads to underestimate the ranking on the wort-case function $\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda})$. As a result, \wraslsqp{} could not converge to $x^*$ because it explored the optimal solution for a function that is significantly different from the worst-case function $F$. On test problem $f_{11}$ with $b \geq 3$, only \wracma{} among the applied approaches obtained the successful convergence in any trials, and the required $f$-calls was insensitive.

% \Cref{fig:ex1_sensb}より、テスト問題$f_6$--$f_8$では、既存手法はどの試行においても最適解へ収束できなかった。
% \Cref{sec:limit}で述べた通り、既存手法が対象とする問題では、目的関数がtwice continuously differentiable で globally strongly convex-concave であり，その勾配が Lipschitz continuousとなることを仮定している。一方、テスト問題$f_6$--$f_8$の目的関数はこれらの仮定とは異なるため、既存手法では対処できなかったと考えられる。


% As shown in \Cref{fig:ex1_sensb}, for test problems $f_6$--$f_8$, the existing approaches failed to converge to the optimal solution in any trials.
% As mentioned in \Cref{sec:limit}, the existing approaches target the problems whose the objective function is twice continuously differentiable and globally strongly convex-concave, and its gradient is Lipschitz continuous. On the other hand, the objective functions of test problems $f_6$--$f_8$ were different from these assumptions. Thus, they could not address test problems $f_6$--$f_8$.

% As shown in \Cref{fig:ex1_sensb}, on test problems $f_6$--$f_8$, the existing approaches failed to converge to the optimal solution in any trials. 
% As mentioned in \Cref{sec:limit}, the existing approaches target the problems whose the objective function is twice continuously differentiable and globally strongly convex-concave, and its gradient is Lipschitz continuous \cite{Liu2020, akimoto2022berthing}. The objective functions on test problems $f_6$--$f_8$ does not satisfy this assumption.
% Thus, they could not address test problems $f_6$--$f_8$.


% 一方、テスト問題$f_5$と$f_{10}$の結果では、$b$が比較的小さい場合、提案アプローチよりも既存手法、\zopgda{}もしくは\acma{}、のほうが少ない$f$-callsで収束できている。テスト問題$f_5$と$f_{10}$の目的関数が既存研究\cite{Liu2020,akimoto2022berthing}での仮定と同じであったため、既存手法により最適解に収束できたと考えられる。ここで、$b$が大きい場合に既存手法が最適解収束に失敗した理由について考察する。まず、テスト問題$f_5$における\acma{}の結果について考察する。ワーストシナリオの変化が比較的小さい$b=\{1,3\}$のとき、$f$-callsは$b^2$で増加していることがわかる。これは、既存研究\cite{akimoto2022berthing}での理論解析通りである。また、$b=10$では、\Cref{fig:ex1_sensb}中のfitting curveより、\acma{}は$f$-callsの上限以内で最適解に収束することが予想された。しかし、最適化に失敗した。これは矩形制約のためと考えている。既往研究\cite{akimoto2022berthing}の理論解析では探査区間を設けないことを想定している。今回の実験設定のように探査区間に制約を設けた$b=10$の$f_5$は$f_1$と類似した問題になる。具体的には、設計変数$x$中のある$i$軸が$\abs{x_i}>0.3=3/b$となるとき、ワーストシナリオは$\hat{y}_i (x) = 3 \sign(x_i)$である。これは$f_1$と同じである。次の数値実験結果で示すが、\acma{}は$f_1$に対して最適解収束に失敗している。このため、\acma{}は$\abs{x_i}<0.3=3/b$ ($i=1,...,m$) となる領域に向かうことが困難になったと考えている。テスト問題$f_{10}$における$b \geq 3$でも同様の理由により\acma{}は最適解収束に失敗したと考えられる。テスト問題$f_{10}$における\acma{}の結果では、$b=1$においても、多くの軸のワーストシナリオが$f_1$と類似した問題になっている。このため、$f_5$で最適解に収束できた$b=3$であっても、$f_{10}$では最適解収束に失敗したと考えられる。
% 最後に、\zopgda{}は$b \geq 10$とした$f_5$及び$f_{10}$において、いづれの試行でも最適解に収束できなかった。これは、\zopgda{}の性能が学習率等のパラメータに敏感であるためと考えられ、今回の学習率の設定が不適切であったためと考えられる。


% On the other hand, \Cref{fig:ex1_sensb} for test problems $f_5$ and $f_{11}$ shows that the existing approaches, \zopgda{} and \acma{}, converge with less $f$-calls than the proposed approach when $b$ is relatively small. 
% This is because that the objective functions of test problems $f_5$ and $f_{11}$ were the same as the assumptions in the previous studies \cite{Liu2020, akimoto2022berthing}.
% Here, we discuss the reasons why the existing approaches failed to converge to the optimal solution when $b$ is large. 
% First, we consider the results of \acma{} on test problem $f_5$. \acma{} converged with $f$-calls in proportional to $b^2$ when the change in the worst scenario is relatively small ($b=\{1,3\}$). 
% This results follow the theoretical analysis in the previous study \cite{akimoto2022berthing}. 
% For $b=10$, from the fitting curve in \Cref{fig:ex1_sensb}, it was expected that \acma{} would converge to the optimal solution. However, the optimization failed. 
% This is probably due to the box constraint. The theoretical analysis of the previous study \cite{akimoto2022berthing} assumes no exploration domain. Test problem $f_5$ for $b=10$ with a box constraint becomes similar to $f_1$. 
% Specifically, when some $i$-axis in the design vector $x$ is $\abs{x_i}>0.3=3/b$, the worst-case scenario is $[\yw(x)]_i = 3 \sign(x_i)$. This is the same as that of $f_1$. 
% As the experimental results are shown in \Cref{sec:ex2}, \acma{} failed to converge to the optimal solution on $f_1$. For this reason, \acma{} was not able to converge to the region where $\abs{x_i}<0.3=0.3=3/b$ ($i=1,...,m$).
% For the same reason in test problem $b \geq 3$ in $f_{11}$, \acma{} failed to converge to the optimal solution. In test problem $f_{11}$, even at $b=1$, the worst scenario for many axes is similar to that on $f_1$. 
% Therefore, \acma{} failed to converge to the optimal solution on $f_{11}$ with $b=3$ which \acma{} could converge to the optimal solution on $f_5$.
% Finally, \zopgda{} failed to converge to the optimal solution on both test problems $f_5$ and $f_{11}$ with $b \geq 10$ in any trials. This is because the performance of \zopgda{} is sensitive to hyperparameters such as the learning rate, and their settings was not tuned in this experiment.


% We discuss the reasons why the existing approaches, \zopgda{} and \acma{}, failed to converge to the optimal solution when $b$ is large. 
% First, we consider the results of \acma{} on test problem $f_5$. \acma{} converged with $f$-calls in proportional to $b^2$ when the change in the worst scenario is relatively small ($b=\{1,3\}$). 
% This results follow the theoretical analysis in the previous study \cite{akimoto2022berthing}. 
% For $b=10$, from the fitting curve in \Cref{fig:ex1_sensb}, it was expected that \acma{} would converge to the optimal solution. However, the optimization failed. 
% This is probably due to the box constraint. The theoretical analysis of the previous study \cite{akimoto2022berthing} assumes no exploration domain. Test problem $f_5$ for $b=10$ with a box constraint becomes similar to $f_1$. 
% Specifically, when some $i$-axis in the design vector $x$ is $\abs{x_i}>0.3=3/b$, the worst scenario is $[\yw(x)]_i = 3 \sign(x_i)$. This is the same as that of $f_1$. 
% As the experimental results are shown in \Cref{sec:ex2}, \acma{} failed to converge to the optimal solution on $f_1$. For this reason, \acma{} was not able to converge to the region where $\abs{x_i}<0.3=0.3=3/b$ ($i=1,...,m$).
% For the same reason in test problem $b \geq 3$ in $f_{11}$, \acma{} failed to converge to the optimal solution. In test problem $f_{11}$, even at $b=1$, the worst scenario for many axes is similar to that on $f_1$. 
% Therefore, \acma{} failed to converge to the optimal solution on $f_{11}$ with $b=3$ which \acma{} could converge to the optimal solution on $f_5$.
% Finally, \zopgda{} failed to converge to the optimal solution on both test problems $f_5$ and $f_{11}$ with $b \geq 10$ in any trials. 
% This is because the performance of \zopgda{} is sensitive to hyperparameters such as the learning rate, and their settings was not tuned in this experiment.
% On the other hand, \Cref{fig:ex1_sensb} for test problems $f_5$ and $f_{11}$ shows that the existing approaches converge with less $f$-calls than the proposed approach when $b$ is relatively small. 
% % This is because that the objective functions of test problems $f_5$ and $f_{11}$ were the same as the assumptions in the previous studies.


% 総じて、提案アプローチ\wraslsqp{}と\wracma{}は、$\abs{\ywset(x^*)} = 1$ で $\yw(x)$ が$F$の最適解 $x^*$周辺で sensitiveな問題において、最適解に収束できる点で既存手法よりも優位であることが示された。ただし、\wraslsqp{}において、$y$の目的関数がill scaleとなる問題では、\slsqp{}では$\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda})$を適切に近似できないため、最適解に収束できなかった。これより、\wra{}を使用する際は、$\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda})$を適切に近似できるようなソルバーを選択することが重要である。


% Overall, the proposed approaches, \wraslsqp{} and \wracma{}, are superior to the existing approaches in terms of that they can converge to the optimal solution for the problem where the optimal solution $x^*$ on $F$ is a global min--max saddle point and the worst scenario around $x^*$ is sensitive. 
% \wraslsqp{} could not converge to the optimal solution in problems where the objective function of $y$ is ill-scale, because it cannot approximate $\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda})$ appropriately. This result shows, when using \wra{}, it is important to select a solver that can approximate $\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda})$ appropriately.



\subsubsection{Discussion on the effect of the interaction term} \label{sec:dis}

% ここでは、目的関数がconvex-concave関数な場合、ワーストシナリオのsensitivity ($x$と$y$の相互作用項$x^T B y$の係数行列$B$) の影響と提案アプローチ\wracma{}が必要とする$f$-callsの関係について考察する。テスト問題$f_5$--$f_8$において相互作用項$x^T B y$の係数$b=\diag{(B)}$を変化させて数値実験したところ、収束に要した$f$-callsの変化は鈍感であった。特に、テスト問題$f_5$ではおおよそ$O(\log{b})$であった (\Cref{fig:ex1_sensb})。この結果について厳密ではなくとも簡単な考察を述べる。
We discuss the effect of the worst scenario sensitivity (coefficient matrix $B$ of the interaction term $x^T B y$) on $f$-calls spent by the proposed approaches when the objective function is convex-concave.
\Cref{fig:ex1} shows that the numbers of $f$-calls were in $O(\log(b))$ or even in $O(1)$ in terms of the coefficient of the interaction term, $b$. We provide a brief, but not rigorous, explanation of these results.

For the sake of the simplicity, we focus on $f_5$ with $m = n$ and $B = \diag(b, \dots, b)$. 
The worst-case objective function is $F(x)=\frac{(1+b^2)}{2} \norm{x}_2^2$ and the worst scenario is $\yw(x) = b x$ in this case.
Moreover, we focus on \wracma{}. 

% ここでは、\wracma{}が消費する$f$-callsについて議論するために、問題と\cmaes{}について以下を仮定する。
% まず、対象とする問題は$m=n$で探査範囲に制約を設けない$f_5$に限定し、$x$と$y$の相互作用項$x^T B y$の影響は係数$b=\diag{(B)}$により制御されるとし、議論を進める。このため、ワーストケース関数は$F(x)=\frac{(1+b^2)}{2} \norm{x}_2^2$である。
% 次に、\cmaes{}は任意のconvex関数に対して線形収束できることを仮定する。つまり、最適解$x^*$周辺の$\epsilon$-solution $\{x : \norm{x-x^*} \leq \epsilon \cdot \norm{m^0 - x^*}\}$へ収束するために要する$f$-callsは$O(\log(\norm{m^0 - x^*}/\epsilon))$である。これについて、\cmaes{}に対する厳密なruntime analysisは行われていないものの、\cmaes{}の1種として知られている(1+1)-ESは、目的関数がLipschitz smooth and strongly convexな問題で、線形収束することが報告されている\cite{Morinaga2021}。
% このため、この仮定はそれほど大きく間違っていないと考えられる。
% これらの仮定のもと、min--max問題における外側ループで消費される$f$-callsと内側ループで消費される$f$-callsをそれぞれ評価し、min--max問題を解くために消費される全体の$f$-callsについて考える。

To proceed, we assume that CMA-ES converges linearly on such spherical functions. That is, a point in  $\{x : \norm{x-x^*} \leq \epsilon \cdot \norm{m_x^0 - x^*}\}$ around the optimal solution $x^*$ can be found in $O(\log(\norm{m_x^0 - x^*}/\epsilon))$ $f$-calls. Although no rigorous runtime analysis has been done for CMA-ES, we have its ample empirical evidences. Moreover, (1+1)-ES, which is a simplified version of CMA-ES, is proved to converge linearly on Lipschitz smooth and strongly convex objective functions \cite{Morinaga2021}. 


% 次に、内側の最大化問題$\max_{y \in \mathbb{Y}}f(x,y)$で消費される$f$-callsについて考える。
% \cmaes{}がワーストケース関数$F$について最適解探査を実施するためには、各解候補のランクを精度よく推定する必要がある。つまり、反復$t$において、$F(x_1) \lesseqgtr F(x_2)$である2つの解候補$x_1$と$x_2$については、高い確率で、$\hat{F}(x_1) \lesseqgtr \hat{F}(x_2)$となる$\hat{F}(x_1)$と$\hat{F}(x_2)$を推定する必要がある。このような$\hat{F}$を推定するためには$F(x_1)-\hat{F}(x_1)$と$F(x_2)-\hat{F}(x_2)$が$\abs{F(x_2)-F(x_1)}$以下となる精度まで最大化問題を解く必要がある。
% ワーストケース関数の推定精度$F(x)-\hat{F}(x)$は
% \begin{align}
    % F(x) - \hat{F}(x) 
    % &= f(x, \hat{y}(x)) - f(x, \tilde{y}) \nonumber \\
    % &= b x^\T(\hat{y}(x)- \tilde{y}) - \frac{1}{2}(\norm{\hat{y}(x)}^2- \norm{\tilde{y}}^2) \nonumber \\
    % &= \hat{y}(x)^\T(\hat{y}(x)- \tilde{y}) - \frac{1}{2}(\norm{\hat{y}(x)}^2- \norm{\tilde{y}}^2) \nonumber \\
    % &= \frac{1}{2}\norm{\hat{y}(x) - \tilde{y}}^2, \label{eq:Fprecise}
% \end{align}
% である。
% また、$F(x) \propto \norm{x}_2^2$であるため、外側ループのための\cmaes{}は$\Sigma^{t} \approx \sigma \cdot I$に適応すると考えられる。ここで、$\sigma$は$\sigma^2>0$となるスカラーである。これより、反復$t$のガウス分布$\mathcal{N}(m^t, \sigma^t)$から生成された2つの解候補$x_1$と$x_2$のワーストケース関数値の距離は
% \begin{align}
% \E[(F(x_1)-F(x_2))^2]^{1/2}=(1+b^2)(2 \Tr{((\Sigma^t)^2)})^{1/2} \approx (2m)^{1/2} (1+b^2) \sigma^2, \label{eq:Fdistance}
% \end{align}
% である。
% 先に述べた$F(x) - \hat{F}(x) \leq \E[(F(x_1)-F(x_2))^2]^{1/2}$に\eqref{eq:Fprecise}と\eqref{eq:Fdistance}を代入し、$\norm{\hat{y}(x) - \tilde{y}} \leq c \cdot (1 + b^2)^{1/2} \sigma$となる。ここで、$c$は正の定数である。このような$\tilde{y}$に到達するために要する$f$-callsは$O\left(\log \left(\frac{\norm{y^{(0)} - \hat{y}(x)}}{c \cdot (1 + b^2)^{1/2} \sigma}\right)\right)$である。ここで、$\hat{y}(x)=bx$で、$\tilde{y}^{(0)}$は前の反復のガウス分布$\mathcal{N}(m^{t-1}, \Sigma^{t-1})$より生成された解候補$x$に対するワーストシナリオ近傍のシナリオである。外側ループのための\cmaes{}のガウス分布が前の反復から大きく変化しないことを想定すると、$\hat{y}(x)$と$\tilde{y}^{(0)}$は$\mathcal{N}(bm, b^2\Sigma)$の分布に従うと考えられる。このとき、$\hat{y}(x)$と$\tilde{y}^{(0)}$の期待値は$\E[\hat{y}(x)-\tilde{y}^{(0)}] = b^2 \Tr{\Sigma}=m b^2 \sigma^2$である。これより、$\abs{y^{(0)}-\hat{y}(x)} \in O(b\sigma)$であると考えている。このことから、$\tilde{y}$に到達するために要する$f$-callsは$O\left(\log \left(\frac{b}{c \cdot (1 + b^2)^{1/2}}\right)\right)$である。

First, we consider how many iterations CMA-ES for the outer minimization spends to reach a point $m_x^{T_{\epsilon}}$ such $\norm{m_x^{T_\epsilon} - x^*} \leq \epsilon \norm{m_x^0 - x^*}$. Here, we call $T_\epsilon$ the runtime.
WRA returns approximate rankings of given solution candidates and they have a high correlation to the true rankings.
Then, CMA-ES is expected to behave similarly on these two rankings. 
Therefore, if CMA-ES converges linearly on $F$, we expect that CMA-ES converges linearly on the rankings given by WRA as well, which is partly supported by a theoretical investigation \cite{surrogatetheory}. 
Because the worst case objective $F(x)$ is a spherical function, CMA-ES is expected to converge linearly, i.e., the runtime $T_\epsilon$ is in $O(\log(\norm{m_x^0 - x^*} / \epsilon))$. Because CMA-ES executes a WRA every iteration, it is the number of WRA calls. 

Next, we consider how many $f$-calls \wracma{} spends in each call. 
Let the current search distribution of CMA-ES for the outer minimization be $\mathcal{N}(m_x^t, \Sigma_x^t)$. 
Because $F$ is a spherical function, it is expected that $\Sigma_x^t$ is proportional to the identity matrix $I_m$. 
Let us assume that $\Sigma_x^t \approx \sigma_t^2 I_m$.
Then, the solution candidates $x_1, \dots, x_{\lambda_x}$ given to WRA are independently $\mathcal{N}(m_x^t, \sigma_t^2 I_m)$ distributed.
For two solution candidates $x_i$ and $x_j$, the expected difference in worst case objective values is expected
\begin{align}
 \E[(F(x_i)-F(x_j))^2]^{1/2} 
 & =(1+b^2)\Tr((\Sigma_x^t)^2)^{1/2} \nonumber\\ 
& \approx m^{1/2} (1+b^2) \sigma_t^2. \label{eq:Fdistance}
\end{align}
% derivation:
% \begin{align}
%  \E[(F(x_1)-F(x_2))^2]^{1/2} 
%  & =\frac{(1+b^2)}{2}
%  \E[(\norm{x_1}^2 - \norm{x_2}^2)^2]^{1/2}\\
%  & =\frac{(1+b^2)}{2}
%  \E[\norm{x_1}^4 + \norm{x_2}^4 - 2\norm{x_1}^2\norm{x_2}^2]^{1/2}\\
%  & =\frac{(1+b^2)}{2}
%  (2 \E[\norm{x}^4] -2\E[\norm{x_1}^2]^2)^{1/2}\\
%  & =\frac{(1+b^2)}{2}
%  (2 (2 \Tr(\Sigma^2) + \Tr(\Sigma)^2) -2\Tr(\Sigma)^2)^{1/2}\\
%  & =\frac{(1+b^2)}{2}
%  (4 \Tr(\Sigma^2))^{1/2}\\
%  & =\frac{(1+b^2)}{2}
%  2\Tr(\Sigma^2)^{1/2}\\
%  & =(1+b^2)
%  \Tr(\Sigma^2)^{1/2}\\
% \end{align}
%


The early stopping strategy is expected to stop the maximization process once the rankings of given candidate solutions are well approximated in terms of Kendall's rank correlation. To have a high value of the Kendall's rank correlation, the order of $F(x_i)$ and $F(x_j)$ and their approximate values, $f(x_i, \tilde{y}_i)$ and $f(x_j, \tilde{y}_j)$ must be concordant with high probability for each pair $(x_i, x_j)$ among $\lambda_x$ solution candidates $x_1, \dots, x_{\lambda_x}$, where $\tilde{y}_i$ ($i=1,\dots,\lambda_x$) is the approximate worst scenario for $x_i$ obtained in WRA. 
It suffices to obtain $\tilde{y}_i$ and $\tilde{y}_j$ such that $\abs{F(x_i)-f(x_i, \tilde{y}_i)}$ and $\abs{F(x_j)-f(x_j, \tilde{y}_j)}$ are both less than $\abs{F(x_i)-F(x_j)} / 2$.
With a simple derivation we obtain
\begin{align}
    F(x) - f(x, \tilde{y}) 
    % &= f(x, \hat{y}(x)) - f(x, \tilde{y}) \nonumber \\
    % &= b x^\T(\hat{y}(x)- \tilde{y}) - \frac{1}{2}(\norm{\hat{y}(x)}^2- \norm{\tilde{y}}^2) \nonumber \\
    % &= \hat{y}(x)^\T(\hat{y}(x)- \tilde{y}) - \frac{1}{2}(\norm{\hat{y}(x)}^2- \norm{\tilde{y}}^2) \nonumber \\
    &= \frac{1}{2}\norm{\hat{y}(x) - \tilde{y}}^2. \label{eq:Fprecise}
\end{align}
That is, if $\norm{\hat{y}(x_i) - \tilde{y}_i} \leq c \cdot (1 + b^2)^{1/2} \sigma_t$ is satisfied for some $c > 0$, the true order of two points among $\lambda_x$ solution candidates will be correctly identified with high probability. 
Because the objective function of the inner maximization problem is spherical in $y$, to obtain such approximate worst scenarios, the required $f$-calls is $O\left(\log \left(\frac{\norm{y^{(0)} - \hat{y}(x_i)}}{c \cdot (1 + b^2)^{1/2} \sigma_t}\right)\right)$, where $\tilde{y}^{(0)}$ is the scenario selected as the initial scenario for $x_i$. 

Assuming that the Gaussian distribution of CMA-ES for the outer loop does not change significantly from the previous iteration, the worst scenario for the solution candidate in the current iteration, $\hat{y}(x)$, and that in the previous iteration are expected to follow the distribution $\mathcal{N}(bm_x^t, b^2\Sigma_x^t)$. 
Because the warm starting strategy selects the worst scenario among the set of scenarios including the ones obtained in the previous WRA call, the distance between $\yw{(x)}$ and $\tilde{y}$ is expected to be no greater than $\E[\norm{\hat{y}(x)-\tilde{y}^{(0)}}] = b^2 \Tr(\Sigma_x^t)=m b^2 \sigma_t^2$. From this, we estimate $\norm{y^{(0)}-\hat{y}(x)} \in O(b\sigma_t)$. As a result, $f$-calls required to approximate the worst-case function values for each solution candidate is $O\left(\log \left(\frac{b}{c \cdot (1 + b^2)^{1/2}}\right)\right)$.


% 最後に、min--max問題において、\wracma{}が$\epsilon$-solutionに収束するために消費される$f$-callsは
% \begin{equation}
    % O\left( \log \left(\frac{b}{c \cdot (1 + b^2)^{1/2}}\right) \cdot \log\left( \frac{\norm{m^{(0)} - x^*}}{\epsilon}\right)\right) , \label{eq:fcallorder}
% \end{equation}
% であると考えらる。\eqref{eq:fcallorder}中の第一項は、$b\leq 1$でおおよそ$\log{(b)}$となり、$b \to \infty$で定数となる。 

Altogether, the proposed approach is expected to locate a near optimal solution with 
\begin{equation}
    O\left( \lambda_y \log \left(\frac{b}{c \cdot (1 + b^2)^{1/2}}\right) \cdot \log\left( \frac{\norm{m_x^{(0)} - x^*}}{\epsilon}\right)\right)  \label{eq:fcallorder}
\end{equation}
$f$-calls.
It scales as $\log{(b)}$ for $b\leq 1$, and is constant for $b \to \infty$, which well estimates the behavior observed in \Cref{fig:ex1}.


\subsection{Experiment 2} \label{sec:ex2}


% To confirm that the proposed approaches overcome Difficulty (II), the experiments were conducted on $f_1$--$f_4$, $f_9$ and $f_{10}$ with $b=1$.


We applied four approaches to the problems $f_1$--$f_4$, $f_6$, $f_8$--$f_{10}$ to investigate their performance on the functions that are not smooth strongly convex concave. 


% \Cref{fig:ex2}に最適化結果を示す。\wracma{}は$f_4$での試行、\wraslsqp{}は$f_4$と$f_9$での試行以外のすべての試行において最適解探査が成功したことを確認した。一方、既存手法はどの試行においても最適解探査が失敗したことを確認した。これは、Difficulty (II)のためと考えられる。

% The optimization results are shown in \Cref{fig:ex2}. We confirmed that \wracma{} determined the optimal solution in all trials except for the trial at $f_4$ and \wraslsqp{} obtained successful convergence in all trials  except for the trial at $f_4$ and $f_9$. On the other hand, the existing approaches failed to find the optimal solution in all trials. This is due to Difficulty (II).

The results are shown in \Cref{fig:ex2}. We confirm that near optimal solutions were obtained by \wracma{} on $f_1$--$f_3$, $f_6$, $f_8$, and $f_9$, and by \wraslsqp{} on $f_1$--$f_3$, $f_6$, $f_8$, and $f_{10}$. On the other hand, the existing approaches failed to locate near optimal solutions in all trials.

We discuss the performance of the proposed approaches on the following each problem, non-smooth convex--concave functions ($f_3$, $f_6$, $f_8$), the functions of category (W) ($f_1$, $f_2$), the functions category (N) ($f_4$, $f_9$, $f_{10}$),
%smooth but not convex--concave function ($f_{10}$), 
categorized in \Cref{sec:limit}.

\subsubsection{Category S ($f_3$, $f_6$, $f_8$)}

% \ny=1でも解ける。
% f_3, f_6, f_8のようなNon-smooth convex--concave functionsでは、提案手法において\ny=1でも最適解に収束できると考えられる。提案手法では、良い初期値を使用して、各xについての目的関数を最大化し、ワーストケース関数での解候補のランキングRank_F(\{x_i\}_{i=1}^\lambda)を求める。f_3, f_6, f_8はいずれも、最適解でのワーストシナリオは唯一である。このため、良い初期値を一つ保存しておけば（\ny=1）、最適解のワーストシナリオを精度よく近似でき、結果的に最適解に収束できると考えられる。実際に\ny=1とした実験結果のうち、f_8での結果を\Cref{fig:ex2_f8}に示す。f_3とf_6でも同様の結果があったため、ここでは省略する。\Cref{fig:ex2_f8}が示す通り、\ny=1としても最適解に収束できていることがわかる。


% The proposed approach with $\ny=1$ will obtain the optimal solution at $f_3$, $f_6$, and $f_8$. 
The proposed approaches can solve $f_3$, $f_6$, and $f_8$ even with $\ny = 1$. 
\Cref{fig:ex2_f8} demonstrates the results of \wracma{} and \wraslsqp{} with $\ny=1$ on $f_8$.
% The proposed approach approximate the ranking on the worst-case function $Rank_F(\{x_i\}_{i=1}^\lambda)$ by approximately solving maximization problem for $y$ using good initial configuration. 
The worst scenario on $f_3$, $f_6$, and $f_8$ is a singleton $\abs{\ywset(x^*)}=1$ and a constant around the min--max solution $x^*$. Therefore, maintaining a single configuration ($\ny = 1$) was sufficient for the warm starting strategy in WRA to work efficiently on these problems. 
% As a result, the proposed approach will converge to the optimal solution. The numerical experiment results from \wracma{} and \wraslsqp{} with $\ny=1$ on $f_8$ are shown in \Cref{fig:ex2_f8}. As shown by \Cref{fig:ex2_f8}, even when $\ny=1$, the proposed approach could converge to the optimal solution. 
\Cref{fig:ex2_f8} shows \wracma{} and \wraslsqp{} with a smaller $\ny$ could converge to near global min--max solution with fewer f-calls. This may be because $f$-calls spent by warm starting strategy are saved by setting smaller $\ny$. Reduction of $f$-calls by a small $\ny$ was not significant, therefore, we do not consider $\ny$ should be daringly small. 

\subsubsection{Category W ($f_1$, $f_2$)}

% refreshなしでも解ける
% N_\omega=1では解けない。今回のyに関するテスト関数は比較的簡易なため、c_maxとN_\omega=1が小さくてもSLSQPが解くことができた。
% refreshの効果を、fig4(a)とfig6(a)との比較により議論する。

% Category (W)の目的関数f_1及びf_2では、\ny=1とした提案手法では最適解に収束できないと考えられる。f_1とf_2は最適解でのワーストシナリオが複数であり、保存した初期値が、ある解候補xにおける目的関数をyについて最大化する上で良い初期値とならない場合が考えられる。この場合、ワーストシナリオ\yset(x)を近似しづらくなり、Rank_F(\{x_i\}_{i=1}^\lambda)を近似できない場合が考えられる。このようなことが頻繁に発生すれば、最適解への収束が困難になると考えられる。実際に\ny=1とした実験結果のうち、f_1での結果を\Cref{fig:ex2_ny1}に示す。f_2でも同様の結果があったため、ここでは省略する。\Cref{fig:ex2_ny1}が示す通り、\ny=1とした\wracma{}は最適解に収束できていないことがわかる。一方で、\wraslsqp{}は最適解に収束できた。これは、f_1とf_2は、yに対する目的関数がslsqpにとって比較的簡易に最大化可能な関数であったためと考えられる。このため、良い初期を保存できていない場合でも、yset(x^*)を近似でき、Rank_F(\{x_i\}_{i=1}^\lambda)を近似できたため、最適解に収束できたと考えられる。
% また、\wraslsqp{}では、\Cref{fig:ex2}でのf_8での結果と比較すると少ないf-callsで最適解に収束できている。これは、\ny=1なため、warm starting strategyで消費されるf-callsを節約できたためと考えられる。

% At $f_1$ and $f_2$, the proposed approach with $\ny=1$ is not expected to converge to the optimal solution. The worst scenario $\ywset(x^*)$ at $f_1$ and $f_2$ is not unique. Therefore, the saved configuration  may not be good initial configuration to maximize the objective function on some generated solution candidates at the iteration. In this case, \wra will suffer from approximating the ranking $Rank_F(\{x_i\}_{i=1}^\lambda)$ and the outer \cmaes{} will not update the Gaussian distribution properly. If this inappropriate updates are in frequently, convergence to the optimal solution will be difficult. 
Maintaining multiple configurations, i.e., $\ny > 1$, is important for the proposed approach to successfully converge to the near global min--max solution $x^*$ on functions in Category (W) as we discussed in \Cref{sec:warmstart}. 
\Cref{fig:ex2_f1} shows the results of \wracma{} and \wraslsqp{} on $f_1$ with $\ny \in \{1,3,5,7, 12, 24, 36\}$. 
We confirmed that \wracma{} with a small $\ny$ failed to converge to $x^*$. On the other hand, \wraslsqp{} could converge to $x^*$ with $\ny = 1$. 
This may be because AGA can maximize $f_1$ for $y$ quickly from any starting point in $\mathbb{Y}$, and the warm starting strategy is unnecessary for \wraslsqp{} on $f_1$. 


% Category (W)の目的関数f_1及びf_2では、提案手法のrefreshを使用することで効率的になると考えられる。refreshの目的は、warm starting strategyにて使用されていないconfigurationを効率的に使用することである。refreshをオフにすると、warm starting strategyで使用されないconfigurationを保存しておくため、提案手法のパフォーマンスが低下すると考えられる。
% 実際に、p_n=0として、refreshをオフとした実験結果のうち、f_1での結果を\Cref{fig:ex2_pn0}に示す。f_2でも同様の結果があったため、ここでは省略する。\Cref{fig:ex2}中のf_1での結果と、\Cref{fig:ex2_pn0}を比較すると、refreshをオフとした\wracma{}のほうが収束に要するf-callsの幅が大きいことがわかる。つまり、refreshありのほうが安定して最適解に収束できていることがわかる。一方で、\wraslsqp{}にはそこまで大きな違いは見られない。これは、f_1とf_2は、yに対する目的関数がslsqpにとって比較的簡易に最大化可能な関数であったためと考えられる。このため、refreshなしでも、yset(x^*)を近似でき、Rank_F(\{x_i\}_{i=1}^\lambda)を近似できたため、最適解に収束できたと考えられる。

% The proposed approach is efficient on $f_1$ and $f_2$ because of refresh configurations. The purpose of refresh is to efficiently use configurations that are not used in warm starting strategy. If refresh is turned off, the performance of the proposed method will decrease due to saving configurations that are not used in the warm starting strategy.
% The numerical experiment results from the proposed approach with $p_n=0$ (to turn of refresh) are shown in \Cref{fig:ex2_pn0}. Comparing the results at $f_1$ in \Cref{fig:ex2}(a) and \Cref{fig:ex2_pn0}, the standard deviation of f-calls required from \wracma{} with refresh off until convergence was larger. In other words, the proposed approach with refresh convergence to the optimal solution with stable $f$-calls. On the other hand, in \wraslsqp{}, a large difference could not be seen. This may be because \slsqp{} could maximize the objective function  $f_1$ for y relatively easily. Therefore, even without refresh, \wra{} could approximate $ywset(x^*)$ and the rankings $Rank_F(\{x_i\}_{i=1}^\lambda)$.


\subsubsection{Category N ($f_4$, $f_9$, $f_{10}$)}

% 提案アプローチ\wracma{}と\wraslsqp{}の$f_4$における結果について考察する。
% ここで、ある解候補$x \in \mathbb{X}$での$y$に対する目的関数$h(y)=f(x,y)$について考える。テスト問題$f_4$の目的関数$h(y)$は、各次元の探査領域の境界上が局所解となる。そのため、$2^n$個の局所解が存在し、weak structureな多峰性関数であると考えられる。
% このような目的関数では、現状で提案されているどのアルゴリズムでも最適化することは困難である。そのため、提案手法は$F$を近似できず、$F$と大きくことなった関数について最適解を探査したため、$x^*$に収束できなかったと考えられる。
% 一方で、$2^n$個の局所解を含めるように\nyを十分に大きくすれば、いずれの場合でも、良い初期値からワーストシナリオを探査できるため、Rank_F(\{x_i\}_{i=1}^\lambda)を近似できると考えられる。実際に$m=n=5$で$b=1$とした$f_4$において、最適解を探査した結果を\Cref{fig:ex2_f4}に示す。\Cref{fig:ex2_f4}が示す通り、\wraslsqp{}は最適解に収束でき、\wracma{}でも一部の試行で最適解に収束できた。

% importance of hypothesis: f(x, y) can be solved efficiently for y.
% \textcolor{red}{
Multimodality in $y$, especially with a weak global structure, seems to make it difficult to obtain the global min--max solution. 
As we see in \Cref{fig:ex2} on $f_4$, \wracma{} and \wraslsqp{} could not obtain the successful convergence.
The objective function $f(x,\cdot)$ on $f_4$ has $2^n$ local solutions and is a multimodal function with a weak structure. Such an objective function is difficult to optimize efficiently with any of the currently proposed algorithms \cite{hansen2009}. Therefore, we consider that the proposed approaches failed to approximate the worst case function values $\{F(x)\}_{i=1}^{\lambda_x}$ at many iterations, as a result, the outer CMA-ES could not converge to $x^*$.
% }


% We consider the results of the proposed approach in $f_4$.
% Here, consider the objective function $f(x,\cdot)$ ($f(x,y)$ for $y$) at a given solution candidate $x \in \mathbb{X}$. The objective function $f(x,\cdot)$ on test problem $f_4$ has local solutions on the exploration boundary at each dimension. Therefore, the objective function $f(x,\cdot)$ has $2^n$ local solutions and it is considered to be a multimodal function with a weak structure.  
% because they searched the optimal solution for a function that is significantly different from $F$.

% On the other hand, if we set sufficient large $\ny$ to include $2^n$ local solutions, \wra{} will approximate the rankings $Rank_F(\{x_i\}_{i=1}^\lambda)$ because the solver explore the worst scenario using good initial configuration in any case. For example, at $f_4$ with $m=n=5$, $\ny=36$ is sufficient large because the number of the local solutions is $2^5=32$. The numerical experiment results from the proposed approach are shown in \Cref{fig:ex2_f4}. As shown by \Cref{fig:ex2_f4}, \wraslsqp{} converged to the optimal solution, and \wracma{} also converged to the optimal solution in some trials.

% On the other hand, if the dimension of the scenario vector is low, it is possible to find the optimal solution. This is because the proposed approaches are possible to  approximate the ranking on the worst-case function $\{F(x)\}_{i=1}^{\lambda_x}$ when the set of initial scenarios $\{y_k\}_{k=1}^{\ny}$ can include $2^n$ local solutions.
% The experiment results on $f_4$ with $m=n=5$ and $b=1$ are shown in \Cref{fig:ex2_f4}. As \Cref{fig:ex2_f4} shows, the proposed approaches, \wracma{} and\wraslsqp{}, could converge to the optimal solution.


% 提案アプローチ\wraslsqp{}の$f_9$における結果について考察する。
% テスト問題$f_9$は、$f_{10}$と同様に、$y$に対する目的関数$h(y)$がill scaleな問題であったため\slsqp{}では$\mathrm{Rank}_{F}(\{x_{i}\}_{i=1}^{\lambda})$を適切に近似できず、多くの試行で最適解収束に失敗したと考えられる。
% テスト問題$f_9$において、$y$に対する目的関数$h(y)$のヘッセ行列は$H_{(y,y)} = \diag{(\frac{\partial^2 h(y)}{\partial y^2_{1}}, \frac{\partial^2 h(y)}{\partial y^2_{2}}, \frac{\partial^2 h(y)}{\partial y^2_{3}},-2,\dots,-2)}$で、$\frac{\partial^2 h(y)}{\partial y^2} = 2 (\frac{\pi}{b_y})^2 e^{\sign{(y)}} (e^{\sign{(y)}} (\cos^2(\frac{\pi y}{b_y}) - \sin^2(\frac{\pi y}{b_y})) - x \sin(\frac{\pi y}{b_y}))$である。目的関数$h(y)$の2回偏微分$\frac{\partial^2 h(y)}{\partial y^2}$は、$x$と$y$によっては、小さな値となることも考えれ、ヘッセ行列$H_{(y,y)}$の条件数が大きくなることが考えられる。つまり、探査範囲において、目的関数$h(y)$がill scaleになる箇所が局所的に存在し、そのような箇所では\slsqp{}は適切にワーストシナリオを探査できなかったと考えられる。
% ただし、\Cref{fig:ex2}より、一部の試行で最適解に収束できている。これは、初期の解候補もしくはシナリオを変化させることで、ヘッセ行列$H_{(y,y)}$の条件数が大きくなる領域を避けて最適解を探査できたためと考えられる。

% \textcolor{red}{
% If $\ny$ is set greater than the number of local maxima in $f(x, \cdot)$, \wracma{} could locate the global min--max solution.
Setting $\ny$ greater than the number of local maxima in $f(x, \cdot)$ is important to obtain the successful convergence. 
As we see in \Cref{fig:ex2} on $f_9$, \wracma{} could obtain the successful convergence.
The objective function $f(x,\cdot)$ on $f_9$ has $8$ local maxima. When $\{y_k\}_{k=1}^{\ny}$ can include every local solutions because of $\ny=36>8$, the solver explore the worst scenario using good initial configuration in any case, that is, warm starting strategy works in effective.
On the other hand, \wraslsqp{} could not converge to $x^*$ in most of trials. AGA failed to even locally maximize $f$, possibly due to ill-conditioned, more precisely, the Hessian matrix is not necessarily negative definite at some $x$. As a result of approximating the worst case rankings $Rank_F(\{x_i\}_{i=1}^{\lambda_x})$ in many iterations, the outer CMA-ES failed to converge at $x^*$
Furthermore, on $f_4$, we confirmed the benefit of setting $\ny$ greater than the number of local maxima in $f(x, \cdot)$. In case of $n=5$, $\ny=36$ is more than the number of local maxima which is $2^5=32$.
\Cref{fig:ex2_f4} shows the experiment result from \wracma{} and \wraslsqp{} on $f_4$ with $m=n=5$. As shown by \Cref{fig:ex2_f4}, \wraslsqp{} obtained successful convergence, and \wracma{} also converged to near optimal solution.
% }


% On the other hand, we confirmed that if we set sufficient large $\ny$ to include $\abs{\ywset(x^*)}$ local solutions, the performance of \wracma{} and \wraslsqp{} on $f_4$ was significantly improved. For example,  For $f_4$ with $m=n=5$, $\ny=36$ is sufficient large because the number of the local solutions is $2^5=32$. Therefore,  
% We consider the result of \wraslsqp{} on $f_9$.
% \wraslsqp{} failed to converge at the optimal solution in many trials because, possibly, the objective function $f(x,\cdot)$ on $f_9$ was ill-scale, similar to $f_{11}$. As mentioned in \Cref{sec:ex1}, \wraslsqp{} failed the optimization on $f_{11}$ because the objective function $f(x,\cdot)$ was ill scale and \slsqp{} could not approximate the worst scenario on such function.
% In test problem $f_9$, the Hesse matrix of the objective function $f(x,\cdot)$ is $H_{(y,y)} = \diag{(\frac{\partial^2 f(x,\cdot)}{\partial y^2_{1}}, \frac{\partial^2 f(x,\cdot)}{\partial y^2_{2}}, \frac{\partial^2 f(x,\cdot)}{\partial y^2_{3}},-2,\dots,-2)}$, where $\frac{\partial^2 f(x,\cdot)}{\partial y^2} = 2 (\frac{\pi}{b_y})^2 e^{\sign{(y)}} (e^{\sign{(y)}} (\cos^2(\frac{\pi y}{b_y}) - \sin^2(\frac{\pi y}{b_y})) - x \sin(\frac{\pi y}{b_y}))$. The twice partial derivative $\frac{\partial^2 f(x,\cdot)}{\partial y^2}$ of the objective function $f(x,\cdot)$ can be small value depending on $x$ and $y$, and the condition number of the Hesse matrix $H_{(y,y)}$ can be large. In other words, the objective function $f(x,\cdot)$ becomes ill scale at specific local area. In such area, \slsqp{} could not approximate the worst scenario properly. 
% However, as shown in \Cref{fig:ex2}, \wraslsqp{} could converge to the optimal solution in some trials. This is possibly because the optimization could avoid the area where the condition number of the Hesse matrix $H_{(y,y)}$ becomes large by changing the initial solution candidates and scenarios.

% f10においては、\c_maxを大きく設定した\wracma{}を使用することで、精度高くワーストシナリオを近似でき、最適解に収束できる。f10において、ワーストケース関数を適切に近似するためには、比較的厳密にワーストシナリオを求める必要がある。これを示すために、n=m=1とし、yをS個サンプルし、各xでのyに対する最大値max_{s=1,\dots,S}f(x,y_s)を図にプロットした。ワーストケース関数と同じ最適解をもつ関数を構築するために大きなSが必要であれば、より厳密にワーストシナリオを求める必要がある関数となる。同図では、ワーストケース関数も描画し、Sの数に依存するワーストケース関数の近似関数の精度を評価する。図が示す通り、f5ではS=20でワーストケース関数と同じ最適解をもつ関数を構築できている。一方、f10ではS=50においても、ワーストケース関数と同じ最適解をもつ関数を構築できていない。
%ワーストシナリオをより厳密に求めるために$c_{\max}$を増加させることが考えらえる。f10において、$c_{\max} \in \{1,3,5,7,10,20\}$とした実験結果を図に示す。図が示す通り、$\c_{\max}=5$とすることで、\wracma{}はすべての試行で最適解へ収束できた。ただし、$\c_{\max}$を大きく設定すると必要な最適解収束に必要な$f$-callsも増加するため注意が必要である。
% \wraslsqp{}は、f10において、$\c_{\max}=1$としても最適解に収束できた。これは、\Cref{alg:SLSQPinwra}にとって、f10のyについての関数f(x,\dot)は比較的簡単であったため、小さい$\c_{\max}$でも精度高くワーストシナリオを求められ、最適解に収束できたと考えられる。

At $f_{10}$, by setting $c_{\max}$ at a large value, \wracma{} approximate the worst scenario with high accuracy and obtain the successful convergence. To approximate the worst case function properly for $f_{10}$, the worst scenario requires to be approximated with a relatively high accuracy. To demonstrate this, we draw the approximated worst case function $max_{s=1,\dots,S}f(x,y_s)$ with $n=m=1$ at \Cref{fig:ex2_landscape}, where $S$ is the number of samples for $y$. If a larger $S$ is required to construct a function which has the same optimal solution with the worst case function, the worst scenario needs to be approximated with higher accuracy. In the same figure, the worst-case function is also drawn to evaluate the accuracy of the approximated worst case function that depends on $S$. 
As \Cref{fig:ex2_landscape} shows, in case of $f_5$, a function which has the same optimal solution as the optimal solution at the worst case function was constructed at $S=20$. On the other hand, in case of $f_{10}$, a function that has the same optimal solution as the optimal solution at the worst case function was not constructed even at $S=50$.
In order to obtain the worst scenario with higher accuracy, setting $c_{\max}$ at a larger value can be one approach. \Cref{fig:ex2_f10} shows the experimental results with $c_{\max} \in \{1,3,5,7,10,20\}$ at $f_{10}$.
As the figure shows, by setting $c_{\max}=5$, \wracma{} was able to obtain the successful convergence in all trials, while \wracma{} with $c_{\max}=1$ failed. Although the worst case function can be approximated with high accuracy by setting $c_{\max}$ at a large value, the required $f$-calls until the convergence increases.
\wraslsqp{} was able to obtain the successful convergence at $f_{10}$ even with $c_{\max}=1$. This can be because the objective function for $y$ in $f_{10}$ was relatively easy for \Cref{alg:SLSQPinwra}, therefore the worst scenario could be approximated with high accuracy even for small $c_{\max}$.


% We discuss the results of the proposed approach \wracma{} in $f_{10}$. \wracma{} failed to converge to the optimal solution because it could not properly approximate the rankings $Rank_F(\{x_i\}_{i=1}^\lambda)$. To deal with this problem, one is to simply increase $c_{\max}$. We applied \wracma{} with $c_{\max}=5$ to $f_{10}$, and the result is shown in \Cref{fig:ex2_f10}. 
% As \Cref{fig:ex2_f10} shows, \wracma{} converged to the optimal solution. However, comparing with the results of \wraslsqp{} in \Cref{fig:ex2}, \wraslsqp{} with $c_{\max}=5$ required about $7$ times $f$-calls for the successful convergence. Therefore, $c_{\max}$ should be set in carefully.  



% % 総じて、提案アプローチ\wraslsqp{}と\wracma{}は、ワーストシナリオが不連続に変化する問題においても、最適解に収束できる点で既存手法よりも優位であることが示された。また、$y$に対する目的関数が多峰性関数であっても、シナリオ変数が低次元であれば最適解に収束できることが示された。

% Overall, the proposed approaches, \wraslsqp{} and \wracma{}, were shown to be superior to the existing approaches in term of that they can converge to the optimal solution even for problems the optimal solution $x^*$ is not a min--max saddle point.  
% In addition, they can converge to the optimal solution even for the problems where a solver such as \cmaes{} and \slsqp{} is difficult to approximate the worst-case function in regular setting because, for example, the objective function $f(x,\cdot)$ is a multimodal function, when initial scenario set $\{y_k\}_{k=1}^{\ny}$ can include all local solutions on $f(x,\cdot)$.


\begin{figure}[t]
\centering%
\begin{subfigure}{0.5\hsize}%
    \includegraphics[width=\hsize]{./figures/multirun_results_20ntry_fn1.pdf}%
  \caption{$f_1$}%  
\end{subfigure}%
\begin{subfigure}{0.5\hsize}%
    \includegraphics[width=\hsize]{./figures/multirun_results_20ntry_fn2.pdf}%
  \caption{$f_2$}%
\end{subfigure}%
\\
\begin{subfigure}{0.5\hsize}%
    \includegraphics[width=\hsize]{./figures/multirun_results_20ntry_fn3.pdf}%
  \caption{$f_3$}%  
\end{subfigure}%
\begin{subfigure}{0.5\hsize}%
    \includegraphics[width=\hsize]{./figures/multirun_results_20ntry_fn4.pdf}%
  \caption{$f_4$}%  
\end{subfigure}%
\\
\begin{subfigure}{0.5\hsize}%
    \includegraphics[width=\hsize]{./figures/multirun_results_20ntry_fn6.pdf}%
  \caption{$f_6$}%  
\end{subfigure}%
\begin{subfigure}{0.5\hsize}%
    \includegraphics[width=\hsize]{./figures/multirun_results_20ntry_fn8.pdf}%
  \caption{$f_8$}%  
\end{subfigure}%
\\
\begin{subfigure}{0.5\hsize}%
    \includegraphics[width=\hsize]{./figures/multirun_results_20ntry_fn9.pdf}%
  \caption{$f_9$}%  
\end{subfigure}%
\begin{subfigure}{0.5\hsize}
    \includegraphics[width=\hsize]{./figures/multirun_results_20ntry_fn10.pdf}%
  \caption{$f_{10}$}%  
\end{subfigure}
%\\
%\begin{subfigure}{\exs\hsize}
%    \includegraphics[width=\hsize]{./figures/multirun_results_20ntry_fn10v2.pdf}%
%  \caption{$f_{11}$}%  
%\end{subfigure}
\caption{Gap $\abs{F(m^t) - F(x^*)}$ with the number of $f$-calls at $b=1$ on $f_1$--$f_{4}$, $f_6$, and $f_{8}$--$f_{10}$.
Solid line: median (50 percentile) over 20 runs. Shaded area: interquartile range ($25$--$75$ percentile) over 20 runs.}%
\label{fig:ex2}
\end{figure}

\begin{figure}[t]
\centering
\begin{subfigure}{0.8\hsize}
    \includegraphics[width=\hsize]{./figures/sens_Ny_plot_fn8_ntry20.pdf}%
%   \caption{$f_8$}%  
\end{subfigure}
\caption{Average and standard deviation of the number of $f$-calls spent by \wracma{} and \wraslsqp{} on $f_8$ with $\ny \in \{1,3,5,7,12,24,36\}$.
Solid line: median (50 percentile) over 20 runs. Shaded area: interquartile range ($25$--$75$ percentile) over 20 runs.}%
\label{fig:ex2_f8}
\end{figure}

\begin{figure}[t]
\centering
\begin{subfigure}{0.8\hsize}
    \includegraphics[width=\hsize]{./figures/sens_Ny_plot_fn1_ntry20.pdf}%
%   \caption{$f_8$}%  
\end{subfigure}
\caption{Average and standard deviation of the number of $f$-calls spent by \wracma{} and \wraslsqp{} on $f_1$ with $\ny \in \{1,3,5,7,12,24,36\}$.
Solid line: median (50 percentile) over 20 runs. Shaded area: interquartile range ($25$--$75$ percentile) over 20 runs.}%
\label{fig:ex2_f1}
\end{figure}

\begin{figure}[t]
\centering
\begin{subfigure}{0.8\hsize}
    \includegraphics[width=\hsize]{./figures/multirun_results_20ntry_fn4_5D.pdf}%
%   \caption{$f_8$}%  
\end{subfigure}
\caption{Gap $\abs{F(m^t) - F(x^*)}$ with the number of $f$-calls on $f_4$  when $m=n=5$. Solid line: median (50 percentile) over 20 runs. Shaded area: interquartile range ($25$--$75$ percentile) over 20 runs.}%
\label{fig:ex2_f4}
\end{figure}


\begin{figure}[t]
\centering%
\begin{subfigure}{\exss\hsize}%
\centering%
    \includegraphics[width=\hsize]{figures/Landscape_1D_fn5.pdf}%
    \caption{$f_{5}$}%
\end{subfigure}%
\begin{subfigure}{\exss\hsize}%
\centering%
    \includegraphics[width=\hsize]{figures/Landscape_1D_fn10.pdf}%
    \caption{$f_{10}$}%
\end{subfigure}%
\caption{Landscape of the approximated worst case function $\max_{s=1,\dots,S} f(x,y_s)$ with $S \in \{10, 20, 50, 100 \}$ and landscape of the worst case function ($S=\infty$).}
\label{fig:ex2_landscape}
\end{figure}


\begin{figure}[t]
\centering
\begin{subfigure}{0.8\hsize}
    \includegraphics[width=\hsize]{./figures/sens_cmax_plot_fn10_ntry20.pdf}%
%   \caption{$f_8$}%  
\end{subfigure}
\caption{Average and standard deviation of the number of $f$-calls spent by \wracma{} and \wraslsqp{} on $f_{10}$ with $c_{\max} \in \{1,3,5,7,10,20\}$.
Solid line: median (50 percentile) over 20 runs. Shaded area: interquartile range ($25$--$75$ percentile) over 20 runs.}%
\label{fig:ex2_f10}
\end{figure}



% \begin{figure}[t]
% \centering
% \begin{subfigure}{0.5\hsize}%
    % \includegraphics[width=\hsize]{./figures/multirun_results_20ntry_fn4_5D.pdf}%
% \end{subfigure}%
% \begin{subfigure}{0.3\hsize}%
    % \includegraphics[width=\hsize]{./figures/legend.pdf}%
% \end{subfigure}%
% \caption{Gap $\abs{F(m^t) - F(x^*)}$ with the number of $f$-calls on $f_4$  when $m=n=5$.
% Solid line: median (50 percentile) over 20 runs. Shaded area: interquartile range ($25$--$75$ percentile) over 20 runs.}%
% \label{fig:ex2_f4}
% \end{figure}



% \begin{figure}[t]
% \centering%
% \begin{subfigure}{\exss\hsize}%
% \centering%
    % \includegraphics[width=\hsize]{./figures/multirun_results_20ntry_fn10_cmax5.pdf}%
    % \caption{$c_{\max}=5$, $\ny=36$, add \wraslsqp{} result}%
% \end{subfigure}%
% \begin{subfigure}{\exss\hsize}%
% \centering%
    % \includegraphics[width=\hsize]{./figures/multirun_results_20ntry_fn10-sno.pdf}%
    % \caption{$c_{\max}=5$, $\ny=1$}%
% \end{subfigure}%
% \caption{Gap $\abs{F(m^t) - F(x^*)}$ with the number of $f$-calls at $b=1$ on $f_{10}$.
% Solid line: median (50 percentile) over 20 runs. Shaded area: interquartile range ($25$--$75$ percentile) over 20 runs.}
% \label{fig:ex2_f10}
% \end{figure}


% \begin{figure}[t]
% \centering%
% \begin{subfigure}{\exss\hsize}%
% \centering%
    % \includegraphics[width=0.5\hsize]{./figures/multirun_results_20ntry_fn10_cmax5.pdf}%
    % \caption{$c_{\max}=5$, $\ny=36$, add \wraslsqp{} result}%
% \end{subfigure}%
% \begin{subfigure}{\exss\hsize}%
% \centering%
    % \includegraphics[width=\hsize]{./figures/multirun_results_20ntry_fn10-sno.pdf}%
    % \caption{$c_{\max}=5$, $\ny=1$}%
% \end{subfigure}%
% \caption{Gap $\abs{F(m^t) - F(x^*)}$ with the number of $f$-calls at $b=1$ on $f_{10}$ when $c_{\max}=5$.
% Solid line: median (50 percentile) over 20 runs. Shaded area: interquartile range ($25$--$75$ percentile) over 20 runs.}
% \label{fig:ex2_f10}
% \end{figure}


%\clearpage

\section{Application to robust berthing control} \label{sec:berthing}

We confirm the effectiveness of \wracma{}, \wraslsqp{}, \wracmaadv{}, and \wraslsqpadv{} on robust berthing control tasks proposed in \cite{akimoto2022berthing}. 

\subsection{Problem description}

We exactly follow the problem setup in \cite{akimoto2022berthing}. 
Here we briefly describe the problem definition.
For details, we refer to \cite{akimoto2022berthing} and the references therein.

The objective of this problem is to obtain a controller to control a ship to a target state located near a berth while avoiding collision to the berth. 
% The subject ship is a 3 m model ship representing the vessel MV ESSO OSAKA. 
The state of the ship is represented by $s \in \R^6$ and the control signal is represented by $a \in U \subset \R^4$.
% The state of the ship is represented by $s = (X, u, Y, v_m, \phi, r) \in \R^6$, where $X$ [m] and $Y$ [m] are the position of the ship in the earth fixed coordinate system, $u$ [m/s] and $v_m$ [m/s] are the longitudinal and lateral velocities at the mid-ship, $\phi$ [rad] is the yaw angle from the $X$ coordinate, and $r$ [rad/s] is its angular velocity.
% The possible control is represented by $a = (\delta, n_p, n_\mathrm{BT}, n_\mathrm{ST}) \in U$, where $\delta \in [-(35/180)\pi, (35/180)\pi]$ [rad] is the rudder angle, $n_p \in [-20, 20]$ [rps], $n_\mathrm{BT} \in [-20, 20]$ [rps], and $n_\mathrm{ST} \in [-20, 20]$ [rps] are the propeller, bow thruster, and stan thruster revolution numbers, respectively.
The state equation model is the MMG model used in \cite{Miyauchi2022}.
The feedback controller $u_x: \R^6 \to U$ is modeled by a neural network with $m = 99$ parameters. 
The domain of the network parameters is set to $\mathbb{X} = [-1, 1]^{m}$. 

We consider three types of uncertainty sets. 
Case A: We consider the wind condition as the uncertainty vector. 
The wind condition is parameterized by the wind direction in $[-\pi, \pi]$ [rad] and the wind velocity in $[0, 0.5]$ [m/s]. 
Case B: We consider the coefficients in the state equation regarding the wind force as the uncertain vector, which consists of a $10$ dimensional vector. 
Case C: We consider both uncertainties considered in Case A and Case B.
In all cases, the search domain is scaled to $\mathbb{Y} = [-1, 1]^{n}$.

The objective function $f(x, y)$ consists of two components. The first component measures the difference between the final state of the ship and the target state. The second component measures the penalty for collision to the berth. If a ship collides to the berth during the control period consisting of $200$ [s], it receives the penalty greater than $10$. Our objective is to minimize the worst case objective function $\max_{y \in \mathbb{Y}} f(x, y)$, where the uncertainty set $\mathbb{Y}$ differs for Case A, B, and C. 


\subsection{Experiment settings}

% Robust berthing control problemに提案手法、\wracma{}及び\wraslsqp{}、と既存手法、\zopgda{}及び\acma{}、を適用した。今回の問題では、\wracma{}と\wraslsqp{}については、min--max問題\eqref{eq:minmax}の外側ループの\cmaes{}にはDiagonal accelarationを使用した。これは、今回の問題のように、設計変数が高次元数な問題 ($m=99$) では、Diagonal accelarationを使用することで効率的に収束できることが報告されているためである\cite{akimoto2019}。各問題に対して初期乱数を変えた20回の独立試行を行い、各手法を比較する。今回の問題では、$f$-callsの上限は$2 \times 10^6$とした。

The proposed approaches and the existing approaches, \zopgda{} and \acma{}, were applied to the robust berthing control problem. 
For each problem, we run 20 independent trials with different random seed. 
The maximum number of $f$-calls was set to $2 \times 10^6$.


% 設計変数$x$を探査する\cmaes{}の初期平均ベクトルは$m^0 \sim \mathbb{X}$で、初期共分散行列は$\Sigma^0 = \diag(0.5,...,0.5)^2$とした。進化パス等のその他のパラメータ$\theta_x$は推奨パラメータ\cite{akimoto2019}を用いた。
All the approaches were configured in the same way as done in \Cref{sec:test}, where $u_x = 1$, $\ell_x = -1$, and $b_y = 1$ were plugged, except that we turned on the restart strategy of the proposed approaches and \acma{}, and the diagonal acceleration in CMA-ES for the outer minimization and in \wracma{} and \wraslsqp{}. 
% The configurations for \cmaes{} for the outer-minimization of the proposed approaches are as follows.
% The initial mean vector was $m^0 \sim \mathbb{X}$ and the initial covariance matrix was $\Sigma^0 = \diag(0.5,.... ,0.5)^2$. 
We set the number of configurations at $\ny=34 (=2 \times \lambda_x)$.
The termination thresholds were $V_{\min}^x=10^{-6}$ and $\Cond_{\max}^x=10^{14}$.
Additionally, we terminated the proposed approach if the best worst case function value were not significantly improved.
More precisely, we save $\tilde{F}_{\min}^t = \min_{i=1,\dots,\lambda}(\tilde{F}(x_i))$, where $\tilde{F}(x_i)$ is the approximated worst case objective value of $x_i$ computed in WRA, and terminated if $\max_{T=t-10,... ,t}(\tilde{F}_{\min}^t)-\min_{T=t-10,... ,t}(\tilde{F}_{\min}^t) < 0.1$ is satisfied\footnote{
We often observe that (1+1)-CMA-ES converges significantly faster than the standard CMA-ES (non-elitism CMA-ES) when optimizing a neural network. 
Probably because of this effect, \acma{} could perform many restart on this problem, whereas the proposed approaches could not perform any restart without this termination condition. For the proposed approach to be able to perform multiple restarts, we introduced this termination criterion at the risk of too early termination.
As a result, we confirmed that the proposed approaches performed the restart about 2, 4, and 2 times in Case A, B, and C, respectively.
}.


\begin{comment}
%
% \wra{}のパラメータは以下に示す通りで、基本的にはテスト問題での記載\Cref{sec:common}と同じとした : $\tau_{\mathrm{threshold}}=0.7$、$T_{\min}=10$、$\gamma=1/n$ ($c_{\max}=\gamma \times n = 1$)、$\ny=2 \times \lambda_x$、$\{p_k\}_{k=1}^{\ny} = 1$, $p_p=0.4$、$p_n=0.05$、$p_{\mathrm{threshold}}=0.1$。次に、\wracma{}中のソルバー\cmaes{}では、収束を判定するパラメータ$V_{\min}$を$10^{-4}$とし、初期平均ベクトルは$\{ m_k \}_{k=1}^{\ny} \sim \mathcal{U}(-1, 1)^n$で、初期共分散行列は$\{ \Sigma_k \}_{k=1}^{\ny} = \diag(0.5,...,0.5)^2$とした。
% 進化パス等のその他のパラメータ$\theta$は$\theta_x$と同様、推奨パラメータ\cite{akimoto2019}を用いた。
% \wraslsqp{}中のソルバー\slsqp{}では、初期シナリオは$\{y_k\}_{k=1}^{\ny} \sim \mathcal{U}(-1, 1)^n$、初期学習率は$\{\eta_{k}\}_{k=1}^{\ny}=1$、学習率を更新するパラメータ$\beta$は$ 0.5$、ソルバーの収束を判定するパラメータ$U_{\min}$を$10^{-5}$、とした。
% 既存手法\zopgda{}と\acma{}のパラメータは各々の既存研究 (各々\cite{Liu2020}、\cite{akimoto2022berthing}) を参照して設定した。
% \zopgda{}に対しては学習率を$\eta_x=0.02$及び$\eta_y=0.05$とした。また、\acma{}のリスタートに関わるパラメータ$G_{\mathrm{tol}}$は$10^{-6}$とした。
%
% The hyperparameters of \wra{} are shown as followings, which are basically the same as those described in \Cref{sec:common} : $\tau_{\mathrm{threshold}}=0.7$, $T_{\min}=10$, $\gamma=1/n$ ($c_{\max}=\gamma \times n = 1$), $\ny=2 \times \lambda_x$, $\{p_k\}_{k=1}^{\ny} = 1$, $p_p=0.4$, $p_n=0.05$, $ p_{\mathrm{threshold}}=0.1$. In addition, it was set $V_{\min}=10^{-4}$ for \cmaes{}, $\beta=0.5$ and $U_{\min}=10^{-5}$ for \wraslsqp{}. 
% Next, in solver\cmaes{} in\wracma{}, the initial mean vector is $\{ m_k \}_{k=1}^{\ny} \sim \mathcal{U}(-1, 1 )^n$ and the initial covariance matrix is $\{ \Sigma_k \}_{k=1}^{\ny} = \diag(0.5,.... ,0.5)^2$.
% Other parameters $\theta$, such as evolutionary paths, were set as the recommended parameters \cite{akimoto2019}.
% In solver\slsqp{} in\wraslsqp{}, the initial scenarios are $\{y_k\}_{k=1}^{\ny} \sim \mathcal{U}(-1, 1)^n$, the initial learning rate is $\{\eta_{k}\}_{k=1}^{\ny}=1$.
% The hyperparameters of the existing approaches, \zopgda{} and \acma{}, were set as described in \Cref{sec:common}, $\eta_x=0.02$ and $\eta_y=0.05$ for \zopgda{} and $G_{\mathrm{tol}} = 10^{-6}$ for \acma{}.
%
The hyperparameters for the proposed approaches were set as follows.
The hyperparameters were basically set to the same values used in the numerical experiments \Cref{sec:test}. 
We describe only hyperparameters which were different from those used in the numerical experiments.
The initial configurations for \wracma{} was as the follows.
The initial mean vector was $m_k \sim \mathcal{U}(\mathbb{Y})$ and the initial covariance matrix was $\Sigma_k = \diag(0.25,.... ,0.25)^2$ for all $k = 1,\dots,\ny$. 
The initial scenarios for \wraslsqp{} was $y_k \sim \mathcal{U}(\mathbb{Y})$.


For the existing approaches, \zopgda{} and \acma{}, we used different initial settings with those in the numerical experiments \Cref{sec:test}.
For both \zopgda{} and \acma{}, the initial the design vector and the scenario vector were generated from the search domain $x^0 \sim \mathcal{U}(\mathbb{X})$ and $y^0 \sim \mathcal{U}(\mathbb{Y})$.
\end{comment}

% リスタートのための収束判定では、設計変数$x$を探査する\cmaes{}の探査範囲を使用する。例えば、標準偏差の最大値$\max_l \sqrt{\Sigma_{l,l}}$が$10^{-4}$以下になったときや共分散行列の条件数$Cond(\Sigma) $が$10^{6}$以上となったときである。


% The convergence criteria for the restart strategy is followings. One is, \cmaes{} for exploring $x$ is evaluated as convergence when its search distribution becomes small. In this study, when all coordinate wise standard deviation $\max_l \sqrt{\Sigma_{l,l}}$ becomes less than $10^{-4}$ or the condition number of the covariance matrix $Cond(\Sigma) $ becomes greater than $10^{6}$, \cmaes{} is regarded as the convergence. 

% 先に説明した終了条件に加えて、$\min(F^z(x)))$を使用してリスタートの判定を行う。今回、各反復で$\mathbb{H}^t=\min(F^z(x))$を保存しておき、ある反復間（$T=t-a \sim t$）の中で、$f_{gap} = \max_{T=t-a,...,t}(\mathbb{H}^T)-\min_{T=t-a,...,t}(\mathbb{H}^T)$が一定値以下になれば収束したと判定する。これは、$\min(F^z(x)))$の改善が望めず、解候補の大幅な改善が望めないと考えられるためである。
% 今回は、$a=10$で$f_{gap}$が$0.1$以下となったら収束したと判定する。
%提案手法は収束までにf-callを多く消費する傾向にある。そこで，（精度をすこし犠牲にして，）リスタートするために、収束条件を$f_{gap}<0.1$とした．
% この結果，CaseAで約2回、CaseBで約4回、CaseCで約2回程度のリスタートができたことを確認している．






% 各4つの手法の最終的な最適解の出力について説明する。最適解探査が終了したと判定されたときもしくは$f$-callsが$2 \times 10^6$に到達したときにその反復での解候補$x$を$A^x$に、シナリオ$y$を$A^y$に保存しておく。そして、解探査終了後に、解$x=\argmin_{x \in A^x} \max_{y \in A^y} f(x,y)$を最適解として出力する。


% 実際の問題では、解候補毎のワーストケース関数値$F(x)$は与えられないため、既存の研究\cite{akimoto2022berthing}と同様に、得られた最適解を以下の通りに評価する。得られた最適解についてワーストシナリオを探査し、近似したワーストケース関数値で最適解を評価する。具体的には、(1+1)-CMA-ESを用いて$500 \times n$反復を繰り返し、$F(x)$を近似する。シナリオ$y$に関する目的関数が多峰であると考えられるため、このようなワーストシナリオ探査を初期シナリオを変化させて$100$回行った。このときの(1+1)-CMA-ESの初期設定や収束条件は既存の研究\cite{akimoto2022berthing}と同じである。



For each trial, the obtained solution was evaluated on the worst-case objective value in the same way as the previous study \cite{akimoto2022berthing}. 
To estimate the worst-case objective value for each solution, we run the (1+1)-CMA-ES to approximate $\max_{y \in \mathbb{Y}} f(x, y)$ with $100$ different initial points. Then, by taking the maximum of the obtained worst scenario candidates, $y_1, \dots, y_{100}$, the worst case objective value is evaluated. For the configuration of the (1+1)-CMA-ES, we exactly followed the previous study \cite{akimoto2022berthing}.

\subsection{Result and evaluation}

% 最適解探査を20回独立に試行した各手法毎の結果を\Cref{fig:berthing_result}にプロットした。

The worst case performances of the solutions obtained in the experiments were summarized in \Cref{fig:berthing_result}.  

% まず、既存手法の結果について考察する。\zopgda{}は、Case A, B, and Cにおいて、20回の試行の中で得られた解のほとんどが座礁する結果となった。
% また、\acma{}は、Case Bではほとんどの試行で桟橋に座礁せずに、目的関数値を小さくできる解を求めることができた。一方、Case AとCでは、得られた多くの最適解で桟橋に座礁する結果となった。これは、Case AとCでは風向が不確実な問題であり、\Cref{sec:intro}で述べた通り、局所解周辺のワーストシナリオが不連続に変化する問題であったためと考えられる。

\emph{Results of \acma{} and \zopgda}.
\acma{} could find the robust solutions in all but one trials in Case B. 
On the other hand, the median of the worst case performances in Cases A and C were greater than $10$, indicating collision to the berth. 
These results are in agreement with previous study \cite{akimoto2022berthing}.
\zopgda{} failed to obtain solutions that can avoid collision to the berth in the worst case in most trials in all cases.



% 次に、提案アプローチ\wracma{}と\wraslsqp{}の結果について述べる。両手法とも、いずれのCaseにおいても、桟橋に座礁せずに、安定して着桟できる解を求めることができた。Case Cにおいて\wraslsqp{}の1試行の結果のみ座礁する解となったが、それ以外のすべての結果は安全に着桟できる解であったことを確認している。ただし、Case Bにおいては、\acma{}のほうが優れた結果が得られている。この点に関して、\wra{}の収束の条件を厳しくすることで得られる解の精度をあげることが考えられるが、\wra{}の消費される$f$-callsは大きくなる。\wra{}の$f$-callsを減少できる手法については今後の課題である。

\emph{Results of \wracma{} and \wraslsqp}.
Except for a trial of \wraslsqp{} in Case C, \wracma{} and \wraslsqp{} could find the controllers that can avoid collision to the berth in the worst scenarios. 
As we discussed in \Cref{sec:intro}, we hypothesize that the problems in Cases A and C are such that the worst scenario around the optimal controller on $F$ changes discontinuously, and they are difficult for \acma{} and \zopgda{}.
We consider this is one of the reasons for the superior worst case performances of \wracma{} and \wraslsqp{} in Cases A and C.
On the other hand, the worst case performances of \wracma{} and \wraslsqp{} were significantly worse than those of \acma{} in Case B. 
One reason of this result is the termination criterion introduced in this experiment. It prevents performing an intensive local search. At the same time, we confirmed that the worst case performances of \wracma{} and \wraslsqp{} were inferior to those of \acma{} even without this termination condition. This is probably due to the slower convergence of \cmaes{} than (1+1)-CMA-ES on this problem.
%The outputted solution from only one trial of \wraslsqp{} collided with the berth. In Case B, the results from \acma{} were superior. 
% One approach to obtain better solution from \wra{} is to tighten the convergence conditions, however, $f$-calls is required to restart \wra{}.
% Improvement of \wra{} for $f$-calls reduction is involved in future works. 

% 最後に、提案アプローチ\wracmaadv{}と\wraslsqpadv{}の結果について述べる。図より、\wracma{}と\wraslsqp{}の20試行中の最小の目的関数値よりも小さな目的関数値が得られる解を求めることができている。一方で、桟橋に座礁する解が多くなっている。これは、\wra{}の収束時に得られたシナリオ$\{y_k^{\mathrm{converge}}\}_{k=1}^{\ny}$が局所解周辺のワーストシナリオと大きく異なっていたことが原因であると考えられる。つまり、\acma{}はワーストケース関数と異なる関数について最適解を探査したため、得られた解について厳密にワーストシナリオを探査した結果、ワーストシナリオでは桟橋に座礁する解であったということである。
% この点に関して、\wra{}の収束の条件を厳しくし、シナリオ$\{y_k^{\mathrm{converge}}\}_{k=1}^{\ny}$の精度を上げることが考えられるが、先に述べたように、\wra{}の消費される$f$-callsは大きくなる。並列計算\Cref{sec:parallel}や$f$-callsを減少できる手法の開発等、実用的な\wra{}の開発については今後の課題である。

\emph{Results of \wracmaadv{} and \wraslsqpadv}.
In Case B, \wracmaadv{} and \wraslsqp{} were able to obtain better worst case performances in many trials. From these results, we confirm that the motivation of running \acma{} after \wracma{} and \wraslsqp{}, namely, improving the exploitation ability, was realized in Case B. 
On the other hand, the worst case performances exhibited more variance in Case A, and their median were significantly degraded in case C. 
The negative effect of running \acma{} after \wracma{} and \wraslsqp{} may be explained as follows.
The set $\bar{Y}$ of worst scenario candidates given to \acma{} is expected to approximate the worst scenario set $\ywset(\bar{x})$ of a given solution candidate $\bar{x}$ as a subset of $\bar{Y}$. 
During \acma{}, $\bar{Y}$ is fixed and the solution candidate $x$ is optimized under $\bar{Y}$ and a newly added scenario candidate $y_\mathrm{adv}$. 
Because $\ywset(x)$ may change as $x$ changes, $\bar{Y}$ may not approximate $\ywset(x)$ well after \acma{} and a single scenario candidate $y_\mathrm{adv}$ may not be sufficient to recover $\ywset(x)$. 
Then, the solution obtained by $\acma{}$ may be overfitting to $\bar{Y}$, and there may exist scenarios where the performance is worse. 

% From \Cref{fig:berthing_result}, some trials of the results of the proposed approaches, \wracmaadv{} and \wraslsqpadv{}, could obtain better solutions than the best controllers resulted from the $20$ trials of \wracma{} and \wraslsqp{}.
% On the other hand, other trials collided with the berth. 
% This was resulted from that the scenarios $\{y_k^{\mathrm{converge}}\}_{k=1}^{\ny}$ obtained at the convergence of \wra{} was significantly different from the worst scenario around the local solution. 
% In other words, \acma{} explored the local solution for a function different from the worst-case function, therefore, the outputted solution $x_{\mathrm{out}}$ was the controller colliding with the berth on the approximated worst scenario.
% To improve this result, one is to tighten the convergence conditions for \wra{} to approximate the scenario $\{y_k^{\mathrm{converge}}\}_{k=1}^{\ny}$ to the worst scenario around the local solution, however as mentioned earlier, the required $f$- calls will be larger. 



% 提案アプローチは並列計算の導入により高速化が可能である\Cref{sec:parallel}。

% Practically, parallel computation is one way to obtain better solution with many $f$-calls but less computation time. 
% Proper parallel computation can accelerate the optimization of \wracma{} by factor $\lambda \times \lambda_y$ as described in \Cref{sec:parallel}, where $\lambda_y$ is the population size in solver \cmaes{}. On the other hand, parallel computing is not effective for the existing approaches due to the alternative update. Adaptability of the parallel computing for the proposed approaches should be addressed in future works. 


% \Cref{fig:berthing-trajectory-adv}に\wracma{}と\wracmaadv{}の結果の中で、1つの最適解を用いた船舶の着桟の軌道を示す。どちらの解も安定して着桟できていることがわかる。風向に対して船舶の軌道が大きく変わらない点からも、風の不確実性に対してロバストなコントローラーが得られていることがわかる。


\Cref{fig:berthing-trajectory-wra} shows the ship trajectory observed under the best controllers obtained by \wracma{} and \wracmaadv{}. 
Under both controllers, collision is successfully avoided under wind from an arbitrary direction. 
% The performance of the controller obtained by \wracma{} is more uniform with respect to the wind direction than that obtained by \wracmaadv{}, where the best case performance is one order better than the worst case performance. 

% 今回の結果より、Case A, B, Cのすべてにおいて、提案アプローチは、座礁せずに安全に着岸できる解を安定して見つけ出せた点で既存手法よりも有効な手法であることが示された。


% Overall, the results in robust berthing control problems show that the proposed approaches are more effective than the existing approaches in terms of that they can stably find a robust solution to wind uncertainty.



\begin{figure}[t]
  \centering
  \begin{subfigure}{\hsize}
  \centering
    \includegraphics[height=\hsize, angle=-90, clip, trim=5 10 10 0]{figures/berthing_A_boxplot.pdf}%
    \caption{A}
    \end{subfigure}
    \\
    \begin{subfigure}{\hsize}
    \centering
    \includegraphics[height=\hsize, angle=-90, clip, trim=5 10 10 0]{figures/berthing_B_boxplot.pdf}%
    \caption{B}
    \end{subfigure}    
    \\
    \begin{subfigure}{\hsize}
    \centering
    \includegraphics[height=\hsize, angle=-90, clip, trim=5 10 10 0]{figures/berthing_C_boxplot.pdf}%
    \caption{C}
    \end{subfigure}
  \caption{Performance of the solution obtained in 20 independent trials of \zopgda{}, \acma{}, \wraslsqp{}, \wracmaadv{}, and \wraslsqpadv{} on Case A, B, and C. Side edge on each box indicates the lower quartile Q1 and the upper quartile Q3, and middle line in each box indicates the median. The lower and upper whiskers are the lowest datum above Q1-1.5(Q3-Q1) and the highest datum below Q3+1.5(Q3-Q1).}
\label{fig:berthing_result}
\end{figure}

\begin{figure*}
    \centering
    \begin{subfigure}{0.5\hsize}%
    \includegraphics[width=\hsize]{trajectories/wra(ddcma).pdf}%
    \caption{\wracma{}}%
    \end{subfigure}%
    \begin{subfigure}{0.5\hsize}%
    \includegraphics[width=\hsize]{trajectories/wra(ddcma)+adv-cma.pdf}%
    \caption{\wracmaadv{}}%
    \end{subfigure}%
    \caption{Visualization of the trajectories obtained by the controllers for the worst wind condition with the maximum wind velocity of 0.5 [m/s]. The best controller obtained by (a) \wracma{}, and (b) \wracmaadv{} are displayed. See the caption of \Cref{fig:berthing-trajectory-adv} for details of the figures.}
    \label{fig:berthing-trajectory-wra}
\end{figure*}




\section{Conclusion}

To address the limitation of existing approaches for black-box min--max optimization problems, \zopgda{} and \acma{}, we propose a novel approach which aim to minimizes the worst case function by CMA-ES, while the ranking of the worst case objective function values of the solution candidates are approximated by the WRA mechanism, 
% where the ranking of the worst case objective function values of the solution candidates are approximated by the WRA mechanism and CMA-ES tries to minimize the the worst case objective function by using WRA mechanism. 
To save $f$-calls inside the WRA mechanism, we implement a warm starting strategy and an early stopping strategy. We developed WRA-CMA and WRA-AGA by combining the WRA mechanism with CMA-ES and AGA, respectively. 
% The WRA mechanism is implemented with CMA-ES and AGA. 
A restart strategy and a hybridization of the proposed approach and \acma{} are implemented for practical use. The proposed approaches were evaluated on $11$ test problems and three variants of robust berthing control problems.


Important findings from the numerical experiments are as follows.
On smooth strictly convex--concave problems, where \zopgda{} and \acma{} have been analysed for their convergence, the proposed approaches exhibited slower convergence than the existing approaches when the interaction between $x$ and $y$ are relatively weak. However, the number of $f$-calls were not increased significantly for the proposed approach when the interaction became stronger, whereas they were increased significantly for the existing approaches.
On non-smooth strictly convex--concave problems and problems where the global min--max solution is not a strict global min--max saddle point, \zopgda{} and \acma{} failed to converge, whereas the proposed approach converges. For the former, the proposed approach could locate the global min--max solution even with $\ny = 1$, while a sufficiently large $\ny$ was a key to the success of the proposed approach. When the worst case function needed to be approximated with high accuracy, setting $c_{\max}$ at a large value was effective. 

%アプリケーションでの利点を考える%
%  提案アプローチの適用効果を3種類のRobust berthing controlの問題において評価した。風の向きがシナリオパラメータに含まれている問題では、WRA-CMAとWRA-AGAは、ほぼすべての試行で、ワーストケースでも座礁せずに安定して着桟できる解を求めることができた。点で既存手法よりも優位であることが示された。一方で、既存手法により得られた解は、多くの場合、ワーストケースで座礁する結果となった。風の向きがシナリオパラメータに含まれている問題は、ワーストケース関数Fの局所解周辺では、ワーストシナリオが不連続に分布すると考えられる。このような問題は、既存手法が不得意な問題である。一方で、提案手法は、ワーストケース関数Fの局所解周辺でワーストシナリオが不連続に分布する問題にも対処できるため、既存手法よりも優位な結果が得られたと考えられる。
% また、多くの試行で、WRA-CMA及びWRA-AGAの後に\acma{}をランさせることで、より良い結果が得られることが示された。

The effectiveness of the proposed approaches, WRA-CMA and WRA-AGA, were demonstrated in three different Robust berthing control problems. For problems where wind direction was included in $y$, WRA-CMA and WRA-AGA were able to find controllers that avoid collision to the berth in the worst situation, while controllers obtained by the existing approaches, \acma{} and \zopgda{}, often collided with the berth in the worst situation. 
At problems where wind direction is included in $y$, the worst scenario is expected to be changed discontinuously around the optimal controller on $F$, and they are difficult for the existing approaches. On the other hand, the proposed approaches can address such a difficulty. Therefore, we consider that controllers obtained by the proposed approaches were superior to them obtained by the existing approaches.
For the problem where the existing approaches obtained controllers that avoid collision, we confirmed the existing approaches found the better solution than the solutions obtained by the proposed approaches. In addition, for such a problem, many trials showed that better controllers were obtained by running \acma{} after WRA-CMA and WRA-AGA.


Besides the above advantages of the proposed approach, one practical advantage of the proposed approach over \zopgda{} and \acma{} is that it is parallel-implementation friendly.
In WRA, $\lambda_x$ solvers $\mathcal{M}(\omega_k)$ ($k = 1,\dots,\lambda_x$) can be run in parallel.
The $\lambda_x \ny$ evaluations of $f(x_i, y_k)$ at the beginning of WRA can be performed in parallel.
Moreover, if WRA-CMA is used, $\lambda_y$ $f$-calls at each iteration of $\mathcal{M}(\omega_k)$ can be performed in parallel.
In total, roughly $\lambda_x \lambda_y$ times speedup in terms of the wall clock time can be achieved ideally.
For example, in Case C of the robust berthing control problem, we have $m = 99$ and $n = 12$, hence $\lambda_x = 17$ and $\lambda_y = 11$, resulting in a possible speed-up of factor $187$. 
Each $f$ evaluation took about $0.1$ seconds in average, amounting to about 2.3 days for each trial.
If the ideal speed-up is achieved, the wall clock time reduces to about 18 minutes.
This compensates the disadvantage of the proposed approach over \acma{}: slower convergence.

The main limitation of this study is the lack of theoretical guarantee. 
For the WRA mechanism to work effectively, we assume that $\yw(x)$ is almost everywhere continuous and $\max_{y \in \mathbb{Y}}f(x, y)$ can be solved efficiently for each $x$. 
However, questions as how much $\yw$ can be sensitive or how efficiently the internal solver $\mathcal{M}$ should be able to solve $\max_{y \in \mathbb{Y}}f(x, y)$ are not answered formally in this paper. 
Such a theoretical investigation provides not only a guarantee of the performance of the approach, but also a seed to improve the proposed approach.
Therefore, theoretical investigations of the WRA mechanism is an important direction of the future work.

\textcolor{red}{comment on hyper-parameters (how to tune, etc.)}
% ex. $\ny \geq \abs{\ywset(x^*)}$ ?、c_{\max}
% どういうときに、どういうふうに設定すれば、いいかを記載する。（実験結果から）


The black-box min--max optimization lacks the de facto standard benchmarking testbed, covering problems with different characteristics. 
In this paper, we design $11$ test problems from the perspective of the characteristics of the global min--max solution (whether it is a strict min--max saddle point, a weak min--max saddle point, or not a min--max saddle point), and the perspective of the smoothness and the strong convexity of $f$. 
On the other hand, we limit our focus on problems where $f(x, y)$ has relatively simple characteristics with respect to $x$ and $y$ and difficulties considered in black-box optimization such as ruggedness, non-separability, and ill-conditioning are yet to be considered. For example, all the test problems are convex in $x$ except for $f_{10}$ and the effect of the multimodality in $x$ is not considered. The investigation of the effect of ill-conditioning of $f$ is limited to the comparison between $f_5$ and $f_{11}$.
Because approaches for black-box min--max optimization are designed and improved based on benchmarking, and theoretical analyses on black-box min--max optimization are rather limited, developing a benchmarking testbed is highly desired. 
This is also an important direction of the future work.


% % 本研究では、目的関数がblack boxなmin--max continuous optimization problemsについて検討した。さらに、ワーストケース関数の局所解周辺において、設計変数の変化に対するワーストシナリオの変化に着目し、以下の2つの状況を想定した。
% % Situation (a) : ワーストシナリオの変化は連続であるが非常にsensitive
% % Situation (b) : ワーストシナリオの変化は不連続
% % 以下に本研究で得られた知見をまとめる。

% In this study, we considered min--max continuous optimization problems where the objective function is black--box. This study focused on the problems with following situations, 
% This study focused on the problems with following situations. 
% Situation (a) : the optimal solution $x^*$ on the worst-case function $F$ forms a global min--max saddle point of $f$ and the worst scenario around $x^*$ is sensitive with changing the design vector.
% Situation (b) : $x^*$ does not form a global min--max saddle point of $f$.  

% Our contributions are as followings. 
% \begin{enumerate}
%     % \item Situation (a)と(b)に対処するために、新しいアプローチを提案した。提案アプローチは、近似されたワーストケース関数に対してthe Covariance Matrix Adaptation Evolution Strategy (\cmaes{}) により最適解を探査する。\cmaes{}がワーストケース関数に対して解を探査しているように振る舞えるように、生成された解候補のワーストケース関数でのランクを推定するWorst-case Ranking Approximation (\wra{}) を提案した。\wra{}では、min--max問題の内側の最大化問題$\max_{y \in \mathbb{Y}}f(x,y)$を近似的に解く。この最大化問題に対するソルバーとして\cmaes{}を実装したアルゴリズム\wracma{}と\slsqp{}を実装したアルゴリズム\wraslsqp{}を紹介した。
    
%     \item To address Situation (a) and (b), a novel approach is proposed. The proposed approach aim to explore the optimal solution for the worst-case function by using the Covariance Matrix Adaptation Evolution Strategy (\cmaes{}), and the rankings of the solution candidates generated by \cmaes{} are approximated by Worst-case Ranking Approximation (\wra{}) mechanism in each iteration. In \wra{}, the worst-case function value for each solution candidate $x$ is estimated by approximately solving the maximization problem $\max_{y \in \mathbb{Y}}f(x,y)$ inside the min--max problem. We introduced the proposed algorithms \wracma{} and \wraslsqp{}, which  combine \wra{} with respectively \cmaes{} and \slsqp{} as a solver for the internal maximization problem. 
    
%     % \item 数値実験では、Situation (a)を模擬したテスト問題に、既存手法である\zopgda{}と\acma{}と提案アプローチ\wracma{}と\wraslsqp{}を適用し、比較を行った。その結果、提案アプローチのみがSituation (a)に対処できることが示された。ただし、ワーストシナリオの変化が連続で鈍感な問題では、既存手法が少ない$f$-callsで収束できる点で優位であることが示された。

%     \item We conducted numerical experiments on test problems containing situation (a) to compare the proposed approaches, \wracma{} and \wraslsqp{}, with the existing approaches, \zopgda{} and \acma{}. The results showed the efficiency of  the proposed approaches over the existing approaches. 
%     %However, for problems where the worst scenario changes in continuous and insensitive, the existing approaches were superior in terms of that they converged with less $f$-calls.

    
%     % \item 数値実験では、Situation (b)を模擬したテスト問題に、既存手法と提案アプローチを適用し、比較を行った。その結果、提案アプローチのみがSituation (b)に対処できることが示された。ただし、\wra{}で解候補のワーストケース関数でのランクを適切に推定できない場合は最適解に収束できないことが示された。例えば、シナリオ変数が高次元で、$y$に対する目的関数がweak structureな多峰性な場合である。


%     \item We applied the proposed approaches, \wracma{} and \wraslsqp{}, and the existing approaches, \zopgda{} and \acma{}, on test problem containing situation (b). The experiment results showed the only proposed approaches among the applied approaches could address situation (b), whereas the existing approaches failed.
    
%     % \item 提案アプローチの適用効果をRobust berthing controlの問題において評価した。提案アプローチは、ほぼすべての試行でワーストケースでも座礁せずに安定して着桟できる解を求めることができた点で既存手法よりも優位であることが示された。
    
%     \item The effectiveness of the proposed approaches was demonstrated on the robust berthing control problems where wind condition is uncertain, and we compared the results with that obtained from the existing approaches, \zopgda{} and \acma{}. The proposed approaches were superior to the existing approaches because they could stably obtain a robust solution on the problem where the existing approaches frequently outputted the solution colliding with the berth.  
    
% \end{enumerate}

% The limitations of the proposed approaches are described as following. 
% First, the proposed approaches cannot address the problem that the ranking of the worst-case function values for the solution candidates cannot be estimated properly. This limitation was demonstrated, for example, on test problem $f_4$. Therefore, it is necessary for useful \wra{} to select a proper solver which can approximate the worst-case ranking.  
% Second, the proposed approaches required many $f$-calls for the convergence, thus, the optimization time can be bottleneck in practical when they are applied to the problem using the simulation spending many computation times at a time. The proposed approaches should be improved so that they can converge with less $f$-calls. 
% Another option is practical development for the proposed approaches with parallel computation. 
% For example, intuitively, proper parallel computation can accelerate the optimization of \wracma{} by factor $\lambda \times \lambda_y$
% On the other hand, parallel computing is not effective for the existing approaches due to the alternative update. In terms of this point, the proposed approach will be practical in the real-world applications.
% They should be addressed in future works. 


\appendix

% \section{Sensitivity analysis}\label{sec:sens}

% $\wra{}$のパラメータである$\gamma$ ($=c_{\max}/ n$) と$\tau_{\mathrm{threshold}}$の影響とロバスト性を調査するために、テスト問題において、これらのパラメータの感度解析を行った。実験設定は、特段の断りがなければ、\Cref{sec:common}に記載した設定と同じとした。
% テスト問題において、設計変数とシナリオ変数の次元数は$m=n=20$とし、相互作用項$x^T B y$の係数行列は$B=\diag{(1,...,1)}$とした。

% To demonstrate the sensitivity of the proposed approach on the choice of $c_{\max}$ and $\tau_{\mathrm{threshold}}$, we conducted additional experiments on $f_1$--$f_{11}$. The experimental settings are the same as those described in \Cref{sec:common}. We set $b = 1$ in this experiments.


% \subsection{$\gamma$}

% % $\gamma$が大きければ、$c_{\max}$が大きくなり、ワーストケース関数$F$を推定するためにより多くの$f$-callsが必要になると考えられる。$\gamma$が小さければ、$c_{\max}$も小さくなり、最適解に収束するための$f$-callsは小さくなると考えられるが、一方で、$F$を推定できるようにシナリオを改悪する前に\cmaes{}や\slsqp{}インスタンスが終了することが考えられる。この場合、十分にワーストケース関数を推定できていないと考えられ、最適解収束が困難になると考えられる。



% If $\gamma$ is larger, $c_{\max}$ will be larger and more $f$-calls will be required to estimate the worst-case function $F$. If $\gamma$ is smaller, $f$-calls to converge at the optimal solution are expected to be smaller because $c_{\max}$ is also smaller. 
% On the other hand, \wra{} with a small $\gamma$ may terminate $\lambda$ \cmaes{} or\slsqp{} solvers before the scenario is significantly modified. 
% In this case, the proposed approaches will suffer from the convergence at the optimal solution because the ranking of the the worst-case function values $Rank_{F}(x_i)_{i=1}^{\lambda}$ is not sufficiently estimated.


% テスト問題$f_7$でのパラメータ$\gamma$の感度解析結果を\Cref{fig:gamma}に示す。その他のテスト問題の結果は\Cref{fig:gamma}と類似していたため省略した。

% \Cref{fig:gamma} shows the number of $f$-calls spent by \wracma{} and \wraslsqp{} on $f_7$ for different $\gamma = c_{\max} / n$ values. The results for the other test problems were similar to those in \Cref{fig:gamma}, and hence, omitted.
%

% We observed that the smallest $f$-calls for the converged at the optimal solution was respectively obtained from \wracma{} with $\gamma=0.15$ ($c_{\max} = 3$) and \wraslsqp{} with $\gamma=0.05$ ($c_{\max} = 1$). When $\gamma$ was set at too much small value such as $0.05$, \wracma{} required more $f$-calls because the rank $Rank_{F}(x_i)_{i=1}^{\lambda}$ was not sufficiently estimated.
% On the other hand, in this experiments, \wraslsqp{} with a smaller $\gamma$ required less $f$-calls for the convergence. 
% In the result on $f_7$, the ratio of the maximum and minimum $f$-calls was respectively about $4$ for \wracma{} and $7$ for \wraslsqp{}.


% \begin{figure}[t]
% \centering
    % \includegraphics[width=0.8\hsize]{./figures/sens_cmax_plot_fn7_ntry20.pdf}%
%  \caption{$f_{10}$}%
% \end{subfigure}
% \caption{Sensitivity for $\gamma$ on $f_7$. Mean and standard deviation of the number of $f$-calls until successful convergence over 20 runs}
% \label{fig:gamma}
% \end{figure}
% \begin{figure}[t]
% \centering
    % \includegraphics[width=0.8\hsize]{./figures/sens_tau_plot_fn5_ntry20.pdf}%
%  \caption{$f_{10}$}%
% \caption{Sensitivity for $\tau_{\mathrm{threshold}}$ on $f_5$. Mean and standard deviation of the number of $f$-calls until successful convergence over 20 runs}
% \label{fig:tau}
% \end{figure}



% \subsection{$\tau_{\mathrm{threshold}}$}

% % $\tau_{\mathrm{threshold}}$が大きければ、ワーストケース関数$F$を高い精度で推定できると考えられるが、多くの$f$-callsが必要になると考えられる。
% % 一方、$\tau_{\mathrm{threshold}}$が小さければ、$F$推定のためのシナリオの改悪は少ない$f$-callsで終了できるものの、ワーストケース関数$F$の精度も減少すると考えられる。十分な精度でワーストケース関数を推定できていない場合、最適解収束が困難になると考えられる。



% Although \wra{} with a high $\tau_{\mathrm{threshold}}$ will help to approximate the worst-case function $F$ with high accuracy, many $f$-calls will be required. \wra{} with a smaller $\tau_{\mathrm{threshold}}$ will require less $f$-calls to terminate $\lambda$ solvers. However, it may terminate the scenario modification before approximating the ranking of the the worst-case function values $Rank_{F}(x_i)_{i=1}^{\lambda}$ with sufficient accuracy. In many iterations, if the approximation accuracy of $Rank_{F}(x_i)_{i=1}^{\lambda}$ is low, the convergence may be failed.


% テスト問題$f_5$でのパラメータ$\tau_{\mathrm{threshold}}$の感度解析結果を\Cref{fig:tau}に示す。その他のテスト問題の結果は\Cref{fig:tau}と類似していたため省略した。

% \Cref{fig:tau} shows the number of $f$-calls spent by \wracma{} and \wraslsqp{} on $f_5$. The results for the other test problems were similar to those in \Cref{fig:tau}, hence omitted.


% In this experiment, we cannot observe the sensitivity of $\tau_{\mathrm{threshold}}$. However, we advice to avoid setting $\tau_{\mathrm{threshold}}$ at too much small value such as $0.0$ to approximate $Rank_{F}(x_i)_{i=1}^{\lambda}$ with sufficient accuracy.  


% \section{Scalability analysis} \label{sec:scal}

% % シナリオ変数が高次元なテスト問題$f_1$--$f_{10}$に対して、\wracma{}及び\wraslsqp{}が消費する$f$-callsを調査した。
% % 実験設定は、特段の断りがなければ、\Cref{sec:common}に記載した設定と同じとした。
% % テスト問題において、相互作用項$x^T B y$の係数行列はすべての要素が$1$となる行列とした。
% % 設計変数の次元数を$m=10$と固定して、$n=\{1, 2, 3, 5, 7, 10, 20, 40, 80, 120, 160, 200\}$とした。

% The efficiency of \wracma{} and \wraslsqp{} with variable dimension of the scenario vector $n \in \{1, 2, 3, 5, 7, 10, 20, 40, 80, 120, 160, 200\}$ on test problems $f_5$ was investigated with fixing the dimensions of the design vector at $m=10$. 
% The experimental setup was basically the same as that described in \Cref{sec:common}.
% In test problems, the coefficient matrix of the interaction term $x^T B y$ was a matrix with all elements $1$.

% % テスト問題$f_5$でのscalability analysisの結果を\Cref{fig:scaly}に示す。その他のテスト問題の結果は\Cref{fig:scaly}と類似していたため省略した。


% The results of scalability analysis on test problem $f_5$ are shown in \Cref{fig:scaly}. 
% % The results for the other test problems were similar to the results in \Cref{fig:scaly}, therefore, they were omitted.


% % \Cref{fig:scaly}から、$n$の増加に伴って\wracma{}及び\wraslsqp{}が消費する$f$-callsが増加した。\wracma{}について、$n=1$と$n=200$のときの$f$-callsの比は約30であった。また、\wraslsqp{}について、$n=1$と$n=200$のときの$f$-callsの比は約12であった。

% \Cref{fig:scaly} shows $f$-calls is increased with a higher $n$. 
% For \wracma{} and \wraslsqp{}, the ratio of $f$-calls at $n=1$ and $n=200$ was respectively about $30$ and $12$.


% \begin{figure}[t]
% \centering
% %\begin{subfigure}{\exss\hsize}
% %    \includegraphics[width=\hsize]{./figures/scal_y_plot_fn1_ntry20.pdf}%
% %  \caption{$f_{1}$}%
% %\end{subfigure}
%     \includegraphics[width=\hsize]{./figures/scal_y_plot_fn5_ntry20.pdf}%
% %  \caption{$f_{5}$}%
% \caption{Scalability for dimension of scenario vector $n$ on $f_5$. Mean and standard deviation of the number of $f$-calls until successful convergence over 20 runs}
% \label{fig:scaly}
% \end{figure}


% \section{Expected speed-up of \wracma{} by parallel computing} \label{sec:parallel}

% % 今回提案したアプローチは並列計算を導入することで高速化が期待できる。ここでは、WRA-CMAの並列計算を取り入れないときの最適化のための計算時間$T_s$と並列化したときの計算時間$T_p$の比である高速化率$S=T_s/T_p$を評価する。

% The proposed approaches, \wracma{} and \wraslsqp{}, are expected to accelerate the computation time by computing the objective function values for each solution candidate in parallel. In this section, we evaluate the speedup ratio $S=T_s/T_p$ for \wracma{}, where $T_s$ is the computation time resulted from sequential computing and $T_p$ is the computation time with parallel computing.

% % 今回は、簡単に並列化効果を検討するために、以下を仮定する。
% % 並列化を導入するに際して、対象とする問題で、最適解探査に要する時間は数値シミュレーションで消費される時間が支配的となることを仮定する。
% % つまり、最適解探査に要する時間は、おおよそ数値シミュレーションで消費された時間の合計値となる。ここでは、外側ループのCMA-ESの反復を$t$で生成された$i$番目の解候補を$x^t_i$とする。そして、$x^t_i$のためのWRAにおいて、Early Stopping strategyの箇所で要するソルバーの反復は$t'(x^t_i)$とする ($t'(x_i^t) \geq T_{\min}$)。
% % 二つ目は、一回あたりのシミュレーション時間$T_{\mathrm{simulation}}$は入力する$x$と$y$で大きくかわらないことを想定する。
% % 三つ目は、ある反復$t$におけるWRA内のソルバーCMA-ESの反復回数$t'$は同じとした。つまり、$\tau$により$\lambda_x$個のCMA-ESが終了するまでの反復回数$t'$は同じとした。


% For introducing parallelization for \wracma{}, we assume followings. 
% First, the computation time for the optimization mainly depends on the total time spent by the numerical simulations. 
% Second, the simulation time $T_{\mathrm{simulation}}$ for a given solution candidate $x$ and scenario vector $y$ is constant. 
% Third, at iteration $t$ in \cmaes{} for the outer loop, $\lambda$ solvers \cmaes{} in \wracma{} are assumed to be terminated at the same iteration $t'_t$. 


% % 上記3つの仮定のもと、最適解探査に要する計算時間を見積もる。
% % このときの最適解探査のための時間$T_s$は
% % \begin{align}
% % T_s = \sum_{t=1}^T \sum_{i=1}^{\lambda_x} ( \sum_{t'=1}^{t'_t} T_{\mathrm{ES}}^{t'} + \sum_{k=1}^{y_{\mathrm{member}}} T_{\mathrm{simulation}} ) 
% % \end{align}
% % となる。
% % ここで、$T$は最適解探査に要した反復回数、$t'_t$は外側ループの反復$t$で生成されたある解候補$x^t_i$のワーストシナリオ探査のために消費された反復数、$T_{\mathrm{ES}}^{t'}$はEarly Stopping strategyの箇所でソルバーの反復$t'$で要したシミュレーションの合計時間、である。
% % WRA内のソルバーがCMA-ESなため、$T_{\mathrm{ES}}^{t'}(x_i^t)$を細分化すると、
% % $T_{\mathrm{ES}}^{t'}(x_i^t) = \lambda_y \times T_{\mathrm{simulation}}$
% % である。
% % 総じて、
% % \begin{align}
% % &T_s = \sum_{t=1}^T \sum_{i=1}^{\lambda_x} ( \sum_{t'=1}^{t'_t} \lambda_y T_{\mathrm{simulation}} + \sum_{k=1}^{y_{\mathrm{member}}} T_{\mathrm{simulation}} ) \nonumber \\
% % &= \sum_{t=1}^T \sum_{i=1}^{\lambda_x} ( t'_t \lambda_y + y_{\mathrm{member}})T_{\mathrm{simulation}} \nonumber \\
% % &= \sum_{t=1}^T \lambda_x ( t'_t \lambda_y + y_{\mathrm{member}})T_{\mathrm{simulation}} \nonumber \\
% % &= \lambda_x \lambda_y T_{\mathrm{simulation}} \sum_{t=1}^T (t'_t + y_{\mathrm{member}}/\lambda_y) \label{eq:sequential}
% % \end{align}
% % となる。

% Under the above three assumptions, the computation time $t_s$ can be estimated as 
% % \begin{align}
% % T_s = \sum_{t=1}^T \sum_{i=1}^{\lambda_x} ( \sum_{t'=1}^{t'_t} T_{\mathrm{ES}}^{t'} + \sum_{k=1}^{y_{\mathrm{member}}} T_{\mathrm{simulation}} ) \space , 
% % \end{align}
% \begin{align}
% &T_s = \sum_{t=1}^T \sum_{i=1}^{\lambda_x} ( \sum_{t'=1}^{t'_t} \lambda_y T_{\mathrm{simulation}} + \sum_{k=1}^{\ny} T_{\mathrm{simulation}} ) \nonumber \\
% &= \sum_{t=1}^T \sum_{i=1}^{\lambda_x} ( t'_t \lambda_y + \ny)T_{\mathrm{simulation}} \nonumber \\
% &= \sum_{t=1}^T \lambda_x ( t'_t \lambda_y + \ny)T_{\mathrm{simulation}} \nonumber \\
% &= \lambda_x \lambda_y T_{\mathrm{simulation}} \sum_{t=1}^T (t'_t + \ny/\lambda_y) \space , \label{eq:sequential}
% \end{align}
% where $T$ is the number of iterations required in \cmaes{} for the outer loop until convergence and $\lambda_y$ is the population size of a solver \cmaes{}. 
% % and $T_{\mathrm{ES}}^{t'}$ is the total simulation time at iteration $t'$ of a solver \cmaes{} for a solution candidate.   
% % In case of using \cmaes{}, We obtain $T_{\mathrm{ES}}^{t'}(x_i^t) = \lambda_y \times T_{\mathrm{simulation}}$, and $T_s$ is given as 
% %The time $T_{\mathrm{ES}}^{t'}(x_i^t)$ depends on solvers. For example, in case of using \slsqp{}, we obtain $T_{\mathrm{ES}}^{t'}(x_i^t) = n \times T_{\mathrm{simulation}} + T_{\eta}$. In this case of $T_{\mathrm{ES}}^{t'}(x_i^t)$, first term in right hand side is the time for obtaining Jacobian and second term in right hand side is the time for updating the learning rate.


% % 数値シミュレーションを並列に処理できれば最適解探査のための計算時間$T_s$を高速化できる。並列化可能な部分は、\eqref{eq:sequential}から、(1) 外側ループのCMA-ESの解候補に対する処理、(2) warm starting strategy内のシナリオ毎の数値シミュレーション、(3) WRA内のCMA-ESが反復$t'$で生成したシナリオのための数値シミュレーション、である。
% % このため、並列化計算を導入したときの計算時間$T_p$は
% % \begin{align}
% % T_p = T_{\mathrm{simulation}} \sum_{t=1}^T (t'_t + 1/\lambda_y) \label{eq:parallel}
% % \end{align}
% % である。


% The computation time $T_s$ can be accelerated if the numerical simulation can be processed in parallel. From \eqref{eq:sequential}, parallelizable parts are, (1) processing for the solution candidates of \cmaes{} in the outer loop, (2) numerical simulations for each scenario vector in warm starting strategy, and (3) numerical simulation for each scenario vector generated by a solver \cmaes{} at iteration $t '$.
% As a result, the computation time $T_p$ is 
% \begin{align}
% T_p = T_{\mathrm{simulation}} \sum_{t=1}^T (t'_t + 1/\lambda_y) \space . \label{eq:parallel}
% \end{align}


% % 並列計算導入による高速化率$S=T_s/T_p$は
% % \begin{align}
% % & S = \frac{T_s}{t_p} \nonumber \\
% % & = \frac{\lambda_x \lambda_y T_{\mathrm{simulation}} \sum_{t=1}^T (t'_t + y_{\mathrm{member}}/\lambda_y)}{T_{\mathrm{simulation}} \sum_{t=1}^T (t'_t + 1/\lambda_y)} \nonumber \\
% % & = \frac{\lambda_x \lambda_y \sum_{t=1}^T (t'_t + y_{\mathrm{member}}/\lambda_y)}{\sum_{t=1}^T (t'_t + 1/\lambda_y)} \nonumber \\
% % & \geq \lambda_x \lambda_y
% % \label{eq:ratio}
% % \end{align}
% % である。今回の仮定であれば、少なくとも、$\lambda_x \lambda_y$倍の高速化効果が期待できる。


% The speedup rate $S=T_s/T_p$ is
% \begin{align}
% & S = \frac{T_s}{t_p} \nonumber \\
% & = \frac{\lambda_x \lambda_y T_{\mathrm{simulation}} \sum_{t=1}^T (t'_t + \ny/\lambda_y)}{T_{\mathrm{simulation}} \sum_{t=1}^T (t'_t + 1/\lambda_y)} \nonumber \\
% & = \frac{\lambda_x \lambda_y \sum_{t=1}^T (t'_t + \ny/\lambda_y)}{\sum_{t=1}^T (t'_t + 1/\lambda_y)} \nonumber \\
% & \geq \lambda_x \lambda_y \space .
% \label{eq:ratio}
% \end{align}
% Therefore, under the above three assumptions, \wracma{} is expected to accelerate $\lambda_x \lambda_y$ at least.


% % 並列計算の導入による、robust berthing control problemのCase Cの計算時間の短縮効果について検討する。Case Cは$m=99$で$n=12$なため、$\lambda_x = 17$で$\lambda_y=11$である。今回は (CPUの名前) 、Robust berthing control problemでは1回に約0.1秒で、200万回のシミュレーションを行って最適解を探査した。このため、今回の最適解探査では約200000秒 (約2.3日) の時間を要した。
% % 今回の問題であれば、\eqref{eq:ratio}の通りに高速化できる場合、おおよそ187倍の高速化が可能で、最適解探査時間は約1070秒まで短縮できる。

% We discuss the practicality of \wracma{} with parallel computing through Case C of the robust berthing control problem: Case C has $m=99$ and $n=12$, so $\lambda_x = 17$ and $\lambda_y = 11$. 
% In this case 
% \textcolor{red}{
% (name of CPU),
% } 
% one simulation used in the robust berthing control problem took about $0.1$ second at a time, and  the optimization required about $200,000$ seconds (about $2.3$ days) due to running $2$ million simulations.
% If parallel computation can accelerate \wracma{} as shown in \eqref{eq:ratio}, the optimization time can be reduced to approximately $1070$ seconds because speedup rate $S$ is approximately $187$.


% % 今回のrobust berthing control problemのCase Cを参考に、WRA-CMAが適用できるシミュレーション時間の範囲について検討する。並列計算を導入した際の計算時間$T_p$が約1070秒なため、\eqref{eq:parallel}より、$\sum_{t=1}^T (t'_t + 1/\lambda_y) = T_p/ T_{\mathrm{simulation}}=10700 $となる。
% % もしも、robust berthing control problemのCase Cのような問題条件で、計算時間として1か月間 ($T_p = 2592000$) が許容できる場合、適用できるシミュレーション時間の上限は、\eqref{eq:parallel}より、$T_{\mathrm{simulation}} \leq T_p/\sum_{t=1}^T (t'_t + 1/\lambda_y) = 242$となる。つまり、robust berthing control problemのCase Cのような問題条件であれば、並列計算を導入することで、シミュレーション時間が数百秒となる問題までは扱えると考えられる。


% Referring to Case C of this robust berthing control problem as a example of real-world application, we  examine the applicable simulation time for \wracma{}.
% From \eqref{eq:parallel}, $\sum_{t=1}^T (t'_t + 1/\lambda_y) = T_p/ T_{\mathrm{simulation}}=10700 $ when $T_p = 1070$.
% If a month of computation time ($T_p = 2592000$) is acceptable for a problem such as Case C of the robust berthing control problem, the simulation time should be less than $T_p/\sum_{t=1}^T (t'_t + 1/\lambda_y) = 242$ seconds. 


% % 実際の問題では、様々な課題があると考えられ、並列計算を導入した実用的なWRAの開発については今後の課題である。


% For practical use of \wracma{} with parallel computation, some issues will be appeared. The practical \wracma{} introducing parallel computation should be developed in future works.



% \section{Additional results for automatic berthing control problem} \label{sec:addresult}


% \begin{figure}
%     \centering
%     \includegraphics[width=\hsize]{trajectories/wra(slsqp).pdf}%
%     \caption{WRA(SLSQP)}%
% \end{figure}
% \begin{figure}
%     \centering
%     \includegraphics[width=\hsize]{trajectories/wra(slsqp)+adv-cma.pdf}%
%     \caption{WRA(SLSQP)+ADV}%
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=\hsize]{trajectories/adv-cma.pdf}%
%     \caption{ADV}%
% \end{figure}
% \begin{figure}
%     \centering
%     \includegraphics[width=\hsize]{trajectories/cma(no wind).pdf}%
%     \caption{CMA(no wind)}%
% \end{figure}

% \begin{figure}
%     \centering
%     \begin{subfigure}{0.25\hsize}%
%     \includegraphics[width=\hsize]{figures/WRA-ADV-DDCMA-A_A_trajectory.pdf}%
%     \caption{WRA(DDCMA) + ADV-CMA}%
%     \end{subfigure}%
%     \begin{subfigure}{0.25\hsize}%
%     \includegraphics[width=\hsize]{figures/WRA-DDCMA-IF2-A_A_trajectory.pdf}
%     \caption{WRA(DDCMA)}%
%     \end{subfigure}%
%     \begin{subfigure}{0.25\hsize}%
%     \includegraphics[width=\hsize]{figures/WRA-ADV-SLSQP-DDCMA-A_A_trajectory.pdf}
%     \caption{WRA(SLSQP) + ADV-CMA}%
%     \end{subfigure}%
%     \begin{subfigure}{0.25\hsize}%
%     \includegraphics[width=\hsize]{figures/WRA-SLSQP-DDCMA-IF2-A_A_trajectory.pdf}
%     \caption{WRA(SLSQP)}%
%     \end{subfigure}%
%     \caption{A}
% \end{figure}

% \begin{figure}
%     \centering
%     \begin{subfigure}{0.25\hsize}%
%     \includegraphics[width=\hsize]{figures/WRA-ADV-DDCMA-B_B_trajectory.pdf}%
%     \caption{WRA(DDCMA) + ADV-CMA}%
%     \end{subfigure}%
%     \begin{subfigure}{0.25\hsize}%
%     \includegraphics[width=\hsize]{figures/WRA-DDCMA-IF2-B_B_trajectory.pdf}
%     \caption{WRA(DDCMA)}%
%     \end{subfigure}%
%     \begin{subfigure}{0.25\hsize}%
%     \includegraphics[width=\hsize]{figures/WRA-ADV-SLSQP-DDCMA-B_B_trajectory.pdf}
%     \caption{WRA(SLSQP) + ADV-CMA}%
%     \end{subfigure}%
%     \begin{subfigure}{0.25\hsize}%
%     \includegraphics[width=\hsize]{figures/WRA-SLSQP-DDCMA-IF2-B_B_trajectory.pdf}
%     \caption{WRA(SLSQP)}%
%     \end{subfigure}%
%     \caption{B}
% \end{figure}

% \begin{figure}
%     \centering
%     \begin{subfigure}{0.25\hsize}%
%     \includegraphics[width=\hsize]{figures/WRA-ADV-DDCMA-C_C_trajectory.pdf}%
%     \caption{WRA(DDCMA) + ADV-CMA}%
%     \end{subfigure}%
%     \begin{subfigure}{0.25\hsize}%
%     \includegraphics[width=\hsize]{figures/WRA-DDCMA-IF2-C_C_trajectory.pdf}
%     \caption{WRA(DDCMA)}%
%     \end{subfigure}%
%     \begin{subfigure}{0.25\hsize}%
%     \includegraphics[width=\hsize]{figures/WRA-ADV-SLSQP-DDCMA-C_C_trajectory.pdf}
%     \caption{WRA(SLSQP) + ADV-CMA}%
%     \end{subfigure}%
%     \begin{subfigure}{0.25\hsize}%
%     \includegraphics[width=\hsize]{figures/WRA-SLSQP-DDCMA-IF2-C_C_trajectory.pdf}
%     \caption{WRA(SLSQP)}%
%     \end{subfigure}%
%     \caption{C}
% \end{figure}


%\bibliography{ref}
\bibliography{thebibliography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Miyagi.jpg}}]{Atsuhiro Miyagi} received B.S. degree in Civil Engineering from National Defense Academy in 2014, Japan, and received M.S. degree in Civil Engineering from University of Portsmouth in 2015, United Kingdom. He is currently pursuing the Ph.D. degree in computer science at University of Tsukuba, Japan.
Since 2016, he has been a research engineer working in Taisei Advanced Center of Technology at Taisei Corporation, Japan. His research interest includes the simulation-based optimization under uncertainty, robust optimization, parallel computation, and fluid flow simulation in porous media.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Miyauchi.png}}]{Yoshiki Miyauchi} received the B.S. and M.S. degrees in mechanical engineering from Keio University, in 2011 and 2013, respectively. 
After graduation, he got a position as a researcher on naval architecture. Currently, he is a Ph.D. student of Osaka university. His research interests include maneuverability, control of ships and marine hydrodynamics.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{maki.jpg}}]{Atsuo Maki} received the B.S. degree in 2006, and the M.S. and Ph.D. degrees in Naval Architecture and Ocean Engineering from Osaka University, Japan, in 2007 and 2009, respectively. From 2009 to 2010, he was a postdoctoral research Fellow Fellow at Kobe University in Japan. From 2010 to 2017, he was a technical official at the Acquisition, Technology \& Logistics Agency of the Ministry of Defense, Japan. From 2015 to 2016, he was a researcher at Naval Surface Warfare Center Carderock Division (NSWCCD), US Navy. Since 2017, he has been an Associate Professor with University of Osaka, Japan. His research interests include nonlinear dynamics and chaos of ship motion at sea, stochastic process concerning ship motion and control, ship stability, maneuverability and control of ships.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fukuchi.jpg}}]{Kazuto Fukuchi} received a Ph.D. degree from University of Tsukuba, Japan, in 2018. Since 2019, he has been an Assistant Professor with the Faculty of Engineering, Information and Systems, University of Tsukuba, Japan, and also a Visiting Researcher with the Center for Advanced Intelligence Project, RIKEN, Japan. His research interests include mathematical statistics, machine learning, and their application.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{sakuma.png}}]{Jun Sakuma} received the Ph.D. degree in engineering from Tokyo
Institute of Technology, Tokyo, Japan, in 2003. Since 2016, he has been a Professor
with the Department of Computer Science, School of Systems and
Information Engineering, University of Tsukuba, Japan. He has also been Team Leader of the Artificial Intelligence Security and Privacy team in the Center for Advanced Intelligence
Project, RIKEN since 2016. From 2009 to 2016, he was an Associate Professor with the Department of Computer Science, University of Tsukuba. From 2004 to 2009, he was an Assistant Professor with the Department of Computational Intelligence and Systems Science, Interdisciplinary Graduate School of Science and Engineering, Tokyo Institute of
Technology, Tokyo, Japan. From 2003 to 2004, he was a Researcher with
Tokyo Research Laboratory, IBM, Tokyo, Japan. His research
interests include data mining, machine learning, data privacy, and
security. He is a member of the Institute of Electronics, Information
and Communication Engineers of Japan (IEICE).
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{akimoto.jpg}}]{Youhei Akimoto} received the B.S. degree in computer science in 2007, and the M.S. and Ph.D. degrees in computational intelligence and systems science from Tokyo Institute of Technology, Japan, in 2008 and 2011, respectively. From 2010 to 2011, he was a Research Fellow of JSPS in Japan, and from 2011 to 2013, he was a Post-Doctoral Research Fellow with INRIA in France. From 2013 to 2018, he was an Assistant Professor with Shinshu University, Japan. Since 2018, he has been an Associate Professor with University of Tsukuba, Japan as well as a Visiting Researcher with the Center for Advanced Intelligence Projects,  RIKEN. He served as a Track Chair for the continuous optimization track of GECCO in 2015 and 2016. He is an Associate Editor of ACM TELO and is on the editorial board of the ECJ. He won the Best Paper Award at GECCO 2018 and FOGA 2019. His research interests include design principles, theoretical analyses, and applications of stochastic search heuristics.
\end{IEEEbiography}
\EOD
\end{document}
