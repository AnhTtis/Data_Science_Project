\section{Related Work}

\subsection{Deformable Objects and Single-Layer Grasping}
There is a rich literature on deformable object manipulation; see~\cite{manip_deformable_survey_2018,grasp_centered_survey_2019,2021_survey_defs} for representative surveys. Deformable objects are challenging due to their infinite degrees of freedom, which both induce complex dynamics that are hard to model and control and lead to self-occlusions that makes planning challenging. 
%\daniel{I'd cut at least the 1D stuff out} For 1D deformable objects such as cables and ropes, prior work has studied tasks such as insertion~\cite{wire_insertion_1996,wire_insertion_1997}, knot tying and untangling~\cite{knot_planning_2003,tying_precisely_2016, untanglingLongCables2022,grannen2020untangling,SundaresanGrannen-RSS-21}, and reaching goals or target configurations~\cite{zhu_sliding_cables_2019,nair_rope_2017,wang_visual_planning_2019, harry_rope_2021,chi2022irp,lim2022real2sim2real}. She et al.~\cite{tactile_cable_2020} use a tactile-reactive gripper with a GelSight sensor to perform cable following and insertion tasks. 
Among deformable object manipulation, fabric manipulation is one of the most widely-studied areas~\cite{flinging_2022,fabric_vsf_2020,cloth_region_segmentation_2020,seita-bedmaking,ha2021flingbot,fabricflownet,VCD_cloth,lerrel_2020,gdoom2021,speedfolding_2022}. %bodies_uncovered_2022, --cut for space
These works focus on learning grasp locations that are effective for pick-and-place actions or dynamic actions to achieve smoothing and folding for one piece of fabric.

% Daniel: commenting the below out as these probably are not good fits; the fabrics one we can argue from a layering or a 2D perspective. And since it's IROS I think we have to get all references within the 8 page limit, unfortunately, so we should add more from interactive perception (I think Shuran Song also has papers in that area).
%Ichiwara et al.~\cite{ichiwara2022contact} use tactile sensors to perform the unzipping task. Beyond fabrics, prior work has also studied manipulating plush toys, sponges, and dough~\cite{ACID2022,Qi_dough_2022,matl2021Deformable,PASTA_2022}, or objects typically held in containers, such as liquids~\cite{visual_closed_loop_liquids_2017} and granular media~\cite{schenck_2017,samuel_clarke_2018,matl2020inferring}.

While some prior work has studied singulating a single sheet or fabric layer from a stack, most use tactile sensing or specialized end effectors.  Tirumala~et~al.~\cite{tirumala2022} use a ReSkin sensor~\cite{bhirangi2021reskin} to singulate layers of cloth from tactile feedback. Manabe~et~al.~\cite{manabe2021} design a rolling hand mechanism to separate a single sheet from a pile of fabrics. Guo~et~al.~\cite{deformation_page_turning_2021} use a XELA uSkin tactile sensor combined with visual inputs to turn a single book page. In this work, we propose to singulate layers with standard end effectors purely from visual feedback. Demura~et~al.~\cite{demura2018} study grasping the top folded towel from a stack using visual feedback with a scooping action, using towels which are each several millimeters thick. In contrast, we study manipulation tasks where layers can be thinner than 1 mm. 

\subsection{Manipulating Deformable Bags}
Some early work on bag manipulation studies mechanical design or policies for grasping~\cite{grasping_sacks_2005}, lifting~\cite{ayanna_2000} or unloading~\cite{unloading_sacks_2008} large sacks. 
%Recent work also studies more complex contact-rich bag manipulation tasks such as closing ziplock bags~\cite{contour_ziplock_2018}, unzipping a fabric bag~\cite{ichiwara2022contact}, and tying the handles of a plastic bag~\cite{bagKnotting2022}. 
Prior work also studies bag manipulation in simulation. For example, Seita~et~al.~\cite{seita_bags_2021} benchmark several simulation tasks that involve opening a bag and inserting objects into it, and Weng~et~al.~\cite{graph_based_interaction_2021} study modeling bags using graph neural networks. Much of the prior work on physical experiments with deformable bags assume a semi-structured bag state, such as pregrasped~\cite{contour_ziplock_2018,ichiwara2022contact,dextairity2022}, filled with objects~\cite{bagKnotting2022},  oriented upwards with the bag wide open~\cite{seita_bags_iros_2021,bahety2022bag}, and focus on a specific task such as packing and arranging objects~\cite{bahety2022bag}, opening the bag~\cite{dextairity2022}, or lifting the bag~\cite{seita_bags_iros_2021}. In contrast to these works, we study physical bag manipulation where bags start in unstructured states.


% Daniel (March 01): made some cuts here since this was too long.
Recently, Chen~et~al.~\cite{chen2022autobag} propose the AutoBag algorithm for manipulating a thin plastic bag from an unstructured state. In their setting, the bags can be compressed, deformed, and arbitrarily oriented, and the task is to reorient the bag upward, enlarge the opening, insert objects, and then lift the bag up. 
%For perception, Chen~et~al. propose to represent bags using key semantic parts (\eg handles and rim) learned with self-supervision from UV-fluorescent markings. For manipulation, they propose several novel primitives such as ``Compress,'' ``Dilate,'' ``Flip,'' and ``Pin-Pull.'' 
%However, their success rate is low (1/6) 
However, AutoBag frequently fails when attempting to orient the bag upward. %since it is not a stable pose for deformable bags. 
Gu et al.~\cite{gu2023ShakingBot} improve AutoBag by using dynamic shaking actions and performing item insertion with one gripper grasping the bag handle in midair.
% In contrast to AutoBag, we 
%use AutoBag's perception representation but 
% avoid orienting the bag upward. Instead, 
In contrast, we flatten the bag, singulate the top layer to open it, and insert objects sideways, which results in much higher success rates. Moreover, while AutoBag is designed specifically for opening thin plastic bags, we show evidence that \bagging is effective on other bag materials and shapes. %since flattening is easier than orienting a bag upward, and fabric smoothing methods can be applied to flattening a fabric bag.




\subsection{Interactive Perception}

%Interactive perception integrates perception and action, allowing robots to actively gather information about the environment through physical interactions with objects. 
%This approach recognizes that perception and action are tightly intertwined and that a robot's ability to perceive its environment can be greatly enhanced through active exploration and manipulation. 
Interactive perception combines perception and action, enabling a robot to reduce uncertainty through active physical exploration and interaction with objects~\cite{bohg2017}.
Interactive perception has a variety of applications, ranging from manipulation to object segmentation, to grasp planning~\cite{IP_cluttered_2020}. 
Recent work has used interactive perception to better understand properties of geometrically challenging objects for robotic manipulation. For example,~\cite{gadre2021act,nie2022sfa} study how to interact with articulated objects to discover their geometric information.
For deformable object manipulation, Shivakumar~et~al.~\cite{untanglingLongCables2022} use interactive perception to autonomously untangle cables, and Willimon~et~al.~\cite{Willimon2011} use interactive perception to detect and classify clothing. In this work, we use interactive perception to identify and singulate individual layers of bags to improve robotic bag manipulation.
