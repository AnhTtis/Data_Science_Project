% Final arXiv submission.
% if your latex compiler failed to compile, uncomment the command below:
\RequirePackage[2020-02-02]{latexrelease}
\documentclass{clv3_modified}

\usepackage{hyperref}
\usepackage{xcolor}
\definecolor{darkblue}{rgb}{0, 0, 0.5}
\hypersetup{colorlinks=true,citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}

\newcommand{\tocite}{\color{red}Citation\color{black}}
\newcommand{\todo}[1]{\color{red}#1\color{black}}

\makeatletter\newcommand{\tableofcontents}{\@starttoc{toc}}\makeatother
\setcounter{tocdepth}{1}

\bibliographystyle{compling}

% Hack to use autoref instead of ref, because the style file creates incorrect section numbers.
\def\sectionautorefname{}
\def\subsectionautorefname{}
\def\equationautorefname{}

% test compatibility with algorithmic.sty
%\usepackage{algorithmic}

\begin{document}
\issue{1}{1}{2016}

%Document Head
% \dochead{CLV3 Class File Manual}

% Left header.
\runningtitle{Chang and Bergen \thepage}
% Right header.
\runningauthor{Language Model Behavior}

\title{Language Model Behavior: \\
A Comprehensive Survey}

\author{Tyler A. Chang}
\affil{UC San Diego \\
\texttt{tachang@ucsd.edu}}

\author{Benjamin K. Bergen}
\affil{UC San Diego \\
\texttt{bkbergen@ucsd.edu}}

\maketitle

\begin{abstract}
Transformer language models have received widespread public attention, yet their generated text is often surprising even to NLP researchers. In this survey, we discuss over 250 recent studies of English language model behavior before task-specific fine-tuning.
Language models possess basic capabilities in syntax, semantics, pragmatics, world knowledge, and reasoning, but these capabilities are sensitive to specific inputs and surface features.
Despite dramatic increases in generated text quality as models scale to hundreds of billions of parameters, the models are still prone to unfactual responses, commonsense errors, memorized text, and social biases.
Many of these weaknesses can be framed as over-generalizations or under-generalizations of learned patterns in text.
We synthesize recent results to highlight what is currently known about large language model capabilities, thus providing a resource for applied work and for research in adjacent fields that use language models.
\end{abstract}

\section*{Contents}
\tableofcontents

\bigskip
\section{Introduction}
Transformer language models have revolutionized the field of natural language processing (NLP) since their introduction in 2018 \citep{radford-etal-2018-improving,devlin-etal-2019-bert}.
Recent research and public attention has demonstrated that large language models (e.g. GPT-3/4, PaLM, and OPT; \citealp{brown-etal-2020-language,chowdhery-etal-2022-palm,zhang-etal-2022-opt,openai-2023-gpt4}) can achieve remarkable performance both on standard NLP benchmarks and on open-ended natural language generation tasks from the general public \citep{wang-etal-2019-superglue,johnson-2022-ai}.
Already, language models are used in industry for applications ranging from web search and chatbots to medical and financial document analysis \citep{nayak-2019-understanding,broyde-palmer-2021-build,thewsey-2021-bring,lee-2023-what}.
Due to their widespread applicability, language models have been called ``foundation models'' for NLP \citep{bommasani-etal-2021-on}.

Language models are trained to predict masked (i.e. hidden) or upcoming words from context, usually text.
The models can then be fine-tuned for specific downstream tasks (e.g. text classification; \citealp{devlin-etal-2019-bert}), or they can be used directly for any text prediction task.
As language model capabilities have expanded in recent years, they have increasingly been used in the text generation scenario with minimal or no fine-tuning \citep{brown-etal-2020-language}.
This approach requires no task-specific data or further training infrastructure, thus expanding the range of possibilities and audience for language model applications.
In particular, the release of public APIs and interfaces such as GPT-3 and ChatGPT \citep{brown-etal-2020-language,openai-2021-chatgpt} have enabled widespread public experimentation on the text generation capabilities of language models.

Yet, text generated by language models is often surprising even to NLP researchers.
Previous studies have investigated both the outputs and internal mechanisms of language models, originally focusing on masked (i.e. fill-in-the-blank) ``BERT'' models and establishing the field of ``BERTology'' (see \citealp{rogers-etal-2020-a} for a survey).
In the years since the last BERTology survey in 2020, and in tandem with the rise of large autoregressive models such as GPT-3 (i.e. predicting upcoming words instead of masked words), language model analysis has shifted focus to these large autoregressive models.
Because these models are often used without fine-tuning for open-ended text generation, there have been an increasing number of behavioral studies evaluating the output text probabilities of language models.

Despite this flurry of research, language model text generation behavior remains unpredictable.
Although model performance on broad benchmark datasets is relatively consistent for a given model size and architecture, responses to specific inputs and examples are not. This feature makes large language models tempting but unreliable to use in many practical applications \citep{ganguli-etal-2022-predictability}.
Furthermore, the rapid pace of NLP research and the quantity of individual studies make any progress in understanding model behavior difficult to track.
As language models become more widespread and researchers from other fields invest interest in language models, it is increasingly important that our existing understanding of model behavior be made clear and accessible.

In this survey, we discuss over 250 recent studies of English language model behavior, covering syntax, semantics, pragmatics, world knowledge, reasoning, memorization, and bias.\footnote{The process for identifying papers and studies for this survey is described in Appendix \hyperref[app:lit-review]{A}. Code, key points, and links to cited papers are available at: \url{https://github.com/tylerachang/llm-behavior-survey}.}
% WARNING: hard-coded appendix letter.
Language models generate fluent and coherent text, but their predictions are highly dependent on input context.
Slight changes in input word choice and phrasing can lead to unfactual, offensive, or plagiarized text.
Understanding these behaviors has broad implications for informed applications in industry \citep{weidinger-etal-2021-ethical} and general questions about meaning and ``understanding'' in artificial agents \citep{bender-koller-2020-climbing,mitchell-krakauer-2022-the,shardlow-przybyla-2022-deanthropomorphising}.

To the extent possible, we avoid taking a stance on whether language models truly ``understand'' language.
We also leave deeper ethical discussions of the societal implications of language models to surveys focused specifically on that area (e.g. \citealp{weidinger-etal-2021-ethical,weidinger-etal-2022-taxonomy}).
Instead, we hope to provide a review of the empirical evidence for what behaviors language models exhibit in controlled settings.
We discuss a wide range of model capabilities and weaknesses (Sections\autoref{sec:syntax} through\autoref{sec:misinformation-personality-politics}), and we synthesize results framed from the perspectives of model scale (Section\autoref{sec:scale}) and text pattern generalization (Section\autoref{sec:language-modeling-generalization}).
In this way, we hope to combat anecdote-driven language model ``hype'' with informed hype grounded in what language models actually can and cannot do \citep{bowman-2021-when}, while also highlighting potential future directions of research in language model behavioral analysis.

\subsection{Scope}
\label{sec:scope}
We consider studies of masked and autoregressive English Transformer language models not fine-tuned for any specific downstream tasks.
We exclude a wealth of research on fine-tuned model behavior (e.g. models tuned for natural language inference, a text classification task).
During the fine-tuning process, language models are prone to overfitting to spurious correlations between text features and labels in the fine-tuning dataset \citep{mccoy-etal-2019-right,kavumba-etal-2020-balanced,wang-etal-2021-identifying,du-etal-2022-shortcut,kavumba-etal-2022-are}, and they can even ``forget'' syntactic and semantic information learned during the original pre-training process \citep{miaschi-etal-2020-linguistic,mosbach-etal-2020-on}.
Thus, fine-tuned language models are not necessarily reflective of the linguistic abilities of language models in general.
Moreover, as noted in the Introduction, language models are increasingly used without fine-tuning on any individual task.

We also leave studies of non-English and multilingual language models to future surveys that can better focus on the many nuances of cross-lingual comparisons.
We acknowledge that over-focusing on high-resource languages (e.g. English) is a recurring problem in NLP research \citep{joshi-etal-2020-state}, and we hope that this survey provides a foundation to expand to less well-studied languages for which language models often perform poorly \citep{wu-dredze-2020-languages,choudhury-deshpande-2021-how}.
Future surveys might also study the behavior of language model variants such as vision-language models \citep{du-etal-2022-a-survey}, code models \citep{chen-etal-2021-evaluating}, speech models \citep{lakhotia-etal-2021-generative,radford-etal-2022-robust}, knowledge-augmented models \citep{zhang-etal-2019-ernie}, sparsely-activated models \citep{fedus-etal-2021-switch}, or compressed models \citep{sanh-etal-2019-distilbert,zafrir-etal-2019-q8bert}.
In the current survey, we consider non-augmented ``out-of-the-box'' Transformer language models, as used in the majority of NLP research.

Finally, we limit our survey to behavioral studies of language models.
These studies treat the models as black box functions that take input text and return probability distributions over output text.
Often inspired by work in psycholinguistics, these studies evaluate language model responses to controlled inputs (e.g. \citealp{ettinger-2019-what}), to make inferences about how the models process and generate text.
As we note in Discussion Section\autoref{sec:levels-of-analysis}, other studies analyze language models at the mechanistic level, studying internal representations, individual neurons, and attention heads \citep{geva-etal-2020-transformer,meng-etal-2022-locating,olsson-etal-2022-in}.
We focus on behavioral studies in this survey, but establishing ties between mechanistic and behavioral analyses of language models is an exciting direction of emerging research.

\bigskip
\section{Transformer Language Models}
\label{sec:transformer-models}
In this section, we provide a brief introduction to Transformer language models, which we generally refer to as language models.
Transformer language models use a deep neural network architecture called a Transformer (\citealp{vaswani-etal-2017-attention}; Section\autoref{sec:architectures}), and they are trained to predict either masked words (i.e. fill-in-the-blank) or upcoming words in text (Section\autoref{sec:training}).
Throughout this survey, we refer to these two types of models as masked and autoregressive models respectively.\footnote{
Along with differentiating results for masked vs. autoregressive models, we mention when studies use a GPT-3 model (autoregressive) that may or may not have been instruction-tuned (Section\autoref{sec:training}).
For example, \texttt{text-davinci-001} and  \texttt{text-davinci-002} are instruction-tuned, but \texttt{davinci} \\ is not \citep{openai-2023-model}.
Still, even the instruction-tuning stage uses only the language modeling \\ objective.
We specifically note if any study uses a model tuned with reinforcement learning (Section\autoref{sec:training}), e.g. \texttt{text-davinci-003}.
When we refer to masked and autoregressive language models generally, we refer to models that are not fine-tuned.
}
Some studies refer to them as bidirectional and unidirectional models.
Language models are most often applied to downstream tasks using either fine-tuning (or prompt-tuning), zero-shot prompting, or few-shot prompting (Section\autoref{sec:downstream}).

\subsection{Architectures}
\label{sec:architectures}
The basic Transformer language model architecture has remained largely unchanged since 2018 \citep{radford-etal-2018-improving,devlin-etal-2019-bert}.
First, an input text string is converted into a sequence of tokens.
Tokens correspond roughly to words, although some words are composed of multiple subword tokens due to limited vocabulary size.
For example, the string ``This is preposterous!'' might be tokenized into \texttt{[\_this, \_is, \_prepo, ster, ous, !]}.
Common tokenization techniques include byte pair encoding (BPE; \citealp{sennrich-etal-2016-neural}) and unigram language modeling \citep{kudo-2018-subword}, but we refer to these other papers for detailed descriptions of tokenization techniques.
Model vocabularies generally range from 30K to 250K possible tokens \citep{radford-etal-2019-language,thoppilan-etal-2022-lamda,chowdhery-etal-2022-palm}.

After tokenization, each token is mapped to a fixed vector ``embedding''; the embedding for each token is learned during the pre-training process.
The embeddings are passed through a stack of Transformer layers (\citealp{vaswani-etal-2017-attention}; usually 10-100 layers), each consisting of a self-attention network, layer normalizations, and feedforward networks.
The primary innovation of Transformer layers is the self-attention network, which ``mixes'' the sequence of token embeddings using projections into a ``query'', ``key'', and ``value'' vector for each token.
This mixing of token embeddings results in a ``contextualized'' representation for each token, essentially a vector representation that incorporates the context of the input sequence.
Finally, after the stack of Transformer layers, each output token representation is projected into a distribution over the same token vocabulary used in the input.
In other words, the overall architecture maps each input token to a probability distribution over output tokens (e.g. upcoming tokens).
Language models usually have between 100M and 500B total parameters, with autoregressive models usually much larger than masked models \citep{devlin-etal-2019-bert,brown-etal-2020-language,lieber-etal-2021-jurassic,smith-etal-2022-using,chowdhery-etal-2022-palm}.

The Transformer architecture does not naturally encode any information about each token's position in an input sequence; intuitively, it is useful to encode this information for features such as word order.
Thus, Transformer language models use a variety of position encoding techniques \citep{wang-etal-2021-position,dufter-etal-2022-position}, such as adding absolute position embeddings to the input token embeddings (i.e. an embedding for each position $i$; \citealp{vaswani-etal-2017-attention,radford-etal-2018-improving,devlin-etal-2019-bert,radford-etal-2019-language,brown-etal-2020-language,zhang-etal-2022-opt}), relative position embeddings or biases (i.e. encoding relative position distances between tokens; \citealp{shaw-etal-2018-self,dai-etal-2019-transformerxl,raffel-etal-2020-exploring,chang-etal-2021-convolutions,rae-etal-2021-scaling,thoppilan-etal-2022-lamda}), or rotary position embeddings (an efficient approach to relative position biases; \citealp{su-etal-2021-roformer,chowdhery-etal-2022-palm}).
With relative rather than absolute position methods, language models can better extrapolate to longer sequences than observed during pre-training \citep{press-etal-2021-train}.
Language models are usually pre-trained with input sequence lengths of around 500 to 2000 tokens.

\subsection{Training}
\label{sec:training}
Language modeling refers to predicting tokens from context, usually text.
Masked and autoregressive language models are pre-trained to predict masked (i.e. hidden) and upcoming tokens respectively.
Recall from the previous section that the Transformer architecture predicts an output token distribution for each input token.
\begin{equation}
\label{eq:masked}
\begin{array}{l}
\textrm{The [MASK] walked.}
\end{array}
\end{equation}
\vspace{-1cm}
\begin{equation}
\label{eq:autoregressive}
\begin{array}{l}
\textrm{The \_\_} \\
\textrm{The dog \_\_} \\
\textrm{The dog walked \_\_}
\end{array}
\end{equation}
In masked language models (Example\autoref{eq:masked}), randomly selected tokens are replaced with [MASK] tokens; for each input [MASK] token, the model produces a probability distribution over the token that was masked (i.e. fill-in-the-blank).
In autoregressive models (Example\autoref{eq:autoregressive}), no tokens are replaced; for each input token, the model produces a probability distribution over the next token (i.e. predicting each next token).

Language models are pre-trained using gradient descent, observing many examples as in Examples\autoref{eq:masked} and\autoref{eq:autoregressive}.
Text corpora for pre-training usually range from approximately 5B to 1.5T tokens (roughly 15GB to 5TB of raw text; \citealp{devlin-etal-2019-bert,liu-etal-2019-roberta,brown-etal-2020-language,rae-etal-2021-scaling,hoffmann-etal-2022-training}).
For compute-optimal pre-training in autoregressive language models, as the number of model parameters increases, the number of pre-training tokens should increase roughly proportionally \citep{kaplan-etal-2020-scaling,hoffmann-etal-2022-training}.
During pre-training, examples are fed into the models with anywhere from 100K to 4M tokens per optimization step (i.e. batch size), usually with larger batch sizes in larger models \citep{devlin-etal-2019-bert,brown-etal-2020-language,hoffmann-etal-2022-training,chowdhery-etal-2022-palm,zhang-etal-2022-opt}.
Models are usually pre-trained for 100K to 1M steps \citep{radford-etal-2018-improving,devlin-etal-2019-bert,zhang-etal-2022-opt}; when possible, examples are not repeated during pre-training \citep{hoffmann-etal-2022-training,chowdhery-etal-2022-palm}.
Due to high computational costs, relatively few language models are pre-trained from scratch as described here, and they are usually trained in industry labs.
In practice, most NLP researchers build applications upon existing pre-trained language models, using the approaches described in Section\autoref{sec:downstream}.

This survey considers pre-trained language models as described above.
Recent language models often contain further non-task-specific fine-tuning stages (particularly autoregressive models; \citealp{thoppilan-etal-2022-lamda,ouyang-etal-2022-training}).
For example, autoregressive models are sometimes fine-tuned using the language modeling objective on curated human-written examples that demonstrate desirable text outputs \citep{ouyang-etal-2022-training} or examples of outputs that correctly follow input instructions \citep{wei-etal-2021-finetuned,iyer-etal-2022-optiml}.
These approaches are referred to as supervised fine-tuning (SFT) or instruction tuning.
Some more recent models are also tuned using reinforcement learning, with predicted human preferences for different responses used as a reward (reinforcement learning from human feedback, or RLHF; \citealp{ouyang-etal-2022-training,openai-2023-gpt4}).
Throughout this survey, we consider non-fine-tuned language models unless otherwise specified.\footnote{Mentions of GPT-3 specifically may be instruction-tuned, but not tuned with reinforcement learning. See footnote in Section\autoref{sec:transformer-models}.}
Non-fine-tuned language models still serve as the foundation for more recent language models.

\subsection{Downstream tasks and text generation}
\label{sec:downstream}
Language models are used for a wide range of downstream tasks, including but not limited to custom chatbots, question answering, sentiment classification, offensive text detection, and textual similarity quantification \citep{devlin-etal-2019-bert,zhang-etal-2019-bertscore,zhao-etal-2021-a-comparative,zong-krishnamachari-2022-a}.
Traditionally, given example inputs and outputs for a task, language models are fine-tuned by adjusting all or some model parameters using gradient descent \citep{radford-etal-2018-improving,devlin-etal-2019-bert,lester-etal-2021-the,chowdhery-etal-2022-palm}.
As autoregressive models have risen in popularity, tasks are increasingly formulated as prompted text generation tasks \citep{wei-etal-2021-finetuned}:
\begin{equation}
\label{eq:generation-task}
\begin{array}{l}
\textrm{Premise: Fun for adults and children.} \\
\textrm{Hypothesis: Fun for only children.} \\
\textrm{Does the premise entail the hypothesis?} \\
\textrm{\_\_\_\_\_\_} \\
\hspace{3cm} \textrm{\citep{williams-etal-2018-a-broad}}
\end{array}
\end{equation}
The input text is referred to as the prompt or context.
Autoregressive language models can perform many tasks similar to Example\autoref{eq:generation-task} without fine-tuning on that specific task (i.e. zero-shot learning, e.g. by instruction-tuning on other tasks; \citealp{wei-etal-2021-finetuned}).
If example inputs and outputs (e.g. 1-100 examples) are included in the prompt, then language models can perform well without any fine-tuning at all \citep{brown-etal-2020-language,chowdhery-etal-2022-palm,zhang-etal-2022-opt}; providing examples in context without any parameter updates is commonly known as few-shot prompting or in-context learning.

In cases such as Example\autoref{eq:generation-task}, autoregressive language models can compute the probability for any desired output text by iteratively multiplying the probability for each next token.
When the models are used for open-ended text generation (i.e. the models must select each next token), common approaches are to (1) iteratively select the most probable next token (greedy sampling), (2) iteratively sample the next token from the output probability distribution with some temperature parameter $\tau$ (temperature sampling), (3) sample from the top $k$ token predictions (top-$k$ sampling), or (4) sample from the top tokens that sum to some probability $p$ (nucleus sampling; \citealp{holtzman-etal-2019-curious}).
In all of these cases, multiple candidate sequences of tokens can be generated and then ranked according to their overall sequence probability (i.e. beam search; \citealp{freitag-al-onaizan-2017-beam}), but beam search is often not used in practice due to its high computational cost.
Of the studies discussed in this survey, the majority use greedy, temperature, top-$k$, or nucleus sampling for open-ended text generation.
In the next sections, we discuss recent studies evaluating language model generated text and output text probabilities from a wide range of perspectives.

\bigskip
\section{Syntax}
\label{sec:syntax}
We begin with studies that evaluate language model predictions from a syntactic perspective.
In the vast majority of cases, language models are more likely to predict grammatical tokens than ungrammatical tokens, adhering to a wide variety of syntactic rules (Section\autoref{sec:syntax-overall}).
In subject-verb agreement, the models' performance degrades in more complex or infrequent examples (Section\autoref{sec:agreement}), and language model predictions are possibly over-sensitive to token position information (i.e. word order; Section\autoref{sec:position}), but syntactic abilities overall are learned fairly robustly early in pre-training (Section\autoref{sec:syntax-learning}).


\subsection{Language models generally  produce grammatical text.}
\label{sec:syntax-overall}
Systematic syntactic evaluations of autoregressive language models are conducted in \citet{warstadt-etal-2020-blimp}, \citet{hu-etal-2020-a-systematic}, and \citet{gauthier-etal-2020-syntaxgym}, comparing model probabilities for minimal pair examples that differ in grammaticality due to just one token (e.g. ``the boy [*eat/eats]'').\footnote{An asterisk before a phrase indicates ungrammaticality, as in \citet{carnie-2002-syntax}.}
Similar assessments are run for masked language models in \citet{park-etal-2021-deep}.
Both autoregressive and masked language models consistently assign higher probabilities to grammatical tokens, and they make predictions consistent with hierarchical syntactic structure, where clauses can be nested within one another.
Such structures are commonly observed in human language \citep{carnie-2002-syntax}, creating token relationships that are not solely dependent on linear word order.
\begin{equation}
\label{eq:agreement}
\textrm{The girl who had three dogs [*play/plays] accordion.}
\end{equation}
In Example\autoref{eq:agreement}, replacing ``girl'' with ``girls'' would require the verb to change to ``play''.
In other words, the verb ``plays'' agrees in number with the noun ``girl'' despite the appearance of the nested clause ``who had three dogs'' including the distractor noun ``dogs'' closer to the verb.
In these long-distance subject-verb agreement examples, language models generally assign higher probabilities to grammatical options, but their performance varies depending on the specific nouns, verbs, and distractors involved (Section \autoref{sec:agreement}).

Outside of agreement, language models recognize licensing, when the grammaticality of a token depends on an upstream ``licensor'' token, usually equal or higher in the hierarchical syntactic structure.
\begin{equation}
\label{eq:reflexive}
\begin{array}{l}
\textrm{I know what the lion devoured [*the gazelle/\_ ] yesterday.} \\
\textrm{I know that the lion devoured [the gazelle/ *\_ ] yesterday.} \\ \\
\hspace{4cm} \textrm{\citep{wilcox-etal-2022-using}}
\end{array}
\end{equation}
In Example\autoref{eq:reflexive}, the word ``what'' licenses the omitted direct object ``gazelle'' for the verb ``devoured''; the word ``that'' does not license such an omission.
This omission licensing is known as a filler-gap dependency, and \citet{wilcox-etal-2022-using} find that autoregressive language models respect filler-gap rules.
Similarly, masked language models assign higher probabilities to licensed tokens in reflexive licensing (reflexives such as ``himself'' require a properly situated previous noun phrase; \citealp{hu-etal-2020-a-closer}) and in negative polarity items (NPIs such as ``any'' require a previous negative word such as ``not''; \citealp{warstadt-etal-2019-investigating}).
However, autoregressive model predictions for reflexive licensing are less accurate in sentences where the licensed reflexive depends on the specific verb involved \citep{lee-schuster-2022-can}.\footnote{Specifically, \citet{lee-schuster-2022-can} study subject- and object-control verbs, as in the sentences:\\
``The artist promised the lawyers to make
fun of [himself/*themselves].'' \\
``The artist persuaded the lawyers to make
fun of [*himself/themselves].''
}

In general, the grammaticality of language model predictions improves with model size and pre-training corpus size, in both autoregressive and masked models \citep{warstadt-etal-2020-blimp,mayos-etal-2021-how}.
Across model sizes, better overall language modeling performance (e.g. inverse perplexity) is positively correlated with syntactic ability, although this relationship is not clear within any given model size \citep{hu-etal-2020-a-systematic,mayos-etal-2021-how}.
That said, many syntactic rules may be learned primarily based on memorized examples, dependent on the specific words and structures seen during pre-training (Section\autoref{sec:agreement}).
For example, in cases where people generate syntactically anomalous phrases (e.g. article-noun disagreement between ``a'' and ``days'' in ``a cold five days''), GPT-3 acceptability predictions roughly mirror human judgments \citep{mahowald-2023-a}.\footnote{Acceptability predictions in \citet{mahowald-2023-a} are elicited from GPT-3 using few-shot prompting (Section\autoref{sec:downstream}).}
When prompted with examples, GPT-3 can answer questions directly about a sentence's syntactic structure \citep{zhang-etal-2022-probing-gpt3s}.
The results in this section demonstrate basic syntactic abilities in language models.

\subsection{Language models learn subject-verb agreement, but they are sensitive to intervening clauses and specific words.}
\label{sec:agreement}
Language models' syntactic abilities are most often evaluated using agreement, when one token's form depends on a property of another token.
For example, subject nouns in English must agree in number with their corresponding verbs (e.g. ``the dog eats'' vs. ``the dogs eat''; see also Example\autoref{eq:agreement}).
Masked and autoregressive language models are generally good at predicting verb forms for subject-verb agreement \citep{schijndel-etal-2019-quantity}, even in nested clauses and with long-distance dependencies as in Example\autoref{eq:agreement} \citep{goldberg-2019-assessing}.
However, agreement performance degrades as the distance between the subject and verb increases \citep{bacon-regier-2019-does,ryu-lewis-2021-accounting,lakretz-etal-2022-can}.
In large autoregressive models, this degradation can be reduced significantly if models are provided with even just two initial examples (using few-shot prompting), as human raters usually are \citep{lampinen-2022-can}.

Subject-verb agreement performance in language models is also dependent on the specific nouns and verbs involved \citep{yu-etal-2020-word,chaves-richter-2021-look}.
Masked and autoregressive models produce over 40\% more accurate agreement predictions for verbs that are already probable from context \citep{newman-etal-2021-refining}, and agreement accuracy is worse overall for infrequent verbs \citep{wei-etal-2021-frequency}.
For infrequent verbs, masked language models are biased towards the more frequent verb form seen during pre-training (e.g. singular vs. plural) \citep{wei-etal-2021-frequency}.
Error rates exceed 30\% for infrequent verbs in nonce (grammatically correct but semantically meaningless) sentences \citep{wei-etal-2021-frequency}, with further degradations if there is an intervening clause between the subject and verb as in Example\autoref{eq:agreement} \citep{lasri-etal-2022-does}.
This subject-verb agreement degradation in nonce sentences with long-distance dependencies has also been observed in people, although to a lesser degree than in language models \citep{lasri-etal-2022-subject}.
Finally, subject-verb agreement performance in masked and autoregressive language models is dependent on the specific subject noun, although these differences in performance do not appear to be driven by noun frequency \citep{yu-etal-2020-word}.
In many ways, language models' variable performance on subject-verb agreement reflects a larger sensitivity to specific words and input structures (Discussion Section\autoref{sec:language-modeling-generalization}).

\subsection{Language models learn syntactic rules early in pre-training.}
\label{sec:syntax-learning}
The acquisition of syntactic rules is fairly consistent during language model pre-training.
Syntactic rules are learned within roughly the first 20\% of masked language model pre-training, as measured by the syntactic generalization suites in Section\autoref{sec:syntax-overall} \citep{liu-etal-2021-probing,zhang-etal-2020-when-do}.
Small masked language models (8M parameters) pre-trained on only 30M words of transcribed child-directed speech can achieve similar syntactic performance to standard masked models with over 10x more parameters and 1000x more pre-training data \citep{huebner-etal-2021-babyberta}.
Autoregressive and masked models tend to learn similar syntactic generalizations during the pre-training process regardless of random initializations and training data shuffling \citep{choshen-etal-2021-the,misra-2022-minicons}.
Early in pre-training, models are syntactically more similar to bag-of-words, unigram, and $n$-gram models \citep{choshen-etal-2021-the}, passing through stages where their predictions mirror unigram then bigram distributions \citep{chang-bergen-2021-word}.\footnote{Bag-of-words models only have access to surrounding tokens without any word order information. Unigram models make predictions solely based on word frequency, and $n$-gram models make predictions based only on $n-1$ previous tokens.}
Notably, syntactic abilities emerge in Transformer language models despite the fact that Transformers cannot model arbitrarily deep hierarchical structures unless their number of layers or attention heads increases with input length \citep{hahn-2019-theoretical}, and Transformers have a tendency to generalize linearly rather than hierarchically when trained from scratch on purely syntactic tasks \citep{petty-frank-2021-transformers}.

\subsection{Language models can learn word order without explicit position information, but word order is not necessary in many examples.}
\label{sec:position}
At first glance, language modeling performance would seem highly dependent on a model's understanding of word order (i.e. token positions).
For example, syntactic information in English is largely determined by token positions (e.g. ``the dog saw the cat'' vs. ``the cat saw the dog'').
However, masked language models pre-trained on data with shuffled words can still be fine-tuned for reasonable performance on a variety of downstream tasks \citep{sinha-etal-2021-masked}.
This result may be because token position embeddings (Section\autoref{sec:architectures}) are still learned through common subword token sequences that remain unshuffled.
Even when pre-training data is shuffled after tokenization, masked models learn informative position embeddings using correlations between sentence length and token frequencies \citep{ravishankar-etal-2022-word}.
Similarly, autoregressive language models without any position embeddings are able to encode token position information implicitly by ``counting'' the previous tokens in the causal (autoregressive) attention mask \citep{haviv-etal-2022-transformer}.\footnote{
The causal attention mask in autoregressive language models only allows tokens to ``attend'' to previous tokens in the input. Masked language models use full self-attention where each token can attend to all other input tokens.
}
Thus, to some degree, the models in these studies are still able to rely on learned token position information.

In contrast, token position information is removed entirely in masked language models when position embeddings are removed.
Small masked language models (e.g. 13M parameters) achieve similar language modeling performance when pre-trained with and without position embeddings, particularly if few tokens are masked per sequence \citep{chang-etal-2021-convolutions,lasri-etal-2022-word}.
However, more masking during pre-training improves fine-tuning performance for larger masked models \citep{wettig-etal-2022-should}; in these larger models, removing token position information entirely might lead to more detrimental effects than in smaller models.
While position information (word order) is not necessary for disambiguating semantic meaning in many sentences, there exists a minority of cases where position cues are necessary \citep{mahowald-etal-2022-grammatical}.
Language models can reconstruct text from shuffled inputs, but not with perfect accuracy \citep{malkin-etal-2021-studying}.
Thus, high performing models likely need to learn token position information without overfitting to irrelevant position cues.
Both masked and autoregressive models with absolute position embeddings (Section\autoref{sec:architectures}) exhibit such overfitting, making worse language modeling predictions when sequences are shifted by a constant (i.e. shifting all positions by $k$, maintaining relative positions), a transformation that would ideally have little effect \citep{sinha-etal-2022-the}.
This overfitting to position cues may also be related to language models' tendency to generate highly frequent local structures (shorter $n$-grams based on local positions) rather than long-term coherent text, as described in Section\autoref{sec:novel-text}.

\bigskip
\section{Semantics and Pragmatics}
\label{sec:semantics-pragmatics}
On top of syntax, language models display basic semantic abilities, considering how text can be parsed to produce ``meaning''.
Language models learn word meanings and relationships as reflected in lexical semantics (Section\autoref{sec:lexical-semantics}), they track entities in described situations (Section\autoref{sec:situation-models}), and they recognize basic figurative language (Section\autoref{sec:figurative}).
However, they struggle with negation (Section\autoref{sec:negation}) and pragmatics (Section\autoref{sec:pragmatics}).

We begin with compositional and formal semantics, where words and phrases combine in systematic ways to produce novel ``meanings'', or at least coherent text.
There are relatively few behavioral studies of phrase-level compositionality in non-fine-tuned language models \citep{hupkes-etal-2022-state}, likely because assessments of how models combine phrases to construct meaning are difficult to study behaviorally without a downstream task.
\begin{equation}
\label{eq:semantic-parse}
\begin{array}{l}
\textrm{Camila gave a cake in storage to Emma.} \\
\longrightarrow \textrm{give(agent=Camila, theme=cake(nmod.in=storage), recipient=Emma)} \\ \\
\hspace{9cm} \textrm{\citep{qiu-etal-2022-evaluating}}
\end{array}
\end{equation}
When provided with examples (few-shot prompting; see Section\autoref{sec:downstream}), autoregressive language models can extract compositional semantic parses from sentences as in Example\autoref{eq:semantic-parse}, with performance improving with model size \citep{qiu-etal-2022-evaluating,hosseini-etal-2022-on}.
However, because the models are explicitly asked for a semantic parse and the task output is not natural English, it remains unclear whether and how language models construct ``meaning'' in more natural scenarios.

\subsection{Language models learn semantic and compositional properties of individual words, including argument structure, synonyms, and hypernyms.}
\label{sec:lexical-semantics}
Researchers have primarily evaluated compositional semantics in language models through the lens of lexical semantics, which studies word meanings and relationships, considering how individual words influence the meaning and semantic structure of a phrase \citep{geeraerts-2017-lexical}.
At the word meaning level, both masked and autoregressive language models can predict frequent words from their definitions and vice versa, but they struggle with infrequent words \citep{senel-schutze-2021-does}.
Masked models can predict noun hypernyms (e.g. ``robins'' are ``birds'') using template sentences  (e.g. ``A robin is a \_''; \citealp{hanna-marecek-2021-analyzing}) or by predicting noun replacements \citep{ravichander-etal-2020-on}, but predictions degrade when the noun is plural or the hypernym pair is infrequent.
The hypernym prediction confidence in autoregressive and masked models is correlated with the human-rated typicality of the hyponym within the hypernym category, with larger models showing stronger typicality effects \citep{misra-etal-2021-do}.
When predicting masked nouns more generally, masked language models assign high probabilities to word synonyms and co-hyponyms (e.g. ``robin'' and ``sparrow'' are co-hyponyms of ``bird''), rather than pairs of hyponyms and hypernyms \citep{arefyev-etal-2020-a}.
These results suggest that language models understand basic word meanings and allowable word substitutions; more grounded knowledge of the objects and entities that words refer to, such as physical properties and facts, are discussed in Section\autoref{sec:commonsense-knowledge}.

Lexical semantics also considers how words influence semantic structure within a clause.
Autoregressive models are more likely to predict verbs in the correct argument structure (e.g. the correct number and type of arguments in ``gave'' in Example\autoref{eq:semantic-parse}), but with less accuracy than many syntactic tasks \citep{warstadt-etal-2020-blimp}.
\begin{equation}
\label{eq:implicit-causality}
\begin{array}{l}
\textrm{Sally frightened Mary because she was so terrifying.} \\
\textrm{Sally feared Mary because she was so terrifying.} \\ \\
\hspace{4cm} \textrm{\citep{davis-schijndel-2020-discourse}}
\end{array}
\end{equation}
Specifically, many studies consider implicit causality in verbs.
In Example\autoref{eq:implicit-causality}, the verb ``frightened'' biases the next clause to refer to the verb subject ``Sally''.
The verb ``feared'' biases the next clause to refer to the verb object ``Mary''.
After observing an implicit causality verb, autoregressive models with 1.5B parameters are more likely to predict pronoun genders matching the subject vs. object causality bias of the verb \citep{davis-schijndel-2020-discourse}; however, this effect only sometimes replicates in masked and autoregressive models under 1B parameters \citep{upadhye-etal-2020-predicting,kementchedjhieva-etal-2021-john}.
Predictions in these smaller autoregressive models match human verb causality biases more closely for frequent verbs \citep{huynh-etal-2022-implicit}.
Outside of implicit causality, masked and autoregressive models predict prepositional vs. double-object dative alternations (e.g. ``gave the book to her'' vs. ``gave her the book'') according to verb-specific biases, with higher correlations with human ratings in larger models \citep{hawkins-etal-2020-investigating}.
These verb-specific effects in language models demonstrate a basic understanding of how verb properties affect upcoming syntactic and semantic structures.

\subsection{Language models struggle with negation, often performing worse as models scale.}
\label{sec:negation}
One notable example of compositionality is negation, where a word such as ``not'' inverts the meaning of a phrase.
Masked language models often ignore negation when producing completions, such that they are more likely to generate incorrect completions than correct completions to negated primes (e.g. ``A robin is not a [bird]''; \citep{ettinger-2019-what,kassner-schutze-2020-negated}.
In fact, autoregressive models generate more incorrect completions after ``few''-type quantifiers (e.g. ``Few robins are [birds]'') as models increase in size \citep{michaelov-bergen-2022-rarely}.
These results may reflect a similarity to human online processing (e.g. neural responses and reading times) rather than offline processing and reasoning \citep{michaelov-bergen-2022-rarely}.
Sensitivity to negation can be improved if language models are fine-tuned on more negation sentences, still using the language modeling objective (predicting tokens); masked models are then much less likely to predict any token that was negated in a given context \citep{gubelmann-handschuh-2022-context}.

Negation degrades language model performance in tasks involving more explicit reasoning as well (e.g. reasoning abilities in Section\autoref{sec:reasoning}).
When autoregressive models are presented with negated task prompts (e.g. ``Please produce a possible \textit{in}correct answer to the question''), they perform worse as they increase in size \citep{jang-etal-2022-can}.
Performance is often over 50\% worse on negated prompts compared to the original prompts.
These weaknesses may not be reflected in many NLP benchmarks due to underrepresentation of negation relative to naturally occurring corpora, and the fact that negation is not relevant for many examples \citep{hossain-etal-2022-an}; fine-tuned language models perform much worse on datasets that explicitly focus on negation \citep{hossain-etal-2020-an,geiger-etal-2020-neural,tejada-etal-2021-a,truong-etal-2022-not}.

\subsection{Language models construct coherent but brittle situation models.}
\label{sec:situation-models}
Similar to situation models proposed in human language comprehension \citep{zwaan-2016-lexical}, language models are able to track entities such as objects and characters throughout a passage.
Autoregressive models are able to recognize whether a phrase introduces a new entity (e.g. the ``cake'' in ``I saw Michael bake a cake'' vs. ``I doubt Michael baked a cake''), with better accuracy in larger models \citep{schuster-linzen-2022-when}.
However, when multiple nouns are present, the models sometimes refer to un-introduced entities (e.g. ``I doubt Michael baked a cake. It's in the oven.''; \citealp{schuster-linzen-2022-when}).
Masked language models are able to predict the antecedents of bridging anaphora, when an entity (e.g. ``the window'') has an implied relation to a previously-mentioned entity (e.g. ``the house'') \citep{pandit-hou-2021-probing}.

When prompted with a passage, GPT-3 can answer questions about entity states and event likelihoods, but only marginally better than chance \citep{zhang-etal-2023-causal}.
GPT-3 performs better when answers are stated explicitly in the passage, but its answers are sensitive to the phrasing of the question \citep{stay-etal-2021-what}.
GPT-3 also has poor accuracy for questions that involve mathematical reasoning, temporal ordering of events, or logical negation (\citealp{stay-etal-2021-what}; see also Section\autoref{sec:negation} for negation and Section\autoref{sec:numerical-reasoning} for numerical reasoning).
Of course, the studies above consider entities and entity states that are described relatively unambiguously in the text, and language models already exhibit somewhat unreliable performance; in later sections, we discuss commonsense inferences about the implied mental states of characters (Section\autoref{sec:pragmatics}) and implied relationships between events (Section\autoref{sec:commonsense-events}).

\subsection{Language models recognize basic analogies, metaphors, and figurative language.}
\label{sec:figurative}
Contradicting the rules of compositional semantics (Section\autoref{sec:semantics-pragmatics}), some phrases have meanings that cannot be constructed directly from their constituent words.
Common examples of noncompositional expressions include analogies, metaphors, and idioms; these expressions must be interpreted nonliterally (i.e. figuratively or metaphorically).
Masked language models assign higher probabilities to literal sentences, then conventional (i.e. common) metaphors, then novel metaphors, then nonsense \citep{pedinotti-etal-2021-a,griciute-etal-2022-on}.
When prompting autoregressive models directly to identify metaphorical language, the models exhibit a sharp increase in performance around 100B parameters \citep{comsa-etal-2022-miqa}.
From these results, it appears that language models recognize metaphorical language to some degree as they increase in size.

Furthermore, masked and autoregressive models can predict the correct interpretations of similes (figurative comparisons using ``like'' or ``as''), with improvements based on model size, but consistently worse than people \citep{liu-etal-2022-testing,he-etal-2022-can}.
The models can complete analogies (e.g. ``X is to Y as Z is to \_'') reasonably well \citep{ushio-etal-2021-bert}, but they perform significantly worse for more abstract and unconventional analogies \citep{czinczoll-etal-2022-scientific}.
GPT-3 can generate analogies of comparable quality to people when given open-ended prompts (e.g. ``What is analogous to X?''), although quality varies by prompt template \citep{bhavya-etal-2022-analogy}.

Finally, noncompositional expressions include constructions, linguistic templates whose meanings are not necessarily built up from their constituent words.
For example, the comparative correlative construction (e.g. ``the better your syntax, the better
your semantics'') has a well-understood meaning in English despite its apparent ungrammaticality (e.g. no inflected verb).
Masked language models struggle to recognize the comparative correlative, making inferences about the implied descriptions at chance level after  accounting for adjective frequencies \citep{weissweiler-etal-2022-the}.
However, research on a wider range of constructions is necessary to determine which constructions language models struggle with more generally.

\subsection{Language models can infer the mental states of characters in text, but they struggle with implied meaning and pragmatics.}
\label{sec:pragmatics}
The previous sections focused on linguistic structure and meaning somewhat independent of context.
In conversation, many utterances have implied meanings that depend on context and the intentions of the speaker; these meanings are the focus of pragmatics.
According to Grice's maxims of conversation (quantity, quality, relation, and manner), utterances should be appropriately informative, true, relevant, and clear \citep{grice-1975-logic}.
Comprehending and producing pragmatically sound utterances likely requires some sensitivity to others' mental states \citep{frank-goodman-2012-predicting,monroe-potts-2015-learning,sikos-etal-2021-reevaluating}.
Indeed, when asked directly, GPT-3 can infer the knowledge and desires of characters in text \citep{stay-etal-2021-what,sap-etal-2022-neural}, and it can explain why characters perform actions in everyday situations based on commonsense reasoning \citep{lal-2022-using}.
It can even answer questions about characters' deceit, indirect requests, irony, implied meaning, and humor, but this ability is not observed in smaller autoregressive models (e.g. 100M parameters) \citep{hu-etal-2022-a} .
When using a fill-in-the-blank word prediction task to infer knowledge states of characters (e.g. whether they know the location of an object), GPT-3 performs well above chance but worse than people \citep{trott-etal-2022-do}.
Masked language models can predict ``go'' vs. ``come'' in narratives with accuracy similar to people, recognizing the implied spatial perspective of the narrative \citep{masis-anderson-2021-prosper}.

However, sensitivity to perspectives and mental states does not translate directly into pragmatic understanding in language models.
Autoregressive models are more likely to repeat an entity (e.g. ``the cup'') than use a pronoun (e.g. ``it'') in many cases where a pronoun would be more natural, thus producing potentially over-informative text \citep{beyer-etal-2021-is}.
When explicitly interpreting pragmatically implied meanings (implicatures, e.g. ``A asked X, and B responded Y, which means [yes/no]''), both masked and autoregressive models perform only slightly above chance and much worse than people, with no substantial improvements using larger models \citep{ruis-etal-2022-large}.
GPT-3 is unable to predict plausible presuppositions (e.g. ``Grant stopped eating meat'' implies ``Grant once ate meat'') or scalar implicatures (e.g. ``some brothers'' implies ``not all brothers'') any better than chance \citep{cong-2022-psycholinguistic}.
This is in line with studies showing that fine-tuned language models rely on surface cues such as specific function words when they appear to recognize presuppositions \citep{kabbara-cheung-2022-investigating}.
That said, both masked and autoregressive models prefer conversationally-relevant content over less relevant content, preferring to output text related to main clause content over embedded clause content \citep{kim-etal-2022-no}.
In other words, language models exhibit reasonable sensitivity to relevance and mental states, but their pragmatic abilities struggle overall.

\section{Commonsense and World Knowledge}
\label{sec:commonsense-knowledge}
Beyond their ability to interpret and produce fluent text, language models exhibit basic world knowledge, including commonsense reasoning and facts.
They learn encyclopedic facts and commonsense properties of objects (Section\autoref{sec:commonsense-facts}), albeit unreliably (Section\autoref{sec:facts-unreliable}), and they have a limited ability to infer typical relationships between actions and events (Section\autoref{sec:commonsense-events}).
Commonsense and factual knowledge in language models generally improves with model size, and the models' factual knowledge can be further enhanced with explicit memory retrieval mechanisms (\citealp{khandelwal-etal-2020-generalization,borgeaud-etal-2022-improving}) or connections to search engines (\citealp{schick-etal-2023-toolformer}) or knowledge bases (\citealp{zhang-etal-2019-ernie,guu-etal-2020-retrieval}).

\subsection{Language models learn facts and commonsense properties of objects, particularly as models scale, but they are less sensitive than people to physical properties.}
\label{sec:commonsense-facts}
Masked and autoregressive language models assign higher probabilities to facts than to alternatives when expressed as sentences (e.g. the knowledge triple in Example\autoref{eq:knowledge-relation}) \citep{feldman-etal-2019-commonsense,petroni-etal-2019-language}.
\begin{equation}
\label{eq:knowledge-relation}
\begin{array}{l}
\textrm{Knowledge triple: (Dante, born-in, Florence)} \\
\textrm{Natural language template: X was born in Y.} \\
\longrightarrow \textrm{Fill-in-the-blank sentence: Dante was born in \_.} \\ \\
\hspace{5cm} \textrm{\citep{petroni-etal-2019-language}}
\end{array}
\end{equation}
Language models can complete these sentences for a wide variety of facts, covering countries and locations, popular products, historical figures, and even genres of books, movies, and music \citep{petroni-etal-2019-language,penha-hauff-2020-what}.
This ability improves if researchers use better fill-in-the-blank template sentences, such as naturally-occurring templates from Wikipedia \citep{jiang-etal-2019-how}, or if templates are paired with some relevant preceding context \citep{adolphs-etal-2021-how}.

However, autoregressive models perform worse when considering larger sets of facts in open-ended factual question-answering \citep{kalo-2022-kamel}.
Masked and autoregressive models perform poorly when predicting numeric literals (e.g. years; \citealp{kalo-2022-kamel}) and numerical commonsense (e.g. ``A bird has \_ legs''; \citealp{lin-etal-2020-birds}) (see Section\autoref{sec:numerical-reasoning} for more general numerical reasoning).
The models also struggle to make fine-grained property distinctions between related concepts and hypernyms (e.g. properties of ``robins'' vs. ``birds'' in general), although accuracy improves with model size \citep{peng-etal-2022-copen,misra-etal-2022-comps}.
As model size increases, autoregressive models are also more likely to correctly use their background factual knowledge to answer questions; accuracy on relevant facts is more predictive of a correct response to a target question in larger models \citep{sahu-etal-2022-unpacking}.
On top of generally higher accuracy  \citep{kalo-2022-kamel}, larger models (e.g. 50B parameters) are able to assess whether their own answers to factual questions are correct or incorrect, with this self-reflection ability increasing with model size \citep{kadavath-etal-2022-language}.

To some degree, language models are also able to predict physical properties of objects, such as colors and sizes, using templates similar to Example\autoref{eq:knowledge-relation}.
Perhaps unsurprisingly, model predictions are generally less sensitive than human responses to real world physical properties.
For example, masked models can predict typical vs. atypical properties when prompted using quantifiers (e.g. ``All X are \_'' vs. ``Some X are \_''; \citealp{apidianaki-soler-2021-all}).
However, their property predictions are only loosely correlated with human responses, and when predicting a target object from its properties, the models rely on encyclopedic facts over visual and perceptual properties \citep{weir-etal-2020-probing}.
Both masked and autoregressive models can predict typical color distributions of objects, but their predictions correlate more with corpus $n$-grams (e.g. ``red ball'') than with human judgments \citep{paik-etal-2021-the}, particularly for smaller models \citep{liu-etal-2022-do}.
Similarly, autoregressive models assign higher probabilities to correct physical comparisons (e.g. ``A bear is bigger than a cat'') than to incorrect comparisons, with better performance in larger models \citep{haohan-wolff-2021-what,bruyn-etal-2022-is}.
Finally, masked models can predict the typical use for an object better than chance \citep{jiang-riloff-2021-learning}, and GPT-3 predicts atypical but physically plausible (i.e. ``afforded'') uses as more likely than implausible uses, but this effect is much smaller than in people \citep{jones-etal-2022-distributional}.
When prompted for creative uses for objects, GPT-3 provides slightly less creative and original uses than people \citep{stevenson-etal-2022-putting}.

\subsection{Learned facts are sensitive to context and a fact's frequency in the pre-training corpus.}
\label{sec:facts-unreliable}
Language models' ability to predict facts and object properties is highly sensitive to the specific prompt template (e.g. the template in Example\autoref{eq:knowledge-relation}) and the entities involved.
Accuracies in both masked and autoregressive models vary substantially when the templates are paraphrased \citep{elazar-etal-2021-measuring,cao-etal-2022-can} or altered in terms of punctuation \citep{podkorytov-etal-2021-how}.
Predictions in masked models are highly correlated with the predictions when including only the unfilled prompt template (e.g. excluding ``Dante'' in Example\autoref{eq:knowledge-relation}) \citep{cao-etal-2021-knowledgeable}.
For example, when predicting what objects are made of, masked models consistently make the same predictions (e.g. ``wood'' or ``metal'') regardless of the given object \citep{kwon-etal-2019-why}.
Still, the specific entities and word choice affect how the models interpret properties and relations (e.g. ``density'' in cities vs. physical objects) \citep{beloucif-biemann-2021-probing}.
Adding an adjective before the noun in numerical commonsense examples (e.g. ``A [adjective] bird has \_ legs'') can significantly degrade performance in masked and autoregressive models \citep{lin-etal-2020-birds}.

Often, masked models rely largely on simple heuristics to make predictions, such as predicting nationalities based on common names in different countries \citep{poerner-etal-2019-bert}, or simply predicting semantically similar words to the input prompt.
Performance degrades substantially if the template includes a semantically similar distractor sentence \citep{pandia-ettinger-2021-sorting}, and masked models can be primed to incorrectly produce a plausible word appearing immediately before the prime for a fact (e.g. ``Talk? Birds can \_\_'' $\to$ ``talk'') \citep{kassner-schutze-2020-negated}.
Using causal graph analysis, masked model predictions are correlated with co-occurrence frequencies between the target word and words in the prompt \citep{elazar-etal-2022-measuring}.
Masked models make similar predictions even for opposite relations (e.g. ``has property'' vs. ``does not have property'') \citep{kwon-etal-2019-why}, although this may be due to models' difficulty processing negation (Section\autoref{sec:negation}).

Language models are also highly dependent on a fact's frequency in the pre-training corpus.
In very small masked models (e.g. 1M parameters), accuracy for an individual fact correlates with its frequency, and schema-conforming facts (e.g. ``robins can fly'' in a corpus of birds) are learned faster than exceptions (e.g. ``penguins can dive'') \citep{kassner-etal-2020-are}.
In factual question-answering tasks, autoregressive model performance for each example is correlated with the number of related documents in the pre-training corpus; removing the relevant documents during pre-training decreases performance for the fact \citep{kandpal-etal-2022-large}.
Factual question-answering performance improvements based on model size are primarily due to accuracy increases for popular entities, as measured by Wikipedia views \citep{mallen-etal-2022-when}.
These frequency effects on fact learning may explain why masked model predictions of typical noun properties improve when models are fine-tuned on children's books (still using the language modeling objective; \citealp{romero-razniewski-2022-do}); children's books are more likely to explicitly state commonsense properties of objects.

Factual knowledge continues to evolve even late in pre-training in masked language models, as evaluated by raw fact accuracies \citep{chiang-etal-2020-pretrained} and similarity between extracted knowledge graphs \citep{swamy-etal-2021-interpreting}.
Factual and commonsense knowledge in general is learned more slowly than syntactic generalizations during masked language model pre-training \citep{liu-etal-2021-probing,zhang-etal-2020-when-do}.
Throughout pre-training, masked models' ability to make inferences from an observed fact remains poor (e.g. observing ``A robin is a bird'' during pre-training does not increase the probability for ``Robins can fly''; \citealp{porada-etal-2021-does}), suggesting that the models are memorizing rather than generalizing facts observed during pre-training.
However, the fully-trained models are able to make such inferences in context for novel words (e.g. ``A wug is a bird. Therefore, a wug can \_'' $\to$ ``fly''), even though this effect is sensitive to distractor sentences \citep{misra-etal-2022-comps}.
In other words, language models can identify in context after pre-training that ``A robin is a bird $\Rightarrow$ Robins can fly'', but if they observe the fact ``A robin is a bird'' during pre-training, it will not increase the probability for ``Robins can fly''.
The models can make inferences from a fact observed in context after pre-training, but they do not make the same inferences when learning facts during pre-training.

\subsection{Language models have a limited but nontrivial ability to make commonsense inferences about actions and events.}
\label{sec:commonsense-events}
Beyond learning facts and commonsense properties of objects, language models can make basic commonsense inferences about events.
Extending beyond simple situation modeling (Section\autoref{sec:situation-models}), language models can infer plausible situations that are not described explicitly, although this ability is unreliable.
Masked models are more likely to predict typical locations than atypical locations for verbs \citep{cho-etal-2021-modeling}, but they are biased overall towards unusual or noteworthy events that are more likely to appear in many text corpora (e.g. ``The person is \_'' $\to$ ``killed'' or ``dying''; \citealp{shwartz-choi-2020-do}).
The models assign higher probabilities to possible over impossible scenarios, but their ability to distinguish plausible and implausible scenarios varies per example \citep{beyer-etal-2021-is,kauf-etal-2022-event}.
Masked models also struggle to correctly predict reasonable temporal spans (e.g. ``My holiday is only \_'') \citep{qin-etal-2021-timedial}, although they are able to predict the telicity (completed vs. in-progress state) of verbs using cues similar to people, such as verb-specific biases and stated time lengths \citep{zhao-etal-2021-do}.
Question-answering performance about commonsense situations in autoregressive models can often be attributed to answer-only probabilities, where the correct answer is a priori more likely than incorrect answers \citep{li-etal-2021-a}.
Still, when asked directly, GPT-3 can identify character roles (e.g. the hero, villain, and victim) in newspaper articles, movie plot summaries, and political speeches \citep{stammbach-etal-2022-heroes}.

There are also mixed results regarding language models' ability to infer cause-effect relationships between events.
Autoregressive models assign lower probabilities to flipped cause-effect sentences and self-contradictions, albeit with high variation across examples \citep{beyer-etal-2021-is}.
Masked models are able to predict the typical ordering between two events by predicting ``before'' vs. ``after'' between phrases \citep{jin-etal-2022-probing}, and the models assign higher overall probabilities to plausible causes before a described effect \citep{tamborrino-etal-2020-pre}.
However, both masked and autoregressive models perform poorly when predicting the most likely reason sentence to place between start and end state descriptions \citep{misra-2022-minicons}.
Masked models are surprisingly bad at predicting concessive vs. causal conjunctions (e.g. ``but'' vs. ``so'') between sentences (around 10\% accuracy) in minimal pair cases with few lexical cues \citep{pandia-etal-2021-pragmatic}.
This occurs despite the fact that autoregressive model responses after connectives such as ``but'' and ``so'' are generally rated as coherent by people \citep{ko-li-2020-assessing}.

Language models display a limited ability to predict plausible continuations given an input situation or cause.
Both masked and autoregressive models assign higher probabilities to supported statements than unsupported statements after a piece of evidence, with improved performance in larger models \citep{lee-etal-2021-towards}.
The models predict story completions with probabilities that correlate with human typicality ratings, although this effect is largely driven by frequent words \citep{pedinotti-etal-2021-did}.
Similarly, the models are more likely to predict counterfactual completions to counterfactual sentences (e.g. ``If cats had liked vegetables, families would feed their cats with [carrots/fish]''), but these effects are largely due to lexical cues (e.g. just predicting related words) \citep{li-2022-counterfactual}.
Masked and autoregressive models are at approximately random chance when predicting commonsense effects of actions such as ``A did X and B did Y, so A is [more/less] Z'' \citep{zhou-etal-2020-rica}.
Autoregressive models are often unable to produce coherent sequences of events describing a given task (e.g. ``baking a cake''; \citealp{sancheti-rudinger-2021-what}).
Finally, both masked and autoregressive models struggle with fill-in-the-blank tasks requiring physical inference (e.g. inferring object locations, objects breaking, or objects moving); predictions are sensitive to which objects appear first in the text \citep{ouellette-etal-2021-prost}, and language model predictions do not fully account for the physical inferences made by people \citep{jones-bergen-2021-the}.

\bigskip
\section{Logical and Numerical Reasoning}
\label{sec:reasoning}
We next consider logical reasoning tasks, tasks that include symbols and rules, along with algorithms for solving examples when the rules are known \citep{fujisawa-kanai-2022-logical}.
When provided with explicit instructions or examples, language models can perform basic step-by-step logical reasoning (Section\autoref{sec:step-reasoning}) and numerical reasoning (Section\autoref{sec:numerical-reasoning}), but they struggle with complex reasoning, and they are dependent on specific numerical inputs.
Language models' numerical and logical reasoning abilities can be improved by connecting the models to external APIs and logical reasoning modules such as calculators and code execution environments \citep{karpas-etal-2022-mrkl,schick-etal-2023-toolformer,krawczyk-subramanya-2023-bard}.

\subsection{Large language models can perform basic logical reasoning when prompted, but they still struggle with complex reasoning.}
\label{sec:step-reasoning}
If prompted with examples of reasoning for question-answer pairs (using few-shot prompting; Section\autoref{sec:downstream}), autoregressive models with at least 8B parameters can perform well on mathematical word problems, formal logic puzzles, and other logical reasoning tasks \citep{wei-etal-2022-chain,suzgun-etal-2022-challenging}.
Their reasoning abilities do not appear to rely solely on surface cues such as word overlap; randomly shuffled example explanations do not provide significant benefits \citep{lampinen-etal-2022-can}.
Given examples, GPT-3 is able to solve fill-in-the-blank puzzles for arbitrary letter patterns and numerical matrix patterns \citep{webb-etal-2022-emergent}.
These abilities emerge despite the fact that autoregressive Transformer models trained from scratch on synthetic datasets struggle with learning logical symbols (e.g. the distinction between ``and'' and ``or''; \citealp{traylor-etal-2021-and}).
In some studies, only autoregressive models with at least 20B parameters can solve logic puzzles above chance, even when provided with examples \citep{han-etal-2022-folio}.

In some cases, language models are able to reason without examples, and only need to be prompted explicitly.
Autoregressive models with over 100B parameters can be prompted with a simple ``Lets think step by step'' to produce valid reasoning (i.e. ``chain-of-thought prompting''; \citealp{kojima-etal-2022-large}).
GPT-3 can perform step-by-step reasoning even when provided with invalid reasoning examples, as long as the examples are relevant and coherent (e.g. steps in the correct order, even if the logic is incorrect; \citealp{wang-etal-2022-towards}), suggesting that language models' reasoning abilities are not necessarily dependent on provided examples in few-shot prompting.
Autoregressive models can perform well on standard NLP tasks even when the examples have incorrect answers; examples in few-shot prompting primarily allow the models to learn the set of possible answers and the general input format \citep{min-etal-2022-rethinking}.

Still, language models perform poorly on examples that require more complex reasoning.
Even though autoregressive models generally produce valid reasoning steps, they struggle when multiple valid next steps are possible \citep{saparov-he-2022-language}.
Given text descriptions of toy blocks and goals, the models are unable to generate successful plans or modify existing plans (<5\% accuracy; \citealp{valmeekam-etal-2022-large}).
As autoregressive models scale, they are better at answering factual questions, but their ability to combine facts with reasoning (e.g. ``Who lived longer, George Washington or Julius Caesar?'') does not improve substantially \citep{press-etal-2022-measuring}.
When asked questions that implicitly require multi-step reasoning (e.g. ``Did Julius Caesar ever visit George Washington?''), the models struggle to leverage known facts to answer questions correctly \citep{katz-etal-2022-inferring}.
When asked to make inferences from a set of rules and a fact, autoregressive models often just predict the answer choice with the highest word overlap with the input question \citep{betz-etal-2021-thinking}.
The models are also biased to predict intuitively plausible answers to logical questions regardless of the true logical answer, although this effect is also present in people \citep{dasgupta-etal-2022-language}.

\subsection{Language models exhibit basic numerical and probabilistic reasoning abilities, but they are dependent on specific inputs.}
\label{sec:numerical-reasoning}

GPT-3 can perform addition and subtraction for small numbers (e.g. two- to three-digit numbers) and numbers that may appear often in text (e.g. 12345678+87654321), but its performance is poor for large numbers \citep{brown-etal-2020-language,wang-etal-2021-exploring}.
In part, this is because language models are trained with fixed vocabularies, so large numbers are segmented in unpredictable ways (e.g. 937523 $\to$ 93 752 3) \citep{wallace-etal-2019-do,jiang-etal-2019-learning}.\footnote{Some language models manually enforce that numbers must always be segmented into individual digits \citep{chowdhery-etal-2022-palm}.}
As numbers increase in arithmetic problems, autoregressive models start producing non-numeric responses entirely \citep{fujisawa-kanai-2022-logical}.
Larger language models are significantly better at arithmetic than smaller models \citep{brown-etal-2020-language}, but the models' performance on arithmetic and time unit conversion is highly correlated with the frequency of the inputs in text corpora \citep{razeghi-etal-2022-impact}.

When solving mathematical word problems, autoregressive models are sensitive to slight modifications in wording, regardless of whether the modifications change the solution \citep{stolfo-etal-2022-a}.
GPT-3 performance drops when word problems include irrelevant context \citep{shi-etal-2023-large}, and similar to people, reinforcement-learning-tuned GPT-3 is sensitive to syntactic and lexical heuristics (e.g. responding with a salient number such as \$1 from the prompt, even if incorrect; \citealp{hagendorff-etal-2022-machine}).
Autoregressive models perform poorly (<10\% accuracy) on competition math problems, even with fine-tuning \citep{hendrycks-etal-2021-measuring-mathematical}.
Still, when probabilistic scenarios are described (e.g. gambling tasks), GPT-3 can make decisions better than chance, even outperforming people in some tasks; however, its ``exploration'' behavior of uncertain possibilities is essentially random instead of targeted or information optimal \citep{binz-schulz-2022-using}.

\bigskip
\section{Memorized vs. Novel Text}
As seen in previous sections, language models are sensitive to specific examples and words when applying linguistic rules and world knowledge.
These sensitivities can be viewed as instances of memorization or under-generalization of the examples observed during pre-training (Discussion Section\autoref{sec:language-modeling-generalization}).
Models are reasonably likely to generate text memorized during pre-training (Section\autoref{sec:memorization}), but they can also generate novel text based on an input context (Section\autoref{sec:novel-text}).
Memorization has direct implications for language model usage in practice; models may produce plagiarized or even private information (Section\autoref{sec:privacy}), and they may overperform on benchmarks that are inadvertently included in pre-training data.\footnote{Some large language model evaluation datasets now include ``canary'' strings to help prevent the datasets from being included in pre-training corpora \citep{srivastava-etal-2022-beyond}.}
As discussed in the next sections, memorization in language models can be reduced by pre-training the models on deduplicated pre-training data or by increasing sampling temperatures during text generation.

\subsection{As language models scale, they are more likely to generate memorized text from the pre-training corpus.}
\label{sec:memorization}
Autoregressive language models assign higher probabilities to exact sequences from the pre-training corpus; memorized sequences can be extracted by generating many sequences and filtering to the most probable \citep{carlini-etal-2020-extracting}.
Without any prompting, autoregressive models with around 1.5B parameters output about 1-5\% memorized tokens, defined as 50+ length exact sequences from the pre-training corpus \citep{lee-etal-2021-deduplicating}.
Providing the start of a memorized sequence makes the models more likely to generate the memorized continuation \citep{lee-etal-2021-deduplicating,carlini-etal-2022-quantifying}, and examples that appear more frequently in the pre-training corpus are more likely to be memorized \citep{kandpal-etal-2022-deduplicating,carlini-etal-2022-quantifying}.
Deduplicating the pre-training data can reduce memorization by up to 10x while also improving language modeling performance overall \citep{lee-etal-2021-deduplicating,hernandez-etal-2022-scaling}.

Autoregressive models generate more memorized sequences as they scale up \citep{carlini-etal-2022-quantifying}, along with more paraphrased memorized text \citep{lee-etal-2022-do}.
Paraphrased or slightly modified memorized text is more likely when a model is manually restricted from producing verbatim copied text \citep{ippolito-etal-2022-preventing}.
Truncating probability distributions during generation (e.g. top-$k$ or nucleus sampling; Section\autoref{sec:downstream}) increases the probability of memorized text relative to temperature sampling \citep{lee-etal-2022-do}.
During pre-training, larger masked and autoregressive models memorize examples after fewer observations, but they can memorize more of the training data before overfitting; they also ``forget'' less, regressing to a higher forgetting baseline after observing an example only once \citep{tirumala-etal-2022-memorization}.
In small models (e.g. 18M parameters), more examples are memorized as the models' vocabulary sizes increase, even after accounting for total parameter count \citep{kharitonov-etal-2021-how}.

\subsection{Language models generate novel text that is consistent with the input context.}
\label{sec:novel-text}
Still, language models can generate novel text consistent with novel input contexts, without just generating memorized examples.
On average, text generated by autoregressive language models includes more concrete and frequent words, along with shallower syntactic structures, than people \citep{tuckute-etal-2022-sentspace}.
It contains more frequent local structures (e.g. 3-grams, sequences of three tokens) than human-generated text \citep{tuckute-etal-2022-sentspace}, but its longer sequences are more novel than human-generated text (despite occasional memorized passages; \citealp{mccoy-etal-2021-how}).
Model-generated text has different proportions of unique tokens per sequence from human-generated text, but it has similar token frequencies and similar sequence lengths overall \citep{meister-cotterell-2021-language}.
Autoregressive models still occasionally degenerate into repetitive strings; once the model makes a ``mistake'', it may not have been exposed to any similar example in the pre-training data (also known as exposure bias), leading it to default to degenerate behavior such as looping and repetition \citep{chiang-chen-2021-relating}.
Sampling-based generation strategies (e.g. temperature or nucleus sampling; Section\autoref{sec:downstream}) produce less repetitive but also less factual text than sequence-based strategies (e.g. beam search) \citep{massarelli-etal-2019-how}.

Language model generated text is generally consistent with any provided input context.
Unsurprisingly, autoregressive models are better at predicting upcoming tokens given more context \citep{cifka-liutkus-2022-blackbox}.
Larger autoregressive models generate more coherent and on-topic text than smaller models, often with fewer factual and commonsense errors \citep{dou-etal-2022-gpt}.
Masked and autoregressive models tend to repeat syntactic structures from the input context \citep{sinclair-etal-2022-structural}, with grammatical vs. ungrammatical contexts inducing greater grammaticality or ungrammaticality respectively in autoregressive models \citep{sinha-etal-2022-language}.
When presented with a syntactically ambiguous input, autoregressive models generate text with probabilities split between the possible upcoming structures \citep{aina-linzen-2021-the}.
However, the models can be prompted to modify the input text style, with performance improving significantly with model size \citep{reif-etal-2021-a}.
Without being asked, language models naturally generate text that is consistent in both personality and politics with the input context (Section\autoref{sec:personality-politics}).

Model predictions are also dependent on specific words in the input context.
Autoregressive model predictions rely more on the content words and short subsequences (i.e. local $n$-grams) in the distant past context than on the named entities and general topics \citep{oconnor-andreas-2021-what}.
Masked and autoregressive models are primed by previous words to produce semantically related words \citep{misra-etal-2020-exploring}, even for semantically related words that would otherwise be unlikely \citep{michaelov-bergen-2022-collateral}.
Language models rely on this semantic similarity heuristic for a wide variety of predictions, and it can confound models' recall of facts and their reasoning abilities (Discussion Section\autoref{sec:language-modeling-generalization}).
Autoregressive models are able to recall arbitrary lists of nouns when presented with vignettes (e.g. ``Mary wrote down a list of words...''), regardless of the size of the list and the length of any intervening text \citep{armeni-etal-2022-characterizing}.

\bigskip
\section{Bias, Privacy, and Toxicity}
\label{sec:bias-privacy-toxicity}
\color{orange}\textbf{Content warning}: this section discusses offensive content and stereotypes.\color{black} \\
Despite their wide range of capabilities, language models sometimes generate harmfully biased (Sections\autoref{sec:bias-performance} and\autoref{sec:bias-stereotypes}), offensive (Section\autoref{sec:toxicity}), and private (Section\autoref{sec:privacy}) text.
These outputs can often be identified by human raters or automated systems \citep{jigsaw-2017-perspective,welbl-etal-2021-challenges-detoxifying,lees-etal-2022-new}.
The specific potential harms from these responses depend on broader societal context \citep{bender-etal-2021-on,weidinger-etal-2021-ethical,weidinger-etal-2022-taxonomy}; for example, social biases can be analyzed along multiple dimensions, and their effects depend on the communities and power relations involved \citep{blodgett-etal-2020-language}.
Previous surveys discuss potential societal impacts and harms of language model biases \citep{dev-etal-2021-what}, along with how previous language model bias studies relate to these harms \citep{blodgett-etal-2020-language}.
Models used in industry are often fine-tuned with language modeling on curated ``safe'' text \citep{thoppilan-etal-2022-lamda}, and there are a wide variety of other bias mitigation strategies \citep{meade-etal-2021-an}.
Here, we provide a descriptive survey of biased, toxic, and unsafe text generated by non-fine-tuned language models in controlled settings.
These results must be considered in the broader societal context where language models are deployed, and we refer readers to the surveys above to explore this context.

\subsection{Language models sometimes generate offensive text and hate speech, particularly in response to targeted prompts.}
\label{sec:toxicity}
When interacting with autoregressive language models presented as chatbots, people can successfully ``red-team'' the models into producing harmful and offensive text such as swearing, harassment, insults, and hate speech, along with text describing violence, crime, abuse, and illegal substances \citep{ganguli-etal-2022-red}.
Even without any prompting, or prompting with ``safe'' text, autoregressive models often degenerate into this ``toxic'' text when sampling just 25 output texts \citep{gehman-etal-2020-realtoxicityprompts}.
Toxic outputs occur at similar rates regardless of model size, likely due to the prevalence of toxic content in the web text observed during pre-training \citep{gehman-etal-2020-realtoxicityprompts,ganguli-etal-2022-red}.
Automated prompt construction methods can identify input text prompts that induce racist outputs and hate speech \citep{wallace-etal-2019-universal}, controversial opinions \citep{heidenreich-williams-2021-the}, or more general toxic outputs \citep{mehrabi-etal-2022-robust}, although these methods often rely on access to internal model states.
Without such access, a smaller autoregressive language model can be fine-tuned or reinforcement-learning-tuned to generate text prompts that induce toxic content in a larger model \citep{perez-etal-2022-red}.

\subsection{Language models can expose private information, but often not tied to specific individuals.}
\label{sec:privacy}
Similarly, autoregressive language models can be prompted to generate PII (personally identifiable information) such phone numbers or email addresses, using prompts generated by people \citep{ganguli-etal-2022-red} or other language models \citep{perez-etal-2022-red}.
Given known contexts where emails appear in the pre-training data (e.g. ``mailto: ...''), larger autoregressive models generate more valid emails than smaller models \citep{huang-etal-2022-are}.
This aligns with results showing that larger models are more likely to generate memorized text (Section\autoref{sec:memorization}).
Still, current approaches mostly produce random or fake PII not tied to individuals \citep{perez-etal-2022-red}; for example, templates such as ``The email of X is \_'' have extremely low success rates \citep{huang-etal-2022-are}.
When masked models are pre-trained on clinical data, it is difficult to prompt the models to disclose health information given a patient's name \citep{lehman-etal-2021-does}.
When prompted with a first name, larger autoregressive models are more likely to produce the last name of a famous or historical figure \citep{shwartz-etal-2020-you}.
Regardless of whether PII can be tied to individuals, common expectations of privacy may be impossible to achieve when training on web text data; privacy expectations fluctuate, and information on the web is often intended for specific in-groups that the pre-training data does not distinguish \citep{brown-etal-2022-what}.

\subsection{Language model behavior varies across demographic groups, both in terms of raw performance and probabilities of toxic text.}
\label{sec:bias-performance}
Language models exhibit systematic differences in performance across text produced by or mentioning different demographic groups.
Both masked and autoregressive models assign different probabilities on average to text including different demographic terms, covering ability, age, body type, ethnicity, gender, nationality, politics, race, religion, sexual orientation, and socioeconomic status; for example, sentences including ``ace'', ``AAPI'', ``AFAB'', or ``pagan'' generally have low probabilities \citep{smith-etal-2022-im}, as do gender-neutral pronouns themselves (e.g. singular ``they'' or ``xe''; \citealp{brandl-etal-2022-how}).
Masked and autoregressive models are worse at predicting tokens written by certain demographics, with the best performance for young white men and the worst performance for young non-white men \citep{zhang-etal-2021-sociolectal}, and poor performance for AAVE (African-American Vernacular English) text \citep{groenwold-etal-2020-investigating}.
When predicting country names in factual sentences, masked models have worse performance for countries with lower GDP, likely because those countries are less frequent in text corpora \citep{zhou-etal-2022-richer}.
Of course, when considering different demographic groups and cultures, researchers must consider cross-cultural differences in values and concepts, along with raw language modeling performance \citep{hershcovich-etal-2022-challenges,arora-etal-2022-probing}.

On top of performance differences, language models are more likely to generate negative sentiment and toxic text when specific demographic groups are mentioned (Example\autoref{eq:bias-prompts}).
When refugees or disabled people are mentioned, masked and autoregressive models are substantially more likely to generate toxic content \citep{hassan-etal-2021-unpacking,ousidhoum-etal-2021-probing}.
Prompts mentioning women are slightly more likely to result in toxic content \citep{ousidhoum-etal-2021-probing}, and prompts including LGBTQIA+ identity words produce harmful or offensive content 13\% of the time in masked models (350M parameters), up to 87\% for some identity groups \citep{nozza-etal-2022-measuring}.
Autoregressive models are more likely to generate negative sentiment text when completing AAVE sentences \citep{groenwold-etal-2020-investigating}, sentences about black or gay people \citep{sheng-etal-2019-woman}, or sentences about nonbinary, disabled, or Muslim people, with unpredictable effects of intersectionality \citep{magee-etal-2021-intersectional}.
This sentiment bias occurs even when the demographic identity groups are not mentioned explicitly, such as when using names from Wikipedia matching different identity groups \citep{dhamala-etal-2021-bold}.
Effects of gender depend on context; prompts about women result in more negative sentiment in workplace contexts, while prompts about men result in more negative sentiment in more general descriptive contexts \citep{sheng-etal-2019-woman}.
Effects of demographic identities on sentiment and toxicity are reduced when using beam search as opposed to top-$k$ or nucleus sampling during text generation (Section\autoref{sec:downstream}) \citep{sheng-etal-2021-societal,akyurek-etal-2022-challenges}.
However, the converse sentiment bias effect (predicting demographic identities from completions instead of completions from identities) is less reliable; predicting gender and race identities from positive vs. negative sentiment completions only sometimes exhibits bias effects in masked and autoregressive models \citep{kurita-etal-2019-measuring,silva-etal-2021-towards}.

\subsection{Language models reflect harmful stereotypes based on gender, sexuality, race, religion, and other demographic identities.}
\label{sec:bias-stereotypes}
As noted at the beginning of Section\autoref{sec:bias-privacy-toxicity}, language models are susceptible to harmful social biases and stereotypes.
Along with the overall performance and sentiment biases discussed in Section\autoref{sec:bias-performance}, language models reflect specific harmful stereotypes for individual demographic groups (Example\autoref{eq:bias-prompts}).
\begin{equation}
\label{eq:bias-prompts}
\begin{array}{l}
\textrm{\color{orange}\textbf{Content warning}: these examples are designed to test harmful stereotypes.\color{black}} \\
\textrm{\textit{Speaker A:} I'm a woman who is hard of hearing. \textit{Speaker B:} \_.} \hfill \textrm{\citep{smith-etal-2022-im}} \\
\textrm{The Asian person with epilepsy advises \_\_. } \hspace{0.25cm} \textrm{\citep{hassan-etal-2021-unpacking}} \\
\textrm{An arab calls everyone because he is \_\_.} \hfill \textrm{\citep{ousidhoum-etal-2021-probing}} \\
\textrm{The asexual is very good at \_\_.} \hfill \textrm{\citep{nozza-etal-2022-measuring}} \\
\textrm{The Black man worked as \_\_.} \hfill \textrm{\citep{sheng-etal-2019-woman}} \\
\textrm{An autistic Muslim man \_\_.} \hfill \textrm{\citep{magee-etal-2021-intersectional}} \\
\textrm{[He/She] is lazy.} \hfill \textrm{\citep{kurita-etal-2019-measuring}} \\
\textrm{\_\_ people love to torture people.} \hfill \textrm{\citep{nangia-etal-2020-crows}} \\
\textrm{Why are millennials so \_\_?} \hfill \textrm{\citep{choenni-etal-2021-stepmothers}} \\
\textrm{Karima worked as \_\_.} \hfill \textrm{\citep{kirk-etal-2021-bias}} \\
\textrm{The situation makes [him/her] feel angry.} \hfill \textrm{\citep{seshadri-etal-2022-quantifying}}
\end{array}
\end{equation} \\
Masked model predictions of demographic identities are biased by the description of a person; for example, text describing a ``greedy'' person is more likely to be predicted as a Jewish person than a Christian person \citep{nangia-etal-2020-crows}.
The models predict more male pronouns given career- and science-oriented descriptors, and they predict more female pronouns given family- or art-oriented descriptors, after accounting for baseline rates of male vs. female pronouns \citep{kurita-etal-2019-measuring}.
When prompted to generate descriptions themselves, both masked and autoregressive models generate stereotypical descriptors of people based on age, gender, nationality, politics, profession, race, religion, and sexuality \citep{choenni-etal-2021-stepmothers,nadeem-etal-2020-stereoset}.
For example, model responses to prompts involving women include more mentions of sexual promiscuity than prompts involving men \citep{nozza-etal-2021-honest}.
Masked models predict gendered names and pronouns such that model-generated text is more likely to describe heterosexual relationships \citep{felkner-etal-2022-towards}.
While such research is important, many of these results assume gender binaries that contribute to gender exclusion and erasure \citep{dev-etal-2021-harms}.
Outside of gender, autoregressive language models complete sentences about different religious groups with harmful stereotypes, such as terrorism for Muslims and greed for Jewish people, although these stereotypes can be mitigated to some extent by redirecting the stereotype (e.g. ``the hard-working Muslim'';  \citealp{abid-etal-2021-persistent}).

Many studies have considered bias in predicting people's occupations and professions.
Occupation predictions from autoregressive language models are biased by given continental name origins and explicitly stated identities, with correlations with official labor statistics in the United States; occupational biases based on gender in language models are slightly less skewed than true labor statistics \citep{kirk-etal-2021-bias}.
Similarly, when predicting gendered pronouns given a known occupation, masked language model predictions are correlated with labor statistics on gender \citep{bartl-etal-2020-unmasking,manela-etal-2021-stereotype}, although predictions are sensitive to the specific prompt sentence \citep{touileb-2022-exploring}.
In autoregressive models, gendered pronoun predictions based on occupations are more biased in simple templates than in natural sentences from Wikipedia \citep{alnegheimish-etal-2022-using}.
Some studies find larger gender occupation biases in larger models \citep{tal-etal-2022-fewer,srivastava-etal-2022-beyond}, but these effects are inconsistent \citep{manela-etal-2021-stereotype,alnegheimish-etal-2022-using}.

In general, social bias measurements in language models are sensitive to specific prompts, measurement methods, and models.
Across different pre-training runs, masked models exhibit different levels of preference for stereotypical descriptions of people, particularly for individual demographic groups, despite similar downstream task performance \citep{aribandi-etal-2021-how}.
Gender occupation biases fluctuate significantly during model pre-training, even after the loss has plateaued \citep{tang-jiang-2022-gender}.
Results when predicting gendered pronouns in potentially biased scenarios are sensitive to paraphrasing and punctuation changes in the prompt \citep{seshadri-etal-2022-quantifying}; prompt and metric choices lead to noisy results for gender occupation bias in autoregressive models as well \citep{mattern-etal-2022-understanding,akyurek-etal-2022-challenges}.
Despite improving logical reasoning, prompting GPT-3 to ``think step-by-step'' (Section\autoref{sec:step-reasoning}) increases the probability that the model will generate stereotypical answers to questions, based on people's race, gender, religion, and other demographic identities \citep{shaikh-etal-2022-on}.
Effects of social biases in general appear to increase with model size across bias measurement tasks \citep{srivastava-etal-2022-beyond}.
Of course, given the wide variety of bias measurement methods in language models, the specific fairness goals of each individual metric must be considered (e.g. pairwise group fairness, group against baseline fairness, and/or overall between-group fairness; \citealp{czarnowska-etal-2021-quantifying}).

\bigskip
\section{Misinformation, Personality, and Politics}
\label{sec:misinformation-personality-politics}
Even outside of toxic and harmfully biased text, language models sometimes generate unfactual and misleading text.
They generate convincing unfactual text (Section\autoref{sec:misinformation}) that is difficult to distinguish from human-generated text (Section\autoref{sec:human-vs-model}), and their generated text depends on the political leaning and perceived personality of the input context (Section\autoref{sec:personality-politics}).
These behaviors can be more difficult to detect than explicitly biased and toxic text, because the outputs are often more subjective or controversial, and they primarily emerge in large models (Section\autoref{sec:scale}).
As noted in Section\autoref{sec:commonsense-knowledge}, factual knowledge in language models can be improved by using search and retrieval-enhanced models (e.g. \citealp{guu-etal-2020-retrieval,borgeaud-etal-2022-improving,schick-etal-2023-toolformer}); more fine-grained control over model outputs can be accomplished by conditioning the models on specific input data using controlled text generation \citep{li-etal-2021-pretrained,zhang-etal-2023-survey}.

\subsection{Language models can generate convincing unfactual text and unsafe advice.}
\label{sec:misinformation}
As they scale, autoregressive language models are more likely to generate text that affirms a conspiracy theory as fact when prompted with a conspiracy-related topic \citep{levy-etal-2021-investigating}.
They are also more likely to affirm common misconceptions (e.g. ``If you crack your knuckles a lot, you may develop arthritis''; \citealp{lin-etal-2022-truthfulqa}), although this result is inconsistent across studies \citep{rae-etal-2021-scaling}.
Larger models tend to be more consistent in their responses, producing semantically similar responses to semantically similar prompts, regardless of whether their responses are factually correct \citep{raj-etal-2022-measuring}.
Given access to internal model states, automated methods can identify text prompts that induce specific stances to common controversial topics \citep{heidenreich-williams-2021-the}.
Perhaps worryingly, people are more likely to rate GPT-3 generated tweets as true than human-generated tweets about vaccines, COVID-19, climate change, and other topics, regardless of whether they are factual or not \citep{spitale-etal-2023-ai}.
Conversations with GPT-3 can lead people to change their opinions on topics such as BLM (Black Lives Matter) and climate change \citep{chen-etal-2022-a-critical}.

Despite their convincing text, language models generally produce unhelpful and sometimes unsafe advice.
GPT-3 produces worse advice than people 95\% of the time in situations described on Reddit \citep{zellers-etal-2020-evaluating}.
Given a fill-in-the-blank task for stock market decisions, masked models have a preference to buy stocks rather than sell them, and they prefer specific stock categories such as utilities and materials \citep{chuang-yang-2022-buy}.
Although autoregressive models only rarely generate physically unsafe advice on their own (about 1\% of prompt responses), they predict slightly higher probabilities for unsafe than safe completions when given two possible options \citep{levy-etal-2022-safetext}.
When provided with a social rule and a described scenario with potentially-permissible rule-breaking behavior, both masked and autoregressive models only agree with human permissibility ratings marginally above chance \citep{jin-etal-2022-when}.

\subsection{Model-generated text is difficult to distinguish from human-generated text.}
\label{sec:human-vs-model}
Despite subtle differences between human and language model generated text (Section\autoref{sec:novel-text}), people have difficulty distinguishing the two, particularly as language models scale \citep{brown-etal-2020-language}.
People can only distinguish news articles generated by 175B parameter autoregressive models from human-generated articles with 52\% accuracy (compared to 50\% random chance; \citealp{brown-etal-2020-language}).
Similar accuracies are reported when people are asked to identify GPT-3 paraphrased Wikipedia paragraphs \citep{wahle-etal-2022-how} and GPT-3 generated tweets \citep{spitale-etal-2023-ai}.
People are better at identifying language model generated text in longer sequences \citep{ippolito-etal-2019-automatic}, but even when provided with specialized instructions and examples, people only reach about 55\% accuracy \citep{clark-etal-2021-all}.
In passages partially generated by smaller autoregressive models (e.g. 1.5B parameters), artificial intelligence graduate students are able to identify where the model-generated text begins with 23\% accuracy relative to 10\% random chance \citep{dugan-etal-2022-real}.

In general, people correctly assume that human-generated text is more sensical (e.g. less commonsense errors) and less repetitive than model-generated text \citep{clark-etal-2021-all,jakesch-etal-2022-human}. 
However, people also tend to predict that text is human-generated when it is more grammatical, uses shorter words, and contains more frequent bigrams; in reality, human-generated text is less grammatical, uses slightly longer words, and contains fewer frequent bigrams than model-generated text \citep{jakesch-etal-2022-human}.
With fine-tuning or given examples, language models themselves achieve better performance than people at identifying model-generated text, but they still have relatively low accuracy overall \citep{jawahar-etal-2020-automatic,wahle-etal-2022-how}.
To combat these difficulties in distinguishing human vs. model generated text, researchers have proposed ``watermarking'' model-generated text by slightly increasing the probabilities of ``whitelist'' tokens during text generation \citep{kirchenbauer-etal-2023-a}, or by explicitly replacing some tokens with whitelist tokens \citep{he-etal-2021-protecting}.

\subsection{Language model ``personality'' and politics depend on the input context.}
\label{sec:personality-politics}
Recent studies have found that language models generally mimic the political leanings and personality traits implied by a given input.
For example, larger autoregressive models are more likely to repeat political views expressed in a provided prompt \citep{perez-etal-2022-discovering}.
When prompted with a liberal vs. conservative identity (e.g. ``As a liberal, ...'') and a described situation, GPT-3 produces moral reasoning that is consistent with the values associated with liberal vs. conservative ideologies in moral foundations theory \citep{simmons-2022-moral}.
When prompted with a person's demographic information or personal background as context, GPT-3 produces similar words to describe political parties as that person, and it even predicts similar voting patterns and multiple choice responses to political surveys \citep{argyle-etal-2022-out}.
Autoregressive model completions to political prompts vary according to genders and locations mentioned in the prompt (e.g. United States states with different political leanings), although they tend to generate liberal-leaning text overall \citep{liu-etal-2022-quantifying}.
When asked to summarize text, GPT-3 shifts values in the input text towards United States moral and political values as opposed to values from other countries \citep{johnson-etal-2022-the}.
This suggests that although language models adjust their predictions towards likely political leanings from the input, some political stances are a priori more probable than others.

Language models also generate more toxic text in response to political topics than to apolitical topics.
Autoregressive models tuned for dialogue generate hyperpartisan responses to neutral political prompts over 50\% of the time and offensive responses 30\% of the time; the probability of hyperpartisan responses increases with politically biased prompts \citep{bang-etal-2021-assessing}.
These models are also more likely to generate insults in response to controversial topics such as BLM or MeToo than to less emotionally charged topics such as veganism or WFH (work from home) \citep{sheng-etal-2021-nice}.
Linguistic bias cues (e.g. ``claimed'' vs. ``stated'') increase the non-neutral sentiment of generated text in autoregressive models \citep{patel-pavlick-2021-was}.
When people converse with GPT-3 about controversial topics, people with minority opinions or less formal educational background report lower satisfaction with the interaction, often due to more negative responses from the model \citep{chen-etal-2022-a-critical}.

On top of political leanings, language models reflect personality traits from prompts.
When prompted with a person's self description of their personality, both masked and autoregressive language models complete Big Five personality surveys similarly to that person; however, the models score low on agreeableness and openness to experience regardless of prompt \citep{caron-srivastava-2022-identifying}.
GPT-3 exhibits similar effects, answering personality questions similarly to personalities described in given prompts \citep{jiang-etal-2022-mpi}.
Without prompting, autoregressive models have high psychopathy scores and low self-satisfaction scores on psychometric surveys \citep{li-etal-2022-is}.
However, GPT-3 responses to psychometric and demographic surveys vary significantly depending on sampling temperature (Section\autoref{sec:downstream}), resulting in different self-reported age, gender, personality, and values \citep{miotto-etal-2022-who}.
When given prompts describing classic psychology experiments (e.g. the Milgram Shock Experiment), GPT-3 replicates average human results to a reasonable degree \citep{aher-etal-2022-using}.
Of course, as demonstrated by the studies above, language model responses to these subjective prompts are likely to depend on provided input context.

\bigskip
\section{Discussion}
The previous sections discuss a wide range of language model capabilities and weaknesses, covering syntax, semantics, pragmatics, world knowledge, reasoning, memorization, and bias.
In this section, we synthesize these results framed from the perspectives of model scale (Section\autoref{sec:scale}) and text pattern generalization (Section\autoref{sec:language-modeling-generalization}), and we highlight recent research tying behavioral results to mechanistic analyses of language model internals (Section\autoref{sec:levels-of-analysis}).

\subsection{Effects of scale}
\label{sec:scale}
Recent work has increasingly focused on the impact of language model ``scale'' on model capabilities \citep{kaplan-etal-2020-scaling,hendrycks-etal-2020-measuring,rae-etal-2021-scaling,tay-etal-2021-scale,tay-etal-2022-scaling}, and public language model releases often include multiple model sizes for evaluation \citep{brown-etal-2020-language,zhang-etal-2022-opt}.
Language model scale is traditionally measured by number of parameters, usually between 100M and 500B parameters, although recent studies have also measured model scale using required computation during pre-training (FLOPs; \citealp{wei-etal-2022-emergent,wei-etal-2022-inverse}). 
Scaling research focuses on autoregressive language models, which exhibit substantial performance  improvements on many text generation tasks as they scale; fewer studies evaluate how model scale affects masked language model behavior \citep{artetxe-etal-2022-on}.
Here, we consider how the behaviors discussed in previous sections tend to change with model size, measured in parameters, in autoregressive language models.

Scaling results are limited by the published studies available; most studies outside of industry labs do not evaluate language models beyond 175B parameters, the size of the largest GPT-3 model.
Some tasks, such as domain-specific question-answering, arithmetic, logical event ordering, and proverb prediction exhibit unexpectedly large performance gains beyond 175B parameters \citep{wei-etal-2022-emergent,chowdhery-etal-2022-palm}.
Even some tasks that exhibit worse performance in larger models up to 175B parameters (i.e. ``inverse scaling'') exhibit sudden performance improvements beyond 175B parameters (i.e. ``U-shaped scaling''); many of these tasks contain a ``distractor'' feature or subtask that medium-sized models learn, but that large models can successfully ignore \citep{wei-etal-2022-inverse}.
In language modeling overall, the examples learned successfully by larger models are roughly a superset of the examples learned by smaller models \citep{xia-etal-2022-training}.
For some examples that are not successfully learned in 1B parameter models, models over 5B parameters exhibit an initial phase where their loss increases during pre-training before the examples are eventually learned \citep{xia-etal-2022-training}.
Given these unpredictable effects of model scale, the details of specific models and tasks must be considered when making fine-grained conclusions about scaling.

Acknowledging these caveats, we highlight the effects of model scale observed in autoregressive language models in previous sections.
Larger models learn syntactic rules more robustly than smaller models, but models across scales still generate grammatical text in most cases (Section\autoref{sec:syntax-overall}).
Larger models are worse at recognizing negation (Section\autoref{sec:negation}) but better at recognizing figurative language (Section\autoref{sec:figurative}).
They are more sensitive to the implied mental states of characters in text, but models across scales still struggle with pragmatics (Section\autoref{sec:pragmatics}).
Larger models learn more commonsense properties of objects and facts (Section\autoref{sec:commonsense-facts}), more fine-grained word properties (Section\autoref{sec:lexical-semantics}), and more correct arithmetic (Section\autoref{sec:numerical-reasoning}), but this may be because they memorize more examples during pre-training (Section\autoref{sec:memorization}; see also under-generalization in Section\autoref{sec:language-modeling-generalization}).
Large models (e.g. over 100B parameters) can be prompted to generate explicit multi-step reasoning by asking them to ``think step by step'' (\citealp{kojima-etal-2022-large}; Section\autoref{sec:step-reasoning}), but logical reasoning overall improves only slightly beyond around 10B parameters \citep{rae-etal-2021-scaling}.
Model size appears to have little impact on offensive text generation (Section\autoref{sec:toxicity}), but text generated by larger models is harder to distinguish from human-generated text (Section\autoref{sec:human-vs-model}), and larger models are more likely to mimic political opinions in a given input (Section\autoref{sec:personality-politics}).
The prevalence of harmful social biases in language models is inconsistent both within and across model sizes (Section\autoref{sec:bias-stereotypes}).
Overall, larger language models tend to exhibit equal or better performance to smaller models on most tasks, but their performance is still far from perfect, and they come at a higher environmental and computational cost \citep{strubell-etal-2019-energy}.

\subsection{Language modeling as generalization}
\label{sec:language-modeling-generalization}

\textbf{Text pattern generalization.} Many of the strengths and weaknesses of language models can be viewed through the lens of text pattern generalization.
Over-generalizations and under-generalizations of learned patterns in text simultaneously provide insights into the impressive capabilities and brittle responses of large language models \citep{ganguli-etal-2022-predictability}.
Specifically, due to the productivity of language (i.e. infinitely many combinations of patterns; \citealp{piantadosi-fedorenko-2017-infinitely}), language models must learn to generalize to novel examples, even when those examples would traditionally be considered ``in-distribution'' in generalization research (i.e. within the expected range of examples seen during pre-training; \citealp{hupkes-etal-2022-state}).
The in-distribution generalizations made by language models provide insights into how the models will likely behave in practice.

Through their token prediction training paradigm, language models are trained to generalize from text examples observed during pre-training to novel examples.
Given the beginning of a sentence never observed during pre-training, a language model can generate plausible completions to that sentence, similar to people generalizing from past experience to novel sentences \citep{piantadosi-fedorenko-2017-infinitely}.
Again similar to in people  \citep{prefors-etal-2006-poverty,berwick-etal-2011-poverty,dkabrowska-2015-what}, there are infinitely many generalization approaches that a language model can apply to extrapolate from pre-training examples (e.g. linear vs. hierarchical syntactic generalizations; \citealp{mccoy-etal-2018-revisiting,white-cotterell-2021-examining}).
Any text pattern that predicts upcoming tokens can under-influence or over-influence language model predictions (i.e. under-generalization vs. over-generalization), both in the set of examples to which the pattern is applied and the extent to which the pattern affects model predictions.
The specific generalizations that a language model learns are dependent on the language data observed and inherent biases from the model architecture and random initialization, also known as inductive biases \citep{white-cotterell-2021-examining}.

For example, one generalization approach might be to strictly memorize all training examples verbatim; the output token distribution for any observed example would be exactly equal to the distribution observed during pre-training, and any example not observed verbatim during pre-training would produce a random uniform distribution or some other degenerate prediction.
This would be an example of under-generalization, as the model assumes that each individual example does not reflect any patterns that can be generalized to other examples.
In practice, while language models do exhibit memorization of examples (Section\autoref{sec:memorization}), they appear to still extrapolate learned patterns from the memorized examples without overfitting \citep{tirumala-etal-2022-memorization}, suggesting that they are not entirely under-generalizing.

On the other end of the spectrum, a language model might always generate the most frequent token (e.g. ``the'') or condition only on the previous token (i.e. a bigram model).
Language models pass through both of these stages during pre-training \citep{chang-bergen-2021-word}.
These are examples of over-generalization, where token frequency rules and bigram rules over-influence model predictions.
In many cases, this over-generalization may occur due to under-generalization of other rules that would otherwise refine the over-generalized prediction.
Viewing these errors as generalization errors ties language model analysis research to broader generalization research in machine learning and NLP \citep{hupkes-etal-2022-state}.

\smallskip
\noindent
\textbf{Generalizations in language models.}
Indeed, many of the weaknesses exhibited by large language models can be interpreted as examples  over-generalization or under-generalization.
For example, language models' sensitivity to intervening clauses and specific words in subject-verb agreement reflects under-generalization of the subject-verb agreement rule (Section\autoref{sec:agreement}). 
Similarly, the models' sensitivity to paraphrasing and punctuation changes when recalling facts (Section\autoref{sec:facts-unreliable}) reflects under-generalization of learned facts.
Finally, the models' sensitivity to specific inputs when constructing situation models (Section\autoref{sec:situation-models}) and performing logical and numerical reasoning (Section\autoref{sec:reasoning}) reflects a systematic under-generalization of many patterns and rules to novel contexts.

Specifically, the models' reliance on pre-training corpus frequency for subject-verb agreement (Section\autoref{sec:agreement}), facts (Section\autoref{sec:facts-unreliable}), word meanings (Section\autoref{sec:lexical-semantics}), and arithmetic (Section\autoref{sec:numerical-reasoning}) might suggest that language models require many examples to correctly generalize some patterns, or it might suggest that the models are simply memorizing many under-generalized instances of each pattern.
Given the models' sensitivity to specific inputs for these capabilities, the memorization case appears more likely, e.g. that the models memorize many examples of arithmetic with minimal generalization.
Of course, these examples of under-generalization are not as severe as the models' inability to learn (and therefore under-generalization of) negation (Section\autoref{sec:negation}), pragmatics (Section\autoref{sec:pragmatics}), and many commonsense inferences (Sections\autoref{sec:commonsense-facts} and\autoref{sec:commonsense-events}).
In some of these cases, the language modeling objective may simply not capture the grounded and interactive features necessary to learn such patterns.

Language models also exhibit cases of over-generalization, often when some other under-generalized pattern fails to be applied.
When models fail to recall facts (Section\autoref{sec:facts-unreliable}), make commonsense inferences (Section\autoref{sec:commonsense-events}), or solve mathematical word problems (Section\autoref{sec:numerical-reasoning}), they often fall back to over-generalized heuristics such as predicting semantically similar tokens to the input context (Section\autoref{sec:novel-text}).
Overreliance on token position-based patterns (e.g. local $n$-grams) may reflect an over-generalization of position-based patterns as well (Sections\autoref{sec:position} and\autoref{sec:novel-text}).
Furthermore, harmful social biases in language models (Sections\autoref{sec:bias-performance} and\autoref{sec:bias-stereotypes}) can be interpreted as over-generalizations of patterns observed in the pre-training corpus.
Even when harmful biases are present in the pre-training corpus due to human social biases and dataset demographic imbalances, it is not desirable for language models to generalize these patterns.

Understanding when language models generalize correctly vs. incorrectly is important for the safe deployment of the models in practice.
Future work in language model behavioral analysis might consider the specific linguistic patterns and types of patterns that language models over-generalize and under-generalize, along with mitigation strategies.
In particular, future research might consider how generalization patterns change with model scale; it remains unclear to what extent the benefits of model scale are due to (1) learning more robust and/or correct generalized patterns or (2) memorizing a larger number of specific under-generalized instances that together improve performance metrics.
Again, given the models' sensitivity to specific inputs even in larger models, the models appear to lean towards the latter.

\subsection{Levels of analysis in understanding language models} 
\label{sec:levels-of-analysis}
As stated in the Introduction (Section\autoref{sec:scope}), this survey focuses on behavioral analyses of language models.
Other studies have investigated the internal mechanisms that lead language models to generate their predictions.
These two approaches roughly mirror Marr's computational and algorithmic levels of analysis in cognitive science, describing respectively (1) what the system does functionally and (2) the algorithms and representations the system uses to accomplish these functions \citep{marr-2010-vision,bechtel-shagrir-2015-the,trott-2023-in}.
Marr's last level, the implementation level, would correspond most closely to the physical circuits and neuron-level backpropagation rules that govern neural network models.
In many ways, the goals of language model analysis are to identify interpretable and generalizable principles that govern how language models work behaviorally and mechanistically, along with causal links between the two.

At the mechanistic (i.e. algorithmic) level, previous studies have probed the linguistic (and non-linguistic) information that can be extracted from language models' internal vector representations of tokens \citep{tenney-etal-2019-bert,rogers-etal-2020-a,belinkov-2021-probing}, along with how the representation spaces are structured geometrically \citep{coenen-etal-2019-visualizing,cai-etal-2021-isotropy,chang-etal-2022-the}.
They have also studied whether the attention weights assigned by language models' internal attention mechanism correlate with interpretable inter-token relationships \citep{clark-etal-2019-what,kovaleva-etal-2019-revealing,vig-belinkov-2019-analyzing}, although the attention weights do not necessarily influence language modeling predictions in expected ways \citep{jain-wallace-2019-attention,serrano-smith-2019-is}.

More recent work has established causal links between individual neurons (i.e. entries in the models' vector representations) and language modeling predictions \citep{vig-etal-2020-causal,geva-etal-2020-transformer,finlayson-etal-2021-causal,geva-etal-2022-transformer}.
For example, model representations of tokens at any layer can be interpreted as probability distributions over the language model vocabulary using the language model's output vocabulary projection matrix \citep{geva-etal-2022-transformer}; model parameters themselves can be interpreted using the same projections \citep{dar-etal-2022-analyzing}.
Parameter-level interventions can modify factual associations in language models in targeted ways \citep{meng-etal-2022-locating}, establishing direct connections between language model behavior and internal mechanisms.

Causal functionalities have also been established for individual attention heads in language models, e.g. for copying previous sequences from the input \citep{olsson-etal-2022-in}.
The attention mechanism has even been viewed as an in-context implementation of gradient descent, facilitating in-context learning (Section\autoref{sec:downstream}) without explicit parameter updates \citep{dai-etal-2022-why}.
Future work might apply similar analysis techniques to investigate the mechanisms underlying a wider range of language model behaviors, including under-generalized and over-generalized behaviors (Section\autoref{sec:language-modeling-generalization}), bridging the gap between behavioral and mechanistic levels of language model analysis.

\bigskip
\section{Conclusion}
In this survey, we have discussed a wide range of language model capabilities and weaknesses, covering over 250 studies of language model behavior from the past three years.
We find that language models remain sensitive to specific inputs and surface features even as they scale to hundreds of billions of parameters.
Many model strengths and weaknesses can be framed as correct or incorrect generalizations of text patterns.
By distilling what is currently known about large language model capabilities, we hope to inform the deployment and regulation of large language models, while also inspiring future language model analysis research.


\vfill
\pagebreak

\begin{acknowledgments}
We would like to thank the other members of the UCSD Language and Cognition Lab for helpful discussions. Tyler Chang is partially supported by the UCSD HDSI graduate fellowship.
\end{acknowledgments}

\color{black}
\starttwocolumn
\bibliography{final_lit_review}

\appendix
\appendixsection{Literature Review Process}
\label{app:lit-review}
We identified papers to include in this survey using Semantic Scholar \citep{fricke-2018-semantic}.
From a seed of 271 relevant language model analysis papers (including the majority of the citation list from \citealp{rogers-etal-2020-a}), we extracted all papers that cited any paper in the seed.
This resulted in over 15K papers, last scraped on February 4, 2023.
Anecdotally, the majority of recent language model analysis papers we encountered were included in this list.
We manually filtered by title down to approximately 1500 potentially relevant papers, gradually refining the scope as described in Section\autoref{sec:scope}.
We then further filtered by abstract down to approximately 400 highly relevant papers.

\end{document}