\section{Introduction}\label{sec:intro}
Vertical federated learning (VFL) is a newly developed machine learning scenario in distributed optimization, where clients share data with the same sample identity but each client possesses only a subset of the features for each sample. The goal is for the clients to collaboratively learn a model based on all features. Such a scenario appears in many applications, including healthcare, finance, and recommendation systems~\citep{chen2020vafl, liu2022fedbcd,gu2021privacy}. For example, in healthcare, each hospital may collect partial clinical data of a patient such that their conditions and treatments are best predicted through learning from the data collectively; in finance, banks or e-commerce providers may jointly analyze a customer's credit with their trade histories and personal information; and in recommendation systems, online social/review platforms may collect a user's comments and reviews left at different websites to predict suitable products for the user.

Most of the current VFL solutions~\citep{chen2020vafl, liu2022fedbcd,xu2021fedv,romanini2021pyvertical,gu2021privacy,yang2019quasi} treat the case where samples are independent, but omit their relational structure. However, the pairwise relationship between samples emerges in many occasions, and it can be crucial in several learning scenarios, including the low-labeling-rate scenario in semi-supervised learning and the no-labeling scenario in self-supervised learning. Take the financial application as an example: customers and institutions are related through transactions. Such relations can be used to trace finance crimes such as money laundering, assess a customer's credit risk, and even recommend products to them. Each bank and e-commerce provider can infer the relations of the financial individuals registered to them and create a relational graph, in addition to the individual customer information they possess.

One of the most effective machine learning models to handle relational data is graph neural networks (GNNs)~\citep{kipf2016semi,hamilton2017inductive,chen2018fastgcn,velickovic2018graph,chen2020simple}. This model performs neighborhood aggregation in every feature transformation layer, such that the prediction of a graph node is based on not only the information of this node but also that of its neighbors. Although GNNs have been used in federated learning, a majority of the cases therein are horizontal: each client possesses a local dataset of graphs and all clients collaborate to train a unified model to predict graph properties, rather than node properties~\citep{he2021fedgraphnn,bayram2021federated,xie2021federated}. Our case is different. We are concerned with \emph{subgraph level, vertical} federated learning~\citep{zhou2020vertically, ni2021vertical}: each client holds a subgraph of the global graph, part of the features for nodes in this subgraph, and part of the whole model; all clients collaboratively predict node properties. Our vertical setting is exemplified by not only the partitioning of node features, but also the sub-graphs among the nodes.

The setting under our consideration is fundamentally challenging, because fully leveraging features within neighborhoods causes an enormous amount of communication. One method to design and train a GNN is that each client uses a local GNN to extract node representations from its own subgraph and the server aggregates these representations to make predictions~\citep{zhou2020vertically}. The drawback of this method is that the partial features of a node outside one client's neighborhood are not used, even if this node appears in another client's neighborhood. Another method to train a GNN is to simulate centralized training: transformed features of each node are aggregated by the server, from where neighborhood aggregation is performed~\citep{ni2021vertical}. This method suffers the communication overhead incurred in each layer computation.

In this work, we propose a federated GNN model and a communication-efficient training algorithm, named \ourmethod, for federated learning with vertically distributed graph data. The model is split across the clients and the server, such that the clients can use a majority of existing GNNs as the backbone, while the server contains no model parameters. The server only aggregates and disseminates computed data with the clients. The communication frequency between the clients and the server is mitigated through \emph{lazy aggregation and stale updates} (hence the name of the method), with convergence guarantees. Moreover, \ourmethod can be considered as a framework that encompasses many well-known models and algorithms as special cases, including the work of~\citet{liu2022fedbcd} when the subgraphs are absent, the work of~\citet{zhou2020vertically} when all aggregations but the final one are skipped, the work of~\citet{ni2021vertical} when no aggregations are skipped, and centralized training when only a single client exists.

\begin{figure}[tb!]
    \centering
    \includegraphics[width=0.9\linewidth]{figure/Data_v1.png}
    \caption{Data isolation of vertically distributed graph-structured data over three clients.}
    \label{fig:data}
\end{figure}

We summarize the main contributions of this work below:
\begin{itemize}[leftmargin=*]
\item {\bf Model design:} We propose a flexible federated GNN architecture that is compatible with a majority of existing GNN models.
\item {\bf Algorithm design:} We propose the communication-efficient \ourmethod algorithm to train the model. Therein, lazy aggregation saves communication for each joint inference round, by skipping some aggregation layers in the GNN; while stale updates further save communication by allowing the clients to use stale global information for multiple local model updates.
\item {\bf Theoretical analysis:} We provide theoretical convergence analysis for \ourmethod by addressing the challenges of biased stochastic gradient estimation caused by neighborhood sampling and correlated update steps caused by using stale global information.
\item {\bf Numerical results:} We conduct extensive experiments, together with ablation studies, to demonstrate that \ourmethod can achieve comparable performance as the centralized model on multiple datasets and multiple GNN backbones, and that \ourmethod effectively saves \xwedit{$58\%$ and $82\%$ of the total runtime by lazy aggregation and stale update, respectively.}
\end{itemize}

\subsection{Problem Setup}

Consider $M$ clients, indexed by $m = 1,\ldots,M$, each of which holds a part of a graph with the node feature matrix $\mX\in\R^{N \times d}$ and the edge set $\cE$. Here, $N$ is the number of nodes in the graph and $d$ is the feature dimension. We assume that each client has the same node set and the same set of training labels, $\yy$, but a different private edge set $\cE_m$ and a non-overlapping node feature matrix $\mX_m\in\R^{N\times d_m}$, such that $\cE = \bigcup_{m=1}^M\cE_m$, $\mX=[\mX_1,\ldots,\mX_M]$, and $d=\sum_{m=1}^M d_m$. We denote the client dataset as $\cD_m = \{\mX_m, \cE_m, \yy\}$ and the full dataset as $\cD = \{\mX, \cE, \yy\}$. The task is for the clients to collaboratively infer the labels of nodes in the test set. See Figure~\ref{fig:data} for an illustration. 

%------------------------------------------------------------------------------
\subsection{Graph Convolutional Network}\label{sec:GCN}

The graph convolution network (GCN)~\citep{kipf2016semi} is a typical example of the family of GNNs. Inside GCN, a graph convolution layer reads
\begin{equation}\label{eqn:GCN}
\mH[l+1] = \sigma\Big(\mA(\cE)\cdot\mH[l]\cdot\mW[l]\Big),
\end{equation}
where $\sigma(\cdot)$ denotes the point-wise nonlinear activation function, $\mA(\cE)\in \R^{N\times N}$ denotes the adjacency matrix defined by the edge set $\cE$ with proper normalization, $\mH[l] \in \R^{N\times d[l]}$ denotes the node representation matrix at layer $l$, and $\mW[l] \in \R^{d[l]\times d[l+1]}$ denotes the weight matrix at the same layer. The initial node representation matrix $\mH[0]=\mX$. The classifier is denoted as $\hat{\yy} = f(\mH[L],\mW[L])$ with weight matrix $\mW[L]$ and the loss function is denoted as $\ell(\yy, \hat{\yy})$. Therefore, the overall model parameter is $\mW = \{\mW[0],\dots,\mW[L-1],\mW[L]\}$.

Mini-batch training of GCN (and GNNs in general) faces a scalability challenge, because computing one or a few rows of $\mH[L]$ (i.e., the representations of a mini-batch), requires more and more rows of $\mH[L-1]$, $\mH[L-2]$, \ldots recursively, in light of the multiplication with $\mA(\cE)$ in~\eqref{eqn:GCN}. This is known as the \emph{explosive neighborhood problem} unique to graph-structured data. Several sampling strategies were proposed in the past to mitigate the explosion; in this work, we adopt the layer-wise sampling proposed by FastGCN~\citep{chen2018fastgcn}. Starting from the output layer $L$, which is associated with a mini-batch of training nodes, $\cS[L]$, we iterate over the layers backward such that at layer $l$, we sample a subset of neighbors for $\cS[l+1]$, namely $\cS[l]$. In doing so, at each layer, we form a bipartite graph with edge set $\cE(\cS[l+1],\cS[l]) = \{(i,j)\vert i\in \cS[l+1], j\in\cS[l]\}$. Then, each graph convolution layer becomes
\begin{equation}\label{eqn:FastGCN}
\mH[l+1][\cS[l+1]] = \sigma\Big(\mA(\cE(\cS[l+1], \cS[l]))\cdot\mH[l][\cS[l]]\cdot\mW[l]\Big),
\end{equation}
where $\mA(\cE(\cS[l+1], \cS[l]))\in \R^{\abs{\cS[l+1]}\times\abs{\cS[l]}}$ is a properly scaled submatrix of $\mA(\cE)$ and $\mH[l][\cS[l]]$ denotes the rows of $\mH[l]$ corresponding to $\cS[l]$. Such a mini-batch sampling and training procedure fundamentally differs from the usual mini-batch training for non-graph data in VFL.

%------------------------------------------------------------------------------
\subsection{Related Works}
\xwedit{
{\bf VFL with non-graph data:} In VFL, a few large entities (around $2\sim20$) collaborate together, and each client holds partial features of all samples~\citep{liu2022fedbcd, chen2020vafl,romanini2021pyvertical,yang2019parallel,gu2021privacy,yang2019quasi,xu2021fedv}. Because the features of the sample are distributed to all clients, it is unavoidable also to split the model among the clients and communicate sample information across the clients. Therefore, the key challenge in VFL is the heavy communication burden in exchanging the partial sample information for computing the losses and gradients for each sample during training. 

\citet{gu2021privacy,chen2020vafl,xu2021fedv,yang2019parallel} consider a linear combination of the features, e.g., SVM, linear regression, and logistic regression problems. Therefore, they develop possibly asynchronous communication-efficient protocols for aggregating the linear combination of the features, which cannot be 
generalized to complex models with a highly nonlinear combination of the features that require multiple rounds of communication, such as multi-layer perceptions (MLP), convolution neural networks (CNN). In \citet{yang2019quasi}, the authors use the second-order Taylor expansion to linearize the training problem for efficient communication. However, all the existing work assumes that one round of communication is sufficient for exchanging partial information for computing the losses and gradients at each round, and does not apply to GNN models.

{\bf FL with graph data:} FL with graph data falls into four major scenarios. The {\it horizontal}, or {\it graph-level} scenario, where each client possesses a collection of graphs, and all clients collaborate to train a unified model~\citep{zhang2021federated,he2021fedgraphnn,bayram2021federated,xie2021federated}. The task of graph-level FL is to predict the graph properties (such as molecular properties).

The {\it vertical sub-graph level} scenario considers that each client holds a part of the node features, a partial model, and a sub-graph contains all nodes of the global graph~\citep{zhou2020vertically,ni2021vertical}. The clients aim to collaboratively train a global (combined from those of each client) to predict node properties (such as the category of a paper in a citation network). Our work addresses this scenario.

The third scenario in the {\it horizontal sub-graph level}, where the graph is partitioned, and each client holds one partition~\citep{zhang2021subgraph,wu2021fedgnn,chen2022fedgraph,yao2022fedgcn}. The methods for this scenario need to address the aggregation of information along the edges crossing different clients. This scenario differs from the {\it vertical} scenario in the sense that the features of each node are not partitioned and the nodes are distributed to the clients without overlap. 

The last scenario considers the {\it node-level}: the clients are connected by a graph, and thus each client is treated as a node. In other words, the clients, rather than the data, are graph-structured. It is akin to {\it decentralized training}, where clients communicate to each other via the graph, or the models are mixed through a graph, to train a unified model~\cite{lalitha2019peer,meng2021cross,caldarola2021cluster,rizk2021graph}. 

See \appref{app:related} for more discussion on the related works.}