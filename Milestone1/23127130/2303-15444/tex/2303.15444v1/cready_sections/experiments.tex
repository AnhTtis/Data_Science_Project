\section{Experiments} \label{sec:experimental-results}

In this section, we illustrate several experiments on synthetic and real datasets to evaluate the proposed approaches. 
The chosen metric is the \emph{misclassification error}, that is the percentage of wrongly classified points, as it is common practice in this field. Our comparisons mainly concentrate on the classical \ransacov \cite{magrifusiello16} and the hybrid quantum-classical \suter \cite{Doan_2022_CVPR}, which are the most related to our approaches, as explained in Sec.~\ref{sec:related-work}.
Quantum experiments run on the DWave Advantage System 4.1, a modern AQC by DWave built with the Pegasus topology \cite{Boothby2020}, comprising approximately 5000 physical qubits. Communication with the AQC is managed via the Ocean software stack \cite{OceanTools2021} and every experiment is executed with an annealing time of 20$\mu s$.

\smallskip
\textbf{Selection of $\lambda$.} In every experiment, we set $\lambda = 1.1$. 
This value is obtained with a two-step procedure: 
1) Initially, following common practices (see, \emph{e.g.}, \cite{QuantumSync2021,Arrigoni2022}), we perform an exhaustive grid search using \ourmethod to solve small-scale problems (encoded by at most $20$ qubits), from which we get an initial guess for $\lambda$ by looking at the \emph{optimal solution probability} \footnote{We refer to \emph{optimal solution probability} as the ratio between the anneals ended in a globally optimal state and the total number of anneals.}; 
2) Then, we evaluate the initial guess further, solving problems encoded by at most 100 qubits with \ourmethodsa, verifying whether or not the initial guess for $\lambda$ generalizes well to larger problems, too. 
In this second step, the reference metric is the misclassification error.






\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{imgs/star5_comparison_small_169.pdf}
    \vspace{-20pt}
    \caption{Misclassification Error for several methods on the \emph{Star5} dataset \cite{toldo2008robust}. The number of points $n$ is fixed to $n = 30$, the number of ground-truth structures is fixed to $k = 5$, and the number of sampled models $m$ is arranged on the x-axis. \vspace{-1em}
    }
    \label{fig:small_problems}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.8\columnwidth]{imgs/phylog_line_169_n250.pdf}
    \vspace{-10pt}
    \caption{Relationship between \emph{physical qubits} and \emph{logical qubits} in embeddings produced with small-scale preference matrices from the \emph{Star5} dataset \cite{toldo2008robust}. \vspace{-1.5em}}
    \label{fig:phylog}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{imgs/cready/star5_increasing_noise_169_multirun.pdf}
    \vspace{-20pt}
    \caption{Misclassification Error for several methods on the \emph{Star5} dataset \cite{toldo2008robust}. The number of points $n$ is fixed to $n = 250$, the number of ground-truth structures is fixed to $k = 5$, and the number of sampled models $m$ (corresponding to the dimension of the search space) is arranged on the x-axis.
    \vspace{-2em}
    }
    \label{fig:star5_increasing_noise}
\end{figure}

\subsection{Synthetic Data}
\label{sec:results-synth}

We consider the synthetic \emph{Star5} dataset \cite{toldo2008robust}, that is used to fit five lines (depicting a star) given a set of 2D points. Outliers are removed to satisfy our assumptions.  

\smallskip
\textbf{Small-scale Data.}
In this experiment, we select a subset of $n=30$ points. 
The number of ground-truth models is fixed to $k = 5$, and additional $m-k$ excess models are sampled in order to study performances with increasing problem size. 
Note that this corresponds to including false models (i.e., columns) in the preference matrix, thereby representing noise.
For each configuration, 20 preference matrices are generated and averaged results are reported.
We consider our single-QUBO quantum approach \ourmethod and its CPU counterpart \ourmethodsa.
We also compare our approaches with \ransacov \cite{magrifusiello16}, whose code is available online\footnote{\url{https://fusiello.github.io/demo/cov/index.html}}, that can be viewed as a baseline implementing a sophisticated integer programming solver. 
All the methods receive as input the same preference matrix and optimize the same objective, as already observed in Sec.~\ref{sec-related-mmf}. 

Results are given in Fig.~\ref{fig:small_problems}, showing that \ourmethod can exactly solve problems up to 40 qubits, but it can not solve larger problems with high accuracy: this is an expected behaviour (in line with previous quantum papers, see Sec.~\ref{sec:related_quantum}), as \emph{current} quantum hardware is limited and far from maturity.
It is worth noting that our \ourmethodsa achieves excellent results (on par with \ransacov) for all the analyzed problem sizes: considering that \ourmethodsa uses the same QUBO as \ourmethod, we can thus view the former as an indication on the performances the latter can achieve in the future thanks to the continuous improvements in the quantum hardware.

\smallskip
\textbf{Physical vs Logical Qubits.}
We analyze here how the allocation of QPU resources (\emph{physical qubits}) grows with increasing problem size (\emph{logical qubits}). Similarly to our previous experiment, we use synthetic preference matrices generated from the \emph{Star5} dataset with increasing number of columns. However, we consider here all of the 250 points in the dataset in order to study denser cases. Our analysis is displayed in Fig.~\ref{fig:phylog}, showing that physical qubits grow super-linearly with respect to logical ones.
Such relationship makes it infeasible to allocate real world problems, as they typically require thousands of logical qubits. 
This finding, \emph{given the current state of the hardware}, strengthens the need for a decomposition approach.
Hence, we analyze our decomposition policy in the next paragraph.

\smallskip
\textbf{Large-scale Data.}
In this experiment, we consider up to 1000 models in the input preference matrix (also including the $k=5$ ground-truth models) as well as all of the $n=250$ points in the \emph{Star5} dataset. Since \ourmethod is not able to manage such large-scale data, it is not considered here. Instead, it is used as a sub-component of our decomposed approach \ourmethoddec: each sub-problem has a dimension that can be solved with high accuracy by \ourmethod, namely 20 qubits. %
As a reference, we also include \ourmethodsa and its decomposed version \ourmethodsadec as well as \ransacov \cite{magrifusiello16}.

Results are given in Fig.~\ref{fig:star5_increasing_noise}, showing that our \ourmethodsadec achieves nearly zero misclassification error in all the cases, on par with \ransacov. 
Average \ourmethoddec errors lie in the range $[0,10]\%$ for all problem sizes, showing the great effectiveness of the decomposition principle.
As for \ourmethodsa, this specific dataset represents a failure case: after inspecting the solution, we hypothesize that a possible reason is the sparsity pattern of the preference matrix, causing the algorithm to select more models than needed.







\begingroup
\renewcommand{\arraystretch}{1.25}
\begin{table*}[t]
    \caption{Misclassification Error [\%] on the \emph{Traffic3} and \emph{Traffic2} subsets of the Hopkins benchmark \cite{TronVidal07} 
    and on the York DB \cite{Denis2008ECCV}.
    \vspace{-1em}}
    \centering{\scriptsize
    \begin{tabular}{cccccccccccc}
         \toprule
         & & Multi-X \cite{BarathMatas17} & J-Linkage \cite{toldo2008robust} & T-Linkage \cite{MagriFusiello14} & RPA \cite{MagriFusiello17}
         & \ransacov \cite{magrifusiello16} & \ourmethodsa & \ourmethoddec & \ourmethodsadec \\
         \midrule
         \multirow{2}{6em}{\emph{Traffic3} \cite{TronVidal07}} & \emph{mean} & 0.32 & 1.58 & 0.48 & \textbf{0.19} & 0.35 & 5.14 & 8.74 & {0.55}\\
         & \emph{median} & \textbf{0} & 0.34 & 0.19 & \textbf{0} & 0.19 & 2.85 & 7.50 & 0.28\\
         \midrule
         \multirow{2}{6em}{\emph{Traffic2} \cite{TronVidal07}} & \emph{mean} & \textbf{0.09} & 1.75 & 1.31 & 0.14 & 0.54 & 6.04 & - & {0.10}\\
         & \emph{median} & \textbf{0} & \textbf{0} & \textbf{0} & \textbf{0} & \textbf{0} & 3.17 & - & \textbf{0}\\
         \midrule
         \multirow{2}{6em}{\emph{York} \cite{Denis2008ECCV}} &\emph{mean} & - & 2.85 & 1.44 & {1.08} & \textbf{0.19} & 12.29 & - & {0.74} \\
         & \emph{median} & - & 1.80 & \textbf{0} & \textbf{0} & \textbf{0} & 3.78 & - & \textbf{0} \\
    \bottomrule
    \end{tabular}
    \vspace{-1em}
    }
    \label{tab:hopkins}
\end{table*}
\endgroup



\subsection{Real Data}
\label{sec:results-real}

In this section, we test our framework on several real world datasets, each associated to a different task. Since these are large-scale problems, we do not consider \ourmethod but its CPU variant \ourmethodsa. %


\smallskip
\textbf{AdelaideRMF dataset \cite{wong2011dynamic}.}
From this benchmark, we consider the 15 sequences involving multiple fundamental matrices (which, in turn, can be used to perform motion segmentation in two images) and we remove outliers beforehand. All the analyzed methods are given the same preference matrices as input. Results are given in Tab.~\ref{tab:adel_mmf}. 
Both \ourmethodsa and \ourmethodsadec significantly outperform \ransacov \cite{magrifusiello16} in this scenario. 
In particular, \ourmethodsadec performs better than its quantum version \ourmethoddec, in agreement with our previous experiments.

\begingroup
\renewcommand{\arraystretch}{1.25}
\begin{table}[t]
    \centering{\footnotesize
    \resizebox{1\columnwidth}{!}
    {
    \begin{tabular}{ccccc}
         \toprule
         & \ransacov \cite{magrifusiello16} & \ourmethodsa & \ourmethoddec & \ourmethodsadec \\
          \midrule
         \emph{mean} & 9.79 & 3.85 & 16.22 & \textbf{0.77}\\
         \emph{median} & 7.97 & 3.54 & 11.0 & \textbf{0.18}\\
         \bottomrule
    \end{tabular} 
    }
    }
    \caption{Misclassification Error [\%] for several methods on the 15 \textbf{multi-model} sequences of the AdelaideRMF dataset \cite{wong2011dynamic}. \vspace{-2em}}
    \label{tab:adel_mmf}
\end{table}
\endgroup


\smallskip
\textbf{Hopkins benchmark \cite{TronVidal07}.}
From this dataset, we consider the \emph{Traffic2} and \emph{Traffic3} sequences, which comprise 31 and 7 scenes, respectively. Here, the task is to fit multiple subspaces to point trajectories in the context of video motion segmentation. This benchmark provides outlier-free data, hence no pre-processing is needed. Results are given in Tab.~\ref{tab:hopkins}: in addition to \ransacov \cite{magrifusiello16}, other multi-model fitting approaches have been considered, whose results are taken from the respective papers. All the variants of our approach\footnote{We are not able to run \ourmethod on \emph{Traffic2} due to the limited amount of QPU time available in our monthly subscription combined with the large dimension of the dataset, hence results are not reported.} perform reasonably well, with \ourmethodsadec being on par with the state of the art.%


    

\smallskip
\textbf{York Urban Line Segment Database \cite{Denis2008ECCV}.}
This dataset contains 102 images used for vanishing point detection from multiple line segments without outliers.
Results are presented in Table \ref{tab:hopkins}, %
following the same rationales as per the Hopkins benchmark. %
Such results are in line with our previous experiments: in particular, \ourmethodsadec exhibits competitive performances with the state of the art.






\subsection{Comparison with Quantum State-of-the-art}
\label{sec:quantum_comparison}
Our method is the first quantum solution to \emph{multi-model} fitting, hence there are no quantum competitors in the literature.
The most similar work to ours is \suter \cite{Doan_2022_CVPR}, whose code is available online\footnote{\url{https://github.com/dadung/HQC-robust-fitting}}, which addresses \emph{single-model} fitting in the presence of outliers (see Sec.~\ref{sec:related_quantum}).
\suter has strong theoretical justifications, and the proposed algorithm terminates with either the globally optimal solution or a feasible solution paired with its \emph{error bound}.
Neither \ourmethod or \ourmethoddec have this property.
On the other hand, our framework is general and can be used for a single model as well, allowing for an experimental comparison with \suter \cite{Doan_2022_CVPR}. 
Specifically, we exploit the information that only a single model is present in the data and we select, from the solution returned by set-cover, only the model with the maximum consensus set. All the other models in the solution are treated as outliers.

We focus on fundamental matrix estimation and use the 4 sequences of the AdelaideRMF dataset \cite{wong2011dynamic}  characterized by the presence of a single model. 
Since this is a large-scale dataset, we omit \ourmethod and consider \ourmethodsa, \ourmethoddec and \ourmethodsadec, as previously done.
We analyze how performances vary with different outlier ratios: we consider $10\%$ and $20\%$ of outliers, in addition to the evaluation on the original sequences, where no fixed outlier ratio is imposed.
Results are summarized in Tab.~\ref{tab:adelrmf_single}.

\suter shows remarkable behaviour in controlled scenarios with low outlier ratios; however, it is worth noting that our method is more robust to higher outlier ratios, although not explicitly designed for single-model fitting.
See also Fig.~\ref{fig:qualitative} for qualitative results of worst-case scenarios.





\begin{table}
    \centering
    \resizebox{0.75\columnwidth}{!}
    {\small
    \begin{tabular}{ccccccc} 
    \toprule
    & \multicolumn{3}{c}{Outlier ratio} \\
   \cmidrule(lr){2-4} Algorithm & 10\% & 20\% & Full sequences \\
    \midrule
    \ourmethodsa & 7.22 & 11.34 & 13.23 \\
    \ourmethoddec & \textbf{2.41} & 10.53 & 16.17\\
    \ourmethodsadec & 6.26 & \textbf{8.28} & \textbf{10.83} \\
    \suter \cite{Doan_2022_CVPR} & 3.71 & 37.0 & 45.84 \\
    \bottomrule
    \end{tabular}
}
    \caption{Misclassification Error [\%] for quantum methods on the \textbf{single-model} sequences of the AdelaideRMF dataset \cite{wong2011dynamic}. \vspace{-1em}}
    \label{tab:adelrmf_single}
\end{table}



\begin{figure}
    \centering
    \includegraphics[width=0.48\columnwidth]{imgs/noi_si.jpg}
     \includegraphics[width=0.48\columnwidth]{imgs/suter_no.jpg} 
    \ourmethodsadec \quad\quad\quad\quad  \suter \cite{Doan_2022_CVPR}  \\ 
    \caption{
    A sample of the worst results of the considered quantum methods on the \emph{book} image pair from the AdealideRMF  \cite{wong2011dynamic} with $20\%$ outliers. Outliers and inliers are shown in red and green, respectively. On the right, \suter \cite{Doan_2022_CVPR} classifies all the points as outliers, whereas our method achieves $8.49\%$ error.}
    \vspace{-1.75em}
    \label{fig:qualitative}
\end{figure}
