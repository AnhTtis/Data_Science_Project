\section{The Proposed Method}
\label{sec:proposed-methodology}

In this section we derive our approach: first, we define an optimization function for the multi-model fitting task; then, we align it to the standard QUBO formulation, in order to enable quantum optimization; finally, we propose a decomposed version that can manage large-scale problems. 

\subsection{Problem formulation}

Consider a set of data points $ X= \{ x_1, \dots, x_n \} $ and a set $\Theta =\{ \theta_1, \dots, \theta_m \}$ of tentative and redundant (parametric) models, which, in practice, are obtained by a random sample procedure as done in \ransac \cite{ransac}.  Specifically, the models are instantiated by randomly selecting the minimum number of points necessary to unambiguously constrain the parameters of a model (\emph{e.g.}, 2 points for a line, 8 for a fundamental matrix). 
The aim of multi-model fitting is to extract from $\Theta$ the ``best'' models that describe all the data $X$. 
Equivalently, MMF can be seen as a clustering problem, where points belonging to the same model must be grouped together. 
This is an ill-posed task since different interpretations of the same data are possible and recovering the best models requires the definition of a proper notion of optimality. 
To this end, the problem has been cast as the optimization of an objective function encoding different regularization strategies to disambiguate between interpretations and include additional prior about the data. 
The choice of the objective function is critical and determines the performance of the algorithm. 

\subsection{Choice of Optimization Objective}
\label{sec:mmf-optimization}



In this paper, we focus on the objective function of \ransacov (Random Sample Coverage) \cite{magrifusiello16} where MMF is treated as a set-cover problem. 
This choice is due to: i) the \emph{simplicity} of the objective, which does not involve complex regularization terms as well as does not require to know in advance the number of models; ii) its \emph{combinatorial nature}, that is well capitalized by a quantum formulation; iii) its \emph{generality}, as this approach manages the broad situation of fitting {multiple} models, in contrast to previous work on quantum model fitting \cite{Doan_2022_CVPR}.
\ransacov \cite{magrifusiello16} casts MMF as an integer linear program. Such formulation is 
based on the notion of \textit{Preference-Consensus Matrix}, which is commonly denoted by $P$ and defined as:
\begin{equation}
    P[i,j]=
    \begin{cases}
        1 & \text{if $\error(x_i, \theta_j) < \epsilon$}\\
        0 & \text{otherwise}\\
    \end{cases}
    \label{eq:preference}
\end{equation}
where: $x_i$ and $\theta_j$ are the $i$-th point in the dataset and the $j$-th sampled model respectively; $\error$ expresses the residual of point $x_i$ w.r.t. model $\theta_j$ and $\epsilon$ is the inlier threshold to assign the point to the model. $P$ is a matrix of size is $n \times m$, where $n$ and $m$ are the number of points and models, respectively. Intuitively, the columns of $P$ represent the consensus sets of models, while the rows are points' preference sets. 

In this setup, selecting the subset of sampled  models that best describe the data, is equivalent to selecting the minimum number of columns of $P$ that explain all the rows. More formally:
\begin{equation} \label{eq:set-cover}
    \begin{gathered}
        \min_{\mathbf{z} \in \mathbb{B}^m}{\mathbbm{1}_{m}^{\mathsf{T}}\mathbf{z}} \quad 
        \text{s.t. } P\mathbf{z} \geq \mathbbm{1}_{n}
    \end{gathered}
\end{equation}
where $\mathbbm{1}$ denotes a vector of ones (whose length is given as subscript) and $ \mathbf{z}$ is a binary vector representing which models (i.e., columns of the preference matrix $P$) are selected.
Solving \eqref{eq:set-cover} entails minimizing the number of selected models, while ensuring that each point is explained by \emph{at least} one of them (as expressed by the inequality constraint). This problem is also known as \emph{set cover}: we are looking for a \emph{cover} of the data (i.e., a collection of sets whose union contains all the points) consisting of a minimum number of models, thus discouraging redundancy.

Our goal is to express MMF as a QUBO, in order to make it suitable for optimization on an AQC and capitalize all the advantages of quantum computing.
In general, posing a computer vision problem as a QUBO may not be straightforward, hence a careful choice of assumptions is %
essential.
First, in this paper, we assume that the true number of models/clusters (denoted by $k$) is \emph{unknown}, so our approach is %
general. 
Second, we assume that outliers are not present in the data, \emph{i.e.}, there exist \emph{exactly} $k$ models that describe all the points. 
This is not a major limitation because,  when outliers are present in the data, the models fitted to outliers could be easily recognized and discarded a-posteriori, using \emph{a-contrario} reasoning \cite{moulon2013global}, as usually done by MMF algorithms based on clustering \cite{MagriFusiello14,TepperSapiro14}. 
Third, we focus on the case where \emph{disjoint} models are being sought, which is highly common in practical scenarios. In this situation, the inequality in \eqref{eq:set-cover} can be turned into an equality constraint. 

These assumptions allow to rewrite the problem in terms of \textbf{disjoint set cover}, which is the basis of our work: 
\begin{equation} \label{eq:disjoint-set-cover}
    \begin{gathered}
        \min_{\mathbf{z} \in \mathbb{B}^m}{\mathbbm{1}_{m}^{\mathsf{T}}\mathbf{z}}\quad
        \text{s.t. } P\mathbf{z} = \mathbbm{1}_{n}.
    \end{gathered}
\end{equation}
The above constraint, instead of ensuring that each point is explained by \textit{at least} one selected model, forces each point to be explained by \textit{exactly} one model, hence imposing the consensus sets of the selected models are disjoint. Problem \eqref{eq:disjoint-set-cover} is the starting point of our QUBO formulation.




\subsection{QUBO Formulation}
\label{sec:mmf-qubo-formulation}

Problem \eqref{eq:disjoint-set-cover} is an integer linear program which is known to be NP-hard, hence it represents a suitable candidate to exploit the advantages of quantum optimization. To accomplish such a task, we need to turn  \eqref{eq:disjoint-set-cover} into a QUBO.
Being a linear program, Problem \eqref{eq:disjoint-set-cover} is, in particular, a special case of a quadratic program with linear equality constraints. Hence, if we pose constraints as soft ones (instead of hard ones), then we can rewrite \eqref{eq:disjoint-set-cover} as \eqref{eq:qubo_soft} using the following correspondences:
\begin{equation} 
\label{eq:mmf-qubo-terms}
Q=0, \ \quad \mathbf{s} = \mathbbm{1}_m, \  \quad A = P , \  \quad \mathbf{b} = \mathbbm{1}_n 
\end{equation}
where optimization is carried out over $\mathbf{z} \in \mathbb{B}^m$, that represents the choice of a subset of optimal models.
Putting together the equalities in \eqref{eq:general-constraints-subs} and \eqref{eq:mmf-qubo-terms}, it follows that:
\begin{equation} \label{eq:mmf-constrained-substitutions}
        \widetilde{Q} = \lambda P^{\mathsf{T}} P, \ \quad
        \Tilde{ \mathbf{s} } = \mathbbm{1}_m - 2\lambda P^{\mathsf{T}}\mathbbm{1}_n
\end{equation}
where no matrix of quadratic coefficients is present in $\widetilde{Q}$ as the considered formulation only contains linear terms, i.e., $Q=0$. 
Hence, Problem \eqref{eq:disjoint-set-cover} can be turned into a QUBO of the form \eqref{eq:soft-constrained-qubo}, namely:
\begin{equation} \label{eq:mmf-qubo}
    \min_{ \mathbf{z} \in \mathbb{B}^m}{\lambda \mathbf{z}^{\mathsf{T}} (P^{\mathsf{T}} P )\mathbf{z} + (\mathbbm{1}_m-2\lambda P^{\mathsf{T}}\mathbbm{1}_n)^{\mathsf{T}} \mathbf{z}} .
\end{equation}
Once a QUBO formulation has been obtained for a specific problem as in \eqref{eq:mmf-qubo}, an AQC can be leveraged to find an optimal solution, following the procedure from Sec.~\ref{sec:background}. 
This gives rise to our quantum approach for multi-model fitting, hereafter named \ourmethod.

\subsection{Iterative Decomposed Set-Cover}
\label{sec:id-qmmf}



\begin{algorithm}[t]
\caption{\ourmethoddec}\label{algo:decomposition}
\small 
\begin{algorithmic}
\Require $P$, $s$
\While{$|P.\operatorname{columns}| > s$}
\State $\text{subproblems} = \operatorname{ColumnPartition}(P, s)$
\State $i \gets 0$
\While{$i < |\text{subproblems}|$}
\State $\mathcal{J}_i \gets$ models in the $i$-th subproblem
\State $P_{\mathcal{J}_i}\gets $  $P$ retaining only the $\mathcal{J}_i$ columns
\State $\mathbf{z}_i = \ourmethod(P_{\mathcal{J}_i})$
\State remove from $P$ columns $\ P_{\mathcal{J}_i}[:,1-\mathbf{z}_i]$%
\State $i \gets i+1$
\EndWhile
\EndWhile
\State $\mathbf{z} \gets \ourmethod(P)$\\
\Return $P$[:,$\mathbf{z}$]
\end{algorithmic}
\end{algorithm}


While the capabilities of quantum hardware are rapidly evolving \cite{Boothby2020,Jurcevic2021}, current AQC architectures offer a limited number of physical qubits, thus the maximum dimension of the problems that can be solved is constrained in terms of $m$.
In order to overcome this limitation, we present a decomposed approach that iteratively prunes out columns of the preference matrix $P$ and reduces the dimension of the problem to fit the number of available physical qubits. 

Our iterative pruning procedure is named \ourmethoddec and is detailed in Algorithm~\ref{algo:decomposition}.
Instead of directly solving the problem encoded by the preference matrix $P$, at each iteration we partition the columns of $P$ into multiple submatrices $P_{\mathcal{J}_{i}}$, where $\mathcal{J}_i$ indicates the set of selected column indices and $|\mathcal{J}_i|=s$. We choose the subproblem size $s$ so that the partitioned problems can be solved with high confidence by \ourmethod.
Each subproblem encoded by $P_{\mathcal{J}_{i}}$ is independently solved using \ourmethod (see Sec.~\ref{sec:mmf-qubo-formulation}), yielding an optimal binary vector $\mathbf{z}_i$ that represents the subset of selected models for the current subproblem. Note that models not selected in $\mathbf{z}_i$ are unlikely to be solutions to the original problem since, by construction, $\mathbf{z}_i$ %
maximizes the coverage on \textit{all points} with only the restricted subset of models $\mathcal{J}_i$. Then, after all subproblems have been solved, we prune $P$ by retaining only the columns that are selected in the corresponding optimal $\mathbf{z}_i$. 
This has the effect of significantly decreasing the dimensionality of the search space (see Fig.~\ref{fig:decomposed}).

We iteratively apply the above procedure until the number of remaining columns in $P$ falls below $s$, at which point the problem is directly and reliably solved using \ourmethod.

\begin{figure}
    \centering
    \includegraphics[width=0.8\columnwidth]{imgs/decomposed.pdf}
    \caption{Illustration of the iterative pruning technique applied to $P$ in \ourmethoddec over consecutive iterations.
    At time $t$, $P$ is partitioned in different sub-problems of the same size (in the figure, 4 colour-coded problems with 6 models each) which are independently solved with \ourmethod.
    At time $t+1$, models belonging to the sub-solutions (highlighted in red in the figure) are retained; the others are discarded.}
    \label{fig:decomposed}
    \vspace{-0.75em}
\end{figure}
