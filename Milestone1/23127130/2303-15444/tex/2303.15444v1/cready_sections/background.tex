\section{Background}\label{sec:background} 
The \textbf{QUBO formulation} in a \emph{binary} search space is an optimization problem of the following form:
\begin{equation}
    \min_{\mathbf{y} \in \mathbb{B}^d}{\mathbf{y}^{\mathsf{T}}Q\mathbf{y} + \mathbf{s}^{\mathsf{T}}\mathbf{y}}, 
    \label{eq_qubo}
\end{equation}
where $\mathbb{B}^d$ denotes the set of binary vectors of length $d$, $Q \in \mathbb{R}^{d \times d}$ is a real symmetric matrix and, in a QUBO with $d$ variables, their linear coefficients are packed into $\mathbf{s} \in \mathbb{R}^d$. When the problem is subject to linear constraints of the form $A \mathbf{y} = \mathbf{b} $, a common approach is to relax such constraints and reformulate the QUBO as:
\begin{equation}
    \min_{\mathbf{y} \in \mathbb{B}^d}{\mathbf{y}^{\mathsf{T}}Q\mathbf{y} + \mathbf{s}^{\mathsf{T}}\mathbf{y} + \lambda||A\mathbf{y} - \mathbf{b}||_{2}^{2}}, 
    \label{eq:qubo_soft}
\end{equation}
where $\lambda$ must be tuned to balance the contribution of the constraint term. Unravelling such a term leads to:
\begin{equation} \label{eq:soft-constrained-qubo}
    \min_{ \mathbf{y} \in B^d}{\mathbf{y}^{^{\mathsf{T}}}\widetilde{Q}\mathbf{y} + \Tilde{\mathbf{s}}^{^{\mathsf{T}}}\mathbf{y}}
\end{equation}
where the following simple substitutions are implied:
\begin{equation} \label{eq:general-constraints-subs}
    \widetilde{Q} = Q + \lambda A^{\mathsf{T}} A, \ \quad 
    \Tilde{\mathbf{s}} = \mathbf{s} - 2\lambda A^{\mathsf{T}} \mathbf{b}
\end{equation}
For the proof, the reader can refer to, e.g., Birdal \textit{et al.}~\cite{QuantumSync2021}. 

\smallskip
\textbf{Adiabatic Quantum Computers (AQCs)} are capable of solving QUBO problems. 
An AQC is organized as a fixed and architecture-dependent undirected graph, whose nodes correspond to \emph{physical qubits} and whose edges correspond to \emph{couplers} (defining the connectivity pattern among qubits) \cite{Dattani2019}. 
Such graph structure, in principle, can be mapped to a QUBO problem \eqref{eq_qubo} as follows: each physical qubit represents an element of the solution (i.e., of the binary vector $\mathbf{y}$), while each coupler models an element of $Q$ (i.e., the coupler between the physical qubits $i$ and $j$ maps to entry $Q_{ij}$). 
An additional weight term is assigned to each physical qubit, modelling the linear coefficients in $\mathbf{s}$.

Because AQC graphs are not fully connected, mapping an arbitrary QUBO to quantum hardware requires additional steps. 
This concept is easier to visualize if the notion of \emph{logical graph} is introduced: the QUBO itself can also be represented with a graph, having $d$ nodes (called \emph{logical qubits}\footnote{Hereafter we will sometimes omit the term ``logical'' or ``physical'' as it will be clear from the context which type of qubits are being involved.}), corresponding to the $d$ entries of $\mathbf{y}$ and edges corresponding to the non-zero entries of $Q$.


The logical graph undergoes \emph{minor embedding}, where an algorithm such as 
\cite{Cai2014} maps the logical graph to the physical one. During this process, a single logical qubit can be mapped to a set of physical qubits, \textit{i.e.,} a \emph{chain}. 
All the physical qubits in a chain must be constrained to share the same state during annealing 
\cite{pelofske2020advanced}. 
The magnitude of such constraint is called \emph{chain strength} and the number of physical qubits in the chain is called \emph{chain length}. 
An equivalent QUBO that can be directly embedded on the physical graph is obtained as the output of minor embedding. 

Then, optimization of the combinatorial objective takes place: we say that the AQC undergoes an \emph{annealing} process. In this phase, the physical system transitions from a high-energy initial state to a low-energy final state, representing the solution of the mapped QUBO problem, according to the laws of quantum mechanics \cite{Farhi2001,mcgeoch2014adiabatic}.
At the end of the annealing, a binary assignment is produced for each physical qubit.
A final postprocessing step is needed to go from physical qubits back to logical ones, thus obtaining a candidate solution for the original QUBO formulation (prior to minor embedding). 
Due to noise during the annealing process, the obtained solution may not correspond to the global optimum of the QUBO problem. 
In other terms, the annealing is inherently probabilistic and has to be repeated multiple times. 
The final solution is obtained as the one that achieves the minimum value of the objective function (lowest energy). 
Performing multiple annealings of the same QUBO form is called \emph{sampling}. 

It is common practice \cite{SeelbachBenkner2021, Doan_2022_CVPR, Zaech_2022_CVPR, Arrigoni2022} to test the QUBO objective on classic computers using \emph{simulated annealing} (SA) \cite{Kirkpatrick1983}, a probabilistic optimization technique able to operate in the binary space.
SA is probabilistic, so solutions are sampled multiple times, as it happens during quantum annealing. 
In contrast, SA operates on the original QUBO without embedding (i.e. directly on the logical graph) since no physical graph is involved. 
Importantly, SA can be used to evaluate performance on larger problem instances, in anticipation of larger AQC architectures.
