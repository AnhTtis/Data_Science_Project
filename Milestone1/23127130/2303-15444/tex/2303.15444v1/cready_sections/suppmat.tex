\clearpage
\appendix
\begin{center}
\textbf{\Large Supplementary Material} 
\end{center} 
\section{Parameters and Implementation Details} 
This section highlights the parameter settings of the experiments presented in Sec.~\ref{sec:experimental-results} of the main paper. 

\begin{figure}[b!]
    \centering
    
    \begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{imgs/supplementary/dequmfbest_breadcube_00.jpg}
    \caption{\ourmethoddec outcome, misclassification error = 1.7\%.}
    \label{fig:adel_qa_best}
    \end{subfigure}
    
    \begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{imgs/supplementary/dequmfsabest_breadcube_02.jpg}
    \caption{\ourmethodsadec outcome, misclassification error = 0\%.}
    \label{fig:adel_sa_best}
    \end{subfigure}
    
    \begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{imgs/supplementary/breadcube_gt.jpg}
    \caption{Ground-truth segmentation.}
    \label{fig:adel_best_gt}
    \end{subfigure}
    
    \caption{A sample of the \emph{best} case for our approach on the \emph{breadcube} sequence of the \emph{AdelaideRMF} dataset \cite{wong2011dynamic}.} 
    \label{fig:adel_qualitatives_best}
\end{figure}


\begin{figure}[b!]
    \centering
    
    \begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{imgs/supplementary/dequmf_worst_gamebiscuit_00.jpg}
    \caption{\ourmethoddec outcome, misclassification error = 47.9\%.}
    \label{fig:adel_qa_worst}
    \end{subfigure}
    
    \begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{imgs/supplementary/dequmfsa_worst_gamebiscuit_01.jpg}
    \caption{\ourmethodsadec outcome, misclassification error = 1.7\%.}
    \label{fig:adel_sa_worst}
    \end{subfigure}
    
    \begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{imgs/supplementary/gamebiscuit_gt.jpg}
    \caption{Ground-truth segmentation.}
    \label{fig:adel_worst_gt}
    \end{subfigure}
    
    \caption{A sample of the \emph{worst} case for our approach on the \emph{gamebiscuit} sequence of the \emph{AdelaideRMF} dataset \cite{wong2011dynamic}.} 
    \label{fig:adel_qualitatives_worst}
\end{figure}

\paragraph{The \emph{chain strength} parameter.}
Following previous work (e.g., \cite{QuantumSync2021,Arrigoni2022}), we employ the \emph{maximum chain length criterion} for \emph{all} our experiments: 
given a logical graph (i.e., a QUBO problem to be solved), we first minor-embed it to the physical one. Once the final embedding is found, the length $l$ of the longest qubit chain is computed. The chain strength parameter is then set to $l$ plus a small offset, \textit{i.e.,} $0.5$ in our experiments. 

\paragraph{Number of anneals.}
We set the number of anneals as follows:
algorithms solving QUBOs built with the entire preference matrix ("one-sweep" methods \ourmethod and \ourmethodsa) are executed with 5000 and 100 anneals, respectively.
Algorithms employing our iterative
decomposition principle %
are configured differently:
\begin{enumerate}
    \item \ourmethoddec executes 2500 anneals for each subproblem, with subproblems containing 20 models each;
    \item \ourmethodsadec runs with 100 anneals for each subproblem, with subproblems having 40 models.
\end{enumerate} 
In Sec.~\ref{sec:quantum_comparison}, when evaluating \ourmethoddec and \suter \cite{Doan_2022_CVPR}, to provide a fair comparison we fix the amount of available resources per-routine.
In this setup, we configured all the optimization routines (SAs/QAs) with 100 anneals, regardless of the algorithm. 


\paragraph{Synthetic experiments.}
To collect statistics, we execute each CPU-based algorithm ten times and report average results.
Due to limited QPU availability in our subscription, \ourmethoddec is run 5 times.
In Figs.~\ref{fig:small_problems}-\ref{fig:star5_increasing_noise}, the 95\% confidence intervals are displayed around data points.
To provide a comparison on an equal basis, all the methods have been tested on the \emph{same} preference matrices. 
In addition, for this test we used a modified version of \textsc{RanSaCov} \cite{magrifusiello16}, which enforces disjointedness among the sets of the retrieved solution, as done in the (soft) constraint of \ourmethod.

\paragraph{Real experiments.} In the context of multi-model experiments of Sec.~\ref{sec:experimental-results} of the main paper (Tables~\ref{tab:hopkins}-\ref{tab:adel_mmf}), for a fair comparison, the  inlier threshold $\epsilon$ has been tuned per sequence as in \cite{magrifusiello16}. 
Preference matrices are instantiated  with a fixed points-to-models ratio, \textit{i.e.,} the number of models $m$ is always $m=\sigma n$. 
In our experiments, we select $\sigma=6$ for \emph{AdelaideRMF} \cite{wong2011dynamic} and the \emph{Hopkins} \cite{TronVidal07} benchmarks, $\sigma=10$ for the \emph{York} dataset \cite{Denis2008ECCV}.
Recall that the number of models is equal to the number of logical qubits, which essentially determines the problem complexity.
Additional insights on the characteristics of the involved datasets are provided in Tab.~\ref{tab:datasets}. 
Following synthetic experiments, CPU-based algorithms are run ten times and averaged results are reported; \ourmethoddec, when available, is executed once.

\begin{table}[t]
    \centering
    \resizebox{1\columnwidth}{!}
    {
    \begin{tabular}{cccccc}
         & & \emph{AdelaideRMF} \cite{wong2011dynamic} & \emph{Traffic2} \cite{TronVidal07} & \emph{Traffic3} \cite{TronVidal07} & \emph{York} \cite{Denis2008ECCV} \\
         \toprule 
         \multirow{3}{1.25em}{$n$} & \emph{mean} & 160 & 241 & 332 & 119\\
         & \emph{min} & 105 & 41 & 123 & 25 \\
         & \emph{max} & 239 & 511 & 548 & 627 \\
         \midrule
         \multirow{3}{1.25em}{$m$} & \emph{mean} & 960 & 1446 & 1994 & 1188\\
         & \emph{min} & 630 & 246 & 738 & 250 \\
         & \emph{max} & 1434 & 3066 & 3288 & 6270 \\
         \midrule
         \multirow{2}{1.25em}{$k$} %
         & \emph{min} & 2 & 2 & 3 & 2 \\
         & \emph{max} & 4 & 2 & 3 & 3 \\
         \midrule
         $\sigma$ & \emph{const} & 6 & 6 & 6 & 10\\
         \bottomrule
    \end{tabular}
    }
    \caption{Details of each real dataset used in the experiments of Sec.~\ref{sec:experimental-results} in the main paper: $n$ refers to the number of points, $k$ to the number of ground-truth structures, $m$ is the number of models computed as $m=\sigma n$.}
    \label{tab:datasets}
\end{table}


\paragraph{Runtimes for real experiments.}
Runtimes of the experiments executed on real world datasets are reported in Tab.~\ref{tab:runtimes} to provide completeness to our experimental evaluation.
However, these numbers may report a distorted comparison because of:
\begin{enumerate}
    \item \emph{Implementation differences}, as \ransacov  \cite{magrifusiello16} is MATLAB-based, whilst \suter \cite{Doan_2022_CVPR} and all of our methods are Python-based;
    \item \emph{Hardware differences}, highlighted in the caption of Tab. \ref{tab:runtimes};
    \item \emph{Overheads} related to AQCs; specifically, we are including \emph{network communication times} since we must access a shared AQC we do not have in loco, and \emph{resource allocation times} due to other users that would not be present with an in-house machine.
\end{enumerate} 
These technicalities currently make the comparison \emph{unfair}.
Additionally, we point out that the annealing time ($20\mu s$) is independent on the problem size.
Hence, in anticipation of stable AQCs, we believe our quantum approach is a prominent research direction in terms of runtime, too.

\begin{table}[t!]
    \centering
    \begin{adjustbox}{max width=\columnwidth}
    {
    \begin{tabular}{lccccc}
        \toprule
         &  {\emph{AdelaideRMF-S}} & {\emph{AdelaideRMF-M}} & {\emph{Traffic2}} & {\emph{Traffic3}} & {\emph{York}} \\
         \midrule
         $^{\dagger}$\ransacov \cite{magrifusiello16} & - & \textbf{0.48} & \textbf{1.46 } & \textbf{2.14}  & \textbf{0.24} \\
         $^{\dagger}$\ourmethodsadec (ours) & \textbf{0.99} & 0.77 & 2.54 & 3.65 & 1.93 \\
         $^{\ddagger}$\ourmethodsa (ours) & 23.92 & 10.72 & 53.05 & 76.46 & 102.38 \\
         $^{\ddagger}$\ourmethoddec (ours) & 89.62 & 116.71 & - & 376.09 & - \\
         $^{\ddagger}$\suter \cite{Doan_2022_CVPR} & 2349.27 & - & - & - & - \\
         \bottomrule
    \end{tabular}
    }
    \end{adjustbox}
    \caption{Mean runtimes [$s$]. \emph{AdelaideRMF-S} is single-model, with \emph{M} is multi-model. ``$^{\dagger}$'': run on an Intel-i7-8575U; ``$^{\ddagger}$'': run on an Intel-i9-7900X. Please zoom in.} 
    \label{tab:runtimes} 
\end{table}

\paragraph{Comparison with quantum state of the art.}
Concerning Tab.~\ref{tab:adelrmf_single}, we fix the inlier threshold as $\epsilon = 0.045$ for all the sequences for both our method and \suter \cite{Doan_2022_CVPR}.
Apart from $\epsilon$ (that essentially defines a bound on the distance of inlier points to models), when evaluating \suter, we use the source code provided by the authors and the default parameter settings therein. 

\paragraph{Additional insights on \ourmethod.} Recall that the quadratic term in our QUBO formulation is determined by the product $P^{\mathsf{T}}P$, with $P$ being the preference matrix. Hence, from an interpretative perspective, the quadratic term between models $i$ and $j$ (which we denote by $q_{ij}$ from now on) is equal to the number of points the two have in common. By leveraging this interpretation, a useful observation can be made for a very peculiar situation, namely the case where, for a fixed $i$, we have $q_{ij} = 0 \  \forall j \neq i$. This means that model $i$ does not share any point with the remaining models in $P$: In such a situation, we can conclude that model $i$ is an essential component of the final solution $\mathbf{z}$, as it covers {at least} one point that is not explained by any other model. In other terms, $\mathbf{z}_i = 1$ must be true at the global optimum. This simple observation thus allows to prune the search space and proceed with embedding a \emph{reduced} (i.e., smaller) QUBO. 
On the contrary, if the full logical graph was considered, then the minor embedding procedure would avoid mapping model $i$ to the QPU; this would result in unmapped logical qubits, which is undesirable in practice. 

\section{Qualitative Results}
In this section, we provide qualitative results to visually interpret the behaviour of the proposed approaches. 
Results are given in Figs.~\ref{fig:adel_qualitatives_best}, \ref{fig:adel_qualitatives_worst} and \ref{fig:hopkins_qualitatives_best}.
In particular, a failure case is reported in Fig.~\ref{fig:adel_qa_worst} where \ourmethoddec exhibits pseudo-random behaviour. 
Apart from that, in general, both variants of our approach---\ourmethoddec and \ourmethodsadec---return accurate results, confirming the outcome of the quantitative analysis reported in the main paper.

\begin{figure}
    \centering
    
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{imgs/supplementary/dequmf_cars2B_best.jpg}
        \caption{\ourmethoddec outcome, misclassification error = 0.2\%.}
        \label{fig:hopkins_qa_best}
    \end{subfigure}

    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{imgs/supplementary/dequmfsa_cars2B_00_best.jpg}
        \caption{\ourmethodsadec outcome, misclassification error = 0\%.}
        \label{fig:hopkins_sa_best}
    \end{subfigure}
        
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{imgs/supplementary/cars2B_gt.jpg}
        \caption{Ground-truth segmentation.}
        \label{fig:hopkins_gt}
    \end{subfigure}

    \caption{A sample case for our approach on the \emph{cars2B} sequence of the \emph{Hopkins} benchmark \cite{TronVidal07}. Point membership is colour-coded and ground-truth segmentation is also reported.} 
    \label{fig:hopkins_qualitatives_best}
\end{figure}


\section{Minor Embeddings} 
Finally, we visually analyze the concepts of logical and physical graphs.
Specifically, Fig.~\ref{fig:star5embed} reports both the logical graph and its mapping to the QPU (obtained as a result of a minor embedding procedure) on exemplary problems with 20 and 100 logical qubits, respectively. 
Fig.~\ref{fig:star5embed} clearly shows that the problem with 100 logical qubits requires a significantly larger amount of physical qubits, as already observed in the main paper. \newpage

\begin{figure}[h!]
    \centering
    \begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{imgs/supplementary/FINAL_FULL_MERGED.png}
    \caption{Visualization of a full QUBO from \emph{Star5} \cite{toldo2008robust}. The 100 nodes composing the logical graph (left) are mapped to 869 physical qubits (right).}
    \label{fig:fullstar5embed}
    \end{subfigure}

    \begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{imgs/supplementary/FINAL_MERGED_20.png}
    \caption{Visualization of a subproblem extracted from \emph{Star5} \cite{toldo2008robust}. The 20 nodes composing the logical graph (left) are mapped to 32 physical qubits (right).}
    \label{fig:substar5embed}
    \end{subfigure}
    
    \caption{
    Visual representation of logical and physical graphs when working with the \emph{Star5} dataset \cite{toldo2008robust}. Sub-figure (a) refers to a preference matrix with $n=250$ points and $m=100$ models while sub-figure (b) considers a small portion of such preference matrix, corresponding to a sub-problem with $m=20$ models (as tackled by our decomposed approach).
    } 
    \label{fig:star5embed} 
\end{figure} 
