\section{Filtering algorithms based on the Kalman filter}

% The simplest НижеBayesian filter is Kalman filter~\cite{Kalman}.
The Kalman filter is based on the assumption of Gaussian distribution of a state, control, and measurements vectors, i.e. $\vx_t\sim\mathcal{N}(\rvx_t, \mP_t)$, $\vu_t\sim \mathcal{N}(\rvu_t, \mQ)$, and $\vz_t\sim\mathcal{N}(\rvz_t, \mR)$, where~$\mP_t$, $\mQ$, and $\mR$ are the state, process, and measurement covariance matrices.
At each step, the state $\rvx_t$ is updated with a linear transformation $\mF$ and the known control dynamics~$\rvu_t$: 
\begin{equation}
    \rvx^-_{t+1} = \mF \rvx_t + \rvu_t.
    \label{eq::Kalman_motion}
\end{equation} 
Besides the measurement vector $\rvz^*_t$, we compute the predicted measurement vector 
\begin{equation}
    \rvz_t=\mH\rvx_t,
    \label{eq::Kalman_pred_measurement}
\end{equation}
which plays the crucial role in the correction of $\rvx^-_{t+1}$.
To correct updated state $\rvx^-_{t+1}$, Kalman filter uses the following equation 
\begin{equation}
    \rvx_{t+1} = \rvx_{t+1}^- + \mK_{t+1}(\rvz^*_{t+1} - \rvz_{t+1}),
    \label{eq::Kalman_x_update}
\end{equation}
where the Kalman gain $\mK_{t+1} = \mP_{t+1}^- \mH^\top (\mH \mP_{t+1}^- \mH^\top + \mR)^{-1}$.
The state covariance matrix~$\mP_{t+1}$ firstly predicted from motion equation as 
\begin{equation}
    \mP_{t+1}^- = \mF \mP_{t} \mF^\top + \mQ,
    \label{eq::Kalman_P_update}
\end{equation}
where $\mQ$ is a pre-defined constant process covariance matrix related to the motion noise $\boldsymbol{\eta}$ from~\eqref{eq::motion}.
And then it is updated with the Kalman gain $\mK_{t+1}$ according to the following formula: $\mP_{t+1} = (\mI - \mK_{t+1} \mH)\mP_{t+1}^-$.
The equations of the state~\eqref{eq::Kalman_motion} and measurement~\eqref{eq::Kalman_pred_measurement} updates are the particular cases of~\eqref{eq::motion} and~\eqref{eq::measurement}, respectively.
% The measurement true result is $\rvz_t$, while its predicted value $\tilde{\rvz}$ is found from a measurement matrix $\mH$, such that  $\tilde{\rvz}_t=\mH\rvx_t$, 
% The process covariance matrix $\mQ_t$ is 
% \begin{align*}
% \mQ_t= \langle (\rvx_t(\boldsymbol{\eta}) - \rvx_t(\boldsymbol{0})) (\rvx_t(\boldsymbol{\eta}) - \rvx_t(\boldsymbol{0}))^{\top} \rangle_{\boldsymbol{\eta}}
% \end{align*}

% \begin{align*}
% \mR=\sigma d^2 {\bf 1},
% \end{align*}

% \begin{align*}
% \mQ_t = \begin{bmatrix}
%        \cos\phi^2\,\sigma u^2 + u^2\,\sin\phi^2\, \sigma \phi^2  & (\sigma u^2 + u^2 \sigma\phi^2) \sin2\phi/2 & -u\,\sin\phi\,\sigma\phi^2 \\
%        ( \sigma u^2 + u^2 \sigma\phi^2) \sin2\phi/2 & \sin\phi^2\,\sigma u^2  + u^2 \, \cos\phi^2\sigma \phi^2 & u \cos\phi\,\sigma\phi^2\\
%        -u\,\sin\phi\,\sigma\phi^2 & u\, \cos\phi\,\sigma\phi^2 & \sigma\phi^2
% \end{bmatrix},
% \end{align*}

% \begin{align*}
% \mP_0=
% \begin{bmatrix}
%            \sigma x_0^2 & 0 & 0\\
%            0 & \sigma y_0^2  & 0\\
%            0 & 0 & \sigma \phi_0^2.
%     \end{bmatrix}
% \end{align*}
% These results are also noisy and the noise is described by the measurement covariance matrix $\mR$.
%Here  noise $\zeta \sim \mathcal{N}(0,\mR)$, where $\mR$ is measurement covariance matrix.
% Note, that the state, process, i.e, $\rvx_{t-1}$ to $\rvx_t$ transition, and measurement covariance matrices $\mP$, $\mQ_t$, and $\mR$ must be input parameters in the algorithm. 
% The first one determines the uncertainty of initial state knowledge and thus initialization is task-dependent. 
% The matrix $\mQ_t$ is a process covariance matrix, which determines the correlation between two consecutive states $\rvx_{t+1}$ and $\rvx_t$. 
% The measurement covariance matrix is also task-dependent and in the simplest case contains square errors on the diagonal. 
% The example will be considered later for the object on the plane motion. 
% To apply the filter for the plane motion directly its extended version will be used. 
% The complete description of the Kalman filter algorithm is the following:
% \begin{itemize}
% \item Predict stage
%  \begin{equation}
%  \begin{split}
% &\rvx_{t+1}^- = \mF \rvx_t + \rvu_t\\
% &\mP_{t+1}^- = \mF \mP_{t} \mF^T + \mQ_{t+1},
% \end{split}
%  \label{eq::SKF}
% \end{equation}
% \item Update stage
%  \begin{equation}
%  \begin{split}
%  &\tilde{\rvz}_{t+1} = \mH \rvx_{t+1}^-\\
%  &\rvy_{t+1} = \rvz_{t+1} - \tilde{\rvz}_{t+1}\\
%  &\mK_{t+1} = \mP_{t+1}^- \mH^\top (\mH \mP_{t+1}^- \mH^\top + \mR)^{-1}\\
%  &\rvx_{t+1} = \rvx_{t+1}^- + \mK_{t+1} \rvy_{t+1}\\ &\mP_{t+1} = (\mI - \mK_{t+1} \mH)\mP_{t+1}^-.
%  \end{split}
%  \end{equation}
%  \end{itemize}

This method iteratively gives estimate of state $\rvx_{t+1}$ and covariance matrix $\mP_{t+1}$ from the previous state $\rvx_{t}$ and measurement results $\rvz_{t+1}$. 
The main advantages of Kalman filter are low computational costs and low memory consumption. 
At the same time, it has significant drawbacks, like the linearity of motion and measurement equations, and the assumptions on the normal noise.
 
To generalize the Kalman filter to non-linear motion and measurement equations, the Extended Kalman Filter was proposed~\cite{EKF}. 
It operates with non-linear equations on coordinates $\rvx$ and measurements $\rvz$. 
These non-linear equations are incorporated in the standard Kalman filter as: 
\begin{equation}
\begin{split}%\label{5a}
&\rvx_{t+1}^- = f(\rvx_t, \rvu_{t+1}, 0)\\
&\mF_{t+1} = \left.\frac{\partial f(\rvx, \rvu_{t+1})}{\partial \rvx}\right|_{\rvx = \rvx_t}
\end{split}
\qquad \begin{split}%\label{6a}
 &\rvz_{t+1} = g(\rvx_{t+1}^-, 0)\\ 
 &\mH_{t+1} = \left.\frac{\partial g(\rvx)}{\partial \rvx}\right|_{\rvx = \rvx^-_{t+1}}\\
 \end{split}
 \label{eq::EKF}
 \end{equation}
In addition, since motion noise $\boldsymbol{\eta}$ is not additive, the covariance process matrix $\mQ \equiv \mQ_t$ depends on time moment  and is recomputed in every time step as follows
\begin{equation}
    \mQ_t = \frac1s \sum_{i=1}^s (\rvx_{t+1}^- - f(\rvx_t, \rvu_{t+1}, \boldsymbol{\eta}_i)) 
 (\rvx_{t+1}^- - f(\rvx_t, \rvu_{t+1}, \boldsymbol{\eta}_i)^\top,
 \label{eq::Qt_EKF}
\end{equation}
where $\boldsymbol{\eta}_i, \; i=1,\ldots,s$ are sampled from some pre-defined distribution $\mathcal{N}(0, \mM)$ corresponding to our assumption on the motion noise.
Other equations in the Extended Kalman filter coincide with equations in the Kalman filter. 

% \begin{itemize}
% \item Predict stage
% \begin{equation}
% \begin{split}%\label{5a}
% &\rvx_{t+1}^- = f(\rvx_t, \rvu_{t+1})\\
% &\mP_{t+1}^- = \mF \mP_{t} \mF^T + \mQ_{t+1}\\
% % &\mF = \frac{\partial f(\rvx_{t, 1}, \rvx_{t, 2}, ...)}
% % {\partial(\rvx_{t,1}, \rvx_{t2}, ...)}
% &\mF = \left.\frac{\partial f(\rvx, \rvu_{t+1})}{\partial \rvx}\right|_{\rvx = \rvx_t}
% \end{split}
% \label{eq::EKF}
% \end{equation}
% \item Update stage 
%  \begin{equation}
%  \begin{split}%\label{6a}
%  &\tilde{\rvz}_{t+1} = g(\rvx_{t+1}^-)\\ 
%  &\mH_{t+1} = \left.\frac{\partial g(\rvx)}{\partial \rvx}\right|_{\rvx = \rvx^-_{t+1}}\\
%  &\mK_{t+1} = \mP_{t+1}^- \mH_{t+1}^T (\mH \mP_{t+1}^- \mH_{t+1}^T + \mR)^{-1}\\
%  &\rvx_{t+1} = \rvx_{t+1}^- + \mK_{t+1} (\rvz_{t+1} - \tilde{\rvz}_{t+1})\\
%  &\mP_{t+1} = (\mI - \mK_{t+1} \mH_{t+1})\mP_{t+1}^-
%  \end{split}
%  \end{equation}
% \end{itemize}

Since the exact computation of Jacobians in~\eqref{eq::EKF} can be computationally intensive,
the unscented Kalman filter~\cite{Julier} computes $f$ and $g$ at the specific sigma points and uses the computed values to approximate the corresponding Jacobians.
This approach reduces the runtime of every iteration but can lead to slow convergence if the approximations of Jacobians are not sufficiently accurate.

In this section, we have briefly described some filtering algorithms inspired by the classical Kalman filter.
They extend the classical Kalman filter to non-linear motion and measurement equations.
However, they still assume that the noise distribution is normal and require the initial object state and its covariance.
These assumptions and requirements limit the practical usage of the aforementioned filtering algorithms in a real-world scenario.



% Although the Extended Kalman filter addresses the non-linearity of motion and measurement equations, it still suffers from non-Gaussian noise.
% Moreover, this filtering algorithm requires the initial object state and its covariance, which may not be available in a real-world scenario. 
 
% This method iteratively gives new expectations for state $\rvx_{t+1}$ and covariance matrix $\mP_{t+1}$ from the previous moment of time, motion, and measurement results. 
% The main advantages are low complexity $O(d^{2.376})$ and low memory consumption $O(d^2)$, where $d$ is a state dimension.
% At the same time, it is still valid under assumptuion of the normal distribution of noise. Moreover, the iteration algorithm requires knowledge of the initial state and its covariance, which might not be true in the general case. 
 
% The only challenge in nonlinear equations is covariance matrix description. 
% This is reached via Taylor expansion and uses Jacobians for covariance matrices $\mP_t$ and $\mH$. 
% The following equation~(\ref{eq::EKF}) are used instead of~(\ref{eq::SKF}):

% The other modification of the Kalman filter -- unscented Kalman filter. It also deals with nonlinear functions $f$ and $g$ but instead of direct Jacobian computation, its estimation is used. The estimation is based on the states at the specific, so-called sigma points. It speeds up the Kalman steps computations. 
% However, all other Kalman filter drawbacks remain. 
% Though this might be important in high dimensional systems, in the considered test problems the Jacobian may be explicitly computed and the analytical expression is used, which is always better than the approximate methods.

% One more extension of Kalman filter, the so called ensemble Kalman filter is interesting, as uses idea similar to particle filtering. Here the ensemble of states is used to avoid using covariance matrix. This makes it faster for high-dimensional filtering. However, the method still relies on the approximation of the prior states by a Gaussian, which is a strong limitation in many cases.
