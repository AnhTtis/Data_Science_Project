% Invited review for Patterns, March 2023
% Arxiv version 1
% Difference to submitted version:
%     layout of title, figure paths removed, bibliography embedded

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  Welcome to the Cell Press LaTeX template,   %%%
%%%  version 1.3. This is a minimalist template  %%%
%%%  to help you organize your article for       %%%
%%%  publication at Cell Press.                  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt,letterpaper]{article}
\usepackage[a4paper, total={7in, 10in}]{geometry}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{helvet}
\usepackage{authblk}
\usepackage{hyperref}
% My own packages:
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\makeatletter
\renewcommand{\maketitle}{\bgroup\setlength{\parindent}{0pt}
\begin{flushleft}
  \textbf{\@title}
  \\ \vspace*{1cm} %Added by Aapo for arxiv
  \@author
\end{flushleft}\egroup}
\makeatother

\newcommand{\thetab}{{\boldsymbol{\theta}}}
\newcommand{\lambdab}{{\boldsymbol{\lambda}}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\s}{\mathbf{s}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\n}{\mathbf{n}}
\renewcommand{\b}{\mathbf{b}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\cc}{\mathbf{c}}
\renewcommand{\u}{\mathbf{u}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\U}{\mathbf{U}}
\newcommand{\myindex}{\text{$t$}} 
\newcommand{\compIndex}{\text{$j$}} 
\newcommand{\A}{\mathbf{A}}
\newcommand{\J}{\mathbf{J}}
\newcommand{\C}{\mathbf{C}}
\newcommand{\M}{\mathbf{M}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\ee}{\mathbf{e}}
\renewcommand{\t}{\tau}
\newcommand{\dm}{\Delta \mathbf{E}}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newcommand{\var}{\text{var}}
\newcommand{\diag}{\text{diag}}
\newcommand{\Ctwo}{\mathcal{C}^2}
\renewcommand{\b}{{\bf b}}
\renewcommand{\a}{{\bf a}}
\renewcommand{\v}{{\bf v}}
\newcommand{\xb}{{\bf x}}
\newcommand{\ub}{{\bf u}}
\newcommand{\zb}{{\bf z}}
\newcommand{\fb}{{\bf f}}
\newcommand{\hb}{{\bf h}}
\newcommand{\defeq}{:=}
\newcommand{\dd}{\text{d}}




%%%  Insert title below; no date is needed

\title{\Large %Large added by Aapo for Arxiv
  Nonlinear Independent Component Analysis for Principled Disentanglement in Unsupervised Deep Learning}
\date{}

%%%  Insert author names, affiliations and corresponding author 
%%%  email (do not include titles, positions, or degrees).

\author[1*]{Aapo Hyv\"arinen}
\author[2]{Ilyes Khemakhem}
\author[3]{Hiroshi Morioka}

\affil[1]{Department of Computer Science, University of Helsinki}
\affil[2]{Gatsby Computational Neuroscience Unit, University College London}
\affil[3]{RIKEN AIP}
\affil[*]{Correspondence: aapo.hyvarinen@helsinki.fi}


\usepackage[super,comma,sort&compress]{natbib}\bibliographystyle{NUMBERED}


\begin{document}

\maketitle

\section*{Summary} %150 words

A central problem in unsupervised deep learning is how to find useful representations of high-dimensional data, sometimes called "disentanglement". Most approaches are heuristic and lack a proper theoretical foundation. In linear representation learning, independent component analysis (ICA) has been successful in many applications areas, and it is  principled, i.e.\ based on a well-defined probabilistic model. However, extension of ICA to the nonlinear case has been problematic due to the lack of identifiability, i.e.\ uniqueness of the representation. Recently, nonlinear extensions that utilize temporal structure or some auxiliary information have been proposed. Such models are in fact identifiable, and consequently, an increasing number of algorithms have been developed. In particular, some self-supervised algorithms can be shown to estimate nonlinear ICA, even though they have initially been proposed from heuristic perspectives. This paper reviews the state-of-the-art of nonlinear ICA theory and algorithms.


\section*{Keywords}

Unsupervised learning ; representation learning ; disentanglement ; independent component analysis ; nonlinear ICA

\section*{Introduction}

Recent advances in data collection have resulted in very large datasets,
including images
\citep{lecun1998gradientbased,krizhevsky2012imageneta,deng2009imagenet},
3D shapes~\citep{chang2015shapenet},
text~\citep{marcus1993building,maas2011learning},
music~\citep{bertin-mahieux2011million},
and graphs and networks~\citep{hu2020open,yanardag2015deep}.
As the amount and complexity of the
data started growing, most of the work in machine learning research went towards
developing preprocessing pipelines to assist the extraction of meaningful
information from large datasets, allowing for efficient learning.  With the
rise of deep learning, preprocessing shifted from hand-crafted, expertise-based
feature engineering to utilizing neural networks to implicitly learn useful
representations.  This is known as \textit{representation learning}, and it
has grown to be one of the pillars of modern machine learning.
Representations learned by deep neural networks are now
widely used in many machine learning applications, including speech recognition
and processing~\citep{dahl2011contextdependent,seide2011conversational},
natural language processing~\citep{bengio2003neural,devlin2019bert}, %
action recognition~\citep{korbar2018cooperative}, domain adaptation~\citep{wang2018deep}, and many more.

Learning good representations can have a significant impact on the performance
of subsequent machine learning~\citep{bengio2013representation}.
Representation learning can sometimes be based on supervised learning with labelled data, in which case the representation is transferred to a new data set. But since  labelling is a costly and time-consuming endeavour and only a small percentage of today's datasets are labelled, it would be better to learn the representation without any labels or targets, that is, in an unsupervised way.

The quality of a learned representation is frequently characterized by its capacity to
improve the performance of a ``downstream'' task in which the user is currently
engaged. This criterion, however, is only meaningful when such a task exists
and is clearly defined; typically, it consists of classification or regression
on a labelled dataset. 
However, different representations may be optimal for different classification tasks.
It would better to be able to assess the quality of a representation by a criterion that is inherent to the representation itself, rather than reliant on the context or task in which
it may be employed. Here, we consider the problem of finding a generally useful representation based on unsupervised learning.

Well-known unsupervised methods include variational autoencoders (VAE)~\citep{kingma2014autoencoding,rezende2014stochastic} and normalizing flows~\citep{kobyzev2020normalizing} learn a posterior
distribution over a possibly
lower-dimensional latent variable.  It is hoped that such a posterior will
correspond to the underlying distribution of statistically independent sources
of variation.
A related line of research is being developed for the related goal of learning \textit{disentangled representations}~\citep{higgins2017betavae,alemi2017fixing,burgess2018understanding,chen2018isolating,esmaeili2019structured,mathieu2018disentangling,kim2018disentangling}. The objective is to isolate the influence of all factors of variation, which again translates to learning a representation with independent components~\citep{bengio2013representation,burgess2018understanding}. Many methods thus learn disentangled representations by imposing independence on the latent variables and adding regularization terms to the VAE objective in an ad-hoc manner~\citep{zhao2017infovae,gao2019autoencoding,achille2018information,kumar2017variational,esmaeili2019structured}. 


A recent line of research aims to go further than mere independence by  learning representations
that are true to the explanatory factors of variation behind the data.
This desideratum is formalized by the notion of \textit{identifiability}.
Fundamentally, an identifiable %
probabilistic model
can only learn one representation in the limit of infinite data: the ground
truth generative factors.
Identifiability is thus necessary for learning representations that are
%
semantically meaningful, reproducible, interpretable and better suited for
downstream tasks~\citep{bengio2013representation,peters2017elements,schmidhuber1996semilinear}.


Unfortunately, the above-mentioned techniques do not allow for any
theoretical identifiability guarantees.
In fact, disentangled representations are not identifiable in general.
In other words, learning nonlinear models that seek independence results in
arbitrary representations that are not always related to the ground truth
factors of variation.
A large scale empirical study~\citep{locatello2019challenging} showed that the proposed models for disentanglement exhibit substantial variance depending on hyperparameters and random seeds.
Unsupervised learning of identifiable nonlinear
representations has long been known to be theoretically impossible~\citep{Hyva99NN,locatello2019challenging} without any ``inductive biases'', i.e.\ suitable constraints on the model.


Within representation learning, identifiability has mostly been studied in the
context of \textit{independent component analysis} (ICA).
In ICA, the observations are considered to be a mixture of independent latent components.  The goal is to learn an ``demixing'' transformation capable of recovering the original components based on their independence and the observed mixed data. In the linear case, the theory and algorithms are already quite developed~\citep{Hyva00NN,Hyvabook}, while nonlinear versions of ICA are quite recent. The promise is that being probabilistic and identifiable, nonlinear ICA is a general, principled solution for the problem of disentanglement.

Meanwhile, recent work in computer vision has successfully proposed ``self-supervised'' feature extraction methods from a purely heuristic perspective. The idea is to reformulate the unsupervised learning problem as a supervised learning problem using a judiciously defined ``pretext'' task.
One of the most fundamental cases is to train a neural network to discriminate the observed, unlabelled data from some artificially generated noise~\citep{Gutmann12JMLR}.
A large number of heuristic methods have been proposed based on the intuitively comprehensible structure of images ~\citep{misra2016shuffle,noroozi2016unsupervised,larsson2017colorization}.  
As such, self-supervised learning has the potential of providing computationally efficient algorithms for disentanglement. Empirically, such approaches have allowed unsupervised learning to be leveraged for supervised tasks resulting in dramatic performance improvements. However,  it is widely  acknowledged that most such methods lack theoretical grounding. Since such methods are not necessarily based on probabilistic modelling, the question of identifiability cannot always be meaningfully approached, although uniqueness can be considered from a more general perspective~\citep{d2020underspecification}. Ideally, we would like to combine self-supervised learning with probabilistic modelling and achieve identifiability.

In this paper, we review recent methods for unsupervised representation
learning that aim to learn the ground truth generative factors. Thus, we focus on probabilistic models which are identifiable, the main framework being nonlinear ICA.
Particular emphasis will here be put on algorithms, especially of the self-supervised kind. (See our companion paper~\citep{Hyva23AISM} for a more theoretical treatment of identifiability.)



%\section*{Results}

\section*{Background: Linear ICA}
\label{sec:intro:lin_ica}

Over the decades, ICA has been
extensively studied in the linear setting, where the mixing is considered to be
a performed by a matrix~\citep{comon1994independent,Hyva00NN,Hyvabook,cardoso2001three}. %
Linear ICA has applications in neuroscience, including functional magnetic resonance imaging (fMRI)~\citep{mckeown1998analysis,calhoun2003ica,Beckmann05}
and EEG/MEG~\citep{delorme2007enhanced,milne2009independent,Brookes11, Hyva10NI},
document analysis~\citep{bingham2002ica,podosinnikova2015rethinking}, finance~\citep{back1997first,oja2000independent}, astronomy~\citep{nuzillard2000blind}, image processing~\citep{Hyvanisbook}
and many more fields.  The central theoretical result is that if all the latent components (``sources'') are
\textit{non-Gaussian}, linear ICA is identifiable.


Consider a vector of latent variables $\mathbf{s}=(s_1,\ldots,s_d)$ which is transformed through an unknown linear mixing into observations $\mathbf{x}$:
%
\begin{equation}
        \label{eq:intro:ica}
        \mathbf{x} = \mathbf{A} \mathbf{s},
\end{equation}
where $\mathbf{A} \in \mathbb{R}^{d\times d}$ is an invertible ``mixing'' matrix.
We want to know if we can recover the original but unknown signals $s_i$ while making no or only very weak assumptions on its distribution.
Both the distribution of $\mathbf{s}$ and the mixing matrix $\mathbf{A}$ are unknown, making it difficult to determine whether a good fit to the data is related to the true generative process. This problem is also known as blind source separation (BSS).

A well-known result is that if $\s$ is Gaussian, we cannot recover it from the mixtures. This is easy to prove. It is enough to consider the special case where $\s$ is constrained to be white in the sense that the $s_i$ are uncorrelated variables with unit variance. Then, any orthogonal transformation of the components has exactly the same distribution, which is due to the rotational symmetry of the white Gaussian distribution. Its probability density function (pdf) is $p(\s)\propto \exp(\|\s\|^2/2)$ which only depends on the norm; it will not change if $\s$ is transformed by an orthogonal transformation. Thus, an arbitrary orthogonal transformation could always be made on $\s$, resulting in exactly the same observed distribution, so that orthogonal transformation cannot be determined from the data. Another way of looking at this is that 
a (zero-mean) Gaussian distribution is completely determined by covariances. Now, the number of covariances is $\approx d^2/2$ due to symmetry ($\text{cov}(x_i,x_j)=\text{cov}(x_j,x_i)$). So, we cannot solve for the $d^2$ parameters in the mixing matrix; we have ``more variables than equations''.



The framework of Independent Component Analysis (ICA)~\citep{Jutten91,comon1994independent,Hyvabook} provides a solution to this problem by making two  assumptions.
First, the components $s_1, \ldots s_d$ of the latent vector $\mathbf{s}$ are statistically \textit{independent}. This means the pdf's factorize as
       \begin{equation} \label{factorize}
                p(\mathbf{s}) = \prod_i p_i(s_i).
        \end{equation}
As we saw above, this is not enough since the model is not identifiable with white Gaussian variables, and in the special case of Gaussian variables, whiteness implies independence. 
        Thus, most importantly, we make the second assumption that all the components have \textit{non-Gaussian} distributions (except perhaps one).
  %
Under these assumptions, the model~(\ref{eq:intro:ica}) is identifiable, meaning that the linear mixing, as well as the true components, can be estimated.
Linear ICA achieves this goal by learning an \textit{demixing} matrix $\mathbf{B}$ such that
   $     \mathbf{z}:=\mathbf{B}\mathbf{x}$
 has statistically independent components: it can be proven that only the true components are independent and no mixtures can be, if the true components are non-Gaussian as well as independent.
We note that still, the linear ICA problem has two indeterminacies: The ICA model does not detemine the permutation and scaling of the independent components. The components can be arbitrarily permuted, and any component can be multiplied by a scalar constant if the corresponding column of $\A$ is divided by that scalar.


The idea of blind source separation is illustrated in Fig.~\ref{separation.fig}. We have four original signals which have visually nice shapes for the purpose of this illustration. They are linearly mixed, and we apply principal component analysis (PCA) and linear ICA on them (middle row of Fig.~\ref{separation.fig})). PCA does not recover the original signals, while ICA does. Next we consider the case of nonlinear mixtures, i.e.\ nonlinear ICA, which is already alluded to at the bottom row of that figure.


\begin{figure}
\begin{center}
  \resizebox{\textwidth}{!}{\includegraphics{separation}}
  \caption{Identifiability of ICA and its application on blind source separation illustrated. The original signals (top row) are mixed either linearly or nonlinearly, in the middle and bottow rows, respectively. Then linear ICA (FastICA) or nonlinear ICA (in this case, PCL) is applied on those two mixtures.  Such methods do recover the original signals, as seen in the right-most column. For comparison, PCA and its nonlinear counterpart, VAE, are applied on the same mixtures in the middle column, and we see that separation is not achieved. } \label{separation.fig}
\end{center}
\end{figure}


\section*{Nonlinear ICA: Problem of Identifiability}
\label{sec:intro:nica}


%
A straightforward generalization of ICA to the nonlinear setting would assume
that the independent components are mixed into an
observed data vector through an arbitrary but usually smooth transformation.
The matrix $\mathbf{A}$ in the linear ICA model in Eq.~(\ref{eq:intro:ica}) is replaced by an invertible mixing function
$\mathbf{f}: \mathbb{R}^d \to \mathbb{R}^d$:
%
\begin{align}
        \label{NICA}
%
                \mathbf{x} &= \mathbf{f}(\mathbf{s}).
%
  %
\end{align}
The goal of nonlinear ICA is to learn an demixing function $\mathbf{g}$ that generalizes
the demixing matrix $\mathbf{B}$ such that
\begin{equation}
        \label{NICA_g}
\mathbf{z}\defeq\mathbf{g}(\mathbf{x})
\end{equation}
gives the original independent components as $\zb=\s$. Such a nonlinear mixing is illustrated in Fig.~\ref{separation.fig}, bottom row.


In the linear setting,
solving the problem of recovering the original signal $\mathbf{s}$ is equivalent to finding statistically independent components as we saw above.
However, a fundamental problem with nonlinear ICA is that solutions to Eq.~(\ref{NICA_g})
such that $\mathbf{z}$ has independent components
exist, and they are highly non-unique.
%
In fact, in the nonlinear case,
identifiability is a far more difficult aim to achieve. Nonlinear transformations introduce numerous degrees of freedom, rendering the problem ill-defined.

%

Unlike  in the linear case,
two non-Gaussian independent components $s_i$ and $s_j$ can be mixed nonlinearly while remaining statistically independent.
Equivalently, it is 
possible to explicitly construct a representation $\mathbf{z} = \mathbf{g}(\mathbf{x})$
with independent components that is
nonetheless a nonlinear mixture of the underlying independent generative
factors~\citep{Hyva99NN}.
This construction can be traced back to Darmois' work in the 1950's~\citep{darmois1953analyse},
which showed that for any two independent random variables $\xi_1, \xi_2$, we can
construct infinitely many random variables $y_1 = f_1(\xi_1,\xi_2)$ and $y_2 =
f_2(\xi_1, \xi_2)$ that are also independent.
This fundamental unidentifiability result is summarized by the following theorem:\citep{Hyva99NN}

\begin{theorem}
\label{th:intro:nica_niden}
       Let $\mathbf{x}$ be a random vector of any distribution. Then there exists a transformation
       $\mathbf{g}:\mathbb{R}^d\to[0,1]^d$
       such that
       $\mathbf{z} = \mathbf{g}(\mathbf{x})$ has a uniform distribution.
       In particular, the components $z_i \in \mathbf{z}$ are independent. Furthermore, the function $\mathbf{g}$ can be chosen so that the first variable is simply transformed by a scalar function: $z_1=g_1(x_1)$.
\end{theorem}

The function $\mathbf{g}$ in Theorem~\ref{th:intro:nica_niden} is constructed through
an iterative procedure analogous to Gram-Schmidt orthogonalization, by
recursively applying the conditional cumulative distribution function (CDF) of
$\mathbf{x}$:
\begin{equation}
        \label{eq:intro:darmois}
        z_i = g_i(x_{1},\ldots,x_i) \defeq \int_{-\infty}^{x_i} p(\tilde{x}_i \vert x_{1},\ldots,x_{i-1}) \dd \tilde{x}_i.
\end{equation}
This theorem indicates that nonlinear ICA is unidentifiable. One way is to notice that the $z_i$ can be easily point-wise transformed into independent Gaussian variables (by putting them through the inverse Gaussian cdf), and then the rotational indeterminacy holds as in the linear case~\citep{Hyva99NN}.
Another is to construct examples where it is clear that $z_i$ obtained by Equation~(\ref{eq:intro:darmois}) are not equal to the original $s_i$ even up to some nonlinear scaling indeterminacies. In particular, since $z_1 = g_1(x_1)$, as is clear from (\ref{eq:intro:darmois}), we %
would conclude that  $x_1$ is always one of the independent components,
which is absurd. The unidentifiability is illustrated in Fig.~\ref{ident.fig}~\textsf{a)-c)}.

Thus, as far as disentanglement is considered to mean finding the original components $\s$ in a nonlinear mixing such as Eq.~(\ref{NICA}), the very problem seems to be ill-defined. This is a fundamental problem which is receiving increasing attention in the deep learning community, and forms the basic motivation for nonlinear ICA theory.

\begin{figure}
\begin{center}
 \parbox{0.4\textwidth}{\hspace*{5mm}\vspace*{4mm}
\raisebox{0cm}{ \textsf{ a) Sources (s)}}\\
  \includegraphics[width=0.3\columnwidth]{scatter_s.png}\\
}
\parbox{0.4\textwidth}{\hspace*{5mm}\vspace*{4mm}
\raisebox{0cm}{ \textsf{b) Mixtures (x)}}\\
  \includegraphics[width=0.3\columnwidth,height=0.3\columnwidth]{scatter_x.png}\\ 
}
\parbox{0.4\textwidth}{\hspace*{5mm}\vspace*{4mm}
\raisebox{0cm}{\textsf{c)  Darmois construction}}\\
  \includegraphics[width=0.3\columnwidth]{scatter_hyv99.png}\\ 
}
\parbox{0.4\textwidth}{\hspace*{5mm}\vspace*{4mm}
\raisebox{0cm}{\textsf{d) Nonlinear ICA (PCL)}}\\
  \includegraphics[width=0.3\columnwidth]{scatter_pcl.png}\\ 
}
\end{center}
\caption{Illustration of the unidentifiability of the basic formulation of nonlinear ICA. a) Scatterplot of two original independent components. The points are colour-coded merely for the purpose of this illustration. b) A nonlinear mixing of those two independent components. c) Two estimated components obtained by the Darmois construction in Theorem 1. The components are independent, but clearly not equal to the original independent components. d) The components estimated by an identifiable version nonlinear ICA (PCL, based on temporal structure, explained later in the text); these components are a good match to the original components.}  \label{ident.fig}
\end{figure}



 \subsection*{Variational autoencoders}
 We next point out how the problem of unidentifiability concerns  VAEs~\citep{kingma2014autoencoding,rezende2014stochastic},
 which serve as the foundation for most of the recent disentanglement methods
~\citep{higgins2017betavae,burgess2018understanding,chen2018isolating,gao2019autoencoding,esmaeili2019structured,mathieu2018disentangling,kim2018disentangling,zhao2017infovae,achille2018information}.
 %
           %
 We suppose that the observation $\mathbf{x}$ is generated by a latent
 variable, which we denote by $\mathbf{z}$ as usual in that context. This generative process consists of sampling from a prior
 $p_\thetab(\mathbf{z})$ and then sampling from the ``likelihood''
 $p_\thetab(\mathbf{x}\vert\mathbf{z})$, also known as a \textit{decoder}.
 In practice, most work uses a nonlinear mixing model which formally looks very much like nonlinear ICA, but with Gaussian noise added:
 \begin{equation} \label{VAE}
   \x=\f(\zb) + \n
\end{equation}
where $\n$ is Gaussian noise of covariance $\sigma^2 \mathbf{I}$. Importantly, even the latent variables $\zb$ are Gaussian and have covariance equal to identity.
Therefore, in such a deep latent variable model, the unidentifiability is even more serious and can be more easily demonstrated. %
Since the latent vector $\zb$ is assumed to be Gaussian and white (uncorrelated variables of unit variance), any orthogonal transformation of the components has exactly the same distribution, as was already pointed out in the case of linear ICA above.
Thus, even the basic unidentifiability theory of linear ICA with Gaussian components shows that the latent variables cannot be recovered. Moreover, the Darmois theory applies as well, so the unidentifiability is even worse.
We note that exactly the same could be said about generative adversarial networks (GANs), which also start with a white Gaussian vector of latent variables and an arbitrary mixing function (but without noise).

Nevertheless, VAE is widely used for disentanglement, i.e.\ finding interesting features from the data. 
Some modifications of VAEs have also been proposed with the goal of improving disentanglement. One proposed solution~\citep{higgins2017betavae} is the $\beta$-VAE, a variant where a positive
 hyperparameter is added to the original VAE objective function. Further alternatives are provided by FactorVAE~\citep{kim2018disentangling} and
 $\beta$-TC-VAE~\citep{chen2018isolating}. However, there is little reason to assume that such variants would solve the fundamental problem of identifiability already pointed out by Darmois in the 1950's, and aggravated by using white Gaussian latents. In light of the theory above, it is thus questionable if VAE, or most of its variants, are well-suited for the purpose of disentanglement. Nevertheless, since the equation in (\ref{VAE}) is almost identical to the nonlinear ICA model, it is clear that some variant must be identiable since it can eventually coincide with nonlinear ICA (which will be made identifiable below). 

 In fact, it might be more meaningful to see VAE as a nonlinear version of PCA, which is a fundamental method for dimension reduction. Just like linear PCA, VAE can perform dimension reduction quite well, but there is no guarantee that the components obtained would be meaningful individually: only the low-dimensional manifold that they define can be considered meaningful. We note that autoencoders have been used for dimension reduction for a very long time~\citep{HechtNielsen95}. Empirically, Fig.~\ref{separation.fig} shows in the bottom row that VAE does not find the original components, i.e.\ it does not separate signals.

 On the other hand, VAE is also a general-purpose method for estimating deep latent variable models, and not at all restricted to the model just mentioned. The term ``VAE'' thus has two different meanings in the literature, which is sometimes confusing.  Below, we will actually discuss estimation methods based on VAE  which estimate identifiable versions of the nonlinear ICA model. Those models can in their turn be interpreted as  identifiable versions of the ``VAE model'' in (\ref{VAE}).

 %
%
%
%
%
%
%
%
 

\section*{Nonlinear ICA: Identifiable models and algorithms}

While the results above are negative, the main point in this review is to discuss how it is in fact possible to make nonlinear ICA models identifiable. The key is to provide some additional information to the model. The Darmois construction assumes that the data points are all obtained independently of each other and have identical distributions (called ``i.i.d.\ sampling''). However, this is often not the case in reality. A fundamental case is time series, where the time points are not independent of each other since there can be, for example, autocorrelations; nor are the time points necessarily identically distributed since the time series can be nonstationary.
           %

In recent years, a number of identifiability results
have been based on the temporal structure of the observed data, or, equivalently, the temporal structure of the independent components. Thus, we modify the mixing equation in (\ref{NICA}) to explicitly include the time index $t$:
\begin{align}        \label{NICA_t}
                \mathbf{x}(t) &= \mathbf{f}(\mathbf{s}(t)) 
\end{align}
Initial work assumed that the independent components are
autocorrelated time series~\citep{Harmeling03,Sprekeler14}.
Further models were subsequently proposed assuming that
the data come from non-stationary time series~\citep{Hyva16NIPS} or 
have general non-Gaussian temporal dependencies~\citep{Hyva17AISTATS}. %
However, temporal structure is not the only approach that leads to identifiability.
Alternatively, it can be that we have access to an auxiliary
variable that modulates the distributions of the independent components~\citep{Hyva19AISTATS,Khemakhem20iVAE} and leads to identifiability. The three properties  leading to identifiability are illustrated in Fig.~\ref{three.fig}. These models
achieved significant progress towards providing identifiability guarantees
%
by integrating side information into the generative model.
%
In the following, we go through the main models and learning methods.




\begin{figure}
\begin{center}
  \resizebox{\textwidth}{!}{\includegraphics{threenew}}
  \caption{Three properties of independent components that allow identifiability in nonlinear ICA: a) temporal correlations or other dependencies, b) nonstationarity, here depicted as nonstationarity of variance, c) an auxiliary variable $u$ that modulates the distribution of the component, without any temporal structure.} \label{three.fig}
\end{center}
\end{figure}


%
%
%
%
%
%



\subsection*{Time-Contrastive Learning}
Time-Contrastive Learning (TCL)~\citep{Hyva16NIPS} is a method for nonlinear ICA based on the
assumption that while the sources are independent, they are also \textit{non-stationary}
time series. 
This implies that they can be divided into non-overlapping segments, such
that their distributions vary across segments. Such an idea is well-known in the theory of linear blind source separation~\citep{Matsuoka95,Pham01,Cardoso01}. 
The non-stationarity is supposed to be slow compared to the sampling rate,
allowing us to consider the distributions inside each segment to be constant
over time, and resulting in a piece-wise stationary process. We can give an intuitive justification for why such a model is identifiable:   We impose the estimated components to be \textit{independent at every segment}, which means we get many more independence constraints in finding the independent components. Thus, it is intuitively plausible that we get a unique solution.

Formally, given a segment index $\tau \in \{1,\ldots,T\} $ where $T$ is
the number of segments, the distribution of each latent component $s_i$ within
that segment is modelled as an exponential family:
\begin{equation}
\label{eq:intro:tcl}
\log p_\tau(s_i) = \log q_{i,0}(s_i) + \sum_{j=1}^k\lambda_{i,j}(\tau) q_{i,j}(s_i) - \log Z_i(\lambda_{i,1}(\tau), \ldots, \lambda_{i,k}(\tau)),
\end{equation}
where $q_{i,0}$ is a stationary base density and $\mathbf{q}_i \defeq (q_{i,1},\ldots,q_{i,k})$
are the sufficient statistics
for the exponential family of the component $s_i$, and $Z_i$ is the normalization constant.
Importantly, the parameters $\lambdab_i\defeq(\lambda_{i,1},\ldots,\lambda_{i,k})$
depend on the segment index, indicating that the
distributions of the components change across segments. 
%

TCL recovers the inverse transformation $\g=\mathbf{f}^{-1}$ by \textit{self-supervised
learning}, where the pretext task is to classify original data points with segment
indices giving the labels, using multinomial logistic regression. To this end, TCL employs a deep neural network consisting of a feature extractor $\mathbf{h}(\xb; \thetab)$, with $\thetab$  parametrizing the
neural network, followed by a final classifying layer
(e.g. softmax).
Intuitively, this
is premised on the fact that in
order to optimally classify observations $\x(t)$ into their corresponding
segments $\tau$, the feature extractor $\mathbf{h}(\xb; \thetab)$
must learn about the temporal changes in the underlying distribution of
latent sources.

The theory of TCL~\citep{Hyva16NIPS} shows that the method can learn the independent
components up to
pointwise nonlinear transformations given by the $q$ above,
%
and a linear transformation $\mathbf{A}$. This is rather surprising since the self-supervised method does not make any reference to independent components.
A further linear ICA can recover the linear mixing $\mathbf{A}$ if
the number of segments grows to infinity and the segment distributions are random
in a certain sense. %
Thus, the theory proves that TCL (when supplemented by linear ICA) is consistent in the sense of estimation theory: when the number of data points grows infinite, the method finds the right components up to the point-wise nonlinearities. Such statistical theory assumes that the optimization does not fail by getting stuck in a local optimum, which is a however a typical practical problem in deep learning.
The self-supervised learning is illustrated in Fig.~\ref{tcl.fig}.

Furthermore, with this self-supervised approach it is possible to combine estimation of components with dimension reduction. Heuristically, one can simply have a smaller dimension $d'$ in the feature extractor than the dimension of the data. This can be given a rigorous probabilistic interpretation, if we assume some of the components are actually uninteresting, ``noise'' components, characterized by being stationary, unlike the actual components~\citep{Hyva16NIPS}. The feature extractor will then simply ignore those dimensions which are not non-stationary.

In the basic model, it is assumed that the segmentation is known, or manually imposed. This may be a restriction in practice, although the method seems to work even if the segmentation is not very well specified. However, it is also possible to model the segmentation by a Hidden Markov Model~\citep{Halva20UAI,Halva21}, and estimate both the segmentation and the demixing by maximum likelihood, which will be considered below.

\begin{figure}
\begin{center}
  \resizebox{7cm}{!}{\includegraphics{tcl_patterns}}
  \caption{Illustration of time-contrastive learning. The time points are segmented, as shown in different colours here. Each time point goes through a feature extractor $\hb$ which feeds the features to a multinomial regression layer. Together they learn to tell for each data point which segment it is from. Rather surprisingly, the feature extractor learns the independent componentns. } \label{tcl.fig}
\end{center}
\end{figure}

\subsection*{Permutation-Contrastive Learning}
Another approach to nonlinear ICA is to use the temporal dependencies of the independent components. Using the (linear) autocorrelations of stationary sources enables separation of the sources in the linear mixing case, although under some restrictive conditions~\citep{Tong91,Belo97}. A major advance in the field was to show how this framework can be extended to the nonlinear case~\citep{Harmeling03,Sprekeler14}. Related proposals have also been made under the heading ``slow feature analysis''~\citep{Wiskott02,Foldiak91}. Recent deep learning research~\citep{mobahi2009deep,springenberg2012learning,goroshin2015unsupervised} %
uses similar ideas, often called ``temporal coherence'', ``temporal stability'', or ``slowness'' of the features. Lack of rigorous theory has been a major impediment for development of such methods in the nonlinear case. 

The framework of Permutation-Contrastive Learning (PCL)~\citep{Hyva17AISTATS} enables a rigorous treatment of the identifability of such models in a nonlinear generative model setting. 
The basic idea is to assume that the independent components are again time series, but this time they are \textit{stationary} and have \textit{temporal dependencies}. As a simple example of great practical utility, each independent component might follow a possibly nonlinear autoregressive process with possibly non-Gaussian innovations, given in the basic case of one time lag by
\begin{equation} \label{AR}
  s_i(t)=r_i(s_i(t-1))+n_i(t)
\end{equation}
for some scalar autoregressive function $r_i$, and an innovation process $n_i(t)$. 

An intuitive justification for why such a model is identifiable is that the model imposes \textit{independence over all time lags}, i.e.\ between $s_i(t)$ and $s_j(t-\tau)$ for any $\tau$. This is another way of creating more constraints for finding the independent components~\citep{schell2023nonlinear}. Thus, it is intuitively plausible that we get rid of the non-uniqueness of the basic i.i.d.\ case. The proof assumes certain conditions which essentially mean that the components are sufficiently temporally dependent and non-Gaussian~\citep{Hyva17AISTATS,Halva21,schell2023nonlinear}. For example, in the autoregressive model in Eq.~(\ref{AR}), as soon as either $r_i$ is nonlinear, or $n_i$ is non-Gaussian, the model is identifiable~\citep{Hyva17AISTATS}. (If the autoregressive model is linear and Gaussian, separation is possible but only if the autocorrelations are different from one component to another~\citep{Tong91,Belo97,Sprekeler14}.)

%
PCL is also a self-supervised algorithm, which in the simplest case proceeds as follows. Collect data points in two subsequent time points to construct a sample of a new random vector $\y$:
\begin{equation} \label{y}
\y(t)=\begin{pmatrix} \x(t) \\ \x(t-1) \end{pmatrix}
\end{equation}
which gives a ``minimal description'' of the temporal dependencies in the data.  Here, the same $t$ is used as the sample index for $\y(t)$ as for $\x(t)$. 
As a contrast, create a \textit{permuted} data sample by randomly permuting (shuffling) the time indices:
\begin{equation} \label{ystar}
\y^*(t)=\begin{pmatrix} \x(t) \\ \x(t^*) \end{pmatrix}
\end{equation}
where $t^*$ is a randomly selected time point. In other words, we create data with the same marginal distribution (on the level of the vectors $\x$ instead of single variables), but which does not reflect the temporal structure of the data at all.
Next, learn to discriminate between real data $\y(t)$ and time-permuted data $\y^*(t)$. 
We use logistic regression with a regression function of a special form, where like in TCL, the neural network can be divided into a feature extractor $\mathbf{h}$ and the final logistic regression layer.
%
%
%
           %
The learning system is illustrated in Fig.~\ref{pcl.fig}.
We note that very similar ideas have been proposed heuristically in more applied contexts~\citep{misra2016shuffle,Banville21}.


Intuitively speaking, it is plausible that the feature extractor $\hb$ somehow recovers the temporal structure of the data since recovering such structure is necessary to discriminate real data from permuted data. In particular, since the most parsimonious description of the temporal structure can be found by separating the sources and then modelling the temporal structure of each source separately, it is plausible that the discrimination works best when the $h_i$ separate the sources. Like with TCL, the theory of PCL\citep{Hyva17AISTATS} rigorously proves that the algorithm is actually statistically consistent, and that dimension reduction is possible by just using a smaller number of hidden units.

In Fig.~\ref{separation.fig}, we show that PCL performs nonlinear ICA: The bottom row shows the results of PCL applied on the nonlinear mixtures of the four original signals. Clearly, PCL found very good approximations of the original signals. In fact, these four signals are characterized by temporal dependencies, and they are not Gaussian processes, so the conditions of PCL can be fullfilled. Likewise, Fig.~\ref{ident.fig} d) shows the results of applying PCL on the data in Fig.~\ref{ident.fig} a); the data was actually generated with temporally dependent component although that could not be seen in the figure. Again, PCL finds the original components up to small errors. 



\begin{figure}
\begin{center}
  \resizebox{7cm}{!}{\includegraphics{pcl_patterns}}
  \caption{Illustration of permutation-constrative learning. Short windows $\y$ are sampled from the data (one window is given in red). Likewise, windows are sampled from data where the time dependencies are destroyed by random time-permutation. The feature extractor, together with a binary logistic regression classifier, learn to tell which input $\y$ is real and which is permuted (randomized). In the process, the feature extractor learns the independent components. }\label{pcl.fig}
\end{center}
\end{figure}




\subsection*{Combining temporal dependencies and nonstationarity}

While TCL and PCL probe two different kinds of temporal structure, nonstationarity and temporal dependencies respectively, it would be of great interest to develop a model and an algorithm that combines the two.
It is in fact rather straightforward to combine these two properties in a single statistical model~\citep{Halva21}. In the simplest case, we simply augment the AR model in Eq.~(\ref{AR}) so that the variance of the \textit{innovation} is nonstationary:
\begin{equation} \label{ARnonstat}
  s_i(t)=r(s_i(t-1))+\sigma_i(t)n_i(t)
\end{equation}
for some non-stationary signal $\sigma_i(t)$ which follows another AR (or Markov) model, thus constituing a Hidden Markov Model (HMM)~\citep{Halva20UAI,Halva21}. This basic idea has been generalized to create a very general framework called Structured Nonlinear ICA, or SNICA~\citep{Halva21}. Maximum likelihood estimation can be used to learn such a model, as will be explained below..

A self-supervised method that incorporates the same idea is Independent Innovation Analysis~\citep{Morioka21AISTATS}. In fact, it goes a bit further and proposes a model in which it is not the components themselves but the innovations that are independent. The model starts by assuming a completely general autoregressive model as
\begin{equation}
  \x(t)=\f(\x(t-1),\s(t))
\end{equation}
where the $\s(t)$ now take the role of nonlinear innovations, and $\f$ combines the autoregressive function and the mixing function. Like in basic nonlinear ICA, we aim to estimate $\f$ and $\s$. The entries of $s_i(t)$ are assumed independent, and typically nonstationary, although different assumptions are possible here. A self-supervised method can then be develop by considering the augmented data vector $(\x(t),\x(t-1))$ and applying a variant of TCL on it.

Contrastive Predictive Coding (CPC)~\citep{oord2018representation} is a related self-supervised method. The system learns nonlinear features from time windows, and models the distributions of the latent components based on those features as well as a latent ``context'' variables  which summarize the history of the time series. The context model is not unlike the nonstationarity model with a HMM. However, we are not aware of work directly connecting CPC to a identifiable latent-variable model.
%



\subsection*{Nonlinear ICA using auxiliary variables}
%
%
Another generalization~\citep{Hyva19AISTATS} of nonlinear ICA
%
is to assume that each component $s_i$ is dependent on some \textit{observed}
\textit{auxiliary variable} $\ub$, but independent of all the other
components, conditionally on $\ub$:
%
\begin{align*}
%
    p(\s\vert\ub) &= \prod_i p_i(s_i\vert\ub).
\end{align*}
which is to be compared with the basic independence in Eq.~(\ref{factorize}).
This formulation is so general that it subsumes TCL  as a special case:
in the case of non-stationary sources, the auxiliary variable $\ub$
can be the segment label.
More generally, the auxiliary variable $\mathbf{u}$ can be a class label, the
index of a pixel in an image, some description of an image, the sound of a
video~\citep{arandjelovic2017look} amongst others. Related approaches assume we have multiple views of the same data~\citep{gresele2020incomplete} or the data is multimodal~\citep{Morioka23AISTATS}.
A clear connection to PCL can be made as well, by considering $\ub$ to contain the history of $\x(t)$, perhaps simply $\x(t-1)$. Thus, we see that nonlinear ICA is possible without any time structure, at the expense of having some additional observed data in the form of $\ub$.

Various estimation methods have been developed for this model.
One method, reminiscent of PCL, is to  learn the demixing function using a self-supervised binary discrimination task based on randomization:
new data is constructed from the observations $\xb$ and $\ub$ to obtain two datasets
\begin{align*}
	%
	\tilde{\xb} &= (\xb, \ub), \\
	%
	\tilde{\xb}^* &= (\xb, \ub^*),
\end{align*}
where $\ub^*$ is drawn randomly from the distribution of $\ub$ and independent of $\xb$~\citep{Hyva19AISTATS}.
Then, nonlinear logistic regression is performed using a regression function of a specific form
%
to discriminate between actual samples $\tilde{\xb}$ and shuffled samples $\tilde{\xb}^*$. 
The intuitive justification is that according to the generative model,
the observed and the auxiliary variables in the non-shuffled dataset $\tilde{\mathbf{x}}$
are linked through shared latent variables,
whereas this link is broken in the shuffled dataset $\tilde{\mathbf{x}}^*$.
Thus, the regression function makes use of
a feature extractor $\hb$
like in TCL and PCL, the purpose of which
is to extract the latent features
that allow distinguishing between the two datasets.
The theory~\citep{Hyva19AISTATS,Khemakhem20iVAE} shows that
the model is identifiable up to component-wise invertible transformations, and that the estimator given by this self-supervised method is consistent, 
provided that the latent distribution $p_i(s_i\vert\mathbf{u})$ satisfies some
regularity constraints. 



\subsection*{Maximum likelihood estimation}

Once a probabilistic model for nonlinear ICA has been defined, it should be possible to estimate it by maximization of likelihood. Maximum likelihood estimation is statistically optimal in the sense that it is asymptotically efficient (achieves the smallest statistical error for a finite data set) under mild conditions. Another the benefit of maximum likelihood methods is that they can be seamlessly integrated with further probabilistic inference. For example, the actual values of components can be inferred in case of observational noise\citep{Halva21}, or a segmentation of the time series can be inferred simultaneously with the estimation\citep{Halva20UAI}. (But it is also possible to first estimate the mixing function by, say, a self-supervised method, and do probabilistic inference of latent quantities afterwards.)

The problem with maximum likelihood estimation is that it can be computationally very demanding. This is in contrast to the self-supervised methods presented above which tend to be algorithmically simpler while statistically less optimal. It is an empirical question which class of methods is better for nonlinear ICA; a general answer can hardly be given since it depends on the data set being analyzed as well as the computational environment being used. 

Maximum likelihood methods have been developed for nonlinear ICA based on two different approaches. The first  possibility is to consider the (exact) likelihood of the models considered above. In the case of time-series, the likelihood of such models can be generically expressed in a simple formula:
\begin{equation}
  \log p(\x(1),\ldots,\x(t_\text{max});\g)=\sum_{i=1}^d \log p_i(g_i(\x(1)),\ldots,g_i(\x(t_\text{max})))+\sum_{t=1}^{t_\text{max}} \log|\det \J\g(\x(t))|
\end{equation}
where the important point is the apparition of the determinant of the Jacobian $\J\g$ of the demixing function. The first term on the right-hand-side presents no particular difficulties regarding its computation and optimization: it  is simply the likelihood given by the time series model (e.g.\ autoregressive) for each estimated component, for the whole time series with time index from $1$ to $t_\text{max}$. But the apparently simple determinant in the second term is computationally very difficult to optimize when $\g$ is a neural network. This problem has been extensively considered in the case of normalizing flows~\citep{kobyzev2020normalizing}, where the typical solution is to strongly constrain the function $\g$, for example so that it has a triangular Jacobian. However, in our case we don't want to constrain the function $\g$ in any way because we want to be able to estimate general nonlinear mixing functions. Thus, the solutions offered by the literature on normalizing flows is of little use here.

Fortunately, it is possible to use what is called the (Riemannian) relative gradient for computationally efficient optimization~\citep{Gresele20}. Thus, maximum likelihood estimation of the noise-free model becomes  possible in practice. This would seem to be the statistically ultimate method in the sense of being not only consistent, but even asymptotically efficient. The downside is that this method requires the number of independent components to be equal to the number of observed variables, not allowing for simultaneous dimension reduction, unlike almost all the other methods considered in this paper. In practice, the dimension would need to be first reduced by some other method similar to PCA, as is almost always done with linear ICA.

Another possibility is use variational approximations of the likelihood. This assumes we add a noise term to the mixing, as typical of VAEs in Eq.~(\ref{VAE}), since otherwise the posterior distributions are degenerate and variational methods do not work. Such variational methods are basically variants of the well-known VAE framework. Thus, this approach also shows how a VAE can be made identifiable: either by looking at time structure, which results in the SNICA~\citep{Halva21} and SlowVAE~\citep{klindt2020towards} models, or by conditioning by an auxiliary variable, which results in a very general model called iVAE\cite{Khemakhem20iVAE}. In both cases, VAE gives a variational method for estimation of the model.

Such variational methods have the advantage that they can reduce the dimension of the data at the same time as estimating components; above we argued that dimension reduction is actually the main utility of the plain VAE. This is an improvement on the noise-free maximum likelihood considered above, but the self-supervised methods are also able to reduce the dimension. On the negative side, variational methods are based on approximations and thus unlikely to be statistically consistent (i.e.\ to converge to the right solution in the limit of infinite data). Depending on the data, the bias introduced may be negligible with respect to the gain in statistical efficiency compared to self-supervised methods, or it may not be so.

Another related approach is \textit{energy-based} modelling~\citep{song2021train}. It can also be used for probabilistically principled estimation of the nonlinear ICA model~\citep{Khemakhem20NIPS}. Instead of the likelihood, some other objective is maximized (e.g.\ score matching distance), and the latent variables are not explicitly given by the model. The estimation is thus greatly simplified at the expense of losing some of the benefits of maximum likelihood estimation. This may offer an interesting compromise between statistical efficiency and computational efficiency. Finally, let us mention that the principle of adversarial learning as in GANs can also be used to estimate the nonlinear ICA model~\citep{LuopajarviThesis}: just like VAE, GAN can in fact be seen as a general principle for estimating a latent-variable model.


\section*{Discussion}

In this paper, we reviewed recent research on nonlinear ICA. It provides identifiable models, i.e.\ probabilistic models for which a unique solution can be shown to exist (up to trivial indeterminacies), and this solution finds the original components postulated in the model. While in many other fields of machine learning, identifiability is not a problem, it is a fundamental problem in the case of finding hidden factors, or disentanglement, as has been known since the 1950s at least. Identifiability can be attained for nonlinear ICA by postulating time series, or observing an additional auxiliary variable. After defining an identifiable model, different algorithms can be devised; we focused here on self-supervised algorithms and maximum likelihood estimation (including variational methods).

\subsection*{Applications}

Since nonlinear ICA is a very recent method, its utility for real data analysis is still largely to be explored. Analysis of EEG and MEG data has already shown that nonlinear ICA~\citep{Morioka21AISTATS,Zhu23} or very closely related self-supervised methods~\citep{Banville21} provide a representation which is very useful for classification. This work typically considers a \textit{semi-supervised} setting\citep{Khemakhem20NIPS,Zhu23}, which means that a representation is learned from large unlabelled data sets in a unsupervised manner, and the learned neural network is then applied on a new labelled data set to compute features which are useful for classification. The point is that it is often easy to find a big data set which is unlabelled, while the data sets where the classification is of practical  significance are often small. Especially when learning a representation with a deep neural network, it is crucial to be able to do it from a big data set. This is closely related to \textit{transfer learning}, where the learned features are used on a \textit{new} data set\citep{Khemakhem20NIPS}. In neuroscience, another utility of such features is that they can provide insight into the structure of the data~\citep{zhou2020learning,schneider2022learnable}, with the caveat that interpretation of neural networks is notoriously difficult.

Learning image features is an application that holds great promise~\citep{klindt2020towards}. This should be straightforward using video as input data, but requires huge computational resources. In fact, a large number of self-supervised learning methods have been proposed in the context of computer visions, and some of them are almost identical to nonlinear ICA~\citep{misra2016shuffle,arandjelovic2017look}. Thus, the theory of nonlinear ICA could also be seen as a post hoc theoretical justification of some of the existing self-supervised methods in computer vision. Such self-supervised methods are often completely heuristic, and building a proper theory to explain their behaviour is most interesting.  Audio data has great similarities to image and video data, so nonlinear ICA  is likely to work there as well, even if existing methods tend to use audio- or speech-specific methods~\citep{ravanelli2020multi}.

An interesting question is whether nonlinear ICA could provide a successful method for generating new data points. It should be particularly suitable for conditional generation, since many models assume some kind of conditioner (auxiliary variable). The promise  would be that the independent components correspond to some meaningful quantities, so that it would be useful to explicitly manipulate them in some use cases. To our knowledge, this has not been seriously investigated at the moment.

Another extremely interesting application can be found in causal discovery. Nonlinear ICA allows for the determination of the direction of effect, i.e.\ which variable causes which, even in completely nonlinear regression models~\citep{Monti19UAI}. Here, identifiability is absolutely essential since the very point of the analysis is to interpret the parameters estimated, as they express the direction of effect. For details, we refer to our review on identifiability theory~\citep{Hyva23AISM}.

It should be emphasized, however, that due to the great generality of the nonlinear ICA model, it can be applied in many different domains. The most successful applications may be something quite different from what was just discussed.


\subsection*{Extensions}

Here we focused on models that achieve identifiability by statistical assumptions: assuming temporal or other dependencies inside the components. An alternative is to consider restrictions on the nonlinearity~\citep{gresele2021independent,zimmermann2021contrastive,buchholz2022function}. For example, in earlier research some very strong restrictions have been proposed, in particular constraining the mixing to be linear followed by point-wise nonlinearities~\citep{taleb1999source}. However, in the case of general, non-parametric nonlinearities, it is not quite clear what kind of conditions would be flexible enough to be useful, while enable identifiability~\citep{Hyva23AISM}.

Models for dependencies \textit{between} the components have also been developed~\citep{Khemakhem20NIPS}. A special case, and a topic of great current interest, is how to combine estimation of components with analysis of their causal relations, leading to causal representation learning~\citep{lachapelle2022disentanglement,Morioka23AISTATS}. Indeed, finding models which allow for both a general nonlinear (observational) mixing and some kind of (causal) dependencies between the components are of great interest in future research. At first sight, there may seem to be some inherent contradiction between finding components which are dependent, since any such dependencies might be modelled by the mixing. However, a possible resolution to this contradiction might be to use more than one of the statistical principles given above. For example, the components might be identifiable based on their temporal dependencies, while the causal relations might be found, perhaps afterwards, by looking at their nonstationary dependencies~\citep{Monti19UAI,Zhang10UAI}, or even instantaneous dependencies~\citep{Khemakhem20NIPS}.


\subsection*{Goals of nonlinear ICA and unsupervised learning}

While we might casually just say that nonlinear ICA is a method for unsupervised learning, we think it is important to note that unsupervised learning can have different goals. While different lists of goals have been given~\citep{theis2015note}, we propose to consider the following four:
(Goal 1) Estimating an  accurate model of the data distribution. Energy-based modelling in terms of score matching or noise-contrastive estimation, as well as VAEs and normalizing flows are fundamentally designed for this purpose. (Goal 2) Sampling points from the data distribution. Generative adversarial networks (GANs) were conceived for this very purpose, although more recently, generative diffusion models may have been more successful. (Goal 3) Obtaining useful features or a representation for supervised learning. Here, we come to the question of representation learning which was one of the starting points of this paper. While it is often performed by VAEs and even GANs, the problems of identifiability reviewed here suggest that nonlinear ICA should be better. (Goal 4) Revealing underlying structure in data. In this case, the question of identifiability becomes paramount: The features learned cannot be meaningfully considered to reveal the underlying structure unless the model is identifiable. This is particularly important in scientific data analysis where the features are often assumed to correspond to some scientifically interesting quantities.

Importantly, these four goals are partly orthogonal, even contradictory. In particular, goals 1 and 2 are essentially non-parametric problems which require an arbitrarily good approximation of the probability distribution. Such an approximation can be very well done by a black box whose inner working are neither understood nor computationally accessible. In contrast, for goals 3 and 4, we need a system which is not a black box, and a model parametrized by a judiciously chosen, possibly low-dimensional parameter vector may be best.  Therefore, it seems unlikely that any single method could accomplish all of these goals. We propose that in unsupervised learning research, one should specify the more specific goal; unsupervised learning in itself is not a properly defined goal. As for nonlinear ICA, the primary goals are 3 and 4: learning a good representation for supervised learning and revealing the underlying strucure of multi-dimensional data.





\subsection*{Resource availability}



Software implementations are available for many methods described in this paper, \\see {\tt https://www.cs.helsinki.fi/u/ahyvarin/software.shtml}.


\section*{Acknowledgments}

A.H.\ was supported by CIFAR and the Academy of Finland.

\section*{Declaration of interests}

No conflicts of interests.


\begin{thebibliography}{102}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\href}[2]{#2}
\providecommand{\path}[1]{#1}
\providecommand{\DOIprefix}{doi:}
\providecommand{\ArXivprefix}{arXiv:}
\providecommand{\URLprefix}{URL: }
\providecommand{\Pubmedprefix}{pmid:}
\providecommand{\doi}[1]{\href{http://dx.doi.org/#1}{\path{#1}}}
\providecommand{\Pubmed}[1]{\href{pmid:#1}{\path{#1}}}
\providecommand{\BIBand}{and}
\providecommand{\bibinfo}[2]{#2}
\ifx\xfnm\undefined \def\xfnm[#1]{\unskip,\space#1}\fi
\makeatletter\def\@biblabel#1{#1.}\makeatother
%Type = Article
\bibitem[{LeCun et~al.(1998)LeCun, Bottou, Bengio and
  Haffner}]{lecun1998gradientbased}
\bibinfo{author}{LeCun, Y.}, \bibinfo{author}{Bottou, L.},
  \bibinfo{author}{Bengio, Y.}, and \bibinfo{author}{Haffner, P.}
  (\bibinfo{year}{1998}). \bibinfo{title}{Gradient-based learning applied to
  document recognition}.
\newblock \bibinfo{journal}{Proceedings of the IEEE}
  \emph{\bibinfo{volume}{86}}, \bibinfo{pages}{2278--2324}.
%Type = Article
\bibitem[{Krizhevsky et~al.(2012)Krizhevsky, Sutskever and
  Hinton}]{krizhevsky2012imageneta}
\bibinfo{author}{Krizhevsky, A.}, \bibinfo{author}{Sutskever, I.}, and
  \bibinfo{author}{Hinton, G.~E.} (\bibinfo{year}{2012}).
  \bibinfo{title}{{{ImageNet}} classification with deep convolutional neural
  networks}.
\newblock \bibinfo{journal}{Communications of the ACM}
  \emph{\bibinfo{volume}{60}}, \bibinfo{pages}{84--90}.
  \DOIprefix\doi{10.1145/3065386}.
%Type = Inproceedings
\bibitem[{Deng et~al.(2009)Deng, Dong, Socher, Li, Li and
  Fei-Fei}]{deng2009imagenet}
\bibinfo{author}{Deng, J.}, \bibinfo{author}{Dong, W.},
  \bibinfo{author}{Socher, R.}, \bibinfo{author}{Li, L.-J.},
  \bibinfo{author}{Li, K.}, and \bibinfo{author}{Fei-Fei, L.}
\newblock \bibinfo{title}{Imagenet: {{A}} large-scale hierarchical image
  database}.
\newblock In: \emph{\bibinfo{booktitle}{{{IEEE}} Conference on Computer Vision
  and Pattern Recognition}}. \bibinfo{publisher}{{IEEE}}
  (\bibinfo{year}{2009}):\unskip( \bibinfo{pages}{248--255}).
%Type = Article
\bibitem[{Chang et~al.(2015)Chang, Funkhouser, Guibas, Hanrahan, Huang, Li,
  Savarese, Savva, Song and Su}]{chang2015shapenet}
\bibinfo{author}{Chang, A.~X.}, \bibinfo{author}{Funkhouser, T.},
  \bibinfo{author}{Guibas, L.}, \bibinfo{author}{Hanrahan, P.},
  \bibinfo{author}{Huang, Q.}, \bibinfo{author}{Li, Z.},
  \bibinfo{author}{Savarese, S.}, \bibinfo{author}{Savva, M.},
  \bibinfo{author}{Song, S.}, and \bibinfo{author}{Su, H.}
  (\bibinfo{year}{2015}). \bibinfo{title}{Shapenet: {{An}} information-rich 3d
  model repository}.
\newblock \href{http://arxiv.org/abs/1512.03012}{\tt arXiv:1512.03012}.
%Type = Article
\bibitem[{Marcus et~al.(1993)Marcus, Santorini and
  Marcinkiewicz}]{marcus1993building}
\bibinfo{author}{Marcus, M.}, \bibinfo{author}{Santorini, B.}, and
  \bibinfo{author}{Marcinkiewicz, M.~A.} (\bibinfo{year}{1993}).
  \bibinfo{title}{Building a large annotated corpus of {{English}}: {{The Penn
  Treebank}}}.
\newblock \bibinfo{journal}{Computational Linguistics}
  \emph{\bibinfo{volume}{19}}, \bibinfo{pages}{313--330}.
%Type = Inproceedings
\bibitem[{Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng and
  Potts}]{maas2011learning}
\bibinfo{author}{Maas, A.}, \bibinfo{author}{Daly, R.~E.},
  \bibinfo{author}{Pham, P.~T.}, \bibinfo{author}{Huang, D.},
  \bibinfo{author}{Ng, A.~Y.}, and \bibinfo{author}{Potts, C.}
\newblock \bibinfo{title}{Learning word vectors for sentiment analysis}.
\newblock In: \emph{\bibinfo{booktitle}{Proceedings of the 49th Annual Meeting
  of the Association for Computational Linguistics: {{Human}} Language
  Technologies}} (\bibinfo{year}{2011}):\unskip( \bibinfo{pages}{142--150}).
%Type = Inproceedings
\bibitem[{Bertin-Mahieux et~al.(2011)Bertin-Mahieux, Ellis, Whitman and
  Lamere}]{bertin-mahieux2011million}
\bibinfo{author}{Bertin-Mahieux, T.}, \bibinfo{author}{Ellis, D.~P.},
  \bibinfo{author}{Whitman, B.}, and \bibinfo{author}{Lamere, P.}
\newblock \bibinfo{title}{The million song dataset}.
\newblock In: \emph{\bibinfo{booktitle}{Proceedings of the 12th {{International
  Society}} for {{Music Information Retrieval Conference}}}}
  (\bibinfo{year}{2011}):\unskip\DOIprefix\doi{10.7916/D8NZ8J07}.
%Type = Article
\bibitem[{Hu et~al.(2020)Hu, Fey, Zitnik, Dong, Ren, Liu, Catasta and
  Leskovec}]{hu2020open}
\bibinfo{author}{Hu, W.}, \bibinfo{author}{Fey, M.}, \bibinfo{author}{Zitnik,
  M.}, \bibinfo{author}{Dong, Y.}, \bibinfo{author}{Ren, H.},
  \bibinfo{author}{Liu, B.}, \bibinfo{author}{Catasta, M.}, and
  \bibinfo{author}{Leskovec, J.} (\bibinfo{year}{2020}). \bibinfo{title}{Open
  graph benchmark: {{Datasets}} for machine learning on graphs}.
\newblock \href{http://arxiv.org/abs/2005.00687}{\tt arXiv:2005.00687}.
%Type = Inproceedings
\bibitem[{Yanardag and Vishwanathan(2015)}]{yanardag2015deep}
\bibinfo{author}{Yanardag, P.}, and \bibinfo{author}{Vishwanathan, S. V.~N.}
\newblock \bibinfo{title}{Deep graph kernels}.
\newblock In: \emph{\bibinfo{booktitle}{Proceedings of the 21th {{ACM SIGKDD}}
  International Conference on Knowledge Discovery and Data Mining}}
  (\bibinfo{year}{2015}):\unskip( \bibinfo{pages}{1365--1374}).
%Type = Article
\bibitem[{Dahl et~al.(2011)Dahl, Yu, Deng and Acero}]{dahl2011contextdependent}
\bibinfo{author}{Dahl, G.~E.}, \bibinfo{author}{Yu, D.}, \bibinfo{author}{Deng,
  L.}, and \bibinfo{author}{Acero, A.} (\bibinfo{year}{2011}).
  \bibinfo{title}{Context-dependent pre-trained deep neural networks for
  large-vocabulary speech recognition}.
\newblock \bibinfo{journal}{IEEE Transactions on audio, speech, and language
  processing} \emph{\bibinfo{volume}{20}}, \bibinfo{pages}{30--42}.
%Type = Inproceedings
\bibitem[{Seide et~al.(2011)Seide, Li and Yu}]{seide2011conversational}
\bibinfo{author}{Seide, F.}, \bibinfo{author}{Li, G.}, and \bibinfo{author}{Yu,
  D.}
\newblock \bibinfo{title}{Conversational speech transcription using
  context-dependent deep neural networks}.
\newblock In: \emph{\bibinfo{booktitle}{Twelfth Annual Conference of the
  International Speech Communication Association}}
  (\bibinfo{year}{2011}):\unskip.
%Type = Article
\bibitem[{Bengio et~al.(2003)Bengio, Ducharme, Vincent and
  Janvin}]{bengio2003neural}
\bibinfo{author}{Bengio, Y.}, \bibinfo{author}{Ducharme, R.},
  \bibinfo{author}{Vincent, P.}, and \bibinfo{author}{Janvin, C.}
  (\bibinfo{year}{2003}). \bibinfo{title}{A neural probabilistic language
  model}.
\newblock \bibinfo{journal}{The journal of machine learning research}
  \emph{\bibinfo{volume}{3}}, \bibinfo{pages}{1137--1155}.
%Type = Inproceedings
\bibitem[{Devlin et~al.(9 06)Devlin, Chang, Lee and Toutanova}]{devlin2019bert}
\bibinfo{author}{Devlin, J.}, \bibinfo{author}{Chang, M.-W.},
  \bibinfo{author}{Lee, K.}, and \bibinfo{author}{Toutanova, K.}
\newblock \bibinfo{title}{{{BERT}}: {{Pre-training}} of {{Deep Bidirectional
  Transformers}} for {{Language Understanding}}}.
\newblock In: \emph{\bibinfo{booktitle}{Proceedings of the 2019 {{Conference}}
  of the {{North American Chapter}} of the {{Association}} for {{Computational
  Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and
  {{Short Papers}})}}. \bibinfo{publisher}{{Association for Computational
  Linguistics}} (\bibinfo{year}{2019-06}):\unskip(
  \bibinfo{pages}{4171--4186}).
\newblock \DOIprefix\doi{10.18653/v1/N19-1423}.
  \href{http://arxiv.org/abs/1810.04805}{\tt arXiv:1810.04805}.
%Type = Article
\bibitem[{Korbar et~al.(6 30)Korbar, Tran and
  Torresani}]{korbar2018cooperative}
\bibinfo{author}{Korbar, B.}, \bibinfo{author}{Tran, D.}, and
  \bibinfo{author}{Torresani, L.} (\bibinfo{year}{2018-06-30}).
  \bibinfo{title}{Cooperative {{Learning}} of {{Audio}} and {{Video Models}}
  from {{Self-Supervised Synchronization}}}.
\newblock \href{http://arxiv.org/abs/1807.00230}{\tt arXiv:1807.00230}.
%Type = Article
\bibitem[{Wang and Deng(2018)}]{wang2018deep}
\bibinfo{author}{Wang, M.}, and \bibinfo{author}{Deng, W.}
  (\bibinfo{year}{2018}). \bibinfo{title}{Deep visual domain adaptation: {{A}}
  survey}.
\newblock \bibinfo{journal}{Neurocomputing} \emph{\bibinfo{volume}{312}},
  \bibinfo{pages}{135--153}.
%Type = Article
\bibitem[{Bengio et~al.(2013)Bengio, Courville and
  Vincent}]{bengio2013representation}
\bibinfo{author}{Bengio, Y.}, \bibinfo{author}{Courville, A.}, and
  \bibinfo{author}{Vincent, P.} (\bibinfo{year}{2013}).
  \bibinfo{title}{Representation learning: {{A}} review and new perspectives}.
\newblock \bibinfo{journal}{IEEE transactions on pattern analysis and machine
  intelligence} \emph{\bibinfo{volume}{35}}, \bibinfo{pages}{1798--1828}.
  \href{http://arxiv.org/abs/1206.5538}{\tt arXiv:1206.5538}.
%Type = Inproceedings
\bibitem[{Kingma and Welling(2014)}]{kingma2014autoencoding}
\bibinfo{author}{Kingma, D.~P.}, and \bibinfo{author}{Welling, M.}
\newblock \bibinfo{title}{Auto-{{Encoding Variational Bayes}}}.
\newblock In: \emph{\bibinfo{booktitle}{Proceedings of the 2nd {{International
  Conference}} on {{Learning Representations}} ({{ICLR}})}}
  (\bibinfo{year}{2014}):\unskip\href{http://arxiv.org/abs/1312.6114}{\tt
  arXiv:1312.6114}.
%Type = Article
\bibitem[{Rezende et~al.(1 16)Rezende, Mohamed and
  Wierstra}]{rezende2014stochastic}
\bibinfo{author}{Rezende, D.~J.}, \bibinfo{author}{Mohamed, S.}, and
  \bibinfo{author}{Wierstra, D.} (\bibinfo{year}{2014-01-16}).
  \bibinfo{title}{Stochastic {{Backpropagation}} and {{Approximate Inference}}
  in {{Deep Generative Models}}}.
\newblock \href{http://arxiv.org/abs/1401.4082}{\tt arXiv:1401.4082}.
%Type = Article
\bibitem[{Kobyzev et~al.(2020)Kobyzev, Prince and
  Brubaker}]{kobyzev2020normalizing}
\bibinfo{author}{Kobyzev, I.}, \bibinfo{author}{Prince, S. J.~D.}, and
  \bibinfo{author}{Brubaker, M.~A.} (\bibinfo{year}{2020}).
  \bibinfo{title}{Normalizing {{Flows}}: {{An Introduction}} and {{Review}} of
  {{Current Methods}}}.
\newblock \bibinfo{journal}{IEEE Transactions on Pattern Analysis and Machine
  Intelligence} ( \bibinfo{pages}{1--1}).
  \DOIprefix\doi{10.1109/TPAMI.2020.2992934}.
  \href{http://arxiv.org/abs/1908.09257}{\tt arXiv:1908.09257}.
%Type = Inproceedings
\bibitem[{Higgins et~al.(2017)Higgins, Matthey, Pal, Burgess, Glorot,
  Botvinick, Mohamed and Lerchner}]{higgins2017betavae}
\bibinfo{author}{Higgins, I.}, \bibinfo{author}{Matthey, L.},
  \bibinfo{author}{Pal, A.}, \bibinfo{author}{Burgess, C.},
  \bibinfo{author}{Glorot, X.}, \bibinfo{author}{Botvinick, M.},
  \bibinfo{author}{Mohamed, S.}, and \bibinfo{author}{Lerchner, A.}
\newblock \bibinfo{title}{Beta-{{VAE}}: {{Learning Basic Visual Concepts}} with
  a {{Constrained Variational Framework}}}.
\newblock In: \emph{\bibinfo{booktitle}{Proc.\ Int.\ Conf.\ on Learning
  Representations (ICLR)}} (\bibinfo{year}{2017}):\unskip.
%Type = Article
\bibitem[{Alemi et~al.(2017)Alemi, Poole, Fischer, Dillon, Saurous and
  Murphy}]{alemi2017fixing}
\bibinfo{author}{Alemi, A.~A.}, \bibinfo{author}{Poole, B.},
  \bibinfo{author}{Fischer, I.}, \bibinfo{author}{Dillon, J.~V.},
  \bibinfo{author}{Saurous, R.~A.}, and \bibinfo{author}{Murphy, K.}
  (\bibinfo{year}{2017}). \bibinfo{title}{Fixing a {{Broken ELBO}}}.
\newblock \href{http://arxiv.org/abs/1711.00464}{\tt arXiv:1711.00464}.
%Type = Article
\bibitem[{Burgess et~al.(2018)Burgess, Higgins, Pal, Matthey, Watters,
  Desjardins and Lerchner}]{burgess2018understanding}
\bibinfo{author}{Burgess, C.~P.}, \bibinfo{author}{Higgins, I.},
  \bibinfo{author}{Pal, A.}, \bibinfo{author}{Matthey, L.},
  \bibinfo{author}{Watters, N.}, \bibinfo{author}{Desjardins, G.}, and
  \bibinfo{author}{Lerchner, A.} (\bibinfo{year}{2018}).
  \bibinfo{title}{Understanding disentangling in {$\beta$}-{{VAE}}}.
\newblock \href{http://arxiv.org/abs/1804.03599}{\tt arXiv:1804.03599}.
%Type = Inproceedings
\bibitem[{Chen et~al.(2018)Chen, Li, Grosse and Duvenaud}]{chen2018isolating}
\bibinfo{author}{Chen, R. T.~Q.}, \bibinfo{author}{Li, X.},
  \bibinfo{author}{Grosse, R.~B.}, and \bibinfo{author}{Duvenaud, D.~K.}
\newblock \bibinfo{title}{Isolating {{Sources}} of {{Disentanglement}} in
  {{Variational Autoencoders}}}.
\newblock In: \emph{\bibinfo{booktitle}{Advances in {{Neural Information
  Processing Systems}}}} vol.~\bibinfo{volume}{31}. \bibinfo{publisher}{{Curran
  Associates, Inc.}}
  (\bibinfo{year}{2018}):\unskip\href{http://arxiv.org/abs/1802.04942}{\tt
  arXiv:1802.04942}.
%Type = Inproceedings
\bibitem[{Esmaeili et~al.(2019)Esmaeili, Wu, Jain, Bozkurt, Siddharth, Paige,
  Brooks, Dy and Meent}]{esmaeili2019structured}
\bibinfo{author}{Esmaeili, B.}, \bibinfo{author}{Wu, H.},
  \bibinfo{author}{Jain, S.}, \bibinfo{author}{Bozkurt, A.},
  \bibinfo{author}{Siddharth, N.}, \bibinfo{author}{Paige, B.},
  \bibinfo{author}{Brooks, D.~H.}, \bibinfo{author}{Dy, J.}, and
  \bibinfo{author}{Meent, J.-W.}
\newblock \bibinfo{title}{Structured {{Disentangled Representations}}}.
\newblock In: \emph{\bibinfo{booktitle}{The 22nd {{International Conference}}
  on {{Artificial Intelligence}} and {{Statistics}}}}.
  \bibinfo{publisher}{{PMLR}} (\bibinfo{year}{2019}):\unskip(
  \bibinfo{pages}{2525--2534}).
\newblock \href{http://arxiv.org/abs/1804.02086}{\tt arXiv:1804.02086}.
%Type = Article
\bibitem[{Mathieu et~al.(2 06)Mathieu, Rainforth, Siddharth and
  Teh}]{mathieu2018disentangling}
\bibinfo{author}{Mathieu, E.}, \bibinfo{author}{Rainforth, T.},
  \bibinfo{author}{Siddharth, N.}, and \bibinfo{author}{Teh, Y.~W.}
  (\bibinfo{year}{2018-12-06}). \bibinfo{title}{Disentangling
  {{Disentanglement}} in {{Variational Autoencoders}}}.
\newblock \href{http://arxiv.org/abs/1812.02833}{\tt arXiv:1812.02833}.
%Type = Inproceedings
\bibitem[{Kim and Mnih(2018)}]{kim2018disentangling}
\bibinfo{author}{Kim, H.}, and \bibinfo{author}{Mnih, A.}
\newblock \bibinfo{title}{Disentangling by factorising}.
\newblock In: \emph{\bibinfo{booktitle}{International {{Conference}} on
  {{Machine Learning}}}}. \bibinfo{publisher}{{PMLR}}
  (\bibinfo{year}{2018}):\unskip( \bibinfo{pages}{2649--2658}).
\newblock \href{http://arxiv.org/abs/1802.05983}{\tt arXiv:1802.05983}.
%Type = Article
\bibitem[{Zhao et~al.(2017)Zhao, Song and Ermon}]{zhao2017infovae}
\bibinfo{author}{Zhao, S.}, \bibinfo{author}{Song, J.}, and
  \bibinfo{author}{Ermon, S.} (\bibinfo{year}{2017}).
  \bibinfo{title}{{{InfoVAE}}: {{Information}} maximizing variational
  autoencoders}.
\newblock \href{http://arxiv.org/abs/1706.02262}{\tt arXiv:1706.02262}.
%Type = Inproceedings
\bibitem[{Gao et~al.(2019)Gao, Brekelmans, Ver~Steeg and
  Galstyan}]{gao2019autoencoding}
\bibinfo{author}{Gao, S.}, \bibinfo{author}{Brekelmans, R.},
  \bibinfo{author}{Ver~Steeg, G.}, and \bibinfo{author}{Galstyan, A.}
\newblock \bibinfo{title}{Auto-encoding total correlation explanation}.
\newblock In: \emph{\bibinfo{booktitle}{The 22nd {{International Conference}}
  on {{Artificial Intelligence}} and {{Statistics}}}}.
  \bibinfo{publisher}{{PMLR}} (\bibinfo{year}{2019}):\unskip(
  \bibinfo{pages}{1157--1166}).
%Type = Article
\bibitem[{Achille and Soatto(2018)}]{achille2018information}
\bibinfo{author}{Achille, A.}, and \bibinfo{author}{Soatto, S.}
  (\bibinfo{year}{2018}). \bibinfo{title}{Information dropout: {{Learning}}
  optimal representations through noisy computation}.
\newblock \bibinfo{journal}{IEEE transactions on pattern analysis and machine
  intelligence} \emph{\bibinfo{volume}{40}}, \bibinfo{pages}{2897--2905}.
%Type = Article
\bibitem[{Kumar et~al.(2017)Kumar, Sattigeri and
  Balakrishnan}]{kumar2017variational}
\bibinfo{author}{Kumar, A.}, \bibinfo{author}{Sattigeri, P.}, and
  \bibinfo{author}{Balakrishnan, A.} (\bibinfo{year}{2017}).
  \bibinfo{title}{Variational inference of disentangled latent concepts from
  unlabeled observations}.
\newblock \href{http://arxiv.org/abs/1711.00848}{\tt arXiv:1711.00848}.
%Type = Book
\bibitem[{Peters et~al.(2017)Peters, Janzing and
  Sch{\"o}lkopf}]{peters2017elements}
\bibinfo{author}{Peters, J.}, \bibinfo{author}{Janzing, D.}, and
  \bibinfo{author}{Sch{\"o}lkopf, B.}
\newblock \bibinfo{title}{Elements of causal inference: foundations and
  learning algorithms}.
\newblock \bibinfo{publisher}{MIT press} (\bibinfo{year}{2017}).
%Type = Article
\bibitem[{Schmidhuber et~al.(1996)Schmidhuber, Eldracher and
  Foltin}]{schmidhuber1996semilinear}
\bibinfo{author}{Schmidhuber, J.}, \bibinfo{author}{Eldracher, M.}, and
  \bibinfo{author}{Foltin, B.} (\bibinfo{year}{1996}).
  \bibinfo{title}{Semilinear {{Predictability Minimization Produces Well-Known
  Feature Detectors}}}.
\newblock \bibinfo{journal}{Neural Computation} \emph{\bibinfo{volume}{8}},
  \bibinfo{pages}{773--786}. \DOIprefix\doi{10.1162/neco.1996.8.4.773}.
%Type = Inproceedings
\bibitem[{Locatello et~al.(2019)Locatello, Bauer, Lucic, Raetsch, Gelly,
  Sch\"olkopf and Bachem}]{locatello2019challenging}
\bibinfo{author}{Locatello, F.}, \bibinfo{author}{Bauer, S.},
  \bibinfo{author}{Lucic, M.}, \bibinfo{author}{Raetsch, G.},
  \bibinfo{author}{Gelly, S.}, \bibinfo{author}{Sch\"olkopf, B.}, and
  \bibinfo{author}{Bachem, O.}
\newblock \bibinfo{title}{Challenging common assumptions in the unsupervised
  learning of disentangled representations}.
\newblock In: \emph{\bibinfo{booktitle}{International Conference on Machine
  Learning}}. \bibinfo{publisher}{{PMLR}} (\bibinfo{year}{2019}):\unskip(
  \bibinfo{pages}{4114--4124}).
\newblock \href{http://arxiv.org/abs/1811.12359}{\tt arXiv:1811.12359}.
%Type = Article
\bibitem[{Hyv\"arinen and Pajunen(1999)}]{Hyva99NN}
\bibinfo{author}{Hyv\"arinen, A.}, and \bibinfo{author}{Pajunen, P.}
  (\bibinfo{year}{1999}). \bibinfo{title}{Nonlinear independent component
  analysis: Existence and uniqueness results}.
\newblock \bibinfo{journal}{Neural Networks} \emph{\bibinfo{volume}{12}},
  \bibinfo{pages}{429--439}.
%Type = Article
\bibitem[{Hyv\"arinen and Oja(2000)}]{Hyva00NN}
\bibinfo{author}{Hyv\"arinen, A.}, and \bibinfo{author}{Oja, E.}
  (\bibinfo{year}{2000}). \bibinfo{title}{Independent component analysis:
  Algorithms and applications}.
\newblock \bibinfo{journal}{Neural Networks} \emph{\bibinfo{volume}{13}},
  \bibinfo{pages}{411--430}.
%Type = Book
\bibitem[{Hyv\"arinen et~al.(2001)Hyv\"arinen, Karhunen and Oja}]{Hyvabook}
\bibinfo{author}{Hyv\"arinen, A.}, \bibinfo{author}{Karhunen, J.}, and
  \bibinfo{author}{Oja, E.}
\newblock \bibinfo{title}{Independent Component Analysis}.
\newblock \bibinfo{publisher}{Wiley Interscience} (\bibinfo{year}{2001}).
%Type = Article
\bibitem[{Gutmann and Hyv\"arinen(2012)}]{Gutmann12JMLR}
\bibinfo{author}{Gutmann, M.~U.}, and \bibinfo{author}{Hyv\"arinen, A.}
  (\bibinfo{year}{2012}). \bibinfo{title}{Noise-contrastive estimation of
  unnormalized statistical models, with applications to natural image
  statistics}.
\newblock \bibinfo{journal}{J.\ of Machine Learning Research}
  \emph{\bibinfo{volume}{13}}, \bibinfo{pages}{307--361}.
%Type = Inproceedings
\bibitem[{Misra et~al.(2016)Misra, Zitnick and Hebert}]{misra2016shuffle}
\bibinfo{author}{Misra, I.}, \bibinfo{author}{Zitnick, C.~L.}, and
  \bibinfo{author}{Hebert, M.}
\newblock \bibinfo{title}{Shuffle and learn: unsupervised learning using
  temporal order verification}.
\newblock In: \emph{\bibinfo{booktitle}{European Conference on Computer
  Vision}}. \bibinfo{organization}{Springer} (\bibinfo{year}{2016}):\unskip(
  \bibinfo{pages}{527--544}).
%Type = Inproceedings
\bibitem[{Noroozi and Favaro(2016)}]{noroozi2016unsupervised}
\bibinfo{author}{Noroozi, M.}, and \bibinfo{author}{Favaro, P.}
\newblock \bibinfo{title}{Unsupervised learning of visual representations by
  solving jigsaw puzzles}.
\newblock In: \emph{\bibinfo{booktitle}{Proc.\ European Conference on Computer
  Vision}} (\bibinfo{year}{2016}):\unskip.
%Type = Inproceedings
\bibitem[{Larsson et~al.(2017)Larsson, Maire and
  Shakhnarovich}]{larsson2017colorization}
\bibinfo{author}{Larsson, G.}, \bibinfo{author}{Maire, M.}, and
  \bibinfo{author}{Shakhnarovich, G.}
\newblock \bibinfo{title}{Colorization as a proxy task for visual
  understanding}.
\newblock In: \emph{\bibinfo{booktitle}{CVPR}} vol.~\bibinfo{volume}{2}
  (\bibinfo{year}{2017}):\unskip(~\bibinfo{pages}{8}).
%Type = Article
\bibitem[{DAmour et~al.(2022)DAmour, Heller, Moldovan, Adlam, Alipanahi,
  Beutel, Chen, Deaton, Eisenstein, Hoffman et~al.}]{d2020underspecification}
\bibinfo{author}{DAmour, A.}, \bibinfo{author}{Heller, K.},
  \bibinfo{author}{Moldovan, D.}, \bibinfo{author}{Adlam, B.},
  \bibinfo{author}{Alipanahi, B.}, \bibinfo{author}{Beutel, A.},
  \bibinfo{author}{Chen, C.}, \bibinfo{author}{Deaton, J.},
  \bibinfo{author}{Eisenstein, J.}, \bibinfo{author}{Hoffman, M.~D.} et~al.
  (\bibinfo{year}{2022}). \bibinfo{title}{Underspecification presents
  challenges for credibility in modern machine learning}.
\newblock \bibinfo{journal}{Journal of Machine Learning Research}
  \emph{\bibinfo{volume}{23}}, \bibinfo{pages}{1--61}.
%Type = Article
\bibitem[{Hyv\"arinen et~al.(2023)Hyv\"arinen, Khemakhem and
  Monti}]{Hyva23AISM}
\bibinfo{author}{Hyv\"arinen, A.}, \bibinfo{author}{Khemakhem, I.}, and
  \bibinfo{author}{Monti, R.~P.} (\bibinfo{year}{2023}).
  \bibinfo{title}{Identifiability of latent-variable and structural-equation
  models: from linear to nonlinear}.
\newblock \bibinfo{journal}{arXiv preprint arXiv:2302.02672}.
%Type = Article
\bibitem[{Comon(1994)}]{comon1994independent}
\bibinfo{author}{Comon, P.} (\bibinfo{year}{1994}). \bibinfo{title}{Independent
  component analysis, a new concept?}
\newblock \bibinfo{journal}{Signal processing} \emph{\bibinfo{volume}{36}},
  \bibinfo{pages}{287--314}.
%Type = Inproceedings
\bibitem[{Cardoso(2001{\natexlab{a}})}]{cardoso2001three}
\bibinfo{author}{Cardoso, J.-F.}
\newblock \bibinfo{title}{The three easy routes to independent component
  analysis; contrasts and geometry}.
\newblock In: \emph{\bibinfo{booktitle}{Proc. {{ICA}}}} vol.
  \bibinfo{volume}{2001}. \bibinfo{publisher}{{Citeseer}}
  (\bibinfo{year}{2001}{\natexlab{a}}):\unskip( \bibinfo{pages}{1--6}).
%Type = Article
\bibitem[{McKeown et~al.(1998)McKeown, Makeig, Brown, Jung, Kindermann, Bell
  and Sejnowski}]{mckeown1998analysis}
\bibinfo{author}{McKeown, M.~J.}, \bibinfo{author}{Makeig, S.},
  \bibinfo{author}{Brown, G.~G.}, \bibinfo{author}{Jung, T.-P.},
  \bibinfo{author}{Kindermann, S.~S.}, \bibinfo{author}{Bell, A.~J.}, and
  \bibinfo{author}{Sejnowski, T.~J.} (\bibinfo{year}{1998}).
  \bibinfo{title}{Analysis of {{fMRI}} data by blind separation into
  independent spatial components}.
\newblock \bibinfo{journal}{Human brain mapping} \emph{\bibinfo{volume}{6}},
  \bibinfo{pages}{160--188}.
%Type = Inproceedings
\bibitem[{Calhoun et~al.(2003)Calhoun, Adali, Hansen, Larsen and
  Pekar}]{calhoun2003ica}
\bibinfo{author}{Calhoun, V.~D.}, \bibinfo{author}{Adali, T.},
  \bibinfo{author}{Hansen, L.~K.}, \bibinfo{author}{Larsen, J.}, and
  \bibinfo{author}{Pekar, J.~J.}
\newblock \bibinfo{title}{{{ICA}} of functional {{MRI}} data: An overview}.
\newblock In: \emph{\bibinfo{booktitle}{In {{Proceedings}} of the
  {{International Workshop}} on {{Independent Component Analysis}} and {{Blind
  Signal Separation}}}}. \bibinfo{publisher}{{Citeseer}}
  (\bibinfo{year}{2003}):\unskip.
%Type = Article
\bibitem[{Beckmann et~al.(2005)Beckmann, DeLuca, Devlin and Smith}]{Beckmann05}
\bibinfo{author}{Beckmann, C.~F.}, \bibinfo{author}{DeLuca, M.},
  \bibinfo{author}{Devlin, J.~T.}, and \bibinfo{author}{Smith, S.~M.}
  (\bibinfo{year}{2005}). \bibinfo{title}{Investigations into resting-state
  connectivity using independent component analysis}.
\newblock \bibinfo{journal}{Philos.\ Trans.\ R.\ Soc.\ Lond.\ B.\ Biol.\ Sci.}
  \emph{\bibinfo{volume}{360}}, \bibinfo{pages}{1001--13}.
%Type = Article
\bibitem[{Delorme et~al.(2007)Delorme, Sejnowski and
  Makeig}]{delorme2007enhanced}
\bibinfo{author}{Delorme, A.}, \bibinfo{author}{Sejnowski, T.}, and
  \bibinfo{author}{Makeig, S.} (\bibinfo{year}{2007}). \bibinfo{title}{Enhanced
  detection of artifacts in {{EEG}} data using higher-order statistics and
  independent component analysis}.
\newblock \bibinfo{journal}{Neuroimage} \emph{\bibinfo{volume}{34}},
  \bibinfo{pages}{1443--1449}.
%Type = Article
\bibitem[{Milne et~al.(2009)Milne, Scope, Pascalis, Buckley and
  Makeig}]{milne2009independent}
\bibinfo{author}{Milne, E.}, \bibinfo{author}{Scope, A.},
  \bibinfo{author}{Pascalis, O.}, \bibinfo{author}{Buckley, D.}, and
  \bibinfo{author}{Makeig, S.} (\bibinfo{year}{2009}).
  \bibinfo{title}{Independent component analysis reveals atypical
  electroencephalographic activity during visual perception in individuals with
  autism}.
\newblock \bibinfo{journal}{Biological psychiatry} \emph{\bibinfo{volume}{65}},
  \bibinfo{pages}{22--30}.
%Type = Article
\bibitem[{Brookes et~al.(2011)Brookes, Woolrich, Luckhoo, Price, Hale,
  Stephenson, Barnes, Smith and Morris}]{Brookes11}
\bibinfo{author}{Brookes, M.}, \bibinfo{author}{Woolrich, M.},
  \bibinfo{author}{Luckhoo, H.}, \bibinfo{author}{Price, D.},
  \bibinfo{author}{Hale, J.}, \bibinfo{author}{Stephenson, M.},
  \bibinfo{author}{Barnes, G.}, \bibinfo{author}{Smith, S.}, and
  \bibinfo{author}{Morris, P.} (\bibinfo{year}{2011}).
  \bibinfo{title}{Investigating the electrophysiological basis of resting state
  networks using magnetoencephalography}.
\newblock \bibinfo{journal}{Proc.\ National Academy of Sciences (USA)}
  \emph{\bibinfo{volume}{108}}, \bibinfo{pages}{16783--16788}.
%Type = Article
\bibitem[{Hyv\"arinen et~al.(2010)Hyv\"arinen, Ramkumar, Parkkonen and
  Hari}]{Hyva10NI}
\bibinfo{author}{Hyv\"arinen, A.}, \bibinfo{author}{Ramkumar, P.},
  \bibinfo{author}{Parkkonen, L.}, and \bibinfo{author}{Hari, R.}
  (\bibinfo{year}{2010}). \bibinfo{title}{Independent component analysis of
  short-time {F}ourier transforms for spontaneous {EEG/MEG} analysis}.
\newblock \bibinfo{journal}{NeuroImage} \emph{\bibinfo{volume}{49}},
  \bibinfo{pages}{257--271}.
%Type = Inproceedings
\bibitem[{Bingham et~al.(2002)Bingham, Kuusisto and Lagus}]{bingham2002ica}
\bibinfo{author}{Bingham, E.}, \bibinfo{author}{Kuusisto, J.}, and
  \bibinfo{author}{Lagus, K.}
\newblock \bibinfo{title}{{{ICA}} and {{SOM}} in text document analysis}.
\newblock In: \emph{\bibinfo{booktitle}{Proceedings of the 25th Annual
  International {{ACM SIGIR}} Conference on {{Research}} and Development in
  Information Retrieval}} (\bibinfo{year}{2002}):\unskip(
  \bibinfo{pages}{361--362}).
%Type = Inproceedings
\bibitem[{Podosinnikova et~al.(2015)Podosinnikova, Bach and
  Lacoste-Julien}]{podosinnikova2015rethinking}
\bibinfo{author}{Podosinnikova, A.}, \bibinfo{author}{Bach, F.}, and
  \bibinfo{author}{Lacoste-Julien, S.}
\newblock \bibinfo{title}{Rethinking {{LDA}}: {{Moment Matching}} for
  {{Discrete ICA}}}.
\newblock In: \bibinfo{editor}{Cortes, C.}, \bibinfo{editor}{Lawrence, N.~D.},
  \bibinfo{editor}{Lee, D.~D.}, \bibinfo{editor}{Sugiyama, M.}, and
  \bibinfo{editor}{Garnett, R.}, eds. \emph{\bibinfo{booktitle}{Advances in
  {{Neural Information Processing Systems}} 28}}. \bibinfo{publisher}{{Curran
  Associates, Inc.}} (\bibinfo{year}{2015}):\unskip(
  \bibinfo{pages}{514--522}).
%Type = Article
\bibitem[{Back and Weigend(1997)}]{back1997first}
\bibinfo{author}{Back, A.~D.}, and \bibinfo{author}{Weigend, A.~S.}
  (\bibinfo{year}{1997}). \bibinfo{title}{A first application of independent
  component analysis to extracting structure from stock returns}.
\newblock \bibinfo{journal}{International journal of neural systems}
  \emph{\bibinfo{volume}{8}}, \bibinfo{pages}{473--484}.
%Type = Inproceedings
\bibitem[{Oja et~al.(0 10)Oja, Kiviluoto and Malaroiu}]{oja2000independent}
\bibinfo{author}{Oja, E.}, \bibinfo{author}{Kiviluoto, K.}, and
  \bibinfo{author}{Malaroiu, S.}
\newblock \bibinfo{title}{Independent component analysis for financial time
  series}.
\newblock In: \emph{\bibinfo{booktitle}{Proceedings of the {{IEEE}} 2000
  {{Adaptive Systems}} for {{Signal Processing}}, {{Communications}}, and
  {{Control Symposium}} ({{Cat}}. {{No}}.{{00EX373}})}}
  (\bibinfo{year}{2000-10}):\unskip( \bibinfo{pages}{111--116}).
\newblock \DOIprefix\doi{10.1109/ASSPCC.2000.882456}.
%Type = Article
\bibitem[{Nuzillard and Bijaoui(2000)}]{nuzillard2000blind}
\bibinfo{author}{Nuzillard, D.}, and \bibinfo{author}{Bijaoui, A.}
  (\bibinfo{year}{2000}). \bibinfo{title}{Blind source separation and analysis
  of multispectral astronomical images}.
\newblock \bibinfo{journal}{Astronomy and Astrophysics Supplement Series}
  \emph{\bibinfo{volume}{147}}, \bibinfo{pages}{129--138}.
%Type = Book
\bibitem[{Hyv\"arinen et~al.(2009)Hyv\"arinen, Hurri and Hoyer}]{Hyvanisbook}
\bibinfo{author}{Hyv\"arinen, A.}, \bibinfo{author}{Hurri, J.}, and
  \bibinfo{author}{Hoyer, P.~O.}
\newblock \bibinfo{title}{Natural Image Statistics}.
\newblock \bibinfo{publisher}{Springer-Verlag} (\bibinfo{year}{2009}).
%Type = Article
\bibitem[{Jutten and H\'erault(1991)}]{Jutten91}
\bibinfo{author}{Jutten, C.}, and \bibinfo{author}{H\'erault, J.}
  (\bibinfo{year}{1991}). \bibinfo{title}{Blind separation of sources, part
  {I}: An adaptive algorithm based on neuromimetic architecture}.
\newblock \bibinfo{journal}{Signal Processing} \emph{\bibinfo{volume}{24}},
  \bibinfo{pages}{1--10}.
%Type = Article
\bibitem[{Darmois(1953)}]{darmois1953analyse}
\bibinfo{author}{Darmois, G.} (\bibinfo{year}{1953}). \bibinfo{title}{Analyse
  g\'en\'erale des liaisons stochastiques: Etude particuli\`ere de l'analyse
  factorielle lin\'eaire}.
\newblock \bibinfo{journal}{Revue de l'Institut International de Statistique} (
  \bibinfo{pages}{2--8}).
%Type = Article
\bibitem[{Hecht-Nielsen(1995)}]{HechtNielsen95}
\bibinfo{author}{Hecht-Nielsen, R.} (\bibinfo{year}{1995}).
  \bibinfo{title}{Replicator neural networks for universal optimal source
  coding}.
\newblock \bibinfo{journal}{Science} \emph{\bibinfo{volume}{269}},
  \bibinfo{pages}{1860--1863}.
%Type = Article
\bibitem[{Harmeling et~al.(2003)Harmeling, Ziehe, Kawanabe and
  M{\"u}ller}]{Harmeling03}
\bibinfo{author}{Harmeling, S.}, \bibinfo{author}{Ziehe, A.},
  \bibinfo{author}{Kawanabe, M.}, and \bibinfo{author}{M{\"u}ller, K.-R.}
  (\bibinfo{year}{2003}). \bibinfo{title}{Kernel-based nonlinear blind source
  separation}.
\newblock \bibinfo{journal}{Neural Computation} \emph{\bibinfo{volume}{15}},
  \bibinfo{pages}{1089--1124}.
%Type = Article
\bibitem[{Sprekeler et~al.(2014)Sprekeler, Zito and Wiskott}]{Sprekeler14}
\bibinfo{author}{Sprekeler, H.}, \bibinfo{author}{Zito, T.}, and
  \bibinfo{author}{Wiskott, L.} (\bibinfo{year}{2014}). \bibinfo{title}{An
  extension of slow feature analysis for nonlinear blind source separation}.
\newblock \bibinfo{journal}{J.\ of Machine Learning Research}
  \emph{\bibinfo{volume}{15}}, \bibinfo{pages}{921--947}.
%Type = Inproceedings
\bibitem[{Hyv\"arinen and Morioka(2016)}]{Hyva16NIPS}
\bibinfo{author}{Hyv\"arinen, A.}, and \bibinfo{author}{Morioka, H.}
\newblock \bibinfo{title}{Unsupervised feature extraction by time-contrastive
  learning and nonlinear {ICA}}.
\newblock In: \emph{\bibinfo{booktitle}{Advances in Neural Information
  Processing Systems (NIPS2016)}}. \bibinfo{address}{Barcelona, Spain}
  (\bibinfo{year}{2016}):\unskip.
%Type = Inproceedings
\bibitem[{Hyv\"arinen and Morioka(2017)}]{Hyva17AISTATS}
\bibinfo{author}{Hyv\"arinen, A.}, and \bibinfo{author}{Morioka, H.}
\newblock \bibinfo{title}{Nonlinear {ICA} of temporally dependent stationary
  sources}.
\newblock In: \emph{\bibinfo{booktitle}{Proc.\ Artificial Intelligence and
  Statistics (AISTATS2017)}}. \bibinfo{address}{Fort Lauderdale, Florida}
  (\bibinfo{year}{2017}):\unskip.
%Type = Inproceedings
\bibitem[{Hyv\"arinen et~al.(2019)Hyv\"arinen, Sasaki and
  Turner}]{Hyva19AISTATS}
\bibinfo{author}{Hyv\"arinen, A.}, \bibinfo{author}{Sasaki, H.}, and
  \bibinfo{author}{Turner, R.}
\newblock \bibinfo{title}{Nonlinear {ICA} using auxiliary variables and
  generalized contrastive learning}.
\newblock In: \emph{\bibinfo{booktitle}{Proc.\ Artificial Intelligence and
  Statistics (AISTATS2019)}}. \bibinfo{address}{Okinawa, Japan}
  (\bibinfo{year}{2019}):\unskip.
%Type = Inproceedings
\bibitem[{Khemakhem et~al.(2020{\natexlab{a}})Khemakhem, Kingma, Monti and
  Hyv\"arinen}]{Khemakhem20iVAE}
\bibinfo{author}{Khemakhem, I.}, \bibinfo{author}{Kingma, D.~P.},
  \bibinfo{author}{Monti, R.~P.}, and \bibinfo{author}{Hyv\"arinen, A.}
\newblock \bibinfo{title}{Variational autoencoders and nonlinear {ICA}: A
  unifying framework}.
\newblock In: \emph{\bibinfo{booktitle}{Proc.\ Artificial Intelligence and
  Statistics (AISTATS2020)}} (\bibinfo{year}{2020}{\natexlab{a}}):\unskip.
%Type = Article
\bibitem[{Matsuoka et~al.(1995)Matsuoka, Ohya and Kawamoto}]{Matsuoka95}
\bibinfo{author}{Matsuoka, K.}, \bibinfo{author}{Ohya, M.}, and
  \bibinfo{author}{Kawamoto, M.} (\bibinfo{year}{1995}). \bibinfo{title}{A
  neural net for blind separation of nonstationary signals}.
\newblock \bibinfo{journal}{Neural Networks} \emph{\bibinfo{volume}{8}},
  \bibinfo{pages}{411--419}.
%Type = Article
\bibitem[{Pham and Cardoso(2001)}]{Pham01}
\bibinfo{author}{Pham, D.-T.}, and \bibinfo{author}{Cardoso, J.-F.}
  (\bibinfo{year}{2001}). \bibinfo{title}{Blind separation of instantaneous
  mixtures of nonstationary sources}.
\newblock \bibinfo{journal}{IEEE Trans.\ Signal Processing}
  \emph{\bibinfo{volume}{49}}, \bibinfo{pages}{1837--1848}.
%Type = Inproceedings
\bibitem[{Cardoso(2001{\natexlab{b}})}]{Cardoso01}
\bibinfo{author}{Cardoso, J.-F.}
\newblock \bibinfo{title}{The three easy routes to independent component
  analysis: contrasts and geometry}.
\newblock In: \emph{\bibinfo{booktitle}{Proc. Int.\ Workshop on Independent
  Component Analysis and Blind Signal Separation (ICA2001)}}.
  \bibinfo{address}{San Diego, California}
  (\bibinfo{year}{2001}{\natexlab{b}}):\unskip.
%Type = Inproceedings
\bibitem[{H\"alv\"a and Hyv\"arinen(2020)}]{Halva20UAI}
\bibinfo{author}{H\"alv\"a, H.}, and \bibinfo{author}{Hyv\"arinen, A.}
\newblock \bibinfo{title}{Hidden {M}arkov nonlinear {ICA}: Unsupervised
  learning from nonstationary time series}.
\newblock In: \emph{\bibinfo{booktitle}{Proc.\ 36th Conf.~ on Uncertainty in
  Artificial Intelligence (UAI2020)}}. \bibinfo{address}{Toronto, Canada
  (virtual)} (\bibinfo{year}{2020}):\unskip.
%Type = Inproceedings
\bibitem[{H\"alv\"a et~al.(2021)H\"alv\"a, Corff, Lehricy, So, Zhu, Gassiat
  and Hyv\"arinen}]{Halva21}
\bibinfo{author}{H\"alv\"a, H.}, \bibinfo{author}{Corff, S.~L.},
  \bibinfo{author}{Lehricy, L.}, \bibinfo{author}{So, J.},
  \bibinfo{author}{Zhu, Y.}, \bibinfo{author}{Gassiat, E.}, and
  \bibinfo{author}{Hyv\"arinen, A.}
\newblock \bibinfo{title}{Disentangling identifiable features from noisy data
  with structured nonlinear {ICA}}.
\newblock In: \emph{\bibinfo{booktitle}{Advances in Neural Information
  Processing Systems (NeurIPS2021)}}. \bibinfo{address}{Virtual}
  (\bibinfo{year}{2021}):\unskip.
%Type = Article
\bibitem[{Tong et~al.(1991)Tong, Liu, Soon and Huang}]{Tong91}
\bibinfo{author}{Tong, L.}, \bibinfo{author}{Liu, R.-W.},
  \bibinfo{author}{Soon, V.~C.}, and \bibinfo{author}{Huang, Y.-F.}
  (\bibinfo{year}{1991}). \bibinfo{title}{Indeterminacy and identifiability of
  blind identification}.
\newblock \bibinfo{journal}{IEEE Trans.\ on Circuits and Systems}
  \emph{\bibinfo{volume}{38}}, \bibinfo{pages}{499--509}.
%Type = Article
\bibitem[{Belouchrani et~al.(1997)Belouchrani, Meraim, Cardoso and
  Moulines}]{Belo97}
\bibinfo{author}{Belouchrani, A.}, \bibinfo{author}{Meraim, K.~A.},
  \bibinfo{author}{Cardoso, J.-F.}, and \bibinfo{author}{Moulines, E.}
  (\bibinfo{year}{1997}). \bibinfo{title}{A blind source separation technique
  based on second order statistics}.
\newblock \bibinfo{journal}{IEEE Trans.\ on Signal Processing}
  \emph{\bibinfo{volume}{45}}, \bibinfo{pages}{434--444}.
%Type = Article
\bibitem[{Wiskott and Sejnowski(2002)}]{Wiskott02}
\bibinfo{author}{Wiskott, L.}, and \bibinfo{author}{Sejnowski, T.~J.}
  (\bibinfo{year}{2002}). \bibinfo{title}{Slow feature analysis: Unsupervised
  learning of invariances}.
\newblock \bibinfo{journal}{Neural Computation} \emph{\bibinfo{volume}{14}},
  \bibinfo{pages}{715--770}.
%Type = Article
\bibitem[{F\"oldi\'ak(1991)}]{Foldiak91}
\bibinfo{author}{F\"oldi\'ak, P.} (\bibinfo{year}{1991}).
  \bibinfo{title}{Learning invariance from transformation sequences}.
\newblock \bibinfo{journal}{Neural Computation} \emph{\bibinfo{volume}{3}},
  \bibinfo{pages}{194--200}.
%Type = Inproceedings
\bibitem[{Mobahi et~al.(2009)Mobahi, Collobert and Weston}]{mobahi2009deep}
\bibinfo{author}{Mobahi, H.}, \bibinfo{author}{Collobert, R.}, and
  \bibinfo{author}{Weston, J.}
\newblock \bibinfo{title}{Deep learning from temporal coherence in video}.
\newblock In: \emph{\bibinfo{booktitle}{Proceedings of the 26th Annual
  International Conference on Machine Learning}}
  (\bibinfo{year}{2009}):\unskip( \bibinfo{pages}{737--744}).
%Type = Inproceedings
\bibitem[{Springenberg and Riedmiller(2012)}]{springenberg2012learning}
\bibinfo{author}{Springenberg, J.~T.}, and \bibinfo{author}{Riedmiller, M.}
\newblock \bibinfo{title}{Learning temporal coherent features through life-time
  sparsity}.
\newblock In: \emph{\bibinfo{booktitle}{Neural Information Processing}}.
  \bibinfo{organization}{Springer} (\bibinfo{year}{2012}):\unskip(
  \bibinfo{pages}{347--356}).
%Type = Inproceedings
\bibitem[{Goroshin et~al.(2015)Goroshin, Bruna, Tompson, Eigen and
  LeCun}]{goroshin2015unsupervised}
\bibinfo{author}{Goroshin, R.}, \bibinfo{author}{Bruna, J.},
  \bibinfo{author}{Tompson, J.}, \bibinfo{author}{Eigen, D.}, and
  \bibinfo{author}{LeCun, Y.}
\newblock \bibinfo{title}{Unsupervised learning of spatiotemporally coherent
  metrics}.
\newblock In: \emph{\bibinfo{booktitle}{IEEE Int.\ Conf.\ on Computer Vision}}
  (\bibinfo{year}{2015}):\unskip.
%Type = Article
\bibitem[{Schell and Oberhauser(2023)}]{schell2023nonlinear}
\bibinfo{author}{Schell, A.}, and \bibinfo{author}{Oberhauser, H.}
  (\bibinfo{year}{2023}). \bibinfo{title}{Nonlinear independent component
  analysis for discrete-time and continuous-time signals}.
\newblock \bibinfo{journal}{Annals of Statistics}.
%Type = Article
\bibitem[{Banville et~al.(2021)Banville, Chehab, Hyv\"arinen, Engemann and
  Gramfort}]{Banville21}
\bibinfo{author}{Banville, H.}, \bibinfo{author}{Chehab, O.},
  \bibinfo{author}{Hyv\"arinen, A.}, \bibinfo{author}{Engemann, D.-A.}, and
  \bibinfo{author}{Gramfort, A.} (\bibinfo{year}{2021}).
  \bibinfo{title}{Uncovering the structure of clinical {EEG} signals with
  self-supervised learning}.
\newblock \bibinfo{journal}{J.\ Neural Engineering}
  \emph{\bibinfo{volume}{18}}.
%Type = Inproceedings
\bibitem[{Morioka et~al.(2021)Morioka, H\"alv\"a and
  Hyv\"arinen}]{Morioka21AISTATS}
\bibinfo{author}{Morioka, H.}, \bibinfo{author}{H\"alv\"a, H.}, and
  \bibinfo{author}{Hyv\"arinen, A.}
\newblock \bibinfo{title}{Independent innovation analysis for nonlinear vector
  autoregressive process}.
\newblock In: \emph{\bibinfo{booktitle}{Proc.\ Artificial Intelligence and
  Statistics (AISTATS2021)}}. \bibinfo{address}{Virtual}
  (\bibinfo{year}{2021}):\unskip.
%Type = Article
\bibitem[{Oord et~al.(2018)Oord, Li and Vinyals}]{oord2018representation}
\bibinfo{author}{Oord, A. v.~d.}, \bibinfo{author}{Li, Y.}, and
  \bibinfo{author}{Vinyals, O.} (\bibinfo{year}{2018}).
  \bibinfo{title}{Representation learning with contrastive predictive coding}.
\newblock \bibinfo{journal}{arXiv preprint arXiv:1807.03748}.
%Type = Inproceedings
\bibitem[{Arandjelovic and Zisserman(2017)}]{arandjelovic2017look}
\bibinfo{author}{Arandjelovic, R.}, and \bibinfo{author}{Zisserman, A.}
\newblock \bibinfo{title}{Look, listen and learn}.
\newblock In: \emph{\bibinfo{booktitle}{2017 IEEE International Conference on
  Computer Vision (ICCV)}}. \bibinfo{organization}{IEEE}
  (\bibinfo{year}{2017}):\unskip( \bibinfo{pages}{609--617}).
%Type = Inproceedings
\bibitem[{Gresele et~al.(2020{\natexlab{a}})Gresele, Rubenstein, Mehrjou,
  Locatello and Sch\"olkopf}]{gresele2020incomplete}
\bibinfo{author}{Gresele, L.}, \bibinfo{author}{Rubenstein, P.~K.},
  \bibinfo{author}{Mehrjou, A.}, \bibinfo{author}{Locatello, F.}, and
  \bibinfo{author}{Sch\"olkopf, B.}
\newblock \bibinfo{title}{The {{Incomplete Rosetta Stone Problem}}:
  {{Identifiability Results}} for {{Multi-View Nonlinear ICA}}}.
\newblock In: \emph{\bibinfo{booktitle}{Uncertainty in {{Artificial
  Intelligence}}}}. \bibinfo{publisher}{{PMLR}}
  (\bibinfo{year}{2020}{\natexlab{a}}):\unskip( \bibinfo{pages}{217--227}).
\newblock \href{http://arxiv.org/abs/1905.06642}{\tt arXiv:1905.06642}.
%Type = Inproceedings
\bibitem[{Morioka and Hyv\"arinen(2023)}]{Morioka23AISTATS}
\bibinfo{author}{Morioka, H.}, and \bibinfo{author}{Hyv\"arinen, A.}
\newblock \bibinfo{title}{Connectivity-contrastive learning: Combining causal
  discovery and representation learning for multimodal data}.
\newblock In: \emph{\bibinfo{booktitle}{Proc.\ Artificial Intelligence and
  Statistics (AISTATS2023)}}. \bibinfo{address}{Valencia, Spain}
  (\bibinfo{year}{2023}):\unskip.
%Type = Inproceedings
\bibitem[{Gresele et~al.(2020{\natexlab{b}})Gresele, Fissore, Javaloy,
  Sch\"olkopf and Hyv\"arinen}]{Gresele20}
\bibinfo{author}{Gresele, L.}, \bibinfo{author}{Fissore, G.},
  \bibinfo{author}{Javaloy, A.}, \bibinfo{author}{Sch\"olkopf, B.}, and
  \bibinfo{author}{Hyv\"arinen, A.}
\newblock \bibinfo{title}{Relative gradient optimization of the jacobian term
  in unsupervised deep learning}.
\newblock In: \emph{\bibinfo{booktitle}{Advances in Neural Information
  Processing Systems (NeurIPS2020)}}. \bibinfo{address}{Virtual}
  (\bibinfo{year}{2020}{\natexlab{b}}):\unskip.
%Type = Article
\bibitem[{Klindt et~al.(2020)Klindt, Schott, Sharma, Ustyuzhaninov, Brendel,
  Bethge and Paiton}]{klindt2020towards}
\bibinfo{author}{Klindt, D.}, \bibinfo{author}{Schott, L.},
  \bibinfo{author}{Sharma, Y.}, \bibinfo{author}{Ustyuzhaninov, I.},
  \bibinfo{author}{Brendel, W.}, \bibinfo{author}{Bethge, M.}, and
  \bibinfo{author}{Paiton, D.} (\bibinfo{year}{2020}). \bibinfo{title}{Towards
  nonlinear disentanglement in natural data with temporal sparse coding}.
\newblock \bibinfo{journal}{arXiv preprint arXiv:2007.10930}.
%Type = Article
\bibitem[{Song and Kingma(2021)}]{song2021train}
\bibinfo{author}{Song, Y.}, and \bibinfo{author}{Kingma, D.~P.}
  (\bibinfo{year}{2021}). \bibinfo{title}{How to train your energy-based
  models}.
\newblock \bibinfo{journal}{arXiv preprint arXiv:2101.03288}.
%Type = Inproceedings
\bibitem[{Khemakhem et~al.(2020{\natexlab{b}})Khemakhem, Monti, Kingma and
  Hyv\"arinen}]{Khemakhem20NIPS}
\bibinfo{author}{Khemakhem, I.}, \bibinfo{author}{Monti, R.~P.},
  \bibinfo{author}{Kingma, D.~P.}, and \bibinfo{author}{Hyv\"arinen, A.}
\newblock \bibinfo{title}{{ICE-BeeM}: Identifiable conditional energy-based
  deep models based on nonlinear {ICA}}.
\newblock In: \emph{\bibinfo{booktitle}{Advances in Neural Information
  Processing Systems (NeurIPS2020)}}. \bibinfo{address}{Virtual}
  (\bibinfo{year}{2020}{\natexlab{b}}):\unskip.
%Type = Mastersthesis
\bibitem[{Luopaj\"arvi(2022)}]{LuopajarviThesis}
\bibinfo{author}{Luopaj\"arvi, K.}
\newblock \bibinfo{title}{Estimation of the {iVAE} model with generative
  adversarial networks}.
\newblock Master's thesis University of Helsinki (\bibinfo{year}{2022}).
\newblock \bibinfo{note}{Supervised by A.~Hyv\"arinen and H.~H\"alv\"a}.
%Type = Article
\bibitem[{Zhu et~al.(2022)Zhu, Parviainen, Heinil\"a, Parkkonen and
  Hyv\"arinen}]{Zhu23}
\bibinfo{author}{Zhu, Y.}, \bibinfo{author}{Parviainen, T.},
  \bibinfo{author}{Heinil\"a, E.}, \bibinfo{author}{Parkkonen, L.}, and
  \bibinfo{author}{Hyv\"arinen, A.} (\bibinfo{year}{2022}).
  \bibinfo{title}{Unsupervised representation learning of spontaneous {MEG}
  data with nonlinear {ICA}}.
\newblock \bibinfo{journal}{Submitted manuscript}.
%Type = Article
\bibitem[{Zhou and Wei(2020)}]{zhou2020learning}
\bibinfo{author}{Zhou, D.}, and \bibinfo{author}{Wei, X.-X.}
  (\bibinfo{year}{2020}). \bibinfo{title}{Learning identifiable and
  interpretable latent models of high-dimensional neural activity using
  pi-vae}.
\newblock \bibinfo{journal}{Advances in Neural Information Processing Systems}
  \emph{\bibinfo{volume}{33}}, \bibinfo{pages}{7234--7247}.
%Type = Article
\bibitem[{Schneider et~al.(2022)Schneider, Lee and
  Mathis}]{schneider2022learnable}
\bibinfo{author}{Schneider, S.}, \bibinfo{author}{Lee, J.~H.}, and
  \bibinfo{author}{Mathis, M.~W.} (\bibinfo{year}{2022}).
  \bibinfo{title}{Learnable latent embeddings for joint behavioral and neural
  analysis}.
\newblock \bibinfo{journal}{arXiv preprint arXiv:2204.00673}.
%Type = Inproceedings
\bibitem[{Ravanelli et~al.(2020)Ravanelli, Zhong, Pascual, Swietojanski,
  Monteiro, Trmal and Bengio}]{ravanelli2020multi}
\bibinfo{author}{Ravanelli, M.}, \bibinfo{author}{Zhong, J.},
  \bibinfo{author}{Pascual, S.}, \bibinfo{author}{Swietojanski, P.},
  \bibinfo{author}{Monteiro, J.}, \bibinfo{author}{Trmal, J.}, and
  \bibinfo{author}{Bengio, Y.}
\newblock \bibinfo{title}{Multi-task self-supervised learning for robust speech
  recognition}.
\newblock In: \emph{\bibinfo{booktitle}{ICASSP 2020-2020 IEEE International
  Conference on Acoustics, Speech and Signal Processing (ICASSP)}}.
  \bibinfo{organization}{IEEE} (\bibinfo{year}{2020}):\unskip(
  \bibinfo{pages}{6989--6993}).
%Type = Inproceedings
\bibitem[{Monti et~al.(2019)Monti, Zhang and Hyv\"arinen}]{Monti19UAI}
\bibinfo{author}{Monti, R.~P.}, \bibinfo{author}{Zhang, K.}, and
  \bibinfo{author}{Hyv\"arinen, A.}
\newblock \bibinfo{title}{Causal discovery with general non-linear
  relationships using non-linear {ICA}}.
\newblock In: \emph{\bibinfo{booktitle}{Proc.\ 35th Conf.~ on Uncertainty in
  Artificial Intelligence (UAI2019)}}. \bibinfo{address}{Tel Aviv, Israel}
  (\bibinfo{year}{2019}):\unskip.
%Type = Article
\bibitem[{Gresele et~al.(2021)Gresele, Von~K{\"u}gelgen, Stimper, Sch{\"o}lkopf
  and Besserve}]{gresele2021independent}
\bibinfo{author}{Gresele, L.}, \bibinfo{author}{Von~K{\"u}gelgen, J.},
  \bibinfo{author}{Stimper, V.}, \bibinfo{author}{Sch{\"o}lkopf, B.}, and
  \bibinfo{author}{Besserve, M.} (\bibinfo{year}{2021}).
  \bibinfo{title}{Independent mechanism analysis, a new concept?}
\newblock \bibinfo{journal}{Advances in neural information processing systems}
  \emph{\bibinfo{volume}{34}}, \bibinfo{pages}{28233--28248}.
%Type = Inproceedings
\bibitem[{Zimmermann et~al.(2021)Zimmermann, Sharma, Schneider, Bethge and
  Brendel}]{zimmermann2021contrastive}
\bibinfo{author}{Zimmermann, R.~S.}, \bibinfo{author}{Sharma, Y.},
  \bibinfo{author}{Schneider, S.}, \bibinfo{author}{Bethge, M.}, and
  \bibinfo{author}{Brendel, W.}
\newblock \bibinfo{title}{Contrastive learning inverts the data generating
  process}.
\newblock In: \emph{\bibinfo{booktitle}{International Conference on Machine
  Learning}}. \bibinfo{organization}{PMLR} (\bibinfo{year}{2021}):\unskip(
  \bibinfo{pages}{12979--12990}).
%Type = Article
\bibitem[{Buchholz et~al.(2022)Buchholz, Besserve and
  Sch{\"o}lkopf}]{buchholz2022function}
\bibinfo{author}{Buchholz, S.}, \bibinfo{author}{Besserve, M.}, and
  \bibinfo{author}{Sch{\"o}lkopf, B.} (\bibinfo{year}{2022}).
  \bibinfo{title}{Function classes for identifiable nonlinear independent
  component analysis}.
\newblock \bibinfo{journal}{arXiv preprint arXiv:2208.06406}.
%Type = Article
\bibitem[{Taleb and Jutten(9 10)}]{taleb1999source}
\bibinfo{author}{Taleb, A.}, and \bibinfo{author}{Jutten, C.}
  (\bibinfo{year}{1999-10}). \bibinfo{title}{Source separation in
  post-nonlinear mixtures}.
\newblock \bibinfo{journal}{IEEE Transactions on Signal Processing}
  \emph{\bibinfo{volume}{47}}, \bibinfo{pages}{2807--2820}.
  \DOIprefix\doi{10.1109/78.790661}.
%Type = Inproceedings
\bibitem[{Lachapelle et~al.(2022)Lachapelle, Rodriguez, Sharma, Everett,
  Le~Priol, Lacoste and Lacoste-Julien}]{lachapelle2022disentanglement}
\bibinfo{author}{Lachapelle, S.}, \bibinfo{author}{Rodriguez, P.},
  \bibinfo{author}{Sharma, Y.}, \bibinfo{author}{Everett, K.~E.},
  \bibinfo{author}{Le~Priol, R.}, \bibinfo{author}{Lacoste, A.}, and
  \bibinfo{author}{Lacoste-Julien, S.}
\newblock \bibinfo{title}{Disentanglement via mechanism sparsity
  regularization: A new principle for nonlinear {ICA}}.
\newblock In: \emph{\bibinfo{booktitle}{Conference on Causal Learning and
  Reasoning}}. \bibinfo{organization}{PMLR} (\bibinfo{year}{2022}):\unskip(
  \bibinfo{pages}{428--484}).
%Type = Inproceedings
\bibitem[{Zhang and Hyv\"arinen(2010)}]{Zhang10UAI}
\bibinfo{author}{Zhang, K.}, and \bibinfo{author}{Hyv\"arinen, A.}
\newblock \bibinfo{title}{Source separation and higher-order causal analysis of
  {MEG} and {EEG}}.
\newblock In: \emph{\bibinfo{booktitle}{Proc.\ 26th Conference on Uncertainty
  in Artificial Intelligence (UAI2010)}}. \bibinfo{address}{Catalina Island,
  California} (\bibinfo{year}{2010}):\unskip.
%Type = Article
\bibitem[{Theis et~al.(2015)Theis, Oord and Bethge}]{theis2015note}
\bibinfo{author}{Theis, L.}, \bibinfo{author}{Oord, A. v.~d.}, and
  \bibinfo{author}{Bethge, M.} (\bibinfo{year}{2015}). \bibinfo{title}{A note
  on the evaluation of generative models}.
\newblock \bibinfo{journal}{arXiv preprint arXiv:1511.01844}.

\end{thebibliography}




\end{document}