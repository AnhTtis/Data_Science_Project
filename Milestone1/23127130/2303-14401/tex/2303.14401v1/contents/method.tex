This section of the paper gives detailed insights about the implementation and approach we have taken to solve the problem. The model considering the implementation with respect to Deep LDA \cite{dorfer2015deep,8014812} revolves around the idea of the convolutional neural networks \cite{lecun1995convolutional,alzubaidi2021review,gupta2022detection,joshi2022refactoring}. The modification can be done to work with numerical values. This has been implemented by various developers and names of the developers are given in the acknowledgement section of the paper. The Deep LDA is basically an implementation of latent representations in linearly separable method. The Deep LDA is an extensive implementation of the traditional Linear Discriminant Analysis which was intended for dimensionality reduction based classification methods. The implementation consists of 2 phases, first phase consists of linear discriminator as the deep neural network and second phase consists of support vector machine for detailed classification.

\subsection{First Phase}

\begin{figure}[htbp]
    \centering
    \includegraphics[angle=270,scale=0.9]{images/figure1.png}
    \caption{First Phase with LDA Implementation}
    \label{fig:a}
\end{figure}

The figure \ref{fig:a} gives depiction of first phase of the implementation. The input layer takes 41 dimension of features from the data. This is passed on to one dense layer that has 1024 hidden neurons. The L2 regularization \cite{cortes2012l2} is applied as a kernel regularization for the layer. The activation function is used sigmoid \cite{10.1016/S0893-6080(05)80129-7}. The rectified linear unit abbreviated as ReLU \cite{agarap2018deep} is the activation function commonly used for deep learning methods but using sigmoid ensures linear based system such as developed for linear discriminant analysis. The parameters learned from the first hidden layer are 43,008. Similar type of hidden layers are repeated twice, where second hidden layer learns 1,049,600 parameters and third hidden layer also learns same amount of parameters. The output layer consists of 1 hidden neuron with sigmoid activation function and learns 1025 parameters. The network learns total of 2,143,233 parameters where all the parameters are trainable. The loss function used is binary cross-entropy that differs from the original implementation of deep LDA paper. The loss optimizer used is Adam \cite{kingma2014adam} optimizer with learning rate of $1*10^{-5}$ that roughly denotes 0.00001. The implementation is done using Keras \cite{chollet2015keras} over Tensorflow \cite{tensorflow2015-whitepaper} back-end trained for 100 epochs with 64 as batch size.

\subsection{Second Phase}

\begin{figure}[htbp]
    \centering
    \includegraphics[angle=270,scale=0.9]{images/figure2.png}
    \caption{Second Phase with SVM Implementation}
    \label{fig:b}
\end{figure}

The figure \ref{fig:b} depicts the second phase implementation. This is done using Support Vector Machine \cite{708428} implementation with neural network inclination. The input layer is connected to hidden layer with 100 hidden neurons with ReLU \cite{agarap2018deep} activation function. This layer learns 200 parameters. This layer is connected to dropout \cite{JMLR:v15:srivastava14a} layer with 50\% threshold and does not learn any parameters. The output layer has 1 hidden neuron and has sigmoid activation function for binary classification and learns 101 parameters. The total parameters learned are 301 and the network uses binary cross-entropy. The loss optimizer used is Adam \cite{kingma2014adam} optimizer with  $1*10^{-5}$ that approximately denotes 0.00001 learning rate. The network is trained with 100 epochs and 64 as batch size.

\subsection{Complete Network}

\begin{figure}[htbp]
    \centering
    \includegraphics[angle=270,scale=0.8]{images/figure3.png}
    \caption{Complete Network}
    \label{fig:c}
\end{figure}

The complete network is accumulation of first and second phase where the output of the first phase is the input for second phase. The output of the first phase is 1-dimensional array from 41-dimensional output. This is given as an input to the second phase of the network and final prediction which is 1-dimensional is achieved. The both phases are trained independently and the output is retained from first phase and given as second phase. The results of the network will be given in succeeding section of the paper.