\subsection{Accuracy and Loss for First Phase}

\begin{figure}[htbp]
    \centering
    \begin{tabular}{c c}
        \begin{tikzpicture}[scale=0.6]
            \begin{axis}[xlabel={Epochs},ylabel ={Accuracies},enlargelimits=false,
            grid=both,
            scale only axis=true,legend pos=south east,style={ultra thick}, axis line style={ultra thin}]
            \addplot+[no markers] table[x=Epochs,y=accuracy,col sep=comma]{plots/lda.csv}; 
            \addplot+[no markers] table[x=Epochs,y=val_accuracy,col sep=comma]{plots/lda.csv};
            \addlegendentry{Training Accuracy}
            \addlegendentry{Validation Accuracy}
        \end{axis}
        \end{tikzpicture}
        &
        \begin{tikzpicture}[scale=0.6]
            \begin{axis}[xlabel={Epochs},ylabel ={Losses},enlargelimits=false,
            grid=both,
            scale only axis=true,legend pos=north east,style={ultra thick}, axis line style={ultra thin}]
            \addplot+[no markers] table[x=Epochs,y=loss,col sep=comma]{plots/lda.csv}; 
            \addplot+[no markers] table[x=Epochs,y=val_loss,col sep=comma]{plots/lda.csv};
            \addlegendentry{Training Loss}
            \addlegendentry{Validation Loss}
        \end{axis}
        \end{tikzpicture}
    \end{tabular}
    \caption{Accuracy and Loss for the First Phase}
    \label{fig:d}
\end{figure}

The training and validation accuracy graph can be seen from figure \ref{fig:d} and the infractions between the training and validation accuracy seem a bit wider but the values for training are 98.35\% and validation 90.909\% respectively. Considering the loss, the validation loss has some infraction that can be seen with different metrics the training loss is 6.79\% whereas the validation loss is 38.05\% respectively.

\subsection{Accuracy and Loss for Second Phase}
The second phase contains support vector machine and not necessarily the graph depiction is right measure but we have included the graph. The inference can be drawn as there are no significant changes in the accuracy or loss. All the metrics are learnt from the trained parameters of the first phase and it does not make much sense to make mappings out of it. The training accuracy for the support vector machine phase is generated as 98.354\% and validation accuracy is obtained as 90.909\% which is similar to the first phase training and validation accuracy. The training loss is 6.79\% and validation loss is 38.052\% which is again similar to the first phase. The better inference can be generated from different metrics intended for classification and not just the graph depictions of the training and validation accuracy as well as loss.


\begin{figure}[htbp]
    \centering
    \begin{tabular}{c c}
        \begin{tikzpicture}[scale=0.6]
            \begin{axis}[xlabel={Epochs},ylabel ={Accuracies},enlargelimits=false,
            grid=both,
            scale only axis=true,legend pos=south east,style={ultra thick}, axis line style={ultra thin}]
            \addplot+[no markers] table[x=Epochs,y=accuracy,col sep=comma]{plots/svm.csv}; 
            \addplot+[no markers] table[x=Epochs,y=val_accuracy,col sep=comma]{plots/svm.csv};
            \addlegendentry{Training Accuracy}
            \addlegendentry{Validation Accuracy}
        \end{axis}
        \end{tikzpicture}
        &
        \begin{tikzpicture}[scale=0.6]
            \begin{axis}[xlabel={Epochs},ylabel ={Losses},enlargelimits=false,
            grid=both,
            scale only axis=true,legend pos=north east,style={ultra thick}, axis line style={ultra thin}]
            \addplot+[no markers] table[x=Epochs,y=loss,col sep=comma]{plots/svm.csv}; 
            \addplot+[no markers] table[x=Epochs,y=val_loss,col sep=comma]{plots/svm.csv};
            \addlegendentry{Training Loss}
            \addlegendentry{Validation Loss}
        \end{axis}
        \end{tikzpicture}
    \end{tabular}
    \caption{Accuracy and Loss for the Second Phase}
    \label{fig:e}
\end{figure}

\subsection{Precision}
The precision \cite{powers2020evaluation} is dependent on total number of samples that are predicted to be positive among all the set of samples. This is a popular metric for prognostication algorithms and requires basic elements of a confusion matrix \cite{ting2017confusion}, viz. true positives, true negatives, false positives and false negatives. The precision score obtained in 88.88\% which is very close to 1 as expected.

\subsection{Recall}
The recall \cite{powers2020evaluation} is a metrics which states all the positive elements from the every single predicted element. The recall also utilizes every single element from the confusion matrix same as precision. The recall generated for the model is 80\% which is again a very good score and gives inference that model has performed adequately. The recall is not only metric that gives the final inference and more precise metric can be obtained.

\subsection{F-Score}
This is a subtle metric that gives the overall flow of how efficiently does the model perform. The building blocks of F-score \cite{powers2020evaluation,goutte2005probabilistic} are precision and recall. The F-Score we got for the model is 84.21\%, which is adequately good and proves that model performs better on average.