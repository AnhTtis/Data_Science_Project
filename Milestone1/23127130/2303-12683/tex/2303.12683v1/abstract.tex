\begin{abstract}
    Adaptive design optimization (ADO) is a state-of-the-art technique for experimental design \citep{cavagnaro_adaptive_2010}. 
    ADO dynamically identifies stimuli that, in expectation, yield the most information about a hypothetical construct of interest (e.g., parameters of a cognitive model). 
    To calculate this expectation, ADO leverages the modelerâ€™s existing knowledge, specified in the form of a prior distribution.
    \emph{Informative} priors align with the distribution of the focal construct in the participant population. 
    This alignment is assumed by ADO's internal assessment of expected information gain. 
    If the prior is instead \emph{misinformative}, i.e., does not align with the participant population, ADO's estimates of expected information gain could be inaccurate.
    In many cases, the true distribution that characterizes the participant population is unknown, and experimenters rely on heuristics in their choice of prior and without an understanding of how this choice affects ADO's behavior.

    Our work introduces a mathematical framework that facilitates investigation of the consequences of the choice of prior distribution on the efficiency of experiments designed using ADO.
    Through theoretical and empirical results, we show that, in the context of \emph{prior misinformation}, measures of expected information gain are distinct from the correctness of the corresponding inference.
    Through a series of simulation experiments, we show that, in the case of parameter estimation, ADO nevertheless outperforms other design methods.
    Conversely, in the case of model selection, misinformative priors can lead inference to favor the wrong model, and rather than mitigating this pitfall, ADO exacerbates it.
\end{abstract}