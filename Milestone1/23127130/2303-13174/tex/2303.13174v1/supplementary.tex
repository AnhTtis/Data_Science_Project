\documentclass{article}

\usepackage{geometry}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\renewcommand{\thefigure}{S\arabic{figure}}

\renewcommand{\thetable}{S\arabic{table}}

\newcommand{\etal}{\textit{et al}.}
\newcommand{\ie}{\textit{i}.\textit{e}.}
\newcommand{\eg}{\textit{e}.\textit{g}.}


\title{Supplementary Materials: 3D-POP - An automated annotation approach to facilitate markerless 2D-3D tracking of freely moving birds}

%\author{Paper ID: 8975}

\begin{document}

\maketitle


\begin{abstract}
    The following text is supplementary text for the paper "3D-POP - An automated annotation approach to facilitate markerless 2D-3D tracking of freely moving birds with marker-based motion capture". 
    The text includes details of methods and results that are not part of the main text which are described in more detail here. We also outline detailed method that can be helpful to replicate the setup and annotation process, especially for biologists. 
%    Each section also contains a line number "ln xx" in the title which serves the function of a bookmark and leads the reader to corresponding section in the main text.  
\end{abstract}
    

%1.0
%\section{state of the art table?}

\section{Affiliations}
Here, we provide complete affiliations for the authors from the main text, with identical numbering.

1. Department of Collective Behaviour and Department of Ecology of Animal Societies, Max Planck Institute of Animal Behavior, 78464 Konstanz, Germany.

2. Department of Biology, University of Konstanz, 78464 Konstanz, Germany.

3. Centre for the Advanced Study of Collective Behaviour, University of Konstanz, 78464 Konstanz, Germany.

4. Computer Aided Medial Procedures, Informatik Department, Technische Universität München, Boltzmannstraße 3, 85748, Garching bei München, Germany. 

5. Department of Biological Physics, Eötvös Loránd University, Pázmány Péter sétány 1A, Budapest 1117, Hungary.

6. MTA-ELTE ‘Lendület’ Collective Behaviour Research Group, Hungarian Academy of Sciences, Budapest 1117, Hungary.

\section{Supplementary Methods}
% 2.1?
\subsection{Camera Calibration:}
The Infrared cameras of the vicon motion capture system (Vicon Vero,Vantage) are calibrated using built in software (Vicon Nexus) with a calibration wand. 
All cameras are also time synchronized when recording. 
The calibration of the Vicon system forms the basis of the whole dataset, in which we use the vicon coordinate system for all 3D coordinates. 

For the 4 high definition RGB action cameras, we performed intrinsic calibration, extrinsic calibration and time synchronization independently. 

\subsubsection{Intrinsic and Extrinsic Calibration}
For intrinsic calibration, we used an A0 (84.1 x 118.9 cm) charuco checkerboard before each day of recordings and undistorted all videos from each camera view using the obtained distortion and camera matrix from opencv.

For extrinsic calibration, we adopted a subject based approach, where we manually annotated the 2D position of a motion capture markers visible in the image (\eg, backpack marker) on a moving pigeon subject over up to 30 frames. 
For each frame, we compute camera pose using 2D marker positions on the backpack and the 3D coordinates of the backpack in the vicon coordinate system.
The combination of both provide us the extrinsic parameters for each camera. 
We ensure that sampled 3D positions are well distributed in the tracking volume to avoid bias in extrinsic parameters. 
This approach is useful as it allows us to move the tripod positions between sessions and perform fast extrinsic calibration without using the checkerboard. 

%The RGB cameras were placed on portable tripods during the experiment, we show that this approach provides flexibility with changing positions of cameras as well as avoiding complicated calibration protocols every time cameras were moved (intentionally or accidental). 
In the future, we plan to make the method for marker selection automatic, which would improve accuracy as the system will recompute extrinsics in real-time and change in camera position would not require calibration.

%\subsection{Extrinsic Calibration}
%For extrinsic calibration, we used a unique way to compute pose of each RGB camera with the Vicon coordinate system. Our method is a flexible system that uses the pigeon subjects as calibration targets, by annotating one backpack marker of a pigeon moving in a recording sequence for around 30 frames. For each frame, we used the annotated 2D coordinates and corresponding 3D positions from the motion tracking system to compute camera pose, by ensuring the pigeon covered large amount of the experimental area. With this method, we do not need to complete complex calibration procedures every time cameras are moved, and gives researchers the freedom to manipulate the experimental setup between each recording sequence. In the dataset, we provide pose of each camera relative to the vicon coordinate system.


% 2.2?
\subsection{Temporal synchronization}
To synchronize the RGB action cameras, we attached a camera control box (Sony CBB-WD1) to each action camera. The control boxes has built-in functionality to synchronize video streams from multiple cameras over ethernet and a network switch. 

Since our data are collected by two independent systems (Motion tracking system and RGB cameras), we designed an arduino based synchronization device with 3 RGB LED lights and 2 infra-red lights that blinks for 1 second at 5 second intervals. For the RGB videos, we computed the change in maximum pixel values of the cropped box area through time, and detected light flashes based on a change of more than 30 units. For the vicon system, we attached 4 additional markers onto the arduino box, which allow a new object to be defined within the mo-cap software together with the 2 infra-red lights. Flashes can then be detected based on the number of markers present in the object (6 markers for flash on, 4 markers for flash off). The systems are then temporally synchronized by matching the detected light flashes from both systems. In cases where a light flash were not detected, we manually filled in the flash by assuming the flash is exactly 6 seconds after the previous one. 
%This temporal synchronization technique was effective, though can lead to de-synced sequences when pigeon subjects are moving too quickly

\subsection{Markerless Data}
POP-3D dataset contains ground truth annotation for pigeons with markers attached to their body. However, we expect this dataset to play a role in development of markeless algorithms for tracking pigeons and other birds. 
In experiment 2, we show that models trained on 3D-POP dataset do work well with pigeons recorded in the same arena without any markers. 
To validate results in future methods, 3D-POP dataset also includes trials of freely moving birds without any marker attachment (n = 1,2,5,11). 
The videos will serve useful for making qualitative claim about performance of algorithms developed with mocap annotated posture or identity data. 
It is worth noting that we only demonstrate that position of markers do not play role in prediction of keypoints but do not show same validation for problem of identity recognition. 
Markers may play a role in identity recognition, we would like to test it in future work. 
Please see Table \ref{table:datasum-womarkers} for more details of sessions recorded with pigeons without markers.

\begin{table}[h]
\begin{center}
\begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|}
\hline
No. individuals & Available frames & Video length (min)\\
\hline
1 & 36,825 & 20\\
2 & 36,600 & 20 \\
5 & 9,810& 5.5\\
11 & 36,225& 20\\
\hline
\end{tabular}
\end{center}
\caption{Markerless Data Summary: Total number of frames and video length for markerless data.}
\label{table:datasum-womarkers}
\end{table}


%2.3
\subsection{Post-processing Mo-cap data}
The data obtained from motion capture is often not directly usable for the annotation pipeline. 
It requires a post-processing step to ensure smooth annotation process. 
Marker-based motion capture has a limitations related to tracking loss and marker identification (for 6-DOF tracking).
This results in error of identification of pigeons and 6-DOF pose of rigid bodies defined by the mo-cap system. 
The problem stems from limited availability of space on the pigeon body. 
The markers are forced to be close to each other (in 4-marker pattern) which leads to error in correspondence matching required for pose computation. 
To solve for these changes, we applied a post-processing pipeline introduced by Kano \etal \cite{kano2022birds} to fix mis-labelled frames by detecting large changes in the distance between defined markers, then determining the correct labels through permutation techniques.
Detailed description and code used is provided in Kano \etal \cite{kano2022birds}. 


\begin{figure*} [!h]
   \includegraphics[width=\textwidth,trim=0cm 1cm 3cm 0cm]{ComputePoseDiagram.pdf}
   \caption{Schematic for computing 3D orientation angles of the pigeon's body parts (head and body) for a given frame. A) Obtain 3D coordinates of keypoints in the world coordinate system (vicon). B) Translate all keypoints to new origin (beak) and compute vectors towards the origin using keypoints. C) Define a plane and compute surface normal. D) Calculate angle between surface normals and the 3 primary axes.}
\label{fig:computePose}
\end{figure*}

\begin{figure*}
   \includegraphics[width=\textwidth,trim=0cm 1cm 1cm 0cm]{PoseRotationHistograms.pdf}
   \caption{Frame distribution of the head and backpack rotation angles with respect to the 3 primary axes of pigeon subjects present in the dataset. A) Distribution of head rotation angles. B) distribution of backpack rotation angles. }
\label{fig:PoseHistogram}
\end{figure*}

%2.4
\subsection{Computing Pose Variation:}
Computing variation in pose is important to understand how many different postures are recorded in the dataset. 
Definition of coordinate system defined for each pigeon's body part (head, body) is slightly different because the definition is based on marker positions, which is different for each pigeon each day.
%they are defined using markers. 
Therefore, the orientation angles  defined by the 6-DOF pose w.r.t the canonical frame (world coordinate system) are different for each pigeon even if absolute posture of pigeons is the same.
This problem does not allow us to compute variation of pose for each pigeon in a standard manner.
We argue that pigeon posture can be measured in a standard way if orientation of the body parts are defined using positions of 3D keypoints in a unified coordinate system \ie, world coordinate system. 
The keypoints such as beak, eyes or shoulders are common features recorded for each bird and thus a posture representation involving these features would provide means for comparing postures of different pigeons.
Using this logic we designed a new technique to measure 3D orientation (rotational angles)  of pigeon body parts relative to each axis. 
Please refer Figure \ref{fig:computePose} for a pictorial representation. 
 
In this text, we will describe the steps to obtain rotation angles of the head in the world coordinate system. 
Firstly, we select beak as primary keypoint and shift the origin of world coordinate system to this point (translation).
This is done because we are interested in comparing rotation units and shifting origin reduces complexity of pose representation from 6-DOF (rotation and translation) to 3-DOF (rotation).
In other words, we change the representation of a rigid body (pigeon head) from 6-DOF pose representation to a plane representation (passing through origin).
The plane is defined using 3D positions of beak, left eye and right eye (Figure \ref{fig:computePose}). 
The normal of the plane is the cross product of two vectors originating from beak to left and right eye.
This normal is defined at the origin and it's angles with respect to the primary x-y-z axis, which represent one unique head orientation in the world coordinate system.
This process is repeated to compute head posture of all pigeons in all sequences.
The comparison between all posture is only possible because the world coordinate system is consistent for all sequences. 
Finally, we show a histogram of the occurrences of the different rotation angles to indicate pose variation  (Figure \ref{fig:PoseHistogram}).
The same process is repeated for body pose by defining a plane using shoulder and tail keypoints, with tail as origin.

%For each pigeon, at each given frame in the vicon coordinate system (Figure \ref{fig:computePose}). The head plane is defined by the beak and L/R eyes, with the beak as origin, and the body plane is defined by the tail and L/R shoulder with the tail as origin. 
%For each plane, we then computed the unit vector of the vector normal, and calculated the angle of the vector normal relative to the 3 axis in the Vicon coordinate system (Figure \ref{fig:computePose}). 
%We then transformed the obtained values to angles in degrees.
\subsection{Dataset comparison}
There are many datasets available with animals that target one or more problems. 
We provide a short overview 18 different datasets and provide a table at the end of this text as auxiliary information. 

\section{Experimental results}

\subsection{Outlier Detection and Filtering Pipeline}
Our outlier detection and filtering pipeline introduces gaps in the dataset. Here is a quantification of the number and length of gaps that are present in the dataset. (Table \ref{table:Gaps})

\begin{table}[h]
\begin{center}
\begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|}
\hline
1 frame & 2-30 frames & $>$30 frames\\
\hline
3585 & 5062 & 351\\
\hline
\end{tabular}
\end{center}
\caption{Frame length of consecutive gaps present in the dataset}
\label{table:Gaps}
\end{table}

%\begin{figure}
%\centering
%   \includegraphics[width=0.9\textwidth, trim=0cm 2.2cm 2cm 0cm]{DLCHistogram.pdf}
%   \caption{Distribution of Euclidean distances (px) between model predictions of a trained DLC model and annotations, after outlier frames were filtered. Frequency shown in the y-axis and and only points of up to 10px error is shown on the x-axis. A) Head keypoints B) Backpack keypoints}
%\label{fig:DLCError}
%\end{figure}

\subsection{Experiment 1 - Hybrid Approach:}
In the paper, we performed an experiment to show that markerless tracking is possible for pigeons using the 3D-POP dataset. 
First, experiment was performed with pigeons with marker attached, like experimental scenario of Kano \etal \cite{kano2022birds}.
We claim that markerless solution is directly useful to improve tracking performance for cases where motion capture is already in use.
We took a sample data from our recording sequence and introduced gaps in trajectories to simulate loss of tracking, then further used markerless approach to fill the gaps to evaluate quality of markerless 3D tracking in comparison with ground truth.
The results are demonstrated in Table \ref{table:Hybrid}, which show that using a 2D keypoint detection model and simple triangulation, the gap filling algorithm provides good accuracy.
This is useful already for experiments where missing data with mo-cap setup is a consistent problem. 
We also show a simple comparison with interpolation approach to show that markerless solution have higher chance of filling gaps than data interpolation methods. 
In future work, we want to try different strategies to combine multi-view data to get higher prediction accuracy. 


\begin{table*}[ht]
\begin{center}
\begin{tabular}{|p{2cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|}
\hline
RMSE$_{\text{Method}}$ \newline (mm) & Beak & Nose & Left Eye & Right Eye& Left Shoulder & Right Shoulder & Top Keel & Bottom Keel & Tail \\
\hline
RMSE$_{\text{Hybrid}}$ & 8.2&6.5&7.3&6.3&13.8&9.4&13.3&8.9&9.2\\ 
RMSE$_{\text{Linear}}$ & 66.3 & 64.8 & 62.5& 62.7 & 45.5 & 42.7 & 41.3 & 37.6 & 45.7\\
\hline
\end{tabular}
\end{center}
\caption{Root mean squared Euclidean error (mm) of different approaches used to fill artificially introduced gaps in a 5 min single pigeon sequence. Hybrid approach uses a 2D DLC model from each view and triangulated and the linear approach interpolates missing data linearly with data before and after a given gap.
}
\label{table:Hybrid}
\end{table*}



\section{Additional Files}
We have provided additional material along with this file. %The following text describes the nature of the additional material and files. 
\begin{itemize}
    \item Supplementary video : The video is designed to introduce the reader with 3D-POP dataset. It shows the setup, diversity of the dataset and the annotations on video images. 
    
    Click \href{https://youtu.be/er4u0WpRJeQ}{here} for youtube link of the video. 
    
%    \item Annotation.zip : The folder shows sample of annotation data obtained from a sample sequence of 5 pigeons from 1 camera view (Cam1). Video were not included due to size limitations. Please refer to README.txt for details on data structure and data conventions. The "SampleTrial\_3DKeypoint.csv" file contain 3D data of the keypoints and markers. "SampleTrial\_FinalFeature2D\_Cam1.csv" contains 2D coordinates of all keypoints and markers. "SampleTrial\_FinalFeatureBBox\_Cam1.csv" contains information on bounding box for each individual. "*.customfeatures" contains information on 3D keypoints for each bird in the head and backpack object coordinate system. Finally, we also provide intrinsic and extrinsic calibration information for the RGB camera saved in pickles.
  
    
    
    
    
   % xxx.txt contains information of 3D keypoint for each bird registered in the coordinate system of the body part. xxxx.cal are the calibration files of the video camera. abc.cal are the calibration files of the motion capture system. 
    
\end{itemize}


%Videos:
%Sample video for dataset
%{
%\bibliography{egbib}
%}

\bibliographystyle{plain} % We choose the "plain" reference style
\bibliography{egbib}

\end{document}
