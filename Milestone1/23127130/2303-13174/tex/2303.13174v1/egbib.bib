@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@article{anderson_toward_2014,
	title = {Toward a {Science} of {Computational} {Ethology}},
	volume = {84},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627314007934},
	doi = {10.1016/j.neuron.2014.09.005},
	abstract = {The new field of “Computational Ethology” is made possible by advances in technology, mathematics, and engineering that allow scientists to automate the measurement and the analysis of animal behavior. We explore the opportunities and long-term directions of research in this area.},
	language = {en},
	number = {1},
	urldate = {2022-05-24},
	journal = {Neuron},
	author = {Anderson, David J. and Perona, Pietro},
	month = oct,
	year = {2014},
	pages = {18--31},
	file = {Anderson_Perona_2014_Toward a Science of Computational Ethology.pdf:C\:\\Users\\alexh\\Zotero\\storage\\XIL55XV4\\Anderson_Perona_2014_Toward a Science of Computational Ethology.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\alexh\\Zotero\\storage\\CYX6DPU3\\S0896627314007934.html:text/html},
}


@misc{Yu_2021,
  doi = {10.48550/ARXIV.2108.12617},
  
  url = {https://arxiv.org/abs/2108.12617},
  
  author = {Yu, Hang and Xu, Yufei and Zhang, Jing and Zhao, Wei and Guan, Ziyu and Tao, Dacheng},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {AP-10K: A Benchmark for Animal Pose Estimation in the Wild},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


%inproceedings{ng_animal_2022,
%	title = {Animal {Kingdom}: {A} {Large} and {Diverse} {Dataset} for {Animal} {Behavior} {Understanding}},
%	shorttitle = {Animal {Kingdom}},
%	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Ng_Animal_Kingdom_A_Large_and_Diverse_Dataset_for_Animal_Behavior_CVPR_2022_paper.html},
%	language = {en},
%	urldate = {2022-10-10},
%	author = {Ng, Xun Long and Ong, Kian Eng and Zheng, Qichen and Ni, Yun and Yeo, Si Yong and Liu, Jun},
%	year = {2022},
%	pages = {19023--19034},
%	file = {Full Text PDF:C\:\\Users\\alexh\\Zotero\\storage\\RTDWWL8B\\Ng et al. - 2022 - Animal Kingdom A Large and Diverse Dataset for An.pdf:application/pdf;Snapshot:C\:\\Users\\alexh\\Zotero\\storage\\JNUUHHLB\\Ng_Animal_Kingdom_A_Large_and_Diverse_Dataset_for_Animal_Behavior_CVPR_2022_paper.html:text/html},
%}

@inproceedings{ng_animal_2022,
  title={Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understanding},
  author={Ng, Xun Long and Ong, Kian Eng and Zheng, Qichen and Ni, Yun and Yeo, Si Yong and Liu, Jun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={19023--19034},
  year={2022}
}

@article{mathis_deep_2020,
	series = {Neurobiology of {Behavior}},
	title = {Deep learning tools for the measurement of animal behavior in neuroscience},
	volume = {60},
	issn = {0959-4388},
	url = {https://www.sciencedirect.com/science/article/pii/S0959438819301151},
	doi = {10.1016/j.conb.2019.10.008},
	abstract = {Recent advances in computer vision have made accurate, fast and robust measurement of animal behavior a reality. In the past years powerful tools specifically designed to aid the measurement of behavior have come to fruition. Here we discuss how capturing the postures of animals—pose estimation - has been rapidly advancing with new deep learning methods. While challenges still remain, we envision that the fast-paced development of new deep learning tools will rapidly change the landscape of realizable real-world neuroscience.},
	language = {en},
	urldate = {2022-07-20},
	journal = {Current Opinion in Neurobiology},
	author = {Mathis, Mackenzie Weygandt and Mathis, Alexander},
	month = feb,
	year = {2020},
	pages = {1--11},
	file = {Mathis_Mathis_2020_Deep learning tools for the measurement of animal behavior in neuroscience.pdf:C\:\\Users\\alexh\\Zotero\\storage\\6J9BSLCZ\\Mathis_Mathis_2020_Deep learning tools for the measurement of animal behavior in neuroscience.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\alexh\\Zotero\\storage\\29V9HECU\\S0959438819301151.html:text/html},
}


@inproceedings{mathis_pretraining_2021,
	title = {Pretraining {Boosts} {Out}-of-{Domain} {Robustness} for {Pose} {Estimation}},
	url = {https://openaccess.thecvf.com/content/WACV2021/html/Mathis_Pretraining_Boosts_Out-of-Domain_Robustness_for_Pose_Estimation_WACV_2021_paper.html},
	language = {en},
	urldate = {2022-10-10},
	author = {Mathis, Alexander and Biasi, Thomas and Schneider, Steffen and Yuksekgonul, Mert and Rogers, Byron and Bethge, Matthias and Mathis, Mackenzie W.},
	year = {2021},
	pages = {1859--1868},
	file = {Full Text PDF:C\:\\Users\\alexh\\Zotero\\storage\\HTY9M3JL\\Mathis et al. - 2021 - Pretraining Boosts Out-of-Domain Robustness for Po.pdf:application/pdf;Snapshot:C\:\\Users\\alexh\\Zotero\\storage\\9VA2HQX5\\Mathis_Pretraining_Boosts_Out-of-Domain_Robustness_for_Pose_Estimation_WACV_2021_paper.html:text/html},
}


@article{lauer_multi-animal_2022,
	title = {Multi-animal pose estimation, identification and tracking with {DeepLabCut}},
	volume = {19},
	copyright = {2022 The Author(s)},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-022-01443-0},
	doi = {10.1038/s41592-022-01443-0},
	abstract = {Estimating the pose of multiple animals is a challenging computer vision problem: frequent interactions cause occlusions and complicate the association of detected keypoints to the correct individuals, as well as having highly similar looking animals that interact more closely than in typical multi-human scenarios. To take up this challenge, we build on DeepLabCut, an open-source pose estimation toolbox, and provide high-performance animal assembly and tracking—features required for multi-animal scenarios. Furthermore, we integrate the ability to predict an animal’s identity to assist tracking (in case of occlusions). We illustrate the power of this framework with four datasets varying in complexity, which we release to serve as a benchmark for future algorithm development.},
	language = {en},
	number = {4},
	urldate = {2022-05-19},
	journal = {Nature Methods},
	author = {Lauer, Jessy and Zhou, Mu and Ye, Shaokai and Menegas, William and Schneider, Steffen and Nath, Tanmay and Rahman, Mohammed Mostafizur and Di Santo, Valentina and Soberanes, Daniel and Feng, Guoping and Murthy, Venkatesh N. and Lauder, George and Dulac, Catherine and Mathis, Mackenzie Weygandt and Mathis, Alexander},
	month = apr,
	year = {2022},
	note = {Number: 4
Publisher: Nature Publishing Group},
	keywords = {Behavioural methods, Computational neuroscience, Machine learning, Zoology},
	pages = {496--504},
	file = {Lauer et al_2022_Multi-animal pose estimation, identification and tracking with DeepLabCut.pdf:C\:\\Users\\alexh\\Zotero\\storage\\PBT2PDR4\\Lauer et al_2022_Multi-animal pose estimation, identification and tracking with DeepLabCut.pdf:application/pdf;Snapshot:C\:\\Users\\alexh\\Zotero\\storage\\LQERHNFL\\s41592-022-01443-0.html:text/html},
}


@techreport{marshall_pair-r24m_2021,
	title = {The {PAIR}-{R24M} {Dataset} for {Multi}-animal {3D} {Pose} {Estimation}},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2021.11.23.469743v1},
	abstract = {Understanding the biological basis of social and collective behaviors in animals is a key goal of the life sciences, and may yield important insights for engineering intelligent multi-agent systems. A critical step in interrogating the mechanisms underlying social behaviors is a precise readout of the 3D pose of interacting animals. While approaches for multi-animal pose estimation are beginning to emerge, they remain challenging to compare due to the lack of standardized training and benchmark datasets. Here we introduce the PAIR-R24M (Paired Acquisition of Interacting oRganisms - Rat) dataset for multi-animal 3D pose estimation, which contains 24.3 million frames of RGB video and 3D ground-truth motion capture of dyadic interactions in laboratory rats. PAIR-R24M contains data from 18 distinct pairs of rats and 24 different viewpoints. We annotated the data with 11 behavioral labels and 3 interaction categories to facilitate benchmarking in rare but challenging behaviors. To establish a baseline for markerless multi-animal 3D pose estimation, we developed a multi-animal extension of DANNCE, a recently published network for 3D pose estimation in freely behaving laboratory animals. As the first large multi-animal 3D pose estimation dataset, PAIR-R24M will help advance 3D animal tracking approaches and aid in elucidating the neural basis of social behaviors.},
	language = {en},
	urldate = {2022-05-27},
	institution = {bioRxiv},
	author = {Marshall, Jesse D. and Klibaite, Ugne and Gellis, Amanda and Aldarondo, Diego E. and Ölveczky, Bence P. and Dunn, Timothy W.},
	month = nov,
	year = {2021},
	doi = {10.1101/2021.11.23.469743},
	note = {Section: New Results
Type: article},
	pages = {2021.11.23.469743},
	file = {Marshall et al_2021_The PAIR-R24M Dataset for Multi-animal 3D Pose Estimation.pdf:C\:\\Users\\alexh\\Zotero\\storage\\PXYQHFCW\\Marshall et al_2021_The PAIR-R24M Dataset for Multi-animal 3D Pose Estimation.pdf:application/pdf},
}


@article{yao_openmonkeychallenge_2022,
	title = {{OpenMonkeyChallenge}: {Dataset} and {Benchmark} {Challenges} for {Pose} {Estimation} of {Non}-human {Primates}},
	issn = {1573-1405},
	shorttitle = {{OpenMonkeyChallenge}},
	url = {https://doi.org/10.1007/s11263-022-01698-2},
	doi = {10.1007/s11263-022-01698-2},
	abstract = {The ability to automatically estimate the pose of non-human primates as they move through the world is important for several subfields in biology and biomedicine. Inspired by the recent success of computer vision models enabled by benchmark challenges (e.g., object detection), we propose a new benchmark challenge called OpenMonkeyChallenge that facilitates collective community efforts through an annual competition to build generalizable non-human primate pose estimation models. To host the benchmark challenge, we provide a new public dataset consisting of 111,529 annotated (17 body landmarks) photographs of non-human primates in naturalistic contexts obtained from various sources including the Internet, three National Primate Research Centers, and the Minnesota Zoo. Such annotated datasets will be used for the training and testing datasets to develop generalizable models with standardized evaluation metrics. We demonstrate the effectiveness of our dataset quantitatively by comparing it with existing datasets based on seven state-of-the-art pose estimation models.},
	language = {en},
	urldate = {2022-10-25},
	journal = {International Journal of Computer Vision},
	author = {Yao, Yuan and Bala, Praneet and Mohan, Abhiraj and Bliss-Moreau, Eliza and Coleman, Kristine and Freeman, Sienna M. and Machado, Christopher J. and Raper, Jessica and Zimmermann, Jan and Hayden, Benjamin Y. and Park, Hyun Soo},
	month = oct,
	year = {2022},
	keywords = {Behavioral tracking, Dataset and benchmark challenge, Deep learning, Non-human primates},
	file = {Full Text PDF:C\:\\Users\\alexh\\Zotero\\storage\\AAFQ2IFC\\Yao et al. - 2022 - OpenMonkeyChallenge Dataset and Benchmark Challen.pdf:application/pdf},
}


@article{jiang_animal_2022,
	title = {Animal pose estimation: {A} closer look at the state-of-the-art, existing gaps and opportunities},
	volume = {222},
	issn = {1077-3142},
	shorttitle = {Animal pose estimation},
	url = {https://www.sciencedirect.com/science/article/pii/S1077314222000893},
	doi = {10.1016/j.cviu.2022.103483},
	abstract = {Over the past few years, research on animal pose estimation in computer vision field has grown in many aspects such as 2D and 3D pose estimation, 3D mesh reconstruction, and behavior prediction. Promoted by deep learning, more and more animal pose estimation tools and animal pose datasets have also been made publicly available. However, compared to human pose estimation, which already has high accuracy and high applicability for complex scenes, animal pose estimation is still at a preliminary stage. The huge domain shift between each species, the scarce datasets, and uncooperative research subjects all pose intractable challenges to the development of robust and accurate animal pose estimation algorithms. In this review paper, we summarize the recent (from 2013 to 2021) work in animal pose estimation from computer vision perspective in order to present the state-of-the-art approaches and highlight the challenges they face in this field. We first categorize the various methods of animal pose estimation and present them according to several keywords. Also, we sort and introduce the released annotated image, video, and 3D models of animal poses as well as a promising substitute for real dataset. We also report the performances of the existing algorithms and visualize their results. Finally, we provide an in-depth analysis of the persisting obstacles in this field based on existing work, and offer potential solutions.},
	language = {en},
	urldate = {2022-10-09},
	journal = {Computer Vision and Image Understanding},
	author = {Jiang, Le and Lee, Caleb and Teotia, Divyang and Ostadabbas, Sarah},
	month = sep,
	year = {2022},
	keywords = {3D mesh reconstruction, Animal behavior, Animal pose estimation, Comparative analysis, Convolutional neural networks, Synthetic datasets},
	pages = {103483},
	file = {ScienceDirect Snapshot:C\:\\Users\\alexh\\Zotero\\storage\\TVK4ESPL\\S1077314222000893.html:text/html},
}

@article{mathis_deeplabcut_2018,
	title = {{DeepLabCut}: markerless pose estimation of user-defined body parts with deep learning},
	volume = {21},
	copyright = {2018 The Author(s)},
	issn = {1546-1726},
	shorttitle = {{DeepLabCut}},
	url = {https://www.nature.com/articles/s41593-018-0209-y},
	doi = {10.1038/s41593-018-0209-y},
	abstract = {Quantifying behavior is crucial for many applications in neuroscience. Videography provides easy methods for the observation and recording of animal behavior in diverse settings, yet extracting particular aspects of a behavior for further analysis can be highly time consuming. In motor control studies, humans or other animals are often marked with reflective markers to assist with computer-based tracking, but markers are intrusive, and the number and location of the markers must be determined a priori. Here we present an efficient method for markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results with minimal training data. We demonstrate the versatility of this framework by tracking various body parts in multiple species across a broad collection of behaviors. Remarkably, even when only a small number of frames are labeled ({\textasciitilde}200), the algorithm achieves excellent tracking performance on test frames that is comparable to human accuracy.},
	language = {en},
	number = {9},
	urldate = {2022-05-18},
	journal = {Nature Neuroscience},
	author = {Mathis, Alexander and Mamidanna, Pranav and Cury, Kevin M. and Abe, Taiga and Murthy, Venkatesh N. and Mathis, Mackenzie Weygandt and Bethge, Matthias},
	month = sep,
	year = {2018},
	note = {Number: 9
Publisher: Nature Publishing Group},
	keywords = {Behavioural methods, Computational neuroscience, Machine learning},
	pages = {1281--1289},
	file = {Mathis et al_2018_DeepLabCut.pdf:C\:\\Users\\alexh\\Zotero\\storage\\RL64T5KV\\Mathis et al_2018_DeepLabCut.pdf:application/pdf},
}

@article{graving_deepposekit_2019,
	title = {{DeepPoseKit}, a software toolkit for fast and robust animal pose estimation using deep learning},
	volume = {8},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.47994},
	doi = {10.7554/eLife.47994},
	abstract = {Quantitative behavioral measurements are important for answering questions across scientific disciplines—from neuroscience to ecology. State-of-the-art deep-learning methods offer major advances in data quality and detail by allowing researchers to automatically estimate locations of an animal’s body parts directly from images or videos. However, currently available animal pose estimation methods have limitations in speed and robustness. Here, we introduce a new easy-to-use software toolkit, DeepPoseKit, that addresses these problems using an efficient multi-scale deep-learning model, called Stacked DenseNet, and a fast GPU-based peak-detection algorithm for estimating keypoint locations with subpixel precision. These advances improve processing speed {\textgreater}2x with no loss in accuracy compared to currently available methods. We demonstrate the versatility of our methods with multiple challenging animal pose estimation tasks in laboratory and field settings—including groups of interacting individuals. Our work reduces barriers to using advanced tools for measuring behavior and has broad applicability across the behavioral sciences.},
	urldate = {2022-05-27},
	journal = {eLife},
	author = {Graving, Jacob M and Chae, Daniel and Naik, Hemal and Li, Liang and Koger, Benjamin and Costelloe, Blair R and Couzin, Iain D},
	editor = {Baldwin, Ian T and Shaevitz, Josh W and Shaevitz, Josh W and Stephens, Greg},
	month = oct,
	year = {2019},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {D. melanogaster, desert locust, Equus grevyi, Grévy's zebra, Schistocerca gregaria},
	pages = {e47994},
	file = {Graving et al_2019_DeepPoseKit, a software toolkit for fast and robust animal pose estimation.pdf:C\:\\Users\\alexh\\Zotero\\storage\\WPRWN4IL\\Graving et al_2019_DeepPoseKit, a software toolkit for fast and robust animal pose estimation.pdf:application/pdf},
}


@article{pereira_fast_2019,
	title = {Fast animal pose estimation using deep neural networks},
	volume = {16},
	copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-018-0234-5},
	doi = {10.1038/s41592-018-0234-5},
	abstract = {The need for automated and efficient systems for tracking full animal pose has increased with the complexity of behavioral data and analyses. Here we introduce LEAP (LEAP estimates animal pose), a deep-learning-based method for predicting the positions of animal body parts. This framework consists of a graphical interface for labeling of body parts and training the network. LEAP offers fast prediction on new data, and training with as few as 100 frames results in 95\% of peak performance. We validated LEAP using videos of freely behaving fruit flies and tracked 32 distinct points to describe the pose of the head, body, wings and legs, with an error rate of {\textless}3\% of body length. We recapitulated reported findings on insect gait dynamics and demonstrated LEAP’s applicability for unsupervised behavioral classification. Finally, we extended the method to more challenging imaging situations and videos of freely moving mice.},
	language = {en},
	number = {1},
	urldate = {2022-10-10},
	journal = {Nature Methods},
	author = {Pereira, Talmo D. and Aldarondo, Diego E. and Willmore, Lindsay and Kislin, Mikhail and Wang, Samuel S.-H. and Murthy, Mala and Shaevitz, Joshua W.},
	month = jan,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational neuroscience, Motor control},
	pages = {117--125},
	file = {Full Text PDF:C\:\\Users\\alexh\\Zotero\\storage\\7LNZN5CS\\Pereira et al. - 2019 - Fast animal pose estimation using deep neural netw.pdf:application/pdf},
}


@article{karashchuk_anipose_2021,
	title = {Anipose: {A} toolkit for robust markerless {3D} pose estimation},
	volume = {36},
	issn = {2211-1247},
	shorttitle = {Anipose},
	url = {https://www.sciencedirect.com/science/article/pii/S2211124721011797},
	doi = {10.1016/j.celrep.2021.109730},
	abstract = {Quantifying movement is critical for understanding animal behavior. Advances in computer vision now enable markerless tracking from 2D video, but most animals move in 3D. Here, we introduce Anipose, an open-source toolkit for robust markerless 3D pose estimation. Anipose is built on the 2D tracking method DeepLabCut, so users can expand their existing experimental setups to obtain accurate 3D tracking. It consists of four components: (1) a 3D calibration module, (2) filters to resolve 2D tracking errors, (3) a triangulation module that integrates temporal and spatial regularization, and (4) a pipeline to structure processing of large numbers of videos. We evaluate Anipose on a calibration board as well as mice, flies, and humans. By analyzing 3D leg kinematics tracked with Anipose, we identify a key role for joint rotation in motor control of fly walking. To help users get started with 3D tracking, we provide tutorials and documentation at http://anipose.org/.},
	language = {en},
	number = {13},
	urldate = {2022-05-27},
	journal = {Cell Reports},
	author = {Karashchuk, Pierre and Rupp, Katie L. and Dickinson, Evyn S. and Walling-Bell, Sarah and Sanders, Elischa and Azim, Eiman and Brunton, Bingni W. and Tuthill, John C.},
	month = sep,
	year = {2021},
	keywords = {deep learning, pose estimation, behavior, 3D, camera calibration, Drosophila joint rotation, markerless tracking, neuroscience, robust tracking, visualization},
	pages = {109730},
	file = {Karashchuk et al_2021_Anipose.pdf:C\:\\Users\\alexh\\Zotero\\storage\\493RHXI4\\Karashchuk et al_2021_Anipose.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\alexh\\Zotero\\storage\\D6N29LK3\\S2211124721011797.html:text/html},
}


@article{pereira_sleap_2022,
	title = {{SLEAP}: {A} deep learning system for multi-animal pose tracking},
	volume = {19},
	copyright = {2022 The Author(s)},
	issn = {1548-7105},
	shorttitle = {{SLEAP}},
	url = {https://www.nature.com/articles/s41592-022-01426-1},
	doi = {10.1038/s41592-022-01426-1},
	abstract = {The desire to understand how the brain generates and patterns behavior has driven rapid methodological innovation in tools to quantify natural animal behavior. While advances in deep learning and computer vision have enabled markerless pose estimation in individual animals, extending these to multiple animals presents unique challenges for studies of social behaviors or animals in their natural environments. Here we present Social LEAP Estimates Animal Poses (SLEAP), a machine learning system for multi-animal pose tracking. This system enables versatile workflows for data labeling, model training and inference on previously unseen data. SLEAP features an accessible graphical user interface, a standardized data model, a reproducible configuration system, over 30 model architectures, two approaches to part grouping and two approaches to identity tracking. We applied SLEAP to seven datasets across flies, bees, mice and gerbils to systematically evaluate each approach and architecture, and we compare it with other existing approaches. SLEAP achieves greater accuracy and speeds of more than 800 frames per second, with latencies of less than 3.5 ms at full 1,024 × 1,024 image resolution. This makes SLEAP usable for real-time applications, which we demonstrate by controlling the behavior of one animal on the basis of the tracking and detection of social interactions with another animal.},
	language = {en},
	number = {4},
	urldate = {2022-05-27},
	journal = {Nature Methods},
	author = {Pereira, Talmo D. and Tabris, Nathaniel and Matsliah, Arie and Turner, David M. and Li, Junyu and Ravindranath, Shruthi and Papadoyannis, Eleni S. and Normand, Edna and Deutsch, David S. and Wang, Z. Yan and McKenzie-Smith, Grace C. and Mitelut, Catalin C. and Castro, Marielisa Diez and D’Uva, John and Kislin, Mikhail and Sanes, Dan H. and Kocher, Sarah D. and Wang, Samuel S.-H. and Falkner, Annegret L. and Shaevitz, Joshua W. and Murthy, Mala},
	month = apr,
	year = {2022},
	note = {Number: 4
Publisher: Nature Publishing Group},
	keywords = {Computational neuroscience, Machine learning, Software},
	pages = {486--495},
	file = {Pereira et al_2022_SLEAP.pdf:C\:\\Users\\alexh\\Zotero\\storage\\V6MT32HC\\Pereira et al_2022_SLEAP.pdf:application/pdf;Snapshot:C\:\\Users\\alexh\\Zotero\\storage\\QV56BVE9\\s41592-022-01426-1.html:text/html},
}


@inproceedings{biggs_creatures_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Creatures {Great} and {SMAL}: {Recovering} the {Shape} and {Motion} of {Animals} from {Video}},
	isbn = {978-3-030-20873-8},
	shorttitle = {Creatures {Great} and {SMAL}},
	doi = {10.1007/978-3-030-20873-8_1},
	abstract = {We present a system to recover the 3D shape and motion of a wide variety of quadrupeds from video. The system comprises a machine learning front-end which predicts candidate 2D joint positions, a discrete optimization which finds kinematically plausible joint correspondences, and an energy minimization stage which fits a detailed 3D model to the image. In order to overcome the limited availability of motion capture training data from animals, and the difficulty of generating realistic synthetic training images, the system is designed to work on silhouette data. The joint candidate predictor is trained on synthetically generated silhouette images, and at test time, deep learning methods or standard video segmentation tools are used to extract silhouettes from real data. The system is tested on animal videos from several species, and shows accurate reconstructions of 3D shape and pose.},
	language = {en},
	booktitle = {Computer {Vision} – {ACCV} 2018},
	publisher = {Springer International Publishing},
	author = {Biggs, Benjamin and Roddick, Thomas and Fitzgibbon, Andrew and Cipolla, Roberto},
	editor = {Jawahar, C.V. and Li, Hongdong and Mori, Greg and Schindler, Konrad},
	year = {2019},
	pages = {3--19},
	file = {Full Text PDF:C\:\\Users\\alexh\\Zotero\\storage\\DLXVEUJR\\Biggs et al. - 2019 - Creatures Great and SMAL Recovering the Shape and.pdf:application/pdf},
}

@article{h36m_pami,
author = {Ionescu, Catalin and Papava, Dragos and Olaru, Vlad and Sminchisescu,  Cristian},
title = {Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
publisher = {IEEE Computer Society},
volume = {36},
number = {7},
pages = {1325-1339},
month = {jul},
year = {2014}
}


@misc{wah_caltech-ucsd_2011,
	type = {Report or {Paper}},
	title = {The {Caltech}-{UCSD} {Birds}-200-2011 {Dataset}},
	copyright = {other},
	url = {https://resolver.caltech.edu/CaltechAUTHORS:20111026-120541847},
	abstract = {CUB-200-2011 is an extended version of CUB-200 [7], a challenging dataset of 200 bird species. The extended version roughly doubles the number of images per category and adds new part localization annotations. All images are annotated with bounding boxes, part locations, and at- tribute labels. Images and annotations were filtered by mul- tiple users of Mechanical Turk. We introduce benchmarks and baseline experiments for multi-class categorization and part localization.},
	language = {en},
	urldate = {2022-10-26},
	author = {Wah, Catherine and Branson, Steve and Welinder, Peter and Perona, Pietro and Belongie, Serge},
	month = jul,
	year = {2011},
	note = {Issue: 2010-001
Num Pages: 8
Number: 2010-001
Place: Pasadena, CA
Publisher: California Institute of Technology},
	file = {Full Text PDF:C\:\\Users\\alexh\\Zotero\\storage\\SEZLDNXW\\Wah et al. - 2011 - The Caltech-UCSD Birds-200-2011 Dataset.pdf:application/pdf},
}


%incollection{badger_3d_2020,
%	title = {{3D} {Bird} {Reconstruction}: a {Dataset}, {Model}, and {Shape} {Recovery} from a {Single} {View}},
%	volume = {12363},
%	shorttitle = {{3D} {Bird} {Reconstruction}},
%	url = {http://arxiv.org/abs/2008.06133},
%	abstract = {Automated capture of animal pose is transforming how we study neuroscience and social behavior. Movements carry important social cues, but current methods are not able to robustly estimate pose and shape of animals, particularly for social animals such as birds, which are often occluded by each other and objects in the environment. To address this problem, we ﬁrst introduce a model and multi-view optimization approach, which we use to capture the unique shape and pose space displayed by live birds. We then introduce a pipeline and experiments for keypoint, mask, pose, and shape regression that recovers accurate avian postures from single views. Finally, we provide extensive multi-view keypoint and mask annotations collected from a group of 15 social birds housed together in an outdoor aviary. The project website with videos, results, code, mesh model, and the Penn Aviary Dataset can be found at https://marcbadger.github.io/avian-mesh.},
%	language = {en},
%	urldate = {2022-05-18},
%	author = {Badger, Marc and Wang, Yufu and Modh, Adarsh and Perkes, Ammon and Kolotouros, Nikos and Pfrommer, Bernd G. and Schmidt, Marc F. and Daniilidis, Kostas},
%	year = {2020},
%	doi = {10.1007/978-3-030-58523-5_1},
%	note = {arXiv:2008.06133 [cs]},
%	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.4.8},
%	pages = {1--17},
%	file = {Badger et al. - 2020 - 3D Bird Reconstruction a Dataset, Model, and Shap.pdf:C\:\\Users\\alexh\\Zotero\\storage\\X9IF4UQ6\\Badger et al. - 2020 - 3D Bird Reconstruction a Dataset, Model, and Shap.pdf:application/pdf},
%}

@inproceedings{badger_3d_2020,
  title={3D bird reconstruction: a dataset, model, and shape recovery from a single view},
  author={Badger, Marc and Wang, Yufu and Modh, Adarsh and Perkes, Ammon and Kolotouros, Nikos and Pfrommer, Bernd G and Schmidt, Marc F and Daniilidis, Kostas},
  booktitle={European Conference on Computer Vision},
  pages={1--17},
  year={2020},
  organization={Springer}
}

@article{gosztolai_liftpose3d_2021,
	title = {{LiftPose3D}, a deep learning-based approach for transforming two-dimensional to three-dimensional poses in laboratory animals},
	volume = {18},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-021-01226-z},
	doi = {10.1038/s41592-021-01226-z},
	abstract = {Markerless three-dimensional (3D) pose estimation has become an indispensable tool for kinematic studies of laboratory animals. Most current methods recover 3D poses by multi-view triangulation of deep network-based two-dimensional (2D) pose estimates. However, triangulation requires multiple synchronized cameras and elaborate calibration protocols that hinder its widespread adoption in laboratory studies. Here we describe LiftPose3D, a deep network-based method that overcomes these barriers by reconstructing 3D poses from a single 2D camera view. We illustrate LiftPose3D’s versatility by applying it to multiple experimental systems using flies, mice, rats and macaques, and in circumstances where 3D triangulation is impractical or impossible. Our framework achieves accurate lifting for stereotypical and nonstereotypical behaviors from different camera angles. Thus, LiftPose3D permits high-quality 3D pose estimation in the absence of complex camera arrays and tedious calibration procedures and despite occluded body parts in freely behaving animals.},
	language = {en},
	number = {8},
	urldate = {2022-10-09},
	journal = {Nature Methods},
	author = {Gosztolai, Adam and Günel, Semih and Lobato-Ríos, Victor and Pietro Abrate, Marco and Morales, Daniel and Rhodin, Helge and Fua, Pascal and Ramdya, Pavan},
	month = aug,
	year = {2021},
	note = {Number: 8
Publisher: Nature Publishing Group},
	keywords = {Computational neuroscience, Machine learning, Motor control},
	pages = {975--981},
	file = {Gosztolai et al_2021_LiftPose3D, a deep learning-based approach for transforming two-dimensional to.pdf:C\:\\Users\\alexh\\Zotero\\storage\\F37275NK\\Gosztolai et al_2021_LiftPose3D, a deep learning-based approach for transforming two-dimensional to.pdf:application/pdf;Snapshot:C\:\\Users\\alexh\\Zotero\\storage\\TE6LYFEI\\s41592-021-01226-z.html:text/html},
}

%misc{Rolnick_Machine_2017,
%  doi = {10.48550/ARXIV.1705.10694},
%  url = {https://arxiv.org/abs/1705.10694},
%  author = {Rolnick, David and Veit, Andreas and Belongie, Serge and Shavit, Nir},
%  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
%  title = {Deep Learning is Robust to Massive Label Noise},
%  publisher = {arXiv},
%  year = {2017},
%  copyright = {arXiv.org perpetual, non-exclusive license}
%}

@article{rolnick2017deep,
  title={Deep learning is robust to massive label noise},
  author={Rolnick, David and Veit, Andreas and Belongie, Serge and Shavit, Nir},
  journal={arXiv preprint arXiv:1705.10694},
  year={2017}
}

%article{itahara2022corvid,
%  title={" Corvid Tracking Studio": A custom-built motion capture system to track head movements of corvids.},
%  author={ITAHARA, AKIHIRO and KANO, FUMIHIRO},
%  journal={Japanese Journal of Animal Psychology},
%  pages={72--1},
%  year={2022},
%  publisher={THE JAPANESE SOCIETY FOR ANIMAL PSYCHOLOGY}
%}

@article{itahara2022corvid,
  title={"{Corvid} {Tracking} {Studio}": A custom-built motion capture system to track head movements of corvids.},
  author={Itahara, Akihiro and Kano, Fumihiro},
  journal={Japanese Journal of Animal Psychology},
  pages={72--1},
  year={2022},
  publisher={The Japanese Society for Animal Psychology}
}



@article{bala2020automated,
  title={Automated markerless pose estimation in freely moving macaques with OpenMonkeyStudio},
  author={Bala, Praneet C and Eisenreich, Benjamin R and Yoo, Seng Bum Michael and Hayden, Benjamin Y and Park, Hyun Soo and Zimmermann, Jan},
  journal={Nature communications},
  volume={11},
  number={1},
  pages={1--12},
  year={2020},
  publisher={Nature Publishing Group}
}

@inproceedings{joska2021acinoset,
  title={AcinoSet: a 3D pose estimation dataset and baseline models for Cheetahs in the wild},
  author={Joska, Daniel and Clark, Liam and Muramatsu, Naoya and Jericevich, Ricardo and Nicolls, Fred and Mathis, Alexander and Mathis, Mackenzie W and Patel, Amir},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={13901--13908},
  year={2021},
  organization={IEEE}
}

@article{gunel2019deepfly3d,
  title={DeepFly3D, a deep learning-based approach for 3D limb and appendage tracking in tethered, adult Drosophila},
  author={G{\"u}nel, Semih and Rhodin, Helge and Morales, Daniel and Campagnolo, Jo{\~a}o and Ramdya, Pavan and Fua, Pascal},
  journal={Elife},
  volume={8},
  pages={e48571},
  year={2019},
  publisher={eLife Sciences Publications Limited}
}


%article{rosner_percentage_1983,
%	title = {Percentage {Points} for a {Generalized} {ESD} {Many}-{Outlier} {Procedure}},
%	volume = {25},
%	issn = {0040-1706},
%	url = {https://www.tandfonline.com/doi/abs/10.1080/00401706.1983.10487848},
%	doi = {10.1080/00401706.1983.10487848},
%	abstract = {A generalized (extreme Studentized deviate) ESD many-outlier procedure is given for detecting from 1 to k outliers in a data set. This procedure has an advantage over the original ESD many-outlier procedure (Rosner 1975) in that it controls the type I error both under the hypothesis of no outliers and under the alternative hypotheses of 1, 2, …. k-l outliers. A method is given for approximating percentiles for this procedure based on the t distribution. This method is shown to be adequately accurate using Monte Carlo simulation, for detecting up to 10 outliers in samples as small as 25. Tables are given for implementing this method for n = 25(1)50(10)100(50)500; k = 10, α = .05, .Ol, .005.},
%	number = {2},
%	urldate = {2022-11-04},
%	journal = {Technometrics},
%	author = {Rosner, Bernard},
%	month = may,
%	year = {1983},
%	note = {Publisher: Taylor \& Francis
%\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00401706.1983.10487848},
%	keywords = {Detection of outliers, Extreme Studentized deviate, Multiple outliers, Outliers, t distribution},
%	pages = {165--172},
%}

@article{rosner_percentage_1983,
  title={Percentage points for a generalized {ESD} many-outlier procedure},
  author={Rosner, Bernard},
  journal={Technometrics},
  volume={25},
  number={2},
  pages={165--172},
  year={1983},
  publisher={Taylor \& Francis}
}

@article{dunn2021geometric,
  title={Geometric deep learning enables 3D kinematic profiling across species and environments},
  author={Dunn, Timothy W and Marshall, Jesse D and Severson, Kyle S and Aldarondo, Diego E and Hildebrand, David GC and Chettih, Selmaan N and Wang, William L and Gellis, Amanda J and Carlson, David E and Aronov, Dmitriy and others},
  journal={Nature methods},
  volume={18},
  number={5},
  pages={564--573},
  year={2021},
  publisher={Nature Publishing Group}
}

@inproceedings{kearney2020rgbd,
  title={Rgbd-dog: Predicting canine pose from rgbd sensors},
  author={Kearney, Sinead and Li, Wenbin and Parsons, Martin and Kim, Kwang In and Cosker, Darren},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8336--8345},
  year={2020}
}

%article{h36m_pami,
%author = {Ionescu, Catalin and Papava, Dragos and Olaru, Vlad and Sminchisescu,  Cristian},
%title = {Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments},
%journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
%publisher = {IEEE Computer Society},
%volume = {36},
%number = {7},
%pages = {1325-1339},
%month = {jul},
%year = {2014}
%}

%article{kano2022birds,
%  title={What are birds looking at? A large-scale motion-capture system reveals the visual attention of freely-behaving pigeons},
%  author={Kano, Fumihiro and Naik, Hemal and Keskin, G{\"o}ksel and Couzin, Iain and Nagy, Mate},
%  year={2022}
%}

@Article{kano2022birds,
author={Kano, Fumihiro
and Naik, Hemal
and Keskin, G{\"o}ksel
and Couzin, Iain D.
and Nagy, M{\'a}t{\'e}},
title={Head-tracking of freely-behaving pigeons in a motion-capture system reveals the selective use of visual field regions},
journal={Scientific Reports},
year={2022},
month={Nov},
day={09},
volume={12},
number={1},
pages={19113},
issn={2045-2322},
doi={10.1038/s41598-022-21931-9},
url={https://doi.org/10.1038/s41598-022-21931-9}
}




@article{tuia_perspectives_2022,
	title = {Perspectives in machine learning for wildlife conservation},
	volume = {13},
	copyright = {2022 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-27980-y},
	doi = {10.1038/s41467-022-27980-y},
	abstract = {Inexpensive and accessible sensors are accelerating data acquisition in animal ecology. These technologies hold great potential for large-scale ecological understanding, but are limited by current processing approaches which inefficiently distill data into relevant information. We argue that animal ecologists can capitalize on large datasets generated by modern sensors by combining machine learning approaches with domain knowledge. Incorporating machine learning into ecological workflows could improve inputs for ecological models and lead to integrated hybrid modeling tools. This approach will require close interdisciplinary collaboration to ensure the quality of novel approaches and train a new generation of data scientists in ecology and conservation.},
	language = {en},
	number = {1},
	urldate = {2022-11-04},
	journal = {Nature Communications},
	author = {Tuia, Devis and Kellenberger, Benjamin and Beery, Sara and Costelloe, Blair R. and Zuffi, Silvia and Risse, Benjamin and Mathis, Alexander and Mathis, Mackenzie W. and van Langevelde, Frank and Burghardt, Tilo and Kays, Roland and Klinck, Holger and Wikelski, Martin and Couzin, Iain D. and van Horn, Grant and Crofoot, Margaret C. and Stewart, Charles V. and Berger-Wolf, Tanya},
	month = feb,
	year = {2022},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computer science, Conservation biology},
	pages = {792},
	file = {Full Text PDF:C\:\\Users\\alexh\\Zotero\\storage\\W6TW6X82\\Tuia et al. - 2022 - Perspectives in machine learning for wildlife cons.pdf:application/pdf;Snapshot:C\:\\Users\\alexh\\Zotero\\storage\\NU5DKAG4\\s41467-022-27980-y.html:text/html},
}


@article{borowiec_deep_2022,
	title = {Deep learning as a tool for ecology and evolution},
	volume = {13},
	issn = {2041-210X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13901},
	doi = {10.1111/2041-210X.13901},
	abstract = {Deep learning is driving recent advances behind many everyday technologies, including speech and image recognition, natural language processing and autonomous driving. It is also gaining popularity in biology, where it has been used for automated species identification, environmental monitoring, ecological modelling, behavioural studies, DNA sequencing and population genetics and phylogenetics, among other applications. Deep learning relies on artificial neural networks for predictive modelling and excels at recognizing complex patterns. In this review we synthesize 818 studies using deep learning in the context of ecology and evolution to give a discipline-wide perspective necessary to promote a rethinking of inference approaches in the field. We provide an introduction to machine learning and contrast it with mechanistic inference, followed by a gentle primer on deep learning. We review the applications of deep learning in ecology and evolution and discuss its limitations and efforts to overcome them. We also provide a practical primer for biologists interested in including deep learning in their toolkit and identify its possible future applications. We find that deep learning is being rapidly adopted in ecology and evolution, with 589 studies (64\%) published since the beginning of 2019. Most use convolutional neural networks (496 studies) and supervised learning for image identification but also for tasks using molecular data, sounds, environmental data or video as input. More sophisticated uses of deep learning in biology are also beginning to appear. Operating within the machine learning paradigm, deep learning can be viewed as an alternative to mechanistic modelling. It has desirable properties of good performance and scaling with increasing complexity, while posing unique challenges such as sensitivity to bias in input data. We expect that rapid adoption of deep learning in ecology and evolution will continue, especially in automation of biodiversity monitoring and discovery and inference from genetic data. Increased use of unsupervised learning for discovery and visualization of clusters and gaps, simplification of multi-step analysis pipelines, and integration of machine learning into graduate and postgraduate training are all likely in the near future.},
	language = {en},
	number = {8},
	urldate = {2022-11-04},
	journal = {Methods in Ecology and Evolution},
	author = {Borowiec, Marek L. and Dikow, Rebecca B. and Frandsen, Paul B. and McKeeken, Alexander and Valentini, Gabriele and White, Alexander E.},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13901},
	keywords = {artificial intelligence, automation, computer vision, machine learning, modelling, neural networks, statistics},
	pages = {1640--1660},
	file = {Full Text PDF:C\:\\Users\\alexh\\Zotero\\storage\\IHQG5G23\\Borowiec et al. - 2022 - Deep learning as a tool for ecology and evolution.pdf:application/pdf;Snapshot:C\:\\Users\\alexh\\Zotero\\storage\\UTEGYW4P\\2041-210X.html:text/html},
}


@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	month = jun,
	year = {2009},
	note = {ISSN: 1063-6919},
	keywords = {Explosions, Image databases, Image retrieval, Information retrieval, Internet, Large-scale systems, Multimedia databases, Ontologies, Robustness, Spine},
	pages = {248--255},
}



@inproceedings{lin_microsoft_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	isbn = {978-3-319-10602-1},
	shorttitle = {Microsoft {COCO}},
	doi = {10.1007/978-3-319-10602-1_48},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollár, Piotr and Zitnick, C. Lawrence},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	keywords = {Common Object, Object Category, Object Detection, Object Instance, Scene Understanding},
	pages = {740--755},
	file = {Full Text PDF:C\:\\Users\\alexh\\Zotero\\storage\\XECVMMJC\\Lin et al. - 2014 - Microsoft COCO Common Objects in Context.pdf:application/pdf},
}

@article{walter2021trex,
  title={TRex, a fast multi-animal tracking system with markerless identification, and 2D estimation of posture and visual fields},
  author={Walter, Tristan and Couzin, Iain D},
  journal={Elife},
  volume={10},
  pages={e64000},
  year={2021},
  publisher={eLife Sciences Publications Limited}
}

@article{bozek2021markerless,
  title={Markerless tracking of an entire honey bee colony},
  author={Bozek, Katarzyna and Hebert, Laetitia and Portugal, Yoann and Mikheyev, Alexander S and Stephens, Greg J},
  journal={Nature communications},
  volume={12},
  number={1},
  pages={1--13},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{ferreira2020deep,
  title={Deep learning-based methods for individual recognition in small birds},
  author={Ferreira, Andr{\'e} C and Silva, Liliana R and Renna, Francesco and Brandl, Hanja B and Renoult, Julien P and Farine, Damien R and Covas, Rita and Doutrelant, Claire},
  journal={Methods in Ecology and Evolution},
  volume={11},
  number={9},
  pages={1072--1085},
  year={2020},
  publisher={Wiley Online Library}
}

@article{nagy2013context,
  title={Context-dependent hierarchies in pigeons},
  author={Nagy, M{\'a}t{\'e} and V{\'a}s{\'a}rhelyi, G{\'a}bor and Pettit, Benjamin and Roberts-Mariani, Isabella and Vicsek, Tam{\'a}s and Biro, Dora},
  journal={Proceedings of the National Academy of Sciences},
  volume={110},
  number={32},
  pages={13049--13054},
  year={2013},
  publisher={National Acad Sciences}
}

@article{li2019atrw,
  title={ATRW: a benchmark for Amur tiger re-identification in the wild},
  author={Li, Shuyuan and Li, Jianguo and Tang, Hanlin and Qian, Rui and Lin, Weiyao},
  journal={arXiv preprint arXiv:1906.05586},
  year={2019}
}

@article{swanson2015snapshot,
  title={Snapshot Serengeti, high-frequency annotated camera trap images of 40 mammalian species in an African savanna},
  author={Swanson, Alexandra and Kosmala, Margaret and Lintott, Chris and Simpson, Robert and Smith, Arfon and Packer, Craig},
  journal={Scientific data},
  volume={2},
  number={1},
  pages={1--14},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{gagne2021florida,
  title={Florida wildlife camera trap dataset},
  author={Gagne, Crystal and Kini, Jyoti and Smith, Daniel and Shah, Mubarak},
  journal={arXiv preprint arXiv:2106.12628},
  year={2021}
}

@inproceedings{van2018inaturalist,
  title={The inaturalist species classification and detection dataset},
  author={Van Horn, Grant and Mac Aodha, Oisin and Song, Yang and Cui, Yin and Sun, Chen and Shepard, Alex and Adam, Hartwig and Perona, Pietro and Belongie, Serge},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={8769--8778},
  year={2018}
}


@article{labuguen2021macaquepose,
  title={MacaquePose: A novel “in the wild” macaque monkey pose dataset for markerless motion capture},
  author={Labuguen, Rollyn and Matsumoto, Jumpei and Negrete, Salvador Blanco and Nishimaru, Hiroshi and Nishijo, Hisao and Takada, Masahiko and Go, Yasuhiro and Inoue, Ken-ichi and Shibata, Tomohiro},
  journal={Frontiers in behavioral neuroscience},
  volume={14},
  pages={581154},
  year={2021},
  publisher={Frontiers Media SA}
}


@article{theunissen_head_2017,
	title = {Head {Stabilization} in the {Pigeon}: {Role} of {Vision} to {Correct} for {Translational} and {Rotational} {Disturbances}},
	volume = {11},
	issn = {1662-453X},
	shorttitle = {Head {Stabilization} in the {Pigeon}},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2017.00551},
	abstract = {Stabilization of the head in animals with limited capacity to move their eyes is key to maintain a stable image on the retina. In many birds, including pigeons, a prominent example for the important role of head stabilization is the characteristic head-bobbing behavior observed during walking. Multimodal sensory feedback from the eyes, the vestibular system and proprioceptors in body and neck is required to control head stabilization. Here, we trained unrestrained pigeons (Columba livia) to stand on a perch that was sinusoidally moved with a motion platform along all three translational and three rotational degrees of freedom. We varied the frequency of the perturbation and we recorded the pigeons' responses under both light and dark conditions. Head, body, and platform movements were assessed with a high-speed motion capture system and the data were used to compute gain and phase of head and body movements in response to the perturbations. Comparing responses under dark and light conditions, we estimated the contribution of visual feedback to the control of the head. Our results show that the head followed the movement of the motion platform to a large extent during translations, but it was almost perfectly stabilized against rotations. Visual feedback only improved head stabilization during translations but not during rotations. The body compensated rotations around the forward-backward and the lateral axis, but did not contribute to head stabilization during translations and rotations around the vertical axis. From the results, we conclude that head stabilization in response to translations and rotations depends on different sensory feedback and that visual feedback plays only a limited role for head stabilization during standing.},
	urldate = {2022-11-09},
	journal = {Frontiers in Neuroscience},
	author = {Theunissen, Leslie M. and Troje, Nikolaus F.},
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\alexh\\Zotero\\storage\\SD6MQ9AW\\Theunissen and Troje - 2017 - Head Stabilization in the Pigeon Role of Vision t.pdf:application/pdf},
}


@article{janisch_deciphering_2021,
	title = {Deciphering choreographies of elaborate courtship displays of golden-collared manakins using markerless motion capture},
	volume = {127},
	issn = {1439-0310},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/eth.13161},
	doi = {10.1111/eth.13161},
	abstract = {Courtship displays are complex behaviours that evolve mainly through sexual selection. Males of golden-collared manakins (Manacus vitellinus) gather in leks and perform very elaborate courtship displays in forest courts to attract females. The rapid movements of the display, that involve acrobatic jumps between saplings, are challenging to record and investigate. Here we describe the use of a combination of tools to quantify the choreographies of manakin displays and reveal previously unknown aspects of the courtship. To test the prediction that aspects of male jump trajectories vary among males and may be subject to female choice, we evaluated whether parameters including take-off angle and velocity vary between individual males and displays. We used a custom-built synchronized camera system to record courtship displays in the field, under highly variable lighting conditions. We then used automatic image pattern recognition software to track the movements of the birds and extract three-dimensional (3D) coordinates of the birds' movements. After post-processing and cleaning of the raw 3D data, we compared automated and manually produced annotations to test the reliability of the 3D tracking methods. A subsequent analysis of tracked movements revealed that individual males performed their displays consistently across different recordings. We found that males express extreme values of force when they jump off saplings and the jump trajectory has a ballistic shape, confirming that no additional propulsion is provided by the wings. We also applied the analysis approach to non-jumping birds and found that they move at greater heights than jumping males. The acquired knowledge and the developed methods will allow us to compare different males in relation to courtship success in order to understand the role of choreographies in mate choice.},
	language = {en},
	number = {7},
	urldate = {2022-10-14},
	journal = {Ethology},
	author = {Janisch, Judith and Perinot, Elisa and Fusani, Leonida and Quigley, Cliodhna},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/eth.13161},
	keywords = {3D motion capture, automated animal tracking, ballistic motion, courtship behaviour, movement reconstruction, sexual selection},
	pages = {550--562},
	file = {Full Text PDF:C\:\\Users\\alexh\\Zotero\\storage\\S9DU3FRA\\Janisch et al. - 2021 - Deciphering choreographies of elaborate courtship .pdf:application/pdf;Snapshot:C\:\\Users\\alexh\\Zotero\\storage\\LL5RGADZ\\eth.html:text/html},
}

@article{stowers2017virtual,
  title={Virtual reality for freely moving animals},
  author={Stowers, John R and Hofbauer, Maximilian and Bastien, Renaud and Griessner, Johannes and Higgins, Peter and Farooqui, Sarfarazhussain and Fischer, Ruth M and Nowikovsky, Karin and Haubensak, Wulf and Couzin, Iain D and others},
  journal={Nature methods},
  volume={14},
  number={10},
  pages={995--1002},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{nourizonoz2020etholoop,
  title={EthoLoop: automated closed-loop neuroethology in naturalistic environments},
  author={Nourizonoz, Ali and Zimmermann, Robert and Ho, Chun Lum Andy and Pellat, Sebastien and Ormen, Yannick and Pr{\'e}vost-Soli{\'e}, Cl{\'e}ment and Reymond, Gilles and Pifferi, Fabien and Aujard, Fabienne and Herrel, Anthony and others},
  journal={Nature methods},
  volume={17},
  number={10},
  pages={1052--1059},
  year={2020},
  publisher={Nature Publishing Group}
}

%article{swanson2015snapshot,
%  title={Snapshot Serengeti, high-frequency annotated camera trap images of 40 mammalian species in an African savanna},
%  author={Swanson, Alexandra and Kosmala, Margaret and Lintott, Chris and Simpson, Robert and Smith, Arfon and Packer, Craig},
%  journal={Scientific data},
%  volume={2},
%  number={1},
%  pages={1--14},
%  year={2015},
%  publisher={Nature Publishing Group}
%}

@article{naik2019animals,
  title={Animals in virtual environments},
  author={Naik, Hemal and Bastien, Renaud and Navab, Nassir and Couzin, Iain},
  journal={arXiv preprint arXiv:1912.12763},
  year={2019}
}

@article{jafferis2019untethered,
  title={Untethered flight of an insect-sized flapping-wing microscale aerial vehicle},
  author={Jafferis, Noah T and Helbling, E Farrell and Karpelson, Michael and Wood, Robert J},
  journal={Nature},
  volume={570},
  number={7762},
  pages={491--495},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{kleinheerenbrink2022optimization,
  title={Optimization of avian perching manoeuvres},
  author={KleinHeerenbrink, Marco and France, Lydia A and Brighton, Caroline H and Taylor, Graham K},
  journal={Nature},
  volume={607},
  number={7917},
  pages={91--96},
  year={2022},
  publisher={Nature Publishing Group}
}

@inproceedings{joo2015panoptic,
  title={Panoptic studio: A massively multiview system for social motion capture},
  author={Joo, Hanbyul and Liu, Hao and Tan, Lei and Gui, Lin and Nabbe, Bart and Matthews, Iain and Kanade, Takeo and Nobuhara, Shohei and Sheikh, Yaser},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={3334--3342},
  year={2015}
}

@article{bolanos2021three,
  title={A three-dimensional virtual mouse generates synthetic training data for behavioral analysis},
  author={Bola{\~n}os, Luis A and Xiao, Dongsheng and Ford, Nancy L and LeDue, Jeff M and Gupta, Pankaj K and Doebeli, Carlos and Hu, Hao and Rhodin, Helge and Murphy, Timothy H},
  journal={Nature methods},
  volume={18},
  number={4},
  pages={378--381},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{chard1938structure,
  title={The structure of the eye of the homing pigeon.},
  author={Chard, Ray D and Gundlach, Ralph H},
  journal={Journal of Comparative Psychology},
  volume={25},
  number={2},
  pages={249},
  year={1938},
  publisher={Williams \& Wilkins Company}
}

@inproceedings{waldmann2022muppet,
  title={I-MuPPET: Interactive multi-pigeon pose estimation and tracking},
  author={Waldmann, Urs and Naik, Hemal and M{\'a}t{\'e}, Nagy and Kano, Fumihiro and Couzin, Iain D and Deussen, Oliver and Goldl{\"u}cke, Bastian},
  booktitle={DAGM German Conference on Pattern Recognition},
  pages={513--528},
  year={2022},
  organization={Springer}
}

@inproceedings{zuffi20173d,
  title={3D menagerie: Modeling the 3D shape and pose of animals},
  author={Zuffi, Silvia and Kanazawa, Angjoo and Jacobs, David W and Black, Michael J},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6365--6373},
  year={2017}
}

@inproceedings{redmon2016you,
  title={You only look once: Unified, real-time object detection},
  author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={779--788},
  year={2016}
}

@article{couzin2022emerging,
  title={Emerging technologies for behavioral research in changing environments},
  author={Couzin, Iain D and Heins, Conor},
  journal={Trends in Ecology \& Evolution},
  year={2022},
  publisher={Elsevier}
}

@article{xiao2022multi,
  title={Multi-view Tracking, Re-ID, and Social Network Analysis of a Flock of Visually Similar Birds in an Outdoor Aviary},
  author={Xiao, Shiting and Wang, Yufu and Perkes, Ammon and Pfrommer, Bernd and Schmidt, Marc and Daniilidis, Kostas and Badger, Marc},
  journal={arXiv preprint arXiv:2212.00266},
  year={2022}
}

@article{whitehead1997analysing,
  title={Analysing animal social structure},
  author={Whitehead, Hal},
  journal={Animal behaviour},
  volume={53},
  number={5},
  pages={1053--1067},
  year={1997},
  publisher={Elsevier}
}

@article{alarcon2018automated,
  title={An automated barcode tracking system for behavioural studies in birds},
  author={Alarc{\'o}n-Nieto, Gustavo and Graving, Jacob M and Klarevas-Irby, James A and Maldonado-Chaparro, Adriana A and Mueller, Inge and Farine, Damien R},
  journal={Methods in Ecology and Evolution},
  volume={9},
  number={6},
  pages={1536--1547},
  year={2018},
  publisher={Wiley Online Library}
}
@article{williamson2021lightweight,
  title={A lightweight backpack harness for tracking hummingbirds},
  author={Williamson, Jessie L and Witt, Christopher C},
  journal={Journal of Avian Biology},
  volume={52},
  number={9},
  year={2021},
  publisher={Wiley Online Library}
}