In the supplementary material, the analysis of dense distinct queries (DDQ) to Deformable DETR~\cite{zhu2020deformable} is first sketched in Sec.~\ref{sec:deformable_detr}. The details about auxiliary loss are added in Sec.~\ref{sec:loss}.   Sec.~\ref{sec:ana_py} gives more detailed ablation studies and analysis of pyramid shuffle.  Sec.~\ref{sec:stage} gives more detailed ablation studies about the number of queries and refining stages in DDQ R-CNN.   Sec.~\ref{sec:heavy} show the details about DDQ R-CNN with encoder. Sec.~\ref{sec:imporovedeform} elaborate and improved Deformable DETR and discuss the difference with DINO. Sec.~\ref{sec:latency} gives the latency benchmark.
Sec.~\ref{sec.query_form} reports the results of DDQ R-CNN with other ways to construct the query. Sec.~\ref{sec:nms} show the results of traditional detectors with different IOU thresholds on CrowdHuman. At last, Sec.~\ref{sec:social} illustrates our social impact.

\section{Analysis of DDQ in Deformable DETR}
\label{sec:deformable_detr}
For fast verification of Distinct Queries Selection(DQS) in such a heavy model, we adopt the standard 1x setting on COCO and keep other hyperparameters (such as learning rate and weight decay) the same with that in Deformable DETR~\cite{zhu2020deformable}.
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\linewidth]{latex/resources/nms_deformdetrv2.pdf}
    \caption{The performance comparison of Deformable DETR with and without distinct queries selection}
    \label{fig:nms_deform_detr}
\end{figure}

As shown in  Fig.\ref{fig:nms_deform_detr},  when we increase the number of queries in Deformable DETR, there is a similar trend as it in Sparse R-CNN~\cite{sun2021sparse}, as shown in the main manuscript. When the number of queries naively increases without distinct queries selection, the performance increases at the beginning but decreases as the number of queries reaches $\sim5000$. It is due to the more difficult training with more similar queries out of the dense queries. By imposing a distinct queries selection pre-processing to filter out similar queries and keeping only distinct queries before each stage of iterative refinement, the performance is improved with a clear margin, and the performance margin consistently increases along with more queries.

Therefore, we believe Dense Distinct Queries (DDQ) is a principle of designing an object detector with a fast convergence based on recent end-to-end detectors. 



\section{Auxiliary Loss for Dense Queries}
\label{sec:loss}
We follow the  TOOD~\cite{feng2021tood}  to design our auxiliary loss. We select $K$ samples with the smallest cost of each ground truth as positive samples. $P$ means the index set of positive samples which correspond to the same ground truth. The classification score target of a sample $i$ in this set
is 
\begin{equation}
 \frac{ score_i * IoU_i ^{6}} {Max(score_j * IoU_j ^{6})_{j\in{P}}} * Max(IoU_j)_{j\in{P}}
\end{equation}


The GIoU loss of each sample is reweighted by the classification target. The classification loss and regression loss weight keep consistent with the main loss weight (1 and 2 respectively) for distinct queries. The performance is quite stable for DDQ FCN when $K$ ranges between 5 and 16. We adopt 8 and 4 for DDQ FCN and DDQ DETR respectively in this study. The auxiliary loss also works for refining heads in DDQ RCNN. Due to time issues, we will supplement relevant results in the future version.


\section{More Analysis and Ablation for Pyramid Shuffle}
\label{sec:ana_py}
We provide an in-depth analysis of pyramid shuffle. Firstly, we compare it with 3D MAX Filter in DeFCN~\cite{wang2021end} in Table.~\ref{tab:3d}. Then we visualize the change of score maps in different levels after adding pyramid shuffle operations in Fig.~\ref{fig:vis}. At last, Table.~\ref{tab:pyshuffle} gives the results under different shuffle channels.


 \begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{latex/resources/vis.pdf}
      \caption{
    \textbf{Visualization of score map of adjacent levels}. We visualize classification scores with the rainbow color system.  The left side of each subfigure(with blue background) shows score maps with pyramid shuffles, and the corresponding right side (with yellow background) means without pyramid shuffles. The top-left corner marks the feature level. The red circle represents the duplication predictions in the adjacent level.}
    
    \label{fig:vis}
\end{figure*}


\begin{table*}[!h]
\vspace{-1mm}
    
    \centering
        \caption{Latency(ms) of different models with batch size 1}
        \vspace{-1mm}
        
    \begin{tabular}{c|c|c|c|c|c|c}
    \hline
      \ours{\textbf{ DDQ FCN}} & Cascade R-CNN  & Sparse R-CNN  & Deformable DETR  & \ours{\textbf{DDQ R-CNN} } & DINO & \ours{\textbf{DDQ DETR}}\\
                \hline
 \ours{~\textbf{44.8}} AP & 44.3 AP &  45.0 AP & 46.2  AP & \ours{ \textbf{48.1} AP } & {50.9} AP & \ours{ \textbf{52.0} AP }  \\
        \hline
          \ours{~\textbf{22.4} ms} &   28.5 ms   & 31.0 ms & 40.0 ms & \ours{\textbf{ 31.3}} ms  & 46 ms & \ours{\textbf{ 58}} ms

\label{tab:latency}
    \end{tabular}
    \vspace{-3mm}
\end{table*}



\begin{table}[!h]
    \begin{center}
\caption{\textbf{Comparison between 3D MAX Filter and Pyramid Shuffle.} * means the results is unstable}
    \label{tab:3d}
\scalebox{0.9}{ \begin{tabular}{l|c|c|c}
    \hline
        Flops & Parameters & Operations  & AP   \\
        
        \hline
         - & - & - & 41.0*   \\ 
        12.2 G & 0.59M & Conv\&GN\&Relu\&MaxPool3d & 41.2  \\ 
      \ours{0.2 G} & \ours{0.00M} & \ours{Shuffle x3}  & \ours{\textbf{41.5}} \\
        \ours{12.2 G} & \ours{0.59M} &  \ours{Conv\&GN\&Relu\& Shuffle x3}  & \ours{\textbf{42.0}} \\
         
    \end{tabular}}
    \end{center}

\vspace{-6mm}
\end{table}





\noindent\textbf{Comparision with 3D MAX Filter} Table.~\ref{tab:3d} shows the comparison between pyramid shuffle and 3D MAX Filtering in DeFCN. DeFCN believes there should be extra parameters and max pooling operation to facilitate the optimization under the one-to-one assignment. However, we argue that only the interaction of cross-level queries matters, and extra parameters or max operations are unnecessary. Our pyramid shuffle is more lightweight and with better performance. When we add an extra convolution to regression branches to fair compare with the 3D MAX Filter, we can surpass it by 0.8 AP.

\noindent\textbf{Visualization of Adjacent Level Score Map} Fig.~\ref{fig:vis} shows the scores map of adjacent levels. The left side of each subfigure(with blue background) shows score maps with pyramid shuffles, and the corresponding right side (with yellow background) means without pyramid shuffles. The top-left corner marks the feature level. The red circle represents the duplication predictions in the adjacent level. We can find pyramid shuffle effectively reduces the cross-level high score false positives.



\noindent\textbf{Results under Different Shuffle Channels} Table~\ref{tab:pyshuffle} gives the results under different shuffle channels; we can find that the number of channels can even be reduced to 16 when there is already cross-level distinct queries selection, making it more lightweight. When the number of shuffle channels is 128, which means no remaining channels for the current level, the performance will dramatically drop 1.5 AP because of missing information on the current level queries in the interaction.

\begin{table}[!h]
    
    \begin{center}
\caption{Performance of one-stage DDQ with different numbers of shuffle channels.}
    \label{tab:pyshuffle}
    

    \begin{tabular}{l|c|c|c|c}
    \hline
        Number   & AP  & AP$_{50}$ & AP$_{75}$ \\
        \hline
        0 &  41.0* & 59.9 & 45.1   \\
        8   & 41.2 & 60.3 & 45.4    \\
        16   & 41.3 & 60.6 & 45.4    \\
        32    & 41.4 & 60.6  & 45.5    \\
        \ours{64}    & \ours{\textbf{41.5}}  & \ours{60.9} & \ours{45.4}  \\
        96    &  41.6 & 61.1 & 45.7   \\
       128    & 39.9  & 59.9 & 43.6   \\
 
    \end{tabular}
    \end{center}

\vspace{-6mm}
\end{table}


\subsection{Number of Pyramid Shuffle Operations}
We report the results of DDQ FCN with the different number of pyramid shuffle operations in the classification and regression branches. When no pyramid shuffle is adopted in the DDQ FCN, its performance is unstable and fluctuates between 40.8 AP and 41.1 AP. We report an average performance of 41.0 AP. Even though there has been a cross-level distinct queries selection operation, compared to adopting only 2 and 1  operations to two branches respectively, there is still a 0.5 AP drop.

\begin{table}[!h]
    
    \begin{center}
    \caption{ Different number Pyramid shuffle operations in DDQ FCN. Cls means the classification branch and Reg means the regression branch. * indicate the performance is unstable}
    \label{tab:pyshuffle}
    
     \scalebox{0.90}{
    \begin{tabular}{l|c|c|c|c}
    \hline
        Cls & Reg   & AP  & AP$_{50}$ & AP$_{75}$ \\
        \hline
        0  & 0 &  41.0* & 59.9 & 45.1   \\
        1  & 0   & 41.3 & 60.1 & 45.6    \\
        0  & 1   & 41.3 & 60.6 & 45.4    \\
        0  & 2 & 41.3  & 60.0 & 45.6   \\
        2  & 0 & 41.2  & 60.1 &  45.5  \\
        1  & 1    & 41.3 & 60.6  & 45.8    \\
        \ours{2}  & \ours{1}    & \ours{\textbf{41.5}}  & \ours{60.9} & \ours{45.4}  \\
        2  & 2    &  41.4 & 60.6 & 45.5   \\
       4  & 4    & 41.5  & 61.1 & 45.6   \\
 
    \end{tabular}}
    \end{center}

\vspace{-6mm}
\end{table}

\vspace{-2mm}






\section{DDQ R-CNN with Encoder}
\label{sec:heavy}
The encoder can provide a more powerful feature representation for the decoder head which is explored in~\cite{zhu2020deformable,dai2021dynamicdetr, Wang_2020_CVPR}. We simply add 6 dynamic blocks in DyHead~\cite{dai2021dynamichead} as our encoder. 500 queries and 3 refinement stages are adopted. It is observed in the main manuscript that there is about 3 AP improvement for DDQ R-CNN.

\section{Details of Improved DeformableDETR and Comparison with DINO}
\label{sec:imporovedeform}
DINO~\cite{zhang2022dino} adopt some techniques that significantly improve the Deformable DETR. We remove the CDN and mix query selection from DINO to form our baseline. DDQ is a concurrent work of DINO. The contrastive denoising training (CDN) is not
intended to relieve the optimization difficulty of very similar
queries among dense queries. In their implementation, the
generated positive and negative samples in each pair are
always significantly distinct from each other. Mix query
selection increases the distinctness of queries by additionally initializing content embeddings, but the position embeddings are still created from top-k dense regression predictions which can be very similar and still hinder the
optimization. We have shown our components can be combined with DINO.





\section{Number of Queries and Stages in DDQ R-CNN }
\label{sec:stage}
\begin{table}[!h]
    \begin{center}
     \caption{Performance with different number of refine stages(S) and queries(Q).}
    \label{tab:queryandstage}
     \scalebox{0.90}{
    \begin{tabular}{l|c|c|c|c}
    \hline
          & 100 Q   & 200 Q & 300 Q & 400 Q \\
      \hline
        S=1  & 43.0  &  43.2  & 43.4 & 43.2   \\
      
        S=2  & 44.2  & 44.2  & \ours{\textbf{44.6} }    & 44.8    \\
   
        S=3  & 44.4  &  44.8 & 45.1  &  44.9       \\
 
        S=4  & 44.5   & 45.0  & 45.0 &    45.2   \\

    \end{tabular}}
    
    \vspace{-7mm}
    \end{center}
\end{table}
We analyze the combination of different numbers of stages and queries for DDQ R-CNN. Table.~\ref{tab:queryandstage} shows that the best number of stages is proportional to the number of queries. This is easy to understand. When the number of queries increases, the newly added queries are of low quality, and more stages are needed to refine these queries. It is worth emphasizing that we use 2 stages and 300 queries to trade off the performance and latency. When using 3 stages with the same number of queries, our method achieves an even higher performance of 45.1 AP on MS COCO.




\section{Latency Benchmark}
\label{sec:latency}
As for the details of latency calibration, We compare the speed (forwarding + post-processing) of  different methods  with batch size 1 in Table.~\ref{tab:latency}. All evaluations were performed on Tesla A100 GPU with Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz. The Pytorch version is 1.9.0 with CudaToolkit 11.1 and Cudnn 8.0.5. An average of 200 iterations during model inference is adopted as the latency reported in this study.

\section{Impact of Query Construction in DDQ R-CNN }
\label{sec.query_form}




We try five ways to construct queries in DDQ R-CNN. As shown in Table. \ref{tab:constrctquery}, None means all query embeddings are set to a zero tensor, and the refinement stages only get meaningful query bounding boxes. This attempt reduces the performance to 43.2 AP. Simply constructing queries from the FPN results in 1.0 AP degradation. Reg means only using the last feature map of the regression branch, which drops the performance by 0.6 AP.  Constructing queries from the last feature map in the classification branch can be an alternative as it can get a comparable performance(only 0.3 AP drop).

\vspace{-3mm}
\begin{table}[!h]
    \centering
    \caption{Impact of Query Construction in DDQ RCNN}
     \scalebox{0.90}{
    \begin{tabular}{l|c|c|c}
    \hline
            &AP  & AP$_{50}$ & AP$_{75}$   \\
        \hline
        None &  43.2 & 61.0 & 47.9   \\
        FPN & 43.6 & 61.7 & 48.0      \\
        Cls & 44.3 & 62.2 & 48.6 \\
        Reg & 44.0 &  62.5 & 48.3\\
        \ours{Cls\&Reg}  & \ours{\textbf{44.6}} & \ours{63.0} & \ours{48.8}
    \end{tabular}}
    \label{tab:constrctquery}
    \vspace{-3mm}
\end{table}





\section{DQS with Different IoU Threshold in CrowdHuman }
\label{sec:nms}
In this section, we show the robustness of distinct queries selection(DQS) with different IoU thresholds in CrowdHuman. We can find there is a clear performance bottleneck for traditional detector ATSS even with carefully adjusting the threshold of NMS.


\begin{table}[!h]

    \begin{center}
         \caption{Performance of DDQ on COCO  when DQS adopts different IoU thresholds. Results of ATSS adopting different IoU thresholds in post-processing are also reported. None means we remove DQS or post-processing from the inference pipeline.}
    \label{tab:distinct}
\scalebox{0.90}{
    \begin{tabular}{l|c|c|c|c|c|c}
     %\hspace{-7mm}
    \hline

          \hline
           CrowdHuman  & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 & None \\
           \hline
        %& 0.5 & 0.6 & 0.7 & 0.8 & 0.9 & 1.0\\
        DDQ FCN  &  88.0 & 91.8  & \ours{\textbf {92.7}}  & 92.8 & 92.3 & 91.7  \\
         DDQ RCNN   & 91.8  & 92.9 & \ours{\textbf {93.5}} & 93.3 & 93.2  & 93.2 \\
          ATSS  & 88.3 &\textbf{89.6} & 88.4 & 85.3 & 78.9 & 42.7


    \end{tabular}}

    \end{center}
\end{table}





\section{Social Impact}
\label{sec:social}
The potential social impact of this work inherits from object detection. Because human behaviors often cause crowded scenes, and DDQ achieves excellent performance in such scenes, it may be applied to some applications that violate human privacy, such as surveillance.

\clearpage
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{egbib}


