\section{Analysis of Sparse and Dense Queries}

Current end-to-end detectors use either dense or sparse queries, both of which are however problematic during training. 
Specifically, sparse queries suffer a low recall rate, and dense queries have issues in optimization. To illustrate this, we increase the number of queries from 10 to 7000 in Sparse R-CNN, and the performance is shown as the black line in Fig.~\ref{fig:nms}. The performance first keeps rising as the number of queries increases to around 2000, implying that the sparse queries ($\sim$ 300) in Sparse R-CNN are far from enough due to the low recall rate.  On the other hand, the performance finally plateaus and even decreases as queries number further increases. This phenomenon can be explained by the difficulty in distinguishing similar queries in end-to-end detectors with one-to-one assignment, especially when queries become denser. 

\begin{figure}[!h]
\centering
\includegraphics[width=0.75\linewidth]{latex/resources/nmsv2.pdf}

\caption{The performance comparison of Sparse R-CNN with and without Distinct Queries Selection (a class-agnostic NMS with a threshold 0.7 before each refining head). All models are trained using the standard 1x setting. The green dotted line represents the default number(300) of queries adopted in Sparse R-CNN. The subplot denotes the memory consumption per GPU as the number of queries increases in Sparse R-CNN. 
}
\vspace{-5mm}
\label{fig:nms}
\end{figure}
\label{sec.distincness}

To understand how similar queries would hinder optimization, we provide a simplified example where we assume there exist two identical queries. In this case, the one-to-one assignment assigns a foreground label to one of them but a background label to another. Without loss of generality, we adopt binary cross-entropy loss for classification. Therefore, the loss from these two queries becomes $L_1=-\log(p_1) - \log(1 - p_2)$, where $p_1$ and $p_2$ are the probability scores of the positive and negative query, respectively, and satisfy $p_1 = p_2 = p$ as they are identical queries. In contrast, the loss value when only one of the duplicated queries exists is $L_0=-\log(p)$. The ratio $\alpha$ of the gradient between the duplicate and non-duplicate query is.
\vspace{-2mm}
\begin{equation}
\alpha = \frac{\partial L_1}{ \partial p} / \frac{\partial L_0}{ \partial p}  = 1 - \frac{p}{1-p} \
\vspace{-1mm}
\end{equation}
It is obvious that the gradient is scaled down (\emph{i.e.}, $\alpha < 1$) at $0<p<0.5$ and may even cause negative training (\emph{i.e.}, $\alpha < 0$) at $p > 0.5$.

As shown in the toy example, duplicated queries reduce gradients and even cause negative training, which dramatically suppresses convergence. To avoid this issue, we impose a distinct queries selection operation before the one-to-one assignment process.  The distinct queries selection strategy is realized by a simple class-agnostic NMS. The filtered distinct queries are thus easier to optimize, and such an operation improves the performance by a clear margin, as seen from the red curve in Fig.~\ref{fig:nms}. More surprisingly, the performance margin consistently increases along with more queries. A similar trend is also observed for Deformable DETR, which can be found in the supplementary material. 

In other words, once we make sure the selected queries are distinct, the performance of Sparse R-CNN can be improved consistently with the increasing number of distinct queries. However, using a large number of distinct queries causes a significant memory footprint. For example, Sparse R-CNN requires around 45G memory per GPU with 7000 queries. To leverage the advantage of dense distinct queries(DDQ) with a reasonable computation cost, we give practical designs for all popular detector architectures (FCN, R-CNN, and DETRs).



\section{Method}

Dense distinct queries (DDQ) is our principle for designing an object detector and can be integrated into different architectures. We first briefly describe the design of DDQ followed by detailed descriptions for the three architectures: FCN, R-CNN, and DETRs. The overall pipeline is sketched in Fig.~\ref{fig:pipeline}. 

\subsection{Paradigm of DDQ} 
\label{sec:method}
\noindent\textbf{Dense Queries}. As shown in Fig.~\ref{fig:nms}, the memory cost soars for dense queries. The main reason for this is the heavy calculation for each query. Instead of adopting learnable positional embedding in DETR, DDQ directly takes the feature point on each feature map as densely distributed initial queries. The number of queries in the feature pyramid can easily surpass 10000 given an input resolution of 800x800. 
To discriminate dense queries with reasonable computation cost, a light-weighted convolutional/linear network serves as the first stage and processes all queries in a sliding window manner.\\

 \begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{latex/resources/ddq.pdf}
      \caption{
    \textbf{The pipeline of DDQ}. (a) shows the application of DDQ to an FCOS-like structure, which is a fully-convolutional network (FCN). It is thus dubbed \textbf{DDQ FCN}. The pyramid shuffle is applied to the last two and last convolution layers in the classification and regression branches, respectively. The class-agnostic NMS act as the distinct queries selection operation. At last, only distinct queries will be assigned labels before calculating loss. (b) shows the design of DDQ for R-CNN structures (DDQ R-CNN). The last feature maps of the classification and regression branches of DDQ FCN are concatenated and filtered as distinct queries. The distinct queries are then sent to the refining heads with their corresponding bounding boxes. (c) shows the design of DDQ for DETRs  (DDQ DETR). After distinct queries selection, the remaining feature embedding in the encoder is projected with a linear to the content part of distinct queries. Their corresponding bounding boxes will be mapped to the position embedding part. Both parts will be sent to 6 refine stages. In such a long refine architecture, DQS will be applied before each refine stage to ensure distinctness.}
    
    \label{fig:pipeline}
    \vspace{-3mm}
\end{figure*}


\noindent\textbf{Distinct Queries} Now that the importance of query distinctness for optimization has been revealed in Sec.~\ref{sec.distincness}, we would discuss in this section why a class-agnostic non-maximum suppression (NMS) can be used to select distinct queries and how it differs from the traditional NMS as post-processing in traditional detectors. Since each query represents a potential instance in an image, and an instance can be uniquely represented by its location in an image~\cite{wang2020solo}, it comes naturally to detect similar queries using the class-agnostic overlapping ratio between the bounding boxes predicted by queries. More specifically, we apply a class-agnostic NMS to select distinct queries for the following one-to-one assignment. The loss is thus only computed on the selected distinct queries. \emph{It should be noted that such an operation is adopted in both training and inference, instead of only in inference as an extra post-processing in traditional detectors. Therefore, such a pipeline still abides by the definition of end-to-end detectors.} Compared to the training-unaware NMS in traditional detectors, it is designed to relieve the burden of one-to-one assignment during training, and can thus be set with an aggressive IoU threshold (0.7 in DDQ FCN and DDQ R-CNN, 0.8 in DDQ DETR), which is robust even on CrowdHuman dataset ~\cite{shao2018crowdhuman}. Such crowd scenes can not be properly handled by NMS as post-processing in traditional detectors. We validate this in Table.~\ref{tab:crowdhuman}.\\

\noindent\textbf{Loss Components} \label{sec:mainloss} \\
\emph{(1). Main Loss for Dense Distinct Queries}. We simply apply the bipartite matching algorithm in DETR~\cite{carion2020end} with the same cost weight in the one-to-one assignment. No extra prior (such as center priors in ~\cite{wang2021end}) is adopted for a fair comparison with DETRs. After discriminating positive and negative samples, DDQ FCN adopts GIoU loss~\cite{rezatofighi2019generalized} and QFocal loss~\cite{li2021generalized} with weights 2 and 1. For DDQ R-CNN and DDQ DETR, we just follow the implementation of Sparse R-CNN~\cite{sun2021sparse} and DINO~\cite{zhang2022dino}.

\noindent\emph{(2). Auxiliary Loss for Dense Queries}. Despite the more efficient optimization in DDQ due to the removal of similar queries, it also results in numerous "leaf" queries through which no gradients are back-propagated. Therefore, we design an auxiliary head and an auxiliary loss to further harness the potential of the filtered queries following the design in DeFCN~\cite{wang2021end}. The auxiliary head is mostly identical to the main head, except that it adopts a soft one-to-many assignment for dense queries to allow for dense gradients and more positive samples to speed up training. More details can be found in our supplementary material.
\vspace{-1mm}
\subsection{DDQ FCN}   As shown in Fig.~\ref{fig:pipeline}. (a), the DDQ principle is first applied to FCOS as an example of the FCN structure for object detection. It is found that dense queries are already available on the dense feature pyramid. However, as the dense queries are processed level by level with convolutional layers. The missing interaction across different levels poses a challenge for the optimization of one-to-one assignments.

\begin{figure}[!h]
\centering
\includegraphics[width=0.75\linewidth]{latex/resources/pyshufflev4.pdf}
\vspace{-3mm}
\caption{\textbf{An illustration of pyramid shuffle.} For queries in a specific scale level,  it can do the interaction with queries in an adjacent level by shuffling $S$ channels. Before concatenating to the feature of the target level, the feature from other levels should be interplate to the same size as the target level.}
\label{fig:shuffle}
\vspace{-3mm}
\end{figure}

Inspired by channel shuffle operation in ShuffleNet~\cite{zhang2018shufflenet}, we propose a pyramid shuffle to compensate for the interaction between queries in different levels where $S$ channels across adjacent levels are shuffled to form a new feature pyramid. Specifically, features at level $i$ exchange $S$ channels with those at level $i-1$ and level $i+1$ simultaneously. To account for the different spatial dimensions on the feature pyramid, a bilinear interpolation is adopted when exchanging features. We apply the pyramid shuffle operation on the last two and one convolution layer in the classification and regression branches, respectively. This approach stabilizes training and improves performance with negligible additional computation costs. In this work, we set $S$ to 64 which means each feature level exchanges information from 128 channels with other levels. (Comparison with other approaches to model the interaction among dense queries, ablation, and the analysis of pyramid shuffle can be found in our supplementary material.) 

As for the distinct queries selection module in DDQ FCN, we first select the top 1000 predictions according to the classification score from each feature level and then apply a class-agnostic non-maximum suppression with a threshold of 0.7 to ensure both distinctness and generality across different datasets.

\subsection{DDQ R-CNN} We combine DDQ FCN with two refine stages in Sparse R-CNN to construct the DDQ R-CNN.  As shown in Fig.~\ref{fig:pipeline}. (b), thanks to the fast processing of dense distinct queries in DDQ FCN, we select 300 most representative queries according to the classification score from the remaining distinct queries. Then we concatenate the feature in the distinct position of the last feature map of the classification branch and regression branch to construct the query embedding. The query embedding and the corresponding bounding box prediction will be passed to the refinement head of Sparse R-CNN. Different from Sparse R-CNN which requires 6 stages of iterative query refinement, DDQ R-CNN needs as few as 2 refinement stages. Actually, the long iteration stages in Sparse R-CNN mainly compensate for the drawbacks caused by the sparse and sometimes similar input queries. For one thing, sparse queries could not cover all instances at initialization and thus need long cascading stages to refine. For another, similar queries also require long refinements to distinguish from each other to output a one-hot prediction for each instance ~\cite{carion2020end}. In contrast, the dense distinct queries from DDQ R-CNN have addressed the above issues, and hence the number of iterative refinements can be significantly reduced. We also report the results when we change the number of queries and refinement heads of DDQ R-CNN in the supplementary material. 

\subsection{DDQ DETR} We construct DDQ DETR based on Deformable DETR*~\footnote{* indicates it is an improved version based on techniques in DINO~\cite{zhang2022dino}. Details can be found in our supplementary material}. As shown in Fig.~\ref{fig:pipeline}. (c), We follow Two-Stage Deformable DETR~\cite{zhu2020deformable} to process dense queries. Instead of initializing the content part with transformed coordinates, we fuse the feature map embedding of distinct positions as the content part, which makes the initial queries more distinct. A class-agnostic NMS with a threshold of 0.8 is set to select distinct queries before each refining stage. To compare with recent DETRs, we keep the original 6 refining stages and select $K$ distinct queries for the refining stages. We also select the top 1.5$K$ queries directly according to classification scores as dense queries for the auxiliary head in the decoder. The parallel forward of dense queries and distinct queries follows the H-DeformableDETR~\cite{jia2022detrs} and Group DETR\cite{chen2022group}. We set $K$ to 900, following DINO~\cite{zhang2022dino}.