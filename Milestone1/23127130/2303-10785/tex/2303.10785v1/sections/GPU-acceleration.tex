\newpage
\section{GPU acceleration model and speedup analysis} \label{sec:gpu-accel}

This section discusses the GPU parallelization model employed in the present work to implement the MEG6 and MIG4 schemes along with the performance results achieved on various GPUs. In presenting the performance details, a particular emphasis was laid on subjects such as `the contribution of various functions to the overall computational time', `the effect of cell count', `memory occupancy versus cell count', and `the effect of working precision'.

In order to specify the parallelism on GPUs, the directive based `OpenACC' programming language \cite{openacc-web1} is used in the present work. The use of OpenACC directives over the more popular `CUDA' programming language is motivated by less code development time associated with OpenACC while still being able to achieve beneficial speedup results that are comparable to CUDA, as will be demonstrated soon. The details of OpenACC implementation are being skipped here for brevity. Interested readers are refered to the OpenACC API guide \cite{openacc-web2} and other resources available in their website \cite{openacc-web1}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{Images/gpu-strategy1.pdf}
    \caption{Schematic diagram of the parallelization model employed for single device GPU computations. Shown in the left of the figure, is an example multi-block computational domain (of an airfoil) whose geometrical and initial conditions data is distributed to the host memory. The three steps involved in the parallelization model are shown in blue circles with numbers.}
    \label{gpu-strategy1}
\end{figure}

A high-level view of the computing model adapted for single GPU computations is pictorially illustrated in Fig. \ref{gpu-strategy1}. It consists of a CPU-GPU pair connected via a PCI Express bus or an NVLink interconnect; the latter option which is an NVIDIA proprietary bus is designed to offer a higher bandwidth. The program is hosted by the CPU (or called `host'), which initializes the compute environment, reads the geometry, mesh, and other input variables. The compute routines of the program are off-loaded onto the GPU; GPU is refereed to as `device'. Fig. \ref{gpu-strategy1}, shows the three elemental steps involved in performing a single device GPU simulation. The first step before starting the main time loop involves creating a copy of all the necessary data, such as mesh, initial conditions, and the flow parameters on the device memory. Since scientific computing based programs such as present are mainly bandwidth bounded, this step is done as a one time act to restrict the communication between the host and device during the simulation. It is followed by executing all the routines that are part of the main time loop listed in Fig. \ref{algo} until the simulation reaches its end-time. Once the computations inside the main time loop are completed, the solution is transferred to the host memory (step-3 in Fig. \ref{gpu-strategy1}), from where it can be viewed and post-processed in the end. But often in many cases, the user requires to store the solution corresponding to intermediate time-steps for temporal analysis. In such a scenario, only the primitive variable data is transferred to the host whenever necessary. Such communication during the middle of a simulation, if carried out just once per hundred iterations or more, was noted to add only a negligible amount of computational time to the overall simulation. 

% Since the amount of memory is limited on the GPU, it consequently limits the size of the simulation that can be run on the GPU without memory over-subscription. The higher the GPU memory the larger grid one can accommodate on the GPU.

% In multi-GPU computations, the compute environment consists of multiple pairs of hosts and devices (CPUs and GPUs). A schematic of the decomposed computational domain and the parallelization model adapted in the present work for multi-GPU computations is shown in Fig. \ref{gpu-strategy1}b. Firstly, an MPI environment is initialized among the CPU cores, virtually creating an individual memory space for each processor. Then each CPU processor is assigned with a single GPU privately. The information corresponding to decomposed blocks, such as grid, initial conditions, boundary conditions, etc., are fed into the corresponding host memory spaces. The computations on each host and device pair in this multi-GPU setup are performed through a similar strategy described above for single GPU computations. This essentially means that the computations belonging to a device are considered independent of others and carried out as standalone individual simulations except for the communication required at the block boundaries.

% Firstly, as a pre-processing step, if the domain does not already have enough blocks to be distributed amongst the available GPUs, the computational domain is decomposed into multiple blocks.

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=150mm]{Images/gpu-strategy2.pdf}
%     \caption{Schematics of the halo-region data flow routes in multi-GPU computations. (a) Through host-1 and host-2 (option-1), and (b) CUDA-Aware MPI based direct communication between GPUs (option-2). }
%     \label{gpu-strategy2}
% \end{figure}

% At the end of each time-step the halo-region data has to be shared with the neighbouring processors. To accomplish this, the data from the halo regions (ghost cells) is packed from the `device memory of the source GPU' and transferred to the `device memory of the destination GPU' via one of the two communication options presented in Fig. \ref{gpu-strategy2}. The first option represented as `path-1' in Fig. \ref{gpu-strategy2}a takes a longer route via host CPUs passing through a total of three communication bridges. The second option denoted as `path-2' in Fig. \ref{gpu-strategy2}b takes a direct route between the GPUs and is often preferred if a communication bus exists between the GPUs. The second option can be enabled in Nvidia GPUs having an `NVLink', using the \texttt{DCUDA\_AWARE\_MPI} environment variable available in the OpenACC compilers. Due to the lack of NVLink interconnect amongst some of our GPU resources, the first option (path-1) is used as the inter-GPU communication model in all the computations presented in the current study to maintain consistency. It is worth noting that, since MPI is employed here, the strategy remains unchanged even when multiple CPU/GPU `nodes' are employed. One can also note that, if the GPUs are taken out of the computing environment and computations are chosen to be performed only on the CPUs (taking out the GPUs out the model), such an environment will yield MPI parallel computations on CPUs. This essentially means the same code can be used for both CPU and GPU parallel computations, which makes this strategy highly portable.

Now that the parallelization strategy is outlined, the performance details achieved employing both MEG6 and MIG4 schemes will be presented. All the computations are performed using double precision by default unless any other working precision is specified. The high performance `Intel(R) Xeon(R) Gold 5115 CPU @ 2.40GHz' is used for all the CPU computations. For GPU computations, three generations of data center GPUs, namely, NVIDIA Quadro A6000, NVIDIA V100, and NVIDIA A100 are employed to present a comparative study of their efficiencies. These GPUs are referred to as A6000, V100 and A100 for short in this paper. The acceleration offered by a GPU is mainly dependent on three key hardware attributes which are listed in table \ref{gpuspecs}.

% It can be noted that A100 GPU has both highest Streaming Multiprocessor (SM) count, double precision processing speed, and memory bandwidth.

\begin{table}[h!]
    \centering
    \includegraphics[width=120mm]{Images/gpu-specs.pdf}
    \caption{Key hardware specifications of GPUs used in the current study.}
    \label{gpuspecs}
\end{table}

% \begin{table}[]
% \caption{Key hardware specifications of GPUs used in the current study.}
% {%
% \begin{tabular}{@{}cccccc@{}}
% \toprule
% \textbf{GPU} & \textbf{SMs} & \textbf{Memory} & \textbf{Memory Bandwidth} & \textbf{Processing speed (DP)} & \textbf{Processing speed (SP)} \\ \midrule
% A6000 & 84  & 48 GB & 768 GB/s  & 1.2 TFLOPS          & 38.7 TFLOPS \\
% V100  & 80  & 32 GB & 1134 GB/s & 7 TFLOPS   & 14 TFLOPS   \\
% A100  & 128 & 40 GB & 2039 GB/s & 9.7 TFLOPS & 19.5 TFLOPS \\ \bottomrule
% \end{tabular}%
% }
% \label{gpuspecs}
% \end{table}

\subsection{Speedup comparison to single core CPU} % correction - three test cases were used and not two
Tables \ref{gpu_table1} and \ref{gpu_table2} show the elapsed simulation times per hundred iterations on the CPU and various GPUs. To access the performance, three test cases are employed. The first is the two-dimensional Inviscid Double Mach Reflection (DMR) case (described in section \ref{DMR-case}) consisting of 3 million cells. The second is the three-dimensional Viscous Taylor Greene Vortex (TGV) case (details of the test case can be found in \cite{sainadh2022spectral}) consisting of 27 million cells. The relative difference in the grid resolution between the two cases and the fact that the viscous TGV test case has extra routines to be executed (thus higher exposed parallelism) is planned intentionally to note the performance variation between two such contrasting compute scenarios. To also understand the speedup on a practically relevant multi-block test case, the supersonic jet noise case discussed in Sec. \ref{sec:jet} is simulated at a grid resolution of 13 million.

\begin{table}[h!]
    \centering
    \includegraphics[width=155mm]{Images/gpu_table1.pdf}
    \caption{Comparison of time taken per 100 iterations on CPU versus various Nvidia GPUs employing MEG6 scheme.}
    \label{gpu_table1}
\end{table}

\begin{table}[h!]
    \centering
    \includegraphics[width=155mm]{Images/gpu_table2.pdf}
    \caption{Comparison of time taken per 100 iterations on CPU versus various Nvidia GPUs employing MIG4 scheme.}
    \label{gpu_table2}
\end{table}

\begin{table}[h!]
    \centering
    \includegraphics[width=155mm]{Images/gpu_table3.pdf}
    \caption{Comparison of time taken per 100 iterations on CPU versus various Nvidia GPUs employing WENO-Z scheme.}
    \label{gpu_table3}
\end{table}

The results presented in Tables \ref{gpu_table1}, \ref{gpu_table2}, and \ref{gpu_table3} suggest that the computations performed on V100 and A100 are about two orders of magnitude faster than the calculations performed on CPU. These speedup results can be noted to be on par with the CUDA based parallelization models implemented in other studies, for instance Refs-\cite{cernetic2022high,crespo2015dualsphysics,laufer2022gpu}. Amongst the GPUs, the A100 can be noted to perform the best by producing a maximum speedup of greater than $200\times$ for both MEG6 and MIG4 schemes. V100 stands next in terms of performance, followed by A6000 in the last position. This is consistent with the hardware specifications presented in table \ref{gpuspecs}; the double precision processing speed and memory bandwidth of Nvidia A100 is the highest which is followed by V100 and A6000. The effect of superior single precision speed corresponding to A6000 on the simulation times will be discussed later in section \ref{effect-WP}. 

The results also indicate that the speedup of the simulation is influenced by its parallelism and size. The viscous TGV simulation at 27 million cell count was found to run efficiently compared to the 3 million cell count inviscid DMR test case. The MEG6 scheme was the most efficient among the three schemes tested, due to its explicit derivative computations as opposed to implicit derivatives being used in MIG4 and WENO-Z tests. Since WENO-Z does not use gradients for inviscid flux calculations, for the inviscid DMR case, the WENO-Z scheme was the fastest. However, for the viscous cases (viscous TGV and supersonic jet), the computational time of WENO-Z and MIG4 were similar, with MIG4 being slightly more efficient. Although the computational time of WENO-Z and MIG4 are close for viscous cases, the solution resolution is significantly different, with MIG4 being superior.

% In summary the efficiency is dependent on the nature of scheme (explicit/implicit), size of the simulation and of course the GPU hardware.
% max limited to ~30M on 40gb a6000
% not fair comparison but gives an idea

\subsection{Contribution of various routines to the overall computational time}
Fig. \ref{gpu1}a-d shows a pie-chart view representing the contribution of various routines to the overall computational time in performing one RK-iteration of MEG6 and MIG4 algorithms on both CPU (Intel Xeon Gold 5115) and GPU (Nvidia A100). All the tests were run using the Viscous Taylor-Greene-Vortex case at a grid resolution of $256^3$. The plots suggest that more than 50\% of the time on both CPU and GPU is spent just performing the `reconstruction routine' (indicated with number `1') in all the scenarios. This is mainly because of the multiple sub-steps involved in the reconstruction routine described in \ref{inv_algo}. Furthermore, the contribution of primitive variable gradients (indicated with the number `6') can be noted to occupy a more significant portion of the pie in the MIG4 scheme compared to MEG6. This is due to the extra tri-diagonal matrix inversion step involved in the MIG4 algorithm. Apart from reconstruction and gradient computation routines, the other routines can be noted to occupy a relatively smaller portion of the pie due to the relatively simple nature of the corresponding kernels.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{Images/gpu1.pdf}
    \caption{Contribution of various routines to the total computational time while executing one RK-time step, corresponding to (a) MEG-6, (b) MIG-4 schemes tested using Viscous Taylor Greene Vortex test case ($256\times256\times256$) on Nvidia A100 GPU.}
    \label{gpu1}
\end{figure}

To understand the efficiency of parallelization corresponding to each routine, the individual speedup values for each routine achieved for both MEG6 and MIG4 are computed and tabulated in Fig. \ref{gpu1}e. Interestingly, the speedup values corresponding to MEG6 and MIG4 are nearly similar for all the routines, including the `primitive variable gradient computation'. The `Riemann solver' routine can be noted to run with highest efficiency achieving a speedup of about $\approx 420 \times$ in both MEG6 and MIG4 approaches. On the other hand, the least speedup is noted for the residual computations, time integration, and the boundary condition routines.

\subsection{Effect of cell count on the speedup}
Variation of speedup with increasing cell count is studied relative to the computations on a single core CPU. The test runs were performed on the Nvidia A100 using the Viscous Taylor Greene Vortex test case. The speedup was measured by taking the ratio of time taken per hundred iterations on a single GPU to that of a single core CPU. To avoid GPU memory over-subscription (40GB - A100), the simulations were not carried out beyond 27 Million cells. Fig. \ref{gpu3} shows a monotonically increasing trend in the efficiency for all the three schemes tested. However, due to the implicit nature of the derivative computation involved in the MIG4 and WENO-Z schemes, their parallel efficiency turned out to be slightly lower compared to MEG6. In comparison, WENO-Z was noted to produce the least amount of efficiency, which can be acknowledged by comparing the curves in Fig. \ref{gpu3} near the 27 million range.

%While the speedup has raised from 0 to $150\times$ in the 0 to 5 millions cell count range, the speedup has only increased from $150\times$ to $200\times$ in the 5 Million and 27 Million range.

\begin{figure}[h!]
    \centering
    \includegraphics[width=100mm]{Images/gpu3.pdf}
    \caption{Speedup efficiencies of MEG6, MIG4, and WENO-Z schemes with increasing grid size on the Nvidia A100 GPU with reference to single-core CPU.}
    \label{gpu3}
\end{figure}

The curves shown in Fig. \ref{gpu3} suggest that the rate of rise in speedup is considerably high in the initial range of $0$ to $2.5$ million cells. Although the speedup still increases beyond the $0-2.5$ million range, the curve gets progressively plateaued with increasing cell count. This trend can be explained as follows. Initially, when the cell count is increased, the threads in the GPU are progressively occupied, resulting in a steady and almost linear rise in the speedup until all the GPU threads are fully occupied. This maximum GPU occupancy reaches at close to two million cells in the specific case studied here. However, when the cell count is further increased, the kernel divides the program loops into multiple sets to run each of them in parallel resulting in multiple computation cycles within the kernel. This effect manifests as a slow rise in the parallel efficiency after a certain cell count limit. Despite this plateauing effect, keeping the GPU unit as occupied as possible is always a good practice to extract the highest possible efficiency, especially keeping in view the net power consumption when multiple GPUs are employed.


\subsection{Memory occupancy of the solver on GPU} \label{effect-WP}

\begin{figure}[h!]
    \centering
    \includegraphics[width=92mm]{Images/GPU-mem-consumption.pdf}
    \caption{The GPU memory consumption with respect to variation in the grid size of the simulation, as demonstrated by tests performed on an NVIDIA A100 with 40GB RAM using the Viscous Taylor-Green Vortex test case. The gray dashed line corresponds to the approximate estimate of memory consumption based on the number of global variables defined in the solver.}
    \label{mem_cons}
\end{figure}

The capability to run a simulation of given size on a GPU is largely determined by the available RAM on the device. Memory utilization is a crucial factor for finite difference algorithms based on curvilinear coordinates, as they require multiple variables representing metric terms to be defined at each cell center and interface. The memory footprint of a three-dimensional solver scales approximately as $O(N^3)$, where $N$ represents the number of cells in each direction, and for simplicity, it is assumed to be equal in all directions.

Fig. \ref{mem_cons} displays the memory occupancy ($\mathcal{M}$) of the flow solver at different grid resolutions. The plot also includes a rough estimate of the expected memory consumption ($\mathcal{M}=\left[117(N+8)^3\right] \times 64 \text { bits }$) derived based on the number of variables in the solver. The solver uses $117$ global 3D double-precision arrays, each of size $(N+8)^3$ (including ghost cells), excluding small scalars and arrays. The discrepancy between the blue line and gray dashed line in Fig. \ref{mem_cons} is due to arrays/variables defined in each function that consume memory on GPU RAM. For the present flow solver (employing MEG6 or MIG4), tests show that a Nvidia A100 (40GB) can handle simulations with up to 34 million cells without any decrease in performance or memory over-subscription issues. Among the various data variables, metric terms consume about $60\%$ of the total memory as they are needed at cell centers and six-cell interfaces surrounding each cell. $71$ out of a total $117$ variables belong to the metric terms such as $(\xi_{x})_i$, $(\xi_{x})_{i+\frac{1}{2}}$, $(\hat{\xi}_{x})_{i+\frac{1}{2}}$, and $(\Tilde{\xi}_{x})_{i+\frac{1}{2}}$.


\subsection{Effect of working precision on the speedup} \label{effect-WP}

The floating point representation in computer calculations can influence the computational time as it can directly affect the number of arithmetic operations and the overall memory movement required to complete a task. A 32-bit/single-precision memory allocation (FP32) is generally used for tasks where the accuracy of calculations is not critical and the tasks are bounded by limitations in allocated memory. On the other hand, a 64-bit/double-precision (FP64) allocation is usually employed for tasks like scientific calculations where accuracy is needed. Although FP64 is widely used for CFD simulations, a few studies show that the working precision does not significantly influence the solution and the corresponding physics. One can also improve the simulation turnaround times by employing FP32 for the transient phase of simulation (where the flowfield is not of interest to the user) and switching back to FP64 for the rest of the flowfield calculations when the accuracy of the solution is essential.

\begin{figure}[h!]
    \centering
    \includegraphics[width=150mm]{Images/gpu-SP-DP.pdf}
    \caption{Comparison of elapsed times using Single Precision (FP32) and Double Precision (FP64) data variables.}
    \label{gpu-SP-DP}
\end{figure}

Fig. \ref{gpu-SP-DP} compares FP32 and FP64 computational times for both MEG6 and MIG4 schemes on various GPUs. The tests were performed by running one hundred iterations of the Viscous Taylor Greene vortex test case at $256^3$ resolution. The simulations that use FP32 can be noted to run faster than FP64 by a factor of $\approx 1.3\times$ to $1.4\times$ contrary to the anticipated $2\times$ speedup that is suggestive from Table \ref{gpuspecs}. There is a high discrepancy between the FP32 processing speed of A6000 displayed in table \ref{gpuspecs} and the FP32 speedup achieved here. Furthermore, the FP32 calculations on A6000 can be noted to only run almost as fast as FP64 calculations on the V100. This suggests that rather than the processing speed, the computations are primarily bounded by the memory bandwidth of GPU. Which means that the overall simulation time is not only spent in performing the floating point operations but also on other essential tasks such as initializing the kernels, moving data across GPU RAM and various levels of cache memory storage, etc. It is worth noting that scientific computing applications like the present solver are generally bounded by memory bandwidth as opposed to arithmetic operations. Therefore, the working precision does not hugely affect the performance, as demonstrated here. Nevertheless, the speedup from the employment of FP32 can still be used to accelerate the simulations.
