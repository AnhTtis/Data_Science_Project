\section{Introduction}
\label{sec:introduction}

The rise of \emph{Do-It-Yourself} (DIY) videos on the web has made it possible even for an unskilled person (or a skilled robot) to imitate and follow instructions to complete complex real world tasks~\cite{bonardi2020learning,liu2018imitation,torabi2019recent}.
One such task that is often cumbersome to infer from instruction descriptions yet easy to imitate from a video is the task of assembling furniture from its parts.
Often times the instruction steps involved in such a task are depicted in pictorial form, so that they are comprehensible beyond the boundaries of language (e.g.,~Ikea assembly manuals).
However, such instructional diagrams can sometimes be ambiguous, unclear, or may not match the furniture parts at disposal due to product variability.
Having access to video sequences that demonstrate the precise assembly process could be very useful in such cases.

Unfortunately, most DIY videos on the web are created by amateurs and often involve content that is not necessarily related to the task at hand.
For example, such videos may include commentary about the furniture being assembled, or personal assembly preferences that are not captured in the instruction manual.
Further, there could be large collections of videos on the web that demonstrate the assembly process for the same furniture but in diverse assembly settings; watching them could consume significant time from the assembly process.
Thus, it is important to have a mechanism that can effectively align relevant video segments against the instructions steps illustrated in a manual.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{img/teaser.pdf}
    \caption{An illustration of video-diagram alignment between a YouTube video (top) \href{https://www.youtube.com/watch?v=He0pCeCTJQM}{He0pCeCTJQM} and an Ikea furniture manual (bottom) \href{https://www.ikea.com/au/en/p/tarva-bed-frame-pine-luroey-s49069795/}{s49069795}.}
    \label{fig:demo}
\end{figure}

In this paper, we consider this novel task as a multimodal alignment problem~\cite{radford2021learning,luo2022clip4clip}, specifically for aligning in-the-wild web videos of furniture assembly and the respective diagrams in the instruction manuals as shown in~\figref{fig:demo}.
In contrast to prior approaches for such multimodal alignment, which usually uses audio, visual, and language modalities, our task of aligning images with video sequences brings in several unique challenges.
First, instructional diagrams can be significantly more abstract compared to text and audio descriptions.
Second, illustrations of the assembly process can vary subtly from step-to-step (e.g., a rectangle placed on another rectangle could mean placing a furniture part on top of another).
Third, the assembly actions, while depicted in a form that is easy for humans to understand, can be incomprehensible for a machine.
And last, there need not be common standard or visual language followed when creating such manuals (e.g., a furniture piece could be represented as a rectangle based on its aspect ratio, or could be marked with an identifier, such as a part number).
These issues make automated reasoning of instruction manuals against their video enactments extremely challenging.

In order to tackle the above challenges, we propose a novel contrastive learning framework for aligning videos and instructional diagrams, which better suits the specifics of our task.
We utilize two important priors---a video only needs to align with its own manual and adjacent steps in a manual share common semantics---that we encode as terms in our loss function with multimodal features computed from video and image encoder networks.

To study the task in a realistic setting, we introduce a new dataset as part of this paper, dubbed \dataset for Ikea assembly in the wild.
Our dataset consists of nearly 8,300 illustrative diagrams from 420 unique furniture types scraped from the web and 1,005 videos capturing real-world furniture assembly in a variety of settings.
We used the Amazon Mechanical Turk to obtain ground truth alignments of the videos to their instruction manuals.
The videos involve significant camera motions, diverse viewpoints, changes in lighting conditions, human poses, assembly actions, and tool use.
Such in-the-wild videos offer a compelling setting for studying our alignment task within its full generality and brings with it a novel research direction for exploring the multimodal alignment problem with exciting real-world applications, e.g., robotic imitation learning, guiding human assembly, etc.

To evaluate the performance of our learned alignment, we propose two tasks on our dataset: (i) nearest neighbor retrieval between videos and instructional diagrams, and (ii) alignment of the set of instruction steps from the manual to clips from an associated video sequence.
Our experimental results show that our proposed approach leads to promising results against a compelling alternative, CLIP~\cite{radford2021learning}, demonstrating 9.68\% improvement on the retrieval task and 12\% improvement on the video-to-diagram alignment task.
