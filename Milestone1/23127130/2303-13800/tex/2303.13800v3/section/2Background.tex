\section{Related Work}
\label{sec:background}

\begin{figure*}
    \centering
    \begin{tabular}{cc}
        \includegraphics[height=5cm]{img/model-training.pdf} &
        \includegraphics[height=5cm]{img/model-inference.pdf}
        \\ {\footnotesize (a) Training stage.} & {\footnotesize{(b) Inference stage.}}
    \end{tabular}
    \caption{Schematic of our overall architecture.
        During training we extract features from each input video clip and set of instructional diagrams, respectively, using pre-trained encoders. We concatenate these with sinusoidal progress rate features (SPRF) introduced in~\secref{sec:sprf} and project into the same $D^\ell$ dimensionality space. The matched video clip and instructional diagram feature pairs are used for Video-Diagram Contrastive Loss, the video clip feature and $M$ instructional diagram features are fed into Video-Manual Contrastive Loss, and $M$ instructional diagram features themselves are used in Intra-Manual Contrastive Loss introduced in~\secref{sec:losses}. During inference all video features from $N$ sequential video clips, and all $M$ instructional diagram features from the corresponding manual are computed. We then form a similarity matrix and apply optimal transport (OT) introduced in~\secref{sec:ot} to produce the final alignment probabilities.}
    \label{fig:model}
\end{figure*}

\noindent\textbf{Assembly and Instructional Datasets.}
Multimodal video datasets (e.g., \cite{zhou2018towards, damen2018scaling, toyer2017human, ben2021ikea, wang2022translating, shao2016dynamic}) bridge the gap between video and other modalities such as the narratives from the video or instruction texts.
Among them, EPIC Kitchens~\cite{damen2018scaling} and YouCook2~\cite{zhou2018towards} align each video clip with the cooking procedure narratives.
Our dataset is more closely related to IKEA ASM~\cite{ben2021ikea} and IKEA-FA~\cite{toyer2017human}, which demonstrate furniture assembly instructions.
There are some other datasets focusing on converting assembly manuals to more comprehensible formats.
LEGO~\cite{wang2022translating} demonstrates how to obtain an executable plan from the assembly manuals while Shao et al.~\cite{shao2016dynamic} parses furniture assembly instructions into 3D models based on their manuals.
Unlike all of the above datasets, the proposed \dataset dataset aims to achieve the novel multimodal task of aligning in-the-wild web videos with step-by-step instructional diagrams.

\noindent\textbf{Multimodal Alignment.}
The classic work of Everingham et al.~\cite{everingham2006hello} focuses on aligning subtitle-transcript with person IDs in videos. Later works~\cite{tapaswi2015book2movie, dogan2018neural} started aligning video segments with text story-lines.
Recently, different approaches have been proposed for the text-video retrieval task, e.g., extracting fine-grained text features~\cite{wu2021hanet, han2021fine}, augmenting with more modalities~\cite{gabeur2020multi, wang2021t2vlad}, and contrastive text-video learning~\cite{liu2021hit, luo2022clip4clip, cheng2021improving, bogolin2022cross}.
Among them, Han et al.~\cite{han2022temporal} tackles alignment between assembly videos and text manuals.
However, due to the modality distinctions between text and image, these methods cannot be directly adopted to solve our problem.
Apart from the video modality, sketch images are similar to our instructional diagrams in the sense that both are black-white, text-free, and highly iconic abstract images.
Sketch-based video retrieval~\cite{collomosse2009storyboard} aims to retrieve specific video clips given a sketch image or sequence.
A recent related work to ours is Xu et al.~\cite{xu2020fine}, which extracts image features from both sketches and motion vector images, and optical flow from video clips.
They apply a triplet loss and a relation module on these extracted multimodal features to train the model.
However, their motion vector sketch is ad-hoc to specific video types, such as sports.
Compared with Xu et al.~\cite{xu2020fine}, our method is more general and supports two tasks from both video-to-diagram and diagram-to-video directions.

\noindent\textbf{Contrastive Learning.}
Contrastive learning was first introduced for self-supervised representation learning~\cite{oord2018representation, chen2020simple, he2020momentum, grill2020bootstrap, chen2021exploring}.
Then, the idea was naturally adapted for multimodal learning tasks, such as text-image alignment~\cite{radford2021learning} and text-video retrieval~\cite{luo2022clip4clip}.
CLIP~\cite{radford2021learning} designs a contrastive pre-training approach by predicting the correct pairs between images and their captions.
Since CLIP verifies the effectiveness of cross-modality contrastive learning, recent works~\cite{xue2022clip, bain2021frozen, wang2022omnivl} have incorporated it into the related models, facilitating the cross-modality video retrieval task.
Different from existing works, our alignment problem not only requires good contrastive between video clips and instructional diagrams but also entails distinguishing the subtle details in step-by-step manuals.
This motivates us to design three task-specific contrastive losses.
