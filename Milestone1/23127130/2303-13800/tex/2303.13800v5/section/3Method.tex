\section{Video-Instruction Alignment}
\label{sec:method}

\begin{figure*}
    \centering
    \includegraphics[width=0.99\linewidth]{img/method.pdf}
    \caption{Visualization of our three losses described in~\secref{sec:losses}. The intent is depicted in the first row and the batch formation in the second row. Loss (a) tries to pair video and image up across the entire dataset. Loss (b) only matches video clips and images corresponding to the same manual. And loss (c) push images from the same manual apart from each other for better feature discrimination.}
    \label{fig:losses}
\end{figure*}

We formulate the task of aligning video segments to instructional diagrams as a variant of video-to-image matching.
Here the idea is to retrieve the image from a candidate set that most closely depicts the activity occurring in the short video clips and vice versa.
Importantly, since different instructional videos will involve different numbers of steps the candidate set will necessarily have variable cardinality (unlike, say, multi-class classification tasks).

Formally, given a set of $N$ video clips $\{V_i\}_{i=1}^{N}$ and a set of $M$ instructional diagrams $\{I_j\}_{j=1}^{M}$, our goal is to train a model to predict the correspondence between diagrams and clips.
A standard approach for addressing this problem is to learn a joint embedding space for videos and diagrams such that matching video-diagram pairs map near to each other in the embedding space.
Let $\mathbf{f}_i^\text{V}$ and $\mathbf{f}_j^{\text{I}}$ denote the feature embedding for the $i$-th video clip and $j$-th instructional diagram, respectively, and let $f_{\text{sim}}$ be some similarity measure.
Then, once the embedding space is learned we can use the model to predict the index of the instructional diagram corresponding to a given video clip $V$ as
%
\begin{align}
    j^\star & = \mathop{\text{argmax}}_{j=1,\dots,M} f_{\text{sim}}(\mathbf{f}^\text{V}, \mathbf{f}_j^\text{I}).
\end{align}
%
Likewise, we can find the video segment that most closely matches a given instructional diagram $I$ as
%
\begin{align}
    i^\star & = \mathop{\text{argmax}}_{i=1,\dots,N} f_{\text{sim}}(\mathbf{f}_i^\text{V}, \mathbf{f}^\text{I}).
\end{align}
%
This can be generalized to top-$k$ retrieval.
Last, we can enforce matching constraints, such as through optimal transport or dynamic time warping if order information is available, to jointly match all clips in a video to all steps in an instruction manual.
\figref{fig:model} depicts the overall model.

In this work, we use cosine similarity for $f_{\text{sim}}$.
The embedding vectors $\mathbf{f}_i^\text{V}$ and $\mathbf{f}_j^\text{I}$ are computed using video and image encoders trained under a contrastive loss and optionally augmented with temporal features such as we now describe.

\subsection{Sinusoidal Progress Rate Feature}
\label{sec:sprf}
Instruction manuals contain an ordered sequence of steps that is typically, although not always, followed during the assembly process.
However, the time needed to perform each step varies greatly depending on complexity of the step and experience of the assembler.
This suggests a weak correlation between (proportional) timestamps in the video and progress through the assembly process.
We can make use of this prior by including temporal ordering information in the video and diagram feature representations.

Given a video clip $V$ sampled from a full video of length $t_\text{duration}$ seconds, with start time $t_\text{start}$ and end time $t_\text{end}$, we define the video progress rate $r^\text{V}$ of that video clip as
%
\begin{align}
    r^\text{V} & =(t_\text{start}+t_\text{end})/2 t_\text{duration}
\end{align}
%
and the instructional diagram progress rate $r^\text{I}$ for the $j$-th step from a manual with $M$ total steps is simply $ r^\text{I} = j / M$.
%
Because we are using a cosine similarity function $f_\text{sim}$, we map the progress feature onto a half circle so that high similarity score coincides with when they align.
The final sinusoidal progress rate feature (SPRF) is then
%
\begin{align}
    (\sin(\pi r^V), \cos(\pi r^V))
\end{align}
%
for video and similarly for the instructional diagram, which we append to the feature embeddings extracted from the respective encoders (see~\figref{fig:model}).
Before and after the concatenation, the features are L2 normalized to alleviate side-effect due to fluctuation of numerical value scale.
Two fully connected layers then project each modality feature into the same dimensional space to form representations $\mathbf{f}^I$ and $\mathbf{f}^V$ for further similarity comparison.

\subsection{Training Losses}
\label{sec:losses}

Starting with pre-trained video and image encoders we finetune our model using variants of contrastive learning, which has recently been made popular for cross-modal matching by models like CLIP~\cite{radford2021learning}.
In this setting mini-batches are constructed by sampling video clip-instructional diagram pairs $(V_i, I_i)$ to optimize an infoNCE loss~\cite{hadsell2006dimensionality,oord2018representation} where pairs $(V_i, I_i)$ are considered positive and $(V_i, I_j)$, $i \neq j$ are considered negative.
Here we sample randomly from all videos and instruction manuals in the training data.

Formally, for mini-batch containing $B$ pairs, define
%
\begin{align}
    p^{V2I}_{ij} & = \frac{\exp(f_\text{sim}(\boldf^V_i,\boldf^I_j)/\tau)}{\sum_{b=1}^B{\exp(f_\text{sim}(\boldf^V_i,\boldf^I_b)/\tau)}}
    \label{eqn:video_to_image}
    \\
    p^{I2V}_{ji} & = \frac{\exp(f_\text{sim}(\boldf^V_i,\boldf^I_j)/\tau)}{\sum_{b=1}^B{\exp(f_\text{sim}(\boldf^V_b,\boldf^I_j)/\tau)}}
    \label{eqn:image_to_video}
\end{align}
%
to be the probability of matching video $V_i$ to image $I_j$ and the probability of matching image $I_j$ to video $V_i$, respectively.
Here $\tau$ is a temperature parameter that controls the bias towards difficult examples~\cite{wang2021understanding}.
%
Standard contrastive learning then minimizes
%
\begin{align}
    \Ell_\text{infoNCE} & = -\frac{1}{2B}\left(\sum_{i=1}^B \log p^{V2I}_{ii} + \sum_{j=1}^B \log p^{I2V}_{jj}\right).
\end{align}

We note that this vanilla version of contrastive learning does not consider situations where there may be many-to-one matches between pairs.
Specifically, in our application multiple video clips may map to the same step.
We introduce a specialized loss to deal with this scenario.

\noindent\textbf{Video-Diagram Contrastive Loss (\figref{fig:losses}(a)).}
%
Contrastive learning frameworks benefit from large batch sizes~\cite{radford2021learning}.
However, as batch size increases there is a greater chance that we sample multiple videos matching to the same diagram within the batch, which violates the assumptions of the infoNCE loss.
To better handle these cases we build on the work of~\cite{wang2021actionclip} that introduces a Kullback-Leibler (KL) divergence loss between predicted and ground truth distributions, $\bp$ and $\bq$, respectively.
However, rather than KL-divergence, we prefer Jensen-Shannon (JS) divergence, which we find improves training stability.

Let $\bp^{V2I}$ and $\bp^{I2V}$ be vectors containing all video-to-diagram and diagram-to-video probabilities introduced in Eqs.
\ref{eqn:video_to_image} and \ref{eqn:image_to_video}, respectively.
Similarly, let $\bq^{V2I}$ and $\bq^{I2V}$ be the corresponding ground truth alignment distributions.
Then our video-diagram contrastive loss is defined as
%
\begin{align}
    \Ell^\text{VI} & = \frac{1}{2}\left(D_{JS}(\bp^{V2I} \| \bq^{V2I}) + D_{JS}(\bp^{I2V} \| \bq^{I2V})\right)
\end{align}
%
where $D_{JS}$ is the Jensen-Shannon divergence.

\noindent\textbf{Video-Manual Contrastive Loss (\figref{fig:losses}(b)).}
%
The above losses align video and diagram pairs globally across the entire training dataset.
However, for our task we know that a given video clip only needs to match against one of the steps in its corresponding instruction manual, not other manuals.
Hence, we can perform a more task-specific discrimination by exploiting this prior information in the model.
To do so we modify our procedure for constructing a mini-batch to first sample a video clip $V_i$ and then include all instructional diagrams $\{I_1, \ldots, I_{M_i}\}$ from the video's corresponding manual.
One of these diagrams will be the ground truth positive match for the clip.
We then employ a classification loss based on cross entropy (CE) as
%
\begin{align}
    \Ell^{VM} & = \sum_{i=1}^B\frac{M_i}{\sum_{b=1}^B M_b} CE(\bp_{i}^{V2I}, \bp_{i}^{gt})
\end{align}
%
where $M_i$ indicates the length of the manual corresponding to the $i$-th video.
Here $\bp_{i}^{V2I} \subseteq (p^{V2I}_{ij})_{j=1}^{B}$ is a subvector of probabilities for matching video $V_i$ to all diagrams $I_j$ from the corresponding manual and $\bp_i^{gt}$ is the associated one-hot ground truth encoding.
We weight each term in the loss by $\frac{M_i}{\sum_{b=1}^B M_b}$ to give more emphasis to more difficult assemblies, assumed to be the ones containing more steps.

\noindent\textbf{Intra-Manual Contrastive Loss (\figref{fig:losses}(c)).}
%
The previous losses only consider contrasting embeddings between videos and diagrams.
However, most furniture assembly tasks involve a progressive process where the visual similarity between successive steps is large.
Indeed, the main component of the assembly is often introduced early in the assembly process and dominates the instructional diagram.
This makes it challenging to distinguish between steps.
To encourage diagrams from the same manual to be spread out in embedding space, so that they are more easily distinguished, we introduce an intra-manual contrastive loss.

Similar to the video-to-diagram and diagram-to-video matching probabilities defined above, let
%
\begin{align}
    p^{I2I}_{jk} & = \frac{\exp(f_\text{sim}(\boldf^I_j,\boldf^I_k)/\tau)}{\sum_{m=1}^M{\exp(f_\text{sim}(\boldf^I_j,\boldf^I_m)/\tau)}}
\end{align}
%
be the probability of matching diagram $I_j$ to diagram $I_k$ from the same manual according to our similarity metric.
Then we define our intra-manual contrastive loss as
%
\begin{align}
    \Ell^M & = \sum_{j=1}^B\frac{M_j}{\sum_{b=1}^B M_b} D_{JS}\left(\bp_j^{I2I} \| \N(j, \theta)\right)
\end{align}
%
where $\bp_j^{I2I}$ is the softmax normalized diagram-to-diagram probability vector associated with diagram $I_j$,
and $\N(j, \theta)$ is a univariate Gaussian distribution with mean $j$, learnable variance $\theta$
and discretized on support $\{1, \ldots, M_j\}$.
This encourages distances in diagram embedding space to correspond to distances between steps in the manual.
We use a normal distribution instead of a delta distribution as a relaxation since nearby negative diagrams are still likely to share some semantics.

\subsection{Set Matching}
\label{sec:ot}
Our model is very general.
Given a single video clip we can retrieve the most likely diagram showing the assembly step and given a single diagram we can retrieve a set of best matching video clips.
To align an entire video (sequence of clips) to an entire instruction manual, we can add approximate one-to-one matching priors or temporal constraints, through optimal transport (OT)
or dynamic time warping (DTW), respectively.
As we will see in our experiments, the absence of temporal order constraints in OT slightly outperforms DTW due to occasional out-of-order execution of assembly steps or strong false matches.

To apply either method we first extract features $\boldf^V_i$ for an entire video $\{V_i\}_{i=1}^N$ and $\boldf^I_j$ for all instructional diagrams in the corresponding manual $\{I_j\}_{j=1}^M$.
Denote by $s_{ij}$ the similarity $f_\text{sim}(\boldf_i^V, \boldf_j^I)$ between video clip $V_i$ and diagram $I_j$.
Let $\overline{s} = \max_{i,j} s_{ij}$ and $\underline{s} = \min_{ij} s_{ij}$.
We then construct a cost matrix $C \in \reals^{N \times M}$ with entries
%
\begin{align}
    C_{ij} & = \frac{s_{ij}^\alpha - \underline{s}^\alpha}{\overline{s}^\alpha -  \underline{s}^\alpha}.
\end{align}
%
Here $\alpha > 1$ accentuates the similarity differences and the normalization by $\overline{s}^\alpha - \underline{s}^\alpha$ restricts the range of $C_{ij}$ to $[0, 1]$.
The optimal transportation plan $T^{\star}$ obtained by solving the entropy regularized optimal transport problem,
%
\begin{align}
    \begin{array}{rl}
        \text{minimize}   & \sum_{i=1}^{N} \sum_{j=1}^{M} T_{ij} C_{ij} - \epsilon H(T)           \\
        \text{subject to} & \sum_{i=1}^{M} T_{ij} = \frac{1}{N}, \,\text{for $j = 1, \ldots, N$}  \\
                          & \sum_{j=1}^{N} T_{ij} = \frac{1}{M}, \,\text{for $i = 1, \ldots, M$},
    \end{array}
\end{align}
%
gives the joint probability of matching videos and diagrams.
It can be found efficiently by applying the Sinkhorn-Knopp algorithm~\cite{sinkhorn1967diagonal} to the optimization problem defined above.

In a similar fashion, we can use DTW to find the optimal path through the cost matrix to give the most likely matching subject to the ordering constraint that later video clips cannot match to earlier instructional diagrams and vice versa.
More formally, if video clip $V_i$ matches to diagram $I_j$ then clip $V_{i+1}$ cannot match to diagram $I_{j'}$ with $j' < j$ and diagram $I_{j+1}$ cannot match to video clip $V_{i'}$ with $i' < i$.
