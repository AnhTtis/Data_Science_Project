\section{Experiments}
\label{sec:experiments}

\input{tbl/result}

We evaluate our model on the \dataset dataset for tasks of finding the best instructional diagram for a given video clip (video-to-diagram retrieval) and finding the top-$k$ video clips corresponding to a given diagram (diagram-to-video retrieval).
We consider two settings: independent retrieval where we are given one query video (resp. diagram) at a time, and set retrieval where we are given an entire video and corresponding instruction manual.
The latter is the alignment problem and allows us to use structured inference methods such as optimal transport (OT) and dynamic time warping (DTW).

\noindent\textbf{Preprocessing.} We re-sample all videos to 30fps and then sub-sample into 10s segments to align with common practice of action recognition tasks.
Video clips of duration 2.13s (64 frames) are used as input to the video encoder as shown in~\cref{fig:sampler}.
The short side of each video frame is down-sampled to 224, maintaining aspect ratio.
The long side of each instructional diagram is down-sampled to 224, also maintaining aspect ratio and padding the short side with white pixels.
Random resize crops are used as data augmentation for videos, and random resize crops, horizontal flips and rotations are applied to instructional diagrams.

\noindent\textbf{Architecture.} We choose ResNet-50~\cite{he2016deep} pretrained on ImageNet~\cite{deng2009imagenet} as our backbone image encoder, and a ResNet-50 based Kinetics 400~\cite{carreira2017quo} pretrained Slowfast-8x8~\cite{feichtenhofer2019slowfast} for video encoding.
For each 64-frame clip, 8 frames are uniformly sub-sampled for the slow path, and 32 frames for the fast path.
We remove the classification heads from these two backbone models and freeze the entire video encoder, but only first three layers of the image encoder allowing the later layers to be finetuned.
The dimensionality of both video and diagram features is set to 1024.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{img/sampler.pdf}
    \caption{Demonstration of the video clip sampler for a task with four steps, A, B, C and D. Each step is sub-sampled to multiple 10s video segments (\eg B $\rightarrow$ B1 and B2) with back padding. As input to the video encoder, each video segment is further divided into 2.13s video clips. We randomly sample one clips for training (1), choose a constant single clip for validation (2), and average over five clips for testing (3).}
    \label{fig:sampler}
\end{figure}

\noindent\textbf{Training Details.} A dedicated learnable temperature parameter $\tau$ is assigned to each loss and initialized to 0.07 following~\cite{wu2018unsupervised}.
The variance $\sigma$ in intra-manual contrastive loss is initialized to 1 to represent a standard normal distribution.
We use AdamW~\cite{loshchilov2017decoupled} as the optimizer with learning rate $5 \times 10^{-4}$ and weight decay $5 \times 10^{-3}$.
All models are trained for 20 epochs with 128 video clips per batch (and number of instructional diagram depending on the losses being used as described in~\secref{sec:method}).
We select the model from the epoch with highest top-1 accuracy on the validation set for reporting test set results.
It takes approximately 20 hours on a single Nvidia A100 GPU 80GB per experiment.
During alignment testing, similarity scores are aggregated into a single $N \times M$ matrix for each video and corresponding instruction manual and optimal transport applied with hyper-parameters $\epsilon=4$ and $\alpha=7$.

\noindent\textbf{Evaluation metrics.} We report average top-1 accuracy and average index error (AIE) for the video-to-diagram retrieval task on the test set.
AIE is useful for characterizing errors since predicting a step near to the ground truth is better than predicting one that is far away.
It is defined as,
%
\begin{align}
    \text{AIE} & = \frac{1}{N} \sum_{i=1}^N |j_i^\star - j_i^\text{gt}|
\end{align}
%
where $j_i^\star$ is the predicted diagram index for the $i$-th video and $j_i^\text{gt}$ is its true index.
Since a single instructional diagram can correspond to multiple video clips we adopt recall@1, recall@3 and area under the ROC curve as metrics for the diagram-to-video task.
Unless otherwise stated we report results at the video segment level.
Here we sample consecutive 64-frame video clips (2.13s) from each 10s video segment and average the features from the video encoder.

\subsection{Main Results}

Our main results are reported in~\tabref{tbl:result}.
We compare to two baseline methods, {\sc CosSim} and CLIP, which use a cosine similarity loss and infoNCE loss only on paired features.
We report results on four variants of our approach using all three losses described in~\secref{sec:method}.
The first (``Ours'') includes the sinusoidal progress rate features (SPRF) capturing temporal information.
Note that we also experimented with more standard positions encoding methods popular with transformers~\cite{vaswani2017attention} but found these to produce inferior results (omitted here for brevity).
The second variant (``w/o SPRF'') shows results with this feature removed.
The last two variants use dynamic time warping (DTW) and optimal transport (OT) in the alignment setting, i.e., with access to complete videos and instruction manuals.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{img/ot.pdf}
    \caption{An example of post processing with DTW and OT with furniture \href{https://www.ikea.com/au/en/p/friheten-three-seat-sofa-bed-skiftebo-dark-grey-30341149/}{30341149} and video \href{https://www.youtube.com/watch?v=dzLNgz861Hk}{dzLNgz861Hk}.}
    \label{fig:ot}
\end{figure}

Observe that our method significantly outperforms the baseline approaches on both video-to-diagram retrieval and diagram-to-video retrieval.
This is largely due to our SPRF feature but also thanks to the improved loss functions (we provide a complete ablation analysis below).
Further improvement in performance can be gained by post processing with DTW or OT.
Interestingly, OT does slightly better than DTW indicating the the ordering constraint imposed by DTW is too restrictive for this task.
See~\figref{fig:ot} for an example alignment.

Qualitative results are shown in~\figref{fig:qualitative}.
We show one correct alignment and one incorrect alignment for matching to assembly steps.
Notice the high degree of similarity between steps in the assembly process, which makes this an extremely challenging task.
Further examples are included on the project website for this paper.

\begin{figure*}
    \centering
    \begin{tabular}{cc}
        \includegraphics[width=0.476\linewidth]{img/success.pdf}
         &
        \includegraphics[width=0.476\linewidth]{img/fail.pdf}
        \\
        \parbox{0.476\linewidth}{\vspace{0.3cm} \small (a) Successful alignment between YouTube video \href{https://www.youtube.com/watch?v=moq_A1o3ZKw}{moq\_A1o3ZKw} and Ikea furniture manual \href{https://www.ikea.com/au/en/p/rast-chest-of-3-drawers-pine-60356219/}{60356219}.}
         &
        \parbox{0.476\linewidth}{\vspace{0.3cm} \small (b) A failed alignment between YouTube video \href{https://www.youtube.com/watch?v=d6sbVuHV0bc}{d6sbVuHV0bc} and Ikea furniture manual \href{https://www.ikea.com/au/en/p/laett-childrens-table-with-2-chairs-white-pine-10178413/}{10178413}.}
    \end{tabular}
    \caption{Qualitative results. Rows show frames from a single video clip; step instructional diagrams (subset shown); and page instructional diagrams. Prediction is highlighted by a green box for correct or a red box for incorrect; ground truth is then highlighted with a blue box.}
    \label{fig:qualitative}
\end{figure*}

\subsection{Effect of Losses}

\input{tbl/losses}

Our work introduces three novel loss terms for the task of aligning videos to step-by-step instructions.
We now analyze the effectiveness of each loss by evaluating our model trained using different combinations.
The results are summarized in~\tabref{tbl:losses}.
We can draw several conclusions from these results
First, our video-diagram contrastive loss (A) slightly outperforms the standard infoNCE loss used by CLIP.
This confirms our intuition that infoNCE is adversely affected by the many-to-one matchings between video clips and instructional diagrams albeit only slightly.

Second, the video-manual contrastive loss (B) gives the greatest boost in performance over the baseline approaches and once used gain little benefit from the video-diagram contrastive loss (A).
The intra-manual contrastive loss (C) combined with the other losses slightly improves results.

Last, including losses on page diagrams even when evaluating on step diagrams improves results (but not vice versa).
We hypothesize that this is because page diagrams provide a regularizing effect on learning since it is easier to match against pages than individual steps.
