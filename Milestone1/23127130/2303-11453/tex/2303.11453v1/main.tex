\documentclass[letterpaper,11pt]{article}

\usepackage{etoc}

\usepackage[margin=1.0in]{geometry}



\input{defs}

\usepackage[colorlinks=true,linkcolor=blue]{hyperref}

\hypersetup{citecolor=blue}

\title{\textbf{\Large{Greedy Pruning with Group Lasso Provably Generalizes for\\ Matrix Sensing and Neural Networks with Quadratic Activations}}
\vspace{5mm}
}

\author{
Nived Rajaraman\thanks{Dept. of Electrical Engineering and Computer Sciences, UC Berkeley, \texttt{nived.rajaraman@eecs.berkeley.edu} } 
\quad Devvrit\thanks{Dept. of Computer Science, UT Austin, \texttt{devvrit.03@gmail.com} } \quad Aryan Mokhtari\thanks{Dept. of Electrical and Computer Engineering, UT Austin, \texttt{mokhtari@austin.utexas.edu} } \quad Kannan Ramchandran\thanks{Dept. of Electrical Engineering and Computer Sciences, UC Berkeley, \texttt{kannanr@eecs.berkeley.edu} }
\vspace{3mm}
}



\begin{document}

\maketitle

\begin{abstract}
Pruning schemes have been widely used in practice to reduce the complexity of trained models with a massive number of parameters. Several practical studies have shown that pruning an overparameterized model and fine-tuning generalizes well to new samples. Although the above pipeline, which we refer to as pruning + fine-tuning, has been extremely successful in lowering the complexity of trained models, there is very little known about the theory behind this success. In this paper we address this issue by investigating the pruning + fine-tuning framework on the overparameterized matrix sensing problem, with the ground truth denoted $U_\star \in \mathbb{R}^{d \times r}$ and the overparameterized model $U \in \mathbb{R}^{d \times k}$ with $k \gg r$. We study the approximate local minima of the empirical mean square error, augmented with a smooth version of a group Lasso regularizer, $\sum_{i=1}^k \| U e_i \|_2$ and show that pruning the low $\ell_2$-norm columns results in a solution $U_{\text{prune}}$ which has the minimum number of columns $r$, yet is close to the ground truth in training loss. Initializing the subsequent fine-tuning phase from $U_{\text{prune}}$, the resulting solution converges linearly to a generalization error of $O(\sqrt{rd/n})$ ignoring lower order terms, which is statistically optimal. While our analysis provides insights into the role of regularization in pruning, we also show that running gradient descent in the absence of regularization results in models which {are not suitable for greedy pruning}, i.e., many columns could have their $\ell_2$ norm comparable to that of the maximum. Lastly, we extend our results for the training and pruning of two-layer neural networks with quadratic activation functions. Our results provide the first rigorous insights on why greedy pruning + fine-tuning leads to smaller models which also generalize well.
\end{abstract}

\newpage

\tableofcontents

\newpage

\section{Introduction}

Training overparameterized models with a massive number of parameters has become the norm in almost all machine learning applications. While these large models are successful in achieving low training error and often good generalization performance, due to their massive sizes, they are hard to store, communicate or infer on. Moreover, inference with such large models is computationally prohibitive which limits their application. 


To address these issues, a large effort has gone into compressing these overparameterized models via different approaches, such as quantization schemes \citep{krishnamoorthi_18,guo_18}, unstructured \citep{han2015} and structured \citep{he_2018,li_compression,kusupati_20} pruning mechanisms, and distillation techniques using student-teacher models \citep{https://doi.org/10.48550/arxiv.1503.02531,https://doi.org/10.48550/arxiv.1802.05668}. Among these approaches, \textit{greedy pruning}, in which we greedily eliminate the parameters of the trained model based on some measure of their size (e.g., the norms of the weight vectors associated with individual neurons) has received widespread attention \citep{deep_compression,han2015,li2016pruning,lebedev_15,wen_16,klivans}. In addition to learning compressed models, in practice, pruning and fine-tuning overparameterized models can also result in better generalization performance compared to models trained without any pruning \citep{prune_generalization}.

\paragraph{Greedy pruning pipeline.} The greedy pruning framework follows a $3$ step approach for training models:
\begin{enumerate}
    \item[(a)] Train an overparameterized model on the empirical loss (usually with some added regularization to promote sparsity).
    \item[(b)] Greedily prune the ``small'' weights/neurons based on some measure of their size/importance.
    \item[(c)] Fine-tune the resulting pruned model for a few iterations using gradient descent on the empirical loss.
\end{enumerate}

While the greedy pruning framework has shown impressive results, there is little to no theory backing why this pipeline works so well in practice. At a finer level, a phenomenon that has been observed in several practical studies, is that different forms of regularization while training the overparameterized model, such as $\ell_0$ or $\ell_1$ regularization   \citep{l0_reg,liu_slimming,ye_pruning} or $\ell_2$ regularization (group Lasso) \citep{SCARDAPANE201781} lead to models that are better suited for pruning, and lead to better generalization post fine-tuning. The role of regularization in helping generate models which are suitable for greedy pruning is also poorly understood. In this work, the main questions we ask are the following:


\begin{quote}
\vspace{-2mm}
  {\textit{Does the greedy pruning pipeline provably lead to a simple model with good generalization guarantees? What is the role of regularization in pruning?}}
  \vspace{-2mm}
\end{quote}

In this paper, we study the greedy pruning framework through the lens of the symmetric matrix sensing problem \citep{li_17}. In addition to being an interesting model on its own, when the underlying measurement matrices are rank $1$, it is known that symmetric matrix sensing with a factored model captures $1$-hidden layer neural networks with quadratic activation functions \citep{quadratic_networks,li_17}.

With i.i.d. Gaussian measurements, in the population setting, the expected square loss of the model is $\cL_{\text{pop}} (U) = \| UU^T - U_\star U_\star^T \|_F^2$, where $U_\star \in \mathbb{R}^{d \times r}$ is an unknown ground-truth rank-$r$ matrix, with $r$ also being unknown, and $U \in \mathbb{R}^{d \times k}$ is the overparameterized learning model  with $k \gg r$. In drawing the connection to $1$-hidden layer neural networks with quadratic activation functions, under rank $1$ measurements, the columns of $U$ correspond to the weight vectors associated with individual neurons.  Thus, the data generating model has $r$ neurons, while the learner trains an overparameterized model with $k$ neurons. More discussion is provided in Section~\ref{sec:quad}.

While the statistical and computational complexity of the overparameterized matrix sensing problem has been studied extensively, we use it to study the performance of greedy pruning. In particular, we aim to answer the following questions:  Does there exist a simple pruning criteria for which we can \textit{provably} show that the pruned model generalizes well after fine-tuning \textit{while having the minimal necessary number of parameters}? Furthermore, what is the role of regularization during training in promoting models which are compatible with greedy pruning?


Our main contribution is to show that an appropriately instantiated greedy pruning pipeline not only recovers the correct ground-truth $U_\star U_\star^T$ approximately, but also learns solutions with the minimum number of columns, $r$, and automatically adapt to the sparsity of the ground truth model. In particular, we show that training an overparameterized model $U$ on the empirical mean squared error with an added regularizer to promote column sparsity, followed by a simple norm-based pruning strategy learns a model $U_{\text{prune}}$ which has exactly the minimum number of columns, $r$. In addition, the pruned solution satisfies the property that $\| U_{\text{prune}} U_{\text{prune}}^T - U_\star U_\star^T \|_F^2$ is small, but non-zero. The pruned model is subsequently fine-tuned using a small number of gradient steps, and in this regime, $\| U_t U_t^T - U_\star U_\star^T \|_F^2$ shows linear convergence to a statistical error floor. The generalization bounds achieved by the terminal fine-tuned solution are also shown to be statistically minimax optimal, scaling as $O (\sqrt{rd/n})$ when $n$ is sufficiently large.


The regularizer we use in the initial training phase is based on the $L_2$ group Lasso \cite{grouplasso} and is a special case of the popular Structured Sparsity Learning (SSL) regularizer \cite{ssl}. A priori, however, it is unclear whether an explicit regularizer to promote column sparsity is even necessary in order to learn models suitable for greedy pruning. In particular, it is known that gradient descent starting from a small initialization, also known as implicit regularization, suffices to learn models which generalize well for matrix sensing \citep{li_17,ye_2021}. We show a negative result that this approach in fact learns a solution with a large number of columns having $\ell_2$-norms comparable to that of the maximum, even if $r=1$. It is therefore unclear how to prune such models based on the norms of their columns. Moreover, in the absence of any pruning at all, the best known generalization bound for vanilla gradient descent is known to scale as $O(\sqrt{kd/n})$, which degrades with the degree of overparameterization, which is also observed empirically \Cref{fig:2}.

\subsection{Outline}

In \Cref{sec:prob-def}, we first introduce the problem formulation and discuss related work. Subsequently, in \Cref{sec:negative}, we show a negative result - the model obtained by minimizing the \textit{unregularized} mean square error using gradient descent updates could fail to learn a solution which is suitable for greedy pruning. In \Cref{sec:pop}, we study the second-order stationary points of the mean square error with an added group Lasso based regularizer (\cref{eq:regloss}) in the population setting. In \Cref{sec:finite_sample}, we extend this analsysis to the finite sample setting under the presence of RIP measurements. In \Cref{sec:oracle} we discuss implementing algorithms for finding second-order stationary points of the regularized loss, required in the initial training phase of the learner. Finally in \Cref{sec:quad} we provide a discussion on extensions to neural networks with quadratic activation functions. In \Cref{sec:conc} we conclude the paper and discussion some open questions.


\section{Problem definition} \label{sec:prob-def}

In the matrix sensing framework, given $n$ observation matrices $\{A_i\}_{i=1}^n$, the learner is provided measurements $y_i = \langle A_i, U_\star U_\star^T \rangle + \varepsilon_i$ where $\langle \cdot,\cdot \rangle$ indicates the trace inner product and $\varepsilon_i$ is measurement noise assumed to be distributed i.i.d. $\sim \mathcal{N} (0,\sigma^2)$\footnote{All proofs in the paper go through as long as the noise is i.i.d. sub-Gaussian with variance proxy $\sigma^2$. We study the Gaussian case for simplicity of exposition.}. Here $U_\star \in \mathbb{R}^{d \times r}$ is the unknown parameter, and the rank $r \le d$ is unknown. To normalize the scale, we assume that $\| U_\star \|_{\text{op}} \le 1$. Furthermore, let $\{ \sigma_i^\star \}_{i \in [r]}$ be the non-zero singular values of $U_\star$ and $\kappa = \sigma_1^\star / \sigma_r^\star$ be its condition number. Informally, the goal of the matrix sensing problem is to learn a candidate matrix $X$ such that $X \approx U_\star U_\star^T$. In this paper, we study the setting where $X$ is explicitly factored as $UU^T$ for $U \in \mathbb{R}^{d \times k}$ in the presence of overparameterization, where $k \gg r$. The empirical mean square error is,
\begin{equation} \label{eq:Lemp}
    \cL_{\text{emp}} (U) = \frac{1}{n} \sum_{i=1}^n \left(\langle A_i, UU^T \rangle - y_i \right)^2
\end{equation}
For the case that $A_i$'s are sampled entry-wise i.i.d.  $\mathcal{N}(0,1)$ and as $n \rightarrow \infty$, up to additive constants which we ignore, the population mean square error can be written down as,
\begin{equation} \label{eq:Lpop}
    \cL_{\text{pop}} (U) = \|UU^T - U_\star U_\star^T\|_F^2.
\end{equation}
The objective of the learner is to recover a solution $U$ which generalizes well, in that $\cL_{\text{pop}} (U)$ is minimized, and at the same time, $U$ has approximately the minimum number of non-zero columns to be able to achieve this, $r$.

There is an extensive literature on how to efficiently learn the right product $U_\star U_\star^T$ in both finite sample and population settings \citep{seewong,park2017non}. In particular, there are several works on the efficiency of  gradient-based methods with or without regularization for minimizing the population loss \citep{li_17,constantine21}. While these approaches are guaranteed to approximately learn the correct product, $UU^T \approx U_\star U_\star^T$, in the overparameterized setting, the obtained solutions are not column sparse, i.e. having the number of non-zero columns being $\approx r$, up to constant or polylogarithmic factors. As a result, to recover a solution with the correct column sparsity from the obtained solution denoted $U_{\text{out}}\in \mathbb{R}^{d \times k}$, one has to do a principle component analysis (PCA), an operation which is costly and impractical in high-dimensional settings.


\paragraph{Algorithmic framework.}
In this paper, we instantiate the greedy pruning framework in the following manner. In the initial phase, the training loss we consider is the empirical risk augmented with a group Lasso based regularizer \citep{grouplasso,SCARDAPANE201781},
\begin{equation} \label{eq:regloss}
    \cL_{\text{emp}} (U) + \lambda \cR (U),\qquad  \text{where}\quad  \cR (U) = \sum_{i=1}^{k} \| U e_i \|_2,
\end{equation} 
where $\lambda>0$ is a regularization parameter. The particular choice of the regularizer $\cR$ is a special case of a widely used approach commonly known as \textit{Structured Sparsity Learning} (SSL) \citep{wen_16} used while pruning neural networks. This regularizer promotes sparsity across ``groups'' as discussed by \cite{grouplasso}. Here, the groups correspond to the columns of $U$. Formally, we study a smooth variant of the loss in \cref{eq:regloss}.

After training the overparameterized model, the columns below a certain $\ell_2$-norm threshold are removed, resulting in a model with fewer columns, denoted $U_{\text{prune}}$. We show that when the hyperparameters are chosen appropriately, the pruned model $U_{\text{prune}}$ is not only reasonably close to the ground-truth model $U_{\star}$ in terms of population loss, but also has exactly $r$ columns. Subsequently, fine-tuning this model by running a few gradient updates on the mean square error, it can be ensured that the iterates $U_t U_t^T$ converges to $U_{\star} U_\star^T$ at a linear rate. \Cref{alg:main} summarizes the framework that we study.

\subsection{Related work}

\paragraph{Matrix sensing.} A number of algorithms have been proposed in the literature for solving matrix sensing and related problems such as matrix factorization and matrix completion. For unfactored models, the most popular algorithms are variants of nuclear norm regularization \citep{RECHT,hu2012fast}. For factored models, the non-convex loss is often minimized by alternating minimization (ADMM) \cite{jain2013low,hardt2014fast,sun2016guaranteed}, or gradient based methods \citep{wainwright2015,impreg,ma2021beyond,ye2021global,jin2023understanding}. The use of regularization for matrix sensing and related problems has been well-studied in the literature. Regularization may serve many purposes - to encourage low rankness in the solution  \citep{cai_10,meka_09,keshavan_09,ma_09,recht_09,RECHT,toh_2010}, to control the norm of the model and improve the landscape of the loss by eliminating spurious local minima \citep{ge_16, bhojanapalli_16,rong,park2017non}. While these approaches implicitly or explicitly regularize for the rank of the learned matrix, the solution learned as a result is often not column sparse. Indeed, note that a matrix can be low rank and dense at the same time, if many columns are linear combinations of the others. 
 

\paragraph{Greedy Pruning.} In the literature, various criteria have been proposed for greedy pruning. Magnitude-based approaches prune away the small weights/neurons based on some measure of their size, such as the $\ell_1/\ell_2$ norm of the associated vectors \citep{liu_slimming,li2016pruning}, average gradient norms \citep{yu2018nisp,lee2018snip}, or (approximations to) the change in the training loss after removing the weight/neuron \citep{lecun1989optimal,hassibi1993optimal}. Across a variety of domains, it has been shown that pruning and fine-tuning overparameterized models generalize far better than small but dense models with the same memory footprint \cite{zhu2017prune}. These approaches have been used across diverse architectures as well, including Convolutional Neural Networks \cite{li2016pruning,he2017channel} and Transformer models \cite{wang2019-structured,voita-etal-2019-analyzing}.



\section{Implicit regularization does not lead to greedy pruning-friendly models}  \label{sec:negative}




In various overparameterized learning problems, it has been shown that first-order methods, starting from a small initialization, implicitly biases the model toward ``simple'' solutions, resulting in models that generalize well \citep{neyshabur_14,https://doi.org/10.48550/arxiv.1710.10345}, a phenomenon known as implicit regularization. In particular, for the matrix sensing problem, \citep{gunasekar2017implicit,impreg,ye_2021} show that in the absence of any regularization, running gradient descent on the population loss \textit{starting from a small initialization} biases $UU^T$ to low rank solutions, and learns the correct outer product $UU^T \approx U_\star U_\star^T$. However, as discussed earlier, low-rank solutions are not necessarily column sparse, nor is it clear how to sparsify them without computing a PCA. It is a-priori unclear whether implicit regularization suffices to learn models that are also amenable for greedy pruning. 

\begin{figure}[t!]
\begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{img/fig1a.png}
    \caption{For $x \in [0,1]$ along the horizontal axis, the vertical axis represents the fraction of columns having $\ell_2$ norm at least $x$ times the largest $\ell_2$ norm across columns.}
\end{subfigure}\quad
\begin{subfigure}[t]{0.51\textwidth}
    \centering
    \includegraphics[width=\textwidth]{img/fig1b.png}
    \caption{A histogram of the $\ell_2$ norms of columns.}
\end{subfigure}
    \caption{\small \textbf{Comparison of the sparsity of models trained with and without regularization.} We run gradient updates to minimize the unregularized loss in \cref{eq:Lemp} (blue) and group Lasso regularized loss in \cref{eq:regloss} (orange) until convergence in the population setting. In this figure, $d=k=500$ and $r=4$, resulting in a highly overparameterized learner. 
    Using regularization leads to solutions with most of the columns having small $\ell_2$ norm, and only a few significant columns. Without regularization, a large number of columns have their $\ell_2$ norm comparable to the largest column norm.}
    \label{fig:1}
\end{figure}

In this section, we address this question and show that minimizing the unregularized population loss $\cL_{\text{pop}}$ leads to models which are not suitable for greedy pruning, i.e., have many columns with large $\ell_2$ norm. Specifically, we show that by running gradient flow from a small random initialization, even if the ground truth $U_\star$ is just a single column and $r=1$, the learnt solution $U$ has a large number of columns that are ``active'', i.e. having $\ell_2$-norm comparable to that of the column with maximum norm. Thus, in the absence of the knowledge of $r$, it is unclear how to determine it from just observing the  columns $\ell_2$ norm. We thus claim that such trained models are not compatible with greedy pruning. The formal result is stated below.


\begin{theorem}
\label{theorem:lb-implicit}
Consider the population loss in \cref{eq:Lpop}, for  $r=1$ and $k\gg 1$. Moreover, assume that the entries of the initial model $U_0$ are i.i.d. samples from $\mathcal{N} (0,\alpha^2)$, where $\alpha \le c_{\ref{theorem:lb-implicit}} /k^3 d \log(kd)$ for some absolute constant $c_{\ref{theorem:lb-implicit}} > 0$. For another absolute constant $c_{\ref{theorem:lb-implicit}}' > 0$, as $t \to \infty$, the iterates of gradient flow converge to a model $U_{gd}$ where $\widetilde{\Omega} (k^{c_{\ref{theorem:lb-implicit}}'})$ active columns which satisfy,
\begin{align} \label{eq:123121112}
    \frac{\| U_{gd} e_i \|_2}{\max_{j \in [k]} \| U_{gd} e_j \|_2} \ge 0.99,
\end{align}
 with probability $\ge 1 - O(1/k^{c_{\ref{theorem:lb-implicit}}'})$.
\end{theorem}



Note that the constant $0.99$ in the result of \Cref{theorem:lb-implicit} is arbitrary and can be extended to any constant on the RHS bounded away from $1$ for appropriately small choices of $c_1$ and $c_1'$, and a similar guarantee can be established when in the LHS of \cref{eq:123121112}, the $\ell_2$ norm is replaced by the $\ell_p$ norm for any $p \ge 1$. We provide the general guarantee in \Cref{theorem:lb-implicit-restate} (\Cref{app:1}). It is worth noting that from \citep{li_17} that the learned model $U_{gd} \in \mathbb{R}^{d\times k}$ is guaranteed to achieve a small generalization error, i.e., $U_{gd}U_{gd}^T\approx U_\star U_\star^T$. Hence, the algorithm does learn the correct ground-truth product; what we show is that the learnt model cannot be sparsified to the correct level by pruning away all the columns below any constant threshold on their $\ell_2$ norm. Indeed, this is also observed in \Cref{fig:1}: the model trained with implicit regularization has many significant columns and it is unclear how many to prune. Training on the empirical loss augmented by the group Lasso regularizer (\cref{eq:regloss}) results in a much sparser solution.



\paragraph{Proof sketch of \Cref{theorem:lb-implicit}.} Without loss of generality we assume in the proof that $\|U_\star\|_2 =1$. Defining $U(t)$ as the iterate at time $t$, gradient flow on $\cL_{\text{pop}}$ follows the below dynamic,
\begin{align} \label{eq:gradflow-main}
    \frac{dU}{dt} = -\nabla \cL_{\text{pop}} (U) = - (UU^T - U_\star U_\star^T) U,
\end{align}
where for ease of notation we drop the explicit dependence on $t$ in $U(t)$. In the proof of this result, we exactly characterize the limiting point of gradient flow as a function of the initialization in a certain sense, which may be of independent interest. In particular, up to a small multiplicative error dependent on the size of the initialization,
\begin{align}
    \label{eq:similar-dist-main}
    \forall i \in [k],\ \| U_{\text{gd}} e_i \|_2 \approx \frac{|\langle U_\star, U(0) e_i \rangle|}{\| U_\star^T U(0) \|_2}.
\end{align}
Up to a normalization factor, the RHS is the correlation of $U_\star$ with the $i^{th}$ column of the gradient descent iterate at initialization. By the Gaussian initialization, $\langle U_\star, U(0) e_i \rangle$ is independent and identically distributed as $\mathcal{N} (0,\alpha^2)$ across different values of $i \in [k]$. Since the random variables are i.i.d. across $i$, we expect no single $\langle U_\star, U(0) e_i \rangle$ to be larger than the others. In particular, we show that for some absolute constant $c_{\ref{theorem:lb-implicit}}' > 0$,\ $m = \widetilde{\Omega}(k^{c_{\ref{theorem:lb-implicit}}'})$ columns $i \in [k]$ will have correlations comparable to the maximum, with $|\langle U_\star, U(0) e_i \rangle| \ge 0.99 \cdot \max_{j \in [k]} |\langle U_\star, U(0) e_j \rangle|$. For any of these columns, $i \in [k]$,
\begin{align}
    \| U_{\text{gd}} e_i \|_2 \approx \frac{|\langle U_\star, U(0) e_i \rangle|}{\| U_\star^T U(0) \|_2} \ge 0.99 \max_{j \in [k]} \frac{|\langle U_\star, U(0) e_j \rangle|}{\| U_\star^T U(0) \|_2} \approx 0.99 \max_{j \in [k]} \| U_{\text{gd}} e_j \|_2.
\end{align}
This shows that $m \gg 1$ columns of $U_{\text{gd}}$ have $\ell_2$ norms comparable with the maximum, resulting in the statement of \Cref{theorem:lb-implicit}.


To understand why \cref{eq:similar-dist-main} is true, it is informative to look at the gradient flow equation in more detail. Define $r(t) = (U(t))^T U_\star$, which captures the ``signal component'', the alignment of the columns of $U(t)$ with the correct subspace $U_\star$, and $E(t) = (I - U_\star U_\star^T ) U(t)$ as the orthogonal (error) component. From \cref{eq:gradflow-main}, $r(t)$ and $E(t)$, abbreviated to $r$ and $E$ can be shown to evolve per the following differential equations,

\vspace{-1.5em}
\begin{minipage}[t]{.47\linewidth}
\begin{align}
    \frac{d r}{dt} &= r(1 - \| r \|_2^2) - E^T E r. \label{eq:rtdiff-main}
\end{align}
\end{minipage}
\begin{minipage}[t]{.47\linewidth}
\begin{align}
    \frac{d \|E\|_F^2}{dt} \le 0 \label{eq:Etdiff-main}
\end{align}
\end{minipage}
\vspace{1em}

\noindent With a small initialization, by \cref{eq:Etdiff-main}, the error term $\| E(t) \| \le \| E(0) \|_F \approx 0$ remains small throughout training. This means that $\| U(t) e_i \|_2 \approx |\langle r(t), e_i \rangle|$ and therefore it suffices to study the trajectory of $ |\langle r(t), e_i \rangle|$ as a function of $t$. Since $\| E \|_F$ is small, by \cref{eq:rtdiff-main}, $\frac{dr}{dt} \approx r ( 1 - \| r \|_2^2)$, which shows linear convergence of $r$ until the point when $\| r \|_2$ approaches very close to $1$. Therefore, 
\begin{equation} \label{eq:rtei}
    \langle r(t),e_i \rangle \approx \langle r(0), e_i\rangle e^t
\end{equation}
until some time $T_0$ when $\|r(t)\|_2$ gets sufficiently close to $1$. This time $T_0$ can be shown to be approximately $-\log \| r(0) \|_2$. Plugging into \cref{eq:rtei} results in the bound,
\begin{align}
    \| U(T_0) e_i \|_2 \approx \langle r(T_0),e_i \rangle \approx \langle r(0), e_i\rangle e^{T_0} \approx \frac{\langle r(0), e_i\rangle}{\| r (0) \|_2}.
\end{align}
After this time $T_0$, it can be shown that gradient flow does not change $\| U(t) e_i \|_2$ significantly.
$\hfill \blacksquare$

\vspace{3mm}
The conclusion of this section is that greedy pruning is not compatible with models trained with implicit regularization. In the next section, we discuss positive results, showing that greedy pruning can be shown to work with the right regularization strategy in the training phase.


\section{Explicit regularization for pruning-friendly models: population analysis} \label{sec:pop}

In this section, we study the properties of the square error augmented with group Lasso regularization in the population (infinite sample) setting. We show that SOSPs of the regularized loss are suitable for greedy pruning, while at the same time achieving a small but non-zero generalization error.

Note that the $\ell_2$ norm is a non-smooth function at the origin, and therefore, the overall regularized loss is non-smooth and non-convex. While there are several notions of approximate stationary points for non-differentiable and non-convex functions, for technical convenience, we replace $\cR$ by a smooth proxy. In particular, for a smoothing parameter $\beta > 0$, define a smooth version of the $\ell_2$ norm, and the corresponding smooth regularizer $\cR_\beta$ as,
\begin{align} \label{eq:reg-smooth}
    \cR_\beta (U) = \sum_{i=1}^{k} \Lsm (Ue_i), \quad \text{ where} \quad \Lsm (v) = \frac{\| v \|_2^2}{\sqrt{\| v \|_2^2 + \beta}}.
\end{align}
Note that the smaller the value of $\beta$ is, the closer is $\ell_2^\beta (v)$ to $\| v \|_2$. Considering this definition, the overall regularized loss we study in the population setting is
\begin{align}
    &f_{\text{pop}} (U) = \cL_{\text{pop}} (U) + \lambda \cR_\beta (U), \label{eq:fregpop}
\end{align}
where $\cL_{\text{pop}} = \| UU^T - U_\star U_\star^T \|_F^2$ as defined in \cref{eq:Lpop}. 

The above optimization problem is nonconvex due to the structure of $\cL_{\text{pop}} (U)$, and finding its global minimizer is computationally prohibitive. Fortunately, for our theoretical results, we do not require achieving global optimality and we only require an approximate second-order stationary point of the loss in \cref{eq:fregpop}, which is defined below. 

\begin{definition} \label{def:init-pop}
 $U$ is an $(\epsilon, \gamma)$-approximate second-order stationary point (SOSP) of $f$ if,
\begin{enumerate}
\vspace{-1mm}
    \item The gradient norm is bounded above by $\epsilon$, i.e.,  $\| \nabla f(U) \|_2 \le \epsilon$.
    \vspace{-1mm}
    \item The eigenvalues of the Hessian are larger than $-\gamma$, i.e., $\lambda_{\min}(\nabla^2 f(U)) \ge - \gamma$.
\end{enumerate}
\end{definition} 

\noindent The full algorithmic procedure that we analyze in this section is summarized in \Cref{alg:main}. 
Once we find a $(\epsilon, \gamma)$-approximate second-order stationary point $U$ of $f_{\text{pop}}$ (\cref{eq:fregpop}) with appropriately chosen $\epsilon $ and $\gamma$, we apply greedy pruning on the obtained model $U$ by eliminating all of its columns with norm less than a specific threshold as mentioned in Step 3 of \Cref{alg:main}. As a result, the pruned model $U_{\text{prune}}$ has fewer than $k$ columns. 
In fact, in our following theoretical results, we show that if the parameters are properly selected $U_{\text{prune}}$ can be shown to have exactly $r$ columns, which is the same as $U_\star$. Finally, we fine-tune the pruned solution by running gradient descent on the loss function without any added regularization. Note that \Cref{alg:main} requires a subroutine which finds an SOSP $U$ of $f_{\text{pop}}$ \emph{which is also bounded}. We discuss this point in more detail later in \Cref{sec:oracle}, but in summary, a number of perturbed first-order methods can be used to instantiate this subroutine.

Next, we state the properties of the pruned model generated by \Cref{alg:main}, in the population case. 

\begin{algorithm}[t!]
\small
\hspace*{\algorithmicindent} \textbf{Inputs}: Measurements $\{ (A_i,y_i) \text{ where } y_i 
 = \langle A_i, U_\star U_\star^T \rangle + \varepsilon_i \}_{i=1}^n$ (in the population setting $n=\infty$)  \\
\hspace*{\algorithmicindent} \textbf{Initialization}: Set parameters $\lambda,\beta,\epsilon,\gamma$ 
\begin{algorithmic}[1] 

\vspace{0.4em}
\item[] \Comment{Greedy pruning phase:}

\State Find an $(\epsilon,\gamma)$-approximate SOSP of $f_{\text{emp}}$ (resp. $f_{\text{pop}}$) satisfying $\| U \|_{\text{op}} \le 3$.

\State Let $S = \{ i \in [k] : \| U e_i \|_2 \le \max \{ 2 \sqrt{\beta}, \epsilon k \}\}$ denote the set of columns with small $\ell_2$ norm.

\State Delete column $i$ of $U$ for $i \not\in S$. Let the resulting solution be denoted $U_{\text{prune}}$.

\vspace{0.5em}
\item[] \Comment{Fine-tuning phase:}

\item[{\footnotesize 1:}]  Run  a few iterations of gradient descent on $\cL_{\text{emp}}$ (resp. $\cL_{\text{pop}}$) intialized at $U_{\text{prune}}$ to get $U_{\text{out}}$
\item[] (gradient descent parameters provided in \Cref{theorem:fine-tuning_finite})

\vspace{0.5em}
\item[] \textbf{Return} $U_{\text{out}}$.

\end{algorithmic}
\caption{Greedy pruning based on group-Lasso regularization}

\label{alg:main}
\end{algorithm}


\vspace{2mm}
\begin{theorem} \label{theorem:main-population}
Consider the population loss with regularization in \cref{eq:fregpop}, where $U_\star$ has rank $r$ and its smallest singular value is denoted by $\sigma_r^\star$.
Let $U_{\text{prune}}$ be the output of the pruning phase in \Cref{alg:main} with  parameters $\beta, \lambda, \epsilon, \gamma$ satisfying $\beta \le c_\beta (\sigma_r^\star)^2/k$, $\lambda \le c (\sigma_r^\star)^2 \sqrt{\beta/k}$, $\gamma \le c_\gamma \lambda/r^2$ and $\epsilon \le c \min \{ (\sigma_r^\star)^3/k^{1/4} , \sigma_r^\star/k^{3/2}, \sqrt{\beta^{1/2} \lambda / k r^2} , \lambda/r^2 \}$ for absolute constants $c_\beta,c_\lambda,c_\epsilon, c_\gamma$. Then,
\begin{enumerate}
    \item $U_{\text{prune}}$ has exactly $r$ columns.
    \item $\| U_{\text{prune}} U_{\text{prune}}^T - U_\star U_\star^T \|_F \le \frac{1}{2} (\sigma_r^\star)^2$.
\end{enumerate}
\end{theorem}

This result implies that all bounded SOSPs of the regularized loss in \cref{eq:fregpop} are suitable for greedy pruning: removing the columns of $U$ below a certain $\ell_2$-norm threshold results in a solution $U_{\text{prune}}$ having exactly $r$ columns, while at the same time having a reasonably small (but non-zero) generalization error. Hence, it serves as a warm-start for the fine-tuning phase. 


\paragraph{Proof sketch of \Cref{theorem:main-population}.} The key idea in the proof relies on the following fact: consider a matrix $\widehat{U}$ such that $\widehat{U} \widehat{U}^T = U_\star U_\star^T$, and the columns of $\widehat{U}$ are orthogonal to one another. Then $\widehat{U}$ has exactly $r $ non-zero columns. This statement can be shown to hold even when $\widehat{U} \widehat{U}^T \approx U_\star U_\star^T$ and the columns of $\widehat{U}$ are only approximately orthogonal.

The main observation we prove is that a bounded $(\epsilon,\gamma)$-approximate SOSP of \cref{eq:fregpop} denoted $U$, satisfies the following condition:
\begin{equation} \label{eq:apxorth}
    \forall i,j : \| U e_i \|_2, \| U e_j \|_2 \ge 2 \sqrt{\beta},\quad \frac{\langle U e_i, U e_j \rangle}{\| U e_i \|_2 \| U e_j \|_2} \approx 0.
\end{equation}
In other words, all the large columns of $U$ have their pairwise angle approximately $90^\circ$. This is an even stronger condition that orthogonality when the columns of $U$ are bounded in norm, which is guaranteed by the boundedness condition $\| U \|_{\text{op}} \le 3$ on $U$. Thus, by pruning away the columns of $U$ that have an $\ell_2$ norm less than $2 \sqrt{\beta}$, the remaining columns of $U$, i.e., the columns of $U_{\text{prune}}$, are now approximately at $90^\circ$ angles to one another. If $\beta$ is chosen sufficiently small, even after deleting the low-norm columns, the approximation $U_{\text{prune}} U_{\text{prune}}^T \approx UU^T$. Moreover, noting that $U$ itself was an approximate SOSP to begin with, if $\lambda$ is sufficiently small, we can establish that $UU^T \approx U_\star U_\star^T$. Together, these facts imply that $U_{\text{prune}} U_{\text{prune}}^T \approx U_\star U_\star^T$ and at the same time, the columns of $U_\text{prune}$ are approximately orthogonal to one another. Together, these facts imply that $U_{\text{prune}}$ has exactly $r$ columns.

In order to establish a  bound on the generalization error, we simply use the triangle inequality that $\| U_{\text{prune}} U_{\text{prune}}^T - U_\star U_\star^T \|_F \le \| UU^T - U_\star U_\star^T \|_F + \| UU^T - U_{\text{prune}} U_{\text{prune}}^T \|_F$. The first term on the RHS is small by virtue of the fact that $U$ is an approximate SOSP and since $\lambda$ is small; the second term on the LHS is small by the fact that only the small norm columns of $U$ were pruned away.

Now the only missing part that remains to justify is why \cref{eq:apxorth} holds and why the $\cR_\beta$ regularizer promotes orthogonality in the columns of approximate SOSPs. This is best understood by looking at the regularized loss for the case $\beta = 0$, which is equivalent to $\| UU^T - U_\star U_\star^T \|_F^2 + \lambda \sum_{i=1}^k \| U e_i \|_2$. Consider any candidate first-order stationary point $U$ and any variable $\tilde{U}$ satisfying the constraint $\tilde{U} \tilde{U}^T = UU^T$. Note that $\tilde{U} = U$ must also be a first-order stationary point of the constrained optimization problem,
\begin{align}
    \text{Minimize: } \| \tilde{U} \tilde{U}^T - U_\star U_\star^T \|_F^2 + \lambda \sum_{i=1}^k \| \tilde{U} e_i \|_2, \label{eq:constopt} \qquad \text{Subject to: } \tilde{U} \tilde{U}^T = U U^T. 
\end{align}
The first term in the objective is a constant under the constraint and we remove it altogether. When $U$ is a full-rank stationary point, constraint qualification holds, and it is possible to write down the necessary KKT first-order optimality conditions, which reduce to,
\begin{align} \label{eq:kkt}
    \forall i \in [k], \quad - \lambda \frac{\tilde{U} e_i}{\| \tilde{U} e_i \|_2} + (\Lambda + \Lambda^T) \tilde{U} e_i = 0
\end{align}
where $\Lambda \in \mathbb{R}^{d \times d}$ is the set of optimal dual variables. Since $\tilde{U} = U$ is a first-order stationary point of the problem in \cref{eq:constopt} and it satisfies \cref{eq:kkt}, the above condition means that the columns $U e_i$ are the eigenvectors of the symmetric matrix $\Lambda + \Lambda^T$. If all the eigenvalues of $\Lambda + \Lambda^T$ were distinct, then this implies that the eigenvectors are orthogonal and $Ue_i \perp U e_j$ for all $i \ne j$.  $\hfill \blacksquare$

While this analysis conveys an intuitive picture, there are several challenges in implementing it formally. First and foremost, it is unclear how to establish that the eigenvalues of $\Lambda + \Lambda^T$ are distinct. Secondly, this analysis only applies for full-rank stationary points and does not say anything about rank deficient stationary points, where constraint qualification does not hold. It is even more unclear how to extend this analysis to approximate stationary points.

Our proof will circumvents each of these challenges by $(a)$ showing that at approximate SOSPs, even if the eigenvalues of $\Lambda + \Lambda^T$ are not distinct, the columns of $U$ are orthogonal, and $(b)$ directly bounding the gradients and Hessians of the regularized loss, rather than studying the KKT conditions to establish guarantees which hold even for approximate stationary points which may be rank deficient. Having established guarantees for the pruning phase of \Cref{alg:main} in the population setting, we next prove a result in the finite sample setting.

\section{Finite sample analysis} \label{sec:finite_sample}

Next, we extend the results of the previous section to the finite sample setting. Here, we also focus on the smooth version of the regularizer and study the following problem 

\begin{equation}
    f_{\text{emp}} (U) = \cL_{\text{emp}} (U) + \lambda \cR_\beta (U), \label{eq:fregemp}
\end{equation}
where the empirical loss $\cL_{\text{emp}} $ is defined in \cref{eq:Lemp} and the smooth version of the group Lasso regularizer $\cR_\beta (U)$ is defined in \cref{eq:reg-smooth}. Prior to introducing the main result, we impose that the measurement matrices satisfy the restricted isometry property (RIP).

\begin{assumption} \label{assump:RIP}
Assume that the measurement matrices $\{ A_1,\cdots,A_m\}$ are $( \max \{ 2k, 4r \},\delta)$-RIP for any $\delta \le 1/12$. In other words, for any $d \times d$ matrix with rank $\le \max \{ 2k,4r \}$,
\begin{equation}
    \frac{11}{12} \| X \|_F^2 \le \frac{1}{m} \sum_{i=1}^m \langle A_i, X \rangle^2 \le \frac{13}{12} \| X \|_F^2.
\end{equation}
With Gaussian measurements ($A_i \sim \mathcal{N}(0,1)$ entry-wise), \Cref{assump:RIP} is satisfied with probability $\ge 1 - e^{-\Omega (d)}$ as long as $n \gtrsim kd \log (d)$. \citep{RECHT}.
\end{assumption}

\noindent Below, we state the result in the case when $\sigma_r^\star$ is approximately known to make the dependency on $\sigma_r^\star$ explicit. In \Cref{def:parameter} (\Cref{app:finite}), we explain the parameter selection when $\sigma_r^\star$ is unknown.



\vspace{2mm}
\begin{theorem} \label{theorem:main_finite}
Consider the empirical loss with smooth regularization in \cref{eq:fregemp}, where $U_\star$ has rank $r$ and its
smallest singular value is denoted by $\sigma_r^\star$, and the noise variance is denoted by $\sigma^2$. Let $U_{\text{prune}}$ denote the output of the pruning phase in \Cref{alg:main} with parameters $\beta, \lambda, \epsilon, \gamma$, satisfying the conditions $\lambda = c_\lambda (\sigma_r^\star)^3 / r^2 k^{7/2}$ and $\beta = c_\beta (\sigma_r^\star)^2 / k$,
and choose any $\epsilon \le c_\epsilon (\sigma_r^\star)^3 / r^4 k^4$ and $\gamma \le c_\gamma (\sigma^\star_r)^3 / r^4 k^{7/2}$ for some absolute constants $c_\beta,c_\lambda,c_\epsilon, c_\gamma > 0$. If \Cref{assump:RIP} holds and the number of samples is at least   $n \ge C_{\ref{theorem:main_finite}} \frac{\sigma^2 d \log(d/\eta) \cdot k^6 r^3}{(\sigma^\star_r)^4}$, where $C_{\ref{theorem:main_finite}} > 0$ is a sufficiently large constant, then with probability at least $ 1 - \eta$,

\begin{enumerate}
    \item $U_{\text{prune}}$ has exactly $r$ columns.
    \item $U_{\text{prune}}$ satisfies the spectral initialization condition:
    $
        \| U_{\text{prune}} U_{\text{prune}}^T - U_\star U_\star^T \|_F \le \frac{1}{2} (\sigma_r^\star)^2
        $.
\end{enumerate}
\end{theorem}
\noindent Given Gaussian measurements, the sample requirement in the RIP condition \Cref{assump:RIP} of $n = \widetilde{\Omega} (kd)$ and \Cref{theorem:main_finite} of $n = \widetilde{\Omega} \left(\frac{\sigma^2 d \cdot \poly{k,r}}{(\sigma_r^\star)^4} \right)$ grow linearly in the input dimension $d$. Improving the dependency on $k,r$ and $\sigma_r^\star$ in \Cref{theorem:main_finite} is left for future work. The high level analysis of this result largely follows that of \Cref{theorem:main-population} in the population setting -- we approximate the finite-sample gradient and Hessian by their population counterparts and show that the approximation error decays with the number of samples as $O(1/\sqrt{n})$. 



\subsection{Fine-tuning phase: The benefit of pruning}

We next give the guarantee for the fine-tuning phase of the algorithm. Since the pruned model is exactly specified (i.e. has $r$ columns) and is no longer overparameterized, there are several works analyzing the generalization performance and iteration complexity of gradient descent. The only condition we need to verify is that the pruned model $U_{\text{prune}}$ satisfies the local condition needed for linear convergence of gradient descent when applied to the (non-regularized) empirical loss empirical loss $\cL_{\text{emp}} $. Here we borrow the result of \cite{wainwright2015} which requires the initial condition that $\|U_0U_0^T-U_\star U_\star^T\|_F \leq  c (\sigma_r^\star)^2$, where $c$ is any constant less than $1$. As shown in part 2 of  Theorem~\ref{theorem:main_finite}, this initial condition is satisfied by $U_{\text{prune}}$.

\vspace{2mm}
\begin{theorem}{\cite[Corollary 2]{wainwright2015}} \label{theorem:fine-tuning_finite}
Let the output of the greedy pruning stage in \Cref{alg:main}, $U_{\text{prune}} \in \mathbb{R}^{d \times r}$ be the initial iterate for the fine-tuning phase, and assume that $U_{\text{prune}}$ satisfies the properties in \Cref{theorem:main_finite}. 

Running gradient descent (with iterates $\{ U_t \}_{t \ge 0}$) on the unregularized loss $\cL_{\text{emp}}$ (\cref{eq:Lemp} with $k=r$) initialized at $U_0 = U_{\text{prune}}$, with step-size $\alpha = \frac{C_{\ref{theorem:fine-tuning_finite}}}{\kappa^{10} (\sigma_r^\star)^2}$ for an appropriate constant $C_{\ref{theorem:fine-tuning_finite}} > 0$. Under \Cref{assump:RIP}, for any $t \ge C_{\ref{theorem:fine-tuning_finite}}' \kappa^{10} \log \left(\frac{\kappa n}{d} \right)$,
\begin{align}
    \| U_t U_t^T - U_\star U_\star^T \|_F \lesssim \kappa \frac{\sigma}{\sigma_r^\star} \sqrt{\frac{rd}{n}},
\end{align}
where $C_{\ref{theorem:fine-tuning_finite}}' > 0$ is a sufficiently large absolute constant and $\kappa$ is the condition number $\sigma_1^\star/ \sigma_r^\star$.
\end{theorem}

\noindent  Combining \Cref{theorem:main_finite,theorem:fine-tuning_finite} results in a generalization bound for greedy pruning + fine-tuning.

\begin{corollary}
In the finite sample setting, consider greedy pruning + fine-tuning (\Cref{alg:main}) with appropriately initialized values of $\lambda, \beta, \gamma, \epsilon$ (see \Cref{theorem:main_finite}). With probability $1-\eta$, the resulting solution $U_{\text{out}}$ satisfies,
\begin{align}
    \| U_{\text{out}} U_{\text{out}}^T - U_\star U_\star^T \|_F \lesssim \kappa \frac{\sigma}{\sigma_r^\star} \sqrt{\frac{rd}{n}},
\end{align}
as long as \Cref{assump:RIP} holds and the number of samples is at least $n \ge C_{\ref{theorem:main_finite}} \frac{\sigma^2 d \log(d/\eta) \cdot k^6 r^3}{(\sigma^\star_r)^4}$, for a sufficiently large absolute constant $C_{\ref{theorem:main_finite}} > 0$. Here $\kappa$ is the condition number $\sigma_1^\star/ \sigma_r^\star$.
\end{corollary}


\label{sec:fine-tuning_finite_case}
\Cref{theorem:fine-tuning_finite} shows that in the exactly setting, the iterates of gradient descent converges at a linear rate to the generalization error floor of $O(\sqrt{rd/n})$, which is also known to be statistically minimax optimal \citep{https://doi.org/10.48550/arxiv.1009.2118,Koltchinskii2010}. This is enabled by the fact that in the pruning phase, the learner is able to correctly identify the rank of the model and operate in the exactly specified setting in the fine-tuning phase. In contrast, one may ask how this guarantee compares with running vanilla factored gradient descent in the overparameterized setting. This corresponds to the case where no pruning is carried out to reduce the size of the model. \cite{constantine21} presented a guarantee for the convergence of factored gradient descent from a warm start in the overparameterized setting. In comparison with the exactly specified case, the generalization error floor $\lim_{t \to \infty} \| U_t U_t^T - U_\star U_\star^T \|_F$ is shown to scale as $O(\sqrt{kd/n})$ which now depends on $k$ and deteriorates as the model becomes more overparameterized. Furthermore, linear convergence can no longer be established because of the ill-conditioning of the objective. This is not just an artifact of the proof - experimentally too, the convergence slowdown was first noticed in \cite[Figure 1]{constantine21}. We also validate these findings in \Cref{fig:2}.

This discussion shows that greedy pruning the model \textit{first}, prior to running gradient descent, in fact generates solutions which generalize better and also converge much faster.



\begin{figure}[t!]
    \centering
    \includegraphics[width=0.5\textwidth]{img/fig2a.png}
    \caption{\small \textbf{Comparison of generalization of vanilla gradient descent vs. greedy pruning + fine-tuning.} Vanilla gradient descent is carried out to minimize the unregularized loss in \cref{eq:Lemp} (red). The group Lasso regularized loss, \cref{eq:regloss}, is used in the initial training phase of greedy pruning + fine-tuning (blue). This is subsequently pruned based on column norms and fine-tuned using gradient descent on the unregularized loss. In this figure, $d=k=20$ and $r=3$ and the learner has access to $n=100$ Gaussian measurements with no added label noise. The figures plot the generalization error, $\| U_t U_t^T - U_\star U_\star^T \|_F^2$ as a function of the iteration number, $t$. The regularization parameter in \cref{eq:regloss} was tuned minimally for greedy pruning + fine-tuning objective. A sharp improvement in the generalization error is observed after pruning is carried out and the error floor is observed to be an order of magnitude better for the pruned + fine-tuned solution.
    }
    \label{fig:2}
\end{figure}

\section{Implementing \Cref{alg:main}: Smoothness and optimization oracles}
\label{sec:oracle}

Recall that \Cref{alg:main} requires access to a subroutine which outputs an approximate SOSP with bounded operator norm. In this section we show that a number of perturbed gradient first-order methods with a small initialization satisfy the conditions required by this optimization oracle. First, we establish that the loss $f_{\text{emp}}$ is well behaved on the domain $\{ U : \| U \|_{\text{op}} \le 3 \}$, in that its gradient and Hessian are Lipschitz continuous. These conditions are required by many generic optimization algorithms which return approximate SOSPs \citep{chi_escaping,NEURIPS2018_069654d5}. We establish these properties for the population loss for simplicity and leave extensions to the empirical loss for future work.

\vspace{2mm}
\begin{theorem} \label{theorem:lipgradhess}
Consider the population loss $f_{\text{pop}}$ in \cref{eq:fregpop} and assume that $\lambda \le \min \{ \beta, \sqrt{\beta} \}$. The objective $f_{\text{pop}} (\cdot)$ defined in \cref{eq:fregpop} satisfies for any $U,V \in \mathbb{R}^{d \times k}$ such that $\| U \|_{\text{op}} , \| V \|_{\text{op}} \le 3$,
\begin{enumerate}
    \item[(a)] Lipschitz gradients: $\| \nabla f_{\text{pop}} (U) - \nabla f_{\text{pop}} (V) \|_F \lesssim \| U - V \|_F$,
    \item[(b)] Lipschitz Hessians: $\| \nabla^2 f_{\text{pop}} (U) - \nabla^2 f_{\text{pop}} (V) \|_{\text{op}} \lesssim \| U - V \|_F$.
\end{enumerate}
\end{theorem}

\vspace{2mm}
Under the Lipschitz gradients and Lipschitz Hessian condition, a large number of algorithms in the literature show convergence to an approximate SOSP. A  common approach for finding such a point is using the noisy gradient descent algorithm \citep{chi_escaping}.
However, note that we establish these Lipschitzness properties on the bounded domain $\{ U : \| U \|_{\text{op}} \le 3 \}$, and it remains to verify whether these algorithms indeed approach such points. A similar concern is present with \Cref{alg:main}, which requires access to an optimization oracle which finds an $(\epsilon,\delta)$-approximate SOSP, $U$, of $f_{\text{emp}}$ which is also bounded, in that $\| U \|_{\text{op}} \le 3$. 
In the sequel we introuduce a class of perturbed first-order methods and identify conditions under which these algorithms indeed output stationary points which are bounded, satisfying $\| U \|_{\text{op}} \le 3$.


\paragraph{Perturbed first-order method:} We consider a perturbed version of gradient descent with the following update rule: starting from the initialization $U_0$, for all $t \ge 0$,
\begin{align} \label{eq:gd}
    U_{t+1} \gets U_t - \alpha ( \nabla (\cL_{\text{pop}} + \lambda \cR_\beta) (U_t) + P_t)
\end{align}
where $P_t$ is a perturbation term, which for example, could be the explicit noise $(P_t \sim \mathrm{Unif} (\mathbb{B} (r))$ for appropriate $r$) added to escape strict saddle points in \cite{chi_escaping}. Over the course of running the update rule \cref{eq:gd}, we show that $\| U_t \|_{\text{op}}$ remains bounded under mild conditions if the algorithm is initialized within this ball. In combination with \Cref{theorem:lipgradhess}, this shows that the noisy gradient descent approach of \citep{chi_escaping} can be used to find the SOSPs required for \Cref{alg:main}.

\begin{theorem} \label{theorem:bounded}
Consider the population setting. Suppose $\alpha \le 1/8$, $\lambda \le \sqrt{\beta}$ and $\| P_t \|_{\text{op}} \le 1$ almost surely for each $t \ge 0$. For the update rule \cref{eq:gd}, assuming that $\| U_0 \|_{\text{op}} \le 3$ at initialization, for every $t \ge 1$, $\| U_t \|_{\text{op}} \le 3$.
\end{theorem}

The implication of this result is that when perturbed gradient descent is run starting from a small initialization, the iterates always remain within the ball $\{ \| U \|_{\text{op}} \le 3\}$ at each time $t$, as long as the learning rate and perturbations are not too large. By the smoothness of the gradient and Hessian within this ball, the algorithm of \cite{chi_escaping} which is an instance of the perturbed gradient descent framework in \cref{eq:gd} can be used to find a bounded SOSP efficiently.

In the next section we provide an algorithm for the training and pruning of $2$-layer neural networks with quadratic activation functions.

\section{$2$-layer neural networks with quadratic activation functions}\label{sec:quad}

The connection of matrix sensing to shallow neural networks having quadratic activation functions was first observed in \citep{nn-quadratic-ms}. Indeed, when the measurement matrices are of the form $A_i = x_i x_i^T$ for some vector $x_i \in \mathbb{R}^d$, the functional representation of the output can be written as $\langle A_i, UU^T \rangle = \sum_{i=1}^k \sigma(\langle x_i, U e_i\rangle)$, where $\sigma (\cdot) = (\cdot)^2$ takes the form of a $1$-hidden layer shallow network with quadratic activations, with the weights of the output neuron clamped to $1$. The columns of $U$ correspond to the weight vectors associated with individual neurons of the network. Likewise, sparsity in the column domain corresponds to learning networks with only a few non-zero neurons. We focus on the population setting here, and mention that the same techniques can be extended to the finite-sample setting. By \cite[Theorem 3.1 (b)]{https://doi.org/10.48550/arxiv.1912.01599}, when $x_i \sim \mathcal{N}(0,I)$ are distributed i.i.d. Gaussian, in the population setting,
\begin{align}\label{eq:LNN}
    \cL_{\text{NN}} (U) = \mathbb{E} [ (\langle x x^T , U U^T - U_\star U_\star^T \rangle)^2 ] &= \left( \| U \|_F^2 - \| U_\star \|_F^2 \right)^2 + 2 \| UU^T - U_\star U_\star^T \|_F^2
\end{align}
A similar expansion can be written in the finite sample setting as well. Here, $\| U_\star \|_F^2$ can be estimated to a small error using $\frac{1}{n} \sum_{i=1}^n y_i$ and therefore the first term on the RHS can be estimated even with finite samples. Assuming that $\| U_\star \|_F^2$ was known exactly, we consider updates of the form,
\begin{align} \label{eq:gdNN}
    U_{t+1} \gets U_t - \alpha \left( \nabla \cL_{\text{NN}} (U) + \lambda \nabla \cR_\beta (U) - \nabla (\| U \|_F^2 - \| U_\star \|_F^2)^2 \right) + P_t,
\end{align}
where $P_t$ are again perturbations similar to in \cref{eq:gd}. By definition of \cref{eq:LNN} it is easily verified that the update \cref{eq:gdNN} matches that in \cref{eq:gd}. Thus running the updates in \cref{eq:gdNN} serve to minimize the loss $\| UU^T - U_\star U_\star^T \|_F^2$. In particular, for the update rule in \cref{eq:gdNN}, the results in \Cref{theorem:main-population,theorem:fine-tuning_finite} can be used to establish guarantees for greedy pruning + fine-tuning even for $2$-layer neural networks with quadratic activations, which we avoid restating in the interest of space. While we assumed that $\| U_\star \|_F^2$ is known in order to be able to compute the updates \cref{eq:gdNN}, in the finite sample setting, the algorithm is likely to be robust to this error by absorbing it into $P_t$.

\section{Conclusion} \label{sec:conc}
In this paper, we studied the efficacy of the greedy pruning + fine-tuning pipeline in learning low-complexity solutions for the matrix sensing problem, as well as for learning shallow neural networks with quadratic activation functions. We showed that training on the mean square error augmented by a natural group Lasso regularizer results in models which are suitable for greedy pruning. Moreover, we proved that after pruning the remaining columns by removing the columns which are below a certain $\ell_2$-norm threshold, we arrive at a solution with correct column sparsity of $r$. Running a few iterations of gradient descent to fine-tune the resulting model, the population loss was shown to converge at a linear rate to an error floor of $O(\sqrt{rd/n})$, which is also statistically optimal, improving over the best-known generalization error bound of $O(\sqrt{kd/n})$ for training overparameterized models using gradient descent. We also presented a negative result showing the importance of regularization while training the model. To the best of our knowledge, our results provide the first theoretical guarantee on the generalization error of the model obtained via the greedy pruning + fine-tuning framework.



\section*{Acknowledgements}
The research of A. Mokhtari is supported in part by NSF Grants 2007668 and 2127697, ARO Grant W911NF2110226, the National AI Institute for Foundations of Machine Learning (IFML), and the Machine Learning Lab (MLL) at UT Austin. Kannan Ramchandran would like to acknowledge support from NSF CIF-2002821 and ARO fund 051242-00.

\section*{}

\bibliographystyle{alpha}
\bibliography{ref.bib}

\newpage
\input{appendix.tex}


\end{document}
