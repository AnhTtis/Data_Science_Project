@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@misc{Authors14,
 author = {FirstName LastName},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {FirstName LastName},
 title = {Frobnication tutorial},
 note = {Supplied as supplemental material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = PAMI,
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@article{Alpher04,
author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}

@inproceedings{Alpher05,
author = {FirstName Alpher and FirstName Gamow},
title = {Can a computer frobnicate?},
booktitle = CVPR,
pages = {234--778},
year = 2005
}

@inproceedings{paranjape-etal-2020-information,
    title = "An Information Bottleneck Approach for Controlling Conciseness in Rationale Extraction",
    author = "Paranjape, Bhargavi  and
      Joshi, Mandar  and
      Thickstun, John  and
      Hajishirzi, Hannaneh  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.153",
    doi = "10.18653/v1/2020.emnlp-main.153",
    pages = "1938--1952",
    abstract = "Decisions of complex models for language understanding can be explained by limiting the inputs they are provided to a relevant subsequence of the original text {---} a rationale. Models that condition predictions on a concise rationale, while being more interpretable, tend to be less accurate than models that are able to use the entire context. In this paper, we show that it is possible to better manage the trade-off between concise explanations and high task accuracy by optimizing a bound on the Information Bottleneck (IB) objective. Our approach jointly learns an explainer that predicts sparse binary masks over input sentences without explicit supervision, and an end-task predictor that considers only the residual sentences. Using IB, we derive a learning objective that allows direct control of mask sparsity levels through a tunable sparse prior. Experiments on the ERASER benchmark demonstrate significant gains over previous work for both task performance and agreement with human rationales. Furthermore, we find that in the semi-supervised setting, a modest amount of gold rationales (25{\%} of training examples with gold masks) can close the performance gap with a model that uses the full input.",
}

@inproceedings{
alemi2017deep,
title={Deep Variational Information Bottleneck},
author={Alexander A. Alemi and Ian Fischer and Joshua V. Dillon and Kevin Murphy},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=HyxQzBceg}
}

@inproceedings{li2021dual,
  title={Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning},
  author={Li, Bin and Li, Yin and Eliceiri, Kevin W},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14318--14328},
  year={2021}
}

@article{campanella2019clinical,
  title={Clinical-grade computational pathology using weakly supervised deep learning on whole slide images},
  author={Campanella, Gabriele and Hanna, Matthew G and Geneslaw, Luke and Miraflor, Allen and Werneck Krauss Silva, Vitor and Busam, Klaus J and Brogi, Edi and Reuter, Victor E and Klimstra, David S and Fuchs, Thomas J},
  journal={Nature medicine},
  volume={25},
  number={8},
  pages={1301--1309},
  year={2019},
  publisher={Nature Publishing Group}
}

@InProceedings{pmlr-v80-ilse18a,
  title = 	 {Attention-based Deep Multiple Instance Learning},
  author =       {Ilse, Maximilian and Tomczak, Jakub and Welling, Max},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2127--2136},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/ilse18a/ilse18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/ilse18a.html},
  abstract = 	 {Multiple instance learning (MIL) is a variation of supervised learning where a single class label is assigned to a bag of instances. In this paper, we state the MIL problem as learning the Bernoulli distribution of the bag label where the bag label probability is fully parameterized by neural networks. Furthermore, we propose a neural network-based permutation-invariant aggregation operator that corresponds to the attention mechanism. Notably, an application of the proposed attention-based operator provides insight into the contribution of each instance to the bag label. We show empirically that our approach achieves comparable performance to the best MIL methods on benchmark MIL datasets and it outperforms other methods on a MNIST-based MIL dataset and two real-life histopathology datasets without sacrificing interpretability.}
}

@inproceedings{chen2022scaling,
    author    = {Chen, Richard J. and al, et},
    title     = {Scaling Vision Transformers to Gigapixel Images via Hierarchical Self-Supervised Learning},
    booktitle = {CVPR},
    month     = {June},
    year      = {2022},
    pages     = {16144-16155}
}

@inproceedings{NEURIPS2021_10c272d0,
 author = {Shao, Zhuchen and Bian, Hao and Chen, Yang and Wang, Yifeng and Zhang, Jian and Ji, Xiangyang and zhang, yongbing},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {2136--2147},
 publisher = {Curran Associates, Inc.},
 title = {TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification},
 url = {https://proceedings.neurips.cc/paper/2021/file/10c272d06794d3e5785d5e7c5356e9ff-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{lu2021data,
  title={Data-efficient and weakly supervised computational pathology on whole-slide images},
  author={Lu, Ming Y and Williamson, Drew FK and Chen, Tiffany Y and Chen, Richard J and Barbieri, Matteo and Mahmood, Faisal},
  journal={Nature Biomedical Engineering},
  volume={5},
  number={6},
  pages={555--570},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{xiong2021nystromformer,
  title={Nystr{\"o}mformer: A Nystr{\"o}m-based Algorithm for Approximating Self-Attention},
  author={Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and Tan, Mingxing and Fung, Glenn and Li, Yin and Singh, Vikas},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2021}
}

@article{Zhang2022DTFDMILDF,
  title={DTFD-MIL: Double-Tier Feature Distillation Multiple Instance Learning for Histopathology Whole Slide Image Classification},
  author={Hongrun Zhang and Yanda Meng and Yitian Zhao and Yihong Qiao and Xiaoyun Yang and Sarah E. Coupland and Yalin Zheng},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.12081}
}

@INPROCEEDINGS{MOCO,  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Momentum Contrast for Unsupervised Visual Representation Learning},   year={2020},  volume={},  number={},  pages={9726-9735},  doi={10.1109/CVPR42600.2020.00975}}

@article{chen2020simple,
  title={A Simple Framework for Contrastive Learning of Visual Representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:2002.05709},
  year={2020}
}

@article{MAE,
  author    = {Kaiming He and
               Xinlei Chen and
               Saining Xie and
               Yanghao Li and
               Piotr Doll{\'{a}}r and
               Ross B. Girshick},
  title     = {Masked Autoencoders Are Scalable Vision Learners},
  journal   = {CoRR},
  volume    = {abs/2111.06377},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.06377},
  eprinttype = {arXiv},
  eprint    = {2111.06377},
  timestamp = {Tue, 16 Nov 2021 12:12:31 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-06377.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{devlin2018pretraining,
  added-at = {2019-02-05T23:35:51.000+0100},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  biburl = {https://www.bibsonomy.org/bibtex/210c860e3f390c6fbfd78a3b91ab9b0af/albinzehe},
  description = {[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  interhash = {a74f4c3853d3f0340e75546639134e91},
  intrahash = {10c860e3f390c6fbfd78a3b91ab9b0af},
  keywords = {bert elmo embeddings kallimachos nlp proposal-knowledge wordembeddings},
  note = {cite arxiv:1810.04805Comment: 13 pages},
  timestamp = {2020-07-28T14:17:24.000+0200},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding},
  url = {http://arxiv.org/abs/1810.04805},
  year = 2018
}

@inproceedings{Yang2019XLNetGA,
  title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  author={Zhilin Yang and Zihang Dai and Yiming Yang and Jaime G. Carbonell and Ruslan Salakhutdinov and Quoc V. Le},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{peters-etal-2018-deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}

@article{dino,
  author    = {Mathilde Caron and
               Hugo Touvron and
               Ishan Misra and
               Herv{\'{e}} J{\'{e}}gou and
               Julien Mairal and
               Piotr Bojanowski and
               Armand Joulin},
  title     = {Emerging Properties in Self-Supervised Vision Transformers},
  journal   = {CoRR},
  volume    = {abs/2104.14294},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.14294},
  eprinttype = {arXiv},
  eprint    = {2104.14294},
  timestamp = {Tue, 04 May 2021 15:12:43 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-14294.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{shwartz2017opening,
  title={Opening the black box of deep neural networks via information},
  author={Shwartz-Ziv, Ravid and Tishby, Naftali},
  journal={arXiv preprint arXiv:1703.00810},
  year={2017}
}

@article{saxe2019information,
  title={On the information bottleneck theory of deep learning},
  author={Saxe, Andrew M and Bansal, Yamini and Dapello, Joel and Advani, Madhu and Kolchinsky, Artemy and Tracey, Brendan D and Cox, David D},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2019},
  number={12},
  pages={124020},
  year={2019},
  publisher={IOP Publishing}
}

@article{tishby2000information,
  title={The information bottleneck method},
  author={Tishby, Naftali and Pereira, Fernando C and Bialek, William},
  journal={arXiv preprint physics/0004057},
  year={2000}
}

@article{goldfeld2018estimating,
  title={Estimating information flow in deep neural networks},
  author={Goldfeld, Ziv and Berg, Ewout van den and Greenewald, Kristjan and Melnyk, Igor and Nguyen, Nam and Kingsbury, Brian and Polyanskiy, Yury},
  journal={arXiv preprint arXiv:1810.05728},
  year={2018}
}

@incollection{lehmann2012completeness,
  title={Completeness, similar regions, and unbiased estimation-Part I},
  author={Lehmann, Erich Leo and Scheff{\'e}, Henry},
  booktitle={Selected works of EL Lehmann},
  pages={233--268},
  year={2012},
  publisher={Springer}
}

@article{achille2018information,
  title={Information dropout: Learning optimal representations through noisy computation},
  author={Achille, Alessandro and Soatto, Stefano},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={12},
  pages={2897--2905},
  year={2018},
  publisher={IEEE}
}

@INPROCEEDINGS{CAM,  author={Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Learning Deep Features for Discriminative Localization},   year={2016},  volume={},  number={},  pages={2921-2929},  doi={10.1109/CVPR.2016.319}}

@INPROCEEDINGS{Grad-CAM,  author={Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)},   title={Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization},   year={2017},  volume={},  number={},  pages={618-626},  doi={10.1109/ICCV.2017.74}}

@article{bach2015pixel,
  title={On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation},
  author={Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  journal={PloS one},
  volume={10},
  number={7},
  pages={e0130140},
  year={2015},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{schulz2020restricting,
  title={Restricting the flow: Information bottlenecks for attribution},
  author={Schulz, Karl and Sixt, Leon and Tombari, Federico and Landgraf, Tim},
  journal={arXiv preprint arXiv:2001.00396},
  year={2020}
}

@article{bejnordi2017diagnostic,
  title={Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer},
  author={Bejnordi, Babak Ehteshami and Veta, Mitko and Van Diest, Paul Johannes and Van Ginneken, Bram and Karssemeijer, Nico and Litjens, Geert and Van Der Laak, Jeroen AWM and Hermsen, Meyke and Manson, Quirine F and Balkenhol, Maschenka and others},
  journal={Jama},
  volume={318},
  number={22},
  pages={2199--2210},
  year={2017},
  publisher={American Medical Association}
}

@inproceedings{zhang2022benchmarking,
  title={Benchmarking the Robustness of Deep Neural Networks to Common Corruptions in Digital Pathology},
  author={Zhang, Yunlong and Sun, Yuxuan and Li, Honglin and Zheng, Sunyi and Zhu, Chenglu and Yang, Lin},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={242--252},
  year={2022},
  organization={Springer}
}

@article{javed2022rethinking,
  title={Rethinking Machine Learning Model Evaluation in Pathology},
  author={Javed, Syed Ashar and Juyal, Dinkar and Shanis, Zahil and Chakraborty, Shreya and Pokkalla, Harsha and Prakash, Aaditya},
  journal={arXiv preprint arXiv:2204.05205},
  year={2022}
}

@article{10.1001/jama.2017.14585,
    author = {Ehteshami Bejnordi, Babak and Veta, Mitko and Johannes van Diest, Paul and van Ginneken, Bram and Karssemeijer, Nico and Litjens, Geert and van der Laak, Jeroen A. W. M. and and the CAMELYON16 Consortium},
    title = "{Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer}",
    journal = {JAMA},
    volume = {318},
    number = {22},
    pages = {2199-2210},
    year = {2017},
    month = {12},
    issn = {0098-7484},
    doi = {10.1001/jama.2017.14585},
    url = {https://doi.org/10.1001/jama.2017.14585},
    eprint = {https://jamanetwork.com/journals/jama/articlepdf/2665774/jama\_ehteshami\_bejnordi\_2017\_oi\_170113.pdf},
}

@article{bulten2022artificial,
  title={Artificial intelligence for diagnosis and Gleason grading of prostate cancer: the PANDA challenge},
  author={Bulten, Wouter and Kartasalo, Kimmo and Chen, Po-Hsuan Cameron and Str{\"o}m, Peter and Pinckaers, Hans and Nagpal, Kunal and Cai, Yuannan and Steiner, David F and van Boven, Hester and Vink, Robert and others},
  journal={Nature medicine},
  volume={28},
  number={1},
  pages={154--163},
  year={2022},
  publisher={Nature Publishing Group}
}

@article{maron1997framework,
  title={A framework for multiple-instance learning},
  author={Maron, Oded and Lozano-P{\'e}rez, Tom{\'a}s},
  journal={Advances in neural information processing systems},
  volume={10},
  year={1997}
}


@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@article{kingma2015variational,
  title={Variational dropout and the local reparameterization trick},
  author={Kingma, Durk P and Salimans, Tim and Welling, Max},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@inproceedings{
anonymous2023exploring,
title={Exploring Low-Rank Property in Multiple Instance Learning for Whole Slide Image Classification},
author={Anonymous},
booktitle={Submitted to The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=01KmhBsEPFO},
note={under review}
}

@article{chen2022self,
  title={Self-supervised vision transformers learn visual concepts in histopathology},
  author={Chen, Richard J and Krishnan, Rahul G},
  journal={arXiv preprint arXiv:2203.00585},
  year={2022}
}

@article{DBLP:journals/corr/abs-2106-08254,
  author    = {Hangbo Bao and
               Li Dong and
               Furu Wei},
  title     = {BEiT: {BERT} Pre-Training of Image Transformers},
  journal   = {CoRR},
  volume    = {abs/2106.08254},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.08254},
  eprinttype = {arXiv},
  eprint    = {2106.08254},
  timestamp = {Tue, 29 Jun 2021 16:55:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-08254.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{floridi2020gpt,
  title={GPT-3: Its nature, scope, limits, and consequences},
  author={Floridi, Luciano and Chiriatti, Massimo},
  journal={Minds and Machines},
  volume={30},
  number={4},
  pages={681--694},
  year={2020},
  publisher={Springer}
}

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}

@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@article{litjens20181399,
  title={1399 H\&E-stained sentinel lymph node sections of breast cancer patients: the CAMELYON dataset},
  author={Litjens, Geert and Bandi, Peter and Ehteshami Bejnordi, Babak and Geessink, Oscar and Balkenhol, Maschenka and Bult, Peter and Halilovic, Altuna and Hermsen, Meyke and van de Loo, Rob and Vogels, Rob and others},
  journal={GigaScience},
  volume={7},
  number={6},
  pages={giy065},
  year={2018},
  publisher={Oxford University Press}
}

@article{petrick2021spie,
  title={SPIE-AAPM-NCI BreastPathQ Challenge: an image analysis challenge for quantitative tumor cellularity assessment in breast cancer histology images following neoadjuvant treatment},
  author={Petrick, Nicholas A and Akbar, Shazia and Cha, Kenny HH and Nofech-Mozes, Sharon and Sahiner, Berkman and Gavrielides, Marios A and Kalpathy-Cramer, Jayashree and Drukker, Karen and Martel, Anne LL and others},
  journal={Journal of Medical Imaging},
  volume={8},
  number={3},
  pages={034501},
  year={2021},
  publisher={SPIE}
}

@article{liang2022not,
  title={Not all patches are what you need: Expediting vision transformers via token reorganizations},
  author={Liang, Youwei and Ge, Chongjian and Tong, Zhan and Song, Yibing and Wang, Jue and Xie, Pengtao},
  journal={arXiv preprint arXiv:2202.07800},
  year={2022}
}

@Misc{hipt_issue,
 
howpublished = {\url{https://github.com/mahmoodlab/HIPT/issues/6#issuecomment-1175792504}},
 
note = {20230127},
 
title = {github1},
 
author = {Richarizardd}
 
}

@article{song2022learning,
  title={Learning from noisy labels with deep neural networks: A survey},
  author={Song, Hwanjun and al, et},
  journal={TNNLS},
  year={2022},
  publisher={IEEE}
}

@InProceedings{Zhao_2022_CVPR,
    author    = {Zhao, Yunqing and Ding, Henghui and Huang, Houjing and Cheung, Ngai-Man},
    title     = {A Closer Look at Few-Shot Image Generation},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {9140-9150}
}
@article{zhao2022few,
  title={Few-shot Image Generation via Adaptation-Aware Kernel Modulation},
  author={Zhao, Yunqing and Chandrasegaran, Keshigeyan and Abdollahzadeh, Milad and Cheung, Ngai-Man},
  journal={arXiv preprint arXiv:2210.16559},
  year={2022}
}

@article{li2020few,
  title={Few-shot image generation with elastic weight consolidation},
  author={Li, Yijun and Zhang, Richard and Lu, Jingwan and Shechtman, Eli},
  journal={arXiv preprint arXiv:2012.02780},
  year={2020}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@book{nayar2015bethesda,
  title={The Bethesda system for reporting cervical cytology: definitions, criteria, and explanatory notes},
  author={Nayar, Ritu and Wilbur, David C},
  year={2015},
  publisher={Springer}
}