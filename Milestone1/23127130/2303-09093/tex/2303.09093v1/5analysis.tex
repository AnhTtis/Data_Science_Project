\section{Analysis}
In this section, we investigate the following questions: (1) Which component is the main bottleneck for performance? (Section \ref{sec:per_stage}); (2) Does our model suffer from label imbalance between types? (Section \ref{sec:imbalance}); 
(3) How does our self-labeling procedure contribute to the performance? (Section \ref{sec:denoising}) 
\subsection{Per-Stage Performance}
\label{sec:per_stage}
\input{tables/exp_results_TR}
Our model comprises three components: trigger identification, event type ranking, and event type classification. Table \ref{tab:main_results} shows precision, recall, and F1 scores for trigger identification, while Table \ref{tab:TR&TC} presents the performance of event type ranking and classification. 
We assess per-stage performance using Hit@k metrics for event type ranking on ground truth trigger spans and event type classification on event mentions where the ground truth event type appears in the ranked results by the type ranker.
The scores indicate that the primary bottleneck exists in the precision of trigger identification and the Hit@1 score of type classification.


\subsection{Type Imbalance}
\label{sec:imbalance} 
\input{figures/frequency_F1}
Figure \ref{fig:glen_data_longtail} illustrates the long-tailed label distribution in our dataset. To investigate whether our model is affected by this imbalance, we divided the event types into four groups separated at the quartiles based on their frequency and calculated the performance per group. The resulting figure is shown in Figure \ref{fig:frequency_f1}. While we do see that the most popular group has the highest F1 score, the remaining groups have comparable scores.
In fact, we see two factors at play in defining the dataset difficulty: the ambiguity of the event type and the frequency of the event type. Event types that are more popular are also often associated with rolesets that have a high level of ambiguity, which balances out the gains from frequency. 

\subsection{Impact of Self-labeling}
\label{sec:denoising} 
\input{tables/self_labeling_investigation}
To investigate how self-labeling contributes to the improvements, we categorize test instances into three groups based on their PropBank rolesets, as shown in Table \ref{tab:self_labeling_investigation}. The `Clean' rolesets map to only one event type in DWD Overlay. We train the base classifier on 71,834 training instances corresponding to these `Clean' rolesets, which naturally performs significantly well on this portion of data. The model after self-labeling is trained with an additional 25,549 self-labeled data. The rolesets corresponding to these data are categorized as `Covered'. Table \ref{tab:self_labeling_investigation} indicates that the main performance gain comes from the `Covered' data, which is boosted directly by including corresponding training data. The `Other' category also sees some improvement, at the cost of a slight drop in the `Clean' category.

\subsection{Remaining Errors}
\begin{figure}[th]
    \centering
    \includegraphics[width=\linewidth]{figures/error_analysis_detailed.pdf}
    \caption{Categorization of remaining errors from our system.}
    \label{fig:error_categories}
\end{figure}

\begin{table*}[t]
    \centering
    \small
    \begin{tabular}{m{6em}|m{26em} m{8em} m{8em}}
    \toprule 
      Error Category   & Context & Predicted & Gold  \\
    \midrule 
       Candidate & 3 workers set off a critical \textbf{reaction} in 990000 when they poured too much uranium into a precipitation tank. & reaction (response to stimulus) & chemical\_reaction \\ 
       \cmidrule{2-4}
       & France is urged to increase \textbf{research} funding and support for innovations as a way to deal with the problem. & research\_method & research (systematic study) \\
       \midrule 
       Extended Candidate & Regulators and research firms promised that the \$1.5 billion \textbf{settlement} would be finalized two months ago. & settlement(distortion of a building) & settlement(operations relating to the payment) \\
       \midrule 
       Child & People \textbf{working} for minimum wage are producing a large number of products or services. & work (activities performed as a means of support) & work (activity done by a person for economic gain) \\
       \midrule 
       Sibling & In the middle east \textbf{conflict}, do you think the United States should take Israel's side, take the Palestinians' side, or not take either side? & social\_conflict (struggle for agency or power in society) & armed\_conflict (conflict including violence) \\
       \midrule 
       Parent & Doctors will \textbf{examine} him for signs that the cancer may have come back while he awaiting trial in a Russian jail. & inspection & physical\_examination (process by medical professional) \\
       \midrule 
       Other 
       & In theory, one could argue that the computer models are accurate and that the real \textbf{measurements} have some problems. & quantification & measurement \\
       \cmidrule{2-4} 
       & Though \textbf{funds} have already been allocated and voted on for the project, Blair himself insists that things are still ``very much open'', because materials have n ' t yet been chosen nor pose agreed upon (though he himself favours sitting on top of a pile of city academies looking ahead of him into the windows of a much larger city academy). & voting & fund \\
       \bottomrule 
    \end{tabular}
    \caption{Examples of erroneous type predictions. The trigger word (phrase) is shown in \textbf{bold}. 
    In some cases, the error falls into multiple categories. 
    We prioritize the XPO hierarchy-related categories since they are rarer. }
    \label{tab:error_cases}
\end{table*}


We categorize the remaining errors from our type classification model based on the relationship between our predicted event type and the ground truth event type as shown in Figure \ref{fig:error_categories} and Table \ref{tab:error_cases}.

Most of our errors come from the noisy annotation (\textbf{Candidate Set}): our model can predict an event type that falls within the set of candidate types associated with the ground truth PropBank roleset but fails to find the correct one. \textbf{Extended Roleset} refers to the predicted event being associated with a roleset that shares the same predicate as the ground truth. 
In another 22.6\% of the cases, we predict an event that is close to the ground truth on the XPO hierarchy, with the ground truth either being the child (\textbf{Child}), parent (\textbf{Parent}), or sibling node (\textbf{Sibling}) of our predicted type. The uncategorized errors are often due to the imperfect recall of our event ranking module (as in the second example where the context is long and ``fund'' fails to be included in the top-10 ranked event types), or cases where our model prediction is related semantically to the ground truth but the event types have no connections in the hierarchy (as in the first example where we predicted ``quantification'' instead of ``measurement''). 

