\section{Experiments}
\subsection{Experiment Setting}
\paragraph{Evaluation Metrics}
Previous work mainly use \textbf{trigger identification F1} and \textbf{trigger classification F1} to evaluate event detection performance. An event trigger is correctly identified if the span matches one of the ground truth trigger spans and it is correctly classified if the event type is also correct. 
In addition to these two metrics, due to the size of our ontology, we also report \textbf{Hit@K} which measures if the ground truth event type is within the top-$K$ ranked event types for the event trigger (the event trigger span needs to be correct). This can be seen as a more lenient version of  trigger classification F1. 
\begin{table*}[th]
    \centering
    \small 
    \begin{tabular}{l c c c c c c c c c}
    \toprule 
    \multirow{2}{4em}{Model}  &  \multicolumn{3}{c}{Trigger Identification} &  \multicolumn{3}{c}{Trigger Classification} & \multicolumn{3}{c}{Hit@k} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \\
    & Prec & Recall & F1 & Prec & Recall & F1 & Hit@1 & Hit@2 & Hit@5 \\
    \midrule 
    DMBERT~\cite{wang-etal-2019-adversarial-training} & 55.90 & 83.86 & 67.08 & 32.72 & 49.09 & 39.27 & - & - & - \\
    Token Classification &  66.65 & 82.72 & 73.82 & 40.62 & 50.41 & 44.99 & - & - & - \\
    Span Classification & 60.97 & 83.69 & 70.54 & 38.30 & 52.57 & 44.32 & - & - & - \\ 
    \midrule 
    BinarySpanCls + ZED & - & -& - & 38.18 & 48.36 & 42.67 & 56.78 & 60.29 & 60.62 \\ 
    TokCls + ZED~\cite{zhang-etal-2022-efficient-zero} & - & -& - & 40.01 & 56.25 & 46.76 & 56.84 & 60.35 & 60.80 \\
    \midrule 
    InstructGPT (32 shot) & 28.41 & 42.30 & 33.99 & 11.76 & 17.52 & 14.08 & - & - & - \\
    \midrule 
    Ours w/o self-labeling &
    - & -& - & 48.61 & 61.72 & 54.38 & 67.89 & 79.71 & 88.38\\
    Ours  &
    70.00 & 88.88 & \textbf{78.32} & 50.17 & 63.70 & \textbf{56.13} & 70.50 & 80.25 & 87.84\\
    \bottomrule 
    \end{tabular}
    \caption{Quantitative evaluation for event detection on \datasetname.
    }
    \label{tab:main_results}
\end{table*}

\paragraph{Baselines}
\begin{enumerate}
    \item \textbf{Token Classification}: We use the IO tagging scheme to classify tokens. 
    \item \textbf{Span Classification}: We use the embedding of the first and last token to represent the span and perform $(N+1)$ way classification. 
    \item \textbf{DMBERT}~\cite{wang-etal-2019-adversarial-training} is a BERT-based model that applies dynamic pooling~\cite{chen-etal-2015-event} according to the candidate trigger's location. We consider single tokens 
    to be candidate triggers in our dataset during testing.
    \item \textbf{ZED}~\cite{zhang-etal-2022-efficient-zero} is an event detection model that utilizes definitions. 
    Instead of pre-training on WordNet, we train the model on our noisy training data. As ZED only performs classification, we report the results of two variants,
    one with the trigger spans predicted by our trigger identification component (BinarySpanCls+ZED) %
    and one with the trigger spans predicted by the token classification model (TokCls+ZED). 
    \item \textbf{InstructGPT}\cite{Ouyang2022InstructGPT}, also referred to as GPT-3.5, is the improved version of GPT3 trained with instruction-tuning. We use the \texttt{text-davinci-003} model through the OpenAI API. We provide the model with an instruction for the task and 32 training examples from our training set for in-context learning.  
\end{enumerate}

\paragraph{Implementation Details}
Table \ref{tab:hyper-parameters} presents the hyperparameters for three components. During trigger identification, we restrict token spans to a maximum length of 10 tokens. For event type ranking, we use a designed loss function with $\tau = 1.0$. The top 10 ranked events are selected as candidates for event type classification. In event type classification, we do one round of self-labeling. Our model is trained on a single Tesla P100 GPU with 16GB DRAM.
\begin{table}[thbp]
\centering
    \small 
    \begin{tabular}{l | c c c}
    \toprule 
    Parameter & TI & ETR & ETC\\
    \midrule
    Training epochs & 5 & 5 & 2\\
    Batch size & 128 & 64 & 32 \\
    Max sequence length & 128 & 128 & 512 \\
    Base Model & \multicolumn{3}{c}{Bert-base-uncased}\\
    learning rate & \multicolumn{3}{c}{1e-5}\\
    Weight decay & \multicolumn{3}{c}{0.01}\\
    Scheduler & \multicolumn{3}{c}{Linear (with 50 warmup steps)} \\
    \bottomrule
\end{tabular}
\caption{
The training hyper-parameters of our model. \textbf{TI}: Trigger Identification. \textbf{ETR}: Event Type Ranking. \textbf{ETC}: Event Type Classification.
}
\label{tab:hyper-parameters}
\end{table}%
The implementation details for our baselines can be found in Appendix \ref{sec:baseline_implementation}.

\subsection{Results}
We show the evaluation results in Table \ref{tab:main_results} and some example predictions in Table \ref{tab:case_study}. 
\begin{table*}[t]
    \centering
    \small 
    \begin{tabular}{m{20em}|l}
    \toprule 
       Context  &  Predictions\\
    \midrule 
   \multirow{4}{20em}{You do \textbf{pay} income tax on your paycheck and the sales tax on consumable goods? }  & Ours: \textcolor{emerald}{payment(transfer of an item of value)} \\
   & TokCls: \textcolor{orange}{disbursement (payment from a public fund)} \\
   & ZED: \textcolor{orange}{income (consumption and savings opportunity)} \\
   & GPT3: None \\
   \midrule 
   \multirow{4}{20em}{... a situation ( brought on mostly by the police ) where information is \textbf{discovered} in a way that perpetuates the story} & Ours: \textcolor{emerald}{discovery (detecting something new)} \\
   & TokCls: \textcolor{orange}{medical\_finding} \\
   & ZED: \textcolor{orange}{physical\_finding (from a physical examination of patient)} \\
   & GPT3: \textcolor{emerald}{discovery (detecting something new)} \\
   \midrule 
   \multirow{4}{20em}{40\% of the female students at Georgetown law \textbf{reported} to us that they struggle financially as a result of this policy.} & 
   Ours: \textcolor{emerald}{reporting (producing a oral or written report)} \\
   & TokCls: \textcolor{orange}{scoop (journalism term for a new interesting story)} \\
   & ZED: \textcolor{emerald}{reporting (producing a oral or written report)} \\
   &GPT3: None \\
   \bottomrule 
    \end{tabular}
    \caption{Comparison across different systems. Correct predictions are shown in \textcolor{emerald}{green}. Predictions that map to the same PropBank roleset are shown in \textcolor{orange}{orange}. For ZED, we show the TokCls+ZED variant which has better performance.}
    \label{tab:case_study}
\end{table*}
The first group of baselines (DMBERT/TokCls/SpanCls) are all classification-based, which means that they treat event types as indexes and do not utilize any semantic representation of the labels. We observe that DMBERT will have a substantially lower trigger identification score if we allow spans of more than 1 token to be predicted 
due to its max pooling mechanism
(it will produce overlapping predictions such as ``served'', ``served in'', ``served in congress'') but only allowing the model to predict single token triggers limits its performance. 
TokCls and SpanCls achieve similar performance on the final trigger classification evaluation which hints on the performance limit for learning only with partial labels. As shown in the example, TokCls usually predicts event types that are within the candidate set for the roleset, but since it has no extra information to tell the candidates apart, the prediction is often wrong.  

Although ZED utilizes the event type definitions, it only achieves a minor 1.4\% F1 boost in performance compared to TokCls. ZED employs mean pooling to compress the definition embedding into a single vector, which is more restricted compared to both the ``bag of embeddings'' representation produced by our event type ranking module and the joint encoding used by our event type classification module. 

We observe that InstructGPT with in-context learning does not perform well on our task. The low trigger identification scores might be attributed to the lack of fine-tuning and under-utilization of the training set. The low classification scores are mainly caused by the restriction in input length \footnote{At the time of our experiment, the maximum input length was 2049 tokens.} which makes it impossible to let the model have full knowledge of the ontology. As a result, only 57.8\% of the event type names generated by InstructGPT could be matched in the ontology. With a larger input window and possibly fine-tuning, we believe that large LMs could achieve better performance. 

For our own model, we show that decoupling trigger identification from classification improves TI performance and performing joint encoding of the definition and the context improves TC performance. Furthermore, using self-labeling can help improve top-1 classification performance by converting partial labels into clean labels (we break down the detailed gains in Section \ref{sec:denoising}). 






