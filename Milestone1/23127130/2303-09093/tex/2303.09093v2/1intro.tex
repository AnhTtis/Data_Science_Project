\section{Introduction}
The development and application of event extraction techniques has long been stagnated by the limited availability of datasets. 
Despite the fact that it is 18 years old, ACE 2005~\footnote{\url{https://www.ldc.upenn.edu/collaborations/past-projects/ace}} is still the de facto standard evaluation benchmark.
Key limitations of ACE include its small event ontology of 33 types, small dataset size of around 600 documents and restricted domain (with a significant portion concentrated on military conflicts).
The largest effort towards event extraction annotation is MAVEN~\cite{wang-etal-2020-maven}, which expands the ontology to 168 types, but still exhibits limited domain diversity (32.5\% of the documents are about military conflict). 

The focus of current benchmarks on \textit{restricted ontologies in limited domains} is harmful to both developers and users of the system: it distracts researchers from building scalable general-purpose models for the sake of achieving higher scores on said benchmarks and it discourages users as they are left with the burden of defining the ontology and collecting data with no clue how well the models will adapt to their domain. In fact, widely-used Natural Language Processing (NLP) packages such as \texttt{spacy}\footnote{\url{https://spacy.io}} and \texttt{Stanza} \cite{qi-etal-2020-stanza}\footnote{\url{https://stanfordnlp.github.io/stanza/}} include named entity recognition as a standard component but event extraction packages are scarcely found. We believe that event extraction can, and should be made accessible to more users. %

\input{figures/data}
\input{figures/model}
To develop a general-purpose event extraction system, we first seek to build a high-quality  open-domain event extraction dataset with low cost. %
Annotation quality has been a long-standing issue for event extraction as the task is defined with lengthy and under-specified annotation guidelines, requires trained annotators, and still suffers from low inter-annotator agreement~\cite{ji-grishman-2008-refining}.
Thus, instead of aiming for perfect annotation from scratch, we start with the curated DWD  Overlay%
which defines a mapping between Wikidata\footnote{\url{wikidata.org}} and PropBank rolesets~\cite{Palmer2005Propbank}\footnote{A roleset is a set of roles that correspond to the distinct usage of a predicate.} as displayed in Figure \ref{fig:dataset}. 
While Wikidata Qnodes can serve as our general-purpose event ontology, the PropBank roleset information allows us to build a large-scale distantly supervised dataset by linking our event types to existing expert-annotated resources. 
In this process, we reuse the span-level annotations from experts, while dramatically expanding the size of the target ontology. 

 After extensive data cleaning for both the ontology and the data, our resulting dataset, \datasetname  (The GeneraL-purpose EveNt Benchmark), covers 3,465 event types over 208k sentences, which is a 20x increase in ontology size and 4x increase in dataset size compared to the previous largest event extraction dataset MAVEN.  We also show that our dataset has better type diversity and the label distribution is more natural (Figure \ref{fig:xpo_ace_comparison}, \ref{fig:glen_data_longtail}, \ref{fig:data_distribution}).
 


The expanded ontology poses unique challenges for model design. 
Conventionally, event detection is formulated as a sequence labeling problem, with labels treated as mere indexes. 
More recently, prompt-based event detection models take advantage of event type semantics by using event type names and definitions in prompts~\cite{yu-etal-2022-building}. 
However, they either enumerate all pairs of sentences and event types 
which is very inefficient for large ontologies. 
Our dataset also contains partial labels due to the distant supervision procedure. Specifically, in the DWD Overlay, multiple WikiData nodes are often mapped to the same PropBank roleset, resulting in multiple candidate labels for each data instance (Figure \ref{fig:dataset}).

To this end, we design our multi-stage model closely revolving around the challenges of large ontology size and distant-supervised data as shown in Figure \ref{fig:model}.
In the first stage, we perform \textbf{trigger identification} to find the possible trigger spans from each sentence. Since the span-level annotations are directly reused from PropBank annotation, this model circumvents the noise brought by our distant supervision. %
In the second stage, we perform \textbf{type ranking} between sentences and all event types. This model is based on ColBERT~\cite{Khattab2020ColBERT}, which is a very efficient ranking model based on independent encoding of the event type definition and the sentence. 
This stage allows us to reduce the number of type candidates for each sentence from thousands to dozens, retaining $\sim$90\% recall@10.
In the last stage, for each trigger detected, we perform \textbf{type classification} to connect it with one of the top-ranked event types from the first stage. We use a joint encoder over both the sentence and the event type definition for higher accuracy. 


\input{tables/data_stat}

We summarize the contributions of our paper as follows: 
\begin{enumerate}
    \item We present a new event detection benchmark \datasetname~which covers 3,465 event types over 208k sentences. Our dataset can serve as the basis for developing general-purpose event detection tools. 
    \item We design a multi-stage event detection model that works with our large ontology and distant-supervised annotation. Our model is able to achieve $\sim10\%$ F1 improvement over conventional classification models and recent definition-based event detection models. 
\end{enumerate}









