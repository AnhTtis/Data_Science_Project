\section{Data Filtering and Annotation Details}
\label{sec:appendix_annotation}
Table \ref{tab:removed_nodes} shows some examples of removed Qnodes from DWD Overlay in our ontology with different kinds of reasons.

Figure \ref{fig:annotation_interface} displays our annotation interface. The left box features the context, highlighting a single trigger, while the right box enumerates candidate event types, expressed as a combination of name and description, with an extra choice labeled "None of the above options is correct." The annotator's task is to select the option that most accurately represents the trigger word.


\include{tables/removed_nodes}










\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/GLEN_annotation_interface.png}
    \caption{Annotation interface built with Amazon Mechanical Turk for labeling the development and test set.}
    \label{fig:annotation_interface}
\end{figure*}



\section{Baseline Implementation Details}
\label{sec:baseline_implementation}
\begin{table*}[th]
\centering
    \small 
    \begin{tabular}{l | c c c c  }
    \toprule 
    Model & DMBERT & SpanCls & TokCls & ZED   \\
    \midrule
    Training epochs & 10 & 10 & 10 & 10 \\
    Batch size &  64 & 64  & 16 & 16 \\
    Negative samples & 5 & 5 & - & 5 \\
    Learning rate & {5e-5} & 5e-5 & 2e-5 & 2e-5 \\
    Max sequence length & \multicolumn{4}{c}{64} \\
    Base Model & \multicolumn{4}{c}{Bert-base-uncased}\\
    Weight decay & \multicolumn{4}{c}{0.0}\\
    Scheduler & \multicolumn{4}{c}{Linear} \\
    \bottomrule
\end{tabular}
\caption{Hyperparameter settings for baseline models.}
\label{tab:baseline_param}
\end{table*} 

We list the hyperparameters for our baseline models in Table \ref{tab:baseline_param}. 
For ZED, we follow the original paper and set the margin to 0.2. We set the threshold for predicting an event type to 0.3 (if the cosine similarity between the event type representation and the trigger representation is smaller than this value, we will refrain from predicting any event type). 

For the InstructGPT baseline, we use the \texttt{text-davinci-003} model with a temperature of 0.2 and \texttt{top\_p} set to 0.95 for decoding. We show our detailed prompt in Figure \ref{fig:gpt-prompt}. The first part of the prompt is the task instruction and then we include 32 input-output examples. Due to the current input length limit of InstructGPT, we were unable to feed the ontology into the model as part of the input. 


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/GPT-Prompt.pdf}
    \caption{Truncated version of our prompt to InstructGPT. }
    \label{fig:gpt-prompt}
\end{figure*}

