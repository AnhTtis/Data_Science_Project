\section{The \datasetname\ Benchmark}


To build the \datasetname\ benchmark, we first build a general-purpose ontology based on the curated DWD Overlay (Section \ref{sec:ontology}) and then create distantly-supervised training data based on the refined ontology (Section \ref{sec:data_cleaning}). In order to evaluate our model, we also create a labeled development set and test set through crowdsourcing (Section \ref{sec:annotation}). 
\subsection{Event Ontology Construction}
\label{sec:ontology}
The DWD Overlay is an effort to align WikiData Qnodes to PropBank rolesets, their argument structures, and LDC tagsets. Each event type entry in this ontology includes properties from WikiData (\texttt{name}, \texttt{definition}, \texttt{overlay\_parent} and \texttt{similar\_nodes}), properties from PropBank (\texttt{arguments}) and additional mappings to LDC event types (\texttt{{ldc\_types}}). This mapping ensures that our ontology is a superset of the ontology used in ACE and ERE\cite{song-etal-2015-light}. (See Section \ref{sec:data_analysis} for a detailed comparison of the ontology coverage.) 

To make this ontology more suitable for the event extraction task, we remove the Qnodes related to cognitive events that do not involve any physical state change such as \texttt{belief} and \texttt{doubt}. We also discovered that many rolesets were heavily reused across the ontology (mainly due to the inclusion of very fine-grained types), therefore we manually cleaned up the event types that were associated with these rolesets. 
We show some examples of removed Qnodes in Appendix Table \ref{tab:removed_nodes}.





\subsection{Distant Supervision Data}
\label{sec:data_cleaning}
Since the DWD Overlay is aligned with PropBank, we propose to reuse the existing PropBank annotations\footnote{\url{https://github.com/propbank/propbank-release}}. 
Since the events are labeled with PropBank rolesets and not event types from our ontology, after the automatic mapping, each event mention in the dataset would be associated with one or more Qnodes, which leads to the \textbf{partial label} challenge when using this distantly-supervised data.

To improve dataset quality, we perform sentence de-duplication, remove sentences with less than 3 tokens and omit special tokens (marked by * or brackets). We ensure that every trigger is a continuous token span and we remove events with overlapping triggers. For the AMR dataset, we additionally remove triggers with a part-of-speech tag of MD (modal verbs) or TO (the word ``to'') (such cases do not appear in other datasets).

Based on this distant supervision dataset, we make further adjustments to the ontology. We manually inspected the most popular rolesets that have more than 1000 event mentions and removed some of the ones that are too general or ambiguous (for instance, \texttt{cause.01} and \texttt{see.01}).  Finally, we remove the rolesets that have less than 3 event mentions across all datasets. After these cleaning efforts, we used the annotation for 1,804 PropBank rolesets, which are mapped to a total of 3,465 event types. 



        

\input{figures/xpo_ace_comparison}

\subsection{Data Split and Annotation}
\label{sec:annotation} 
To make our data split more realistic and to preserve the document-level context which could be useful for argument extraction, we split the dataset into train, dev, and test sets based on documents using a ratio of 90/5/5. Note that although our test set is only 5\% of the full data, it is already similar in scale to the entire ACE05 dataset. 
For datasets such as OntoNotes and AMR that contain documents from multiple genres (newswire, broadcast, web blogs etc.), we perform stratified sampling to preserve the ratio between genres. 
The statistics of our dataset are listed in Table \ref{tab:data}. Compared with ACE05 and MAVEN, \datasetname\ utilizes a 20x larger ontology and 4x larger corpus. 


Since the datasets already have PropBank roleset annotation, instead of performing annotation from scratch, we formulate the annotation task as a multiple-choice question: the annotators are presented with the trigger word in context and asked to choose from the Qnodes that are mapped to the roleset. Each Qnode was represented by its name and description. 
The number of options varies from 2 to 9 based on the ontology. We also added the option ``None of the above'' in case the annotator believes that none of the Qnodes was a good match. 

For the test set, we hired graduate students with linguistic knowledge to perform annotation.
For the development set, we screened Mechanical Turk workers that had high agreement with our in-house annotators and asked those who passed the screening to participate in the annotation. Each instance was annotated by two annotators separately. 
For PropBank rolesets that were frequently labeled as ``None of the above'', we performed manual inspection to determine if the mapping should be removed or revised. 
Finally, for the affected instances and the instances with disagreement, we asked our in-house annotators to perform a third pass as adjudication. 
For more details on the annotation interface, see Appendix \ref{sec:appendix_annotation}.



\subsection{Data Analysis}
\label{sec:data_analysis}
We first examine our event ontology by visualizing it as a hierarchy (as shown in Figure \ref{fig:xpo_ace_comparison}) with the parent-child relations taken from Wikidata (the \texttt{overlay\_parent} field in DWD Overlay).
The blue blocks represent event types whose corresponding branches are not covered by ACE2005 at all. Our ontology offers a wider range of diverse events, including those related to military, disaster, sports, social phenomena, chemicals, and other topics, indicating its novelty and potential usefulness.

\input{figures/glen_data_longtail}
\input{figures/data_distribution}
Next, we turn to the dataset and show the event type distribution in our dataset in 
Figure \ref{fig:glen_data_longtail}. For the training set, if the instance is associated with $N$ labels, we count each candidate label as $\frac{1}{N}$. 
Our type distribution closely mirrors real-world event distributions. The distribution exhibits frequent events such as \texttt{come} and \texttt{use}, along with a long tail of rare events such as \texttt{defect} and \texttt{impiety}. 
In Figure \ref{fig:data_distribution} we compare our type distribution to that of ACE and MAVEN. For ACE, the single most popular event type \texttt{attack} accounts for 28.8\% of the instances and the top-10 event types account for 79.4\% of the data. Our type distribution is much less skewed with the top-10 events composing 8.3\% of the data. 

\input{figures/pos}
Figure \ref{fig:pos_dis} illustrates the part-of-speech distribution of trigger words in our dataset\footnote{We use universal POS tagging tools from \url{https://www.nltk.org/}. }. Over 96\% of trigger words are verbs or nouns, which is similar to that of 94\% in MAVEN and 90\% in ACE2005. In addition, 0.6\% of the triggers are multi-word phrases. 


