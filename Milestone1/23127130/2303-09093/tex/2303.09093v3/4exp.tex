\section{Experiments}
\subsection{Experiment Setting}
\paragraph{Evaluation Metrics}
Previous work mainly uses \textbf{trigger identification F1} and \textbf{trigger classification F1} to evaluate event detection performance. An event trigger is correctly identified if the span matches one of the ground truth trigger spans and it is correctly classified if the event type is also correct. 
In addition to these two metrics, due to the size of our ontology, we also report \textbf{Hit@K} which measures if the ground truth event type is within the top-$K$ ranked event types for the event trigger (the event trigger span needs to be correct). This can be seen as a more lenient version of  trigger classification F1. 
\begin{table*}[th]
    \centering
    \small 
    \begin{tabular}{l c c c c c c c c c}
    \toprule 
    \multirow{2}{4em}{Model}  &  \multicolumn{3}{c}{Trigger Identification} &  \multicolumn{3}{c}{Trigger Classification} & \multicolumn{3}{c}{Hit@k} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \\
    & Prec & Recall & F1 & Prec & Recall & F1 & Hit@1 & Hit@2 & Hit@5 \\
    \midrule 
    DMBERT~\cite{wang-etal-2019-adversarial-training} 
    & 56.87 & 84.32 & 67.93 & 32.93 & 48.84 & 39.34 
    & - & - & - \\
    Token Classification &  68.04 & 82.19 & 74.46 & 41.48 & 50.10 & 45.38
    & - & - & - \\
    Span Classification  & 62.36 & 78.71 & 69.58 & 37.36 & 47.16 & 41.69 
    &- & - & - \\ 
    \midrule 
    TokCls + ZED~\cite{zhang-etal-2022-efficient-zero} & - & -& - & 40.01 & 56.25 & 46.76 & 56.84 & 60.35 & 60.80 \\
    \midrule 
    InstructGPT (32 shot) & 28.41 & 42.30 & 33.99 & 11.76 & 17.52 & 14.08 & - & - & - \\
    \midrule 
    \ours\ w/o self-labeling &
- & -& - & 49.21 & 61.62 & 54.72 & 68.21 & 80.21 & 89.18\\

    \ours  &
71.05 & 88.96 & \textbf{79.00} & 50.91 & 63.74 & \textbf{56.60} & 71.30 & 80.84 & 88.63\\

    \bottomrule 
    \end{tabular}
    \caption{Quantitative evaluation results (\%) for event detection on \datasetname.
    }
    \label{tab:main_results}
\end{table*}

\paragraph{Baselines}
We compare with three categories of models: (1) classification models including \textbf{DMBERT}~\cite{wang-etal-2019-adversarial-training}, \textbf{token-level classification} and \textbf{span-level classification}; (2) a definition-based model \textbf{ZED}~\cite{zhang-etal-2022-efficient-zero} and (3) \textbf{InstructGPT}\cite{Ouyang2022InstructGPT} with few-shot prompting. We attempted to compare with CRF models such as OneIE model~\citep{DBLP:conf/acl/LinJHW20} but were unable to do so due to the memory cost of training CRF with the large label space. For detailed baseline descriptions and hyperparameters, see Appendix \ref{sec:baseline_implementation}.



\subsection{Results}
We show the evaluation results in Table \ref{tab:main_results} and some example predictions in Table \ref{tab:case_study}. 
\begin{table*}[t]
    \centering
    \small 
    \begin{tabular}{m{20em}|l}
    \toprule 
       Context  &  Predictions\\
    \midrule 
   \multirow{4}{20em}{You do \textbf{pay} income tax on your paycheck and the sales tax on consumable goods? }  
   & \ours: \textcolor{emerald}{payment(transfer of an item of value)} \\
   & TokCls: \textcolor{orange}{disbursement (payment from a public fund)} \\
   & ZED: \textcolor{orange}{income (consumption and savings opportunity)} \\
   & GPT3: None \\
   \midrule 
   \multirow{4}{20em}{... a situation ( brought on mostly by the police ) where information is \textbf{discovered} in a way that perpetuates the story} 
   & \ours: \textcolor{emerald}{discovery (detecting something new)} \\
   & TokCls: \textcolor{orange}{medical\_finding} \\
   & ZED: \textcolor{orange}{physical\_finding (from a physical examination of patient)} \\
   & GPT3: \textcolor{emerald}{discovery (detecting something new)} \\
   \midrule 
   \multirow{4}{20em}{40\% of the female students at Georgetown law \textbf{reported} to us that they struggle financially as a result of this policy.} & 
   \ours\: \textcolor{emerald}{reporting (producing a oral or written report)} \\
   & TokCls: \textcolor{orange}{scoop (journalism term for a new interesting story)} \\
   & ZED: \textcolor{emerald}{reporting (producing a oral or written report)} \\
   &GPT3: None \\
   \bottomrule 
    \end{tabular}
    \caption{Comparison across different systems. Correct predictions are shown in \textcolor{emerald}{green}. Predictions that map to the same PropBank roleset are shown in \textcolor{orange}{orange}. For ZED, we show the TokCls+ZED variant which has better performance.}
    \label{tab:case_study}
\end{table*}
The first group of baselines (DMBERT/TokCls/SpanCls) are all classification-based, which means that they treat event types as indexes and do not utilize any semantic representation of the labels. We observe that DMBERT will have a substantially lower trigger identification score if we allow spans of more than 1 token to be predicted 
due to its max pooling mechanism
(it will produce overlapping predictions such as ``served'', ``served in'', ``served in congress''). 
TokCls's performance hints on the limit for learning only with partial labels. As shown in the example, TokCls usually predicts event types that are within the candidate set for the roleset, but since it has no extra information to tell the candidates apart, the prediction is often wrong.  

Although ZED utilizes the event type definitions, it only achieves a minor improvement in performance compared to TokCls. ZED employs mean pooling to compress the definition embedding into a single vector, which is more restricted compared to 
the joint encoding used by our event type classification module. 

We observe that InstructGPT with in-context learning does not perform well on our task. The low trigger identification scores might be attributed to the lack of fine-tuning and under-utilization of the training set. The low classification scores are mainly caused by the restriction in input length \footnote{The maximum input length was 2049 tokens at the time of our experiments.} which makes it impossible to let the model have full knowledge of the ontology. As a result, only 57.8\% of the event type names generated by InstructGPT could be matched in the ontology. With a larger input window and possibly fine-tuning, we believe that large LMs could achieve better performance. 

For our own model, we show that decoupling trigger identification from classification improves TI performance, and performing joint encoding of the definition and the context improves TC performance. Furthermore, using self-labeling can help improve top-1 classification performance by converting partial labels into clean labels. 






