\section{The \datasetname\ Benchmark}


\input{figures/glen_data_longtail}
To build the \datasetname\ benchmark, we first build a general-purpose ontology based on the curated DWD Overlay and then create distantly-supervised training data based on the refined ontology (Section \ref{sec:ontology}). In order to evaluate our model, we also create a labeled development set and test set through crowdsourcing (Section \ref{sec:annotation}). 
\subsection{Event Ontology and Data}
\label{sec:ontology}
The DWD Overlay is an effort to align WikiData Qnodes to PropBank rolesets, their argument structures, and LDC tagsets. 
This mapping ensures that our ontology is a superset of the ontology used in ACE and ERE~\cite{song-etal-2015-light}. (See Section \ref{sec:data_analysis} for a detailed comparison of the ontology coverage.) 

To make this ontology more suitable for the event extraction task, we remove the Qnodes related to cognitive events that do not involve any physical state change such as \texttt{belief} and \texttt{doubt}. \footnote{This is in line with the scope of events as defined in previous ACE and ERE datasets.}
We also discovered that many rolesets such as \texttt{ill.01} were heavily reused across the ontology (mainly due to the inclusion of very fine-grained types), therefore we manually cleaned up the event types that were associated with these rolesets. 
We show some examples of removed Qnodes in Appendix Table \ref{tab:removed_nodes}.




\label{sec:data_cleaning}
Since the DWD Overlay is aligned with PropBank, we propose to reuse the existing PropBank annotations\footnote{\url{https://github.com/propbank/propbank-release}}. 
After the automatic mapping, each event mention in the dataset would be associated with one or more Qnodes, which leads to the \textbf{partial label} challenge when using this distantly-supervised data.
We then perform another round of data filtering based on the frequency of rolesets (details in Appendix \ref{sec:appendix_annotation}).
After these cleaning efforts, we used the annotation for 1,804 PropBank rolesets, which are mapped to a total of 3,465 event types. 

To make our data split more realistic and to preserve the document-level context, 
we split the dataset into train, development, and test sets based on documents using a ratio of 90/5/5. Note that although our test set is only 5\% of the full data, it is already similar in scale to the entire ACE05 dataset. 
For datasets such as OntoNotes and AMR that contain documents from multiple genres (newswire, broadcast, web blogs etc.), we perform stratified sampling to preserve the ratio between genres. 
The statistics of our dataset are listed in Table \ref{tab:data}. Compared with ACE05 and MAVEN, \datasetname\ utilizes a 20x larger ontology and 4x larger corpus. 

        

\input{figures/xpo_ace_comparison}

\subsection{Data Annotation}
\label{sec:annotation} 
Instead of performing annotation from scratch, we formulate the annotation task as a multiple-choice question: the annotators are presented with the trigger word in context and asked to choose from the Qnodes that are mapped to the roleset. 


For the test set, we hired graduate students with linguistic knowledge to perform annotation.
For the development set, we screened Mechanical Turk workers that had high agreement with our in-house annotators and asked those who passed the screening to participate in the annotation. 
The weighted average kappa value for exact match on the test set (27 annotators), is 0.60, while for the dev set (981 annotators) is 0.37 over 5.2 options.  If we allow for a soft match for event types of different granularity, 
such as \texttt{trade} and \texttt{international\_trade},
the kappa value is 0.90 for the test set and 0.69 for the dev set.
For more details on the annotation interface, see Appendix \ref{sec:appendix_annotation}.



\subsection{Data Analysis}
\label{sec:data_analysis}
We first examine our event ontology by visualizing it as a hierarchy (as shown in Figure \ref{fig:xpo_ace_comparison}) with the parent-child relations taken from Wikidata (the \texttt{overlay\_parent} field in DWD Overlay).
Our ontology offers a wider range of diverse events, including those related to military, disaster, sports, social phenomena, chemicals, and other topics, indicating its novelty and potential usefulness.


We show the event type distribution in our dataset in 
Figure \ref{fig:glen_data_longtail}. 
Our type distribution closely mirrors real-world event distributions. The distribution exhibits frequent events such as \texttt{come} and \texttt{use}, along with a long tail of rare events such as \texttt{defect} and \texttt{impiety}. 
In terms of type diversity, for ACE, the single most popular event type \texttt{attack} accounts for 28.8\% of the instances and the top-10 event types account for 79.4\% of the data. Our type distribution is much less skewed with the top-10 events composing 8.3\% of the data. 

\input{figures/pos}
Figure \ref{fig:pos_dis} illustrates the part-of-speech distribution of trigger words in our dataset\footnote{We use universal POS tagging tools from \url{https://www.nltk.org/}. }. Over 96\% of trigger words are verbs or nouns, which is similar to that of 94\% in MAVEN and 90\% in ACE2005. In addition, 0.6\% of the triggers are multi-word phrases. 


