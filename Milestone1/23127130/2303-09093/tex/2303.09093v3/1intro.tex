\section{Introduction}
As one of the core IE tasks, event extraction involves event detection (identifying event trigger mentions and classifying them into event types), and argument extraction (extracting the participating arguments). Event extraction serves as the basis for the analysis of complex procedures and news stories which involve multiple entities and events scattered across a period of time, and can also be used to assist question-answering and dialog systems. 

The development and application of event extraction techniques have long been stymied by the limited availability of datasets. 
Despite the fact that it is 18 years old, ACE 2005~\footnote{\url{https://www.ldc.upenn.edu/collaborations/past-projects/ace}} is still the de facto standard evaluation benchmark.
Key limitations of ACE include its small event ontology of 33 types, small dataset size of around 600 documents and restricted domain (with a significant portion concentrated on military conflicts).
The largest effort towards event extraction annotation is MAVEN~\cite{wang-etal-2020-maven}, which expands the ontology to 168 types, but still exhibits limited domain diversity (32.5\% of the documents are about military conflicts). 

The focus of current benchmarks on \textit{restricted ontologies in limited domains} is harmful to both developers and users of the system: it distracts researchers from building scalable general-purpose models for the sake of achieving higher scores on said benchmarks and it discourages users as they are left with the burden of defining the ontology and collecting data with little certainty as to how well the models will adapt to their domain.
We believe that event extraction can, and should be made accessible to more users.
\input{figures/data}
\input{figures/model}

To develop a general-purpose event extraction system, we first seek to efficiently build a high-quality open-domain event detection dataset. 
The difficulty of annotation has been a long-standing issue for event extraction as the task is defined with lengthy annotation guidelines, which require training expert annotators. 
Thus, instead of aiming for perfect annotation from scratch, we start with the curated DWD Overlay~\cite{spaulding-2023-darpa} %
which defines a mapping between Wikidata\footnote{\url{wikidata.org}} and PropBank rolesets~\cite{Palmer2005Propbank}\footnote{A roleset is a set of roles that correspond to the distinct usage of a predicate.} as displayed in Figure \ref{fig:dataset}. 
While Wikidata Qnodes can serve as our general-purpose event ontology, the PropBank roleset information allows us to build a large-scale distantly supervised dataset by linking our event types to existing expert-annotated resources. 
In this process, we reuse the span-level annotations from experts, while dramatically expanding the size of the target ontology. 
 Our resulting dataset, \datasetname  (The GeneraL-purpose EveNt Benchmark), covers 3,465 event types over 208k sentences, which is a 20x increase in the number of event types and 4x increase in dataset size compared to the previous largest event extraction dataset MAVEN.  We also show that our dataset has better type diversity and the label distribution is more natural (Figures \ref{fig:glen_data_longtail} and \ref{fig:xpo_ace_comparison}).
 



We design a multi-stage cascaded event detection model \ours\ to address the challenges of large ontology size and distant-supervised data as shown in Figure \ref{fig:model}.
In the first stage, we perform \textbf{trigger identification} to find the possible trigger spans from each sentence. We can reuse the span-level annotations, circumventing the noise brought by our distant supervision. %
In the second stage, we perform \textbf{type ranking} between sentences and all event types. This model is based on ColBERT~\cite{Khattab2020ColBERT}, which is a very efficient ranking model based on separate encoding of the event type definition and the sentence. 
This stage allows us to reduce the number of type candidates for each sentence from thousands to dozens, retaining $\sim$90\% recall@10.
In the last stage, for each trigger detected, we perform \textbf{type classification} to connect it with one of the top-ranked event types from the first stage. We use a joint encoder over both the sentence and the event type definition for higher accuracy. 


\input{tables/data_stat}

In summary, our paper makes contributions in (1)  introducing a new event detection benchmark \datasetname~which covers 3,465 event types over 208k sentences that can serve as the basis for developing general-purpose event detection tools;  
(2) designing a multi-stage event detection model \ours\ for our large ontology and annotation via distant supervision which shows large improvement over a range of single-stage models and few-shot InstructGPT.









