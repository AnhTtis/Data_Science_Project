

\section{Implementation Details}
Table \ref{tab:hyper-parameters} presents the hyperparameters for three components in our model. During trigger identification, we restrict token spans to a maximum length of 10 tokens. For event type ranking, we use a designed loss function with $\tau = 1.0$. The top 10 ranked events are selected as candidates for event type classification. In event type classification, we do one round of self-labeling. Our model is trained on a single Tesla P100 GPU with 16GB DRAM.
\begin{table}[thbp]
\centering
    \small 
    \begin{tabular}{l | c c c}
    \toprule 
    Component & TI & ETR & ETC\\
    \midrule
    Training epochs & 5 & 5 & 2\\
    Batch size & 128 & 64 & 32 \\
    Max sequence length & 128 & 128 & 512 \\
    Base Model & \multicolumn{3}{c}{Bert-base-uncased}\\
    learning rate & \multicolumn{3}{c}{1e-5}\\
    Weight decay & \multicolumn{3}{c}{0.01}\\
    Scheduler & \multicolumn{3}{c}{Linear (with 50 warmup steps)} \\
    \bottomrule
\end{tabular}
\caption{
The training hyper-parameters of our model. \textbf{TI}: Trigger Identification. \textbf{ETR}: Event Type Ranking. \textbf{ETC}: Event Type Classification.
}
\label{tab:hyper-parameters}
\end{table}%

\section{Baseline Implementation Details}
\label{sec:baseline_implementation}
We list the details of our baselines as below:
\begin{enumerate}
    \item \textbf{Token Classification}: We use the IO tagging scheme to classify tokens. 
    \item \textbf{Span Classification}: We use the embedding of the first and last token to represent the span for classification.
    \item \textbf{DMBERT}~\cite{wang-etal-2019-adversarial-training} is a BERT-based model that applies dynamic pooling~\cite{chen-etal-2015-event} according to the candidate trigger's location. We consider single tokens 
    to be candidate triggers in our dataset during testing.
    \item \textbf{ZED}~\cite{zhang-etal-2022-efficient-zero} is an event detection model that utilizes definitions. 
    Instead of pre-training on WordNet, we train the model on our noisy training data. As ZED only performs classification, we report the results 
    with the trigger spans predicted by the token classification model (TokCls+ZED). 
    \item \textbf{InstructGPT}\cite{Ouyang2022InstructGPT}, also referred to as GPT-3.5, is the improved version of GPT3 trained with instruction-tuning. We use the \texttt{text-davinci-003} model through the OpenAI API. We provide the model with an instruction for the task and 32 training examples from our training set for in-context learning. 
\end{enumerate}



\begin{table*}[th]
\centering
    \small 
    \begin{tabular}{l | c c c c  }
    \toprule 
    Model & DMBERT & SpanCls & TokCls & ZED   \\
    \midrule
    Training epochs & 10 & 10 & 10 & 10 \\
    Batch size &  64 & 64  & 16 & 16 \\
    Negative samples & 5 & 5 & - & 5 \\
    Learning rate & {5e-5} & 5e-5 & 2e-5 & 2e-5 \\
    Max sequence length & \multicolumn{4}{c}{64} \\
    Base Model & \multicolumn{4}{c}{Bert-base-uncased}\\
    Weight decay & \multicolumn{4}{c}{0.0}\\
    Scheduler & \multicolumn{4}{c}{Linear} \\
    \bottomrule
\end{tabular}
\caption{Hyperparameter settings for baseline models.}
\label{tab:baseline_param}
\end{table*} 

We list the hyperparameters for our baseline models in Table \ref{tab:baseline_param}. 
For ZED, we follow the original paper and set the margin to 0.2. We set the threshold for predicting an event type to 0.3 (if the cosine similarity between the event type representation and the trigger representation is smaller than this value, we will refrain from predicting any event type). 

For the InstructGPT baseline, we use the \texttt{text-davinci-003} model with a temperature of 0.2 and \texttt{top\_p} set to 0.95 for decoding. We show our detailed prompt in Figure \ref{fig:gpt-prompt}. The first part of the prompt is the task instruction and then we include 32 input-output examples. Due to the current input length limit of InstructGPT, we were unable to feed the ontology into the model as part of the input. 


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/GPT-Prompt.pdf}
    \caption{Truncated version of our prompt to InstructGPT. }
    \label{fig:gpt-prompt}
\end{figure*}


\section{Data Filtering and Annotation Details}

\label{sec:appendix_annotation}
Table \ref{tab:removed_nodes} shows some examples of removed Qnodes from DWD Overlay in our ontology with different kinds of reasons.

To improve dataset quality, we perform sentence de-duplication, remove sentences with less than 3 tokens and omit special tokens (marked by * or brackets). We ensure that every trigger is a continuous token span and we remove events with overlapping triggers. For the AMR dataset, we additionally remove triggers with a part-of-speech tag of MD (modal verbs) or TO (the word ``to'') (such cases do not appear in other datasets).

Based on this distant supervision dataset, we make further adjustments to the ontology. We manually inspected the most popular rolesets that have more than 1000 event mentions and removed rolesets that are too general or ambiguous (for instance, \texttt{cause.01} and \texttt{see.01}).  Finally, we remove the rolesets that have less than 3 event mentions across all datasets. 


Figure \ref{fig:annotation_interface} displays our annotation interface. The left box features the context, highlighting a single trigger, while the right box enumerates candidate event types, expressed as a combination of name and description, with an extra choice labeled "None of the above options is correct." 
Each Qnode is represented by its name and description. 
The number of options varies from 2 to 9 based on the ontology. 
The annotator's task is to select the option that most accurately represents the trigger word.

Each instance was annotated by two annotators separately. 
For PropBank rolesets that were frequently labeled as ``None of the above'', we performed manual inspection to determine if the mapping should be removed or revised. 
Finally, for the affected instances and the instances with disagreement, we asked our in-house annotators to perform a third pass as adjudication. 


\include{tables/removed_nodes}










\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/GLEN_annotation_interface.png}
    \caption{Annotation interface built with Amazon Mechanical Turk for labeling the development and test set.}
    \label{fig:annotation_interface}
\end{figure*}




\input{figures/self-labeling}

\section{Impact of Self-labeling}
\label{sec:denoising} 



\input{tables/self_labeling_investigation}

Figure \ref{fig:self_labeling} indicates that with a threshold\footnote{The threshold is the margin between the probability of the top 1 event type and the other types in a candidate set. A higher threshold means a higher level of confidence.} of 0.9, the accuracy of selecting the correct label from a candidate set reaches 57.8\% on the dev set.
To investigate how self-labeling contributes to the improvements, we categorize test instances into three groups based on their PropBank rolesets, as shown in Table \ref{tab:self_labeling_investigation}. The `Clean' rolesets map to only one event type in DWD Overlay. We train the base classifier on 71,834 training instances corresponding to these `Clean' rolesets, which naturally performs significantly well on this portion of data. The model after self-labeling is trained with an additional 25,549 self-labeled data. The rolesets corresponding to these data are categorized as ``Covered''. Table \ref{tab:self_labeling_investigation} indicates that the main performance gain comes from the ``Covered'' data, which is boosted directly by including corresponding training data. The ``Other'' category also sees some improvement, at the cost of a slight drop in the ``Clean'' category.


