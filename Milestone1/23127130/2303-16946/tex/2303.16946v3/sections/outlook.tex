\section{Outlook}
\label{sec:outlook}

\subsection{Generalizations of the Basic Architecture}

We presented one simple architecture (Figure~\ref{fig:arch}) which was motivated by the entropy and complexity of mean-field quantum models, especially the SYK model. The particular scaling-inspired ansatz with $k=r^a$ and $N = r^a + r^{a+b}$ is one instance of that architecture, but one could well imagine other choices.

Moreover, inspired by branching MERA~\cite{Evenbly_2014} and s-sourcery~\cite{Swingle_2016}, one can consider other architectures in which the added thermal degrees of freedom are not just in a product state. As a basic example, consider the following structure. Take the encoding circuits for two $[[n,k,\delta]]$ codes and mix their physical qubits using an additional depth $D$ quantum circuit. The result is a code on $n' = 2n$ qubits with $k'=2k$. Hence, the rate is the same, $k'/n' = k/n$. The distance will also increase by a factor with some probability, as will the weights of the stabilizers. Starting with a root $[[n_0,k_0,\delta_0]$ code, $L$ layers of this construction produces a code with parameters $[2^L n_0, 2^L k_0, \delta]$ with $\delta$ some $L$-dependent distance.

In the above construction, the rate of the final code is determined by the rate of the root code. We could also vary the rate by introducing additional product qubits in each iteration of the process (analogous to the thermal qubits above) or by combining codes with different rates at each iteration. 

In all these constructions, the distance is also expected to grow exponentially with $L$, the number of layers. However, the weight of the checks will also generically grow when we use random depth-$D$ circuits. From this perspective, the challenge of producing a good quantum LPDC code is the challenge of keeping the weights of checks low while keeping the distance high. This clearly requires tuning of the layer circuits, likely made possible by the addition of some structure to the problem. In light of the recent rapid progress in the area of good quantum LDPC codes, it would be interesting to understand if our architecture can capture these recently discovered codes.


%A \emph{quantum low-density-parity-check} (qLDPC) code is a code with constant weight, regardless of the code length $n$. This means that each stabilizer only acts on a constant number of qudits and each qudit is acted on by only a constant number of stabilizer elements. A \emph{good} qLDPC code has its code space dimension $k$ and distance $d$ scale linearly with the length $n$. \bgs{maybe move good ldpc to the discussion?}

\subsection{Further Weight Reductions}
\label{sec:weight_reduction}

One direction we intend to explore in the future is finding alternative bases of given generated stabilizer code that minimize the overall weights. In the exact case, such a basis is unique and given by the \emph{reduced-row echelon form (RREF)} of the stabilizer matrix i.e.\ the matrix with all stabilizer basis elements as row vectors.
The RREF for a general matrix is defined in terms of the following rules:
\begin{enumerate}
    \item All rows consisting of only zeroes are at the bottom.
    \item The leading entry (that is the left-most nonzero entry) of every nonzero row is a 1, and to the right of the leading entry of every row above.
    \item Each column containing a leading 1 has zeros in all its other entries.
\end{enumerate}
In the case of a stabilizer matrix the first rule can be ignored since the matrix has maximum rank. The remaining two requirements can be easily met by applying a Gaussian elimination algorithm. An example of a possible resulting RREF is given by
\begin{equation}
    \begin{pmatrix}
        1 & 0 & a_1 & 0 & b_1 & 0 \\
        0 & 1 & a_2 & 0 & b_2 & 0 \\
        0 & 0 & 0 & 1 & b_3 & 0 \\
        0 & 0 & 0 & 0 & 0 & 1
    \end{pmatrix}
\end{equation}
From this it should be intuitively clear that the RREF maximizes the number of zero-valued elements in the matrix, thus minimizing the weights of the stabilizer basis elements.

Another possible method of weight reduction could be finding a set of stabilizers that approximately replicates our model, but whose basis has low weights. A potential way to achieve this is to use so-called \emph{perturbative gadgets} \cite{Jordan_2008}. Considering the Hamiltonian representation \eqref{eq:stab_hamiltonian} of a given stabilizer code $[[n,k,\delta]]$, each term in the sum acts on a number of qudits equivalent to its weight. Let $w$ be the largest weight of all terms in the Hamiltonian, then we speak of a $w$-local Hamiltonian. Using $w$th-order perturbation theory it can then be shown that there must exist a 2-local Hamiltonian that approximately has the same ground space i.e.\ the desired quantum error-correcting code. Said Hamiltonian is called the \emph{gadget Hamiltonian} and its construction involves introducing $w \cdot (n-k)$ ancillary qudits, where $n-k$ is the number of terms in the original Hamiltonian. The approximate ground space Hamiltonian can then be recovered by block-diagonalizing the gadget Hamiltonian and only considering the entries with unit eigenvalue in the ancillary space.% This gives us a Trotterization of the initial stabilizer Hamiltonian such that the stabilizers have at most weight 2. \bgs{trotterization?}
In future work we intend to explore both approaches for weight reduction in the context of our tensor network ansatz and compare them to the baseline considerations made in this paper.% Eventually this could even lead to the construction of qLDPC codes.

\subsection{Towards a Closer Link With SYK}

It is also interesting to move towards closer contact with SYK. The first step is to develop a fermionic analogue of our architecture. Then, because the network is not efficiently contractible in general on a classical computer, it is interesting to pursue a quantum simulation strategy where we treat our architectecture as a variational ansatz. The variational parameters would be the gates within each circuit layer as well as the discrete data of the network, e.g. the number of qubits at each layer. It would also be important to understand and adapt the construction to some of the details of SYK, e.g. the specific expected form of the ground state degeneracy.

A related setting where we should be able to carry out classical simulations is the SYK model for $q=2$, i.e. a non-interacting fermion model with random all-to-all hoppings. In this case, similar to the Clifford case, we can efficiently simulate the network using non-interacting fermion machinery. There is no ground state degeneracy, so $k=0$ in this case, but one could still test other properties of the network. We are currently exploring this direction.

In the spirit of generalizing to fermionic models, it is also interesting to consider fermionic generalizations of the Clifford formalism, e.g. the subgroup of the full set of fermionic unitaries that maps strings of fermion operators to other strings of fermion operators. By developing methods to sample these transformations and to compute entropies of subsets of the fermions, one would be able to repeat the studies in this work in the language of fermionic codes~\cite{Bravyi_2010}. This is also a work in progress.

\subsection{Connections With Holographic Models}

Finally, let us comment further on the connection to low-dimensional models of quantum gravity. We already made use of these connections as part of the motivation for our ansatz, in particular, we checked the complexity of our network against holographic estimates of complexity.

The basic point is that our architecture mimics the structure of two-dimensional Jackiw-Teitelboim (JT) gravity in AdS2. The analogue of the area formula for black hole entropy is the statement that the entropy of black hole is equal to the value of a scalar field called the dilaton evaluated at the event horizon (technically, at the bifurcation point), see e.g.~\cite{jafferis2022entanglement}. This in turn leads to a low-dimensional version of the Ryu-Takayanagi formula for entanglement (for a review see \cite{Rangamani_2017}), also in terms of the dilaton field.

After solving the equations of motion, one finds a solution in which the metric is 
\begin{equation}
    ds^2 = \frac{-dt^2 + dz^2}{z^2}
\end{equation}
and the dilaton is
\begin{equation}
    \phi(z) = \phi_1/z.
\end{equation}
The number of degrees of freedom at a scale determined by $z$ is proportional to
\begin{equation}
    \phi_0 + \phi(z),
\end{equation}
where $\phi_0$ is some background value. Comparing to our network, we want to interpret $\ln z$ as analogous to $L-\ell$, which increases from zero as we go from the UV (top of Fig.~\ref{fig:arch}) to the IR (bottom of Fig.~\ref{fig:arch}) . The number of qudits at ``layer $z$'' would be $k+ (N-k) e^{- \ln z}$. Then, since $\phi_0 + \phi(z)$ also has the form
\begin{equation}
    \text{constant} + \text{constant}' e^{- \ln z},
\end{equation}
we could intepret $\frac{\phi_0}{4 G_N}$ as analogous to $k$ and $\frac{\phi_1}{4 G_N}$ as analogous to $N-k$. It will also be interesting to further explore connections between the NoRA network and low-dimensional quantum gravity models.

\paragraph{Acknowledgements}

We thank Isaac Kim, Vincent Su, Michael Walter and Gregory Bentsen for discussions. We acknowledge support from the U.S. Department of Energy grant DE-SC0009986 (V.B.) and from the AFOSR under grant number FA9550-19-1-0360 (B.G.S.).



