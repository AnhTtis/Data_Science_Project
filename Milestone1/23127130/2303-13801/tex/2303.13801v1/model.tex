\section{Proposed Framework: {\ourmodel}}
\label{model}


In this section, we introduce a novel self-supervised co-training framework {\ourmodel}.
The proposed framework is illustrated in Figure~\ref{fig:intro_model} and works in three phases.
Phase one automatically generates two sets of pseudo labels.
We use a combination of off-the-shelf pre-trained POS and NER taggers, knowledge graph, and GPT-2 scorer for generating the first set of pseudo labels automatically without any hand-crafted rules for matching the slot values.
The other set of pseudo labels is acquired through a zero-shot slot filling model~\cite{liu2020coach}, trained on the out-of-domain dataset.
It is critical to emphasize that both sets of labels are noisy and incomplete which poses serious challenges to training effective models for the task of open-domain slot filling.
Phase two fine-tunes the pre-trained BERT to the slot filling task that effectively transfers the knowledge from the pre-trained language model~(LM) to overcome the issue of label incompleteness to some extent. 
Further, we employ the early stopping technique to minimize the noise in the labels.
The output of this phase is two BERT models that can generate soft labels for self-supervision during co-training in phase three.
Phase three leverages the fine-tuned models and further trains them in an iterative fashion.
Specifically, the proposed peer training approach facilitates high-confidence soft label selection for the other peer to perform training. This phase progressively reduces the noise in the labels and enables effective model fitting. 



\subsection{Phase One: Automatic Label Generation}
To acquire the first set of labels, we perform the following steps.
First of all, off-the-shelf trained POS and NER taggers are used to predict initial estimates of the slot values irrespective of the slot types. Then, the type information of the slot values is queried from the KG and the slot value is tagged for the most appropriate slot in the target domain.
This approach, however, produces low recall. 
To expand the candidate slot values, we generate n-grams of the natural language text and employ a partial matching scheme to query the KG for type information (e.g., \myspecial{Jason} \myspecial{Aldean} = \myspecial{American} \myspecial{singer}) of the n-grams if the entry exists.
This process generates multiple overlapping hypotheses about the slot values.
We replace a span of text that corresponds to a slot value by its type information and a GPT-2 based scorer (see Section~\ref{sec:nlpmodels}) is used to select the best candidate based on the fluency of the text.
Naturally, if a token (or span of tokens) is replaced by its type, the sentence should score higher as compared to the case where an inappropriate substitution is performed. 
We select the best hypothesis if the score is greater than the threshold.
Intuitively, the candidate selection threshold can automatically be searched based on a small validation set from the target domain, making the label generation process fully automatic. 
The other set of noisy labels is acquired by the zero-shot slot filling model~\cite{liu2020coach} that has been trained using an out-of-domain dataset. It is important to highlight that the zero-shot slot filling model does not require any labeled in-domain training example. 
To summarize the automatic label generation phase, both sets of labels are acquired in a fully automatic fashion without any hand-crafting.


In contrast to previous work in weak supervision~\cite{ren2015clustype,he2017autoentity,fries2017swellshark,giannakopoulos2017unsupervised} that obtains a single set of noisy labels and then propose techniques to overcome the challenge of fitting an effective model to the noisy labels, we acquire two sets of complementary labels.
The choice of these two sets of labels is guided by the intuition that they should be complementary and the models trained on these sets of labels should be able to share complementary information with the other to improve the performance in the later phases of the framework.
Essentially, the first set of labels carries information from external knowledge sources, whereas the labels generated through the pre-trained zero-shot slot filling model capture how the slot values are mentioned in other domains.
%
To further elaborate on the motivation and our process for the first set of labels (i.e., labels using KG and other NLP models), the pre-trained LMs have been shown to have a great deal of knowledge~\cite{petroni2019language}, thus should be capable of generating automatic labels with no need of external KG. 
To the best of our knowledge, there exists no work that shows that accurate token-level automatic labeling (e.g., slot filling task) is possible with pre-trained LMs. 
Moreover, such approaches would require heavy prompting in each new target domain, whereas our label generation process is fully automatic and only relies on the readily-available pre-trained NLP models and external KG.

\subsection{Phase Two: LM-assisted Weak Supervision}
Since we do not have access to dataset $\{(\mathbf{X}_n,\mathbf{Y}_n)\}_{n=1}^N$ with true ground-truth labels.
We use pseudo labels generated in phase one, $\{(\mathbf{X}_n,\mathbf{D}_n)\}_{n=1}^N$, to learn 
$f_{m,c}(\cdot; \cdot)$ that outputs the probability of the $m$-th token to take on class $c$. 
We learn $f_{m,c}(\cdot; \cdot)$ by minimizing the following loss over the noisy dataset $\{(\mathbf{X}_n,\mathbf{D}_n)\}_{n=1}^N$: 
$$
\hat\theta = \argmin_{\theta}\frac{1}{N}\sum_{n=1}^{N} \ell(\mathbf{D}_n, f(\mathbf{X}_{n}; \theta)),
\label{eq:stage1}
$$
where $\ell(\mathbf{D}_n, f(\mathbf{X}_{n}; \theta)) = \frac{1}{M} \sum_{m=1}^{M} -\log{f_{m,d_{n, m}}(\mathbf{X}_{n}; \theta)}$. 
We employ the pre-trained multilingual BERT with token-level classification head that uses Adam optimizer \cite{kingma2014adam,Liu2019} with early stopping and multiple random initializations. 


Since slot filling task is similar to the MLM training objective of the BERT, we employ pre-trained BERT as the backbone model.
That is, MLM's goal is to predict the masked tokens using bidirectional contexts. Similarly, slot filling tries to predict the label for a token leveraging both left and right contexts simultaneously, which makes the pre-trained BERT an ideal model of choice that greatly facilitates minimizing incomplete labels.
It is important to highlight that our automatically generated labels are not only incomplete but also potentially wrong.
The training strategies employed in this phase minimize the noise in the label to some extent. 
Specifically, early stopping can provide a strong regularization and would not let the model overfit to the noisy labels, especially wrong labels. 
Moreover, early stopping does not let the model forget the knowledge in the pre-trained model.
Similarly, multiple random initializations enforce robustness. 
Since the model is fine-tuned on the noisy labels, averaging the predictions of multiple models for each token ensures that wrong labels end up with low probabilities and true labels consistently achieve high probabilities.
Using the above-mentioned strategies, we train two slot filling models, which we call the peers. The peer one is trained on the first set of pseudo labels that were generated using POS and NER taggers, KG, and the GPT-2 scorer in phase one. Similarly, peer two is trained using the predictions of the zero-shot slot filling model~\cite{liu2020coach}.
Both models have the same architecture and follow the same training procedures.

\begin{table*}[t!]
\centering
\caption{Dataset statistics.}
\vspace{-7pt}
\label{tab:dataset}
\begin{tabular}{lccccc}
\toprule
\textbf{Dataset}  & \textbf{Dataset Size} & \textbf{Vocab. Size} & \textbf{Avg. Length} & \textbf{\# of Domains} & \textbf{\# of Slots} \\ \hline
\textbf{SGD}      & 188K                  & 33.6K                & 13.8                 & 20                     & 240                  \\
\textbf{MultiWoZ} & 67.4K                 & 10.5K                & 13.3                 & 8                      & 61 \\
\bottomrule
\end{tabular}
\vspace{-7pt}
\end{table*}

\subsection{Phase Three: Self-supervised Co-training}
We introduce an iterative peer training algorithm where both peers generate high-confidence soft labels for training the other peer in the next iteration. 
Theoretically, these peers can be anything, but in this work, 
we explore two of the most promising directions that have shown the promise to minimize the need for manual labeling for the task: zero-shot learning and distant supervision.
This phase uses a self-supervised co-training scheme to exploit the patterns of slot values from other domains through the labels generated by the zero-shot filling model (i.e., peer two)~\cite{liu2020coach} as well as utilize the knowledge in external KGs and pre-trained models via labels provided by the peer one.
Specifically, we initialize the peers trained in phase two and use their pseudo labels to kick-start training in this phase.
Specifically, peer one $f_{m,c}(\cdot; \theta_{\textrm{p1}})$ would generate labels $\{\tilde{\mathbf{Y}}^{(t)}_n = [\tilde{y}_{n,1}^{(t)}, ..., \tilde{y}_{n,m}^{(t)}]\}_{n=1}^{N}$ for peer two $f_{m,c}(\cdot; \theta_{\textrm{p2}})$ at the $t$-th iteration by:
$$
\tilde{y}_{n,m}^{(t)} = \argmax_{c}{f_{m,c}(\mathbf{X}_n; \theta_{\textrm{p1}}^{(t)})}. 
\label{eq:pseudo}
$$

Based on these labels, the peer two can be fine-tuned by: 
$$
\hat\theta_{\textrm{p2}}^{(t+1)} = \argmin_{\theta}\frac{1}{N}\sum_{n=1}^N \ell(\tilde{\mathbf{Y}}_n^{(t)}, f(\mathbf{X}_{n}; \theta)).
\label{eq:self_train1}
$$

Similarly, peer two $f_{m,c}(\cdot; \theta_{\textrm{p2}})$ would generate pseudo labels for peer one $f_{m,c}(\cdot; \theta_{\textrm{p1}})$ that are used to fine-tune peer one. 
We also notice that it is beneficial to stop early during this phase as well, to improve the model fitting and gradually reduce the noise associated with the automatically generated labels.
Since pseudo labels are refined gradually in an iterative way, both peers can benefit from the knowledge contained within the labels of the other while avoiding overfitting.
Furthermore, as an alternative to pseudo labels, we also generate soft labels that are used for confidence re-weighting. 
The high-confidence soft label selection strategy enables better model fitting and efficient learning via better quality of the automatic labels.
Specifically, for the given $m$-th token in the $n$-th training example, the probability for all classes $C$ is $[f_{m,1}(\mathbf{X}_n;\theta),...,f_{m,C}(\mathbf{X}_n;\theta)]$. 
Following ~\cite{xie2016unsupervised}, at $t$-th iteration, peer one generates soft labels, $\{\mathbf{S}_n^{(t)} = [\mathbf{s}_{n,m}^{(t)}]_{m=1}^M \}_{n=1}^N$, as given below:
$$
\mathbf{s}_{n,m}^{(t)} = [s_{n,m,c}^{(t)}]_{c=1}^{C} = \Bigg[  \frac{f_{m,c}^2(\mathbf{X}_n;\theta_{\textrm{peer1}}^{(t)})/p_{c}}{\sum_{c'=1}^C f_{m,c'}^2(\mathbf{X}_n;\theta_{\textrm{peer1}}^{(t)})/p_{c'}}\Bigg]_{c=1}^{C}
\label{eq:soft}
$$ 
where $p_{c} = \sum_{n=1}^N \sum_{m=1}^M f_{m,c}(\mathbf{X}_n;\theta_{\textrm{p1}}^{(t)})$ computes the frequency of the tokens for the $c$-th class. 
Then, peer two $f(\cdot; \theta_{\textrm{p2}}^{(t+1)})$ is fine-tuned by:
$$
\theta_{\textrm{p2}}^{(t+1)} = \argmin_{\theta} \frac{1}{N} \sum_{n=1}^{N} \ell_{\rm KL}(\mathbf{S}_n^{(t)}, f(\mathbf{X}_{n}; \theta)),
$$
where $\ell_{\rm KL}(\cdot,\cdot)$ is the KL-divergence-based loss:
$$
\ell_{\rm KL}(\mathbf{S}_n^{(t)}, f(\mathbf{X}_{n}; \theta))=\frac{1}{M}\sum_{m=1}^M\sum_{c=1}^C - s_{n,m,c}^{(t)} \log f_{m,c}(\mathbf{X}_{n}; \theta).
\label{eq:klloss}
$$

Moreover, we also investigate selecting tokens that have high confidence. 
For instance, we pick high-confidence tokens from the $m$-th input example at the $t$-th iteration by  
$
H^{(t)}_n = \{m : \max_{c} s_{n,m,c}^{(t)} > \epsilon \},
$
where $\epsilon\in [0,1]$ is a threshold that can be searched based on a small validation set. 
Then, peer two $f(\cdot; \theta_{\textrm{p2}}^{(t+1)})$ is fine-tuned by:
$$
\theta_{\textrm{p2}}^{(t+1)} %&= \argmin_{\theta} \frac{1}{N} \sum_{n=1}^{N} \ell_{\rm S-KL}(\bS_n^{(t)}, f(\bX_{n}; \theta)) \\
= \argmin_{\theta} \frac{1}{N|H^{(t)}_n|}\sum_{n=1}^{N} \sum_{m\in H^{(t)}_n}\sum_{c=1}^C - s_{n,m,c}^{(t)} \log f_{m,c}(\mathbf{X}_{n}; \theta).
$$

This phase improves the robustness to effectively fit the model for tokens with high confidence. 
Both peers keep sharing information and their confidence by producing soft labels for their counterparts until they approximate to the true labels while employing early stopping and scheduled learning rates.
It is important to remind that phase three is the most important phase that progressively reduces noise from the labels to a great extent and enables superior performance for the task of open-domain slot filling.