\section{Experimental Setup}
\label{experiments}
\subsection{Datasets}
\label{datasets}
We conducted extensive experiments to evaluate the performance of our model {\ourmodel} using two well-known public benchmark datasets: Schema Guided Dialogue~(\myspecial{SGD})~\cite{rastogi2019towards} and Multi-Domain Wizard-of-Oz~(\myspecial{MultiWoZ})~\cite{zang-etal-2020-multiwoz}.
The important statistics about datasets are presented in Table~\ref{tab:dataset}.

\stitle{SGD~\cite{rastogi2019towards}:}
\myspecial{SGD} is one of the most challenging and comprehensive publicly available datasets for evaluating tasks related to dialog systems. 
It consists of dialog utterances from 20 domains and covers 240 slots. Following ~\cite{siddique2021linguistically}, we pre-processed the dataset for the task of slot filling. 
Moreover, all the domains that have less than 1850 utterances (i.e., $<~1\%$ of the dataset) have been merged into a single domain, named ``\myspecial{Others}'' in our experiments.


\stitle{MultiWoZ~\cite{zang-etal-2020-multiwoz}:}
\myspecial{MultiWoZ} has been extensively used to evaluate a wide range of tasks in dialog systems.
In our experiments, we used \myspecial{MultiWoZ} 2.2 which consists of dialog utterances for 61 slots in 8 domains. 
We followed the experimental setup of ~\cite{siddique2021linguistically} and merged all the utterances into a single domain, called ``\myspecial{Others}'', which accounted for $<~1\%$ of the dataset.




\begin{table*}[t!]
\footnotesize
\centering
\caption{Slot F1 score for all the domains in SGD dataset. The zero-label proposed framework {\ourmodel} achieves competitive performance to the fully supervised methods for each domain in the dataset.}
\vspace{-7pt}
\label{tab:sgd-domains}
\begin{tabular}{l|cc|cc|cc|c|cc|c}
\toprule
\multicolumn{1}{c|}{}        & \multicolumn{2}{c|}{\textbf{Unsupervised Learning}}                  & \multicolumn{2}{c|}{\textbf{Weak Supervision}}                             & \multicolumn{2}{c|}{\textbf{Zero-shot Learning}}                & \multicolumn{1}{c|}{\textbf{Few-shot}} & \multicolumn{2}{c|}{\textbf{Supervised Learning}}                        & \multicolumn{1}{c}{\textbf{Self-supervised}} \\
\multicolumn{1}{c|}{\textbf{Domains $\downarrow$}} & \multicolumn{1}{c}{\textbf{DSI}} & \multicolumn{1}{c|}{\textbf{RCAP}} & \multicolumn{1}{c}{\textbf{Inter-Slot}} & \multicolumn{1}{c|}{\textbf{Merge-Select}} & \multicolumn{1}{c}{\textbf{Coach}} & \multicolumn{1}{c|}{\textbf{LEONA}} & \multicolumn{1}{c|}{\textbf{GPT-3}}    & \multicolumn{1}{c}{\textbf{JointBERT}} & \multicolumn{1}{c|}{\textbf{Seq2Seq-DU}} & \multicolumn{1}{c}{\textbf{{\ourmodel} (this work)}}            \\ \hline
Buses                        & 0.2533                  & 0.3705                            & 0.3006	& 0.2860                             & 0.6281                    & 0.6978                     & 0.5054                        & 0.9161                        & \textbf{0.9318}                          & \underline{0.7701}                              \\
Calendar                     & 0.3963                  & 0.5253                            & 0.4810	&0.5134                            & 0.6023                    & 0.7436                     & 0.6269                        & \textbf{0.9862}                        & 0.9605                          & \underline{0.9725}                              \\
Events                       & 0.2778                  & 0.5162                            & 0.2562	&0.3922                            & 0.5486                    & 0.7619                     & 0.6658                        & \textbf{0.9403}                        & 0.9346                          & \underline{0.8773}                              \\
Flights                      & 0.3605                  & 0.4154                            & 0.4240	&0.5220                             & 0.4898                    & 0.5901                     & 0.7166                        & \textbf{0.9612}                        & 0.9286                          & \underline{0.9026}                              \\
Homes                        & 0.4356                  & 0.4747                            & 0.3558&	0.5586                            & 0.6235                    & 0.7698                     & 0.7734                        & 0.9498                        & 0.9860                           & \underline{\textbf{0.9862}}                              \\
Hotels                       & 0.2630                   & 0.3584                            & 0.1566 &	0.4252                            & 0.7216                    & 0.7677                     & 0.6825                        & \textbf{0.9219}                        & 0.9096                          & \underline{0.8542}                              \\
Movies                       & 0.3407                  & 0.3631                            & 0.1414&	0.5420                             & 0.5537                    & 0.7285                     & 0.5377                        & \textbf{0.9423}                        & 0.9281                          & \underline{0.8728}                              \\
Music                        & 0.4146                  & 0.5613                            & 0.3134&	0.4844                            & 0.5786                    & 0.7613                     & 0.6098                        & 0.9387                        & 0.9373                          & \underline{\textbf{0.9419}}                              \\
RentalCars                   & 0.3661                  & 0.4964                            & 0.1664&	0.4114                            & 0.6576                    & 0.7389                     & 0.5339                        & 0.8753                        & \textbf{0.9232}                          & \underline{0.9123}                              \\
Restaurants                  & 0.3537                  & 0.4457                            & 0.1316&	0.6534                            & 0.7195                    & 0.7574                     & 0.5513                        & \textbf{0.9534}                        & 0.9345                          & \underline{0.8928}                              \\
RideSharing                  & 0.3642                  & 0.5164                            & 0.2860	&0.6628                            & 0.7273                    & 0.8172                     & 0.7589                        & 0.9490                         & 0.9429                          & \underline{\textbf{0.9712}}                              \\
Services                     & 0.3902                  & 0.5834                            & 0.3342&	0.6598                            & 0.7607                    & 0.8182                     & 0.5819                        & \textbf{0.9714}                        & 0.9151                          & \underline{0.9688}                              \\
Travel                       & 0.3606                  & 0.5894                            & 0.1950&	0.4618                            & 0.8403                    & 0.9234                     & 0.6715                        & 0.9344                        & 0.9226                          & \underline{\textbf{0.9552}}                              \\
Weather                      & 0.3498                  & 0.5363                            & 0.1484&	0.6390                            & 0.6003                    & 0.8223                     & 0.7040                         & 0.9820                         & 0.8981                          & \underline{\textbf{0.9821}}                              \\
Others                       & 0.3305                  & 0.4323                            & 0.2824&	0.5026                            & 0.4921                    & 0.5592                     & 0.6660                         & \textbf{0.9662}                        & 0.9353                          & \underline{0.9291}                              \\ \hline
Average                      & 0.3519                  & 0.4823                            & 0.2636&	0.5151                            & 0.6466                    & 0.7642                     & 0.6371                        & \textbf{0.9444}                        & 0.9324                          & \underline{0.9186}  \\
\bottomrule
\end{tabular}
\vspace{-7pt}
\end{table*}

\subsection{Evaluation Methodology}
\label{testing}

The Slot F1 score is the standard metric for evaluating the slot filling task, that has been used in our experiments.
We present our evaluations for the following settings.

\stitle{Evaluation using a single domain in the dataset.}
We evaluate using each domain separately in both datasets.
The zero-shot learning model is trained using all the domains in the dataset except the target domain (i.e., being used for evaluation) and the model's predictions for the target domain are used as one set of automatic labels.
Since there exists a wide range of domains in both datasets, the evaluation is used to validate the robustness of the proposed framework across domains.


\stitle{Evaluation using the full dataset.}
We also perform evaluations using full datasets. 
In this setup, the zero-shot learning model is trained using the dataset that is not being used for evaluation.
For example, when we conduct an evaluation on \myspecial{SGD} dataset, the zero-shot slot filling model is trained on \myspecial{MultiWoZ}, and its predictions for the \myspecial{SGD} dataset are considered as automatic labels.



\subsection{Competing Methods}
\label{baselines}
We compare {\ourmodel} with state-of-the-art (SOTA) methods that employ a variety of learning paradigms, including supervised learning for the slot filling task.

\begin{description}[leftmargin=1.2\parindent,labelindent=-3.5pt, itemsep=-1pt]
\item \textbf{DSI~\cite{min2020dialogue}:}
A SOTA unsupervised learning method that automatically mines the slot values from unlabeled data by building two neural latent variable models.

\item \textbf{RCAP~\cite{zeng2021automatic}:}
An unsupervised pipeline that works in coarse-to-fine fashion for automatically inferring intents as well as slots labels via sequence labeling, clustering, and Apriori algorithm.


\item \textbf{Inter-Slot~\cite{chen2015jointly}:}
An automatic labeling approach that leverages a random walk inference algorithm
for jointly capturing token-to-token, token-to-slot, and slot-to-slot relationships
by building word-based lexical and slot-based semantic graphs and combining them using dependency grammar.


\item \textbf{Merge-Select~\cite{hudevcek2021discovering}:}
A weak supervision-based approach, which automatically identifies candidate slot values relevant to a given domain using clustering algorithms, then these candidates are used to train a neural network-based slot tagger.



\item \textbf{Coach~\cite{liu2020coach}:}
A zero-shot slot filling model that adapts a coarse-to-fine approach. 
First, it identifies potential slot values irrespective of the slot type and then matches the slot values to appropriate slots using the representation of the slot description.
In this work, we use this model to generate one set of pseudo labels.


\item \textbf{LEONA~\cite{siddique2021linguistically}:}
A zero-shot learning approach, that exploits the embeddings of POS and NER taggers as well as learns a contextual tokens-slot similarity function from seen domains to make predictions about unseen domains.


\item \textbf{GPT-3~\cite{brown2020language}:}
A SOTA model that has shown zero-shot learning capabilities for many NLP tasks. 
In this work, we use this model in the context of few-shot learning and provide five labeled training examples as prompts for each domain (or dataset) and expect the model to make inferences for the given domain. 

\item \textbf{JointBert~\cite{chen2019bert}:}
A supervised learning method that fine-tunes BERT for intent detection and slot filling tasks jointly.
We perform this comparison to understand the performance gap between supervised models that use expensive labeled training data and our proposed zero-label approach. 

\item \textbf{Seq2Seq-DU~\cite{feng2020sequence}:}
a supervised learning approach that formulates the task of dialog understanding as sequence-to-sequence and fine-tunes two BERT encoders, one for utterance and the other for schema descriptions, and jointly models intents and slots.

\end{description}


