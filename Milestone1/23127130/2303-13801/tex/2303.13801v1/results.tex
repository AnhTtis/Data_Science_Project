\section{Results}
\label{results}


\begin{table*}[t!]
\footnotesize
\centering
\caption{Slot F1 score for all the domains in MultiWoZ dataset.}
\vspace{-7pt}
\label{tab:multiwoz-domains}
\begin{tabular}{l|cc|cc|cc|c|cc|c}
\toprule
\multicolumn{1}{c|}{}        & \multicolumn{2}{c|}{\textbf{Unsupervised Learning}}                  & \multicolumn{2}{c|}{\textbf{Weak Supervision}}                             & \multicolumn{2}{c|}{\textbf{Zero-shot Learning}}                & \multicolumn{1}{c|}{\textbf{Few-shot}} & \multicolumn{2}{c|}{\textbf{Supervised Learning}}                        & \multicolumn{1}{c}{\textbf{Self-supervised}} \\
\multicolumn{1}{c|}{\textbf{Domains}} & \multicolumn{1}{c}{\textbf{DSI}} & \multicolumn{1}{c|}{\textbf{RCAP}} & \multicolumn{1}{c}{\textbf{Inter-Slot}} & \multicolumn{1}{c|}{\textbf{Merge-Select}} & \multicolumn{1}{c}{\textbf{Coach}} & \multicolumn{1}{c|}{\textbf{LEONA}} & \multicolumn{1}{c|}{\textbf{GPT-3}}    & \multicolumn{1}{c}{\textbf{JointBERT}} & \multicolumn{1}{c|}{\textbf{Seq2Seq-DU}} & \multicolumn{1}{c}{\textbf{{\ourmodel} (this work)}}            \\ \hline
Hotels                       & 0.6007            & 0.7182                 & 0.4904	& 0.5801             & 0.6481             & 0.7319             & 0.6967   & 0.9292             & \textbf{0.9236}              & \underline{0.9012}          \\
Restaurants                  & 0.5162            & 0.7318                 & 0.4548           & 0.5512             & 0.6137             & 0.7387             & 0.7256   & \textbf{0.9687}             & 0.9475              & \underline{0.8991}          \\
Trains                       & 0.5994            & 0.6637                 & 0.4288	           & 0.5612             & 0.6804             & 0.7926             & 0.5493   & \textbf{0.9524}             & 0.8788              & \underline{0.8741}          \\
Attractions                  & 0.5441            & 0.6446                 & 0.3735          & 0.4305             & 0.3029             & 0.3834             & 0.6057   & 0.9316             & \textbf{0.9880}               & \underline{0.8393}          \\
Taxi                         & 0.5903            & 0.4982                 & 0.2968	& 0.3827                        & 0.1260              & 0.1824             & 0.5175   & \textbf{0.9496}             & 0.7991              & \underline{0.8471}          \\
Others                       & 0.5148            & 0.5990                  & 0.2638	& 0.3132             & 0.1201             & 0.1721             & 0.6689   & \textbf{0.9314}             & 0.9183              & \underline{0.9236}          \\ \hline
Average                      & 0.5609            & 0.6426                 & 0.3847	&0.4698             & 0.4152             & 0.5002             & 0.6273   & \textbf{0.9438}             & 0.9092              & \underline{0.8807}    \\
\bottomrule
\end{tabular}
\vspace{-7pt}
\end{table*}


\begin{table*}[t!]
\footnotesize
\centering
\caption{Slot F1 score for the full datasets: SGD and MultiWoZ.}
\vspace{-7pt}
\label{tab:multiwoz-sgd}
\begin{tabular}{l|cc|ccc|cc|c|cc|c}
\toprule
\multicolumn{1}{c|}{}        & \multicolumn{2}{c|}{\textbf{Unsupervised}}                  & \multicolumn{3}{c|}{\textbf{Weak Supervision}}                             & \multicolumn{2}{c|}{\textbf{Zero-shot Learning}}                & \multicolumn{1}{c|}{\textbf{Few-shot}} & \multicolumn{2}{c|}{\textbf{Supervised Learning}}                        & \multicolumn{1}{c}{\textbf{Self-supervised}} \\
\multicolumn{1}{c|}{\textbf{Datasets}} & \multicolumn{1}{c}{\textbf{DSI}} & \multicolumn{1}{c|}{\textbf{RCAP}} & \multicolumn{1}{c}{\textbf{KG}} &\multicolumn{1}{c}{\textbf{Inter-Slot}} & \multicolumn{1}{c|}{\textbf{Merge-Select}} & \multicolumn{1}{c}{\textbf{Coach}} & \multicolumn{1}{c|}{\textbf{LEONA}} & \multicolumn{1}{c|}{\textbf{GPT-3}}    & \multicolumn{1}{c}{\textbf{JointBERT}} & \multicolumn{1}{c|}{\textbf{Seq2Seq-DU}} & \multicolumn{1}{c}{\textbf{{\ourmodel} (this work)}}            \\ \hline
SGD                          & 0.3352                  & 0.4539        &0.4591                    & 0.3274	& 0.5512                            & 0.1102                    & 0.1621                     & 0.6015                        & \textbf{0.9156}                        & 0.9028                          & \underline{0.8756}                              \\
MultiWoZ                     & 0.4961                  & 0.5937     & 0.4839                      & 0.4483	& 0.5219                            & 0.1903                    & 0.2884                     & 0.6123                        & \textbf{0.9323}                        & 0.8923                          & \underline{0.8423}                              \\ \hline
Average                      & 0.4157                  & 0.5238      & 0.4715                     & 0.3879	& 0.5366                            & 0.1503                    & 0.2253                     & 0.6069                        & \textbf{0.9240}                         & 0.8976                          & \underline{0.8590}     \\
\bottomrule
\end{tabular}
\vspace{-7pt}
\end{table*}



%We present in the next subsections quantitative and qualitative analysis of all competing models. We first present the quantitative analysis in Subsection~\ref{results-quantitative} and show that our model consistently outperforms the competing models in all settings. Furthermore, this subsection also has an ablation study that quantifies the role of each conceptual step in our model. We dig deeper into the limitations of each competing model in our qualitative analysis in Subsection~\ref{results-qualitative}.

\subsection{Quantitative Analysis}
\label{results-quantitative}
\stitle{Evaluation using a single domain in the dataset.}
Tables~\ref{tab:sgd-domains} and ~\ref{tab:multiwoz-domains} present F1 scores for each domain in \myspecial{SGD} and \myspecial{MultiWoZ} datasets, respectively. 
The best F1 score is shown as bold including the supervised learning methods that were trained using labeled training data for all the domains.
We also highlight the best model by underlining the results excluding the supervised models.
Our proposed framework {\ourmodel} achieves significant performance gains against all the competing models (excluding supervised models) including few-shot GPT-3 on both datasets for every single domain consistently.
Specifically, {\ourmodel} outperforms SOTA models by 20.20\% and 37.05\% on average for all the domains on \myspecial{SGD} and \myspecial{MultiWoZ} datasets, respectively. 
This performance improvement against all the unsupervised learning, weak supervision, zero-shot learning, and few-shot learning approaches can be attributed to the iterative refinement of the noisy labels throughout all the phases of the framework, especially the self-supervised co-training phase.
Moreover, we also notice that our proposed approach which does not use any in-domain labeled data achieves competitive performance to fully supervised models that were trained using labeled training data. 
Specifically, the performance gap is only 2.58 percentage points on average for all the domains in \myspecial{SGD} dataset and 6.31 percentage points for \myspecial{MultiWoZ}.
Interestingly, {\ourmodel} achieves slightly better performance than supervised models for five domains in \myspecial{SGD} dataset.
This incredible performance improvement is due to the outstanding co-training approach of {\ourmodel} that progressively allows the peers to refine the noisy labels. 
Specifically, the zero-shot learning labels carry information from other (seen) domains about how slot values are mentioned across domains, whereas the other set of labels carries the knowledge from KGs and other NLP models. 
The self-supervised co-training phase facilitates combining this information progressively by sharing high-confidence information among peers.
For example, {\ourmodel} achieves a better F1 score than supervised models in the domain \myspecial{Music}. Upon further analysis, we noticed that most of the values for slots \myspecial{song\_name} and \myspecial{artist} are present in the KG, whereas the domain \myspecial{Movies} is quite similar to \myspecial{Music} and the zero-shot model successfully transferred knowledge to {\ourmodel}. 


\stitle{Evaluation using the full dataset.}
Table~\ref{tab:multiwoz-sgd} presents the results for the full datasets.
We note that the proposed zero-label framework has, once more, comparable performance to the fully supervised learning methods.
For example, the average F1 score is only 6.5 percentage points lower than the best-performing supervised learning model.
It is important to highlight that this evaluation setup is more challenging because {\ourmodel} automatically generates labels for the whole dataset (i.e., \myspecial{SGD} dataset has 240 slot types) with no human intervention and labeling through heuristic functions or zero-shot learning models is extremely error-prone because of the big number of classes, i.e., 240 in case of \myspecial{SGD} dataset.
To highlight the importance of the gradual noise reduction approach of {\ourmodel}, we also experiment with a KG baseline that uses our approach from phase one to generate pseudo labels and then uses Merge-Select~\cite{hudevcek2021discovering} to further refine the labels.
If we compare our proposed approach to the methods that do not have access to labeled data (or only a few labeled training examples), there is a significant performance gain for {\ourmodel}.
Specifically, {\ourmodel} outperforms unsupervised models by 92.91\% and 41.87\% on \myspecial{SGD} and \myspecial{MultiWoZ} datasets, respectively.
Similarly, other learning paradigms show inferior performance when compared to {\ourmodel}. 
For example, the F1 score of the weak supervision models is 32.44 and 32.04 percentage points lower than our proposed approach for \myspecial{SGD} and \myspecial{MultiWoZ} datasets, respectively.
Last but not least, our performance improvement over SOTA results is statistically significant, with a P-value < 0.01 (i.e., highly significant) for all evaluation settings.



\subsection{Ablation study}
\label{ablation}


\begin{table}[t!]
\centering
\caption{Ablation Study: slot F1 score when certain components are removed from {\ourmodel}}
\vspace{-7pt}
\label{tab:ablation}
\begin{tabular}{lcc}
\toprule
Configuration         & SGD    & MultiWoZ \\
\hline
$\wo$ KG Labels           & 0.8231 & 0.7838   \\
$\wo$ POS / NER Taggers   & 0.7954 & 0.7762   \\
$\wo$ KG, POS/NER Taggers & 0.5767 & 0.5923   \\
$\wo$ Zero-shot Labels    & 0.5091 & 0.4786   \\
$\wo$ Co-training         & 0.5712 & 0.5681   \\
$\wo$ Soft labels         & 0.7145 & 0.6992  \\
\bottomrule
\end{tabular}
\vspace{-7pt}
\end{table}


\stitle{Effect of removing a certain component.} 
Since our proposed framework {\ourmodel} uses several mature components, it is critical to investigate the role of each component.
Table~\ref{tab:ablation} presents the results on both datasets by removing a certain component. 
For example, if we remove KG from phase one and generate automatic labels, the performance slightly declined.
Similarly, removing the pre-trained POS and NER taggers also makes the performance worse.
Moreover, we also note that the effect of removing POS and NER taggers is somewhat greater than KG.   Upon further investigation, we observed that many of the KG entries are eventually automatically labeled in phase two of the framework by the pre-trained BERT.
Moreover, we also look into the result of removing one set of labels entirely from the framework. 
We notice that with only one set of labels, our proposed framework can not utilize phase three effectively and, as a result, shows significantly poor performance.
Moreover, we also note that the effect of removing the zero-shot learning labels is more notable quantitatively as compared to the labels from KG, POS/NER taggers.
The significance of phase three (i.e., self-supervised co-training) is also highlighted by skipping this phase.
In fact, by skipping this phase or not being able to use this phase properly, our proposed framework shows poor performance as compared to other unsupervised or weak supervision learning approaches.
Finally, we also study the effect of high-confidence soft label selection in phase three.
As expected, removing the high-confidence soft label selection mechanism in phase three significantly reduces the performance on both datasets. 
To summarize, {\ourmodel} can still outperform other models that do not use labeled data even after removing these individual components: KG labels, POS/NER taggers, and soft labels.



\stitle{Effect of increasing the out-of-domain data for zero-shot slot filling model.} 
The zero-shot learning models have been shown to improve their performance on new unseen (target) domains, when they are provided with more (out-of-domain) data~\cite{siddique2021linguistically, liu2020coach}. 
We also investigated whether increasing out-of-domain training data for the zero-shot slot filling model improves the performance of our model.
Table~\ref{tab:seen-percentage} presents the F1 score of our proposed framework for both datasets when we vary the percentage of out-of-domain data to train the zero-shot slot filling model.
It can be observed from the results that increasing the out-of-domain training data for the zero-shot learning model improves its prediction accuracy for the target domains, which eventually improves the performance of {\ourmodel}.
Following~\cite{siddique2021linguistically}, we randomly selected domains for out-of-domain training of the zero-shot model and reported average results of five runs.



\begin{table}[t!]
\footnotesize
\centering
\caption{Slot F1 score for different percentages of out-of-domain training data for zero-shot learning model.}
\vspace{-10pt}
\label{tab:seen-percentage}
\begin{tabular}{l|ccc|ccc}
\toprule
Dataset   $\rightarrow$            & \multicolumn{3}{c|}{\textbf{SGD}}  & \multicolumn{3}{c}{\textbf{MultiWoZ}} \\
OOD Seen \% $\rightarrow$ & 25\%   & 50\%   & 75\%   & 25\%     & 50\%    & 75\%    \\ \hline
Coach                 & 0.5888 & 0.6419 & 0.6725 & 0.4408   & 0.4505  & 0.6522  \\
LEONA                 & 0.7181 & 0.7925 & 0.8324 & 0.5248   & 0.5533  & 0.8581  \\ \hline
\textbf{{\ourmodel} (this work)}                  & 0.8592 & 0.9324 & 0.9512 & 0.7937   & 0.8282  & 0.9472 \\
\bottomrule
\end{tabular}
\vspace{-12pt}
\end{table}





