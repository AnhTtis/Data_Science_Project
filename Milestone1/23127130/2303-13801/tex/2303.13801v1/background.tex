\section{Preliminaries}
\label{problem}
\subsection{Problem Formulation}

Given a natural language text $\bigX_\smalli = (\smallx_\one, \smallx_\two,\cdots, \smallx_{|\bigX_\smalli|})$, the value for a slot $\bigS_\smallr$ is a contiguous segment of tokens $(\smallx_\smallj,\cdots,\smallx_\smallk)$ such that $0 \leq \smallj \leq \smallk \leq |\bigX_\smalli|$.
Slot filling is generally formulated as a token classification task that predicts the labels $\bigY_\smalli = (\smally_\one, \smally_\two,\cdots, \smally_{|\bigX_\smalli|})$ for the input ${\bigX_\smalli}$ using the \myspecial{IOB} labeling scheme~\cite{ramshaw-marcus-1995-text}. 
Specifically, the first token that points to the value of a slot $\bigS_\smallr$ is provided the label \myvalue{B}-$\bigS_\smallr$, the other tokens inside the slot value are given the label \myvalue{I}-$\bigS_\smallr$, whereas non-slot tokens are labeled as \myvalue{O}.
%
In the traditional supervised setting, a dataset with $\smalln$ token-level labeled training examples $\{(\bigX_\smalli, \bigY_\smalli)\}_{\smalli=1}^\smalln$ is provided to train the model, whereas acquiring such dataset for each new slot is expensive and labor-intensive. In this work, we focus on a challenging problem setup where we are  provided with unlabeled training examples $\{\bigX_\smalli\}_{\smalli=1}^\smalln$ only, and corresponding labels $\{\bigY_\smalli\}_{\smalli=1}^\smalln$ are not available.
In contrast to the named entity recognition (NER) task where only a limited set of entities (e.g., \myspecial{LOC}, \myspecial{PER}, \myspecial{ORG}) is considered, the slot filling task is more demanding because a wide range of slot types (e.g., \myspecial{price}, \myspecial{destination}, \myspecial{departure}\_\myspecial{time} in the \myspecial{Buses} domain) may appear in the natural language text.



\subsection{Pre-trained NLP Models}
\label{sec:nlpmodels}
This work uses several pre-trained and readily-available natural language processing (NLP) models.
In the following, we provide a brief overview of these models.


\stitle{Pre-trained POS tagger.}
The POS tagger is trained to predict part-of-speech tags for each token in a given natural language text, such as \myspecial{PROPN}, \myspecial{VERB}, and \myspecial{ADJ}.
The POS tags provide useful syntactic cues for slot value identification from natural language texts.
Since POS tagger is domain-independent, it facilitates automatic label generation for the task of open-domain slot filling.
For example, the values for slots are generally tagged as proper nouns by the POS tagger. 
We use the production-ready pre-trained POS tagger\footnote{\url{https://spacy.io/api/annotation\#pos-tagging}} from SpaCy that has produced a high performance for the task. 



\stitle{Pre-trained NER model.}
The NER model tags each token in the natural language text for the mentions of the four entities: \myspecial{LOC}, \myspecial{PER}, \myspecial{ORG}, and \myspecial{MISC}.
Although a limited number of entities are tagged by the NER model, these entities can be used to provide cues for many slots (though not all).
For example, \myspecial{LOC} can be used to narrow down the search to \myspecial{source} or \myspecial{destination} slots in the \myspecial{Buses} domain.
Nevertheless, it still proves challenging to automatically assign labels to the given natural language text for the slot filling task.
Furthermore, open-domain NER models are still evolving and may miss mentions of the entities, especially slots of interest in the target domain, which results in a low recall. 
Nonetheless, the NER model facilitates automatic labeling and reduces the task's complexity.
We leverage the pre-trained NER model\footnote{\url{https://spacy.io/api/annotation\#named-entities}} from SpaCy.


\stitle{Pre-trained GPT-2.} 
GPT-2~\cite{radford2019language} is trained for predicting the next token, i.e., autoregressive objective, using 40 GB textual data of WebText dataset in an unsupervised setting.
It also follows transformer architecture~\cite{vaswani2017attention}. 
For a given natural language sequence $(\smallx_1, \cdots , \smallx_{|\smallx|})$ where token $\smallx_\smalli$ is sampled from a fixed vocabulary of tokens, factorizing the joint probabilities over tokens as a product of conditional probabilities~\cite{bengio2003neural} leads to $ \prod_{i=1}^{|\smallx|} p(\smallx_\smalli | \smallx_1, \cdots ,\smallx_{\smalli-1})
$.
This approach not only enables tractable sampling, but also allows for computing $p(\smallx)$, and estimating any conditionals of the form: $p(\smallx_{\smalli - k}, \cdots , \smallx_\smalli | \smallx_1, \cdots , \smallx_{\smalli - k - 1} )$.
In this work, we use the pre-trained GPT-2 model\footnote{\url{https://huggingface.co/gpt2}} to score a given natural language sequence by computing the log probabilities of the tokens to select the best hypothesis for automatic label generation in phase one of {\ourmodel}.

\stitle{Pre-trained BERT.}
BERT~\cite{DBLP:journals/corr/abs-1810-04805} is an unsupervised model that follows the transformer architecture~\cite{vaswani2017attention} by conditioning on bidirectional contexts. 
In contrast to traditional unidirectional language modeling~\cite{bengio2003neural}, BERT has been trained to condition on both left and right contexts simultaneously across all layers to learn deep bidirectional representations from the unlabeled text.
It is trained for masked language model (MLM) and the next sentence prediction (NSP) task. 
In the MLM task, for a given natural language sequence $(\smallx_1, \smallx_2, \cdots, [MASK], \cdots, \smallx_{|\smallx|})$, the goal is to predict $[MASK]$ tokens successfully, whereas the next sentence prediction task focuses on whether sentence B naturally follows sentence A, given two natural language sequences $(\smallx_1, \cdots, \smallx_{|\smallx|})$ and $(\smallx^\prime_1, \cdots, \smallx^\prime_{|\smallx^\prime|})$, using the BooksCorpus (i.e., 800 million words)~\cite{zhu2015aligning} and textual data from Wikipedia English articles (i.e., 2,500 million words). 
The pre-trained BERT has produced state-of-the-art results on many natural language understanding benchmarks, including slot filling task ~\cite{chen2019bert} in the supervised setting.
In our proposed framework {\ourmodel}, we employ the pre-trained multilingual BERT\footnote{\url{https://huggingface.co/bert-base-multilingual-cased}} as the backbone model and adapt it for open-domain slot filling task.

\subsection{Knowledge Graphs}
Knowledge graphs (KG) are used to store information about entities or concepts and are often used as knowledge sources for a range of NLP tasks. 
In this work, we use Google Knowledge Graph Search API
\footnote{\url{https://developers.google.com/knowledge-graph}} to retrieve type information about entity mentions in the natural language.
Information from KG facilitates the generation of several hypotheses about slot labels in phase one of the framework.
Moreover, matching the n-grams in the natural language text to the KG entities also assists in improving recall for the cases where entities or concepts are missed by POS and NER taggers.


\subsection{Zero shot Learning}
Zero-shot learning models have the capability to generalize to new unseen classes, not encountered during training.
Zero-shot learning is one of the promising approaches to address the data scarcity issue.
A few zero-shot learning models~\cite{liu2020coach,shah2019robust,bapna2017towards,siddique2021linguistically} have been proposed for the slot filling task in the literature that has shown encouraging results.
Yet, there exists a huge performance gap compared to the supervised slot filling models.
Nonetheless, zero-shot slot filling models can generate decent quality labels (though noisy) for new unseen domains and slots at \emph{zero cost}.
In this work, we pre-train a zero-shot slot filling model~\cite{liu2020coach} on the out-of-domain slot filling dataset. 
Since the zero-shot slot filling model can infer labels for unseen classes, we use the trained model to generate pseudo labels for the target domain/dataset in phase one.
