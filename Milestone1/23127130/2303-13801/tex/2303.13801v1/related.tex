\section{Related Work}
\label{related}


\stitle{Supervised Slot Filling.}
Supervised approaches for the slot filling task have been extensively studied.
Recurrent neural networks have been leveraged to learn the temporal interactions of the tokens within a sentence using  Long Short-Term Memory (LSTM) or Gated Recurrent Unit (GRU) networks and detect the spans of the text associated with certain slots~\cite{mesnil2014using,kurata2016leveraging}. 
Moreover, deep learning-based approaches have employed Conditional Random Fields (CRFs) to learn global interactions along with LSTMs and GRUs~\cite{huang2015bidirectional,reimers2017optimal}.
The self-attention technique has also been used for token classification tasks, such as slot filling in the supervised setting~\cite{shen2017disan,tan2017deep}.
Moreover, formulations exist to jointly optimize the tasks of slot filling and intent detection in the supervised setting~\cite{goo2018slot, hakkani2016multi, liu2016attention, zhang2018joint, xu2013convolutional}.
The capsule neural network was trained to jointly model the tasks of slot filling and intent detection in ~\cite{zhang2018joint}, pre-trained BERT was fine-tuned in ~\cite{chen2019bert}, and both tasks were formulated as sequence-to-sequence labeling task in~\cite{feng2020sequence}.
Although supervised approaches have proven effective for the task of slot filling, they require labeled training data for every new domain as well as slot, which prohibits their scalability. In this work, we focus on the open-domain slot filling task and propose a zero-label slot filling framework that can automatically generate labels with no human intervention.




\stitle{Few-shot and Zero-shot Slot Filling.}
To minimize the human labor of annotating data for each new slot, zero-shot learning along with few-shot learning methods have shown promising results for the slot filling task.
Few-shot learning methods rely on a small set of in-domain labeled data to nudge the model for accurate predictions. 
Recently, employing regular expressions~\cite{luo2018marrying}, the prototypical network~\cite{fritzler2019few}, and extensions to CRFs~\cite{hou2020few} have been proposed for the few-shot slot filling task.
Furthermore, several works~\cite{krone2020learning,bhathiya2020meta} have leveraged model agnostic meta-learning to jointly model the slot filling and intent classification tasks.
In a similar line of work, in the zero-shot setting, no in-domain labeled training data is required, which has shown encouraging results to eliminate the need for manual labeling. 
Using LSTM-based models along with the natural language descriptions of the slots has been shown to overcome the challenge of different names of the slot for apparently similar slots (e.g., \myspecial{city} vs \myspecial{destination} in \myspecial{Travel} domain) for the zero-shot adaptation of slot filling task~\cite{bapna2017towards}.
Similarly, attention mechanisms have further improved the performance~\cite{mukherjee2020uncertainty,lee2019zero}.
The concatenation of character and word level embeddings have been employed along with bidirectional LSTMs~\cite{shah2019robust}. 
Moreover, the coarse-to-fine approach in ~\cite{liu2020coach, siddique2021linguistically} was proposed to improve the robustness of the zero-shot slot filling task.
In spite of promising results, there is a huge performance gap between zero-shot learning methods and supervised learning approaches that limit their applicability.
In this work, we employ a zero-shot slot filling model to automatically generate a set of labels in the target domain and further improve the performance of the slot filling task in an open-domain setting.



\stitle{Unsupervised and Weak Supervision Slot Filling.}
The other line of work to eliminate the need for manual labeling is weak supervision and unsupervised learning. 
Unsupervised learning methods~\cite{siddique2021unsupervised} 
have used mining to build latent variable models~\cite{min2020dialogue}.
Moreover, a coarse-to-fine approach using clustering has been explored in~\cite{zeng2021automatic}.
Furthermore, extensions to the DBSCAN~\cite{chatterjee2020intent}, automatic induction of slot values using frame semantics theory~\cite{chen2013unsupervised}, and autoencoders along with hierarchical clustering~\cite{shi2018auto} have been proposed in the unsupervised learning setup.
The weak supervision approaches generally employ a heuristic function~\cite{ren2015clustype,he2017autoentity,fries2017swellshark} to automatically generate labels (i.e., no human intervention) and overcome the challenge of modeling on the noisy labels by employing methods, such as distant-LSTM-CRF~\cite{giannakopoulos2017unsupervised} and dictionaries~\cite{shang2018learning}.
Recently, weak supervision has also been leveraged using clustering algorithms for the automatic selection of candidate values~\cite{hudevcek2021discovering}. Furthermore, random walk inference algorithms were used in~\cite{chen2015jointly} to automatically generate supervision data.
It is important to recall that weak supervision and unsupervised learning models show inferior performance as compared to supervised learning models.
In this work, we employ a GPT-2-based heuristic function to automatically generate one set of labels using KG and other pre-trained NLP models and our proposed framework {\ourmodel} combines this set of labels with the labels generated through the zero-shot model by introducing a self-supervised co-training mechanism.