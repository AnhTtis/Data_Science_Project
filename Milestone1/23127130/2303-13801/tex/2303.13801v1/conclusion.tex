\section{Conclusion}
\label{conclusion}
We have presented a self-supervised co-training slot filling framework that automatically generates two sets of complementary pseudo labels for co-training two peers in a self-supervised fashion, progressively reduces the amount of noise in the labels by automatically selecting high-confidence soft labels, and improves their prediction power.
In contrast to zero-shot learning and weak supervision approaches that produce inferior quality results as compared to supervised learning approaches, the proposed framework {\ourmodel} achieves competitive performance to supervised slot filling models and shows the promise of open-domain slot filling without any human labeling.
Our extensive evaluations using \myspecial{SGD} and \myspecial{MultiWoZ} datasets that contain dialogs across 20 domains and cover 240 slots, demonstrate that {\ourmodel} outperforms state-of-the-art few-shot GPT-3 by 44.18\% and 40.40\% on average for \myspecial{SGD} and \myspecial{MultiWoZ} datasets, respectively.

