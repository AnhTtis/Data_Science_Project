\begin{figure}[t!]
\centering
   \includegraphics[width=0.99\linewidth]{fig1.pdf}
    \caption{(a) State-of-the-art approaches that do not require manual labels show poor performance as compared to supervised learning models. (b) The proposed self-supervised co-training framework shows comparable performance to state-of-the-art supervised learning models.}
    \label{fig:intro}
    \vspace{-8pt}
\end{figure}

\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.95\linewidth]{fig2.pdf}
  \caption{Overview of {\ourmodel}. The self-supervised co-training phase facilitates significant performance gains.  
  }
  \vspace{-12pt}
  \label{fig:intro_model}
\end{figure*}
\section{Introduction}
\label{intro}
Conversational Systems empower users by facilitating interactions through expressive and accessible natural language. 
These systems need to accurately perform \emph{slot filling}, among other tasks, in order to fulfill usersâ€™ requests in task-oriented dialog systems or make meaningful recommendations in conversational recommendation systems.
Slot filling is the task of detecting contiguous spans of text that correspond to a slot of interest, usually referred to as a query parameter. 
Figure~\ref{fig:intro} highlights values for various slots.
Due to the vast diversity in users' expressions, slot filling is a challenging task that has been studied extensively in the supervised setting~\cite{goo2018slot,zhang2018joint,young2002talking,bellegarda2014spoken,mesnil2014using,kurata2016leveraging,hakkani2016multi,xu2013convolutional}.
The critical drawback of the supervised learning slot filling methods is that they require a massive amount of labeled training data for each new domain (and slot type) which can be costly and time-consuming. 
This \emph{unscalable} requirement has become a major bottleneck and renders supervised learning methods ineffective because practical conversational systems need to continuously expand their conversational capabilities (e.g., \myspecial{Alexa} \myspecial{Skills}) and be able to converse in new emerging domains. 


Recently, researchers have proposed a wide range of approaches to address the label scarcity issue for the slot filling task, such as zero-shot learning~\cite{liu2020coach,siddique2021linguistically} and weak supervision~\cite{hudevcek2021discovering,chen2015jointly}.
Zero-shot learning methods~\cite{siddique2021generalized,siddique2022personalizing} are capable of classifying instances of new unseen classes at inference time, they had not encountered during training, and thus do not require training data for each new domain.
Weak supervision approaches eliminate the need for manually labeled data by automatically generating noisy labels using a heuristic labeling function, powered by almost freely-available external knowledge bases, off-the-shelf models (e.g., NER models), or their combination.
Both learning paradigms have emerged as promising alternatives to manual labeling of the data for every single domain.
However, both approaches suffer from significantly poor performance when compared to the supervised methods.
Motivated by this huge performance gap and the pressing need for slot filling models with no restriction on the domain, we explore the possibility of developing an open-domain slot filling model with comparable performance to the supervised models without incurring the cost and effort associated with manual annotations.

We introduce a \textbf{S}elf-supervised \textbf{Co}-\textbf{t}raining framework, {\ourmodel}, that automatically generates pseudo labels for initial supervision, then it selects high-confidence soft labels to gradually guide the framework to superior prediction performance by leveraging the power of pre-trained language models. 
The proposed framework is also presented in Figure~\ref{fig:intro_model}.
{\ourmodel} employs the pre-trained multilingual BERT and works in three phases.
Phase one acquires two sets of complementary pseudo labels automatically, with no need for human intervention. 
The first set of pseudo labels is generated using a combination of knowledge graphs, off-the-shelf taggers (e.g., NER/POS taggers), and a GPT-2-based~\cite{radford2019language} scorer. 
The other set of pseudo labels is obtained using  a zero-shot slot filling model that has been trained on the out-of-domain dataset and does not require a single in-domain training example. 
Essentially, the first set of labels exploits freely available and domain-independent knowledge sources to acquire labels, whereas the other set of labels leverages the power of zero-shot learning to pick up generalizable patterns of how slot values are mentioned in other domains.


Phase two leverages the power of the pre-trained language models by adapting the multilingual BERT to the task of slot filling.
This phase fine-tunes the model using pseudo labels from phase one with the early stopping strategy to avoid overfitting to the noisy labels.
This phase overcomes the challenge of incomplete and noisy labels to some extent.
Using these sets of pseudo labels from phase one, both models are trained that are capable of making better quality predictions, so pseudo labels from phase one are dropped.
Phase three introduces an iterative self-supervised co-training mechanism that further trains the models on the soft labels.
This phase also proposes a peer training approach where one peer automatically generates soft labels for training the other peer in the next iteration. 
Using this progressive self-supervised training strategy, we can exploit the patterns of slot values in other domains as well as information from external knowledge sources.
Moreover, the high-confidence soft label selection mechanism facilitates efficient learning and enables better model fitting through quality soft labels.
Phase three is the most critical phase that facilitates significant performance gains for the open-domain slot filling task.

Our evaluations using two benchmark datasets, \myspecial{SGD}~\cite{rastogi2019towards} and \myspecial{MultiWoZ}~\cite{zang-etal-2020-multiwoz}, that span up to 20 domains, demonstrate that {\ourmodel} achieves significant performance gains as compared to the learning paradigms~\cite{liu2020coach,siddique2021linguistically,hudevcek2021discovering,chen2015jointly,min2020dialogue,zeng2021automatic} that do not require in-domain labeled data for training. 
Moreover, {\ourmodel} with zero in-domain labeled training examples~\cite{maqbool2022zero}, achieves \emph{competitive} performance as compared to \emph{fully} supervised slot filling models~\cite{chen2019bert,feng2020sequence}.


The contributions of this work are summarized as follows:
\begin{itemize}[leftmargin=1.2\parindent,labelindent=-1pt, itemsep=-1pt]

    \item We propose an iterative self-supervised co-training framework to enable open-domain slot filling that requires zero in-domain manually labeled training examples.

    \item We demonstrate that combining the power of zero-shot learning with external knowledge sources can lead to superior performance and better model fitting.

    \item Our comprehensive experimental analysis using two public datasets \myspecial{SGD} and \myspecial{MultiWoZ} shows that {\ourmodel} outperforms state-of-the-art models consistently and achieves comparable performance to fully supervised models.
\end{itemize}