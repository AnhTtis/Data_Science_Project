\section{Implementation Details}
In this work, we use several pre-trained models, such as POS tagger, NER model, GPT-2, zero-shot slot filling model, multilingual BERT, and an API to query entities from the knowledge graph.
The specific model names along with their versions have been presented in Section~\ref{problem}. 
Here, we present further implementation details and values of (hyper)parameters for each phase of the proposed framework {\ourmodel}.

\stitle{Automatic label generation.}
We use a python package \myspecial{fuzzywuzzy}\footnote{\url{https://pypi.org/project/fuzzywuzzy/}} for partial matching of n-grams and entities in the KG. 
Partial matching is performed on case-insensitive strings. If the matching ratio is over 80\%, we consider it a match in both datasets. We set this threshold a bit lower because we want to generate many potential candidates.
Moreover, the lower threshold does not hurt us because the GPT-2-based scorer selects only the top candidate hypothesis for labeling.
Finally, if all candidates score less than half of the original sentence for fluency, we do not consider any candidate, and the natural language text is labeled as all \myspecial{O}s (i.e., outside tag).
We manually set the above parameters based on a small validation set (100 training examples) from \myspecial{SGD} dataset and remained consistent with these values throughout the experiments.
%
We employed the zero-shot slot filling model from~\cite{liu2020coach} and used their public implementation\footnote{\url{https://github.com/zliucr/coach}} and did not change any parameters to generate the other set of pseudo labels.


\stitle{LM-assisted weak supervision.}
We use pre-trained multilingual BERT and added token classification head \footnote{\url{https://huggingface.co/docs/transformers/model_doc/bert\#transformers.BertForTokenClassification}} as the final layer.
We did not change anything in the architecture of the model.
We set batch size = 16, dropout = 0.1, $\beta_1$ = 0.9, $\beta_2$ = 0.98, an initial learning rate of $3\times10^{-5}$, and a linear decay of $10^{-2}$ in learning rate for AdamW optimizer that trains the models for 10 epochs with early stopping with five different random initializations and predictions are averaged at the end of phase two.
Since BERT uses WordPiece for tokenizing the natural language text, we put the tokenized words back to generate pseudo labels (or soft labels) to remain consistent with the original labeling scheme of the slot filling task.

\stitle{Self-supervised co-training.}
For high-confidence label selection, we use $\epsilon$ = 0.9.
If the performance of one peer does not improve at any iteration, we re-initialize it with the pre-trained BERT and resume its training with the soft labels generated by the other peer at that iteration.
Once both peers start generating very similar labels (i.e., converged), we stop training in this phase. 
In our experiments, our models converged before 100 iterations on both datasets.
