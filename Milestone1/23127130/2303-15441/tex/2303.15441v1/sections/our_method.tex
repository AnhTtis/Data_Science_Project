\section{Method}
\label{sec:our_method}
\vspace{-1mm}
This section firstly describes our method to generate counterfactual images guided by CLIP in a zero-shot manner. We then introduce how we perform the sensitivity analysis across attributes of interest. Fig.~\ref{fig:model} shows the overview of our framework.

\subsection{Notation and Problem Definition}

Let $f_\theta$, parameterized by $\theta$, be the target model that we want to diagnose. In this paper, $f_\theta$ denotes two types of deep nets: binary attribute classifiers and face keypoint detectors. Note that our approach is extendable to any end-to-end differentiable target deep models. Let $\mathcal{G}_\phi$, parameterized by $\phi$, be the style generator that synthesizes images by $\mathbf{x} = \mathcal{G}_\phi(\mathbf{s})$ where $\mathbf{s}$ is the style vector in Style Space $\mathcal{S}$ \cite{stylespace}.
%Our goal is to search and visualize the $\hat{u}$ following the user's flexible prompt, which can inspire future application of model debiasing. 
We denote a counterfactual image as $\mathbf{\hat{x}}$, which is a synthesized image that misleads the target model $f_\theta$, and denote the 
original reference image as $\mathbf{x}$. $a$ is defined as a single user input text-based attribute, with its domain $\mathcal{A}  = \{a_i\}_{i=1}^N$ for $N$ input attributes.
$\mathbf{\hat{x}}$ and $\mathbf{x}$ differs only along attribute directions $\mathcal{A}$. Given a set of $\{f_\theta, \mathcal{G}_\phi, \mathcal{A}\}$, our goal is to perform counterfactual-based diagnosis to interpret where the model fails without manually collecting nor labeling any test set. 
% The counterfactual images will be created with adversarial learning; however, 
Unlike traditional approaches of image-space noises which lack explainability to users, our method adversarially searches the counterfactual in the user-designed semantic space. To this end, our diagnosis will have three outputs, namely counterfactual images (Sec.~\ref{sec:Counterfactual_Synthesis}), sensitivity histograms (Sec.~\ref{sec:Attribute_Sensitivity_Analysis}), and distributionally robust models (Sec.~\ref{sec:ct}).

\subsection{Extracting Edit Directions}\label{sec:global_direction_extraction}
\vspace{-1mm}
%In order to make the paper self-contained, 
This section examines the terminologies, method, and modification we adopt in \ourmodel to extract suitable global directions for attribute editing. Since CLIP has shown strong capability in disentangling visual representation \cite{2022Disentangling}, we incorporate 
style channel relevance from StyleCLIP~\cite{2021StyleCLIP} to find edit directions for each attribute. 

 Given the user's input strings of attributes, we want to find an image manipulation direction $\Delta \mathbf{\mathbf{s}}$ for any $\mathbf{s} \sim \mathcal{S}$, such that the generated image $\mathcal{G}_\phi(\mathbf{s} + \Delta{ \mathbf{s}) }$ {\em only} varies in the input attributes. Recall that CLIP maps strings into a text embedding $\mathbf{t} \in \mathcal{T}$, the text embedding space. For a string attribute description $a$ and a neutral prefix $p$, we obtain the CLIP text embedding difference $\Delta\mathbf{t}$ by:
\begin{align}
        \Delta\mathbf{t} = \operatorname{CLIP_{text}}(p \oplus a) - \operatorname{CLIP_{text}}(p)
                % \Delta\mathbf{t} = \{&\operatorname{CLIP}_{text}(p \oplus a_i) -\operatorname{CLIP}_{text}(p)\}_{i=1}^{N}
\end{align}
where $\oplus$ is the string concatenation operator. To take `Eyeglasses' as an example, we can get $\Delta\mathbf{t}= \operatorname{CLIP_{text}}(\textnormal{`a face with Eyeglasses'}) -\operatorname{CLIP_{text}}(\textnormal{`a face'}) $. 

To get the edit direction, $\Delta \mathbf{s}$, we need to utilize a style relevance mapper $\mathbf{M} \in \mathbbm{R}^{c_\mathcal{S} \times c_\mathcal{T}}$ to map between the CLIP text embedding vectors of length $c_\mathcal{T}$
and the Style space vector of length $c_\mathcal{S}$. 
StyleCLIP optimizes $\mathbf{M}$ by iteratively searching meaningful style channels: mutating each channel in $\mathcal{S}$ and encoding the mutated images by CLIP to assess whether there is a significant change in $\mathcal{T}$ space. To prevent undesired edits that are irrelevant to the user prompt, the edit direction $\Delta \mathbf{s}$ will filter out channels that the style value change is insignificant:
\begin{align}
    \label{eq:CRM}
    \Delta \mathbf{s} &= (\mathbf{M} \cdot \Delta\mathbf{t}) \odot \mathbbm{1}((\mathbf{M} \cdot \Delta\mathbf{t}) > \lambda),
\end{align}
 where $\lambda$ is the hyper-parameter for the threshold value. $\mathbbm{1}(\cdot)$ is the indicator function, and $\odot$ is the element-wise product operator. Since the success of attribute editing by the extracted edit directions will be the key to our approach, Appendix A will show the capability of CLIP by visualizing the global edit direction on multiple sampled images, conducting the user study, and analyzing the effect of $\lambda$.

\subsection{Style Counterfactual Synthesis}
\label{sec:Counterfactual_Synthesis}

Identifying semantic counterfactuals necessitates a manageable parametrization of the semantic space for effective exploration. For ease of notation, we denote $(\Delta \mathbf{s})_{i}$ as the global edit direction for $i^{th}$ attribute $a_i \in \mathcal{A}$ from the user input. After these $N$ attributes are provided and the edit directions are calculated, we initialize the control vectors $\mathbf{w}$ of length $N$ where the $i^{th}$ element $w_i$ controls the strength of the $i^{th}$ edit direction. Our counterfactual edit will be a linear combination of normalized edit directions: $\mathbf{s}_{edit} = \sum_{i=1}^N w_i \frac{(\Delta \mathbf{s})_{i}}{||(\Delta \mathbf{s})_{i}||}$.

The black arrows in Fig.~\ref{fig:model} show the forward inference to synthesize counterfactual images. Given the parametrization of attribute editing strengths and the final loss value, our framework searches for counterfactual examples in the optimizable edit weight space. The original sampled image is $\mathbf{x} = G_{\phi}(\mathbf{s})$, and the counterfactual image is
\begin{align}
\label{eq:total_edit}
    \mathbf{\hat{x}} = G_{\phi}(\mathbf{s} + \mathbf{s}_{edit}) = G_{\phi}\left(\mathbf{s} + \sum_{i=1}^N w_i \frac{(\Delta \mathbf{s})_{i}}{||(\Delta \mathbf{s})_{i}||}\right),
\end{align}
which is obtained by minimizing the following loss, $\mathcal{L}$, that is the weighted sum of three terms:
\begin{align}
    \mathcal{L} (\mathbf{s}, \mathbf{w}) &= \alpha \mathcal{L}_{target}(\mathbf{\hat{x}}) + \beta \mathcal{L}_{struct}(\mathbf{\hat{x}}) + \gamma \mathcal{L}_{attr}(\mathbf{\hat{x}}).
\end{align}
 We back-propagate to optimize $\mathcal{L}$ w.r.t the weights of the edit directions $\mathbf{w}$, shown as the red pipeline in Fig.~\ref{fig:model}.

 The targeted adversarial loss $\mathcal{L}_{target}$ for binary attribute classifiers minimizes the distance between the current model prediction $f_{\theta}(\mathbf{\hat{x}})$ with the flip of original prediction $\hat{p}_{cls} = 1 - f_{\theta}(\mathbf{x})$. In the case of an eyeglass classifier on a person wearing eyeglasses, $\mathcal{L}_{target}$ will guide the optimization to search $\mathbf{w}$ such that the model predicts no eyeglasses. For a keypoint detector, the adversarial loss will minimize the distance between the model keypoint prediction with a set of \textit{random} points $\hat{p}_{kp} \sim \mathcal{N}$:
\begin{align}
    &\text{(binary classifier) }\mathcal{L}_{target}(\mathbf{\hat{x}}) = {L}_{CE}(f_{\theta}(\mathbf{\hat{x}}), \hat{p}_{cls}),\\
    &\text{(keypoint detector) }\mathcal{L}_{target}(\mathbf{\hat{x}}) = {L}_{MSE}(f_{\theta}(\mathbf{\hat{x}}), \hat{p}_{kp}).
\end{align}


If we only optimize $\mathcal{L}_{target}$ w.r.t the global edit directions, it is possible that the method will not preserve image statistics of the original image and can include the particular attribute that we are diagnosing. To constrain the optimization, we added a structural loss $\mathcal{L}_{struct}$ and  an attribute consistency loss $\mathcal{L}_{attr}$ to avoid  generation collapse. $\mathcal{L}_{struct}$ \cite{SSIM} aims to preserve global image statistics of the original image $\mathbf{x}$ including image contrasts, background, or shape identity  during the adversarial editing. While $\mathcal{L}_{attr}$ enforces that the target attribute (perceived ground truth) be consistent on the style edits. For example, when diagnosing the eyeglasses classifier, \ourmodel preserves the original status of eyeglasses and precludes direct eyeglasses addition/removal. 
\begin{align}
    \mathcal{L}_{struct}(\mathbf{\hat{x}}) &= L_{SSIM}(\mathbf{\hat{x}}, \mathbf{x})\\
    \mathcal{L}_{attr}(\mathbf{\hat{x}}) &= {L}_{CE}(\operatorname{CLIP}(\mathbf{\hat{x}}), \operatorname{CLIP}(\mathbf{x})) % \|\mathbf{s}_{edit}\|_2^2
\end{align}

Given a pretrained target model $f_{\theta}$, a domain-specific style generator $G_{\phi}$, and a text-driven attribute space $\mathcal{A}$, our goal is to sample an original style vector $\mathbf{s}$ for each image and search its counterfactual edit strength $\mathbf{\hat{w}}$:
\begin{align}
    \mathbf{\hat{w}} &= \argmin_{\mathbf{w}} \mathcal{L} (\mathbf{s}, \mathbf{w}).% \argmin_{w}\mathcal{L}\left(f_{\theta},G_{\phi},\mathcal{A} \right)
\end{align}
\looseness=-1
Unless otherwise stated, we iteratively update $ \mathbf{w}$ as:  

\begin{align}
    \mathbf{w} &= \mbox{clamp}_{\mathrm{[-\epsilon, \epsilon]}}(\mathbf{w} - \eta \nabla_{\mathbf{w}} \mathcal{L}),
\end{align}
where $\eta$ is the step size and $\epsilon$ is the clamp bound to avoid synthesis collapse caused by exaggerated edit. Note that the maximum counterfactual effectiveness does not indicate the maximum edit strength (i.e., $w_i = \epsilon$), since the attribute edit direction does not necessarily overlap with the target classifier direction. The attribute change is bi-directional, as the $w_i$ can be negative in Eq.~\ref{eq:total_edit}. Details of using other optimization approaches (e.g., linear approximation \cite{madry2018towards}) will be discussed in Appendix C. 

\subsection{Attribute Sensitivity Analysis}
\label{sec:Attribute_Sensitivity_Analysis}

Single-attribute counterfactual reflects the sensitivity of target model on the individual attribute. By optimizing independently along the edit direction for a single attribute and averaging the model probability changes over images, our model generates independent sensitivity score $h_{i}$ for each attribute $a_{i}$:
\begin{align}
h_{i} = \mathbb{E}_{\mathbf{x}\sim\mathcal{P}(\mathbf{x}), \mathbf{\hat{x}} = \text{\ourmodel}(\mathbf{x}, a_{i}) }|f_{\theta}(\mathbf{x}) - f_{\theta}(\mathbf{\hat{x}})|.
\end{align} 
The sensitivity score $h_{i}$ is the probability difference between the original image  $ \mathbf{x} $ and generated image $\mathbf{\hat{x}}$, at the most counterfactual point when changing attribute $a_{i}$.

We synthesize a number of images from $ \mathcal{G}_\phi$, then iteratively compute the sensitivity for each given attribute, and finally normalize all sensitivities to draw the histogram as shown in Fig.~\ref{fig:histograms}. The histogram indicates the sensitivity of the evaluated model $f_{\theta}$ on each of the user-defined attributes. Higher sensitivity of one attribute means that the model is more easily affected by that attribute.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.49\linewidth]{images/single_attr_dog.pdf}
    \includegraphics[width=0.49\linewidth]{images/single_attr_young.pdf}
    \vspace{-2mm}
    \caption{Effect of progressively generating counterfactual images on (left) cat/dog classifier (0-Cat / 1-Dog), and (right) perceived age classifier (0-Senior / 1-Young). Model probability prediction during the process is attached at the top right corner.}
    \label{fig:age_classifier_single}
    \vspace{-2mm}
\end{figure*}

\begin{figure*}[h]
    \centering
    \begin{subfigure}[b]{\linewidth}
    \includegraphics[width=0.245\linewidth]{images/histograms/dog.pdf}
    \includegraphics[width=0.245\linewidth]{images/histograms/age.pdf}
    \includegraphics[width=0.245\linewidth]{images/histograms/eyeglasses.pdf}
    \includegraphics[width=0.245\linewidth]{images/histograms/biglips.pdf}
    \caption{Model diagnosis histograms generated by \ourmodel on four facial attribute classifiers.}
    \label{fig:histograms_vanilla}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
    \includegraphics[width=0.245\linewidth]{images/histograms/eyeglasses_biased_smiling.pdf}
    \includegraphics[width=0.245\linewidth]{images/histograms/age_biased_beard.pdf}
    \includegraphics[width=0.245\linewidth]{images/histograms/mustache_biased_paleskin.pdf}
    \includegraphics[width=0.245\linewidth]{images/histograms/mustache_biased_smiling.pdf}
    \caption{Model diagnosis histograms generated by \ourmodel on four classifiers trained on manually-crafted imbalance data.}
    \label{fig:histograms_unbalanced}
    \end{subfigure}
    \vspace{-5mm}
    \caption{Model diagnosis histograms generated by \ourmodel. The vertical axis values reflect the attribute sensitivities calculated by averaging the model probability change over all sampled images. The horizontal axis is the attribute space input by user.}
    \label{fig:histograms}
    \vspace{-3mm}
\end{figure*}

\subsection{Counterfactual Training}
\label{sec:ct}

The multi-attribute counterfactual approach visualizes semantic combinations that cause the model to falter, providing valuable insights for enhancing the model's robustness.
We naturally adopt the concept of iterative adversarial training \cite{madry2018towards} to robustify the target model. For each iteration, \ourmodel receives the target model parameter and returns a batch of mutated counterfactual images with the model's original predictions as labels. Then the target model will be trained on the counterfactually-augmented images to achieve the robust goal:
\begin{equation}
\small
\label{eq:counterfactual_training}
    \theta^{*} = \argmin_{\theta} \mathbb{E}_{\mathbf{x}\sim\mathcal{P}(\mathbf{x}), \mathbf{\hat{x}} = \text{\ourmodel}(\mathbf{x}, \mathcal{A}) } {L}_{CE}(f_{\theta}(\mathbf{\hat{x}}), f_{\theta}(\mathbf{x}))
\end{equation}
where batches of $\mathbf{x}$ are randomly sampled from the StyleGAN generator $ \mathcal{G}_\phi$. 
In the following, we abbreviate the process as Counterfactual Training (CT). Note that, although not explicitly expressed in Eq.~\ref{eq:counterfactual_training}, the CT process is a min-max game. \ourmodel synthesizes counterfactuals to maximize the variation of model prediction (while persevering the perceived ground truth), and the target model is learned with the counterfactual images to minimize the variation. 

