\section{Experimental Results}
\label{sec:experimental_results}

This section describes the experimental validations on the effectiveness and reliability of \ourmodel. First, we describe the model setup in Sec.~\ref{sec:experiment_setups}. Sec.~\ref{sec:single_attr_diagnosis} and Sec.~\ref{sec:validation_diagnosis} visualize and validate the model diagnosis results for the single-attribute setting. In Sec.~\ref{sec:multiple_attr_diagnosis}, we show results on synthesized multiple-attribute counterfactual images and apply them to counterfactual training.

\subsection{Model Setup}
\label{sec:experiment_setups}
{\bf Pre-trained models:} We used Stylegan2-ADA \cite{Karras2020ada} pretrained on FFHQ \cite{2019stylegan} and AFHQ \cite{choi2020starganv2} as our base generative networks, and the pre-trained CLIP model \cite{CLIP}  which is parameterized by ViT-B/32. We followed StyleCLIP \cite{2021StyleCLIP} setups to compute the channel relevance matrices $\mathcal{M}$.

{\bf Target models:} Our classifier models are ResNet50 with single fully-connected head initialized by TorchVision\footnote{https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/}. In training the binary classifiers, we use the Adam optimizer with learning rate 0.001 and batch size 128. We train binary classifiers for \textit{Eyeglasses, Perceived Gender, Mustache, Perceived Age} attributes on CelebA and for \textit{cat/dog} classification on AFHQ. For the 98-keypoint detectors, we used the HR-Net architecture~\cite{WangSCJDZLMTWLX19} on WFLW~\cite{wayne2018lab}. %Unless explicitly mentioned, our approach samples 1000 images from StyleGAN for each diagnosis by histogram.

\subsection{Visual Model Diagnosis: Single-Attribute}
\label{sec:single_attr_diagnosis}
Understanding where deep learning model fails is
an essential step towards building trustworthy models. Our proposed work allows us to generate counterfactual images (Sec.~\ref{sec:Counterfactual_Synthesis}) and provide insights on sensitivities of the target model (Sec.~\ref{sec:Attribute_Sensitivity_Analysis}). This section visualizes the counterfactual images in which only one attribute is modified at a time. 

Fig. \ref{fig:age_classifier_single} shows the single-attribute counterfactual images. Interestingly (but not unexpectedly), 
we can see that reducing the hair length or joyfulness causes the age classifier more likely to label the face to an older person. Note that our approach is extendable to multiple domains, as we change the cat-like pupil to dog-like, the dog-cat classification tends towards the dog. 
Using the counterfactual images, we can conduct model diagnosis with the method mentioned in Sec.~\ref{sec:Attribute_Sensitivity_Analysis}, on which attributes the model is sensitive to. In the histogram generated in model diagnosis, a higher bar means the model is more sensitive toward the corresponding attribute.

Fig.~\ref{fig:histograms_vanilla} shows the model diagnosis histograms on regularly-trained classifiers. For instance, the cat/dog classifier histogram shows outstanding sensitivity to green eyes and vertical pupil.
The analysis is intuitive since these are cat-biased traits rarely observed in dog photos. Moreover, the histogram of eyeglasses classifier shows that the mutation on bushy eyebrows is more influential for flipping the model prediction. 
It potentially reveals the positional correlation between eyeglasses and bushy eyebrows. The advantage of single-attribute model diagnosis is that the score of each attribute in the histogram are independent from other attributes, enabling unambiguous understanding of exact semantics that make the model fail. Diagnosis results for additional target models can be found in Appendix B.

\subsection{Validation of Visual Model Diagnosis} 
\label{sec:validation_diagnosis}
Evaluating whether our zero-shot sensitivity histograms (Fig.~\ref{fig:histograms}) explain the true vulnerability is a difficult task, since we do not have access to a sufficiently large and balanced test set fully annotated in an open-vocabulary setting. To verify the performance, we create synthetically imbalanced cases where the model bias is known. We then compare our results with a supervised diagnosis setting~\cite{sia}. In addition, we will validate the decoupling of the attributes by CLIP. 

\vspace{-2mm}
\subsubsection{Creating imbalanced classifiers}
\label{sec:creating_imbalance_classifiers}
\vspace{-1mm}
In order to evaluate whether our sensitivity histogram is correct, we train classifiers that are highly imbalanced towards a known attribute and see whether \ourmodel can detect the sensitivity w.r.t the attribute. For instance, when training the perceived-age classifier (binarized as Young in CelebA), we created a dataset on which the trained classifier is strongly sensitive to Bangs (hair over forehead). The custom dataset is a CelebA training subset that consists of $20,200$ images. More specifically, there are $10,000$ images that have both young people that have bangs, represented as $(1,1)$, 
and $10,000$ images of people that are not young and have no bangs, represented as $(0,0)$. The remaining combinations of $(1,0)$ and $(0,1)$ have only 100 images.
With this imbalanced dataset, bangs is the attribute that dominantly correlates with whether the person is young, and hence the perceived-age classifier would be highly sensitive towards bangs.
% will learn that bangs is the most sensitive attribute to predict age. 
See Fig.~\ref{fig:histogram_attgan} (the right histograms) for an illustration of the sensitivity histogram computed by our method for the case of an age classifier with bangs (top) and lipstick (bottom) being imbalanced. 
\begin{figure}[t]
    \begin{subfigure}[b]{\linewidth}
        \label{fig:histogram_attgan_1}
         \centering
         \includegraphics[width=\linewidth]{images/histograms/attgan_histogram_1.pdf}\\
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
    \label{fig:histogram_attgan_2}
         \includegraphics[width=\linewidth]{images/histograms/attgan_histogram_2.pdf}
    \end{subfigure}
        \vspace{-6mm}
         \caption{ The sensitivity of the age classifier is evaluated with \ourmodel (right) and AttGAN (left), achieving comparable results. }
         \label{fig:histogram_attgan}
         \vspace{-1mm}
    %  \end{subfigure}
\end{figure}

 We trained multiple imbalanced classifiers with this methodology,  and visualize the model diagnosis histograms of these imbalanced classifiers in Fig.~\ref{fig:histograms_unbalanced}. We can observe that the \ourmodel histograms successfully detect the synthetically-made bias, which are shown as the highest bars in histograms. See the caption for more information. 

\begin{figure}[t]
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/matrix/confusion-matrix-Mustache.pdf}
        \caption{Mustache classifier}
        \label{fig:matrix_CLIP_Score_a}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/matrix/confusion-matrix-Young.pdf}
        \caption{Perceived age classifier}
        \label{fig:matrix_CLIP_Score_b}
    \end{subfigure}
    \vspace{-2mm}
    \caption{Confusion matrix of CLIP score variation (vertical axis) when perturbing attributes (horizontal axis). This shows that attributes in \ourmodel are highly decoupled. }
    \label{fig:matrix_CLIP_Score}
    \vspace{-3mm}
\end{figure}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{images/multi_attr_human.pdf}
    \caption{Multi-attribute counterfactual in faces. The model probability is documented in the upper right corner of each image.}
    \label{fig:human_classifier_multiattr}
    \vspace{-4mm}
\end{figure*}

\vspace{-2mm}
\subsubsection{Comparison with supervised diagnosis}
\vspace{-1mm}
We also validated our histogram by comparing it with the case in which we have access to a generative model that has been explicitly trained to disentangle attributes.  We follow the work on~\cite{sia} and used AttGAN~\cite{attGAN} trained on the CelebA training set over $15$ attributes\footnote{\textit{Bald, Bangs, Black\_Hair, Blond\_Hair, Brown\_Hair, Bushy\_Eyebrows, Eyeglasses, Male, Mouth\_Slightly\_Open, Mustache, No\_Beard, Pale\_Skin, Young, Smiling, Wearing\_Lipstick.}}.
After the training converged, we performed adversarial learning in the attribute space of AttGAN and create a sensitivity histogram using the same approach as Sec.~\ref{sec:Attribute_Sensitivity_Analysis}. Fig.~\ref{fig:histogram_attgan} shows the result of this method on the perceived-age classifier which is made biased towards bangs.  As anticipated, the AttGAN histogram (left) corroborates the histogram derived from our method (right). Interestingly, unlike \ourmodel, AttGAN show less sensitivity to remaining attributes. This is likely because AttGAN has a latent space learned in a supervised manner and hence attributes are better disentangled than with StyleGAN. Note that AttGAN is trained with a fixed set of attributes; if a new attribute of interest is introduced, the dataset needs to be re-labeled and AttGAN retrained. ZOOM, however, merely calls for the addition of a new text prompt.  More results in Appendix B.

\vspace{-2mm}
\subsubsection{Measuring disentanglement of attributes}
\vspace{-1mm}
Previous works demonstrated that the StyleGAN's latent space can be entangled~\cite{interfacegan, EditinginStyle}, adding undesired dependencies when searching single-attribute counterfactuals. This section verifies that our framework can disentangle the attributes and mostly edit the desirable attributes.

We use CLIP as a super annotator to measure attribute changes during single-attribute modifications. For $1,000$ images, we record the attribute change after performing adversarial learning in each attribute, and average the attribute score change. Fig.~\ref{fig:matrix_CLIP_Score} shows the confusion matrix during single-attribute counterfactual synthesis. The horizontal axis is the attribute being edited during the optimization, and the vertical axis represents the CLIP prediction changed by the process. For instance, the first column of Fig.~\ref{fig:matrix_CLIP_Score_a} is generated when we optimize over bangs for the mustache classifier. We record the CLIP prediction variation. It clearly shows that bangs is the dominant attribute changing during the optimization. From the main diagonal of matrices, it is evident that the \ourmodel mostly perturbs the attribute of interest. The results indicate reasonable disentanglement among attributes.



\subsection{Visual Model Diagnosis: Multi-Attributes}
\label{sec:multiple_attr_diagnosis}
In the previous sections, we have visualized and validated single-attribute model diagnosis histograms and counterfactual images. 
In this section, we will assess \ourmodel's ability to produce counterfactual images by concurrently exploring multiple attributes within $\mathcal{A}$, the domain of user-defined attributes.  The approach conducts multi-attribute counterfactual searches across various edit directions, identifying distinct semantic combinations that result in the target model's failure. By doing so, we can effectively create more powerful counterfactuals images (see Fig.~\ref{fig:multiple_attribute_is_more_powerful}).


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/multi_attr_dog_cut.pdf}
    \caption{Multi-attribute counterfactual on Cat/Dog classifier. The number in each image is the predicted probability of being a dog.}
    \label{fig:dog_classifier_multiattr}
    \vspace{-2mm}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/multi_attr_is_more_powerful/multi_attr_is_more_powerful_2.pdf}\\
    \vspace{-1mm}
    \includegraphics[width=\linewidth]{images/multi_attr_is_more_powerful/multi_attr_is_more_powerful_1.pdf}
    \vspace{-8mm}
    \caption{ Multiple-Attribute Counterfactual (MAC, Sec.~\ref{sec:multiple_attr_diagnosis}) compared with Single-Attribute Counterfactual (SAC, Sec.~\ref{sec:single_attr_diagnosis}). We can see that optimization along multiple directions enable the generation of more powerful counterfactuals.}
    \label{fig:multiple_attribute_is_more_powerful}
    \vspace{-4mm}
\end{figure}

Fig.~\ref{fig:human_classifier_multiattr} and Fig.~\ref{fig:dog_classifier_multiattr} show examples of multi-attribute counterfactual
images generated by \ourmodel, against human and animal face classifiers. 
It can be observed that multiple face attributes such as lipsticks or hair color are edited in Fig.~\ref{fig:human_classifier_multiattr}, and various cat/dog attributes like nose pinkness, eye shape, and fur patterns are edited in Fig.~\ref{fig:dog_classifier_multiattr}. 
These attribute edits are blended to affect the target model prediction. Appendix B further illustrates \ourmodel counterfactual images for semantic segmentation, multi-class classification, and a church classifier. By mutating semantic representations, \ourmodel reveals semantic combinations as outliers where the target model underfits.


In the following sections, we 
will use the Flip Rate (the percentage of counterfactuals that flipped the model prediction) and Flip Resistance (the percentage of counterfactuals for which the model successfully withheld its prediction) to evaluate the multi-attribute setting. 
\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{\linewidth}
    \includegraphics[width=0.495\linewidth]{images/histograms/multi_attr_eyeglasses.pdf}
    \includegraphics[width=0.495\linewidth]{images/histograms/multi_attr_age_biased_beard.pdf}
    \caption{Sensitivity histograms generated by \ourmodel on attribute combinations.}
    \label{fig:histograms_combination}
    \end{subfigure}\\
    \begin{subfigure}[b]{\linewidth}
    \includegraphics[width=\linewidth]{images/histograms/grand_histogram.pdf}
    \caption{Model diagnosis by \ourmodel over $19$ attributes. Our framework is generalizable to analyze facial attributes of various domains.}
    \label{fig:histograms_grand}
    \end{subfigure}
    \vspace{-6mm}
    \caption{Customizing attribute space for \ourmodel.}
    \label{fig:multiple_attribute_histogram}
    \vspace{-4mm}
\end{figure}
\vspace{-3mm}
\subsubsection{Customizing attribute space}
\vspace{-2mm}
\looseness=-1

In some circumstances,  users may finish one round of model diagnosis and proceed to another round by adding new attributes, or trying a new attribute space.
The linear nature of attribute editing (Eq.~\ref{eq:total_edit}) in \ourmodel makes it possible to easily add or remove attributes. 
Table~\ref{tab:model_flip_rate} shows the flip rates results when adding new attributes into $\mathcal{A}$ for perceived age classifier and big lips classifier.  We can observe that a different attribute space will results in different effectiveness of counterfactual images. Also, increasing the search iteration will make counterfactual more effective (see last row). 
 Note that neither re-training the StyleGAN nor user-collection/labeling of data is required at any point in this procedure.  Moreover, Fig.~\ref{fig:histograms_combination} shows the model diagnosis histograms generated with combinations of two attributes. Fig.~\ref{fig:histograms_grand} demonstrates the capability of \ourmodel in a rich vocabulary setting where we can analyze attributes that are not labeled in existing datasets~\cite{liu2015celeba,MAAD}.
 
\vspace{-4mm}
\subsubsection{Counterfactual training results}
\label{sec:ct_result}
\vspace{-1mm}


This section evaluates regular classifiers trained on CelebA~\cite{liu2015celeba} and counterfactually-trained (CT) classifiers on a mix of CelebA data and counterfactual images as described in Sec.~\ref{sec:ct}. Table \ref{tab:ct_acc_table} presents accuracy and flip resistance (FR) results. CT outperforms the regular classifier. FR is assessed over 10,000 counterfactual images, with FR-25 and FR-100 denoting Flip Resistance after 25 and 100 optimization iterations, respectively. Both use $\eta=0.2$ and $\epsilon=30$. We can observe that the classifiers after CT are way less likely to be flipped by counterfactual images while maintaining a decent accuracy on the CalebA testset. Our approach robustifies the model by increasing the tolerance toward counterfactuals. Note that CT slightly improves the CelebA classifier when trained on a mixture of CelebA images (original images) and the counterfactual images generated with a generative model  trained in the FFHQ~\cite{2019stylegan} images (different domain).  


\begin{table}[t]
  \centering
  \footnotesize
  \begin{tabular}{@{}lccc@{}}
     \toprule
     Method & \makecell{AC Flip Rate (\%)} & \makecell{BC Flip Rate (\%)} \\
     \midrule
     Initialize \ourmodel by $\mathcal{A}$                        & 61.95 &  83.47\\
     + Attribute: Beard                                           &  72.08 & 90.07\\
     + Attribute: Smiling                                        &  87.47 &  \textbf{96.27}\\
     + Attribute: Lipstick                                         &  90.96 &  94.07\\
     + Iterations increased to 200                                &  \textbf{92.91} &  94.87\\
     \bottomrule
  \end{tabular}
  \caption{\label{tab:model_flip_rate} Model flip rate study. The initial attribute space $\mathcal{A} =$ \{Bangs, Blond Hair, Bushy Eyebrows, Pale Skin, Pointy Nose\}. AC is the perceived age classifier and BC is the big lips classifier.} 
  \vspace{-3mm}
\end{table}


\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{ccccc}
        \toprule
         Attribute & \makecell{Metric} & \makecell{Regular (\%)} & \makecell{CT (Ours) (\%)} \\

\midrule
        \multirow{3}{*}{Perceived Age} & CelebA Accuracy   & 86.10 & \textbf{86.29}   \\
        & \ourmodel FR-25  & 19.54 & \textbf{97.36}  \\
        & \ourmodel FR-100  & 9.04 & \textbf{95.65}  \\
        \midrule
        \multirow{3}{*}{Big Lips} & CelebA Accuracy   & 74.36 & \textbf{75.39}    \\
        & \ourmodel FR-25  & 14.12 & \textbf{99.19}  \\
        & \ourmodel FR-100  & 5.93 & \textbf{88.91}  \\
        \bottomrule
    \end{tabular}
    \caption{\label{tab:ct_acc_table} Results of network inference on CelebA original images and \ourmodel-generated counterfactual. The CT classifier is significantly less prone to be flipped by counterfactual images, while test accuracy on CelebA remains performant.}
    \vspace{-6mm}
\end{table}

\vspace{-2mm}

\section{Conclusion and Discussion} \label{conclusion_and_future}
\looseness=-1
\vspace{-2mm}

In this paper, we present \ourmodel, a zero-shot model diagnosis framework that generates sensitivity histograms based on 
user's input of natural language attributes. 
\ourmodel assesses failures and generates corresponding sensitivity histograms for each attribute.  A significant advantage
of our technique is its ability to analyze the failures of a target deep model without the need for laborious collection and annotation of test sets. \ourmodel effectively visualizes the correlation between attributes and model outputs, elucidating model behaviors and intrinsic biases.

Our work has three primary limitations. First, users should possess domain knowledge as their input (text of attributes of interest) should be relevant to the target domain.  Recall that it is a small price to pay for model evaluation without an annotated test set. Second, StyleGAN2-ADA struggles with generating out-of-domain samples. Nevertheless, our adversarial learning framework can be adapted to other generative models (e.g., stable diffusion), and the generator can be improved by training on more images. We have rigorously tested our generator with various user inputs, confirming its effectiveness for regular diagnosis requests. Currently, we are exploring stable diffusion models to generate a broader range of classes while maintaining the core concept. Finally, we rely on a pre-trained model like CLIP which we presume to be free of bias and capable of encompassing all relevant attributes.

{\bf Acknowledgements: }We would like to thank George Cazenavette, Tianyuan Zhang, Yinong Wang, Hanzhe Hu, Bharath Raj for suggestions in the presentation and experiments. We sincerely thank Ken Ziyu Liu, Jiashun Wang, Bowen Li, and Ce Zheng for revisions to improve this work.