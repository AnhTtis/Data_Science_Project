\vspace{-1mm}
\section{Related Work}
\label{sec:related_work}

This section reviews prior work on attribute editing with generative models and recent efforts on model diagnosis. 

\subsection{Attribute Editing with Generative Models} 
\vspace{-1mm}
With recent progress in generative models, GANs supports high-quality image synthesis, as well as semantic attributes editing \cite{Xia2022GANIA}. \cite{attGAN, choi2020starganv2} edit the images by perturbing the intermediate latent space encoded from the original images. These methods rely on images to be encoded to latent vectors to perform attribute editing. On the contrary, StyleGAN~\cite{2019stylegan} can produce images by sampling the latent space. Many works have explored ways to edit attributes in the latent space of StyleGAN, either by relying on image annotations~\cite{interfacegan} or in an unsupervised manner~\cite{sefa, ganspace}. StyleSpace~\cite{stylespace} further disentangles the latent space of StyleGAN and can perform specific attribute edits by disentangled style vectors. Based upon StyleSpace, StyleCLIP~\cite{2021StyleCLIP} builds the connection between the CLIP language space and StyleGAN latent space to enable arbitrary edits specified by the text. Our work adopts this concept for fine-grained attribute editing.


\subsection{Model Diagnosis}

To the best of our knowledge, model diagnosis without a test set is a relatively unexplored problem. In the adversarial learning literature, it is common to find methods that show how image-space perturbations \cite{goodfellow2014fgsm,madry2018towards} flip the model prediction; however, such perturbations lack visual interpretability. \cite{2018GANAE} pioneers in synthesizing adversaries by GANs. More recently, \cite{2019SemanticAE,CounterfactualGAN,2020semanticadv} propose generative methods to synthesize semantically perturbed images to visualize where the target model fails. However, their attribute editing is limited within the dataset's annotated labels. Instead, our framework allows users to easily customize their own attribute space, in which we visualize and quantify the biased factors that affect the model prediction. On the bias detection track, \cite{Lang_2021_ICCV} co-trains a model-specific StyleGAN with each target model, and requires human annotators to name attribute coordinates in the Stylespace. \cite{li-2021-discover,denton2019image,attractive} synthesize counterfactual images by either optimally traversing the latent space or learning an attribute hyperplane, after which the user will inspect the represented bias. Unlike previous work, we propose to diagnose a deep learning model without any model-specific re-training, new test sets, or manual annotations/inspections.
 