\section{Validation for CLIP-guided Editing}
\label{sec:appendix_sectiona}

Our methodology relies on CLIP-guided fine-grained image editing to provide adequate model diagnostics. It is critical to verify CLIP's ability to link language and visual representations. This section introduces two techniques for validating CLIP's capabilities.


\subsection{Visualization for edited images}
In this section, we analyze the decoupling of attribute editing used in StyleCLIP~\cite{2021StyleCLIP} in our domain. 


\textbf{Effect of $\lambda$.} Fig.~\ref{fig:appendix_lambda} shows the effect of $\lambda$ in Equation 2 of the main text~\cite{2021StyleCLIP} . 
Originally in StyleCLIP, this filter parameter (denoted as $\beta$ in~\cite{2021StyleCLIP}) helps the style disentanglement for editing. 
As we have normalized the edit vectors, which contributes to disentanglement in our framework, the impact of $\lambda$ on style disentanglement is reduced. Consequently, $\lambda$ primarily influences intensity control and denoising.


\textbf{Single-attribute editing.} Fig.~\ref{fig:appendix_attribute_editing_afhq} and Fig.~\ref{fig:appendix_attribute_editing_ffhq} show a set of images of different object categories by editing different attributes extracted with the global edit directions method (as described in Section 3.2 of the main text). By analyzing the user's input attribute string, we can see that the modified image only alters in the attribute direction while maintaining the other attributes. 

\textbf{Multiple-attribute editing.} 
We demonstrate the validity of our method for editing multiple attributes through linear combination (as outlined in Equation 3 of the main text) by presenting illustrations of combined edits in Figure ~\ref{fig:appendix_interpolation}.


\subsection{User study for edited images}
To validate that our counterfactual image synthesis preserve fine-grained details and authenticity, we conducted a user study validating two aspects: synthesis fidelity and attribute consistency. 

\textbf{User study for synthesis fidelity.} 
The classification of the counterfactual synthesis image vs real images by the user is employed to confirm that no unrealistic artifacts are introduced throughout the process of our model Fig.~\ref{fig:user_study_visual_fidelity} shows sample questions of this study.  In theory, the worst-case scenario is that users can accurately identify the semantic modification and achieve a user recognition rate of $100\%$. Conversely, the best-case scenario would be that users are unable to identify any counterfactual synthesis and make random guesses, resulting in a user recognition rate of $50\%$.


\textbf{User study for attribute consistency.} We ask users whether they agree that the counterfactual and original images are consistent on the ground truth w.r.t. the target classifier. For example, during the counterfactual synthesis for the cat/dog classifier, a counterfactual cat image should stay consistent as a cat. Fig.~\ref{fig:user_study_attribute_consistency} shows another sample questions. The worst case is that the counterfactual changes the ground truth label to affect the target model, which makes the user agreement rate very low (even to zero). 

The user study statistics are presented in Table ~\ref{tab:user_study}. The study involved $34$ participants with at least an undergraduate level of education, who were divided into two groups using separate collector links. The participants themselves randomly selected their group (i.e., the link clicked), and their responses were collected.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.49\linewidth]{images/single_attr_dog_2.pdf}
    \includegraphics[width=0.49\linewidth]{images/single_attr_Male.pdf}
    \vspace{-3mm}
    \caption{Effect of progressively generating counterfactual images on the Cat/Dog classifier (0-Cat / 1-Dog), and the Perceived Gender classifier (0-Female / 1-Male). Model probability prediction during the process is attached at the top right corner.}
    \label{fig:appendix_classifier_single}
\end{figure*}

The production of high-quality counterfactual images is supported by the difficulty users had in differentiating them. Additionally, the majority of users concurred that the counterfactual images do not change the ground truth concerning the target classifier, confirming that our methodology generates meaningful counterfactuals. However, it should be noted that due to the nature of our recognition system, human volunteers are somewhat more responsive to human faces. As a result, we observed a slightly higher recognition rate in the human face (FFHQ) domain than in the animal face (AFHQ) domain.

\subsection{Stability across CLIP phrasing/wording:} 

It is worth noting that the resulting counterfactual image is affected by the wording of the prompt used. In our framework, we subtract the neutral phrase (such as "a face") after encoding in CLIP space to ensure that the attribute edit direction is unambiguous enough. Our experimentation has shown that as long as the prompt accurately describes the object, comparable outcomes can be achieved. For instance, we obtained similar sensitivity results on the perceived-age classifier using prompts like "a picture of a person with X," "a portrait of a person with X," or other synonyms. Examples of this are presented in Figure ~\ref{fig:stability_histogram}.


\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.9\linewidth]{Rebuttal_Images/rebuttal_more_models.pdf}
   \caption{\ourmodel counterfactuals on more tasks (segmentation, multi-class classifier) and additional visual domains (cars, churches). Zoom in for better visibility.}
   \label{fig:more_models}
   \vspace{-3mm}
\end{figure*}

\begin{table}[h]
   \centering
   \small	
   \begin{tabular}{@{}lccc@{}}
     \toprule
    \multicolumn{1}{c}{Name of Study} & Domain & Group 1    & Group 2\\
     \midrule
    \multicolumn{1}{c}{\multirow{2}{*}{\makecell{Synthesis Fidelity (\\Recognition Rate $\downarrow$, \%)}}} & FFHQ &62.12 & 71.79 \\
                                                                & AFHQ & 51.30 & 50.55  \\
                        \midrule
     \multicolumn{1}{c}{\multirow{2}{*}{\makecell{Attribute Consistency (\\Agreement Rate $\uparrow$, \%)}}} & FFHQ &94.12 & 90.76  \\
                                                          & AFHQ & 89.92 & 88.26 \\


     \bottomrule
   \end{tabular}
   %\end{adjustbox}
   \caption{User study results. We can see from the table that our counterfactual synthesis preserves the visual quality and maintains the ground truth labels from the user's perspective.}
   \label{tab:user_study}
   \vspace{-3mm}
\end{table}