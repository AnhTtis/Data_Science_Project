% \jinqi{I think we have two points that need discussion across the whole paper: \\
% 1. we are using a bit much (e.g., ...), (i.e., ...). Maybe re-paraphrase them is a good choice.\\
% 2. we sometimes repeatedly saying the same thing in Figure, caption of Figure, and the description text. For example, Fig.~\ref{sec:single_attr_diagnosis} "multi-attr is powerful". The redundancy reduces the information density.\\
% 3. Sec 4.3 is about single-attribute validation, need minor change about description.\\
% 4. The Title of our work and the abbreviation needs thorough discussion.\\
% }

\begin{figure}[!t]
\centering
  \includegraphics[width=\linewidth]{images/teaser_figure_v5.pdf}
\caption{Given a differentiable deep learning model (e.g., a cat/dog classifier) and user-defined text attributes, how can we determine the model's sensitivity to specific attributes without using labeled test data? 
Our system generates counterfactual images (bottom right) based on the textual directions provided by the user, while also computing the sensitivity histogram (top right).
%Our system creates counterfactual images (see right bottom) in directions specified in texts by the user, as well as computing the sensitivity histogram (see right top).
}
\label{fig:first_fig}
\vspace{-4mm}
\end{figure}

\vspace{-.15in}

\section{Introduction}
\label{sec:intro}

Deep learning models inherit data biases, which can be accentuated or downplayed depending on the model's architecture and optimization strategy. 
Deploying a computer vision deep learning model requires extensive testing and evaluation, with a particular focus on features with potentially dire social consequences (e.g., non-uniform behavior across gender or ethnicity). Given the importance of the problem, it is common to collect and label large-scale datasets to evaluate the behavior of these models across attributes of interest. Unfortunately, collecting these test datasets is extremely time-consuming, error-prone, and expensive. 
Moreover, a balanced dataset, that is uniformly distributed across all attributes of interest, is also typically impractical to acquire due to its combinatorial nature. Even with careful metric analysis in this test set, no robustness nor fairness can be guaranteed since there can be a mismatch between the real and test distributions~\cite{Ramaswamy_2021_CVPR}. This research will explore model diagnosis without relying on a test set in an effort to {\em democratize} model diagnosis and lower the associated cost. 

Counterfactual explainability as a means of model diagnosis is drawing the community's attention \cite{Mothilal_2020, pmlr-v97-goyal19a}.
Counterfactual images visualize the sensitive factors of an input image that can influence the model's outputs. In other words, counterfactuals answer the question: \textit{``How can we modify the input image $\mathbf{x}$ (while fixing the ground truth) so that the model prediction would diverge from $\mathbf{y}$ to $\hat{\mathbf{y}}$?''}. The parameterization of such counterfactuals will provide insights into identifying key factors of where the model fails. Unlike existing image-space adversary techniques \cite{goodfellow2014fgsm, madry2018towards}, counterfactuals provide semantic perturbations that are interpretable by humans. However, existing counterfactual studies require the user to either collect uniform test sets~\cite{karkkainenfairface}, annotate discovered bias~\cite{li-2021-discover}, or train a model-specific explanation every time the user wants to diagnose a new model \cite{Lang_2021_ICCV}.
%How to design a plug-and-play model diagnosis toolkit that can democratize to greater community has become a challenging question.

On the other hand, recent advances in Contrastive Language-Image Pretraining (CLIP)~\cite{CLIP} can help to overcome the above challenges. CLIP enables text-driven applications that map user text representations to visual manifolds for downstream tasks such as avatar generation \cite{hong2022avatarclip}, motion generation \cite{Zhang2022MotionDiffuseTH} or neural rendering \cite{Poole2022DreamFusionTU,2022CLIPNeRF}. In the domain of image synthesis, StyleCLIP~\cite{2021StyleCLIP} reveals that text-conditioned optimization in the StyleGAN \cite{2019stylegan} latent space can decompose latent directions for image editing, allowing for the mutation of a specific attribute without disturbing others. 
With such capability, users can freely edit semantic attributes conditioned on text inputs. This paper further explores its use in the scope of model diagnosis. 
% Freedom of users to edit semantic attributes conditioned on text inputs inspires us to further explore its potential in the scope of model diagnosis. 

The central concept of the paper is depicted in Fig.~\ref{fig:first_fig}. Consider a user interested in evaluating which factors contribute to the lack of robustness in a cat/dog classifier (target model).  By selecting a list of keyword attributes, the user is able to (1) see counterfactual images where semantic variations flip the target model predictions (see the classifier score in the top-right corner of the counterfactual images) and (2) quantify the sensitivity of each attribute for the target model (see sensitivity histogram on the top).  Instead of using a test set, we propose using a StyleGAN generator as the picture engine for sampling counterfactual images. CLIP transforms user's text input, and enables model diagnosis in an open-vocabulary setting. This is a major advantage since there is no need for collecting and annotating images and minimal user expert knowledge. In addition, we are not tied to a particular annotation from datasets (e.g., specific attributes in CelebA~\cite{liu2015celeba}). 

To summarize, our proposed work offers three major improvements over earlier efforts:
\vspace{-1mm}
\begin{itemize}

 \item The user requires neither a labeled, balanced test dataset, and minimal expert knowledge in order to evaluate where a model fails (i.e., model diagnosis). In addition, the method provides a sensitivity histogram across the attributes of interest. 

 \item When a different target model or a new user-defined attribute space is introduced, it is not necessary to re-train our system, allowing for practical use. 

 \item The target model fine-tuned with counterfactual images not only slightly improves the classification performance, but also greatly increases the distributional robustness against counterfactual images. 

 \end{itemize}
 
\begin{figure*}[!t]
    \centering
    \vspace{-2mm}
    \includegraphics[width=0.95\linewidth]{images/overall_pipeline.pdf}
    \vspace{-1mm}
    \caption{The \ourmodel framework. Black solid lines stand for forward passes, red dashed lines stand for backpropagation, and purple dashed lines stand for inference after the optimization converges. The user inputs single or multiple attributes, and we map them into edit directions with the method in Sec.~\ref{sec:global_direction_extraction}. Then we assign to each edit direction (attribute) a weight, which represents how much we are adding/removing this attribute. We iteratively perform adversarial learning on the attribute space to maximize the counterfactual effectiveness.
    }
    \label{fig:model}
    \vspace{-2mm}
\end{figure*}