% mnras_template.tex 
%
% LaTeX template for creating an MNRAS paper
%
% v3.0 released 14 May 2015
% (version numbers match those of mnras.cls)
%
% Copyright (C) Royal Astronomical Society 2015
% Authors:
% Keith T. Smith (Royal Astronomical Society)

% Change log
%
% v3.0 May 2015
%    Renamed to match the new package name
%    Version number matches mnras.cls
%    A few minor tweaks to wording
% v1.0 September 2013
%    Beta testing only - never publicly released
%    First version: a simple (ish) template for creating an MNRAS paper

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Basic setup. Most papers should leave these options alone.
\documentclass[fleqn,usenatbib]{mnras}
\usepackage[section]{placeins}
% MNRAS is set in Times font. If you don't have this installed (most LaTeX
% installations will be fine) or prefer the old Computer Modern fonts, comment
% out the following line
\usepackage{newtxtext,newtxmath}
% Depending on your LaTeX fonts installation, you might get better results with one of these:
%\usepackage{mathptmx}
%\usepackage{txfonts}
\usepackage{adjustbox}
% Use vector fonts, so it zooms properly in on-screen viewing software
% Don't change these lines unless you know what you are doing
\usepackage[T1]{fontenc}
\usepackage{ae,aecompl}

% Allow "Thomas van Noord" and "Simon de Laguarde" and alike to be sorted by "N" and "L" etc. in the bibliography.
% Write the name in the bibliography as "\VAN{Noord}{Van}{van} Noord, Thomas"
%\DeclareRobustCommand{\VAN}[3]{#2}
%\let\VANthebibliography\thebibliography
%\def\thebibliography{\DeclareRobustCommand{\VAN}[3]{##3}\VANthebibliography}

%%%%% AUTHORS - PLACE YOUR OWN PACKAGES HERE %%%%%

% Only include extra packages if you really need them. Common packages are:
\usepackage{graphicx}	% Including figure files
\usepackage{amsmath}	% Advanced maths commands
%\usepackage{amssymb}	% Extra maths symbols
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{algorithm, algpseudocode}
\usepackage{lineno}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage[english]{babel}
\usepackage{mleftright}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{hyperref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%% AUTHORS - PLACE YOUR OWN COMMANDS HERE %%%%%

% Please keep new commands to a minimum, and use \newcommand not \def to avoid
% overwriting existing commands. Example:
%\newcommand{\pcm}{\,cm$^{-2}$}	% per cm-squared

\newcommand{\DifRel}{ \Delta^{\! \mathrm{r}} }
\newcommand{\DifAbs}{ \Delta^{\! \mathrm{a}} }
\newcommand{\SubMax}{ _{\mathrm{max} } }
\newcommand{\dd}{\mathrm{d}}
\newcommand{\VS}{\mathbb{V}}
\newcommand{\condarg}[2]{\mleft(#1\middle\vert#2\mright)}
\renewcommand{\vec}{\mathbf}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%% TITLE PAGE %%%%%%%%%%%%%%%%%%%

% Title of the paper, and the short title which is used in the headers.
% Keep the title short and informative.
\title[APES: Approximate Posterior Ensemble Sampler]{APES: Approximate Posterior Ensemble Sampler}

% The list of authors, and the short list which is used in the headers.
% If you need two or more lines of authors, add an extra line using \newauthor
\author[Vitenti, S.D.P., and Barroso E.J.]{
Sandro D.~P.~Vitenti$^{1}$\thanks{E-mail: vitenti@uel.br} and
    Eduardo J.~Barroso$^{1}$\thanks{E-mail: eduardo.jsbarroso@uel.br}
\\
% List of institutions
$^{1}$Physics Department, Universidade Estadual de Londrina Campus Universit√°rio, CEP 86057-970, Londrina, Brasil \\
}

% These dates will be filled out by the publisher
\date{Accepted XXX. Received YYY; in original form ZZZ}

% Enter the current year, for the copyright statements etc.
\pubyear{2021}

% Don't change these lines
\begin{document}
\label{firstpage}
\pagerange{\pageref{firstpage}--\pageref{lastpage}}
\maketitle

% Abstract of the paper
\begin{abstract}
This paper proposes a novel approach to generate samples from target distributions that are difficult to sample from using Markov Chain Monte Carlo (MCMC) methods. Traditional MCMC algorithms often face slow convergence due to the difficulty in finding proposals that suit the problem at hand. To address this issue, the paper introduces the Approximate Posterior Ensemble Sampler (APES) algorithm, which employs kernel density estimation and radial basis interpolation to create an adaptive proposal, leading to fast convergence of the chains. The APES algorithm's scalability to higher dimensions makes it a practical solution for complex problems. The proposed method generates an approximate posterior probability that closely approximates the desired distribution and is easy to sample from, resulting in smaller autocorrelation times and a higher probability of acceptance by the chain. In this work, we compare the performance of the APES algorithm with the affine invariance ensemble sampler with the stretch move in various contexts, demonstrating the efficiency of the proposed method. For instance, on the Rosenbrock function, the APES presented an autocorrelation time 140 times smaller than the affine invariance ensemble sampler. The comparison showcases the effectiveness of the APES algorithm in generating samples from challenging distributions. This paper presents a practical solution to generating samples from complex distributions while addressing the challenge of finding suitable proposals. With new cosmological surveys set to deal with many new systematics, which will require many new nuisance parameters in the models, this method offers a practical solution for the upcoming era of cosmological analyses.
\end{abstract}

% Select between one and six entries from the list of approved keywords.
% Don't make up new ones.
\begin{keywords}
methods: data analysis -- methods: statistical --  cosmological parameters -- software: data analysis -- cosmology: miscellaneous -- cosmology: observations.
\end{keywords}


\section{Introduction}

Markov Chain Monte Carlo (MCMC) methods have become a prominent tool in Bayesian inference since their inception by Metropolis in 1953~\cite{Metropolis1953}. MCMC has proven to be a versatile and effective method for generating samples from complicated and high-dimensional probability distributions, making it popular among scientists across various disciplines. In particular, MCMC methods have found widespread application in cosmology and astrophysics for estimating model parameters from observational data, as evidenced by several studies.

Obtaining samples from the posterior distribution, which is computed using Bayes' theorem with prior distributions and the likelihood function of the data, can be computationally demanding, especially for high-dimensional problems~\cite{Trotta2008}. However, MCMC methods offer a viable solution to this issue by enabling the generation of samples from the posterior distribution, which can then be used to infer properties of the underlying physical process. The effectiveness of MCMC in this context has been instrumental in advancing our understanding of complex physical systems.

Indeed, MCMC methods have played a significant role in addressing computational challenges in Bayesian inference, providing a powerful tool for generating samples from complex probability distributions. The ability to sample from the posterior distribution has enabled the exploration of parameter space and the estimation of uncertainties, making MCMC an essential tool in many scientific disciplines.

MCMC methods have found numerous applications in cosmology, beginning with their early use in analyzing cosmic microwave data using Metropolis-Hastings algorithms~\cite{Christensen2001, Christensen2001a, Hastings1970}. In recent years, MCMC methods have been the subject of intense research in cosmology, with numerous studies exploring their use in various contexts. For instance, the 21cmMC algorithm has been developed to study the 21 cm signal~\cite{Greig2015}, and the cosmic UV background has been studied using MCMC methods~\cite{Caruso2019}. Furthermore, various works have focused on optimizing sampling techniques for cosmological parameter estimation~\cite{Dunkley2005}.

While MCMC methods are a powerful tool for generating samples from posterior distributions, other efficient techniques exist, such as Nested Sampling (NS)~\cite{Skilling2006}. NS computes the integral of the posterior and produces a weighted sample of the posterior by generating a sample of the prior distribution. However, computing the marginal likelihood of the data required by NS can be computationally demanding, making the technique less efficient than MCMC methods in some cases. On the other hand, NS can be advantageous in performing Bayesian model comparison based on the Bayes factor~\cite{Congdon2006}. It should be noted that NS can be less efficient when there is a significant difference between the prior and posterior distributions of the parameters. Furthermore, the technique can require a significant amount of computational resources, especially for high-dimensional problems. For a recent review of NS and its applications, we refer the reader to Ref.~\cite{Alsing2021}. 

The development of ensemble samplers has led to the emergence of new algorithms that offer superior performance compared to traditional single-particle methods. For instance, Goodman's multiple-particle method~\cite{Goodman2010} is a notable improvement over such methods. One widely used ensemble sampler is \textit{Emcee}, a Python implementation that employs parallel stretch moves and other techniques, making it a popular choice for Bayesian analysis in cosmology~\cite{Foreman-Mackey2013}. Similarly, the \textit{CosmoHammer} package provides an optimized implementation of MCMC ensemble methods that is tailored to the requirements of the cosmology community for parameter estimation~\cite{Akeret2013}.

The ensemble sampler has been found to face significant challenges when applied to high-dimensional problems. For such, algorithms that exhibit superior performance for low-to-moderate dimensional problems often behave as a random walk sampler, leading to large self-correlation between points and high autocorrelation of the posterior sample. These issues compromise the efficiency of the algorithm and are commonly referred to as the curse of dimensionality~\cite{Jeffrey2020}. Further discussions about this problem can be found in~\cite{Morzfeld2019}.

There have been several proposals to address the curse of dimensionality in high-dimensional problems, such as the ensemble slice sampling~\cite{Karamanis2020} and the dynamic temperature selection~\cite{Vousden2016}. In this paper, we present a new method called the Approximate Posterior Ensemble Sampler (APES), which is a Metropolis-Hastings ensemble sampler that uses the APES move to generate high-dimensional samples with low autocorrelation and fast convergence. This proposal employs kernel density estimation and radial basis function interpolation to construct an approximation of the posterior, which is then used to propose points for the Metropolis-Hastings ensemble step. The result is an asymptotically high acceptance ratio and an effective solution to high-dimensional problems.

The algorithms presented in this paper are implemented in the Numerical Cosmology library (NumCosmo)~\cite{Vitenti2012c}, which fully integrates the sampling algorithms with cosmological and astrophysical models.\footnote{The project repository can be found here: \href{https://github.com/NumCosmo/NumCosmo}{NumCosmo github}} The library is written in C and is developed in an object-oriented fashion using the GObject framework. It has automatic bindings for all languages that support GObject introspection, including Perl and Python and many others. Specifically, we utilize the Python interface to provide notebooks and examples demonstrating various applications of the algorithms.

In Sec.~\ref{ensemblesampler}, we provide a detailed description of the ensemble sampler, including its key features and the stretch move that it employs. We use this as a basis for comparing our proposed sampler, which we introduce in Sec.~\ref{APES} and the Stretch move~\cite{Foreman-Mackey2013}. In this section, we provide a comprehensive explanation of the APES move, along with a thorough discussion of the techniques we use to implement it, including kernel density estimation and radial basis interpolation. To evaluate the effectiveness of our proposal, we present results in Sec.~\ref{comptests} for a range of test distributions, including the Rosenbrock distribution, a Gaussian Mixture distribution, the Funnel distribution, and a cosmology likelihood. We conclude with a discussion of the results and our main findings in Sec.~\ref{conclusion}.

\section{Ensemble Sampler}
\label{ensemblesampler}

The main objective of the ensemble sampler is to obtain a sample from a probability distribution $\pi(x)$ defined on a $n$-dimensional parametric space $\VS \subset \mathbb{R}^n$. In our setting, this distribution is generally a posterior distribution computed from the likelihood of a dataset and a set of priors. Nevertheless, as usual, our treatment here applies to any probability distribution. One limitation of MCMC algorithms is that the transition kernel is conditional to the current position and cannot use more information about the sample to generate the next proposal point. In the literature, there are many different ways to circumvent this limitation. For example, a popular approach is the use of many parallel chains combined with an update method to improve the transition kernel, see~\cite{Lewis2002, Dunkley2005, Audren2013, Lewis2013}. More precisely, given the probability distribution $\pi(x)$ for $x \in \VS$ we define a Metropolis-Hastings transition kernel as
\begin{equation}
	\label{KMH}
	\begin{split}
	T\condarg{y}{x} &= \alpha(x, y)q\condarg{y}{x}\\
	&+ \delta^n(x-y)\int_\VS \left[1-\alpha(x, z)\right]q\condarg{z}{x}\dd^nz,
	\end{split}
\end{equation}
where $y \in \VS$, $q\condarg{y}{x}$ is an arbitrary conditional probability distribution, which we call proposal distribution on $\VS$, $\dd^nz$ a measure on $\VS$, and the acceptance probability $\alpha(x, y)$ is defined by
\begin{equation}\label{accept}
\alpha(x, y) \equiv \mathrm{MIN}\left[1, \frac{q\condarg{x}{y}\pi(y)}{q\condarg{y}{x}\pi(x)}\right].
\end{equation}
It is easy to check that the above transition kernel satisfies the detailed balance condition, i.e., $T\condarg{y}{x}\pi(x) = T\condarg{x}{y}\pi(y)$. Consequently, the distribution $\pi(x)$ is left invariant by this kernel, that is,
\begin{equation}\label{invariantpi}
\int_\VS T\condarg{y}{x}\pi(x)\dd^nx = \pi(y).
\end{equation}
Furthermore, this is true for any intermediate step given by $q\condarg{y}{x}$. For this reason, one can provide any distribution $q\condarg{y}{x}$ (usually one that is easy to sample from) and build an algorithm that requires one evaluation of the distribution $\pi(x)$ per step. Of course, there are many caveats. A proposal distribution not well adapted to $\pi(x)$ (that is, frequently proposing points where $\pi(y)$ is very small) results in a tiny acceptance probability and consequently in repetitions on the chain and large auto-correlation. On the other hand, if $q\condarg{y}{x}$ frequently proposes points $y$ close to $x$, in the sense that $\pi(x) \approx \pi(y)$, they will have a high acceptance probability but this also results in chains with large auto-correlation.

The parallel chain approach starts with a proposal distribution $q\condarg{y}{x}$. After some steps, one updates the proposal using the computed chains and restarts the algorithm. This method has some drawbacks. First, it is necessary to wait for the computation of some steps before updating the proposal. Second, one discards the computed points every time $q\condarg{y}{x}$ is updated. In this work, we use the ensemble sampler, which solves these problems by changing the probability space $\VS$ and $\pi(x)$, as we explain below.

Instead of dealing with one $n$-dimensional point in the parametric space, the ensemble sampler considers a set of $L$ independent and identically distributed (iid) realizations of $\pi(x)$
\begin{align}
\label{ensemblepoint}
\vec{x}&\equiv\left(x_1,x_2,\dots,x_L\right),
\end{align}
where $x_i \in \VS$ for $i = 1,2,\dots,L$. We refer to each component $X_i$ as a walker and an ensemble point as an $L \times n$ dimensional point $\vec{x} \in \mathbb{V}^{L}$. The target joint distribution for the iid realizations of $\pi(x)$ is
\begin{align}
\label{jointdist}
\Pi(\vec{x})=\prod_{i=1}^{L}\pi(x_i).
\end{align}
As with other MCMC algorithms, the goal is to generate a sample from $\Pi(\vec{x})$. Since all points $x_i$ in $\vec{X}$ are iid, a sample of $N$ vectors $\vec{x}^j$, for $j=1,2,\dots,N$, produces a set of $N \times L$ realizations of $\pi(x)$.

The ensemble sampler works as an MCMC algorithm, and thus the ensemble point $\vec{x}$ must be iterated until the chain reaches convergence. We label the steps (or time) using a superscript ${}^j$ starting at the $j = 0$, such that every time all components of $\vec{x}^j$ are updated we increase the time by one, which we call an ensemble sampler iteration. The advantage of this scheme is that the proposal distribution is conditioned on the previous ensemble point, that is, $Q\condarg{\vec{y}}{\vec{x}}$. For this reason, if $\vec{x}$ provides an approximated sample of $\pi(x)$ then it can be used to build a good proposal for the next step. Note that, in the conventional MCMC, the proposal $q\condarg{y}{x}$ can use a single point (which is usually taken as the location of $q\condarg{y}{x}$ to propose a new point), now in an ensemble sampler the proposal can use a sample with $L$ elements to build the proposal distribution that is well adapted to $\pi(x)$ and consequently $\Pi(\vec{x})$. In this scenario, we avoid the two problems stated above since we can use a set of points to build the proposal without needing to restart the chains or discard any point.

In our work, instead of building a proposal distribution $Q\condarg{\vec{y}}{\vec{x}}$ for the whole ensemble point $\vec{x}$, we use a strategy called \textit{partial resampling}, see for example~\cite{Liu2000, Liu2001}. In practice, this technique is similar to a Gibbs sampler. First, we define a partition of $\vec{x} = \left(x_i, \vec{x}_{[i]}\right)$ where $x_i \in \VS$ is the $i$-th component of $\vec{x}$ and $\vec{x}_{[i]}$ is an element of $\VS^{L-1}$ obtained by removing the $i$-th component of $\vec{x}$. Any transition kernel $T\condarg{y_i}{x_i,\vec{x}_{[i]}}$ that leaves invariant the conditional
\begin{equation}\label{Picond}
\Pi\condarg{x_i}{\vec{x}_{[i]}} = \frac{\Pi\mleft(\vec{x}\mright)}{\int_\VS\Pi\left(\vec{x}\right)\dd^n x_i},
\end{equation}
also leaves $\Pi\mleft(\vec{x}\mright)$ invariant. This is a well-known result that we reproduce here for completeness. Starting with the invariant condition on $\Pi\condarg{x_i}{\vec{x}_{[i]}}$, that is
\begin{equation}
\int_\VS T\condarg{y_i}{x_i,\vec{x}_{[i]}}\Pi\condarg{x_i}{\vec{x}_{[i]}}\dd^nx_i = \Pi\condarg{y_i}{\vec{x}_{[i]}},
\end{equation}
and substituting Eq.~\eqref{Picond}, we obtain
\begin{equation}
\int_\VS T\condarg{y_i}{x_i,\vec{x}_{[i]}}\Pi\mleft(\vec{x}\mright)\dd^nx_i = \Pi\mleft(y_i,\vec{x}_{[i]}\mright).
\end{equation}

In our case, the joint distribution is given by Eq.~\eqref{jointdist}, consequently, the proposal must leave the original distribution invariant, that is $\Pi\condarg{x_i}{\vec{x}_{[i]}} = \pi(x_i)$. Now, the advantage is that the proposal can depend on $\vec{x}_{[i]}$, i.e., it can be expressed by $q\condarg{y_i}{x_i,\vec{x}_{[i]}}$. Thus, the Metropolis-Hastings transition kernel~\eqref{KMH} can be used. This results in a kernel that not only leaves the distribution invariant but also satisfies the stronger requirement of the detailed balance assuring that the resulting chain is reversible, see for example~\cite{Robert2013}. In the current context, it is written as
\begin{equation}
	\begin{split}
	T\condarg{y_i}{x_i,\vec{x}_{[i]}} &= \alpha_i(\vec{x}, y_i)q\condarg{y_i}{x_i,\vec{x}_{[i]}}\\
	&+ \delta^n(x_i-y_i)\int_\VS \left[1-\alpha_i(\vec{x}, z)\right]q\condarg{z}{\vec{x}}\dd^nz,
\end{split}
\end{equation}
and the acceptance probability $\alpha_i(\vec{x}, y_i)$ for the $i$-th walker is defined by
\begin{equation}
	\label{def:ap}
	\alpha_i(\vec{x}, y_i) \equiv \mathrm{MIN}\left[1, \frac{q\condarg{x_i}{y_i,\vec{x}_{[i]}}\pi(y_i)}{q\condarg{y_i}{\vec{x}}\pi\mleft(x_i\mright)}\right].
\end{equation}
Using this framework, we are allowed to use the remaining points $\vec{x}_{[i]}$ to build a proposal. As the chain converges, $\vec{x}_{[i]}$ will resemble a sample from $\pi(x)$, consequently by using the properties of $\vec{x}_{[i]}$ one can develop different proposals that are automatically updated by the properties of the sample and that keep the Markovian property of the chain.

This method has already been used in the literature. For example~\cite{Goodman2010, Foreman-Mackey2013} developed a set of affine invariant proposals using the complementary set $\vec{x}_{[i]}$. The affine invariance property~\cite{Goodman2010, Coullon2020} assures that the sampler can efficiently sample from distributions that are not well distributed in all directions. The stretch move, for example, proved itself to be efficient for generic target distributions, but there are some issues when it comes to high-dimensional problems. As discussed in Ref.~\cite{Huijser2015}, the presence of a dimension-dependent term in the acceptance probability may cause its increase/decrease too fast in high-dimensional problems. Moreover, in high dimensions, there are spaces with low probability but with large volumes, and thus most of the proposals end up in those areas. This diminishes the acceptance ratio and increases the autocorrelation (since only proposals close to the originating point will be accepted) and can slow down the convergence.

In this work, we introduce a new proposal for the ensemble sampler algorithm that uses kernel density estimation and radial basis interpolation to generate the proposal points for a Metropolis-Hastings algorithm. These methods are well suited for high-dimensional problems and thus can provide a robust algorithm for these settings.

\section{The APES Proposal}
\label{APES}

In the ensemble sampler, the proposal distribution $q\condarg{y_i}{x_i,\vec{x}_{[i]}}$ may depend on the complementary set $\vec{x}_{[i]}$. The objective is to create an approximate distribution, $\tilde{\pi}\condarg{x}{\vec{x}_{[i]}}$, of the target $\pi(x)$ using the sample $\vec{x}_{[i]}$. However, in the \emph{partial resampling} scheme, it is necessary to update all components of $\vec{x}$ serially. The reason behind this is that once $x_i$ is updated to $x_i'$, the new ensemble point is given by $\vec{x}' = (x_1, x_2, \dots, x_i', \dots, x_L)$, and in order to update $x_{i+1}$, an approximation must be computed using $\vec{x}_{[i]}'$. This is problematic for two reasons: first, the need to recompute the approximation $L$ times for a single ensemble step, and second, it is not possible to parallelize the algorithm as each new proposal depends on the previously computed point. To address these issues, we propose splitting the ensemble point into two parts, that is, $\vec{x} = (\vec{x}_{L_1}, \vec{x}_{L_2})$, where $\vec{x}_{L_1} = (x_1,x_2,\dots,x_{L/2})$ and $\vec{x}_{L_2} = (x_{L/2},x_{L/2+1},\dots,x_{L})$, and we assume $L$ is even for simplicity. We define two index sets as $L_1 = [1,2,\dots,L/2]$ and $L_2 = [L/2,L/2+1,\dots,L]$, and the function
\begin{align}
	s(i) = \left\{\begin{array}{ccc}
		L_1 & \text{if} & i \in L_2 \\
		L_2 & \text{if} & i \in L_1
	\end{array}\right.
\end{align}
By exploiting the fact that $\vec{x}_{L_2}$ is a subset of $\vec{x}_{[i]}$ for any $i \in L_1$, we can build an approximation using $\vec{x}_{L_2}$ and use it to update all elements of $\vec{x}_{L_1}$ in parallel. Similarly, we can use the same approach to update $\vec{x}_{L_2}$ based on the approximation built from $\vec{x}_{L_1}$. This eliminates the need to recompute the approximation multiple times, which enables efficient parallelization. We now require a method to construct the approximation using $\vec{x}_{L_i}$ for $i=1,2$.

Given a sample $\pmb{S}$ of $m$ points in $\mathbb{R}^n$, we apply the following strategy, first we use the natural vector space properties of $\mathbb{R}^n$ to define the Mahalanobis distance between $x\in\VS$ and a point $x_k \in \pmb{S}$ ($k=1,2,\dots,m$) as
\begin{equation}\label{MDist}
D^2_k(x, x_k) \equiv \mleft(x-x_k\mright)^{\intercal} \mathbf{C}_k^{-1}\mleft(x-x_k\mright), 
\end{equation}
where $\mathbf{C}_k$ is a positive definite symmetric $n \times n$ matrix. Then, the approximation is given by
\begin{equation}\label{defapprox}
\tilde{\pi}\condarg{x}{\pmb{S}} = \sum_{k =1}^m\frac{w_k}{h^n}K_k\mleft[\frac{D_k(x, x_k)}{h}\mright],\qquad \sum_{k =1}^m w_k = 1,
\end{equation}
where $h$ is the bandwidth parameter, $w_k\geq0$ the weights and $K_k$ a normalized multivariate distribution on $x$, that is,
\begin{equation}
K_k\mleft[D_k(x, x_k)\mright] \geq 0, \forall x\in\mathbb{R}^n, \quad \int_{\mathbb{R}^n}K_k\mleft[D_k(x, x_k)\mright]\dd^n x = 1.
\end{equation}
Generating a sample from $\tilde{\pi}\condarg{x}{\pmb{S}}$ is computationally straightforward. The process consists in selecting a kernel at random, based on the weights, and generating one random realization from that kernel. To use the approximation in Eq.~\eqref{defapprox}, the following must be defined: matrices $\mathbf{C}_k$ and distances $D^2_k(x, x_k)$ (Sec.\ref{sec:dist}), kernel function $K$ (Sec.~\ref{sec:K}), weights $w_k$ (Sec.~\ref{sec:interpopt}), and bandwidth $h$ (Sec.~\ref{sec:bandwidth}). The methods for determining these quantities are discussed in Sec.~\ref{sec:apespopt}.

Given the approximation method, we set the proposal distribution to be the approximation itself, i.e., $q\condarg{y_i}{x_i,\vec{x}_{[i]}} = \tilde{\pi}\condarg{x}{\vec{x}_{s(i)}}$. The acceptance probability, defined in Eq.~\eqref{def:ap}, is then given by:
\begin{equation}\label{defalpha}
	\alpha_i\mleft(\vec{x}_{s(i)}, y_i\mright) = \mathrm{MIN}\left[1, \frac{\tilde{\pi}\condarg{x_i}{\vec{x}_{s(i)}}\pi(y_i)}{\tilde{\pi}\condarg{y_i}{\vec{x}_{s(i)}}\pi\mleft(x_i\mright)}\right].
\end{equation}
If $\tilde{\pi}\condarg{x}{\vec{x}_{s(i)}}$ is a good approximation of $\pi(x)$, such that $\tilde{\pi}\condarg{x}{\vec{x}_{s(i)}} = \pi(x)\left[1 + \delta(x)\right]$ with $\vert\delta(x)\vert \ll 1$, then we have:
\begin{equation}
	\alpha_i\mleft(\vec{x}_{s(i)}, y_i\mright) = \mathrm{MIN}\left[1,\frac{1+\delta(x_i)}{1+\delta(y_i)}\right] \approx 1+\mathrm{MIN}\left[0,\delta(x_i)-\delta(y_i)\right].
\end{equation}
As a result, even for relatively large errors, of up to 20\%, the acceptance probability may remain larger than that of many other MCMC samplers, and it is close to one when the errors are small. The following section will provide a detailed explanation of the step proposal and the algorithm being proposed in this paper.

\subsection{The APES Move}
\label{apesmove}

As in any MCMC algorithm, we need a starting point $\vec{x}^0$. The main difference is that now $\vec{x}^0$ is a sample of points in $\VS^L$. Naturally, we cannot start with a sample from $\pi(x)$. Two options for obtaining $\vec{x}^0$ include sampling from the priors or computing the best-fit $x_\mathrm{bf}$ and Fisher matrix of $\pi(x)$ and sampling from the Gaussian approximation of $\pi(x)$. Both options are discussed in more detail in the next section and are available in our implementation.

The algorithm starts with the determination of an initial point $\vec{x}^0$. The approximation $\tilde{\pi}\condarg{x}{\vec{x}^0_{L_2}}$ is then calculated, from which $L/2$ samples are drawn, resulting in $\vec{x}^{\star1}_{L_1}$. The acceptance probability for each sample $x_k^{\star1}$ is determined using Eq.~\eqref{defalpha}, which is given by:
\begin{equation}\label{acceptprob}
	\alpha_k\mleft(\vec{x}_{L_2}^0, x_k^{\star1}\mright) = \mathrm{MIN}\left[1, \frac{\tilde{\pi}\condarg{x^0_k}{\vec{x}_{L_2}^0}\pi\mleft(x_k^{\star1}\mright)}{\tilde{\pi}\condarg{x_k^{\star1}}{\vec{x}_{L_2}^0}\pi\mleft(x_k^0\mright)}\right],
\end{equation}
forming the tuple $(t=1)$:
\begin{equation}
	\pmb{\alpha}^t_{L_1} = \left[\alpha_k\mleft(\vec{x}_{L_2}^{t-1}, x_k^{\star t}\mright)\right]_{k \in L_1}.
\end{equation}
If the sample $x_k^{\star1}$ is accepted, $x_k^{1} = x_k^{\star1}$, otherwise $x_k^{1} = x_k^{0}$.  The algorithm then proceeds to calculate the next set of points, $\vec{x}_{L_2}^1$. This is done by computing the approximation $\tilde{\pi}\condarg{x}{\vec{x}^1_{L_1}}$ and drawing $L/2$ samples, referred to as $\vec{x}^{\star1}_{L_2}$. These samples are then accepted with a probability given by:
\begin{equation}
	\alpha_k\mleft(\vec{x}_{L_1}^1, x_k^{\star1}\mright) = \mathrm{MIN}\left[1, \frac{\tilde{\pi}\condarg{x^0_k}{\vec{x}_{L_1}^1}\pi\mleft(x_k^{\star1}\mright)}{\tilde{\pi}\condarg{x_k^{\star1}}{\vec{x}_{L_1}^1}\pi\mleft(x_k^0\mright)}\right],
\end{equation}
which compose the second tuple $(t=1)$:
\begin{equation*}
	\pmb{\alpha}^t_{L_2} = \left[\alpha_k\mleft(\vec{x}_{L_1}^{t}, x_k^{\star t}\mright)\right]_{k \in L_2}.
\end{equation*}
The new point $x_k^1$ is then set to $x_k^{\star1}$ if the probability is accepted, otherwise it remains $x_k^0$. After these two steps the next sample point $\vec{x}^1$ is determined. Now, these steps are repeated until the desired time $t$ is reached. The full step is illustrated in Algorithm~\ref{alg:walker_apes}.
\begin{algorithm}[t]
	\caption{APES Algorithm}
	\label{alg:walker_apes}
	\begin{algorithmic}[1]
		\State Separate the $L$ walkers into two blocks $L_1$ and $L_2$
		\State Compute the initial sample with $L$ points $\vec{x}^0$ and set $t=0$
		\While{$t \leq t_\mathrm{max}$}
			\State Compute $\tilde{\pi}\condarg{x}{\vec{x}^t_{L_2}}$ and draw $\vec{x}^{\star t+1}_{L_1}$
	    	\For{$k \in L_1$}{ (parallelized loop)}
	    		\State Compute $\pi\mleft(x^{\star t+1}_{k}\mright)$ and $\alpha_k\mleft(\vec{x}_{L_2}^{t},x_k^{\star t+1}\mright)$
		    \EndFor
		    \State Update $\vec{x}_{L_1}^{t+1}$ using $\pmb{\alpha}^{t+1}_{L_1}$ computed above
			\State Compute $\tilde{\pi}\condarg{x}{\vec{x}^{t+1}_{L_1}}$ and draw $\vec{x}^{\star t+1}_{L_2}$
	    	\For{$k \in L_2$}{ (parallelized loop)}
				\State Compute $\pi\mleft(x^{\star t+1}_{k}\mright)$ and $\alpha_k\mleft(\vec{x}_{L_1}^{t+1},x_k^{\star t+1}\mright)$
			\EndFor
		\State Update $\vec{x}_{L_2}^{t+1}$ using $\pmb{\alpha}^{t+1}_{L_2}$ computed above
		\State Set $\vec{x}^{t+1} = \left(\vec{x}_{L_1}^{t+1},\;\vec{x}_{L_2}^{t+1}\right)$
		\State $t = t+1$
		\EndWhile
	\end{algorithmic}
\end{algorithm}

\subsection{APES Options}
\label{sec:apespopt}

The APES proposal's success relies on the accuracy of approximations made using two sub-samples, $L_i$, at each iteration. This accuracy is connected to the choice of distance function and its associated covariance matrix, $\mathbf{C}_k$, the kernel function $K_k$, as well as the bandwidth parameter, $h$, and the weights, $w_k$, for $k \in L_i$. The implementation offers two options for distances, a "same kernel approach" that uses a single distance function and a "variable kernel approach" that employs a different distance for each kernel. The weights can be calculated through either kernel density estimation or Radial Basis Function (RBF) interpolation. Finally, the bandwidth $h$ can be determined through either a Role of Thumb (RoT) or cross-validation. In the next subsections, we discuss these options in detail, providing a more thorough understanding of the implementation.

\subsubsection{Distance functions}
\label{sec:dist}

In the same kernel approach, a single covariance matrix, $\widehat{\mathbf{C}}\left(\vec{x}_{L_i}\right)$, is used for all distance computations, meaning $\mathbf{C}_k = \widehat{\mathbf{C}}\left(\vec{x}_{L_i}\right)$. This matrix can be computed using an unbiased estimator based on the sample $\vec{x}_{L_i}$, as shown in the equation below:
\begin{equation}
	\widehat{C}_\mathrm{b}\mleft(\vec{x}_{L_i}\mright) = \frac{1}{L/2-1}\sum_{k \in L_i} \left(x_{ki} - \bar{x}_k\right)\left(x - \bar{x}_k\right)^\intercal.
\end{equation}
In our study, we recognized the potential presence of outliers in the sample $\vec{x}_{L_i}$, particularly when the sample size is small or the chain has not yet converged. To mitigate this issue, we incorporated an Orthogonalized Gnanadesikan-Kettenring (OGK) algorithm~\cite{Maronna2002} into our implementation, which enables robust estimation of the covariance matrices $\widehat{C}_\mathrm{OGK}\mleft(\vec{x}_{L_i}\mright)$. To implement the OGK algorithm, we utilized the NumCosmo tools, which are built on top of the Lapack and BLAS library~\cite{Angerson1990, Lawson1979}. We estimated the underlying robust scale using the $Q_n$ robust scale estimators, which are implemented in the GSL library~\cite{GSLPC2010}. By incorporating the OGK algorithm into our implementation, we were able to better account for potential outliers in the sample and produce more robust covariance estimates for downstream analyses.

In the variable kernel approach, instead of using a single covariance matrix for all computations, a different covariance matrix $\mathbf{C}_k$ is determined for each $x_k$ with $k\in L_i$ using a defined number of the nearest points to $x_k$ from $\vec{x}_{L_i}$. This approach allows for greater adaptability, but it may not be the best choice for cases with a low number of walkers in the ensemble. The difficulty in finding the nearest points of a position in multi-dimensional problems arises from differing scales between dimensions. For instance, in a 2-dimensional scenario where $x = (\theta_1, \theta_2)$ with variances $\mathrm{Var}(\theta_1) = 1$ and $\mathrm{Var}(\theta_2) = 10^4$ and the points $x_1 = (-3, 30)$, $x_2 = (3, 30)$ and $x_3 = (-3, 40)$. Using an identity matrix to calculate the distance $D^2(x, x')$ shows that $D(x_1, x_2) < D(x_1, x_3)$, even though $x_1$ and $x_2$ differ by six standard deviations in $\theta_1$ while $x_1$ and $x_3$ are within one standard deviation in both dimensions. To address this, we compute $\widehat{\mathbf{C}}\left(\vec{x}_{L_i}\right)$ following the same process as in the single kernel approach and using one of the available options for estimating covariance. Then, we use this matrix to compute the distances between points in $\vec{x}_{L_i}$.

We define the number of nearest neighbors $m_k$ of $x_k$ as a percentage $p$ of $L/2$, rounded up to the nearest integer, i.e., $m_k = \lceil p L/2 \rceil$. We then use a fast k-Nearest Neighbor (kNN) search algorithm implementation~\cite{Ma2017} to find the $m_k$ closest points to $x_k$ defined as the subset $\vec{x}_{L_i}[\vec{x}_k, p] \subset \vec{x}_{L_i}$ using the distance calculated from the estimated global covariance matrix $\widehat{\mathbf{C}}\left(\vec{x}_{L_i}\right)$. Finally, we compute the covariance matrix $\mathbf{C}_k$ of each of these sub-samples $\vec{x}_{L_i}[\vec{x}_k, p]$ for use in the kernel $K_k$.  It is worth noting that different algorithms can be used to estimate the covariance matrix as discussed above.

The variable kernel approach offers versatility in various scenarios. It proves effective in modeling multi-modal distributions $\pi(x)$ by adapting to the variance at each point and providing a more accurate approximation. Additionally, it handles distributions where the correlations between variables in $\VS$ vary significantly from point to point. The only drawback is that it requires a larger number of walkers, which is proportional to the dimension of $\VS$.

\subsubsection{Kernel Function}
\label{sec:K}

In order to efficiently compute the approximation in Eq.~\eqref{defapprox} and facilitate easy sampling, a suitable kernel function $K_k$ must be selected. This study proposes two options: Gaussian (Gauss) and Student's t (ST) kernels. Both of these kernels are widely recognized and have readily accessible efficient implementations. Their mathematical expressions are respectively:
\begin{align}
	\label{kg}K^\mathrm{G}_k(d) &= \frac{\exp \left[-\frac{d^2}{2}\right]}{\sqrt{(2 \pi)^{n}\mleft|\mathbf{C}_k\mright|}}, \\
	\label{kt}K^\mathrm{ST}_k(d) &= \frac{\Gamma[(\nu+n) / 2]}{\Gamma(\nu / 2) \sqrt{\left(\nu\pi\right)^{ d}\left|\mathbf{C}_k\right|}}\left[1+\frac{d^2}{\nu}\right]^{-\frac{\nu+n}{2}},
\end{align}
where $\left|\mathbf{C}_k\right|$ is the determinant of $\mathbf{C}_k$.

The shape of the kernel as a function of $x$ is primarily controlled by the correlation structure in $\mathbf{C}_k$, which determines how the kernel varies in different directions of $\VS$. The choice of kernel function mainly determines the rate of decay for large distances $d \gg h$. The Gaussian kernel decays exponentially with $\exp{\left[-(d/h)^2/2\right]}$, while the ST kernel decays as $K_k(d/h) \propto (d/h)^{-(n+\nu)}$. With $\nu=1$, it is known as the Cauchy distribution and has the longest tail among all choices.

The approximation is constructed using a sample $\vec{x}_{L_i}$ of $\pi(x)$. As a result, most points in $\vec{x}_{L_i}$ will be from the high-probability region of $\VS$, leading to the underrepresentation of the tails of $\pi(x)$. If $\pi(x)$ decays quickly away from the most probable region, the Gaussian kernel is appropriate. However, if $\pi(x)$ has long tails or plateaus, the approximation $\tilde{\pi}\condarg{x}{\vec{x}_{L_2}}$ will have significant errors in those regions. To address this, a long-tailed kernel like the Cauchy kernel can be used. Additionally, Cauchy kernels have an advantage in that they lack well-defined mean, variance, and higher moments. This leads to proposal points that can be far from their location, acting as a probe to uncover disconnected regions of high probability in $\pi(x)$ for the MCMC algorithm.

\subsubsection{Weights Choice}
\label{sec:interpopt}

The kernel density estimation method, first introduced by~\cite{Rosenblatt1956, Parzen1962}, involves the use of kernel functions, which are probability distributions, to estimate the target probability distribution. According to~\cite{Silverman2018}, for a $n$-dimensional function $\pi(x)$, the multivariate density estimation is given by Eq.~\eqref{defapprox} with $w_k = 2 / L$. This method is straightforward and computationally efficient, but its accuracy depends on the representativeness of the sample points $\vec{x}_{L_i}^t$ at time $t$, as it does not use any information about $\pi(x)$ other than these points.

The second approach involves the use of RBI interpolation, which uses the approximation~\eqref{defapprox} to approximate the actual distribution function $\pi(x)$. During the MCMC algorithm, we must compute the posterior at $\vec{x}_{L_i}^t$ to determine the acceptance probability~\eqref{defalpha}. To take advantage of this, we can compute the weights $w_k$ by solving the following system:
\begin{align}
	\tilde{\pi}(x_i) = \pi(x_i), \quad \text{for} \quad i = 1, 2, ..., L/2.
\end{align}
This requires solving the linear system
\begin{align}
\label{linsys}	\mathbf{K}\pmb{w} &= \pmb{\pi}, \\ 
\label{linsyscmp}	\mathbf{K}_{ij} &= \frac{1}{h^n}K_j\mleft[\frac{D_j(x_i, x_j)}{h}\mright], & \pmb{w}_j &= w_j, & \pmb{\pi}_i = \pi(x_i),
\end{align}
where the second line defines the components of the matrix $\mathbf{K}$ and vectors $\pmb{w}$ and $\pmb{\pi}$.

The challenge of solving Eq.~\eqref{linsys} is to find a solution that satisfies the constraint $w_i \geq 0$ since the $w_i$ values represent probabilities in~\eqref{defapprox}. This is a Non-Negative Least Squares (NNLS) problem that we addressed using a C algorithm based on the algorithm described in~\cite{Bro1997} and the Lapack Library for linear algebra computations. Note that the weights in regions of low probability have larger errors in the solution of Eq.~\eqref{linsys} as they contribute less to the least squares. To improve the solution, a weighted least squares approach can be used, incorporating weights of $1/\pi(x_i)$. Additionally, interpolation may provide a more precise approximation in higher-probability areas compared to those with lower probability.

 \subsubsection{Bandwidth Selection}
 \label{sec:bandwidth}
 
The final step in calculating the approximation (given by equation~\eqref{defapprox}) involves determining the bandwidth $h$. To do this, we utilize the Rule-of-Thumb (RoT) for density estimation interpolation described in~\cite{Silverman2018}. For Gauss and ST kernels, the RoT is straightforward and simply given by:
\begin{align*}
h_\mathrm{RoT}^\mathrm{G} &= \left[\frac{4}{L/2(n+2)}\right]^{\frac{1}{n+4}}, \\
h_\mathrm{RoT}^\mathrm{ST} &= \left[\frac{16 (\nu - 2)^2 (1 + n + \nu) (3 + n + \nu)}{(2 + n) (n + \nu) (2 + n + \nu) (n + 2 \nu) (2 + n + 2 \nu) L/2}\right]^{\frac{1}{n + 4}}.
\end{align*}
This offers a preliminary estimate for the bandwidth. The final bandwidth for both the same and variable kernel approaches is calculated by setting $h = o h_{RoT}$ for the same kernel approach, and $h = o h_{RoT} / p$ for the variable kernel approach, where $p$ is the sample percentage as discussed in Sec.~\ref{sec:dist} and $o$ is the over-smoothing parameter. The value of $o$ can be freely chosen by the user. 

A cross-validation procedure is available with RBF interpolation. The process involves dividing the sample $\vec{x}_{L_i}$ into two subsets: one for the left-hand side of~\eqref{linsys}, containing only kernels with locations on the subset, and the full sample for the right-hand side. In other words, given a split-fraction parameter $f\in(0,1)$ we define the sub-sample $\vec{x}_{fL_i}$ containing the first $m = \lceil f\times L/2 \rceil$ points of $L_i$. The approximation is obtained setting $\pmb{S} = \vec{x}_{fL_i}$ in Eq.~\eqref{defapprox}, where the covariances are still computed using the full sample $\vec{x}_{L_i}$. Now, for each value of $o$, we computed the function 
\begin{equation}
C(o) = \left\|\mathbf{K}_f\pmb{w} - \pmb{\pi}_f\right\|^2,
\end{equation}
where $\|\|$ represents the L2 norm. The matrix $\mathbf{K}_f$ and vector $\pmb{\pi}_f$ are computed as in Eq.~\eqref{linsyscmp} but with $i=1,2,\dots m$ and $j=1,2,\dots,L/2$. Finally, $C(o)$ is minimized with respect to $o$. The cross-validation method can increase the computational cost of the MCMC algorithm, so it's often more efficient to use a fixed over-smoothing parameter $o$. After some iterations, the algorithm can be stopped to compute the cross-validation estimate for $o$, and then continue the chain using this updated value, rather than performing cross-validation at each step.

\section{Computational tests and Discussion}
\label{comptests}

In this section, we provide a detailed analysis of the quality of samples generated by the APES algorithm. To evaluate the algorithm's performance, we compare it to the stretch-move algorithm as implemented in Emcee~\cite{Goodman2010, Foreman-Mackey2013}, which we refer to as Stretch. All the necessary components for running the APES algorithm are available in the Numerical Cosmology (NumCosmo) library~\cite{Vitenti2012c}.

To assess the efficiency of the APES algorithm, we use two metrics: the autocorrelation time ($\tau$) and the acceptance rate of the chains. The autocorrelation time $\tau$ is a measure of the number of iterations required for the next sample point in the chain to become independent~\cite{Goodman2010}. We use the precise definition of $\tau$ for an ensemble sampler given in \cite{Goodman2010} to estimate its value for the APES algorithm. The acceptance rate of the chains reflects the percentage of proposed moves that are accepted by the algorithm. Finally, to simplify notation, we represent the mean of a quantity given a sample with an overline.

\begin{table}
	\centering
	\caption{NumCosmo approximation options. The Robust column relates to the option to use the robust covariance estimation based on the OGK algorithm.}
	\label{tab:appopt}
	\begin{tabular}{lccc}
		\toprule
		Approximation Options & Kernel & Robust & Interpolation \\ 
		\midrule
		KDE & Same & - & - \\
		VKDE & Variable & - & - \\
		Robust-KDE & Same & X & - \\
		Robust-VKDE & Variable & X & - \\
		Interp-KDE & Same & - & X \\
		Interp-VKDE & Variable & - & X \\
		Interp-Robust-KDE & Same & X & X \\
		Interp-Robust-VKDE & Variable & X & X \\
		\bottomrule
	\end{tabular}
\end{table}

NumCosmo provides additional diagnostics for analyzing chain convergence, which we also utilize. We automatically compute the autocorrelation time using NumCosmo and verify the results and diagnostics using GetDist~\cite{Lewis2019}. Sokal~\cite{Sokal1997} provides a detailed discussion of the autocorrelation time and its role in assessing the convergence of the chains and the efficiency of the algorithm.

In addition to examining the autocorrelation time and acceptance rate, we also compare the analytical mean and variance of the parameters for some distributions with those estimated from the chains. It is worth noting that a high acceptance rate does not necessarily imply a low autocorrelation and vice versa. A high autocorrelation and acceptance rate may indicate that the sampling algorithm is not well-adjusted and is proposing new points that are too correlated with the current position. Therefore, we carefully evaluate both parameters to ensure the reliability of the generated samples.

The NumCosmo package offers a variety of kernel functions for density estimation, including Gaussian (Gauss), Student's t with $\nu=3$ (ST3), and Student's t with $\nu=1$ (Cauchy). Each kernel has its own properties, which are discussed in Sec.~\ref{sec:K}.

Users can customize their density estimation approach by combining different kernel types with various interpolation and robustness options. For example, the "Interp-Robust-VKDE:Cauchy" approach combines variable kernel density estimation (VKDE) with robust covariances and radial basis function (RBF) interpolation using Cauchy kernels. This approach provides the benefits of robust statistics, the flexibility of interpolation, and the heavy-tailed properties of Cauchy kernels. However, it has an increased computational cost.

Alternatively, the ``KDE:Gauss'' approach uses kernel density estimation (KDE) with equal weights and Gaussian kernels, which is a simpler option that is still effective for many applications. Other options, such as ``VKDE:ST3'' that uses the t-distribution kernel with heavier tails than the Gaussian kernel, offer even more flexibility and robustness at a moderate computational cost.

A quick reference guide with labels for each option, along with the kernel, robustness, and interpolation options used, is provided in Tables~\ref{tab:appopt} and \ref{tab:kerntype}. All interpolation options can be combined with every kernel type, allowing users to fine-tune their density estimation approach to meet their specific needs.

\begin{table}
	\centering
	\caption{NumCosmo kernel types for density estimation.}
	\label{tab:kerntype}
	\begin{tabular}{lcc}
		\toprule
		Kernel Type & Density Function & Properties \\
		\midrule
		Gauss & Eq.~\eqref{kg} & Light-tailed \\
		ST3 & Eq.~\eqref{kt} $\nu=3$ & Heavier tails than Gaussian \\
		Cauchy & Eq.~\eqref{kt} $\nu=1$ & Heaviest tails \\
		\bottomrule
	\end{tabular}
\end{table}
\subsection{The 2-dimensional Rosenbrock Distribution}

The first test for the sampler was to generate samples from the 2-dimensional Rosenbrock distribution (see~\cite{Pagani2020} for a discussion on the use of n-dimensional Rosenbrock distributions to test MCMC algorithms). The distribution is given by
\begin{align}
\label{eqRB}
\pi(x_1,x_2) &\propto \exp\left[-100\left(x_2-x_1^2\right)^2+(1-x_1)^2\right], \; x_1,\,x_2 \in \mathbb{R}.
\end{align}
where we did not include the normalization. The Rosenbrock distribution is known to be hard to sample from due to its thin tails. Before testing the APES algorithm, we study the interpolation of this distribution using the packages implemented in NumCosmo. Thus, we first generate perfect samples from the Rosenbrock distribution and compute the approximation using different kernels and options.  

\begin{table}
	\caption{Relative difference, acceptance rate, and over-smooth for each interpolation applied to the same sample of 160 draws from the Rosenbrock distribution.}
	\label{fig:RBfit}
	\centering
	\begin{tabular}{|l|c|c|c|}
		\hline
		Interpolation & $|\tilde{\pi}/\pi-1|<0.2$ & $\overline\alpha$ & $o$ \\ \hline\hline
		Interp-KDE:Cauchy & 9\% & 64\% &  0.05 \\ \hline
		Interp-KDE:ST3 & 23\% & 65\% &  0.07 \\ \hline
		Interp-KDE:Gauss & 32\% & 66\% &  0.05 \\ \hline
		Interp-VKDE:Cauchy & 9\% & 69\% &  0.08 \\ \hline
		Interp-VKDE:ST3 & 33\% & 72\% &  0.13 \\ \hline
		Interp-VKDE:Gauss & 51\% & 76\% &  0.14 \\ \hline
	\end{tabular}
\end{table}

In Fig.~\ref{fig:RBapprox}, we present different approximations of~\eqref{eqRB}, all of which were generated using the same sample of 160 draws from~\eqref{eqRB}. The VKDE method is found to be superior to the KDE method in accurately describing the target distribution due to its ability to adapt the covariance of the kernels locally. As shown in the first line of the plot, where all ellipses representing the kernel covariances are equal and adapted to the full sample covariance, the position-dependent correlation between $x_1$ and $x_2$ is not accurately captured.

To test the approximation we generated a test sample $\pmb{T}$ from~\eqref{eqRB} with 100,000 points. Table~\ref{fig:RBfit} shows the percentage of points where the approximation had a relative error smaller than 20\%. Furthermore, we also computed the mean acceptance probability\eqref{acceptprob}. To obtain this probability, we compute 
\begin{equation}
	\alpha_k\mleft(\pmb{S}, x_{2k}\mright) = \mathrm{MIN}\left[1, \frac{\tilde{\pi}\condarg{x_{2k+1}}{\pmb{S}}\pi\mleft(x_{2k}\mright)}{\tilde{\pi}\condarg{x_{2k}}{\pmb{S}}\pi\mleft(x_{2k+1}\mright)}\right],\quad x_{2k} \in \pmb{T},
\end{equation}
obtaining 50,000 values of $\alpha$ and computing its mean $\overline{\alpha}$. The Gaussian kernels yield the smallest errors when fitting~\eqref{eqRB}. Moreover, the mean acceptance probability also follows the same pattern. However, it is important to note that the values in the table are based on a sample of~\eqref{eqRB}, which has higher density in regions of higher probability.  Therefore, we can conclude that Gaussian kernels are better suited to fit these regions of the distribution. 

To gain a better understanding of the errors and the performance of different kernels on the peaks and tails of the distribution, we repeat the same calculation but first we binned the test sample on the values of $\pi(x)$ for $x\in \pmb{T}$ to compute the relative errors and we binned on the target probability $\pi(x_{2k})$ to compute the mean $\alpha$. The results can be seen on Fig.~\ref{fig:RBerralpja}. This plot allows us to differentiate the approximation error on peaks and tails, which is crucial for selecting a proposal that can sample the tails effectively.

Our analysis reveals that although Gaussian kernels provide an overall better approximation, they are less accurate on the tails compared to Cauchy kernels. Moreover, the overall performance of the Cauchy kernels is not significantly worse than that of the Gaussian kernels. Therefore, we conclude that Cauchy kernels are better suited to explore the tails of the distribution.

\begin{figure*}
\caption{We generated plots of approximated distributions using 160 points distributed according to Eq.~\eqref{eqRB}. For each approximation, we also plotted the $4\sigma$ ellipse in red, which represents the covariance of each kernel. The first line shows plots of the approximated distribution using Interp-KDE with all supported kernels. In the second line, we used VKDE with $5\%$ ($p=0.05$, as explained in Sec.~\ref{sec:dist}) of the nearest sample points to compute the kernel covariance for each point. All samples were generated with an over-smooth factor $o$ obtained by applying split cross-validation with $60\%$ of the sample ($f=0.6$, as described in Sec.~\ref{sec:bandwidth}). Finally, the third line shows a plot of the original Rosenbrock function for comparison. We renormalized all plots so that the maximum probability is equal to one, and we displayed the probability decay until a threshold of $10^{-8}$.	The interpolation with heavier tails tends to overestimate the probability away from the peaks, which is a desirable feature to enable our MCMC algorithm to explore the entire parametric space. Note that the fixed kernel approach is unable to adapt to the local features of the distribution and therefore fails to describe the tails properly. This highlights the advantages of using the VKDE approach to capture the intricate features of the Rosenbrock distribution.}
\label{fig:RBapprox}
\centering
\includegraphics{rosenbrock_kernel_interp.pdf}
\end{figure*}

We executed both algorithms using $15625\times320 = 5,000,000$ points, where $15625$ represents the number of iterations and $320$ the number of walkers. In both cases, we discarded the initial 5000 iterations to allow the chains to reach a stationary distribution before collecting samples for analysis. For the APES algorithm, a smaller burn-in period may have been sufficient, but we removed the same number of initial iterations to ensure a fair comparison of the results between the two algorithms. This approach also allowed us to present a clear and consistent analysis of the performance of both algorithms. 

Let us analyze the results in Table~\ref{RBstats} and Figure~\ref{RBparamevol}. The Rosenbrock sample generated by the APES converged for the true variance value for the Rosenbrock distribution ($\mathrm{Var}\left(-2\ln \pi\right)=4$), while the Stretch sample seems to have not converged yet, as shown in Figure~\ref{RBparamevol}. The autocorrelation time of the APES sample was significantly smaller than the Stretch sample. While the APES provided a time around $\tau = 10$, the Stretch move had an autocorrelation of the order of $\tau = 1400$, and therefore the sample generated by the APES is more independent. One important issue that may be addressed is the acceptance ratio of the APES. Not only APES had a larger acceptance ratio of $47\%$ when compared to the Stretch's acceptance of $23\%$, but the points that are being accepted are more independent than the ones from the stretch sample. The combined effect ends up making APES more efficient than Stretch by a margin of $140$ less autocorrelated.

One important limitation of APES is its computational cost. Building a new approximation of the posterior at each step makes it much more costly than Stretch. In practice, the cost of the Stretch move is zero, while APES requires around 3 minutes to compute 5,000,000 points when using 320 walkers. Despite its cost, APES has shown to be more efficient than Stretch, producing a sample with lower autocorrelation and better exploring the tails of the distribution. While APES can be improved to decrease its computational cost, for now, we leave that as a topic for future work.

\begin{figure}
	\caption{The upper panel of the graph displays the percentage of test points for which the relative error was smaller than 20\% in bins of $\pi(x)$. It is important to note that all approximations experience a loss of precision as they approach the tails, with lighter-tailed kernels performing better near the peak but suffering from decreased precision in the tails. In contrast, heavier-tailed kernels demonstrate greater stability across the entire range, providing a better overall approximation. To explore the relationship between approximation precision and proposal effectiveness, we have included a plot of the mean acceptance probability in the figure below (see equation~\eqref{acceptprob}). The acceptance probability is shown as a function of the true distribution on the proposed point. Notably, heavier-tailed kernels have the highest acceptance probability in the tails, indicating that these kernels provide a more effective proposal and increase the likelihood of sampling in the tails.}
	\label{fig:RBerralpja}
	\centering
	\includegraphics{rosenbrock_kernel_err_acpr.pdf}
\end{figure}

\begin{table}
\caption{Results of an ensemble sampler run using APES and the Stretch move to generate a 2-dimensional Rosenbrock sample. The APES configuration is Interp-VKDE:Cauchy with an over-smooth parameter of 0.2 and a local fraction of walkers of 0.05. Both samples were generated using a burn-in of 5000 iterations and a total of 1.6 million points (5000 iterations multiplied by 320 chains). The mean variance and other mean results were computed using all chains. Analytical results were also included for comparison.}
\label{RBstats}
\centering
\begin{tabular}{ |lrrr|}
	\hline
	\multicolumn{3}{|c|}{2-dimensional Rosenbrock Run} \\
	\hline
        & APES & Stretch & Analytical~\eqref{eqRB}\\
	\hline
	Number of walkers   &  $320$   & $320$ & -- \\
	Number of points & $5\times 10^6$     & $5\times 10^6$  & -- \\
	 $\tau_{x_1}$ &$6.3$  &  $ 1437.8$ & -- \\
     $\tau_{x_2}$ &$10.7$ &  $ 1421.6$ & --\\
	$\bar{x}_1$  & $1.0$ & $ 0.9 $  & $1$  \\
	$\bar{x}_2$ &$11.0$ & $10.2$  & $11$ \\
    $-2\overline{\ln\pi}$ & $2.0$ & $2.0$  & $2$ \\
	$\mathrm{Var}\left(x_1\right)$  & $10.0 $ & $9.5$ & $10$  \\
	$\mathrm{Var}\left(x_2\right)$  & $236.7 $ & $173.3$ & $2401/10$  \\
	$\mathrm{Cor}\left(x_1, x_2\right)$  & $0.41 $ & $0.35$ & $20/49\approx 0.41$  \\
	$\mathrm{Var}\left(-2\ln \pi\right)$  & $4.0 $ & $3.6$ & $4$  \\
	Acceptance rate    & $47\%$&  $23\% $ & -- \\
	\hline
\end{tabular}
\end{table}

\begin{figure}
\caption{Plot showing the iterations $(x_1^t, x_2^t)$ of the MCMC samples generated from the Rosenbrock distribution using APES and Stretch proposals. The plot includes three chains (out of 320 possible) from each algorithm, where the red points correspond to the APES sample, and the blue points correspond to the Stretch sample. The iterations shown in the plot were obtained after discarding the burn-in period. The visual inspection reveals that the APES algorithm explores the tails more efficiently, with more points distributed away from the mean and exhibiting only small autocorrelation. On the other hand, the Stretch algorithm shows a stronger autocorrelation, with the generated points tending to stay closer to their previous positions. The statistical properties for these two runs are summarized in Table~\ref{RBstats}.}
\label{RBparamevol}
\includegraphics{rosenbrock_param_evol.pdf}
\centering
\end{figure}

You can find the notebook exploring the approximations of the Rosenbrock distribution,\footnote{\href{https://github.com/NumCosmo/NumCosmo/blob/master/notebooks/stats_dist_tests/stats_dist_rosenbrock.ipynb}{notebooks/stats\_dist\_tests/stats\_dist\_rosenbrock.ipynb}} and the notebook with the MCMC runs and their analysis at NumCosmo github project.\footnote{\href{https://github.com/NumCosmo/NumCosmo/blob/master/notebooks/apes_tests/rosenbrock_mcmc.ipynb}{notebooks/apes\_tests/rosenbrock\_mcmc.ipynb}}

\subsection{Gaussian Mixture}

In the previous section, we compared the performance of two algorithms on a unimodal distribution. In this section, we extend our analysis to a multimodal distribution and investigate the ability of APES to explore multiple modes. Specifically, we sample from the Gaussian mixture model given  by
\begin{equation}\label{GM2D}
	\begin{split}
		\pi(x_1, x_2) = &\frac{1}{2}\mathcal{N}(x_1,x_2, -1.5, 0, 0.4, 0.4, +0.6)\\ 
		+&\frac{1}{2}\mathcal{N}(x_1,x_2, +1.5,0, 0.2, 0.2, -0.6),
	\end{split}
\end{equation}
where $\mathcal{N}(x_1,x_2,\mu_1,\mu_2,\sigma_1,\sigma_2,\rho)$ represent a bivariate Gaussian distribution with means $(\mu_1, \mu_2)$, standard deviations $(\sigma_1, \sigma_2)$ and correlation $\rho$.  This distribution consists of two disjoint Gaussian components, each with a different mean and covariance structure. Our goal is to assess how well APES can identify and explore each mode of the distribution.

Again, we executed both algorithms using $15625\times320 = 5,000,000$ points and used the same 5000 iterations as burn-in. In Table~\ref{GM2Dstats}, we present the results of the runs. Note that $x_1$ has a very large autocorrelation only for the Stretch move. This can be understood by inspecting the corner plot in Fig.~\ref{GM2Dcorner}, where one can see that the marginal for $x_1$ is strongly bimodal with two prominent peaks. The Stretch move has difficulty with this kind of distribution since it uses a pair of points and makes proposals on the line that connects them. Since these points can be at different parts of the mixture, the proposals end up frequently between the peaks in the valley. The Jupyter notebook used to produce these results can be found at the gihub project page.\footnote{\href{https://github.com/NumCosmo/NumCosmo/blob/master/notebooks/apes_tests/gaussmix2d_mcmc.ipynb}{notebooks/apes\_tests/gaussmix2d\_mcmc.ipynb}}

In this section, we conducted a comparative study of two MCMC algorithms, APES and Stretch, on a Gaussian mixture model with strong bimodal behavior. Our goal was to demonstrate the ability of APES to handle multimodal distributions and compare it with the performance of the Stretch algorithm. The results clearly show that the APES algorithm converges to the underlying distribution much faster than the Stretch algorithm, which can struggle with this kind of distribution due to its use of a pair of points to make proposals. Moreover, the approximation method used in APES has two features that facilitate its handling of multimodal distributions. First, since we use Cauchy kernels, which have heavy tails, APES has the capability of finding the second peak. Second, since we use kernel basis, we can easily handle multiple modes as long as one has enough kernels to approximate both modes. These results demonstrate the effectiveness of APES in handling multimodal distributions and highlight the advantages of the APES algorithm for this type of problem. 

\begin{table}
	\caption{Results of an ensemble sampler run using APES and the Stretch move to generate a sample of~\eqref{GM2D}. The APES configuration is Interp-VKDE:Cauchy with an over-smooth parameter of 0.2 and a local fraction of walkers of 0.05. Both samples were generated using a burn-in of 5000 iterations and a total of 1.6 million points (5000 iterations multiplied by 320 chains). The mean-variance and other mean results were computed using all chains. Analytical results were also included for comparison. The analytical results with ${}^\star$ were computed using numerical integration.}
	\label{GM2Dstats}
	\centering
	\begin{tabular}{ |lrrr|}
		\hline
		\multicolumn{4}{|c|}{2-dimensional Gaussian Mixture Run} \\
		\hline
		& APES & Stretch & Analytical~\eqref{GM2D}\\
		\hline
		Number of walkers   &  $320$   & $320$ & -- \\
		Number of points & $5\times 10^6$     & $5\times 10^6$  & -- \\
		$\tau_{x_1}$ &$2.2$  &  $ 4017.8$ & -- \\
		$\tau_{x_2}$ &$2.4$ &  $ 48.0$ & --\\
		$\bar{x}_1$  & $0.0$ & $ -0.6 $  & $0$  \\
		$\bar{x}_2$ &$0.0$ & $0.0$  & $0$ \\
		$-2\overline{\ln\pi}$ & $1.57$ & $2.12$  & $1.56^\star$ \\
		$\mathrm{Var}\left(x_1\right)$  & $2.35 $ & $2.02$ & $47/20 \approx 2.35$  \\
		$\mathrm{Var}\left(x_2\right)$  & $0.1$ & $0.12$ & $1/10$  \\
		$\mathrm{Cor}\left(x_1, x_2\right)$  & $0.41 $ & $0.35$ & $20/49\approx 0.41$  \\
		$\mathrm{Var}\left(-2\ln \pi\right)$  & $5.92 $ & $5.63$ & $5.92^\star$  \\
		Acceptance rate    & $47\%$&  $23\% $ & -- \\
		\hline
	\end{tabular}
\end{table}

\begin{figure}
	\caption{Corner plot of MCMC runs using APES and Stretch for the Gaussian mixture model defined in Eq.~\eqref{GM2D}. The runs were configured and their burn-ins were determined as described in Table~\ref{GM2Dstats}. The APES sampler has already converged to the underlying distribution, with the mode on the left having a lower peak due to its larger variance compared to the mode on the right. The bimodal nature of the marginal on $x_1$ poses a challenge in achieving convergence with Stretch, whereas the approximation used in APES can effectively handle multimodal distributions, resulting in a run with low autocorrelation.}
	\label{GM2Dcorner}
	\includegraphics{gaussmix2d_corner.pdf}
	\centering
\end{figure}

\subsection{The Funnel Distribution}

Our research aims to test the adaptability and effectiveness of our APES method. To achieve this, here we apply our method to the Funnel distribution, which is a well-known benchmark distribution used to validate other sampling methods in the literature~\cite{Neal2003, Thompson2010, Karamanis2020}. The Funnel distribution is particularly challenging due to its complex and multi-dimensional nature. It consists of a Gaussian-distributed variable $\nu$ with a mean of zero and a standard deviation of $\sigma_\nu = 3$, along with $n$ other independent and identically distributed (iid) variables $x_1$ to $x_n$, where their variances $\sigma_{x_i}^2$ are conditional on $\nu$ and determined by $e^\nu$. To obtain samples from the Funnel distribution, we define a $n+1$-dimensional vector $x$ as follows:
\begin{equation}
	x = (\nu,x_1,,\dots,,x_n)
.\end{equation}
The probability density is then
\begin{align}
	\label{FD}
	\pi(x) &= \frac{\exp \left(-\frac{\chi^2}{2}\right)}{3 (2 \pi )^{\frac{n+1}{2}}}, & \chi^2 &\equiv \frac{\sum _{i=1}^n x_i^2}{e^{\nu }}+\frac{\nu ^2}{9}+\nu n.
\end{align}

As shown in \cite{Neal2003}, this distribution exhibits a characteristic feature where one region has a low probability density but a high volume of points, while the other region is the opposite. This is due to the large difference in variance for the variables $x_i$ when conditioned to different values of $\nu$. As a result, the Funnel distribution requires a sampler that is adaptive to the local scales of the problem. One such method is slice sampling \cite{Karamanis2020}, which is able to automatically adapt to different scales of a given distribution without requiring any hand-tuning.

Slice sampling has been shown to be a highly effective method for sampling from distributions with complex geometries and varying local scales, such as the Funnel distribution. Our proposed Interp-VKDE:Cauchy APES method was developed precisely for this type of problem, where traditional MCMC methods can struggle due to the extreme variation in scales across the distribution. To provide a comprehensive evaluation of the performance of our method, we will compare it to the Stretch move.

\begin{table}
	\caption{Data from the ensemble sampler algorithm using the APES and the Stretch move to generate a 10-dimensional Funnel sample. Both samples were generated using a burn-in of $4.2 \times 10^6$ points. The APES sample was generated with the Interp-VKDE:Cauchy method. Also, we used an over-smooth factor of $0.2$ and local fraction of $0.05$. It was taken the mean value of the results related to the variables $x_1-x_9$ since they have equal moments.} \label{FU10stats}
	\centering
	\begin{tabular}{ |lrrr|}
		\hline
		\multicolumn{4}{|c|}{10-dimensional Funnel Run} \\
		\hline
		& APES & Stretch & Analytical~\eqref{FD}\\
		\hline
		Number of walkers   &  $3000$   & $3000$ & -- \\
		Number of points & $7\times 10^6$     &$7\times 10^6$  & -- \\
		$\tau_{\nu}$ &$33.1$  &  $518.5$ & -- \\
		$\bar{\tau}_{x_1 - x_9}$ & $68.2$ &  $ 172.7 $ & -- \\
		$\bar{\nu}$ & $0.011$ & $1.2$ & $0$ \\
		$\mathrm{mean}(\bar{x}_{n})$ & $0.024$ &  $0.073$ & $0$ \\
		$\overline{\chi^2}$ & $10.1$ & $20.4$  & $10$ \\
		$\mathrm{Var}(\nu)$  & $8.87$ & $ 5.62$  & $9$ \\
		$\mathrm{mean}\left[\mathrm{Var}(x_n)\right]$ & $85.68$ & $171.84$ & $\approx 90.0$ \\
		$\mathrm{Var}\left(\chi^2\right)$  & $740.5$ & $510.9$ & $749$  \\
		Acceptance Ratio    & $10\%$&  $ 36\% $ & -- \\
		\hline
	\end{tabular}
\end{table}

Table~\ref{FU10stats} shows the outcomes of the sampling method utilized, including both APES and the Stretch move. It is worth noting that the APES sample was able to depict the underlying distribution described by~\eqref{FD} as one can see in Fig.~\ref{FU10corner}. The acceptance rate underwent considerable changes throughout the sampling process. Initially, as the initial sample did not represent the distribution in~\eqref{FD} well, the kernel interpolation failed to approximate the distribution accurately. During this phase, the acceptance probability in~\eqref{acceptprob} ensured that the sample moved in the direction of higher probability in $\pi(x)$, while the terms involving the approximate distribution were unrelated to $\pi(x)$. This led to a low acceptance rate of 2\% to 5\%. However, as the sample $\vec{x}$ began resembling a distribution from $\pi(x)$, the acceptance rate improved to 10\%. It was observed that during the burn-in phase, the interpolation could make the steps less efficient, as it attempted to fit the distribution using points that were not good representatives. Consequently, the covariances were not well adapted to the true distribution. To address this issue, the interpolation was disabled during the burn-in phase, resulting in an improved acceptance rate. Once the ensemble started to represent the distribution well, the interpolation was re-enabled, leading to better mixing and lower autocorrelation. However, it is important to note that even with the interpolation turned on, the chains eventually converge to a good distribution, albeit with longer chains.

In~\cite{Karamanis2020}, the mean autocorrelation time was found to be $\tau = 129$ for a $25$-dimensional Funnel distribution using ensemble slice sampling. In our experiment with the same $25$-dimensional Funnel distribution, we employed APES with 5000 walkers, resulting in an acceptance rate of $\approx 1\%-2\%$ and an autocorrelation of $\approx 150$. One challenge of increasing the ensemble size is that it takes longer to move past the burn-in phase, as more points need to be moved to the correct distribution. One possible strategy to address the challenge of a longer burn-in phase with larger ensemble sizes is to employ fewer walkers in the initial sampling phase until a reasonable approximation of the posterior distribution is achieved. This approximation can then be used to generate an initial distribution for further analysis with a larger ensemble. We followed the strategy of increasing the number of walkers from 5000 to 8000 in the analysis of the 25-dimensional Funnel distribution, resulting in a mean autocorrelation of approximately 80 after the burn-in is removed. The Funnel analysis can also be found on the NumCosmo github.\footnote{\href{https://github.com/NumCosmo/NumCosmo/blob/master/notebooks/apes_tests/funnel_mcmc.ipynb}{notebooks/apes\_tests/funnel\_mcmc.ipynb}}

\begin{figure*}
	\caption{Corner plot comparing the performance of MCMC runs using the APES and Stretch methods for the Funnel 10-dimensional model defined in Eq.~\eqref{FD}. The runs were configured and their burn-ins were determined as described in Table~\ref{FU10stats}. The APES sampler has already converged to the underlying distribution, as indicated by the marginal distribution for $\nu$ which shows the exploration of both the high ($\nu>0$) and low ($\nu<0$) variance regions. In contrast, the marginal distribution obtained using the Stretch move does not accurately reflect the distribution in the low variance region, as it cannot easily explore this region. To account for the large variance difference of the variables $x_n$ conditional to $\nu$, we plotted transformed variables $\sinh^{-1}(x_n)$. Additionally, note that the high variance region is much larger than the low variance region, resulting in a less dense sample in this region. To improve the approximation, more points are needed in the ensemble.}
	\label{FU10corner}
	\includegraphics{funnel10_corner.pdf}
	\centering
\end{figure*}

\subsection{Cosmological Model}

This section delves into an examination of the parametric space within the standard cosmological model, which incorporates a single massive neutrino. Our analysis involves fitting several parameters, namely the Hubble parameter $H_0$, the cold dark matter density parameter $\Omega_{c0}$, the curvature parameter $\Omega_{k0}$, the dark energy equation of state $w$, the supernovae type Ia absolute magnitude $\mathcal{M}_1$, and a massive neutrino $m_{\nu0}$ (in electron-volt eV), as described in detail in~\cite{Doux2018}. The six-dimensional parametric space is denoted as $x = (H_0,\,\Omega_{c0},\,\Omega_{k0},\,w,\,m_{\nu0},\,\mathcal{M}_1)$. We adopt simple flat priors for all parameters, which have minimal impact on the results since the fitting region is sufficiently far from the borders, except for cases where $m_{\nu0}>0$ and $\Omega_{k0}>-0.3$.
 The likelihood function used in our analysis comprises the Pantheon+ likelihood~\cite{Brout2022, Scolnic2022}, the Baryon Acoustic Oscillation (BAO) derived cosmological distances obtained from Data Release 16 (DR16) of the SDSS-III~\cite{Alam2016, Alam2021}, which also incorporates all non-overlapping DR12 distances, and the cosmological chronometers collated in~\cite{GomezValent2019}. Our deliberate selection of a poorly constrained parametric space aims to study the behavior of the sampler when dealing with intricate and unconstrained parametric spaces in a real-world scenario.

As shown in Table~\ref{CA}, the APES sample exhibited an autocorrelation time that was over 10 times smaller than that of the Stretch sample in this test. Although the distribution of this problem is mostly well-behaved, as evidenced by the acceptance ratio of both samples, the difficulty of the analysis is increased by the presence of tails in the poorly determined parameters $m_{\nu0}$ and $\Omega_{k0}$, as shown in Fig.~\ref{CAFigure}. The APES algorithm achieved a high acceptance ratio and a significantly better autocorrelation time compared to the Stretch sample. Moreover, the APES sampler obtained better results with a smaller number of points, demonstrating the efficiency of the algorithm.

It is important to note that the Stretch move can cause the chains to appear falsely converged early in the analysis due to its larger autocorrelation. This slow convergence can lead to false positives when diagnosing the results. As depicted in Fig.~\ref{CAFigure}, the tails of the distribution are not well-represented by the Stretch chains. This is because the chains spend a longer time in the center of the distribution during the slow phase, which is reflected by the large autocorrelation. Moreover, Table~\ref{CA} shows that the slow convergence phase can bias the mean and report a lower variance due to the over-representation of the distribution center. However, increasing the burn-in to 500 iterations yields practically the same results for both chains. Nevertheless, it is crucial to emphasize that in the absence of a more efficient sampler, problems with very large autocorrelation can result in false positives when diagnosing the convergence of the chains.

\begin{table}
	\caption{Results from the cosmological analysis are presented. The APES data were generated using the Interp-VKDE:Cauchy option with a local fraction of 5\% and an over-smooth factor of 0.2. Both samples were generated using 2000 walkers and 1.4 million points, with a burn-in of 200 iterations removed in a total of 400,000 points. Although the APES chains had converged since the 200th iteration, the larger autocorrelation resulting from the Stretch move necessitated increasing the chain sizes to obtain a better approximation of the posterior using this sampler. In this study, we intentionally included the slow-convergence phase of the Stretch move to show its influence and bias on the parameters. Increasing the burn-in to 500 iterations removes the bias and reduces the autocorrelation to about 40 for all parameters.} \label{CA}
	\centering
	\begin{tabular}{ |lrr|  }
		\hline
		\multicolumn{3}{|c|}{Pantheon BAO sample} \\
		\hline
		& APES & Stretch\\
		\hline
		Number of Walkers   &  $2000$   & $2000$ \\
		Number of Points & $1.4 \times 10^{6}$     &$ 1.6 \times10^{6}$  \\
		$\tau_{H_0}$ &$2.8$  &  $33$ \\
		$\tau_{\Omega_{c0}}$ &$3.6$ &  $403$ \\
		$\tau_{\Omega_{k0}}$  & $3.0$ & $147$   \\
		$\tau_{w}$ &$2.8$ & $53$   \\
		$\tau_{m_{\nu0}}$ &$3.8$ & $369$   \\
		$\tau_{\mathcal{M}_1}$  &$2.7$ & $33$  \\
		$\overline{H}_0$ & $72.3 \pm 0.9$ & $72.3 \pm 0.9$ \\
		$\overline{\Omega}_{c0}$ & $0.27 \pm 0.04$ & $0.28 \pm 0.03$ \\
		$\overline{\Omega}_{k0}$ & $-0.21 \pm 0.05$ & $-0.21 \pm 0.05$ \\
		$\overline{w}$ & $-0.9 \pm 0.05$ & $-0.9 \pm 0.05$ \\
		$\overline{m}_{\nu0}$ & $1.4 \pm 2.0$ & $1.0 \pm 1.6$ \\
		$\overline{\mathcal{M}}_1$ & $-19.3 \pm 0.03$ & $-19.3 \pm 0.03$ \\
		Acceptance Rate  & $60 \%$&  $52\% $ \\
		\hline
	\end{tabular}
\end{table}

\section{Conclusion}
\label{conclusion}

In summary, the radial basis interpolation technique employed in the APES algorithm has shown great efficiency in generating approximate posterior distributions and generating sample points from them, with smaller autocorrelation times and good acceptance ratios compared to other MCMC algorithms. Our evaluation of the APES and Stretch sampler using the same posterior distributions demonstrated that the APES algorithm outperformed the Stretch move sampler, particularly in adaptive sampling problems such as the Funnel and Rosenbrock distributions.

However, it is important to note that in the large autocorrelation cases, the chains tend to falsely appear converged early in the analysis. This is because the slow convergence can result in false positives when diagnosing the results, as shown in our experiments. The tails of the distribution are not well-represented by the chains when the slow phase is included. In this phase, the chains are still exploring relevant regions of the parametric space, as evidenced by the large autocorrelation. Moreover, the slow convergence phase can bias the mean and report a lower variance due to the over-representation of the distribution center.

\begin{figure*}
	\caption{Corner plot comparing the performance of MCMC runs using the APES and Stretch methods for the cosmological analysis. The Stretch chains in this plot include the slow-convergence phase to illustrate its potential bias on the results. Note that the tails of the distribution are not well represented in the Stretch chains due to the longer time spent in the center of the distribution. Increasing the burn-in to 500 iterations removes the bias and shows consistent results between the two samplers.}
	\label{CAFigure}
	\includegraphics{xcdm_nopert_corner.pdf}
	\centering
\end{figure*}

We have also tested the algorithm for a cosmology likelihood, and our results showed that the APES algorithm had a higher acceptance ratio and a much smaller autocorrelation time when compared to the Stretch move sampler. For wider samples, the APES algorithm is expected to have a smaller autocorrelation, which was confirmed by our experiments.

In the Funnel distribution test, the APES algorithm demonstrated its effectiveness in solving problems that necessitate an adaptive sampler. While the APES algorithm exhibited satisfactory overall performance, there is scope for improvement by integrating a combination of different approximation methods at different stages of the sampling process. Furthermore, in certain situations, the interpolation technique may not be optimal for the burn-in phase or the over-fitting parameter may require more precise adjustment. Therefore, users should adapt the sampler to suit the specific problem and assess which approach produces the best outcomes. Through this approach, the APES algorithm can offer even more efficient and accurate results for a wide range of applications.

We thoroughly tested the robust method in various scenarios and found that while it did not show a significant improvement in the sampling process as a whole, it did come at a significant cost. Despite its potential benefits, the computational expense associated with the robust method makes it impractical for large-scale applications. Therefore, in practical settings where computational efficiency is paramount, our findings suggest that the standard approach may still be preferable. Nonetheless, further research into more computationally efficient robust methods could yield valuable insights for future applications. For instance, during the burn-in phase of the sampling process, robust methods can better approximate the covariance even in the presence of many outliers. This capability can be particularly important in certain applications where accurate estimation of the covariance is critical for the downstream analysis. Therefore, it may be worth considering the use of robust methods during the burn-in phase or in situations where outliers are expected to be prevalent. Nonetheless, it's important to weigh the potential benefits against the added computational cost when making this decision.

The versatility of the APES algorithm enables it to integrate with any available approximation technique, making it a powerful tool for various applications. Its adaptable framework and efficient sampling of the target distribution has been demonstrated to yield faster convergence rates and less autocorrelated results compared to other widely-used MCMC algorithms. Although the integration of additional kernels, cross-validation, and fitting techniques could enhance the framework's capabilities in the future, the current framework is already highly effective and appropriate for solving a wide range of problems.

\section*{Acknowledgements}

SDPV acknowledges the support of CNPq of Brazil under grant PQ-II 316734/2021-7. EJB acknowledges the support of CAPES under the grant PDSE 88881.690203/2022-01.   

\bibliographystyle{mnras}
\bibliography{main} 

% Don't change these lines
\bsp	% typesetting comment
\label{lastpage}
\end{document}