
%% bare_adv.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See: 
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the advanced use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE Computer
%% Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


% IEEEtran V1.7 and later provides for these CLASSINPUT macros to allow the
% user to reprogram some IEEEtran.cls defaults if needed. These settings
% override the internal defaults of IEEEtran.cls regardless of which class
% options are used. Do not use these unless you have good reason to do so as
% they can result in nonIEEE compliant documents. User beware. ;)
%
%\newcommand{\CLASSINPUTbaselinestretch}{1.0} % baselinestretch
%\newcommand{\CLASSINPUTinnersidemargin}{1in} % inner side margin
%\newcommand{\CLASSINPUToutersidemargin}{1in} % outer side margin
%\newcommand{\CLASSINPUTtoptextmargin}{1in}   % top text margin
%\newcommand{\CLASSINPUTbottomtextmargin}{1in}% bottom text margin




%
\documentclass[10pt,journal,compsoc]{IEEEtran}
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}


% For Computer Society journals, IEEEtran defaults to the use of 
% Palatino/Palladio as is done in IEEE Computer Society journals.
% To go back to Times Roman, you can use this code:
%\renewcommand{\rmdefault}{ptm}\selectfont

\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage{algorithmic}
% \usepackage{algpseudocode}
\usepackage{url}
\usepackage{subfigure}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)



% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % The IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%\usepackage{acronym}
% acronym.sty was written by Tobias Oetiker. This package provides tools for
% managing documents with large numbers of acronyms. (You don't *have* to
% use this package - unless you have a lot of acronyms, you may feel that
% such package management of them is bit of an overkill.)
% Do note that the acronym environment (which lists acronyms) will have a
% problem when used under IEEEtran.cls because acronym.sty relies on the
% description list environment - which IEEEtran.cls has customized for
% producing IEEE style lists. A workaround is to declared the longest
% label width via the IEEEtran.cls \IEEEiedlistdecl global control:
%
% \renewcommand{\IEEEiedlistdecl}{\IEEEsetlabelwidth{SONET}}
% \begin{acronym}
%
% \end{acronym}
% \renewcommand{\IEEEiedlistdecl}{\relax}% remember to reset \IEEEiedlistdecl
%
% instead of using the acronym environment's optional argument.
% The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/acronym


%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/pkg/mdwtools


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/pkg/eqparbox




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix


%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.


% NOTE: PDF thumbnail features are not required in IEEE papers
%       and their use requires extra complexity and work.
%\ifCLASSINFOpdf
%  \usepackage[pdftex]{thumbpdf}
%\else
%  \usepackage[dvips]{thumbpdf}
%\fi
% thumbpdf.sty and its companion Perl utility were written by Heiko Oberdiek.
% It allows the user a way to produce PDF documents that contain fancy
% thumbnail images of each of the pages (which tools like acrobat reader can
% utilize). This is possible even when using dvi->ps->pdf workflow if the
% correct thumbpdf driver options are used. thumbpdf.sty incorporates the
% file containing the PDF thumbnail information (filename.tpm is used with
% dvips, filename.tpt is used with pdftex, where filename is the base name of
% your tex document) into the final ps or pdf output document. An external
% utility, the thumbpdf *Perl script* is needed to make these .tpm or .tpt
% thumbnail files from a .ps or .pdf version of the document (which obviously
% does not yet contain pdf thumbnails). Thus, one does a:
% 
% thumbpdf filename.pdf 
%
% to make a filename.tpt, and:
%
% thumbpdf --mode dvips filename.ps
%
% to make a filename.tpm which will then be loaded into the document by
% thumbpdf.sty the NEXT time the document is compiled (by pdflatex or
% latex->dvips->ps2pdf). Users must be careful to regenerate the .tpt and/or
% .tpm files if the main document changes and then to recompile the
% document to incorporate the revised thumbnails to ensure that thumbnails
% match the actual pages. It is easy to forget to do this!
% 
% Unix systems come with a Perl interpreter. However, MS Windows users
% will usually have to install a Perl interpreter so that the thumbpdf
% script can be run. The Ghostscript PS/PDF interpreter is also required.
% See the thumbpdf docs for details. The latest version and documentation
% can be obtained at.
% http://www.ctan.org/pkg/thumbpdf


% NOTE: PDF hyperlink and bookmark features are not required in IEEE
%       papers and their use requires extra complexity and work.
% *** IF USING HYPERREF BE SURE AND CHANGE THE EXAMPLE PDF ***
% *** TITLE/SUBJECT/AUTHOR/KEYWORDS INFO BELOW!!           ***
\newcommand\MYhyperrefoptions{bookmarks=true,bookmarksnumbered=true,
pdfpagemode={UseOutlines},plainpages=false,pdfpagelabels=true,
colorlinks=true,linkcolor={black},citecolor={black},urlcolor={black},
pdftitle={Bare Demo of IEEEtran.cls for Computer Society Journals},%<!CHANGE!
pdfsubject={Typesetting},%<!CHANGE!
pdfauthor={Michael D. Shell},%<!CHANGE!
pdfkeywords={Computer Society, IEEEtran, journal, LaTeX, paper,
             template}}%<^!CHANGE!
%\ifCLASSINFOpdf
%\usepackage[\MYhyperrefoptions,pdftex]{hyperref}
%\else
%\usepackage[\MYhyperrefoptions,breaklinks=true,dvips]{hyperref}
%\usepackage{breakurl}
%\fi
% One significant drawback of using hyperref under DVI output is that the
% LaTeX compiler cannot break URLs across lines or pages as can be done
% under pdfLaTeX's PDF output via the hyperref pdftex driver. This is
% probably the single most important capability distinction between the
% DVI and PDF output. Perhaps surprisingly, all the other PDF features
% (PDF bookmarks, thumbnails, etc.) can be preserved in
% .tex->.dvi->.ps->.pdf workflow if the respective packages/scripts are
% loaded/invoked with the correct driver options (dvips, etc.). 
% As most IEEE papers use URLs sparingly (mainly in the references), this
% may not be as big an issue as with other publications.
%
% That said, Vilar Camara Neto created his breakurl.sty package which
% permits hyperref to easily break URLs even in dvi mode.
% Note that breakurl, unlike most other packages, must be loaded
% AFTER hyperref. The latest version of breakurl and its documentation can
% be obtained at:
% http://www.ctan.org/pkg/breakurl
% breakurl.sty is not for use under pdflatex pdf mode.
%
% The advanced features offer by hyperref.sty are not required for IEEE
% submission, so users should weigh these features against the added
% complexity of use.
% The package options above demonstrate how to enable PDF bookmarks
% (a type of table of contents viewable in Acrobat Reader) as well as
% PDF document information (title, subject, author and keywords) that is
% viewable in Acrobat reader's Document_Properties menu. PDF document
% information is also used extensively to automate the cataloging of PDF
% documents. The above set of options ensures that hyperlinks will not be
% colored in the text and thus will not be visible in the printed page,
% but will be active on "mouse over". USING COLORS OR OTHER HIGHLIGHTING
% OF HYPERLINKS CAN RESULT IN DOCUMENT REJECTION BY THE IEEE, especially if
% these appear on the "printed" page. IF IN DOUBT, ASK THE RELEVANT
% SUBMISSION EDITOR. You may need to add the option hypertexnames=false if
% you used duplicate equation numbers, etc., but this should not be needed
% in normal IEEE work.
% The latest version of hyperref and its documentation can be obtained at:
% http://www.ctan.org/pkg/hyperref





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{PBScaler: A Bottleneck-aware Autoscaling Framework for Microservice-based Applications}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{Shuaiyu~Xie,
        Jian~Wang,
        Bing~Li,
        Zekun~Zhang,
        Duantengchuan Li,
        Patrick C. K. Hung
        % <-this % stops a space
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem S. Xie,  J. Wang, B. Li, Z. Zhang, and D. Li are with the School of Computer Science, Wuhan University, China. P. C. K. Hung is with the Faculty of Business and Information Technology, Ontario Tech University, Canada. \protect\\
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
E-mail: \{theory,jianwang,bingli,zekunzhang,dtclee1222\}@whu.edu.cn, patrick.hung@ontariotechu.ca}% <-this % stops a space
\thanks{Manuscript received xxx xx, xxxx; revised xxx xx, xxxx.}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Advanced Demo of IEEEtran.cls for IEEE Computer Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society journal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}
% In recent years, a significant number of Web applications have migrated from the monolithic architecture to the microservice architecture due to the benefits of microservices like resilience and faster delivery. Dynamic workloads and intricate interactions between microservices lead to severe issues in performance assurance and resource conservation for online applications. Therefore, many cloud providers use elastic scaling technology to dynamically adjust the number of replicas for each microservice to balance application performance and resource consumption. 
Autoscaling is critical for ensuring optimal performance and resource utilization in cloud applications with dynamic workloads. 
However, traditional autoscaling technologies are typically no longer applicable in microservice-based applications due to the diverse workload patterns and complex interactions between microservices. 
% Diverse workload patterns and complex interactions in microservice-based applications bring new challenges to autoscaling technology.
Specifically, the propagation of performance anomalies through interactions leads to a high number of abnormal microservices, making it difficult to identify the root performance bottlenecks (PBs) and formulate appropriate scaling strategies.
% For a performance bottleneck (PB) microservice that needs to be scaled up, the performance degradation of it can propagate along the invocation chain and lead to a large number of abnormal microservices. As a result, it's hard to identify the initial PB from abnormal microservices and formulate corresponding scaling strategies. 
In addition, to balance resource consumption and performance, the existing mainstream approaches based on online optimization algorithms require multiple iterations, leading to oscillation and elevating the likelihood of performance degradation.
% To trade off resource consumption and performance, existing works adopt optimization algorithms to find near-optimal autoscaling strategies. However, the process of finding a near-optimal scaling strategy requires continuous trial and improvement based on application feedback, which causes oscillation in online application and increase the risk of performance degradation.
% To tackle these issues, we propose  TopoRank, a random walk algorithm based on the topological potential theory, which can locate the PBs of performance degradation. We also propose PBScaler, an autoscaling system that integrates TopoRank with an offline performance-aware optimization algorithm, resulting in optimized replica management without disrupting the online application.
To tackle these issues, we propose PBScaler, a bottleneck-aware autoscaling framework designed to prevent performance degradation in a microservice-based application. The key  insight of PBScaler is to locate the PBs. Thus, we propose TopoRank, a novel random walk algorithm based on the topological potential to reduce unnecessary scaling. By integrating TopoRank with an offline performance-aware optimization algorithm, PBScaler optimizes replica management without disrupting the online application. Comprehensive experiments demonstrate that PBScaler outperforms existing state-of-the-art approaches in mitigating performance issues while conserving resources efficiently.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
microservice, autoscaling, performance bottleneck, replica management 
\end{IEEEkeywords}}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when compsoc mode
% is not selected <OR> if conference mode is selected - because compsoc
% conference papers position the abstract like regular (non-compsoc)
% papers do!
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc under a non-conference mode.


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


\ifCLASSOPTIONcompsoc
\IEEEraisesectionheading{\section{Introduction}\label{Introduction}}
% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.




% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps (small caps for compsoc).
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
\IEEEPARstart{W}{ith} the advancement of microservice architecture, an increasing number of cloud applications are migrating from monolithic architecture to microservice architecture \cite{ma2020automap, kim2013root, shan2019diagnosis, qiu2020firm, liu2018fuzzy, zhang2022robust}. This new architecture reduces application coupling by breaking a monolithic application into multiple microservices that communicate with each other via HTTP or RPC protocols \cite{lin2018microscope}. Moreover, each microservice can be developed, deployed, and scaled independently by separate teams, enabling rapid application development and iteration. Nevertheless, the unpredictability of external workloads and the complexity of interactions between microservices can result in performance degradation \cite{liu2021microhecl, choi2021phpa, kwan2019hyscale}. Cloud providers must prepare excessive resources to meet the service level objective (SLO) of application owners, which usually causes unnecessary waste of resources \cite{baarzi2021showar, park2021graf}. As a result, the imbalance between satisfying SLO and minimizing resource consumption becomes a major challenge encountered by resource management in microservices.

Microservice autoscaling refers to the capability of allocating resources elastically in response to workload variations \cite{al2017elasticity}. By utilizing the elasticity property of microservices, autoscaling can mitigate the conflict between resource cost and performance. However, the autoscaling of microservices suffers from accurately scaling the performance bottleneck (PB) in a short period. 
% due to the need to make decisions for all abnormal microservices.
Due to the complexity of communication between microservices, the performance degradation of a PB may propagate to other microservices via message passing \cite{kim2013root}, resulting in a high number of abnormal microservices at the same time. We demonstrated this by injecting burst workloads to specific microservices in Online Boutique \footnote{https://github.com/GoogleCloudPlatform/microservices-demo}, an open-source microservice application developed by Google. Fig. \ref{fig:part-dependency} shows that the performance degradation in the PB \textit{Recommend} can spread to the upstream microservices like \textit{Checkout} and \textit{Frontend}. To further verify the importance of accurately scaling the PB, we conducted stress testing and scaled different microservices separately. As shown in Fig. \ref{fig:root-test}, abnormal microservice (\textit{Frontend}) scaling cannot alleviate SLO violations. However, when we identified and scaled the PB \textit{Recommend}, the performance of the microservice application improved. Unfortunately, locating PBs is usually time-consuming and can occasionally make mistakes  \cite{chen2016causeinfer}.

In recent years, several approaches have been proposed to identify critical microservices before autoscaling. For example, the default autoscaler of Kubernetes \footnote{https://github.com/kubernetes/autoscaler} filters microservices for direct scaling based on a static threshold of computing resources. Yu \textit{et al.} \cite{yu2019microscaler} defined the boundaries of elastic scaling by calculating the service power, which is the ratio between the 50th percentile response time (P50) and the 90th percentile response time (P90). Furthermore, Qiu \textit{et al.} \cite{qiu2020firm} introduced an SVM-based approach for extracting critical paths by analyzing the ratio of various tail latencies. Although these studies have narrowed the scope of autoscaling, they still take into account non-bottleneck microservices that may affect scaling strategies, especially when a large number of microservices in the application are abnormal at the same time. Consequently, there is an urgent need to pinpoint bottleneck microservices accurately before autoscaling.

To balance resource consumption and performance, existing works have employed online optimization algorithms to find near-optimal autoscaling strategies. However, due to the vast range of possible strategies for autoscaling, these approaches require a significant number of attempts, which will be problematic for online applications. For example, Train Ticket\footnote{https://github.com/FudanSELab/train-ticket} is the largest open-source microservice application, consisting of nearly 40 microservices. Assuming that each microservice can have up to 15 replicas, determining the optimal allocation strategy for this application is undoubtedly an NP-hard problem, as there are a maximum of $15^{40}$ scaling alternatives. Additionally, the duration of the feedback loop in online optimization is too long to achieve model convergence. It is also essential to consider the potential risks of performance degradation caused by online optimization. Fig. \ref{fig:microscaler-wave} illustrates the impact of burst workloads on the replica fluctuation and latency fluctuation of  MicroScaler \cite{yu2019microscaler}, an online autoscaling approach incorporating online Bayesian optimization to find the global minimizer of the total cost. The frequent online attempts to create replicas (Fig. \ref{fig:microscaler-wave}a) caused by online optimization result in oscillations and performance degradation (Fig. \ref{fig:microscaler-wave}b). As a result, we are inspired to design an offline optimization process fueled by feedback from a simulator.

% For the first obstacle, it is essential to isolate abnormal microservices and further identify PB microservices causing performance issues. In recent years, many studies have been proposed to identify the critical microservices before elastic scaling. Yu \textit{et al.} \cite{yu2019microscaler} demarcated the boundaries of elastic scaling by calculating the service power, which is the ratio between 50th percentile response time (P50) and 90th percentile response time (P90). Furthermore, Qiu \textit{et al.} \cite{qiu2020firm} introduced an SVM-based critical path extraction approach for ratio of various tail latency. The default autoscaler of Kubernetes \footnote{https://github.com/kubernetes/autoscaler} filtered microservices for direct scaling based on the static threshold of computing resources. Although these studies do narrow the scope of elastic scaling, they nevertheless leave a lot of non-bottleneck microservices to influence scaling decisions while a large number of microservices in the application are abnormal at the same time. Consequently, there is an urgent need to pinpoint bottleneck microservices.


% \begin{figure}[t]
% \centering{
%  \subfigure[Dependency Graph]{
%  \includegraphics[width=0.38\linewidth]{fig/dependency.pdf} } 
%  \subfigure[Lateny Fluctuation]{
%  \includegraphics[width=\linewidth]{fig/root-test.pdf} } 
% }
% \caption{Latency of elastic scaling for specific microservices under burst workloads (grey). The end-to-end latency (green) is still high when only the Frontend service is scaled up (t = 38). The performance of microservices is greatly improved when the Recommendation service is expanded (t = 65).}
% \label{fig:root-test}

% \end{figure}
\begin{figure}[t]
\centering{
 \includegraphics[width=\linewidth]{fig/part-dependency.pdf}
}
\caption{Part of the invocation relationship in Online Boutique.}
\label{fig:part-dependency}
\end{figure}

\begin{figure}[t]
\centering{
 \includegraphics[width=0.9\linewidth]{fig/root-test.pdf}
}
\caption{Latency distribution in four scenarios.}
\label{fig:root-test}
\end{figure}

% To address the challenge in terms of wide strategy space, several methods \cite{park2021graf, yu2019microscaler, gias2019atom, baarzi2021showar} are proposed to determine the number of microservice replicas based on optimization techniques. MicroScaler \cite{yu2019microscaler} incorporates Bayesian optimization to find the global minimizer of the total cost online while satisfying SLO requirements. However, as shown in Fig. \ref{fig:microscaler-wave}, the frequent online attempts for microservice replicas (Fig. \ref{fig:microscaler-wave}a) required by optimization algorithms will cause oscillations and performance degradation (Fig. \ref{fig:microscaler-wave}b). Park \textit{et al.} \cite{park2021graf} optimized resource allocation with the aid of the GNN latency prediction model and achieved performance improvements without interrupting the online application. By simulating the feedback from online application, the oscillations caused by strategy attempts can be avoided. Unfortunately, as the number of microservices increases, it will be challenging to accurately predict the latency of each microservice on different scales. In contrast, it is more feasible to classify the SLO violations based solely on workloads and resource configuration \cite{zhang2021sinan}. This motivates us to design an SLO violation predictor to guide the optimization process.

This paper presents PBScaler, a horizontal autoscaling framework designed to prevent performance degradation in microservice-based applications by identifying and addressing bottlenecks. Instead of optimizing resources for all abnormal microservices, as was done in previous work \cite{yu2019microscaler, baarzi2021showar}, we propose TopoRank, a random walk algorithm based on topological potential theory (TPT) to identify performance bottlenecks (PBs). By taking into account microservice dependencies and anomaly potential, TopoRank improves the accuracy and explainability of bottleneck localization. 
% An intuitive solution is to optimize the resources for all abnormal microservices \cite{yu2019microscaler, baarzi2021showar}. However, plenty of abnormal microservices caused by the complex interaction between microservices will expand the strategy space, and it is difficult to resolve the performance issue by performing scaling for non-bottleneck microservices (Fig. \ref{fig:root-test}).
After identifying the PBs by TopoRank, PBScaler further employs a genetic algorithm to find nearly optimal strategies. To avoid application oscillation caused by excessive optimization, the process is conducted offline and is guided by an SLO violation predictor, which simulates the online application and provides feedback to the scaling strategies. The main contributions of the paper are summarized as follows:

\begin{itemize}
\item We propose PBScaler, a bottleneck-aware autoscaling framework designed to prevent performance degradation in a microservice-based application.
% TopoRank, a novel random walk algorithm based on the topological potential theory, to identify performance bottlenecks. 
By pinpointing bottlenecks, PBScaler can reduce unnecessary scaling and expedite the optimization process.

\item 
% To optimize resource consumption while avoiding SLO violations, we employ a genetic algorithm-based offline optimization process.
We employ a genetic algorithm-based offline optimization process to optimize resource consumption while avoiding SLO violations. This process is guided by an SLO violation predictor and is designed to strike a balance between resource consumption and performance without disrupting online applications.

\item We design and implement PBScaler in the Kubernetes system. To evaluate its effectiveness, we conduct extensive experiments with real-world and emulated workload injection on two widely-used microservice systems running in an online environment. Experimental results demonstrate that PBScaler outperforms several state-of-the-art elastic scaling methods.

\end{itemize}

The rest of the paper is organized as follows. In Section 2, we discuss the related work about bottleneck analysis and autoscaling for microservices. In Section 3, we describe the overall system in detail. In Section 4, we present the evaluations and experimental results. Section 5 concludes our work and discusses the future research direction.

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.
% However, the Computer Society has been known to put floats at the bottom.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.


\begin{figure}[t]
\centering{
 \subfigure[ Replica fluctuation ]{
 \includegraphics[width=0.47\linewidth]{fig/microscaler-wave.pdf} } 
 \subfigure[ Latency fluctuation ]{  \includegraphics[width=0.48\linewidth]{fig/microscaler-latency-wave.pdf}}
}
\caption{The replica fluctuation and latency fluctuation of MicroScaler under burst workloads. Excessive online attempts (a) cause oscillations and performance degradation (b).}
\label{fig:microscaler-wave}
\end{figure}


\section{Related Work}

With the advancement of cloud computing, numerous autoscaling methods for cloud resources, such as virtual machines or containers, have been proposed in academia and industry \cite{sharma2011cost, da2015autoelastic, roy2011efficient, wang2022trust}. However, 
% due to the intricate dependencies between microservices, autoscaling for microservices can be much more complicated. 
autoscaling for microservices can be much more complicated due to the intri- cate dependencies between microservices.

Performance bottleneck analysis, also known as root cause analysis, is a useful way for quickly locating the bottleneck responsible for the performance degradation of microservices, hence reducing the time and effort required for autoscaling. In this section, we will analyze the related work on bottleneck analysis and autoscaling for microservices.


\begin{figure*}
	\centering
	\includegraphics[width=0.95\textwidth]{fig/structure.pdf}
	\caption{Framework of PBScaler}
	\label{fig:structure}
 \vspace{-2mm}
\end{figure*}

\subsection{Bottleneck Analysis}

In recent years, numerous methods for bottleneck analysis in microservice scenarios have been developed, most of which rely on three types of data: logs, traces, and metrics. 1) Logs. Jia \textit{et al.} \cite{jia2017approach} and Nandi \textit{et al.} \cite{nandi2016anomaly} first extracted templates and flows from normal-state logs, matched them with target logs, and filtered out abnormal logs. 2) Traces. Trace is an event tracking-based record that reproduces the request process between microservices. Several studies \cite{yu2021microrank, yu2021tracerank, mi2013toward, li2021practical} have been introduced to pinpoint the bottlenecks using traces. Yu \textit{et al.} \cite{yu2021microrank, yu2021tracerank} located bottlenecks by combining spectrum analysis and the PageRank algorithm on the dependency graph constructed by traces, while Mi \textit{et al.} \cite{mi2013toward} presented an unsupervised machine learning prototype to learn the pattern of microservices and filter out abnormal microservices. However, using traces can be intrusive to the code and requires operators to have a deep understanding of the structure of the microservices. 3) Metrics. Some approaches \cite{zhangaamr, wu2020microrca, kim2013root} leverage graph random walk algorithms to simulate the propagation process of anomalies and then find bottlenecks by integrating statistical features of metrics and dependencies between microservices. Additionally, methods such as CauseInfer \cite{chen2016causeinfer} and MicroCause \cite{meng2020localizing} focused on building metrics causality graphs with causal inference, which typically involve hidden indirect relationships between metrics. 

Since the workflow code is rarely modified when monitoring metrics, collecting metrics for microservices is usually cheaper than using trace. Moreover, using metrics as the primary monitoring data can reduce the cost of integrating bottleneck analysis and autoscaling, as metrics are widely used in the latter scenario. Despite these approaches' advantages, most have no preference in selecting the starting point for abnormal backtracking. In contrast, our approach begins random walks from microservices with greater anomaly potential, accelerating convergence speed and improving bottleneck localization accuracy.


\subsection{Autocaling for Microservices}

Existing autoscaling methods for microservices can be categorized into five groups. 1) Rule-based heuristic approach. KHPA, Libra \cite{balla2020adaptive}, KHPA-A \cite{casalicchio2017auto}, and PEMA \cite{hossen2022lightweight} manage the number of microservice replicas based on resource thresholds and specific rules. However, since different microservices have varying sensitivities to specific resources,  expert knowledge is needed to support autoscaling for these various microservices. 2) Model-based approach. Microservices can be modeled to predict their status under particular configurations and workloads. Queuing theory \cite{baresi2020simulation, tong2021holistic} and graph neural network (GNN) \cite{park2021graf} are commonly used to build performance prediction models for microservices. 3) Control theory-based approach \cite{baarzi2021showar, baresi2020simulation}. Using the  control theory, SHOWAR \cite{baarzi2021showar} dynamically adjusts the microservice replicas to correct the error between monitoring metrics and thresholds. 4) Optimization-based approach. These methods \cite{yu2019microscaler, gias2019atom} make a large number of attempts to find the optimal strategy given the present resources and workloads. The key to these approaches is to reduce the decision-making scope to speed up the process. 5) RL-based Approach. Reinforcement learning (RL) has been widely used in resource management for microservices. MIRAS \cite{yang2019miras} adopts a model-based RL method for decision-making to avoid the high sampling complexity of the real environment. FIRM \cite{qiu2020firm} leverages a support vector machine (SVM) to identify the critical path in microservices and  a deep deterministic policy gradient (DDPG) algorithm to make hybrid scaling strategies for microservices along the path. RL-based methods require constant interactions with the environment during exploration and are incapable of adapting to the dynamic microservices architecture. 

In conclusion, while the aforementioned autoscaling techniques have their respective advantages, they pay little attention to performance bottlenecks. Consuming computer resources for non-bottleneck microservices will inevitably increase scaling costs and lengthen decision-making. Our method, on the other hand, focuses on locating performance bottlenecks.


\section{System Design}
 
We present PBScaler, a PB-centric autoscaling controller, to locate PBs and optimize replicas for them. As shown in Fig.~\ref{fig:structure}, PBScaler comprises three components: 1) \textit{Metric Collector}: To provide real-time insights into the applications' status, we design a metric collector that captures and integrates monitoring metrics from Prometheus\footnote{ https://prometheus.io} at fixed intervals. 2) \textit{Performance Bottleneck Analysis}: With the assistance of the \textit{metric collector}, this component performs SLO violation detection and redundancy checking to identify microservices with abnormal behavior. Next, the bottleneck localization process will be triggered to pinpoint the PBs in the abnormal microservices. 3) \textit{Scaling Decision}: This component aims to determine the optimal number of replicas for  PBs using an evolutionary algorithm. Finally, PBScaler generates configuration files with optimized strategies and commits them to the kubernetes-client\footnote{https://github.com/kubernetes-client/python}, which regulates the replica count of microservices.



\subsection{Metric Collector}

The autoscaling system relies on real-time access to metrics, such as memory usage data, system load and tail latency, to determine whether elastic scaling should be performed and how many resources should be allocated in a microservice application. Unlike trace-based monitors that require a deep understanding of the program and code injection \cite{lin2018microscope}, the \textit{Metric Collector} reports metrics based on the service mesh to minimize disruptions to business flows. As shown in Table \ref{tab:metric-labels}, PBScaler uses Prometheus and kube-state-metrics\footnote{https://github.com/kubernetes/kube-state-metrics} to gather and categorize these metrics, including response latency, invocation relationships between microservices,
% golden metrics of performance, 
resource consumption, and microservice workload. For example, \textit{container\_cpu\_usage\_seconds\_total} is a label in Prometheus, which records the Central Processing Unit (CPU) usage at the container level. We set the monitoring interval of Prometheus to five seconds and store the collected metrics data in a time-series database. The P90 tail latency of each microservice is collected and used to represent the performance of an application. The invocation relationships imply the association between microservices, which can be used to build the microservice correlation graph.

\textbf{Service Mesh}. A service mesh is an infrastructure that enables developers to add advanced features, such as observability and traffic management, to cloud applications without requiring additional code. One popular open-source service mesh implementation is Istio\footnote{https://istio.io}, designed to seamlessly integrate with Kubernetes. When a pod starts up in Kubernetes, Istio launches an envoy proxy within the pod to intercept network traffic, enabling workload balancing and monitoring.


\begin{table}[t]
	\caption{Labels of metrics collected from the monitoring tools. 
 % Istio and kube-state-metric report the service-level and container-level information, which is gathered and summarized by Prometheus.
}
 \label{tab:metric-labels}
	\begin{tabular}{ll}
		\hline
		% \multicolumn{2}{c}{\textbf{Prometheus \& kube-state-metrics \& istio}}\\
		% \midrule
        \textbf{Level} & \textbf{Label} \\
        \hline
		 &kube\_pod\_info\\
		&container\_cpu\_usage\_seconds\_total\\
		&container\_memory\_usage\_bytes\\ 
		&container\_spec\_cpu\_quota\\
		&container\_spec\_memory\_limit\_bytes\\ 
		Container Level&container\_fs\_usage\_bytes\\ 
		&container\_fs\_write\_seconds\_total\\
		&container\_fs\_read\_seconds\_total\\ &container\_network\_receive\_bytes\_total\\
		&container\_network\_transmit\_bytes\_total\\
		\hline
		&istio\_request\_duration\_milliseconds\_bucket\\
		Microservice Level&istio\_requests\_total\\
		&istio\_tcp\_received\_bytes\_total\\
		\hline
	\end{tabular}
\end{table}

\subsection{Performance Bottleneck Analysis}

\textit{Performance Bottleneck Analysis} (PBA) is a process designed to discover performance degradation and resource waste in microservice applications to infer PBs of the current problem. As stated in Section \ref{Introduction}, by accurately locating these bottlenecks, PBA can enhance the performance of autoscaling and reduce excessive resource consumption. The PBA process in PBScaler is depicted in Algorithm \ref{al:PBA}.


\subsubsection{SLO Violation Detection} 
To detect abnormalities in microservices, PBScaler uses service level objectives (SLOs) to compare with specific metrics. It is considered abnormal if a microservice has numerous SLO violations, i.e., performance degradation. As discussed in \cite{wu2020microrca, chen2016causeinfer}, detecting SLO violations is a critical step in triggering bottleneck localization. The invocation relationships collected by the \textit{Metric Collector} can be leveraged to build a microservice correlation graph $\mathcal{G}_{c}$. PBScaler inspects the P90 tail latency of all invocation edges in $\mathcal{G}_{c}$ every 15 seconds to timely detect performance degradation. If the tail latency of an invocation exceeds a predetermined threshold (such as the SLO), the invoked microservice of the invocation will be added to the set of abnormal microservices ($\mathbb{S}$), and the bottleneck localization process will be activated. To account for occasional noise in the microservice latency, the threshold is set to SLO$ \times (1 + \frac{\alpha}{2})$, where $\alpha$ is used to adjust the tolerance to noise. 



\subsubsection{Redundancy Checking}

In the absence of performance anomalies, some microservices may be allocated more resources than required. However, identifying such cases can be difficult through metrics alone, potentially leading to wasting limited hardware resources. To avoid this, it is essential to identify which microservices have allocated excess resources. PBScaler uses the rate of workload change per second of microservices to determine whether resources are redundant. This strategy is more effective than relying only on resource consumption because different microservices may have varying sensitivity to heterogeneous resources. The main idea behind redundancy checking is to employ hypothesis testing to detect whether a microservice's current workload $\bm{w_c}^i$ is significantly lower than its past workload (denoted as $\bm{w_p}^i$). The degree of significance is adjusted by the parameter $\beta$, and the hypothesis test can be formulated as:

\begin{equation}\label{eq:hypothesis-test}
	\left \{
	\begin{aligned}
		H_{0},  & \quad \bm{w_c}^i \geq \bm{w_p}^i \times \beta \\
		H_{1},  & \quad   \bm{w_c}^i < \bm{w_p}^i \times \beta.
	\end{aligned}
	\right.
\end{equation}

To perform the hypothesis test, we first fetch the current and historical workloads of the target microservices from the \textit{Metric Collector}. We then use a one-sided test to compute the p-value $P$. If $P$ does not exceed the confidence level $cl$ (which is set to 0.05 by default), we reject the null hypothesis $H_{0}$ and consider the microservice $i$ to have redundant resources. 
% The algorithm for SLO violation detection and redundancy checking is shown in s Algorithm \ref{al:detect}.


\begin{algorithm}[t]
	%\textsl{}\setstretch{1.8}
	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\caption{Performance Bottleneck Analysis}
	\label{al:PBA}
	\begin{algorithmic}[1]
		\REQUIRE SLO, \\
 microservice correlation graph $\mathcal{G}_{c}$ \\
  confidence level $cl$ \\ 
  tolerance to noise $\alpha$ \\
  degree of significance $\beta$ \\
  impact factor $\sigma$
		\ENSURE ranking list $\bm{rl}$
        % \STATE $\bm{rl} \gets$ New array
        % \STATE $\mathbb{S} \gets$ New set
        \STATE Initialize $\bm{rl}$ and abnormal microservice set $\mathbb{S}$
        
		\FOR{each $v_{i}$ in $\mathcal{G}_{c}$.nodes}
		\FOR{each $e_{j,i}$ in in-edges of $v_{i}$}
        \STATE /* SLO violation detection */
		\IF {$P90(e_{j,i}) \textgreater $ SLO $ * (1 + \alpha / 2)$}
		\STATE Store $v_{i}$ in $\mathbb{S}$
		\ENDIF
		\ENDFOR
		\ENDFOR
		\IF {$\mathbb{S}$ is empty}
        \STATE /* Redundancy checking */
        		\FOR{each $v_{i}$ in $\mathcal{G}_{c}.nodes$}
          \STATE $\bm{w_p}^i$, $\bm{w_c}^i$ $\gets$ Get workloads for microservice $i$ from the Metric Collector
        		\STATE $t$, $P$ $\gets$ $ttest$($\bm{w_c}^i$, $\bm{w_p}^i * \beta$)
        		\IF {$t< 0$ and $P < cl$}
        		\STATE Store $v_{i}$ in $\bm{rl}$
        		\ENDIF
          	\STATE Return $\bm{rl}$	\ENDFOR
		\ENDIF
  \STATE $\bm{rl}$ = $TopoRank$($\mathbb{S}$)
  \STATE Return $\bm{rl}$
	\end{algorithmic}  
\end{algorithm}

  
\begin{algorithm}[t]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\caption{TopoRank}
	\label{al:TopoRank}
	\begin{algorithmic}[1]
		\REQUIRE abnormal microservice set $\mathbb{S}$ \\ 
  impact factor $\sigma$
		\ENSURE ranking list $\bm{rl}$
  % \STATE $\bm{rl} \gets$ New array
  % \STATE $\bm{u} \gets$ New array
  % \STATE $\mathcal{G}_{a} \gets$ Build abnormal subgraph for $\mathbb{S}$
  \STATE Initialize $\bm{rl}$ and preference vector $\bm{u}$
  
  \FOR{each $v_{i}$ in $\mathcal{G}_{a}$.nodes}
		\STATE /* Calculate the topological potential for abnormal microservice $i$ */
        \STATE $\varphi(v_i) \gets 0$ 
            \FOR{each $v_{j}$ in in-edges of $v_{i}$}
            \STATE $\varphi(v_i) \gets \varphi(v_i) + m_je^{-(d_{ji} / \sigma)}$
            \ENDFOR
            \STATE $\bm{u}_{i}\gets \varphi(v_i)$ 
        \ENDFOR
      \FOR{each $v_{i}$ in $\mathcal{G}_{a}$.nodes}
    		\STATE $\bm{l}^{i}$ $\gets$
    		Get P90 tail latency series for $v_{i}$
            \STATE $\mathbb{M}^{j} \gets$ Get metric set for $v_{j}$
		\FOR{each $v_{j}$ in out-edges of $v_{i}$}
		\STATE $ \textbf{\textit{P}}_{i, j} \gets \mathop{\max}\limits_{k:{\bm{m}}^{k}\in{\mathbb{M}^{j}}}(corr({\bm{l}}^{i}, {\bm{m}}^{k}))$
		\ENDFOR
		\ENDFOR
		\STATE $scoreMap$ $\gets$ $pageRank$($\textbf{\textit{P}}$, $\bm{u}$)
		\STATE $\bm{rl}$ $\gets$ $sort$($scoreMap$)
  \STATE Return $\bm{rl}$
	\end{algorithmic}  
\end{algorithm}


% \begin{algorithm}[t]
% 	%\textsl{}\setstretch{1.8}
% 	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
% 	\renewcommand{\algorithmicensure}{\textbf{Output:}}
% 	\caption{Bottleneck Localization}
% 	\begin{algorithmic}[1]
% 		\REQUIRE abnormal subgraph $\mathcal{G}$, metric set of microservices $M$, personal array $\bm{u}$
% 		\ENSURE ranking list $\bm{rl}$
% 		% \STATE $\mathcal{G}$ $\gets$ \textbf{buildGraph($FSS$)}
% 		% \STATE $MD$ $\gets$ \textbf{getMetricsFromPrometheus($FSS$)}
% 		\FOR{each $v_{i}$ in $\mathcal{G}$.nodes}
% 		\STATE /* calculate the topological potential for abnormal microservice $i$ */
%         \STATE $\varphi(v_i)= 0$ 
%             \FOR{each $v_{j}$ in in-edges of $v_{i}$}
%             \STATE $\varphi(v_i) = \varphi(v_i) + m_je^{-(d_{ji} / \sigma)}$
%             \ENDFOR
%             \STATE $\bm{u}^{(i)}\gets \varphi(v_i)$ 
%         \ENDFOR
%       \FOR{each $v_{i}$ in $\mathcal{G}$.nodes}
%     		\STATE $\bm{l}^{(i)}$ $\gets$
%     		\textbf{getP90Latency}($v_{i}$)
% 		\FOR{each $v_{j}$ in out-edges of $v_{i}$}
% 		\STATE $ \textbf{\textit{P}}_{i, j} = \mathop{\max}_{k:{\bm{m}}^{(j)}_{k}\in{M_j}}(corr({\bm{l}}^{(i)}, {\bm{m}}^{(j)}_{k}))$
% 		\ENDFOR
% 		\ENDFOR
% 		\STATE $scoreMap$ $\gets$ \textbf{pageRank}($\textbf{\textit{P}}$, $\bm{u}$)
% 		\STATE $\bm{rl}$ $\gets$ \textbf{sort}($scoreMap$)
% 	\end{algorithmic}
%     \label{al:rca}
% \end{algorithm}


\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{fig/abnomal_propagate.pdf}
  \caption{Example of anomaly propagation in microservices.}
\label{fig:abnomal_propagate}
\end{figure}

\subsubsection{Bottleneck Localization}
Because of the complex interactions in a microservice application \cite{zhou2019latent}, not every abnormal microservice needs to be scaled up. For example, Fig. \ref{fig:abnomal_propagate} illustrates how the performance degradation of a bottleneck microservice (e.g., \textit{Product}) can propagate to its upstream microservices (e.g., \textit{Recommend}, \textit{Frontend}, and \textit{Checkout}) along the invocation chains, even if the upstream microservices are not overloaded. Therefore, only the bottleneck microservice must be scaled up while the other abnormal microservices are merely implicated. To pinpoint the bottleneck microservice, we introduce the concept of anomaly potential, which aggregates the anomaly impact of all microservices in a given position. The anomaly potential of PB is usually high because it is surrounded by many abnormal microservices that are affected by it. We design a novel bottleneck localization algorithm, TopoRank, which introduces the topological potential theory (TPT) in random walks to calculate the scores for all abnormal microservices and finally outputs a ranking list ($\bm{rl}$).
% which focuses on anomalous microservice detection to output a ranking list of the abnormal microservices. 
The microservices with the highest scores in the $\bm{rl}$ can be recognized as the PBs.

TPT, which originated from the concept of "field" in physics, 
% Taking the gravitational field as an example, the gravitational force on any point is proportional to the mass of the field source and inversely proportional to the square of the distance from the field source. 
 has been widely used in various works \cite{hu2010topological,mao2020tps} to measure the mutual influence between nodes in complex networks. Since the microservice correlation graph can also be viewed as a complex network, we use TPT to evaluate the anomaly potential of microservices. Specifically, we have observed that in a microservice correlation graph $\mathcal{G}_{c}$, the microservices closer to the PBs, i.e., those with fewer hops, are more likely to be abnormal, as they often have frequent direct or indirect invocations with the PBs. Based on this observation, we evaluate the anomaly potential of microservices using the TPT. To do this, we first extract the abnormal subgraph $\mathcal{G}_{a}$ by identifying the abnormal microservices and their invocation relationships in $\mathcal{G}_{c}$. We then calculate the topological potential for microservice $v_i$ in the abnormal subgraph $\mathcal{G}_{a}$ using the TPT.

\begin{equation}
    \varphi(v_i) = \sum_{j=1}^N{m_j e^{-(d_{ji} / \sigma)}},
\end{equation}
where $N$ is the number of upstream microservices of $v_i$ and $m_j$ represents the anomaly degree of $v_j$. PBScaler defines the anomaly degree as the number of SLO violations for a microservice in a time window. $d_{ji}$ denotes the minimum number of hops required from $v_j$ to $v_i$. We use the impact factor $\sigma$ to control the influence range of a microservice. 

% The basic idea of bottleneck localization originates from two observations. First, abnormal microservices tend to be clustered on the microservice correlation graph. PBScaler distinguishes all abnormal microservices from the microservice correlation graph to build the abnormal subgraph ($\mathcal{G}$). For a microservice node $v_i$, PBScaler defines $AAN(v_i)$ as the adjacent abnormal node set and $NHAN(v_i)$ as the next hop abnormal node set. In addition, each microservice will be assigned an anomaly score $AS(v_i )$, which denotes the number of SLO violations within a period. The adjacent abnormal clustering score $iScore$ and one-hop abnormal clustering score $xScore$ of microservice $v_i$ can be calculated as:

% \vspace{-3mm}
% \begin{equation}
% 	iScore(v_i) = \frac{\sum_{j=1}^{N}AS(v_j)}{Degree(v_i)},  \quad v_j \in AAN(v_i),
% \end{equation}

% \begin{equation}
% 	xScore(v_i) = \frac{\sum_{j=1}^{N}AS(v_j)}{Degree(v_i)},  \quad v_j \in NHAN(v_i).
% \end{equation}
% PBScaler defines $ixScore$, the sum of $iScore$ and $xScore$ as the final abnormal clustering score:

% \vspace{-3mm}
% \begin{equation}
% 	ixScore(v_i) = iScore(v_i) + xScore(v_i).
% \end{equation}

However, microservices with high topological potential values are not necessarily PBs, since anomalies are usually propagated along the microservice correlation graph. Hence, relying solely on the TPT is insufficient for diagnosing PBs. To address this issue, PBScaler incorporates the Personalized PageRank algorithm \cite{jeh2003scaling} to reverse the anomaly propagation on the abnormal subgraph $\mathcal{G}_{a}$ and locate PBs.
% and finally outputs a Personalized PageRank vector ($\bm{v}$), which can be regarded as the probability that each microservice is diagnosed as the PB. 
Let $\textbf{\textit{P}}$ be the transition matrix of $\mathcal{G}_{a}$ and $\textbf{\textit{P}}_{i, j}$ be the probability of anomaly tracking from $v_i$ to its downstream node $v_j$. Given $v_i$ with out-degree $d$, the standard Personalized PageRank algorithm sets $\textit{P}_{i, j}$ as:
\begin{equation}
\textbf{\textit{P}}_{i, j} = \frac{1}{d}, 
\end{equation}
which means that the algorithm is not biased toward any downstream microservice. However, this definition fails to consider the association between the downstream microservices and the anomaly of the current microservice. Consequently, PBScaler adapts the calculation by giving more attention to the downstream microservices whose metrics are more relevant to upstream response time. For each microservice $v_i$, PBScaler collects a tail latency series (${\bm{l}}^{i}$) and a set of metrics arrays $\mathbb{M}^{i} = \{{\bm{m}}^{1},{\bm{m}}^{2},\cdots,{\bm{m}}^{k}\}$, where ${\bm{m}}^{k}$ can be seen as a fluctuating array of a metric ($e.g.$, memory usage) in a given time window. PBScaler defines that $\textbf{\textit{P}}_{i, j}$ depends on the maximum value of the Pearson correlation coefficient between ${\bm{l}}^{i}$ and metric arrays in $\mathbb{M}^{j}$:

\begin{equation}
\textbf{\textit{P}}_{i, j} = \mathop{\max}\limits_{k:{\bm{m}}^{k}\in{\mathbb{M}^{j}}}(corr({\bm{l}}^{i}, {\bm{m}}^{k})).
 % \quad {\bm{m}}^{(j)}_{k} \in M_j.
\end{equation}

The Personalized PageRank algorithm determines the popularity of each node by randomly walking on the directed graph. However, some nodes may never point to others, causing the scores of all nodes to tend toward zero after many iterations of the random walk. To avoid falling into this "trap," a damping factor $\delta$ is applied, which allows the algorithm to jump out from these nodes according to a predefined rule. Typically $\delta$ is set to 0.15. The Personalized PageRank is represented as follows:

\begin{equation}
	\bm{v} = (1 - \delta)  \cdot \textbf{\textit{P}} \bm{v} + \delta  \cdot \bm{u},
\end{equation}
where $\bm{v}$ represents the probability that each microservice node is diagnosed as a PB. The preference vector $\bm{u}$  serves as the personalized rule to guide the algorithm to leap from the trap. The value of $\bm{u}$ is determined by the anomaly potential of each node. The nodes with greater anomaly potential are preferred as starting points for the algorithm. The equation of the $k$-th iteration can be represented as:

\begin{equation}
	{\bm{v}}^{(k)} = (1 - \delta)  \cdot \textbf{\textit{P}} {\bm{v}}^{(k-1)} + \delta  \cdot \bm{u}.
\end{equation}

After multiple rounds of iterations,  $\bm{v}$ gradually converges. PBScaler then sorts the final results and produces the ranking list $\bm{rl}$. The top-$k$ microservices with the highest ranking list scores can be recognized as PBs. The whole process of TopoRank is depicted as Algorithm \ref{al:TopoRank}.

\subsection{Scaling Decision}

Given the PBs identified by the \textit{Performance Bottleneck Analysis}, the replicas for the PBs will be scaled to minimize the application's resource consumption while ensuring the end-to-end latency of microservices meets the SLO. Although abundant replicas can alleviate the performance degradation problem, they also consume a significant amount of resources. Consequently, it is essential to maintain a balance between performance guarantees and resource consumption. The process of \textit{Scaling Decision} will be modeled as a constrained optimization problem to achieve this balance. 
% Another important module in \textit{Scaling Decision} is to evaluate the strategies generated by the optimization algorithm, taking into account both SLO violations and resource consumption.

\subsubsection{Constrained Optimization Model}
\label{sec:Constrained_Optimization_Model}

The autoscaling optimization in our scenario seeks to identify an allocation schema that allocates a variable number of replicas for each PB. Given $n$ PBs that require scaling, we define a strategy as a set $\mathbb{X} = \{x_1, x_2, \cdots, x_n\}$, where $x_i$ denotes the number of replicas allocated for PB $i$. Before the optimization, the initial number of replicas for PBs can be expressed as $\mathbb{C} = \{c_1, c_2, \cdots, c_n\}$. It should be noted that the replicas constraint in PBScaler should be defined separately for scaled-down and scaled-up processes. During the scaled-up process, we limit the number of replicas for PBs as follows:

\begin{equation}
\begin{aligned}
\label{eq:constraint-scaled-up}
	s.t. \quad x_i &\geq c_i + 1 , \forall x_i \in \mathbb{X}, \forall c_i \in \mathbb{C},\\
 x_i &\leq c^{max}, \forall x_i \in \mathbb{X},
 \end{aligned}
\end{equation}
where $c^{max}$ represents the maximum number of replicas that a
microservice can scale to, given limited server resources. The constraint of the number of replicas during the scaled-down process can be expressed as:

\begin{equation}
\label{eq:constraint-scaled-down}
\begin{aligned}
	s.t. \quad x_i &\geq \max(c_i - \gamma, 1) , \forall x_i \in \mathbb{X}, \forall c_i \in \mathbb{C},\\
 x_i &\leq c_i, \forall x_i \in \mathbb{X},\forall c_i \in \mathbb{C}.
 \end{aligned}
\end{equation}
In Eq. (\ref{eq:constraint-scaled-down}), $\gamma$ (with the default value of two) denotes the maximum number of replicas reductions. This limit is reasonable since  reducing the number of microservice replicas drastically can cause a short latency peak, as observed in experiments. 

The goal of the \textit{Scaling Descision} is to minimize the applications resource consumption while maintaining its performance. Application performance is usually expressed by SLO violations that users are more concerned about. Therefore, the application performance reward can be detailed as:

\begin{equation}\label{eq:R1}
	R_1 = \left \{
	\begin{aligned}
		0,  &\quad SLO\; violation, \\
		1,  &\quad w/o \; SLO \; violation.
	\end{aligned}
	\right.
\end{equation}
During the  optimization process, the application's resource consumption, such as CPU and memory usage,  is unpredictable. To conservatively estimate resource consumption, we consider the ratio of PB replicas to the maximum number of allocatable replicas,  rather than calculating the cost of CPU and memory. We calculate the resource reward as:
\begin{equation}\label{eq:R2}
\begin{aligned}
	R_2 = 1 - \frac{\sum_{i=1} ^{n}{x_i}}{c^{max}\times n}.
	\end{aligned}
\end{equation}
Our objective is to guarantee performance while minimizing resource consumption. We leverage a weighted linear combination (WLC) method to balance the two objectives. The final optimization objective is defined as:
\begin{equation}\label{eq:R}
\begin{aligned}
	\max_{\mathbb{X}} (\lambda \cdot R_1(\mathbb{X}) + (1-\lambda) \cdot R_2(\mathbb{X})),
	\end{aligned}
\end{equation}
where $\lambda \in [0,1]$. We set $\lambda$ as a parameter to balance the application performance and resource consumption.


\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{fig/GA_process.pdf}
  \caption{Illustration of the autoscaling optimization process.}
  \label{fig:GA_process}
\end{figure}


\subsubsection{SLO Violation Predictor}
To calculate the performance reward $R_1$, evaluating whether a strategy will cause the SLO violation in online applications is necessary.
% There are two techniques to obtain the SLO violation results of the strategies generated by the optimization algorithm. 
A simple way is to execute candidate strategies directly in online applications and wait for feedback from the monitoring system. However, oscillations caused by frequent scaling in online applications will be inevitable. An alternative method is to train an evaluation model with historical metric data, which can simulate the feedback from online applications. Without interacting with  online applications, this model predicts application performance based on the current application state. 

We use a vector $\bm{r}$ to denote the number of replicas for each microservice  after executing the scaling strategy $\mathbb{X}$. $\bm{w}$ is a vector that denotes the current workload for each microservice. Because of the low time cost of bottleneck-aware optimization, it is reasonable to hypothesize that $\bm{w}$ will not change significantly during this period (see Section \ref{sec:PE}). Given the application state represented by workload $\bm{w}$ and replicas $\bm{r}$ of all microservices, an SLO violation predictor can be designed as:

\begin{equation}\label{eq:SLO-predictor}
	\psi(\bm{r}, \bm{w}) = \left \{
	\begin{aligned}
		0,  &\quad SLO\; violation, \\
		1,  &\quad w/o \; SLO \; violation,
	\end{aligned}
	\right.
\end{equation}
where $\psi$ is a binary classification model. Details of selecting an appropriate classification model will be discussed in Section \ref{sec:exp-validation}. The historical metric data used for training can be generated using either a classical scaling method (the Kubernetes autoscaler by default) or a stochastic method. We deployed an open-source microservice system on three nodes (with a total of 44 CPU cores and 220 GB of RAM) and performed elastic scaling. Prometheus gathered each microservice's workload and P90 tail latency at regular time intervals. By comparing the tail latency of front-end microservice with the SLO, monitoring data for each time interval can be easily labeled.


\begin{algorithm}[t]
	%\textsl{}\setstretch{1.8}
	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\caption{GA-based Autoscaling Optimization}
	\begin{algorithmic}[1]
		\REQUIRE the number of iteration $N$, the size of population $M$, the size of elites $Z$, the probability of crossover $p_c$, the probability of mutation $p_m$
		\ENSURE the best scaling strategy $\mathbb{X}_{best}$
		% \STATE $\mathcal{G}$ $\gets$ \textbf{buildGraph($FSS$)}
		% \STATE $MD$ $\gets$ \textbf{getMetricsFromPrometheus($FSS$)}
        \STATE $population$ $\gets$ Generate a population with $N$ chromosomes 
        \STATE $fs \gets$ $fitness$($population$)
        \STATE $elites \gets$ Sort and select $Z$ chromosomes with higher fitness from $fs$
        \STATE $\mathbb{X}_{best} \gets elites[0]$
		\WHILE{$i < N$}
        % \STATE /* calculate fitness for all chromosome in $population$ */
        % \STATE /* select top-(M-1) in $population$ */
        \STATE $parents \gets$ Select $(M-Z)$ chromosomes from $fs$
        % \STATE /* generate offspring by crossover */
        \STATE $offspring \gets$ Recombine $parents$ with probability of $p_c$
        % \STATE /* generate offspring by mutation */
        \STATE $offspring \gets$ Mutate $offspring$ with probability of $p_m$
        % \IF{$p < p_c$}
            
        %     \STATE $CH_1,CH_2 \gets crossover(P_1, P_2)$
        % \ELSE
        %     \STATE $CH_1,CH_2 \gets P_1,P_2$
        % \ENDIF
        % \STATE generate random number $p \in (0,1)$
        % \IF{$p < p_m$}
        %     \STATE /* generate chromosomes by mutation */
        %     \STATE $CH_1,CH_2 \gets mutate(CH_1), mutate(CH_2)$
        % \ENDIF
        \STATE $population \gets (elites + offspring)$
        \STATE $fs \gets fitness$ ($population$)
        \STATE $elites \gets$ Sort and select $Z$ chromosomes with higher fitness from $fs$
        \STATE $f_x, f_e \gets$ $fitness$($\mathbb{X}_{best}$), $fitness$($elites[0]$)
        \IF{$f_x < f_e$}
            \STATE $\mathbb{X}_{best} \gets elites[0]$
        \ENDIF
        \STATE Update $i \gets i + 1$
        \ENDWHILE
	\end{algorithmic}
    \label{al:ga}
\end{algorithm}

\subsubsection{Autocaling Optimization}

As mentioned in Section \ref{sec:Constrained_Optimization_Model}, the tradeoff between performance and resource consumption can be modeled as a constrained optimization problem. To find a near-optimal strategy, PBScaler employs a genetic algorithm (GA) to generate and optimize scaling strategies that reduce resource consumption while meeting SLO requirements. By emulating natural selection in evolution, the GA improves the superior offspring while eliminating the inferior ones. Initially, the GA performs a random search to initialize a population of chromosomes in the strategy space, with each chromosome indicating a potential strategy for the optimization problem. Next, in each iteration, elite chromosomes with high fitness, called elites, will be selected for crossover or mutation to produce the next generation.

The autoscaling optimization in our scenario seeks to identify a scaling strategy that allocates a variable number of replicas for each PB. The process of autoscaling optimization is illustrated in Fig. \ref{fig:GA_process}. In the beginning, PBScaler obtains each microservice's current number of replicas $\bm{r}$ and workload $\bm{w}$. After the \textit{Performance Bottleneck Analysis}, PBScaler identifies the PBs from $\bm{r}$ and filters out them to get $\bm{r^{\prime}}$. Then, the population of strategies for PBs is generated by the Decision Maker. Since the number of  microservices to be scaled influences the speed and effect of the optimization algorithm (Section \ref{sec:exp-validation}), PBScaler assumes that only PBs need to be elastically scaled. In other words, the number of replicas in $\bm{r^{\prime}}$ will remain unchanged. The SLO violation predictor is responsible for evaluating the generated strategies. It should be noted that the strategy is merged with $\bm{r^{\prime}}$ and input to the SLO violation predictor together with $\bm{w}$. With the help of GA, the superior strategy $\mathbb{X}_{best}$ is selected  and then merged with $\bm{r^{\prime}}$ to generate the final decision.

% \vspace{-2mm}
% \begin{equation}\label{eq:lb}
% 	lb_i = 
% 	\begin{cases}
% 		r_i + 1,  &\quad scale\;up, \\
% 		max(r_i-\gamma, 1),  &\quad scale\;down,
% 	\end{cases}
% \end{equation}

% \begin{equation}\label{eq:ub}
% 	ub_i =
% 	\begin{cases}
% 		C^{max},  &\quad \quad \quad \quad scale\;up, \\
% 		r_i,  &\quad \quad \quad \quad scale\;down,
% 	\end{cases}
% \end{equation}
% where $r_i$ is the current number of replicas for PB $i$ and $C^{max}$ represents the maximum number of replicas that a microservice can scale to. According to the findings of experiments, drastically reducing the number of microservice replicas causes a short latency peak, so we set $\gamma$ (the default value is two) to limit the maximum number of microservice reductions.

In the optimization phase, the Decision Maker generates and improves the scaling strategy for PB using the GA, as described in Algorithm \ref{al:ga}. After randomly generating a population within the strategy scope of each PB (Line 1), the Decision Maker estimates the fitness of each strategy based on  Eq.  (\ref{eq:R}) and stores the elites (Lines 2-3). In each iteration, the GA uses a tournament-based selection operator to pick out outstanding $parents$ (Line 6). New offspring are generated through recombination and mutation (Lines 7-8) using a two-point crossover operator and a binary-chromosome mutation operator. By simulating natural selection, new offspring and parent elites form a new population that enters the next iteration (Line 9). The Decision Maker returns the scaling decision $\mathbb{X}_{best}$ with the highest fitness when the GA reaches the specified number of iterations.
% The scope of a strategy is determined by specifying the lower bound and upper bound of a strategy for PB, which depends on the current replica number of the PB and the scaling direction. Thus, the lower bound and upper bound of the PB can be defined based on the constraint in Eq. (\ref{eq:constraint-scaled-up}) and Eq.  (\ref{eq:constraint-scaled-down}). 

% The Decision Maker can also estimate the fitness function of each strategy based on the predicted SLO violation and resource cost. 
% In reality, the CPU and memory usage of the GA-generated solutions are unpredictable. As a result, instead of calculating the cost of CPU and memory, we consider the ratio of the number of microservice replicas to the maximum number of allocatable replicas as resource consumption. $m$ is the number of microservices in the application. The fitness function is calculated as follows:
% \begin{equation}\label{eq:cost_resource}
%     R_{1} = \psi(\bm{r}, \bm{w}),\quad
%     R_{2} = 1 - \frac{\Vert \bm{r^{\prime}} \Vert_{1} + \sum_{i=1} ^{n}{x_i}}{C^{max}\times m},
% \end{equation}
% \begin{equation}\label{eq:fitness}
% 	fitness(X) = \lambda R_1  + (1-\lambda) R_2,
% \end{equation}
% where $R_1$ and $R_2$ represent the SLO violation and resource cost, respectively. We set $\lambda$ to balance the SLO violation cost and resource consumption. 




\begin{figure}[t]
\centering{
 \subfigure[ Wiki ]{
 \includegraphics[width=0.31\linewidth]{fig/workloads_1.pdf} } 
 \subfigure[ EW1 ]{  \includegraphics[width=0.31\linewidth]{fig/workloads_2.pdf}}
 \subfigure[ EW2 ]{  \includegraphics[width=0.31\linewidth]{fig/workloads_3.pdf}}
 \subfigure[ EW3 ]{  \includegraphics[width=0.31\linewidth]{fig/workloads_4.pdf}}
 \subfigure[ EW4 ]{  \includegraphics[width=0.31\linewidth]{fig/workloads_5.pdf}}
 \subfigure[ EW5 ]{  \includegraphics[width=0.31\linewidth]{fig/workloads_6.pdf}}
}
\caption{The fluctuation of one-hour real-world workloads (Wiki) and twenty minutes emulated workloads (EW).}
\vspace{-4mm}
\label{fig:workloads}
\end{figure}


\section{EVALUATIONS}

In this section, we present the details of experimental scenarios for autoscaling, including a comparison of PBScaler with several state-of-the-art autoscaling algorithms from academia and industry.

\subsection{Experimental Setup}
\subsubsection{Microservice Platform}
Experiments were carried out in our private cloud cluster, consisting of three physical computers (one master node and two worker nodes) with a total of 44 Intel 2.40 GHz CPU cores and 220 GB of RAM. To evaluate autoscaling, we selected two open-source microservice applications as benchmarks: a) Online Boutique\footnote{https://github.com/GoogleCloudPlatform/microservices-demo}, a Web-based E-commerce demo application developed by Google. The system implements basic functions such as browsing products, adding items to shopping carts, and payment processing through the collaboration of ten stateless microservices and a Redis cache. b) Train Ticket\footnote{https://github.com/FudanSELab/train-ticket}:  a large-scale,  open-source microservice system developed by Fudan University. With more than 40 microservices and the usage of MongoDB and MySQL for data storage, Train Ticket can satisfy a variety of workflows, such as online ticket browsing,  booking, and purchasing. Because of cluster resource constraints, we limited each microservice to no more than eight replicas. The source code is available on Github\footnote{https://github.com/WHU-AISE/PBScaler}
% Appendix \ref{apx:software-version} shows the software information of our experimental environment.

\subsubsection{Workload}
We evaluated the effectiveness of PBScaler under various traffic scenarios, using a real-world Wikipedia workload from the Wiki-Pageviews \cite{wiki-dataset} on March 16, 2015, and five emulated workloads (EW1 $ \sim $ EW5), inspired by the experiments conducted by Abdullah \textit{et al.} in \cite{abdullah2020burst}. We compressed the real-world workload to one hour and scaled it to an appropriate level for our cluster. The five emulated workloads exhibited various patterns, such as single peak, multiple peaks, rising, and dropping, and were limited to a duration of twenty minutes. Fig. \ref{fig:workloads} depicts the fluctuation of these workloads.

\subsubsection{Baseline Methods}
We compare PBScaler with several state-of-the-art microservice autoscaling methods from academia and industry, which perform dynamic horizontal scaling of microservices from the perspectives of static thresholds, control theory, and black-box optimization. 

\begin{itemize}
\item \textbf{Kubernetes Horizontal Pod Autoscaling (KHPA)}: It is the default horizontal scaling scheme of Kubernetes. By customizing a threshold $T$ for a certain resource $R$ (CPU usage as the default) and aggregating the resource usage $U_i^R$ from all replicas of a microservice, KHPA defines the target number of replicas as $n = \lceil \sum_{i \in ActivePods}{U_i^R} \, / \, T  \rceil$. 

\item \textbf{MicroScaler} \cite{yu2019microscaler}: It is an autoscaling tool that uses a black-box optimization algorithm to determine the optimal number of replicas for a microservice. MicroScaler calculates the microservice's P90/P50 for classification and then performs four iterations of Bayesian Optimization to make a scaling decision. 
\item \textbf{SHOWAR} \cite{baarzi2021showar}: It is a hybrid autoscaling technology. We reproduced the horizontal scaling part in SHOWAR, which uses the PID control theory to gradually bring the observed metric close to the user-specified threshold. In our implementation, we replaced the run queue latency with the more common P90 latency since the former requires an additional eBPF tool.

\end{itemize}


\definecolor{mygray}{gray}{.9}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\grey}[1]{\textcolor{gray}{#1}}
\begin{table*}[t]
\small
\caption{Performance of state-of-the-art methods and PBScaler under real and emulated workloads. SLO violation rate and cost are reported.}
\label{tab:all-performance}

\setlength{\tabcolsep}{3mm}{
\begin{tabular}{l||cccccc||cccccc}
\hline
\multicolumn{1}{c||}{\multirow{2}{*}{Methods}} & \multicolumn{6}{c||}{Online Boutique} & \multicolumn{6}{c}{Train Ticket} \\ \cline{2-13} 
\multicolumn{1}{c||}{} & \multicolumn{1}{c}{Wiki} & \multicolumn{1}{c}{EW1} & \multicolumn{1}{l}{EW2} & \multicolumn{1}{c}{EW3} & \multicolumn{1}{l}{EW4} & \multicolumn{1}{c||}{EW5} & \multicolumn{1}{c}{Wiki} & \multicolumn{1}{c}{EW1} & \multicolumn{1}{l}{EW2} & \multicolumn{1}{c}{EW3} & \multicolumn{1}{l}{EW4} & \multicolumn{1}{c}{EW5} \\ \cline{1-1}
\hline
\rowcolor{mygray}\multicolumn{13}{l}{SLO violation rate (\%)} \\

\hline
None & 79.64 & 57.26 & 67.22 & 70.12 & 56.85 & 39.00 & 94.87 & 48.37 & 53.11 & 57.54 & 35.88 & 37.89 \\
KHPA & \textbf{4.86} & 13.45 & 37.08 & 31.95 & 20.00 & 25.10 & 10.48 & 37.50 & 40.09 & 30.46 & 29.72 & 30.90 \\
MicroScaler & 8.18 & 17.43 & 17.45 & 31.54 & 22.41 & 18.26 & 46.70 & 29.61 & 30.48 & 47.57 & 50.29 & 42.16 \\
SHOWAR & 13.74 & 13.33 & \textbf{10.92} & 27.39 & 12.08 & 25.00 & 9.39 & 20.14 & 22.92 & 19.23 & 25.66 & 21.04 \\
PBScaler & 5.69 & \textbf{7.88} & 12.45 & \textbf{8.30} & \textbf{11.62} & \textbf{8.71} & \textbf{5.62} & \textbf{15.00} & \textbf{19.40} & \textbf{18.14} & \textbf{16.24} & \textbf{14.22} \\
\hline
\rowcolor{mygray}\multicolumn{13}{l}{Cost (\$)} \\
\hline
None & 1.93 & 0.50 & 0.62 & 0.65 & 0.61 & 0.64 & 11.62 & 3.03 & 3.06 & 3.25 & 3.15 & 3.39 \\
KHPA & 3.48 & 1.03 & 1.06 & 1.08 & 1.11 & 1.09 & 18.29 & 5.50 & 5.71 & 5.66 & 5.84 & 5.27 \\
MicroScaler & 3.09 & 0.79 & 0.72 & \textbf{0.71} & 0.76 & \textbf{0.68} & 15.44 & 3.76 & 3.52 & 4.70 & 4.86 & 4.46 \\
SHOWAR & \textbf{2.49} & 0.71 & 0.83 & 0.85 & 0.79 & 0.69 & \textbf{13.64} & 3.86 & 3.82 & 4.21 & 4.09 & 3.56 \\
PBScaler & 2.57 & \textbf{0.68} & \textbf{0.69} & 0.72 & \textbf{0.63} & 0.69 & 14.02 & \textbf{3.57} & \textbf{3.30} & \textbf{3.64} & \textbf{3.69} & \textbf{3.14}\\
\hline
\end{tabular}}
\end{table*}



\begin{figure*}[t]
\centering{
\subfigure[ EW1 ]{
\includegraphics[width=0.3\linewidth]{fig/EW1.pdf}
}
\subfigure[ EW2 ]{
\includegraphics[width=0.3\linewidth]{fig/EW2.pdf}
}
\subfigure[ EW3 ]{
\includegraphics[width=0.3\linewidth]{fig/EW3.pdf}
}
\subfigure[ EW4 ]{
\includegraphics[width=0.3\linewidth]{fig/EW4.pdf}
}
\subfigure[ EW5 ]{
\includegraphics[width=0.3\linewidth]{fig/EW5.pdf}
}
\subfigure[ Wiki ]{
\includegraphics[width=0.3\linewidth]{fig/Wiki.pdf}
}
}
\caption{The latency distribution for different methods under real and emulated workloads.}
\label{fig:latency-distribution}
\end{figure*}

\subsubsection{Experimental Parameters and Evaluation Criteria}
In our experiments, we fixed the collection interval of Prometheus to five seconds. With the increase in experiment time and workloads, the data volume required by stateful microservices like MongoDB will also grow. Eventually, the data volume will exceed the available memory, necessitating the use of disk storage. This transition can cause performance degradation that cannot be remedied through autoscaling. Hence, we limit workload testing to stateless traces. The SLO values for the Online Boutique and the Train Ticket were set to 500 ms and 200 ms, respectively. In the SLO violation detection and redundancy checking module, PBScaler first sets the action boundary $\alpha$ to 0.2 to reduce noise interference, as done in SHOWAR. Then, we 
empirically set the degree of significance $\beta$ to 0.9 to control the workload level that triggers scaling. For bottleneck localization, the impact factor $\sigma$ of the topological potential is set to 1, and the top-$k$ ($k$ =2) microservices with the highest score in $\bm{rl}$ will be considered PBs.


We choose the SLO violation rate, resource consumption, and response time to evaluate the performance of autoscaling methods. An autoscaling approach is considered more effective if it can reduce response time, SLO violation rate, and resource consumption. We define the SLO violation rate as the percentage of the end-to-end P90 tail latency that exceeds the SLO. Resource consumption is calculated following the method presented in \cite{ding2021copa}, where the CPU price is 0.00003334\$ (vCPU/s) and the memory price is 0.00001389\$ (G/s). The total resource consumption is obtained by summing the cost of memory and CPU.




\begin{table}[t]\tiny
\caption{Time cost of four modules in PBScaler.}
\label{tab:time-cost}
\resizebox{\linewidth}{!}{ 
\begin{tabular}{l||c||c}
\hline
\multicolumn{1}{c||}{\textbf{Modules}} & \textbf{\begin{tabular}[c]{@{}c@{}}Online \\ Boutique\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Train \\ Ticket\end{tabular}} \\
\hline
SLO Violation Detection & 0.29s & 1.03s \\
Redundancy Checking & 0.11s & 0.15s \\
PBA & 0.79s & 3.1s \\
Decision Maker & 3.36s & 3.58s \\
\hline
\end{tabular}
}
\vspace{-2mm}
\end{table}


\subsection{Performance Evaluation}
\label{sec:PE}

Table \ref{tab:all-performance} compares the SLO violation rates and resource costs for the four autoscaling methods in two microservice applications with different workloads. The None method is used as a reference and performs no autoscaling operation. Its results are presented in grey and are excluded from the comparison. 

In general, PBScaler outperforms the competing approaches in reducing SLO violations and minimizing resource overhead under six workloads in both microservices systems. In particular, the SLO violation rate of PBScaler in Train Ticket is,  on average,  4.96\% lower than that of the baseline methods, while the resource cost is reduced by an average of 0.24\$. These results show that PBScaler can perform elastic scaling for bottleneck microservices in large-scale microservice systems quickly and precisely, thereby reducing SLO violations and saving resources. Regarding the six workloads in Online Boutique, PBScaler also achieves the lowest SLO violations in four of them and minimizes resource consumption in three emulated workloads.

% \begin{figure}[t]
% \centering{
% \includegraphics[width=0.8\linewidth]{fig/latency-distribution-boutique.pdf}
% }
% \caption{The latency distribution for different methods in Online Boutique.}
% \label{fig:latency-distribution-boutique}
% \end{figure}

% \begin{figure}[t]
% \centering{
% \includegraphics[width=0.8\linewidth]{fig/latency-distribution-trainticket.pdf}
% }
% \caption{The latency distribution for different methods in Train Ticket.}
% \label{fig:latency-distribution-trainticket}
% \end{figure}


Fig. \ref{fig:latency-distribution} depicts the box plots of latency distribution for different methods under six workloads,  exploring the impact of each method on the performance of the microservice system. It can be seen that the majority of the autoscaling methods can keep the median of the latency distribution below the red dotted line (SLO). However, only PBScaler goes a step further to reduce the third quartile significantly below the SLO for all workloads.


To evaluate the time cost of using PBScaler for elastic scaling, the average time required by each module in PBScaler is collected and counted. As reported in Table \ref{tab:time-cost}, the total time cost of all PBScaler modules in Online Boutique is less than one monitoring interval (i.e., 5s), while the same metric for Train Ticket is less than two monitoring intervals. Thanks to the PBA that narrows the decision-making scope, the time cost of the Decision Maker does not increase much (no more than 6.6\%) when the application is switched from Online Boutique to Train Ticket, despite the increased number of microservices.  However, we recognize the limitation that the time consumption of PBA quickly rises as the microservice scale grows, which will be our future work.




\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{fig/pr_map.pdf}
  \caption{Performance comparision on Online Boutique(OB) and Train Ticket(TT).}
  \label{fig:PBA-performance}
\end{figure}

\begin{figure}[t]
\centering{
\subfigure[Online Boutique]{\includegraphics[width=0.48\linewidth]{fig/iter_boutique.pdf} } 
\subfigure[Train Ticket]{\includegraphics[width=0.48\linewidth]{fig/iter-trainticket.pdf} } 
}
\caption{The iterative process of the genetic algorithm under Online Boutique and Train Ticket.}
\label{fig:fitness-iteration-train}
\end{figure}


\begin{table}[t]
\caption{Precision and Recall of four ML models for SLO violation prediction}
\label{tab:PR&Recall}
\begin{tabular}{l||c||c||c||c}
\hline
             & SVM   & Decision Tree & Random Forest  & MLP   \\
\hline
Precision(A) & 0.819 & 0.891         & \textbf{0.919} & 0.799 \\
Recall(A)    & 0.961 & 0.918         & \textbf{0963}  & 0.930 \\
Precision(B) & 0.865 & 0.927         & \textbf{0.956} & 0.831 \\
Recall(B)    & 0.915 & 0.941         & \textbf{0.969} & 0.907\\
\hline
\end{tabular}
\end{table}

\begin{figure}[t]
\centering{
 \subfigure[ Replicas fluctuation]{\includegraphics[width=0.48\linewidth]{fig/wave-compare.pdf} } 
 \subfigure[ Latency fluctuation]{  \includegraphics[width=0.48\linewidth]{fig/latency-compare.pdf}}
}
\caption{The replica and latency fluctuation of PBScaler and MicroScaler under the Wiki workload.}
\label{fig:burst-wave-comparision}
\end{figure}

\subsection{Effectiveness Analysis of Components}
\label{sec:exp-validation}

% The primary contributions of the proposed method are incorporating bottleneck localization into elastic scaling and using the SLO violation predictor to steer optimization. We evaluate these two components in PBScaler separately.

\subsubsection{Performance Comparision of Bottleneck Localization}
To evaluate whether the TopoRank algorithm can effectively locate PBs caused by burst workloads, we injected exceptions, such as CPU overload, memory overflow, and network congestion, into Online Boutique and Train Ticket through Chaos Mesh. These exceptions are typically caused by high-workload conditions. The TopoRank algorithm was used to analyze the metrics and identify performance bottlenecks for these exceptions. The localization results were then compared to MicroRCA \cite{wu2020microrca}, a baseline method for microservice root cause analysis. $AC@k$ measures the accuracy of the real PBs in the top-$k$ results, and $Avg@k$ is the average accuracy at the top-$k$ results. These metrics can be calculated as follows.

\vspace{-2mm}
\begin{equation}\label{eq:PR@N}
	AC@k = \frac{1}{\lvert A \rvert} \sum \limits_{a \in A} { \frac{\lvert RT@k \cap PBs \rvert}{min(k, \lvert PBs \rvert)}},
\end{equation}

\begin{equation}\label{eq:MAP}
	Avg@k = \frac{1}{\lvert A \rvert} \sum \limits_{a \in A} {\sum \limits_{k =1}^{\lvert A \rvert} AC@k},
\end{equation}
where $A$ represents the set of exceptions, and $RT@k$ refers to the top-k microservices in the ranking list. Fig. \ref{fig:PBA-performance} presents the $AC@1$ and $Avg@5$ values of TopoRank and MicroRCA across different microservice applications. The results indicate that TopoRank performs better than MicroRCA in both metrics. This is primarily due to the fact that TopoRank takes into account both the anomaly potential and microservice dependencies when performing Personalized PageRank. 

The primary purpose of bottleneck location is to narrow down the strategy space and expedite the discovery of the optimal strategy. We perform GA iterations  on both PBs and all microservices to demonstrate the influence of bottleneck localization on optimization. Fig. \ref{fig:fitness-iteration-train} depicts the iterative process under the microservice systems and demonstrates that as the population (Pop) increases, the PB-aware strategy significantly outperforms the approach that scales for all microservices in terms of fitness. The PB-aware strategy can obtain superior fitness in less than five iterations. In contrast, the all-microservices-involved method requires larger populations and more iterations to achieve the same level of fitness. This is attributed to the fact that the PB-aware strategy aids the genetic algorithm in reducing the optimization range precisely and accelerating the acquisition of superior solutions.




\subsubsection{Effectiveness of the SLO Violation Predictor}
The objective of the SLO violation predictor is to forecast the  result of the optimization strategy directly rather than waiting for feedback from the online application. We determine whether performance issues will occur based on the number of replicas and workloads of each microservice. Selecting a suitable binary classification model for the prediction task is critical. With a data collection interval of five seconds, we collected two datasets, including a 3.1k historical sampling dataset (A) for Train Ticket and a 1.5k dataset (B) for Online Boutique, in our cluster. For training and testing on these two datasets, we adopt four classical machine learning methods, including Support Vector Machine (SVM), Random Forest, Multilayer Perceptron (MLP), and Decision Tree. Table. \ref{tab:PR&Recall} shows the precision and recall of the four models for SLO violation prediction. According to the effects of the two datasets, we finally choose Random Forest as the primary algorithm for the SLO violation predictor.



To demonstrate that the SLO violation predictor can substitute the feedback from the real environment, we compare PBScaler, which employs the SLO violation predictor, with MicroScaler, which collects feedback from the online system. We injected burst workloads into the Online Boutique and made only one microservice abnormal to eliminate the difference in bottleneck localization between the two methods. As shown in Fig. \ref{fig:burst-wave-comparision}, with the guidance of the predictor, the number and the frequency of decision-making attempts made by PBScaler are much lower than those of MicroScaler. Reducing online attempts in a cluster will evidently reduce the risk of oscillations.


\section{CONCLUSIONS}
This paper presents PBScaler, a bottleneck-aware autoscaling framework designed to prevent performance degeneration in microservice-based applications. PBScaler collects real-time performance metrics of applications using the service mesh technology and dynamically builds a correlation graph among microservices. To handle abnormal microservices caused by external dynamic workloads and intricate invocations among microservices, PBScaler employs TopoRank, a random walk algorithm  based on the topological potential theory, to identify bottleneck microservices. Furthermore, PBScaler performs an offline evolutionary algorithm to optimize scaling strategies guided by an SLO violation predictor. Experimental results indicate that PBScaler can minimize resource consumption while achieving lower SLO violations. 

In the future, we plan to improve our work from the following two aspects. Firstly, we will explore the potential of using bottleneck awareness in finer-grained resource (e.g., CPU and memory) management. Secondly, we will explore how to circumvent the interference of stateful microservices in autoscaling since the performance degradation from stateful microservices may disrupt the autoscaling controller. Thirdly, we will improve the efficiency of performance bottleneck analysis for large-scale microservice systems.

%%One reason is that the response time of stateful microservices is related to the data volume. As more data is accumulated, the consumer microservices of these data will become slower. The performance degradation caused by stateful microservices can be addressed by improving storage structures rather than autoscaling. In the future, 


\section*{Acknowledgement}

This work is supported by the National Key Research and Development Program of China (No. 2022YFF0902701) and the National Natural Science Foundation of China (Nos. 62032016, 61832014 and 61972292).


% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%

% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
% \begin{thebibliography}{1}

% \bibitem{IEEEhowto:kopka}
% H.~Kopka and P.~W. Daly, \emph{A Guide to {\LaTeX}}, 3rd~ed.\hskip 1em plus
%   0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

% \end{thebibliography}
\bibliographystyle{IEEEtran}
\bibliography{PBScaler-base}

% \clearpage

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig/xsy.jpg}}]{Shuaiyu Xie}
 
 received his B.S. degree from the School of Computer Science, Wuhan University, China, in 2021. He is curently pursuing a Ph.D. degree at Wuhan University. His current research interests include services computing, artificial intelligence for IT operations (AIOps), and task scheduling.
\end{IEEEbiography}

\vspace{-5mm}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig/wj.jpg}}]{Jian Wang}
 
received his Ph.D. degree in Computer Science from Wuhan University, China, in 2008. He is currently an Associate Professor in the School of Computer Science, Wuhan University, China. His current research interests include services computing and software engineering. He is now a member of the IEEE, a senior member of the China Computer Federation (CCF), and a member of the CCF Technical Committee on Services Computing (TCSC).
\end{IEEEbiography}

\vspace{-5mm}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig/lb.jpg}}]{Bing Li}
 
 received the Ph.D. degree from Huazhong University of Science and Technology, Wuhan, China, in 2003. He is currently a Professor at the School of Computer Science, Wuhan University. His main research areas are services computing, software engineering, artificial intelligence, and cloud computing.  He is now a member of the IEEE and a distinguished member of the China Computer Federation (CCF).
\end{IEEEbiography}

\vspace{-5mm}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig/zzk.jpg}}]{Zekun Zhang}
 
received his M.S. degree from the school of computer science, Wuhan University, China. He is currently working toward the Ph.D. degree at the school of computer science, Wuhan University. His research interests include cloud computing and fault diagnosis.
\end{IEEEbiography}

\vspace{-5mm}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig/lc.jpg}}]{Duantengchuan Li}
 
is currently working toward the Ph.D. degree in software engineering with the School of Computer Science, Wuhan University, Wuhan, China.
His research interests include recommendation system, representation learning, natural language processing, intention recognition, pattern recognition, computer vision and their applications in software engineering, intelligent education, and intelligent sports. 
% He has frequently been a Reviewer for more than six international journals, including the IEEE Transactions on Industrial Informatics, IEEE Transactions on Neural Networks and Learning Systems, IEEE Transactions on Multimedia, Information Processing and Management, Knowledge Based Systems, Neurocomputing etc.
\end{IEEEbiography}

\vspace{-5mm}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig/Patrick_Hung.jpeg}}]{Patrick C. K. Hung}
 
received the bachelors degree in computer science from the University of New South Wales, Sydney, NSW, Australia, in 1993, the masters and Ph.D. degrees in computer science from Hong Kong University of Science and Technology, Hong Kong, in 1995 and 2001, respectively, and the masters degree in management sciences from the University of Waterloo,  Canada, in 2002. He is a Professor and the Director of International Programs with the Faculty of Business and Information Technology, Ontario Tech University, Canada. His research interests include services computing, cloud computing, big data, and edge computing. He is a Founding Member of the IEEE Technical Committee on Services Computing and the IEEE Transactions on Services Computing.
\end{IEEEbiography}

% \begin{IEEEbiography}{Michael Shell}
% Biography text here.
% \end{IEEEbiography}

% \begin{IEEEbiography}{Michael Shell}
% Biography text here.
% \end{IEEEbiography}

% \begin{IEEEbiography}{Michael Shell}
% Biography text here.
% \end{IEEEbiography}
% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.



% that's all folks
\end{document}


