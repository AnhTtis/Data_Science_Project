\clearpage
\appendix

\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth, clip = true, trim = 0mm 2mm 0mm 0mm]{FIGS/city-tracking.pdf}
   \caption{
   \textbf{Tracking.} We track keypoints (\textbf{above}) and instance masks (\textbf{below}) across several frames. As a 3D representation, \method\ can track correspondences through 2D occluders.}
\label{fig:city-tracking}
\end{figure*}

\section*{Supplemental Materials}
%%%%%%%%% BODY TEXT

\section{Tracking}

We can compute mask and keypoint-level correspondences across frames after detecting instances (Sec.~\ref{sec:city-scale-eval}) by using Best-Buddies similarity~\cite{bestbuddies} on features $\Phi$ within or between instances. As a 3D representation, \method\ can track correspondences through 2D occluders. We show an example in Fig.~\ref{fig:city-tracking}.

\section{Proposal Sampling}
\label{sec:proposal-sampling}

We use a proposal sampling strategy similar to Mip-NeRF 360~\cite{barron2022mipnerf360} that first queries a lightweight occupancy proposal network at uniform intervals along each camera ray and then picks additional samples based on the initial samples. We model our proposal network with separate hash table-backed static and dynamic branches as in Sec.~\ref{sec:representation}. We train each branch of the proposal network with histogram loss~\cite{barron2022mipnerf360} using the weights of the respective branch of our main model and regularize the resulting sample distances and weights using distortion loss~\cite{barron2022mipnerf360}. We find that proposal sampling gives a 2-4x speedup.

\section{Smoothness Priors}
\label{sec:smoothness-priors}

We use the same spatial and temporal smoothness priors as NSFF~\cite{li2020neural} to regularize our scene flow. We specifically denote:
\begin{align}
    \mathcal{L}_{sm}(\textbf{r}) &= \sum_\textbf{x} \sum_{t' \in [-1, 1]}  e^{-2 \big\lVert{\textbf{x} - \textbf{x}'}\big\rVert_2}\big\lVert{s_{t'}(\textbf{x}, \textbf{t}) - s_{t'}(\textbf{x}', \textbf{t})}\big\rVert_1 \nonumber \\
    & +  \sum_\textbf{x} \big\lVert{s_{\textbf{t}-1}(\textbf{x}, \textbf{t}) + s_{\textbf{t}+1}(\textbf{x}, \textbf{t})}\big\rVert_1,
\end{align}
where $\textbf{x}$ and $\textbf{x}'$ indicate neighboring points along the camera ray $\textbf{r}$.

\section{Ablation Details}
\label{sec:ablations}

\textbf{w/o Depth loss.} We remove depth from the reconstruction loss term:
\begin{equation}
\mathcal{L}_{rec} = \mathcal{L}_c + \lambda_f \mathcal{L}_f + \lambda_o \mathcal{L}_o
\end{equation}

\textbf{w/o Optical flow loss.} We remove optical flow from the reconstruction loss term:
\begin{equation}
\mathcal{L}_{rec} = \mathcal{L}_c + \lambda_f \mathcal{L}_f + \lambda_d \mathcal{L}_d
\end{equation}

\textbf{w/o Warping loss.} We remove all warping and flow-related loss terms:
\begin{equation}
\mathcal{L} = {\underbrace {\Big(\mathcal{L}_c + \lambda_f \mathcal{L}_f + \lambda_d \mathcal{L}_d \Big)}_\text{reconstruction losses}} + {\underbrace {\Big(\lambda_e\mathcal{L}_{e} + \lambda_d\mathcal{L}_{d}\Big)}_\text{static-dynamic factorization}} + \lambda_{\rho}\mathcal{L}_{\rho}.
\end{equation}

\textbf{w/o Appearance embedding.} We compute static color without the latent embedding vector $A_{vid}\mathcal{F}(t)$: 

\begin{equation}
\textbf{c}_s(\textbf{x}, \textbf{d}) \in \mathbb{R}^3
\end{equation}

\textbf{w/o Occlusion weights.} We do not use occlusion weights (\ref{eq:occlusion-weights}) to downweight the warping loss terms (\ref{eq:warped-color}, \ref{eq:warped-feature}):

\begin{align}
    &\mathcal{L}^w_{c}(\mathbf{r}) = \sum_{t' \in [-1, 1]} \big\lVert{C(\textbf{r}) - \hat{C}^w_{t'}(\textbf{r})}\big\rVert^2 \\
    &\mathcal{L}^w_{f}(\textbf{r}) = \sum_{t' \in [-1, 1]}  \big\lVert{F(\textbf{r}) - \hat{F}^w_{t'}(\textbf{r})}\big\rVert_1 
\end{align}

\textbf{w/o Separate branches.} We generate all model outputs using a single time-dependent branch:

\begin{align}
    &\sigma(\textbf{x}, \textbf{t}, \textbf{vid}) \in \mathbb{R} \\
    &\textbf{c}(\textbf{x}, \textbf{t}, \textbf{vid}, \textbf{d}) \in \mathbb{R}^3 \\
    &\Phi(\textbf{x}, \textbf{t}, \textbf{vid}) \in \mathbb{R}^C \\
    &s_{t' \in [-1, 1]}(\textbf{x}, \textbf{t}, \textbf{vid}) \in \mathbb{R}^3
\end{align}

We accordingly remove factorization-related loss terms:

\begin{equation}
\begin{aligned}
    \mathcal{L} &= {\underbrace {\Big(\mathcal{L}_c + \lambda_f \mathcal{L}_f + \lambda_d \mathcal{L}_d + \lambda_o \mathcal{L}_o \Big)}_\text{reconstruction losses}}  + {\underbrace {\Big(\mathcal{L}^w_c + \lambda_f \mathcal{L}^w_f \Big)}_\text{warping losses}} \\
    & \lambda_{flo}{\underbrace {\Big(\mathcal{L}_{cyc} + \mathcal{L}_{sm} + \mathcal{L}_{slo} \Big)}_\text{flow losses}}
\end{aligned}
\end{equation}

\section{Additional Training Details}

We divide City-1M into 48 cells using camera-based k-means clustering. Each cell covers 2.9 $km^2$ and 32k frames across 98 videos on average. We evaluate the effect of geographic coverage and number of frames/videos on cell quality in Table~\ref{tab:city-diagnostics}. We train with 1 A100 (40 GB) GPU per cell for 2 days (same for each KITTI scene). We can fit all cells on a single A100 at inference time.

\begin{table*}[htbp!]
\centering
\footnotesize
\subcaptionbox*{\textbf{Images}}{
\begin{tabular}{lcccc}
\toprule
& $\leq$ 15k & 15-30k & 30-45k &  $\geq$ 45k   \\ \midrule
$\uparrow$PSNR     & 22.86 & 21.99 & 21.35 & 20.75  \\
$\uparrow$SSIM     & 0.583 & 0.569 & 0.557 & 0.538  \\
$\downarrow$LPIPS  & 0.516 & 0.545 & 0.564 & 0.578  \\ \bottomrule
\end{tabular}
}
\subcaptionbox*{\textbf{Videos}}{
\begin{tabular}{lcccc}
\toprule
& $\leq$ 60 & 60-90 & 90-120 &  $\geq$ 120   \\ \midrule
$\uparrow$PSNR     & 22.47 & 21.72 & 21.68 & 21.11 \\
$\uparrow$SSIM     & 0.587 & 0.556 & 0.559 & 0.555  \\
$\downarrow$LPIPS  & 0.526 & 0.557 & 0.557 & 0.565  \\ \bottomrule
\end{tabular}
}
\par\medskip
\subcaptionbox*{\textbf{Area}}{
\begin{tabular}{lcccc}
\toprule
& $\leq$ 2 $km^2$ & 2-3 $km^2$ & 3-4 $km^2$ &  $\geq$ 4 $km^2$   \\ \midrule
$\uparrow$PSNR     & 22.73 & 21.47 & 21.53 & 22.18  \\
$\uparrow$SSIM     & 0.609 & 0.556 & 0.561 & 0.557  \\
$\downarrow$LPIPS  & 0.512 & 0.564 & 0.555 & 0.536  \\ \bottomrule
\end{tabular}
}
\vspace*{-2mm}
\caption{\textbf{City-1M scaling.} We evaluate the effect of geographic coverage and the number of images and videos on cell quality. Although performance degrades sublinearly across all metrics, image and video counts have the largest impact.}
\vspace*{-6mm}
\label{tab:city-diagnostics}
\end{table*}

\section{Assets}

\textbf{City-1M.} Our dataset is constructed from street-level videos collected across a vehicle fleet with seven ring cameras that collect 2048x1550 resolution images at 20 Hz with a combined 360Â° field of view. Both VLP-32C LiDAR sensors are synchronized with the cameras and produce point clouds with 100,000 points at 10 Hz on average. We localize camera poses using a combination of GPS-based and sensor-based methods.

\textbf{Third-party assets.} We primarily base the \method\ implementation on Nerfstudio~\cite{nerfstudio} and tiny-cuda-nn~\cite{tiny-cuda-nn} along with various utilities from OpenCV~\cite{opencv_library}, Scikit~\cite{sklearn_api}, and Amir et al's feature extractor implementation~\cite{amir2021deep}, all of which are freely available for noncommercial use. KITTI~\cite{Geiger2012CVPR} is similarly available under an Apache license, whereas VKITTI2~\cite{gaidon2016virtual} uses the noncommercial CC BY-NC-SA 3.0 license.

\section{Limitations}

\textbf{Video boundaries.} Although our global representation of static geometry is consistent across all videos used for reconstruction, all dynamic objects are video-specific. Put otherwise, our method does not allow us to extrapolate the movement of objects outside of the boundaries of videos from which they were captured, nor does it provide a straightforward way of rendering dynamic visuals at boundaries where camera rays intersect regions with training data originating from disjoint video sequences.

\textbf{Camera accuracy.} Accurate camera extrinsics and intrinsics are arguably the largest contributors to high NeRF rendering quality. Although multiple efforts~\cite{lin2021barf, wang2021nerfmm, SCNeRF2021, meng2021gnerf, 10.1007/978-3-031-19827-4_16} attempt to jointly optimize camera parameters during NeRF optimization, we found the results lacking relative to using offline structure-from-motion based approaches as a preprocessing step.

\textbf{Flow quality.} Although our method tolerates some degree of noisiness in the supervisory optical flow input, high-quality flow still has a measurable impact on model performance (and completely incorrect supervision degrades quality). We also assume that flow is linear between observed timestamps to simplify our scene flow representation.

\textbf{Resources.} Modeling city scale requires a large amount of dataset preprocessing, including, but not limited to: extracting DINO features, computing optical flow, deriving normalized coordinate bounds, and storing randomized batches of training data to disk. Collectively, our intermediate representation required more than 20TB of storage even after compression.

\textbf{Shadows.} \method\ attempts to disentangle shadows underneath transient objects. However, if a shadow is present in all observations for a given location (eg: a parking spot that is always occupied, even by different cars), \method\ may attribute the darkness to the static topology, as evidenced in several of our videos, even if the origin of the shadow is correctly assigned to the dynamic branch.

\textbf{Instance-level tasks.} Although we provide initial qualitative results on instance-level tasks as a first step towards true 3D segmentation backed by neural radiance field, \method is not competitive with conventional approaches.

\section{Societal Impact}

As \method\ attempts to model dynamic urban scenes with pedestrians and vehicles, our approach carries surveillance and privacy concerns related to the intentional or inadvertent capture or privacy-sensitive information such as human faces and vehicle license plate numbers. As we distill semantic knowledge into \method, we are able to (imperfectly) filter out either entire categories (people) or components (faces) at render time. However this information would still reside in the model itself. This could in turn be mitigated by preprocessing the input data used to train the model.