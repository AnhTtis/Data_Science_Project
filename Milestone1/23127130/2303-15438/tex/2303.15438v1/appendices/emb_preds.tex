\section{Predictions of embeddings from the NTK after training}
\label{app:emb_pred}

Here we demonstrate that the true embeddings learned by SSL methods using small initialization show fair agreement with predictions computed from our theory using the empirical NTK after training.
The motivation for this experiment comes from an observation of \citet{atanasov:2021-silent-alignment} regarding ResNets with small initialization trained in a supervised setting.
Though the NTK of such a model evolves dramatically during training, the final function is nonetheless well-predicted by kernel regression using the empirical NTK after training.
We find a similar result here.

We train ResNet-50 encoders from moderately small init to convergence with Barlow Twins ($\alpha=0.542$), SimCLR ($\alpha=0.284$), and VICReg ($\alpha=0.604$) losses on STL-10.
These models have only $d = 50$, which is large enough to be nontrivial but small enough to give good statistical power to our subsequent analysis.
After training, we then take a batch of $n = 1000$ random augmented image pairs $\X$ and compute both their joint empirical NTK $\tilde{\mK} \in \R^{2n \times 2n}$ and their embeddings $\vf_\text{exp}(\X) \in \R^{d \times 2n}$.
We trust that $n$ is sufficiently larger than $d$ that it is reasonable to treat $\X$ as the full population distribution.
We then compute the theoretically predicted embeddings using Equation \ref{eqn:kernelized_top_spectral_embs} as
\begin{equation}
    \vf_\text{th}(\X) = \tilde{\mS} [\vb_1 \ ... \ \vb_d]\T \tilde{\mK}^{1/2}.
\end{equation}

It is principally the subspace (in function space) spanned by a vector representation which determines the performance of a linear probe on a downstream task.
As such, we compute the right singular vectors of $\vf_\text{exp}(\X)$ and $\vf_\text{th}$, which we call $\mP_\text{exp} \in \R^{d \times 2n}$ and $\mP_\text{th} \in \R^{d \times 2n}$ and which both have rank $d$.
We then compute the \textit{normalized subspace alignment} $\a(\mP_\text{exp}, \mP_\text{th})$, where $\a(\mP, \mP’) \equiv \frac{1}{d} \norm{\mP (\mP’)\T}_F^2$.
This alignment metric attains its maximal value of $d$ when both subspaces contain the same vectors, and has an expectation of $\frac{d^2}{2 n} \ll 1$ for random subspaces\footnote{One may intuitively think of $\a(\mP, \mP’)$ as the mean fraction of a random vector from the rowspace of $\mP$ which is captured by the rowspace of $\mP'$.}.
As an additional test, we repeated this comparison replacing $\mP_\text{th}$ with $\mP_\text{NTK}$ containing the top $d$ eigenvectors of $\tilde{\mK}$ and found similar alignment.

\begin{table}
\begin{centering}
\begin{tabular}{ccccc}
  & BT & SimCLR & VICReg & (random subspaces)\\ \hline
$\a(\mP_\text{exp}, \mP_\text{th})$ & 0.615 & 0.517 & 0.592 & 0.025 \\
$\a(\mP_\text{exp}, \mP_\text{NTK})$ & 0.510 & 0.450 & 0.481 & 0.025
\end{tabular}
\caption{
\label{tab:th-exp-emb-alignment}
Alignments between true embedding subspaces and those predicted from the final NTK for different SSL methods.
}
\end{centering}
\end{table}

\begin{table}
\begin{centering}
\begin{tabular}{cccc}
$\a(\mP^\text{BT}_\text{exp}, \mP^\text{SC}_\text{exp})$ &
$\a(\mP^\text{BT}_\text{exp}, \mP^\text{VR}_\text{exp})$ &
$\a(\mP^\text{SC}_\text{exp}, \mP^\text{VR}_\text{exp})$ & (random subspaces)
\\ \hline
0.504 & 0.504 & 0.405 & 0.025\\
\end{tabular}
\caption{
\label{tab:cross-method-alignment}
Alignments between true embedding subspaces for different SSL methods.
}
\end{centering}
\end{table}

We report our observed subspace alignments in Table \ref{tab:th-exp-emb-alignment}.
For all three methods, we see an alignment between 0.5 and 0.6, which is significantly higher than expected by chance, but still misses roughly half of the span of the true embeddings\footnote{Agreement with predictions from the \textit{initial} NTK (not reported) is generally worse but still greater than chance.}.
We anticipate that much of the gap from unity is due to approximation error caused by taking a finite dataset of size $2n$.
It is perhaps surprising that our theory seems to work equally well for SimCLR and VICReg as it does for Barlow Twins.

We also report alignment scores \textit{between $\mP_\text{exp}$ from the three SSL methods} in Table \ref{tab:cross-method-alignment}, again finding alignment roughly between 0.4 and 0.5.
This finding, independent of any theoretical result, is evidence that these three methods are in some respects learning very similar things.

