\section{Understanding the effect of the $\lambda$ parameter in Barlow Twins}
\label{sec:loss_hyperparams}

% \subsection[]{Derivation of loss and eigenvalue curves for $\lambda < 1$}

The experiments of Section \ref{sec:exp} with ResNets and realistic hyperparameters do exhibit the stepwise behavior our theory predicts, but the magnitudes of the predicted quantities --- the loss and the eigenvalues --- differ from those of our simplified model of Barlow Twins
Our theory predicts that, at each learning step, the Barlow Twins loss will drop by one and a new embedding eigenvalue will grow from zero and settle at one with those learned prior.
Our experiments, however, find that the drops in the loss are usually greater than one at early index and shrink as more occur, and the eigenvalues come to rest at values much larger than one, but shrink as each new eigenvalue joins the learned.

For SimCLR and VICReg, this discrepancy is not surprising, as our toy model is meant to describe Barlow Twins.
In the case of SimCLR in particular, the embeddings are normalized to the hypersphere, which leads to various geometric factors playing into the dynamics.
However, the difference between our toy model and realistic Barlow Twins merits an explanation.
The purpose of this appendix is to show that this difference is due to the choice of the hyperparameter $\lambda$ in the Barlow Twins loss function.

The full loss function of \citet{zbontar:2021-barlow-twins} takes the form
\begin{equation}
    \L = \sum_{j=1}^d \mC_{jj} + \lambda \sum_\substacp{j , k = 1 \\ j \neq k}^d,
\end{equation}
where $\lambda$ is a parameter which assigns a relative weight to the off-diagonal elements of $\mC$.
(To maintain consistency between the notations of the main text and \citep{zbontar:2021-barlow-twins}, we will use $\lambda$ for this hyperparameter and $\lambda_j(\mC)$ for the $j$-th eigenvalue of $\mC$.)
We take $\lambda = 1$ in our simplified model.
However, in practice, $\lambda$ can be quite small --- a typical case (and one we use in our experiments) might be $d = 2048$, $\lambda = .0051$.

As discussed in Section \ref{sec:linear_th}, when $\lambda = 1$, the loss decomposes as
\begin{equation}
    \L = \sum_j \left( 1 - \lambda_i(\mC) \right)^2.
\end{equation}
This loss is sensitive only to the eigenvalues of $\mC$ and is thus invariant to the choice of eigenvectors.
Given $k$ modes to work with (that is, the freedom to choose any rank-$k$ matrix for $\mC$), the optimal choice is to choose their eigenvalues to be one, and their eigenvectors can be arbitrary.
The loss at this stage will be $\L^{(k)} = d - k$.

However, when $\lambda < 1$, the choice of eigenvectors now matters.
Consider the case when $\mC$ has rank $k=1$.
If the eigenvector is aligned with the basis of the matrix --- for example, $\mC = \eta \ve_1 \ve_1\T$ --- then the loss is indeed minimized with $\eta = 1$, and the story is the same as before.
However, if the eigenvector is maximally delocalized --- that is, if $\mC = \eta \vu_1 \vu_1\T$ for some $\vu_1 \in \R^d$ whose $d$ elements each have norm $|\vu_{1j}| = d^{-1/2}$ --- then the loss-minimizing choice is now $\eta = d / (\lambda d - \lambda + 1) > 1$.
Because the off-diagonal elements are penalized less strongly, a delocalized eigenvector can grow larger, better satisfying the on-diagonal loss without incurring a large penalty.

Let us generalize this to the case of $k$ modes learned.
Assuming all $k$ modes have optimal delocalized eigenvectors\footnote{We will not worry about how to construct these, but note that it is easily done using Haar wavelets when $d = 2^a$ for some $a$.} and share the same magnitude, the optimal scale for all eigenvalues and the resulting loss are given by
\begin{align}
    \label{eqn:lambda_j_k}
    \lambda_j^{(k)}(\mC) &= \frac{d}{\lambda d + (1 - \lambda) k} \\
    \label{eqn:L_k}
    \L^{(k)} &= d - \frac{d k}{\lambda d + (1 - \lambda) k}.
\end{align}

This turns out to match experiment quite well.
Figure [FIGURE] juxtaposes the resulting eigenvalue and loss steps with those seen in real Barlow Twins with $d = 2048$ and $\lambda = .0051$.
We see that, though there are small discrepancies, the overall scales of the eigenvalues and loss drops are matched.

% \subsection[]{Why take $\lambda \ll 1$?}

% Having explained this, let us now turn to the question of why one would need $\lambda \ll 1$.
% \citet{zbontar:2021-barlow-twins} find $\lambda = .0051$ via black-box tuning, and no work we know of has explained why this parameter should be so small.
% The loss function has the same minimum --- i.e., $\mC = \mI_d$ --- regardless of $\lambda$, which suggests the value of $\lambda$ must matter for training purposes, not generalization.






% In particular, for Barlow Twins, the embeddings start collapsed to a point, then expand to line on a line, then a plane, then a 3-manifold, and so on, and each new dimension can be orthogonal to the others.
% However, for SimCLR, these expanding manifolds are constrained to the hypersphere.
% A further thorn is that 