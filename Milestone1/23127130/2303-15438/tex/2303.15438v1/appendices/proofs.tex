\section{Proofs}
\label{app:proofs}

\subsection{Proof of Proposition \ref{prop:Fnorm_min}}

We wish to show that the top spectral $\mW$ are the unique solutions to
\begin{equation}
    \underset{\mW}{\text{argmin}} \norm{\mW}_F
    \ \ \ \text{s.t.} \ \ \
    \L(\mW) = 0.
\end{equation}
Any such solution is also a minimum of
\begin{equation}
    \L_\epsilon(\mW) \equiv \L(\mW) + \norm{\mW}_F^2
\end{equation}
as $\epsilon \rightarrow 0$.
We will show that the set of minima are precisely the top spectral embeddings.

Any local minimum must satisfy
\begin{align}
    \nabla_\mW \L_\epsilon
    &= (\mI - \mW \mGamma \mW\T) \mW \mGamma - \epsilon \mW = \mathbf{0} \\
    \label{eqn:mW_local_min_eqn}
    \Rightarrow
    \mW\T \nabla_\mW \L_\epsilon
    &= \mW\T\mW\mGamma - \left(\mW\T\mW\mGamma\right)^2 - \epsilon \mW\T\mW = \mathbf{0}.
\end{align}
Note that the first and second terms of Equation \ref{eqn:mW_local_min_eqn} share all eigenvectors (i.e. commute), and thus the third term must commute with both of them.
From the fact that $\mW\T\mW\mGamma$ and $\mW\T\mW$ share eigenvectors, we can conclude that $\mW\T\mW$ and $\mGamma$ share eigenvectors.
The eigenvectors of $\mW\T\mW$ are simply the right singular vectors of $\mW$.

Any local minimum must thus take the form
\begin{equation}
    \mW = \mU \mS \mGamma^{(\mathcal{J})},
\end{equation}
where
\begin{itemize}
\item $\mU \in \R^{d \times d}$ is orthogonal,
\item $\mS \in \R^{d \times d} = \text{diag}(s_1, ..., s_d)$,
\item $\mathcal{J}$ is a list of $d$ elements from $\{1,...,d\}$,
\item $\mGamma^{(\mathcal{J})} \in \R^{d \times m}$ contains as rows the $d$ eigenvalues of $\mGamma$ corresponding to the elements of $\mathcal{J}$, and
\item the singular values satisfy
\begin{equation}
    s_j^2 \gamma_{\mathcal{J}_j} - s_j^4 \gamma_{\mathcal{J}_j}^2 - \epsilon s_j^2 = 0
    \ \ \ \Rightarrow \ \ \
    s_j \in \left\{0, \sqrt{\frac{\gamma_{\mathcal{J}_j} - \epsilon}{\gamma_{\mathcal{J}_j}^2}}\right\},
\end{equation}
where $s_j = 0$ is chosen by default when $\gamma_{\mathcal{J}_j} \le 0$.
\end{itemize}
Of these candidate local minima, a global minimum must choose $s_j = 0$ as infrequently as possible, and given that must minimize $\norm{\mW}_F^2 = \sum_j s_j^2$.
This is achieved by choosing $\mathcal{J} = (1,...,d)$, which yields the top spectral solutions.
% \todo{under an assumption about uniqueness and having at least $d$ positive eigenvalues?}

\subsection{Proof of Proposition \ref{prop:kernelized_sol}}
\newcommand{\XXh}{\begin{bmatrix} \mX\T \ {\mX'}\T \end{bmatrix}}
\newcommand{\XXv}{\begin{bmatrix} \mX \\ \mX' \end{bmatrix}}

We want to translate our results for linear models in Section \ref{sec:linear_th} to the kernel setting by phrasing interesting quantities in terms of inner products $\vx_i\T \vx_j$.
After finishing this procedure, we will reinterpret $\vx_i\T \vx_j \rightarrow \vphi(x_i)\T \vphi(x_j) = \K(x_i,x_j)$.

It will be useful to perform some setup before diving in.
We define $\mX = [\vx_1, ..., \vx_d]\T \in \R^{n \times m}$ and $\mX' = [\vx'_1, ..., \vx'_d]\T$ and define the full dataset kernel matrix
$\tilde{\mK} = \XXv \XXh$.
Recall that $\mGamma = \frac{1}{2n} \left( \mX\T \mX' + {\mX'}\T \mX \right)$.
In what follows, all interesting vectors will lie in either the row-space or column-space of $\XXv$ (whichever is appropriate), and so all vectors over the dataset (i.e. with $2n$ elements) will lie in the cokernel of $\tilde{\mK}$, so we are free to use the pseudoinverse of $\tilde{\mK}$ without worrying about the action of null eigendirections.
All matrix inverses henceforth will be interpreted as pseudoinverses.

\textbf{Proof of clause (a).}
An eigenpair $(\gamma, \vg)$ of $\mGamma$ satisfies
\begin{equation} \label{eqn:gamma_eigenval_def}
\mGamma \vg = \gamma \vg.
\end{equation}
Assume that $\gamma \neq 0$ and $\norm{\vg} = 1$.
Let us define a vector $\vb$ dual to $\vg$ as
\begin{equation} \label{eqn:b_def}
    \vb = \tilde{\mK}^{-1/2} \XXv \vg
    \quad
    \Leftrightarrow
    \quad
    \vg = \XXh \tilde{\mK}^{-1/2} \vb.
\end{equation}
Note that $\norm{\vg} = \norm{\vb}$.
Plugging Equation \ref{eqn:b_def} into Equation \ref{eqn:gamma_eigenval_def}, we find that
\begin{equation}
\mGamma \XXh \tilde{\mK}^{-1/2} \vb = \gamma \XXh \tilde{\mK}^{-1/2} \vb.
\end{equation}
Left-multiplying both sides by $\tilde{\mK}^{-1/2} \XXv$ yields
\begin{align}
    \tilde{\mK}^{-1/2} \XXv \mGamma \XXh \tilde{\mK}^{-1/2} \vb
    &= \gamma \tilde{\mK}^{-1/2} \XXv \XXh \tilde{\mK}^{-1/2} \vb \\
    &= \gamma \vb.
\end{align}
Defining
\begin{equation}
    \mZ \equiv
    \XXv \mGamma \XXh =
    \frac{1}{2n}
\begin{bmatrix}
    \mX {\mX'}\T \mX {\mX}\T & \mX {\mX'}\T \mX {\mX'}\T \\
    \mX' {\mX'}\T \mX {\mX}\T & \mX' {\mX'}\T \mX {\mX'}\T
\end{bmatrix}
+
\frac{1}{2n}
\begin{bmatrix}
    \mX {\mX}\T \mX' {\mX}\T & \mX {\mX}\T \mX' {\mX'}\T \\
    \mX' {\mX}\T \mX' {\mX}\T & \mX' {\mX}\T \mX' {\mX'}\T
\end{bmatrix}
\end{equation}
as in the main text and $\mK_\mGamma \equiv \tilde{\mK}^{-1/2} \mZ \tilde{\mK}^{-1/2}$, we find that
$\gamma$ is also an eigenvalue of $\mK_\mGamma$.
The same argument run in reverse (now rewriting $\vb$ in terms of $\vg$) yields that nonzero eigenvalues of $\tilde{\mK}_\mGamma$ are also eigenvalues of $\mGamma.$ \QED

\textbf{Proof of clause (b).}
The top spectral weights given by Definition \ref{def:spectramax} yield the spectral representation
% should we toss this in the main text?
\begin{equation}
    \vf(\vx) = \mU \tilde{\mS} [\vg_1 \ ... \ \vg_d]\T \vx.
\end{equation}
Plugging in Equation \ref{eqn:b_def} yields that
\begin{equation}
   \vf(\vx) = \mU \tilde{\mS} [\vb_1 \ ... \ \vb_d]\T \tilde{\mK}^{-1/2} \XXv \vx
\end{equation}
as claimed by Proposition \ref{prop:kernelized_sol}. \QED

\textbf{Proof of clause (c).}
The RKHS norm of a function with respect to a kernel $\K$ is equivalent to the Frobenius norm of an explicit matrix transformation on the hidden features of the kernel, so this clause follows from Proposition \ref{prop:Fnorm_min}. \QED

\textbf{Kernelized dynamics.}
% Recall from Result \ref{res:small_init} that, in the explicit linear case, the representations trained from small init at an arbitrary finite time $t$ are given by
% \begin{equation}
%     \vf(\vx, t) = \mU \tilde{\mS}(t) [\vg_1 \ ... \ \vg_d]\T \vx,
% \end{equation}
% with the initialization uniquely determining these quantities according to
% \begin{equation}
%     \mU \tilde{\mS}(0) [\vg_1 \ ... \ \vg_d]\T
%     = \mathcal{A}(\mW(0) [\vg_1 \ ... \ \vg_d])[\vg_1 \ ... \ \vg_d]\T.
% \end{equation}
% We can straightforwardly kernelize this expression.
% We first use Equation \ref{eqn:b_def} to get
% \begin{equation}
%     \mU \tilde{\mS}(0) [\vg_1 \ ... \ \vg_d]\T
%     = \mathcal{A}(\mW(0) [\vg_1 \ ... \ \vg_d])[\vg_1 \ ... \ \vg_d]\T.
% \end{equation}

Via direct kernelization of Result \ref{res:small_init}, the kernelized representations trained from small init at an arbitrary finite time $t$ are well-approximated by
\begin{equation} \label{eqn:kernelized_traj}
    \vf(\vx, t)
    = \mU \tilde{\mS}(t) [\vg_1 \ ... \ \vg_d]\T \vx
    = \mU \tilde{\mS}(t) [\vb_1 \ ... \ \vb_d]\T \tilde{\mK}^{-1/2} [\mK_{x \X} \ \mK_{x \X'}]\T,
\end{equation}
with the singular values on the diagonal of $\tilde{\mS}(t)$ evolving according to Proposition \ref{prop:exact_dynamics}.
The initialization-dependent matrix $\mU$ and effective initial singular values are found in the explicit linear case as
\begin{equation}
    \mU \tilde{\mS}(0) = \mathcal{A}(\mW(0) {\mGamma^{(\le d)}}\T) = \mathcal{A}(\mW(0) [\vg_1, ..., \vg_d]).
\end{equation}
Using Equation \ref{eqn:b_def}, we then have that
\begin{equation} \label{eqn:kernelized_init}
    \mU \tilde{\mS}(0) = \mathcal{A}
    \left(
    [\vf(x_1), ..., \vf(x_n), \vf(x'_1), ..., \vf(x'_n)] \tilde{\mK}^{-1/2} [\vb_1, ... \vb_d]
    \right).
\end{equation}
Equations \ref{eqn:kernelized_traj} and \ref{eqn:kernelized_init} together permit one to find the trajectories from arbitrary small init in fully kernelized form.



% \subsection{Formal statement and proof of [THEOREM].}

% Here we give a formal statement of [THEOREM].
% As useful notation, let the eigendecomposition of $\mGamma$ be $\mGamma = \mXi \mLambda \mXi\T$ with $\mLambda = \diag(\gamma_1, ..., \gamma_m)$ (note that $\Gamma^{(\le d)}$ as defined in the main text consists of the first $d$ rows of $\Xi\T$).

% \begin{theorem}[Trajectory from small initialization, formal] \label{thm:small_init_formal}

% Let $\gamma_1, ..., \gamma_d$ be unique and positive.
% Let $\tilde{\mW}_0 \in \R^{d \times m}$ with $\tilde{\mW}_0 \mGamma^{(\le d)}$ full rank.
% Let $\mW_0 = \alpha \tilde{\mW}_0$ with $\alpha > 0$, and let $\mW(t)$ be the solution to Equation \ref{eqn:dW_dt_expanded} with initial condition $\mW(0) = \mW_0$.
% Let $\mU \in \R^{d \times d}$ be a rotation matrix and $\{a_{jk}(t)\}$ and $\{s_j(t)\}$ be the unique scalar functions such that $s_j(t) > 0$ and
% \begin{align} \label{eqn:generic_W_fac}
%     \mW(t) &=
%     \mU
%     \begin{bmatrix} 1  & a _{1 2 }(t)  & a _{1 3 }(t)  & \cdots  & a _{1 d }(t)  & \cdots  & a _{1 m }(t)  \\ 0  & 1  & a _{2 3 }(t)  & \cdots  & a _{2 d }(t)  & \cdots  & a _{2 m }(t)  \\ 0  & 0  & 1  & \cdots  & a _{3 d }(t)  & \cdots  & a _{3 m }(t)  \\ \vdots  & \vdots  & \vdots  & {\ddots}  & \vdots  &   & \vdots  \\ 0  & 0  & 0  & \cdots  & 1  & \cdots  & a _{d m }(t)  \\  \end{bmatrix}
%     \begin{bmatrix} s_1(t)  &   &   &   &   \\   & s_2(t)  &   &   &   \\   &   & s_3(t)  &   &   \\   &   &   & \ddots  &   \\   &   &   &   & s_m(t)  \\  \end{bmatrix}
%     \mXi\T \\
%     &= \mU \mA(t) \mS(t) \mXi\T,
% \end{align}
% where $\mA(t)$ and $\mS(t)$ are the respective matrices on the previous line.

% Then, as $\alpha \rightarrow 0$, $\mW(t)$ converges in Euclidean distance to the spectrally aligned solution $\mW^*(t) = \mU \mS^*(t) \mGamma^{(\le d)}$ initialized with $\mS^*(0) = \diag(s_1(0), ..., s_d(0))$ and whose dynamics are given by Proposition \ref{prop:exact_dynamics}.

% \end{theorem}

% \textbf{Remarks.}
% \begin{itemize}
%     \item Unlike with prior uses of the notation, the scalars $s_j(t)$ in Equation \ref{eqn:generic_W_fac} are \textit{not} singular values because of the presence of the matrix $\mA$.
%     \item The decomposition of $\mW(t)$ in Equation \ref{eqn:generic_W_fac} is essentially a QR factorization.
% \end{itemize}

% \textbf{Proof of Theorem \ref{thm:small_init_formal}}

% \textit{Overall strategy.}
% As first observation, note that if $a_{jk}(0) = 0$ for all $j,k$, then $\mW(0) = \mW^*(0)$ and thus $\mW(t) = \mW^*(t)$.
% We will show that, as $\alpha \rightarrow 0$, the values $a_{jk}(0)$ become irrelevant, so we do effectively have this initialization.

% \textit{Separation of timescales.}
% Suppose $s_j(0) = \alpha \tilde{s}_j(0)$.
% Motivated by Equation \ref{eqn:tau_j_def}, define the timescales
% \begin{equation}
%     \tau_j = \frac{- \log (\alpha^2 \tilde{s}_j^2(0) \gamma_j)}{8 \gamma_j}
% \end{equation}
% which will govern when each mode reaches $\mathcal{O}(1)$ size.
% Observe that, as $\alpha \rightarrow 0$, these times will become arbitrarily well-separated, with
% \begin{equation}
%     \tau_{j+1} - \tau_{j} \sim -\left( \frac{1}{\gamma_{j+1}} - \frac{1}{\gamma_{j}} \right) \log \alpha.
% \end{equation}
% We shall examine the dynamics in disjoint time intervals which correspond to the learning of each mode.
% First we will consider $t \in [0, \tau_1 - c )$, with $c \equiv |\log \alpha|^{1/2}$ a large constant with a convenient scaling.
% Then we will consider $t \in [\tau_1 - c, \tau_1 + c)$, during which the first mode will be learned.
% Then we will consider $t \in [\tau_1 + c, \tau_2 + c)$, then $t \in [\tau_2 + c, \tau_3 + c)$, and so on until $t \in [c \tau_{d-1}, \infty)$.

% \textit{Diagonalization of dynamics.}
% Let us define $\mB(t) \equiv \mA(t) \mS(t)$.
% Diagonalizing Equation \ref{eqn:dW_dt_expanded} reveals that $\mU$ is static and that $\mB(t)$ evolves according to
% \begin{equation} \label{eqn:dB_dt}
%     \frac{d \mB(t)}{d t} = 4 \left( \mI_d - \mB(t) \mLambda \mB\T(t) \right) \mB(t) \mLambda.
% \end{equation}
% We will study the evolution of $\mB(t)$.

% \textit{Stage 0: $t \in [0, \tau_1 - c)$.}
% In this stage, because $\mB(t) \mLambda \mB\T(t) \approx \mathbf{0}$, Equation \ref{eqn:dB_dt} becomes
% \begin{equation} \label{eqn:dB_dt}
%     \frac{d \mB(t)}{d t} = (4 + \O(e^{-c})) \mB(t) \mLambda.
% \end{equation}
% As $\alpha \rightarrow 0$, this error term will become negligible, and we will get $s_j(t) = s_j(0) e^{4 \gamma_j t}$.
% Observe that, by the end of this stage, each $s_j(t)$ dominates the subsequent one, with
% \begin{equation}
%     \frac{s_j(t)}{s_{j+1}(t)} \sim \left( \frac{1}{\alpha} \right)^{\frac{\gamma_j - \gamma_{j+1}}{\gamma_1}}.
% \end{equation}

% \textit{Stage 1A: $t \in [\tau_1 - c, \tau_1 + c)$.} In this stage, the dynamics obey
% \begin{equation}
%     \frac{d \mB(t)}{d t} =
%     4
%     \left(
%     \mI -
%     \fourdiag{\gamma_1 s_1^2(t)}{0}{0}{0}
%     + \O\left( \alpha^{\frac{\gamma_1 - \gamma_2}{\gamma_2}} \right)
%     \right)
%     \mB(t)
%     \mLambda.
% \end{equation}
% Dropping the negligible error term, $s_1(t)$ will evolve precisely as in the case of aligned initialization (i.e. independently of all other degrees of freedom).
% Solving for its dynamics and stitching together the solution from stage 0, we have
% \begin{equation} \label{eqn:s_j(t)}
%     s_1(t) = \frac{e^{4 \gamma_j t}}{\sqrt{s_j^{-2}(0) + (e^{8 \gamma_j t} - 1) \gamma_j}},
% \end{equation}
% the aligned dynamics from Proposition \ref{prop:exact_dynamics}.
% All other $s_j(t)$ will continue to grow exponentially with negligible error.

% \textit{Stage 1B: $t \in [\tau_1 + c, \tau_2 - c)$.} In this stage, the dynamics obey
% \begin{equation}
%     \frac{d \mB(t)}{d t} =
%     4
%     \left(
%     \mI -
%     \fourdiag{1}{0}{0}{0}
%     + \O\left( e^{-c} \right)
%     \right)
%     \mB(t)
%     \mLambda.
% \end{equation}
% Dropping the negligible error term, these dynamics give --- taking care to factor $\mB(t)$ into $\mA(t)\mS(t)$ --- the evolution
% \begin{align}
%     \frac{d s_j(t)}{d t} &= \left\{\begin{array}{ll}
%         0 & \text{for } j = 1, \\
%         4 \gamma_j s_j(t) & \text{for } j > 1,
%         \end{array}\right. \\
%     \frac{d a_{jk}(t)}{d t} &= \left\{\begin{array}{ll}
%         - 4 \gamma_k a_{jk}(t) & \text{for } j = 1, \\
%         0 & \text{for } j > 1.
%         \end{array}\right.
% \end{align}
% The result is that, during this stage of evolution, the $s_{j > 1}$ continue their exponential growth, while the $a_{1k}(t)$ shrink exponentially, and by the end of this stage are $\mathcal{O}(\alpha^{(\gamma_2^{-1} - \gamma_1^{-1})})$ and thus negligible.

% \textit{Stage 2A: $t \in [\tau_2 - c, \tau_2 + c)$.} In this stage, because we now have $a_{1k} = 0$, the dynamics simply obey
% \begin{equation}
%     \frac{d \mB(t)}{d t} =
%     4
%     \left(
%     \mI -
%     \fourdiag{1}{\gamma_2 s_2^2(t)}{0}{0}
%     + \O\left( \text{err scale} \right)
%     \right)
%     \mB(t)
%     \mLambda.
% \end{equation}
% Discarding the negligible error and consulting Equation \ref{eqn:generic_W_fac}, we find that now $s_1(t)$ is saturated and static, and $s_2(t)$ evolves precisely as in the case of aligned initialization (i.e. independently of all other degrees of freedom).
% As with $s_1$ in Stage 1A, we may stitch these dynamics together with the steady exponential growth in earlier phases to obtain precisely the time-evolution of Proposition \ref{prop:exact_dynamics}.

% \textit{Stage 2B: $t \in [\tau_2 + c, \tau_3 - c)$.} Exactly like in Stage 1B, the functions $a_{2k}(t)$ for all $k$ will decay exponentially to zero in this phase.

% \textit{Stages 3A through $d$B: $t \in [\tau_3 - c, \tau_d + c)$}:
% By the same arguments as previous stages, for each $j \in 3...d$, in stage $j$A, mode $j$ grows following the trajectory of Proposition \ref{prop:exact_dynamics}, and in stage $j$B, the functions $a_{jk}(t)$ for all $j$ decay to zero.

% \textit{Final stage: $t \in [\tau_d + c, \infty)$.}
% In the start of the final stage, $\mB \Lambda \mB\T = \mI + \O(e^{-c})$, and $\mB(t)$ changes negligibly between this point and convergence.

% \textit{Summary.}
% At all stages, we have shown that 

% % \textit{Large separation between $s_j$ at early times.}
% % Recall the dynamics of Equation \ref{eqn:dW_dt_expanded}:
% % \begin{equation} \tag{\ref{eqn:dW_dt_expanded}}
% %     \frac{d \mW}{d t} = 4 \left( \mI_d - \mW \mGamma \mW\T \right) \mW \mGamma.
% % \end{equation}
% % Defining $\mB(t) \equiv \mA(t) \mS(t)$, diagonalizing Equation \ref{eqn:dW_dt_expanded} tells us that $\mU$ is static and
% % \begin{equation}
% %     \frac{d \mB(t)}{d t} = 4 \left( \mI_d - \mB(t) \mLambda \mB\T(t) \right) \mB(t) \mLambda.
% % \end{equation}

% % Observe that, for sufficiently small $\mB(t)$, these dynamics reduce to
% % \begin{align}
% % \frac{d \mB(t)}{d t} &= 4 \mB(t) \mLambda \\
% % \Rightarrow
% % \mB(t) &= \mB(0) e^{4 \mLambda t} \\ \label{eqn:exp_growth_in_S_at_early_time}
% % \Rightarrow
% % \mA(t) &= \mA(0), \ \mS(t) = \mS(0) e^{4 \mLambda t},
% % \end{align}
% % with negligible error as $\norm{\mB}_F = \norm{\mW}_F \rightarrow 0$.
% % % (Note that, here and throughout, we \textit{define} the matrices $\mA(t)$ and $\mS(t)$)
% % At early times, then, each $s_j$ grows at an exponential rate $s_j(t) = s_j(0) e^{4 \gamma_j t}$.




% % \begin{equation}
% %     s_j(\tau_j) \sim \alpha e^{- \log \alpha \frac{\gamma_j}{\gamma_1}}
% % \end{equation}

% % \begin{equation}
% %     \frac{s_j(\tau_1)}{s_{j+1}(\tau_1)} \sim e^{- \log \alpha \frac{\gamma_j - \gamma_{j+1}}{\gamma_1}}
% % \end{equation}



% % If we initialize sufficiently small --- say, $\alpha = \frac{}{}$ --- Equation \label{eqn:exp_growth_in_S_at_early_time} remains valid for as long as we would like.


% % define stage times

% % discuss dynamics within stages

