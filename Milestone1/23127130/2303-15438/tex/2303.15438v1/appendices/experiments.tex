\section{Experimental details}
\label{app:exp}

Below we describe our suite of SSL experiments.
All code and experiment configs are available at \href{https://gitlab.com/generally-intelligent/ssl_dynamics}{\url{https://gitlab.com/generally-intelligent/ssl_dynamics}}.

\subsection{Single-hidden-layer MLP}
This experiment follows the linear model numerical experiment described in \ref{subsec:linear_model_sim}. We train a single-hidden-layer MLP for 7000 epochs over a fixed batch of 50 images from CIFAR10 using full-batch SGD. Each image is subject to a random 20x20 crop and no other augmentations. The learning rate is $\eta=0.0001$ and weights are scaled upon initialization by $\alpha=0.0001$. The hidden layer has width 2048 and the network output dimension is $d = 10$. We use Barlow Twins loss, but do not apply batch norm to the embeddings when calculating the cross-covariance matrix. $\lambda$ is set to 1.

\subsection{Full-size models}
We run two sets of experiments experiments with ResNet-50 encoders which respectively use \textit{standard init} and \textit{small init}. The standard set aims to reproducing performance described in the source publication and match original hyperparameters, whereas the small-init set includes modifications which aim to bring out the learning dynamics predicted by our theory more clearly.
Relative to the standard set, the small init set multiplies initial weights by a small scale factor $\alpha$ and uses a reduced learning rate $\eta$.
The parameters used for each method are generally identical across the two sets except for $\alpha$ and $\eta$.

For all experiments in the standard set, we keep parameters as close as possible to those in the source publication but make minor adjustments where we find they improve evaluation metrics. Additionally, in order to avoid a steep compute budget for reproduction, we train our models on STL-10 instead of ImageNet, and reduce batch size, projector layer widths and training epochs so that the models can be trained using a single GPU in under 24 hours. \textbf{Augmentations, optimizer parameters such as momentum and weight decay, and learning rate schedules are generally left unchanged from the source publications.} Below we describe deviations from the stock hyperparameters.

\textbf{Barlow Twins.}
We set batch size to 64, and update the projector to use single hidden layer of width 2048 and $d$=512. We use the LARS optimizer \citep{you:2017-LARS} with stock learning rates. In the small-init case, we additionally set $\lambda=1$ and $\alpha=0.093$ and remove the pre-loss batch norm. In the standard case, we keep $\lambda = 0.0051$ as suggested by \citep{zbontar:2021-barlow-twins}. We train the networks for 320 epochs.

\textbf{SimCLR.}
We set batch size to 256, the cross-entropy temperature $\tau=0.2$ and $\eta=0.5$. We train using SGD with weight decay $1*10^{-5}$ and no LR scheduling. In the small-init case, we use $\eta=0.01$ and additionally set $\alpha=0.087$. We train the networks for 320 epochs.

\textbf{VICReg.}
We set batch size to 256, reduce the projector's hidden layer width to 512 and set $d=128$. We use $\eta=0.5$ and weight decay $1*10^{-5}$ and reduce the warmup epochs to 1. We use the LARS optimizer \citep{you:2017-LARS}. In the small-init case, we use $lr=0.25$ and additionally set $\alpha=0.133$. We train the networks for 500 epochs.

% \subsection{Figure}

% Despite the fact that our theoretical treatment assumes small initialization and  small learning rate, we find good qualitative agreement for realistic model configurations. 
% Indeed, as laid out in Appendix \ref{app:exp}, we train ResNet experiments without small initialization, using the prescribed learning rates and using LARS rather than SGD, and observe that eigenvalues separate into three groups: a thick band of grown eigenvalues, a sparse band of growing ones and another thick band of ones yet to grow. This occurs across experiments with Barlow Twins, SimCLR and VICReg losses (Figure \ref{fig:dynamics_realistic}). Although learning no longer takes place in discrete steps as in the small-init case, we observe that the eigenvalue distribution becomes bimodal throughout training with a spike at zero and a clear threshold that separates fully-grown eigenvalues from small ones (Figure \ref{fig:histograms_realistic}).

\subsection{Evaluating performance}

Our small init experiments cleanly display stepwise learning.
Here we show that these hyperparameter changes do not dramatically worsen model performance.
We evaluate performance by measuring the quality of learned hidden representations using a linear probe, and perform this evaluation many times throughout training.
Validation accuracies increase throughout training (Figure \ref{fig:val_acc}) as the learned representations improve over time, and generally small-init experiments fare similarly to the standard set, with a small performance drop of a few percent, but train slower as expected.

% We measure the quality of the learned representations using a linear probe, gauging validation accuracy throughout training. Validation accuracies increase throughout training (Figure \ref{fig:val_acc}), confirming that the learned representations improve over time, however the final accuracies are lower than the baseline numbers: SimCLR - $79.8\%$ ($9.4\%$ worse than baseline parameters), VICReg - $71.3\%$ ($9.6\%$ worse) and BYOL - $62.1\%$ ($23.6\%$ worse). This gap in validation accuracy is expected since the smaller learning rate and initial weights reduce training speed while training duration in epochs is kept constant (except for Barlow Twins). Additionally, all four experiments use a cosine annealing learning rate scheduler which additionally slows down training towards end of the experiment.

% The relative drops in performance for Barlow Twins and BYOL are also not entirely surprising. For Barlow Twins, unlike VICReg and SimCLR, we had to change an additional hyperparameter ($\lambda$) as mentioned above to a value that was significantly different from the default, higher performing value. BYOL is known to be somewhat more sensitive to hyperparameters and initializations, as it is not a contrastive method, and thus can more often end up with degenerate solutions \citep{fetterman:2020-understanding-byol}.

\begin{figure}
  \includegraphics[width=17cm]{img/updated/fig_acc_final.pdf}
  \vspace{-3mm}
  \caption{
    Validation accuracy of a linear probe throughout training for our standard and small-init experiments.
  }
  % \vspace*{6.5in}
  \label{fig:val_acc}
\end{figure}

\subsection{Measuring eigenvalues}
In the main text, all eigenvalues reported for \textit{embeddings} using the \textit{Barlow Twins loss} are the eigenvalues $\lambda_j(\mC)$, with $\mC$ the cross-covariance matrix across positive pairs as defined in Section \ref{sec:preliminaries}.
Eigenvalues reported for the embeddings of other losses are the eigenvalues $\lambda(\tilde{\mC})$ of the (centered) covariance matrix, with $\tilde{\mC} \equiv \mathbb{E}_x[(\vf(x) - \bar{\vf}) (\vf(x) - \bar{\vf})\T]$ and $\bar{\vf} \equiv \mathbb{E}_x[\vf(x)]$.
The eigenvalues reported in Figure \ref{fig:rep_vals} for hidden representations are all those of the covariance matrix (but of course computed on the hidden representations instead of the embeddings $\vf$).
We vary the matrix studied because our theory deals explicitly with $\mC$, but this cross-correlation matrix is not necessarily meaningful except in the embeddings of Barlow Twins.
In practice, however, we see similar results for either case (and in fact sometimes even see sharper learning steps in $\lambda_j(\tilde{\mC})$ than $\lambda_j(\mC)$ even for Barlow Twins).

\subsection{Measuring the empirical NTK}
In order to calculate the empirical NTK (eNTK), we employ functorch \citep{he:2021-functorch} and express the eNTK as a Jacobian contraction. We find that calculating the full kernel for realistic-sized networks is unfeasible due to memory and runtime constraints, primarily because of the large output dimension $d$ (which is 50 in the experiments of Appendix \ref{app:emb_pred}) as well as large batch sizes. To overcome this, we employ two tricks.
First, we reduce the model output to a scalar by taking an average of the outputs and calculate the eNTK for this modified function instead\footnote{More precisely, we take the average of the $d$ outputs and multiply it by $\sqrt{d}$, a scaling which recovers the original NTK matrix in the ideal, infinite-width case.}\footnote{Note that a simplification like this is necessary to use our theory: our theory assumes the NTK on $n$ samples is captured by a single matrix, whereas in reality it is a \textit{rank-four tensor} for a model with vector output.}. Second, we chunk up the computation for the full batch into minibatches depending on the size of $d$ and available GPU memory. With this approach, computing the eNTK on 1000 image pairs takes on the order of 1 minute on a single consumer GPU.