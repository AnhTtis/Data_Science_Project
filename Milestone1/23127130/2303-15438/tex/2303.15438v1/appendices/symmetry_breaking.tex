\section{Connection to spontaneous symmetry breaking in physical systems}
\label{app:symm}

In the main text, we noted that \citet{landau:1944-turbulence} encountered essentially the same dynamics in the study of turbulent fluid flow as we found for SSL.
This is due to the fact that both are simple processes of \textit{spontaneous symmetry breaking} (SSB), a phenomenon in which a system whose dynamics obey a symmetry spontaneously settles into an asymmetric state chosen as a result of the system's initial conditions.
In this appendix, we will explain how the dynamics of our model of SSL can be understood as a process of SSB.

Recall from Equation \ref{eqn:L_as_eigensum} that the loss of our toy model upon aligned initialization is
\begin{equation}
    \L = \sum_j \L_j = \sum_j (1 - \gamma_j s_j^2)^2 = \sum_j (1 - \bar{s}_j^2)^2,
\end{equation}
where we define $\L_j = (1 - \gamma_j s_j^2)^2$ and $\bar{s}_j = \gamma_j^{1/2} s_j$.
The quantity $\bar{s}_j$ is a (rescaled) singular value of $\mW$.
Singular values are canonically taken to be nonnegative (because one can always enforce this condition by appropriate choice of the singular vectors), but let us pretend for the present discussion that singular values may take any value along the real line.
Note that each singular value evolves via gradient descent according to its respective loss as
\begin{equation}
    \bar{s}_j'(t) = \frac{d \L_j}{d \bar{s}_j(t)}.
\end{equation}

\begin{figure}
  \centering
  \includegraphics[width=12cm]{img/ssb_cartoon_fig.png}
  \vspace{-3mm}
  \caption{
    \textbf{SSL is a process of symmetry breaking, whereas standard supervised learning is not.}
    \textbf{(A)}: Nondimensionalized loss landscape for singular value $\bar{s}_j$ in our linear model of Barlow Twins.
    The system can reach one of two minima depending on the sign of the initialization.
    \textbf{(B)}: Nondimensionalized loss landscape for an eigencoefficient in linearized \textit{supervised} learning with $c_j = 1$.
    There is only one local minimum.
    \textbf{(C)}: Example trajectories for the loss landscape in (A) from small init with different eigenvalues $\gamma_j$.
    \textbf{(D)}: Example trajectories for the loss landscape in (B) from small init with different time constants $\gamma_j$.
  }
  \label{fig:ssb_cartoon}
\end{figure}

Figure \ref{fig:ssb_cartoon}A depicts $\L_j$, with trajectories for various $\gamma_j$ shown in Figure \ref{fig:ssb_cartoon}C.
Note that this 1D landscape is symmetric and has two global minima at $\bar{s}_j = \pm 1$ and one local maximum at $\bar{s}_j = 0$.
When the system is initialized near zero, an unstable equilibrium of the dynamics, it will flow towards one basin or the other depending on its initial sign.
However, for small $\bar{s}_j(0)$, it takes a time $\tau_j = -\log(|\bar{s}_j(0)|) / 4 \gamma_j$ for $\bar{s}_j$ to escape the origin\footnote{To be more precise, it takes a time $\tau_j^{(\epsilon)} \approx -\log(\epsilon / |\bar{s}_j(0)|) / 4 \gamma_j$ to escape a ball of small constant radius $\epsilon > 0$ around the origin, and we drop the sub-leading-order contribution of $\epsilon$.}, whereas the system thereafter approaches the nearest minimum with the faster timescale $\tau'_j = 1/4\gamma_j$.
This slow escape from the origin is what leads to the sharp stepwise behavior we find.
% which is an inevitable consequence of the symmetry and differentiability of the landscape,

It should be noted that the quartic form of $\L_j$ appears in toy models of SSB across physics (see e.g. Landau-Ginzburg theory in \citet{kardar:2007-stat-phys-of-fields}) and is also the aforementioned model of \citet{landau:1944-turbulence}.
In these and other typical cases, SSB entails the breaking of a symmetry apparent at the outset of the problem such as invariance to translation, rotation, or inversion symmetry.
In the case of SSL, the symmetry is the transformation $\vf \rightarrow - \vf$, which does not change the value of the global loss\footnote{Our model in fact obeys the more general symmetry $\vf \rightarrow \mU \vf$ for any orthonormal matrix $\mU$, which is shared by SimCLR but not by VICReg or Barlow Twins with $\lambda \neq 1$.}.

Why does standard supervised learning in the NTK limit not exhibit stepwise behavior upon small initialization?
Following the analysis of \cite{jacot:2018}, the analogous modewise loss for a supervised setup takes the form $\L_j \propto (\bar{c}^*_j - \gamma_j c_j)^2$, with $c_j$ a learnable coefficient of Gram matrix eigenvector $j$ in the representer theorem coefficients of the learned function and $\bar{c}^*_j$ a constant.
We nondimensionalize in this case as $\bar{c}_j = \gamma_j c_j$.
As shown in Figure \ref{fig:ssb_cartoon}B, the landscape in the supervised case is merely quadratic and has no unstable equilibria, reflecting the lack of inversion symmetry in the loss.
Therefore $\bar{c}_j'(0)$ is not small, and the respective coefficients grow immediately rather than first undergoing a period of slow growth, as shown in Figure \ref{fig:ssb_cartoon}D.

The main surprising finding of our experiments is that SSL experiments with ResNets exhibit stepwise learning (especially upon small init) even far from the linear (i.e. lazy) regime.
It thus seems likely that one could derive the stepwise phenomenon from a much looser set of assumptions on the model class.
The view of stepwise learning as a simple consequence of SSB may be of use in the development of a more general theory in this sense.
This SSB view suggests that we may understand the growth of each new embedding direction from zero as the escape from a saddle point and use the fact that the local loss landscape around any saddle point of a given index (and with nondegenerate Hessian) is the same as any other up to rescaling of the domain.
We conjecture that stepwise learning will occur generically modulo edge cases (such as simultaneous growth of multiple directions) given an appropriate choice of loss function under minimal and realistic conditions on the empirical (i.e. time-varying) NTK of the model and assuming the final embeddings are the output of a linear readout layer.


