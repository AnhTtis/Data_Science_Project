\section{Kernelizing the linear model}
\label{sec:kernel_th}

Our discussion has so far dealt with an \textit{explicit} linear regression problem in which we have direct access to the data features $\vx_i$.
We used this explicit representation to construct $\mGamma \in \R^{m \times m}$, which lives in feature space.
However, many models of interest are linear with an \textit{implicit} feature space, with the output a linear function not of the input $x$ but rather of a fixed transformation of the input $\vphi(x)$.
Models of this class include kernel machines \citep{shawe:2004}, random feature models and deep neural networks in which only the last layer is trained \citep{rahimi:2007, lee:2018-nngp}, and infinite-width neural networks, the latter of which evolve under the NTK \citep{jacot:2018}.
While these models are equivalent to linear models, we do not have an explicit representation of the features $\vphi(x)$ (which may be large or even infinite-dimensional).
What we \textit{do} have access to is the model's \textit{kernel function} $\K(x,x') = \vphi(x)\T \vphi(x')$.

In our original linear model, the kernel is simply the inner product $\vx\T \vx'$.
Reinterpreting $\vx$ as $\vphi(x)$, any quantity expressed solely in terms of such inner products can still be computed after kernelization.
Our challenge, then, is to rewrite our theory of dynamics entirely in terms of such inner products so we may apply it to general kernel machines.

\subsection{Kernelized solution}

We will manage to do so.
Let our datasets be $\X \equiv \{x_1, ..., x_n\}$ and $\X' \equiv \{x'_1, ..., x'_n\}$.
Let us define the kernel matrix $\Kaa \in \R^{n \times n}$ such that $[\Kaa]_{ij} = \K(x_i,x_j)$ and define $\Kab$, $\Kba$, and $\Kbb$ analogously.
% Let us define $\mX \equiv [\vx_1, ..., \vx_n]\T$ and $\mX' \equiv [\vx'_1, ..., \vx'_n]\T$.
% Let us define $\Kaa \equiv \mX \mX\T$ and $\Kab, \Kba, \Kbb$ similarly.
Let us also define
\begin{equation}
\tilde{\mK} \equiv \begin{bmatrix}
\Kaa & \Kab\\
\Kba & \Kbb
\end{bmatrix},
\end{equation}
the kernel over the combined dataset, as well as
\begin{equation}
\!\!\!
\mZ \equiv
\frac{1}{2n}
\begin{bmatrix}
\Kab\Kaa & \Kab^2\\
\Kbb\Kaa & \Kbb\Kab 
\end{bmatrix} + \text{[transpose]}.
\end{equation}
% Finally, let us define
% $\mK_\mGamma \equiv (\tilde{\mK}^+)^{1/2} \mG (\tilde{\mK}^+)^{1/2} \in \R^{2n \times 2n}$,
% where $(\, \cdot \,)^+$ denotes a matrix pseudoinverse.
% this is confusing
Finally, let us define
$\mK_\mGamma \equiv \tilde{\mK}^{-1/2} \mZ \tilde{\mK}^{-1/2} \in \R^{2n \times 2n}$,
where $\tilde{\mK}^{-1/2}$ is interpreted as $(\tilde{\mK}^+)^{-1/2}$ if $\tilde{\mK}$ is degenerate.
The matrix $\mK_\mGamma$ is symmetric and akin to a kernelized version of $\mGamma$\footnote{Here $\mGamma = \frac{1}{2n} \sum_i \left(\vphi(x_i) \vphi(x'_i)\T + \vphi(x'_i) \vphi(x_i)\T\right)$ since we have reinterpreted $\vx$ as $\vphi(x)$. This $\mGamma$ would be sufficient to use our theory of Section \ref{sec:linear_th} were it not constructed of \textit{outer} products of $\vphi(x_i)$'s, which are inaccessible after kernelization.}.

Let $(\zeta_j, \vb_j)$ be the eigenvalues and normalized eigenvectors of $\mK_\mGamma$ indexed in descending eigenvalue order.
The kernelization of our theory is given by the following proposition.

\begin{proposition}[Kernelized solution] \label{prop:kernelized_sol} $ $
\begin{enumerate}[(a)]
\item All nonzero eigenvalues of $\mK_\mGamma$ are eigenvalues of
$\mGamma$
% \vspace{-2mm}
% \begin{equation}
%     \mGamma = \frac{1}{2n} \sum_i \left(\vphi(x_i) \vphi(x'_i)\T + \vphi(x'_i) \vphi(x_i)\T\right)
% \end{equation}
% \vspace{-2mm}
and vice versa.
\item \label{propclause:kernelized_embs}
% \todo{check matrix shapes}
The top spectral solution gives the embeddings
\begin{equation} \label{eqn:kernelized_top_spectral_embs}
    \vf(x) = \mU \tilde{\mS} [\vb_1 \ ... \ \vb_d]\T \tilde{\mK}^{-1/2} [\mK_{x \X} \ \mK_{x \X'}]\T
\end{equation}
with
$\mU$ an orthogonal matrix,
$\tilde{\mS} = \diag(\gamma_1^{-1/2}, ..., \gamma_d^{-1/2})$,
and
$\mK_{x \X}, \mK_{x \X'} \in \R^{1 \times n}$
such that 
$[\mK_{\X x}]_i = \K(x_i,x)$ and $[\mK_{\X' x}]_i = \K(x_i,x')$.
\item The top spectral solutions correspond to the embeddings $\vf(x)$ given by
\begin{equation}
    \underset{\vf}{\text{argmin}} \norm{\vf}_\K
    \ \ \ \text{s.t.} \ \ \
    \L(\vf) = 0.
\end{equation}
\end{enumerate}
\end{proposition}
% Proposition \ref{prop:kernelized_sol} allows one to 
We give the proof and an expression for $\vf(x,t)$, the embeddings over time from small (aligned) initialization, in Appendix \ref{app:proofs}.
These results allow us to predict both the training dynamics and final embeddings of our contrastive learning problem with a black-box kernel method.

\subsection{Implications of kernelized solution}
This kernelized solution has several interesting properties that we will briefly discuss here.

\textbf{Special case: $\X = \X'$.}
In the case in which the two views of the data are identical, one finds that $\mK_\mGamma = \tilde\mK$ and the model simply learns the top eigenmodes of its base (neural tangent) kernel.

% should some of this discussion be moved to the intro or to a final discussion section?
\textbf{SSL as kernel PCA.}
Proposition \ref{prop:kernelized_sol}\ref{propclause:kernelized_embs} states that the final embeddings are governed by the top $d$ eigenvectors of the kernel-like matrix $\mK_\Gamma$.
With our setup, then,
\textbf{contrastive learning with neural networks amounts to \textit{kernel PCA} in the infinite-width limit.}
This is analogous to the fact that standard supervised learning approaches kernel \textit{regression} in the infinite-width limit\footnote{This analogy can be furthered by noting that these equivalences both occur as $t \rightarrow \infty$ and both require small or zero initialization.}.
This is rather satisfying in light of the fact that kernel regression and kernel PCA are the simplest supervised and unsupervised kernel methods, respectively.

\textbf{The same theory works for multimodal SSL.}
The above solution holds for the infinite-width NTK limit of any neural network architecture.
It is worth noting that this includes multimodal setups such as CLIP \citep{radford:2021-clip} in which representations for the two datasets $\X$ and $\X'$ are generated by \textit{different} models, and the two datasets may be of different types.
We can view the two models as one combined model with two pathways, and since these pathways share no parameters, we have $\Kab = \Kba = \mathbf{0}$\footnote{In the parlance of the original linear model, the feature vectors of $\X$ and $\X'$ lie in orthogonal subspaces, and the model is tasked with discovering correlations across these subspaces.}.
Both our linear and kernelized solutions remain valid and nonvacuous in this setting.

% gah I feel like there's something here to say and I'm not saying it
% okay maybe I got it now
\textbf{Generalization on downstream tasks.}
The quality of a learned representation is often assessed by fitting a downstream function $g^*$ (such as an image classification) with a linear function of the representation as $\hat{g}(x) = \vbeta\T \vf(x)$.
Downstream task performance will be good if $g^*$ lies largely in the linear span of the components of $\vf(x)$.
Since Proposition \ref{prop:kernelized_sol}\ref{propclause:kernelized_embs} yields $\vf(x)$ in closed form, generalization can thus be assessed in our setting.
We leave this direction for future work.

\textbf{Mapping from initial to final kernel.}
Downstream task performance will be determined by the learned kernel
$\K_{\text{emb}}(x,x') \equiv \vf(x)\T \vf(x')$,
which contains all the information of $\vf$ save for the arbitrary rotation $\mU$.
We can thus think of SSL as a process which maps an initial, naive kernel $\K$ to a final kernel $\K_{\text{emb}}$ which has learned the structure of the data.
Many other poorly-understood processes in deep learning --- most notably that of feature learning --- also have the type signature \texttt{(initial kernel, data) $\rightarrow$ (final kernel)}, but there exist few closed-form algorithms with this structure.
While representation learning and supervised feature learning are different processes, it seems likely that they will prove to be related\footnote{See \citet{geirhos:2020-ssl-vs-sl} and \citet{grigg:2021-ssl-vs-sl} for evidence in this direction.}, and thus our closed-form solution for the final kernel may be useful for the study of feature learning.

