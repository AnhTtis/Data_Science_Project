\section{Introduction}
\label{sec:intro}

% fig: banner_figure
\begin{figure*}
  \centering
  \includegraphics[width=15cm]{img/updated/fig1_upd.pdf}
  \vspace{-3mm}
  \caption{
    \textbf{SSL methods learn embeddings one dimension at a time in a series of discrete steps.}
    \textbf{(A,B)}: Loss and embedding eigenvalues for an analytical model of Barlow Twins using a linear model trained on $n=500$ positive pairs from CIFAR-10. Solid curves show experimental trajectories, and dashed curves show predicted step timescales (A) and eigenvalues (B).
    Curves are plotted against effective time $t = \text{[lr]}\times\text{[step]}$.
    \textbf{(C,D)}: Loss and embedding eigenvalues for Barlow Twins using a deep ResNet on STL-10 with small initialization and $\lambda=1$.
    Learning takes place in discrete, well-separated steps, each of which entails a drop in the loss and an increase in the dimensionality of the embeddings by one.
  }
  \label{fig:banner_figure}
\end{figure*}


Self-supervised learning (SSL) has recently become a leading choice for representation learning using deep neural networks.
Joint embedding methods, a prominent class of SSL methods, aim to ensure that any two ``views" of the same input --- for example, two random crops, or an image and its caption --- are assigned similar representations.
Such self-supervised approaches have yielded a bounty of recent empirical breakthroughs across domains
\citep{hjelm:2018-deep-infomax,
wu:2018-unsupervised-feature-learning,
bachman:2019-cl-from-multiple-views,
% ye:2019-unsupervised-embedding-learning,
he:2020-moco,
henaff:2020-contastive-coding,
chen:2021-simsiam,
radford:2021-clip,
caron:2021-dino,
assran:2022-msn}

Despite SSL's simplicity, however, the field lacks a complete theoretical understanding of the paradigm. A promising place to start may be the fact that although there exist many different self supervised loss functions, these variants all achieve rather similar performance\footnote{For example, \citep{bardes:2021-vicreg} report that SimCLR, SwAV \citep{caron:2020-swav}, Barlow Twins, VICReg, and BYOL \citep{grill:2020-byol} all score within a few percent of each other on ImageNet pretraining tasks.}.
This similarity suggests that there may exist basic commonalities in their learning behavior which are as yet unidentified.

In this work, we propose a candidate for such a shared behavior. We present evidence that the high-dimensional embeddings of Barlow Twins \citep{zbontar:2021-barlow-twins}, SimCLR \citep{chen:2020-simclr}, and VICReg \citep{bardes:2021-vicreg} are learned one dimension at a time in a series of discrete learning stages (see Figure \ref{fig:banner_figure}). These embeddings are initially low-rank and increase in rank by one as each dimension is learned.

To reach this conclusion, we first study the Barlow Twins loss \citep{zbontar:2021-barlow-twins} applied to a linear model. The training of this model takes the form of a matrix factorization problem, and we present exact solutions to the training dynamics from small initialization. These solutions reveal that representation learning occurs in discrete, well-separated steps, each of which learns a top eigendirection of a certain contrastive operator which characterizes the spectral bias of the model on the dataset. We also present a kernelization of our theory that allows its application to generic kernel machines, a class which importantly includes infinite-width neural networks.

Finally, we empirically examine the training of Barlow Twins, SimCLR, and VICReg using ResNets with various initializations and hyperparameters and in all cases clearly observe the stepwise behavior predicted by our analytical model.
This behavior is most apparent upon small parameterwise initialization (Figures \ref{fig:banner_figure}C,D and \ref{fig:deep_net_exps}C-F) but persists even in realistic configurations (Figure \ref{fig:dynamics_realistic}).
In light of this agreement, we suggest that SSL can be understood as a sequential process of learning orthogonal scalar functions in order of importance.
This simple picture of SSL has the potential to both provide a useful lens for understanding the training behavior of self supervised networks and help guide the design of new methods.

Concretely, our contributions are as follows.
\begin{itemize}
    \item We propose a minimal model of Barlow Twins and solve its dynamics from near-zero initialization.
    \item We extend our model to arbitrary kernel machines including infinite-width neural networks and give closed-form equations for the final representations in terms of the training kernel.
    \item We validate our theory using Barlow Twins, VICReg and SimCLR with deep ResNets and find good qualitative agreement.
\end{itemize}

\subsection{Motivation}

Our theoretical approach is motivated by the infinite-width ``neural tangent kernel" (NTK) limit of deep neural networks, in which the model's training dynamics become linear in its parameters \citep{jacot:2018, lee:2019-ntk}.
This limit has been immensely fruitful for the study of ordinary supervised learning with deep neural networks, yielding many useful insights and intuitions regarding training and generalization, many of which hold well even for the finite, highly nonlinear models used in practice\footnote{An incomplete list of examples includes
\citet{du:2019-convergence,
cao:2019-spectral-bias,
yang:2019-tensor-programs-I,
tancik:2020-nerf-with-fourier-features,
fort:2020-deep-learning-versus-kernel-learning,
yang:2021-tensor-programs-IV,
atanasov:2021-silent-alignment}
for training and
\citet{mei:2019-double-descent,
allen:2019-learning-in-wide-nets,
yang:2019-hypercube-spectral-bias,
arora:2019-NTK-gen-bound,
canatar:2021-spectral-bias,
simon:2021-eigenlearning,
mallinar:2022-taxonomy,
loureiro:2021-learning-curves,
wei:2022-more-than-a-toy}
for generalization.}.
Kernel regression with the NTK now serves as a useful simple model for supervised deep neural networks.
Our aim here is to play the same trick with \textit{self-supervised} learning, obtaining an analytical model that can be interrogated to answer questions about SSL training.
% In doing so, we find the satisfying result that, just as kernel regression is a toy model of supervised learning, kernel \textit{PCA} is a toy model of SSL (Section \ref{sec:kernel_th}).

A further motivation is the fact that deep learning methods that learn flexible \textit{representations} from unlabeled data, rather than a narrow target function from labeled data, are increasingly dominant in the modern deep learning ecosystem \citep{brown-2020-gpt3, ramesh:2022-dalle2, chowdhery:2022-palm}. Despite this, many theoretical tools have been developed exclusively in the context of supervised learning.
We aim to close this gap by porting theoretical tools from the study of supervised learning, such as kernel equivalences and spectral bias, over to the study of representation learning.

Similarly, perhaps the biggest open problem in the theory of supervised deep learning is understanding the process of \textit{feature learning} at hidden layers.
It is known that networks in the kernel limit do not adapt their hidden representations to data, so various works have explored avenues beyond the NTK, though none of these efforts have yet yielded simple closed-form equations for the features learned after training \citep{yang:2021-tensor-programs-IV, roberts:2022-principles-of-dl-theory, bordelon:2022-feature-learning-dmft, jacot:2022-dnns-with-weight-decay}.
However, we find that networks in the kernel limit \textit{do} learn nontrivial representations when trained with self supervised learning, and, what is more, we can obtain closed-form expressions for these final representations in terms of the NTK (Section \ref{sec:kernel_th}).
Our work presents an avenue by which feature and representation learning might be studied purely in terms of kernels.