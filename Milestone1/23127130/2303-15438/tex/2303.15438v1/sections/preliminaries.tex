\section{Preliminaries}
\label{sec:preliminaries}

% studying an ordinary linear representation learning problem
We will study an ordinary linear model trained in a contrastive fashion.
Suppose we have a dataset of $n$ ``positive pairs" $\vx_i, \vx'_i \in \R^m$ for $i \in 1...n$\footnote{We use a finite dataset of $n$ positive pairs for simplicity, but our theory works equally well when optimizing on the population loss of a data distribution, which simply corresponds to the case $n \rightarrow \infty$.}.
% jointly from a symmetric distribution $p(\cdot,\cdot)$\footnote{For example, the distribution $p$ might be defined by first sampling a base image and applying two random augmentations.}.
% note about a fixed finite dataset being fine somewhere
Our model will consist of a linear transformation to a $d$-dimensional embedding parameterized as $\vf(\vx) \equiv \mW \mX$ with $\mW \in \R^{d \times m}$.
We would like to learn a transformation $\mW$ such that positive pairs have similar embeddings but the full set of embeddings maintains high diversity.

To encourage such representations, Barlow Twins prescribes a loss function which pushes the cross-correlation matrix between $\vf(\vx)$ and $\vf(\vx')$ towards the identity matrix.
We will use a slightly simplified variant of the Barlow Twins loss given by
\begin{equation} \label{eqn:loss_fn}
\L = \norm{\mC - \mI_d}_F^2,
\end{equation}
where $\norm{\cdot}_F$ is the Frobenius norm and $\mC \in \R^{d \times d}$ is the cross-correlation matrix given by
\begin{equation}
\mC \equiv \frac{1}{2n} \sum_i \left( \vf(\vx_i) \vf(\vx_i')\T + \vf(\vx_i') \vf(\vx_i)\T \right).
\end{equation}
Compared to the original loss of \citep{zbontar:2021-barlow-twins}, we have set a hyperparameter $\lambda$ to $1$, removed batchnorm in the definition of $\mC$, and symmetrized $\mC$.
% We provide a more detailed comparison in Appendix [APPENDIX].

We will initialize $\mW = \mW_0$ and train with full-batch gradient flow as
\begin{equation} \label{eqn:dW_dt}
    \frac{d\mW}{dt} = - \nabla_\mW \L.
\end{equation}
We wish to understand both the dynamics of this training trajectory and the final weights $\mW_\infty \equiv \lim_{t \rightarrow \infty} \mW$.