\section{Experiments}
\label{sec:exp}

\begin{figure*}
  \centering
  \includegraphics[width=17cm]{img/updated/fig2_upd2.pdf}
  \vspace{-3mm}
  \caption{
    \textbf{Stepwise learning in SSL with nonlinear neural networks trained from small initialization.}
    Losses and embedding eigenvalues as a function of time $t = \text{[lr]}\times\text{[step]}$ for
    \textbf{(A, B)} a single-hidden-layer MLP trained with our simplified Barlow Twins loss,
    \textbf{(C, D)} a ResNet trained with SimCLR loss, and
    \textbf{(E, F)} a ResNet trained with VICReg loss, all trained on STL-10.
  }
  \label{fig:deep_net_exps}
\end{figure*}

Since our theory was derived in the case of linear models, a natural question is whether it is useful and informative even for the study of practical deep neural networks.
\textbf{Here we present evidence that the stepwise learning phenomenon we identified occurs even in realistic SSL setups with ResNet-50 encoders, and even with losses besides Barlow Twins.}
We sketch experiments here and provide more details in Appendix \ref{app:exp}.
In all figures, reported eigenvalues are those of the cross-covariance matrix $\mC$ when the loss is Barlow Twins, and are those of the feature covariance matrix of the embeddings when the loss is SimCLR or VICReg unless otherwise stated, though in practice these give quite similar results.

We perform a series of experiments with increasingly realistic models.
As a first nonlinear experiment, we revisit our previous numerical simulation of our linear model (Section \ref{subsec:linear_model_sim}) and simply replace the model with a width-2048 \text{ReLU} MLP with a single hidden layer using the standard PyTorch parameterization.
This model is fairly wide, and so we expect its NTK to remain roughly fixed.
Results are shown in Figure \ref{fig:deep_net_exps} (A,B).

We next train practical deep SSL methods with some hyperparameters modified slightly so as to better align with our theory.
We train VICReg, SimCLR, and Barlow Twins using a ResNet-50 encoder and an MLP projection head on the full STL-10 dataset \citep{coates:2011-stl10}.
These runs use small parameterwise initialization and small learning rate, but important hyperparameters such as momentum and weight decay are otherwise kept largely as in the original publications\footnote{We also changed batch size for practical reasons related to calculating the kernel and set $\lambda=1$ for Barlow Twins. See Appendix \ref{app:exp} for full details.}.
The results, plotted in Figure \ref{fig:banner_figure}(C,D) (Barlow Twins) and Figure \ref{fig:deep_net_exps}(C-F) (SimCLR and VICReg), clearly show the same stepwise learning behavior seen in simpler settings\footnote{Unlike with the three reported methods, we did not immediately see stepwise learning with BYOL.}.
Agreement with predictions using the initial NTK is poor as expected from work on ResNet NTKs \citep{fort:2020-deep-learning-versus-kernel-learning, vyas:2022-limitations-of-the-ntk-for-generalization} and we omit theory curves.

% \begin{figure*}
%   \includegraphics[width=17cm]{img/updated/fig5_realistic_upd.png}
%   \vspace{-3mm}
%   \caption{
%     \textbf{Stepwise learning in SSL with ResNets with standard initialization and hyperparameters.}
%     Embedding eigenvalues over time for \textbf{(A)} Barlow Twins, \textbf{(B)} SimCLR, and \textbf{(C)} VICReg.
%   }
%   % \vspace*{6.5in}
%   \label{fig:dynamics_realistic}
% \end{figure*}

% \begin{figure*}
%   \includegraphics[width=17cm]{img/updated/fig6_hists_updated.pdf}
%   \vspace{-3mm}
%   \caption{
%     \textbf{Bimodal distribution of embedding eigenvalues shown over time for SSL with ResNets with standard initialization and hyperparameters.}.
%     Histograms of embedding eigenvalues at selected times throughout training for \textbf{(A)} Barlow Twins, \textbf{(B)} SimCLR, and \textbf{(C)} VICReg.
%   }
%   % \vspace*{6.5in}
%   \label{fig:histograms_realistic}
% \end{figure*}

Finally, we repeat these experiments in fully realistic settings, with standard initialization and learning rates.
Even though eigenvalues are no longer uniformly small at initialization, we still see stepwise learning in their growth, as shown in Figure \ref{fig:dynamics_realistic}. 
Specifically, we see for each loss that eigenvalues separate clearly into a band of eigenvalues that have not yet grown and a band of those that have fully grown with a sparse region in between, and eigenvalues move from the lower band to the upper band as training proceeds.
This interpretation is affirmed by eigenvalue histograms throughout training, which reveal bimodal distributions as shown in Figure \ref{fig:histograms_realistic}.
Our view is that stepwise learning --- i.e., the sequential growth of the rank of the embeddings, corresponding to the learning of orthogonal functions --- is generic, and is simply made cleaner upon taking small initialization\footnote{Figure \ref{fig:dynamics_realistic} suggests that, at least for Barlow Twins and VICReg, embedding directions may be learned ``a few at a time" rather than one at a time with realistic hyperparameters, but when the embedding dimension is also realistically large, this difference is minor.}.

\textbf{Stepwise behavior in hidden representations.}
All theory and experiments thus far have studied the final embeddings of the network.
However, in practice, one typically uses the hidden representations of the network for downstream tasks.
In light of this, we repeat our experiments with small initialization but measure eigenvalues computed from the hidden representations.
Results are reported in Figure \ref{fig:rep_vals}.
Remarkably, despite the fact that our theory says nothing about hidden representations, \textit{we still clearly see learning steps coinciding with the learning steps at the embeddings.}
Understanding the couplings in these dynamics is an interesting direction for future work.

\textbf{Theoretical predictions of embeddings with the empirical NTK.}
Our theory predicts not only the occurrence of learning steps in the model embeddings but also the precise embedding function learned (up to an orthogonal transformation) in terms of the model's NTK.
As a final experiment, we compare these predictions, evaluated using the empirical NTK after training, with the true embeddings learned in ResNet-50 experiments from small initialization with $d=50$.
These predicted embeddings have significant similarity with the true learned embeddings.
We give details of this experiment in Appendix \ref{app:emb_pred}.