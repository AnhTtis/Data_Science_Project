\section{Related works}
\label{sec:related_works}

\textbf{Theoretical study of SSL.}
Many recent works have sought to understand SSL through varied lenses including statistical learning theory
\citep{arora:2019-cl-learning-theory, wei:2020-self-training-theory, nozawa:2021-understanding-negative-samples, ash:2021-investigating-negative-samples, haochen:2021-provable-guarantees-for-cl},
information theory
\citep{tsai:2020-ssl-multi-view, tsai:2021-ssl-rel-pred-coding, tosh:2021-cl-and-posterior-info, tosh:2021-cl-and-redundancy},
loss landscapes and training dynamics
\citep{tian:2020-understanding-ssl, wang:2020-understanding-cl-thru-align-and-unif, chen:2021-properties-of-contrastive-losses, tian:2021-understanding-ssl-no-neg-pairs, jing:2021-ssl-dim-collapse, wen:2021-cl-feature-learning, pokle:2022-noncontrastive-cl-landscapes, ziyin:2022-loss-landscapes}, assran:2022-ssl-hidden-cluster-prior,
and kernel and spectral methods
\citep{kiani:2022-joint-embedding, haochen:2021-provable-guarantees-for-cl, johnson:2022-optimal-contrastive-reps}.
Our work unifies dynamical and kernel perspectives of SSL pioneered by prior works as follows.

\citet{ziyin:2022-loss-landscapes} showed that most common SSL loss functions take the form of $\Tr{\mW \mA \mW\T} + \Tr{(\mW \mB \mW\T)^2}$ for symmetric matrices $A$ and $B$ when expanded about the origin.
This is the form of the loss function whose dynamics we solve to obtain exact optimization trajectories, and our exact solution can be adapted to other losses of the same form.
% Therefore, we expect the dynamics we derive are not just applicable to Barlow Twins but also to a rather wide range of loss functions including SimCLR.
% This expectation is indeed borne out by our experiments.

In influential work, \citet{haochen:2021-provable-guarantees-for-cl} showed that the optimal representation under a particular contrastive loss consists of the top Laplacian eigenmodes of a certain ``augmentation graph" defined over the data space.
However, as discussed by \citet{saunshi:2022-understanding-cl-req-ind-bias}, their approach is model-agnostic, and these optimal representations are usually highly degenerate in realistic cases\footnote{Specifically, in order to be nonvacuous, their approach requires that it is common that a particular image will appear as an augmentation of \textit{multiple different base images} so that the augmentation graph is largely connected. This will virtually never happen with realistic dataset sizes and step counts.}.
This degeneracy must somehow be broken by the inductive bias of the model and optimization procedure.
Our results complete this picture, characterizing this inductive bias for linearized SSL models as a bias towards small RKHS norm
and thereby identifying which of the many zero-loss solutions is reached in practice.

In a similar spirit as our work, \citet{balestriero:2022-ssl-and-spectral-methods} and \citet{kiani:2022-joint-embedding} study SSL with linear models and kernel methods, deriving and comparing optimal representations for certain toy losses.
They, too, find that kernelized SSL preferentially learns the top eigenmodes of certain operators, though their final representations differ from ours.
One key difference is that these works jump to optima by solving for minimizers of the loss, whereas our focus is on the dynamics of training that lead to optimal representations.
Since the loss often has many inequivalent global minimizers in practice (as is indeed the case for Barlow Twins), understanding these dynamics are necessary for determining \textit{which} global minimum will actually be found by gradient descent.

% As is the story across most of deep learning, the gap between the theory and practice of SSL is rather large.
% Few substantial theoretical works so far have been able to validate their results with large-scale experiments as we would like to be able to do with our theories.
% We note that \textit{we do this}, unambiguously observing our predicted phenomenon of stepwise learning with deep ResNets trained in a near-standard fashion.
% This is made possible by our choice of simplications and focus on model dynamics, which have historically proven more easily understood than generalization.


\textbf{Matrix factorization and deep linear networks.}
Our minimal model of Barlow Twins turns out to be closely related to classic matrix factorization problems \citep{gunasekar:2017-implicit-reg-in-mf, li:2018-matrix-sensing, arora:2019-implicit-reg-in-deep-mfac, chi:2019-nonconvex-opt-meets-mfac} For example, our Equation \ref{eqn:loss_WGW} to Equation 3.1 of \citet{li:2018-matrix-sensing}.
Matrix factorization problems are often studied as paradigmatic examples of tasks with many inequivalent zero-loss solutions, with the question then to characterize the model's inductive bias and understand which of these solutions is actually reached by gradient descent.
This is often doable under an assumption of small initialization, from which gradient descent finds the solution which minimizes a certain matrix norm \citep{gunasekar:2017-implicit-reg-in-mf, li:2018-matrix-sensing}, and this is indeed the case in our model.
% hmm, this makes me think this should go in the signifcance part of the intro, not the related works section
Our work draws a new link between SSL and matrix factorization, and we propose that degenerate matrix factorization problems can serve as illuminating simplified models for representation learning tasks more broadly.

The dynamics of our analytical model (Proposition \ref{prop:exact_dynamics}) bear a striking resemblance to the dynamics of deep linear networks\footnote{Note that the training of a deep linear network is itself an asymmetric matrix factorization problem.} \citep{fukumizu:1998-deep-linear-nets, saxe:2013-deep-linear-nets, du:2018-deep-linear-net-optimization, arora:2018-deep-linear-net-optimization, jacot:2021-saddle-to-saddle}.
Deep linear networks initialized near zero also exhibit learning in discrete, well-separated stages, each of which entails the learning of one singular direction in the model function\footnote{Furthermore, in both cases, the loss plateaus between consecutive learning stages occur when the model passes near saddle points, with the index of the saddle point decreasing by one each stage \citep{jacot:2021-saddle-to-saddle}.}.
However, the problems differ in key ways: in our setting, the stagewise learning behavior is a result of the contrastive loss function, not the depth of the network, and our problem has many inequivalent global minima, unlike the typical deep linear setting.

\textbf{Physics and symmetry breaking.}
Interestingly, \citet{landau:1944-turbulence} encountered the same differential equation we find for embedding eigencoefficients in the study of the onset of turbuluent fluid flow.
This is a consequence of the fact that both representation learning and the onset of turbulence are processes of spontaneous symmetry breaking.
We make this connection explicitly in Appendix \ref{app:symm}.