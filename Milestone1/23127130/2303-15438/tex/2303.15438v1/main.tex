% TODOS:
% experimental stuff
% explicitly give the kernelization of Eq 18 in the appendix
% use a better notation for \Gamma^{\le k}
% cite RankMe

\documentclass[nohyperref]{article}

\usepackage[accepted]{icml2023} % note: should uncomment line 542 in icml2023.sty to add back conference details.

%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\usepackage{macros}

\icmltitlerunning{On the stepwise nature of SSL}

\begin{document}

\twocolumn[
\icmltitle{On the stepwise nature of self-supervised learning}

\newif\ifarxiv
\arxivfalse

% author stuff
{
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{James B. Simon}{berk,gi}
\icmlauthor{Maksis Knutins}{gi}
\icmlauthor{Liu Ziyin}{tok}
\icmlauthor{Daniel Geisz}{berk}
\icmlauthor{Abraham J. Fetterman}{gi}
\icmlauthor{Joshua Albrecht}{gi}
\end{icmlauthorlist}

\icmlaffiliation{berk}{UC Berkeley}
\icmlaffiliation{gi}{Generally Intelligent}
\icmlaffiliation{tok}{University of Tokyo}

\icmlcorrespondingauthor{James Simon}{james.simon@berkeley.edu}
}

\icmlkeywords{self-supervised learning, SSL, contrastive learning, kernels, kernel methods, infinite width, NTK, deep learning, generalization, inductive bias, implicit bias, spectral bias, implicit regularization, Barlow Twins, SimCLR, CLIP}

\vskip 0.3in
]

\printAffiliationsAndNotice{}

\begin{abstract}

We present a simple picture of the training process of self-supervised learning methods with joint embedding networks.
We find that these methods learn their high-dimensional embeddings one dimension at a time in a sequence of discrete, well-separated steps.
We arrive at this conclusion via the study of a linear model of Barlow Twins applicable to the case in which the trained network is infinitely wide.
% We arrive at this picture via the study of Barlow Twins applied to models with \textit{linear dynamics} such as kernel machines or wide neural networks.
We solve the training dynamics of this model from small initialization, finding that the model learns the top eigenmodes of a certain contrastive kernel in a stepwise fashion, and obtain a closed-form expression for the final learned representations.
Remarkably, we then see the same stepwise learning phenomenon when training deep ResNets using the Barlow Twins, SimCLR, and VICReg losses.
Our theory suggests that, just as kernel regression can be thought of as a model of supervised learning, \textit{kernel PCA} may serve as a useful model of self-supervised learning.%
\numberlessfootnote{Code to reproduce results available at \href{https://gitlab.com/generally-intelligent/ssl_dynamics}{\url{https://gitlab.com/generally-intelligent/ssl_dynamics}}.}
\end{abstract}

\input{sections/intro.tex}
\input{sections/related_works.tex}
\input{sections/preliminaries.tex}
\input{sections/linear_theory.tex}
\input{sections/kernel_theory.tex}
\input{sections/experiments.tex}
\input{sections/conclusions.tex}

% This is equivalent to performing kernel regression with the induced kernel
% \begin{equation}
%     \K_{\text{ind}}(x,x') = \vf(x)\T \vf(x').
% \end{equation}
% % This fitting will typically have low error if the downstream target function is mostly captured by the top eigenspace of $\K_{\text{ind}}$.
% In the kernel setting, Equation [EQUATION] for $\vf(x)$ yields a closed-form expression for $\K_{\text{ind}}$, and thus generalization performance can be assessed.
% Downstream task performance will be good if $g^*$ lies largely in the linear span of the components of $\vf(x)$.

% do we want to include this speculation?

% We note that this is an interesting direction for future work.
% If the data is sampled from a known distribution, then the statistics of this downstream performance are given by a random matrix problem which might be solved by recent techniques from [CITATION] for the study of kernel regression, giving generalization performance as a function of $n$, $n'$, $d$, and task-model alignment.

% We note that, if the pretraining and downstream data are sampled from known distributions, then the problem of assessing average-case downstream performance is reduced to a random matrix theory problem which might be solved with tools developed in many recent works on kernel regression with random data [LOTS OF CITATIONS].
% Such a solution would yield many interesting fruit.
% For example, observe that, at finite $n'$, downstream performance will be poor when $d$ is small \textit{and} when $d$ is large ($d \gg n'$).
% Where is the optimum?
% The optimal size of $d$ is a practically-relevant question one can interrogate in the kernel setting.


% \begin{equation}
% \mK_{\mGamma} \equiv
% \begin{bmatrix}
% \mK_{aa}\mK_{aa} + \mK_{aa}\mK_{aa} & \mK_{aa}\mK_{aa} + \mK_{aa}^2\\
% \mK_{aa}\mK_{aa} + \mK_{aa}^2 & \mK_{aa}\mK_{aa} + \mK_{aa}\mK_{aa}
% \end{bmatrix}
% \end{equation}

% These stepwise dynamics will be reflected in the loss: starting at $\L(0) \approx d$, it will decrease by one as each mode is learned, following
% \begin{equation} \label{eqn:stepwise_loss}
%     \L(t) \approx \sum_{j | \tau_j > t} 1.
% \end{equation}
% They will also be reflected in the eigenvalues of $\mC$, which are precisely $\lambda_i(\mC) = \gamma_j \s_j^2(t)$.




% so this means that the loss function will descend in steps. discuss small init here
% give the conjecture that general small inits cleave close to the optimal trajectory
% give numerical evidence

% , though there exists a cleaner approach which we will opt for.
% Note that we can eigendecompose the original loss of Equation [EQUATION] as
% \begin{equation}
%     \L = \sum_{j=1}^d \left(1 - \lambda_j (\mC)\right)^2,
% \end{equation}
% where $\lambda_j(\cdot)$ returns the $j$-th eigenvalue of its argument.
% Barlow Twins thus pushes each eigenvalue of $\mC = \mW \mGamma \mW\T$ toward one independently with a quadratic penalty.
% The eigenvalues of $\mW \mGamma \mW\T$ are precisely
% \begin{equation}
% \end{equation}


% We can give this loss an interpretation by eigendecomposing it as
% \begin{equation}
%     \L = \sum_{j=1}^d \left(1 - \lambda_j (\mW \mGamma \mW\T)\right)^2
% \end{equation}
% % \begin{equation}
% %     \L = \sum_{j=1}^d \left(1 - \lambda_j (\mC)\right)^2
% % \end{equation}



% dynamics decomposed along singular directions


% \begin{equation}
%     % \diag{(\mS_0)} = (s^{(1)}_0, ..., s^{(d)}_0)
%     \diag{(\mS_0)} = (s_1(0), ..., s_d(0))
% \end{equation}

% \begin{equation}
%     % \diag{(\mS_0)} = (s^{(1)}_0, ..., s^{(d)}_0)
%     \diag{(\mS(t))} = (s_1(t), ..., s_d(t))
% \end{equation}

% solution from small init captures the top singular directions of $\mGamma$

% To encourage such representations, Barlow Twins [CITATION] prescribes a loss function 
% \begin{equation}
% \L^\text{(BT)} = \sum_{j=1}^d \left(\mC^\text{(BT)}_{jj} - 1\right)^2 + \lambda \sum_{\substack{j,k=1 \\ j \neq k}}^d \left( \mC^\text{(BT)}_{jk} \right)^2
% \end{equation}
% where $\lambda$ is a tradeoff parameter which weights the importance of the off-diagonals and where $\mC^\text{(BT)}$ is defined as
% \begin{equation}
% \mC^\text{(BT)}_{jk} \equiv
% \frac{
% \sum\limits_{i=1}^n f_j(\vx_i) f_k(\vx_i')
% }{
% \sqrt{\sum\limits_{i=1}^n f_j^2(\vx_i)} \sqrt{\sum\limits_{i=1}^n f_j^2(\vx_i')}
% }.
% \end{equation}
% The Barlow Twins loss function reaches zero when $\mC^\text{(BT)} = \mI_d$.
% note for comparison: the symmetrization of C (which enables our eigenanalysis) oughta happen naturally for symmetric distros at large batch size anyways.


% define model. embedding dimension d

% define loss function

\section*{Acknowledgements}
The authors thank Bobak Kiani, Randall Balestreiro, Vivienne Cabannes, Kanjun Qiu, Ellie Kitanidis, Bryden Fogelman, Bartosz Wr\'{o}blewski, Nicole Seo, Nikhil Vyas, and Michael DeWeese for useful discussions and comments on the manuscript. JS gratefully acknowledges support from the National Science Foundation Graduate Fellow Research Program (NSF-GRFP) under grant DGE 1752814.


\bibliography{ml_refs}

\bibliographystyle{icml2023}

\appendix
\onecolumn

\input{appendices/additional_figures}
\input{appendices/proofs.tex}
\input{appendices/derivation.tex}
\input{appendices/experiments.tex}
\input{appendices/emb_preds}
\input{appendices/symmetry_breaking}
\input{appendices/speedup}

\end{document}