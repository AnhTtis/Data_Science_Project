
\documentclass[nohyperref]{article}

\usepackage[accepted]{icml2023}

\usepackage[textsize=tiny]{todonotes}

\usepackage{macros}

\icmltitlerunning{On the Stepwise Nature of SSL}

\begin{document}

\twocolumn[
\icmltitle{On the Stepwise Nature of Self-Supervised Learning}

\newif\ifarxiv
\arxivfalse

{
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{James B. Simon}{berk,gi}
\icmlauthor{Maksis Knutins}{gi}
\icmlauthor{Liu Ziyin}{tok}
\icmlauthor{Daniel Geisz}{berk}
\icmlauthor{Abraham J. Fetterman}{gi}
\icmlauthor{Joshua Albrecht}{gi}
\end{icmlauthorlist}

\icmlaffiliation{berk}{UC Berkeley}
\icmlaffiliation{gi}{Generally Intelligent}
\icmlaffiliation{tok}{University of Tokyo}

\icmlcorrespondingauthor{James Simon}{james.simon@berkeley.edu}
}

\icmlkeywords{self-supervised learning, SSL, contrastive learning, kernels, kernel methods, infinite width, NTK, deep learning, generalization, inductive bias, implicit bias, spectral bias, implicit regularization, Barlow Twins, SimCLR, CLIP}

\vskip 0.3in
]

\printAffiliationsAndNotice{}

\begin{abstract}

We present a simple picture of the training process of joint embedding self-supervised learning methods.
We find that these methods learn their high-dimensional embeddings one dimension at a time in a sequence of discrete, well-separated steps.
We arrive at this conclusion via the study of a linearized model of Barlow Twins applicable to the case in which the trained network is infinitely wide.
We solve the training dynamics of this model from small initialization, finding that the model learns the top eigenmodes of a certain contrastive kernel in a stepwise fashion, and obtain a closed-form expression for the final learned representations.
Remarkably, we then see the same stepwise learning phenomenon when training deep ResNets using the Barlow Twins, SimCLR, and VICReg losses.
Our theory suggests that, just as kernel regression can be thought of as a model of supervised learning, \textit{kernel PCA} may serve as a useful model of self-supervised learning.
\end{abstract}

\input{sections/intro.tex}
\input{sections/related_works.tex}
\input{sections/preliminaries.tex}
\input{sections/linear_theory.tex}
\input{sections/kernel_theory.tex}
\input{sections/experiments.tex}
\vspace{-2mm}
\input{sections/conclusions.tex}


























\vspace{-3mm}
\section*{Acknowledgements}
\vspace{-2mm}
The authors thank Bobak Kiani, Randall Balestreiro, Vivien Cabannes, Kanjun Qiu, Ellie Kitanidis, Bryden Fogelman, Bartosz Wr\'{o}blewski, Nicole Seo, Nikhil Vyas, and Michael DeWeese for useful discussions and comments on the manuscript. JS gratefully acknowledges support from the National Science Foundation Graduate Fellow Research Program (NSF-GRFP) under grant DGE 1752814.


\bibliography{ml_refs}

\bibliographystyle{icml2023}

\appendix
\onecolumn

\input{appendices/additional_figures}
\input{appendices/proofs.tex}
\input{appendices/derivation.tex}
\input{appendices/experiments.tex}
\input{appendices/emb_preds}
\input{appendices/symmetry_breaking}
\input{appendices/speedup}

\end{document}