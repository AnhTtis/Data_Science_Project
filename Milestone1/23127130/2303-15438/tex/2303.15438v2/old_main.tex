\documentclass[nohyperref]{article}

\usepackage{icml2022}


\usepackage[textsize=tiny]{todonotes}

\usepackage{macros}

\icmltitlerunning{CL theory dynamics paper}

\begin{document}

\twocolumn[
\icmltitle{A theory of the dynamics of CL for linearized models}

{
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Firstname1 Lastname1}{equal,yyy}
\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
\icmlauthor{Firstname3 Lastname3}{comp}
\icmlauthor{Firstname4 Lastname4}{sch}
\icmlauthor{Firstname5 Lastname5}{yyy}
\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
\icmlauthor{Firstname7 Lastname7}{comp}
\icmlauthor{Firstname8 Lastname8}{sch}
\icmlauthor{Firstname8 Lastname8}{yyy,comp}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
\icmlaffiliation{comp}{Company Name, Location, Country}
\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}
}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

\printAffiliationsAndNotice{\icmlEqualContribution} %

\begin{abstract}
[Note: this abstract blows]

Working notes for the CL theory project.
We exactly solve the dynamics for CL with a Barlow-Twins-like loss function with a linearized model.
We find that, throughout training, CL sequentially learns eigenmodes of a certain contrastive kernel depending on the model and data distribution, with the qualitative features of the dynamics determined largely by the structure of the loss surface around the origin.
Our theory explains several commonly-seen behaviors of CL experiment, including the initial dimensional collapse and the subsequent increase in dimension, and it explains why certain loss functions -- namely VICReg -- train faster than others.
We check our theory against ResNets trained using a variety of leading contrastive frameworks and find good (qualitative? quantitative?) agreement in all cases.

\end{abstract}

\section{Introduction}
\label{sec:intro}

\section{Exact gradient flow paths for a linearized model with Barlow Twins loss}

We will first set up a learning problem in which a linear model is optimized to minimize a loss similar to that of Barlow Twins.
We will then describe the optimization trajectories for this loss starting from a certain class of initializations.

\subsection{Setup}

Consider contrastive learning with positive pairs sampled jointly from some symmetric distribution as $\vx,\vx' \sim p$, with $\vx,\vx' \in \mathbb{R}^m$.
Let us define
$\mSigma \equiv \E{\vx}{\vx \vx\T}$ and
$\mDelta \equiv \E{\vx,\vx'}{(\vx-\vx') (\vx-\vx')\T}$.
Our goal will be to learn a linear $d$-dimensional representation $f(\vx) = \mW \vx$ minimizing a contrastive loss which (a) penalizes the difference between positive pairs' representations, but (b) encourages diversity in representations overall to avoid collapse.
For simplicity, we will assume infinite batch size and optimize directly on the population loss.
\todo{we could just as well study full-batch GD on a fixed dataset (with the expectations simply becoming sums over the dataset). that's probs preferable}

To construct such a loss, let us define
\begin{equation}
\tilde\mDelta = \E{\vx,\vx'}{(f(\vx)-f(\vx')) (f(\vx)-f(\vx'))\T} = \mW \mSigma \mW\T
\end{equation}
and
\begin{equation}
\tilde\mSigma = \E{\vx}{f(\vx) f(\vx)\T} = \mW \mSigma \mW\T.
\end{equation}
Intuitively, we would like $\mDelta$ to be small and $\mSigma$ to be large and fairly isotropic.
There are many possible loss functions which encourage these desiderata.
We will begin by studying a family of loss which optimizes these two \textit{jointly}, both because this yields a loss like Barlow Twins and because we can solve the resulting dynamics exactly.

Let us define $\tilde{\mGamma} \equiv \mSigma - \alpha \mDelta$ with tradeoff parameter $\alpha > 0$.
We seek to minimize
\begin{equation} \label{eqn:jloss}
    \L_{\beta} = \norm{\tilde{\mGamma}^\beta - \mathbf{I}_d}^2
\end{equation}
with matrix exponent $\beta > 0$.
We will chiefly be interested in $\beta = \frac{1}{2}, 1$.
\todo{could've explained all this loss setup before stating the model was linear. Maybe makes it make more sense when we go and use it with a deep net.}
When $\alpha = \frac{1}{2}$ and $\beta = 1$, then $\tilde{\mGamma} = \E{\vx,\vx'}{f(\vx)f(\vx')^T}$ and Equation \ref{eqn:jloss} closely resembles the Barlow Twins loss.

Intuition for Equation \ref{eqn:jloss} can be gained by rewriting
\begin{equation}
\mathcal{L}_{\beta} = \sum_{i=1}^d (\lambda_i^\beta(\tilde{\mGamma}) - 1)^2,
\end{equation}
where $\lambda_i(\cdot)$ returns the $i$-th eigenvalue of the argument matrix indexed in descending order.
This loss penalizes the difference of each of the eigenvalues of $\tilde{\mGamma}$ from one.

Let us define $\mGamma \equiv \mSigma - \alpha \mDelta$.
Plugging in the form of our linear model, we find that
\begin{equation}
\L_{\beta}(\mW) = \norm{(\mW \mGamma \mW\T)^\beta - \mathbf{I}_d}^2.
\end{equation}
We wish to find dynamical solutions for $\mW$ evolving under
\begin{equation}
\frac{d \mW}{d t} = - \L_{\beta}(\mW).
\end{equation}

\subsection{Solutions}

Provided the elements of $\mW(t=0)$ are fairly small at init (at least along the positive eigendirections of $\mGamma$), the dynamics will follow the following three stages:
\begin{enumerate}
    \item First, the representation will collapse to the origin.
    \item Second, the singular directions of $\mW$ will align with the top $d$ eigendirections of $\mGamma$ while $f(\vx)$ remains small.
    \item The singular values of $\mW$ will sequentially grow in a manner determined by $\beta$.
\end{enumerate}
At convergence, the model reaches $\L_\beta(\mW)=0$ with $\mW$ projecting $\vx$ onto the top-$d$ eigenspace of $\mGamma$, and is in this sense performing kernel PCA on $\mGamma$.

Now, the first two of these stages I haven't worked out carefully yet (fairly confident it works, tho).
For now, we're just going to skip with the third stage.
We're also going to assume a special initialization to make things exactly solvable.
Under this initialization -- which will resemble the way things look at the start of stage three -- the dynamics of gradient flow can be solved exactly.

\textbf{Special initialization.}
Let ${j_1, ..., j_d} \subset [m]$ denote $d$ eigenvector indices.
We assume that $\mW$ is initialized as $\mW(t=0) = \mV \mS(0) \mU\T$ with $\mV$ an arbitrary orthogonal matrix and $\mS(0)_{ij} = s_i(0) \delta_{i j_i}$ (that is, the columns of $\mS(0)$ are one-hot $m$-vectors).
The singular values are initialized as $s_i(0)$.
It is easily seen that, upon gradient flow on $\mW$, the singular directions decouple, and each one evolves according to an ODE parameterized by its respective eigenvalue.
In particular, one finds that
\begin{equation} \label{eqn:s_val_ode}
    \frac{d s_i}{d t} = - \frac{d}{d s_i} \left( (\lambda_i s_i^2)^\beta - 1 \right)^2.
\end{equation}
This corresponds to gradient flow in a Mexican-hat potential for $\beta=1$ and a, uh, sharpened-Mexican-hat potential for $\beta = \frac{1}{2}$.
\todo{Add + link to figures}

We can solve the dynamics on both of these exactly.
For now, let's focus on the case $\beta = 1$.
Solving Equation \ref{eqn:s_val_ode}, we find that each singular value evolves according to
\begin{equation}
s_i(t) =
\frac{\text{sign} (s_i (0))}
{\lambda_i^{1/2} \left (1 + 
    e^{8 \lambda_i t}\left (\frac {1} {\lambda_i s_i^2 (0)} - 
       1 \right) \right)^{1/2}}.
\end{equation}

Note that $\lambda_i$ can be negative or positive, corresponding to contractive or expulsive dynamics, respectively.
Dimensions with $\lambda_i < 0$ will collapse early in training, while dimensions with $\lambda_i > 0$ will gradually spread out and settle around $s_i^2 \lambda_i = 1$.

[Discussion about the dynamics of this equation in the repulsive case; expansion time $\sim \lambda_i \log s_i(0)$ or whatever; steplike dynamics with one mode going at a time with clear separation]

This gives an exact solution upon a special aligned initialization.
We claim that, if the elements of $\mW(t=0)$ are sampled i.i.d. and in the limit of small norm \todo{add condition for small norm!}, one actually finds fairly early in training that $\mW$ is aligned with the top eigendirections of $\mGamma$.
The result is that, after training, the model's learned the top eigenspace of $\mGamma$.



\section[filler section name]{The kernelization of $\mGamma$}

We have established that the Barlow-Twins-like setup described above will sequentially learn eigendirections of $\Gamma$ in descending order of eigenvalue.
These eigendirections live in the $m \times m$ feature space.
However, as is typical in kernel methods, this feature space will generally be inaccessible to us (or at least so high-dimensional that we wish to avoid constructing it explicitly), and so we want a sort of kernel trick which lets us work only in terms of inner products between these directions and data points.

To make this discussion more concrete, let's forget population distributions and switch to a fixed finite dataset of $2n \le m$ points split into groups $\mX, \mX' \in \R^{n \times m}$ linked by augmentations.
We then have that $\mGamma = \frac{1}{2}( \mX {\mX'}\T + \mX' \mX\T)$.
Note that $\mGamma$ has rank at most $2n$ because we only need to worry about the subspace spanned by the rows of $\mX$ and $\mX'$.
Let $(\ve_i, \lambda_i)_{i=1}^{2n}$ denote the eigenvectors and eigenvalues of $\Gamma$ in this subspace (the remaining eigenvalues will be zero).
After the first mode is learned, we will find the dataset mapped to a line with coordinates $\mX \ve_1$ and $\mX' \ve_1$, and so on for later eigenmodes.
We really care about these vectors
\begin{equation}
\vu_i = \begin{bmatrix}
    \mX \\ {\mX'}
\end{bmatrix} \ve_i
\end{equation}
and not so much about the original eigenvectors.

We can perform the kernel trick to find these vectors without reference to the original feature space.
Let us define $\Kaa \equiv \mX \mX\T$ and $\Kab, \Kba, \Kbb$ similarly.
Let us also define
\begin{equation}
\tilde{\mK} \equiv \begin{bmatrix}
\Kaa & \Kab\\
\Kba & \Kbb
\end{bmatrix},
\end{equation}
the kernel over the combined dataset.

We begin with eigenvectors $\ve_i$ such that
\begin{equation}
\mGamma \ve_i = \lambda_i \ve_i.
\end{equation}
We can relate this to our desired vectors $\vu_i$ by noting that
\begin{equation}
\ve_i = \begin{bmatrix}
    \mX \\ {\mX'}
\end{bmatrix}^+ \vu_i = \begin{bmatrix}
    \mX\T {\mX'}\T
\end{bmatrix} \tilde{\mK}^{-1} \vu_i,
\end{equation}
and so we wish to find $\vu_i$, $\lambda_i$ satisfying
\begin{equation}
\mGamma \begin{bmatrix}
    \mX\T {\mX'}\T
\end{bmatrix} \tilde{\mK}^{-1} \vu_i
= \lambda_i \begin{bmatrix}
    \mX\T {\mX'}\T
\end{bmatrix} \tilde{\mK}^{-1} \vu_i.
\end{equation}

By left-multiplying by $
\begin{bmatrix}
    \mX \\ {\mX'}
\end{bmatrix}
$
and using the definition of $\mGamma$, we find that $\vu_i$ satisfies
\begin{equation}
\mQ \vu_i = \lambda_i \vu_i
\end{equation}
with
\begin{equation}
\mQ = \mK_{\mGamma} \tilde{\mK}^{-1}
\end{equation}
and
\begin{equation}
\mK_{\mGamma} \equiv \begin{bmatrix}
\Kaa\Kba + \Kab\Kaa & \Kaa\Kbb + \Kab^2\\
\Kbb\Kaa + \Kba^2 & \Kbb\Kab + \Kba\Kbb
\end{bmatrix}.
\end{equation}
Note that $\mQ$ is \textit{not} symmetric (unlike $\tilde{\mK}$ and $\mK_{\mGamma})$).

So this lets us make our predictions for Barlow Twins with a kernelized model.
First, compute $\mQ$ for your dataset, and compute its right eigensystem $(\lambda_i, \vu_i)_{i=1}^{2n}$ normalized so that $\norm{\vu_i}^2 = 1$.
At each step in learning, the pointcloud will grow on a new orthogonal direction in embedding space, with the coordinates of the $2n$ points, vectorized, taking the values $\sqrt{2n} \vu_i$.

\section{Reasonable quartic landscapes have no bad local minima}

Here I'll state and prove a theorem about a certain class of quartic loss landscapes.
Consider any loss of the form
\begin{equation} \label{eqn:landscape_loss_W}
    \L(\mW) = -\Tr{\mW\T \mA \mW} + \Tr{(\mW\T \mB \mW)^2}
\end{equation}
$\mW$ is $m \times d$,
$\mA$ is a symmetric $m \times m$ matrix, and
$\mB$ is a symmetric positive definite $m \times m$ matrix.
No special conditions on $\mA$ and $\mB$.
The following theorem holds:
\begin{theorem}
$\L(\mW)$ has no suboptimal local minima.
\end{theorem}
The proof -- in which we characterize the entire loss landscape -- is given below.

\textbf{Proof.}
First, let us substitute $\mZ = \mB^{1/2} \mW$ into the loss to obtain
\begin{equation} \label{eqn:landscape_loss_Z}
    \L(\mZ) = -\Tr{\mZ\T \mC \mZ} + \Tr{(\mZ\T \mZ)^2}
\end{equation}
with $\mC = \mB^{-1/2} \mA \mB^{-1/2}$.
Note that the mapping between $\mW$ and $\mZ$ is invertible, so we are free to characterize the loss landscape in $\mZ$ coordinates.
Note too that the transformed 2nd- and 4th-order matrices in Equation \ref{eqn:landscape_loss_Z} commute (the 4th-order matrix here being the identity), so as far as landscape analysis goes, we don't need to assume they commuted from the start.

It is now relatively easy to find the critical points of Equation \ref{eqn:landscape_loss_Z}.
A critical point must satisfy
\begin{equation}
    \nabla_\mZ \L(\mZ) = - 2 \mC \mZ + 4 \mZ \mZ\T \mZ = 0,
\end{equation}
which can be solved to find that, at a critical point, $\mW$ must have the singular value decomposition
\begin{equation}
\mZ = \mU \mS \mV\T
\end{equation}
in which the first $d$ columns of $\mU$ are unit eigenvectors of $\mC$ and the $d$ singular values on the diagonal of $\mS$ are each $\lambda_i / 2$, with $\lambda_i$ the corresponding eigenvalue.
$\mV$ is arbitrary.
It's not hard to then show that any such critical point has one downward-curving direction for each chosen eigenvalue which can be replaced with a greater one, so the only local minimum can be the global minimum at which the top $d$ eigenvalues of $\mC$ are selected.
\pushQED{\qed}\popQED




















\section{Stuff we should try}

r\subsection{Experimental}
\begin{itemize}
\item Measure growing dimensions in CL with e.g. ResNets and compare with kernel eigenmodes on common CL methods
\item Try quadratic vs. sqrt loss functions and observe empirical differences
\item Why is Barlow Twins best with very small $\lambda$ when $\lambda=1$ would make things nice and isotropic? What happens when $\lambda = 1$?
\item Do toy loss functions we come up with all basically work as well as the defaults?
\item Does maximally elegant loss function with distinct attraction + repulsion terms work as well as or better than the alternatives?
\item Do CL methods collapse even without augs?
\item Take an empirical look at the CL kernel with augs for a real little dataset and a conv model
\item Do you ever see more steps than you have dimensions?
\item Can we explain dimensional collapse at the start of training? Do we want to avoid it?
\item Does init have to be small (i.e. $|f_{t=0}(x)| \ll 1$) to see our dynamics?
\item In general: how do different hyperparams affect our phenomena, or CL more broadly?
\end{itemize}

\subsection{Theoretical}
\begin{itemize}
\item Make a big table of expansions around the origin for all the common CL loss functions
\item What can we say about loss fns where the 2nd- and 4th-order terms have commuting matrices (a la Ziyin's paper) but aren't the same?
\item So if we're just finding the top eigenmodes of some operator, is there some better way of doing that than training a deep net, or of accelerating that training (e.g. using some ideas from 2nd-order optimization)?
\end{itemize}

\subsection{Discussion points to make}
\begin{itemize}
\item Eigenstructure of augmentations is key to the whole thing
\item Is it surprising linear features can or can't do so much interesting stuff?
\item Link between orthogonal eigenmodes and the idea of ``disentanglement"?
\end{itemize}

\end{document}
