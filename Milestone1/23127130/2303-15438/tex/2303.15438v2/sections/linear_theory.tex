\section{Solving the dynamics of the linear model}
\label{sec:linear_th}

\subsection[The feature cross-correlation matrix]{The feature cross-correlation matrix $\mGamma$}
The task we have set up is a matrix optimization problem.
To elucidate the structure of this problem, we can simplify $\mC$ as
\begin{align}
    \mC &= \frac{1}{2n} \sum_i
    \mW \left( \vx_i {\vx_i'}\T \! + \vx_i' \vx_i\T \right) \mW\T = \mW \mGamma \mW\T
\end{align}
where we have defined the feature cross-correlation matrix $\mGamma \equiv \frac{1}{2n} \sum_i ( \vx_i {\vx_i'}\T \! + \vx_i' \vx_i\T )$.
Equation \ref{eqn:loss_fn} then becomes
\begin{equation} \label{eqn:loss_WGW}
    \L = \norm{\mW \mGamma \mW\T - \mI_d}_F^2,
\end{equation}
a form reminiscent of matrix factorization problems, and
Equation \ref{eqn:dW_dt} is
\begin{equation} \label{eqn:dW_dt_expanded}
\frac{d \mW}{d t} = - 4 \left( \mW \mGamma \mW\T - \mI_d \right) \mW \mGamma.
\end{equation}
We will denote by $\gamma_1 \ge \ldots \ge \gamma_m$ the eigenvalues of $\mGamma$ and, for any $k \in 1 \ldots m$, let $\mGamma^{(\le k)} \in \R^{k \times m}$ be the matrix containing the top $k$ eigenvectors of $\mGamma$ as rows.

\subsection{Exact solutions for aligned initialization}
It is nontrivial to solve Equation \ref{eqn:dW_dt_expanded} from arbitrary initialization.
However, as is common in matrix factorization problems, we can obtain exact trajectories starting from special initializations, and these special solutions will shed light on the general dynamics.
We first consider an ``aligned initialization" in which the right singular vectors of $\mW_0$ are the top eigenvectors of $\mGamma$.
Concretely, let
\begin{equation} \label{eqn:W0_spectral_def}
    \mW_0 = \mU \mS_0 \mGamma^{(\le d)}
\end{equation}
be the singular value decomposition of $\mW_0$ with $\mU \in \R^{d \times d}$ an arbitrary orthonormal matrix, and $\mS_0 \in \R^{d \times d}$ is a matrix of singular values given by
\begin{equation} \label{eqn:S0_def}
    \mS_0 = \diag(s_1(0), ..., s_d(0))
\end{equation}
with $s_j(0) > 0$.\footnote{We assume alignment with the top $d$ eigenvectors both for notational simplicity and because this is the solution we will ultimately care about, but our exact solution will hold for any set of $d$ eigenvectors of $\mGamma$.}
The dynamics of $\mW(t)$ under Equation \ref{eqn:dW_dt_expanded} are then given by the following Proposition:
\begin{proposition}[Trajectory of $\mW(t)$ from aligned initialization]
\label{prop:exact_dynamics}
If $\mW(0) = \mW_0$ as given by Equations \ref{eqn:W0_spectral_def} and \ref{eqn:S0_def}, then
\begin{equation} \label{eqn:aligned_W_decomp}
\mW(t) = \mU \mS(t) \mGamma^{(\le d)}
\end{equation}
with
$\mS(t) = \diag(s_1(t), ..., s_d(t))$
and
\begin{equation} \label{eqn:s_j(t)}
    s_j(t) = \frac{e^{4 \gamma_j t}}{\sqrt{s_j^{-2}(0) + (e^{8 \gamma_j t} - 1) \gamma_j}}.
\end{equation}
\end{proposition}

\textit{Proof of Proposition \ref{prop:exact_dynamics}}.
Treating Equation \ref{eqn:aligned_W_decomp} as an ansatz and inserting it into Equation \ref{eqn:dW_dt_expanded}, we find that
\begin{equation}
    \frac{d \mW}{d t} =
    4 \mU (1 - \mD \mS(t)^2) \mD \mS(t) \mGamma^{(\le d)},
\end{equation}
with $\mD = \text{diag}(\gamma_1, \ldots, \gamma_d)$.
It follows that the singular vectors of $\mW(t)$ remain fixed, and the singular values evolve according to
\begin{equation} \label{eqn:sv_ode}
    s_j'(t) = 4 \left( 1 - \gamma_j s_j^2(t) \right) \gamma_j s(t).
\end{equation}


This ODE can be solved to yield Equation \ref{eqn:s_j(t)}.
\QED

When $t \rightarrow \infty$, Proposition \ref{prop:exact_dynamics} prescribes singular values equal to
\begin{equation} \label{eqn:s_j_inf}
s_j(\infty) = \left\{\begin{array}{ll}
        \gamma_j^{-1/2} & \text{for } \gamma_j > 0, \\
        s_j(0) & \text{for } \gamma_j = 0, \\
        0 & \text{for } \gamma_j < 0.
        \end{array}\right.
\end{equation}
Each singular value thus flows monotonically towards either $\gamma_j^{-1/2}$ or zero depending on whether the corresponding eigenvalue is positive or negative.
This can be understood by noting that the loss (Equation \ref{eqn:loss_WGW}) can be rewritten as
\begin{equation} \label{eqn:L_as_eigensum}
    \L = \sum_j (1 - \gamma_j s_j^2)^2,
\end{equation}
which makes clear that if $\gamma_j > 0$, then $s_j = \gamma_j^{-1/2}$ is optimal (and achieves zero loss on the $j$-th term of the sum), but if $\gamma_j < 0$, then the model can do no better than $s_j = 0$

It is worth noting that $\lambda_j = \gamma_j s_j^2$ is the corresponding eigenvalue of $\mC$.
With this change of coordinates, the trajectories of Proposition \ref{prop:exact_dynamics} become nearly sigmoidal, with $\lambda_j \approx (1 + \lambda_j^{-1}(0) e^{-8 \gamma_j t})^{-1}$ when $|\lambda_j(0)| \ll 1$.

We will be particularly interested in the set of terminal solutions.
Accordingly, let us define the set of \textit{top spectral} $\mW$ as follows:
\begin{definition}(Top spectral $\mW$) \label{def:spectramax}
A top spectral $\mW$ is one for which $\mW = \mU \tilde{\mS} \mGamma^{(\le d)}$,
with $\mU$ an orthogonal matrix and
$\tilde{\mS} = \diag(\gamma_1^{-1/2} \mathbbm{1}_{\gamma_1 > 0}, ..., \gamma_d^{-1/2} \mathbbm{1}_{\gamma_d > 0})$.
\end{definition}
(Note that these are precisely the set of $\mW(\infty)$ found by Proposition \ref{prop:exact_dynamics} save for the edge case $\gamma_j = 0$, in which case we set $s_j = 0$.)
These solutions form an equivalence class parameterized by the rotation matrix $\mU$.
As observed by \citet{haochen:2021-provable-guarantees-for-cl}, such a rotation makes no difference for the downstream generalization of a linear probe, so we may indeed view all top spectral $\mW$ as equivalent.

Let us assume henceforth that $\gamma_1, ..., \gamma_d > 0$ and $\gamma_d > \gamma_{d+1}$.
The top spectral $\mW$ achieve $\L(\mW)=0$, but note that there generally exist other optima, such as those aligned with a different set of positive eigenvectors.
However, the top spectral $\mW$ are optimal in the following sense:
\begin{proposition} \label{prop:Fnorm_min}
The top spectral $\mW$ are precisely the solutions to
\begin{equation}
\underset{\mW}{\text{argmin}} \norm{\mW}_F
\ \ \
\text{s.t.}
\ \ \
\L(\mW) = 0.
\end{equation}
\end{proposition}
We relegate the proof to Appendix \ref{app:derivation}.
Proposition \ref{prop:Fnorm_min} implies that, of all solutions achieving $\L = 0$, the top spectral solutions have minimal $\norm{\mW}_F$.
Noting that gradient descent often has an implicit bias towards low-norm solutions \citep{gunasekar:2017-implicit-reg-in-mf, soudry:2018-implicit-bias-of-gd}, we might expect to reach this set of optima from some more general initial conditions.

\subsection{The case of small initialization}
\label{subsec:small_init}
Returning to Proposition \ref{prop:exact_dynamics}, an informative special case of our exact dynamical solution is that in which the initial singular values are small relative to their final values $(s_j(0) \ll \gamma_j^{-1/2})$.
In this case, Equation \ref{eqn:s_j(t)} states that $s_j(t)$ will remain small up to a critical time
\begin{equation} \label{eqn:tau_j_def}
\tau_j = \frac{- \log (s_j^2(0) \gamma_j)}{8 \gamma_j}
\end{equation}
after which it will rapidly grow to its final value.
Note that $\tau_j$ is principally set by $\gamma_j$
, with only a weak logarithmic dependence on initialization $s_j(0)$.
The learning dynamics can thus be understood as a \textit{stepwise process} in which $d$ orthogonal directions are each rapidly learned at their respective timescales $\tau_j$, with plateaus in between adjacent learning phases.

Proposition \ref{prop:exact_dynamics} assumed a special aligned initialization for $\mW$.
We will now give a result which generalizes this significantly, showing that the trajectory from any \textit{small} initialization closely follows that from a particular aligned initialization.

In order to state our result, we will first define the QR factorization and an ``alignment transformation."
\begin{definition}[QR factorization.]
The \textit{QR factorization} of a matrix $\mM \in \R^{a \times b}$ returns a decomposition $\mQ \mR = \mM$ such that $\mQ \in \R^{a \times a}$ is orthogonal and $\mR \in \R^{a \times b}$ is upper-triangular with nonnegative diagonal.
If $a \le b$ and $\mM$ is full rank, then the QR factorization is unique.
\end{definition}
\begin{definition}[Alignment transformation.]
The \textit{alignment transformation} $\mathcal{A}$ of a matrix $\mM$ returns a matrix $\mathcal{A}(\mM) = \mQ \tilde{\mR}$, where $\mQ\mR = \mM$ is a QR factorization and $\tilde\mR$ is $\mR$ with all off-diagonal elements set to zero.
\end{definition}

We can now state the main result of this section.
\begin{result}[Trajectory from generic small initialization] $ $
\label{res:small_init}
\vspace{-2mm}
\begin{itemize}
    \item Let $\gamma_1, ..., \gamma_d$ be unique.
    \item Let $\tilde{\mW}_0 \in \R^{d \times m}$ with $\tilde{\mW}_0 \mGamma^{(\le d)}$ full rank.
    \item Let $\mW(t)$ be the solution to Equation \ref{eqn:dW_dt_expanded} with initial condition $\mW(0) = \alpha \tilde\mW_0$.
    \item Let $\mW^*(t)$ be the aligned solution with initial condition $\mW^*(0) = \mathcal{A}(\mW(0) {\mGamma^{(\le m)}}\T) \mGamma^{(\le m)}$.
\end{itemize}
Then as $\alpha \rightarrow 0$, $\norm{\mW(t) - \mW^*(t)}_F \rightarrow 0$ for all $t$.
\end{result}
We give a derivation of this result in Appendix \ref{app:derivation}.\footnote{We style this conclusion as a Result rather than a Theorem because we give an informal derivation rather than a formal proof.
We conjecture that this result can indeed be made formal.}

Result \ref{res:small_init} states that the trajectory from generic small initialization closely follows a particular aligned trajectory.
This aligned trajectory is given in closed form by Proposition \ref{prop:exact_dynamics}, and so this result gives us equations for the dynamics from arbitrary initialization.

Some intuition for this result can be gained by examining the construction of $\mW^*(0)$.
The aligned solution $\mW^*(t) = \mU \mS^*(t) \mGamma^{(\le d)}$ is composed solely of the top $d$ eigendirections of $\mGamma$, but an arbitrary initialization will have no such preference.
How does this alignment occur?
Note that, at early times when $\mW$ is small, the quadratic term of the loss will dominate, and Equation \ref{eqn:dW_dt_expanded} reduces to
\begin{equation}
    \frac{d \mW}{d t} \approx 4 \mW \mGamma
    \ \ \ \Rightarrow \ \ \
    \mW \approx \mW(0) \, e^{4 \mGamma t}.
\end{equation}
The top eigendirections of $\mGamma$ grow faster and will quickly dominate, and after a time $\tilde{\tau} \gg (\gamma_d - \gamma_{d+1})^{-1}$, we will have
\begin{equation}
    \mW \approx \mW(0) \mPi^{(\le d)} \, e^{4 \mGamma t}
\end{equation}
where $\mPi^{(\le d)} \equiv {\mGamma^{(\le d)}}\T \mGamma^{(\le d)}$ is the projector onto the top-$d$ eigenspace of $\mGamma$.
Components aligned with eigenmodes of index $j > d$ are thus negligible compared to those of index $\le d$ and do not interfere in the learning dynamics, which converge before such later eigenmodes can grow to order one.

Having identified the relevant eigendirections, we must now determine their effective initial singular values $s^*_j(0)$.
Let $\vv_j$ be the $j$-th eigenvector of $\mGamma$ with $\norm{\vv_j} = 1$ and define $\vu_j = \mW(0) \vv_j$.
If $\vv_j$ were a right singular vector of $\mW(0)$, we would have $s^*_j(0) = \norm{\vu_j}$.
We will not be so fortunate in general, however.
Examining Equation \ref{eqn:dW_dt_expanded}, we should expect each eigenmode to only be able to grow in the subspace of $\R^d$ which has not already been filled by earlier eigenmodes, which suggests that we take
\begin{equation} \label{eqn:s_j(0)_approx}
    s^*_j(0) = \norm{\left( 1 - \sum_{k < j} \frac{\vu_k\T \vu_k}{\norm{\vu_k}^2} \right) \vu_j}.
\end{equation}
These are precisely the singular values of $\mW^*(0)$.\footnote{This can be seen by noting that the QR factorization involves the Gram-Schmidt-like process of Equation \ref{eqn:s_j(0)_approx}.}





\subsection{Numerical simulation}
\label{subsec:linear_model_sim}

We perform basic numerical simulations of our linear problem which verify Proposition \ref{prop:exact_dynamics} and Result \ref{res:small_init}.
We sample $n=500$ random images from CIFAR-10 \citep{krizhevsky:2009} and, for each, take two random crops to size $20 \times 20 \times 3$ to obtain $n$ positive pairs (which thus have feature dimension $m = 1200$).
We then randomly initialize a linear model with output dimension $d=10$ and weights drawn i.i.d. from $\mathcal{N}(0,\alpha^2)$ with $\alpha = 10^{-7}$ and train with the loss of Equation \ref{eqn:loss_fn} with learning rate $\eta = 5 \times 10^{-5}$.

During training, we track both the loss and the eigenvalues of the embedding cross-correlation matrix $\mC$.
Our stepwise dynamics predict the loss will start at $\L(0) \approx d$ and decrease by one as each mode is learned, giving
\begin{equation}
    \L(t) \approx \sum_{j | \tau_j > t} 1.
\end{equation}
The eigenvalues of $\mC$ will be $(\gamma_1 s_1^2(t), ..., \gamma_d s_d^2(t))$, with $s_j(t)$ given by Proposition \ref{prop:exact_dynamics}.
The use of Proposition \ref{prop:exact_dynamics} requires values for $s_1(0), ..., s_d(0)$, and these can be found from Equation \ref{eqn:s_j(0)_approx} and the statistics of the random initialization to be roughly
\begin{equation}
    s_j(0) \approx \sigma \sqrt{d - j + 1}.
\end{equation}
The results are plotted in Figure \ref{fig:banner_figure}(A,B).
We find excellent agreement with our theory.

