\section{Proofs}
\label{app:proofs}

\subsection{Proof of Proposition \ref{prop:Fnorm_min}}

We wish to show that the top spectral $\mW$ are the unique solutions to
\begin{equation}
    \underset{\mW}{\text{argmin}} \norm{\mW}_F
    \ \ \ \text{s.t.} \ \ \
    \L(\mW) = 0.
\end{equation}
Any such solution is also a minimum of
\begin{equation}
    \L_\epsilon(\mW) \equiv \L(\mW) + \norm{\mW}_F^2
\end{equation}
as $\epsilon \rightarrow 0$.
We will show that the set of minima are precisely the top spectral embeddings.

Any local minimum must satisfy
\begin{align}
    \nabla_\mW \L_\epsilon
    &= (\mI - \mW \mGamma \mW\T) \mW \mGamma - \epsilon \mW = \mathbf{0} \\
    \label{eqn:mW_local_min_eqn}
    \Rightarrow
    \mW\T \nabla_\mW \L_\epsilon
    &= \mW\T\mW\mGamma - \left(\mW\T\mW\mGamma\right)^2 - \epsilon \mW\T\mW = \mathbf{0}.
\end{align}
Note that the first and second terms of Equation \ref{eqn:mW_local_min_eqn} share all eigenvectors (i.e. commute), and thus the third term must commute with both of them.
From the fact that $\mW\T\mW\mGamma$ and $\mW\T\mW$ share eigenvectors, we can conclude that $\mW\T\mW$ and $\mGamma$ share eigenvectors.
The eigenvectors of $\mW\T\mW$ are simply the right singular vectors of $\mW$.

Any local minimum must thus take the form
\begin{equation}
    \mW = \mU \mS \mGamma^{(\mathcal{J})},
\end{equation}
where
\begin{itemize}
\item $\mU \in \R^{d \times d}$ is orthogonal,
\item $\mS \in \R^{d \times d} = \text{diag}(s_1, ..., s_d)$,
\item $\mathcal{J}$ is a list of $d$ elements from $\{1,...,d\}$,
\item $\mGamma^{(\mathcal{J})} \in \R^{d \times m}$ contains as rows the $d$ eigenvalues of $\mGamma$ corresponding to the elements of $\mathcal{J}$, and
\item the singular values satisfy
\begin{equation}
    s_j^2 \gamma_{\mathcal{J}_j} - s_j^4 \gamma_{\mathcal{J}_j}^2 - \epsilon s_j^2 = 0
    \ \ \ \Rightarrow \ \ \
    s_j \in \left\{0, \sqrt{\frac{\gamma_{\mathcal{J}_j} - \epsilon}{\gamma_{\mathcal{J}_j}^2}}\right\},
\end{equation}
where $s_j = 0$ is chosen by default when $\gamma_{\mathcal{J}_j} \le 0$.
\end{itemize}
Of these candidate local minima, a global minimum must choose $s_j = 0$ as infrequently as possible, and given that must minimize $\norm{\mW}_F^2 = \sum_j s_j^2$.
This is achieved by choosing $\mathcal{J} = (1,...,d)$, which yields the top spectral solutions.

\subsection{Proof of Proposition \ref{prop:kernelized_sol}}
\newcommand{\XXh}{\begin{bmatrix} \mX\T \ {\mX'}\T \end{bmatrix}}
\newcommand{\XXv}{\begin{bmatrix} \mX \\ \mX' \end{bmatrix}}

We want to translate our results for linear models in Section \ref{sec:linear_th} to the kernel setting by phrasing interesting quantities in terms of inner products $\vx_i\T \vx_j$.
After finishing this procedure, we will reinterpret $\vx_i\T \vx_j \rightarrow \vphi(x_i)\T \vphi(x_j) = \K(x_i,x_j)$.

It will be useful to perform some setup before diving in.
We define $\mX = [\vx_1, ..., \vx_d]\T \in \R^{n \times m}$ and $\mX' = [\vx'_1, ..., \vx'_d]\T$ and define the full dataset kernel matrix
$\tilde{\mK} = \XXv \XXh$.
Recall that $\mGamma = \frac{1}{2n} \left( \mX\T \mX' + {\mX'}\T \mX \right)$.
In what follows, all interesting vectors will lie in either the row-space or column-space of $\XXv$ (whichever is appropriate), and so all vectors over the dataset (i.e. with $2n$ elements) will lie in the cokernel of $\tilde{\mK}$, so we are free to use the pseudoinverse of $\tilde{\mK}$ without worrying about the action of null eigendirections.
All matrix inverses henceforth will be interpreted as pseudoinverses.

\textbf{Proof of clause (a).}
An eigenpair $(\gamma, \vg)$ of $\mGamma$ satisfies
\begin{equation} \label{eqn:gamma_eigenval_def}
\mGamma \vg = \gamma \vg.
\end{equation}
Assume that $\gamma \neq 0$ and $\norm{\vg} = 1$.
Let us define a vector $\vb$ dual to $\vg$ as
\begin{equation} \label{eqn:b_def}
    \vb = \tilde{\mK}^{-1/2} \XXv \vg
    \quad
    \Leftrightarrow
    \quad
    \vg = \XXh \tilde{\mK}^{-1/2} \vb.
\end{equation}
Note that $\norm{\vg} = \norm{\vb}$.
Plugging Equation \ref{eqn:b_def} into Equation \ref{eqn:gamma_eigenval_def}, we find that
\begin{equation}
\mGamma \XXh \tilde{\mK}^{-1/2} \vb = \gamma \XXh \tilde{\mK}^{-1/2} \vb.
\end{equation}
Left-multiplying both sides by $\tilde{\mK}^{-1/2} \XXv$ yields
\begin{align}
    \tilde{\mK}^{-1/2} \XXv \mGamma \XXh \tilde{\mK}^{-1/2} \vb
    &= \gamma \tilde{\mK}^{-1/2} \XXv \XXh \tilde{\mK}^{-1/2} \vb \\
    &= \gamma \vb.
\end{align}
Defining
\begin{equation}
    \mZ \equiv
    \XXv \mGamma \XXh =
    \frac{1}{2n}
\begin{bmatrix}
    \mX {\mX'}\T \mX {\mX}\T & \mX {\mX'}\T \mX {\mX'}\T \\
    \mX' {\mX'}\T \mX {\mX}\T & \mX' {\mX'}\T \mX {\mX'}\T
\end{bmatrix}
+
\frac{1}{2n}
\begin{bmatrix}
    \mX {\mX}\T \mX' {\mX}\T & \mX {\mX}\T \mX' {\mX'}\T \\
    \mX' {\mX}\T \mX' {\mX}\T & \mX' {\mX}\T \mX' {\mX'}\T
\end{bmatrix}
\end{equation}
as in the main text and $\mK_\mGamma \equiv \tilde{\mK}^{-1/2} \mZ \tilde{\mK}^{-1/2}$, we find that
$\gamma$ is also an eigenvalue of $\mK_\mGamma$.
The same argument run in reverse (now rewriting $\vb$ in terms of $\vg$) yields that nonzero eigenvalues of $\tilde{\mK}_\mGamma$ are also eigenvalues of $\mGamma.$ \QED

\textbf{Proof of clause (b).}
The top spectral weights given by Definition \ref{def:spectramax} yield the spectral representation
\begin{equation}
    \vf(\vx) = \mU \tilde{\mS} [\vg_1 \ ... \ \vg_d]\T \vx.
\end{equation}
Plugging in Equation \ref{eqn:b_def} yields that
\begin{equation}
   \vf(\vx) = \mU \tilde{\mS} [\vb_1 \ ... \ \vb_d]\T \tilde{\mK}^{-1/2} \XXv \vx
\end{equation}
as claimed by Proposition \ref{prop:kernelized_sol}. \QED

\textbf{Proof of clause (c).}
The RKHS norm of a function with respect to a kernel $\K$ is equivalent to the minimal Frobenius norm of an explicit matrix transformation on the hidden features of the kernel, so this clause follows from Proposition \ref{prop:Fnorm_min}. \QED

\textbf{Kernelized dynamics.}

Via direct kernelization of Result \ref{res:small_init}, the kernelized representations trained from small init at an arbitrary finite time $t$ are well-approximated by
\begin{equation} \label{eqn:kernelized_traj}
    \vf(\vx, t)
    = \mU \tilde{\mS}(t) [\vg_1 \ ... \ \vg_d]\T \vx
    = \mU \tilde{\mS}(t) [\vb_1 \ ... \ \vb_d]\T \tilde{\mK}^{-1/2} [\mK_{x \X} \ \mK_{x \X'}]\T,
\end{equation}
with the singular values on the diagonal of $\tilde{\mS}(t)$ evolving according to Proposition \ref{prop:exact_dynamics}.
The initialization-dependent matrix $\mU$ and effective initial singular values are found in the explicit linear case as
\begin{equation}
    \mU \tilde{\mS}(0) = \mathcal{A}(\mW(0) {\mGamma^{(\le d)}}\T) = \mathcal{A}(\mW(0) [\vg_1, ..., \vg_d]).
\end{equation}
Using Equation \ref{eqn:b_def}, we then have that
\begin{equation} \label{eqn:kernelized_init}
    \mU \tilde{\mS}(0) = \mathcal{A}
    \left(
    [\vf(x_1), ..., \vf(x_n), \vf(x'_1), ..., \vf(x'_n)] \tilde{\mK}^{-1/2} [\vb_1, ... \vb_d]
    \right).
\end{equation}
Equations \ref{eqn:kernelized_traj} and \ref{eqn:kernelized_init} together permit one to find the trajectories from arbitrary small init in fully kernelized form.



































