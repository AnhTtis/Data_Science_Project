\section{Experimental details}
\label{app:exp}

Below we describe our suite of SSL experiments.
All code and experiment configs are available at \href{https://gitlab.com/generally-intelligent/ssl_dynamics}{\url{https://gitlab.com/generally-intelligent/ssl_dynamics}}.

\subsection{Single-hidden-layer MLP}
This experiment follows the linear model numerical experiment described in Section \ref{subsec:linear_model_sim}. We train a single-hidden-layer MLP for 7000 epochs over a fixed batch of 50 images from CIFAR10 using full-batch SGD. Each image is subject to a random 20x20 crop and no other augmentations. The learning rate is $\eta=0.0001$ and weights are scaled upon initialization by $\alpha=0.0001$. The hidden layer has width 2048 and the network output dimension is $d = 10$. We use Barlow Twins loss, but do not apply batch norm to the embeddings when calculating the cross-correlation matrix. $\lambda$ is set to 1.

\subsection{Full-size models}
We run two sets of experiments experiments with ResNet-50 encoders which respectively use \textit{standard init} and \textit{small init} whose results appear in figures in this paper. The standard set aims to reproduce performance reported in source publications and match original hyperparameters, whereas the small-init set includes modifications which aim to bring out the stepwise learning dynamics predicted by our theory more clearly.
Relative to the standard set, the small init set multiplies all initial weights by a small scale factor $\alpha$ and uses a reduced learning rate $\eta$.
The parameters used for each method are generally identical across the two sets except for $\alpha$ and $\eta$.

For all experiments in the standard set, we keep parameters as close as possible to those in the source publication but make minor adjustments where we find they improve evaluation metrics. Additionally, in order to avoid a steep compute budget for reproduction, we train our models on STL-10 instead of ImageNet, and reduce batch size, projector layer widths and training epochs so that the models can be trained using a single GPU in under 24 hours. \textbf{Augmentations, optimizer parameters such as momentum and weight decay, and learning rate schedules are generally left unchanged from the source publications.} Below we describe deviations from the stock hyperparameters.

\textbf{Barlow Twins.}
We set batch size to 64, and update the projector to use single hidden layer of width 2048 and embedding size $d = 512$. We use the LARS optimizer \citep{you:2017-LARS} with stock learning rates. In the small-init case, we additionally set $\lambda=1$ and $\alpha=0.093$ and remove the pre-loss batch norm. In the standard case, we keep $\lambda = 0.0051$ as suggested by \citeauthor{zbontar:2021-barlow-twins} We train the networks for 320 epochs.

\textbf{SimCLR.}
We set batch size to 256, the cross-entropy temperature $\tau=0.2$ and $\eta=0.5$. We train using SGD with weight decay $1*10^{-5}$ and no LR scheduling. In the small-init case, we use $\eta=0.01$ and additionally set $\alpha=0.087$. We train the networks for 320 epochs.%
\footnote{We found that SimCLR showed noticeably more apparent stepwise behavior in the \textit{standard} case than did Barlow Twins or VICReg (see Figure \ref{fig:dynamics_realistic}), with steps visible even in the loss curve (not shown). We are rather surprised by this difference and invite attempts to replicate it.}

\textbf{VICReg.}
We set batch size to 256, reduce the projector's hidden layer width to 512 and set $d=128$. We use $\eta=0.5$ and weight decay $1*10^{-5}$ and reduce the warmup epochs to 1. We use the LARS optimizer. In the small-init case, we use $lr=0.25$ and additionally set $\alpha=0.133$. We train the networks for 500 epochs.

\textbf{Values of $\alpha$.}
The init scales $\alpha$ quoted above were tuned so as to give clear stepwise behavior while also not slowing training too much.
We found that a useful heuristic is to choose $\alpha$ such that the top eigenvalue at init is roughly $10^{-3}$ for Barlow Twins and VICReg and $10^{-6}$ for SimCLR.



\subsection{Evaluating performance}

Our small init experiments cleanly display stepwise learning.
Here we show that these hyperparameter changes do not dramatically worsen model performance.
We evaluate performance by measuring the quality of learned hidden representations using a linear probe, and perform this evaluation many times throughout training.
Validation accuracies increase throughout training (Figure \ref{fig:val_acc}) as the learned representations improve over time, and generally small-init experiments fare similarly to the standard set, with a small performance drop of a few percent, but train slower as expected.



\begin{figure}
  \includegraphics[width=17cm]{img/updated/fig_acc_final.pdf}
  \vspace{-3mm}
  \caption{
    Validation accuracy of a linear probe throughout training for our standard and small-init experiments.
  }
  \label{fig:val_acc}
\end{figure}

\subsection{Measuring eigenvalues}
In the main text, all eigenvalues reported for \textit{embeddings} using the \textit{Barlow Twins loss} are the eigenvalues $\lambda_j(\mC)$, with $\mC$ the cross-correlation matrix across positive pairs as defined in Section \ref{sec:preliminaries}.
Eigenvalues reported for the embeddings of other losses are the eigenvalues $\lambda(\tilde{\mC})$ of the (centered) covariance matrix, with $\tilde{\mC} \equiv \mathbb{E}_x[(\vf(x) - \bar{\vf}) (\vf(x) - \bar{\vf})\T]$ and $\bar{\vf} \equiv \mathbb{E}_x[\vf(x)]$.
The eigenvalues reported in Figure \ref{fig:rep_vals} for hidden representations are all those of the covariance matrix (but of course computed on the hidden representations instead of the embeddings $\vf$).
We vary the matrix studied because our theory deals explicitly with $\mC$, but this cross-correlation matrix is not necessarily meaningful except in the embeddings of Barlow Twins.
In practice, however, we see similar results for either case (and in fact sometimes even see sharper learning steps in $\lambda_j(\tilde{\mC})$ than $\lambda_j(\mC)$ even for Barlow Twins).

\subsection{Measuring the empirical NTK}
In order to calculate the empirical NTK (eNTK), we employ functorch \citep{he:2021-functorch} and express the eNTK as a Jacobian contraction. We find that calculating the full kernel for realistic-sized networks is unfeasible due to memory and runtime constraints, primarily because of the large output dimension $d$ (which is 50 in the experiments of Appendix \ref{app:emb_pred}) as well as large batch sizes. To overcome this, we employ two tricks.
First, we reduce the model output to a scalar by taking an average of the outputs and calculate the eNTK for this modified function instead.\footnote{More precisely, we take the average of the $d$ outputs and multiply it by $\sqrt{d}$, a scaling which recovers the original NTK matrix in the ideal, infinite-width case.}\footnote{Note that a simplification like this is necessary to use our theory: our theory assumes the NTK on $n$ samples is captured by a single matrix, whereas in reality it is a \textit{rank-four tensor} for a model with vector output.} Second, we chunk up the computation for the full batch into minibatches depending on the size of $d$ and available GPU memory. With this approach, computing the eNTK on 1000 image pairs takes on the order of 1 minute on a single consumer GPU.