\section{Potential modifications for speeding up SSL}
\label{app:speedup}

Compared to standard supervised learning, SSL is known in folklore to be much slower to train.
Our work presents a theory of the training dynamics of SSL, and thus it is natural to ask whether it sheds light on this point of difficulty or suggests means by which it might be addressed.
Here we suggest an explanation for the slowness of SSL training and propose various fixes which are ripe for future study.

In our picture of the training dynamics of SSL, embedding eigenvalues start small and grow sequentially, converging when $d$ eigenvalues have sufficiently grown.
Smaller eigendirections require more time to grow.
\textbf{We suggest that SSL is slow to converge because one must wait for the small eigendirections to grow.}
This hypothesis is supported by Figure \ref{fig:dynamics_realistic}, which shows that, in a realistic configuration, a significant fraction of the eigenvalues remain small even late in training.\footnote{This observation is particularly compelling in light of the finding of \citet{garrido:2022-rankme} that generalization is better when embeddings have higher rank. Their work suggests that hyperparameters should be chosen to maximize the embedding rank at the end of training.}

This suggests that SSL can be sped up by modifying training to target small eigenvalues.
Here we suggest one way this might be achieved via preconditioning of gradients and two additional ways this might be achieved by modifying the loss function itself.
We leave our results at the level of theoretical speculation.
We encourage interested SSL practitioners to try implementing the methods described and reach out with questions.

\subsection{Targeting gradients towards small PCA directions.}
One potentially promising idea is to simply compute the PCA matrix of the embeddings and ``manually" increase the gradient pull along directions which are very small, thereby encouraging the distribution to expand in directions in which it is currently flat.
Let us denote by $\mF \in \R^{2n \times d}$ the embeddings for a given data batch.
Backpropagation will compute $\nabla_\theta \L = \text{Tr}[ (\nabla_\theta \mF)\T \ \nabla_\mF \L ]$.
We may simply enact the substitution
\begin{equation}
    \nabla_\mF \L
    \longrightarrow
    (\nabla_\mF \L) (\mF\T \mF + \alpha \mI_d)^{-1}
\end{equation}
with $\alpha > 0$ to suppress $\nabla_\mF \L$ along directions of large variance and increase it along directions of small variance.
We anticipate this will encourage faster growth of small eigenmodes.
This trick could apply to any joint embedding method.

\begin{figure}
  \centering
  \includegraphics[width=12cm]{img/speedup_cartoon.pdf}
  \vspace{-3mm}
  \caption{
    \textbf{Proposed modifications for the modewise SSL loss to encourage or enable the faster growth of slow eigendirections.}
    Losses are expressed as a function of $\bar{s}_j = \gamma_j^{1/2} s_j$ and assume $\gamma_j > 0$.
    \textbf{(A)}: Original Barlow Twins modewise loss.
    \textbf{(B)}: A modified loss with a kink at zero. The origin is no longer an unstable equilibrium and nearby points will quickly escape.
    \textbf{(C, D)}: Two losses modified so as to have smaller curvature at their minima and thereby permit larger learning rates without instability.
  }
  \label{fig:speedup_cartoon}
\end{figure}

\subsection{Sharpening at zero}
This and the following method are specialized to the Barlow Twins loss.
They will make use of visual depictions of the modewise loss landscape as described in Appendix \ref{app:symm}, and so we recommend reading that appendix first.
They will both consider transformations $g(\mC)$ of the correlation matrix $\mC$, where $g$ acts pointwise on the eigenvalues of $\mC$.

Recall that we found that the singular values of the system under aligned conditions evolve under a loss of the form $\L_j = (1 - \lambda_j)^2 = (1 - \gamma_j s_j^2)^2$.
As shown in Figure \ref{fig:speedup_cartoon}A and discussed in Appendix \ref{app:symm}, this means that, when initialized near zero, they initially feel only a small gradient and must spend a long time escaping the origin, with that duration determined by just how small they are initially.
However, this is no longer the case if we change the loss so it has a kink at zero.
For example, if we change the loss to $\L_{\text{sharp}}(\mC) = \norm{g_\text{sqrt}(\mC) - \mI_d}_F$ with $g_\text{sqrt}(\lambda) = \text{sign}(\lambda) |\lambda|^{1/2}$, then all singular values with $\gamma_j > 0$ feel a $\Theta(1)$ repulsive force from the origin regardless of how close they are.
This loss is plotted in Figure \ref{fig:speedup_cartoon}B.
Interventions of this type lead to eigenvalue growth curves of the type shown in Figure \ref{fig:ssb_cartoon}D.\footnote{In fact, the particular choice $g_\text{sqrt}(\lambda) = \text{sign}(\lambda) |\lambda|^{1/2}$ gives dynamics which can be solved exactly for aligned init just like the original Barlow Twins loss studied in the main text.}

\subsection{Smoothing around minima}

The naive way to speed up the growth of slow-growing eigenmodes is simply to increase the learning rate.
This fails because the minima of the fast-growing eigenmodes will eventually become unstable.
For example, if $d=2$ and the total loss is $\L = (1 - s_1^2)^2 + (1 - 10^{-6} s_2^2)^2$, any learning rate above $\eta = 1/4$ will cause instability in $s_1$, but $s_2$ requires a larger learning rate to grow in reasonable time.
One solution here is to modify the loss landscape so that the higher eigendirections see a wider basin around their minima and can thus tolerate a larger learning rate.
One implementation might be $\L_{\text{smooth}}(\mC) = \norm{g_\text{smooth}(\mC) - \mI_d}_F$ with $g_\text{smooth}(\lambda) = \min(\lambda, 1)$.
One might replace this with e.g. $g_\text{smooth} = \min(\lambda, 1 + \epsilon (1 - \lambda))$ for some small $\epsilon > 0$.
Alternatively, one might modify the structure of the loss function itself to be e.g. $\L_j = (1 - \lambda_j)^4$, which has vanishing curvature at its minimum, or implement a similar idea with a hinge function to create a perfectly flat basin of finite width.
Two possibilities for losses modified to have wider minima are shown in Figures \ref{fig:speedup_cartoon}C,D.

Both these last ideas are specialized to Barlow Twins in the case of $\lambda = 1$, but we anticipate they could be easily generalized.
In fact, VICReg already includes a square root in the variance term which we conjecture implicitly does something similar to our proposed sharpening modification.

