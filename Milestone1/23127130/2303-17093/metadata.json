{
    "arxiv_id": "2303.17093",
    "paper_title": "OpenMix: Exploring Outlier Samples for Misclassification Detection",
    "authors": [
        "Fei Zhu",
        "Zhen Cheng",
        "Xu-Yao Zhang",
        "Cheng-Lin Liu"
    ],
    "submission_date": "2023-03-30",
    "revised_dates": [
        "2023-03-31"
    ],
    "latest_version": 1,
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "abstract": "Reliable confidence estimation for deep neural classifiers is a challenging yet fundamental requirement in high-stakes applications. Unfortunately, modern deep neural networks are often overconfident for their erroneous predictions. In this work, we exploit the easily available outlier samples, i.e., unlabeled samples coming from non-target classes, for helping detect misclassification errors. Particularly, we find that the well-known Outlier Exposure, which is powerful in detecting out-of-distribution (OOD) samples from unknown classes, does not provide any gain in identifying misclassification errors. Based on these observations, we propose a novel method called OpenMix, which incorporates open-world knowledge by learning to reject uncertain pseudo-samples generated via outlier transformation. OpenMix significantly improves confidence reliability under various scenarios, establishing a strong and unified framework for detecting both misclassified samples from known classes and OOD samples from unknown classes. The code is publicly available at https://github.com/Impression2805/OpenMix.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.17093v1"
    ],
    "publication_venue": "Accepted by CVPR 2023 (Highlight)"
}