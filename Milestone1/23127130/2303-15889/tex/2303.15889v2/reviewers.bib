@inproceedings{Garcia2023,
  title = {Uncurated {{Image-Text Datasets}}: {{Shedding Light}} on {{Demographic Bias}}},
  shorttitle = {Uncurated {{Image-Text Datasets}}},
  booktitle = {2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Garcia, Noa and Hirota, Yusuke and Wu, Yankun and Nakashima, Yuta},
  year = {2023},
  month = jun,
  pages = {6957--6966},
  issn = {2575-7075},
  doi = {10.1109/CVPR52729.2023.00672},
  urldate = {2023-10-16},
  abstract = {The increasing tendency to collect large and uncurated datasets to train vision-and-language models has raised concerns about fair representations. It is known that even small but manually annotated datasets, such as MSCOCO, are affected by societal bias. This problem, far from being solved, may be getting worse with data crawled from the Internet without much control. In addition, the lack of tools to analyze societal bias in big collections of images makes addressing the problem extremely challenging. Our first contribution is to annotate part of the Google Conceptual Captions dataset, widely used for training vision-and-language models, with four demographic and two contextual attributes. Our second contribution is to conduct a comprehensive analysis of the annotations, focusing on how different demographic groups are represented. Our last contribution lies in evaluating three prevailing vision-and-language tasks: image captioning, text-image CLIP embeddings, and text-to-image generation, showing that societal bias is a persistent problem in all of them. https://github.com/noagarcia/phase},
  annotation = {1 citations (Crossref) [2023-10-16]},
  file = {/home/iris.dominguez/Documents/documents/universidad/docs/Library/Zotero/Garcia et al_2023_Uncurated Image-Text Datasets.pdf}
}

@inproceedings{Hardt2016,
  title = {Equality of Opportunity in Supervised Learning},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
  year = {2016},
  month = dec,
  series = {{{NIPS}}'16},
  pages = {3323--3331},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY, USA}},
  urldate = {2023-10-16},
  abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. We enourage readers to consult the more complete manuscript on the arXiv.},
  isbn = {978-1-5108-3881-9},
  file = {/home/iris.dominguez/Documents/documents/universidad/docs/Library/Zotero/Hardt et al_2016_Equality of opportunity in supervised learning.pdf}
}

@misc{Hardt2016a,
  title = {Equality of {{Opportunity}} in {{Supervised Learning}}},
  author = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
  year = {2016},
  month = oct,
  number = {arXiv:1610.02413},
  eprint = {1610.02413},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1610.02413},
  urldate = {2023-10-16},
  abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. In line with other studies, our notion is oblivious: it depends only on the joint statistics of the predictor, the target and the protected attribute, but not on interpretation of individualfeatures. We study the inherent limits of defining and identifying biases based on such oblivious measures, outlining what can and cannot be inferred from different oblivious tests. We illustrate our notion using a case study of FICO credit scores.},
  archiveprefix = {arxiv},
  file = {/home/iris.dominguez/Documents/documents/universidad/docs/Library/Zotero/Hardt et al_2016_Equality of Opportunity in Supervised Learning.pdf;/home/iris.dominguez/snap/zotero-snap/common/Zotero/storage/RA4UQV5A/1610.html}
}

@article{Hazirbas2022,
  title = {Towards {{Measuring Fairness}} in {{AI}}: {{The Casual Conversations Dataset}}},
  shorttitle = {Towards {{Measuring Fairness}} in {{AI}}},
  author = {Hazirbas, Caner and Bitton, Joanna and Dolhansky, Brian and Pan, Jacqueline and Gordo, Albert and Ferrer, Cristian Canton},
  year = {2022},
  month = jul,
  journal = {IEEE Transactions on Biometrics, Behavior, and Identity Science},
  volume = {4},
  number = {3},
  pages = {324--332},
  issn = {2637-6407},
  doi = {10.1109/TBIOM.2021.3132237},
  urldate = {2023-10-16},
  abstract = {This paper introduces a novel dataset to help researchers evaluate their computer vision and audio models for accuracy across a diverse set of age, genders, apparent skin tones and ambient lighting conditions. Our dataset is composed of 3,011 subjects and contains over 45,000 videos, with an average of 15 videos per person. The videos were recorded in multiple U.S. states with a diverse set of adults in various age, gender and apparent skin tone groups. A key feature is that each subject agreed to participate for their likenesses to be used. Additionally, our age and gender annotations are provided by the subjects themselves. A group of trained annotators labeled the subjects' apparent skin tone using the Fitzpatrick skin type scale. Moreover, annotations for videos recorded in low ambient lighting are also provided. As an application to measure robustness of predictions across certain attributes, we provide a comprehensive study on the top five winners of the DeepFake Detection Challenge (DFDC). Experimental evaluation shows that the winning models are less performant on some specific groups of people, such as subjects with darker skin tones and thus may not generalize to all people. In addition, we also evaluate the state-of-the-art apparent age and gender classification methods. Our experiments provides a thorough analysis on these models in terms of fair treatment of people from various backgrounds.},
  annotation = {13 citations (Crossref) [2023-10-16]},
  file = {/home/iris.dominguez/Documents/documents/universidad/docs/Library/Zotero/Hazirbas et al_2022_Towards Measuring Fairness in AI.pdf}
}

@misc{Hazirbas2022a,
  title = {Casual {{Conversations}} v2: {{Designing}} a Large Consent-Driven Dataset to Measure Algorithmic Bias and Robustness},
  shorttitle = {Casual {{Conversations}} V2},
  author = {Hazirbas, Caner and Bang, Yejin and Yu, Tiezheng and Assar, Parisa and Porgali, Bilal and Albiero, V{\'i}tor and Hermanek, Stefan and Pan, Jacqueline and McReynolds, Emily and Bogen, Miranda and Fung, Pascale and Ferrer, Cristian Canton},
  year = {2022},
  month = nov,
  number = {arXiv:2211.05809},
  eprint = {2211.05809},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.05809},
  urldate = {2023-10-16},
  abstract = {Developing robust and fair AI systems require datasets with comprehensive set of labels that can help ensure the validity and legitimacy of relevant measurements. Recent efforts, therefore, focus on collecting person-related datasets that have carefully selected labels, including sensitive characteristics, and consent forms in place to use those attributes for model testing and development. Responsible data collection involves several stages, including but not limited to determining use-case scenarios, selecting categories (annotations) such that the data are fit for the purpose of measuring algorithmic bias for subgroups and most importantly ensure that the selected categories/subcategories are robust to regional diversities and inclusive of as many subgroups as possible. Meta, in a continuation of our efforts to measure AI algorithmic bias and robustness (https://ai.facebook.com/blog/shedding-light-on-fairness-in-ai-with-a-new-data-set), is working on collecting a large consent-driven dataset with a comprehensive list of categories. This paper describes our proposed design of such categories and subcategories for Casual Conversations v2.},
  archiveprefix = {arxiv},
  file = {/home/iris.dominguez/Documents/documents/universidad/docs/Library/Zotero/Hazirbas et al_2022_Casual Conversations v2.pdf;/home/iris.dominguez/snap/zotero-snap/common/Zotero/storage/2VQ2GADM/2211.html}
}

@misc{He2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  number = {arXiv:1512.03385},
  eprint = {1512.03385},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1512.03385},
  urldate = {2023-12-05},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/iris.dominguez/Documents/documents/universidad/docs/Library/Zotero/He et al_2015_Deep Residual Learning for Image Recognition.pdf;/home/iris.dominguez/snap/zotero-snap/common/Zotero/storage/9AQ4MRWD/1512.html}
}

@inproceedings{Porgali2023,
  title = {The {{Casual Conversations}} v2 {{Dataset}} : {{A}} Diverse, Large Benchmark for Measuring Fairness and Robustness in Audio/Vision/Speech Models},
  shorttitle = {The {{Casual Conversations}} v2 {{Dataset}}},
  booktitle = {2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Porgali, Bilal and Albiero, V{\'i}tor and Ryda, Jordan and Ferrer, Cristian Canton and Hazirbas, Caner},
  year = {2023},
  month = jun,
  pages = {10--17},
  issn = {2160-7516},
  doi = {10.1109/CVPRW59228.2023.00006},
  urldate = {2023-10-16},
  abstract = {This paper introduces a new large consent-driven dataset aimed at assisting in the evaluation of algorithmic bias and robustness of computer vision and audio speech models in regards to 11 attributes that are self-provided or labeled by trained annotators. The dataset includes 26,467 videos of 5,567 unique paid participants, with an average of almost 5 videos per person, recorded in Brazil, India, Indonesia, Mexico, Vietnam, Philippines, and the USA, representing diverse demographic characteristics. The participants agreed for their data to be used in assessing fairness of AI models and provided self-reported age, gender, language/dialect, disability status, physical adornments, physical attributes and geo-location information, while trained annotators labeled apparent skin tone using the Fitzpatrick Skin Type and Monk Skin Tone scales, and voice timbre. Annotators also labeled for different recording setups and per-second activity annotations.},
  annotation = {0 citations (Crossref) [2023-10-16]},
  file = {/home/iris.dominguez/Documents/documents/universidad/docs/Library/Zotero/Porgali et al_2023_The Casual Conversations v2 Dataset.pdf}
}

@inproceedings{Torralba2011,
  title = {Unbiased Look at Dataset Bias},
  booktitle = {{{CVPR}} 2011},
  author = {Torralba, Antonio and Efros, Alexei A.},
  year = {2011},
  month = jun,
  pages = {1521--1528},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2011.5995347},
  urldate = {2023-10-16},
  abstract = {Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech-101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue.},
  annotation = {1069 citations (Crossref) [2023-10-16]},
  file = {/home/iris.dominguez/Documents/documents/universidad/docs/Library/Zotero/Torralba_Efros_2011_Unbiased look at dataset bias.pdf}
}

@incollection{Wang2020,
  title = {{{REVISE}}: {{A Tool}} for {{Measuring}} and {{Mitigating Bias}} in {{Visual Datasets}}},
  shorttitle = {{{REVISE}}},
  booktitle = {Computer {{Vision}} {\textendash} {{ECCV}} 2020},
  author = {Wang, Angelina and Narayanan, Arvind and Russakovsky, Olga},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year = {2020},
  volume = {12348},
  pages = {733--751},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-58580-8_43},
  urldate = {2023-11-27},
  isbn = {978-3-030-58579-2 978-3-030-58580-8},
  langid = {english},
  file = {/home/iris.dominguez/Documents/documents/universidad/docs/Library/Zotero/Wang et al_2020_REVISE.pdf}
}

@article{Wang2022b,
  title = {{{REVISE}}: {{A Tool}} for {{Measuring}} and {{Mitigating Bias}} in {{Visual Datasets}}},
  shorttitle = {{{REVISE}}},
  author = {Wang, Angelina and Liu, Alexander and Zhang, Ryan and Kleiman, Anat and Kim, Leslie and Zhao, Dora and Shirai, Iroha and Narayanan, Arvind and Russakovsky, Olga},
  year = {2022},
  month = jul,
  journal = {International Journal of Computer Vision},
  volume = {130},
  number = {7},
  pages = {1790--1810},
  issn = {1573-1405},
  doi = {10.1007/s11263-022-01625-5},
  urldate = {2023-10-16},
  abstract = {Machine learning models are known to perpetuate and even amplify the biases present in the data. However, these data biases frequently do not become apparent until after the models are deployed. Our work tackles this issue and enables the preemptive analysis of large-scale datasets. REvealing VIsual biaSEs (REVISE) is a tool that assists in the investigation of a visual dataset, surfacing potential biases along three dimensions: (1) object-based, (2) person-based, and (3) geography-based. Object-based biases relate to the size, context, or diversity of the depicted objects. Person-based metrics focus on analyzing the portrayal of people within the dataset. Geography-based analyses consider the representation of different geographic locations. These three dimensions are deeply intertwined in how they interact to bias a dataset, and REVISE sheds light on this; the responsibility then lies with the user to consider the cultural and historical context, and to determine which of the revealed biases may be problematic. The tool further assists the user by suggesting actionable steps that may be taken to mitigate the revealed biases. Overall, the key aim of our work is to tackle the machine learning bias problem early in the pipeline. REVISE is available at https://github.com/princetonvisualai/revise-tool.},
  langid = {english},
  annotation = {10 citations (Crossref) [2023-10-16]},
  file = {/home/iris.dominguez/Documents/documents/universidad/docs/Library/Zotero/Wang et al_2022_REVISE.pdf}
}

@inproceedings{Zafar2017,
  title = {Fairness {{Constraints}}: {{Mechanisms}} for {{Fair Classification}}},
  shorttitle = {Fairness {{Constraints}}},
  booktitle = {Proceedings of the 20th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Zafar, Muhammad Bilal and Valera, Isabel and Rogriguez, Manuel Gomez and Gummadi, Krishna P.},
  year = {2017},
  month = apr,
  pages = {962--970},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-10-16},
  abstract = {Algorithmic decision making systems are ubiquitous across a wide variety of online as well as offline services. These systems rely on complex learning methods and vast amounts of data to optimize the service functionality, satisfaction of the end user and profitability. However, there is a growing concern that these automated decisions can lead, even in the absence of intent, to a lack of fairness, i.e., their outcomes can disproportionately hurt (or, benefit) particular groups of people sharing one or more sensitive attributes (e.g., race, sex). In this paper, we introduce a flexible mechanism to design fair classifiers by leveraging a novel intuitive measure of decision boundary (un)fairness. We instantiate this mechanism with two well-known classifiers, logistic regression and support vector machines, and show on real-world data that our mechanism allows for a fine-grained control on the degree of fairness, often at a small cost in terms of accuracy.},
  langid = {english},
  file = {/home/iris.dominguez/Documents/documents/universidad/docs/Library/Zotero/Zafar et al_2017_Fairness Constraints.pdf;/home/iris.dominguez/snap/zotero-snap/common/Zotero/storage/RR8YQJWS/Zafar et al. - 2017 - Fairness Constraints Mechanisms for Fair Classifi.pdf}
}

@inproceedings{Zhao2021,
  title = {Understanding and {{Evaluating Racial Biases}} in {{Image Captioning}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Zhao, Dora and Wang, Angelina and Russakovsky, Olga},
  year = {2021},
  month = oct,
  pages = {14810--14820},
  issn = {2380-7504},
  doi = {10.1109/ICCV48922.2021.01456},
  urldate = {2023-10-16},
  abstract = {Image captioning is an important task for benchmarking visual reasoning and for enabling accessibility for people with vision impairments. However, as in many machine learning settings, social biases can influence image captioning in undesirable ways. In this work, we study bias propagation pathways within image captioning, focusing specifically on the COCO dataset. Prior work has analyzed gender bias in captions using automatically-derived gender labels; here we examine racial and intersectional biases using manual annotations. Our first contribution is in annotating the perceived gender and skin color of 28,315 of the depicted people after obtaining IRB approval. Using these annotations, we compare racial biases present in both manual and automatically-generated image captions. We demonstrate differences in caption performance, sentiment, and word choice between images of lighter versus darker-skinned people. Further, we find the magnitude of these differences to be greater in modern captioning systems compared to older ones, thus leading to concerns that without proper consideration and mitigation these differences will only become increasingly prevalent. Code and data is available at https://princetonvisualai.github.io/imagecaptioning-bias/.},
  annotation = {18 citations (Crossref) [2023-10-16]},
  file = {/home/iris.dominguez/Documents/documents/universidad/docs/Library/Zotero/Zhao et al_2021_Understanding and Evaluating Racial Biases in Image Captioning.pdf}
}
