@misc{_MMAFACIALEXPRESSION,
  title = {{{MMA FACIAL EXPRESSION}}},
  url = {https://www.kaggle.com/mahmoudima/mma-facial-expression},
  urldate = {2022-06-01},
  abstract = {Facial expression images},
  langid = {english},
  keywords = {datasets},
  file = {../../../../../snap/zotero-snap/common/Zotero/storage/SWZU5JNL/mma-facial-expression.html}
}

@misc{_NaturalHumanFace,
  title = {Natural {{Human Face Images}} for {{Emotion Recognition}}},
  url = {https://www.kaggle.com/sudarshanvaidya/random-images-for-face-emotion-recognition},
  urldate = {2022-06-01},
  abstract = {Natural Human Face Images for Emotion Recognition - 8 emotions},
  langid = {english},
  file = {../../../../../snap/zotero-snap/common/Zotero/storage/3ARAPT5X/random-images-for-face-emotion-recognition.html}
}

@article{Abbasi2019,
  title = {Fairness in Representation: Quantifying Stereotyping as a Representational Harm},
  author = {Abbasi, Tanya Mohsen and Friedler, Sorelle A. and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  year = {2019},
  month = may,
  journal = {Proceedings of the 2019 SIAM International Conference on Data Mining (SDM)},
  pages = {801--809},
  doi = {10.1137/1.9781611975673},
  urldate = {2022-06-29},
  abstract = {While harms of allocation have been increasingly studied as part of the subfield of algorithmic fairness, harms of representation have received considerably less attention. In this paper, we formalize two notions of stereotyping and show how they manifest in later allocative harms within the machine learning pipeline. We also propose mitigation strategies and demonstrate their effectiveness on synthetic datasets.},
  langid = {english},
  file = {Zotero/Abbasi et al_2019_Fairness in representation.pdf}
}

@incollection{Ahmad2022,
  title = {Comparing the {{Performance}} of {{Facial Emotion Recognition Systems}} on {{Real-Life Videos}}: {{Gender}}, {{Ethnicity}} and {{Age}}},
  shorttitle = {Comparing the {{Performance}} of {{Facial Emotion Recognition Systems}} on {{Real-Life Videos}}},
  booktitle = {Proceedings of the {{Future Technologies Conference}} ({{FTC}}) 2021, {{Volume}} 1},
  author = {Ahmad, Khurshid and Wang, Shirui and Vogel, Carl and Jain, Pranav and O'Neill, Oscar and Sufi, Basit Hamid},
  editor = {Arai, Kohei},
  year = {2022},
  volume = {358},
  pages = {193--210},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-89906-6\_14},
  urldate = {2021-10-26},
  abstract = {Dealing with non-verbal communications will be a key breakthrough for future technologies as much of the effort of the 21st century technologies has been in dealing with numbers and verbal communications. The automatic recognition of facial expressions is of theoretical and commercial interests and to this end there must exist video databases that incorporate the idiosyncrasies of human existence \textendash{} ethnicity, gender and age. We compare the performance of three major emotion recognition software systems on real life videos of politicians from across the world. Our sample of 45 videos (total length of 2 h 26 min, with 219150 frames) is composed of male and female politicians ranging in age from 40 to 78 with well-defined differences related to gender and nationality/ethnicity. Our sample of images are partially posed and partially spontaneous \textendash{} the demeanour of politicians when they engage in speech making. Our target systems, Micorosoft Azure Cognitive Services Face API, Affectiva AFFDEX and Emotient FACET, have been trained on posed expressions usually, with limited testing on spontaneous images, so in effect we are operating at the edge of the performance of these systems. There are similarities in the performance of these systems on some emotions, especially joy, but there are differences in emotion recognition, such as anger. There are also gender differences as well as differences based on age and race. This is an important issue as more and more video data is becoming available and video analytics that can deal with aspects of cognition, like emotion, accurately and across cultural/gender/ethnic divides will be a major component of future technologies.},
  isbn = {978-3-030-89905-9 978-3-030-89906-6},
  langid = {english},
  keywords = {bias_example,review},
  file = {Zotero/Ahmad et al_2022_Comparing the Performance of Facial Emotion Recognition Systems on Real-Life.pdf}
}

@article{Aifanti2010,
  title = {The {{MUG}} Facial Expression Database},
  author = {Aifanti, Niki and Papachristou, Christos and Delopoulos, A.},
  year = {2010},
  month = apr,
  journal = {11th International Workshop on Image Analysis for Multimedia Interactive Services WIAMIS 10},
  url = {https://www.semanticscholar.org/paper/The-MUG-facial-expression-database-Aifanti-Papachristou/f1af714b92372c8e606485a3982eab2f16772ad8},
  urldate = {2023-03-27},
  abstract = {This paper presents a new extended collection of posed and induced facial expression image sequences. All sequences were captured in a controlled laboratory environment with high resolution and no occlusions. The collection consists of two parts: The first part depicts eighty six subjects performing the six basic expressions according to the ``emotion prototypes'' as defined in the Investigator's Guide in the FACS manual. The second part contains the same subjects recorded while they were watching an emotion inducing video. Most of the database recordings are available to the scientific community. Beyond the emotion related annotation the database contains also manual and automatic annotation of 80 facial landmark points for a significant number of frames. The database contains sufficient material for the development and the statistical evaluation of facial expression recognition systems using posed and induced expressions.}
}

@article{Aifanti2010old,
  title = {The {{MUG}} Facial Expression Database},
  author = {Aifanti, Niki and Papachristou, Christos and Delopoulos, Anastasios},
  year = {2010},
  month = may,
  pages = {5},
  abstract = {This paper presents a new extended collection of posed and induced facial expression image sequences. All sequences were captured in a controlled laboratory environment with high resolution and no occlusions. The collection consists of two parts: The first part depicts eighty six subjects performing the six basic expressions according to the ``emotion prototypes'' as defined in the Investigator's Guide in the FACS manual. The second part contains the same subjects recorded while they were watching an emotion inducing video. Most of the database recordings are available to the scientific community. Beyond the emotion related annotation the database contains also manual and automatic annotation of 80 facial landmark points for a significant number of frames. The database contains sufficient material for the development and the statistical evaluation of facial expression recognition systems using posed and induced expressions.},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Aifanti et al_2010_The MUG facial expression database.pdf}
}

@inproceedings{Aneja2017,
  title = {Modeling {{Stylized Character Expressions}} via {{Deep Learning}}},
  booktitle = {Computer {{Vision}} \textendash{}  {{ACCV}} 2016},
  author = {Aneja, Deepali and Colburn, Alex and Faigin, Gary and Shapiro, Linda and Mones, Barbara},
  editor = {Lai, Shang-Hong and Lepetit, Vincent and Nishino, Ko and Sato, Yoichi},
  year = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {136--153},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-54184-6\_9},
  abstract = {We propose DeepExpr, a novel expression transfer approach from humans to multiple stylized characters. We first train two Convolutional Neural Networks to recognize the expression of humans and stylized characters independently. Then we utilize a transfer learning technique to learn the mapping from humans to characters to create a shared embedding feature space. This embedding also allows human expression-based image retrieval and character expression-based image retrieval. We use our perceptual model to retrieve character expressions corresponding to humans. We evaluate our method on a set of retrieval tasks on our collected stylized character dataset of expressions. We also show that the ranking order predicted by the proposed features is highly correlated with the ranking order provided by a facial expression expert and Mechanical Turk experiments.},
  isbn = {978-3-319-54184-6},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Aneja et al_2017_Modeling Stylized Character Expressions via Deep Learning.pdf}
}

@article{Assuncao2022,
  title = {An {{Overview}} of {{Emotion}} in {{Artificial Intelligence}}},
  author = {Assuncao, Gustavo and Patrao, Bruno and {Castelo-Branco}, Miguel and Menezes, Paulo},
  year = {2022},
  journal = {IEEE Transactions on Artificial Intelligence},
  pages = {1--1},
  issn = {2691-4581},
  doi = {10.1109/TAI.2022.3159614},
  abstract = {The field of Artificial Intelligence (AI) has gained immense traction over the past decade, producing increasingly successful applications as research strives to understand and exploit neural processing specifics. Nonetheless emotion, despite its demonstrated significance to reinforcement, social integration and general development, remains a largely stigmatized and consequently disregarded topic by most engineers and computer scientists. In this paper we endorse emotion's value for the advancement of artificial cognitive processing, as well as explore real-world use cases of emotion-augmented AI. A schematization is provided on the psychological-neurophysiologic basics of emotion in order to bridge the interdisciplinary gap preventing emulation and integration in AI methodology, as well as exploitation by current systems. In addition we overview three major subdomains of AI greatly benefiting from emotion, and produce a systematic survey of meaningful yet recent contributions to each area. To conclude, we address crucial challenges and promising research paths for the future of emotion in AI with the hope that more researchers will develop an interest for the topic and find it easier to develop their own contributions.},
  keywords = {review},
  file = {Zotero/Assuncao et al_2022_An Overview of Emotion in Artificial Intelligence.pdf}
}

@article{Atkinson2005,
  title = {Asymmetric Interference between Sex and Emotion in Face Perception},
  author = {Atkinson, Anthony P. and Tipples, Jason and Burt, D. Michael and Young, Andrew W.},
  year = {2005},
  month = oct,
  journal = {Perception \& Psychophysics},
  volume = {67},
  number = {7},
  pages = {1199--1213},
  issn = {0031-5117, 1532-5962},
  doi = {10.3758/BF03193553},
  urldate = {2021-11-16},
  langid = {english},
  keywords = {bias_example,psychology},
  file = {Zotero/Atkinson et al_2005_Asymmetric interference between sex and emotion in face perception.pdf}
}

@article{Avella2020,
  title = {Crime {{Prediction Artificial Intelligence}} and the {{Impact}} on {{Human Rights}}},
  author = {Avella, Marcela del Pilar Roa},
  year = {2020},
  month = aug,
  journal = {Telecommunications System \& Management},
  volume = {0},
  number = {0},
  pages = {2--2},
  publisher = {{Hilaris SRL}},
  issn = {2167-0919},
  url = {https://www.hilarispublisher.com/abstract/crime-prediction-artificial-intelligence-and-the-impact-on-human-rights-49422.html},
  urldate = {2022-06-27},
  abstract = {Predicting crime is concerned with the search / examination of predisposing factors of criminal activity in people or places. Currently, there exists A.I tools which attempt to predict levels of risk of crime and facial recognition software. All of these aim to anticipate criminal decisions by tracking trigger factors. Amongst the most recognised algorithms are COMPAS (Correctional Offender Management Profiling for Alternative Sanctions), PROMETEA, Big Brother Watch, PredPol, Harm Assessment Risk Tool (HART) and Actuarial Risk Assessment Instruments (ARAIs). The aim of this paper is to determine the impact of these tools on human rights through a descriptive investigation using deductive analysis. The results demonstrate that the incorporation of A.I tools for predicting crimes does not guarantee the avoidance of discrimination or bias due to human intervention when it comes to selecting the data that feeds into the algorithm. Moreover, the so-called black box prevents reverse engineering to understand the software's intelligent process in the decision-making and deliberation of the factors that are being analysed, which constitutes a violation of human rights},
  langid = {english},
  keywords = {bias_example,harm},
  file = {Zotero/Avella_2020_Crime Prediction Artificial Intelligence and the Impact on Human Rights.pdf;../../../../../snap/zotero-snap/common/Zotero/storage/TEREG6DF/crime-prediction-artificial-intelligence-and-the-impact-on-human-rights-49422.html}
}

@article{Banziger2012,
  title = {Introducing the {{Geneva Multimodal}} Expression Corpus for Experimental Research on Emotion Perception},
  author = {B{\"a}nziger, Tanja and Mortillaro, Marcello and Scherer, Klaus R.},
  year = {2012},
  journal = {Emotion},
  volume = {12},
  number = {5},
  pages = {1161--1179},
  issn = {1931-1516, 1528-3542},
  doi = {10.1037/a0025827},
  urldate = {2021-08-12},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Bänziger et al_2012_Introducing the Geneva Multimodal expression corpus for experimental research.pdf}
}

@book{Barocas2019,
  title = {Fairness and Machine Learning},
  author = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
  year = {2019},
  publisher = {{fairmlbook.org}},
  file = {Zotero/Barocas et al_2019_Fairness and machine learning.pdf}
}

@inproceedings{Barsoum2016,
  title = {Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution},
  booktitle = {Proceedings of the 18th {{ACM International Conference}} on {{Multimodal Interaction}}},
  author = {Barsoum, Emad and Zhang, Cha and Ferrer, Cristian Canton and Zhang, Zhengyou},
  year = {2016},
  month = oct,
  pages = {279--283},
  publisher = {{ACM}},
  address = {{Tokyo Japan}},
  doi = {10.1145/2993148.2993165},
  urldate = {2023-03-23},
  isbn = {978-1-4503-4556-9},
  langid = {english},
  file = {Zotero/Barsoum et al_2016_Training deep networks for facial expression recognition with crowd-sourced.pdf}
}

@article{Barsoum2016arxiv,
  title = {Training {{Deep Networks}} for {{Facial Expression Recognition}} with {{Crowd-Sourced Label Distribution}}},
  author = {Barsoum, Emad and Zhang, Cha and Ferrer, Cristian Canton and Zhang, Zhengyou},
  year = {2016},
  month = sep,
  journal = {arXiv:1608.01041 [cs]},
  eprint = {1608.01041},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1608.01041},
  urldate = {2021-08-10},
  abstract = {Crowd sourcing has become a widely adopted scheme to collect ground truth labels. However, it is a well-known problem that these labels can be very noisy. In this paper, we demonstrate how to learn a deep convolutional neural network (DCNN) from noisy labels, using facial expression recognition as an example. More specifically, we have 10 taggers to label each input image, and compare four different approaches to utilizing the multiple labels: majority voting, multi-label learning, probabilistic label drawing, and cross-entropy loss. We show that the traditional majority voting scheme does not perform as well as the last two approaches that fully leverage the label distribution. An enhanced FER+ data set with multiple labels for each face image will also be shared with the research community.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Barsoum et al_2016_Training Deep Networks for Facial Expression Recognition with Crowd-Sourced.pdf}
}

@article{Beaupre2005,
  title = {Cross-{{Cultural Emotion Recognition}} among {{Canadian Ethnic Groups}}},
  author = {Beaupr{\'e}, Martin G. and Hess, Ursula},
  year = {2005},
  month = may,
  journal = {Journal of Cross-Cultural Psychology},
  volume = {36},
  number = {3},
  pages = {355--370},
  publisher = {{SAGE Publications Inc}},
  issn = {0022-0221},
  doi = {10.1177/0022022104273656},
  urldate = {2022-06-01},
  abstract = {This study aims to investigate cultural differences in recognition accuracy as well as the in-group advantage hypothesis for emotion recognition among sub-Saharan African, Chinese, and French Canadian individuals living in Canada. The participants viewed expressions of happiness, anger, sadness, fear, disgust, and shame selected from the Montreal Set of Facial Displays of Emotion. These data did not support the in-group advantage hypothesis under the condition of stimulus equivalence. However, both encoder and decoder effects were found. Specifically, French Canadians were more accurate for the decoding of expressions of shame and sadness. Moreover, fear expressions were best recognized when shown by sub-Saharan Africans, suggesting an effect of salience of expressive cues due to morphological features of the face.},
  langid = {english}
}

@techreport{Bell1966,
  title = {A Comparison of Some Cluster-Seeking Techniques},
  author = {Bell, Geoffrey H},
  year = {1966},
  address = {{Menlo Park, California}},
  institution = {{Stanford Research Institute}}
}

@inproceedings{Benitez-Quiroz2016,
  title = {{{EmotioNet}}: {{An Accurate}}, {{Real-Time Algorithm}} for the {{Automatic Annotation}} of a {{Million Facial Expressions}} in the {{Wild}}},
  shorttitle = {{{EmotioNet}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {{Benitez-Quiroz}, C. Fabian and Srinivasan, Ramprakash and Martinez, Aleix M.},
  year = {2016},
  month = jun,
  pages = {5562--5570},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.600},
  urldate = {2021-11-23},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Benitez-Quiroz et al_2016_EmotioNet.pdf}
}

@article{Berger1970,
  title = {Diversity of {{Planktonic Foraminifera}} in {{Deep-Sea Sediments}}},
  author = {Berger, Wolfgang H. and Parker, Frances L.},
  year = {1970},
  month = jun,
  journal = {Science},
  volume = {168},
  number = {3937},
  pages = {1345--1347},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.168.3937.1345},
  urldate = {2022-12-04},
  langid = {english},
  keywords = {metrics}
}

@article{Bergsma2013,
  title = {A Bias-Correction for {{Cram\'er}}'s and {{Tschuprow}}'s},
  author = {Bergsma, Wicher},
  year = {2013},
  month = sep,
  journal = {Journal of the Korean Statistical Society},
  volume = {42},
  number = {3},
  pages = {323--328},
  issn = {12263192},
  doi = {10.1016/j.jkss.2012.10.002},
  urldate = {2023-01-04},
  abstract = {Cram\textasciiacute er's V and Tschuprow's T are closely related nominal variable association measures, which are usually estimated by their empirical values. Although these estimators are consistent, they can have large bias for finite samples, making interpretation difficult. We propose a new and simple bias correction and show via simulations that, for larger than 2 \texttimes{} 2 tables, the newly obtained estimators outperform the classical (empirical) ones. For 2 \texttimes{} 2 tables performance is comparable. The larger the table and the smaller the sample size, the greater the superiority of the new estimators.},
  langid = {english},
  file = {Zotero/Bergsma_2013_A bias-correction for Cramér’s and Tschuprow’s.pdf}
}

@article{Berk2018,
  title = {Fairness in {{Criminal Justice Risk Assessments}}: {{The State}} of the {{Art}}},
  shorttitle = {Fairness in {{Criminal Justice Risk Assessments}}},
  author = {Berk, Richard and Heidari, Hoda and Jabbari, Shahin and Kearns, Michael and Roth, Aaron},
  year = {2018},
  month = jul,
  journal = {Sociological Methods \& Research},
  volume = {50},
  number = {1},
  pages = {3--44},
  issn = {0049-1241, 1552-8294},
  doi = {10.1177/0049124118782533},
  urldate = {2022-02-09},
  abstract = {Objectives: Discussions of fairness in criminal justice risk assessments typically lack conceptual precision. Rhetoric too often substitutes for careful analysis. In this article, we seek to clarify the trade-offs between different kinds of fairness and between fairness and accuracy. Methods: We draw on the existing literatures in criminology, computer science, and statistics to provide an integrated examination of fairness and accuracy in criminal justice risk assessments. We also provide an empirical illustration using data from arraignments. Results: We show that there are at least six kinds of fairness, some of which are incompatible with one another and with accuracy. Conclusions: Except in trivial cases, it is impossible to maximize accuracy and fairness at the same time and impossible simultaneously to satisfy all kinds of fairness. In practice, a major complication is different base rates across different legally protected groups. There is a need to consider challenging trade-offs. These lessons apply to applications well beyond criminology where assessments of risk can be used by decision makers. Examples include mortgage lending, employment, college admissions, child welfare, and medical diagnoses.},
  langid = {english},
  keywords = {fairness},
  file = {Zotero/Berk et al_2018_Fairness in Criminal Justice Risk Assessments.pdf}
}

@article{Beyer2020,
  title = {Are We Done with {{ImageNet}}?},
  author = {Beyer, Lucas and H{\'e}naff, Olivier J. and Kolesnikov, Alexander and Zhai, Xiaohua and van den Oord, A{\"a}ron},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.07159 [cs]},
  eprint = {2006.07159},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2006.07159},
  urldate = {2021-09-30},
  abstract = {Yes, and no. We ask whether recent progress on the ImageNet classification benchmark continues to represent meaningful generalization, or whether the community has started to overfit to the idiosyncrasies of its labeling procedure. We therefore develop a significantly more robust procedure for collecting human annotations of the ImageNet validation set. Using these new labels, we reassess the accuracy of recently proposed ImageNet classifiers, and find their gains to be substantially smaller than those reported on the original labels. Furthermore, we find the original ImageNet labels to no longer be the best predictors of this independently-collected set, indicating that their usefulness in evaluating vision models may be nearing an end. Nevertheless, we find our annotation procedure to have largely remedied the errors in the original labels, reinforcing ImageNet as a powerful benchmark for future research in visual recognition3.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {bias_example,datasets},
  file = {Zotero/Beyer et al_2020_Are we done with ImageNet.pdf}
}

@article{Biehl_MatsumotoEkmanJapanese,
  title = {Matsumoto and {{Ekman}}'s {{Japanese}} and {{Caucasian Facial Expressions}} of {{Emotion}} ({{JACFEE}}): {{Reliability Data}} and {{Cross-National Differences}}},
  author = {Biehl, Michael and Matsumoto, David and Ekman, Paul and Meant, Valerie and Heider, Karl and Kudoh, Tsutomu and Ton, Veronica},
  journal = {JOURNAL OF NONVERBAL BEHAVIOR},
  pages = {19},
  abstract = {Substantial research has documented the universality of several emotional expressions. However, recent findings have demonstrated cultural differences in level of recognition and ratings of intensity. When testing cultural differences, stimulus sets must meet certain requirements. Matsumoto and Ekman's Japanese and Caucasian Facial Expressions of Emotion (JACFEE) is the only set that meets these requirements. The purpose of this study was to obtain judgment reliability data on the JACFEE, and to test for possible cross-national differences in judgments as well. Subjects from Hungary, Japan, Poland, Sumatra, United States, and Vietnam viewed the complete JACFEE photo set and judged which emotions were portrayed in the photos and rated the intensity of those expressions. Results revealed high agreement across countries in identifying the emotions portrayed in the photos, demonstrating the reliability of the JACFEE. Despite high agreement, cross-national differences were found in the exact level of agreement for photos of anger, contempt, disgust, fear, sadness, and surprise. Cross-national differences were also found in the level of intensity attributed to the photos. No systematic variation due to either preceding emotion or presentation order of the JACFEE was found. Also, we found that grouping the countries into a Western/Non-Western dichotomy was not justified according to the data. Instead, the cross-national differences are discussed in terms of possible sociopsychological variables that influence emotion judgments.},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Biehl et al_Matsumoto and Ekman's Japanese and Caucasian Facial Expressions of Emotion.pdf}
}

@misc{Birhane2021,
  title = {Multimodal Datasets: Misogyny, Pornography, and Malignant Stereotypes},
  shorttitle = {Multimodal Datasets},
  author = {Birhane, Abeba and Prabhu, Vinay Uday and Kahembwe, Emmanuel},
  year = {2021},
  month = oct,
  number = {arXiv:2110.01963},
  eprint = {arXiv:2110.01963},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2110.01963},
  urldate = {2022-06-23},
  abstract = {We have now entered the era of trillion parameter machine learning models trained on billion-sized datasets scraped from the internet. The rise of these gargantuan datasets has given rise to formidable bodies of critical work that has called for caution while generating these large datasets. These address concerns surrounding the dubious curation practices used to generate these datasets, the sordid quality of alt-text data available on the world wide web, the problematic content of the CommonCrawl dataset often used as a source for training large language models, and the entrenched biases in large-scale visio-linguistic models (such as OpenAI's CLIP model) trained on opaque datasets (WebImageText). In the backdrop of these specific calls of caution, we examine the recently released LAION-400M dataset, which is a CLIP-filtered dataset of Image-Alt-text pairs parsed from the Common-Crawl dataset. We found that the dataset contains, troublesome and explicit images and text pairs of rape, pornography, malign stereotypes, racist and ethnic slurs, and other extremely problematic content. We outline numerous implications, concerns and downstream harms regarding the current state of large scale datasets while raising open questions for various stakeholders including the AI community, regulators, policy makers and data subjects.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {bias_example,stereotypical_bias},
  file = {Zotero/Birhane et al_2021_Multimodal datasets.pdf}
}

@article{Bjornsdottir2017,
  title = {The Visibility of Social Class from Facial Cues},
  author = {Bjornsdottir, R. Thora and Rule, Nicholas O.},
  year = {2017},
  month = oct,
  journal = {Journal of Personality and Social Psychology},
  volume = {113},
  number = {4},
  pages = {530--546},
  issn = {1939-1315},
  doi = {10.1037/pspa0000091},
  abstract = {Social class meaningfully impacts individuals' life outcomes and daily interactions, and the mere perception of one's socioeconomic standing can have significant ramifications. To better understand how people infer others' social class, we therefore tested the legibility of class (operationalized as monetary income) from facial images, finding across 4 participant samples and 2 stimulus sets that perceivers categorized the faces of rich and poor targets significantly better than chance. Further investigation showed that perceivers categorize social class using minimal facial cues and employ a variety of stereotype-related impressions to make their judgments. Of these, attractiveness accurately cued higher social class in self-selected dating profile photos. However, only the stereotype that well-being positively relates to wealth served as a valid cue in neutral faces. Indeed, neutrally posed rich targets displayed more positive affect relative to poor targets and perceivers used this affective information to categorize their social class. Impressions of social class from these facial cues also influenced participants' evaluations of the targets' employability, demonstrating that face-based perceptions of social class may have important downstream consequences. (PsycINFO Database Record},
  langid = {english},
  pmid = {28557470},
  keywords = {bias_against_the_poor},
  file = {Zotero/Bjornsdottir_Rule_2017_The visibility of social class from facial cues.pdf}
}

@inproceedings{Blodgett2020,
  title = {Language ({{Technology}}) Is {{Power}}: {{A Critical Survey}} of ``{{Bias}}'' in {{NLP}}},
  shorttitle = {Language ({{Technology}}) Is {{Power}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Blodgett, Su Lin and Barocas, Solon and Daum{\'e} III, Hal and Wallach, Hanna},
  year = {2020},
  pages = {5454--5476},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.485},
  urldate = {2021-11-03},
  abstract = {We survey 146 papers analyzing ``bias'' in NLP systems, fnding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing ``bias'' is an inherently normative process. We further fnd that these papers' proposed quantitative techniques for measuring or mitigating ``bias'' are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these fndings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing ``bias'' in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of ``bias''\textemdash i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements\textemdash and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.},
  langid = {english},
  keywords = {fairness},
  file = {Zotero/Blodgett et al_2020_Language (Technology) is Power.pdf}
}

@article{Bordalo2016,
  title = {Stereotypes*},
  author = {Bordalo, Pedro and Coffman, Katherine and Gennaioli, Nicola and Shleifer, Andrei},
  year = {2016},
  month = nov,
  journal = {The Quarterly Journal of Economics},
  volume = {131},
  number = {4},
  pages = {1753--1794},
  issn = {0033-5533, 1531-4650},
  doi = {10.1093/qje/qjw029},
  urldate = {2022-06-29},
  abstract = {Abstract             We present a model of stereotypes based on Kahneman and Tversky's representativeness heuristic. A decision maker assesses a target group by overweighting its representative types, defined as the types that occur more frequently in that group than in a baseline reference group. Stereotypes formed this way contain a ``kernel of truth'': they are rooted in true differences between groups. Because stereotypes focus on differences, they cause belief distortions, particularly when groups are similar. Stereotypes are also context dependent: beliefs about a group depend on the characteristics of the reference group. In line with our predictions, beliefs in the lab about abstract groups and beliefs in the field about political groups are context dependent and distorted in the direction of representative types.},
  langid = {english},
  file = {Zotero/Bordalo et al_2016_Stereotypes.pdf}
}

@inproceedings{Bouma2009,
  title = {Normalized (Pointwise) Mutual Information in Collocation Extraction},
  booktitle = {Proceedings of {{GSCL}}},
  author = {Bouma, Gerlof},
  year = {2009},
  volume = {30},
  pages = {31--40},
  address = {{GSCL}},
  file = {Zotero/Bouma_2009_Normalized (pointwise) mutual information in collocation extraction.pdf}
}

@inproceedings{Buolamwini2018,
  title = {Gender Shades: {{Intersectional}} Accuracy Disparities in Commercial Gender Classification},
  booktitle = {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  author = {Buolamwini, Joy and Gebru, Timnit},
  editor = {Friedler, Sorelle A. and Wilson, Christo},
  year = {2018},
  month = feb,
  series = {Proceedings of Machine Learning Research},
  volume = {81},
  pages = {77--91},
  publisher = {{PMLR}},
  url = {https://proceedings.mlr.press/v81/buolamwini18a.html},
  abstract = {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6\% for IJB-A and 86.2\% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7\%). The maximum error rate for lighter-skinned males is 0.8\%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.},
  pdf = {http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf},
  keywords = {bias_example},
  file = {Zotero/Buolamwini_Gebru_2018_Gender shades.pdf}
}

@article{Busso2008,
  title = {{{IEMOCAP}}: Interactive Emotional Dyadic Motion Capture Database},
  shorttitle = {{{IEMOCAP}}},
  author = {Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N. and Lee, Sungbok and Narayanan, Shrikanth S.},
  year = {2008},
  month = dec,
  journal = {Language Resources and Evaluation},
  volume = {42},
  number = {4},
  pages = {335--359},
  issn = {1574-020X, 1574-0218},
  doi = {10.1007/s10579-008-9076-6},
  urldate = {2021-11-16},
  abstract = {Since emotions are expressed through a combination of verbal and non-verbal channels, a joint analysis of speech and gestures is required to understand expressive human communication. To facilitate such investigations, this paper describes a new corpus named the ``interactive emotional dyadic motion capture database'' (IEMOCAP), collected by the Speech Analysis and Interpretation Laboratory (SAIL) at the University of Southern California (USC). This database was recorded from ten actors in dyadic sessions with markers on the face, head, and hands, which provide detailed information about their facial expression and hand movements during scripted and spontaneous spoken communication scenarios. The actors performed selected emotional scripts and also improvised hypothetical scenarios designed to elicit specific types of emotions (happiness, anger, sadness, frustration and neutral state). The corpus contains approximately twelve hours of data. The detailed motion capture information, the interactive setting to elicit authentic emotions, and the size of the database make this corpus a valuable addition to the existing databases in the community for the study and modeling of multimodal and expressive human communication.},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Busso et al_2008_IEMOCAP.pdf}
}

@inproceedings{Calders2009,
  title = {Building {{Classifiers}} with {{Independency Constraints}}},
  booktitle = {2009 {{IEEE International Conference}} on {{Data Mining Workshops}}},
  author = {Calders, Toon and Kamiran, Faisal and Pechenizkiy, Mykola},
  year = {2009},
  month = dec,
  pages = {13--18},
  issn = {2375-9259},
  doi = {10.1109/ICDMW.2009.83},
  abstract = {In this paper we study the problem of classifier learning where the input data contains unjustified dependencies between some data attributes and the class label. Such cases arise for example when the training data is collected from different sources with different labeling criteria or when the data is generated by a biased decision process. When a classifier is trained directly on such data, these undesirable dependencies will carry over to the classifier's predictions. In order to tackle this problem, we study the classification with independency constraints problem: find an accurate model for which the predictions are independent from a given binary attribute. We propose two solutions for this problem and present an empirical validation.},
  keywords = {bias_mitigation},
  file = {Zotero/Calders et al_2009_Building Classifiers with Independency Constraints.pdf}
}

@article{Chen2017,
  title = {Smile Detection in the Wild with Deep Convolutional Neural Networks},
  author = {Chen, Junkai and Ou, Qihao and Chi, Zheru and Fu, Hong},
  year = {2017},
  month = feb,
  journal = {Machine Vision and Applications},
  volume = {28},
  number = {1},
  pages = {173--183},
  issn = {1432-1769},
  doi = {10.1007/s00138-016-0817-z},
  urldate = {2022-03-28},
  abstract = {Smile or happiness is one of the most universal facial expressions in our daily life. Smile detection in the wild is an important and challenging problem, which has attracted a growing attention from affective computing community. In this paper, we present an efficient approach for smile detection in the wild with deep learning. Different from some previous work which extracted hand-crafted features from face images and trained a classifier to perform smile recognition in a two-step approach, deep learning can effectively combine feature learning and classification into a single model. In this study, we apply the deep convolutional network, a popular deep learning model, to handle this problem. We construct a deep convolutional network called Smile-CNN to perform feature learning and smile detection simultaneously. Experimental results demonstrate that although a deep learning model is generally developed for tackling ``big data,'' the model can also effectively deal with ``small data.'' We further investigate into the discriminative power of the learned features, which are taken from the neuron activations of the last hidden layer of our Smile-CNN. By using the learned features to train an SVM or AdaBoost classifier, we show that the learned features have impressive discriminative ability. Experiments conducted on the GENKI4K database demonstrate that our approach can achieve a promising performance in smile detection.},
  langid = {english},
  file = {Zotero/Chen et al_2017_Smile detection in the wild with deep convolutional neural networks.pdf}
}

@article{Chen2018,
  title = {Distinct Facial Expressions Represent Pain and Pleasure across Cultures},
  author = {Chen, Chaona and Crivelli, Carlos and Garrod, Oliver G. B. and Schyns, Philippe G. and {Fern{\'a}ndez-Dols}, Jos{\'e}-Miguel and Jack, Rachael E.},
  year = {2018},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {43},
  pages = {E10013-E10021},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1807862115},
  urldate = {2021-08-24},
  abstract = {Real-world studies show that the facial expressions produced during pain and orgasm\textemdash two different and intense affective experiences\textemdash are virtually indistinguishable. However, this finding is counterintuitive, because facial expressions are widely considered to be a powerful tool for social interaction. Consequently, debate continues as to whether the facial expressions of these extreme positive and negative affective states serve a communicative function. Here, we address this debate from a novel angle by modeling the mental representations of dynamic facial expressions of pain and orgasm in 40 observers in each of two cultures (Western, East Asian) using a data-driven method. Using a complementary approach of machine learning, an information-theoretic analysis, and a human perceptual discrimination task, we show that mental representations of pain and orgasm are physically and perceptually distinct in each culture. Cross-cultural comparisons also revealed that pain is represented by similar face movements across cultures, whereas orgasm showed distinct cultural accents. Together, our data show that mental representations of the facial expressions of pain and orgasm are distinct, which questions their nondiagnosticity and instead suggests they could be used for communicative purposes. Our results also highlight the potential role of cultural and perceptual factors in shaping the mental representation of these facial expressions. We discuss new research directions to further explore their relationship to the production of facial expressions.},
  langid = {english},
  keywords = {bias_example,psychology},
  file = {Zotero/Chen et al_2018_Distinct facial expressions represent pain and pleasure across cultures.pdf}
}

@article{Cheong2021,
  title = {The {{Hitchhiker}}'s {{Guide}} to {{Bias}} and {{Fairness}} in {{Facial Affective Signal Processing}}: {{Overview}} and Techniques},
  shorttitle = {The {{Hitchhiker}}'s {{Guide}} to {{Bias}} and {{Fairness}} in {{Facial Affective Signal Processing}}},
  author = {Cheong, Jiaee and Kalkan, Sinan and Gunes, Hatice},
  year = {2021},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {38},
  number = {6},
  pages = {39--49},
  issn = {1558-0792},
  doi = {10.1109/MSP.2021.3106619},
  abstract = {Given the increasing prevalence of facial analysis technology, the problem of bias in the tools is now becoming an even greater source of concern. Several studies have highlighted the pervasiveness of such discrimination, and many have sought to address the problem by proposing solutions to mitigate it. Despite this effort, to date, understanding, investigating, and mitigating bias for facial affect analysis remain an understudied problem. In this work we aim to provide a guide by 1) providing an overview of the various definitions of bias and measures of fairness within the field of facial affective signal processing and 2) categorizing the algorithms and techniques that can be used to investigate and mitigate bias in facial affective signal processing. We present the opportunities and limitations within the current body of work, discuss the gathered findings, and propose areas that call for further research.},
  keywords = {fairness,review},
  file = {Zotero/Cheong et al_2021_The Hitchhiker’s Guide to Bias and Fairness in Facial Affective Signal.pdf}
}

@article{Christoforaki2022,
  title = {{{AI Ethics}}\textemdash{{A Bird}}'s {{Eye View}}},
  author = {Christoforaki, Maria and Beyan, Oya},
  year = {2022},
  month = jan,
  journal = {Applied Sciences},
  volume = {12},
  number = {9},
  pages = {4130},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2076-3417},
  doi = {10.3390/app12094130},
  urldate = {2023-01-26},
  abstract = {The explosion of data-driven applications using Artificial Intelligence (AI) in recent years has given rise to a variety of ethical issues regarding data collection, annotation, and processing using mostly opaque algorithms, as well as the interpretation and employment of the results of the AI pipeline. The ubiquity of AI applications negatively impacts a variety of sensitive areas, ranging from discrimination against vulnerable populations to privacy invasion and the environmental cost that these algorithms entail, and puts into focus on the ever present domain of AI ethics. In this review article we present a bird's eye view approach of the AI ethics landscape, starting from a historical point of view, examining the moral issues that were introduced by big datasets and the application of non-symbolic AI algorithms, the normative approaches (principles and guidelines) to these issues and the ensuing criticism, as well as the actualization of these principles within the proposed frameworks. Subsequently, we focus on the concept of responsibility, both as personal responsibility of the AI practitioners and sustainability, meaning the promotion of beneficence for both the society and the domain, and the role of professional certification and education in averting unethical choices. Finally, we conclude with indicating the multidisciplinary nature of AI ethics and suggesting future challenges.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {ethics,fairness},
  file = {Zotero/Christoforaki_Beyan_2022_AI Ethics—A Bird’s Eye View.pdf}
}

@book{Cohen1988,
  title = {Statistical {{Power Analysis}} for the {{Behavioral Sciences}}},
  author = {Cohen, Jacob},
  year = {1988},
  month = jul,
  edition = {Second},
  publisher = {{Routledge}},
  address = {{New York}},
  doi = {10.4324/9780203771587},
  abstract = {Statistical Power Analysis is a nontechnical guide to power analysis in research planning that provides users of applied statistics with the tools they need for more effective analysis. The Second Edition includes:  * a chapter covering power analysis in set correlation and multivariate methods; * a chapter considering effect size, psychometric reliability, and the efficacy of "qualifying" dependent variables and; * expanded power and sample size tables for multiple regression/correlation.},
  isbn = {978-0-203-77158-7}
}

@article{Cooper2021,
  title = {Emergent {{Unfairness}} in {{Algorithmic Fairness-Accuracy Trade-Off Research}}},
  author = {Cooper, A. Feder and Abrams, Ellen},
  year = {2021},
  month = jul,
  journal = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
  eprint = {2102.01203},
  pages = {46--54},
  doi = {10.1145/3461702.3462519},
  urldate = {2021-09-17},
  abstract = {Across machine learning (ML) sub-disciplines, researchers make explicit mathematical assumptions in order to facilitate proof-writing. We note that, specifically in the area of fairness-accuracy trade-off optimization scholarship, similar attention is not paid to the normative assumptions that ground this approach. Such assumptions presume that 1) accuracy and fairness are in inherent opposition to one another, 2) strict notions of mathematical equality can adequately model fairness, 3) it is possible to measure the accuracy and fairness of decisions independent from historical context, and 4) collecting more data on marginalized individuals is a reasonable solution to mitigate the effects of the trade-off. We argue that such assumptions, which are often left implicit and unexamined, lead to inconsistent conclusions: While the intended goal of this work may be to improve the fairness of machine learning models, these unexamined, implicit assumptions can in fact result in emergent unfairness. We conclude by suggesting a concrete path forward toward a potential resolution.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {fairness},
  file = {Zotero/Cooper_Abrams_2021_Emergent Unfairness in Algorithmic Fairness-Accuracy Trade-Off Research.pdf}
}

@article{Corbett-Davies2018,
  title = {The {{Measure}} and {{Mismeasure}} of {{Fairness}}: {{A Critical Review}} of {{Fair Machine Learning}}},
  shorttitle = {The {{Measure}} and {{Mismeasure}} of {{Fairness}}},
  author = {{Corbett-Davies}, Sam and Goel, Sharad},
  year = {2018},
  month = aug,
  journal = {arXiv:1808.00023 [cs]},
  eprint = {1808.00023},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1808.00023},
  urldate = {2022-02-14},
  abstract = {The nascent field of fair machine learning aims to ensure that decisions guided by algorithms are equitable. Over the last several years, three formal definitions of fairness have gained prominence: (1) anti-classification, meaning that protected attributes---like race, gender, and their proxies---are not explicitly used to make decisions; (2) classification parity, meaning that common measures of predictive performance (e.g., false positive and false negative rates) are equal across groups defined by the protected attributes; and (3) calibration, meaning that conditional on risk estimates, outcomes are independent of protected attributes. Here we show that all three of these fairness definitions suffer from significant statistical limitations. Requiring anti-classification or classification parity can, perversely, harm the very groups they were designed to protect; and calibration, though generally desirable, provides little guarantee that decisions are equitable. In contrast to these formal fairness criteria, we argue that it is often preferable to treat similarly risky people similarly, based on the most statistically accurate estimates of risk that one can produce. Such a strategy, while not universally applicable, often aligns well with policy objectives; notably, this strategy will typically violate both anti-classification and classification parity. In practice, it requires significant effort to construct suitable risk estimates. One must carefully define and measure the targets of prediction to avoid retrenching biases in the data. But, importantly, one cannot generally address these difficulties by requiring that algorithms satisfy popular mathematical formalizations of fairness. By highlighting these challenges in the foundation of fair machine learning, we hope to help researchers and practitioners productively advance the area.},
  archiveprefix = {arxiv},
  keywords = {fairness},
  file = {Zotero/Corbett-Davies_Goel_2018_The Measure and Mismeasure of Fairness.pdf}
}

@incollection{Cramer1991,
  title = {Chapter 21. {{The}} Two-Dimensional Case},
  booktitle = {Mathematical Methods of Statistics},
  author = {Cram{\'e}r, Harald},
  year = {1991},
  series = {Princeton Mathematical Series},
  number = {9},
  pages = {282},
  publisher = {{Princeton university press}},
  address = {{Princeton}},
  isbn = {978-0-691-08004-8},
  langid = {english},
  lccn = {519.5}
}

@article{Curto2022,
  title = {Are {{AI}} Systems Biased against the Poor? {{A}} Machine Learning Analysis Using {{Word2Vec}} and {{GloVe}} Embeddings},
  shorttitle = {Are {{AI}} Systems Biased against the Poor?},
  author = {Curto, Georgina and Jojoa Acosta, Mario Fernando and Comim, Flavio and {Garcia-Zapirain}, Bego{\~n}a},
  year = {2022},
  month = jun,
  journal = {AI \& SOCIETY},
  issn = {0951-5666, 1435-5655},
  doi = {10.1007/s00146-022-01494-z},
  urldate = {2022-08-10},
  abstract = {Among the myriad of technical approaches and abstract guidelines proposed to the topic of AI bias, there has been an urgent call to translate the principle of fairness into the operational AI reality with the involvement of social sciences specialists to analyse the context of specific types of bias, since there is not a generalizable solution. This article offers an interdisciplinary contribution to the topic of AI and societal bias, in particular against the poor, providing a conceptual framework of the issue and a tailor-made model from which meaningful data are obtained using Natural Language Processing word vectors in pretrained Google Word2Vec, Twitter and Wikipedia GloVe word embeddings. The results of the study offer the first set of data that evidences the existence of bias against the poor and suggest that Google Word2vec shows a higher degree of bias when the terms are related to beliefs, whereas bias is higher in Twitter GloVe when the terms express behaviour. This article contributes to the body of work on bias, both from and AI and a social sciences perspective, by providing evidence of a transversal aggravating factor for historical types of discrimination. The evidence of bias against the poor also has important consequences in terms of human development, since it often leads to discrimination, which constitutes an obstacle for the effectiveness of poverty reduction policies.},
  langid = {english},
  keywords = {bias_against_the_poor,bias_detection,fairness},
  file = {Zotero/Curto et al_2022_Are AI systems biased against the poor.pdf}
}

@inproceedings{Danks2017,
  title = {Algorithmic {{Bias}} in {{Autonomous Systems}}},
  booktitle = {Proceedings of the {{Twenty-Sixth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Danks, David and London, Alex John},
  year = {2017},
  month = aug,
  pages = {4691--4697},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Melbourne, Australia}},
  doi = {10.24963/ijcai.2017/654},
  urldate = {2021-08-24},
  abstract = {Algorithms play a key role in the functioning of autonomous systems, and so concerns have periodically been raised about the possibility of algorithmic bias. However, debates in this area have been hampered by different meanings and uses of the term, "bias." It is sometimes used as a purely descriptive term, sometimes as a pejorative term, and such variations can promote confusion and hamper discussions about when and how to respond to algorithmic bias. In this paper, we first provide a taxonomy of different types and sources of algorithmic bias, with a focus on their different impacts on the proper functioning of autonomous systems. We then use this taxonomy to distinguish between algorithmic biases that are neutral or unobjectionable, and those that are problematic in some way and require a response. In some cases, there are technological or algorithmic adjustments that developers can use to compensate for problematic bias. In other cases, however, responses require adjustments by the agent, whether human or autonomous system, who uses the results of the algorithm. There is no "one size fits all" solution to algorithmic bias.},
  isbn = {978-0-9992411-0-3},
  langid = {english},
  keywords = {fairness},
  file = {Zotero/Danks_London_2017_Algorithmic Bias in Autonomous Systems.pdf}
}

@article{Dantas2022,
  title = {Recognition of {{Emotions}} for {{People}} with {{Autism}}: {{An Approach}} to {{Improve Skills}}},
  shorttitle = {Recognition of {{Emotions}} for {{People}} with {{Autism}}},
  author = {Dantas, Adilmar Coelho and {do Nascimento}, Marcelo Zanchetta},
  editor = {Rusu, Cristian A.},
  year = {2022},
  month = jan,
  journal = {International Journal of Computer Games Technology},
  volume = {2022},
  pages = {1--21},
  issn = {1687-7055, 1687-7047},
  doi = {10.1155/2022/6738068},
  urldate = {2022-03-10},
  abstract = {Autism spectrum disorder refers to a neurodevelopmental disorders characterized by repetitive behavior patterns, impaired social interaction, and impaired verbal and nonverbal communication. The ability to recognize mental states from facial expressions plays an important role in both social interaction and interpersonal communication. Thus, in recent years, several proposals have been presented, aiming to contribute to the improvement of emotional skills in order to improve social interaction. In this paper, a game is presented to support the development of emotional skills in people with autism spectrum disorder. The software used helps to develop the ability to recognize and express six basic emotions: joy, sadness, anger, disgust, surprise, and fear. Based on the theory of facial action coding systems and digital image processing techniques, it is possible to detect facial expressions and classify them into one of the six basic emotions. Experiments were performed using four public domain image databases (CK+, FER2013, RAF-DB, and MMI) and a group of children with autism spectrum disorder for evaluating the existing emotional skills. The results showed that the proposed software contributed to improvement of the skills of detection and recognition of the basic emotions in individuals with autism spectrum disorder.},
  langid = {english},
  keywords = {fer_applications},
  file = {Zotero/Dantas_do Nascimento_2022_Recognition of Emotions for People with Autism.pdf}
}

@article{Das2021,
  title = {Fairness {{Measures}} for {{Machine Learning}} in {{Finance}}},
  author = {Das, Sanjiv and Donini, Michele and Gelman, Jason and Haas, Kevin and Hardt, Mila and Katzman, Jared and Kenthapadi, Krishnaram and Larroy, Pedro and Yilmaz, Pinar and Zafar, Muhammad Bilal},
  year = {2021},
  month = oct,
  journal = {The Journal of Financial Data Science},
  volume = {3},
  number = {4},
  pages = {33--64},
  issn = {2640-3943},
  doi = {10.3905/jfds.2021.1.075},
  urldate = {2022-08-11},
  abstract = {We present a machine learning pipeline for fairness-aware machine learning (FAML) in finance that encompasses metrics for fairness (and accuracy). Whereas accuracy metrics are well understood and the principal ones used frequently, there is no consensus as to which of several available measures for fairness should be used in a generic manner in the financial services industry. We explore these measures and discuss which ones to focus on, at various stages in the ML pipeline, pre-training and post-training, and we also examine simple bias mitigation approaches. Using a standard dataset we show that the sequencing in our FAML pipeline offers a cogent approach to arriving at a fair and accurate ML model. We discuss the intersection of bias metrics with legal considerations in the US, and the entanglement of explainability and fairness is exemplified in the case study. We discuss possible approaches for training ML models while satisfying constraints imposed from various fairness metrics, and the role of causality in assessing fairness.},
  langid = {english},
  keywords = {check,fairness_evaluation},
  file = {Zotero/Das et al_2021_Fairness Measures for Machine Learning in Finance.pdf}
}

@inproceedings{Deng2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and {Kai Li} and {Li Fei-Fei}},
  year = {2009},
  month = jun,
  pages = {248--255},
  publisher = {{IEEE}},
  address = {{Miami, FL}},
  doi = {10.1109/CVPR.2009.5206848},
  urldate = {2023-03-27},
  isbn = {978-1-4244-3992-8}
}

@inproceedings{Deng2009old,
  title = {Construction and Analysis of a Large Scale Image Ontology},
  author = {Deng, J. and Li, K. and Do, M. and Su, H. and {Fei-Fei}, L.},
  year = {2009},
  bibsource = {http://www.image-net.org/papers/ImageNet\textsubscript{V}SS2009.bib},
  organization = {{Vision Sciences Society}},
  keywords = {datasets}
}

@article{Denton2021,
  title = {On the Genealogy of Machine Learning Datasets: {{A}} Critical History of {{ImageNet}}},
  shorttitle = {On the Genealogy of Machine Learning Datasets},
  author = {Denton, Emily and Hanna, Alex and Amironesei, Razvan and Smart, Andrew and Nicole, Hilary},
  year = {2021},
  month = jul,
  journal = {Big Data \& Society},
  volume = {8},
  number = {2},
  pages = {205395172110359},
  issn = {2053-9517, 2053-9517},
  doi = {10.1177/20539517211035955},
  urldate = {2021-10-04},
  abstract = {In response to growing concerns of bias, discrimination, and unfairness perpetuated by algorithmic systems, the datasets used to train and evaluate machine learning models have come under increased scrutiny. Many of these examinations have focused on the contents of machine learning datasets, finding glaring underrepresentation of minoritized groups. In contrast, relatively little work has been done to examine the norms, values, and assumptions embedded in these datasets. In this work, we conceptualize machine learning datasets as a type of informational infrastructure, and motivate a genealogy as method in examining the histories and modes of constitution at play in their creation. We present a critical history of ImageNet as an exemplar, utilizing critical discourse analysis of major texts around ImageNet's creation and impact. We find that assumptions around ImageNet and other large computer vision datasets more generally rely on three themes: the aggregation and accumulation of more data, the computational construction of meaning, and making certain types of data labor invisible. By tracing the discourses that surround this influential benchmark, we contribute to the ongoing development of the standards and norms around data development in machine learning and artificial intelligence research.},
  langid = {english},
  keywords = {datasets,review},
  file = {Zotero/Denton et al_2021_On the genealogy of machine learning datasets.pdf}
}

@article{Deuschel2021,
  title = {Uncovering the {{Bias}} in {{Facial Expressions}}},
  author = {Deuschel, Jessica and Finzel, Bettina and Rieger, Ines},
  year = {2021},
  month = nov,
  journal = {arXiv:2011.11311 [cs]},
  eprint = {2011.11311},
  primaryclass = {cs},
  doi = {10.20378/irb-50304},
  urldate = {2022-04-06},
  abstract = {Over the past decades the machine and deep learning community has celebrated great achievements in challenging tasks such as image classification. The deep architecture of artificial neural networks together with the plenitude of available data makes it possible to describe highly complex relations. Yet, it is still impossible to fully capture what the deep learning model has learned and to verify that it operates fairly and without creating bias, especially in critical tasks, for instance those arising in the medical field. One example for such a task is the detection of distinct facial expressions, called Action Units, in facial images. Considering this specific task, our research aims to provide transparency regarding bias, specifically in relation to gender and skin color. We train a neural network for Action Unit classification and analyze its performance quantitatively based on its accuracy and qualitatively based on heatmaps. A structured review of our results indicates that we are able to detect bias. Even though we cannot conclude from our results that lower classification performance emerged solely from gender and skin color bias, these biases must be addressed, which is why we end by giving suggestions on how the detected bias can be avoided.},
  archiveprefix = {arxiv},
  keywords = {bias_example,review},
  file = {Zotero/Deuschel et al_2021_Uncovering the Bias in Facial Expressions.pdf}
}

@inproceedings{Dhall2011,
  title = {Static Facial Expression Analysis in Tough Conditions: {{Data}}, Evaluation Protocol and Benchmark},
  shorttitle = {Static Facial Expression Analysis in Tough Conditions},
  booktitle = {2011 {{IEEE International Conference}} on {{Computer Vision Workshops}} ({{ICCV Workshops}})},
  author = {Dhall, Abhinav and Goecke, Roland and Lucey, Simon and Gedeon, Tom},
  year = {2011},
  month = nov,
  pages = {2106--2112},
  doi = {10.1109/ICCVW.2011.6130508},
  abstract = {Quality data recorded in varied realistic environments is vital for effective human face related research. Currently available datasets for human facial expression analysis have been generated in highly controlled lab environments. We present a new static facial expression database Static Facial Expressions in the Wild (SFEW) extracted from a temporal facial expressions database Acted Facial Expressions in the Wild (AFEW) [9], which we have extracted from movies. In the past, many robust methods have been reported in the literature. However, these methods have been experimented on different databases or using different protocols within the same databases. The lack of a standard protocol makes it difficult to compare systems and acts as a hindrance in the progress of the field. Therefore, we propose a person independent training and testing protocol for expression recognition as part of the BEFIT workshop. Further, we compare our dataset with the JAFFE and Multi-PIE datasets and provide baseline results.},
  keywords = {datasets},
  file = {Zotero/Dhall et al_2011_Static facial expression analysis in tough conditions.pdf}
}

@article{Dhall2012,
  title = {Collecting {{Large}}, {{Richly Annotated Facial-Expression Databases}} from {{Movies}}},
  author = {Dhall, Abhinav and Goecke, Roland and Lucey, Simon and Gedeon, Tom},
  year = {2012},
  month = jul,
  journal = {IEEE MultiMedia},
  volume = {19},
  number = {3},
  pages = {34--41},
  issn = {1070-986X},
  doi = {10.1109/MMUL.2012.26},
  urldate = {2022-05-31},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Dhall et al_2012_Collecting Large, Richly Annotated Facial-Expression Databases from Movies.pdf}
}

@inproceedings{Dhamala2021,
  title = {{{BOLD}}: {{Dataset}} and {{Metrics}} for {{Measuring Biases}} in {{Open-Ended Language Generation}}},
  shorttitle = {{{BOLD}}},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Dhamala, Jwala and Sun, Tony and Kumar, Varun and Krishna, Satyapriya and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul},
  year = {2021},
  month = mar,
  eprint = {2101.11718},
  primaryclass = {cs},
  pages = {862--872},
  doi = {10.1145/3442188.3445924},
  urldate = {2022-08-11},
  abstract = {Recent advances in deep learning techniques have enabled machines to generate cohesive open-ended text when prompted with a sequence of words as context. While these models now empower many downstream applications from conversation bots to automatic storytelling, they have been shown to generate texts that exhibit social biases. To systematically study and benchmark social biases in open-ended language generation, we introduce the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains: profession, gender, race, religion, and political ideology. We also propose new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles. An examination of text generated from three popular language models reveals that the majority of these models exhibit a larger social bias than human-written Wikipedia text across all domains. With these results we highlight the need to benchmark biases in open-ended language generation and caution users of language generation models on downstream tasks to be cognizant of these embedded prejudices.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {check,fairness_evaluation},
  file = {Zotero/Dhamala et al_2021_BOLD.pdf}
}

@inproceedings{Dominguez-Catena2022,
  title = {Assessing {{Demographic Bias Transfer}} from {{Dataset}} to {{Model}}: {{A Case Study}} in {{Facial Expression Recognition}}},
  booktitle = {Proceedings of the {{Workshop}} on {{Artificial Intelligence Safety}} 2022 ({{AISafety}} 2022)},
  author = {{Dominguez-Catena}, Iris and Paternain, Daniel and Galar, Mikel},
  year = {2022},
  month = jul,
  address = {{Vienna, Austria}},
  abstract = {The increasing amount of applications of Artificial Intelligence (AI) has led researchers to study the social impact of these technologies and evaluate their fairness. Unfortunately, current fairness metrics are hard to apply in multi-class multidemographic classification problems, such as Facial Expression Recognition (FER). We propose a new set of metrics to approach these problems. Of the three metrics proposed, two focus on the representational and stereotypical bias of the dataset, and the third one on the residual bias of the trained model. These metrics combined can potentially be used to study and compare diverse bias mitigation methods. We demonstrate the usefulness of the metrics by applying them to a FER problem based on the popular Affectnet dataset. Like many other datasets for FER, Affectnet is a large Internet-sourced dataset with 291,651 labeled images. Obtaining images from the Internet raises some concerns over the fairness of any system trained on this data and its ability to generalize properly to diverse populations. We first analyze the dataset and some variants, finding substantial racial bias and gender stereotypes. We then extract several subsets with different demographic properties and train a model on each one, observing the amount of residual bias in the different setups. We also provide a second analysis on a different dataset, FER+.},
  langid = {english},
  keywords = {own},
  file = {Zotero/Dominguez-Catena et al_2022_Assessing Demographic Bias Transfer from Dataset to Model2.pdf}
}

@misc{Dominguez-Catena2022arxiv,
  title = {Assessing {{Demographic Bias Transfer}} from {{Dataset}} to {{Model}}: {{A Case Study}} in {{Facial Expression Recognition}}},
  shorttitle = {Assessing {{Demographic Bias Transfer}} from {{Dataset}} to {{Model}}},
  author = {{Dominguez-Catena}, Iris and Paternain, Daniel and Galar, Mikel},
  year = {2022},
  month = may,
  number = {arXiv:2205.10049},
  eprint = {2205.10049},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2205.10049},
  urldate = {2022-05-23},
  abstract = {The increasing amount of applications of Artificial Intelligence (AI) has led researchers to study the social impact of these technologies and evaluate their fairness. Unfortunately, current fairness metrics are hard to apply in multi-class multi-demographic classification problems, such as Facial Expression Recognition (FER). We propose a new set of metrics to approach these problems. Of the three metrics proposed, two focus on the representational and stereotypical bias of the dataset, and the third one on the residual bias of the trained model. These metrics combined can potentially be used to study and compare diverse bias mitigation methods. We demonstrate the usefulness of the metrics by applying them to a FER problem based on the popular Affectnet dataset. Like many other datasets for FER, Affectnet is a large Internet-sourced dataset with 291,651 labeled images. Obtaining images from the Internet raises some concerns over the fairness of any system trained on this data and its ability to generalize properly to diverse populations. We first analyze the dataset and some variants, finding substantial racial bias and gender stereotypes. We then extract several subsets with different demographic properties and train a model on each one, observing the amount of residual bias in the different setups. We also provide a second analysis on a different dataset, FER+.},
  archiveprefix = {arxiv},
  keywords = {fairness_evaluation,own},
  file = {Zotero/Dominguez-Catena et al_2022_Assessing Demographic Bias Transfer from Dataset to Model.pdf;../../../../../snap/zotero-snap/common/Zotero/storage/AJHYLX4Q/2205.html}
}

@incollection{Dominguez-Catena2023,
  title = {Gender {{Stereotyping Impact}} in {{Facial Expression Recognition}}},
  booktitle = {Machine {{Learning}} and {{Principles}} and {{Practice}} of {{Knowledge Discovery}} in {{Databases}}},
  author = {{Dominguez-Catena}, Iris and Paternain, Daniel and Galar, Mikel},
  year = {2023},
  volume = {1752},
  pages = {9--22},
  publisher = {{Springer Nature Switzerland}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-23618-1_1},
  urldate = {2023-02-06},
  isbn = {978-3-031-23617-4 978-3-031-23618-1},
  langid = {english},
  keywords = {own},
  file = {Zotero/Dominguez-Catena et al_2023_Gender Stereotyping Impact in Facial Expression Recognition.pdf}
}

@article{Domnich2021,
  title = {Responsible {{AI}}: {{Gender}} Bias Assessment in Emotion Recognition},
  shorttitle = {Responsible {{AI}}},
  author = {Domnich, Artem and Anbarjafari, Gholamreza},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.11436 [cs]},
  eprint = {2103.11436},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2103.11436},
  urldate = {2022-04-06},
  abstract = {Rapid development of artificial intelligence (AI) systems amplify many concerns in society. These AI algorithms inherit different biases from humans due to mysterious operational flow and because of that it is becoming adverse in usage. As a result, researchers have started to address the issue by investigating deeper in the direction towards Responsible and Explainable AI. Among variety of applications of AI, facial expression recognition might not be the most important one, yet is considered as a valuable part of human-AI interaction. Evolution of facial expression recognition from the feature based methods to deep learning drastically improve quality of such algorithms. This research work aims to study a gender bias in deep learning methods for facial expression recognition by investigating six distinct neural networks, training them, and further analysed on the presence of bias, according to the three definition of fairness. The main outcomes show which models are gender biased, which are not and how gender of subject affects its emotion recognition. More biased neural networks show bigger accuracy gap in emotion recognition between male and female test sets. Furthermore, this trend keeps for true positive and false positive rates. In addition, due to the nature of the research, we can observe which types of emotions are better classified for men and which for women. Since the topic of biases in facial expression recognition is not well studied, a spectrum of continuation of this research is truly extensive, and may comprise detail analysis of state-of-the-art methods, as well as targeting other biases.},
  archiveprefix = {arxiv},
  keywords = {bias_example,gender},
  file = {Zotero/Domnich_Anbarjafari_2021_Responsible AI.pdf}
}

@article{Dooley2021,
  title = {Comparing {{Human}} and {{Machine Bias}} in {{Face Recognition}}},
  author = {Dooley, Samuel and Downing, Ryan and Wei, George and Shankar, Nathan and Thymes, Bradon and Thorkelsdottir, Gudrun and {Kurtz-Miott}, Tiye and Mattson, Rachel and Obiwumi, Olufemi and Cherepanova, Valeriia and Goldblum, Micah and Dickerson, John P. and Goldstein, Tom},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.08396 [cs]},
  eprint = {2110.08396},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2110.08396},
  urldate = {2021-10-26},
  abstract = {Much recent research has uncovered and discussed serious concerns of bias in facial analysis technologies, finding performance disparities between groups of people based on perceived gender, skin type, lighting condition, etc. These audits are immensely important and successful at measuring algorithmic bias but have two major challenges: the audits (1) use facial recognition datasets which lack quality metadata, like LFW and CelebA, and (2) do not compare their observed algorithmic bias to the biases of their human alternatives. In this paper, we release improvements to the LFW and CelebA datasets which will enable future researchers to obtain measurements of algorithmic bias that are not tainted by major flaws in the dataset (e.g. identical images appearing in both the gallery and test set). We also use these new data to develop a series of challenging facial identification and verification questions that we administered to various algorithms and a large, balanced sample of human reviewers. We find that both computer models and human survey participants perform significantly better at the verification task, generally obtain lower accuracy rates on dark-skinned or female subjects for both tasks, and obtain higher accuracy rates when their demographics match that of the question. Computer models are observed to achieve a higher level of accuracy than the survey participants on both tasks and exhibit bias to similar degrees as the human survey participants.},
  archiveprefix = {arxiv},
  keywords = {bias_example,psychology},
  file = {Zotero/Dooley et al_2021_Comparing Human and Machine Bias in Face Recognition.pdf}
}

@article{Douglas-Cowie2003,
  title = {Emotional Speech: {{Towards}} a New Generation of Databases},
  shorttitle = {Emotional Speech},
  author = {{Douglas-Cowie}, Ellen and Campbell, Nick and Cowie, Roddy and Roach, Peter},
  year = {2003},
  month = apr,
  journal = {Speech Communication},
  volume = {40},
  number = {1},
  pages = {33--60},
  issn = {0167-6393},
  doi = {10.1016/S0167-6393(02)00070-5},
  urldate = {2022-04-26},
  abstract = {Research on speech and emotion is moving from a period of exploratory research into one where there is a prospect of substantial applications, notably in human\textendash computer interaction. Progress in the area relies heavily on the development of appropriate databases. This paper addresses four main issues that need to be considered in developing databases of emotional speech: scope, naturalness, context and descriptors. The state of the art is reviewed. A good deal has been done to address the key issues, but there is still a long way to go. The paper shows how the challenge of developing appropriate databases is being addressed in three major recent projects\textendash\textendash the Reading\textendash Leeds project, the Belfast project and the CREST\textendash ESP project. From these and other studies the paper draws together the tools and methods that have been developed, addresses the problems that arise and indicates the future directions for the development of emotional speech databases. R\'esum\'e L'\'etude de la parole et de l'\'emotion, partie du stade de la recherche exploratrice, en arrive maintenant au stade qui est celui d'applications importantes, notamment dans l'interaction homme\textendash machine. Le progr\`es en ce domaine d\'epend \'etroitment du d\'eveloppement de bases de donn\'ees appropri\'ees. Cet article aborde quatre points principaux qui m\'eritent notre attention \`a ce sujet: l'\'etendue, l'authenticit\'e, le contexte et les termes de description. Il pr\'esente un compte-rendu de la situation actuelle dans ce domaine et \'evoque les avanc\'ees faites, et celles qui restent \`a faire. L'article montre comment trois r\'ecents projets importants (celui de Reading\textendash Leeds, celui de Belfast, et celui de CREST\textendash ESP) ont relev\'e le d\'efi pos\'e par la construction de bases de donn\'ees appropri\'ees. A partir de ces trois projets, ainsi que d'autres travaux, les auteurs pr\'esentment un bilan des outils et m\'ethodes utilis\'es, identifient les probl\`emes qui y sont associ\'es, et indiquent la direction dans laquelle devraient s'orienter les recherches \`a venir.},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Douglas-Cowie et al_2003_Emotional speech.pdf;../../../../../snap/zotero-snap/common/Zotero/storage/X7VBJXI5/S0167639302000705.html}
}

@article{Dragan2014,
  title = {Introduction to {{Structural Equation Modeling}}: {{Review}}, {{Methodology}} and {{Practical Applications}}},
  author = {Dragan, Dejan and Topol{\v s}ek, Darja},
  year = {2014},
  pages = {28},
  abstract = {The paper addresses an introduction to the structural equation modeling (SEM), the insight into the methodology, and the importance of this statistical technique for practical applications. SEM is a very powerful statistical modeling tool, which incorporates the measurements models and the path models into the comprehensive covariance structure analysis framework. Herein, the exploratory analysis (EFA) and the confirmatory factor analysis (CFA) are usually employed as the intermediate stages of the modeling design. The main intent of this work is to inform the interesting readers about all the potentials, which can be conducted when use this technique. For encouraging the interested researchers, who are new in this field, the main steps and characteristics of SEM methodology are briefly presented. A short overview of applications based on this advanced statistical methodology is also provided, with emphasis on supply chain (SC) management applications, which study the impact of integration between individual players on the SC performance. An investigaton of Port Economics applications based on SEM is also inspected in this work.},
  langid = {english},
  keywords = {sem},
  file = {Zotero/Dragan_Topolšek_2014_Introduction to Structural Equation Modeling.pdf}
}

@article{Drozdowski2020,
  title = {Demographic {{Bias}} in {{Biometrics}}: {{A Survey}} on an {{Emerging Challenge}}},
  shorttitle = {Demographic {{Bias}} in {{Biometrics}}},
  author = {Drozdowski, Pawel and Rathgeb, Christian and Dantcheva, Antitza and Damer, Naser and Busch, Christoph},
  year = {2020},
  month = jun,
  journal = {IEEE Transactions on Technology and Society},
  volume = {1},
  number = {2},
  pages = {89--103},
  issn = {2637-6415},
  doi = {10.1109/TTS.2020.2992344},
  urldate = {2023-02-21},
  abstract = {Systems incorporating biometric technologies have become ubiquitous in personal, commercial, and governmental identity management applications. Both cooperative (e.g., access control) and noncooperative (e.g., surveillance and forensics) systems have benefited from biometrics. Such systems rely on the uniqueness of certain biological or behavioral characteristics of human beings, which enable for individuals to be reliably recognized using automated algorithms. Recently, however, there has been a wave of public and academic concerns regarding the existence of systemic bias in automated decision systems (including biometrics). Most prominently, face recognition algorithms have often been labeled as ``racist'' or ``biased'' by the media, nongovernmental organizations, and researchers alike. The main contributions of this article are: 1) an overview of the topic of algorithmic bias in the context of biometrics; 2) a comprehensive survey of the existing literature on biometric bias estimation and mitigation; 3) a discussion of the pertinent technical and social matters; and 4) an outline of the remaining challenges and future work items, both from technological and social points of view.},
  langid = {english},
  keywords = {bias_detection,fairness,review},
  file = {../../../../../snap/zotero-snap/common/Zotero/storage/B5MZ3FVQ/Drozdowski et al. - 2020 - Demographic Bias in Biometrics A Survey on an Eme.pdf}
}

@article{Ducher1994,
  title = {Statistical Relationships between Systolic Blood Pressure and Heart Rate and Their Functional Significance in Conscious Rats},
  author = {Ducher, M. and Cerutti, C. and Gustin, M. P. and Paultre, C. Z.},
  year = {1994},
  month = nov,
  journal = {Medical \& Biological Engineering \& Computing},
  volume = {32},
  number = {6},
  pages = {649--655},
  issn = {0140-0118, 1741-0444},
  doi = {10.1007/BF02524241},
  urldate = {2023-01-19},
  langid = {english}
}

@article{Dulhanty2019,
  title = {Auditing {{ImageNet}}: {{Towards}} a {{Model-driven Framework}} for {{Annotating Demographic Attributes}} of {{Large-Scale Image Datasets}}},
  shorttitle = {Auditing {{ImageNet}}},
  author = {Dulhanty, Chris and Wong, Alexander},
  year = {2019},
  month = jun,
  journal = {arXiv:1905.01347 [cs]},
  eprint = {1905.01347},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1905.01347},
  urldate = {2021-09-30},
  abstract = {The ImageNet dataset ushered in a flood of academic and industry interest in deep learning for computer vision applications. Despite its significant impact, there has not been a comprehensive investigation into the demographic attributes of images contained within the dataset. Such a study could lead to new insights on inherent biases within ImageNet, particularly important given it is frequently used to pretrain models for a wide variety of computer vision tasks. In this work, we introduce a model-driven framework for the automatic annotation of apparent age and gender attributes in large-scale image datasets. Using this framework, we conduct the first demographic audit of the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC) subset of ImageNet and the "person" hierarchical category of ImageNet. We find that 41.62\% of faces in ILSVRC appear as female, 1.71\% appear as individuals above the age of 60, and males aged 15 to 29 account for the largest subgroup with 27.11\%. We note that the presented model-driven framework is not fair for all intersectional groups, so annotation are subject to bias. We present this work as the starting point for future development of unbiased annotation models and for the study of downstream effects of imbalances in the demographics of ImageNet. Code and annotations are available at: http://bit.ly/ImageNetDemoAudit},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {bias_example,datasets,fairness,fairness_evaluation,review},
  file = {Zotero/Dulhanty_Wong_2019_Auditing ImageNet.pdf}
}

@article{Duran2021,
  title = {Do Emotions Result in Their Predicted Facial Expressions? {{A}} Meta-Analysis of Studies on the Co-Occurrence of Expression and Emotion},
  shorttitle = {Do Emotions Result in Their Predicted Facial Expressions?},
  author = {Dur{\'a}n, Juan I. and {Fern{\'a}ndez-Dols}, Jos{\'e}-Miguel},
  year = {2021},
  journal = {Emotion},
  volume = {21},
  number = {7},
  pages = {1550--1569},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1931-1516},
  doi = {10.1037/emo0001015},
  abstract = {That basic emotions produce a facial signal would\textemdash if true\textemdash provide a foundation for a science of emotion. Here, random-effects meta-analyses tested whether happiness, sadness, anger, disgust, fear, and surprise each co-occurs with its predicted facial signal. The first meta-analysis examined only those studies that measured full expressions through Facial Actions Coding System (FACS). Average co-occurrence effect size was .13. The second meta-analysis included both full and partial expressions, as measured by FACS or another system. Average co-occurrence effect size rose to .23. A third meta-analysis estimated the Pearson correlation between intensity of the reported emotion and intensity of the predicted facial expression. Average correlation was .30. Overall, co-occurrence and correlation were greatest for disgust, least for surprise. What are commonly known as the six classic basic emotions do not reliably co-occur with their predicted facial signal. Heterogeneity between samples was found, suggesting a more complex account of facial expressions. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  keywords = {emotions},
  file = {../../../../../snap/zotero-snap/common/Zotero/storage/NPZFG534/2022-03375-001.html}
}

@inproceedings{Dwork2012,
  title = {Fairness through Awareness},
  booktitle = {Proceedings of the 3rd {{Innovations}} in {{Theoretical Computer Science Conference}}},
  author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
  year = {2012},
  month = jan,
  series = {{{ITCS}} '12},
  pages = {214--226},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2090236.2090255},
  urldate = {2022-05-11},
  abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
  isbn = {978-1-4503-1115-1},
  keywords = {fairness_evaluation},
  file = {Zotero/Dwork et al_2012_Fairness through awareness.pdf}
}

@article{Ebner2010,
  title = {{{FACES}}\textemdash{{A}} Database of Facial Expressions in Young, Middle-Aged, and Older Women and Men: {{Development}} and Validation},
  shorttitle = {{{FACES}}\textemdash{{A}} Database of Facial Expressions in Young, Middle-Aged, and Older Women and Men},
  author = {Ebner, Natalie C. and Riediger, Michaela and Lindenberger, Ulman},
  year = {2010},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {42},
  number = {1},
  pages = {351--362},
  issn = {1554-3528},
  doi = {10.3758/BRM.42.1.351},
  urldate = {2022-04-06},
  abstract = {Faces are widely used as stimuli in various research fields. Interest in emotion-related differences and age-associated changes in the processing of faces is growing. With the aim of systematically varying both expression and age of the face, we created FACES, a database comprising N=171 naturalistic faces of young, middle-aged, and older women and men. Each face is represented with two sets of six facial expressions (neutrality, sadness, disgust, fear, anger, and happiness), resulting in 2,052 individual images. A total of N=154 young, middleaged, and older women and men rated the faces in terms of facial expression and perceived age. With its large age range of faces displaying different expressions, FACES is well suited for investigating developmental and other research questions on emotion, motivation, and cognition, as well as their interactions. Information on using FACES for research purposes can be found at http://faces.mpib-berlin.mpg.de.},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Ebner et al_2010_FACES—A database of facial expressions in young, middle-aged, and older women.pdf}
}

@article{Ekman1971,
  title = {Constants across Cultures in the Face and Emotion.},
  author = {Ekman, Paul and Friesen, Wallace V.},
  year = {1971},
  journal = {Journal of Personality and Social Psychology},
  volume = {17},
  number = {2},
  pages = {124--129},
  issn = {1939-1315, 0022-3514},
  doi = {10.1037/h0030377},
  urldate = {2022-01-31},
  langid = {english},
  file = {Zotero/Ekman_Friesen_1971_Constants across cultures in the face and emotion.pdf}
}

@book{Ekman1976,
  title = {Pictures of {{Facial Affect}}},
  author = {Ekman, Paul},
  year = {1976}
}

@article{Ekman1978,
  title = {Facial Action Coding System: A Technique for the Measurement of Facial Movement},
  shorttitle = {Facial Action Coding System},
  author = {Ekman, P. and Friesen, W.},
  year = {1978},
  langid = {english},
  keywords = {emotions}
}

@article{Ekman2016,
  title = {What {{Scientists Who Study Emotion Agree About}}},
  author = {Ekman, Paul},
  year = {2016},
  month = jan,
  journal = {Perspectives on Psychological Science},
  volume = {11},
  number = {1},
  pages = {31--34},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691615596992},
  urldate = {2021-09-13},
  abstract = {In recent years, the field of emotion has grown enormously\textemdash recently, nearly 250 scientists were identified who are studying emotion. In this article, I report a survey of the field, which revealed high agreement about the evidence regarding the nature of emotion, supporting some of both Darwin's and Wundt's 19th century proposals. Topics where disagreements remain were also exposed.},
  langid = {english},
  keywords = {emotions}
}

@article{Elfenbein2002,
  title = {On the Universality and Cultural Specificity of Emotion Recognition: {{A}} Meta-Analysis},
  shorttitle = {On the Universality and Cultural Specificity of Emotion Recognition},
  author = {Elfenbein, Hillary Anger and Ambady, Nalini},
  year = {2002},
  journal = {Psychological Bulletin},
  volume = {128},
  number = {2},
  pages = {203--235},
  issn = {1939-1455, 0033-2909},
  doi = {10.1037/0033-2909.128.2.203},
  urldate = {2021-09-13},
  langid = {english},
  keywords = {bias_example,psychology}
}

@misc{Eniser2019,
  title = {{{DeepFault}}: {{Fault Localization}} for {{Deep Neural Networks}}},
  shorttitle = {{{DeepFault}}},
  author = {Eniser, Hasan Ferit and Gerasimou, Simos and Sen, Alper},
  year = {2019},
  month = feb,
  number = {arXiv:1902.05974},
  eprint = {arXiv:1902.05974},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1902.05974},
  urldate = {2022-08-04},
  abstract = {Deep Neural Networks (DNNs) are increasingly deployed in safety-critical applications including autonomous vehicles and medical diagnostics. To reduce the residual risk for unexpected DNN behaviour and provide evidence for their trustworthy operation, DNNs should be thoroughly tested. The DeepFault whitebox DNN testing approach presented in our paper addresses this challenge by employing suspiciousness measures inspired by fault localization to establish the hit spectrum of neurons and identify suspicious neurons whose weights have not been calibrated correctly and thus are considered responsible for inadequate DNN performance. DeepFault also uses a suspiciousness-guided algorithm to synthesize new inputs, from correctly classified inputs, that increase the activation values of suspicious neurons. Our empirical evaluation on several DNN instances trained on MNIST and CIFAR-10 datasets shows that DeepFault is effective in identifying suspicious neurons. Also, the inputs synthesized by DeepFault closely resemble the original inputs, exercise the identified suspicious neurons and are highly adversarial.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {explainability},
  file = {Zotero/Eniser et al_2019_DeepFault.pdf}
}

@article{Fabbrizzi2022,
  title = {A Survey on Bias in Visual Datasets},
  author = {Fabbrizzi, Simone and Papadopoulos, Symeon and Ntoutsi, Eirini and Kompatsiaris, Ioannis},
  year = {2022},
  month = oct,
  journal = {Computer Vision and Image Understanding},
  volume = {223},
  pages = {103552},
  issn = {1077-3142},
  doi = {10.1016/j.cviu.2022.103552},
  urldate = {2022-11-07},
  abstract = {Computer Vision (CV) has achieved remarkable results, outperforming humans in several tasks. Nonetheless, it may result in significant discrimination if not handled properly. Indeed, CV systems highly depend on training datasets and can learn and amplify biases that such datasets may carry. Thus, the problem of understanding and discovering bias in visual datasets is of utmost importance; yet, it has not been studied in a systematic way to date. Hence, this work aims to: (i) describe the different kinds of bias that may manifest in visual datasets; (ii) review the literature on methods for bias discovery and quantification in visual datasets; (iii) discuss existing attempts to collect visual datasets in a bias-aware manner. A key conclusion of our study is that the problem of bias discovery and quantification in visual datasets is still open, and there is room for improvement in terms of both methods and the range of biases that can be addressed. Moreover, there is no such thing as a bias-free dataset, so scientists and practitioners must become aware of the biases in their datasets and make them explicit. To this end, we propose a checklist to spot different types of bias during visual dataset collection.},
  langid = {english},
  keywords = {bias_example,fairness},
  file = {Zotero/Fabbrizzi et al_2022_A survey on bias in visual datasets.pdf;../../../../../snap/zotero-snap/common/Zotero/storage/N5L76QM2/S1077314222001308.html}
}

@article{Feldman2015,
  title = {Certifying and Removing Disparate Impact},
  author = {Feldman, Michael and Friedler, Sorelle and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  year = {2015},
  month = jul,
  journal = {arXiv:1412.3756 [cs, stat]},
  eprint = {1412.3756},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1412.3756},
  urldate = {2022-02-08},
  abstract = {What does it mean for an algorithm to be biased? In U.S. law, unintentional bias is encoded via disparate impact, which occurs when a selection process has widely different outcomes for different groups, even as it appears to be neutral. This legal determination hinges on a definition of a protected class (ethnicity, gender) and an explicit description of the process.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {fairness},
  file = {Zotero/Feldman et al_2015_Certifying and removing disparate impact.pdf}
}

@article{Fish2021,
  title = {Reflexive {{Design}} for {{Fairness}} and {{Other Human Values}} in {{Formal Models}}},
  author = {Fish, Benjamin and Stark, Luke},
  year = {2021},
  month = jul,
  journal = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
  eprint = {2010.05084},
  pages = {89--99},
  doi = {10.1145/3461702.3462518},
  urldate = {2022-04-04},
  abstract = {Algorithms and other formal models purportedly incorporating human values like fairness have grown increasingly popular in computer science. In response to sociotechnical challenges in the use of these models, designers and researchers have taken widely divergent positions on how formal models incorporating aspects of human values should be used: encouraging their use, moving away from them, or ignoring the normative consequences altogether. In this paper, we seek to resolve these divergent positions by identifying the main conceptual limits of formal modeling, and develop four reflexive values--value fidelity, appropriate accuracy, value legibility, and value contestation--vital for incorporating human values adequately into formal models. We then provide a brief methodology for reflexively designing formal models incorporating human values.},
  archiveprefix = {arxiv},
  keywords = {datasets},
  file = {Zotero/Fish_Stark_2021_Reflexive Design for Fairness and Other Human Values in Formal Models.pdf}
}

@book{Gauch1982,
  title = {Multivariate {{Analysis}} in {{Community Ecology}}},
  author = {Gauch, Hugh G.},
  year = {1982},
  month = feb,
  edition = {First},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/CBO9780511623332},
  urldate = {2023-02-22},
  abstract = {Ecologists are making increasing use of computer methods in analyzing ecological data on plant and animal communities. Ecological problems naturally involve numerous variables and numerous individuals or samples. Multivariate techniques permit the summary of large, complex sets of data and provide the means to tackle many problems that cannot be investigated experimentally because of practical restraints. Ecologists are thus enabled to group similar species and similar sample sites together, and to generate hypotheses about environmental and historical factors that affect the communities. This timely book presents a full critical description of three methodologies - direct gradient analysis, ordination, and classification - from both theoretical and practical viewpoints. Both traditional and new methods are presented. Using a wide range of illustrative examples, Hugh Gauch provides an up-to-date synthesis of this field, which will be of interest to advanced students and ecologists. These mathematical tools are also used in a wide variety of other areas, from natural resource management and agronomy to the social and political sciences.},
  isbn = {978-0-521-23820-5 978-0-521-28240-6 978-0-511-62333-2},
  keywords = {metrics}
}

@misc{Gerasimou2020,
  title = {Importance-{{Driven Deep Learning System Testing}}},
  author = {Gerasimou, Simos and Eniser, Hasan Ferit and Sen, Alper and Cakan, Alper},
  year = {2020},
  month = feb,
  number = {arXiv:2002.03433},
  eprint = {arXiv:2002.03433},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2002.03433},
  urldate = {2022-08-04},
  abstract = {Deep Learning (DL) systems are key enablers for engineering intelligent applications due to their ability to solve complex tasks such as image recognition and machine translation. Nevertheless, using DL systems in safety- and security-critical applications requires to provide testing evidence for their dependable operation. Recent research in this direction focuses on adapting testing criteria from traditional software engineering as a means of increasing confidence for their correct behaviour. However, they are inadequate in capturing the intrinsic properties exhibited by these systems. We bridge this gap by introducing DeepImportance, a systematic testing methodology accompanied by an Importance-Driven (IDC) test adequacy criterion for DL systems. Applying IDC enables to establish a layer-wise functional understanding of the importance of DL system components and use this information to assess the semantic diversity of a test set. Our empirical evaluation on several DL systems, across multiple DL datasets and with state-of-the-art adversarial generation techniques demonstrates the usefulness and effectiveness of DeepImportance and its ability to support the engineering of more robust DL systems.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {explainability},
  file = {Zotero/Gerasimou et al_2020_Importance-Driven Deep Learning System Testing.pdf}
}

@incollection{Gonzalez-Sanchez2017,
  title = {Chapter 11 - {{Affect Measurement}}: {{A Roadmap Through Approaches}}, {{Technologies}}, and {{Data Analysis}}},
  shorttitle = {Chapter 11 - {{Affect Measurement}}},
  booktitle = {Emotions and {{Affect}} in {{Human Factors}} and {{Human-Computer Interaction}}},
  author = {{Gonzalez-Sanchez}, Javier and Baydogan, Mustafa and {Chavez-Echeagaray}, Maria Elena and Atkinson, Robert K. and Burleson, Winslow},
  editor = {Jeon, Myounghoon},
  year = {2017},
  month = jan,
  pages = {255--288},
  publisher = {{Academic Press}},
  address = {{San Diego}},
  doi = {10.1016/B978-0-12-801851-4.00011-2},
  urldate = {2022-01-31},
  abstract = {Affect signals what humans care about and what matters to them. By providing computers with the capability to measure affect, researchers aspire to narrow the communication gap between the emotional human and the emotionally detached computer, with the ultimate aim of enhancing human\textendash computer interactions. This chapter explores the multidisciplinary foundations of affective state measurement as a multimodal process. Specifically, it: (1) describes popular sensing technologies, including brain\textendash computer interfaces, face-based emotion recognition systems, eye-tracking systems, physiological sensors, body language recognition, and text-based language processing; (2) explores the data gathered from each technology and its key characteristics; (3) outlines the pros and cons of each technology; (4) examines sampling, filtering, and multimodal affective data integration methodologies; and (5) presents the tools and algorithms used to analyze affective data off-line, seeking to make inferences regarding the meaning of that data and to correlate it with stimuli.},
  isbn = {978-0-12-801851-4},
  langid = {english},
  keywords = {emotions},
  file = {../../../../../snap/zotero-snap/common/Zotero/storage/KCAZ33CV/B9780128018514000112.html}
}

@article{Goodfellow2013,
  title = {Challenges in Representation Learning: {{A}} Report on Three Machine Learning Contests},
  shorttitle = {Challenges in Representation Learning},
  author = {Goodfellow, Ian J. and Erhan, Dumitru and Luc Carrier, Pierre and Courville, Aaron and Mirza, Mehdi and Hamner, Ben and Cukierski, Will and Tang, Yichuan and Thaler, David and Lee, Dong-Hyun and Zhou, Yingbo and Ramaiah, Chetan and Feng, Fangxiang and Li, Ruifan and Wang, Xiaojie and Athanasakis, Dimitris and {Shawe-Taylor}, John and Milakov, Maxim and Park, John and Ionescu, Radu and Popescu, Marius and Grozea, Cristian and Bergstra, James and Xie, Jingjing and Romaszko, Lukasz and Xu, Bing and Chuang, Zhang and Bengio, Yoshua},
  year = {2015},
  month = apr,
  journal = {Neural Networks},
  volume = {64},
  pages = {59--63},
  issn = {08936080},
  doi = {10.1016/j.neunet.2014.09.005},
  urldate = {2023-03-23},
  langid = {english},
  file = {Zotero/Goodfellow et al_2015_Challenges in representation learning2.pdf}
}

@article{Goodfellow2013arxiv,
  title = {Challenges in {{Representation Learning}}: {{A}} Report on Three Machine Learning Contests},
  shorttitle = {Challenges in {{Representation Learning}}},
  author = {Goodfellow, Ian J. and Erhan, Dumitru and Carrier, Pierre Luc and Courville, Aaron and Mirza, Mehdi and Hamner, Ben and Cukierski, Will and Tang, Yichuan and Thaler, David and Lee, Dong-Hyun and Zhou, Yingbo and Ramaiah, Chetan and Feng, Fangxiang and Li, Ruifan and Wang, Xiaojie and Athanasakis, Dimitris and {Shawe-Taylor}, John and Milakov, Maxim and Park, John and Ionescu, Radu and Popescu, Marius and Grozea, Cristian and Bergstra, James and Xie, Jingjing and Romaszko, Lukasz and Xu, Bing and Chuang, Zhang and Bengio, Yoshua},
  year = {2013},
  month = jul,
  journal = {arXiv:1307.0414 [cs, stat]},
  eprint = {1307.0414},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1307.0414},
  urldate = {2022-04-19},
  abstract = {The ICML 2013 Workshop on Challenges in Representation Learning3 focused on three challenges: the black box learning challenge, the facial expression recognition challenge, and the multimodal learning challenge. We describe the datasets created for these challenges and summarize the results of the competitions. We provide suggestions for organizers of future challenges and some comments on what kind of knowledge can be gained from machine learning competitions.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Goodfellow et al_2013_Challenges in Representation Learning.pdf}
}

@article{Goyal2022,
  title = {Fairness {{Indicators}} for {{Systematic Assessments}} of {{Visual Feature Extractors}}},
  author = {Goyal, Priya and Soriano, Adriana Romero and Hazirbas, Caner and Sagun, Levent and Usunier, Nicolas},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.07603 [cs]},
  eprint = {2202.07603},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2202.07603},
  urldate = {2022-03-02},
  abstract = {Does everyone equally benefit from computer vision systems? Answers to this question become more and more important as computer vision systems are deployed at large scale, and can spark major concerns when they exhibit vast performance discrepancies between people from various demographic and social backgrounds. Systematic diagnosis of fairness, harms, and biases of computer vision systems is an important step towards building socially responsible systems. To initiate an effort towards standardized fairness audits, we propose three fairness indicators, which aim at quantifying harms and biases of visual systems. Our indicators use existing publicly available datasets collected for fairness evaluations, and focus on three main types of harms and bias identified in the literature, namely harmful label associations, disparity in learned representations of social and demographic traits, and biased performance on geographically diverse images from across the world.We define precise experimental protocols applicable to a wide range of computer vision models. These indicators are part of an ever-evolving suite of fairness probes and are not intended to be a substitute for a thorough analysis of the broader impact of the new computer vision technologies. Yet, we believe it is a necessary first step towards (1) facilitating the widespread adoption and mandate of the fairness assessments in computer vision research, and (2) tracking progress towards building socially responsible models. To study the practical effectiveness and broad applicability of our proposed indicators to any visual system, we apply them to off-the-shelf models built using widely adopted model training paradigms which vary in their ability to whether they can predict labels on a given image or only produce the embeddings. We also systematically study the effect of data domain and model size.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {fairness_evaluation},
  file = {Zotero/Goyal et al_2022_Fairness Indicators for Systematic Assessments of Visual Feature Extractors.pdf}
}

@article{Greene2020,
  title = {The Spectrum of Facial Palsy: {{The MEEI}} Facial Palsy Photo and Video Standard Set},
  shorttitle = {The Spectrum of Facial Palsy},
  author = {Greene, Jacqueline J. and Guarin, Diego L. and Tavares, Joana and Fortier, Emily and Robinson, Mara and Dusseldorp, Joseph and Quatela, Olivia and Jowett, Nate and Hadlock, Tessa},
  year = {2020},
  journal = {The Laryngoscope},
  volume = {130},
  number = {1},
  pages = {32--37},
  issn = {1531-4995},
  doi = {10.1002/lary.27986},
  urldate = {2022-04-11},
  abstract = {Objectives Facial palsy causes variable facial disfigurement ranging from subtle asymmetry to crippling deformity. There is no existing standard database to serve as a resource for facial palsy education and research. We present a standardized set of facial photographs and videos representing the entire spectrum of flaccid and nonflaccid (aberrantly regenerated or synkinetic) facial palsy. To demonstrate the utility of the dataset, we describe the relationship between level of facial function and perceived emotion expression as determined by an automated emotion detection, machine learning-based algorithm. Methods Photographs and videos of patients with both flaccid and nonflaccid facial palsy were prospectively gathered. The degree of facial palsy was quantified using eFACE, House-Brackmann, and Sunnybrook scales. Perceived emotion during a standard video of facial movements was determined using an automated, machine learning algorithm. Results Sixty participants were enrolled and categorized by eFACE score across the range of facial function. Patients with complete flaccid facial palsy (eFACE {$<$}60) had a significant loss of perceived joy compared to the nonflaccid and normal groups. Additionally, patients with only moderate flaccid and nonflaccid facial palsy had a significant increase in perceived negative emotion (contempt) when compared to the normal group. Conclusion We provide this open-source database to assist in comparing current and future scales of facial function as well as facilitate comprehensive investigation of the entire spectrum of facial palsy. The automated machine learning-based algorithm detected negative emotions at moderate levels of facial palsy and suggested a threshold severity of flaccid facial palsy beyond which joy was not perceived. Level of Evidence NA Laryngoscope, 130:32\textendash 37, 2020},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Greene et al_2020_The spectrum of facial palsy.pdf;../../../../../snap/zotero-snap/common/Zotero/storage/VGKSJ4MY/lary.html}
}

@article{Gross2010,
  title = {Multi-{{PIE}}},
  author = {Gross, Ralph and Matthews, Iain and Cohn, Jeff and Kanade, Takeo and Baker, Simon},
  year = {2010},
  month = may,
  journal = {Proceedings of the ... International Conference on Automatic Face and Gesture Recognition. International Conference on Automatic Face and Gesture Recognition},
  volume = {28},
  number = {5},
  pages = {807--813},
  issn = {1541-5058},
  doi = {10.1016/j.imavis.2009.08.002},
  urldate = {2022-06-01},
  abstract = {A close relationship exists between the advancement of face recognition algorithms and the availability of face databases varying factors that affect facial appearance in a controlled manner. The CMU PIE database has been very influential in advancing research in face recognition across pose and illumination. Despite its success the PIE database has several shortcomings: a limited number of subjects, single recording session and only few expressions captured. To address these issues we collected the CMU Multi-PIE database. It contains 337 subjects, imaged under 15 view points and 19 illumination conditions in up to four recording sessions. In this paper we introduce the database and describe the recording procedure. We furthermore present results from baseline experiments using PCA and LDA classifiers to highlight similarities and differences between PIE and Multi-PIE.},
  pmcid = {PMC2873597},
  pmid = {20490373},
  keywords = {datasets},
  file = {Zotero/Gross et al_2010_Multi-PIE.pdf}
}

@techreport{Grother2019,
  title = {Face Recognition Vendor Test Part 3:: Demographic Effects},
  shorttitle = {Face Recognition Vendor Test Part 3},
  author = {Grother, Patrick and Ngan, Mei and Hanaoka, Kayee},
  year = {2019},
  month = dec,
  number = {NIST IR 8280},
  pages = {NIST IR 8280},
  address = {{Gaithersburg, MD}},
  institution = {{National Institute of Standards and Technology}},
  doi = {10.6028/NIST.IR.8280},
  urldate = {2021-11-08},
  langid = {english},
  file = {Zotero/Grother et al_2019_Face recognition vendor test part 3.pdf}
}

@article{Guerdelli2022,
  title = {Macro- and {{Micro-Expressions Facial Datasets}}: {{A Survey}}},
  shorttitle = {Macro- and {{Micro-Expressions Facial Datasets}}},
  author = {Guerdelli, Hajer and Ferrari, Claudio and Barhoumi, Walid and Ghazouani, Haythem and Berretti, Stefano},
  year = {2022},
  month = feb,
  journal = {Sensors},
  volume = {22},
  number = {4},
  pages = {1524},
  issn = {1424-8220},
  doi = {10.3390/s22041524},
  urldate = {2022-02-22},
  abstract = {Automatic facial expression recognition is essential for many potential applications. Thus, having a clear overview on existing datasets that have been investigated within the framework of face expression recognition is of paramount importance in designing and evaluating effective solutions, notably for neural networks-based training. In this survey, we provide a review of more than eighty facial expression datasets, while taking into account both macro- and micro-expressions. The proposed study is mostly focused on spontaneous and in-the-wild datasets, given the common trend in the research is that of considering contexts where expressions are shown in a spontaneous way and in a real context. We have also provided instances of potential applications of the investigated datasets, while putting into evidence their pros and cons. The proposed survey can help researchers to have a better understanding of the characteristics of the existing datasets, thus facilitating the choice of the data that best suits the particular context of their application.},
  langid = {english},
  keywords = {datasets,review},
  file = {Zotero/Guerdelli et al_2022_Macro- and Micro-Expressions Facial Datasets.pdf}
}

@article{Happy2015,
  title = {Automatic Facial Expression Recognition Using Features of Salient Facial Patches},
  author = {Happy, S L and Routray, Aurobinda},
  year = {2015},
  month = jan,
  journal = {IEEE Transactions on Affective Computing},
  volume = {6},
  number = {1},
  pages = {1--12},
  issn = {1949-3045},
  doi = {10.1109/TAFFC.2014.2386334},
  abstract = {Extraction of discriminative features from salient facial patches plays a vital role in effective facial expression recognition. The accurate detection of facial landmarks improves the localization of the salient patches on face images. This paper proposes a novel framework for expression recognition by using appearance features of selected facial patches. A few prominent facial patches, depending on the position of facial landmarks, are extracted which are active during emotion elicitation. These active patches are further processed to obtain the salient patches which contain discriminative features for classification of each pair of expressions, thereby selecting different facial patches as salient for different pair of expression classes. One-against-one classification method is adopted using these features. In addition, an automated learning-free facial landmark detection technique has been proposed, which achieves similar performances as that of other state-of-art landmark detection methods, yet requires significantly less execution time. The proposed method is found to perform well consistently in different resolutions, hence, providing a solution for expression recognition in low resolution images. Experiments on CK+ and JAFFE facial expression databases show the effectiveness of the proposed system.},
  keywords = {fer_implementation},
  file = {Zotero/Happy_Routray_2015_Automatic facial expression recognition using features of salient facial patches.pdf}
}

@article{Happy2017,
  title = {The {{Indian Spontaneous Expression Database}} for {{Emotion Recognition}}},
  author = {Happy, S L and Patnaik, Priyadarshi and Routray, Aurobinda and Guha, Rajlakshmi},
  year = {2017},
  month = jan,
  journal = {IEEE Transactions on Affective Computing},
  volume = {8},
  number = {1},
  pages = {131--142},
  issn = {1949-3045},
  doi = {10.1109/TAFFC.2015.2498174},
  abstract = {Automatic recognition of spontaneous facial expressions is a major challenge in the field of affective computing. Head rotation, face pose, illumination variation, occlusion etc. are the attributes that increase the complexity of recognition of spontaneous expressions in practical applications. Effective recognition of expressions depends significantly on the quality of the database used. Most well-known facial expression databases consist of posed expressions. However, currently there is a huge demand for spontaneous expression databases for the pragmatic implementation of the facial expression recognition algorithms. In this paper, we propose and establish a new facial expression database containing spontaneous expressions of both male and female participants of Indian origin. The database consists of 428 segmented video clips of the spontaneous facial expressions of 50 participants. In our experiment, emotions were induced among the participants by using emotional videos and simultaneously their self-ratings were collected for each experienced emotion. Facial expression clips were annotated carefully by four trained decoders, which were further validated by the nature of stimuli used and self-report of emotions. An extensive analysis was carried out on the database using several machine learning algorithms and the results are provided for future reference. Such a spontaneous database will help in the development and validation of algorithms for recognition of spontaneous expressions.},
  keywords = {datasets},
  file = {Zotero/Happy et al_2017_The Indian Spontaneous Expression Database for Emotion Recognition.pdf}
}

@inproceedings{Hemmatiyan-Larki2022,
  title = {Facial {{Expression Recognition}}: A {{Comparison}} with {{Different Classical}} and {{Deep Learning Methods}}},
  shorttitle = {Facial {{Expression Recognition}}},
  booktitle = {2022 {{International Conference}} on {{Machine Vision}} and {{Image Processing}} ({{MVIP}})},
  author = {{Hemmatiyan-Larki}, Amir Mohammad and {Rafiee-Karkevandi}, Fatemeh and {Yazdian-Dehkordi}, Mahdi},
  year = {2022},
  month = feb,
  pages = {1--5},
  issn = {2166-6784},
  doi = {10.1109/MVIP53647.2022.9738553},
  abstract = {Facial Expression Recognition (FER), also known as Facial Emotion Recognition, is an active topic in computer vision and machine learning fields. This paper analyzes different feature extraction and classification methods to propose an efficient facial expression recognition system. We have studied several feature extraction methods, including Histogram of Oriented Gradients (HOG), face-encoding, and the features extracted by a VGG16 Network. For classification, different classical classifiers, including Support Vector Machines (SVM), Adaptive Boosting (AdaBoost), and Logistic Regression, are evaluated with these features. Besides, we have trained a ResNet50 model from scratch and also tuned a ResNet50 which is pre-trained on VGGFace2 dataset. Finally, a part-based ensemble classifier is also proposed by focusing on different parts of face images. The experimental results provided on FER-2013 Dataset show that the tuned model of ResNet50 with a complete image of face, achieves higher performance than the other methods.},
  keywords = {review},
  file = {Zotero/Hemmatiyan-Larki et al_2022_Facial Expression Recognition.pdf}
}

@inproceedings{Hernandez2021,
  title = {Guidelines for {{Assessing}} and {{Minimizing Risks}} of {{Emotion Recognition Applications}}},
  booktitle = {International {{Conference}} on {{Affective Computing}} \& {{Intelligent Interaction}} ({{ACII}} 2021)},
  author = {Hernandez, Javier and Lovejoy, Josh and McDuff, Daniel and Suh, Jina and O'Brien, Tim and Sethumadhavan, Arathi and Greene, Gretchen and Picard, Rosalind and Czerwinski, Mary},
  year = {2021},
  month = oct,
  pages = {8},
  abstract = {Society has witnessed a rapid increase in the adoption of commercial uses of emotion recognition. Tools that were traditionally used by domain experts are now being used by individuals who are often unaware of the technology's limitations and may use them in potentially harmful settings. The change in scale and agency, paired with gaps in regulation, urge the research community to rethink how we design, position, implement and ultimately deploy emotion recognition to anticipate and minimize potential risks. To help understand the current ecosystem of applied emotion recognition, this work provides an overview of some of the most frequent commercial applications and identifies some of the potential sources of harm. Informed by these, we then propose 12 guidelines for systematically assessing and reducing the risks presented by emotion recognition applications. These guidelines can help identify potential misuses and inform future deployments of emotion recognition.},
  langid = {english},
  keywords = {fairness,fer_applications,review},
  file = {Zotero/Hernandez et al_2021_Guidelines for Assessing and Minimizing Risks of Emotion Recognition.pdf}
}

@article{Hewahi2011,
  title = {Impact of {{Ethnic Group}} on {{Human Emotion Recognition Using Backpropagation Neural Network}}},
  author = {Hewahi, Nabil M and Baraka, Abdul Rhman M},
  year = {2011},
  volume = {2},
  number = {4},
  pages = {8},
  abstract = {We claim that knowing the ethnic group of human would increase the accuracy of the emotion recognition. This is due to the difference between the face appearances and expressions of various ethnic groups. To test our claim, we developed an approach based on Artificial Neural Networks (ANN) using backpropgation algorithm to recognize the human emotion through facial expressions. Our approach has been tested by using MSDEF dataset, and we found that, there is a positive effect on the accuracy of the recognition of emotion if we consider the ethnic group as input factor while building the recognition model. We achieved 8\% improvement rate.},
  langid = {english},
  keywords = {bias_example},
  file = {Zotero/Hewahi_Baraka_2011_Impact of Ethnic Group on Human Emotion Recognition Using Backpropagation.pdf}
}

@article{Jack2009,
  title = {Cultural {{Confusions Show}} That {{Facial Expressions Are Not Universal}}},
  author = {Jack, Rachael E. and Blais, Caroline and Scheepers, Christoph and Schyns, Philippe G. and Caldara, Roberto},
  year = {2009},
  month = sep,
  journal = {Current Biology},
  volume = {19},
  number = {18},
  pages = {1543--1548},
  issn = {09609822},
  doi = {10.1016/j.cub.2009.07.051},
  urldate = {2021-10-18},
  abstract = {Central to all human interaction is the mutual understanding of emotions, achieved primarily by a set of biologically rooted social signals evolved for this purpose\textemdash facial expressions of emotion. Although facial expressions are widely considered to be the universal language of emotion [1\textendash 3], some negative facial expressions consistently elicit lower recognition levels among Eastern compared to Western groups (see [4] for a meta-analysis and [5, 6] for review). Here, focusing on the decoding of facial expression signals, we merge behavioral and computational analyses with novel spatiotemporal analyses of eye movements, showing that Eastern observers use a culture-specific decoding strategy that is inadequate to reliably distinguish universal facial expressions of ``fear'' and ``disgust.'' Rather than distributing their fixations evenly across the face as Westerners do, Eastern observers persistently fixate the eye region. Using a model information sampler, we demonstrate that by persistently fixating the eyes, Eastern observers sample ambiguous information, thus causing significant confusion. Our results question the universality of human facial expressions of emotion, highlighting their true complexity, with critical consequences for crosscultural communication and globalization.},
  langid = {english},
  keywords = {bias_example,psychology},
  file = {Zotero/Jack et al_2009_Cultural Confusions Show that Facial Expressions Are Not Universal.pdf}
}

@inproceedings{Jannat2021,
  title = {Expression {{Recognition Across Age}}},
  booktitle = {2021 16th {{IEEE International Conference}} on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}} 2021)},
  author = {Jannat, Sk Rahatul and Canavan, Shaun},
  year = {2021},
  month = dec,
  pages = {1--5},
  doi = {10.1109/FG52635.2021.9667062},
  abstract = {Expression recognition is an important and growing field in AI. It has applications in fields including, but not limited to, medicine, security, and entertainment. A large portion of research, in this area, has focused on recognizing expressions of young and middle-age adults with less focus on children and elderly subjects. This focus can lead to unintentional bias across age, resulting in less accurate models. Considering this, we investigate the impact of age on expression recognition. To facilitate this investigation, we evaluate two state-of-the-art datasets, that focus on different age ranges (children and elderly), namely EmoReact and ElderReact. We propose a Siamese-network-based approach that learns the semantic similarity of expressions relative to each age. We show that the proposed approach, to expression recognition, is able to generalize across age. We show the proposed approach is comparable to or outperforms current state-of-the-art on the EmoReact and ElderReact datasets.},
  keywords = {age,bias_example},
  file = {Zotero/Jannat_Canavan_2021_Expression Recognition Across Age.pdf}
}

@article{Jobin2019,
  title = {The Global Landscape of {{AI}} Ethics Guidelines},
  author = {Jobin, Anna and Ienca, Marcello and Vayena, Effy},
  year = {2019},
  month = sep,
  journal = {Nature Machine Intelligence},
  volume = {1},
  number = {9},
  pages = {389--399},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0088-2},
  urldate = {2022-02-16},
  langid = {english},
  keywords = {fairness},
  file = {Zotero/Jobin et al_2019_The global landscape of AI ethics guidelines.pdf}
}

@inproceedings{Joseph2021,
  title = {Facial {{Expression Recognition}} for the {{Blind Using Deep Learning}}},
  booktitle = {2021 {{IEEE}} 4th {{International Conference}} on {{Computing}}, {{Power}} and {{Communication Technologies}} ({{GUCON}})},
  author = {Joseph, Jinu Lilly and Mathew, Santhosh P.},
  year = {2021},
  month = sep,
  pages = {1--5},
  publisher = {{IEEE}},
  address = {{Kuala Lumpur, Malaysia}},
  doi = {10.1109/GUCON50781.2021.9574035},
  urldate = {2022-01-06},
  abstract = {A large number of people living around us are visually impaired. One of the most difficult tasks faced by them is the identification of the expression of the people in front of them. They are not aware of the intentions and emotions of the other person. Thus a system to assist the blind in recognizing the facial expressions of the confronting person can be of great use.},
  isbn = {978-1-72819-951-1},
  langid = {english},
  keywords = {fer_applications},
  file = {Zotero/Joseph_Mathew_2021_Facial Expression Recognition for the Blind Using Deep Learning.pdf}
}

@article{Jost2006,
  title = {Entropy and Diversity},
  author = {Jost, Lou},
  year = {2006},
  month = may,
  journal = {Oikos},
  volume = {113},
  number = {2},
  pages = {363--375},
  issn = {00301299},
  doi = {10.1111/j.2006.0030-1299.14714.x},
  urldate = {2022-12-04},
  langid = {english},
  file = {Zotero/Jost_2006_Entropy and diversity.pdf}
}

@article{Kamiran_ClassificationNoDiscrimination,
  title = {Classification with {{No Discrimination}} by {{Preferential Sampling}}},
  author = {Kamiran, Faisal and Calders, Toon and Kamiran, F and Nl, Tue and Calders, T and Nl, Tue},
  pages = {6},
  abstract = {The concept of classification without discrimination is a new area of research. (Kamiran \& Calders, ) introduced the idea of Classification with No Discrimination (CND) and proposed a solution based on ``massaging'' the data to remove the discrimination from it with the least possible changes. In this paper, we propose a new solution to the CND problem by introducing a sampling scheme for making the data discrimination free instead of relabeling the dataset. On the resulting non-discriminatory dataset we then learn a classifier. This new method is not only less intrusive as compared to the ``massaging'' but also outperforms the ``reweighing'' approach of (Calders et al., 2009). The proposed method has been implemented and experimental results on the Census Income dataset show promising results: in all experiments our method performs onpar with the state-of-the art non-discriminatory techniques.},
  langid = {english},
  keywords = {bias_mitigation},
  file = {Zotero/Kamiran et al_Classiﬁcation with No Discrimination by Preferential Sampling.pdf}
}

@inproceedings{Kamiran2009,
  title = {Classifying without Discriminating},
  booktitle = {2009 2nd {{International Conference}} on {{Computer}}, {{Control}} and {{Communication}}},
  author = {Kamiran, Faisal and Calders, Toon},
  year = {2009},
  month = feb,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Karachi, Pakistan}},
  doi = {10.1109/IC4.2009.4909197},
  urldate = {2022-02-17},
  isbn = {978-1-4244-3313-1},
  keywords = {bias_mitigation},
  file = {Zotero/Kamiran_Calders_2009_Classifying without discriminating.pdf}
}

@article{Kamishima2012,
  title = {Enhancement of the {{Neutrality}} in {{Recommendation}}},
  author = {Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun},
  year = {2012},
  journal = {Proc. of the 2nd Workshop on Human Decision Making in Recommender Systems},
  volume = {893},
  pages = {8--14},
  abstract = {This paper proposes an algorithm for making recommendation so that the neutrality toward the viewpoint specified by a user is enhanced. This algorithm is useful for avoiding to make decisions based on biased information. Such a problem is pointed out as the filter bubble, which is the influence in social decisions biased by a personalization technology. To provide such a recommendation, we assume that a user specifies a viewpoint toward which the user want to enforce the neutrality, because recommendation that is neutral from any information is no longer recommendation. Given such a target viewpoint, we implemented information neutral recommendation algorithm by introducing a penalty term to enforce the statistical independence between the target viewpoint and a preference score. We empirically show that our algorithm enhances the independence toward the specified viewpoint by and then demonstrate how sets of recommended items are changed.},
  langid = {english},
  keywords = {fairness},
  file = {Zotero/Kamishima et al_2012_Enhancement of the Neutrality in Recommendation.pdf}
}

@article{Kammoun2022,
  title = {Generative {{Adversarial Networks}} for Face Generation: {{A}} Survey},
  shorttitle = {Generative {{Adversarial Networks}} for Face Generation},
  author = {Kammoun, Amina and Slama, Rim and Tabia, Hedi and Ouni, Tarek and Abid, Mohmed},
  year = {2022},
  month = mar,
  journal = {ACM Computing Surveys},
  issn = {0360-0300},
  doi = {10.1145/1122445.1122456},
  urldate = {2022-12-20},
  abstract = {Recently, Generative Adversarial Networks (GANs) have received enormous progress, which makes them able to learn complex data distributions in particular faces. More and more efficient GAN architectures have been designed and proposed to learn the different variations of faces, such as cross pose, age, expression and style. These GAN based approaches need to be reviewed, discussed, and categorized in terms of architectures, applications, and metrics. Several reviews that focus on the use and advances of GAN in general have been proposed. However, the GAN models applied to the face, that we call facial GANs, have never been addressed. In this article, we review facial GANs and their different applications. We mainly focus on architectures, problems and performance evaluation with respect to each application and used datasets. More precisely, we reviewed the progress of architectures and we discussed the contributions and limits of each. Then, we exposed the encountered problems of facial GANs and proposed solutions to handle them. Additionally, as GANs evaluation has become a notable current defiance, we investigate the state of the art quantitative and qualitative evaluation metrics and their applications. We concluded the article with a discussion on the face generation challenges and proposed open research issues.},
  keywords = {fer_implementation},
  annotation = {Just Accepted},
  file = {Zotero/Kammoun et al_2022_Generative Adversarial Networks for face generation.pdf}
}

@inproceedings{Kanade2000,
  title = {Comprehensive Database for Facial Expression Analysis},
  booktitle = {Proceedings {{Fourth IEEE International Conference}} on {{Automatic Face}} and {{Gesture Recognition}} ({{Cat}}. {{No}}. {{PR00580}})},
  author = {Kanade, T. and Cohn, J.F. and Tian, Yingli},
  year = {2000},
  month = mar,
  pages = {46--53},
  doi = {10.1109/AFGR.2000.840611},
  abstract = {Within the past decade, significant effort has occurred in developing methods of facial expression analysis. Because most investigators have used relatively limited data sets, the generalizability of these various methods remains unknown. We describe the problem space for facial expression analysis, which includes level of description, transitions among expressions, eliciting conditions, reliability and validity of training and test data, individual differences in subjects, head orientation and scene complexity image characteristics, and relation to non-verbal behavior. We then present the CMU-Pittsburgh AU-Coded Face Expression Image Database, which currently includes 2105 digitized image sequences from 182 adult subjects of varying ethnicity, performing multiple tokens of most primary FACS action units. This database is the most comprehensive testbed to date for comparative studies of facial expression analysis.},
  file = {Zotero/Kanade et al_2000_Comprehensive database for facial expression analysis.pdf;../../../../../snap/zotero-snap/common/Zotero/storage/KRYCTD3V/840611.html}
}

@article{Kang2021,
  title = {{{MultiFair}}: {{Multi-Group Fairness}} in {{Machine Learning}}},
  shorttitle = {{{MultiFair}}},
  author = {Kang, Jian and Xie, Tiankai and Wu, Xintao and Maciejewski, Ross and Tong, Hanghang},
  year = {2021},
  month = may,
  journal = {arXiv:2105.11069 [cs, math, stat]},
  eprint = {2105.11069},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/2105.11069},
  urldate = {2022-02-21},
  abstract = {Algorithmic fairness is becoming increasingly important in data mining and machine learning, and one of the most fundamental notions is group fairness. The vast majority of the existing works on group fairness, with a few exceptions, primarily focus on debiasing with respect to a single sensitive attribute, despite the fact that the co-existence of multiple sensitive attributes (e.g., gender, race, marital status, etc.) in the real-world is commonplace. As such, methods that can ensure a fair learning outcome with respect to all sensitive attributes of concern simultaneously need to be developed. In this paper, we study multi-group fairness in machine learning (MultiFair), where statistical parity, a representative group fairness measure, is guaranteed among demographic groups formed by multiple sensitive attributes of interest. We formulate it as a mutual information minimization problem and propose a generic end-to-end algorithmic framework to solve it. The key idea is to leverage a variational representation of mutual information, which considers the variational distribution between learning outcomes and sensitive attributes, as well as the density ratio between the variational and the original distributions. Our proposed framework is generalizable to many different settings, including other statistical notions of fairness, and could handle any type of learning task equipped with a gradient-based optimizer. Empirical evaluations in the fair classification task on three real-world datasets demonstrate that our proposed framework can effectively debias the classification results with minimal impact to the classification accuracy.},
  archiveprefix = {arxiv},
  keywords = {fairness},
  file = {Zotero/Kang et al_2021_MultiFair.pdf}
}

@inproceedings{Karkkainen2021,
  title = {{{FairFace}}: {{Face Attribute Dataset}} for {{Balanced Race}}, {{Gender}}, and {{Age}} for {{Bias Measurement}} and {{Mitigation}}},
  shorttitle = {{{FairFace}}},
  booktitle = {2021 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Karkkainen, Kimmo and Joo, Jungseock},
  year = {2021},
  month = jan,
  pages = {1547--1557},
  publisher = {{IEEE}},
  address = {{Waikoloa, HI, USA}},
  doi = {10.1109/WACV48630.2021.00159},
  urldate = {2021-08-27},
  isbn = {978-1-66540-477-8},
  keywords = {datasets,demographic_analysis},
  file = {Zotero/Karkkainen_Joo_2021_FairFace.pdf}
}

@article{Keyes2018,
  title = {The {{Misgendering Machines}}: {{Trans}}/{{HCI Implications}} of {{Automatic Gender Recognition}}},
  shorttitle = {The {{Misgendering Machines}}},
  author = {Keyes, Os},
  year = {2018},
  month = nov,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {2},
  number = {CSCW},
  pages = {1--22},
  issn = {2573-0142},
  doi = {10.1145/3274357},
  urldate = {2021-09-30},
  langid = {english},
  keywords = {bias_example,gender},
  file = {Zotero/Keyes_2018_The Misgendering Machines.pdf}
}

@article{Khaireddin2021,
  title = {Facial {{Emotion Recognition}}: {{State}} of the {{Art Performance}} on {{FER2013}}},
  shorttitle = {Facial {{Emotion Recognition}}},
  author = {Khaireddin, Yousif and Chen, Zhuofa},
  year = {2021},
  month = may,
  journal = {arXiv:2105.03588 [cs]},
  eprint = {2105.03588},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2105.03588},
  urldate = {2021-08-11},
  abstract = {Facial emotion recognition (FER) is significant for human-computer interaction such as clinical practice and behavioral description. Accurate and robust FER by computer models remains challenging due to the heterogeneity of human faces and variations in images such as different facial pose and lighting. Among all techniques for FER, deep learning models, especially Convolutional Neural Networks (CNNs) have shown great potential due to their powerful automatic feature extraction and computational efficiency. In this work, we achieve the highest single-network classification accuracy on the FER2013 dataset. We adopt the VGGNet architecture, rigorously fine-tune its hyperparameters, and experiment with various optimization methods. To our best knowledge, our model achieves state-of-the-art single-network accuracy of 73.28 \% on FER2013 without using extra training data.},
  archiveprefix = {arxiv},
  keywords = {methods},
  file = {Zotero/Khaireddin_Chen_2021_Facial Emotion Recognition.pdf}
}

@article{Khan2019,
  title = {A Novel Database of Children's Spontaneous Facial Expressions ({{LIRIS-CSE}})},
  author = {Khan, Rizwan Ahmed and Crenn, Arthur and Meyer, Alexandre and Bouakaz, Saida},
  year = {2019},
  month = mar,
  journal = {Image and Vision Computing},
  volume = {83--84},
  pages = {61--69},
  issn = {02628856},
  doi = {10.1016/j.imavis.2019.02.004},
  urldate = {2021-10-19},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Khan et al_2019_A novel database of children's spontaneous facial expressions (LIRIS-CSE).pdf}
}

@article{Khanh2021,
  title = {Korean Video Dataset for Emotion Recognition in the Wild},
  author = {Khanh, Trinh Le Ba and Kim, Soo-Hyung and Lee, Gueesang and Yang, Hyung-Jeong and Baek, Eu-Tteum},
  year = {2021},
  month = mar,
  journal = {Multimedia Tools and Applications},
  volume = {80},
  number = {6},
  pages = {9479--9492},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-020-10106-1},
  urldate = {2021-11-08},
  abstract = {Emotion recognition is one of the hottest fields in affective computing research. Recognizing emotions is an important task for facilitating communication between machines and humans. However, it is a very challenging task based on a lack of ethnically diverse databases. In particular, emotional expressions tend to be very dissimilar between Western and Eastern people. Therefore, diverse emotion databases are required for studying emotional expression. However, majority of the well-known emotion databases focus on Western people, which exhibit different characteristics compared to Eastern people. In this study, we constructed a novel emotion dataset containing more than 1200 video clips collected from Korean movies, called Korean Video Dataset for Emotion Recognition in the Wild (KVDERW). Which are similar to real-world conditions, with the goal of studying the emotions of Eastern people, particularly Korean people. Additionally, we developed a semi-automatic video emotion labelling tool that could be used to generate video clips and annotate the emotions in clips.},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Khanh et al_2021_Korean video dataset for emotion recognition in the wild.pdf}
}

@inproceedings{Kim2019,
  title = {Learning {{Not}} to {{Learn}}: {{Training Deep Neural Networks With Biased Data}}},
  shorttitle = {Learning {{Not}} to {{Learn}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Kim, Byungju and Kim, Hyunwoo and Kim, Kyungsu and Kim, Sungjin and Kim, Junmo},
  year = {2019},
  month = jun,
  pages = {9004--9012},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00922},
  urldate = {2023-02-08},
  abstract = {We propose a novel regularization algorithm to train deep neural networks, in which data at training time is severely biased. Since a neural network efficiently learns data distribution, a network is likely to learn the bias information to categorize input data. It leads to poor performance at test time, if the bias is, in fact, irrelevant to the categorization. In this paper, we formulate a regularization loss based on mutual information between feature embedding and bias. Based on the idea of minimizing this mutual information, we propose an iterative algorithm to unlearn the bias information. We employ an additional network to predict the bias distribution and train the network adversarially against the feature embedding network. At the end of learning, the bias prediction network is not able to predict the bias not because it is poorly trained, but because the feature embedding network successfully unlearns the bias information. We also demonstrate quantitative and qualitative experimental results which show that our algorithm effectively removes the bias information from feature embedding.},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {../../../../../snap/zotero-snap/common/Zotero/storage/UTI6REUP/Kim et al. - 2019 - Learning Not to Learn Training Deep Neural Networ.pdf}
}

@incollection{Kim2021,
  title = {Age {{Bias}} in {{Emotion Detection}}: {{An Analysis}} of {{Facial Emotion Recognition Performance}} on {{Young}}, {{Middle-Aged}}, and {{Older Adults}}},
  shorttitle = {Age {{Bias}} in {{Emotion Detection}}},
  booktitle = {Proceedings of the 2021 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Kim, Eugenia and Bryant, De'Aira and Srikanth, Deepak and Howard, Ayanna},
  year = {2021},
  month = jul,
  pages = {638--644},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  url = {https://doi.org/10.1145/3461702.3462609},
  urldate = {2022-04-06},
  abstract = {The growing potential for facial emotion recognition (FER) technology has encouraged expedited development at the cost of rigorous validation. Many of its use-cases may also impact the diverse global community as FER becomes embedded into domains ranging from education to security to healthcare. Yet, prior work has highlighted that FER can exhibit both gender and racial biases like other facial analysis techniques. As a result, bias-mitigation research efforts have mainly focused on tackling gender and racial disparities, while other demographic related biases, such as age, have seen less progress. This work seeks to examine the performance of state of the art commercial FER technology on expressive images of men and women from three distinct age groups. We utilize four different commercial FER systems in a black box methodology to evaluate how six emotions - anger, disgust, fear, happiness, neutrality, and sadness - are correctly detected by age group. We further investigate how algorithmic changes over the last year have affected system performance. Our results found that all four commercial FER systems most accurately perceived emotion in images of young adults and least accurately in images of older adults. This trend was observed for analyses conducted in 2019 and 2020. However, little to no gender disparities were observed in either year. While older adults may not have been the initial target consumer of FER technology, statistics show the demographic is quickly growing more keen to applications that use such systems. Our results demonstrate the importance of considering various demographic subgroups during FER system validation and the need for inclusive, intersectional algorithmic developmental practices.},
  isbn = {978-1-4503-8473-5},
  keywords = {age,bias_example},
  file = {Zotero/Kim et al_2021_Age Bias in Emotion Detection.pdf}
}

@article{King2015,
  title = {Max-{{Margin Object Detection}}},
  author = {King, Davis E.},
  year = {2015},
  month = jan,
  journal = {arXiv:1502.00046 [cs]},
  eprint = {1502.00046},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1502.00046},
  urldate = {2022-04-25},
  abstract = {Most object detection methods operate by applying a binary classifier to sub-windows of an image, followed by a non-maximum suppression step where detections on overlapping sub-windows are removed. Since the number of possible sub-windows in even moderately sized image datasets is extremely large, the classifier is typically learned from only a subset of the windows. This avoids the computational difficulty of dealing with the entire set of sub-windows, however, as we will show in this paper, it leads to sub-optimal detector performance. In particular, the main contribution of this paper is the introduction of a new method, Max-Margin Object Detection (MMOD), for learning to detect objects in images. This method does not perform any sub-sampling, but instead optimizes over all sub-windows. MMOD can be used to improve any object detection method which is linear in the learned parameters, such as HOG or bag-of-visual-word models. Using this approach we show substantial performance gains on three publicly available datasets. Strikingly, we show that a single rigid HOG filter can outperform a state-of-the-art deformable part model on the Face Detection Data Set and Benchmark when the HOG filter is learned via MMOD.},
  archiveprefix = {arxiv},
  keywords = {fer_implementation},
  file = {Zotero/King_2015_Max-Margin Object Detection.pdf;../../../../../snap/zotero-snap/common/Zotero/storage/Q4ENPWVI/1502.html}
}

@inproceedings{Kleinberg2017,
  title = {Inherent {{Trade-Offs}} in the {{Fair Determination}} of {{Risk Scores}}},
  booktitle = {8th {{Innovations}} in {{Theoretical Computer Science Conference}} ({{ITCS}} 2017)},
  author = {Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish},
  editor = {Papadimitriou, Christos H.},
  year = {2017},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  volume = {67},
  pages = {43:1--43:23},
  publisher = {{Schloss Dagstuhl\textendash Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  issn = {1868-8969},
  doi = {10.4230/LIPIcs.ITCS.2017.43},
  urldate = {2022-04-04},
  isbn = {978-3-95977-029-3},
  keywords = {fairness,fairness_evaluation},
  file = {Zotero/Kleinberg et al_2017_Inherent Trade-Offs in the Fair Determination of Risk Scores.pdf}
}

@article{Kohn1982,
  title = {Sample {{Size Dependence}} in {{Measures}} of {{Proportional Similarity}}},
  author = {Kohn, Aj and Riggs, Ac},
  year = {1982},
  journal = {Marine Ecology Progress Series},
  volume = {9},
  pages = {147--151},
  issn = {0171-8630, 1616-1599},
  doi = {10.3354/meps009147},
  urldate = {2022-12-04},
  langid = {english},
  file = {Zotero/Kohn_Riggs_1982_Sample Size Dependence in Measures of Proportional Similarity.pdf}
}

@inproceedings{Kollias_Analysingaffectivebehavior,
  title = {Analysing Affective Behavior in the First {{ABAW}} 2020 Competition},
  booktitle = {2020 15th {{IEEE}} International Conference on Automatic Face and Gesture Recognition ({{FG}} 2020)({{FG}})},
  author = {Kollias, D and Schulc, A and Hajiyev, E and Zafeiriou, S},
  pages = {794--800},
  keywords = {aff-wild-2,datasets}
}

@article{Kollias2019,
  title = {Expression, Affect, Action Unit Recognition: {{Aff-wild2}}, Multi-Task Learning and {{ArcFace}}},
  author = {Kollias, Dimitrios and Zafeiriou, Stefanos},
  year = {2019},
  journal = {arXiv preprint arXiv:1910.04855},
  eprint = {1910.04855},
  archiveprefix = {arxiv},
  keywords = {aff-wild-2,datasets}
}

@article{Kollias2019a,
  title = {Face Behavior a La Carte: {{Expressions}}, Affect and Action Units in a Single Network},
  author = {Kollias, Dimitrios and Sharmanska, Viktoriia and Zafeiriou, Stefanos},
  year = {2019},
  journal = {arXiv preprint arXiv:1910.11111},
  eprint = {1910.11111},
  archiveprefix = {arxiv},
  keywords = {aff-wild-2,datasets}
}

@article{Kollias2019b,
  title = {Deep Affect Prediction In-the-Wild: {{Aff-wild}} Database and Challenge, Deep Architectures, and Beyond},
  author = {Kollias, Dimitrios and Tzirakis, Panagiotis and Nicolaou, Mihalis A and Papaioannou, Athanasios and Zhao, Guoying and Schuller, Bj{\"o}rn and Kotsia, Irene and Zafeiriou, Stefanos},
  year = {2019},
  journal = {International Journal of Computer Vision},
  pages = {1--23},
  publisher = {{Springer}},
  keywords = {aff-wild-2,datasets}
}

@article{Kollias2021,
  title = {Affect Analysis In-the-Wild: {{Valence-arousal}}, Expressions, Action Units and a Unified Framework},
  author = {Kollias, Dimitrios and Zafeiriou, Stefanos},
  year = {2021},
  journal = {arXiv preprint arXiv:2103.15792},
  eprint = {2103.15792},
  archiveprefix = {arxiv},
  keywords = {aff-wild-2,datasets}
}

@article{Kollias2021a,
  title = {Distribution Matching for Heterogeneous Multi-Task Learning: A Large-Scale Face Study},
  author = {Kollias, Dimitrios and Sharmanska, Viktoriia and Zafeiriou, Stefanos},
  year = {2021},
  journal = {arXiv preprint arXiv:2105.03790},
  eprint = {2105.03790},
  archiveprefix = {arxiv},
  keywords = {aff-wild-2,datasets}
}

@inproceedings{Kollias2021b,
  title = {Analysing Affective Behavior in the Second Abaw2 Competition},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision},
  author = {Kollias, Dimitrios and Zafeiriou, Stefanos},
  year = {2021},
  pages = {3652--3660},
  keywords = {aff-wild-2,datasets}
}

@article{Kollias2022,
  title = {{{ABAW}}: {{Valence-arousal}} Estimation, Expression Recognition, Action Unit Detection \& Multi-Task Learning Challenges},
  author = {Kollias, Dimitrios},
  year = {2022},
  journal = {arXiv preprint arXiv:2202.10659},
  eprint = {2202.10659},
  archiveprefix = {arxiv},
  keywords = {aff-wild-2,datasets}
}

@article{Kossaifi2021,
  title = {{{SEWA DB}}: {{A Rich Database}} for {{Audio-Visual Emotion}} and {{Sentiment Research}} in the {{Wild}}},
  shorttitle = {{{SEWA DB}}},
  author = {Kossaifi, Jean and Walecki, Robert and Panagakis, Yannis and Shen, Jie and Schmitt, Maximilian and Ringeval, Fabien and Han, Jing and Pandit, Vedhas and Toisoul, Antoine and Schuller, Bj{\"o}rn and Star, Kam and Hajiyev, Elnar and Pantic, Maja},
  year = {2021},
  month = mar,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {43},
  number = {3},
  pages = {1022--1040},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2019.2944808},
  abstract = {Natural human-computer interaction and audio-visual human behaviour sensing systems, which would achieve robust performance in-the-wild are more needed than ever as digital devices are increasingly becoming an indispensable part of our life. Accurately annotated real-world data are the crux in devising such systems. However, existing databases usually consider controlled settings, low demographic variability, and a single task. In this paper, we introduce the SEWA database of more than 2,000 minutes of audio-visual data of 398 people coming from six cultures, 50 percent female, and uniformly spanning the age range of 18 to 65 years old. Subjects were recorded in two different contexts: while watching adverts and while discussing adverts in a video chat. The database includes rich annotations of the recordings in terms of facial landmarks, facial action units (FAU), various vocalisations, mirroring, and continuously valued valence, arousal, liking, agreement, and prototypic examples of (dis)liking. This database aims to be an extremely valuable resource for researchers in affective computing and automatic human sensing and is expected to push forward the research in human behaviour analysis, including cultural studies. Along with the database, we provide extensive baseline experiments for automatic FAU detection and automatic valence, arousal, and (dis)liking intensity estimation.},
  file = {Zotero/Kossaifi et al_2021_SEWA DB.pdf;../../../../../snap/zotero-snap/common/Zotero/storage/2FNSJ4N6/8854185.html}
}

@inproceedings{Kosti2017,
  title = {Emotion {{Recognition}} in {{Context}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Kosti, Ronak and Alvarez, Jose M. and Recasens, Adria and Lapedriza, Agata},
  year = {2017},
  month = jul,
  pages = {1960--1968},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.212},
  urldate = {2021-11-23},
  abstract = {Understanding what a person is experiencing from her frame of reference is essential in our everyday life. For this reason, one can think that machines with this type of ability would interact better with people. However, there are no current systems capable of understanding in detail people's emotional states. Previous research on computer vision to recognize emotions has mainly focused on analyzing the facial expression, usually classifying it into the 6 basic emotions [11]. However, the context plays an important role in emotion perception, and when the context is incorporated, we can infer more emotional states. In this paper we present the ``Emotions in Context Database'' (EMOTIC), a dataset of images containing people in context in non-controlled environments. In these images, people are annotated with 26 emotional categories and also with the continuous dimensions valence, arousal, and dominance [21]. With the EMOTIC dataset, we trained a Convolutional Neural Network model that jointly analyses the person and the whole scene to recognize rich information about emotional states. With this, we show the importance of considering the context for recognizing people's emotions in images, and provide a benchmark in the task of emotion recognition in visual context.},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Kosti et al_2017_Emotion Recognition in Context.pdf}
}

@article{Kring1998,
  title = {Sex {{Differences}} in {{Emotion}}: {{Expression}}, {{Experience}}, and {{Physiology}}},
  author = {Kring, Ann M and Gordon, Albert H},
  year = {1998},
  journal = {Journal of Personality and Social Psychology},
  pages = {18},
  langid = {english},
  file = {Zotero/Kring_Gordon_1998_Sex Differences in Emotion.pdf}
}

@article{Kulkarni2018,
  title = {Automatic {{Recognition}} of {{Facial Displays}} of {{Unfelt Emotions}}},
  author = {Kulkarni, Kaustubh and Corneanu, Ciprian Adrian and Ofodile, Ikechukwu and Escalera, Sergio and Baro, Xavier and Hyniewska, Sylwia and Allik, Juri and Anbarjafari, Gholamreza},
  year = {2018},
  month = jan,
  journal = {arXiv:1707.04061 [cs]},
  eprint = {1707.04061},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1707.04061},
  urldate = {2022-04-08},
  abstract = {Humans modify their facial expressions in order to communicate their internal states and sometimes to mislead observers regarding their true emotional states. Evidence in experimental psychology shows that discriminative facial responses are short and subtle. This suggests that such behavior would be easier to distinguish when captured in high resolution at an increased frame rate. We are proposing SASE-FE, the first dataset of facial expressions that are either congruent or incongruent with underlying emotion states. We show that overall the problem of recognizing whether facial movements are expressions of authentic emotions or not can be successfully addressed by learning spatio-temporal representations of the data. For this purpose, we propose a method that aggregates features along fiducial trajectories in a deeply learnt space. Performance of the proposed model shows that on average it is easier to distinguish among genuine facial expressions of emotion than among unfelt facial expressions of emotion and that certain emotion pairs such as contempt and disgust are more difficult to distinguish than the rest. Furthermore, the proposed methodology improves state of the art results on CK+ and OULU-CASIA datasets for video emotion recognition, and achieves competitive results when classifying facial action units on BP4D datase.},
  archiveprefix = {arxiv},
  file = {Zotero/Kulkarni et al_2018_Automatic Recognition of Facial Displays of Unfelt Emotions.pdf;../../../../../snap/zotero-snap/common/Zotero/storage/C2T5SS3J/1707.html}
}

@article{Langner2010,
  title = {Presentation and Validation of the {{Radboud Faces Database}}},
  author = {Langner, Oliver and Dotsch, Ron and Bijlstra, Gijsbert and Wigboldus, Daniel H. J. and Hawk, Skyler T. and {van Knippenberg}, Ad},
  year = {2010},
  month = dec,
  journal = {Cognition \& Emotion},
  volume = {24},
  number = {8},
  pages = {1377--1388},
  issn = {0269-9931, 1464-0600},
  doi = {10.1080/02699930903485076},
  urldate = {2022-05-27},
  langid = {english},
  keywords = {datasets}
}

@article{LeCun2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  urldate = {2022-04-12},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {machine_learning},
  file = {Zotero/LeCun et al_2015_Deep learning.pdf;../../../../../snap/zotero-snap/common/Zotero/storage/TXS8CRVV/nature14539.html}
}

@article{Lee_ContextAwareEmotionRecognition,
  title = {Context-{{Aware Emotion Recognition Networks}}},
  author = {Lee, Jiyoung and Kim, Seungryong and Kim, Sunok and Park, Jungin and Sohn, Kwanghoon},
  pages = {10},
  abstract = {Traditional techniques for emotion recognition have focused on the facial expression analysis only, thus providing limited ability to encode context that comprehensively represents the emotional responses. We present deep networks for context-aware emotion recognition, called CAERNet, that exploit not only human facial expression but also context information in a joint and boosting manner. The key idea is to hide human faces in a visual scene and seek other contexts based on an attention mechanism. Our networks consist of two sub-networks, including two-stream encoding networks to separately extract the features of face and context regions, and adaptive fusion networks to fuse such features in an adaptive fashion. We also introduce a novel benchmark for context-aware emotion recognition, called CAER, that is more appropriate than existing benchmarks both qualitatively and quantitatively. On several benchmarks, CAER-Net proves the effect of context for emotion recognition. Our dataset is available at http://caer-dataset.github.io.},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Lee et al_Context-Aware Emotion Recognition Networks.pdf}
}

@inproceedings{Lee2019,
  title = {Context-{{Aware Emotion Recognition Networks}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Lee, Jiyoung and Kim, Seungryong and Kim, Sunok and Park, Jungin and Sohn, Kwanghoon},
  year = {2019},
  month = oct,
  pages = {10142--10151},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.01024},
  urldate = {2023-03-27},
  isbn = {978-1-72814-803-8},
  file = {Zotero/Lee et al_2019_Context-Aware Emotion Recognition Networks.pdf}
}

@inproceedings{Li2013,
  title = {A {{Spontaneous Micro-expression Database}}: {{Inducement}}, Collection and Baseline},
  shorttitle = {A {{Spontaneous Micro-expression Database}}},
  booktitle = {2013 10th {{IEEE International Conference}} and {{Workshops}} on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Li, Xiaobai and Pfister, Tomas and Huang, Xiaohua and Zhao, Guoying and Pietikainen, Matti},
  year = {2013},
  month = apr,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Shanghai, China}},
  doi = {10.1109/FG.2013.6553717},
  urldate = {2022-06-07},
  abstract = {Micro-expressions are short, involuntary facial expressions which reveal hidden emotions. Micro-expressions are important for understanding humans' deceitful behavior. Psychologists have been studying them since the 1960's. Currently the attention is elevated in both academic fields and in media. However, while general facial expression recognition (FER) has been intensively studied for years in computer vision, little research has been done in automatically analyzing microexpressions. The biggest obstacle to date has been the lack of a suitable database. In this paper we present a novel Spontaneous Micro-expression Database SMIC, which includes 164 microexpression video clips elicited from 16 participants. Microexpression detection and recognition performance are provided as baselines. SMIC provides sufficient source material for comprehensive testing of automatic systems for analyzing microexpressions, which has not been possible with any previously published database.},
  isbn = {978-1-4673-5546-9 978-1-4673-5545-2 978-1-4673-5544-5},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Li et al_2013_A Spontaneous Micro-expression Database.pdf}
}

@inproceedings{Li2017,
  title = {Reliable {{Crowdsourcing}} and {{Deep Locality-Preserving Learning}} for {{Expression Recognition}} in the {{Wild}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Li, Shan and Deng, Weihong and Du, JunPing},
  year = {2017},
  month = jul,
  pages = {2584--2593},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.277},
  urldate = {2022-06-04},
  abstract = {Past research on facial expressions have used relatively limited datasets, which makes it unclear whether current methods can be employed in real world. In this paper, we present a novel database, RAF-DB, which contains about 30000 facial images from thousands of individuals. Each image has been individually labeled about 40 times, then EM algorithm was used to filter out unreliable labels. Crowdsourcing reveals that real-world faces often express compound emotions, or even mixture ones. For all we know, RAF-DB is the first database that contains compound expressions in the wild. Our cross-database study shows that the action units of basic emotions in RAF-DB are much more diverse than, or even deviate from, those of labcontrolled ones. To address this problem, we propose a new DLP-CNN (Deep Locality-Preserving CNN) method, which aims to enhance the discriminative power of deep features by preserving the locality closeness while maximizing the inter-class scatters. The benchmark experiments on the 7class basic expressions and 11-class compound expressions, as well as the additional experiments on SFEW and CK+ databases, show that the proposed DLP-CNN outperforms the state-of-the-art handcrafted features and deep learning based methods for the expression recognition in the wild.},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {Zotero/Li et al_2017_Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression.pdf}
}

@article{Li2019,
  title = {Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained Facial Expression Recognition},
  author = {Li, Shan and Deng, Weihong},
  year = {2019},
  journal = {IEEE Transactions on Image Processing},
  volume = {28},
  number = {1},
  pages = {356--370},
  publisher = {{IEEE}},
  keywords = {datasets}
}

@article{Li2020,
  title = {Deep {{Facial Expression Recognition}}: {{A Survey}}},
  shorttitle = {Deep {{Facial Expression Recognition}}},
  author = {Li, Shan and Deng, Weihong},
  year = {2020},
  journal = {IEEE Transactions on Affective Computing},
  pages = {1--1},
  issn = {1949-3045, 2371-9850},
  doi = {10/gkk8dv},
  urldate = {2021-08-02},
  abstract = {With the transition of facial expression recognition (FER) from laboratory-controlled to challenging in-the-wild conditions and the recent success of deep learning techniques in various fields, deep neural networks have increasingly been leveraged to learn discriminative representations for automatic FER. Recent deep FER systems generally focus on two important issues: overfitting caused by a lack of sufficient training data and expression-unrelated variations, such as illumination, head pose and identity bias. In this survey, we provide a comprehensive review of deep FER, including datasets and algorithms that provide insights into these intrinsic problems. First, we introduce the available datasets that are widely used in the literature and provide accepted data selection and evaluation principles for these datasets. We then describe the standard pipeline of a deep FER system with the related background knowledge and suggestions for applicable implementations for each stage. For the state-of-the-art in deep FER, we introduce existing novel deep neural networks and related training strategies that are designed for FER based on both static images and dynamic image sequences and discuss their advantages and limitations. Competitive performances and experimental comparisons on widely used benchmarks are also summarized. We then extend our survey to additional related issues and application scenarios. Finally, we review the remaining challenges and corresponding opportunities in this field as well as future directions for the design of robust deep FER systems.},
  langid = {english},
  keywords = {review},
  file = {Zotero/Li_Deng_2020_Deep Facial Expression Recognition.pdf}
}

@article{Li2020a,
  title = {A {{Deeper Look}} at {{Facial Expression Dataset Bias}}},
  author = {Li, Shan and Deng, Weihong},
  year = {2020},
  journal = {IEEE Transactions on Affective Computing},
  eprint = {1904.11150},
  pages = {1--1},
  issn = {1949-3045, 2371-9850},
  doi = {10.1109/TAFFC.2020.2973158},
  urldate = {2022-01-14},
  abstract = {Datasets play an important role in the progress of facial expression recognition algorithms, but they may suffer from obvious biases caused by different cultures and collection conditions. To look deeper into this bias, we first conduct comprehensive experiments on dataset recognition and crossdataset generalization tasks, and for the first time explore the intrinsic causes of the dataset discrepancy. The results quantitatively verify that current datasets have a strong buildin bias and corresponding analyses indicate that the conditional probability distributions between source and target datasets are different. However, previous researches are mainly based on shallow features with limited discriminative ability under the assumption that the conditional distribution remains unchanged across domains. To address these issues, we further propose a novel deep Emotion-Conditional Adaption Network (ECAN) to learn domain-invariant and discriminative feature representations, which can match both the marginal and the conditional distributions across domains simultaneously. In addition, the largely ignored expression class distribution bias is also addressed by a learnable re-weighting parameter, so that the training and testing domains can share similar class distribution. Extensive cross-database experiments on both lab-controlled datasets (CK+, JAFFE, MMI and Oulu-CASIA) and real-world databases (AffectNet, FER2013, RAF-DB 2.0 and SFEW 2.0) demonstrate that our ECAN can yield competitive performances across various facial expression transfer tasks and outperform the state-of-theart methods.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {review},
  file = {Zotero/Li_Deng_2020_A Deeper Look at Facial Expression Dataset Bias.pdf}
}

@article{Lieberson1969,
  title = {Measuring {{Population Diversity}}},
  author = {Lieberson, Stanley},
  year = {1969},
  journal = {American Sociological Review},
  volume = {34},
  number = {6},
  eprint = {2095977},
  eprinttype = {jstor},
  pages = {850--862},
  publisher = {{[American Sociological Association, Sage Publications, Inc.]}},
  issn = {0003-1224},
  doi = {10.2307/2095977},
  urldate = {2022-03-15},
  abstract = {Viewing diversity as the position of a population along a homogeneity-heterogeneity continuum, a general method is presented for describing diversity within and between groups that are classified by one or more qualitative variables. This method has a wide range of applications, including such phenomena as attitudinal concensus, political cleavage, residential isolation, linguistic communication, cohesion, as well as the general diversity of populations. Diversity is operationally defined as the probability of obtaining unlike characteristics when two persons are randomly paired. Computation of the indexes of diversity within a population, Aw, and between two populations, Ab, is illustrated with data drawn from several substantive areas in sociology.},
  keywords = {diversity},
  file = {Zotero/Lieberson_1969_Measuring Population Diversity.pdf}
}

@inproceedings{Liu2015,
  title = {Deep Learning Face Attributes in the Wild},
  booktitle = {Proceedings of International Conference on Computer Vision ({{ICCV}})},
  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  year = {2015},
  month = dec,
  keywords = {datasets}
}

@article{Livingstone2018,
  title = {The {{Ryerson Audio-Visual Database}} of {{Emotional Speech}} and {{Song}} ({{RAVDESS}}): {{A}} Dynamic, Multimodal Set of Facial and Vocal Expressions in {{North American English}}},
  shorttitle = {The {{Ryerson Audio-Visual Database}} of {{Emotional Speech}} and {{Song}} ({{RAVDESS}})},
  author = {Livingstone, Steven R. and Russo, Frank A.},
  year = {2018},
  month = may,
  journal = {PLOS ONE},
  volume = {13},
  number = {5},
  pages = {e0196391},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0196391},
  urldate = {2022-06-07},
  abstract = {The RAVDESS is a validated multimodal database of emotional speech and song. The database is gender balanced consisting of 24 professional actors, vocalizing lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity, with an additional neutral expression. All conditions are available in face-and-voice, face-only, and voice-only formats. The set of 7356 recordings were each rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity and test-retest intrarater reliability were reported. Corrected accuracy and composite "goodness" measures are presented to assist researchers in the selection of stimuli. All recordings are made freely available under a Creative Commons license and can be downloaded at https://doi.org/10.5281/zenodo.1188976.},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Zotero/Livingstone_Russo_2018_The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS).pdf;../../../../../snap/zotero-snap/common/Zotero/storage/9T7XJZ3N/article.html}
}

@article{Lloyd_BiasAmplificationArtificial,
  title = {Bias {{Amplification}} in {{Artificial Intelligence Systems}}},
  author = {Lloyd, Kirsten and Hamilton, Booz Allen},
  pages = {4},
  abstract = {As Artificial Intelligence (AI) technologies proliferate, concern has centered around the long-term dangers of job loss or threats of machines causing harm to humans. All of this concern, however, detracts from the more pertinent and already existing threats posed by AI today: its ability to amplify bias found in training datasets, and swiftly impact marginalized populations at scale. Government and public sector institutions have a responsibility to citizens to establish a dialogue with technology developers and release thoughtful policy around data standards to ensure diverse representation in datasets to prevent bias amplification and ensure that AI systems are built with inclusion in mind.},
  langid = {english},
  keywords = {bias_example},
  file = {Zotero/Lloyd_Hamilton_Bias Amplification in Artificial Intelligence Systems.pdf}
}

@inproceedings{Lucey2010,
  title = {The {{Extended Cohn-Kanade Dataset}} ({{CK}}+): {{A}} Complete Dataset for Action Unit and Emotion-Specified Expression},
  shorttitle = {The {{Extended Cohn-Kanade Dataset}} ({{CK}}+)},
  booktitle = {2010 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} - {{Workshops}}},
  author = {Lucey, Patrick and Cohn, Jeffrey F. and Kanade, Takeo and Saragih, Jason and Ambadar, Zara and Matthews, Iain},
  year = {2010},
  month = jun,
  pages = {94--101},
  publisher = {{IEEE}},
  address = {{San Francisco, CA, USA}},
  doi = {10.1109/CVPRW.2010.5543262},
  urldate = {2021-08-03},
  abstract = {In 2000, the Cohn-Kanade (CK) database was released for the purpose of promoting research into automatically detecting individual facial expressions. Since then, the CK database has become one of the most widely used test-beds for algorithm development and evaluation. During this period, three limitations have become apparent: 1) While AU codes are well validated, emotion labels are not, as they refer to what was requested rather than what was actually performed, 2) The lack of a common performance metric against which to evaluate new algorithms, and 3) Standard protocols for common databases have not emerged. As a consequence, the CK database has been used for both AU and emotion detection (even though labels for the latter have not been validated), comparison with benchmark algorithms is missing, and use of random subsets of the original database makes meta-analyses difficult. To address these and other concerns, we present the Extended Cohn-Kanade (CK+) database. The number of sequences is increased by 22\% and the number of subjects by 27\%. The target expression for each sequence is fully FACS coded and emotion labels have been revised and validated. In addition to this, non-posed sequences for several types of smiles and their associated metadata have been added. We present baseline results using Active Appearance Models (AAMs) and a linear support vector machine (SVM) classifier using a leaveone-out subject cross-validation for both AU and emotion detection for the posed data. The emotion and AU labels, along with the extended image data and tracked landmarks will be made available July 2010.},
  isbn = {978-1-4244-7029-7},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Lucey et al_2010_The Extended Cohn-Kanade Dataset (CK+).pdf}
}

@misc{Lundquist1998,
  title = {Karolinska Directed Emotional Faces},
  author = {Lundquist, D and Flykt, A and Ohman, A},
  year = {1998}
}

@inproceedings{Lyons1998,
  title = {Coding Facial Expressions with {{Gabor}} Wavelets},
  booktitle = {Proceedings {{Third IEEE International Conference}} on {{Automatic Face}} and {{Gesture Recognition}}},
  author = {Lyons, M. and Akamatsu, S. and Kamachi, M. and Gyoba, J.},
  year = {1998},
  pages = {200--205},
  publisher = {{IEEE Comput. Soc}},
  address = {{Nara, Japan}},
  doi = {10.1109/AFGR.1998.670949},
  urldate = {2023-03-23},
  isbn = {978-0-8186-8344-2},
  file = {Zotero/Lyons et al_1998_Coding facial expressions with Gabor wavelets.pdf}
}

@article{Lyons1998arxiv,
  title = {Coding {{Facial Expressions}} with {{Gabor Wavelets}} ({{IVC Special Issue}})},
  author = {Lyons, Michael J. and Kamachi, Miyuki and Gyoba, Jiro},
  year = {1998},
  month = sep,
  journal = {arXiv:2009.05938 [cs]},
  eprint = {2009.05938},
  primaryclass = {cs},
  doi = {10.5281/zenodo.4029679},
  urldate = {2022-04-26},
  abstract = {We present a method for extracting information about facial expressions from digital images.The method codes facial expression images using a multi-orientation, multiresolution set of Gabor filters that are topographically ordered and approximately aligned with the face. A similarity space derived from this code is compared with one derived from semantic ratings of the images by human observers. Interestingly the low-dimensional structure of the image-derived similarity space shares organizational features with the circumplex model of affect, suggesting a bridge between categorical and dimensional representations of facial expression. Our results also indicate that it would be possible to construct a facial expression classifier based on a topographically-linked multi-orientation, multi-resolution Gabor coding of the facial images at the input stage. The significant degree of psychological plausibility exhibited by the proposed code may also be useful in the design of human-computer interfaces.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {fer_implementation},
  file = {Zotero/Lyons et al_1998_Coding Facial Expressions with Gabor Wavelets (IVC Special Issue).pdf}
}

@article{Lyons2021,
  title = {"{{Excavating AI}}" {{Re-excavated}}: {{Debunking}} a {{Fallacious Account}} of the {{JAFFE Dataset}}},
  shorttitle = {"{{Excavating AI}}" {{Re-excavated}}},
  author = {Lyons, Michael J.},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.13998 [cs]},
  eprint = {2107.13998},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2107.13998},
  urldate = {2022-04-26},
  abstract = {Twenty-five years ago, my colleagues Miyuki Kamachi and Jiro Gyoba and I designed and photographed JAFFE, a set of facial expression images intended for use in a study of face perception. In 2019, without seeking permission or informing us, Kate Crawford and Trevor Paglen exhibited JAFFE in two widely publicized art shows. In addition, they published a nonfactual account of the images in the essay ``Excavating AI: The Politics of Images in Machine Learning Training Sets.'' The present article recounts the creation of the JAFFE dataset and unravels each of Crawford and Paglen's fallacious statements. I also discuss JAFFE more broadly in connection with research on facial expression, affective computing, and human-computer interaction.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Lyons_2021_Excavating AI Re-excavated.pdf}
}

@inproceedings{Ma2019,
  title = {{{ElderReact}}: {{A Multimodal Dataset}} for {{Recognizing Emotional Response}} in {{Aging Adults}}},
  shorttitle = {{{ElderReact}}},
  booktitle = {2019 {{International Conference}} on {{Multimodal Interaction}}},
  author = {Ma, Kaixin and Wang, Xinyu and Yang, Xinru and Zhang, Mingtong and Girard, Jeffrey M and Morency, Louis-Philippe},
  year = {2019},
  month = oct,
  series = {{{ICMI}} '19},
  pages = {349--357},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3340555.3353747},
  urldate = {2022-04-08},
  abstract = {Automatic emotion recognition plays a critical role in technologies such as intelligent agents and social robots and is increasingly being deployed in applied settings such as education and healthcare. Most research to date has focused on recognizing the emotional expressions of young and middle-aged adults and, to a lesser extent, children and adolescents. Very few studies have examined automatic emotion recognition in older adults (i.e., elders), which represent a large and growing population worldwide. Given that aging causes many changes in facial shape and appearance and has been found to alter patterns of nonverbal behavior, there is strong reason to believe that automatic emotion recognition systems may need to be developed specifically (or augmented) for the elder population. To promote and support this type of research, we introduce a newly collected multimodal dataset of elders reacting to emotion elicitation stimuli. Specifically, it contains 1323 video clips of 46 unique individuals with human annotations of six discrete emotions: anger, disgust, fear, happiness, sadness, and surprise as well as valence. We present a detailed analysis of the most indicative features for each emotion. We also establish several baselines using unimodal and multimodal features on this dataset. Finally, we show that models trained on dataset of another age group do not generalize well on elders.},
  isbn = {978-1-4503-6860-5},
  keywords = {datasets},
  file = {Zotero/Ma et al_2019_ElderReact.pdf}
}

@article{Martinez1998,
  title = {The {{AR}} Face Database},
  author = {Martinez, A. and Benavente, Robert},
  year = {1998},
  month = jan,
  journal = {Tech. Rep. 24 CVC Technical Report},
  keywords = {datasets},
  file = {Zotero/Martinez_Benavente_1998_The AR face database.pdf}
}

@article{Mavadati2013,
  title = {{{DISFA}}: {{A Spontaneous Facial Action Intensity Database}}},
  shorttitle = {{{DISFA}}},
  author = {Mavadati, S. Mohammad and Mahoor, Mohammad H. and Bartlett, Kevin and Trinh, Philip and Cohn, Jeffrey F.},
  year = {2013},
  month = apr,
  journal = {IEEE Transactions on Affective Computing},
  volume = {4},
  number = {2},
  pages = {151--160},
  issn = {1949-3045},
  doi = {10.1109/T-AFFC.2013.4},
  urldate = {2021-08-10},
  abstract = {Access to well-labeled recordings of facial expression is critical to progress in automated facial expression recognition. With few exceptions [1], publicly available databases are limited to posed facial behavior that can differ markedly in conformation, intensity, and timing from what occurs spontaneously. To meet the need for publicly available corpora of well-labeled video, we collected, ground-truthed, and prepared for distribution the Denver Intensity of Spontaneous Facial Action (DISFA) database. Twenty-seven young adults were video-recorded by a stereo camera while they viewed video clips intended to elicit spontaneous emotion expression. Each video frame was manually coded for presence, absence, and intensity of facial action units according to the Facial Action Unit Coding System [2]. Action units are the smallest visibly discriminable changes in facial action; they may occur individually and in combinations to comprise more molar facial expressions. To provide a baseline for use in future research, protocols and benchmarks for automated action unit intensity measurement are reported. Details are given for accessing the database for research in computer vision, machine learning, and affective and behavioral science.},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Mavadati et al_2013_DISFA.pdf}
}

@inproceedings{Mavadati2016,
  title = {Extended {{DISFA Dataset}}: {{Investigating Posed}} and {{Spontaneous Facial Expressions}}},
  shorttitle = {Extended {{DISFA Dataset}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Mavadati, Mohammad and Sanger, Peyten and Mahoor, Mohammad H.},
  year = {2016},
  month = jun,
  pages = {1452--1459},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPRW.2016.182},
  urldate = {2021-08-10},
  abstract = {Automatic facial expression recognition (FER) is an important component of affect-aware technologies. Because of the lack of labeled spontaneous data, majority of existing automated FER systems were trained on posed facial expressions; however in real-world applications we deal with (subtle) spontaneous facial expression. This paper introduces an extension of DISFA, a previously released and well-accepted face dataset. Extended DISFA (DISFA+) has the following features: 1) it contains a large set of posed and spontaneous facial expressions data for a same group of individuals, 2) it provides the manually labeled framebased annotations of 5-level intensity of twelve FACS facial actions, 3) it provides meta data (i.e. facial landmark points in addition to the self-report of each individual regarding every posed facial expression). This paper introduces and employs DISFA+, to analyze and compare temporal patterns and dynamic characteristics of posed and spontaneous facial expressions.},
  isbn = {978-1-5090-1437-8},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Mavadati et al_2016_Extended DISFA Dataset.pdf}
}

@inproceedings{McDuff2013,
  title = {Affectiva-{{MIT Facial Expression Dataset}} ({{AM-FED}}): {{Naturalistic}} and {{Spontaneous Facial Expressions Collected}} \&\#x0022;{{In-the-Wild}}\&\#x0022;},
  shorttitle = {Affectiva-{{MIT Facial Expression Dataset}} ({{AM-FED}})},
  booktitle = {2013 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  author = {McDuff, Daniel and {el Kaliouby}, Rana and Senechal, Thibaud and Amr, May and Cohn, Jeffrey F. and Picard, Rosalind},
  year = {2013},
  month = jun,
  pages = {881--888},
  publisher = {{IEEE}},
  address = {{OR, USA}},
  doi = {10.1109/CVPRW.2013.130},
  urldate = {2022-04-14},
  abstract = {Computer classification of facial expressions requires large amounts of data and this data needs to reflect the diversity of conditions seen in real applications. Public datasets help accelerate the progress of research by providing researchers with a benchmark resource. We present a comprehensively labeled dataset of ecologically valid spontaneous facial responses recorded in natural settings over the Internet. To collect the data, online viewers watched one of three intentionally amusing Super Bowl commercials and were simultaneously filmed using their webcam. They answered three self-report questions about their experience. A subset of viewers additionally gave consent for their data to be shared publicly with other researchers. This subset consists of 242 facial videos (168,359 frames) recorded in real world conditions. The dataset is comprehensively labeled for the following: 1) frame-by-frame labels for the presence of 10 symmetrical FACS action units, 4 asymmetric (unilateral) FACS action units, 2 head movements, smile, general expressiveness, feature tracker fails and gender; 2) the location of 22 automatically detected landmark points; 3) self-report responses of familiarity with, liking of, and desire to watch again for the stimuli videos and 4) baseline performance of detection algorithms on this dataset. This data is available for distribution to researchers online, the EULA can be found at: http://www.affectiva.com/facialexpression-dataset-am-fed/.},
  isbn = {978-0-7695-4990-3},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/McDuff et al_2013_Affectiva-MIT Facial Expression Dataset (AM-FED).pdf}
}

@article{McKeown2012,
  title = {The {{SEMAINE Database}}: {{Annotated Multimodal Records}} of {{Emotionally Colored Conversations}} between a {{Person}} and a {{Limited Agent}}},
  shorttitle = {The {{SEMAINE Database}}},
  author = {McKeown, Gary and Valstar, Michel and Cowie, Roddy and Pantic, Maja and Schroder, Marc},
  year = {2012},
  month = jan,
  journal = {IEEE Transactions on Affective Computing},
  volume = {3},
  number = {1},
  pages = {5--17},
  issn = {1949-3045},
  doi = {10.1109/T-AFFC.2011.20},
  abstract = {SEMAINE has created a large audiovisual database as a part of an iterative approach to building Sensitive Artificial Listener (SAL) agents that can engage a person in a sustained, emotionally colored conversation. Data used to build the agents came from interactions between users and an "operator'' simulating a SAL agent, in different configurations: Solid SAL (designed so that operators displayed an appropriate nonverbal behavior) and Semi-automatic SAL (designed so that users' experience approximated interacting with a machine). We then recorded user interactions with the developed system, Automatic SAL, comparing the most communicatively competent version to versions with reduced nonverbal skills. High quality recording was provided by five high-resolution, high-framerate cameras, and four microphones, recorded synchronously. Recordings total 150 participants, for a total of 959 conversations with individual SAL characters, lasting approximately 5 minutes each. Solid SAL recordings are transcribed and extensively annotated: 6-8 raters per clip traced five affective dimensions and 27 associated categories. Other scenarios are labeled on the same pattern, but less fully. Additional information includes FACS annotation on selected extracts, identification of laughs, nods, and shakes, and measures of user engagement with the automatic system. The material is available through a web-accessible database.},
  keywords = {datasets},
  file = {Zotero/McKeown et al_2012_The SEMAINE Database.pdf;../../../../../snap/zotero-snap/common/Zotero/storage/EMFV2TLG/5959155.html}
}

@article{Mehrabi2021,
  title = {A {{Survey}} on {{Bias}} and {{Fairness}} in {{Machine Learning}}},
  author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  year = {2021},
  month = jul,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {6},
  pages = {1--35},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3457607},
  urldate = {2022-02-16},
  abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
  langid = {english},
  keywords = {bias_example,fairness},
  file = {Zotero/Mehrabi et al_2021_A Survey on Bias and Fairness in Machine Learning.pdf}
}

@article{Mehrabian1996,
  title = {Pleasure-Arousal-Dominance: {{A}} General Framework for Describing and Measuring Individual Differences in {{Temperament}}},
  shorttitle = {Pleasure-Arousal-Dominance},
  author = {Mehrabian, Albert},
  year = {1996},
  month = dec,
  journal = {Current Psychology},
  volume = {14},
  number = {4},
  pages = {261--292},
  issn = {0737-8262, 1936-4733},
  doi = {10.1007/BF02686918},
  urldate = {2021-09-13},
  langid = {english},
  keywords = {emotions}
}

@article{Mehta2022,
  title = {A {{Review}} of {{Deep Learning}} Models for {{Facial Emotion Detection}}},
  author = {Mehta, Deepshikha and Barhate, Shweta},
  year = {2022},
  month = apr,
  pages = {7},
  abstract = {In the recent years facial emotion recognition has been majorly a very powerful topic among researchers, as it has an impactful contribution in effective Human Computer Interaction. Facial emotion recognition is part of affective computing that enables computers to understand human emotions and respond accordingly. This paper provides a brief review of Deep learning models that can be used to enhance the limitations of facial emotion recognition issues. The focus is on up-to-date deep learning approaches such as Convolutional Neural Network, Recurrent Neural Network, Transfer Learning and Generative Adversarial Network. The purpose of this paper is to assist and guide researchers by providing insights and future directions in terms of enhancing this discipline.},
  langid = {english},
  keywords = {fer_implementation,review},
  file = {Zotero/Mehta_Barhate_2022_A Review of Deep Learning models for Facial Emotion Detection.pdf}
}

@article{Miconi2017,
  title = {The Impossibility of "Fairness": A Generalized Impossibility Result for Decisions},
  shorttitle = {The Impossibility of "Fairness"},
  author = {Miconi, Thomas},
  year = {2017},
  month = sep,
  journal = {arXiv:1707.01195 [cs, stat]},
  eprint = {1707.01195},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1707.01195},
  urldate = {2021-11-08},
  abstract = {Various measures can be used to estimate bias or unfairness in a predictor. Previous work has already established that some of these measures are incompatible with each other. Here we show that, when groups differ in prevalence of the predicted event, several intuitive, reasonable measures of fairness (probability of positive prediction given occurrence or nonoccurrence; probability of occurrence given prediction or non-prediction; and ratio of predictions over occurrences for each group) are all mutually exclusive: if one of them is equal among groups, the other two must differ. The only exceptions are for perfect, or trivial (always-positive or always-negative) predictors. As a consequence, any non-perfect, non-trivial predictor must necessarily be ``unfair'' under two out of three reasonable sets of criteria. This result readily generalizes to a wide range of well-known statistical quantities (sensitivity, specificity, false positive rate, precision, etc.), all of which can be divided into three mutually exclusive groups. Importantly, The results applies to all predictors, whether algorithmic or human. We conclude with possible ways to handle this effect when assessing and designing prediction methods.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {fairness},
  file = {Zotero/Miconi_2017_The impossibility of fairness.pdf}
}

@inproceedings{Mitchell2019,
  title = {Model {{Cards}} for {{Model Reporting}}},
  booktitle = {Proceedings of the {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  year = {2019},
  month = jan,
  series = {{{FAT}}* '19},
  pages = {220--229},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3287560.3287596},
  urldate = {2022-02-15},
  abstract = {Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.},
  isbn = {978-1-4503-6125-5},
  keywords = {fairness,fairness_evaluation},
  file = {Zotero/Mitchell et al_2019_Model Cards for Model Reporting.pdf}
}

@article{Mitchell2021,
  title = {Algorithmic {{Fairness}}: {{Choices}}, {{Assumptions}}, and {{Definitions}}},
  shorttitle = {Algorithmic {{Fairness}}},
  author = {Mitchell, Shira and Potash, Eric and Barocas, Solon and D'Amour, Alexander and Lum, Kristian},
  year = {2021},
  month = mar,
  journal = {Annual Review of Statistics and Its Application},
  volume = {8},
  number = {1},
  pages = {141--163},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-042720-125902},
  urldate = {2023-01-27},
  abstract = {A recent wave of research has attempted to define fairness quantitatively. In particular, this work has explored what fairness might mean in the context of decisions based on the predictions of statistical and machine learning models. The rapid growth of this new field has led to wildly inconsistent motivations, terminology, and notation, presenting a serious challenge for cataloging and comparing definitions. This article attempts to bring much-needed order. First, we explicate the various choices and assumptions made\textemdash often implicitly\textemdash to justify the use of prediction-based decision-making. Next, we show how such choices and assumptions can raise fairness concerns and we present a notationally consistent catalog of fairness definitions from the literature. In doing so, we offer a concise reference for thinking through the choices, assumptions, and fairness considerations of prediction-based decision-making.},
  langid = {english},
  keywords = {fairness},
  file = {Zotero/Mitchell et al_2021_Algorithmic Fairness.pdf}
}

@article{Mollahosseini2019,
  title = {{{AffectNet}}: {{A Database}} for {{Facial Expression}}, {{Valence}}, and {{Arousal Computing}} in the {{Wild}}},
  shorttitle = {{{AffectNet}}},
  author = {Mollahosseini, Ali and Hasani, Behzad and Mahoor, Mohammad H.},
  year = {2019},
  month = jan,
  journal = {IEEE Transactions on Affective Computing},
  volume = {10},
  number = {1},
  eprint = {1708.03985},
  pages = {18--31},
  issn = {1949-3045, 2371-9850},
  doi = {10.1109/TAFFC.2017.2740923},
  urldate = {2021-11-03},
  abstract = {Automated affective computing in the wild setting is a challenging problem in computer vision. Existing annotated databases of facial expressions in the wild are small and mostly cover discrete emotions (aka the categorical model). There are very limited annotated facial databases for affective computing in the continuous dimensional model (e.g., valence and arousal). To meet this need, we collected, annotated, and prepared for public distribution a new database of facial emotions in the wild (called AffectNet). AffectNet contains more than 1,000,000 facial images from the Internet by querying three major search engines using 1250 emotion related keywords in six different languages. About half of the retrieved images were manually annotated for the presence of seven discrete facial expressions and the intensity of valence and arousal. AffectNet is by far the largest database of facial expression, valence, and arousal in the wild enabling research in automated facial expression recognition in two different emotion models. Two baseline deep neural networks are used to classify images in the categorical model and predict the intensity of valence and arousal. Various evaluation metrics show that our deep neural network baselines can perform better than conventional machine learning methods and off-the-shelf facial expression recognition systems.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Mollahosseini et al_2019_AffectNet.pdf}
}

@article{Nimmagadda2022,
  title = {Emotion Recognition Models for Companion Robots},
  author = {Nimmagadda, Ritvik and Arora, Kritika and Martin, Miguel Vargas},
  year = {2022},
  month = mar,
  journal = {The Journal of Supercomputing},
  issn = {1573-0484},
  doi = {10.1007/s11227-022-04416-4},
  urldate = {2022-04-01},
  abstract = {There has been a steep increase in the use of machine learning for various healthcare applications like the diagnosis of diseases, drug discovery, medical image analysis, etc. Machine learning solutions are proven to be more efficient and less time-consuming than conventional approaches. In this paper, we leverage the advantages of machine learning models to enable a humanoid robot to assist mental health patients. Facial expression and human voice are some of the most demonstrative ways to analyze human emotions, especially for the mentally challenged. We carry out this assistance by constantly monitoring the patient's voice (or audio) and facial expressions to predict human emotions. To implement the model of audio monitoring, we train three different machine learning and deep learning models to compare and choose the better model. Similarly, for facial recognition, we train a deep learning model using a specific dataset to predict facial expressions from the video captured in real time. We then integrate the better-performing machine learning models into a web interface for demonstration.},
  langid = {english},
  keywords = {fer_applications},
  file = {Zotero/Nimmagadda et al_2022_Emotion recognition models for companion robots.pdf}
}

@inproceedings{Nojavanasghari2016,
  title = {{{EmoReact}}: A Multimodal Approach and Dataset for Recognizing Emotional Responses in Children},
  shorttitle = {{{EmoReact}}},
  booktitle = {Proceedings of the 18th {{ACM International Conference}} on {{Multimodal Interaction}}},
  author = {Nojavanasghari, Behnaz and Baltru{\v s}aitis, Tadas and Hughes, Charles E. and Morency, Louis-Philippe},
  year = {2016},
  month = oct,
  series = {{{ICMI}} '16},
  pages = {137--144},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2993148.2993168},
  urldate = {2022-04-08},
  abstract = {Automatic emotion recognition plays a central role in the technologies underlying social robots, affect-sensitive human computer interaction design and affect-aware tutors. Although there has been a considerable amount of research on automatic emotion recognition in adults, emotion recognition in children has been understudied. This problem is more challenging as children tend to fidget and move around more than adults, leading to more self-occlusions and non-frontal head poses. Also, the lack of publicly available datasets for children with annotated emotion labels leads most researchers to focus on adults. In this paper, we introduce a newly collected multimodal emotion dataset of children between the ages of four and fourteen years old. The dataset contains 1102 audio-visual clips annotated for 17 different emotional states: six basic emotions, neutral, valence and nine complex emotions including curiosity, uncertainty and frustration. Our experiments compare unimodal and multimodal emotion recognition baseline models to enable future research on this topic. Finally, we present a detailed analysis of the most indicative behavioral cues for emotion recognition in children.},
  isbn = {978-1-4503-4556-9},
  keywords = {datasets},
  file = {Zotero/Nojavanasghari et al_2016_EmoReact.pdf}
}

@article{Ntoutsi2020,
  title = {Bias in Data-driven Artificial Intelligence Systems\textemdash{{An}} Introductory Survey},
  author = {Ntoutsi, Eirini and Fafalios, Pavlos and Gadiraju, Ujwal and Iosifidis, Vasileios and Nejdl, Wolfgang and Vidal, Maria-Esther and Ruggieri, Salvatore and Turini, Franco and Papadopoulos, Symeon and Krasanakis, Emmanouil and Kompatsiaris, Ioannis and Kinder-Kurlanda, Katharina and Wagner, Claudia and Karimi, Fariba and Fernandez, Miriam and Alani, Harith and Berendt, Bettina and Kruegel, Tina and Heinze, Christian and Broelemann, Klaus and Kasneci, Gjergji and Tiropanis, Thanassis and Staab, Steffen},
  year = {2020},
  month = may,
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {10},
  number = {3},
  issn = {1942-4787, 1942-4795},
  doi = {10.1002/widm.1356},
  urldate = {2023-03-23},
  langid = {english},
  file = {Zotero/Ntoutsi et al_2020_Bias in data‐driven artificial intelligence systems—An introductory survey.pdf}
}

@article{Ntoutsi2020arxiv,
  title = {Bias in {{Data-driven AI Systems}} -- {{An Introductory Survey}}},
  author = {Ntoutsi, Eirini and Fafalios, Pavlos and Gadiraju, Ujwal and Iosifidis, Vasileios and Nejdl, Wolfgang and Vidal, Maria-Esther and Ruggieri, Salvatore and Turini, Franco and Papadopoulos, Symeon and Krasanakis, Emmanouil and Kompatsiaris, Ioannis and {Kinder-Kurlanda}, Katharina and Wagner, Claudia and Karimi, Fariba and Fernandez, Miriam and Alani, Harith and Berendt, Bettina and Kruegel, Tina and Heinze, Christian and Broelemann, Klaus and Kasneci, Gjergji and Tiropanis, Thanassis and Staab, Steffen},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.09762 [cs]},
  eprint = {2001.09762},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2001.09762},
  urldate = {2021-08-27},
  abstract = {AI-based systems are widely employed nowadays to make decisions that have far-reaching impacts on individuals and society. Their decisions might affect everyone, everywhere and anytime, entailing concerns about potential human rights issues. Therefore, it is necessary to move beyond traditional AI algorithms optimized for predictive performance and embed ethical and legal principles in their design, training and deployment to ensure social good while still benefiting from the huge potential of the AI technology. The goal of this survey is to provide a broad multi-disciplinary overview of the area of bias in AI systems, focusing on technical challenges and solutions as well as to suggest new research directions towards approaches well-grounded in a legal frame. In this survey, we focus on data-driven AI, as a large part of AI is powered nowadays by (big) data and powerful Machine Learning (ML) algorithms. If otherwise not specified, we use the general term bias to describe problems related to the gathering or processing of data that might result in prejudiced decisions on the bases of demographic features like race, sex, etc.},
  archiveprefix = {arxiv},
  keywords = {fairness,review},
  file = {Zotero/Ntoutsi et al_2020_Bias in Data-driven AI Systems -- An Introductory Survey.pdf}
}

@article{Oliver2020,
  title = {{{UIBVFED}}: {{Virtual}} Facial Expression Dataset},
  shorttitle = {{{UIBVFED}}},
  author = {Oliver, Miquel Mascar{\'o} and Alcover, Esperan{\c c}a Amengual},
  year = {2020},
  month = apr,
  journal = {PLOS ONE},
  volume = {15},
  number = {4},
  pages = {e0231266},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0231266},
  urldate = {2021-11-22},
  abstract = {Facial expression classification requires large amounts of data to reflect the diversity of conditions in the real world. Public databases support research tasks providing researchers an appropriate work framework. However, often these databases do not focus on artistic creation. We developed an innovative facial expression dataset that can help both artists and researchers in the field of affective computing. This dataset can be managed interactively by an intuitive and easy to use software application. The dataset is composed of 640 facial images from 20 virtual characters each creating 32 facial expressions. The avatars represent 10 men and 10 women, aged between 20 and 80, from different ethnicities. Expressions are classified by the six universal expressions according to Gary Faigin classification.},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Oliver_Alcover_2020_UIBVFED.pdf}
}

@article{Olszanowski2015,
  title = {Warsaw Set of Emotional Facial Expression Pictures: A Validation Study of Facial Display Photographs},
  shorttitle = {Warsaw Set of Emotional Facial Expression Pictures},
  author = {Olszanowski, Michal and Pochwatko, Grzegorz and Kuklinski, Krzysztof and {Scibor-Rylski}, Michal and Lewinski, Peter and Ohme, Rafal K.},
  year = {2015},
  month = jan,
  journal = {Frontiers in Psychology},
  volume = {5},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2014.01516},
  urldate = {2021-11-09},
  abstract = {Emotional facial expressions play a critical role in theories of emotion and figure prominently in research on almost every aspect of emotion. This article provides a background for a new database of basic emotional expressions. The goal in creating this set was to provide high quality photographs of genuine facial expressions. Thus, after proper training, participants were inclined to express ``felt'' emotions. The novel approach taken in this study was also used to establish whether a given expression was perceived as intended by untrained judges. The judgment task for perceivers was designed to be sensitive to subtle changes in meaning caused by the way an emotional display was evoked and expressed. Consequently, this allowed us to measure the purity and intensity of emotional displays, which are parameters that validation methods used by other researchers do not capture. The final set is comprised of those pictures that received the highest recognition marks (e.g., accuracy with intended display) from independent judges, totaling 210 high quality photographs of 30 individuals. Descriptions of the accuracy, intensity, and purity of displayed emotion as well as FACS AU's codes are provided for each picture. Given the unique methodology applied to gathering and validating this set of pictures, it may be a useful tool for research using face stimuli. The Warsaw Set of Emotional Facial Expression Pictures (WSEFEP) is freely accessible to the scientific community for non-commercial use by request at http://www.emotional-face.org.},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Olszanowski et al_2015_Warsaw set of emotional facial expression pictures.pdf}
}

@article{Ortigosa-Hernandez2017,
  title = {Measuring the Class-Imbalance Extent of Multi-Class Problems},
  author = {{Ortigosa-Hern{\'a}ndez}, Jonathan and Inza, I{\~n}aki and Lozano, Jose A.},
  year = {2017},
  month = oct,
  journal = {Pattern Recognition Letters},
  volume = {98},
  pages = {32--38},
  issn = {01678655},
  doi = {10.1016/j.patrec.2017.08.002},
  urldate = {2022-08-29},
  abstract = {Since many important real-world classification problems involve learning from unbalanced data, the challenging class-imbalance problem has lately received considerable attention in the community. Most of the methodological contributions proposed in the literature carry out a set of experiments over a battery of specific datasets. In these cases, in order to be able to draw meaningful conclusions from the experiments, authors often measure the class-imbalance extent of each tested dataset using imbalance-ratio, i.e. dividing the frequencies of the majority class by the minority class.},
  langid = {english},
  keywords = {class_imbalance},
  file = {Zotero/Ortigosa-Hernández et al_2017_Measuring the class-imbalance extent of multi-class problems.pdf}
}

@article{Pabba2021,
  title = {An Intelligent System for Monitoring Students' Engagement in Large Classroom Teaching through Facial Expression Recognition},
  author = {Pabba, Chakradhar and Kumar, Praveen},
  year = {2021},
  month = oct,
  journal = {Expert Systems},
  issn = {0266-4720, 1468-0394},
  doi = {10.1111/exsy.12839},
  urldate = {2021-10-19},
  langid = {english},
  keywords = {fer_applications}
}

@inproceedings{Pantic2005,
  title = {Web-{{Based Database}} for {{Facial Expression Analysis}}},
  booktitle = {2005 {{IEEE International Conference}} on {{Multimedia}} and {{Expo}}},
  author = {Pantic, M. and Valstar, M. and Rademaker, R. and Maat, L.},
  year = {2005},
  pages = {317--321},
  publisher = {{IEEE}},
  address = {{Amsterdam, The Netherlands}},
  doi = {10.1109/ICME.2005.1521424},
  urldate = {2021-08-10},
  abstract = {In the last decade, the research topic of automatic analysis of facial expressions has become a central topic in machine vision research. Nonetheless, there is a glaring lack of a comprehensive, readily accessible reference set of face images that could be used as a basis for benchmarks for efforts in the field. This lack of easily accessible, suitable, commonn testing resource forms the major impediment to comparing and extending the issues concerned with automatic facial expression analysis. In this paper, we discuss a nuumber of issues that make the problem of creating a benchmark facial expression database difficult. We then present the MMI Facial Expression Database, which includes more than 1500 samples of both static images and image sequences of faces in frontal and in profile view displaying various expressions of emotion, single and imultiple facial imnuscle activation. It has been built as a web-based direct-manipulation application, allowing easy access and easy search of the available images. This database represents the most comprehensive reference set of images for studies on facial expression analysis to date.},
  isbn = {978-0-7803-9331-8},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Pantic et al_2005_Web-Based Database for Facial Expression Analysis.pdf}
}

@article{Park2021,
  title = {Comparing {{Facial Expression Recognition}} in {{Humans}} and {{Machines}}: {{Using CAM}}, {{GradCAM}}, and {{Extremal Perturbation}}},
  shorttitle = {Comparing {{Facial Expression Recognition}} in {{Humans}} and {{Machines}}},
  author = {Park, Serin and Wallraven, Christian},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.04481 [cs]},
  eprint = {2110.04481},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2110.04481},
  urldate = {2021-10-18},
  abstract = {Facial expression recognition (FER) is a topic attracting significant research in both psychology and machine learning with a wide range of applications. Despite a wealth of research on human FER and considerable progress in computational FER made possible by deep neural networks (DNNs), comparatively less work has been done on comparing the degree to which DNNs may be comparable to human performance. In this work, we compared the recognition performance and attention patterns of humans and machines during a two-alternative forced-choice FER task. Human attention was here gathered through click data that progressively uncovered a face, whereas model attention was obtained using three different popular techniques from explainable AI: CAM, GradCAM and Extremal Perturbation. In both cases, performance was gathered as percent correct. For this task, we found that humans outperformed machines quite significantly. In terms of attention patterns, we found that Extremal Perturbation had the best overall fit with the human attention map during the task.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {methods,review},
  file = {Zotero/Park_Wallraven_2021_Comparing Facial Expression Recognition in Humans and Machines.pdf}
}

@article{Perrett2022,
  title = {Representations of Facial Expressions since {{Darwin}}},
  author = {Perrett, David},
  year = {2022},
  journal = {Evolutionary Human Sciences},
  volume = {4},
  pages = {e22},
  issn = {2513-843X},
  doi = {10.1017/ehs.2022.10},
  urldate = {2022-06-10},
  abstract = {Darwin's book on expressions of emotion was one of the first publications to include photographs (Darwin, The expression of the emotions in Man and animals, 1872). The inclusion of expression photographs meant that readers could form their own opinions and could, like Darwin, survey others for their interpretations. As such, the images provided an evidence base and an `open source'. Since Darwin, increases in the representativeness and realism of emotional expressions have come from the use of composite images, colour, multiple views and dynamic displays. Research on understanding emotional expressions has been aided by the use of computer graphics to interpolate parametrically between different expressions and to extrapolate exaggerations. This review tracks the developments in how emotions are illustrated and studied and considers where to go next.},
  langid = {english},
  keywords = {datasets,review},
  file = {Zotero/Perrett_2022_Representations of facial expressions since Darwin.pdf}
}

@article{Pessach2020,
  title = {Algorithmic {{Fairness}}},
  author = {Pessach, Dana and Shmueli, Erez},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.09784 [cs, stat]},
  eprint = {2001.09784},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2001.09784},
  urldate = {2022-01-31},
  abstract = {An increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence (AI) algorithms in spheres ranging from healthcare, transportation, and education to college admissions, recruitment, provision of loans and many more realms. Since they now touch on many aspects of our lives, it is crucial to develop AI algorithms that are not only accurate but also objective and fair. Recent studies have shown that algorithmic decision-making may be inherently prone to unfairness, even when there is no intention for it. This paper presents an overview of the main concepts of identifying, measuring and improving algorithmic fairness when using AI algorithms. The paper begins by discussing the causes of algorithmic bias and unfairness and the common definitions and measures for fairness. Fairness-enhancing mechanisms are then reviewed and divided into pre-process, in-process and post-process mechanisms. A comprehensive comparison of the mechanisms is then conducted, towards a better understanding of which mechanisms should be used in different scenarios. The paper then describes the most commonly used fairnessrelated datasets in this field. Finally, the paper ends by reviewing several emerging research sub-fields of algorithmic fairness.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {fairness},
  file = {Zotero/Pessach_Shmueli_2020_Algorithmic Fairness.pdf}
}

@article{Pielou1966,
  title = {The Measurement of Diversity in Different Types of Biological Collections},
  author = {Pielou, E.C.},
  year = {1966},
  month = dec,
  journal = {Journal of Theoretical Biology},
  volume = {13},
  pages = {131--144},
  issn = {00225193},
  doi = {10.1016/0022-5193(66)90013-0},
  urldate = {2022-12-04},
  langid = {english}
}

@article{Poyiadzi2021,
  title = {Domain {{Generalisation}} for {{Apparent Emotional Facial Expression Recognition}} across {{Age-Groups}}},
  author = {Poyiadzi, Rafael and Shen, Jie and Petridis, Stavros and Wang, Yujiang and Pantic, Maja},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.09168 [cs]},
  eprint = {2110.09168},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2110.09168},
  urldate = {2021-10-26},
  abstract = {Apparent emotional facial expression recognition has attracted a lot of research attention recently. However, the majority of approaches ignore age differences and train a generic model for all ages. In this work, we study the effect of using different age-groups for training apparent emotional facial expression recognition models. To this end, we study Domain Generalisation in the context of apparent emotional facial expression recognition from facial imagery across different age groups. We first compare several domain generalisation algorithms on the basis of out-of-domain-generalisation, and observe that the Class-Conditional Domain-Adversarial Neural Networks (CDANN) algorithm has the best performance. We then study the effect of variety and number of age-groups used during training on generalisation to unseen age-groups and observe that an increase in the number of training age-groups tends to increase the apparent emotional facial expression recognition performance on unseen age-groups. We also show that exclusion of an age-group during training tends to affect more the performance of the neighbouring age groups.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {age,bias_example,psychology},
  file = {Zotero/Poyiadzi et al_2021_Domain Generalisation for Apparent Emotional Facial Expression Recognition.pdf}
}

@inproceedings{Prabhu2020,
  title = {Large Image Datasets: {{A}} Pyrrhic Win for Computer Vision?},
  shorttitle = {Large Image Datasets},
  booktitle = {2021 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Birhane, Abeba and Prabhu, Vinay Uday},
  year = {2021},
  month = jan,
  pages = {1536--1546},
  publisher = {{IEEE}},
  address = {{Waikoloa, HI, USA}},
  doi = {10.1109/WACV48630.2021.00158},
  urldate = {2023-03-23},
  isbn = {978-1-66540-477-8},
  file = {Zotero/Birhane_Prabhu_2021_Large image datasets.pdf}
}

@article{Prabhu2020arxiv,
  title = {Large Image Datasets: {{A}} Pyrrhic Win for Computer Vision?},
  shorttitle = {Large Image Datasets},
  author = {Prabhu, Vinay Uday and Birhane, Abeba},
  year = {2020},
  month = jul,
  journal = {arXiv:2006.16923 [cs, stat]},
  eprint = {2006.16923},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.16923},
  urldate = {2021-09-28},
  abstract = {In this paper we investigate problematic practices and consequences of large scale vision datasets. We examine broad issues such as the question of consent and justice as well as specific concerns such as the inclusion of verifiably pornographic images in datasets. Taking the ImageNet-ILSVRC-2012 dataset as an example, we perform a cross-sectional model-based quantitative census covering factors such as age, gender, NSFW content scoring, class-wise accuracy, human-cardinality-analysis, and the semanticity of the image class information in order to statistically investigate the extent and subtleties of ethical transgressions. We then use the census to help hand-curate a look-up-table of images in the ImageNet-ILSVRC-2012 dataset that fall into the categories of verifiably pornographic: shot in a non-consensual setting (up-skirt), beach voyeuristic, and exposed private parts. We survey the landscape of harm and threats both society broadly and individuals face due to uncritical and ill-considered dataset curation practices. We then propose possible courses of correction and critique the pros and cons of these. We have duly open-sourced all of the code and the census meta-datasets generated in this endeavor for the computer vision community to build on. By unveiling the severity of the threats, our hope is to motivate the constitution of mandatory Institutional Review Boards (IRB) for large scale dataset curation processes.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {datasets,fairness,review},
  file = {Zotero/Prabhu_Birhane_2020_Large image datasets.pdf}
}

@article{Qi2018,
  title = {Are {{Rich People Perceived}} as {{More Trustworthy}}? {{Perceived Socioeconomic Status Modulates Judgments}} of {{Trustworthiness}} and {{Trust Behavior Based}} on {{Facial Appearance}}},
  shorttitle = {Are {{Rich People Perceived}} as {{More Trustworthy}}?},
  author = {Qi, Yue and Li, Qi and Du, Feng},
  year = {2018},
  journal = {Frontiers in Psychology},
  volume = {9},
  issn = {1664-1078},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00512},
  urldate = {2022-08-12},
  abstract = {In the era of globalization, people meet strangers from different countries more often than ever. Previous research indicates that impressions of trustworthiness based on facial appearance play an important role in interpersonal cooperation behaviors. The current study examined whether additional information about socioeconomic status (SES), including national prosperity and individual monthly income, affects facial judgments and appearance-based trust decisions. Besides reproducing previous conclusions that trustworthy faces receive more money than untrustworthy faces, the present study showed that high-income individuals were judged as more trustworthy than low-income individuals, and also were given more money in a trust game. However, trust behaviors were not modulated by the nationality of the faces. The present research suggests that people are more likely to trust strangers with a high income, compared with individuals with a low income.},
  keywords = {bias_against_the_poor},
  file = {Zotero/Qi et al_2018_Are Rich People Perceived as More Trustworthy.pdf}
}

@inproceedings{Raghavendra2016,
  title = {Impact of {{Drug Abuse}} on {{Face Recognition Systems}}: {{A Preliminary Study}}},
  shorttitle = {Impact of {{Drug Abuse}} on {{Face Recognition Systems}}},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Security}} of {{Information}} and {{Networks}}},
  author = {Raghavendra, R. and Raja, Kiran B. and Busch, Christoph},
  year = {2016},
  month = jul,
  pages = {24--27},
  publisher = {{ACM}},
  address = {{Newark NJ USA}},
  doi = {10.1145/2947626.2947644},
  urldate = {2022-08-12},
  abstract = {Drug abuse leads to high degree of the change in the facial structure of a person due to multiple factors which include loss of fat in face, change of facial structure due to changes in facial muscles and change of skin texture due to appearance of acnes. The combination of such effects lead to completely deformed face which presents a complex challenge to face based authentication systems. To study the impact of the drug-abuse on face recognition systems, in this work, we create a new face database collected before and after the drug-abuse. The newly collected database which is referred as Drug Abuse Database (DAD) consists of images obtained from 101 subjects. Further, the collected database is analysed using various state-of-art face recognition algorithms along with a widely used commercial-off-the-shelf (COTS) system. The obtained performance of GM R = 12.90\% at F M R = 0.01\% indicates the challenge presented by such face images and underlines the importance of newer algorithms to handle such challenge.},
  isbn = {978-1-4503-4764-8},
  langid = {english},
  keywords = {bias_against_the_poor,datasets,drugs},
  file = {Zotero/Raghavendra et al_2016_Impact of Drug Abuse on Face Recognition Systems.pdf}
}

@misc{RecommendationIB,
  title = {Recommendation {{ITU-R Bt}}.601-7},
  author = {{International Telecommunication Union}},
  year = {2011},
  url = {https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.601-7-201103-I!!PDF-E.pdf},
  urldate = {2021-12-20},
  file = {Zotero/International Telecommunication Union_2011_Recommendation ITU-R Bt.pdf}
}

@article{Robinson1951,
  title = {A {{Method}} for {{Chronologically Ordering Archaeological Deposits}}},
  author = {Robinson, W. S.},
  year = {1951},
  month = apr,
  journal = {American Antiquity},
  volume = {16},
  number = {4},
  pages = {293--301},
  issn = {0002-7316, 2325-5064},
  doi = {10.2307/276978},
  urldate = {2022-12-04},
  abstract = {The statistical technique of this paper is based upon the empirically established fact that over the course of time pottery types come into and go out of general use by a given group of people. It is further based upon the established fact that in cultures where chronology has been determined the differential use of types takes on a form illustrated in Figure 89. The data of this diagram are hypothetical, the purpose being merely to illustrate the present discussion.},
  langid = {english}
}

@article{Robinson2021,
  title = {Balancing {{Biases}} and {{Preserving Privacy}} on {{Balanced Faces}} in the {{Wild}}},
  author = {Robinson, Joseph P. and Qin, Can and Henon, Yann and Timoner, Samson and Fu, Yun},
  year = {2021},
  month = jun,
  journal = {arXiv:2103.09118 [cs]},
  eprint = {2103.09118},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2103.09118},
  urldate = {2022-04-06},
  abstract = {There are demographic biases in current models used for facial recognition (FR). Our Balanced Faces In the Wild (BFW) dataset serves as a proxy to measure bias across ethnicity and gender subgroups, allowing one to characterize FR performances per subgroup. We show performances are non-optimal when a single score threshold is used to determine whether sample pairs are genuine or imposter. Across subgroups, performance ratings vary from the reported across the entire dataset. Thus, claims of specific error rates only hold true for populations matching that of the validation data. We mitigate the imbalanced performances using a novel domain adaptation learning scheme on the facial features extracted using state-of-the-art. Not only does this technique balance performance, but it also boosts the overall performance. A benefit of the proposed is to preserve identity information in facial features while removing demographic knowledge in the lower dimensional features. The removal of demographic knowledge prevents future potential biases from being injected into decision-making. This removal satisfies privacy concerns. We explore why this works qualitatively; we also show quantitatively that subgroup classifiers can no longer learn from the features mapped by the proposed.},
  archiveprefix = {arxiv},
  keywords = {bias_example,facial_recognition},
  file = {Zotero/Robinson et al_2021_Balancing Biases and Preserving Privacy on Balanced Faces in the Wild.pdf;../../../../../snap/zotero-snap/common/Zotero/storage/CIBJDHEV/2103.html}
}

@inproceedings{Rocha2022,
  title = {Towards {{Enhancing}} the~{{Multimodal Interaction}} of~a~{{Social Robot}} to~{{Assist Children}} with~{{Autism}} in~{{Emotion Regulation}}},
  booktitle = {Pervasive {{Computing Technologies}} for {{Healthcare}}},
  author = {Rocha, Marcelo and Valentim, Pedro and Barreto, F{\'a}bio and Mitjans, Adrian and {Cruz-Sandoval}, Dagoberto and Favela, Jesus and {Muchaluat-Saade}, D{\'e}bora},
  editor = {Lewy, Hadas and Barkan, Refael},
  year = {2022},
  pages = {398--415},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-99194-4\_25},
  abstract = {Assistive robots are expected to become ubiquitous by transforming everyday life and are expected to be widely used in healthcare therapies. SARs (Socially Assistive Robots) are a class of robots that are at an intersection between the class of assistive robots and that of interactive social robots. SARs are being explored to assist in the diagnosis and treatment of children with ASD (Autism Spectrum Disorder). A SAR called EVA has been used to assist non-pharmacological interventions based on verbal, non-verbal communication and social interaction. The EVA robot can currently speak, listen and express emotions through looking. Towards offering immersive therapies for autistic children, this work enhances EVA's capabilities to recognize user emotions through facial expression recognition and also to create light sensory effects in order to make the therapy more attractive to users. A therapy session was developed through a serious game where the child should recognize the robot's emotions. During the game, EVA recognizes the child's facial expression to check his/her learning progress. We invited a neurotypical 6-year-old child to play the game, with the consent of her parents, and recorded videos of the game session. Those videos were evaluated by 48 expert physicians and psychologists in therapies for ASD using the Technology Acceptance Model (TAM). They considered our work useful and agreed it would help them doing their job more effectively.},
  isbn = {978-3-030-99194-4},
  langid = {english},
  keywords = {fer_applications}
}

@article{Rothe2018,
  title = {Deep {{Expectation}} of {{Real}} and {{Apparent Age}} from a {{Single Image Without Facial Landmarks}}},
  author = {Rothe, Rasmus and Timofte, Radu and Van Gool, Luc},
  year = {2018},
  month = apr,
  journal = {International Journal of Computer Vision},
  volume = {126},
  number = {2},
  pages = {144--157},
  issn = {1573-1405},
  doi = {10.1007/s11263-016-0940-3},
  urldate = {2021-10-26},
  abstract = {In this paper we propose a deep learning solution to age estimation from a single face image without the use of facial landmarks and introduce the IMDB-WIKI dataset, the largest public dataset of face images with age and gender labels. If the real age estimation research spans over decades, the study of apparent age estimation or the age as perceived by other humans from a face image is a recent endeavor. We tackle both tasks with our convolutional neural networks (CNNs) of VGG-16 architecture which are pre-trained on ImageNet for image classification. We pose the age estimation problem as a deep classification problem followed by a softmax expected value refinement. The key factors of our solution are: deep learned models from large data, robust face alignment, and expected value formulation for age regression. We validate our methods on standard benchmarks and achieve state-of-the-art results for both real and apparent age estimation.},
  langid = {english},
  keywords = {age,psychology},
  file = {Zotero/Rothe et al_2018_Deep Expectation of Real and Apparent Age from a Single Image Without Facial.pdf}
}

@article{Sakoda1977,
  title = {Measures of Association for Multivariate Contingency Tables},
  author = {Sakoda, James M},
  year = {1977},
  journal = {Proceedings of the Social Statistics Section of the American Statistical Association (Part III)},
  pages = {777--780},
  langid = {english},
  file = {Zotero/Sakoda_1977_Measures of association for multivariate contingency tables.pdf}
}

@article{SampaioEmmanuelV.B.2022,
  title = {{{EmotionIMX}} 1 - {{Are Facial Expression Recognition Algorithms Reliable}} in the {{Context}} of {{Interactive Media}}? {{A New Metric}} to {{Analyse Their Performance}}},
  shorttitle = {{{EmotionIMX}} 1 - {{Are Facial Expression Recognition Algorithms Reliable}} in the {{Context}} of {{Interactive Media}}?},
  author = {Sampaio, Emmanuel V. B. and L{\'e}v{\^e}que, Lucie and Da Silva, Matthieu Perreira and Le Callet, Patrick},
  year = {2022},
  pages = {6009950 Bytes},
  publisher = {{figshare}},
  doi = {10.6084/M9.FIGSHARE.20069357.V1},
  urldate = {2022-08-18},
  abstract = {Emotions, and consequently facial expressions, play an essential role in communication - and thus in everyday life. With the increase of human-machine interactions, and more especially of multimedia applications, automatic recognition of facial expressions has emerged as a challenging task, particularly under naturalistic conditions. In the present work, a benchmark is firstly conducted using four open source deep learning solutions on four labeled image datasets. Thanks to an exhaustive analysis based on two distinct, yet complementary approaches, we show how the four models performed depending on the studied emotions. Furthermore, we present a novel metric based on the Euclidean distance between two given emotions (i.e., ground truth and predicted) to better measure the performance of said models in the context of interactive media, where human sensibility needs to be taken into consideration.},
  copyright = {Creative Commons Attribution 4.0 International},
  langid = {english},
  keywords = {fer_implementation},
  file = {Zotero/Sampaio, Emmanuel V. B. et al_2022_EmotionIMX 1 - Are Facial Expression Recognition Algorithms Reliable in the.pdf}
}

@article{Sariyanidi2015,
  title = {Automatic {{Analysis}} of {{Facial Affect}}: {{A Survey}} of {{Registration}}, {{Representation}}, and {{Recognition}}},
  shorttitle = {Automatic {{Analysis}} of {{Facial Affect}}},
  author = {Sariyanidi, Evangelos and Gunes, Hatice and Cavallaro, Andrea},
  year = {2015},
  month = jun,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {37},
  number = {6},
  pages = {1113--1133},
  issn = {0162-8828, 2160-9292},
  doi = {10/f7b3ck},
  urldate = {2021-08-02},
  abstract = {Automatic affect analysis has attracted great interest in various contexts including the recognition of action units and basic or non-basic emotions. In spite of major efforts, there are several open questions on what the important cues to interpret facial expressions are and how to encode them. In this paper, we review the progress across a range of affect recognition applications to shed light on these fundamental questions. We analyse the state-of-the-art solutions by decomposing their pipelines into fundamental components, namely face registration, representation, dimensionality reduction and recognition. We discuss the role of these components and highlight the models and new trends that are followed in their design. Moreover, we provide a comprehensive analysis of facial representations by uncovering their advantages and limitations; we elaborate on the type of information they encode and discuss how they deal with the key challenges of illumination variations, registration errors, head-pose variations, occlusions, and identity bias. This survey allows us to identify open issues and to define future directions for designing real-world affect recognition systems.},
  langid = {english},
  keywords = {review},
  file = {Zotero/Sariyanidi et al_2015_Automatic Analysis of Facial Affect.pdf}
}

@article{Scherer2005,
  title = {What Are Emotions? {{And}} How Can They Be Measured?},
  shorttitle = {What Are Emotions?},
  author = {Scherer, Klaus R.},
  year = {2005},
  month = dec,
  journal = {Social Science Information},
  volume = {44},
  number = {4},
  pages = {695--729},
  issn = {0539-0184, 1461-7412},
  doi = {10.1177/0539018405058216},
  urldate = {2021-09-13},
  abstract = {Defining ``emotion'' is a notorious problem. Without consensual conceptualization and operationalization of exactly what phenomenon is to be studied, progress in theory and research is difficult to achieve and fruitless debates are likely to proliferate. A particularly unfortunate example is William James's asking the question ``What is an emotion?'' when he really meant ``feeling'', a misnomer that started a debate which is still ongoing, more than a century later. This contribution attempts to sensitize researchers in the social and behavioral sciences to the importance of definitional issues and their consequences for distinguishing related but fundamentally different affective processes, states, and traits. Links between scientific and folk concepts of emotion are explored and ways to measure emotion and its components are discussed.},
  langid = {english},
  keywords = {emotions}
}

@techreport{Schwartz2022,
  title = {Towards a {{Standard}} for {{Identifying}} and {{Managing Bias}} in {{Artificial Intelligence}}},
  author = {Schwartz, Reva and Vassilev, Apostol and Greene, Kristen and Perine, Lori and Burt, Andrew and Hall, Patrick},
  year = {2022},
  month = mar,
  institution = {{National Institute of Standards and Technology}},
  doi = {10.6028/NIST.SP.1270},
  urldate = {2022-03-31},
  langid = {english},
  keywords = {fairness,review},
  file = {Zotero/Schwartz et al_2022_Towards a Standard for Identifying and Managing Bias in Artificial Intelligence.pdf}
}

@inproceedings{Sechidis2011,
  title = {On the {{Stratification}} of {{Multi-label Data}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Sechidis, Konstantinos and Tsoumakas, Grigorios and Vlahavas, Ioannis},
  editor = {Gunopulos, Dimitrios and Hofmann, Thomas and Malerba, Donato and Vazirgiannis, Michalis},
  year = {2011},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {145--158},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-23808-6\_10},
  abstract = {Stratified sampling is a sampling method that takes into account the existence of disjoint groups within a population and produces samples where the proportion of these groups is maintained. In single-label classification tasks, groups are differentiated based on the value of the target variable. In multi-label learning tasks, however, where there are multiple target variables, it is not clear how stratified sampling could/should be performed. This paper investigates stratification in the multi-label data context. It considers two stratification methods for multi-label data and empirically compares them along with random sampling on a number of datasets and based on a number of evaluation criteria. The results reveal some interesting conclusions with respect to the utility of each method for particular types of multi-label datasets.},
  isbn = {978-3-642-23808-6},
  langid = {english},
  keywords = {methods,stratification},
  file = {Zotero/Sechidis et al_2011_On the Stratification of Multi-label Data.pdf}
}

@inproceedings{Seuss2019,
  title = {Emotion {{Expression}} from {{Different Angles}}: {{A Video Database}} for {{Facial Expressions}} of {{Actors Shot}} by a {{Camera Array}}},
  shorttitle = {Emotion {{Expression}} from {{Different Angles}}},
  booktitle = {2019 8th {{International Conference}} on {{Affective Computing}} and {{Intelligent Interaction}} ({{ACII}})},
  author = {Seuss, Dominik and Dieckmann, Anja and Hassan, Teena and Garbas, Jens-Uwe and Ellgring, Johann Heinrich and Mortillaro, Marcello and Scherer, Klaus},
  year = {2019},
  month = sep,
  pages = {35--41},
  issn = {2156-8111},
  doi = {10.1109/ACII.2019.8925458},
  abstract = {Over the last few decades, there has been an increasing call in the field of computer vision to use machine-learning techniques for the detection, categorization, and indexing of facial behaviors, as well as for the recognition of emotion phenomena. Automated Facial Expression Analysis has become a highly attractive field of competition for academic laboratories, startups and large technology corporations. This paper introduces the new Actor Study Database to address the resulting need for reliable benchmark datasets. The focus of the database is to provide real multi-view data, that is not synthesized through perspective distortion. The database contains 68-minutes of high-quality videos of facial expressions performed by 21 actors. The videos are synchronously recorded from five different angles. The actors' tasks ranged from displaying specific Action Units and their combinations at different intensities to enactment of a variety of emotion scenarios. Over 1.5 million frames have been annotated and validated with the Facial Action Coding System by certified FACS coders. These attributes make the Actor Study Database particularly applicable in machine recognition studies as well as in psychological research into affective phenomena-whether prototypical basic emotions or subtle emotional responses. Two state-of-the-art systems were used to produce benchmark results for all five different views that this new database encompasses. The database is publicly available for non-commercial research.},
  keywords = {datasets},
  file = {Zotero/Seuss et al_2019_Emotion Expression from Different Angles.pdf;../../../../../snap/zotero-snap/common/Zotero/storage/SYU2CNQ5/8925458.html}
}

@article{Sham2022,
  title = {Ethical {{AI}} in Facial Expression Analysis: Racial Bias},
  shorttitle = {Ethical {{AI}} in Facial Expression Analysis},
  author = {Sham, Abdallah Hussein and Aktas, Kadir and Rizhinashvili, Davit and Kuklianov, Danila and Alisinanoglu, Fatih and Ofodile, Ikechukwu and Ozcinar, Cagri and Anbarjafari, Gholamreza},
  year = {2022},
  month = may,
  journal = {Signal, Image and Video Processing},
  issn = {1863-1703, 1863-1711},
  doi = {10.1007/s11760-022-02246-8},
  urldate = {2022-05-16},
  abstract = {Facial expression recognition using deep neural networks has become very popular due to their successful performances. However, the datasets used during the development and testing of these methods lack a balanced distribution of races among the sample images. This leaves a possibility of the methods being biased toward certain races. Therefore, a concern about fairness arises, and the lack of research aimed at investigating racial bias only increases the concern. On the other hand, such bias in the method would decrease the real-world performance due to the wrong generalization. For these reasons, in this study, we investigated the racial bias within popular state-of-the-art facial expression recognition methods such as Deep Emotion, Self-Cure Network, ResNet50, InceptionV3, and DenseNet121. We compiled an elaborated dataset with images of different races, cross-checked the bias for methods trained, and tested on images of people of other races. We observed that the methods are inclined towards the races included in the training data. Moreover, an increase in the performance increases the bias as well if the training dataset is imbalanced. Some methods can make up for the bias if enough variance is provided in the training set. However, this does not mitigate the bias completely. Our findings suggest that an unbiased performance can be obtained by adding the missing races into the training data equally.},
  langid = {english},
  keywords = {bias_example,fairness_evaluation},
  file = {Zotero/Sham et al_2022_Ethical AI in facial expression analysis.pdf}
}

@article{Shan2009,
  title = {Facial Expression Recognition Based on {{Local Binary Patterns}}: {{A}} Comprehensive Study},
  shorttitle = {Facial Expression Recognition Based on {{Local Binary Patterns}}},
  author = {Shan, Caifeng and Gong, Shaogang and McOwan, Peter W.},
  year = {2009},
  month = may,
  journal = {Image and Vision Computing},
  volume = {27},
  number = {6},
  pages = {803--816},
  issn = {0262-8856},
  doi = {10.1016/j.imavis.2008.08.005},
  urldate = {2022-03-28},
  abstract = {Automatic facial expression analysis is an interesting and challenging problem, and impacts important applications in many areas such as human\textendash computer interaction and data-driven animation. Deriving an effective facial representation from original face images is a vital step for successful facial expression recognition. In this paper, we empirically evaluate facial representation based on statistical local features, Local Binary Patterns, for person-independent facial expression recognition. Different machine learning methods are systematically examined on several databases. Extensive experiments illustrate that LBP features are effective and efficient for facial expression recognition. We further formulate Boosted-LBP to extract the most discriminant LBP features, and the best recognition performance is obtained by using Support Vector Machine classifiers with Boosted-LBP features. Moreover, we investigate LBP features for low-resolution facial expression recognition, which is a critical problem but seldom addressed in the existing work. We observe in our experiments that LBP features perform stably and robustly over a useful range of low resolutions of face images, and yield promising performance in compressed low-resolution video sequences captured in real-world environments.},
  langid = {english},
  keywords = {fer_implementation},
  file = {Zotero/Shan et al_2009_Facial expression recognition based on Local Binary Patterns.pdf}
}

@book{Shannon1949,
  title = {The Mathematical Theory of Communication.},
  author = {Shannon, Claude E. and Weaver, Warren},
  year = {1949},
  series = {The Mathematical Theory of Communication.},
  pages = {vi, 117},
  publisher = {{University of Illinois Press}},
  address = {{Champaign,  IL,  US}},
  abstract = {In the second part of this volume Weaver suggests that there are 3 levels of problem in general communication. The first is technical, i.e., "How accurately can the symbols of communication be transmitted?" Second, the semantic problem, i.e., "How precisely do the transmitted symbols convey the desired meaning?" Third, the effectiveness problem, i.e., "How effectively does the received meaning affect conduct in the desired way?" The first level is essentially an engineering one and in the first part of the book Shannon develops a mathematical theory of communication. Weaver discusses the significance of this theory to the other two levels. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}
}

@article{Shawon2021,
  title = {On Age Prediction from Facial Images in Presence of Facial Expressions},
  author = {Shawon, Md Shahedul Haque and Biswas, Sagor and Arefin, Nirob and Pias, Tanmoy Sarkar and Rahman, Ashikur},
  year = {2021},
  month = nov,
  journal = {International Journal of Applied Pattern Recognition},
  publisher = {{Inderscience Publishers (IEL)}},
  url = {https://www.inderscienceonline.com/doi/abs/10.1504/IJAPR.2021.118918},
  urldate = {2022-01-06},
  abstract = {Predicting age and gender from facial images is a fundamental research problem that has many applications in major research areas. The state-of-the-art online APIs can predict age and gender but their accuracy degrades when emotions are present. In this paper, we present feature-extraction based machine learning models that can predict ages with acceptable accuracy in presence of facial-expressions. After identifying 68 facial landmarks, different distances and ratios (that changes with age and expressions) are selected to predict the age that can overcome the impact of emotions with reasonable accuracy. The experimental results show that while neutralising the effect of emotion, the proposed models can perform better on female images compared to the male image set. And images with disgust and contempt expressions deviate most during prediction. In contrast, predicted age is more accurate for angry expressions. Also for different ethnic groups, the predicted age deviates differently from the actual age.},
  copyright = {Copyright \textcopyright{} 2021 Inderscience Enterprises Ltd.},
  langid = {english},
  keywords = {age,psychology},
  file = {../../../../../snap/zotero-snap/common/Zotero/storage/59PBUNPS/IJAPR.2021.html}
}

@article{Siddiqui2022,
  title = {A {{Survey}} on {{Databases}} for {{Multimodal Emotion Recognition}} and an {{Introduction}} to the {{VIRI}} ({{Visible}} and {{InfraRed Image}}) {{Database}}},
  author = {Siddiqui, Mohammad Faridul Haque and Dhakal, Parashar and Yang, Xiaoli and Javaid, Ahmad Y.},
  year = {2022},
  month = jun,
  journal = {Multimodal Technologies and Interaction},
  volume = {6},
  number = {6},
  pages = {47},
  issn = {2414-4088},
  doi = {10.3390/mti6060047},
  urldate = {2022-08-08},
  abstract = {Multimodal human\textendash computer interaction (HCI) systems pledge a more human\textendash human-like interaction between machines and humans. Their prowess in emanating an unambiguous information exchange between the two makes these systems more reliable, efficient, less error prone, and capable of solving complex tasks. Emotion recognition is a realm of HCI that follows multimodality to achieve accurate and natural results. The prodigious use of affective identification in e-learning, marketing, security, health sciences, etc., has increased demand for high-precision emotion recognition systems. Machine learning (ML) is getting its feet wet to ameliorate the process by tweaking the architectures or wielding high-quality databases (DB). This paper presents a survey of such DBs that are being used to develop multimodal emotion recognition (MER) systems. The survey illustrates the DBs that contain multi-channel data, such as facial expressions, speech, physiological signals, body movements, gestures, and lexical features. Few unimodal DBs are also discussed that work in conjunction with other DBs for affect recognition. Further, VIRI, a new DB of visible and infrared (IR) images of subjects expressing five emotions in an uncontrolled, real-world environment, is presented. A rationale for the superiority of the presented corpus over the existing ones is instituted.},
  langid = {english},
  file = {Zotero/Siddiqui et al_2022_A Survey on Databases for Multimodal Emotion Recognition and an Introduction to.pdf}
}

@article{Simonetta2021,
  title = {Metrics for {{Identifying Bias}} in {{Datasets}}},
  author = {Simonetta, Alessandro and Trenta, Andrea and Paoletti, Maria Cristina and Vetr{\`o}, Antonio},
  year = {2021},
  pages = {8},
  abstract = {Nowadays automated decision-making systems are pervasively used and more often, they are used for taking important decisions in sensitive areas such as the granting of a bank overdraft, the susceptibility of an individual to a virus infection, or even the likelihood of repeating a crime. The widespread use of these systems raises a growing ethical concern about the risk of a potential discriminatory impact. In particular, machine-learning systems trained on unbalanced data could rise to systematic discriminations in the real world. One of the most important challenges is to determine metrics capable of detecting when an unbalanced training dataset may lead to discriminatory behaviour of the model built on it. In this paper, we propose an approach based on the notion of data completeness using two different metrics: one based on the combinations of the values of the dataset, which will be our benchmark, and the second using frame theory, widely used among others for quality measures of control systems. It is important to remark that the use of metrics cannot be a substitute for a broader design that must take into account the columns that could lead to the presence of bias in the data. The line of research does not end with these activities but aims to continue the path towards a standardised register of measures.},
  langid = {english},
  keywords = {check,fairness_evaluation},
  file = {Zotero/Simonetta et al_Metrics for Identifying Bias in Datasets.pdf}
}

@misc{Simonyan2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2015},
  month = apr,
  number = {arXiv:1409.1556},
  eprint = {arXiv:1409.1556},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1409.1556},
  urldate = {2022-05-19},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 \texttimes{} 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16\textendash 19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {methods},
  file = {Zotero/Simonyan_Zisserman_2015_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf}
}

@article{Simpson1949,
  title = {Measurement of {{Diversity}}},
  author = {Simpson, E. H.},
  year = {1949},
  month = apr,
  journal = {Nature},
  volume = {163},
  number = {4148},
  pages = {688--688},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/163688a0},
  urldate = {2022-12-04},
  langid = {english},
  file = {Zotero/Simpson_1949_Measurement of Diversity.pdf}
}

@inproceedings{Singh2020,
  title = {Indian {{Semi-Acted Facial Expression}} ({{iSAFE}}) {{Dataset}} for {{Human Emotions Recognition}}},
  booktitle = {Advances in {{Signal Processing}} and {{Intelligent Recognition Systems}}},
  author = {Singh, Shivendra and Benedict, Shajulin},
  editor = {Thampi, Sabu M. and Hegde, Rajesh M. and Krishnan, Sri and Mukhopadhyay, Jayanta and Chaudhary, Vipin and Marques, Oge and Piramuthu, Selwyn and Corchado, Juan M.},
  year = {2020},
  pages = {150--162},
  publisher = {{Springer}},
  address = {{Singapore}},
  doi = {10.1007/978-981-15-4828-4\_13},
  abstract = {Human emotion recognition is an imperative step to handle human computer interactions. It supports several machine learning based applications, including IoT cloud societal applications such as smart driving or smart living applications or medical applications. In fact, the dataset relating to human emotions remains as a crucial pre-requisite for designing efficient machine learning algorithms or applications. The traditionally available datasets are not specific to the Indian context, which lead to an arduous task for designing efficient region-specific applications. In this paper, we propose a new dataset that reveals the human emotions that are specific to India. The proposed dataset was developed at the IoT Cloud Research Laboratory of IIIT-Kottayam \textendash{} the dataset contains 395 clips of 44 volunteers between 17 to 22 years of age; face expressions were captured when volunteers were asked to watch a few stimulant videos; the facial expressions were self annotated by the volunteers and they were cross annotated by annotators. In addition, the developed dataset was analyzed using ResNet34 neural network and the baseline of the dataset was provided for future research and developments in the human computer interaction domain.},
  isbn = {9789811548284},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Singh_Benedict_2020_Indian Semi-Acted Facial Expression (iSAFE) Dataset for Human Emotions.pdf}
}

@article{Smith2018,
  title = {A Disciplined Approach to Neural Network Hyper-Parameters: {{Part}} 1 -- Learning Rate, Batch Size, Momentum, and Weight Decay},
  shorttitle = {A Disciplined Approach to Neural Network Hyper-Parameters},
  author = {Smith, Leslie N.},
  year = {2018},
  month = apr,
  journal = {arXiv:1803.09820 [cs, stat]},
  eprint = {1803.09820},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1803.09820},
  urldate = {2022-01-31},
  abstract = {Although deep learning has produced dazzling successes for applications of image, speech, and video processing in the past few years, most trainings are with suboptimal hyper-parameters, requiring unnecessarily long training times. Setting the hyper-parameters remains a black art that requires years of experience to acquire. This report proposes several efficient ways to set the hyper-parameters that significantly reduce training time and improves performance. Specifically, this report shows how to examine the training validation/test loss function for subtle clues of underfitting and overfitting and suggests guidelines for moving toward the optimal balance point. Then it discusses how to increase/decrease the learning rate/momentum to speed up training. Our experiments show that it is crucial to balance every manner of regularization for each dataset and architecture. Weight decay is used as a sample regularizer to show how its optimal value is tightly coupled with the learning rates and momentums. Files to help replicate the results reported here are available.},
  archiveprefix = {arxiv},
  keywords = {machine_learning},
  file = {Zotero/Smith_2018_A disciplined approach to neural network hyper-parameters.pdf}
}

@article{Sneddon2012,
  title = {The {{Belfast Induced Natural Emotion Database}}},
  author = {Sneddon, I. and McRorie, M. and McKeown, G. and Hanratty, J.},
  year = {2012},
  month = jan,
  journal = {IEEE Transactions on Affective Computing},
  volume = {3},
  number = {1},
  pages = {32--41},
  issn = {1949-3045},
  doi = {10.1109/T-AFFC.2011.26},
  urldate = {2022-04-14},
  abstract = {For many years psychological research on facial expression of emotion has relied heavily on a recognition paradigm based on posed static photographs. There is growing evidence that there may be fundamental differences between the expressions depicted in such stimuli and the emotional expressions present in everyday life. Affective computing, with its pragmatic emphasis on realism, needs examples of natural emotion. This paper describes a unique database containing recordings of mild to moderate emotionally coloured responses to a series of laboratory based emotion induction tasks. The recordings are accompanied by information on self-report of emotion and intensity, continuous trace-style ratings of valence and intensity, the sex of the participant, the sex of the experimenter, the active or passive nature of the induction task and it gives researchers the opportunity to compare expressions from people from more than one culture.},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Sneddon et al_2012_The Belfast Induced Natural Emotion Database.pdf}
}

@misc{Snow2018,
  title = {Amazon's {{Face Recognition Falsely Matched}} 28 {{Members}} of {{Congress With Mugshots}}},
  author = {Snow, Jacob},
  year = {2018},
  month = jul,
  journal = {American Civil Liberties Union},
  url = {https://www.aclu.org/blog/privacy-technology/surveillance-technologies/amazons-face-recognition-falsely-matched-28},
  urldate = {2022-03-15},
  abstract = {Amazon's face surveillance technology is the target of growing opposition nationwide, and today, there are 28 more causes for concern. In a test the ACLU recently conducted of the facial recognition tool, called ``Rekognition,'' the software incorrectly matched 28 members of Congress, identifying them as other people who have been arrested for a crime. The members of Congress},
  langid = {english},
  file = {../../../../../snap/zotero-snap/common/Zotero/storage/B5SRQAET/amazons-face-recognition-falsely-matched-28.html}
}

@article{Sonmez2021,
  title = {{{EMRES}}: {{A}} New {{EMotional RESpondent}} Robot},
  shorttitle = {{{EMRES}}},
  author = {Sonmez, Elena Battini and Han, Hasan and Karadeniz, Oguzcan and Dalyan, Tugba and Sarioglu, Baykal},
  year = {2021},
  journal = {IEEE Transactions on Cognitive and Developmental Systems},
  pages = {1--1},
  issn = {2379-8920, 2379-8939},
  doi = {10.1109/TCDS.2021.3120562},
  urldate = {2021-10-19},
  keywords = {fer_applications}
}

@article{Stahl2018,
  title = {Ethics and {{Privacy}} in {{AI}} and {{Big Data}}: {{Implementing Responsible Research}} and {{Innovation}}},
  shorttitle = {Ethics and {{Privacy}} in {{AI}} and {{Big Data}}},
  author = {Stahl, Bernd Carsten and Wright, David},
  year = {2018},
  month = may,
  journal = {IEEE Security \& Privacy},
  volume = {16},
  number = {3},
  pages = {26--33},
  issn = {1540-7993, 1558-4046},
  doi = {10.1109/MSP.2018.2701164},
  urldate = {2023-01-27},
  keywords = {ethics,privacy}
}

@article{Stoppel_InvestigatingBiasFacial,
  title = {Investigating {{Bias}} in {{Facial Expression Recognition}}},
  author = {Stoppel, Stefanie and Frintrop, Simone},
  pages = {2},
  langid = {english},
  keywords = {bias_example},
  file = {Zotero/Stoppel_Frintrop_Investigating Bias in Facial Expression Recognition.pdf}
}

@article{Stratton2022,
  title = {Bridging the {{Gap Between Automated}} and {{Human Facial Emotion Perception}}},
  author = {Stratton, Derek and Hand, Emily},
  year = {2022},
  pages = {11},
  abstract = {Understanding the complex relationship between emotions and facial expressions is important for both psychologists and computer scientists. A large body of research in psychology investigates facial expressions, emotions, and how emotions are perceived from facial expressions. As computer scientists look to incorporate this research into automatic emotion perception systems, it is important to understand the nature and limitations of human emotion perception. These principles of emotion science affect the way datasets are created, methods are implemented, and results are interpreted in automated emotion perception. This paper aims to distill and align prior work in automated and human facial emotion perception to facilitate future discussions and research at the intersection of the two disciplines.},
  langid = {english},
  keywords = {review},
  file = {Zotero/Stratton_Hand_2022_Bridging the Gap Between Automated and Human Facial Emotion Perception.pdf}
}

@article{Surabhi2022,
  title = {{{TikTok}} for {{Good}}: {{Creating}} a {{Diverse Emotion Expression Database}}},
  author = {Surabhi, Saimourya and Shah, Bhavik and Washington, Peter and Mutlu, Onur Cezmi and Leblanc, Emilie and Mohite, Prathamesh and Husic, Arman and Kline, Aaron and Dunlap, Kaitlyn and McNealis, Maya and Liu, Bennett and Deveaux, Nick and Sleiman, Essam and Wall, Dennis P},
  year = {2022},
  pages = {11},
  abstract = {Facial expression recognition (FER) is a critical computer vision task for a variety of applications. Despite the widespread use of FER, there is a dearth of racially diverse facial emotion datasets which are enriched for children, teens, and adults. To bridge this gap, we have built a diverse expression recognition database using publicly available videos from TikTok, a video-focused social networking service. We describe the construction of the TikTok Facial expression recognition (FER) database. The dataset is extracted from 6428 videos scraped from TikTok. The videos consist of 9392 distinct individuals and labels for 15 emotionrelated prompts. We were able to achieve a F1 score 0.78 for Ekman emotions on expression classification using transfer learning. We hope that the scale and diversity of the TikTokFER dataset will be of use to affective computing practitioners.},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Surabhi et al_2022_TikTok for Good.pdf}
}

@inproceedings{Suresh2021,
  title = {A {{Framework}} for {{Understanding Sources}} of {{Harm}} throughout the {{Machine Learning Life Cycle}}},
  booktitle = {Equity and {{Access}} in {{Algorithms}}, {{Mechanisms}}, and {{Optimization}}},
  author = {Suresh, Harini and Guttag, John},
  year = {2021},
  month = oct,
  pages = {1--9},
  publisher = {{ACM}},
  address = {{-- NY USA}},
  doi = {10.1145/3465416.3483305},
  urldate = {2023-03-23},
  isbn = {978-1-4503-8553-4},
  langid = {english},
  file = {Zotero/Suresh_Guttag_2021_A Framework for Understanding Sources of Harm throughout the Machine Learning2.pdf}
}

@article{Suresh2021arxiv,
  title = {A {{Framework}} for {{Understanding Sources}} of {{Harm}} throughout the {{Machine Learning Life Cycle}}},
  author = {Suresh, Harini and Guttag, John V.},
  year = {2021},
  month = jun,
  journal = {arXiv:1901.10002 [cs, stat]},
  eprint = {1901.10002},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1901.10002},
  urldate = {2021-08-27},
  abstract = {As machine learning (ML) increasingly affects people and society, awareness of its potential unwanted consequences has also grown. To anticipate, prevent, and mitigate undesirable downstream consequences, it is critical that we understand when and how harm might be introduced throughout the ML life cycle. In this paper, we provide a framework that identifies seven distinct potential sources of downstream harm in machine learning, spanning data collection, development, and deployment. In doing so, we aim to facilitate more productive and precise communication around these issues, as well as more direct, application-grounded ways to mitigate them.},
  archiveprefix = {arxiv},
  keywords = {fairness},
  file = {Zotero/Suresh_Guttag_2021_A Framework for Understanding Sources of Harm throughout the Machine Learning.pdf}
}

@techreport{TheGenIUSSGroup2014,
  title = {Best {{Practices}} for {{Asking Questions}} to {{Identify Transgender}} and {{Other Gender Minority Respondents}} on {{Population-Based Surveys}}},
  author = {{The GenIUSS Group}},
  year = {2014},
  address = {{Los Angeles, CA}},
  institution = {{The Williams Institute}},
  file = {Zotero/The GenIUSS Group_2014_Best Practices for Asking Questions to Identify Transgender and Other Gender.pdf}
}

@article{Theil1970,
  title = {On the {{Estimation}} of {{Relationships Involving Qualitative Variables}}},
  author = {Theil, Henri},
  year = {1970},
  journal = {American Journal of Sociology},
  volume = {76},
  number = {1},
  eprint = {2775440},
  eprinttype = {jstor},
  pages = {103--154},
  publisher = {{University of Chicago Press}},
  issn = {00029602, 15375390},
  url = {http://www.jstor.org/stable/2775440},
  urldate = {2022-12-04},
  abstract = {This article is concerned with the specification and estimation of relationships whose dependent variable is qualitative in nature (such as "yes" or "no"). It discusses logit equations with and without interaction, and the estimation procedure is generalized least squares. Part I deals with dependent variables that take only two values, Par II with variables taking more than two values, and part III describes informational measures for the explanatory power of the determining factors. The discussion of more advanced technical matters is contained in various appendixes.}
}

@article{Thomas2020,
  title = {The {{Problem}} with {{Metrics}} Is a {{Fundamental Problem}} for {{AI}}},
  author = {Thomas, Rachel and Uminsky, David},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.08512 [cs]},
  eprint = {2002.08512},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2002.08512},
  urldate = {2022-04-07},
  abstract = {Optimizing a given metric is a central aspect of most current AI approaches, yet overemphasizing metrics leads to manipulation, gaming, a myopic focus on short-term goals, and other unexpected negative consequences. This poses a fundamental contradiction for AI development. Through a series of real-world case studies, we look at various aspects of where metrics go wrong in practice and aspects of how our online environment and current business practices are exacerbating these failures. Finally, we propose a framework towards mitigating the harms caused by overemphasis of metrics within AI by: (1) using a slate of metrics to get a fuller and more nuanced picture, (2) combining metrics with qualitative accounts, and (3) involving a range of stakeholders, including those who will be most impacted.},
  archiveprefix = {arxiv},
  keywords = {fairness,fairness_evaluation},
  file = {Zotero/Thomas_Uminsky_2020_The Problem with Metrics is a Fundamental Problem for AI.pdf}
}

@article{Tottenham2009,
  title = {The {{NimStim}} Set of Facial Expressions: {{Judgments}} from Untrained Research Participants},
  shorttitle = {The {{NimStim}} Set of Facial Expressions},
  author = {Tottenham, Nim and Tanaka, James W. and Leon, Andrew C. and McCarry, Thomas and Nurse, Marcella and Hare, Todd A. and Marcus, David J. and Westerlund, Alissa and Casey, Bj and Nelson, Charles},
  year = {2009},
  month = aug,
  journal = {Psychiatry Research},
  volume = {168},
  number = {3},
  pages = {242--249},
  issn = {01651781},
  doi = {10.1016/j.psychres.2008.05.006},
  urldate = {2022-06-07},
  abstract = {A set of face stimuli called the NimStim Set of Facial Expressions is described. The goal in creating this set was to provide facial expressions that untrained individuals, characteristic of research participants, would recognize. This set is large in number, multiracial, and available to the scientific community online. The results of psychometric evaluations of these stimuli are presented. The results lend empirical support for the validity and reliability of this set of facial expressions as determined by accurate identification of expressions and high intra-participant agreement across two testing sessions, respectively.},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Tottenham et al_2009_The NimStim set of facial expressions.pdf}
}

@book{Tschuprow1939,
  title = {Principles of the Mathematical Theory of Correlation},
  author = {Tschuprow, Alexander Alexandrovich},
  translator = {Kantorowitsch, Myron},
  year = {1939},
  publisher = {{W. Hodge and Co.}},
  address = {{New York, NY, USA}}
}

@misc{UNGeneralAssembly2015,
  title = {Transforming Our World : The 2030 {{Agenda}} for {{Sustainable Development}}, {{A}}/{{RES}}/70/1},
  author = {{UN General Assembly}},
  year = {2015},
  month = oct,
  url = {https://www.refworld.org/docid/57b6e3e44.html}
}

@article{Valstar2010,
  title = {Induced {{Disgust}}, {{Happiness}} and {{Surprise}}: An {{Addition}} to the {{MMI Facial Expression Database}}},
  author = {Valstar, Michel F and Pantic, Maja},
  year = {2010},
  pages = {6},
  abstract = {We have acquired a set of audio-visual recordings of induced emotions. A collage of comedy clips and clips of disgusting content were shown to a number of participants, who displayed mostly expressions of disgust, happiness, and surprise in response. While displays of induced emotions may differ from those shown in everyday life in aspects such as the frequency with which they occur, they are regarded as highly naturalistic and spontaneous. We recorded 25 participants for approximately 5 minutes each. This collection of recordings has been added to the MMI Facial Expression Database, an online accessible, easily searchable resource that is freely available to the scientific community.},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Valstar_Pantic_2010_Induced Disgust, Happiness and Surprise.pdf}
}

@article{vanderSchalk2011,
  title = {Moving Faces, Looking Places: {{Validation}} of the {{Amsterdam Dynamic Facial Expression Set}} ({{ADFES}})},
  shorttitle = {Moving Faces, Looking Places},
  author = {{van der Schalk}, Job and Hawk, Skyler T. and Fischer, Agneta H. and Doosje, Bertjan},
  year = {2011},
  journal = {Emotion},
  volume = {11},
  number = {4},
  pages = {907--920},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1931-1516},
  doi = {10.1037/a0023853},
  abstract = {We report two studies validating a new standardized set of filmed emotion expressions, the Amsterdam Dynamic Facial Expression Set (ADFES). The ADFES is distinct from existing datasets in that it includes a face-forward version and two different head-turning versions (faces turning toward and away from viewers), North-European as well as Mediterranean models (male and female), and nine discrete emotions (joy, anger, fear, sadness, surprise, disgust, contempt, pride, and embarrassment). Study 1 showed that the ADFES received excellent recognition scores. Recognition was affected by social categorization of the model: displays of North-European models were better recognized by Dutch participants, suggesting an ingroup advantage. Head-turning did not affect recognition accuracy. Study 2 showed that participants more strongly perceived themselves to be the cause of the other's emotion when the model's face turned toward the respondents. The ADFES provides new avenues for research on emotion expression and is available for researchers upon request. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {datasets},
  file = {../../../../../snap/zotero-snap/common/Zotero/storage/B8P3AR2H/2011-18271-006.html}
}

@inproceedings{Verma2018,
  title = {Fairness Definitions Explained},
  booktitle = {Proceedings of the {{International Workshop}} on {{Software Fairness}}},
  author = {Verma, Sahil and Rubin, Julia},
  year = {2018},
  month = may,
  pages = {1--7},
  publisher = {{ACM}},
  address = {{Gothenburg Sweden}},
  doi = {10.1145/3194770.3194776},
  urldate = {2022-02-16},
  abstract = {Algorithm fairness has started to attract the attention of researchers in AI, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.},
  isbn = {978-1-4503-5746-3},
  langid = {english},
  keywords = {fairness},
  file = {Zotero/Verma_Rubin_2018_Fairness definitions explained.pdf}
}

@misc{Wang2019,
  title = {Balanced {{Datasets Are Not Enough}}: {{Estimating}} and {{Mitigating Gender Bias}} in {{Deep Image Representations}}},
  shorttitle = {Balanced {{Datasets Are Not Enough}}},
  author = {Wang, Tianlu and Zhao, Jieyu and Yatskar, Mark and Chang, Kai-Wei and Ordonez, Vicente},
  year = {2019},
  month = oct,
  number = {arXiv:1811.08489},
  eprint = {1811.08489},
  primaryclass = {cs},
  institution = {{arXiv}},
  url = {http://arxiv.org/abs/1811.08489},
  urldate = {2022-06-15},
  abstract = {In this work, we present a framework to measure and mitigate intrinsic biases with respect to protected variables --such as gender-- in visual recognition tasks. We show that trained models significantly amplify the association of target labels with gender beyond what one would expect from biased datasets. Surprisingly, we show that even when datasets are balanced such that each label co-occurs equally with each gender, learned models amplify the association between labels and gender, as much as if data had not been balanced! To mitigate this, we adopt an adversarial approach to remove unwanted features corresponding to protected variables from intermediate representations in a deep neural network -- and provide a detailed analysis of its effectiveness. Experiments on two datasets: the COCO dataset (objects), and the imSitu dataset (actions), show reductions in gender bias amplification while maintaining most of the accuracy of the original models.},
  archiveprefix = {arxiv},
  keywords = {age,bias_example,bias_mitigation},
  file = {../../../../../snap/zotero-snap/common/Zotero/storage/L6JGLNSR/Wang et al_2019_Balanced Datasets Are Not Enough.pdf}
}

@article{Wang2022,
  title = {A {{Systematic Review}} on {{Affective Computing}}: {{Emotion Models}}, {{Databases}}, and {{Recent Advances}}},
  author = {Wang, Yan and Song, Wei and Tao, Wei and Liotta, Antonio and Yang, Dawei and Li, Xinlei and Gao, Shuyong and Sun, Yixuan and Ge, Weifeng and Zhang, Wei and Zhang, Wenqiang},
  year = {2022},
  month = mar,
  pages = {48},
  abstract = {With the rapid development of artificial intelligence and the universal promotion of real-life computer applications, affective computing plays a key role in human-computer interactions, entertainment, teaching, safe driving, and multimedia integration. Major breakthroughs have been made recently in the areas of affective computing (i.e., emotion recognition and sentiment analysis). Affective computing is realized based on unimodal or multimodal data, primarily consisting of physical information (e.g., textual, audio, and visual data) and physiological signals (e.g., EEG and ECG signals). Physicalbased affect recognition caters to more researchers due to multiple public databases. However, it is hard to reveal one's inner emotion hidden purposely from facial expressions, audio tones, body gestures, etc. Physiological signals can generate more precise and reliable emotional results; yet, the difficulty in acquiring physiological signals also hinders their practical application. Thus, the fusion of physical information and physiological signals can provide useful features of emotional states and lead to higher accuracy. Instead of focusing on one specific field of affective analysis, we systematically review recent advances in the affective computing, and taxonomize unimodal affect recognition as well as multimodal affective analysis. Firstly, we introduce two typical emotion models followed by commonly used databases for affective computing. Next, we survey and taxonomize state-of-the-art unimodal affect recognition and multimodal affective analysis in terms of their detailed architectures and performances. Finally, we discuss some important aspects on affective computing and their applications and conclude this review with an indication of the most promising future directions, such as the establishment of baseline dataset, fusion strategies for multimodal affective analysis, and unsupervised learning models. The overall goal of this systematic overview is to facilitate academic and industrial researchers alike in understanding the recent advances as well as new developments in this fast-paced, high-impact domain.},
  langid = {english},
  keywords = {review},
  file = {Zotero/Wang et al_2022_A Systematic Review on Affective Computing.pdf}
}

@article{Wang2022a,
  title = {{{FERV39k}}: {{A Large-Scale Multi-Scene Dataset}} for {{Facial Expression Recognition}} in {{Videos}}},
  shorttitle = {{{FERV39k}}},
  author = {Wang, Yan and Sun, Yixuan and Huang, Yiwen and Liu, Zhongying and Gao, Shuyong and Zhang, Wei and Ge, Weifeng and Zhang, Wenqiang},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.09463 [cs]},
  eprint = {2203.09463},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2203.09463},
  urldate = {2022-03-22},
  abstract = {Current benchmarks for facial expression recognition (FER) mainly focus on static images, while there are limited datasets for FER in videos. It is still ambiguous to evaluate whether performances of existing methods remain satisfactory in real-world application-oriented scenes. For example, the "Happy" expression with high intensity in Talk-Show is more discriminating than the same expression with low intensity in Official-Event. To fill this gap, we build a large-scale multi-scene dataset, coined as FERV39k. We analyze the important ingredients of constructing such a novel dataset in three aspects: (1) multi-scene hierarchy and expression class, (2) generation of candidate video clips, (3) trusted manual labelling process. Based on these guidelines, we select 4 scenarios subdivided into 22 scenes, annotate 86k samples automatically obtained from 4k videos based on the well-designed workflow, and finally build 38,935 video clips labeled with 7 classic expressions. Experiment benchmarks on four kinds of baseline frameworks were also provided and further analysis on their performance across different scenes and some challenges for future research were given. Besides, we systematically investigate key components of DFER by ablation studies. The baseline framework and our project will be available.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Wang et al_2022_FERV39k.pdf}
}

@article{Werner2022,
  title = {Automatic {{Recognition Methods Supporting Pain Assessment}}: {{A Survey}}},
  shorttitle = {Automatic {{Recognition Methods Supporting Pain Assessment}}},
  author = {Werner, Philipp and {Lopez-Martinez}, Daniel and Walter, Steffen and {Al-Hamadi}, Ayoub and Gruss, Sascha and Picard, Rosalind W.},
  year = {2022},
  month = jan,
  journal = {IEEE Transactions on Affective Computing},
  volume = {13},
  number = {1},
  pages = {530--552},
  issn = {1949-3045},
  doi = {10.1109/TAFFC.2019.2946774},
  abstract = {Pain is a complex phenomenon, involving sensory and emotional experience, that is often poorly understood, especially in infants, anesthetized patients, and others who cannot speak. Technology supporting pain assessment has the potential to help reduce suffering; however, advances are needed before it can be adopted clinically. This survey paper assesses the state of the art and provides guidance for researchers to help make such advances. First, we overview pain's biological mechanisms, physiological and behavioral responses, emotional components, as well as assessment methods commonly used in the clinic. Next, we discuss the challenges hampering the development and validation of pain recognition technology, and we survey existing datasets together with evaluation methods. We then present an overview of all automated pain recognition publications indexed in the Web of Science as well as from the proceedings of the major conferences on biomedical informatics and artificial intelligence, to provide understanding of the current advances that have been made. We highlight progress in both non-contact and contact-based approaches, tools using face, voice, physiology, and multi-modal information, the importance of context, and discuss challenges that exist, including identification of ground truth. Finally, we identify underexplored areas such as chronic pain and connections to treatments, and describe promising opportunities for continued advances.},
  keywords = {fer_applications,review},
  file = {Zotero/Werner et al_2022_Automatic Recognition Methods Supporting Pain Assessment.pdf}
}

@article{Whittaker1952,
  title = {A {{Study}} of {{Summer Foliage Insect Communities}} in the {{Great Smoky Mountains}}},
  author = {Whittaker, R. H.},
  year = {1952},
  journal = {Ecological Monographs},
  volume = {22},
  number = {1},
  eprint = {1948527},
  eprinttype = {jstor},
  pages = {2--44},
  publisher = {{Ecological Society of America}},
  issn = {0012-9615},
  doi = {10.2307/1948527},
  urldate = {2023-02-22},
  keywords = {metrics},
  file = {Zotero/Whittaker_1952_A Study of Summer Foliage Insect Communities in the Great Smoky Mountains.pdf}
}

@article{Whittaker1960,
  title = {Vegetation of the {{Siskiyou Mountains}}, {{Oregon}} and {{California}}},
  author = {Whittaker, R. H.},
  year = {1960},
  journal = {Ecological Monographs},
  volume = {30},
  number = {3},
  pages = {279--338},
  issn = {1557-7015},
  doi = {10.2307/1943563},
  urldate = {2022-08-29},
  langid = {english},
  keywords = {bias_detection,diversity,ecology},
  file = {../../../../../snap/zotero-snap/common/Zotero/storage/ATN7YCVV/1943563.html}
}

@article{Widen2013,
  title = {Children's Recognition of Disgust in Others},
  author = {Widen, Sherri C. and Russell, James A.},
  year = {2013},
  month = mar,
  journal = {Psychological Bulletin},
  volume = {139},
  number = {2},
  pages = {271--299},
  issn = {1939-1455, 0033-2909},
  doi = {10.1037/a0031640},
  urldate = {2021-10-19},
  langid = {english},
  keywords = {bias_example,psychology}
}

@article{Winfield2019,
  title = {Ethical Standards in Robotics and {{AI}}},
  author = {Winfield, Alan},
  year = {2019},
  month = feb,
  journal = {Nature Electronics},
  volume = {2},
  number = {2},
  pages = {46--48},
  issn = {2520-1131},
  doi = {10.1038/s41928-019-0213-6},
  urldate = {2023-01-27},
  langid = {english},
  keywords = {ethics}
}

@inproceedings{Xu2020,
  title = {Investigating {{Bias}} and {{Fairness}} in {{Facial Expression Recognition}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2020 {{Workshops}}},
  author = {Xu, Tian and White, Jennifer and Kalkan, Sinan and Gunes, Hatice},
  editor = {Bartoli, Adrien and Fusiello, Andrea},
  year = {2020},
  pages = {506--523},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-65414-6\_35},
  abstract = {Recognition of expressions of emotions and affect from facial images is a well-studied research problem in the fields of affective computing and computer vision with a large number of datasets available containing facial images and corresponding expression labels. However, virtually none of these datasets have been acquired with consideration of fair distribution across the human population. Therefore, in this work, we undertake a systematic investigation of bias and fairness in facial expression recognition by comparing three different approaches, namely a baseline, an attribute-aware and a disentangled approach, on two well-known datasets, RAF-DB and CelebA. Our results indicate that: (i) data augmentation improves the accuracy of the baseline model, but this alone is unable to mitigate the bias effect; (ii) both the attribute-aware and the disentangled approaches equipped with data augmentation perform better than the baseline approach in terms of accuracy and fairness; (iii) the disentangled approach is the best for mitigating demographic bias; and (iv) the bias mitigation strategies are more suitable in the existence of uneven attribute distribution or imbalanced number of subgroup data.},
  isbn = {978-3-030-65414-6},
  langid = {english},
  keywords = {fairness},
  file = {Zotero/Xu et al_2020_Investigating Bias and Fairness in Facial Expression Recognition.pdf}
}

@article{Yang2020,
  title = {Tsinghua Facial Expression Database \textendash{} {{A}} Database of Facial Expressions in {{Chinese}} Young and Older Women and Men: {{Development}} and Validation},
  shorttitle = {Tsinghua Facial Expression Database \textendash{} {{A}} Database of Facial Expressions in {{Chinese}} Young and Older Women and Men},
  author = {Yang, Tao and Yang, Zeyun and Xu, Guangzheng and Gao, Duoling and Zhang, Ziheng and Wang, Hui and Liu, Shiyu and Han, Linfeng and Zhu, Zhixin and Tian, Yang and Huang, Yuqi and Zhao, Lei and Zhong, Kui and Shi, Bolin and Li, Juan and Fu, Shimin and Liang, Peipeng and Banissy, Michael J. and Sun, Pei},
  year = {2020},
  month = apr,
  journal = {PLOS ONE},
  volume = {15},
  number = {4},
  pages = {e0231304},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0231304},
  urldate = {2022-06-07},
  abstract = {Perception of facial identity and emotional expressions is fundamental to social interactions. Recently, interest in age associated changes in the processing of faces has grown rapidly. Due to the lack of older faces stimuli, most previous age-comparative studies only used young faces stimuli, which might cause own-age advantage. None of the existing Eastern face stimuli databases contain face images of different age groups (e.g. older adult faces). In this study, a database that comprises images of 110 Chinese young and older adults displaying eight facial emotional expressions (Neutral, Happiness, Anger, Disgust, Surprise, Fear, Content, and Sadness) was constructed. To validate this database, each image was rated on the basis of perceived facial expressions, perceived emotional intensity, and perceived age by two different age groups. Results have shown an overall 79.08\% correct identification rate in the validation. Access to the freely available database can be requested by emailing the corresponding authors.},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Zotero/Yang et al_2020_Tsinghua facial expression database – A database of facial expressions in.pdf;../../../../../snap/zotero-snap/common/Zotero/storage/CT5EDZ39/article.html}
}

@inproceedings{Yang2022,
  title = {Enhancing {{Fairness}} in {{Face Detection}} in {{Computer Vision Systems}} by {{Demographic Bias Mitigation}}},
  booktitle = {Proceedings of the 2022 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Yang, Yu and Gupta, Aayush and Feng, Jianwei and Singhal, Prateek and Yadav, Vivek and Wu, Yue and Natarajan, Pradeep and Hedau, Varsha and Joo, Jungseock},
  year = {2022},
  month = jul,
  pages = {813--822},
  publisher = {{ACM}},
  address = {{Oxford United Kingdom}},
  doi = {10.1145/3514094.3534153},
  urldate = {2023-02-21},
  abstract = {Fairness has become an important agenda in computer vision and artificial intelligence. Recent studies have shown that many computer vision models and datasets exhibit demographic biases and proposed mitigation strategies. These works attempt to address accuracy disparity, spurious correlations, or unbalanced representations in datasets in tasks such as face recognition, verification and expression and attribute classification. These tasks, however, all require face detection as the first preprocessing step, and surprisingly, there has been little effort in identifying or mitigating biases in face detection. Biased face detectors themselves pose a threat against fair and ethical AI systems, and their biases may be further passed on to subsequent downstream tasks such as face recognition in a computer vision pipeline. This paper therefore investigates the problem of biases in face detection, focusing on accuracy disparity of detectors between demographic groups including gender, age group, and skin tone. We collect perceived demographic attributes on a popular face detection benchmark dataset, WIDER FACE, report skewed demographic distributions, and compare detection performance between groups. In order to mitigate the biases, we apply three mitigation methods that have been introduced in the recent literature and also propose two novel methods. Experimental results show that these methods are effective in reducing demographic biases. We also discuss how the effectiveness varies by demographic attributes, detection easiness, and multiple detectors, which will shed light on this new topic of addressing face detection bias.},
  isbn = {978-1-4503-9247-1},
  langid = {english},
  keywords = {bias_detection,bias_example,bias_mitigation,fairness_evaluation},
  file = {../../../../../snap/zotero-snap/common/Zotero/storage/C8RYYNZ4/Yang et al. - 2022 - Enhancing Fairness in Face Detection in Computer V.pdf}
}

@inproceedings{Yin2006,
  title = {A {{3D Facial Expression Database For Facial Behavior Research}}},
  booktitle = {7th {{International Conference}} on {{Automatic Face}} and {{Gesture Recognition}} ({{FGR06}})},
  author = {Yin, Lijun and Wei, Xiaozhou and Sun, Yi and Wang, Jun and Rosato, M.J.},
  year = {2006},
  pages = {211--216},
  publisher = {{IEEE}},
  address = {{Southampton, UK}},
  doi = {10.1109/FGR.2006.6},
  urldate = {2022-04-26},
  abstract = {Traditionally, human facial expressions have been studied using either 2D static images or 2D video sequences. The 2D-based analysis is incapable of handing large pose variations. Although 3D modeling techniques have been extensively used for 3D face recognition and 3D face animation, barely any research on 3D facial expression recognition using 3D range data has been reported. A primary factor for preventing such research is the lack of a publicly available 3D facial expression database. In this paper, we present a newly developed 3D facial expression database, which includes both prototypical 3D facial expression shapes and 2D facial textures of 2,500 models from 100 subjects. This is the first attempt at making a 3D facial expression database available for the research community, with the ultimate goal of fostering the research on affective computing and increasing the general understanding of facial behavior and the fine 3D structure inherent in human facial expressions. The new database can be a valuable resource for algorithm assessment, comparison and evaluation.},
  isbn = {978-0-7695-2503-7},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Yin et al_2006_A 3D Facial Expression Database For Facial Behavior Research.pdf}
}

@inproceedings{Yin2008,
  title = {A High-Resolution {{3D}} Dynamic Facial Expression Database},
  booktitle = {2008 8th {{IEEE International Conference}} on {{Automatic Face}} \& {{Gesture Recognition}}},
  author = {Yin, Lijun and Chen, Xiaochen and Sun, Yi and Worm, Tony and Reale, Michael},
  year = {2008},
  month = sep,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Amsterdam, Netherlands}},
  doi = {10.1109/AFGR.2008.4813324},
  urldate = {2022-04-26},
  abstract = {Face information processing relies on the quality of data resource. From the data modality point of view, a face database can be 2D or 3D, and static or dynamic. From the task point of view, the data can be used for research of computer based automatic face recognition, face expression recognition, face detection, or cognitive and psychological investigation. With the advancement of 3D imaging technologies, 3D dynamic facial sequences (called 4D data) have been used for face information analysis. In this paper, we focus on the modality of 3D dynamic data for the task of facial expression recognition. We present a newly created high-resolution 3D dynamic facial expression database, which is made available to the scientific research community. The database contains 606 3D facial expression sequences captured from 101 subjects of various ethnic backgrounds. The database has been validated through our facial expression recognition experiment using an HMM based 3D spatio-temporal facial descriptor. It is expected that such a database shall be used to facilitate the facial expression analysis from a static 3D space to a dynamic 3D space, with a goal of scrutinizing facial behavior at a higher level of detail in a real 3D spatio-temporal domain.},
  isbn = {978-1-4244-2153-4},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Yin et al_2008_A high-resolution 3D dynamic facial expression database.pdf}
}

@inproceedings{Yu2015,
  title = {Image Based {{Static Facial Expression Recognition}} with {{Multiple Deep Network Learning}}},
  booktitle = {Proceedings of the 2015 {{ACM}} on {{International Conference}} on {{Multimodal Interaction}}},
  author = {Yu, Zhiding and Zhang, Cha},
  year = {2015},
  month = nov,
  series = {{{ICMI}} '15},
  pages = {435--442},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2818346.2830595},
  urldate = {2022-03-28},
  abstract = {We report our image based static facial expression recognition method for the Emotion Recognition in the Wild Challenge (EmotiW) 2015. We focus on the sub-challenge of the SFEW 2.0 dataset, where one seeks to automatically classify a set of static images into 7 basic emotions. The proposed method contains a face detection module based on the ensemble of three state-of-the-art face detectors, followed by a classification module with the ensemble of multiple deep convolutional neural networks (CNN). Each CNN model is initialized randomly and pre-trained on a larger dataset provided by the Facial Expression Recognition (FER) Challenge 2013. The pre-trained models are then fine-tuned on the training set of SFEW 2.0. To combine multiple CNN models, we present two schemes for learning the ensemble weights of the network responses: by minimizing the log likelihood loss, and by minimizing the hinge loss. Our proposed method generates state-of-the-art result on the FER dataset. It also achieves 55.96\% and 61.29\% respectively on the validation and test set of SFEW 2.0, surpassing the challenge baseline of 35.96\% and 39.13\% with significant gains.},
  isbn = {978-1-4503-3912-4},
  keywords = {fer_implementation},
  file = {Zotero/Yu_Zhang_2015_Image based Static Facial Expression Recognition with Multiple Deep Network.pdf}
}

@inproceedings{Zafeiriou2017,
  title = {Aff-Wild: {{Valence}} and Arousal `in-the-Wild'Challenge},
  booktitle = {Computer Vision and Pattern Recognition Workshops ({{CVPRW}}), 2017 {{IEEE}} Conference On},
  author = {Zafeiriou, Stefanos and Kollias, Dimitrios and Nicolaou, Mihalis A and Papaioannou, Athanasios and Zhao, Guoying and Kotsia, Irene},
  year = {2017},
  pages = {1980--1987},
  organization = {{IEEE}},
  keywords = {aff-wild-2,datasets}
}

@article{Zamzmi2019,
  title = {Convolutional {{Neural Networks}} for {{Neonatal Pain Assessment}}},
  author = {Zamzmi, Ghada and Paul, Rahul and Salekin, Md. Sirajus and Goldgof, Dmitry and Kasturi, Rangachar and Ho, Thao and Sun, Yu},
  year = {2019},
  month = jul,
  journal = {IEEE Transactions on Biometrics, Behavior, and Identity Science},
  volume = {1},
  number = {3},
  pages = {192--200},
  issn = {2637-6407},
  doi = {10.1109/TBIOM.2019.2918619},
  abstract = {The current standard for assessing neonatal pain is discontinuous and inconsistent because it depends highly on the observers bias. These drawbacks can result in delayed intervention and inconsistent treatment of pain. Convolutional neural networks (CNNs) have gained much popularity in the last decades due to the wide range of its successful applications in medical image analysis, object and emotion recognition. In this paper, we investigated the use of a novel lightweight neonatal convolutional neural network as well as other popular CNN architectures for assessing neonatal pain. We experimented with various image augmentation techniques and evaluated the CNN architectures using two real-world datasets [COPE and neonatal pain assessment dataset (NPAD)] collected from neonates while being hospitalized in the intensive care unit. The experimental results demonstrate the superiority and efficiency of the novel network in assessing neonatal pain. They also suggest that the automatic recognition of neonatal pain using CNN networks is a viable and more efficient alternative to the current assessment standard.},
  keywords = {fer_applications},
  file = {Zotero/Zamzmi et al_2019_Convolutional Neural Networks for Neonatal Pain Assessment.pdf}
}

@article{Zamzmi2019a,
  title = {A {{Comprehensive}} and {{Context-Sensitive Neonatal Pain Assessment Using Computer Vision}}},
  author = {Zamzmi, Ghada and {Chih-Yun}, Pai and Goldgof, Dmitry and Kasturi, R and Ashmeade, Terri and Sun, Yu},
  year = {2019},
  journal = {IEEE Transactions on Affective Computing},
  pages = {1--1},
  issn = {1949-3045},
  doi = {10.1109/TAFFC.2019.2926710},
  abstract = {Infants receiving care in the Neonatal Intensive Care Unit (NICU) experience several painful procedures during their hospitalization. Assessing neonatal pain is difficult because the current standard for assessment is subjective, inconsistent, and discontinuous. The intermittent and inconsistent assessment can induce poor treatment and, therefore, cause serious long-term outcomes. In this paper, we present a comprehensive pain assessment system that utilizes facial expressions along with crying sounds, body movement, and vital sign changes. The proposed automatic system generates a standardized pain assessment comparable to those obtained by conventional nurse-derived pain scores. The system achieved 95.56\% accuracy using decision fusion of different pain responses that were recorded in a challenging clinical environment. In addition to the decision fusion, we present the performance of multimodal assessment using other fusion schemes as well as a unimodal assessment approach. We also discuss the impact of different factors (e.g., gestational age) on pain, propose several group-specific models for pain assessment (e.g., pre-term and full-term models), and compare the performance of these models with the performance of general models. While further research is needed, our results show that the automatic assessment of neonatal pain is a viable and more efficient alternative to the manual assessment.},
  file = {Zotero/Zamzmi et al_2019_A Comprehensive and Context-Sensitive Neonatal Pain Assessment Using Computer.pdf}
}

@inproceedings{Zhang2015,
  title = {Learning {{Social Relation Traits}} from {{Face Images}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Zhang, Zhanpeng and Luo, Ping and Loy, Chen-Change and Tang, Xiaoou},
  year = {2015},
  month = dec,
  pages = {3631--3639},
  publisher = {{IEEE}},
  address = {{Santiago, Chile}},
  doi = {10.1109/ICCV.2015.414},
  urldate = {2022-04-08},
  abstract = {Social relation defines the association, e.g., warm, friendliness, and dominance, between two or more people. Motivated by psychological studies, we investigate if such fine-grained and high-level relation traits can be characterised and quantified from face images in the wild. To address this challenging problem we propose a deep model that learns a rich face representation to capture gender, expression, head pose, and age-related attributes, and then performs pairwise-face reasoning for relation prediction. To learn from heterogeneous attribute sources, we formulate a new network architecture with a bridging layer to leverage the inherent correspondences among these datasets. It can also cope with missing target attribute labels. Extensive experiments show that our approach is effective for fine-grained social relation learning in images and videos.},
  isbn = {978-1-4673-8391-2},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Zhang et al_2015_Learning Social Relation Traits from Face Images.pdf}
}

@article{Zhang2017,
  title = {From {{Facial Expression Recognition}} to {{Interpersonal Relation Prediction}}},
  author = {Zhang, Zhanpeng and Luo, Ping and Loy, Chen Change and Tang, Xiaoou},
  year = {2018},
  month = may,
  journal = {International Journal of Computer Vision},
  volume = {126},
  number = {5},
  pages = {550--569},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-017-1055-1},
  urldate = {2023-03-23},
  langid = {english},
  file = {Zotero/Zhang et al_2018_From Facial Expression Recognition to Interpersonal Relation Prediction.pdf}
}

@article{Zhang2017arxiv,
  title = {From {{Facial Expression Recognition}} to {{Interpersonal Relation Prediction}}},
  author = {Zhang, Zhanpeng and Luo, Ping and Loy, Chen Change and Tang, Xiaoou},
  year = {2017},
  month = nov,
  journal = {arXiv:1609.06426 [cs]},
  eprint = {1609.06426},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1609.06426},
  urldate = {2021-11-09},
  abstract = {Interpersonal relation defines the association, e.g., warm, friendliness, and dominance, between two or more people. Motivated by psychological studies, we investigate if such fine-grained and high-level relation traits can be characterized and quantified from face images in the wild. We address this challenging problem by first studying a deep network architecture for robust recognition of facial expressions. Unlike existing models that typically learn from facial expression labels alone, we devise an effective multitask network that is capable of learning from rich auxiliary attributes such as gender, age, and head pose, beyond just facial expression data. While conventional supervised training requires datasets with complete labels (e.g., all samples must be labeled with gender, age, and expression), we show that this requirement can be relaxed via a novel attribute propagation method. The approach further allows us to leverage the inherent correspondences between heterogeneous attribute sources despite the disparate distributions of different datasets. With the network we demonstrate state-of-the-art results on existing facial expression recognition benchmarks. To predict inter-personal relation, we use the expression recognition network as branches for a Siamese model. Extensive experiments show that our model is capable of mining mutual context of faces for accurate fine-grained interpersonal prediction.},
  archiveprefix = {arxiv},
  keywords = {datasets},
  file = {Zotero/Zhang et al_2017_From Facial Expression Recognition to Interpersonal Relation Prediction.pdf}
}

@book{Zhang2022,
  title = {The {{AI Index}} 2022 {{Annual Report}}},
  author = {Zhang, Daniel and Maslej, Nestor and Brynjolfsson, Erik and Etchemendy, John and Lyons, Terah and Manyika, James and Ngo, Helen and Niebles, Juan Carlos and Sellitto, Michael and Sakhaee, Ellie and Shoham, Yoav and Clark, Jack and Perrault, Raymond},
  year = {2022},
  month = mar,
  publisher = {{AI Index Steering Committee, Stanford Institute for Human-Centered AI, Stanford University}},
  keywords = {fairness,fairness_evaluation},
  file = {Zotero/Zhang et al_2022_The AI Index 2022 Annual Report.pdf}
}

@article{Zhao2011,
  title = {Facial Expression Recognition from Near-Infrared Videos},
  author = {Zhao, Guoying and Huang, Xiaohua and Taini, Matti and Li, Stan Z. and Pietik{\"a}inen, Matti},
  year = {2011},
  month = aug,
  journal = {Image and Vision Computing},
  volume = {29},
  number = {9},
  pages = {607--619},
  issn = {02628856},
  doi = {10.1016/j.imavis.2011.07.002},
  urldate = {2022-05-31},
  abstract = {Facial expression recognition is to determine the emotional state of the face regardless of its identity. Most of the existing datasets for facial expressions are captured in a visible light spectrum. However, the visible light (VIS) can change with time and location, causing significant variations in appearance and texture. In this paper, we present a novel research on a dynamic facial expression recognition, using near-infrared (NIR) video sequences and LBP-TOP (Local binary patterns from three orthogonal planes) feature descriptors. NIR imaging combined with LBP-TOP features provide an illumination invariant description of face video sequences. Appearance and motion features in slices are used for expression classification, and for this, discriminative weights are learned from training examples. Furthermore, component-based facial features are presented to combine geometric and appearance information, providing an effective way for representing the facial expressions. Experimental results of facial expression recognition using a novel Oulu-CASIA NIR\&VIS facial expression database, a support vector machine and sparse representation classifiers show good and robust results against illumination variations. This provides a baseline for future research on NIR-based facial expression recognition.},
  langid = {english},
  keywords = {datasets},
  file = {Zotero/Zhao et al_2011_Facial expression recognition from near-infrared videos.pdf}
}

@inproceedings{Zhao2017,
  title = {Men {{Also Like Shopping}}: {{Reducing Gender Bias Amplification}} Using {{Corpus-level Constraints}}},
  shorttitle = {Men {{Also Like Shopping}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
  year = {2017},
  pages = {2979--2989},
  publisher = {{Association for Computational Linguistics}},
  address = {{Copenhagen, Denmark}},
  doi = {10.18653/v1/D17-1323},
  urldate = {2022-06-23},
  abstract = {Language is increasingly being used to define rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33\% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68\% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5\% and 40.5\% for multilabel classification and visual semantic role labeling, respectively.},
  langid = {english},
  keywords = {bias_example,bias_mitigation,gender,stereotypical_bias},
  file = {Zotero/Zhao et al_2017_Men Also Like Shopping.pdf}
}

@article{Zhu2018,
  title = {{{LRID}}: {{A}} New Metric of Multi-Class Imbalance Degree Based on Likelihood-Ratio Test},
  shorttitle = {{{LRID}}},
  author = {Zhu, Rui and Wang, Ziyu and Ma, Zhanyu and Wang, Guijin and Xue, Jing-Hao},
  year = {2018},
  month = dec,
  journal = {Pattern Recognition Letters},
  volume = {116},
  pages = {36--42},
  issn = {01678655},
  doi = {10.1016/j.patrec.2018.09.012},
  urldate = {2022-08-31},
  abstract = {In this paper, we introduce a new likelihood ratio imbalance degree (LRID) to measure the class-imbalance extent of multi-class data. Imbalance ratio (IR) is usually used to measure class-imbalance extent in imbalanced learning problems. However, IR cannot capture the detailed information in the class distribution of multi-class data, because it only utilises the information of the largest majority class and the smallest minority class. Imbalance degree (ID) has been proposed to solve the problem of IR for multi-class data. However, we note that improper use of distance metric in ID can have harmful effect on the results. In addition, ID assumes that data with more minority classes are more imbalanced than data with less minority classes, which is not always true in practice. Thus ID cannot provide reliable measurement when the assumption is violated. In this paper, we propose a new metric based on the likelihood-ratio test, LRID, to provide a more reliable measurement of class-imbalance extent for multi-class data. Experiments on both simulated and real data show that LRID is competitive with IR and ID, and can reduce the negative correlation with F1 scores by up to 0.55.},
  langid = {english},
  keywords = {bias_detection,class_imbalance,diversity},
  file = {Zotero/Zhu et al_2018_LRID.pdf}
}
