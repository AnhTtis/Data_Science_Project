\documentclass[12pt]{article} % use larger type; default would be 10pt
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{letterpaper} % or letterpaper (US) or a5paper or....
\geometry{margin=1in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

%%% BEGIN RAGGED RIGHT COLUMNS
\usepackage{kantlipsum}
\newlength{\normalparindent}
\setlength{\normalparindent}{1cm}
\raggedright
\setlength{\parindent}{\normalparindent}
% \usepackage{ragged2e}

% \setlength\RaggedRightParindent{1cm}
% \RaggedRight

%%% END RAGGED RIGHT COLUMNS


\usepackage{hyperref}

\usepackage{graphicx} % support the \includegraphics command and options
%\usepackage{setspace}

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subcaption} % make it possible to include more than one captioned figure/table in a single float
\usepackage{amsbsy}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amscd}
\usepackage{amsthm}
% \usepackage{cite} % AFM: this causes weird ordering issues with citations and seems unneeded
\usepackage{float}
\usepackage[toc,page]{appendix}
\usepackage{natbib}
\usepackage{lineno}
\usepackage{mathrsfs}
\usepackage{bbm}
% \usepackage[nolists]{endfloat}
% ^ puts graphs at end of doc.
\usepackage{lineno}
\usepackage{todonotes}
\usepackage{setspace}
\usepackage{multirow}
\usepackage{todonotes}
% \linenumbers
% These packages are all incorporated in the memoir class to one degree or another...
\doublespacing

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The "real" document content comes below...
%Variable Declarations
%Matricies
% \graphicspath{{Figures/}}

\newcommand{\prob}[1]{{\mathbb P}\hspace{-0.2em} \left(  #1 \right)}
\newcommand{\transpose}{'}

\newcommand{\data}{\mathbf{Y}}

\newcommand{\phylogeny}{{\cal F}}
\newcommand{\nTaxa}{N}

\newcommand{\nSites}{C}
\newcommand{\site}{c}

\newcommand{\category}{r}
\newcommand{\nCategories}{R}
\newcommand{\rateCategory}{\gamma}

% \newcommand{\likelihood}{\mathscr{L}}
\newcommand{\likelihoodAll}[1]{{\mathbb P}\hspace{-0.2em} \left( \data \mid #1 \right)}
\newcommand{\likelihood}[1]{{\mathbb P}\hspace{-0.2em} \left( \data_{\site} \mid #1 \right)}
\newcommand{\argmax}{\operatorname{arg\,max}}
\newcommand{\argmin}{\operatorname{arg\,min}}

\newcommand{\alphabetsize}{S}

\newcommand{\approxnabla}{\widetilde{\nabla}}

\newcommand{\errorangle}{\phi}
\newcommand{\errorelement}{\epsilon}

\newcommand{\param}{\theta}
\newcommand{\params}{\boldsymbol{\param}}
\newcommand{\randomeffect}{\epsilon}
\newcommand{\brandomeffect}{\mathbf{\randomeffect}}
\newcommand{\ratematrix}{\mathbf{Q}}
\newcommand{\ratematrixelement}{\lambda}
\newcommand{\branchlength}{t}

\newcommand{\mapping}{\mathbf{M}}
\newcommand{\allQderivs}{\mathbf{D}}
\newcommand{\allQderivselement}{d}

\newcommand{\node}{v}
\newcommand{\parent}{u}
\newcommand{\sister}{w}

\newcommand{\postPartial}{p}
\newcommand{\prePartial}{q}
\newcommand{\bpostPartial}{\mathbf{\postPartial}}
\newcommand{\bprePartial}{\mathbf{\prePartial}}
\newcommand{\bprePrePartial}{\mathbf{\tilde{\prePartial}}}

\newcommand{\vecm}{\operatorname{vec}}

\newcommand{\indicatormatrix}{\mathbf{E}}
\newcommand{\indicatormatrixij}{\mathbf{E}_{ij}}

\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bZ}{\mathbf{Z}}

\newcommand{\basemodel}{\mathcal{M}}

\newcommand{\bigO}{\mathcal{O}}
\newcommand{\order}[1]{\bigO \hspace{-0.1em} \left( #1 \right)}
\newcommand{\mnorm}{\vert \vert}
\newcommand{\Biggmnorm}{\Bigg\vert \Bigg\vert}

\newcommand{\globalScale}{\tau}
\newcommand{\bridgeExponent}{\alpha}

\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bdelta}{\boldsymbol{\delta}}

\newcommand{\pr}{\text{Pr}}

\newcommand{\indicator}{\mathbbm{1}}
\newcommand{\indicatorvector}{\boldsymbol{\indicator}}

\newcommand{\dbydQij}{\frac{\partial}{\partial \bQ_{ij}}}

\newcommand{\bpi}{\mathbf{\pi}}
\newcommand{\btheta}{\mathbf{\theta}}

\newcommand{\simiid}{\stackrel{\text{iid}}{\sim}}

\newcommand{\mascomment}[1]{({\color{green}{MAS's comment:}} \textbf{\color{green}{#1}})}
\newcommand{\afmcomment}[1]{({\color{orange}{AFM's comment:}} \textbf{\color{orange}{#1}})}

\newcommand{\normalization}{\phi}

\newtheorem{thm}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{cor}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem{mydef}{Definition}
\newtheorem{assumption}{Assumption}

%Define shortcut command
\def\newshortcut#1#2{%
\let#1=\undefined
\newcommand{#1}{#2}}

% http://bytesizebio.net/2013/03/11/adding-supplementary-tables-and-figures-in-latex/
\newcommand{\beginsupplement}{%
        \setcounter{table}{0}
        \renewcommand{\thetable}{S\arabic{table}}%
        \setcounter{figure}{0}
        \renewcommand{\thefigure}{S\arabic{figure}}%
				\renewcommand{\thesection}{S\arabic{section}}%
				\setcounter{section}{0}
     }

\newshortcut{\pyabove}{\cDensity{\latentData_{\nodeIndexOne}}{\latentData_{\above{\nodeIndexOne}}}}

\begin{document}
\singlespacing
\begin{flushright}
Version dated: \today\\
\end{flushright}

\bigskip
\medskip
\begin{center}

\noindent{\Large \bf
%	Scalable random-effects substitution models for phylogenetics
	Random-effects substitution models for phylogenetics via scalable gradient approximations
}
\bigskip

\noindent{\normalsize \sc
	Andrew F.~Magee$^{1}$ \\
	Andrew J.~Holbrook$^{1}$ \\
	Jonathan E.~Pekar$^{2,3}$ \\
	Itzue W.~Caviedes-Solis$^{4}$ \\
	Fredrick A.~Matsen IV$^{5,6,7,8}$ \\
	Guy Baele$^{9}$ \\
	Joel O.~Wertheim$^{10}$ \\
	Xiang Ji$^{11}$ \\
	Philippe Lemey$^{9}$ \\
    and Marc A.~Suchard$^{1,12,13}$} \\

\bigskip
\noindent {\small
  \it $^1$ Department of Biostatistics, Jonathan and Karin Fielding School of Public Health, University of California Los Angeles, Los Angeles, CA, USA \\
  \it $^2$ Bioinformatics and Systems Biology Graduate Program, University of California San Diego, La Jolla, CA, USA \\
  \it $^3$ Department of Biomedical Informatics, University of California San Diega, La Jolla, CA, USA \\
  \it $^4$ Department of Biology, Swarthmore College, Swarthmore, PA, USA \\
  \it $^5$ Howard Hughes Medical Institute, Seattle, Washington, USA \\
  \it $^6$ Computational Biology Program, Fred Hutchinson Cancer Research Center, Seattle, Washington, USA \\
  \it $^7$ Department of Genome Sciences, University of Washington, Seattle, Washington, USA \\
  \it $^8$ Department of Statistics, University of Washington, Seattle, Washington, USA \\
  \it $^9$ Department of Microbiology, Immunology and Transplantation, Rega Institute, KU Leuven, Leuven, Belgium \\
  \it $^{10}$ Department of Medicine, University of California San Diego, La Jolla, CA, USA \\
  \it $^{11}$ Department of Mathematics, Tulane University, New Orleans, LA, USA \\
  \it $^{12}$Department of Biomathematics, David Geffen School of Medicine at UCLA, University of California Los Angeles, Los Angeles, CA, USA \\
  \it $^{13}$ Department of Human Genetics, David Geffen School of Medicine at UCLA, Universtiy of California Los Angeles, Los Angeles, CA, USA
} \\

\end{center}
\medskip
\noindent{\bf Corresponding author:} Marc A.~Suchard, Departments of Biostatistics, Biomathematics, and Human Genetics,
University of California Los Angeles, 695 Charles E.~Young Dr., South,
Los Angeles, CA 90095-7088, USA; E-mail: \url{msuchard@ucla.edu}

\vspace{1in}

\clearpage

\doublespacing

\paragraph{Abstract}
Phylogenetic and discrete-trait evolutionary inference depend heavily on an appropriate characterization of the underlying character substitution process.
In this paper, we present random-effects substitution models that extend common continuous-time Markov chain models into a richer class of processes capable of capturing a wider variety of substitution dynamics.
As these random-effects substitution models often require many more parameters than their usual counterparts, inference can be both statistically and computationally challenging.
Thus, we also propose an efficient approach to compute an approximation to the gradient of the data likelihood with respect to all unknown substitution model parameters.
We demonstrate that this approximate gradient enables scaling of both sampling-based inference (Bayesian inference via Hamiltonian Monte Carlo) and maximization-based inference (maximum \textit{a posteriori} estimation) under random-effects substitution models across large trees and state-spaces.
Applied to a dataset of 583 SARS-CoV-2 sequences, an HKY model with random-effects shows strong signals of nonreversibility in the substitution process, and posterior predictive model checks clearly show that it is a more adequate model than a reversible model.
When analyzing the pattern of phylogeographic spread of 1441 influenza A virus (H3N2) sequences between 14 regions, a random-effects phylogeographic substitution model infers that air travel volume adequately predicts almost all dispersal rates.
A random-effects state-dependent substitution model reveals no evidence for an effect of arboreality on the swimming mode in the tree frog subfamily Hylinae.
On a dataset of 28 taxa spanning the Metazoa, a random-effects amino acid substitution model finds evidence of notable departures from the current best-fit amino acid model in seconds.
We show that our gradient-based inference approach is over an order of magnitude more time efficient than conventional approaches.

\clearpage

\section{Introduction}

Along the branches of a phylogenetic tree, discrete characters such as nucleotides, amino acids, or morphologic traits evolve according to some (typically unknown) substitution process.
Substitution models are probabilistic representations of the substitution process and are central quantities in phylogenetic and phylodynamic models.
Broadly, substitution models describe the relative rates of discrete change from one character state to another.

When inferring phylogenies from character data, the nature of the substitution process is generally not the subject of primary biological interest.
Nevertheless, because substitution models stand as the key link between the phylogenetic tree and the observed discrete character data, appropriate modeling remains of paramount importance to avoid bias and their specification has received considerable attention \citep[see, \textit{e.g.},][]{tavare1986some,suchard2001bayesian,woodhams2015new,abadi2019model,fabreti2022bayesian}.
There are also cases in which the substitution process is itself of direct interest.
In phylogeographic modeling of rapidly evolving pathogens, character states may represent the geographic locations of sampled pathogen sequences and the substitution process describes the spread of the pathogens through geographic space.
In this case, inferring an appropriately parameterized substitution model can deliver insight into the factors driving the spread of disease \citep{lemey2014unifying,dudas2017virus,lemey2020accommodating}.
Questions regarding potentially coevolving traits can also be addressed with substitution models by expanding their state-space, such as to pairs of binary characters \citep{pagel2006bayesian}.

Popular phylogenetic substitution models are continuous-time Markov chain (CTMC) models parameterized in terms of one or more infinitesimal rate matrices
and branch lengths that measure the expected number of substitutions along each branch in the phylogeny.
As the number of possible characters $\alphabetsize$ in the data state-space grows, the number of potential parameters in each rate matrix $\alphabetsize \times (\alphabetsize - 1)$ quickly becomes large.
When inferring a phylogeny from nucleotide sequences, the rate matrix is small, and relatively parameter-rich models have been considered \citep{tavare1986some,yang1994estimating}.
But rate matrices for other data types can easily grow large: there are 20 amino acids, 64 codons, and phylogeographic analyses can easily encompass many dozens of locations \citep{lemey2014unifying,dudas2017virus,gao2022new}.
Models which account for heterogeneity of the substitution process along branches, such as Markov-Modulated models \citep{baele2021markov} may involve hundreds of parameters.
In such cases, inferring the unconstrained model, in which all $\order{S^2}$ non-diagonal elements are free parameters, has been historically prohibitive.
One reason is because the typical approach to Bayesian inference of substitution models is to use random-walk Metropolis-Hastings-based Markov chain Monte Carlo (MCMC) \citep{metropolis53,hastings1970monte}.
Such large rate matrices have many parameters which are (potentially) strongly correlated and often only weakly identifiable, rendering random-walk MCMC burdensome.

When confronted with substitution models for large state-spaces, the historical approach has been to find ways to reduce the number of free parameters in the model.
Amino acid models are often parameterized empirically \citep{dayhoff1978,whelan2001general}, requiring no free parameters for inference.
Codon models are often represented as combinations of site-level nucleotide models and codon-level processes \citep{yang2000codon}, some of which may be measured empirically \citep{hilton2018modeling}.
Such approaches reduce the number of parameters that must be inferred to $\bigO(\alphabetsize)$.
Another approach is to parameterize the rate matrix in terms of linear functions of observed covariates.
This generalized linear model (GLM) approach has been successful in phylogeographic inference, where observed covariates include factors like the distance between locations and air travel volumes \citep{lemey2014unifying,dudas2017virus}.
In addition to making inference tractable, the GLM approach can be used to quantify the strength of evidence for which factors do or do not affect the spread of infectious diseases.

In this paper, we demonstrate the utility of random-effects substitution models.
These models extend a wide class of CTMC models to incorporate additional rate variation by representing the original (base) model as fixed-effect model parameters and allowing the additional random-effects to capture deviations from the simpler process.
We demonstrate the utility of these models in full Bayesian inference and in optimization-based maximum \textit{a posteriori} (MAP) inference, and on a variety of exemplar evolutionary problems.
On a dataset of 583 SARS-CoV-2 genomes, an HKY model with random-effects captures known mutational biases in SARS-CoV-2 and is shown to be superior to the richest, general-time reversible (GTR) model.
Applied to a phylogeographic analysis of influenza A subtype H3N2, a GLM substitution model with random-effects provides evidence that air travel volume captures the geographic process of dispersal for all except a small set of pairs for which it underpredicts dispersal.
As a test for ecologically-dependent trait evolution, a random-effects pairwise-dependent substitution model finds no evidence for an effect of arboreality on the swimming mode in hylid tree frogs.
Using 28 Metazoan taxa, we find that notable deviations from the best-fit empirical amino acid model can be detected.

Before we can perform inference under these models, however, there stand two obstacles that must be overcome.
First and foremost, the parameter-space of the random effects models can be very large, and these parameters may be strongly correlated.
To overcome the dimensionality, we derive an efficient-to-compute approximation of the gradient of the phylogenetic (log-)likelihood with respect to (wrt) all of the parameters simultaneously of an arbitrary CTMC substitution model.
Notably the exact gradient is often computationally prohibitive.
We implement our approximate gradient in the phylogenetic inference software package BEAST 1.10 \citep{suchard2018bayesian} and the high-performance computational library BEAGLE 3 \citep{ayres2019beagle}, enabling the use of Hamiltonian Monte Carlo (HMC), a gradient-based alternative to random-walk MCMC \citep{neal2011mcmc}, for efficient parameter inference.
HMC leverages gradients to take bold steps through even highly correlated parameter spaces and can greatly increase MCMC efficiency.
Second, to avoid identifiability issues with potentially overparameterized models (such as inferring a $14 \times 14$ rate matrix based on a single observed character), we make use of the Bayesian Bridge prior \citep{polson2014bayesian} that is strongly regularizing and allows the data to decide which parameters are important to capture their variability.

The rest of this paper is structured as follows.
In the Methods section, we formally introduce the random-effects substitution model and the Bayesian Bridge prior distribution.
Then we derive our approximate gradient of the phylogenetic log-likelihood with respect to parameters of the substitution model.
We also provide an introduction to gradient-based inference.
In the Results section, we first investigate the increase in efficiency from using our approximate gradients compared to alternative approaches, both for optimization tasks and full Bayesian inference.
Then we apply our random-effects substitution model to a number of real-world examples.
We conclude by contemplating future approaches for improving inference efficiency and additional application areas where random-effects substitution models are likely to be useful.

\section{Methods}
In this paper we assume that there is a (possibly unknown) rooted phylogeny $\phylogeny$ with $\nTaxa$ tips that links the observed character sites and $\nTaxa - 1$ internal nodes.
We index the branch lengths and nodes such that the edge connecting node $\node$ to its parent $\parent$ has length $\branchlength_{\node}$.
Along each branch of the tree, we assume that characters arise from an alphabet of size $\alphabetsize$ and evolve under a CTMC model with instantaneous rate matrix $\ratematrix = \{ \ratematrixelement_{ij} \}$, where $\ratematrixelement_{ij} \ge 0$ for $i \neq j$ and the diagonal elements are fixed such that row-sums of $\ratematrix$ equal 0.
We measure branch length $\branchlength_{\node}$ in expected number of substitutions per site according to an arbitrary probability mass vector $\boldsymbol{\pi} = \left(\pi_1,\ldots,\pi_{\alphabetsize} \right)$ over the characters.
Usually, $\boldsymbol{\pi}$ is taken as the stationary distribution of $\ratematrix$.
To account for variation in evolutionary rates across sites in the character alignment, finite mixture models \citep[\textit{e.g.} the discrete-gamma model of][]{yang1994maximum} modulate the expected number of substitutions along all branches at a specific site.
Consider that there are $\nCategories$ rate categories, the rate scalar in the $r$th category is $\rateCategory_\category$, and the prior probability of being in any particular mixture category is $\prob{\rateCategory_{\category}}$.
Then, the finite-time transition probability matrix along branch $\node$ in category $\category$ is given by $\bP_{\node \category} = \exp(\rateCategory_{\category} \times \ratematrix \times \branchlength_{\node} )$, where we assume that $\ratematrix$ is normalized wrt $\bpi$.
The matrix $\bP_{\node \category}$ governs the probability of change from state $i$ to state $j$ along branch $\node$ in category $\category$.
Note that the subscripts on $\bP$ do not denote elements of the matrix but rather which of the $\nCategories \times (2\nTaxa - 2)$ distinct transition probability matrices---one for each branch and rate category---is under consideration.
In truth, the rate matrix $\ratematrix$ is a function of a vector of estimable parameters $\params$, specifically $\ratematrix(\params)$, but we suppress this notation for ease of presentation.

\subsection{Random-effects substitution models}
Random-effects substitution models are extensions of simpler CTMC substitution models.
We start with a \emph{base model}, which could be as simple as Jukes-Cantor \citep{jukes1969evolution}, as complex as a GLM substitution model \citep{lemey2014unifying} or anything in between.
This base model carries a rate matrix $\mathbf{B} = \{ b_{ij} \}$ and probability mass vector $\boldsymbol{\pi}_{\mathbf{B}}$ over the characters.
We define the random-effects substitution model rate matrix $\ratematrix$ using the following log-linear formulation,
\begin{equation}
\log \ratematrixelement_{ij} = \log b_{ij} + \randomeffect_{ij}
  \text{ for } i \neq j ,
  \label{eqn:refx_model}
\end{equation}
and set $\boldsymbol{\pi} = \boldsymbol{\pi}_{\mathbf{B}}$.
Intuitively, the random-effects $\randomeffect_{ij}$ are multiplicative real-valued parameters which enable each non-diagonal element to deviate from the values specified by the base model.
For example, $\randomeffect_{ij} = \log(2)$ doubles the rate implied by the base model, $\ratematrixelement_{ij} = 2 b_{ij}$.


Random-effects substitution models retain the basic structure of the base model that may be biologically or epidemiologically motivated, while allowing for potentially large deviations from this base process.
In our Metazoan example, we use the best-fitting empirical amino acid model as the base model, which provides a (potentially) realistic starting point without requiring any additional free parameters.
In our phylogeographic example, we start with an epidemiologically motivated model for the spread of rapidly evolving pathogens parameterized using air travel volume between countries with a GLM substitution model (though we could potentially use many more covariates) and allow the random-effects to capture shortcomings of this description.
For convenience, we shorthand the random-effects version of a substitution model $\basemodel + \text{RE}$, where $\basemodel$ is the base model (e.g.~HKY).

\subsection{Bayesian regularization}
In order to regularize this overparameterized model, we employ shrinkage priors on the random-effects.
Shrinkage priors impose sparsity on the model and allow us to perform simultaneous model selection, with the data choosing which parameters belong in or out of the model.
For the random-effects substitution model, a particular random-effect $\randomeffect_{ij}$ is excluded from the model if it is approximately 0.
Where discrete mixture models, such as those used in Bayesian stochastic search variable selection, carry a finite probability that a parameter achieves exactly 0, shrinkage priors instead have a large spike of prior density near 0.
While this occasionally makes it more difficult to declare if a parameter belongs in the model or not, the use of  purely continuous priors usually yields Markov chains that mix more efficiently and, importantly, permits the use of gradient-based inference.

The Bayesian Bridge prior on random-effect $\randomeffect$ has density,
\begin{equation}
  \pr(\randomeffect \mid \globalScale, \bridgeExponent) \propto \text{exp} \left( - \left| \frac{\randomeffect}{\globalScale} \right|^{\bridgeExponent} \right),
\end{equation}
where the global scale $\globalScale$ controls the overall spread of the distribution and the exponent $\bridgeExponent$ controls the shape.
At $\bridgeExponent = 2$, the density coincides with a standard normal distribution, while at $\bridgeExponent = 1$ it is the density of the Laplace distribution.
At lower exponent values, the distribution becomes increasingly peaked around 0 and induces sparsity.
We use $\bridgeExponent = 1/4$, which in practice imposes a useful level of sparsity without compromising MCMC convergence.
The global scale $\globalScale$ also plays an important role in determining the degree of regularization.
To both permit the data to inform the strength of regularization and efficient Gibbs sampling procedures, we place a Gamma(shape=1, rate=2) prior on $\globalScale^{-\bridgeExponent}$ \citep{nishimura2022shrinkage}.

The Bayesian Bridge distribution has particularly fat tails for lower $\bridgeExponent$.
This can hamper sampling, and it can allow parameter values which are, \textit{a priori}, unrealistically large (or small).
Particularly large random-effects can also cause numerical instability when exponentiating the substitution-rate matrix.
These effects can be ameliorated by the use of the shrunken-shoulder Bayesian Bridge \citep{nishimura2022shrinkage}.
This formulation includes a ``slab'' parameter $\zeta$ that controls the tails of the distribution.
Specifically, outside of $[-\zeta,\zeta]$, the tails of the shrunken-shoulder Bayesian Bridge become Normal(0,$\zeta^2$).
We set $\zeta = 2$, which \textit{a priori} specifies that it is unlikely for a particular element of the rate matrix to be more than $e^2 \approx 7$ times larger or smaller than specified by the base model.

\subsection{The gradient of the phylogenetic log-likelihood}

In this paper we are interested in the gradient of the phylogenetic log-likelihood with respect to the parameters of the substitution model.
The data $\data$ are a collection of homologous sites (columns in a multiple sequence alignment), $\data = (\data_1, \dots, \data_\nSites)$.
We will write the likelihood $\likelihoodAll{\params}$, and its gradient $\nabla \likelihoodAll{\params}$.
The gradient is the collection of derivatives wrt all substitution model parameters,
\begin{equation}
	\nabla \likelihoodAll{\params} = \left( \frac{\partial}{\partial \param_1} \likelihoodAll{\params}, \dots, \frac{\partial}{\partial \param_K} \likelihoodAll{\params} \ \right)\transpose .
\end{equation}

Under the common assumption that sites evolve independently and identically, we can express the log-likelihood as a sum across all $C$ sites, and hence derivatives of it as well.
We have
\begin{align}
	\frac{\partial}{\partial \param_k} \log \likelihoodAll{\params} &= \sum_{\site=1}^{\nSites} \frac{\partial}{\partial \param_k} \log \left( \sum_{\category=1}^\nCategories \likelihood{\params, \category} \prob{\rateCategory_{\category}} \right) \nonumber \\
	&= \sum_{\site=1}^{\nSites} \frac{\sum_{\category=1}^\nCategories \frac{\partial}{\partial \param_k} \likelihood{\params, \category} \prob{\rateCategory_{\category}}}{\sum_{\category=1}^\nCategories \likelihood{\params, \category} \prob{\rateCategory_{\category}}}
\end{align}
The denominator is simply the likelihood of a site $\likelihood{\params}$.
For simplicity, we will focus on the computation of $\frac{\partial}{\partial \param_k} \likelihood{\params, \category}$ for site $\site$ under rate category $\category$.

\subsubsection{Partial likelihood vectors and the phylogenetic likelihood}
We can, at any node $\node$ in the tree, compute the likelihood as
\begin{equation}
	\likelihood{\params} =
	 \sum_{r=1}^R  \left[ \bpostPartial_{\node \category \site}\transpose \bprePartial_{\node \category \site} \right] \prob{\rateCategory_{\category}}.
	 \label{eqn:likelihood_from_partials}
\end{equation}
The post-order partial likelihood vector $\bpostPartial_{\node \category \site}$, describes the probability, at node $\node$, in rate category $\category$, at the $\site$th site, of observing the tip-states in all tips which descend from the node, conditioned on the state at the node.
The pre-order partial likelihood vector, $\bprePartial_{\node \category \site}$, describes the \textit{joint} probability of observing the tip-states in all tips \textit{not} descended from the node and the state at the node.

\begin{figure}
	\centering
	\includegraphics[width=0.4\textwidth]{tikz_tree.pdf}
	\caption{
	A phylogenetic tree highlighting three key nodes.
	We will take node $\node$ as our focal node, which here has parent $\parent$ and sister $\sister$.
	We index branch lengths by the node which subtends them, such that the branch with length $\branchlength_{\node}$ is the branch leading to node $\node$.
	}%
	\label{fig:tree_with_labels}
\end{figure}

The post-order partial likelihood vectors are computed via pruning from the tip to the roots (a post-order traversal), for the tree in Figure~\ref{fig:tree_with_labels}, via
\begin{align}
	\bpostPartial_{\parent \category \site} = \bP_{\node \category} \bpostPartial_{\node \category \site} \circ \bP_{\sister \category} \bpostPartial_{\sister \category \site}.
\end{align}
The pre-order partial likelihood vectors are then computed in a root-to-tip pass through the tree (a pre-order traversal) using the relation
\begin{align}
	\bprePartial_{\node \category \site} = \bP_{\node \category}\transpose [\bprePartial_{\parent \category \site} \circ \bP_{\sister \category} \bpostPartial_{\sister \category \site}].
	\label{eqn:preorder}
\end{align}
We note that $\bpostPartial_{\node \category \site}$ is independent of $\bP_{\node \category}$, while $\bprePartial_{\node \category \site}$ is dependent on $\bP_{\node \category}$.

\subsubsection{A na\"ive derivative}

We can use the chain rule, and the fact that $\bpostPartial_{\node \category \site}$ is independent of $\bP_{\node \category}$, to obtain the derivative of the likelihood.
The partial likelihood vector representation of the phylogenetic likelihood allows us to isolate each branch's contribution to the derivative of the likelihood.
By summing over all branches, we can obtain the derivative as
\begin{align}
	\frac{\partial}{\partial \param_k} \likelihood{\params, \category} &= \sum_{\node = 1}^{2 \nTaxa - 2} \frac{\partial}{\partial \param_k} \bpostPartial_{\node \category \site}' \bprePartial_{\node \category \site} \nonumber \\
	&= \sum_{\node = 1}^{2 \nTaxa - 2} \bpostPartial_{\node \category \site}' \frac{\partial}{\partial \param_k} \bprePartial_{\node \category \site} \nonumber \\
  &= \sum_{\node = 1}^{2 \nTaxa - 2} \bpostPartial_{\node \category \site}' \left( \frac{\partial}{\partial \param_k} \bP_{\node \category} \right)' [\bprePartial_{\parent} \circ \bP_{\sister \category} \bpostPartial_{\sister}] \nonumber \\
  &= \sum_{\node = 1}^{2 \nTaxa - 2} \bpostPartial_{\node \category \site}' \left( \frac{\partial}{\partial \param_k} \bP_{\node \category} \right)' \bprePrePartial_{\node \category \site} \nonumber \\
  &= \sum_{\node = 1}^{2 \nTaxa - 2} \bpostPartial_{\node \category \site}' \left( \frac{\partial}{\partial \param_k} e^{\ratematrix \times t_{\node}  \times \rateCategory_{\category}} \right)' \bprePrePartial_{\node \category \site} \nonumber \\
  &= \sum_{\node = 1}^{2 \nTaxa - 2} \bpostPartial_{\node \category \site}' \left( \sum_{i=1}^{\alphabetsize} \sum_{j=1}^{\alphabetsize} \frac{\partial e^{\ratematrix \times t_{\node} \times \rateCategory_{\category}}}{\partial \ratematrixelement_{ij}} \frac{\partial \ratematrixelement_{ij}}{\partial \param_k} \right)' \bprePrePartial_{\node \category \site}.
  \label{eqn:naive_phylo_gradient}
\end{align}
In the third-to-last step, we defined $\bprePrePartial_{\node \category \site} = [\bprePartial_{\parent} \circ \bP^{(\sister)} \bpostPartial_{\sister}]$ to simplify the notation and focus on the part of the equation which depends on $\param_k$.
In the last step, we employed the matrix chain rule \citep{petersen2008matrix}.
The term $\partial \exp(\ratematrix \times t_{\node} \times \rateCategory_{\category})/\partial \ratematrixelement_{ij}$ is the derivative of the matrix exponential with respect to one of the elements of the rate matrix, which we discuss in more detail in Section~\ref{sec:approximation}.

As we discuss in Section~\ref{sec:supplemental_computational_complexity}, the computational cost of obtaining $\partial \exp(\ratematrix \times t_{\node} \times \rateCategory_{\category})/\partial \ratematrixelement_{ij}$ is $\bigO(\alphabetsize^3)$.
The sum in Equation~\ref{eqn:naive_phylo_gradient} requires this quantity for all $\alphabetsize^2$ elements in $\ratematrix$ and for each of the $2 \nTaxa - 2$ branches, making the cost to compute the derivative $\bigO(\nTaxa \alphabetsize^5)$.
Obtaining the gradient requires using Equation~\ref{eqn:naive_phylo_gradient} for all $K$ substitution model parameters, making the cost of the gradient $\bigO(K \nTaxa \alphabetsize^5)$.
For random effects models, this is $\bigO(\nTaxa \alphabetsize^7)$.
Such costs are prohibitive for even moderate $\alphabetsize$, so we turn our attention now to improving the computational efficiency of gradient computations.

\subsubsection{Reducing the computational complexity}\label{sec:mapping}

We can reformulate the na\"ive approach of Equation~\ref{eqn:naive_phylo_gradient} to produce a more efficient gradient computation.
By rearranging the order of summation, we can disentangle the derivative of the rate matrix wrt its elements from the derivative of its elements wrt model parameters.
Specifically,
\begin{align}
	\frac{\partial}{\partial \param_k} \likelihood{\params, \category} &= \sum_{\node = 1}^{2 \nTaxa - 2} \bpostPartial_{\node \category \site}' \left( \sum_{k=1}^{\alphabetsize} \sum_{l=1}^{\alphabetsize} \frac{\partial e^{\ratematrix \times t_{\node} \times \rateCategory_{\category}}}{\partial \ratematrixelement_{ij}} \frac{\partial \ratematrixelement_{ij}}{\partial \param_k} \right)' \bprePrePartial_{\node \category \site} \nonumber \\
	&= \sum_{\node = 1}^{2 \nTaxa - 2} \sum_{k=1}^{\alphabetsize} \sum_{l=1}^{\alphabetsize} \bpostPartial_{\node \category \site}' \left( \frac{\partial e^{\ratematrix \times t_{\node} \times \rateCategory_{\category}}}{\partial \ratematrixelement_{ij}} \right)' \bprePrePartial_{\node \category \site} \frac{\partial \ratematrixelement_{ij}}{\partial \param_k} \nonumber \\
	&= \sum_{\node = 1}^{2 \nTaxa - 2} \mapping_{k} \vecm(\allQderivs_{\node}),
	\label{eqn:almost_the_full_mapping}
\end{align}
where the $\vecm$ operator makes the matrix $\allQderivs_{\node}$ into a column vector by stacking the columns on top of each other, and we obtain the last line by defining two new quantities which we will now discuss.

The matrix $\mapping$ is a $K \times \alphabetsize^2$ mapping matrix, which stores in each row $k$ a vector of the partial derivatives of all elements of $\ratematrix$ wrt $\param_k$,
\begin{equation}
	\mapping_{k \boldsymbol{\cdot}} = \left( \frac{ \partial \ratematrixelement_{11} }{ \partial \param_k}, \dots, \frac{ \partial \ratematrixelement_{\alphabetsize \alphabetsize} }{ \partial \param_k} \right).
\end{equation}
The matrix $\allQderivs_{\node \category} = \{ \allQderivselement_{\node \category i j} \}$ contains the contribution of branch $\node$ to the derivative of the phylogenetic likelihood wrt the $ij^{\text{\tiny th}}$ entry of $\ratematrix$ in rate category $\category$.
Specifically,
\begin{equation}
  \allQderivselement_{\node \category i j} = \bpostPartial_{\node \category \site}' \frac{\partial}{ \partial \ratematrixelement_{ij}} \bprePartial_{\node \category \site} = \bpostPartial_{\node \category \site}' \left( \frac{\partial}{\partial \ratematrixelement_{ij}} e^{\ratematrix \times t_{\node} \times \rateCategory_{\category}} \right)' \bprePrePartial_{\node \category \site} .
  \label{eqn:all_q_derivs_element}
\end{equation}


We arrive at the entire gradient (as opposed to a single entry) and increase computational efficiency by replacing $\mapping_{k \boldsymbol{\cdot}}$ with $\mapping$ in Equation~\ref{eqn:almost_the_full_mapping} and rearranging,
\begin{align}
	\nabla \likelihood{\params, \category} &= \sum_{\node} \mapping \vecm(\allQderivs_{\node}) \nonumber \\
	&= \mapping \sum_{\node} \vecm(\allQderivs_{\node}).
	\label{eqn:derivs_with_mappings}
\end{align}

This approach separates the gradient of the phylogenetic likelihood wrt model parameters into two pieces, a gradient of the phylogenetic likelihood wrt elements of the rate matrix, and a gradient of the elements of the rate matrix wrt the model parameters.
The result is the intermediate quantity $\allQderivs_{\node}$ that can be obtained with only a single computation of the derivative of a matrix exponential per branch.
As this quantity can be summed across the tree prior to mapping it to the substitution model parameters, $\bigO(\nTaxa)$ matrix multiplications are avoided.
The result is that this approach is $\bigO(K \alphabetsize^2 + \nTaxa \alphabetsize^5)$ rather than $\bigO(K \nTaxa \alphabetsize^5)$.
For random-effects substitution models, this is the difference between a $\bigO(\nTaxa \alphabetsize^7)$ computation and a $\bigO(\alphabetsize^4 + \nTaxa \alphabetsize^5) = \bigO(\nTaxa \alphabetsize^5)$ computation.
Note that this approach works for branch-specific models as well, by specifying the mapping matrix $\mapping$ appropriately.

\subsubsection{Efficiently approximating the derivative of the matrix exponential}\label{sec:approximation}
We now turn our attention to an efficient approximation to the derivative of a matrix exponential.
The derivative of a matrix exponential can be represented as a power-series \citep[][Equation 103]{najfeld1995derivatives},
\begin{equation}
	\frac{\partial}{ \partial \ratematrixelement_{ij}} e^{\ratematrix \branchlength} = e^{\ratematrix \branchlength} \sum_{x=0}^{\infty} \frac{\branchlength^{x+1}}{(x+1)!} \{\indicatormatrixij,\ratematrix^x\},
	\label{eqn:NH_103}
\end{equation}
where $\indicatormatrixij$ is a matrix which is 0 for all but the $(ij)$th entry, which is 1.
The matrix commutator power $\{\indicatormatrixij,\ratematrix^x\}$ for non-negative integer $x$ is defined recursively \citep{najfeld1995derivatives}, such that $\{\indicatormatrixij,\ratematrix^0\} = \indicatormatrixij$ and $\{\indicatormatrixij,\ratematrix^x\} = [\{\indicatormatrixij,\ratematrix^{x-1}\},\ratematrix]$ (where $[\bA,\bB]$ is the matrix commutator $\bA \bB - \bB \bA$).

The first-order approximation to Equation~\ref{eqn:NH_103} is taken by keeping only the $x=0$ term, yielding
\begin{equation}
	\frac{\partial}{ \partial \ratematrixelement_{ij}} e^{\ratematrix \branchlength} \approx \branchlength e^{\ratematrix \branchlength} \indicatormatrixij.
	\label{eqn:exponential_approximate_gradient}
\end{equation}
We can use this first-order approximation to approximate $\allQderivs_{\node}$ on each branch.
Specifically,
\begin{align}
	\allQderivselement_{\node i j} &= \bpostPartial_{\node \category \site}' \frac{\partial}{ \partial \ratematrixelement_{ij}} \bprePartial_{\node \category \site} \nonumber \\
  &\approx \bpostPartial_{\node \category \site}' [(t_{\node} \times \rateCategory_{\category} \times e^{\ratematrix \times t_{\node} \times \rateCategory_{\category}} \indicatormatrixij)' [\bprePartial_\parent \circ (e^{\ratematrix \branchlength_{\sister} \times \rateCategory_{\category}} \bpostPartial_{\sister})]] \nonumber \\
	&= t_{\node} \times \rateCategory_{\category} \times \bpostPartial_{\node \category \site}' [ \indicatormatrix_{ji} (e^{\ratematrix \times t_{\node} \times \rateCategory_{\category}})' [\bprePartial_\parent \circ (e^{\ratematrix \times t_{\node} \times \rateCategory_{\category}} \bpostPartial_{\sister})]] \nonumber \\
	&= t_{\node} \times \rateCategory_{\category} \times \bpostPartial_{\node \category \site}' \indicatormatrix_{ji} \bprePartial_{\node \category \site} \nonumber \\
	&= t_{\node} \times \rateCategory_{\category} \times \prePartial_{\node i} \postPartial_{\node j} ,
	\label{eqn:approximate_gradient}
\end{align}
where we get from line 3 to line 4 by noting that $(e^{\ratematrix \times t_{\node} \times \rateCategory_{\category}})' = \bP'_{\node \category}$ and applying Equation~\ref{eqn:preorder}.
Intuitively, we have the (approximate) derivative with respect to an $i \rightarrow j$ transition depending on the pre-order partial likelihood in state $i$ and the post-order partial likelihood in state $j$.

Equation~\ref{eqn:approximate_gradient} means that we can write our approximate $\allQderivs_{\node}$ as an outer product,
\begin{align}
  \allQderivs_{\node \category \site} \approx \branchlength_{\node} \times \rateCategory_{\category} \times \bprePartial_{\node \category \site} \otimes \bpostPartial_{\node \category \site}.
  \label{eqn:approximate_gradient_outer_product}
\end{align}
%
This means that we can obtain all $\alphabetsize^2$ entries of $\allQderivs_{\node \category \site}$ in $\bigO(\alphabetsize^2)$, which is much more efficient than the $\bigO(\alphabetsize^5)$ cost of the non-approximate computation.
Thus, with this approximation and the mapping approach outlined in the previous section, the (approximate) substitution gradient can be obtained in $\bigO(K \alphabetsize^2 + \nTaxa \alphabetsize^3)$, rather than the $\bigO(K \alphabetsize^2 + \nTaxa \alphabetsize^5)$ cost suggested by Equation~\ref{eqn:derivs_with_mappings} or the $\bigO(K \nTaxa \alphabetsize^5)$ cost suggested by Equation~\ref{eqn:naive_phylo_gradient}.
We will denote the approximate gradient that comes from using this approximation to $\allQderivs_{\node \category \site}$ in Equation~\ref{eqn:derivs_with_mappings} as $\approxnabla \log \likelihoodAll{\params}$.

\subsection{Gradient-based inference}\label{sec:inference}
\subsubsection{Hamiltonian Monte Carlo with surrogate trajectories}
\newcommand{\mom}{\boldsymbol{\xi}}
\newcommand{\zero}{\boldsymbol{0}}
\newcommand{\cov}{\mathbf{M}}

HMC \citep{duane1987hybrid,neal2011mcmc} is an advanced MCMC algorithm that uses information captured by the log-posterior gradient to efficiently traverse a model's parameter space.
The method constructs an artificial Hamiltonian system by augmenting the parameter space to include an auxiliary Gaussian `momentum' variable $\mom \sim$ MVN$_K(\zero,\cov)$ that is independent from the target variable $\params$ by construction.
Letting $\pi(\cdot)$ denote the posterior density and $\phi_\cov(\cdot)$ denote the density of a centered Gaussian with covariance $\cov$, the resulting Hamiltonian energy function is the negative logarithm of the joint distribution over $\params$ and $\mom$
\begin{align*}
	H(\params,\mom) \,=\, - \log \left[ \pi(\params) \phi_\cov(\mom) \right] \,\propto\, - \log \pi(\params)  + \frac{1}{2} \mom^T \cov^{-1} \mom \, ,
\end{align*}
and Hamilton's equations are
\begin{align*}
	\dot{\params} &= \frac{\partial H}{\partial \mom} = \cov^{-1} \mom \\ \nonumber
	\dot{\mom}    &= - \frac{\partial H}{\partial \params} =  \nabla \log \pi(\params)  \, .
\end{align*}
On the one hand, one may show that the action of the dynamical system that satisfies these equations leaves the target $\pi(\cdot)$ invariant thanks to the reversibility, volume preservation and energy conservation of Hamiltonian dynamics.
On the other hand, closed-form descriptions of these dynamics are rarely available for arbitrary target distributions, leading to the need for computer intensive approximations.
In particular, the St\"{o}rmer-Verlet (velocity Verlet) or leapfrog method \citep{leimkuhler2004simulating} has become the numerical integrator of choice for obtaining discretized trajectories within HMC.
Beginning at time $\tau$ and letting $\epsilon>0$ be small, a single leapfrog iteration proceeds thus:
    \begin{align}\label{eq:leap}
	\mom\left(\tau + \frac{\epsilon}{2}\right) &= \mom(\tau) + \frac{\epsilon}{2} \nabla \log \pi(\params(\tau))  \\  \nonumber
	\params(\tau+\epsilon) &= \params(\tau) + \epsilon \, \cov^{-1}\mom(\tau+\frac{\epsilon}{2}) \\ \nonumber
	\mom(\tau + \epsilon) &= \mom\left(\tau+\frac{\epsilon}{2}\right) + \frac{\epsilon}{2} \nabla \log \pi(\params(\tau+\epsilon))  \, .
\end{align}
Trajectories arising from concatenated leapfrog iterations maintain some of the desirable qualities of the exact Hamiltonian dynamics (reversibility, voume preservation) but no longer conserve energy.
For this reason, the HMC algorithm features two distinct steps: first, it uses approximate dynamics to generate a proposal; and second, it uses a Metropolis correction \citep{metropolis53,hastings1970monte} to account for approximation error and leave the target distribution invariant.

Indeed, HMC's Metropolis correction allows for additional deviations from Hamiltonian dynamics over and beyond numerical discretization schemes such as \eqref{eq:leap}.
\emph{Surrogate HMC} methods seek to improve computational performance of HMC by approximating the log-posterior gradient with less expensive surrogate models including, e.g., piecewise-approximations \citep{zhang2017precomputing}, Gaussian processes \citep{rasmussen2003gaussian,lan2016emulation} or neural networks \citep{zhang2017hamiltonian,li2019neural}.
Directly relevant to the present work, \citet{li2019neural} show the validity of replacing the log-posterior gradient $\nabla \log \pi(\cdot)$ within the leapfrog method \eqref{eq:leap} with \emph{any} vector function $\mathbf{g}: \mathbb{R}^K \rightarrow \mathbb{R}^K$.
In particular, such an approach maintains the reversibility and volume preservation of Hamiltonian dynamics and, when paired with Metropolis corrections, leaves the target posterior distribution invariant.
In the present work, we select $\mathbf{g} = \approxnabla\log \likelihoodAll{\params} + \nabla \log \prob{\params}$, the approximate posterior gradient obtained by using our approximation to the gradient of the phylogenetic log-likelihood and the true gradient for the prior.
In the supplementary materials, we discuss an alternative justification.

\subsubsection{Maximization-based approaches}
While this paper is primarily interested in Bayesian inference via HMC, the approximate gradients we have derived are also useful in maximization-based approaches to estimation such as maximum likelihood (ML) estimation and maximum \textit{a posteriori} (MAP) estimation.
Let us call the function we wish to optimize $f(\params)$ that is a function of our model parameters $\params$.
If $f(\params) = - \log \likelihoodAll{\params}$, then minimizing $f(\params)$ is equivalent to maximizing the likelihood and our maximum-likelihood parameter estimates are
$
  \hat{\params} = {\argmin}_{\params}\ f(\params)
$.
MAP estimation instead uses $f(\params) = - \log \likelihoodAll{\params} - \log \pr(\params)$ and maximizes the joint posterior density.
We discuss the details of the optimization approaches implemented in BEAST 1.10 \citep{suchard2018bayesian} in Supplemental Section~\ref{sec:bfgs}.

\section{Results}

\subsection{C to T bias in SARS-CoV-2 evolution}
\label{sec:SC2}

The mutational profile of SARS-CoV-2 has been intensely scrutinized, one feature in particular which has been noted is a strongly increased rate of C$\rightarrow$T substitutions over the reverse T$\rightarrow$C substitutions.
We note that while RNA viruses like SARS-CoV-2 use uracil (U) in place of thymine (T), it is generally coded as thymine--the coding of adenosine (A), cytosine (C), and guanine (G) are unchanged.
The elevation of one direction of substitution over its reverse is a violation of the common phylogenetic assumption of reversibility made by the GTR \citep{tavare1986some} family of substitution models.
Random-effects substitution models are suitable for addressing this model violation, in particular we consider an HKY+RE substitution model.
In principle we could choose any GTR-family model.
HKY represents a balance between simplicity the simplicity of JC+RE (where the random-effects would also have to account for uneven nucleotide frequencies) and the complexity of GTR+RE (where the random-effects only capture nonreversibilities).
The rate matrix is
\begin{equation}
	\log \ratematrixelement_{ij} =  \log \kappa \times \indicator(ij \in \mathcal{T}) + \log \pi_j + \randomeffect_{ij},
\end{equation}
where $\kappa$ is the HKY parameter governing relative rate of transitions to transversions, $\indicator(ij \in \mathcal{T})$ indicates that the $i$ to $j$ change is a transition, and $\bpi$ are the HKY stationary frequencies.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.85\textwidth]{refx_vs_gtr_rates.pdf}
	\caption{
	Posterior distributions of the 12 non-diagonal elements of the inferred rate matrices for the dataset of \citet{pekar2021timing}.
	The solid line is the posterior median, the shaded region the 50\% CI.
	The whiskers extend to the posterior samples farthest from the median but within 1.5$\times$ the interquartile range.
  	By comparing HKY+RE (which is not constrained by the assumption of reversibility) to GTR, we can see that the assumption of reversibility leads to the overestimation of the T$\rightarrow$C (and T$\rightarrow$G) rates and the underestimation of the C$\rightarrow$T (and G$\rightarrow$T) rates.
	}%
	\label{fig:refx_vs_gtr_rates}
\end{figure}


We apply this HKY+RE model to a dataset of 583 SARS-CoV-2 sequences from \citet{pekar2021timing}.
Consistent with previous studies \citep[\textit{e.g.}][]{matyavsek2020mutation,tonkin2021patterns}, we find evidence for a greatly elevated rate of C$\rightarrow$T substitutions, as well as an elevated G$\rightarrow$T rate (Figure~\ref{fig:refx_vs_gtr_rates}).
We can test the support for nonreversibilities, for example the difference between the C$\rightarrow$T and T$\rightarrow$C rates, with Bayes Factors.
The fact that a model with the C$\rightarrow$T and T$\rightarrow$C rates equal (reversible wrt C$\leftrightarrow$T) is nested within the random-effects model allows us to use the Savage-Dickey ratio \citep[\textit{e.g.}][]{wagenmakers2010bayesian} to compute the Bayes Factor from the posterior distribution of the random-effects model, as we discuss in Supplemental Section~\ref{sec:savage}.
(There is no need to fit any additional models or estimate marginal likelihoods directly.)
The Bayes Factor provides ``very strong'' \citep{kass1995bayes} support for the nonreversibility of C$\rightarrow$T and G$\rightarrow$T rates (over the reversible model).

\begin{figure}[h]
	\centering
	\includegraphics[width=0.75\textwidth]{SC2_model_adequacy.pdf}
	\caption{
	Posterior predictive distributions of the covariances of the proportions of each nucleotide (denoted $p_A$, $p_C$, $p_G$ and $p_T$) across sites in the alignment (histograms) compared to the true values (vertical black lines).
  	The HKY+RE predictive distributions all closely align with the observed values while all but one of the GTR predictive distributions are discordant.
	}%
	\label{fig:sc2_pps}
\end{figure}

Given the strong evidence for nonreversibilities, we sought to investigate the issue of the adequacy of reversible models (namely GTR) using posterior predictive model checks.
In a posterior predictive framework, a summary of the observed dataset is compared to the distribution of summaries of datasets produced by drawing from the posterior distribution on model parameters.
Broadly, if the model fits the data well, we expect that the predicted summaries will match the observed values, while if the fit is poor there will be a mismatch.
As our test statistics, we consider all pairwise covariances of the proportion of each nucleotide (A, C, G, and T) across the alignment (we discuss this in more detail in Supplemental Section~\ref{sec:PPS}).
These test statistics clearly demonstrate that the HKY+RE model better captures the evolutionary processes at hand (Figure~\ref{fig:sc2_pps}).
Compared to inference using GTR, the analysis with HKY+RE produces notably higher support for the root-most divergence (the 95\% credible set includes 67 possible resolutions for GTR and 1 for HKY+RE) and infers a root time approximately 5 days earlier.


\subsection{Phylogeography of influenza from 2002-2007}

For a larger state-space example of random-effects substitution models, we consider the global spread of human influenza A virus (subtype H3N2) from 2002 to 2007.
\citet{lemey2014unifying} examined the movement patterns between 14 distinct air travel communities using 1441 viral genomes.
The authors used a generalized linear model (GLM) to parameterize the spread of the virus between these communities as a function of a number of covariates, and discovered that the most consistently supported predictor of spread between communities was the volume of air traffic.

We re-analyze this dataset using a GLM substitution model with random-effects.
We now briefly review the setup of a GLM substitution model, and our random-effects extension.
For each pair of locations $i$ and $j$, let $\bX_{ij}$ be a vector of $P$ predictors of the rate of movement from $i$ to $j$ (these may depend on the source $i$, the destination $j$, or both) with associated coefficients $\bbeta$.
A GLM substitution model with random-effects defines the rate matrix through
\begin{equation}
	\log \ratematrixelement_{ij}  = \bX_{ij}' \bbeta + \epsilon_{ij}.
\end{equation}
This is a log-linear model, in which the GLM defines a substitution rate based on predictors and the estimated coefficients, and the random-effects allow for deviations from the model's predictions.


In particular, we employ a simple GLM with only air traffic included as a predictor.
This approach allows us to determine how well air traffic volume predicts the spread of influenza A virus in the mid-2000s.
If most random-effects are negligible, then air traffic volume alone is perhaps adequate for modeling the spread of influenza in this time frame.
On the other hand, if many or most random-effects are not negligible, although air traffic volume may be an important model component, it is not sufficient to explain spread, absent random-effects.
While \citet{lemey2014unifying} used spike-and-slab priors on $\bbeta$ in a Bayesian model averaging approach, since we are using only predictors identified previously to be important, we use a Normal prior instead (corresponding to the slab in the original study).
We apply Bayesian Bridge priors for the random-effects.

We find that air traffic volume sufficiently explains the viral spread between most communities.
That is, for most community pairs, the posterior distribution of the random-effect indicates that the parameter has been declared ``insignificant'', and is a spike centered at 0 (Figure~\ref{fig:flu_refx}).
However, for 5 pairs of communities (from the United States to Japan and South America; from China to the United States and Japan; from Oceania to the United States), the inferred random-effect is clearly significant and strongly positive, indicating 6- to 12-fold higher dispersal than predicted by travel.
There is intermediate support for an additional 6 random-effects (from the United States to Oceania, Russia, and Southeast Asia; from China to Oceania; from Japan to Oceania; from Southeast Asia to Oceania) which indicate 2- to 5-fold higher dispersal than predicted by travel.
Given the offset seasons between hemispheres, some of these connections likely do not represent biologically meaningful connections, and may potentially be attributed to sampling biases.
A comparison of the number of samples in the dataset to the population sizes of the regions (a rough proxy for the number of infections in the regions) reveals that the United States, Oceania, and Japan are strongly oversampled.
Thus, sampling biases likely explains many of the significant random-effects, including the between-hemisphere connections.
As China is not particularly oversampled, the elevated rates of transmission from China may represent source-sink dynamics which are not captured by air travel alone, rather than sampling bias.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.67\textwidth]{flu_refx_heatmap_circles.pdf}
	\caption{
	Summary of all 182 random-effects for the influenza A virus (subtype H3N2), shown in the format of the rate matrix, with the source in rows and destination in columns.
	The circle in each square is colored by the posterior median random-effect.
	The size of the circle denotes how strong the posterior support is that a random-effect is in the model.
	Specifically, the radius corresponds to the posterior sign probability (the probability that the sign of the random-effect is the same as the sign of the posterior median).
	When the prior dominates the posterior distribution, a random-effect gains a larger posterior mass at 0 and becomes increasingly symmetric, the median approaches 0, and the posterior sign probability approaches 0.5.
	When the data are strongly informative, the posterior distribution moves away from 0 and the posterior sign probability gets larger.
	The random-effects which are most strongly supported are all positive, indicating that air travel underpredicts dispersal for those pairs of locations.
	}%
	\label{fig:flu_refx}
\end{figure}

\subsection{Analysis of paired macroevolutionary traits}

Random-effects substitution models can also be used to test for dependent substitution processes between multiple characters as follows.
Let us assume we have two characters of interest, $\bY_1$ and $\bY_2$.
These characters could be morphological, behavioral, or even ecological traits.
If these characters evolved independently along the phylogeny $\phylogeny$, we could model this with two rate matrices, $\ratematrix^1$ and $\ratematrix^2$, a (strict) clock rate which defines the rate of change (in substitutions per year or million years) for $\bY_2$, and a relative rate parameter $\mu$ which defines how much faster (or slower) $\bY_1$ evolves compared to $\bY_2$.
We can define a composite character from $\bY_1$ and $\bY_2$ by considering both states simultaneously.
This yields a new character $\bY$ which is the cartesian product of the two state-spaces, with the combined state-space size $\alphabetsize = \alphabetsize_1 \times \alphabetsize_2$.
The rate matrix $\ratematrix$ for the combined character is 0 for any double substitution and for any single substitution is defined by $\ratematrix^1$ or $\ratematrix^2$ depending on which character changes.
Written on the log-scale, the (unnormalized) rate matrix is given by
\begin{equation}
	\log \ratematrixelement_{ij}  = \\
	\begin{cases}
    \log \ratematrixelement^1_{ij}   & \mbox{if i$\rightarrow$j substitution in $Y_1$}\\
		\log \mu + \log \ratematrixelement^2_{ij}  & \mbox{if i$\rightarrow$j substitution in $Y_2$}\\
		-\infty & \mbox{for all double substitutions}.\\
	\end{cases} \nonumber
\end{equation}
We can test for departures from independent evolution by allowing the state of one character to modulate the rates of change between states in the other through the addition of random-effects.

We employ this random-effects dependent morphological evolution model on a dataset of 29 species of frogs in the family Hylidae (subfamily Hylinae).
We focus on two traits, one ecological and one behavioral.
The ecological trait is the habitat, which is characterized as either arboreal or understory.
The behavioral trait is the swimming mode, which is characterized by whether the back legs move in an alternating or simultaneous fashion or whether both types are observed.
To determine the structure of the underlying independent-trait models, we first fit the independent model using asymmetric rates for both traits.
Bayes Factors show no evidence for any model more complex than the Mk (Jukes-Cantor-like) model \citep{lewis2001likelihood}.

In particular, we are interested in whether the degree of arboreality, defined as habitat preference, impacts the swimming mode, as canopy-dwelling species move the back legs in an alternating fashion while climbing.
Thus, we place random-effects only in the direction of arboreality affecting swimming mode.
Letting $\bY_1$ be arboreality and $\bY_2$ be swimming mode, the unnormalized rate matrix for our random-effects substitution model is,
\begin{equation}
	\log \ratematrixelement_{ij} = \\
	\begin{cases}
    0 & \mbox{if i$\rightarrow$j substitution in $Y_2$}\\
    \log \mu + \randomeffect_{ij} & \mbox{if i$\rightarrow$j substitution in $Y_2$ and $Y_1$ indicates canopy-dwelling}\\
	\log \mu & \mbox{if i$\rightarrow$j substitution in $Y_2$ and $Y_1$ indicates understory-dwelling}\\
	-\infty & \mbox{for all double substitutions}.\\
	\end{cases} \nonumber
\end{equation}

We infer no effect of arboreal habitat on the swimming mode, that is, all random-effects have clearly been deemed insignificant.
We also infer that the rate of habitat evolution is roughly twice that of swimming-mode evolution ($\mu \approx 0.5$, 95\% CI 0.23-1.16).
There are two important caveats to these results.
First, with only 29 species, the power to detect dependent evolution is likely low unless the effect is quite large.
Secondly, by only modeling two traits, we are missing out on possible interactions between other aspects of ecology (such as the aquatic environments the species make use of) and morphology (such as the lengths of limbs and digits) which might modulate this relationship.


\subsection{Performance gains from gradients}

\subsubsection{Performance improvements versus standard MCMC approaches}

For inferring random-effects in nucleotide substitution models, we find a notable improvement in efficiency using HMC with our approximate gradients over using MH-MCMC.
For our measure of efficiency, we consider the number of effectively independent samples taken per second (ESS/s).
This measure incorporates both the increased ability of HMC to move through parameter space, as well as the increased cost per MCMC move required for repeated evaluation of the gradient.
When applied to nucleotide models (HKY+RE) to infer the tree from sequence data, we observe an average increase in efficiency of 6.6 fold, and an increase in the minimum efficiency of 14.8-fold (Figure~\ref{fig:ess_per_second}).
For the larger state-space of the flu phylogeographic example (14 discrete areas), where we average across a set of posterior samples of the tree from the original study, we find an average increase in efficiency of 20.2-fold and an increase in minimum efficiency of 33.6-fold (Figure~\ref{fig:ess_per_second}).
Timing was done on a Macbook Pro with an 8-core CPU M1 Pro chip and 32GB of memory.

\begin{figure}[!htp]
	\centering
	\includegraphics[width=0.7\textwidth]{efficiency.pdf}
	\caption{
		Efficiency, in effective samples per second (ESS/s), of HMC versus MH-MCMC for inferring the random-effects substitution models for both the 12 random-effects in an HKY+RE model and the 182 random-effects in the flu discrete phylogeographic analysis.
		Both data sets show markedly improve estimation efficiency as a result of employing HMC with approximate gradients.
		}%
	\label{fig:ess_per_second}
\end{figure}


\subsubsection{Performance gains for MAP estimation}\label{sec:metazoa}
To test our approximate gradients in an optimization setup, we require an alternative approach for computing gradients.
In particular, we use numerical gradients which represent a gold standard for assuring the correctness of gradient implementations.
For random-effects substitution models the computational complexity of numerical gradients, $\bigO(\nTaxa \alphabetsize^5)$, is the same as the non-approximate gradient with mapping presented in Equation~\ref{eqn:derivs_with_mappings}, making it an appropriate comparison for speed as well.
(We discuss numerical approaches in more detail in Supplemental Section~\ref{sec:numerical_approach}.)
For our example, we seek to estimate the parameters of an amino acid substitution model.
We use a 28 taxon, 445 site amino alignment (locus OG198) from \citet{borowiec2015extracting} which spans the Metazoan tree of life.
We fix the authors' inferred maximum-likelihood tree for the locus \citep{borowiec2015dryad}, and following the authors' choice of best-fit amino acid model, we fit an LG \citep{le2008improved} substitution model with random-effects.

We perform MAP estimation of the model parameters using a BayesianBridge($\globalScale = 1$,\,$\bridgeExponent = 1/4$) prior.
We compare approximate MAP inference using the approximate likelihood gradient derived in this paper to correct MAP inference using numerical (finite-difference) likelihood gradients.
Using numerical gradients, the L-BFGS optimizer requires 1.21 seconds per iteration, while using the approximate gradients 0.023 seconds are required per iteration.
Thus, the approximate gradients are computed over 50 times faster for estimates that are essentially indistinguishable (Figure~\ref{fig:optimization}).
(Timing for optimization was also done on a Macbook Pro with an 8-core CPU M1 Pro chip and 32GB of memory.)
It is also possible to combine both the approximate and numerical gradients.
That is, first we could run until convergence with the approximate gradient, which is faster than the numerical gradient.
Then, we could use the more accurate numerical gradient to get the exact maximum.

\begin{figure}
	\centering
	\includegraphics[width=0.75\textwidth]{optimization.pdf}
	\caption{
		a) Comparison of MAP estimates of random effects in a LG+RE model when estimated using approximate and numeric gradients.
  		b) Histogram of MAP estimates of random effects in a LG+RE model.
  		The shading represents the average (Laplace-approximate) sign probability of the random-effects in the bin, representing the (approximate) strength of evidence for the importance of the random-effect.
  		Most random effects with $|\randomeffect_{ij}| > 2$ are confidently non-zero.
	}%
	\label{fig:optimization}
\end{figure}

Once the posterior mode has been found, it is possible to employ a Laplace approximation \citep{kass1995bayes} and approximate the posterior distribution as a multivariate normal around the mode.
The covariance matrix is estimated as the inverse of the (negative) Hessian matrix.
This allows us to approximate the posterior probability that a coefficient is non-zero (Figure~\ref{fig:optimization}).
The primary time-cost for the Laplace approximation is computing the Hessian matrix, which takes approximately 6.6 seconds using the approximate gradient.
The numerical gradients do not produce an invertible Hessian and thus cannot be used to estimate the covariance matrix.


\section{Discussion}
In this paper, we have demonstrated the versatility and usefulness of random-effects substitution models.
By wrapping around a simpler base substitution model, random-effects substitution models enable increased flexibility while retaining the useful structure of the base model.
Applied to a dataset of 583 SARS-CoV-2 sequences, an HKY+RE model picks up strong C$\rightarrow$T and G$\rightarrow$T mutational biases and is shown by posterior predictive model checks to be an adequate substitution model where reversible models like GTR fail.
Used with a GLM substitution model to analyze the phylogeographic pattern of spread of influenza in humans, the random-effects suggest the air traffic volume alone is a powerful explanation for the spread of influenza from 2002 to 2007.
In examination of the evolution of ecological and behavioral characters in hylid tree frogs, a random-effects model shows no evidence for an effect of arboreality on the mode of swimming.
On a dataset of Metazoan amino acid sequences, an LG+RE model picks up many strong departures from the LG model.

To enable efficient inference of random-effects substitution models, we derived an approximate substitution gradient.
The time-complexity of our approximate approach is cubic in the size of the state-space, while ``exact'' analytical techniques are quintic.
For parameter-rich random-effects substitution models, numerical gradients are also quintic, and our approximate gradients enable MAP inference of parameters over 50 times faster than numerical gradients.
Used in Bayesian inference, we find that HMC using our approximate gradients is 6.6 to 20.2 times more efficient than standard Metropolis-Hastings moves, with even more impressive gains when comparing the dimension with the most difficult sampling (where the efficiency gains are 14.8 and 33.6 fold).
In particular, it appears that the efficiency of HMC with the approximate gradients is roughly invariant to the dimension (Figure~\ref{fig:ess_per_second}).
For our SARS-CoV-2 example, with a $4 \times 4$ rate matrix, the average efficiency of HMC is 6.3 effective samples per second, while for the influenza A virus phylogeographic example, with a $14 \times 14$ rate matrix, it is 7.6 effective samples per second.
However, the efficiency of Metropolis-Hastings moves decreases from 1.5 effective samplers per second to 0.41.
We expect this trend to continue as the size of the state-space increases, and that for sufficiently large models (such as codon models or Markov-modulated amino acid models), HMC will be the only approach capable of inferring random-effects substitution models in any reasonable timeframe.

Although the approximate substitution gradient we derived performed very well in our applications, it cannot be expected to perform ideally in every circumstance.
Mathematical analysis suggests that the error in our approximation grows with the branch length measured in genetic distance (see Supplementary Section~\ref{sec:error}).
Thus, we should expect performance to be best where the tree has few substitutions per site.
\citet{wertheim2022accuracy} refer to this as the near-perfect regime, and it is common in viral phylodynamic applications.
However, we note two reasons for optimism in applying our gradients in regimes with larger numbers of substitutions.
Both the influenza example and the Metazoan amino acid example fall outside the near-perfect regime, in particular the Metazoan tree used has over 5.5 substitutions per site on average, and the approximate gradients perform quite well.
This suggests the error bound we have obtained is quite conservative.
It is also important to note that when used for HMC, the accept-reject step ensures correctness even in regimes where the approximation gets poor.
An open question is to define the regimes where the approximation becomes poor enough that inference becomes inefficient such that other techniques would be preferable.

There are a number of important extensions of this work.
Currently, we have implemented the gradient computations (in BEAST 1.10 \citep{suchard2018bayesian} and BEAGLE 3 \citep{ayres2019beagle}) for use on CPUs, however GPU-based likelihood computations have proven incredibly efficient in many phylogenetic contexts \citep{suchard2009many,dudas2017virus,ayres2019beagle,baele2021markov,lemey2021untangling}.
In particular, \citet{gangavarapu2023many} recently showed minimum increases of 8-fold and 128-fold for nucleotide and codon models respectively when computing gradients with respect to branch rate parameters.
A GPU implementation of our approximate gradients would likely produce notable speedups in inference, especially for large state-space models.
Mathematically, our approximation holds for any case in which there is a single substitution rate matrix on any edge of the phylogeny (though we have currently only implemented the case for a single rate matrix across the whole tree).
However, the process of geographic spread may be temporally inhomogeneous while applying consistently across all lineages alive at any given time.
In such cases, epoch models \citep{bielejec2014inferring,gao2022new} are needed.
The epoch times break branches into multiple regimes, which requires matrix convolutions for likelihood computation and thus an extension of our approach.

Random-effects substitution models are a flexible approach for creating more realistic substitution models, but they are not a panacea.
They cannot, for example, address gross violations of the underlying assumptions of the CTMC model, such as memorylessness.
Nor can they address dependence between characters without carefully predefining the set of (potentially) coevolving characters and expanding the state space of the model.
The Bayesian Bridge provides a robust framework for regularization, and HMC an efficient framework for inference.
However, the additional complexity of random-effects models may occasionally cause challenges for MCMC which require more active user intervention.
Consider, for example, the APOBEC-induced C$\rightarrow$T bias observed in our SARS-CoV-2 example, which, in a double-stranded virus, will also lead to a G$\rightarrow$A bias \citep{gigante2022multiple}.
Application of HKY+RE to such a dataset will lead to multimodality (caused by ridges in the likelihood) jointly involving five substitution model parameters, $\kappa$ and the pairs of random-effects $\randomeffect_{G \rightarrow A},\randomeffect_{C \rightarrow T}$ and $\randomeffect_{A \rightarrow G},\randomeffect_{T \rightarrow C}$.
While such multimodality does not invalidate the model, it could be mitigated by the use of TN93+RE or avoided entirely by using a simpler model like F81+RE.

\section*{Data and code availability}
BEAST XML files for the analyses in this paper are available at \href{https://github.com/suchard-group/approximate_substitution_gradient_supplement}{https://github.com/suchard-group/approximate\_substitution\_gradient\_supplement}.
The approximate gradients have been implemented in the hmc-clock branch of BEAST (\href{https://github.com/beast-dev/beast-mcmc/tree/hmc-clock/}{https://github.com/beast-dev/beast-mcmc/tree/hmc-clock/}) and the v4.0.0 release of BEAGLE (\href{https://github.com/beagle-dev/beagle-lib/releases/tag/v4.0.0}{https://github.com/beagle-dev/beagle-lib/releases/tag/v4.0.0}).

\section*{Acknowledgments}
This work was supported through NSF grants DMS 2152774 and DMS 2236854, as well as NIH grants R01 AI153044, R01 AI162611 and K25 AI153816.
J. O. W. was funded by AI135992.
J. E. P. was funded by NIH T15LM011271.
Dr.\ Matsen is an Investigator of the Howard Hughes Medical Institute.


\bibliographystyle{plainnat}
\bibliography{ms}

\beginsupplement

\section{More on Hamiltonian Monte Carlo with surrogate trajectories}
Recent independent works \citep{neklyudov2020involutive,glatt2020accept,andrieu2020general} have provided an alternative justification (to that presented in the main text) for the theoretical correctness of the present work.
To wit, these works provide mathematical foundations for the use of \emph{involutions}, or deterministic mappings $\mathbf{i}:\mathbb{R}^{2K}\rightarrow \mathbb{R}^{2K}$ that satisfy $\mathbf{i}\left(\mathbf{i}(\params,\mom)\right)=(\params,\mom)$, within general MCMC algorithms.
Such theoretical results relate to the present work insofar as the combination of leapfrog dynamics \eqref{eq:leap} with a sign-flip $(\params,\mom) \mapsto (\params,-\mom)$ constitutes an involution.
To see this, suppose one has performed a single leapfrog iteration followed by a sign-flip.
Next, starting at time $\tau+\epsilon$, a subsequent leapfrog iteration unwinds the first:
    \begin{align}\label{eq:leap2}
	\mom(\tau + \frac{3}{2}\epsilon) &= -\mom(\tau+\epsilon) + \frac{\epsilon}{2} \nabla \log \pi(\params(\tau+\epsilon))  \\  \nonumber
	\params(\tau+2\epsilon) &= \params(\tau+\epsilon) + \epsilon \, \cov^{-1}\mom(\tau + \frac{3}{2}\epsilon) \\ \nonumber
	\mom(\tau + 2\epsilon) &= \mom(\tau + \frac{3}{2}\epsilon) + \frac{\epsilon}{2} \nabla \log \pi(\params(\tau+2\epsilon))  \,.
\end{align}
One may verify that $(\params,\mom)(\tau+2\epsilon)=(\params,\mom)(\tau)$ by substituting individual terms, e.g.,
\begin{align*}
	\params(\tau+2\epsilon) &= \params(\tau+\epsilon) + \epsilon \, \cov^{-1}\mom(\tau + \frac{3}{2}\epsilon) \\
	&= \left(\params(\tau) + \epsilon \, \cov^{-1}\mom(\tau+\epsilon/2) \right) + \epsilon \, \cov^{-1}\left( -\mom(\tau+\epsilon) + \frac{\epsilon}{2} \nabla \log \pi(\params(\tau+\epsilon))  \right) \\
	&=\left(\params(\tau) + \epsilon \, \cov^{-1}\mom(\tau+\epsilon/2) \right) +\\
	&\quad \epsilon \, \cov^{-1}\left( -\left(\mom(\tau+\epsilon/2) + \frac{\epsilon}{2} \nabla \log \pi(\params(\tau+\epsilon))  \right) + \frac{\epsilon}{2} \nabla \log \pi(\params(\tau+\epsilon))  \right) \\
	&= \params(\tau) \, .
\end{align*}
One may similarly show that $\mom(\tau+2\epsilon)=\mom(\tau)$, and analogous results immediately follow for the composition of $L$ leapfrog steps with a momentum sign-flip.
Importantly, this algebra remains the same when one substitutes an arbitrary function $\mathbf{g}$ for the log-posterior gradient (say, an approximation using $\approxnabla \likelihoodAll{\params}$ instead of $\nabla \likelihoodAll{\params}$), and the upshot is a deeper theoretical justification for the use of surrogate gradients within HMC.

\section{Optimization routines in BEAST}\label{sec:bfgs}
Given access to $f(\params)$ and its gradient $\nabla f(\params)$, a variety of algorithms exist for numerically finding the minimum.
BEAST 1.10 \citep{suchard2018bayesian} offers users access to the L-BFGS optimization algorithm, a limited-memory version of the BFGS algorithm \citep[see, \textit{e.g.}][]{dennis1996numerical,ji2020gradients}.
Both the BFGS and L-BFGS algorithms use information about the curvature of the likelihood surface from the Hessian (the matrix of second derivatives) to guide the search,
At each step in the algorithm $t$, the L-BFGS algorithm uses the current gradient $\nabla f(\params_{t})$, and an approximation to the inverse Hessian $\boldsymbol{H}_{t}$ to define a direction of descent $\boldsymbol{p}_t = \boldsymbol{H}_{t} \nabla f(\params_{t})$.
Then a line-search is used to choose the step size $\alpha_{t}$ that minimizes $f(\params_{t} + \alpha_{t} \boldsymbol{p}_t)$, and the parameters are updated accordingly, $\params_{t+1} = \params_{t} + \alpha_{t} \boldsymbol{p}_t$.
These steps are repeated until the minimum is obtained.
Where the BFGS algorithm stores and updates the entire approximate inverse Hessian $\boldsymbol{H}_{t}$, the L-BFGS algorithm stores only a recent history of iterations.
At each step, this history is used to implicitly carry out operations requiring $\boldsymbol{H}_{t}$, reducing the computational complexity by reducing the requisite number of matrix multiplications.

By using our approximate gradient $\approxnabla \log \likelihoodAll{\params}$ in place of the true gradient $\nabla \log \likelihoodAll{\params}$, impressive speed gains may be realized.
Optimization solely using  $\approxnabla \log \likelihoodAll{\params}$ produces approximate inference; however, a final round of optimization based on $\nabla \log \likelihoodAll{\params}$ may be performed to often obtain exact estimates without requiring extensive use of the more expensive true gradient.

\section{Computational complexity of alternative approaches to computing the gradient of a matrix exponential}\label{sec:supplemental_computational_complexity}

In this paper, we have employed a first-order approximation to the derivative of the matrix exponential with respect to its elements, $\frac{\partial}{\partial \ratematrixelement_{ij}} \exp(t \ratematrix)$.
In this section, we contemplate the computational efficiency of alternative means of evaluating relevant gradients more exactly.
First, we consider a more exact approach to evaluating the gradient of a matrix exponential with respect to its elements.
Then, we consider finite differences to directly attack the gradient of the phylogenetic log-likelihood.

\subsection{A less approximate approach}
Following \cite{najfeld1995derivatives}, let us define the block-matrix Z,
\begin{align*}
  \bZ =
  \begin{bmatrix}
    \ratematrix & \indicatormatrix_{ij}\\
    0 & \ratematrix
  \end{bmatrix}
\end{align*}
Then, we have (Equation 90, \cite{najfeld1995derivatives}),
\begin{equation}
  \exp(t \bZ) =
  \begin{bmatrix}
    \exp(t \ratematrix) & \frac{\partial}{\partial \ratematrixelement_{ij}} \exp(t \ratematrix)\\
    0 & \exp(t \ratematrix)
  \end{bmatrix}
  \label{eqn:nh_ex\postPartial_algorithm}
\end{equation}
Thus on a single branch $\node$ for the $\bigO(\alphabetsize^3)$ cost of exponentiating one $2 \alphabetsize \times 2 \alphabetsize$ matrix, we obtain both the matrix exponential and its derivative with respect to a single element of the rate matrix, $\partial \exp(t \ratematrix) / \partial \ratematrixelement_{ij}$.
However, we will need the full matrix of all such partial derivatives, $\allQderivs_{\node}$, meaning we need to repeat the process for all $\alphabetsize^2$ elements of $\ratematrix$, which is $\bigO(\alphabetsize^5)$.
Summing the per-branch contributions across the tree and employing the mapping procedure of Section~\ref{sec:mapping} allows us to obtain the gradient in $\bigO(K \alphabetsize^2 + \nTaxa \alphabetsize^5)$.
This cost is prohibitively large for even moderately-sized $\alphabetsize$.

\subsection{The numerical approach}\label{sec:numerical_approach}
The gradient of the log-likelihood can also be obtained numerically via finite differences.
To approximate $\frac{\partial}{\partial \param_k} \likelihoodAll{\params}$ using finite differences, we simply need to evaluate the likelihood twice.
If $\indicatorvector_k$ is a vector in which all elements are 0 except the $k$th, to obtain the central finite difference we need to evaluate $\likelihoodAll{\params + (h/2) \indicatorvector_k}$ and $\likelihoodAll{\params - (h/2) \indicatorvector_k}$, where $h$ is a very small quantity.
The numerical approximation is then
\[
  \frac{\partial}{\partial \param_k} \likelihoodAll{\params} = \frac{\big( \likelihoodAll{\params + (h/2) \indicatorvector_k} - \likelihoodAll{\params - (h/2) \indicatorvector_k} \big)}{h}.
\]
This is the numerical approach employed in Section~\ref{sec:metazoa}.

To evaluate the log-likelihood, a matrix exponential must be evaluated on each of $\bigO(\nTaxa)$ branches on the tree.
When computing the exponential of an $\alphabetsize \times \alphabetsize$ matrix by eigendecomposition, the requirements of diagonalization and matrix multiplcation make the operation $\bigO(\alphabetsize^3)$ \citep{suchard2009many}.
This makes the numerical gradient for a single parameter $\bigO(\nTaxa \alphabetsize^3)$, and the gradient $\bigO(K \nTaxa \alphabetsize^3)$.
Recall that the cost of our approximate gradients using parameter mappings is $\bigO(K \alphabetsize^2 + \nTaxa \alphabetsize^3)$.
If the number of parameters $K$ in the rate matrix is small, then numerical approaches may be viable.
Among commonly-used GTR-family models, gradient-based inference for the K2P model would likely be more efficient with numerical derivatives than the approaches outlined in this paper.
However, any model with unequal base frequencies (\textit{e.g.} moving from K2P to HKY) has $K \gtrsim \alphabetsize$, such that the numerical approach is expected to be less efficient than the approximate approach with parameter mappings.
For a richly-parameterized random-effects model where every element of $\ratematrix$ has an effect and there are roughly $\alphabetsize^2$ parameters, the approximate gradient with mapping is clearly more efficient.
As the state-space increases in size, the magnitude of the increase in efficiency of the approximate gradient will become larger.
This is corroborated by the 50-fold speed increase of MAP optimization using the approximate gradient over numeric gradients reported in Section~\ref{sec:metazoa}.

\section{Error in the approximate gradient}\label{sec:error}

As we are approximating the gradient, one natural question is, how good is the approximation?
We start by splitting Equation~\ref{eqn:NH_103} into two parts, the portion of the sum $\bS$ which we use for the first-order approximation, and the remainder term $\bR$
\begin{align}
	\label{eqn:error_norm}
	\frac{\partial}{ \partial \ratematrixelement_{ij}} e^{\ratematrix \branchlength} &= e^{\ratematrix \branchlength} \sum_{x=0}^{\infty} \frac{\branchlength^{x+1}}{(x+1)!} \{\indicatormatrixij,\ratematrix^x\}\\ \nonumber
	&= e^{\ratematrix \branchlength} \Big( \branchlength \indicatormatrixij + \sum_{x=1}^{\infty} \frac{\branchlength^{x+1}}{(x+1)!} \{\indicatormatrixij,\ratematrix^x\} \Big) \\ \nonumber
	&= e^{\ratematrix \branchlength} \big( \bS + \bR \big). \nonumber
\end{align}
Revisiting Equations~\ref{eqn:approximate_gradient} and~\ref{eqn:all_q_derivs_element} with this formulation in mind, we have that the true gradient is
\begin{align}
	\allQderivselement_{\node i j} &= \bpostPartial_{\node \category \site}' \frac{\partial}{ \partial \ratematrixelement_{ij}} \bprePartial_{\node \category \site} \nonumber \\
	&= \bpostPartial_{\node \category \site}' \left( \frac{\partial}{\partial \ratematrixelement_{ij}} e^{\ratematrix \times t_{\node} \times \rateCategory_{\category}} \right)' \bprePrePartial_{\node \category \site} \nonumber \\
	&=  \bpostPartial_{\node \category \site}' \left(  e^{\ratematrix \times t_{\node} \times \rateCategory_{\category}} \left( \bS + \bR \right) \right)' \bprePrePartial_{\node \category \site}  \nonumber \\
	&=\bprePartial_{\node \category \site}' (\bS + \bR) \bpostPartial_{\node \category \site},
\end{align}
while our approximation is
\begin{align}
	\allQderivselement_{\node i j} &\approx \bprePartial_{\node \category \site}' \, \bS \, \bpostPartial_{\node \category \site}.
\end{align}  

To understand how far our approximation deviates from the truth, we must quantify the remainder term $\bR$.
Using matrix norms, we will examine its magnitude, $\mnorm \bR \mnorm$ and the perhaps more-informative relative magnitude $\mnorm \bR \mnorm / \mnorm \bS \mnorm)$.
We will require two matrix norm identities.
First, we have (from repeated application of the fact that matrix norms obey the triangle inequality) that,
\[
 \mnorm\sum_i \bA_i\mnorm \leq \sum_i \mnorm\bA_i\mnorm.
\]
Second, we have for matrix commutator series $\{\bA,\bB^n\}$ that,
\[
 \mnorm\{\bA,\bB^n\}\mnorm \leq 2^n \times \mnorm A \mnorm \times \mnorm B \mnorm,
\]
which can be found in \citet{najfeld1995derivatives}.
From here on, for simplicity of notation, we drop per-branch subscripts and use $t$ for the product $t_{\node} \times \rateCategory_{\category}$.

Now, starting with our definitions of $\bS$ and $\bR$ from Equation~\ref{eqn:error_norm} and our first identity, we have
\begin{align*}
	\mnorm \bR \mnorm &=  \Biggmnorm \sum_{x=0}^{\infty} \frac{\branchlength^{x+1}}{(x+1)!} \{\indicatormatrixij,\ratematrix^x\} \Biggmnorm \\ \nonumber\\
  &\leq \sum_{x=1}^{\infty} \Biggmnorm \frac{\branchlength^{x+1}}{(x+1)!} \{\indicatormatrixij,\ratematrix^x\} \Biggmnorm \\ \nonumber
	&\leq  \sum_{x=1}^{\infty} \frac{\branchlength \times \mnorm \indicatormatrixij \mnorm \times \mnorm \bQ \mnorm}{x+1} \frac{(2 \branchlength)^{x}}{x!} \\ \nonumber
	&= (\branchlength \times \mnorm \indicatormatrixij \mnorm \times \mnorm \bQ \mnorm) \sum_{x=1}^{\infty} \frac{1}{x+1} \frac{(2 \branchlength)^{x}}{x!}\\
  &= \frac{\mnorm \indicatormatrixij \mnorm \times \mnorm \bQ \mnorm}{2}(e^{2t} - 2t - 1).
\end{align*}
We can also note that,
\[
  \mnorm \bS \mnorm = t \mnorm \indicatormatrixij \mnorm,
\]
meaning that we can bound the relative magnitude $\mnorm \bR \mnorm / \mnorm \bS \mnorm)$ as,
\[
  \frac{\mnorm \bR \mnorm}{\mnorm \bS \mnorm} \leq \frac{\mnorm \bQ \mnorm}{2t}(e^{2t} - 2t - 1)
\]

Note that $\branchlength$ and $\mnorm \bQ \mnorm$ are strictly non-negative.
The bounds on both the absolute and relative error depend on the norm of the rate matrix and get larger as the branch length (measured in genetic distance) increases.
Thus we should expect approximate gradient should perform best when branches are short, as in the near-perfect regime of \citet{wertheim2022accuracy}.
We note also that this is not a particularly tight error bound.

\section{Assessing the strength of evidence for nonreversibilities}\label{sec:savage}
In some cases, such as examining the geographic spread of influenza A virus, it will be of interest to assess the strength of evidence that a particular random-effect is non-zero.
In other cases, such as our SARS-CoV-2 example, however, the question is instead about the evidence for asymmetry in the substitution model.
In this case, we are instead interested in the distribution of $\randomeffect_{ij} - \randomeffect_{ji}$ and the evidence that this is non-zero.
To assess the evidence for nonreversibilities, non-zero random-effects, or other similar questions (such as strand-symmetry), we can use Bayes Factors.
In particular, as our models are nested, we may use the Savage-Dickey ratio to obtain the Bayes Factor.

In the case where we are interested in asymmetry, we must first re-parameterize our model.
Previously, we wrote the log-scale random-effects extension of a base model $\basemodel$ with instantaneous rate matrix $\boldsymbol{B} = \{ b_{ij} \}$, as,
\[
  \log(\ratematrixelement_{ij}) = \log(b_{ij}) + \randomeffect_{ij}.
\]
Instead we will write the log-scale rate matrix elements for reverse substitution directions $i \rightarrow j$ and $j \rightarrow i$ together, as,
\begin{align}
  \ln(\ratematrixelement_{ij}) &= \log(b_{ij}) + \randomeffect_{ij} \nonumber \\
  \ln(\ratematrixelement_{ji}) &= \log(b_{ji}) + \randomeffect_{ij} + \Delta_{ij}.  \nonumber
\end{align}
If $\basemodel$ is reversible, then its random-effects extension ($\basemodel$+RE) will be reversible when $\Delta_{ij} = 0$.
In this case, we will not have changed the ratio between the rate matrix elements, $\ratematrixelement_{ij}/\ratematrixelement_{ji} = b_{ij}/b_{ji} = \pi_j / \pi_i$.

The distribution on $\Delta_{ij}$ is the distribution of the difference of two Bayesian-Bridge-distributed variables (namely $\randomeffect_{ji} - \randomeffect_{ij}$), such that we can define $\randomeffect_{ji} = \randomeffect_{ij} + \Delta_{ij}$ and recover the model as written previously.
The Bayes Factor in favor of $\Delta_{ij} = 0$ (Model 0, against Model 1 where $\Delta_{ij}$ is a free parameter) is the ratio of the posterior density to the prior density at $\Delta_{ij} = 0$,
\[
  \text{BF}_{01} = \frac{\pr(\Delta_{ij} = 0 \mid \by)}{\pr(\Delta_{ij} = 0)}
\]

The posterior distribution will not in general have a known closed-form solution, so kernel density estimation will be needed to estimate $\pr(\Delta_{ij} = 0 \mid \by)$ from posterior samples.
The prior density $\pr(\Delta_{ij} = 0)$ can be written in closed form if we are not employing shrunken-shoulder regularization on the Bayesian Bridge.
In the case where shrunken-shoulder regularization is employed and the tails of the distribution are much lighter, we can run our model under the prior and use kernel density estimation to estimate the prior density at 0.

Let us now derive the (non-regularized) Bayesian Bridge density at 0.
Denote the Bayesian Bridge density with global scale $\globalScale$, exponent $\bridgeExponent$, and no shrunken-shoulder regularization as $f_{\text{BB}}(x;\globalScale,\bridgeExponent)$.
Let $Z(\globalScale,\bridgeExponent)$ be the normalizing constant for the Bayesian Bridge with those paramaters.
The probability density of $\Delta_{ij}$ at 0 is given by,
\begin{align*}
  f_{\Delta_{ij}}(0) &= \int_{-\infty}^{\infty} f_{\text{BB}}(x;\globalScale,\bridgeExponent) f_{\text{BB}}(-x;\globalScale,\bridgeExponent) dx \\
  &= \frac{1}{Z(\globalScale,\bridgeExponent)^2} \int_{-\infty}^{\infty} \exp({-|x/\globalScale|^\bridgeExponent}) \exp({-|-x/\globalScale|^\bridgeExponent}) dx\\
  &= \frac{1}{Z(\globalScale,\bridgeExponent)^2} \int_{-\infty}^{\infty} \exp({-2|x/\globalScale|^\bridgeExponent}) dx.
\end{align*}
We can recognize the integrand as the kernel of a Bayesian Bridge distribution with global scale $2^{-1/\bridgeExponent} \globalScale$ and exponent $\bridgeExponent$.
Thus we have,
\[
  f_{\Delta_{ij}}(0) = \frac{Z(2^{-1/\bridgeExponent} \globalScale,\bridgeExponent)}{Z(\globalScale,\bridgeExponent)^2}.
\]
The Bayesian Bridge distribution (without shrunken-shoulder regularization) is also known as the Exponential Power Distribution and the Generalized Normal Distribution and has normalizing constant \citep{griffin2018gnorm},
\[
  Z(\globalScale,\bridgeExponent) = \frac{\bridgeExponent}{2 \globalScale \Gamma(1/\bridgeExponent)}.
\]
Thus, the prior density at $\Delta_{ij} = 0$ is,
\[
  f_{\Delta_{ij}}(0) = \frac{2^{1 + 1/\bridgeExponent} \globalScale \Gamma(1/\bridgeExponent)}{\bridgeExponent}
\]
When the global scale is a parameter in the model (as it is in all of our applications), this can be numerically integrated over the prior on $\globalScale$.
In the case where shrunken-shoulder regularization is employed, the marginal Bayesian Bridge distribution is instead proportional to $\exp({-|x/\globalScale|^\bridgeExponent}) \exp(-x^2/(2\xi^2))$ and the normalizing constant is not known.

\section{Posterior predictive p-values for proportions}\label{sec:PPS}
For completeness, we now write out explicitly how to compute the posterior predictive summaries used to compare HKY+RE to GTR on the SARS-CoV-2 data in Section~\ref{sec:SC2}.
Recall that our summaries treat the columns as observations of the proportion of nucleotides at each site.
We use proportions, rather than counts, such that the statistic can be computed comparably across all sites for all non-ambiguous nucleotide states.
That is, ambiguities can be ignored, as long as the same sites are masked out in the posterior predictive alignment.

Denote the alignment as $\bY$, it has $\nTaxa$ rows and $\nSites$ sites, and $\bY_j$ is a single column.
Let $\mathcal{A}$ be the alphabet (of size $\alphabetsize$) and $\mathcal{A}_i$ be a particular character in the alphabet.
We define a new $\nSites \times \alphabetsize$ matrix, $\bp$, of per-site proportions of characters by,
\begin{equation}
  \postPartial_{ij} = \frac{\sum_{k=1}^{\nTaxa} \indicator(y_{ki} = \mathcal{A}_j)}{\sum_{l=1}^{\alphabetsize} \sum_{k=1}^{\nTaxa} \indicator(y_{ki} = \mathcal{A}_l)}.
\end{equation}
That is, the rows in this new matrix are the sites in the alignment, and the columns are the proportions (among all non-ambiguous characters) of each character at that site.

We now have $\alphabetsize$ new variables, $\bp_1,\dots,\bp_{\alphabetsize}$, stored as columnns in $\bp$.
We restrict our attention to variable sites, such that the number of rows in $\bp$ is the number of variable sites.
For DNA the alphabet consists of the nucleotides A, C, G, and T, and $\bpostPartial$ is a $\nSites \times 4$ matrix.
The summaries of interest are the means, variances, and covariances of these new variables.
The means, give or take discrepancies from ignoring ambiguities, are the proportions of each of the characters in the alignment.
Thus, they should largely reflect a model's ability to capture large-scale features of the substitution process, like the stationary frequencies of a GTR model.
The variances should, at least partially, reflect the tree length.
For a tree of length 0, each site is exclusively one character, and each site is essentially a draw from a categorical distribution with probabilities given by the root frequency distribution.
For a tree of infinite length, each site is a draw from an $\nTaxa$-dimensional multinomial distribution with probabilities given by the equilibrium frequencies.
The (finite, non-zero) length of the tree will determine where along this continuum our variances fall.
The covariances describe the strength of association of two characters.
We might expect if a model misses an extremely large rate, such as $C \rightarrow T$, it might under-estimate the corresponding covariance.
Though since normalization (and potentially assumptions of symmetry) bind the rates together, the effect may cascade and lead to over-estimation of other covariances.
\end{document}
