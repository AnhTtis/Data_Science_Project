Different from SFDA methods that narrowly focus on adaptation under data distribution change $p_\mathcal{S}(x) \not = p_\mathcal{T}(x)$, this section reviews another family of test-time adaptation methods under label distribution change, $p_\mathcal{S}(y) \not = p_\mathcal{T}(y)$. 

\subsection{Problem Definition}
In 2002, Saerens \etal \cite{saerens2002adjusting} proposes a well-known \emph{prior adaptation} framework that adapts an off-the-shelf classifier to a new label distribution with unlabeled data at test time. 
We reuse the variables defined in the above section and give the definition of \emph{test-time prior adaptation} as follows.

\begin{definition}[Test-Time Prior Adaptation, TTPA]
Given a classifier $f_\mathcal{S}$ learned on the source domain $\mathcal{D}_\mathcal{S}$ and an unlabeled target domain $\mathcal{D}_\mathcal{T}$, \emph{test-time prior adaptation} aims to correct the probabilistic predictions $p_\mathcal{S}(y|x)=f_\mathcal{S}(x)$ for samples in $\mathcal{D}_\mathcal{T}$ under the prior shift assumption. 
\end{definition}

Formally, prior shift \cite{sipka2022hitchhiker} (\aka label shift \cite{garg2020unified,lipton2018detecting}), denotes that the marginal label distributions differ but the conditional distributions of input given label remain the same across domains, namely, $p_\mathcal{S}(y) \not = p_\mathcal{T}(y), p_\mathcal{S}(x|y) = p_\mathcal{T}(x|y)$.
Based on the Bayes theorem and the label shift assumption, we have the following equations,
\begin{equation}
\begin{aligned}
p_\mathcal{T}&(y|x) = \frac{p_\mathcal{T}(x|y)p_\mathcal{T}(y)}{p_\mathcal{T}(x)}=p_\mathcal{S}(x|y) \cdot \frac{p_\mathcal{T}(y)}{p_\mathcal{T}(x)}\\
&=\frac{p_\mathcal{S}(y|x)p_\mathcal{S}(x)}{p_\mathcal{S}(y)}\cdot \frac{p_\mathcal{T}(y)}{p_\mathcal{T}(x)} \propto p_\mathcal{S}(y|x) \cdot \frac{p_\mathcal{T}(y)}{p_\mathcal{S}(y)},
\label{eq:bayes}
\end{aligned}
\end{equation}
where $p_\mathcal{T}(y|x)$ denotes the corrected posterior probability from the biased prediction $p_\mathcal{S}(y|x) \in \mathbb{R}^{C}$ for each sample $x$ in the target set $\mathcal{D}_\mathcal{T}$. 
Therefore, the key to the prior-shift adaptation problem lies in how to obtain the test-time prior $p_\mathcal{T}(y)$ or the prior ratio $w(y) = p_\mathcal{T}(y)/p_\mathcal{S}(y)$.
Moreover, the prior ratio could be also considered as the importance weight \cite{cortes2010learning} ($p_\mathcal{T}(x,y)/p_\mathcal{S}(x,y)$), which further facilitates re-training the source classifier with a weighted empirical risk minimization.
Note that, we only review different strategies of estimating the prior or prior ratio without the source training data, regardless of the follow-up correction strategy.

\subsection{Taxonomy on TTPA Algorithms}
\subsubsection{Confusion Matrix}
\method{Prior estimation}
As introduced in \cite{mclachlan1992discriminant,saerens2002adjusting}, a standard procedure used for estimating the test-time prior involves the computation of the confusion matrix, $[C_{\hat{y}|y}]_{i,j}=p(\hat{y}=j|y=i)$, denoting the probability of predicting class $j$ when the observation in fact belongs to the $i$-th class.
Specifically, the confusion matrix-based method tries to solve the following system of $C$ linear equations with respect to $p_\mathcal{T}(\hat{y})$,
\begin{equation}
p_\mathcal{T}(\hat{y}=i) = \sum\nolimits_{j=1}^{C} [C_{\hat{y}|y}]_{i,j} p_\mathcal{T}(y=j), \;i=1,\dots,C.
\label{eq:confusion}
\end{equation}
Intuitively, there exists a closed-form solution, $\hat{p}_\mathcal{T}(y) = [C_{\hat{y}|y}]^{-1} p_\mathcal{T}(\hat{y})$, where $p_\mathcal{T}(\hat{y})$ denotes the estimated class label frequency over the original predictions $f_\mathcal{S}(x)$ on the unlabeled test data set.
Note that, the confusion matrix $C_{\hat{y}|y}$ could be obtained using a holdout training data set \cite{lipton2018detecting}, which is proven to be unchanged under the label shift assumption.
To avoid such infeasible solutions, a bootstrap method \cite{vucetic2001classification} is developed to alternately calculate the confusion matrix $C_{\hat{y}|y}$ on holdout data and estimate the predicted class frequency $p_\mathcal{T}(\hat{y})$ on test data. 

\method{Prior ratio estimation}
BBSE \cite{lipton2018detecting} exploits the confusion matrix with joint probability instead of the conditional one above, and directly estimates the prior ratio $w(y) = p_\mathcal{T}(y)/p_\mathcal{S}(y)$ based on the following equation,
\begin{equation}
p_\mathcal{T}(\hat{y}=i) = \sum\nolimits_{j=1}^{C} [C_{\hat{y},y}]_{i,j}w(y=j), \;i=1,\dots,C,
\label{eq:confusion-ratio}
\end{equation}
where the confusion matrix $[C_{\hat{y},y}]_{i,j}$ is computed with hard predictions over the holdout training data set, and a soft variant is further explored in \cite{sipka2022hitchhiker}. 
To diminish large variances in prior ratio estimation, RLLS \cite{azizzadenesheli2019regularized} proposes to incorporate an additional regularization term $\|\theta\|_2$, forming the constrained optimization problem as follows,
\begin{equation}
    \min_\theta \|C_{\hat{y},y}\cdot(\theta+\textbf{1}) - p_\mathcal{T}(\hat{y})\|_2 + \lambda \|\theta\|_2,
\end{equation}
where $\theta =w-\textbf{1}$ is called the amount of weight shift, and $\lambda$ is a regularization hyper-parameter.

\subsubsection{Maximum Likelihood Estimation (MLE)}
MLLS \cite{latinne2001adjusting,saerens2002adjusting} proposes a straightforward instance of the Expectation-Maximization (EM) algorithm to maximize the likelihood, providing a simple iterative procedure for estimating the prior $p_\mathcal{T}(y)$. 
In particular, MLLS alternately re-estimates the posterior probabilities $\hat{p}_\mathcal{T}(y|x)$ for each test sample and the prior probability $\hat{p}_\mathcal{T}(y)$ as follows,
\begin{equation}
\begin{aligned}
    \hat{p}_\mathcal{T}(y=i|x) &= \frac{\hat{w}(y=i) \cdot p_\mathcal{S}(y=i|x)}{\sum_{j=1}^{C} \hat{w}(y=j) \cdot p_\mathcal{S}(y=j|x)}, \;(\textbf{E}\text{-step})\\
    \hat{p}_\mathcal{T}(y=i) &= \frac{1}{n_t}\sum\nolimits_{x\in \mathcal{X}_t} \hat{p}_\mathcal{T}(y=i|x), \;(\textbf{M}\text{-step})\\
    \hat{w}(y=i) &= \hat{p}_\mathcal{T}(y=i)/p_\mathcal{S}(y=i), \;\forall i \in [1, C],
\end{aligned}
\label{eq:mlls}
\end{equation}
where $p_\mathcal{S}(y|x)$ is modeled by the classifier's output $f_\mathcal{S}(x)$, and $\hat{p}_\mathcal{T}(y)$ is initialized by the estimation of class frequencies in the target domain. 
As proven in \cite{saerens2002adjusting}, this iterative procedure will increase the likelihood of target data at each step.
Besides, the EM algorithm could be also reformulated as minimizing the KL divergence between the target data distribution $p_\mathcal{T}(x)$ and its approximation by a linear combination of class-wise distributions $p'_\mathcal{T}(x)$ \cite{du2012semi,du2014semi}, 
\begin{equation}
    p'_\mathcal{T}(x)=\sum\nolimits_c \lambda_c p_\mathcal{T}(x|y=c), \;s.t. \sum\nolimits_c \lambda_c=1,
\end{equation}
where $\lambda_c$ can be considered as an exact approximation of $p_\mathcal{T}(y=c)$.
Thereby, a variant is proposed by replacing the KL divergence with the Pearson divergence \cite{pearson1900criterion}, which is computed efficiently and analytically, and shown to be more robust against noise and outliers.

\method{Calibration}
Two following works \cite{chan2006estimating,alexandari2020maximum} focus on the calibration of estimated probabilities in the EM algorithm.
On the one hand, the probability estimates are calibrated from naive Bayes \cite{chan2006estimating} in the maximization step (\textbf{M}-step).
On the other hand, BCTS \cite{alexandari2020maximum} offers a temperature scaling solution to achieve an explicit calibration for $p_\mathcal{T}(y|x)$ in the expectation step (\textbf{E}-step), which is inspired by a prominent method \cite{guo2017calibration}.
To be specific, BCTS scales the softmax logits $l(x)$ with a temperature parameter $T$ and additionally considers the class-specific bias terms $\{b_i\}_{i=1}^{C}$ as follows,
\begin{equation}
    p_\mathcal{T}(y=i|x) = \frac{exp([l(x)]_i/T) + b_i}{\sum_j exp([l(x)]_j/T)+b_j}.
\end{equation}
Further, the EM algorithm is proven to converge to a global maximum for the likelihood over holdout data \cite{alexandari2020maximum}.
On the contrary, TTLSA \cite{sun2022invariance} tries to learn a well-calibrated classifier in the training time using logit adjustment.

\method{Maximum a posterior estimation}
As shown in \cite{sulc2019improving}, the Dirichlet hyper-prior on the class prior probabilities is integrated into the MLE framework, forming the Maximum a Posteriori (MAP) estimation framework.
Besides, the MAP objective is optimized by the projected gradient descent algorithm under the simplex constraint.

\subsubsection{MLE with Confusion Matrix}
MLLS-CM \cite{garg2020unified} unifies the confusion matrix strategy \cite{lipton2018detecting} and the EM-based MLE strategy \cite{saerens2002adjusting} under a common framework. 
In particular, the confusion matrix strategy is employed to obtain a calibrated predictor in the \textbf{E}-step.
Besides, SCM$^{\text{L}}$ \cite{sipka2022hitchhiker,sipka2021adapting} provides another confusion-based MLE approach that inserts the confusion matrix-based prior into the log-likelihood maximization objective as follows,
\begin{equation}
\begin{aligned}
    & \max_{p_\mathcal{T}(y)} \; \sum\nolimits_{i=1}^{C} m_i\log(C_{\hat{y}=i|y}\cdot p_\mathcal{T}(y))), \\
    & s.t. \sum\nolimits_i p_\mathcal{T}(y=i) = 1, \; p_\mathcal{T}(y=i) \geq 0, i\in [1,C],
\end{aligned}
\end{equation}
where $m_i$ is the number of samples predicted to the $i$-th class on the target set.
The projected gradient ascent algorithm with a simplex constraint is iteratively employed to maximize such a convex objective above.

\method{Remarks}
In addition, the prior over the target set could be directly used without the prior estimation \cite{sulc2019improving}, \eg, the uniform prior is utilized in the winning submission of FGVCx Fungi Classification Kaggle competition \cite{sulc2020fungi}.
Moreover, the post-training prior rebalancing technique \cite{tian2020posterior} even finds an interpolated distribution between $p_\mathcal{S}(y|x)$ and $p_\mathcal{T}(y|x)$.
% with a regularizing parameter $\lambda$, namely, $p_\mathcal{T}(y|x)^{\lambda} \cdot p_\mathcal{S}(y|x)^{1- \lambda}$.
Recently, SADE \cite{zhang2022self} provides a different TTPA strategy from the perspective of model aggregation, requiring multiple source expert classifiers and only learning the expert-wise weights for the target domain by maximizing the consistency under different stochastic data augmentations.

\subsubsection{Online TTPA Algorithms}
Without requiring the whole dataset at test time, another line of TTPA methods \cite{yang2008non,royer2015classifier} focus on the case of online adaptation at prediction time (\ie, sample after sample).
It is worth noting that offline TTPA algorithms could also address the online label shift scenario, for instance, estimating the priors from the already-seen examples \cite{sulc2019improving}.
In the following, we mainly introduce online TTPA methods when historical samples could not be stored.

OEM \cite{yang2008non} provides an online (or evolving) prior estimation approach to adjust the posterior probability sample by sample.
OEM is built on the seminal work \cite{saerens2002adjusting} in Eq.~(\ref{eq:mlls}) that extends the prior estimation step as:
\begin{equation}
  \hat{p}_\mathcal{T}(y=i) \gets (1-\alpha)\hat{p}_\mathcal{T}(y=i) + \alpha \hat{p}_\mathcal{T}(y=i|x_t),
\end{equation}
where $x_t$ denotes the $t$-th test data from the sequence, and $\alpha$ is the hyper-parameter to control the updating rate.
Besides, PTCA$_\text{U}$ \cite{royer2015classifier} directly estimates a categorical distribution from a set of $L$ most recent target samples below,
\begin{equation}
    \hat{p}_\mathcal{T}(y=i|x_t)= \frac{\sum_{r=t-L+1}^{t} 
    [\delta_r(y)]_i +\alpha}{L + C\cdot \alpha},
    \label{eq:online}
\end{equation}
where $\delta_r(y) \in \mathbb{R}^C$ denotes the prediction of $x_r$, and $\alpha>0$ is the parameter of the Dirichlet prior.
Then the corrected prediction is obtained using the Bayes theorem in Eq.~(\ref{eq:bayes}).
By contrast, OGD \cite{wu2021online} obtains an unbiased estimate of the expected 0-1 loss in terms of the confusion matrix $C_{\hat{y}|y}$ and the label marginal probabilities $p_\mathcal{T}(y)$, then employs the online gradient descent technique to update the re-weighted classifier in Eq.~(\ref{eq:bayes}) after each unlabeled sample.

\subsection{Learning Scenarios of TTPA Algorithms}
Almost all TTPA methods require not only the source classifier but also additional information about the source domain, \eg, the source label distribution $p_\mathcal{S}(y)$ \cite{saerens2002adjusting}, and the conditional confusion matrix over holdout data $C_{\hat{y}|y}$ \cite{lipton2018detecting}. 

\method{Offline \vs Online}
Regarding the target domain, TTPA methods could be divided into two categories: offline TTPA \cite{saerens2002adjusting,lipton2018detecting,alexandari2020maximum,zhang2022self} where the whole target domain is available; and online TTPA \cite{yang2008non,royer2015classifier,wu2021online} where the class distribution varies over time.
Additionally, two supervised cases with online feedback are studied in \cite{royer2015classifier}, \ie, online feedback (the correct label is revealed to the system after prediction) and bandit feedback (the decision made by the system is correct or not is revealed).