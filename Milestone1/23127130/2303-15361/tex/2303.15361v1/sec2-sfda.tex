\subsection{Problem Definition}
\begin{definition}[Domain]
A domain $\mathcal{D}$ is a joint distribution $p(x,y)$ defined on the input-output space $\mathcal{X} \times \mathcal{Y}$, where random variables $x \in \mathcal{X}$ and $y \in \mathcal{Y}$ denote the input data and the label (output), respectively.
\end{definition}

In a well-studied domain adaptation problem, the domain of interest is called the target domain $p_\mathcal{T}(x,y)$ and the domain with labeled data is called the source domain $p_\mathcal{S}(x,y)$.
The label $y$ can either be discrete (in a classification task) or continuous (in a regression task).
Unless otherwise specified, $\mathcal{Y}$ is a $C$-cardinality label set, and we usually have one labeled source domain $\mathcal{D}_\mathcal{S}=\{(x_1,y_1),\dots,(x_{n_s},y_{n_s})\}$ and one unlabeled target domain $\mathcal{D}_\mathcal{T}=\{x_1,\dots,x_{n_t}\}$ under data distribution shifts: $\mathcal{X}_\mathcal{S}=\mathcal{X}_\mathcal{T}, p_\mathcal{S}(x) \not= p_\mathcal{T}(x)$, including the \emph{covariate shift} \cite{huang2006correcting} assumption ($p_\mathcal{S}(y|x) = p_\mathcal{T}(y|x)$).
% \mathcal{Y}_\mathcal{S}=\mathcal{Y}_\mathcal{T}
Typically, the unsupervised domain adaptation (UDA) paradigm aims to leverage supervised knowledge in $\mathcal{D}_\mathcal{S}$ to help infer the label of each target sample in $\mathcal{D}_\mathcal{T}$.

Chidlovskii \etal \cite{chidlovskii2016domain} for the first time consider performing adaptation with no access to source domain data.
Specifically, they propose three scenarios for feature-based domain adaptation with: source classifier with accessible models and parameters, source classifier as a black-box model, and source class means as representatives.
This new setting utilizes all the test data to adjust the classifier learned from the training data, which could be considered as a broad test-time adaptation scheme. 
% narrow
Several methods \cite{clinchant2016transductive,van2017unsupervised,liang2019distant} follow this learning mechanism and adapt the source classifier to unlabeled target features.
To gain benefits from end-to-end representation learning, researchers are more interested in generalization with deep models.
Such a setting without access to source data during adaptation is termed as source data-absent (free) domain adaptation \cite{liang2020we,liang2021source}, model adaptation \cite{li2020model}, and source-free domain adaptation \cite{kundu2020universal}, respectively.
For the sake of simplicity, we utilize the term \emph{source-free domain adaptation} and give a unified definition.

\begin{definition}[Source-free Domain Adaptation, SFDA]
Given a well-trained classifier $f_\mathcal{S}: \mathcal{X}_\mathcal{S} \to \mathcal{Y}_\mathcal{S}$ on the source domain $\mathcal{D}_\mathcal{S}$ and an unlabeled target domain $\mathcal{D}_\mathcal{T}$, \emph{source-free domain adaptation} aims to leverage the labeled knowledge implied in $f_\mathcal{S}$ to infer labels of all the samples in $\mathcal{D}_\mathcal{T}$, in a transductive learning \cite{joachims1999transductive} manner. Note that, all test data (target data) are required to be seen during adaptation.
\end{definition}

So far as we know, the term \emph{source-free domain adaptation} is first proposed by Nelakurthi \etal \cite{nelakurthi2018source}, where they try to leverage the noisy predictions of an off-the-shelf classifier and a few labeled examples from the target domain, in order to obtain better predictions for all the unlabeled target samples.
The definition here covers \cite{nelakurthi2018source} as a special case, where the classifier $f_\mathcal{S}$ is not accessible but provides the predictions of target data $\{f_\mathcal{S}(x)|x\in \mathcal{D}_\mathcal{T}\}$.

\setlength{\tabcolsep}{4.0pt}
\begin{table}[!t]
\caption{A taxonomy on SFDA methods with representative strategies.}
\resizebox{0.49\textwidth}{!}{
    \begin{tabular}{cll}
        \toprule
        \textbf{Families} & \textbf{Model Rationale} & \textbf{Representative Strategies}\\
        \midrule
        & \textbf{centroid-based} & SHOT \cite{liang2020we,liang2021source}, BMD \cite{qu2022bmd} \\
        \textbf{pseudo-} & \textbf{neighbor-based} & NRC \cite{yang2021exploiting}, SSNLL \cite{chen2022self}  \\
        \textbf{labeling} & \textbf{complementary labels} & LD \cite{you2021domain}, ATP \cite{wang2022source} \\
        & \textbf{optimization-based} & ASL \cite{yan2021augmented}, KUDA \cite{sun2022prior}  \\
        \midrule 
        & \textbf{data variations} & G-SFDA \cite{yang2021generalized}, APA \cite{sun2023domain} \\
        \textbf{consistency} & \textbf{model variations} & SFDA-UR \cite{sivaprasad2021uncertainty}, FMML \cite{peng2022toward}\\
        & \textbf{both variations} & AdaContrast \cite{chen2022contrastive}, MAPS \cite{ding2023maps} \\
        \midrule 
        & \textbf{entropy minimization} & ASFA \cite{xia2022privacy}, 3C-GAN \cite{li2020model}\\
        \textbf{clustering} & \textbf{mutual information} & SHOT \cite{liang2020we,liang2021source}, UMAD \cite{liang2021umad} \\
        & \textbf{explicit clustering} & ISFDA \cite{li2021imbalanced}, SDA-FAS \cite{liu2022source_eccv} \\
        \midrule 
        & \textbf{data generation} & 3C-GAN \cite{li2020model}, DI \cite{nayak2021mining} \\
        \textbf{source} & \textbf{data translation} & SFDA-IT \cite{hou2020source}, ProSFDA \cite{hu2022prosfda}\\
        \textbf{estimation} & \textbf{data selection} & SHOT++ \cite{liang2021source}, DaC \cite{zhang2022divide} \\
        & \textbf{feature estimation} & VDM-DA \cite{tian2022vdm}, CPGA \cite{qiu2021source}\\
        \midrule
        \textbf{self-supervision} & \textbf{auxiliary tasks} & SHOT++ \cite{liang2021source}, StickerDA \cite{kundu2022concurrent}\\
        \bottomrule
\end{tabular}}
\end{table}

\subsection{Taxonomy on SFDA Algorithms}
\subsubsection{Pseudo-labeling}
To adapt a pre-trained model to an unlabeled target domain, a majority of SFDA methods take inspiration from the semi-supervised learning (SSL) field \cite{van2020survey} and employ various prevalent SSL techniques tailored for unlabeled data during adaptation.
A simple yet effective technique, pseudo-labeling \cite{lee2013pseudo}, aims to assign a class label $\hat{y} \in \mathbb{R}^{C}$ for each unlabeled sample $x$ in $\mathcal{X}_t$ and optimize the following supervised learning objective to guide the learning process, 
\begin{equation}
    \min_\theta \mathbb{E}_{\{x,\hat{y}\} \in \mathcal{D}_t}\; w_{pl}(x)\cdot {d}_{pl}(\hat{y}, p(y|x;\theta)),
    \label{eq:pl}
\end{equation}
where $w_{pl}(x) \in \mathbb{R}$ denotes the weight associated with each pseudo-labeled sample $\{x, \hat{y}\}$, and $d_{pl}(\cdot)$ denotes the divergence between the predicted label probability distribution and the pseudo label probability $\hat{y}$, \eg, $-\sum_c \hat{y}_c \log [p(y|x;\theta)]_c$ if using the cross entropy as the divergence measure.
Since the pseudo labels of target data are inevitably inaccurate under domain shift, there exist three different solutions: (1) improving the quality of pseudo labels via denoising; (2) filtering out inaccurate pseudo labels with $w_{pl}(\cdot)$; (3) developing a robust divergence measure ${d}_{pl}(\cdot,\cdot)$ for pseudo-labeling. 

Following the classic pseudo-labeling work \cite{lee2013pseudo}, many SFDA methods directly obtain the pseudo label $\hat{y}_t$ as the one-hot encoding of the class that has the maximum predicted probability \cite{kim2021domain,li2021free,peng2022multi,chen2021source,song2022source}.
To reduce the effects of noisy pseudo labels based on the argmax operation, most of these methods develop various filtering mechanisms to consider only reliable pseudo labels during pseudo-labeling, \eg, maximum prediction probability \cite{zhou2022domain,song2022source,prabhu2022augco,kothandaraman2021ss,dasgupta2022overcoming}, self-entropy \cite{tian2023robust}, and consistency score \cite{chen2021source,hou2021visualizing,prabhu2022augco}.
In the following, we review not only different types of denoised pseudo labels but also different forms of the pseudo-labeling objective.

\method{Centroid-based pseudo labels}
Inspired by a classic self-supervised approach, DeepCluster \cite{caron2018deep}, SHOT \cite{liang2020we} and SHOT++ \cite{liang2021source} resort to target-specific clustering for denoising the pseudo labels. 
The key idea is to obtain target-specific class centroids based on the network predictions and the target features and then derive the unbiased pseudo labels via the nearest centroid classifier. 
Formally, the class centroids and pseudo labels are updated as follows,
\begin{equation}
\renewcommand{\arraystretch}{1.5}
\left\{
\begin{array}{lr}
m_c = \sum_{x} [p_\theta(y_c|x)\cdot g(x)] / \sum_{x} p_\theta(y_c|x), \ c=1,\dots,C, &  \\
\hat{y} = \arg\min_c d(g(x), m_c),\ \forall x \in \mathcal{D}_t, &  
\end{array}
\right.
\label{eq:pl-cluster}
\end{equation}
where $p_\theta(y_c|x)=[p(y|x;\theta)]_c$ denotes the probability associated with the $c$-th class, and $g(x)$ denotes the feature of input $x$.
$m_c$ denotes the $c$-th class centroid, and $d(\cdot,\cdot)$ denotes the cosine distance function.
Note that, $p_\theta(y_c|x)$ is initialized by the soft network prediction and further updated by the one-hot encoding of the new pseudo-label $\hat{y}$.
Besides, CDCL \cite{wang2022cross} directly performs K-means clustering in the target feature space with the cluster initialized by the source class prototypes.
As class centroids always contain robust discriminative information and meanwhile weaken the category imbalance problem, this label refinery is prevalent in follow-up SFDA studies \cite{du2021generation,yang2022self,zhao2022adaptive,li2022jacobian,pei2022uncertainty,li2022teacher,zhang2022divide,tang2021model,qiu2021source,diamant2022reconciling,taufique2023continual,jiao2022source,chen2022source,zhang2022source_b,feng2023cross,yuan2023data}.

To construct more accurate centroids, some methods (\eg, PPDA \cite{kim2020towards}, PDA-RoG \cite{bohdal2022feed_eccvw}, and Twofer \cite{liu2023twofer}) propose to identify confident samples to obtain the class centroids, and SCLM \cite{tang2022semantic} further integrates the enhanced centroids with the coarse centroids.
By leveraging more discriminative representations, TransDA \cite{yang2022self} maintains a mean teacher model \cite{tarvainen2017mean} as guidance, while Co-learn \cite{zhang2022colearning} exploits an additional pre-trained feature extractor.
Besides, BMD \cite{qu2022bmd} and PCSR \cite{guan2022polycentric} conjecture that a coarse centroid cannot effectively represent ambiguous data, and instead utilize K-means clustering to discover multiple prototypes for each class.
In addition, ADV-M \cite{wang2021providing} directly employs the Mahalanobis distance to measure the similarity between samples and centroids, while CoWA-JMDS \cite{lee2022confidence} and PDA-RoG \cite{bohdal2022feed_eccvw} perform Gaussian Mixture Modeling (GMM) in the target feature space to obtain the log-likelihood and pseudo label of each sample.
Except for hard pseudo labels, some recent works \cite{lee2022feature,han2022privacy,yang2022self} explore soft pseudo labels based on the class centroids, \eg, $\hat{y}_c = \frac{exp(-d(g(x), m_c)/\tau)}{\sum_c exp(-d(g(x), m_c)/\tau)}$, where $\tau$ denotes the temperature.
In contrast to offline clustering in Eq.~(\ref{eq:pl-cluster}), BMD \cite{qu2022bmd} and DMAPL \cite{yan2022dual} develop a dynamic clustering strategy that uses an exponential moving average (EMA) to accumulate the class centroids in mini-batches.

\method{Neighbor-based pseudo labels}
Based on the assumption of local smoothness among neighbors, another popular label denoising strategy generates the pseudo-label by incorporating the predictions of its neighbors \cite{chen2022self,wang2022exploring,tang2021nearest,cao2021towards,chen2022contrastive,ding2022proxymix}.
In particular, SSNLL \cite{chen2022self} conducts K-means clustering in the target domain and then aggregates predictions of its neighbors within the same cluster, and SCLM \cite{tang2022semantic} fuses the predictions of its two nearest neighbors and itself.
DIPE \cite{wang2022exploring} diminishes label ambiguity by correcting the pseudo label to the label of the majority of its neighbors.
Moreover, one line of work \cite{kim2021domain,tang2021nearest,dong2021confident} constructs an anchor set comprising only highly confident target samples, and a greedy chain-search strategy is proposed to find its nearest neighbor in the anchor set \cite{tang2021nearest,dong2021confident}.
While SFDA-APM \cite{kim2021domain} employs a point-to-set distance function to generate the pseudo labels, CAiDA \cite{dong2021confident} interpolates its nearest anchor to the target feature and uses the prediction of the synthetic feature instead.

Inspired by neighborhood aggregation \cite{liang2021domain}, one line of work \cite{cao2021towards,chen2022contrastive,ding2022proxymix,gao2022visual,tian2023robust,litrico2023guiding} maintains a memory bank storing both features and predictions of the target data $\{g(x_i),q_i\}_{i=1}^{n_t}$, allowing online refinement of pseudo labels.
Typically, the refined pseudo label is obtained through
\begin{equation}
    \hat{p}_i = \frac{1}{m}\sum\nolimits_{j \in \mathcal{N}_i} q_j,
\end{equation}
where $\mathcal{N}_i$ denotes the indices of $m$ nearest neighbors of $g(x_i)$ in the memory bank.
Specifically, ProxyMix \cite{ding2022proxymix} sharpens the network output $\bar{p}$ with the class frequency to avoid class imbalance and ambiguity, while RS2L \cite{tian2023robust} and NRC \cite{yang2021exploiting} devise different weighting schemes for neighbors during aggregation, respectively.
Instead of the soft pseudo label $\hat{p}$, AdaContrast \cite{chen2022contrastive} and DePT \cite{gao2022visual} utilize the hard pseudo label with the argmax operation.

\method{Complementary pseudo labels}
Motivated by the idea of negative learning \cite{kim2019nlnl}, PR-SFDA \cite{luo2021exploiting} randomly chooses a label from the set $\{1, \dots, C\} \backslash \{\hat{y}_i\}$ as the complementary label $\bar{y}_i$ and thus optimizes the following loss function,
\begin{equation}
    \min_\theta - \sum\nolimits_{i=1}^{n_t} \sum\nolimits_{c=1}^{C} \mathds{1}(\bar{y}_i = c) \log (1 - p_\theta(y_c|x_i)),
    \label{eq:npl}
\end{equation}
where $\hat{y}_i$ denotes the inferred hard pseudo label. 
We term $\bar{y}$ as a negative pseudo label that indicates the given input does not belong to this label.
Note that, the probability of correctness is $\frac{C-1}{C}$ for the complementary label $\bar{y}_i$, providing correct information even from wrong labels $\hat{y}_i$.
LD \cite{you2021domain} further develops a heuristic strategy to randomly select an informative complementary label with medium prediction scores, followed by CST \cite{xie2022learn}.
Besides, NEL \cite{ahmed2022cleaning} and PLUE \cite{litrico2023guiding} randomly pick up multiple complementary labels except for the inferred pseudo label and optimizes the multi-class variant of Eq.~(\ref{eq:npl}).
Further, some alternative methods \cite{wang2022source,xie2022towards,yang2022source_icme} generate multiple complementary labels according to a pre-defined threshold on the prediction scores.

\method{Optimization-based pseudo labels}
By leveraging the prior knowledge of the target label distribution like class balance \cite{zou2018unsupervised}, a line of SFDA methods \cite{you2021domain,sivaprasad2021uncertainty,huang2021model,zhao2022source} varies the threshold for each class so that a certain proportion of points per class are selected.
Such a strategy would avoid the `winner-takes-all' dilemma where pseudo labels come from several major categories and deteriorate the following training process.
In addition, ASL \cite{yan2021augmented} directly imposes the equi-partition constraint on the pseudo labels $\hat{p}_{i}$ and solves the following optimization problem,
\begin{equation}
\begin{aligned}
    \min_{\hat{p}_{i}} & - \sum\nolimits_i\sum\nolimits_c \hat{p}_{ic} \log p_\theta(y_c|x_i) + \lambda \sum\nolimits_i\sum\nolimits_c \hat{p}_{ic} \log \hat{p}_{ic}, \\
    s.t. \; & \forall i,c:\; \hat{p}_{ic}\in [0,1], \;\sum\nolimits_c \hat{p}_{ic}=1, \;\sum\nolimits_i \hat{p}_{ic}=\frac{n_t}{C}.
\end{aligned}
\end{equation}
Similarly, IterNLL \cite{zhang2021unsupervised} provides a closed-form solution of $\{\hat{p}\}$ under the uniform prior assumption, and NOTELA \cite{boudiaf2023in} exploits the Laplacian regularizer to promote similar pseudo-labels for nearby points in the feature space. 
Later, KUDA \cite{sun2022prior} comes up with the hard constraint $\hat{p}_{ic}\in \{0,1\}$ and solves the zero-one programming problem.
Moreover, U-D4R \cite{xu2022denoising} estimates a joint distribution matrix between the observed and latent labels to correct the pseudo labels.

\method{Ensemble-based pseudo labels}
Rather than rely on one single noisy pseudo label, ADV-M \cite{wang2021providing} and ISFDA \cite{li2021imbalanced} generate a secondary pseudo label to aid the primary one.
Besides, ASL \cite{yan2021augmented} and C-SFDA \cite{karim2023csfda} adopt a weighted average of predictions under multiple random data augmentation, while ST3D \cite{yang2021st3d} and ELR \cite{yi2023when} ensemble historical predictions from previous training epochs.
NEL \cite{ahmed2022cleaning} further aggregates the logits under different data augmentation and trained models at the same time.
Inspired by a classic semi-supervised learning method \cite{laine2017temporal}, a few SFDA methods \cite{liang2022dine,panagiotakopoulos2022online} maintain an EMA of predictions at different time steps as pseudo labels.
By contrast, some methods \cite{hegde2021uncertainty,truong2022conda,karim2023csfda} maintain a mean teacher model \cite{tarvainen2017mean} that generates pseudo labels for the current student network.
Additionally, other methods try to generate the pseudo label based on predictions from different models, \eg, multiple source models \cite{liang2022dine,li2022union,li2022improving}, a multi-head classifier \cite{kundu2021generalize,li2022adaptive}, and models from both source and target domains \cite{hou2021visualizing}.
In particular, SFDA-VS \cite{ye2021source} follows the idea of Monte Carlo (MC) dropout \cite{gal2016dropout} and obtains the final prediction through multiple stochastic forward passes.
Co-learn \cite{zhang2022colearning} further trains an auxiliary target model in another backbone and produces pseudo labels based on the consistency of dual models and confidence filtering.

Another line of ensemble-based SFDA methods \cite{cao2021towards,xiong2022source,yeh2022boosting} tries to integrate predictions from different labeling criteria using a weighted average. 
For example, e-SHOT-CE \cite{cao2021towards} utilizes both centroid-based and neighbor-based pseudo labels, and PLA-DAP \cite{xiong2022source} combines centroid-based pseudo labels and the original network outputs.
CSFA \cite{yeh2022boosting} further develops a switching strategy to choose between centroid-based pseudo labels or neighbor-based pseudo labels.
Further, OnDA \cite{panagiotakopoulos2022online} integrates the centroid-based pseudo labels and the temporal ensemble of network predictions using a dot product operation.  
Apart from the weighting scheme, other approaches \cite{qiu2021source,dong2021confident,wang2022exploring,yan2022dual,kumar2023conmix} explore different labeling criteria in a cascade manner.
For instance, several methods (\eg, CAiDA \cite{dong2021confident}, DIPE \cite{wang2022exploring}, and Twofer \cite{liu2023twofer}) employ the neighbor-based labeling criterion based on the centroid-based pseudo labels.
CPGA \cite{qiu2021source} and DMAPL \cite{yan2022dual} also adopt the centroid-based pseudo labels but employ a self-ensembling technique to obtain the final predictions over historical predictions.
Besides, CoNMix \cite{kumar2023conmix} further calculates a cluster consensus matrix and utilizes it in the temporal ensemble over centroid-based pseudo labels. 

\method{Learning with pseudo labels}
Different robust divergence measures ${d}_{pl}$ have been employed in existing pseudo-labeling-based SFDA methods.
Generally, most of them employed the standard cross-entropy loss for all the target samples with hard pseudo labels \cite{liang2020we,liang2021source,yan2021augmented,kothandaraman2021ss,wang2022cross,peng2022multi,xiong2022source,guan2022polycentric,tang2022semantic,luo2022multi,kumar2023conmix} or soft pseudo labels \cite{tang2021model,deng2021universal}.
Note that, several methods \cite{ding2022proxymix,tian2022source_tist,tian2023robust} convert hard pseudo labels into soft pseudo labels using the label smoothing trick \cite{muller2019does}.
As pseudo labels are noisy, many SFDA methods incorporate an instance-specific weighting scheme into the standard cross-entropy loss, including hard weights \cite{kim2020towards,sivaprasad2021uncertainty,kim2021domain,hou2021visualizing,you2021domain,paul2021unsupervised,chen2021source,dasgupta2022overcoming}, and soft weights \cite{huang2021model,ye2021source,chen2022learning,lee2022confidence,liu2022memory,li2022adaptive,song2022ss8}.
Besides, AUGCO \cite{prabhu2022augco} considers the class-specific weight in the cross-entropy loss to mitigate the label imbalance.
In addition to the cross-entropy loss, alternative choices include the Kullback–Leibler divergence \cite{tian2023robust}, the generalized cross entropy \cite{rusak2022if}, the inner product distance between the pseudo label and the prediction \cite{yang2021exploiting,qiu2021source}, and a new discrepancy measure $\log(1-\hat{y}^Tp(y|x;\theta))$ \cite{yi2023when}.
Inspired by a classic noisy label learning approach \cite{wang2019symmetric}, BMD \cite{qu2022bmd} and OnDA \cite{panagiotakopoulos2022online} employ the symmetric cross-entropy loss to guide the self-labeling process.
SFAD \cite{jiao2022source} further develops a normalized symmetric cross-entropy variant by incorporating the normalized cross-entropy loss \cite{ma2020normalized}, and CATTAn \cite{thopalli2022domain} exploits the negative log-likelihood ratio between correct and competing classes \cite{yao2020negative}.

\subsubsection{Consistency Training}
As a prevailing strategy in recent semi-supervised learning literature \cite{yang2021survey,chen2022semi}, consistency regularization is primarily built on the smoothness assumption or the manifold assumption, which aims to enforce consistent network predictions or features under variations in the input data space or the model parameter space.
Besides, another line of consistency training methods tries to match the statistics of different domains even without the source data. 
In the following, we review different consistency regularizations under data and model variations together with other consistency-based distribution matching methods.

\method{Consistency under data variations}
Benefiting from advanced data augmentation techniques (\eg, Cutout \cite{devries2017improved}, RandAugment \cite{cubuk2020randaugment}), several prominent semi-supervised learning methods \cite{xie2020unsupervised,sohn2020fixmatch} unleash the power of consistency regularization over unlabeled data that can be effortlessly adopted in SFDA approaches.
Formally, an exemplar of consistency regularization \cite{sohn2020fixmatch} is expressed as:
\begin{equation}
    \mathcal{L}_{fm}^{con} = \frac{1}{n_t}\sum_{i=1}^{n_t} \text{CE}\left(p_{\tilde{\theta}}(y|x_i), p_{\theta}(y_c|\hat{x}_i)\right),
\label{eq:fixmatch}
\end{equation}
where $p_{\theta}(y|x_i)=p(y|x_i;\theta)$, and $\text{CE}(\cdot,\cdot)$ refers to cross-entropy between two distributions.
Besides, $\hat{x}_i$ represents the variant of $x_i$ under another augmentation transformation, and $\tilde{\theta}$ is a fixed copy of current network parameters $\theta$.
Another representative consistency regularization is virtual adversarial training (VAT) \cite{miyato2018virtual} that devises a smoothness constraint as follows,
\begin{equation}
    \mathcal{L}_{vat}^{con} = \frac{1}{n_t}\sum_{i=1}^{n_t} \max_{\|\Delta_i\| \leq \epsilon}[\text{KL}(p_{\tilde{\theta}}(y|x_i) \;||
    \; p_{\theta}(y|x_i + \Delta_i))],
\label{eq:vat}
\end{equation}
where $\Delta_i$ is a perturbation that disperses the prediction most within an intensity range of $\epsilon$ for the target data $x_i$, and $\text{KL}$ denotes the Kullback–Leibler divergence.

ATP \cite{wang2022source} and ALT \cite{wu2023when} directly employ the same consistency regularization in Eq.~(\ref{eq:fixmatch}), while other SFDA methods \cite{wang2021learning,chen2022contrastive,zhang2022divide,gao2022visual,lee2022feature,kumar2023conmix} replace $p_{\tilde{\theta}}(y|x_i)$ with hard pseudo labels for target data under weak augmentation, followed by a cross-entropy loss for target data under strong augmentation.
Note that, many of these hard labels are obtained using label denoising techniques mentioned earlier.
Apart from strong augmentations, SFDA-ST \cite{paul2021unsupervised} and RuST \cite{luo2022multi} seek geometric consistency by switching the order of the model and spatial transforms (\eg, image mirroring and rotation).
FMML \cite{peng2022toward} and FTA-FDA \cite{peng2022multi} resort to Fourier-based data augmentation \cite{yang2020fda}, while ProSFDA \cite{hu2022prosfda} and SFDA-FSM \cite{yang2022source} need to learn the domain translation module at first.
In addition to the output-level consistency, FAUST \cite{lee2022feature} and ProSFDA \cite{hu2022prosfda} seek feature-level consistency under different augmentations, and TeST \cite{sinha2023test} even introduce a flexible mapping network to match features under two different augmentations. 
On the contrary, OSHT \cite{feng2021open} maximizes the mutual information between predictions of two different transformed inputs to retain the semantic information as much as possible.

Following the objective in Eq.~(\ref{eq:vat}), another line of SFDA methods \cite{li2020model,yan2021augmented,zhou2022domain_bibm} attempts to encourage consistency between target samples with their data-level neighbors, and APA \cite{sun2023domain} learns the neighbors in the feature space.
Instead of generating the most divergent neighbor $x_i+\Delta_i$ according to the predictions, JN \cite{li2022jacobian} devises a Jacobian norm regularization to control the smoothness in the neighborhood of the target sample.
Further, G-SFDA \cite{yang2021generalized} discovers multiple neighbors from a memory bank and minimizes their inner product distances over the predictions.
Moreover, Mixup \cite{zhang2018mixup} performs linear interpolations on two inputs and their corresponding labels, which can be treated as seeking consistency under data variation \cite{liang2022dine,lee2022confidence,guan2022polycentric,pei2022uncertainty,kumar2023conmix}.

\method{Consistency under model variations}
Reducing model uncertainty \cite{gal2016dropout} is also beneficial for learning robust features for SFDA tasks, on top of uncertainty measured with input change.
Following a classic uncertainty estimation framework, MC dropout \cite{gal2016dropout}, FAUST \cite{lee2022feature} activates dropout in the model and performs multiple stochastic forward passes to estimate the epistemic uncertainty.
SFDA-UR \cite{sivaprasad2021uncertainty} and RuST \cite{luo2022multi} append multiple extra dropout layers behind the feature encoder and minimize the mean squared error between predictions as uncertainty.
Further, several methods \cite{xia2022privacy,luo2022multi,yuan2023data} add different perturbations to the intermediate features to promote predictive consistency.
In addition, FMML \cite{peng2022toward} offers another type of model variation by network slimming and sought predictive consistency across different networks.

Another model-based consistency regularization requires the existence of the source and target models and thus minimizes the difference across different models, \eg, feature-level discrepancy \cite{kothandaraman2023salad} and output-level discrepancy \cite{ye2021source,zhang2021matching,liang2022dine,liu2022unsupervised,conti2022cluster,sinha2023test}.
Furthermore, the mean teacher framework \cite{tarvainen2017mean} is also utilized to form a strong teacher model and a learnable student model, where the teacher and the student share the same architecture, and the weights of the teacher model $\theta_{tea}$ is gradually updated by,
\begin{equation}
    \theta_{tea} = (1-\eta) \theta_{tea} + \eta \theta
\end{equation}
where $\theta$ denotes the weights of the student model, and $\eta$ is the momentum coefficient.
Therefore, the mean teacher model is regarded as a temporal ensemble of student models with more accurate predictions.
Anat-SFDA \cite{bigalke2022anatomy} minimizes the $L_1$ distance between predictions of the teacher and student models, and TransDA \cite{yang2022self} exploits the features extracted by the teacher model to obtain better pseudo labels for supervising the student model.
Moreover, UAMT \cite{hegde2021uncertainty} even incorporates MC dropout into the mean teacher model for reliable pseudo labels along with the sample variances.
Similarly, MoTE \cite{vs2022mixture} obtains multiple mean teacher models individually and seeks predictive consistency for each teacher-student pair.
In reality, a small number of SFDA methods \cite{lao2021hypothesis,li2022adaptive,zong2022domain} also consider the multi-head classifier and encourage predictions by different heads to be consistent for robust features. 
TAN \cite{xu2020transfer} introduces an extra module to align feature encoders across domains with the source classifier fixed.

\method{Consistency under data \& model variations}
In reality, data variation and model variation could be integrated into a unified framework.
For example, the mean teacher framework \cite{tarvainen2017mean} is enhanced by blending strong data augmentation techniques, and the discrepancy between predictions of the student and teacher models is minimized as follows, 
\begin{equation}
    \mathcal{L}_{mt}^{con} = \mathbb{E}_{x \in \mathcal{D}_t} d_{mt}(p(y|x, \theta), p(y|\tau(x), \theta_{tea})),
\end{equation}
where $\tau(\cdot)$ denotes the strong data augmentation, and $d_{mt}$ denotes the divergence measure, \eg, the mean squared error \cite{zhang2021source,wang2022unsupervised}, the Kullback-Leibler divergence \cite{liu2022source_eccv,hou2021visualizing}, and the cross-entropy loss \cite{xiong2021source,chen2022self,chen2022learning,gao2022visual,vs2023towards,chu2023adversarial,zhao20231st,zhang2023refined}.
SFDA-DML \cite{zhang2022source_b} proposes a mutual learning framework that involves two target networks and optimizes one target network with pseudo labels from the mean teacher model of the other network under different data variations.
Besides, several methods \cite{vs2022target,vs2022instance,liu2022source,huang2021model,li2022source_cvpr,yang2023source} attempt to extract useful information from the teacher and employ task-specific loss functions to seek consistency.
Apart from the output-level consistency, TT-SFUDA \cite{vs2022target} matches the features extracted by different models with the MSE distance, while AdaContrast \cite{chen2022contrastive} and PLUE \cite{litrico2023guiding} learn semantically consistent features like \cite{he2020momentum}.

Instead of strong data augmentations, LODS \cite{li2022source_cvpr} and SFIT \cite{hou2021visualizing} use the style transferred image instead, MAPS \cite{ding2023maps} considers spatial transforms, and SMT \cite{zhang2021source} elaborates the domain-specific perturbation by averaging the target images. 
In addition to model variations of the mean teacher scheme, OnTA \cite{wang2021ontarget} distills knowledge from the source model to the target model, while GarDA \cite{chen2023generative} and HCL \cite{huang2021model} promote consistency among the current model and historical model in feature space and prediction space, respectively.
To selectively distill knowledge from the teacher, MAPS \cite{ding2023maps} also develops a sample filtering strategy to only consider confident samples during consistency training.

\method{Miscellaneous consistency regularizations}
To prevent excessive deviation from the original source model, a flexible strategy is adopted by a few methods \cite{li2020model,yan2021augmented,zhou2022domain_bibm,liu2022source_ai,xiong2022source,yuan2022source} by establishing a parameter-based regularization term $\|\theta_s - \theta\|_2^2$, where $\theta_s$ is the fixed source weight.
Further, DTAC \cite{yang2022source_icme} utilizes a robust $L_1$ distance instead.
Another line of research focuses on matching the batch normalization (BN) statistics (\ie, the mean and the variance) across models with different measures, \eg, the Kullback–Leibler divergence \cite{ishii2021source}, and the MSE error \cite{zhang2021source,yang2022source_icme,ahmed2022cross,li2022source}, whereas OSUDA \cite{liu2021adapting} encourages the learned scaling and shifting parameters in BN layers to be consistent.
Similarly, an explicit feature-level regularization \cite{liu2021ttt++,bozorgtabar2022anomaly} is further developed that matches the first and second-order moments of features of different domains. 

\method{Remarks}
As for the network architecture in the target domain, a unique design termed dual-classifier structure is utilized to seek robust domain-invariant representations.
For example, BAIT \cite{yang2021casting} introduces an extra $C$-dimensional classifier to the source model, forming a dual-classifier model with a shared feature encoder. 
During adaptation in the target domain, the shared feature encoder and the new classifier are trained with the classifier from the source domain head fixed. 
Such a training scheme has also been utilized by many SFDA methods \cite{du2021generation,tian2022source_tist,wang2022exploring,tian2022source_cee,xia2021adaptive,yuan2022source,sivaprasad2021uncertainty,xia2022privacy,yeh2022boosting} through modeling the consistency between different classifiers.
Besides, SFDA-APM \cite{kim2021domain} develops a self-training framework that optimizes the shared feature encoder and two classification heads with different pseudo-labeling losses, respectively. 
By contrast, CdKD-TSML \cite{li2022teacher} puts forwards two feature encoders with the shared classifier and performs mutual knowledge distillation across models to enhance the generalization.

\subsubsection{Clustering-based Training}
Except for the pseudo-labeling paradigm, one assumption explicitly or implicitly made in virtually all semi-supervised learning algorithms is the cluster assumption \cite{chapelle2002cluster,chapelle2005semi,van2020survey}, which states that the decision boundary should not cross high-density regions, but instead lie in low-density regions. 
Motivated by this, another popular family of SFDA approaches favors low-density separation through reducing the uncertainty of the target network predictions \cite{liang2020we,li2020model,ye2022alleviating} or encouraging clustering over the target features \cite{li2021imbalanced,qiu2021source}.

\method{Entropy minimization}
To encourage confident predictions for unlabeled target data, ASFA \cite{xia2022privacy} borrows robust measures from information theory and minimizes the following $\alpha$-Tsallis entropy \cite{tsallis1988possible},
\begin{equation}
    \mathcal{L}_{tsa} = \frac{1}{n_t} \sum_{i=1}^{n_t} \frac{1}{\alpha - 1} [1 - \sum_{c=1}^{C} p_\theta(y_c|x_i)^\alpha],
\label{eq:tsa}
\end{equation}
where $\alpha>$ 0 is called the entropic index. 
Note that, when $\alpha$ approaches 1, the Tsallis entropy exactly recovers the standard Shannon entropy in $\mathcal{H}(p_\theta(y|x_i)) = \sum_c p_\theta(y_c|x_i) \log p_\theta(y_c|x_i)$.
In practice, the conditional Shannon entropy $\mathcal{H}(p_\theta(y|x))$ has been widely used in SFDA methods \cite{li2020model,bateson2020source,liu2021adapting,yan2021augmented,sivaprasad2021uncertainty,you2021domain,kundu2021generalize,bateson2022source,liu2022memory,zhou2022domain_bibm,pei2022uncertainty,lee2022feature,zhang2022colearning,liu2022unsupervised,kondo2022source,sinha2023test}.
Besides, there exist many variants of standard entropy minimization.
For example, SS-SFDA \cite{kothandaraman2021ss} ranks the target images first and minimizes the entropy from confident data to less-confident data, SFDA-VS \cite{ye2021source} develops a nonlinear weighted entropy minimization loss that weighed more on the low-entropy samples.
Moreover, TT-SFUDA \cite{vs2022target} focuses on the entropy of the ensemble predictions under multiple augmentations, and SFDA-GEM \cite{yuan2022source} accommodates the Gumbel-softmax trick by modifying the predictions with additive Gumbel noise to improve the robustness.

When $\alpha$ equals 2, the Tsallis entropy in Eq.~(\ref{eq:tsa}) corresponds to the maximum squares loss \cite{chen2019domain}, $\sum_c p_\theta(y_c|x_i)^2$.
Compared with the Shannon entropy, the gradient of the maximum squares loss increases linearly, preventing easy samples from dominating the training process in the high probability area.
Also, another line of SFDA methods \cite{liu2021source,luo2021exploiting,li2022union,li2022improving,kumar2023conmix} has employed the maximum squares loss. 
Built on this, Batch Nuclear-norm Maximization (BNM) \cite{cui2020towards} approximates the prediction diversity using the matrix rank, which is further adopted in other SFDA methods \cite{wang2021learning,tian2022source_cee,jiao2022source}. 
Additionally, another line of SFDA methods \cite{ye2022alleviating,plananamente2022test} pays attention to the class confusion matrix, which minimizes the inter-class confusion to ensure no samples are ambiguously classified into two classes at the same time.
RNA++ \cite{plananamente2022test} also discusses the usage of complement entropy that flattens the predicted probabilities of classes except for the pseudo label.

\method{Mutual information maximization}
Another favorable clustering-based regularization is mutual information maximization, which tries to maximize the mutual information \cite{bridle1991unsupervised,shi2012information} between inputs and the discrete labels as follows,
\begin{equation}
\begin{aligned}
    &\max_\theta \mathcal{I} (\mathcal{X}_t, \hat{\mathcal{Y}_t}) = \mathcal{H}(\hat{\mathcal{Y}_t}) - \mathcal{H}(\hat{\mathcal{Y}_t}|\mathcal{X}_t) \\
    =& -\sum_{c=1}^{C} \bar{p}_\theta(y_c) \log \bar{p}_\theta(y_c) + \frac{1}{n_t} \sum_{i=1}^{n_t}\sum_{c=1}^{C} p_\theta(y_c|x_i) \log p_\theta(y_c|x_i),
\end{aligned}
\label{eq:mi}
\end{equation}
where $\bar{p}_\theta(y_c)=\frac{1}{n_t}\sum_i p_\theta(y_c|x_i)$ denotes the $c$-th element in the estimated class label distribution.
Intuitively, increasing the additional diversity term $\mathcal{H}(\hat{\mathcal{Y}_t})$ encourages the target labels to be uniformly distributed, circumventing the degenerate solution where every sample is assigned to the same class.
Such a regularization is first introduced in SHOT \cite{liang2020we} and SHOT++ \cite{liang2021source} for image classification and then employed in plenty of SFDA methods \cite{ishii2021source,lao2021hypothesis,yan2021source,wang2021ontarget,peng2022multi,yang2022self,zhao2022adaptive,li2022jacobian,guan2022polycentric,li2022teacher,diamant2022reconciling,wang2022exploring,zhang2022source,han2022privacy,taufique2023continual,song2022ss8,zhang2022source_b,feng2023cross,dai2023mvit,litrico2023guiding}.
Beyond visual classification tasks, it has also been applied in other source-free applications \cite{cao2021towards,xu2022learning,huang2022relative,xu2022extern,chen2022source,an2022privacy,liu2022source_arxiv,ahmed2022cross}.
Instead of using the network prediction $p_\theta(y|x)$, a few methods \cite{tang2021model,tang2021nearest,tang2022semantic} utilize the ensemble prediction based on its neighbors for mutual information maximization.
DaC \cite{zhang2022divide} and U-SFAN \cite{roy2022uncertainty} introduce a balancing parameter between two terms in Eq.~(\ref{eq:mi}) to increase the flexibility.
In particular, U-SFAN \cite{roy2022uncertainty} develops an uncertainty-guided entropy minimization loss by focusing more on the low-entropy predictions, whereas ATP \cite{wang2022source} encompasses the instance-wise uncertainty in both terms of Eq.~(\ref{eq:mi}). 
VMP \cite{jing2022variational} further provides a probabilistic framework based on Bayesian neural networks and integrated mutual information into the likelihood function.

Note that the diversity term can be rewritten as $\mathcal{H}(\hat{\mathcal{Y}_t}) = -\text{KL}(\bar{p}_\theta(y)||\mathcal{U}) + \log C$, where $\bar{p}_\theta(y)$ is the average label distribution in the target domain, and $\mathcal{U}$ denotes a $C$-dimensional uniform vector.
This term alone has also been used in many SFDA methods \cite{hou2021visualizing,du2021generation,yang2021exploiting,chen2022contrastive,kundu2022concurrent,panagiotakopoulos2022online,tian2022source_tist,panagiotakopoulos2022online,thopalli2022domain} to avoid class collapse.
To better guide the learning process, previous works \cite{krause2010discriminative,hu2017learning} modify the mutual information regularization by substituting a reference class-ratio distribution in place of $\mathcal{U}$.
Different from AdaMI \cite{bateson2022source} that leverages the target class ratio as a prior, UMAD \cite{liang2021umad} utilizes the flattened label distribution within a mini-batch instead to mitigate the class imbalance problem, and AUGCO \cite{prabhu2022augco} maintains the moving average of the predictions as the reference distribution.

\method{Explicit clustering}
On top of these clustering-promoting output-level regularizations, other clustering-based methods \cite{li2021imbalanced,liu2022source_eccv,zhao2022adaptive} explicitly perform feature-level clustering to encourage more discriminative decision boundaries.
Specifically, both ISFDA \cite{li2021imbalanced} and LCCL \cite{zhao2022adaptive} rely on the pseudo labels and pull closer features from the same class while pushing farther those from different classes.
SDA-FAS \cite{liu2022source_eccv} leverages the prototypical contrastive alignment scheme to match target features and features of the source prototypes.
Inspired by neighborhood clustering \cite{saito2020universal}, CPGA \cite{qiu2021source} and StickerDA \cite{kundu2022concurrent} calculate the normalized instance-to-instance similarity with a memory bank $\{\bar{g}(x_i)\}_{i=1}^{n_t}$ and thus minimize the following entropy,
\begin{equation}
    \mathcal{L}_{nc} = -\frac{1}{n_t} \sum_{i=1}^{n_t} \sum_{j\not=i} s_{ij}\log s_{ij},
\end{equation}
where $s_{ij}=\frac{exp(g(x_i)^T \bar{g}(x_j)/\tau)}{\sum_{k\not=i} exp(g(x_i)^T \bar{g}(x_k)/\tau)}$ represents the normalized similarity between the $i$-th target feature and the $j$-th feature stored in the memory bank, and $\tau$ denotes the temperature parameter.
Later, A$^2$Net \cite{xia2021adaptive} and CSFA \cite{yeh2022boosting} integrate the pseudo labels with the normalized similarity to seek semantic clustering.
Several remaining methods \cite{yang2022attracting,wu2023when,chen2023contrast} develop a simplified variant to learn class-discriminative features with local clustering in the target dataset, respectively.
In addition to the feature-level clustering strategies, DECDA \cite{zhu2021source} takes inspiration from the classic deep clustering method \cite{xie2016unsupervised} and learns feature representations and cluster assignments simultaneously.

\subsubsection{Source Distribution Estimation}
Another favored family of SFDA approaches compensates for the absence of source data by inferring data from the pre-trained model, which turns the challenging SFDA problem into a well-studied DA problem.
Existing distribution estimation methods could be divided into three categories: data generation from random noises \cite{morerio2020generative,li2020model,kurmi2021domain}, data translation from target samples \cite{hou2021visualizing,yan2021source,zhou2022generative}, and data selection from the target domain \cite{liang2021source,yang2021casting,ding2022proxymix}.

\method{Data generation}
To produce valid target-style training samples, 3C-GAN \cite{li2020model} introduces a data generator $G(\cdot;\theta_G)$ conditioned on randomly sampled labels along with a binary discriminator $D(\cdot;\theta_D)$ and aims to optimize a similar objective to conditional GAN \cite{mirza2014conditional} in the following,
\begin{equation}
\begin{aligned}
    \min_{\theta_G}\max_{\theta_D}\ &\mathbb{E}_{x_t \in \mathcal{X}_t}[\log D(x_t)] + \mathbb{E}_{y_t,z} [\log(1 - D(G(y_t,z)))] \\
    & - \lambda_s \mathbb{E}_{y_t,z} \sum\nolimits_{c}\mathds{1}(y_t=c) \log p(y_c|G(y_t,z),\theta),
\end{aligned}
\label{eq:cgan}
\end{equation}
where $z$ is a random noise vector, $y_t$ is a pre-defined label, $\lambda_s>0$ is a balancing parameter, and $\theta$ denotes the parameters of the pre-trained prediction model.
By alternately optimizing $\theta_G$ and $\theta_D$, the resulting class conditional generator $G$ is able to produce multiple surrogate labeled source instances for the following domain alignment step, \ie, $\mathcal{D}_g = \{x_i,y_i\}_{i=1}^{n_g}$, where $x_i=G(y_i,z)$ and $n_g$ is the number of generated samples. 
Different from SDG-CMT \cite{zhou2022domain_bibm} that employs the same objective for medical data, PLR \cite{morerio2020generative} ignores the last term in Eq.~(\ref{eq:cgan}) to infer diverse target-like samples.
SDDA \cite{kurmi2021domain} additionally maximizes the log-likelihood of generated data $x_g$ and employs two different domain discriminators, \ie, a data-level GAN discriminator and a feature-level domain discriminator.
SFDA-ADT \cite{liu2022source_ai} integrates multiple discriminators together to match the data-label joint distribution between $\mathcal{D}_g$ and $\mathcal{D}_t$.

In addition to adversarial training, DI \cite{nayak2021mining} performs Dirichlet modeling with the source class similarity matrix and directly optimizes the noisy input to match its network output with the sampled softmax vector $q$ as follows,
\begin{equation}
    x_g = \arg\min_x \text{CE}(q, p_\theta(y|x))
\end{equation}
is also known as data impression of the source domain.
SDG-MA \cite{xu2023universal} employs the relationship preserving loss \cite{gatys2016image} to ensure style consistency during learning the label-conditional data generator.
Moreover, SPGM \cite{yang2022revealing} first estimates the target distribution using Gaussian Mixture Models (GMM) and then constrains the generated data to be derived from the target distribution. 

Motivated by recent advances in data-free knowledge distillation \cite{yin2020dreaming,liu2021data}, SFDA-KTMA \cite{liu2021source} exploits the moving average statistics of activations stored in BN layers of the pre-trained source model and imposes the following BN matching constraint on the generator, 
\begin{equation}
    \mathcal{L}_{bn} = \sum\nolimits_l \sum\nolimits_i \|\mu_{g,l}^{(i)} - \mu_{s,l}^{(i)}\|_2 + \|{\delta_{g,l}^{(i)}}^2 - {\delta_{s,l}^{(i)}}^2\|_2,
\label{eq:bns}
\end{equation}
where $B$ is the size of a mini-batch, $\mu_{s,l}^{(i)}$ and ${\delta_{s,l}^{(i)}}^2$ represent the corresponding running mean and variance stored in the source model, and $\mu_{g,l}^{(i)} = \frac{1}{B}\sum\nolimits_z f_l^{(i)}(x_g)$ and ${\delta_{g,l}^{(i)}}^2 = \frac{1}{B}\sum\nolimits_z (f_l^{(i)}(x_g) - \mu_{g,l}^{(i)})^2$ denote the batch-wise mean and variance estimates of the $i$-th feature channel at the $l$-th layer for synthetic data from the generator, respectively.
As indicated in \cite{li2017demystifying}, matching the BN statistics facilitates forcing the generated data equipped with the source style.
SFDA-FSM \cite{yang2022source} further minimizes the $L_2$-norm difference between intermediate features (\aka, the content loss \cite{gatys2016image}) to preserve the content knowledge of the target domain.

\method{Data translation}
Under the assumption that the style information is embedded in low-frequency components, FTA-FDA \cite{peng2022multi} relies on spectrum mixup \cite{yang2020fda} across domains to synthesize samples through inverse Fourier Transform.
SSFT-SSD \cite{yan2021source} initializes $x_g$ as $x_t \in \mathcal{X}_t$ and directly performs optimization on the input space with the gradient of the $L_2$-norm regularized cross-entropy loss being zero.
Note that these two methods do not include any additional modules in the translation process.
On the contrary, SFDA-TN \cite{sahoo2020unsupervised} optimizes a learnable data transformation network that maps target data to the source domain so that the maximum class probability is maximized.
Inspired by the success of prompt learning in computer vision \cite{bahng2022visual}, ProSFDA \cite{hu2022prosfda} adds a learnable image perturbation to all target data so that the BN statistics can be aligned with those stored in the source model.
Hereafter, the style-transferred image is generated using spectrum mixup \cite{yang2020fda} between the target image and its perturbed version.

Another line of data translation methods \cite{hou2020source,hou2021visualizing,zhou2022generative} explicitly introduces an additional module $\mathcal{A}$ to transfer target data to source-like style.
In particular, SFDA-IT \cite{hou2020source} optimizes the translator with the style matching loss in Eq.~(\ref{eq:bns}) as well as the feature-level content loss, with the source model frozen.
In addition, SFDA-IT \cite{hou2020source} further employs entropy minimization over the fixed source model to promote semantic consistency. 
To improve the performance of style transfer, SFIT \cite{hou2021visualizing} further develops a variant of the style reconstruction loss \cite{gatys2016image} as follows,
\begin{equation}
    \mathcal{L}_{style} = \|g(x)g(x)^{T} - g(\mathcal{A}(x))g(\mathcal{A}(x))^{T}\|_2,
\end{equation}
where $g(x) \in \mathcal{R}^{n_c \times HW}$ denotes the reshaped feature map, and $H, W$ and $n_c$ represent the feature map height, width, and the number of channels, respectively. 
The channel-wise self correlations $g(x)g(x)^T$ are also known as the Gram matrix.
Meanwhile, SFIT \cite{hou2021visualizing} maintains the relationship of outputs between different networks.
GDA \cite{zhou2022generative} also relies on BN-based style matching and entropy minimization but further enforces the phase consistency and the feature-level consistency between the original image and the stylized image to preserve the semantic content.
By contrast, AOS \cite{hong2022source} replaces the first convolution block in the source model with a learnable encoder to ensure that BN statistics at the remaining layers match those stored statistics.

\method{Data selection}
Besides the synthesized source samples with data generation or data translation, another family of SFDA methods \cite{liang2021source,yang2021casting,ding2022proxymix,wang2022source,chen2022self,yang2023divide} selects source-like samples from the target domain as surrogate source data, greatly reducing computational costs.
Typically, the whole target domain is divided into two splits, \ie, a labeled subset $\hat{\mathcal{X}}_{tl}$ and an unlabeled subset $\hat{\mathcal{X}}_{tu}$, and the labeled subset acts as the inaccessible source domain.
Based on network outputs of the adapted model in the target domain, SHOT++ \cite{liang2021source} makes the first attempt towards data selection by selecting low-entropy samples in each class for an extra intra-domain alignment step.
Such an adapt-and-divide strategy has been adopted in later works \cite{ye2021source,liu2021source,wang2022source} where the ratio or the number of selected samples per class is always kept same to prevent severe class imbalance.
Besides the entropy criterion, DaC \cite{zhang2022divide} utilizes the maximum softmax probability, and A2SFOD \cite{chu2023adversarial} exploits the predictive variance based on MC dropout.
Furthermore, BETA \cite{yang2023divide} constructs a two-component GMM over all the target features to separate the confident subset $\hat{\mathcal{X}}_{tl}$ from the less confident subset $\hat{\mathcal{X}}_{tu}$.

Instead of the adapted target model, a few approaches \cite{du2021generation,ding2022proxymix,yan2022dual,liu2022self,huang2022relative,zhang2022lightweight,shen2022benefits} utilize the source model to partition the target domain before the following intra-domain adaptation step. 
For each class separately, PS \cite{du2021generation} and MTRAN \cite{huang2022relative} select the low-entropy sample, ProxyMix \cite{ding2022proxymix} leverages the distance from target features to source class prototypes, and DMAPL \cite{yan2022dual} and SAB \cite{liu2022self} adopt the maximum prediction probability.
With multiple pre-trained source models being available, LSFT \cite{zhang2022lightweight} computes the variance of different target predictions, while MSFDA-SPL \cite{shen2022benefits} obtains the confidence from a weighted average prediction.
To simulate the source domain more accurately, PS \cite{du2021generation} and MTRAN \cite{huang2022relative} further apply the mixup augmentation technique after the dataset partition step.
On the contrary, other existing methods \cite{yang2021casting,xia2021adaptive,tian2021source,ma2021uncertainty,ye2021source,chen2022self,chu2022denoised,zhang2022colearning,yuan2022source,tian2022source_tist,tian2022source_cee,yeh2022boosting,wu2023when} do not fix the domain partition but alternately update the domain partition and learn the target model in the adaptation step.
For example, SSNLL \cite{chen2022self} follows the small loss trick for noisy label learning and assigns samples with small loss to the labeled subset at the beginning of each epoch.
Except for the global division, BAIT \cite{yang2021casting} and SFDA-KTMA \cite{liu2021source} split each mini-batch into two sets according to the criterion of entropy ranking.
Besides, D-MCD \cite{chu2022denoised} and RCHC \cite{diamant2022reconciling} employ the classifier determinacy disparity and the agreement of different self-labeling strategies for dividing the current mini-batch, respectively.

\method{Feature estimation}
In contrast to source data synthesis, previous works \cite{qiu2021source,tian2022vdm,ding2022source} provide a cost-effective alternative by simulating the source features.
Intuitively, MAS$^3$ \cite{stan2021unsupervised} and its following works \cite{rostami2021lifelong,stan2022domain} require learning a GMM over the source features before model adaptation, which may not hold in real-world scenarios.
Instead, VDM-DA \cite{tian2022vdm} constructs a proxy source domain by randomly sampling features from the following GMM, 
\begin{equation}
    p_v(z) = \sum\nolimits_{c=1}^{C} \pi_c\; \mathcal{N}(z|\mu_c,\Sigma_c),
\end{equation}
where $z$ denotes the virtual domain feature, and $p_v(z)$ is the distribution of the virtual domain in the feature space.
For each Gaussian component, $\pi_c \ge 0$ represents the mixing coefficient satisfying $\sum_c \pi_c=1$, and $\mu_c,\Sigma_c$ represent the mean and the covariance matrix, respectively.
Specifically, $\mu_c$ is approximated by the $L_2$-normalized class prototype \cite{chen2018closer} that corresponds to the $c$-th row of weights in the source classifier, and a class-agnostic covariance matrix is heuristically determined by pairwise distances among different class prototypes. 
To incorporate relevant knowledge in the target domain, SFDA-DE \cite{ding2022source} further selects confident pseudo-labeled target samples and re-estimates the mean and covariance over these source-like samples as an alternative.
In contrast, CPGA \cite{qiu2021source} trains a prototype generator from conditional noises to generate multiple avatar feature prototypes for each class, so that class prototypes are intra-class compact and inter-class separated, which is followed by later works \cite{zhang2022source,xu2022source}.

\method{Virtual domain alignment}
Once the source distribution is estimated, it is essential to seek virtual domain alignment between the proxy source domain and the target domain for knowledge transfer.
We review a variety of virtual domain alignment techniques as follows.
Firstly, SHOT++ \cite{liang2021source} and ProxyMix \cite{ding2022proxymix} follow a classic semi-supervised approach, MixMatch \cite{berthelot2019mixmatch}, to bridge the domain gap.
Secondly, SDDA \cite{kurmi2021domain} and PS \cite{du2021generation} adopt the widely-used domain adversarial alignment technique \cite{ganin2015unsupervised} that is formally written as:
\begin{equation}
    \min_{\theta_H}\max_{\theta_D}\ \mathbb{E}_{x_t \in \mathcal{X}_p}[\log D(H(x_t))] + \mathbb{E}_{x_t \in \mathcal{X}_t} [\log(1 - D(H(x_t)))],
\end{equation}
where $H$ and $D$ respectively represent the feature encoder and the binary domain discriminator, and $\mathcal{X}_p$ denotes the proxy source domain.
Due to its simplicity, the domain adversarial training strategy has also been utilized in the following works \cite{liu2021source,ye2021source,tian2022vdm,liu2022source_ai,chu2023adversarial}.
Besides, a certain number of methods \cite{nayak2021mining,yan2021source,stan2021unsupervised,stan2022domain} further employ advanced domain adversarial training strategies to achieve better adaptation.
Thirdly, BAIT \cite{yang2021casting} leverages the maximum classifier discrepancy \cite{saito2018maximum} between two classifiers' outputs in an adversarial manner to achieve feature alignment, which has been followed by \cite{tian2022source_tist,tian2022source_cee,chu2022denoised,yuan2022source}.
Fourthly, FTA-FDA \cite{peng2022multi} applies the maximum mean discrepancy (MMD) \cite{gretton2012kernel} to reduce the difference of features across domains, and its improved variants like conditional MMD and contrastive domain discrepancy have also been used in \cite{tian2021source,zhang2022lightweight,ding2022source,zhang2022divide,liu2022self}.
In addition, features from different domains could be further aligned through contrastive learning between source prototypes and target samples \cite{qiu2021source,zhang2022source,xu2022source,zhang2022divide}.
To further model the instance-level alignment, MTRAN \cite{huang2022relative} reduces the difference between features from the target data and its corresponding variant in the virtual source domain.

\subsubsection{Self-supervised Learning}
Self-supervised learning is another learning paradigm tailored to learn feature representation from unlabeled data based on auxiliary prediction tasks (pretext tasks) \cite{gidaris2018unsupervised,caron2018deep,caron2020unsupervised,he2020momentum,chen2020simple}.
As mentioned above, the centroid-based pseudo labels are similar to the learning manner of DeepCluster \cite{caron2018deep}.
Inspired by rotation prediction \cite{gidaris2018unsupervised}, SHOT++ \cite{liang2021source} further comes up with a relative rotation prediction task and introduces a 4-way classification head in addition to the $C$-dimensional semantic classification head during adaptation, which has been adopted by later methods \cite{yeh2022boosting,diamant2022reconciling,feng2023cross}.
Besides, OnTA \cite{wang2021ontarget} and CluP \cite{conti2022cluster} exploit the self-supervised learning frameworks \cite{he2020momentum,caron2020unsupervised} for learning discriminative features as initialization, respectively.
TTT++ \cite{liu2021ttt++} learns an extra self-supervised branch using contrastive learning \cite{chen2020simple} in the source model, which facilitates the adaptation in the target domain with the same objective.
Recently, StickerDA \cite{kundu2022concurrent} designs three self-supervision objectives (\ie, sticker location, sticker rotation, and sticker classification) and optimizes the sticker intervention-based pretext task with the auxiliary classification head in both the source training and target adaptation phases.

\subsubsection{Optimization Strategy}
Apart from the learning objectives categorized above, we further review a variety of optimization strategies of existing SFDA methods.
Generally, most of the pseudo-labeling based SFDA methods \cite{sivaprasad2021uncertainty,peng2022multi,yang2022self,zhao2022adaptive,kundu2022concurrent,li2022jacobian,qu2022bmd,kumar2023conmix,zong2022domain,ding2022proxymix,wang2022cross,tang2021model,ishii2021source,ding2022source,tian2022vdm,diamant2022reconciling,chhabra2023generative,taufique2023continual,cao2021towards,jiao2022source,xia2022privacy,wang2022exploring,feng2023cross,shen2023e2,zhang2023universal} adopt the optimization strategy developed in SHOT \cite{liang2020we} that only updates parameters of the feature encoder with the parameters of the source classifier frozen.
In such a manner, the target features are learned to approach source prototypes in the classifier under objectives like pseudo-labeling, clustering-based training, and consistency training.
Note that, SHOT++ \cite{liang2021source} and StickerDA \cite{kundu2022concurrent} also optimize the classification head associated with the self-supervised task as well as the feature encoder, whereas TTT++ \cite{liu2021ttt++} fixes the additional self-supervised head during adaptation.
To generate domain-invariant representations, DIPE \cite{wang2022exploring} divides the parameters of the feature encoder into two groups based on the consistency of the source model and the target model, followed by a group-specific optimization rule.
By contrast, SCA \cite{maracani2023key} proposes to adapt the classifier to the target domain while freezing the feature encoder.

To enable more efficient optimization, VMP \cite{jing2022variational} further provides an alternative solution by only perturbing the weights of the convolutional and FC layers in the source model.
Following a seminal work \cite{li2017revisiting}, SFDA-ST \cite{paul2021unsupervised} and UBNA \cite{klingner2022unsupervised} focus on updating parameters of the BN layers, mean and variance instead of weights of the entire network.
Typically, updating the BN statistics is also computationally efficient as it does not require any backward propagation but calculates a weighted average of previous statistics and those in the current batch, which is also utilized in \cite{yazdanpanah2022visual,chattopadhyay2022fusion}.
To bridge the domain gap, several methods (\ie, AUGCO \cite{prabhu2022augco}, CATTAn \cite{thopalli2022domain} and RPL \cite{rusak2022if}) further optimize the affine parameters in the BN layers through backward propagation.
Similarly, SMPT++ \cite{liu2022source} learns the affine parameters in the Group Normalization layers.

For a majority of SFDA methods based on source distribution estimation \cite{hou2020source,yang2022revealing,yang2022source,sahoo2020unsupervised,hu2022prosfda,gao2022visual}, the pre-trained source model during data synthesis is kept frozen to achieve implicit alignment. 
For instance, SFDA-IT \cite{hou2020source} only learns the domain translator in the target domain, and ProSFDA \cite{hu2022prosfda} and DePT \cite{gao2022visual} learn the visual prompts in the input space, respectively.
To improve the quality of data estimation, the source model could be replaced by a learnable target model \cite{li2020model,liu2021source,zhou2022domain_bibm,morerio2020generative}.

\subsubsection{Beyond Vanilla Source Model}
Although most SFDA methods adapt a vanilla source model to the target domain, there still exist a certain number of methods that required a customized source model \cite{bateson2020source,lao2021hypothesis,kundu2022balancing,kundu2022concurrent} or extra information about the source or target domain \cite{stan2021unsupervised,eastwood2022source,peng2022multi,roy2022uncertainty} for model adaptation.

\method{Customized source training}
To obtain an accurate source model, a popular framework developed in SHOT \cite{liang2020we} learns the classification network with the label smoothing trick \cite{muller2019does}.
PR-SFDA \cite{luo2021exploiting} adopts a class-balanced cross-entropy loss to alleviate the class imbalance in the source model, SFDA-NS \cite{kondo2022source} introduces an additional ring loss over feature vectors for both domains, and CP3Net \cite{feng2023cross} employs the focal loss \cite{lin2017focal} to addresses the data imbalance.
Intuitively, these methods still work under a standard cross-entropy loss though the performance would degrade.
Besides, PR-SFDA \cite{luo2021exploiting} employs data augmentation like color perturbation in the source domain.
CPSS \cite{zhao2022source} and SI-SFDA \cite{ye2022alleviating} even come up with a specific data augmentation called cross-patch style swap for both domains.
SFDA-mixup \cite{kundu2022balancing} leverages task-specific priors to identify domain-invariant augmentation for mixup operations in both domains.

As for the network design, AdaEnt \cite{bateson2020source} learns an extra prior predictor besides the primary segmentation network in the source domain.
In a similar fashion, SFDA-NS \cite{kondo2022source} introduces a shape auto-encoder to preserve the shape prior knowledge.
TAN \cite{xu2020transfer} individually learns the feature encoder and the classifier in the source training stage in which the encoder is optimized based on a reconstruction objective.
StickerDA \cite{kundu2022concurrent} and TTT++ \cite{liu2021ttt++} introduce an auxiliary self-supervised classification head in the source model, and GarDA \cite{chen2023generative} and ADV-M \cite{wang2021providing} need to learn a domain discriminator from the source domain to help the semantic model adapt to the target domain.
Besides, a few methods \cite{lao2021hypothesis,zong2022domain,chu2022denoised,li2022adaptive} employ the multi-head classifier learning strategy by learning multiple classification heads with one shared feature encoder in the source domain.
On top of the prior-enforcing auto-encoder, SoMAN-cPAE \cite{kundu2021generalize} also learns a global classification head and multiple local classification heads for different data augmentations.

\method{Extra supervision}
In addition to the source model, MAS$^3$ \cite{stan2021unsupervised} also provides the estimated GMM of source features for the target domain.
BUFR \cite{eastwood2022source} further requires the marginal feature distributions in softly-binned histograms for measurement shifts.
U-SFAN \cite{roy2022uncertainty} utilizes the distribution of the Bayesian source classifier to quantify the uncertainty for more accurate adaptation.
Moreover, Prototype-DA \cite{zhou2022domain} and OnDA \cite{panagiotakopoulos2022online} need the existence of class prototypes calculated in the source domain.
FTA-FDA \cite{peng2022multi} needs to retain some random spectrum maps of source data for the following data translation step, while CATTAn \cite{thopalli2022domain} transfers the subspace learned from source features to the target domain.
Some approaches \cite{liu2021ttt++,bozorgtabar2022anomaly} achieve feature alignment using different orders of moments for features in the source domain. 
Besides, SFDA-NS \cite{kondo2022source} leverages the learned shape auto-encoder from the source domain to match shape prior across domains.
GAP \cite{chhabra2023generative} and ADV-M \cite{wang2021providing} perform indirect cross-domain alignment with the help of a domain discriminator learned from the source domain.  
In addition, CONDA \cite{truong2022conda} requires the label distribution in the source domain, while KUDA \cite{sun2022prior} and AdaMI \cite{bateson2022source} utilize the prior knowledge about label distribution in the target domain.

\method{Remarks}
There are several remaining SFDA methods that have not been covered in the previous discussions.
PCT \cite{tanwisuth2021prototype} considers the weights in the classifier layer as source prototypes and develops an optimal transport-based feature alignment strategy between target features and source prototypes, while FTA-FDA \cite{peng2022multi} readily reduces the distance between the estimated target prototypes and source prototypes.
Besides, target prototypes could also be considered representative labeled data, and such a prototypical augmentation helps correct the classifier with pseudo-labeling \cite{xiong2022source,zhou2022domain}. 
SFISA \cite{zhang2022source} employs implicit feature-level augmentation, and LA-VAE \cite{yang2021model,yeh2021sofa} exploits variational auto-encoder to achieve latent feature alignment.
In addition, the meta-learning mechanism is adopted in a few studies \cite{wang2021self,bohdal2022feed,li2022meta} for the SFDA problem. 
A recent work \cite{naik2023machine} even generates common sense rules and adapts models to the target domain to reduce rule violations.

\subsection{Learning Scenarios of SFDA algorithms}
\label{sec:sfda-task}
\method{Closed-set \vs Open-set}
Most existing SFDA methods focus on a closed-set scenario, \ie, $\mathcal{C}_s = \mathcal{C}_t$, and some SFDA algorithms \cite{liang2020we,huang2021model} are also validated in a relaxed partial-set setting \cite{liang2020balanced}, \ie, $\mathcal{C}_t \subset \mathcal{C}_s$.
Besides, several SFDA works \cite{liang2020we,kundu2020towards,feng2021open} consider the open-set learning scenario where the target label space $\mathcal{C}_t$ subsumes the source label space $\mathcal{C}_s$.
To allow more flexibility, open-partial-set domain adaptation \cite{you2019universal} ($\mathcal{C}_s \setminus \mathcal{C}_t \ne \emptyset, \mathcal{C}_t \setminus \mathcal{C}_s \ne \emptyset$,) is studied in SFDA methods \cite{kundu2020universal,deng2021universal,yang2023one,xu2023universal,zhang2023universal}.
Several recent studies \cite{liang2021umad,shen2023e2,qu2023upcycling,wan2022scemail} even develop a unified framework for both open-set and open-partial-set scenarios.

\method{Single-source \vs Multi-source}
To fully transfer knowledge from multiple source models, prior SFDA methods \cite{liang2020we,liang2021source,kundu2022balancing} extend the single-source SFDA algorithms by combining these adapted model together in the target domain.
Besides, a couple of works \cite{ahmed2021unsupervised,dong2021confident} are elaborately designed for adaptation with multiple source models.
While each source domain typically shares the same label space with the target domain, UnMSMA-MiFL \cite{li2022union} considers a union-set multi-source scenario where the union set of the source label spaces is the same as the target label space.

\method{Single-target \vs Multi-target}
Several SFDA methods \cite{ahmed2022cleaning,zhao2022source,kumar2023conmix} also validate the effectiveness of their proposed methods for multi-target domain adaptations where multiple unlabeled target domains exist at the same time.
Different from \cite{ahmed2022cleaning,kumar2023conmix}, the domain label of each target data is even unknown \cite{zhao2022source}.
Note that, each target domain may come in a streaming manner, thus the model is successively adapted to different target domains \cite{taufique2023continual,rostami2021lifelong,panagiotakopoulos2022online}.

\method{Unsupervised \vs Semi-supervised}
Some SFDA methods \cite{wang2021learning,ma2022source,ma2022context} adapt the source model to the target domain with only a few labeled target samples and adequate unlabeled target samples.
In these semi-supervised learning scenarios, the standard classification loss over the labeled data could be readily incorporated to further enhance the adaptation performance \cite{liang2021source,wang2021learning}. 

\method{White-box \vs Black-box}
In fact, sharing a model with all the parameters is not flexible for adjustment if the model turns out to have harmful applications \footnote{\url{https://openai.com/blog/openai-api/}}. 
Instead, the source model is accessible as a black-box module through the cloud application programming interface (API).
At an early time, IterLNL \cite{zhang2021unsupervised} treats this black-box SFDA problem as learning with noisy labels, and DINE \cite{liang2022dine} develops several structural regularizations within the knowledge distillation framework, which is further extended by several methods \cite{sun2022prior,peng2022toward}.
Beyond the deep learning framework, several shallow studies \cite{chidlovskii2016domain,clinchant2016transductive} focus on the black-box SFDA problem with the target features and their predictions.

\method{Active SFDA}
To improve the limited performance gains, MHPL \cite{wang2022active} introduces a new setting, active SFDA, where a few target data can be selected to be labeled by human annotators.
This active SFDA setting is also studied by other methods \cite{su2021university,li2022source_mm,xie2022towards,kothandaraman2023salad}, and the key point lies in how to select valuable target samples for labeling.

\method{Imbalanced SFDA}
ISFDA \cite{li2021imbalanced} and two following methods \cite{sun2022source,sun2023domain} pay attention to the class-imbalanced SFDA problem setting where the source and target label distributions are different and extremely imbalanced.
The main challenge is that the source model would be severely biased toward majority classes and overlook minority classes.

\method{Miscellaneous SFDA tasks}
In addition, researchers also focus on other aspects of SFDA, \eg, the robustness against adversarial attacks \cite{agarwal2022unsupervised}, the forgetting of source knowledge \cite{yang2021generalized,liu2023twofer}, and the vulnerability to membership inference attack \cite{an2022privacy} and image-agnostic attacks (\eg, blended backdoor attack) \cite{sheng2023adaptguard}.
In addition, one recent work \cite{tanwisuth2023prototype} explores source-free unsupervised clustering under domain shift, and NOTELA \cite{boudiaf2023in} even introduces the SFDA scheme for multi-label classification tasks.
