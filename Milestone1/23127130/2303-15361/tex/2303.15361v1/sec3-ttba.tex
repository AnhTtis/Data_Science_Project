During the testing phase, it is possible that there may exist a single instance or instances from different distributions.
This situation necessitates the development of techniques that can adapt off-the-shelf models to individual instances.
To be concise, we refer to this learning scheme as \emph{test-time instance adaptation} (\aka, standard test-time training \cite{sun2020test} and one-sample generalization \cite{dinnocente2019learning}), which can be viewed as a special case of source-free domain adaptation ($n_t=1$). 

\subsection{Problem Definition}
\begin{definition}[Test-Time Instance Adaptation, TTIA]
Given a classifier $f_\mathcal{S}$ learned on the source domain $\mathcal{D}_\mathcal{S}$, and an unlabeled target instance $x_t \in \mathcal{D}_\mathcal{T}$ under distribution shift, \emph{test-time instance adaptation} aims to leverage the labeled knowledge implied in $f_\mathcal{S}$ to infer the label of $x_t$ adaptively.
\end{definition}

To the best of our knowledge, the concept \emph{test-time adaptation} is first introduced by Wegmann \etal 
\cite{wegmann1998dragon} in 1998, where the speaker-independent acoustic model is adapted to a new speaker with unlabeled data at test time.
However, this differs from the definition of \emph{test-time instance adaptation} mentioned earlier, as it involves using a few instances instead of a single instance for personalized adaptation.
This scenario is frequently encountered in real-world applications, such as in single-image models that are tested on real-time video data \cite{brahmbhatt2018geometry,azimi2022self,wang2023test}.
To avoid ambiguity, we further introduce a generalized learning scheme, \emph{test-time batch adaptation}, and give its definition as follows.

\begin{definition}[Test-Time Batch Adaptation, TTBA]
Given a classifier $f_\mathcal{S}$ learned on the source domain $\mathcal{D}_\mathcal{S}$, and a mini-batch of unlabeled target instances $\{x_t^1, x_t^2, \cdots, x_t^B\}$ from $\mathcal{D}_\mathcal{T}$ under distribution shift ($B\geq 1$), \emph{test-time batch adaptation} aims to leverage the labeled knowledge implied in $f_\mathcal{S}$ to infer the label of each instance at the same time. 
\end{definition}

It is important to acknowledge that the inference of each instance is not independent, but rather influenced by the other instances in the mini-batch.
Test-Time Batch Adaptation (TTBA) can be considered a form of SFDA \cite{liang2020we} when the batch size $B$ is sufficiently large. 
Conversely, when the batch size $B$ is equal to 1, TTBA degrades to TTIA \cite{sun2020test}.
Typically, these schemes assume no access to the source data or the ground-truth labels of data on the target distribution.
In the following, we provide a taxonomy of TTBA (including TTIA) algorithms, as well as the learning scenarios.

\setlength{\tabcolsep}{4.0pt}
\begin{table}[!t]
\caption{A taxonomy on TTBA methods with representative strategies.}
\resizebox{0.49\textwidth}{!}{
    \begin{tabular}{ll}
        \toprule
        \textbf{Families} & \textbf{Representative Strategies}\\
        \midrule
        \textbf{batch normalization calibration} & PredBN \cite{nado2020evaluating,schneider2020improving},  InstCal \cite{zou2022learning}\\
        \textbf{model optimization} & TTT \cite{sun2020test}, GeOS \cite{dinnocente2019learning}, MEMO \cite{zhang2022memo} \\
        \textbf{meta-learning} & MLSR \cite{park2020fast}, Full-OSHOT \cite{borlino2022self}\\
        \textbf{input adaptation} & TPT \cite{shu2022test}, TTA-DAE \cite{karani2021test}\\
        \textbf{dynamic inference} & EMEA \cite{wang2021efficient}, DRM \cite{zhang2023domain}\\
        \bottomrule
\end{tabular}}
\end{table}

\subsection{Taxonomy on TTBA Algorithms}
\subsubsection{Batch Normalization Calibration}
Normalization layers (\eg, batch normalization \cite{ioffe2015batch} and layer normalization \cite{ba2016layer}) are considered essential components of modern neural networks.
For example, a batch normalization (BN) layer calculates the mean and variance for each activation over the training data $\mathcal{X}_\mathcal{S}$, and normalizes each incoming sample $x_s$ as follows,
\begin{equation}
    \hat{x}_s = \gamma \cdot \frac{x_s - \mathbb{E}[X_\mathcal{S}]}{\sqrt{\mathbb{V}[X_\mathcal{S}] + \epsilon}} + \beta,
\end{equation}
where $\gamma$ and $\beta$ denote the scale and shift parameters (\aka the learnable affine transformation parameters), and $\epsilon$ is a small constant introduced for numerical stability. 
The BN statistics (\ie, the mean $\mathbb{E}[\mathcal{X}_\mathcal{S}]$ and variance $\mathbb{V}[\mathcal{X}_\mathcal{S}]$) are typically approximated using exponential moving averages over batch-level estimates $\{\mu_k,\sigma^2_k\}$, 
\begin{equation}
    \hat{\mu}_{k+1} = (1-\rho) \cdot\hat{\mu}_{k} + \rho \cdot\mu_k,\; \hat{\sigma}^2_{k+1} = (1-\rho) \cdot\hat{\sigma}^2_{k} + \rho \cdot\sigma^2_k,
\end{equation}
where $\rho$ is the momentum term, $k$ denotes the training step, and the BN statistics over the $k$-th mini-batch $\{x_i\}_{i=1}^{B_s}$ are
\begin{equation}
\mu_{k} = \frac{1}{B_s} \sum\nolimits_i x_i,\; \sigma^2_{k}=\frac{1}{B_s} \sum\nolimits_i (x_i - \mu_{k})^2,
\label{eq:bn}
\end{equation}
where $B_s$ denotes the batch size at training time.
During inference, the BN statistics estimated at training time are frozen for each test sample.

AdaBN \cite{li2017revisiting}, a classic work in the domain adaptation literature, posits that the statistics in the BN layers represent domain-specific knowledge.
To bridge the domain gap, the authors suggest replacing the training BN statistics with new statistics estimated over the entire target domain. 
This method has been shown to be effective in addressing changes in low-level features such as style or texture \cite{burns2021limitations}.
Similarly, DARE \cite{rosenfeld2022domain} utilizes a domain-specific whitening strategy to ensure zero mean and identity variance.
Inspired by these techniques, a pioneering TTBA work, PredBN \cite{nado2020evaluating}, replaces the training BN statistics with those estimated per test batch.
L2GP \cite{duboudin2022learning} further introduces a specific training-time method that facilitates the use of PredBN at test time.

PredBN+ \cite{schneider2020improving} adopts the running averaging strategy for BN statistics during training and suggests mixing the BN statistics per batch with the training statistics $\{\mu_s,\sigma^2_s\}$ as, 
\begin{equation}
    \bar{\mu}_{t} = (1-\rho_t) \cdot\mu_{s} + \rho_t \cdot\mu_t,\; \bar{\sigma}^2_{t} = (1-\rho_t) \cdot\sigma^2_{s} + \rho_t \cdot\sigma^2_t,
\end{equation}
where the test statistics $\{\mu_t,\sigma^2_t\}$ are estimated via Eq.~(\ref{eq:bn}), and the hyperpaerameter $\rho_t$ controls the trade-off between training and estimated test statistics.
This method has been extensively examined in \cite{benz2021revisiting}, \eg, the impacts of mean and variance, and the location of rectified BN statistics. 
When combined with adversarially trained models, PredBN+ has been shown to achieve certified accuracy and corruption robustness while maintaining its state-of-the-art empirical robustness against adversarial attacks \cite{nandy2021covariate}.
Moreover, TTN \cite{lim2023ttn} presents an alternative solution that calibrates the estimation of the variance as follows,
\begin{equation}
   \bar{\sigma}^2_{t} = (1-\rho_t) \cdot\sigma^2_{s} + \rho_t \cdot\sigma^2_t + \rho_t (1-\rho_t) (\mu_t - \mu_s)^2.
\end{equation}
Instead of using the same value for different BN layers, TTN optimizes the interpolating weight $\rho_t$ during the post-training phase using labeled source data.

Typically, methods that rectify BN statistics may suffer from limitations when the batch size $B$ is small, particularly when $B=1$.
CBNA \cite{klingner2022continual} and SaN \cite{bahmani2022adaptive} directly attempt to mix instance normalization (IN) \cite{ulyanov2016instance} statistics estimated per instance with the training BN statistics.
Instead of manually specifying a fixed value at test time, InstCal \cite{zou2022learning} introduces an additional module during training to learn the interpolating weight between IN and BN statistics, allowing the network to dynamically adjust the importance of training statistics for each test instance.
By contrast, AugBN \cite{khurana2021sita} expands a single instance to a batch of instances using random augmentation, then estimates the BN statistics using the weighted average over these augmented instances.

\subsubsection{Model Optimization}
Another family of test-time adaptation methods involves adjusting the parameters of a pre-trained model for each unlabeled test instance (batch). 
These methods are generally divided into two main categories: (1) training with auxiliary tasks \cite{dinnocente2019learning,sun2020test,dinnocente2020one}, which introduces an additional self-supervised learning task in the primary task during both training and test phases, and (2) fine-tuning with unsupervised objectives \cite{wang2019dynamic,zhang2022memo,reddy2022master}, which elaborately designs a task-specific objective for updating the pre-trained model.

\method{Training with auxiliary tasks}
Motivated by prior works \cite{carlucci2019domain,sun2019unsupervised} in which incorporating self-supervision with supervised learning in a unified multi-task framework enhances adaptation and generalization, TTT \cite{sun2020test} and OSHOT \cite{dinnocente2020one} are two pioneering works that leverage the same self-supervised learning (SSL) task at both training and test phases, to implicitly align features from the training domain and the test instance.
Specifically, they adopt a common multi-task architecture, comprising the primary classification head $h_c(\cdot;\theta_c)$, the SSL head $h_s(\cdot;\theta_s)$, and the shared feature encoder $f_e(\cdot;\theta_e)$.
The following joint objective of TTT or OSHOT is optimized at the training stage,
\begin{equation}
	\theta_e^{*}, \theta_c^{*}, \theta_s^{*} = \mathop{\arg\min}_{\theta_e, \theta_c, \theta_s} \sum_{i=1}^{n_s} \mathcal{L}_{pri}(x_i,y_i;\theta_c,\theta_e) + \mathcal{L}_{ssl}(x_i;\theta_s,\theta_e),
	\label{eq:ttt}
\end{equation}
where $\mathcal{L}_{pri}$ denotes the primary objective (\eg, cross-entropy for classification tasks), and $\mathcal{L}_{ssl}$ denotes the auxiliary SSL objective (\eg, rotation prediction \cite{gidaris2018unsupervised} and solving jigsaw puzzles \cite{carlucci2019domain}).
For each test instance $x_t$, TTT \cite{sun2020test} first adjusts the feature encoder $f_e(\cdot;\theta_e)$ by optimizing the SSL objective, 
\begin{equation}
	\theta_e(x_t) = \mathop{\arg\min}_{\theta_e} \mathcal{L}_{ssl}(x_t;\theta_s^{*},\theta_e),
\end{equation}
then obtains the prediction with the adjusted model as $\hat{y} = h_c(f_e(x;\theta_e(x_t));\theta_s^{*})$.
By contrast, OSHOT \cite{dinnocente2020one} modifies the parameters of both the feature encoder and the SSL head according to the SSL objective at test time.
Generally, many follow-up methods adopt the same auxiliary training strategy by developing various self-supervisions for different applications \cite{zhang2020inference,li2021test,hansen2021self,mirza2022mate,prabhudesai2022test,gandelsman2022test,osowiechi2023tttflow}.
Among them, TTT-MAE \cite{gandelsman2022test} is a recent extension of TTT that utilizes the transformer backbone and replaces the self-supervision with masked autoencoders \cite{he2022masked}.
TTTFlow \cite{osowiechi2023tttflow} employs the unsupervised normalizing flows as an alternative to self-supervision for the auxiliary task.

To increase the dependency between the primary task and the auxiliary task, GeOS \cite{dinnocente2019learning} further adds the features of the SSL head to the primary classification head.
SSDN-TTT \cite{cohen2020self} develops an input-dependent mechanism that allows a self-supervised network to predict the weights of the primary network through filter-level linear combination.
Similarly, SR-TTT \cite{lyu2022learning} does not follow the Y-shaped architecture but instead utilizes an explicit connection between the primary task and the auxiliary task.
Specifically, SR-TTT takes the output of the primary task as the input of the auxiliary task.
Besides, TTCP \cite{sarkar2022leveraging} adopts the pipeline of TTT, but it leverages a test-time prediction ensemble strategy by identifying augmented samples that the SSL head could correctly classify.
TTAPS \cite{bartler2022ttaps} modifies another popular SSL technique \cite{caron2020unsupervised} as self-supervision and merely updates the shared feature encoder for an augmented batch of a single test image.

\method{Training-agnostic fine-tuning}
To avoid altering training with auxiliary tasks in the source domain, the other methods focus on developing unsupervised objectives only for test-time model optimization. 
DIEM \cite{wang2019dynamic} proposes a selective entropy minimization objective for pixel-level semantic segmentation, while MALL \cite{reddy2022master} enforces edge consistency prior with a weighted normalized cut loss.
Besides, MEMO \cite{zhang2022memo} optimizes the entropy of the averaged prediction over multiple random augmentations of the input sample.
TTAS \cite{bateson2022test} further develops a class-weighted entropy objective, while SUTA \cite{lin2022listen} additionally incorporates minimum class confusion to reduce the uncertainty.
In contrast, TTA-FoE \cite{karani2022field} partitions the input into multiple patches and matches the feature distributions through a Field of Experts \cite{roth2005fields}, and ShiftMatch \cite{wang2023robustness} focuses on matching spatial correlations per channel for Bayesian neural networks.

Self-supervised consistency regularization under various input variations is also favorable in customizing the pre-trained model for each test input \cite{liu2022single,jin2023empowering}.
In particular, SSL-MOCAP \cite{tung2017self} leverages the cycle consistency between 3D and 2D data, while REFINE \cite{leung2022black} encourages both silhouette consistency and confidence-based mesh symmetry for the test input.
SCIO \cite{kan2022self} develops a self-constrained optimization method to learn the coherent spatial structure.
While adapting image models to a video input \cite{brahmbhatt2018geometry,casser2019depth,chen2019self,li2020online,luo2020consistent,mutlu2023tempt}, ensuring temporal consistency between adjacent frames is a crucial aspect of the unsupervised learning objective.
Many other methods directly update the model with the unlabeled objectives tailored to specific tasks, such as image matching \cite{zhu2021test,hong2021deep}, image denoising \cite{vaksman2020lidia,mohan2021adaptive}, generative modeling \cite{bau2019semantic,nitzan2022mystyle}, and style transfer \cite{kim2022controllable,ding2023modify}.
As an example, R\&R+ \cite{gilton2021model} fine-tunes the reconstruction model and an unknown original image using the cost function over a single instance.

In addition, the model could be adapted to each instance by utilizing the generated data \cite{banerjee2021self,ozer2022source} at test time.
As an illustration, TTL-EQA \cite{banerjee2021self} generates numerous synthetic question-answer pairs and subsequently leverages them to infer answers in the given context.
ZSSR \cite{shocher2018zero} trains a super-resolution network using solely down-sampled examples extracted from the test image itself.
Instead of leveraging task-related self-supervisions, TTT-UCDR \cite{paul2022ttt} even utilizes several off-the-shelf self-supervisions (\eg, rotation prediction \cite{gidaris2018unsupervised}) to adjust the feature representations at test time, showing improved cross-domain retrieval performance.

\subsubsection{Meta-Learning}
MAML \cite{finn2017model}, a notable example of meta-learning \cite{hospedales2021meta}, learns a meta-model that can be quickly adapted to perform well on a new task using a small number of samples and gradient steps.
Such a learning paradigm is typically well-suited for test-time adaptation, where we can update the meta-model using an unlabeled objective over a few test data.
There exist two distinct categories: backward propagation \cite{park2020fast,borlino2022self}, and forward propagation \cite{dubey2021adaptive,kim2022variational}. The latter category does not alter the trained model but includes the instance-specific information in the dynamical neural network.

\method{Backward propagation} Inspired by the pioneering work \cite{shocher2018zero}, MLSR \cite{park2020fast} develops a meta-learning method based on MAML for single-image super-resolution.
Concretely, the meta-objective \wrt the network parameter $\theta$ is shown as,
\begin{equation}
  \theta^* = \arg\min_{\theta} \sum\nolimits_i \mathcal{L}(\text{LR}_i,\text{HR}_i; \theta - \alpha \textcolor{black}{\nabla_\theta \mathcal{L}(\text{LR}_i\downarrow,\text{LR}_i;\theta})),
  \label{eq:meta-train}
\end{equation}
where $\mathcal{L}(A, B; \theta)=\|f_\theta(A)-B\|_2^2$ is the loss function, $\alpha$ is the learning rate of gradient descent, and $\text{LR}_i\downarrow$ denotes the down-scaled version of the low-resolution input in the paired trained data $(\text{LR}_i,\text{HR}_i)$. 
At inference time, MLSR first adapts the meta-learned network to the low-resolution test image and its down-sized image (LR$\downarrow$) using the parameter $\theta^*$ learned in Eq.~(\ref{eq:meta-train}) as initialization,
\begin{equation}
  \theta_t \gets \theta^* - \alpha \textcolor{black}{\nabla_\theta \mathcal{L}(\text{LR}\downarrow,\text{LR};\theta^*)},
  \label{eq:meta-test}
\end{equation}
then generates the high-resolution (HR) image as $f_{\theta_t}(\text{LR})$.
Such a meta-learning mechanism based on self-supervised internal learning has also been utilized by follow-up methods \cite{soh2020meta,chi2021test,liu2022towards,min2023meta}.
Among them, MetaVFI \cite{choi2021test} further introduces self-supervised cycle consistency for video frame interpolation, and OST \cite{chen2022ost} requires an extra subset of training data to form the meta-test objective in Eq.~(\ref{eq:meta-test}) for deepfake detection.

As an alternative, Full-OSHOT \cite{borlino2022self} proposes a meta-auxiliary learning approach that optimizes the shared encoder with an inner auxiliary task, providing a better initialization for the subsequent primary task as follows:
\begin{equation}
   \min_{\theta_e, \theta_c} \sum\nolimits_i \mathcal{L}_{pri}(x_i,y_i;\theta_e - \alpha \textcolor{black}{\nabla_{\theta_e} \mathcal{L}_{ssl}(x_i;\theta_e,\theta_s)}, \theta_c),
\end{equation}
and the definitions of variables are the same as OSHOT \cite{dinnocente2020one} in Eq.~(\ref{eq:ttt}).
After meta-training, the parameters $(\theta_e, \theta_s)$ are updated for each test sample according to the auxiliary self-supervised objective.
This learning paradigm is also known as meta-tailoring \cite{alet2021tailoring}, where $\mathcal{L}_{ssl}$ in the inner loop affects the optimization of $\mathcal{L}_{pri}$ in the outer loop.
Follow-up methods exploit various self-supervisions in the inner loop, including contrastive learning \cite{alet2021tailoring,bartler2022mt3}, rotation recognition \cite{huang2023testtime}, and reconstruction \cite{sain2022sketch3t,liu2023meta}.

\method{Forward propagation} Apart from the shared encoder $f_e(\theta_e)$ above, several other meta-learning methods exploit the normalization statistics \cite{zhang2021adaptive,jiang2022domain} or domain prototypes \cite{dubey2021adaptive,kim2022variational} or source dictionary \cite{li2022plug} from the inner loop, allowing backward-free adaptation at inference time. 
Besides, some works incorporate extra meta-adjusters \cite{sun2022dynamic,hu2022domain} or learnable prompts \cite{zheng2022prompt,ben2022pada}, by taking the instance embedding as input, to dynamically generate a small subset of parameters in the network, which are optimized at the training phase.
DSON \cite{seo2020learning} proposes to fuse IN with BN statistics by linearly interpolating the means and variances, incorporating the instance-specific information in the trained model.
Following another popular meta-learning framework with episodic training \cite{li2019episodic}, SSGen \cite{xiao2022learning} suggests episodically dividing the training data into meta-train and meta-test to learn the meta-model, which is generalized to the entire training data for final test-time inference. 
The same idea is employed by \cite{zhong2022meta,xu2022mimic,segu2023batch} where multiple source domains are involved during training.

\subsubsection{Input Adaptation}
In contrast to model-level optimization, which updates pre-trained models for input data, another line of test-time adaptation methods focuses on changing input data or features for pre-trained models \cite{he2020self,karani2021test,zhao2022test,cordier2022test,gao2023back}.
This approach bears resemblance to prompt tuning \cite{lester2021power}, which involves modifying the input prompt rather than the model itself during fine-tuning.
For example, TPT \cite{shu2022test} employs entropy minimization as \cite{zhang2022adaptive} to optimize instance-specific prompts for test-time adaptation.
CVP \cite{tsai2023self} optimizes the convolutional visual prompts in the input under the guidance of self-supervised contrastive learning objective.

TTA-AE \cite{he2020self,he2021autoencoder} additionally learns a set of auto-encoders in each layer of the trained model at training time.
It is posited that unseen inputs have larger reconstruction errors than seen inputs, thus a set of domain adaptors is introduced at test time to minimize the reconstruction loss.
Similarly, TTA-DAE \cite{karani2021test} only learns an image-to-image translator (\aka, input adaptor) for each input so that the frozen training-time denoising auto-encoder could well reconstruct the network output.
TTO-AE \cite{li2022self} follows the Y-shaped architecture of TTT and optimizes both the shared encoder and the additional input adaptor to minimize reconstruction errors in both heads.
Instead of auxiliary auto-encoders, AdvTTT \cite{valvano2022reusing} leverages a discriminator that is adversarially trained to distinguish real from predicted network outputs, so that the prediction output for each adapted test input satisfies the adversarial output prior. 
PINER \cite{song2023piner} even aims for constructing a good input for the black-box model by analyzing the output change rate.

Besides, OST \cite{termohlen2021continual} proposes mapping the target input onto the source data manifold using Fourier style transfer \cite{yang2020fda}, serving as a pre-processor to the primary network.
SBTS \cite{park2022style} shifts the style statistics of the test sample to the nearest source domain using AdaIN \cite{huang2017arbitrary}.
By contrast, TAF-Cal \cite{zhao2022test} further utilizes the average amplitude feature over the training data to perform Fourier style calibration \cite{yang2020fda} at both training and test phases, bridging the gap between training and test data.
It is noteworthy that imposing a data manifold constraint \cite{pandey2021generalization,sarkar2022leveraging,gao2023back,xiao2023energy} can aid in achieving better alignment between the test data and unseen training data. 
Specifically, ITTP \cite{pandey2021generalization} trains a generative model over source features with target features projected onto points in the source feature manifold for final inference.
DDA \cite{gao2023back} exploits the generative diffusion model for target data, while ESA \cite{xiao2023energy} updates the target feature by energy minimization through Langevin dynamics.

In addition to achieving improved recognition results against domain shifts, a certain number of test-time adaptation methods also explore input adaptation for the purpose of test-time adversarial defense \cite{shi2021online,yoon2021adversarial,mao2021adversarial,wu2021attacking,alfarra2022combating,wu2023uncovering}.
Among them, Anti-Adv \cite{alfarra2022combating} perturbs the test input to maximize the classifier's prediction confidence, and Hedge \cite{wu2021attacking} alters the test input by maximizing the KL-divergence between the classifier's predictions and the uniform distribution.
Besides, SOAP \cite{shi2021online} leverages self-supervisions like rotation prediction at both training and test phases and purifies adversarial test examples based on self-supervision only.
SSRA \cite{mao2021adversarial} only exploits the self-supervised consistency under different augmentations at test time to remove adversarial noises in the attacked data.

\subsubsection{Dynamic Inference}
Upon multiple pre-trained models learned from the source data, a few works \cite{wang2021efficient,foll2022gated,zhang2023domain} learn the weights for each model, without making any changes to the models themselves. 
For example, EMEA \cite{wang2021efficient} employs entropy minimization to update the ensemble coefficients before each model.
GDU \cite{foll2022gated} directly obtains the weight by calculating the similarity between the test embedding and the domain embeddings associated with each model.
Besides, GPR \cite{jain2011online} is one of the early works for test-time adaptation that only adjusts the network predictions instead of the pre-trained model.
In particular, it bootstraps the more difficult faces in an image from the more easily detected faces and adopts Gaussian process regression to encourage smooth predictions for similar patches.

\subsection{Learning Scenarios of TTBA Algorithms}
\method{Instance \vs Batch}
As defined above, test-time adaptation could be divided into two cases: instance adaptation \cite{sun2020test,zhang2022memo} and batch adaptation \cite{schneider2020improving,brahmbhatt2018geometry}, according to whether a single instance or a batch of instances exist at test time. 

\method{Single \vs Multiple}
In contrast to vanilla test-time adaptation that utilizes the pre-trained model from one single source domain, some works (\eg,  \cite{dinnocente2019learning,pandey2021generalization,wang2021efficient,xiao2022learning,zhao2022test,park2022style,xiao2023energy,zhang2023domain}) are interested in domain generalization problems where multiple source domains exist.

\method{White-box \vs Black-box}
A majority of test-time adaptation methods focus on adapting white-box pre-trained models to test instances, while some other works (\eg, \cite{jain2011online,chen2019self,zhang2023domain}) do not have access to the parameters of the pre-trained model (black-box) and instead adjust the predictions according to generic structural constraints.

\method{Customized \vs On-the-fly}
Most existing TTA methods require training one or more customized models in the source domain,\eg, TTT \cite{sun2020test} employs a Y-shaped architecture with an auxiliary head.
However, it may be not allowed to train the source model in a customized manner for some real-world applications.
Other works \cite{zhang2022memo,alfarra2022combating} do not rely on customized training in the source domain but develop flexible techniques for adaptation with on-the-fly models. 
