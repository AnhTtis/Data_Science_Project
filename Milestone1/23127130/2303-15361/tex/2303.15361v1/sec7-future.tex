\subsection{Emerging Trends}
\method{Diverse downstream fields}
Although most existing efforts in the TTA field have been devoted to visual tasks such as image classification and semantic segmentation, a growing number of TTA methods are now focusing on other understanding problems over video data \cite{xu2022learning}, multi-modal data \cite{shin2022mmtta}, and 3D point clouds \cite{saltori2022gipso}, as well as regression problems like pose estimation \cite{ding2023maps,lee2023ttacope}.

\method{Black-box pre-trained models}
In contrast to existing SFDA methods that use white-box models from the source domain, some recent works \cite{liang2022dine,yang2023divide} have focused on adaptation with black-box models, which provide only probability scores or even discrete labels for the target data.
Moreover, methods tailored for black-box scenarios can enable knowledge transfer from large models to edge devices.

\method{Open-world adaptation}
Existing TTA methods always follow the closed-set assumption; however, a growing number of SFDA methods \cite{liang2021umad,yang2023one,qu2023upcycling} are beginning to explore model adaptation under an open-set setting.
A recent OTTA method \cite{yang2023auto} further focus on the performance of out-of-distribution detection task at test time.
Besides, for large distribution shifts, it is challenging to perform effective knowledge transfer by relying solely on unlabeled target data, thus several recent works \cite{li2022source_mm,kothandaraman2023salad} also introduce active learning to involve human in the loop.

\method{Memory-efficient continual adaptation}
In real-world applications, test samples may come from a continually changing environment \cite{wang2022continual,niu2022efficient}, leading to catastrophic forgetting.
To reduce memory consumption while maintaining accuracy, recent works \cite{song2023ecotta,hong2023mecta} propose different memory-friendly OTTA solutions for resource-limited end devices.

\method{On-the-fly adaptation}
The majority of existing TTA methods require a customized pre-trained model from the source domain, bringing the inconvenience for instant adaptation.
Thus, fully test-time adaptation \cite{wang2021tent}, which allows adaptation with an on-the-fly model, has attracted increasing attention.

\subsection{Open Problems}
\method{Theoretical analysis}
While most existing TTA works focus on developing effective methods to obtain better empirical performance, the theoretical analysis remains an open problem. 
We believe that rigorous analyses can provide in-depth insights and inspire the development of new TTA methods.

\method{Benchmark and validation}
As there does not exist a labeled validation set, validation also remains a significant and unsolved issue for TTA methods. 
Existing methods often determine hyper-parameters through grid search in the test data, which is infeasible in real-world applications.
Alternatively, a new benchmark can be built where a labeled validation set and an unlabeled test set exist at test time, providing a more realistic evaluation scenario for TTA methods.

\method{New applications}
Tabular data \cite{borisov2022deep} in vectors of heterogeneous features is essential for numerous industrial applications, and time series data \cite{ragab2023adatime} is predominant in many real-world applications including healthcare and manufacturing.
To our knowledge, few prior work has studied TTA in the context of tabular data or time series data, despite their importance and prevalence in real-world scenarios.

\method{Trustworthiness}
Current TTA methods tend to focus more on recognition performance under different distribution shifts and robustness against different attacks, while ignoring other goals of trustworthy machine learning \cite{eshete2021making}, \eg, fairness, security, privacy, and explainability.

\method{Big models}
Recently, big models (\eg, ChatGPT and GPT-4) have attracted widespread attention due to their surprisingly strong ability in a variety of machine learning tasks. 
However, it remains an open problem on how to leverage them for better generalization ability in downstream tasks.