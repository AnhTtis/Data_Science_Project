\begin{figure*}
\begin{minipage}{\linewidth}
  \centering
  \begin{minipage}[t]{0.30\linewidth}
      \begin{figure}[H]
        \begin{center}
        \includegraphics[width=0.95\linewidth]{figures/convergence.pdf}
        \end{center}
           \caption{Test error over time while training either regular or Voronoi variants of NeRF\cite{vanilla} or mip-NeRF\cite{mip} on the ship dataset.}
        \label{fig:single_vs_vornoi}
    \end{figure}
  \end{minipage}
     \hfill
  \begin{minipage}[t]{0.30\linewidth}
      \begin{figure}[H]
        \begin{center}
        \includegraphics[width=1.01\linewidth]{figures/init_vs_no_init.pdf}
        \end{center}
           \caption{Test error for cells trained without initialisation (blue), and cells subdivided with copy initialisation (red, subdivision at red line).}
        \label{fig:rnd_vs_mother}
    \end{figure}
  \end{minipage}
     \hfill
  \begin{minipage}[t]{0.30\linewidth}
      \begin{figure}[H]
        \begin{center}
        \includegraphics[width=0.95\linewidth]{figures/init_vs_no_init.png}
        \end{center}
           \caption{Artifacts occurring from novel views when not using proper initialisation.}
        \label{fig:rnd_vs_mother_img}
    \end{figure}
  \end{minipage}
\end{minipage}
\end{figure*}
\begin{figure*}
\begin{minipage}{\linewidth}
  \centering
  \begin{minipage}[t]{0.3\linewidth}
      \begin{figure}[H]
        \begin{center}
        \includegraphics[width=0.95\linewidth]{figures/optimised_vs_not.pdf}
        \end{center}
           \caption{Comparing uniform cell distribution, random samples as unoptimised cell centres, and optimised cell centres.}
        \label{fig:split_kinds}
    \end{figure}
  \end{minipage}
     \hfill
  \begin{minipage}[t]{0.3\linewidth}
      \begin{figure}[H]
        \begin{center}
        \includegraphics[width=1.01\linewidth]{figures/earlylate.pdf}
        \end{center}
           \caption{Early versus late partitioning of the scene. Early can cause tiny artifacts, late can slow learning.}
        \label{fig:earlylate}
    \end{figure}
  \end{minipage}
     \hfill
  \begin{minipage}[t]{0.3\linewidth}
      \begin{figure}[H]
        \begin{center}
        \includegraphics[width=1.01\linewidth]{figures/cellno.pdf}
        \end{center}
           \caption{Different numbers of cells for subdivision, all with roughly the same number of total parameters.}
        \label{fig:cellno}
    \end{figure}
  \end{minipage}
\end{minipage}
\end{figure*}
\begin{table}
    \begin{center}
    \begin{tabular}{|l||c|c|c|}
    \hline
    NeRF & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIP$\downarrow$ \\
    \hline
    %Uniform & 25.4502 & 0.9121 & 0.1435\\
    %+Voronoi & \textbf{25.6033} & \textbf{0.9170} & \textbf{0.1343}\\
    Uniform & 25.450 & 0.912 & 0.144\\
    +Voronoi & \textbf{25.603} & \textbf{0.917} & \textbf{0.134}\\
    \hline
    \hline
    mip-NeRF & PSNR$\uparrow$ & SSIM$\uparrow$ & LPIP$\downarrow$ \\
    \hline
    Uniform & 27.401 & 0.945 & 0.111\\
    +Voronoi & \textbf{27.979} & \textbf{0.949} & \textbf{0.105}\\
    \hline
    \end{tabular}
    \end{center}
    \caption{Performance of NeRF\cite{vanilla} and mip-NeRF\cite{mip} for training with stratified sampling instead of hierarchical sampling on the test set after 24 hours of pure training time, compared to their respective Voronoi variants. Bold indicates better.}\label{table:sampling_agnostic}
    \vspace{-0.2cm}
\end{table}
\begin{table}
    \begin{center}
    \begin{tabular}{|l||c|c||c|c|}
    \hline
    PSNR$\uparrow$ & NeRF & +Voronoi & mip-NeRF & +Voronoi \\
    \hline
    %Lego & 27.3544905 & \textbf{27.6523073} & 30.5377628 & \textbf{31.2339924} \\
    %Ship & 26.7169872 & \textbf{27.3730824} & 28.2255695 & \textbf{29.0069451} \\
    %Drums & 23.0771655 & \textbf{23.4900181} & 24.4735294 & \textbf{24.7601137} \\
    Lego & 27.355 & \textbf{27.652} & 30.538 & \textbf{31.234} \\
    Ship & 26.717 & \textbf{27.373} & 28.226 & \textbf{29.007} \\
    Drums & 23.077 & \textbf{23.490} & 24.474 & \textbf{24.760} \\
    \hline
    SSIM$\uparrow$ & NeRF & +Voronoi & mip-NeRF & +Voronoi \\
    \hline
    %Lego & 0.9451271 & \textbf{0.9482914} & 0.9711934 & \textbf{0.975235} \\
    %Ship & 0.8967283 & \textbf{0.9067823} & 0.9164283 & \textbf{0.9250461} \\
    %Drums & 0.9145142 & \textbf{0.9229578} & 0.9354133 & \textbf{0.9409269} \\
    Lego & 0.9451 & \textbf{0.9483} & 0.9712 & \textbf{0.9752} \\
    Ship & 0.8967 & \textbf{0.9068} & 0.9164 & \textbf{0.9250} \\
    Drums & 0.9145 & \textbf{0.9230} & 0.9354 & \textbf{0.9409} \\
    \hline
    LPIP$\downarrow$ & NeRF & +Voronoi & mip-NeRF & +Voronoi \\
    \hline
    %Lego & 0.0910465 & \textbf{0.0831431} & 0.0506565 & \textbf{0.0416824} \\
    %Ship & 0.2016092 & \textbf{0.1820656} & 0.1658293 & \textbf{0.1438596} \\
    %Drums & 0.131984 & \textbf{0.1128118} & 0.1054726 & \textbf{0.0868588} \\
    Lego & 0.0910 & \textbf{0.0831} & 0.0507 & \textbf{0.0417} \\
    Ship & 0.2016 & \textbf{0.1820} & 0.1658 & \textbf{0.1439} \\
    Drums & 0.1320 & \textbf{0.1128} & 0.1054 & \textbf{0.0869} \\
    \hline
    \end{tabular}
    \end{center}
    \caption{Performance of NeRF\cite{vanilla} and mip-NeRF\cite{mip} on the test set of different datasets using hierarchical sampling after 24 hours of pure training time, compared to their respective Voronoi variants. Bold indicates better. See \cref{fig:single_vs_vornoi} for convergence behavior.}\label{table:architecture_agnostic}
    \vspace{-0.5cm}
\end{table}
\section{Evaluation}
Our proposed method is built to be an independent extension for existing methods that speeds up train and inference time without sacrificing quality or causing artifacts.\\
We first evaluate possible hyper parameter choices, then provide an ablation to highlight the impact of all our components (\cref{sec:eval_ablation}). We then provide experiments and argumentation to why our approach is agnostic to \eg the underlying foundation for sampling strategies, underlying NeRF architecture, and ray formulation (\cref{sec:eval_agnostic}). Then, we discuss our approach in comparison to other work in terms of evaluation speed, quality, and benefit for large-scale scenes (\cref{sec:comparison}).\\
Throughout this evaluation, we focus our evaluation on train speed, accuracy on test images, and inference speed. For fairness, we always measure performance over training time, as measuring with epochs alone would not take the computational overhead of our method, \ie assigning points into their cells, into consideration. We would also gain an unfair advantage from our much shorter inference times for the test set. If not specified otherwise, due to limited resources, our experiments are run on 400-by-400 pixel versions of sets, if not further specified the Lego model, from the NeRF datasets\cite{vanilla} with a single GeForce RTX 2080 Ti. We use 256 channels (one single network) versus 64 channels (Voronoi variants) resulting in 12 cells with slightly fewer total parameters for the different NeRF architectures, and train for 24 hours. Note that we tested our approach with simple PyTorch code, with no refined performance boosts or possibly even specific CUDA optimisations, for better comparability. We limit ourselves to a single level of subdivision for simplicity, while we did observe best performance when using multiple levels (see \cref{fig:cellno}). We always measure the PSNR of the mean squared error on the whole testset for any plots and otherwise give the average PSNR per image over the testset. In general, we show that our approach improves convergence speed and quality, as can be seen in \cref{fig:single_vs_vornoi}.\\
\textbf{Limitations}~~~ While our approach is only an extension, hence inherits weaknesses from the underlying NeRFs, our approach has an additional weakness shown by short training time for the global scene: Our approach struggles with scenes with initially unclear geometry, \eg many translucent objects, as the initially learned scene will struggle to provide a meaningful geometric prior and may lead to a bad subdivision that can not be changed anymore over the training.
\subsection{Hyper Parameter Choices and Ablation}\label{sec:eval_ablation}\label{sec:eval_hyper}
\textbf{Subdivision strategies}~~~Optimising Voronoi cells is key to the success of our approach: We want to distribute information evenly between the cells to make best use of each network for fast convergence and high quality. Large differences between the total weight of our cells would indicate a bad distribution, and similar total weights would indicate a good distribution. Our proposed algorithm reduces the median/average standard deviation over 100 examples on different datasets from $~0.2458 / ~0.2479$ to $~0.0018 / 0.2479$. In consequence, networks using Voronoi diagrams with optimised cell positions show a clear advantage, as shown in \cref{fig:split_kinds}.\\
\textbf{Number of cells}~~~For the decision on the number of cells, we evaluate 4, 8, 16, and 32 cells, while also considering 4 cells subdivided in 4 further cells (16 in total), each with roughly the same number of total parameters. As can be seen in \cref{fig:earlylate}, subdividing early is no issue, but can cause tiny artifacts not impacting the scores, while subdividing too late wastes computation time. Similarly, \cref{fig:cellno} shows that too few or too many cells slow convergence.\\
\textbf{Initialisation}~~~Training a distributed network without the right prior can lead to failure of generalisation. The result of this are local optima that prevent generalisation and that can cause artifacts from novel views (see \cref{fig:rnd_vs_mother} and \cref{fig:rnd_vs_mother_img}). These artifacts are results of improving train quality by creating dense regions where there should not be any. Inheriting network parameters from a cell with larger global context prevents this effectively, while not requiring any form of distillation as in \eg KiloNeRF\cite{kilonerf}. We also observe no issues with the learned geometric representation that this prior affects, even at times better learning the scene geometry, as can be seen from the visualisation of the total density for each ray, see inset (left: mip-NeRF, right: Voronoi mip-NeRF). \begin{wrapfigure}{l}{0.25\textwidth}
    \vspace{-0.4cm}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/geometry.png}
    \label{fig:ray_density}
    \vspace{-1.cm}
\end{wrapfigure}
\subsection{Independence of Underlying Architecture}\label{sec:eval_agnostic}
Our approach introduces a geometrically inspired dynamically learned decomposition of the NeRF problem into many localised, smaller problems. Since the underlying mathematical formulation is not altered, this speedup from decomposition can be easily combined with many other improvements introduced subsequently to the initial NeRF.
We demonstrate the effectiveness of our approach on different NeRF variants by investigating different sampling strategies, different approaches to the ray formulation, and by exploring further approaches that have recently gathered attention.\\
First, we show that our Voronoi NeRF approach is improving results regardless of the sampling strategies used. For this, we compare results of stratified and hierarchical sampling with either mip-NeRF \cite{mip} and \cite{vanilla}, showing clear improvement for both stratified (see \cref{table:sampling_agnostic}) and hierarchical samplings strategies (see \cref{table:architecture_agnostic}). In more detail, we then demonstrate that Voronoi NeRF as an extension for different architectures, namely NeRF\cite{vanilla} or mip-NeRF\cite{mip}, outperforms their counterparts on various datasets, see \cref{table:architecture_agnostic}. We also show that our approach can work with the small network architecture proposed by KiloNeRF\cite{kilonerf} in \cref{sec:comparison}.\\
In summary, we provide improvements in particular early in training, converging much faster while having fewer fluctuations during training, as can be seen in \cref{fig:single_vs_vornoi}.
We further discuss how our approach is agnostic to other architectures and approaches: mip-NeRF 360 \cite{mip360} and DONeRF \cite{donerfdepthoracle} use a non-linear contraction function to restrict the coordinate range of points far from the origin. Furthermore, additional simplified NeRF-like networks are often used for more feasible samplings \cite{vanilla}\cite{mip360}. These extensions are in no way in conflict with our approach, in fact one might even accelerate the sample predictor networks by localisation as well. Inference accelerations such as precomputed opacity grids, empty space skipping and early ray termination \cite{kilonerf} are also complementary to our approach, as they do not change the actual NeRF mechanism. Interpolating outputs of multiple cells can be used to smooth transitions of neighboring cells \cite{blocknerfwaymo} or improve cell placement \cite{derfdecomposed}. While we explored this and saw improvements in early stages about occasionally visible, very tiny seams, the prior given through initialisation with a global NeRF is enough to avoid any visible seams after the first few epochs after a subdivision.
\subsection{Comparison to Others}\label{sec:comparison}
As discussed in \cref{sec:eval_agnostic}, our approach is compatible with many improvements suggested for NeRFs when it comes to training speed and inference quality. However, the faster training times obtained through an approach exploiting geometric knowledge can also be used for both faster inference and learning larger scenes faster and in better quality. KiloNeRF\cite{kilonerf} provides faster inference by first training a large NeRF that is then distilled into many smaller NeRFs arranged in a grid fashion. They obtain further speed by optimising the sampling process, \eg skipping largely empty sections, terminating rays early, and optimising CUDA code. As all of these options are available to our approach as well, we only discuss the qualitative results. Analogue to their distillation, our parameter inheritance works in a top-down fashion while training the network, avoiding the extra step of distilling. For comparison and as another example for the flexibility towards the underlying architecture, we trained a Voronoi NeRF with 16 cells, subdivided each cell into another 16 cells, obtaining 256 cells in total. We compare to their approach with 512 uniformly distributed cells, trained for the same duration. We use the same architecture as they do, and use our inheritance initialisation instead of their distillation. Effectively, this can be considered an on-the-fly distillation process from coarse to fine. With the same inference speed and no need for the extra distillation step, and geometry-sensitive cell distribution, our approach can outperform KiloNeRF in terms of quality at half the number of cells, see \cref{table:vs_kilonerf}. We attribute this to our representation making better use of its parameters by making sure every cell is filled with about the same amount of information, where KiloNeRF can place cells entirely inside an object or in thin air.\\
As can be seen in \cref{fig:single_vs_vornoi}, our approach trains faster later on through multiple smaller networks, while particularly the early convergence benefits from having only one small network that learns the scene. Our top-down parameter inheritance can thus boost performance in particular early for scenes that are far from convergence. These two qualities make our approach particularly valuable for large datasets like the ones proposed by MegaNeRF\cite{meganerf} or Block-NeRF\cite{blocknerfwaymo}, as the larger or more detailed the scene grows, only our number of networks increases while individual ray sample evaluations, except for the relatively cheap assignment of each sample to a cell, do not become any more expensive. We further argue that, as we have shown, dynamically partitioned space leads to better performance. In particular, our results indicate that adaptively subdividing a scene more than twice will bring even more relative improvement than for the tested small-scale scenes.
\input{4_figures_eval.tex}
In summary, our approach does significantly speed up training, allows for fast inference, and is invariant to the underlying architecture and other degrees of freedom within the NeRF formulation. Opposing to other distributed approaches, it does not require any expensive precomputation\cite{kilonerf}, requires no interpolation between cells\cite{meganerf,blocknerfwaymo,derf}, and does not require previous knowledge of the scene, \eg as human-given specification for the layout of the learned partition\cite{meganerf,blocknerfwaymo,kilonerf}.