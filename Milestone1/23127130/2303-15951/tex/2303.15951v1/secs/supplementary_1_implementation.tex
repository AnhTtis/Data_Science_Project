\section{Additional Implementation Details}
\input{figures/supp_impl_rectification.tex}

\subsection{Camera processing}
{\bf Camera rectification.} After the procedure of space-subdivision (Sec~3.3 of the main paper), for each sub-region $S$, we obtain a set of visible cameras whose view frustums intersect with $S$. 
We note that visible cameras are not suitable to be directly used in computing of the perspective warping, because some cameras do not fully cover the region and look at the region as shown in Fig.~\ref{fig:camera_rectification}(a).
Thus, we propose an empirical but effective camera rectification strategy, that we rotate the camera view directions to make them look at the center of the region $S$. This simple strategy helps ensure most points inside $S$ can be warped to meaningful coordinates. Moreover, we find that aligning their distances to the center of $S$ with the same distance $r$ can help improve the rendering quality, as shown in Fig.~\ref{fig:dis_alignment}. Here $r$ is empirically set as the mean distance to the region center among the 1/4 nearest visible cameras.

{\bf Camera selection.}
When the number of visible cameras is larger than $n_c = 4$, we select a subset of the visible cameras for better computational efficiency. We select the cameras based on the farthest point sampling: First, we randomly select a camera as the seed, and then we repeatedly add the farthest visible camera for $n_c - 1$ times.

\input{figures/supp_impl_dis_align.tex}

\subsection{Construction of $M$}
In this section, we give a detailed description of how we construct the matrix $M$ for our perspective warping.

{\bf Principal component analysis.}
Given the region $S$ with the selected cameras, we first uniformly sample $n = 32^3$ points $\{ {\bf x}_i\}$ inside $S$. Then, we project the points to the selected cameras, concatenate the projected coordinates, and obtain the high-dimensional coordinates $\left\{\left[C_1({\bf x}_i), ..., C_{n_c}({\bf x}_i) \right] = \left[u_1^i, v_1^i, ..., u_{n_c}^i, v_{n_c}^i \right]\right\}.$ These coordinates are formed in a coordinate matrix $K \in \mathbb{R}^{2n_c \times n}$, then we compute the covariance matrix $Q = (K - \overline{K})(K - \overline{K})^{\top}$, where $\overline{K}$ is the mean coordinate of all projected coordinates. By eigendecomposition, we obtain the matrix $M' \in \mathbb{R}^{3\times 2n_c}$ formed by the eigenvectors with the first three largest eigenvalues. The matrix $M'$ defines the directions of the projection axes.

\input{figures/supp_impl_scale.tex}

{\bf Computing the axis length.}
After the matrix $M' \in \mathbb{R}^{3\times 2n_c}$ is found, we now need to perform a post normalization by scaling each axis. Specifically, we would like to find three proper scale parameters $\{s_1, s_2, s_3\}$, and $M = SM'$, where $S \in \mathbb{R}^{3\time 3}$ is the diagonal matrix formed by $\{s_1, s_2, s_3\}$. The key idea of these scaling parameters is that we expect the unit length in the warp space can be approximately aligned with the unit length in the image space. More specifically, for each axis in the warp space, when a point moves along the axis by a unit length, we expect the maximum spatial transition of all image coordinates to be approximately a pixel length. 

We take a point ${\bf x}$ inside the region $S$ for example. 
Let us denote the Jacobian matrix from the original space to the image space as $J_{O\rightarrow I} \in \mathbb{R}^{3\times 2n_c} $ derived from the image projection function, and the Jacobian matrix from the image space to the warp space as $J_{I\rightarrow W}=M=SM'$. 
Our target is to compute the Jacobian matrix $J_{W\rightarrow I} \in \mathbb{R}^{2n_c \times 3}$ and put a constraint that the maximum value of each column vector of $J_{W\rightarrow I}$ equals one.
% The spatial transition reflected on the image space from the warp space can be expressed by the matrix $J_{W\rightarrow I} \in \mathbb{R}^{2n_c \times 3}$. 
Note $J_{W\rightarrow I}$ may not be directly computed by inverting $J_{I\rightarrow W}$, which is not a square matrix. Alternatively, we present it by $J_{W\rightarrow I} = J_{O\rightarrow I}J_{W\rightarrow O}$, which can be further represented by 
\begin{equation}
\begin{split}
J_{W\rightarrow I} &= J_{O\rightarrow I}J_{O\rightarrow W}^{-1} \\
&=J_{O\rightarrow I}(J_{I \rightarrow W}J_{O\rightarrow I})^{-1} \\
&=J_{O\rightarrow I}(SM'J_{O\rightarrow I})^{-1} \\
&=J_{O\rightarrow I}(M'J_{O\rightarrow I})^{-1}S^{-1}.
\end{split}
\end{equation}

% \input{figures/supp_impl_lin_matrix.tex}

What we expect is that the maximum value of each column vector of $J_{W\rightarrow I}$ equals one. This constraint can solve the values of $\{s_1, s_2, s_3\}$ for the example point ${\bf x}$. For all the sampled points $\{{\bf x}_i\}$, we take the average values of $\{s_1, s_2, s_3\}$ for our final scale parameters.



\subsection{Perspective sampling}
As stated in the main paper (Sec 3.5), when sampling points in ray marching, we perform uniform sampling on the warp space, and we get a non-uniform sampling in the original space. To be specific, for the current sample point ${\bf x}_i = {\bf o} + t_i{\bf d}$, we expect to find the next sample point ${\bf x}_{i+1} = {\bf x}_i + \delta_i{\bf d}$, such that $\|F({\bf x}_{i+1}) - F({\bf x}_{i})\|_2 = l$. Here $l$ is the parameter controlling sample density and we empirically set $l=\sqrt{3}$, i.e., the diagonal length of the unit cube in the warp space~\cite{mueller2022instant}. To compute the marching step $\delta_i$ in the original space efficiently, we perform a linear approximation that
\begin{equation}
    F({\bf x}_{i+1}) \approx F({\bf x}_{i}) + \delta_i \cdot J_i{\bf d},
\end{equation}
Where $J_i$ is the Jacobian matrix at ${\bf x}_i$ from the original space to the warp space. Hence, the distance between $F({\bf x}_{i+1})$ and $F({\bf x}_i)$ is approximated by $\delta_i \|J_i{\bf d}\|_2$. We let it equals $l$ and get $\delta_i = \frac{l}{\|J_i{\bf d}\|_2}$. 

\subsection{Loss functions}
As described in the main paper, the loss of training is defined as
\begin{equation}
    \mathcal{L} = \mathcal{L}_{recon(c(r), c_{{\rm gt}})} + \lambda_{\rm Disp}\mathcal{L}_{\rm Disp} + \lambda_{\rm TV}\mathcal{L}_{\rm TV},
\end{equation} 
where the first term $\mathcal{L}_{recon(c(r), c_{\rm gt})}=\sqrt{(c(r) - c_{{\rm gt}})^2 + \epsilon}$ is a color reconstruction loss~\cite{BarronMVSH22} with $\epsilon = 10^{-4}$, and the last two terms are the regularization losses.

The disparity loss $\mathcal{L}_{\rm Disp}$ of the sampled rays is defined by
\begin{equation}
\mathcal{L}_{\rm Disp} = \frac{1}{n_r}\sum_{k}{\rm disp}_k^2,
\end{equation}
where the disparity of a ray is computed by the weighted sum of the sampled inverse distance that ${\rm disp} = \sum_i w_i\frac{1}{t_i}$, and $\{w_i\}$ are the weights computed by volume rendering.

The aim of total variation loss $\mathcal{L}_{\rm TV}$ is to encourage the border points of two neighboring octree nodes to have similar densities and colors. To achieve this goal, in each training iteration, we randomly sample $n_b=8192$ points on the borders of the octree nodes, then the loss is defined by
\begin{equation}
    \mathcal{L}_{\rm TV} = \frac{1}{n_b} \sum_k \|{\rm feat}_0^k - {\rm feat}_1^k\|_2^2.
\end{equation}
Here, for each sample point $k$, ${\rm feat}_0^k$ and ${\rm feat}_1^k$ are the feature vectors fetched from the hash table using two different functions conditioned on its two neighboring octree nodes.

In LLFF dataset we set $\lambda_{\rm Disp}=2.5\times 10^{-4}, \lambda_{\rm TV}=10^{-1}$, and in Free dataset and NeRF-360-V2 dataset we set $\lambda_{\rm Disp}=10^{-3}, \lambda_{\rm TV}=10^{-1}$.



\subsection{More implementation details}
{\bf Architecture details.} We follow a similar setting to Instant-NGP~\cite{mueller2022instant} and use the hash table with $16$ levels, and each level contain $2^{19}$ feature vectors with dimension of $2$. The fetched hash feature vectors of size 32 are fed to a tiny MLP with one hidden layer of width 64, to get the scene features and the volume densities, then, the scene features are concatenated with the spherical harmonics encoding of view directions and are fed to another rendering MLP with two hidden layers of width 64 to get the RGB colors.

{\bf Training details.} We follow Instant-NGP~\cite{mueller2022instant} and set the fixed batch size of point samples as 256k while the batch size of rays is dynamic, depending on the average sampled points on rays. We train the parameters with Adam optimizer~\cite{kingma2014adam}, whose learning rate linearly grows from zero to $1\times 10^{-1}$ in the first 1k steps and the decay to $10^{-2}$ at the end of training with cosine scheduling. For all the scenes in the experiments, we train \ourmethod for 20k steps. We implement \ourmethod using LibTorch~\cite{paszke2019pytorch}. The training time depends on the scene's complexity, and for most cases, it is between 10 minutes and 15 minutes on a single Nvidia 2080Ti GPU.
