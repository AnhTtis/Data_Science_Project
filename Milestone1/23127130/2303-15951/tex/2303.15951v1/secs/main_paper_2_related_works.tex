\section{Related Works}
\textbf{Novel view synthesis.}
%
Novel view synthesis (NVS) aims to synthesize novel view images from input posed images. The NVS problem has been extensively studied with lumigraph~\cite{BuehlerBMGC01,GortlerGSC96} and light field functions~\cite{DavisLD12,LevinD10} to directly interpolate input images. To improve the quality of the synthesized image, many methods resort to an explicit 3D reconstruction of the scene via meshes~\cite{DebevecTM96,ThiesZN19,WaechterMG14,WoodAACDSS00}, voxels~\cite{HeCJS20,LombardiSSSLS19,LombardiSSZSS21,SitzmannTHNWZ19}, point clouds~\cite{aliev2020neural,xu2022point}, depth maps~\cite{DhamoTLNT19,ShadeGHS98,ShihSKH20,TulsianiTS18}, and multi-plane images (MPI)~\cite{FlynnBDDFOST19,LiXDS20,MildenhallSCKRN19,SrinivasanTBRNS19,TuckerS20,ZhouTFFS18}, and then synthesize novel images with the help of these 3D reconstructions. \ourmethod also aims to solve the NVS task but with a neural representation.

\textbf{Neural scene representations.}
Since the emergence of NeRF~\cite{MildenhallSTBRN20, BarronMTHMS21, tancik2023nerfstudio}, there have been intensive studies on neural representations for the tasks of novel view synthesis~\cite{MildenhallSTBRN20,SitzmannZW19,tewari2022advances}, relighting~\cite{BossBJBLL21,munkberg2022extracting,zhang2021physg,ZhangSDDFB21}, generalization to new scenes~\cite{YuYTK21,WangWGSZBMSF21,LiuPLWWTZW21,ChenXZZXYS21,wang2022generalizable,suhail2022generalizable}, shape representation~\cite{martel2021acorn, takikawa2021neural, mueller2022instant}, and multi-view reconstruction~\cite{yariv2020multiview, wang2021neus, OechslePG21, yariv2021volume}. The representation can be either totally neural networks~\cite{MildenhallSTBRN20, sitzmann2020implicit, fathony2020multiplicative, lindell2022bacon, ramasinghe2022beyond}, or hybrid parametric encodings with space subdivisions~\cite{takikawa2021neural,martel2021acorn,mueller2022instant,LiuGLCT20} for efficient training and inference. \ourmethod also subdivides the scene for flexible space warping and uses the hybrid neural scene representation~\cite{mueller2022instant} for fast training and high-quality rendering.

\textbf{Fast NeRF training with space warping.}
%
Recent works show that the training of NeRF can be accelerated significantly with grid-based representations~\cite{YuFTCR22,ChenXGYS22,SunSC22,mueller2022instant}. Instead of using a huge MLP network to predict the density and color, Plenoxels~\cite{YuFTCR22} directly store the density values and colors on a voxel grid. Instant-NGP~\cite{mueller2022instant}, TensoRF~\cite{ChenXGYS22} and DVGO~\cite{SunSC22} construct a feature grid and the density and compute the density and the color for a specific point from an interpolated feature vector using a tiny MLP network. However, these grids are regularly constructed in an axis-aligned manner and require additional space warping to represent unbounded scenes. There are two kinds of existing space warping functions, the NDC warping~\cite{MildenhallSTBRN20} for the forward-facing scenes and the inverse sphere warping~\cite{BarronMVSH22,BarronMTHMS21,ZhangRSK20} for the 360$^\circ$ object-centric scenes. Both of these warping functions cannot handle long and narrow trajectories. In \ourmethod, we propose a novel perspective warping to enable these fast grid-based methods to process arbitrary camera trajectories.

\textbf{Large-scale Neural Radiance Fields.}
Recent works~\cite{turki2022mega,xiangli2022bungeenerf,tancik2022block} managed to reconstruct the radiance field on a large-scale scene by decomposing the scene into blocks and separately training different NeRFs for different blocks. \ourmethod aims at small-scale scenes with arbitrary camera trajectories, which has the potential to serve as the backbone NeRF of a single block in large-scale NeRFs.
