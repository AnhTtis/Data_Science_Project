
\input{figures/method_2d_warp.tex}

\section{Our Approach}
Given a set of images $\{\mathcal{I}_i \}$ with arbitrary but known poses in an unbounded scene, the goal of \ourmethod is to reconstruct a radiance field of the scene for the novel view synthesis task. In the following, we first give an overview of \ourmethod.

\subsection{Overview}
\label{sec:pre}
In order to build a grid-based neural representation in an unbounded scene, a space warping function $F(\mathbf{x})$ is introduced to warp the unbounded space to a bounded region. In Sec.~\ref{sec:per}, we first analyze the mechanism of space warping and propose a novel perspective warping method. The proposed perspective warping subdivides whole the space under consideration into small regions, as shown in Sec.~\ref{sec:sub}. Then, on the warp space, we build a grid-based neural representation in Sec.~\ref{sec:rep}. 
Based on the built representation, we adopt the volume rendering~\cite{MildenhallSTBRN20} to render novel-view images. 

\textbf{Volume rendering with perspective warping}.
In order to render the color $\hat c$ for a pixel, we first apply a novel point sampling strategy, called the \textit{perspective sampling} in Sec.~\ref{sec:method_sampling}, to sample points $\mathbf{x}_i$ on the camera ray emitting from the pixel.
Then, these sampled points are warped by the perspective warping 
to the warp space, and the density $\sigma_i$ and the color $c_i$ on sampled points are computed from the neural representation built on the warp space.
Finally, we composite the colors to compute the pixel color $\hat c$ by
\begin{equation}
\hat c = \sum_i^{}{T_i\alpha_i c_i},
\end{equation}
where $T_i = \prod_{j=0}^{i-1} (1 - \alpha_j)$ is the accumulated transmittance and $\alpha_i = 1 - \exp(-\delta_i\sigma_i)$ is the opacity of the point.

\subsection{Perspective warping}
\label{sec:per}
It has been demonstrated that space warping functions, such as the NDC warping~\cite{MildenhallSTBRN20} and the inverse sphere warping~\cite{ZhangRSK20,BarronMVSH22}, are effective for rendering unbounded scenes.
In this section, we start with a 2D intuitive analysis of why a space warping method is effective.

{\bf 2D analysis.} First, let us consider a simple case in the 2D space as shown in Fig.~\ref{fig:method_2d_warp}~(a). In this setting, two 2D cameras project the points from 2D space onto their 1D image planes. Consider the two orange points in the figure. The gray rhombuses are the irregular grids formed by camera rays and the gray regions are the smallest distinguishable region due to the limited resolution of the two cameras. 
However, a vanilla grid-based representation consists of axis-aligned regular grids as shown in Fig.~\ref{fig:method_2d_warp}~(b), which is not aligned with the gray rhombuses.
Moreover, such misalignment becomes more severe as the distance from the cameras increases.
The key requirement for space warping is that we need to warp the original Euclidean space and build axis-aligned grids in the warp space so that these grids are aligned with the camera rays. 
Clearly, in this 2D case, a proper warping function $F(\mathbf{x}): \mathbb{R}^2 \rightarrow \mathbb{R}^2$ can be constructed by $F(\mathbf{x}) = (C_1(\mathbf{x}), C_2(\mathbf{x}))$, where $C_1(\mathbf{x})$ and $C_2(\mathbf{x})$ denotes the 1D image coordinates of projecting $\mathbf{x}$ onto the camera 1 and camera 2 respectively. Then, the axis-aligned grids built on the $F(\mathbf{x})$ space will exactly align with camera rays.

{\bf Proper space warping.} Based on the 2D analysis above, we define a proper space warping function as follows.
\begin{definition}
Given a region $S$ in the 3D Euclidean space and a set of cameras $\{C_i|i=1,2,...,n_c\}$ which are visible to $S$, a warping function $F:\mathbb{R}^{3}\to \mathbb{R}^{3}$ is called a {\em proper warping function}, if for any two points $\mathbf{x}_1, \mathbf{x}_2\in S$, the distance between these two points in the warp space equals to the sum of distances between these two points on all visible cameras, i.e.  $\|F(\mathbf{x}_1)-F(\mathbf{x}_2)\|_2^2=\sum_i^n \|C_i(\mathbf{x}_1)-C_i(\mathbf{x}_2)\|_2^2$.
\label{def:warp}
\end{definition}
Clearly, the warping function $F(\mathbf{x})=(C_1(\mathbf{x}), C_2(\mathbf{x}))$ in the 2D toy example is a proper 2D warping function.
Note that whether a warping function is proper or not is a local property, which only relates to the visible cameras.

\input{figures/method_warp.tex}
{\bf 3D perspective warping.} Given the region $S$ and the cameras $C_i$, we now would like to construct a proper 3D warping function $F(\mathbf{x}): \mathbb{R}^3 \rightarrow \mathbb{R}^3$.
We begin with an observation that the function $\mathbf{y}=G(\mathbf{x})=[C_1(\mathbf{x}),...,C_{n_c}(\mathbf{x})]: \mathbb{R}^3 \to \mathbb{R}^{2n_c}$ which maps the 3D point to its projected coordinates on all $n$ cameras $C_i$ is a proper warping function because this is a trivial construction based on the definition of a proper warping function. However, what we want to construct eventually is a function that maps a 3D space to a 3D space. Hence, we consider constructing an approximately proper warping function $F$ with the following formulation.
\begin{problem}
Let $\{\mathbf{x}_j|j=1,2,...,n_p\}$ denote $n_p$ evenly-sampled points in the local region $S$ of the original Euclidean space, we want to find a projection matrix $M\in \mathbb{R}^{3\times 2n_c}$ that maps the coordinate $\mathbf{y}_j=G(\mathbf{x}_j)\in \mathbb{R}^{2n_c}$ to $\mathbf{z}_j\in \mathbb{R}^{3}$ by $\mathbf{z}_j=M\mathbf{y}_j$, so that $M$ minimizes $\sum_j^K \|M^\intercal\mathbf{z}_j-\mathbf{y}_j\|_2^2$
\label{prob:pca}
\end{problem}
In comparison with Definition~\ref{def:warp}, Problem~\ref{prob:pca} makes two relaxations. First, we only consider the distances between sampled points $x_i\in S$ in Problem~\ref{prob:pca} while Definition~\ref{def:warp} considers arbitrary point pairs. Second, we apply a projection matrix such that the distances between $\mathbf{y}_i$ are preserved as much as possible.
%
It can be shown that the solution to this problem is exactly the Principle Component Analysis (PCA)~\cite{wold1987principal} on the set of projection points $\{\mathbf{y}_j\}$. The matrix $M$ is constructed from the first three eigenvectors of the covariance matrix of $\{\mathbf{y}_j\}$. Therefore, the proposed perspective warping function is $F(\mathbf{x}) = M G(\mathbf{x})$, as shown Fig.~\ref{fig:method_warp}. In our implementation, we perform a post normalization on $F(\mathbf{x})$ to make the resulting points $\{\mathbf{z}_j\}$ in the warp space located around the origin, which is introduced in details in the supplementary material.

\input{figures/method_warp_vis.tex}

\textbf{Intuition of $F(\mathbf{x})$}. 
$F(\mathbf{x})$ maps the region $S$ in the original space to a region around the origin of the warp space. 
Fig.~\ref{fig:method_warp_vis} shows the perspective warping with different angles between two neighboring cameras. As we can see, when the angle $\theta$ is small, the space is squashed more on the far region, which is a similar behavior to the NDC warping. In this case, the perspective warping reduces to the NDC warping if all the cameras are forward-facing. When the angle becomes larger, the warp space is more similar to the original Euclidean space. 


\input{figures/inv_sphere}

\input{figures/method_overview.tex}
{\bf Relationship with inverse sphere warping}. 
We empirically show that inverse sphere warping~\cite{ZhangRSK20,BarronMVSH22} is also a handcrafted approximation of our perspective warping function.
As shown in Fig.~\ref{fig:inv_sphere}~(a), the warp space of the inverse sphere warping for the inner unit sphere is simply the original Euclidean space because all the cameras around are visible to this unit sphere, which corresponds to the $180^\circ$ case in Fig.~\ref{fig:method_warp_vis} of the perspective warping. The outer space (Fig.~\ref{fig:inv_sphere}~(b)) is only visible from a few far cameras and thus is warped similarly as the NDC warping, which corresponds to the $30^\circ$ case in Fig.~\ref{fig:method_warp_vis} of the perspective warping.

\subsection{Space subdivision}
\label{sec:sub}
In order to apply the perspective warping function $F(\mathbf{x})$, we need to specify $n_c$ cameras onto which we project the point $\mathbf{x}$. According to Definition~\ref{def:warp}, the properness of the warping function is a local property which only relates the visible cameras for the point $\mathbf{x}$. However, for a free trajectory like Fig.~\ref{fig:teaser}~(c), visible cameras for different regions are different. This motivates us to adaptively subdivide the space into different regions such that the visible cameras used in $F(\mathbf{x})$ are the same inside a region but are different across regions. In this case, in each subdivided region $S_i$, a warping function $F_i(\mathbf{x})$ is applied to map $S_i$ to the warp space. 

\textbf{Subdivision strategy}. We adopt an octree data structure to store the subdivided regions, which enables us to quickly search for a region and retrieve visible cameras. 
To construct the octree, we begin with an extremely large bounding box as the root node.
Here the box size is $512$ times the bounding box size of all camera centers, which is able to contain extremely far-away sky or other objects.
Then, starting from the octree root node, we perform a check-and-subdivide procedure. 
Specifically, on a tree node with $s$ as the side length, we retrieve all visible cameras whose view frustums intersect with this node. Then, if there is any visible camera center whose distance $d$ to the node center is $d\le \lambda s$, where $\lambda$ is preset to 3, the node is subdivided into 8 child nodes with side length of $s/2$. Otherwise, the current node is small enough and we stop subdividing it and mark it as a leaf node. For each subdivided child node, we further check the distance and repeat this procedure until we get all $n_l$ leaf nodes $\{S_i|i=1,2,...,n_l\}$ as shown in Fig.~\ref{fig:method_overview}~(a). Each leaf node is treated as the region $S$ in Problem~\ref{prob:pca}, and for those regions that are visible by more than $n_c=4$ cameras, we further select $n_c$ visible cameras by making the minimal pair-wise distance of the selected cameras as large as possible. A more detailed description of the camera selection strategy can be found in the supplementary material. By applying the warping function $F_i$, each leaf node is mapped to a region around the origin of its warp space.

\subsection{Scene representation}
\label{sec:rep}
In this section, we will introduce how to build our grid-based scene representation on the warp space, which allows color and density computation for a given point in the warp space.
Since the warping functions are different for different leaf nodes, we actually have $n_l$ different warp spaces. A naive solution would be to build $n_l$ different grid representations on each warp space. However, this would cause the number of parameters to grow with the number of leaf nodes. 
To limit parameter number, we suppose that all warping functions map different leaf nodes to the same warp space and build a hash-grid representation~\cite{mueller2022instant} on the warp space with multiple hash functions as shown in Fig.~\ref{fig:method_overview}. 


\textbf{Hash grid with multiple hash functions}. Sharing the same warp space for different leaf nodes will inevitably lead to conflicts, which means two different points in two leaf nodes with different densities and colors are mapped to the same point in the warp space.
In the original Instant-NGP~\cite{mueller2022instant}, there is only one hash function to compute hash values for grid vertices.
Here, we use different hash functions for different leaf nodes to alleviate the conflict problem.
Specifically, for a point $\mathbf{x}$ in the $i$-th leaf node, we map it to $\mathbf{z}=F_i(\mathbf{x})$ in the warp space and find $z$'s eight neighboring grid vertices $\hat{\mathbf{v}}$ with integer coordinates.
Then, we compute a hash value for each vertex $\hat{\mathbf{v}}$ by a hash function conditioned on the leaf node index $i$ as follows
\begin{equation}
{\rm Hash}_i(\hat{\mathbf{v}}) = \left(\bigoplus_{k=1}^{3}\hat{\mathbf{v}}_k\pi_{i,k} + \Delta_{i,k} \right)\mod L,
\label{eq:subhash}
\end{equation}
where $\bigoplus$ denotes the bitwise xor operation, and both $\left\{\pi_{i,k}\right\}$ and $\left\{\Delta_{i,k}\right\}$ are random large prime numbers, which are fixed for a specific leaf node, $k=1,2,3$ means the index of $x,y,z$ coordinate of the warp space, $L$ is the length of the hash table. The computed hash value will be used in indexing the hash table to retrieve a feature vector for the vertex $\hat{\mathbf{v}}$ and then the feature vector of the point $z$ is trilinearly-interpolated from 8 vertex feature vectors. Finally, the feature vector of $z$ and the view direction $\mathbf{d}$ are fed into a tiny MLP network to produce color and density for the point $z$, as shown in Fig.~\ref{fig:method_overview}~(d).

\textbf{Intuition of Eq.~\ref{eq:subhash}}. 
Using different numbers $\pi_{i,k}$ and different offsets $\Delta_{i,k}$ leads to different hash functions in Eq.~\ref{eq:subhash}. 
We use an example in Fig.~\ref{fig:method_overview} to show how Eq.~\ref{eq:subhash} works: The green point and the yellow point in two different leaf nodes (Fig.~\ref{fig:method_overview}~(a)) are mapped by two different warping functions to the same warp space (Fig.~\ref{fig:method_overview}~(b)). Suppose that both points reside in the same voxel of the warp space.  Although the coordinates of their neighboring vertices are the same, the two points will use different hash functions Eq.~\ref{eq:subhash} to compute different hash values for these neighboring vertices (Fig.~\ref{fig:method_overview}~(c)). 
Then, the computed hash values will be used in indexing the same hash table to retrieve feature vectors (Fig.~\ref{fig:method_overview}~(d)). In this case, two points from different leaf nodes share the same neighboring vertices in the warp space but retrieve different vertex feature vectors, which greatly reduces the probability of conflicts.
Though some conflicts still remain, they can naturally be resolved by the tiny MLP during the optimization process as observed by Instant-NGP~\cite{mueller2022instant}.


\subsection{Perspective sampling} 
\label{sec:method_sampling}
A proper warping function $F$ also gives us a guideline for sampling points on rays in volume rendering. 
According to Definition~\ref{def:warp}, the distance between two points in the proper warp space equals the sum of distances between two projected points on image planes.
In this case, by uniformly sampling the points in the warp space, we get a non-uniform sampling in the original Euclidean space but an approximately uniform sampling on images, which improves the sampling efficiency and brings more stable convergence. 
Specifically, considering a sample point $\mathbf{x}_i = \mathbf{o} + t_i\mathbf{d}$ where $\mathbf{o},\mathbf{d}$ are the camera origin and direction respectively, we first compute the Jacobian matrix $J_i \in \mathbb{R}^{3\times 3}$ of the perspective warping function $F$ at $\mathbf{x}_i$. 
Then, the next sample point $\mathbf{x}_{i+1} = \mathbf{x}_{i} + \frac{l}{\|J_i\mathbf{d}\|_2} \mathbf{d}$, where $l$ is a preset parameter controlling the sampling interval and we make a linear approximation here as discussed in the supplementary material.



\subsection{Rendering with perspective warping}
Based on the descriptions above, we now summarize the rendering procedure in two stages: the preparation stage and the actual rendering stage. (1) In the preparation stage, we subdivide the original space according to the view frustums of the cameras (Sec.~\ref{sec:sub}), and construct the local warping functions based on the selected cameras for each sub-region (Sec.~\ref{sec:per}). (2) In the actual rendering stage, we follow the framework of volume rendering to render a pixel color, by sampling points on the camera ray (Sec.~\ref{sec:method_sampling}) and conducting weighted accumulation of the sampled colors (Sec.~\ref{sec:pre}). The sampled densities and colors are fetched from the multi-resolution hash grid (Sec.~\ref{sec:rep}).

\subsection{Training}
The training loss is defined as
\begin{equation}
    \mathcal{L} = \mathcal{L}_{recon(c(r), c_{{\rm gt}})} + \lambda_{\rm Disp}\mathcal{L}_{\rm Disp} + \lambda_{\rm TV}\mathcal{L}_{\rm TV},
\end{equation} 
where $\mathcal{L}_{recon(c(r), c_{\rm gt})}=\sqrt{(c(r) - c_{{\rm gt}})^2 + \epsilon}$ is a color reconstruction loss~\cite{BarronMVSH22} with $\epsilon = 10^{-4}$.
% To reduce the artifacts of \ourmethod, we additionally introduce two regularization losses.
$\mathcal{L}_{\rm Disp}$ and $\mathcal{L}_{\rm TV}$ are two regularization losses. The first is a disparity loss $\mathcal{L}_{\rm Disp}$ that encourages the disparity (inverse depth) not excessively large, which is useful to reduce the floating artifacts. 
The second is a total variance loss~\cite{rudin1994total} $L_{\rm TV}$ that encourages the points at the borders of two neighboring octree nodes $i,j$ to have similar densities and colors. For all losses, we provide more details in the supplementary material. 
