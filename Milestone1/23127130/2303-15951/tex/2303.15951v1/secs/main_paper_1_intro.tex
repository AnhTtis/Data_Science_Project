\section{Introduction}
\label{sec:intro}

\input{figures/teaser.tex}


The research progress of novel view synthesis has advanced drastically in recent years since the emergence of the Neural Radiance Field (NeRF)~\cite{MildenhallSTBRN20,tewari2022advances}. Once the training is done, NeRF is able to render high-quality images from novel camera poses. The key idea of NeRF is to represent the scene as a density field and a radiance field encoded by Multi-layer Perceptron (MLP) networks, and optimize the MLP networks with the differentiable volume rendering technique. Though NeRF is able to achieve photo-realistic rendering results, training a NeRF takes hours or days due to the slow optimization of deep neural networks, which limits its application scopes. 

Recent works demonstrate that grid-based methods, such as Plenoxels~\cite{YuFTCR22}, DVGO~\cite{SunSC22}, TensoRF~\cite{ChenXGYS22}, and Instant-NGP~\cite{mueller2022instant}, enable fast training a NeRF within a few minutes. However, the memory consumption of such grid-based representations grows in cubic order with the size of the scene. Though various techniques, such as voxel pruning~\cite{YuFTCR22,SunSC22}, tensor decomposition~\cite{ChenXGYS22} or hash indexing~\cite{mueller2022instant}, are proposed to reduce the memory consumption, these methods still can only process bounded scenes when grids are built in the original Euclidean space.

To represent unbounded scenes, a commonly-adopted strategy is to use a space-warping method that maps an unbounded space to a bounded space~\cite{MildenhallSTBRN20, BarronMVSH22, ZhangRSK20}.
There are typically two kinds of warping functions.
(1) For forward-facing scenes (Fig.~\ref{fig:teaser}~(a)), the Normalized Device Coordinate (NDC) warping is used to map an infinitely-far view frustum to a bounded box by squashing the space along the z-axis~\cite{MildenhallSTBRN20}; (2) For 360$^\circ$ object-centric unbounded scenes (Fig.~\ref{fig:teaser}~(b)), the inverse-sphere warping can be used to map an infinitely large space to a bounded sphere by the sphere inversion transformation~\cite{BarronMVSH22, ZhangRSK20}. Nevertheless, these two warping methods assume special camera trajectory patterns and cannot handle arbitrary ones. In particular, when a trajectory is long and contains multiple objects of interest, called \textit{free trajectories}, as shown in Fig.~\ref{fig:teaser}~(c), the quality of rendered images degrades severely.

The performance degradation on free trajectories is caused by the imbalanced allocation of spatial representation capacity. Specifically, when the trajectory is narrow and long, many regions in the scenes are empty and invisible to any input views. However, the grids of existing methods are regularly tiled in the whole scene, no matter whether the space is empty or not. Thus, much representation capacity is wasted on empty space. 
%
Although such wasting can be alleviated by using the progressive empty-voxel-pruning~\cite{YuFTCR22, SunSC22}, tensor decomposition~\cite{ChenXGYS22} or hash indexing~\cite{mueller2022instant}, it still causes blurred images due to limited GPU memory. Furthermore, in the visible spaces, multiple foreground objects in Fig.~\ref{fig:teaser}~(c) are observed with dense and near input views while background spaces are only covered by sparse and far input views. In this case, for the optimal use of the spatial representation of the grid, dense grids should be allocated for the foreground objects to preserve shape details and coarse grids should be put in background space.
However, current grid-based methods allocate grids evenly in the space, causing the inefficient use of the representation capacity.

To address the above problems, we propose \ourmethod (Fast-Free-NeRF), the first fast NeRF training method that accommodates free camera trajectories for large, unbounded scenes. Built upon the framework of Instant-NGP~\cite{mueller2022instant}, \ourmethod can efficiently be trained on unbounded scenes with diverse camera trajectories
and maintains the fast convergence speed of the hash-grid representation.

In \ourmethod, we give the criterion on a proper warping function under an arbitrary camera configuration. Based on this criterion, we develop a general space-warping scheme called the \textit{perspective warping} that is applicable to arbitrary camera trajectories. The key idea of perspective warping is to first represent the location of a 3D point ${\bf p}$ by the concatenation of the 2D coordinates of the projections of ${\bf p}$ in the input images and then map these 2D coordinates into a compact 3D subspace space using Principle Component Analysis (PCA)~\cite{wold1987principal}. We empirically show that the proposed perspective warping is a generalization of the existing NDC warping~\cite{MildenhallSTBRN20} and the inverse sphere warping~\cite{ZhangRSK20,BarronMVSH22} to arbitrary trajectories in a sense that the perspective warping is able to handle arbitrary trajectories while could automatically degenerate to these two warping functions in forward-facing scenes or 360$^\circ$ object-centric scenes. In order to implement the perspective warping in a grid-based NeRF framework, we further propose a space subdivision algorithm to adaptively use coarse grids for background regions and fine grids for foreground regions.

We conduct extensive experiments on the unbounded forward-facing dataset, the unbounded 360$^\circ$ object-centric dataset, and a new unbounded free trajectory dataset. The experiments show that \ourmethod uses the same perspective warping to render high-quality images on the three datasets with different trajectory patterns. On the new Free dataset with free camera trajectories, our method outperforms baseline grid-based NeRF methods,while only using $\sim$12 minutes on training on a 2080Ti GPU.

