{
    "arxiv_id": "2303.15951",
    "paper_title": "F$^{2}$-NeRF: Fast Neural Radiance Field Training with Free Camera Trajectories",
    "authors": [
        "Peng Wang",
        "Yuan Liu",
        "Zhaoxi Chen",
        "Lingjie Liu",
        "Ziwei Liu",
        "Taku Komura",
        "Christian Theobalt",
        "Wenping Wang"
    ],
    "submission_date": "2023-03-28",
    "revised_dates": [
        "2023-03-29"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "cs.GR"
    ],
    "abstract": "This paper presents a novel grid-based NeRF called F2-NeRF (Fast-Free-NeRF) for novel view synthesis, which enables arbitrary input camera trajectories and only costs a few minutes for training. Existing fast grid-based NeRF training frameworks, like Instant-NGP, Plenoxels, DVGO, or TensoRF, are mainly designed for bounded scenes and rely on space warping to handle unbounded scenes. Existing two widely-used space-warping methods are only designed for the forward-facing trajectory or the 360-degree object-centric trajectory but cannot process arbitrary trajectories. In this paper, we delve deep into the mechanism of space warping to handle unbounded scenes. Based on our analysis, we further propose a novel space-warping method called perspective warping, which allows us to handle arbitrary trajectories in the grid-based NeRF framework. Extensive experiments demonstrate that F2-NeRF is able to use the same perspective warping to render high-quality images on two standard datasets and a new free trajectory dataset collected by us. Project page: https://totoro97.github.io/projects/f2-nerf.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.15951v1"
    ],
    "publication_venue": "CVPR 2023. Project page: https://totoro97.github.io/projects/f2-nerf"
}