{
\newcommand{\cb}{\cellcolor{bluegrad}}
\newcommand{\rb}{\cellcolor{redgrad}}
\newcommand{\gr}[1]{\textcolor{gray!90}{#1}}
\newcommand{\st}{$^*$}
% \fontsize{9pt}{9pt}\selectfont % Custom font size
\renewcommand{\arraystretch}{0.6} % Reduces the padding within cells & making rows more compact
\begin{table}
\centering
\begin{tabular}{@{}ll|c|r|r|r|r|r@{}}
\toprule
%\multirow{2}{*}{Model} & \multirow{2}{*}{PEFT Method} & \multirow{2}{*}{Score} & \coloredcell{>{\columncolor[rgb]{0.9, 0.9, 1}}c}{\cbTrainable params} & \coloredcell{>{\columncolor[rgb]{0.95, 0.9, 0.9}}c}{\rb\%} & \coloredcell{>{\columncolor[rgb]{1, 0.9, 0.9}}c}{\rbRAM} & \coloredcell{>{\columncolor[rgb]{1, 0.95, 0.95}}c}{\rbTrain.} & \coloredcell{>{\columncolor[rgb]{1, 1, 1}}c}{\rbInfer.} \\

\multirow{2}{*}{Model} & \multirow{2}{*}{PEFT Method} & \multirow{2}{*}{Score} & \multicolumn{2}{c|}{Trainable params} & RAM & \multicolumn{2}{c}{Throughput} \\
& & & \multicolumn{1}{c|}{Millions} & \multicolumn{1}{c|}{\%} & \multicolumn{1}{c|}{\footnotesize{(GB)}} & \multicolumn{1}{c|}{Train.} & \multicolumn{1}{c}{Infer.}\\

\midrule
T5$_{Large}$ & Full tuning    & $67.22\hspace{1.8em}$ & 737.67 & 
\rb100 &
\rb20.7 &
100   &
\cb100 \\
T5$_{Large}$ & Houlsby    & $\textbf{67.34}_{\pm9.58}$& 12.69  & 1.720  &\cb12.1 & 101.7 & 70.1 \\
T5$_{Large}$ & Pfeiffer           & $62.93_{\pm3.52}$ & 6.34   & 0.860  & 14.0 & 108.6 & 76.8 \\
T5$_{Large}$ & Parallel Adapter   & $66.78_{\pm3.85}$ & 50.41  & 6.833  & 14.7 & 123.1 & 73.2 \\
T5$_{Large}$ & IA3                & $55.06_{\pm1.80}$ & 0.34   & 0.047  & 16.2 & \cb124.7 & \gr{76.2} \\
T5$_{Large}$ & Prefix tuning      & $45.05_{\pm3.89}$ & 77.31  & 10.481 & 15.4 & 113.2 & 65.0 \\
T5$_{Large}$ & Prompt tuning      & $8.97_{\pm30.91}$ & 4.42   & 0.600 &\cb10.6& 108.8 & 64.5 \\
T5$_{Large}$ & LN tuning          & $64.68_{\pm4.59}$ & 0.12   & \cb0.017&14.6 & \cb143.2 &\cb103.1 \\
T5$_{Large}$&LoRa (q and v)&$\textbf{67.42}_{\pm2.32}$& 2.36   & 0.320  & 14.3 & \cb123.7 & \cb\gr{87.4} \\
T5$_{Large}$&LoRA (all)    &$\textbf{68.76}_{\pm1.83}$& 8.65   & 1.173  & 15.9 & \rb80.9  & \gr{66.9} \\
T5$_{Large}$ & KronA              & $65.68_{\pm3.27}$ & 0.22   &\cb0.030& 14.6 & 109.3 & \gr{82.3} \\
T5$_{Large}$ & Compacter          & $64.48_{\pm1.81}$ & 0.30   & 0.041  & 14.4 & \rb81.3  & \rb59.0 \\
T5$_{Large}$ & Compacter++        & $64.78_{\pm2.23}$ & 0.15   & \cb0.021 & \cb13.9 & 95.4 & 68.1 \\
T5$_{Large}$ & MAM                & $46.90_{\pm6.47}$ & 171.07 & \rb23.191& \rb16.6 & 106.1  & \rb59.1 \\
T5$_{Large}$ & Unipelt            & $44.10_{\pm15.48}$& 86.22  & \rb11.689& \rb16.7 & \rb61.1  & \rb45.2 \\
\addlinespace
%                              & score                & params&%params  & RAM     & throughput & test
T5$_{3B}$    & Full tuning     & $74.83\hspace{1.8em}$&2851.60& \rb100  &\rb32.9& 100   &\cb100.0 \\
T5$_{3B}$    & Houlsby            & $74.66_{\pm1.68}$ & 12.69 & 0.445   &\cb9.3 & 93.1  & 58.2 \\
T5$_{3B}$    & Pfeiffer           & $69.92_{\pm5.61}$ & 6.34  & 0.223   &\cb9.6 & 102.9 & 64.5 \\
T5$_{3B}$    & Parallel Adapter   & $74.15_{\pm0.88}$ & 50.41 & 1.768   & 10.0  & 102.7  & 61.2 \\
T5$_{3B}$    & IA3                & $41.77_{\pm0.50}$ & 1.38  & 0.048   & 10.8  &\cb103.9  &\gr{65.6} \\
T5$_{3B}$    & Prefix tuning      & $48.90_{\pm5.37}$ & 77.25 & 2.709   & 12.3  &\cb105.2  & 56.8 \\
T5$_{3B}$    & Prompt tuning      & $8.38_{\pm0.50}$  & 17.69 & 0.621   &\cb8.9 & 100.0 & 58.8 \\
T5$_{3B}$    & LN tuning          & $72.95_{\pm1.38}$ & 0.12  &\cb0.004 & 9.7   &\cb140.7&\cb101.8 \\
T5$_{3B}$   &LoRa (q and v)&$\textbf{75.49}_{\pm1.71}$& 3.93  & 0.138   &  9.6  & 103.0  &\cb\gr{79.8} \\
T5$_{3B}$   &LoRA (all)    &$\textbf{75.22}_{\pm1.28}$& 25.17 & 0.883   & 10.6  &\rb70.2   & \gr{56.7} \\
T5$_{3B}$    & KronA              & $71.98_{\pm0.57}$ & 0.41  & 0.014   & 10.8  & 81.2   & \gr{74.0} \\
T5$_{3B}$    & Compacter          & $70.72_{\pm0.87}$ & 0.30  &\cb0.011 &  9.7  &\rb71.5 &\rb46.0 \\
T5$_{3B}$    & Compacter++        & $71.00_{\pm1.62}$ & 0.15  &\cb0.005 &  9.5  & 86.9 & 54.3 \\
T5$_{3B}$    & MAM                & $45.57_{\pm4.67}$ & 171.07&\rb5.999 &\rb14.2& 84.5  & \rb52.1 \\
T5$_{3B}$    & Unipelt            & $47.16_{\pm4.84}$ & 316.70&\rb11.106&\rb12.8&\rb51.0  & \rb38.2 \\
\addlinespace
%                              & score                & params&%params & RAM     & throughput & test
T5$_{11B}$   & Full tuning    & $73.25\hspace{1.8em}$&11307.32 & \rb100&\rb104.9&\rb100.0 & \cb100  \\
T5$_{11B}$   & Houlsby   & $\textbf{76.16}_{\pm1.47}$  & 12.69 & 0.112 & 28.8  & 214.9 & 70.3 \\
T5$_{11B}$   & Pfeiffer           & $50.72_{\pm1.69}$  &  6.34 & 0.056&\cb28.6 & \cb239.5 &\cb 76.3 \\
T5$_{11B}$   & Parallel Adapter   & $68.74_{\pm12.73}$ & 50.41 & 0.446 & 29.0  & \cb233.3 & 73.9 \\
T5$_{11B}$   & IA3                & $61.05_{\pm3.42}$  &  3.48 & 0.031 & 31.0  & 232.2 & \gr{75.5} \\
T5$_{11B}$   & Prefix tuning      & $51.93_{\pm2.21}$ &1211.99 & 10.719& 37.9  & 182.3 & 62.3 \\
T5$_{11B}$   & Prompt tuning      & -                  & 70.78 & 0.626 & 30.2  & 209.5 & 64.0 \\
T5$_{11B}$   & LN tuning     &$\textbf{73.77}_{\pm0.93}$&0.12 &\cb0.001&\cb28.4&\cb275.9 &\cb97.8 \\
T5$_{11B}$   & LoRa (q and v)&$\textbf{76.20}_{\pm1.27}$&20.05 & 0.177 & 28.9 & 216.9 & \gr{81.3} \\
T5$_{11B}$   & LoRA (all)    &$\textbf{76.58}_{\pm2.16}$&91.23 & 0.807 & 30.8 & 161.7 & \gr{62.4} \\
T5$_{11B}$   & KronA              & $72.13_{\pm7.30}$  & 0.77  & 0.007 & 33.2 & 174.0 & \gr{71.8} \\
T5$_{11B}$   & Compacter  & $\textbf{74.33}_{\pm1.40}$ & 0.30  & \cb0.003 & \cb28.6 & 162.7 &\rb56.8 \\
T5$_{11B}$   & Compacter++& $\textbf{74.72}_{\pm0.82}$ & 0.15  & \cb0.001 & \cb28.5 & 208.8 & 64.2 \\
T5$_{11B}$   & MAM                & $51.49_{\pm0.54}$  &1262.39&\rb11.164 & \rb43.3 & \rb152.8 & \rb57.3 \\
T5$_{11B}$   & Unipelt            & $52.29_{\pm3.09}$  &1224.78&\rb10.832 & \rb38.9 & \rb100.2 & \rb46.8 \\
\bottomrule
\end{tabular}
\caption{
Average model performance on our collection of datasets and efficiency metrics: number of trainable parameters, GPU memory consumption, training and inference throughput. We use \textbf{bold} scores to indicate that method outperforms full fine-tuning. \gr{Gray} values for inference throughput indicate that PEFT weights can be merged into the network (made 100\%) if needed. Throughput 95\%-confidence intervals are: $\pm6$ (T5$_{LARGE}$), $\pm7$ (T5$_{3B}$), and $\pm3$ (T5$_{11B}$).
}
\label{tab:peft_comparison_dataset_average_tall}
\end{table}
}