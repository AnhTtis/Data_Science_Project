\begin{table*}[t]
% \begin{small}
\centering
\begin{tabular}{l|c|cc|c|ccc}
% \begin{tabular}{l|>{\columncolor{greenish}}c>{\columncolor{greenish}}c>{\columncolor{redish}}c|c}
\toprule
\textbf{Method} & \textbf{Type} & \textbf{Storage} & \textbf{Memory} & \textbf{Overhead} & \textbf{$<$1B} & \textbf{$<$20B} & \textbf{$>$20B} \\
\midrule
Adapters          \cite{adapters}                & \ad     & \yes & \yes & FFN        & \yes & \yes & \yes \\
AdaMix            \cite{adamix}                  & \ad     & \yes & \yes & FFN        & \yes & \no  & \no \\
SparseAdapter     \cite{sparse_adapter}          & \ad \se & \yes & \yes & FFN        & \yes & \no  & \no \\
Cross-Attn tuning \cite{cross_attention_tuning}  & \se     & \yes & \yes & \gren{no}  & \yes & \no  & \no \\
BitFit            \cite{bitfit}                  & \se     & \yes & \yes & \gren{no}  & \yes & \yes & \no \\
DiffPruning       \cite{diff_pruning}            & \se     & \yes & \no  & \gren{no}  & \yes & \no  & \no \\
Fish-Mask         \cite{fish_mask}               & \se     & \yes & \orange{maybe}\footnotemark[5] & \gren{no}  & \yes & \yes & \no \\
LT-SFT            \cite{lottery_ticket_tuning}   & \se     & \yes & \orange{maybe}\footnotemark[5] & \gren{no} & \yes & \no & \no \\
Prompt Tuning     \cite{prompt_tuning}           & \ad     & \yes & \yes & Seq.       & \yes & \yes & \yes \\
Prefix-Tuning     \cite{prefix_tuning}           & \ad     & \yes & \yes & Seq.       & \yes & \yes & \yes \\
Spot              \cite{spot}                    & \ad     & \yes & \yes & Seq.       & \yes & \yes & \no \\
IPT               \cite{ipt}                     & \ad     & \yes & \yes & FFN, seq.  & \yes & \no  & \no \\
MAM Adapter       \cite{parallel_adapter}        & \ad     & \yes & \yes & FFN, seq.  & \yes & \no  & \no \\
Parallel Adapter  \cite{parallel_adapter}        & \ad     & \yes & \yes & FFN        & \yes & \no  & \no \\
\footnotesize{Intrinsinc SAID} \cite{intrinsic_said} & \rp & \no  & \no  & \gren{no}  & \yes & \yes & \no \\
LoRa              \cite{lora}                    & \rp     & \yes & \yes & \gren{no}  & \yes & \yes & \yes \\
% DyLoRa            \cite{dylora}                  & \rp     & \yes & \yes & \no  & \gren{No}         \\
UniPELT           \cite{unipelt}                 & \ad \rp & \yes & \yes & FFN, seq.  & \yes & \no  & \no \\
\footnotesize{Compacter}   \cite{compacter}      & \ad \rp & \yes & \yes & FFN & \yes & \yes  & \no \\
\footnotesize{PHM Adapter} \cite{compacter}      & \ad \rp & \yes & \yes & FFN        & \yes & \no  & \no \\
KronA             \cite{krona}                   &     \rp & \yes & \yes & \gren{no}  & \yes & \no  & \no \\
KronA$^B_{res}$   \cite{krona}                   & \ad \rp & \yes & \yes & FFN        & \yes & \no  & \no \\
(IA)$^3$          \cite{t_few}                   & \ad     & \yes & \yes & $l_{ff}$      & \no  & \yes  & \no \\
% \footnotesize{WARP \cite{Hambardzumyan2021WARPWA}} & \ad   & \yes & \yes & \no  & Extra input                \\
% IPT               \cite{ipt}                     & \ad     & \yes & \yes & \no  & Extra input                  \\
Attention Fusion  \cite{attention_fusion}        & \ad     & \yes & \yes & Decoder    & \yes & \no  & \no \\
LeTS              \cite{lets}                    & \ad     & \yes & \yes & FFN        & \yes & \no  & \no \\
Ladder Side-Tuning \cite{ladder_side_tuning}     & \ad     & \yes & \yes & Decoder    & \yes & \yes & \no \\
FAR               \cite{far_edge}                & \se     & \yes & \orange{maybe}\footnotemark[6] & \gren{no} & \yes & \no  & \no \\
S4          \cite{design_spaces}                 &\ad \rp \se& \yes & \yes & FFN, seq.& \yes & \yes  & \no \\
\bottomrule
% Meta-Adapters     \cite{meta_adapters}           & A      & \yes & \yes & \no  & Extra FFN                  \\
% Priming           \cite{}                        & A      & \yes & \yes & \no  &                              \\
% Prompt Mapping    \cite{prompt_mapping}          & A      & \yes & \yes & \no  & Extra input                \\
% Polyhistor        \cite{polyhistor}              & A      & \yes & \yes & \no  &                              \\
% PETT              \cite{pett}                    & \rped{?}& \yes & \yes & \no  &                              \\
\end{tabular}
\vspace{2pt}
% \end{small}
\caption{Comparing \peft{} methods across storage, memory, and computational efficiency in terms of reducing backpropagation costs and inference overhead. \textbf{\ad} -- additive, \textbf{\se} -- selective, \textbf{\rp} -- reparametrization-based. Inference overhead: FFN -- extra FFN layer, seq. -- longer attention sequence, decoder -- extra transformer decoder.
% Note that S4-model has smaller inference overhead than other methods that combine Adapters and Prefix-tuning, because it uses differnt combinations of \peft{} methods on different layer (Section \rpef{sec:s4}). FishMask and FAR memory efficiency depend on weight pruning method. FishMask can significantly benefint from sparse operations hardware support (Section \rpef{sec:fishmask}).
% ^* Kronerker layer is a linear layer where weight matrix is constructed as a kroneker product of
}
\label{tab:efficiency_2}
\end{table*}
