\newcommand{\unsim}{\mathord{\sim}}

% NOTE: For IPT, we train 56% of parameters, but then we throw away the autoencoder encoder, so in some sence, only 42% are chaged

\begin{table*}[ht]
\centering
\begin{small}
\begin{tabular}{l|cc|ccc}
% \begin{tabular}{l|>{\columncolor{greenish}}c>{\columncolor{greenish}}c>{\columncolor{redish}}c|c}
\toprule
% \textbf{Method}                                     & Trainable parameters & \textbf{<1B} & \textbf{<20B} & \textbf{>20B} \\
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{2cm}{\centering\textbf{\%Trainable parameters}} & \multirow{2}{2cm}{\centering\textbf{\%Changed parameters}} & \multicolumn{3}{c}{\textbf{Evaluated on}} \\
& & & \textbf{\textless1B} & \textbf{\textless20B} & \textbf{\textgreater20B} \\
\midrule
Adapters          \cite{adapters}                   & 0.1 - 6    & 0.1 - 6      & \yes & \yes & \yes \\
AdaMix            \cite{adamix}                     & 0.1 - 0.2  &0.1 - 0.2     & \yes & \no  & \no  \\
SparseAdapter     \cite{sparse_adapter}             & 2.0        & 2.0          & \yes & \no  & \no  \\
BitFit            \cite{bitfit}                     & 0.05 - 0.1 &0.05 - 0.1    & \yes & \yes & \yes \\
DiffPruning       \cite{diff_pruning}               & 200        & \textbf{0.5} & \yes & \no  & \no  \\
Fish-Mask         \cite{fish_mask}                  & 0.01 - 0.5 &0.01 - 0.5    & \yes & \yes & \no  \\
Prompt Tuning     \cite{prompt_tuning}              & 0.1        & 0.1          & \yes & \yes & \yes \\
Prefix-Tuning     \cite{prefix_tuning}              & 0.1 - 4.0  & 0.1 - 4.0    & \yes & \yes & \yes \\
IPT               \cite{ipt}                        & 56.0       & 56.0     & \yes & \no  & \no \\
MAM Adapter       \cite{parallel_adapter}           & 0.5        & 0.5          & \yes & \no  & \no  \\
Parallel Adapter  \cite{parallel_adapter}           & 0.5        & 0.5          & \yes & \no  & \no  \\
Intrinsinc SAID   \cite{intrinsic_said}             & 0.001 - 0.1& \textbf{100}          & \yes & \yes & \no  \\
LoRa              \cite{lora}                       & 0.01 - 0.5 & \textbf{$\unsim$30}   & \yes & \yes & \yes \\
UniPELT           \cite{unipelt}                    & 1.0        & 1.0      & \yes & \no  & \no  \\ 
Compacter         \cite{compacter}                  & 0.05-0.07  & \textbf{$\unsim$0.1} & \yes & \yes & \no  \\
PHM Adapter       \cite{compacter}                  & 0.2        & \textbf{$\unsim$1.0} & \yes & \no  & \no  \\
KronA             \cite{krona}                      & 0.07       & \textbf{$\unsim$30.0}& \yes& \no  & \no  \\
KronA$^B_{res}$   \cite{krona}                      & 0.07       & \textbf{$\unsim$1.0} & \yes & \no  & \no  \\
(IA)$^3$          \cite{t_few}                      & 0.02       & 0.02     & \no  & \yes & \no  \\
% IPT               \cite{ipt}                        &            &          & \yes &      &      \\
Ladder Side-Tuning\cite{ladder_side_tuning}         & 7.5        & 7.5      & \yes & \yes & \no  \\
FAR               \cite{far_edge}                   & 6.6-26.4   & 6.6-26.4 & \yes & \no  & \no  \\
S4-model          \cite{design_spaces}              & 0.5        &\textbf{\textgreater 0.5} & \yes & \yes & \no  \\

% WARP              \cite{Hambardzumyan2021WARPWA}    &            & \yes &      &      \\
% LeTS              \cite{}                           & \yes &      &      \\
% Cross-Attn tuning \cite{cross_attention_tuning}     & \yes &      &      \\
% Attention Fusion  \cite{attention_fusion}           & \yes &      &      \\
% LT-SFT            \cite{}                           & \yes &      &      \\
% Priming           \cite{}                           & \yes &      &      \\
% Polyhistor        \cite{polyhistor}                 & \yes &      &      \\
% PETT              \cite{pett}                       & \yes &      &      \\
\bottomrule
\end{tabular}
\vspace{2pt}
\end{small}
\caption{What model sizes \peft{} methods have been evaluated on and their typical amount of trainable parameters.}
\label{tab:scale_table}
\end{table*}
