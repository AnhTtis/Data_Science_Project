% 
% !! Old version of the table !!
% 
% The one we are actually using is peft_comparison_dataset_average_tall.tex
% 

{
% \fontsize{9pt}{9pt}\selectfont % Custom font size
\renewcommand{\arraystretch}{0.6} % Reduces the padding within cells & making rows more compact
\begin{table}
\centering
\begin{tabular}{@{}ll|c|r|r|r|r|r@{}}
\toprule
\multirow{2}{*}{Model} & \multirow{2}{*}{Method} & \multirow{2}{*}{Score} & \multicolumn{2}{c|}{Trainable params} & RAM & \multicolumn{2}{c}{Throughput} \\
& & & \multicolumn{1}{c|}{Millions} & \multicolumn{1}{c|}{\%} & \multicolumn{1}{c|}{\footnotesize{(GB)}} & \multicolumn{1}{c|}{Train.} & \multicolumn{1}{c}{Infer.}\\

\midrule
T5$_{Large}$ & Full tuning        & $67.22\hspace{1.8em}$ & 737.67& 100 & 20.7 & 349.6 & 12.88 \\
T5$_{Large}$ & Houlsby            & $\textbf{67.34}_{\pm9.58}$&12.69& 1.720 & 12.1 & 355.4 & 4.64 \\
T5$_{Large}$ & Pfeiffer           & $62.93_{\pm3.52}$ & 6.34   & 0.860  & 14.0 & 379.4 & 5.35 \\
T5$_{Large}$ & Parallel Adapter   & $66.78_{\pm3.85}$ & 50.41  & 6.833  & 14.7 & 430.3 & 5.42 \\
T5$_{Large}$ & IA3                & $55.06_{\pm1.80}$ & 0.34   & 0.047  & 16.2 & 435.8 & 4.21 \\
T5$_{Large}$ & Prefix tuning      & $45.05_{\pm3.89}$ & 77.31  & 10.481 & 15.4 & 395.8 & 4.59 \\
T5$_{Large}$ & Prompt tuning      & $8.97_{\pm30.91}$ & 4.42   & 0.600  & 10.6 & 380.3 & 2.45 \\
T5$_{Large}$ & LN tuning          & $64.68_{\pm4.59}$ & 0.12   & 0.017  & 14.6 & 500.4 & 6.48 \\
T5$_{Large}$ & LoRa (q and v)     & $\textbf{67.42}_{\pm2.32}$ & 2.36  & 0.320  & 14.3 & 432.5 & 6.33 \\
T5$_{Large}$ & LoRA (all)         & $\textbf{68.76}_{\pm1.83}$ & 8.65  & 1.173  & 15.9 & 282.9 & 4.84 \\
T5$_{Large}$ & KronA              & $65.68_{\pm3.27}$ & 0.22   & 0.030  & 14.6 & 381.9 & 5.13 \\
T5$_{Large}$ & MAM                & $46.90_{\pm6.47}$ & 171.07 & 23.191 & 16.6 & 370.8 & 3.16 \\
T5$_{Large}$ & Compacter          & $64.48_{\pm1.81}$ & 0.30   & 0.041  & 14.4 & 284.1 & 3.37 \\
T5$_{Large}$ & Compacter++        & $64.78_{\pm2.23}$ & 0.15   & 0.021  & 13.9 & 333.6 & 4.06 \\
T5$_{Large}$ & Unipelt            & $44.10_{\pm15.48}$& 86.22  & 11.689 & 16.7 & 213.5 & 3.59 \\
\addlinespace
T5$_{3B}$    & Full tuning        & $74.83\hspace{1.8em}$& 2851.60 & 100 & 32.9 & 346.9 & 1.36 \\
T5$_{3B}$    & Houlsby            & $74.66_{\pm1.68}$ & 12.69 & 0.445 &  9.3 & 323.1 & 0.69 \\
T5$_{3B}$    & Pfeiffer           & $69.92_{\pm5.61}$ & 6.34  & 0.223 &  9.6 & 357.1 & 0.75 \\
T5$_{3B}$    & Parallel Adapter   & $74.15_{\pm0.88}$ & 50.41 & 1.768 & 10.0 & 356.4 & 0.75 \\
T5$_{3B}$    & IA3                & $41.77_{\pm0.50}$ & 1.38  & 0.048 & 10.8 & 360.4 & 0.45 \\
T5$_{3B}$    & Prefix tuning      & $48.90_{\pm5.37}$ & 77.25 & 2.709 & 12.3 & 365.1 & 0.56 \\
T5$_{3B}$    & Prompt tuning      & $8.38_{\pm0.50}$  & 17.69 & 0.621 &  8.9 & 346.7 & 0.36 \\
T5$_{3B}$    & LN tuning          & $72.95_{\pm1.38}$ & 0.12  & 0.004 &  9.7 & 488.2 & 0.88 \\
T5$_{3B}$    & LoRa (q and v)     & $\textbf{75.49}_{\pm1.71}$& 3.93  & 0.138 & 9.6 & 357.4 & 0.91 \\
T5$_{3B}$    & LoRA (all)         & $\textbf{75.22}_{\pm1.28}$& 25.17 & 0.883 & 10.6 & 243.7 & 0.74 \\
T5$_{3B}$    & KronA              & $71.98_{\pm0.57}$ & 0.41  & 0.014 & 10.8 & 286.3 & 0.68 \\
T5$_{3B}$    & MAM                & $45.57_{\pm4.67}$ & 171.07& 5.999 & 14.2 & 293.2 & 0.49 \\
T5$_{3B}$    & Compacter          & $70.72_{\pm0.87}$ & 0.30  & 0.011 & 9.7 & 248.1 & 0.36 \\
T5$_{3B}$    & Compacter++        & $71.00_{\pm1.62}$ & 0.15  & 0.005 & 9.5 & 301.6 & 0.45 \\
T5$_{3B}$    & Unipelt            & $47.16_{\pm4.84}$ & 316.70& 11.106& 12.8 & 176.8 & 0.37 \\
\addlinespace
% \midrule
T5$_{11B}$   & Full tuning        & $73.25\hspace{1.8em}$ & 11307.32 & 100  \\
T5$_{11B}$   & Houlsby            & $\textbf{76.16}_{\pm1.47}$  & 12.69 & 0.112 & 28.8 & 293.5 & 0.24     \\
T5$_{11B}$   & Pfeiffer           & $50.72_{\pm1.69}$  & 6.34 & 0.056 & 28.6 & 327.2 & 0.22      \\
T5$_{11B}$   & Parallel Adapter   & $68.74_{\pm12.73}$ & 50.41 & 0.446 & 29.0 & 318.6 & 0.20     \\
T5$_{11B}$   & IA3                & $61.05_{\pm3.42}$  & 3.48 & 0.031 & 31.0 & 317.1 & 0.15      \\
T5$_{11B}$   & Prefix tuning      & $51.93_{\pm2.21}$  & 1211.99 & 10.719 & 35.6 & 62.5 & 0.12  \\
T5$_{11B}$   & Prompt tuning      & -                  & 70.78 & 0.626 & 30.2 & 286.2 & 0.12     \\
T5$_{11B}$   & LN tuning          & $\textbf{73.77}_{\pm0.93}$  & 0.12 & 0.001 & 28.4 & 376.8 & 0.32      \\
T5$_{11B}$   & LoRa (q and v)     & $\textbf{76.20}_{\pm1.27}$  & 20.05 & 0.177 & 28.9 & 296.2 & 0.31     \\
T5$_{11B}$   & LoRA (all)         & $\textbf{76.58}_{\pm2.16}$  & 91.23 & 0.807 & 30.8 & 207.2 & 0.26     \\
T5$_{11B}$   & KronA              & $72.13_{\pm7.30}$  & 0.77 & 0.007 & 33.2 & 237.6 & 0.23      \\
T5$_{11B}$   & MAM                & $51.49_{\pm0.54}$  & 1262.39 & 11.164 &   &   &    \\
T5$_{11B}$   & Compacter          & $\textbf{74.33}_{\pm1.40}$  & 0.30 & 0.003 & 28.6 & 222.2 & 0.12      \\
T5$_{11B}$   & Compacter++        & $\textbf{74.72}_{\pm0.82}$  & 0.15 & 0.001 & 28.5 & 285.2 & 0.14      \\
T5$_{11B}$   & Unipelt            & $52.29_{\pm3.09}$  & 1224.78 & 10.832 & 36.4 & 56.5 & 0.08   \\
\bottomrule
\end{tabular}
\caption{Average model performance on our collection of datasets (Section \ref{sec:comparison_datasets}). Standard deviation is computed over datasets. Raw results (about \red{600} experiments) are available in Appendix \ref{sec:peft_comparison_raw}. \textbf{Bold} values outperform full fine-tuning.
}
\label{tab:peft_comparison_tall}
\end{table}
}