\newcommand{\unsim}{\mathord{\sim}}

% NOTE: For IPT, we train 56% of parameters, but then we throw away the autoencoder encoder, so in some sence, only 42% are chaged

\begin{table*}[ht]
\centering
\begin{small}
\begin{tabular}{l|cc|ccc}
% \begin{tabular}{l|>{\columncolor{greenish}}c>{\columncolor{greenish}}c>{\columncolor{redish}}c|c}
\toprule
% \textbf{Method}                                     & Trainable parameters & \textbf{<1B} & \textbf{<20B} & \textbf{>20B} \\
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{2cm}{\centering\textbf{\%Trainable parameters}} & \multirow{2}{2cm}{\centering\textbf{\%Changed parameters}} & \multicolumn{3}{c}{\textbf{Evaluated on}} \\
& & & \textbf{\textless1B} & \textbf{\textless20B} & \textbf{\textgreater20B} \\
\midrule
Adapters            & 0.1 - 6    & 0.1 - 6      & \yes & \yes & \yes \\
AdaMix              & 0.1 - 0.2  &0.1 - 0.2     & \yes & \no  & \no  \\
SparseAdapter       & 2.0        & 2.0          & \yes & \no  & \no  \\
BitFit              & 0.05 - 0.1 &0.05 - 0.1    & \yes & \yes & \yes \\
DiffPruning         & 200        & \textbf{0.5} & \yes & \no  & \no  \\
Fish-Mask           & 0.01 - 0.5 &0.01 - 0.5    & \yes & \yes & \no  \\
Prompt Tuning       & 0.1        & 0.1          & \yes & \yes & \yes \\
Prefix-Tuning       & 0.1 - 4.0  & 0.1 - 4.0    & \yes & \yes & \yes \\
IPT                 & 56.0       & 56.0     & \yes & \no  & \no \\
MAM Adapter         & 0.5        & 0.5          & \yes & \no  & \no  \\
Parallel Adapter    & 0.5        & 0.5          & \yes & \no  & \no  \\
Intrinsinc SAID     & 0.001 - 0.1& \textbf{100}          & \yes & \yes & \no  \\
LoRa                & 0.01 - 0.5 & \textbf{$\unsim$30}   & \yes & \yes & \yes \\
DoRA                & 0.01 - 0.5 & \textbf{$\unsim$30}   & \no  & \yes & \no \\
UniPELT             & 1.0        & 1.0      & \yes & \no  & \no  \\ 
Compacter           & 0.05-0.07  & \textbf{$\unsim$0.1} & \yes & \yes & \no  \\
PHM Adapter         & 0.2        & \textbf{$\unsim$1.0} & \yes & \no  & \no  \\
KronA               & 0.07       & \textbf{$\unsim$30.0}& \yes& \no  & \no  \\
KronA$^B_{res}$     & 0.07       & \textbf{$\unsim$1.0} & \yes & \no  & \no  \\
(IA)$^3$            & 0.02       & 0.02     & \no  & \yes & \no  \\
% IPT               &            &          & \yes &      &      \\
Ladder Side-Tuning  & 7.5        & 7.5      & \yes & \yes & \no  \\
FAR                 & 6.6-26.4   & 6.6-26.4 & \yes & \no  & \no  \\
S4-model            & 0.5        &\textbf{\textgreater 0.5} & \yes & \yes & \no  \\

% WARP              &            & \yes &      &      \\
% LeTS              & \yes &      &      \\
% Cross-Attn tuning & \yes &      &      \\
% Attention Fusion  & \yes &      &      \\
% LT-SFT            & \yes &      &      \\
% Priming           & \yes &      &      \\
% Polyhistor        & \yes &      &      \\
% PETT              & \yes &      &      \\
\bottomrule
\end{tabular}
\vspace{2pt}
\end{small}
\caption{What model sizes PEFT methods have been evaluated on and their typical amount of trainable parameters as reported in the literature. For updated number of trainable parameters lookup Table \ref{tab:peft_comparison_dataset_average_tall}.}
\label{tab:scale_table}
\end{table*}
