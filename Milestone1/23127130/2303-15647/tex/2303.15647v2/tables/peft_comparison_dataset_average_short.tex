\begin{table}[t]
\centering
\begin{small}
\begin{tabular}{l|ccc}
\toprule
\textbf{Method} & T5$_{LARGE}$ & T5$_{3B}$ & T5$_{11B}$ \\
\midrule
\textbf{Additive methods}&&&\\
Adapters (Houlsby) &
$\textbf{67.34}_{\pm9.58}$ &
$\uline{74.66}_{\pm1.68}$ &
$\textbf{76.16}_{\pm1.47}$ \\
Adapters (Pfeiffer)& $62.93_{\pm3.52}$ & $\uline{69.92}_{\pm5.61}$ & $50.72_{\pm1.69}$ \\
Parallel Adapter   & $\uline{66.78}_{\pm3.85}$ & $\uline{74.15}_{\pm0.88}$ & $\uline{68.74}_{\pm12.73}$ \\
IA3                & $55.06_{\pm1.80}$ & $41.77_{\pm0.50}$ & $61.05_{\pm3.42}$ \\
Prefix Tuning      & $45.05_{\pm3.89}$ & $48.90_{\pm5.37}$ & $51.93_{\pm2.21}$ \\
Prompt Tuning      & $8.97_{\pm30.91}$ & $8.38_{\pm0.50}$ & - \\

\textbf{Selective methods}&&&\\
LN Tuning         & $\uline{64.68}_{\pm4.59}$ & $72.95_{\pm1.38}$ & $\textbf{73.77}_{\pm0.93}$ \\

\textbf{Reparametrization-based methods}&&&\\
LoRA (q and v)   &$\textbf{67.42}_{\pm2.32}$& $\textbf{75.49}_{\pm1.71}$ & $\textbf{76.20}_{\pm1.27}$ \\
LoRA (all linear)&$\textbf{68.76}_{\pm1.83}$& $\textbf{75.22}_{\pm1.28}$ & $\textbf{76.58}_{\pm2.16}$ \\
KronA             & $\uline{65.68}_{\pm3.27}$ & $71.98_{\pm0.57}$ & $\uline{72.13}_{\pm7.30}$ \\

\textbf{Hybrid methods}&&&\\
MAM               & $46.90_{\pm6.47}$ & $45.57_{\pm4.67}$ & $51.49_{\pm0.54}$ \\
Compacter         & $64.48_{\pm1.81}$ & $70.72_{\pm0.87}$ & $\textbf{74.33}_{\pm1.40}$ \\
Compacter++       & $64.78_{\pm2.23}$ & $71.00_{\pm1.62}$ & $\textbf{74.72}_{\pm0.82}$ \\
Unipelt           & $44.10_{\pm15.48}$ & $47.16_{\pm4.84}$ & $52.29_{\pm3.09}$ \\

\midrule
Full tuning       & $67.22$ & $74.83$ & $73.25$ \\
\bottomrule
\end{tabular}

\vspace{2pt}
\end{small}
\caption{Average model performance on our collection of datasets (Section \ref{sec:peft_comparison_setup}) with 95\% confidence intervals (two standard deviations). We \textbf{bold} values that outperform full-tuning by mean value. We \uline{underline} values that achieve full-tuning performance within the confidence interval.}
\label{tab:peft_comparison_dataset_average_short}
\end{table}
% NOTE: I can't get the new results without knowing where they are coming from and the google sheet wasn't updated
% Specifically, I don't know the source of: 1) updated full fine-tuning numbers
% Also, we dont' have all the new results for prompt tuning, only T5_Large
% \textbf{Additive methods}&&&\\
% Adapters (Houlsby)      & $67.34_{\pm9.58}$ & $74.66_{\pm1.68}$ & $76.16_{\pm1.47}$ \\
% Adapters (Pfeiffer)     & $62.93_{\pm3.52}$ & $69.92_{\pm5.59}$ & $50.72_{\pm1.69}$ \\
% Scaled Parallel Adapter & $66.78_{\pm3.85}$ & $74.15_{\pm0.72}$ & $68.74_{\pm12.73}$ \\
% AI$^3$                  & $55.25_{\pm1.80}$ & $41.77_{\pm0.00}$ & $61.20_{\pm3.42}$ \\
% Prefix Tuning           & $45.05_{\pm3.89}$ & $49.09_{\pm5.44}$ & $51.93_{\pm2.21}$ \\
% Prompt Tuning           & $55.48_{\pm1.77}$ & $??_{\pm??}$ & $??_{\pm??}$ \\
% \textbf{Selective methods}&&&\\
% LN Tuning               & $64.68_{\pm4.59}$ & $72.95_{\pm1.38}$ & $74.53_{\pm0.93}$\\
% Attention Tuning        & & & \\
% % LT-SFT                  & & & \\
% Layer Freezing (train last layer)           & $64.53_{\pm2.63}$ & $72.13_{\pm2.11}$ & $74.96_{\pm2.48}$\\
% Layer Freezing (train last 2)               & $65.28_{\pm1.23}$ & $72.26_{\pm2.31}$ & $74.57_{\pm1.72}$\\
% Layer Freezing (train last 4)               & $65.57_{\pm4.04}$ & $72.42_{\pm1.02}$ & $74.83_{\pm1.57}$\\
% Layer Freezing (train last 8)               & $65.72_{\pm1.22}$ & $72.70_{\pm1.21}$ & $75.95_{\pm1.45}$\\
% \textbf{Reparametrization-based methods}&&&\\
% LoRA (q and v)          & $67.42_{\pm2.32}$ & $75.49_{\pm1.64}$ & $76.20_{\pm1.27}$ \\
% LoRA (all matricies)    & $68.76_{\pm1.83}$ & $75.22_{\pm2.28}$ & $76.58_{\pm2.16}$ \\
% KronA                   & $65.68_{\pm3.27}$ & $71.98_{\pm0.57}$ & $72.14_{\pm7.3}$ \\
% \textbf{Hybrid methods}&&&\\
% MAM Adapter             & $46.90_{\pm6.47}$ & $45.57_{\pm4.64}$ & $51.49_{\pm0.54}$ \\
% Compacter               & $64.48_{\pm1.81}$ & $70.72_{\pm0.72}$ & $74.33_{\pm1.40}$ \\
% Compacter++             & $64.78_{\pm2.23}$ & $71.00_{\pm1.54}$ & $74.72_{\pm0.82}$ \\
% UniPELT                 & $44.10_{\pm15.48}$ & $47.16_{\pm4.82}$ & $52.29_{\pm3.09}$ \\
% LoRA + LN Tuning        & & & \\
% \midrule
% Full fine-tuning        & $85.53$ & $90.67$ & \\
% \bottomrule
