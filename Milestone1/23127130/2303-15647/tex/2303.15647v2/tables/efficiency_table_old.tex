\begin{table*}[t]
\centering
\begin{small}
\begin{tabular}{l|c|ccc|c}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Type}} & \multicolumn{3}{c|}{\textbf{Efficiency}} & \multirow{2}{*}{\textbf{Inference overhead}} \\
& & \textbf{Disk} & \textbf{RAM} & \textbf{BP} & \\
\midrule
Adapters          \cite{adapters}                & \ad     & \yes & \yes & \no  & + FFN                    \\
AdaMix            \cite{adamix}                  & \ad     & \yes & \yes & \no  & + FFN                    \\
SparseAdapter     \cite{sparse_adapter}          & \ad \se & \yes & \yes & \no  & + FFN                    \\
Cross-Attn tuning \cite{cross_attention_tuning}  & \se     & \yes & \yes & \no  & \gren{No overhead}           \\
BitFit            \cite{bitfit}                  & \se     & \yes & \yes & \no  & \gren{No overhead}           \\
DiffPruning       \cite{diff_pruning}            & \se     & \yes & \no  & \no  & \gren{No overhead}           \\
Fish-Mask         \cite{fish_mask}               & \se     & \yes & \orange{\xmark}\footnotemark[5] & \no  & \gren{No overhead} \\
LT-SFT            \cite{lottery_ticket_tuning}   & \se     & \yes & \orange{\xmark}\footnotemark[5] & \no  & \gren{No overhead} \\
Prompt Tuning     \cite{prompt_tuning}           & \ad     & \yes & \yes & \no  & + input                  \\
Prefix-Tuning     \cite{prefix_tuning}           & \ad     & \yes & \yes & \no  & + input                  \\
Spot              \cite{spot}                    & \ad     & \yes & \yes & \no  & + input                \\
IPT               \cite{ipt}                     & \ad     & \yes & \yes & \no  & + FFN and input \\
MAM Adapter       \cite{parallel_adapter}        & \ad     & \yes & \yes & \no  & + FFN and input          \\
Parallel Adapter  \cite{parallel_adapter}        & \ad     & \yes & \yes & \no  & + FFN                    \\
\footnotesize{Intrinsinc SAID} \cite{intrinsic_said} & \rp & \yes  & \no & \no  & \gren{No overhead}            \\
LoRa              \cite{lora}                    & \rp     & \yes & \yes & \no  & \gren{No overhead}           \\
% DyLoRa            \cite{dylora}                  & \rp     & \yes & \yes & \no  & \gren{No overhead}         \\
DoRA              \cite{liu2024dora}            & \rp      & \yes & \yes & \no  & \gren{No overhead}        \\
UniPELT           \cite{unipelt}                 & \ad \rp & \yes & \yes & \no  & + FFN and input          \\
\footnotesize{Compacter}         \cite{compacter}               & \ad \rp & \yes & \yes & \no  & + FFN                    \\
\footnotesize{PHM Adapter} \cite{compacter}      & \ad \rp & \yes & \yes & \no  & + FFN                    \\
KronA             \cite{krona}                   &     \rp & \yes & \yes & \no  & \gren{No overhead}           \\
KronA$^B_{res}$   \cite{krona}                   & \ad \rp & \yes & \yes & \no  & +  linear layer          \\
(IA)$^3$          \cite{t_few}                   & \ad     & \yes & \yes & \no  & + gating                 \\
% \footnotesize{WARP \cite{Hambardzumyan2021WARPWA}} & \ad   & \yes & \yes & \no  & + input                \\
% IPT               \cite{ipt}                     & \ad     & \yes & \yes & \no  & + input                  \\
Attention Fusion  \cite{attention_fusion}        & \ad     & \yes & \yes & \yes & + decoder         \\
LeTS              \cite{lets}                    & \ad     & \yes & \yes & \yes & + FFN                    \\
Ladder Side-Tuning \cite{ladder_side_tuning}     & \ad     & \yes & \yes & \yes & + decoder                \\
FAR               \cite{far_edge}                & \se     & \yes & \yes & \no  & \gren{No overhead}           \\
S4-model          \cite{design_spaces}           &\ad \rp \se& \yes & \yes & \no  & + FFN and input          \\
\bottomrule
% Meta-Adapters     \cite{meta_adapters}           & A      & \yes & \yes & \no  & + FFN                  \\
% Priming           \cite{}                        & A      & \yes & \yes & \no  &                              \\
% Prompt Mapping    \cite{prompt_mapping}          & A      & \yes & \yes & \no  & + input                \\
% Polyhistor        \cite{polyhistor}              & A      & \yes & \yes & \no  &                              \\
% PETT              \cite{pett}                    & \rped{?}& \yes & \yes & \no  &                              \\
\end{tabular}
\end{small}
\vspace{2pt}
\caption{Comparing \peft{} methods across storage (\textbf{disk}), memory (\textbf{RAM}), and computational efficiency in terms of reducing backpropagation costs (\textbf{BP}) and \textbf{inference overhead}. The \yes{} means that the method is more effective than full fine-tuning, the \no{} means that it is less efficient than full fine-tuning.\\Types: \textbf{\ad} -- additive, \textbf{\se}~--~selective, \textbf{\rp} -- reparametrization-based.
% Note that S4-model has smaller inference overhead than other methods that combine Adapters and Prefix-tuning, because it uses differnt combinations of \peft{} methods on different layer (Section \rpef{sec:s4}). FishMask and FAR memory efficiency depend on weight pruning method. FishMask can significantly benefint from sparse operations hardware support (Section \rpef{sec:fishmask}).
% ^* Kronerker layer is a linear layer where weight matrix is constructed as a kroneker product of
}
\label{tab:efficiency}
\end{table*}
