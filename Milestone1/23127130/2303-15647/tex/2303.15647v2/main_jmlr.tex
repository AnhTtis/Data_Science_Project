\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% OUR IMPORTS
\usepackage{natbib}
\usepackage{epigraph}
\usepackage{hyperref}
\usepackage{booktabs}
\setlength {\marginparwidth }{2cm}
\usepackage[textsize=tiny]{todonotes}
\usepackage{namedtensor}
\usepackage{multirow}
\usepackage{floatflt}
\usepackage{makecell}
\usepackage{fancyhdr}
\usepackage{pifont}
\usepackage{comment}
\usepackage{tcolorbox}

\newcommand{\smallerfontsize}{\fontsize{9.5}{9.5}\selectfont}
\pagestyle{fancy}
\fancyhf{}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0pt}
\newcommand{\xmark}{\ding{55}}

\usepackage{listings}
% \lstset{
%     language=Python,
%     basicstyle=\ttfamily\smallerfontsize,
%     morekeywords={self, True, False, with},
%     keywordstyle=\color{blue},
%     stringstyle=\color{red},
%     commentstyle=\color{green!50!black},
%     showstringspaces=false,
%     tabsize=4
% }
\lstset{
    language=Python,
    basicstyle=\ttfamily\small\linespread{0.8}\selectfont,
    morekeywords={self, True, False, with},
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    commentstyle=\color{green!50!black},
    showstringspaces=false,
    tabsize=4,
    % backgroundcolor=\color{gray!10},
    frame=single,                    % adds a frame around the code
    framesep=3pt,                    % padding space between frame and code
    rulecolor=\color{black!30},      % frame color
    numberstyle=\tiny\color{gray},   % style for line numbers
    numbersep=10pt,                  % space between line numbers and code
    xleftmargin=15pt,                % left padding
    xrightmargin=15pt,               % right padding
    breaklines=true,                 % sets automatic line breaking
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space}, % indicator for line breaks
    aboveskip=1em,                   % space above the listing
    belowskip=1em,                   % space below the listing
    frameround=tttt,                 % rounded corners for frame
}

% axis names
\usepackage{amsfonts}
\ndef{\ax}{ax}
\ndef{\dd}{d}
\ndef{\layer}{layer}
\ndef{\seq}{seq}
\ndef{\subseq}{subseq}
\ndef{\key}{key}
\ndef{\val}{val}
\ndef{\heads}{heads}
\ndef{\batch}{batch}
\ndef{\inp}{input} \ndef{\hidden}{hidden} \ndef{\out}{out}
\ndef{\height}{height} \ndef{\width}{width} \ndef{\chans}{chans}
\ndef{\kernel}{kernel} \ndef{\kh}{kh} \ndef{\kw}{kw}
\ndef{\vocab}{vocab}
\ndef{\classes}{classes}
\ndef{\state}{state}
\ndef{\emb}{emb}

\usepackage{ulem}
\usepackage{hhline}
\usepackage{colortbl}

\usepackage{caption}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{adjustbox}
\usepackage{wrapfig}


\ndef{\R}{\mathbf{R}}

% \newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\orange}[1]{\textcolor{orange}{#1}}

% Define the gradient color for table coloring
\definecolor{bluegrad}{RGB}{200, 220, 240}
\definecolor{redgrad}{RGB}{255, 210, 210}

% Define a command for gradient shading for table coloring
\newcommand{\coloredcell}[2]{
  \multicolumn{1}{#1>{\columncolor[rgb]{#2}}c}
}

\definecolor{tablegreen}{RGB}{34, 139, 34}
\definecolor{tablered}{RGB}{220, 20, 60}

\definecolor{a_color}{HTML}{4285F4}
\definecolor{r_color}{HTML}{ED8E55}
\definecolor{s_color}{HTML}{E6C800} % FADD87, D2D200, D2D264, E6C800, 

\newcommand{\ad}{\textcolor{a_color}{A}}
\newcommand{\rp}{\textcolor{r_color}{R}}
\newcommand{\se}{\textcolor{s_color}{S}}

\newcommand{\tred}[1]{\textcolor{tablered}{#1}}
\newcommand{\gren}[1]{\textcolor{tablegreen}{#1}}

\newcommand{\yes}[0]{\gren{\checkmark}}
\newcommand{\no}[0]{\tred{\xmark}}

\newcommand{\peft}{PEFT}

\renewcommand{\cite}[1]{\citep{#1}}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Definitions of handy macros can go here

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

% \jmlrheading{1}{2023}{1-48}{4/00}{10/00}{Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky}

% Short headings should be running head and authors last names

\ShortHeadings{Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning}{Lialin, Deshpande, Yao, and Rumshisky}
\firstpageno{1}

\begin{document}

\title{Scaling Down to Scale Up: \\A Guide to Parameter-Efficient Fine-Tuning}



\author{\name Vladislav\ Lialin \email vlialin@cs.uml.edu \thanks{Correspondence to: \texttt{vlad.lialin@gmail.com}} \\
    \addr University of Massachusetts Lowell
    \AND
    \name Vijeta\ Deshpande \email vijeta\_deshpande@student.uml.edu \\
    \addr University of Massachusetts Lowell
    \AND
    \name Xiaowei\ Yao \email xiaowei\_yao@student.uml.edu \\
    \addr University of Massachusetts Lowell
    \AND
    \name Anna\ Rumshisky \email arum@cs.uml.edu \\
    \addr University of Massachusetts Lowell \\
    \addr Amazon Alexa AI
}

\editor{}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
%This paper presents a systematic overview of parameter-efficient fine-tuning methods covering over 50 papers published between February 2019 and February 2024. 
This paper presents a systematic overview of parameter-efficient fine-tuning methods, covering over 50 papers published between early 2019 and mid-2024. These methods aim to address the challenges of fine-tuning large language models by training only a small subset of parameters. We provide a taxonomy that covers a broad range of methods and present a detailed method comparison with a specific focus on real-life efficiency in fine-tuning multibillion-scale language models.
We also conduct an extensive head-to-head experimental comparison of 15 diverse PEFT methods, evaluating their performance and eﬀiciency on models up to 11B parameters. Our findings reveal that methods previously shown to surpass a strong LoRA baseline face diﬀiculties in resource-constrained settings, where hyperparameter optimization is limited and the network is fine-tuned only for a few epochs. Finally, we provide a set of practical recommendations for using PEFT methods and outline potential future research directions.
\end{abstract}

\begin{keywords}
    Parameter-Efficient Fine-Tuning, Large Language Models
\end{keywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Start of Introduction
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{floatingfigure}[r]{0.4\textwidth}
\epigraph{One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation...}{Rich Sutton, The Bitter Lesson}
\vspace{-1em}
\end{floatingfigure}

In October 2018, BERT Large \cite{devlin2018bert} with 350 million parameters was the biggest Transformer model \cite{vaswani2017attention} ever trained. At the time, contemporary hardware struggled to fine-tune this model. The section ``Out-of-memory issues'' on BERT's GitHub\footnotemark{} specifies the maximum batch size for BERT Large given 12Gb of GPU RAM and 512 tokens as \textbf{zero}.
Five years in, publicly available models grew to 176 billion parameters \cite{bloom,zhang2022opt,zeng2022glm130b}, i.e. by a factor of 500. Published literature includes models up to 1 trillion parameters \cite{palm,megatron,switch}. However, single-GPU RAM increased less than 10 times due to the high cost of HBM memory.
Model size scales almost \textbf{two orders of magnitude quicker} than computational resources making fine-tuning the largest models to downstream tasks infeasible for most and impractical for everyone.
% The Bitter Lesson \cite{} only works if our computation continues to scale 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Moved BERT link to here to make the first page footnotes look a bit nicer
\footnotetext{\href{https://github.com/google-research/bert}{github.com/google-research/bert}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In-context learning \cite{radford2019language} thus became the new normal, the standard way to pass downstream task training data to billion-scale language models. However, the limited context length imposed by the transformer architecture \cite{vaswani2017attention, huang2018music}, the absence of ICL abilities in moderately large language models \cite{lu2023emergent}, the quadratic increase in computational cost with an increase in context length (or demonstrations in ICL) \cite{keles2023computational}, and the sensitivity of ICL performance \cite{bertsch2024context} present challenges in the utility, reliability, and efficiency of ICL. 
%However, the limited context window of Transformer architecture artificially limits the training set size to just a few examples, typically less than 100 . This constraint, coupled with the absence of in-context learning performance guarantees even on the training data, presents a challenge. Finally, expanding the context size leads to a quadratic increase in inference costs \cite{keles2023computational}. 
In cases where the model performs at par or better in the ICL setting compared to the fine-tuned model, fine-tuning is still a lucrative strategy due to the impractical inference cost of ICL \cite{bertsch2024context}.
%Even though language models perform exceptionally well \cite{brown2020language_gpt3, bertsch2024context} in a few-shot scenario, ``get more data'' is still the most reliable way to improve on any given task\footnote{\href{http://karpathy.github.io/2019/04/25/recipe/}{A Recipe for Training Neural Networks, A. Karpathy}}. 
Thus, we, as a community of researchers and engineers, need efficient ways to train on downstream task data.

Parameter-efficient fine-tuning (PEFT) aims to resolve this problem by only training a small set of parameters, which might be a subset of the existing model parameters or a set of newly added parameters. These methods differ in parameter and memory efficiency, training speed, final model quality, and additional inference costs (if any).

In the last few years, more than a hundred PEFT papers have been published, with several studies \cite{delta_tuning} providing a good overview of the most popular methods, such as Adapters \cite{adapters}, BitFit \cite{bitfit}, LoRA \cite{lora}, Compacter \cite{compacter}, and Soft Prompts \cite{p_tuning,prefix_tuning}.

\citet{modular_deep_learning} presented a survey on modular deep learning, providing an overview of several similar methods from the perspective of modularity and multi-task inference. Our focus differs by concentrating on PEFT methods, specifically for fine-tuning large language models, where minimizing RAM consumption and training time without sacrificing performance is crucial.

This survey presents a systematic overview, comparison, and taxonomy of 30 parameter-efficient fine-tuning methods. Over the last year, research efforts have also focused on replicating the success of PEFT in the pre-training regime. Hence, we also discuss a few prominent methods that aim to achieve efficiency gains during pre-training. We discuss 30 methods in-depth, covering over 50 papers published from %February 2019 to February 2024
early 2019 to mid-2024. We highlight the current unresolved challenges in PEFT, including the limited theoretical understanding, the performance gap between PEFT and traditional fine-tuning, and reporting issues.

We conduct the most 
extensive experimental comparison of PEFT methods 
%to date (as of March 2024)
, evaluating 14 methods and their variations across five datasets and three model sizes (0.7B, 3B, and 11B). The study includes a detailed comparison of these methods' efficiency in terms of GPU memory consumption and throughput. Our findings reveal that methods previously shown to outperform LoRA struggle to do so in resource-constrained settings and exhibit high hyperparameter sensitivity in hybrid PEFT methods. 

We found that Kronecker-based reparametrizations, while not enhancing memory efficiency compared to matrix-product counterparts, can improve training and inference speeds with efficient implementation. Surprisingly, Layer Norm tuning performs exceptionally well compared to most PEFT methods in our study. We also note a significant discrepancy between reported and actual trainable parameter counts in PEFT methods. This leads to unforeseen outcomes, such as the high computational costs of Prompt Tuning and Hybrid methods.
Our code is available on Github\footnote{\href{http://github.com/guitaricet/peft_comparison}{github.com/guitaricet/peft\_comparison}}.

In conclusion, we suggest several avenues for improvement, such as developing standardized PEFT benchmarks,
conducting in-depth studies on hyperparameters and interpretability,
exploring the difference in training dynamics of reparametrized neural networks,
further improving training and inference efficiency of PEFT methods,
%and drawing inspiration from on-device (edge) machine learning research.
and utility of PEFT methods with quantized backbone models.

% % last paragraph from Vlad's Theis/introduction
% \begin{comment}
% A shift from neural network architecture engineering to scaling is akin to a shift from feature engineering to representation learning. The convergence of architectures and training objectives created a new level of abstraction that we work with today. It allows us to largely ignore the problems of data preprocessing, inductive bias, and data labelling, and simplifies the formalization of real-world tasks to the ML framework. This comes at a pretty high cost. Specifically, the cost of computation and large-scale data mining. In this thesis, we focus on the former, as computational costs are the biggest limiting factor for most deep learning (DL) researchers and practitioners.
% \end{comment}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Background
\section{Background: Transformer}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Many of the parameter-efficient fine-tuning techniques discussed in this survey can be applied to all neural networks, though some are specifically designed for the Transformer architecture \cite{vaswani2017attention}. Given that Transformers are the largest neural networks ever trained, these methods are particularly valuable. Thus, we present a brief overview of the Transformer to provide context for these techniques.

The core building block of the Transformer architecture consists of multi-head attention (MHA) followed by a fully-connected layer (FFN), as illustrated in Figure~\ref{fig:transformer}. Both attention and fully-connected layers incorporate residual connections \cite{resnet} and Layer Normalization \cite{layer_norm} to improve trainability.

The heart of the Transformer is the attention operation \cite{bahdanau2014neural}. It computes the softmax-normalized weighted average of the input tokens. Attention weights are computed according to the pairwise dot-product between each token key and query. Keys and queries are typically computed as simple linear projections of the input tokens.
% Key-query pairwise products allow for direct token-to-token communication in contrast to recurrent neural networks. However, this comes at a cost of quadratic computational time with respect to the sequence length.
Equation \ref{eq:def_att} describes it in the NamedTensor notation \cite{named_tensor}.

% \documentclass{article}
% \usepackage{amsmath}  % for equations
% \usepackage{amsfonts} % for mathbb
% \usepackage{graphicx} % for figures

\begin{figure}[h]
    \begin{minipage}{0.5\textwidth} % Adjust the width as needed
        \begin{equation}
            \label{eq:def_att}
            \begin{aligned}
                & \operatorname{Att}
                \colon
                \mathbb{R}^{\key}
                \times
                \mathbb{R}^{\seq \times\key}
                \times \mathbb{R}^{\seq \times \val}
                \rightarrow \mathbb{R}^{\val} \\
                & \operatorname{Att}(Q,K,V) = \left( \nfun{\seq}{softmax} \frac{Q \ndot{\key} K}{\sqrt{|\key|}} \right) \ndot{\seq} V \\
            \end{aligned}
        \end{equation}
        % where keys, queries, and values are defined as
        \begin{equation*}
            \label{eq:kqv}
            \begin{aligned}
                Q(x) &= x \cdot W_Q + b_k, \\
                K(x) &= x \cdot W_K + b_q, \\
                V(x) &= x \cdot W_V + b_v, \\
            \end{aligned}
        \end{equation*}

        \begin{equation*}
            \begin{aligned}
                W_Q, \text{ } W_K &\in \R^{\inp \times \key}, W_V \in \R^{\inp \times \val} \\
                b_Q, \text{ } b_K &\in \R^{\key}, \text{ } b_V \in \R^{\val}
            \end{aligned}
        \end{equation*}
    \end{minipage}%
    \begin{minipage}{0.5\textwidth} % Adjust the width as needed
        \centering
        \includegraphics[width=0.6\linewidth]{img/transformer_diagram.pdf}
        \caption{Basic Transformer block}
        \label{fig:transformer}
    \end{minipage}
\end{figure}


A number of methods act specifically on the matrices $W_K$, $W_Q$, $W_V$; they provide the main mechanism to pass information from one token to another and control what information (value) is being passed.

Although specific implementations of the Transformer may vary, such as incorporating a cross-attention layer in seq2seq networks or using LayerNorm before sublayers (Pre-LN), most parameter-efficient fine-tuning methods for Transformers only rely on the basic MHA + FFN structure. These methods can be readily adapted to architectural variations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Taxonomy section
\section{Taxonomy of PEFT: a birds-eye view}
\label{sec:taxonomy}

% PEFT methods can be classified based on their approach and objective. Approach-wise, methods can introduce new parameters or fine-tune existing ones. In terms of objectives, methods aim to minimize memory footprint and/or storage efficiency. In this section, we begin by presenting a taxonomy based on the former. Figure \ref{fig:taxonomy} and sections \ref{sec:additive}-\ref{sec:hybrid} present an overview of the taxonomy and illustrate 30 PEFT methods.
% Additionally, we describe 20 PEFT methods in detail, accompanied by pseudocode, in Sections \ref{sec:additive_section_adapters} - \ref{sec:hybrid_section}.

PEFT methods can be categorized by their approach or objective. In terms of approach, they can introduce new parameters, fine-tune existing ones, or reparameterize them. In terms of objectives, they aim to minimize memory footprint, improve storage efficiency, or add modularity.
In this section, we begin by presenting a taxonomy based on the former. Figure \ref{fig:taxonomy} and Sections \ref{sec:additive}-\ref{sec:hybrid} present an overview of the taxonomy and illustrate over 30 PEFT methods. In the following sections, we describe these PEFT methods in detail, \textbf{accompanied by easy-to-understand pseudo-code}, in Sections \ref{sec:additive_section_adapters} - \ref{sec:hybrid_section}.

\begin{figure*}
    \centering
    % \includegraphics[width=1.0 \textwidth]{img/peft_taxonomy_v4.0.jpg}
    \includegraphics[width=1.0 \textwidth]{img/peft_taxonomy_v4.1.pdf}
    % \includegraphics[width=1.0 \textwidth]{img/peft_taxonomy_v2.0.jpg}
    \caption{Parameter-efficient fine-tuning methods taxonomy. We identify three main classes of methods: \textbf{Addition}-based, \textbf{Selection}-based, and \textbf{Reparametrization}-based. Within additive methods, we distinguish two large included groups: \textbf{Adapter-like} methods and \textbf{Soft prompts}.}
    \label{fig:taxonomy}
\end{figure*}


\subsection{\ad dditiion-based methods}
\label{sec:additive}

Addition-based methods augment the pre-trained model with additional parameters or layers and train only the newly introduced elements. This is the largest and most widely explored category of PEFT methods. Within this category, two large subcategories have emerged: Adapter-like methods and Soft Prompts.

% The main idea behind additive methods is augmenting the existing pre-trained model with extra parameters or layers and training only the newly added parameters.
% As of now, this is the largest and widely explored category of PEFT methods.
% Within this category, two large subcategories have emerged: Adapter-like methods and soft prompts.
% 
\paragraph{Adapters}
Adapters \cite{adapters} are a type of additive PEFT method that introduces small fully-connected networks after Transformer sub-layers. The idea has been widely adopted \cite{adapterhub} \footnote{\href{https://github.com/adapter-hub/adapter-transformers}{github.com/adapter-hub/adapter-transformers}}, and multiple variations of Adapters have been proposed. These variations include modifying the placement of adapters \cite{parallel_adapter,parallel_adapter2}, pruning \cite{sparse_adapter}, and using reparametrization to reduce the number of trainable parameters \cite{compacter}.
Section \ref{sec:additive_section_adapters} discusses Adapter-based methods in detail.

\paragraph{Soft Prompts}

Language model prompting \cite{radford2019language} aims to control the behavior of a language model by modifying the input text, which typically consists of a task description accompanied by a few in-context examples. However, these methods are difficult to optimize and are inherently limited by the maximum model input length. To address these drawbacks, the concept of ``soft'' prompts was introduced \cite{p_tuning,prompt_tuning,prefix_tuning}, where a part of the model's input embeddings is fine-tuned via gradient descent. This pivots the problem of finding prompts in a discrete space to a continuous optimization problem. Soft prompts can be trained for the input layer only \cite{p_tuning,prompt_tuning} or for all layers \cite{prefix_tuning}. Recent advancements explore how soft prompts could be pre-trained or partially reused to reduce the fine-tuning costs \cite{spot,Hambardzumyan2021WARPWA,prompt_mapping,ipt}. We discuss Soft Prompts in Section \ref{sec:additive_section_soft_prompts}.

\paragraph{Other additive approaches}
Additive methods are a diverse category of parameter-efficient fine-tuning techniques that extend beyond adapters and soft prompts. We discuss these in Section \ref{sec:additive_section_others}.
% For example, LeTS \cite{lets}, LST \cite{ladder_side_tuning}, and (IA)$^3$ \cite{t_few} introduce novel ways to add parameters that improve adapters or soft prompts in terms of memory, computation, or accuracy.

\paragraph{Why add parameters?}
Although these methods introduce additional parameters to the network, they achieve significant speed and memory improvements. This is achieved by reducing the size of the gradients and optimizer states. In the case of Adam \cite{adam}, for every byte of trainable parameter, one extra byte is needed for its gradient, and two more bytes are needed to store the optimizer state: the first and second moments of the gradient. In practice, training a model requires 12-20 times more GPU memory than the model weights. By saving memory on optimizer states, gradients, and quantizing frozen model parameters \cite{dettmers2023qlora}, additive PEFT methods enable the fine-tuning of much larger networks or the use of larger microbatch sizes\footnote{Batch size = microbatch size $\times$ gradient accumulation $\times$ num. devices}, which improves training throughput on GPUs.
Moreover, the optimizer update step takes less time for PEFT methods than for full fine-tuning due to the smaller number of parameters to be updated. This effect is very noticeable in practice. Finally, optimizing fewer parameters in distributed setups drastically reduces communication volume.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Efficiency table footnote
\footnotetext[5]{Depends on sparse operations hardware support.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{\se election-based methods}

Arguably the earliest example of selective PEFT is fine-tuning only a few top layers of a network \cite{last_layer_tuning}. Modern approaches are usually based on the type of layer \cite{cross_attention_tuning,layer_norm_tuning} or the internal structure, such as tuning only model biases \cite{bitfit} or particular rows \cite{far_edge}.

%An extreme version of selective methods is sparse training, which completely ignores the structure of the model and selects parameters individually \cite{fish_mask,lottery_ticket_tuning,diff_pruning}. 
Sparse training is another version of selective PEFT that often ignores the structure and selects parameters to tune based on carefully designed selection criteria \cite{fish_mask,lottery_ticket_tuning,diff_pruning}.
However, sparse parameter updates present multiple engineering and efficiency challenges. Some of them have been tackled in recent research on parameter reconfiguration \cite{far_edge} (Section \ref{sec:far}) and NxM sparsity \cite{nxm_transformer}. Nevertheless, unrestricted unstructured sparsity is still impractical on contemporary hardware.

We discuss selective methods in detail in Section \ref{sec:selective_methods}.

\subsection{\rp eparametrization-based methods}
Reparametrization-based parameter-efficient fine-tuning methods leverage low-rank representations to minimize the number of trainable parameters. The notion that neural networks have low-dimensional representations has been widely explored in both empirical and theoretical analyses of deep learning \cite{parametercounting,measuring_the_intrinsic_dimension,Arora2018StrongerGB,malladi2022kernel}.

\citet{intrinsic_said} have demonstrated that fine-tuning can be performed effectively in low-rank subspaces. Furthermore, they showed that the size of the subspace that needs adaptation is smaller for larger models or models pre-trained for longer periods. Their approach, referred to as Intrinsic SAID (Section \ref{sec:intrinsic_said}), employs the Fastfood transform \cite{Le2013FastfoodAK} to reparametrize the update to neural network parameters. This inspired multiple methods, such as IntrinsicSAID, LoRA, and KronA (Sections \ref{sec:intrinsic_said}, \ref{sec:lora}, \ref{sec:krona}).

However, perhaps the most well-known reparametrization-based method is Low-Rank Adaptation or LoRA \cite{lora}, which employs a simple low-rank matrix decomposition to parametrize the weight update $\delta W = W^{\text{down}}W^{\text{up}}$. This approach is straightforward to implement and has been evaluated on models with up to 175 billion parameters. We provide a detailed discussion of this method in Section \ref{sec:lora}. More recent works \cite{compacter,krona} have also explored the use of Kronecker product reparametrization ($\delta W = A \otimes B$), which yields a more favorable tradeoff between rank and parameter count. Reparametrization-based methods have recently become widely popular, demonstrating their effectiveness on models up to 175B parameters \cite{lora}. Section \ref{sec:reparametrization_based_methods} discusses these methods in detail.

\subsection{Hybrid methods}
\label{sec:hybrid}
A number of methods combine ideas from multiple categories of PEFT \cite{parallel_adapter,sparse_adapter,unipelt,compacter}. This allows the use of different algorithmic tradeoffs to optimize for a specific goal, such as the number of trainable parameters. For instance, the MAM Adapter (Section \ref{sec:mam_adapter}) incorporates both Adapters and Prompt tuning. UniPELT (Section \ref{sec:unipelt}) adds LoRA to the mixture. Compacter and KronA$^B_{res}$ reparametrize the adapters using the Kronecker product to reduce their parameter count (Sections \ref{sec:compacter} and \ref{sec:krona}).
% Finally, S4 (Section \ref{sec:s4}) is a result of an automated algorithm search that combines all PEFT classes to maximize accuracy at $0.5\%$ of extra parameter count.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Efficiency table
\input{tables/efficiency_table_old}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Efficiency table footnote
% \footnotetext[5]{Depends on sparse operations hardware support.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Scale Table
\input{tables/scale_table_nocite}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Main section
\section{A Deep Dive into PEFT}
\label{sec:methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In the following sections, we dive into the details of various parameter-efficient fine-tuning approaches. We describe the distinctions and tradeoffs between them in terms of the dimensions outlined in Section \ref{sec:peft_comparison_theory}. We \textbf{bold} a one-sentence summary of each method to simplify skimming.

In the method description, we also indicate whether it has been applied to models with fewer than 1 billion, 20 billion, or more than 20 billion parameters. We stick to indicating parameter counts where possible because the words ``small'' and ``large'' change their meaning too quickly. For a summary, refer to Table \ref{tab:scale_table}. Finally, we provide a brief pseudo-PyTorch implementation of the most important part of the algorithm where feasible.

\section{Additive methods: Adapters}
\label{sec:additive_section_adapters}
We begin our exploration of PEFT methods with one of the largest sub-families: methods that add fully-connected networks between model layers, known as adapters.

\subsection{Adapters}
\label{sec:adapters}
% The idea of Adapters was initially developed for multi-domain image classification \cite{Rebuffi2017Adapters,Rebuffi2018EfficientPO} and consisted in adding domain-specific layers between neural network modules.

\citet{adapters} introduced the idea of adapters for NLP. Rebuffi et al. \cite{Rebuffi2017Adapters,Rebuffi2018EfficientPO} initially proposed similar concepts for image classification tasks, and Houlsby et al. extended this concept to NLP by proposing \textbf{the addition of fully-connected networks after attention and FFN layers} in Transformer. Unlike the transformer FFN block, Adapters usually have a smaller hidden dimension than the input. Adapters have demonstrated impressive parameter efficiency, showing that it is possible to achieve full fine-tuning performance by tuning less than 4\% of the total model parameters.

% A schematic adapter implementation:
\begin{lstlisting}
def transformer_block_with_adapter(x):
    residual = x
    x = SelfAttention(x)
    x = FFN(x)  
    x = Adapter(x)  # Adapter after FFN
    x = LN(x + residual)
\end{lstlisting}

\citet{adapter_fusion} found that inserting the adapter only after the self-attention layer (after normalization) achieves similar performance as using two adapters per transformer block. 

\subsection{AdaMix}

AdaMix \cite{adamix} improves the performance of adapters by \textbf{utilizing multiple adapters in a mixture-of-experts (MoE) fashion} \cite{moe}. This means each adapter layer consists of a set of layers (experts), and for each forward pass, only a small set of experts is activated. In contrast to a regular MoE, which selects and weights multiple experts using a routing network, AdaMix randomly selects a single expert for each forward pass to minimize computational costs.

To stabilize training, the authors propose consistency regularization, which minimizes the symmetrized KL divergence between two models' forward passes with different sets of experts selected. Another difference from a regular MoE layer is that up and down projections of the adapter are selected independently. After training, the adapter weights are averaged across the experts to reduce inference costs.

% Here we provide AdaMix pseudocode:
\begin{lstlisting}
def transformer_block_with_adamix(x):
    residual = x
    x = SelfAttention(x)
    x = LN(x + residual)
    residual = x
    x = FFN(x)
    # adamix starts here
    x = random_choice(experts_up)(x)
    x = nonlinearity(x)
    x = random_choice(experts_down)(x)
    x = LN(x + residual)
    return x

def consistency_regularization(x):
    logits1 = transformer_adamix(x)
    # second pass uses different experts
    logits2 = transformer_adamix(x)
    r = symmetrized_KL(logits1, logits2)
    return r
\end{lstlisting}

Although AdaMix achieves better performance than regular adapters with the same inference cost, it can use more memory during training. \citet{adamix} show that AdaMix can use much smaller adapter hidden states than regular adapters, which amortizes trainable parameter overhead over the number of experts ($\sim$4-8). However, consistency regularization increases computational memory requirements, as it needs to keep two versions of the hidden states and gradients over two forward passes with different experts.

% NOTE: AdapterFusion is not MoE, because it only learns to average adapter outputs, it doesn't route to them and has no sparse activation. 
% \citet{adapter_fusion} also combine multiple adapters in MoE-like fashion, but they focus on multi-task setting. In their approach, each adapter is trained for a specific task. After this, a separate step of adaptation is peformed which is to train a routing network that fuses the adapters. Their approach 

\section{Additive Methods: Soft Prompts}
\label{sec:additive_section_soft_prompts}

Prompting language models has demonstrated remarkable performance in zero- and few-shot scenarios \cite{brown2020language_gpt3,pet}. However, optimizing discrete natural language prompts or using in-context learning becomes impractical with many training examples. To overcome this challenge, the concept of ``soft'' or ``continuous'' prompts was proposed \cite{prefix_tuning,prompt_tuning,p_tuning}, converting the discrete optimization problem of finding the best "hard" prompt into a continuous one.

\subsection{Prompt Tuning}
\label{sec:prompt_tuning}

Prompt tuning \cite{prompt_tuning} proposes to \textbf{prepend the input embeddings}. These tensors are commonly referred to as ``soft prompts,'' and they are optimized directly through gradient descent.

% \begin{lstlisting}
% def soft_prompted_model(input_ids):
%     x = Embed(input_ids)
%     x = concat([soft_prompt, x], dim=seq)
%     return model(x)
% \end{lstlisting}

\begin{lstlisting}
def prompt_tuning_attention(input_ids):
    q = x @ W_q
    k = cat([s_k, x]) @ W_k  # prepend a
    v = cat([s_v, x]) @ W_v  # soft prompt
    return softmax(q @ k.T) @ V
\end{lstlisting}

% Soft prompts are incredibly parameter-efficient 
Ablation studies by \citet{promspt_mapping} on prompt length (1 to 150 tokens) and model size (10M to 11B parameters) reveal that prompt tuning becomes more parameter efficient as the model size increases. For instance, prompt tuning of T5-11B achieves the same SuperGLUE \cite{wang2019superglue} performance with either 5 or 150 soft prompt tokens.

Furthermore, efficiency increases more rapidly with model size. T5-large performance saturates at prompt length 20 or 20K trainable parameters ($0.002\%$), and T5-XL performance saturates at prompt length 5, also 20K trainable parameters ($0.0002\%$). However, prompt tuning only becomes comparable with full fine-tuning at the 10B model scale. Additionally, increasing sequence length by 20-100 tokens can significantly increase computation, given the quadratic complexity of the transformer. Overall, soft prompts are incredibly parameter-efficient but come with inference overhead and are more applicable to larger models.

\subsection{Prefix Tuning}
\label{sec:prefix_tuning}

\citet{prefix_tuning} independently develop the idea of soft prompts with a distinctive flavor: \textbf{shared trainable parameters are prepended to the hidden states of all layers}. The same prefix $\mathbf{P}_\theta \in \R^{l \times h}$ is prepended to all of the hidden states. They observe that directly optimizing the soft prompt leads to instabilities during training. Instead, soft prompts are parameterized through a feed-forward network $\mathbf{P}_\theta = \operatorname{FFN}(\hat{\mathbf{P}}_\theta)$. During training, $\hat{\mathbf{P}}_\theta$ and the parameters of the FFN are optimized. After training, only $\mathbf{P}_\theta$ is needed for inference, and the FFN can be discarded.

Pseudocode for a single layer:
\begin{lstlisting}
def transformer_block_for_prefix_tuning(x):
    soft_prompt = FFN(soft_prompt)
    x = concat([soft_prompt, x], dim=seq)
    return transformer_block(x)
\end{lstlisting}

Note that the approach is very similar to Prompt Tuning (Section \ref{sec:prompt_tuning}), but the soft prompts are added in each layer.

In their experiments, \citet{prefix_tuning} apply prefix tuning to the BART \cite{lewis2019bart} model (less than 1B parameters) for different generation tasks and show a performance close to full fine-tuning by training only $0.1\%$ of the parameters. Soft prompt lengths used in the study vary from 10 to 200 tokens.

\subsection{Adaptive Prefix Tuning}

\citet{zhang2023towards} extends prefix tuning and proposes adaptive changes to the prompt length. The authors achieve the \textbf{adaptive adjustment via gating and scaling of the input to each layer}. Specifically, the input to each transformer layer is first passed through an FFN ($W$) with sigmoid activation. The sigmoid-activated values act as gates for each pseudo token added in the soft prompt, providing a structure to adaptively select or deselect certain soft tokens in certain layers. Furthermore, the sigmoid-activated values are scaled with separate $\lambda$ parameters. $W$ and $\lambda$ are both learnable parameters and are separately defined for each layer in the language model. Hence, $W$ and $\lambda$ cause significant computation, memory, and storage overhead on top of the prefix-tuning approach \cite{prefix_tuning}.

\begin{lstlisting}
def adaptive_adjustment(x, layer_index):
    alpha = sigmoid(W[layer_index, ...] @ x)
    adaptive_factor = lambda_[layer_index] * alpha
    return adaptive_factor

def prefix_tuning(x, layer_index):
    # adaptive gating and scaling
    adaptive_factor = adaptive_adjustment(x, layer_index)
    
    # soft prompt
    soft_prompt = FFN(soft_prompt)
    soft_prompt *= adaptive_factor

    # regular prefix-tuning
    x = concat([soft_prompt, x], dim=seq)
    return transformer_block(x)
\end{lstlisting}

However, the overhead is shown to be beneficial in the BERT and DeBERTa series models. Adaptive prefix tuning consistently outperforms the standard prefix tuning approach and even surpasses the full fine-tuning performance in most cases.

\subsection{Intrinsic Prompt Tuning (IPT)}
Prompt tuning is slow to converge.
% Prompt tuning methods, despite being parameter efficient, present practical problems such as slow convergence and a need to store all of the task-specific parameters.
% Such challenges can increase the compute and storage expenses.
A few studies \cite{prompt_mapping,spot} have proposed pre-training soft prompts to improve performance and convergence speed. However, these methods do not provide solutions to reduce the number of parameters per task.

% \citet{ipt} hypothesize that the h-dimensional space used to define soft prompt parameters contains an ``intrinsic task subspace'' that can differentiate between various tasks.

\citet{ipt} hypothesize that the high-dimensional space used to define a soft prompt contains a low-dimensional ``intrinsic task subspace'' and \textbf{learn it using an autoencoder in a multi-task fashion}.

The IPT method works in three steps. First, given a set of training tasks, their soft prompts are learned in the standard way (Section \ref{sec:prompt_tuning}). Then, these prompts are used to train an autoencoder that compresses their dimensionality. After this, the encoder part is discarded, and only the input to the autoencoder decoder is trained on new tasks.

\begin{lstlisting}
def autoencoder(soft_prompt):
    soft_prompt = soft_prompt.flatten()
    P = FFN_A(soft_prompt)  # encoder
    P = FFN_B(P)            # decoder
    P = P.reshape(prompt_len, hidden)
    return P

def ipt_model(x):
    P = FFN_B(intrinsic_subspace_prompt)
    P = P.reshape(prompt_len, hidden)
    x = concat([P, x], dim=seq)
    return model(x)
\end{lstlisting}

Even though the IPT framework reduces the number of parameters for the unseen tasks, this reduction comes at the price of training the autoencoder. The authors conduct experiments with the BART-base model and a prompt length of 100. The resulting autoencoder, which is implemented\footnote{\href{https://github.com/thunlp/Intrinsic-Prompt-Tuning/blob/master/bartPrompt.py}{github.com/thunlp/Intrinsic-Prompt-Tuning}} as a fully-connected network that accepts a one-dimensional tensor of size $76800$, reaches 78 million parameters. This constitutes over 56\% of the total parameters in the BART-base model. Therefore, significantly more efficient methods of prompt autoencoding are required to make IPT practically applicable.

\section{Additive Methods: Other Approaches}
\label{sec:additive_section_others}

% Several of the additive PEFT methods do not follow the idea of either adapters or soft prompts and propose to augment a pre-trained network in an original way.

\subsection{Ladder-Side Tuning (LST)}
\label{sec:lst}

Ladder-Side Tuning \cite{ladder_side_tuning} \textbf{trains a small transformer network on the side of the pre-trained network}. This side network combines the hidden states of the pre-trained backbone network with its own hidden states.

This way, the side network only uses the pre-trained model as a feature extractor, and backpropagation must only be computed through the side network, saving on both memory and compute during training.
% The authors also use multiple tricks no improve the performance and parameter efficiency of LST. Namely, initializing the side network from the pre-trained model parameters using structural pruning and using twice (or 4x) fewer layers in the side network than in the backbone network.
To improve the performance and parameter efficiency of LST, the side network is initialized from the structurally pruned pre-trained model parameters and uses half as many layers.

Here \texttt{h\_pt} is the output of the corresponding layer of the pre-trained network, and \texttt{alpha} is an input-independent trainable scalar gate:


% The pseudocode:
% Pseudocode:
\begin{lstlisting}
def ladder_side_layer(x, h_pt):
    h_pt = h_pt @ W_down  # to x.shape
    gate = sigmoid(alpha)
    x = gate * x + (1 - gate) * h_pt
    return transformer_block(x)

def ladder_side_network(x):
    with no_grad():
        H_pt = pretrained_network(x, return_all_hiddens=True)
    for i in range(layers):
        layer = ladder_side_layers[i]
        x = layer(x, H_pt[i])
    return x
\end{lstlisting}

LST demonstrated a three-fold RAM reduction in fine-tuning T5-Base compared to full fine-tuning and a two-fold RAM usage reduction compared to LoRA (Section \ref{sec:lora}) with a small degradation in accuracy. Moreover, LST outperforms these methods when controlling for RAM usage.

\subsection{(IA)$^3$}
\label{sec:ia3}

\citet{t_few} proposes a new parameter-efficient method to multi-task fine-tune T0 \cite{t_few}. Their proposed fine-tuning method, (IA)$^3$, learns new parameters $l_v$, $l_k$, and $l_{ff}$, which \textbf{rescale key, value, and hidden FFN activations}. Specifically:

\begin{lstlisting}
def transformer_block_with_ia3(x):
    residual = x
    x = ia3_self_attention(x)
    x = LN(x + residual)
    residual = x
    x = x @ W_1         # FFN input
    x = l_ff * gelu(x)  # (IA)3 scaling
    x = x @ W_2         # FFN output
    x = LN(x + residual)
    return x

def ia3_self_attention(x):
    k, q, v = x @ W_k, x @ W_q, x @ W_v
    k = l_k * k
    v = l_v * v
    return softmax(q @ k.T) @ V
\end{lstlisting}

Training only these three vectors, $l_v$, $l_k$, and $l_{ff}$, for each transformer block leads to high parameter efficiency. For T0-3B, it only updates about $0.02\%$ of model parameters and outperforms other methods, including Compacter (Section \ref{sec:compacter}), which has a similar parameter count, and LoRA (Section \ref{sec:lora}), which has 16 times more trainable parameters. Unlike adapter-tuned models, (IA)$^3$-tuned models exhibit minimal overhead. Vectors $l_v$ and $l_k$ can be integrated into the corresponding linear layers, and the only overhead comes from $l_{ff}$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% SELECTIVE METHODS
\section{Selective Methods}
\label{sec:selective_methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Selective methods fine-tune a subset of the existing parameters of the model. It could be a layer depth-based selection, layer type-based selection, or even individual parameter selection.

\subsection{BitFit}
\label{sec:bitfit}

\citet{bitfit} proposes to \textbf{only fine-tune the biases of the network}. That is, for every layer, $W$ is unchanged and only $b$ is trained.

BitFit only updates about 0.05\% of the model parameters. The original paper demonstrated that the method achieves similar or better performance than full fine-tuning in low- and medium-data scenarios for BERT models (less than 1B parameters). Further research showed that for models larger than 1B parameters, BitFit significantly underperforms compared to full fine-tuning and other PEFT methods \cite{t_zero,t_few,lora}.
% At this scale, BitFit significantly underperforms full fine-tuning, and other PEFT approaches.

\begin{lstlisting}
  params = (p for n, p in model.named_parameters()
            if "bias" in n)
  optimizer = Optimizer(params)
\end{lstlisting}

\paragraph{Bias-less architectures} In our experimental comparison, we noticed that several popular architectures do not use bias terms in the network. For example, T5 only uses bias terms in the relative attention weights, and LLaMA does not use bias terms throughout the network.

% \subsection{Cross-Attention tuning}
% \red{
% Just fine-tuning attention. They succesfully applied to to machine translation domain adaptation.
% }

\subsection{DiffPruning}
\label{sec:diff_pruning}

DiffPruning \cite{diff_pruning} aims to achieve parameter efficiency by learning a sparse update of a neural network's weights. The method introduces a learnable binary mask on the weights, denoted by $\delta = z \circ \Delta W$, where $\circ$ represents the Hadamard product. This \textbf{parameter mask is learned during model fine-tuning} as part of the regularization objective, which is a differentiable approximation to the $L_0$ norm of the update vector $\delta$.

DiffPruning achieves comparable performance to full fine-tuning while modifying only 0.5\% of the model parameters in \textless1B scenarios. This makes it a useful method for multi-task deployment in edge (mobile) scenarios where storage is limited. However, this method requires more memory than traditional fine-tuning, as it involves optimizing all parameters during training in addition to the learnable binary mask.

\subsection{Freeze and Reconfigure (FAR)}
\label{sec:far}

FAR \cite{far_edge} \textbf{selects columns of parameter matrices to train and reconfigures linear layers into trainable and frozen components}. The method operates in two stages. In the first stage, the most important rows of parameter matrices are identified for updating. This process is similar to structured pruning and can use any pruning method. In their paper, the authors fine-tune the model on a percentage of the data and select the top-$r$ rows based on the $L_1$ distance between the fine-tuned and original models.

In the second stage, the network is reconfigured by splitting each parameter matrix $W \in \mathbb{R}^{in \times h}$ into a trainable component $W_{t} \in \mathbb{R}^{in \times h'}$ and a frozen component $W_{f} \in \mathbb{R}^{in \times (h - h')}$, where $h'$ is the desired number of trainable parameters. The matrix multiplications with $W_{t}$ and $W_{f}$ are computed independently, and the results are concatenated. A similar operation is performed on biases.
\vspace{1.5em}  % added to make sure that the code is not split

% Pseudocode implementation is rather simple
\begin{lstlisting}
def far_layer(x):
    h1 = x @ W_t
    h2 = x @ W_f
    return concat([h1, h2], dim=-1)
\end{lstlisting}

While this approach creates additional compute overhead during training, it provides great flexibility in terms of parameter selection on modern hardware using standard frameworks like PyTorch. After training, the parameters can be reconfigured back, removing any inference overhead.

The original paper focused on edge scenarios and used DistilBERT (66M parameters) for their experiments. It was only applied to feed-forward layers, as these make up the majority of the DistilBERT parameters. FAR achieved similar performance to fine-tuning on GLUE and SQuAD 2.0 \cite{squad2} while updating only 6\% of the parameters.

\subsection{FishMask}

FishMask \cite{fish_mask} is a \textbf{sparse fine-tuning method that selects the top-p parameters of the model based on their Fisher information}. Fisher information is estimated in a common way through a diagonal approximation.
\begin{equation*}
    \hat{F_{\theta}} = \frac{1}{N} \sum_{i = 1}^{N} \mathbb{E}_{y \sim p_{\theta}(y|x_i)}(\nabla_{\theta} \log p_{\theta}(y|x_i))^2
    \label{eqn:fisher_approximation}
\end{equation*}

Or in pseudocode:
\begin{lstlisting}
  sparsity = 0.99
  N = len(data)
  for x, y in data:
      loss = loss_fn(model(x), y)
      loss.backward()
      for n, p in model.named_params():
          fisher[n] += p.grad ** 2 / N
  
  threshold = percentile(fisher, sparsity)
  masks = {n: f > threshold
           for n, f in fisher.items()}
\end{lstlisting}

FishMask requires computing gradients for all parameters on several batches of the data. However, after the highest-Fisher parameters are selected, only they need to be optimized.

The method generally performs on par with adapters but sub-par to LoRA and (IA)$^3$ (Sections \ref{sec:lora} and \ref{sec:ia3}). It has been evaluated on BERT (less than 1B parameters) and T0-3B models. However, FishMask is computationally intensive and inefficient on contemporary deep learning hardware due to the lack of support for sparse operations.

\subsection{DiffFit}

\citet{xie2023difffit} combines various selective approaches with additive style scaling and specifically targets tuning of the diffusion model. In their study, \citet{xie2023difffit} \textbf{tunes the bias, layer normalization, and embedding parameters and adds new scaling parameters for residual mixing}. The scaling parameters are added separately for the self-attention and the FFN layers. The pseudocode for the scaling is as follows:

\begin{lstlisting}
def forward(x, c, t):
    x += gamma_1 * self_attn(x, c, t)
    x += gamma_2 * FFN(x, c, t)
    return x
\end{lstlisting}

The authors conduct experiments focused on diffusion models and show that DiffFit outperforms other PEFT approaches. In addition, due to the minor overhead of the scaling parameters, DiffFit provides faster tuning speed and minimal degradation in inference and storage efficiency.

\section{Reparametrization-based methods}
\label{sec:reparametrization_based_methods}

These methods use the idea of reparametrizing the weights of the network using a low-rank transformation. This decreases the trainable parameter count while still allowing the method to work with high-dimensional matrices, such as the pre-trained parameters of the networks.

\subsection{Intrinsic SAID}
\label{sec:intrinsic_said}

In their work, \citet{intrinsic_said} investigates the intrinsic dimensionality of fine-tuning and demonstrates that this process can be performed in a low-rank subspace. Specifically, they use the \textbf{Fastfood transform to reparametrize the update to the model weights}. Fastfood is a compute-efficient dimensionality expansion transform $F:\R^d \rightarrow \R^D$ that can be done in~$O(D \log d)$ time and $O(D)$ memory.

They show that larger models require changes in a lower-rank subspace compared to smaller models to achieve the same fine-tuning performance. This observation motivates both scaling large models and parameter-efficient fine-tuning. It is important to note that, unlike methods that select a particular subset of parameters for fine-tuning, Intrinsic SAID updates all model parameters in a low-rank manner, i.e., $\theta = \theta_0 + F(\theta^d)$, where $\theta_0 \in \R^D$ denotes the pre-trained model parameters and $\theta^d \in \R^d$ denotes the parameters to be optimized. Therefore, while the number of optimizable parameters is low, the $O(D)$ memory complexity of Fastfood and the update to all of the model's parameters make Intrinsic SAID impractical for fine-tuning large networks. For more details on Fastfood, we refer the reader to the original paper by \citet{Le2013FastfoodAK}.

% While IntrinsicSAID only updates a low-rank subspace of model's parameters, 
% \todo{I spent like 2 hours figuring out Fastfood and }
% Speficially, for parameters $\theta \in \R^D$, the update is paramtrized through $\theta \in \R^d$
% \begin{equation}
% \begin{aligned}
%     \delta \theta &= \hat{\theta}
%     V = H G \Pi H B \\
% \end{aligned}
% \end{equation}
% where $\theta^d \in \R^d$ are the optmizable parameters, $H$ - Hadamard matrix

\subsection{LoRA}
\label{sec:lora}

LoRA \cite{lora} takes inspiration from Intrinsic SAID and proposes a simpler way to perform low-rank fine-tuning. \textbf{Parameter update for a weight matrix in LoRA is decomposed into a product of two low-rank matrices:}
\begin{equation*}
\begin{aligned}
    % \delta W &= W^{\text{down}} W^{\text{up}} \\
    % W^{\text{down}} &\in \R^{\text{in} \times k}, W^{\text{up}} \in \R^{k \times \text{out}}\\
    % \delta W &= W_A W_B \\
    % W_A &\in \R^{\text{in} \times r}, W_B \in \R^{r \times \text{out}}\\
    \delta W = W_A W_B, \ W_A \in \R^{\text{in} \times r}, W_B \in \R^{r \times \text{out}}.\\
\end{aligned}
\end{equation*}
%% NOTE: we use r instead of k to make this consistent with Compacter

All pre-trained model parameters are kept frozen, and only $W_A$ and $W_B$ matrices are trainable. The scaling factor is constant and typically equals $\frac{1}{r}$. After training, they can be integrated into the original $W$ by just adding the matrix $W_A W_B$ to the original matrix $W$.

Pseudocode is very simple:
\begin{lstlisting}
def lora_linear(x):
    h = x @ W          # regular linear
    dh = x @ W_A @ W_B # low-rank update
    h += scale * dh    # scaling
    return h
\end{lstlisting}

In Transformers, LoRA is typically used for $W_K$ and $W_V$ projection matrices in multi-head attention modules. However, to achieve the best possible performance, it is best to apply LoRA to all weight matrices in the model \cite{dettmers2023qlora}. The method outperforms BitFit and Adapters and has been evaluated on models with up to 175B parameters.

% \subsection{DyLoRA}
%% NOTE: their results are inconclusive, LoRA performs on-par with DyLoRA when rank is 8. The biggest difference is with rank 1. I suggest not including it in v1.0 and take a second look at the paper.
% Automatic hparam selection for LoRA. Should we merge it with the LoRA section?

\subsection{KronA}
\label{sec:krona}
% KronA \cite{krona} replaces matrix factorization $\delta W=W_A W_B$ in LoRA (Section \ref{sec:lora}) with a \textbf{matrix factorization through a Kronecker product $\delta W = W_A \otimes W_B$}.
KronA \cite{krona} replaces matrix factorization $\delta W \mathrel{\mkern-10mu}=\mathrel{\mkern-10mu}W_A W_B$ in LoRA with a \textbf{matrix factorization through a Kronecker product $\delta W \mathrel{\mkern-5mu} = \mathrel{\mkern-5mu} W_A \otimes W_B$}.

This yields a better rank-to-parameters tradeoff because the Kronecker product preserves the rank of the original matrices being multiplied. Or, in other words, $\operatorname{rank}(A \otimes B) = \operatorname{rank}A \cdot \operatorname{rank}B$. Additionally, \citet{krona} uses an efficient Kronecker product-vector product operation $x (A \otimes B)$, which avoids representing $\delta W$ explicitly and leads to significant speedups. KronA$^B_{res}$, also presented in \citet{krona}, is a parallel adapter that uses Kronecker product parameterization of the weights and includes a residual connection.

Krona pseudocode:
\begin{lstlisting}
def krona_linear(x):
    x = x @ W  # regular linear
    x += kronecker_vector_prod(x, W_A, W_B)
    return scale * x

# same as x @ kronecker_product(A, B)
def kronecker_vector_prod(x, A, B):
    x = x.reshape(A.shape[1], B.shape[1])
    x = A.T @ x @ B
    return x.reshape(-1)
\end{lstlisting}

On GLUE, KronA methods perform on par or better than adapters (Section \ref{sec:adapters}), LoRA (Section \ref{sec:lora}), and Compacter (Section \ref{sec:compacter}) at the same trainable parameter count of $0.07\%$, while being significantly faster than adapters or Compacter at inference time. The method was evaluated only on small models (less than 1B parameters).

\paragraph{Background: Kronecker product}
Kronecker product is a tensor operation defined as
\begin{equation}
\begin{aligned}
\mathbf{A} \otimes \mathbf{B} &: \R^{n \times m} \times \R^{k \times l} \rightarrow \R^{nk \times ml}\\
\mathbf{A} \otimes \mathbf{B} &= 
    \begin{bmatrix}
    a_{1,1} \mathbf{B} & \cdots & a_{1,n} \mathbf{B} \\
    \vdots & \ddots & \vdots \\
    a_{m,1} \mathbf{B} & \cdots & a_{m,n} \mathbf{B}
    \end{bmatrix}
\end{aligned}
\end{equation}

\vspace{1em}

% and it can be implemented in
It can be easily implemented \footnote{Source: \url{github.com/rabeehk/compacter}} in PyTorch using the command {\small{ \texttt{torch.einsum} }}
% the following way
\begin{lstlisting}
def batched_kronecker_product(a, b):
    bs, i, j = a.shape
    bs, k, m = b.shape
    res = einsum("bij,bkm->bikjm", a, b)
    return res.view(bs, i * k, j * m)
\end{lstlisting}

\subsection{DoRA}
In the study conducted by \citet{liu2024dora}, the authors examine the differences between full fine-tuning and LoRA fine-tuning. Particularly, the authors analyze the change in direction ($\Delta D$) and magnitude ($\Delta M$) of the weight matrices, comparing pre- and post-full fine-tuning checkpoints. With values of $\Delta D$ and $\Delta M$ calculated for various checkpoints and multiple layers, the authors show the existence of an inverse relationship between $\Delta D$ and $\Delta M$ for full fine-tuning. However, for LoRA tuning, the $\Delta D$ and $\Delta M$ are directly proportional to each other. Hence, the authors propose DoRA, a method that \textbf{decouples magnitude from the rest of the weight update}, similar to weight normalization \cite{weight_normalization}. The implementation of DoRA is as follows:

% https://github.com/catid/dora/blob/main/dora.py#L32
\begin{lstlisting}
def initialize_m(W):
    # learnable scaling factor (magnitude component)
    m = Parameter(l2_norm(W), requires_grad=True)
    return m

def dora_linear(x):
    W_cur = W + (W_A @ W_B)
    v = W_cur / l2_norm(W_cur)  # normalize weight
    W_cur = m * v               # mult. by learnable magnitude

    # regular forward pass
    return x @ W_cur + b
\end{lstlisting}

The authors theoretically show that with the proposed scaling, an inverse relationship between $\Delta D$ and $\Delta M$ can be achieved with LoRA-style fine-tuning. In other words, DoRA can achieve either large directional changes with small adjustments to the magnitude, or vice versa. Experiments conducted on various tasks and models show consistent improvements in DoRA over LoRA. Interestingly, DoRA improves sample efficiency on instruction tuning tasks and shows significant performance improvements in the low-rank (4, 8) regime over LoRA. 

\begin{comment}
\subsection{SLoRA}
In LoRA, the parameters of the $W_A$ matrix are initialized to a Gaussian distribution, and the $W_B$ matrix is initialized to zeros. \citet{babakniya2023slora} argue that in a Federated Learning setting, where more diverse data is common, such initialization may cause LoRA performance to degrade compared to full-tuning. Hence, the authors propose to include a warm-up period in LoRA that can provide data-guided weight initialization. The authors experiment with two types of warm-ups: a full fine-tuning warm-up (F-LoRA) and a sparse fine-tuning warm-up (S-LoRA). This warm-up period is referred as stage-1 in the study and stage is regular LoRA fine-tuning. 


\begin{lstlisting}
# stage 1: training with sparse fine-tuning
w_s1 = w_pt
for round in rounds:
    for client in clients:
        w_client = train(w_s1, sparse_mask)
    w_s1 = aggregate([w_0, w_1, ..., w_clients]

# Calculate delta and decompose
delta_w = w_s1 - w_0
w_a, w_b = SVD(delta_w, r)

# stage 2: LoRA fine-tuning
w_s2 = lora_train(w_pt, w_a, w_b, r)
\end{lstlisting}

Experiments with ALBERT and DistilBERT highlight the considerable training time reductions in stage-2, 40 minutes for SLoRA compared to 49 minutes for LoRA, and 596 minutes for full-tuning. Overall, the importance of a warm-up-based weight initialization is highlighted in this study, especially for the federated learning cases.
\end{comment}


\subsection{GLoRA}
In LoRA, the pre-trained weights are varied only through addition. While effective, more difficult tasks might benefit from further scaling and activation. Following the same hypothesis, \citet{chavan2023one} proposes a variation of LoRA called GLoRA (short for Generalized-LoRA). In GLoRA, the authors improve the capability of LoRA at the cost of \textbf{more learnable parameters that scale and shift parameters or activations or both}. However, all learnable parameters of GLoRA are merged back into the frozen model, hence incurring no additional inference cost. The GLoRA update can be written as follows: 

\begin{equation*}
    f(x) = (W_{0} + W_{0}A + B)x + CW_{0} + Db_{0} + E + b_0 
\end{equation*}

Where, $W_0$ and $b_0$ are pre-trained parameters and are kept frozen during the fine-tuning process. The GLoRA parameters, $A, B, C, D$, and $E$ are learnable and are tuned during fine-tuning. In the above expression, the matrix $B$ replicates the LoRA update. All other learnable parameters i.e., $A, C, D$, and $E$ hence, add an overhead for fine-tuning compared to LoRA. However, to reduce this overhead, matrices $A, B$ and $C$ can be implemented with low-rank approximation i.e., $A = A_{in} \cdot A_{out}$ such that $A_{in} \in \mathbb{R}^{d_1 \times r}$ and $A_{out} \in \mathbb{R}^{r \times d_2}$, for $A \in \mathbb{R}^{d_1 \times d_2}$. The authors also specify that all new parameters ($A$ to $E$) can be converted into vectors or scalars to manage the computational budget.  

\begin{lstlisting}
def glora_linear(x):
    # adjustment to the input
    W_cur = W_0 + AW_0 + B 
    h = x @ W_cur + CW_0

    # add bias (mostly not present in recent transformer LLMs)
    if bias_present:
        h += Db_0 + E + b_0

    return h
\end{lstlisting}

In the above GLoRA update, all learnable parameters (A, B, C, D, and E) can merge back into the original model after fine-tuning. 
%Among all learnable parameters, A, B, and C can be low-rank adapted. 
Across various suites of vision and language tasks, the authors show that GLoRA outperforms LoRA. When the number of trainable parameters is held constant for both LoRA and GLoRA, GLoRA outperforms LoRA on the VTAB-1K benchmark. Additionally, GLoRA shows better sample efficiency than LoRA and other PEFT methods with a comparable number of trainable parameters.

\subsection{AdaLoRA}

In AdaLoRA, Zhang et al. \cite{zhang2023adaptive} introduce an adaptive method to reduce the ranks of the $W_A$ and $W_B$ matrices in LoRA. \textbf{They reformulate the LoRA update in an SVD-like format}: \begin{equation*} W = W + W_A\ \Lambda\ W_B \end{equation*} where $\Lambda$ is a diagonal matrix representing singular values. To adjust the rank, the authors prune $\Lambda$, affecting the dimensions of $W_A$ and $W_B$ based on importance scores that reflect the impact on loss. These scores are smoothed exponentially to guide parameter elimination. The method includes regularization to preserve orthogonality between $W_A$ and $W_B$. Their experiments with DeBERTa-base and BART-large demonstrate that AdaLoRA outperforms LoRA and other parameter-efficient fine-tuning (PEFT) methods under a parameter budget constraint. The study highlights that the initial transformer layers do not need high-rank updates. However, it requires tracking additional variables (namely $I, \Bar{I}, \Bar{U}$, and $S$, refer to \cite{zhang2023adaptive}) to prune $\Lambda$ values. This can induce significant memory overhead as the language model size or the rank is scaled. Nonetheless, AdaLoRA presents insightful findings that motivate the need to develop a selective ranking strategy for LoRA that can function without significant memory overhead. We present a simplified pseudocode of the main operations in AdaLoRA as follows,


%The ranks of the $W_A$ and $W_B$ matrices remain constant throughout the fine-tuning process in LoRA. In AdaLoRA \cite{zhang2023adaptive}, the authors propose a method that can adaptively reduce the ranks of the $W_A$ and $W_B$ matrices. For rank adaptation, the authors first reformulate the LoRA update to resemble the Singular Value Decomposition (SVD) format as follows:

%\begin{equation*}
%    W = W + W_A\ \Lambda\ W_B
%\end{equation*}

%Where $\Lambda$ is a diagonal matrix, equivalent to the singular value matrix. To control the rank of the update, the authors propose to prune the diagonal values in $\Lambda$, consequently pruning rows in $W_A$ and columns in $W_B$. The pruning of $\Lambda$ values is guided by importance scores calculated for all elements of $\Lambda$ as well as $W_A$ and $W_B$. The importance scores primarily calculate the effect of eliminating specific parameters on the loss (i.e., $ I(w_i) = | w_i \Delta_{w_i} \text{Loss} |$) and are smoothed exponentially (i.e., $I_{\text{estimate}} = \beta I_{\text{estimate}} + (1 - \beta) I_{\text{sample}}$). Lastly, the authors propose regularization to maintain an orthogonal relationship between $W_A$ and $W_B$. The pseudocode can be represented as follows:


\begin{comment}
\begin{lstlisting}
def score(w, dw):
    # change in loss wrt w
    sample = abs(w * dw)
    estimate = beta_1 * estimate + (1 - beta_1) * sample

    # variation in the i
    u = beta_2 * u + (1 - beta_2) * (estimate - sample)

    # importance score for W
    s = estimate * u
    return s

def importance_score(W_A, W_B, lambda_, dW_A, dW_B, dlambda_):
    s_a = score(W_A, dW_a)
    s_b = score(W_B, dW_b)
    s_lambda = score(lambda_, dlambda_)
    s = s_lambda + s_a.sum(dim=1) + s_b.sum(dim=0)
    return s

def prune(W_A, ..., dW_A, ..., k):
    # calculate importance score
    s = importance_score(W_A, ..., dW_A, ...)

    # find bottom-k values
    indices = find_bottom_k(s, k)

    # prune
    W_A[indices, :] = W_A[indices, :].detach()
    W_B[:, indices] = W_A[:, indices].detach()
    lambda_[indices] = 0
    lambda_[indices] = lambda_[indices].detach()
    return W_A, W_B, lambda_

def ada_lora_linear(x):
    h = x @ W                    # regular linear
    h += x @ W_A @ lambda_ @ W_B # low-rank update
    return h

def train():
    ...
    # mini-batch forward pass
    # calculate dW_A, dW_B, dlambda_
    # compute the importance score
    # update W_A, W_B
    # prune and update lambda_
    ...
    return
\end{lstlisting}
\end{comment}

\begin{lstlisting}
def prune(W_A, ..., dW_A, ..., k):
    # calculate the importance score
    s = importance_score(W_A, ..., dW_A, ...)

    # find bottom-k values
    indices = find_bottom_k(s, k)

    # prune
    W_A[indices, :] = W_A[indices, :].detach()
    W_B[:, indices] = W_A[:, indices].detach()
    lambda_[indices] = 0
    lambda_[indices] = lambda_[indices].detach()
    return W_A, W_B, lambda_

def ada_lora_linear(x):
    h = x @ W                    # regular linear
    h += x @ W_A @ lambda_ @ W_B # low-rank update
    return h

\end{lstlisting}


%Through experiments conducted with DeBERTa-base and BART-large, the authors show that AdaLoRA achieves better performance than LoRA and other PEFT methods, given a fixed budget for the number of trainable parameters. In addition, when DeBERTa-base is fine-tuned with AdaLoRA, the initial transformer layers in the model do not require a high-rank update. However, for every additional trainable parameter in AdaLoRA, four new variables need to be tracked, namely $I, \Bar{I}, \Bar{U}$, and $S$, to prune $\Lambda$ values. This can induce significant memory overhead as the language model size or the rank upper bound for AdaLoRA is scaled. Nonetheless, AdaLoRA presents insightful findings that motivate the need to develop a selective ranking strategy for LoRA that can function without significant memory overhead.

\subsection{GaLore}

%In PEFT methods, the goal is to reduce the number of trainable parameters so that the overall memory consumption is reduced by a large margin. However, reducing the number of parameters significantly hampers the expressivity. In other words, if the optimal weights for a language modeling problem are high-rank then, reducing the rank in re-parameterization methods will introduce a performance degrading bias. In a study conducted by \citet{zhao2024galore}, the authors propose a methodology to tackle this challenge. The authors of \citet{zhao2024galore} proposed a method, GaLore, for parameter-efficient tuning of language models, primarily focusing on the pre-training phase. The authors highlight and leverage two theoretical properties of gradients. First, the gradients become low-rank during training and reach a rank of one if certain conditions are met. Second, the gradients take the form $G = A - BWC$ for reversible networks trained with softmax-loss (e.g. language models).

%Based on the above theoretical properties, the authors hypothesize and theoretically show that if the gradient statistics (first- and second-order momentum) are represented with a low-rank approximation then, the optimization process converges with significant memory improvements. In essence, the number of parameters are not reduced in GaLore. The memory benefits are achieved by reducing the memory requirements of the gradient statistics. To achieve this, the authors project original gradients ($G_t$) into lower dimensional space ($R_t$). In this lower dimensional space, gradient statistics ($M_t, V_t$) are tracked and the gradients are regularized ($N_t$). The authors then project the regularized gradients back to the original full-rank space ($\Tilde{G}_t$) and update the parameters. The transition between high and low-dimensional spaces is guided by the SVD of the original high-dimensional gradients ($G_t = USV$). 


%For fine-tuning methods, the primary source of reducing the computation load is to reduce the number of trainable parameters. Such a reduction in computation load comes at the price of reduced expressivity due to fewer learnable parameters. 

Usually PEFT methods improve computational efficiency by reducing the number of trainable parameters at the cost of lowering the expressivity. \citet{zhao2024galore} propose to \textbf{train all model's parameters, but using low-rank regularization of gradients}. 
%Storing low-rank optimizer state bridges the memory consumption gap between GaLore and LoRA.
In essence, GaLore makes full-tuning efficient by lowering the memory consumption of optimizer states (first and second momentum).
The authors additionally provide a theoretical justification of the method.

% \citet{zhao2024galore} investigate the reduction in computation load without reducing the number of learnable parameters. With a theoretical analysis, the authors show that the gradients become low-rank during training. Extending this theoretical analysis the authors prove that if the gradient statistics (first- and second-order momentum) are represented with a low-rank approximation then, the optimization process converges with significant memory benefits.

Based on the theoretical analysis, the authors propose GaLore with the following salient properties: 1. No low-rank approximation of parameter matrices; 2. Decomposition of gradients in lower dimensional space; 3. Gradient regularization in lower dimensional space; 4. Projection of the regularized gradient in the original space and updating parameters. GaLore achieves a reduction in memory benefits by reducing storage requirements for the gradient statistics (first- and second-order momentum) and by making the gradient regularization leaner. 

\begin{lstlisting}
def galore_update(w, grad, lr):
    u, s, v = SVD(grad)
    m, n = u.shape[-1], v.shape[0]
    p = u if m < n else v
    grad_ = p.T @ grad
    # update optimizer momentum
    m, v = update_momentum(grad_)
    # regularize grad
    g = m / (sqrt(v) + eps)
    # project back
    grad = alpha * (p @ g)
    return  w - lr * grad  # param update

def update_momentum(grad_):
    # first order
    m = beta_1 * m + (1 - beta_1) * grad_
    m \= (1 - beta_1**t)
    # second order
    v = beta_2 * m + (1 - beta_2) * grad_**2
    v \= (1 - beta_2**t)
    return m, v
\end{lstlisting}

The authors evaluate the proposed method for pre-training as well as fine-tuning. On the pre-training end, GaLore achieves better validation perplexity on C4 data compared to LoRA \cite{lora} and ReLoRA \cite{relora} for a range of model sizes varying from 60M to 1B. Compared to full-rank pre-training GaLore achieves comparable but slightly worse perplexity values while significantly reducing memory usage. For the fine-tuning end, the authors fine-tuned RoBERTa-base on GLUE tasks and highlighted the benefit of GaLore over LoRA.

\subsection{Quantization and LoRA}
In LoRA, the language model is held frozen in full precision. Due to the increasing size of language models, the frozen language model causes a significantly large memory footprint. Hence, recent studies have focused on reducing the memory load of backbone LLMs by reducing the precision of the backbone LLM parameters.

A study conducted by \citet{dettmers2023qlora} addresses this issue with QLoRA. The authors achieve memory benefits in three ways. First, the authors introduce a 4-bit NormalFloat quantization method that reduces the memory required to store the backbone model. Second, the authors reduce the memory footprint of quantization constants by introducing double quantization, i.e., quantization of the quantization constants. Both of the first two steps realize significant memory gains in storing the backbone model on GPU. Lastly, the authors implement a paging mechanism between the CPU and GPU for optimizer states, offloading optimizer states to the CPU in case of memory spikes. With extensive experiments, the authors highlight the performance benefits of their proposed quantization approach.

However, post-training quantization of the final LoRA-tuned model often results in performance discrepancies \cite{xu2023qa}. In another study, \citet{xu2023qa} presents a group-wise quantization strategy that overcomes the need for post-training quantization, hence leading to better performance, even in lower precision regimes. More studies have proposed better quantization-aware fine-tuning methods in recent literature, e.g., IR-QLoRA \cite{qin2024accurate}, LoftQ \cite{li2023loftq}, L4Q \cite{jeon2024l4q}, LQ-LoRA \cite{guo2023lq}.

% \subsection{Papers not added}

% I do not understand the benefit of LoRAPrune. They are pruning W_a and W_b and for doing so importance score are carried for each LoRA weight. On top of that, a gate M also needs to be carries to do the final pruning. M is of size [d, d]. So, lot of memory overhead at the cost of performance reduction. They also have not compared performance with LoRA. 

% IncreLoRA: paper is not detailed enough. It looks like it is just a arxiv pre-print and I don't know if it is published anywhere. Official repository also does not have any citation mentioned. 

% layer pruning + lora: evaluation metrics are limited to rouge/bleu etc. No objective metric of evaluation. The evaluation is also limited to the healthcare dataset, where it is more important to evaluate it objectively. 

\section{Hybrid Approaches}
\label{sec:hybrid_section}

% Hybrid methods for parameter-efficient fine-tuning combine different techniques and strategies to achieve better performance while reducing the computational costs associated with fine-tuning large neural networks. These methods can be viewed as a synergistic blend of multiple approaches. The resulting hybrid methods can leverage the strengths of each individual technique, while mitigating their weaknesses, leading to improved performance and efficiency.

Hybrid methods leverage the strengths of various techniques while mitigating their weaknesses to improve the performance and efficiency of PEFT.

\subsection{SparseAdapter}
\label{sec:sparse_adapter}

\citet{sparse_adapter} propose a \textit{Large-Sparse} strategy to train adapter layers. In this strategy, they use a \textbf{large hidden dimension for the added module and prune around 40\% of the values at initialization}. \textit{Large-Sparse} consistently outperforms its non-sparse counterpart with the same trainable parameter count. However, training and inference costs can be higher depending on hardware support for sparse tensors and operations. It is also worth noting that computing the pruning mask for this method may require obtaining gradients for all newly added parameters.

\subsection{MAM Adapters}
\label{sec:mam_adapter}

In their study, \citet{parallel_adapter} thoroughly investigated adapter placement and soft prompts. They concluded that scaled parallel adapters outperform sequentially-placed adapters and that placing an adapter in parallel to FFN outperforms multi-head attention-parallel adapters.
% \footnote{While at first this might look contradicting the finding of \citet{adapter_fusion}, it actually supports it because the FFN-parallel adapter modifies the outputs of attention, just like the MHA-sequential adapter.}
They also noticed that soft prompts can efficiently modify attentions by only changing 0.1\% of the parameters and propose to 'mix-and-match' (MAM) these ideas. Their final model, \textbf{MAM Adapter, is a combination of scaled parallel adapter for the FFN layer and soft prompt}.
\begin{lstlisting}
def transformer_block_mam(x):
    x = concat([x, soft_prompt], dim=seq)
    residual = x
    x = SelfAttention(x)
    x = LN(x + residual)
    x_a = FFN(x) # parallel adapter
    x_a = scale * x_a
    x = LN(x + x_adapter)
    return x
\end{lstlisting}
The MAM method outperforms BitFit and PromptTuning by a large margin and consistently outperforms LoRA (Section \ref{sec:lora}), Adapters (Section \ref{sec:adapters}), and Prefix Tuning (Section \ref{sec:prefix_tuning}) with a 200 soft prompt length and 7\% extra parameters. The experiments were conducted on models with fewer than 1 billion parameters.

It is worth noting that parallel adapters were independently studied by \citet{parallel_adapter2} in the domain of machine translation.

% \subsection{Meta-Adapters}
% Use adapters, but pre-train parts of them them with MAML-like algorithm. After pre-training, do not fine-tune meta-adapters and only tune the regular adapter part. So its kind of tricky, but 4 and 8-shot improvement over AdapterFusion is very large

% \subsection{PALs}
% \red{
% Parallel attention layers \cite{bert_and_pals} are MHA adapters parallel to MHA. Adds 13\% parameters, but seems to work well with BERT. No experiments beyond 1B, because its an old paper.
% }

% \subsection{LeTS}
% \red{
% A side-tuning method that uses (OMG) LSTMs and NAS to select the input to each attention layer and the final pooling layer, early-stage pruning.
% }

\subsection{UniPELT}
\label{sec:unipelt}
UniPELT \cite{unipelt} is a \textbf{gated combination of LoRA, Prefix-tuning, and Adapters}. LoRA reparametrization is used for $W_Q$ and $W_V$ attention matrices, prefix-tuning is applied to keys and values of each layer, and adapters are added after the feed-forward layer of the transformer block. For each of the modules, gating is implemented as a linear layer projecting the module input into a dimension of size one, sigmoid activation, and averaging the resulting vector over the sequence length. Trainable parameters include LoRA matrices $W_A, W_B$, prompt tuning parameters $P_q, P_k$, adapter parameters, and gating function weights.

Schematic implementation of UniPELT (omitting attention heads for simplicity):
\begin{lstlisting}
def transformer_block_with_unipelt(x):
    residual = x
    x = unipelt_self_attention(x)
    x = LN(x + residual)
    residual = x
    x = FFN(x)
    adapter_gate = gate(x)
    x = adapter_gate * FFN(x)
    x = LN(x + residual)
    return x

def unipelt_self_attention(x):
    k, q, v = x @ W_k, x @ W_q, x @ W_v
    # lora for queries and values
    lora_gate = gate(x)
    q += lora_gate * W_qA @ W_aB
    v += lora_gate * W_vA @ W_vB
    # prefix tuning
    pt_gate = gate(x)
    q_prefix = pt_gate * P_q
    k_prefix = pt_gate * P_k
    return softmax(q @ k.T) @ V

def gate(x):
    x = Linear(x)
    x = sigmoid(x)
    return mean(x, dim=seq)
\end{lstlisting}

UniPELT demonstrates significant improvements over individual LoRA, Adapters, and Prefix Tuning approaches in low-data scenarios with only 100 examples. In higher data scenarios, UniPELT performs on par or better than these approaches. \citet{unipelt} reports that UniPELT uses 1.3\% trainable model parameters on BERT models with fewer than 1 billion parameters.

\subsection{Compacter}
\label{sec:compacter}
Compacter \cite{compacter} \textbf{utilizes the Kronecker product, low-rank matrices, and parameter sharing across layers to produce adapter weights}. Each parameter $W$ in an adapter is equal to the sum of Kronecker products
\begin{equation}
\begin{aligned}
    \hat W &= \sum_{i=0}^{n} A_i \otimes B_i \\
    \hat W \in \R^{k \times d}, \text{ } &A_i \in \R^{n \times n}, \text{ } B_i \in \R^{\frac{k}{n} \times \frac{d}{n}}.
\end{aligned}
\end{equation}
A linear layer $x \hat W + b$ with this parameterization is called a parametrized hypercomplex multiplication (PHM) layer \cite{pha}. Compacter takes this idea further, parametrizing $B_i$ similar to LoRA (Section \ref{sec:lora}) as $B_i = B_i^{\text{down}} B_i^{\text{up}}$, where all matrices are of rank at most $r$. Matrices $A_i$ are shared across all adapter layers for further parameter efficiency. The corresponding layer is referred to as Low-rank PHM (LPHM).
Note that all $A_i$ and $B_i$ tensors are 3D tensors with the first dimension equal to $n$, the number of Kronecker products in the PHM layer.
Compacter layer pseudocode:
\begin{lstlisting}
def compacter(x):
    x = LPHM(x)  # Essentially an FFN
    x = gelu(x)  # but
    x = LPHM(x)  # LPHM replaces linear
    return x

def lphm_forward(x):
    B = B_d @ B_u
    W = batched_kronecker_product(A, B)
    W = sum(W, dim=0)
    return x @ W + b
\end{lstlisting}


Compacter comes in two flavors: two adapters per transformer block or a single adapter after a feedforward layer (Compacter++). With only 0.05\% additional parameters, Compacter++ performs on par or better than adapters with 0.8\% additional parameters.
% \red{The paper also provides memory and epoch time comparison to other PEFT methods and demonstrates 42\% memory reduction and 26\% speedup compared to full fine-tuning.}
The model has been evaluated on T5 Base (less than 1B parameters) and T0-3B models.


\subsection{S4}
\label{sec:s4}
\citet{design_spaces} conducted a search of various \textbf{combinations of parameter-efficient fine-tuning techniques}. Their search space includes dividing consecutive layers into four uneven groups, allocating varying amounts of trainable parameters to each layer, determining which groups to fine-tune, and which PEFT methods to apply to each group.

Their proposed method, S4, divides layers into four groups ($G_{1,2,3,4}$) using a ``spindle'' pattern: more layers are allocated to the middle groups and fewer to the top and bottom groups. All groups are trainable, with trainable parameters uniformly allocated across the layers within each group. Different combinations of PEFT methods are applied to different groups. Specifically:
\begin{equation}
\begin{aligned}
G_1 &: A, L & G_3 &: A, P, B \\
G_2 &: A, P & G_4 &: P, B, L \\
\end{aligned}
\end{equation}

where A stands for Adapters (Section \ref{sec:adapters}), P for Prefix-Tuning (Section \ref{sec:prefix_tuning}), B for BitFit (Section \ref{sec:bitfit}), and L for LoRA (Section \ref{sec:lora}).

The search experiments were conducted on the T5-base model and the GLUE dataset with 0.5\% trainable parameters. The S4 method was then applied to T5-3B, RoBERTa, and XL-Net, consistently outperforming individual BitFit, Prefix Tuning, LoRA, and Adapters across different architectures, model sizes, and tasks.


\section{Comparison of PEFT Methods}
\label{sec:peft_comparison_theory}

Parameter efficiency involves multiple aspects: storage, memory, computation, and performance. However, achieving parameter efficiency alone does not necessarily lead to reduced RAM usage or faster training. When evaluating PEFT methods, it is important to consider various aspects of performance and efficiency, yet many publications focus solely on downstream performance and the number of parameters used by the PEFT method.

The ``number of parameters'' is a commonly reported metric in PEFT literature, but it can refer to different things: the number of \textbf{trainable parameters}, \textbf{changed parameters}, or the \textbf{rank of the update} (e.g., \citet{intrinsic_said}). In some cases, the number of trainable vs. changed parameters can differ by orders of magnitude. Examples include DiffPruning and LT-SFT \cite{diff_pruning,lottery_ticket_tuning}, which first fine-tune the full network and then prune the update afterward, or Prefix Tuning \cite{prefix_tuning}, which uses an FCN reparametrization of the prompt parameters.

We provide a detailed comparison of the trainable and updated parameters for 28 PEFT methods in Table \ref{tab:scale_table} and discuss this issue further in Section \ref{sec:best_practices}. Generally, sparse methods tend to have more trainable than changed parameters, while reparametrization methods often have fewer trainable parameters due to the nature of reparametrization.

In our study, we consider \textbf{five key dimensions} essential to benchmark the effectiveness of parameter-efficient fine-tuning methods. These dimensions include storage efficiency, memory efficiency, computational efficiency, inference overhead, and downstream performance metrics (e.g. accuracy). Our analysis of the published literature shows that while these dimensions are interconnected, improvements along one of the axes do not necessarily translate into improvements along the others. For example, optimizing for parameter efficiency alone does not guarantee reduced RAM usage. Table \ref{tab:efficiency} summarizes our findings.

Despite their significance, PEFT performance metrics such as memory efficiency, training speed, and inference overhead (e.g., throughput) are only occasionally quantified in the papers. However, presenting these metrics only helps to further analyze a particular PEFT method of interest, in isolation. Head-to-head comparison across different PEFT methods is still challenging, primarily because of the impact of experimental setup on performance metrics. Thus, to address this gap, we fix the experimental set-up and perform a \textbf{large-scale experimental comparison of PEFT methods} as a part of this survey. We discuss details of the experimental setup and our results in the following sections.

%Despite their significance for fine-tuning large models, metrics such as memory efficiency, training speed, and inference overhead (e.g., throughput) are only occasionally quantified in the papers analyzed in this study \cite{ladder_side_tuning,dettmers2023qlora}. When these metrics are reported, comparing results across different studies is challenging due to the need for identical models, datasets, hardware setups, and implementation details. This limitation is a key reason why an analysis of published literature doesn't fully illustrate the landscape of PEFT methods. In addition, inconsistencies in reporting, as discussed in Section \ref{sec:reporting_issues}, make it impossible to compare most of the methods head-to-head. Thus, we perform a \textbf{large-scale experimental comparison of PEFT methods} as a part of this survey.

\subsection{Experimental Comparison: Setup}
\label{sec:peft_comparison_setup}

Our experimental comparison is designed to provide a comprehensive evaluation of PEFT methods, exceeding the scope and depth of existing studies such as \citet{delta_tuning}. We have carefully selected 14 PEFT methods representing diverse categories within our taxonomy – including Additive, Selective, Reparametrization-based, and Hybrid methods – to ensure a broad and inclusive analysis. Notably, we exclude sparse selective methods, acknowledging their limited practicality on modern hardware and their primary focus on storage efficiency. Furthermore, our study omits BitFit in the context of T5 networks, which do not utilize biases. 
In addition to these PEFT methods, we include a full fine-tuning baseline to provide a reference point for both downstream task performance and efficiency metrics. Unlike \citet{delta_tuning}, which limited their experimental comparison to four PEFT methods and focused primarily on downstream performance and memory consumption, our experimental design covers a wider range of methods and evaluates them in all the following: memory efficiency, training speed, inference speed, and downstream performance.

\paragraph{Datasets}

We use both natural language understanding (NLU) and natural language generation (NLG) tasks for the comparison of the methods.

While the GLUE benchmark \cite{wang2018glue} is commonly used to evaluate parameter-efficient fine-tuning methods in the existing literature, many models now match or exceed human performance on GLUE tasks, some even with no or minimal training\footnote{\href{https://gluebenchmark.com/leaderboard}{gluebenchmark.com/leaderboard}}. This makes GLUE less effective at evaluating fine-tuning procedure performance. More recently, proposed alternatives to GLUE include MMLU \cite{hendrycks2021measuring}, HELM \cite{liang2023holistic}, and BigBench \cite{srivastava2022imitation}. MMLU emphasizes zero- or few-shot evaluations, making it unsuitable for assessing fine-tuning. Both HELM and BigBench present computational challenges due to their task diversity, especially when comparing a broad range of methods and models of up to 11B parameters.

In contrast, SuperGLUE tasks remain both demanding (with only a few models surpassing the human baseline) and computationally manageable. Specifically, we selected BoolQ, RTE, and COPA for this study. BoolQ is a yes/no question-answering dataset mainly evaluating models' world knowledge. COPA focuses on commonsense causal reasoning, for example, ``Premise: The man broke his toe. What was the CAUSE of this? Alternative 1: He got a hole in his sock. Alternative 2: He dropped a hammer on his foot.'' RTE is a natural language inference dataset where, given a premise, the model needs to predict if the hypothesis can be inferred from it, contradicts it, or is not relevant.

For more diverse comparisons, we also include one natural language generation dataset: CNN-Dailymail \cite{cnn_dailymail}, a large (300K training examples) summarization dataset. From the surveyed literature, we found that summarization usually highlights differences between PEFT methods and full fine-tuning, making this dataset particularly useful.

\paragraph{Models}

To compare parameter-efficient fine-tuning methods, we apply them to three sizes of T5: Large (0.7B), 3B, and 11B \cite{t5}. The range from 0.7B to 11B models not only tests each method's effectiveness at different scales but also presents common challenges associated with large-scale model training. A key aspect of this comparison is to demonstrate how PEFT methods can address practical issues such as memory constraints. For instance, the 11B model allows us to compare PEFT methods' performance and efficiency in one of the most relevant practical cases when full fine-tuning does not fit even into 80GB of GPU memory.

\paragraph{PEFT Methods}

We use the following fine-tuning methods in our comparison:

\begin{itemize}
\setlength\itemsep{0em}
    \item Full tuning -- regular fine-tuning of all model parameters
    \item Houlsby -- Adapters inserted after the attention and FCN layers of the Transformer as described in Section \ref{sec:additive_section_adapters} \cite{adapters}
    \item Pfeiffer -- Adapters inserted only after the FCN layers \cite{modular_deep_learning}
    \item Parallel Adapter -- scaled parallel adapter as described in \cite{parallel_adapter}
    \item IA3 -- (IA)$^3$ learns re-scaling vectors for keys, values, and hidden FFN activations (Section \ref{sec:ia3}) \cite{}
    \item Prefix Tuning -- learns a prefix added to keys and values and uses FCN reparametrization for these parameters (Section \ref{sec:prefix_tuning}) \cite{prefix_tuning}
    \item Prompt Tuning -- learns a prefix added to keys directly (Section \ref{sec:prompt_tuning}) \cite{prompt_tuning}
    \item LN tuning -- fine-tune only layer-norm parameters \cite{layer_norm_tuning}
    \item LoRA (q and v) -- LoRA applied to the query and value networks only (Section \ref{sec:lora}) \cite{lora}
    \item LoRA (all linear) -- LoRA applied to all linear layers in the model \cite{lora}
    \item KronA -- LoRA-like Kronecker product-based reparametrization of the weight matrices (Section \ref{sec:krona}) \cite{krona}
    \item MAM -- Mix-and-Match Adapters (Section \ref{sec:mam_adapter}) \cite{}
    \item Compacter -- Kronecker product-based reparametrization of Adapter layers as described in Section \ref{sec:compacter} \cite{compacter}
    \item Compacter++ -- Compacter layers that are only applied after the FCN in the Transformer, similar to the idea of Pfeiffer vs Houlsby Adapters \cite{}
    \item UniPELT -- a hybrid method that combines LoRA, Prefix Tuning, and Adapters through gating (Section \ref{sec:unipelt}) \cite{unipelt}
\end{itemize}

\paragraph{Metrics}

In our evaluation, we focus on assessing PEFT method efficiency in terms of memory consumption, training speed, and inference speed, and then compare models on downstream metrics.

To quantify memory efficiency, we track the maximum RAM consumption during training using \texttt{torch.cuda.max\_memory\_allocated()}. Training speed is quantified by the number of input tokens processed per second during training and for inference -- during evaluation. We do not explicitly merge reparametrization-based methods into model weights during evaluation to present results for a typical use-case when methods like LoRA are used in the adapter fashion. When merged, there should be no difference between reparametrization-based methods and regular training in terms of the inference speed. We use accuracy for SuperGLUE datasets and ROUGE-L for summarization.

\paragraph{Implementation details and hyperparameters}

All models are fine-tuned in text-to-text fashion following \citet{t5}. We use Adapters and PEFT libraries \cite{adapters_library_v2,peft_libarry} for most of the methods and implement several methods in our repository\footnote{\href{http://github.com/guitaricet/peft_comparison}{github.com/guitaricet/peft\_comparison}} from scratch. When using existing implementations, we utilize default architecture hyperparameters for the method from the corresponding library, which are usually close to the hyperparameters reported in the method's original paper.

For all NLU datasets, we perform a learning rate sweep over values \{1e-3, 1e-4, and 1e-5\} and select the best, which we then train on two more random seeds to estimate the standard deviation. Our preliminary experiments indicate a negligible impact of weight decay in our setup (<0.01). Due to computational constraints, CNN/Dailymail experiments only use one seed and a learning rate of 1e-3, which we found to perform consistently well across PEFT methods. We estimate the standard deviation of CNN/Dailymail runs via several random seeds on randomly selected PEFT methods and find it to be lower than 0.5 ROUGE-L points, which we use for the standard deviation estimate for the rest of the methods.

We use a maximum input sequence length of 512 tokens in all our experiments. In NLU experiments, the maximum output sequence length is 8, and for summarization, it is 128 tokens.

Each NLU model undergoes training for either 3 epochs or a minimum of 100 update steps. For CNN/Dailymail, the training duration is set to one epoch (9 thousand update steps). We use a batch size of 32 in all our experiments, utilizing gradient accumulation to achieve this batch size when needed. While all SuperGLUE tasks converge by the end of training in most of our experiments, we observe that CNN/Dailymail continues to improve throughout the training and does not plateau. Our setup thereby favors methods exhibiting faster learning, which is especially relevant for low-resource scenarios commonly faced in PEFT applications.

In total, we train three models of size 0.7-11B parameters with 14 PEFT methods, five runs for each of the three NLU datasets, and one for the summarization dataset. Together with the full fine-tuning baseline, this brings the total experiment count to around 700. We report raw (non-aggregated) results in Appendix \ref{sec:peft_comparison_raw}.

\begin{table}[t]
    \centering
    \begin{tabular}{l|l|c}
    \toprule
        Model & Datasets & Microbatch size  \\
    \midrule
        T5$_{LARGE}$ & RTE, COPA & 32 \\
        T5$_{LARGE}$ & BoolQ & 16 \\
        T5$_{LARGE}$ & CNN & 4 \\
        T5$_{3B}$    & RTE, COPA & 4 \\
        T5$_{3B}$ & BoolQ & 2 \\
        T5$_{3B}$ & CNN & 1 \\
        T5$_{11B}$   & RTE, COPA, BoolQ, CNN & 1 \\
    \bottomrule
    \end{tabular}
    \caption{Microbatch sizes used in our experiments}
    \label{tab:peft_comparison_microbatch}
\end{table}

\paragraph{Hardware setup}
We estimate throughput using a single A100 40GB GPU for most of the experiments, with several exceptions due to out-of-memory issues. UniPELT, MAM, and Prefix Tuning for T5-11B were trained with a single A100 80GB GPU, which should give comparable throughput numbers to the A100 40GB. Full fine-tuning T5-11B experiments were performed with two A100 80GB GPUs using model-parallel distributed training. RAM estimates for this model training are the total memory consumption of both GPUs, which should give an estimate comparable to the rest of the experiments, as optimizer states are not shared between GPUs in model-parallel training.

Table \ref{tab:peft_comparison_microbatch} specifies the number of examples processed simultaneously (microbatch size) in our experiments. Microbatch sizes are kept consistent across different tuning methods to enable fair comparison of memory efficiency. This also allows us to isolate the throughput improvements from the PEFT method itself, rather than increased batch size. Methods with lower memory consumption could further benefit from increased batch sizes.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\subsection{Comparison Results: Downstream Performance}
\label{sec:peft_comparison_downstream}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Table \ref{tab:peft_comparison_dataset_average_short} shows downstream metrics averaged over the datasets. Scores are averaged, and standard deviations are aggregated using the Euclidean mean of per-dataset variances. This table compares the downstream performance of PEFT methods across model scales. Non-aggregated results for all our experiments are available in Appendix \ref{sec:peft_comparison_raw}.

%We want to highlight the following observations:

We note a few key observations:

\paragraph{Houlsby Adapters and LoRA consistently perform the best}

Houlsby Adapters and LoRA are the only methods that consistently achieve full-tuning performance with little to no effort in hyperparameter tuning.

\paragraph{Hybrid methods are especially sensitive to hyperparameters}

MAM Adapters and UniPELT were consistently hard to train. While the results in Table \ref{tab:peft_comparison_dataset_average_short} include only the best model from our sweep over three learning rates, additional experiments to improve MAM and UniPELT only marginally improved their performance. We attribute this to the generally poor performance of Prompt Tuning when trained in a compute-limited scenario.

\input{tables/peft_comparison_dataset_average_short}

\paragraph{Prefix Tuning and Prompt Tuning significantly differ in performance}

Prefix Tuning \cite{prefix_tuning} and Prompt Tuning \cite{prompt_tuning} are two different PEFT methods that are easy to confuse in terms of naming and concept. Both methods use the idea of continuous prompt optimization, but Prefix Tuning reparametrizes the trainable prefix via a fully-connected network (Section~\ref{sec:prefix_tuning}). In contrast, Prompt Tuning directly optimizes the prefix, albeit at the cost of slower convergence and typically much larger prefix length.
% To clarify the difference between the methods we will use the terminology of Adapter Transformers library \cite{adapterhub} and call Prompt Tuning as Flat Prefix Tuning when appropriate.
We observe significant differences between these methods in our experiments. Both of them suffer from slow convergence, which substantially hurts performance in our setup. However, Prompt Tuning never outperformed the constant prediction baseline.\footnote{Since we use a text-to-text approach, results worse than the constant prediction are expected. A badly trained network will never generate a correct class name.} Additionally, Prompt Tuning was extremely sensitive to the random seed (especially for T5-large and 3B models), as observed by its high standard deviation from the mean.

% Finally, we notice that Prompt Tuning is consistently much slower than Prefix Tuning.

\paragraph{Multiple methods underperform their reported values}

Multiple methods that had claimed to outperform Adapters or LoRA (virtually all other methods) do not perform well in our setup. This includes most of the methods with the exception of Parallel Adapter, Compacter, and KronA, which perform on par with the best methods in several cases, especially for 11B models.

\paragraph{Pfeiffer Adapters Perform Significantly Worse Than Houlsby}

\citet{adapter_fusion} observes that inserting adapters only after the FCN in the Transformer achieves similar performance as inserting adapters after both FCN and Attention (MHA) layers. However, in our experiments we find a significant and consistent difference of up to 15 points that increases with model scale. This highlights the importance of evaluating methods for both small and large models.

\paragraph{Layer Norm Tuning is unexpectedly competitive}

Layer Norm parameters are rarely used for parameter-efficient fine-tuning; we found only a few mentions of the method \cite{layer_norm_tuning,t_few}. However, it can be implemented in one line of code and shows performance competitive to full fine-tuning for T5-Large and T5-11B. We want to highlight this result and recommend using LN tuning as a baseline for future PEFT work.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\subsection{Comparison Results: Efficiency}
\label{sec:peft_comparison_efficiency}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{tables/peft_comparison_dataset_average_tall_colored}
% \input{tables/peft_comparison_dataset_average_tall}

Table \ref{tab:peft_comparison_dataset_average_tall} presents a detailed comparison of efficiency and performance for the 14 PEFT methods compared in our study. We show the actual number of \textbf{trainable parameters} (as opposed to changed parameters), the maximum GPU memory consumption during training, and the throughput in ktok/s (thousands of tokens per second) both during training and inference.

\paragraph{All PEFT methods reduce memory consumption}

As expected, all methods from our study significantly reduce memory consumption. The smallest improvement we see is \textbf{4GB} in the UniPELT and T5-Large combination, which is quite considerable because it is 10\% of the GPU RAM. The biggest improvement is \textbf{71.5GB} in the Compacter++ and T5-11B combination. This allows fine-tuning T5-11B on a single 40GB GPU instead of two 80GB GPUs and dramatically improves training speed by a factor of more than two.

\paragraph{Smaller models (<1B) can train slower with PEFT}

Any PEFT (Parameter-Efficient Fine-Tuning) method that adds parameters to the network involves additional forward (and potentially backward) pass overhead. For sufficiently large models or when only a few parameters are added, this overhead can be negligible. However, if the method adds too many parameters, it can lead to \textbf{slower training compared to regular fine-tuning}. We observe this in T5-Large models, which are small only compared to billion-scale models, as they have 738M parameters\footnote{T5-Large is significantly bigger than BERT and RoBERTa models}. For instance, applying LoRA to all T5-Large parameters results in a 20\% training slowdown. Similar slowdowns are noted for MAM adapters, Compacter, and UniPELT, with 20\%, 5\%, and 40\% slower training, respectively, compared to full fine-tuning. Despite these slowdowns, they all offer memory improvements.

\paragraph{PEFT significantly affects inference speed}

In all PEFT methods that add trainable parameters to the network, we observe a significant slowdown in inference speed. The slowdown ranges from 33-55\% for T5-Large, 20-60\% for T5-3B, and 20-55\% for T5-11B (absolute points). Within the set of additive methods, we observe that Pfeiffer adapters and (IA)$^3$ offer the best inference speeds. 
%An exception is (IA)$^3$, which does not significantly affect inference speeds in the T5-3B and T5-11B models. 
It is important to note that in our throughput estimation for reparametrization-based methods, we did not merge the method parameters into the network. If merged, they would have the same inference speed as regular fine-tuning, as no additional parameters are present. However, methods like LoRA are increasingly used in modular approaches, such as referenced in \cite{lorahub}, without merging LoRA parameters. The results from Table \ref{tab:peft_comparison_dataset_average_tall} are relevant for these scenarios.

\paragraph{Kronecker-Based Reparametrizations Do Not Improve Memory Efficiency, But Improve Speed}

Across different model scales, we observe that extremely parameter-efficient methods like Compacter and KronA, which employ Kronecker products to enhance parameter efficiency, do not significantly reduce memory usage. Despite training with two orders of magnitude fewer parameters than LoRA, the memory consumption of Compacter and KronA is nearly identical to that of LoRA. For instance, LoRA optimizes 20 million parameters for T5-11B, while KronA and Compacter each optimize less than 0.5 million. Nevertheless, all methods consume approximately 28.6GB of GPU memory. This result becomes intuitive in hindsight: beyond a certain point, the memory used for optimizer states and gradients becomes negligible, overshadowed by other factors such as model weights and hidden states. Nevertheless, we observe significant training and inference speed improvements with KronA over LoRA. This likely occurs due to the efficient Kronecker-vector product implementation in KronA (Section \ref{sec:krona}).

% We also find several \textbf{contradictions to published results}. We expect them to be explained by either 1) model scale 2) our compute-restricted setup where we only train for 3 epochs (or 100 update steps) on SuperGLUE and 1 epoch on CNN/Dailymail 3) minimal hyperparameter tuning.

\textbf{In conclusion}, our experimental comparison shows several expected results, such as significant improvements in memory consumption and speed. However, we also observed some surprising results. Notably, we observed that methods like Layer Norm Tuning, which are often overlooked, can be unexpectedly effective. Additionally, the effects of various PEFT methods on inference speed, especially in larger models, highlight the complex trade-offs between efficiency and performance. These insights emphasize the need for a comprehensive evaluation of PEFT methods, taking into account not only memory and speed but also their scalability across different model sizes.


Based on our experiments, we note a few key take-away points for the practitioners. 

\begin{tcolorbox}[
    colback=gray!05,  % Background color (super faint green)
    colframe=black!50, % Border color (faint green)
    boxrule=0.5pt,     % Border thickness
    arc=3mm,           % Rounded corners
    width=\linewidth,  % Box width
    leftrule=0.1mm,      % Thickness of left border
    rightrule=0.1mm,     % Thickness of right border
    bottomrule=0.1mm,    % Thickness of bottom border
    toprule=0.1mm,       % Thickness of top border
]
\textbf{Key Findings: }
\begin{itemize}
    \item Houlsby Adapters \cite{adapters} and LoRA \cite{lora} perform at par or better than full-tuning, with little to no effort in hyperparameter tuning. 
    \item Layer Norm \cite{layer_norm_tuning} tuning provides a highly competitive and efficient method that is easy to implement. 
\end{itemize}

\end{tcolorbox}

% Comparing PEFT categories
\section{Challenges and guidelines}

Survey papers tend to discuss reporting issues, and this one is no exception. We identified several challenges and inconsistencies that make it difficult to evaluate PEFT methods and draw direct comparisons between different PEFT methods, which warrant discussion.

\paragraph{Reporting parameter count} 
One of the primary challenges stems from the difference in the way researchers report parameter counts. These inconsistencies arise from the inherent complexity of the problem. Parameter counts can be categorized into three types: the number of \textbf{trainable parameters}, the number of \textbf{changed parameters} between the original and fine-tuned models, and the \textbf{rank} of the difference between the original and fine-tuned models. These parameter counts are not equivalent. For example, IntrinsicSAID (Section \ref{sec:intrinsic_said}) learns a low-rank ($\sim$100-1000) transformation of model parameters. However, it changes all ($100\%$) of the model's parameters. DiffPruning (Section \ref{sec:diff_pruning}) learns an update of $0.5\%$ of the parameters, but it actually trains $200\%$ of the parameters: fine-tuning the model and learning the binary mask. For reparameterization-based methods (Sections \ref{sec:lora}, \ref{sec:krona}, \ref{sec:compacter}), memory requirements may vary depending on the implementation design choices. Of the three types, the number of trainable parameters is the most reliable predictor of memory efficiency. However, it is still imperfect: Ladder-side Tuning trains more parameters than LoRA or BitFit, but it uses less RAM by avoiding backpropagation to the main network. 

\begin{tcolorbox}[
    colback=gray!05,  % Background color (super faint green)
    colframe=black!50, % Border color (faint green)
    boxrule=0.5pt,     % Border thickness
    arc=3mm,           % Rounded corners
    width=\linewidth,  % Box width
    leftrule=0.1mm,      % Thickness of left border
    rightrule=0.1mm,     % Thickness of right border
    bottomrule=0.1mm,    % Thickness of bottom border
    toprule=0.1mm,       % Thickness of top border
]
\textbf{Guideline - 1}: Explicit reporting of the number of parameters and the type i.e. trainable parameters or changed parameters or rank of the changes to the model being tuned.
\end{tcolorbox}

\paragraph{Reporting efficiency}
Evaluating the efficiency of PEFT methods solely based on parameter count is challenging due to the non-linear relationship between parameter count and efficiency. 
Efficiency in training time is better assessed through memory consumption and training speed. Most PEFT categories, except for Sparse-selective methods, significantly improve RAM usage. However, the Intrinsic SAID \cite{intrinsic_said} method, which is Reparametrization-based, can result in higher memory usage than full training due to the Fastfood transformation's demands.
Our experiments revealed that modularity in hybrid PEFT (e.g., MAM adapters \cite{}, UniPELT \cite{unipelt}) methods comes at the cost of notably higher memory consumption. This emphasizes the need for studies to report memory consumption to help practitioners make informed decisions. We also noticed considerable variability in training speed even with similar RAM usage, suggesting that RAM consumption should be considered alongside training speed.
After training, the storage space required for the changed parameters is crucial for evaluating PEFT methods. Unlike full fine-tuning, which alters all model parameters, PEFT only requires saving a subset, significantly improving storage efficiency. However, methods like IPT require saving different parameter sets at various training stages, making clear reporting of space requirements essential.
Inference latency is another critical factor in practice. Additive methods typically introduce overhead because they require computations on both the original network and the added parameters, whereas Selective methods do not, as they operate on existing model weights. Moreover, additive and reparametrization-based methods (e.g., LoRA, KronA) offer advantages in multi-task inference by reducing memory usage from $O(NM)$ to $O(M + NA)$, where $A$ is the number of added weights per task. Some additive methods, like LST, can also enhance inference speed by using the original network solely as a feature extractor. For further details on multi-task training and inference, we refer readers to Modular Deep Learning \cite{modular_deep_learning}.

\begin{tcolorbox}[
    colback=gray!05,  % Background color (super faint green)
    colframe=black!50, % Border color (faint green)
    boxrule=0.5pt,     % Border thickness
    arc=3mm,           % Rounded corners
    width=\linewidth,  % Box width
    leftrule=0.1mm,      % Thickness of left border
    rightrule=0.1mm,     % Thickness of right border
    bottomrule=0.1mm,    % Thickness of bottom border
    toprule=0.1mm,       % Thickness of top border
]
\textbf{Guideline - 2}: Explicit reporting of memory (RAM) consumption during training, token throughput during training and inference, and storage requirements are necessary to evaluate the efficiency of the PEFT method. Furthermore, if the proposed methods consist of multiple stages, all metrics should be reported for each stage. 
\end{tcolorbox}

\paragraph{Model sizes}
Another challenge arises from the variation in model sizes used in the evaluation of PEFT methods. It is important to assess methods fine-tuning different model sizes, especially \textgreater1B and \textless20B parameters. With the increase in the backbone model size, the need and usefulness of PEFT methods increase rapidly. Several studies \cite{intrinsic_said,lora} have demonstrated that larger models require fewer parameters to be updated during fine-tuning, both in terms of percentage and when the model is large enough, sometimes even in absolute terms \cite{prefix_tuning}. We would like to particularly stress this, considering that even recent papers often focus solely on BERT. Furthermore, in our experiments, Layer Norm tuning \cite{layer_norm_tuning} was the only consistently efficient method at different scales, while maintaining a competitive performance now downstream tasks. For all other methods, efficiency, and performance considerably varies at different model sizes. Thus, model size must be considered when reporting PEFT methods.

\begin{tcolorbox}[
    colback=gray!05,  % Background color (super faint green)
    colframe=black!50, % Border color (faint green)
    boxrule=0.5pt,     % Border thickness
    arc=3mm,           % Rounded corners
    width=\linewidth,  % Box width
    leftrule=0.1mm,      % Thickness of left border
    rightrule=0.1mm,     % Thickness of right border
    bottomrule=0.1mm,    % Thickness of bottom border
    toprule=0.1mm,       % Thickness of top border
]
\textbf{Guideline - 3}: Efficiency metrics and downstream performance should be reported across multiple model scales. Authors should also consider reporting fine-tuning results for the model sizes that are most commonly used by the research community at the time of experimentation.
\end{tcolorbox}

\paragraph{Method Implementation} Another issue encountered is the state of published implementations. Many codebases are simply copies of the Transformers library \cite{Wolf_Transformers_State-of-the-Art_Natural_2020} or other repositories with only minor modifications. These copies often do not use git forks, making it difficult to identify the differences unless they are highlighted in the README file. But even when differences are easy to find, the code is frequently not readable or reusable. Users are often required to install a modified version of the Transformers library, which conflicts with the most recent version and lacks documentation or examples of how to reuse the method outside of the existing codebase. Despite these challenges, there are some methods with reusable implementations worth highlighting, such as LoRA\footnote{\href{https://github.com/microsoft/LoRA}{github.com/microsoft/LoRA}} and Compacter\footnote{\href{https://github.com/rabeehk/compacter}{github.com/rabeehk/compacter}}. These implementations stand out for their user-friendliness and adaptability, providing a solid foundation for further research and development.

\begin{tcolorbox}[
    colback=gray!05,  % Background color (super faint green)
    colframe=black!50, % Border color (faint green)
    boxrule=0.5pt,     % Border thickness
    arc=3mm,           % Rounded corners
    width=\linewidth,  % Box width
    leftrule=0.1mm,      % Thickness of left border
    rightrule=0.1mm,     % Thickness of right border
    bottomrule=0.1mm,    % Thickness of bottom border
    toprule=0.1mm,       % Thickness of top border
]
\textbf{Guideline - 4}: The study presenting a PEFT method should be accompanied by an easy-to-use implementation of the method.
\end{tcolorbox}

\paragraph{Comparison} 
Intuitively, the presented PEFT method should be compared against popular approaches (e.g., LoRA, BitFit, Adapters) and the methods that share conceptual and architectural similarities with the presented method. However, the absence of standard benchmarks and metrics complicates the comparison of PEFT methods. New methods are often evaluated on different model/dataset combinations, making it challenging to draw meaningful conclusions.
We would like to highlight the papers that report a variety of metrics on standard datasets, simplifying comparison to other methods. For example, KronA \cite{krona} evaluated T5-base on the GLUE benchmark and reported accuracy, training time, and inference time while maintaining the same number of trainable parameters. UniPELT \cite{unipelt} assessed BERT on the GLUE benchmark and reported accuracy, training time, and inference latency, although it used different parameter counts for various methods. LST \cite{ladder_side_tuning} evaluated different T5 sizes on the GLUE benchmark, reporting metrics such as accuracy, training time, the number of updated parameters, and memory usage. MAM \cite{parallel_adapter} applied multiple models to the XSUM benchmark and reported accuracy across a range of trainable parameters, although memory comparisons were not provided.
However, even these papers lack full comparability due to differences in their evaluation settings, such as varying parameter counts or the absence of certain metrics like memory comparisons. These inconsistencies highlight the need for a standardized benchmark and unified metrics to facilitate more accurate comparisons and evaluations of PEFT methods.
Based on our survey and experiments we identified the principal qualities of each of the categories and summarized them in this section and Table~\ref{tab:type_comparison}.

\begin{tcolorbox}[
    colback=gray!05,  % Background color (super faint green)
    colframe=black!50, % Border color (faint green)
    boxrule=0.5pt,     % Border thickness
    arc=3mm,           % Rounded corners
    width=\linewidth,  % Box width
    leftrule=0.1mm,      % Thickness of left border
    rightrule=0.1mm,     % Thickness of right border
    bottomrule=0.1mm,    % Thickness of bottom border
    toprule=0.1mm,       % Thickness of top border
]
Call for community efforts in developing standardized benchmarks and competition for evaluation of PEFT methods.
\end{tcolorbox}


\begin{comment}

\section{Reporting and comparison issues}
\label{sec:reporting_issues}

Survey papers tend to discuss reporting issues, and this one is no exception. We identified several challenges and inconsistencies that make it difficult to draw direct comparisons between methods, which warrant discussion.
% We identified several challenges and inconsistencies that warrant discussion. These challenges make it difficult to draw direct comparisons between methods and evaluate their true performance.

\paragraph{Inconsistent Parameter Counts}

One of the primary challenges stems from the difference in the way researchers report parameter counts. These inconsistencies are not a result of dishonesty but arise from the inherent complexity of the problem. Generally, parameter counts can be categorized into three types: the number of \textbf{trainable parameters}, the number of \textbf{changed parameters} between the original and fine-tuned models, and the \textbf{rank} of the difference between the original and fine-tuned models.

These parameter counts are not equivalent. For example, IntrinsicSAID (Section \ref{sec:intrinsic_said}) learns a low-rank ($\sim$100-1000) transformation of model parameters. However, it changes all ($100\%$) of the model's parameters. DiffPruning (Section \ref{sec:diff_pruning}) learns an update of $0.5\%$ of the parameters, but it actually trains $200\%$ of the parameters: fine-tuning the model and learning the binary mask. For reparameterization-based methods (Sections \ref{sec:lora}, \ref{sec:krona}, \ref{sec:compacter}), memory requirements may vary depending on the implementation design choices.

Of the three types, the number of trainable parameters is the most reliable predictor of memory efficiency. However, it is still imperfect: Ladder-side Tuning trains more parameters than LoRA or BitFit, but it uses less RAM by avoiding backpropagation to the main network.

\paragraph{Model Size}
Another challenge arises from the variation in model sizes used in the evaluation of PEFT methods. Several studies \cite{intrinsic_said,lora} have demonstrated that larger models require fewer parameters to be updated during fine-tuning, both in terms of percentage and when the model is large enough, sometimes even in absolute terms \cite{prefix_tuning}. Thus, \textbf{model size must be considered when comparing PEFT methods}, not just the ratio of trainable parameters.

\paragraph{Lack of Standard Benchmarks and Metrics}

The absence of standard benchmarks and metrics further complicates comparisons. New methods are often evaluated on different model/dataset combinations, making it challenging to draw meaningful conclusions.

We would like to highlight the papers that report a variety of metrics on standard datasets, simplifying comparison to other methods. For example, KronA \cite{krona} evaluated T5-base on the GLUE benchmark and reported accuracy, training time, and inference time while maintaining the same number of trainable parameters. UniPELT \cite{unipelt} assessed BERT on the GLUE benchmark and reported accuracy, training time, and inference latency, although it used different parameter counts for various methods. LST \cite{ladder_side_tuning} evaluated different T5 sizes on the GLUE benchmark, reporting metrics such as accuracy, training time, the number of updated parameters, and memory usage. MAM \cite{parallel_adapter} applied multiple models to the XSUM benchmark and reported accuracy across a range of trainable parameters, although memory comparisons were not provided.

However, even these papers lack full comparability due to differences in their evaluation settings, such as varying parameter counts or the absence of certain metrics like memory comparisons. These inconsistencies highlight the need for a standardized benchmark and unified metrics to facilitate more accurate comparisons and evaluations of PEFT methods.

\paragraph{Issues with Published Implementations}

Another issue encountered is the state of published implementations. Many codebases are simply copies of the Transformers library \cite{Wolf_Transformers_State-of-the-Art_Natural_2020} or other repositories with only minor modifications. These copies often \textbf{do not use git forks}, making it difficult to identify the differences unless they are highlighted in the README file. But even when differences are easy to find, the code is frequently not readable or reusable. Users are often required to install a modified version of the Transformers library, which conflicts with the most recent version and lacks documentation or examples of how to reuse the method outside of the existing codebase.

Despite these challenges, there are some methods with reusable implementations worth highlighting, such as LoRA\footnote{\href{https://github.com/microsoft/LoRA}{github.com/microsoft/LoRA}} and Compacter\footnote{\href{https://github.com/rabeehk/compacter}{github.com/rabeehk/compacter}}. These implementations stand out for their user-friendliness and adaptability, providing a solid foundation for further research and development.

\section{Best Practices}
\label{sec:best_practices}

To address the issues identified, we put forward the following best practices for future research:

\paragraph{Explicit reporting of parameter count type:}
We encourage authors to clearly specify the parameter count being reported in their papers or, ideally, report all three types of parameter count: trainable, changed, and rank. This will improve understanding and allow for more accurate comparisons between methods.

\paragraph{Evaluate with different model sizes:}
It is important to assess methods using different model sizes, especially \textgreater1B and \textless20B parameters, where PEFT is most useful. We would like to particularly stress this, considering that even recent papers often focus solely on BERT.

\paragraph{Comparisons to similar methods:}
In addition to comparing their methods with popular approaches (e.g., LoRA, BitFit, Adapters), we should also analyze their methods alongside other techniques that share conceptual and architectural similarities. In our review, we often came across methods that were based on very similar ideas and designs but were never directly compared. Undertaking such comparisons will offer a more comprehensive understanding of a method's performance and its relative strengths in relation to existing techniques.

\paragraph{Standardized PEFT benchmarks:}
We propose the development of standardized PEFT benchmarks and competitions, which would require participants to compete under the same conditions and facilitate direct comparisons of results. These benchmarks should provide standardized data, models at different scales, and metrics to evaluate both training efficiency and the quality of the fine-tuned model.

To assess training time and memory efficiency in a common setup, competitions can offer centralized servers or specify a comprehensive configuration that outlines the CPU type, amount of memory, and GPU types and quantities. Ideally, this could take the form of an instance template for one of the major cloud providers. GPU memory consumption should be evaluated in a standardized way.

\paragraph{Emphasize code clarity and minimal implementations:}
As a community, we need to prioritize code that is easy to understand and features simple, reusable implementations. In some cases, such implementations provide additional insights to the paper and could be written in a concise manner. This proposal is in the best interests of individual researchers as well, as easy-to-reuse methods may become more popular and, consequently, more cited.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{How to Compare PEFT Methods}
\label{sec:peft_comparison}

When evaluating PEFT methods, it is crucial to consider various aspects of parameter efficiency, including storage, memory, computation, and performance. However, optimizing parameter efficiency alone does not guarantee reduced RAM usage. For example, DiffPruning (Section \ref{sec:diff_pruning}) trains both the model and a binary mask, effectively doubling the number of parameters being trained. Thus, it can only be considered storage-efficient but not memory-efficient.

To compare PEFT methods, we consider five key dimensions: storage efficiency, memory efficiency, computation efficiency, accuracy, and inference overhead.
% We observe that while they aren't independent of one another, improvements along one of the axes do not necessarily translate into improvements along others (Table \ref{tab:efficiency}).

\subsection{PEFT by Category: Pros and Cons}

Many practical aspects and trade-offs of applying PEFT can be determined based on the method's category.
% For example, reparameterization-based methods
% The category of a PEFT method can already say a lot about its strengths and weaknesses.
We identified the principal qualities of each of the categories and summarized them in this section and Table \ref{tab:type_comparison}.

\paragraph{Storage:} All PEFT categories significantly improve storage efficiency. Unlike full fine-tuning that changes all of the model parameters, only some parameters need to be saved for PEFT. In some cases, less than $1/10,000$-th the size of the original model checkpoint (Table~\ref{tab:scale_table}, changed parameters column).

\paragraph{Training efficiency (memory):} All PEFT categories, except for Sparse-selective, generally significantly improve RAM usage. Intrinsic SAID is the only exception to this rule—it is a Reparametrization-based method that can show significantly worse RAM usage than full training due to the high memory usage of the Fastfood transformation.

\paragraph{Training efficiency (speed):} In general, PEFT methods slightly improve training speed for networks smaller than 1B parameters or can even decrease it (Section \ref{sec:peft_comparison_efficiency}). However, for larger networks, PEFT can bring significant improvements. In our experiments, we observed up to a three times speedup for T5-11B.
% \red{In our experiments} (Section \ref{sec:experiments}) we found that fine-tuning T5-large with \red{METHODNAME} takes \red{THIS MUCH} more time to train than regular fine-tuning.
% However, for larger networks we observed significant improvements, up to \red{THIS MUCH}\%. In summary, we've observed training speed improvements in several \red{Additive, Structured Selective, Reparametrization-based and Hybrid methods} and only under certain, although favorable, conditions. Specifically: large model size and either a very small number of trainable parameters or no backpropagation into the original model (e.g., LST). \red{Full list of methods can be found in Table \ref{tab:experiments}.}

\paragraph{Inference efficiency (single-task):} Additive methods always incur inference time overhead, because of the need to perform computation on both the original network and the added parameters. In contrast to that, Selective methods never add overhead, as they are applied to the existing model weights. However, selective methods perform poorly in terms of the downstream metrics when compared to other PEFT. Reparametrization-based methods kill two birds with one stone: they outperform Selective methods metric-wise and can be incorporated into the parameters of the original network.

\paragraph{Inference efficiency (multi-task):} Additive and adapter-like reparametrization-based methods (LoRA, KronA) naturally benefit in a multi-task inference scenario. Keeping a separate model of size $M$ for each task can be wasteful in terms of both memory and speed. Additive methods, some structured selective methods, and several reparametrization-based methods allow one to only store one copy of the original model weights and ``switch'' the sets of task-specific weights dynamically, improving memory usage from $O(NM)$ to $O(M + NA)$, where $A$ is the number of added weights per task. Some Additive methods, like LST (Section \ref{sec:lst}), can also improve inference speed as they only use the original network as a feature extractor. For more details on multi-task training and inference, we refer the reader to Modular Deep Learning \cite{modular_deep_learning}.

\paragraph{A note on hybrid methods:} As hybrid methods usually combine the strengths (and often weaknesses) of the methods they use, it is not possible to generally compare them to the others. However, based on the constituent methods of a hybrid method, one could take an educated guess based on the PEFT type properties discussed above.

\end{comment}


\input{tables/categories_pros_cons}

% \input{tables/comparison_efficiency}

\section{Discussion}
\label{sec:discussion}
The growing accessibility of large language models \cite{zhang2022opt,zeng2022glm130b,yalm,touvron2023llama} and the democratization of their inference through low-bit quantization \cite{dettmers2022llm,Dettmers2022TheCF} have enabled the research community to study, experiment, and tackle new tasks with relatively modest compute budgets. Parameter-efficient fine-tuning is the next step that allows us not just to infer, but to modify these models.

Some methods, including Adapters, Prompt Tuning, LoRA, and (IA)$^3$, have shown their practicality at scale (Table~\ref{tab:scale_table}). However, in practice, matching the performance of full fine-tuning remains a challenge. One of the reasons is high \textbf{sensitivity to hyperparameters}, with optimal hyperparameters often significantly deviating from those used in full fine-tuning due to the varying number of trainable parameters. For instance, the optimal learning rate for parameter-efficient fine-tuning is generally much higher than that for full fine-tuning. The research community should promote in-depth investigations into the impact of hyperparameters on these methods and find reasonable defaults, as parameter-efficient fine-tuning of large models can be noticeably costly at the 20-100B scale. Additionally, efforts should be directed towards developing methods that minimize hyperparameter sensitivity, such as pre-training new parameters \cite{spot,prompt_mapping}.

Examining the taxonomy of methods and the progress made thus far, it is evident that low-rank reparameterization has been remarkably successful in enhancing parameter efficiency. LoRA-style (Section \ref{sec:lora}) and Kronecker-product (Sections \ref{sec:compacter} and \ref{sec:krona}) reparameterizations both decrease the number of trainable parameters while requiring minimal extra computation. A possible future direction for finding new PEFT models is exploring different \textbf{reparametrization techniques} with favorable trainable parameter count vs. rank ratio.

Another possible direction for improvement is utilizing what we know about \textbf{how transformer models process texts} \cite{rogers-etal-2020-primer}. Most of the PEFT methods work uniformly for the model, while we know that models process input differently at different layers. Utilizing this knowledge or building systems that have an adaptive number of parameters per layer could further improve parameter efficiency and accuracy.

% I suggest:
In many respects, fine-tuning large language models faces the same challenges as those encountered in \textbf{edge machine learning} -- we consistently face constraints on memory, computation, and even energy consumption.
% "In many respects, fine-tuning large language models faces the same challenges as those encountered by machine learning on edge devices -- we consistently face constraints on memory, computation, and even energy consumption."  
% In many respects, our current situation resembles the challenges from \textbf{edge machine learning}: we consistently face constraints in memory, computation, and even energy consumption.
Techniques like quantization and pruning \cite{pmlr-v37-gupta15,optimal_brain_damage} that are widely used in edge machine learning now benefit large language models. As we move forward, it is not only plausible but also likely that more ideas could be exchanged between these two areas. Cross-disciplinary collaboration could facilitate the exchange of ideas, accelerating innovation and progress in parameter-efficient fine-tuning.

% Acknowledgements should go at the end, before appendices and references

\acks{This work was funded in part by an Amazon Alexa AI research award to Anna Rumshisky. We would like to thank Vladimir Kluenkov and Victoria Maltseva for their help with Figure~2.}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

% \appendix
% \section*{Appendix A.}

\vskip 0.2in
% \bibliographystyle{plain}
\bibliography{bibliography}

\appendix

\section{Prompts}
\label{Appendix:prompts}

Prompts used in our experimental comparison
\begin{itemize}
    \item BoolQ: "Given a passage and a yes/no question, identify if the answer is \"yes\" or \"no\"."
    \item CB: "Given a premise, identify if the hypothesis entails, contradicts or is neutral to the premise."
    \item COPA: "Given a premise, a question (cause/effect) and two alternative choices, identify plausible answer from the alternative choices."
    \item RTE: "Given a premise, identify if the hypothesis entails premise or not."
\end{itemize}

\section{PEFT Comparison: full experimental results}
\label{sec:peft_comparison_raw}

The following table contains raw results used to produce Tables \ref{tab:peft_comparison_dataset_average_short} and \ref{tab:peft_comparison_dataset_average_tall}. Before these experiments, each model-method-dataset combination was swept over learning rates 1e-3, 1e-4, 5e-5. In our initial experiments we also included a sweep over weight decay (0 and 0.1), but we found it to minimally affect the results of these models on these datasets, always less than 0.01.

%% Numbers to be updated
% Total number of experiments in the table below: 529. Total number of experimetns including hparam search: approximately 1400.

\input{tables/peft_comparison_raw}


\end{document}
