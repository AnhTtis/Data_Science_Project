% File tacl2021v1.tex
% Dec. 15, 2021

% The English content of this file was modified from various *ACL instructions
% by Lillian Lee and Kristina Toutanova
%
% LaTeXery is mostly all adapted from acl2018.sty.

\documentclass[11pt,a4paper]{article}
\usepackage{times,latexsym}
\usepackage{url}
\usepackage[T1]{fontenc}

%% Package options:
%% Short version: "hyperref" and "submission" are the defaults.
%% More verbose version:
%% Most compact command to produce a submission version with hyperref enabled
%%    \usepackage[]{tacl2021v1}
%% Most compact command to produce a "camera-ready" version
   \usepackage[acceptedWithA]{tacl2021v1}
%% Most compact command to produce a double-spaced copy-editor's version
%%    \usepackage[acceptedWithA,copyedit]{tacl2021v1}
%
%% If you need to disable hyperref in any of the above settings (see Section
%% "LaTeX files") in the TACL instructions), add ",nohyperref" in the square
%% brackets. (The comma is a delimiter in case there are multiple options specified.)

\usepackage{tacl2021v1}
% \setlength\titlebox{10cm} % <- for Option 2 below

%%%% Material in this block is specific to generating TACL instructions
\usepackage{xspace,mfirstuc,tabulary}
\newcommand{\dateOfLastUpdate}{Dec. 15, 2021}
\newcommand{\styleFileVersion}{tacl2021v1}

\newcommand{\ex}[1]{{\sf #1}}

\newif\iftaclinstructions
\taclinstructionsfalse % AUTHORS: do NOT set this to true
\iftaclinstructions
\renewcommand{\confidential}{}
\renewcommand{\anonsubtext}{(No author info supplied here, for consistency with
TACL-submission anonymization requirements)}
\newcommand{\instr}es
\fi

%
\iftaclpubformat % this "if" is set by the choice of options
\newcommand{\taclpaper}{final version\xspace}
\newcommand{\taclpapers}{final versions\xspace}
\newcommand{\Taclpaper}{Final version\xspace}
\newcommand{\Taclpapers}{Final versions\xspace}
\newcommand{\TaclPapers}{Final Versions\xspace}
\else
\newcommand{\taclpaper}{submission\xspace}
\newcommand{\taclpapers}{{\taclpaper}s\xspace}
\newcommand{\Taclpaper}{Submission\xspace}
\newcommand{\Taclpapers}{{\Taclpaper}s\xspace}
\newcommand{\TaclPapers}{Submissions\xspace}
\fi

%%%% End TACL-instructions-specific macro block
%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% OUR IMPORTS
\usepackage{epigraph}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage[textsize=tiny]{todonotes}
\usepackage{namedtensor/namedtensor}
\usepackage{multirow}
\newcommand{\smallerfontsize}{\fontsize{8.5}{9.5}\selectfont}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0pt}

\usepackage{listings}

\lstset{
    language=Python,
    basicstyle=\ttfamily\smallerfontsize,
    morekeywords={self, True, False, with},
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    commentstyle=\color{green!50!black},
    showstringspaces=false,
    tabsize=4
}

% axis names
\usepackage{amsfonts}
\ndef{\ax}{ax}
\ndef{\dd}{d}
\ndef{\layer}{layer}
\ndef{\seq}{seq}
\ndef{\subseq}{subseq}
\ndef{\key}{key}
\ndef{\val}{val}
\ndef{\heads}{heads}
\ndef{\batch}{batch}
\ndef{\inp}{input} \ndef{\hidden}{hidden} \ndef{\out}{out}
\ndef{\height}{height} \ndef{\width}{width} \ndef{\chans}{chans}
\ndef{\kernel}{kernel} \ndef{\kh}{kh} \ndef{\kw}{kw}
\ndef{\vocab}{vocab}
\ndef{\classes}{classes}
\ndef{\state}{state}
\ndef{\emb}{emb}

\ndef{\R}{\mathbf{R}}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\orange}[1]{\textcolor{orange}{#1}}

\definecolor{tablegreen}{RGB}{34, 139, 34}
\definecolor{tablered}{RGB}{220, 20, 60}

\definecolor{a_color}{HTML}{4285F4}
\definecolor{r_color}{HTML}{ED8E55}
\definecolor{s_color}{HTML}{E6C800} % FADD87, D2D200, D2D264, E6C800, 

\newcommand{\ad}{\textcolor{a_color}{A}}
\newcommand{\rp}{\textcolor{r_color}{R}}
\newcommand{\se}{\textcolor{s_color}{S}}

\newcommand{\tred}[1]{\textcolor{tablered}{#1}}
\newcommand{\gren}[1]{\textcolor{tablegreen}{#1}}

\newcommand{\peft}{PEFT}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \title{Parameter-efficient Fine-tuning Methods: a Survey}
% \title{The Future of Fine-Tuning:\\A Comprehensive Guide to Parameter-Efficient Techniques}
\title{Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning}

% Author information does not appear in the pdf unless the "acceptedWithA" option is given

% The author block may be formatted in one of two ways:

% Option 1. Authorâ€™s address is underneath each name, centered.

\author{
  Vladislav Lialin \thanks{\hspace{1pt} Correspondance to \texttt{vlialin@cs.uml.edu} or \texttt{vlad.lialin@gmail.com}}
  \\
  UMass Lowell
  \And
  Vijeta Deshpande
  \\
  UMass Lowell
  % \\
  % \texttt{vdeshpande@cs.uml.edu}
  \\
  \And
  Anna Rumshisky
  \\
  UMass Lowell
  \\
  Alexa AI
  % \\
  % \texttt{arum@cs.uml.edu}
}


\date{}

\begin{document}
\maketitle
\begin{abstract}

This paper presents a systematic overview and comparison of parameter-efficient fine-tuning methods covering over 40 papers published between February 2019 and February 2023. These methods aim to resolve the infeasibility and impracticality of fine-tuning large language models by only training a small set of parameters. We provide a taxonomy that covers a broad range of methods and present a detailed method comparison with a specific focus on real-life efficiency and fine-tuning multibillion-scale language models.

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Start of Introduction
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\epigraph{One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation...}{Rich Sutton, The Bitter Lesson}

In October 2018, BERT Large \cite{devlin2018bert} with 350 million parameters was the biggest Transformer model \cite{vaswani2017attention} ever trained. At the time, contemporary hardware struggled to fine-tune this model. The section ``Out-of-memory issues'' on BERT's GitHub\footnotemark{} specifies the maximum batch size for BERT Large given 12Gb of GPU RAM and 512 tokens as \textbf{zero}.
Four years in, publically available models grew up to 176 billion parameters \cite{bloom,zhang2022opt,zeng2022glm130b}, by a factor of 500. Published literature includes models up to 1 trillion parameters \cite{palm,megatron,switch}. However, single-GPU RAM increased less than 10 times (to 80Gb) due to the high cost of HBM memory.
Model size scales almost \textbf{two orders of magnitude quicker} than computational resources making fine-tuning the largest models to downstream tasks infeasible for most and impractical for everyone.
% The Bitter Lesson \cite{} only works if our computation continues to scale 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Moved BERT link to here to make the first page footnotes look a bit nicer
\footnotetext{\href{https://github.com/google-research/bert}{github.com/google-research/bert}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In-context learning \cite{radford2019language} thus became the new normal, the standard way to pass downstream task training data to billion-scale language models. However, the limited context size of Transformers artificially limits the training set size to just a few examples, typically less than 100. This constraint, coupled with the absence of in-context learning performance guarantees even on the training data, presents a challenge. Additionally, expanding the context size leads to a quadratic increase in inference costs. Even though language models perform exceptionally well \cite{brown2020language_gpt3} in a few-shot scenario, ``get more data'' is still the most reliable way to improve on any given task\footnote{\href{http://karpathy.github.io/2019/04/25/recipe/}{A Recipe for Training Neural Networks, A. Karpathy}}. Thus, we, as a community of researchers and engineers, need efficient ways to train on downstream task data.

Parameter-efficient fine-tuning, which we denote as \peft{}, aims to resolve this problem by only training a small set of parameters which might be a subset of the existing model parameters or a set of newly added parameters. These methods differ in parameter efficiency, memory efficiency, training speed, final quality of the model, and additional inference costs (if any).
In the last few years, more than a hundred of \peft{} papers have been published, with several studies \cite{delta_tuning} providing a good overview of the most popular methods, such as Adapters \cite{adapters}, BitFit \cite{bitfit}, LoRa \cite{lora}, Compacter \cite{compacter}, and Soft Prompts \cite{p_tuning,prefix_tuning}. Recently, \citet{modular_deep_learning} presented a survey on modular deep learning overviewing similar methods from the perspective of modularity and multi-task inference.

This survey presents a systematic overview, comparison, and taxonomy of 30 parameter-efficient fine-tuning methods with 20 methods discussed in-depth, covering over 40 papers published from February 2019 to February 2023. We highlight the current unresolved challenges in \peft{}, including the limited theoretical understanding, the gap between \peft{} and fine-tuning performance, and reporting issues. In conclusion, we suggest several avenues for improvement, such as developing standardized \peft{} benchmarks, investigating novel reparameterization techniques with superior parameter-to-rank ratios, conducting in-depth studies on hyperparameters and interpretability, and drawing inspiration from on-device (edge) machine learning research.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Background
\section{Background: Transformer}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Many of the parameter-efficient fine-tuning techniques discussed in this survey can be applied to all neural networks, but some are specifically designed to take advantage of the Transformer architecture \cite{vaswani2017attention}. Given that Transformers are the largest neural networks ever trained, these methods are particularly valuable. Thus, we present a brief overview of the Transformer to provide context for these techniques.

The core building block of the Transformer architecture consists of multi-head attention (MHA) followed by a fully-connected network (FFN), as illustrated in Figure~\ref{fig:transformer}. Both attention and fully-connected layers incorporate residual connections \cite{resnet} and Layer Normalization \cite{layer_norm} to improve trainability.

The heart of the Transformer is attention operation. Following the NamedTensor notation \cite{named_tensor}, it can be described as

\begin{equation*}
\label{eq:def_att}
\begin{aligned}
  & \operatorname{Att}
  \colon
  \mathbb{R}^{\key}
  \times
  \mathbb{R}^{\seq \times\key}
  \times \mathbb{R}^{\seq \times \val}
  \rightarrow \mathbb{R}^{\val} \\
  & \operatorname{Att}(Q,K,V) = \left( \nfun{\seq}{softmax} \frac{Q \ndot{\key} K}{\sqrt{|\key|}} \right) \ndot{\seq} V \\
  % LayerNorm(X) &= \frac{X - \nfun{\layer}{mean}(X)}{\sqrt{\nfun{\layer}{var}(X) + \epsilon}} \odot \gamma + \beta \\
\end{aligned}
\end{equation*}
% where keys queries and values are defined as
\begin{equation*}
\label{eq:kqv}
\begin{aligned}
    Q(x) &= x \cdot W_Q + b_k, \\% \text{ } W_Q \in \R^{\inp \times \hidden} \\
    K(x) &= x \cdot W_K + b_q, \\% \text{ } W_K \in \R^{\inp \times \hidden} \\
    V(x) &= x \cdot W_V + b_v, \\% \text{ } W_V \in \R^{\inp \times \val}    \\
\end{aligned}
\end{equation*}

\begin{equation*}
\begin{aligned}
    W_Q, \text{ } W_K &\in \R^{\inp \times \key}, W_V \in \R^{\inp \times \val} \\
    b_Q, \text{ } b_K &\in \R^{\key}, \text{ } b_V \in \R^{\val}
\end{aligned}
\end{equation*}

A number of methods act specifically on the matrices $W_K$, $W_Q$, $W_V$,  as they provide the main mechanism to pass information from one token to another and control what information (value) is being passed.

Although specific implementations of the Transformer may vary, such as incorporating a cross-attention layer in seq2seq networks or using LayerNorm before sublayers (Pre-LN), most parameter-efficient fine-tuning methods for Transformers only rely on the basic MHA + FFN structure and can be readily adapted to architecture variations.

\begin{figure}
    \centering
    \includegraphics[width=0.65\linewidth]{img/transformer_diagram.pdf}
    \caption{Basic Transformer block}
    \label{fig:transformer}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Taxonomy section
\section{Taxonomy of \peft{}: a birds-eye view}
\label{sec:taxonomy}

\peft{} methods can be classified in multiple ways. They may be differentiated by their underlying approach or conceptual framework: does the method introduce new parameters to the model, or does it fine-tune a small subset of existing parameters? Alternatively, they may be categorized according to their primary objective: does the method aim to minimize memory footprint or only storage efficiency? In this section, we begin by presenting a taxonomy based on the former. We depict this taxonomy and 30 \peft{} methods in Figure \ref{fig:taxonomy}. Sections \ref{sec:additive}-\ref{sec:hybrid} give a brief taxonomy overview. Then, based on our taxonomy classification, we describe 20 \peft{} methods in detail, accompanied by the pseudocode in Sections \ref{sec:additive_section} - \ref{sec:hybrid_section}.

\begin{figure*}
    \centering
    \includegraphics[width=1.0 \textwidth]{img/peft_taxonomy_v3.2.jpg}
    % \includegraphics[width=1.0 \textwidth]{img/peft_taxonomy_v2.0.jpg}
    \caption{Parameter-efficient fine-tuning methods taxonomy. We identify three main classes of methods: \textbf{Addition}-based, \textbf{Selection}-based, and \textbf{Reparametrization}-based. Within additive methods, we distinguish two large included groups: \textbf{Adapter-like} methods and \textbf{Soft prompts}.}
    \label{fig:taxonomy}
\end{figure*}

\subsection{\ad dditive methods}
\label{sec:additive}

The main idea behind additive methods is augmenting the existing pre-trained model with extra parameters or layers and training only the newly added parameters.
As of now, this is the largest and widely explored category of \peft{} methods.
Within this category, two large subcategories have emerged: Adapter-like methods and soft prompts.
% 
\paragraph{Adapters}
Adapters \cite{adapters} are a type of additive parameter-efficient fine-tuning method that involves introducing small fully-connected networks after Transformer sub-layers. The idea has been widely adopted \cite{adapterhub} \footnote{\url{https://github.com/adapter-hub/adapter-transformers}}, and multiple variations of Adapters have been proposed. These variations include modifying the placement of adapters \cite{parallel_adapter,parallel_adapter2}, pruning \cite{sparse_adapter}, and using reparametrization to reduce the number of trainable parameters \cite{compacter,krona}.

\paragraph{Soft Prompts}

Language model prompting \cite{radford2019language} aims to control the behavior of a language model by modifying the input text, which typically consists of a task description accompanied by a few in-context examples. However, these methods are difficult to optimize and are inherently limited in the number of training examples by the maximum model input length. To address these drawbacks, the concept of ``soft'' prompts was introduced \cite{p_tuning,prompt_tuning,prefix_tuning}, where a part of the model's input embeddings is fine-tuned via gradient descent. This pivots the problem of finding prompts in a discrete space to a continuous optimization problem.
Soft prompts can be trained for the input layer only \cite{p_tuning,prompt_tuning} or for all layers \cite{prefix_tuning}. Recent advancements explore how soft prompts could be pre-trained or prompts for different tasks utilized to reduce the computation required for fine-tuning a soft prompt for a new task \cite{spot,Hambardzumyan2021WARPWA,prompt_mapping,ipt}.

\paragraph{Other additive approaches}
Additive methods are a diverse category of parameter-efficient fine-tuning techniques that extends beyond adapters and soft prompts. For example, LeTS \cite{lets}, LST \cite{ladder_side_tuning}, and (IA)$^3$ \cite{t_few} introduce novel ways to add parameters that improve adapters or soft prompts in terms of memory, computation, or accuracy.

\paragraph{Why add parameters?}
Although these methods introduce additional parameters to the network, they achieve significant training time and memory efficiency improvements by reducing the size of the gradients and the optimizer states. Note that in the case of Adam \cite{adam}, for every byte of trainable parameter, one extra byte is needed for its gradient, and two more bytes are needed to store the optimizer state: the first and second moments of the gradient. In practice, depending on the setup, training a model requires 12-20 times more GPU memory than the model weights. By saving memory on optimizer states, gradients, and allowing frozen model parameters to be quantized \cite{dettmers2022llm}, additive \peft{} methods enable the fine-tuning of much larger networks or the use of larger microbatch sizes.
% \footnote{Batch size = microbatch size $\times$ gradient accumulation}.
Which improves training throughput on GPUs.
Moreover, optimizing fewer parameters in distributed setups drastically reduces communication volume.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Efficiency table
\input{tables/efficiency_table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{\se elective methods}

Arguably the earliest example of selective \peft{} is fine-tuning only a few top layers of a network \cite{last_layer_tuning}.
Modern approaches are usually based on the type of the layer \cite{cross_attention_tuning} or the internal structure, such as tuning only model biases \cite{bitfit} or only particular rows \cite{far_edge}.

An extreme version of selective methods is sparse update methods which can completely ignore the structure of the model, and select parameters individually \cite{fish_mask,lottery_ticket_tuning,diff_pruning}.
However, sparse parameter updates present multiple engineering and efficiency challenges, some of which have been tackled in recent research on parameter reconfiguration \cite{far_edge} (Section \ref{sec:far}) and NxM sparsity \cite{nxm_transformer}. Nevertheless, unrestricted unstructured sparsity is still impractical on contemporary hardware.

\subsection{\rp eparametrization-based methods}
Reparametrization-based parameter-efficient fine-tuning methods leverage low-rank representations to minimize the number of trainable parameters. The notion that neural networks have low-dimensional representations has been widely explored in both empirical and theoretical analysis of deep learning \cite{parametercounting,measuring_the_intrinsic_dimension,Arora2018StrongerGB,malladi2022kernel}.

\citet{intrinsic_said} have demonstrated that fine-tuning can be performed effectively in low-rank subspaces. Further, they showed that the size of the subspace that needs adaption is smaller for bigger models or the models pre-trained for longer.
Their approach, referred to as Intrinsic SAID (Section \ref{sec:intrinsic_said}), employs the Fastfood transform \cite{Le2013FastfoodAK} to reparametrize the update to neural network parameters.

However, perhaps the most well-known reparametrization-based method is Low-Rank Adaptation or LoRa  \cite{lora}, which employs a simple low-rank matrix decomposition to parametrize the weight update $\delta W = W^{\text{down}}W^{\text{up}}$. This approach is straightforward to implement and has been evaluated on models with up to 175 billion parameters. We provide a detailed discussion of this method in Section \ref{sec:lora}.
More recent works \cite{compacter,krona} have also explored the use of Kronecker product reparametrization ($\delta W = A \otimes B$), which yields a more favorable tradeoff between rank and parameter count.

\subsection{Hybrid methods}
\label{sec:hybrid}
A number of methods have emerged that combine ideas from multiple categories of \peft{} \cite{parallel_adapter,sparse_adapter,unipelt,compacter}.
For instance, MAM Adapter (Section \ref{sec:mam_adapter}) incorporates both Adapters and Prompt tuning.
UniPELT (Section \ref{sec:unipelt}) adds LoRa to the mixture.
Compacter and KronA$^B_{res}$ reparametrize the adapters to reduce their parameter count (Sections \ref{sec:compacter} and \ref{sec:krona}).
Finally, S4 (Section \ref{sec:s4}) is a result of an automated algorithm search that combines all \peft{} classes to maximize accuracy at $0.5\%$ of extra parameter count.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Scale Table
\input{tables/scale_table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Comparing \peft{} methods}
\label{sec:axes}

Parameter efficiency encompasses multiple aspects, including storage, memory, computation, and performance. However, achieving parameter efficiency alone does not necessarily lead to reduced RAM usage. For example, DiffPruning (Section \ref{sec:diff_pruning}) entails training a binary mask with the same number of parameters as the model. Consequently, this method can be only considered storage-efficient, while still incurring considerable RAM and time costs during fine-tuning.

To compare \peft{} methods, we keep five dimensions of comparison in mind: storage efficiency, memory efficiency, computation efficiency, accuracy, and inference overhead.
We observe that while they aren't completely independent from one another, improvements along one of the axes do not necessarily translate into improvements along others (Table \ref{tab:efficiency}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Main section
\section{Overview of \peft{} Approaches}
\label{sec:methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In the next sections, we dive into the details of several parameter-efficient fine-tuning approaches. We will describe the distinctions and tradeoffs between them in terms of the axes outlined in Section \ref{sec:axes}.
We \textbf{bold} a one-sentence summary of each method to simplify skimming through them.

In the method description, we also indicate whether it has been applied to models with fewer than 1 billion, fewer than 20 billion, or more than 20 billion parameters. The model size summary can be found in Table \ref{tab:scale_table}.
We stick to the parameter counts indication where possible because the words ``small'' and ``large'' change their meaning too quickly.
Finally, we provide a brief pseudo-PyTorch implementation of the most important part of the algorithm where it's feasible.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Efficiency table footnote
\footnotetext[5]{Depends on sparse operations hardware support.}
\footnotetext[6]{Depends on the weight pruning method.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{\ad dditive methods: Adapters}
\label{sec:additive_section}
We start our dive into \peft{} methods with one of the largest sub-families of methods that are built on the idea of adding fully-connected networks between model layers, i.e., adapters.
\subsection{Adapters}
\label{sec:adapters}
The idea of Adapters was initially developed for multi-domain image classification \cite{Rebuffi2017Adapters,Rebuffi2018EfficientPO} and consisted in adding domain-specific layers between neural network modules.

\citet{adapters} adapt this idea to NLP. They propose to \textbf{add fully-connected networks after attention and FFN layers} in Transformer. Unlike the transformer FFN block, Adapters usually have a smaller hidden dimension than the input. Adapters have demonstrated impressive parameter efficiency at the time, showing that it is possible to achieve performance competitive to full fine-tuning by tuning less than 4\% of the total model parameters.

A schematic adapter implementation:
\begin{lstlisting}
def transformer_block_with_adapter(x):
    residual = x
    x = SelfAttention(x)
    x = FFN(x)  # adapter
    x = LN(x + residual)
    residual = x
    x = FFN(x)  # transformer FFN
    x = FFN(x)  # adapter
    x = LN(x + residual)
    return x
\end{lstlisting}

\citet{adapter_fusion} found that inserting the adapter only after the self-attention layer (after normalization) achieves similar performance as using two adapters per transformer block. 

\subsection{AdaMix}

AdaMix \cite{adamix} improves the performance of adapters by \textbf{utilizing multiple adapters in a mixture-of-experts (MoE) fashion} \cite{moe}. Meaning, that each adapter layer is a set of  layers (experts), and for each forward pass only a small set of experts is activated.
In contrast to a regular MoE, which selects and weights multiple experts using a routing network AdaMix randomly selects a single expert for each forward pass. This minimizes computational costs and, according to \citet{adamix}, doesn't degrade performance.
Another difference from a regular MoE layer is that up and down projections of the adapter are selected independently. After training, the adapter weights are averaged across the experts which makes inference more memory-efficient.
To stabilize training, the authors propose consistency regularization which minimizes symmetrized $KL$ between two models' forwards with different sets of experts selected.

Although AdaMix achieves better performance than regular adapters with the same inference cost, it can use more memory during training.
\citet{adamix} show that AdaMix can use much smaller adapter hidden states, which amortizes trainable parameter overhead over the number of experts ($\sim$4-8).
However, the consistency regularization technique increases computational requirements and memory consumption, as it needs to keep two versions of the hidden states and gradients over two model forward passes with different experts.

\begin{lstlisting}
def transformer_block_with_adamix(x):
    residual = x
    x = SelfAttention(x)
    x = LN(x + residual)
    residual = x
    x = FFN(x)
    # adamix starts here
    x = random_choice(experts_up)(x)
    x = nonlinearity(x)
    x = random_choice(experts_down)(x)
    x = LN(x + residual)
    return x

def consistency_regularization(x):
    logits1 = transformer_adamix(x)
    # second pass uses different experts
    logits2 = transformer_adamix(x)
    r = symmetrized_KL(logits1, logits2)
    return r
\end{lstlisting}
% NOTE: AdapterFusion is not MoE, because it only learns to average adapter outputs, it doesn't route to them and has no sparse activation. 
% \citet{adapter_fusion} also combine multiple adapters in MoE-like fashion, but they focus on multi-task setting. In their approach, each adapter is trained for a specific task. After this, a separate step of adaptation is peformed which is to train a routing network that fuses the adapters. Their approach 

\section{\ad dditive Methods: Soft Prompts}

Prompting language models has demonstrated remarkable performance in zero- and few-shot scenarios \cite{brown2020language_gpt3, pet}. However, optimizing discrete natural language prompts or using in-context learning is impractical when there are many training examples.
To overcome this challenge, the concept of ``soft'' or ``continuous'' prompts was proposed \cite{prefix_tuning,prompt_tuning,p_tuning} that converts the discrete optimization problem of finding the best "hard" prompt to a continuous one.

\subsection{Prompt Tuning}
\label{sec:prompt_tuning}

Prompt tuning \cite{prompt_tuning} proposes to \textbf{prepend the model input embeddings with a trainable tensor} $\mathbf{P}\in \R^{l \times h}$. This tensor is commonly referred to as ``soft prompt'' and it is optimized directly through gradient descent.

\begin{lstlisting}
def soft_prompted_model(input_ids):
    x = Embed(input_ids)
    x = concat([soft_prompt, x], dim=seq)
    return model(x)
\end{lstlisting}

% Soft prompts are incredibly parameter-efficient 
Ablation studies by \citet{prompt_mapping} over prompt length from 1 to 150 tokens and model size from 10M to 11B parameters reveal that prompt tuning is more parameter efficient the larger the model. For instance, prompt tuning of T5-11B achieves the same SuperGLUE \cite{wang2019superglue} performance with either 5 or 150 soft prompt length.
% \todo{In the future version, we can add more details on how these are initialized}

Furthermore, efficiency grows faster than the model size: T5-large performance saturates at prompt length 20 or 20K trainable parameters ($0.002\%$), and T5-XL performance saturates prompt length 5, also 20K trainable parameters ($0.0002\%$).
However, prompt tuning only becomes comparable with full fine-tuning at the 10B model scale. Also, increasing the %size 
length of the input by 20-100 tokens can significantly increase computation, given the quadratic complexity of the transformer.
Overall, soft prompts are incredibly parameter-efficient at the cost of inference overhead and more applicable to larger models.

\subsection{Prefix-Tuning}
\label{sec:prefix_tuning}
\citet{prefix_tuning} independently develop the idea of soft prompts with a distinctive flavor: instead of adding a soft prompt to the model input, \textbf{trainable parameters are prepended to the hidden states of all layers}.
The same prefix $\mathbf{P}_\theta \in \R^{l \times h}$
% where $l$ is prefix length and $h$ is hidden size of the model,
is prepended to all of the hidden states.

They observe that directly optimizing the soft prompt leads to instabilities during training.
Instead, soft prompts are parametrized through a feed-forward network $\mathbf{P}_\theta = \operatorname{FFN}(\hat{\mathbf{P}}_\theta)$.
During training, $\hat{\mathbf{P}}_\theta$ and the parameters of the FFN are optimized. After, only $\mathbf{P}_\theta$ is needed for inference, and the FFN can be discarded.

Pseudocode for a single layer:
\begin{lstlisting}
def transformer_block_for_prefix_tuning(x):
    soft_prompt = FFN(soft_prompt)
    x = concat([soft_prompt, x], dim=seq)
    return transformer_block(x)
\end{lstlisting}

Note that the approach is very similar to Prompt-tuning (Section \ref{sec:prompt_tuning}), but the soft prompts are added in each layer.

In their experiments, \citet{prefix_tuning} apply BART \cite{lewis2019bart} model (<1B) to different generation tasks and show a performance close to the full fine-tuning by training only $0.1\%$ parameters. Soft prompt lengths used in the study vary from 10 to 200 tokens.


\subsection{Intrinsic Prompt Tuning (IPT)}
Prompt tuning methods, despite being parameter efficient, present practical problems such as slow convergence and a need to store all of the task-specific parameters.
% Such challenges can increase the compute and storage expenses.
A few studies \cite{prompt_mapping,spot} proposed to pre-train soft prompts to improve performance and convergence speed. However, these methods do not provide solutions to reduce the number of parameters per task.

\citet{ipt} hypothesize that the h-dimensional space used to define soft prompt parameters contains an ``intrinsic task subspace'' that can differentiate between various tasks.

IPT method works in three steps.
First, given a set of training tasks, their soft prompts are learned in a regular way (Section \ref{sec:prompt_tuning}).
Then, these prompts are used to train an autoencoder that compresses their dimensionality.
After this, the encoder part is discarded, and only the input to the autoencoder decoder is trained on new tasks.
In summary, \textbf{IPT uses an autoencoder to (de)compress the soft prompt}.

\begin{lstlisting}
def autoencoder(soft_prompt):
    soft_prompt = soft_prompt.flatten()
    P = FFN_A(soft_prompt)  # encoder
    P = FFN_B(P)            # decoder
    P = P.reshape(prompt_len, hidden)
    return P

def soft_prompted_model(x):
    P = FFN_B(intrinsic_subspace_prompt)
    P = P.reshape(prompt_len, hidden)
    x = concat([P, x], dim=seq)
    return model(x)
\end{lstlisting}

Even though the IPT framework reduces the number of parameters for the unseen tasks, this reduction comes at the price of training the autoencoder. The authors conduct experiments with the BART-base model and a prompt length of 100.
The resulting autoencoder, which is implemented\footnote{\href{https://github.com/thunlp/Intrinsic-Prompt-Tuning/blob/master/bartPrompt.py}{github.com/thunlp/Intrinsic-Prompt-Tuning}}
as a fully-connected network that accepts a one-dimensional tensor of size $768 \cdot 100$ reaches approximately 78 million parameters. I.e., over 56\% of total parameters in the BART-base model.
Hence, significantly more efficient ways of prompt autoencoding are required to make IPT practically applicable.

\section{\ad dditive Methods: Other Approaches}

Several of the additive \peft{} methods do not follow the idea of either adapters or soft prompts and propose to augment a pre-trained network in an original way.


\subsection{Ladder-Side Tuning (LST)}
\label{sec:lst}

Ladder-Side Tuning \cite{ladder_side_tuning} \textbf{trains a small transformer network on the side of the pre-trained network}. This side network combines the hidden states of the pre-trained backbone network with its own hidden states. 

This way, the side network only uses the pre-trained model as a feature extractor, and backpropagation must only be computed through the side network saving on both memory and compute during training.
The authors also use multiple tricks no improve the performance and parameter efficiency of LST. Namely, initializing the side network from the pre-trained model parameters using structural pruning and using twice (or 4x) fewer layers in the side network than in the backbone network.

The pseudocode:
% Pseudocode:
\begin{lstlisting}
def ladder_side_layer(x, h_pt):
    h_pt = h_pt @ W_down  # to x.shape
    gate = sigmoid(alpha)
    x = gate * x + (1 - gate) * h_pt
    return transformer_block(x)

def ladder_side_network(x):
    with no_grad():
        H_pt = pretrained_network(
            x, return_all_hiddens=True
        )
    for i in range(layers):
        layer = ladder_side_layers[i]
        x = layer(x, H_pt[i])
    return x
\end{lstlisting}
Where \texttt{h\_pt} is the output of the corresponding layer of the pre-trained network, and \texttt{alpha} is an input-independent trainable scalar gate.

LST demonstrated a three-fold RAM reduction in fine-tuning T5-Base compared to full fine-tuning and a two-fold RAM usage reduction compared to LoRa (Section \ref{sec:lora}) with a small degradation in accuracy and outperforms these methods when controlling for RAM usage.

\subsection{(IA)$^3$}
\label{sec:ia3}

\citet{t_few} propose a new parameter-efficient method to multi-task fine-tune T-few. (IA)$^3$ learns new parameters $l_v$, $l_k$, $l_{ff}$ which \textbf{rescale key, value, and hidden FFN activations}.
Specifically,
\begin{lstlisting}
def transformer_block_with_ia3(x):
    residual = x
    x = ia3_self_attention(x)
    x = LN(x + residual)
    residual = x
    x = x @ W_1         # FFN in
    x = l_ff * gelu(x)  # (IA)3 scaling
    x = x @ W_2         # FFN out
    x = LN(x + residual)
    return x

def ia3_self_attention(x):
    k, q, v = x @ W_k, x @ W_q, x @ W_v
    k = l_k * k
    v = l_v * v
    return softmax(q @ k.T) @ V
\end{lstlisting}

Training only these three vectors $l_v$, $l_k$, $l_{ff}$ for each transformer block leads to high parameter efficiency. For T0-3B, it only updates about $0.02\%$ of model parameters and outperforms other methods, including Compacter (Section \ref{sec:compacter}) with similar parameter count and LoRa (Section \ref{sec:lora}) with 16 times more trainable parameters.
Unlike adapters-tuned models, (IA)$^3$-tuned models exhibit minimal overhead. Vectors $l_v$ and $l_k$ can be integrated into the corresponding linear layers, and the only overhead comes from $l_{ff}$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% SELECTIVE METHODS
\section{\se elective Methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Selective methods fine-tune a subset of the existing parameters of the model. It could be a layer depth-based selection, layer type-based lection, or even individual parameter selection.

\subsection{BitFit}
\label{sec:bitfit}

\citet{bitfit} propose to \textbf{only fine-tune the biases of the network}. That is, for every linear or convolutional layer, the weight matrix $W$ is left as is, and only the bias vector $b$ is optimized.

\begin{lstlisting}
  params = (p for n, p
            in model.named_parameters()
            if "bias" in n)
  optimizer = Optimizer(params)
\end{lstlisting}

BitFit only updates about 0.05\% of the model parameters. The original paper demonstrated that the method achieves similar performance to full fine-tuning or better performance in low and medium-data scenarios in BERT models (<1B parameters). Further research evaluated the method on larger networks such as T0-3B \cite{t_zero,t_few} or GPT-3 \cite{lora}.
At this scale, BitFit significantly underperforms full fine-tuning, and other \peft{} approaches.

% \subsection{Cross-Attention tuning}
% \red{
% Just fine-tuning attention. They succesfully applied to to machine translation domain adaptation.
% }

\subsection{DiffPruning}
\label{sec:diff_pruning}

DiffPruning \cite{diff_pruning} aims to achieve parameter efficiency by learning a sparse update of a neural network's weights. The method introduces a learnable binary mask on the weights, denoted by $\delta = z \circ \Delta W$, where $\circ$ represents the Hadamard product. This \textbf{parameter mask is learned during model fine-tuning} as part of the regularization objective, which is a differentiable approximation to the $L_0$ norm of the update vector $\delta$.

DiffPruning has achieved comparable performance to full fine-tuning while modifying only 0.5\% of the model parameters in smaller-scale (<1B) scenarios. This makes it a useful method for multi-task deployment for edge (mobile) scenarios where storage is limited. However, this method requires more memory than traditional fine-tuning, as it involves optimizing all parameters during training in addition to the learnable binary mask.

\subsection{Freeze and Reconfigure (FAR)}
\label{sec:far}

FAR \cite{far_edge} \textbf{selects columns of parameter matrices to prune and reconfigures linear layers into trainable and frozen}.
The method operates in two stages. In the first stage, the most important rows of parameter matrices are identified for updating. This process is similar to structured pruning and can use any pruning method. In their paper, the authors fine-tune the model on a percentage of the data and select the top-$r$ rows based on the $L_1$ distance between the fine-tuned and original models.

In the second stage, the network is reconfigured by splitting each parameter $W \in \mathbb{R}^{in \times h}$ into a trainable component $W_{t} \in \mathbb{R}^{in \times h'}$ and a frozen component $W_{f} \in \mathbb{R}^{in \times (h - h')}$, where $h'$ is the desired number of trainable parameters. The matrix multiplications with $W_{t}$ and $W_{f}$ are computed independently, and the results are concatenated. A similar operation is performed on biases.

Pseudocode implementation is rather simple
\begin{lstlisting}
def far_layer(x):
    h1 = x @ W_t
    h2 = x @ W_f
    return concat([h1, h2], dim=-1)
\end{lstlisting}

While this approach creates additional compute overhead during training, it provides great flexibility in terms of parameter selection on modern hardware using standard frameworks like PyTorch. After training, the parameters can be reconfigured back removing any inference overhead.

The original paper focused on edge scenarios and used DistilBERT (66M) for their experiments. FAR was only applied to feed-forward layers, as these make up the majority of the DistilBERT parameters. The authors showed that FAR achieved similar performance to fine-tuning on five GLUE tasks and SQuAD 2.0 \cite{squad2} while updating 6\% of the parameters.

\subsection{FishMask}

FishMask \cite{fish_mask} is a \textbf{sparse fine-tuning method that selects top-p parameters of the model based on their Fisher information}.
Fisher information is estimated in a common way through a diagonal approximation
\begin{equation*}
    \hat{F_{\theta}} = \frac{1}{N} \sum_{i = 1}^{N} \mathbb{E}_{y \sim p_{\theta}(y|x_i)}(\nabla_{\theta} \log p_{\theta}(y|x_i))^2
    \label{eqn:fisher_approximation}
\end{equation*}

Thus, the method requires computing gradients for all parameters on (several batches of) the data. However, after the highest-Fisher parameters are selected, only they need to be optimized.

Pseudocode to compute the masks:
\begin{lstlisting}
  sparsity = 0.99
  N = len(data)
  for x, y in data:
      loss = loss_fn(model(x), y)
      loss.backward()
      for n, p in model.named_params():
          fisher[n] += p.grad ** 2 / N
  
  threshold = percentile(fisher, sparsity)
  masks = {n: f > threshold
           for n, f in fisher.items()}
\end{lstlisting}

The method generally performs on-par with adapters but sub-par to LoRa and (IA)$^3$ (Sections \ref{sec:lora} and \ref{sec:ia3}). It has been evaluated on BERT (<1B) and T0-3B models.
However, FishMask is computationally intensive and inefficient on contemporary deep learning hardware due to the lack of support for sparse operations.

\section{\rp eparametrization-based methods}

These methods use the idea of reparametrizing the weights of the network using a low-rank transformation.
This decreases the trainable parameter count while still allowing the method to work with high-dimensional matrices, such as the pre-trained parameters of the networks.

\subsection{Intrinsic SAID}
\label{sec:intrinsic_said}

In their work, \citet{intrinsic_said} investigate the intrinsic dimensionality of fine-tuning and demonstrate that this process can be performed in a low-rank subspace. Specifically, they use the \textbf{Fastfood transform to reparametrize the update to the model weights}. Fastfood is a compute-efficient dimensionality expansion transform $F:\R^d \rightarrow \R^D$ that can be done in~$O(D \log d)$ time and $O(D)$ memory.

Their results indicate that larger models require changes in a lower-rank subspace compared to smaller models to achieve the same fine-tuning performance. This observation motivates both scaling large models and parameter-efficient fine-tuning. It is important to note that, unlike methods that select a particular subset of parameters for fine-tuning, Intrinsic SAID updates all model parameters in a low-rank manner, i.e., $\theta = \theta_0 + F(\theta^d)$, where $\theta_0 \in \R^D$ denotes the pre-trained model parameters and $\theta^d \in \R^d$ denotes the parameters to be optimized. Therefore, while the number of optimizable parameters is low, the $O(D)$ memory complexity of Fastfood and the update to all of the model's parameters make Intrinsic SAID impractical for fine-tuning large networks. For more details on Fastfood, we refer the  reader to the original paper by \citet{Le2013FastfoodAK}.

% While IntrinsicSAID only updates a low-rank subspace of model's parameters, 
% \todo{I spent like 2 hours figuring out Fastfood and }
% Speficially, for parameters $\theta \in \R^D$, the update is paramtrized through $\theta \in \R^d$
% \begin{equation}
% \begin{aligned}
%     \delta \theta &= \hat{\theta}
%     V = H G \Pi H B \\
% \end{aligned}
% \end{equation}
% where $\theta^d \in \R^d$ are the optmizable parameters, $H$ - Hadamard matrix

\subsection{LoRa}
\label{sec:lora}

LoRa \cite{lora} takes inspiration from IntrinsicSAID and proposes a simpler way to perform low-rank fine-tuning.
\textbf{Parameter update for a weight matrix in LoRa is decomposed into a product of two low-rank matricies}
\begin{equation}
\begin{aligned}
    % \delta W &= W^{\text{down}} W^{\text{up}} \\
    % W^{\text{down}} &\in \R^{\text{in} \times k}, W^{\text{up}} \in \R^{k \times \text{out}}\\
    \delta W &= W_A W_B \\
    W_A &\in \R^{\text{in} \times r}, W_B \in \R^{r \times \text{out}}\\
\end{aligned}
\end{equation}
%% NOTE: we use r instead of k to make this consistent with Compacter

All pre-trained model parameters are kept frozen, and only $W_A$ and $W_B$ matrices are trainable. The scaling factor is constant and typically equals $\frac{1}{r}$. After training, they can be integrated into the original $W$ by just adding the matrix $W_A W_B$ to the original matrix $W$.

Pseudocode is very simple:
\begin{lstlisting}
def lora_linear(x):
    h = x @ W # regular linear
    h += x @ W_A @ W_B # low-rank update
    return scale * h
\end{lstlisting}

In Transformers, LoRa is typically used for $W_K$ and $W_V$ projection matrices in multi-head attention modules.
The method outperforms BitFit and Adapters and has been evaluated on the models up to 175B parameters.

% \subsection{DyLoRa}
%% NOTE: their results are inconclusive, LoRa performs on-par with DyLoRa when rank is 8. The biggest difference is with rank 1. I suggest not including it in v1.0 and take a second look at the paper.
% Automatic hparam selection for LoRa. Should we merge it with the LoRa section?

\subsection{KronA}
\label{sec:krona}
KronA \cite{krona} replaces matrix factorization $\delta W=W_A W_B$ in LoRa (Section \ref{sec:lora}) with a \textbf{matrix factorization through a Kronecker product $\delta W = W_A \otimes W_B$}.
This yields a better
rank per parameters tradeoff because the Kronecker product keeps the rank of the original matrices being multiplied.
Or, in other words, $\operatorname{rank}(A \otimes B) = \operatorname{rank}A \cdot \operatorname{rank}B$.
Additionally, \citet{krona} propose to use efficient Kronecker product-vector product operation $x (A \otimes B)$ which avoids representing $\delta W$ explicitly and leads to significant speedups

Krona pseudocode:
\begin{lstlisting}
def krona_linear(x):
    x = x @ W  # regular linear
    x += kronecker_vector_prod(x, W_A, W_B)
    return scale * x

# same as x @ kronecker_product(A, B)
def kronecker_vector_prod(x, A, B):
    x = x.reshape(A.shape[1], B.shape[1])
    x = A.T @ x @ B
    return x.reshape(-1)
\end{lstlisting}

KronA$^B_{res}$ is another method presented in \citet{krona}. It is a parallel adapter with Kronecker product-parametrization of the weights and a residual connection.

On GLUE, KronA methods perform on-par or better than adapters (Section \ref{sec:adapters}), LoRa (Section \ref{sec:lora}), and Compacter (Section \ref{sec:compacter}) at the same trainable parameter count $0.07\%$ while being significantly faster than adapters or Compacter at inference time. The method was evaluated only on small (<1B) models.

\paragraph{Background: What is a Kronecker product?}
Kronecker product is a tensor operation defined as
\begin{equation}
\begin{aligned}
\mathbf{A} \otimes \mathbf{B} &: \R^{n \times m} \times \R^{k \times l} \rightarrow \R^{nk \times ml}\\
\mathbf{A} \otimes \mathbf{B} &= 
    \begin{bmatrix}
    a_{1,1} \mathbf{B} & \cdots & a_{1,n} \mathbf{B} \\
    \vdots & \ddots & \vdots \\
    a_{m,1} \mathbf{B} & \cdots & a_{m,n} \mathbf{B}
    \end{bmatrix}
\end{aligned}
\end{equation}

% and it can be implemented in
It can be easily implemented \footnote{Source: \url{github.com/rabeehk/compacter}} in PyTorch using the command {\small{ \texttt{torch.einsum} }}
% the following way
\begin{lstlisting}
def batched_kronecker_product(a, b):
    bs, i, j = a.shape
    bs, k, m = b.shape
    res = einsum("bij,bkm->bikjm", a, b)
    return res.view(bs, i * k, j * m)
\end{lstlisting}

\section{Hybrid Approaches}
\label{sec:hybrid_section}

Hybrid methods for parameter-efficient fine-tuning combine different techniques and strategies to achieve better performance while reducing the computational costs associated with fine-tuning large neural networks. These methods can be viewed as a synergistic blend of multiple approaches. The resulting hybrid methods can leverage the strengths of each individual technique, while mitigating their weaknesses, leading to improved performance and efficiency.


\subsection{SparseAdapter}
\label{sec:sparse_adapter}

\citet{sparse_adapter} propose \textit{Large-Sparse} strategy to train adapter layers.
In this strategy, they use a \textbf{large hidden dimension for the added module and prune around 40\% of the values at initialization}.
\textit{Large-Sparse} consistently outperforms its non-sparse counterpart with the same trainable parameter count.
However, training and inference costs can be higher depending on hardware support for sparse tensors and operations.
It is also worth noting that computing the pruning mask for this method may require obtaining gradients for all newly added parameters.

\subsection{MAM Adapters}
\label{sec:mam_adapter}

In their study, \citet{parallel_adapter} conducted a thorough investigation of adapter placement and soft prompts.
They concluded that scaled parallel adapters outperform sequentially-placed adapters and that placing an adapter in parallel to FFN outperforms multi-head attention-parallel adapters.
\footnote{While at first this might look contradicting the finding of \citet{adapter_fusion}, it actually supports it because the FFN-parallel adapter modifies the outputs of attention, just like the MHA-sequential adapter.}
They also notice that soft prompts can efficiently modify attentions by only changing 0.1\% of the parameters and propose to ``mix-and-match'' (MAM) these ideas.
Their final model, \textbf{MAM Adapter is a combination of scaled parallel adapter for FFN layer and soft prompt}.
\begin{lstlisting}
def transformer_block_mam(x):
    x = concat([x, soft_prompt], dim=seq)
    residual = x
    x = SelfAttention(x)
    x = LN(x + residual)
    x_a = FFN(x) # parallel adapter
    x_a = scale * x_a
    x = LN(x + x_adapter)
    return x
\end{lstlisting}
MAM method outperforms BitFit and PromptTuning by a large margin and consistently outperforms LoRa (Section \ref{sec:lora}), Adapters (Section \ref{sec:adapters}), and Prefix Tuning (Section \ref{sec:prefix_tuning}) with 200 soft prompt length at 7\% extra parameters. The experiments were concluded on <1B models.

It's worth noting that parallel adapters were independently studied by \citet{parallel_adapter2} in the domain of machine translation.

% \subsection{Meta-Adapters}
% Use adapters, but pre-train parts of them them with MAML-like algorithm. After pre-training, do not fine-tune meta-adapters and only tune the regular adapter part. So its kind of tricky, but 4 and 8-shot improvement over AdapterFusion is very large

% \subsection{PALs}
% \red{
% Parallel attention layers \cite{bert_and_pals} are MHA adapters parallel to MHA. Adds 13\% parameters, but seems to work well with BERT. No experiments beyond 1B, because its an old paper.
% }

% \subsection{LeTS}
% \red{
% A side-tuning method that uses (OMG) LSTMs and NAS to select the input to each attention layer and the final pooling layer, early-stage pruning.
% }

\subsection{UniPELT}
\label{sec:unipelt}
UniPELT \cite{unipelt} is a \textbf{gated combination of LoRa, Prefix-tuning, and Adapters}.
More specifically, LoRa reparametrization is used for $W_Q$ and $W_V$ attention matrices, prefix-tuning is applied to keys and values of each layer, and adapters are added after the feed-forward layer of the transformer block.
For each of the modules, gating is implemented as a linear layer projecting the module input into a dimension of size one, sigmoid activation, and averaging the resulting vector over the sequence length.
Trainable parameters include LoRa matrices $W_A, W_B$, prompt tuning parameters $P_q, P_k$, adapter parameters, and gating function weights.

Next, we present a schematic implementation of UniPELT.
We omit multiple attention heads for simplicity.
\begin{lstlisting}
def transformer_block_with_unipelt(x):
    residual = x
    x = unipelt_self_attention(x)
    x = LN(x + residual)
    residual = x
    x = FFN(x)
    adapter_gate = gate(x)
    x = adapter_gate * FFN(x)
    x = LN(x + residual)
    return x

def unipelt_self_attention(x):
    k, q, v = x @ W_k, x @ W_q, x @ W_v
    # lora for queries and values
    lora_gate = gate(x)
    q += lora_gate * W_qA @ W_aB
    v += lora_gate * W_vA @ W_vB
    # prefix tuning
    pt_gate = gate(x)
    q_prefix = pt_gate * P_q
    k_prefix = pt_gate * P_k
    return softmax(q @ k.T) @ V

def gate(x):
    x = Linear(x)
    x = sigmoid(x)
    return mean(x, dim=seq)
\end{lstlisting}

UniPELT demonstrates significant improvements over individual LoRa, Adapters, and Prefix Tuning approaches in low-data scenarios with only 100 examples. In higher data scenarios, UniPELT performs on par or better than these approaches.
\citet{unipelt} reports 1.3\% trainable model parameters using UniPELT on BERT (<1B parameters) models.

\subsection{Compacter}
\label{sec:compacter}
Compacter \cite{compacter} \textbf{utilizes Kronecker product, low-rank matrices, and parameter sharing across layers to produce adapter weights}.
Each parameter $W$ in an adapter is equal to a sum of Kronecker products
\begin{equation}
\begin{aligned}
    \hat W &= \sum_{i=0}^{n} A_i \otimes B_i \\
    \hat W \in \R^{k \times d}, \text{ } &A_i \in \R^{n \times n}, \text{ } B_i \in \R^{\frac{k}{n} \times \frac{d}{n}}.
\end{aligned}
\end{equation}
A linear layer $x \hat W + b$ with this parametrization is called parametrized hypercomplex multiplication (PHM) layer \cite{pha}.
Compacter takes this idea futher, parametrizing $B_i$ similar to LoRa (Section \ref{sec:lora}) $B_i = B_i^{\text{down}} B_i^{\text{up}}$ where all matricies are of rank at most $r$. Matrices $A_i$ are shared across all adapter layers for further parameter efficiency. The~corresponding layer is named Low-rank PHM or LPHM.
Compacter layer pseudocode:
\begin{lstlisting}
def compacter(x):
    x = LPHM(x)  # Essentially an FFN
    x = gelu(x)  # but
    x = LPHM(x)  # LPHM replaces linear
    return x

def lphm_forward(x):
    B = B_d @ B_u
    W = batched_kronecker_product(A, B)
    W = sum(W, dim=0)
    return x @ W + b
\end{lstlisting}

Note that all $A$ and $B$ are 3D tensors with the first dimension equal to $n$, the number of Kronecker products in the PHM layer.

Compacter comes in two flavors: two adapters per transformer block or a single adapter after a feedforward layer (Compacter++).
With only 0.05\% additional parameters Compacter++ performs on par or better than adapters with 0.8\% additional parameters.
% \red{The paper also provides memory and epoch time comparison to other \peft{} methods and demonstrates 42\% memory reduction and 26\% speedup compared to full fine-tuning.}
The model has been evaluated on T5 Base (<1B) and T0-3B.


\subsection{S4}
\label{sec:s4}
\citet{design_spaces} conduct an extensive exploration of various \textbf{combinations of parameter-efficient fine-tuning techniques}. Their search space includes dividing consecutive layers into four uneven groups, allocating varying amounts of trainable parameters to each layer, determining which groups to fine-tune, and deciding which \peft{} methods to apply to each group.

Their proposed method, S4, divides layers into four groups ($G_{1,2,3,4}$) using a ``spindle'' pattern: more layers are allocated to the middle groups and fewer to the top and bottom groups. All groups are trainable, with trainable parameters uniformly allocated across the layers (not groups).
Different combinations of PEFT methods are applied to different groups. Specifically,
\begin{equation}
\begin{aligned}
G_1 &: A, L & G_3 &: A, P, B \\
G_2 &: A, P & G_4 &: P, B, L \\
\end{aligned}
\end{equation}

where A stands for Adapters (Section \ref{sec:adapters}), P for Prefix-Tuning (Section \ref{sec:prefix_tuning}), B for BitFit (Section \ref{sec:bitfit}), and L for LoRa (Section \ref{sec:lora}).

The search experiments were conducted on T5-base and the GLUE dataset at 0.5\% trainable parameters. The S4 method was then applied to T5-3B, RoBERTa, and XL-Net, consistently outperforming individual BitFit, Prefix Tuning, LoRa, and Adapters across different architectures, model sizes, and tasks.


\section{Reporting and comparison issues}

Survey papers tend to discuss reporting issues, and unfortunately, this one is not an exception.
We identified several challenges and inconsistencies that warrant discussion. These challenges make it difficult to draw direct comparisons between methods and evaluate their true performance.

\paragraph{Inconsistent Parameter Counts}

One of the primary challenges stems from the difference in the way researchers report parameter counts. These inconsistencies are not a result of dishonesty but arise from the inherent complexity of the problem. Generally, parameter counts can be categorized into three types: the number of \textbf{trainable parameters}, the number of \textbf{changed parameters} between the original and fine-tuned models, and the \textbf{rank} of the difference between the original and fine-tuned models.

These distinctions can have significant implications.
For example, IntrinsicSAID (Section \ref{sec:intrinsic_said}) learns a low-rank ($\sim$100-1000) transformation of model parameters.
However, it changes all of the model's parameters.
DiffPruning (Section \ref{sec:diff_pruning}) learns an update of $0.5\%$ parameters, but it actually trains $200\%$ parameters: fine-tuning the model and learning the binary mask.
For reparameterization-based methods (Sections \ref{sec:lora}, \ref{sec:krona}, \ref{sec:compacter}), memory requirements may vary depending on the implementation design choices.

Of the three types, the number of trainable parameters is the most reliable predictor of memory efficiency. However, it is still imperfect. For instance, Ladder-side Tuning (Section \ref{sec:lst}) uses a separate side-network with more parameters than LoRa or BitFit, but it requires less RAM, 
 since backpropagation is not computed through the main network.

\paragraph{Model Size}
Another challenge arises from the variation in model sizes used in the evaluation of \peft{} methods. Several studies \cite{intrinsic_said,lora} have demonstrated that larger models require fewer parameters to be updated during fine-tuning, both in terms of percentage and when the model is large enough, sometimes even in absolute terms \cite{prefix_tuning}. Thus, \textbf{model size must be considered when comparing \peft{} methods}, not just the ratio of trainable parameters.

\paragraph{Lack of Standard Benchmarks and Metrics}

The absence of standard benchmarks and metrics further complicates comparisons. New methods are often evaluated on different model/dataset combinations, making it challenging to draw meaningful conclusions.

We would like to highlight the papers that report a variety of metrics on standard datasets simplifying comparison to other methods.
For example, KronA \cite{krona} evaluated T5-base on the GLUE benchmark and reported accuracy, training time, and inference time while maintaining the same number of trainable parameters. UniPELT \cite{unipelt} assessed BERT on the GLUE benchmark and reported accuracy, training time, and inference latency, although it used different parameter counts for various methods. LST \cite{ladder_side_tuning} evaluated different T5 sizes on the GLUE benchmark, reporting metrics such as accuracy, training time, the number of updated parameters, and memory usage.
MAM \cite{parallel_adapter} applied multiple models to the XSUM benchmark and reported accuracy across a range of trainable parameters, although memory comparisons were not provided.

However, even these papers lack full comparability due to differences in their evaluation settings, such as varying parameter counts or the absence of certain metrics like memory comparisons. These inconsistencies highlight the need for a standardized benchmark and unified metrics to facilitate more accurate comparisons and evaluations of \peft{} methods.

\paragraph{Issues with Published Implementations}

Another issue encountered is the state of published implementations.
Many codebases are simply copies of the Transformers library \cite{Wolf_Transformers_State-of-the-Art_Natural_2020} or other repositories with only minor modifications. These copies often do not use git forks, making it difficult to identify the differences unless they are highlighted in the README file.

Furthermore, even when differences are easy to find, the code is frequently not reusable.
Users are often required to install a modified version of the Transformers library, which conflicts with the most recent version and lacks documentation or any examples of how to reuse the method outside of the existing codebase.

Despite these challenges, there are some methods with reusable implementations worth highlighting, such as LoRa\footnote{\href{https://github.com/microsoft/LoRA}{github.com/microsoft/LoRA}} and Compacter\footnote{\href{https://github.com/rabeehk/compacter}{github.com/rabeehk/compacter}}. These implementations stand out for their user-friendliness and adaptability, providing a solid foundation for further research and development.


\section{Best Practices}

To address different issues identified in the parameter-efficient fine-tuning literature, we put forward the following best practices for future research:

\paragraph{Explicit reporting of parameter count type:} We encourage authors to clearly specify the parameter count being reported in their papers or, ideally, report all three types of parameter count: trainable, changed, and rank. This will improve understanding and allow for more accurate comparisons between methods.

\paragraph{Evaluate with different model sizes:} It is important to assess their methods using different model sizes, as this can provide a more comprehensive understanding of each method's strengths and limitations. This is particularly important considering that even recent papers often focus solely on BERT.

\paragraph{Comparisons to similar methods:} 
In addition to comparing their methods with popular approaches (e.g., LoRa, BitFit, Adapters) we should also analyze their methods alongside other techniques that share conceptual and architectural resemblances.
In our review, we often came across methods that were based on very similar ideas and designs but were never directly compared. Undertaking such comparisons will offer a more comprehensive understanding of a method's performance and its relative strengths in relation to existing techniques.

\paragraph{Standardized PEFT benchmarks and competitions:} We propose the development of standardized PEFT benchmarks and competitions, which would require participants to compete under the same conditions and facilitate direct comparisons of results. These benchmarks should provide standardized data and models at different scales to evaluate training efficiency.

To assess training time and memory efficiency, competitions can offer a centralized server or specify a comprehensive server configuration that outlines the CPU type, amount of memory, and GPU type and quantity. Ideally, this could take the form of an instance template to one of the major cloud providers.
GPU memory consumption should be evaluated in a standardized way.

\paragraph{Emphasize code clarity and minimal implementations:} As a community, we need to prioritize code that is easy to understand and features simple, reusable implementations. In some cases, such implementations provide additional insight to the paper and could be written in a concise manner. This proposal is in the interest of individual researchers as well, as easy-to-reuse methods may become more popular and, consequently, more cited.


\section{Discussion}
\label{sec:discussion}
The growing accessibility of large language models \cite{zhang2022opt,zeng2022glm130b,yalm,touvron2023llama} and the democratization of their inference through low-bit quantization \cite{dettmers2022llm,Dettmers2022TheCF} has enabled the research community to study, experiment, and tackle new tasks with relatively modest compute budgets.
Parameter-efficient fine-tuning is the next step that will allow us not just to inference, but to modify these models.

Among the developed methods, some have demonstrated their practicality at scale (Table \ref{tab:scale_table}), such as Adapters (Section \ref{sec:adapters}), Prompt Tuning (Section \ref{sec:prompt_tuning}), LoRa (Section \ref{sec:lora}), and (IA)$^3$ (Section \ref{sec:ia3}).
However, in practice, matching the performance of full fine-tuning remains a challenge. One of the reasons is high \textbf{sensitivity to hyperparameters}, with optimal hyperparameters often significantly deviating from those used in full fine-tuning due to the varying number of trainable parameters. For instance, the optimal learning rate for parameter-efficient fine-tuning is generally much higher than that for full fine-tuning.
The research community should promote in-depth investigations into the impact of hyperparameters on these methods and finding reasonable defaults, as parameter-efficient fine-tuning or large models can be noticeably costly at 20-100B scale.
Additionally, efforts should be directed towards developing methods that minimize hyperparameter sensitivity, such as pre-training new parameters \cite{spot,prompt_mapping}.

Examining the taxonomy of methods and the progress made thus far, it is evident that low-rank reparameterization has been remarkably successful in enhancing parameter efficiency. LoRa-style (Section \ref{sec:lora}) and Kronecker-product (Sections \ref{sec:compacter} and \ref{sec:krona}) reparameterizations both decrease the number of trainable parameters while requiring minimal extra computation.
A possible future direction of finding new \peft{} models is exploring different \textbf{reparametrization techniques} with favorable trainable parameter count vs. rank ratio.

Another possible direction of improvement is utilizing what we know about \textbf{how transformer models process texts} \cite{rogers-etal-2020-primer}.
Most of the \peft{} methods work uniformly for the model, while we know that models process input differently at different layers. Utilizing this knowledge or building systems that have an adaptive number of parameters per layer could further improve parameter efficiency and accuracy.

% I suggest:
% "In many respects, fine-tuning large language models faces the same challenges as those encountered by machine learning on edge devices -- we consistently face constraints on memory, computation, and even energy consumption."  
In many respects, our current situation resembles the challenges from \textbf{edge machine learning}: we consistently face constraints in memory, computation, and even energy consumption.
Techniques like quantization and pruning \cite{pmlr-v37-gupta15,optimal_brain_damage} widely used in edge machine learning, now benefit large language models.
As we move forward, it is not only plausible but also likely that more ideas could be exchanged between these two areas. Cross-disciplinary collaboration could further exchange ideas, accelerating innovation and progress in parameter-efficient fine-tuning.

\bibliography{bibliography}
\bibliographystyle{acl_natbib}

\appendix

\section{Acknowledgements}
We would like to thank Vladimir Kluenkov and Victoria Maltseva for their help with Figure 2.

\end{document}
