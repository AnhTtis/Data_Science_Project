@software{yalm,
author = {Khrushchev, Mikhail and Vasilev, Ruslan and Petrov, Alexey and Zinov, Nikolay},
month = {6},
title = {{YaLM 100B}},
url = {https://github.com/yandex/YaLM-100B},
year = {2022}
}

@inproceedings{adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{modular_deep_learning,
  title={Modular Deep Learning},
  author={Jonas Pfeiffer and Sebastian Ruder and Ivan Vulic and E. Ponti},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.11529}
}

@inproceedings{optimal_brain_damage,
 author = {LeCun, Yann and Denker, John and Solla, Sara},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Optimal Brain Damage},
 url = {https://proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf},
 volume = {2},
 year = {1989}
}



@InProceedings{pmlr-v37-gupta15,
  title = 	 {Deep Learning with Limited Numerical Precision},
  author = 	 {Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1737--1746},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/gupta15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/gupta15.html},
  abstract = 	 {Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network’s behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding}
}


@inproceedings{life_after_bert,
    title = "Life after {BERT}: What do Other Muppets Understand about Language?",
    author = "Lialin, Vladislav  and
      Zhao, Kevin  and
      Shivagunde, Namrata  and
      Rumshisky, Anna",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.227",
    doi = "10.18653/v1/2022.acl-long.227",
    pages = "3180--3193",
    abstract = "Existing pre-trained transformer analysis works usually focus only on one or two model families at a time, overlooking the variability of the architecture and pre-training objectives. In our work, we utilize the oLMpics bench- mark and psycholinguistic probing datasets for a diverse set of 29 models including T5, BART, and ALBERT. Additionally, we adapt the oLMpics zero-shot setup for autoregres- sive models and evaluate GPT networks of different sizes. Our findings show that none of these models can resolve compositional questions in a zero-shot fashion, suggesting that this skill is not learnable using existing pre-training objectives. Furthermore, we find that global model decisions such as architecture, directionality, size of the dataset, and pre-training objective are not predictive of a model{'}s linguistic capabilities.",
}


@article{rogers-etal-2020-primer,
    title = "A Primer in {BERT}ology: What We Know About How {BERT} Works",
    author = "Rogers, Anna  and
      Kovaleva, Olga  and
      Rumshisky, Anna",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2020.tacl-1.54",
    doi = "10.1162/tacl_a_00349",
    pages = "842--866",
    abstract = "Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.",
}


@inproceedings{tenney2019bert,
    title = "{BERT} Rediscovers the Classical {NLP} Pipeline",
    author = "Tenney, Ian  and
      Das, Dipanjan  and
      Pavlick, Ellie",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1452",
    doi = "10.18653/v1/P19-1452",
    pages = "4593--4601",
    abstract = "Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.",
}

@InProceedings{lets,
  title = 	 {Learn-to-Share: A Hardware-friendly Transfer Learning Framework Exploiting Computation and Parameter Sharing},
  author =       {Fu, Cheng and Huang, Hanxian and Chen, Xinyun and Tian, Yuandong and Zhao, Jishen},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {3469--3479},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/fu21a/fu21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/fu21a.html},
}


@inproceedings{Wolf_Transformers_State-of-the-Art_Natural_2020,
author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Perric and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
month = {10},
pages = {38--45},
publisher = {Association for Computational Linguistics},
title = {{Transformers: State-of-the-Art Natural Language Processing}},
url = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
year = {2020}
}

@article{Dettmers2022TheCF,
  title={The case for 4-bit precision: k-bit Inference Scaling Laws},
  author={Tim Dettmers and Luke Zettlemoyer},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.09720}
}

@article{dettmers2022llm,
  title={Llm. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2208.07339},
  year={2022}
}

@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@inproceedings{pet,
    title = "It{'}s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners",
    author = {Schick, Timo  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.185",
    doi = "10.18653/v1/2021.naacl-main.185",
    pages = "2339--2352",
    abstract = "When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much {``}greener{''} in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.",
}

@article{malladi2022kernel,
      title={A Kernel-Based View of Language Model Fine-Tuning},
      author={Malladi, Sadhika and Wettig, Alexander and Yu, Dingli and Chen, Danqi and Arora, Sanjeev},
      journal={arXiv preprint arXiv:2210.05643},
      year={2022}
}

@inproceedings{Arora2018StrongerGB,
  title={Stronger generalization bounds for deep nets via a compression approach},
  author={Sanjeev Arora and Rong Ge and Behnam Neyshabur and Yi Zhang},
  booktitle={International Conference on Machine Learning},
  year={2018}
}

@article{parametercounting,
  title={Rethinking Parameter Counting: Effective Dimensionality Revisted},
  author={Maddox, Wesley J. and Benton, Gregory and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:2003.02139},
  year={2020}
}

@inproceedings{lottery_ticket_tuning,
    title = "Composable Sparse Fine-Tuning for Cross-Lingual Transfer",
    author = "Ansell, Alan  and
      Ponti, Edoardo  and
      Korhonen, Anna  and
      Vuli{\'c}, Ivan",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.125",
    doi = "10.18653/v1/2022.acl-long.125",
    pages = "1778--1796",
}

@article{squad2,
   title={Know What You Don’t Know: Unanswerable Questions for SQuAD},
   url={http://dx.doi.org/10.18653/v1/P18-2124},
   DOI={10.18653/v1/p18-2124},
   journal={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
   publisher={Association for Computational Linguistics},
   author={Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
   year={2018} }


@inproceedings{einops,
    title={Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation},
    author={Alex Rogozhnikov},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=oapKSVM2bcj}
}

@inproceedings{pha,
  title={Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with $1/n$ Parameters},
  author={Zhang, Aston and Tay, Yi and Zhang, Shuai and Chan, Alvin and Luu, Anh Tuan and Hui, Siu Cheung and Fu, Jie},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{measuring_the_intrinsic_dimension,
  title={Measuring the Intrinsic Dimension of Objective Landscapes},
  author={Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{moe,
    title={ Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
    author={Noam Shazeer and *Azalia Mirhoseini and *Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
    booktitle={International Conference on Learning Representations},
    year={2017},
    url={https://openreview.net/forum?id=B1ckMDqlg}
}

@article{unipelt,
  title={UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning},
  author={Yuning Mao and Lambert Mathias and Rui Hou and Amjad Almahairi and Hao Ma and Jiawei Han and Wen-tau Yih and Madian Khabsa},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.07577}
}

@inproceedings{parallel_adapter2,
    title = "Counter-Interference Adapter for Multilingual Machine Translation",
    author = "Zhu, Yaoming  and
      Feng, Jiangtao  and
      Zhao, Chengqi  and
      Wang, Mingxuan  and
      Li, Lei",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.240",
    doi = "10.18653/v1/2021.findings-emnlp.240",
    pages = "2812--2823",
    abstract = "Developing a unified multilingual model has been a long pursuing goal for machine translation. However, existing approaches suffer from performance degradation - a single multilingual model is inferior to separately trained bilingual ones on rich-resource languages. We conjecture that such a phenomenon is due to interference brought by joint training with multiple languages. To accommodate the issue, we propose CIAT, an adapted Transformer model with a small parameter overhead for multilingual machine translation. We evaluate CIAT on multiple benchmark datasets, including IWSLT, OPUS-100, and WMT. Experiments show that the CIAT consistently outperforms strong multilingual baselines on 64 of total 66 language directions, 42 of which have above 0.5 BLEU improvement.",
}

@inproceedings{t_zero,
    title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
    author={Victor Sanh and Albert Webson and Colin Raffel and Stephen Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Teven Le Scao and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M Rush},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=9Vrb9D0WI4}
}

@InProceedings{last_layer_tuning,
  title = {DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition},
  author = 	 {Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {647--655},
  year = 	 {2014},
  editor = 	 {Xing, Eric P. and Jebara, Tony},
  volume = 	 {32},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/donahue14.pdf},
  url = 	 {https://proceedings.mlr.press/v32/donahue14.html},
  abstract = 	 {We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks.  Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks.  We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges.  We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges.  We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.}
}

@InProceedings{bert_and_pals,
  title = 	 {{BERT} and {PAL}s: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning},
  author =       {Stickland, Asa Cooper and Murray, Iain},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5986--5995},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/stickland19a/stickland19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/stickland19a.html},
  abstract = 	 {Multi-task learning shares information between related tasks, sometimes reducing the number of parameters required. State-of-the-art results across multiple natural language understanding tasks in the GLUE benchmark have previously used transfer from a single large task: unsupervised pre-training with BERT, where a separate BERT model was fine-tuned for each task. We explore multi-task approaches that share a \hbox{single} BERT model with a small number of additional task-specific parameters. Using new adaptation modules, PALs or ‘projected attention layers’, we match the performance of separately fine-tuned models on the GLUE benchmark with $\approx$7 times fewer parameters, and obtain state-of-the-art results on the Recognizing Textual Entailment dataset.}
}


@article{Rebuffi2018EfficientPO,
  title={Efficient Parametrization of Multi-domain Deep Neural Networks},
  author={Sylvestre-Alvise Rebuffi and Hakan Bilen and Andrea Vedaldi},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2018},
  pages={8119-8127}
}

@inproceedings{Rebuffi2017Adapters,
 author = {Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning multiple visual domains with residual adapters},
 url = {https://proceedings.neurips.cc/paper/2017/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{adapterhub,
   title={AdapterHub: A Framework for Adapting Transformers},
   url={http://dx.doi.org/10.18653/v1/2020.emnlp-demos.7},
   DOI={10.18653/v1/2020.emnlp-demos.7},
   journal={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
   publisher={Association for Computational Linguistics},
   author={Pfeiffer, Jonas and Rücklé, Andreas and Poth, Clifton and Kamath, Aishwarya and Vulić, Ivan and Ruder, Sebastian and Cho, Kyunghyun and Gurevych, Iryna},
   year={2020} }


@article{Le2013FastfoodAK,
  title={Fastfood: Approximate Kernel Expansions in Loglinear Time},
  author={Quoc V. Le and Tam{\'a}s Sarl{\'o}s and Alex Smola},
  journal={ArXiv},
  year={2013},
  volume={abs/1408.3060}
}

@inproceedings{nxm_transformer,
 author = {Holmes, Connor and Zhang, Minjia and He, Yuxiong and Wu, Bo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {1818--1830},
 publisher = {Curran Associates, Inc.},
 title = {NxMTransformer: Semi-Structured Sparsification for Natural Language Understanding via ADMM},
 url = {https://proceedings.neurips.cc/paper/2021/file/0e4f5cc9f4f3f7f1651a6b9f9214e5b1-Paper.pdf},
 volume = {34},
 year = {2021}
}



@inproceedings{Wang2018GLUEAM,
  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
  booktitle={BlackboxNLP@EMNLP},
  year={2018}
}

@misc{srivastava2015highway,
    title={Highway Networks},
    author={Rupesh Kumar Srivastava and Klaus Greff and Jürgen Schmidhuber},
    year={2015},
    eprint={1505.00387},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{resnet,
   title={Deep Residual Learning for Image Recognition},
   url={http://dx.doi.org/10.1109/cvpr.2016.90},
   DOI={10.1109/cvpr.2016.90},
   journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   publisher={IEEE},
   author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
   year={2016},
   month={Jun}
}

@article{layer_norm,
  title={Layer Normalization},
  author={Jimmy Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
  journal={ArXiv},
  year={2016},
  volume={abs/1607.06450}
}

@article{named_tensor,
  title={Named Tensor Notation},
  author={David Chiang and Alexander M. Rush and Boaz Barak},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.13196}
}

@inproceedings{layer_connect,
  title={LayerConnect: Hypernetwork-Assisted Inter-Layer Connector to Enhance Parameter Efficiency},
  author={Haoxiang Shi and Rongsheng Zhang and Jiaan Wang and Cen Wang and Yinhe Zheng and Tetsuya Sakai},
  booktitle={International Conference on Computational Linguistics},
  year={2022}
}

@article{far_edge,
  title={Efficient Fine-Tuning of BERT Models on the Edge},
  author={Danilo Vucetic and Mohammadreza Tayaranian and Maryam Ziaeefard and James J. Clark and Brett H. Meyer and Warren J. Gross},
  journal={2022 IEEE International Symposium on Circuits and Systems (ISCAS)},
  year={2022},
  pages={1838-1842}
}

@article{ladder_side_tuning,
  title={LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning},
  author={Yi-Lin Sung and Jaemin Cho and Mohit Bansal},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.06522}
}

@article{design_spaces,
  title={Parameter-Efficient Fine-Tuning Design Spaces},
  author={Jiaao Chen and Aston Zhang and Xingjian Shi and Mu Li and Alexander J. Smola and Diyi Yang},
  journal={ArXiv},
  year={2023},
  volume={abs/2301.01821}
}

@article{pett,
  title={Towards a Unified View on Visual Parameter-Efficient Transfer Learning},
  author={Bruce X. B. Yu and Jianlong Chang and Lin Liu and Qi Tian and Changan Chen},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.00788}
}

@article{polyhistor,
  title={Polyhistor: Parameter-Efficient Multi-Task Adaptation for Dense Vision Tasks},
  author={Yen-Cheng Liu and Chih-Yao Ma and Junjiao Tian and Zijian He and Zsolt Kira},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.03265}
}

@article{krona,
  title={KronA: Parameter Efficient Tuning with Kronecker Adapter},
  author={Ali Edalati and Marzieh S. Tahaei and Ivan Kobyzev and V. Nia and James J. Clark and Mehdi Rezagholizadeh},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.10650}
}

@inproceedings{intrinsic_said,
  title={Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning},
  author={Armen Aghajanyan and Luke Zettlemoyer and Sonal Gupta},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2020}
}

@inproceedings{ipt,
  title={Exploring Universal Intrinsic Task Subspace via Prompt Tuning},
  author={Yujia Qin and Xiaozhi Wang and Yusheng Su and Yankai Lin and Ning Ding and Jing Yi and Weize Chen and Zhiyuan Liu and Juanzi Li and Lei Hou and Peng Li and Maosong Sun and Jie Zhou},
  year={2021}
}

@inproceedings{spot,
  title={SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer},
  author={Tu Vu and Brian Lester and Noah Constant and Rami Al-Rfou and Daniel Matthew Cer},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2021}
}

@article{prompt_mapping,
  title={On Transferability of Prompt Tuning for Natural Language Understanding},
  author={Yusheng Su and Xiaozhi Wang and Yujia Qin and Chi-Min Chan and Yankai Lin and Zhiyuan Liu and Peng Li and Juan-Zi Li and Lei Hou and Maosong Sun and Jie Zhou},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.06719}
}

@inproceedings{Hambardzumyan2021WARPWA,
  title={WARP: Word-level Adversarial ReProgramming},
  author={Karen Hambardzumyan and Hrant Khachatrian and Jonathan May},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2021}
}

@article{Gheini2022KnowWY,
  title={Know Where You're Going: Meta-Learning for Parameter-Efficient Fine-tuning},
  author={Mozhdeh Gheini and Xuezhe Ma and Jonathan May},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.12453}
}

@inproceedings{fish_mask,
     author = {Sung, Yi-Lin and Nair, Varun and Raffel, Colin A},
     booktitle = {Advances in Neural Information Processing Systems},
     editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
     pages = {24193--24205},
     publisher = {Curran Associates, Inc.},
     title = {Training Neural Networks with Fixed Sparse Masks},
     url = {https://proceedings.neurips.cc/paper/2021/file/cb2653f548f8709598e8b5156738cc51-Paper.pdf},
     volume = {34},
     year = {2021}
}

@inproceedings{Ansell2021ComposableSF,
  title={Composable Sparse Fine-Tuning for Cross-Lingual Transfer},
  author={Alan Ansell and E. Ponti and Anna Korhonen and Ivan Vulic},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2021}
}

@article{t_few,
  title={Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning},
  author={Haokun Liu and Derek Tam and Mohammed Muqeeth and Jay Mohta and Tenghao Huang and Mohit Bansal and Colin Raffel},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.05638}
}

@inproceedings{cross_attention_tuning,
  title={Cross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation},
  author={Mozhdeh Gheini and Xiang Ren and Jonathan May},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2021}
}

@article{dylora,
  title={DyLoRA: Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation},
  author={Mojtaba Valipour and Mehdi Rezagholizadeh and Ivan Kobyzev and Ali Ghodsi},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.07558}
}

@inproceedings{attention_fusion,
  title={Attention Fusion: a light yet efficient late fusion mechanism for task adaptation in NLU},
  author={Jin Cao and Chandan Prakash and Wael Hamza},
  booktitle={NAACL-HLT},
  year={2022}
}

@InProceedings{meta_adapters,
  title = 	 {Meta-Adapters: Parameter Efficient Few-shot Fine-tuning through Meta-Learning},
  author =       {Bansal, Trapit and Alzubi, Salaheddin and Wang, Tong and Lee, Jay-Yoon and McCallum, Andrew},
  booktitle = 	 {Proceedings of the First International Conference on Automated Machine Learning},
  pages = 	 {19/1--18},
  year = 	 {2022},
  editor = 	 {Guyon, Isabelle and Lindauer, Marius and van der Schaar, Mihaela and Hutter, Frank and Garnett, Roman},
  volume = 	 {188},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v188/bansal22a/bansal22a.pdf},
  url = 	 {https://proceedings.mlr.press/v188/bansal22a.html},
}


@article{perfect,
  title={Prompt-free and Efficient Few-shot Learning with Language Models},
  author={Rabeeh Karimi Mahabadi and Luke Zettlemoyer and James Henderson and Marzieh Saeidi and Lambert Mathias and Ves Stoyanov and Majid Yazdani},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.01172}
}

@article{adamix,
  title={AdaMix: Mixture-of-Adapter for Parameter-efficient Tuning of Large Language Models},
  author={Yaqing Wang and Subhabrata Mukherjee and Xiaodong Liu and Jing Gao and Ahmed Hassan Awadallah and Jianfeng Gao},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.12410}
}

@inproceedings{diff_pruning,
  title={Parameter-Efficient Transfer Learning with Diff Pruning},
  author={Demi Guo and Alexander M. Rush and Yoon Kim},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2020}
}

@article{prompt_tuning,
  title={The Power of Scale for Parameter-Efficient Prompt Tuning},
  author={Brian Lester and Rami Al-Rfou and Noah Constant},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.08691}
}


@inproceedings{prefix_tuning,
    title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    author = "Li, Xiang Lisa  and
      Liang, Percy",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.353",
    doi = "10.18653/v1/2021.acl-long.353",
    pages = "4582--4597",
}


@inproceedings{
    parallel_adapter,
    title={Towards a Unified View of Parameter-Efficient Transfer Learning},
    author={Junxian He and Chunting Zhou and Xuezhe Ma and Taylor Berg-Kirkpatrick and Graham Neubig},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=0RDcd5Axok}
}

@article{Zhuang2023ASO_survey,
  title={A Survey on Efficient Training of Transformers},
  author={Bohan Zhuang and Jing Liu and Zizheng Pan and Haoyu He and Yuetian Weng and Chunhua Shen},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.01107}
}

@article{Cambier2020ShiftedAS_fp8,
  title={Shifted and Squeezed 8-bit Floating Point format for Low-Precision Training of Deep Neural Networks},
  author={L{\'e}opold Cambier and Anahita Bhiwandiwalla and Ting Gong and Mehran Nekuii and Oguz H. Elibol and Hanlin Tang},
  journal={ArXiv},
  year={2020},
  volume={abs/2001.05674}
}

@article{Wang2018TrainingDN_fp8,
  title={Training Deep Neural Networks with 8-bit Floating Point Numbers},
  author={Naigang Wang and Jungwook Choi and Daniel Brand and Chia-Yu Chen and K. Gopalakrishnan},
  journal={ArXiv},
  year={2018},
  volume={abs/1812.08011}
}

@inproceedings{attention_bottlenecks,
  title={Attention Bottlenecks for Multimodal Fusion},
  author={Arsha Nagrani and Shan Yang and Anurag Arnab and Aren Jansen and Cordelia Schmid and Chen Sun},
  booktitle={Neural Information Processing Systems},
  year={2021}
}

@article{Lin2022VisionTA,
  title={Vision Transformers are Parameter-Efficient Audio-Visual Learners},
  author={Yan-Bo Lin and Yi-Lin Sung and Jie Lei and Mohit Bansal and Gedas Bertasius},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.07983}
}

@article{Geiping2022CrammingTA,
  title={Cramming: Training a Language Model on a Single GPU in One Day},
  author={Jonas Geiping and Tom Goldstein},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.14034}
}

@article{emb_recycling,
  title={Embedding Recycling for Language Models},
  author={Jon Saad-Falcon and Amanpreet Singh and Luca Soldaini and Mike D'Arcy and Arman Cohan and Doug Downey},
  journal={ArXiv},
  year={2022},
  volume={abs/2207.04993}
}

@inproceedings{k_adapter,
  title={K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters},
  author={Ruize Wang and Duyu Tang and Nan Duan and Zhongyu Wei and Xuanjing Huang and Jianshu Ji and Guihong Cao and Daxin Jiang and Ming Zhou},
  booktitle={Findings},
  year={2020}
}

@article{flamingo,
  title={Flamingo: a Visual Language Model for Few-Shot Learning},
  author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andy Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.14198}
}

@inproceedings{m4_adapter,
  title={m4Adapter: Multilingual Multi-Domain Adaptation for Machine Translation with a Meta-Adapter},
  author={Wen Lai and Alexandra Chronopoulou and Alexander Fraser},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2022}
}

@article{p_tuning,
  title={GPT Understands, Too},
  author={Xiao Liu and Yanan Zheng and Zhengxiao Du and Ming Ding and Yujie Qian and Zhilin Yang and Jie Tang},
  journal={ArXiv},
  year={2021},
  volume={abs/2103.10385}
}

@inproceedings{ppt,
  title={PPT: Pre-trained Prompt Tuning for Few-shot Learning},
  author={Yuxian Gu and Xu Han and Zhiyuan Liu and Minlie Huang},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2021}
}

@inproceedings{madg,
  title={MAD-G: Multilingual Adapter Generation for Efficient Cross-Lingual Transfer},
  author={Alan Ansell and E. Ponti and Jonas Pfeiffer and Sebastian Ruder and Goran Glavas and Ivan Vulic and Anna Korhonen},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2021}
}

@inproceedings{madx,
  title={MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer},
  author={Jonas Pfeiffer and Ivan Vulic and Iryna Gurevych and Sebastian Ruder},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2020}
}

@article{adapter_fusion,
  title={AdapterFusion: Non-Destructive Task Composition for Transfer Learning},
  author={Jonas Pfeiffer and Aishwarya Kamath and Andreas R{\"u}ckl{\'e} and Kyunghyun Cho and Iryna Gurevych},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.00247}
}

@article{panda,
  title={PANDA: Prompt Transfer Meets Knowledge Distillation for Efficient Model Adaptation},
  author={Qihuang Zhong and Liang Ding and Juhua Liu and Bo Du and Dacheng Tao},
  journal={ArXiv},
  year={2022},
  volume={abs/2208.10160}
}

@inproceedings{sparse_adapter,
    title = "{S}parse{A}dapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters",
    author = "He, Shwai  and
      Ding, Liang  and
      Dong, Daize  and
      Zhang, Jeremy  and
      Tao, Dacheng",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.160",
    pages = "2184--2190",
}


@article{delta_tuning,
  title={Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models},
  author={Ning Ding and Yujia Qin and Guang Yang and Fu Wei and Zonghan Yang and Yusheng Su and Shengding Hu and Yulin Chen and Chi-Min Chan and Weize Chen and Jing Yi and Weilin Zhao and Xiaozhi Wang and Zhiyuan Liu and Haitao Zheng and Jianfei Chen and Yang Liu and Jie Tang and Juan Li and Maosong Sun},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.06904}
}

@inproceedings{compacter,
 author = {Karimi Mahabadi, Rabeeh and Henderson, James and Ruder, Sebastian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {1022--1035},
 publisher = {Curran Associates, Inc.},
 title = {Compacter: Efficient Low-Rank Hypercomplex Adapter Layers},
 url = {https://proceedings.neurips.cc/paper/2021/file/081be9fdff07f3bc808f935906ef70c0-Paper.pdf},
 volume = {34},
 year = {2021}
}


@article{lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Weizhu Chen},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.09685}
}

@article{bitfit,
  title={BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models},
  author={Elad Ben-Zaken and Shauli Ravfogel and Yoav Goldberg},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.10199}
}

@inproceedings{adapters,
  title={Parameter-Efficient Transfer Learning for NLP},
  author={Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
  booktitle={International Conference on Machine Learning},
  year={2019}
}

@article{switch,
  title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author={William Fedus and Barret Zoph and Noam M. Shazeer},
  journal={ArXiv},
  year={2021},
  volume={abs/2101.03961}
}

@article{megatron,
  title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author={Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.08053}
}

@article{palm,
  title={PaLM: Scaling Language Modeling with Pathways},
  author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam M. Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Benton C. Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garc{\'i}a and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark D{\'i}az and Orhan Firat and Michele Catasta and Jason Wei and Kathleen S. Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.02311}
}

@article{Ding2022DeltaTuning,
  title={Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models},
  author={Ning Ding and Yujia Qin and Guang Yang and Fu Wei and Zonghan Yang and Yusheng Su and Shengding Hu and Yulin Chen and Chi-Min Chan and Weize Chen and Jing Yi and Weilin Zhao and Xiaozhi Wang and Zhiyuan Liu and Haitao Zheng and Jianfei Chen and Yang Liu and Jie Tang and Juan Li and Maosong Sun},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.06904v1}
}

@misc{zeng2022glm130b,
    title={GLM-130B: An Open Bilingual Pre-trained Model},
    author={Aohan Zeng and Xiao Liu and Zhengxiao Du and Zihan Wang and Hanyu Lai and Ming Ding and Zhuoyi Yang and Yifan Xu and Wendi Zheng and Xiao Xia and Weng Lam Tam and Zixuan Ma and Yufei Xue and Jidong Zhai and Wenguang Chen and Peng Zhang and Yuxiao Dong and Jie Tang},
    year={2022},
    eprint={2210.02414v1},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{zhang2022opt,
    title={OPT: Open Pre-trained Transformer Language Models},
    author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
    year={2022},
    eprint={2205.01068v4},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{bloom,
  title={BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  author={Teven Le Scao and Angela Fan and Christopher Akiki and Elizabeth-Jane Pavlick and Suzana Ili'c and Daniel Hesslow and Roman Castagn'e and Alexandra Sasha Luccioni and Franccois Yvon and Matthias Gall{\'e} and Jonathan Tow and Alexander M. Rush and Stella Rose Biderman and Albert Webson and Pawan Sasanka Ammanamanchi and Thomas Wang and Beno{\^i}t Sagot and Niklas Muennighoff and Albert Villanova del Moral and Olatunji Ruwase and Rachel Bawden and Stas Bekman and Angelina McMillan-Major and Iz Beltagy and Huu Nguyen and Lucile Saulnier and Samson Tan and Pedro Ortiz Suarez and Victor Sanh and Hugo Laurenccon and Yacine Jernite and Julien Launay and Margaret Mitchell and Colin Raffel and Aaron Gokaslan and Adi Simhi and Aitor Soroa Etxabe and Alham Fikri Aji and Amit Alfassy and Anna Rogers and Ariel Kreisberg Nitzav and Canwen Xu and Chenghao Mou and Chris C. Emezue and Christopher Klamm and Colin Leong and Daniel Alexander van Strien and David Ifeoluwa Adelani and Dragomir R. Radev and Eduardo G. Ponferrada and Efrat Levkovizh and Ethan Kim and Eyal Bar Natan and Francesco De Toni and G{\'e}rard Dupont and Germ{\'a}n Kruszewski and Giada Pistilli and Hady ElSahar and Hamza Benyamina and Hieu Tran and Ian Yu and Idris Abdulmumin and Isaac Johnson and Itziar Gonzalez-Dios and Javier de la Rosa and Jenny Chim and Jesse Dodge and Jian Zhu and Jonathan Chang and Jorg Frohberg and Josephine L. Tobing and Joydeep Bhattacharjee and Khalid Almubarak and Kimbo Chen and Kyle Lo and Leandro von Werra and Leon Weber and Long Phan and Loubna Ben Allal and Ludovic Tanguy and Manan Dey and Manuel Romero Mu{\~n}oz and Maraim Masoud and Mar'ia Grandury and Mario vSavsko and Max Huang and Maximin Coavoux and Mayank Singh and Mike Tian-Jian Jiang and Minh Chien Vu and Mohammad Ali Jauhar and Mustafa Ghaleb and Nishant Subramani and Nora Kassner and Nurulaqilla Khamis and Olivier Nguyen and Omar Espejel and Ona de Gibert and Paulo Villegas and Peter Henderson and Pierre Colombo and Priscilla Amuok and Quentin Lhoest and Rheza Harliman and Rishi Bommasani and Roberto L'opez and Rui Ribeiro and Salomey Osei and Sampo Pyysalo and Sebastian Nagel and Shamik Bose and Shamsuddeen Hassan Muhammad and Shanya Sharma and S. Longpre and Somaieh Nikpoor and Stanislav Silberberg and Suhas Pai and Sydney Zink and Tiago Timponi Torrent and Timo Schick and Tristan Thrush and Valentin Danchev and Vassilina Nikoulina and Veronika Laippala and Violette Lepercq and Vrinda Prabhu and Zaid Alyafeai and Zeerak Talat and Arun Raja and Benjamin Heinzerling and Chenglei Si and Elizabeth Salesky and Sabrina J. Mielke and Wilson Y. Lee and Abheesht Sharma and Andrea Santilli and Antoine Chaffin and Arnaud Stiegler and Debajyoti Datta and Eliza Szczechla and Gunjan Chhablani and Han Wang and Harshit Pandey and Hendrik Strobelt and Jason Alan Fries and Jos Rozen and Leo Gao and Lintang Sutawika and M Saiful Bari and Maged S. Al-shaibani and Matteo Manica and Nihal V. Nayak and Ryan Teehan and Samuel Albanie and Sheng Shen and Srulik Ben-David and Stephen H. Bach and Taewoon Kim and Tali Bers and Thibault F{\'e}vry and Trishala Neeraj and Urmish Thakker and Vikas Raunak and Xiang Tang and Zheng Xin Yong and Zhiqing Sun and Shaked Brody and Y Uri and Hadar Tojarieh and Adam Roberts and Hyung Won Chung and Jaesung Tae and Jason Phang and Ofir Press and Conglong Li and Deepak Narayanan and Hatim Bourfoune and Jared Casper and Jeff Rasley and Max Ryabinin and Mayank Mishra and Minjia Zhang and Mohammad Shoeybi and Myriam Peyrounette and Nicolas Patry and Nouamane Tazi and Omar Sanseviero and Patrick von Platen and Pierre Cornette and Pierre Franccois Lavall'ee and R{\'e}mi Lacroix and Samyam Rajbhandari and Sanchit Gandhi and Shaden Smith and St{\'e}phane Requena and Suraj Patil and Tim Dettmers and Ahmed Baruwa and Amanpreet Singh and Anastasia Cheveleva and Anne-Laure Ligozat and Arjun Subramonian and Aur'elie N'ev'eol and Charles Lovering and Daniel H Garrette and Deepak R. Tunuguntla and Ehud Reiter and Ekaterina Taktasheva and Ekaterina Voloshina and Eli Bogdanov and Genta Indra Winata and Hailey Schoelkopf and Jan-Christoph Kalo and Jekaterina Novikova and Jessica Zosa Forde and Jordan Clive and Jungo Kasai and Ken Kawamura and Liam Hazan and Marine Carpuat and Miruna Clinciu and Najoung Kim and Newton Cheng and Oleg Serikov and Omer Antverg and Oskar van der Wal and Rui Zhang and Ruochen Zhang and Sebastian Gehrmann and S. Osher Pais and Tatiana Shavrina and Thomas Scialom and Tian Yun and Tomasz Limisiewicz and Verena Rieser and Vitaly Protasov and Vladislav Mikhailov and Yada Pruksachatkun and Yonatan Belinkov and Zachary Bamberger and Zdenvek Kasner and Alice Rueda and Amanda Pestana and Amir Feizpour and Ammar Khan and Amy Faranak and Ananda Santa Rosa Santos and Anthony Hevia and Antigona Unldreaj and Arash Aghagol and Arezoo Abdollahi and Aycha Tammour and Azadeh HajiHosseini and Bahareh Behroozi and Benjamin Olusola Ajibade and Bharat Kumar Saxena and Carlos Mu{\~n}oz Ferrandis and Danish Contractor and David M. Lansky and Davis David and Douwe Kiela and Duong Anh Nguyen and Edward Tan and Emily Baylor and Ezinwanne Ozoani and Fatim T Mirza and Frankline Ononiwu and Habib Rezanejad and H.A. Jones and Indrani Bhattacharya and Irene Solaiman and Irina Sedenko and Isar Nejadgholi and Jan Passmore and Joshua Seltzer and Julio Bonis Sanz and Karen Fort and L{\'i}via Macedo Dutra and Mairon Samagaio and Maraim Elbadri and Margot Mieskes and Marissa Gerchick and Martha Akinlolu and Michael McKenna and Mike Qiu and M. K. K. Ghauri and Mykola Burynok and Nafis Abrar and Nazneen Rajani and Nour Elkott and Nourhan Fahmy and Olanrewaju Modupe Samuel and Ran An and R. P. Kromann and Ryan Hao and Samira Alizadeh and Sarmad Shubber and Silas L. Wang and Sourav Roy and Sylvain Viguier and Thanh-Cong Le and Tobi Oyebade and Trieu Nguyen Hai Le and Yoyo Yang and Zachary Kyle Nguyen and Abhinav Ramesh Kashyap and Alfredo Palasciano and Alison Callahan and Anima Shukla and Antonio Miranda-Escalada and Ayush Kumar Singh and Benjamin Beilharz and Bo Wang and Caio Matheus Fonseca de Brito and Chenxi Zhou and Chirag Jain and Chuxin Xu and Cl{\'e}mentine Fourrier and Daniel Le'on Perin'an and Daniel Molano and Dian Yu and Enrique Manjavacas and Fabio Barth and Florian Fuhrimann and Gabriel Altay and Giyaseddin Bayrak and Gully A. Burns and Helena U. Vrabec and Iman I.B. Bello and Isha Dash and Ji Soo Kang and John Giorgi and Jonas Golde and Jose David Posada and Karthi Sivaraman and Lokesh Bulchandani and Lu Liu and Luisa Shinzato and Madeleine Hahn de Bykhovetz and Maiko Takeuchi and Marc P{\`a}mies and Mar{\'i}a Andrea Castillo and Marianna Nezhurina and Mario Sanger and Matthias Samwald and Michael Cullan and Michael Weinberg and M Wolf and Mina Mihaljcic and Minna Liu and Moritz Freidank and Myungsun Kang and Natasha Seelam and Nathan Dahlberg and Nicholas Michio Broad and Nikolaus Muellner and Pascale Fung and Patricia Haller and R. Chandrasekhar and R. Eisenberg and Robert Martin and Rodrigo L. Canalli and Rosaline Su and Ruisi Su and Samuel Cahyawijaya and Samuele Garda and Shlok S Deshmukh and Shubhanshu Mishra and Sid Kiblawi and Simon Ott and Sinee Sang-aroonsiri and Srishti Kumar and Stefan Schweter and Sushil Pratap Bharati and T. A. Laud and Th'eo Gigant and Tomoya Kainuma and Wojciech Kusa and Yanis Labrak and Yashasvi Bajaj and Y. Venkatraman and Yifan Xu and Ying Xu and Yun-chao Xu and Zhee Xao Tan and Zhongli Xie and Zifan Ye and Mathilde Bras and Younes Belkada and Thomas Wolf},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.05100v2}
}
@misc{srivastava2022imitation,
    title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
    author={Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adrià Garriga-Alonso and Agnieszka Kluska and Aitor Lewkowycz and Akshat Agarwal and Alethea Power and Alex Ray and Alex Warstadt and Alexander W. Kocurek and Ali Safaya and Ali Tazarv and Alice Xiang and Alicia Parrish and Allen Nie and Aman Hussain and Amanda Askell and Amanda Dsouza and Ambrose Slone and Ameet Rahane and Anantharaman S. Iyer and Anders Andreassen and Andrea Madotto and Andrea Santilli and Andreas Stuhlmüller and Andrew Dai and Andrew La and Andrew Lampinen and Andy Zou and Angela Jiang and Angelica Chen and Anh Vuong and Animesh Gupta and Anna Gottardi and Antonio Norelli and Anu Venkatesh and Arash Gholamidavoodi and Arfa Tabassum and Arul Menezes and Arun Kirubarajan and Asher Mullokandov and Ashish Sabharwal and Austin Herrick and Avia Efrat and Aykut Erdem and Ayla Karakaş and B. Ryan Roberts and Bao Sheng Loe and Barret Zoph and Bartłomiej Bojanowski and Batuhan Özyurt and Behnam Hedayatnia and Behnam Neyshabur and Benjamin Inden and Benno Stein and Berk Ekmekci and Bill Yuchen Lin and Blake Howald and Cameron Diao and Cameron Dour and Catherine Stinson and Cedrick Argueta and César Ferri Ramírez and Chandan Singh and Charles Rathkopf and Chenlin Meng and Chitta Baral and Chiyu Wu and Chris Callison-Burch and Chris Waites and Christian Voigt and Christopher D. Manning and Christopher Potts and Cindy Ramirez and Clara E. Rivera and Clemencia Siro and Colin Raffel and Courtney Ashcraft and Cristina Garbacea and Damien Sileo and Dan Garrette and Dan Hendrycks and Dan Kilman and Dan Roth and Daniel Freeman and Daniel Khashabi and Daniel Levy and Daniel Moseguí González and Danielle Perszyk and Danny Hernandez and Danqi Chen and Daphne Ippolito and Dar Gilboa and David Dohan and David Drakard and David Jurgens and Debajyoti Datta and Deep Ganguli and Denis Emelin and Denis Kleyko and Deniz Yuret and Derek Chen and Derek Tam and Dieuwke Hupkes and Diganta Misra and Dilyar Buzan and Dimitri Coelho Mollo and Diyi Yang and Dong-Ho Lee and Ekaterina Shutova and Ekin Dogus Cubuk and Elad Segal and Eleanor Hagerman and Elizabeth Barnes and Elizabeth Donoway and Ellie Pavlick and Emanuele Rodola and Emma Lam and Eric Chu and Eric Tang and Erkut Erdem and Ernie Chang and Ethan A. Chi and Ethan Dyer and Ethan Jerzak and Ethan Kim and Eunice Engefu Manyasi and Evgenii Zheltonozhskii and Fanyue Xia and Fatemeh Siar and Fernando Martínez-Plumed and Francesca Happé and Francois Chollet and Frieda Rong and Gaurav Mishra and Genta Indra Winata and Gerard de Melo and Germán Kruszewski and Giambattista Parascandolo and Giorgio Mariani and Gloria Wang and Gonzalo Jaimovitch-López and Gregor Betz and Guy Gur-Ari and Hana Galijasevic and Hannah Kim and Hannah Rashkin and Hannaneh Hajishirzi and Harsh Mehta and Hayden Bogar and Henry Shevlin and Hinrich Schütze and Hiromu Yakura and Hongming Zhang and Hugh Mee Wong and Ian Ng and Isaac Noble and Jaap Jumelet and Jack Geissinger and Jackson Kernion and Jacob Hilton and Jaehoon Lee and Jaime Fernández Fisac and James B. Simon and James Koppel and James Zheng and James Zou and Jan Kocoń and Jana Thompson and Jared Kaplan and Jarema Radom and Jascha Sohl-Dickstein and Jason Phang and Jason Wei and Jason Yosinski and Jekaterina Novikova and Jelle Bosscher and Jennifer Marsh and Jeremy Kim and Jeroen Taal and Jesse Engel and Jesujoba Alabi and Jiacheng Xu and Jiaming Song and Jillian Tang and Joan Waweru and John Burden and John Miller and John U. Balis and Jonathan Berant and Jörg Frohberg and Jos Rozen and Jose Hernandez-Orallo and Joseph Boudeman and Joseph Jones and Joshua B. Tenenbaum and Joshua S. Rule and Joyce Chua and Kamil Kanclerz and Karen Livescu and Karl Krauth and Karthik Gopalakrishnan and Katerina Ignatyeva and Katja Markert and Kaustubh D. Dhole and Kevin Gimpel and Kevin Omondi and Kory Mathewson and Kristen Chiafullo and Ksenia Shkaruta and Kumar Shridhar and Kyle McDonell and Kyle Richardson and Laria Reynolds and Leo Gao and Li Zhang and Liam Dugan and Lianhui Qin and Lidia Contreras-Ochando and Louis-Philippe Morency and Luca Moschella and Lucas Lam and Lucy Noble and Ludwig Schmidt and Luheng He and Luis Oliveros Colón and Luke Metz and Lütfi Kerem Şenel and Maarten Bosma and Maarten Sap and Maartje ter Hoeve and Maheen Farooqi and Manaal Faruqui and Mantas Mazeika and Marco Baturan and Marco Marelli and Marco Maru and Maria Jose Ramírez Quintana and Marie Tolkiehn and Mario Giulianelli and Martha Lewis and Martin Potthast and Matthew L. Leavitt and Matthias Hagen and Mátyás Schubert and Medina Orduna Baitemirova and Melody Arnaud and Melvin McElrath and Michael A. Yee and Michael Cohen and Michael Gu and Michael Ivanitskiy and Michael Starritt and Michael Strube and Michał Swędrowski and Michele Bevilacqua and Michihiro Yasunaga and Mihir Kale and Mike Cain and Mimee Xu and Mirac Suzgun and Mo Tiwari and Mohit Bansal and Moin Aminnaseri and Mor Geva and Mozhdeh Gheini and Mukund Varma T and Nanyun Peng and Nathan Chi and Nayeon Lee and Neta Gur-Ari Krakover and Nicholas Cameron and Nicholas Roberts and Nick Doiron and Nikita Nangia and Niklas Deckers and Niklas Muennighoff and Nitish Shirish Keskar and Niveditha S. Iyer and Noah Constant and Noah Fiedel and Nuan Wen and Oliver Zhang and Omar Agha and Omar Elbaghdadi and Omer Levy and Owain Evans and Pablo Antonio Moreno Casares and Parth Doshi and Pascale Fung and Paul Pu Liang and Paul Vicol and Pegah Alipoormolabashi and Peiyuan Liao and Percy Liang and Peter Chang and Peter Eckersley and Phu Mon Htut and Pinyu Hwang and Piotr Miłkowski and Piyush Patil and Pouya Pezeshkpour and Priti Oli and Qiaozhu Mei and Qing Lyu and Qinlang Chen and Rabin Banjade and Rachel Etta Rudolph and Raefer Gabriel and Rahel Habacker and Ramón Risco Delgado and Raphaël Millière and Rhythm Garg and Richard Barnes and Rif A. Saurous and Riku Arakawa and Robbe Raymaekers and Robert Frank and Rohan Sikand and Roman Novak and Roman Sitelew and Ronan LeBras and Rosanne Liu and Rowan Jacobs and Rui Zhang and Ruslan Salakhutdinov and Ryan Chi and Ryan Lee and Ryan Stovall and Ryan Teehan and Rylan Yang and Sahib Singh and Saif M. Mohammad and Sajant Anand and Sam Dillavou and Sam Shleifer and Sam Wiseman and Samuel Gruetter and Samuel R. Bowman and Samuel S. Schoenholz and Sanghyun Han and Sanjeev Kwatra and Sarah A. Rous and Sarik Ghazarian and Sayan Ghosh and Sean Casey and Sebastian Bischoff and Sebastian Gehrmann and Sebastian Schuster and Sepideh Sadeghi and Shadi Hamdan and Sharon Zhou and Shashank Srivastava and Sherry Shi and Shikhar Singh and Shima Asaadi and Shixiang Shane Gu and Shubh Pachchigar and Shubham Toshniwal and Shyam Upadhyay and Shyamolima and Debnath and Siamak Shakeri and Simon Thormeyer and Simone Melzi and Siva Reddy and Sneha Priscilla Makini and Soo-Hwan Lee and Spencer Torene and Sriharsha Hatwar and Stanislas Dehaene and Stefan Divic and Stefano Ermon and Stella Biderman and Stephanie Lin and Stephen Prasad and Steven T. Piantadosi and Stuart M. Shieber and Summer Misherghi and Svetlana Kiritchenko and Swaroop Mishra and Tal Linzen and Tal Schuster and Tao Li and Tao Yu and Tariq Ali and Tatsu Hashimoto and Te-Lin Wu and Théo Desbordes and Theodore Rothschild and Thomas Phan and Tianle Wang and Tiberius Nkinyili and Timo Schick and Timofei Kornev and Timothy Telleen-Lawton and Titus Tunduny and Tobias Gerstenberg and Trenton Chang and Trishala Neeraj and Tushar Khot and Tyler Shultz and Uri Shaham and Vedant Misra and Vera Demberg and Victoria Nyamai and Vikas Raunak and Vinay Ramasesh and Vinay Uday Prabhu and Vishakh Padmakumar and Vivek Srikumar and William Fedus and William Saunders and William Zhang and Wout Vossen and Xiang Ren and Xiaoyu Tong and Xinran Zhao and Xinyi Wu and Xudong Shen and Yadollah Yaghoobzadeh and Yair Lakretz and Yangqiu Song and Yasaman Bahri and Yejin Choi and Yichi Yang and Yiding Hao and Yifu Chen and Yonatan Belinkov and Yu Hou and Yufang Hou and Yuntao Bai and Zachary Seid and Zhuoye Zhao and Zijian Wang and Zijie J. Wang and Zirui Wang and Ziyi Wu},
    year={2022},
    eprint={2206.04615},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@article{chow2016AM,
  title={A “bag-of-arguments” mechanism for initial verb predictions},
  author={Wing-Yee Chow and Cybelle Smith and Ellen F. Lau and Colin Phillips},
  journal={Language, Cognition and Neuroscience},
  year={2016},
  volume={31},
  pages={577 - 596}
}

@article{talmor2019olmpics,
  title={oLMpics--On what Language Model Pre-training Captures},
  author={Talmor, Alon and Elazar, Yanai and Goldberg, Yoav and Berant, Jonathan},
  journal={arXiv preprint arXiv:1912.13283},
  year={2019}
}

@inproceedings{kobayashi2020attention,
  title={Attention is not only a weight: Analyzing transformers with vector norms},
  author={Kobayashi, Goro and Kuribayashi, Tatsuki and Yokoi, Sho and Inui, Kentaro},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={7057--7075},
  year={2020}
}

@article{voita2019analyzing,
  title={Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  journal={arXiv preprint arXiv:1905.09418},
  year={2019}
}

@inproceedings{devlin2018bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@article{raffel2019exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@inproceedings{brown2020language_gpt3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{michel2019sixteen,
 author = {Michel, Paul and Levy, Omer and Neubig, Graham},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Are Sixteen Heads Really Better than One?},
 url = {https://proceedings.neurips.cc/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf},
 volume = {32},
 year = {2019}
}



@inproceedings{prasanna2020bert,
  title={When BERT Plays the Lottery, All Tickets Are Winning},
  author={Prasanna, Sai and Rogers, Anna and Rumshisky, Anna},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={3208--3229},
  year={2020}
}

@inproceedings{liu2019linguistic,
  title={Linguistic Knowledge and Transferability of Contextual Representations},
  author={Liu, Nelson F and Gardner, Matt and Belinkov, Yonatan and Peters, Matthew E and Smith, Noah A},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={1073--1094},
  year={2019}
}


@article{tenney2019you,
  title={What do you learn from context? probing for sentence structure in contextualized word representations},
  author={Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R Thomas and Kim, Najoung and Van Durme, Benjamin and Bowman, Samuel R and Das, Dipanjan and others},
  journal={arXiv preprint arXiv:1905.06316},
  year={2019}
}

@inproceedings{clark2019does,
  title={What Does {B}{E}{R}{T} Look at? An Analysis of {B}{E}{R}{T}’s Attention},
  author={Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
  booktitle={Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={276--286},
  year={2019}
}


@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}

@inproceedings{ding2017visualizing,
  title={Visualizing and understanding neural machine translation},
  author={Ding, Yanzhuo and Liu, Yang and Luan, Huanbo and Sun, Maosong},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1150--1159},
  year={2017}
}

@article{lin2019open,
  title={Open sesame: Getting inside bert's linguistic knowledge},
  author={Lin, Yongjie and Tan, Yi Chern and Frank, Robert},
  journal={arXiv preprint arXiv:1906.01698},
  year={2019}
}

@article{vig2019analyzing,
  title={Analyzing the structure of attention in a transformer language model},
  author={Vig, Jesse and Belinkov, Yonatan},
  journal={arXiv preprint arXiv:1906.04284},
  year={2019}
}

@article{clark2020electra,
  title={Electra: Pre-training text encoders as discriminators rather than generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  journal={arXiv preprint arXiv:2003.10555},
  year={2020}
}

@article{dong2019unified,
  title={Unified language model pre-training for natural language understanding and generation},
  author={Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  journal={arXiv preprint arXiv:1905.03197},
  year={2019}
}

@article{peters2018deep,
  title={Deep contextualized word representations},
  author={Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1802.05365},
  year={2018}
}

@article{wallace2019nlp,
  title={Do nlp models know numbers? probing numeracy in embeddings},
  author={Wallace, Eric and Wang, Yizhong and Li, Sujian and Singh, Sameer and Gardner, Matt},
  journal={arXiv preprint arXiv:1909.07940},
  year={2019}
}

@article{durrani2021transfer,
  title={How transfer learning impacts linguistic knowledge in deep NLP models?},
  author={Durrani, Nadir and Sajjad, Hassan and Dalvi, Fahim},
  journal={arXiv preprint arXiv:2105.15179},
  year={2021}
}

article{tenney2019you,
  title={What do you learn from context? probing for sentence structure in contextualized word representations},
  author={Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R Thomas and Kim, Najoung and Van Durme, Benjamin and Bowman, Samuel R and Das, Dipanjan and others},
  journal={arXiv preprint arXiv:1905.06316},
  year={2019}
}

@article{mccoy2019right,
  title={Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference},
  author={McCoy, R Thomas and Pavlick, Ellie and Linzen, Tal},
  journal={arXiv preprint arXiv:1902.01007},
  year={2019}
}

@inproceedings{ross2019well,
  title={How well do NLI models capture verb veridicality?},
  author={Ross, Alexis and Pavlick, Ellie},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={2230--2240},
  year={2019}
}

@article{lai2017race,
  title={Race: Large-scale reading comprehension dataset from examinations},
  author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  journal={arXiv preprint arXiv:1704.04683},
  year={2017}
}

@article{jain2019attention,
  title={Attention is not explanation},
  author={Jain, Sarthak and Wallace, Byron C},
  journal={arXiv preprint arXiv:1902.10186},
  year={2019}
}

@inproceedings{serrano2019attention,
  title={Is Attention Interpretable?},
  author={Serrano, Sofia and Smith, Noah A},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={2931--2951},
  year={2019}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{kovaleva2021busters,
    title = "{BERT} Busters: Outlier Dimensions that Disrupt Transformers",
    author = "Kovaleva, Olga  and
      Kulshreshtha, Saurabh  and
      Rogers, Anna  and
      Rumshisky, Anna",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.300",
    doi = "10.18653/v1/2021.findings-acl.300",
    pages = "3392--3405",
}

@article{kovaleva2019revealing,
  title={Revealing the dark secrets of BERT},
  author={Kovaleva, Olga and Romanov, Alexey and Rogers, Anna and Rumshisky, Anna},
  journal={arXiv preprint arXiv:1908.08593},
  year={2019}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{wang2019superglue,
  title={Superglue: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1905.00537},
  year={2019}
}

@article{hermann2015teaching,
  title={Teaching machines to read and comprehend},
  author={Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
  journal={Advances in neural information processing systems},
  volume={28},
  pages={1693--1701},
  year={2015}
}

@misc{schick2020fewshot,
    title={Few-Shot Text Generation with Pattern-Exploiting Training},
    author={Timo Schick and Hinrich Schütze},
    year={2020},
    eprint={2012.11926},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


@misc{kaplan2020scaling,
    title={Scaling Laws for Neural Language Models},
    author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
    year={2020},
    eprint={2001.08361},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{ettinger2020bert,
      title={What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models}, 
      author={Allyson Ettinger},
      year={2020},
      eprint={1907.13528},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Soler2020BertKnows,
   title={BERT Knows Punta Cana is not just beautiful, it’s gorgeous: Ranking Scalar Adjectives with Contextualised Representations},
   url={http://dx.doi.org/10.18653/v1/2020.emnlp-main.598},
   DOI={10.18653/v1/2020.emnlp-main.598},
   journal={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
   publisher={Association for Computational Linguistics},
   author={Garí Soler, Aina and Apidianaki, Marianna},
   year={2020}
}

@inproceedings{ravichander2020systematicity,
    title = "On the Systematicity of Probing Contextualized Word Representations: The Case of Hypernymy in {BERT}",
    author = "Ravichander, Abhilasha  and
      Hovy, Eduard  and
      Suleman, Kaheer  and
      Trischler, Adam  and
      Cheung, Jackie Chi Kit",
    booktitle = "Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.starsem-1.10",
    pages = "88--102",
}

@inproceedings{Zagoury2021WhatsTB,
  title={What's the best place for an AI conference, Vancouver or \_\_\_\_\_\_: Why completing comparative questions is difficult},
  author={Avishai Zagoury and Einat Minkov and Idan Szpektor and William W. Cohen},
  booktitle={AAAI},
  year={2021}
}

@inproceedings{Kassner2020ArePL,
  title={Are Pretrained Language Models Symbolic Reasoners over Knowledge?},
  author={Nora Kassner and Benno Krojer and Hinrich Sch{\"u}tze},
  booktitle={CONLL},
  year={2020}
}

@inproceedings{Mohebbi2021ExploringTR,
  title={Exploring the Role of BERT Token Representations to Explain Sentence Probing Results},
  author={Hosein Mohebbi and Ali Modarressi and Mohammad Taher Pilehvar},
  booktitle={EMNLP},
  year={2021}
}

@article{Clark2020TransformersAS,
  title={Transformers as Soft Reasoners over Language},
  author={Peter E. Clark and Oyvind Tafjord and Kyle Richardson},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.05867}
}

@article{Liu2021ProbingAT,
  title={Probing Across Time: What Does RoBERTa Know and When?},
  author={Leo Z. Liu and Yizhong Wang and Jungo Kasai and Hannaneh Hajishirzi and Noah A. Smith},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.07885}
}

@inproceedings{Zhou2021RICAER,
  title={RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms},
  author={Pei Zhou and Rahul Khanna and Bill Yuchen Lin and Daniel Ho and Jay Pujara and Xiang Ren},
  booktitle={EMNLP},
  year={2021}
}

@inproceedings{Ilharco2021ProbingCL,
  title={Probing Contextual Language Models for Common Ground with Visual Representations},
  author={Gabriel Ilharco and Rowan Zellers and Ali Farhadi and Hannaneh Hajishirzi},
  booktitle={NAACL},
  year={2021}
}

@article{Rajaee2021HowDF,
  title={How Does Fine-tuning Affect the Geometry of Embedding Space: A Case Study on Isotropy},
  author={S. Rajaee and Mohammad Taher Pilehvar},
  journal={ArXiv},
  year={2021},
  volume={abs/2109.04740}
}

@inproceedings{Mosbach2020OnTI,
  title={On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers},
  author={Marius Mosbach and Anna Khokhlova and Michael A. Hedderich and Dietrich Klakow},
  booktitle={FINDINGS},
  year={2020}
}

@article{Phang2021FineTunedTS,
  title={Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers},
  author={Jason Phang and Haokun Liu and Samuel R. Bowman},
  journal={ArXiv},
  year={2021},
  volume={abs/2109.08406}
}

@article{Jiang2021HowCW,
  title={How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering},
  author={Zhengbao Jiang and J. Araki and Haibo Ding and Graham Neubig},
  journal={Transactions of the Association for Computational Linguistics},
  year={2021},
  volume={9},
  pages={962-977}
}


@misc{Gokaslan2019OpenWeb,
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
	year={2019}
}

@misc{trinh2018simple,
    title={A Simple Method for Commonsense Reasoning},
    author={Trieu H. Trinh and Quoc V. Le},
    year={2018},
    eprint={1806.02847},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@software{gpt-neo,
  author       = {Black, Sid and
                  Gao, Leo and
                  Wang, Phil and
                  Leahy, Connor and
                  Biderman, Stella},
  title        = {{GPT-Neo: Large Scale Autoregressive Language 
                   Modeling with Mesh-Tensorflow}},
  month        = mar,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {1.0},
  doi          = {10.5281/zenodo.5297715},
  url          = {https://doi.org/10.5281/zenodo.5297715}
}

@article{gao2020pile,
  title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@misc{shazeer2020glu,
    title={GLU Variants Improve Transformer},
    author={Noam Shazeer},
    year={2020},
    eprint={2002.05202},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{Hendrycks2016BridgingNA,
  title={Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units},
  author={Dan Hendrycks and Kevin Gimpel},
  journal={ArXiv},
  year={2016},
  volume={abs/1606.08415}
}

@article{you2019large,
  title={Large batch optimization for deep learning: Training bert in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1904.00962},
  year={2019}
}

@inproceedings{kobayashi2021IncorporatingRA,
  title={Incorporating Residual and Normalization Layers into Analysis of Masked Language Models},
  author={Goro Kobayashi and Tatsuki Kuribayashi and Sho Yokoi and Kentarou Inui},
  booktitle={EMNLP},
  year={2021}
}

@inproceedings{luo2021positional,
  title={Positional artefacts propagate through masked language model embeddings},
  author={Luo, Ziyang and Kulmizev, Artur and Mao, Xiaoxi},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={5312--5327},
  year={2021}
}

@article{brunner2020OnIdentifiability,
  title={On Identifiability in Transformers},
  author={Gino Brunner and Yang Liu and Damian Pascual and Oliver Richter and Massimiliano Ciaramita and Roger Wattenhofer},
  journal={arXiv: Computation and Language},
  year={2020}
}

@inproceedings{Howard2018UniversalLM,
  title={Universal Language Model Fine-tuning for Text Classification},
  author={Jeremy Howard and Sebastian Ruder},
  booktitle={ACL},
  year={2018}
}

@inproceedings{Peters2018DeepCW,
  title={Deep Contextualized Word Representations},
  author={Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
  booktitle={NAACL},
  year={2018}
}

@inproceedings{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018}
}

@inproceedings{mccoy-etal-2019-right,
    title = "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
    author = "McCoy, Tom  and
      Pavlick, Ellie  and
      Linzen, Tal",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1334",
    doi = "10.18653/v1/P19-1334",
    pages = "3428--3448",
}

@article{Goldberg2019AssessingBS,
  title={Assessing BERT's Syntactic Abilities},
  author={Yoav Goldberg},
  journal={ArXiv},
  year={2019},
  volume={abs/1901.05287}
}

@inproceedings{Conneau2018WhatYC,
  title={What you can cram into a single \$\&!\#* vector: Probing sentence embeddings for linguistic properties},
  author={Alexis Conneau and Germ{\'a}n Kruszewski and Guillaume Lample and Lo{\"i}c Barrault and Marco Baroni},
  booktitle={ACL},
  year={2018}
}

@article{federmeier1999rose,
  title={A rose by any other name: Long-term memory structure and sentence processing},
  author={Federmeier, Kara D and Kutas, Marta},
  journal={Journal of memory and Language},
  volume={41},
  number={4},
  pages={469--495},
  year={1999},
  publisher={Elsevier}
}

@article{chow2016bag,
  title={A “bag-of-arguments” mechanism for initial verb predictions},
  author={Chow, Wing-Yee and Smith, Cybelle and Lau, Ellen and Phillips, Colin},
  journal={Language, Cognition and Neuroscience},
  volume={31},
  number={5},
  pages={577--596},
  year={2016},
  publisher={Taylor \& Francis}
}

@article{fischler1983brain,
  title={Brain potentials related to stages of sentence verification},
  author={Fischler, Ira and Bloom, Paul A and Childers, Donald G and Roucos, Salim E and Perry Jr, Nathan W},
  journal={Psychophysiology},
  volume={20},
  number={4},
  pages={400--409},
  year={1983},
  publisher={Wiley Online Library}
}


@inproceedings{card-etal-2020-little_power,
    title = "With Little Power Comes Great Responsibility",
    author = "Card, Dallas  and
      Henderson, Peter  and
      Khandelwal, Urvashi  and
      Jia, Robin  and
      Mahowald, Kyle  and
      Jurafsky, Dan",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.745",
    doi = "10.18653/v1/2020.emnlp-main.745",
    pages = "9263--9274",
}
