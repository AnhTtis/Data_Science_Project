{
    "arxiv_id": "2303.15647",
    "paper_title": "Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning",
    "authors": [
        "Vladislav Lialin",
        "Vijeta Deshpande",
        "Anna Rumshisky"
    ],
    "submission_date": "2023-03-28",
    "revised_dates": [
        "2023-03-29"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CL"
    ],
    "abstract": "This paper presents a systematic overview and comparison of parameter-efficient fine-tuning methods covering over 40 papers published between February 2019 and February 2023. These methods aim to resolve the infeasibility and impracticality of fine-tuning large language models by only training a small set of parameters. We provide a taxonomy that covers a broad range of methods and present a detailed method comparison with a specific focus on real-life efficiency and fine-tuning multibillion-scale language models.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.15647v1"
    ],
    "publication_venue": null
}