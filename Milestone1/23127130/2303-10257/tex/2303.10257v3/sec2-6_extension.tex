\subsubsection{Stochastic control with delay}\label{sec:sc_delay}

In direct parameterization approaches as discussed above, the neural network is used to approximate the optimal control or the value function. The type of inputs depends on the class of controls considered in the optimal control problem. For Markovian controls, which are used in \cite{han2016deep-googlecitations} and \cite{bachouch2021deepnumerical}, the input shall be $\checkX_{t_n}$ for $\alpha_{t_n}(\cdot; \theta_n)$. For open-loop controls, one can naturally consider $(\check W_{t_0}, \ldots, \check W_{t_n})$, while for closed-loop controls, one can use $(\checkX_{t_0}, \ldots, \checkX_{t_n})$ as inputs of the neural network. 
However, the sequence length increases with the number of time steps, which leads to a high computational cost. Furthermore, passing the whole sequence as a single input does not make use of the time structure. For these reasons, other architectures have been considered.


We now illustrate the direct parameterization methods by studying SC problems with delay. Such problems have found many applications, \textit{e.g.}, modeling systems with the aftereffect in mechanics and engineering, biology, and medicine \cite[Chapter 1]{kolmanovskiui1996control}, time-to-build problems in economics \cite{kydland1982time,asea1999time}, modeling the ``carryover'' or ``distributed lag''   advertising effect in marketing \cite{gozzi200513,gozzi2009controlled}, and portfolio selection under the market with memory and delayed responses in finance \cite{oksendal2000maximum,federico2011stochastic,elsanosi2001optimal,li2018portfolio}. 



Note that feedforward neural network (FNN) architecture is the most common architecture in deep learning and performs well as a function approximator of Markovian controls. Another popular type of neural networks is recurrent neural networks (RNNs). In a study by Han and Hu \cite{han2020rnn}, it is shown that RNNs have better performance in control problems with a delay effect. 


To distinguish between the value of a process at a given time and the portion of trajectory ending at time $t$ with $\delta$ history, for any process $P$, we denote by $\underline P_t = (\underline P_t\bigl(s)\bigr)_{s \in [-\delta, 0]}$ the trajectory of $P$ from time $t-\delta$ to $t$, \emph{i.e.}, $\underline P_t(s) = P_{t+s}$, for $-\delta \leq s \leq 0$. Specifically, we consider a SC problem in which the state process $X$ is characterized by a stochastic delay differential equation (SDDE),
\begin{equation}\label{def:delay-Xt}
\begin{dcases}
    \ud X_t = b(t, \underline X_{t}, \ctrl_t) \ud t + \sigma(t, \underline X_t, \ctrl_t) \ud W_t,  & t \in [0, T],\\
    X_t = \varphi_t, & t \in [-\delta, 0],
\end{dcases}
\end{equation}
and the objective function is given by
\begin{equation}\label{def:delay-cost}
J(\alpha) = \EE\left[\int_0^T f(t, \underline X_t, \ctrl_t) \ud t  + g(\underline X_T)\right].
\end{equation}
Here $\delta \geq 0$ is the fixed delay, and $(\varphi_t)_{t \in [-\delta, 0]}$ is a given process on $[-\delta, 0]$ for the initial condition of $X$, and $X_t$ denotes the value of the state process at time $t$ as usual. The SDDE \eqref{def:delay-Xt} has been well studied in the literature \cite{mohammed1984stochastic,mohammed1998stochastic} (see also Appendix~\ref{supp:SDDE} some preliminaries on SDDEs).


The key difficulty is that, when optimizing over closed-loop controls, one should, in principle, take into account the whole trajectory of the state, which is computationally costly. The authors in \cite{han2020rnn} analyzed this problem with a focus on the deep neural networks' (DNNs) architecture design in order to handle the high dimensionality arising from the delay. 
Without loss of generality, let us consider that the fixed delay $\delta <\infty$ covers $N_\delta$ subintervals, {\it i.e.}, $\delta = N_\delta \Delta t$, 
the partition on $[0,T]$ is extended to $[-\delta, 0]$,
\begin{equation}
 \; -\delta = t_{-N_\delta} \leq t_{-N_\delta+1} \leq \ldots \leq t_0 = 0, \text{ with } t_{n+1} - t_n \equiv \Delta t, \; \forall n = -N_\delta, \ldots, -1.
\end{equation}
and the discretized version of \eqref{def:delay-Xt}--\eqref{def:delay-cost} becomes
\begin{align}
    & \check X_{t_{n+1}} = \check X_{t_n} + b(t_n, \check{\underline{X}}_{t_n}, \ctrl_{t_n}) \Delta t + \sigma(t_n, \check{\underline X}_{t_n}, \ctrl_{t_n}) \Delta \check W_{t_n}, \label{def:Xtdiscrete} \\
    & \inf_{\{\ctrl_{t_n}\}_{n=0}^{N_T-1}} \EE\left[\sum_{n=0}^{N_T-1} f(t_n, \check{\underline X}_{t_n}, \ctrl_{t_n}) \Delta t + g(\check {\underline X}_T)\right], \label{def:costdiscrete}
\end{align}
where for each $n$,  $\check {\underline X}_{t_n}$ represents the discrete path with $N_\delta$ lags and $\Delta \check W_{t_n}$ is the increment in Brownian motions,
\begin{equation}
    \check {\underline X}_{t_n} = (\check X_{t_{n-N_\delta}},\ldots, \check X_{t_n}), \quad \Delta \check W_{t_n} = \check W_{t_{n+1}} - \check W_{t_n}.
\end{equation}
Here $\ctrl_{t_n}$ is a function of time and the past state trajectory $\check{\underline X}_{t_n}$ taking values in $\RR^k$. 
The next two architectures are proposed for approximating $\ctrl_{t_n}$. 

\medskip

\noindent \textbf{Feedforward architecture.}
Motivated by the path-dependent structure of the considered problems (the change of the current state only depends on the history up to lag $\delta$), a natural idea is to approximate $\ctrl_{t_n}$ by a feedforward neural network taking the state history up to lag $\bar{\delta}$ as the input. Note here it could be $\bar{\delta}\neq \delta$ since one may not know the underlying true $\delta$ a priori.
Without loss of generality, one can assume $\bar{\delta}=N_{\bar{\delta}}h~ (N_{\bar{\delta}}\in\mathbb{N}^+$) and define
$\bar{X}_{t_n} \equiv (\check X_{t_{n-N_{\bar{\delta}}}},\ldots, \check X_{t_n})\in \RR^{d\times (N_{\bar{\delta}}+1)}$. Then, the control at time $t_n$ can be approximated by a feedforward fully connected network taking $\bar X_{t_n}$ as input. 

\medskip

\noindent\textbf{Recurrent architecture.} 
Alternatively, the sequence $\bar X_{t_n}$ can be processed by a recurrent neural network (RNN), such as a long-short term memory (LSTM) neural network. The basic principle of an RNN is that the elements of the sequence are processed one by one by applying the same neural network in a recurrent manner, and some information is saved between two applications until the output is calculated. Here, we can use a single RNN for all time steps. At time $t_n$, to produce the value of the control, it uses $\check{X}_{t_n}$ as an input, along with the information saved from the previous time step. We refer to Appendix~\ref{sec:RNNdetails} and Appendix~\ref{sec:LSTMdetails} for more details. 



\begin{remark}
Although for both feedforward neural networks and RNN, the input dimensions remain constant as $k$ changes, using the former requires prior knowledge of $\delta$. For the feedforward fully connected neural network, one feeds the discretized state values \eqref{def:Xtdiscrete} of length $N_{\bar\delta} + 1$, so to obtain the best performance, one needs to get a good estimate $\bar\delta$ of $\delta$ first. On the other hand, for the RNN, one only needs to provide the current state value $X_{t_n}$. Notice that in an LSTM all input information up to time $t_n$ is summarized by the $n^{th}$ cell, but if the optimal control depends only on the past up to $\delta$, the forget gates allow to drop out the unneeded information. The exact way information should be dropped is determined by the neural network parameters training. %
Though the authors in \cite{han2020rnn} only experimented with LSTM, other variations of RNNs such as gated recurrent units (GRUs)~\cite{cho2014learning} or peephole LSTM~\cite{gers2002learning} can be considered. 
\end{remark}


Before illustrating the above method, we stress that other methods also exist. For example, \cite{lefebvre2021linear}, a PDE-based approach,  uses a deep learning technique inspired by the physics-informed neural networks (PINNs)  and applies it to solve a mean-variance portfolio selection with execution delay. One may also reformulate the delay problem using anticipated backward stochastic differential equations (ABSDEs) and solve it through BSDE-based methods. Along this line, \cite{fouque2019deep} addressed mean-field control with delay.




\paragraph*{Numerical illustration: a linear-quadratic regulator problem with delay.}  
LQ problems with delay were first investigated by Kolmanovski{\u{\i}} and Sha{\u{\i}}khet \cite{kolmanovskiui1996control}. In the version presented here, the aim is to minimize 
\begin{align}
&\EE_{\varphi} \left[\int_0^T (X_t + e^{\lambda \delta}A_3 Y_t)\transpose Q(t) (X_t + e^{\lambda \delta}A_3 Y_t) + \ctrl_t\transpose R(t)\ctrl_t\ud t \right.+ (X_T + e^{\lambda\delta} A_3Y_T)\transpose G (X_T + e^{\lambda\delta} A_3Y_T)\Bigg], \\
&\text{subject to } \ud X_t = (A_1(t)X_t + A_2(t)Y_t + A_3 Z_t + B(t)\ctrl_t) \ud t + \sigma(t) \ud W_t, \quad t  \in[0,T],\label{eq:lq}
\end{align}
where $X_0 = \varphi \in L^2(\Omega, C([-\delta, 0], \RR^d))$ is a given initial segment, $Y_t = \int_{-\delta}^0 e^{\lambda s} X_{t+s} \ud s$ is the distributed delay and $Z_t = X_{t-\delta}$ is the discrete delay,
$A_1, A_2, Q \in L^\infty([0,T]; \RR^{d\times d})$, $B \in L^\infty([0,T]; \RR^{d \times k})$, $R \in L^\infty([0,T]; \RR^{k\times k})$ are deterministic matrix-valued functions, $\sigma \in L^2([0,T]; \RR^{d \times m})$ is a deterministic matrix-valued function, $A_3, G \in \RR^{d \times d}$ are deterministic matrices. It is assumed that $Q(t), G$ are positive semi-definite and $R(t)$ is positive definite for all $t \in [0,T]$ and continuous on $[0,T]$. To have a tractable solution, a further relation is prescribed,
\begin{equation}\label{lq:parameters}
A_2(t) = e^{\lambda\delta}(\lambda I_d + A_1(t) + e^{\lambda\delta} A_3) A_3,
\end{equation}
where $I_d$ is the identity matrix with rank $d$. This example was studied in
\cite[Section 4]{bauer2005stochastic}, and the main results are also summarized in \cite{han2020rnn}. 

We present results below for a ten-dimensional example. The model parameters are chosen as follows. The dimensions are $d = k = m = 10$, and $\lambda=0.1$.
In~\eqref{eq:lq}, $A_1, A_3, B, \sigma$ are constant coefficient matrices (generated randomly), $Q, R, G$ are constant matrices proportional to identity matrices, and $A_2$ is determined by~\eqref{lq:parameters}. For implementation details, we refer to~\cite{han2020rnn}. The left panel of Figure~\ref{fig:lq_train_curve} displays the total cost of the validation data against training time. The values are averaged every 200 steps. The feedforward model takes the state history as inputs up to lag $\bar{\delta}=\delta$ with $N_{\bar{\delta}}=40$.
The right panel of Figure~\ref{fig:lq_train_curve} displays the optimized cost as a function of the processed lag time $\bar{\delta}$ from $0.2$ to $1$ with step size 0.1, while the actual lag $\delta=1$. If the chosen lag time $\bar{\delta}$ is smaller than the actual lag $\delta$ time, there is a loss of information when the feedforward network processes the data. As expected, we observe that the cost increases as the lag time processed by the feedforward model decreases. A higher optimized cost indicates that the model can only find a strictly sub-optimal strategy due to the lack of information. 
Figure~\ref{fig:lq_path_lstm_shff} displays one sample path (first five dimensions only) of the optimal state $X$ and control $\ctrl$ provided by two neural networks in comparison with the analytical solution, in which the LSTM architecture presents a better agreement. 
The lag time $\bar{\delta}$ processed by the feedforward model is chosen to be the same as $\delta$.
One main drawback of the feedforward model is that it requires to know the true lag time $\delta$ a priori to determine the network's size. 


\begin{figure}[!htb]
\centering
\includegraphics[width=0.45\textwidth]{figure/lq-d10_train_curve.pdf}
\includegraphics[width=0.45\textwidth]{figure/lq-d10_shff_lag.pdf}
  \caption{The linear-quadratic regulator problem with delay in Section~\ref{sec:sc_delay}. Left: Training curve of two models in the example of linear-quadratic problem. Right: The effect of lag time $\bar{\delta}$ processed by the feedforward model in the example of the linear-quadratic problem. The lag time $\delta$ in the actual system is $1$. %
  }
  \label{fig:lq_train_curve}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=0.98\textwidth, trim = {0, 18em, 0, 18em}, clip]{figure/lq-d10_lstm_path.pdf}\\
\includegraphics[width=0.98\textwidth,  trim = {0, 18em, 0, 18em}, clip]{figure/lq-d10_shff_path.pdf}
  \caption{The linear-quadratic regulator problem with delay in Section~\ref{sec:sc_delay}. 
      A sample path of the first 5 dimensions of the state $X_t$ and control $\ctrl_t$ obtained from the LSTM (top) model and FNN (top) model.
      Left: the optimal state process discretized from the analytical solution $(X_t)_i$ (solid lines) and its approximation $(\hat{X}_t)_i$ (dashed lines) provided by the approximating control, under the same realized path of Brownian motion.
      Right: comparisons of the optimal control $(\ctrl_t)_i$ (solid lines) and $(\hat{\ctrl}_t)_i$ (dashed lines). %
  }
  \label{fig:lq_path_lstm_shff}
\end{figure}




\subsubsection{Mean-field type control}\label{sec:directMethod}
We now discuss how direct parameterization methods can be adapted for mean field control (MFC) problems, which are an extension of standard optimal control in which there are mean field interactions. MFC can be interpreted as a social optimum but also has applications in risk, {\it e.g.}, heating or electric loads management~\cite{KIZILKALE20141867,Mathieu2013StateEA},  risk management in finance~\cite{MR2784835} or optimal control with a cost involving a conditional expectation~\cite{achdoulaurierelions2020optimal,MR4133380}, to cite just a few examples. We refer to~\cite{MR3134900} for more background on MFC.


As before, let $\mc{A} \subset \RR^k$ denote the set of admissible actions and let $\mathbb{A}$ be the set of so-called admissible controls. We denote by $\cP_2(\RR^d)$ the set of probability measures on $\RR^d$ which admit a second moment. Let $b, \sigma, f, g$ be Borel-measurable functions,
\begin{equation}
    (b, \sigma, f) : [0,T] \times \RR^d \times \cP_2(\RR^d) \times \mc{A} \to (\RR^d, \RR^{d \times m}, \RR), \quad g : \cP_2(\RR^d) \times \RR^d \to \RR.
\end{equation}
Compared with standard optimal control, see~\eqref{def:oc-bsigmaf}, the functions take as an extra input the population distribution.


\begin{defn}[MFC optimum]
\label{def:mfc-optimum}
A feedback control $\ctrl^* \in \mathbb{A}$ is an optimal control for the MFC problem for a given initial distribution $\mu_0 \in \cP_2(\RR^d)$ if it minimizes
\begin{align}
\label{subchapRCML-num-eq:def-J-MFC}
	J(\alpha): \ctrl \mapsto \EE \left[\int_0^T f(t, X_t, \mu_t, \ctrl_t ) dt + g(X_T, \mu_T) \right],
\end{align}
where $\mu_t$ is the probability distribution of the law of $X_t$, under the constraint that the process $X = (X_t)_{t \ge 0}$ solves the stochastic differential equation of the McKean-Vlasov (MKV) type,
\begin{equation}
\label{subchapRCML-num-eq:dyn-X-general-MFC}
	\ud X_t = b(t, X_t, \mu_t, \ctrl_t) \ud t + \sigma(t, X_t, \mu_t, \ctrl_t) \ud W_t, \qquad t \ge 0,
\end{equation}
with $X_0$ having distribution $\mu_0$. 
\end{defn}
Existence and uniqueness results have been studied in the literature, and we refer the interested reader to {\it e.g.}~\cite{carmona2018probabilistic2} and in particular the assumption  ``Control of MKV Dynamics'' on page 555. Furthermore, under mild conditions, there is an optimal control that is Markovian, so it is sufficient to focus on controls that are functions of time and the individual state. The framework can also encompass problems in which the interactions are through the joint distribution of states and actions and not just the distribution of states. Such problems are sometimes referred to as extended MFC problems. Although the numerical method discussed below can be adapted in a rather straightforward way (as illustrated below in an example below), the theoretical framework is more challenging. 



To solve the MFC problem using deep learning, one can for example follow the lines of the global approach described in Section~\ref{sec:direct-global-local} to discretize time and to replace the control by a sequence of neural network functions of the state. However, in contrast with standard OC problems, here, the costs and the dynamics can depend on mean field terms, so we also need to approximate the distribution of the state. To this end, The simplest approach is to use the empirical distribution of a population of particles. 

Following this approach, the original MFC problem is approximated by the following problem: minimize over the neural network parameters $\theta = (\theta_n)_{n=0,\dots,N_T-1}$
\begin{equation}
\label{subchapRCML-num-eq:NN-MFC-cost-totalapprox}
	\check J^{N}(\theta): \theta \mapsto \frac{1}{N} \sum_{i=1}^N \EE \left[\sum_{n=0}^{N_T-1} f(t_n, \check X_{t_n}^{i, \theta}, \check \mu^{N, \theta}_{t_n}, \ctrl_{t_n}(\check X_{t_n}^{i, \theta}; \theta_n) ) \Delta t + g(\check X_T^{i, \theta}, \check \mu^{N,\theta}_T) \right],
\end{equation}
subject to
\begin{equation}
\label{subchapRCML-num-eq:NN-MFC-Nparticles-Deltat}
	\check X_{t_{n+1}}^{i, \theta} = \check X_{t_{n}}^{i, \theta} + b(t_n,  \check X_{t_{n}}^{i, \theta}, \check \mu^{N, \theta}_{t_{n}}, \ctrl_{t_n}(\check X_{t_{n}}^{i, \theta}; \theta_n)) \Delta t + \sigma(t_n, \check X_{t_{n}}^{i, \theta}, \check \mu^{N, \theta}_{t_{n}}, \ctrl_{t_n}(\check X_{t_{n}}^{i, \theta}; \theta_n)) \Delta \check W^i_n, \quad n = 0, \dots, N_T-1,
\end{equation}
where the initial positions $(\check X_0^{i, \theta})_{i=1,\dots,N}$ are i.i.d. with distribution $\mu_0$, the empirical distribution $\mu^{N, \theta}_{t_{n}}$ is 
$$
	\check \mu^{N, \theta}_{t_{n}} = \frac{1}{N} \sum_{j=1}^N \delta_{\check X_{t_{n}}^{j, \theta}}, 
$$ 
and $(\Delta \check W_n^i)_{i=1,\dots,N,\;n=0,\dots,N_T-1}$ denotes i.i.d. random variables with Gaussian distribution $\mathcal N(0, \Delta t)$. Note that this problem is not equivalent to solving an $N$-agent optimal control problem since, in the latter case, the control would in general need to be a function of all the agents' states, while here, we consider only distributed controls, functions of each agent's state. Intuitively, this approximation is justified by the propagation of chaos results~\cite{sznitman1991topics}: as $N\to+\infty$ the particles become independent, and the empirical distribution converges to the distribution of the MKV SDE~\eqref{subchapRCML-num-eq:dyn-X-general-MFC}. See~\cite{lacker2017limit} in the context of MFC problems. 

This technique was proposed in~\cite{carmona2019convergence2} (with a single neural network function of time and space instead of a sequence of neural network functions of space online). Although we focus here on a basic setting for which a simple feedforward fully connected architecture performs well, other architectures may yield better results for problems with more complex time dependencies; see \textit{e.g.}, \cite{fouque2019deep,gomes2022machineprice} for applications with RNNs. See also~\cite{germain2019numerical,agrambakdioksendal2020deep} and the survey~\cite{carmonalauriere2021deepmfgsurvey} for more details. 



\begin{rem}[Theoretical analysis]
In \cite[Theorem 3]{carmona2019convergence2} Carmona and Lauri\`{e}re provided a proof of convergence  of the discrete problem to the original MFC problem in the following sense: under suitable assumptions, the difference between the optimal value of this problem $\inf_\theta \check J^{N}(\theta)$ and the optimal value of the original problem $\inf_\ctrl J(\ctrl)$ goes to $0$ as $N_T$, $N$ and the number of parameters in the neural network go to infinity. 
\end{rem}

From here, one can use SGD or one of its variants to optimize $\check J^N$ over the parameters. In contrast with the methods discussed previously for standard OC, here one sample corresponds to one population with $N$ particles. In this context, one sample corresponds to $\xi = ( \check X^{i}_0, (\Delta \check W^i_n)_{n=0,\dots,N_T-1})_{i=1,\dots,N}$, from which we can compute the $N$ (interacting) trajectories and the total empirical cost:
\begin{equation}
\label{subchapRCML-num-eq:NN-MFC-cost-totalapprox-oneS}
	\check J^{\xi, N}(\theta) = \frac{1}{N} \sum_{i=1}^N \left[\sum_{n=0}^{N_T-1} f(\check X_{t_n}^{i, \theta, \xi}, \check \mu^{N, \theta, \xi}_{t_n}, \ctrl_{t_n}(\check X_{t_n}^{i, \theta, \xi}; \theta_n) ) \Delta t + g(\check X_T^{i, \theta, \xi}, \check \mu^{N, \theta, \xi}_T) \right].
\end{equation}
From here, Algorithm~\ref{algo:SGD-generic} in Appendix~\ref{sec:SGD-var} can be applied.


Before illustrating the above method, we would like to note that there are both PDE-based and BSDE-based algorithms for solving MFC problems. These involve reformulating them into HJB and FBSDE systems. As these systems share similarities with those derived from MFG, interested readers are directed to the corresponding sections in Section~\ref{sec:MFG}.
 
\paragraph*{Numerical illustration: a mean-field price impact problem.} We now illustrate the method with a financial application on a problem of optimal execution. The model describes a large group of traders interacting through the price of an asset. The large group of traders has a non-negligible influence on the price, referred to as price impact. For example, when the traders decide to buy at the same moment, the price is driven up, and vice versa when the traders decide to sell simultaneously.

The $N$-agent problem was originally solved as a mean-field game (MFG) by Carmona and Lacker in the weak formulation (\cite{MR3325272}), and revisited in the book of Carmona and Delarue  \cite[Sections 1.3.2 and 4.7.1]{carmona2018probabilistic} in the strong formulation. Here, we focus on the mean-field control setting. In this model, the inventory representative trader is denoted by $X_t$. The control of the trader is denoted by $\ctrl_t$ and corresponds to the trading rate. The dynamics of the inventory are given by
\begin{equation*}
    \ud X_t = \ctrl_t \ud t +\sigma \ud W_t,
\end{equation*}
where $W$ is a standard Brownian motion, and the cost can be rewritten in terms of $X$ only as
\begin{equation*}
J(\ctrl)=\mathbb{E}\Big[ \int_0^T \left(c_{\ctrl}(\ctrl_t)+c_X(X_t)-\gamma X_t \int_{\mathbb{R}} a  \ud \nu^\ctrl_t(a)\right)\ud t +g(X_T)\Big].  
\end{equation*}
Following the Almgren-Chriss linear price impact model, we assume that the functions $c_X$, $c_{\ctrl}$ and $g$ are quadratic. Thus, the cost is of the form
\begin{equation}\label{eq:priceimpact}
J(\alpha)=\mathbb{E}\left[ \int_0^T \left( \frac{c_{\alpha}}{2}{\alpha_t}^2+\frac{c_X}{2}X_t^2-\gamma X_t\int_{\mathbb{R}} a \ud\nu^\ctrl_t(a) \right)\ud t + \frac{c_g}{2}X_T^2\right].
\end{equation}
This model has a semi-explicit solution obtained by reducing the problem to a system of ordinary differential equations (ODEs) as explained in~\cite[Sections 1.3.2 and 4.7.1]{carmona2018probabilistic}. 



The deep learning method described above can readily be adapted to solve MFC with interactions through the control's distribution by computing the empirical distribution of controls for an interacting system of $N$ particles. Figure~\ref{fig:ex-price-impact-1} shows the control and the distribution at various time steps. Here, we plot the values of the control represented by the neural network evaluated at samples generated by following the $N$-interacting particle system. We see that the shape is approximately linear at every time step, and furthermore that it coincides with the lines corresponding to the theoretical optimal solution. The distribution, represented by histograms computed using the $N$ particles, starts on the right and moves towards $0$, which can be interpreted as the fact that the traders liquidate their portfolios. We used the parameters: $T=1$, $c_X = 2$, $c_{\ctrl} = 1$, $c_g = 0.3$, $\sigma = 0.5$ and $\gamma = 0.2$.  When using a larger value of $\gamma$, the collective influence of the traders on the price is higher. As can be seen 
in Figure~\ref{fig:ex-price-impact-2} obtained with $\gamma=1$, the traders do not liquidate all the time from $t=0$ until time $t=T$. Instead, they liquidate their portfolio at the beginning and then start buying. We can explain this behavior as follows: since this is a cooperative problem, the traders can use the price impact to drive the price up to increase the value of the stock they own, thus increasing their final reward. 

\begin{figure}[ht]
  \subfloat[]{
  \includegraphics[width=0.45\linewidth]{{figure/PRICEIMPACTDIRECT/test2i2_hbar0p2_control_t45}.pdf}
    }
  \subfloat[]{
  \includegraphics[width=0.45\linewidth]{{figure/PRICEIMPACTDIRECT/test2i2_hbar0p2_density_t50}.pdf}
    }
\caption{Price impact MFC example in Section~\ref{sec:directMethod} solved by direct method. Left: Control learnt (dots) and exact solution (lines). Right: associated empirical state distribution. Here we take $\gamma = 0.2$ in \eqref{eq:priceimpact}. %
}
\label{fig:ex-price-impact-1}
\end{figure}






\begin{figure}[ht]
  \subfloat[]{
  \includegraphics[width=0.45\linewidth]{{figure/PRICEIMPACTDIRECT/test2h2_hbar1_control_t45}.pdf}
    }
  \subfloat[]{
  \includegraphics[width=0.45\linewidth]{{figure/PRICEIMPACTDIRECT/test2h2_hbar1_density_t50}.pdf}
    }
\caption{Price impact MFC example in Section~\ref{sec:directMethod} solved by direct method. Left: Control learned (dots) and exact solution (lines). Right: associated empirical state distribution. Here, $\gamma = 1$ in \eqref{eq:priceimpact}. %
}
\label{fig:ex-price-impact-2}
\end{figure}



