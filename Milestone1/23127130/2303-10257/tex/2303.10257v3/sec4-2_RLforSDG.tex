\subsubsection{Multi-agent reinforcement learning (MARL)}
Multi-agent reinforcement learning (MARL) studies reinforcement learning methods for multiple learners. The main difficulty is that, when several agents learn while interacting, from the point of view of each agent, the environment is non-stationary. Another issue is the question of scalability, which arises when the number of learners is very large. However, for a small number of agents, MARL has led to recent breakthrough results; see {\it e.g.} in autonomous driving ~\cite{shalev2016safe}, the game of Go~\cite{silver2016mastering}, or video games such as Star Craft~\cite{vinyals2019grandmaster}. 

Several viewpoints can be adopted. Relying on dynamical systems theory, one approach is to consider that each agent uses a learning algorithm, and to study the resulting behavior of the group of agents viewed as a system evolving in discrete or continuous time. Another approach, based on game theory and closer to the topics discussed in Section~\ref{sec:SDG}, is to look for notions of solutions such as Nash equilibria and to design algorithms that let the agents learn such solutions. A typical example is Nash Q-learning, in which every player runs their own version of Q-learning simultaneously with the other players. Each player tries to compute its optimal Q-function, but the optimal policy of player $i$ depends on the policies implemented by the other players. To be specific, consider an $N$-player game as in Section~\ref{sec:Nplayer} but now in discrete time. Note that the problem faced by player $i$ is not an MDP with state $X^i$ because the cost and dynamics of player $i$ depend on the other players. Assume the players use a strategy profile $\bm\pi = (\pi^1,\dots,\pi^N)$. Then the Q-function of player $i$ is: for $\bm{x} = (x^1,\dots,x^N)$ and $\bm{a} = (a^1,\dots,a^N)$,
\begin{equation}
    Q^{i,\bm\pi}_{t_n}(\bm{x}, \bm{a}) = \EE^{\bm\pi}\left[\sum_{j=n}^{N_T-1} f^i(t_j, \bm{X}_{t_j}, \bm{\alpha}_{t_j})\Delta t + g^i(\bm{X}_T) \Big\vert \bm{X}_{t_n} = \bm{x}, \; \bm{\alpha}_{t_n} = \bm{a}\right].
\end{equation}
Hu and Wellman proposed in~\cite{hu2003Nash} a version of Q-learning for (infinite horizon discounted) $N$-player games, called Nash Q-learning, and identified conditions under which this algorithm converges to a Nash equilibrium. The method can be adapted with deep neural networks, as done for instance in~\cite{casgrain2019deepnashq}.  We refer the interested reader to, {\it e.g.}, \cite{Busoniu08,tuyls2012multiagent,bloembergen2015evolutionary,lanctot2017unified,yang2020overview,zhang2021multi,gronauer2021multi} for more details on MARL. Recently, \cite{gu2021mean,gu2021mean2} also studied mean-field control RL in a decentralized way using cooperative MARL.










\subsubsection{Reinforcement learning for mean-field games}
\label{sec5_RLforMFG}

We now turn our attention to RL methods for MFG. As pointed out in Section~\ref{sec:MFG}, finding a mean-field Nash equilibrium boils down to (1) finding a control that is optimal for a representative infinitesimal player facing the equilibrium distribution flow, and (2) computing the induced distribution flow, which should match the equilibrium one. These two elements can be tackled alternatively, as described in Section~\ref{sec:Nplayer} in the N-player case and in Section~\ref{sec4_MFG_with_CN} in the mean-field case. The first part is a standard optimal control problem, which can thus be tackled using standard RL techniques, see Section~\ref{sec:MDP}. In this setting, we assume that the agent who is learning can repeat experiments of the following form: given the current state, the agent chooses an action (or a sequence of actions), and the environment returns the new state as well as the reward (or a sequence of states and rewards). In the representative player's MDP, the distribution enters as a parameter that influences the reward and dynamics, but is fixed when the player learns an optimal policy. During such experiments, we generally assume that the population distribution is fixed, and it is updated after a number of iterations; see {\it e.g.}~\cite{guo2019learning,EliePerolatLauriereGeistPietquin-2019_AFP-MFG}. Alternatively, we can assume that it is updated at every iteration but at a slow rate; see {\it e.g.}~\cite{subramanianpolicy,angiuli2020unified,xie2021learning}. Most of the literature thus far focuses on tabular methods. A few works have used deep RL methods to compute the best response. For example, DDPG has been used in~\cite{EliePerolatLauriereGeistPietquin-2019_AFP-MFG}, soft actor-critic (SAC) has been used for a flocking model in~\cite{perrin2021mfgflockrl}, while deep Q-learning or some variants of it has been used in~\cite{pmlr-cui21-approximately,perrin2021generalizationmfg,lauriere2022scalable}. Recently, several works have studied the advantages and the limitations brought by the regularization of the policy through penalization terms in the cost function~\cite{anahtarci2020mfgqregu,pmlr-cui21-approximately,guo2022mfgentropyregu}. We refer to~\cite{lauriere2022learningmfgsurvey} for a survey of learning algorithms and reinforcement learning methods to approximate MFG solutions. 




\paragraph*{Numerical illustration: an example with explicit solution.}
For the sake of illustration, we consider an MFG model which admits an explicit solution in the continuous time ergodic setting. The model has been introduced and solved in~\cite{almulla2017two}. The MFG is defined as follows. The state space is the one-dimensional unit torus, {\it i.e.}, $\mathbb{T} = [0,1]$ with periodic boundary conditions. The action space is $\mathbb{R}$ (or in practice any bounded interval containing $[-2\pi,2\pi]$, which is the range of the equilibrium control). The drift function is 
$$
  b(x,m,a) = a.
$$
The running cost is
$$
	f(x,m,a) = \tilde f(x) + \frac{1}{2}|a|^2 + \log(m),
$$
where the first term is the following cost, which encodes spatial preferences for some regions of the domain
$$
	\tilde f(x)= -2 \pi^2 \sin(2 \pi x) + 2 \pi^2 \cos(2 \pi x)^2 - 2 \sin(2 \pi x).
$$
In the ergodic MFG setting, the objective of an infinitesimal representative player is to minimize
$$
    \lim_{T \to +\infty}\frac{1}{T} \EE\left[\int_0^T f(X_t, \mu_t(X_t), \alpha_t(X_t)) \ud t \right],
$$
where $X$ is controlled by $\alpha$. Here $\mu_t$ is assumed to have a density for every $t \ge 0$, and we identify it with its density. So $\mu_t(X_t)$ denotes the value of the density of $\mu_t$ at $X_t$.
The equilibrium control and the equilibrium mean-field distribution are respectively given by
$$
  a^*:x\mapsto 2 \pi \cos(2\pi x)\;\quad \mbox{and} \;\quad 
 \mu^*:x\mapsto  \frac{e^{2 \sin(2\pi x)}}{\int_{\mathbb{T}} e^{2 \sin(2\pi y)} \ud y}\;.
$$
We use fictitious play~\cite{cardaliaguet2015learning} combined with a deep RL algorithm to learn the best response at each iteration for the solution. The problem is in continuous state and action spaces and admits a deterministic equilibrium control. Hence, following~\cite{EliePerolatLauriereGeistPietquin-2019_AFP-MFG}, at each iteration, we solve the representative player's MDP using DDPG~\cite{lillicrap2015continuous} reviewed in Section~\ref{sec:MDP-PGM}. 


The plots in Figure~\ref{fig:analytical-mfg-ddpg} are borrowed from~\cite{EliePerolatLauriereGeistPietquin-2019_AFP-MFG}. The left plot displays the $L^2$  distance between the true equilibrium control and the control learnt by the algorithm. The right plot shows the stationary distribution learnt by the algorithm, which is to be compared with the distribution described in~\cite{almulla2017two} for the ergodic problem. Although the two problems are slightly different (one being in the infinite horizon discounted setting and the other one in the ergodic setting),  we can see that the distribution has the same shape, for suitable choises of parameters.    
We refer to \cite{EliePerolatLauriereGeistPietquin-2019_AFP-MFG} for more details on the implementation and the choice of parameters and hyperparameters for the results shown in Figure~\ref{fig:analytical-mfg-ddpg}.
\begin{figure}[!htb]
\centering
\includegraphics[width=0.45\textwidth]%
{{figure/RL-MFG-ANALYTICAL/ddpg-mfg-aaaipaper-error_control}}%
\includegraphics[width=0.45\textwidth]%
{{figure/RL-MFG-ANALYTICAL/ddpg-mfg-aaaipaper-iteration_125}}
  \caption{
      MFG described in Section~\ref{sec5_RLforMFG}, solved with fictitious play and DDPG. Left: $L^2$ error on the analytical control; right: stationary distribution. Results obtained by 125 iterations of fictitious play. 
  }
  \label{fig:analytical-mfg-ddpg}
\end{figure}


