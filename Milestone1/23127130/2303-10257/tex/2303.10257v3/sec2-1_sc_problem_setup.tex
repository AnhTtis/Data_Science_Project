
Let $T<\infty$ be a time horizon. Let $(\Omega, \MCF, \PP)$ be a probability space supporting an $m$-dimensional Brownian motion $W = (W_t)_{t \in [0,T]}$, and $\mathbb{F} = (\mc{F}_t)_{t\in[0,T]}$ be the natural filtration generated by $W$. In the most common case, a stochastic control problem is formulated as follows. Let $d$ and $k$ be integers for the dimensions of the state and the action. Let $\mc{A} \subset \RR^k$ denote the set of admissible actions. Let $b, \sigma, f, g$ be Borel-measurable functions,
\begin{equation}
\label{def:oc-bsigmaf}
    (b, \sigma, f) : [0,T] \times \RR^d \times \mc{A} \to (\RR^d, \RR^{d \times m}, \RR), \quad g : \RR^d \to \RR.
\end{equation}
We denote by $\mathbb{A}$ the set of so-called admissible controls. This set describes the integrability and measurability conditions required on $\alpha$. We usually require that $\alpha$ is square-integrable. For measurability, two popular choices are that either $\alpha$ should be a $\mc{F}_t$-progressively measurable process, or $\alpha$ should be expressed as a measurable function of $(t, X_t)$. The former case is called \emph{adaptive controls} (also called \emph{open-loop}) while the latter one is referred to as \emph{Markovian controls} or \emph{closed-loop controls in feedback form} (we use the two terms interchangeably in the sequel).  We will sometimes consider \emph{closed-loop controls}, which means controls that are adapted to the filtration generated by $X$. 
We shall see that different choices of $\mathbb{A}$ might lead to different algorithm designs in Section~\ref{sec:control-direct}. 

\begin{defi}[Stochastic control problem]
An agent controls her state process $X$ through an action process $\alpha$ taking values respectively in $\RR^d$ and $\mc{A}$, where the dynamics of $X$ are given by the stochastic differential equation (SDE), 
\begin{equation}\label{def:control-Xt}
    \ud X_t = b(t, X_t, \alpha_t) \ud t + \sigma(t, X_t, \alpha_t) \ud W_t, \quad X_0 = x_0.
\end{equation}
The agent aims to minimize the expected cost
\begin{equation}\label{def:control-cost}
    J(\alpha): \ctrl \mapsto \EE \left[\int_0^T f(t, X_t, \alpha_t) \ud t + g(X_T) \right],
\end{equation}
over the set of admissible action processes, denoted by $\mathbb{A}$ and to be discussed below. 
\end{defi}




We briefly describe how the above problem can be tackled by PDEs, or (F)BSDEs.

\noindent
\textbf{PDE approach.} When considering Markovian controls, one can define the value function $u:[0,T] \times \RR^d \to \RR$,
\begin{equation}
    u(t, x) = \inf_\alpha \EE\left[\int_t^T f(s, X_s, \alpha_s) \ud s + g(X_T) \vert X_t = x\right], 
\end{equation}
and employ the dynamic programming principle (DPP) \cite[Section~3]{PH:09}: $u(T, x) = g(x)$ and for any stopping time $\tau \in [t, T]$, 
\begin{equation}
    u(t,x) = \inf_\alpha \EE\left[\int_t^\tau f(s, X_s, \alpha_s) \ud s + u(\tau, X_\tau) \vert X_t = x\right].
\end{equation}
Then, one can derive the Hamilton-Jacobi-Bellman (HJB) equation, which describes the evolution of the value function. Under suitable conditions, $u$ solves
\begin{equation}\label{def:control-HJB}
    \begin{cases}
   	\partial_t u (t,x)  + \min_{\alpha \in \mc{A}} H(t, x, \grad_x u(t,x), \Hess_x u(t, x), \alpha) = 0, \qquad (t,x) \in [0,T) \times \RR^d 
   	\\
   	u(T,x) = g(x), \qquad x \in \RR^d,
   	\end{cases}
\end{equation}
where the Hamiltonian $H$ is defined as
\begin{equation}\label{def:control-H}
    H(t, x, p, q, \alpha) = b(t, x, \alpha) \cdot p + \half \Tr(\sigma(t, x, \alpha)\sigma(t,x, \alpha)\transpose q) + f(t, x, \alpha),
\end{equation} 
and $\Hess_x u(t, x)$ as the Hessian matrix of $u$ with respect to $x$. 
If \eqref{def:control-HJB} has a classical solution, then the optimal control is given by
\begin{equation*}
	\hat{\ctrl}(t,x) = \hat{\ctrl}(t, x, \grad_x u(t,x), \Hess_x u(t,x)) = 
	\argmin_{\alpha \in \mc{A}} H(t, x, \grad_x u(t, x), \Hess_x u(t, x), \alpha).
\end{equation*}

\noindent
\textbf{BSDE approach.} The connection between SC and BSDEs can be established in two different ways: by representing the value function or its derivative as the solution of a BSDE. When the volatility is uncontrolled,  that is, $\sigma(t, x, \alpha)$ is free of $\alpha$, then $\hat \ctrl$ does not depend on $\Hess_x u(t,x)$ and the PDE \eqref{def:control-HJB} becomes semi-linear
\begin{multline}
	\partial_t u (t,x)  + \half \Tr(\sigma(t, x)\sigma(t,x)\transpose \Hess_x u(t, x)) + b(t, x, 	\hat{\ctrl}(t, x, \grad_x u(t,x))) \cdot  \grad_x u(t,x) \\
	+ f(t, x, \hat{\ctrl}(t, x, \grad_x u(t,x))) = 0.
\end{multline}
In this case, suppose that there exist functions $\tilde b(t, x)$ and $h(t, x, z)$ such that 
$$
    \tilde b(t, x) \cdot \grad_x u(t,x) + h(t, x, \sigma(t,x) \transpose \grad_x u(t,x)) = b(t, x, \hat{\ctrl}(t, x, \grad_x u(t,x))) \cdot  \grad_x u(t,x) + f(t, x, \hat{\ctrl}(t, x, \grad_x u(t,x))).
$$
Then the nonlinear Feynman-Kac formula \cite{PaPe:90} gives the following BSDE interpretation of $u$,
\begin{equation}\label{def_control_BSDEreform}
    \begin{dcases}
    \ud \mc{X}_t = \tilde b(t, \mc{X}_t) \ud t + \sigma(t, \mc{X}_t) \ud W_t, \quad \mc{X}_0 \sim \mu_0, \\
    \ud \mc{Y}_t = -h(t, \mc{X}_t, \mc{Z}_t)\ud t + \mc{Z}_t \ud W_t, \quad \mc{Y}_T = g(\mc{X}_T),
    \end{dcases}
\end{equation}
through the relations
$$
    \mc{Y}_t = u(t, \mc{X}_t), \quad \mc{Z}_t = \sigma(t, \mc{X}_t) \transpose \grad_x u(t, \mc{X}_t).
$$
If $\mu_0 = \delta_{x_0}$ is concentrated on a single initial state $x_0$, then the optimal value is given by the value of the BSDE solution at time $0$, {\it i.e.}, $\inf_{\alpha} J(\alpha) = \mc{Y}_0$. If $\tilde b(t, x)$ is chosen to be identically zero, this equality can also be obtained by the comparison principle of BSDEs \cite[Proposition 4.1]{Ca:16}.

In the controlled volatility case, the PDE \eqref{def:control-HJB} is fully nonlinear, and its solution is connected to a solution of the second order BSDE (2BSDE) \cite{cheridito2007second}. If one chooses $\tilde b(t,x)$ and $\Sigma(t,x)$ such that $h$ is determined by
\begin{equation}
    H(t, x, p, q, \hat\alpha(t, x, p, q)) = \tilde b(t, x) \cdot p + h(t, x, p, q) + \half \Tr(\Sigma(t, x)\Sigma(t, x)\transpose q),
\end{equation}
then the solution to the 2BSDE
\begin{equation}\label{def_control_2BSDEreform}
    \begin{dcases}
    \ud \mc{X}_t = \tilde b(t, \mc{X}_t) \ud t + \Sigma(t, \mc{X}_t) \ud W_t, \quad \mc{X}_0 = x_0, \\
    \ud \mc{Y}_t = -h(t, \mc{X}_t, \mc{Y}_t, \mc{Z}_t)\ud t + \mc{Z}_t\transpose \Sigma(t, \mc{X}_t)\ud W_t, \quad Y_T =  g(\mc{X}_T),\\
    \ud \mc{Z}_t = \mc{A}_t \ud t + \Gamma_t \Sigma(t, \mc{X}_t)\ud W_t,  \quad \mc{Z}_T = \grad_x g(\mc{X}_T),
    \end{dcases}
\end{equation}
gives an interpretation of the solution to the PDE \eqref{def:control-HJB} through the relations
$$
    \mc{Y}_t = u(t, \mc{X}_t), \quad \mc{Z}_t =  \grad_x u(t, \mc{X}_t), \quad \Gamma_t = \Hess_x u(t, \mc{X}_t), \quad \mc{A}_t = \mathfrak{L}\grad_x u(t, \mc{X}_t),
$$
where $\mathfrak{L}$ denotes the infinitesimal generator of $\mc{X}$.

\noindent
\textbf{FBSDE approach.} 
The Pontryagin stochastic maximum principle provides the connection to the FBSDE, where the forward and backward equations are coupled. Define the generalized Hamiltonian $\mathcal{H}$ by
\begin{equation}
\label{eq:Hamiltonian-bsde-control}
\mc{H}(t,x,y,z,\alpha) = b(t, x, \alpha) y + \Tr(\sigma(t, x, \alpha)z) + f(t, x, \alpha).
\end{equation}
If the Hamiltonian $\mc{H}$ is convex in $(x, \alpha)$, 
and $(X_t, Y_t, Z_t)$ solves
\begin{equation}\label{def_control_FBSDEreform}
    \begin{dcases}
    \ud {X}_t = b(t, {X}_t, \hat \alpha_t) \ud t +\sigma(t, {X}_t, \hat \alpha_t) \ud W_t, \qquad {X}_0 = x_0,\\
    \ud {Y}_t = -\grad_x \mc{H}(t, {X}_t, {Y}_t,  {Z}_t,  \hat \alpha_t ) \ud t + {Z}_t \ud W_t, \quad Y_T = \partial_x g({X}_T),\\
    \hat{\alpha}_t = \inf_{\alpha} \mc{H}(t, {X}_t, {Y}_t,  {Z}_t, \alpha ),
    \end{dcases}
\end{equation}
then $\hat \alpha$ is the optimal control.  If the value function is smooth enough, then 
\begin{equation}
     Y_t = \grad_x u(t,  X_t), \quad Z_t = \sigma(t, X_t, \hat \alpha_t)\transpose\Hess_x u(t,  X_t).
\end{equation}







\noindent
\textbf{Classical numerical methods. }
To solve stochastic optimal control problems, one typically tries to compute either the solution to the HJB equation~\eqref{def:control-HJB} or the solution to the BSDE in~\eqref{def_control_BSDEreform}. To solve HJB PDEs, classical methods include finite difference schemes~\cite{bonnans2003consistency,oberman2006convergent,jensen2013convergence}, semi-Lagrangian schemes~\cite{debrabant2013semi} and finite element schemes~\cite{beard1997galerkin,boulbrachene2001finite}. We refer to~\cite{feng2013recent} for a review of numerical methods for non-linear second order PDEs (including HJB equations) and to \cite{falcone2016numerical} for a review of finite-difference and semi-Lagrangian schemes for Hamilton-Jacobi type equations. 
For a review of numerical methods for HJB equations arising in finance, we refer to~\cite{forsyth2007numerical}. To solve BSDEs, several numerical methods have also been proposed. Most approaches approximate the backward variable $Y$ using an Euler scheme along a time grid by representing it as a conditional expectation~\cite{bouchard2004discrete,zhang2004numerical}, which can be computed in various ways. Common methods include least-square regression using a set of basis functions~\cite{gobet2005regression,bender2012least}, quantization~\cite{bally2001stochastic,bally2003error} and tree-based approximation~\cite{chevance1997numerical,ma2002numerical}. 
Furthermore, several refinements have been studied, such as higher-order schemes~\cite{zhao2006new,zhao2010stable,chassagneux2014linear}.  We refer to~\cite{chessari2023numerical} for a recent review of numerical methods for BSDEs. We stress that the two approaches (based on PDEs and BSDEs) are connected, see {\it e.g.}~\cite{bouchard2009discrete}. Last, there are other methods, such as the Markov chain approximation method~\cite{kushner2013numerical}. 
Some of these methods have been extended to MFC problems, for which deep learning methods will be discussed in the sequel. For instance~\cite{achdoulauriere2015systemmfc} used a finite-difference scheme for the PDE system, \cite{MR3575615} proposed an augmented Lagrangian method, and \cite{balata2019class} adapted several numerical methods for BSDEs to finite-dimensional MFC problems. 

These classical numerical methods are well understood thanks to the rich background of numerical analysis and most of them have been extensively studied. In many cases, rigorous proofs of convergence have been obtained, for both the schemes ({\it e.g.}, time and space discretizations) and the algorithms. Rates of convergence have also been obtained under suitable assumptions but, to the best of our knowledge, in general these rates suffer from the curse of dimensionality. In practice, Monte Carlo-based methods tend to scale up better than PDE-based methods but remain limited in terms of dimensionality except for specific classes of functions, see {\it e.g.}~\cite{sloan1998quasi} for the provable efficiency of quasi-Monte Carlo methods to compute high-dimensional integrals in particular situations. 







For machine learning algorithms introduced in the following sections, if a temporal discretization is needed, we shall consider, for simplicity, a uniform grid $\pi$ on the interval $[0,T]$, {\it i.e.}, a partition $0 = t_0 < t_1 \dots < t_{N_T} = T$, with $t_n - t_{n-1} = \Delta t = T / N_T$. 
