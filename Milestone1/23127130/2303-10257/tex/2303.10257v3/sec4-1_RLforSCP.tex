Recall the stochastic control problems studied in Section~\ref{sec:SCP}, see~\eqref{def:control-cost}--\eqref{def:control-Xt}. 
In this section, we will assume that the agent can not directly access $b, \sigma, f$ and $g$, but can observe the ``next step'' information given the current state and control. We consider the time discretized problem
\begin{align}
 &\checkX_{t_{n+1}} = \checkX_{t_n} + b(t_n, \checkX_{t_n}, \alpha_{t_n}) \Delta t + \sigma(t_n, \checkX_{t_n}, \alpha_{t_n}) \Delta \check W_{t_n}, \label{eq:rl-Xt-discrete} \\
&\min_{(\alpha_{t_n})_{n=0,\dots,N_T-1}} \EE\left[\sum_{n=0}^{N_T-1} f(t_n, \checkX_{t_n}, \alpha_{t_n}) \Delta t + g(\checkX_T)\right] \label{eq:rl-Xt-cost},
\end{align}
where
\begin{equation}
    0 = t_0 < t_1 < \ldots < t_{N_T} = T, \text{ with } t_n - t_{n-1} = \Delta t = T/N_T,
\end{equation}
is the temporal discretization on $[0,T]$ as before. By doing so, the system is Markovian, and can be viewed as a Markov decision process (MDP). 






\subsubsection{Markov decision processes}\label{sec:MDP}



Problem \eqref{eq:rl-Xt-discrete}--\eqref{eq:rl-Xt-cost} can be recast as an MDP, which is a tuple $(\mc{X},\mc{A},p, f, g, N_T)$, where
\begin{itemize}
    \item $\mc{X}$ is the set of states called  the {state space};
    \item $\mc{A}$ is the set of actions called the {action space};
    \item $N_T<+\infty$ is the time horizon;
    \item $p: \mc{X} \times \mc{A} \times \{0,\Delta t, 2\Delta t, \dots,T\} \to \mathcal{P}(\mc{X})$ is the transition kernel, and $p(\cdot \vert x, a, t_n)$ is a probability density function; 
    \item $f:\{0,\Delta t, 2\Delta t, \dots,T\} \times \mc{X} \times \mc{A} \to \mathbb{R}$ is the one-step cost function, and $f(t_n, x, a)$ is the immediate cost at time $t_n$ at state $x$ due to action $a$;
    \item $g: \mc{X} \to \mathbb{R}$ is the terminal cost function, and $g(x)$ is the terminal cost at the final time $N_T$.
\end{itemize} 
 A large part of the RL literature focuses on the infinite horizon setting with discounted costs. Furthermore, the state space and action space are usually discrete (and they are often in fact finite), in which case  $p(x' \vert x, a, t_n) = \PP(X_{t_{n+1}} = x' \vert X_{t_n} = x, \alpha_{t_n} = a)$ is the probability to go to state $x'$ at time $t_{n+1}$ if at time $t_n$ the state $x$ and the action is $a$. 
 However, for the sake of consistency with the previous sections and the literature on optimal control, we stick to the finite horizon and continuous space setting in this section.

In model-free RL, the agent typically uses multiple episodes to learn the control that optimizes \eqref{eq:rl-Xt-discrete} with a simulator. In one episode of learning, the agent-environment interaction is as follows: Starting with $X_0 \in \mc{X}$, the agent chooses $\alpha_0 \in \mc{A}$, pays a one-step cost $f(0, X_0, \alpha_0)\Delta t$ and finds herself in a new state $X_{t_1}$; the process continues, forming a sequence:
\begin{equation}\label{eq:rl-sequence}
    X_0, \; \alpha_0, \; f(0, X_0, \alpha_0)\Delta t, \; X_{t_1}, \; \alpha_{t_1}, \; f(t_1, X_{t_1}, \; \alpha_{t_1})\Delta t, \; \ldots,\; X_T, \; g(X_T).    
\end{equation}


Under the Euler scheme \eqref{eq:rl-Xt-discrete}, given the state-action pair $(\check X_{t_n}, \alpha_{t_n}) = (x, a)$ at time $t_n$, $X_{t_{n+1}}$ follows a normal distribution $\mc{N}(x + b(t_n, x, a)\Delta t, \sigma^2(t_n, x, a)\Delta t)$.



In RL, there are four main components: policy, reward signal, value function, and optionally, a model of the environment. The MDP provides a mathematical framework to describe the agent-environment interface. A \emph{policy} $\pi: \{0,\Delta t, 2\Delta t, \dots,T\} \times \mc{X} \to \mc{P}(\mc{A})$ is a mapping from the state space to the probability space of the action space, and $\pi_t(a\vert x)$ describes the probability of choosing action $a$ at state $x$, which is in general random. 

The \emph{value function} associated to a specific policy $\pi$ is denoted by $V^\pi_t$ and defined as the expected cost when starting from $x$ at time $t$ and following $\pi$ thereafter, {\it i.e.},
\begin{equation}
    V_{t_n}^\pi(x) = \EE^\pi\left[\sum_{j=n}^{N_T-1} f(t_j, X_{t_j}, \alpha_{t_j})\Delta t + g(X_T) \vert X_{t_n} = x\right],
\end{equation}
where the superscript $\pi$ over the expectation means that, at each time step, the action is sampled according to $\pi$. 
Similarly, the \emph{action-value function} $Q^\pi_t$ associated to $\pi$ is defined as the expected cost when starting from $x$ at time $t$, taking the action $a$ and then following $\pi$, {\it i.e.},
\begin{equation}
    Q^\pi_{t_n}(x, a) = \EE^\pi\left[\sum_{j=n}^{N_T-1} f(t_j, X_{t_j}, \alpha_{t_j})\Delta t + g(X_T) \vert X_{t_n} = x, \; \alpha_{t_n} = a\right].
\end{equation}
Both functions satisfy the dynamic programming equations (also called Bellman equations),
\begin{align}
    &V^\pi_{t_n}(x) = \int_{a \in \mc{A}}\pi_{t_n}(a\vert x) \int_{x' \in \mc{X}} p(x' \vert x, a, t_n)[f(t_n, x, a)\Delta t + V^\pi_{t_{n+1}}(x')] \ud x' \ud a, \\
    & Q_{t_n}^\pi(x, a) = \int_{x' \in \mc{X}} p(x' \vert x, a, t_n) [f(t_n, x, a) \Delta t + V_{t_{n+1}}^\pi(x')] \ud x',
\end{align}
with terminal conditions $V_T^\pi(x) = Q_T^\pi(x, a) = g(x)$, where we have simplified the subscript $t_{N_T} = T$. The goal of RL is to identify the optimal $\pi^\ast = (\pi^\ast_t)_{t}$ that minimizes $V_t^\pi(x)$ for every $t$ and $x \in \mc{X}$. 

To this end, one also works with the \emph{optimal value function} $V^\ast_t(x) = \inf_\pi V_t^\pi(x)$ and the \emph{optimal action-value function} defined as $Q^\ast_t(x, a ) = \inf_\pi Q^\pi_t(x, a)$, which satisfy and the optimal Bellman equation reads,
\begin{align}
    &V^\ast_{t_n}(x) = \inf_{a \in \mc{A}} \int_{x' \in \mc{X}} p(x' \vert x, a, t_n)[f(t_n, x, a) \Delta t + V^\ast_{t_{n+1}}(x')] \ud x',
    \\
    &Q^\ast_{t_n}(x, a) = f(t_n, x, a) \Delta t +  \int_{x' \in \mc{X}} p(x' \vert x, a, t_n) \inf_{a' \in \mc{A}} Q^\ast_{t_{n+1}}(x', a') \ud x',
\end{align}
with terminal conditions $V_T^\ast(x) = Q_T^\ast(x, a) = g(x)$.

Model-free RL aims at computing $\pi^*$ without using the knowledge of the transition probability kernel $p$, and by instead relying on samples of transitions $X_{t_{n+1}} \sim p(x' \vert x, a, t_n)$. 
There are primarily two categories of learning methods: value-based methods and policy gradient methods. In the continuous time framework, the connection between policy evaluation in value-based methods and policy gradient methods has been developed in \cite{jia2021policy,jia2021policy2}.





\paragraph{Value-based methods}
For value-based methods, the workflow can be summarized as follows: starting with an arbitrary policy $\pi$, evaluate its value, improve the policy, and repeat until convergence:
\begin{equation}
    \pi_0 \to V^{\pi_0} \to \pi_1 \to V^{\pi_1} \to \ldots \pi_\ast \to V^{\ast}.
\end{equation}
The symbol $\pi_{\mt{k}} \to V^{\pi_{\mt{k}}}$ denotes a \emph{policy evaluation}, and the symbol $\pi_{\mt{k}} \to V^{\pi_{\mt{k}+1}}$ denotes a \emph{policy improvement}. 
Evaluating a given policy $\pi_{\mt{k}} \to V^{\pi_{\mt{k}}}$ exactly is not possible since we assume that $p(\cdot\vert x, a, t_n)$ is unknown. The Temporal-Difference (TD) learning remedies this issue by updating $V^\pi_{t_n}(x)$ with one sample drawn according to $X_{t_{n+1}} \sim p(x' \vert x, a, t_n)$,
$$
V^\pi_{t_n}(X_{t_n}) \leftarrow V^\pi_{t_n}(X_{t_n}) + \beta[f(t_n, X_{t_n}, \alpha_{t_n})\Delta t + V_{t_{n+1}}^\pi(X_{t_{n+1}}) - V_{t_n}^\pi(X_{t_n})],
$$
where $\beta>0$ is a learning rate. 
This is the simplest TD method, usually denoted by TD(0). To unify TD methods and MC methods, one can view the latter as updating $V^\pi_{t_n}$ using the entire sequence of observed cost from time $t_n$ until the end of the episode $T$. The $n$-step TD method lies in between and consists of simulating $n$ Monte Carlo samples to update $V^\pi$.

TD learning can also be applied to action-value functions, for example by using the update rule:
$$
    Q^{\pi}_{t_n}(X_{t_n}, \alpha_{t_n}) \leftarrow Q^{\pi}_{t_n}(X_{t_n}, \alpha_{t_n}) + \beta[f(t_n, X_{t_n}, \alpha_{t_n})\Delta t + Q^{\pi}_{t_{n+1}}(X_{t_{n+1}}, \alpha_{t_{n+1}}) - Q^{\pi}_{t_n}(X_{t_n}, \alpha_{t_n})], 
$$
where $X_{t_{n+1}}, \alpha_{t_{n+1}}$ are random samples from \eqref{eq:rl-Xt-discrete} and from $Q$ plus some randomization using for instance the $\eps$-greedy policy, which picks the currently optimal action with probability $1-\epsilon$ and, with probability $\epsilon$, picks an action uniformly at random. This approach is called SARSA.
Then the optimal action-value function $Q^\ast$ can be learned as follows: choose $\alpha_{t_n}$ according to $Q$ plus $\epsilon$-greedy for some exploration, then update $Q$ using SARSA. This method falls into the category of \emph{on-policy} algorithms since it evaluates or improves the policy that is used to make decisions. In fact, it uses an $\epsilon$-greedy policy to balance between learning an optimal behavior and behaving non-optimally for exploration, so it learns the value function for a sub-optimal policy that still explores. \emph{Off-policy} methods, on the contrary, use different policies for evaluation and data generation. \emph{Q-learning} may be the earliest well-known off-policy algorithm, which directly approximates $Q^\ast$ using the update rule:
$$
Q^{\pi}_{t_n}(X_{t_n}, \alpha_{t_n}) \leftarrow Q^{\pi}_{t_n}(X_{t_n}, \alpha_{t_n}) + \beta[f(t_n, X_{t_n}, \alpha_{t_n})\Delta t + \max_a Q^{\pi}_{t_{n+1}}(X_{t_{n+1}}, a) - Q^{\pi}_{t_n}(X_{t_n}, \alpha_{t_n})].
$$





\paragraph{Policy gradient methods}\label{sec:MDP-PGM}

This section describes some methods that aim at directly learning an optimal policy without deducing it from the value function. They use a parameterized class of policies. We denote by $\pi_t(a\vert x; \theta)$ the probability of taking action $a$ at state $x$ with parameter $\theta$. In practice, this can be a linear function $\theta\transpose\mathrm{f}(x, a)$ where $\mathrm{f}(x, a)$ is called feature vector, or a neural network taking $x$ as input and outputting a probability distribution over actions. Policy gradient methods update the policy parameter $\theta$ based on the gradient of some performance measure $L(\theta)$, with updates of the form
$$
    \theta \leftarrow \theta - \beta \widehat{\nabla J(\theta)},
$$
where $\widehat{\nabla L(\theta)}$ denotes an estimation of $\nabla L(\theta)$ based on Monte Carlo samples. A natural choice of $L(\theta)$ is the value function $V^{\pi_\theta}$ we aim to minimize. According to the policy gradient theorem,
$$
    \nabla V^{\pi_\theta}_{t_n}(x) = \EE_\pi \left[\int_{\mc{A}} Q^\pi_{t_n}(x, a)\nabla \pi_{t_n}(a\vert x; \theta) \ud a\right].
$$
Multiplying the first term by $\pi_{t_n}(a \vert x; \theta)$, dividing the second term by $\pi_{t_n}(a \vert x; \theta)$, replacing $a$ by a sample $\alpha_{t_n}$, and using $\EE_\pi[G_{t_n}\vert x, a] = Q^\pi_{t_n}(x, a)$ leads to the REINFORCE algorithm~\cite{williams1992simple},
$$
\theta \leftarrow \theta - \beta G_{t_n} \frac{\nabla_\theta \pi_{t_n}(\alpha_{t_{n}}\vert X_{t_n}; \theta)}{\pi_{t_n}(\alpha_{t_{n}}\vert X_{t_n}; \theta)},
$$
where $G_{t_n} = \sum_{n' = n+1}^{N_T-1} f(t_{n'}, \check X_{t_{n'}}, \alpha_{t_{n'}})\Delta t + g(\check X_T)$ denotes the cumulated cost from time $t_{n+1}$ to $T$.


\begin{remark}[Theoretical analysis]
Convergence of policy gradient has been studied in various settings. As for the model-based framework, LQ problems have attracted a particular interest since the optimal control can be written as a linear function of the state. Global convergence in the infinite horizon setting has been proved by Fazel et al. in~\cite[Theorems 7 and 9]{fazel2018global}. This result has been extended in various directions, such as the finite horizon setting in~\cite{hambly2021policy}, the neural setting \cite{wang2019neural}, problems with entropy regularization \cite{cen2022fast}, and MFC~\cite{carmona2019linear}, to cite just a few examples.
\end{remark}


With an additional parameterized value function $V_{t_n}(x; \theta')$, this leads to the actor-critic algorithm (see, {\it e.g.}, \cite[Section~13.5]{sutton2018reinforcement} or \cite{degris2012off}), 
\begin{align}
& \delta_{t_n} = f(t_n, X_{t_n}, \alpha_{t_n})\Delta t + V_{t_{n+1}}(X_{t_{n+1}}; \theta') - V_{t_n}(X_{t_n}; \theta'),\\
&\theta' \leftarrow \theta' - \beta' \delta_{t_n}\nabla_{\theta'} V_{t_n}(X_{t_n}; \theta'),\\
&\theta \leftarrow \theta - \beta \delta_{t_n} \nabla_\theta \ln \pi_{t_n}(\alpha_{t_n}\vert X_{t_n}; \theta).
\end{align}

Both REINFORCE and actor-critic methods mentioned above stochastically select an action $a$ when in state $x$ according to the parameter $\theta$, {\textit i.e.}, $a \sim \pi_{t_n}(\cdot \vert x, \theta)$. For some problems, it is more appropriate to look for a deterministic policy $\alpha_{t_n}(x; \theta) \in \mc{A}$. To ensure exploration, one can use an off-policy approach: a stochastic policy $\tilde\pi_{t_n}(a\vert x)$ is used to choose the action, and a deterministic policy $\alpha_{t_n}(x; \theta)$ is learned to approximate the optimal one.
An example of such methods is the deterministic policy gradient (DPG) \cite{pmlr-v32-silver14}, which is an off-policy actor-critic algorithm that learns a deterministic target policy $\alpha_{t_n}(x; \theta)$ from an exploratory behavior policy $\tilde\pi_{t_n}(a\vert x)$. In particular, a differentiable critic $Q(x, a; \theta')$ is used to approximate $Q^{\alpha(\cdot;\theta)}(x, a)$ and is updated via Q-learning: at each step, we  sample $\alpha_{t_n}$ from $\tilde\pi_{t_n}(a\vert x)$ and
\begin{align}
    & \delta_{t_n} = f(t_n, X_{t_n}, \alpha_{t_n})\Delta t + Q_{t_{n+1}}(X_{t_{n+1}}, \alpha_{t_{n+1}}(X_{t_{n+1}}; \theta); \theta') - Q_{t_n}(X_{t_n}, \alpha_{t_n}; \theta'),\\
&\theta' \leftarrow \theta' - \beta' \delta_{t_n}\nabla_{\theta'} Q_{t_n}(X_{t_n}, \alpha_{t_n}; \theta'),\\
&\theta \leftarrow \theta - \beta \delta_{t_n} \nabla_\theta \alpha_{t_n}(X_{t_n}; \theta)\nabla_a Q(X_{t_n}, \alpha_{t_n}; \theta')\vert_{a = \alpha_{t_n}(X_{t_n}; \theta)}.
\end{align}



When using neural networks to approximate $Q^{\alpha(\cdot;\theta)}$ and the deterministic policy $\alpha_{t_n}(x; \theta)$, one can use the Deep DPG (DDPG) algorithm \cite{lillicrap2015continuous}, which is based on the same intuition as DPG. For the sake of robustness it uses the ``replay-buffer'' idea borrowed from the Deep Q Network (DQN) algorithm, see~\cite{mnih2015human}: the network parameters are learned in mini-batches rather than online by using a replay buffer, so that correlation between samples are kept minimal. Another pair of networks $Q'(x, a; \hat \theta')$ and $\alpha'_{t_n}(x; \hat \theta)$ are copied from $Q(x, a; \theta')$ and $\alpha_{t_n}(x; \theta)$ for calculating the target value in order to improve the stability. At each step, an action $\alpha_{t_n}$ is sampled from $\alpha_{t_n}(X_{t_n}; \theta) + \mc{N}_{t_n}$ where $\mc{N}_{t}$ is a noise process for exploration; then the cost $f(t_n, X_{t_n}, \alpha_{t_n})\Delta t$ and the new state $X_{t_{n+1}}$ are observed and saved to the buffer. A mini-batch of $N$ transitions $(X_{t_n}, \alpha_{t_n}, f, X_{t_{n+1}})$ are sampled from the buffer, acting as supervised learning data for the critic $Q(x, a; \theta')$. The loss to be minimized is the mean-squared error of $Q_{t_n}(X_{t_n}, \alpha_{t_n}; \theta')$ and $f(t_n, X_{t_n}, \alpha_{t_n})\Delta t +  Q'_{t_{n+1}}(X_{t_{n+1}}, \alpha'_{t_{n+1}}(X_{t_{n+1}}; \hat \theta); \hat\theta')$. The actor network and both copies are updated via
\begin{align}
 & \theta \leftarrow \theta - \beta \frac{1}{N} \sum_{i}\nabla_a Q_{t_n}(x,a;\theta')\vert_{x = X_{t_n}^i, a = \alpha_{t_n}(X_{t_n}^i; \theta)} \nabla_\theta \alpha_{t_n}(X_{t_n}; \theta), \\
 & \hat \theta' \leftarrow \tau \theta' + (1-\tau) \hat \theta', \quad \hat \theta \leftarrow \tau \theta + (1-\tau) \hat \theta,
\end{align}
where the superscript $i$ indicates the $i^{th}$ sample from the mini-batch, and $\tau \ll 1$ is used to slowly track the learnt counterparts $\theta$ and $\theta'$.












\subsubsection{Mean-field MDP and reinforcement learning for mean-field control problems}
\label{sec5_RLforMFC}




We now consider the MFC setting discussed in Section~\ref{sec:directMethod} as an extension of standard OC and we present an RL framework for this setting. MFC can be viewed as an optimal control problem in which a ``state'' is a population configuration. However an ``action'' is not a finite-dimensional object but rather a function providing a control for every individual state. Intuitively, in discrete time, this yields an MDP of the form $(\mc{P}(\mc{X}), \mc{F}_{\mc{A}}, \bar{p}, \bar {f}, \bar{g}, N_T)$, where \begin{itemize}
    \item The state space is the set $\mc{P}(\mc{X})$ of probability measures on $\mc{X}$; 
    \item The action space $\mc{F}_{\mc{A}}$ is a suitable subset of $\mc{A}^{\mc{X}}$, the set of functions from $\mc{X}$ to $\mc{A}$;
    \item The transition kernel is given by  
$$
    \bar{p}:   \{t_0,t_1,\dots,T\} \times \mc{P}(\mc{X}) \times \mc{F}_\mc{A} \to \mc{P}(\mc{P}(\mc{X})), 
    \quad \bar{p}(\cdot|t, \mu, \bar a) = \delta_{\int p(\cdot|t, x, \mu, \bar{a}(x)) \mu(x) dx} \,,
$$
    meaning that with probability one, the new mean field state is given by one transition of the population distribution. Here $\mu$ represents a population distribution, $\bar a$ is an action at the population level, and $\bar{p}(\cdot|t, \mu, \bar a)$ is the distribution of the next population distribution, which is a Dirac mass at the next population distribution since there is no common noise in the present model;
    
    \item The running and terminal cost functions are given by
$$
    \bar{f}: \{t_0,t_1,\dots,T\} \times \mc{P}(\mc{X}) \times \mc{F}_{\mc{A}} \to\RR, \qquad \bar{f}(t, \mu, \bar{a}) = \int_{x} f(t, x, \mu, \bar{a}(x)) \mu(x) \ud x \,,
$$
and
$$
    \bar{g}: \mc{P}(\mc{X}) \to\RR, \qquad \bar{g}(\mu) = \int_{x} g(x, \mu) \mu(x) \ud x.
$$

\end{itemize}
Such MDPs have been referred to as mean field MDPs (MFMDP for short) in the literature~\cite{gast2012mean,carmona2019modelfree,gu2019dynamicmfc,gu2021mean,motte2019mean,bayraktar2022finite}. These MDPs can be rigorously studied using the tools developed for instance by Bertsekas and Shreve in~\cite{bertsekasshreve1996stochastic}. 
Since this problem fits in the framework of MDPs, one can directly apply RL methods in principle. For instance, the Q-function of the MDP naturally satisfies a dynamic programming principle; see~\cite{carmona2019modelfree,gu2019dynamicmfc,gu2021mean,motte2019mean}. Note that, if there is no common noise (as in the setting presented above), the evolution of the population distribution is purely deterministic. 

To implement RL methods for MFC, the main difficulties are related to handling the distribution and the class of controls. In particular, we note that
\begin{itemize}
    \item If $\mc{X}$ is finite, then the state of the MDP, namely $\mu$, is a finite-dimensional vector; if $\mc{A}$ is also finite, then $\mc{F}_{\mc{A}}$ can simply be taken as $\mc{A}^{\mc{X}}$, which is a finite set as well;
    \item If $\mc{X}$ is not finite, then $\mu$ is infinite-dimensional and likewise for the elements of $\mc{A}^{\mc{X}}$.
\end{itemize}

One simple approach is to discretize $\mc{P}(\mc{X})$ and $\mc{A}^{\mc{X}}$, and then use standard RL techniques for finite state, finite action MDPs, such as the ones described in Section~\ref{sec:MDP}. For instance tabular Q-learning has been used {\it e.g.} in~\cite{carmona2019modelfree,gu2021mean} in the first case above by identifying $\mc{P}(\mc{X})$ with the simplex $\Delta_{\mc{X}}$ in dimension $|\mc{X}|$ and by approximating the latter with an $\epsilon$-net. However, this approach does not scale well when the number of states is large or when $\mc{X}$ is continuous. In this case, one can use RL methods for continuous state space, such as deep RL methods, see for instance~\cite{carmona2019modelfree}.


\begin{remark}[Theoretical analysis]
    The convergence of Q-learning for MFMDP has been analyzed in~\cite{carmona2019modelfree} and~\cite{gu2021mean} using tabular or kernel-based methods respectively. The convergence of a policy gradient method for LQ MFC has been proved in~\cite{carmona2019linear} based the ideas of~\cite{fazel2018global}. 
\end{remark}


For the sake of illustration, we provide an example in a setting where $\mc{X}$ is finite. Let $d = |\mc{X}|$ be the number of states. As mentioned above, we view $\mc{P}(\mc{X})$ as the $d$-dimensional simplex $\Delta_{\mc{X}}$. In this case, the MFMDP is an MDP over a finite-dimensional continuous state space. To avoid discretizing the space, deep RL methods rely on neural networks to efficiently approximate the value function or the policy. 


\paragraph*{Numerical illustration: A Cybersecurity model revisited.} 
We consider the cybersecurity model introduced in~\cite{MR3575619} (see also~\cite[Section 7.2.3]{carmona2018probabilistic}) and that we already discussed in Section~\ref{sec:mastereq-deeplearning}. We revisit this problem from the point of view of MFC, meaning that the players cooperate to jointly minimize the social cost. 

To be able to tackle this problem using RL, we discrete time using a mesh $\{t_n = n \Delta t, n = 0,1,2,\dots, N_T\}$ where $\Delta t = T / N_T>0$. The total cost for the whole population is
\begin{align*}
	J(\ctrl) 
	=  \sum_{n=0}^{N_T-1} \bar f(\mu_{t_n}, \ctrl(t_n, \cdot)) \Delta t,
\end{align*}
under the constraint that the evolution of distribution is given by
\begin{equation}
    \label{eq:cyber-mfc-dyn-mu}
	\mu_{t_{n+1}} 
	= \bar p(\mu_{t_n}, \ctrl(t_n,\cdot))
	= (\mu_{t_n})\transpose (I + P^{\ctrl(t_n,\cdot), \mu_{t_n}} \Delta t), \qquad n = 0, 1, \dots, N_T-1,
\end{equation}
with a given initial condition $\mu_0$. The population-wise cost function 
$\bar f: \mc{P}(\mc{X}) \times \mc{A}^{\mc{X}} \to \RR$ is defined based on the individual cost function $f$ by
$$
	\bar f(m, \ctrl) = \sum_{x\in\mc{X}} f(x, m, \ctrl(x) ) m(x), \qquad (m, \ctrl) \in \mc{P}(\mc{X}) \times \mc{A}^\mc{X}, 
$$
and $P^{\ctrl, m}$ denotes the matrix whose coefficients are given by
$$
	P^{\ctrl, m}(x', x) = \lambda(x', x, m, \ctrl(x')), \qquad (x',x,m,\ctrl) \in \mc{X} \times \mc{X} \times \mc{P}(\mc{X}) \times \mc{A}^\mc{X}.
$$
From this formulation, we see that the problem fits in the framework of MFMDPs, or MDPs with finite horizon and continuous space, the state being the distribution.


In~\cite{AMSnotesLauriere}, the solution is learned using tabular Q-learning after discretizing the simplex: replacing $\mc{P}(\mc{X})$ by an $\epsilon$-net with a finite number of distributions allows one to replace the MFMDP by a finite-state MFMDP on which tabular RL methods can be applied. This approach is convenient in that tabular methods typically have fewer hyperparameters and furthermore convergence results are easier to obtain. However, the main drawback is that such methods do not scale well to very large state space. In our case, discretizing the simplex requires a large number of points when the number of states increases.

Alternatively, the value function can be approximated directly on the simplex $\mc{P}(\mc{S})$, without any discretization. For example, we can replace the Q-function by a neural network and employ deep RL techniques to train the parameters. Here we follow the approach proposed in~\cite{carmona2019modelfree} and we focus on deterministic controls. The control and the value function are approximated by neural networks and trained using the DDPG method~\cite{lillicrap2015continuous}, which has been reviewed in Section~\ref{sec:MDP-PGM}. Since this method allows the control to take continuous values, we replace $A=\{0,1\}$ by $A =[0,1]$ (without changing the transition rate matrix), which amounts to letting the player choose the intensity with which she seeks to change her computer's level of protection. 

We aim at learning the solution for various distributions. To train the neural networks, we sample at each iteration a random initial distribution $\mu_0$ and generate a trajectory in the simplex by following the dynamics~\eqref{eq:cyber-mfc-dyn-mu}. Figure~\ref{fig:cyber-ddpg-cn-testing} displays the evolution of the population when using the learned control starting from five initial distributions of the testing set and one initial distribution of the training set. The testing set of initial distributions is: $\{(0.25,0.25,0.25,0.25),$ $(1, 0, 0, 0),$ $(0, 0, 0, 1),$ $(0.3, 0.1, 0.3, 0.1),$ $(0.5, 0.2, 0.2, 0.1)\}$. We see that, in this setting, the distribution always evolves towards a configuration in which there is no defended agents, and the proportion of undefended infected and undefended susceptible are roughly $0.43$ and $0.57$, respectively. 


\begin{figure}[!htb]
\centering
\includegraphics[width=0.4\textwidth]%
{{figure/RL-MFC-CYBERSECURITY/mfc_cybersecurity_mu_evol_testdistrib1}}%
\includegraphics[width=0.4\textwidth]%
{{figure/RL-MFC-CYBERSECURITY/mfc_cybersecurity_mu_evol_testdistrib2}}\\
\includegraphics[width=0.4\textwidth]%
{{figure/RL-MFC-CYBERSECURITY/mfc_cybersecurity_mu_evol_testdistrib3}}%
\includegraphics[width=0.4\textwidth]%
{{figure/RL-MFC-CYBERSECURITY/mfc_cybersecurity_mu_evol_testdistrib4}}\\
\includegraphics[width=0.4\textwidth]%
{{figure/RL-MFC-CYBERSECURITY/mfc_cybersecurity_mu_evol_testdistrib5}}
  \caption{
      Cybersecurity MFC model solved with DDPG in Section~\ref{sec5_RLforMFC}: Evolution of the population distribution for five initial distributions. 
  }
  \label{fig:cyber-ddpg-cn-testing}
\end{figure}









