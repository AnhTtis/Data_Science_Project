We now turn our attention to alternative deep learning methods for control problems \eqref{def:control-Xt}--\eqref{def:control-cost}, which are based on solving the associated backward stochastic differential equations (BSDEs) (\emph{cf.} \eqref{def_control_BSDEreform} and \eqref{def_control_2BSDEreform}). To simplify the presentation, we explain the algorithms on a generic BSDE,
\begin{equation}\label{def:BSDE-Yt}
      \ud Y_t = - F(t, X_t, Y_t, Z_t) \ud t + Z_t\transpose \ud W_t, \quad t \in [0,T], \qquad Y_T = G(X_T),
\end{equation}
where $X$ solves the (possibly) coupled forward equation
\begin{equation}\label{def:BSDE-Xt}
   \ud X_t = B(t, X_t, Y_t, Z_t) \ud t + \sigma(t, X_t) \ud W_t, \quad t \in [0,T], \qquad X_0 \sim \mu_0.
\end{equation}

As mentioned in Section~\ref{sec:control_intro}, the connection between optimal control and BSDEs (or FBSDEs) can be established in several ways.   

\subsubsection{Deep backward stochastic differential equation (Deep BSDE) method}
\label{subsubsec:deepBSDE}
The Deep BSDE method was proposed by E, Han and Jentzen in~\cite{MR3736669,HaJeE:18}, which, to the best of our knowledge, were the first works to use deep learning to solve BSDEs in high dimension and has inspired many followup works. This method relies on a stochastic version of a shooting method that is extensively used to solve ODEs. The strategy has been successfully applied to problems in economic contract theory where it is sometimes referred to as Sannikov's trick; see for example \cite{MR1766421,MR2963805,MR3738664}. The idea is to try to guess the initial value $Y_0$ and the $(Z_t)_{t \in [0,T]}$ process so as to meet the terminal condition $Y_T = g(X_T)$. It has originally been proposed in~\cite{MR3736669} for decoupled FBSDE systems, and then extended to fully coupled FBSDEs; see {\it e.g.}~\cite{hanlong2020convergence, ji2020three}. 

To be precise, solving the system \eqref{def:BSDE-Yt}--\eqref{def:BSDE-Xt} is reduced to identifying $(Y_0, (Z_t)_{t \in [0,T]})$ in \eqref{def:BSDE-Yt} which is characterized as the solution to the following control problem, {\it i.e.}, to minimize the cost over $(y_0,z)$ 
$$
    J(y_0,z) = \EE \left[ \| Y_T - g(X_T)\|^2\right],
$$
subject to:
$$
\begin{cases}
     \ud X_t = B(t, X_t, Y_t, z(t,X_t)) \ud t + \sigma(t, X_t) \ud W_t, \qquad t \in [0,T],
     \\
     \ud Y_t = - F(t, X_t, Y_t, z(t,X_t)) \ud t + z(t,X_t)\transpose \ud W_t, \qquad t \in [0,T],
     \\
     X_0 \sim \mu_0, \qquad Y_0 = y_0(X_0).
\end{cases}
$$
Then, as in the methods presented above (see Section~\ref{sec:control-direct}), the control functions $y_0$ and $z$ are replaced by deep neural networks, say $y_0(\cdot; \theta^Y)$ and $z(\cdot; \theta^Z)$ with parameters $\theta^Y$ and $\theta^Z$, respectively. Furthermore, time is discretized using a uniform grid $t_0 < t_1 \dots < t_{N_T} = T$, $t_n - t_{n-1} = \Delta t = T / N_T$. Then the problem becomes to minimize over $\theta = (\theta^Y,\theta^Z)$ the cost 
$$
    \check{J}(\theta) = \EE \left[ \| \check{Y}^{\theta}_T - g(\check{X}^{\theta}_T)\|^2\right],
$$
subject to:
$$
\begin{cases}
     \check{X}^{\theta}_{t_{n+1}} - \check{X}^{\theta}_{t_{n}} = B(t_n, \check{X}^{\theta}_{t_{n}}, \check{Y}^{\theta}_{t_{n}}, z(t_{n},\check{X}^{\theta}_{t_{n}}); \theta^Z) \Delta t + \sigma(t_n, \check{X}^{\theta}_{t_{n}}) \Delta \check{W}_{t_{n+1}}, \qquad n=0,\dots,N_T-1,
     \\
     \check{Y}^{\theta}_{t_{n+1}} - \check{Y}^{\theta}_{t_{n}}  = - F(t_n, \check{X}^{\theta}_{t_{n}}, \check{Y}^{\theta}_{t_{n}}, z(t_{n},\check{X}^{\theta}_{t_{n}}); {\theta^Z}) \Delta t + z(t_{n},\check{X}^{\theta}_{t_{n}}; {\theta^Z})\transpose \Delta \check{W}_{t_{n+1}}, \qquad n=0,\dots,N_T-1,
     \\
     \check{X}^{\theta}_0 = X_0 \sim \mu_0, \qquad \check{Y}^{\theta}_0 = y_{0}(\check{X}^{\theta}_0; \theta^Y),
\end{cases}
$$
where 
$
    \Delta \check{W}_{t_{n+1}} = \check{W}_{t_{n+1}}-\check{W}_{t_{n}}.
$

Finally, the optimization can be carried out by applying the SGD algorithm (Algorithm~\ref{algo:SGD-generic} in Appendix~\ref{sec:SGD-var}) with $L(\theta) = \check{J}(\theta)$ and one sample is 
$
    \xi = (X_0, (\Delta \check{W}_{t_{n}})_{n=1,\dots,N_T}),
$
which is sufficient to simulate $(\check{X}^{\theta}_{t_{n}}, \check{Y}^{\theta}_{t_{n}})_{n=0,\dots,N_T}$. 



\begin{remark}
    Instead of using a single neural network for $z$, viewed as a function of $t$ and $x$, another possibility is to put a different neural network at each time step, which is a function of $x$ only. With this approach, each neural network can use fewer parameters since there are fewer inputs. However, possible drawbacks are that: (a) the number of neural networks grows linearly with the number of time steps; and (b) the time dependence is not captured (at least not directly). 
    
    A possible shortcoming of this method is that the optimization is done globally in time: the loss function is computed only after simulating a whole trajectory, and only then can the parameters be updated. For problems with long time horizons or complicated terminal conditions, the method may have difficulty in converging, as has been pointed out in~\cite{hurephamwarin2020deep}. 
\end{remark}

\begin{remark}[Theoretical analysis]
    A posteriori error bounds for the Deep BSDE method have been proved by Han and Long in~\cite[Theorems 1 and 2]{hanlong2020convergence}, extending the results of~\cite{bendersteiner2013posteriori} for uncoupled FBSDEs. They have shown, under suitable conditions, that the numerical error can be bounded by the value of the loss, and that this loss can be made as small as desired provided the approximation capability of the neural network is sufficient. Such results have then been extended to the fully coupled mean-field setting by Reisinger, Stockinger, and Zhang in \cite[Theorems 3.2 and 4.3]{reisinger2020posteriori}.
\end{remark}

Several variants of the Deep BSDE method have been proposed. For example, \cite{ji2020three} considers learning $Y$ as a feedback function of $X$ or using Picard iterations to learn feedback controls based on $(X,Y,Z)$. The Deep BSDE method has been extended to include control problems with mean-field effects~\cite[Secion~4.2]{carmona2019convergence2} and delay~\cite[Section~3.2]{fouque2019deep}, and generalized to stochastic differential games \cite[Section~3.2]{HaHu:19}.

As mentioned in Section~\ref{sec:control_intro}, the solution to a BSDE is closely related to the solution of a semi-linear PDE, which could be derived from an uncontrolled volatility problem. In the case of a fully controlled volatility problem, a similar relation exists, and Beck, E, and Jentzen \cite{beck2019machine} propose the corresponding deep 2BSDE method. It is noteworthy that several works have refined and extended the Deep BSDE method. For instance, \cite{chanwainam2019machine} improved the performance of the Deep BSDE method with specific architectures and training methods, illustrating them on various examples of semi-linear PDEs, including an HJB equation. 



\subsubsection{Deep backward dynamic programming (DBDP)}



The DBDP method has been proposed by Hur\'e, Pham and Warin in \cite{hurephamwarin2020deep}, based on ideas similar to the local approach discussed in Section~\ref{sec:direct-global-local}. The main idea is to learn $\check{Y}_{t_n}$ and $\check{Z}_{t_n}$ at each $t_n$ as functions of $\check{X}_{t_n}$ by backward induction in time. So the resolution of the BSDE is decomposed as a sequence of optimization problems that are solved backward in time. This is in contrast with the Deep BSDE method, which goes forward in time, just like the difference between the two algorithms introduced in Section~\ref{sec:control-direct}. 

In DBDP, for each $n$, $\check{Y}_{t_n}$ and $\check{Z}_{t_n}$ are replaced by neural networks, say $y_n(\cdot; {\theta^Y_n})$ and $z_n(\cdot; {\theta^Z_n})$, with possibly different parameters at each time step. Here, one first chooses a sequence of distributions, say $\mu_{t_n}$ from which $\check{X}_{t_n}$ can be sampled for each $t_n$, $n=0,\dots,N_T$. The algorithm proceeds with a backward induction. First, $\theta^Y_{N_T}$ is trained such that $y_{N_T}(\cdot; {\theta^Y_{N_T}}) \approx g(\cdot)$, for example by minimizing
$$
    \check J(\theta^Y) = \EE\left[ \|y_{N_T}(\check{X}_{T}; {\theta^Y}) - g(\check{X}_{T})\|^2 \right],
$$
where $\check{X}_{T} \sim \mu_{N_T}$. Note that the value of $\theta^Z_{N_T}$ is not relevant to the result of the method.
 Then,  the neural networks $y_n(\cdot; {\theta^Y_n})$ and $z_n(\cdot;{\theta^Z_n})$ are trained for $n=N_T-1, N_T-2, \dots, 0$. There are at least two different ways to train these neural networks: 
\begin{itemize}
	\item Version 1: $\theta_n = (\theta^Y_n, \theta^Z_{n})$ is trained to minimize over $\theta = (\theta^Y,\theta^Z)$,
	\begin{align*}
		\check J^1_n(\theta) 
		&= \EE\Big[ \Big\|y_{n+1}(\check{X}^{\theta}_{t_{n+1}}; {\theta^Y_{n+1}}) - y_n(\check{X}_{t_n}; {\theta^Y})   +F(t_n, \check{X}_{t_n}, y_n(\check{X}_{t_n}; {\theta^Y}), z_n(\check{X}_{t_n}; {\theta^Z})) \Delta t 
		\\
		&\qquad\qquad - z_n(\check{X}_{t_n}; {\theta^Z}) \cdot \Delta \check{W}_{t_{n+1}}\Big\|^2\Big],	
	\end{align*}
	with
	$$
        \check{X}^{\theta}_{t_{n+1}} = \check{X}_{t_n} + B(t_n, \check{X}_{t_{n}}, y_n(\check{X}_{t_n}; {\theta^Y}), z_n(\check{X}_{t_{n}}; {\theta^Z})) \Delta t + \sigma(t_n, \check{X}_{t_n})  \Delta \check{W}_{t_{n+1}}, \qquad \check{X}_{t_n} \sim \mu_{t_n}.
    $$
	\item Version 2: $\theta^Y_n$ is trained to minimize over $\theta = \theta^Y$,
	\begin{align*}
		\check J^2_n(\theta) 
		&= \EE\Big[ \Big\|y_{n+1}(\check{X}^{\theta}_{t_{n+1}}; {\theta^Y_{n+1}}) - y_n(\check{X}_{t_n}; {\theta^Y})  + F(t_n, \check{X}_{t_n}, y(\check{X}_{t_n}; {\theta^Y}), \sigma\transpose \mathcal{D}_x y_n(\check{X}_{t_n}; {\theta^Y})) \Delta t 
		\\
		&\qquad\qquad - \mathcal{D}_x y_n(\check{X}_{t_n}; {\theta^Y})\transpose \sigma \Delta \check{W}_{t_{n+1}}\Big\|^2\Big],	
	\end{align*}
	with
	$$
        \check{X}^{\theta}_{t_{n+1}} = \check{X}_{t_n} + B(t_n, \check{X}_{t_{n}}, y_n(\check{X}_{t_n}; {\theta^Y}), \sigma\transpose \mathcal{D}_x y_n(\check{X}_{t_n}; {\theta^Y})) \Delta t + \sigma(t_n, \check{X}_{t_n}) \Delta \check{W}_{t_{n+1}}, \qquad \check{X}_{t_n} \sim \mu_{t_n},
    $$
    where the derivative $\mathcal{D}_x y_n$ represents the numerical differentiation of the neural network $y_n$. In this version, the $Z$ component is directly approximated by the derivative of the neural network for $Y$, so we do not use any parameter $\theta^Z$.
\end{itemize}

Then at each time step $t_n$, the optimization can be carried out by applying the SGD algorithm (see Algorithm~\ref{algo:SGD-generic} in Appendix~\ref{sec:SGD-var}) to the loss $\check J_n^1(\theta)$ or $\check J_n^2(\theta)$, and one sample is
$
    \xi = (\check{X}_{t_n}, \Delta \check{W}_{t_{n+1}}).
$ 
The DBDP method has been used successfully to solve BSDEs associated with semi-linear PDEs and fully non-linear PDEs (\cite{hurephamwarin2020deep} and \cite{pham2021neural}). Several refinements have been studied ({\it e.g.}, higher-order scheme in \cite{chassagneux2022deep}).

\begin{remark} 
In \cite{hurephamwarin2020deep}, the authors also extended this idea to reflected BSDEs that arise in optimal stopping problems and American option pricing in finance \cite[Section~6.5]{PH:09}. However, as mentioned in the introduction, we do not discuss optimal stopping problems in this survey for the sake of brevity.

The difference between the two versions of DBDP is: Version~1 uses independent neural networks for the approximation of $Y_t$ and $Z_t$ in \eqref{def:BSDE-Yt}; while Version~2 only approximates $Y_t$ by neural networks and represents $Z_t$ through auto-differentiating $y_n(\cdot; \theta^Y_n)$. Though the former one has more modeling flexibility, it may also introduce some inconsistency, as in many scenarios, the backward process is a function of time and the forward process $Y_t = u(t, X_t)$ and the adjoint process is indeed the derivative of this function up to a scaling factor: $Z_t = \sigma\transpose(t, X_t) \nabla_x u(t, X_t)$. 

The main advantage of the DBDP method is that it makes use of the time structure of the problem to split it into much simpler problems. Two possible shortcomings of the DBDP method are that: (1) the number of neural networks grows linearly with the number of time steps; and (2) it is not always clear how to choose the sampling distributions $\mu_{t_n}$, which have an impact on the way the neural networks are trained. 
\end{remark}

\begin{remark}[Theoretical analysis]
    Hur\'e, Pham and Warin analyzed in~\cite[Theorems 4.1 and 4.2]{hurephamwarin2020deep} the error and showed, assuming the gradient descent method used to solve the local problems converges to the true optimum, that the approximation error goes to zero as one increases the number of time steps and the number of neurons.
\end{remark}



A similar approach based on dynamic programming and backward induction can be applied to MFC (see Section~\ref{sec:directMethod}), but in this case, the value function is a function of the probability distribution, see e.g.~\cite{MR3343705,MR3258261,MR3631380,carmona2018probabilistic}. Since it is impossible to represent numerically all probability distributions, an approximation must be made. For instance, ~\cite{germain2021deepsets} used empirical distributions combined with symmetric neural networks, and \cite{hanhulong:22} used neural networks to represent directly the function maps evaluated at the optimal state's distribution.  