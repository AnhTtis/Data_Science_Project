

Mean-field games, introduced independently by Lasry and Lions in \cite{LaLi1:2006,LaLi2:2006,LaLi:2007} and by Huang, Malham\'{e} and Caines in \cite{HuMaCa:06,HuCaMa:07}, provide a paradigm to approximate the solutions of stochastic games with very large number of players. The approximation relies on two main assumptions: anonymity, which means that the players interact only through the population's empirical distribution, and indistinguishability, which means that $(b^i, \sigma^i, f^i, g^i)$ are the same for all $i$. Then, one can pass (at least formally) to the limit by letting the number of players grow to infinity. In the asymptotic problem, the influence of each individual player on the rest of the population vanishes and the Nash equilibrium can be characterized by studying the problem posed to a representative player. Under suitable assumptions, it can be shown that solving this limiting problem provides an approximate equilibrium for the finite-player game.  
We refer to the notes~\cite{Cardaliaguet-2013-notes} and the books \cite{MR3134900,carmona2018probabilistic,carmona2018probabilistic2} as well as the references therein for further background on mean-field games.  Mean field games are usually studied using the notion of mean field Nash equilibrium. As in finite-player games, intuitively this notion corresponds to a situation in which no player can benefit from unilateral deviation. This is different from the solution to MFC problems discussed in Section~\ref{sec:directMethod}, which can be interpreted as a social optimum, \textit{i.e.}, a situation in which the players collectively choose their control in order to minimize the social cost.



Numerical methods for such games have been developed using mostly traditional techniques such as finite difference schemes~\cite{achdoucapuzzo2010mean,achdoucamillicapuzzo2012mean}, semi-Lagrangian schemes~\cite{carlinisilva2014fully,carlinisilva2018discretization}, or methods based on probabilistic approaches~\cite{chassagneuxcrisandelarue2019numerical,angiuli2019cemracs}. See, {\it e.g.}, \cite{achdoulauriere2020meansurvey,AMSnotesLauriere} for recent surveys. However, similarly to control problems or finite-player games, these methods do not scale well in terms of dimensionality, and in particular, they are not very suitable for problems with delay or with common noise. This motivates the development of deep learning methods, some of which are described in~\cite{carmonalauriere2021deepmfgsurvey}. In the sequel, we describe the theoretical framework of mean field games and then survey recent deep learning methods.








\subsubsection{Theoretical background}
\label{sec:mfg-theoretical-background}
We start by defining the notion of MFG, and then we discuss how equilibria can be characterized in terms of PDEs, BSDEs, and the so-called master equation. 

\paragraph{Definition of the problem}
Going back to problem~\eqref{def:DFP-open-cost}, let us assume that $b^i, \sigma^0, \sigma^i, f^i, g^i$ depend on the rest of the population's states and actions in an anonymous way, {\it i.e.}, there exists functions $b, \sigma^0, \sigma, f, g$ such that 
\begin{equation}
\begin{aligned}
&b^i(t, \bm{X}_t, \bm{\alpha}_t) = b(t, X^i_t, \nu^N_t, \alpha^i_t), \quad \sigma^0(t, \bm{X}_t, \bm{\alpha}_t) = \sigma^0(t, X^i_t, \nu^N_t, \alpha^i_t), \quad \sigma^i(t, \bm{X}_t, \bm{\alpha}_t) = \sigma(t, X^i_t, \nu^N_t, \alpha^i_t), \\ & f^i(t, \bm{X}_t, \bm{\alpha}_t) = f(t, X^i_t, \nu_t^N, \alpha^i_t), \quad g^i(\bm{X}_T) = g(X_T^i, \mu_T^N),
\end{aligned}
\end{equation}
where $\nu^N_t = \frac{1}{N} \sum_{j=1}^N \delta_{(X^j_t, \alpha^j_t)}$ is the empirical state-action distribution of the population and $\mu^N_t = \frac{1}{N} \sum_{j=1}^N \delta_{X^j_t}$  is its first marginal, which corresponds to the state distribution. We keep the same notation $\sigma^0$ for simplicity, with a slight abuse of notation. 

Then the cost associated to a strategy profile $\bm{\alpha}$ is defined as
\begin{equation}\label{def:MFG-cost}
J^i(\bm{\alpha}) = \EE\left[\int_0^T f(t, X^i_t, \nu_t^N, \alpha^i_t) \ud t + g(X_T^i, \mu_T^N)\right],
	\end{equation}
where the processes $X^j$, $j=1,\dots,N$, solve the SDE system
\begin{equation}%
\ud X_t^j = b(t, X^j_t, \nu^N_t, \alpha^j_t) \ud t + \sigma(t, X^j_t, \nu^N_t, \alpha^j_t) \ud W_t^j + \sigma^0(t, X^j_t, \nu^N_t, \alpha^j_t) \ud W_t^0, \quad X_0^j \sim \mu_0, \quad j \in \mc{I},
\end{equation}
where the initial positions are i.i.d., with $\nu$ and $\mu$ being as above the flows of empirical state-action and empirical state distributions. The influence of a given player on the dynamics and the cost of another player occurs only through the empirical distribution flow $\nu^N$. So when $N$ increases, the influence of each player decreases. By symmetry, we can expect that in the limit it is sufficient to study the problem for a single representative player.  


To formulate the MFG, let  $\nu=(\nu_t)_{0\le t\le T}$ be a stochastic distribution flow adapted to the filtration generated by $W^0$, which is interpreted at the evolution of the population's state-action configuration. Let $\alpha$ be an open-loop control. A representative player's dynamics are given by,
\begin{align}
\label{eq:dyn-X-general-MFG}
\begin{dcases}
    \ud X_t^{\nu, \ctrl} 
    = b(t, X_t^{\nu, \ctrl}, \nu_t, \ctrl_t) \ud t + \sigma(t, X_t^{\nu, \ctrl}, \nu_t, \ctrl_t) \ud W_t 
	 + \sigma^0(t, X_t^{\nu, \ctrl}, \nu_t, \ctrl_t) \ud W^0_t, \quad t \ge 0
	 \\
	 X^{\nu, \ctrl}_0 \sim \mu_0,
\end{dcases}
\end{align}
where $W$ is a standard $m$-dimensional Brownian motion independent of $W^0$. For a representative player, the cost associated to using the control $\alpha$ when the population is given by the distribution flow  $\nu=(\nu_t)_{0\le t\le T}$ is defined as
\begin{align}
\label{subchapRCML-num-eq:def-J-MFG}
	J^{MFG}(\ctrl; \nu) = \EE \left[\int_0^T f(t, X_t^{\nu, \ctrl}, \nu_t, \ctrl_t ) \ud t + g(X_T^{\nu, \ctrl}, \mu_T) \right],
\end{align}
under the constraint that the  process $X^{\nu, \ctrl} = (X_t^{\nu, \ctrl})_{t \ge 0}$ solves the SDE~\eqref{eq:dyn-X-general-MFG}.


\begin{defn}[Mean-field Nash equilibrium] 
\label{def:MFGeq-finitehorizon}
Consider the MFG problem introduced above. A pair $(\hat{\nu},\hat{\ctrl})$ consisting of a stochastic flow $\hat{\nu}=(\hat \nu_t)_{0\le t\le T}$ of probability measures in $\cP_2(\RR^d)$ adapted to the common noise filtration and an open-loop control $\hat{\ctrl} = (\hat{\ctrl}_t)_{t \in [0,T]}$ is a mean-field Nash equilibrium if it
satisfies the following two conditions
\begin{enumerate}
	\item $\hat{\ctrl}$ minimizes $J^{MFG}(\cdot; \hat{\nu})$;
	\item For all $t \in [0,T]$, $\hat{\nu}_t$ is the probability distribution of $(X_t^{\hat{\nu}, \hat{\ctrl}}, \hat{\ctrl}_t)$ conditioned on $W^0$.
\end{enumerate}
\end{defn}
Note that, in the first condition, $\hat\nu$ is fixed when an infinitesimal agent performs their optimization. The second condition ensures that if all the players use the control $\hat{\ctrl}$, the law of their individual states and actions is indeed $\hat{\nu}$. 
The original formulation of MFGs~\cite{MR2295621} considers interactions through the state distribution only. MFGs with interactions through the joint distribution of state and actions, as presented in the above definition, are sometimes referred to as extended MFGs or MFGs of controls, see {\it e.g.}, \cite{GoPaVo:14,GoVo:16,cardaliaguet2017mean,kobeissi2022classical,laurieretangpi2022convergence}.


\begin{remark}
    For a given model (cost functions and dynamics), one can look for a mean field Nash equilibrium (Definition~\ref{def:MFGeq-finitehorizon}) or a mean field social optimum (Definition~\ref{def:mfc-optimum}). As already mentioned, the first notion corresponds to a situation in which the agents selfishly minimize their individual cost, while the second notion corresponds to a situation in which the agents cooperate to minimize the social cost. From the mathematical viewpoint, the key difference is that in Definition~\ref{def:MFGeq-finitehorizon}, the distribution is fixed when one looks for an optimal control, while in Definition~\ref{def:mfc-optimum}, the control directly influences the distribution. As a consequence an MFG (Nash equilibrium) problem is a fixed point problem, while an MFC (social optimum) is an optimization problem (or more precisely an optimal control problem for McKean-Vlasov dynamics). In general, the two solutions are different, which leads to a notion of price of anarchy. See e.g.~\cite{graber2016linear,MR3968548,cardaliaguet2019efficiency,AMSnotesLauriere,angiulifouquelauriere2020unified} for more details and comparisons of MFG and MFC solutions.
\end{remark}




Next, we review several ways to characterize the mean field Nash equilibrium concept using analytical and probabilistic techniques. 



\vskip 6pt
\paragraph{PDE system}
\label{sec:MFG-PDE-system}

For simplicity, let us assume that there is no common noise. We assume that there exists an equilibrium, and we denote by $\hat{\nu} = (\hat{\nu}_t)_{t \ge 0}$ the associated mean-field flow of distributions. When considering Markovian controls, one can define the value function $u$ by
\begin{equation}
    u(t, x) = \inf_\alpha \EE\left[\int_t^T f(s, X_s, \hat{\nu}_s, \alpha_s) \ud s + g(X_T, \hat \mu_T) \vert X_t = x\right]. 
\end{equation}
As in standard OC problems,  
under suitable conditions, $u(t, x)$ solves the HJB equation:
\begin{equation}
    \label{def:control-HJB-MFG}
    \begin{dcases}
   	\partial_t u (t,x)  + \min_{\alpha \in \mc{A}} H(t, x, \hat{\nu}_t, \grad_x u(t,x), \Hess_x u(t, x), \alpha) = 0,
   	\\
   	u(T, x) = g(x,  \hat{\mu}_T),
    \end{dcases}
\end{equation}
where
\begin{equation}
\label{def:control-H-MFG}
    H(t, x, \nu, p, q, \alpha) = b(t, x, \nu, \alpha) \cdot p + \half \Tr(\sigma(t, x, \nu, \alpha)\sigma(t,x, \nu, \alpha)\transpose q) + f(t, x, \nu, \alpha).
\end{equation} 
If \eqref{def:control-HJB-MFG} has a classical solution, then the optimal control is given by
\begin{equation*}
	\hat{\ctrl}(t, x)
	= \ctrl(t, x, \hat{\nu}_t, \grad_x u(t,x), \Hess_x u(t,x)),
\end{equation*}
where
\begin{equation*}
	\ctrl(t, x, \nu, p, q)
	= 
	\argmin_{\alpha \in \mc{A}} H(t, x, \nu, p, q, \alpha).
\end{equation*}
The consistency condition for the equilibrium mean-field flow is equivalent to: the state distribution flow $\hat{\mu} = (\hat{\mu}_t)_{t \ge 0}$ solves the following Kolmogorov-Fokker-Planck (KFP) PDE
\begin{equation}
    \label{eq:MFG-general-KFP}
    \begin{dcases}
        \displaystyle \partial_t \hat{\mu}(t,x)  - \sum_{i,j} \frac{\partial^2}{\partial_{x_i}\partial_{x_j}}\left( \hat{D}_{i,j}(t,x)\hat{\mu}(t,x)\right) + \diver\Bigl( \hat{\mu}(t,x) \hat{b}(t,x)\Bigr) = 0,
        \\
        \hat{\mu}(0) = \mu_0,
    \end{dcases}
\end{equation}
where
\begin{equation}
    \label{eq:def-KFP-hat-D-b}
    \hat{D}(t,x) = \frac{1}{2} \sigma(t, x, \hat{\nu}_t, \hat{\alpha}(t,x))\sigma(t,x, \hat{\nu}_t, \hat{\alpha}(t,x))\transpose, \qquad \hat{b}(t,x) = b(t,x,\hat{\nu}_t,\hat{\ctrl}(t,x)),
\end{equation}
and the state-action distribution $\hat{\nu}_t$ at time time $t$ is the push forward of $ \hat{\mu}_t$ by $(I_d, \hat\alpha(t,\cdot))$, which we will denote by 
$
    \hat{\nu}_t =  \hat{\mu}_t \circ (I_d, \hat\alpha(t,\cdot))^{-1}. 
$
The forward-backward PDE system \eqref{def:control-H-MFG}--\eqref{eq:MFG-general-KFP} characterizes the mean field Nash equilibrium. We refer to {\it e.g.}, \cite{kobeissi2022classical} for the existence of classical solutions to such PDE systems under suitable assumptions. 

\begin{remark}
MFC problems also give rise to analogous forward-backward PDE systems, except that the solution $u$ of the backward equation is not interpreted as a value function of an optimal control problem but rather as an adjoint state. We refer to \cite{MR3134900,achdoulauriere2015systemmfc} for more details. The KFP equation remains the same, but the HJB equation has one extra term reflecting the fact that the whole population performs the optimization simultaneously.
\end{remark}

In the presence of common noise, the HJB and KFP equations become stochastic. We will not discuss this system in the sequel, and refer the interested readers to \cite{peng1992stochastichjb} for the derivation of stochastic HJB equations and~\cite{carmonadelarue2014mastereqlarge,cardaliaguetdelaruelasrylions2019master} for stochastic HJB-KFP systems arising in MFG (with the state distribution only). 



\vskip 6pt
\paragraph{FBSDE system}

We now review the characterization of MFG equilibria using BSDEs. As for standard OC (see Section~\ref{sec:control_intro}), BSDEs can be used to characterize the value function or its gradient. For simplicity, we assume that there is no common noise. We further assume that the volatility of the idiosyncratic noise is uncontrolled, in which case $\hat \ctrl$ is independent of $\Hess_x u(t,x)$ and the PDE \eqref{def:control-HJB-MFG} becomes semi-linear:
\begin{multline}
	\partial_t u (t,x)  + \half \Tr(\sigma(t, x, \hat{\nu}_t)\sigma(t,x, \hat{\nu}_t)\transpose \Hess_x u(t, x)) + b(t, x, \hat{\nu}_t,	\hat{\ctrl}(t, x, \hat{\nu}_t, \grad_x u(t,x))) \cdot  \grad_x u(t,x) \\
	+ f(t, x, \hat{\ctrl}(t, x, \hat{\nu}_t, \grad_x u(t,x))) = 0.
\end{multline}
Suppose that there exist functions $\mu(t, \nu, x)$ and $h(t, x, \nu, z)$ such that 
\begin{multline*} 
    \tilde{b}(t, \hat{\nu}_t, x) \cdot \grad_x u(t,x) + h(t, x, \hat{\nu}_t, \sigma(t,x) \transpose \grad_x u(t,x)) 
    \\= b(t, x, \hat{\nu}_t, \hat{\ctrl}(t, x, \hat{\nu}_t, \grad_x u(t,x))) \cdot  \grad_x u(t,x) 
+ f(t, x, \hat{\nu}_t, \hat{\ctrl}(t, x, \grad_x u(t,x))).
\end{multline*}
Then the non-linear Feynman-Kac formula (see \cite{PaPe:90}) gives the following BSDE interpretation of $u(t, x)$:
\begin{equation}\label{def_MF_BSDEreform}
    \begin{dcases}
    \ud \mc{X}_t = \tilde{b}(t, \hat{\nu}_t, \mc{X}_t) \ud t + \sigma(t, \hat{\nu}_t, \mc{X}_t) \ud W_t, \quad \mc{X}_0 \sim \mu_0, \\
    \ud \mc{Y}_t = -h(t, \hat{\nu}_t, \mc{X}_t, \mc{Z}_t)\ud t + \mc{Z}_t \ud W_t, \quad \mc{Y}_T = g(\mc{X}_T, \hat{\mu}_T),
    \end{dcases}
\end{equation}
by the relation
$$
\mc{Y}_t = u(t, \mc{X}_t), \quad \mc{Z}_t = \sigma(t, \hat{\nu}_t, \mc{X}_t) \transpose \grad_x u(t, \mc{X}_t).
$$
Moreover, the optimal value is given by $\mathbb{E}[\mc{Y}_0] = \mathbb{E}[u(0, \mc{X}_0)]$. This BSDE characterizes the value function for a representative player given the mean field flow $\hat\nu$. Then, the consistency condition reads: 
$$
    \hat{\nu}_t = \Law\left(\mc{X}_t, \hat{\ctrl}(t, \mc{X}_t, \hat{\nu}_t, (\sigma(t, \hat{\nu}_t, \mc{X}_t) \transpose)^{-1}\mc{Z}_t)\right).
$$

In the controlled volatility case, the PDE \eqref{def:control-HJB-MFG} is fully nonlinear, and its solution is connected to a solution of the 2BSDE, see~\cite{cheridito2007second} and Section~\ref{sec:control_intro}.  

The Pontryagin stochastic maximum principle provides the connection to the FBSDE. Define the generalized Hamiltonian $\mc{H}$ by
\begin{equation}
\label{eq:Hamiltonian-bsde-control-MFG}
\mc{H}(t,x,\nu,y,z,\alpha) = b(t, x, \nu, \alpha) y + \Tr(\sigma\transpose(t, x, \nu, \alpha)z) + f(t, x, \nu, \alpha).
\end{equation}
If the Hamiltonian $\mc{H}$ is convex in $(x, \alpha)$, 
and $(X_t, Y_t, Z_t)$ solve
\begin{equation}
    \begin{dcases}
    \ud X_t = b(t, X_t, \hat{\nu}_t, \hat \alpha_t) \ud t +\sigma(t, X_t, \hat{\nu}_t, \hat \alpha_t) \ud W_t, \qquad X_0 \sim \mu_0,\\
    \ud Y_t = -\grad_x \mc{H}(t, X_t, \hat{\nu}_t, Y_t,  Z_t,  \hat \alpha_t ) \ud t + Z_t \ud W_t, \quad Y_T = \partial_x g(X_T, \hat{\mu}_T),
    \end{dcases}
\end{equation}
such that $\hat \alpha$ minimizes $\mc{H}$ along $(X_t, \hat{\nu}_t, Y_t, Z_t)$, then $\hat \alpha$ is the optimal control. If the value function is smooth enough, then 
\begin{equation}
     Y_t = \grad_x u(t,  X_t), \quad Z_t = \sigma(t, X_t, \hat{\nu}_t, \hat \alpha)\transpose\Hess_x u(t,  X_t).
\end{equation}
In this case, the consistency condition for the equilibrium mean field flow $\hat{\nu}$ reads
$$
    \hat{\nu}_t = \Law\left(X_t, \hat{\ctrl}(t, X_t, \hat{\nu}_t, (\sigma(t, \hat{\nu}_t, X_t) \transpose)^{-1} Y_t, Z_t)\right).
$$
When there is common noise, the FBSDE system becomes
\begin{equation}
    \begin{dcases}
    \ud X_t = b(t, X_t, \hat{\nu}_t, \hat \alpha_t) \ud t +\sigma(t, X_t, \hat{\nu}_t, \hat \alpha_t) \ud W_t
    +\sigma^0(t, X_t, \hat{\nu}_t, \hat \alpha_t) \ud W^0_t, \qquad X_0 \sim \mu_0,\\
    \ud Y_t = -\grad_x \mc{H}(t, X_t, \hat{\nu}_t, Y_t,  Z_t,  Z^0_t,  \hat \alpha_t ) \ud t + Z_t \ud W_t + Z^0_t \ud W^0_t, \quad Y_T = \partial_x g(X_T, \hat{\mu}_T),
    \end{dcases}
\end{equation}
where, compared with~\eqref{eq:Hamiltonian-bsde-control-MFG},  the definition of $\mc{H}$ includes an extra term $\Tr({\sigma^0}\transpose(t, x, \nu, \alpha)z^0)$.

\vskip 6pt

\begin{remark} 
MFC problems also lead to analogous FBSDE systems. In the absence of common noise, Pontryagin's maximum principle is derived for instance in~\cite{MR3045029} and~\cite{acciaio2019extendedmfc} when the interactions are through the state or the state-action distributions, respectively. This leads to a BSDE with an extra term accounting for the variation of the distribution during the optimization of the control.
\end{remark}




\vskip 6pt 
All the above systems are particular cases of the following generic system of FBSDEs of McKean-Vlasov type (MKV FBSDE for short)
\begin{equation}
\label{eq:MKV-FBSDE-general}
\left\{
\begin{aligned}
	\ud X_t
	=
	\,& B\left(t, X_t, \Law(X_t, Y_t, Z_t |W^0), Y_t, Z_t, Z^0_t \right) \ud t
		\\
		&\qquad
		+  \sigma(t, X_t, \Law(X_t, Y_t, Z_t |W^0), Y_t, Z_t, Z^0_t) \ud W_t 
		\\
		&\qquad + \sigma^0(t, X_t, \Law(X_t, Y_t, Z_t |W^0), Y_t, Z_t, Z^0_t) \ud W^0_t,
	\\
	\ud Y_t
	=
	\,& - F\Big(t, X_t, \Law(X_t, Y_t, Z_t |W^0), Y_t, \sigma\transpose(t, X_t, \Law(X_t, Y_t, Z_t |W^0), Y_t, Z_t, Z^0_t) Z_t, 
	\\
	&\quad\qquad{\sigma^0}\transpose(t, X_t, \Law(X_t, Y_t, Z_t |W^0), Y_t, Z_t, Z^0_t) Z^0_t \Big) \ud t
		\\
		&\qquad
		+ Z_t \ud W_t + Z^0_t \ud W^0_t,
    \\
    \Law(X_0) &= \mu_0, \qquad Y_T = G(X_T, \Law(X_T|W^0)).
\end{aligned}
\right.
\end{equation}


\begin{remark}
When there is no common noise, $W^0$ and $Z^0$ are dropped, and the system becomes
\begin{equation}
\label{eq:MKV-FBSDE-general-no-CN}
\left\{
\begin{aligned}
	\ud X_t
	=
	& B\left(t, X_t, \Law(X_t, Y_t, Z_t), Y_t, Z_t \right) \ud t
			+  \sigma(t, X_t, \Law(X_t, Y_t, Z_t), Y_t, Z_t) \ud W_t ,
	\\
	\ud Y_t
	=
	& - F\Big(t, X_t, \Law(X_t, Y_t, Z_t), Y_t, \sigma\transpose(t, X_t, \Law(X_t, Y_t, Z_t), Y_t, Z_t) Z_t\Big) \ud t
			+ Z_t \ud W_t,
    \\
    \Law(X_0) &= \mu_0, \qquad Y_T = G(X_T, \Law(X_T)).
\end{aligned}
\right.
\end{equation}
When the interactions are not through the state-action distribution but through the state distribution only, $\Law(X_t, Y_t, Z_t)$ is reduced to $\Law(X_t)$.
\end{remark}



\paragraph{Master equation}
\label{sec:background-master-eq}


As mentioned earlier, in the PDE system~\eqref{def:control-HJB-MFG}--\eqref{eq:MFG-general-KFP}, $u$ plays the role of the value function of a representative player when the rest of the population is at equilibrium. This function depends explicitly on $t$ and $x$ but, intuitively, a player's value function can also depend on the population distribution. When there is no common noise, this distribution evolves in a deterministic way, so knowing $\mu_0$ and $t$ as well as the control used by the population (which is the equilibrium control $\hat\alpha$, assuming the population is at equilibrium) is enough to recover $\hat\mu(t)$, {\it e.g.}, by solving the corresponding KFP equation~\eqref{eq:MFG-general-KFP}. However, we can make this dependence explicit by considering a function $\cU: [0,T] \times \RR^d \times \cP(\RR^d) \to \RR$ such that
\begin{equation}
\label{eq:masterfield-to-u}
    \cU(t,x,\hat\mu(t)) = u(t,x),
\end{equation}
where $\hat\mu = (\hat\mu(t))_t$ is the mean-field equilibrium distribution flow. This correspondence is even more useful when common noise influences the dynamics of the players. In this case, $u(t,x)$ is a random variable whereas $\cU$ is still a deterministic function and the lefthand side of~\eqref{eq:masterfield-to-u} is random only due to $\hat\mu(t)$. This function $\cU$ has been instrumental in proving the convergence of finite-player Nash equilibria towards mean field Nash equilibria, see~\cite{cardaliaguetdelaruelasrylions2019master} for more details.

It turns out that, under suitable conditions, $\cU$ satisfies the PDE that we will present below, introduced by Pierre-Louis Lions and called the Master equation. It involves partial derivatives with respect to the probability measure argument in $\cU$. We say that a function $F: \cP(\RR^d) \to \RR$ is $\mathcal{C}^1$ if there exists a continuous map $\displaystyle\frac{\delta F}{\delta \mu}: \cP(\RR^d) \times \RR^d \to \RR$ such that, for any $\mu,\mu' \in \cP(\RR^d)$,
$$
\lim_{s \to 0^+} \frac{F((1-s)\mu + s\mu') - F(\mu)}{s} = \int_{\RR^d} \frac{\delta F}{\delta \mu}(\mu,y) d(\mu'-\mu)(y).
$$
The derivative $\displaystyle\frac{\delta F}{\delta \mu}$ is sometimes referred to as the flat derivative. 
If $\displaystyle\frac{\delta F}{\delta \mu}$ is of class $\mathcal{C}^1$ with respect to the second variable, the intrinsic derivative $\partial_\mu F:\cP(\RR^d) \times \RR^d \to \RR$ is defined by
$$
    \partial_\mu F(\mu,y) = \partial_y \frac{\delta F}{\delta \mu}(\mu,y).
$$
We will write $\partial_\mu F(\mu)(y)$ instead of $\partial_\mu F(\mu,y)$. For more details, we refer to the lectures of Pierre-Louis Lions~\cite{PLL-CDF}, as well as \cite{Cardaliaguet-2013-notes} and \cite[Chapter 5]{carmona2018probabilistic}.


We can now present the Master equation. To the best of our knowledge, the theory has not yet been developed for the general MFG model described above. We thus consider the case in which the volatility is not controlled, and the interactions are only through the state distribution instead of the state-action distribution. For the sake of brevity, we omit the derivation and refer to {\it e.g.}~\cite[Section 4.4]{carmona2018probabilistic2}. The Master equation is the following backward PDE, posed on the space $[0,T] \times \RR^d \times \cP_2(\RR^d)$,
\begin{align*}
    &\partial_t \cU(t,x,\mu)
    \\
    &\quad +b(t,x,\mu,\hat\alpha(t, x, \mu, \partial_x \cU(t,x,\mu))) \cdot \partial_x \cU(t,x,\mu)
    \\
    &\quad + \int_{\RR^d} b(t,v,\mu,\hat\alpha(t,v,\partial_x \cU(t,v,\mu))) \cdot \partial_\mu \cU(t,x,\mu)(v) d \mu(v)
    \\
    &\quad + \frac{1}{2} \Tr \left[ (\sigma \sigma\transpose + \sigma^0 (\sigma^0)\transpose)(t,x,\mu) \partial_{xx}^2 \cU(t,x,\mu)\right]
    \\
    &\quad + \frac{1}{2} \int_{\RR^d} \Tr \left[ (\sigma \sigma\transpose + \sigma^0 (\sigma^0)\transpose)(t,v,\mu) \partial_v\partial_\mu \cU(t,x,\mu)(v)\right] d \mu (v)
    \\
    &\quad + \frac{1}{2} \int_{\RR^{2d}} \Tr \left[ (\sigma \sigma\transpose + \sigma^0 (\sigma^0)\transpose)(t,v,\mu) \partial_\mu^2 \cU(t,x,\mu)(v,v')\right] d \mu (v) d \mu (v')
    \\
    &\quad + \int_{\RR^d} \Tr \left[ (\sigma^0(t,x,\mu) (\sigma^0)\transpose)(t,v,\mu) \partial_x\partial_\mu \cU(t,x,\mu)(v)\right] d \mu (v)
    \\
    &\quad + f(t,x,\mu,\hat\alpha(t, x, \mu, \partial_x \cU(t,x,\mu)))
    =0,
\end{align*}
for $t \in [0,T]$, $x \in \RR^d$ and $\mu \in \cP_2(\RR^d)$, and with the terminal condition: for every $x \in \RR^d$ and $\mu \in \cP_2(\RR^d)$,
$$
    \cU(T,x,\mu) = g(x,\mu).
$$
For more details on the analysis of this PDE, we refer the interested reader to the monographs~\cite{cardaliaguetdelaruelasrylions2019master}, \cite[Chapters 4 to 7]{carmona2018probabilistic2}, and~\cite{ChassagneuxCrisanDelarue_Master} concerning the existence of classical solutions under suitable conditions. 












\subsubsection{Direct parameterization}
\label{sec4_MFG_with_CN}


As discussed earlier (Section~\ref{sec:control-direct}), the direct parameterization approach for optimal control (Section~\ref{sec:open-loop}) can be extended to finite-player games. It can also be extended to mean-field games by updating alternatively the control (using the direct parameterization method) and the mean-field instead of updating individually all the player's controls as in finite-player games. This idea can be applied to various classes of controls (\textit{e.g.}, open-loop and closed-loop ones). To avoid repetition, we discuss below the application of this approach to a class of MFGs with common noise, which forces to use a more general class of controls.



As seen in Definition~\ref{def:MFGeq-finitehorizon}, a mean-field equilibrium is a standard control problem (corresponding to the first item in the definition) plus a fixed point problem (corresponding to the second item). Motivated by MFG models with common noise,   
Min and Hu \cite{MinHu:21} proposed an algorithm called Sig-DFP utilizing the concept of signature in rough path theory \cite{LyonsTerryJ2007DEDb} and fictitious play from game theory \cite{Br:49,Br:51}. Signature is used to accurately represent the conditional distribution of the state given the common noise, and fictitious play is used to solve the fixed-point problem and identify the equilibrium \cite{cardaliaguet2015learning}.

For a path $x:[0,T]\to \RR^d$, the $p$-variation is defined by 
\begin{equation}
    \|x\|_{p} = \left( \sup_{D\subset[0,T]} \sum_{n=0}^{r-1} \|x_{t_{n+1}}-x_{t_n}\|^p \right)^{1/p},
\end{equation} 
where $D \subset [0,T]$ denotes a partition $0 \leq t_0 < t_1 < \ldots < t_r \leq T$. Let $T((\R^d))=\bigoplus_{k=0}^\infty (\R^d)^{\bigotimes k}$ be the tensor algebra. Let  $\mathcal{V}^p([0,T], \R^d)$ be the space of continuous mappings from $[0,T]$ to $\R^d$ with finite $p$-variation, equipped with norm $\|\cdot\|_{\mathcal{V}^p}=\|\cdot\|_{\infty}+\|\cdot\|_{p}$. 

\begin{defn}[Signature]
Let $X\in \mathcal{V}^p([0,T], \R^d)$ such that the following integral is well defined. The signature of $X$, denoted by $S(X)$, is the element of $T((\R^d))$ defined by $S(X) = (1, X^1, \cdots, X^k \cdots)$ with
\begin{equation}\label{def:signature}
    X^k = \int_{0<t_1<t_2<\cdots<t_k<T} \ud X_{t_1}\otimes\cdots\otimes \ud X_{t_k}.
\end{equation}
Denoting by $S^M(X)$ the truncated signature of $X$ of depth $M$, {\it i.e.}, $S^M(X) = (1, X^1, \cdots, X^M)$ which has the dimension $\frac{d^{M+1}-1}{d-1}$.
\end{defn}
In the current setting, $X$ is a semi-martingale, thus equation \eqref{def:signature} is understood in the Stratonovich sense. The signature has many nice properties, including the following ones. First, it characterizes paths uniquely up to the tree-like equivalence, and the equivalence is removed if at least one dimension of the path is strictly increasing \cite{boedihardjo2014signature}. Therefore, in practice one usually augments the original path $X_t$ with the time dimension, {\it i.e.}, working with $\hat{X}_t = (t, X_t)$ since $S(\hat{X})$ characterizes paths $\hat{X}$ uniquely. Second, terms in the signature present a factorial decay property \cite{lyons2002system}, which implies that a path can be well approximated with just a few terms in the signature ({\it i.e.}, a small $M$). Last, As a feature map of sequential data, the signature has a universality property \cite{NEURIPS2019_deepsig}, which is summarized below. 


    Let $p\ge 1$ and $f: \mathcal{V}^p([0,T], \R^d)\to \R$ be a continuous function. For any compact set $K\subset \mathcal{V}^p([0,T], \R^d)$, if $S(x)$ is a geometric rough path (see \cite[Definition 3.13]{LyonsTerryJ2007DEDb} for a detailed definition) for any $x\in K$, then for any $\eps >0$, there exists a linear functional $l$ in the dual space of  $\in T((\RR^d))$ such that
    \begin{equation}
        \sup_{x\in K} |f(x) - \langle l, S(x) \rangle| <\epsilon.
    \end{equation}

Motivated by the unique characterization of $(W_s^0)_{s \in [0,t]}$ by $S(\hat W_t^0)$ and the factorial decay property, one can approximate 
\begin{equation}
  \nu_t \equiv \mc{L}(X_t, \alpha_t \vert \mc{F}_t^0 ) = \mc{L}(X_t, \alpha_t \vert S(\hat W_t^0)), \label{eq:propbysig}
\end{equation}
by $\mc{L}(X_t, \alpha_t \vert S^M(\hat W_t^0))$, for $\hat W^0_t = (t, W^0_t)$. 
In particular, if the mean-field interaction is through moments $\bar\nu_t = \EE[\iota(X_t, \alpha_t) \vert \F_t^0]$, for some measurable function $\iota$, the approximation can be arbitrarily accurate for sufficiently large $M$, see \cite[Lemma 4.1]{MinHu:21}. Then \cite{MinHu:21} proposed to use the approximation
\begin{equation}\label{eq:mfg_cn_linearregression}
    \bar\nu_t \approx \langle \tilde l, S^M(\hat W_t^0) \rangle, \; \text{ where } \tilde l =  \argmin_{\bm\beta} \|\bm y - \bm X \bm\beta\|^2,  \;
    \bm y = \{\iota(X_t(\omega_i), \alpha_t(\omega_i))\}_{i=1}^N,\;    \bm X = \{S^M(\hat{W}^{0}_{t}(\omega_i))\}_{i=1}^N,
\end{equation}
where $\omega_i$ denotes the $i^{th}$ sample path. The rationale behind this approximation is the universality of signatures and the interpretation of ordinary linear regression: the least square minimization gives the best possible prediction of $\EE[\bm y | \bm X]$ using linear relations. Once $\tilde l$ is obtained, the prediction on an unseen common noise is efficient: $\bar\nu_t(\tilde\omega)\approx \langle \tilde l, S^M(\hat{W}^0_{t}(\tilde\omega)) \rangle$ for any $\tilde \omega$ and $t$.


Then finding the mean-field equilibrium is broken down into the following steps. We start with an initial value $\bar\nu^{(0)}$. Then, we solve the standard control problem given $\bar\nu^{(0)}$ in \eqref{subchapRCML-num-eq:def-J-MFG} in the spirit of \cite{han2016deep-googlecitations}. From here, we approximate $\bar\nu^{(1)}$ via signature using \eqref{eq:mfg_cn_linearregression}, i.e., compute $\tilde l^{(1)}$. These steps are repeated until convergence. The update of $\bar\nu_t$ from step to step is done by averaging $\tilde l^{(n)}$. The Sig-DFP algorithm consists of repeatedly solving \eqref{eq:dyn-X-general-MFG}--\eqref{subchapRCML-num-eq:def-J-MFG} for a given $\bar\nu$ using deep learning in the spirit of \cite{han2016deep-googlecitations}, and passing the obtained $\bar\nu$ to the next iteration by using signatures. A flowchart illustrating the ideas is given in Figure~\ref{fig:algo}.

\begin{figure}
    \centering
    \includegraphics[width = 0.7\textwidth]{figure/algoGraph.png}
    \caption{Flowchart of one iteration in the Sig-DFP Algorithm. Input: idiosyncratic noise $W$, common noise $W^0$, initial position $X_0$ and vector $\hat{\nu}^{(\mt{k}-1)}$ from the last iteration. Output: vector $\hat{\nu}^{(\mt{k})}$ for the next iteration. %
    }
    \label{fig:algo}
\end{figure}

More precisely, at each step, given a proxy $\hat \nu^{(\mt{k-1})}$ of the equilibrium distribution $\hat \nu$, the problem \eqref{eq:dyn-X-general-MFG}--\eqref{subchapRCML-num-eq:def-J-MFG} becomes a standard stochastic control and is solved by using the direct parameterization approach reviewed in Section~\ref{sec:direct-global-local}: the loss function will be the discretized version of \eqref{subchapRCML-num-eq:def-J-MFG},  $\check X$ will be follow the Euler scheme of \eqref{eq:dyn-X-general-MFG} with $\nu$ replaced by $\hat \nu^{(\mt{k}-1)}$ and so do $f$ and $g$, and the control $\alpha_{t_n}$ is parameterized by a neural network of the following form
\begin{equation}
     \alpha_{t_n}= \alpha(t_{n}, \check X_{t_n}, \hat\nu^{(\mt{k}-1)}_{t_n}; \theta),
\end{equation}
which takes $\hat\nu^{(\mt{k}-1)}_{t_n}$ as an extra input on top of $(t_{n}, \check X_{t_n})$. 
The optimizer $\theta^\ast$ obtained in this way gives $\alpha_{t_n}^{(\mt k)}$, with which the optimized state process paths are simulated. The conditional law, denoted by $\nu^{(\mt k)}$, is approximated using signatures via \eqref{eq:mfg_cn_linearregression}. This finishes one iteration of fictitious play. Denote by $\tilde\nu^{(\mt k)}$ the approximation of $\nu^{(\mt k)}$, we then pass $\tilde\nu^{(\mt k)}$ to the next iteration via updating $\hat\nu^{(\mt k)} = \frac{1}{\mt k}\tilde\nu^{(\mt k)} + \frac{\mt k-1}{\mt k}\hat \nu^{(\mt k-1)}$ by averaging the coefficients obtained in \eqref{eq:mfg_cn_linearregression}. We summarize it in Algorithm \ref{alg:sig-dfp} in Appendix~\ref{supp:pseudocode}; see~\cite[Appendix B]{MinHu:21} for the implementation details. We remark that signatures can also be useful for generating multimodal data \cite{MinHuIch:23}.


\begin{rem}[Theoretical analysis]
In \cite[Theorems 4.1 and 4.2]{MinHu:21} Min and Hu provided a proof of convergence of this algorithm showing that, under suitable assumptions, the difference between the $\mt{k}^{th}$ iteration solution and the mean-field equilibrium can be made arbitrarily small, provided that $\mt{k}$ is sufficiently large and $\nu^{(\mt k)}$ can be approximated sufficiently well by truncated signatures.
\end{rem}



\paragraph*{Numerical illustration: MFG of optimal consumption and investment.}\label{sec:MFG-sig} 

We consider an extended heterogeneous MFG proposed by \cite{lacker2020many}, where agents interact via both states and controls. The setup is similar to \cite{LaZa:17} except for including consumption and using power utilities. Each agent's type is characterized by a random vector $\zeta=(\xi, \delta, \theta, b, \sigma, \sigma^0, \epsilon)$, and the optimization problem reads 
\begin{equation}
        \sup_{\pi, c} \EE\biggl[ 
    \int_0^T U(c_t X_t(\Gamma_t m_t)^{-\theta}; \delta)\ud t + \epsilon U(X_T m^{-\theta}_T;\delta)
    \biggr],     
 \label{def: InvestConsumpValue}
\end{equation}
where 
$U(x; \delta) = \frac{1}{1-\frac{1}{\delta}}x^{1-\frac{1}{\delta}}$, $\delta\ne 1$, is the power utility function, 
$X_t$ follows
\begin{equation}
    \ud X_t = \pi_t X_t(b \ud t + \sigma \ud W_t + \sigma^0 \ud W_t^0) - c_tX_t \ud t,  \label{def: InvestConsumpSDE}
\end{equation}
and $X_0 = \xi$. The processes $\Gamma_t = \exp\EE[\log c_t |\mcF^0_t]$ and $m_t = \exp\EE[\log X_t |\mcF^0_t]$ are the mean-field interactions from the control and state processes. Two constraints are posed: $X_t \geq 0$, $c_t \geq 0$.

The interpretation of this problem is as follows. There are infinitely many agents trade in a common investment horizon $[0,T]$, each invests between a bond (with constant return rate $r$) and a private stock with dynamics $\ud S_t/S_t = b \ud t + \sigma \ud W_t + \sigma^0\ud W_t^0$, and consume $c_t$ of his wealth at time $t$. The portion of wealth into $S_t$ is denoted by $\pi_t$. Assuming $r \equiv 0$ without loss of generality, the wealth process reads \eqref{def: InvestConsumpSDE}. Then each agent aims to maximize his utility of consumption plus his terminal wealth compared to his peers' averages $\Gamma_t$ and $m_t$. To relate it to the formulation \eqref{eq:dyn-X-general-MFG}--\eqref{subchapRCML-num-eq:def-J-MFG}, $\ctrl \equiv (\alpha^1, \ctrl^2) = (\pi, c)$ will be a 2D control with the constraint $\alpha^2_t \geq 0$, $b(t, x, \nu, \alpha) = b\alpha^1 x  - \alpha^2 x$, $\sigma(t, x, \nu, \alpha) = \sigma \alpha^1 x$, $\sigma^0(t, x, \nu, \alpha) = \sigma^0 \alpha^1 x$, $f = -U$ and $g = -U$. The explicit solutions are derived in \cite{lacker2020many} and summarized in \cite[Appendix D]{MinHu:21}.

For this experiment, we use truncated signatures of depth $M=4$. The optimal controls $(\pi_t, c_t)_{0\le t\le 1}$ are parameterized by two neural networks $\pi(\cdot; \theta)$ and $c(\cdot; \theta)$, each with three hidden layers of size 64 and taking $(\zeta, t, X_t, m_t, \Gamma_t)$ as inputs due to the nature of heterogeneous extended MFG. Due to the extended mean-field interaction term $\Gamma_t$, we will propagate two conditional distribution flows, {\it i.e.}, two linear functionals $\hat {l}^{(\mt{k})}, \hat {l}_c^{(\mt{k})}$ during each iteration of fictitious play. Instead of estimating $m_t, \Gamma_t$ directly, we estimate $\EE[\log X_t|\mcF^0_t], \EE[\log c_t|\mcF^0_t]$ by $\langle \hat {l}^{(\mt{k})}, S^4(W_t^0) \rangle$, $\langle \hat{l}_c^{(\mt{k})}, S^4(W_t^0) \rangle$ and then take exponential to get $m_t, \Gamma_t$. To ensure the non-negativity condition of $X_t$, we evolve $\log X_t$ and then take exponential to get $X_t$. For optimal consumption, $c(\cdot; \theta)$ is used to predicted $\log c_t$ and thus $\exp c(\cdot; \theta)$ gives the predicted $c_t$. With 600 iterations of fictitious play and a learning rate of 0.1 decaying by a factor of 5 for every 200 iterations, the relative $L^2$ errors for $\pi_t, c_t, m_t, \Gamma_t$ are 0.1126, 0.0614, 0.0279, 0.0121, respectively. Figure \ref{fig:OCI} compares $X$ and $m$ to their approximations, and plots the maximized utilities. Further comparison with the existing literature, different choices of truncation $M$, and the ability to deal with higher $m_0$ are also discussed in \cite{MinHu:21}. 



\begin{figure}[hbt!]
    \centering
    \subfloat[$X_t$]{
         \includegraphics[width=0.32\columnwidth]{figure/InvestConsumption/SDE.pdf}
     }
     \subfloat[$m_t =  \exp\EE(\log X_t |\mcF^B_t)$]{
         \includegraphics[width=0.32\columnwidth]{figure/InvestConsumption/Xbar.pdf}
     }
     \subfloat[Maximized Utility]{
         \includegraphics[width=0.32\columnwidth, trim = {0em 15em 0 10em}]{figure/InvestConsumption/valid_util2.pdf}}
    
    \caption{MFG of optimal consumption and investment in Section~\ref{sec4_MFG_with_CN}. Panels (a) and (b) give three trajectories of $X_t$ and $m_t=\exp\bigl(\EE(\log X_t |\mcF^0_t)\bigr)$ (solid lines) and their approximations $\hat X_t$ and $\hat m_t$ (dashed lines) using different $(X_0, W, W^0)$ from validation data. Panel (c) shows the maximized utility computed using validation data over fictitious play iterations. Parameter choices are: $\delta \sim U(2, 2.5), b \sim U(0.25, 0.35), \sigma\sim U(0.2, 0.4), \theta, \xi \sim U(0,1), \sigma^0\sim U(0.2, 0.4)$, $\epsilon\sim U(0.5, 1)$. 
    }
    \label{fig:OCI}
\end{figure}






\subsubsection{BSDE-based deep learning algorithms}
\label{sec4_MFGFBSDE}

We now explain how to adapt the Deep BSDE method introduced in~\cite{MR3736669} and reviewed in Section~\ref{subsubsec:deepBSDE}  to mean-field FBSDEs. We recall that the principle of the method is to use neural networks to approximate $Y_0$ and $(Z_t)_{t \in [0,T]}$ and to train the neural network parameters by relying on Monte Carlo samples until the terminal condition is approximately matched. In the mean-field setting, the same idea can be used to solve forward-backward systems of McKean-Vlasov (MKV) SDEs;  see~\cite{fouque2019deep,carmona2019convergence2,germain2019numerical, hanhulong:22}. 


Let us consider the FBSDE~\eqref{eq:MKV-FBSDE-general-no-CN} in the absence of common noise, with interactions through the state distribution only, and uncontrolled volatility. We rewrite the problem as: minimize over $y_0 : \RR^d \to \RR^d$ and $z: \RR_+ \times \RR^d \to \RR^{d \times m}$ the cost functional
$$
	J(y_0, z) = \EE \left[ \, \left| Y^{y_0,z}_T - G(X^{y_0,z}_T, \Law(X^{y_0,z}_T)) \right|^2 \, \right],
$$
where $(X^{y_0,z}, Y^{y_0,z})$ solves
\begin{equation}
\label{eq:MKV-FBSDE-2forward}
\begin{cases}
\ud X^{y_0,z}_t
	=
	B\left(t, X^{y_0,z}_t, \Law(X^{y_0,z}_t), Y^{y_0,z}_t \right) \ud t
		+  \sigma(t, X_t) \ud  W_t, \quad t \ge 0,
	\\
	\ud  Y^{y_0,z}_t
	=
	- F\left(t, X^{y_0,z}_t, \Law(X^{y_0,z}_t), Y^{y_0,z}_t, \sigma\transpose(t, X_t) z(t, X^{y_0,z}_t)  \right) \ud t
		+ z(t, X^{y_0,z}_t) \ud  W_t, \quad t \ge 0, 
  \\
  X^{y_0,z}_0 \sim \mu_0, \qquad Y^{y_0,z}_0 = y_0 (X_0^{y_0,z}). 
\end{cases}
\end{equation}
The above problem is an MFC problem if we view $(X^{y_0,z}_t,Y^{y_0,z}_t)$ as state and $(y_0,z)$ as control. Under suitable conditions, the optimally controlled process $(X,Y)$ solves the MKV FBSDE system~\eqref{eq:MKV-FBSDE-general-no-CN} and vice versa. 


Then, to be able to implement this method, we can proceed similarly to the method described in Section~\ref{sec:directMethod}. The mean-field distribution can be approximated by an empirical distribution based on a finite population of interacting particles. Furthermore, the controls $y_0$ and $z$ can be replaced by neural networks, say $y_{\theta}$ and $z_{\omega}$ with parameters $\theta$ and $\omega$ respectively. Time can be discretized using for instance an Euler-Maruyama scheme. We thus obtain a new optimization problem over finite-dimensional parameters that can be adjusted using SGD. 


\begin{remark}[Theoretical analysis]
    Motivated by numerical schemes and in particular the above adaptation of the deep BSDE method \cite{MR3736669,HaJeE:18} to MKV FBSDEs, Reisinger, Stockinger and Zhang
    in~\cite{reisinger2020posteriori} analyzed a posteriori error for approximate solutions based on a discrete time scheme and a finite population of interacting particles \cite[Theorems 3.2 and 4.3]{reisinger2020posteriori}. \cite{hanhulong:22} proposed a deep learning method for computing MKV FBSDEs with a general form of mean-field interactions, and proved that the convergence of the numerical solution obtained by the proposed method to the true solution is free of the curse of dimensionality \cite[Theorem 3.9]{hanhulong:22} by using a special class of integral probability metrics previously developed in \cite{hanhulong:21}.
\end{remark}

 Although we focus here on the continuous-state space setting, the same strategy can be applied to finite-state MFGs;  see, \textit{e.g.},~\cite{aurell2022optimalstackelberg,aurell2022finitegraphon}. 






\paragraph*{Numerical illustration: a linear-quadratic mean-field game in systemic risk problems.} 
\label{sec:MFG-sysrisk-example-deepBSDE-CN}
We now consider the MFG version of the systemic risk model introduced in Section~\ref{sec:intro-LQsysrisk} which has been studied in Section~\ref{par:LQ-systemic-risk-finite-N} and revisited in Section~\ref{sec:LQsysrisk-revisited}. This MFG mean-field game has been analyzed in~\cite{CaFoSu:15}. Given a mean-field flow $\mu = (\mu_t)_{t\in[0,T]}$, the log-monetary reserves of a typical bank evolves according to the dynamics:
\begin{equation}
\label{eq:ex1_dynamics-mfg}
    \ud X_t = [a(\bar \mu_t - X_t)   + \alpha_t ] \ud t  + \sigma \left(\rho \ud W_t^0 + \sqrt{1-\rho^2} \ud W_t\right). %
\end{equation}
The standard Brownian motions $W^0$ and $W$ are independent, in which $W$ stands for the idiosyncratic noises and $W^0$ denotes the systemic shock, which is an example of common noise (see also Section~\ref{sec4_MFG_with_CN}).

The cost functions $f$ and $g$ appearing in \eqref{def:MFG-cost} takes the following form:
\begin{equation}
f(t, x, \nu,\alpha) = \half \alpha^2 - q \alpha(\bar \mu - x) + \frac{\eps}{2}(\bar \mu - x)^2,  \quad g(x, \nu) = \frac{c}{2}(\bar \mu - x)^2,
\end{equation}
which depend only on the mean $\bar \mu = \EE_{X\sim\mu}[X]$ of the first marginal of the state-action distribution $\nu$. 
It has been shown in~\cite{CaFoSu:15} that, in the MFG setting, the open-loop equilibrium is the same as the closed-loop Nash equilibrium, and it admits an explicit solution. Furthermore, it can be characterized using an MKV FBSDE system, that we omit for brevity; see~\cite{CaFoSu:15} for the details. If $\rho = 0$, then one can apply directly the method described above, with $y_0$ a function of $X_0$ and $z$ a function of $(t,X_t)$. When $\rho>0$, two changes need to be made: first, there is an extra process $Z^0$ to be learned, for which we use a neural network approximation as for $Z$; second, we expect the random variables $Z_t$ and $Z^0_t$ to depend not only on $X_t$ but also on the past of the common noise. In general, this would mean learning functions of the common noise's trajectory. However, in the present case, it is enough to rely on finite-dimensional information. Here, we add $\bar{\mu}_t$ as an input to the neural networks playing the roles of  $Z_t$ and $Z^0_t$, and this is sufficient to learn the optimal solution, see~\cite{CaFoSu:15}.


Figure~\ref{fig:ex-mfg-sysrisk-traj} displays three sample trajectories of $X$ and $Y$, obtained after training the neural networks for $Y_0, Z$ and $Z^0$, by simulating in a forward fashion the trajectories of $X$ and $Y$ using Monte Carlo samples and the same Euler-Maruyama scheme used in the numerical method. One can see that the approximation is better for $X$ than for $Y$, particularly towards the end of the time interval. This is probably because the BSDE is solved by guessing the initial point instead of starting from a terminal point, resulting in errors accumulated over time. Furthermore, \cite{carmona2019convergence2} shows that the results improve as the number of time steps, particles, and units in the neural network increase. In the numerical experiments presented here, we used the following parameters: $\sigma = 0.5, \rho = 0.5, q = 0.5, \epsilon = q^2 + 0.5 = 0.75, a = 1, c = 1.0$ and $T = 0.5$.


\begin{figure}[ht]
\centering
\subfloat[Trajectory of $X^i, i=1,2,3$]{
\includegraphics[width=.45\linewidth]{{figure/SYSRISKFBSDE/test2o_cmp_trajX_forHandbookMLFinance}.pdf}
}
\subfloat[Trajectory of $Y^i, i=1,2,3$] {
  \includegraphics[width=.45\linewidth]{{figure/SYSRISKFBSDE/test2o_cmp_trajY_forHandbookMLFinance}.pdf}
  }
\caption{Systemic risk MFG example solved by the algorithm described in Section~\ref{sec4_MFGFBSDE}. Left: three sample trajectories of $X$ using the neural network approach ('Deep Solver' with full lines, in cyan, blue, and green) or using the analytical formula ('benchmark' with dashed lines, in orange, red and purple). Right: three sample trajectories of $Y$ (similar labels and colors). Note that the analytical formula satisfies the true terminal condition, whereas the solution computed by neural networks satisfies it only approximately since the trajectories are generated in a forward way starting from the learned initial condition. %
}
\label{fig:ex-mfg-sysrisk-traj}
\end{figure}






\subsubsection{PDE-based deep learning algorithms}

Besides direct parameterization of the controls and BSDE-based methods, it is also possible to adapt PDE-based methods to solve MFGs. In fact, such methods can be used in two different ways. First, one can use them to tackle the forward-backward PDE system characteriziting the distribution and the value function (see Section~\ref{sec:MFG-PDE-system}). One can also try to solve the Master equation (see Section~\ref{sec:background-master-eq}). 



\paragraph{Deep learning for mean-field PDE systems}
\label{sec:MFPDE-deeplearning}




We now consider the PDE systems describing the equilibrium or social optimum in MFG or MFC, respectively. The Deep Galerkin Method (DGM) introduced in~\cite{SiSp:18} and reviewed in Section~\ref{sec:PDE} has been adapted to solve such PDE systems, see~\cite{al2022extensions,carmona2019convergence1,ruthotto2020machine,cao2020connecting,lin2020apac,AMSnotesLauriere}. We recall that the principle of the method is, for a single PDE, to replace the unknown function by a neural network and to optimize the parameters so as to minimize the residual of the PDE. 

For the sake of the presentation, we consider the MFG PDE system~\eqref{def:control-HJB-MFG}--\eqref{eq:MFG-general-KFP}. In line with the DGM method described in section~\ref{sec:PDE}, we proceed as follows. First, the MFG PDE system is rewritten as a minimization problem over the pair consisting of the density and the value function. The loss function is the sum of the two PDE residuals, as well as penalization terms for the initial and terminal conditions. Instead of the whole state space $\RR^d$, we focus on a compact subset $\tilde\dom \subset \RR^d$. If needed, extra penalization terms taking into account the boundary conditions can be added in the loss function. 
To be specific, we introduce the following loss function
\begin{equation}
\label{eq:loss-DGM-total}
	L(\mu,u) = L^\text{(KFP)}(\mu,u) + L^\text{(HJB)}(\mu,u),
\end{equation}
which is composed of one term for each PDE of the MFG system~\eqref{def:control-HJB-MFG}--\eqref{eq:MFG-general-KFP}. Each term is itself split into two terms: one for the residual inside the domain and one for the initial or terminal condition. The KFP loss function is
\begin{align}
\label{eq:loss-DGM-KFP}
	 L^\text{(KFP)}(\mu,u)
	 &= C^\text{(KFP)} \left\| 
        \displaystyle \partial_t \mu  - \sum_{i,j} \frac{\partial^2}{\partial_{x_i}\partial_{x_j}}\left( D_{i,j}\mu\right) + \diver\left( \mu b\right) \right\|^2_{L^2([0,T] \times \tilde\dom)} + C^\text{(KFP)}_0 \left\|  \mu(0)  - \mu_0 \right\|^2_{L^2(\tilde\dom)},
\end{align}
with $\nu_t = \mu_t \circ (I_d, \alpha(t,\cdot))^{-1}$ where $\ctrl(t,x) = \ctrl(t, x, \nu_t, \grad_x u(t,x), \Hess_x u(t,x))$, and $D$ and $b$ are defined as: 
\begin{align}
   D(t,x) = \frac{1}{2} \sigma(t, x, \nu_t, \alpha(t,x))\sigma(t,x, \nu_t, \alpha(t,x))\transpose, 
   \quad 
   b(t,x) = b(t,x,\nu_t,\ctrl(t,x)).
\end{align} 
The HJB loss function is
\begin{equation}
\label{eq:loss-DGM-HJB}
	 L^\text{(HJB)}(\mu,u)
	= C^\text{(HJB)} \left\| \partial_t u   + \min_{\alpha \in \mc{A}} H(\cdot,\cdot, \nu, \grad_x u, \Hess_x u, \alpha) \right\|^2_{L^2([0,T] \times \tilde\dom)}
	+ C^\text{(HJB)}_T \left\| u(T) - g(\cdot,  \mu(T) ) \right\|^2_{L^2(\tilde\dom)},
\end{equation}
with $H$ defined by~\eqref{def:control-H-MFG}. The weights $C^\text{(KFP)}, C^\text{(KFP)}_0, C^\text{(HJB)},$ and $C^\text{(HJB)}_T$ are positive constants that are used to tune the importance of each component relatively to the other components. If $(\mu,u)$ is a smooth enough solution to the PDE system~\eqref{def:control-HJB-MFG}--\eqref{eq:MFG-general-KFP}, then $L(\mu,u) = 0$. From here, the same strategy as in the DGM can be applied: one can look for an approximate solution using a class of parameterized functions for $\mu$ and $u$, replace the $L^2$ norms by integrals, and use samples to get Monte Carlo estimates; see~\cite{SiSp:18} and Section~\ref{sec:PDE} for more details.



\begin{remark} 
The same ideas can be applied to a variety of settings such as ergodic MFG~\cite{carmona2019convergence1}, non-separable Hamiltonians~\cite{ASSOULI2023113802} or finite-state MFGs~\cite{luo2023deep}. 
Each case may require some adjustments. For instance, in the case of ergodic MFGs, the initial and terminal conditions are replaced by normalization conditions; see~\cite{LaLi:2007}. Furthermore, if the PDE system was initially posed on a bounded domain and the solution had to satisfy boundary conditions, then these extra conditions could be dealt with by adding more penalty terms as in~\cite{carmona2019convergence1} or changing the architecture of the neural networks~\cite{cao2020connecting}. 
\end{remark}






\vskip 6pt

\paragraph*{Numerical illustration: a mean-field model of optimal execution.}


We now present an example based on a model of optimal execution. This model is similar to the one studied in Subsection \ref{sec:directMethod}. We consider a population of traders in which each trader wants to liquidate $Q_0$ shares of a given stock by a fixed time horizon $T$. At time $t\in[0,T]$, we denote by $S_t$ the price of the stock, by $Q_t$ the inventory ({\it i.e.}, number of shares) held by the representative trader, and by $X_t$ their wealth. These state variables are subject to the following dynamics 
\begin{equation*}
\begin{cases}
    dS_t = \gamma \bar{\mu}_t  dt + \sigma \ud W_t,\\
    dQ_t = \ctrl_t \ud t,\\
    dX_t = -\ctrl_t(S_t+\kappa \ctrl_t)\ud t.
\end{cases}
\end{equation*}
The evolution of the price $S$ is stochastic, representing that it can not be predicted with certainty. The randomness, scaled by $\sigma$, comes in through a standard Wiener process $W$. Furthermore, the drift of $S$ captures the permanent price impact $\gamma\bar{\mu}_t$ at time $t$. Here $\gamma>0$ is a multiplicative constant and $\bar{\mu}_t$ is the aggregate trading rate of all the traders.
The control $\ctrl_t$ at time $t$ corresponds to the individual rate of trading of the representative trader. Last, $\kappa>0$ is a constant that represents a quadratic transaction cost.


We assume that the representative agent tries
to maximize the following quantity, in which the first two terms reflect their payoff while the last two terms capture their risk aversion
$$
    \mathbb{E} \left[ X_T + Q_TS_T - A |Q_T|^2 - \phi \int_0^T |Q_t|^2 \ud t \right].
$$
 The constants $\phi>0$ and $A>0$ give weights to penalties for holding inventory through time and at the terminal time, respectively. 
 
 \begin{remark}
 Except for the fact that $\bar{\mu}_t$ is here endogenous, this is the model considered in \cite{MR3500455}, to which a deep learning method has been applied in~\cite{leal2020learning} to approximate the optimal control on real data.
In contrast with the model studied in Subsection \ref{sec:directMethod}, the model considered here is not linear-quadratic and the inventory is not directly subject to random shocks. We refer the interested reader to \cite{CarmonaWebster} and  \cite{CarmonaLeal} for more details and variants of these models.
\end{remark}

Although this problem is formulated with three state variables, we can actually reduce the complexity of the problem in the following way. When $(\bar{\mu}_t)_{0\le t\le T}$ is given, the optimal control of the representative agent can be found by solving an HJB equation. Following~ \cite{MR3500455}, the value function $V(t,x,s,q)$ can be decomposed as
 $V(t,x,s,q) = x + qs + v(t,q)$ for some function $v$ which is a solution to
$$
    -\gamma\bar{\mu} q = \partial_t v - \phi q^2 + \sup_\ctrl \{\ctrl \partial_q v - \kappa \ctrl^2\},
$$
with terminal condition $v(T,q) = - A q^2$. The maximizer in the supremum leads to the optimal control, which can be expressed as: $\ctrl^*_t(q) = \frac{\partial_q v(t,q)}{2\kappa}$. Based on this and going back to the consistency condition yields that, at equilibrium, the aggregate trading rate is
$$
    \bar{\mu}_t = \int \ctrl^*_t(q) \mu(t,dq) = \int \frac{\partial_q v(t,q)}{2\kappa} \mu(t, dq),
$$
 where $\mu(t,\cdot)$ is the distribution of inventories at time $t$ satisfying the KFP PDE:
$$
    \partial_t \mu + \partial_q\left( \mu  \frac{\partial_q v(t,q)}{2\kappa}\right) = 0, t \ge 0, \qquad \mu(0,\cdot) = \mu_ 0.
$$
As a consequence, the equilibrium solution of the MFG satisfies
\begin{equation}
\label{eq:CL-PDE-reduced}
\left\{
\begin{aligned}
    &\quad -\gamma\bar{\mu} q = \partial_t v - \phi q^2 + \frac{|\partial_q v(t,q)|^2}{4\kappa},
    \\
    &\quad \partial_t \mu + \partial_q\left( \mu  \frac{\partial_q v(t,q)}{2\kappa}\right) = 0,
    \\
    &\quad \bar{\mu}_t = \int \frac{\partial_q v(t,q)}{2\kappa} \mu(t, dq),
    \\
    &\quad \mu(0,\cdot) = \mu_ 0, v(T,q) = - A q^2. 
\end{aligned}
\right.
\end{equation}
The mean-field coupling between the two equations is non-local since it involves $\bar{\mu}_t$, and it combines the population distribution with the HJB solution. The value function, the associated control, and the mean of the distribution can be computed by solving a system of ODEs, which provides a benchmark to test numerical methods. We refer to \cite{cardaliaguet2017mean} for more details.

Moreover, \cite{al2022extensions} proposed a further change of variables to simplify the numerical computations and then used the DGM to approximate the solution of the transformed PDE system. Here, to simplify the presentation, we stick to the above PDE system~\eqref{eq:CL-PDE-reduced} and solve it directly using DGM. The initial and terminal conditions are imposed by penalization. For the non-local term $\bar{\mu}_t$, we use Monte Carlo samples to estimate the integral. In the implementation, we used the following values for the parameters: $T=1$, $\sigma=0.3$, $A = 1$, $\phi = 1$, $\kappa = 1$, $\gamma = 1$, and a Gaussian initial distribution with mean $4$ and variance $0.3$. To ensure that the neural network for the distribution always outputs positive values, we used on the last layer the exponential function as an activation function.   Figure~\ref{fig:ex-mfg-crowd-trade-distrib} shows the evolution of the distribution $m$. Figure~\ref{fig:ex-mfg-crowd-trade-value} shows the control obtained from the neural network approximating the HJB solution. The final distribution is concentrated near $0$, which is consistent with the intuition that the traders need to liquidate. Furthermore, the learned control coincides with the theoretical optimal control that can be computed by solving an ODE system~\cite{cardaliaguet2017mean}.


\begin{figure}[ht]
\subfloat[]{
  \includegraphics[width=0.45\linewidth]{{figure/CROWDTRADEDGM/test14n_fig_nu_surface_step25001}.pdf}
  }
\subfloat[]{
\includegraphics[width=0.45\linewidth]{{figure/CROWDTRADEDGM/test14n_fig_nu_contour_benchmark_step25001}.pdf}
}
\caption{Trade crowding MFG example in Section~\ref{sec:MFPDE-deeplearning} solved by DGM. Evolution of the distribution $m$. Left: surface with the horizontal axes representing time and space and the vertical axis representing the value of the density. Right: contour plot of the density with a dashed red line corresponding to the mean of the density computed by the semi-explicit formula. %
}
\label{fig:ex-mfg-crowd-trade-distrib}
\end{figure}

\begin{figure}[ht]
\subfloat[]{
\includegraphics[width=0.3\linewidth]{{figure/CROWDTRADEDGM/test14n_fig_ctrl_curves_0_step25001}.pdf}
}
\subfloat[]{
\includegraphics[width=0.3\linewidth]{{figure/CROWDTRADEDGM/test14n_fig_ctrl_curves_30_step25001}.pdf}
}
\subfloat[]{
\includegraphics[width=0.3\linewidth]{{figure/CROWDTRADEDGM/test14n_fig_ctrl_curves_49_step25001}.pdf}
}
\caption{Trade crowding MFG example in Section~\ref{sec:MFPDE-deeplearning} solved by DGM. Each plot corresponds to the control at a different time step: Optimal control $\ctrl^*$ (dashed line) and learned control (full line). %
}
\label{fig:ex-mfg-crowd-trade-value}
\end{figure}




























\paragraph{Deep learning for mean-field master equation}
\label{sec:mastereq-deeplearning}


We now turn our attention to the question of solving the MFG master equation (Section~\ref{sec:background-master-eq}). Intuitively, the main motivation is to be able to approximate the value function of a representative player for any population distribution. This is in contrast with the methods presented above, which are based on controls that are fully decentralized in the sense that they are functions of the time and the state of the agent only. The fact that they do not depend on the population distribution is an advantage in that it simplifies the implementation, but it is also a limitation since the agent is not able to react to new distributions. For example, if the initial distribution is not known, the agent is not able to solve the forward equation and hence she is not able to anticipate the distribution at future times. The presence of common noise in the dynamics poses a similar challenge. For these reasons, being able to approximately solve the master equation is interesting for applications. When the state space is continuous, the distribution is an infinite dimensional object which is hard to approximate. For simplicity, we will thus focus here on a finite-state setting, in which case the distribution is simply a histogram. The convergence of finite-state MFGs to continuous-state MFGs has been studied for instance in~\cite{hadikhanloo2019finite}. Even though assuming the state space to be finite resolves the question of approximating the distribution, the master equation is posed on a high-dimensional space if the number of states is large. We will hence rely once again on neural networks to approximate the solution to this equation. 




\smallskip
\noindent {\bf Master equation for finite state MFG.} We consider a finite-state MFG model based on the presentation of such models in~\cite[Section 7.2]{carmona2018probabilistic}. We consider a finite state space $\cE = \{e_1, \dots, e_d\}$ and an action space $\cA \subseteq \RR^\ctrldim$, which can be discrete or continuous. The states can be viewed as one-hot vectors, \textit{i.e.}, as the elements of a canonical basis of $\RR^d$. Then, the set of probability distributions on $\cE$ is the simplex $\{m \in \RR^d \,| \, \sum_{i=1}^d m_i = 1\}$, and we will sometimes write $m(x)$ instead of $m(\{x\})$. The running cost and the terminal cost are denoted by $f: \cE \times \cP(\cE) \times \cA \to \RR$ and  $g: \cE \times \cP(\cE) \to \RR$. The dynamics are given by a jump rate function denoted by $\lambda : \cE \times \cP(\cE) \times \cA \to \RR$. 
We denote by $\RR^\cE$ the set of functions from $\cE$ to $\RR$.

In this context, a finite state MFG equilibrium is a pair $(\hat{m}, \hat{\ctrl})$ with $\hat{m}: [0,T] \times \cE \to \RR$ and $\hat{\ctrl}: [0,T] \times \cE \to \cA$ such that
\begin{enumerate}
	\item $\hat{\ctrl}$ minimizes
\begin{align*}
	J^{MFG}_{\hat{m}}: \ctrl \mapsto  \EE \left[\int_0^T f(X_t^{\hat{m}, \ctrl}, \hat{m}(t,\cdot), \ctrl(t,X_t^{\hat{m}, \ctrl}) ) dt + g(X_T^{\hat{m}, \ctrl}, \hat{m}(T,\cdot)) \right],
\end{align*}
subject to: $X^{\hat{m}, \ctrl} = (X_t^{\hat{m}, \ctrl})_{t \ge 0}$ is a nonhomogeneous $\cE$-valued Markov chain with transition probabilities determined by the $Q$-matrix of rates $q^{\hat{m}, \ctrl}: [0,T] \times \cE \times \cE \to \RR$ given by
\begin{equation}
\label{master-eq-num:q-finite-MFG}
	q^{\hat{m}, \ctrl}(t, x,x') = \lambda(x, x', \hat{m}(t,\cdot), \ctrl(t,\cdot)), \qquad (t,x,x') \in [0,T] \times \cE \times \cE,
\end{equation}
and $X_0^{\hat{m}, \ctrl}$ has distribution with density $m_0$;
	\item For all $t \in [0,T]$, $\hat{m}(t,\cdot)$ is the law of $X_t^{\hat{m}, \hat{\ctrl}}$.
\end{enumerate}

The Hamiltonian of the problem is defined as
$$
	H(x, m, h) = \sup_{\ctrl \in \cA} -L(x, m, h, \ctrl)
$$ 
where $L: \cE \times \cP(\cE) \times \RR^\cE \times \cA \to \RR$ denotes the Lagrangian
$$
	L(x, m, h, \ctrl) = \sum_{x' \in \cE} \lambda(x, x', m, \ctrl) h(x') + f(x, m, \ctrl).
$$
Under suitable assumptions on the model, the supremum in the definition of $H$ admits a unique maximizer for every $(x, m, h) \in \cE \times \cP(\cE) \times \RR^\cE$, that we denote by
\begin{equation}
\label{master-eq-num:finiteMFG-ctrl-argmax}
	\ctrl^*(x, m, h) = \argmax_{\ctrl \in \cA} -L(x, m, h, \ctrl).
\end{equation}
The coefficients of the rates of the $Q$-matrix when using the optimal control are denoted by 
$$
	q^*(x,x',m,h) = \lambda\big(x, x', m, \ctrl^*(x,m, h)\big),
$$
where $q^*: \cE \times \cE \times \cP(\cE) \times \RR^\cE \to \RR$.


Similarly to the continuous setting (see Section~\ref{sec:mfg-theoretical-background}) the mean-field Nash equilibrium can be characterized using a forward-backward system of deterministic or stochastic equations. Using the deterministic approach, the optimal conditions take the form of an ODE system (instead of a PDE system as in the continuous space case). The system is composed of a forward ODE for the mean-field $m: [0,T] \times \cE \to \RR$ and a backward ODE for the value function $u: [0,T] \times \cE \to \RR$. Under suitable assumptions (see, \textit{e.g.}, \cite[section 7.2]{carmona2018probabilistic}), there is a unique MFG equilibrium $(\hat{m}, \hat{\ctrl})$, which is characterized by:
$$
	\hat{\ctrl}(t,x) = \ctrl^*(x, \hat m(t,\cdot), \hat u(t,\cdot)), 
$$  
where $\ctrl^*$ is defined by~\eqref{master-eq-num:finiteMFG-ctrl-argmax} and $(u,m)$ solves the forward-backward system
\begin{equation}
    \label{master-eq-num:ODE-system-finiteMFG}
    \begin{dcases}
        \displaystyle 0 
	   = - \partial_t \hat u(t, x) + H(x, \hat m(t,\cdot), \hat u(t,\cdot)),
	    \quad (t,x) \in [0,T) \times \cE,,
        \\
        0 
	    = \partial_t \hat m(t, x) - \sum_{x' \in \cE} \hat m(t, x') q^*(x', x, \hat m(t,\cdot), \hat u(t,\cdot)), 
	    \quad (t,x) \in (0,T] \times \cE,
	    \\
	    \hat u(T,x) = g(x, \hat m(T,\cdot)), \qquad \hat m(0,x) = m_0(x), 
	    \quad x \in \cE.
    \end{dcases}
\end{equation}

This ODE system can be solved using for example techniques discussed in previous sections for forward-backward PDE or SDE systems. However, this assumes that the initial distribution $m_0$ is known and when it is unknown, new techniques are required. We thus consider the master equation. 


As in the continuous space case described in Section~\ref{sec:background-master-eq}, the solution to the master equation makes the dependence of $\hat u$ and $\hat m$ completely explicit. In the present discrete space setting, the master equation can be written as follows (see, \textit{e.g.}, \cite[section 7.2]{carmona2018probabilistic})
 \begin{equation}
 \label{master-eq-num:master-finiteMFG}
 	-\partial_t \cU(t,x,m) 
	+ H(x,m,\cU(t, \cdot, m))
	- \sum_{x' \in \cE} h^*(m ,\cU(t, \cdot, m))(x') \frac{\partial \cU(t, x ,m)}{\partial m(x')} = 0, 
 \end{equation}
for $(t,x,m) \in [0,T] \times \cE \times \cP(\cE)$, with the terminal condition $\cU(T,x,m) = g(x,m)$, for $(x,m) \in \cE \times \cP(\cE)$. The function $h^*: [0,T] \times \cP(\cE) \times \RR^\cE \times \cE \to \RR$ is defined as
$$
	h^*(m, u)(x') = \sum_{x \in \cE} \lambda(x, x', m, \ctrl^*(x, m, u)) m(x).
$$
Besides a simple representation of probability distributions, the fact that the state space is finite has another advantage: we do not need to involve the notions of derivative with respect to a measure discussed in Section~\ref{sec:background-master-eq}. Instead, we can rely on standard partial derivatives with respect to the finite-dimensional inputs of $\cU$. As a matter of fact, in the above equation, $\displaystyle \frac{\partial \cU(t, x ,m)}{\partial m(x')}$ denotes the standard partial derivative of $\RR^d \ni m \mapsto \cU(t,x,m)$ with respect to the coordinate corresponding to $x'$ (recall that $m$ is viewed as a vector of dimension $d$). The analog of~\eqref{eq:masterfield-to-u} in the continuous case is 
\begin{equation}
\label{eq:master-eq-to-hjb-sol-finitestate}
    \cU(t,x,\hat m(t)) = \hat u(t,x),
\end{equation}
where $\hat m = (\hat m(t))_t$ is the mean-field equilibrium distribution flow. Notice that both $\hat m$ and $\hat u$ implicitly depend on the initial distribution $m_0$, but $\cU$ does not. 





The master equation~\eqref{master-eq-num:master-finiteMFG} is posed on a possibly high dimensional space since the number $d$ of states can be large. To numerically solve this equation, we can thus rely on deep learning methods for high-dimensional PDEs, such as the DGM introduced in~\cite{SiSp:18} and already discussed above in Sections~\ref{sec:PDE} and~\ref{sec:MFPDE-deeplearning}. 
For the sake of completeness, let us mention that this technique boils down to approximating $\cU$ by a neural network, say $\cU_\theta$ with parameters $\theta$, and using SGD to adjust the parameters $\theta$ such that the residual of~\eqref{master-eq-num:master-finiteMFG} is minimized and the terminal condition is satisfied. SGD as described in Algorithm~\ref{algo:SGD-generic} in Appendix~\ref{sec:SGD-var} is used, where a sample is $\xi = (t,x,m) \in [0,T] \times \cE \times \cP(\cE)$ and the loss function is
\begin{equation}
\label{master-eq-num:master-finiteMFG-residual}
	\mathfrak{L}(\cU_\theta, \xi) = \left|\partial_t \cU_\theta(t,x,m) - H(x,m,\cU_\theta(t,x,m))
	+ \sum_{x' \in \cE} h^*(m ,\cU_\theta(t,x,m))(x') \frac{\partial \cU_\theta(t,x,m)}{\partial m(x')}\right|^2.
\end{equation}

 








\paragraph*{Numerical illustration: A Cybersecurity model.} 
Here we present an example of the application of the above method. We consider the cybersecurity introduced in~\cite{MR3575619}; see also~\cite[Section 7.2.3]{carmona2018probabilistic}. Each player owns a computer and her goal is to avoid being infected by a malware. The state space is denoted by $\cE = \{DI, DS, UI, US\}$, which represents the four possible states in which a computer can be depending on its protection level -- defended (D) or undefended (U) -- and on its infection status  -- infected (I) or susceptible (S) of infection. The player can choose to switch its protection level between D and U. The change is not instantaneous so the player can only influence the transition rate. We represent by ``$1$'' the fact that the player has the intention to change its level of protection (be it from D to U or from U to D). On the other hand, ``$0$'' corresponds to the situation where the player does not try to change her protection level. So the set of possible actions is $\cA = \{0,1\}$. When the action is equal to $1$, the change of level of protection takes place at a rate  $\rho >0$. A computer in states DS or US might get infected either directly by a hacker or by getting the virus from an infected computer. We denote by $v_H q_{inf}^D$ (resp. $v_H q_{inf}^U$) the rate of infection from a hacker if the computer is defended (resp. undefended). We denote by $\beta_{UU}\mu(\{UI\})$ (resp. $\beta_{UD}\mu(\{UI\})$) the rate of infection from an undefended infected computer if the computer under consideration is undefended (resp. defended). Likewise, we denote by $\beta_{DU}\mu(\{DI\})$ (resp. $\beta_{DD}\mu(\{DI\})$) the rate of infection from a defended infected computer if the computer under consideration is undefended (resp. defended). Note that these rates involve the distribution since the probability of getting infected should increase with the number of infected computers in the rest of the population. Last, an infected computer can recover and switch to the susceptible state at rate $q_{rec}^D$ or $q_{rec}^U$ depending on whether it is defended or not.
These transition rates can be summarized in a matrix form: for $m \in \cP(\cE), a \in \cA$, 
$$
	\lambda(\cdot, \cdot, m, a) 
	= \left( \lambda(x, x', m, a) \right)_{x,x' \in \cE}
	= \begin{pmatrix}
	\dots 	& 		P^{m,a}_{DS \rightarrow DI}	&	 \rho a 	&	0
	\\
	q_{rec}^D 	& 	\dots 		&	 0	&	\rho a
	\\
	\rho a 	& 	0 		&	 \dots	&	P^{m,a}_{US \rightarrow UI}
	\\
	0	&	\rho a	&	q_{rec}^U	& \dots
	\end{pmatrix},
$$
where 
\begin{align*}
	&P^{m,a}_{DS \rightarrow DI} = v_H q_{inf}^D + \beta_{DD} m(\{DI\})  + \beta_{UD} m(\{UI\}) ,
	\\
	&P^{m,a}_{US \rightarrow UI} = v_H q_{inf}^U + \beta_{UU} m(\{UI\}) + \beta_{DU} m(\{DI\}).
\end{align*}
The dots ($\dots$) on each row stand for the value such that the sum of the coefficients on this row equals $0$. 

We assume that each player wants to avoid seeing her computer being infected, but protecting a computer costs some resources. So the running cost is of the form
$$
	f(t, x, \nu, \ctrl) = -\left[ k_D \indic_{\{DI, DS\}}(x) + k_I \indic_{\{DI, UI\}}(x)\right],
$$
where $k_D>0$ is a protection cost to be paid whenever the computer is defended, and $k_I>0$ is a penalty incurred if the computer is infected. We consider $g \equiv 0$ (no terminal cost).

By using the DGM, we train a neural network $\cU_\theta$ to approximate the solution $\cU$ to the master equation. Equation~\eqref{eq:master-eq-to-hjb-sol-finitestate} provides us with a way to check how accurate this approximation is:  we can fix an initial distribution, solve the forward-backward ODE system (which is easy given the initial condition), and then compare the value of the neural network $\cU_\theta$ evaluated along the equilibrium flow of distributions with the solution to the backward ODE for the value function. To be specific, for every $m_0$, we first compute the equilibrium value function $\hat u^{m_0}$ and the equilibrium flow of distributions $\hat m^{m_0})$. We then evaluate $\cU_\theta(t,x,\hat m^{m_0}(t,\cdot))$ for all $t \in [0,T]$ and check how close it is to $\hat u^{m_0}(t,x)$ for each of the four possible states $x$.  Figures~\ref{AMS-num-fig:finiteMFG-cyber-Master-m0-1}--\ref{AMS-num-fig:finiteMFG-cyber-Master-m0-3} show that the two curves (for each state) coincide for at least three different initial conditions. This means that, using the DGM, we managed to train a neural network that accurately represents the value function of a representative player for various distributions at once. In the numerical experiments, we used the following values for the parameters
\begin{align*}
&\beta_{UU} = 0.3,
\beta_{UD} = 0.4,
\beta_{DU} = 0.3,
\beta_{DD} = 0.4,
\qquad v_H = 0.2,
\lambda = 0.5,
\\
&q_{rec}^D = 0.1, 
q_{rec}^U = 0.65, 
q_{inf}^D = 0.4, 
q_{inf}^U = 0.3, 
\qquad k_D = 0.3, k_I = 0.5.
\end{align*}

\begin{figure}[ht]
\subfloat[]{		\includegraphics[width=.45\columnwidth]{{figure/MASTER-FINITEMFG/Master_v21/test5/testE_diffShapeUcross/test1_mu_evol}.pdf}
}
\subfloat[]{
		\includegraphics[width=.45\columnwidth]{{figure/MASTER-FINITEMFG/Master_v21/test5/test1_u_master_evol}.pdf}
}
	\caption{MFG Cybersecurity example in Section~\ref{sec:mastereq-deeplearning}. Test case 1: Evolution of the distribution $m^{m_0}$ (left) and the value function $u^{m_0}$ and $\cU(\cdot, \cdot, m^{m_0}(\cdot))$ (right) for $m_0 = (1/4, 1/4, 1/4, 1/4)$. First published in~\cite{AMSnotesLauriere} by the American Mathematical Society.} 
 	\label{AMS-num-fig:finiteMFG-cyber-Master-m0-1}
\end{figure}



\begin{figure}[ht]
\subfloat[]{
		\includegraphics[width=.45\columnwidth]{{figure/MASTER-FINITEMFG/Master_v21/test5/testE_diffShapeUcross/test2_mu_evol}.pdf}
}
\subfloat[]{
\includegraphics[width=.45\columnwidth]{{figure/MASTER-FINITEMFG/Master_v21/test5/test2_u_master_evol}.pdf}
}
	\caption{MFG Cybersecurity example in Section~\ref{sec:mastereq-deeplearning}. Test case 2: Evolution of the distribution $m^{m_0}$ (left) and the value function $u^{m_0}$ and $\cU(\cdot, \cdot, m^{m_0}(\cdot))$ (right) for  $m_0 = (1, 0, 0, 0)$. First published in~\cite{AMSnotesLauriere} by the American Mathematical Society.} 
 	\label{AMS-num-fig:finiteMFG-cyber-Master-m0-2}
\end{figure}


\begin{figure}[ht]
\subfloat[]{
\includegraphics[width=.45\columnwidth]{{figure/MASTER-FINITEMFG/Master_v21/test5/testE_diffShapeUcross/test3_mu_evol}.pdf}
}
\subfloat[]{
\includegraphics[width=.45\columnwidth]{{figure/MASTER-FINITEMFG/Master_v21/test5/test3_u_master_evol}.pdf}
}
	\caption{MFG Cybersecurity example in Section~\ref{sec:mastereq-deeplearning}. Test case 3: Evolution of the distribution $m^{m_0}$ (left) and the value function $u^{m_0}$ and $\cU(\cdot, \cdot, m^{m_0}(\cdot))$ (right) for  $m_0 = (0, 0, 0, 1)$. First published in~\cite{AMSnotesLauriere} by the American Mathematical Society.} 
 	\label{AMS-num-fig:finiteMFG-cyber-Master-m0-3}
\end{figure}






