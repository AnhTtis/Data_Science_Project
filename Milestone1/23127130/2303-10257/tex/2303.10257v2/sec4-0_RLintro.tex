
All the previous methods rely, in one way or another, on the fact that the cost functions $f$ and $g$ as well as the drift $b$ and the volatility $\sigma$ (\emph{cf.} \eqref{def:control-Xt}--\eqref{def:control-cost}) are known. However, in many applications, coming up with a realistic and accurate model is a daunting task. It is sometimes impossible to guess the form of the dynamics, or the way the costs are incurred. This has motivated the development of so-called model-free methods. The reinforcement learning (RL) theory provides a framework for studying such problems.  Intuitively, an agent evolving in an environment can take actions and observe the consequences of her actions: the state of the environment (or her own state) changes, and a cost is incurred to the agent. The agent does not know how the new state and the cost are computed. The goal for the agent is then to learn an optimal behavior by trial and error.


Numerous algorithms have been developed under the topic of RL; see, {\it e.g.}, the surveys and books \cite{kaelbling1996reinforcement,Busoniu08,li2017deep,sutton2018reinforcement,hambly2021recent}. Most of them focus on RL itself, with state-of-the-art methods in single-agent or multi-agent problems and some provide theoretical guarantees of numerical performances. We aim to review its connections to stochastic control and games, as well as the mean-field setting. We shall start by discussing how problems in Section~\ref{sec:SCP} are formulated as single-agent RL\footnote{The terminology RL is from the perspective of artificial intelligence/computer science. In the operation research community, it is often called approximate dynamic programming (ADP) \cite{powell2007approximate}.}. Although we here focus on the traditional presentation of RL in discrete time, let us mention that a continuous-time stochastic optimal control viewpoint on RL has also been studied, see {\it e.g.}, \cite{munos2006policy} for a policy gradient algorithm,  and \cite{wang2020continuousmeanvar,wang2020reinforcement} for a mean-variance portfolio problem and for generic continuous time and space problems. 
Furthermore, \cite{jia2021policy,jia2021policy2} studied policy-based and value function-based RL methods.
It has also been extended in several directions, such as variance reduction techniques~\cite{kobeissi2022variance}, risk-aware problems~\cite{jaimungal2022robustrl} and mean-field games~\cite{guo2022mfgentropyregu} and \cite{firoozi2022exploratory}. 






Since the environment is unknown to the agent, a recurring question that needs to be addressed is the tradeoff between exploration and exploitation, whether in the single-agent, multiple-agent, or mean-field settings. Exploitation involves choosing actions that the agent believes will yield the highest immediate rewards based on her current knowledge, or equivalently; the agent follows the best-known strategies to maximize its short-term gains. Exploration, on the other hand, means taking actions that the agent has less information about, even if it might not result in the highest immediate rewards. The purpose of exploration is to gather more data and learn about the environment to improve the agent's long-term performance. Pure exploitation can lead to suboptimal decisions if the agent's initial knowledge is incomplete or incorrect. She may miss out on potentially better actions. Pure exploration, while informative, can be inefficient and may lead to delayed or reduced rewards, as the agent keeps trying unproven actions.

Effective RL algorithms aim to strike a balance between these opposing objectives. Alongside classical approaches like $\epsilon$-greedy and Upper-Confidence-Bound for exploration, recent developments have expanded our understanding of managing this trade-off to optimize an agent's learning and decision-making process in dynamic and uncertain environments. Notable contributions include \cite{wang2020reinforcement}, which elucidates the trade-off through entropy regularization from a continuous-time stochastic control perspective and offers theoretical support for Gaussian exploration, particularly in the context of a linear-quadratic regulator. \cite{guo2022mfgentropyregu} explores entropy regularization and devises a policy-gradient algorithm for exploration within the mean-field setting. Furthermore, \cite{delarue2021exploration} demonstrates that common noise can serve as an exploration mechanism for learning the solution of a mean-field game, through a linear-quadratic model.
