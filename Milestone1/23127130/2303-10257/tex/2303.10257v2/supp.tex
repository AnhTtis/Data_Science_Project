\section{List of Acronyms}

\begin{longtable}{l|l}
ADAM    & Adaptive Moments Gradient Descent\\
BSDE    & Backward Stochastic Differential Equation \\
2BSDE   & Second Order Backward Stochastic Differential Equation \\
CDC     & Centers for Disease Control and Prevention \\
DNN     & Deep Neural Networks \\
DBDP    & Deep Backward Dynamic Programming \\
Deep BSDE & Deep Backward Stochastic Differential Equation \\
DDPG & Deep Deterministic Policy Gradient \\
DGM     & Deep Galerkin Method \\
DFP     & Deep Fictitious Play \\
DPG     & Deterministic Policy Gradient \\
DPP     & Dynamic Programming Principle \\
FBSDE   & Forward Backward Stochastic Differential Equation \\
FNN     & Feedforward Neural Network \\
FP      & Fokker-Planck \\
GRU     & Gated Recurrent Unit\\
HJB     & Hamilton-Jacobi-Bellman \\
KFP     & Kolmogorov-Fokker-Planck \\
LQ      & Linear-Quadratic \\
LSTM    & Long Short-term Memory \\
MARL    & Multi-agent Reinforcement Learning \\
MC      & Monte Carlo \\
MDP     & Markov Decision Process \\
MFC     & Mean-Field Control \\
MFG     & Mean-Field Game \\
MFMDP   & Mean-Field Markov Decision Process \\
MKV     & McKean-Vlasov \\
MKV FBSDE & McKean-Vlasov Forward Backward Stochastic Differential Equation \\
NE      & Nash Equilibrium \\
NN & Neural Network\\
ODE     & Ordinary Differential Equation \\
PDE     & Partial Differential Equations \\
PINN    & Physics Informed Neural Network \\
ReLu    & Rectified Linear Unit \\
RL      & Reinforcement Learning \\
RNN     & Recurrent Neural Network \\
SC      & Stochastic Control \\
SDE     & Stochastic Differential Equation \\
SDDE    &  Stochastic Differential Delay Equation \\
SDFP    & Scaled Deep Fictitious Play \\
SGD     & Stochastic Gradient Descent \\
Sig-DFP & Signatured Deep Fictitious Play \\
TD      & Temporal-Difference
\end{longtable}



\section{List of Frequently Used Notations}
\begin{longtable}{l|l}
    \hline
    \hline
    \multicolumn{2}{l}{Deep learning related notations}
    \\
    \hline
    $\theta$ & neural network parameters 
    \\
    $\varphi^\theta$ & neural network with parameters $\theta$ \\
    $J(\theta)$ & optimization function for training the parameters $\theta$ \\
    \hline
    \hline
    \multicolumn{2}{l}{Optimal control related notations}
    \\
    \hline
    $x$ & state variable
    \\
    $(X_t)_{t \in [0,T]}$ & state process
    \\
    $(Y_t, Z_t)_{t \in [0,T]}$ & backward and adjoint processes 
    \\
    $(\ctrl_t)_{t \in [0,T]}$, $(\beta_t)_{t \in [0,T]}$ & control process
    \\
    $(W_t)_{t \in [0,T]}$, $(W^0_t)_{t \in [0,T]}$ & Wiener process
    \\
    $d$ & dimension of the state
    \\
    $k$ & dimension of the action
    \\
    $m$ & dimension of the noise
    \\
    $b(t, x, \ctrl)$ & drift coefficient of the state process
    \\
    $\sigma(t, x, \ctrl)$ & diffusion coefficient of the state process
    \\
    $f(t, x, \alpha)$ & instantaneous cost function
    \\
    $g(x)$ & terminal cost function
    \\
    $J(\alpha)$ & total cost function associated with the control $\alpha$
    \\
    $\check J(\theta)$ & total cost function associated with the discretized stochastic control problem 
    \\
    & with neural network parameters $\theta$
    \\
    $(\overline X_t)_{t \in [0,T]}$ & empirical average of $N$-players' states
    \\     
    $\underline X_t= \bigl(\underline X_t(s)\bigr)_{s \in [-\delta, 0]}$ & trajectory of $X_t$ from time $t-\delta$ to $t$ %
    \\
    $T$ & time horizon
    \\
     $\Hess_x u(t, x)$ & Hessian matrix of $u$ with respect to $x$ \\
    $H$ & Hamiltonian function
    \\
    $u(t, x)$ & value function
    \\
    \hline
    \hline
    \multicolumn{2}{l}{Stochastic games related notations}
    \\ 
    \hline
    $X^i_t$ & state process for player $i$\\
    $\alpha^i_t$ & action function for player $i$\\
    $\bm{\alpha} = [\alpha^1, \alpha^2, \ldots, \alpha^N]$ & a collection of all players' strategy profiles \\
    $\bm{\alpha}^{-i} = [\alpha^1, \ldots, \alpha^{i-1}, \alpha^{i+1}, \ldots, \alpha^{N}]$ & the strategy profiles excluding player $i$'s \\
    $\bm{W} = [W^0, W^1, \ldots, W^N]$ & $(N+1)$-vector of $m$-dimensional independent Brownian motions \\
    $\mathbb{F} = \{\MCF_t, 0\leq t \leq T\}$ & the augmented filtration generated by $\bm{W}$ \\
    $J^i(\boldsymbol{\alpha})$ & cost function for player $i$ \\
    $\mathrm{k}$ & stage index in deep fictitious play \\
    $m$ & mean process (or conditional mean if there is common noise)
    \\
    $\mu$ & state distribution process
    \\
    $\mu^N$ & empirical state distribution process
    \\
    $\nu$ & state-action distribution process
    \\
    $\nu^N$ & empirical state-action distribution process

\end{longtable}

