
Stochastic optimal control and games have been extensively studied throughout the twentieth century and have found a wide range of applications in various areas such as finance, social sciences, operations research, and epidemic management problems, among others.
In recent years, computational methods for stochastic control and games have made great progress with the help of machine learning tools. A striking example of recent breakthroughs in applied mathematics using such tools is the numerical resolution of general nonlinear parabolic partial differential equations and backward stochastic differential equations in high dimensions~\cite{hutzenthaler2019multilevel, MR3736669,HaJeE:18,SiSp:18}. In short, stochastic control problems study how an agent optimally controls a stochastic dynamical system. The agent perceives some observations of the system's state and can decide to influence the evolution of the state based on these observations. The goal is to optimize an objective function that typically incorporates the cost of controlling the system and the reward for reaching some state. One of the most popular methods to solve such problems is dynamic programming, developed by Richard Bellman in the 1950s~\cite{bellman1957markovian}. However, this method suffers from what Bellman called the \emph{curse of dimensionality}, meaning that its complexity increases drastically with the number of possible states. This is a significant issue for systems that  evolve in continuous and high-dimensional spaces, since they can not be approximated by a small number of states. In such cases, using exact dynamic programming becomes computationally infeasible. 
Additionally, complexity can also arise from the structure of the system's evolution.
For example, in some cases, the system's evolution or its observation may be subject to delay, which appears in many real-world applications, {\it e.g.}, in economics, mechanics, or biology.  
To model the delay feature, the dynamics of the controlled system will depend not only on the current state but also on the history prior to the current time. This makes the problem path-dependent and, therefore, infinite-dimensional.

On the other hand, stochastic differential game theory, which was initiated by \cite{Isaacs1965}, combines theory and optimal control, and provides a framework for modeling and analyzing the behavior of strategic agents in the context of a dynamical system. The theory has been extensively employed in many disciplines, including management science,  economics, social science, and biology. One of the core objectives in differential games is to compute Nash equilibria, \textit{i.e.}, strategy profiles according to which no player has an incentive to deviate unilaterally~\cite{nash1951noncoop}. However, computing Nash equilibria in $N$-agent games is a notoriously hard problem, and direct computation of Nash equilibria is extremely demanding in terms of time and memory \cite{DaGoPa:2009} even for moderately large $N$. The mean-field game paradigm has been introduced independently by Lasry and Lions in \cite{LaLi:2007} and by Huang, Malham\'{e} and Caines in \cite{HuMaCa:06} to provide a tractable approximation for games with very large populations. A mean-field game is a game with a continuum of infinitesimal agents, where any single agent does not influence the rest of the population, which forms the mean field with which each agent interacts. This framework provides an efficient way to compute approximate Nash equilibria for symmetric $N$-agent games when $N$ is large. However, challenges remain in terms of computational complexity for games with high dimensional or complex environments, or when common noise affects the population dynamics. Furthermore, in the intermediate regime when $N$ is moderately large, the mean-field theory does not provide a good approximation of the $N$-player game. In such cases, one may still face a high-dimensional problem. To make these challenges more concrete, we discuss some illustrative examples.



\subsection{Some high-dimensional examples in applications}
We first discuss an example in financial markets. Many problems in economics or finance involve multiple interacting agents. For instance, we may consider a group of traders who buy and sell stocks in a financial market such as the S\&P 500, a free-float weighted measurement stock market index of 500 of the largest companies listed on stock exchanges in the United States. Each trader's portfolio describes the investment in stocks available on the market. To describe the investments of the whole group of traders, we need to incorporate all the traders' portfolios, and hence this description can be very high dimensional. However, if the traders have similar risk preferences, it is sufficient to study how one representative trader optimizes their payoff to understand the whole group's behavior. Any single agent has only a negligible impact on the stocks' prices. However, the impact of the group might be significant because if everyone wants to buy or sell the same stock, then the price will probably be shifted up or down. The portfolio optimization problem then becomes to find a Nash equilibrium. Each agent can anticipate that every other agent will behave like themselves and can thus predict the impact of the group on the stocks' prices. The mean-field game paradigm provides a rigorous framework in which each agent, taken individually, has no impact at all on the group's dynamics, and the problem is to find a fixed point at the population level. This simplifies the analysis. A first approach to describe the solution is through a forward-backward system of partial differential equations (PDEs) composed of a Fokker-Plank (FP) equation for the population distribution and a Hamilton-Jacobi-Bellman (HJB) equation for the value function of an infinitesimal player. However, several difficulties arise. Firstly, even solving the optimal control problem for the representative agent can be challenging: Given the number of stocks, the problem is in high dimension, and using an exact dynamic programming algorithm can be computationally too expensive. Secondly, even if the agent's portfolio state is in a low dimension, the optimal strategy may depend on the average state or control of the group. If the agents are trading the same stocks, then their trading strategies are all subject to the same source of randomness, which implies that the group's average strategy itself is stochastic. In the context of mean-field games, this is formalized through the notion of common noise. In such cases, the PDEs of the forward-backward system characterizing the solutions are no longer deterministic but stochastic {\cite{peng1992stochastichjb,carmonadelarue2014mastereqlarge}, making the system considerably harder to solve. Another approach \cite[Chapter~2]{CaDe2:17} characterizes the solution by a forward-backward system of stochastic differential equations (FBSDEs) which involves the conditional distribution of the forward and the backward processes, given the common noise. A third approach consists of describing the solution by a PDE on the space of probability distributions \cite{cardaliaguetdelaruelasrylions2019master}, but here again, this PDE is difficult to solve since it is posed on a high or infinite-dimensional space. In any case, in this application to optimal trading and as in many others, realistic models lead to the need to approximate functions of high or infinite-dimensional inputs. Altogether, this makes it crucial to develop efficient and accurate deep learning algorithms and theories for computing optimal controls and Nash equilibria in high dimensions.



A second example arises in infectious disease control of multiple regions. In a classic compartmental epidemiological model, each individual in a geographical region is assigned a label, for instance, \textbf{S}usceptible, \textbf{E}xposed, \textbf{I}nfectious, \textbf{R}emoved, \textbf{V}accinated. The transmission of a virus, being infected or recovered, moves individuals from one compartment to another, and this transition is usually described by stochastic dynamical equations. When a disease outbreak is reported, the region planner needs to take measurements to control its spread. The ongoing COVID-19 includes issuing lockdown or work from home policies, developing vaccines and later expanding equitable vaccine distribution, providing telehealth programs, distributing free testing kits, educating the public on how the virus transits, and focusing on surface disinfection. As a region planner, the decisions are usually made by weighting different costs, including the economic loss due to less productivity during lockdown policy or work from home policy, the economic value of life due to the death of infected individuals, and various social-welfare costs due to measurements mentioned above, and many more. Moreover, as the world is more interconnected than ever before, one region's decision will inevitably influence its neighboring regions. For instance, in the US, the decision made by the New York governor will affect the situation in New Jersey as so many people travel daily between the two states. Imagining that both state governors make decisions representing their own benefits but also take into account others' rational decisions, and they may even compete for scarce resources ({\it e.g.}, frontline workers and personal protective equipment), these are precisely the features of a non-cooperative game. A Nash equilibrium computed from such a game will definitely provide some qualitative guidance and insights for policymakers on the impact of certain policies. However, even with only three states (New York, New Jersey, and Pennsylvania) and a simple stochastic SEIR model as in \cite{xuan2020optimal,xuan2020ams}, this problem's state space is already twelve dimensions. Figure~\ref{fig:pandemic} below showcases the equilibrium lockdown policy corresponding to the multi-region SEIR model solved by a deep learning algorithm proposed in \cite{HaHu:19} (see Section~\ref{sec:MarkovianNE}) between the three states. The model parameters are estimated from real data posted by the Centers for Disease Control and Prevention (CDC). In general, the problem dimension is proportional to the number of compartments in the epidemiological model multiplied by the number of regions considered. For the most basic SIR model, the dimension of the problem for US governors will be $3 \times 50 = 150$. 


\begin{figure}[!htb]
    \centering
    \includegraphics[width = 0.75\textwidth, trim = {2em 2em 5em 6em}, clip, keepaspectratio=True]{figure/thetap99_a100.png}
    \caption{A case study of the COVID-19 pandemic in three states: New York (NY), New Jersey (NJ), and Pennsylvania (PA) in \cite{xuan2020optimal}. Plots of optimal policies (top-left), Susceptibles (top-right), Exposed (bottom-left), and Infectious (bottom-right) for three states: New York (blue), New Jersey (orange), and Pennsylvania (green). Large $\ell$ indicates high intensity of lockdown policy. Choices of parameters are referred to \cite[Section 4.2]{xuan2020optimal}. %
    }
    \label{fig:pandemic}
\end{figure}



\subsection{An illustrative linear quadratic model}
\label{sec:intro-LQsysrisk}
To illustrate numerical methods and show that they can correctly compute the problem's solution, it is convenient to have examples with analytical or semi-explicit solutions. We present here an example introduced in \cite{CaFoSu:15} to model the interactions in a system of banks. This model and other similar models with linear-quadratic structures admit a closed-form solution and have found applications in various fields.

We consider a stochastic differential game with $N$ players, and we denote by $\mathcal{I} =  \{1, 2, \ldots, N\}$ the set of players. Each player is interpreted as a bank and the state is its log-reserve.  Let $T$ be a finite time horizon. At each time $t \in [0,T]$,  player $i \in \mathcal{I}$ has a state $X_t^i \in \RR$ and takes an action $\alpha_t^i \in \RR$. The information structure will be discussed later and for now, we proceed informally but we can think of $\alpha^i_t$ as a stochastic process adapted to a filtration that represents the information available to player $i$. The dynamics of the controlled state process on $[0,T]$ are given by
$$
    \ud X_t^i = [a(\overline{X}_t - X_t^i)   + \alpha_t^i ] \ud t  + \sigma \left(\rho \ud W_t^0 + \sqrt{1-\rho^2} \ud W_t^i\right), \quad X_0^i \sim \mu_0, \quad i \in \mc{I}, \quad \overline{X}_t = \frac{1}{N}\sum_{i=1}^N X_t^i,
$$
where $\mu_0$ is a given initial distribution and $\bm{W} =[W^0, W^1,\ldots, W^N]$ are $(N+1)$ $m$-dimensional independent Brownian motions. %
We shall call $W^i$ the idiosyncratic (i.e., individual) noises and $W^0$ the common noise. The parameter $\rho \in[0,1]$ characterizes the noise correlation between agents. Here $a(\overline{X}_t - X_t^i)$ represents the rate at which bank $i$ borrows from or lends to other banks in the lending market, while $\alpha_t^i$  denotes its control rate of cash flows to a central bank. %
Furthermore, $\overline{X}_t = \frac{1}{N} \sum_{i=1}^N X^i_t$ denotes the average state. The $N$ dynamics are thus coupled since all the states $\bm{X}_t = [X_t^1, \ldots, X_t^N]$ affect the drift of every agent. Given a set of strategies $(\bm{\alpha}_t)_{t \in [0,T]} = ([\alpha_t^1, \ldots, \alpha_t^N])_{t \in [0,T]}$, the cost incurred to player $i$ is
\begin{equation}%
    J^i(\bm{\alpha}) = \EE\left[\int_0^T f^i(t, \bm X_t, \bm\alpha_t) \ud t + g^i(\bm X_T)\right],
\end{equation}
where the running cost $f^i: [0,T] \times \RR^{N}\times \RR^N \to \RR$ and the terminal cost $g^i: \RR^{N} \to \RR$ are given by
\begin{equation}
    f^i(t, \bm{x},\balpha) = \half (\alpha^i)^2 - q \alpha^i(\overline{x} - x^i) + \frac{\eps}{2}(\overline{x} - x^i)^2,  \quad g^i(\bm{x}) = \frac{c}{2}(\overline{x} - x^i)^2, \quad \overline{x} = \frac{1}{N} \sum_{i=1}^N x^i. 
\end{equation}
where $\bm{x} = [x^1, \ldots, x^N]$ and $\bm{\alpha} = [\alpha^1, \ldots, \alpha^N]$. 
All the parameters are non-negative. Here $\half(\ctrl^i)^2$ denotes the quadratic cost of the control, and $-q\ctrl^i(\overline{x} - x^i)$ models the incentive to borrowing or lending: Bank $i$ will want to borrow if $X_t^i$ is smaller than $\overline{X}_t$ and lend if $X_t^i$ is larger than $\overline{X}_t$. The quadratic term $(\overline{x} - x^i)^2$ in $f^i$ and $g^i$ penalizes the deviation from the average, given the other players' states. Player $i$ chooses $(\alpha_t^i)_{t \in [0,T]}$ to minimize her cost $J^i(\balpha)$ within some set of admissible strategies. We assume $q\le \epsilon^2$ so that the Hamiltonian is jointly convex in state and control variables, ensuring that there is at most one best response and then at most one Nash equilibrium. In the original work \cite[Section~3.1]{CaFoSu:15}, open-loop and closed-loop equilibria are characterized by semi-explicit formulas using ordinary differential equations.

As the number of agents $N$ grows to infinity, the idiosyncratic noises have a smaller and smaller influence on $\overline{X}$, which, in the limit, depends only on the common noise $W^0$. This is formalized in the following mean-field game (MFG). Let $(W_t)_{0 \leq t \leq T}$ and $(W_t^0)_{0 \leq t \leq T}$  be independent $m$-dimensional Brownian motions. We shall refer to $W$ as the {idiosyncratic noise} of the representative player and to $W^0$ as the {common noise} of the system. We consider the stochastic control problem
\begin{equation*} \inf_{\alpha} \EE\biggl\{\int_0^T \left[
    \frac{\alpha_t^2}{2}-q\alpha_t(m_t-X_t)+\frac{\epsilon}{2}(m_t-X_t)^2
    \right]\ud t +\frac{c}{2}(m_T-X_T)^2 \biggr\}, \end{equation*}
 \begin{equation}\label{def:LQ_SDE} \text{where }\quad\displaystyle \ud X_t = [a(m_t-X_t)+\alpha_t]\ud t  + \sigma(\rho \ud W_t^0 + \sqrt{1-\rho^2}\ud W_t), \quad X_0 \sim \mu_0,\end{equation} and the representative agent controls her state $X$ through a control process $\alpha$. Here $m_t = \EE[X_t|\mcF^{W^0}_t]$ is the conditional population mean given the common noise. As in the $N$-player case, one advantage of LQ models lies in the existence of an analytical solution for the mean-field equilibrium, which can provide a benchmark to test numerical algorithms. In this model, at equilibrium, we have
\begin{align}
    & m_t = \EE[X_0] + \rho\sigma W_t^0, \quad t\in[0,T], \label{def:LQ_m} \\
    & \alpha_t = (q+\eta_t) (m_t -X_t), 
    \quad t\in[0,T], \label{def:LQ_alpha}
\end{align}
where $\eta$ is a deterministic function of time solving the Riccati equation,
\begin{equation*}
    \dot \eta_t = 2(a+q)\eta_t + \eta_t^2 - (\eps - q^2), \quad \eta_T = c.
\end{equation*}
The solution is given by
\begin{equation*}
    \eta_t = \frac{-(\epsilon-q^2)(e^{(\delta^+-\delta^-)(T-t)}-1) -c(\delta^+e^{(\delta^+-\delta^-)(T-t)}-\delta^-)}{(\delta^-e^{(\delta^+-\delta^-)(T-t)} - \delta^+) -c(e^{(\delta^+-\delta^-)(T-t)} -1)},
\end{equation*}
where $\delta^\pm = -(a+q)\pm \sqrt{R}$, $R = (a+q)^2 + (\epsilon-q^2)>0$. At equilibrium, \textit{i.e.}, when all the payers use the equilibrium control, and the minimal expected cost for a representative player is 
\begin{equation*}
u(0, x_0 - \EE[x_0]),\quad\text{with}\quad    u(t,x) = \frac{\eta_t}{2}x^2 + \half \sigma^2(1-\rho^2)\int_t^T \eta_s \ud s.
\end{equation*}


Even though the state is in dimension one only, the presence of the common noise means that the optimal control is a function of the common noise. In this example, the equilibrium control actually depends on the common noise only through the first conditional moment of the distribution. In this case, the equilibrium can be found using neural networks which take as inputs not only the individual player's state but also an estimate of this first conditional moment. This idea can be extended to scenarios where the dependence on the common noise occurs only through a finite dimensional vector of information; see~\cite[Test cases 5 and 6]{carmona2019convergence2} for more details. However, this approach requires estimating aggregate quantities, for example by simulating a finite but large population of particles for many realizations of the common noise. When the interactions are through moments, another approach is to use only one realization of the idiosyncratic noise for each realization of the common noise: 
In \cite{MinHu:21}, based on the rough path theory, a single-loop algorithm called signatured deep fictitious play has been proposed. The proposed algorithm can accurately capture the effect of common uncertainty changes on mean-field equilibria without further training of neural networks, as previously needed in the existing machine learning algorithms. We will provide more details on this method in Section~\ref{sec4_MFG_with_CN} below. Figure~\ref{fig:LQ} showcases the performance for this LQ MFG with common noise, where the benchmark trajectories are simulated according to \eqref{def:LQ_SDE}  with $m_t$ and $\alpha_t$ in \eqref{def:LQ_m} and \eqref{def:LQ_alpha}. 

\begin{figure}[!htb]
    \centering
    \subfloat[$X_t$]{
         \includegraphics[width=0.3\columnwidth]{figure/SDE.pdf}}
    \subfloat[$m_t = \EE(X_t \vert \mcF_t^{W^0})$]{
         \includegraphics[width=0.3\columnwidth]{figure/mt.pdf}}
    \subfloat[Minimized Cost]{
         \includegraphics[width=0.3\columnwidth]{figure/valid_cost.pdf}}
    \caption{The illustrative linear quadratic model in Section~\ref{sec:intro-LQsysrisk}. Panels (a) and (b) give three trajectories of $X_t$, $m_t = \EE[X_t \vert \mcF_t^{W^0}]$ (solid lines) and their approximations $\widehat X_t$ (dashed lines) using different realizations of $(X_0, W, W^0)$ from validation data. Panel (c) shows the minimized cost computed using validation data over fictitious play iterations. 
    Parameter choices are given in \cite[Section 5]{MinHu:21}. %
    }
    \label{fig:LQ}
\end{figure}


\subsection{Organization of the survey}
In the rest of this survey, we review recent developments in machine learning methods and theory for stochastic control and differential games. We shall also identify unsolved challenges, and make connections to real applications. The topics are organized based on the number of players involved in the problem, first in the model-based setting, and then in the model-free setting.
We first review deep learning algorithms for stochastic control problems in Section~\ref{sec:SCP}, including mean field control problems, viewed as a type of control problem where the individual stateâ€™s dynamics are influenced by its own law. In Section~\ref{sec:SDG}, we focus on deep learning for stochastic differential games, including (moderately large) $N$-player games and mean-field games. In Section~\ref{sec:RL}, we review the basic principles underpinning model-free reinforcement learning methods for stochastic control and games. We make conclusive remarks and discuss unsolved challenges in Section~\ref{sec:conclusion}. In the appendix, we provide more background on two main tools of modern machine learning, namely, neural network architectures and stochastic gradient descent. We also summarize all the acronyms and the notations frequently used in this paper.

\medskip
\noindent{\bf The scope of the survey. }
This paper focuses on the recently developed neural network-based algorithms aiming at high-dimensional problems. The main reason why we focus on the stochastic setting is that in the deterministic setting, the problems can usually be tackled using open loop controls by reducing the problem to a two-point boundary value problem \cite{kierzenka2001bvp,zang2022machine}, which can be solved in high dimension without machine learning. 

Besides the methods reviewed in this paper, there is a rich literature on methods, some of them related to machine learning but without neural networks. For example, to cite just a few examples, Markov chain based methods \cite{budhiraja2007convergent}, regression based methods \cite{barrera2006numerical,bouchard2004discrete}, and approaches based on PDEs and BSDEs \cite{chen2008semi,forsyth2007numerical}; see, {\it e.g.}, %
the survey papers \cite{kushner1990numerical,jin2022survey} and the references therein. Furthermore, we focus mostly on standard classes of stochastic control problems and games but many other problems are considered in the literature. For instance, we do not discuss in this paper optimal switching and optimal stopping problems, for which numerical algorithms have been extensively developed, \emph{e.g.}, in \cite{kohler2010pricing,HuLudkovski17,becker2019deep,becker2020pricing,hu2020deep,hurephamwarin2020deep,lapeyre2021neural,reppen2022deep,reppen2022neural,gao2022convergence,bayraktar2022deep}. 