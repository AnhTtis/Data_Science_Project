\section{Pseudo-codes of Algorithms}\label{supp:pseudocode}


\begin{algorithm}[h t p]
		\caption{Deep Fictitious Play for Finding Open-loop Nash Equilibrium \label{algo:DFP-open}}
	\begin{algorithmic}[1]
		\REQUIRE $N$ = \# of players, $N_T$ = \# of subintervals on $[0,T]$, $M$ = \# of training paths, $M'$ = \# of out-of-sample paths for final evaluation, $\bm{\alpha}^0 = \{\alpha_{t)n}^{i,0} \in \mc{A} \subset \RR^k, i \in \mc{I}\}_{n=0}^{N_T-1}$ = initial belief, $\bm{X}_0 = \{x_0^i \in \RR^d, i \in \mc{I}\}$ = initial states
		  \STATE Create $N$ separated deep neural networks as described in Eq.~\eqref{def:NN-OL} 
		  \STATE  Generate $M$ discrete sample path of BM: $\bm{W} = \{W_{t_n}^{i} \in \RR^m, i \in \mc{I}\cup \{0\} \}_{n=1}^{N_T} $
		  \STATE $\mt{k} \gets 0$
		  \REPEAT
		  \FORALLP{$i \in \mc{I}$} 
		  \STATE $\mt{k} \gets \mt{k}+1$
		  \STATE (Continue to) Train $i^{th}$ NN with data $\{\bm{X}_0, \bm\alpha^{-i,\mt{k}-1} = \{\alpha_{t_n}^{j,\mt{k}-1}, j \in \mc{I}\setminus \{i\}\}_{n=0}^{N_T-1}, \bm W\}$
		  \STATE Obtain the approximated optimal strategy  $\alpha^{i,\mt{k}}$ and cost $J^i(\alpha^{i,\mt{k}}; \bm\alpha^{-i,\mt{k}-1})$ 
		  \ENDFOR
		  \STATE Collect optimal policies at stage $\mt{k}$: $\bm{\alpha}^\mt{k} \gets (\alpha^{1,\mt{k}}, \ldots, \alpha^{N,\mt{k}})$
		  \STATE Compute relative change of cost $\displaystyle err^\mt{k} = \max_{i \in \mc{I}}\left\{\frac{\abs{J^i(\alpha^{i,\mt{k}}; \bm\alpha^{-i,\mt{k}-1}) - J^i(\alpha^{i,\mt{k}-1}; \bm{\alpha}^{-i,\mt{k}-2})}}{ J^i(\alpha^{i,\mt{k}-1}; \bm{\alpha}^{-i,\mt{k}-2})}\right\}$

		  \UNTIL$err^\mt{k}$ go below a threshold
		  
		\STATE Generate $M'$ out-of-sample paths of BM for final evaluation
		\STATE $\mt{k}' \gets 0$
		\REPEAT 
		\STATE $\mt{k}' \gets \mt{k}'+1$
		\STATE Evaluate $i^{th}$ NN with  \{$\bm X_0$, $\bm{\alpha}^{-i,\mt{k}'-1}$, out-of-sample paths\}, $\forall i \in \mc{I}$
		\STATE Obtain $\alpha^{i,\mt{k}'}$ and $J^{i,\mt{k}'} = J^i(\alpha^{i,\mt{k}'};\bm{\alpha}^{-i, \mt{k}'-1})$ $\forall i \in \mc{I}$
		\UNTIL $J^{i,\mt{k}'}$ converges in $\mt{k}'$, $\forall i \in \mc{I}$
		\RETURN The optimal policy $\alpha^{i,\mt{k}'}$, and the final cost for each player $J^{i,\mt{k}'}$
	\end{algorithmic}
\end{algorithm}


\begin{algorithm}[h t p]
\caption{Deep Fictitious Play for Finding Markovian Nash Equilibrium \label{def_algorithm1}}
    \begin{algorithmic}[1]
	\REQUIRE $N$ = \# of players, $N_T$ = \# of subintervals on $[0,T]$, $\mt{K}$ = \# of total stages in fictitious play, $N_{\text{sample}}$ = \# of sample paths generated for each player at each stage of fictitious play, $N_{\text{SGD\_per\_stage}}$ = \# of SGD steps for each player at each stage, $N_{\text{batch}}$ = batch size per SGD update, $\balpha^0\colon$ the initial policies that are smooth enough
	    \STATE Initialize $N$ deep neural networks to represent $u^{i,0}, i \in \mc{I}$
		\FOR{$\mt{k} \gets 1$ to $\mt{K}$}
		\FORALLP{$i \in \mc{I}$}
		\STATE  Generate $N_\text{sample}$ sample paths $\{\check \bX_{t_n}^{i}\}_{n=0}^{N_T}$ according to \eqref{eq:disc_X_path} and the realized optimal policies $\balpha^{-i, \mt{k}-1}(t_n, \check {\bm X}_{t_n}^{i})$
		\FOR{$\ell \gets 1$ to $N_{\text{SGD}\_\text{per}\_\text{stage}}$}
		  %
		    \STATE Update the parameters of the $i^{th}$ neural network one step with $N_{\text{batch}}$ paths using the SGD algorithm (or its variant), based on the loss function \eqref{eq:disc_objective}
		  \ENDFOR
		  \STATE Obtain the approximate optimal policy  $\alpha^{i,\mt{k}}$  according to \eqref{def_alphaast}
		  \ENDFOR
		  \STATE Collect the optimal policies at stage $\mt{k}$: $\bm{\alpha}^\mt{k} \gets (\alpha^{1,\mt{k}}, \ldots, \alpha^{N,\mt{k}})$
		  \ENDFOR
		\RETURN The optimal policy $\balpha^{\mt{K}}$
	\end{algorithmic}
\end{algorithm}



\begin{algorithm}[h t p]
   \caption{The Sig-DFP Algorithm}
   \label{alg:sig-dfp}
\begin{algorithmic}
   \STATE {\bfseries Input:} $b, \sigma, \sigma_0, f, g, \iota$ and $X_0(\omega_i), \{W_{t_n}(\omega_i)\}_{n=0}^{N_T}, \{W^0_{t_n}(\omega_i)\}_{n=0}^{N_T}$ for $i=1,2,\dots, N$; $\mt{K}$: rounds for FP;    $B$: minibatch size; $N_{\text{batch}}$: number of minibatches. 
   \STATE Compute the signatures of $\hat{W}^0_{t_n}(\omega_i)$ for $i=1, \dots, N$, $n=1,\dots, N_T$;
   \STATE Initialize $\hat{\nu}^{(0)}$, $\theta$;
   \FOR{$\mt{k}=1$ {\bfseries to} $\mt{K}$}
   \FOR{$r=1$ {\bfseries to} $N_{\text{batch}}$}
   \STATE Simulate the $r^{th}$ minibatch of $X^{(\mt{k})}(\omega_i)$ using $\hat{\nu}^{(\mt{k}-1)}$ and compute $J_B(\theta, \hat{\nu}^{(\mt{k}-1)})$;
   \STATE Minimize $J_B(\theta, \hat{\nu}^{(\mt{k}-1)})$ over $\theta$, then update $\alpha(\cdot; \theta)$;
   \ENDFOR
   \STATE Simulate $X^{(\mt{k})}(\omega_i)$ with the optimized $\alpha(\cdot; \theta^\ast)$, 
   for $i=1, \dots, N$;
   \STATE Regress $\iota(X^{(\mt{k})}_0(\omega_i), \alpha^{(\mt{k})}_0(\omega_i))$, $\iota(X^{(\mt{k})}_{T/2}(\omega_i), \alpha^{(\mt{k})}_{T/2}(\omega_i))$, $\iota(X^{(\mt{k})}_{T}(\omega_i), \alpha^{(\mt{k})}_T(\omega_i))$ on $S^M(\hat{W}^0_0{\omega_i})$, $S^M(\hat{W}^0_{t_{N_T/2}}(\omega_i))$, $S^M(\hat{W}^0_{T}(\omega_i))$ to get $\tilde l^{(\mt{k})}$; 
   \STATE Update $\hat{l}^{(\mt{k})} = \frac{\mt{k}-1}{\mt{k}}\hat{l}^{(\mt{k}-1)} + \frac{1}{\mt{k}}\tilde l^{(\mt{k})}$;
   \STATE Compute $\hat{\nu}^{(\mt{k})}$ by $\hat{\nu}^{(\mt{k})}_{t_n}(\omega^i) = \langle \hat{l}^{(\mt{k})}, S^M(\hat{W}^0_{t_n}(\omega_i)) \rangle$, for $i=1,2,\dots, N, n=1,\dots, N_T$;
   \ENDFOR
   \STATE {\bfseries Output:} the optimized $\alpha_\varphi^\ast$ and  $\hat {l}^{(\mt{K})}$.
\end{algorithmic}
\end{algorithm}