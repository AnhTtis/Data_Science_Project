We now discuss how direct parameterization methods can be adapted for mean field control (MFC) problems, which are an extension of standard optimal control in which there are mean field interactions. MFC can be interpreted as a social optimum but also has applications in risk, {\it e.g.}, heating or electric loads management~\cite{KIZILKALE20141867,Mathieu2013StateEA},  risk management in finance~\cite{MR2784835} or optimal control with a cost involving a conditional expectation~\cite{achdoulaurierelions2020optimal,MR4133380}, to cite just a few examples. We refer to~\cite{MR3134900} for more background on MFC.


As before, let $\mc{A} \subset \RR^k$ denote the set of admissible actions and let $\mathbb{A}$ be the set of so-called admissible controls. We denote by $\cP_2(\RR^d)$ the set of probability measures on $\RR^d$ which admit a second moment. Let $b, \sigma, f, g$ be Borel-measurable functions,
\begin{equation}
    (b, \sigma, f) : [0,T] \times \RR^d \times \cP_2(\RR^d) \times \mc{A} \to (\RR^d, \RR^{d \times m}, \RR), \quad g : \cP_2(\RR^d) \times \RR^d \to \RR.
\end{equation}
Compared with standard optimal control, see~\eqref{def:oc-bsigmaf}, the functions take as an extra input the population distribution.


\begin{defn}[MFC optimum]

A feedback control $\ctrl^* \in \mathbb{A}$ is an optimal control for the MFC problem for a given initial distribution $\mu_0 \in \cP_2(\RR^d)$ if it minimizes
\begin{align}
\label{subchapRCML-num-eq:def-J-MFC}
	J(\alpha): \ctrl \mapsto \EE \left[\int_0^T f(t, X_t, \mu_t, \ctrl_t ) dt + g(X_T, \mu_T) \right],
\end{align}
where $\mu_t$ is the probability distribution of the law of $X_t$, under the constraint that the process $X = (X_t)_{t \ge 0}$ solves the stochastic differential equation of the McKean-Vlasov (MKV) type,
\begin{equation}
\label{subchapRCML-num-eq:dyn-X-general-MFC}
	\ud X_t = b(t, X_t, \mu_t, \ctrl_t) \ud t + \sigma(t, X_t, \mu_t, \ctrl_t) \ud W_t, \qquad t \ge 0,
\end{equation}
with $X_0$ having distribution $\mu_0$. 
\end{defn}
Existence and uniqueness results have been studied in the literature and we refer the interested reader to {\it e.g.}~\cite{carmona2018probabilistic2} and in particular the assumption  ``Control of MKV Dynamics'' on page 555. Furthermore, under mild conditions there is an optimal control that is Markovian, so it is sufficient to focus on controls that are functions of time and the individual state. The framework can also encompass problems in which the interactions are through the joint distribution of state and actions and not just the distribution of states. Such problems are sometimes referred to as extended MFC problems. Although the numerical method discussed below can be adapted in a rather straightforward way (as illustrated below in an example below), the theoretical framework is more challenging. 



To solve the MFC problem using deep learning, one can for example follow the lines of the global approach described in Section~\ref{sec:direct-global-local} to discretize time and to replace the control by a sequence of neural networks functions of the state. However, in contrast with standard OC problems, here, the costs and the dynamics can depend on mean field terms, so we also need to approximate the distribution of the state. To this end, The simplest approach is to use the empirical distribution of a population of particles. 

Following this approach, the original MFC problem is approximated by the following problem: minimize over the neural network parameters $\theta = (\theta_n)_{n=0,\dots,N_T-1}$
\begin{equation}
\label{subchapRCML-num-eq:NN-MFC-cost-totalapprox}
	\check J^{N}(\theta): \theta \mapsto \frac{1}{N} \sum_{i=1}^N \EE \left[\sum_{n=0}^{N_T-1} f(t_n, \check X_{t_n}^{i, \theta}, \check \mu^{N, \theta}_{t_n}, \ctrl_{t_n}(\check X_{t_n}^{i, \theta}; \theta_n) ) \Delta t + g(\check X_T^{i, \theta}, \check \mu^{N,\theta}_T) \right],
\end{equation}
subject to
\begin{equation}
\label{subchapRCML-num-eq:NN-MFC-Nparticles-Deltat}
	\check X_{t_{n+1}}^{i, \theta} = \check X_{t_{n}}^{i, \theta} + b(t_n,  \check X_{t_{n}}^{i, \theta}, \check \mu^{N, \theta}_{t_{n}}, \ctrl_{t_n}(\check X_{t_{n}}^{i, \theta}; \theta_n)) \Delta t + \sigma(t_n, \check X_{t_{n}}^{i, \theta}, \check \mu^{N, \theta}_{t_{n}}, \ctrl_{t_n}(\check X_{t_{n}}^{i, \theta}; \theta_n)) \Delta \check W^i_n, \quad n = 0, \dots, N_T-1,
\end{equation}
where the initial positions $(\check X_0^{i, \theta})_{i=1,\dots,N}$ are i.i.d. with distribution $\mu_0$, the empirical distribution is denoted by
$$
	\check \mu^{N, \theta}_{t_{n}} = \frac{1}{N} \sum_{j=1}^N \delta_{\check X_{t_{n}}^{j, \theta}}, 
$$ 
and $(\Delta \check W_n^i)_{i=1,\dots,N,\;n=0,\dots,N_T-1}$ denotes i.i.d. random variables with Gaussian distribution $\mathcal N(0, \Delta t)$. Note that this problem is not equivalent to solving an $N$-agent optimal control problem since, in the latter case, the control would in general need to be a function of all the agents' states while here we consider only distributed controls, functions of each agent's state. Intuitively, this approximation is justified by propagation of chaos results~\cite{sznitman1991topics}: as $N\to+\infty$ the particles become independent and the empirical distribution converges to the distribution of the MKV SDE~\eqref{subchapRCML-num-eq:dyn-X-general-MFC}. See~\cite{lacker2017limit} in the context of MFC problems. 

This technique was proposed in~\cite{carmona2019convergence2} (with a single neural network function of time and space instead of a sequence of neural networks functions of space online). Although we focus here on a basic setting for which a simple feedforward fully connected architecture performs well, other architectures may yield better results for problems with more complex time dependencies; see \textit{e.g.}, \cite{fouque2019deep,gomes2022machineprice} for applications with RNNs. See also~\cite{germain2019numerical,agrambakdioksendal2020deep} and the survey~\cite{carmonalauriere2021deepmfgsurvey} for more details. 



\begin{rem}[Theoretical analysis]
In \cite{carmona2019convergence2} Carmona and Lauri\`{e}re provided a proof of convergence of the discrete problem to the original MFC problem in the following sense: under suitable assumptions, the difference between the optimal value of this problem $\inf_\theta \check J^{N}(\theta)$ and the optimal value of the original problem $\inf_\ctrl J(\ctrl)$ goes to $0$ as $N_T$, $N$ and the number of parameters in the neural network go to infinity. 
\end{rem}

From here, one can use SGD or one of its variants to optimize $\check J^N$ over the parameters. In contrast with the methods discussed previously for standard OC, here one sample corresponds to one population with $N$ particles. In this context, one sample corresponds to $\xi = ( \check X^{i}_0, (\Delta \check W^i_n)_{n=0,\dots,N_T-1})_{i=1,\dots,N}$, from which we can compute the $N$ (interacting) trajectories and the total empirical cost:
\begin{equation}
\label{subchapRCML-num-eq:NN-MFC-cost-totalapprox-oneS}
	\check J^{\xi, N}(\theta) = \frac{1}{N} \sum_{i=1}^N \left[\sum_{n=0}^{N_T-1} f(\check X_{t_n}^{i, \theta, \xi}, \check \mu^{N, \theta, \xi}_{t_n}, \ctrl_{t_n}(\check X_{t_n}^{i, \theta, \xi}; \theta_n) ) \Delta t + g(\check X_T^{i, \theta, \xi}, \check \mu^{N, \theta, \xi}_T) \right].
\end{equation}
From here, Algorithm~\ref{algo:SGD-generic} in Appendix~\ref{sec:SGD-var} can be applied.



 
\paragraph*{Numerical illustration: a mean-field price impact problem.} We now illustrate the method with a financial application on a problem of optimal execution. The model describes a alrge of group traders interacting through the price of an asset. The large group of traders has a non-negligible influence on the price, which is referred to price impact. For example when the traders decide to buy at the same moment, the price is driven up, and vice versa when the traders decide to sell simultaneously.

The $N$-agent problem was originally solved as a mean-field game (MFG) by Carmona and Lacker in the weak formulation (\cite{MR3325272}), and revisited in the book of Carmona and Delarue  \cite[Sections 1.3.2 and 4.7.1]{carmona2018probabilistic} in the strong formulation. Here, we focus on the mean-field setting. In this model, the inventory representative trader is denoted by $X_t$. The control of the trader is denoted by $\ctrl_t$ and corresponds to the trading rate. The dynamics of the inventory are given by
\begin{equation*}
    \ud X_t = \ctrl_t \ud t +\sigma \ud W_t,
\end{equation*}
where $W$ is a standard Brownian motion, and the cost can be rewritten in terms of $X$ only as
\begin{equation*}
J(\ctrl)=\mathbb{E}\Big[ \int_0^T \left(c_{\ctrl}(\ctrl_t)+c_X(X_t)-\gamma X_t \int_{\mathbb{R}} a  \ud \nu^\ctrl_t(a)\right)\ud t +g(X_T)\Big].  
\end{equation*}
Following the Almgren-Chriss linear price impact model, we assume that the functions $c_X$, $c_{\ctrl}$ and $g$ are quadratic. Thus, the cost is of the form
\begin{equation}\label{eq:priceimpact}
J(\alpha)=\mathbb{E}\left[ \int_0^T \left( \frac{c_{\alpha}}{2}{\alpha_t}^2+\frac{c_X}{2}X_t^2-\gamma X_t\int_{\mathbb{R}} a \ud\nu^\ctrl_t(a) \right)\ud t + \frac{c_g}{2}X_T^2\right].
\end{equation}
This model has a semi-explicit solution obtained by reducing the problem to a system of ordinary differential equations (ODEs) as explained in~\cite[Sections 1.3.2 and 4.7.1]{carmona2018probabilistic}. 



The deep learning method described above can readily be adapted to solve MFC with interactions through the control's distribution by computing the empirical distribution of controls for an interacting system of $N$ particles. Figure~\ref{fig:ex-price-impact-1} shows the control and the distribution at various time steps. Here we plot the values of the control represented by the neural network evaluated at samples generated by following the $N$-interacting particle system. We see that the shape is approximately linear at every time steps, and furthermore that it coincides with the lines corresponding to the theoretical optimal solution. The distribution, represented by histograms computed using the $N$ particles, starts on the right and moves towards $0$, which can be interpreted as the fact that the traders liquidate their portfolios. We used the parameters: $T=1$, $c_X = 2$, $c_{\ctrl} = 1$, $c_g = 0.3$, $\sigma = 0.5$ and $\gamma = 0.2$.  When using a larger value of $\gamma$, the collective influence of the traders on the price is higher. As can be seen 
in Figure~\ref{fig:ex-price-impact-2} obtained with $\gamma=1$, the traders do not liquidate all the time from $t=0$ until time $t=T$. Instead, they liquidate their portfolio at the beginning and then start buying. We can explain this behavior as follows: since this is a cooperative problem, the traders can use the price impact to drive the price up to increase the value of the stock they own, thus increasing their final reward. 

\begin{figure}[ht]
  \subfloat[]{
  \includegraphics[width=0.45\linewidth]{{figure/PRICEIMPACTDIRECT/test2i2_hbar0p2_control_t45}.pdf}
    }
  \subfloat[]{
  \includegraphics[width=0.45\linewidth]{{figure/PRICEIMPACTDIRECT/test2i2_hbar0p2_density_t50}.pdf}
    }
\caption{Price impact MFC example in Section~\ref{sec:directMethod} solved by direct method. Left: Control learnt (dots) and exact solution (lines). Right: associated empirical state distribution. Here we take $\gamma = 0.2$ in \eqref{eq:priceimpact}. %
}
\label{fig:ex-price-impact-1}
\end{figure}






\begin{figure}[ht]
  \subfloat[]{
  \includegraphics[width=0.45\linewidth]{{figure/PRICEIMPACTDIRECT/test2h2_hbar1_control_t45}.pdf}
    }
  \subfloat[]{
  \includegraphics[width=0.45\linewidth]{{figure/PRICEIMPACTDIRECT/test2h2_hbar1_density_t50}.pdf}
    }
\caption{Price impact MFC example in Section~\ref{sec:directMethod} solved by direct method. Left: Control learnt (dots) and exact solution (lines). Right: associated empirical state distribution. Here we take $\gamma = 1$ in \eqref{eq:priceimpact}. %
}
\label{fig:ex-price-impact-2}
\end{figure}

