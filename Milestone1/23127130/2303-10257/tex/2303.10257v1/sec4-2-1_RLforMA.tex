Multi-agent reinforcement learning (MARL) studies reinforcement learning methods for multiple learners. The main difficulty is that, when several agents learn while interacting, from the point of view of each agent, the environment is non-stationary. Another issue is the question of scalability, which arises when the number of learners is very large. However, for a small number of agents, MARL has led to recent breakthrough results; see {\it e.g.} in autonomous driving ~\cite{shalev2016safe}, the game of Go~\cite{silver2016mastering}, or video games such as Star Craft~\cite{vinyals2019grandmaster}. 

Several viewpoints can be adopted. Relying on dynamical systems theory, one approach is to consider that each agent uses a learning algorithm, and to study the resulting behavior of the group of agents viewed as a system evolving in discrete or continuous time. Another approach, based on game theory and closer to the topics discussed in Section~\ref{sec:SDG}, is to look for notions of solutions such as Nash equilibria and to design algorithms that let the agents learn such solutions. A typical example is Nash Q-learning, in which every player runs their own version of Q-learning simultaneously with the other players. Each player tries to compute its optimal Q-function, but the optimal policy of player $i$ depends on the policies implemented by the other players. To be specific, consider an $N$-player game as in Section~\ref{sec:Nplayer} but now in discrete time. Note that the problem faced by player $i$ is not an MDP with state $X^i$ because the cost and dynamics of player $i$ depend on the other players. Assume the players use a strategy profile $\bm\pi = (\pi^1,\dots,\pi^N)$. Then the Q-function of player $i$ is: for $\bm{x} = (x^1,\dots,x^N)$ and $\bm{a} = (a^1,\dots,a^N)$,
\begin{equation}
    Q^{i,\bm\pi}_{t_n}(\bm{x}, \bm{a}) = \EE^{\bm\pi}\left[\sum_{j=n}^{N_T-1} f^i(t_j, \bm{X}_{t_j}, \bm{\alpha}_{t_j})\Delta t + g^i(\bm{X}_T) \Big\vert \bm{X}_{t_n} = \bm{x}, \; \bm{\alpha}_{t_n} = \bm{a}\right].
\end{equation}
Hu and Wellman proposed in~\cite{hu2003Nash} a version of Q-learning for (infinite horizon discounted) $N$-player games, called Nash Q-learning, and identified conditions under which this algorithm converges to a Nash equilibrium. The method can be adapted with deep neural networks, as done for instance in~\cite{casgrain2019deepnashq}.  We refer the interested reader to, {\it e.g.}, \cite{Busoniu08,tuyls2012multiagent,bloembergen2015evolutionary,lanctot2017unified,yang2020overview,zhang2021multi,gronauer2021multi} for more details on MARL. Recently, \cite{gu2021mean,gu2021mean2} also studied mean-field control RL in a decentralized way using cooperative MARL.
