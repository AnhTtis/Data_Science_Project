



We now consider the MFC setting discussed in Section~\ref{sec:directMethod} as an extension of standard OC and we present an RL framework for this setting. MFC can be viewed as an optimal control problem in which a ``state'' is a population configuration. However an ``action'' is not a finite-dimensional object but rather a function providing a control for every individual state. Intuitively, in discrete time, this yields an MDP of the form $(\mc{P}(\mc{X}), \mc{F}_{\mc{A}}, \bar{p}, \bar {f}, \bar{g}, N_T)$, where \begin{itemize}
    \item The state space is the set $\mc{P}(\mc{X})$ of probability measures on $\mc{X}$; 
    \item The action space $\mc{F}_{\mc{A}}$ is a suitable subset of $\mc{A}^{\mc{X}}$, the set of functions from $\mc{X}$ to $\mc{A}$;
    \item The transition kernel is given by  
$$
    \bar{p}:   \{t_0,t_1,\dots,T\} \times \mc{P}(\mc{X}) \times \mc{F}_\mc{A} \to \mc{P}(\mc{P}(\mc{X})), 
    \quad \bar{p}(\cdot|t, \mu, \bar a) = \delta_{\int p(\cdot|t, x, \mu, \bar{a}(x)) \mu(x) dx} \,,
$$
    meaning that with probability one, the new mean field state is given by one transition of the population distribution. Here $\mu$ represents a population distribution, $\bar a$ is an action at the population level, and $\bar{p}(\cdot|t, \mu, \bar a)$ is the distribution of the next population distribution, which is a Dirac mass at the next population distribution since there is no common noise in the present model;
    
    \item The running and terminal cost functions are given by
$$
    \bar{f}: \{t_0,t_1,\dots,T\} \times \mc{P}(\mc{X}) \times \mc{F}_{\mc{A}} \to\RR, \qquad \bar{f}(t, \mu, \bar{a}) = \int_{x} f(t, x, \mu, \bar{a}(x)) \mu(x) \ud x \,,
$$
and
$$
    \bar{g}: \mc{P}(\mc{X}) \to\RR, \qquad \bar{g}(\mu) = \int_{x} g(x, \mu) \mu(x) \ud x.
$$

\end{itemize}
Such MDPs have been referred to as mean field MDPs (MFMDP for short) in the literature~\cite{gast2012mean,carmona2019modelfree,gu2019dynamicmfc,gu2021mean2,motte2019mean,bayraktar2022finite}. These MDPs can be rigorously studied using the tools developed for instance by Bertsekas and Shreve in~\cite{bertsekasshreve1996stochastic}. 
Since this problem fits in the framework of MDPs, one can directly apply RL methods in principle. For instance, the Q-function of the MDP naturally satisfies a dynamic programming principle; see~\cite{carmona2019modelfree,gu2019dynamicmfc,gu2021mean2,motte2019mean}. Note that, if there is no common noise (as in the setting presented above), the evolution of the population distribution is purely deterministic. 

To implement RL methods for MFC, the main difficulties are related to handling the distribution and the class of controls. In particular, we note that
\begin{itemize}
    \item If $\mc{X}$ is finite, then the state of the MDP, namely $\mu$, is a finite dimensional vector; if $\mc{A}$ is also finite, then $\mc{F}_{\mc{A}}$ can simply be taken as $\mc{A}^{\mc{X}}$, which is a finite set as well;
    \item If $\mc{X}$ is not finite, then $\mu$ is infinite-dimensional and likewise for the elements of $\mc{A}^{\mc{X}}$.
\end{itemize}

One simple approach is to discretize $\mc{P}(\mc{X})$ and $\mc{A}^{\mc{X}}$, and then use standard RL techniques for finite state, finite action MDPs, such as the ones described in Section~\ref{sec:MDP}. For instance tabular Q-learning has been used {\it e.g.} in~\cite{carmona2019modelfree,gu2021mean2} in the first case above by identifying $\mc{P}(\mc{X})$ with the simplex $\Delta_{\mc{X}}$ in dimension $|\mc{X}|$ and by approximating the latter with an $\epsilon$-net. However, this approach does not scale well when the number of states is large or when $\mc{X}$ is continuous. In this case, one can use RL methods for continuous state space, such as deep RL methods, see for instance~\cite{carmona2019modelfree}.

For the sake of illustration, we provide an example in a setting where $\mc{X}$ is finite. Let $d = |\mc{X}|$ be the number of states. As mentioned above, we view $\mc{P}(\mc{X})$ as the $d$-dimensional simplex $\Delta_{\mc{X}}$. In this case, the MFMDP is an MDP over a finite-dimensional continuous state space. To avoid discretizing the space, deep RL methods rely on neural networks to efficiently approximate the value function or the policy. 


\paragraph*{Numerical illustration: A Cybersecurity model revisited.} 
We consider the cybersecurity model introduced in~\cite{MR3575619} (see also~\cite[Section 7.2.3]{carmona2018probabilistic}) and that we already discussed in Section~\ref{sec:mastereq-deeplearning}. We revisit this problem from the point of view of MFC, meaning that the players cooperate to jointly minimize the social cost. 

To be able to tackle this problem using RL, we discrete time using a mesh $\{t_n = n \Delta t, n = 0,1,2,\dots, N_T\}$ where $\Delta t = T / N_T>0$. The total cost for the whole population is
\begin{align*}
	J(\ctrl) 
	=  \sum_{n=0}^{N_T-1} \bar f(\mu_{t_n}, \ctrl(t_n, \cdot)) \Delta t,
\end{align*}
under the constraint that the evolution of distribution is given by
\begin{equation}
    \label{eq:cyber-mfc-dyn-mu}
	\mu_{t_{n+1}} 
	= \bar p(\mu_{t_n}, \ctrl(t_n,\cdot))
	= (\mu_{t_n})\transpose (I + P^{\ctrl(t_n,\cdot), \mu_{t_n}} \Delta t), \qquad n = 0, 1, \dots, N_T-1,
\end{equation}
with a given initial condition $\mu_0$. The population-wise cost function 
$\bar f: \mc{P}(\mc{X}) \times \mc{A}^{\mc{X}} \to \RR$ is defined based on the individual cost function $f$ by
$$
	\bar f(m, \ctrl) = \sum_{x\in\mc{X}} f(x, m, \ctrl(x) ) m(x), \qquad (m, \ctrl) \in \mc{P}(\mc{X}) \times \mc{A}^\mc{X}, 
$$
and $P^{\ctrl, m}$ denotes the matrix whose coefficients are given by
$$
	P^{\ctrl, m}(x', x) = \lambda(x', x, m, \ctrl(x')), \qquad (x',x,m,\ctrl) \in \mc{X} \times \mc{X} \times \mc{P}(\mc{X}) \times \mc{A}^\mc{X}.
$$
From this formulation, we see that the problem fits in the framework of MFMDPs, or MDPs with finite horizon and continuous space, the state being the distribution.


In~\cite{AMSnotesLauriere}, the solution is learned using tabular Q-learning after discretizing the simplex: replacing $\mc{P}(\mc{X})$ by an $\epsilon$-net with a finite number of distributions allows one to replace the MFMDP by a finite-state MFMDP on which tabular RL methods can be applied. This approach is convenient in that tabular methods typically have fewer hyperparameters and furthermore convergence results are easier to obtain. However, the main drawback is that such methods do not scale well to very large state space. In our case, discretizing the simplex requires a large number of points when then number of states increases.

Alternatively, the value function can be approximated directly on the simplex $\mc{P}(\mc{S})$, without any discretization. For example, we can replace the Q-function by a neural network and employ deep RL techniques to train the parameters. Here we follow the approach proposed in~\cite{carmona2019modelfree} and we focus on deterministic controls. The control and the value function are approximated by neural networks and trained using the DDPG method~\cite{lillicrap2015continuous}, which has been reviewed in Section~\ref{sec:MDP-PGM}. Since this method allows the control to take continuous values, we replace $A=\{0,1\}$ by $A =[0,1]$ (without changing the transition rate matrix), which amounts to letting the player choose the intensity with which she seeks to change her computer's level of protection. 

We aim at learning the solution for various distributions. To train the neural networks, we sample at each iteration a random initial distribution $\mu_0$ and generate a trajectory in the simplex by following the dynamics~\eqref{eq:cyber-mfc-dyn-mu}. Figure~\ref{fig:cyber-ddpg-cn-testing} displays the evolution of the population when using the learned control starting from five initial distributions of the testing set and one initial distribution of the training set. The testing set of initial distributions is: $\{(0.25,0.25,0.25,0.25),$ $(1, 0, 0, 0),$ $(0, 0, 0, 1),$ $(0.3, 0.1, 0.3, 0.1),$ $(0.5, 0.2, 0.2, 0.1)\}$. We see that, in this setting, the distribution always evolves towards a configuration in which there is no defended agents, and the proportion of undefended infected and undefended susceptible are roughly $0.43$ and $0.57$, respectively. 


\begin{figure}[!htb]
\centering
\includegraphics[width=0.4\textwidth]%
{{figure/RL-MFC-CYBERSECURITY/mfc_cybersecurity_mu_evol_testdistrib1}}%
\includegraphics[width=0.4\textwidth]%
{{figure/RL-MFC-CYBERSECURITY/mfc_cybersecurity_mu_evol_testdistrib2}}\\
\includegraphics[width=0.4\textwidth]%
{{figure/RL-MFC-CYBERSECURITY/mfc_cybersecurity_mu_evol_testdistrib3}}%
\includegraphics[width=0.4\textwidth]%
{{figure/RL-MFC-CYBERSECURITY/mfc_cybersecurity_mu_evol_testdistrib4}}\\
\includegraphics[width=0.4\textwidth]%
{{figure/RL-MFC-CYBERSECURITY/mfc_cybersecurity_mu_evol_testdistrib5}}
  \caption{
      Cybersecurity MFC model solved with DDPG in Section~\ref{sec5_RLforMFC}: Evolution of the population distribution for five initial distributions. %
  }
  \label{fig:cyber-ddpg-cn-testing}
\end{figure}

