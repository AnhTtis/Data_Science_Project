We refer to the first class of algorithms as direct parameterization methods, which directly replace the control function by a neural network with appropriate inputs.  Such ideas can be traced back to earlier works such as {\it e.g.}~\cite{psaltis1988multilayered,hunt1992neuralnetworkcontrolsurvey,lehalle1998piecewise,MR2137498}, in which neural networks are used for control and optimal control problems in relatively low dimension and with shallow networks. %

\subsubsection{Global and local in time approaches} 
\label{sec:direct-global-local}

There are two types of direct parameterization methods, depending on how the neural networks get trained. 



\vskip 6pt
\noindent\textbf{Global in time approach.  }%
We start with the method which trains the neural networks used for the control using the whole time horizon at once. More recently, Han and E \cite{han2016deep-googlecitations} were the first to generalize this type of methods to solve problems in high dimensions by using recent modern machine learning techniques such as deep neural networks and efficient built-in stochastic gradient descent solvers. This has motivated fruitful studies on high-dimensional control problems. In particular, the algorithms reviewed below in Sections~\ref{sec:sc_delay} and \ref{sec:directMethod} for SC problems with delay and mean field control are both in the spirit of \cite{han2016deep-googlecitations}. More precisely, \cite{han2016deep-googlecitations} solved the following discrete time version of \eqref{def:control-Xt}--\eqref{def:control-cost},
\begin{align}
     &\checkX_{t_{n+1}} = \checkX_{t_n} + b(t_n, \checkX_{t_n}, \alpha_{t_n}) \Delta t + \sigma(t_n, \checkX_{t_n}, \alpha_{t_n}) \Delta \check W_{t_n}, \label{eq:control-Xt-discrete} \\
    &\min_{(\alpha_{t_n})_{n}} \EE\left[\sum_{n=0}^{N_T-1} f(t_n, \checkX_{t_n}, \alpha_{t_n}) \Delta t + g(\checkX_T)\right] \label{eq:control-Xt-cost},
\end{align}
where $\Delta \check W_{t_n} = \check W_{t_{n+1}} - \check W_{t_n}$ are i.i.d random variables with distribution $\mathcal{N}(0, \Delta t)$. Focusing on Markovian controls, they approximate at each time step the control $\alpha_{t_n}$ in \eqref{eq:control-Xt-discrete} by a feedforward neural network $\alpha_{t_n}(\cdot;\theta_n)$ taking inputs $\check X_{t_n}$, where $\theta_n$ denotes all NN's parameters at time $t_n$. We thus obtain the following cost, which is interpreted as a loss that can be minimized by SGD:
\begin{equation}\label{def:control-loss}
    \check J(\theta) = \EE\left[\sum_{n=0}^{N_T-1} f(t_n, \checkX_{t_n}^\theta, \alpha_{t_n}(\check X_{t_n}^\theta; \theta_n)) \Delta t + g(\checkX_T^\theta)\right], 
    \quad \theta = (\theta_n)_{n=0}^{N_T},
\end{equation}
where $(\checkX_{t_n}^\theta)_{n=1}^{N_T}$ follows
\begin{equation}\label{def:control-Xt-nn}
 \checkX_{t_{n+1}}^\theta = \checkX_{t_n}^\theta + b(t_n, \checkX_{t_n}^\theta, \alpha_{t_n}(\check X_{t_n}^\theta; \theta_n)) \Delta t + \sigma(t_n, \checkX_{t_n}^\theta, \alpha_{t_n}(\check X_{t_n}^\theta; \theta_n)) \Delta \check W_{t_n}.
 \end{equation}
In practice, the expected value in \eqref{def:control-loss} is approximated by the following quantity based on Monte Carlo simulations:
\begin{equation}
    L(\theta,S) = \frac{1}{N}\sum_{j=1}^N\left[\sum_{n=0}^{N_T-1} f(t_n, \checkX_{t_n}^{j,\theta}, \alpha_{t_n}(\checkX_{t_n}^{j,\theta}; \theta_n)) \Delta t + g(\checkX_T^{j,\theta})\right],
\end{equation}
where $\{(\check X_{t_n}^{j, \theta})_{n=1}^{N_T}\}_{j = 1, \ldots, N}$ are sample paths of $(\checkX_{t_n}^{\theta})_{n=1}^{N_T}$ in \eqref{def:control-Xt-nn}  using i.i.d. samples of  $(\Delta \check W_{t_n})_{n=1}^{N_T}$. 

Constraints on the states and the controls can be taken into account by adding penalty terms in the loss function as in {\it e.g.} \cite{han2016deep-googlecitations}. 

\begin{remark}
  Alternatively, one can use a single neural network $(t,x) \mapsto \alpha(t,x;\theta)$ and evaluate it at $(t,x) = (t_n,\check X_{t_n})$, $n = 0, \ldots N_T-1$. In this case, the network can directly capture the time continuity in the control process (if it is continuous). Furthermore, it can be trained for various grids of points in time, and after training it can be used for arbitrary time points. 
\end{remark}



\vskip 6pt

\noindent\textbf{Local in time approach. }
The second approach has been proposed by Bachouch, Hur\'{e},  Langren\'{e} and Pham in \cite{bachouch2021deepnumerical}. It combines classical dynamic programming (DP) and deep neural networks for approximating the control and possibly the value function. There are two main versions. The first version, called NNContPI, is desgined as follows. Assuming that the optimal controls at time steps $t_{n+1}, \ldots, t_{N_T-1}$ are already learnt with neural network parameters $\hat \theta_{n+1}, \ldots, \hat \theta_{N_T-1}$,
the optimal control at time $t_n$ is approximated by a neural network $\alpha_{t_n}(\cdot; \hat \theta_n)$ where the optimized parameters are determined by
\begin{equation}\label{eq:NNContPI}
    \hat \theta_n \in \argmin_{\theta} \EE\Big[f(t_n, \checkX_{t_n}^\theta, \alpha_{t_n}(\checkX_{t_n}^\theta; \theta))\Delta t + \sum_{n'=n+1}^{N_T-1}f(t_{n'}, \checkX_{t_{n'}}^\theta, \alpha_{t_{n'}}(\checkX_{t_{n'}}^\theta; \hat \theta_{n'}))\Delta t + g(\checkX_T^\theta)\Big],
\end{equation}
where $(\checkX_{t_{n'}}^\theta)_{n'=n+1}^{N_T}$ follows \eqref{def:control-Xt-nn} with  $\alpha_{t_n}(\cdot; \theta), \alpha_{t_{n+1}}(\cdot; \hat \theta_{n+1}), \ldots, \alpha_{t_{N_T-1}}(\cdot; \hat \theta_{N_T-1})$ being used. Then, as always, the expected value is approximated by sample paths of $\check X^{j, \theta}$, and the optimal $\hat \theta_n$ is obtained by SGD as described in Appendix~\ref{sec:DLtools}. The second version, termed as Hybrid-now, further approximates the value function ({\it i.e.}, the cost-to-go) at time $t_{n+1}$ using a deep neural network to avoid repeated computation of the last two terms in \eqref{eq:NNContPI}.
More precisely, given the learnt approximated value function $ V_{t_{n+1}}(\cdot; \tilde\theta_{n+1})$ at time $t_{n+1}$, the optimal policy at time $t_n$ is determined in a manner that is similar to the NNContPI algorithm
\begin{equation}\label{eq:Hybrid-Now}
    \hat\theta_n \in \argmin_{\theta} \EE[f(t_n, \checkX_{t_n}^\theta, \alpha_{t_n}(\checkX_{t_n}^\theta; \theta))\Delta t +  V_{t_{n+1}}(\checkX_{t_{n+1}}^\theta; \tilde \theta_{n+1})],
\end{equation}
and the value function $V_{t_n}$ at time $t_n$ is then approximated by another neural network $V_{t_n}(\cdot; \tilde \theta_n)$ whose parameters are determined by
\begin{equation}
     \tilde \theta_n \in \argmin_\theta \EE\Big[ |f(t_n, \checkX_{t_n}^\theta, \alpha_{t_n}(\checkX_{t_n}^\theta; \hat\theta_n)) \Delta t +  V_{t_{n+1}}(\checkX_{t_{n+1}}^\theta; \tilde \theta_{n+1}) - V_{t_n}(\check X_{t_n}^\theta; \theta) |^2 \Big].
\end{equation}



\begin{remark}
 The first approach discussed above (global in time, developed by \cite{han2016deep-googlecitations}) learns all the optimal controls $\alpha_{t_n}(\cdot; \hat\theta_n)$, $n = 0, \ldots, N_T-1$ at once, by performing a unique SGD with a loss function that involves the whole time horizon, while the second approach \cite{bachouch2021deepnumerical} learns $\alpha_{t_n}$ sequentially and backwardly, for $n = N_T-1, N_T-2, \ldots, 0$. Though the first work may be efficient since NNs for optimal controls at each time are designed in practice to share parameters and they are getting trained at the same time, it may encounter vanishing or exploding gradient problem for large $N_T$ as remarked in \cite{bachouch2021deepnumerical}. 
\end{remark}

\begin{remark}
It is common to use the same architecture for the neural networks $\alpha_{t_n}$ of all the time steps. Even with the same architecture, the parameters can be different, which yields different functions for different time steps.  %
\end{remark}


\begin{rem}[Theoretical analysis]
In \cite{hure2021deepconvergence}, the authors provided the consistency and rates of convergence for the control and the value function subject to universal approximation error of the neural networks.
\end{rem}
