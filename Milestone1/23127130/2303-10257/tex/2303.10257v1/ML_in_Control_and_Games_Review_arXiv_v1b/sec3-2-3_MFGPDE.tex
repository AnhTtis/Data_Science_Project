

We now consider the PDE systems describing the equilibrium or social optimum in MFG or MFC, respectively. The Deep Galerkin Method (DGM) introduced in~\cite{SiSp:18} and reviewed in Section~\ref{sec:PDE} has been adapted to solve such PDE systems, see~\cite{al2018solving,carmona2019convergence1,ruthotto2020machine,cao2020connecting,lin2020apac,AMSnotesLauriere}. We recall that the principle of the method is, for a single PDE, to replace the unknown function by a neural network and to optimize the parameters so as to minimize the residual of the PDE. 

For the sake of the presentation, we consider the MFG PDE system~\eqref{def:control-HJB-MFG}--\eqref{eq:MFG-general-KFP}. In line with the DGM method described in section~\ref{sec:PDE}, we proceed as follows. First, the MFG PDE system is rewritten as a minimization problem over the pair consisting of the density and the value function. The loss function is the sum of the two PDE residuals, as well as penalization terms for the initial and terminal conditions. Instead of the whole state space $\RR^d$, we focus on a compact subset $\tilde\dom \subset \RR^d$. If needed, extra penalization terms taking into account the boundary conditions can be added in the loss function. 
To be specific, we introduce the following loss function
\begin{equation}
\label{eq:loss-DGM-total}
	L(\mu,u) = L^\text{(KFP)}(\mu,u) + L^\text{(HJB)}(\mu,u),
\end{equation}
which is composed of one term for each PDE of the MFG system~\eqref{def:control-HJB-MFG}--\eqref{eq:MFG-general-KFP}. Each term is itself split into two terms: one for the residual inside the domain and one for the initial or terminal condition. The KFP loss function is
\begin{align}
\label{eq:loss-DGM-KFP}
	 L^\text{(KFP)}(\mu,u)
	 &= C^\text{(KFP)} \left\| 
        \displaystyle \partial_t \mu  - \sum_{i,j} \frac{\partial^2}{\partial_{x_i}\partial_{x_j}}\left( D_{i,j}\mu\right) + \diver\left( \mu b\right) \right\|^2_{L^2([0,T] \times \tilde\dom)} + C^\text{(KFP)}_0 \left\|  \mu(0)  - \mu_0 \right\|^2_{L^2(\tilde\dom)},
\end{align}
with $\nu_t = \mu_t \circ (I_d, \alpha(t,\cdot))^{-1}$ where $\ctrl(t,x) = \ctrl(t, x, \nu_t, \grad_x u(t,x), \Hess_x u(t,x))$, and $D$ and $b$ are defined as: 
\begin{align}
   D(t,x) = \frac{1}{2} \sigma(t, x, \nu_t, \alpha(t,x))\sigma(t,x, \nu_t, \alpha(t,x))\transpose, 
   \quad 
   b(t,x) = b(t,x,\nu_t,\ctrl(t,x)).
\end{align} 
The HJB loss function is
\begin{equation}
\label{eq:loss-DGM-HJB}
	 L^\text{(HJB)}(\mu,u)
	= C^\text{(HJB)} \left\| \partial_t u   + \min_{\alpha \in \mc{A}} H(\cdot,\cdot, \nu, \grad_x u, \Hess_x u, \alpha) \right\|^2_{L^2([0,T] \times \tilde\dom)}
	+ C^\text{(HJB)}_T \left\| u(T) - g(\cdot,  \mu(T) ) \right\|^2_{L^2(\tilde\dom)},
\end{equation}
with $H$ defined by~\eqref{def:control-H-MFG}. The weights $C^\text{(KFP)}, C^\text{(KFP)}_0, C^\text{(HJB)},$ and $C^\text{(HJB)}_T$ are positive constants that are used to tune the importance of each component relatively to the other components. If $(\mu,u)$ is a smooth enough solution to the PDE system~\eqref{def:control-HJB-MFG}--\eqref{eq:MFG-general-KFP}, then $L(\mu,u) = 0$. From here, the same strategy as in the DGM can be applied: one can look for an approximate solution using a class of parameterized functions for $\mu$ and $u$, replace the $L^2$ norms by integrals, and use samples to get Monte Carlo estimates; see~\cite{SiSp:18} and Section~\ref{sec:PDE} for more details.  %



\begin{remark} The same ideas can be applied to tackle the PDE systems arising in MFC, or other settings such as ergodic MFG.
In this latter case, the initial and terminal conditions are replaced by normalization conditions; see~\cite{LaLi:2007}. Furthermore, if the PDE system was initially posed on a bounded domain and the solution had to satisfy boundary conditions, then these extra conditions could be dealt with by adding more penalty terms. See \it{e.g.}, \cite{carmona2019convergence1} for more details on these settings. For MFC with nonsmooth costs, Reisinger,  Stockinger and Zhang \cite{reisinger2021fast} developed an iterative algorithm  incorporating the gradient information and the proximal map of the nonsmooth cost.
\end{remark}


















\vskip 6pt

\paragraph*{Numerical illustration: a mean-field model of optimal execution.}


We now present an example based on a model of optimal execution. This model is similar to the one studied in Subsection \ref{sec:directMethod}. We consider a population of traders in which each trader wants to liquidate $Q_0$ shares of a given stock by a fixed time horizon $T$. At time $t\in[0,T]$, we denote by $S_t$ the price of the stock, by $Q_t$ the inventory ({\it i.e.}, number of shares) held by the representative trader, and by $X_t$ their wealth. These state variables are subject to the following dynamics 
\begin{equation*}
\begin{cases}
    dS_t = \gamma \bar{\mu}_t  dt + \sigma \ud W_t,\\
    dQ_t = \ctrl_t \ud t,\\
    dX_t = -\ctrl_t(S_t+\kappa \ctrl_t)\ud t.
\end{cases}
\end{equation*}
The evolution of the price $S$ is stochastic, representing that it can not be predicted with certainty. The randomness, scaled by $\sigma$, comes in through a standard Wiener process $W$. Furthermore, the drift of $S$ captures the permanent price impact $\gamma\bar{\mu}_t$ at time $t$. Here $\gamma>0$ is a multiplicative constant and $\bar{\mu}_t$ is the aggregate trading rate of all the traders.
The control $\ctrl_t$ at time $t$ corresponds to the individual rate of trading of the representative trader. Last, $\kappa>0$ is a constant that represents a quadratic transaction cost.


We assume that the representative agent tries
to maximize the following quantity, in which the first two terms reflect their payoff while the last two terms captures their risk aversion
$$
    \mathbb{E} \left[ X_T + Q_TS_T - A |Q_T|^2 - \phi \int_0^T |Q_t|^2 \ud t \right].
$$
 The constants $\phi>0$ and $A>0$ give weights to penalties for holding inventory through time and at the terminal time, respectively. 
 
 \begin{remark}
 Except for the fact that $\bar{\mu}_t$ is here endogenous, this is the model considered in \cite{MR3500455}, to which a deep learning method has been applied in~\cite{leal2020learning} to approximate the optimal control on real data.

In contrast with the model studied in Subsection \ref{sec:directMethod}, the model considered here is not linear-quadratic and the inventory is not directly subject to random shocks. We refer the interested reader to \cite{CarmonaWebster} and  \cite{CarmonaLeal} for more details and variants of these models.
\end{remark}

Although this problem is formulated with three state variables, we can actually reduce the complexity of the problem in the following way. When $(\bar{\mu}_t)_{0\le t\le T}$ is given, the optimal control of the representative agent can be found by solving an HJB equation. Following~ \cite{MR3500455}, the value function $V(t,x,s,q)$ can be decomposed as
 $V(t,x,s,q) = x + qs + v(t,q)$ for some function $v$ which is a solution to
$$
    -\gamma\bar{\mu} q = \partial_t v - \phi q^2 + \sup_\ctrl \{\ctrl \partial_q v - \kappa \ctrl^2\},
$$
with terminal condition $v(T,q) = - A q^2$. The maximizer in the supremum leads to the optimal control, which can be expressed as: $\ctrl^*_t(q) = \frac{\partial_q v(t,q)}{2\kappa}$. Based on this and going back to the consistency condition yields that, at equilibrium, the aggregate trading rate is
$$
    \bar{\mu}_t = \int \ctrl^*_t(q) \mu(t,dq) = \int \frac{\partial_q v(t,q)}{2\kappa} \mu(t, dq),
$$
 where $\mu(t,\cdot)$ is the distribution of inventories at time $t$ satisfying the KFP PDE:
$$
    \partial_t \mu + \partial_q\left( \mu  \frac{\partial_q v(t,q)}{2\kappa}\right) = 0, t \ge 0, \qquad \mu(0,\cdot) = \mu_ 0.
$$
As a consequence, the equilibrium solution of the MFG satisfies
\begin{equation}
\label{eq:CL-PDE-reduced}
\left\{
\begin{aligned}
    &\quad -\gamma\bar{\mu} q = \partial_t v - \phi q^2 + \frac{|\partial_q v(t,q)|^2}{4\kappa},
    \\
    &\quad \partial_t \mu + \partial_q\left( \mu  \frac{\partial_q v(t,q)}{2\kappa}\right) = 0,
    \\
    &\quad \bar{\mu}_t = \int \frac{\partial_q v(t,q)}{2\kappa} \mu(t, dq),
    \\
    &\quad \mu(0,\cdot) = \mu_ 0, v(T,q) = - A q^2. 
\end{aligned}
\right.
\end{equation}
The mean-field coupling between the two equations is non-local since it involves $\bar{\mu}_t$, and it combines the population distribution with the HJB solution. The value function, the associated control, and the mean of the distribution can be computed by solving a system of ODEs, which provides a benchmark to test numerical methods. We refer to \cite{cardaliaguet2017mean} for more details.

Moreover, \cite{al2018solving} proposed a further change of variables to simplify the numerical computations  and then used the DGM to approximate the solution of the transformed PDE system. Here, to simplify the presentation, we stick to the above PDE system~\eqref{eq:CL-PDE-reduced} and solve it directly using DGM. The initial and terminal conditions are imposed by penalization. For the non-local term $\bar{\mu}_t$, we use Monte Carlo samples to estimate the integral. In the implementation, we used the following values for the parameters: $T=1$, $\sigma=0.3$, $A = 1$, $\phi = 1$, $\kappa = 1$, $\gamma = 1$, and a Gaussian initial distribution with mean $4$ and variance $0.3$. To ensure that the neural network for the distribution always outputs positive values, we used on the last layer the exponential function as an activation function.   Figure~\ref{fig:ex-mfg-crowd-trade-distrib} shows the evolution of the distribution $m$. Figure~\ref{fig:ex-mfg-crowd-trade-value} shows the control obtained from the neural network approximating the HJB solution. The final distribution is concentrated near $0$, which is consistent with the intuition that the traders need to liquidate. Furthermore, the learnt control coincides with the theoretical optimal control that can be computed by solving an ODE system~\cite{cardaliaguet2017mean}.





\begin{figure}[ht]
\subfloat[]{
  \includegraphics[width=0.45\linewidth]{{figure/CROWDTRADEDGM/test14n_fig_nu_surface_step25001}.pdf}
  }
\subfloat[]{
\includegraphics[width=0.45\linewidth]{{figure/CROWDTRADEDGM/test14n_fig_nu_contour_benchmark_step25001}.pdf}
}
\caption{Trade crowding MFG example in Section~\ref{sec:MFPDE-deeplearning} solved by DGM. Evolution of the distribution $m$. Left: surface with the horizontal axes representing time and space and the vertical axis representing the value of the density. Right: contour plot of the density with a dashed red line corresponding to the mean of the density computed by the semi-explicit formula. %
}
\label{fig:ex-mfg-crowd-trade-distrib}
\end{figure}

\begin{figure}[ht]
\subfloat[]{
\includegraphics[width=0.3\linewidth]{{figure/CROWDTRADEDGM/test14n_fig_ctrl_curves_0_step25001}.pdf}
}
\subfloat[]{
\includegraphics[width=0.3\linewidth]{{figure/CROWDTRADEDGM/test14n_fig_ctrl_curves_30_step25001}.pdf}
}
\subfloat[]{
\includegraphics[width=0.3\linewidth]{{figure/CROWDTRADEDGM/test14n_fig_ctrl_curves_49_step25001}.pdf}
}
\caption{Trade crowding MFG example in Section~\ref{sec:MFPDE-deeplearning} solved by DGM. Each plot corresponds to the control at a different time step: Optimal control $\ctrl^*$ (dashed line) and learnt control (full line). %
}
\label{fig:ex-mfg-crowd-trade-value}
\end{figure}


