

We now turn our attention to the question of solving the MFG master equation (Section~\ref{sec:background-master-eq}). Intuitively, the main motivation is to be able to approximate the value function of a representative player for any population distribution. This is in contrast with the methods presented above, which are based on controls that are fully decentralized in the sense that they are functions of the time and the state of the agent only. The fact that they do not depend on the population distribution is an advantage in that it simplifies the implementation, but it is also a limitation since the agent is not able to react to new distributions. For example, if the initial distribution is not known, the agent is not able to solve the forward equation and hence she is not able to anticipate the distribution at future times. The presence of common noise in the dynamics poses a similar challenge. For these reasons, being able to approximately solve the master equation is interesting for applications. When the state space is continuous, the distribution is an infinite dimensional object which is hard to approximate. For simplicity, we will thus focus here on a finite-state setting, in which case the distribution is simply a histogram. The convergence of finite-state MFGs to continuous-state MFGs has been studied for instance in~\cite{hadikhanloo2019finite}. Even though assuming the state space to be finite resolves the question of approximating the distribution, the master equation is posed on a high-dimensional space if the number of states is large. We will hence rely once again on neural networks to approximate the solution to this equation. 




\paragraph*{Master equation for finite state MFG.} 
We consider a finite-state MFG model based on the presentation of such models in~\cite[Section 7.2]{carmona2018probabilistic}. We consider a finite state space $\cE = \{e_1, \dots, e_d\}$ and an action space $\cA \subseteq \RR^\ctrldim$, which can be discrete or continuous. The states can be viewed as one-hot vectors, \textit{i.e.}, as the elements of a canonical basis of $\RR^d$. Then, the set of probability distributions on $\cE$ is the simplex $\{m \in \RR^d \,| \, \sum_{i=1}^d m_i = 1\}$, and we will sometimes write $m(x)$ instead of $m(\{x\})$. The running cost and the terminal cost are denoted by $f: \cE \times \cP(\cE) \times \cA \to \RR$ and  $g: \cE \times \cP(\cE) \to \RR$. The dynamics are given by a jump rate function denoted by $\lambda : \cE \times \cP(\cE) \times \cA \to \RR$. 
We denote by $\RR^\cE$ the set of functions from $\cE$ to $\RR$.

In this context, a finite state MFG equilibrium is is a pair $(\hat{m}, \hat{\ctrl})$ with $\hat{m}: [0,T] \times \cE \to \RR$ and $\hat{\ctrl}: [0,T] \times \cE \to \cA$ such that
\begin{enumerate}
	\item $\hat{\ctrl}$ minimizes
\begin{align*}
	J^{MFG}_{\hat{m}}: \ctrl \mapsto  \EE \left[\int_0^T f(X_t^{\hat{m}, \ctrl}, \hat{m}(t,\cdot), \ctrl(t,X_t^{\hat{m}, \ctrl}) ) dt + g(X_T^{\hat{m}, \ctrl}, \hat{m}(T,\cdot)) \right],
\end{align*}
subject to: $X^{\hat{m}, \ctrl} = (X_t^{\hat{m}, \ctrl})_{t \ge 0}$ is a nonhomogeneous $\cE$-valued Markov chain with transition probabilities determined by the $Q$-matrix of rates $q^{\hat{m}, \ctrl}: [0,T] \times \cE \times \cE \to \RR$ given by
\begin{equation}
\label{master-eq-num:q-finite-MFG}
	q^{\hat{m}, \ctrl}(t, x,x') = \lambda(x, x', \hat{m}(t,\cdot), \ctrl(t,\cdot)), \qquad (t,x,x') \in [0,T] \times \cE \times \cE,
\end{equation}
and $X_0^{\hat{m}, \ctrl}$ has distribution with density $m_0$;
	\item For all $t \in [0,T]$, $\hat{m}(t,\cdot)$ is the law of $X_t^{\hat{m}, \hat{\ctrl}}$.
\end{enumerate}

The Hamiltonian of the problem is defined as
$$
	H(x, m, h) = \sup_{\ctrl \in \cA} -L(x, m, h, \ctrl)
$$ 
where $L: \cE \times \cP(\cE) \times \RR^\cE \times \cA \to \RR$ denotes the Lagrangian
$$
	L(x, m, h, \ctrl) = \sum_{x' \in \cE} \lambda(x, x', m, \ctrl) h(x') + f(x, m, \ctrl).
$$
Under suitable assumptions on the model, the supremum in the definition of $H$ admits a unique maximizer for every $(x, m, h) \in \cE \times \cP(\cE) \times \RR^\cE$, that we denote by
\begin{equation}
\label{master-eq-num:finiteMFG-ctrl-argmax}
	\ctrl^*(x, m, h) = \argmax_{\ctrl \in \cA} -L(x, m, h, \ctrl).
\end{equation}
The coefficients of the rates of the $Q$-matrix when using the optimal control are denoted by 
$$
	q^*(x,x',m,h) = \lambda\big(x, x', m, \ctrl^*(x,m, h)\big),
$$
where $q^*: \cE \times \cE \times \cP(\cE) \times \RR^\cE \to \RR$.


Similarly to the continuous setting (see Section~\ref{sec:mfg-theoretical-background}) the mean field Nash equilibrium can be characterized using a forward-backward system of deterministic or stochastic equations. Using the deterministic approach, the optimal conditions take the form of an ODE system (instead of a PDE system as in the continuous space case). The system is composed of a forward ODE for the mean field $m: [0,T] \times \cE \to \RR$ and a backward ODE for the value function $u: [0,T] \times \cE \to \RR$. Under suitable assumptions (see, \textit{e.g.}, \cite[section 7.2]{carmona2018probabilistic}), there is a unique MFG equilibrium $(\hat{m}, \hat{\ctrl})$, which is characterized by:
$$
	\hat{\ctrl}(t,x) = \ctrl^*(x, \hat m(t,\cdot), \hat u(t,\cdot)), 
$$  
where $\ctrl^*$ is defined by~\eqref{master-eq-num:finiteMFG-ctrl-argmax} and $(u,m)$ solves the forward-backward system
\begin{equation}
    \label{master-eq-num:ODE-system-finiteMFG}
    \begin{dcases}
        \displaystyle 0 
	   = - \partial_t \hat u(t, x) + H(x, \hat m(t,\cdot), \hat u(t,\cdot)),
	    \quad (t,x) \in [0,T) \times \cE,,
        \\
        0 
	    = \partial_t \hat m(t, x) - \sum_{x' \in \cE} \hat m(t, x') q^*(x', x, \hat m(t,\cdot), \hat u(t,\cdot)), 
	    \quad (t,x) \in (0,T] \times \cE,
	    \\
	    \hat u(T,x) = g(x, \hat m(T,\cdot)), \qquad \hat m(0,x) = m_0(x), 
	    \quad x \in \cE.
    \end{dcases}
\end{equation}

This ODE system can be solved using for example techniques discussed in previous sections for forward-backward PDE or SDE systems. However, this assumes that the initial distribution $m_0$ is known and when it is unknown, new techniques are required. We thus consider the master equation. 


As in the continuous space case described in Section~\ref{sec:background-master-eq}, the solution to the master equation makes the dependence of $\hat u$ and $\hat m$ completely explicit. In the present discrete space setting, the master equation can be written as follows (see, \textit{e.g.}, \cite[section 7.2]{carmona2018probabilistic})
 \begin{equation}
 \label{master-eq-num:master-finiteMFG}
 	-\partial_t \cU(t,x,m) 
	+ H(x,m,\cU(t, \cdot, m))
	- \sum_{x' \in \cE} h^*(m ,\cU(t, \cdot, m))(x') \frac{\partial \cU(t, x ,m)}{\partial m(x')} = 0, 
 \end{equation}
for $(t,x,m) \in [0,T] \times \cE \times \cP(\cE)$, with the terminal condition $\cU(T,x,m) = g(x,m)$, for $(x,m) \in \cE \times \cP(\cE)$. The function $h^*: [0,T] \times \cP(\cE) \times \RR^\cE \times \cE \to \RR$ is defined as
$$
	h^*(m, u)(x') = \sum_{x \in \cE} \lambda(x, x', m, \ctrl^*(x, m, u)) m(x).
$$
Besides a simple representation of probability distributions, the fact that the state space is finite has another advantage: we do not need to involve the notions of derivative with respect to a measure discussed in Section~\ref{sec:background-master-eq}. Instead, we can rely on standard partial derivatives with respect to the finite-dimensional inputs of $\cU$. As a matter of fact, in the above equation, $\displaystyle \frac{\partial \cU(t, x ,m)}{\partial m(x')}$ denotes the standard partial derivative of $\RR^d \ni m \mapsto \cU(t,x,m)$ with respect to the coordinate corresponding to $x'$ (recall that $m$ is viewed as a vector of dimension $d$). The analog of~\eqref{eq:masterfield-to-u} in the continuous case is 
\begin{equation}
\label{eq:master-eq-to-hjb-sol-finitestate}
    \cU(t,x,\hat m(t)) = \hat u(t,x),
\end{equation}
where $\hat m = (\hat m(t))_t$ is the mean-field equilibrium distribution flow. Notice that both $\hat m$ and $\hat u$ implicitly depend on the initial distribution $m_0$, but $\cU$ does not. 





The master equation~\eqref{master-eq-num:master-finiteMFG} is posed on a possibly high dimensional space since the number $d$ of states can be large. To numerically solve this equation, we can thus rely on deep learning methods for high-dimensional PDEs, such as the DGM introduced in~\cite{SiSp:18} and already discussed above in Sections~\ref{sec:PDE} and~\ref{sec:MFPDE-deeplearning}. 
For the sake of completeness, let us mention that this technique boils down to approximating $\cU$ by a neural network, say $\cU_\theta$ with parameters $\theta$, and using SGD to adjust the parameters $\theta$ such that the residual of~\eqref{master-eq-num:master-finiteMFG} is minimized and the terminal condition is satisfied. SGD as described in Algorithm~\ref{algo:SGD-generic} in Appendix~\ref{sec:SGD-var} is used, where a sample is $\xi = (t,x,m) \in [0,T] \times \cE \times \cP(\cE)$ and the loss function is
\begin{equation}
\label{master-eq-num:master-finiteMFG-residual}
	\mathfrak{L}(\cU_\theta, \xi) = \left|\partial_t \cU_\theta(t,x,m) - H(x,m,\cU_\theta(t,x,m))
	+ \sum_{x' \in \cE} h^*(m ,\cU_\theta(t,x,m))(x') \frac{\partial \cU_\theta(t,x,m)}{\partial m(x')}\right|^2.
\end{equation}

 
    
\begin{remark}
    We have discussed here how to solve the master equation arising in finite-state MFGs. This yields an approach to solving the continuous space master equation described in Section~\ref{sec:background-master-eq} by first introducing a finite-state model that approximates the continuous model and then using the DGM method. Other methods could be investigated, such as the one proposed in~\cite{germain2021deepsets}: a combination of dynamic programming, Monte Carlo simulations and symmetric neural networks is used to solve the Bellman equation arising in a continuous space MFC problem.
\end{remark}









\paragraph*{Numerical illustration: A Cybersecurity model.} 
Here we present an example of application of the above method. We consider the cybersecurity introduced in~\cite{MR3575619}; see also~\cite[Section 7.2.3]{carmona2018probabilistic}. Each player owns a computer and her goal is to avoid being infected by a malware. The state space is denoted by $\cE = \{DI, DS, UI, US\}$, which represents the four possible states in which a computer can be depending on its protection level -- defended (D) or undefended (U) -- and on its infection status  -- infected (I) or susceptible (S) of infection. The player can choose to switch its protection level between D and U. The change is not instantaneous so the player can only influence the transition rate. We represent by ``$1$'' the fact that the player has the intention to change its level of protection (be it from D to U or from U to D). On the other hand, ``$0$'' corresponds to the situation where the player does not try to change her protection level. So the set of possible actions is $\cA = \{0,1\}$. When the action is equal to $1$, the change of level of protection takes place at a rate  $\rho >0$. A computer in states DS or US might get infected either directly by a hacker or by getting the virus from an infected computer. We denote by $v_H q_{inf}^D$ (resp. $v_H q_{inf}^U$) the rate of infection from a hacker if the computer is defended (resp. undefended). We denote by $\beta_{UU}\mu(\{UI\})$ (resp. $\beta_{UD}\mu(\{UI\})$) the rate of infection from an undefended infected computer if the computer under consideration is undefended (resp. defended). Likewise, we denote by $\beta_{DU}\mu(\{DI\})$ (resp. $\beta_{DD}\mu(\{DI\})$) the rate of infection from a defended infected computer if the computer under consideration is undefended (resp. defended). Note that these rates involve the distribution since the probability of getting infected should increase with the number of infected computers in the rest of the population. Last, an infected computer can recover and switch to the susceptible state at rate $q_{rec}^D$ or $q_{rec}^U$ depending on whether it is defended or not.
These transition rates can be summarized in a matrix form: for $m \in \cP(\cE), a \in \cA$, 
$$
	\lambda(\cdot, \cdot, m, a) 
	= \left( \lambda(x, x', m, a) \right)_{x,x' \in \cE}
	= \begin{pmatrix}
	\dots 	& 		P^{m,a}_{DS \rightarrow DI}	&	 \rho a 	&	0
	\\
	q_{rec}^D 	& 	\dots 		&	 0	&	\rho a
	\\
	\rho a 	& 	0 		&	 \dots	&	P^{m,a}_{US \rightarrow UI}
	\\
	0	&	\rho a	&	q_{rec}^U	& \dots
	\end{pmatrix},
$$
where 
\begin{align*}
	&P^{m,a}_{DS \rightarrow DI} = v_H q_{inf}^D + \beta_{DD} m(\{DI\})  + \beta_{UD} m(\{UI\}) ,
	\\
	&P^{m,a}_{US \rightarrow UI} = v_H q_{inf}^U + \beta_{UU} m(\{UI\}) + \beta_{DU} m(\{DI\}).
\end{align*}
The dots ($\dots$) on each row stand for the value such that the sum of the coefficients on this row equals $0$. 

We assume that each player wants to avoid seeing her computer being infected, but protecting a computer costs some resources. So the running cost is of the form
$$
	f(t, x, \nu, \ctrl) = -\left[ k_D \indic_{\{DI, DS\}}(x) + k_I \indic_{\{DI, UI\}}(x)\right],
$$
where $k_D>0$ is a protection cost  to be paid whenever the computer is defended, and $k_I>0$ is a penalty incurred  if the computer is infected. We consider $g \equiv 0$ (no terminal cost).

By using the DGM, we train a neural network $\cU_\theta$ to approximate the solution $\cU$ to the master equation. Equation~\eqref{eq:master-eq-to-hjb-sol-finitestate} provides us with a way to check how accurate this approximation is:  we can fix an initial distribution, solve the forward-backward ODE system (which is easy given the initial condition), and then compare the value of the neural network $\cU_\theta$ evaluated along the equilibrium flow of distributions with the solution to the backward ODE for the value function. To be specific, for every $m_0$, we first compute the equilibrium value function $\hat u^{m_0}$ and the equilibrium flow of distributions $\hat m^{m_0})$. We then evaluate $\cU_\theta(t,x,\hat m^{m_0}(t,\cdot))$ for all $t \in [0,T]$ and check how close it is to $\hat u^{m_0}(t,x)$ for each of the four possible states $x$.  Figures~\ref{AMS-num-fig:finiteMFG-cyber-Master-m0-1}--\ref{AMS-num-fig:finiteMFG-cyber-Master-m0-3} show that the two curves (for each state) coincide for at least three different initial conditions. This means that, using the DGM, we managed to train a neural network that accurately represent the value function of a representative player for various distributions at once. In the numerical experiments, we used the following values for the parameters
\begin{align*}
&\beta_{UU} = 0.3,
\beta_{UD} = 0.4,
\beta_{DU} = 0.3,
\beta_{DD} = 0.4,
\qquad v_H = 0.2,
\lambda = 0.5,
\\
&q_{rec}^D = 0.1, 
q_{rec}^U = 0.65, 
q_{inf}^D = 0.4, 
q_{inf}^U = 0.3, 
\qquad k_D = 0.3, k_I = 0.5.
\end{align*}




\begin{figure}[ht]
\subfloat[]{		\includegraphics[width=.45\columnwidth]{{figure/MASTER-FINITEMFG/Master_v21/test5/testE_diffShapeUcross/test1_mu_evol}.pdf}
}
\subfloat[]{
		\includegraphics[width=.45\columnwidth]{{figure/MASTER-FINITEMFG/Master_v21/test5/test1_u_master_evol}.pdf}
}
	\caption{MFG Cybersecurity example in Section~\ref{sec:mastereq-deeplearning}. Test case 1: Evolution of the distribution $m^{m_0}$ (left) and the value function $u^{m_0}$ and $\cU(\cdot, \cdot, m^{m_0}(\cdot))$ (right) for $m_0 = (1/4, 1/4, 1/4, 1/4)$. First published in~\cite{AMSnotesLauriere} by the American Mathematical Society.} 
 	\label{AMS-num-fig:finiteMFG-cyber-Master-m0-1}
\end{figure}



\begin{figure}[ht]
\subfloat[]{
		\includegraphics[width=.45\columnwidth]{{figure/MASTER-FINITEMFG/Master_v21/test5/testE_diffShapeUcross/test2_mu_evol}.pdf}
}
\subfloat[]{
\includegraphics[width=.45\columnwidth]{{figure/MASTER-FINITEMFG/Master_v21/test5/test2_u_master_evol}.pdf}
}
	\caption{MFG Cybersecurity example in Section~\ref{sec:mastereq-deeplearning}. Test case 2: Evolution of the distribution $m^{m_0}$ (left) and the value function $u^{m_0}$ and $\cU(\cdot, \cdot, m^{m_0}(\cdot))$ (right) for  $m_0 = (1, 0, 0, 0)$. First published in~\cite{AMSnotesLauriere} by the American Mathematical Society.} 
 	\label{AMS-num-fig:finiteMFG-cyber-Master-m0-2}
\end{figure}


\begin{figure}[ht]
\subfloat[]{
\includegraphics[width=.45\columnwidth]{{figure/MASTER-FINITEMFG/Master_v21/test5/testE_diffShapeUcross/test3_mu_evol}.pdf}
}
\subfloat[]{
\includegraphics[width=.45\columnwidth]{{figure/MASTER-FINITEMFG/Master_v21/test5/test3_u_master_evol}.pdf}
}
	\caption{MFG Cybersecurity example in Section~\ref{sec:mastereq-deeplearning}. Test case 3: Evolution of the distribution $m^{m_0}$ (left) and the value function $u^{m_0}$ and $\cU(\cdot, \cdot, m^{m_0}(\cdot))$ (right) for  $m_0 = (0, 0, 0, 1)$. First published in~\cite{AMSnotesLauriere} by the American Mathematical Society.} 
 	\label{AMS-num-fig:finiteMFG-cyber-Master-m0-3}
\end{figure}


