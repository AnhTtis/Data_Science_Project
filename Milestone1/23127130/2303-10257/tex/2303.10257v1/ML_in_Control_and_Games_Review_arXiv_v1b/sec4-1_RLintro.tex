
All the previous methods rely, in one way or another, on the fact that the cost functions $f$ and $g$ as well as the drift $b$ and the volatility $\sigma$ (\emph{cf.} \eqref{def:control-Xt}--\eqref{def:control-cost}) are known. However, in many applications, coming up with a realistic and accurate model is a daunting task. It is sometimes impossible to guess the form of the dynamics, or the way the costs are incurred. This has motivated the development of so-called model-free methods. The reinforcement learning (RL) theory provides a framework for studying such problems.  Intuitively, an agent evolving in an environment can take actions and observe the consequences of her actions: the state of the environment (or her own state) changes, and a cost is incurred to the agent. The agent does not know how the new state and the cost are computed. The goal for the agent is then to learn an optimal behavior by trial and error.


Numerous algorithms have been developed under the topic of RL; see, {\it e.g.}, the surveys and books \cite{kaelbling1996reinforcement,Busoniu08,li2017deep,sutton2018reinforcement,hambly2021recent}. Most of them focus on RL itself, with state-of-the-art methods in single-agent or multi-agent problems and some provide theoretical guarantees of numerical performances. We aim to review its connections to stochastic control and games, as well as the mean-field setting. We shall start by discussing how problems in Section~\ref{sec:SCP} are formulated as single-agent RL\footnote{The terminology RL is from the perspective of artificial intelligence/computer science. In the operation research community, it is often called approximate dynamic programming (ADP) \cite{powell2007approximate}.}. Although we here focus on the traditional presentation of RL in discrete time, let us mention that a continuous-time stochastic optimal control viewpoint on RL has been also been studied, see {\it e.g.}, \cite{munos2006policy} for a policy gradient algorithm,  and \cite{wang2020continuousmeanvar,wang2020reinforcement} for a mean-variance portfolio problem and for generic continuous time and space problems. It has also been extended in several directions, such as variance reduction techniques~\cite{kobeissi2022variance}, risk-aware problems~\cite{jaimungal2022robustrl} and mean-field games~\cite{guo2022mfgentropyregu} and \cite{firoozi2022exploratory}.


