
We now turn our attention to RL methods for MFG. As pointed out in Section~\ref{sec:MFG}, finding a mean-field Nash equilibrium boils down to (1) finding a control that is optimal for a representative infinitesimal player facing the equilibrium distribution flow, and (2) computing the induced distribution flow, which should match the equilibrium one. These two elements can be tackled alternatively, as described in Section~\ref{sec:Nplayer} in the N-player case and in Section~\ref{sec4_MFG_with_CN} in the mean-field case. The first part is a standard optimal control problem, which can thus be tackled using standard RL techniques, see Section~\ref{sec:MDP}. In this setting, we assume that the agent who is learning can repeat experiments of the following form: given the current state, the agent chooses an action (or a sequence of actions), and the environment returns the new state as well as the reward (or a sequence of states and rewards). In the representative player's MDP, the distribution enters as a parameter that influences the reward and dynamics, but is fixed when the player learns an optimal policy. During such experiments, we generally assume that the population distribution is fixed, and it is updated after a number of iterations; see {\it e.g.}~\cite{guo2019learning,EliePerolatLauriereGeistPietquin-2019_AFP-MFG}. Alternatively, we can assume that it is updated at every iteration but at a slow rate; see {\it e.g.}~\cite{subramanianpolicy,angiuli2020unified,xie2021learning}. Most of the literature thus far focuses on tabular methods. A few works have used deep RL methods to compute the best response. For example, DDPG have been used in~\cite{EliePerolatLauriereGeistPietquin-2019_AFP-MFG}, soft actor-critic (SAC) has been used for a flocking model in~\cite{perrin2021mfgflockrl}, while deep Q-learning or some variants of it has been used in~\cite{pmlr-cui21-approximately,perrin2021generalizationmfg,lauriere2022scalable}. Recently, several works have studied the advantages and the limitations brought by the regularization of the policy through penalization terms in the cost function~\cite{anahtarci2020mfgqregu,pmlr-cui21-approximately,guo2022mfgentropyregu}. We refer to~\cite{lauriere2022learningmfgsurvey} for a survey of learning algorithms and reinforcement learning methods to approximate MFG solutions. 




\paragraph*{Numerical illustration: an example with explicit solution.}
For the sake of illustration, we consider an MFG model which admits an explicit solution in the continuous time ergodic setting. The mode has been introduced and solved in~\cite{almulla2017two}. The MFG is defined as follows. The state space is the one-dimensional unit torus, {\it i.e.}, $\mathbb{T} = [0,1]$ with periodic boundary condition. The action space is $\mathbb{R}$ (or in practice any bounded interval containing $[-2\pi,2\pi]$, which is the range of the equilibrium control). The drift function is 
$$
  b(x,m,a) = a.
$$
The running cost is
$$
	f(x,m,a) = \tilde f(x) + \frac{1}{2}|a|^2 + \log(m),
$$
where the first term is the following cost, which encodes spatial preferences for some regions of the domain
$$
	\tilde f(x)= -2 \pi^2 \sin(2 \pi x) + 2 \pi^2 \cos(2 \pi x)^2 - 2 \sin(2 \pi x).
$$
In the ergodic MFG setting, the objective of an infinitesimal representative player is to minimize
$$
    \lim_{T \to +\infty}\frac{1}{T} \EE\left[\int_0^T f(X_t, \mu_t(X_t), \alpha_t(X_t)) \ud t \right],
$$
where $X$ is controlled by $\alpha$. Here $\mu_t$ is assumed to have a density for every $t \ge 0$, and we identify it with its density. So $\mu_t(X_t)$ denotes the value of the density of $\mu_t$ at $X_t$.
The equilibrium control and the equilibrium mean-field distribution are respectively given by
$$
  a^*:x\mapsto 2 \pi \cos(2\pi x)\;\quad \mbox{and} \;\quad 
 \mu^*:x\mapsto  \frac{e^{2 \sin(2\pi x)}}{\int_{\mathbb{T}} e^{2 \sin(2\pi y)} \ud y}\;.
$$
We use fictitious play~\cite{cardaliaguet2015learning} combined with a deep RL algorithm to learn the best response at each iteration for the solution. The problem is in continuous state and action spaces and admits a deterministic equilibrium control. Hence, following~\cite{EliePerolatLauriereGeistPietquin-2019_AFP-MFG}, at each iteration, we solve the representative player's MDP using DDPG~\cite{lillicrap2015continuous} reviewed in Section~\ref{sec:MDP-PGM}. 

The left plot of Figure~\ref{fig:analytical-mfg-ddpg} displays (blue line) the population distribution learned by fictitious play, which corresponds to the distribution induced by the average of past best responses. This is to be compared with the ergodic distribution (red dashed line). The right plot displays the last iteration of the best response. This control is approximated by two neural networks: a target network (green dashed line) and an actor network (blue line). We also display the ergodic equilibrium control (red dashed line). 

\begin{figure}[!htb]
\centering
\includegraphics[width=0.45\textwidth]%
{{figure/RL-MFG-ANALYTICAL/ddpg-mfg-distrib}}%
\includegraphics[width=0.45\textwidth]%
{{figure/RL-MFG-ANALYTICAL/ddpg-mfg-control}}
  \caption{
      MFG with analytical solution for the ergodic setting in Section~\ref{sec5_RLforMFG}, solved with fictitious play and DDPG: stationary distribution and control. %
  }
  \label{fig:analytical-mfg-ddpg}
\end{figure}
