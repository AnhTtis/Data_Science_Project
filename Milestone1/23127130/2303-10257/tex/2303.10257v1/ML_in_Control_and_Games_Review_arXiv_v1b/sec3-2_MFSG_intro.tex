

Mean-field games, introduced independently by Lasry and Lions in \cite{LaLi1:2006,LaLi2:2006,LaLi:2007} and by Huang, Malham\'{e} and Caines in \cite{HuMaCa:06,HuCaMa:07}, provide a paradigm to approximate the solutions of stochastic games with very large number of players. The approximation relies on two main assumptions: anonymity, which means that the players interact only through the population's empirical distribution, and indistinguishability, which means that $(b^i, \sigma^i, f^i, g^i)$ are the same for all $i$. Then, one can pass (at least formally) to the limit by letting the number of players grow to infinity. In the asymptotic problem, the influence of each individual player on the rest of the population vanishes and the Nash equilibrium can be characterized by studying the problem posed to a representative player. Under suitable assumptions, it can be shown that solving this limiting problem provides an approximate equilibrium for the finite-player game.  
We refer to the notes~\cite{Cardaliaguet-2013-notes} and the books \cite{MR3134900,carmona2018probabilistic,carmona2018probabilistic2} as well as the references therein for further background on mean-field games. Numerical methods for such games have been developed using mostly traditional techniques such as finite difference schemes~\cite{achdoucapuzzo2010mean,achdoucamillicapuzzo2012mean}, semi-Lagrangian schemes~\cite{carlinisilva2014fully,carlinisilva2018discretization}, or methods based on probabilistic approaches~\cite{chassagneuxcrisandelarue2019numerical,angiuli2019cemracs}. See, {\it e.g.}, \cite{achdoulauriere2020meansurvey,AMSnotesLauriere} for recent surveys. However, similarly to control problems or finite-player games, these methods do not scale well in terms of dimensionality, and in particular, they are not very suitable for problems with delay or with common noise. This motivates the development of deep learning methods, some of which are described in~\cite{carmonalauriere2021deepmfgsurvey}. In the sequel, we describe the theoretical framework of mean field games and then survey recent deep learning methods.








\subsubsection{Theoretical background}
\label{sec:mfg-theoretical-background}
We start by defining the notion of MFG, and then we discuss how equilibria can be characterized in terms of PDEs, BSDEs, and the so-called mater equation. 

\paragraph{Definition of the problem}
Going back to problem~\eqref{def:DFP-open-cost}, let us assume that $b^i, \sigma^0, \sigma^i, f^i, g^i$ depend on the rest of the population's states and actions in an anonymous way, {\it i.e.}, there exists functions $b, \sigma^0, \sigma, f, g$ such that: $b^i(t, \bm{X}_t, \bm{\alpha}_t) = b(t, X^i_t, \nu^N_t, \alpha^i_t)$, $\sigma^0(t, \bm{X}_t, \bm{\alpha}_t) = \sigma^0(t, X^i_t, \nu^N_t, \alpha^i_t)$, $\sigma^i(t, \bm{X}_t, \bm{\alpha}_t) = \sigma(t, X^i_t, \nu^N_t, \alpha^i_t)$, $f^i(t, \bm{X}_t, \bm{\alpha}_t) = f(t, X^i_t, \nu_t^N, \alpha^i_t)$ and $g^i(\bm{X}_T) = g(X_T^i, \mu_T^N)$, where $\nu^N_t = \frac{1}{N} \sum_{j=1}^N \delta_{(X^j_t, \alpha^j_t)}$ is the empirical state-action distribution of the population and $\mu^N_t = \frac{1}{N} \sum_{j=1}^N \delta_{X^j_t}$  is its first marginal, which corresponds to the state distribution. We keep the same notation $\sigma^0$ for simplicity, with a slight abuse of notation. 

Then the cost associated to a strategy profile $\bm{\alpha}$ is defined as
\begin{equation}\label{def:MFG-cost}
J^i(\bm{\alpha}) = \EE\left[\int_0^T f(t, X^i_t, \nu_t^N, \alpha^i_t) \ud t + g(X_T^i, \mu_T^N)\right],
	\end{equation}
where the processes $X^j$, $j=1,\dots,N$, solve the SDE system
\begin{equation}%
\ud X_t^j = b(t, X^j_t, \nu^N_t, \alpha^j_t) \ud t + \sigma(t, X^j_t, \nu^N_t, \alpha^j_t) \ud W_t^j + \sigma^0(t, X^j_t, \nu^N_t, \alpha^j_t) \ud W_t^0, \quad X_0^j \sim \mu_0, \quad j \in \mc{I},
\end{equation}
where the initial positions are i.i.d., with $\nu$ and $\mu$ being as above the flows of empirical state-action and empirical state distributions. The influence of a given player on the dynamics and the cost of another player occurs only through the empirical distribution flow $\nu^N$. So when $N$ increases, the influence of each player decreases. By symmetry, we can expect that in the limit it is sufficient to study the problem for a single representative player.  


To formulate the MFG, let  $\nu=(\nu_t)_{0\le t\le T}$ be a stochastic distribution flow adapted to the filtration generated by $W^0$, which is interpreted at the evolution of the population's state-action configuration. Let $\alpha$ be an open-loop control. A representative player's dynamics are given by,
\begin{align}
\label{eq:dyn-X-general-MFG}
\begin{dcases}
    \ud X_t^{\nu, \ctrl} 
    = b(t, X_t^{\nu, \ctrl}, \nu_t, \ctrl_t) \ud t
    \\ 
    \qquad\qquad + \sigma(t, X_t^{\nu, \ctrl}, \nu_t, \ctrl_t) \ud W_t 
	 + \sigma^0(t, X_t^{\nu, \ctrl}, \nu_t, \ctrl_t) \ud W^0_t, \quad t \ge 0
	 \\
	 X^{\nu, \ctrl}_0 \sim \mu_0,
\end{dcases}
\end{align}
where $W$ is a standard $m$-dimensional Brownian motion independent of $W^0$. For a representative player, the cost associated to using the control $\alpha$ when the population is given by the distribution flow  $\nu=(\nu_t)_{0\le t\le T}$ is defined as
\begin{align}
\label{subchapRCML-num-eq:def-J-MFG}
	J^{MFG}(\ctrl; \nu) = \EE \left[\int_0^T f(t, X_t^{\nu, \ctrl}, \nu_t, \ctrl_t ) \ud t + g(X_T^{\nu, \ctrl}, \mu_T) \right],
\end{align}
under the constraint that the  process $X^{\nu, \ctrl} = (X_t^{\nu, \ctrl})_{t \ge 0}$ solves the SDE~\eqref{eq:dyn-X-general-MFG}.


\begin{defn}[mean field Nash equilibrium] 
\label{def:MFGeq-finitehorizon}
Consider the MFG problem introduced above. A pair $(\hat{\nu},\hat{\ctrl})$ consisting of a stochastic flow $\hat{\nu}=(\hat \nu_t)_{0\le t\le T}$ of probability measures in $\cP_2(\RR^d)$ adapted to the common noise filtration and an open-loop control $\hat{\ctrl} = (\hat{\ctrl}_t)_{t \in [0,T]}$ is a mean field Nash equilibrium if it
satisfies the following two conditions
\begin{enumerate}
	\item $\hat{\ctrl}$ minimizes $J^{MFG}(\cdot; \hat{\nu})$;
	\item For all $t \in [0,T]$, $\hat{\nu}_t$ is the probability distribution of $(X_t^{\hat{\nu}, \hat{\ctrl}}, \hat{\ctrl}_t)$ conditioned on $W^0$.
\end{enumerate}
\end{defn}
Note that, in the first condition, $\hat\nu$ is fixed when an infinitesimal agent performs their optimization. The second condition ensures that if all the players use the control $\hat{\ctrl}$, the law of their individual states and actions is indeed $\hat{\nu}$. 
The original formulation of MFGs~\cite{MR2295621} considers interactions through the state distribution only. MFGs with interactions through the joint distribution of state and actions, as presented in the above definition, are sometimes referred to as extended MFGs or MFGs of controls, see {\it e.g.}, \cite{GoPaVo:14,GoVo:16,cardaliaguet2017mean,kobeissi2022classical,laurieretangpi2022convergence}.

Next, we review several ways to characterize the mean field Nash equilibrium concept using analytical and probabilistic techniques. 



\vskip 6pt
\paragraph{PDE system}

For simplicity, let us assume that there is no common noise. We assume that there exists an equilibrium and we denote by $\hat{\nu} = (\hat{\nu}_t)_{t \ge 0}$ the associated mean-field flow of distributions. When considering Markovian controls, one can define the value function $u$ by
\begin{equation}
    u(t, x) = \inf_\alpha \EE\left[\int_t^T f(s, X_s, \hat{\nu}_s, \alpha_s) \ud s + g(X_T, \hat \mu_T) \vert X_t = x\right]. 
\end{equation}
As in standard OC problems, 
under suitable conditions, $u(t, x)$ solves the HJB equation:
\begin{equation}
    \label{def:control-HJB-MFG}
    \begin{dcases}
   	\partial_t u (t,x)  + \min_{\alpha \in \mc{A}} H(t, x, \hat{\nu}_t, \grad_x u(t,x), \Hess_x u(t, x), \alpha) = 0,
   	\\
   	u(T, x) = g(x,  \hat{\mu}_T),
    \end{dcases}
\end{equation}
where
\begin{equation}
\label{def:control-H-MFG}
    H(t, x, \nu, p, q, \alpha) = b(t, x, \nu, \alpha) \cdot p + \half \Tr(\sigma(t, x, \nu, \alpha)\sigma(t,x, \nu, \alpha)\transpose q) + f(t, x, \nu, \alpha).
\end{equation} 
If \eqref{def:control-HJB-MFG} has a classical solution, then the optimal control is given by
\begin{equation*}
	\hat{\ctrl}(t, x)
	= \ctrl(t, x, \hat{\nu}_t, \grad_x u(t,x), \Hess_x u(t,x)),
\end{equation*}
where
\begin{equation*}
	\ctrl(t, x, \nu, p, q)
	= 
	\argmin_{\alpha \in \mc{A}} H(t, x, \nu, p, q, \alpha).
\end{equation*}
The consistency condition for the equilibrium mean-field flow is equivalent to: the state distribution flow $\hat{\mu} = (\hat{\mu}_t)_{t \ge 0}$ solves the following Kolmogorov-Fokker-Planck (KFP) PDE
\begin{equation}
    \label{eq:MFG-general-KFP}
    \begin{dcases}
        \displaystyle \partial_t \hat{\mu}(t,x)  - \sum_{i,j} \frac{\partial^2}{\partial_{x_i}\partial_{x_j}}\left( \hat{D}_{i,j}(t,x)\hat{\mu}(t,x)\right) + \diver\Bigl( \hat{\mu}(t,x) \hat{b}(t,x)\Bigr) = 0,
        \\
        \hat{\mu}(0) = \mu_0,
    \end{dcases}
\end{equation}
where
\begin{equation}
    \label{eq:def-KFP-hat-D-b}
    \hat{D}(t,x) = \frac{1}{2} \sigma(t, x, \hat{\nu}_t, \hat{\alpha}(t,x))\sigma(t,x, \hat{\nu}_t, \hat{\alpha}(t,x))\transpose, \qquad \hat{b}(t,x) = b(t,x,\hat{\nu}_t,\hat{\ctrl}(t,x)),
\end{equation}
and the state-action distribution $\hat{\nu}_t$ at time time $t$ is the push forward of $ \hat{\mu}_t$ by $(I_d, \hat\alpha(t,\cdot))$, which we will denote by 
$
    \hat{\nu}_t =  \hat{\mu}_t \circ (I_d, \hat\alpha(t,\cdot))^{-1}. 
$
The forward-backward PDE system \eqref{def:control-H-MFG}--\eqref{eq:MFG-general-KFP} characterizes the mean field Nash equilibrium. We refer to {\it e.g.}, \cite{kobeissi2022classical} for the existence of classical solutions to such PDE systems under suitable assumptions. 

\begin{remark}
MFC problems also give rise to analogous forward-backward PDE systems, except that the solution $u$ of the backward equation is not interpreted as a value function of an optimal control problem but rather as an adjoint state. We refer to \cite{MR3134900,achdoulauriere2015systemmfc} for more details. The KFP equation remains the same, but the HJB equation has one extra term reflecting the fact that the whole population performs the optimization simultaneously.
\end{remark}

In the presence of common noise, the HJB and KFP equations become stochastic. We will not discuss this system in the sequel, and refer the interested readers to \cite{peng1992stochastichjb} for the derivation of stochastic HJB equations and~\cite{carmonadelarue2014mastereqlarge,cardaliaguetdelaruelasrylions2019master} for stochastic HJB-KFP systems arising in MFG (with the state distribution only). %




\vskip 6pt
\paragraph{FBSDE system}

We now review the characterization of MFG equilibria using BSDEs. As for standard OC (see Section~\ref{sec:control_intro}), BSDEs can be used to characterize the value function or its gradient. For simplicity, we assume that there is no common noise. We further assume that the volatility of the idiosyncratic noise is uncontrolled, in which case $\hat \ctrl$ is independent of $\Hess_x u(t,x)$ and the PDE \eqref{def:control-HJB-MFG} becomes semi-linear:
\begin{multline}
	\partial_t u (t,x)  + \half \Tr(\sigma(t, x, \hat{\nu}_t)\sigma(t,x, \hat{\nu}_t)\transpose \Hess_x u(t, x)) + b(t, x, \hat{\nu}_t,	\hat{\ctrl}(t, x, \hat{\nu}_t, \grad_x u(t,x))) \cdot  \grad_x u(t,x) \\
	+ f(t, x, \hat{\ctrl}(t, x, \hat{\nu}_t, \grad_x u(t,x))) = 0.
\end{multline}
Suppose that there exist functions $\mu(t, \nu, x)$ and $h(t, x, \nu, z)$ such that 
\begin{multline*} 
    \tilde{b}(t, \hat{\nu}_t, x) \cdot \grad_x u(t,x) + h(t, x, \hat{\nu}_t, \sigma(t,x) \transpose \grad_x u(t,x)) 
    \\= b(t, x, \hat{\nu}_t, \hat{\ctrl}(t, x, \hat{\nu}_t, \grad_x u(t,x))) \cdot  \grad_x u(t,x) 
+ f(t, x, \hat{\nu}_t, \hat{\ctrl}(t, x, \grad_x u(t,x))).
\end{multline*}
Then the non-linear Feynman-Kac formula (see \cite{PaPe:90}) gives the following BSDE interpretation of $u(t, x)$:
\begin{equation}\label{def_MF_BSDEreform}
    \begin{dcases}
    \ud \mc{X}_t = \tilde{b}(t, \hat{\nu}_t, \mc{X}_t) \ud t + \sigma(t, \hat{\nu}_t, \mc{X}_t) \ud W_t, \quad \mc{X}_0 \sim \mu_0, \\
    \ud \mc{Y}_t = -h(t, \hat{\nu}_t, \mc{X}_t, \mc{Z}_t)\ud t + \mc{Z}_t \ud W_t, \quad \mc{Y}_T = g(\mc{X}_T, \hat{\mu}_T),
    \end{dcases}
\end{equation}
by the relation
$$
\mc{Y}_t = u(t, \mc{X}_t), \quad \mc{Z}_t = \sigma(t, \hat{\nu}_t, \mc{X}_t) \transpose \grad_x u(t, \mc{X}_t).
$$
Moreover, the optimal value is given by $\mathbb{E}[\mc{Y}_0] = \mathbb{E}[u(0, \mc{X}_0)]$. This BSDE characterizes the value function for a representative player given the mean field flow $\hat\nu$. Then consistency condition reads: 
$$
    \hat{\nu}_t = \Law\left(\mc{X}_t, \hat{\ctrl}(t, \mc{X}_t, \hat{\nu}_t, (\sigma(t, \hat{\nu}_t, \mc{X}_t) \transpose)^{-1}\mc{Z}_t)\right).
$$

In the controlled volatility case, the PDE \eqref{def:control-HJB-MFG} is fully nonlinear, and its solution is connected to a solution of the 2BSDE, see~\cite{cheridito2007second} and Section~\ref{sec:control_intro}. 

The Pontryagin stochastic maximum principle provides the connection to the FBSDE. Define the generalized Hamiltonian $\mc{H}$ by
\begin{equation}
\label{eq:Hamiltonian-bsde-control-MFG}
\mc{H}(t,x,\nu,y,z,\alpha) = b(t, x, \nu, \alpha) y + \Tr(\sigma\transpose(t, x, \nu, \alpha)z) + f(t, x, \nu, \alpha).
\end{equation}
If the Hamiltonian $\mc{H}$ is convex in $(x, \alpha)$, 
and $(X_t, Y_t, Z_t)$ solve
\begin{equation}
    \begin{dcases}
    \ud X_t = b(t, X_t, \hat{\nu}_t, \hat \alpha_t) \ud t +\sigma(t, X_t, \hat{\nu}_t, \hat \alpha_t) \ud W_t, \qquad X_0 \sim \mu_0,\\
    \ud Y_t = -\grad_x \mc{H}(t, X_t, \hat{\nu}_t, Y_t,  Z_t,  \hat \alpha_t ) \ud t + Z_t \ud W_t, \quad Y_T = \partial_x g(X_T, \hat{\mu}_T),
    \end{dcases}
\end{equation}
such that $\hat \alpha$ minimizes $\mc{H}$ along $(X_t, \hat{\nu}_t, Y_t, Z_t)$, then $\hat \alpha$ is the optimal control. If the value function is smooth enough, then 
\begin{equation}
     Y_t = \grad_x u(t,  X_t), \quad Z_t = \sigma(t, X_t, \hat{\nu}_t, \hat \alpha)\transpose\Hess_x u(t,  X_t).
\end{equation}
In this case, the consistency condition for the equilibrium mean field flow $\hat{\nu}$ reads
$$
    \hat{\nu}_t = \Law\left(X_t, \hat{\ctrl}(t, X_t, \hat{\nu}_t, (\sigma(t, \hat{\nu}_t, X_t) \transpose)^{-1} Y_t, Z_t)\right).
$$
When there is common noise, the FBSDE system becomes
\begin{equation}
    \begin{dcases}
    \ud X_t = b(t, X_t, \hat{\nu}_t, \hat \alpha_t) \ud t +\sigma(t, X_t, \hat{\nu}_t, \hat \alpha_t) \ud W_t
    +\sigma^0(t, X_t, \hat{\nu}_t, \hat \alpha_t) \ud W^0_t, \qquad X_0 \sim \mu_0,\\
    \ud Y_t = -\grad_x \mc{H}(t, X_t, \hat{\nu}_t, Y_t,  Z_t,  Z^0_t,  \hat \alpha_t ) \ud t + Z_t \ud W_t + Z^0_t \ud W^0_t, \quad Y_T = \partial_x g(X_T, \hat{\mu}_T),
    \end{dcases}
\end{equation}
where, compared with~\eqref{eq:Hamiltonian-bsde-control-MFG},  the definition of $\mc{H}$ includes an extra term $\Tr({\sigma^0}\transpose(t, x, \nu, \alpha)z^0)$.

\vskip 6pt

\begin{remark} 
MFC problems also lead to analogous FBSDE systems. In the absence of common noise, Pontryagin's maximum principle is derived for instance in~\cite{MR3045029} and~\cite{acciaio2019extendedmfc} when the interactions are through the state or the state-action distributions respectively. This leads to a BSDE with an extra term accounting for the variation of the distribution during the optimization of the control.
\end{remark}




\vskip 6pt 
All the above systems are particular cases of the following generic system of FBSDEs of McKean-Vlasov type (MKV FBSDE for short)
\begin{equation}
\label{eq:MKV-FBSDE-general}
\left\{
\begin{aligned}
	\ud X_t
	=
	\,& B\left(t, X_t, \Law(X_t, Y_t, Z_t |W^0), Y_t, Z_t, Z^0_t \right) \ud t
		\\
		&\qquad
		+  \sigma(t, X_t, \Law(X_t, Y_t, Z_t |W^0), Y_t, Z_t, Z^0_t) \ud W_t 
		\\
		&\qquad + \sigma^0(t, X_t, \Law(X_t, Y_t, Z_t |W^0), Y_t, Z_t, Z^0_t) \ud W^0_t,
	\\
	\ud Y_t
	=
	\,& - F\Big(t, X_t, \Law(X_t, Y_t, Z_t |W^0), Y_t, \sigma\transpose(t, X_t, \Law(X_t, Y_t, Z_t |W^0), Y_t, Z_t, Z^0_t) Z_t, 
	\\
	&\quad\qquad{\sigma^0}\transpose(t, X_t, \Law(X_t, Y_t, Z_t |W^0), Y_t, Z_t, Z^0_t) Z^0_t \Big) \ud t
		\\
		&\qquad
		+ Z_t \ud W_t + Z^0_t \ud W^0_t,
    \\
    \Law(X_0) &= \mu_0, \qquad Y_T = G(X_T, \Law(X_T|W^0)).
\end{aligned}
\right.
\end{equation}


\begin{remark}
When there is no common noise, $W^0$ and $Z^0$ are dropped and the system becomes
\begin{equation}
\label{eq:MKV-FBSDE-general-no-CN}
\left\{
\begin{aligned}
	\ud X_t
	=
	& B\left(t, X_t, \Law(X_t, Y_t, Z_t), Y_t, Z_t \right) \ud t
			+  \sigma(t, X_t, \Law(X_t, Y_t, Z_t), Y_t, Z_t) \ud W_t ,
	\\
	\ud Y_t
	=
	& - F\Big(t, X_t, \Law(X_t, Y_t, Z_t), Y_t, \sigma\transpose(t, X_t, \Law(X_t, Y_t, Z_t), Y_t, Z_t) Z_t\Big) \ud t
			+ Z_t \ud W_t,
    \\
    \Law(X_0) &= \mu_0, \qquad Y_T = G(X_T, \Law(X_T)).
\end{aligned}
\right.
\end{equation}
When the interactions are not through the state-action distribution but through the state distribution only, $\Law(X_t, Y_t, Z_t)$ is reduced to $\Law(X_t)$.
\end{remark}



\paragraph{Master equation}
\label{sec:background-master-eq}


As mentioned earlier, in the PDE system~\eqref{def:control-HJB-MFG}--\eqref{eq:MFG-general-KFP}, $u$ plays the role of the value function of a representative player when the rest of the population is at equilibrium. This function depends explicitly on $t$ and $x$ but, intuitively, a player's value function can also depend on the population distribution. When there is no common noise, this distribution evolves in a deterministic way, so knowing $\mu_0$ and $t$ as well as the control used by the population (which is the equilibrium control $\hat\alpha$, assuming the population is at equilibrium) is enough to recover $\hat\mu(t)$, {\it e.g.}, by solving the corresponding KFP equation~\eqref{eq:MFG-general-KFP}. However, we can make this dependence explicit by considering a function $\cU: [0,T] \times \RR^d \times \cP(\RR^d) \to \RR$ such that
\begin{equation}
\label{eq:masterfield-to-u}
    \cU(t,x,\hat\mu(t)) = u(t,x),
\end{equation}
where $\hat\mu = (\hat\mu(t))_t$ is the mean-field equilibrium distribution flow. This correspondence is even more useful when common noise influences the dynamics of the players. In this case, $u(t,x)$ is a random variable whereas $\cU$ is still a deterministic function and the lefthand side of~\eqref{eq:masterfield-to-u} is random only due to $\hat\mu(t)$. This function $\cU$ has been instrumental in proving the convergence of finite-player Nash equilibria towards mean field Nash equilibria, see~\cite{cardaliaguetdelaruelasrylions2019master} for more details.

It turns out that, under suitable conditions, $\cU$ satisfies the a PDE that we will present below, introduced by Pierre-Louis Lions and called the Master equation. It involves partial derivatives with respect to the probability measure argument in $\cU$. We say that a function $F: \cP(\RR^d) \to \RR$ is $\mathcal{C}^1$ if there exists a continuous map $\displaystyle\frac{\delta F}{\delta \mu}: \cP(\RR^d) \times \RR^d \to \RR$ such that, for any $\mu,\mu' \in \cP(\RR^d)$,
$$
\lim_{s \to 0^+} \frac{F((1-s)\mu + s\mu') - F(\mu)}{s} = \int_{\RR^d} \frac{\delta F}{\delta \mu}(\mu,y) d(\mu'-\mu)(y).
$$
The derivative $\displaystyle\frac{\delta F}{\delta \mu}$ is sometimes referred to as the flat derivative. 
If $\displaystyle\frac{\delta F}{\delta \mu}$ is of class $\mathcal{C}^1$ with respect to the second variable, the intrinsic derivative $\partial_\mu F:\cP(\RR^d) \times \RR^d \to \RR$ is defined by
$$
    \partial_\mu F(\mu,y) = \partial_y \frac{\delta F}{\delta \mu}(\mu,y).
$$
We will write $\partial_\mu F(\mu)(y)$ instead of $\partial_\mu F(\mu,y)$. For more details, we refer to the lectures of Pierre-Louis Lions~\cite{PLL-CDF}, as well as \cite{Cardaliaguet-2013-notes} and \cite[Chapter 5]{carmona2018probabilistic}.


We can now present the Master equation. To the best of our knowledge, the theory has not yet been developed for the general MFG model described above. We thus consider the case in which the volatility is not controlled and the interactions are only through the state distribution instead of the state-action distribution. For the sake of brevity, we omit the derivation and refer to {\it e.g.}~\cite[Section 4.4]{carmona2018probabilistic2}. The Master equation is the following backward PDE, posed on the space $[0,T] \times \RR^d \times \cP_2(\RR^d)$,
\begin{align*}
    &\partial_t \cU(t,x,\mu)
    \\
    &\quad +b(t,x,\mu,\hat\alpha(t, x, \mu, \partial_x \cU(t,x,\mu))) \cdot \partial_x \cU(t,x,\mu)
    \\
    &\quad + \int_{\RR^d} b(t,v,\mu,\hat\alpha(t,v,\partial_x \cU(t,v,\mu))) \cdot \partial_\mu \cU(t,x,\mu)(v) d \mu(v)
    \\
    &\quad + \frac{1}{2} \Tr \left[ (\sigma \sigma\transpose + \sigma^0 (\sigma^0)\transpose)(t,x,\mu) \partial_{xx}^2 \cU(t,x,\mu)\right]
    \\
    &\quad + \frac{1}{2} \int_{\RR^d} \Tr \left[ (\sigma \sigma\transpose + \sigma^0 (\sigma^0)\transpose)(t,v,\mu) \partial_v\partial_\mu \cU(t,x,\mu)(v)\right] d \mu (v)
    \\
    &\quad + \frac{1}{2} \int_{\RR^{2d}} \Tr \left[ (\sigma \sigma\transpose + \sigma^0 (\sigma^0)\transpose)(t,v,\mu) \partial_\mu^2 \cU(t,x,\mu)(v,v')\right] d \mu (v) d \mu (v')
    \\
    &\quad + \int_{\RR^d} \Tr \left[ (\sigma^0(t,x,\mu) (\sigma^0)\transpose)(t,v,\mu) \partial_x\partial_\mu \cU(t,x,\mu)(v)\right] d \mu (v)
    \\
    &\quad + f(t,x,\mu,\hat\alpha(t, x, \mu, \partial_x \cU(t,x,\mu)))
    =0,
\end{align*}
for $t \in [0,T]$, $x \in \RR^d$ and $\mu \in \cP_2(\RR^d)$, and with the terminal condition: for every $x \in \RR^d$ and $\mu \in \cP_2(\RR^d)$,
$$
    \cU(T,x,\mu) = g(x,\mu).
$$
For more details on the analysis of this PDE, we refer the interested reader to the monographs~\cite{cardaliaguetdelaruelasrylions2019master}, \cite[Chapters 4 to 7]{carmona2018probabilistic2}, and~\cite{ChassagneuxCrisanDelarue_Master} concerning the existence of classical solutions under suitable conditions. 

