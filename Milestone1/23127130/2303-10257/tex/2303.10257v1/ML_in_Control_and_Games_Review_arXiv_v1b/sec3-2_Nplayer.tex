We first  consider a stochastic differential game with $N$ players indexed by $i \in \mathcal{I} =  \{1, 2, \ldots, N\}$. Each player has a state process $X_t^i \in \RR^d$ and takes an action $\alpha_t^i$ in the action set $\mc{A} \subset \RR^k$. In the context of games, we will use interchangeably the terms \emph{control} and \emph{strategy}. The dynamics of the controlled state process $X^i$ on $[0,T]$ are given by
\begin{equation}\label{def_Xt_general}
\ud X_t^i = b^i(t, \bm{X}_t, \bm{\alpha}_t) \ud t + \sigma^i(t, \bm{X}_t, \bm{\alpha}_t) \ud W_t^i + \sigma^0(t, \bm{X}_t, \bm{\alpha}_t) \ud W_t^0, \quad X_0^i = x^i, \quad i \in \mc{I},
\end{equation}
where $\bm{W} =[W^0, W^1,\ldots, W^N]$ are $(N+1)$ $m$-dimensional independent Brownian motions, %
and we shall call $W^i$ the individual noises and $W^0$ the common noise. Although at the $N$-player level, adding an extra common noise process is equivalent to choosing $N$ correlated Brownian motions, the current formulation is more convenient when passing the limit $N \to \infty$.
$(b^i, \sigma^i)$ are deterministic functions: $[0,T] \times \RR^{dN} \times \mc{A}^N \to \RR^d \times \RR^{d\times m}$. 
The $N$ dynamics are coupled since all states $\bm{X}_t = [X_t^1, \ldots, X_t^N]$ and all the controls\footnote {Although in the literature of financial mathematics, one usually models $b^i$ and $\sigma^i$ to only depend on player $i$'s own action, it is common in the economics literature that player $i$'s state is also influenced by others' actions. For instance, $\alpha_t^i$ and $X_t^i$ can be respectively the price set by the company and the quantity it produces. To be general, we include this feature in our model, yielding \eqref{def_Xt_general}. Note that by choosing $b^i$ and $\sigma^i$ properly, one can reduce it to the simpler case where each player controls her \emph{private} state through $\alpha^i_t$.}  $\bm{\alpha}_t = [\alpha_t^1, \ldots, \alpha_t^N]$ affect the drifts $b^i$ and diffusion coefficients $\sigma^i$. Given a set of strategies $(\bm{\alpha}_t)_{t \in [0,T]}$, the cost associated to player $i$ is of the form
\begin{equation}\label{def:DFP-open-cost}
J^i(\bm{\alpha}) = \EE\left[\int_0^T f^i(t, \bm X_t, \bm\alpha_t) \ud t + g^i(\bm X_T)\right],
	\end{equation}
where the running cost $f^i: [0,T] \times \RR^{dN}\times \mc{A}^N \to \RR$ and terminal cost $g^i: \RR^{dN} \to \RR$ are deterministic measurable functions. 

Player $i$ chooses $(\alpha_t^i)_{t \in [0,T]}$ to minimize her cost $J^i(\balpha)$ within a set $\mathbb{A}^i$ of admissible strategies. As in Section~\ref{sec:SCP}, the set $\mathbb{A}^i$ usually describes the measurability and integrability of $\alpha_t^i$. Different measurabilities correspond to different information structures available to the players, and they lead to different types of solutions to the game. In the noncooperative setting, the notion of optimality is the so-called \emph{Nash equilibrium} (NE), and the three main types are open-loop NE ($\bm{W}_{[0,t]}$),  closed-loop NE ($\bm{X}_{[0,t]}$), and closed-loop in feedback form NE ($\bm{X}_t$) also known as Markovian Nash equilibrium.  In this section, we will focus on the open-loop case (Section ~\ref{sec:open-loop}) and the closed-loop in feedback form case (Section~\ref{sec:MarkovianNE}).



Before proceeding to the open-loop case, we first summarize some commonly used notations as below. Given a probability space $(\Omega, \MCF, \PP)$, we consider 
\begin{itemize}
\item $\bm{W} = [W^0, W^1, \ldots, W^N]$, a $(N+1)$-vector of $m$-dimensional independent Brownian motions;
\item $\mathbb{F} = \{\MCF_t, 0\leq t \leq T\}$, the augmented filtration generated by $\bm{W}$;
\item $\mathbb{H}^{2}_T(\RR^d)$, the space of all progressively measurable $\RR^d$-valued stochastic processes $\alpha: [0,T] \times \Omega \to \RR^d$ such that $\ltwonorm{\alpha}^2 = \EE[\int_0^T \abs{\alpha_t}^2 \ud t ] < \infty$;

\item $\bm{\alpha} = [\alpha^1, \alpha^2, \ldots, \alpha^N]$, a strategy profile, {\it i.e.}, a collection of all players' strategies. The notation $\bm{\alpha}^{-i} = [\alpha^1, \ldots, \alpha^{i-1}, \alpha^{i+1}, \ldots, \alpha^{N}]$ with a negative superscript means the strategies of all players excluding player $i$'s. If a non-negative superscript $\mt{k}$ appears ({\it e.g.}, $\bm{\alpha}^\mt{k}$), this $N$-tuple stands for the strategies from stage $\mt{k}$.  $\bm{\alpha}^{-i,\mt{k}} =  [\alpha^{1,\mt{k}}, \ldots, \alpha^{i-1,\mt{k}}, \alpha^{i+1,\mt{k}}, \ldots, \alpha^{N,\mt{k}}]$ is a $(N-1)$-tuple representing strategies excluding player $i$ at stage $\mt{k}$. We use the same notations for other stochastic processes ({\it e.g.}, $\bm{X}^{-i}, \bm{X}^\mt{k}$).

\end{itemize}




\subsubsection{Open-loop Nash equilibrium}\label{sec:open-loop}

We first consider the open-loop structure. In this setting, each player's control $\alpha^i$ is in the space $\mathbb{A} = \mathbb{H}^2_T(\mc{A})$. 




\begin{defn}[Open-loop Nash equilibrium]
    A strategy profile $(\hat{\bm{\alpha}}_t)_{t \in [0,T]} = (\hat \alpha^{1}_t, \ldots, \hat\alpha^{N}_t)_{t \in [0,T]} \in \mbA^N$ is called an open-loop Nash equilibrium if
\begin{equation}\label{def_Nash}
\forall i \in \mc{I} \text{ and } \beta^i \in \mbA, \quad J^i(\hat{\bm{\alpha}}) \leq J^i(\beta^i, \hat{\bm{\alpha}}^{-i}),
\end{equation}
where $\hat{\bm{\alpha}}^{-i}= [\hat \alpha^{1}, \ldots, \hat \alpha^{i-1}, \hat \alpha^{i+1}, \ldots, \hat \alpha^{N}] \in \mbA^{N-1}$ is the equilibrium strategies of all the players except the $i$-th one.
\end{defn}


Hu proposed in~\cite{Hu2:19} the idea of deep fictitious play (DFP for short), which consists in decoupling the $N$-player game into $N$ individual decision problems using \emph{fictitious play}, and then solving these $N$ individual problems using deep neural networks. Fictitious play was firstly introduced by Brown for static games \cite{Br:49,Br:51}, and was recently adapted to the mean-field setting by \cite{cardaliaguet2015learning,hadikhanloo2018ph,BrCa:18}. It is a simple yet important learning scheme in game theory for finding Nash equilibria. Deep learning provides efficient tools for solving the decoupled yet still high-dimensional optimization problems. 

The DFP algorithm starts with an initial strategy profile $(\balpha_t^0)_{t \in [0,T]} \in \mbA^N$, which can be interpreted as the initial belief of all players, and updates $\mt{K}$ times: $\balpha_t^0 \to \balpha_t^1 \to \ldots \to \balpha_t^\mt{k} \to \ldots \to \balpha_t^\mt{K}$ or until some stopping criterion is satisfied. At the beginning of stage $\mt{k}+1$, $\bm{\alpha}^\mt{k}$ is observable by all players. Player $i$ then considers that other players are going to reuse the same strategy as they used in the previous iteration, namely $\bm{\alpha}^{-i,\mt{k}}$, and looks for a best response. Then, player $i$ faces an optimization problem
\begin{equation}\label{def_J_SFP}
    \inf_{\beta^i \in \mbA} J^{i}(\beta^i;\bm{\alpha}^{-i, \mt{k}}), \quad 
    J^{i}(\beta^i; \bm\alpha^{-i, \mt{k}}) = \EE\left[\int_0^T f^i(t,\bm{X}_t^{\balpha}, (\beta^i, \bm{\alpha}^{-i, \mt{k}})) \ud t + g^i(\bm X_T^{\balpha})\right],
\end{equation}
where $\bm X_t^{\balpha} = [X_t^{1,{\balpha}}, X_t^{2,{\balpha}}, \ldots, X_t^{N,{\balpha}}]$ are state processes controlled by $(\beta^i, \bm\alpha^{-i,\mt{k}})$,
\begin{align}\label{eq:DPF-open-Xt}
\ud X_t^{\ell,\balpha}= b^\ell(t, \bm{X}_t^{\balpha}, (\beta^i, \bm{\alpha}^{-i, \mt{k}})) \ud t  +  \sigma^\ell(t, \bm{X}_t^{\balpha}, (\beta^i, \bm{\alpha}^{-i, \mt{k}})) \ud W_t^\ell   +  \sigma^0(t, \bm{X}_t^{\balpha}, (\beta^i, \bm{\alpha}^{-i, \mt{k}})) \ud W_t^0, \; 
\end{align}
with $X_0^{\ell,\alpha} = x^\ell$, for all $\ell \in \mc{I}$. Denote by $\alpha^{i, \mt{k}+1}$ the minimizer in \eqref{def_J_SFP},
\begin{equation}\label{def_SFP_opt-i}
    \alpha^{i,\mt{k} +1} = \argmin_{\beta^i \in \mbA^i} J^{i}(\beta^i; \bm{ \alpha}^{-i, \mt{k}}), \quad \forall i \in \mc{I}, \; \mt{k} < \mt{K}.
\end{equation}
We assume $\alpha^{i,\mt{k}+1}$ exists and is unique throughout this section. More precisely, $\alpha^{i,\mt{k}+1}$ is player $i$'s optimal strategy at stage $\mt{k}+1$ when the other players' controls are \eqref{def_Xt_general} evolve according to $\alpha^{j,\mt{k}}$, $j \neq i$. All players find their best responses simultaneously, which together form $\bm{\alpha}^{\mt{k}+1}$.

\begin{rem}
	Note that the above learning process is different than the usual simultaneous fictitious play \cite{Br:49,Br:51}, where the belief is described by the average over strategies played in previous iterations of the algorithm: $\frac{1}{\mt{k}}\sum_{\mt{k}'=1}^\mt{k} \bm{\alpha}^{-i,\mt{k}'}$. 
\end{rem}

In general, one can not expect that the players' actions $\balpha^\mt{k}$ always converge. However, if the sequence $\{\bm{\alpha}^{\mt{k}}\}_{\mt{k}=1}^\infty$ ever admits a limit, denoted by $\bm\alpha^{\infty}$, one would expect it to form an open-loop Nash equilibrium under suitable assumptions. Intuitively, in the limiting situation, when all other players are using strategies $\alpha^{j,\infty}$, $j \neq i$, by some stability argument, player $i$'s optimal strategy to the control problem \eqref{def_J_SFP} should be $\alpha^{i,\infty}$, meaning that she will not deviate from $\alpha^{i,\infty}$, which makes $(\alpha^{i,\infty})_{i=1}^N$ an open-loop equilibrium. 





\begin{rem}[Theoretical analysis]
    In \cite{Hu2:19}, Hu proved that for linear-quadratic games, under appropriate conditions, the family $\{\bm \alpha^n\}_{n \in \NN}$  converges to an open-loop Nash equilibrium of the original problem \eqref{def_Xt_general}-\eqref{def:DFP-open-cost}. Moreover, the limit, denoted by $\bm\alpha^\infty$, is independent of the choice of initial belief $\bm\alpha^0$. 
\end{rem}






When seeking for an open-loop equilibrium, admissible controls in the optimization problem~\eqref{def_SFP_opt-i} for player $i$ are  $\bW$-adapted. After a time discretization, the control at time $t_n$ can be expressed as a function of $\bW$ at the previous time steps. So the problem is by nature high-dimensional, which motivates the use of deep neural networks. Similarly to the direct approach reviewed in Section~\ref{sec:control-direct}, Hu~\cite{Hu2:19} solved each player's control problems \eqref{def_J_SFP} at stage $\mt{k}$ by using a feedforward fully connected network to directly parameterize $\alpha^{i, \mt{k}}$. 
When seeking for an open-loop equilibrium, the $N$ optimization problems~\eqref{def_SFP_opt-i} for $i=1,\dots,N$ can be solved in parallel, as $\alpha^{i, \mt{k}}$ are meant to be $\bW$-adapted thus are not influenced by changes in the other players' states $\bX_t^{-i}$. More precisely, to solve the optimization problem in~\eqref{def_SFP_opt-i}, each $\beta_{t_n}^{i, \mt{k}}$, $n=0,\dots,N_T-1$, is implemented by
\begin{equation}\label{def:NN-OL}
    \beta_{t_n}^{i, \mt{k}} \sim \beta_{t_n}^{i, \mt{k}}(\check\bX_0, \check\bW_{t_1}, \ldots, \check\bW_{t_n}; \theta_n^i),
\end{equation}
which maps $\RR^{N(n+1)}$ to $\RR$, 
and the optimal strategy at the $\mt{k}^{th}$ stage, denoted by $(\alpha_{t_n}^{i, \mt{k}}(\cdot; \theta_{n}^{i, \mt{k}}))_{n = 0}^{N_T-1}$, is determined by minimizing a discretized version of \eqref{def_J_SFP}, {\it i.e.},
\begin{equation}\label{eq:OL-cost}
   \{\theta_n^{i, \mt{k}}\}_{n=0}^{N_T-1} \in \argmin_{\{\theta_n^i\}_{n=0}^{N_T-1}} \EE\left[\sum_{n=0}^{N_T - 1} f^i(t_n, \check\bX_{t_n}^\theta, (\beta^{i, \mt{k}}_{t_n}(\check\bX_0, \check\bW_{t_1}, \ldots, \check\bW_{t_n}; \theta_n^i), \balpha_{t_n}^{-i, \mt{k}-1})) \Delta t + g^i(\check\bX_T^\theta)\right],
\end{equation}
where $(\check \bX_{t_n}^\theta)_{n=1}^{N_T}$ follows an Euler scheme corresponding to \eqref{eq:DPF-open-Xt} with controls $(\beta_{t_n}^{i,\mt{k}}, \balpha_{t_n}^{-i, \mt{k}-1})_{n=0}^{N_T-1}$ being used. The pseudo-code is given in Algorithm~\ref{algo:DFP-open} in Appendix~\ref{supp:pseudocode}. 




\paragraph*{Numerical illustration: a linear-quadratic systemic risk problem with $24$ players.} \label{par:LQ-systemic-risk-finite-N} 

We illustrate the DFP algorithm on the LQ systemic risk game presented in Section~\ref{sec:intro-LQsysrisk} and originally introduced in \cite{CaFoSu:15}. For explicit formulas describing the open-loop Nash equilibrium, we refer to~\cite[Section~3.1]{CaFoSu:15}. 

In the numerical illustration, the number of players is set to be $N = 24$, and  the time steps is set at $N_T = 20$, after observing the maximum relative errors, defined by
\begin{equation}
    \max_{i \in \mathcal{I}} \frac{J^i(\alpha^{i, \mt{k}}; \balpha^{-i, \mt{k}-1}) - J^i(\hat \balpha)}{J^i(\hat \balpha)},
\end{equation}
did not increase too much from $N_T = 50$ to $N_T = 20$. The initial positions for the $i^{th}$ player is $x_0^i = 0.5i$, and results are presented in Figures~\ref{fig:N24_cost_traj}-\ref{fig:N24control}. Some key features that have been observed: the maximum of relative error drops below $3\%$ after ten iterations; the average error of estimated trajectories are convex/concave functions of time $t$; the standard deviation of estimated error aggregates from steps to steps. In fact, the convexity/concavity with respect to time $t$ is caused by two factors: the propagation of errors, which produces an increase in error mean; and the existence of terminal cost, which puts more weight on $X_T$ than $X_t, t \in (0,T)$, resulting in a better estimate of $X_T$ and a decreasing effect. 



\begin{figure}[h t p]
	\begin{tabular}{cc}
		\includegraphics[width=0.45\textwidth]{figure/n24/N=24_relativeerr.png}	&	
		\includegraphics[width=0.45\textwidth]{figure/n24/N=24_trajactory.png}
	\end{tabular}
	\caption{Comparisons of cost functions and optimal trajectories for $N=24$ players in the linear quadratic systemic risk problem in Section~\ref{sec:open-loop}. Left: the maximum relative errors of the cost functions for 24 players; Right: for a sake of clarity, the comparison of optimal trajectories in only presented for the $1^\text{st}$, $4^\text{th}$, $7^\text{th}$, $10^\text{th}$, $13^\text{th}$, $16^\text{th}$, $19^\text{th}$ and $22^\text{th}$ players, where the solid lines are given by the closed-form solution and the stars are computed by deep fictitious play. %
 }\label{fig:N24_cost_traj}
\end{figure}


\begin{figure}[h t b]
	\begin{tabular}{ccc}
		\includegraphics[width=0.3\textwidth]{figure/n24/N=24_tra_errorbar_0.png} &
		\includegraphics[width=0.3\textwidth]{figure/n24/N=24_tra_errorbar_3.png}	&	
		\includegraphics[width=0.3\textwidth]{figure/n24/N=24_tra_errorbar_6.png} \\
		\includegraphics[width=0.3\textwidth]{figure/n24/N=24_tra_errorbar_9.png} &
		\includegraphics[width=0.3\textwidth]{figure/n24/N=24_tra_errorbar_10.png} &
		\includegraphics[width=0.3\textwidth]{figure/n24/N=24_tra_errorbar_12.png} \\
		\includegraphics[width=0.3\textwidth]{figure/n24/N=24_tra_errorbar_15.png} &
		\includegraphics[width=0.3\textwidth]{figure/n24/N=24_tra_errorbar_18.png} &
		\includegraphics[width=0.3\textwidth]{figure/n24/N=24_tra_errorbar_21.png} \\
	\end{tabular}
	\caption{Comparisons of trajectories for $N=24$ players in the linear quadratic game in Section~\ref{sec:open-loop} using the learnt equilibrium strategy profile. For the sake of clarity, we only show the mean (blue triangles) and standard deviation (red bars) of trajectories errors for the $1^\text{st}$, $4^\text{th}$, $7^\text{th}$, $10^\text{th}$, $11^\text{th}$, $13^\text{th}$, $16^\text{th}$, $19^\text{th}$ and $22^\text{th}$ player, respectively. The results are based on a total of $65536$ sample paths. They show that deep fictitious play provides a relatively uniformly good accuracy. %
 }\label{fig:N24traj}
\end{figure}


\begin{figure}[h t b]
	\begin{tabular}{ccc}
		\includegraphics[width=0.3\textwidth]{figure/n24/N=24_control_player0.png} &
		\includegraphics[width=0.3\textwidth]{figure/n24/N=24_control_player3.png}	&	
		\includegraphics[width=0.3\textwidth]{figure/n24/N=24_control_player6.png} \\
		\includegraphics[width=0.3\textwidth]{figure/n24/N=24_control_player9.png} &
		\includegraphics[width=0.3\textwidth]{figure/n24/N=24_control_player10.png} &
		\includegraphics[width=0.3\textwidth]{figure/n24/N=24_control_player12.png} \\
		\includegraphics[width=0.3\textwidth]{figure/n24/N=24_control_player15.png} &
		\includegraphics[width=0.3\textwidth]{figure/n24/N=24_control_player18.png} &
		\includegraphics[width=0.3\textwidth]{figure/n24/N=24_control_player21.png} 
	\end{tabular}
	\caption{Comparisons of optimal controls for $N=24$ players in the linear quadratic game in Section~\ref{sec:open-loop}. For clarity, we only show two sample paths of optimal controls for the $1^\text{st}$, $4^\text{th}$, $7^\text{th}$, $10^\text{th}$, $11^\text{th}$, $13^\text{th}$, $16^\text{th}$, $19^\text{th}$ and $22^\text{th}$ player, respectively. The solid lines are optimal controls given by the closed-form solution, and the dotted dash lines are computed by deep fictitious play. %
 }\label{fig:N24control}
\end{figure}

In the original paper, the author also gave numerical experiments with $N = 5$ and $N = 10$ players. To better illustrate that the algorithm can mitigate the curse of dimensionality, the performance comparison is made across different $N$. In particular, the author computed the error $\max_{i \in \mc{I}}\max_{n \leq N_T}|X_{t_n}^i - \check X_{t_n}^{i, \theta}|$,
where $X$ denotes the state process following the open-loop Nash equilibrium, while $\check X^\theta$ is the deep fictitious play counterpart. The error is $1.09\times 10^{-2}$ for $N = 5$, $1.49\times 10^{-2}$ for $N = 10$ and $2.08\times 10^{-2}$ for $N = 24$.



\subsubsection{Markovian Nash equilibrium}\label{sec:MarkovianNE}
In this subsection, consider closed-loop Nash equilibria and in particular  Markovian Nash equilibria. A Markovian strategy for player $i$ is a measurable functions of $(t, \bX_t)$. Given a Markovian strategy profile  $\balpha_t$, \eqref{def_Xt_general} rewrites as
\begin{equation}
    \ud X_t^i = b^i(t, \bm{X}_t, \bm{\alpha}(t, \bX_t)) \ud t + \sigma^i(t, \bm{X}_t, \bm{\alpha}(t, \bX_t)) \ud W_t^i + \sigma^0(t, \bm{X}_t, \bm{\alpha}(t, \bX_t)) \ud W_t^0, \quad X_0^i = x^i_0, \quad i \in \mc{I}.
\end{equation}
Each player aims to minimize the cost 
\begin{equation}\label{def:DFP-closed-cost}
J^i(\bm{\alpha}) = \EE\left[\int_0^T f^i(t, \bm X_t, \bm\alpha(t, \bm X_t)) \ud t + g^i(\bm X_T)\right].
\end{equation}
The dependence of $b^i$ and $\sigma^i$ on $\balpha$ leads to a problem with stronger coupling than the open-loop setting. The problem is thus harder to solve, both theoretically and numerically. 

\begin{defi}
A strategy profile $\hat{\bm \alpha} = (\hat \alpha^{1}, \ldots, \hat \alpha^{N})$ consisting of Markovian strategies is called a Markovian Nash equilibrium, if 
\begin{equation}
\forall i \in \mc{I}, \emph{ and any Markovian strategy $\beta^i$} , \quad J^i(\hat {\bm \alpha}) \leq J^i(\hat \alpha^{1}, \ldots,\hat \alpha^{i-1}, \beta^i, \hat \alpha^{i+1}, \ldots, \hat \alpha^{N}),
\end{equation}
where on the right-hand side, $(\hat \alpha^1, \ldots, \hat \alpha^{i-1}, \beta^i, \hat \alpha^{i+1}, \ldots, \hat \alpha^N)(t, \bX_t)$ is used in \eqref{def_Xt_general} to solve for $\bX_t$. 
\end{defi}


In this case, due to the fact that each player's control is a function of the other players' states, any change in $\bX_t$ will cause a change in $\balpha_t$. As a consequence, merely adapting the algorithm in Section~\ref{sec:open-loop} will not be efficient nor parallelizable. In the Markovian setting, finding a Nash equilibrium can be reduced to solving $N$ coupled HJB equations. To this end, we define $u^i(t, \bm x)$ as the value function of player $i$. 
For the sake of notation clarity, we present the discussion based on the ``vectorized'' system,
\begin{equation}\label{def_Xt_markovian}
\ud \bm X_t = b(t, \bm X_t, \bm \alpha(t, \bm X_t)) \ud t + \Sigma(t, \bm X_t, \balpha(t, \bX_t)) \ud \bm W_t, \quad \bm X_0 = \bm x_0,
\end{equation}
where $\bX$, $b(t, \bx, \balpha(t, \bx))$ and $\bW$ are vectorizations of $X^i, b^i, W^i$ respectively, and $\Sigma(t, \bx, \balpha(t, \bx))$ is matrix-valued given by
\begin{equation}
   \bX_t = \begin{bmatrix}
  X_t^1 \\
  X_t^2\\
  \vdots\\
  X_t^N
\end{bmatrix}, \quad
b = \begin{bmatrix}
     b^1\\
     b^2\\
     \vdots\\
     b^N
\end{bmatrix}, \quad 
\bW_t =\begin{bmatrix}
     W_t^0\\
     W_t^1 \\
     \vdots\\
     W_t^N
\end{bmatrix}, \quad
\Sigma = \begin{bmatrix}
     \sigma^0 & \sigma^1\\
     \sigma^0 & \sigma^2 \\
     \vdots & \vdots \\
     \sigma^0 & \sigma^N
\end{bmatrix}.
\end{equation}
Using the dynamic programming principle, the HJB system reads
\begin{align}
\label{def_HJB}
\begin{dcases}
    \partial_t u^i(t,\bx) + G^i(t,\bm x,\hat{\bm \alpha}(t,\bx), \nabla_{\bx} u^i(t,\bx), \text{Hess}_{\bx} u^i(t,\bx))  = 0,\\
u^i(T,\bx) = g^i(\bx), \quad i \in \mc{I},
    \\
    \hat\alpha^i(t,\bx) = \inf_{\alpha^i \in \mc{A}} G^i(t,\bm x, (\alpha^i, \hat{\bm \alpha}^{-i}(t,\bx)), \nabla_{\bx} u^i(t,\bx), \text{Hess}_{\bx} u^i(t,\bx)),
\end{dcases}
\end{align}
where $G^i$ is given by 
\begin{equation}\label{def:Gi}
    G^i = G^i(t,\bm x, \bm \alpha, p, q) =  b(t,\bm x, \bm \alpha) \cdot p + f^i(t, \bm x, \bm \alpha) + \half \text{Tr}(\Sigma\Sigma\transpose(t, \bx, \balpha) q).
\end{equation}

Note that when $N = 1$, $G^i$ coincides with the $H$ function defined in \eqref{def:control-H}. When $N>1$, the equation for $u^i$ in \eqref{def_HJB} depends on the equilibrium controls of the other players, which themselves depend on the value functions of the these players. Hence the $N$ equations are coupled. 


Han and Hu proposed in~\cite{HaHu:19} a version of the DFP algorithm for finding Markovian NE. Similar to  ideas reviewed in Section~\ref{sec:open-loop}, the optimization problems for the $N$ players are decoupled by fictitious play in the following sense. Starting with a guess of the solution $\bm\alpha^0 \in \mathbb{A}$, the algorithm iteratively updates $\bm\alpha^{\mt{k}+1}$: at iteration $\mt{k}+1$, $\bm \alpha^\mt{k}$ is observed by all players, and player $i$'s decision problem is 
\begin{equation}\label{def_J_fictitious}
\inf_{\alpha^i \in \mathbb{A}^i} J^i(\alpha^i; \bm \alpha^{-i,\mt{k}}),
\end{equation}
where $J^i$ is defined in \eqref{def:DFP-open-cost}, and the state process $\bm X_t$ is given in \eqref{def_Xt_markovian} with $\bm \alpha$ replaced by $(\alpha^i, \bm \alpha^{-i,\mt{k}})$. This problem is decoupled from the problems solved by the other players at the same iteration. The optimal strategy, assuming it exists, is denoted by $\alpha^{i, \mt{k}+1}$. 
Problems \eqref{def_J_fictitious} for all $i \in \mc{I}$ are solved simultaneously using $\bm \alpha^{-i, \mt{k}}$, and the optimal responses together form $\bm \alpha^{\mt{k}+1}$. Thanks to the Markovian structure, problem \eqref{def_J_fictitious} corresponds to solving the HJB equation
\begin{equation}
\label{eq:FP_PDE}
    \partial_t u^{i,\mt{k}+1} + \inf_{\alpha^i \in \mc{A}^i} G^i(t, \bm x, (\alpha^i, \bm \alpha^{-i,\mt{k}}(t, \bm x)), \nabla_{\bx} u^{i, \mt{k}+1}, \text{Hess}_{\bx}u^{i, \mt{k}+1}, u^{i, \mt{k}+1}) = 0,
\end{equation}
with the terminal condition $u^{i,\mt{k}+1}(T,\bx) = g^i(\bx)$. In the uncontrolled volatility problem (as {\it e.g.} in~\cite{HaHu:19}), $G^i$ does not depend on the $q$ variable and hence the PDE is semi-linear. Let us assume that the minimizer $\argmin_{\alpha^i \in \mc{A}^i} G^i(t, \bm x, \bm \alpha, p, s)$ exists, is unique, and can be computed as an explicit function of other arguments:  $(t, \bm x, (\alpha^j)_{j \neq i}, p, s) \mapsto \alpha^{i,\mt{k}+1,*}(t, \bm x, (\alpha^j)_{j \neq i}, p, s)$. Then, the new control for player $i$ is given by: 
\begin{equation}\label{def_alphaast}
    \alpha^{i, \mt{k}+1}(t, \bm x) = \alpha^{i,\mt{k}+1,*}(t, \bm x, \bm \alpha^{-i,\mt{k}}(t, \bm x), \nabla_{\bx} u^{i, \mt{k}+1}(t, \bx), u^{i, \mt{k}+1}(t, \bx)).
\end{equation} 
Solving~\eqref{eq:FP_PDE} for all $i\in \mc{I}$ completes one stage in the loop of fictitious play. 

To solve numerically the $N$ decoupled semi-linear PDEs, one can for example interpret their solution through BSDEs via the non-linear Feynman-Kac formula~\eqref{def_control_BSDEreform}:
\begin{empheq}[left=\empheqlbrace]{align}
    &\bX_t^{i, \mt{k}+1} = \bx_0 + \int_{0}^{t}\tilde b^{i}(s,\bX_s^{i, \mt{k}+1}; \balpha^{-i, \mt{k}}(s, \bX_s^{i, \mt{k}+1}))\, \mathrm{d}s + \int_{0}^{t}\Sigma(s,\bX_s^{i, \mt{k}+1})\, \mathrm{d}\bW_s, \label{eq:BSDE_forward} \\
    &Y_t^{i, \mt{k}+1} = g^i(\bX_T^{i, \mt{k}+1}) \nonumber \\ &\hspace{5em} + \int_{t}^{T}h^i(s,\bX_s^{i, \mt{k}+1}, Y_s^{i, \mt{k}+1}, \bZ_s^{i, \mt{k}+1};  \balpha^{-i, \mt{k}}(s, \bX_s^{i, \mt{k}+1}))\, \mathrm{d}s - \int_{t}^{T}(\bZ_s^{i, \mt{k}+1})\transpose\, \mathrm{d}\bW_s, \label{eq:BSDE_backward} 
\end{empheq}
with $ Y_t^{i, \mt{k}+1} = u^{i, \mt{k}+1}(t, \bX_t^{i, \mt{k}+1})$ and $\bZ_t^{i, \mt{k}+1} = \Sigma(t, \bX_t^{i, \mt{k}+1})\transpose\nabla_{\bx} u^{i, \mt{k}+1}(t, \bX_t^{i, \mt{k}+1})$, where $\tilde b^i$ and $h^i$ are functions such that~\eqref{eq:FP_PDE} can be rewritten as
\begin{align}
     \partial_t u^{i,\mt{k}+1} + \half \text{Tr}(\Sigma\transpose \text{Hess}_{\bx} u^{i,\mt{k}+1} \Sigma) &+ \tilde b^i(t, \bx; \balpha^{-i, \mt{k}})\cdot \nabla_{\bx} u^{i, \mt{k}+1} 
     \notag 
     \\ 
     &+h^i(t, \bx,  u^{i, \mt{k}+1}, \Sigma\transpose\nabla_{\bx} u^{i, \mt{k}+1}; \balpha^{-i, \mt{k}})=0. \label{eq:FP_PDE_explicit}
\end{align}
We stress once again that $\balpha^{-i,\mt{k}}$ are known functions when solving the BSDE,  which is why the BSDEs for $i \in \mc{I}$ are decoupled. They can thus be solved in parallel, for example using the deep BSDE algorithm (see Section~\ref{subsubsec:deepBSDE}).





For each player $i$, the BSDE \eqref{eq:BSDE_forward}--\eqref{eq:BSDE_backward} is then solved using the deep BSDE algorithm (see Section~\ref{subsubsec:deepBSDE}). More precisely, 
let us consider the minimization problem (for less cumbersome notations, the subscript $t_n$ in $\bX, \bY, \bZ$ has been replaced by $n$, and the superscript $\mt{k}$ that denotes the index of fictitious play is dropped for simplicity), 
\begin{align}
&\inf_{\psi_0\in \cN_0^{i'},~\{\phi_n\in \cN_n^i\}_{n=0}^{N_T-1} } \EE|g^i(\check\bX_{N_T}^{i}) - \check \bY_{N_T}^{i}|^2, \label{eq:disc_objective}\\
&s.t.~~ \check\bX_0^{i}=\bx_0, \quad \check \bY_0^{i} =\psi_0(\check \bX_0^{i}), \quad \check \bZ_{n}^{i}=\phi_n(\check \bX_{n}^{i}), \quad n=0,\dots,N_T-1\notag \\
&\qquad \check\bX_{n+1}^{i} = \check \bX_{n}^{i} + \tilde b^i(t_n,\check\bX_{n}^{i}; \balpha^{-i}(t_n, \check\bX_n^{i}))\Delta t +\Sigma(t_n,\check\bX_{n}^{i})\Delta \bW_{t_n}, \label{eq:disc_X_path} \\
 &\qquad \check \bY_{n+1}^{i} = \check \bY_{n}^{i} - h^i(t_n,\check \bX_{n}^{i},\check \bY_{n}^{i},\check \bZ_{n}^{i}; \balpha^{-i}(t_n, \check\bX_n^{i}))\Delta t + (\check\bZ_{n}^{i})\transpose\Delta \bW_{t_n}, \label{eq:disc_Y_path}
\end{align}
where we recall that
  $\Delta t = t_{n+1} - t_n, \quad \Delta \bW_{t_n} = \bW_{ t_{n+1}}-\bW_{t_n}.$
Here $\cN_0^{i'}$ and $\{\cN_n^i\}_{n=0}^{N_T-1}$ are hypothesis spaces of player $i$ related to deep neural networks. The goal of the optimization is to find optimal deterministic maps $\psi_0^{i,\ast}, \{\phi_n^{i,\ast}\}_{n=0}^{N_T-1}$ such that the loss function is small. The pseudo-code of the proposed deep fictitious play algorithm is summarized in Algorithm \ref{def_algorithm1} in Appendix~\ref{supp:pseudocode}.



\begin{rem}
In additional to the cost functional considered in \eqref{def:DFP-open-cost}, in \cite{HaHu:19} Han and Hu also considered the risk-sensitive minimization problem,
\begin{equation}\label{def_J'}
J^i(\bm \alpha) = \EE\left[\xi_i \exp\left\{\xi_i\left(\int_0^T f^i(t, \bm X_t, \bm \alpha(t, \bm X_s)) \ud t + g^i(\bm X_T)\right)\right\} \right],
\end{equation}
where $\xi_i$ is a parameter characterizing how risk-averse or risk-seeking player $i$ is. This flexibility allows one to model much broader classes of games that accommodate the players' attitudes to risk. In this case, the $G^i$ function defined in \eqref{def:Gi} becomes
\begin{equation}
     G^i = G^i(t,\bm x, \bm \alpha, p, q, s) =  b(t,\bm x, \bm \alpha) \cdot p + \xi_i sf^i(t, \bm x, \bm \alpha) + \half \text{Tr}(\Sigma\Sigma\transpose (t, \bx, \balpha) q ).  
\end{equation}
Numerical examples on risk-sensitive problems are also presented therein.
\end{rem}



\begin{rem}[Theoretical analysis]
In \cite{han2020convergence}, Han, Hu and Long provided a theoretical foundation for the DFP algorithm for Markovian Nash equilibria with the objective~\eqref{def:DFP-open-cost}. They proved the convergence to the true Nash equilibrium if the decoupled sub-problems \eqref{def_J_fictitious}--\eqref{eq:FP_PDE} are solved exactly and repeatedly. They also gave an a posteriori error bound on the numerical error on the deep BSDE algorithm,  identified the $\eps$-Nash equilibrium produced by DFP, and analyzed the numerical performance of the algorithm on the original game. 
\end{rem}

















The DFP algorithm has then been extended in \cite{chen2021large} with the Scaled Deep Fictitious Play (SDFP) algorithm. There, the authors integrated the importance sampling and invariant layer embedding into DFP. Then focusing on the homogeneous agent problem, they utilized the symmetry and showed numerical experiments with up to $N = 3,000$ agents. Along with the idea of combining fictitious play and deep learning, \cite{han2021deepham} recently studied the heterogeneous agent model in macroeconomics. In addition to parameterizing the control (as proposed in \cite{han2016deep-googlecitations}), they also use neural networks to approximate the value function to reduce further the computational complexity of evaluating expectations of the type \eqref{eq:OL-cost}.


\paragraph*{Numerical illustration: the linear-quadratic systemic risk example revisited.}\label{sec:LQsysrisk-revisited} 
Here we revisit the example introduced in Section~\ref{sec:intro-LQsysrisk} and studied in Section~\ref{sec:open-loop}, but focus on the Markovian Nash equilibrium.


To describe the model in the form of \eqref{def_Xt_markovian}, we concatenate the log-monetary reserves $X_t^i$ of $N$ banks to form $\bX_t=[X_t^1,\dots,X_t^N]\transpose$.
The associated drift term and diffusion term are defined as
\begin{equation}
\label{eq:example1_driftdiff}
b(t, \bm x, \balpha)=
[a(\bar x - x^1) + \alpha^1, 
\ldots, 
a(\bar x - x^N) + \alpha^N
]\transpose\in \RR^{N\times1}
, \quad \bar x = \frac{1}{N}\sum_{i=1}^N x^i, \end{equation}
\begin{equation}
\label{eq:example1_sigma}
\Sigma(t, \bm x)=
\begin{bmatrix}
     \sigma\rho & \sigma\sqrt{1-\rho^2} & 0 & \cdots & 0\\
     \sigma\rho & 0 & \sigma\sqrt{1-\rho^2} & \cdots & 0\\
     \vdots & \vdots & \vdots & \ddots & \vdots\\
     \sigma\rho & 0 & 0 & \cdots &\sigma\sqrt{1-\rho^2}
\end{bmatrix}\in \RR^{N\times (N+1)},
\end{equation}
and $\bW_t = (W_t^0, \ldots, W_t^N)$ is $(N+1)$-dimensional. Recall the running and terminal costs that player $i$ aims to minimize
\begin{equation}
f^i(t, \bm{x},\balpha) = \half (\alpha^i)^2 - q \alpha^i(\bar x - x^i) + \frac{\eps}{2}(\bar x - x^i)^2,  \quad g^i(\bm{x}) = \frac{c}{2}(\bar x - x^i)^2.
\end{equation}
The closed-form Nash equilibrium is detailed in \cite[Sections 3.2-3.3]{CaFoSu:15}.

The coupled HJB system corresponding to this game reads
\begin{equation}\label{eq_HJBexample1}
\partial_t u^i + \inf_{\alpha^i}\left\{\sum_{j=1}^N [a(\bar x - x^j) + \alpha^j] \partial_{x^j}u^i + \frac{(\alpha^i)^2}{2} - q\alpha^i(\bar x - x^i) + \frac{\eps}{2}(\bar x - x^i)^2 \right\} 
+ \half \text{Tr}(\Sigma\transpose \text{Hess}_{\bx} u^i \Sigma) = 0,
\end{equation} 
with the terminal condition $u^i(T,\bm x) = \frac{c}{2}(\bar x - x^i)^2$, $i \in \mc{I}$. The minimizer in the infimum gives a candidate of the optimal control for player $i$:
$\alpha^{i}(t, \bm x) = q(\bar x - x^i) - \partial_{x^i}u^i(t, \bm x).$ 
Plugging it back into the $i^{th}$ equation yields a PDE of form \eqref{eq:FP_PDE_explicit},
\begin{multline}\label{eq:example1}
    \partial_t u^i + \half \text{Tr}(\Sigma\transpose \text{Hess}_{\bx} u^i \Sigma) + a(\bar x - x^i)\partial_{x^i}u^i + \sum_{j\neq i} [a(\bar x - x^j) + \alpha^{j}(t,\bm x)] \partial_{x^j}u^i \\
 + \frac{\eps}{2}(\bar x - x^i)^2 - \half(q(\bar x - x^i) - \partial_{x^i} u^i)^2= 0,
\end{multline}
where $\alpha^j$ with $j\neq i$ are considered exogenous for player $i$'s problem, and are given by the best responses of the other players from the previous stage. To be precise, $\tilde b^i$ and $h^i$ in \eqref{eq:FP_PDE_explicit} are defined as
\begin{align}
\tilde b^i(t, \bx; \balpha^{-i})&=
[
a(\bar{x} - x^1) + \alpha^1, 
\ldots,
a(\bar{x} - x^i),
\ldots,
a(\bar{x} - x^N) + \alpha^N
]\transpose,\\
h^i(t, \bx,  y, \bz;  \balpha^{-i})&=\frac{\eps}{2}(\bar x - x^i)^2 - \half(q(\bar x - x^i) - \frac{z^i}{\sigma\sqrt{1-\rho^2}})^2,
\end{align}
where $\bz=(z^0, z^1,\dots, z^N)\in\RR^{N+1}$. 


Figures~\ref{fig:LQMFG_err}--\ref{fig:LQMFG_path} show the performance of the DFP algorithm on a ten-player game, using the parameter,
\begin{equation}\label{def_parameters}
    a = 0.1,\quad  q = 0.1, \quad c = 0.5,\quad  \eps = 0.5,\quad  \rho = 0.2, \quad \sigma = 1, \quad T = 1.
\end{equation}
The relative squared error (RSE) is defined  by
{\footnotesize
\begin{align*}
    \text{RSE} = \frac{\sum_{\substack{i \in \mathcal{I}\\1 \leq  j \leq J}} \left(u^i(0, \bm x_{t_0}^{(j)}) - \widehat u^i(0,\bm x_{t_0}^{(j)})\right)^2}{\sum_{\substack{i \in \mathcal{I}\\ 1 \leq j \leq J}} \left(u^i(0, \bm x_{t_0}^{(j)}) - \bar u^i\right)^2},
    \;\text{or }\; 
    \text{RSE} = \frac{\sum_{\substack{i \in \mathcal{I}\\ 0 \leq n \leq N_T-1 \\ 1 \leq j \leq J}} \left(\nabla_{\bx} u^i(t_n, \bm x_{t_n}^{(j)}) - \nabla_{\bx} \widehat u^i(t_n,\bm x_{t_n}^{(j)})\right)^2}{\sum_{\substack{i \in \mathcal{I}\\ 0 \leq n \leq N_T-1\\1 \leq j \leq J}} \left(\nabla_{\bx} u^i(t_n, \bm x_{t_n}^{(j)})- \overline {\nabla_{\bx} u}^i\right)^2},
\end{align*}
}where  $\hat{u}^i$ is the prediction from the neural networks,
and $\bar u^i$ (resp. $\overline {\nabla_{\bx} u}^i$) is the average of $u^i ~(\emph{resp.~} \nabla_{\bx}u^i)$ evaluated at all the indices $j, n$. 
To compute the relative error, $J=256$ ground truth sample paths $\{\bm x_{t_n}^{(j)}\}_{n=0}^{N_T-1}$ are generated using an Euler scheme based on  \eqref{def_Xt_markovian}\eqref{eq:example1_driftdiff}\eqref{eq:example1_sigma} and the true optimal strategy. Note that the superscript ${(j)}$ here does not mean the player index, but the $j^{th}$ path for all players. 

In particular, Figure~\ref{fig:LQMFG_err} compares the relative squared error
as $N_{\text{SGD\_per\_stage}}$ varies from 10 to 400. The convergence of the learning curves with small $N_{\text{SGD\_per\_stage}}$ asserts that each individual problem does not need to be solved so accurately. Furthermore, the fact that the performances are similar under different $N_{\text{SGD\_per\_stage}}$ with the same total budget of SGD updates suggest that the algorithm is insensitive to the choice of this hyperparameter. %
The final relative squared errors of $u$ and $\nabla u$ averaged from three independent runs of deep fictitious play are 4.6\% and 0.2\%,  respectively. 
Figure~\ref{fig:LQMFG_path} presents one sample path for each player of the optimal state process $X_t^i$ and the optimal control $\alpha_t^i$ \emph{vs.} their approximations $\hat{X}_t^i, \hat{\alpha}_t^i$ provided by the optimized neural networks.



\begin{figure}[!ht]
    \centering
    \includegraphics[width=\figwidth]{figure/LQMFG_err.pdf}
    \caption{Linear-quadratic systemic risk example in Section~\ref{sec:MarkovianNE}. The relative squared errors of $u^i$ (left) and $\nabla u^i$ (right) along the training process of deep fictitious play for the inter-bank game. 
    The relative squared errors of $u^i(0, \check \bX_0^{i})$ and $\{\nabla u^i(t_n, \check \bX_n^{i})\}_{n=0}^{N_{T-1}}$ are evaluated. 
    The error is computed every 400 SGD updates, averaged over all the players. A smoothed moving average with window size 3 is applied in the final plots. %
    }
    \label{fig:LQMFG_err}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\figwidth]{figure/LQMFG_path.pdf}
    \caption{Linear-quadratic systemic risk example in Section~\ref{sec:MarkovianNE}. A sample path for each player of the inter-bank game with $N=10$. Top: the optimal state process $X_t^i$ (solid lines) and its approximation $\hat{X}_t^i$ (circles) provided by the optimized neural networks, under the same realized path of Brownian motion. Bottom: comparisons of the strategies $\alpha_t^i$ and $\hat{\alpha}_t^i$ (dashed lines). %
    }
    \label{fig:LQMFG_path}
\end{figure}
